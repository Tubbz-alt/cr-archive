<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/share/c1/c1_LIRGenerator.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="c1_LIR.hpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="../ci/ciField.cpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/share/c1/c1_LIRGenerator.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #include &quot;precompiled.hpp&quot;
  26 #include &quot;c1/c1_Compilation.hpp&quot;
  27 #include &quot;c1/c1_Defs.hpp&quot;
  28 #include &quot;c1/c1_FrameMap.hpp&quot;
  29 #include &quot;c1/c1_Instruction.hpp&quot;
  30 #include &quot;c1/c1_LIRAssembler.hpp&quot;
  31 #include &quot;c1/c1_LIRGenerator.hpp&quot;
  32 #include &quot;c1/c1_ValueStack.hpp&quot;
  33 #include &quot;ci/ciArrayKlass.hpp&quot;
  34 #include &quot;ci/ciInstance.hpp&quot;
  35 #include &quot;ci/ciObjArray.hpp&quot;
  36 #include &quot;ci/ciUtilities.hpp&quot;


  37 #include &quot;gc/shared/barrierSet.hpp&quot;
  38 #include &quot;gc/shared/c1/barrierSetC1.hpp&quot;
  39 #include &quot;oops/klass.inline.hpp&quot;
  40 #include &quot;runtime/arguments.hpp&quot;
  41 #include &quot;runtime/sharedRuntime.hpp&quot;
  42 #include &quot;runtime/stubRoutines.hpp&quot;
  43 #include &quot;runtime/vm_version.hpp&quot;
  44 #include &quot;utilities/bitMap.inline.hpp&quot;
  45 #include &quot;utilities/macros.hpp&quot;
  46 #include &quot;utilities/powerOfTwo.hpp&quot;
  47 
  48 #ifdef ASSERT
  49 #define __ gen()-&gt;lir(__FILE__, __LINE__)-&gt;
  50 #else
  51 #define __ gen()-&gt;lir()-&gt;
  52 #endif
  53 
  54 #ifndef PATCHED_ADDR
  55 #define PATCHED_ADDR  (max_jint)
  56 #endif
</pre>
<hr />
<pre>
 194   ResolveNode* source = source_node(src);
 195   source-&gt;append(destination_node(dest));
 196 }
 197 
 198 
 199 //--------------------------------------------------------------
 200 // LIRItem
 201 
 202 void LIRItem::set_result(LIR_Opr opr) {
 203   assert(value()-&gt;operand()-&gt;is_illegal() || value()-&gt;operand()-&gt;is_constant(), &quot;operand should never change&quot;);
 204   value()-&gt;set_operand(opr);
 205 
 206   if (opr-&gt;is_virtual()) {
 207     _gen-&gt;_instruction_for_operand.at_put_grow(opr-&gt;vreg_number(), value(), NULL);
 208   }
 209 
 210   _result = opr;
 211 }
 212 
 213 void LIRItem::load_item() {


 214   if (result()-&gt;is_illegal()) {
 215     // update the items result
 216     _result = value()-&gt;operand();
 217   }
 218   if (!result()-&gt;is_register()) {
 219     LIR_Opr reg = _gen-&gt;new_register(value()-&gt;type());
 220     __ move(result(), reg);
 221     if (result()-&gt;is_constant()) {
 222       _result = reg;
 223     } else {
 224       set_result(reg);
 225     }
 226   }
 227 }
 228 
 229 
 230 void LIRItem::load_for_store(BasicType type) {
 231   if (_gen-&gt;can_store_as_constant(value(), type)) {
 232     _result = value()-&gt;operand();
 233     if (!_result-&gt;is_constant()) {
</pre>
<hr />
<pre>
 621     assert(right_op != result_op, &quot;malformed&quot;);
 622     __ move(left_op, result_op);
 623     left_op = result_op;
 624   }
 625 
 626   switch(code) {
 627     case Bytecodes::_iand:
 628     case Bytecodes::_land:  __ logical_and(left_op, right_op, result_op); break;
 629 
 630     case Bytecodes::_ior:
 631     case Bytecodes::_lor:   __ logical_or(left_op, right_op, result_op);  break;
 632 
 633     case Bytecodes::_ixor:
 634     case Bytecodes::_lxor:  __ logical_xor(left_op, right_op, result_op); break;
 635 
 636     default: ShouldNotReachHere();
 637   }
 638 }
 639 
 640 
<span class="line-modified"> 641 void LIRGenerator::monitor_enter(LIR_Opr object, LIR_Opr lock, LIR_Opr hdr, LIR_Opr scratch, int monitor_no, CodeEmitInfo* info_for_exception, CodeEmitInfo* info) {</span>

 642   if (!GenerateSynchronizationCode) return;
 643   // for slow path, use debug info for state after successful locking
<span class="line-modified"> 644   CodeStub* slow_path = new MonitorEnterStub(object, lock, info);</span>
 645   __ load_stack_address_monitor(monitor_no, lock);
 646   // for handling NullPointerException, use debug info representing just the lock stack before this monitorenter
<span class="line-modified"> 647   __ lock_object(hdr, object, lock, scratch, slow_path, info_for_exception);</span>
 648 }
 649 
 650 
 651 void LIRGenerator::monitor_exit(LIR_Opr object, LIR_Opr lock, LIR_Opr new_hdr, LIR_Opr scratch, int monitor_no) {
 652   if (!GenerateSynchronizationCode) return;
 653   // setup registers
 654   LIR_Opr hdr = lock;
 655   lock = new_hdr;
 656   CodeStub* slow_path = new MonitorExitStub(lock, UseFastLocking, monitor_no);
 657   __ load_stack_address_monitor(monitor_no, lock);
 658   __ unlock_object(hdr, object, lock, scratch, slow_path);
 659 }
 660 
 661 #ifndef PRODUCT
 662 void LIRGenerator::print_if_not_loaded(const NewInstance* new_instance) {
 663   if (PrintNotLoaded &amp;&amp; !new_instance-&gt;klass()-&gt;is_loaded()) {
 664     tty-&gt;print_cr(&quot;   ###class not loaded at new bci %d&quot;, new_instance-&gt;printable_bci());
 665   } else if (PrintNotLoaded &amp;&amp; (TieredCompilation &amp;&amp; new_instance-&gt;is_unresolved())) {
 666     tty-&gt;print_cr(&quot;   ###class not resolved at new bci %d&quot;, new_instance-&gt;printable_bci());
 667   }
</pre>
<hr />
<pre>
 769       if (src_type != NULL) {
 770         if (src_type-&gt;element_type()-&gt;is_subtype_of(dst_type-&gt;element_type())) {
 771           is_exact = true;
 772           expected_type = dst_type;
 773         }
 774       }
 775     }
 776     // at least pass along a good guess
 777     if (expected_type == NULL) expected_type = dst_exact_type;
 778     if (expected_type == NULL) expected_type = src_declared_type;
 779     if (expected_type == NULL) expected_type = dst_declared_type;
 780 
 781     src_objarray = (src_exact_type &amp;&amp; src_exact_type-&gt;is_obj_array_klass()) || (src_declared_type &amp;&amp; src_declared_type-&gt;is_obj_array_klass());
 782     dst_objarray = (dst_exact_type &amp;&amp; dst_exact_type-&gt;is_obj_array_klass()) || (dst_declared_type &amp;&amp; dst_declared_type-&gt;is_obj_array_klass());
 783   }
 784 
 785   // if a probable array type has been identified, figure out if any
 786   // of the required checks for a fast case can be elided.
 787   int flags = LIR_OpArrayCopy::all_flags;
 788 










 789   if (!src_objarray)
 790     flags &amp;= ~LIR_OpArrayCopy::src_objarray;
 791   if (!dst_objarray)
 792     flags &amp;= ~LIR_OpArrayCopy::dst_objarray;
 793 
 794   if (!x-&gt;arg_needs_null_check(0))
 795     flags &amp;= ~LIR_OpArrayCopy::src_null_check;
 796   if (!x-&gt;arg_needs_null_check(2))
 797     flags &amp;= ~LIR_OpArrayCopy::dst_null_check;
 798 
 799 
 800   if (expected_type != NULL) {
 801     Value length_limit = NULL;
 802 
 803     IfOp* ifop = length-&gt;as_IfOp();
 804     if (ifop != NULL) {
 805       // look for expressions like min(v, a.length) which ends up as
 806       //   x &gt; y ? y : x  or  x &gt;= y ? y : x
 807       if ((ifop-&gt;cond() == If::gtr || ifop-&gt;cond() == If::geq) &amp;&amp;
 808           ifop-&gt;x() == ifop-&gt;fval() &amp;&amp;
</pre>
<hr />
<pre>
1411       case T_FLOAT:
1412         if (c-&gt;as_jint_bits() != other-&gt;as_jint_bits()) continue;
1413         break;
1414       case T_LONG:
1415       case T_DOUBLE:
1416         if (c-&gt;as_jint_hi_bits() != other-&gt;as_jint_hi_bits()) continue;
1417         if (c-&gt;as_jint_lo_bits() != other-&gt;as_jint_lo_bits()) continue;
1418         break;
1419       case T_OBJECT:
1420         if (c-&gt;as_jobject() != other-&gt;as_jobject()) continue;
1421         break;
1422       default:
1423         break;
1424       }
1425       return _reg_for_constants.at(i);
1426     }
1427   }
1428 
1429   LIR_Opr result = new_register(t);
1430   __ move((LIR_Opr)c, result);
<span class="line-modified">1431   _constants.append(c);</span>
<span class="line-modified">1432   _reg_for_constants.append(result);</span>


1433   return result;
1434 }
1435 






1436 //------------------------field access--------------------------------------
1437 
1438 void LIRGenerator::do_CompareAndSwap(Intrinsic* x, ValueType* type) {
1439   assert(x-&gt;number_of_arguments() == 4, &quot;wrong type&quot;);
1440   LIRItem obj   (x-&gt;argument_at(0), this);  // object
1441   LIRItem offset(x-&gt;argument_at(1), this);  // offset of field
1442   LIRItem cmp   (x-&gt;argument_at(2), this);  // value to compare with field
1443   LIRItem val   (x-&gt;argument_at(3), this);  // replace field with val if matches cmp
1444   assert(obj.type()-&gt;tag() == objectTag, &quot;invalid type&quot;);
1445 
1446   // In 64bit the type can be long, sparc doesn&#39;t have this assert
1447   // assert(offset.type()-&gt;tag() == intTag, &quot;invalid type&quot;);
1448 
1449   assert(cmp.type()-&gt;tag() == type-&gt;tag(), &quot;invalid type&quot;);
1450   assert(val.type()-&gt;tag() == type-&gt;tag(), &quot;invalid type&quot;);
1451 
1452   LIR_Opr result = access_atomic_cmpxchg_at(IN_HEAP, as_BasicType(type),
1453                                             obj, offset, cmp, val);
1454   set_result(x, result);
1455 }
</pre>
<hr />
<pre>
1514       value.load_byte_item();
1515     } else  {
1516       value.load_item();
1517     }
1518   } else {
1519     value.load_for_store(field_type);
1520   }
1521 
1522   set_no_result(x);
1523 
1524 #ifndef PRODUCT
1525   if (PrintNotLoaded &amp;&amp; needs_patching) {
1526     tty-&gt;print_cr(&quot;   ###class not loaded at store_%s bci %d&quot;,
1527                   x-&gt;is_static() ?  &quot;static&quot; : &quot;field&quot;, x-&gt;printable_bci());
1528   }
1529 #endif
1530 
1531   if (x-&gt;needs_null_check() &amp;&amp;
1532       (needs_patching ||
1533        MacroAssembler::needs_explicit_null_check(x-&gt;offset()))) {
<span class="line-modified">1534     // Emit an explicit null check because the offset is too large.</span>
<span class="line-modified">1535     // If the class is not loaded and the object is NULL, we need to deoptimize to throw a</span>
<span class="line-modified">1536     // NoClassDefFoundError in the interpreter instead of an implicit NPE from compiled code.</span>
<span class="line-modified">1537     __ null_check(object.result(), new CodeEmitInfo(info), /* deoptimize */ needs_patching);</span>












1538   }
1539 
1540   DecoratorSet decorators = IN_HEAP;
1541   if (is_volatile) {
1542     decorators |= MO_SEQ_CST;
1543   }
1544   if (needs_patching) {
1545     decorators |= C1_NEEDS_PATCHING;
1546   }
1547 
1548   access_store_at(decorators, field_type, object, LIR_OprFact::intConst(x-&gt;offset()),
1549                   value.result(), info != NULL ? new CodeEmitInfo(info) : NULL, info);
1550 }
1551 




























































































































1552 void LIRGenerator::do_StoreIndexed(StoreIndexed* x) {
1553   assert(x-&gt;is_pinned(),&quot;&quot;);


1554   bool needs_range_check = x-&gt;compute_needs_range_check();
1555   bool use_length = x-&gt;length() != NULL;
1556   bool obj_store = is_reference_type(x-&gt;elt_type());
<span class="line-modified">1557   bool needs_store_check = obj_store &amp;&amp; (x-&gt;value()-&gt;as_Constant() == NULL ||</span>
<span class="line-modified">1558                                          !get_jobject_constant(x-&gt;value())-&gt;is_null_object() ||</span>
<span class="line-modified">1559                                          x-&gt;should_profile());</span>
1560 
1561   LIRItem array(x-&gt;array(), this);
1562   LIRItem index(x-&gt;index(), this);
1563   LIRItem value(x-&gt;value(), this);
1564   LIRItem length(this);
1565 
1566   array.load_item();
1567   index.load_nonconstant();
1568 
1569   if (use_length &amp;&amp; needs_range_check) {
1570     length.set_instruction(x-&gt;length());
1571     length.load_item();
<span class="line-modified">1572 </span>
<span class="line-modified">1573   }</span>

1574   if (needs_store_check || x-&gt;check_boolean()) {
1575     value.load_item();
1576   } else {
1577     value.load_for_store(x-&gt;elt_type());
1578   }
1579 
1580   set_no_result(x);
1581 
1582   // the CodeEmitInfo must be duplicated for each different
1583   // LIR-instruction because spilling can occur anywhere between two
1584   // instructions and so the debug information must be different
1585   CodeEmitInfo* range_check_info = state_for(x);
1586   CodeEmitInfo* null_check_info = NULL;
1587   if (x-&gt;needs_null_check()) {
1588     null_check_info = new CodeEmitInfo(range_check_info);
1589   }
1590 
1591   if (GenerateRangeChecks &amp;&amp; needs_range_check) {
1592     if (use_length) {
1593       __ cmp(lir_cond_belowEqual, length.result(), index.result());
1594       __ branch(lir_cond_belowEqual, new RangeCheckStub(range_check_info, index.result(), array.result()));
1595     } else {
1596       array_range_check(array.result(), index.result(), null_check_info, range_check_info);
1597       // range_check also does the null check
1598       null_check_info = NULL;
1599     }
1600   }
1601 














1602   if (GenerateArrayStoreCheck &amp;&amp; needs_store_check) {
1603     CodeEmitInfo* store_check_info = new CodeEmitInfo(range_check_info);
<span class="line-modified">1604     array_store_check(value.result(), array.result(), store_check_info, x-&gt;profiled_method(), x-&gt;profiled_bci());</span>
1605   }
1606 
<span class="line-modified">1607   DecoratorSet decorators = IN_HEAP | IS_ARRAY;</span>
<span class="line-modified">1608   if (x-&gt;check_boolean()) {</span>
<span class="line-modified">1609     decorators |= C1_MASK_BOOLEAN;</span>
<span class="line-modified">1610   }</span>














1611 
<span class="line-modified">1612   access_store_at(decorators, x-&gt;elt_type(), array, index.result(), value.result(),</span>
<span class="line-modified">1613                   NULL, null_check_info);</span>










1614 }
1615 
1616 void LIRGenerator::access_load_at(DecoratorSet decorators, BasicType type,
1617                                   LIRItem&amp; base, LIR_Opr offset, LIR_Opr result,
1618                                   CodeEmitInfo* patch_info, CodeEmitInfo* load_emit_info) {
1619   decorators |= ACCESS_READ;
1620   LIRAccess access(this, decorators, base, offset, type, patch_info, load_emit_info);
1621   if (access.is_raw()) {
1622     _barrier_set-&gt;BarrierSetC1::load_at(access, result);
1623   } else {
1624     _barrier_set-&gt;load_at(access, result);
1625   }
1626 }
1627 
1628 void LIRGenerator::access_load(DecoratorSet decorators, BasicType type,
1629                                LIR_Opr addr, LIR_Opr result) {
1630   decorators |= ACCESS_READ;
1631   LIRAccess access(this, decorators, LIR_OprFact::illegalOpr, LIR_OprFact::illegalOpr, type);
1632   access.set_resolved_addr(addr);
1633   if (access.is_raw()) {
</pre>
<hr />
<pre>
1683   decorators |= ACCESS_WRITE;
1684   // Atomic operations are SEQ_CST by default
1685   decorators |= ((decorators &amp; MO_DECORATOR_MASK) == 0) ? MO_SEQ_CST : 0;
1686   LIRAccess access(this, decorators, base, offset, type);
1687   if (access.is_raw()) {
1688     return _barrier_set-&gt;BarrierSetC1::atomic_add_at(access, value);
1689   } else {
1690     return _barrier_set-&gt;atomic_add_at(access, value);
1691   }
1692 }
1693 
1694 LIR_Opr LIRGenerator::access_resolve(DecoratorSet decorators, LIR_Opr obj) {
1695   // Use stronger ACCESS_WRITE|ACCESS_READ by default.
1696   if ((decorators &amp; (ACCESS_READ | ACCESS_WRITE)) == 0) {
1697     decorators |= ACCESS_READ | ACCESS_WRITE;
1698   }
1699 
1700   return _barrier_set-&gt;resolve(this, decorators, obj);
1701 }
1702 










































































1703 void LIRGenerator::do_LoadField(LoadField* x) {
1704   bool needs_patching = x-&gt;needs_patching();
1705   bool is_volatile = x-&gt;field()-&gt;is_volatile();
1706   BasicType field_type = x-&gt;field_type();
1707 
1708   CodeEmitInfo* info = NULL;
1709   if (needs_patching) {
1710     assert(x-&gt;explicit_null_check() == NULL, &quot;can&#39;t fold null check into patching field access&quot;);
1711     info = state_for(x, x-&gt;state_before());
1712   } else if (x-&gt;needs_null_check()) {
1713     NullCheck* nc = x-&gt;explicit_null_check();
1714     if (nc == NULL) {
1715       info = state_for(x);
1716     } else {
1717       info = state_for(nc);
1718     }
1719   }
1720 
1721   LIRItem object(x-&gt;obj(), this);
1722 
1723   object.load_item();
1724 
1725 #ifndef PRODUCT
1726   if (PrintNotLoaded &amp;&amp; needs_patching) {
1727     tty-&gt;print_cr(&quot;   ###class not loaded at load_%s bci %d&quot;,
1728                   x-&gt;is_static() ?  &quot;static&quot; : &quot;field&quot;, x-&gt;printable_bci());
1729   }
1730 #endif
1731 





1732   bool stress_deopt = StressLoopInvariantCodeMotion &amp;&amp; info &amp;&amp; info-&gt;deoptimize_on_exception();
1733   if (x-&gt;needs_null_check() &amp;&amp;
1734       (needs_patching ||
1735        MacroAssembler::needs_explicit_null_check(x-&gt;offset()) ||
1736        stress_deopt)) {
1737     LIR_Opr obj = object.result();
1738     if (stress_deopt) {
1739       obj = new_register(T_OBJECT);
1740       __ move(LIR_OprFact::oopConst(NULL), obj);
1741     }
1742     // Emit an explicit null check because the offset is too large.
1743     // If the class is not loaded and the object is NULL, we need to deoptimize to throw a
1744     // NoClassDefFoundError in the interpreter instead of an implicit NPE from compiled code.
1745     __ null_check(obj, new CodeEmitInfo(info), /* deoptimize */ needs_patching);
1746   }
1747 
1748   DecoratorSet decorators = IN_HEAP;
1749   if (is_volatile) {
1750     decorators |= MO_SEQ_CST;
1751   }
1752   if (needs_patching) {
1753     decorators |= C1_NEEDS_PATCHING;
1754   }
1755 
1756   LIR_Opr result = rlock_result(x, field_type);
1757   access_load_at(decorators, field_type,
1758                  object, LIR_OprFact::intConst(x-&gt;offset()), result,
1759                  info ? new CodeEmitInfo(info) : NULL, info);










1760 }
1761 
1762 
1763 //------------------------java.nio.Buffer.checkIndex------------------------
1764 
1765 // int java.nio.Buffer.checkIndex(int)
1766 void LIRGenerator::do_NIOCheckIndex(Intrinsic* x) {
1767   // NOTE: by the time we are in checkIndex() we are guaranteed that
1768   // the buffer is non-null (because checkIndex is package-private and
1769   // only called from within other methods in the buffer).
1770   assert(x-&gt;number_of_arguments() == 2, &quot;wrong type&quot;);
1771   LIRItem buf  (x-&gt;argument_at(0), this);
1772   LIRItem index(x-&gt;argument_at(1), this);
1773   buf.load_item();
1774   index.load_item();
1775 
1776   LIR_Opr result = rlock_result(x);
1777   if (GenerateRangeChecks) {
1778     CodeEmitInfo* info = state_for(x);
1779     CodeStub* stub = new RangeCheckStub(info, index.result());
</pre>
<hr />
<pre>
1854       __ move(LIR_OprFact::oopConst(NULL), obj);
1855       __ null_check(obj, new CodeEmitInfo(null_check_info));
1856     }
1857   }
1858 
1859   if (GenerateRangeChecks &amp;&amp; needs_range_check) {
1860     if (StressLoopInvariantCodeMotion &amp;&amp; range_check_info-&gt;deoptimize_on_exception()) {
1861       __ branch(lir_cond_always, new RangeCheckStub(range_check_info, index.result(), array.result()));
1862     } else if (use_length) {
1863       // TODO: use a (modified) version of array_range_check that does not require a
1864       //       constant length to be loaded to a register
1865       __ cmp(lir_cond_belowEqual, length.result(), index.result());
1866       __ branch(lir_cond_belowEqual, new RangeCheckStub(range_check_info, index.result(), array.result()));
1867     } else {
1868       array_range_check(array.result(), index.result(), null_check_info, range_check_info);
1869       // The range check performs the null check, so clear it out for the load
1870       null_check_info = NULL;
1871     }
1872   }
1873 
<span class="line-modified">1874   DecoratorSet decorators = IN_HEAP | IS_ARRAY;</span>




















































1875 
<span class="line-modified">1876   LIR_Opr result = rlock_result(x, x-&gt;elt_type());</span>
<span class="line-modified">1877   access_load_at(decorators, x-&gt;elt_type(),</span>
<span class="line-modified">1878                  array, index.result(), result,</span>
<span class="line-modified">1879                  NULL, null_check_info);</span>







1880 }
1881 










1882 
1883 void LIRGenerator::do_NullCheck(NullCheck* x) {
1884   if (x-&gt;can_trap()) {
1885     LIRItem value(x-&gt;obj(), this);
1886     value.load_item();
1887     CodeEmitInfo* info = state_for(x);
1888     __ null_check(value.result(), info);
1889   }
1890 }
1891 
1892 
1893 void LIRGenerator::do_TypeCast(TypeCast* x) {
1894   LIRItem value(x-&gt;obj(), this);
1895   value.load_item();
1896   // the result is the same as from the node we are casting
1897   set_result(x, value.result());
1898 }
1899 
1900 
1901 void LIRGenerator::do_Throw(Throw* x) {
</pre>
<hr />
<pre>
2525   Compilation* comp = Compilation::current();
2526   if (do_update) {
2527     // try to find exact type, using CHA if possible, so that loading
2528     // the klass from the object can be avoided
2529     ciType* type = obj-&gt;exact_type();
2530     if (type == NULL) {
2531       type = obj-&gt;declared_type();
2532       type = comp-&gt;cha_exact_type(type);
2533     }
2534     assert(type == NULL || type-&gt;is_klass(), &quot;type should be class&quot;);
2535     exact_klass = (type != NULL &amp;&amp; type-&gt;is_loaded()) ? (ciKlass*)type : NULL;
2536 
2537     do_update = exact_klass == NULL || ciTypeEntries::valid_ciklass(profiled_k) != exact_klass;
2538   }
2539 
2540   if (!do_null &amp;&amp; !do_update) {
2541     return result;
2542   }
2543 
2544   ciKlass* exact_signature_k = NULL;
<span class="line-modified">2545   if (do_update) {</span>
2546     // Is the type from the signature exact (the only one possible)?
2547     exact_signature_k = signature_at_call_k-&gt;exact_klass();
2548     if (exact_signature_k == NULL) {
2549       exact_signature_k = comp-&gt;cha_exact_type(signature_at_call_k);
2550     } else {
2551       result = exact_signature_k;
2552       // Known statically. No need to emit any code: prevent
2553       // LIR_Assembler::emit_profile_type() from emitting useless code
2554       profiled_k = ciTypeEntries::with_status(result, profiled_k);
2555     }
2556     // exact_klass and exact_signature_k can be both non NULL but
2557     // different if exact_klass is loaded after the ciObject for
2558     // exact_signature_k is created.
2559     if (exact_klass == NULL &amp;&amp; exact_signature_k != NULL &amp;&amp; exact_klass != exact_signature_k) {
2560       // sometimes the type of the signature is better than the best type
2561       // the compiler has
2562       exact_klass = exact_signature_k;
2563     }
2564     if (callee_signature_k != NULL &amp;&amp;
2565         callee_signature_k != signature_at_call_k) {
</pre>
<hr />
<pre>
2610         assert(!src-&gt;is_illegal(), &quot;check&quot;);
2611         BasicType t = src-&gt;type();
2612         if (is_reference_type(t)) {
2613           intptr_t profiled_k = parameters-&gt;type(j);
2614           Local* local = x-&gt;state()-&gt;local_at(java_index)-&gt;as_Local();
2615           ciKlass* exact = profile_type(md, md-&gt;byte_offset_of_slot(parameters_type_data, ParametersTypeData::type_offset(0)),
2616                                         in_bytes(ParametersTypeData::type_offset(j)) - in_bytes(ParametersTypeData::type_offset(0)),
2617                                         profiled_k, local, mdp, false, local-&gt;declared_type()-&gt;as_klass(), NULL);
2618           // If the profile is known statically set it once for all and do not emit any code
2619           if (exact != NULL) {
2620             md-&gt;set_parameter_type(j, exact);
2621           }
2622           j++;
2623         }
2624         java_index += type2size[t];
2625       }
2626     }
2627   }
2628 }
2629 















































2630 void LIRGenerator::do_Base(Base* x) {
2631   __ std_entry(LIR_OprFact::illegalOpr);
2632   // Emit moves from physical registers / stack slots to virtual registers
2633   CallingConvention* args = compilation()-&gt;frame_map()-&gt;incoming_arguments();
2634   IRScope* irScope = compilation()-&gt;hir()-&gt;top_scope();
2635   int java_index = 0;
2636   for (int i = 0; i &lt; args-&gt;length(); i++) {
2637     LIR_Opr src = args-&gt;at(i);
2638     assert(!src-&gt;is_illegal(), &quot;check&quot;);
2639     BasicType t = src-&gt;type();
2640 
2641     // Types which are smaller than int are passed as int, so
2642     // correct the type which passed.
2643     switch (t) {
2644     case T_BYTE:
2645     case T_BOOLEAN:
2646     case T_SHORT:
2647     case T_CHAR:
2648       t = T_INT;
2649       break;
</pre>
<hr />
<pre>
2694       LIR_Opr lock = syncLockOpr();
2695       __ load_stack_address_monitor(0, lock);
2696 
2697       CodeEmitInfo* info = new CodeEmitInfo(scope()-&gt;start()-&gt;state()-&gt;copy(ValueStack::StateBefore, SynchronizationEntryBCI), NULL, x-&gt;check_flag(Instruction::DeoptimizeOnException));
2698       CodeStub* slow_path = new MonitorEnterStub(obj, lock, info);
2699 
2700       // receiver is guaranteed non-NULL so don&#39;t need CodeEmitInfo
2701       __ lock_object(syncTempOpr(), obj, lock, new_register(T_OBJECT), slow_path, NULL);
2702     }
2703   }
2704   if (compilation()-&gt;age_code()) {
2705     CodeEmitInfo* info = new CodeEmitInfo(scope()-&gt;start()-&gt;state()-&gt;copy(ValueStack::StateBefore, 0), NULL, false);
2706     decrement_age(info);
2707   }
2708   // increment invocation counters if needed
2709   if (!method()-&gt;is_accessor()) { // Accessors do not have MDOs, so no counting.
2710     profile_parameters(x);
2711     CodeEmitInfo* info = new CodeEmitInfo(scope()-&gt;start()-&gt;state()-&gt;copy(ValueStack::StateBefore, SynchronizationEntryBCI), NULL, false);
2712     increment_invocation_counter(info);
2713   }








2714 
2715   // all blocks with a successor must end with an unconditional jump
2716   // to the successor even if they are consecutive
2717   __ jump(x-&gt;default_sux());
2718 }
2719 
2720 
2721 void LIRGenerator::do_OsrEntry(OsrEntry* x) {
2722   // construct our frame and model the production of incoming pointer
2723   // to the OSR buffer.
2724   __ osr_entry(LIR_Assembler::osrBufferPointer());
2725   LIR_Opr result = rlock_result(x);
2726   __ move(LIR_Assembler::osrBufferPointer(), result);
2727 }
2728 


















2729 
2730 void LIRGenerator::invoke_load_arguments(Invoke* x, LIRItemList* args, const LIR_OprList* arg_list) {
2731   assert(args-&gt;length() == arg_list-&gt;length(),
2732          &quot;args=%d, arg_list=%d&quot;, args-&gt;length(), arg_list-&gt;length());
2733   for (int i = x-&gt;has_receiver() ? 1 : 0; i &lt; args-&gt;length(); i++) {
2734     LIRItem* param = args-&gt;at(i);
2735     LIR_Opr loc = arg_list-&gt;at(i);
<span class="line-modified">2736     if (loc-&gt;is_register()) {</span>
<span class="line-removed">2737       param-&gt;load_item_force(loc);</span>
<span class="line-removed">2738     } else {</span>
<span class="line-removed">2739       LIR_Address* addr = loc-&gt;as_address_ptr();</span>
<span class="line-removed">2740       param-&gt;load_for_store(addr-&gt;type());</span>
<span class="line-removed">2741       if (addr-&gt;type() == T_OBJECT) {</span>
<span class="line-removed">2742         __ move_wide(param-&gt;result(), addr);</span>
<span class="line-removed">2743       } else</span>
<span class="line-removed">2744         if (addr-&gt;type() == T_LONG || addr-&gt;type() == T_DOUBLE) {</span>
<span class="line-removed">2745           __ unaligned_move(param-&gt;result(), addr);</span>
<span class="line-removed">2746         } else {</span>
<span class="line-removed">2747           __ move(param-&gt;result(), addr);</span>
<span class="line-removed">2748         }</span>
<span class="line-removed">2749     }</span>
2750   }
2751 
2752   if (x-&gt;has_receiver()) {
2753     LIRItem* receiver = args-&gt;at(0);
2754     LIR_Opr loc = arg_list-&gt;at(0);
2755     if (loc-&gt;is_register()) {
2756       receiver-&gt;load_item_force(loc);
2757     } else {
2758       assert(loc-&gt;is_address(), &quot;just checking&quot;);
2759       receiver-&gt;load_for_store(T_OBJECT);
2760       __ move_wide(receiver-&gt;result(), loc-&gt;as_address_ptr());
2761     }
2762   }
2763 }
2764 
2765 
2766 // Visits all arguments, returns appropriate items without loading them
2767 LIRItemList* LIRGenerator::invoke_visit_arguments(Invoke* x) {
2768   LIRItemList* argument_items = new LIRItemList();
2769   if (x-&gt;has_receiver()) {
</pre>
<hr />
<pre>
2910   __ move(tmp, reg);
2911 }
2912 
2913 
2914 
2915 // Code for  :  x-&gt;x() {x-&gt;cond()} x-&gt;y() ? x-&gt;tval() : x-&gt;fval()
2916 void LIRGenerator::do_IfOp(IfOp* x) {
2917 #ifdef ASSERT
2918   {
2919     ValueTag xtag = x-&gt;x()-&gt;type()-&gt;tag();
2920     ValueTag ttag = x-&gt;tval()-&gt;type()-&gt;tag();
2921     assert(xtag == intTag || xtag == objectTag, &quot;cannot handle others&quot;);
2922     assert(ttag == addressTag || ttag == intTag || ttag == objectTag || ttag == longTag, &quot;cannot handle others&quot;);
2923     assert(ttag == x-&gt;fval()-&gt;type()-&gt;tag(), &quot;cannot handle others&quot;);
2924   }
2925 #endif
2926 
2927   LIRItem left(x-&gt;x(), this);
2928   LIRItem right(x-&gt;y(), this);
2929   left.load_item();
<span class="line-modified">2930   if (can_inline_as_constant(right.value())) {</span>
2931     right.dont_load_item();
2932   } else {

2933     right.load_item();
2934   }
2935 
2936   LIRItem t_val(x-&gt;tval(), this);
2937   LIRItem f_val(x-&gt;fval(), this);
2938   t_val.dont_load_item();
2939   f_val.dont_load_item();
<span class="line-modified">2940   LIR_Opr reg = rlock_result(x);</span>
<span class="line-modified">2941 </span>
<span class="line-modified">2942   __ cmp(lir_cond(x-&gt;cond()), left.result(), right.result());</span>

























































2943   __ cmove(lir_cond(x-&gt;cond()), t_val.result(), f_val.result(), reg, as_BasicType(x-&gt;x()-&gt;type()));
2944 }
2945 
2946 #ifdef JFR_HAVE_INTRINSICS
2947 void LIRGenerator::do_ClassIDIntrinsic(Intrinsic* x) {
2948   CodeEmitInfo* info = state_for(x);
2949   CodeEmitInfo* info2 = new CodeEmitInfo(info); // Clone for the second null check
2950 
2951   assert(info != NULL, &quot;must have info&quot;);
2952   LIRItem arg(x-&gt;argument_at(0), this);
2953 
2954   arg.load_item();
2955   LIR_Opr klass = new_register(T_METADATA);
2956   __ move(new LIR_Address(arg.result(), java_lang_Class::klass_offset(), T_ADDRESS), klass, info);
2957   LIR_Opr id = new_register(T_LONG);
2958   ByteSize offset = KLASS_TRACE_ID_OFFSET;
2959   LIR_Address* trace_id_addr = new LIR_Address(klass, in_bytes(offset), T_LONG);
2960 
2961   __ move(trace_id_addr, id);
2962   __ logical_or(id, LIR_OprFact::longConst(0x01l), id);
</pre>
<hr />
<pre>
3235   if (x-&gt;recv() != NULL || x-&gt;nb_profiled_args() &gt; 0) {
3236     profile_parameters_at_call(x);
3237   }
3238 
3239   if (x-&gt;recv() != NULL) {
3240     LIRItem value(x-&gt;recv(), this);
3241     value.load_item();
3242     recv = new_register(T_OBJECT);
3243     __ move(value.result(), recv);
3244   }
3245   __ profile_call(x-&gt;method(), x-&gt;bci_of_invoke(), x-&gt;callee(), mdo, recv, tmp, x-&gt;known_holder());
3246 }
3247 
3248 void LIRGenerator::do_ProfileReturnType(ProfileReturnType* x) {
3249   int bci = x-&gt;bci_of_invoke();
3250   ciMethodData* md = x-&gt;method()-&gt;method_data_or_null();
3251   assert(md != NULL, &quot;Sanity&quot;);
3252   ciProfileData* data = md-&gt;bci_to_data(bci);
3253   if (data != NULL) {
3254     assert(data-&gt;is_CallTypeData() || data-&gt;is_VirtualCallTypeData(), &quot;wrong profile data type&quot;);
<span class="line-modified">3255     ciReturnTypeEntry* ret = data-&gt;is_CallTypeData() ? ((ciCallTypeData*)data)-&gt;ret() : ((ciVirtualCallTypeData*)data)-&gt;ret();</span>
3256     LIR_Opr mdp = LIR_OprFact::illegalOpr;
3257 
3258     bool ignored_will_link;
3259     ciSignature* signature_at_call = NULL;
3260     x-&gt;method()-&gt;get_method_at_bci(bci, ignored_will_link, &amp;signature_at_call);
3261 
3262     // The offset within the MDO of the entry to update may be too large
3263     // to be used in load/store instructions on some platforms. So have
3264     // profile_type() compute the address of the profile in a register.
3265     ciKlass* exact = profile_type(md, md-&gt;byte_offset_of_slot(data, ret-&gt;type_offset()), 0,
3266         ret-&gt;type(), x-&gt;ret(), mdp,
3267         !x-&gt;needs_null_check(),
3268         signature_at_call-&gt;return_type()-&gt;as_klass(),
3269         x-&gt;callee()-&gt;signature()-&gt;return_type()-&gt;as_klass());
3270     if (exact != NULL) {
3271       md-&gt;set_return_type(bci, exact);
3272     }
3273   }
3274 }
3275 
</pre>
</td>
<td>
<hr />
<pre>
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #include &quot;precompiled.hpp&quot;
  26 #include &quot;c1/c1_Compilation.hpp&quot;
  27 #include &quot;c1/c1_Defs.hpp&quot;
  28 #include &quot;c1/c1_FrameMap.hpp&quot;
  29 #include &quot;c1/c1_Instruction.hpp&quot;
  30 #include &quot;c1/c1_LIRAssembler.hpp&quot;
  31 #include &quot;c1/c1_LIRGenerator.hpp&quot;
  32 #include &quot;c1/c1_ValueStack.hpp&quot;
  33 #include &quot;ci/ciArrayKlass.hpp&quot;
  34 #include &quot;ci/ciInstance.hpp&quot;
  35 #include &quot;ci/ciObjArray.hpp&quot;
  36 #include &quot;ci/ciUtilities.hpp&quot;
<span class="line-added">  37 #include &quot;ci/ciValueArrayKlass.hpp&quot;</span>
<span class="line-added">  38 #include &quot;ci/ciValueKlass.hpp&quot;</span>
  39 #include &quot;gc/shared/barrierSet.hpp&quot;
  40 #include &quot;gc/shared/c1/barrierSetC1.hpp&quot;
  41 #include &quot;oops/klass.inline.hpp&quot;
  42 #include &quot;runtime/arguments.hpp&quot;
  43 #include &quot;runtime/sharedRuntime.hpp&quot;
  44 #include &quot;runtime/stubRoutines.hpp&quot;
  45 #include &quot;runtime/vm_version.hpp&quot;
  46 #include &quot;utilities/bitMap.inline.hpp&quot;
  47 #include &quot;utilities/macros.hpp&quot;
  48 #include &quot;utilities/powerOfTwo.hpp&quot;
  49 
  50 #ifdef ASSERT
  51 #define __ gen()-&gt;lir(__FILE__, __LINE__)-&gt;
  52 #else
  53 #define __ gen()-&gt;lir()-&gt;
  54 #endif
  55 
  56 #ifndef PATCHED_ADDR
  57 #define PATCHED_ADDR  (max_jint)
  58 #endif
</pre>
<hr />
<pre>
 196   ResolveNode* source = source_node(src);
 197   source-&gt;append(destination_node(dest));
 198 }
 199 
 200 
 201 //--------------------------------------------------------------
 202 // LIRItem
 203 
 204 void LIRItem::set_result(LIR_Opr opr) {
 205   assert(value()-&gt;operand()-&gt;is_illegal() || value()-&gt;operand()-&gt;is_constant(), &quot;operand should never change&quot;);
 206   value()-&gt;set_operand(opr);
 207 
 208   if (opr-&gt;is_virtual()) {
 209     _gen-&gt;_instruction_for_operand.at_put_grow(opr-&gt;vreg_number(), value(), NULL);
 210   }
 211 
 212   _result = opr;
 213 }
 214 
 215 void LIRItem::load_item() {
<span class="line-added"> 216   assert(!_gen-&gt;in_conditional_code(), &quot;LIRItem cannot be loaded in conditional code&quot;);</span>
<span class="line-added"> 217 </span>
 218   if (result()-&gt;is_illegal()) {
 219     // update the items result
 220     _result = value()-&gt;operand();
 221   }
 222   if (!result()-&gt;is_register()) {
 223     LIR_Opr reg = _gen-&gt;new_register(value()-&gt;type());
 224     __ move(result(), reg);
 225     if (result()-&gt;is_constant()) {
 226       _result = reg;
 227     } else {
 228       set_result(reg);
 229     }
 230   }
 231 }
 232 
 233 
 234 void LIRItem::load_for_store(BasicType type) {
 235   if (_gen-&gt;can_store_as_constant(value(), type)) {
 236     _result = value()-&gt;operand();
 237     if (!_result-&gt;is_constant()) {
</pre>
<hr />
<pre>
 625     assert(right_op != result_op, &quot;malformed&quot;);
 626     __ move(left_op, result_op);
 627     left_op = result_op;
 628   }
 629 
 630   switch(code) {
 631     case Bytecodes::_iand:
 632     case Bytecodes::_land:  __ logical_and(left_op, right_op, result_op); break;
 633 
 634     case Bytecodes::_ior:
 635     case Bytecodes::_lor:   __ logical_or(left_op, right_op, result_op);  break;
 636 
 637     case Bytecodes::_ixor:
 638     case Bytecodes::_lxor:  __ logical_xor(left_op, right_op, result_op); break;
 639 
 640     default: ShouldNotReachHere();
 641   }
 642 }
 643 
 644 
<span class="line-modified"> 645 void LIRGenerator::monitor_enter(LIR_Opr object, LIR_Opr lock, LIR_Opr hdr, LIR_Opr scratch, int monitor_no,</span>
<span class="line-added"> 646                                  CodeEmitInfo* info_for_exception, CodeEmitInfo* info, CodeStub* throw_imse_stub) {</span>
 647   if (!GenerateSynchronizationCode) return;
 648   // for slow path, use debug info for state after successful locking
<span class="line-modified"> 649   CodeStub* slow_path = new MonitorEnterStub(object, lock, info, throw_imse_stub, scratch);</span>
 650   __ load_stack_address_monitor(monitor_no, lock);
 651   // for handling NullPointerException, use debug info representing just the lock stack before this monitorenter
<span class="line-modified"> 652   __ lock_object(hdr, object, lock, scratch, slow_path, info_for_exception, throw_imse_stub);</span>
 653 }
 654 
 655 
 656 void LIRGenerator::monitor_exit(LIR_Opr object, LIR_Opr lock, LIR_Opr new_hdr, LIR_Opr scratch, int monitor_no) {
 657   if (!GenerateSynchronizationCode) return;
 658   // setup registers
 659   LIR_Opr hdr = lock;
 660   lock = new_hdr;
 661   CodeStub* slow_path = new MonitorExitStub(lock, UseFastLocking, monitor_no);
 662   __ load_stack_address_monitor(monitor_no, lock);
 663   __ unlock_object(hdr, object, lock, scratch, slow_path);
 664 }
 665 
 666 #ifndef PRODUCT
 667 void LIRGenerator::print_if_not_loaded(const NewInstance* new_instance) {
 668   if (PrintNotLoaded &amp;&amp; !new_instance-&gt;klass()-&gt;is_loaded()) {
 669     tty-&gt;print_cr(&quot;   ###class not loaded at new bci %d&quot;, new_instance-&gt;printable_bci());
 670   } else if (PrintNotLoaded &amp;&amp; (TieredCompilation &amp;&amp; new_instance-&gt;is_unresolved())) {
 671     tty-&gt;print_cr(&quot;   ###class not resolved at new bci %d&quot;, new_instance-&gt;printable_bci());
 672   }
</pre>
<hr />
<pre>
 774       if (src_type != NULL) {
 775         if (src_type-&gt;element_type()-&gt;is_subtype_of(dst_type-&gt;element_type())) {
 776           is_exact = true;
 777           expected_type = dst_type;
 778         }
 779       }
 780     }
 781     // at least pass along a good guess
 782     if (expected_type == NULL) expected_type = dst_exact_type;
 783     if (expected_type == NULL) expected_type = src_declared_type;
 784     if (expected_type == NULL) expected_type = dst_declared_type;
 785 
 786     src_objarray = (src_exact_type &amp;&amp; src_exact_type-&gt;is_obj_array_klass()) || (src_declared_type &amp;&amp; src_declared_type-&gt;is_obj_array_klass());
 787     dst_objarray = (dst_exact_type &amp;&amp; dst_exact_type-&gt;is_obj_array_klass()) || (dst_declared_type &amp;&amp; dst_declared_type-&gt;is_obj_array_klass());
 788   }
 789 
 790   // if a probable array type has been identified, figure out if any
 791   // of the required checks for a fast case can be elided.
 792   int flags = LIR_OpArrayCopy::all_flags;
 793 
<span class="line-added"> 794   if (!src-&gt;is_loaded_flattened_array() &amp;&amp; !dst-&gt;is_loaded_flattened_array()) {</span>
<span class="line-added"> 795     flags &amp;= ~LIR_OpArrayCopy::always_slow_path;</span>
<span class="line-added"> 796   }</span>
<span class="line-added"> 797   if (!src-&gt;maybe_flattened_array()) {</span>
<span class="line-added"> 798     flags &amp;= ~LIR_OpArrayCopy::src_valuetype_check;</span>
<span class="line-added"> 799   }</span>
<span class="line-added"> 800   if (!dst-&gt;maybe_flattened_array() &amp;&amp; !dst-&gt;maybe_null_free_array()) {</span>
<span class="line-added"> 801     flags &amp;= ~LIR_OpArrayCopy::dst_valuetype_check;</span>
<span class="line-added"> 802   }</span>
<span class="line-added"> 803 </span>
 804   if (!src_objarray)
 805     flags &amp;= ~LIR_OpArrayCopy::src_objarray;
 806   if (!dst_objarray)
 807     flags &amp;= ~LIR_OpArrayCopy::dst_objarray;
 808 
 809   if (!x-&gt;arg_needs_null_check(0))
 810     flags &amp;= ~LIR_OpArrayCopy::src_null_check;
 811   if (!x-&gt;arg_needs_null_check(2))
 812     flags &amp;= ~LIR_OpArrayCopy::dst_null_check;
 813 
 814 
 815   if (expected_type != NULL) {
 816     Value length_limit = NULL;
 817 
 818     IfOp* ifop = length-&gt;as_IfOp();
 819     if (ifop != NULL) {
 820       // look for expressions like min(v, a.length) which ends up as
 821       //   x &gt; y ? y : x  or  x &gt;= y ? y : x
 822       if ((ifop-&gt;cond() == If::gtr || ifop-&gt;cond() == If::geq) &amp;&amp;
 823           ifop-&gt;x() == ifop-&gt;fval() &amp;&amp;
</pre>
<hr />
<pre>
1426       case T_FLOAT:
1427         if (c-&gt;as_jint_bits() != other-&gt;as_jint_bits()) continue;
1428         break;
1429       case T_LONG:
1430       case T_DOUBLE:
1431         if (c-&gt;as_jint_hi_bits() != other-&gt;as_jint_hi_bits()) continue;
1432         if (c-&gt;as_jint_lo_bits() != other-&gt;as_jint_lo_bits()) continue;
1433         break;
1434       case T_OBJECT:
1435         if (c-&gt;as_jobject() != other-&gt;as_jobject()) continue;
1436         break;
1437       default:
1438         break;
1439       }
1440       return _reg_for_constants.at(i);
1441     }
1442   }
1443 
1444   LIR_Opr result = new_register(t);
1445   __ move((LIR_Opr)c, result);
<span class="line-modified">1446   if (!in_conditional_code()) {</span>
<span class="line-modified">1447     _constants.append(c);</span>
<span class="line-added">1448     _reg_for_constants.append(result);</span>
<span class="line-added">1449   }</span>
1450   return result;
1451 }
1452 
<span class="line-added">1453 void LIRGenerator::set_in_conditional_code(bool v) {</span>
<span class="line-added">1454   assert(v != _in_conditional_code, &quot;must change state&quot;);</span>
<span class="line-added">1455   _in_conditional_code = v;</span>
<span class="line-added">1456 }</span>
<span class="line-added">1457 </span>
<span class="line-added">1458 </span>
1459 //------------------------field access--------------------------------------
1460 
1461 void LIRGenerator::do_CompareAndSwap(Intrinsic* x, ValueType* type) {
1462   assert(x-&gt;number_of_arguments() == 4, &quot;wrong type&quot;);
1463   LIRItem obj   (x-&gt;argument_at(0), this);  // object
1464   LIRItem offset(x-&gt;argument_at(1), this);  // offset of field
1465   LIRItem cmp   (x-&gt;argument_at(2), this);  // value to compare with field
1466   LIRItem val   (x-&gt;argument_at(3), this);  // replace field with val if matches cmp
1467   assert(obj.type()-&gt;tag() == objectTag, &quot;invalid type&quot;);
1468 
1469   // In 64bit the type can be long, sparc doesn&#39;t have this assert
1470   // assert(offset.type()-&gt;tag() == intTag, &quot;invalid type&quot;);
1471 
1472   assert(cmp.type()-&gt;tag() == type-&gt;tag(), &quot;invalid type&quot;);
1473   assert(val.type()-&gt;tag() == type-&gt;tag(), &quot;invalid type&quot;);
1474 
1475   LIR_Opr result = access_atomic_cmpxchg_at(IN_HEAP, as_BasicType(type),
1476                                             obj, offset, cmp, val);
1477   set_result(x, result);
1478 }
</pre>
<hr />
<pre>
1537       value.load_byte_item();
1538     } else  {
1539       value.load_item();
1540     }
1541   } else {
1542     value.load_for_store(field_type);
1543   }
1544 
1545   set_no_result(x);
1546 
1547 #ifndef PRODUCT
1548   if (PrintNotLoaded &amp;&amp; needs_patching) {
1549     tty-&gt;print_cr(&quot;   ###class not loaded at store_%s bci %d&quot;,
1550                   x-&gt;is_static() ?  &quot;static&quot; : &quot;field&quot;, x-&gt;printable_bci());
1551   }
1552 #endif
1553 
1554   if (x-&gt;needs_null_check() &amp;&amp;
1555       (needs_patching ||
1556        MacroAssembler::needs_explicit_null_check(x-&gt;offset()))) {
<span class="line-modified">1557     if (needs_patching &amp;&amp; x-&gt;field()-&gt;is_flattenable()) {</span>
<span class="line-modified">1558       // We are storing a field of type &quot;QT;&quot; into holder class H, but H is not yet</span>
<span class="line-modified">1559       // loaded. (If H had been loaded, then T must also have already been loaded</span>
<span class="line-modified">1560       // due to the &quot;Q&quot; signature, and needs_patching would be false).</span>
<span class="line-added">1561       assert(!x-&gt;field()-&gt;holder()-&gt;is_loaded(), &quot;must be&quot;);</span>
<span class="line-added">1562       // We don&#39;t know the offset of this field. Let&#39;s deopt and recompile.</span>
<span class="line-added">1563       CodeStub* stub = new DeoptimizeStub(new CodeEmitInfo(info),</span>
<span class="line-added">1564                                           Deoptimization::Reason_unloaded,</span>
<span class="line-added">1565                                           Deoptimization::Action_make_not_entrant);</span>
<span class="line-added">1566       __ branch(lir_cond_always, stub);</span>
<span class="line-added">1567     } else {</span>
<span class="line-added">1568       // Emit an explicit null check because the offset is too large.</span>
<span class="line-added">1569       // If the class is not loaded and the object is NULL, we need to deoptimize to throw a</span>
<span class="line-added">1570       // NoClassDefFoundError in the interpreter instead of an implicit NPE from compiled code.</span>
<span class="line-added">1571       __ null_check(object.result(), new CodeEmitInfo(info), /* deoptimize */ needs_patching);</span>
<span class="line-added">1572     }</span>
1573   }
1574 
1575   DecoratorSet decorators = IN_HEAP;
1576   if (is_volatile) {
1577     decorators |= MO_SEQ_CST;
1578   }
1579   if (needs_patching) {
1580     decorators |= C1_NEEDS_PATCHING;
1581   }
1582 
1583   access_store_at(decorators, field_type, object, LIR_OprFact::intConst(x-&gt;offset()),
1584                   value.result(), info != NULL ? new CodeEmitInfo(info) : NULL, info);
1585 }
1586 
<span class="line-added">1587 // FIXME -- I can&#39;t find any other way to pass an address to access_load_at().</span>
<span class="line-added">1588 class TempResolvedAddress: public Instruction {</span>
<span class="line-added">1589  public:</span>
<span class="line-added">1590   TempResolvedAddress(ValueType* type, LIR_Opr addr) : Instruction(type) {</span>
<span class="line-added">1591     set_operand(addr);</span>
<span class="line-added">1592   }</span>
<span class="line-added">1593   virtual void input_values_do(ValueVisitor*) {}</span>
<span class="line-added">1594   virtual void visit(InstructionVisitor* v)   {}</span>
<span class="line-added">1595   virtual const char* name() const  { return &quot;TempResolvedAddress&quot;; }</span>
<span class="line-added">1596 };</span>
<span class="line-added">1597 </span>
<span class="line-added">1598 void LIRGenerator::access_flattened_array(bool is_load, LIRItem&amp; array, LIRItem&amp; index, LIRItem&amp; obj_item) {</span>
<span class="line-added">1599   // Find the starting address of the source (inside the array)</span>
<span class="line-added">1600   ciType* array_type = array.value()-&gt;declared_type();</span>
<span class="line-added">1601   ciValueArrayKlass* value_array_klass = array_type-&gt;as_value_array_klass();</span>
<span class="line-added">1602   assert(value_array_klass-&gt;is_loaded(), &quot;must be&quot;);</span>
<span class="line-added">1603 </span>
<span class="line-added">1604   ciValueKlass* elem_klass = value_array_klass-&gt;element_klass()-&gt;as_value_klass();</span>
<span class="line-added">1605   int array_header_size = value_array_klass-&gt;array_header_in_bytes();</span>
<span class="line-added">1606   int shift = value_array_klass-&gt;log2_element_size();</span>
<span class="line-added">1607 </span>
<span class="line-added">1608 #ifndef _LP64</span>
<span class="line-added">1609   LIR_Opr index_op = new_register(T_INT);</span>
<span class="line-added">1610   // FIXME -- on 32-bit, the shift below can overflow, so we need to check that</span>
<span class="line-added">1611   // the top (shift+1) bits of index_op must be zero, or</span>
<span class="line-added">1612   // else throw ArrayIndexOutOfBoundsException</span>
<span class="line-added">1613   if (index.result()-&gt;is_constant()) {</span>
<span class="line-added">1614     jint const_index = index.result()-&gt;as_jint();</span>
<span class="line-added">1615     __ move(LIR_OprFact::intConst(const_index &lt;&lt; shift), index_op);</span>
<span class="line-added">1616   } else {</span>
<span class="line-added">1617     __ shift_left(index_op, shift, index.result());</span>
<span class="line-added">1618   }</span>
<span class="line-added">1619 #else</span>
<span class="line-added">1620   LIR_Opr index_op = new_register(T_LONG);</span>
<span class="line-added">1621   if (index.result()-&gt;is_constant()) {</span>
<span class="line-added">1622     jint const_index = index.result()-&gt;as_jint();</span>
<span class="line-added">1623     __ move(LIR_OprFact::longConst(const_index &lt;&lt; shift), index_op);</span>
<span class="line-added">1624   } else {</span>
<span class="line-added">1625     __ convert(Bytecodes::_i2l, index.result(), index_op);</span>
<span class="line-added">1626     // Need to shift manually, as LIR_Address can scale only up to 3.</span>
<span class="line-added">1627     __ shift_left(index_op, shift, index_op);</span>
<span class="line-added">1628   }</span>
<span class="line-added">1629 #endif</span>
<span class="line-added">1630 </span>
<span class="line-added">1631   LIR_Opr elm_op = new_pointer_register();</span>
<span class="line-added">1632   LIR_Address* elm_address = new LIR_Address(array.result(), index_op, array_header_size, T_ADDRESS);</span>
<span class="line-added">1633   __ leal(LIR_OprFact::address(elm_address), elm_op);</span>
<span class="line-added">1634 </span>
<span class="line-added">1635   for (int i = 0; i &lt; elem_klass-&gt;nof_nonstatic_fields(); i++) {</span>
<span class="line-added">1636     ciField* inner_field = elem_klass-&gt;nonstatic_field_at(i);</span>
<span class="line-added">1637     assert(!inner_field-&gt;is_flattened(), &quot;flattened fields must have been expanded&quot;);</span>
<span class="line-added">1638     int obj_offset = inner_field-&gt;offset();</span>
<span class="line-added">1639     int elm_offset = obj_offset - elem_klass-&gt;first_field_offset(); // object header is not stored in array.</span>
<span class="line-added">1640 </span>
<span class="line-added">1641     BasicType field_type = inner_field-&gt;type()-&gt;basic_type();</span>
<span class="line-added">1642     switch (field_type) {</span>
<span class="line-added">1643     case T_BYTE:</span>
<span class="line-added">1644     case T_BOOLEAN:</span>
<span class="line-added">1645     case T_SHORT:</span>
<span class="line-added">1646     case T_CHAR:</span>
<span class="line-added">1647      field_type = T_INT;</span>
<span class="line-added">1648       break;</span>
<span class="line-added">1649     default:</span>
<span class="line-added">1650       break;</span>
<span class="line-added">1651     }</span>
<span class="line-added">1652 </span>
<span class="line-added">1653     LIR_Opr temp = new_register(field_type);</span>
<span class="line-added">1654     TempResolvedAddress* elm_resolved_addr = new TempResolvedAddress(as_ValueType(field_type), elm_op);</span>
<span class="line-added">1655     LIRItem elm_item(elm_resolved_addr, this);</span>
<span class="line-added">1656 </span>
<span class="line-added">1657     DecoratorSet decorators = IN_HEAP;</span>
<span class="line-added">1658     if (is_load) {</span>
<span class="line-added">1659       access_load_at(decorators, field_type,</span>
<span class="line-added">1660                      elm_item, LIR_OprFact::intConst(elm_offset), temp,</span>
<span class="line-added">1661                      NULL, NULL);</span>
<span class="line-added">1662       access_store_at(decorators, field_type,</span>
<span class="line-added">1663                       obj_item, LIR_OprFact::intConst(obj_offset), temp,</span>
<span class="line-added">1664                       NULL, NULL);</span>
<span class="line-added">1665     } else {</span>
<span class="line-added">1666       access_load_at(decorators, field_type,</span>
<span class="line-added">1667                      obj_item, LIR_OprFact::intConst(obj_offset), temp,</span>
<span class="line-added">1668                      NULL, NULL);</span>
<span class="line-added">1669       access_store_at(decorators, field_type,</span>
<span class="line-added">1670                       elm_item, LIR_OprFact::intConst(elm_offset), temp,</span>
<span class="line-added">1671                       NULL, NULL);</span>
<span class="line-added">1672     }</span>
<span class="line-added">1673   }</span>
<span class="line-added">1674 }</span>
<span class="line-added">1675 </span>
<span class="line-added">1676 void LIRGenerator::check_flattened_array(LIR_Opr array, LIR_Opr value, CodeStub* slow_path) {</span>
<span class="line-added">1677   LIR_Opr tmp = new_register(T_METADATA);</span>
<span class="line-added">1678   __ check_flattened_array(array, value, tmp, slow_path);</span>
<span class="line-added">1679 }</span>
<span class="line-added">1680 </span>
<span class="line-added">1681 void LIRGenerator::check_null_free_array(LIRItem&amp; array, LIRItem&amp; value, CodeEmitInfo* info) {</span>
<span class="line-added">1682   LabelObj* L_end = new LabelObj();</span>
<span class="line-added">1683   LIR_Opr tmp = new_register(T_METADATA);</span>
<span class="line-added">1684   __ check_null_free_array(array.result(), tmp);</span>
<span class="line-added">1685   __ branch(lir_cond_equal, L_end-&gt;label());</span>
<span class="line-added">1686   __ null_check(value.result(), info);</span>
<span class="line-added">1687   __ branch_destination(L_end-&gt;label());</span>
<span class="line-added">1688 }</span>
<span class="line-added">1689 </span>
<span class="line-added">1690 bool LIRGenerator::needs_flattened_array_store_check(StoreIndexed* x) {</span>
<span class="line-added">1691   if (x-&gt;elt_type() == T_OBJECT &amp;&amp; x-&gt;array()-&gt;maybe_flattened_array()) {</span>
<span class="line-added">1692     ciType* type = x-&gt;value()-&gt;declared_type();</span>
<span class="line-added">1693     if (type != NULL &amp;&amp; type-&gt;is_klass()) {</span>
<span class="line-added">1694       ciKlass* klass = type-&gt;as_klass();</span>
<span class="line-added">1695       if (!klass-&gt;can_be_value_klass() || (klass-&gt;is_valuetype() &amp;&amp; !klass-&gt;as_value_klass()-&gt;flatten_array())) {</span>
<span class="line-added">1696         // This is known to be a non-flattenable object. If the array is flattened,</span>
<span class="line-added">1697         // it will be caught by the code generated by array_store_check().</span>
<span class="line-added">1698         return false;</span>
<span class="line-added">1699       }</span>
<span class="line-added">1700     }</span>
<span class="line-added">1701     // We&#39;re not 100% sure, so let&#39;s do the flattened_array_store_check.</span>
<span class="line-added">1702     return true;</span>
<span class="line-added">1703   }</span>
<span class="line-added">1704   return false;</span>
<span class="line-added">1705 }</span>
<span class="line-added">1706 </span>
<span class="line-added">1707 bool LIRGenerator::needs_null_free_array_store_check(StoreIndexed* x) {</span>
<span class="line-added">1708   return x-&gt;elt_type() == T_OBJECT &amp;&amp; x-&gt;array()-&gt;maybe_null_free_array();</span>
<span class="line-added">1709 }</span>
<span class="line-added">1710 </span>
1711 void LIRGenerator::do_StoreIndexed(StoreIndexed* x) {
1712   assert(x-&gt;is_pinned(),&quot;&quot;);
<span class="line-added">1713   assert(x-&gt;elt_type() != T_ARRAY, &quot;never used&quot;);</span>
<span class="line-added">1714   bool is_loaded_flattened_array = x-&gt;array()-&gt;is_loaded_flattened_array();</span>
1715   bool needs_range_check = x-&gt;compute_needs_range_check();
1716   bool use_length = x-&gt;length() != NULL;
1717   bool obj_store = is_reference_type(x-&gt;elt_type());
<span class="line-modified">1718   bool needs_store_check = obj_store &amp;&amp; !(is_loaded_flattened_array &amp;&amp; x-&gt;is_exact_flattened_array_store()) &amp;&amp;</span>
<span class="line-modified">1719                                         (x-&gt;value()-&gt;as_Constant() == NULL ||</span>
<span class="line-modified">1720                                          !get_jobject_constant(x-&gt;value())-&gt;is_null_object());</span>
1721 
1722   LIRItem array(x-&gt;array(), this);
1723   LIRItem index(x-&gt;index(), this);
1724   LIRItem value(x-&gt;value(), this);
1725   LIRItem length(this);
1726 
1727   array.load_item();
1728   index.load_nonconstant();
1729 
1730   if (use_length &amp;&amp; needs_range_check) {
1731     length.set_instruction(x-&gt;length());
1732     length.load_item();
<span class="line-modified">1733   }</span>
<span class="line-modified">1734 </span>
<span class="line-added">1735   if (needs_store_check || x-&gt;check_boolean()</span>
1736       || is_loaded_flattened_array || needs_flattened_array_store_check(x) || needs_null_free_array_store_check(x)) {
1737     value.load_item();
1738   } else {
1739     value.load_for_store(x-&gt;elt_type());
1740   }
1741 
1742   set_no_result(x);
1743 
1744   // the CodeEmitInfo must be duplicated for each different
1745   // LIR-instruction because spilling can occur anywhere between two
1746   // instructions and so the debug information must be different
1747   CodeEmitInfo* range_check_info = state_for(x);
1748   CodeEmitInfo* null_check_info = NULL;
1749   if (x-&gt;needs_null_check()) {
1750     null_check_info = new CodeEmitInfo(range_check_info);
1751   }
1752 
1753   if (GenerateRangeChecks &amp;&amp; needs_range_check) {
1754     if (use_length) {
1755       __ cmp(lir_cond_belowEqual, length.result(), index.result());
1756       __ branch(lir_cond_belowEqual, new RangeCheckStub(range_check_info, index.result(), array.result()));
1757     } else {
1758       array_range_check(array.result(), index.result(), null_check_info, range_check_info);
1759       // range_check also does the null check
1760       null_check_info = NULL;
1761     }
1762   }
1763 
<span class="line-added">1764   if (x-&gt;should_profile()) {</span>
<span class="line-added">1765     ciMethodData* md = NULL;</span>
<span class="line-added">1766     ciArrayLoadStoreData* load_store = NULL;</span>
<span class="line-added">1767     profile_array_type(x, md, load_store);</span>
<span class="line-added">1768     if (is_loaded_flattened_array) {</span>
<span class="line-added">1769       int flag = ArrayLoadStoreData::flat_array_byte_constant() | ArrayLoadStoreData::null_free_array_byte_constant();</span>
<span class="line-added">1770       assert(md != NULL, &quot;should have been initialized&quot;);</span>
<span class="line-added">1771       profile_array_load_store_flags(md, load_store, flag);</span>
<span class="line-added">1772     } else if (x-&gt;array()-&gt;maybe_null_free_array()) {</span>
<span class="line-added">1773       profile_null_free_array(array, md, load_store);</span>
<span class="line-added">1774     }</span>
<span class="line-added">1775     profile_element_type(x-&gt;value(), md, load_store);</span>
<span class="line-added">1776   }</span>
<span class="line-added">1777 </span>
1778   if (GenerateArrayStoreCheck &amp;&amp; needs_store_check) {
1779     CodeEmitInfo* store_check_info = new CodeEmitInfo(range_check_info);
<span class="line-modified">1780     array_store_check(value.result(), array.result(), store_check_info, NULL, -1);</span>
1781   }
1782 
<span class="line-modified">1783   if (is_loaded_flattened_array) {</span>
<span class="line-modified">1784     if (!x-&gt;value()-&gt;is_never_null()) {</span>
<span class="line-modified">1785       __ null_check(value.result(), new CodeEmitInfo(range_check_info));</span>
<span class="line-modified">1786     }</span>
<span class="line-added">1787     access_flattened_array(false, array, index, value);</span>
<span class="line-added">1788   } else {</span>
<span class="line-added">1789     StoreFlattenedArrayStub* slow_path = NULL;</span>
<span class="line-added">1790 </span>
<span class="line-added">1791     if (needs_flattened_array_store_check(x)) {</span>
<span class="line-added">1792       // Check if we indeed have a flattened array</span>
<span class="line-added">1793       index.load_item();</span>
<span class="line-added">1794       slow_path = new StoreFlattenedArrayStub(array.result(), index.result(), value.result(), state_for(x, x-&gt;state_before()));</span>
<span class="line-added">1795       check_flattened_array(array.result(), value.result(), slow_path);</span>
<span class="line-added">1796       set_in_conditional_code(true);</span>
<span class="line-added">1797     } else if (needs_null_free_array_store_check(x)) {</span>
<span class="line-added">1798       CodeEmitInfo* info = new CodeEmitInfo(range_check_info);</span>
<span class="line-added">1799       check_null_free_array(array, value, info);</span>
<span class="line-added">1800     }</span>
1801 
<span class="line-modified">1802     DecoratorSet decorators = IN_HEAP | IS_ARRAY;</span>
<span class="line-modified">1803     if (x-&gt;check_boolean()) {</span>
<span class="line-added">1804       decorators |= C1_MASK_BOOLEAN;</span>
<span class="line-added">1805     }</span>
<span class="line-added">1806 </span>
<span class="line-added">1807     access_store_at(decorators, x-&gt;elt_type(), array, index.result(), value.result(),</span>
<span class="line-added">1808                     NULL, null_check_info);</span>
<span class="line-added">1809     if (slow_path != NULL) {</span>
<span class="line-added">1810       __ branch_destination(slow_path-&gt;continuation());</span>
<span class="line-added">1811       set_in_conditional_code(false);</span>
<span class="line-added">1812     }</span>
<span class="line-added">1813   }</span>
1814 }
1815 
1816 void LIRGenerator::access_load_at(DecoratorSet decorators, BasicType type,
1817                                   LIRItem&amp; base, LIR_Opr offset, LIR_Opr result,
1818                                   CodeEmitInfo* patch_info, CodeEmitInfo* load_emit_info) {
1819   decorators |= ACCESS_READ;
1820   LIRAccess access(this, decorators, base, offset, type, patch_info, load_emit_info);
1821   if (access.is_raw()) {
1822     _barrier_set-&gt;BarrierSetC1::load_at(access, result);
1823   } else {
1824     _barrier_set-&gt;load_at(access, result);
1825   }
1826 }
1827 
1828 void LIRGenerator::access_load(DecoratorSet decorators, BasicType type,
1829                                LIR_Opr addr, LIR_Opr result) {
1830   decorators |= ACCESS_READ;
1831   LIRAccess access(this, decorators, LIR_OprFact::illegalOpr, LIR_OprFact::illegalOpr, type);
1832   access.set_resolved_addr(addr);
1833   if (access.is_raw()) {
</pre>
<hr />
<pre>
1883   decorators |= ACCESS_WRITE;
1884   // Atomic operations are SEQ_CST by default
1885   decorators |= ((decorators &amp; MO_DECORATOR_MASK) == 0) ? MO_SEQ_CST : 0;
1886   LIRAccess access(this, decorators, base, offset, type);
1887   if (access.is_raw()) {
1888     return _barrier_set-&gt;BarrierSetC1::atomic_add_at(access, value);
1889   } else {
1890     return _barrier_set-&gt;atomic_add_at(access, value);
1891   }
1892 }
1893 
1894 LIR_Opr LIRGenerator::access_resolve(DecoratorSet decorators, LIR_Opr obj) {
1895   // Use stronger ACCESS_WRITE|ACCESS_READ by default.
1896   if ((decorators &amp; (ACCESS_READ | ACCESS_WRITE)) == 0) {
1897     decorators |= ACCESS_READ | ACCESS_WRITE;
1898   }
1899 
1900   return _barrier_set-&gt;resolve(this, decorators, obj);
1901 }
1902 
<span class="line-added">1903 Constant* LIRGenerator::flattenable_load_field_prolog(LoadField* x, CodeEmitInfo* info) {</span>
<span class="line-added">1904   ciField* field = x-&gt;field();</span>
<span class="line-added">1905   ciInstanceKlass* holder = field-&gt;holder();</span>
<span class="line-added">1906   Constant* default_value = NULL;</span>
<span class="line-added">1907 </span>
<span class="line-added">1908   // Unloaded &quot;QV;&quot; klasses are represented by a ciInstanceKlass</span>
<span class="line-added">1909   bool field_type_unloaded = field-&gt;type()-&gt;is_instance_klass() &amp;&amp; !field-&gt;type()-&gt;as_instance_klass()-&gt;is_loaded();</span>
<span class="line-added">1910 </span>
<span class="line-added">1911   // Check for edge cases (1), (2) and (3) for getstatic and getfield</span>
<span class="line-added">1912   bool deopt = false;</span>
<span class="line-added">1913   bool need_default = false;</span>
<span class="line-added">1914   if (field-&gt;is_static()) {</span>
<span class="line-added">1915       // (1) holder is unloaded -- no problem: it will be loaded by patching, and field offset will be determined.</span>
<span class="line-added">1916       // No check needed here.</span>
<span class="line-added">1917 </span>
<span class="line-added">1918     if (field_type_unloaded) {</span>
<span class="line-added">1919       // (2) field type is unloaded -- problem: we don&#39;t know what the default value is. Let&#39;s deopt.</span>
<span class="line-added">1920       //                               FIXME: consider getting the default value in patching code.</span>
<span class="line-added">1921       deopt = true;</span>
<span class="line-added">1922     } else {</span>
<span class="line-added">1923       need_default = true;</span>
<span class="line-added">1924     }</span>
<span class="line-added">1925 </span>
<span class="line-added">1926       // (3) field is not flattened -- we don&#39;t care: static fields are never flattened.</span>
<span class="line-added">1927       // No check needed here.</span>
<span class="line-added">1928   } else {</span>
<span class="line-added">1929     if (!holder-&gt;is_loaded()) {</span>
<span class="line-added">1930       // (1) holder is unloaded -- problem: we needed the field offset back in GraphBuilder::access_field()</span>
<span class="line-added">1931       //                           FIXME: consider getting field offset in patching code (but only if the field</span>
<span class="line-added">1932       //                           type was loaded at compilation time).</span>
<span class="line-added">1933       deopt = true;</span>
<span class="line-added">1934     } else if (field_type_unloaded) {</span>
<span class="line-added">1935       // (2) field type is unloaded -- problem: we don&#39;t know whether it&#39;s flattened or not. Let&#39;s deopt</span>
<span class="line-added">1936       deopt = true;</span>
<span class="line-added">1937     } else if (!field-&gt;is_flattened()) {</span>
<span class="line-added">1938       // (3) field is not flattened -- need default value in cases of uninitialized field</span>
<span class="line-added">1939       need_default = true;</span>
<span class="line-added">1940     }</span>
<span class="line-added">1941   }</span>
<span class="line-added">1942 </span>
<span class="line-added">1943   if (deopt) {</span>
<span class="line-added">1944     assert(!need_default, &quot;deopt and need_default cannot both be true&quot;);</span>
<span class="line-added">1945     assert(x-&gt;needs_patching(), &quot;must be&quot;);</span>
<span class="line-added">1946     assert(info != NULL, &quot;must be&quot;);</span>
<span class="line-added">1947     CodeStub* stub = new DeoptimizeStub(new CodeEmitInfo(info),</span>
<span class="line-added">1948                                         Deoptimization::Reason_unloaded,</span>
<span class="line-added">1949                                         Deoptimization::Action_make_not_entrant);</span>
<span class="line-added">1950     __ branch(lir_cond_always, stub);</span>
<span class="line-added">1951   } else if (need_default) {</span>
<span class="line-added">1952     assert(!field_type_unloaded, &quot;must be&quot;);</span>
<span class="line-added">1953     assert(field-&gt;type()-&gt;is_valuetype(), &quot;must be&quot;);</span>
<span class="line-added">1954     ciValueKlass* value_klass = field-&gt;type()-&gt;as_value_klass();</span>
<span class="line-added">1955     assert(value_klass-&gt;is_loaded(), &quot;must be&quot;);</span>
<span class="line-added">1956 </span>
<span class="line-added">1957     if (field-&gt;is_static() &amp;&amp; holder-&gt;is_loaded()) {</span>
<span class="line-added">1958       ciInstance* mirror = field-&gt;holder()-&gt;java_mirror();</span>
<span class="line-added">1959       ciObject* val = mirror-&gt;field_value(field).as_object();</span>
<span class="line-added">1960       if (val-&gt;is_null_object()) {</span>
<span class="line-added">1961         // This is a non-nullable static field, but it&#39;s not initialized.</span>
<span class="line-added">1962         // We need to do a null check, and replace it with the default value.</span>
<span class="line-added">1963       } else {</span>
<span class="line-added">1964         // No need to perform null check on this static field</span>
<span class="line-added">1965         need_default = false;</span>
<span class="line-added">1966       }</span>
<span class="line-added">1967     }</span>
<span class="line-added">1968 </span>
<span class="line-added">1969     if (need_default) {</span>
<span class="line-added">1970       default_value = new Constant(new InstanceConstant(value_klass-&gt;default_value_instance()));</span>
<span class="line-added">1971     }</span>
<span class="line-added">1972   }</span>
<span class="line-added">1973 </span>
<span class="line-added">1974   return default_value;</span>
<span class="line-added">1975 }</span>
<span class="line-added">1976 </span>
1977 void LIRGenerator::do_LoadField(LoadField* x) {
1978   bool needs_patching = x-&gt;needs_patching();
1979   bool is_volatile = x-&gt;field()-&gt;is_volatile();
1980   BasicType field_type = x-&gt;field_type();
1981 
1982   CodeEmitInfo* info = NULL;
1983   if (needs_patching) {
1984     assert(x-&gt;explicit_null_check() == NULL, &quot;can&#39;t fold null check into patching field access&quot;);
1985     info = state_for(x, x-&gt;state_before());
1986   } else if (x-&gt;needs_null_check()) {
1987     NullCheck* nc = x-&gt;explicit_null_check();
1988     if (nc == NULL) {
1989       info = state_for(x);
1990     } else {
1991       info = state_for(nc);
1992     }
1993   }
1994 
1995   LIRItem object(x-&gt;obj(), this);
1996 
1997   object.load_item();
1998 
1999 #ifndef PRODUCT
2000   if (PrintNotLoaded &amp;&amp; needs_patching) {
2001     tty-&gt;print_cr(&quot;   ###class not loaded at load_%s bci %d&quot;,
2002                   x-&gt;is_static() ?  &quot;static&quot; : &quot;field&quot;, x-&gt;printable_bci());
2003   }
2004 #endif
2005 
<span class="line-added">2006   Constant* default_value = NULL;</span>
<span class="line-added">2007   if (x-&gt;field()-&gt;is_flattenable()) {</span>
<span class="line-added">2008     default_value = flattenable_load_field_prolog(x, info);</span>
<span class="line-added">2009   }</span>
<span class="line-added">2010 </span>
2011   bool stress_deopt = StressLoopInvariantCodeMotion &amp;&amp; info &amp;&amp; info-&gt;deoptimize_on_exception();
2012   if (x-&gt;needs_null_check() &amp;&amp;
2013       (needs_patching ||
2014        MacroAssembler::needs_explicit_null_check(x-&gt;offset()) ||
2015        stress_deopt)) {
2016     LIR_Opr obj = object.result();
2017     if (stress_deopt) {
2018       obj = new_register(T_OBJECT);
2019       __ move(LIR_OprFact::oopConst(NULL), obj);
2020     }
2021     // Emit an explicit null check because the offset is too large.
2022     // If the class is not loaded and the object is NULL, we need to deoptimize to throw a
2023     // NoClassDefFoundError in the interpreter instead of an implicit NPE from compiled code.
2024     __ null_check(obj, new CodeEmitInfo(info), /* deoptimize */ needs_patching);
2025   }
2026 
2027   DecoratorSet decorators = IN_HEAP;
2028   if (is_volatile) {
2029     decorators |= MO_SEQ_CST;
2030   }
2031   if (needs_patching) {
2032     decorators |= C1_NEEDS_PATCHING;
2033   }
2034 
2035   LIR_Opr result = rlock_result(x, field_type);
2036   access_load_at(decorators, field_type,
2037                  object, LIR_OprFact::intConst(x-&gt;offset()), result,
2038                  info ? new CodeEmitInfo(info) : NULL, info);
<span class="line-added">2039 </span>
<span class="line-added">2040   if (default_value != NULL) {</span>
<span class="line-added">2041     LabelObj* L_end = new LabelObj();</span>
<span class="line-added">2042     __ cmp(lir_cond_notEqual, result, LIR_OprFact::oopConst(NULL));</span>
<span class="line-added">2043     __ branch(lir_cond_notEqual, L_end-&gt;label());</span>
<span class="line-added">2044     set_in_conditional_code(true);</span>
<span class="line-added">2045     __ move(load_constant(default_value), result);</span>
<span class="line-added">2046     __ branch_destination(L_end-&gt;label());</span>
<span class="line-added">2047     set_in_conditional_code(false);</span>
<span class="line-added">2048   }</span>
2049 }
2050 
2051 
2052 //------------------------java.nio.Buffer.checkIndex------------------------
2053 
2054 // int java.nio.Buffer.checkIndex(int)
2055 void LIRGenerator::do_NIOCheckIndex(Intrinsic* x) {
2056   // NOTE: by the time we are in checkIndex() we are guaranteed that
2057   // the buffer is non-null (because checkIndex is package-private and
2058   // only called from within other methods in the buffer).
2059   assert(x-&gt;number_of_arguments() == 2, &quot;wrong type&quot;);
2060   LIRItem buf  (x-&gt;argument_at(0), this);
2061   LIRItem index(x-&gt;argument_at(1), this);
2062   buf.load_item();
2063   index.load_item();
2064 
2065   LIR_Opr result = rlock_result(x);
2066   if (GenerateRangeChecks) {
2067     CodeEmitInfo* info = state_for(x);
2068     CodeStub* stub = new RangeCheckStub(info, index.result());
</pre>
<hr />
<pre>
2143       __ move(LIR_OprFact::oopConst(NULL), obj);
2144       __ null_check(obj, new CodeEmitInfo(null_check_info));
2145     }
2146   }
2147 
2148   if (GenerateRangeChecks &amp;&amp; needs_range_check) {
2149     if (StressLoopInvariantCodeMotion &amp;&amp; range_check_info-&gt;deoptimize_on_exception()) {
2150       __ branch(lir_cond_always, new RangeCheckStub(range_check_info, index.result(), array.result()));
2151     } else if (use_length) {
2152       // TODO: use a (modified) version of array_range_check that does not require a
2153       //       constant length to be loaded to a register
2154       __ cmp(lir_cond_belowEqual, length.result(), index.result());
2155       __ branch(lir_cond_belowEqual, new RangeCheckStub(range_check_info, index.result(), array.result()));
2156     } else {
2157       array_range_check(array.result(), index.result(), null_check_info, range_check_info);
2158       // The range check performs the null check, so clear it out for the load
2159       null_check_info = NULL;
2160     }
2161   }
2162 
<span class="line-modified">2163   ciMethodData* md = NULL;</span>
<span class="line-added">2164   ciArrayLoadStoreData* load_store = NULL;</span>
<span class="line-added">2165   if (x-&gt;should_profile()) {</span>
<span class="line-added">2166     profile_array_type(x, md, load_store);</span>
<span class="line-added">2167   }</span>
<span class="line-added">2168 </span>
<span class="line-added">2169   Value element;</span>
<span class="line-added">2170   if (x-&gt;vt() != NULL) {</span>
<span class="line-added">2171     assert(x-&gt;array()-&gt;is_loaded_flattened_array(), &quot;must be&quot;);</span>
<span class="line-added">2172     // Find the destination address (of the NewValueTypeInstance).</span>
<span class="line-added">2173     LIR_Opr obj = x-&gt;vt()-&gt;operand();</span>
<span class="line-added">2174     LIRItem obj_item(x-&gt;vt(), this);</span>
<span class="line-added">2175 </span>
<span class="line-added">2176     access_flattened_array(true, array, index, obj_item);</span>
<span class="line-added">2177     set_no_result(x);</span>
<span class="line-added">2178     element = x-&gt;vt();</span>
<span class="line-added">2179     if (x-&gt;should_profile()) {</span>
<span class="line-added">2180       int flag = ArrayLoadStoreData::flat_array_byte_constant() | ArrayLoadStoreData::null_free_array_byte_constant();</span>
<span class="line-added">2181       profile_array_load_store_flags(md, load_store, flag);</span>
<span class="line-added">2182     }</span>
<span class="line-added">2183   } else {</span>
<span class="line-added">2184     LIR_Opr result = rlock_result(x, x-&gt;elt_type());</span>
<span class="line-added">2185     LoadFlattenedArrayStub* slow_path = NULL;</span>
<span class="line-added">2186 </span>
<span class="line-added">2187     if (x-&gt;should_profile() &amp;&amp; x-&gt;array()-&gt;maybe_null_free_array()) {</span>
<span class="line-added">2188       profile_null_free_array(array, md, load_store);</span>
<span class="line-added">2189     }</span>
<span class="line-added">2190 </span>
<span class="line-added">2191     if (x-&gt;elt_type() == T_OBJECT &amp;&amp; x-&gt;array()-&gt;maybe_flattened_array()) {</span>
<span class="line-added">2192       index.load_item();</span>
<span class="line-added">2193       // if we are loading from flattened array, load it using a runtime call</span>
<span class="line-added">2194       slow_path = new LoadFlattenedArrayStub(array.result(), index.result(), result, state_for(x, x-&gt;state_before()));</span>
<span class="line-added">2195       check_flattened_array(array.result(), LIR_OprFact::illegalOpr, slow_path);</span>
<span class="line-added">2196       set_in_conditional_code(true);</span>
<span class="line-added">2197     }</span>
<span class="line-added">2198 </span>
<span class="line-added">2199     DecoratorSet decorators = IN_HEAP | IS_ARRAY;</span>
<span class="line-added">2200     access_load_at(decorators, x-&gt;elt_type(),</span>
<span class="line-added">2201                    array, index.result(), result,</span>
<span class="line-added">2202                    NULL, null_check_info);</span>
<span class="line-added">2203 </span>
<span class="line-added">2204     if (slow_path != NULL) {</span>
<span class="line-added">2205       __ branch_destination(slow_path-&gt;continuation());</span>
<span class="line-added">2206       set_in_conditional_code(false);</span>
<span class="line-added">2207     }</span>
<span class="line-added">2208 </span>
<span class="line-added">2209     element = x;</span>
<span class="line-added">2210   }</span>
<span class="line-added">2211 </span>
<span class="line-added">2212   if (x-&gt;should_profile()) {</span>
<span class="line-added">2213     profile_element_type(element, md, load_store);</span>
<span class="line-added">2214   }</span>
<span class="line-added">2215 }</span>
2216 
<span class="line-modified">2217 void LIRGenerator::do_WithField(WithField* x) {</span>
<span class="line-modified">2218   // This happens only when a class X uses the withfield bytecode to refer to</span>
<span class="line-modified">2219   // an inline class V, where V has not yet been loaded. This is not a common</span>
<span class="line-modified">2220   // case. Let&#39;s just deoptimize.</span>
<span class="line-added">2221   CodeEmitInfo* info = state_for(x, x-&gt;state_before());</span>
<span class="line-added">2222   CodeStub* stub = new DeoptimizeStub(new CodeEmitInfo(info),</span>
<span class="line-added">2223                                       Deoptimization::Reason_unloaded,</span>
<span class="line-added">2224                                       Deoptimization::Action_make_not_entrant);</span>
<span class="line-added">2225   __ branch(lir_cond_always, stub);</span>
<span class="line-added">2226   LIR_Opr reg = rlock_result(x, T_OBJECT);</span>
<span class="line-added">2227   __ move(LIR_OprFact::oopConst(NULL), reg);</span>
2228 }
2229 
<span class="line-added">2230 void LIRGenerator::do_DefaultValue(DefaultValue* x) {</span>
<span class="line-added">2231   // Same as withfield above. Let&#39;s deoptimize.</span>
<span class="line-added">2232   CodeEmitInfo* info = state_for(x, x-&gt;state_before());</span>
<span class="line-added">2233   CodeStub* stub = new DeoptimizeStub(new CodeEmitInfo(info),</span>
<span class="line-added">2234                                       Deoptimization::Reason_unloaded,</span>
<span class="line-added">2235                                       Deoptimization::Action_make_not_entrant);</span>
<span class="line-added">2236   __ branch(lir_cond_always, stub);</span>
<span class="line-added">2237   LIR_Opr reg = rlock_result(x, T_OBJECT);</span>
<span class="line-added">2238   __ move(LIR_OprFact::oopConst(NULL), reg);</span>
<span class="line-added">2239 }</span>
2240 
2241 void LIRGenerator::do_NullCheck(NullCheck* x) {
2242   if (x-&gt;can_trap()) {
2243     LIRItem value(x-&gt;obj(), this);
2244     value.load_item();
2245     CodeEmitInfo* info = state_for(x);
2246     __ null_check(value.result(), info);
2247   }
2248 }
2249 
2250 
2251 void LIRGenerator::do_TypeCast(TypeCast* x) {
2252   LIRItem value(x-&gt;obj(), this);
2253   value.load_item();
2254   // the result is the same as from the node we are casting
2255   set_result(x, value.result());
2256 }
2257 
2258 
2259 void LIRGenerator::do_Throw(Throw* x) {
</pre>
<hr />
<pre>
2883   Compilation* comp = Compilation::current();
2884   if (do_update) {
2885     // try to find exact type, using CHA if possible, so that loading
2886     // the klass from the object can be avoided
2887     ciType* type = obj-&gt;exact_type();
2888     if (type == NULL) {
2889       type = obj-&gt;declared_type();
2890       type = comp-&gt;cha_exact_type(type);
2891     }
2892     assert(type == NULL || type-&gt;is_klass(), &quot;type should be class&quot;);
2893     exact_klass = (type != NULL &amp;&amp; type-&gt;is_loaded()) ? (ciKlass*)type : NULL;
2894 
2895     do_update = exact_klass == NULL || ciTypeEntries::valid_ciklass(profiled_k) != exact_klass;
2896   }
2897 
2898   if (!do_null &amp;&amp; !do_update) {
2899     return result;
2900   }
2901 
2902   ciKlass* exact_signature_k = NULL;
<span class="line-modified">2903   if (do_update &amp;&amp; signature_at_call_k != NULL) {</span>
2904     // Is the type from the signature exact (the only one possible)?
2905     exact_signature_k = signature_at_call_k-&gt;exact_klass();
2906     if (exact_signature_k == NULL) {
2907       exact_signature_k = comp-&gt;cha_exact_type(signature_at_call_k);
2908     } else {
2909       result = exact_signature_k;
2910       // Known statically. No need to emit any code: prevent
2911       // LIR_Assembler::emit_profile_type() from emitting useless code
2912       profiled_k = ciTypeEntries::with_status(result, profiled_k);
2913     }
2914     // exact_klass and exact_signature_k can be both non NULL but
2915     // different if exact_klass is loaded after the ciObject for
2916     // exact_signature_k is created.
2917     if (exact_klass == NULL &amp;&amp; exact_signature_k != NULL &amp;&amp; exact_klass != exact_signature_k) {
2918       // sometimes the type of the signature is better than the best type
2919       // the compiler has
2920       exact_klass = exact_signature_k;
2921     }
2922     if (callee_signature_k != NULL &amp;&amp;
2923         callee_signature_k != signature_at_call_k) {
</pre>
<hr />
<pre>
2968         assert(!src-&gt;is_illegal(), &quot;check&quot;);
2969         BasicType t = src-&gt;type();
2970         if (is_reference_type(t)) {
2971           intptr_t profiled_k = parameters-&gt;type(j);
2972           Local* local = x-&gt;state()-&gt;local_at(java_index)-&gt;as_Local();
2973           ciKlass* exact = profile_type(md, md-&gt;byte_offset_of_slot(parameters_type_data, ParametersTypeData::type_offset(0)),
2974                                         in_bytes(ParametersTypeData::type_offset(j)) - in_bytes(ParametersTypeData::type_offset(0)),
2975                                         profiled_k, local, mdp, false, local-&gt;declared_type()-&gt;as_klass(), NULL);
2976           // If the profile is known statically set it once for all and do not emit any code
2977           if (exact != NULL) {
2978             md-&gt;set_parameter_type(j, exact);
2979           }
2980           j++;
2981         }
2982         java_index += type2size[t];
2983       }
2984     }
2985   }
2986 }
2987 
<span class="line-added">2988 void LIRGenerator::profile_array_load_store_flags(ciMethodData* md, ciArrayLoadStoreData* load_store, int flag, LIR_Opr mdp) {</span>
<span class="line-added">2989   assert(md != NULL &amp;&amp; load_store != NULL, &quot;should have been initialized&quot;);</span>
<span class="line-added">2990   if (mdp == NULL) {</span>
<span class="line-added">2991     mdp = new_register(T_METADATA);</span>
<span class="line-added">2992     __ metadata2reg(md-&gt;constant_encoding(), mdp);</span>
<span class="line-added">2993   }</span>
<span class="line-added">2994   LIR_Address* addr = new LIR_Address(mdp, md-&gt;byte_offset_of_slot(load_store, DataLayout::flags_offset()), T_BYTE);</span>
<span class="line-added">2995   LIR_Opr id = new_register(T_INT);</span>
<span class="line-added">2996   __ move(addr, id);</span>
<span class="line-added">2997   __ logical_or(id, LIR_OprFact::intConst(flag), id);</span>
<span class="line-added">2998   __ store(id, addr);</span>
<span class="line-added">2999 }</span>
<span class="line-added">3000 </span>
<span class="line-added">3001 void LIRGenerator::profile_null_free_array(LIRItem array, ciMethodData* md, ciArrayLoadStoreData* load_store) {</span>
<span class="line-added">3002   LabelObj* L_end = new LabelObj();</span>
<span class="line-added">3003   LIR_Opr tmp = new_register(T_METADATA);</span>
<span class="line-added">3004   LIR_Opr mdp = new_register(T_METADATA);</span>
<span class="line-added">3005   assert(md != NULL, &quot;should have been initialized&quot;);</span>
<span class="line-added">3006   __ metadata2reg(md-&gt;constant_encoding(), mdp);</span>
<span class="line-added">3007   __ check_null_free_array(array.result(), tmp);</span>
<span class="line-added">3008   __ branch(lir_cond_equal, L_end-&gt;label());</span>
<span class="line-added">3009 </span>
<span class="line-added">3010   profile_array_load_store_flags(md, load_store, ArrayLoadStoreData::null_free_array_byte_constant(), mdp);</span>
<span class="line-added">3011 </span>
<span class="line-added">3012   __ branch_destination(L_end-&gt;label());</span>
<span class="line-added">3013 }</span>
<span class="line-added">3014 </span>
<span class="line-added">3015 void LIRGenerator::profile_array_type(AccessIndexed* x, ciMethodData*&amp; md, ciArrayLoadStoreData*&amp; load_store) {</span>
<span class="line-added">3016   int bci = x-&gt;profiled_bci();</span>
<span class="line-added">3017   md = x-&gt;profiled_method()-&gt;method_data();</span>
<span class="line-added">3018   assert(md != NULL, &quot;Sanity&quot;);</span>
<span class="line-added">3019   ciProfileData* data = md-&gt;bci_to_data(bci);</span>
<span class="line-added">3020   assert(data != NULL &amp;&amp; data-&gt;is_ArrayLoadStoreData(), &quot;incorrect profiling entry&quot;);</span>
<span class="line-added">3021   load_store = (ciArrayLoadStoreData*)data;</span>
<span class="line-added">3022   LIR_Opr mdp = LIR_OprFact::illegalOpr;</span>
<span class="line-added">3023   profile_type(md, md-&gt;byte_offset_of_slot(load_store, ArrayLoadStoreData::array_offset()), 0,</span>
<span class="line-added">3024                load_store-&gt;array()-&gt;type(), x-&gt;array(), mdp, true, NULL, NULL);</span>
<span class="line-added">3025 }</span>
<span class="line-added">3026 </span>
<span class="line-added">3027 void LIRGenerator::profile_element_type(Value element, ciMethodData* md, ciArrayLoadStoreData* load_store) {</span>
<span class="line-added">3028   assert(md != NULL &amp;&amp; load_store != NULL, &quot;should have been initialized&quot;);</span>
<span class="line-added">3029   LIR_Opr mdp = LIR_OprFact::illegalOpr;</span>
<span class="line-added">3030   profile_type(md, md-&gt;byte_offset_of_slot(load_store, ArrayLoadStoreData::element_offset()), 0,</span>
<span class="line-added">3031                load_store-&gt;element()-&gt;type(), element, mdp, false, NULL, NULL);</span>
<span class="line-added">3032 }</span>
<span class="line-added">3033 </span>
<span class="line-added">3034 </span>
3035 void LIRGenerator::do_Base(Base* x) {
3036   __ std_entry(LIR_OprFact::illegalOpr);
3037   // Emit moves from physical registers / stack slots to virtual registers
3038   CallingConvention* args = compilation()-&gt;frame_map()-&gt;incoming_arguments();
3039   IRScope* irScope = compilation()-&gt;hir()-&gt;top_scope();
3040   int java_index = 0;
3041   for (int i = 0; i &lt; args-&gt;length(); i++) {
3042     LIR_Opr src = args-&gt;at(i);
3043     assert(!src-&gt;is_illegal(), &quot;check&quot;);
3044     BasicType t = src-&gt;type();
3045 
3046     // Types which are smaller than int are passed as int, so
3047     // correct the type which passed.
3048     switch (t) {
3049     case T_BYTE:
3050     case T_BOOLEAN:
3051     case T_SHORT:
3052     case T_CHAR:
3053       t = T_INT;
3054       break;
</pre>
<hr />
<pre>
3099       LIR_Opr lock = syncLockOpr();
3100       __ load_stack_address_monitor(0, lock);
3101 
3102       CodeEmitInfo* info = new CodeEmitInfo(scope()-&gt;start()-&gt;state()-&gt;copy(ValueStack::StateBefore, SynchronizationEntryBCI), NULL, x-&gt;check_flag(Instruction::DeoptimizeOnException));
3103       CodeStub* slow_path = new MonitorEnterStub(obj, lock, info);
3104 
3105       // receiver is guaranteed non-NULL so don&#39;t need CodeEmitInfo
3106       __ lock_object(syncTempOpr(), obj, lock, new_register(T_OBJECT), slow_path, NULL);
3107     }
3108   }
3109   if (compilation()-&gt;age_code()) {
3110     CodeEmitInfo* info = new CodeEmitInfo(scope()-&gt;start()-&gt;state()-&gt;copy(ValueStack::StateBefore, 0), NULL, false);
3111     decrement_age(info);
3112   }
3113   // increment invocation counters if needed
3114   if (!method()-&gt;is_accessor()) { // Accessors do not have MDOs, so no counting.
3115     profile_parameters(x);
3116     CodeEmitInfo* info = new CodeEmitInfo(scope()-&gt;start()-&gt;state()-&gt;copy(ValueStack::StateBefore, SynchronizationEntryBCI), NULL, false);
3117     increment_invocation_counter(info);
3118   }
<span class="line-added">3119   if (method()-&gt;has_scalarized_args()) {</span>
<span class="line-added">3120     // Check if deoptimization was triggered (i.e. orig_pc was set) while buffering scalarized value type arguments</span>
<span class="line-added">3121     // in the entry point (see comments in frame::deoptimize). If so, deoptimize only now that we have the right state.</span>
<span class="line-added">3122     CodeEmitInfo* info = new CodeEmitInfo(scope()-&gt;start()-&gt;state()-&gt;copy(ValueStack::StateBefore, 0), NULL, false);</span>
<span class="line-added">3123     CodeStub* deopt_stub = new DeoptimizeStub(info, Deoptimization::Reason_none, Deoptimization::Action_none);</span>
<span class="line-added">3124     __ append(new LIR_Op0(lir_check_orig_pc));</span>
<span class="line-added">3125     __ branch(lir_cond_notEqual, deopt_stub);</span>
<span class="line-added">3126   }</span>
3127 
3128   // all blocks with a successor must end with an unconditional jump
3129   // to the successor even if they are consecutive
3130   __ jump(x-&gt;default_sux());
3131 }
3132 
3133 
3134 void LIRGenerator::do_OsrEntry(OsrEntry* x) {
3135   // construct our frame and model the production of incoming pointer
3136   // to the OSR buffer.
3137   __ osr_entry(LIR_Assembler::osrBufferPointer());
3138   LIR_Opr result = rlock_result(x);
3139   __ move(LIR_Assembler::osrBufferPointer(), result);
3140 }
3141 
<span class="line-added">3142 void LIRGenerator::invoke_load_one_argument(LIRItem* param, LIR_Opr loc) {</span>
<span class="line-added">3143   if (loc-&gt;is_register()) {</span>
<span class="line-added">3144     param-&gt;load_item_force(loc);</span>
<span class="line-added">3145   } else {</span>
<span class="line-added">3146     LIR_Address* addr = loc-&gt;as_address_ptr();</span>
<span class="line-added">3147     param-&gt;load_for_store(addr-&gt;type());</span>
<span class="line-added">3148     assert(addr-&gt;type() != T_VALUETYPE, &quot;not supported yet&quot;);</span>
<span class="line-added">3149     if (addr-&gt;type() == T_OBJECT) {</span>
<span class="line-added">3150       __ move_wide(param-&gt;result(), addr);</span>
<span class="line-added">3151     } else {</span>
<span class="line-added">3152       if (addr-&gt;type() == T_LONG || addr-&gt;type() == T_DOUBLE) {</span>
<span class="line-added">3153         __ unaligned_move(param-&gt;result(), addr);</span>
<span class="line-added">3154       } else {</span>
<span class="line-added">3155         __ move(param-&gt;result(), addr);</span>
<span class="line-added">3156       }</span>
<span class="line-added">3157     }</span>
<span class="line-added">3158   }</span>
<span class="line-added">3159 }</span>
3160 
3161 void LIRGenerator::invoke_load_arguments(Invoke* x, LIRItemList* args, const LIR_OprList* arg_list) {
3162   assert(args-&gt;length() == arg_list-&gt;length(),
3163          &quot;args=%d, arg_list=%d&quot;, args-&gt;length(), arg_list-&gt;length());
3164   for (int i = x-&gt;has_receiver() ? 1 : 0; i &lt; args-&gt;length(); i++) {
3165     LIRItem* param = args-&gt;at(i);
3166     LIR_Opr loc = arg_list-&gt;at(i);
<span class="line-modified">3167     invoke_load_one_argument(param, loc);</span>













3168   }
3169 
3170   if (x-&gt;has_receiver()) {
3171     LIRItem* receiver = args-&gt;at(0);
3172     LIR_Opr loc = arg_list-&gt;at(0);
3173     if (loc-&gt;is_register()) {
3174       receiver-&gt;load_item_force(loc);
3175     } else {
3176       assert(loc-&gt;is_address(), &quot;just checking&quot;);
3177       receiver-&gt;load_for_store(T_OBJECT);
3178       __ move_wide(receiver-&gt;result(), loc-&gt;as_address_ptr());
3179     }
3180   }
3181 }
3182 
3183 
3184 // Visits all arguments, returns appropriate items without loading them
3185 LIRItemList* LIRGenerator::invoke_visit_arguments(Invoke* x) {
3186   LIRItemList* argument_items = new LIRItemList();
3187   if (x-&gt;has_receiver()) {
</pre>
<hr />
<pre>
3328   __ move(tmp, reg);
3329 }
3330 
3331 
3332 
3333 // Code for  :  x-&gt;x() {x-&gt;cond()} x-&gt;y() ? x-&gt;tval() : x-&gt;fval()
3334 void LIRGenerator::do_IfOp(IfOp* x) {
3335 #ifdef ASSERT
3336   {
3337     ValueTag xtag = x-&gt;x()-&gt;type()-&gt;tag();
3338     ValueTag ttag = x-&gt;tval()-&gt;type()-&gt;tag();
3339     assert(xtag == intTag || xtag == objectTag, &quot;cannot handle others&quot;);
3340     assert(ttag == addressTag || ttag == intTag || ttag == objectTag || ttag == longTag, &quot;cannot handle others&quot;);
3341     assert(ttag == x-&gt;fval()-&gt;type()-&gt;tag(), &quot;cannot handle others&quot;);
3342   }
3343 #endif
3344 
3345   LIRItem left(x-&gt;x(), this);
3346   LIRItem right(x-&gt;y(), this);
3347   left.load_item();
<span class="line-modified">3348   if (can_inline_as_constant(right.value()) &amp;&amp; !x-&gt;substitutability_check()) {</span>
3349     right.dont_load_item();
3350   } else {
<span class="line-added">3351     // substitutability_check() needs to use right as a base register.</span>
3352     right.load_item();
3353   }
3354 
3355   LIRItem t_val(x-&gt;tval(), this);
3356   LIRItem f_val(x-&gt;fval(), this);
3357   t_val.dont_load_item();
3358   f_val.dont_load_item();
<span class="line-modified">3359 </span>
<span class="line-modified">3360   if (x-&gt;substitutability_check()) {</span>
<span class="line-modified">3361     substitutability_check(x, left, right, t_val, f_val);</span>
<span class="line-added">3362   } else {</span>
<span class="line-added">3363     LIR_Opr reg = rlock_result(x);</span>
<span class="line-added">3364     __ cmp(lir_cond(x-&gt;cond()), left.result(), right.result());</span>
<span class="line-added">3365     __ cmove(lir_cond(x-&gt;cond()), t_val.result(), f_val.result(), reg, as_BasicType(x-&gt;x()-&gt;type()));</span>
<span class="line-added">3366   }</span>
<span class="line-added">3367 }</span>
<span class="line-added">3368 </span>
<span class="line-added">3369 void LIRGenerator::substitutability_check(IfOp* x, LIRItem&amp; left, LIRItem&amp; right, LIRItem&amp; t_val, LIRItem&amp; f_val) {</span>
<span class="line-added">3370   assert(x-&gt;cond() == If::eql || x-&gt;cond() == If::neq, &quot;must be&quot;);</span>
<span class="line-added">3371   bool is_acmpeq = (x-&gt;cond() == If::eql);</span>
<span class="line-added">3372   LIR_Opr equal_result     = is_acmpeq ? t_val.result() : f_val.result();</span>
<span class="line-added">3373   LIR_Opr not_equal_result = is_acmpeq ? f_val.result() : t_val.result();</span>
<span class="line-added">3374   LIR_Opr result = rlock_result(x);</span>
<span class="line-added">3375   CodeEmitInfo* info = state_for(x, x-&gt;state_before());</span>
<span class="line-added">3376 </span>
<span class="line-added">3377   substitutability_check_common(x-&gt;x(), x-&gt;y(), left, right, equal_result, not_equal_result, result, info);</span>
<span class="line-added">3378 }</span>
<span class="line-added">3379 </span>
<span class="line-added">3380 void LIRGenerator::substitutability_check(If* x, LIRItem&amp; left, LIRItem&amp; right) {</span>
<span class="line-added">3381   LIR_Opr equal_result     = LIR_OprFact::intConst(1);</span>
<span class="line-added">3382   LIR_Opr not_equal_result = LIR_OprFact::intConst(0);</span>
<span class="line-added">3383   LIR_Opr result = new_register(T_INT);</span>
<span class="line-added">3384   CodeEmitInfo* info = state_for(x, x-&gt;state_before());</span>
<span class="line-added">3385 </span>
<span class="line-added">3386   substitutability_check_common(x-&gt;x(), x-&gt;y(), left, right, equal_result, not_equal_result, result, info);</span>
<span class="line-added">3387 </span>
<span class="line-added">3388   assert(x-&gt;cond() == If::eql || x-&gt;cond() == If::neq, &quot;must be&quot;);</span>
<span class="line-added">3389   __ cmp(lir_cond(x-&gt;cond()), result, equal_result);</span>
<span class="line-added">3390 }</span>
<span class="line-added">3391 </span>
<span class="line-added">3392 void LIRGenerator::substitutability_check_common(Value left_val, Value right_val, LIRItem&amp; left, LIRItem&amp; right,</span>
<span class="line-added">3393                                                  LIR_Opr equal_result, LIR_Opr not_equal_result, LIR_Opr result,</span>
<span class="line-added">3394                                                  CodeEmitInfo* info) {</span>
<span class="line-added">3395   LIR_Opr tmp1 = LIR_OprFact::illegalOpr;</span>
<span class="line-added">3396   LIR_Opr tmp2 = LIR_OprFact::illegalOpr;</span>
<span class="line-added">3397   LIR_Opr left_klass_op = LIR_OprFact::illegalOpr;</span>
<span class="line-added">3398   LIR_Opr right_klass_op = LIR_OprFact::illegalOpr;</span>
<span class="line-added">3399 </span>
<span class="line-added">3400   ciKlass* left_klass  = left_val -&gt;as_loaded_klass_or_null();</span>
<span class="line-added">3401   ciKlass* right_klass = right_val-&gt;as_loaded_klass_or_null();</span>
<span class="line-added">3402 </span>
<span class="line-added">3403   if ((left_klass == NULL || right_klass == NULL) ||// The klass is still unloaded, or came from a Phi node.</span>
<span class="line-added">3404       !left_klass-&gt;is_valuetype() || !right_klass-&gt;is_valuetype()) {</span>
<span class="line-added">3405     init_temps_for_substitutability_check(tmp1, tmp2);</span>
<span class="line-added">3406   }</span>
<span class="line-added">3407 </span>
<span class="line-added">3408   if (left_klass != NULL &amp;&amp; left_klass-&gt;is_valuetype() &amp;&amp; left_klass == right_klass) {</span>
<span class="line-added">3409     // No need to load klass -- the operands are statically known to be the same value klass.</span>
<span class="line-added">3410   } else {</span>
<span class="line-added">3411     BasicType t_klass = UseCompressedOops ? T_INT : T_METADATA;</span>
<span class="line-added">3412     left_klass_op = new_register(t_klass);</span>
<span class="line-added">3413     right_klass_op = new_register(t_klass);</span>
<span class="line-added">3414   }</span>
<span class="line-added">3415 </span>
<span class="line-added">3416   CodeStub* slow_path = new SubstitutabilityCheckStub(left.result(), right.result(), info);</span>
<span class="line-added">3417   __ substitutability_check(result, left.result(), right.result(), equal_result, not_equal_result,</span>
<span class="line-added">3418                             tmp1, tmp2,</span>
3419                             left_klass, right_klass, left_klass_op, right_klass_op, info, slow_path);
3420 }
3421 
3422 #ifdef JFR_HAVE_INTRINSICS
3423 void LIRGenerator::do_ClassIDIntrinsic(Intrinsic* x) {
3424   CodeEmitInfo* info = state_for(x);
3425   CodeEmitInfo* info2 = new CodeEmitInfo(info); // Clone for the second null check
3426 
3427   assert(info != NULL, &quot;must have info&quot;);
3428   LIRItem arg(x-&gt;argument_at(0), this);
3429 
3430   arg.load_item();
3431   LIR_Opr klass = new_register(T_METADATA);
3432   __ move(new LIR_Address(arg.result(), java_lang_Class::klass_offset(), T_ADDRESS), klass, info);
3433   LIR_Opr id = new_register(T_LONG);
3434   ByteSize offset = KLASS_TRACE_ID_OFFSET;
3435   LIR_Address* trace_id_addr = new LIR_Address(klass, in_bytes(offset), T_LONG);
3436 
3437   __ move(trace_id_addr, id);
3438   __ logical_or(id, LIR_OprFact::longConst(0x01l), id);
</pre>
<hr />
<pre>
3711   if (x-&gt;recv() != NULL || x-&gt;nb_profiled_args() &gt; 0) {
3712     profile_parameters_at_call(x);
3713   }
3714 
3715   if (x-&gt;recv() != NULL) {
3716     LIRItem value(x-&gt;recv(), this);
3717     value.load_item();
3718     recv = new_register(T_OBJECT);
3719     __ move(value.result(), recv);
3720   }
3721   __ profile_call(x-&gt;method(), x-&gt;bci_of_invoke(), x-&gt;callee(), mdo, recv, tmp, x-&gt;known_holder());
3722 }
3723 
3724 void LIRGenerator::do_ProfileReturnType(ProfileReturnType* x) {
3725   int bci = x-&gt;bci_of_invoke();
3726   ciMethodData* md = x-&gt;method()-&gt;method_data_or_null();
3727   assert(md != NULL, &quot;Sanity&quot;);
3728   ciProfileData* data = md-&gt;bci_to_data(bci);
3729   if (data != NULL) {
3730     assert(data-&gt;is_CallTypeData() || data-&gt;is_VirtualCallTypeData(), &quot;wrong profile data type&quot;);
<span class="line-modified">3731     ciSingleTypeEntry* ret = data-&gt;is_CallTypeData() ? ((ciCallTypeData*)data)-&gt;ret() : ((ciVirtualCallTypeData*)data)-&gt;ret();</span>
3732     LIR_Opr mdp = LIR_OprFact::illegalOpr;
3733 
3734     bool ignored_will_link;
3735     ciSignature* signature_at_call = NULL;
3736     x-&gt;method()-&gt;get_method_at_bci(bci, ignored_will_link, &amp;signature_at_call);
3737 
3738     // The offset within the MDO of the entry to update may be too large
3739     // to be used in load/store instructions on some platforms. So have
3740     // profile_type() compute the address of the profile in a register.
3741     ciKlass* exact = profile_type(md, md-&gt;byte_offset_of_slot(data, ret-&gt;type_offset()), 0,
3742         ret-&gt;type(), x-&gt;ret(), mdp,
3743         !x-&gt;needs_null_check(),
3744         signature_at_call-&gt;return_type()-&gt;as_klass(),
3745         x-&gt;callee()-&gt;signature()-&gt;return_type()-&gt;as_klass());
3746     if (exact != NULL) {
3747       md-&gt;set_return_type(bci, exact);
3748     }
3749   }
3750 }
3751 
</pre>
</td>
</tr>
</table>
<center><a href="c1_LIR.hpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="../ci/ciField.cpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>