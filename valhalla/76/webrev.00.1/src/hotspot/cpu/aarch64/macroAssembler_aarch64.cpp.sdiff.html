<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/cpu/aarch64/macroAssembler_aarch64.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="c1_LIRGenerator_aarch64.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="macroAssembler_aarch64.hpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/cpu/aarch64/macroAssembler_aarch64.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
  29 #include &quot;jvm.h&quot;
  30 #include &quot;asm/assembler.hpp&quot;
  31 #include &quot;asm/assembler.inline.hpp&quot;
  32 #include &quot;gc/shared/barrierSet.hpp&quot;
  33 #include &quot;gc/shared/cardTable.hpp&quot;
  34 #include &quot;gc/shared/barrierSetAssembler.hpp&quot;
  35 #include &quot;gc/shared/cardTableBarrierSet.hpp&quot;
  36 #include &quot;interpreter/interpreter.hpp&quot;
  37 #include &quot;compiler/disassembler.hpp&quot;
  38 #include &quot;memory/resourceArea.hpp&quot;
  39 #include &quot;memory/universe.hpp&quot;
  40 #include &quot;nativeInst_aarch64.hpp&quot;
  41 #include &quot;oops/accessDecorators.hpp&quot;
  42 #include &quot;oops/compressedOops.inline.hpp&quot;
  43 #include &quot;oops/klass.inline.hpp&quot;
  44 #include &quot;runtime/biasedLocking.hpp&quot;
  45 #include &quot;runtime/icache.hpp&quot;
  46 #include &quot;runtime/interfaceSupport.inline.hpp&quot;
  47 #include &quot;runtime/jniHandles.inline.hpp&quot;
  48 #include &quot;runtime/sharedRuntime.hpp&quot;

  49 #include &quot;runtime/thread.hpp&quot;
  50 #include &quot;utilities/powerOfTwo.hpp&quot;
  51 #ifdef COMPILER1
  52 #include &quot;c1/c1_LIRAssembler.hpp&quot;
  53 #endif
  54 #ifdef COMPILER2
  55 #include &quot;oops/oop.hpp&quot;
  56 #include &quot;opto/compile.hpp&quot;
  57 #include &quot;opto/node.hpp&quot;
  58 #include &quot;opto/output.hpp&quot;
  59 #endif
  60 
  61 #ifdef PRODUCT
  62 #define BLOCK_COMMENT(str) /* nothing */
  63 #else
  64 #define BLOCK_COMMENT(str) block_comment(str)
  65 #endif
  66 #define STOP(str) stop(str);
  67 #define BIND(label) bind(label); BLOCK_COMMENT(#label &quot;:&quot;)
  68 
</pre>
<hr />
<pre>
1297   ldrb(scratch, Address(klass, InstanceKlass::init_state_offset()));
1298   subs(zr, scratch, InstanceKlass::fully_initialized);
1299   br(Assembler::EQ, *L_fast_path);
1300 
1301   // Fast path check: current thread is initializer thread
1302   ldr(scratch, Address(klass, InstanceKlass::init_thread_offset()));
1303   cmp(rthread, scratch);
1304 
1305   if (L_slow_path == &amp;L_fallthrough) {
1306     br(Assembler::EQ, *L_fast_path);
1307     bind(*L_slow_path);
1308   } else if (L_fast_path == &amp;L_fallthrough) {
1309     br(Assembler::NE, *L_slow_path);
1310     bind(*L_fast_path);
1311   } else {
1312     Unimplemented();
1313   }
1314 }
1315 
1316 void MacroAssembler::verify_oop(Register reg, const char* s) {
<span class="line-modified">1317   if (!VerifyOops) return;</span>




1318 
1319   // Pass register number to verify_oop_subroutine
1320   const char* b = NULL;
1321   {
1322     ResourceMark rm;
1323     stringStream ss;
1324     ss.print(&quot;verify_oop: %s: %s&quot;, reg-&gt;name(), s);
1325     b = code_string(ss.as_string());
1326   }
1327   BLOCK_COMMENT(&quot;verify_oop {&quot;);
1328 
1329   stp(r0, rscratch1, Address(pre(sp, -2 * wordSize)));
1330   stp(rscratch2, lr, Address(pre(sp, -2 * wordSize)));
1331 
1332   mov(r0, reg);
1333   mov(rscratch1, (address)b);
1334 
1335   // call indirectly to solve generation ordering problem
1336   lea(rscratch2, ExternalAddress(StubRoutines::verify_oop_subroutine_entry_address()));
1337   ldr(rscratch2, Address(rscratch2));
1338   blr(rscratch2);
1339 
1340   ldp(rscratch2, lr, Address(post(sp, 2 * wordSize)));
1341   ldp(r0, rscratch1, Address(post(sp, 2 * wordSize)));
1342 
1343   BLOCK_COMMENT(&quot;} verify_oop&quot;);
1344 }
1345 
1346 void MacroAssembler::verify_oop_addr(Address addr, const char* s) {
<span class="line-modified">1347   if (!VerifyOops) return;</span>




1348 
1349   const char* b = NULL;
1350   {
1351     ResourceMark rm;
1352     stringStream ss;
1353     ss.print(&quot;verify_oop_addr: %s&quot;, s);
1354     b = code_string(ss.as_string());
1355   }
1356   BLOCK_COMMENT(&quot;verify_oop_addr {&quot;);
1357 
1358   stp(r0, rscratch1, Address(pre(sp, -2 * wordSize)));
1359   stp(rscratch2, lr, Address(pre(sp, -2 * wordSize)));
1360 
1361   // addr may contain sp so we will have to adjust it based on the
1362   // pushes that we just did.
1363   if (addr.uses(sp)) {
1364     lea(r0, addr);
1365     ldr(r0, Address(r0, 4 * wordSize));
1366   } else {
1367     ldr(r0, addr);
</pre>
<hr />
<pre>
1420 
1421 void MacroAssembler::call_VM_leaf(address entry_point, Register arg_0) {
1422   pass_arg0(this, arg_0);
1423   call_VM_leaf_base(entry_point, 1);
1424 }
1425 
1426 void MacroAssembler::call_VM_leaf(address entry_point, Register arg_0, Register arg_1) {
1427   pass_arg0(this, arg_0);
1428   pass_arg1(this, arg_1);
1429   call_VM_leaf_base(entry_point, 2);
1430 }
1431 
1432 void MacroAssembler::call_VM_leaf(address entry_point, Register arg_0,
1433                                   Register arg_1, Register arg_2) {
1434   pass_arg0(this, arg_0);
1435   pass_arg1(this, arg_1);
1436   pass_arg2(this, arg_2);
1437   call_VM_leaf_base(entry_point, 3);
1438 }
1439 




1440 void MacroAssembler::super_call_VM_leaf(address entry_point, Register arg_0) {
1441   pass_arg0(this, arg_0);
1442   MacroAssembler::call_VM_leaf_base(entry_point, 1);
1443 }
1444 
1445 void MacroAssembler::super_call_VM_leaf(address entry_point, Register arg_0, Register arg_1) {
1446 
1447   assert(arg_0 != c_rarg1, &quot;smashed arg&quot;);
1448   pass_arg1(this, arg_1);
1449   pass_arg0(this, arg_0);
1450   MacroAssembler::call_VM_leaf_base(entry_point, 2);
1451 }
1452 
1453 void MacroAssembler::super_call_VM_leaf(address entry_point, Register arg_0, Register arg_1, Register arg_2) {
1454   assert(arg_0 != c_rarg2, &quot;smashed arg&quot;);
1455   assert(arg_1 != c_rarg2, &quot;smashed arg&quot;);
1456   pass_arg2(this, arg_2);
1457   assert(arg_0 != c_rarg1, &quot;smashed arg&quot;);
1458   pass_arg1(this, arg_1);
1459   pass_arg0(this, arg_0);
</pre>
<hr />
<pre>
1469   assert(arg_1 != c_rarg2, &quot;smashed arg&quot;);
1470   pass_arg2(this, arg_2);
1471   assert(arg_0 != c_rarg1, &quot;smashed arg&quot;);
1472   pass_arg1(this, arg_1);
1473   pass_arg0(this, arg_0);
1474   MacroAssembler::call_VM_leaf_base(entry_point, 4);
1475 }
1476 
1477 void MacroAssembler::null_check(Register reg, int offset) {
1478   if (needs_explicit_null_check(offset)) {
1479     // provoke OS NULL exception if reg = NULL by
1480     // accessing M[reg] w/o changing any registers
1481     // NOTE: this is plenty to provoke a segv
1482     ldr(zr, Address(reg));
1483   } else {
1484     // nothing to do, (later) access of M[reg + offset]
1485     // will provoke OS NULL exception if reg = NULL
1486   }
1487 }
1488 

































1489 // MacroAssembler protected routines needed to implement
1490 // public methods
1491 
1492 void MacroAssembler::mov(Register r, Address dest) {
1493   code_section()-&gt;relocate(pc(), dest.rspec());
1494   u_int64_t imm64 = (u_int64_t)dest.target();
1495   movptr(r, imm64);
1496 }
1497 
1498 // Move a constant pointer into r.  In AArch64 mode the virtual
1499 // address space is 48 bits in size, so we only need three
1500 // instructions to create a patchable instruction sequence that can
1501 // reach anywhere.
1502 void MacroAssembler::movptr(Register r, uintptr_t imm64) {
1503 #ifndef PRODUCT
1504   {
1505     char buffer[64];
1506     snprintf(buffer, sizeof(buffer), &quot;0x%&quot; PRIX64, imm64);
1507     block_comment(buffer);
1508   }
</pre>
<hr />
<pre>
3672   ldr(rscratch1, Address(rscratch1, offset));
3673   cmp(src1, rscratch1);
3674 }
3675 
3676 void MacroAssembler::cmpoop(Register obj1, Register obj2) {
3677   BarrierSetAssembler* bs = BarrierSet::barrier_set()-&gt;barrier_set_assembler();
3678   bs-&gt;obj_equals(this, obj1, obj2);
3679 }
3680 
3681 void MacroAssembler::load_method_holder_cld(Register rresult, Register rmethod) {
3682   load_method_holder(rresult, rmethod);
3683   ldr(rresult, Address(rresult, InstanceKlass::class_loader_data_offset()));
3684 }
3685 
3686 void MacroAssembler::load_method_holder(Register holder, Register method) {
3687   ldr(holder, Address(method, Method::const_offset()));                      // ConstMethod*
3688   ldr(holder, Address(holder, ConstMethod::constants_offset()));             // ConstantPool*
3689   ldr(holder, Address(holder, ConstantPool::pool_holder_offset_in_bytes())); // InstanceKlass*
3690 }
3691 
<span class="line-modified">3692 void MacroAssembler::load_klass(Register dst, Register src) {</span>
3693   if (UseCompressedClassPointers) {
3694     ldrw(dst, Address(src, oopDesc::klass_offset_in_bytes()));
<span class="line-removed">3695     decode_klass_not_null(dst);</span>
3696   } else {
3697     ldr(dst, Address(src, oopDesc::klass_offset_in_bytes()));
3698   }
3699 }
3700 










3701 // ((OopHandle)result).resolve();
3702 void MacroAssembler::resolve_oop_handle(Register result, Register tmp) {
3703   // OopHandle::resolve is an indirection.
3704   access_load_at(T_OBJECT, IN_NATIVE, result, Address(result, 0), tmp, noreg);
3705 }
3706 
3707 // ((WeakHandle)result).resolve();
3708 void MacroAssembler::resolve_weak_handle(Register rresult, Register rtmp) {
3709   assert_different_registers(rresult, rtmp);
3710   Label resolved;
3711 
3712   // A null weak handle resolves to null.
3713   cbz(rresult, resolved);
3714 
3715   // Only 64 bit platforms support GCs that require a tmp register
3716   // Only IN_HEAP loads require a thread_tmp register
3717   // WeakHandle::resolve is an indirection like jweak.
3718   access_load_at(T_OBJECT, IN_NATIVE | ON_PHANTOM_OOP_REF,
3719                  rresult, Address(rresult), rtmp, /*tmp_thread*/noreg);
3720   bind(resolved);
3721 }
3722 
3723 void MacroAssembler::load_mirror(Register dst, Register method, Register tmp) {
3724   const int mirror_offset = in_bytes(Klass::java_mirror_offset());
3725   ldr(dst, Address(rmethod, Method::const_offset()));
3726   ldr(dst, Address(dst, ConstMethod::constants_offset()));
3727   ldr(dst, Address(dst, ConstantPool::pool_holder_offset_in_bytes()));
3728   ldr(dst, Address(dst, mirror_offset));
3729   resolve_oop_handle(dst, tmp);
3730 }
3731 









3732 void MacroAssembler::cmp_klass(Register oop, Register trial_klass, Register tmp) {
3733   if (UseCompressedClassPointers) {
3734     ldrw(tmp, Address(oop, oopDesc::klass_offset_in_bytes()));
3735     if (CompressedKlassPointers::base() == NULL) {
3736       cmp(trial_klass, tmp, LSL, CompressedKlassPointers::shift());
3737       return;
3738     } else if (((uint64_t)CompressedKlassPointers::base() &amp; 0xffffffff) == 0
3739                &amp;&amp; CompressedKlassPointers::shift() == 0) {
3740       // Only the bottom 32 bits matter
3741       cmpw(trial_klass, tmp);
3742       return;
3743     }
3744     decode_klass_not_null(tmp);
3745   } else {
3746     ldr(tmp, Address(oop, oopDesc::klass_offset_in_bytes()));
3747   }
3748   cmp(trial_klass, tmp);
3749 }
3750 
3751 void MacroAssembler::load_prototype_header(Register dst, Register src) {
</pre>
<hr />
<pre>
4049   narrowKlass nk = CompressedKlassPointers::encode(k);
4050   movz(dst, (nk &gt;&gt; 16), 16);
4051   movk(dst, nk &amp; 0xffff);
4052 }
4053 
4054 void MacroAssembler::access_load_at(BasicType type, DecoratorSet decorators,
4055                                     Register dst, Address src,
4056                                     Register tmp1, Register thread_tmp) {
4057   BarrierSetAssembler *bs = BarrierSet::barrier_set()-&gt;barrier_set_assembler();
4058   decorators = AccessInternal::decorator_fixup(decorators);
4059   bool as_raw = (decorators &amp; AS_RAW) != 0;
4060   if (as_raw) {
4061     bs-&gt;BarrierSetAssembler::load_at(this, decorators, type, dst, src, tmp1, thread_tmp);
4062   } else {
4063     bs-&gt;load_at(this, decorators, type, dst, src, tmp1, thread_tmp);
4064   }
4065 }
4066 
4067 void MacroAssembler::access_store_at(BasicType type, DecoratorSet decorators,
4068                                      Address dst, Register src,
<span class="line-modified">4069                                      Register tmp1, Register thread_tmp) {</span>

4070   BarrierSetAssembler *bs = BarrierSet::barrier_set()-&gt;barrier_set_assembler();
4071   decorators = AccessInternal::decorator_fixup(decorators);
4072   bool as_raw = (decorators &amp; AS_RAW) != 0;
4073   if (as_raw) {
<span class="line-modified">4074     bs-&gt;BarrierSetAssembler::store_at(this, decorators, type, dst, src, tmp1, thread_tmp);</span>
4075   } else {
<span class="line-modified">4076     bs-&gt;store_at(this, decorators, type, dst, src, tmp1, thread_tmp);</span>
4077   }
4078 }
4079 
4080 void MacroAssembler::resolve(DecoratorSet decorators, Register obj) {
4081   // Use stronger ACCESS_WRITE|ACCESS_READ by default.
4082   if ((decorators &amp; (ACCESS_READ | ACCESS_WRITE)) == 0) {
4083     decorators |= ACCESS_READ | ACCESS_WRITE;
4084   }
4085   BarrierSetAssembler* bs = BarrierSet::barrier_set()-&gt;barrier_set_assembler();
4086   return bs-&gt;resolve(this, decorators, obj);
4087 }
4088 
4089 void MacroAssembler::load_heap_oop(Register dst, Address src, Register tmp1,
4090                                    Register thread_tmp, DecoratorSet decorators) {
4091   access_load_at(T_OBJECT, IN_HEAP | decorators, dst, src, tmp1, thread_tmp);
4092 }
4093 
4094 void MacroAssembler::load_heap_oop_not_null(Register dst, Address src, Register tmp1,
4095                                             Register thread_tmp, DecoratorSet decorators) {
4096   access_load_at(T_OBJECT, IN_HEAP | IS_NOT_NULL | decorators, dst, src, tmp1, thread_tmp);
4097 }
4098 
4099 void MacroAssembler::store_heap_oop(Address dst, Register src, Register tmp1,
<span class="line-modified">4100                                     Register thread_tmp, DecoratorSet decorators) {</span>
<span class="line-modified">4101   access_store_at(T_OBJECT, IN_HEAP | decorators, dst, src, tmp1, thread_tmp);</span>
4102 }
4103 
4104 // Used for storing NULLs.
4105 void MacroAssembler::store_heap_oop_null(Address dst) {
<span class="line-modified">4106   access_store_at(T_OBJECT, IN_HEAP, dst, noreg, noreg, noreg);</span>
4107 }
4108 
4109 Address MacroAssembler::allocate_metadata_address(Metadata* obj) {
4110   assert(oop_recorder() != NULL, &quot;this assembler needs a Recorder&quot;);
4111   int index = oop_recorder()-&gt;allocate_metadata_index(obj);
4112   RelocationHolder rspec = metadata_Relocation::spec(index);
4113   return Address((address)obj, rspec);
4114 }
4115 
4116 // Move an oop into a register.  immediate is true if we want
4117 // immediate instructions and nmethod entry barriers are not enabled.
4118 // i.e. we are not going to patch this instruction while the code is being
4119 // executed by another thread.
4120 void MacroAssembler::movoop(Register dst, jobject obj, bool immediate) {
4121   int oop_index;
4122   if (obj == NULL) {
4123     oop_index = oop_recorder()-&gt;allocate_oop_index(obj);
4124   } else {
4125 #ifdef ASSERT
4126     {
</pre>
<hr />
<pre>
5159 // get_thread() can be called anywhere inside generated code so we
5160 // need to save whatever non-callee save context might get clobbered
5161 // by the call to JavaThread::aarch64_get_thread_helper() or, indeed,
5162 // the call setup code.
5163 //
5164 // aarch64_get_thread_helper() clobbers only r0, r1, and flags.
5165 //
5166 void MacroAssembler::get_thread(Register dst) {
5167   RegSet saved_regs = RegSet::range(r0, r1) + lr - dst;
5168   push(saved_regs, sp);
5169 
5170   mov(lr, CAST_FROM_FN_PTR(address, JavaThread::aarch64_get_thread_helper));
5171   blr(lr);
5172   if (dst != c_rarg0) {
5173     mov(dst, c_rarg0);
5174   }
5175 
5176   pop(saved_regs, sp);
5177 }
5178 






































































































































































































































































































































































































5179 void MacroAssembler::cache_wb(Address line) {
5180   assert(line.getMode() == Address::base_plus_offset, &quot;mode should be base_plus_offset&quot;);
5181   assert(line.index() == noreg, &quot;index should be noreg&quot;);
5182   assert(line.offset() == 0, &quot;offset should be 0&quot;);
5183   // would like to assert this
5184   // assert(line._ext.shift == 0, &quot;shift should be zero&quot;);
5185   if (VM_Version::supports_dcpop()) {
5186     // writeback using clear virtual address to point of persistence
5187     dc(Assembler::CVAP, line.base());
5188   } else {
5189     // no need to generate anything as Unsafe.writebackMemory should
5190     // never invoke this stub
5191   }
5192 }
5193 
5194 void MacroAssembler::cache_wbsync(bool is_pre) {
5195   // we only need a barrier post sync
5196   if (!is_pre) {
5197     membar(Assembler::AnyAny);
5198   }
</pre>
</td>
<td>
<hr />
<pre>
  29 #include &quot;jvm.h&quot;
  30 #include &quot;asm/assembler.hpp&quot;
  31 #include &quot;asm/assembler.inline.hpp&quot;
  32 #include &quot;gc/shared/barrierSet.hpp&quot;
  33 #include &quot;gc/shared/cardTable.hpp&quot;
  34 #include &quot;gc/shared/barrierSetAssembler.hpp&quot;
  35 #include &quot;gc/shared/cardTableBarrierSet.hpp&quot;
  36 #include &quot;interpreter/interpreter.hpp&quot;
  37 #include &quot;compiler/disassembler.hpp&quot;
  38 #include &quot;memory/resourceArea.hpp&quot;
  39 #include &quot;memory/universe.hpp&quot;
  40 #include &quot;nativeInst_aarch64.hpp&quot;
  41 #include &quot;oops/accessDecorators.hpp&quot;
  42 #include &quot;oops/compressedOops.inline.hpp&quot;
  43 #include &quot;oops/klass.inline.hpp&quot;
  44 #include &quot;runtime/biasedLocking.hpp&quot;
  45 #include &quot;runtime/icache.hpp&quot;
  46 #include &quot;runtime/interfaceSupport.inline.hpp&quot;
  47 #include &quot;runtime/jniHandles.inline.hpp&quot;
  48 #include &quot;runtime/sharedRuntime.hpp&quot;
<span class="line-added">  49 #include &quot;runtime/signature_cc.hpp&quot;</span>
  50 #include &quot;runtime/thread.hpp&quot;
  51 #include &quot;utilities/powerOfTwo.hpp&quot;
  52 #ifdef COMPILER1
  53 #include &quot;c1/c1_LIRAssembler.hpp&quot;
  54 #endif
  55 #ifdef COMPILER2
  56 #include &quot;oops/oop.hpp&quot;
  57 #include &quot;opto/compile.hpp&quot;
  58 #include &quot;opto/node.hpp&quot;
  59 #include &quot;opto/output.hpp&quot;
  60 #endif
  61 
  62 #ifdef PRODUCT
  63 #define BLOCK_COMMENT(str) /* nothing */
  64 #else
  65 #define BLOCK_COMMENT(str) block_comment(str)
  66 #endif
  67 #define STOP(str) stop(str);
  68 #define BIND(label) bind(label); BLOCK_COMMENT(#label &quot;:&quot;)
  69 
</pre>
<hr />
<pre>
1298   ldrb(scratch, Address(klass, InstanceKlass::init_state_offset()));
1299   subs(zr, scratch, InstanceKlass::fully_initialized);
1300   br(Assembler::EQ, *L_fast_path);
1301 
1302   // Fast path check: current thread is initializer thread
1303   ldr(scratch, Address(klass, InstanceKlass::init_thread_offset()));
1304   cmp(rthread, scratch);
1305 
1306   if (L_slow_path == &amp;L_fallthrough) {
1307     br(Assembler::EQ, *L_fast_path);
1308     bind(*L_slow_path);
1309   } else if (L_fast_path == &amp;L_fallthrough) {
1310     br(Assembler::NE, *L_slow_path);
1311     bind(*L_fast_path);
1312   } else {
1313     Unimplemented();
1314   }
1315 }
1316 
1317 void MacroAssembler::verify_oop(Register reg, const char* s) {
<span class="line-modified">1318   if (!VerifyOops || VerifyAdapterSharing) {</span>
<span class="line-added">1319     // Below address of the code string confuses VerifyAdapterSharing</span>
<span class="line-added">1320     // because it may differ between otherwise equivalent adapters.</span>
<span class="line-added">1321     return;</span>
<span class="line-added">1322   }</span>
1323 
1324   // Pass register number to verify_oop_subroutine
1325   const char* b = NULL;
1326   {
1327     ResourceMark rm;
1328     stringStream ss;
1329     ss.print(&quot;verify_oop: %s: %s&quot;, reg-&gt;name(), s);
1330     b = code_string(ss.as_string());
1331   }
1332   BLOCK_COMMENT(&quot;verify_oop {&quot;);
1333 
1334   stp(r0, rscratch1, Address(pre(sp, -2 * wordSize)));
1335   stp(rscratch2, lr, Address(pre(sp, -2 * wordSize)));
1336 
1337   mov(r0, reg);
1338   mov(rscratch1, (address)b);
1339 
1340   // call indirectly to solve generation ordering problem
1341   lea(rscratch2, ExternalAddress(StubRoutines::verify_oop_subroutine_entry_address()));
1342   ldr(rscratch2, Address(rscratch2));
1343   blr(rscratch2);
1344 
1345   ldp(rscratch2, lr, Address(post(sp, 2 * wordSize)));
1346   ldp(r0, rscratch1, Address(post(sp, 2 * wordSize)));
1347 
1348   BLOCK_COMMENT(&quot;} verify_oop&quot;);
1349 }
1350 
1351 void MacroAssembler::verify_oop_addr(Address addr, const char* s) {
<span class="line-modified">1352   if (!VerifyOops || VerifyAdapterSharing) {</span>
<span class="line-added">1353     // Below address of the code string confuses VerifyAdapterSharing</span>
<span class="line-added">1354     // because it may differ between otherwise equivalent adapters.</span>
<span class="line-added">1355     return;</span>
<span class="line-added">1356   }</span>
1357 
1358   const char* b = NULL;
1359   {
1360     ResourceMark rm;
1361     stringStream ss;
1362     ss.print(&quot;verify_oop_addr: %s&quot;, s);
1363     b = code_string(ss.as_string());
1364   }
1365   BLOCK_COMMENT(&quot;verify_oop_addr {&quot;);
1366 
1367   stp(r0, rscratch1, Address(pre(sp, -2 * wordSize)));
1368   stp(rscratch2, lr, Address(pre(sp, -2 * wordSize)));
1369 
1370   // addr may contain sp so we will have to adjust it based on the
1371   // pushes that we just did.
1372   if (addr.uses(sp)) {
1373     lea(r0, addr);
1374     ldr(r0, Address(r0, 4 * wordSize));
1375   } else {
1376     ldr(r0, addr);
</pre>
<hr />
<pre>
1429 
1430 void MacroAssembler::call_VM_leaf(address entry_point, Register arg_0) {
1431   pass_arg0(this, arg_0);
1432   call_VM_leaf_base(entry_point, 1);
1433 }
1434 
1435 void MacroAssembler::call_VM_leaf(address entry_point, Register arg_0, Register arg_1) {
1436   pass_arg0(this, arg_0);
1437   pass_arg1(this, arg_1);
1438   call_VM_leaf_base(entry_point, 2);
1439 }
1440 
1441 void MacroAssembler::call_VM_leaf(address entry_point, Register arg_0,
1442                                   Register arg_1, Register arg_2) {
1443   pass_arg0(this, arg_0);
1444   pass_arg1(this, arg_1);
1445   pass_arg2(this, arg_2);
1446   call_VM_leaf_base(entry_point, 3);
1447 }
1448 
<span class="line-added">1449 void MacroAssembler::super_call_VM_leaf(address entry_point) {</span>
<span class="line-added">1450   MacroAssembler::call_VM_leaf_base(entry_point, 1);</span>
<span class="line-added">1451 }</span>
<span class="line-added">1452 </span>
1453 void MacroAssembler::super_call_VM_leaf(address entry_point, Register arg_0) {
1454   pass_arg0(this, arg_0);
1455   MacroAssembler::call_VM_leaf_base(entry_point, 1);
1456 }
1457 
1458 void MacroAssembler::super_call_VM_leaf(address entry_point, Register arg_0, Register arg_1) {
1459 
1460   assert(arg_0 != c_rarg1, &quot;smashed arg&quot;);
1461   pass_arg1(this, arg_1);
1462   pass_arg0(this, arg_0);
1463   MacroAssembler::call_VM_leaf_base(entry_point, 2);
1464 }
1465 
1466 void MacroAssembler::super_call_VM_leaf(address entry_point, Register arg_0, Register arg_1, Register arg_2) {
1467   assert(arg_0 != c_rarg2, &quot;smashed arg&quot;);
1468   assert(arg_1 != c_rarg2, &quot;smashed arg&quot;);
1469   pass_arg2(this, arg_2);
1470   assert(arg_0 != c_rarg1, &quot;smashed arg&quot;);
1471   pass_arg1(this, arg_1);
1472   pass_arg0(this, arg_0);
</pre>
<hr />
<pre>
1482   assert(arg_1 != c_rarg2, &quot;smashed arg&quot;);
1483   pass_arg2(this, arg_2);
1484   assert(arg_0 != c_rarg1, &quot;smashed arg&quot;);
1485   pass_arg1(this, arg_1);
1486   pass_arg0(this, arg_0);
1487   MacroAssembler::call_VM_leaf_base(entry_point, 4);
1488 }
1489 
1490 void MacroAssembler::null_check(Register reg, int offset) {
1491   if (needs_explicit_null_check(offset)) {
1492     // provoke OS NULL exception if reg = NULL by
1493     // accessing M[reg] w/o changing any registers
1494     // NOTE: this is plenty to provoke a segv
1495     ldr(zr, Address(reg));
1496   } else {
1497     // nothing to do, (later) access of M[reg + offset]
1498     // will provoke OS NULL exception if reg = NULL
1499   }
1500 }
1501 
<span class="line-added">1502 void MacroAssembler::test_klass_is_value(Register klass, Register temp_reg, Label&amp; is_value) {</span>
<span class="line-added">1503   ldrw(temp_reg, Address(klass, Klass::access_flags_offset()));</span>
<span class="line-added">1504   andr(temp_reg, temp_reg, JVM_ACC_VALUE);</span>
<span class="line-added">1505   cbnz(temp_reg, is_value);</span>
<span class="line-added">1506 }</span>
<span class="line-added">1507 </span>
<span class="line-added">1508 void MacroAssembler::test_field_is_flattenable(Register flags, Register temp_reg, Label&amp; is_flattenable) {</span>
<span class="line-added">1509   (void) temp_reg; // keep signature uniform with x86</span>
<span class="line-added">1510   tbnz(flags, ConstantPoolCacheEntry::is_flattenable_field_shift, is_flattenable);</span>
<span class="line-added">1511 }</span>
<span class="line-added">1512 </span>
<span class="line-added">1513 void MacroAssembler::test_field_is_not_flattenable(Register flags, Register temp_reg, Label&amp; not_flattenable) {</span>
<span class="line-added">1514   (void) temp_reg; // keep signature uniform with x86</span>
<span class="line-added">1515   tbz(flags, ConstantPoolCacheEntry::is_flattenable_field_shift, not_flattenable);</span>
<span class="line-added">1516 }</span>
<span class="line-added">1517 </span>
<span class="line-added">1518 void MacroAssembler::test_field_is_flattened(Register flags, Register temp_reg, Label&amp; is_flattened) {</span>
<span class="line-added">1519   (void) temp_reg; // keep signature uniform with x86</span>
<span class="line-added">1520   tbnz(flags, ConstantPoolCacheEntry::is_flattened_field_shift, is_flattened);</span>
<span class="line-added">1521 }</span>
<span class="line-added">1522 </span>
<span class="line-added">1523 void MacroAssembler::test_flattened_array_oop(Register oop, Register temp_reg, Label&amp; is_flattened_array) {</span>
<span class="line-added">1524   load_storage_props(temp_reg, oop);</span>
<span class="line-added">1525   andr(temp_reg, temp_reg, ArrayStorageProperties::flattened_value);</span>
<span class="line-added">1526   cbnz(temp_reg, is_flattened_array);</span>
<span class="line-added">1527 }</span>
<span class="line-added">1528 </span>
<span class="line-added">1529 void MacroAssembler::test_null_free_array_oop(Register oop, Register temp_reg, Label&amp; is_null_free_array) {</span>
<span class="line-added">1530   load_storage_props(temp_reg, oop);</span>
<span class="line-added">1531   andr(temp_reg, temp_reg, ArrayStorageProperties::null_free_value);</span>
<span class="line-added">1532   cbnz(temp_reg, is_null_free_array);</span>
<span class="line-added">1533 }</span>
<span class="line-added">1534 </span>
1535 // MacroAssembler protected routines needed to implement
1536 // public methods
1537 
1538 void MacroAssembler::mov(Register r, Address dest) {
1539   code_section()-&gt;relocate(pc(), dest.rspec());
1540   u_int64_t imm64 = (u_int64_t)dest.target();
1541   movptr(r, imm64);
1542 }
1543 
1544 // Move a constant pointer into r.  In AArch64 mode the virtual
1545 // address space is 48 bits in size, so we only need three
1546 // instructions to create a patchable instruction sequence that can
1547 // reach anywhere.
1548 void MacroAssembler::movptr(Register r, uintptr_t imm64) {
1549 #ifndef PRODUCT
1550   {
1551     char buffer[64];
1552     snprintf(buffer, sizeof(buffer), &quot;0x%&quot; PRIX64, imm64);
1553     block_comment(buffer);
1554   }
</pre>
<hr />
<pre>
3718   ldr(rscratch1, Address(rscratch1, offset));
3719   cmp(src1, rscratch1);
3720 }
3721 
3722 void MacroAssembler::cmpoop(Register obj1, Register obj2) {
3723   BarrierSetAssembler* bs = BarrierSet::barrier_set()-&gt;barrier_set_assembler();
3724   bs-&gt;obj_equals(this, obj1, obj2);
3725 }
3726 
3727 void MacroAssembler::load_method_holder_cld(Register rresult, Register rmethod) {
3728   load_method_holder(rresult, rmethod);
3729   ldr(rresult, Address(rresult, InstanceKlass::class_loader_data_offset()));
3730 }
3731 
3732 void MacroAssembler::load_method_holder(Register holder, Register method) {
3733   ldr(holder, Address(method, Method::const_offset()));                      // ConstMethod*
3734   ldr(holder, Address(holder, ConstMethod::constants_offset()));             // ConstantPool*
3735   ldr(holder, Address(holder, ConstantPool::pool_holder_offset_in_bytes())); // InstanceKlass*
3736 }
3737 
<span class="line-modified">3738 void MacroAssembler::load_metadata(Register dst, Register src) {</span>
3739   if (UseCompressedClassPointers) {
3740     ldrw(dst, Address(src, oopDesc::klass_offset_in_bytes()));

3741   } else {
3742     ldr(dst, Address(src, oopDesc::klass_offset_in_bytes()));
3743   }
3744 }
3745 
<span class="line-added">3746 void MacroAssembler::load_klass(Register dst, Register src) {</span>
<span class="line-added">3747   load_metadata(dst, src);</span>
<span class="line-added">3748   if (UseCompressedClassPointers) {</span>
<span class="line-added">3749     andr(dst, dst, oopDesc::compressed_klass_mask());</span>
<span class="line-added">3750     decode_klass_not_null(dst);</span>
<span class="line-added">3751   } else {</span>
<span class="line-added">3752     ubfm(dst, dst, 0, 63 - oopDesc::storage_props_nof_bits);</span>
<span class="line-added">3753   }</span>
<span class="line-added">3754 }</span>
<span class="line-added">3755 </span>
3756 // ((OopHandle)result).resolve();
3757 void MacroAssembler::resolve_oop_handle(Register result, Register tmp) {
3758   // OopHandle::resolve is an indirection.
3759   access_load_at(T_OBJECT, IN_NATIVE, result, Address(result, 0), tmp, noreg);
3760 }
3761 
3762 // ((WeakHandle)result).resolve();
3763 void MacroAssembler::resolve_weak_handle(Register rresult, Register rtmp) {
3764   assert_different_registers(rresult, rtmp);
3765   Label resolved;
3766 
3767   // A null weak handle resolves to null.
3768   cbz(rresult, resolved);
3769 
3770   // Only 64 bit platforms support GCs that require a tmp register
3771   // Only IN_HEAP loads require a thread_tmp register
3772   // WeakHandle::resolve is an indirection like jweak.
3773   access_load_at(T_OBJECT, IN_NATIVE | ON_PHANTOM_OOP_REF,
3774                  rresult, Address(rresult), rtmp, /*tmp_thread*/noreg);
3775   bind(resolved);
3776 }
3777 
3778 void MacroAssembler::load_mirror(Register dst, Register method, Register tmp) {
3779   const int mirror_offset = in_bytes(Klass::java_mirror_offset());
3780   ldr(dst, Address(rmethod, Method::const_offset()));
3781   ldr(dst, Address(dst, ConstMethod::constants_offset()));
3782   ldr(dst, Address(dst, ConstantPool::pool_holder_offset_in_bytes()));
3783   ldr(dst, Address(dst, mirror_offset));
3784   resolve_oop_handle(dst, tmp);
3785 }
3786 
<span class="line-added">3787 void MacroAssembler::load_storage_props(Register dst, Register src) {</span>
<span class="line-added">3788   load_metadata(dst, src);</span>
<span class="line-added">3789   if (UseCompressedClassPointers) {</span>
<span class="line-added">3790     asrw(dst, dst, oopDesc::narrow_storage_props_shift);</span>
<span class="line-added">3791   } else {</span>
<span class="line-added">3792     asr(dst, dst, oopDesc::wide_storage_props_shift);</span>
<span class="line-added">3793   }</span>
<span class="line-added">3794 }</span>
<span class="line-added">3795 </span>
3796 void MacroAssembler::cmp_klass(Register oop, Register trial_klass, Register tmp) {
3797   if (UseCompressedClassPointers) {
3798     ldrw(tmp, Address(oop, oopDesc::klass_offset_in_bytes()));
3799     if (CompressedKlassPointers::base() == NULL) {
3800       cmp(trial_klass, tmp, LSL, CompressedKlassPointers::shift());
3801       return;
3802     } else if (((uint64_t)CompressedKlassPointers::base() &amp; 0xffffffff) == 0
3803                &amp;&amp; CompressedKlassPointers::shift() == 0) {
3804       // Only the bottom 32 bits matter
3805       cmpw(trial_klass, tmp);
3806       return;
3807     }
3808     decode_klass_not_null(tmp);
3809   } else {
3810     ldr(tmp, Address(oop, oopDesc::klass_offset_in_bytes()));
3811   }
3812   cmp(trial_klass, tmp);
3813 }
3814 
3815 void MacroAssembler::load_prototype_header(Register dst, Register src) {
</pre>
<hr />
<pre>
4113   narrowKlass nk = CompressedKlassPointers::encode(k);
4114   movz(dst, (nk &gt;&gt; 16), 16);
4115   movk(dst, nk &amp; 0xffff);
4116 }
4117 
4118 void MacroAssembler::access_load_at(BasicType type, DecoratorSet decorators,
4119                                     Register dst, Address src,
4120                                     Register tmp1, Register thread_tmp) {
4121   BarrierSetAssembler *bs = BarrierSet::barrier_set()-&gt;barrier_set_assembler();
4122   decorators = AccessInternal::decorator_fixup(decorators);
4123   bool as_raw = (decorators &amp; AS_RAW) != 0;
4124   if (as_raw) {
4125     bs-&gt;BarrierSetAssembler::load_at(this, decorators, type, dst, src, tmp1, thread_tmp);
4126   } else {
4127     bs-&gt;load_at(this, decorators, type, dst, src, tmp1, thread_tmp);
4128   }
4129 }
4130 
4131 void MacroAssembler::access_store_at(BasicType type, DecoratorSet decorators,
4132                                      Address dst, Register src,
<span class="line-modified">4133                                      Register tmp1, Register thread_tmp, Register tmp3) {</span>
<span class="line-added">4134 </span>
4135   BarrierSetAssembler *bs = BarrierSet::barrier_set()-&gt;barrier_set_assembler();
4136   decorators = AccessInternal::decorator_fixup(decorators);
4137   bool as_raw = (decorators &amp; AS_RAW) != 0;
4138   if (as_raw) {
<span class="line-modified">4139     bs-&gt;BarrierSetAssembler::store_at(this, decorators, type, dst, src, tmp1, thread_tmp, tmp3);</span>
4140   } else {
<span class="line-modified">4141     bs-&gt;store_at(this, decorators, type, dst, src, tmp1, thread_tmp, tmp3);</span>
4142   }
4143 }
4144 
4145 void MacroAssembler::resolve(DecoratorSet decorators, Register obj) {
4146   // Use stronger ACCESS_WRITE|ACCESS_READ by default.
4147   if ((decorators &amp; (ACCESS_READ | ACCESS_WRITE)) == 0) {
4148     decorators |= ACCESS_READ | ACCESS_WRITE;
4149   }
4150   BarrierSetAssembler* bs = BarrierSet::barrier_set()-&gt;barrier_set_assembler();
4151   return bs-&gt;resolve(this, decorators, obj);
4152 }
4153 
4154 void MacroAssembler::load_heap_oop(Register dst, Address src, Register tmp1,
4155                                    Register thread_tmp, DecoratorSet decorators) {
4156   access_load_at(T_OBJECT, IN_HEAP | decorators, dst, src, tmp1, thread_tmp);
4157 }
4158 
4159 void MacroAssembler::load_heap_oop_not_null(Register dst, Address src, Register tmp1,
4160                                             Register thread_tmp, DecoratorSet decorators) {
4161   access_load_at(T_OBJECT, IN_HEAP | IS_NOT_NULL | decorators, dst, src, tmp1, thread_tmp);
4162 }
4163 
4164 void MacroAssembler::store_heap_oop(Address dst, Register src, Register tmp1,
<span class="line-modified">4165                                     Register thread_tmp, Register tmp3, DecoratorSet decorators) {</span>
<span class="line-modified">4166   access_store_at(T_OBJECT, IN_HEAP | decorators, dst, src, tmp1, thread_tmp, tmp3);</span>
4167 }
4168 
4169 // Used for storing NULLs.
4170 void MacroAssembler::store_heap_oop_null(Address dst) {
<span class="line-modified">4171   access_store_at(T_OBJECT, IN_HEAP, dst, noreg, noreg, noreg, noreg);</span>
4172 }
4173 
4174 Address MacroAssembler::allocate_metadata_address(Metadata* obj) {
4175   assert(oop_recorder() != NULL, &quot;this assembler needs a Recorder&quot;);
4176   int index = oop_recorder()-&gt;allocate_metadata_index(obj);
4177   RelocationHolder rspec = metadata_Relocation::spec(index);
4178   return Address((address)obj, rspec);
4179 }
4180 
4181 // Move an oop into a register.  immediate is true if we want
4182 // immediate instructions and nmethod entry barriers are not enabled.
4183 // i.e. we are not going to patch this instruction while the code is being
4184 // executed by another thread.
4185 void MacroAssembler::movoop(Register dst, jobject obj, bool immediate) {
4186   int oop_index;
4187   if (obj == NULL) {
4188     oop_index = oop_recorder()-&gt;allocate_oop_index(obj);
4189   } else {
4190 #ifdef ASSERT
4191     {
</pre>
<hr />
<pre>
5224 // get_thread() can be called anywhere inside generated code so we
5225 // need to save whatever non-callee save context might get clobbered
5226 // by the call to JavaThread::aarch64_get_thread_helper() or, indeed,
5227 // the call setup code.
5228 //
5229 // aarch64_get_thread_helper() clobbers only r0, r1, and flags.
5230 //
5231 void MacroAssembler::get_thread(Register dst) {
5232   RegSet saved_regs = RegSet::range(r0, r1) + lr - dst;
5233   push(saved_regs, sp);
5234 
5235   mov(lr, CAST_FROM_FN_PTR(address, JavaThread::aarch64_get_thread_helper));
5236   blr(lr);
5237   if (dst != c_rarg0) {
5238     mov(dst, c_rarg0);
5239   }
5240 
5241   pop(saved_regs, sp);
5242 }
5243 
<span class="line-added">5244 // C2 compiled method&#39;s prolog code</span>
<span class="line-added">5245 // Moved here from aarch64.ad to support Valhalla code belows</span>
<span class="line-added">5246 void MacroAssembler::verified_entry(Compile* C, int sp_inc) {</span>
<span class="line-added">5247 </span>
<span class="line-added">5248 // n.b. frame size includes space for return pc and rfp</span>
<span class="line-added">5249   const long framesize = C-&gt;frame_size_in_bytes();</span>
<span class="line-added">5250   assert(framesize % (2 * wordSize) == 0, &quot;must preserve 2 * wordSize alignment&quot;);</span>
<span class="line-added">5251 </span>
<span class="line-added">5252   // insert a nop at the start of the prolog so we can patch in a</span>
<span class="line-added">5253   // branch if we need to invalidate the method later</span>
<span class="line-added">5254   nop();</span>
<span class="line-added">5255 </span>
<span class="line-added">5256   int bangsize = C-&gt;bang_size_in_bytes();</span>
<span class="line-added">5257   if (C-&gt;need_stack_bang(bangsize) &amp;&amp; UseStackBanging)</span>
<span class="line-added">5258      generate_stack_overflow_check(bangsize);</span>
<span class="line-added">5259 </span>
<span class="line-added">5260   build_frame(framesize);</span>
<span class="line-added">5261 </span>
<span class="line-added">5262   if (VerifyStackAtCalls) {</span>
<span class="line-added">5263     Unimplemented();</span>
<span class="line-added">5264   }</span>
<span class="line-added">5265 }</span>
<span class="line-added">5266 </span>
<span class="line-added">5267 int MacroAssembler::store_value_type_fields_to_buf(ciValueKlass* vk, bool from_interpreter) {</span>
<span class="line-added">5268   // A value type might be returned. If fields are in registers we</span>
<span class="line-added">5269   // need to allocate a value type instance and initialize it with</span>
<span class="line-added">5270   // the value of the fields.</span>
<span class="line-added">5271   Label skip;</span>
<span class="line-added">5272   // We only need a new buffered value if a new one is not returned</span>
<span class="line-added">5273   cmp(r0, (u1) 1);</span>
<span class="line-added">5274   br(Assembler::EQ, skip);</span>
<span class="line-added">5275   int call_offset = -1;</span>
<span class="line-added">5276 </span>
<span class="line-added">5277   Label slow_case;</span>
<span class="line-added">5278 </span>
<span class="line-added">5279   // Try to allocate a new buffered value (from the heap)</span>
<span class="line-added">5280   if (UseTLAB) {</span>
<span class="line-added">5281 </span>
<span class="line-added">5282     if (vk != NULL) {</span>
<span class="line-added">5283       // Called from C1, where the return type is statically known.</span>
<span class="line-added">5284       mov(r1, (intptr_t)vk-&gt;get_ValueKlass());</span>
<span class="line-added">5285       jint lh = vk-&gt;layout_helper();</span>
<span class="line-added">5286       assert(lh != Klass::_lh_neutral_value, &quot;inline class in return type must have been resolved&quot;);</span>
<span class="line-added">5287       mov(r14, lh);</span>
<span class="line-added">5288     } else {</span>
<span class="line-added">5289        // Call from interpreter. R0 contains ((the ValueKlass* of the return type) | 0x01)</span>
<span class="line-added">5290        andr(r1, r0, -2);</span>
<span class="line-added">5291        // get obj size</span>
<span class="line-added">5292        ldrw(r14, Address(rscratch1 /*klass*/, Klass::layout_helper_offset()));</span>
<span class="line-added">5293     }</span>
<span class="line-added">5294 </span>
<span class="line-added">5295      ldr(r13, Address(rthread, in_bytes(JavaThread::tlab_top_offset())));</span>
<span class="line-added">5296 </span>
<span class="line-added">5297      // check whether we have space in TLAB,</span>
<span class="line-added">5298      // rscratch1 contains pointer to just allocated obj</span>
<span class="line-added">5299       lea(r14, Address(r13, r14));</span>
<span class="line-added">5300       ldr(rscratch1, Address(rthread, in_bytes(JavaThread::tlab_end_offset())));</span>
<span class="line-added">5301 </span>
<span class="line-added">5302       cmp(r14, rscratch1);</span>
<span class="line-added">5303       br(Assembler::GT, slow_case);</span>
<span class="line-added">5304 </span>
<span class="line-added">5305       // OK we have room in TLAB,</span>
<span class="line-added">5306       // Set new TLAB top</span>
<span class="line-added">5307       str(r14, Address(rthread, in_bytes(JavaThread::tlab_top_offset())));</span>
<span class="line-added">5308 </span>
<span class="line-added">5309       // Set new class always locked</span>
<span class="line-added">5310       mov(rscratch1, (uint64_t) markWord::always_locked_prototype().value());</span>
<span class="line-added">5311       str(rscratch1, Address(r13, oopDesc::mark_offset_in_bytes()));</span>
<span class="line-added">5312 </span>
<span class="line-added">5313       store_klass_gap(r13, zr);  // zero klass gap for compressed oops</span>
<span class="line-added">5314       if (vk == NULL) {</span>
<span class="line-added">5315         // store_klass corrupts rbx, so save it in rax for later use (interpreter case only).</span>
<span class="line-added">5316          mov(r0, r1);</span>
<span class="line-added">5317       }</span>
<span class="line-added">5318 </span>
<span class="line-added">5319       store_klass(r13, r1);  // klass</span>
<span class="line-added">5320 </span>
<span class="line-added">5321       if (vk != NULL) {</span>
<span class="line-added">5322         // FIXME -- do the packing in-line to avoid the runtime call</span>
<span class="line-added">5323         mov(r0, r13);</span>
<span class="line-added">5324         far_call(RuntimeAddress(vk-&gt;pack_handler())); // no need for call info as this will not safepoint.</span>
<span class="line-added">5325       } else {</span>
<span class="line-added">5326 </span>
<span class="line-added">5327         // We have our new buffered value, initialize its fields with a</span>
<span class="line-added">5328         // value class specific handler</span>
<span class="line-added">5329         ldr(r1, Address(r0, InstanceKlass::adr_valueklass_fixed_block_offset()));</span>
<span class="line-added">5330         ldr(r1, Address(r1, ValueKlass::pack_handler_offset()));</span>
<span class="line-added">5331 </span>
<span class="line-added">5332         // Mov new class to r0 and call pack_handler</span>
<span class="line-added">5333         mov(r0, r13);</span>
<span class="line-added">5334         blr(r1);</span>
<span class="line-added">5335       }</span>
<span class="line-added">5336       b(skip);</span>
<span class="line-added">5337   }</span>
<span class="line-added">5338 </span>
<span class="line-added">5339   bind(slow_case);</span>
<span class="line-added">5340   // We failed to allocate a new value, fall back to a runtime</span>
<span class="line-added">5341   // call. Some oop field may be live in some registers but we can&#39;t</span>
<span class="line-added">5342   // tell. That runtime call will take care of preserving them</span>
<span class="line-added">5343   // across a GC if there&#39;s one.</span>
<span class="line-added">5344 </span>
<span class="line-added">5345 </span>
<span class="line-added">5346   if (from_interpreter) {</span>
<span class="line-added">5347     super_call_VM_leaf(StubRoutines::store_value_type_fields_to_buf());</span>
<span class="line-added">5348   } else {</span>
<span class="line-added">5349     ldr(rscratch1, RuntimeAddress(StubRoutines::store_value_type_fields_to_buf()));</span>
<span class="line-added">5350     blr(rscratch1);</span>
<span class="line-added">5351     call_offset = offset();</span>
<span class="line-added">5352   }</span>
<span class="line-added">5353 </span>
<span class="line-added">5354   bind(skip);</span>
<span class="line-added">5355   return call_offset;</span>
<span class="line-added">5356 }</span>
<span class="line-added">5357 </span>
<span class="line-added">5358 // Move a value between registers/stack slots and update the reg_state</span>
<span class="line-added">5359 bool MacroAssembler::move_helper(VMReg from, VMReg to, BasicType bt, RegState reg_state[], int ret_off, int extra_stack_offset) {</span>
<span class="line-added">5360   if (reg_state[to-&gt;value()] == reg_written) {</span>
<span class="line-added">5361     return true; // Already written</span>
<span class="line-added">5362   }</span>
<span class="line-added">5363 </span>
<span class="line-added">5364   if (from != to &amp;&amp; bt != T_VOID) {</span>
<span class="line-added">5365     if (reg_state[to-&gt;value()] == reg_readonly) {</span>
<span class="line-added">5366       return false; // Not yet writable</span>
<span class="line-added">5367     }</span>
<span class="line-added">5368     if (from-&gt;is_reg()) {</span>
<span class="line-added">5369       if (to-&gt;is_reg()) {</span>
<span class="line-added">5370         mov(to-&gt;as_Register(), from-&gt;as_Register());</span>
<span class="line-added">5371       } else {</span>
<span class="line-added">5372         int st_off = to-&gt;reg2stack() * VMRegImpl::stack_slot_size + extra_stack_offset;</span>
<span class="line-added">5373         Address to_addr = Address(sp, st_off);</span>
<span class="line-added">5374         if (from-&gt;is_FloatRegister()) {</span>
<span class="line-added">5375           if (bt == T_DOUBLE) {</span>
<span class="line-added">5376              strd(from-&gt;as_FloatRegister(), to_addr);</span>
<span class="line-added">5377           } else {</span>
<span class="line-added">5378              assert(bt == T_FLOAT, &quot;must be float&quot;);</span>
<span class="line-added">5379              strs(from-&gt;as_FloatRegister(), to_addr);</span>
<span class="line-added">5380           }</span>
<span class="line-added">5381         } else {</span>
<span class="line-added">5382           str(from-&gt;as_Register(), to_addr);</span>
<span class="line-added">5383         }</span>
<span class="line-added">5384       }</span>
<span class="line-added">5385     } else {</span>
<span class="line-added">5386       Address from_addr = Address(sp, from-&gt;reg2stack() * VMRegImpl::stack_slot_size + extra_stack_offset);</span>
<span class="line-added">5387       if (to-&gt;is_reg()) {</span>
<span class="line-added">5388         if (to-&gt;is_FloatRegister()) {</span>
<span class="line-added">5389           if (bt == T_DOUBLE) {</span>
<span class="line-added">5390              ldrd(to-&gt;as_FloatRegister(), from_addr);</span>
<span class="line-added">5391           } else {</span>
<span class="line-added">5392             assert(bt == T_FLOAT, &quot;must be float&quot;);</span>
<span class="line-added">5393             ldrs(to-&gt;as_FloatRegister(), from_addr);</span>
<span class="line-added">5394           }</span>
<span class="line-added">5395         } else {</span>
<span class="line-added">5396           ldr(to-&gt;as_Register(), from_addr);</span>
<span class="line-added">5397         }</span>
<span class="line-added">5398       } else {</span>
<span class="line-added">5399         int st_off = to-&gt;reg2stack() * VMRegImpl::stack_slot_size + extra_stack_offset;</span>
<span class="line-added">5400         ldr(rscratch1, from_addr);</span>
<span class="line-added">5401         str(rscratch1, Address(sp, st_off));</span>
<span class="line-added">5402       }</span>
<span class="line-added">5403     }</span>
<span class="line-added">5404   }</span>
<span class="line-added">5405 </span>
<span class="line-added">5406   // Update register states</span>
<span class="line-added">5407   reg_state[from-&gt;value()] = reg_writable;</span>
<span class="line-added">5408   reg_state[to-&gt;value()] = reg_written;</span>
<span class="line-added">5409   return true;</span>
<span class="line-added">5410 }</span>
<span class="line-added">5411 </span>
<span class="line-added">5412 // Read all fields from a value type oop and store the values in registers/stack slots</span>
<span class="line-added">5413 bool MacroAssembler::unpack_value_helper(const GrowableArray&lt;SigEntry&gt;* sig, int&amp; sig_index, VMReg from, VMRegPair* regs_to,</span>
<span class="line-added">5414                                          int&amp; to_index, RegState reg_state[], int ret_off, int extra_stack_offset) {</span>
<span class="line-added">5415   Register fromReg = from-&gt;is_reg() ? from-&gt;as_Register() : noreg;</span>
<span class="line-added">5416   assert(sig-&gt;at(sig_index)._bt == T_VOID, &quot;should be at end delimiter&quot;);</span>
<span class="line-added">5417 </span>
<span class="line-added">5418 </span>
<span class="line-added">5419   int vt = 1;</span>
<span class="line-added">5420   bool done = true;</span>
<span class="line-added">5421   bool mark_done = true;</span>
<span class="line-added">5422   do {</span>
<span class="line-added">5423     sig_index--;</span>
<span class="line-added">5424     BasicType bt = sig-&gt;at(sig_index)._bt;</span>
<span class="line-added">5425     if (bt == T_VALUETYPE) {</span>
<span class="line-added">5426       vt--;</span>
<span class="line-added">5427     } else if (bt == T_VOID &amp;&amp;</span>
<span class="line-added">5428                sig-&gt;at(sig_index-1)._bt != T_LONG &amp;&amp;</span>
<span class="line-added">5429                sig-&gt;at(sig_index-1)._bt != T_DOUBLE) {</span>
<span class="line-added">5430       vt++;</span>
<span class="line-added">5431     } else if (SigEntry::is_reserved_entry(sig, sig_index)) {</span>
<span class="line-added">5432       to_index--; // Ignore this</span>
<span class="line-added">5433     } else {</span>
<span class="line-added">5434       assert(to_index &gt;= 0, &quot;invalid to_index&quot;);</span>
<span class="line-added">5435       VMRegPair pair_to = regs_to[to_index--];</span>
<span class="line-added">5436       VMReg to = pair_to.first();</span>
<span class="line-added">5437 </span>
<span class="line-added">5438       if (bt == T_VOID) continue;</span>
<span class="line-added">5439 </span>
<span class="line-added">5440       int idx = (int) to-&gt;value();</span>
<span class="line-added">5441       if (reg_state[idx] == reg_readonly) {</span>
<span class="line-added">5442          if (idx != from-&gt;value()) {</span>
<span class="line-added">5443            mark_done = false;</span>
<span class="line-added">5444          }</span>
<span class="line-added">5445          done = false;</span>
<span class="line-added">5446          continue;</span>
<span class="line-added">5447       } else if (reg_state[idx] == reg_written) {</span>
<span class="line-added">5448         continue;</span>
<span class="line-added">5449       } else {</span>
<span class="line-added">5450         assert(reg_state[idx] == reg_writable, &quot;must be writable&quot;);</span>
<span class="line-added">5451         reg_state[idx] = reg_written;</span>
<span class="line-added">5452       }</span>
<span class="line-added">5453 </span>
<span class="line-added">5454       if (fromReg == noreg) {</span>
<span class="line-added">5455         int st_off = from-&gt;reg2stack() * VMRegImpl::stack_slot_size + extra_stack_offset;</span>
<span class="line-added">5456         ldr(rscratch2, Address(sp, st_off));</span>
<span class="line-added">5457         fromReg = rscratch2;</span>
<span class="line-added">5458       }</span>
<span class="line-added">5459 </span>
<span class="line-added">5460       int off = sig-&gt;at(sig_index)._offset;</span>
<span class="line-added">5461       assert(off &gt; 0, &quot;offset in object should be positive&quot;);</span>
<span class="line-added">5462       bool is_oop = (bt == T_OBJECT || bt == T_ARRAY);</span>
<span class="line-added">5463 </span>
<span class="line-added">5464       Address fromAddr = Address(fromReg, off);</span>
<span class="line-added">5465       bool is_signed = (bt != T_CHAR) &amp;&amp; (bt != T_BOOLEAN);</span>
<span class="line-added">5466 </span>
<span class="line-added">5467       if (!to-&gt;is_FloatRegister()) {</span>
<span class="line-added">5468 </span>
<span class="line-added">5469         Register dst = to-&gt;is_stack() ? rscratch1 : to-&gt;as_Register();</span>
<span class="line-added">5470 </span>
<span class="line-added">5471         if (is_oop) {</span>
<span class="line-added">5472           load_heap_oop(dst, fromAddr);</span>
<span class="line-added">5473         } else {</span>
<span class="line-added">5474           load_sized_value(dst, fromAddr, type2aelembytes(bt), is_signed);</span>
<span class="line-added">5475         }</span>
<span class="line-added">5476         if (to-&gt;is_stack()) {</span>
<span class="line-added">5477           int st_off = to-&gt;reg2stack() * VMRegImpl::stack_slot_size + extra_stack_offset;</span>
<span class="line-added">5478           str(dst, Address(sp, st_off));</span>
<span class="line-added">5479         }</span>
<span class="line-added">5480       } else {</span>
<span class="line-added">5481         if (bt == T_DOUBLE) {</span>
<span class="line-added">5482           ldrd(to-&gt;as_FloatRegister(), fromAddr);</span>
<span class="line-added">5483         } else {</span>
<span class="line-added">5484           assert(bt == T_FLOAT, &quot;must be float&quot;);</span>
<span class="line-added">5485           ldrs(to-&gt;as_FloatRegister(), fromAddr);</span>
<span class="line-added">5486         }</span>
<span class="line-added">5487      }</span>
<span class="line-added">5488 </span>
<span class="line-added">5489     }</span>
<span class="line-added">5490 </span>
<span class="line-added">5491   } while (vt != 0);</span>
<span class="line-added">5492 </span>
<span class="line-added">5493   if (mark_done &amp;&amp; reg_state[from-&gt;value()] != reg_written) {</span>
<span class="line-added">5494     // This is okay because no one else will write to that slot</span>
<span class="line-added">5495     reg_state[from-&gt;value()] = reg_writable;</span>
<span class="line-added">5496   }</span>
<span class="line-added">5497   return done;</span>
<span class="line-added">5498 }</span>
<span class="line-added">5499 </span>
<span class="line-added">5500 // Pack fields back into a value type oop</span>
<span class="line-added">5501 bool MacroAssembler::pack_value_helper(const GrowableArray&lt;SigEntry&gt;* sig, int&amp; sig_index, int vtarg_index,</span>
<span class="line-added">5502                                        VMReg to, VMRegPair* regs_from, int regs_from_count, int&amp; from_index, RegState reg_state[],</span>
<span class="line-added">5503                                        int ret_off, int extra_stack_offset) {</span>
<span class="line-added">5504   assert(sig-&gt;at(sig_index)._bt == T_VALUETYPE, &quot;should be at end delimiter&quot;);</span>
<span class="line-added">5505   assert(to-&gt;is_valid(), &quot;must be&quot;);</span>
<span class="line-added">5506 </span>
<span class="line-added">5507   if (reg_state[to-&gt;value()] == reg_written) {</span>
<span class="line-added">5508     skip_unpacked_fields(sig, sig_index, regs_from, regs_from_count, from_index);</span>
<span class="line-added">5509     return true; // Already written</span>
<span class="line-added">5510   }</span>
<span class="line-added">5511 </span>
<span class="line-added">5512   Register val_array = r0;</span>
<span class="line-added">5513   Register val_obj_tmp = r11;</span>
<span class="line-added">5514   Register from_reg_tmp = r10;</span>
<span class="line-added">5515   Register tmp1 = r14;</span>
<span class="line-added">5516   Register tmp2 = r13;</span>
<span class="line-added">5517   Register tmp3 = r1;</span>
<span class="line-added">5518   Register val_obj = to-&gt;is_stack() ? val_obj_tmp : to-&gt;as_Register();</span>
<span class="line-added">5519 </span>
<span class="line-added">5520   if (reg_state[to-&gt;value()] == reg_readonly) {</span>
<span class="line-added">5521     if (!is_reg_in_unpacked_fields(sig, sig_index, to, regs_from, regs_from_count, from_index)) {</span>
<span class="line-added">5522       skip_unpacked_fields(sig, sig_index, regs_from, regs_from_count, from_index);</span>
<span class="line-added">5523       return false; // Not yet writable</span>
<span class="line-added">5524     }</span>
<span class="line-added">5525     val_obj = val_obj_tmp;</span>
<span class="line-added">5526   }</span>
<span class="line-added">5527 </span>
<span class="line-added">5528   int index = arrayOopDesc::base_offset_in_bytes(T_OBJECT) + vtarg_index * type2aelembytes(T_VALUETYPE);</span>
<span class="line-added">5529   load_heap_oop(val_obj, Address(val_array, index));</span>
<span class="line-added">5530 </span>
<span class="line-added">5531   ScalarizedValueArgsStream stream(sig, sig_index, regs_from, regs_from_count, from_index);</span>
<span class="line-added">5532   VMRegPair from_pair;</span>
<span class="line-added">5533   BasicType bt;</span>
<span class="line-added">5534 </span>
<span class="line-added">5535   while (stream.next(from_pair, bt)) {</span>
<span class="line-added">5536     int off = sig-&gt;at(stream.sig_cc_index())._offset;</span>
<span class="line-added">5537     assert(off &gt; 0, &quot;offset in object should be positive&quot;);</span>
<span class="line-added">5538     bool is_oop = (bt == T_OBJECT || bt == T_ARRAY);</span>
<span class="line-added">5539     size_t size_in_bytes = is_java_primitive(bt) ? type2aelembytes(bt) : wordSize;</span>
<span class="line-added">5540 </span>
<span class="line-added">5541     VMReg from_r1 = from_pair.first();</span>
<span class="line-added">5542     VMReg from_r2 = from_pair.second();</span>
<span class="line-added">5543 </span>
<span class="line-added">5544     // Pack the scalarized field into the value object.</span>
<span class="line-added">5545     Address dst(val_obj, off);</span>
<span class="line-added">5546 </span>
<span class="line-added">5547     if (!from_r1-&gt;is_FloatRegister()) {</span>
<span class="line-added">5548       Register from_reg;</span>
<span class="line-added">5549       if (from_r1-&gt;is_stack()) {</span>
<span class="line-added">5550         from_reg = from_reg_tmp;</span>
<span class="line-added">5551         int ld_off = from_r1-&gt;reg2stack() * VMRegImpl::stack_slot_size + extra_stack_offset;</span>
<span class="line-added">5552         load_sized_value(from_reg, Address(sp, ld_off), size_in_bytes, /* is_signed */ false);</span>
<span class="line-added">5553       } else {</span>
<span class="line-added">5554         from_reg = from_r1-&gt;as_Register();</span>
<span class="line-added">5555       }</span>
<span class="line-added">5556 </span>
<span class="line-added">5557       if (is_oop) {</span>
<span class="line-added">5558         DecoratorSet decorators = IN_HEAP | ACCESS_WRITE;</span>
<span class="line-added">5559         store_heap_oop(dst, from_reg, tmp1, tmp2, tmp3, decorators);</span>
<span class="line-added">5560       } else {</span>
<span class="line-added">5561         store_sized_value(dst, from_reg, size_in_bytes);</span>
<span class="line-added">5562       }</span>
<span class="line-added">5563     } else {</span>
<span class="line-added">5564       if (from_r2-&gt;is_valid()) {</span>
<span class="line-added">5565         strd(from_r1-&gt;as_FloatRegister(), dst);</span>
<span class="line-added">5566       } else {</span>
<span class="line-added">5567         strs(from_r1-&gt;as_FloatRegister(), dst);</span>
<span class="line-added">5568       }</span>
<span class="line-added">5569     }</span>
<span class="line-added">5570 </span>
<span class="line-added">5571     reg_state[from_r1-&gt;value()] = reg_writable;</span>
<span class="line-added">5572   }</span>
<span class="line-added">5573   sig_index = stream.sig_cc_index();</span>
<span class="line-added">5574   from_index = stream.regs_cc_index();</span>
<span class="line-added">5575 </span>
<span class="line-added">5576   assert(reg_state[to-&gt;value()] == reg_writable, &quot;must have already been read&quot;);</span>
<span class="line-added">5577   bool success = move_helper(val_obj-&gt;as_VMReg(), to, T_OBJECT, reg_state, ret_off, extra_stack_offset);</span>
<span class="line-added">5578   assert(success, &quot;to register must be writeable&quot;);</span>
<span class="line-added">5579 </span>
<span class="line-added">5580   return true;</span>
<span class="line-added">5581 }</span>
<span class="line-added">5582 </span>
<span class="line-added">5583 // Unpack all value type arguments passed as oops</span>
<span class="line-added">5584 void MacroAssembler::unpack_value_args(Compile* C, bool receiver_only) {</span>
<span class="line-added">5585   int sp_inc = unpack_value_args_common(C, receiver_only);</span>
<span class="line-added">5586   // Emit code for verified entry and save increment for stack repair on return</span>
<span class="line-added">5587   verified_entry(C, sp_inc);</span>
<span class="line-added">5588 }</span>
<span class="line-added">5589 </span>
<span class="line-added">5590 int MacroAssembler::shuffle_value_args(bool is_packing, bool receiver_only, int extra_stack_offset,</span>
<span class="line-added">5591                                        BasicType* sig_bt, const GrowableArray&lt;SigEntry&gt;* sig_cc,</span>
<span class="line-added">5592                                        int args_passed, int args_on_stack, VMRegPair* regs,            // from</span>
<span class="line-added">5593                                        int args_passed_to, int args_on_stack_to, VMRegPair* regs_to) { // to</span>
<span class="line-added">5594   // Check if we need to extend the stack for packing/unpacking</span>
<span class="line-added">5595   int sp_inc = (args_on_stack_to - args_on_stack) * VMRegImpl::stack_slot_size;</span>
<span class="line-added">5596   if (sp_inc &gt; 0) {</span>
<span class="line-added">5597     sp_inc = align_up(sp_inc, StackAlignmentInBytes);</span>
<span class="line-added">5598     if (!is_packing) {</span>
<span class="line-added">5599       // Save the return address, adjust the stack (make sure it is properly</span>
<span class="line-added">5600       // 16-byte aligned) and copy the return address to the new top of the stack.</span>
<span class="line-added">5601       // (Note: C1 does this in C1_MacroAssembler::scalarized_entry).</span>
<span class="line-added">5602       // FIXME: We need not to preserve return address on aarch64</span>
<span class="line-added">5603       pop(rscratch1);</span>
<span class="line-added">5604       sub(sp, sp, sp_inc);</span>
<span class="line-added">5605       push(rscratch1);</span>
<span class="line-added">5606     }</span>
<span class="line-added">5607   } else {</span>
<span class="line-added">5608     // The scalarized calling convention needs less stack space than the unscalarized one.</span>
<span class="line-added">5609     // No need to extend the stack, the caller will take care of these adjustments.</span>
<span class="line-added">5610     sp_inc = 0;</span>
<span class="line-added">5611   }</span>
<span class="line-added">5612 </span>
<span class="line-added">5613   int ret_off; // make sure we don&#39;t overwrite the return address</span>
<span class="line-added">5614   if (is_packing) {</span>
<span class="line-added">5615     // For C1 code, the VVEP doesn&#39;t have reserved slots, so we store the returned address at</span>
<span class="line-added">5616     // rsp[0] during shuffling.</span>
<span class="line-added">5617     ret_off = 0;</span>
<span class="line-added">5618   } else {</span>
<span class="line-added">5619     // C2 code ensures that sp_inc is a reserved slot.</span>
<span class="line-added">5620     ret_off = sp_inc;</span>
<span class="line-added">5621   }</span>
<span class="line-added">5622 </span>
<span class="line-added">5623   return shuffle_value_args_common(is_packing, receiver_only, extra_stack_offset,</span>
<span class="line-added">5624                                    sig_bt, sig_cc,</span>
<span class="line-added">5625                                    args_passed, args_on_stack, regs,</span>
<span class="line-added">5626                                    args_passed_to, args_on_stack_to, regs_to,</span>
<span class="line-added">5627                                    sp_inc, ret_off);</span>
<span class="line-added">5628 }</span>
<span class="line-added">5629 </span>
<span class="line-added">5630 VMReg MacroAssembler::spill_reg_for(VMReg reg) {</span>
<span class="line-added">5631   return (reg-&gt;is_FloatRegister()) ? v0-&gt;as_VMReg() : r14-&gt;as_VMReg();</span>
<span class="line-added">5632 }</span>
<span class="line-added">5633 </span>
5634 void MacroAssembler::cache_wb(Address line) {
5635   assert(line.getMode() == Address::base_plus_offset, &quot;mode should be base_plus_offset&quot;);
5636   assert(line.index() == noreg, &quot;index should be noreg&quot;);
5637   assert(line.offset() == 0, &quot;offset should be 0&quot;);
5638   // would like to assert this
5639   // assert(line._ext.shift == 0, &quot;shift should be zero&quot;);
5640   if (VM_Version::supports_dcpop()) {
5641     // writeback using clear virtual address to point of persistence
5642     dc(Assembler::CVAP, line.base());
5643   } else {
5644     // no need to generate anything as Unsafe.writebackMemory should
5645     // never invoke this stub
5646   }
5647 }
5648 
5649 void MacroAssembler::cache_wbsync(bool is_pre) {
5650   // we only need a barrier post sync
5651   if (!is_pre) {
5652     membar(Assembler::AnyAny);
5653   }
</pre>
</td>
</tr>
</table>
<center><a href="c1_LIRGenerator_aarch64.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="macroAssembler_aarch64.hpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>