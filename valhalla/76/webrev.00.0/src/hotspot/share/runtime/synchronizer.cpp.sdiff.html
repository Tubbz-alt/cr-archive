<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/share/runtime/synchronizer.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="sharedRuntime.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="synchronizer.hpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/share/runtime/synchronizer.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #include &quot;precompiled.hpp&quot;
  26 #include &quot;classfile/vmSymbols.hpp&quot;
  27 #include &quot;logging/log.hpp&quot;
  28 #include &quot;logging/logStream.hpp&quot;
  29 #include &quot;jfr/jfrEvents.hpp&quot;
  30 #include &quot;memory/allocation.inline.hpp&quot;
  31 #include &quot;memory/metaspaceShared.hpp&quot;
  32 #include &quot;memory/padded.hpp&quot;
  33 #include &quot;memory/resourceArea.hpp&quot;
  34 #include &quot;memory/universe.hpp&quot;
  35 #include &quot;oops/markWord.hpp&quot;
  36 #include &quot;oops/oop.inline.hpp&quot;
  37 #include &quot;runtime/atomic.hpp&quot;
  38 #include &quot;runtime/biasedLocking.hpp&quot;
  39 #include &quot;runtime/handles.inline.hpp&quot;

  40 #include &quot;runtime/interfaceSupport.inline.hpp&quot;
  41 #include &quot;runtime/mutexLocker.hpp&quot;
  42 #include &quot;runtime/objectMonitor.hpp&quot;
  43 #include &quot;runtime/objectMonitor.inline.hpp&quot;
  44 #include &quot;runtime/osThread.hpp&quot;

  45 #include &quot;runtime/safepointVerifiers.hpp&quot;
  46 #include &quot;runtime/sharedRuntime.hpp&quot;
  47 #include &quot;runtime/stubRoutines.hpp&quot;
  48 #include &quot;runtime/synchronizer.hpp&quot;
  49 #include &quot;runtime/thread.inline.hpp&quot;
  50 #include &quot;runtime/timer.hpp&quot;
  51 #include &quot;runtime/vframe.hpp&quot;
  52 #include &quot;runtime/vmThread.hpp&quot;
  53 #include &quot;utilities/align.hpp&quot;
  54 #include &quot;utilities/dtrace.hpp&quot;
  55 #include &quot;utilities/events.hpp&quot;
  56 #include &quot;utilities/preserveException.hpp&quot;
  57 
  58 // The &quot;core&quot; versions of monitor enter and exit reside in this file.
  59 // The interpreter and compilers contain specialized transliterated
  60 // variants of the enter-exit fast-path operations.  See i486.ad fast_lock(),
  61 // for instance.  If you make changes here, make sure to modify the
  62 // interpreter, and both C1 and C2 fast-path inline locking code emission.
  63 //
  64 // -----------------------------------------------------------------------------
</pre>
<hr />
<pre>
 101   }
 102 
 103 #else //  ndef DTRACE_ENABLED
 104 
 105 #define DTRACE_MONITOR_WAIT_PROBE(obj, thread, millis, mon)    {;}
 106 #define DTRACE_MONITOR_PROBE(probe, obj, thread, mon)          {;}
 107 
 108 #endif // ndef DTRACE_ENABLED
 109 
 110 // This exists only as a workaround of dtrace bug 6254741
 111 int dtrace_waited_probe(ObjectMonitor* monitor, Handle obj, Thread* thr) {
 112   DTRACE_MONITOR_PROBE(waited, monitor, obj(), thr);
 113   return 0;
 114 }
 115 
 116 #define NINFLATIONLOCKS 256
 117 static volatile intptr_t gInflationLocks[NINFLATIONLOCKS];
 118 
 119 // global list of blocks of monitors
 120 PaddedObjectMonitor* ObjectSynchronizer::g_block_list = NULL;



 121 
 122 struct ObjectMonitorListGlobals {
 123   char         _pad_prefix[OM_CACHE_LINE_SIZE];
 124   // These are highly shared list related variables.
 125   // To avoid false-sharing they need to be the sole occupants of a cache line.
 126 
 127   // Global ObjectMonitor free list. Newly allocated and deflated
 128   // ObjectMonitors are prepended here.
 129   ObjectMonitor* _free_list;
 130   DEFINE_PAD_MINUS_SIZE(1, OM_CACHE_LINE_SIZE, sizeof(ObjectMonitor*));
 131 
 132   // Global ObjectMonitor in-use list. When a JavaThread is exiting,
 133   // ObjectMonitors on its per-thread in-use list are prepended here.
 134   ObjectMonitor* _in_use_list;
 135   DEFINE_PAD_MINUS_SIZE(2, OM_CACHE_LINE_SIZE, sizeof(ObjectMonitor*));
 136 







 137   int _free_count;    // # on free_list
<span class="line-modified"> 138   DEFINE_PAD_MINUS_SIZE(3, OM_CACHE_LINE_SIZE, sizeof(int));</span>
 139 
 140   int _in_use_count;  // # on in_use_list
<span class="line-modified"> 141   DEFINE_PAD_MINUS_SIZE(4, OM_CACHE_LINE_SIZE, sizeof(int));</span>
 142 
 143   int _population;    // # Extant -- in circulation
<span class="line-modified"> 144   DEFINE_PAD_MINUS_SIZE(5, OM_CACHE_LINE_SIZE, sizeof(int));</span>



 145 };
 146 static ObjectMonitorListGlobals om_list_globals;
 147 
 148 #define CHECK_THROW_NOSYNC_IMSE(obj)  \
 149   if ((obj)-&gt;mark().is_always_locked()) {  \
 150     ResourceMark rm(THREAD);                \
 151     THROW_MSG(vmSymbols::java_lang_IllegalMonitorStateException(), obj-&gt;klass()-&gt;external_name()); \
 152   }
 153 
 154 #define CHECK_THROW_NOSYNC_IMSE_0(obj)  \
 155     if ((obj)-&gt;mark().is_always_locked()) {  \
 156     ResourceMark rm(THREAD);                  \
 157     THROW_MSG_0(vmSymbols::java_lang_IllegalMonitorStateException(), obj-&gt;klass()-&gt;external_name()); \
 158   }
 159 
 160 
 161 #define CHAINMARKER (cast_to_oop&lt;intptr_t&gt;(-1))
 162 
 163 
 164 // =====================&gt; Spin-lock functions
</pre>
<hr />
<pre>
 295       Atomic::add(&amp;om_list_globals._population, _BLOCKSIZE - 1);
 296       break;
 297     }
 298     // Implied else: try it all again
 299   }
 300 
 301   // Second we handle om_list_globals._free_list:
 302   prepend_list_to_common(new_blk + 1, &amp;new_blk[_BLOCKSIZE - 1], _BLOCKSIZE - 1,
 303                          &amp;om_list_globals._free_list, &amp;om_list_globals._free_count);
 304 }
 305 
 306 // Prepend a list of ObjectMonitors to om_list_globals._free_list.
 307 // &#39;tail&#39; is the last ObjectMonitor in the list and there are &#39;count&#39;
 308 // on the list. Also updates om_list_globals._free_count.
 309 static void prepend_list_to_global_free_list(ObjectMonitor* list,
 310                                              ObjectMonitor* tail, int count) {
 311   prepend_list_to_common(list, tail, count, &amp;om_list_globals._free_list,
 312                          &amp;om_list_globals._free_count);
 313 }
 314 









 315 // Prepend a list of ObjectMonitors to om_list_globals._in_use_list.
 316 // &#39;tail&#39; is the last ObjectMonitor in the list and there are &#39;count&#39;
 317 // on the list. Also updates om_list_globals._in_use_list.
 318 static void prepend_list_to_global_in_use_list(ObjectMonitor* list,
 319                                                ObjectMonitor* tail, int count) {
 320   prepend_list_to_common(list, tail, count, &amp;om_list_globals._in_use_list,
 321                          &amp;om_list_globals._in_use_count);
 322 }
 323 
 324 // Prepend an ObjectMonitor to the specified list. Also updates
 325 // the specified counter.
 326 static void prepend_to_common(ObjectMonitor* m, ObjectMonitor** list_p,
 327                               int* count_p) {
 328   while (true) {
 329     om_lock(m);  // Lock m so we can safely update its next field.
 330     ObjectMonitor* cur = NULL;
 331     // Lock the list head to guard against races with a list walker
<span class="line-modified"> 332     // thread:</span>
 333     if ((cur = get_list_head_locked(list_p)) != NULL) {
 334       // List head is now locked so we can safely switch it.
 335       m-&gt;set_next_om(cur);  // m now points to cur (and unlocks m)
 336       Atomic::store(list_p, m);  // Switch list head to unlocked m.
 337       om_unlock(cur);
 338       break;
 339     }
 340     // The list is empty so try to set the list head.
 341     assert(cur == NULL, &quot;cur must be NULL: cur=&quot; INTPTR_FORMAT, p2i(cur));
 342     m-&gt;set_next_om(cur);  // m now points to NULL (and unlocks m)
 343     if (Atomic::cmpxchg(list_p, cur, m) == cur) {
 344       // List head is now unlocked m.
 345       break;
 346     }
 347     // Implied else: try it all again
 348   }
 349   Atomic::inc(count_p);
 350 }
 351 
 352 // Prepend an ObjectMonitor to a per-thread om_free_list.
 353 // Also updates the per-thread om_free_count.
 354 static void prepend_to_om_free_list(Thread* self, ObjectMonitor* m) {
 355   prepend_to_common(m, &amp;self-&gt;om_free_list, &amp;self-&gt;om_free_count);
 356 }
 357 
 358 // Prepend an ObjectMonitor to a per-thread om_in_use_list.
 359 // Also updates the per-thread om_in_use_count.
 360 static void prepend_to_om_in_use_list(Thread* self, ObjectMonitor* m) {
 361   prepend_to_common(m, &amp;self-&gt;om_in_use_list, &amp;self-&gt;om_in_use_count);
 362 }
 363 
 364 // Take an ObjectMonitor from the start of the specified list. Also
 365 // decrements the specified counter. Returns NULL if none are available.
 366 static ObjectMonitor* take_from_start_of_common(ObjectMonitor** list_p,
 367                                                 int* count_p) {
 368   ObjectMonitor* take = NULL;
 369   // Lock the list head to guard against races with a list walker
<span class="line-modified"> 370   // thread:</span>
 371   if ((take = get_list_head_locked(list_p)) == NULL) {
 372     return NULL;  // None are available.
 373   }
 374   ObjectMonitor* next = unmarked_next(take);
 375   // Switch locked list head to next (which unlocks the list head, but
 376   // leaves take locked):
 377   Atomic::store(list_p, next);
 378   Atomic::dec(count_p);
 379   // Unlock take, but leave the next value for any lagging list
 380   // walkers. It will get cleaned up when take is prepended to
 381   // the in-use list:
 382   om_unlock(take);
 383   return take;
 384 }
 385 
 386 // Take an ObjectMonitor from the start of the om_list_globals._free_list.
 387 // Also updates om_list_globals._free_count. Returns NULL if none are
 388 // available.
 389 static ObjectMonitor* take_from_start_of_global_free_list() {
 390   return take_from_start_of_common(&amp;om_list_globals._free_list,
</pre>
<hr />
<pre>
 465 
 466 
 467 // The LockNode emitted directly at the synchronization site would have
 468 // been too big if it were to have included support for the cases of inflated
 469 // recursive enter and exit, so they go here instead.
 470 // Note that we can&#39;t safely call AsyncPrintJavaStack() from within
 471 // quick_enter() as our thread state remains _in_Java.
 472 
 473 bool ObjectSynchronizer::quick_enter(oop obj, Thread* self,
 474                                      BasicLock * lock) {
 475   assert(!SafepointSynchronize::is_at_safepoint(), &quot;invariant&quot;);
 476   assert(self-&gt;is_Java_thread(), &quot;invariant&quot;);
 477   assert(((JavaThread *) self)-&gt;thread_state() == _thread_in_Java, &quot;invariant&quot;);
 478   NoSafepointVerifier nsv;
 479   if (obj == NULL) return false;       // Need to throw NPE
 480   assert(!EnableValhalla || !obj-&gt;klass()-&gt;is_value(), &quot;monitor op on value type&quot;);
 481   const markWord mark = obj-&gt;mark();
 482 
 483   if (mark.has_monitor()) {
 484     ObjectMonitor* const m = mark.monitor();
<span class="line-modified"> 485     assert(m-&gt;object() == obj, &quot;invariant&quot;);</span>









 486     Thread* const owner = (Thread *) m-&gt;_owner;
 487 
 488     // Lock contention and Transactional Lock Elision (TLE) diagnostics
 489     // and observability
 490     // Case: light contention possibly amenable to TLE
 491     // Case: TLE inimical operations such as nested/recursive synchronization
 492 
 493     if (owner == self) {
 494       m-&gt;_recursions++;
 495       return true;
 496     }
 497 
 498     // This Java Monitor is inflated so obj&#39;s header will never be
 499     // displaced to this thread&#39;s BasicLock. Make the displaced header
 500     // non-NULL so this BasicLock is not seen as recursive nor as
 501     // being locked. We do this unconditionally so that this thread&#39;s
 502     // BasicLock cannot be mis-interpreted by any stack walkers. For
 503     // performance reasons, stack walkers generally first check for
 504     // Biased Locking in the object&#39;s header, the second check is for
 505     // stack-locking in the object&#39;s header, the third check is for
</pre>
<hr />
<pre>
 546     // Anticipate successful CAS -- the ST of the displaced mark must
 547     // be visible &lt;= the ST performed by the CAS.
 548     lock-&gt;set_displaced_header(mark);
 549     if (mark == obj()-&gt;cas_set_mark(markWord::from_pointer(lock), mark)) {
 550       return;
 551     }
 552     // Fall through to inflate() ...
 553   } else if (mark.has_locker() &amp;&amp;
 554              THREAD-&gt;is_lock_owned((address)mark.locker())) {
 555     assert(lock != mark.locker(), &quot;must not re-lock the same lock&quot;);
 556     assert(lock != (BasicLock*)obj-&gt;mark().value(), &quot;don&#39;t relock with same BasicLock&quot;);
 557     lock-&gt;set_displaced_header(markWord::from_pointer(NULL));
 558     return;
 559   }
 560 
 561   // The object header will never be displaced to this lock,
 562   // so it does not matter what the value is, except that it
 563   // must be non-zero to avoid looking like a re-entrant lock,
 564   // and must not look locked either.
 565   lock-&gt;set_displaced_header(markWord::unused_mark());
<span class="line-modified"> 566   inflate(THREAD, obj(), inflate_cause_monitor_enter)-&gt;enter(THREAD);</span>








 567 }
 568 
 569 void ObjectSynchronizer::exit(oop object, BasicLock* lock, TRAPS) {
 570   markWord mark = object-&gt;mark();
 571   if (EnableValhalla &amp;&amp; mark.is_always_locked()) {
 572     return;
 573   }
 574   assert(!EnableValhalla || !object-&gt;klass()-&gt;is_value(), &quot;monitor op on value type&quot;);
 575   // We cannot check for Biased Locking if we are racing an inflation.
 576   assert(mark == markWord::INFLATING() ||
 577          !mark.has_bias_pattern(), &quot;should not see bias pattern here&quot;);
 578 
 579   markWord dhw = lock-&gt;displaced_header();
 580   if (dhw.value() == 0) {
 581     // If the displaced header is NULL, then this exit matches up with
 582     // a recursive enter. No real work to do here except for diagnostics.
 583 #ifndef PRODUCT
 584     if (mark != markWord::INFLATING()) {
 585       // Only do diagnostics if we are not racing an inflation. Simply
 586       // exiting a recursive enter of a Java Monitor that is being
</pre>
<hr />
<pre>
 599         // does not own the Java Monitor.
 600         ObjectMonitor* m = mark.monitor();
 601         assert(((oop)(m-&gt;object()))-&gt;mark() == mark, &quot;invariant&quot;);
 602         assert(m-&gt;is_entered(THREAD), &quot;invariant&quot;);
 603       }
 604     }
 605 #endif
 606     return;
 607   }
 608 
 609   if (mark == markWord::from_pointer(lock)) {
 610     // If the object is stack-locked by the current thread, try to
 611     // swing the displaced header from the BasicLock back to the mark.
 612     assert(dhw.is_neutral(), &quot;invariant&quot;);
 613     if (object-&gt;cas_set_mark(dhw, mark) == mark) {
 614       return;
 615     }
 616   }
 617 
 618   // We have to take the slow-path of possible inflation and then exit.
<span class="line-modified"> 619   inflate(THREAD, object, inflate_cause_vm_internal)-&gt;exit(true, THREAD);</span>



 620 }
 621 
 622 // -----------------------------------------------------------------------------
 623 // Class Loader  support to workaround deadlocks on the class loader lock objects
 624 // Also used by GC
 625 // complete_exit()/reenter() are used to wait on a nested lock
 626 // i.e. to give up an outer lock completely and then re-enter
 627 // Used when holding nested locks - lock acquisition order: lock1 then lock2
 628 //  1) complete_exit lock1 - saving recursion count
 629 //  2) wait on lock2
 630 //  3) when notified on lock2, unlock lock2
 631 //  4) reenter lock1 with original recursion count
 632 //  5) lock lock2
 633 // NOTE: must use heavy weight monitor to handle complete_exit/reenter()
 634 intx ObjectSynchronizer::complete_exit(Handle obj, TRAPS) {
 635   assert(!EnableValhalla || !obj-&gt;klass()-&gt;is_value(), &quot;monitor op on value type&quot;);
 636   if (UseBiasedLocking) {
 637     BiasedLocking::revoke(obj, THREAD);
 638     assert(!obj-&gt;mark().has_bias_pattern(), &quot;biases should be revoked by now&quot;);
 639   }
 640 


 641   ObjectMonitor* monitor = inflate(THREAD, obj(), inflate_cause_vm_internal);
<span class="line-modified"> 642 </span>
<span class="line-modified"> 643   return monitor-&gt;complete_exit(THREAD);</span>
 644 }
 645 
 646 // NOTE: must use heavy weight monitor to handle complete_exit/reenter()
 647 void ObjectSynchronizer::reenter(Handle obj, intx recursions, TRAPS) {
 648   assert(!EnableValhalla || !obj-&gt;klass()-&gt;is_value(), &quot;monitor op on value type&quot;);
 649   if (UseBiasedLocking) {
 650     BiasedLocking::revoke(obj, THREAD);
 651     assert(!obj-&gt;mark().has_bias_pattern(), &quot;biases should be revoked by now&quot;);
 652   }
 653 
<span class="line-modified"> 654   ObjectMonitor* monitor = inflate(THREAD, obj(), inflate_cause_vm_internal);</span>
<span class="line-modified"> 655 </span>
<span class="line-modified"> 656   monitor-&gt;reenter(recursions, THREAD);</span>







 657 }

 658 // -----------------------------------------------------------------------------
 659 // JNI locks on java objects
 660 // NOTE: must use heavy weight monitor to handle jni monitor enter
 661 void ObjectSynchronizer::jni_enter(Handle obj, TRAPS) {
 662   // the current locking is from JNI instead of Java code
 663   CHECK_THROW_NOSYNC_IMSE(obj);
 664   if (UseBiasedLocking) {
 665     BiasedLocking::revoke(obj, THREAD);
 666     assert(!obj-&gt;mark().has_bias_pattern(), &quot;biases should be revoked by now&quot;);
 667   }
 668   THREAD-&gt;set_current_pending_monitor_is_from_java(false);
<span class="line-modified"> 669   inflate(THREAD, obj(), inflate_cause_jni_enter)-&gt;enter(THREAD);</span>








 670   THREAD-&gt;set_current_pending_monitor_is_from_java(true);
 671 }
 672 
 673 // NOTE: must use heavy weight monitor to handle jni monitor exit
 674 void ObjectSynchronizer::jni_exit(oop obj, Thread* THREAD) {
 675   CHECK_THROW_NOSYNC_IMSE(obj);
 676   if (UseBiasedLocking) {
 677     Handle h_obj(THREAD, obj);
 678     BiasedLocking::revoke(h_obj, THREAD);
 679     obj = h_obj();
 680   }
 681   assert(!obj-&gt;mark().has_bias_pattern(), &quot;biases should be revoked by now&quot;);
 682 


 683   ObjectMonitor* monitor = inflate(THREAD, obj, inflate_cause_jni_exit);
 684   // If this thread has locked the object, exit the monitor. We
 685   // intentionally do not use CHECK here because we must exit the
 686   // monitor even if an exception is pending.
 687   if (monitor-&gt;check_owner(THREAD)) {
 688     monitor-&gt;exit(true, THREAD);
 689   }
 690 }
 691 
 692 // -----------------------------------------------------------------------------
 693 // Internal VM locks on java objects
 694 // standard constructor, allows locking failures
 695 ObjectLocker::ObjectLocker(Handle obj, Thread* thread, bool do_lock) {
 696   _dolock = do_lock;
 697   _thread = thread;
 698   _thread-&gt;check_for_valid_safepoint_state();
 699   _obj = obj;
 700 
 701   if (_dolock) {
 702     ObjectSynchronizer::enter(_obj, &amp;_lock, _thread);
</pre>
<hr />
<pre>
 705 
 706 ObjectLocker::~ObjectLocker() {
 707   if (_dolock) {
 708     ObjectSynchronizer::exit(_obj(), &amp;_lock, _thread);
 709   }
 710 }
 711 
 712 
 713 // -----------------------------------------------------------------------------
 714 //  Wait/Notify/NotifyAll
 715 // NOTE: must use heavy weight monitor to handle wait()
 716 int ObjectSynchronizer::wait(Handle obj, jlong millis, TRAPS) {
 717   CHECK_THROW_NOSYNC_IMSE_0(obj);
 718   if (UseBiasedLocking) {
 719     BiasedLocking::revoke(obj, THREAD);
 720     assert(!obj-&gt;mark().has_bias_pattern(), &quot;biases should be revoked by now&quot;);
 721   }
 722   if (millis &lt; 0) {
 723     THROW_MSG_0(vmSymbols::java_lang_IllegalArgumentException(), &quot;timeout value is negative&quot;);
 724   }



 725   ObjectMonitor* monitor = inflate(THREAD, obj(), inflate_cause_wait);
 726 
 727   DTRACE_MONITOR_WAIT_PROBE(monitor, obj(), THREAD, millis);
 728   monitor-&gt;wait(millis, true, THREAD);
 729 
 730   // This dummy call is in place to get around dtrace bug 6254741.  Once
 731   // that&#39;s fixed we can uncomment the following line, remove the call
 732   // and change this function back into a &quot;void&quot; func.
 733   // DTRACE_MONITOR_PROBE(waited, monitor, obj(), THREAD);
<span class="line-modified"> 734   return dtrace_waited_probe(monitor, obj, THREAD);</span>

 735 }
 736 
 737 void ObjectSynchronizer::wait_uninterruptibly(Handle obj, jlong millis, TRAPS) {
 738   CHECK_THROW_NOSYNC_IMSE(obj);
 739   if (UseBiasedLocking) {
 740     BiasedLocking::revoke(obj, THREAD);
 741     assert(!obj-&gt;mark().has_bias_pattern(), &quot;biases should be revoked by now&quot;);
 742   }
 743   if (millis &lt; 0) {
 744     THROW_MSG(vmSymbols::java_lang_IllegalArgumentException(), &quot;timeout value is negative&quot;);
 745   }
<span class="line-modified"> 746   inflate(THREAD, obj(), inflate_cause_wait)-&gt;wait(millis, false, THREAD);</span>




 747 }
 748 
 749 void ObjectSynchronizer::notify(Handle obj, TRAPS) {
 750   CHECK_THROW_NOSYNC_IMSE(obj);
 751   if (UseBiasedLocking) {
 752     BiasedLocking::revoke(obj, THREAD);
 753     assert(!obj-&gt;mark().has_bias_pattern(), &quot;biases should be revoked by now&quot;);
 754   }
 755 
 756   markWord mark = obj-&gt;mark();
 757   if (mark.has_locker() &amp;&amp; THREAD-&gt;is_lock_owned((address)mark.locker())) {
 758     return;
 759   }
<span class="line-modified"> 760   inflate(THREAD, obj(), inflate_cause_notify)-&gt;notify(THREAD);</span>



 761 }
 762 
 763 // NOTE: see comment of notify()
 764 void ObjectSynchronizer::notifyall(Handle obj, TRAPS) {
 765   CHECK_THROW_NOSYNC_IMSE(obj);
 766   if (UseBiasedLocking) {
 767     BiasedLocking::revoke(obj, THREAD);
 768     assert(!obj-&gt;mark().has_bias_pattern(), &quot;biases should be revoked by now&quot;);
 769   }
 770 
 771   markWord mark = obj-&gt;mark();
 772   if (mark.has_locker() &amp;&amp; THREAD-&gt;is_lock_owned((address)mark.locker())) {
 773     return;
 774   }
<span class="line-modified"> 775   inflate(THREAD, obj(), inflate_cause_notify)-&gt;notifyAll(THREAD);</span>



 776 }
 777 
 778 // -----------------------------------------------------------------------------
 779 // Hash Code handling
 780 //
 781 // Performance concern:
 782 // OrderAccess::storestore() calls release() which at one time stored 0
 783 // into the global volatile OrderAccess::dummy variable. This store was
 784 // unnecessary for correctness. Many threads storing into a common location
 785 // causes considerable cache migration or &quot;sloshing&quot; on large SMP systems.
 786 // As such, I avoided using OrderAccess::storestore(). In some cases
 787 // OrderAccess::fence() -- which incurs local latency on the executing
 788 // processor -- is a better choice as it scales on SMP systems.
 789 //
 790 // See http://blogs.oracle.com/dave/entry/biased_locking_in_hotspot for
 791 // a discussion of coherency costs. Note that all our current reference
 792 // platforms provide strong ST-ST order, so the issue is moot on IA32,
 793 // x64, and SPARC.
 794 //
 795 // As a general policy we use &quot;volatile&quot; to control compiler-based reordering
</pre>
<hr />
<pre>
 955       Handle hobj(self, obj);
 956       // Relaxing assertion for bug 6320749.
 957       assert(Universe::verify_in_progress() ||
 958              !SafepointSynchronize::is_at_safepoint(),
 959              &quot;biases should not be seen by VM thread here&quot;);
 960       BiasedLocking::revoke(hobj, JavaThread::current());
 961       obj = hobj();
 962       assert(!obj-&gt;mark().has_bias_pattern(), &quot;biases should be revoked by now&quot;);
 963     }
 964   }
 965 
 966   // hashCode() is a heap mutator ...
 967   // Relaxing assertion for bug 6320749.
 968   assert(Universe::verify_in_progress() || DumpSharedSpaces ||
 969          !SafepointSynchronize::is_at_safepoint(), &quot;invariant&quot;);
 970   assert(Universe::verify_in_progress() || DumpSharedSpaces ||
 971          self-&gt;is_Java_thread() , &quot;invariant&quot;);
 972   assert(Universe::verify_in_progress() || DumpSharedSpaces ||
 973          ((JavaThread *)self)-&gt;thread_state() != _thread_blocked, &quot;invariant&quot;);
 974 
<span class="line-modified"> 975   ObjectMonitor* monitor = NULL;</span>
<span class="line-modified"> 976   markWord temp, test;</span>
<span class="line-modified"> 977   intptr_t hash;</span>
<span class="line-modified"> 978   markWord mark = read_stable_mark(obj);</span>




































































 979 
<span class="line-modified"> 980   // object should remain ineligible for biased locking</span>
<span class="line-removed"> 981   assert(!mark.has_bias_pattern(), &quot;invariant&quot;);</span>
 982 
<span class="line-modified"> 983   if (mark.is_neutral()) {            // if this is a normal header</span>





 984     hash = mark.hash();
<span class="line-modified"> 985     if (hash != 0) {                  // if it has a hash, just return it</span>
<span class="line-modified"> 986       return hash;</span>
<span class="line-modified"> 987     }</span>
<span class="line-modified"> 988     hash = get_next_hash(self, obj);  // get a new hash</span>
<span class="line-modified"> 989     temp = mark.copy_set_hash(hash);  // merge the hash into header</span>
<span class="line-modified"> 990                                       // try to install the hash</span>
<span class="line-modified"> 991     test = obj-&gt;cas_set_mark(temp, mark);</span>
<span class="line-modified"> 992     if (test == mark) {               // if the hash was installed, return it</span>
<span class="line-modified"> 993       return hash;</span>
<span class="line-modified"> 994     }</span>
<span class="line-modified"> 995     // Failed to install the hash. It could be that another thread</span>
<span class="line-modified"> 996     // installed the hash just before our attempt or inflation has</span>
<span class="line-modified"> 997     // occurred or... so we fall thru to inflate the monitor for</span>
<span class="line-modified"> 998     // stability and then install the hash.</span>
<span class="line-modified"> 999   } else if (mark.has_monitor()) {</span>
<span class="line-modified">1000     monitor = mark.monitor();</span>
<span class="line-modified">1001     temp = monitor-&gt;header();</span>
<span class="line-modified">1002     assert(temp.is_neutral(), &quot;invariant: header=&quot; INTPTR_FORMAT, temp.value());</span>
<span class="line-modified">1003     hash = temp.hash();</span>
<span class="line-modified">1004     if (hash != 0) {                  // if it has a hash, just return it</span>
<span class="line-modified">1005       return hash;</span>
<span class="line-modified">1006     }</span>
<span class="line-modified">1007     // Fall thru so we only have one place that installs the hash in</span>
<span class="line-modified">1008     // the ObjectMonitor.</span>
<span class="line-modified">1009   } else if (self-&gt;is_lock_owned((address)mark.locker())) {</span>
<span class="line-modified">1010     // This is a stack lock owned by the calling thread so fetch the</span>
<span class="line-modified">1011     // displaced markWord from the BasicLock on the stack.</span>
<span class="line-modified">1012     temp = mark.displaced_mark_helper();</span>
<span class="line-removed">1013     assert(temp.is_neutral(), &quot;invariant: header=&quot; INTPTR_FORMAT, temp.value());</span>
<span class="line-removed">1014     hash = temp.hash();</span>
<span class="line-removed">1015     if (hash != 0) {                  // if it has a hash, just return it</span>
<span class="line-removed">1016       return hash;</span>
<span class="line-removed">1017     }</span>
<span class="line-removed">1018     // WARNING:</span>
<span class="line-removed">1019     // The displaced header in the BasicLock on a thread&#39;s stack</span>
<span class="line-removed">1020     // is strictly immutable. It CANNOT be changed in ANY cases.</span>
<span class="line-removed">1021     // So we have to inflate the stack lock into an ObjectMonitor</span>
<span class="line-removed">1022     // even if the current thread owns the lock. The BasicLock on</span>
<span class="line-removed">1023     // a thread&#39;s stack can be asynchronously read by other threads</span>
<span class="line-removed">1024     // during an inflate() call so any change to that stack memory</span>
<span class="line-removed">1025     // may not propagate to other threads correctly.</span>
<span class="line-removed">1026   }</span>
<span class="line-removed">1027 </span>
<span class="line-removed">1028   // Inflate the monitor to set the hash.</span>
<span class="line-removed">1029   monitor = inflate(self, obj, inflate_cause_hash_code);</span>
<span class="line-removed">1030   // Load ObjectMonitor&#39;s header/dmw field and see if it has a hash.</span>
<span class="line-removed">1031   mark = monitor-&gt;header();</span>
<span class="line-removed">1032   assert(mark.is_neutral(), &quot;invariant: header=&quot; INTPTR_FORMAT, mark.value());</span>
<span class="line-removed">1033   hash = mark.hash();</span>
<span class="line-removed">1034   if (hash == 0) {                    // if it does not have a hash</span>
<span class="line-removed">1035     hash = get_next_hash(self, obj);  // get a new hash</span>
<span class="line-removed">1036     temp = mark.copy_set_hash(hash);  // merge the hash into header</span>
<span class="line-removed">1037     assert(temp.is_neutral(), &quot;invariant: header=&quot; INTPTR_FORMAT, temp.value());</span>
<span class="line-removed">1038     uintptr_t v = Atomic::cmpxchg((volatile uintptr_t*)monitor-&gt;header_addr(), mark.value(), temp.value());</span>
<span class="line-removed">1039     test = markWord(v);</span>
<span class="line-removed">1040     if (test != mark) {</span>
<span class="line-removed">1041       // The attempt to update the ObjectMonitor&#39;s header/dmw field</span>
<span class="line-removed">1042       // did not work. This can happen if another thread managed to</span>
<span class="line-removed">1043       // merge in the hash just before our cmpxchg().</span>
<span class="line-removed">1044       // If we add any new usages of the header/dmw field, this code</span>
<span class="line-removed">1045       // will need to be updated.</span>
<span class="line-removed">1046       hash = test.hash();</span>
<span class="line-removed">1047       assert(test.is_neutral(), &quot;invariant: header=&quot; INTPTR_FORMAT, test.value());</span>
<span class="line-removed">1048       assert(hash != 0, &quot;should only have lost the race to a thread that set a non-zero hash&quot;);</span>
<span class="line-removed">1049     }</span>
<span class="line-removed">1050   }</span>
<span class="line-removed">1051   // We finally get the hash.</span>
<span class="line-removed">1052   return hash;</span>
1053 }
1054 
1055 
1056 bool ObjectSynchronizer::current_thread_holds_lock(JavaThread* thread,
1057                                                    Handle h_obj) {
1058   if (EnableValhalla &amp;&amp; h_obj-&gt;mark().is_always_locked()) {
1059     return false;
1060   }
1061   if (UseBiasedLocking) {
1062     BiasedLocking::revoke(h_obj, thread);
1063     assert(!h_obj-&gt;mark().has_bias_pattern(), &quot;biases should be revoked by now&quot;);
1064   }
1065 
1066   assert(thread == JavaThread::current(), &quot;Can only be called on current thread&quot;);
1067   oop obj = h_obj();
1068 
1069   markWord mark = read_stable_mark(obj);
1070 
1071   // Uncontended case, header points to stack
1072   if (mark.has_locker()) {
1073     return thread-&gt;is_lock_owned((address)mark.locker());
1074   }
1075   // Contended case, header points to ObjectMonitor (tagged pointer)
1076   if (mark.has_monitor()) {


1077     ObjectMonitor* monitor = mark.monitor();
1078     return monitor-&gt;is_entered(thread) != 0;
1079   }
1080   // Unlocked case, header in place
1081   assert(mark.is_neutral(), &quot;sanity check&quot;);
1082   return false;
1083 }
1084 
1085 // Be aware of this method could revoke bias of the lock object.
1086 // This method queries the ownership of the lock handle specified by &#39;h_obj&#39;.
1087 // If the current thread owns the lock, it returns owner_self. If no
1088 // thread owns the lock, it returns owner_none. Otherwise, it will return
1089 // owner_other.
1090 ObjectSynchronizer::LockOwnership ObjectSynchronizer::query_lock_ownership
1091 (JavaThread *self, Handle h_obj) {
1092   // The caller must beware this method can revoke bias, and
1093   // revocation can result in a safepoint.
1094   assert(!SafepointSynchronize::is_at_safepoint(), &quot;invariant&quot;);
1095   assert(self-&gt;thread_state() != _thread_blocked, &quot;invariant&quot;);
1096 
</pre>
<hr />
<pre>
1098 
1099   if (UseBiasedLocking &amp;&amp; h_obj()-&gt;mark().has_bias_pattern()) {
1100     // CASE: biased
1101     BiasedLocking::revoke(h_obj, self);
1102     assert(!h_obj-&gt;mark().has_bias_pattern(),
1103            &quot;biases should be revoked by now&quot;);
1104   }
1105 
1106   assert(self == JavaThread::current(), &quot;Can only be called on current thread&quot;);
1107   oop obj = h_obj();
1108   markWord mark = read_stable_mark(obj);
1109 
1110   // CASE: stack-locked.  Mark points to a BasicLock on the owner&#39;s stack.
1111   if (mark.has_locker()) {
1112     return self-&gt;is_lock_owned((address)mark.locker()) ?
1113       owner_self : owner_other;
1114   }
1115 
1116   // CASE: inflated. Mark (tagged pointer) points to an ObjectMonitor.
1117   // The Object:ObjectMonitor relationship is stable as long as we&#39;re
<span class="line-modified">1118   // not at a safepoint.</span>
1119   if (mark.has_monitor()) {
<span class="line-modified">1120     void* owner = mark.monitor()-&gt;_owner;</span>



1121     if (owner == NULL) return owner_none;
1122     return (owner == self ||
1123             self-&gt;is_lock_owned((address)owner)) ? owner_self : owner_other;
1124   }
1125 
1126   // CASE: neutral
1127   assert(mark.is_neutral(), &quot;sanity check&quot;);
1128   return owner_none;           // it&#39;s unlocked
1129 }
1130 
1131 // FIXME: jvmti should call this
1132 JavaThread* ObjectSynchronizer::get_lock_owner(ThreadsList * t_list, Handle h_obj) {
1133   if (UseBiasedLocking) {
1134     if (SafepointSynchronize::is_at_safepoint()) {
1135       BiasedLocking::revoke_at_safepoint(h_obj);
1136     } else {
1137       BiasedLocking::revoke(h_obj, JavaThread::current());
1138     }
1139     assert(!h_obj-&gt;mark().has_bias_pattern(), &quot;biases should be revoked by now&quot;);
1140   }
1141 
1142   oop obj = h_obj();
1143   address owner = NULL;
1144 
1145   markWord mark = read_stable_mark(obj);
1146 
1147   // Uncontended case, header points to stack
1148   if (mark.has_locker()) {
1149     owner = (address) mark.locker();
1150   }
1151 
1152   // Contended case, header points to ObjectMonitor (tagged pointer)
1153   else if (mark.has_monitor()) {


1154     ObjectMonitor* monitor = mark.monitor();
1155     assert(monitor != NULL, &quot;monitor should be non-null&quot;);
1156     owner = (address) monitor-&gt;owner();
1157   }
1158 
1159   if (owner != NULL) {
1160     // owning_thread_from_monitor_owner() may also return NULL here
1161     return Threads::owning_thread_from_monitor_owner(t_list, owner);
1162   }
1163 
1164   // Unlocked case, header in place
1165   // Cannot have assertion since this object may have been
1166   // locked by another thread when reaching here.
1167   // assert(mark.is_neutral(), &quot;sanity check&quot;);
1168 
1169   return NULL;
1170 }
1171 
1172 // Visitors ...
1173 
1174 void ObjectSynchronizer::monitors_iterate(MonitorClosure* closure) {
1175   PaddedObjectMonitor* block = Atomic::load(&amp;g_block_list);
1176   while (block != NULL) {
1177     assert(block-&gt;object() == CHAINMARKER, &quot;must be a block header&quot;);
1178     for (int i = _BLOCKSIZE - 1; i &gt; 0; i--) {
1179       ObjectMonitor* mid = (ObjectMonitor *)(block + i);
<span class="line-modified">1180       oop object = (oop)mid-&gt;object();</span>
<span class="line-removed">1181       if (object != NULL) {</span>
1182         // Only process with closure if the object is set.







1183         closure-&gt;do_monitor(mid);
1184       }
1185     }
1186     // unmarked_next() is not needed with g_block_list (no locking
1187     // used with block linkage _next_om fields).
1188     block = (PaddedObjectMonitor*)block-&gt;next_om();
1189   }
1190 }
1191 
1192 static bool monitors_used_above_threshold() {
1193   int population = Atomic::load(&amp;om_list_globals._population);
1194   if (population == 0) {
1195     return false;
1196   }
1197   if (MonitorUsedDeflationThreshold &gt; 0) {
<span class="line-modified">1198     int monitors_used = population - Atomic::load(&amp;om_list_globals._free_count);</span>

1199     int monitor_usage = (monitors_used * 100LL) / population;
1200     return monitor_usage &gt; MonitorUsedDeflationThreshold;
1201   }
1202   return false;
1203 }
1204 
<span class="line-modified">1205 bool ObjectSynchronizer::is_cleanup_needed() {</span>
<span class="line-modified">1206   return monitors_used_above_threshold();</span>





































1207 }
1208 
1209 void ObjectSynchronizer::oops_do(OopClosure* f) {
1210   // We only scan the global used list here (for moribund threads), and
1211   // the thread-local monitors in Thread::oops_do().
1212   global_used_oops_do(f);
1213 }
1214 
1215 void ObjectSynchronizer::global_used_oops_do(OopClosure* f) {
1216   assert(SafepointSynchronize::is_at_safepoint(), &quot;must be at safepoint&quot;);
1217   list_oops_do(Atomic::load(&amp;om_list_globals._in_use_list), f);
1218 }
1219 
1220 void ObjectSynchronizer::thread_local_used_oops_do(Thread* thread, OopClosure* f) {
1221   assert(SafepointSynchronize::is_at_safepoint(), &quot;must be at safepoint&quot;);
1222   list_oops_do(thread-&gt;om_in_use_list, f);
1223 }
1224 
1225 void ObjectSynchronizer::list_oops_do(ObjectMonitor* list, OopClosure* f) {
1226   assert(SafepointSynchronize::is_at_safepoint(), &quot;must be at safepoint&quot;);
1227   // The oops_do() phase does not overlap with monitor deflation
1228   // so no need to lock ObjectMonitors for the list traversal.
1229   for (ObjectMonitor* mid = list; mid != NULL; mid = unmarked_next(mid)) {
1230     if (mid-&gt;object() != NULL) {
1231       f-&gt;do_oop((oop*)mid-&gt;object_addr());
1232     }
1233   }
1234 }
1235 
1236 
1237 // -----------------------------------------------------------------------------
1238 // ObjectMonitor Lifecycle
1239 // -----------------------
1240 // Inflation unlinks monitors from om_list_globals._free_list or a per-thread
1241 // free list and associates them with objects. Deflation -- which occurs at
<span class="line-modified">1242 // STW-time -- disassociates idle monitors from objects.</span>
1243 // Such scavenged monitors are returned to the om_list_globals._free_list.
1244 //
1245 // ObjectMonitors reside in type-stable memory (TSM) and are immortal.
1246 //
1247 // Lifecycle:
1248 // --   unassigned and on the om_list_globals._free_list
1249 // --   unassigned and on a per-thread free list
1250 // --   assigned to an object.  The object is inflated and the mark refers
1251 //      to the ObjectMonitor.
1252 
1253 ObjectMonitor* ObjectSynchronizer::om_alloc(Thread* self) {
1254   // A large MAXPRIVATE value reduces both list lock contention
1255   // and list coherency traffic, but also tends to increase the
1256   // number of ObjectMonitors in circulation as well as the STW
1257   // scavenge costs.  As usual, we lean toward time in space-time
1258   // tradeoffs.
1259   const int MAXPRIVATE = 1024;
1260   NoSafepointVerifier nsv;
1261 
1262   for (;;) {
1263     ObjectMonitor* m;
1264 
1265     // 1: try to allocate from the thread&#39;s local om_free_list.
1266     // Threads will attempt to allocate first from their local list, then
1267     // from the global list, and only after those attempts fail will the
1268     // thread attempt to instantiate new monitors. Thread-local free lists
1269     // improve allocation latency, as well as reducing coherency traffic
1270     // on the shared global list.
1271     m = take_from_start_of_om_free_list(self);
1272     if (m != NULL) {
1273       guarantee(m-&gt;object() == NULL, &quot;invariant&quot;);

1274       prepend_to_om_in_use_list(self, m);
1275       return m;
1276     }
1277 
1278     // 2: try to allocate from the global om_list_globals._free_list
1279     // If we&#39;re using thread-local free lists then try
1280     // to reprovision the caller&#39;s free list.
1281     if (Atomic::load(&amp;om_list_globals._free_list) != NULL) {
1282       // Reprovision the thread&#39;s om_free_list.
1283       // Use bulk transfers to reduce the allocation rate and heat
1284       // on various locks.
1285       for (int i = self-&gt;om_free_provision; --i &gt;= 0;) {
1286         ObjectMonitor* take = take_from_start_of_global_free_list();
1287         if (take == NULL) {
1288           break;  // No more are available.
1289         }
1290         guarantee(take-&gt;object() == NULL, &quot;invariant&quot;);


















1291         take-&gt;Recycle();




1292         om_release(self, take, false);
1293       }
1294       self-&gt;om_free_provision += 1 + (self-&gt;om_free_provision / 2);
1295       if (self-&gt;om_free_provision &gt; MAXPRIVATE) self-&gt;om_free_provision = MAXPRIVATE;
1296       continue;
1297     }
1298 
1299     // 3: allocate a block of new ObjectMonitors
1300     // Both the local and global free lists are empty -- resort to malloc().
1301     // In the current implementation ObjectMonitors are TSM - immortal.
1302     // Ideally, we&#39;d write &quot;new ObjectMonitor[_BLOCKSIZE], but we want
1303     // each ObjectMonitor to start at the beginning of a cache line,
1304     // so we use align_up().
1305     // A better solution would be to use C++ placement-new.
1306     // BEWARE: As it stands currently, we don&#39;t run the ctors!
1307     assert(_BLOCKSIZE &gt; 1, &quot;invariant&quot;);
1308     size_t neededsize = sizeof(PaddedObjectMonitor) * _BLOCKSIZE;
1309     PaddedObjectMonitor* temp;
1310     size_t aligned_size = neededsize + (OM_CACHE_LINE_SIZE - 1);
1311     void* real_malloc_addr = NEW_C_HEAP_ARRAY(char, aligned_size, mtInternal);
1312     temp = (PaddedObjectMonitor*)align_up(real_malloc_addr, OM_CACHE_LINE_SIZE);
1313     (void)memset((void *) temp, 0, neededsize);
1314 
1315     // Format the block.
1316     // initialize the linked list, each monitor points to its next
1317     // forming the single linked free list, the very first monitor
1318     // will points to next block, which forms the block list.
1319     // The trick of using the 1st element in the block as g_block_list
1320     // linkage should be reconsidered.  A better implementation would
1321     // look like: class Block { Block * next; int N; ObjectMonitor Body [N] ; }
1322 
1323     for (int i = 1; i &lt; _BLOCKSIZE; i++) {
1324       temp[i].set_next_om((ObjectMonitor*)&amp;temp[i + 1]);

1325     }
1326 
1327     // terminate the last monitor as the end of list
1328     temp[_BLOCKSIZE - 1].set_next_om((ObjectMonitor*)NULL);
1329 
1330     // Element [0] is reserved for global list linkage
1331     temp[0].set_object(CHAINMARKER);
1332 
1333     // Consider carving out this thread&#39;s current request from the
1334     // block in hand.  This avoids some lock traffic and redundant
1335     // list activity.
1336 
1337     prepend_block_to_lists(temp);
1338   }
1339 }
1340 
1341 // Place &quot;m&quot; on the caller&#39;s private per-thread om_free_list.
1342 // In practice there&#39;s no need to clamp or limit the number of
1343 // monitors on a thread&#39;s om_free_list as the only non-allocation time
1344 // we&#39;ll call om_release() is to return a monitor to the free list after
1345 // a CAS attempt failed. This doesn&#39;t allow unbounded #s of monitors to
1346 // accumulate on a thread&#39;s free list.
1347 //
1348 // Key constraint: all ObjectMonitors on a thread&#39;s free list and the global
1349 // free list must have their object field set to null. This prevents the
<span class="line-modified">1350 // scavenger -- deflate_monitor_list() -- from reclaiming them while we</span>
<span class="line-modified">1351 // are trying to release them.</span>
1352 
1353 void ObjectSynchronizer::om_release(Thread* self, ObjectMonitor* m,
1354                                     bool from_per_thread_alloc) {
1355   guarantee(m-&gt;header().value() == 0, &quot;invariant&quot;);
1356   guarantee(m-&gt;object() == NULL, &quot;invariant&quot;);
1357   NoSafepointVerifier nsv;
1358 
1359   if ((m-&gt;is_busy() | m-&gt;_recursions) != 0) {
1360     stringStream ss;
1361     fatal(&quot;freeing in-use monitor: %s, recursions=&quot; INTX_FORMAT,
1362           m-&gt;is_busy_to_string(&amp;ss), m-&gt;_recursions);
1363   }

1364   // _next_om is used for both per-thread in-use and free lists so
1365   // we have to remove &#39;m&#39; from the in-use list first (as needed).
1366   if (from_per_thread_alloc) {
1367     // Need to remove &#39;m&#39; from om_in_use_list.
1368     ObjectMonitor* mid = NULL;
1369     ObjectMonitor* next = NULL;
1370 
<span class="line-modified">1371     // This list walk can only race with another list walker since</span>
<span class="line-modified">1372     // deflation can only happen at a safepoint so we don&#39;t have to</span>
<span class="line-modified">1373     // worry about an ObjectMonitor being removed from this list</span>
<span class="line-removed">1374     // while we are walking it.</span>
1375 
<span class="line-modified">1376     // Lock the list head to avoid racing with another list walker.</span>

1377     if ((mid = get_list_head_locked(&amp;self-&gt;om_in_use_list)) == NULL) {
1378       fatal(&quot;thread=&quot; INTPTR_FORMAT &quot; in-use list must not be empty.&quot;, p2i(self));
1379     }
1380     next = unmarked_next(mid);
1381     if (m == mid) {
1382       // First special case:
1383       // &#39;m&#39; matches mid, is the list head and is locked. Switch the list
1384       // head to next which unlocks the list head, but leaves the extracted
1385       // mid locked:
1386       Atomic::store(&amp;self-&gt;om_in_use_list, next);
1387     } else if (m == next) {
1388       // Second special case:
1389       // &#39;m&#39; matches next after the list head and we already have the list
1390       // head locked so set mid to what we are extracting:
1391       mid = next;
<span class="line-modified">1392       // Lock mid to prevent races with a list walker:</span>


1393       om_lock(mid);
1394       // Update next to what follows mid (if anything):
1395       next = unmarked_next(mid);
1396       // Switch next after the list head to new next which unlocks the
1397       // list head, but leaves the extracted mid locked:
1398       self-&gt;om_in_use_list-&gt;set_next_om(next);
1399     } else {
1400       // We have to search the list to find &#39;m&#39;.
<span class="line-removed">1401       om_unlock(mid);  // unlock the list head</span>
1402       guarantee(next != NULL, &quot;thread=&quot; INTPTR_FORMAT &quot;: om_in_use_list=&quot; INTPTR_FORMAT
1403                 &quot; is too short.&quot;, p2i(self), p2i(self-&gt;om_in_use_list));
1404       // Our starting anchor is next after the list head which is the
1405       // last ObjectMonitor we checked:
1406       ObjectMonitor* anchor = next;





1407       while ((mid = unmarked_next(anchor)) != NULL) {
1408         if (m == mid) {
1409           // We found &#39;m&#39; on the per-thread in-use list so extract it.
<span class="line-removed">1410           om_lock(anchor);  // Lock the anchor so we can safely modify it.</span>
1411           // Update next to what follows mid (if anything):
1412           next = unmarked_next(mid);
1413           // Switch next after the anchor to new next which unlocks the
1414           // anchor, but leaves the extracted mid locked:
1415           anchor-&gt;set_next_om(next);
1416           break;
1417         } else {
<span class="line-modified">1418           anchor = mid;</span>






1419         }
1420       }
1421     }
1422 
1423     if (mid == NULL) {
1424       // Reached end of the list and didn&#39;t find &#39;m&#39; so:
1425       fatal(&quot;thread=&quot; INTPTR_FORMAT &quot; must find m=&quot; INTPTR_FORMAT &quot;on om_in_use_list=&quot;
1426             INTPTR_FORMAT, p2i(self), p2i(m), p2i(self-&gt;om_in_use_list));
1427     }
1428 
1429     // At this point mid is disconnected from the in-use list so
1430     // its lock no longer has any effects on the in-use list.
1431     Atomic::dec(&amp;self-&gt;om_in_use_count);
1432     // Unlock mid, but leave the next value for any lagging list
1433     // walkers. It will get cleaned up when mid is prepended to
1434     // the thread&#39;s free list:
1435     om_unlock(mid);
1436   }
1437 
1438   prepend_to_om_free_list(self, m);

1439 }
1440 
1441 // Return ObjectMonitors on a moribund thread&#39;s free and in-use
1442 // lists to the appropriate global lists. The ObjectMonitors on the
1443 // per-thread in-use list may still be in use by other threads.
1444 //
1445 // We currently call om_flush() from Threads::remove() before the
1446 // thread has been excised from the thread list and is no longer a
1447 // mutator. This means that om_flush() cannot run concurrently with
1448 // a safepoint and interleave with deflate_idle_monitors(). In
1449 // particular, this ensures that the thread&#39;s in-use monitors are
1450 // scanned by a GC safepoint, either via Thread::oops_do() (before
1451 // om_flush() is called) or via ObjectSynchronizer::oops_do() (after
1452 // om_flush() is called).





1453 
1454 void ObjectSynchronizer::om_flush(Thread* self) {
1455   // Process the per-thread in-use list first to be consistent.
1456   int in_use_count = 0;
1457   ObjectMonitor* in_use_list = NULL;
1458   ObjectMonitor* in_use_tail = NULL;
1459   NoSafepointVerifier nsv;
1460 
<span class="line-modified">1461   // This function can race with a list walker thread so we lock the</span>
<span class="line-modified">1462   // list head to prevent confusion.</span>



1463   if ((in_use_list = get_list_head_locked(&amp;self-&gt;om_in_use_list)) != NULL) {
1464     // At this point, we have locked the in-use list head so a racing
1465     // thread cannot come in after us. However, a racing thread could
1466     // be ahead of us; we&#39;ll detect that and delay to let it finish.
1467     //
1468     // The thread is going away, however the ObjectMonitors on the
1469     // om_in_use_list may still be in-use by other threads. Link
1470     // them to in_use_tail, which will be linked into the global
1471     // in-use list (om_list_globals._in_use_list) below.
1472     //
1473     // Account for the in-use list head before the loop since it is
1474     // already locked (by this thread):
1475     in_use_tail = in_use_list;
1476     in_use_count++;
<span class="line-modified">1477     for (ObjectMonitor* cur_om = unmarked_next(in_use_list); cur_om != NULL; cur_om = unmarked_next(cur_om)) {</span>
1478       if (is_locked(cur_om)) {
<span class="line-modified">1479         // cur_om is locked so there must be a racing walker thread ahead</span>
<span class="line-modified">1480         // of us so we&#39;ll give it a chance to finish.</span>
1481         while (is_locked(cur_om)) {
1482           os::naked_short_sleep(1);
1483         }











1484       }
1485       in_use_tail = cur_om;
1486       in_use_count++;

1487     }
1488     guarantee(in_use_tail != NULL, &quot;invariant&quot;);
1489     int l_om_in_use_count = Atomic::load(&amp;self-&gt;om_in_use_count);
<span class="line-modified">1490     assert(l_om_in_use_count == in_use_count, &quot;in-use counts don&#39;t match: &quot;</span>
<span class="line-modified">1491           &quot;l_om_in_use_count=%d, in_use_count=%d&quot;, l_om_in_use_count, in_use_count);</span>
1492     Atomic::store(&amp;self-&gt;om_in_use_count, 0);
1493     // Clear the in-use list head (which also unlocks it):
1494     Atomic::store(&amp;self-&gt;om_in_use_list, (ObjectMonitor*)NULL);
1495     om_unlock(in_use_list);
1496   }
1497 
1498   int free_count = 0;
1499   ObjectMonitor* free_list = NULL;
1500   ObjectMonitor* free_tail = NULL;
1501   // This function can race with a list walker thread so we lock the
1502   // list head to prevent confusion.
1503   if ((free_list = get_list_head_locked(&amp;self-&gt;om_free_list)) != NULL) {
1504     // At this point, we have locked the free list head so a racing
1505     // thread cannot come in after us. However, a racing thread could
1506     // be ahead of us; we&#39;ll detect that and delay to let it finish.
1507     //
1508     // The thread is going away. Set &#39;free_tail&#39; to the last per-thread free
1509     // monitor which will be linked to om_list_globals._free_list below.
1510     //
1511     // Account for the free list head before the loop since it is
</pre>
<hr />
<pre>
1513     free_tail = free_list;
1514     free_count++;
1515     for (ObjectMonitor* s = unmarked_next(free_list); s != NULL; s = unmarked_next(s)) {
1516       if (is_locked(s)) {
1517         // s is locked so there must be a racing walker thread ahead
1518         // of us so we&#39;ll give it a chance to finish.
1519         while (is_locked(s)) {
1520           os::naked_short_sleep(1);
1521         }
1522       }
1523       free_tail = s;
1524       free_count++;
1525       guarantee(s-&gt;object() == NULL, &quot;invariant&quot;);
1526       if (s-&gt;is_busy()) {
1527         stringStream ss;
1528         fatal(&quot;must be !is_busy: %s&quot;, s-&gt;is_busy_to_string(&amp;ss));
1529       }
1530     }
1531     guarantee(free_tail != NULL, &quot;invariant&quot;);
1532     int l_om_free_count = Atomic::load(&amp;self-&gt;om_free_count);
<span class="line-modified">1533     assert(l_om_free_count == free_count, &quot;free counts don&#39;t match: &quot;</span>
<span class="line-modified">1534            &quot;l_om_free_count=%d, free_count=%d&quot;, l_om_free_count, free_count);</span>
1535     Atomic::store(&amp;self-&gt;om_free_count, 0);
1536     Atomic::store(&amp;self-&gt;om_free_list, (ObjectMonitor*)NULL);
1537     om_unlock(free_list);
1538   }
1539 
1540   if (free_tail != NULL) {
1541     prepend_list_to_global_free_list(free_list, free_tail, free_count);
1542   }
1543 
1544   if (in_use_tail != NULL) {
1545     prepend_list_to_global_in_use_list(in_use_list, in_use_tail, in_use_count);
1546   }
1547 
1548   LogStreamHandle(Debug, monitorinflation) lsh_debug;
1549   LogStreamHandle(Info, monitorinflation) lsh_info;
1550   LogStream* ls = NULL;
1551   if (log_is_enabled(Debug, monitorinflation)) {
1552     ls = &amp;lsh_debug;
1553   } else if ((free_count != 0 || in_use_count != 0) &amp;&amp;
1554              log_is_enabled(Info, monitorinflation)) {
</pre>
<hr />
<pre>
1559                  &quot;, in_use_count=%d&quot; &quot;, om_free_provision=%d&quot;,
1560                  p2i(self), free_count, in_use_count, self-&gt;om_free_provision);
1561   }
1562 }
1563 
1564 static void post_monitor_inflate_event(EventJavaMonitorInflate* event,
1565                                        const oop obj,
1566                                        ObjectSynchronizer::InflateCause cause) {
1567   assert(event != NULL, &quot;invariant&quot;);
1568   assert(event-&gt;should_commit(), &quot;invariant&quot;);
1569   event-&gt;set_monitorClass(obj-&gt;klass());
1570   event-&gt;set_address((uintptr_t)(void*)obj);
1571   event-&gt;set_cause((u1)cause);
1572   event-&gt;commit();
1573 }
1574 
1575 // Fast path code shared by multiple functions
1576 void ObjectSynchronizer::inflate_helper(oop obj) {
1577   markWord mark = obj-&gt;mark();
1578   if (mark.has_monitor()) {
<span class="line-modified">1579     assert(ObjectSynchronizer::verify_objmon_isinpool(mark.monitor()), &quot;monitor is invalid&quot;);</span>
<span class="line-modified">1580     assert(mark.monitor()-&gt;header().is_neutral(), &quot;monitor must record a good object header&quot;);</span>


1581     return;
1582   }
<span class="line-modified">1583   inflate(Thread::current(), obj, inflate_cause_vm_internal);</span>
1584 }
1585 
<span class="line-modified">1586 ObjectMonitor* ObjectSynchronizer::inflate(Thread* self,</span>
<span class="line-modified">1587                                            oop object, const InflateCause cause) {</span>
1588   // Inflate mutates the heap ...
1589   // Relaxing assertion for bug 6320749.
1590   assert(Universe::verify_in_progress() ||
1591          !SafepointSynchronize::is_at_safepoint(), &quot;invariant&quot;);
1592 
1593   if (EnableValhalla) {
1594     guarantee(!object-&gt;klass()-&gt;is_value(), &quot;Attempt to inflate value type&quot;);
1595   }
1596 
1597   EventJavaMonitorInflate event;
1598 
1599   for (;;) {
1600     const markWord mark = object-&gt;mark();
1601     assert(!mark.has_bias_pattern(), &quot;invariant&quot;);
1602 
1603     // The mark can be in one of the following states:
1604     // *  Inflated     - just return
1605     // *  Stack-locked - coerce it to inflated
1606     // *  INFLATING    - busy wait for conversion to complete
1607     // *  Neutral      - aggressively inflate the object.
1608     // *  BIASED       - Illegal.  We should never see this
1609 
1610     // CASE: inflated
1611     if (mark.has_monitor()) {
1612       ObjectMonitor* inf = mark.monitor();
1613       markWord dmw = inf-&gt;header();
1614       assert(dmw.is_neutral(), &quot;invariant: header=&quot; INTPTR_FORMAT, dmw.value());
<span class="line-modified">1615       assert(inf-&gt;object() == object, &quot;invariant&quot;);</span>
1616       assert(ObjectSynchronizer::verify_objmon_isinpool(inf), &quot;monitor is invalid&quot;);
1617       return inf;
1618     }
1619 
1620     // CASE: inflation in progress - inflating over a stack-lock.
1621     // Some other thread is converting from stack-locked to inflated.
1622     // Only that thread can complete inflation -- other threads must wait.
1623     // The INFLATING value is transient.
1624     // Currently, we spin/yield/park and poll the markword, waiting for inflation to finish.
1625     // We could always eliminate polling by parking the thread on some auxiliary list.
1626     if (mark == markWord::INFLATING()) {
1627       read_stable_mark(object);
1628       continue;
1629     }
1630 
1631     // CASE: stack-locked
1632     // Could be stack-locked either by this thread or by some other thread.
1633     //
1634     // Note that we allocate the objectmonitor speculatively, _before_ attempting
1635     // to install INFLATING into the mark word.  We originally installed INFLATING,
</pre>
<hr />
<pre>
1643     // critical INFLATING...ST interval.  A thread can transfer
1644     // multiple objectmonitors en-mass from the global free list to its local free list.
1645     // This reduces coherency traffic and lock contention on the global free list.
1646     // Using such local free lists, it doesn&#39;t matter if the om_alloc() call appears
1647     // before or after the CAS(INFLATING) operation.
1648     // See the comments in om_alloc().
1649 
1650     LogStreamHandle(Trace, monitorinflation) lsh;
1651 
1652     if (mark.has_locker()) {
1653       ObjectMonitor* m = om_alloc(self);
1654       // Optimistically prepare the objectmonitor - anticipate successful CAS
1655       // We do this before the CAS in order to minimize the length of time
1656       // in which INFLATING appears in the mark.
1657       m-&gt;Recycle();
1658       m-&gt;_Responsible  = NULL;
1659       m-&gt;_SpinDuration = ObjectMonitor::Knob_SpinLimit;   // Consider: maintain by type/class
1660 
1661       markWord cmp = object-&gt;cas_set_mark(markWord::INFLATING(), mark);
1662       if (cmp != mark) {

1663         om_release(self, m, true);
1664         continue;       // Interference -- just retry
1665       }
1666 
1667       // We&#39;ve successfully installed INFLATING (0) into the mark-word.
1668       // This is the only case where 0 will appear in a mark-word.
1669       // Only the singular thread that successfully swings the mark-word
1670       // to 0 can perform (or more precisely, complete) inflation.
1671       //
1672       // Why do we CAS a 0 into the mark-word instead of just CASing the
1673       // mark-word from the stack-locked value directly to the new inflated state?
1674       // Consider what happens when a thread unlocks a stack-locked object.
1675       // It attempts to use CAS to swing the displaced header value from the
1676       // on-stack BasicLock back into the object header.  Recall also that the
1677       // header value (hash code, etc) can reside in (a) the object header, or
1678       // (b) a displaced header associated with the stack-lock, or (c) a displaced
1679       // header in an ObjectMonitor.  The inflate() routine must copy the header
1680       // value from the BasicLock on the owner&#39;s stack to the ObjectMonitor, all
1681       // the while preserving the hashCode stability invariants.  If the owner
1682       // decides to release the lock while the value is 0, the unlock will fail
1683       // and control will eventually pass from slow_exit() to inflate.  The owner
1684       // will then spin, waiting for the 0 value to disappear.   Put another way,
1685       // the 0 causes the owner to stall if the owner happens to try to
1686       // drop the lock (restoring the header from the BasicLock to the object)
1687       // while inflation is in-progress.  This protocol avoids races that might
1688       // would otherwise permit hashCode values to change or &quot;flicker&quot; for an object.
1689       // Critically, while object-&gt;mark is 0 mark.displaced_mark_helper() is stable.
1690       // 0 serves as a &quot;BUSY&quot; inflate-in-progress indicator.
1691 
1692 
1693       // fetch the displaced mark from the owner&#39;s stack.
1694       // The owner can&#39;t die or unwind past the lock while our INFLATING
1695       // object is in the mark.  Furthermore the owner can&#39;t complete
1696       // an unlock on the object, either.
1697       markWord dmw = mark.displaced_mark_helper();
1698       // Catch if the object&#39;s header is not neutral (not locked and
1699       // not marked is what we care about here).
<span class="line-modified">1700       assert(dmw.is_neutral(), &quot;invariant: header=&quot; INTPTR_FORMAT, dmw.value());</span>
1701 
1702       // Setup monitor fields to proper values -- prepare the monitor
1703       m-&gt;set_header(dmw);
1704 
1705       // Optimization: if the mark.locker stack address is associated
1706       // with this thread we could simply set m-&gt;_owner = self.
1707       // Note that a thread can inflate an object
1708       // that it has stack-locked -- as might happen in wait() -- directly
1709       // with CAS.  That is, we can avoid the xchg-NULL .... ST idiom.
<span class="line-modified">1710       m-&gt;set_owner_from(NULL, mark.locker());</span>




1711       m-&gt;set_object(object);
1712       // TODO-FIXME: assert BasicLock-&gt;dhw != 0.
1713 
1714       // Must preserve store ordering. The monitor state must
1715       // be stable at the time of publishing the monitor address.
1716       guarantee(object-&gt;mark() == markWord::INFLATING(), &quot;invariant&quot;);
1717       object-&gt;release_set_mark(markWord::encode(m));
1718 





1719       // Hopefully the performance counters are allocated on distinct cache lines
1720       // to avoid false sharing on MP systems ...
1721       OM_PERFDATA_OP(Inflations, inc());
1722       if (log_is_enabled(Trace, monitorinflation)) {
1723         ResourceMark rm(self);
1724         lsh.print_cr(&quot;inflate(has_locker): object=&quot; INTPTR_FORMAT &quot;, mark=&quot;
1725                      INTPTR_FORMAT &quot;, type=&#39;%s&#39;&quot;, p2i(object),
1726                      object-&gt;mark().value(), object-&gt;klass()-&gt;external_name());
1727       }
1728       if (event.should_commit()) {
1729         post_monitor_inflate_event(&amp;event, object, cause);
1730       }
1731       return m;
1732     }
1733 
1734     // CASE: neutral
1735     // TODO-FIXME: for entry we currently inflate and then try to CAS _owner.
1736     // If we know we&#39;re inflating for entry it&#39;s better to inflate by swinging a
1737     // pre-locked ObjectMonitor pointer into the object header.   A successful
1738     // CAS inflates the object *and* confers ownership to the inflating thread.
1739     // In the current implementation we use a 2-step mechanism where we CAS()
1740     // to inflate and then CAS() again to try to swing _owner from NULL to self.
1741     // An inflateTry() method that we could call from enter() would be useful.
1742 
1743     // Catch if the object&#39;s header is not neutral (not locked and
1744     // not marked is what we care about here).
<span class="line-modified">1745     assert(mark.is_neutral(), &quot;invariant: header=&quot; INTPTR_FORMAT, mark.value());</span>
1746     ObjectMonitor* m = om_alloc(self);
1747     // prepare m for installation - set monitor to initial state
1748     m-&gt;Recycle();
1749     m-&gt;set_header(mark);




1750     m-&gt;set_object(object);
1751     m-&gt;_Responsible  = NULL;
1752     m-&gt;_SpinDuration = ObjectMonitor::Knob_SpinLimit;       // consider: keep metastats by type/class
1753 
1754     if (object-&gt;cas_set_mark(markWord::encode(m), mark) != mark) {
1755       m-&gt;set_header(markWord::zero());
1756       m-&gt;set_object(NULL);
1757       m-&gt;Recycle();

1758       om_release(self, m, true);
1759       m = NULL;
1760       continue;
1761       // interference - the markword changed - just retry.
1762       // The state-transitions are one-way, so there&#39;s no chance of
1763       // live-lock -- &quot;Inflated&quot; is an absorbing state.
1764     }
1765 





1766     // Hopefully the performance counters are allocated on distinct
1767     // cache lines to avoid false sharing on MP systems ...
1768     OM_PERFDATA_OP(Inflations, inc());
1769     if (log_is_enabled(Trace, monitorinflation)) {
1770       ResourceMark rm(self);
1771       lsh.print_cr(&quot;inflate(neutral): object=&quot; INTPTR_FORMAT &quot;, mark=&quot;
1772                    INTPTR_FORMAT &quot;, type=&#39;%s&#39;&quot;, p2i(object),
1773                    object-&gt;mark().value(), object-&gt;klass()-&gt;external_name());
1774     }
1775     if (event.should_commit()) {
1776       post_monitor_inflate_event(&amp;event, object, cause);
1777     }
1778     return m;
1779   }
1780 }
1781 
1782 
1783 // We maintain a list of in-use monitors for each thread.
1784 //

1785 // deflate_thread_local_monitors() scans a single thread&#39;s in-use list, while
1786 // deflate_idle_monitors() scans only a global list of in-use monitors which
1787 // is populated only as a thread dies (see om_flush()).
1788 //
1789 // These operations are called at all safepoints, immediately after mutators
1790 // are stopped, but before any objects have moved. Collectively they traverse
1791 // the population of in-use monitors, deflating where possible. The scavenged
1792 // monitors are returned to the global monitor free list.
1793 //
1794 // Beware that we scavenge at *every* stop-the-world point. Having a large
1795 // number of monitors in-use could negatively impact performance. We also want
1796 // to minimize the total # of monitors in circulation, as they incur a small
1797 // footprint penalty.
1798 //
1799 // Perversely, the heap size -- and thus the STW safepoint rate --
1800 // typically drives the scavenge rate.  Large heaps can mean infrequent GC,
1801 // which in turn can mean large(r) numbers of ObjectMonitors in circulation.
1802 // This is an unfortunate aspect of this design.


































1803 
1804 // Deflate a single monitor if not in-use
1805 // Return true if deflated, false if in-use
1806 bool ObjectSynchronizer::deflate_monitor(ObjectMonitor* mid, oop obj,
1807                                          ObjectMonitor** free_head_p,
1808                                          ObjectMonitor** free_tail_p) {
1809   bool deflated;
1810   // Normal case ... The monitor is associated with obj.
1811   const markWord mark = obj-&gt;mark();
1812   guarantee(mark == markWord::encode(mid), &quot;should match: mark=&quot;
1813             INTPTR_FORMAT &quot;, encoded mid=&quot; INTPTR_FORMAT, mark.value(),
1814             markWord::encode(mid).value());
1815   // Make sure that mark.monitor() and markWord::encode() agree:
1816   guarantee(mark.monitor() == mid, &quot;should match: monitor()=&quot; INTPTR_FORMAT
1817             &quot;, mid=&quot; INTPTR_FORMAT, p2i(mark.monitor()), p2i(mid));
1818   const markWord dmw = mid-&gt;header();
1819   guarantee(dmw.is_neutral(), &quot;invariant: header=&quot; INTPTR_FORMAT, dmw.value());
1820 
1821   if (mid-&gt;is_busy()) {
1822     // Easy checks are first - the ObjectMonitor is busy so no deflation.
1823     deflated = false;
1824   } else {
1825     // Deflate the monitor if it is no longer being used
1826     // It&#39;s idle - scavenge and return to the global free list
1827     // plain old deflation ...
1828     if (log_is_enabled(Trace, monitorinflation)) {
1829       ResourceMark rm;
1830       log_trace(monitorinflation)(&quot;deflate_monitor: &quot;
1831                                   &quot;object=&quot; INTPTR_FORMAT &quot;, mark=&quot;
1832                                   INTPTR_FORMAT &quot;, type=&#39;%s&#39;&quot;, p2i(obj),
1833                                   mark.value(), obj-&gt;klass()-&gt;external_name());
1834     }
1835 
1836     // Restore the header back to obj
1837     obj-&gt;release_set_mark(dmw);





1838     mid-&gt;clear();
1839 
1840     assert(mid-&gt;object() == NULL, &quot;invariant: object=&quot; INTPTR_FORMAT,
1841            p2i(mid-&gt;object()));

1842 
1843     // Move the deflated ObjectMonitor to the working free list
1844     // defined by free_head_p and free_tail_p.
1845     if (*free_head_p == NULL) *free_head_p = mid;
1846     if (*free_tail_p != NULL) {
1847       // We append to the list so the caller can use mid-&gt;_next_om
1848       // to fix the linkages in its context.
1849       ObjectMonitor* prevtail = *free_tail_p;
1850       // Should have been cleaned up by the caller:
1851       // Note: Should not have to lock prevtail here since we&#39;re at a
1852       // safepoint and ObjectMonitors on the local free list should
1853       // not be accessed in parallel.
1854 #ifdef ASSERT
1855       ObjectMonitor* l_next_om = prevtail-&gt;next_om();
1856 #endif
1857       assert(l_next_om == NULL, &quot;must be NULL: _next_om=&quot; INTPTR_FORMAT, p2i(l_next_om));
1858       prevtail-&gt;set_next_om(mid);
1859     }
1860     *free_tail_p = mid;
1861     // At this point, mid-&gt;_next_om still refers to its current
1862     // value and another ObjectMonitor&#39;s _next_om field still
1863     // refers to this ObjectMonitor. Those linkages have to be
1864     // cleaned up by the caller who has the complete context.
1865     deflated = true;
1866   }
1867   return deflated;
1868 }
1869 


























































































































1870 // Walk a given monitor list, and deflate idle monitors.
1871 // The given list could be a per-thread list or a global list.
1872 //
1873 // In the case of parallel processing of thread local monitor lists,
1874 // work is done by Threads::parallel_threads_do() which ensures that
1875 // each Java thread is processed by exactly one worker thread, and
1876 // thus avoid conflicts that would arise when worker threads would
1877 // process the same monitor lists concurrently.
1878 //
1879 // See also ParallelSPCleanupTask and
1880 // SafepointSynchronize::do_cleanup_tasks() in safepoint.cpp and
1881 // Threads::parallel_java_threads_do() in thread.cpp.
1882 int ObjectSynchronizer::deflate_monitor_list(ObjectMonitor** list_p,
1883                                              int* count_p,
1884                                              ObjectMonitor** free_head_p,
1885                                              ObjectMonitor** free_tail_p) {
1886   ObjectMonitor* cur_mid_in_use = NULL;
1887   ObjectMonitor* mid = NULL;
1888   ObjectMonitor* next = NULL;
1889   int deflated_count = 0;
</pre>
<hr />
<pre>
1900       // by unlinking mid from the global or per-thread in-use list.
1901       if (cur_mid_in_use == NULL) {
1902         // mid is the list head so switch the list head to next:
1903         Atomic::store(list_p, next);
1904       } else {
1905         // Switch cur_mid_in_use&#39;s next field to next:
1906         cur_mid_in_use-&gt;set_next_om(next);
1907       }
1908       // At this point mid is disconnected from the in-use list.
1909       deflated_count++;
1910       Atomic::dec(count_p);
1911       // mid is current tail in the free_head_p list so NULL terminate it:
1912       mid-&gt;set_next_om(NULL);
1913     } else {
1914       cur_mid_in_use = mid;
1915     }
1916   }
1917   return deflated_count;
1918 }
1919 













































































































































1920 void ObjectSynchronizer::prepare_deflate_idle_monitors(DeflateMonitorCounters* counters) {
1921   counters-&gt;n_in_use = 0;              // currently associated with objects
1922   counters-&gt;n_in_circulation = 0;      // extant
1923   counters-&gt;n_scavenged = 0;           // reclaimed (global and per-thread)
1924   counters-&gt;per_thread_scavenged = 0;  // per-thread scavenge total
1925   counters-&gt;per_thread_times = 0.0;    // per-thread scavenge times
1926 }
1927 
1928 void ObjectSynchronizer::deflate_idle_monitors(DeflateMonitorCounters* counters) {
1929   assert(SafepointSynchronize::is_at_safepoint(), &quot;must be at safepoint&quot;);









1930   bool deflated = false;
1931 
1932   ObjectMonitor* free_head_p = NULL;  // Local SLL of scavenged monitors
1933   ObjectMonitor* free_tail_p = NULL;
1934   elapsedTimer timer;
1935 
1936   if (log_is_enabled(Info, monitorinflation)) {
1937     timer.start();
1938   }
1939 
1940   // Note: the thread-local monitors lists get deflated in
1941   // a separate pass. See deflate_thread_local_monitors().
1942 
1943   // For moribund threads, scan om_list_globals._in_use_list
1944   int deflated_count = 0;
1945   if (Atomic::load(&amp;om_list_globals._in_use_list) != NULL) {
1946     // Update n_in_circulation before om_list_globals._in_use_count is
1947     // updated by deflation.
1948     Atomic::add(&amp;counters-&gt;n_in_circulation,
1949                 Atomic::load(&amp;om_list_globals._in_use_count));
</pre>
<hr />
<pre>
1962 #endif
1963     assert(l_next_om == NULL, &quot;must be NULL: _next_om=&quot; INTPTR_FORMAT, p2i(l_next_om));
1964     prepend_list_to_global_free_list(free_head_p, free_tail_p, deflated_count);
1965     Atomic::add(&amp;counters-&gt;n_scavenged, deflated_count);
1966   }
1967   timer.stop();
1968 
1969   LogStreamHandle(Debug, monitorinflation) lsh_debug;
1970   LogStreamHandle(Info, monitorinflation) lsh_info;
1971   LogStream* ls = NULL;
1972   if (log_is_enabled(Debug, monitorinflation)) {
1973     ls = &amp;lsh_debug;
1974   } else if (deflated_count != 0 &amp;&amp; log_is_enabled(Info, monitorinflation)) {
1975     ls = &amp;lsh_info;
1976   }
1977   if (ls != NULL) {
1978     ls-&gt;print_cr(&quot;deflating global idle monitors, %3.7f secs, %d monitors&quot;, timer.seconds(), deflated_count);
1979   }
1980 }
1981 






































































































































































































1982 void ObjectSynchronizer::finish_deflate_idle_monitors(DeflateMonitorCounters* counters) {
1983   // Report the cumulative time for deflating each thread&#39;s idle
1984   // monitors. Note: if the work is split among more than one
1985   // worker thread, then the reported time will likely be more
1986   // than a beginning to end measurement of the phase.
1987   log_info(safepoint, cleanup)(&quot;deflating per-thread idle monitors, %3.7f secs, monitors=%d&quot;, counters-&gt;per_thread_times, counters-&gt;per_thread_scavenged);
1988 







1989   if (log_is_enabled(Debug, monitorinflation)) {
1990     // exit_globals()&#39;s call to audit_and_print_stats() is done
1991     // at the Info level and not at a safepoint.



1992     ObjectSynchronizer::audit_and_print_stats(false /* on_exit */);
1993   } else if (log_is_enabled(Info, monitorinflation)) {
1994     log_info(monitorinflation)(&quot;global_population=%d, global_in_use_count=%d, &quot;
<span class="line-modified">1995                                &quot;global_free_count=%d&quot;,</span>
1996                                Atomic::load(&amp;om_list_globals._population),
1997                                Atomic::load(&amp;om_list_globals._in_use_count),
<span class="line-modified">1998                                Atomic::load(&amp;om_list_globals._free_count));</span>

1999   }
2000 
2001   OM_PERFDATA_OP(Deflations, inc(counters-&gt;n_scavenged));
2002   OM_PERFDATA_OP(MonExtant, set_value(counters-&gt;n_in_circulation));
2003 
2004   GVars.stw_random = os::random();
2005   GVars.stw_cycle++;




2006 }
2007 
2008 void ObjectSynchronizer::deflate_thread_local_monitors(Thread* thread, DeflateMonitorCounters* counters) {
2009   assert(SafepointSynchronize::is_at_safepoint(), &quot;must be at safepoint&quot;);
2010 





2011   ObjectMonitor* free_head_p = NULL;  // Local SLL of scavenged monitors
2012   ObjectMonitor* free_tail_p = NULL;
2013   elapsedTimer timer;
2014 
2015   if (log_is_enabled(Info, safepoint, cleanup) ||
2016       log_is_enabled(Info, monitorinflation)) {
2017     timer.start();
2018   }
2019 
2020   // Update n_in_circulation before om_in_use_count is updated by deflation.
2021   Atomic::add(&amp;counters-&gt;n_in_circulation, Atomic::load(&amp;thread-&gt;om_in_use_count));
2022 
2023   int deflated_count = deflate_monitor_list(&amp;thread-&gt;om_in_use_list, &amp;thread-&gt;om_in_use_count, &amp;free_head_p, &amp;free_tail_p);
2024   Atomic::add(&amp;counters-&gt;n_in_use, Atomic::load(&amp;thread-&gt;om_in_use_count));
2025 
2026   if (free_head_p != NULL) {
2027     // Move the deflated ObjectMonitors back to the global free list.
2028     guarantee(free_tail_p != NULL &amp;&amp; deflated_count &gt; 0, &quot;invariant&quot;);
2029 #ifdef ASSERT
2030     ObjectMonitor* l_next_om = free_tail_p-&gt;next_om();
</pre>
<hr />
<pre>
2164   if (Atomic::load(&amp;om_list_globals._population) == chk_om_population) {
2165     ls-&gt;print_cr(&quot;global_population=%d equals chk_om_population=%d&quot;,
2166                  Atomic::load(&amp;om_list_globals._population), chk_om_population);
2167   } else {
2168     // With fine grained locks on the monitor lists, it is possible for
2169     // log_monitor_list_counts() to return a value that doesn&#39;t match
2170     // om_list_globals._population. So far a higher value has been
2171     // seen in testing so something is being double counted by
2172     // log_monitor_list_counts().
2173     ls-&gt;print_cr(&quot;WARNING: global_population=%d is not equal to &quot;
2174                  &quot;chk_om_population=%d&quot;,
2175                  Atomic::load(&amp;om_list_globals._population), chk_om_population);
2176   }
2177 
2178   // Check om_list_globals._in_use_list and om_list_globals._in_use_count:
2179   chk_global_in_use_list_and_count(ls, &amp;error_cnt);
2180 
2181   // Check om_list_globals._free_list and om_list_globals._free_count:
2182   chk_global_free_list_and_count(ls, &amp;error_cnt);
2183 



2184   ls-&gt;print_cr(&quot;Checking per-thread lists:&quot;);
2185 
2186   for (JavaThreadIteratorWithHandle jtiwh; JavaThread *jt = jtiwh.next(); ) {
2187     // Check om_in_use_list and om_in_use_count:
2188     chk_per_thread_in_use_list_and_count(jt, ls, &amp;error_cnt);
2189 
2190     // Check om_free_list and om_free_count:
2191     chk_per_thread_free_list_and_count(jt, ls, &amp;error_cnt);
2192   }
2193 
2194   if (error_cnt == 0) {
2195     ls-&gt;print_cr(&quot;No errors found in monitor list checks.&quot;);
2196   } else {
2197     log_error(monitorinflation)(&quot;found monitor list errors: error_cnt=%d&quot;, error_cnt);
2198   }
2199 
2200   if ((on_exit &amp;&amp; log_is_enabled(Info, monitorinflation)) ||
2201       (!on_exit &amp;&amp; log_is_enabled(Trace, monitorinflation))) {
2202     // When exiting this log output is at the Info level. When called
2203     // at a safepoint, this log output is at the Trace level since
</pre>
<hr />
<pre>
2214 void ObjectSynchronizer::chk_free_entry(JavaThread* jt, ObjectMonitor* n,
2215                                         outputStream * out, int *error_cnt_p) {
2216   stringStream ss;
2217   if (n-&gt;is_busy()) {
2218     if (jt != NULL) {
2219       out-&gt;print_cr(&quot;ERROR: jt=&quot; INTPTR_FORMAT &quot;, monitor=&quot; INTPTR_FORMAT
2220                     &quot;: free per-thread monitor must not be busy: %s&quot;, p2i(jt),
2221                     p2i(n), n-&gt;is_busy_to_string(&amp;ss));
2222     } else {
2223       out-&gt;print_cr(&quot;ERROR: monitor=&quot; INTPTR_FORMAT &quot;: free global monitor &quot;
2224                     &quot;must not be busy: %s&quot;, p2i(n), n-&gt;is_busy_to_string(&amp;ss));
2225     }
2226     *error_cnt_p = *error_cnt_p + 1;
2227   }
2228   if (n-&gt;header().value() != 0) {
2229     if (jt != NULL) {
2230       out-&gt;print_cr(&quot;ERROR: jt=&quot; INTPTR_FORMAT &quot;, monitor=&quot; INTPTR_FORMAT
2231                     &quot;: free per-thread monitor must have NULL _header &quot;
2232                     &quot;field: _header=&quot; INTPTR_FORMAT, p2i(jt), p2i(n),
2233                     n-&gt;header().value());
<span class="line-modified">2234     } else {</span>

2235       out-&gt;print_cr(&quot;ERROR: monitor=&quot; INTPTR_FORMAT &quot;: free global monitor &quot;
2236                     &quot;must have NULL _header field: _header=&quot; INTPTR_FORMAT,
2237                     p2i(n), n-&gt;header().value());

2238     }
<span class="line-removed">2239     *error_cnt_p = *error_cnt_p + 1;</span>
2240   }
2241   if (n-&gt;object() != NULL) {
2242     if (jt != NULL) {
2243       out-&gt;print_cr(&quot;ERROR: jt=&quot; INTPTR_FORMAT &quot;, monitor=&quot; INTPTR_FORMAT
2244                     &quot;: free per-thread monitor must have NULL _object &quot;
2245                     &quot;field: _object=&quot; INTPTR_FORMAT, p2i(jt), p2i(n),
2246                     p2i(n-&gt;object()));
2247     } else {
2248       out-&gt;print_cr(&quot;ERROR: monitor=&quot; INTPTR_FORMAT &quot;: free global monitor &quot;
2249                     &quot;must have NULL _object field: _object=&quot; INTPTR_FORMAT,
2250                     p2i(n), p2i(n-&gt;object()));
2251     }
2252     *error_cnt_p = *error_cnt_p + 1;
2253   }
2254 }
2255 
2256 // Lock the next ObjectMonitor for traversal and unlock the current
2257 // ObjectMonitor. Returns the next ObjectMonitor if there is one.
2258 // Otherwise returns NULL (after unlocking the current ObjectMonitor).
2259 // This function is used by the various list walker functions to
</pre>
<hr />
<pre>
2286       if (cur == NULL) {
2287         break;
2288       }
2289     }
2290   }
2291   int l_free_count = Atomic::load(&amp;om_list_globals._free_count);
2292   if (l_free_count == chk_om_free_count) {
2293     out-&gt;print_cr(&quot;global_free_count=%d equals chk_om_free_count=%d&quot;,
2294                   l_free_count, chk_om_free_count);
2295   } else {
2296     // With fine grained locks on om_list_globals._free_list, it
2297     // is possible for an ObjectMonitor to be prepended to
2298     // om_list_globals._free_list after we started calculating
2299     // chk_om_free_count so om_list_globals._free_count may not
2300     // match anymore.
2301     out-&gt;print_cr(&quot;WARNING: global_free_count=%d is not equal to &quot;
2302                   &quot;chk_om_free_count=%d&quot;, l_free_count, chk_om_free_count);
2303   }
2304 }
2305 






























2306 // Check the global in-use list and count; log the results of the checks.
2307 void ObjectSynchronizer::chk_global_in_use_list_and_count(outputStream * out,
2308                                                           int *error_cnt_p) {
2309   int chk_om_in_use_count = 0;
2310   ObjectMonitor* cur = NULL;
2311   if ((cur = get_list_head_locked(&amp;om_list_globals._in_use_list)) != NULL) {
2312     // Marked the global in-use list head so process the list.
2313     while (true) {
2314       chk_in_use_entry(NULL /* jt */, cur, out, error_cnt_p);
2315       chk_om_in_use_count++;
2316 
2317       cur = lock_next_for_traversal(cur);
2318       if (cur == NULL) {
2319         break;
2320       }
2321     }
2322   }
2323   int l_in_use_count = Atomic::load(&amp;om_list_globals._in_use_count);
2324   if (l_in_use_count == chk_om_in_use_count) {
2325     out-&gt;print_cr(&quot;global_in_use_count=%d equals chk_om_in_use_count=%d&quot;,
</pre>
<hr />
<pre>
2509           out-&gt;print(&quot; (%s)&quot;, cur-&gt;is_busy_to_string(&amp;ss));
2510           ss.reset();
2511         }
2512         out-&gt;cr();
2513 
2514         cur = lock_next_for_traversal(cur);
2515         if (cur == NULL) {
2516           break;
2517         }
2518       }
2519     }
2520   }
2521 
2522   out-&gt;flush();
2523 }
2524 
2525 // Log counts for the global and per-thread monitor lists and return
2526 // the population count.
2527 int ObjectSynchronizer::log_monitor_list_counts(outputStream * out) {
2528   int pop_count = 0;
<span class="line-modified">2529   out-&gt;print_cr(&quot;%18s  %10s  %10s  %10s&quot;,</span>
<span class="line-modified">2530                 &quot;Global Lists:&quot;, &quot;InUse&quot;, &quot;Free&quot;, &quot;Total&quot;);</span>
<span class="line-modified">2531   out-&gt;print_cr(&quot;==================  ==========  ==========  ==========&quot;);</span>
2532   int l_in_use_count = Atomic::load(&amp;om_list_globals._in_use_count);
2533   int l_free_count = Atomic::load(&amp;om_list_globals._free_count);
<span class="line-modified">2534   out-&gt;print_cr(&quot;%18s  %10d  %10d  %10d&quot;, &quot;&quot;, l_in_use_count,</span>
<span class="line-modified">2535                 l_free_count, Atomic::load(&amp;om_list_globals._population));</span>
<span class="line-modified">2536   pop_count += l_in_use_count + l_free_count;</span>


2537 
2538   out-&gt;print_cr(&quot;%18s  %10s  %10s  %10s&quot;,
2539                 &quot;Per-Thread Lists:&quot;, &quot;InUse&quot;, &quot;Free&quot;, &quot;Provision&quot;);
2540   out-&gt;print_cr(&quot;==================  ==========  ==========  ==========&quot;);
2541 
2542   for (JavaThreadIteratorWithHandle jtiwh; JavaThread *jt = jtiwh.next(); ) {
2543     int l_om_in_use_count = Atomic::load(&amp;jt-&gt;om_in_use_count);
2544     int l_om_free_count = Atomic::load(&amp;jt-&gt;om_free_count);
2545     out-&gt;print_cr(INTPTR_FORMAT &quot;  %10d  %10d  %10d&quot;, p2i(jt),
2546                   l_om_in_use_count, l_om_free_count, jt-&gt;om_free_provision);
2547     pop_count += l_om_in_use_count + l_om_free_count;
2548   }
2549   return pop_count;
2550 }
2551 
2552 #ifndef PRODUCT
2553 
2554 // Check if monitor belongs to the monitor cache
2555 // The list is grow-only so it&#39;s *relatively* safe to traverse
2556 // the list of extant blocks without taking a lock.
</pre>
</td>
<td>
<hr />
<pre>
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #include &quot;precompiled.hpp&quot;
  26 #include &quot;classfile/vmSymbols.hpp&quot;
  27 #include &quot;logging/log.hpp&quot;
  28 #include &quot;logging/logStream.hpp&quot;
  29 #include &quot;jfr/jfrEvents.hpp&quot;
  30 #include &quot;memory/allocation.inline.hpp&quot;
  31 #include &quot;memory/metaspaceShared.hpp&quot;
  32 #include &quot;memory/padded.hpp&quot;
  33 #include &quot;memory/resourceArea.hpp&quot;
  34 #include &quot;memory/universe.hpp&quot;
  35 #include &quot;oops/markWord.hpp&quot;
  36 #include &quot;oops/oop.inline.hpp&quot;
  37 #include &quot;runtime/atomic.hpp&quot;
  38 #include &quot;runtime/biasedLocking.hpp&quot;
  39 #include &quot;runtime/handles.inline.hpp&quot;
<span class="line-added">  40 #include &quot;runtime/handshake.hpp&quot;</span>
  41 #include &quot;runtime/interfaceSupport.inline.hpp&quot;
  42 #include &quot;runtime/mutexLocker.hpp&quot;
  43 #include &quot;runtime/objectMonitor.hpp&quot;
  44 #include &quot;runtime/objectMonitor.inline.hpp&quot;
  45 #include &quot;runtime/osThread.hpp&quot;
<span class="line-added">  46 #include &quot;runtime/safepointMechanism.inline.hpp&quot;</span>
  47 #include &quot;runtime/safepointVerifiers.hpp&quot;
  48 #include &quot;runtime/sharedRuntime.hpp&quot;
  49 #include &quot;runtime/stubRoutines.hpp&quot;
  50 #include &quot;runtime/synchronizer.hpp&quot;
  51 #include &quot;runtime/thread.inline.hpp&quot;
  52 #include &quot;runtime/timer.hpp&quot;
  53 #include &quot;runtime/vframe.hpp&quot;
  54 #include &quot;runtime/vmThread.hpp&quot;
  55 #include &quot;utilities/align.hpp&quot;
  56 #include &quot;utilities/dtrace.hpp&quot;
  57 #include &quot;utilities/events.hpp&quot;
  58 #include &quot;utilities/preserveException.hpp&quot;
  59 
  60 // The &quot;core&quot; versions of monitor enter and exit reside in this file.
  61 // The interpreter and compilers contain specialized transliterated
  62 // variants of the enter-exit fast-path operations.  See i486.ad fast_lock(),
  63 // for instance.  If you make changes here, make sure to modify the
  64 // interpreter, and both C1 and C2 fast-path inline locking code emission.
  65 //
  66 // -----------------------------------------------------------------------------
</pre>
<hr />
<pre>
 103   }
 104 
 105 #else //  ndef DTRACE_ENABLED
 106 
 107 #define DTRACE_MONITOR_WAIT_PROBE(obj, thread, millis, mon)    {;}
 108 #define DTRACE_MONITOR_PROBE(probe, obj, thread, mon)          {;}
 109 
 110 #endif // ndef DTRACE_ENABLED
 111 
 112 // This exists only as a workaround of dtrace bug 6254741
 113 int dtrace_waited_probe(ObjectMonitor* monitor, Handle obj, Thread* thr) {
 114   DTRACE_MONITOR_PROBE(waited, monitor, obj(), thr);
 115   return 0;
 116 }
 117 
 118 #define NINFLATIONLOCKS 256
 119 static volatile intptr_t gInflationLocks[NINFLATIONLOCKS];
 120 
 121 // global list of blocks of monitors
 122 PaddedObjectMonitor* ObjectSynchronizer::g_block_list = NULL;
<span class="line-added"> 123 bool volatile ObjectSynchronizer::_is_async_deflation_requested = false;</span>
<span class="line-added"> 124 bool volatile ObjectSynchronizer::_is_special_deflation_requested = false;</span>
<span class="line-added"> 125 jlong ObjectSynchronizer::_last_async_deflation_time_ns = 0;</span>
 126 
 127 struct ObjectMonitorListGlobals {
 128   char         _pad_prefix[OM_CACHE_LINE_SIZE];
 129   // These are highly shared list related variables.
 130   // To avoid false-sharing they need to be the sole occupants of a cache line.
 131 
 132   // Global ObjectMonitor free list. Newly allocated and deflated
 133   // ObjectMonitors are prepended here.
 134   ObjectMonitor* _free_list;
 135   DEFINE_PAD_MINUS_SIZE(1, OM_CACHE_LINE_SIZE, sizeof(ObjectMonitor*));
 136 
 137   // Global ObjectMonitor in-use list. When a JavaThread is exiting,
 138   // ObjectMonitors on its per-thread in-use list are prepended here.
 139   ObjectMonitor* _in_use_list;
 140   DEFINE_PAD_MINUS_SIZE(2, OM_CACHE_LINE_SIZE, sizeof(ObjectMonitor*));
 141 
<span class="line-added"> 142   // Global ObjectMonitor wait list. Deflated ObjectMonitors wait on</span>
<span class="line-added"> 143   // this list until after a handshake or a safepoint for platforms</span>
<span class="line-added"> 144   // that don&#39;t support handshakes. After the handshake or safepoint,</span>
<span class="line-added"> 145   // the deflated ObjectMonitors are prepended to free_list.</span>
<span class="line-added"> 146   ObjectMonitor* _wait_list;</span>
<span class="line-added"> 147   DEFINE_PAD_MINUS_SIZE(3, OM_CACHE_LINE_SIZE, sizeof(ObjectMonitor*));</span>
<span class="line-added"> 148 </span>
 149   int _free_count;    // # on free_list
<span class="line-modified"> 150   DEFINE_PAD_MINUS_SIZE(4, OM_CACHE_LINE_SIZE, sizeof(int));</span>
 151 
 152   int _in_use_count;  // # on in_use_list
<span class="line-modified"> 153   DEFINE_PAD_MINUS_SIZE(5, OM_CACHE_LINE_SIZE, sizeof(int));</span>
 154 
 155   int _population;    // # Extant -- in circulation
<span class="line-modified"> 156   DEFINE_PAD_MINUS_SIZE(6, OM_CACHE_LINE_SIZE, sizeof(int));</span>
<span class="line-added"> 157 </span>
<span class="line-added"> 158   int _wait_count;    // # on wait_list</span>
<span class="line-added"> 159   DEFINE_PAD_MINUS_SIZE(7, OM_CACHE_LINE_SIZE, sizeof(int));</span>
 160 };
 161 static ObjectMonitorListGlobals om_list_globals;
 162 
 163 #define CHECK_THROW_NOSYNC_IMSE(obj)  \
 164   if ((obj)-&gt;mark().is_always_locked()) {  \
 165     ResourceMark rm(THREAD);                \
 166     THROW_MSG(vmSymbols::java_lang_IllegalMonitorStateException(), obj-&gt;klass()-&gt;external_name()); \
 167   }
 168 
 169 #define CHECK_THROW_NOSYNC_IMSE_0(obj)  \
 170     if ((obj)-&gt;mark().is_always_locked()) {  \
 171     ResourceMark rm(THREAD);                  \
 172     THROW_MSG_0(vmSymbols::java_lang_IllegalMonitorStateException(), obj-&gt;klass()-&gt;external_name()); \
 173   }
 174 
 175 
 176 #define CHAINMARKER (cast_to_oop&lt;intptr_t&gt;(-1))
 177 
 178 
 179 // =====================&gt; Spin-lock functions
</pre>
<hr />
<pre>
 310       Atomic::add(&amp;om_list_globals._population, _BLOCKSIZE - 1);
 311       break;
 312     }
 313     // Implied else: try it all again
 314   }
 315 
 316   // Second we handle om_list_globals._free_list:
 317   prepend_list_to_common(new_blk + 1, &amp;new_blk[_BLOCKSIZE - 1], _BLOCKSIZE - 1,
 318                          &amp;om_list_globals._free_list, &amp;om_list_globals._free_count);
 319 }
 320 
 321 // Prepend a list of ObjectMonitors to om_list_globals._free_list.
 322 // &#39;tail&#39; is the last ObjectMonitor in the list and there are &#39;count&#39;
 323 // on the list. Also updates om_list_globals._free_count.
 324 static void prepend_list_to_global_free_list(ObjectMonitor* list,
 325                                              ObjectMonitor* tail, int count) {
 326   prepend_list_to_common(list, tail, count, &amp;om_list_globals._free_list,
 327                          &amp;om_list_globals._free_count);
 328 }
 329 
<span class="line-added"> 330 // Prepend a list of ObjectMonitors to om_list_globals._wait_list.</span>
<span class="line-added"> 331 // &#39;tail&#39; is the last ObjectMonitor in the list and there are &#39;count&#39;</span>
<span class="line-added"> 332 // on the list. Also updates om_list_globals._wait_count.</span>
<span class="line-added"> 333 static void prepend_list_to_global_wait_list(ObjectMonitor* list,</span>
<span class="line-added"> 334                                              ObjectMonitor* tail, int count) {</span>
<span class="line-added"> 335   prepend_list_to_common(list, tail, count, &amp;om_list_globals._wait_list,</span>
<span class="line-added"> 336                          &amp;om_list_globals._wait_count);</span>
<span class="line-added"> 337 }</span>
<span class="line-added"> 338 </span>
 339 // Prepend a list of ObjectMonitors to om_list_globals._in_use_list.
 340 // &#39;tail&#39; is the last ObjectMonitor in the list and there are &#39;count&#39;
 341 // on the list. Also updates om_list_globals._in_use_list.
 342 static void prepend_list_to_global_in_use_list(ObjectMonitor* list,
 343                                                ObjectMonitor* tail, int count) {
 344   prepend_list_to_common(list, tail, count, &amp;om_list_globals._in_use_list,
 345                          &amp;om_list_globals._in_use_count);
 346 }
 347 
 348 // Prepend an ObjectMonitor to the specified list. Also updates
 349 // the specified counter.
 350 static void prepend_to_common(ObjectMonitor* m, ObjectMonitor** list_p,
 351                               int* count_p) {
 352   while (true) {
 353     om_lock(m);  // Lock m so we can safely update its next field.
 354     ObjectMonitor* cur = NULL;
 355     // Lock the list head to guard against races with a list walker
<span class="line-modified"> 356     // or async deflater thread (which only races in om_in_use_list):</span>
 357     if ((cur = get_list_head_locked(list_p)) != NULL) {
 358       // List head is now locked so we can safely switch it.
 359       m-&gt;set_next_om(cur);  // m now points to cur (and unlocks m)
 360       Atomic::store(list_p, m);  // Switch list head to unlocked m.
 361       om_unlock(cur);
 362       break;
 363     }
 364     // The list is empty so try to set the list head.
 365     assert(cur == NULL, &quot;cur must be NULL: cur=&quot; INTPTR_FORMAT, p2i(cur));
 366     m-&gt;set_next_om(cur);  // m now points to NULL (and unlocks m)
 367     if (Atomic::cmpxchg(list_p, cur, m) == cur) {
 368       // List head is now unlocked m.
 369       break;
 370     }
 371     // Implied else: try it all again
 372   }
 373   Atomic::inc(count_p);
 374 }
 375 
 376 // Prepend an ObjectMonitor to a per-thread om_free_list.
 377 // Also updates the per-thread om_free_count.
 378 static void prepend_to_om_free_list(Thread* self, ObjectMonitor* m) {
 379   prepend_to_common(m, &amp;self-&gt;om_free_list, &amp;self-&gt;om_free_count);
 380 }
 381 
 382 // Prepend an ObjectMonitor to a per-thread om_in_use_list.
 383 // Also updates the per-thread om_in_use_count.
 384 static void prepend_to_om_in_use_list(Thread* self, ObjectMonitor* m) {
 385   prepend_to_common(m, &amp;self-&gt;om_in_use_list, &amp;self-&gt;om_in_use_count);
 386 }
 387 
 388 // Take an ObjectMonitor from the start of the specified list. Also
 389 // decrements the specified counter. Returns NULL if none are available.
 390 static ObjectMonitor* take_from_start_of_common(ObjectMonitor** list_p,
 391                                                 int* count_p) {
 392   ObjectMonitor* take = NULL;
 393   // Lock the list head to guard against races with a list walker
<span class="line-modified"> 394   // or async deflater thread (which only races in om_list_globals._free_list):</span>
 395   if ((take = get_list_head_locked(list_p)) == NULL) {
 396     return NULL;  // None are available.
 397   }
 398   ObjectMonitor* next = unmarked_next(take);
 399   // Switch locked list head to next (which unlocks the list head, but
 400   // leaves take locked):
 401   Atomic::store(list_p, next);
 402   Atomic::dec(count_p);
 403   // Unlock take, but leave the next value for any lagging list
 404   // walkers. It will get cleaned up when take is prepended to
 405   // the in-use list:
 406   om_unlock(take);
 407   return take;
 408 }
 409 
 410 // Take an ObjectMonitor from the start of the om_list_globals._free_list.
 411 // Also updates om_list_globals._free_count. Returns NULL if none are
 412 // available.
 413 static ObjectMonitor* take_from_start_of_global_free_list() {
 414   return take_from_start_of_common(&amp;om_list_globals._free_list,
</pre>
<hr />
<pre>
 489 
 490 
 491 // The LockNode emitted directly at the synchronization site would have
 492 // been too big if it were to have included support for the cases of inflated
 493 // recursive enter and exit, so they go here instead.
 494 // Note that we can&#39;t safely call AsyncPrintJavaStack() from within
 495 // quick_enter() as our thread state remains _in_Java.
 496 
 497 bool ObjectSynchronizer::quick_enter(oop obj, Thread* self,
 498                                      BasicLock * lock) {
 499   assert(!SafepointSynchronize::is_at_safepoint(), &quot;invariant&quot;);
 500   assert(self-&gt;is_Java_thread(), &quot;invariant&quot;);
 501   assert(((JavaThread *) self)-&gt;thread_state() == _thread_in_Java, &quot;invariant&quot;);
 502   NoSafepointVerifier nsv;
 503   if (obj == NULL) return false;       // Need to throw NPE
 504   assert(!EnableValhalla || !obj-&gt;klass()-&gt;is_value(), &quot;monitor op on value type&quot;);
 505   const markWord mark = obj-&gt;mark();
 506 
 507   if (mark.has_monitor()) {
 508     ObjectMonitor* const m = mark.monitor();
<span class="line-modified"> 509     if (AsyncDeflateIdleMonitors) {</span>
<span class="line-added"> 510       // An async deflation can race us before we manage to make the</span>
<span class="line-added"> 511       // ObjectMonitor busy by setting the owner below. If we detect</span>
<span class="line-added"> 512       // that race we just bail out to the slow-path here.</span>
<span class="line-added"> 513       if (m-&gt;object() == NULL) {</span>
<span class="line-added"> 514         return false;</span>
<span class="line-added"> 515       }</span>
<span class="line-added"> 516     } else {</span>
<span class="line-added"> 517       assert(m-&gt;object() == obj, &quot;invariant&quot;);</span>
<span class="line-added"> 518     }</span>
 519     Thread* const owner = (Thread *) m-&gt;_owner;
 520 
 521     // Lock contention and Transactional Lock Elision (TLE) diagnostics
 522     // and observability
 523     // Case: light contention possibly amenable to TLE
 524     // Case: TLE inimical operations such as nested/recursive synchronization
 525 
 526     if (owner == self) {
 527       m-&gt;_recursions++;
 528       return true;
 529     }
 530 
 531     // This Java Monitor is inflated so obj&#39;s header will never be
 532     // displaced to this thread&#39;s BasicLock. Make the displaced header
 533     // non-NULL so this BasicLock is not seen as recursive nor as
 534     // being locked. We do this unconditionally so that this thread&#39;s
 535     // BasicLock cannot be mis-interpreted by any stack walkers. For
 536     // performance reasons, stack walkers generally first check for
 537     // Biased Locking in the object&#39;s header, the second check is for
 538     // stack-locking in the object&#39;s header, the third check is for
</pre>
<hr />
<pre>
 579     // Anticipate successful CAS -- the ST of the displaced mark must
 580     // be visible &lt;= the ST performed by the CAS.
 581     lock-&gt;set_displaced_header(mark);
 582     if (mark == obj()-&gt;cas_set_mark(markWord::from_pointer(lock), mark)) {
 583       return;
 584     }
 585     // Fall through to inflate() ...
 586   } else if (mark.has_locker() &amp;&amp;
 587              THREAD-&gt;is_lock_owned((address)mark.locker())) {
 588     assert(lock != mark.locker(), &quot;must not re-lock the same lock&quot;);
 589     assert(lock != (BasicLock*)obj-&gt;mark().value(), &quot;don&#39;t relock with same BasicLock&quot;);
 590     lock-&gt;set_displaced_header(markWord::from_pointer(NULL));
 591     return;
 592   }
 593 
 594   // The object header will never be displaced to this lock,
 595   // so it does not matter what the value is, except that it
 596   // must be non-zero to avoid looking like a re-entrant lock,
 597   // and must not look locked either.
 598   lock-&gt;set_displaced_header(markWord::unused_mark());
<span class="line-modified"> 599   // An async deflation can race after the inflate() call and before</span>
<span class="line-added"> 600   // enter() can make the ObjectMonitor busy. enter() returns false if</span>
<span class="line-added"> 601   // we have lost the race to async deflation and we simply try again.</span>
<span class="line-added"> 602   while (true) {</span>
<span class="line-added"> 603     ObjectMonitor* monitor = inflate(THREAD, obj(), inflate_cause_monitor_enter);</span>
<span class="line-added"> 604     if (monitor-&gt;enter(THREAD)) {</span>
<span class="line-added"> 605       return;</span>
<span class="line-added"> 606     }</span>
<span class="line-added"> 607   }</span>
 608 }
 609 
 610 void ObjectSynchronizer::exit(oop object, BasicLock* lock, TRAPS) {
 611   markWord mark = object-&gt;mark();
 612   if (EnableValhalla &amp;&amp; mark.is_always_locked()) {
 613     return;
 614   }
 615   assert(!EnableValhalla || !object-&gt;klass()-&gt;is_value(), &quot;monitor op on value type&quot;);
 616   // We cannot check for Biased Locking if we are racing an inflation.
 617   assert(mark == markWord::INFLATING() ||
 618          !mark.has_bias_pattern(), &quot;should not see bias pattern here&quot;);
 619 
 620   markWord dhw = lock-&gt;displaced_header();
 621   if (dhw.value() == 0) {
 622     // If the displaced header is NULL, then this exit matches up with
 623     // a recursive enter. No real work to do here except for diagnostics.
 624 #ifndef PRODUCT
 625     if (mark != markWord::INFLATING()) {
 626       // Only do diagnostics if we are not racing an inflation. Simply
 627       // exiting a recursive enter of a Java Monitor that is being
</pre>
<hr />
<pre>
 640         // does not own the Java Monitor.
 641         ObjectMonitor* m = mark.monitor();
 642         assert(((oop)(m-&gt;object()))-&gt;mark() == mark, &quot;invariant&quot;);
 643         assert(m-&gt;is_entered(THREAD), &quot;invariant&quot;);
 644       }
 645     }
 646 #endif
 647     return;
 648   }
 649 
 650   if (mark == markWord::from_pointer(lock)) {
 651     // If the object is stack-locked by the current thread, try to
 652     // swing the displaced header from the BasicLock back to the mark.
 653     assert(dhw.is_neutral(), &quot;invariant&quot;);
 654     if (object-&gt;cas_set_mark(dhw, mark) == mark) {
 655       return;
 656     }
 657   }
 658 
 659   // We have to take the slow-path of possible inflation and then exit.
<span class="line-modified"> 660   // The ObjectMonitor* can&#39;t be async deflated until ownership is</span>
<span class="line-added"> 661   // dropped inside exit() and the ObjectMonitor* must be !is_busy().</span>
<span class="line-added"> 662   ObjectMonitor* monitor = inflate(THREAD, object, inflate_cause_vm_internal);</span>
<span class="line-added"> 663   monitor-&gt;exit(true, THREAD);</span>
 664 }
 665 
 666 // -----------------------------------------------------------------------------
 667 // Class Loader  support to workaround deadlocks on the class loader lock objects
 668 // Also used by GC
 669 // complete_exit()/reenter() are used to wait on a nested lock
 670 // i.e. to give up an outer lock completely and then re-enter
 671 // Used when holding nested locks - lock acquisition order: lock1 then lock2
 672 //  1) complete_exit lock1 - saving recursion count
 673 //  2) wait on lock2
 674 //  3) when notified on lock2, unlock lock2
 675 //  4) reenter lock1 with original recursion count
 676 //  5) lock lock2
 677 // NOTE: must use heavy weight monitor to handle complete_exit/reenter()
 678 intx ObjectSynchronizer::complete_exit(Handle obj, TRAPS) {
 679   assert(!EnableValhalla || !obj-&gt;klass()-&gt;is_value(), &quot;monitor op on value type&quot;);
 680   if (UseBiasedLocking) {
 681     BiasedLocking::revoke(obj, THREAD);
 682     assert(!obj-&gt;mark().has_bias_pattern(), &quot;biases should be revoked by now&quot;);
 683   }
 684 
<span class="line-added"> 685   // The ObjectMonitor* can&#39;t be async deflated until ownership is</span>
<span class="line-added"> 686   // dropped inside exit() and the ObjectMonitor* must be !is_busy().</span>
 687   ObjectMonitor* monitor = inflate(THREAD, obj(), inflate_cause_vm_internal);
<span class="line-modified"> 688   intptr_t ret_code = monitor-&gt;complete_exit(THREAD);</span>
<span class="line-modified"> 689   return ret_code;</span>
 690 }
 691 
 692 // NOTE: must use heavy weight monitor to handle complete_exit/reenter()
 693 void ObjectSynchronizer::reenter(Handle obj, intx recursions, TRAPS) {
 694   assert(!EnableValhalla || !obj-&gt;klass()-&gt;is_value(), &quot;monitor op on value type&quot;);
 695   if (UseBiasedLocking) {
 696     BiasedLocking::revoke(obj, THREAD);
 697     assert(!obj-&gt;mark().has_bias_pattern(), &quot;biases should be revoked by now&quot;);
 698   }
 699 
<span class="line-modified"> 700   // An async deflation can race after the inflate() call and before</span>
<span class="line-modified"> 701   // reenter() -&gt; enter() can make the ObjectMonitor busy. reenter() -&gt;</span>
<span class="line-modified"> 702   // enter() returns false if we have lost the race to async deflation</span>
<span class="line-added"> 703   // and we simply try again.</span>
<span class="line-added"> 704   while (true) {</span>
<span class="line-added"> 705     ObjectMonitor* monitor = inflate(THREAD, obj(), inflate_cause_vm_internal);</span>
<span class="line-added"> 706     if (monitor-&gt;reenter(recursions, THREAD)) {</span>
<span class="line-added"> 707       return;</span>
<span class="line-added"> 708     }</span>
<span class="line-added"> 709   }</span>
 710 }
<span class="line-added"> 711 </span>
 712 // -----------------------------------------------------------------------------
 713 // JNI locks on java objects
 714 // NOTE: must use heavy weight monitor to handle jni monitor enter
 715 void ObjectSynchronizer::jni_enter(Handle obj, TRAPS) {
 716   // the current locking is from JNI instead of Java code
 717   CHECK_THROW_NOSYNC_IMSE(obj);
 718   if (UseBiasedLocking) {
 719     BiasedLocking::revoke(obj, THREAD);
 720     assert(!obj-&gt;mark().has_bias_pattern(), &quot;biases should be revoked by now&quot;);
 721   }
 722   THREAD-&gt;set_current_pending_monitor_is_from_java(false);
<span class="line-modified"> 723   // An async deflation can race after the inflate() call and before</span>
<span class="line-added"> 724   // enter() can make the ObjectMonitor busy. enter() returns false if</span>
<span class="line-added"> 725   // we have lost the race to async deflation and we simply try again.</span>
<span class="line-added"> 726   while (true) {</span>
<span class="line-added"> 727     ObjectMonitor* monitor = inflate(THREAD, obj(), inflate_cause_jni_enter);</span>
<span class="line-added"> 728     if (monitor-&gt;enter(THREAD)) {</span>
<span class="line-added"> 729       break;</span>
<span class="line-added"> 730     }</span>
<span class="line-added"> 731   }</span>
 732   THREAD-&gt;set_current_pending_monitor_is_from_java(true);
 733 }
 734 
 735 // NOTE: must use heavy weight monitor to handle jni monitor exit
 736 void ObjectSynchronizer::jni_exit(oop obj, Thread* THREAD) {
 737   CHECK_THROW_NOSYNC_IMSE(obj);
 738   if (UseBiasedLocking) {
 739     Handle h_obj(THREAD, obj);
 740     BiasedLocking::revoke(h_obj, THREAD);
 741     obj = h_obj();
 742   }
 743   assert(!obj-&gt;mark().has_bias_pattern(), &quot;biases should be revoked by now&quot;);
 744 
<span class="line-added"> 745   // The ObjectMonitor* can&#39;t be async deflated until ownership is</span>
<span class="line-added"> 746   // dropped inside exit() and the ObjectMonitor* must be !is_busy().</span>
 747   ObjectMonitor* monitor = inflate(THREAD, obj, inflate_cause_jni_exit);
 748   // If this thread has locked the object, exit the monitor. We
 749   // intentionally do not use CHECK here because we must exit the
 750   // monitor even if an exception is pending.
 751   if (monitor-&gt;check_owner(THREAD)) {
 752     monitor-&gt;exit(true, THREAD);
 753   }
 754 }
 755 
 756 // -----------------------------------------------------------------------------
 757 // Internal VM locks on java objects
 758 // standard constructor, allows locking failures
 759 ObjectLocker::ObjectLocker(Handle obj, Thread* thread, bool do_lock) {
 760   _dolock = do_lock;
 761   _thread = thread;
 762   _thread-&gt;check_for_valid_safepoint_state();
 763   _obj = obj;
 764 
 765   if (_dolock) {
 766     ObjectSynchronizer::enter(_obj, &amp;_lock, _thread);
</pre>
<hr />
<pre>
 769 
 770 ObjectLocker::~ObjectLocker() {
 771   if (_dolock) {
 772     ObjectSynchronizer::exit(_obj(), &amp;_lock, _thread);
 773   }
 774 }
 775 
 776 
 777 // -----------------------------------------------------------------------------
 778 //  Wait/Notify/NotifyAll
 779 // NOTE: must use heavy weight monitor to handle wait()
 780 int ObjectSynchronizer::wait(Handle obj, jlong millis, TRAPS) {
 781   CHECK_THROW_NOSYNC_IMSE_0(obj);
 782   if (UseBiasedLocking) {
 783     BiasedLocking::revoke(obj, THREAD);
 784     assert(!obj-&gt;mark().has_bias_pattern(), &quot;biases should be revoked by now&quot;);
 785   }
 786   if (millis &lt; 0) {
 787     THROW_MSG_0(vmSymbols::java_lang_IllegalArgumentException(), &quot;timeout value is negative&quot;);
 788   }
<span class="line-added"> 789   // The ObjectMonitor* can&#39;t be async deflated because the _waiters</span>
<span class="line-added"> 790   // field is incremented before ownership is dropped and decremented</span>
<span class="line-added"> 791   // after ownership is regained.</span>
 792   ObjectMonitor* monitor = inflate(THREAD, obj(), inflate_cause_wait);
 793 
 794   DTRACE_MONITOR_WAIT_PROBE(monitor, obj(), THREAD, millis);
 795   monitor-&gt;wait(millis, true, THREAD);
 796 
 797   // This dummy call is in place to get around dtrace bug 6254741.  Once
 798   // that&#39;s fixed we can uncomment the following line, remove the call
 799   // and change this function back into a &quot;void&quot; func.
 800   // DTRACE_MONITOR_PROBE(waited, monitor, obj(), THREAD);
<span class="line-modified"> 801   int ret_code = dtrace_waited_probe(monitor, obj, THREAD);</span>
<span class="line-added"> 802   return ret_code;</span>
 803 }
 804 
 805 void ObjectSynchronizer::wait_uninterruptibly(Handle obj, jlong millis, TRAPS) {
 806   CHECK_THROW_NOSYNC_IMSE(obj);
 807   if (UseBiasedLocking) {
 808     BiasedLocking::revoke(obj, THREAD);
 809     assert(!obj-&gt;mark().has_bias_pattern(), &quot;biases should be revoked by now&quot;);
 810   }
 811   if (millis &lt; 0) {
 812     THROW_MSG(vmSymbols::java_lang_IllegalArgumentException(), &quot;timeout value is negative&quot;);
 813   }
<span class="line-modified"> 814   // The ObjectMonitor* can&#39;t be async deflated because the _waiters</span>
<span class="line-added"> 815   // field is incremented before ownership is dropped and decremented</span>
<span class="line-added"> 816   // after ownership is regained.</span>
<span class="line-added"> 817   ObjectMonitor* monitor = inflate(THREAD, obj(), inflate_cause_wait);</span>
<span class="line-added"> 818   monitor-&gt;wait(millis, false, THREAD);</span>
 819 }
 820 
 821 void ObjectSynchronizer::notify(Handle obj, TRAPS) {
 822   CHECK_THROW_NOSYNC_IMSE(obj);
 823   if (UseBiasedLocking) {
 824     BiasedLocking::revoke(obj, THREAD);
 825     assert(!obj-&gt;mark().has_bias_pattern(), &quot;biases should be revoked by now&quot;);
 826   }
 827 
 828   markWord mark = obj-&gt;mark();
 829   if (mark.has_locker() &amp;&amp; THREAD-&gt;is_lock_owned((address)mark.locker())) {
 830     return;
 831   }
<span class="line-modified"> 832   // The ObjectMonitor* can&#39;t be async deflated until ownership is</span>
<span class="line-added"> 833   // dropped by the calling thread.</span>
<span class="line-added"> 834   ObjectMonitor* monitor = inflate(THREAD, obj(), inflate_cause_notify);</span>
<span class="line-added"> 835   monitor-&gt;notify(THREAD);</span>
 836 }
 837 
 838 // NOTE: see comment of notify()
 839 void ObjectSynchronizer::notifyall(Handle obj, TRAPS) {
 840   CHECK_THROW_NOSYNC_IMSE(obj);
 841   if (UseBiasedLocking) {
 842     BiasedLocking::revoke(obj, THREAD);
 843     assert(!obj-&gt;mark().has_bias_pattern(), &quot;biases should be revoked by now&quot;);
 844   }
 845 
 846   markWord mark = obj-&gt;mark();
 847   if (mark.has_locker() &amp;&amp; THREAD-&gt;is_lock_owned((address)mark.locker())) {
 848     return;
 849   }
<span class="line-modified"> 850   // The ObjectMonitor* can&#39;t be async deflated until ownership is</span>
<span class="line-added"> 851   // dropped by the calling thread.</span>
<span class="line-added"> 852   ObjectMonitor* monitor = inflate(THREAD, obj(), inflate_cause_notify);</span>
<span class="line-added"> 853   monitor-&gt;notifyAll(THREAD);</span>
 854 }
 855 
 856 // -----------------------------------------------------------------------------
 857 // Hash Code handling
 858 //
 859 // Performance concern:
 860 // OrderAccess::storestore() calls release() which at one time stored 0
 861 // into the global volatile OrderAccess::dummy variable. This store was
 862 // unnecessary for correctness. Many threads storing into a common location
 863 // causes considerable cache migration or &quot;sloshing&quot; on large SMP systems.
 864 // As such, I avoided using OrderAccess::storestore(). In some cases
 865 // OrderAccess::fence() -- which incurs local latency on the executing
 866 // processor -- is a better choice as it scales on SMP systems.
 867 //
 868 // See http://blogs.oracle.com/dave/entry/biased_locking_in_hotspot for
 869 // a discussion of coherency costs. Note that all our current reference
 870 // platforms provide strong ST-ST order, so the issue is moot on IA32,
 871 // x64, and SPARC.
 872 //
 873 // As a general policy we use &quot;volatile&quot; to control compiler-based reordering
</pre>
<hr />
<pre>
1033       Handle hobj(self, obj);
1034       // Relaxing assertion for bug 6320749.
1035       assert(Universe::verify_in_progress() ||
1036              !SafepointSynchronize::is_at_safepoint(),
1037              &quot;biases should not be seen by VM thread here&quot;);
1038       BiasedLocking::revoke(hobj, JavaThread::current());
1039       obj = hobj();
1040       assert(!obj-&gt;mark().has_bias_pattern(), &quot;biases should be revoked by now&quot;);
1041     }
1042   }
1043 
1044   // hashCode() is a heap mutator ...
1045   // Relaxing assertion for bug 6320749.
1046   assert(Universe::verify_in_progress() || DumpSharedSpaces ||
1047          !SafepointSynchronize::is_at_safepoint(), &quot;invariant&quot;);
1048   assert(Universe::verify_in_progress() || DumpSharedSpaces ||
1049          self-&gt;is_Java_thread() , &quot;invariant&quot;);
1050   assert(Universe::verify_in_progress() || DumpSharedSpaces ||
1051          ((JavaThread *)self)-&gt;thread_state() != _thread_blocked, &quot;invariant&quot;);
1052 
<span class="line-modified">1053   while (true) {</span>
<span class="line-modified">1054     ObjectMonitor* monitor = NULL;</span>
<span class="line-modified">1055     markWord temp, test;</span>
<span class="line-modified">1056     intptr_t hash;</span>
<span class="line-added">1057     markWord mark = read_stable_mark(obj);</span>
<span class="line-added">1058 </span>
<span class="line-added">1059     // object should remain ineligible for biased locking</span>
<span class="line-added">1060     assert(!mark.has_bias_pattern(), &quot;invariant&quot;);</span>
<span class="line-added">1061 </span>
<span class="line-added">1062     if (mark.is_neutral()) {            // if this is a normal header</span>
<span class="line-added">1063       hash = mark.hash();</span>
<span class="line-added">1064       if (hash != 0) {                  // if it has a hash, just return it</span>
<span class="line-added">1065         return hash;</span>
<span class="line-added">1066       }</span>
<span class="line-added">1067       hash = get_next_hash(self, obj);  // get a new hash</span>
<span class="line-added">1068       temp = mark.copy_set_hash(hash);  // merge the hash into header</span>
<span class="line-added">1069                                         // try to install the hash</span>
<span class="line-added">1070       test = obj-&gt;cas_set_mark(temp, mark);</span>
<span class="line-added">1071       if (test == mark) {               // if the hash was installed, return it</span>
<span class="line-added">1072         return hash;</span>
<span class="line-added">1073       }</span>
<span class="line-added">1074       // Failed to install the hash. It could be that another thread</span>
<span class="line-added">1075       // installed the hash just before our attempt or inflation has</span>
<span class="line-added">1076       // occurred or... so we fall thru to inflate the monitor for</span>
<span class="line-added">1077       // stability and then install the hash.</span>
<span class="line-added">1078     } else if (mark.has_monitor()) {</span>
<span class="line-added">1079       monitor = mark.monitor();</span>
<span class="line-added">1080       temp = monitor-&gt;header();</span>
<span class="line-added">1081       assert(temp.is_neutral(), &quot;invariant: header=&quot; INTPTR_FORMAT, temp.value());</span>
<span class="line-added">1082       hash = temp.hash();</span>
<span class="line-added">1083       if (hash != 0) {</span>
<span class="line-added">1084         // It has a hash.</span>
<span class="line-added">1085 </span>
<span class="line-added">1086         // Separate load of dmw/header above from the loads in</span>
<span class="line-added">1087         // is_being_async_deflated().</span>
<span class="line-added">1088         if (support_IRIW_for_not_multiple_copy_atomic_cpu) {</span>
<span class="line-added">1089           // A non-multiple copy atomic (nMCA) machine needs a bigger</span>
<span class="line-added">1090           // hammer to separate the load above and the loads below.</span>
<span class="line-added">1091           OrderAccess::fence();</span>
<span class="line-added">1092         } else {</span>
<span class="line-added">1093           OrderAccess::loadload();</span>
<span class="line-added">1094         }</span>
<span class="line-added">1095         if (monitor-&gt;is_being_async_deflated()) {</span>
<span class="line-added">1096           // But we can&#39;t safely use the hash if we detect that async</span>
<span class="line-added">1097           // deflation has occurred. So we attempt to restore the</span>
<span class="line-added">1098           // header/dmw to the object&#39;s header so that we only retry</span>
<span class="line-added">1099           // once if the deflater thread happens to be slow.</span>
<span class="line-added">1100           monitor-&gt;install_displaced_markword_in_object(obj);</span>
<span class="line-added">1101           continue;</span>
<span class="line-added">1102         }</span>
<span class="line-added">1103         return hash;</span>
<span class="line-added">1104       }</span>
<span class="line-added">1105       // Fall thru so we only have one place that installs the hash in</span>
<span class="line-added">1106       // the ObjectMonitor.</span>
<span class="line-added">1107     } else if (self-&gt;is_lock_owned((address)mark.locker())) {</span>
<span class="line-added">1108       // This is a stack lock owned by the calling thread so fetch the</span>
<span class="line-added">1109       // displaced markWord from the BasicLock on the stack.</span>
<span class="line-added">1110       temp = mark.displaced_mark_helper();</span>
<span class="line-added">1111       assert(temp.is_neutral(), &quot;invariant: header=&quot; INTPTR_FORMAT, temp.value());</span>
<span class="line-added">1112       hash = temp.hash();</span>
<span class="line-added">1113       if (hash != 0) {                  // if it has a hash, just return it</span>
<span class="line-added">1114         return hash;</span>
<span class="line-added">1115       }</span>
<span class="line-added">1116       // WARNING:</span>
<span class="line-added">1117       // The displaced header in the BasicLock on a thread&#39;s stack</span>
<span class="line-added">1118       // is strictly immutable. It CANNOT be changed in ANY cases.</span>
<span class="line-added">1119       // So we have to inflate the stack lock into an ObjectMonitor</span>
<span class="line-added">1120       // even if the current thread owns the lock. The BasicLock on</span>
<span class="line-added">1121       // a thread&#39;s stack can be asynchronously read by other threads</span>
<span class="line-added">1122       // during an inflate() call so any change to that stack memory</span>
<span class="line-added">1123       // may not propagate to other threads correctly.</span>
<span class="line-added">1124     }</span>
1125 
<span class="line-modified">1126     // Inflate the monitor to set the hash.</span>

1127 
<span class="line-modified">1128     // An async deflation can race after the inflate() call and before we</span>
<span class="line-added">1129     // can update the ObjectMonitor&#39;s header with the hash value below.</span>
<span class="line-added">1130     monitor = inflate(self, obj, inflate_cause_hash_code);</span>
<span class="line-added">1131     // Load ObjectMonitor&#39;s header/dmw field and see if it has a hash.</span>
<span class="line-added">1132     mark = monitor-&gt;header();</span>
<span class="line-added">1133     assert(mark.is_neutral(), &quot;invariant: header=&quot; INTPTR_FORMAT, mark.value());</span>
1134     hash = mark.hash();
<span class="line-modified">1135     if (hash == 0) {                    // if it does not have a hash</span>
<span class="line-modified">1136       hash = get_next_hash(self, obj);  // get a new hash</span>
<span class="line-modified">1137       temp = mark.copy_set_hash(hash);  // merge the hash into header</span>
<span class="line-modified">1138       assert(temp.is_neutral(), &quot;invariant: header=&quot; INTPTR_FORMAT, temp.value());</span>
<span class="line-modified">1139       uintptr_t v = Atomic::cmpxchg((volatile uintptr_t*)monitor-&gt;header_addr(), mark.value(), temp.value());</span>
<span class="line-modified">1140       test = markWord(v);</span>
<span class="line-modified">1141       if (test != mark) {</span>
<span class="line-modified">1142         // The attempt to update the ObjectMonitor&#39;s header/dmw field</span>
<span class="line-modified">1143         // did not work. This can happen if another thread managed to</span>
<span class="line-modified">1144         // merge in the hash just before our cmpxchg().</span>
<span class="line-modified">1145         // If we add any new usages of the header/dmw field, this code</span>
<span class="line-modified">1146         // will need to be updated.</span>
<span class="line-modified">1147         hash = test.hash();</span>
<span class="line-modified">1148         assert(test.is_neutral(), &quot;invariant: header=&quot; INTPTR_FORMAT, test.value());</span>
<span class="line-modified">1149         assert(hash != 0, &quot;should only have lost the race to a thread that set a non-zero hash&quot;);</span>
<span class="line-modified">1150       }</span>
<span class="line-modified">1151       if (monitor-&gt;is_being_async_deflated()) {</span>
<span class="line-modified">1152         // If we detect that async deflation has occurred, then we</span>
<span class="line-modified">1153         // attempt to restore the header/dmw to the object&#39;s header</span>
<span class="line-modified">1154         // so that we only retry once if the deflater thread happens</span>
<span class="line-modified">1155         // to be slow.</span>
<span class="line-modified">1156         monitor-&gt;install_displaced_markword_in_object(obj);</span>
<span class="line-modified">1157         continue;</span>
<span class="line-modified">1158       }</span>
<span class="line-modified">1159     }</span>
<span class="line-modified">1160     // We finally get the hash.</span>
<span class="line-modified">1161     return hash;</span>
<span class="line-modified">1162   }</span>








































1163 }
1164 
1165 
1166 bool ObjectSynchronizer::current_thread_holds_lock(JavaThread* thread,
1167                                                    Handle h_obj) {
1168   if (EnableValhalla &amp;&amp; h_obj-&gt;mark().is_always_locked()) {
1169     return false;
1170   }
1171   if (UseBiasedLocking) {
1172     BiasedLocking::revoke(h_obj, thread);
1173     assert(!h_obj-&gt;mark().has_bias_pattern(), &quot;biases should be revoked by now&quot;);
1174   }
1175 
1176   assert(thread == JavaThread::current(), &quot;Can only be called on current thread&quot;);
1177   oop obj = h_obj();
1178 
1179   markWord mark = read_stable_mark(obj);
1180 
1181   // Uncontended case, header points to stack
1182   if (mark.has_locker()) {
1183     return thread-&gt;is_lock_owned((address)mark.locker());
1184   }
1185   // Contended case, header points to ObjectMonitor (tagged pointer)
1186   if (mark.has_monitor()) {
<span class="line-added">1187     // The first stage of async deflation does not affect any field</span>
<span class="line-added">1188     // used by this comparison so the ObjectMonitor* is usable here.</span>
1189     ObjectMonitor* monitor = mark.monitor();
1190     return monitor-&gt;is_entered(thread) != 0;
1191   }
1192   // Unlocked case, header in place
1193   assert(mark.is_neutral(), &quot;sanity check&quot;);
1194   return false;
1195 }
1196 
1197 // Be aware of this method could revoke bias of the lock object.
1198 // This method queries the ownership of the lock handle specified by &#39;h_obj&#39;.
1199 // If the current thread owns the lock, it returns owner_self. If no
1200 // thread owns the lock, it returns owner_none. Otherwise, it will return
1201 // owner_other.
1202 ObjectSynchronizer::LockOwnership ObjectSynchronizer::query_lock_ownership
1203 (JavaThread *self, Handle h_obj) {
1204   // The caller must beware this method can revoke bias, and
1205   // revocation can result in a safepoint.
1206   assert(!SafepointSynchronize::is_at_safepoint(), &quot;invariant&quot;);
1207   assert(self-&gt;thread_state() != _thread_blocked, &quot;invariant&quot;);
1208 
</pre>
<hr />
<pre>
1210 
1211   if (UseBiasedLocking &amp;&amp; h_obj()-&gt;mark().has_bias_pattern()) {
1212     // CASE: biased
1213     BiasedLocking::revoke(h_obj, self);
1214     assert(!h_obj-&gt;mark().has_bias_pattern(),
1215            &quot;biases should be revoked by now&quot;);
1216   }
1217 
1218   assert(self == JavaThread::current(), &quot;Can only be called on current thread&quot;);
1219   oop obj = h_obj();
1220   markWord mark = read_stable_mark(obj);
1221 
1222   // CASE: stack-locked.  Mark points to a BasicLock on the owner&#39;s stack.
1223   if (mark.has_locker()) {
1224     return self-&gt;is_lock_owned((address)mark.locker()) ?
1225       owner_self : owner_other;
1226   }
1227 
1228   // CASE: inflated. Mark (tagged pointer) points to an ObjectMonitor.
1229   // The Object:ObjectMonitor relationship is stable as long as we&#39;re
<span class="line-modified">1230   // not at a safepoint and AsyncDeflateIdleMonitors is false.</span>
1231   if (mark.has_monitor()) {
<span class="line-modified">1232     // The first stage of async deflation does not affect any field</span>
<span class="line-added">1233     // used by this comparison so the ObjectMonitor* is usable here.</span>
<span class="line-added">1234     ObjectMonitor* monitor = mark.monitor();</span>
<span class="line-added">1235     void* owner = monitor-&gt;owner();</span>
1236     if (owner == NULL) return owner_none;
1237     return (owner == self ||
1238             self-&gt;is_lock_owned((address)owner)) ? owner_self : owner_other;
1239   }
1240 
1241   // CASE: neutral
1242   assert(mark.is_neutral(), &quot;sanity check&quot;);
1243   return owner_none;           // it&#39;s unlocked
1244 }
1245 
1246 // FIXME: jvmti should call this
1247 JavaThread* ObjectSynchronizer::get_lock_owner(ThreadsList * t_list, Handle h_obj) {
1248   if (UseBiasedLocking) {
1249     if (SafepointSynchronize::is_at_safepoint()) {
1250       BiasedLocking::revoke_at_safepoint(h_obj);
1251     } else {
1252       BiasedLocking::revoke(h_obj, JavaThread::current());
1253     }
1254     assert(!h_obj-&gt;mark().has_bias_pattern(), &quot;biases should be revoked by now&quot;);
1255   }
1256 
1257   oop obj = h_obj();
1258   address owner = NULL;
1259 
1260   markWord mark = read_stable_mark(obj);
1261 
1262   // Uncontended case, header points to stack
1263   if (mark.has_locker()) {
1264     owner = (address) mark.locker();
1265   }
1266 
1267   // Contended case, header points to ObjectMonitor (tagged pointer)
1268   else if (mark.has_monitor()) {
<span class="line-added">1269     // The first stage of async deflation does not affect any field</span>
<span class="line-added">1270     // used by this comparison so the ObjectMonitor* is usable here.</span>
1271     ObjectMonitor* monitor = mark.monitor();
1272     assert(monitor != NULL, &quot;monitor should be non-null&quot;);
1273     owner = (address) monitor-&gt;owner();
1274   }
1275 
1276   if (owner != NULL) {
1277     // owning_thread_from_monitor_owner() may also return NULL here
1278     return Threads::owning_thread_from_monitor_owner(t_list, owner);
1279   }
1280 
1281   // Unlocked case, header in place
1282   // Cannot have assertion since this object may have been
1283   // locked by another thread when reaching here.
1284   // assert(mark.is_neutral(), &quot;sanity check&quot;);
1285 
1286   return NULL;
1287 }
1288 
1289 // Visitors ...
1290 
1291 void ObjectSynchronizer::monitors_iterate(MonitorClosure* closure) {
1292   PaddedObjectMonitor* block = Atomic::load(&amp;g_block_list);
1293   while (block != NULL) {
1294     assert(block-&gt;object() == CHAINMARKER, &quot;must be a block header&quot;);
1295     for (int i = _BLOCKSIZE - 1; i &gt; 0; i--) {
1296       ObjectMonitor* mid = (ObjectMonitor *)(block + i);
<span class="line-modified">1297       if (mid-&gt;object() != NULL) {</span>

1298         // Only process with closure if the object is set.
<span class="line-added">1299 </span>
<span class="line-added">1300         // monitors_iterate() is only called at a safepoint or when the</span>
<span class="line-added">1301         // target thread is suspended or when the target thread is</span>
<span class="line-added">1302         // operating on itself. The current closures in use today are</span>
<span class="line-added">1303         // only interested in an owned ObjectMonitor and ownership</span>
<span class="line-added">1304         // cannot be dropped under the calling contexts so the</span>
<span class="line-added">1305         // ObjectMonitor cannot be async deflated.</span>
1306         closure-&gt;do_monitor(mid);
1307       }
1308     }
1309     // unmarked_next() is not needed with g_block_list (no locking
1310     // used with block linkage _next_om fields).
1311     block = (PaddedObjectMonitor*)block-&gt;next_om();
1312   }
1313 }
1314 
1315 static bool monitors_used_above_threshold() {
1316   int population = Atomic::load(&amp;om_list_globals._population);
1317   if (population == 0) {
1318     return false;
1319   }
1320   if (MonitorUsedDeflationThreshold &gt; 0) {
<span class="line-modified">1321     int monitors_used = population - Atomic::load(&amp;om_list_globals._free_count) -</span>
<span class="line-added">1322                         Atomic::load(&amp;om_list_globals._wait_count);</span>
1323     int monitor_usage = (monitors_used * 100LL) / population;
1324     return monitor_usage &gt; MonitorUsedDeflationThreshold;
1325   }
1326   return false;
1327 }
1328 
<span class="line-modified">1329 bool ObjectSynchronizer::is_async_deflation_needed() {</span>
<span class="line-modified">1330   if (!AsyncDeflateIdleMonitors) {</span>
<span class="line-added">1331     return false;</span>
<span class="line-added">1332   }</span>
<span class="line-added">1333   if (is_async_deflation_requested()) {</span>
<span class="line-added">1334     // Async deflation request.</span>
<span class="line-added">1335     return true;</span>
<span class="line-added">1336   }</span>
<span class="line-added">1337   if (AsyncDeflationInterval &gt; 0 &amp;&amp;</span>
<span class="line-added">1338       time_since_last_async_deflation_ms() &gt; AsyncDeflationInterval &amp;&amp;</span>
<span class="line-added">1339       monitors_used_above_threshold()) {</span>
<span class="line-added">1340     // It&#39;s been longer than our specified deflate interval and there</span>
<span class="line-added">1341     // are too many monitors in use. We don&#39;t deflate more frequently</span>
<span class="line-added">1342     // than AsyncDeflationInterval (unless is_async_deflation_requested)</span>
<span class="line-added">1343     // in order to not swamp the ServiceThread.</span>
<span class="line-added">1344     _last_async_deflation_time_ns = os::javaTimeNanos();</span>
<span class="line-added">1345     return true;</span>
<span class="line-added">1346   }</span>
<span class="line-added">1347   return false;</span>
<span class="line-added">1348 }</span>
<span class="line-added">1349 </span>
<span class="line-added">1350 bool ObjectSynchronizer::is_safepoint_deflation_needed() {</span>
<span class="line-added">1351   if (!AsyncDeflateIdleMonitors) {</span>
<span class="line-added">1352     if (monitors_used_above_threshold()) {</span>
<span class="line-added">1353       // Too many monitors in use.</span>
<span class="line-added">1354       return true;</span>
<span class="line-added">1355     }</span>
<span class="line-added">1356     return false;</span>
<span class="line-added">1357   }</span>
<span class="line-added">1358   if (is_special_deflation_requested()) {</span>
<span class="line-added">1359     // For AsyncDeflateIdleMonitors only do a safepoint deflation</span>
<span class="line-added">1360     // if there is a special deflation request.</span>
<span class="line-added">1361     return true;</span>
<span class="line-added">1362   }</span>
<span class="line-added">1363   return false;</span>
<span class="line-added">1364 }</span>
<span class="line-added">1365 </span>
<span class="line-added">1366 jlong ObjectSynchronizer::time_since_last_async_deflation_ms() {</span>
<span class="line-added">1367   return (os::javaTimeNanos() - _last_async_deflation_time_ns) / (NANOUNITS / MILLIUNITS);</span>
1368 }
1369 
1370 void ObjectSynchronizer::oops_do(OopClosure* f) {
1371   // We only scan the global used list here (for moribund threads), and
1372   // the thread-local monitors in Thread::oops_do().
1373   global_used_oops_do(f);
1374 }
1375 
1376 void ObjectSynchronizer::global_used_oops_do(OopClosure* f) {
1377   assert(SafepointSynchronize::is_at_safepoint(), &quot;must be at safepoint&quot;);
1378   list_oops_do(Atomic::load(&amp;om_list_globals._in_use_list), f);
1379 }
1380 
1381 void ObjectSynchronizer::thread_local_used_oops_do(Thread* thread, OopClosure* f) {
1382   assert(SafepointSynchronize::is_at_safepoint(), &quot;must be at safepoint&quot;);
1383   list_oops_do(thread-&gt;om_in_use_list, f);
1384 }
1385 
1386 void ObjectSynchronizer::list_oops_do(ObjectMonitor* list, OopClosure* f) {
1387   assert(SafepointSynchronize::is_at_safepoint(), &quot;must be at safepoint&quot;);
1388   // The oops_do() phase does not overlap with monitor deflation
1389   // so no need to lock ObjectMonitors for the list traversal.
1390   for (ObjectMonitor* mid = list; mid != NULL; mid = unmarked_next(mid)) {
1391     if (mid-&gt;object() != NULL) {
1392       f-&gt;do_oop((oop*)mid-&gt;object_addr());
1393     }
1394   }
1395 }
1396 
1397 
1398 // -----------------------------------------------------------------------------
1399 // ObjectMonitor Lifecycle
1400 // -----------------------
1401 // Inflation unlinks monitors from om_list_globals._free_list or a per-thread
1402 // free list and associates them with objects. Deflation -- which occurs at
<span class="line-modified">1403 // STW-time or asynchronously -- disassociates idle monitors from objects.</span>
1404 // Such scavenged monitors are returned to the om_list_globals._free_list.
1405 //
1406 // ObjectMonitors reside in type-stable memory (TSM) and are immortal.
1407 //
1408 // Lifecycle:
1409 // --   unassigned and on the om_list_globals._free_list
1410 // --   unassigned and on a per-thread free list
1411 // --   assigned to an object.  The object is inflated and the mark refers
1412 //      to the ObjectMonitor.
1413 
1414 ObjectMonitor* ObjectSynchronizer::om_alloc(Thread* self) {
1415   // A large MAXPRIVATE value reduces both list lock contention
1416   // and list coherency traffic, but also tends to increase the
1417   // number of ObjectMonitors in circulation as well as the STW
1418   // scavenge costs.  As usual, we lean toward time in space-time
1419   // tradeoffs.
1420   const int MAXPRIVATE = 1024;
1421   NoSafepointVerifier nsv;
1422 
1423   for (;;) {
1424     ObjectMonitor* m;
1425 
1426     // 1: try to allocate from the thread&#39;s local om_free_list.
1427     // Threads will attempt to allocate first from their local list, then
1428     // from the global list, and only after those attempts fail will the
1429     // thread attempt to instantiate new monitors. Thread-local free lists
1430     // improve allocation latency, as well as reducing coherency traffic
1431     // on the shared global list.
1432     m = take_from_start_of_om_free_list(self);
1433     if (m != NULL) {
1434       guarantee(m-&gt;object() == NULL, &quot;invariant&quot;);
<span class="line-added">1435       m-&gt;set_allocation_state(ObjectMonitor::New);</span>
1436       prepend_to_om_in_use_list(self, m);
1437       return m;
1438     }
1439 
1440     // 2: try to allocate from the global om_list_globals._free_list
1441     // If we&#39;re using thread-local free lists then try
1442     // to reprovision the caller&#39;s free list.
1443     if (Atomic::load(&amp;om_list_globals._free_list) != NULL) {
1444       // Reprovision the thread&#39;s om_free_list.
1445       // Use bulk transfers to reduce the allocation rate and heat
1446       // on various locks.
1447       for (int i = self-&gt;om_free_provision; --i &gt;= 0;) {
1448         ObjectMonitor* take = take_from_start_of_global_free_list();
1449         if (take == NULL) {
1450           break;  // No more are available.
1451         }
1452         guarantee(take-&gt;object() == NULL, &quot;invariant&quot;);
<span class="line-added">1453         if (AsyncDeflateIdleMonitors) {</span>
<span class="line-added">1454           // We allowed 3 field values to linger during async deflation.</span>
<span class="line-added">1455           // Clear or restore them as appropriate.</span>
<span class="line-added">1456           take-&gt;set_header(markWord::zero());</span>
<span class="line-added">1457           // DEFLATER_MARKER is the only non-NULL value we should see here.</span>
<span class="line-added">1458           take-&gt;try_set_owner_from(DEFLATER_MARKER, NULL);</span>
<span class="line-added">1459           if (take-&gt;contentions() &lt; 0) {</span>
<span class="line-added">1460             // Add back max_jint to restore the contentions field to its</span>
<span class="line-added">1461             // proper value.</span>
<span class="line-added">1462             take-&gt;add_to_contentions(max_jint);</span>
<span class="line-added">1463 </span>
<span class="line-added">1464 #ifdef ASSERT</span>
<span class="line-added">1465             jint l_contentions = take-&gt;contentions();</span>
<span class="line-added">1466 #endif</span>
<span class="line-added">1467             assert(l_contentions &gt;= 0, &quot;must not be negative: l_contentions=%d, contentions=%d&quot;,</span>
<span class="line-added">1468                    l_contentions, take-&gt;contentions());</span>
<span class="line-added">1469           }</span>
<span class="line-added">1470         }</span>
1471         take-&gt;Recycle();
<span class="line-added">1472         // Since we&#39;re taking from the global free-list, take must be Free.</span>
<span class="line-added">1473         // om_release() also sets the allocation state to Free because it</span>
<span class="line-added">1474         // is called from other code paths.</span>
<span class="line-added">1475         assert(take-&gt;is_free(), &quot;invariant&quot;);</span>
1476         om_release(self, take, false);
1477       }
1478       self-&gt;om_free_provision += 1 + (self-&gt;om_free_provision / 2);
1479       if (self-&gt;om_free_provision &gt; MAXPRIVATE) self-&gt;om_free_provision = MAXPRIVATE;
1480       continue;
1481     }
1482 
1483     // 3: allocate a block of new ObjectMonitors
1484     // Both the local and global free lists are empty -- resort to malloc().
1485     // In the current implementation ObjectMonitors are TSM - immortal.
1486     // Ideally, we&#39;d write &quot;new ObjectMonitor[_BLOCKSIZE], but we want
1487     // each ObjectMonitor to start at the beginning of a cache line,
1488     // so we use align_up().
1489     // A better solution would be to use C++ placement-new.
1490     // BEWARE: As it stands currently, we don&#39;t run the ctors!
1491     assert(_BLOCKSIZE &gt; 1, &quot;invariant&quot;);
1492     size_t neededsize = sizeof(PaddedObjectMonitor) * _BLOCKSIZE;
1493     PaddedObjectMonitor* temp;
1494     size_t aligned_size = neededsize + (OM_CACHE_LINE_SIZE - 1);
1495     void* real_malloc_addr = NEW_C_HEAP_ARRAY(char, aligned_size, mtInternal);
1496     temp = (PaddedObjectMonitor*)align_up(real_malloc_addr, OM_CACHE_LINE_SIZE);
1497     (void)memset((void *) temp, 0, neededsize);
1498 
1499     // Format the block.
1500     // initialize the linked list, each monitor points to its next
1501     // forming the single linked free list, the very first monitor
1502     // will points to next block, which forms the block list.
1503     // The trick of using the 1st element in the block as g_block_list
1504     // linkage should be reconsidered.  A better implementation would
1505     // look like: class Block { Block * next; int N; ObjectMonitor Body [N] ; }
1506 
1507     for (int i = 1; i &lt; _BLOCKSIZE; i++) {
1508       temp[i].set_next_om((ObjectMonitor*)&amp;temp[i + 1]);
<span class="line-added">1509       assert(temp[i].is_free(), &quot;invariant&quot;);</span>
1510     }
1511 
1512     // terminate the last monitor as the end of list
1513     temp[_BLOCKSIZE - 1].set_next_om((ObjectMonitor*)NULL);
1514 
1515     // Element [0] is reserved for global list linkage
1516     temp[0].set_object(CHAINMARKER);
1517 
1518     // Consider carving out this thread&#39;s current request from the
1519     // block in hand.  This avoids some lock traffic and redundant
1520     // list activity.
1521 
1522     prepend_block_to_lists(temp);
1523   }
1524 }
1525 
1526 // Place &quot;m&quot; on the caller&#39;s private per-thread om_free_list.
1527 // In practice there&#39;s no need to clamp or limit the number of
1528 // monitors on a thread&#39;s om_free_list as the only non-allocation time
1529 // we&#39;ll call om_release() is to return a monitor to the free list after
1530 // a CAS attempt failed. This doesn&#39;t allow unbounded #s of monitors to
1531 // accumulate on a thread&#39;s free list.
1532 //
1533 // Key constraint: all ObjectMonitors on a thread&#39;s free list and the global
1534 // free list must have their object field set to null. This prevents the
<span class="line-modified">1535 // scavenger -- deflate_monitor_list() or deflate_monitor_list_using_JT()</span>
<span class="line-modified">1536 // -- from reclaiming them while we are trying to release them.</span>
1537 
1538 void ObjectSynchronizer::om_release(Thread* self, ObjectMonitor* m,
1539                                     bool from_per_thread_alloc) {
1540   guarantee(m-&gt;header().value() == 0, &quot;invariant&quot;);
1541   guarantee(m-&gt;object() == NULL, &quot;invariant&quot;);
1542   NoSafepointVerifier nsv;
1543 
1544   if ((m-&gt;is_busy() | m-&gt;_recursions) != 0) {
1545     stringStream ss;
1546     fatal(&quot;freeing in-use monitor: %s, recursions=&quot; INTX_FORMAT,
1547           m-&gt;is_busy_to_string(&amp;ss), m-&gt;_recursions);
1548   }
<span class="line-added">1549   m-&gt;set_allocation_state(ObjectMonitor::Free);</span>
1550   // _next_om is used for both per-thread in-use and free lists so
1551   // we have to remove &#39;m&#39; from the in-use list first (as needed).
1552   if (from_per_thread_alloc) {
1553     // Need to remove &#39;m&#39; from om_in_use_list.
1554     ObjectMonitor* mid = NULL;
1555     ObjectMonitor* next = NULL;
1556 
<span class="line-modified">1557     // This list walk can race with another list walker or with async</span>
<span class="line-modified">1558     // deflation so we have to worry about an ObjectMonitor being</span>
<span class="line-modified">1559     // removed from this list while we are walking it.</span>

1560 
<span class="line-modified">1561     // Lock the list head to avoid racing with another list walker</span>
<span class="line-added">1562     // or with async deflation.</span>
1563     if ((mid = get_list_head_locked(&amp;self-&gt;om_in_use_list)) == NULL) {
1564       fatal(&quot;thread=&quot; INTPTR_FORMAT &quot; in-use list must not be empty.&quot;, p2i(self));
1565     }
1566     next = unmarked_next(mid);
1567     if (m == mid) {
1568       // First special case:
1569       // &#39;m&#39; matches mid, is the list head and is locked. Switch the list
1570       // head to next which unlocks the list head, but leaves the extracted
1571       // mid locked:
1572       Atomic::store(&amp;self-&gt;om_in_use_list, next);
1573     } else if (m == next) {
1574       // Second special case:
1575       // &#39;m&#39; matches next after the list head and we already have the list
1576       // head locked so set mid to what we are extracting:
1577       mid = next;
<span class="line-modified">1578       // Lock mid to prevent races with a list walker or an async</span>
<span class="line-added">1579       // deflater thread that&#39;s ahead of us. The locked list head</span>
<span class="line-added">1580       // prevents races from behind us.</span>
1581       om_lock(mid);
1582       // Update next to what follows mid (if anything):
1583       next = unmarked_next(mid);
1584       // Switch next after the list head to new next which unlocks the
1585       // list head, but leaves the extracted mid locked:
1586       self-&gt;om_in_use_list-&gt;set_next_om(next);
1587     } else {
1588       // We have to search the list to find &#39;m&#39;.

1589       guarantee(next != NULL, &quot;thread=&quot; INTPTR_FORMAT &quot;: om_in_use_list=&quot; INTPTR_FORMAT
1590                 &quot; is too short.&quot;, p2i(self), p2i(self-&gt;om_in_use_list));
1591       // Our starting anchor is next after the list head which is the
1592       // last ObjectMonitor we checked:
1593       ObjectMonitor* anchor = next;
<span class="line-added">1594       // Lock anchor to prevent races with a list walker or an async</span>
<span class="line-added">1595       // deflater thread that&#39;s ahead of us. The locked list head</span>
<span class="line-added">1596       // prevents races from behind us.</span>
<span class="line-added">1597       om_lock(anchor);</span>
<span class="line-added">1598       om_unlock(mid);  // Unlock the list head now that anchor is locked.</span>
1599       while ((mid = unmarked_next(anchor)) != NULL) {
1600         if (m == mid) {
1601           // We found &#39;m&#39; on the per-thread in-use list so extract it.

1602           // Update next to what follows mid (if anything):
1603           next = unmarked_next(mid);
1604           // Switch next after the anchor to new next which unlocks the
1605           // anchor, but leaves the extracted mid locked:
1606           anchor-&gt;set_next_om(next);
1607           break;
1608         } else {
<span class="line-modified">1609           // Lock the next anchor to prevent races with a list walker</span>
<span class="line-added">1610           // or an async deflater thread that&#39;s ahead of us. The locked</span>
<span class="line-added">1611           // current anchor prevents races from behind us.</span>
<span class="line-added">1612           om_lock(mid);</span>
<span class="line-added">1613           // Unlock current anchor now that next anchor is locked:</span>
<span class="line-added">1614           om_unlock(anchor);</span>
<span class="line-added">1615           anchor = mid;  // Advance to new anchor and try again.</span>
1616         }
1617       }
1618     }
1619 
1620     if (mid == NULL) {
1621       // Reached end of the list and didn&#39;t find &#39;m&#39; so:
1622       fatal(&quot;thread=&quot; INTPTR_FORMAT &quot; must find m=&quot; INTPTR_FORMAT &quot;on om_in_use_list=&quot;
1623             INTPTR_FORMAT, p2i(self), p2i(m), p2i(self-&gt;om_in_use_list));
1624     }
1625 
1626     // At this point mid is disconnected from the in-use list so
1627     // its lock no longer has any effects on the in-use list.
1628     Atomic::dec(&amp;self-&gt;om_in_use_count);
1629     // Unlock mid, but leave the next value for any lagging list
1630     // walkers. It will get cleaned up when mid is prepended to
1631     // the thread&#39;s free list:
1632     om_unlock(mid);
1633   }
1634 
1635   prepend_to_om_free_list(self, m);
<span class="line-added">1636   guarantee(m-&gt;is_free(), &quot;invariant&quot;);</span>
1637 }
1638 
1639 // Return ObjectMonitors on a moribund thread&#39;s free and in-use
1640 // lists to the appropriate global lists. The ObjectMonitors on the
1641 // per-thread in-use list may still be in use by other threads.
1642 //
1643 // We currently call om_flush() from Threads::remove() before the
1644 // thread has been excised from the thread list and is no longer a
1645 // mutator. This means that om_flush() cannot run concurrently with
1646 // a safepoint and interleave with deflate_idle_monitors(). In
1647 // particular, this ensures that the thread&#39;s in-use monitors are
1648 // scanned by a GC safepoint, either via Thread::oops_do() (before
1649 // om_flush() is called) or via ObjectSynchronizer::oops_do() (after
1650 // om_flush() is called).
<span class="line-added">1651 //</span>
<span class="line-added">1652 // With AsyncDeflateIdleMonitors, deflate_global_idle_monitors_using_JT()</span>
<span class="line-added">1653 // and deflate_per_thread_idle_monitors_using_JT() (in another thread) can</span>
<span class="line-added">1654 // run at the same time as om_flush() so we have to follow a careful</span>
<span class="line-added">1655 // protocol to prevent list corruption.</span>
1656 
1657 void ObjectSynchronizer::om_flush(Thread* self) {
1658   // Process the per-thread in-use list first to be consistent.
1659   int in_use_count = 0;
1660   ObjectMonitor* in_use_list = NULL;
1661   ObjectMonitor* in_use_tail = NULL;
1662   NoSafepointVerifier nsv;
1663 
<span class="line-modified">1664   // This function can race with a list walker or with an async</span>
<span class="line-modified">1665   // deflater thread so we lock the list head to prevent confusion.</span>
<span class="line-added">1666   // An async deflater thread checks to see if the target thread</span>
<span class="line-added">1667   // is exiting, but if it has made it past that check before we</span>
<span class="line-added">1668   // started exiting, then it is racing to get to the in-use list.</span>
1669   if ((in_use_list = get_list_head_locked(&amp;self-&gt;om_in_use_list)) != NULL) {
1670     // At this point, we have locked the in-use list head so a racing
1671     // thread cannot come in after us. However, a racing thread could
1672     // be ahead of us; we&#39;ll detect that and delay to let it finish.
1673     //
1674     // The thread is going away, however the ObjectMonitors on the
1675     // om_in_use_list may still be in-use by other threads. Link
1676     // them to in_use_tail, which will be linked into the global
1677     // in-use list (om_list_globals._in_use_list) below.
1678     //
1679     // Account for the in-use list head before the loop since it is
1680     // already locked (by this thread):
1681     in_use_tail = in_use_list;
1682     in_use_count++;
<span class="line-modified">1683     for (ObjectMonitor* cur_om = unmarked_next(in_use_list); cur_om != NULL;) {</span>
1684       if (is_locked(cur_om)) {
<span class="line-modified">1685         // cur_om is locked so there must be a racing walker or async</span>
<span class="line-modified">1686         // deflater thread ahead of us so we&#39;ll give it a chance to finish.</span>
1687         while (is_locked(cur_om)) {
1688           os::naked_short_sleep(1);
1689         }
<span class="line-added">1690         // Refetch the possibly changed next field and try again.</span>
<span class="line-added">1691         cur_om = unmarked_next(in_use_tail);</span>
<span class="line-added">1692         continue;</span>
<span class="line-added">1693       }</span>
<span class="line-added">1694       if (cur_om-&gt;object() == NULL) {</span>
<span class="line-added">1695         // cur_om was deflated and the object ref was cleared while it</span>
<span class="line-added">1696         // was locked. We happened to see it just after it was unlocked</span>
<span class="line-added">1697         // (and added to the free list). Refetch the possibly changed</span>
<span class="line-added">1698         // next field and try again.</span>
<span class="line-added">1699         cur_om = unmarked_next(in_use_tail);</span>
<span class="line-added">1700         continue;</span>
1701       }
1702       in_use_tail = cur_om;
1703       in_use_count++;
<span class="line-added">1704       cur_om = unmarked_next(cur_om);</span>
1705     }
1706     guarantee(in_use_tail != NULL, &quot;invariant&quot;);
1707     int l_om_in_use_count = Atomic::load(&amp;self-&gt;om_in_use_count);
<span class="line-modified">1708     ADIM_guarantee(l_om_in_use_count == in_use_count, &quot;in-use counts don&#39;t match: &quot;</span>
<span class="line-modified">1709                    &quot;l_om_in_use_count=%d, in_use_count=%d&quot;, l_om_in_use_count, in_use_count);</span>
1710     Atomic::store(&amp;self-&gt;om_in_use_count, 0);
1711     // Clear the in-use list head (which also unlocks it):
1712     Atomic::store(&amp;self-&gt;om_in_use_list, (ObjectMonitor*)NULL);
1713     om_unlock(in_use_list);
1714   }
1715 
1716   int free_count = 0;
1717   ObjectMonitor* free_list = NULL;
1718   ObjectMonitor* free_tail = NULL;
1719   // This function can race with a list walker thread so we lock the
1720   // list head to prevent confusion.
1721   if ((free_list = get_list_head_locked(&amp;self-&gt;om_free_list)) != NULL) {
1722     // At this point, we have locked the free list head so a racing
1723     // thread cannot come in after us. However, a racing thread could
1724     // be ahead of us; we&#39;ll detect that and delay to let it finish.
1725     //
1726     // The thread is going away. Set &#39;free_tail&#39; to the last per-thread free
1727     // monitor which will be linked to om_list_globals._free_list below.
1728     //
1729     // Account for the free list head before the loop since it is
</pre>
<hr />
<pre>
1731     free_tail = free_list;
1732     free_count++;
1733     for (ObjectMonitor* s = unmarked_next(free_list); s != NULL; s = unmarked_next(s)) {
1734       if (is_locked(s)) {
1735         // s is locked so there must be a racing walker thread ahead
1736         // of us so we&#39;ll give it a chance to finish.
1737         while (is_locked(s)) {
1738           os::naked_short_sleep(1);
1739         }
1740       }
1741       free_tail = s;
1742       free_count++;
1743       guarantee(s-&gt;object() == NULL, &quot;invariant&quot;);
1744       if (s-&gt;is_busy()) {
1745         stringStream ss;
1746         fatal(&quot;must be !is_busy: %s&quot;, s-&gt;is_busy_to_string(&amp;ss));
1747       }
1748     }
1749     guarantee(free_tail != NULL, &quot;invariant&quot;);
1750     int l_om_free_count = Atomic::load(&amp;self-&gt;om_free_count);
<span class="line-modified">1751     ADIM_guarantee(l_om_free_count == free_count, &quot;free counts don&#39;t match: &quot;</span>
<span class="line-modified">1752                    &quot;l_om_free_count=%d, free_count=%d&quot;, l_om_free_count, free_count);</span>
1753     Atomic::store(&amp;self-&gt;om_free_count, 0);
1754     Atomic::store(&amp;self-&gt;om_free_list, (ObjectMonitor*)NULL);
1755     om_unlock(free_list);
1756   }
1757 
1758   if (free_tail != NULL) {
1759     prepend_list_to_global_free_list(free_list, free_tail, free_count);
1760   }
1761 
1762   if (in_use_tail != NULL) {
1763     prepend_list_to_global_in_use_list(in_use_list, in_use_tail, in_use_count);
1764   }
1765 
1766   LogStreamHandle(Debug, monitorinflation) lsh_debug;
1767   LogStreamHandle(Info, monitorinflation) lsh_info;
1768   LogStream* ls = NULL;
1769   if (log_is_enabled(Debug, monitorinflation)) {
1770     ls = &amp;lsh_debug;
1771   } else if ((free_count != 0 || in_use_count != 0) &amp;&amp;
1772              log_is_enabled(Info, monitorinflation)) {
</pre>
<hr />
<pre>
1777                  &quot;, in_use_count=%d&quot; &quot;, om_free_provision=%d&quot;,
1778                  p2i(self), free_count, in_use_count, self-&gt;om_free_provision);
1779   }
1780 }
1781 
1782 static void post_monitor_inflate_event(EventJavaMonitorInflate* event,
1783                                        const oop obj,
1784                                        ObjectSynchronizer::InflateCause cause) {
1785   assert(event != NULL, &quot;invariant&quot;);
1786   assert(event-&gt;should_commit(), &quot;invariant&quot;);
1787   event-&gt;set_monitorClass(obj-&gt;klass());
1788   event-&gt;set_address((uintptr_t)(void*)obj);
1789   event-&gt;set_cause((u1)cause);
1790   event-&gt;commit();
1791 }
1792 
1793 // Fast path code shared by multiple functions
1794 void ObjectSynchronizer::inflate_helper(oop obj) {
1795   markWord mark = obj-&gt;mark();
1796   if (mark.has_monitor()) {
<span class="line-modified">1797     ObjectMonitor* monitor = mark.monitor();</span>
<span class="line-modified">1798     assert(ObjectSynchronizer::verify_objmon_isinpool(monitor), &quot;monitor=&quot; INTPTR_FORMAT &quot; is invalid&quot;, p2i(monitor));</span>
<span class="line-added">1799     markWord dmw = monitor-&gt;header();</span>
<span class="line-added">1800     assert(dmw.is_neutral(), &quot;sanity check: header=&quot; INTPTR_FORMAT, dmw.value());</span>
1801     return;
1802   }
<span class="line-modified">1803   (void)inflate(Thread::current(), obj, inflate_cause_vm_internal);</span>
1804 }
1805 
<span class="line-modified">1806 ObjectMonitor* ObjectSynchronizer::inflate(Thread* self, oop object,</span>
<span class="line-modified">1807                                            const InflateCause cause) {</span>
1808   // Inflate mutates the heap ...
1809   // Relaxing assertion for bug 6320749.
1810   assert(Universe::verify_in_progress() ||
1811          !SafepointSynchronize::is_at_safepoint(), &quot;invariant&quot;);
1812 
1813   if (EnableValhalla) {
1814     guarantee(!object-&gt;klass()-&gt;is_value(), &quot;Attempt to inflate value type&quot;);
1815   }
1816 
1817   EventJavaMonitorInflate event;
1818 
1819   for (;;) {
1820     const markWord mark = object-&gt;mark();
1821     assert(!mark.has_bias_pattern(), &quot;invariant&quot;);
1822 
1823     // The mark can be in one of the following states:
1824     // *  Inflated     - just return
1825     // *  Stack-locked - coerce it to inflated
1826     // *  INFLATING    - busy wait for conversion to complete
1827     // *  Neutral      - aggressively inflate the object.
1828     // *  BIASED       - Illegal.  We should never see this
1829 
1830     // CASE: inflated
1831     if (mark.has_monitor()) {
1832       ObjectMonitor* inf = mark.monitor();
1833       markWord dmw = inf-&gt;header();
1834       assert(dmw.is_neutral(), &quot;invariant: header=&quot; INTPTR_FORMAT, dmw.value());
<span class="line-modified">1835       assert(AsyncDeflateIdleMonitors || inf-&gt;object() == object, &quot;invariant&quot;);</span>
1836       assert(ObjectSynchronizer::verify_objmon_isinpool(inf), &quot;monitor is invalid&quot;);
1837       return inf;
1838     }
1839 
1840     // CASE: inflation in progress - inflating over a stack-lock.
1841     // Some other thread is converting from stack-locked to inflated.
1842     // Only that thread can complete inflation -- other threads must wait.
1843     // The INFLATING value is transient.
1844     // Currently, we spin/yield/park and poll the markword, waiting for inflation to finish.
1845     // We could always eliminate polling by parking the thread on some auxiliary list.
1846     if (mark == markWord::INFLATING()) {
1847       read_stable_mark(object);
1848       continue;
1849     }
1850 
1851     // CASE: stack-locked
1852     // Could be stack-locked either by this thread or by some other thread.
1853     //
1854     // Note that we allocate the objectmonitor speculatively, _before_ attempting
1855     // to install INFLATING into the mark word.  We originally installed INFLATING,
</pre>
<hr />
<pre>
1863     // critical INFLATING...ST interval.  A thread can transfer
1864     // multiple objectmonitors en-mass from the global free list to its local free list.
1865     // This reduces coherency traffic and lock contention on the global free list.
1866     // Using such local free lists, it doesn&#39;t matter if the om_alloc() call appears
1867     // before or after the CAS(INFLATING) operation.
1868     // See the comments in om_alloc().
1869 
1870     LogStreamHandle(Trace, monitorinflation) lsh;
1871 
1872     if (mark.has_locker()) {
1873       ObjectMonitor* m = om_alloc(self);
1874       // Optimistically prepare the objectmonitor - anticipate successful CAS
1875       // We do this before the CAS in order to minimize the length of time
1876       // in which INFLATING appears in the mark.
1877       m-&gt;Recycle();
1878       m-&gt;_Responsible  = NULL;
1879       m-&gt;_SpinDuration = ObjectMonitor::Knob_SpinLimit;   // Consider: maintain by type/class
1880 
1881       markWord cmp = object-&gt;cas_set_mark(markWord::INFLATING(), mark);
1882       if (cmp != mark) {
<span class="line-added">1883         // om_release() will reset the allocation state from New to Free.</span>
1884         om_release(self, m, true);
1885         continue;       // Interference -- just retry
1886       }
1887 
1888       // We&#39;ve successfully installed INFLATING (0) into the mark-word.
1889       // This is the only case where 0 will appear in a mark-word.
1890       // Only the singular thread that successfully swings the mark-word
1891       // to 0 can perform (or more precisely, complete) inflation.
1892       //
1893       // Why do we CAS a 0 into the mark-word instead of just CASing the
1894       // mark-word from the stack-locked value directly to the new inflated state?
1895       // Consider what happens when a thread unlocks a stack-locked object.
1896       // It attempts to use CAS to swing the displaced header value from the
1897       // on-stack BasicLock back into the object header.  Recall also that the
1898       // header value (hash code, etc) can reside in (a) the object header, or
1899       // (b) a displaced header associated with the stack-lock, or (c) a displaced
1900       // header in an ObjectMonitor.  The inflate() routine must copy the header
1901       // value from the BasicLock on the owner&#39;s stack to the ObjectMonitor, all
1902       // the while preserving the hashCode stability invariants.  If the owner
1903       // decides to release the lock while the value is 0, the unlock will fail
1904       // and control will eventually pass from slow_exit() to inflate.  The owner
1905       // will then spin, waiting for the 0 value to disappear.   Put another way,
1906       // the 0 causes the owner to stall if the owner happens to try to
1907       // drop the lock (restoring the header from the BasicLock to the object)
1908       // while inflation is in-progress.  This protocol avoids races that might
1909       // would otherwise permit hashCode values to change or &quot;flicker&quot; for an object.
1910       // Critically, while object-&gt;mark is 0 mark.displaced_mark_helper() is stable.
1911       // 0 serves as a &quot;BUSY&quot; inflate-in-progress indicator.
1912 
1913 
1914       // fetch the displaced mark from the owner&#39;s stack.
1915       // The owner can&#39;t die or unwind past the lock while our INFLATING
1916       // object is in the mark.  Furthermore the owner can&#39;t complete
1917       // an unlock on the object, either.
1918       markWord dmw = mark.displaced_mark_helper();
1919       // Catch if the object&#39;s header is not neutral (not locked and
1920       // not marked is what we care about here).
<span class="line-modified">1921       ADIM_guarantee(dmw.is_neutral(), &quot;invariant: header=&quot; INTPTR_FORMAT, dmw.value());</span>
1922 
1923       // Setup monitor fields to proper values -- prepare the monitor
1924       m-&gt;set_header(dmw);
1925 
1926       // Optimization: if the mark.locker stack address is associated
1927       // with this thread we could simply set m-&gt;_owner = self.
1928       // Note that a thread can inflate an object
1929       // that it has stack-locked -- as might happen in wait() -- directly
1930       // with CAS.  That is, we can avoid the xchg-NULL .... ST idiom.
<span class="line-modified">1931       if (AsyncDeflateIdleMonitors) {</span>
<span class="line-added">1932         m-&gt;set_owner_from(NULL, DEFLATER_MARKER, mark.locker());</span>
<span class="line-added">1933       } else {</span>
<span class="line-added">1934         m-&gt;set_owner_from(NULL, mark.locker());</span>
<span class="line-added">1935       }</span>
1936       m-&gt;set_object(object);
1937       // TODO-FIXME: assert BasicLock-&gt;dhw != 0.
1938 
1939       // Must preserve store ordering. The monitor state must
1940       // be stable at the time of publishing the monitor address.
1941       guarantee(object-&gt;mark() == markWord::INFLATING(), &quot;invariant&quot;);
1942       object-&gt;release_set_mark(markWord::encode(m));
1943 
<span class="line-added">1944       // Once ObjectMonitor is configured and the object is associated</span>
<span class="line-added">1945       // with the ObjectMonitor, it is safe to allow async deflation:</span>
<span class="line-added">1946       assert(m-&gt;is_new(), &quot;freshly allocated monitor must be new&quot;);</span>
<span class="line-added">1947       m-&gt;set_allocation_state(ObjectMonitor::Old);</span>
<span class="line-added">1948 </span>
1949       // Hopefully the performance counters are allocated on distinct cache lines
1950       // to avoid false sharing on MP systems ...
1951       OM_PERFDATA_OP(Inflations, inc());
1952       if (log_is_enabled(Trace, monitorinflation)) {
1953         ResourceMark rm(self);
1954         lsh.print_cr(&quot;inflate(has_locker): object=&quot; INTPTR_FORMAT &quot;, mark=&quot;
1955                      INTPTR_FORMAT &quot;, type=&#39;%s&#39;&quot;, p2i(object),
1956                      object-&gt;mark().value(), object-&gt;klass()-&gt;external_name());
1957       }
1958       if (event.should_commit()) {
1959         post_monitor_inflate_event(&amp;event, object, cause);
1960       }
1961       return m;
1962     }
1963 
1964     // CASE: neutral
1965     // TODO-FIXME: for entry we currently inflate and then try to CAS _owner.
1966     // If we know we&#39;re inflating for entry it&#39;s better to inflate by swinging a
1967     // pre-locked ObjectMonitor pointer into the object header.   A successful
1968     // CAS inflates the object *and* confers ownership to the inflating thread.
1969     // In the current implementation we use a 2-step mechanism where we CAS()
1970     // to inflate and then CAS() again to try to swing _owner from NULL to self.
1971     // An inflateTry() method that we could call from enter() would be useful.
1972 
1973     // Catch if the object&#39;s header is not neutral (not locked and
1974     // not marked is what we care about here).
<span class="line-modified">1975     ADIM_guarantee(mark.is_neutral(), &quot;invariant: header=&quot; INTPTR_FORMAT, mark.value());</span>
1976     ObjectMonitor* m = om_alloc(self);
1977     // prepare m for installation - set monitor to initial state
1978     m-&gt;Recycle();
1979     m-&gt;set_header(mark);
<span class="line-added">1980     if (AsyncDeflateIdleMonitors) {</span>
<span class="line-added">1981       // DEFLATER_MARKER is the only non-NULL value we should see here.</span>
<span class="line-added">1982       m-&gt;try_set_owner_from(DEFLATER_MARKER, NULL);</span>
<span class="line-added">1983     }</span>
1984     m-&gt;set_object(object);
1985     m-&gt;_Responsible  = NULL;
1986     m-&gt;_SpinDuration = ObjectMonitor::Knob_SpinLimit;       // consider: keep metastats by type/class
1987 
1988     if (object-&gt;cas_set_mark(markWord::encode(m), mark) != mark) {
1989       m-&gt;set_header(markWord::zero());
1990       m-&gt;set_object(NULL);
1991       m-&gt;Recycle();
<span class="line-added">1992       // om_release() will reset the allocation state from New to Free.</span>
1993       om_release(self, m, true);
1994       m = NULL;
1995       continue;
1996       // interference - the markword changed - just retry.
1997       // The state-transitions are one-way, so there&#39;s no chance of
1998       // live-lock -- &quot;Inflated&quot; is an absorbing state.
1999     }
2000 
<span class="line-added">2001     // Once the ObjectMonitor is configured and object is associated</span>
<span class="line-added">2002     // with the ObjectMonitor, it is safe to allow async deflation:</span>
<span class="line-added">2003     assert(m-&gt;is_new(), &quot;freshly allocated monitor must be new&quot;);</span>
<span class="line-added">2004     m-&gt;set_allocation_state(ObjectMonitor::Old);</span>
<span class="line-added">2005 </span>
2006     // Hopefully the performance counters are allocated on distinct
2007     // cache lines to avoid false sharing on MP systems ...
2008     OM_PERFDATA_OP(Inflations, inc());
2009     if (log_is_enabled(Trace, monitorinflation)) {
2010       ResourceMark rm(self);
2011       lsh.print_cr(&quot;inflate(neutral): object=&quot; INTPTR_FORMAT &quot;, mark=&quot;
2012                    INTPTR_FORMAT &quot;, type=&#39;%s&#39;&quot;, p2i(object),
2013                    object-&gt;mark().value(), object-&gt;klass()-&gt;external_name());
2014     }
2015     if (event.should_commit()) {
2016       post_monitor_inflate_event(&amp;event, object, cause);
2017     }
2018     return m;
2019   }
2020 }
2021 
2022 
2023 // We maintain a list of in-use monitors for each thread.
2024 //
<span class="line-added">2025 // For safepoint based deflation:</span>
2026 // deflate_thread_local_monitors() scans a single thread&#39;s in-use list, while
2027 // deflate_idle_monitors() scans only a global list of in-use monitors which
2028 // is populated only as a thread dies (see om_flush()).
2029 //
2030 // These operations are called at all safepoints, immediately after mutators
2031 // are stopped, but before any objects have moved. Collectively they traverse
2032 // the population of in-use monitors, deflating where possible. The scavenged
2033 // monitors are returned to the global monitor free list.
2034 //
2035 // Beware that we scavenge at *every* stop-the-world point. Having a large
2036 // number of monitors in-use could negatively impact performance. We also want
2037 // to minimize the total # of monitors in circulation, as they incur a small
2038 // footprint penalty.
2039 //
2040 // Perversely, the heap size -- and thus the STW safepoint rate --
2041 // typically drives the scavenge rate.  Large heaps can mean infrequent GC,
2042 // which in turn can mean large(r) numbers of ObjectMonitors in circulation.
2043 // This is an unfortunate aspect of this design.
<span class="line-added">2044 //</span>
<span class="line-added">2045 // For async deflation:</span>
<span class="line-added">2046 // If a special deflation request is made, then the safepoint based</span>
<span class="line-added">2047 // deflation mechanism is used. Otherwise, an async deflation request</span>
<span class="line-added">2048 // is registered with the ServiceThread and it is notified.</span>
<span class="line-added">2049 </span>
<span class="line-added">2050 void ObjectSynchronizer::do_safepoint_work(DeflateMonitorCounters* counters) {</span>
<span class="line-added">2051   assert(SafepointSynchronize::is_at_safepoint(), &quot;must be at safepoint&quot;);</span>
<span class="line-added">2052 </span>
<span class="line-added">2053   // The per-thread in-use lists are handled in</span>
<span class="line-added">2054   // ParallelSPCleanupThreadClosure::do_thread().</span>
<span class="line-added">2055 </span>
<span class="line-added">2056   if (!AsyncDeflateIdleMonitors || is_special_deflation_requested()) {</span>
<span class="line-added">2057     // Use the older mechanism for the global in-use list or if a</span>
<span class="line-added">2058     // special deflation has been requested before the safepoint.</span>
<span class="line-added">2059     ObjectSynchronizer::deflate_idle_monitors(counters);</span>
<span class="line-added">2060     return;</span>
<span class="line-added">2061   }</span>
<span class="line-added">2062 </span>
<span class="line-added">2063   log_debug(monitorinflation)(&quot;requesting async deflation of idle monitors.&quot;);</span>
<span class="line-added">2064   // Request deflation of idle monitors by the ServiceThread:</span>
<span class="line-added">2065   set_is_async_deflation_requested(true);</span>
<span class="line-added">2066   MonitorLocker ml(Service_lock, Mutex::_no_safepoint_check_flag);</span>
<span class="line-added">2067   ml.notify_all();</span>
<span class="line-added">2068 </span>
<span class="line-added">2069   if (log_is_enabled(Debug, monitorinflation)) {</span>
<span class="line-added">2070     // exit_globals()&#39;s call to audit_and_print_stats() is done</span>
<span class="line-added">2071     // at the Info level and not at a safepoint.</span>
<span class="line-added">2072     // For safepoint based deflation, audit_and_print_stats() is called</span>
<span class="line-added">2073     // in ObjectSynchronizer::finish_deflate_idle_monitors() at the</span>
<span class="line-added">2074     // Debug level at a safepoint.</span>
<span class="line-added">2075     ObjectSynchronizer::audit_and_print_stats(false /* on_exit */);</span>
<span class="line-added">2076   }</span>
<span class="line-added">2077 }</span>
2078 
2079 // Deflate a single monitor if not in-use
2080 // Return true if deflated, false if in-use
2081 bool ObjectSynchronizer::deflate_monitor(ObjectMonitor* mid, oop obj,
2082                                          ObjectMonitor** free_head_p,
2083                                          ObjectMonitor** free_tail_p) {
2084   bool deflated;
2085   // Normal case ... The monitor is associated with obj.
2086   const markWord mark = obj-&gt;mark();
2087   guarantee(mark == markWord::encode(mid), &quot;should match: mark=&quot;
2088             INTPTR_FORMAT &quot;, encoded mid=&quot; INTPTR_FORMAT, mark.value(),
2089             markWord::encode(mid).value());
2090   // Make sure that mark.monitor() and markWord::encode() agree:
2091   guarantee(mark.monitor() == mid, &quot;should match: monitor()=&quot; INTPTR_FORMAT
2092             &quot;, mid=&quot; INTPTR_FORMAT, p2i(mark.monitor()), p2i(mid));
2093   const markWord dmw = mid-&gt;header();
2094   guarantee(dmw.is_neutral(), &quot;invariant: header=&quot; INTPTR_FORMAT, dmw.value());
2095 
2096   if (mid-&gt;is_busy()) {
2097     // Easy checks are first - the ObjectMonitor is busy so no deflation.
2098     deflated = false;
2099   } else {
2100     // Deflate the monitor if it is no longer being used
2101     // It&#39;s idle - scavenge and return to the global free list
2102     // plain old deflation ...
2103     if (log_is_enabled(Trace, monitorinflation)) {
2104       ResourceMark rm;
2105       log_trace(monitorinflation)(&quot;deflate_monitor: &quot;
2106                                   &quot;object=&quot; INTPTR_FORMAT &quot;, mark=&quot;
2107                                   INTPTR_FORMAT &quot;, type=&#39;%s&#39;&quot;, p2i(obj),
2108                                   mark.value(), obj-&gt;klass()-&gt;external_name());
2109     }
2110 
2111     // Restore the header back to obj
2112     obj-&gt;release_set_mark(dmw);
<span class="line-added">2113     if (AsyncDeflateIdleMonitors) {</span>
<span class="line-added">2114       // clear() expects the owner field to be NULL.</span>
<span class="line-added">2115       // DEFLATER_MARKER is the only non-NULL value we should see here.</span>
<span class="line-added">2116       mid-&gt;try_set_owner_from(DEFLATER_MARKER, NULL);</span>
<span class="line-added">2117     }</span>
2118     mid-&gt;clear();
2119 
2120     assert(mid-&gt;object() == NULL, &quot;invariant: object=&quot; INTPTR_FORMAT,
2121            p2i(mid-&gt;object()));
<span class="line-added">2122     assert(mid-&gt;is_free(), &quot;invariant&quot;);</span>
2123 
2124     // Move the deflated ObjectMonitor to the working free list
2125     // defined by free_head_p and free_tail_p.
2126     if (*free_head_p == NULL) *free_head_p = mid;
2127     if (*free_tail_p != NULL) {
2128       // We append to the list so the caller can use mid-&gt;_next_om
2129       // to fix the linkages in its context.
2130       ObjectMonitor* prevtail = *free_tail_p;
2131       // Should have been cleaned up by the caller:
2132       // Note: Should not have to lock prevtail here since we&#39;re at a
2133       // safepoint and ObjectMonitors on the local free list should
2134       // not be accessed in parallel.
2135 #ifdef ASSERT
2136       ObjectMonitor* l_next_om = prevtail-&gt;next_om();
2137 #endif
2138       assert(l_next_om == NULL, &quot;must be NULL: _next_om=&quot; INTPTR_FORMAT, p2i(l_next_om));
2139       prevtail-&gt;set_next_om(mid);
2140     }
2141     *free_tail_p = mid;
2142     // At this point, mid-&gt;_next_om still refers to its current
2143     // value and another ObjectMonitor&#39;s _next_om field still
2144     // refers to this ObjectMonitor. Those linkages have to be
2145     // cleaned up by the caller who has the complete context.
2146     deflated = true;
2147   }
2148   return deflated;
2149 }
2150 
<span class="line-added">2151 // Deflate the specified ObjectMonitor if not in-use using a JavaThread.</span>
<span class="line-added">2152 // Returns true if it was deflated and false otherwise.</span>
<span class="line-added">2153 //</span>
<span class="line-added">2154 // The async deflation protocol sets owner to DEFLATER_MARKER and</span>
<span class="line-added">2155 // makes contentions negative as signals to contending threads that</span>
<span class="line-added">2156 // an async deflation is in progress. There are a number of checks</span>
<span class="line-added">2157 // as part of the protocol to make sure that the calling thread has</span>
<span class="line-added">2158 // not lost the race to a contending thread.</span>
<span class="line-added">2159 //</span>
<span class="line-added">2160 // The ObjectMonitor has been successfully async deflated when:</span>
<span class="line-added">2161 //   (contentions &lt; 0)</span>
<span class="line-added">2162 // Contending threads that see that condition know to retry their operation.</span>
<span class="line-added">2163 //</span>
<span class="line-added">2164 bool ObjectSynchronizer::deflate_monitor_using_JT(ObjectMonitor* mid,</span>
<span class="line-added">2165                                                   ObjectMonitor** free_head_p,</span>
<span class="line-added">2166                                                   ObjectMonitor** free_tail_p) {</span>
<span class="line-added">2167   assert(AsyncDeflateIdleMonitors, &quot;sanity check&quot;);</span>
<span class="line-added">2168   assert(Thread::current()-&gt;is_Java_thread(), &quot;precondition&quot;);</span>
<span class="line-added">2169   // A newly allocated ObjectMonitor should not be seen here so we</span>
<span class="line-added">2170   // avoid an endless inflate/deflate cycle.</span>
<span class="line-added">2171   assert(mid-&gt;is_old(), &quot;must be old: allocation_state=%d&quot;,</span>
<span class="line-added">2172          (int) mid-&gt;allocation_state());</span>
<span class="line-added">2173 </span>
<span class="line-added">2174   if (mid-&gt;is_busy()) {</span>
<span class="line-added">2175     // Easy checks are first - the ObjectMonitor is busy so no deflation.</span>
<span class="line-added">2176     return false;</span>
<span class="line-added">2177   }</span>
<span class="line-added">2178 </span>
<span class="line-added">2179   // Set a NULL owner to DEFLATER_MARKER to force any contending thread</span>
<span class="line-added">2180   // through the slow path. This is just the first part of the async</span>
<span class="line-added">2181   // deflation dance.</span>
<span class="line-added">2182   if (mid-&gt;try_set_owner_from(NULL, DEFLATER_MARKER) != NULL) {</span>
<span class="line-added">2183     // The owner field is no longer NULL so we lost the race since the</span>
<span class="line-added">2184     // ObjectMonitor is now busy.</span>
<span class="line-added">2185     return false;</span>
<span class="line-added">2186   }</span>
<span class="line-added">2187 </span>
<span class="line-added">2188   if (mid-&gt;contentions() &gt; 0 || mid-&gt;_waiters != 0) {</span>
<span class="line-added">2189     // Another thread has raced to enter the ObjectMonitor after</span>
<span class="line-added">2190     // mid-&gt;is_busy() above or has already entered and waited on</span>
<span class="line-added">2191     // it which makes it busy so no deflation. Restore owner to</span>
<span class="line-added">2192     // NULL if it is still DEFLATER_MARKER.</span>
<span class="line-added">2193     if (mid-&gt;try_set_owner_from(DEFLATER_MARKER, NULL) != DEFLATER_MARKER) {</span>
<span class="line-added">2194       // Deferred decrement for the JT EnterI() that cancelled the async deflation.</span>
<span class="line-added">2195       mid-&gt;add_to_contentions(-1);</span>
<span class="line-added">2196     }</span>
<span class="line-added">2197     return false;</span>
<span class="line-added">2198   }</span>
<span class="line-added">2199 </span>
<span class="line-added">2200   // Make a zero contentions field negative to force any contending threads</span>
<span class="line-added">2201   // to retry. This is the second part of the async deflation dance.</span>
<span class="line-added">2202   if (Atomic::cmpxchg(&amp;mid-&gt;_contentions, (jint)0, -max_jint) != 0) {</span>
<span class="line-added">2203     // Contentions was no longer 0 so we lost the race since the</span>
<span class="line-added">2204     // ObjectMonitor is now busy. Restore owner to NULL if it is</span>
<span class="line-added">2205     // still DEFLATER_MARKER:</span>
<span class="line-added">2206     if (mid-&gt;try_set_owner_from(DEFLATER_MARKER, NULL) != DEFLATER_MARKER) {</span>
<span class="line-added">2207       // Deferred decrement for the JT EnterI() that cancelled the async deflation.</span>
<span class="line-added">2208       mid-&gt;add_to_contentions(-1);</span>
<span class="line-added">2209     }</span>
<span class="line-added">2210     return false;</span>
<span class="line-added">2211   }</span>
<span class="line-added">2212 </span>
<span class="line-added">2213   // Sanity checks for the races:</span>
<span class="line-added">2214   guarantee(mid-&gt;owner_is_DEFLATER_MARKER(), &quot;must be deflater marker&quot;);</span>
<span class="line-added">2215   guarantee(mid-&gt;contentions() &lt; 0, &quot;must be negative: contentions=%d&quot;,</span>
<span class="line-added">2216             mid-&gt;contentions());</span>
<span class="line-added">2217   guarantee(mid-&gt;_waiters == 0, &quot;must be 0: waiters=%d&quot;, mid-&gt;_waiters);</span>
<span class="line-added">2218   guarantee(mid-&gt;_cxq == NULL, &quot;must be no contending threads: cxq=&quot;</span>
<span class="line-added">2219             INTPTR_FORMAT, p2i(mid-&gt;_cxq));</span>
<span class="line-added">2220   guarantee(mid-&gt;_EntryList == NULL,</span>
<span class="line-added">2221             &quot;must be no entering threads: EntryList=&quot; INTPTR_FORMAT,</span>
<span class="line-added">2222             p2i(mid-&gt;_EntryList));</span>
<span class="line-added">2223 </span>
<span class="line-added">2224   const oop obj = (oop) mid-&gt;object();</span>
<span class="line-added">2225   if (log_is_enabled(Trace, monitorinflation)) {</span>
<span class="line-added">2226     ResourceMark rm;</span>
<span class="line-added">2227     log_trace(monitorinflation)(&quot;deflate_monitor_using_JT: &quot;</span>
<span class="line-added">2228                                 &quot;object=&quot; INTPTR_FORMAT &quot;, mark=&quot;</span>
<span class="line-added">2229                                 INTPTR_FORMAT &quot;, type=&#39;%s&#39;&quot;,</span>
<span class="line-added">2230                                 p2i(obj), obj-&gt;mark().value(),</span>
<span class="line-added">2231                                 obj-&gt;klass()-&gt;external_name());</span>
<span class="line-added">2232   }</span>
<span class="line-added">2233 </span>
<span class="line-added">2234   // Install the old mark word if nobody else has already done it.</span>
<span class="line-added">2235   mid-&gt;install_displaced_markword_in_object(obj);</span>
<span class="line-added">2236   mid-&gt;clear_common();</span>
<span class="line-added">2237 </span>
<span class="line-added">2238   assert(mid-&gt;object() == NULL, &quot;must be NULL: object=&quot; INTPTR_FORMAT,</span>
<span class="line-added">2239          p2i(mid-&gt;object()));</span>
<span class="line-added">2240   assert(mid-&gt;is_free(), &quot;must be free: allocation_state=%d&quot;,</span>
<span class="line-added">2241          (int)mid-&gt;allocation_state());</span>
<span class="line-added">2242 </span>
<span class="line-added">2243   // Move the deflated ObjectMonitor to the working free list</span>
<span class="line-added">2244   // defined by free_head_p and free_tail_p.</span>
<span class="line-added">2245   if (*free_head_p == NULL) {</span>
<span class="line-added">2246     // First one on the list.</span>
<span class="line-added">2247     *free_head_p = mid;</span>
<span class="line-added">2248   }</span>
<span class="line-added">2249   if (*free_tail_p != NULL) {</span>
<span class="line-added">2250     // We append to the list so the caller can use mid-&gt;_next_om</span>
<span class="line-added">2251     // to fix the linkages in its context.</span>
<span class="line-added">2252     ObjectMonitor* prevtail = *free_tail_p;</span>
<span class="line-added">2253     // prevtail should have been cleaned up by the caller:</span>
<span class="line-added">2254 #ifdef ASSERT</span>
<span class="line-added">2255     ObjectMonitor* l_next_om = unmarked_next(prevtail);</span>
<span class="line-added">2256 #endif</span>
<span class="line-added">2257     assert(l_next_om == NULL, &quot;must be NULL: _next_om=&quot; INTPTR_FORMAT, p2i(l_next_om));</span>
<span class="line-added">2258     om_lock(prevtail);</span>
<span class="line-added">2259     prevtail-&gt;set_next_om(mid);  // prevtail now points to mid (and is unlocked)</span>
<span class="line-added">2260   }</span>
<span class="line-added">2261   *free_tail_p = mid;</span>
<span class="line-added">2262 </span>
<span class="line-added">2263   // At this point, mid-&gt;_next_om still refers to its current</span>
<span class="line-added">2264   // value and another ObjectMonitor&#39;s _next_om field still</span>
<span class="line-added">2265   // refers to this ObjectMonitor. Those linkages have to be</span>
<span class="line-added">2266   // cleaned up by the caller who has the complete context.</span>
<span class="line-added">2267 </span>
<span class="line-added">2268   // We leave owner == DEFLATER_MARKER and contentions &lt; 0</span>
<span class="line-added">2269   // to force any racing threads to retry.</span>
<span class="line-added">2270   return true;  // Success, ObjectMonitor has been deflated.</span>
<span class="line-added">2271 }</span>
<span class="line-added">2272 </span>
2273 // Walk a given monitor list, and deflate idle monitors.
2274 // The given list could be a per-thread list or a global list.
2275 //
2276 // In the case of parallel processing of thread local monitor lists,
2277 // work is done by Threads::parallel_threads_do() which ensures that
2278 // each Java thread is processed by exactly one worker thread, and
2279 // thus avoid conflicts that would arise when worker threads would
2280 // process the same monitor lists concurrently.
2281 //
2282 // See also ParallelSPCleanupTask and
2283 // SafepointSynchronize::do_cleanup_tasks() in safepoint.cpp and
2284 // Threads::parallel_java_threads_do() in thread.cpp.
2285 int ObjectSynchronizer::deflate_monitor_list(ObjectMonitor** list_p,
2286                                              int* count_p,
2287                                              ObjectMonitor** free_head_p,
2288                                              ObjectMonitor** free_tail_p) {
2289   ObjectMonitor* cur_mid_in_use = NULL;
2290   ObjectMonitor* mid = NULL;
2291   ObjectMonitor* next = NULL;
2292   int deflated_count = 0;
</pre>
<hr />
<pre>
2303       // by unlinking mid from the global or per-thread in-use list.
2304       if (cur_mid_in_use == NULL) {
2305         // mid is the list head so switch the list head to next:
2306         Atomic::store(list_p, next);
2307       } else {
2308         // Switch cur_mid_in_use&#39;s next field to next:
2309         cur_mid_in_use-&gt;set_next_om(next);
2310       }
2311       // At this point mid is disconnected from the in-use list.
2312       deflated_count++;
2313       Atomic::dec(count_p);
2314       // mid is current tail in the free_head_p list so NULL terminate it:
2315       mid-&gt;set_next_om(NULL);
2316     } else {
2317       cur_mid_in_use = mid;
2318     }
2319   }
2320   return deflated_count;
2321 }
2322 
<span class="line-added">2323 // Walk a given ObjectMonitor list and deflate idle ObjectMonitors using</span>
<span class="line-added">2324 // a JavaThread. Returns the number of deflated ObjectMonitors. The given</span>
<span class="line-added">2325 // list could be a per-thread in-use list or the global in-use list.</span>
<span class="line-added">2326 // If a safepoint has started, then we save state via saved_mid_in_use_p</span>
<span class="line-added">2327 // and return to the caller to honor the safepoint.</span>
<span class="line-added">2328 //</span>
<span class="line-added">2329 int ObjectSynchronizer::deflate_monitor_list_using_JT(ObjectMonitor** list_p,</span>
<span class="line-added">2330                                                       int* count_p,</span>
<span class="line-added">2331                                                       ObjectMonitor** free_head_p,</span>
<span class="line-added">2332                                                       ObjectMonitor** free_tail_p,</span>
<span class="line-added">2333                                                       ObjectMonitor** saved_mid_in_use_p) {</span>
<span class="line-added">2334   assert(AsyncDeflateIdleMonitors, &quot;sanity check&quot;);</span>
<span class="line-added">2335   JavaThread* self = JavaThread::current();</span>
<span class="line-added">2336 </span>
<span class="line-added">2337   ObjectMonitor* cur_mid_in_use = NULL;</span>
<span class="line-added">2338   ObjectMonitor* mid = NULL;</span>
<span class="line-added">2339   ObjectMonitor* next = NULL;</span>
<span class="line-added">2340   ObjectMonitor* next_next = NULL;</span>
<span class="line-added">2341   int deflated_count = 0;</span>
<span class="line-added">2342   NoSafepointVerifier nsv;</span>
<span class="line-added">2343 </span>
<span class="line-added">2344   // We use the more complicated lock-cur_mid_in_use-and-mid-as-we-go</span>
<span class="line-added">2345   // protocol because om_release() can do list deletions in parallel;</span>
<span class="line-added">2346   // this also prevents races with a list walker thread. We also</span>
<span class="line-added">2347   // lock-next-next-as-we-go to prevent an om_flush() that is behind</span>
<span class="line-added">2348   // this thread from passing us.</span>
<span class="line-added">2349   if (*saved_mid_in_use_p == NULL) {</span>
<span class="line-added">2350     // No saved state so start at the beginning.</span>
<span class="line-added">2351     // Lock the list head so we can possibly deflate it:</span>
<span class="line-added">2352     if ((mid = get_list_head_locked(list_p)) == NULL) {</span>
<span class="line-added">2353       return 0;  // The list is empty so nothing to deflate.</span>
<span class="line-added">2354     }</span>
<span class="line-added">2355     next = unmarked_next(mid);</span>
<span class="line-added">2356   } else {</span>
<span class="line-added">2357     // We&#39;re restarting after a safepoint so restore the necessary state</span>
<span class="line-added">2358     // before we resume.</span>
<span class="line-added">2359     cur_mid_in_use = *saved_mid_in_use_p;</span>
<span class="line-added">2360     // Lock cur_mid_in_use so we can possibly update its</span>
<span class="line-added">2361     // next field to extract a deflated ObjectMonitor.</span>
<span class="line-added">2362     om_lock(cur_mid_in_use);</span>
<span class="line-added">2363     mid = unmarked_next(cur_mid_in_use);</span>
<span class="line-added">2364     if (mid == NULL) {</span>
<span class="line-added">2365       om_unlock(cur_mid_in_use);</span>
<span class="line-added">2366       *saved_mid_in_use_p = NULL;</span>
<span class="line-added">2367       return 0;  // The remainder is empty so nothing more to deflate.</span>
<span class="line-added">2368     }</span>
<span class="line-added">2369     // Lock mid so we can possibly deflate it:</span>
<span class="line-added">2370     om_lock(mid);</span>
<span class="line-added">2371     next = unmarked_next(mid);</span>
<span class="line-added">2372   }</span>
<span class="line-added">2373 </span>
<span class="line-added">2374   while (true) {</span>
<span class="line-added">2375     // The current mid is locked at this point. If we have a</span>
<span class="line-added">2376     // cur_mid_in_use, then it is also locked at this point.</span>
<span class="line-added">2377 </span>
<span class="line-added">2378     if (next != NULL) {</span>
<span class="line-added">2379       // We lock next so that an om_flush() thread that is behind us</span>
<span class="line-added">2380       // cannot pass us when we unlock the current mid.</span>
<span class="line-added">2381       om_lock(next);</span>
<span class="line-added">2382       next_next = unmarked_next(next);</span>
<span class="line-added">2383     }</span>
<span class="line-added">2384 </span>
<span class="line-added">2385     // Only try to deflate if there is an associated Java object and if</span>
<span class="line-added">2386     // mid is old (is not newly allocated and is not newly freed).</span>
<span class="line-added">2387     if (mid-&gt;object() != NULL &amp;&amp; mid-&gt;is_old() &amp;&amp;</span>
<span class="line-added">2388         deflate_monitor_using_JT(mid, free_head_p, free_tail_p)) {</span>
<span class="line-added">2389       // Deflation succeeded and already updated free_head_p and</span>
<span class="line-added">2390       // free_tail_p as needed. Finish the move to the local free list</span>
<span class="line-added">2391       // by unlinking mid from the global or per-thread in-use list.</span>
<span class="line-added">2392       if (cur_mid_in_use == NULL) {</span>
<span class="line-added">2393         // mid is the list head and it is locked. Switch the list head</span>
<span class="line-added">2394         // to next which is also locked (if not NULL) and also leave</span>
<span class="line-added">2395         // mid locked:</span>
<span class="line-added">2396         Atomic::store(list_p, next);</span>
<span class="line-added">2397       } else {</span>
<span class="line-added">2398         ObjectMonitor* locked_next = mark_om_ptr(next);</span>
<span class="line-added">2399         // mid and cur_mid_in_use are locked. Switch cur_mid_in_use&#39;s</span>
<span class="line-added">2400         // next field to locked_next and also leave mid locked:</span>
<span class="line-added">2401         cur_mid_in_use-&gt;set_next_om(locked_next);</span>
<span class="line-added">2402       }</span>
<span class="line-added">2403       // At this point mid is disconnected from the in-use list so</span>
<span class="line-added">2404       // its lock longer has any effects on in-use list.</span>
<span class="line-added">2405       deflated_count++;</span>
<span class="line-added">2406       Atomic::dec(count_p);</span>
<span class="line-added">2407       // mid is current tail in the free_head_p list so NULL terminate it</span>
<span class="line-added">2408       // (which also unlocks it):</span>
<span class="line-added">2409       mid-&gt;set_next_om(NULL);</span>
<span class="line-added">2410 </span>
<span class="line-added">2411       // All the list management is done so move on to the next one:</span>
<span class="line-added">2412       mid = next;  // mid keeps non-NULL next&#39;s locked state</span>
<span class="line-added">2413       next = next_next;</span>
<span class="line-added">2414     } else {</span>
<span class="line-added">2415       // mid is considered in-use if it does not have an associated</span>
<span class="line-added">2416       // Java object or mid is not old or deflation did not succeed.</span>
<span class="line-added">2417       // A mid-&gt;is_new() node can be seen here when it is freshly</span>
<span class="line-added">2418       // returned by om_alloc() (and skips the deflation code path).</span>
<span class="line-added">2419       // A mid-&gt;is_old() node can be seen here when deflation failed.</span>
<span class="line-added">2420       // A mid-&gt;is_free() node can be seen here when a fresh node from</span>
<span class="line-added">2421       // om_alloc() is released by om_release() due to losing the race</span>
<span class="line-added">2422       // in inflate().</span>
<span class="line-added">2423 </span>
<span class="line-added">2424       // All the list management is done so move on to the next one:</span>
<span class="line-added">2425       if (cur_mid_in_use != NULL) {</span>
<span class="line-added">2426         om_unlock(cur_mid_in_use);</span>
<span class="line-added">2427       }</span>
<span class="line-added">2428       // The next cur_mid_in_use keeps mid&#39;s lock state so</span>
<span class="line-added">2429       // that it is stable for a possible next field change. It</span>
<span class="line-added">2430       // cannot be modified by om_release() while it is locked.</span>
<span class="line-added">2431       cur_mid_in_use = mid;</span>
<span class="line-added">2432       mid = next;  // mid keeps non-NULL next&#39;s locked state</span>
<span class="line-added">2433       next = next_next;</span>
<span class="line-added">2434 </span>
<span class="line-added">2435       if (SafepointMechanism::should_block(self) &amp;&amp;</span>
<span class="line-added">2436           cur_mid_in_use != Atomic::load(list_p) &amp;&amp; cur_mid_in_use-&gt;is_old()) {</span>
<span class="line-added">2437         // If a safepoint has started and cur_mid_in_use is not the list</span>
<span class="line-added">2438         // head and is old, then it is safe to use as saved state. Return</span>
<span class="line-added">2439         // to the caller before blocking.</span>
<span class="line-added">2440         *saved_mid_in_use_p = cur_mid_in_use;</span>
<span class="line-added">2441         om_unlock(cur_mid_in_use);</span>
<span class="line-added">2442         if (mid != NULL) {</span>
<span class="line-added">2443           om_unlock(mid);</span>
<span class="line-added">2444         }</span>
<span class="line-added">2445         return deflated_count;</span>
<span class="line-added">2446       }</span>
<span class="line-added">2447     }</span>
<span class="line-added">2448     if (mid == NULL) {</span>
<span class="line-added">2449       if (cur_mid_in_use != NULL) {</span>
<span class="line-added">2450         om_unlock(cur_mid_in_use);</span>
<span class="line-added">2451       }</span>
<span class="line-added">2452       break;  // Reached end of the list so nothing more to deflate.</span>
<span class="line-added">2453     }</span>
<span class="line-added">2454 </span>
<span class="line-added">2455     // The current mid&#39;s next field is locked at this point. If we have</span>
<span class="line-added">2456     // a cur_mid_in_use, then it is also locked at this point.</span>
<span class="line-added">2457   }</span>
<span class="line-added">2458   // We finished the list without a safepoint starting so there&#39;s</span>
<span class="line-added">2459   // no need to save state.</span>
<span class="line-added">2460   *saved_mid_in_use_p = NULL;</span>
<span class="line-added">2461   return deflated_count;</span>
<span class="line-added">2462 }</span>
<span class="line-added">2463 </span>
2464 void ObjectSynchronizer::prepare_deflate_idle_monitors(DeflateMonitorCounters* counters) {
2465   counters-&gt;n_in_use = 0;              // currently associated with objects
2466   counters-&gt;n_in_circulation = 0;      // extant
2467   counters-&gt;n_scavenged = 0;           // reclaimed (global and per-thread)
2468   counters-&gt;per_thread_scavenged = 0;  // per-thread scavenge total
2469   counters-&gt;per_thread_times = 0.0;    // per-thread scavenge times
2470 }
2471 
2472 void ObjectSynchronizer::deflate_idle_monitors(DeflateMonitorCounters* counters) {
2473   assert(SafepointSynchronize::is_at_safepoint(), &quot;must be at safepoint&quot;);
<span class="line-added">2474 </span>
<span class="line-added">2475   if (AsyncDeflateIdleMonitors) {</span>
<span class="line-added">2476     // Nothing to do when global idle ObjectMonitors are deflated using</span>
<span class="line-added">2477     // a JavaThread unless a special deflation has been requested.</span>
<span class="line-added">2478     if (!is_special_deflation_requested()) {</span>
<span class="line-added">2479       return;</span>
<span class="line-added">2480     }</span>
<span class="line-added">2481   }</span>
<span class="line-added">2482 </span>
2483   bool deflated = false;
2484 
2485   ObjectMonitor* free_head_p = NULL;  // Local SLL of scavenged monitors
2486   ObjectMonitor* free_tail_p = NULL;
2487   elapsedTimer timer;
2488 
2489   if (log_is_enabled(Info, monitorinflation)) {
2490     timer.start();
2491   }
2492 
2493   // Note: the thread-local monitors lists get deflated in
2494   // a separate pass. See deflate_thread_local_monitors().
2495 
2496   // For moribund threads, scan om_list_globals._in_use_list
2497   int deflated_count = 0;
2498   if (Atomic::load(&amp;om_list_globals._in_use_list) != NULL) {
2499     // Update n_in_circulation before om_list_globals._in_use_count is
2500     // updated by deflation.
2501     Atomic::add(&amp;counters-&gt;n_in_circulation,
2502                 Atomic::load(&amp;om_list_globals._in_use_count));
</pre>
<hr />
<pre>
2515 #endif
2516     assert(l_next_om == NULL, &quot;must be NULL: _next_om=&quot; INTPTR_FORMAT, p2i(l_next_om));
2517     prepend_list_to_global_free_list(free_head_p, free_tail_p, deflated_count);
2518     Atomic::add(&amp;counters-&gt;n_scavenged, deflated_count);
2519   }
2520   timer.stop();
2521 
2522   LogStreamHandle(Debug, monitorinflation) lsh_debug;
2523   LogStreamHandle(Info, monitorinflation) lsh_info;
2524   LogStream* ls = NULL;
2525   if (log_is_enabled(Debug, monitorinflation)) {
2526     ls = &amp;lsh_debug;
2527   } else if (deflated_count != 0 &amp;&amp; log_is_enabled(Info, monitorinflation)) {
2528     ls = &amp;lsh_info;
2529   }
2530   if (ls != NULL) {
2531     ls-&gt;print_cr(&quot;deflating global idle monitors, %3.7f secs, %d monitors&quot;, timer.seconds(), deflated_count);
2532   }
2533 }
2534 
<span class="line-added">2535 class HandshakeForDeflation : public HandshakeClosure {</span>
<span class="line-added">2536  public:</span>
<span class="line-added">2537   HandshakeForDeflation() : HandshakeClosure(&quot;HandshakeForDeflation&quot;) {}</span>
<span class="line-added">2538 </span>
<span class="line-added">2539   void do_thread(Thread* thread) {</span>
<span class="line-added">2540     log_trace(monitorinflation)(&quot;HandshakeForDeflation::do_thread: thread=&quot;</span>
<span class="line-added">2541                                 INTPTR_FORMAT, p2i(thread));</span>
<span class="line-added">2542   }</span>
<span class="line-added">2543 };</span>
<span class="line-added">2544 </span>
<span class="line-added">2545 void ObjectSynchronizer::deflate_idle_monitors_using_JT() {</span>
<span class="line-added">2546   assert(AsyncDeflateIdleMonitors, &quot;sanity check&quot;);</span>
<span class="line-added">2547 </span>
<span class="line-added">2548   // Deflate any global idle monitors.</span>
<span class="line-added">2549   deflate_global_idle_monitors_using_JT();</span>
<span class="line-added">2550 </span>
<span class="line-added">2551   int count = 0;</span>
<span class="line-added">2552   for (JavaThreadIteratorWithHandle jtiwh; JavaThread *jt = jtiwh.next(); ) {</span>
<span class="line-added">2553     if (Atomic::load(&amp;jt-&gt;om_in_use_count) &gt; 0 &amp;&amp; !jt-&gt;is_exiting()) {</span>
<span class="line-added">2554       // This JavaThread is using ObjectMonitors so deflate any that</span>
<span class="line-added">2555       // are idle unless this JavaThread is exiting; do not race with</span>
<span class="line-added">2556       // ObjectSynchronizer::om_flush().</span>
<span class="line-added">2557       deflate_per_thread_idle_monitors_using_JT(jt);</span>
<span class="line-added">2558       count++;</span>
<span class="line-added">2559     }</span>
<span class="line-added">2560   }</span>
<span class="line-added">2561   if (count &gt; 0) {</span>
<span class="line-added">2562     log_debug(monitorinflation)(&quot;did async deflation of idle monitors for %d thread(s).&quot;, count);</span>
<span class="line-added">2563   }</span>
<span class="line-added">2564 </span>
<span class="line-added">2565   log_info(monitorinflation)(&quot;async global_population=%d, global_in_use_count=%d, &quot;</span>
<span class="line-added">2566                              &quot;global_free_count=%d, global_wait_count=%d&quot;,</span>
<span class="line-added">2567                              Atomic::load(&amp;om_list_globals._population),</span>
<span class="line-added">2568                              Atomic::load(&amp;om_list_globals._in_use_count),</span>
<span class="line-added">2569                              Atomic::load(&amp;om_list_globals._free_count),</span>
<span class="line-added">2570                              Atomic::load(&amp;om_list_globals._wait_count));</span>
<span class="line-added">2571 </span>
<span class="line-added">2572   // The ServiceThread&#39;s async deflation request has been processed.</span>
<span class="line-added">2573   set_is_async_deflation_requested(false);</span>
<span class="line-added">2574 </span>
<span class="line-added">2575   if (Atomic::load(&amp;om_list_globals._wait_count) &gt; 0) {</span>
<span class="line-added">2576     // There are deflated ObjectMonitors waiting for a handshake</span>
<span class="line-added">2577     // (or a safepoint) for safety.</span>
<span class="line-added">2578 </span>
<span class="line-added">2579     ObjectMonitor* list = Atomic::load(&amp;om_list_globals._wait_list);</span>
<span class="line-added">2580     ADIM_guarantee(list != NULL, &quot;om_list_globals._wait_list must not be NULL&quot;);</span>
<span class="line-added">2581     int count = Atomic::load(&amp;om_list_globals._wait_count);</span>
<span class="line-added">2582     Atomic::store(&amp;om_list_globals._wait_count, 0);</span>
<span class="line-added">2583     Atomic::store(&amp;om_list_globals._wait_list, (ObjectMonitor*)NULL);</span>
<span class="line-added">2584 </span>
<span class="line-added">2585     // Find the tail for prepend_list_to_common(). No need to mark</span>
<span class="line-added">2586     // ObjectMonitors for this list walk since only the deflater</span>
<span class="line-added">2587     // thread manages the wait list.</span>
<span class="line-added">2588     int l_count = 0;</span>
<span class="line-added">2589     ObjectMonitor* tail = NULL;</span>
<span class="line-added">2590     for (ObjectMonitor* n = list; n != NULL; n = unmarked_next(n)) {</span>
<span class="line-added">2591       tail = n;</span>
<span class="line-added">2592       l_count++;</span>
<span class="line-added">2593     }</span>
<span class="line-added">2594     ADIM_guarantee(count == l_count, &quot;count=%d != l_count=%d&quot;, count, l_count);</span>
<span class="line-added">2595 </span>
<span class="line-added">2596     // Will execute a safepoint if !ThreadLocalHandshakes:</span>
<span class="line-added">2597     HandshakeForDeflation hfd_hc;</span>
<span class="line-added">2598     Handshake::execute(&amp;hfd_hc);</span>
<span class="line-added">2599 </span>
<span class="line-added">2600     prepend_list_to_common(list, tail, count, &amp;om_list_globals._free_list,</span>
<span class="line-added">2601                            &amp;om_list_globals._free_count);</span>
<span class="line-added">2602 </span>
<span class="line-added">2603     log_info(monitorinflation)(&quot;moved %d idle monitors from global waiting list to global free list&quot;, count);</span>
<span class="line-added">2604   }</span>
<span class="line-added">2605 }</span>
<span class="line-added">2606 </span>
<span class="line-added">2607 // Deflate global idle ObjectMonitors using a JavaThread.</span>
<span class="line-added">2608 //</span>
<span class="line-added">2609 void ObjectSynchronizer::deflate_global_idle_monitors_using_JT() {</span>
<span class="line-added">2610   assert(AsyncDeflateIdleMonitors, &quot;sanity check&quot;);</span>
<span class="line-added">2611   assert(Thread::current()-&gt;is_Java_thread(), &quot;precondition&quot;);</span>
<span class="line-added">2612   JavaThread* self = JavaThread::current();</span>
<span class="line-added">2613 </span>
<span class="line-added">2614   deflate_common_idle_monitors_using_JT(true /* is_global */, self);</span>
<span class="line-added">2615 }</span>
<span class="line-added">2616 </span>
<span class="line-added">2617 // Deflate the specified JavaThread&#39;s idle ObjectMonitors using a JavaThread.</span>
<span class="line-added">2618 //</span>
<span class="line-added">2619 void ObjectSynchronizer::deflate_per_thread_idle_monitors_using_JT(JavaThread* target) {</span>
<span class="line-added">2620   assert(AsyncDeflateIdleMonitors, &quot;sanity check&quot;);</span>
<span class="line-added">2621   assert(Thread::current()-&gt;is_Java_thread(), &quot;precondition&quot;);</span>
<span class="line-added">2622 </span>
<span class="line-added">2623   deflate_common_idle_monitors_using_JT(false /* !is_global */, target);</span>
<span class="line-added">2624 }</span>
<span class="line-added">2625 </span>
<span class="line-added">2626 // Deflate global or per-thread idle ObjectMonitors using a JavaThread.</span>
<span class="line-added">2627 //</span>
<span class="line-added">2628 void ObjectSynchronizer::deflate_common_idle_monitors_using_JT(bool is_global, JavaThread* target) {</span>
<span class="line-added">2629   JavaThread* self = JavaThread::current();</span>
<span class="line-added">2630 </span>
<span class="line-added">2631   int deflated_count = 0;</span>
<span class="line-added">2632   ObjectMonitor* free_head_p = NULL;  // Local SLL of scavenged ObjectMonitors</span>
<span class="line-added">2633   ObjectMonitor* free_tail_p = NULL;</span>
<span class="line-added">2634   ObjectMonitor* saved_mid_in_use_p = NULL;</span>
<span class="line-added">2635   elapsedTimer timer;</span>
<span class="line-added">2636 </span>
<span class="line-added">2637   if (log_is_enabled(Info, monitorinflation)) {</span>
<span class="line-added">2638     timer.start();</span>
<span class="line-added">2639   }</span>
<span class="line-added">2640 </span>
<span class="line-added">2641   if (is_global) {</span>
<span class="line-added">2642     OM_PERFDATA_OP(MonExtant, set_value(Atomic::load(&amp;om_list_globals._in_use_count)));</span>
<span class="line-added">2643   } else {</span>
<span class="line-added">2644     OM_PERFDATA_OP(MonExtant, inc(Atomic::load(&amp;target-&gt;om_in_use_count)));</span>
<span class="line-added">2645   }</span>
<span class="line-added">2646 </span>
<span class="line-added">2647   do {</span>
<span class="line-added">2648     if (saved_mid_in_use_p != NULL) {</span>
<span class="line-added">2649       // We looped around because deflate_monitor_list_using_JT()</span>
<span class="line-added">2650       // detected a pending safepoint. Honoring the safepoint is good,</span>
<span class="line-added">2651       // but as long as is_special_deflation_requested() is supported,</span>
<span class="line-added">2652       // we can&#39;t safely restart using saved_mid_in_use_p. That saved</span>
<span class="line-added">2653       // ObjectMonitor could have been deflated by safepoint based</span>
<span class="line-added">2654       // deflation and would no longer be on the in-use list where we</span>
<span class="line-added">2655       // originally found it.</span>
<span class="line-added">2656       saved_mid_in_use_p = NULL;</span>
<span class="line-added">2657     }</span>
<span class="line-added">2658     int local_deflated_count;</span>
<span class="line-added">2659     if (is_global) {</span>
<span class="line-added">2660       local_deflated_count =</span>
<span class="line-added">2661           deflate_monitor_list_using_JT(&amp;om_list_globals._in_use_list,</span>
<span class="line-added">2662                                         &amp;om_list_globals._in_use_count,</span>
<span class="line-added">2663                                         &amp;free_head_p, &amp;free_tail_p,</span>
<span class="line-added">2664                                         &amp;saved_mid_in_use_p);</span>
<span class="line-added">2665     } else {</span>
<span class="line-added">2666       local_deflated_count =</span>
<span class="line-added">2667           deflate_monitor_list_using_JT(&amp;target-&gt;om_in_use_list,</span>
<span class="line-added">2668                                         &amp;target-&gt;om_in_use_count, &amp;free_head_p,</span>
<span class="line-added">2669                                         &amp;free_tail_p, &amp;saved_mid_in_use_p);</span>
<span class="line-added">2670     }</span>
<span class="line-added">2671     deflated_count += local_deflated_count;</span>
<span class="line-added">2672 </span>
<span class="line-added">2673     if (free_head_p != NULL) {</span>
<span class="line-added">2674       // Move the deflated ObjectMonitors to the global free list.</span>
<span class="line-added">2675       guarantee(free_tail_p != NULL &amp;&amp; local_deflated_count &gt; 0, &quot;free_tail_p=&quot; INTPTR_FORMAT &quot;, local_deflated_count=%d&quot;, p2i(free_tail_p), local_deflated_count);</span>
<span class="line-added">2676       // Note: The target thread can be doing an om_alloc() that</span>
<span class="line-added">2677       // is trying to prepend an ObjectMonitor on its in-use list</span>
<span class="line-added">2678       // at the same time that we have deflated the current in-use</span>
<span class="line-added">2679       // list head and put it on the local free list. prepend_to_common()</span>
<span class="line-added">2680       // will detect the race and retry which avoids list corruption,</span>
<span class="line-added">2681       // but the next field in free_tail_p can flicker to marked</span>
<span class="line-added">2682       // and then unmarked while prepend_to_common() is sorting it</span>
<span class="line-added">2683       // all out.</span>
<span class="line-added">2684 #ifdef ASSERT</span>
<span class="line-added">2685       ObjectMonitor* l_next_om = unmarked_next(free_tail_p);</span>
<span class="line-added">2686 #endif</span>
<span class="line-added">2687       assert(l_next_om == NULL, &quot;must be NULL: _next_om=&quot; INTPTR_FORMAT, p2i(l_next_om));</span>
<span class="line-added">2688 </span>
<span class="line-added">2689       prepend_list_to_global_wait_list(free_head_p, free_tail_p, local_deflated_count);</span>
<span class="line-added">2690 </span>
<span class="line-added">2691       OM_PERFDATA_OP(Deflations, inc(local_deflated_count));</span>
<span class="line-added">2692     }</span>
<span class="line-added">2693 </span>
<span class="line-added">2694     if (saved_mid_in_use_p != NULL) {</span>
<span class="line-added">2695       // deflate_monitor_list_using_JT() detected a safepoint starting.</span>
<span class="line-added">2696       timer.stop();</span>
<span class="line-added">2697       {</span>
<span class="line-added">2698         if (is_global) {</span>
<span class="line-added">2699           log_debug(monitorinflation)(&quot;pausing deflation of global idle monitors for a safepoint.&quot;);</span>
<span class="line-added">2700         } else {</span>
<span class="line-added">2701           log_debug(monitorinflation)(&quot;jt=&quot; INTPTR_FORMAT &quot;: pausing deflation of per-thread idle monitors for a safepoint.&quot;, p2i(target));</span>
<span class="line-added">2702         }</span>
<span class="line-added">2703         assert(SafepointMechanism::should_block(self), &quot;sanity check&quot;);</span>
<span class="line-added">2704         ThreadBlockInVM blocker(self);</span>
<span class="line-added">2705       }</span>
<span class="line-added">2706       // Prepare for another loop after the safepoint.</span>
<span class="line-added">2707       free_head_p = NULL;</span>
<span class="line-added">2708       free_tail_p = NULL;</span>
<span class="line-added">2709       if (log_is_enabled(Info, monitorinflation)) {</span>
<span class="line-added">2710         timer.start();</span>
<span class="line-added">2711       }</span>
<span class="line-added">2712     }</span>
<span class="line-added">2713   } while (saved_mid_in_use_p != NULL);</span>
<span class="line-added">2714   timer.stop();</span>
<span class="line-added">2715 </span>
<span class="line-added">2716   LogStreamHandle(Debug, monitorinflation) lsh_debug;</span>
<span class="line-added">2717   LogStreamHandle(Info, monitorinflation) lsh_info;</span>
<span class="line-added">2718   LogStream* ls = NULL;</span>
<span class="line-added">2719   if (log_is_enabled(Debug, monitorinflation)) {</span>
<span class="line-added">2720     ls = &amp;lsh_debug;</span>
<span class="line-added">2721   } else if (deflated_count != 0 &amp;&amp; log_is_enabled(Info, monitorinflation)) {</span>
<span class="line-added">2722     ls = &amp;lsh_info;</span>
<span class="line-added">2723   }</span>
<span class="line-added">2724   if (ls != NULL) {</span>
<span class="line-added">2725     if (is_global) {</span>
<span class="line-added">2726       ls-&gt;print_cr(&quot;async-deflating global idle monitors, %3.7f secs, %d monitors&quot;, timer.seconds(), deflated_count);</span>
<span class="line-added">2727     } else {</span>
<span class="line-added">2728       ls-&gt;print_cr(&quot;jt=&quot; INTPTR_FORMAT &quot;: async-deflating per-thread idle monitors, %3.7f secs, %d monitors&quot;, p2i(target), timer.seconds(), deflated_count);</span>
<span class="line-added">2729     }</span>
<span class="line-added">2730   }</span>
<span class="line-added">2731 }</span>
<span class="line-added">2732 </span>
2733 void ObjectSynchronizer::finish_deflate_idle_monitors(DeflateMonitorCounters* counters) {
2734   // Report the cumulative time for deflating each thread&#39;s idle
2735   // monitors. Note: if the work is split among more than one
2736   // worker thread, then the reported time will likely be more
2737   // than a beginning to end measurement of the phase.
2738   log_info(safepoint, cleanup)(&quot;deflating per-thread idle monitors, %3.7f secs, monitors=%d&quot;, counters-&gt;per_thread_times, counters-&gt;per_thread_scavenged);
2739 
<span class="line-added">2740   bool needs_special_deflation = is_special_deflation_requested();</span>
<span class="line-added">2741   if (AsyncDeflateIdleMonitors &amp;&amp; !needs_special_deflation) {</span>
<span class="line-added">2742     // Nothing to do when idle ObjectMonitors are deflated using</span>
<span class="line-added">2743     // a JavaThread unless a special deflation has been requested.</span>
<span class="line-added">2744     return;</span>
<span class="line-added">2745   }</span>
<span class="line-added">2746 </span>
2747   if (log_is_enabled(Debug, monitorinflation)) {
2748     // exit_globals()&#39;s call to audit_and_print_stats() is done
2749     // at the Info level and not at a safepoint.
<span class="line-added">2750     // For async deflation, audit_and_print_stats() is called in</span>
<span class="line-added">2751     // ObjectSynchronizer::do_safepoint_work() at the Debug level</span>
<span class="line-added">2752     // at a safepoint.</span>
2753     ObjectSynchronizer::audit_and_print_stats(false /* on_exit */);
2754   } else if (log_is_enabled(Info, monitorinflation)) {
2755     log_info(monitorinflation)(&quot;global_population=%d, global_in_use_count=%d, &quot;
<span class="line-modified">2756                                &quot;global_free_count=%d, global_wait_count=%d&quot;,</span>
2757                                Atomic::load(&amp;om_list_globals._population),
2758                                Atomic::load(&amp;om_list_globals._in_use_count),
<span class="line-modified">2759                                Atomic::load(&amp;om_list_globals._free_count),</span>
<span class="line-added">2760                                Atomic::load(&amp;om_list_globals._wait_count));</span>
2761   }
2762 
2763   OM_PERFDATA_OP(Deflations, inc(counters-&gt;n_scavenged));
2764   OM_PERFDATA_OP(MonExtant, set_value(counters-&gt;n_in_circulation));
2765 
2766   GVars.stw_random = os::random();
2767   GVars.stw_cycle++;
<span class="line-added">2768 </span>
<span class="line-added">2769   if (needs_special_deflation) {</span>
<span class="line-added">2770     set_is_special_deflation_requested(false);  // special deflation is done</span>
<span class="line-added">2771   }</span>
2772 }
2773 
2774 void ObjectSynchronizer::deflate_thread_local_monitors(Thread* thread, DeflateMonitorCounters* counters) {
2775   assert(SafepointSynchronize::is_at_safepoint(), &quot;must be at safepoint&quot;);
2776 
<span class="line-added">2777   if (AsyncDeflateIdleMonitors &amp;&amp; !is_special_deflation_requested()) {</span>
<span class="line-added">2778     // Nothing to do if a special deflation has NOT been requested.</span>
<span class="line-added">2779     return;</span>
<span class="line-added">2780   }</span>
<span class="line-added">2781 </span>
2782   ObjectMonitor* free_head_p = NULL;  // Local SLL of scavenged monitors
2783   ObjectMonitor* free_tail_p = NULL;
2784   elapsedTimer timer;
2785 
2786   if (log_is_enabled(Info, safepoint, cleanup) ||
2787       log_is_enabled(Info, monitorinflation)) {
2788     timer.start();
2789   }
2790 
2791   // Update n_in_circulation before om_in_use_count is updated by deflation.
2792   Atomic::add(&amp;counters-&gt;n_in_circulation, Atomic::load(&amp;thread-&gt;om_in_use_count));
2793 
2794   int deflated_count = deflate_monitor_list(&amp;thread-&gt;om_in_use_list, &amp;thread-&gt;om_in_use_count, &amp;free_head_p, &amp;free_tail_p);
2795   Atomic::add(&amp;counters-&gt;n_in_use, Atomic::load(&amp;thread-&gt;om_in_use_count));
2796 
2797   if (free_head_p != NULL) {
2798     // Move the deflated ObjectMonitors back to the global free list.
2799     guarantee(free_tail_p != NULL &amp;&amp; deflated_count &gt; 0, &quot;invariant&quot;);
2800 #ifdef ASSERT
2801     ObjectMonitor* l_next_om = free_tail_p-&gt;next_om();
</pre>
<hr />
<pre>
2935   if (Atomic::load(&amp;om_list_globals._population) == chk_om_population) {
2936     ls-&gt;print_cr(&quot;global_population=%d equals chk_om_population=%d&quot;,
2937                  Atomic::load(&amp;om_list_globals._population), chk_om_population);
2938   } else {
2939     // With fine grained locks on the monitor lists, it is possible for
2940     // log_monitor_list_counts() to return a value that doesn&#39;t match
2941     // om_list_globals._population. So far a higher value has been
2942     // seen in testing so something is being double counted by
2943     // log_monitor_list_counts().
2944     ls-&gt;print_cr(&quot;WARNING: global_population=%d is not equal to &quot;
2945                  &quot;chk_om_population=%d&quot;,
2946                  Atomic::load(&amp;om_list_globals._population), chk_om_population);
2947   }
2948 
2949   // Check om_list_globals._in_use_list and om_list_globals._in_use_count:
2950   chk_global_in_use_list_and_count(ls, &amp;error_cnt);
2951 
2952   // Check om_list_globals._free_list and om_list_globals._free_count:
2953   chk_global_free_list_and_count(ls, &amp;error_cnt);
2954 
<span class="line-added">2955   // Check om_list_globals._wait_list and om_list_globals._wait_count:</span>
<span class="line-added">2956   chk_global_wait_list_and_count(ls, &amp;error_cnt);</span>
<span class="line-added">2957 </span>
2958   ls-&gt;print_cr(&quot;Checking per-thread lists:&quot;);
2959 
2960   for (JavaThreadIteratorWithHandle jtiwh; JavaThread *jt = jtiwh.next(); ) {
2961     // Check om_in_use_list and om_in_use_count:
2962     chk_per_thread_in_use_list_and_count(jt, ls, &amp;error_cnt);
2963 
2964     // Check om_free_list and om_free_count:
2965     chk_per_thread_free_list_and_count(jt, ls, &amp;error_cnt);
2966   }
2967 
2968   if (error_cnt == 0) {
2969     ls-&gt;print_cr(&quot;No errors found in monitor list checks.&quot;);
2970   } else {
2971     log_error(monitorinflation)(&quot;found monitor list errors: error_cnt=%d&quot;, error_cnt);
2972   }
2973 
2974   if ((on_exit &amp;&amp; log_is_enabled(Info, monitorinflation)) ||
2975       (!on_exit &amp;&amp; log_is_enabled(Trace, monitorinflation))) {
2976     // When exiting this log output is at the Info level. When called
2977     // at a safepoint, this log output is at the Trace level since
</pre>
<hr />
<pre>
2988 void ObjectSynchronizer::chk_free_entry(JavaThread* jt, ObjectMonitor* n,
2989                                         outputStream * out, int *error_cnt_p) {
2990   stringStream ss;
2991   if (n-&gt;is_busy()) {
2992     if (jt != NULL) {
2993       out-&gt;print_cr(&quot;ERROR: jt=&quot; INTPTR_FORMAT &quot;, monitor=&quot; INTPTR_FORMAT
2994                     &quot;: free per-thread monitor must not be busy: %s&quot;, p2i(jt),
2995                     p2i(n), n-&gt;is_busy_to_string(&amp;ss));
2996     } else {
2997       out-&gt;print_cr(&quot;ERROR: monitor=&quot; INTPTR_FORMAT &quot;: free global monitor &quot;
2998                     &quot;must not be busy: %s&quot;, p2i(n), n-&gt;is_busy_to_string(&amp;ss));
2999     }
3000     *error_cnt_p = *error_cnt_p + 1;
3001   }
3002   if (n-&gt;header().value() != 0) {
3003     if (jt != NULL) {
3004       out-&gt;print_cr(&quot;ERROR: jt=&quot; INTPTR_FORMAT &quot;, monitor=&quot; INTPTR_FORMAT
3005                     &quot;: free per-thread monitor must have NULL _header &quot;
3006                     &quot;field: _header=&quot; INTPTR_FORMAT, p2i(jt), p2i(n),
3007                     n-&gt;header().value());
<span class="line-modified">3008       *error_cnt_p = *error_cnt_p + 1;</span>
<span class="line-added">3009     } else if (!AsyncDeflateIdleMonitors) {</span>
3010       out-&gt;print_cr(&quot;ERROR: monitor=&quot; INTPTR_FORMAT &quot;: free global monitor &quot;
3011                     &quot;must have NULL _header field: _header=&quot; INTPTR_FORMAT,
3012                     p2i(n), n-&gt;header().value());
<span class="line-added">3013       *error_cnt_p = *error_cnt_p + 1;</span>
3014     }

3015   }
3016   if (n-&gt;object() != NULL) {
3017     if (jt != NULL) {
3018       out-&gt;print_cr(&quot;ERROR: jt=&quot; INTPTR_FORMAT &quot;, monitor=&quot; INTPTR_FORMAT
3019                     &quot;: free per-thread monitor must have NULL _object &quot;
3020                     &quot;field: _object=&quot; INTPTR_FORMAT, p2i(jt), p2i(n),
3021                     p2i(n-&gt;object()));
3022     } else {
3023       out-&gt;print_cr(&quot;ERROR: monitor=&quot; INTPTR_FORMAT &quot;: free global monitor &quot;
3024                     &quot;must have NULL _object field: _object=&quot; INTPTR_FORMAT,
3025                     p2i(n), p2i(n-&gt;object()));
3026     }
3027     *error_cnt_p = *error_cnt_p + 1;
3028   }
3029 }
3030 
3031 // Lock the next ObjectMonitor for traversal and unlock the current
3032 // ObjectMonitor. Returns the next ObjectMonitor if there is one.
3033 // Otherwise returns NULL (after unlocking the current ObjectMonitor).
3034 // This function is used by the various list walker functions to
</pre>
<hr />
<pre>
3061       if (cur == NULL) {
3062         break;
3063       }
3064     }
3065   }
3066   int l_free_count = Atomic::load(&amp;om_list_globals._free_count);
3067   if (l_free_count == chk_om_free_count) {
3068     out-&gt;print_cr(&quot;global_free_count=%d equals chk_om_free_count=%d&quot;,
3069                   l_free_count, chk_om_free_count);
3070   } else {
3071     // With fine grained locks on om_list_globals._free_list, it
3072     // is possible for an ObjectMonitor to be prepended to
3073     // om_list_globals._free_list after we started calculating
3074     // chk_om_free_count so om_list_globals._free_count may not
3075     // match anymore.
3076     out-&gt;print_cr(&quot;WARNING: global_free_count=%d is not equal to &quot;
3077                   &quot;chk_om_free_count=%d&quot;, l_free_count, chk_om_free_count);
3078   }
3079 }
3080 
<span class="line-added">3081 // Check the global wait list and count; log the results of the checks.</span>
<span class="line-added">3082 void ObjectSynchronizer::chk_global_wait_list_and_count(outputStream * out,</span>
<span class="line-added">3083                                                         int *error_cnt_p) {</span>
<span class="line-added">3084   int chk_om_wait_count = 0;</span>
<span class="line-added">3085   ObjectMonitor* cur = NULL;</span>
<span class="line-added">3086   if ((cur = get_list_head_locked(&amp;om_list_globals._wait_list)) != NULL) {</span>
<span class="line-added">3087     // Marked the global wait list head so process the list.</span>
<span class="line-added">3088     while (true) {</span>
<span class="line-added">3089       // Rules for om_list_globals._wait_list are the same as for</span>
<span class="line-added">3090       // om_list_globals._free_list:</span>
<span class="line-added">3091       chk_free_entry(NULL /* jt */, cur, out, error_cnt_p);</span>
<span class="line-added">3092       chk_om_wait_count++;</span>
<span class="line-added">3093 </span>
<span class="line-added">3094       cur = lock_next_for_traversal(cur);</span>
<span class="line-added">3095       if (cur == NULL) {</span>
<span class="line-added">3096         break;</span>
<span class="line-added">3097       }</span>
<span class="line-added">3098     }</span>
<span class="line-added">3099   }</span>
<span class="line-added">3100   if (Atomic::load(&amp;om_list_globals._wait_count) == chk_om_wait_count) {</span>
<span class="line-added">3101     out-&gt;print_cr(&quot;global_wait_count=%d equals chk_om_wait_count=%d&quot;,</span>
<span class="line-added">3102                   Atomic::load(&amp;om_list_globals._wait_count), chk_om_wait_count);</span>
<span class="line-added">3103   } else {</span>
<span class="line-added">3104     out-&gt;print_cr(&quot;ERROR: global_wait_count=%d is not equal to &quot;</span>
<span class="line-added">3105                   &quot;chk_om_wait_count=%d&quot;,</span>
<span class="line-added">3106                   Atomic::load(&amp;om_list_globals._wait_count), chk_om_wait_count);</span>
<span class="line-added">3107     *error_cnt_p = *error_cnt_p + 1;</span>
<span class="line-added">3108   }</span>
<span class="line-added">3109 }</span>
<span class="line-added">3110 </span>
3111 // Check the global in-use list and count; log the results of the checks.
3112 void ObjectSynchronizer::chk_global_in_use_list_and_count(outputStream * out,
3113                                                           int *error_cnt_p) {
3114   int chk_om_in_use_count = 0;
3115   ObjectMonitor* cur = NULL;
3116   if ((cur = get_list_head_locked(&amp;om_list_globals._in_use_list)) != NULL) {
3117     // Marked the global in-use list head so process the list.
3118     while (true) {
3119       chk_in_use_entry(NULL /* jt */, cur, out, error_cnt_p);
3120       chk_om_in_use_count++;
3121 
3122       cur = lock_next_for_traversal(cur);
3123       if (cur == NULL) {
3124         break;
3125       }
3126     }
3127   }
3128   int l_in_use_count = Atomic::load(&amp;om_list_globals._in_use_count);
3129   if (l_in_use_count == chk_om_in_use_count) {
3130     out-&gt;print_cr(&quot;global_in_use_count=%d equals chk_om_in_use_count=%d&quot;,
</pre>
<hr />
<pre>
3314           out-&gt;print(&quot; (%s)&quot;, cur-&gt;is_busy_to_string(&amp;ss));
3315           ss.reset();
3316         }
3317         out-&gt;cr();
3318 
3319         cur = lock_next_for_traversal(cur);
3320         if (cur == NULL) {
3321           break;
3322         }
3323       }
3324     }
3325   }
3326 
3327   out-&gt;flush();
3328 }
3329 
3330 // Log counts for the global and per-thread monitor lists and return
3331 // the population count.
3332 int ObjectSynchronizer::log_monitor_list_counts(outputStream * out) {
3333   int pop_count = 0;
<span class="line-modified">3334   out-&gt;print_cr(&quot;%18s  %10s  %10s  %10s  %10s&quot;,</span>
<span class="line-modified">3335                 &quot;Global Lists:&quot;, &quot;InUse&quot;, &quot;Free&quot;, &quot;Wait&quot;, &quot;Total&quot;);</span>
<span class="line-modified">3336   out-&gt;print_cr(&quot;==================  ==========  ==========  ==========  ==========&quot;);</span>
3337   int l_in_use_count = Atomic::load(&amp;om_list_globals._in_use_count);
3338   int l_free_count = Atomic::load(&amp;om_list_globals._free_count);
<span class="line-modified">3339   int l_wait_count = Atomic::load(&amp;om_list_globals._wait_count);</span>
<span class="line-modified">3340   out-&gt;print_cr(&quot;%18s  %10d  %10d  %10d  %10d&quot;, &quot;&quot;, l_in_use_count,</span>
<span class="line-modified">3341                 l_free_count, l_wait_count,</span>
<span class="line-added">3342                 Atomic::load(&amp;om_list_globals._population));</span>
<span class="line-added">3343   pop_count += l_in_use_count + l_free_count + l_wait_count;</span>
3344 
3345   out-&gt;print_cr(&quot;%18s  %10s  %10s  %10s&quot;,
3346                 &quot;Per-Thread Lists:&quot;, &quot;InUse&quot;, &quot;Free&quot;, &quot;Provision&quot;);
3347   out-&gt;print_cr(&quot;==================  ==========  ==========  ==========&quot;);
3348 
3349   for (JavaThreadIteratorWithHandle jtiwh; JavaThread *jt = jtiwh.next(); ) {
3350     int l_om_in_use_count = Atomic::load(&amp;jt-&gt;om_in_use_count);
3351     int l_om_free_count = Atomic::load(&amp;jt-&gt;om_free_count);
3352     out-&gt;print_cr(INTPTR_FORMAT &quot;  %10d  %10d  %10d&quot;, p2i(jt),
3353                   l_om_in_use_count, l_om_free_count, jt-&gt;om_free_provision);
3354     pop_count += l_om_in_use_count + l_om_free_count;
3355   }
3356   return pop_count;
3357 }
3358 
3359 #ifndef PRODUCT
3360 
3361 // Check if monitor belongs to the monitor cache
3362 // The list is grow-only so it&#39;s *relatively* safe to traverse
3363 // the list of extant blocks without taking a lock.
</pre>
</td>
</tr>
</table>
<center><a href="sharedRuntime.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="synchronizer.hpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>