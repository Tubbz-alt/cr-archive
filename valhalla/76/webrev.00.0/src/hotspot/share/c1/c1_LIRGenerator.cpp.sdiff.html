<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/share/c1/c1_LIRGenerator.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="c1_LIR.hpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="../ci/ciField.cpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/share/c1/c1_LIRGenerator.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
 465 void LIRGenerator::klass2reg_with_patching(LIR_Opr r, ciMetadata* obj, CodeEmitInfo* info, bool need_resolve) {
 466   /* C2 relies on constant pool entries being resolved (ciTypeFlow), so if TieredCompilation
 467    * is active and the class hasn&#39;t yet been resolved we need to emit a patch that resolves
 468    * the class. */
 469   if ((TieredCompilation &amp;&amp; need_resolve) || !obj-&gt;is_loaded() || PatchALot) {
 470     assert(info != NULL, &quot;info must be set if class is not loaded&quot;);
 471     __ klass2reg_patch(NULL, r, info);
 472   } else {
 473     // no patching needed
 474     __ metadata2reg(obj-&gt;constant_encoding(), r);
 475   }
 476 }
 477 
 478 
 479 void LIRGenerator::array_range_check(LIR_Opr array, LIR_Opr index,
 480                                     CodeEmitInfo* null_check_info, CodeEmitInfo* range_check_info) {
 481   CodeStub* stub = new RangeCheckStub(range_check_info, index, array);
 482   if (index-&gt;is_constant()) {
 483     cmp_mem_int(lir_cond_belowEqual, array, arrayOopDesc::length_offset_in_bytes(),
 484                 index-&gt;as_jint(), null_check_info);
<span class="line-modified"> 485     __ branch(lir_cond_belowEqual, T_INT, stub); // forward branch</span>
 486   } else {
 487     cmp_reg_mem(lir_cond_aboveEqual, index, array,
 488                 arrayOopDesc::length_offset_in_bytes(), T_INT, null_check_info);
<span class="line-modified"> 489     __ branch(lir_cond_aboveEqual, T_INT, stub); // forward branch</span>
 490   }
 491 }
 492 
 493 
 494 void LIRGenerator::nio_range_check(LIR_Opr buffer, LIR_Opr index, LIR_Opr result, CodeEmitInfo* info) {
 495   CodeStub* stub = new RangeCheckStub(info, index);
 496   if (index-&gt;is_constant()) {
 497     cmp_mem_int(lir_cond_belowEqual, buffer, java_nio_Buffer::limit_offset(), index-&gt;as_jint(), info);
<span class="line-modified"> 498     __ branch(lir_cond_belowEqual, T_INT, stub); // forward branch</span>
 499   } else {
 500     cmp_reg_mem(lir_cond_aboveEqual, index, buffer,
 501                 java_nio_Buffer::limit_offset(), T_INT, info);
<span class="line-modified"> 502     __ branch(lir_cond_aboveEqual, T_INT, stub); // forward branch</span>
 503   }
 504   __ move(index, result);
 505 }
 506 
 507 
 508 
 509 void LIRGenerator::arithmetic_op(Bytecodes::Code code, LIR_Opr result, LIR_Opr left, LIR_Opr right, bool is_strictfp, LIR_Opr tmp_op, CodeEmitInfo* info) {
 510   LIR_Opr result_op = result;
 511   LIR_Opr left_op   = left;
 512   LIR_Opr right_op  = right;
 513 
 514   if (TwoOperandLIRForm &amp;&amp; left_op != result_op) {
 515     assert(right_op != result_op, &quot;malformed&quot;);
 516     __ move(left_op, result_op);
 517     left_op = result_op;
 518   }
 519 
 520   switch(code) {
 521     case Bytecodes::_dadd:
 522     case Bytecodes::_fadd:
</pre>
<hr />
<pre>
 674 #endif
 675 
 676 void LIRGenerator::new_instance(LIR_Opr dst, ciInstanceKlass* klass, bool is_unresolved, LIR_Opr scratch1, LIR_Opr scratch2, LIR_Opr scratch3, LIR_Opr scratch4, LIR_Opr klass_reg, CodeEmitInfo* info) {
 677   klass2reg_with_patching(klass_reg, klass, info, is_unresolved);
 678   // If klass is not loaded we do not know if the klass has finalizers:
 679   if (UseFastNewInstance &amp;&amp; klass-&gt;is_loaded()
 680       &amp;&amp; !Klass::layout_helper_needs_slow_path(klass-&gt;layout_helper())) {
 681 
 682     Runtime1::StubID stub_id = klass-&gt;is_initialized() ? Runtime1::fast_new_instance_id : Runtime1::fast_new_instance_init_check_id;
 683 
 684     CodeStub* slow_path = new NewInstanceStub(klass_reg, dst, klass, info, stub_id);
 685 
 686     assert(klass-&gt;is_loaded(), &quot;must be loaded&quot;);
 687     // allocate space for instance
 688     assert(klass-&gt;size_helper() &gt;= 0, &quot;illegal instance size&quot;);
 689     const int instance_size = align_object_size(klass-&gt;size_helper());
 690     __ allocate_object(dst, scratch1, scratch2, scratch3, scratch4,
 691                        oopDesc::header_size(), instance_size, klass_reg, !klass-&gt;is_initialized(), slow_path);
 692   } else {
 693     CodeStub* slow_path = new NewInstanceStub(klass_reg, dst, klass, info, Runtime1::new_instance_id);
<span class="line-modified"> 694     __ branch(lir_cond_always, T_ILLEGAL, slow_path);</span>
 695     __ branch_destination(slow_path-&gt;continuation());
 696   }
 697 }
 698 
 699 
 700 static bool is_constant_zero(Instruction* inst) {
 701   IntConstant* c = inst-&gt;type()-&gt;as_IntConstant();
 702   if (c) {
 703     return (c-&gt;value() == 0);
 704   }
 705   return false;
 706 }
 707 
 708 
 709 static bool positive_constant(Instruction* inst) {
 710   IntConstant* c = inst-&gt;type()-&gt;as_IntConstant();
 711   if (c) {
 712     return (c-&gt;value() &gt;= 0);
 713   }
 714   return false;
</pre>
<hr />
<pre>
1218     args-&gt;append(meth);
1219     call_runtime(&amp;signature, args, CAST_FROM_FN_PTR(address, SharedRuntime::dtrace_method_exit), voidType, NULL);
1220   }
1221 
1222   if (x-&gt;type()-&gt;is_void()) {
1223     __ return_op(LIR_OprFact::illegalOpr);
1224   } else {
1225     LIR_Opr reg = result_register_for(x-&gt;type(), /*callee=*/true);
1226     LIRItem result(x-&gt;result(), this);
1227 
1228     result.load_item_force(reg);
1229     __ return_op(result.result());
1230   }
1231   set_no_result(x);
1232 }
1233 
1234 // Examble: ref.get()
1235 // Combination of LoadField and g1 pre-write barrier
1236 void LIRGenerator::do_Reference_get(Intrinsic* x) {
1237 
<span class="line-modified">1238   const int referent_offset = java_lang_ref_Reference::referent_offset;</span>
<span class="line-removed">1239   guarantee(referent_offset &gt; 0, &quot;referent offset not initialized&quot;);</span>
1240 
1241   assert(x-&gt;number_of_arguments() == 1, &quot;wrong type&quot;);
1242 
1243   LIRItem reference(x-&gt;argument_at(0), this);
1244   reference.load_item();
1245 
1246   // need to perform the null check on the reference objecy
1247   CodeEmitInfo* info = NULL;
1248   if (x-&gt;needs_null_check()) {
1249     info = state_for(x);
1250   }
1251 
1252   LIR_Opr result = rlock_result(x, T_OBJECT);
1253   access_load_at(IN_HEAP | ON_WEAK_OOP_REF, T_OBJECT,
1254                  reference, LIR_OprFact::intConst(referent_offset), result);
1255 }
1256 
1257 // Example: clazz.isInstance(object)
1258 void LIRGenerator::do_isInstance(Intrinsic* x) {
1259   assert(x-&gt;number_of_arguments() == 2, &quot;wrong type&quot;);
</pre>
<hr />
<pre>
1305   __ move_wide(new LIR_Address(temp, in_bytes(Klass::java_mirror_offset()), T_ADDRESS), temp);
1306   // mirror = ((OopHandle)mirror)-&gt;resolve();
1307   access_load(IN_NATIVE, T_OBJECT,
1308               LIR_OprFact::address(new LIR_Address(temp, T_OBJECT)), result);
1309 }
1310 
1311 // java.lang.Class::isPrimitive()
1312 void LIRGenerator::do_isPrimitive(Intrinsic* x) {
1313   assert(x-&gt;number_of_arguments() == 1, &quot;wrong type&quot;);
1314 
1315   LIRItem rcvr(x-&gt;argument_at(0), this);
1316   rcvr.load_item();
1317   LIR_Opr temp = new_register(T_METADATA);
1318   LIR_Opr result = rlock_result(x);
1319 
1320   CodeEmitInfo* info = NULL;
1321   if (x-&gt;needs_null_check()) {
1322     info = state_for(x);
1323   }
1324 
<span class="line-modified">1325   __ move(new LIR_Address(rcvr.result(), java_lang_Class::klass_offset_in_bytes(), T_ADDRESS), temp, info);</span>
1326   __ cmp(lir_cond_notEqual, temp, LIR_OprFact::metadataConst(0));
1327   __ cmove(lir_cond_notEqual, LIR_OprFact::intConst(0), LIR_OprFact::intConst(1), result, T_BOOLEAN);
1328 }
1329 
1330 
1331 // Example: Thread.currentThread()
1332 void LIRGenerator::do_currentThread(Intrinsic* x) {
1333   assert(x-&gt;number_of_arguments() == 0, &quot;wrong type&quot;);
1334   LIR_Opr reg = rlock_result(x);
1335   __ move_wide(new LIR_Address(getThreadPointer(), in_bytes(JavaThread::threadObj_offset()), T_OBJECT), reg);
1336 }
1337 
1338 
1339 void LIRGenerator::do_RegisterFinalizer(Intrinsic* x) {
1340   assert(x-&gt;number_of_arguments() == 1, &quot;wrong type&quot;);
1341   LIRItem receiver(x-&gt;argument_at(0), this);
1342 
1343   receiver.load_item();
1344   BasicTypeList signature;
1345   signature.append(T_OBJECT); // receiver
</pre>
<hr />
<pre>
1547 
1548 #ifndef PRODUCT
1549   if (PrintNotLoaded &amp;&amp; needs_patching) {
1550     tty-&gt;print_cr(&quot;   ###class not loaded at store_%s bci %d&quot;,
1551                   x-&gt;is_static() ?  &quot;static&quot; : &quot;field&quot;, x-&gt;printable_bci());
1552   }
1553 #endif
1554 
1555   if (x-&gt;needs_null_check() &amp;&amp;
1556       (needs_patching ||
1557        MacroAssembler::needs_explicit_null_check(x-&gt;offset()))) {
1558     if (needs_patching &amp;&amp; x-&gt;field()-&gt;is_flattenable()) {
1559       // We are storing a field of type &quot;QT;&quot; into holder class H, but H is not yet
1560       // loaded. (If H had been loaded, then T must also have already been loaded
1561       // due to the &quot;Q&quot; signature, and needs_patching would be false).
1562       assert(!x-&gt;field()-&gt;holder()-&gt;is_loaded(), &quot;must be&quot;);
1563       // We don&#39;t know the offset of this field. Let&#39;s deopt and recompile.
1564       CodeStub* stub = new DeoptimizeStub(new CodeEmitInfo(info),
1565                                           Deoptimization::Reason_unloaded,
1566                                           Deoptimization::Action_make_not_entrant);
<span class="line-modified">1567       __ branch(lir_cond_always, T_ILLEGAL, stub);</span>
1568     } else {
1569       // Emit an explicit null check because the offset is too large.
1570       // If the class is not loaded and the object is NULL, we need to deoptimize to throw a
1571       // NoClassDefFoundError in the interpreter instead of an implicit NPE from compiled code.
1572       __ null_check(object.result(), new CodeEmitInfo(info), /* deoptimize */ needs_patching);
1573     }
1574   }
1575 
1576   DecoratorSet decorators = IN_HEAP;
1577   if (is_volatile) {
1578     decorators |= MO_SEQ_CST;
1579   }
1580   if (needs_patching) {
1581     decorators |= C1_NEEDS_PATCHING;
1582   }
1583 
1584   access_store_at(decorators, field_type, object, LIR_OprFact::intConst(x-&gt;offset()),
1585                   value.result(), info != NULL ? new CodeEmitInfo(info) : NULL, info);
1586 }
1587 
</pre>
<hr />
<pre>
1666     } else {
1667       access_load_at(decorators, field_type,
1668                      obj_item, LIR_OprFact::intConst(obj_offset), temp,
1669                      NULL, NULL);
1670       access_store_at(decorators, field_type,
1671                       elm_item, LIR_OprFact::intConst(elm_offset), temp,
1672                       NULL, NULL);
1673     }
1674   }
1675 }
1676 
1677 void LIRGenerator::check_flattened_array(LIR_Opr array, LIR_Opr value, CodeStub* slow_path) {
1678   LIR_Opr tmp = new_register(T_METADATA);
1679   __ check_flattened_array(array, value, tmp, slow_path);
1680 }
1681 
1682 void LIRGenerator::check_null_free_array(LIRItem&amp; array, LIRItem&amp; value, CodeEmitInfo* info) {
1683   LabelObj* L_end = new LabelObj();
1684   LIR_Opr tmp = new_register(T_METADATA);
1685   __ check_null_free_array(array.result(), tmp);
<span class="line-modified">1686   __ branch(lir_cond_equal, T_ILLEGAL, L_end-&gt;label());</span>
1687   __ null_check(value.result(), info);
1688   __ branch_destination(L_end-&gt;label());
1689 }
1690 
1691 bool LIRGenerator::needs_flattened_array_store_check(StoreIndexed* x) {
1692   if (x-&gt;elt_type() == T_OBJECT &amp;&amp; x-&gt;array()-&gt;maybe_flattened_array()) {
1693     ciType* type = x-&gt;value()-&gt;declared_type();
1694     if (type != NULL &amp;&amp; type-&gt;is_klass()) {
1695       ciKlass* klass = type-&gt;as_klass();
1696       if (!klass-&gt;can_be_value_klass() || (klass-&gt;is_valuetype() &amp;&amp; !klass-&gt;as_value_klass()-&gt;flatten_array())) {
1697         // This is known to be a non-flattenable object. If the array is flattened,
1698         // it will be caught by the code generated by array_store_check().
1699         return false;
1700       }
1701     }
1702     // We&#39;re not 100% sure, so let&#39;s do the flattened_array_store_check.
1703     return true;
1704   }
1705   return false;
1706 }
</pre>
<hr />
<pre>
1737       || is_loaded_flattened_array || needs_flattened_array_store_check(x) || needs_null_free_array_store_check(x)) {
1738     value.load_item();
1739   } else {
1740     value.load_for_store(x-&gt;elt_type());
1741   }
1742 
1743   set_no_result(x);
1744 
1745   // the CodeEmitInfo must be duplicated for each different
1746   // LIR-instruction because spilling can occur anywhere between two
1747   // instructions and so the debug information must be different
1748   CodeEmitInfo* range_check_info = state_for(x);
1749   CodeEmitInfo* null_check_info = NULL;
1750   if (x-&gt;needs_null_check()) {
1751     null_check_info = new CodeEmitInfo(range_check_info);
1752   }
1753 
1754   if (GenerateRangeChecks &amp;&amp; needs_range_check) {
1755     if (use_length) {
1756       __ cmp(lir_cond_belowEqual, length.result(), index.result());
<span class="line-modified">1757       __ branch(lir_cond_belowEqual, T_INT, new RangeCheckStub(range_check_info, index.result(), array.result()));</span>
1758     } else {
1759       array_range_check(array.result(), index.result(), null_check_info, range_check_info);
1760       // range_check also does the null check
1761       null_check_info = NULL;
1762     }
1763   }
1764 
1765   if (x-&gt;should_profile()) {
1766     ciMethodData* md = NULL;
1767     ciArrayLoadStoreData* load_store = NULL;
1768     profile_array_type(x, md, load_store);
1769     if (is_loaded_flattened_array) {
1770       int flag = ArrayLoadStoreData::flat_array_byte_constant() | ArrayLoadStoreData::null_free_array_byte_constant();
1771       assert(md != NULL, &quot;should have been initialized&quot;);
1772       profile_array_load_store_flags(md, load_store, flag);
1773     } else if (x-&gt;array()-&gt;maybe_null_free_array()) {
1774       profile_null_free_array(array, md, load_store);
1775     }
1776     profile_element_type(x-&gt;value(), md, load_store);
1777   }
</pre>
<hr />
<pre>
1931       // (1) holder is unloaded -- problem: we needed the field offset back in GraphBuilder::access_field()
1932       //                           FIXME: consider getting field offset in patching code (but only if the field
1933       //                           type was loaded at compilation time).
1934       deopt = true;
1935     } else if (field_type_unloaded) {
1936       // (2) field type is unloaded -- problem: we don&#39;t know whether it&#39;s flattened or not. Let&#39;s deopt
1937       deopt = true;
1938     } else if (!field-&gt;is_flattened()) {
1939       // (3) field is not flattened -- need default value in cases of uninitialized field
1940       need_default = true;
1941     }
1942   }
1943 
1944   if (deopt) {
1945     assert(!need_default, &quot;deopt and need_default cannot both be true&quot;);
1946     assert(x-&gt;needs_patching(), &quot;must be&quot;);
1947     assert(info != NULL, &quot;must be&quot;);
1948     CodeStub* stub = new DeoptimizeStub(new CodeEmitInfo(info),
1949                                         Deoptimization::Reason_unloaded,
1950                                         Deoptimization::Action_make_not_entrant);
<span class="line-modified">1951     __ branch(lir_cond_always, T_ILLEGAL, stub);</span>
1952   } else if (need_default) {
1953     assert(!field_type_unloaded, &quot;must be&quot;);
1954     assert(field-&gt;type()-&gt;is_valuetype(), &quot;must be&quot;);
1955     ciValueKlass* value_klass = field-&gt;type()-&gt;as_value_klass();
1956     assert(value_klass-&gt;is_loaded(), &quot;must be&quot;);
1957 
1958     if (field-&gt;is_static() &amp;&amp; holder-&gt;is_loaded()) {
1959       ciInstance* mirror = field-&gt;holder()-&gt;java_mirror();
1960       ciObject* val = mirror-&gt;field_value(field).as_object();
1961       if (val-&gt;is_null_object()) {
1962         // This is a non-nullable static field, but it&#39;s not initialized.
1963         // We need to do a null check, and replace it with the default value.
1964       } else {
1965         // No need to perform null check on this static field
1966         need_default = false;
1967       }
1968     }
1969 
1970     if (need_default) {
1971       default_value = new Constant(new InstanceConstant(value_klass-&gt;default_value_instance()));
</pre>
<hr />
<pre>
2024     // NoClassDefFoundError in the interpreter instead of an implicit NPE from compiled code.
2025     __ null_check(obj, new CodeEmitInfo(info), /* deoptimize */ needs_patching);
2026   }
2027 
2028   DecoratorSet decorators = IN_HEAP;
2029   if (is_volatile) {
2030     decorators |= MO_SEQ_CST;
2031   }
2032   if (needs_patching) {
2033     decorators |= C1_NEEDS_PATCHING;
2034   }
2035 
2036   LIR_Opr result = rlock_result(x, field_type);
2037   access_load_at(decorators, field_type,
2038                  object, LIR_OprFact::intConst(x-&gt;offset()), result,
2039                  info ? new CodeEmitInfo(info) : NULL, info);
2040 
2041   if (default_value != NULL) {
2042     LabelObj* L_end = new LabelObj();
2043     __ cmp(lir_cond_notEqual, result, LIR_OprFact::oopConst(NULL));
<span class="line-modified">2044     __ branch(lir_cond_notEqual, T_OBJECT, L_end-&gt;label());</span>
2045     set_in_conditional_code(true);
2046     __ move(load_constant(default_value), result);
2047     __ branch_destination(L_end-&gt;label());
2048     set_in_conditional_code(false);
2049   }
2050 }
2051 
2052 
2053 //------------------------java.nio.Buffer.checkIndex------------------------
2054 
2055 // int java.nio.Buffer.checkIndex(int)
2056 void LIRGenerator::do_NIOCheckIndex(Intrinsic* x) {
2057   // NOTE: by the time we are in checkIndex() we are guaranteed that
2058   // the buffer is non-null (because checkIndex is package-private and
2059   // only called from within other methods in the buffer).
2060   assert(x-&gt;number_of_arguments() == 2, &quot;wrong type&quot;);
2061   LIRItem buf  (x-&gt;argument_at(0), this);
2062   LIRItem index(x-&gt;argument_at(1), this);
2063   buf.load_item();
2064   index.load_item();
2065 
2066   LIR_Opr result = rlock_result(x);
2067   if (GenerateRangeChecks) {
2068     CodeEmitInfo* info = state_for(x);
2069     CodeStub* stub = new RangeCheckStub(info, index.result());
2070     LIR_Opr buf_obj = access_resolve(IS_NOT_NULL | ACCESS_READ, buf.result());
2071     if (index.result()-&gt;is_constant()) {
2072       cmp_mem_int(lir_cond_belowEqual, buf_obj, java_nio_Buffer::limit_offset(), index.result()-&gt;as_jint(), info);
<span class="line-modified">2073       __ branch(lir_cond_belowEqual, T_INT, stub);</span>
2074     } else {
2075       cmp_reg_mem(lir_cond_aboveEqual, index.result(), buf_obj,
2076                   java_nio_Buffer::limit_offset(), T_INT, info);
<span class="line-modified">2077       __ branch(lir_cond_aboveEqual, T_INT, stub);</span>
2078     }
2079     __ move(index.result(), result);
2080   } else {
2081     // Just load the index into the result register
2082     __ move(index.result(), result);
2083   }
2084 }
2085 
2086 
2087 //------------------------array access--------------------------------------
2088 
2089 
2090 void LIRGenerator::do_ArrayLength(ArrayLength* x) {
2091   LIRItem array(x-&gt;array(), this);
2092   array.load_item();
2093   LIR_Opr reg = rlock_result(x);
2094 
2095   CodeEmitInfo* info = NULL;
2096   if (x-&gt;needs_null_check()) {
2097     NullCheck* nc = x-&gt;explicit_null_check();
</pre>
<hr />
<pre>
2131   }
2132 
2133   CodeEmitInfo* range_check_info = state_for(x);
2134   CodeEmitInfo* null_check_info = NULL;
2135   if (x-&gt;needs_null_check()) {
2136     NullCheck* nc = x-&gt;explicit_null_check();
2137     if (nc != NULL) {
2138       null_check_info = state_for(nc);
2139     } else {
2140       null_check_info = range_check_info;
2141     }
2142     if (StressLoopInvariantCodeMotion &amp;&amp; null_check_info-&gt;deoptimize_on_exception()) {
2143       LIR_Opr obj = new_register(T_OBJECT);
2144       __ move(LIR_OprFact::oopConst(NULL), obj);
2145       __ null_check(obj, new CodeEmitInfo(null_check_info));
2146     }
2147   }
2148 
2149   if (GenerateRangeChecks &amp;&amp; needs_range_check) {
2150     if (StressLoopInvariantCodeMotion &amp;&amp; range_check_info-&gt;deoptimize_on_exception()) {
<span class="line-modified">2151       __ branch(lir_cond_always, T_ILLEGAL, new RangeCheckStub(range_check_info, index.result(), array.result()));</span>
2152     } else if (use_length) {
2153       // TODO: use a (modified) version of array_range_check that does not require a
2154       //       constant length to be loaded to a register
2155       __ cmp(lir_cond_belowEqual, length.result(), index.result());
<span class="line-modified">2156       __ branch(lir_cond_belowEqual, T_INT, new RangeCheckStub(range_check_info, index.result(), array.result()));</span>
2157     } else {
2158       array_range_check(array.result(), index.result(), null_check_info, range_check_info);
2159       // The range check performs the null check, so clear it out for the load
2160       null_check_info = NULL;
2161     }
2162   }
2163 
2164   ciMethodData* md = NULL;
2165   ciArrayLoadStoreData* load_store = NULL;
2166   if (x-&gt;should_profile()) {
2167     profile_array_type(x, md, load_store);
2168   }
2169 
2170   Value element;
2171   if (x-&gt;vt() != NULL) {
2172     assert(x-&gt;array()-&gt;is_loaded_flattened_array(), &quot;must be&quot;);
2173     // Find the destination address (of the NewValueTypeInstance).
2174     LIR_Opr obj = x-&gt;vt()-&gt;operand();
2175     LIRItem obj_item(x-&gt;vt(), this);
2176 
</pre>
<hr />
<pre>
2206       __ branch_destination(slow_path-&gt;continuation());
2207       set_in_conditional_code(false);
2208     }
2209 
2210     element = x;
2211   }
2212 
2213   if (x-&gt;should_profile()) {
2214     profile_element_type(element, md, load_store);
2215   }
2216 }
2217 
2218 void LIRGenerator::do_WithField(WithField* x) {
2219   // This happens only when a class X uses the withfield bytecode to refer to
2220   // an inline class V, where V has not yet been loaded. This is not a common
2221   // case. Let&#39;s just deoptimize.
2222   CodeEmitInfo* info = state_for(x, x-&gt;state_before());
2223   CodeStub* stub = new DeoptimizeStub(new CodeEmitInfo(info),
2224                                       Deoptimization::Reason_unloaded,
2225                                       Deoptimization::Action_make_not_entrant);
<span class="line-modified">2226   __ branch(lir_cond_always, T_ILLEGAL, stub);</span>
2227   LIR_Opr reg = rlock_result(x, T_OBJECT);
2228   __ move(LIR_OprFact::oopConst(NULL), reg);
2229 }
2230 
2231 void LIRGenerator::do_DefaultValue(DefaultValue* x) {
2232   // Same as withfield above. Let&#39;s deoptimize.
2233   CodeEmitInfo* info = state_for(x, x-&gt;state_before());
2234   CodeStub* stub = new DeoptimizeStub(new CodeEmitInfo(info),
2235                                       Deoptimization::Reason_unloaded,
2236                                       Deoptimization::Action_make_not_entrant);
<span class="line-modified">2237   __ branch(lir_cond_always, T_ILLEGAL, stub);</span>
2238   LIR_Opr reg = rlock_result(x, T_OBJECT);
2239   __ move(LIR_OprFact::oopConst(NULL), reg);
2240 }
2241 
2242 void LIRGenerator::do_NullCheck(NullCheck* x) {
2243   if (x-&gt;can_trap()) {
2244     LIRItem value(x-&gt;obj(), this);
2245     value.load_item();
2246     CodeEmitInfo* info = state_for(x);
2247     __ null_check(value.result(), info);
2248   }
2249 }
2250 
2251 
2252 void LIRGenerator::do_TypeCast(TypeCast* x) {
2253   LIRItem value(x-&gt;obj(), this);
2254   value.load_item();
2255   // the result is the same as from the node we are casting
2256   set_result(x, value.result());
2257 }
</pre>
<hr />
<pre>
2581 
2582   LIR_Opr result;
2583   if (x-&gt;is_add()) {
2584     result = access_atomic_add_at(decorators, type, src, off, value);
2585   } else {
2586     result = access_atomic_xchg_at(decorators, type, src, off, value);
2587   }
2588   set_result(x, result);
2589 }
2590 
2591 void LIRGenerator::do_SwitchRanges(SwitchRangeArray* x, LIR_Opr value, BlockBegin* default_sux) {
2592   int lng = x-&gt;length();
2593 
2594   for (int i = 0; i &lt; lng; i++) {
2595     C1SwitchRange* one_range = x-&gt;at(i);
2596     int low_key = one_range-&gt;low_key();
2597     int high_key = one_range-&gt;high_key();
2598     BlockBegin* dest = one_range-&gt;sux();
2599     if (low_key == high_key) {
2600       __ cmp(lir_cond_equal, value, low_key);
<span class="line-modified">2601       __ branch(lir_cond_equal, T_INT, dest);</span>
2602     } else if (high_key - low_key == 1) {
2603       __ cmp(lir_cond_equal, value, low_key);
<span class="line-modified">2604       __ branch(lir_cond_equal, T_INT, dest);</span>
2605       __ cmp(lir_cond_equal, value, high_key);
<span class="line-modified">2606       __ branch(lir_cond_equal, T_INT, dest);</span>
2607     } else {
2608       LabelObj* L = new LabelObj();
2609       __ cmp(lir_cond_less, value, low_key);
<span class="line-modified">2610       __ branch(lir_cond_less, T_INT, L-&gt;label());</span>
2611       __ cmp(lir_cond_lessEqual, value, high_key);
<span class="line-modified">2612       __ branch(lir_cond_lessEqual, T_INT, dest);</span>
2613       __ branch_destination(L-&gt;label());
2614     }
2615   }
2616   __ jump(default_sux);
2617 }
2618 
2619 
2620 SwitchRangeArray* LIRGenerator::create_lookup_ranges(TableSwitch* x) {
2621   SwitchRangeList* res = new SwitchRangeList();
2622   int len = x-&gt;length();
2623   if (len &gt; 0) {
2624     BlockBegin* sux = x-&gt;sux_at(0);
2625     int key = x-&gt;lo_key();
2626     BlockBegin* default_sux = x-&gt;default_sux();
2627     C1SwitchRange* range = new C1SwitchRange(key, sux);
2628     for (int i = 0; i &lt; len; i++, key++) {
2629       BlockBegin* new_sux = x-&gt;sux_at(i);
2630       if (sux == new_sux) {
2631         // still in same range
2632         range-&gt;set_high_key(key);
</pre>
<hr />
<pre>
2712       __ cmp(lir_cond_equal, value, i + lo_key);
2713       __ move(data_offset_reg, tmp_reg);
2714       __ cmove(lir_cond_equal,
2715                LIR_OprFact::intptrConst(count_offset),
2716                tmp_reg,
2717                data_offset_reg, T_INT);
2718     }
2719 
2720     LIR_Opr data_reg = new_pointer_register();
2721     LIR_Address* data_addr = new LIR_Address(md_reg, data_offset_reg, data_reg-&gt;type());
2722     __ move(data_addr, data_reg);
2723     __ add(data_reg, LIR_OprFact::intptrConst(1), data_reg);
2724     __ move(data_reg, data_addr);
2725   }
2726 
2727   if (UseTableRanges) {
2728     do_SwitchRanges(create_lookup_ranges(x), value, x-&gt;default_sux());
2729   } else {
2730     for (int i = 0; i &lt; len; i++) {
2731       __ cmp(lir_cond_equal, value, i + lo_key);
<span class="line-modified">2732       __ branch(lir_cond_equal, T_INT, x-&gt;sux_at(i));</span>
2733     }
2734     __ jump(x-&gt;default_sux());
2735   }
2736 }
2737 
2738 
2739 void LIRGenerator::do_LookupSwitch(LookupSwitch* x) {
2740   LIRItem tag(x-&gt;tag(), this);
2741   tag.load_item();
2742   set_no_result(x);
2743 
2744   if (x-&gt;is_safepoint()) {
2745     __ safepoint(safepoint_poll_register(), state_for(x, x-&gt;state_before()));
2746   }
2747 
2748   // move values into phi locations
2749   move_to_phi(x-&gt;state());
2750 
2751   LIR_Opr value = tag.result();
2752   int len = x-&gt;length();
</pre>
<hr />
<pre>
2771       __ move(data_offset_reg, tmp_reg);
2772       __ cmove(lir_cond_equal,
2773                LIR_OprFact::intptrConst(count_offset),
2774                tmp_reg,
2775                data_offset_reg, T_INT);
2776     }
2777 
2778     LIR_Opr data_reg = new_pointer_register();
2779     LIR_Address* data_addr = new LIR_Address(md_reg, data_offset_reg, data_reg-&gt;type());
2780     __ move(data_addr, data_reg);
2781     __ add(data_reg, LIR_OprFact::intptrConst(1), data_reg);
2782     __ move(data_reg, data_addr);
2783   }
2784 
2785   if (UseTableRanges) {
2786     do_SwitchRanges(create_lookup_ranges(x), value, x-&gt;default_sux());
2787   } else {
2788     int len = x-&gt;length();
2789     for (int i = 0; i &lt; len; i++) {
2790       __ cmp(lir_cond_equal, value, x-&gt;key_at(i));
<span class="line-modified">2791       __ branch(lir_cond_equal, T_INT, x-&gt;sux_at(i));</span>
2792     }
2793     __ jump(x-&gt;default_sux());
2794   }
2795 }
2796 
2797 
2798 void LIRGenerator::do_Goto(Goto* x) {
2799   set_no_result(x);
2800 
2801   if (block()-&gt;next()-&gt;as_OsrEntry()) {
2802     // need to free up storage used for OSR entry point
2803     LIR_Opr osrBuffer = block()-&gt;next()-&gt;operand();
2804     BasicTypeList signature;
2805     signature.append(NOT_LP64(T_INT) LP64_ONLY(T_LONG)); // pass a pointer to osrBuffer
2806     CallingConvention* cc = frame_map()-&gt;c_calling_convention(&amp;signature);
2807     __ move(osrBuffer, cc-&gt;args()-&gt;at(0));
2808     __ call_runtime_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::OSR_migration_end),
2809                          getThreadTemp(), LIR_OprFact::illegalOpr, cc-&gt;args());
2810   }
2811 
</pre>
<hr />
<pre>
2989 void LIRGenerator::profile_array_load_store_flags(ciMethodData* md, ciArrayLoadStoreData* load_store, int flag, LIR_Opr mdp) {
2990   assert(md != NULL &amp;&amp; load_store != NULL, &quot;should have been initialized&quot;);
2991   if (mdp == NULL) {
2992     mdp = new_register(T_METADATA);
2993     __ metadata2reg(md-&gt;constant_encoding(), mdp);
2994   }
2995   LIR_Address* addr = new LIR_Address(mdp, md-&gt;byte_offset_of_slot(load_store, DataLayout::flags_offset()), T_BYTE);
2996   LIR_Opr id = new_register(T_INT);
2997   __ move(addr, id);
2998   __ logical_or(id, LIR_OprFact::intConst(flag), id);
2999   __ store(id, addr);
3000 }
3001 
3002 void LIRGenerator::profile_null_free_array(LIRItem array, ciMethodData* md, ciArrayLoadStoreData* load_store) {
3003   LabelObj* L_end = new LabelObj();
3004   LIR_Opr tmp = new_register(T_METADATA);
3005   LIR_Opr mdp = new_register(T_METADATA);
3006   assert(md != NULL, &quot;should have been initialized&quot;);
3007   __ metadata2reg(md-&gt;constant_encoding(), mdp);
3008   __ check_null_free_array(array.result(), tmp);
<span class="line-modified">3009   __ branch(lir_cond_equal, T_ILLEGAL, L_end-&gt;label());</span>
3010 
3011   profile_array_load_store_flags(md, load_store, ArrayLoadStoreData::null_free_array_byte_constant(), mdp);
3012 
3013   __ branch_destination(L_end-&gt;label());
3014 }
3015 
3016 void LIRGenerator::profile_array_type(AccessIndexed* x, ciMethodData*&amp; md, ciArrayLoadStoreData*&amp; load_store) {
3017   int bci = x-&gt;profiled_bci();
3018   md = x-&gt;profiled_method()-&gt;method_data();
3019   assert(md != NULL, &quot;Sanity&quot;);
3020   ciProfileData* data = md-&gt;bci_to_data(bci);
3021   assert(data != NULL &amp;&amp; data-&gt;is_ArrayLoadStoreData(), &quot;incorrect profiling entry&quot;);
3022   load_store = (ciArrayLoadStoreData*)data;
3023   LIR_Opr mdp = LIR_OprFact::illegalOpr;
3024   profile_type(md, md-&gt;byte_offset_of_slot(load_store, ArrayLoadStoreData::array_offset()), 0,
3025                load_store-&gt;array()-&gt;type(), x-&gt;array(), mdp, true, NULL, NULL);
3026 }
3027 
3028 void LIRGenerator::profile_element_type(Value element, ciMethodData* md, ciArrayLoadStoreData* load_store) {
3029   assert(md != NULL &amp;&amp; load_store != NULL, &quot;should have been initialized&quot;);
</pre>
<hr />
<pre>
3106       // receiver is guaranteed non-NULL so don&#39;t need CodeEmitInfo
3107       __ lock_object(syncTempOpr(), obj, lock, new_register(T_OBJECT), slow_path, NULL);
3108     }
3109   }
3110   if (compilation()-&gt;age_code()) {
3111     CodeEmitInfo* info = new CodeEmitInfo(scope()-&gt;start()-&gt;state()-&gt;copy(ValueStack::StateBefore, 0), NULL, false);
3112     decrement_age(info);
3113   }
3114   // increment invocation counters if needed
3115   if (!method()-&gt;is_accessor()) { // Accessors do not have MDOs, so no counting.
3116     profile_parameters(x);
3117     CodeEmitInfo* info = new CodeEmitInfo(scope()-&gt;start()-&gt;state()-&gt;copy(ValueStack::StateBefore, SynchronizationEntryBCI), NULL, false);
3118     increment_invocation_counter(info);
3119   }
3120   if (method()-&gt;has_scalarized_args()) {
3121     // Check if deoptimization was triggered (i.e. orig_pc was set) while buffering scalarized value type arguments
3122     // in the entry point (see comments in frame::deoptimize). If so, deoptimize only now that we have the right state.
3123     CodeEmitInfo* info = new CodeEmitInfo(scope()-&gt;start()-&gt;state()-&gt;copy(ValueStack::StateBefore, 0), NULL, false);
3124     CodeStub* deopt_stub = new DeoptimizeStub(info, Deoptimization::Reason_none, Deoptimization::Action_none);
3125     __ append(new LIR_Op0(lir_check_orig_pc));
<span class="line-modified">3126     __ branch(lir_cond_notEqual, T_ADDRESS, deopt_stub);</span>
3127   }
3128 
3129   // all blocks with a successor must end with an unconditional jump
3130   // to the successor even if they are consecutive
3131   __ jump(x-&gt;default_sux());
3132 }
3133 
3134 
3135 void LIRGenerator::do_OsrEntry(OsrEntry* x) {
3136   // construct our frame and model the production of incoming pointer
3137   // to the OSR buffer.
3138   __ osr_entry(LIR_Assembler::osrBufferPointer());
3139   LIR_Opr result = rlock_result(x);
3140   __ move(LIR_Assembler::osrBufferPointer(), result);
3141 }
3142 
3143 void LIRGenerator::invoke_load_one_argument(LIRItem* param, LIR_Opr loc) {
3144   if (loc-&gt;is_register()) {
3145     param-&gt;load_item_force(loc);
3146   } else {
</pre>
<hr />
<pre>
3413     left_klass_op = new_register(t_klass);
3414     right_klass_op = new_register(t_klass);
3415   }
3416 
3417   CodeStub* slow_path = new SubstitutabilityCheckStub(left.result(), right.result(), info);
3418   __ substitutability_check(result, left.result(), right.result(), equal_result, not_equal_result,
3419                             tmp1, tmp2,
3420                             left_klass, right_klass, left_klass_op, right_klass_op, info, slow_path);
3421 }
3422 
3423 #ifdef JFR_HAVE_INTRINSICS
3424 void LIRGenerator::do_ClassIDIntrinsic(Intrinsic* x) {
3425   CodeEmitInfo* info = state_for(x);
3426   CodeEmitInfo* info2 = new CodeEmitInfo(info); // Clone for the second null check
3427 
3428   assert(info != NULL, &quot;must have info&quot;);
3429   LIRItem arg(x-&gt;argument_at(0), this);
3430 
3431   arg.load_item();
3432   LIR_Opr klass = new_register(T_METADATA);
<span class="line-modified">3433   __ move(new LIR_Address(arg.result(), java_lang_Class::klass_offset_in_bytes(), T_ADDRESS), klass, info);</span>
3434   LIR_Opr id = new_register(T_LONG);
3435   ByteSize offset = KLASS_TRACE_ID_OFFSET;
3436   LIR_Address* trace_id_addr = new LIR_Address(klass, in_bytes(offset), T_LONG);
3437 
3438   __ move(trace_id_addr, id);
3439   __ logical_or(id, LIR_OprFact::longConst(0x01l), id);
3440   __ store(id, trace_id_addr);
3441 
3442 #ifdef TRACE_ID_META_BITS
3443   __ logical_and(id, LIR_OprFact::longConst(~TRACE_ID_META_BITS), id);
3444 #endif
3445 #ifdef TRACE_ID_SHIFT
3446   __ unsigned_shift_right(id, TRACE_ID_SHIFT, id);
3447 #endif
3448 
3449   __ move(id, rlock_result(x));
3450 }
3451 
3452 void LIRGenerator::do_getEventWriter(Intrinsic* x) {
3453   LabelObj* L_end = new LabelObj();
3454 


3455   LIR_Address* jobj_addr = new LIR_Address(getThreadPointer(),
3456                                            in_bytes(THREAD_LOCAL_WRITER_OFFSET_JFR),
<span class="line-modified">3457                                            T_OBJECT);</span>
3458   LIR_Opr result = rlock_result(x);
<span class="line-modified">3459   __ move_wide(jobj_addr, result);</span>
<span class="line-modified">3460   __ cmp(lir_cond_equal, result, LIR_OprFact::oopConst(NULL));</span>
<span class="line-modified">3461   __ branch(lir_cond_equal, T_OBJECT, L_end-&gt;label());</span>


3462 
<span class="line-removed">3463   LIR_Opr jobj = new_register(T_OBJECT);</span>
<span class="line-removed">3464   __ move(result, jobj);</span>
3465   access_load(IN_NATIVE, T_OBJECT, LIR_OprFact::address(new LIR_Address(jobj, T_OBJECT)), result);
3466 
3467   __ branch_destination(L_end-&gt;label());
3468 }
3469 
3470 #endif
3471 
3472 
3473 void LIRGenerator::do_RuntimeCall(address routine, Intrinsic* x) {
3474   assert(x-&gt;number_of_arguments() == 0, &quot;wrong type&quot;);
3475   // Enforce computation of _reserved_argument_area_size which is required on some platforms.
3476   BasicTypeList signature;
3477   CallingConvention* cc = frame_map()-&gt;c_calling_convention(&amp;signature);
3478   LIR_Opr reg = result_register_for(x-&gt;type());
3479   __ call_runtime_leaf(routine, getThreadTemp(),
3480                        reg, new LIR_OprList());
3481   LIR_Opr result = rlock_result(x);
3482   __ move(reg, result);
3483 }
3484 
</pre>
<hr />
<pre>
3802   }
3803   increment_event_counter_impl(info, info-&gt;scope()-&gt;method(), step, right_n_bits(freq_log), bci, backedge, true);
3804 }
3805 
3806 void LIRGenerator::decrement_age(CodeEmitInfo* info) {
3807   ciMethod* method = info-&gt;scope()-&gt;method();
3808   MethodCounters* mc_adr = method-&gt;ensure_method_counters();
3809   if (mc_adr != NULL) {
3810     LIR_Opr mc = new_pointer_register();
3811     __ move(LIR_OprFact::intptrConst(mc_adr), mc);
3812     int offset = in_bytes(MethodCounters::nmethod_age_offset());
3813     LIR_Address* counter = new LIR_Address(mc, offset, T_INT);
3814     LIR_Opr result = new_register(T_INT);
3815     __ load(counter, result);
3816     __ sub(result, LIR_OprFact::intConst(1), result);
3817     __ store(result, counter);
3818     // DeoptimizeStub will reexecute from the current state in code info.
3819     CodeStub* deopt = new DeoptimizeStub(info, Deoptimization::Reason_tenured,
3820                                          Deoptimization::Action_make_not_entrant);
3821     __ cmp(lir_cond_lessEqual, result, LIR_OprFact::intConst(0));
<span class="line-modified">3822     __ branch(lir_cond_lessEqual, T_INT, deopt);</span>
3823   }
3824 }
3825 
3826 
3827 void LIRGenerator::increment_event_counter_impl(CodeEmitInfo* info,
3828                                                 ciMethod *method, LIR_Opr step, int frequency,
3829                                                 int bci, bool backedge, bool notify) {
3830   assert(frequency == 0 || is_power_of_2(frequency + 1), &quot;Frequency must be x^2 - 1 or 0&quot;);
3831   int level = _compilation-&gt;env()-&gt;comp_level();
3832   assert(level &gt; CompLevel_simple, &quot;Shouldn&#39;t be here&quot;);
3833 
3834   int offset = -1;
3835   LIR_Opr counter_holder = NULL;
3836   if (level == CompLevel_limited_profile) {
3837     MethodCounters* counters_adr = method-&gt;ensure_method_counters();
3838     if (counters_adr == NULL) {
3839       bailout(&quot;method counters allocation failed&quot;);
3840       return;
3841     }
3842     counter_holder = new_pointer_register();
</pre>
<hr />
<pre>
3849                                  MethodData::invocation_counter_offset());
3850     ciMethodData* md = method-&gt;method_data_or_null();
3851     assert(md != NULL, &quot;Sanity&quot;);
3852     __ metadata2reg(md-&gt;constant_encoding(), counter_holder);
3853   } else {
3854     ShouldNotReachHere();
3855   }
3856   LIR_Address* counter = new LIR_Address(counter_holder, offset, T_INT);
3857   LIR_Opr result = new_register(T_INT);
3858   __ load(counter, result);
3859   __ add(result, step, result);
3860   __ store(result, counter);
3861   if (notify &amp;&amp; (!backedge || UseOnStackReplacement)) {
3862     LIR_Opr meth = LIR_OprFact::metadataConst(method-&gt;constant_encoding());
3863     // The bci for info can point to cmp for if&#39;s we want the if bci
3864     CodeStub* overflow = new CounterOverflowStub(info, bci, meth);
3865     int freq = frequency &lt;&lt; InvocationCounter::count_shift;
3866     if (freq == 0) {
3867       if (!step-&gt;is_constant()) {
3868         __ cmp(lir_cond_notEqual, step, LIR_OprFact::intConst(0));
<span class="line-modified">3869         __ branch(lir_cond_notEqual, T_ILLEGAL, overflow);</span>
3870       } else {
<span class="line-modified">3871         __ branch(lir_cond_always, T_ILLEGAL, overflow);</span>
3872       }
3873     } else {
3874       LIR_Opr mask = load_immediate(freq, T_INT);
3875       if (!step-&gt;is_constant()) {
3876         // If step is 0, make sure the overflow check below always fails
3877         __ cmp(lir_cond_notEqual, step, LIR_OprFact::intConst(0));
3878         __ cmove(lir_cond_notEqual, result, LIR_OprFact::intConst(InvocationCounter::count_increment), result, T_INT);
3879       }
3880       __ logical_and(result, mask, result);
3881       __ cmp(lir_cond_equal, result, LIR_OprFact::intConst(0));
<span class="line-modified">3882       __ branch(lir_cond_equal, T_INT, overflow);</span>
3883     }
3884     __ branch_destination(overflow-&gt;continuation());
3885   }
3886 }
3887 
3888 void LIRGenerator::do_RuntimeCall(RuntimeCall* x) {
3889   LIR_OprList* args = new LIR_OprList(x-&gt;number_of_arguments());
3890   BasicTypeList* signature = new BasicTypeList(x-&gt;number_of_arguments());
3891 
3892   if (x-&gt;pass_thread()) {
3893     signature-&gt;append(LP64_ONLY(T_LONG) NOT_LP64(T_INT));    // thread
3894     args-&gt;append(getThreadPointer());
3895   }
3896 
3897   for (int i = 0; i &lt; x-&gt;number_of_arguments(); i++) {
3898     Value a = x-&gt;argument_at(i);
3899     LIRItem* item = new LIRItem(a, this);
3900     item-&gt;load_item();
3901     args-&gt;append(item-&gt;result());
3902     signature-&gt;append(as_BasicType(a-&gt;type()));
</pre>
<hr />
<pre>
3976     ValueTag tag = x-&gt;x()-&gt;type()-&gt;tag();
3977     If::Condition cond = x-&gt;cond();
3978     LIRItem xitem(x-&gt;x(), this);
3979     LIRItem yitem(x-&gt;y(), this);
3980     LIRItem* xin = &amp;xitem;
3981     LIRItem* yin = &amp;yitem;
3982 
3983     assert(tag == intTag, &quot;Only integer deoptimizations are valid!&quot;);
3984 
3985     xin-&gt;load_item();
3986     yin-&gt;dont_load_item();
3987     set_no_result(x);
3988 
3989     LIR_Opr left = xin-&gt;result();
3990     LIR_Opr right = yin-&gt;result();
3991 
3992     CodeEmitInfo *info = state_for(x, x-&gt;state());
3993     CodeStub* stub = new PredicateFailedStub(info);
3994 
3995     __ cmp(lir_cond(cond), left, right);
<span class="line-modified">3996     __ branch(lir_cond(cond), right-&gt;type(), stub);</span>
3997   }
3998 }
3999 
4000 
4001 LIR_Opr LIRGenerator::call_runtime(Value arg1, address entry, ValueType* result_type, CodeEmitInfo* info) {
4002   LIRItemList args(1);
4003   LIRItem value(arg1, this);
4004   args.append(&amp;value);
4005   BasicTypeList signature;
4006   signature.append(as_BasicType(arg1-&gt;type()));
4007 
4008   return call_runtime(&amp;signature, &amp;args, entry, result_type, info);
4009 }
4010 
4011 
4012 LIR_Opr LIRGenerator::call_runtime(Value arg1, Value arg2, address entry, ValueType* result_type, CodeEmitInfo* info) {
4013   LIRItemList args(2);
4014   LIRItem value1(arg1, this);
4015   LIRItem value2(arg2, this);
4016   args.append(&amp;value1);
</pre>
</td>
<td>
<hr />
<pre>
 465 void LIRGenerator::klass2reg_with_patching(LIR_Opr r, ciMetadata* obj, CodeEmitInfo* info, bool need_resolve) {
 466   /* C2 relies on constant pool entries being resolved (ciTypeFlow), so if TieredCompilation
 467    * is active and the class hasn&#39;t yet been resolved we need to emit a patch that resolves
 468    * the class. */
 469   if ((TieredCompilation &amp;&amp; need_resolve) || !obj-&gt;is_loaded() || PatchALot) {
 470     assert(info != NULL, &quot;info must be set if class is not loaded&quot;);
 471     __ klass2reg_patch(NULL, r, info);
 472   } else {
 473     // no patching needed
 474     __ metadata2reg(obj-&gt;constant_encoding(), r);
 475   }
 476 }
 477 
 478 
 479 void LIRGenerator::array_range_check(LIR_Opr array, LIR_Opr index,
 480                                     CodeEmitInfo* null_check_info, CodeEmitInfo* range_check_info) {
 481   CodeStub* stub = new RangeCheckStub(range_check_info, index, array);
 482   if (index-&gt;is_constant()) {
 483     cmp_mem_int(lir_cond_belowEqual, array, arrayOopDesc::length_offset_in_bytes(),
 484                 index-&gt;as_jint(), null_check_info);
<span class="line-modified"> 485     __ branch(lir_cond_belowEqual, stub); // forward branch</span>
 486   } else {
 487     cmp_reg_mem(lir_cond_aboveEqual, index, array,
 488                 arrayOopDesc::length_offset_in_bytes(), T_INT, null_check_info);
<span class="line-modified"> 489     __ branch(lir_cond_aboveEqual, stub); // forward branch</span>
 490   }
 491 }
 492 
 493 
 494 void LIRGenerator::nio_range_check(LIR_Opr buffer, LIR_Opr index, LIR_Opr result, CodeEmitInfo* info) {
 495   CodeStub* stub = new RangeCheckStub(info, index);
 496   if (index-&gt;is_constant()) {
 497     cmp_mem_int(lir_cond_belowEqual, buffer, java_nio_Buffer::limit_offset(), index-&gt;as_jint(), info);
<span class="line-modified"> 498     __ branch(lir_cond_belowEqual, stub); // forward branch</span>
 499   } else {
 500     cmp_reg_mem(lir_cond_aboveEqual, index, buffer,
 501                 java_nio_Buffer::limit_offset(), T_INT, info);
<span class="line-modified"> 502     __ branch(lir_cond_aboveEqual, stub); // forward branch</span>
 503   }
 504   __ move(index, result);
 505 }
 506 
 507 
 508 
 509 void LIRGenerator::arithmetic_op(Bytecodes::Code code, LIR_Opr result, LIR_Opr left, LIR_Opr right, bool is_strictfp, LIR_Opr tmp_op, CodeEmitInfo* info) {
 510   LIR_Opr result_op = result;
 511   LIR_Opr left_op   = left;
 512   LIR_Opr right_op  = right;
 513 
 514   if (TwoOperandLIRForm &amp;&amp; left_op != result_op) {
 515     assert(right_op != result_op, &quot;malformed&quot;);
 516     __ move(left_op, result_op);
 517     left_op = result_op;
 518   }
 519 
 520   switch(code) {
 521     case Bytecodes::_dadd:
 522     case Bytecodes::_fadd:
</pre>
<hr />
<pre>
 674 #endif
 675 
 676 void LIRGenerator::new_instance(LIR_Opr dst, ciInstanceKlass* klass, bool is_unresolved, LIR_Opr scratch1, LIR_Opr scratch2, LIR_Opr scratch3, LIR_Opr scratch4, LIR_Opr klass_reg, CodeEmitInfo* info) {
 677   klass2reg_with_patching(klass_reg, klass, info, is_unresolved);
 678   // If klass is not loaded we do not know if the klass has finalizers:
 679   if (UseFastNewInstance &amp;&amp; klass-&gt;is_loaded()
 680       &amp;&amp; !Klass::layout_helper_needs_slow_path(klass-&gt;layout_helper())) {
 681 
 682     Runtime1::StubID stub_id = klass-&gt;is_initialized() ? Runtime1::fast_new_instance_id : Runtime1::fast_new_instance_init_check_id;
 683 
 684     CodeStub* slow_path = new NewInstanceStub(klass_reg, dst, klass, info, stub_id);
 685 
 686     assert(klass-&gt;is_loaded(), &quot;must be loaded&quot;);
 687     // allocate space for instance
 688     assert(klass-&gt;size_helper() &gt;= 0, &quot;illegal instance size&quot;);
 689     const int instance_size = align_object_size(klass-&gt;size_helper());
 690     __ allocate_object(dst, scratch1, scratch2, scratch3, scratch4,
 691                        oopDesc::header_size(), instance_size, klass_reg, !klass-&gt;is_initialized(), slow_path);
 692   } else {
 693     CodeStub* slow_path = new NewInstanceStub(klass_reg, dst, klass, info, Runtime1::new_instance_id);
<span class="line-modified"> 694     __ branch(lir_cond_always, slow_path);</span>
 695     __ branch_destination(slow_path-&gt;continuation());
 696   }
 697 }
 698 
 699 
 700 static bool is_constant_zero(Instruction* inst) {
 701   IntConstant* c = inst-&gt;type()-&gt;as_IntConstant();
 702   if (c) {
 703     return (c-&gt;value() == 0);
 704   }
 705   return false;
 706 }
 707 
 708 
 709 static bool positive_constant(Instruction* inst) {
 710   IntConstant* c = inst-&gt;type()-&gt;as_IntConstant();
 711   if (c) {
 712     return (c-&gt;value() &gt;= 0);
 713   }
 714   return false;
</pre>
<hr />
<pre>
1218     args-&gt;append(meth);
1219     call_runtime(&amp;signature, args, CAST_FROM_FN_PTR(address, SharedRuntime::dtrace_method_exit), voidType, NULL);
1220   }
1221 
1222   if (x-&gt;type()-&gt;is_void()) {
1223     __ return_op(LIR_OprFact::illegalOpr);
1224   } else {
1225     LIR_Opr reg = result_register_for(x-&gt;type(), /*callee=*/true);
1226     LIRItem result(x-&gt;result(), this);
1227 
1228     result.load_item_force(reg);
1229     __ return_op(result.result());
1230   }
1231   set_no_result(x);
1232 }
1233 
1234 // Examble: ref.get()
1235 // Combination of LoadField and g1 pre-write barrier
1236 void LIRGenerator::do_Reference_get(Intrinsic* x) {
1237 
<span class="line-modified">1238   const int referent_offset = java_lang_ref_Reference::referent_offset();</span>

1239 
1240   assert(x-&gt;number_of_arguments() == 1, &quot;wrong type&quot;);
1241 
1242   LIRItem reference(x-&gt;argument_at(0), this);
1243   reference.load_item();
1244 
1245   // need to perform the null check on the reference objecy
1246   CodeEmitInfo* info = NULL;
1247   if (x-&gt;needs_null_check()) {
1248     info = state_for(x);
1249   }
1250 
1251   LIR_Opr result = rlock_result(x, T_OBJECT);
1252   access_load_at(IN_HEAP | ON_WEAK_OOP_REF, T_OBJECT,
1253                  reference, LIR_OprFact::intConst(referent_offset), result);
1254 }
1255 
1256 // Example: clazz.isInstance(object)
1257 void LIRGenerator::do_isInstance(Intrinsic* x) {
1258   assert(x-&gt;number_of_arguments() == 2, &quot;wrong type&quot;);
</pre>
<hr />
<pre>
1304   __ move_wide(new LIR_Address(temp, in_bytes(Klass::java_mirror_offset()), T_ADDRESS), temp);
1305   // mirror = ((OopHandle)mirror)-&gt;resolve();
1306   access_load(IN_NATIVE, T_OBJECT,
1307               LIR_OprFact::address(new LIR_Address(temp, T_OBJECT)), result);
1308 }
1309 
1310 // java.lang.Class::isPrimitive()
1311 void LIRGenerator::do_isPrimitive(Intrinsic* x) {
1312   assert(x-&gt;number_of_arguments() == 1, &quot;wrong type&quot;);
1313 
1314   LIRItem rcvr(x-&gt;argument_at(0), this);
1315   rcvr.load_item();
1316   LIR_Opr temp = new_register(T_METADATA);
1317   LIR_Opr result = rlock_result(x);
1318 
1319   CodeEmitInfo* info = NULL;
1320   if (x-&gt;needs_null_check()) {
1321     info = state_for(x);
1322   }
1323 
<span class="line-modified">1324   __ move(new LIR_Address(rcvr.result(), java_lang_Class::klass_offset(), T_ADDRESS), temp, info);</span>
1325   __ cmp(lir_cond_notEqual, temp, LIR_OprFact::metadataConst(0));
1326   __ cmove(lir_cond_notEqual, LIR_OprFact::intConst(0), LIR_OprFact::intConst(1), result, T_BOOLEAN);
1327 }
1328 
1329 
1330 // Example: Thread.currentThread()
1331 void LIRGenerator::do_currentThread(Intrinsic* x) {
1332   assert(x-&gt;number_of_arguments() == 0, &quot;wrong type&quot;);
1333   LIR_Opr reg = rlock_result(x);
1334   __ move_wide(new LIR_Address(getThreadPointer(), in_bytes(JavaThread::threadObj_offset()), T_OBJECT), reg);
1335 }
1336 
1337 
1338 void LIRGenerator::do_RegisterFinalizer(Intrinsic* x) {
1339   assert(x-&gt;number_of_arguments() == 1, &quot;wrong type&quot;);
1340   LIRItem receiver(x-&gt;argument_at(0), this);
1341 
1342   receiver.load_item();
1343   BasicTypeList signature;
1344   signature.append(T_OBJECT); // receiver
</pre>
<hr />
<pre>
1546 
1547 #ifndef PRODUCT
1548   if (PrintNotLoaded &amp;&amp; needs_patching) {
1549     tty-&gt;print_cr(&quot;   ###class not loaded at store_%s bci %d&quot;,
1550                   x-&gt;is_static() ?  &quot;static&quot; : &quot;field&quot;, x-&gt;printable_bci());
1551   }
1552 #endif
1553 
1554   if (x-&gt;needs_null_check() &amp;&amp;
1555       (needs_patching ||
1556        MacroAssembler::needs_explicit_null_check(x-&gt;offset()))) {
1557     if (needs_patching &amp;&amp; x-&gt;field()-&gt;is_flattenable()) {
1558       // We are storing a field of type &quot;QT;&quot; into holder class H, but H is not yet
1559       // loaded. (If H had been loaded, then T must also have already been loaded
1560       // due to the &quot;Q&quot; signature, and needs_patching would be false).
1561       assert(!x-&gt;field()-&gt;holder()-&gt;is_loaded(), &quot;must be&quot;);
1562       // We don&#39;t know the offset of this field. Let&#39;s deopt and recompile.
1563       CodeStub* stub = new DeoptimizeStub(new CodeEmitInfo(info),
1564                                           Deoptimization::Reason_unloaded,
1565                                           Deoptimization::Action_make_not_entrant);
<span class="line-modified">1566       __ branch(lir_cond_always, stub);</span>
1567     } else {
1568       // Emit an explicit null check because the offset is too large.
1569       // If the class is not loaded and the object is NULL, we need to deoptimize to throw a
1570       // NoClassDefFoundError in the interpreter instead of an implicit NPE from compiled code.
1571       __ null_check(object.result(), new CodeEmitInfo(info), /* deoptimize */ needs_patching);
1572     }
1573   }
1574 
1575   DecoratorSet decorators = IN_HEAP;
1576   if (is_volatile) {
1577     decorators |= MO_SEQ_CST;
1578   }
1579   if (needs_patching) {
1580     decorators |= C1_NEEDS_PATCHING;
1581   }
1582 
1583   access_store_at(decorators, field_type, object, LIR_OprFact::intConst(x-&gt;offset()),
1584                   value.result(), info != NULL ? new CodeEmitInfo(info) : NULL, info);
1585 }
1586 
</pre>
<hr />
<pre>
1665     } else {
1666       access_load_at(decorators, field_type,
1667                      obj_item, LIR_OprFact::intConst(obj_offset), temp,
1668                      NULL, NULL);
1669       access_store_at(decorators, field_type,
1670                       elm_item, LIR_OprFact::intConst(elm_offset), temp,
1671                       NULL, NULL);
1672     }
1673   }
1674 }
1675 
1676 void LIRGenerator::check_flattened_array(LIR_Opr array, LIR_Opr value, CodeStub* slow_path) {
1677   LIR_Opr tmp = new_register(T_METADATA);
1678   __ check_flattened_array(array, value, tmp, slow_path);
1679 }
1680 
1681 void LIRGenerator::check_null_free_array(LIRItem&amp; array, LIRItem&amp; value, CodeEmitInfo* info) {
1682   LabelObj* L_end = new LabelObj();
1683   LIR_Opr tmp = new_register(T_METADATA);
1684   __ check_null_free_array(array.result(), tmp);
<span class="line-modified">1685   __ branch(lir_cond_equal, L_end-&gt;label());</span>
1686   __ null_check(value.result(), info);
1687   __ branch_destination(L_end-&gt;label());
1688 }
1689 
1690 bool LIRGenerator::needs_flattened_array_store_check(StoreIndexed* x) {
1691   if (x-&gt;elt_type() == T_OBJECT &amp;&amp; x-&gt;array()-&gt;maybe_flattened_array()) {
1692     ciType* type = x-&gt;value()-&gt;declared_type();
1693     if (type != NULL &amp;&amp; type-&gt;is_klass()) {
1694       ciKlass* klass = type-&gt;as_klass();
1695       if (!klass-&gt;can_be_value_klass() || (klass-&gt;is_valuetype() &amp;&amp; !klass-&gt;as_value_klass()-&gt;flatten_array())) {
1696         // This is known to be a non-flattenable object. If the array is flattened,
1697         // it will be caught by the code generated by array_store_check().
1698         return false;
1699       }
1700     }
1701     // We&#39;re not 100% sure, so let&#39;s do the flattened_array_store_check.
1702     return true;
1703   }
1704   return false;
1705 }
</pre>
<hr />
<pre>
1736       || is_loaded_flattened_array || needs_flattened_array_store_check(x) || needs_null_free_array_store_check(x)) {
1737     value.load_item();
1738   } else {
1739     value.load_for_store(x-&gt;elt_type());
1740   }
1741 
1742   set_no_result(x);
1743 
1744   // the CodeEmitInfo must be duplicated for each different
1745   // LIR-instruction because spilling can occur anywhere between two
1746   // instructions and so the debug information must be different
1747   CodeEmitInfo* range_check_info = state_for(x);
1748   CodeEmitInfo* null_check_info = NULL;
1749   if (x-&gt;needs_null_check()) {
1750     null_check_info = new CodeEmitInfo(range_check_info);
1751   }
1752 
1753   if (GenerateRangeChecks &amp;&amp; needs_range_check) {
1754     if (use_length) {
1755       __ cmp(lir_cond_belowEqual, length.result(), index.result());
<span class="line-modified">1756       __ branch(lir_cond_belowEqual, new RangeCheckStub(range_check_info, index.result(), array.result()));</span>
1757     } else {
1758       array_range_check(array.result(), index.result(), null_check_info, range_check_info);
1759       // range_check also does the null check
1760       null_check_info = NULL;
1761     }
1762   }
1763 
1764   if (x-&gt;should_profile()) {
1765     ciMethodData* md = NULL;
1766     ciArrayLoadStoreData* load_store = NULL;
1767     profile_array_type(x, md, load_store);
1768     if (is_loaded_flattened_array) {
1769       int flag = ArrayLoadStoreData::flat_array_byte_constant() | ArrayLoadStoreData::null_free_array_byte_constant();
1770       assert(md != NULL, &quot;should have been initialized&quot;);
1771       profile_array_load_store_flags(md, load_store, flag);
1772     } else if (x-&gt;array()-&gt;maybe_null_free_array()) {
1773       profile_null_free_array(array, md, load_store);
1774     }
1775     profile_element_type(x-&gt;value(), md, load_store);
1776   }
</pre>
<hr />
<pre>
1930       // (1) holder is unloaded -- problem: we needed the field offset back in GraphBuilder::access_field()
1931       //                           FIXME: consider getting field offset in patching code (but only if the field
1932       //                           type was loaded at compilation time).
1933       deopt = true;
1934     } else if (field_type_unloaded) {
1935       // (2) field type is unloaded -- problem: we don&#39;t know whether it&#39;s flattened or not. Let&#39;s deopt
1936       deopt = true;
1937     } else if (!field-&gt;is_flattened()) {
1938       // (3) field is not flattened -- need default value in cases of uninitialized field
1939       need_default = true;
1940     }
1941   }
1942 
1943   if (deopt) {
1944     assert(!need_default, &quot;deopt and need_default cannot both be true&quot;);
1945     assert(x-&gt;needs_patching(), &quot;must be&quot;);
1946     assert(info != NULL, &quot;must be&quot;);
1947     CodeStub* stub = new DeoptimizeStub(new CodeEmitInfo(info),
1948                                         Deoptimization::Reason_unloaded,
1949                                         Deoptimization::Action_make_not_entrant);
<span class="line-modified">1950     __ branch(lir_cond_always, stub);</span>
1951   } else if (need_default) {
1952     assert(!field_type_unloaded, &quot;must be&quot;);
1953     assert(field-&gt;type()-&gt;is_valuetype(), &quot;must be&quot;);
1954     ciValueKlass* value_klass = field-&gt;type()-&gt;as_value_klass();
1955     assert(value_klass-&gt;is_loaded(), &quot;must be&quot;);
1956 
1957     if (field-&gt;is_static() &amp;&amp; holder-&gt;is_loaded()) {
1958       ciInstance* mirror = field-&gt;holder()-&gt;java_mirror();
1959       ciObject* val = mirror-&gt;field_value(field).as_object();
1960       if (val-&gt;is_null_object()) {
1961         // This is a non-nullable static field, but it&#39;s not initialized.
1962         // We need to do a null check, and replace it with the default value.
1963       } else {
1964         // No need to perform null check on this static field
1965         need_default = false;
1966       }
1967     }
1968 
1969     if (need_default) {
1970       default_value = new Constant(new InstanceConstant(value_klass-&gt;default_value_instance()));
</pre>
<hr />
<pre>
2023     // NoClassDefFoundError in the interpreter instead of an implicit NPE from compiled code.
2024     __ null_check(obj, new CodeEmitInfo(info), /* deoptimize */ needs_patching);
2025   }
2026 
2027   DecoratorSet decorators = IN_HEAP;
2028   if (is_volatile) {
2029     decorators |= MO_SEQ_CST;
2030   }
2031   if (needs_patching) {
2032     decorators |= C1_NEEDS_PATCHING;
2033   }
2034 
2035   LIR_Opr result = rlock_result(x, field_type);
2036   access_load_at(decorators, field_type,
2037                  object, LIR_OprFact::intConst(x-&gt;offset()), result,
2038                  info ? new CodeEmitInfo(info) : NULL, info);
2039 
2040   if (default_value != NULL) {
2041     LabelObj* L_end = new LabelObj();
2042     __ cmp(lir_cond_notEqual, result, LIR_OprFact::oopConst(NULL));
<span class="line-modified">2043     __ branch(lir_cond_notEqual, L_end-&gt;label());</span>
2044     set_in_conditional_code(true);
2045     __ move(load_constant(default_value), result);
2046     __ branch_destination(L_end-&gt;label());
2047     set_in_conditional_code(false);
2048   }
2049 }
2050 
2051 
2052 //------------------------java.nio.Buffer.checkIndex------------------------
2053 
2054 // int java.nio.Buffer.checkIndex(int)
2055 void LIRGenerator::do_NIOCheckIndex(Intrinsic* x) {
2056   // NOTE: by the time we are in checkIndex() we are guaranteed that
2057   // the buffer is non-null (because checkIndex is package-private and
2058   // only called from within other methods in the buffer).
2059   assert(x-&gt;number_of_arguments() == 2, &quot;wrong type&quot;);
2060   LIRItem buf  (x-&gt;argument_at(0), this);
2061   LIRItem index(x-&gt;argument_at(1), this);
2062   buf.load_item();
2063   index.load_item();
2064 
2065   LIR_Opr result = rlock_result(x);
2066   if (GenerateRangeChecks) {
2067     CodeEmitInfo* info = state_for(x);
2068     CodeStub* stub = new RangeCheckStub(info, index.result());
2069     LIR_Opr buf_obj = access_resolve(IS_NOT_NULL | ACCESS_READ, buf.result());
2070     if (index.result()-&gt;is_constant()) {
2071       cmp_mem_int(lir_cond_belowEqual, buf_obj, java_nio_Buffer::limit_offset(), index.result()-&gt;as_jint(), info);
<span class="line-modified">2072       __ branch(lir_cond_belowEqual, stub);</span>
2073     } else {
2074       cmp_reg_mem(lir_cond_aboveEqual, index.result(), buf_obj,
2075                   java_nio_Buffer::limit_offset(), T_INT, info);
<span class="line-modified">2076       __ branch(lir_cond_aboveEqual, stub);</span>
2077     }
2078     __ move(index.result(), result);
2079   } else {
2080     // Just load the index into the result register
2081     __ move(index.result(), result);
2082   }
2083 }
2084 
2085 
2086 //------------------------array access--------------------------------------
2087 
2088 
2089 void LIRGenerator::do_ArrayLength(ArrayLength* x) {
2090   LIRItem array(x-&gt;array(), this);
2091   array.load_item();
2092   LIR_Opr reg = rlock_result(x);
2093 
2094   CodeEmitInfo* info = NULL;
2095   if (x-&gt;needs_null_check()) {
2096     NullCheck* nc = x-&gt;explicit_null_check();
</pre>
<hr />
<pre>
2130   }
2131 
2132   CodeEmitInfo* range_check_info = state_for(x);
2133   CodeEmitInfo* null_check_info = NULL;
2134   if (x-&gt;needs_null_check()) {
2135     NullCheck* nc = x-&gt;explicit_null_check();
2136     if (nc != NULL) {
2137       null_check_info = state_for(nc);
2138     } else {
2139       null_check_info = range_check_info;
2140     }
2141     if (StressLoopInvariantCodeMotion &amp;&amp; null_check_info-&gt;deoptimize_on_exception()) {
2142       LIR_Opr obj = new_register(T_OBJECT);
2143       __ move(LIR_OprFact::oopConst(NULL), obj);
2144       __ null_check(obj, new CodeEmitInfo(null_check_info));
2145     }
2146   }
2147 
2148   if (GenerateRangeChecks &amp;&amp; needs_range_check) {
2149     if (StressLoopInvariantCodeMotion &amp;&amp; range_check_info-&gt;deoptimize_on_exception()) {
<span class="line-modified">2150       __ branch(lir_cond_always, new RangeCheckStub(range_check_info, index.result(), array.result()));</span>
2151     } else if (use_length) {
2152       // TODO: use a (modified) version of array_range_check that does not require a
2153       //       constant length to be loaded to a register
2154       __ cmp(lir_cond_belowEqual, length.result(), index.result());
<span class="line-modified">2155       __ branch(lir_cond_belowEqual, new RangeCheckStub(range_check_info, index.result(), array.result()));</span>
2156     } else {
2157       array_range_check(array.result(), index.result(), null_check_info, range_check_info);
2158       // The range check performs the null check, so clear it out for the load
2159       null_check_info = NULL;
2160     }
2161   }
2162 
2163   ciMethodData* md = NULL;
2164   ciArrayLoadStoreData* load_store = NULL;
2165   if (x-&gt;should_profile()) {
2166     profile_array_type(x, md, load_store);
2167   }
2168 
2169   Value element;
2170   if (x-&gt;vt() != NULL) {
2171     assert(x-&gt;array()-&gt;is_loaded_flattened_array(), &quot;must be&quot;);
2172     // Find the destination address (of the NewValueTypeInstance).
2173     LIR_Opr obj = x-&gt;vt()-&gt;operand();
2174     LIRItem obj_item(x-&gt;vt(), this);
2175 
</pre>
<hr />
<pre>
2205       __ branch_destination(slow_path-&gt;continuation());
2206       set_in_conditional_code(false);
2207     }
2208 
2209     element = x;
2210   }
2211 
2212   if (x-&gt;should_profile()) {
2213     profile_element_type(element, md, load_store);
2214   }
2215 }
2216 
2217 void LIRGenerator::do_WithField(WithField* x) {
2218   // This happens only when a class X uses the withfield bytecode to refer to
2219   // an inline class V, where V has not yet been loaded. This is not a common
2220   // case. Let&#39;s just deoptimize.
2221   CodeEmitInfo* info = state_for(x, x-&gt;state_before());
2222   CodeStub* stub = new DeoptimizeStub(new CodeEmitInfo(info),
2223                                       Deoptimization::Reason_unloaded,
2224                                       Deoptimization::Action_make_not_entrant);
<span class="line-modified">2225   __ branch(lir_cond_always, stub);</span>
2226   LIR_Opr reg = rlock_result(x, T_OBJECT);
2227   __ move(LIR_OprFact::oopConst(NULL), reg);
2228 }
2229 
2230 void LIRGenerator::do_DefaultValue(DefaultValue* x) {
2231   // Same as withfield above. Let&#39;s deoptimize.
2232   CodeEmitInfo* info = state_for(x, x-&gt;state_before());
2233   CodeStub* stub = new DeoptimizeStub(new CodeEmitInfo(info),
2234                                       Deoptimization::Reason_unloaded,
2235                                       Deoptimization::Action_make_not_entrant);
<span class="line-modified">2236   __ branch(lir_cond_always, stub);</span>
2237   LIR_Opr reg = rlock_result(x, T_OBJECT);
2238   __ move(LIR_OprFact::oopConst(NULL), reg);
2239 }
2240 
2241 void LIRGenerator::do_NullCheck(NullCheck* x) {
2242   if (x-&gt;can_trap()) {
2243     LIRItem value(x-&gt;obj(), this);
2244     value.load_item();
2245     CodeEmitInfo* info = state_for(x);
2246     __ null_check(value.result(), info);
2247   }
2248 }
2249 
2250 
2251 void LIRGenerator::do_TypeCast(TypeCast* x) {
2252   LIRItem value(x-&gt;obj(), this);
2253   value.load_item();
2254   // the result is the same as from the node we are casting
2255   set_result(x, value.result());
2256 }
</pre>
<hr />
<pre>
2580 
2581   LIR_Opr result;
2582   if (x-&gt;is_add()) {
2583     result = access_atomic_add_at(decorators, type, src, off, value);
2584   } else {
2585     result = access_atomic_xchg_at(decorators, type, src, off, value);
2586   }
2587   set_result(x, result);
2588 }
2589 
2590 void LIRGenerator::do_SwitchRanges(SwitchRangeArray* x, LIR_Opr value, BlockBegin* default_sux) {
2591   int lng = x-&gt;length();
2592 
2593   for (int i = 0; i &lt; lng; i++) {
2594     C1SwitchRange* one_range = x-&gt;at(i);
2595     int low_key = one_range-&gt;low_key();
2596     int high_key = one_range-&gt;high_key();
2597     BlockBegin* dest = one_range-&gt;sux();
2598     if (low_key == high_key) {
2599       __ cmp(lir_cond_equal, value, low_key);
<span class="line-modified">2600       __ branch(lir_cond_equal, dest);</span>
2601     } else if (high_key - low_key == 1) {
2602       __ cmp(lir_cond_equal, value, low_key);
<span class="line-modified">2603       __ branch(lir_cond_equal, dest);</span>
2604       __ cmp(lir_cond_equal, value, high_key);
<span class="line-modified">2605       __ branch(lir_cond_equal, dest);</span>
2606     } else {
2607       LabelObj* L = new LabelObj();
2608       __ cmp(lir_cond_less, value, low_key);
<span class="line-modified">2609       __ branch(lir_cond_less, L-&gt;label());</span>
2610       __ cmp(lir_cond_lessEqual, value, high_key);
<span class="line-modified">2611       __ branch(lir_cond_lessEqual, dest);</span>
2612       __ branch_destination(L-&gt;label());
2613     }
2614   }
2615   __ jump(default_sux);
2616 }
2617 
2618 
2619 SwitchRangeArray* LIRGenerator::create_lookup_ranges(TableSwitch* x) {
2620   SwitchRangeList* res = new SwitchRangeList();
2621   int len = x-&gt;length();
2622   if (len &gt; 0) {
2623     BlockBegin* sux = x-&gt;sux_at(0);
2624     int key = x-&gt;lo_key();
2625     BlockBegin* default_sux = x-&gt;default_sux();
2626     C1SwitchRange* range = new C1SwitchRange(key, sux);
2627     for (int i = 0; i &lt; len; i++, key++) {
2628       BlockBegin* new_sux = x-&gt;sux_at(i);
2629       if (sux == new_sux) {
2630         // still in same range
2631         range-&gt;set_high_key(key);
</pre>
<hr />
<pre>
2711       __ cmp(lir_cond_equal, value, i + lo_key);
2712       __ move(data_offset_reg, tmp_reg);
2713       __ cmove(lir_cond_equal,
2714                LIR_OprFact::intptrConst(count_offset),
2715                tmp_reg,
2716                data_offset_reg, T_INT);
2717     }
2718 
2719     LIR_Opr data_reg = new_pointer_register();
2720     LIR_Address* data_addr = new LIR_Address(md_reg, data_offset_reg, data_reg-&gt;type());
2721     __ move(data_addr, data_reg);
2722     __ add(data_reg, LIR_OprFact::intptrConst(1), data_reg);
2723     __ move(data_reg, data_addr);
2724   }
2725 
2726   if (UseTableRanges) {
2727     do_SwitchRanges(create_lookup_ranges(x), value, x-&gt;default_sux());
2728   } else {
2729     for (int i = 0; i &lt; len; i++) {
2730       __ cmp(lir_cond_equal, value, i + lo_key);
<span class="line-modified">2731       __ branch(lir_cond_equal, x-&gt;sux_at(i));</span>
2732     }
2733     __ jump(x-&gt;default_sux());
2734   }
2735 }
2736 
2737 
2738 void LIRGenerator::do_LookupSwitch(LookupSwitch* x) {
2739   LIRItem tag(x-&gt;tag(), this);
2740   tag.load_item();
2741   set_no_result(x);
2742 
2743   if (x-&gt;is_safepoint()) {
2744     __ safepoint(safepoint_poll_register(), state_for(x, x-&gt;state_before()));
2745   }
2746 
2747   // move values into phi locations
2748   move_to_phi(x-&gt;state());
2749 
2750   LIR_Opr value = tag.result();
2751   int len = x-&gt;length();
</pre>
<hr />
<pre>
2770       __ move(data_offset_reg, tmp_reg);
2771       __ cmove(lir_cond_equal,
2772                LIR_OprFact::intptrConst(count_offset),
2773                tmp_reg,
2774                data_offset_reg, T_INT);
2775     }
2776 
2777     LIR_Opr data_reg = new_pointer_register();
2778     LIR_Address* data_addr = new LIR_Address(md_reg, data_offset_reg, data_reg-&gt;type());
2779     __ move(data_addr, data_reg);
2780     __ add(data_reg, LIR_OprFact::intptrConst(1), data_reg);
2781     __ move(data_reg, data_addr);
2782   }
2783 
2784   if (UseTableRanges) {
2785     do_SwitchRanges(create_lookup_ranges(x), value, x-&gt;default_sux());
2786   } else {
2787     int len = x-&gt;length();
2788     for (int i = 0; i &lt; len; i++) {
2789       __ cmp(lir_cond_equal, value, x-&gt;key_at(i));
<span class="line-modified">2790       __ branch(lir_cond_equal, x-&gt;sux_at(i));</span>
2791     }
2792     __ jump(x-&gt;default_sux());
2793   }
2794 }
2795 
2796 
2797 void LIRGenerator::do_Goto(Goto* x) {
2798   set_no_result(x);
2799 
2800   if (block()-&gt;next()-&gt;as_OsrEntry()) {
2801     // need to free up storage used for OSR entry point
2802     LIR_Opr osrBuffer = block()-&gt;next()-&gt;operand();
2803     BasicTypeList signature;
2804     signature.append(NOT_LP64(T_INT) LP64_ONLY(T_LONG)); // pass a pointer to osrBuffer
2805     CallingConvention* cc = frame_map()-&gt;c_calling_convention(&amp;signature);
2806     __ move(osrBuffer, cc-&gt;args()-&gt;at(0));
2807     __ call_runtime_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::OSR_migration_end),
2808                          getThreadTemp(), LIR_OprFact::illegalOpr, cc-&gt;args());
2809   }
2810 
</pre>
<hr />
<pre>
2988 void LIRGenerator::profile_array_load_store_flags(ciMethodData* md, ciArrayLoadStoreData* load_store, int flag, LIR_Opr mdp) {
2989   assert(md != NULL &amp;&amp; load_store != NULL, &quot;should have been initialized&quot;);
2990   if (mdp == NULL) {
2991     mdp = new_register(T_METADATA);
2992     __ metadata2reg(md-&gt;constant_encoding(), mdp);
2993   }
2994   LIR_Address* addr = new LIR_Address(mdp, md-&gt;byte_offset_of_slot(load_store, DataLayout::flags_offset()), T_BYTE);
2995   LIR_Opr id = new_register(T_INT);
2996   __ move(addr, id);
2997   __ logical_or(id, LIR_OprFact::intConst(flag), id);
2998   __ store(id, addr);
2999 }
3000 
3001 void LIRGenerator::profile_null_free_array(LIRItem array, ciMethodData* md, ciArrayLoadStoreData* load_store) {
3002   LabelObj* L_end = new LabelObj();
3003   LIR_Opr tmp = new_register(T_METADATA);
3004   LIR_Opr mdp = new_register(T_METADATA);
3005   assert(md != NULL, &quot;should have been initialized&quot;);
3006   __ metadata2reg(md-&gt;constant_encoding(), mdp);
3007   __ check_null_free_array(array.result(), tmp);
<span class="line-modified">3008   __ branch(lir_cond_equal, L_end-&gt;label());</span>
3009 
3010   profile_array_load_store_flags(md, load_store, ArrayLoadStoreData::null_free_array_byte_constant(), mdp);
3011 
3012   __ branch_destination(L_end-&gt;label());
3013 }
3014 
3015 void LIRGenerator::profile_array_type(AccessIndexed* x, ciMethodData*&amp; md, ciArrayLoadStoreData*&amp; load_store) {
3016   int bci = x-&gt;profiled_bci();
3017   md = x-&gt;profiled_method()-&gt;method_data();
3018   assert(md != NULL, &quot;Sanity&quot;);
3019   ciProfileData* data = md-&gt;bci_to_data(bci);
3020   assert(data != NULL &amp;&amp; data-&gt;is_ArrayLoadStoreData(), &quot;incorrect profiling entry&quot;);
3021   load_store = (ciArrayLoadStoreData*)data;
3022   LIR_Opr mdp = LIR_OprFact::illegalOpr;
3023   profile_type(md, md-&gt;byte_offset_of_slot(load_store, ArrayLoadStoreData::array_offset()), 0,
3024                load_store-&gt;array()-&gt;type(), x-&gt;array(), mdp, true, NULL, NULL);
3025 }
3026 
3027 void LIRGenerator::profile_element_type(Value element, ciMethodData* md, ciArrayLoadStoreData* load_store) {
3028   assert(md != NULL &amp;&amp; load_store != NULL, &quot;should have been initialized&quot;);
</pre>
<hr />
<pre>
3105       // receiver is guaranteed non-NULL so don&#39;t need CodeEmitInfo
3106       __ lock_object(syncTempOpr(), obj, lock, new_register(T_OBJECT), slow_path, NULL);
3107     }
3108   }
3109   if (compilation()-&gt;age_code()) {
3110     CodeEmitInfo* info = new CodeEmitInfo(scope()-&gt;start()-&gt;state()-&gt;copy(ValueStack::StateBefore, 0), NULL, false);
3111     decrement_age(info);
3112   }
3113   // increment invocation counters if needed
3114   if (!method()-&gt;is_accessor()) { // Accessors do not have MDOs, so no counting.
3115     profile_parameters(x);
3116     CodeEmitInfo* info = new CodeEmitInfo(scope()-&gt;start()-&gt;state()-&gt;copy(ValueStack::StateBefore, SynchronizationEntryBCI), NULL, false);
3117     increment_invocation_counter(info);
3118   }
3119   if (method()-&gt;has_scalarized_args()) {
3120     // Check if deoptimization was triggered (i.e. orig_pc was set) while buffering scalarized value type arguments
3121     // in the entry point (see comments in frame::deoptimize). If so, deoptimize only now that we have the right state.
3122     CodeEmitInfo* info = new CodeEmitInfo(scope()-&gt;start()-&gt;state()-&gt;copy(ValueStack::StateBefore, 0), NULL, false);
3123     CodeStub* deopt_stub = new DeoptimizeStub(info, Deoptimization::Reason_none, Deoptimization::Action_none);
3124     __ append(new LIR_Op0(lir_check_orig_pc));
<span class="line-modified">3125     __ branch(lir_cond_notEqual, deopt_stub);</span>
3126   }
3127 
3128   // all blocks with a successor must end with an unconditional jump
3129   // to the successor even if they are consecutive
3130   __ jump(x-&gt;default_sux());
3131 }
3132 
3133 
3134 void LIRGenerator::do_OsrEntry(OsrEntry* x) {
3135   // construct our frame and model the production of incoming pointer
3136   // to the OSR buffer.
3137   __ osr_entry(LIR_Assembler::osrBufferPointer());
3138   LIR_Opr result = rlock_result(x);
3139   __ move(LIR_Assembler::osrBufferPointer(), result);
3140 }
3141 
3142 void LIRGenerator::invoke_load_one_argument(LIRItem* param, LIR_Opr loc) {
3143   if (loc-&gt;is_register()) {
3144     param-&gt;load_item_force(loc);
3145   } else {
</pre>
<hr />
<pre>
3412     left_klass_op = new_register(t_klass);
3413     right_klass_op = new_register(t_klass);
3414   }
3415 
3416   CodeStub* slow_path = new SubstitutabilityCheckStub(left.result(), right.result(), info);
3417   __ substitutability_check(result, left.result(), right.result(), equal_result, not_equal_result,
3418                             tmp1, tmp2,
3419                             left_klass, right_klass, left_klass_op, right_klass_op, info, slow_path);
3420 }
3421 
3422 #ifdef JFR_HAVE_INTRINSICS
3423 void LIRGenerator::do_ClassIDIntrinsic(Intrinsic* x) {
3424   CodeEmitInfo* info = state_for(x);
3425   CodeEmitInfo* info2 = new CodeEmitInfo(info); // Clone for the second null check
3426 
3427   assert(info != NULL, &quot;must have info&quot;);
3428   LIRItem arg(x-&gt;argument_at(0), this);
3429 
3430   arg.load_item();
3431   LIR_Opr klass = new_register(T_METADATA);
<span class="line-modified">3432   __ move(new LIR_Address(arg.result(), java_lang_Class::klass_offset(), T_ADDRESS), klass, info);</span>
3433   LIR_Opr id = new_register(T_LONG);
3434   ByteSize offset = KLASS_TRACE_ID_OFFSET;
3435   LIR_Address* trace_id_addr = new LIR_Address(klass, in_bytes(offset), T_LONG);
3436 
3437   __ move(trace_id_addr, id);
3438   __ logical_or(id, LIR_OprFact::longConst(0x01l), id);
3439   __ store(id, trace_id_addr);
3440 
3441 #ifdef TRACE_ID_META_BITS
3442   __ logical_and(id, LIR_OprFact::longConst(~TRACE_ID_META_BITS), id);
3443 #endif
3444 #ifdef TRACE_ID_SHIFT
3445   __ unsigned_shift_right(id, TRACE_ID_SHIFT, id);
3446 #endif
3447 
3448   __ move(id, rlock_result(x));
3449 }
3450 
3451 void LIRGenerator::do_getEventWriter(Intrinsic* x) {
3452   LabelObj* L_end = new LabelObj();
3453 
<span class="line-added">3454   // FIXME T_ADDRESS should actually be T_METADATA but it can&#39;t because the</span>
<span class="line-added">3455   // meaning of these two is mixed up (see JDK-8026837).</span>
3456   LIR_Address* jobj_addr = new LIR_Address(getThreadPointer(),
3457                                            in_bytes(THREAD_LOCAL_WRITER_OFFSET_JFR),
<span class="line-modified">3458                                            T_ADDRESS);</span>
3459   LIR_Opr result = rlock_result(x);
<span class="line-modified">3460   __ move(LIR_OprFact::oopConst(NULL), result);</span>
<span class="line-modified">3461   LIR_Opr jobj = new_register(T_METADATA);</span>
<span class="line-modified">3462   __ move_wide(jobj_addr, jobj);</span>
<span class="line-added">3463   __ cmp(lir_cond_equal, jobj, LIR_OprFact::metadataConst(0));</span>
<span class="line-added">3464   __ branch(lir_cond_equal, L_end-&gt;label());</span>
3465 


3466   access_load(IN_NATIVE, T_OBJECT, LIR_OprFact::address(new LIR_Address(jobj, T_OBJECT)), result);
3467 
3468   __ branch_destination(L_end-&gt;label());
3469 }
3470 
3471 #endif
3472 
3473 
3474 void LIRGenerator::do_RuntimeCall(address routine, Intrinsic* x) {
3475   assert(x-&gt;number_of_arguments() == 0, &quot;wrong type&quot;);
3476   // Enforce computation of _reserved_argument_area_size which is required on some platforms.
3477   BasicTypeList signature;
3478   CallingConvention* cc = frame_map()-&gt;c_calling_convention(&amp;signature);
3479   LIR_Opr reg = result_register_for(x-&gt;type());
3480   __ call_runtime_leaf(routine, getThreadTemp(),
3481                        reg, new LIR_OprList());
3482   LIR_Opr result = rlock_result(x);
3483   __ move(reg, result);
3484 }
3485 
</pre>
<hr />
<pre>
3803   }
3804   increment_event_counter_impl(info, info-&gt;scope()-&gt;method(), step, right_n_bits(freq_log), bci, backedge, true);
3805 }
3806 
3807 void LIRGenerator::decrement_age(CodeEmitInfo* info) {
3808   ciMethod* method = info-&gt;scope()-&gt;method();
3809   MethodCounters* mc_adr = method-&gt;ensure_method_counters();
3810   if (mc_adr != NULL) {
3811     LIR_Opr mc = new_pointer_register();
3812     __ move(LIR_OprFact::intptrConst(mc_adr), mc);
3813     int offset = in_bytes(MethodCounters::nmethod_age_offset());
3814     LIR_Address* counter = new LIR_Address(mc, offset, T_INT);
3815     LIR_Opr result = new_register(T_INT);
3816     __ load(counter, result);
3817     __ sub(result, LIR_OprFact::intConst(1), result);
3818     __ store(result, counter);
3819     // DeoptimizeStub will reexecute from the current state in code info.
3820     CodeStub* deopt = new DeoptimizeStub(info, Deoptimization::Reason_tenured,
3821                                          Deoptimization::Action_make_not_entrant);
3822     __ cmp(lir_cond_lessEqual, result, LIR_OprFact::intConst(0));
<span class="line-modified">3823     __ branch(lir_cond_lessEqual, deopt);</span>
3824   }
3825 }
3826 
3827 
3828 void LIRGenerator::increment_event_counter_impl(CodeEmitInfo* info,
3829                                                 ciMethod *method, LIR_Opr step, int frequency,
3830                                                 int bci, bool backedge, bool notify) {
3831   assert(frequency == 0 || is_power_of_2(frequency + 1), &quot;Frequency must be x^2 - 1 or 0&quot;);
3832   int level = _compilation-&gt;env()-&gt;comp_level();
3833   assert(level &gt; CompLevel_simple, &quot;Shouldn&#39;t be here&quot;);
3834 
3835   int offset = -1;
3836   LIR_Opr counter_holder = NULL;
3837   if (level == CompLevel_limited_profile) {
3838     MethodCounters* counters_adr = method-&gt;ensure_method_counters();
3839     if (counters_adr == NULL) {
3840       bailout(&quot;method counters allocation failed&quot;);
3841       return;
3842     }
3843     counter_holder = new_pointer_register();
</pre>
<hr />
<pre>
3850                                  MethodData::invocation_counter_offset());
3851     ciMethodData* md = method-&gt;method_data_or_null();
3852     assert(md != NULL, &quot;Sanity&quot;);
3853     __ metadata2reg(md-&gt;constant_encoding(), counter_holder);
3854   } else {
3855     ShouldNotReachHere();
3856   }
3857   LIR_Address* counter = new LIR_Address(counter_holder, offset, T_INT);
3858   LIR_Opr result = new_register(T_INT);
3859   __ load(counter, result);
3860   __ add(result, step, result);
3861   __ store(result, counter);
3862   if (notify &amp;&amp; (!backedge || UseOnStackReplacement)) {
3863     LIR_Opr meth = LIR_OprFact::metadataConst(method-&gt;constant_encoding());
3864     // The bci for info can point to cmp for if&#39;s we want the if bci
3865     CodeStub* overflow = new CounterOverflowStub(info, bci, meth);
3866     int freq = frequency &lt;&lt; InvocationCounter::count_shift;
3867     if (freq == 0) {
3868       if (!step-&gt;is_constant()) {
3869         __ cmp(lir_cond_notEqual, step, LIR_OprFact::intConst(0));
<span class="line-modified">3870         __ branch(lir_cond_notEqual, overflow);</span>
3871       } else {
<span class="line-modified">3872         __ branch(lir_cond_always, overflow);</span>
3873       }
3874     } else {
3875       LIR_Opr mask = load_immediate(freq, T_INT);
3876       if (!step-&gt;is_constant()) {
3877         // If step is 0, make sure the overflow check below always fails
3878         __ cmp(lir_cond_notEqual, step, LIR_OprFact::intConst(0));
3879         __ cmove(lir_cond_notEqual, result, LIR_OprFact::intConst(InvocationCounter::count_increment), result, T_INT);
3880       }
3881       __ logical_and(result, mask, result);
3882       __ cmp(lir_cond_equal, result, LIR_OprFact::intConst(0));
<span class="line-modified">3883       __ branch(lir_cond_equal, overflow);</span>
3884     }
3885     __ branch_destination(overflow-&gt;continuation());
3886   }
3887 }
3888 
3889 void LIRGenerator::do_RuntimeCall(RuntimeCall* x) {
3890   LIR_OprList* args = new LIR_OprList(x-&gt;number_of_arguments());
3891   BasicTypeList* signature = new BasicTypeList(x-&gt;number_of_arguments());
3892 
3893   if (x-&gt;pass_thread()) {
3894     signature-&gt;append(LP64_ONLY(T_LONG) NOT_LP64(T_INT));    // thread
3895     args-&gt;append(getThreadPointer());
3896   }
3897 
3898   for (int i = 0; i &lt; x-&gt;number_of_arguments(); i++) {
3899     Value a = x-&gt;argument_at(i);
3900     LIRItem* item = new LIRItem(a, this);
3901     item-&gt;load_item();
3902     args-&gt;append(item-&gt;result());
3903     signature-&gt;append(as_BasicType(a-&gt;type()));
</pre>
<hr />
<pre>
3977     ValueTag tag = x-&gt;x()-&gt;type()-&gt;tag();
3978     If::Condition cond = x-&gt;cond();
3979     LIRItem xitem(x-&gt;x(), this);
3980     LIRItem yitem(x-&gt;y(), this);
3981     LIRItem* xin = &amp;xitem;
3982     LIRItem* yin = &amp;yitem;
3983 
3984     assert(tag == intTag, &quot;Only integer deoptimizations are valid!&quot;);
3985 
3986     xin-&gt;load_item();
3987     yin-&gt;dont_load_item();
3988     set_no_result(x);
3989 
3990     LIR_Opr left = xin-&gt;result();
3991     LIR_Opr right = yin-&gt;result();
3992 
3993     CodeEmitInfo *info = state_for(x, x-&gt;state());
3994     CodeStub* stub = new PredicateFailedStub(info);
3995 
3996     __ cmp(lir_cond(cond), left, right);
<span class="line-modified">3997     __ branch(lir_cond(cond), stub);</span>
3998   }
3999 }
4000 
4001 
4002 LIR_Opr LIRGenerator::call_runtime(Value arg1, address entry, ValueType* result_type, CodeEmitInfo* info) {
4003   LIRItemList args(1);
4004   LIRItem value(arg1, this);
4005   args.append(&amp;value);
4006   BasicTypeList signature;
4007   signature.append(as_BasicType(arg1-&gt;type()));
4008 
4009   return call_runtime(&amp;signature, &amp;args, entry, result_type, info);
4010 }
4011 
4012 
4013 LIR_Opr LIRGenerator::call_runtime(Value arg1, Value arg2, address entry, ValueType* result_type, CodeEmitInfo* info) {
4014   LIRItemList args(2);
4015   LIRItem value1(arg1, this);
4016   LIRItem value2(arg2, this);
4017   args.append(&amp;value1);
</pre>
</td>
</tr>
</table>
<center><a href="c1_LIR.hpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="../ci/ciField.cpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>