diff a/src/hotspot/share/c1/c1_LIRGenerator.cpp b/src/hotspot/share/c1/c1_LIRGenerator.cpp
--- a/src/hotspot/share/c1/c1_LIRGenerator.cpp
+++ b/src/hotspot/share/c1/c1_LIRGenerator.cpp
@@ -480,28 +480,28 @@
                                     CodeEmitInfo* null_check_info, CodeEmitInfo* range_check_info) {
   CodeStub* stub = new RangeCheckStub(range_check_info, index, array);
   if (index->is_constant()) {
     cmp_mem_int(lir_cond_belowEqual, array, arrayOopDesc::length_offset_in_bytes(),
                 index->as_jint(), null_check_info);
-    __ branch(lir_cond_belowEqual, T_INT, stub); // forward branch
+    __ branch(lir_cond_belowEqual, stub); // forward branch
   } else {
     cmp_reg_mem(lir_cond_aboveEqual, index, array,
                 arrayOopDesc::length_offset_in_bytes(), T_INT, null_check_info);
-    __ branch(lir_cond_aboveEqual, T_INT, stub); // forward branch
+    __ branch(lir_cond_aboveEqual, stub); // forward branch
   }
 }
 
 
 void LIRGenerator::nio_range_check(LIR_Opr buffer, LIR_Opr index, LIR_Opr result, CodeEmitInfo* info) {
   CodeStub* stub = new RangeCheckStub(info, index);
   if (index->is_constant()) {
     cmp_mem_int(lir_cond_belowEqual, buffer, java_nio_Buffer::limit_offset(), index->as_jint(), info);
-    __ branch(lir_cond_belowEqual, T_INT, stub); // forward branch
+    __ branch(lir_cond_belowEqual, stub); // forward branch
   } else {
     cmp_reg_mem(lir_cond_aboveEqual, index, buffer,
                 java_nio_Buffer::limit_offset(), T_INT, info);
-    __ branch(lir_cond_aboveEqual, T_INT, stub); // forward branch
+    __ branch(lir_cond_aboveEqual, stub); // forward branch
   }
   __ move(index, result);
 }
 
 
@@ -689,11 +689,11 @@
     const int instance_size = align_object_size(klass->size_helper());
     __ allocate_object(dst, scratch1, scratch2, scratch3, scratch4,
                        oopDesc::header_size(), instance_size, klass_reg, !klass->is_initialized(), slow_path);
   } else {
     CodeStub* slow_path = new NewInstanceStub(klass_reg, dst, klass, info, Runtime1::new_instance_id);
-    __ branch(lir_cond_always, T_ILLEGAL, slow_path);
+    __ branch(lir_cond_always, slow_path);
     __ branch_destination(slow_path->continuation());
   }
 }
 
 
@@ -1233,12 +1233,11 @@
 
 // Examble: ref.get()
 // Combination of LoadField and g1 pre-write barrier
 void LIRGenerator::do_Reference_get(Intrinsic* x) {
 
-  const int referent_offset = java_lang_ref_Reference::referent_offset;
-  guarantee(referent_offset > 0, "referent offset not initialized");
+  const int referent_offset = java_lang_ref_Reference::referent_offset();
 
   assert(x->number_of_arguments() == 1, "wrong type");
 
   LIRItem reference(x->argument_at(0), this);
   reference.load_item();
@@ -1320,11 +1319,11 @@
   CodeEmitInfo* info = NULL;
   if (x->needs_null_check()) {
     info = state_for(x);
   }
 
-  __ move(new LIR_Address(rcvr.result(), java_lang_Class::klass_offset_in_bytes(), T_ADDRESS), temp, info);
+  __ move(new LIR_Address(rcvr.result(), java_lang_Class::klass_offset(), T_ADDRESS), temp, info);
   __ cmp(lir_cond_notEqual, temp, LIR_OprFact::metadataConst(0));
   __ cmove(lir_cond_notEqual, LIR_OprFact::intConst(0), LIR_OprFact::intConst(1), result, T_BOOLEAN);
 }
 
 
@@ -1562,11 +1561,11 @@
       assert(!x->field()->holder()->is_loaded(), "must be");
       // We don't know the offset of this field. Let's deopt and recompile.
       CodeStub* stub = new DeoptimizeStub(new CodeEmitInfo(info),
                                           Deoptimization::Reason_unloaded,
                                           Deoptimization::Action_make_not_entrant);
-      __ branch(lir_cond_always, T_ILLEGAL, stub);
+      __ branch(lir_cond_always, stub);
     } else {
       // Emit an explicit null check because the offset is too large.
       // If the class is not loaded and the object is NULL, we need to deoptimize to throw a
       // NoClassDefFoundError in the interpreter instead of an implicit NPE from compiled code.
       __ null_check(object.result(), new CodeEmitInfo(info), /* deoptimize */ needs_patching);
@@ -1681,11 +1680,11 @@
 
 void LIRGenerator::check_null_free_array(LIRItem& array, LIRItem& value, CodeEmitInfo* info) {
   LabelObj* L_end = new LabelObj();
   LIR_Opr tmp = new_register(T_METADATA);
   __ check_null_free_array(array.result(), tmp);
-  __ branch(lir_cond_equal, T_ILLEGAL, L_end->label());
+  __ branch(lir_cond_equal, L_end->label());
   __ null_check(value.result(), info);
   __ branch_destination(L_end->label());
 }
 
 bool LIRGenerator::needs_flattened_array_store_check(StoreIndexed* x) {
@@ -1752,11 +1751,11 @@
   }
 
   if (GenerateRangeChecks && needs_range_check) {
     if (use_length) {
       __ cmp(lir_cond_belowEqual, length.result(), index.result());
-      __ branch(lir_cond_belowEqual, T_INT, new RangeCheckStub(range_check_info, index.result(), array.result()));
+      __ branch(lir_cond_belowEqual, new RangeCheckStub(range_check_info, index.result(), array.result()));
     } else {
       array_range_check(array.result(), index.result(), null_check_info, range_check_info);
       // range_check also does the null check
       null_check_info = NULL;
     }
@@ -1946,11 +1945,11 @@
     assert(x->needs_patching(), "must be");
     assert(info != NULL, "must be");
     CodeStub* stub = new DeoptimizeStub(new CodeEmitInfo(info),
                                         Deoptimization::Reason_unloaded,
                                         Deoptimization::Action_make_not_entrant);
-    __ branch(lir_cond_always, T_ILLEGAL, stub);
+    __ branch(lir_cond_always, stub);
   } else if (need_default) {
     assert(!field_type_unloaded, "must be");
     assert(field->type()->is_valuetype(), "must be");
     ciValueKlass* value_klass = field->type()->as_value_klass();
     assert(value_klass->is_loaded(), "must be");
@@ -2039,11 +2038,11 @@
                  info ? new CodeEmitInfo(info) : NULL, info);
 
   if (default_value != NULL) {
     LabelObj* L_end = new LabelObj();
     __ cmp(lir_cond_notEqual, result, LIR_OprFact::oopConst(NULL));
-    __ branch(lir_cond_notEqual, T_OBJECT, L_end->label());
+    __ branch(lir_cond_notEqual, L_end->label());
     set_in_conditional_code(true);
     __ move(load_constant(default_value), result);
     __ branch_destination(L_end->label());
     set_in_conditional_code(false);
   }
@@ -2068,15 +2067,15 @@
     CodeEmitInfo* info = state_for(x);
     CodeStub* stub = new RangeCheckStub(info, index.result());
     LIR_Opr buf_obj = access_resolve(IS_NOT_NULL | ACCESS_READ, buf.result());
     if (index.result()->is_constant()) {
       cmp_mem_int(lir_cond_belowEqual, buf_obj, java_nio_Buffer::limit_offset(), index.result()->as_jint(), info);
-      __ branch(lir_cond_belowEqual, T_INT, stub);
+      __ branch(lir_cond_belowEqual, stub);
     } else {
       cmp_reg_mem(lir_cond_aboveEqual, index.result(), buf_obj,
                   java_nio_Buffer::limit_offset(), T_INT, info);
-      __ branch(lir_cond_aboveEqual, T_INT, stub);
+      __ branch(lir_cond_aboveEqual, stub);
     }
     __ move(index.result(), result);
   } else {
     // Just load the index into the result register
     __ move(index.result(), result);
@@ -2146,16 +2145,16 @@
     }
   }
 
   if (GenerateRangeChecks && needs_range_check) {
     if (StressLoopInvariantCodeMotion && range_check_info->deoptimize_on_exception()) {
-      __ branch(lir_cond_always, T_ILLEGAL, new RangeCheckStub(range_check_info, index.result(), array.result()));
+      __ branch(lir_cond_always, new RangeCheckStub(range_check_info, index.result(), array.result()));
     } else if (use_length) {
       // TODO: use a (modified) version of array_range_check that does not require a
       //       constant length to be loaded to a register
       __ cmp(lir_cond_belowEqual, length.result(), index.result());
-      __ branch(lir_cond_belowEqual, T_INT, new RangeCheckStub(range_check_info, index.result(), array.result()));
+      __ branch(lir_cond_belowEqual, new RangeCheckStub(range_check_info, index.result(), array.result()));
     } else {
       array_range_check(array.result(), index.result(), null_check_info, range_check_info);
       // The range check performs the null check, so clear it out for the load
       null_check_info = NULL;
     }
@@ -2221,22 +2220,22 @@
   // case. Let's just deoptimize.
   CodeEmitInfo* info = state_for(x, x->state_before());
   CodeStub* stub = new DeoptimizeStub(new CodeEmitInfo(info),
                                       Deoptimization::Reason_unloaded,
                                       Deoptimization::Action_make_not_entrant);
-  __ branch(lir_cond_always, T_ILLEGAL, stub);
+  __ branch(lir_cond_always, stub);
   LIR_Opr reg = rlock_result(x, T_OBJECT);
   __ move(LIR_OprFact::oopConst(NULL), reg);
 }
 
 void LIRGenerator::do_DefaultValue(DefaultValue* x) {
   // Same as withfield above. Let's deoptimize.
   CodeEmitInfo* info = state_for(x, x->state_before());
   CodeStub* stub = new DeoptimizeStub(new CodeEmitInfo(info),
                                       Deoptimization::Reason_unloaded,
                                       Deoptimization::Action_make_not_entrant);
-  __ branch(lir_cond_always, T_ILLEGAL, stub);
+  __ branch(lir_cond_always, stub);
   LIR_Opr reg = rlock_result(x, T_OBJECT);
   __ move(LIR_OprFact::oopConst(NULL), reg);
 }
 
 void LIRGenerator::do_NullCheck(NullCheck* x) {
@@ -2596,22 +2595,22 @@
     int low_key = one_range->low_key();
     int high_key = one_range->high_key();
     BlockBegin* dest = one_range->sux();
     if (low_key == high_key) {
       __ cmp(lir_cond_equal, value, low_key);
-      __ branch(lir_cond_equal, T_INT, dest);
+      __ branch(lir_cond_equal, dest);
     } else if (high_key - low_key == 1) {
       __ cmp(lir_cond_equal, value, low_key);
-      __ branch(lir_cond_equal, T_INT, dest);
+      __ branch(lir_cond_equal, dest);
       __ cmp(lir_cond_equal, value, high_key);
-      __ branch(lir_cond_equal, T_INT, dest);
+      __ branch(lir_cond_equal, dest);
     } else {
       LabelObj* L = new LabelObj();
       __ cmp(lir_cond_less, value, low_key);
-      __ branch(lir_cond_less, T_INT, L->label());
+      __ branch(lir_cond_less, L->label());
       __ cmp(lir_cond_lessEqual, value, high_key);
-      __ branch(lir_cond_lessEqual, T_INT, dest);
+      __ branch(lir_cond_lessEqual, dest);
       __ branch_destination(L->label());
     }
   }
   __ jump(default_sux);
 }
@@ -2727,11 +2726,11 @@
   if (UseTableRanges) {
     do_SwitchRanges(create_lookup_ranges(x), value, x->default_sux());
   } else {
     for (int i = 0; i < len; i++) {
       __ cmp(lir_cond_equal, value, i + lo_key);
-      __ branch(lir_cond_equal, T_INT, x->sux_at(i));
+      __ branch(lir_cond_equal, x->sux_at(i));
     }
     __ jump(x->default_sux());
   }
 }
 
@@ -2786,11 +2785,11 @@
     do_SwitchRanges(create_lookup_ranges(x), value, x->default_sux());
   } else {
     int len = x->length();
     for (int i = 0; i < len; i++) {
       __ cmp(lir_cond_equal, value, x->key_at(i));
-      __ branch(lir_cond_equal, T_INT, x->sux_at(i));
+      __ branch(lir_cond_equal, x->sux_at(i));
     }
     __ jump(x->default_sux());
   }
 }
 
@@ -3004,11 +3003,11 @@
   LIR_Opr tmp = new_register(T_METADATA);
   LIR_Opr mdp = new_register(T_METADATA);
   assert(md != NULL, "should have been initialized");
   __ metadata2reg(md->constant_encoding(), mdp);
   __ check_null_free_array(array.result(), tmp);
-  __ branch(lir_cond_equal, T_ILLEGAL, L_end->label());
+  __ branch(lir_cond_equal, L_end->label());
 
   profile_array_load_store_flags(md, load_store, ArrayLoadStoreData::null_free_array_byte_constant(), mdp);
 
   __ branch_destination(L_end->label());
 }
@@ -3121,11 +3120,11 @@
     // Check if deoptimization was triggered (i.e. orig_pc was set) while buffering scalarized value type arguments
     // in the entry point (see comments in frame::deoptimize). If so, deoptimize only now that we have the right state.
     CodeEmitInfo* info = new CodeEmitInfo(scope()->start()->state()->copy(ValueStack::StateBefore, 0), NULL, false);
     CodeStub* deopt_stub = new DeoptimizeStub(info, Deoptimization::Reason_none, Deoptimization::Action_none);
     __ append(new LIR_Op0(lir_check_orig_pc));
-    __ branch(lir_cond_notEqual, T_ADDRESS, deopt_stub);
+    __ branch(lir_cond_notEqual, deopt_stub);
   }
 
   // all blocks with a successor must end with an unconditional jump
   // to the successor even if they are consecutive
   __ jump(x->default_sux());
@@ -3428,11 +3427,11 @@
   assert(info != NULL, "must have info");
   LIRItem arg(x->argument_at(0), this);
 
   arg.load_item();
   LIR_Opr klass = new_register(T_METADATA);
-  __ move(new LIR_Address(arg.result(), java_lang_Class::klass_offset_in_bytes(), T_ADDRESS), klass, info);
+  __ move(new LIR_Address(arg.result(), java_lang_Class::klass_offset(), T_ADDRESS), klass, info);
   LIR_Opr id = new_register(T_LONG);
   ByteSize offset = KLASS_TRACE_ID_OFFSET;
   LIR_Address* trace_id_addr = new LIR_Address(klass, in_bytes(offset), T_LONG);
 
   __ move(trace_id_addr, id);
@@ -3450,20 +3449,22 @@
 }
 
 void LIRGenerator::do_getEventWriter(Intrinsic* x) {
   LabelObj* L_end = new LabelObj();
 
+  // FIXME T_ADDRESS should actually be T_METADATA but it can't because the
+  // meaning of these two is mixed up (see JDK-8026837).
   LIR_Address* jobj_addr = new LIR_Address(getThreadPointer(),
                                            in_bytes(THREAD_LOCAL_WRITER_OFFSET_JFR),
-                                           T_OBJECT);
+                                           T_ADDRESS);
   LIR_Opr result = rlock_result(x);
-  __ move_wide(jobj_addr, result);
-  __ cmp(lir_cond_equal, result, LIR_OprFact::oopConst(NULL));
-  __ branch(lir_cond_equal, T_OBJECT, L_end->label());
+  __ move(LIR_OprFact::oopConst(NULL), result);
+  LIR_Opr jobj = new_register(T_METADATA);
+  __ move_wide(jobj_addr, jobj);
+  __ cmp(lir_cond_equal, jobj, LIR_OprFact::metadataConst(0));
+  __ branch(lir_cond_equal, L_end->label());
 
-  LIR_Opr jobj = new_register(T_OBJECT);
-  __ move(result, jobj);
   access_load(IN_NATIVE, T_OBJECT, LIR_OprFact::address(new LIR_Address(jobj, T_OBJECT)), result);
 
   __ branch_destination(L_end->label());
 }
 
@@ -3817,11 +3818,11 @@
     __ store(result, counter);
     // DeoptimizeStub will reexecute from the current state in code info.
     CodeStub* deopt = new DeoptimizeStub(info, Deoptimization::Reason_tenured,
                                          Deoptimization::Action_make_not_entrant);
     __ cmp(lir_cond_lessEqual, result, LIR_OprFact::intConst(0));
-    __ branch(lir_cond_lessEqual, T_INT, deopt);
+    __ branch(lir_cond_lessEqual, deopt);
   }
 }
 
 
 void LIRGenerator::increment_event_counter_impl(CodeEmitInfo* info,
@@ -3864,24 +3865,24 @@
     CodeStub* overflow = new CounterOverflowStub(info, bci, meth);
     int freq = frequency << InvocationCounter::count_shift;
     if (freq == 0) {
       if (!step->is_constant()) {
         __ cmp(lir_cond_notEqual, step, LIR_OprFact::intConst(0));
-        __ branch(lir_cond_notEqual, T_ILLEGAL, overflow);
+        __ branch(lir_cond_notEqual, overflow);
       } else {
-        __ branch(lir_cond_always, T_ILLEGAL, overflow);
+        __ branch(lir_cond_always, overflow);
       }
     } else {
       LIR_Opr mask = load_immediate(freq, T_INT);
       if (!step->is_constant()) {
         // If step is 0, make sure the overflow check below always fails
         __ cmp(lir_cond_notEqual, step, LIR_OprFact::intConst(0));
         __ cmove(lir_cond_notEqual, result, LIR_OprFact::intConst(InvocationCounter::count_increment), result, T_INT);
       }
       __ logical_and(result, mask, result);
       __ cmp(lir_cond_equal, result, LIR_OprFact::intConst(0));
-      __ branch(lir_cond_equal, T_INT, overflow);
+      __ branch(lir_cond_equal, overflow);
     }
     __ branch_destination(overflow->continuation());
   }
 }
 
@@ -3991,11 +3992,11 @@
 
     CodeEmitInfo *info = state_for(x, x->state());
     CodeStub* stub = new PredicateFailedStub(info);
 
     __ cmp(lir_cond(cond), left, right);
-    __ branch(lir_cond(cond), right->type(), stub);
+    __ branch(lir_cond(cond), stub);
   }
 }
 
 
 LIR_Opr LIRGenerator::call_runtime(Value arg1, address entry, ValueType* result_type, CodeEmitInfo* info) {
