<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/cpu/aarch64/macroAssembler_aarch64.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="gc/shared/barrierSetAssembler_aarch64.hpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="macroAssembler_aarch64.hpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/cpu/aarch64/macroAssembler_aarch64.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
  29 #include &quot;jvm.h&quot;
  30 #include &quot;asm/assembler.hpp&quot;
  31 #include &quot;asm/assembler.inline.hpp&quot;
  32 #include &quot;gc/shared/barrierSet.hpp&quot;
  33 #include &quot;gc/shared/cardTable.hpp&quot;
  34 #include &quot;gc/shared/barrierSetAssembler.hpp&quot;
  35 #include &quot;gc/shared/cardTableBarrierSet.hpp&quot;
  36 #include &quot;interpreter/interpreter.hpp&quot;
  37 #include &quot;compiler/disassembler.hpp&quot;
  38 #include &quot;memory/resourceArea.hpp&quot;
  39 #include &quot;memory/universe.hpp&quot;
  40 #include &quot;nativeInst_aarch64.hpp&quot;
  41 #include &quot;oops/accessDecorators.hpp&quot;
  42 #include &quot;oops/compressedOops.inline.hpp&quot;
  43 #include &quot;oops/klass.inline.hpp&quot;
  44 #include &quot;runtime/biasedLocking.hpp&quot;
  45 #include &quot;runtime/icache.hpp&quot;
  46 #include &quot;runtime/interfaceSupport.inline.hpp&quot;
  47 #include &quot;runtime/jniHandles.inline.hpp&quot;
  48 #include &quot;runtime/sharedRuntime.hpp&quot;

  49 #include &quot;runtime/thread.hpp&quot;
  50 #include &quot;utilities/powerOfTwo.hpp&quot;
  51 #ifdef COMPILER1
  52 #include &quot;c1/c1_LIRAssembler.hpp&quot;
  53 #endif
  54 #ifdef COMPILER2
  55 #include &quot;oops/oop.hpp&quot;
  56 #include &quot;opto/compile.hpp&quot;
  57 #include &quot;opto/node.hpp&quot;
  58 #include &quot;opto/output.hpp&quot;
  59 #endif
  60 
  61 #ifdef PRODUCT
  62 #define BLOCK_COMMENT(str) /* nothing */
  63 #define STOP(error) stop(error)
  64 #else
  65 #define BLOCK_COMMENT(str) block_comment(str)
  66 #define STOP(error) block_comment(error); stop(error)
  67 #endif
  68 
</pre>
<hr />
<pre>
1299   ldrb(scratch, Address(klass, InstanceKlass::init_state_offset()));
1300   subs(zr, scratch, InstanceKlass::fully_initialized);
1301   br(Assembler::EQ, *L_fast_path);
1302 
1303   // Fast path check: current thread is initializer thread
1304   ldr(scratch, Address(klass, InstanceKlass::init_thread_offset()));
1305   cmp(rthread, scratch);
1306 
1307   if (L_slow_path == &amp;L_fallthrough) {
1308     br(Assembler::EQ, *L_fast_path);
1309     bind(*L_slow_path);
1310   } else if (L_fast_path == &amp;L_fallthrough) {
1311     br(Assembler::NE, *L_slow_path);
1312     bind(*L_fast_path);
1313   } else {
1314     Unimplemented();
1315   }
1316 }
1317 
1318 void MacroAssembler::verify_oop(Register reg, const char* s) {
<span class="line-modified">1319   if (!VerifyOops) return;</span>




1320 
1321   // Pass register number to verify_oop_subroutine
1322   const char* b = NULL;
1323   {
1324     ResourceMark rm;
1325     stringStream ss;
1326     ss.print(&quot;verify_oop: %s: %s&quot;, reg-&gt;name(), s);
1327     b = code_string(ss.as_string());
1328   }
1329   BLOCK_COMMENT(&quot;verify_oop {&quot;);
1330 
1331   stp(r0, rscratch1, Address(pre(sp, -2 * wordSize)));
1332   stp(rscratch2, lr, Address(pre(sp, -2 * wordSize)));
1333 
1334   mov(r0, reg);
1335   mov(rscratch1, (address)b);
1336 
1337   // call indirectly to solve generation ordering problem
1338   lea(rscratch2, ExternalAddress(StubRoutines::verify_oop_subroutine_entry_address()));
1339   ldr(rscratch2, Address(rscratch2));
1340   blr(rscratch2);
1341 
1342   ldp(rscratch2, lr, Address(post(sp, 2 * wordSize)));
1343   ldp(r0, rscratch1, Address(post(sp, 2 * wordSize)));
1344 
1345   BLOCK_COMMENT(&quot;} verify_oop&quot;);
1346 }
1347 
1348 void MacroAssembler::verify_oop_addr(Address addr, const char* s) {
<span class="line-modified">1349   if (!VerifyOops) return;</span>




1350 
1351   const char* b = NULL;
1352   {
1353     ResourceMark rm;
1354     stringStream ss;
1355     ss.print(&quot;verify_oop_addr: %s&quot;, s);
1356     b = code_string(ss.as_string());
1357   }
1358   BLOCK_COMMENT(&quot;verify_oop_addr {&quot;);
1359 
1360   stp(r0, rscratch1, Address(pre(sp, -2 * wordSize)));
1361   stp(rscratch2, lr, Address(pre(sp, -2 * wordSize)));
1362 
1363   // addr may contain sp so we will have to adjust it based on the
1364   // pushes that we just did.
1365   if (addr.uses(sp)) {
1366     lea(r0, addr);
1367     ldr(r0, Address(r0, 4 * wordSize));
1368   } else {
1369     ldr(r0, addr);
</pre>
<hr />
<pre>
1422 
1423 void MacroAssembler::call_VM_leaf(address entry_point, Register arg_0) {
1424   pass_arg0(this, arg_0);
1425   call_VM_leaf_base(entry_point, 1);
1426 }
1427 
1428 void MacroAssembler::call_VM_leaf(address entry_point, Register arg_0, Register arg_1) {
1429   pass_arg0(this, arg_0);
1430   pass_arg1(this, arg_1);
1431   call_VM_leaf_base(entry_point, 2);
1432 }
1433 
1434 void MacroAssembler::call_VM_leaf(address entry_point, Register arg_0,
1435                                   Register arg_1, Register arg_2) {
1436   pass_arg0(this, arg_0);
1437   pass_arg1(this, arg_1);
1438   pass_arg2(this, arg_2);
1439   call_VM_leaf_base(entry_point, 3);
1440 }
1441 




1442 void MacroAssembler::super_call_VM_leaf(address entry_point, Register arg_0) {
1443   pass_arg0(this, arg_0);
1444   MacroAssembler::call_VM_leaf_base(entry_point, 1);
1445 }
1446 
1447 void MacroAssembler::super_call_VM_leaf(address entry_point, Register arg_0, Register arg_1) {
1448 
1449   assert(arg_0 != c_rarg1, &quot;smashed arg&quot;);
1450   pass_arg1(this, arg_1);
1451   pass_arg0(this, arg_0);
1452   MacroAssembler::call_VM_leaf_base(entry_point, 2);
1453 }
1454 
1455 void MacroAssembler::super_call_VM_leaf(address entry_point, Register arg_0, Register arg_1, Register arg_2) {
1456   assert(arg_0 != c_rarg2, &quot;smashed arg&quot;);
1457   assert(arg_1 != c_rarg2, &quot;smashed arg&quot;);
1458   pass_arg2(this, arg_2);
1459   assert(arg_0 != c_rarg1, &quot;smashed arg&quot;);
1460   pass_arg1(this, arg_1);
1461   pass_arg0(this, arg_0);
</pre>
<hr />
<pre>
1471   assert(arg_1 != c_rarg2, &quot;smashed arg&quot;);
1472   pass_arg2(this, arg_2);
1473   assert(arg_0 != c_rarg1, &quot;smashed arg&quot;);
1474   pass_arg1(this, arg_1);
1475   pass_arg0(this, arg_0);
1476   MacroAssembler::call_VM_leaf_base(entry_point, 4);
1477 }
1478 
1479 void MacroAssembler::null_check(Register reg, int offset) {
1480   if (needs_explicit_null_check(offset)) {
1481     // provoke OS NULL exception if reg = NULL by
1482     // accessing M[reg] w/o changing any registers
1483     // NOTE: this is plenty to provoke a segv
1484     ldr(zr, Address(reg));
1485   } else {
1486     // nothing to do, (later) access of M[reg + offset]
1487     // will provoke OS NULL exception if reg = NULL
1488   }
1489 }
1490 

































1491 // MacroAssembler protected routines needed to implement
1492 // public methods
1493 
1494 void MacroAssembler::mov(Register r, Address dest) {
1495   code_section()-&gt;relocate(pc(), dest.rspec());
1496   u_int64_t imm64 = (u_int64_t)dest.target();
1497   movptr(r, imm64);
1498 }
1499 
1500 // Move a constant pointer into r.  In AArch64 mode the virtual
1501 // address space is 48 bits in size, so we only need three
1502 // instructions to create a patchable instruction sequence that can
1503 // reach anywhere.
1504 void MacroAssembler::movptr(Register r, uintptr_t imm64) {
1505 #ifndef PRODUCT
1506   {
1507     char buffer[64];
1508     snprintf(buffer, sizeof(buffer), &quot;0x%&quot; PRIX64, imm64);
1509     block_comment(buffer);
1510   }
</pre>
<hr />
<pre>
3687   ldr(rscratch1, Address(rscratch1, offset));
3688   cmp(src1, rscratch1);
3689 }
3690 
3691 void MacroAssembler::cmpoop(Register obj1, Register obj2) {
3692   BarrierSetAssembler* bs = BarrierSet::barrier_set()-&gt;barrier_set_assembler();
3693   bs-&gt;obj_equals(this, obj1, obj2);
3694 }
3695 
3696 void MacroAssembler::load_method_holder_cld(Register rresult, Register rmethod) {
3697   load_method_holder(rresult, rmethod);
3698   ldr(rresult, Address(rresult, InstanceKlass::class_loader_data_offset()));
3699 }
3700 
3701 void MacroAssembler::load_method_holder(Register holder, Register method) {
3702   ldr(holder, Address(method, Method::const_offset()));                      // ConstMethod*
3703   ldr(holder, Address(holder, ConstMethod::constants_offset()));             // ConstantPool*
3704   ldr(holder, Address(holder, ConstantPool::pool_holder_offset_in_bytes())); // InstanceKlass*
3705 }
3706 
<span class="line-modified">3707 void MacroAssembler::load_klass(Register dst, Register src) {</span>
3708   if (UseCompressedClassPointers) {
3709     ldrw(dst, Address(src, oopDesc::klass_offset_in_bytes()));
<span class="line-removed">3710     decode_klass_not_null(dst);</span>
3711   } else {
3712     ldr(dst, Address(src, oopDesc::klass_offset_in_bytes()));
3713   }
3714 }
3715 










3716 // ((OopHandle)result).resolve();
3717 void MacroAssembler::resolve_oop_handle(Register result, Register tmp) {
3718   // OopHandle::resolve is an indirection.
3719   access_load_at(T_OBJECT, IN_NATIVE, result, Address(result, 0), tmp, noreg);
3720 }
3721 
3722 // ((WeakHandle)result).resolve();
3723 void MacroAssembler::resolve_weak_handle(Register rresult, Register rtmp) {
3724   assert_different_registers(rresult, rtmp);
3725   Label resolved;
3726 
3727   // A null weak handle resolves to null.
3728   cbz(rresult, resolved);
3729 
3730   // Only 64 bit platforms support GCs that require a tmp register
3731   // Only IN_HEAP loads require a thread_tmp register
3732   // WeakHandle::resolve is an indirection like jweak.
3733   access_load_at(T_OBJECT, IN_NATIVE | ON_PHANTOM_OOP_REF,
3734                  rresult, Address(rresult), rtmp, /*tmp_thread*/noreg);
3735   bind(resolved);
3736 }
3737 
3738 void MacroAssembler::load_mirror(Register dst, Register method, Register tmp) {
3739   const int mirror_offset = in_bytes(Klass::java_mirror_offset());
3740   ldr(dst, Address(rmethod, Method::const_offset()));
3741   ldr(dst, Address(dst, ConstMethod::constants_offset()));
3742   ldr(dst, Address(dst, ConstantPool::pool_holder_offset_in_bytes()));
3743   ldr(dst, Address(dst, mirror_offset));
3744   resolve_oop_handle(dst, tmp);
3745 }
3746 









3747 void MacroAssembler::cmp_klass(Register oop, Register trial_klass, Register tmp) {
3748   if (UseCompressedClassPointers) {
3749     ldrw(tmp, Address(oop, oopDesc::klass_offset_in_bytes()));
3750     if (CompressedKlassPointers::base() == NULL) {
3751       cmp(trial_klass, tmp, LSL, CompressedKlassPointers::shift());
3752       return;
3753     } else if (((uint64_t)CompressedKlassPointers::base() &amp; 0xffffffff) == 0
3754                &amp;&amp; CompressedKlassPointers::shift() == 0) {
3755       // Only the bottom 32 bits matter
3756       cmpw(trial_klass, tmp);
3757       return;
3758     }
3759     decode_klass_not_null(tmp);
3760   } else {
3761     ldr(tmp, Address(oop, oopDesc::klass_offset_in_bytes()));
3762   }
3763   cmp(trial_klass, tmp);
3764 }
3765 
3766 void MacroAssembler::load_prototype_header(Register dst, Register src) {
</pre>
<hr />
<pre>
4064   narrowKlass nk = CompressedKlassPointers::encode(k);
4065   movz(dst, (nk &gt;&gt; 16), 16);
4066   movk(dst, nk &amp; 0xffff);
4067 }
4068 
4069 void MacroAssembler::access_load_at(BasicType type, DecoratorSet decorators,
4070                                     Register dst, Address src,
4071                                     Register tmp1, Register thread_tmp) {
4072   BarrierSetAssembler *bs = BarrierSet::barrier_set()-&gt;barrier_set_assembler();
4073   decorators = AccessInternal::decorator_fixup(decorators);
4074   bool as_raw = (decorators &amp; AS_RAW) != 0;
4075   if (as_raw) {
4076     bs-&gt;BarrierSetAssembler::load_at(this, decorators, type, dst, src, tmp1, thread_tmp);
4077   } else {
4078     bs-&gt;load_at(this, decorators, type, dst, src, tmp1, thread_tmp);
4079   }
4080 }
4081 
4082 void MacroAssembler::access_store_at(BasicType type, DecoratorSet decorators,
4083                                      Address dst, Register src,
<span class="line-modified">4084                                      Register tmp1, Register thread_tmp) {</span>

4085   BarrierSetAssembler *bs = BarrierSet::barrier_set()-&gt;barrier_set_assembler();
4086   decorators = AccessInternal::decorator_fixup(decorators);
4087   bool as_raw = (decorators &amp; AS_RAW) != 0;
4088   if (as_raw) {
<span class="line-modified">4089     bs-&gt;BarrierSetAssembler::store_at(this, decorators, type, dst, src, tmp1, thread_tmp);</span>
4090   } else {
<span class="line-modified">4091     bs-&gt;store_at(this, decorators, type, dst, src, tmp1, thread_tmp);</span>
4092   }
4093 }
4094 
4095 void MacroAssembler::resolve(DecoratorSet decorators, Register obj) {
4096   // Use stronger ACCESS_WRITE|ACCESS_READ by default.
4097   if ((decorators &amp; (ACCESS_READ | ACCESS_WRITE)) == 0) {
4098     decorators |= ACCESS_READ | ACCESS_WRITE;
4099   }
4100   BarrierSetAssembler* bs = BarrierSet::barrier_set()-&gt;barrier_set_assembler();
4101   return bs-&gt;resolve(this, decorators, obj);
4102 }
4103 
4104 void MacroAssembler::load_heap_oop(Register dst, Address src, Register tmp1,
4105                                    Register thread_tmp, DecoratorSet decorators) {
4106   access_load_at(T_OBJECT, IN_HEAP | decorators, dst, src, tmp1, thread_tmp);
4107 }
4108 
4109 void MacroAssembler::load_heap_oop_not_null(Register dst, Address src, Register tmp1,
4110                                             Register thread_tmp, DecoratorSet decorators) {
4111   access_load_at(T_OBJECT, IN_HEAP | IS_NOT_NULL | decorators, dst, src, tmp1, thread_tmp);
4112 }
4113 
4114 void MacroAssembler::store_heap_oop(Address dst, Register src, Register tmp1,
<span class="line-modified">4115                                     Register thread_tmp, DecoratorSet decorators) {</span>
<span class="line-modified">4116   access_store_at(T_OBJECT, IN_HEAP | decorators, dst, src, tmp1, thread_tmp);</span>
4117 }
4118 
4119 // Used for storing NULLs.
4120 void MacroAssembler::store_heap_oop_null(Address dst) {
<span class="line-modified">4121   access_store_at(T_OBJECT, IN_HEAP, dst, noreg, noreg, noreg);</span>
4122 }
4123 
4124 Address MacroAssembler::allocate_metadata_address(Metadata* obj) {
4125   assert(oop_recorder() != NULL, &quot;this assembler needs a Recorder&quot;);
4126   int index = oop_recorder()-&gt;allocate_metadata_index(obj);
4127   RelocationHolder rspec = metadata_Relocation::spec(index);
4128   return Address((address)obj, rspec);
4129 }
4130 
4131 // Move an oop into a register.  immediate is true if we want
4132 // immediate instructions and nmethod entry barriers are not enabled.
4133 // i.e. we are not going to patch this instruction while the code is being
4134 // executed by another thread.
4135 void MacroAssembler::movoop(Register dst, jobject obj, bool immediate) {
4136   int oop_index;
4137   if (obj == NULL) {
4138     oop_index = oop_recorder()-&gt;allocate_oop_index(obj);
4139   } else {
4140 #ifdef ASSERT
4141     {
</pre>
<hr />
<pre>
5174 // get_thread() can be called anywhere inside generated code so we
5175 // need to save whatever non-callee save context might get clobbered
5176 // by the call to JavaThread::aarch64_get_thread_helper() or, indeed,
5177 // the call setup code.
5178 //
5179 // aarch64_get_thread_helper() clobbers only r0, r1, and flags.
5180 //
5181 void MacroAssembler::get_thread(Register dst) {
5182   RegSet saved_regs = RegSet::range(r0, r1) + lr - dst;
5183   push(saved_regs, sp);
5184 
5185   mov(lr, CAST_FROM_FN_PTR(address, JavaThread::aarch64_get_thread_helper));
5186   blr(lr);
5187   if (dst != c_rarg0) {
5188     mov(dst, c_rarg0);
5189   }
5190 
5191   pop(saved_regs, sp);
5192 }
5193 






































































































































































































































































































































































































5194 void MacroAssembler::cache_wb(Address line) {
5195   assert(line.getMode() == Address::base_plus_offset, &quot;mode should be base_plus_offset&quot;);
5196   assert(line.index() == noreg, &quot;index should be noreg&quot;);
5197   assert(line.offset() == 0, &quot;offset should be 0&quot;);
5198   // would like to assert this
5199   // assert(line._ext.shift == 0, &quot;shift should be zero&quot;);
5200   if (VM_Version::supports_dcpop()) {
5201     // writeback using clear virtual address to point of persistence
5202     dc(Assembler::CVAP, line.base());
5203   } else {
5204     // no need to generate anything as Unsafe.writebackMemory should
5205     // never invoke this stub
5206   }
5207 }
5208 
5209 void MacroAssembler::cache_wbsync(bool is_pre) {
5210   // we only need a barrier post sync
5211   if (!is_pre) {
5212     membar(Assembler::AnyAny);
5213   }
</pre>
</td>
<td>
<hr />
<pre>
  29 #include &quot;jvm.h&quot;
  30 #include &quot;asm/assembler.hpp&quot;
  31 #include &quot;asm/assembler.inline.hpp&quot;
  32 #include &quot;gc/shared/barrierSet.hpp&quot;
  33 #include &quot;gc/shared/cardTable.hpp&quot;
  34 #include &quot;gc/shared/barrierSetAssembler.hpp&quot;
  35 #include &quot;gc/shared/cardTableBarrierSet.hpp&quot;
  36 #include &quot;interpreter/interpreter.hpp&quot;
  37 #include &quot;compiler/disassembler.hpp&quot;
  38 #include &quot;memory/resourceArea.hpp&quot;
  39 #include &quot;memory/universe.hpp&quot;
  40 #include &quot;nativeInst_aarch64.hpp&quot;
  41 #include &quot;oops/accessDecorators.hpp&quot;
  42 #include &quot;oops/compressedOops.inline.hpp&quot;
  43 #include &quot;oops/klass.inline.hpp&quot;
  44 #include &quot;runtime/biasedLocking.hpp&quot;
  45 #include &quot;runtime/icache.hpp&quot;
  46 #include &quot;runtime/interfaceSupport.inline.hpp&quot;
  47 #include &quot;runtime/jniHandles.inline.hpp&quot;
  48 #include &quot;runtime/sharedRuntime.hpp&quot;
<span class="line-added">  49 #include &quot;runtime/signature_cc.hpp&quot;</span>
  50 #include &quot;runtime/thread.hpp&quot;
  51 #include &quot;utilities/powerOfTwo.hpp&quot;
  52 #ifdef COMPILER1
  53 #include &quot;c1/c1_LIRAssembler.hpp&quot;
  54 #endif
  55 #ifdef COMPILER2
  56 #include &quot;oops/oop.hpp&quot;
  57 #include &quot;opto/compile.hpp&quot;
  58 #include &quot;opto/node.hpp&quot;
  59 #include &quot;opto/output.hpp&quot;
  60 #endif
  61 
  62 #ifdef PRODUCT
  63 #define BLOCK_COMMENT(str) /* nothing */
  64 #define STOP(error) stop(error)
  65 #else
  66 #define BLOCK_COMMENT(str) block_comment(str)
  67 #define STOP(error) block_comment(error); stop(error)
  68 #endif
  69 
</pre>
<hr />
<pre>
1300   ldrb(scratch, Address(klass, InstanceKlass::init_state_offset()));
1301   subs(zr, scratch, InstanceKlass::fully_initialized);
1302   br(Assembler::EQ, *L_fast_path);
1303 
1304   // Fast path check: current thread is initializer thread
1305   ldr(scratch, Address(klass, InstanceKlass::init_thread_offset()));
1306   cmp(rthread, scratch);
1307 
1308   if (L_slow_path == &amp;L_fallthrough) {
1309     br(Assembler::EQ, *L_fast_path);
1310     bind(*L_slow_path);
1311   } else if (L_fast_path == &amp;L_fallthrough) {
1312     br(Assembler::NE, *L_slow_path);
1313     bind(*L_fast_path);
1314   } else {
1315     Unimplemented();
1316   }
1317 }
1318 
1319 void MacroAssembler::verify_oop(Register reg, const char* s) {
<span class="line-modified">1320   if (!VerifyOops || VerifyAdapterSharing) {</span>
<span class="line-added">1321     // Below address of the code string confuses VerifyAdapterSharing</span>
<span class="line-added">1322     // because it may differ between otherwise equivalent adapters.</span>
<span class="line-added">1323     return;</span>
<span class="line-added">1324   }</span>
1325 
1326   // Pass register number to verify_oop_subroutine
1327   const char* b = NULL;
1328   {
1329     ResourceMark rm;
1330     stringStream ss;
1331     ss.print(&quot;verify_oop: %s: %s&quot;, reg-&gt;name(), s);
1332     b = code_string(ss.as_string());
1333   }
1334   BLOCK_COMMENT(&quot;verify_oop {&quot;);
1335 
1336   stp(r0, rscratch1, Address(pre(sp, -2 * wordSize)));
1337   stp(rscratch2, lr, Address(pre(sp, -2 * wordSize)));
1338 
1339   mov(r0, reg);
1340   mov(rscratch1, (address)b);
1341 
1342   // call indirectly to solve generation ordering problem
1343   lea(rscratch2, ExternalAddress(StubRoutines::verify_oop_subroutine_entry_address()));
1344   ldr(rscratch2, Address(rscratch2));
1345   blr(rscratch2);
1346 
1347   ldp(rscratch2, lr, Address(post(sp, 2 * wordSize)));
1348   ldp(r0, rscratch1, Address(post(sp, 2 * wordSize)));
1349 
1350   BLOCK_COMMENT(&quot;} verify_oop&quot;);
1351 }
1352 
1353 void MacroAssembler::verify_oop_addr(Address addr, const char* s) {
<span class="line-modified">1354   if (!VerifyOops || VerifyAdapterSharing) {</span>
<span class="line-added">1355     // Below address of the code string confuses VerifyAdapterSharing</span>
<span class="line-added">1356     // because it may differ between otherwise equivalent adapters.</span>
<span class="line-added">1357     return;</span>
<span class="line-added">1358   }</span>
1359 
1360   const char* b = NULL;
1361   {
1362     ResourceMark rm;
1363     stringStream ss;
1364     ss.print(&quot;verify_oop_addr: %s&quot;, s);
1365     b = code_string(ss.as_string());
1366   }
1367   BLOCK_COMMENT(&quot;verify_oop_addr {&quot;);
1368 
1369   stp(r0, rscratch1, Address(pre(sp, -2 * wordSize)));
1370   stp(rscratch2, lr, Address(pre(sp, -2 * wordSize)));
1371 
1372   // addr may contain sp so we will have to adjust it based on the
1373   // pushes that we just did.
1374   if (addr.uses(sp)) {
1375     lea(r0, addr);
1376     ldr(r0, Address(r0, 4 * wordSize));
1377   } else {
1378     ldr(r0, addr);
</pre>
<hr />
<pre>
1431 
1432 void MacroAssembler::call_VM_leaf(address entry_point, Register arg_0) {
1433   pass_arg0(this, arg_0);
1434   call_VM_leaf_base(entry_point, 1);
1435 }
1436 
1437 void MacroAssembler::call_VM_leaf(address entry_point, Register arg_0, Register arg_1) {
1438   pass_arg0(this, arg_0);
1439   pass_arg1(this, arg_1);
1440   call_VM_leaf_base(entry_point, 2);
1441 }
1442 
1443 void MacroAssembler::call_VM_leaf(address entry_point, Register arg_0,
1444                                   Register arg_1, Register arg_2) {
1445   pass_arg0(this, arg_0);
1446   pass_arg1(this, arg_1);
1447   pass_arg2(this, arg_2);
1448   call_VM_leaf_base(entry_point, 3);
1449 }
1450 
<span class="line-added">1451 void MacroAssembler::super_call_VM_leaf(address entry_point) {</span>
<span class="line-added">1452   MacroAssembler::call_VM_leaf_base(entry_point, 1);</span>
<span class="line-added">1453 }</span>
<span class="line-added">1454 </span>
1455 void MacroAssembler::super_call_VM_leaf(address entry_point, Register arg_0) {
1456   pass_arg0(this, arg_0);
1457   MacroAssembler::call_VM_leaf_base(entry_point, 1);
1458 }
1459 
1460 void MacroAssembler::super_call_VM_leaf(address entry_point, Register arg_0, Register arg_1) {
1461 
1462   assert(arg_0 != c_rarg1, &quot;smashed arg&quot;);
1463   pass_arg1(this, arg_1);
1464   pass_arg0(this, arg_0);
1465   MacroAssembler::call_VM_leaf_base(entry_point, 2);
1466 }
1467 
1468 void MacroAssembler::super_call_VM_leaf(address entry_point, Register arg_0, Register arg_1, Register arg_2) {
1469   assert(arg_0 != c_rarg2, &quot;smashed arg&quot;);
1470   assert(arg_1 != c_rarg2, &quot;smashed arg&quot;);
1471   pass_arg2(this, arg_2);
1472   assert(arg_0 != c_rarg1, &quot;smashed arg&quot;);
1473   pass_arg1(this, arg_1);
1474   pass_arg0(this, arg_0);
</pre>
<hr />
<pre>
1484   assert(arg_1 != c_rarg2, &quot;smashed arg&quot;);
1485   pass_arg2(this, arg_2);
1486   assert(arg_0 != c_rarg1, &quot;smashed arg&quot;);
1487   pass_arg1(this, arg_1);
1488   pass_arg0(this, arg_0);
1489   MacroAssembler::call_VM_leaf_base(entry_point, 4);
1490 }
1491 
1492 void MacroAssembler::null_check(Register reg, int offset) {
1493   if (needs_explicit_null_check(offset)) {
1494     // provoke OS NULL exception if reg = NULL by
1495     // accessing M[reg] w/o changing any registers
1496     // NOTE: this is plenty to provoke a segv
1497     ldr(zr, Address(reg));
1498   } else {
1499     // nothing to do, (later) access of M[reg + offset]
1500     // will provoke OS NULL exception if reg = NULL
1501   }
1502 }
1503 
<span class="line-added">1504 void MacroAssembler::test_klass_is_value(Register klass, Register temp_reg, Label&amp; is_value) {</span>
<span class="line-added">1505   ldrw(temp_reg, Address(klass, Klass::access_flags_offset()));</span>
<span class="line-added">1506   andr(temp_reg, temp_reg, JVM_ACC_VALUE);</span>
<span class="line-added">1507   cbnz(temp_reg, is_value);</span>
<span class="line-added">1508 }</span>
<span class="line-added">1509 </span>
<span class="line-added">1510 void MacroAssembler::test_field_is_flattenable(Register flags, Register temp_reg, Label&amp; is_flattenable) {</span>
<span class="line-added">1511   (void) temp_reg; // keep signature uniform with x86</span>
<span class="line-added">1512   tbnz(flags, ConstantPoolCacheEntry::is_flattenable_field_shift, is_flattenable);</span>
<span class="line-added">1513 }</span>
<span class="line-added">1514 </span>
<span class="line-added">1515 void MacroAssembler::test_field_is_not_flattenable(Register flags, Register temp_reg, Label&amp; not_flattenable) {</span>
<span class="line-added">1516   (void) temp_reg; // keep signature uniform with x86</span>
<span class="line-added">1517   tbz(flags, ConstantPoolCacheEntry::is_flattenable_field_shift, not_flattenable);</span>
<span class="line-added">1518 }</span>
<span class="line-added">1519 </span>
<span class="line-added">1520 void MacroAssembler::test_field_is_flattened(Register flags, Register temp_reg, Label&amp; is_flattened) {</span>
<span class="line-added">1521   (void) temp_reg; // keep signature uniform with x86</span>
<span class="line-added">1522   tbnz(flags, ConstantPoolCacheEntry::is_flattened_field_shift, is_flattened);</span>
<span class="line-added">1523 }</span>
<span class="line-added">1524 </span>
<span class="line-added">1525 void MacroAssembler::test_flattened_array_oop(Register oop, Register temp_reg, Label&amp; is_flattened_array) {</span>
<span class="line-added">1526   load_storage_props(temp_reg, oop);</span>
<span class="line-added">1527   andr(temp_reg, temp_reg, ArrayStorageProperties::flattened_value);</span>
<span class="line-added">1528   cbnz(temp_reg, is_flattened_array);</span>
<span class="line-added">1529 }</span>
<span class="line-added">1530 </span>
<span class="line-added">1531 void MacroAssembler::test_null_free_array_oop(Register oop, Register temp_reg, Label&amp; is_null_free_array) {</span>
<span class="line-added">1532   load_storage_props(temp_reg, oop);</span>
<span class="line-added">1533   andr(temp_reg, temp_reg, ArrayStorageProperties::null_free_value);</span>
<span class="line-added">1534   cbnz(temp_reg, is_null_free_array);</span>
<span class="line-added">1535 }</span>
<span class="line-added">1536 </span>
1537 // MacroAssembler protected routines needed to implement
1538 // public methods
1539 
1540 void MacroAssembler::mov(Register r, Address dest) {
1541   code_section()-&gt;relocate(pc(), dest.rspec());
1542   u_int64_t imm64 = (u_int64_t)dest.target();
1543   movptr(r, imm64);
1544 }
1545 
1546 // Move a constant pointer into r.  In AArch64 mode the virtual
1547 // address space is 48 bits in size, so we only need three
1548 // instructions to create a patchable instruction sequence that can
1549 // reach anywhere.
1550 void MacroAssembler::movptr(Register r, uintptr_t imm64) {
1551 #ifndef PRODUCT
1552   {
1553     char buffer[64];
1554     snprintf(buffer, sizeof(buffer), &quot;0x%&quot; PRIX64, imm64);
1555     block_comment(buffer);
1556   }
</pre>
<hr />
<pre>
3733   ldr(rscratch1, Address(rscratch1, offset));
3734   cmp(src1, rscratch1);
3735 }
3736 
3737 void MacroAssembler::cmpoop(Register obj1, Register obj2) {
3738   BarrierSetAssembler* bs = BarrierSet::barrier_set()-&gt;barrier_set_assembler();
3739   bs-&gt;obj_equals(this, obj1, obj2);
3740 }
3741 
3742 void MacroAssembler::load_method_holder_cld(Register rresult, Register rmethod) {
3743   load_method_holder(rresult, rmethod);
3744   ldr(rresult, Address(rresult, InstanceKlass::class_loader_data_offset()));
3745 }
3746 
3747 void MacroAssembler::load_method_holder(Register holder, Register method) {
3748   ldr(holder, Address(method, Method::const_offset()));                      // ConstMethod*
3749   ldr(holder, Address(holder, ConstMethod::constants_offset()));             // ConstantPool*
3750   ldr(holder, Address(holder, ConstantPool::pool_holder_offset_in_bytes())); // InstanceKlass*
3751 }
3752 
<span class="line-modified">3753 void MacroAssembler::load_metadata(Register dst, Register src) {</span>
3754   if (UseCompressedClassPointers) {
3755     ldrw(dst, Address(src, oopDesc::klass_offset_in_bytes()));

3756   } else {
3757     ldr(dst, Address(src, oopDesc::klass_offset_in_bytes()));
3758   }
3759 }
3760 
<span class="line-added">3761 void MacroAssembler::load_klass(Register dst, Register src) {</span>
<span class="line-added">3762   load_metadata(dst, src);</span>
<span class="line-added">3763   if (UseCompressedClassPointers) {</span>
<span class="line-added">3764     andr(dst, dst, oopDesc::compressed_klass_mask());</span>
<span class="line-added">3765     decode_klass_not_null(dst);</span>
<span class="line-added">3766   } else {</span>
<span class="line-added">3767     ubfm(dst, dst, 0, 63 - oopDesc::storage_props_nof_bits);</span>
<span class="line-added">3768   }</span>
<span class="line-added">3769 }</span>
<span class="line-added">3770 </span>
3771 // ((OopHandle)result).resolve();
3772 void MacroAssembler::resolve_oop_handle(Register result, Register tmp) {
3773   // OopHandle::resolve is an indirection.
3774   access_load_at(T_OBJECT, IN_NATIVE, result, Address(result, 0), tmp, noreg);
3775 }
3776 
3777 // ((WeakHandle)result).resolve();
3778 void MacroAssembler::resolve_weak_handle(Register rresult, Register rtmp) {
3779   assert_different_registers(rresult, rtmp);
3780   Label resolved;
3781 
3782   // A null weak handle resolves to null.
3783   cbz(rresult, resolved);
3784 
3785   // Only 64 bit platforms support GCs that require a tmp register
3786   // Only IN_HEAP loads require a thread_tmp register
3787   // WeakHandle::resolve is an indirection like jweak.
3788   access_load_at(T_OBJECT, IN_NATIVE | ON_PHANTOM_OOP_REF,
3789                  rresult, Address(rresult), rtmp, /*tmp_thread*/noreg);
3790   bind(resolved);
3791 }
3792 
3793 void MacroAssembler::load_mirror(Register dst, Register method, Register tmp) {
3794   const int mirror_offset = in_bytes(Klass::java_mirror_offset());
3795   ldr(dst, Address(rmethod, Method::const_offset()));
3796   ldr(dst, Address(dst, ConstMethod::constants_offset()));
3797   ldr(dst, Address(dst, ConstantPool::pool_holder_offset_in_bytes()));
3798   ldr(dst, Address(dst, mirror_offset));
3799   resolve_oop_handle(dst, tmp);
3800 }
3801 
<span class="line-added">3802 void MacroAssembler::load_storage_props(Register dst, Register src) {</span>
<span class="line-added">3803   load_metadata(dst, src);</span>
<span class="line-added">3804   if (UseCompressedClassPointers) {</span>
<span class="line-added">3805     asrw(dst, dst, oopDesc::narrow_storage_props_shift);</span>
<span class="line-added">3806   } else {</span>
<span class="line-added">3807     asr(dst, dst, oopDesc::wide_storage_props_shift);</span>
<span class="line-added">3808   }</span>
<span class="line-added">3809 }</span>
<span class="line-added">3810 </span>
3811 void MacroAssembler::cmp_klass(Register oop, Register trial_klass, Register tmp) {
3812   if (UseCompressedClassPointers) {
3813     ldrw(tmp, Address(oop, oopDesc::klass_offset_in_bytes()));
3814     if (CompressedKlassPointers::base() == NULL) {
3815       cmp(trial_klass, tmp, LSL, CompressedKlassPointers::shift());
3816       return;
3817     } else if (((uint64_t)CompressedKlassPointers::base() &amp; 0xffffffff) == 0
3818                &amp;&amp; CompressedKlassPointers::shift() == 0) {
3819       // Only the bottom 32 bits matter
3820       cmpw(trial_klass, tmp);
3821       return;
3822     }
3823     decode_klass_not_null(tmp);
3824   } else {
3825     ldr(tmp, Address(oop, oopDesc::klass_offset_in_bytes()));
3826   }
3827   cmp(trial_klass, tmp);
3828 }
3829 
3830 void MacroAssembler::load_prototype_header(Register dst, Register src) {
</pre>
<hr />
<pre>
4128   narrowKlass nk = CompressedKlassPointers::encode(k);
4129   movz(dst, (nk &gt;&gt; 16), 16);
4130   movk(dst, nk &amp; 0xffff);
4131 }
4132 
4133 void MacroAssembler::access_load_at(BasicType type, DecoratorSet decorators,
4134                                     Register dst, Address src,
4135                                     Register tmp1, Register thread_tmp) {
4136   BarrierSetAssembler *bs = BarrierSet::barrier_set()-&gt;barrier_set_assembler();
4137   decorators = AccessInternal::decorator_fixup(decorators);
4138   bool as_raw = (decorators &amp; AS_RAW) != 0;
4139   if (as_raw) {
4140     bs-&gt;BarrierSetAssembler::load_at(this, decorators, type, dst, src, tmp1, thread_tmp);
4141   } else {
4142     bs-&gt;load_at(this, decorators, type, dst, src, tmp1, thread_tmp);
4143   }
4144 }
4145 
4146 void MacroAssembler::access_store_at(BasicType type, DecoratorSet decorators,
4147                                      Address dst, Register src,
<span class="line-modified">4148                                      Register tmp1, Register thread_tmp, Register tmp3) {</span>
<span class="line-added">4149 </span>
4150   BarrierSetAssembler *bs = BarrierSet::barrier_set()-&gt;barrier_set_assembler();
4151   decorators = AccessInternal::decorator_fixup(decorators);
4152   bool as_raw = (decorators &amp; AS_RAW) != 0;
4153   if (as_raw) {
<span class="line-modified">4154     bs-&gt;BarrierSetAssembler::store_at(this, decorators, type, dst, src, tmp1, thread_tmp, tmp3);</span>
4155   } else {
<span class="line-modified">4156     bs-&gt;store_at(this, decorators, type, dst, src, tmp1, thread_tmp, tmp3);</span>
4157   }
4158 }
4159 
4160 void MacroAssembler::resolve(DecoratorSet decorators, Register obj) {
4161   // Use stronger ACCESS_WRITE|ACCESS_READ by default.
4162   if ((decorators &amp; (ACCESS_READ | ACCESS_WRITE)) == 0) {
4163     decorators |= ACCESS_READ | ACCESS_WRITE;
4164   }
4165   BarrierSetAssembler* bs = BarrierSet::barrier_set()-&gt;barrier_set_assembler();
4166   return bs-&gt;resolve(this, decorators, obj);
4167 }
4168 
4169 void MacroAssembler::load_heap_oop(Register dst, Address src, Register tmp1,
4170                                    Register thread_tmp, DecoratorSet decorators) {
4171   access_load_at(T_OBJECT, IN_HEAP | decorators, dst, src, tmp1, thread_tmp);
4172 }
4173 
4174 void MacroAssembler::load_heap_oop_not_null(Register dst, Address src, Register tmp1,
4175                                             Register thread_tmp, DecoratorSet decorators) {
4176   access_load_at(T_OBJECT, IN_HEAP | IS_NOT_NULL | decorators, dst, src, tmp1, thread_tmp);
4177 }
4178 
4179 void MacroAssembler::store_heap_oop(Address dst, Register src, Register tmp1,
<span class="line-modified">4180                                     Register thread_tmp, Register tmp3, DecoratorSet decorators) {</span>
<span class="line-modified">4181   access_store_at(T_OBJECT, IN_HEAP | decorators, dst, src, tmp1, thread_tmp, tmp3);</span>
4182 }
4183 
4184 // Used for storing NULLs.
4185 void MacroAssembler::store_heap_oop_null(Address dst) {
<span class="line-modified">4186   access_store_at(T_OBJECT, IN_HEAP, dst, noreg, noreg, noreg, noreg);</span>
4187 }
4188 
4189 Address MacroAssembler::allocate_metadata_address(Metadata* obj) {
4190   assert(oop_recorder() != NULL, &quot;this assembler needs a Recorder&quot;);
4191   int index = oop_recorder()-&gt;allocate_metadata_index(obj);
4192   RelocationHolder rspec = metadata_Relocation::spec(index);
4193   return Address((address)obj, rspec);
4194 }
4195 
4196 // Move an oop into a register.  immediate is true if we want
4197 // immediate instructions and nmethod entry barriers are not enabled.
4198 // i.e. we are not going to patch this instruction while the code is being
4199 // executed by another thread.
4200 void MacroAssembler::movoop(Register dst, jobject obj, bool immediate) {
4201   int oop_index;
4202   if (obj == NULL) {
4203     oop_index = oop_recorder()-&gt;allocate_oop_index(obj);
4204   } else {
4205 #ifdef ASSERT
4206     {
</pre>
<hr />
<pre>
5239 // get_thread() can be called anywhere inside generated code so we
5240 // need to save whatever non-callee save context might get clobbered
5241 // by the call to JavaThread::aarch64_get_thread_helper() or, indeed,
5242 // the call setup code.
5243 //
5244 // aarch64_get_thread_helper() clobbers only r0, r1, and flags.
5245 //
5246 void MacroAssembler::get_thread(Register dst) {
5247   RegSet saved_regs = RegSet::range(r0, r1) + lr - dst;
5248   push(saved_regs, sp);
5249 
5250   mov(lr, CAST_FROM_FN_PTR(address, JavaThread::aarch64_get_thread_helper));
5251   blr(lr);
5252   if (dst != c_rarg0) {
5253     mov(dst, c_rarg0);
5254   }
5255 
5256   pop(saved_regs, sp);
5257 }
5258 
<span class="line-added">5259 // C2 compiled method&#39;s prolog code</span>
<span class="line-added">5260 // Moved here from aarch64.ad to support Valhalla code belows</span>
<span class="line-added">5261 void MacroAssembler::verified_entry(Compile* C, int sp_inc) {</span>
<span class="line-added">5262 </span>
<span class="line-added">5263 // n.b. frame size includes space for return pc and rfp</span>
<span class="line-added">5264   const long framesize = C-&gt;frame_size_in_bytes();</span>
<span class="line-added">5265   assert(framesize % (2 * wordSize) == 0, &quot;must preserve 2 * wordSize alignment&quot;);</span>
<span class="line-added">5266 </span>
<span class="line-added">5267   // insert a nop at the start of the prolog so we can patch in a</span>
<span class="line-added">5268   // branch if we need to invalidate the method later</span>
<span class="line-added">5269   nop();</span>
<span class="line-added">5270 </span>
<span class="line-added">5271   int bangsize = C-&gt;bang_size_in_bytes();</span>
<span class="line-added">5272   if (C-&gt;need_stack_bang(bangsize) &amp;&amp; UseStackBanging)</span>
<span class="line-added">5273      generate_stack_overflow_check(bangsize);</span>
<span class="line-added">5274 </span>
<span class="line-added">5275   build_frame(framesize);</span>
<span class="line-added">5276 </span>
<span class="line-added">5277   if (VerifyStackAtCalls) {</span>
<span class="line-added">5278     Unimplemented();</span>
<span class="line-added">5279   }</span>
<span class="line-added">5280 }</span>
<span class="line-added">5281 </span>
<span class="line-added">5282 int MacroAssembler::store_value_type_fields_to_buf(ciValueKlass* vk, bool from_interpreter) {</span>
<span class="line-added">5283   // A value type might be returned. If fields are in registers we</span>
<span class="line-added">5284   // need to allocate a value type instance and initialize it with</span>
<span class="line-added">5285   // the value of the fields.</span>
<span class="line-added">5286   Label skip;</span>
<span class="line-added">5287   // We only need a new buffered value if a new one is not returned</span>
<span class="line-added">5288   cmp(r0, (u1) 1);</span>
<span class="line-added">5289   br(Assembler::EQ, skip);</span>
<span class="line-added">5290   int call_offset = -1;</span>
<span class="line-added">5291 </span>
<span class="line-added">5292   Label slow_case;</span>
<span class="line-added">5293 </span>
<span class="line-added">5294   // Try to allocate a new buffered value (from the heap)</span>
<span class="line-added">5295   if (UseTLAB) {</span>
<span class="line-added">5296 </span>
<span class="line-added">5297     if (vk != NULL) {</span>
<span class="line-added">5298       // Called from C1, where the return type is statically known.</span>
<span class="line-added">5299       mov(r1, (intptr_t)vk-&gt;get_ValueKlass());</span>
<span class="line-added">5300       jint lh = vk-&gt;layout_helper();</span>
<span class="line-added">5301       assert(lh != Klass::_lh_neutral_value, &quot;inline class in return type must have been resolved&quot;);</span>
<span class="line-added">5302       mov(r14, lh);</span>
<span class="line-added">5303     } else {</span>
<span class="line-added">5304        // Call from interpreter. R0 contains ((the ValueKlass* of the return type) | 0x01)</span>
<span class="line-added">5305        andr(r1, r0, -2);</span>
<span class="line-added">5306        // get obj size</span>
<span class="line-added">5307        ldrw(r14, Address(rscratch1 /*klass*/, Klass::layout_helper_offset()));</span>
<span class="line-added">5308     }</span>
<span class="line-added">5309 </span>
<span class="line-added">5310      ldr(r13, Address(rthread, in_bytes(JavaThread::tlab_top_offset())));</span>
<span class="line-added">5311 </span>
<span class="line-added">5312      // check whether we have space in TLAB,</span>
<span class="line-added">5313      // rscratch1 contains pointer to just allocated obj</span>
<span class="line-added">5314       lea(r14, Address(r13, r14));</span>
<span class="line-added">5315       ldr(rscratch1, Address(rthread, in_bytes(JavaThread::tlab_end_offset())));</span>
<span class="line-added">5316 </span>
<span class="line-added">5317       cmp(r14, rscratch1);</span>
<span class="line-added">5318       br(Assembler::GT, slow_case);</span>
<span class="line-added">5319 </span>
<span class="line-added">5320       // OK we have room in TLAB,</span>
<span class="line-added">5321       // Set new TLAB top</span>
<span class="line-added">5322       str(r14, Address(rthread, in_bytes(JavaThread::tlab_top_offset())));</span>
<span class="line-added">5323 </span>
<span class="line-added">5324       // Set new class always locked</span>
<span class="line-added">5325       mov(rscratch1, (uint64_t) markWord::always_locked_prototype().value());</span>
<span class="line-added">5326       str(rscratch1, Address(r13, oopDesc::mark_offset_in_bytes()));</span>
<span class="line-added">5327 </span>
<span class="line-added">5328       store_klass_gap(r13, zr);  // zero klass gap for compressed oops</span>
<span class="line-added">5329       if (vk == NULL) {</span>
<span class="line-added">5330         // store_klass corrupts rbx, so save it in rax for later use (interpreter case only).</span>
<span class="line-added">5331          mov(r0, r1);</span>
<span class="line-added">5332       }</span>
<span class="line-added">5333 </span>
<span class="line-added">5334       store_klass(r13, r1);  // klass</span>
<span class="line-added">5335 </span>
<span class="line-added">5336       if (vk != NULL) {</span>
<span class="line-added">5337         // FIXME -- do the packing in-line to avoid the runtime call</span>
<span class="line-added">5338         mov(r0, r13);</span>
<span class="line-added">5339         far_call(RuntimeAddress(vk-&gt;pack_handler())); // no need for call info as this will not safepoint.</span>
<span class="line-added">5340       } else {</span>
<span class="line-added">5341 </span>
<span class="line-added">5342         // We have our new buffered value, initialize its fields with a</span>
<span class="line-added">5343         // value class specific handler</span>
<span class="line-added">5344         ldr(r1, Address(r0, InstanceKlass::adr_valueklass_fixed_block_offset()));</span>
<span class="line-added">5345         ldr(r1, Address(r1, ValueKlass::pack_handler_offset()));</span>
<span class="line-added">5346 </span>
<span class="line-added">5347         // Mov new class to r0 and call pack_handler</span>
<span class="line-added">5348         mov(r0, r13);</span>
<span class="line-added">5349         blr(r1);</span>
<span class="line-added">5350       }</span>
<span class="line-added">5351       b(skip);</span>
<span class="line-added">5352   }</span>
<span class="line-added">5353 </span>
<span class="line-added">5354   bind(slow_case);</span>
<span class="line-added">5355   // We failed to allocate a new value, fall back to a runtime</span>
<span class="line-added">5356   // call. Some oop field may be live in some registers but we can&#39;t</span>
<span class="line-added">5357   // tell. That runtime call will take care of preserving them</span>
<span class="line-added">5358   // across a GC if there&#39;s one.</span>
<span class="line-added">5359 </span>
<span class="line-added">5360 </span>
<span class="line-added">5361   if (from_interpreter) {</span>
<span class="line-added">5362     super_call_VM_leaf(StubRoutines::store_value_type_fields_to_buf());</span>
<span class="line-added">5363   } else {</span>
<span class="line-added">5364     ldr(rscratch1, RuntimeAddress(StubRoutines::store_value_type_fields_to_buf()));</span>
<span class="line-added">5365     blr(rscratch1);</span>
<span class="line-added">5366     call_offset = offset();</span>
<span class="line-added">5367   }</span>
<span class="line-added">5368 </span>
<span class="line-added">5369   bind(skip);</span>
<span class="line-added">5370   return call_offset;</span>
<span class="line-added">5371 }</span>
<span class="line-added">5372 </span>
<span class="line-added">5373 // Move a value between registers/stack slots and update the reg_state</span>
<span class="line-added">5374 bool MacroAssembler::move_helper(VMReg from, VMReg to, BasicType bt, RegState reg_state[], int ret_off, int extra_stack_offset) {</span>
<span class="line-added">5375   if (reg_state[to-&gt;value()] == reg_written) {</span>
<span class="line-added">5376     return true; // Already written</span>
<span class="line-added">5377   }</span>
<span class="line-added">5378 </span>
<span class="line-added">5379   if (from != to &amp;&amp; bt != T_VOID) {</span>
<span class="line-added">5380     if (reg_state[to-&gt;value()] == reg_readonly) {</span>
<span class="line-added">5381       return false; // Not yet writable</span>
<span class="line-added">5382     }</span>
<span class="line-added">5383     if (from-&gt;is_reg()) {</span>
<span class="line-added">5384       if (to-&gt;is_reg()) {</span>
<span class="line-added">5385         mov(to-&gt;as_Register(), from-&gt;as_Register());</span>
<span class="line-added">5386       } else {</span>
<span class="line-added">5387         int st_off = to-&gt;reg2stack() * VMRegImpl::stack_slot_size + extra_stack_offset;</span>
<span class="line-added">5388         Address to_addr = Address(sp, st_off);</span>
<span class="line-added">5389         if (from-&gt;is_FloatRegister()) {</span>
<span class="line-added">5390           if (bt == T_DOUBLE) {</span>
<span class="line-added">5391              strd(from-&gt;as_FloatRegister(), to_addr);</span>
<span class="line-added">5392           } else {</span>
<span class="line-added">5393              assert(bt == T_FLOAT, &quot;must be float&quot;);</span>
<span class="line-added">5394              strs(from-&gt;as_FloatRegister(), to_addr);</span>
<span class="line-added">5395           }</span>
<span class="line-added">5396         } else {</span>
<span class="line-added">5397           str(from-&gt;as_Register(), to_addr);</span>
<span class="line-added">5398         }</span>
<span class="line-added">5399       }</span>
<span class="line-added">5400     } else {</span>
<span class="line-added">5401       Address from_addr = Address(sp, from-&gt;reg2stack() * VMRegImpl::stack_slot_size + extra_stack_offset);</span>
<span class="line-added">5402       if (to-&gt;is_reg()) {</span>
<span class="line-added">5403         if (to-&gt;is_FloatRegister()) {</span>
<span class="line-added">5404           if (bt == T_DOUBLE) {</span>
<span class="line-added">5405              ldrd(to-&gt;as_FloatRegister(), from_addr);</span>
<span class="line-added">5406           } else {</span>
<span class="line-added">5407             assert(bt == T_FLOAT, &quot;must be float&quot;);</span>
<span class="line-added">5408             ldrs(to-&gt;as_FloatRegister(), from_addr);</span>
<span class="line-added">5409           }</span>
<span class="line-added">5410         } else {</span>
<span class="line-added">5411           ldr(to-&gt;as_Register(), from_addr);</span>
<span class="line-added">5412         }</span>
<span class="line-added">5413       } else {</span>
<span class="line-added">5414         int st_off = to-&gt;reg2stack() * VMRegImpl::stack_slot_size + extra_stack_offset;</span>
<span class="line-added">5415         ldr(rscratch1, from_addr);</span>
<span class="line-added">5416         str(rscratch1, Address(sp, st_off));</span>
<span class="line-added">5417       }</span>
<span class="line-added">5418     }</span>
<span class="line-added">5419   }</span>
<span class="line-added">5420 </span>
<span class="line-added">5421   // Update register states</span>
<span class="line-added">5422   reg_state[from-&gt;value()] = reg_writable;</span>
<span class="line-added">5423   reg_state[to-&gt;value()] = reg_written;</span>
<span class="line-added">5424   return true;</span>
<span class="line-added">5425 }</span>
<span class="line-added">5426 </span>
<span class="line-added">5427 // Read all fields from a value type oop and store the values in registers/stack slots</span>
<span class="line-added">5428 bool MacroAssembler::unpack_value_helper(const GrowableArray&lt;SigEntry&gt;* sig, int&amp; sig_index, VMReg from, VMRegPair* regs_to,</span>
<span class="line-added">5429                                          int&amp; to_index, RegState reg_state[], int ret_off, int extra_stack_offset) {</span>
<span class="line-added">5430   Register fromReg = from-&gt;is_reg() ? from-&gt;as_Register() : noreg;</span>
<span class="line-added">5431   assert(sig-&gt;at(sig_index)._bt == T_VOID, &quot;should be at end delimiter&quot;);</span>
<span class="line-added">5432 </span>
<span class="line-added">5433 </span>
<span class="line-added">5434   int vt = 1;</span>
<span class="line-added">5435   bool done = true;</span>
<span class="line-added">5436   bool mark_done = true;</span>
<span class="line-added">5437   do {</span>
<span class="line-added">5438     sig_index--;</span>
<span class="line-added">5439     BasicType bt = sig-&gt;at(sig_index)._bt;</span>
<span class="line-added">5440     if (bt == T_VALUETYPE) {</span>
<span class="line-added">5441       vt--;</span>
<span class="line-added">5442     } else if (bt == T_VOID &amp;&amp;</span>
<span class="line-added">5443                sig-&gt;at(sig_index-1)._bt != T_LONG &amp;&amp;</span>
<span class="line-added">5444                sig-&gt;at(sig_index-1)._bt != T_DOUBLE) {</span>
<span class="line-added">5445       vt++;</span>
<span class="line-added">5446     } else if (SigEntry::is_reserved_entry(sig, sig_index)) {</span>
<span class="line-added">5447       to_index--; // Ignore this</span>
<span class="line-added">5448     } else {</span>
<span class="line-added">5449       assert(to_index &gt;= 0, &quot;invalid to_index&quot;);</span>
<span class="line-added">5450       VMRegPair pair_to = regs_to[to_index--];</span>
<span class="line-added">5451       VMReg to = pair_to.first();</span>
<span class="line-added">5452 </span>
<span class="line-added">5453       if (bt == T_VOID) continue;</span>
<span class="line-added">5454 </span>
<span class="line-added">5455       int idx = (int) to-&gt;value();</span>
<span class="line-added">5456       if (reg_state[idx] == reg_readonly) {</span>
<span class="line-added">5457          if (idx != from-&gt;value()) {</span>
<span class="line-added">5458            mark_done = false;</span>
<span class="line-added">5459          }</span>
<span class="line-added">5460          done = false;</span>
<span class="line-added">5461          continue;</span>
<span class="line-added">5462       } else if (reg_state[idx] == reg_written) {</span>
<span class="line-added">5463         continue;</span>
<span class="line-added">5464       } else {</span>
<span class="line-added">5465         assert(reg_state[idx] == reg_writable, &quot;must be writable&quot;);</span>
<span class="line-added">5466         reg_state[idx] = reg_written;</span>
<span class="line-added">5467       }</span>
<span class="line-added">5468 </span>
<span class="line-added">5469       if (fromReg == noreg) {</span>
<span class="line-added">5470         int st_off = from-&gt;reg2stack() * VMRegImpl::stack_slot_size + extra_stack_offset;</span>
<span class="line-added">5471         ldr(rscratch2, Address(sp, st_off));</span>
<span class="line-added">5472         fromReg = rscratch2;</span>
<span class="line-added">5473       }</span>
<span class="line-added">5474 </span>
<span class="line-added">5475       int off = sig-&gt;at(sig_index)._offset;</span>
<span class="line-added">5476       assert(off &gt; 0, &quot;offset in object should be positive&quot;);</span>
<span class="line-added">5477       bool is_oop = (bt == T_OBJECT || bt == T_ARRAY);</span>
<span class="line-added">5478 </span>
<span class="line-added">5479       Address fromAddr = Address(fromReg, off);</span>
<span class="line-added">5480       bool is_signed = (bt != T_CHAR) &amp;&amp; (bt != T_BOOLEAN);</span>
<span class="line-added">5481 </span>
<span class="line-added">5482       if (!to-&gt;is_FloatRegister()) {</span>
<span class="line-added">5483 </span>
<span class="line-added">5484         Register dst = to-&gt;is_stack() ? rscratch1 : to-&gt;as_Register();</span>
<span class="line-added">5485 </span>
<span class="line-added">5486         if (is_oop) {</span>
<span class="line-added">5487           load_heap_oop(dst, fromAddr);</span>
<span class="line-added">5488         } else {</span>
<span class="line-added">5489           load_sized_value(dst, fromAddr, type2aelembytes(bt), is_signed);</span>
<span class="line-added">5490         }</span>
<span class="line-added">5491         if (to-&gt;is_stack()) {</span>
<span class="line-added">5492           int st_off = to-&gt;reg2stack() * VMRegImpl::stack_slot_size + extra_stack_offset;</span>
<span class="line-added">5493           str(dst, Address(sp, st_off));</span>
<span class="line-added">5494         }</span>
<span class="line-added">5495       } else {</span>
<span class="line-added">5496         if (bt == T_DOUBLE) {</span>
<span class="line-added">5497           ldrd(to-&gt;as_FloatRegister(), fromAddr);</span>
<span class="line-added">5498         } else {</span>
<span class="line-added">5499           assert(bt == T_FLOAT, &quot;must be float&quot;);</span>
<span class="line-added">5500           ldrs(to-&gt;as_FloatRegister(), fromAddr);</span>
<span class="line-added">5501         }</span>
<span class="line-added">5502      }</span>
<span class="line-added">5503 </span>
<span class="line-added">5504     }</span>
<span class="line-added">5505 </span>
<span class="line-added">5506   } while (vt != 0);</span>
<span class="line-added">5507 </span>
<span class="line-added">5508   if (mark_done &amp;&amp; reg_state[from-&gt;value()] != reg_written) {</span>
<span class="line-added">5509     // This is okay because no one else will write to that slot</span>
<span class="line-added">5510     reg_state[from-&gt;value()] = reg_writable;</span>
<span class="line-added">5511   }</span>
<span class="line-added">5512   return done;</span>
<span class="line-added">5513 }</span>
<span class="line-added">5514 </span>
<span class="line-added">5515 // Pack fields back into a value type oop</span>
<span class="line-added">5516 bool MacroAssembler::pack_value_helper(const GrowableArray&lt;SigEntry&gt;* sig, int&amp; sig_index, int vtarg_index,</span>
<span class="line-added">5517                                        VMReg to, VMRegPair* regs_from, int regs_from_count, int&amp; from_index, RegState reg_state[],</span>
<span class="line-added">5518                                        int ret_off, int extra_stack_offset) {</span>
<span class="line-added">5519   assert(sig-&gt;at(sig_index)._bt == T_VALUETYPE, &quot;should be at end delimiter&quot;);</span>
<span class="line-added">5520   assert(to-&gt;is_valid(), &quot;must be&quot;);</span>
<span class="line-added">5521 </span>
<span class="line-added">5522   if (reg_state[to-&gt;value()] == reg_written) {</span>
<span class="line-added">5523     skip_unpacked_fields(sig, sig_index, regs_from, regs_from_count, from_index);</span>
<span class="line-added">5524     return true; // Already written</span>
<span class="line-added">5525   }</span>
<span class="line-added">5526 </span>
<span class="line-added">5527   Register val_array = r0;</span>
<span class="line-added">5528   Register val_obj_tmp = r11;</span>
<span class="line-added">5529   Register from_reg_tmp = r10;</span>
<span class="line-added">5530   Register tmp1 = r14;</span>
<span class="line-added">5531   Register tmp2 = r13;</span>
<span class="line-added">5532   Register tmp3 = r1;</span>
<span class="line-added">5533   Register val_obj = to-&gt;is_stack() ? val_obj_tmp : to-&gt;as_Register();</span>
<span class="line-added">5534 </span>
<span class="line-added">5535   if (reg_state[to-&gt;value()] == reg_readonly) {</span>
<span class="line-added">5536     if (!is_reg_in_unpacked_fields(sig, sig_index, to, regs_from, regs_from_count, from_index)) {</span>
<span class="line-added">5537       skip_unpacked_fields(sig, sig_index, regs_from, regs_from_count, from_index);</span>
<span class="line-added">5538       return false; // Not yet writable</span>
<span class="line-added">5539     }</span>
<span class="line-added">5540     val_obj = val_obj_tmp;</span>
<span class="line-added">5541   }</span>
<span class="line-added">5542 </span>
<span class="line-added">5543   int index = arrayOopDesc::base_offset_in_bytes(T_OBJECT) + vtarg_index * type2aelembytes(T_VALUETYPE);</span>
<span class="line-added">5544   load_heap_oop(val_obj, Address(val_array, index));</span>
<span class="line-added">5545 </span>
<span class="line-added">5546   ScalarizedValueArgsStream stream(sig, sig_index, regs_from, regs_from_count, from_index);</span>
<span class="line-added">5547   VMRegPair from_pair;</span>
<span class="line-added">5548   BasicType bt;</span>
<span class="line-added">5549 </span>
<span class="line-added">5550   while (stream.next(from_pair, bt)) {</span>
<span class="line-added">5551     int off = sig-&gt;at(stream.sig_cc_index())._offset;</span>
<span class="line-added">5552     assert(off &gt; 0, &quot;offset in object should be positive&quot;);</span>
<span class="line-added">5553     bool is_oop = (bt == T_OBJECT || bt == T_ARRAY);</span>
<span class="line-added">5554     size_t size_in_bytes = is_java_primitive(bt) ? type2aelembytes(bt) : wordSize;</span>
<span class="line-added">5555 </span>
<span class="line-added">5556     VMReg from_r1 = from_pair.first();</span>
<span class="line-added">5557     VMReg from_r2 = from_pair.second();</span>
<span class="line-added">5558 </span>
<span class="line-added">5559     // Pack the scalarized field into the value object.</span>
<span class="line-added">5560     Address dst(val_obj, off);</span>
<span class="line-added">5561 </span>
<span class="line-added">5562     if (!from_r1-&gt;is_FloatRegister()) {</span>
<span class="line-added">5563       Register from_reg;</span>
<span class="line-added">5564       if (from_r1-&gt;is_stack()) {</span>
<span class="line-added">5565         from_reg = from_reg_tmp;</span>
<span class="line-added">5566         int ld_off = from_r1-&gt;reg2stack() * VMRegImpl::stack_slot_size + extra_stack_offset;</span>
<span class="line-added">5567         load_sized_value(from_reg, Address(sp, ld_off), size_in_bytes, /* is_signed */ false);</span>
<span class="line-added">5568       } else {</span>
<span class="line-added">5569         from_reg = from_r1-&gt;as_Register();</span>
<span class="line-added">5570       }</span>
<span class="line-added">5571 </span>
<span class="line-added">5572       if (is_oop) {</span>
<span class="line-added">5573         DecoratorSet decorators = IN_HEAP | ACCESS_WRITE;</span>
<span class="line-added">5574         store_heap_oop(dst, from_reg, tmp1, tmp2, tmp3, decorators);</span>
<span class="line-added">5575       } else {</span>
<span class="line-added">5576         store_sized_value(dst, from_reg, size_in_bytes);</span>
<span class="line-added">5577       }</span>
<span class="line-added">5578     } else {</span>
<span class="line-added">5579       if (from_r2-&gt;is_valid()) {</span>
<span class="line-added">5580         strd(from_r1-&gt;as_FloatRegister(), dst);</span>
<span class="line-added">5581       } else {</span>
<span class="line-added">5582         strs(from_r1-&gt;as_FloatRegister(), dst);</span>
<span class="line-added">5583       }</span>
<span class="line-added">5584     }</span>
<span class="line-added">5585 </span>
<span class="line-added">5586     reg_state[from_r1-&gt;value()] = reg_writable;</span>
<span class="line-added">5587   }</span>
<span class="line-added">5588   sig_index = stream.sig_cc_index();</span>
<span class="line-added">5589   from_index = stream.regs_cc_index();</span>
<span class="line-added">5590 </span>
<span class="line-added">5591   assert(reg_state[to-&gt;value()] == reg_writable, &quot;must have already been read&quot;);</span>
<span class="line-added">5592   bool success = move_helper(val_obj-&gt;as_VMReg(), to, T_OBJECT, reg_state, ret_off, extra_stack_offset);</span>
<span class="line-added">5593   assert(success, &quot;to register must be writeable&quot;);</span>
<span class="line-added">5594 </span>
<span class="line-added">5595   return true;</span>
<span class="line-added">5596 }</span>
<span class="line-added">5597 </span>
<span class="line-added">5598 // Unpack all value type arguments passed as oops</span>
<span class="line-added">5599 void MacroAssembler::unpack_value_args(Compile* C, bool receiver_only) {</span>
<span class="line-added">5600   int sp_inc = unpack_value_args_common(C, receiver_only);</span>
<span class="line-added">5601   // Emit code for verified entry and save increment for stack repair on return</span>
<span class="line-added">5602   verified_entry(C, sp_inc);</span>
<span class="line-added">5603 }</span>
<span class="line-added">5604 </span>
<span class="line-added">5605 int MacroAssembler::shuffle_value_args(bool is_packing, bool receiver_only, int extra_stack_offset,</span>
<span class="line-added">5606                                        BasicType* sig_bt, const GrowableArray&lt;SigEntry&gt;* sig_cc,</span>
<span class="line-added">5607                                        int args_passed, int args_on_stack, VMRegPair* regs,            // from</span>
<span class="line-added">5608                                        int args_passed_to, int args_on_stack_to, VMRegPair* regs_to) { // to</span>
<span class="line-added">5609   // Check if we need to extend the stack for packing/unpacking</span>
<span class="line-added">5610   int sp_inc = (args_on_stack_to - args_on_stack) * VMRegImpl::stack_slot_size;</span>
<span class="line-added">5611   if (sp_inc &gt; 0) {</span>
<span class="line-added">5612     sp_inc = align_up(sp_inc, StackAlignmentInBytes);</span>
<span class="line-added">5613     if (!is_packing) {</span>
<span class="line-added">5614       // Save the return address, adjust the stack (make sure it is properly</span>
<span class="line-added">5615       // 16-byte aligned) and copy the return address to the new top of the stack.</span>
<span class="line-added">5616       // (Note: C1 does this in C1_MacroAssembler::scalarized_entry).</span>
<span class="line-added">5617       // FIXME: We need not to preserve return address on aarch64</span>
<span class="line-added">5618       pop(rscratch1);</span>
<span class="line-added">5619       sub(sp, sp, sp_inc);</span>
<span class="line-added">5620       push(rscratch1);</span>
<span class="line-added">5621     }</span>
<span class="line-added">5622   } else {</span>
<span class="line-added">5623     // The scalarized calling convention needs less stack space than the unscalarized one.</span>
<span class="line-added">5624     // No need to extend the stack, the caller will take care of these adjustments.</span>
<span class="line-added">5625     sp_inc = 0;</span>
<span class="line-added">5626   }</span>
<span class="line-added">5627 </span>
<span class="line-added">5628   int ret_off; // make sure we don&#39;t overwrite the return address</span>
<span class="line-added">5629   if (is_packing) {</span>
<span class="line-added">5630     // For C1 code, the VVEP doesn&#39;t have reserved slots, so we store the returned address at</span>
<span class="line-added">5631     // rsp[0] during shuffling.</span>
<span class="line-added">5632     ret_off = 0;</span>
<span class="line-added">5633   } else {</span>
<span class="line-added">5634     // C2 code ensures that sp_inc is a reserved slot.</span>
<span class="line-added">5635     ret_off = sp_inc;</span>
<span class="line-added">5636   }</span>
<span class="line-added">5637 </span>
<span class="line-added">5638   return shuffle_value_args_common(is_packing, receiver_only, extra_stack_offset,</span>
<span class="line-added">5639                                    sig_bt, sig_cc,</span>
<span class="line-added">5640                                    args_passed, args_on_stack, regs,</span>
<span class="line-added">5641                                    args_passed_to, args_on_stack_to, regs_to,</span>
<span class="line-added">5642                                    sp_inc, ret_off);</span>
<span class="line-added">5643 }</span>
<span class="line-added">5644 </span>
<span class="line-added">5645 VMReg MacroAssembler::spill_reg_for(VMReg reg) {</span>
<span class="line-added">5646   return (reg-&gt;is_FloatRegister()) ? v0-&gt;as_VMReg() : r14-&gt;as_VMReg();</span>
<span class="line-added">5647 }</span>
<span class="line-added">5648 </span>
5649 void MacroAssembler::cache_wb(Address line) {
5650   assert(line.getMode() == Address::base_plus_offset, &quot;mode should be base_plus_offset&quot;);
5651   assert(line.index() == noreg, &quot;index should be noreg&quot;);
5652   assert(line.offset() == 0, &quot;offset should be 0&quot;);
5653   // would like to assert this
5654   // assert(line._ext.shift == 0, &quot;shift should be zero&quot;);
5655   if (VM_Version::supports_dcpop()) {
5656     // writeback using clear virtual address to point of persistence
5657     dc(Assembler::CVAP, line.base());
5658   } else {
5659     // no need to generate anything as Unsafe.writebackMemory should
5660     // never invoke this stub
5661   }
5662 }
5663 
5664 void MacroAssembler::cache_wbsync(bool is_pre) {
5665   // we only need a barrier post sync
5666   if (!is_pre) {
5667     membar(Assembler::AnyAny);
5668   }
</pre>
</td>
</tr>
</table>
<center><a href="gc/shared/barrierSetAssembler_aarch64.hpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="macroAssembler_aarch64.hpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>