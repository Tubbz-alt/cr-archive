<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Frames src/hotspot/cpu/aarch64/gc/shared/barrierSetAssembler_aarch64.cpp</title>
    <link rel="stylesheet" href="../../../../../../style.css" />
    <script type="text/javascript" src="../../../../../../navigation.js"></script>
  </head>
<body onkeypress="keypress(event);">
<a name="0"></a>
<hr />
<pre>  1 /*
  2  * Copyright (c) 2018, 2020, Oracle and/or its affiliates. All rights reserved.
  3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  4  *
  5  * This code is free software; you can redistribute it and/or modify it
  6  * under the terms of the GNU General Public License version 2 only, as
  7  * published by the Free Software Foundation.
  8  *
  9  * This code is distributed in the hope that it will be useful, but WITHOUT
 10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
 11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
 12  * version 2 for more details (a copy is included in the LICENSE file that
 13  * accompanied this code).
 14  *
 15  * You should have received a copy of the GNU General Public License version
 16  * 2 along with this work; if not, write to the Free Software Foundation,
 17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 18  *
 19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
 20  * or visit www.oracle.com if you need additional information or have any
 21  * questions.
 22  *
 23  */
 24 
 25 #include &quot;precompiled.hpp&quot;
 26 #include &quot;gc/shared/barrierSet.hpp&quot;
 27 #include &quot;gc/shared/barrierSetAssembler.hpp&quot;
 28 #include &quot;gc/shared/barrierSetNMethod.hpp&quot;
 29 #include &quot;gc/shared/collectedHeap.hpp&quot;
 30 #include &quot;interpreter/interp_masm.hpp&quot;
 31 #include &quot;memory/universe.hpp&quot;
 32 #include &quot;runtime/jniHandles.hpp&quot;
 33 #include &quot;runtime/sharedRuntime.hpp&quot;
 34 #include &quot;runtime/thread.hpp&quot;
 35 
 36 
 37 #define __ masm-&gt;
 38 
 39 void BarrierSetAssembler::load_at(MacroAssembler* masm, DecoratorSet decorators, BasicType type,
 40                                   Register dst, Address src, Register tmp1, Register tmp_thread) {
 41 
 42   // LR is live.  It must be saved around calls.
 43 
 44   bool in_heap = (decorators &amp; IN_HEAP) != 0;
 45   bool in_native = (decorators &amp; IN_NATIVE) != 0;
 46   bool is_not_null = (decorators &amp; IS_NOT_NULL) != 0;
 47   switch (type) {
 48   case T_OBJECT:
 49   case T_ARRAY: {
 50     if (in_heap) {
 51       if (UseCompressedOops) {
 52         __ ldrw(dst, src);
 53         if (is_not_null) {
 54           __ decode_heap_oop_not_null(dst);
 55         } else {
 56           __ decode_heap_oop(dst);
 57         }
 58       } else {
 59         __ ldr(dst, src);
 60       }
 61     } else {
 62       assert(in_native, &quot;why else?&quot;);
 63       __ ldr(dst, src);
 64     }
 65     break;
 66   }
 67   case T_BOOLEAN: __ load_unsigned_byte (dst, src); break;
 68   case T_BYTE:    __ load_signed_byte   (dst, src); break;
 69   case T_CHAR:    __ load_unsigned_short(dst, src); break;
 70   case T_SHORT:   __ load_signed_short  (dst, src); break;
 71   case T_INT:     __ ldrw               (dst, src); break;
 72   case T_LONG:    __ ldr                (dst, src); break;
 73   case T_ADDRESS: __ ldr                (dst, src); break;
 74   case T_FLOAT:   __ ldrs               (v0, src);  break;
 75   case T_DOUBLE:  __ ldrd               (v0, src);  break;
 76   default: Unimplemented();
 77   }
 78 }
 79 
 80 void BarrierSetAssembler::store_at(MacroAssembler* masm, DecoratorSet decorators, BasicType type,
<a name="1" id="anc1"></a><span class="line-modified"> 81                                    Address dst, Register val, Register tmp1, Register tmp2, Register tmp3) {</span>
 82   bool in_heap = (decorators &amp; IN_HEAP) != 0;
 83   bool in_native = (decorators &amp; IN_NATIVE) != 0;
<a name="2" id="anc2"></a><span class="line-added"> 84   bool is_not_null = (decorators &amp; IS_NOT_NULL) != 0;</span>
<span class="line-added"> 85 </span>
 86   switch (type) {
 87   case T_OBJECT:
 88   case T_ARRAY: {
<a name="3" id="anc3"></a><span class="line-modified"> 89    if (in_heap) {</span>
<span class="line-modified"> 90       if (val == noreg) {</span>
<span class="line-modified"> 91         assert(!is_not_null, &quot;inconsistent access&quot;);</span>
<span class="line-modified"> 92         if (UseCompressedOops) {</span>
<span class="line-modified"> 93           __ strw(zr, dst);</span>
<span class="line-modified"> 94         } else {</span>
<span class="line-added"> 95           __ str(zr, dst);</span>
 96         }
<a name="4" id="anc4"></a><span class="line-modified"> 97       } else {</span>
<span class="line-modified"> 98         if (UseCompressedOops) {</span>
<span class="line-added"> 99           assert(!dst.uses(val), &quot;not enough registers&quot;);</span>
<span class="line-added">100           if (is_not_null) {</span>
<span class="line-added">101             __ encode_heap_oop_not_null(val);</span>
<span class="line-added">102           } else {</span>
<span class="line-added">103             __ encode_heap_oop(val);</span>
<span class="line-added">104           }</span>
<span class="line-added">105           __ strw(val, dst);</span>
<span class="line-added">106         } else {</span>
<span class="line-added">107           __ str(val, dst);</span>
108         }
109       }
110     } else {
111       assert(in_native, &quot;why else?&quot;);
<a name="5" id="anc5"></a><span class="line-added">112       assert(val != noreg, &quot;not supported&quot;);</span>
113       __ str(val, dst);
114     }
115     break;
116   }
117   case T_BOOLEAN:
118     __ andw(val, val, 0x1);  // boolean is true if LSB is 1
119     __ strb(val, dst);
120     break;
121   case T_BYTE:    __ strb(val, dst); break;
122   case T_CHAR:    __ strh(val, dst); break;
123   case T_SHORT:   __ strh(val, dst); break;
124   case T_INT:     __ strw(val, dst); break;
125   case T_LONG:    __ str (val, dst); break;
126   case T_ADDRESS: __ str (val, dst); break;
127   case T_FLOAT:   __ strs(v0,  dst); break;
128   case T_DOUBLE:  __ strd(v0,  dst); break;
129   default: Unimplemented();
130   }
131 }
132 
133 void BarrierSetAssembler::obj_equals(MacroAssembler* masm,
134                                      Register obj1, Register obj2) {
135   __ cmp(obj1, obj2);
136 }
137 
138 void BarrierSetAssembler::try_resolve_jobject_in_native(MacroAssembler* masm, Register jni_env,
139                                                         Register obj, Register tmp, Label&amp; slowpath) {
140   // If mask changes we need to ensure that the inverse is still encodable as an immediate
141   STATIC_ASSERT(JNIHandles::weak_tag_mask == 1);
142   __ andr(obj, obj, ~JNIHandles::weak_tag_mask);
143   __ ldr(obj, Address(obj, 0));             // *obj
144 }
145 
146 // Defines obj, preserves var_size_in_bytes, okay for t2 == var_size_in_bytes.
147 void BarrierSetAssembler::tlab_allocate(MacroAssembler* masm, Register obj,
148                                         Register var_size_in_bytes,
149                                         int con_size_in_bytes,
150                                         Register t1,
151                                         Register t2,
152                                         Label&amp; slow_case) {
153   assert_different_registers(obj, t2);
154   assert_different_registers(obj, var_size_in_bytes);
155   Register end = t2;
156 
157   // verify_tlab();
158 
159   __ ldr(obj, Address(rthread, JavaThread::tlab_top_offset()));
160   if (var_size_in_bytes == noreg) {
161     __ lea(end, Address(obj, con_size_in_bytes));
162   } else {
163     __ lea(end, Address(obj, var_size_in_bytes));
164   }
165   __ ldr(rscratch1, Address(rthread, JavaThread::tlab_end_offset()));
166   __ cmp(end, rscratch1);
167   __ br(Assembler::HI, slow_case);
168 
169   // update the tlab top pointer
170   __ str(end, Address(rthread, JavaThread::tlab_top_offset()));
171 
172   // recover var_size_in_bytes if necessary
173   if (var_size_in_bytes == end) {
174     __ sub(var_size_in_bytes, var_size_in_bytes, obj);
175   }
176   // verify_tlab();
177 }
178 
179 // Defines obj, preserves var_size_in_bytes
180 void BarrierSetAssembler::eden_allocate(MacroAssembler* masm, Register obj,
181                                         Register var_size_in_bytes,
182                                         int con_size_in_bytes,
183                                         Register t1,
184                                         Label&amp; slow_case) {
185   assert_different_registers(obj, var_size_in_bytes, t1);
186   if (!Universe::heap()-&gt;supports_inline_contig_alloc()) {
187     __ b(slow_case);
188   } else {
189     Register end = t1;
190     Register heap_end = rscratch2;
191     Label retry;
192     __ bind(retry);
193     {
194       unsigned long offset;
195       __ adrp(rscratch1, ExternalAddress((address) Universe::heap()-&gt;end_addr()), offset);
196       __ ldr(heap_end, Address(rscratch1, offset));
197     }
198 
199     ExternalAddress heap_top((address) Universe::heap()-&gt;top_addr());
200 
201     // Get the current top of the heap
202     {
203       unsigned long offset;
204       __ adrp(rscratch1, heap_top, offset);
205       // Use add() here after ARDP, rather than lea().
206       // lea() does not generate anything if its offset is zero.
207       // However, relocs expect to find either an ADD or a load/store
208       // insn after an ADRP.  add() always generates an ADD insn, even
209       // for add(Rn, Rn, 0).
210       __ add(rscratch1, rscratch1, offset);
211       __ ldaxr(obj, rscratch1);
212     }
213 
214     // Adjust it my the size of our new object
215     if (var_size_in_bytes == noreg) {
216       __ lea(end, Address(obj, con_size_in_bytes));
217     } else {
218       __ lea(end, Address(obj, var_size_in_bytes));
219     }
220 
221     // if end &lt; obj then we wrapped around high memory
222     __ cmp(end, obj);
223     __ br(Assembler::LO, slow_case);
224 
225     __ cmp(end, heap_end);
226     __ br(Assembler::HI, slow_case);
227 
228     // If heap_top hasn&#39;t been changed by some other thread, update it.
229     __ stlxr(rscratch2, end, rscratch1);
230     __ cbnzw(rscratch2, retry);
231 
232     incr_allocated_bytes(masm, var_size_in_bytes, con_size_in_bytes, t1);
233   }
234 }
235 
236 void BarrierSetAssembler::incr_allocated_bytes(MacroAssembler* masm,
237                                                Register var_size_in_bytes,
238                                                int con_size_in_bytes,
239                                                Register t1) {
240   assert(t1-&gt;is_valid(), &quot;need temp reg&quot;);
241 
242   __ ldr(t1, Address(rthread, in_bytes(JavaThread::allocated_bytes_offset())));
243   if (var_size_in_bytes-&gt;is_valid()) {
244     __ add(t1, t1, var_size_in_bytes);
245   } else {
246     __ add(t1, t1, con_size_in_bytes);
247   }
248   __ str(t1, Address(rthread, in_bytes(JavaThread::allocated_bytes_offset())));
249 }
250 
251 void BarrierSetAssembler::nmethod_entry_barrier(MacroAssembler* masm) {
252   BarrierSetNMethod* bs_nm = BarrierSet::barrier_set()-&gt;barrier_set_nmethod();
253 
254   if (bs_nm == NULL) {
255     return;
256   }
257 
258   Label skip, guard;
259   Address thread_disarmed_addr(rthread, in_bytes(bs_nm-&gt;thread_disarmed_offset()));
260 
261   __ ldrw(rscratch1, guard);
262 
263   // Subsequent loads of oops must occur after load of guard value.
264   // BarrierSetNMethod::disarm sets guard with release semantics.
265   __ membar(__ LoadLoad);
266   __ ldrw(rscratch2, thread_disarmed_addr);
267   __ cmpw(rscratch1, rscratch2);
268   __ br(Assembler::EQ, skip);
269 
270   __ mov(rscratch1, StubRoutines::aarch64::method_entry_barrier());
271   __ blr(rscratch1);
272   __ b(skip);
273 
274   __ bind(guard);
275 
276   __ emit_int32(0);   // nmethod guard value. Skipped over in common case.
277 
278   __ bind(skip);
279 }
280 
281 void BarrierSetAssembler::c2i_entry_barrier(MacroAssembler* masm) {
282   BarrierSetNMethod* bs = BarrierSet::barrier_set()-&gt;barrier_set_nmethod();
283   if (bs == NULL) {
284     return;
285   }
286 
287   Label bad_call;
288   __ cbz(rmethod, bad_call);
289 
290   // Pointer chase to the method holder to find out if the method is concurrently unloading.
291   Label method_live;
292   __ load_method_holder_cld(rscratch1, rmethod);
293 
294   // Is it a strong CLD?
295   __ ldr(rscratch2, Address(rscratch1, ClassLoaderData::keep_alive_offset()));
296   __ cbnz(rscratch2, method_live);
297 
298   // Is it a weak but alive CLD?
299   __ stp(r10, r11, Address(__ pre(sp, -2 * wordSize)));
300   __ ldr(r10, Address(rscratch1, ClassLoaderData::holder_offset()));
301 
302   // Uses rscratch1 &amp; rscratch2, so we must pass new temporaries.
303   __ resolve_weak_handle(r10, r11);
304   __ mov(rscratch1, r10);
305   __ ldp(r10, r11, Address(__ post(sp, 2 * wordSize)));
306   __ cbnz(rscratch1, method_live);
307 
308   __ bind(bad_call);
309 
310   __ far_jump(RuntimeAddress(SharedRuntime::get_handle_wrong_method_stub()));
311   __ bind(method_live);
312 }
313 
<a name="6" id="anc6"></a><b style="font-size: large; color: red">--- EOF ---</b>
















































































</pre>
<input id="eof" value="6" type="hidden" />
</body>
</html>