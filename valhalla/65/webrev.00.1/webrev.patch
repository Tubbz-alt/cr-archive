diff a/.hgtags b/.hgtags
--- a/.hgtags
+++ b/.hgtags
@@ -487,10 +487,11 @@
 758deedaae8406ae60147486107a54e9864aa7b0 jdk-11+13
 3595bd343b65f8c37818ebe6a4c343ddeb1a5f88 jdk-11+14
 a11c1cb542bbd1671d25b85efe7d09b983c48525 jdk-11+15
 02934b0d661b82b7fe1052a04998d2091352e08d jdk-11+16
 64e4b1686141e57a681936a8283983341484676e jdk-11+17
+d2aa5d494481a1039a092d70efa1f5c9826c5b77 lw1_0
 e1b3def126240d5433902f3cb0e91a4c27f6db50 jdk-11+18
 36ca515343e00b021dcfc902e986d26ec994a2e5 jdk-11+19
 95aad0c785e497f1bade3955c4e4a677b629fa9d jdk-12+0
 9816d7cc655e53ba081f938b656e31971b8f097a jdk-11+20
 14708e1acdc3974f4539027cbbcfa6d69f83cf51 jdk-11+21
@@ -509,10 +510,11 @@
 ef57958c7c511162da8d9a75f0b977f0f7ac464e jdk-12+7
 76072a077ee1d815152d45d1692c4b36c53c5c49 jdk-11+28
 492b366f8e5784cc4927c2c98f9b8a3f16c067eb jdk-12+8
 31b159f30fb281016c5f0c103552809aeda84063 jdk-12+9
 8f594f75e0547d4ca16649cb3501659e3155e81b jdk-12+10
+6132641c6ff61b7b8f3f10b9cd385aafadbd72ef lworld_stable
 f0f5d23449d31f1b3580c8a73313918cafeaefd7 jdk-12+11
 15094d12a632f452a2064318a4e416d0c7a9ce0c jdk-12+12
 511a9946f83e3e3c7b9dbe1840367063fb39b4e1 jdk-12+13
 8897e41b327c0a5601c6ba2bba5d07f15a3ffc91 jdk-12+14
 8897e41b327c0a5601c6ba2bba5d07f15a3ffc91 jdk-12+14
@@ -565,10 +567,14 @@
 22b3b7983adab54e318f75aeb94471f7a4429c1e jdk-14+0
 22b3b7983adab54e318f75aeb94471f7a4429c1e jdk-13+25
 2f4e214781a1d597ed36bf5a36f20928c6c82996 jdk-14+1
 0692b67f54621991ba7afbf23e55b788f3555e69 jdk-13+26
 43627549a488b7d0b4df8fad436e36233df89877 jdk-14+2
+6132641c6ff61b7b8f3f10b9cd385aafadbd72ef lworld_stable
+2b098533f1e52d7d541121409b745d9420886945 lworld_stable
+2b098533f1e52d7d541121409b745d9420886945 lworld_stable
+7c637fd25e7d6fccdab1098bedd48ed195a86cc7 lworld_stable
 b7f68ddec66f996ae3aad03291d129ca9f02482d jdk-13+27
 e64383344f144217c36196c3c8a2df8f588a2af3 jdk-14+3
 1e95931e7d8fa7e3899340a9c7cb28dbea50c10c jdk-13+28
 19d0b382f0869f72d4381b54fa129f1c74b6e766 jdk-14+4
 3081f39a3d30d63b112098386ac2bb027c2b7223 jdk-13+29
diff a/make/Images.gmk b/make/Images.gmk
--- a/make/Images.gmk
+++ b/make/Images.gmk
@@ -52,10 +52,15 @@
 
 ################################################################################
 
 BASE_RELEASE_FILE := $(JDK_OUTPUTDIR)/release
 
+$(JDK_IMAGE_DIR)/README: $(TOPDIR)/src/README
+	$(install-file)
+
+JDK_TARGETS += $(JDK_IMAGE_DIR)/README
+
 JMODS := $(wildcard $(IMAGES_OUTPUTDIR)/jmods/*.jmod)
 
 JLINK_ORDER_RESOURCES := **module-info.class
 JLINK_JLI_CLASSES :=
 ifeq ($(ENABLE_GENERATE_CLASSLIST), true)
diff a/make/conf/jib-profiles.js b/make/conf/jib-profiles.js
--- a/make/conf/jib-profiles.js
+++ b/make/conf/jib-profiles.js
@@ -1367,10 +1367,11 @@
             preString = version_numbers.get("DEFAULT_PROMOTED_VERSION_PRE");
         }
         args = concat(args, "--with-version-pre=" + preString,
                      "--with-version-opt=" + optString);
     } else {
+        args = concat(args, "--with-version-pre=lworld2ea");
         args = concat(args, "--with-version-opt=" + common.build_id);
     }
     return args;
 }
 
diff a/make/modules/java.base/gensrc/GensrcVarHandles.gmk b/make/modules/java.base/gensrc/GensrcVarHandles.gmk
--- a/make/modules/java.base/gensrc/GensrcVarHandles.gmk
+++ b/make/modules/java.base/gensrc/GensrcVarHandles.gmk
@@ -50,13 +50,23 @@
 
   ifneq ($$(findstring $$($1_Type), Byte Short Char), )
     $1_ARGS += -KShorterThanInt
   endif
 
+  ifeq ($$($1_Type), Reference)
+    $1_ARGS += -KReference
+  endif
+
+  ifeq ($$($1_Type), Value)
+    $1_ARGS += -KValue
+  endif
+
   $$($1_FILENAME): $(VARHANDLES_SRC_DIR)/X-VarHandle.java.template $(BUILD_TOOLS_JDK)
         ifeq ($$($1_Type), Reference)
 	  $$(eval $1_type := Object)
+        else ifeq ($$($1_Type), Value)
+	  $$(eval $1_type := Object)
         else
 	  $$(eval $1_type := $$$$(shell $(TR) '[:upper:]' '[:lower:]' <<< $$$$($1_Type)))
         endif
 	$$(call MakeDir, $$(@D))
 	$(RM) $$@
@@ -260,11 +270,11 @@
 endef
 
 ################################################################################
 
 # List the types to generate source for, with capitalized first letter
-VARHANDLES_TYPES := Boolean Byte Short Char Int Long Float Double Reference
+VARHANDLES_TYPES := Boolean Byte Short Char Int Long Float Double Reference Value
 $(foreach t, $(VARHANDLES_TYPES), \
   $(eval $(call GenerateVarHandle,VAR_HANDLE_$t,$t)))
 
 # List the types to generate source for, with capitalized first letter
 VARHANDLES_BYTE_ARRAY_TYPES := Short Char Int Long Float Double
diff a/make/test/BuildMicrobenchmark.gmk b/make/test/BuildMicrobenchmark.gmk
--- a/make/test/BuildMicrobenchmark.gmk
+++ b/make/test/BuildMicrobenchmark.gmk
@@ -88,11 +88,12 @@
 # Build microbenchmark suite for the current JDK
 $(eval $(call SetupJavaCompilation, BUILD_JDK_MICROBENCHMARK, \
     TARGET_RELEASE := $(TARGET_RELEASE_NEWJDK_UPGRADED), \
     SMALL_JAVA := false, \
     CLASSPATH := $(MICROBENCHMARK_CLASSPATH), \
-    DISABLED_WARNINGS := processing rawtypes cast serial, \
+    DISABLED_WARNINGS := processing rawtypes unchecked cast serial deprecation, \
+    JAVAC_FLAGS := -XDallowWithFieldOperator, \
     SRC := $(MICROBENCHMARK_SRC), \
     BIN := $(MICROBENCHMARK_CLASSES), \
     JAVA_FLAGS := --add-modules jdk.unsupported --limit-modules java.management, \
 ))
 
diff a/src/hotspot/cpu/aarch64/aarch64.ad b/src/hotspot/cpu/aarch64/aarch64.ad
--- a/src/hotspot/cpu/aarch64/aarch64.ad
+++ b/src/hotspot/cpu/aarch64/aarch64.ad
@@ -1654,10 +1654,12 @@
 
 void MachPrologNode::emit(CodeBuffer &cbuf, PhaseRegAlloc *ra_) const {
   Compile* C = ra_->C;
   C2_MacroAssembler _masm(&cbuf);
 
+  __ verified_entry(C, 0);
+  __ bind(*_verified_entry);
   // n.b. frame size includes space for return pc and rfp
   const long framesize = C->output()->frame_size_in_bytes();
   assert(framesize%(2*wordSize) == 0, "must preserve 2*wordSize alignment");
 
   // insert a nop at the start of the prolog so we can patch in a
@@ -1996,12 +1998,50 @@
 uint BoxLockNode::size(PhaseRegAlloc *ra_) const {
   // BoxLockNode is not a MachNode, so we can't just call MachNode::size(ra_).
   return 4;
 }
 
-//=============================================================================
+///=============================================================================
+#ifndef PRODUCT
+void MachVEPNode::format(PhaseRegAlloc* ra_, outputStream* st) const
+{
+  st->print_cr("# MachVEPNode");
+  if (!_verified) {
+    st->print_cr("\t load_class");
+  } else {
+    st->print_cr("\t unpack_value_arg");
+  }
+}
+#endif
+
+void MachVEPNode::emit(CodeBuffer& cbuf, PhaseRegAlloc* ra_) const
+{
+  MacroAssembler _masm(&cbuf);
+
+  if (!_verified) {
+    Label skip;
+    __ cmp_klass(j_rarg0, rscratch2, rscratch1);
+    __ br(Assembler::EQ, skip);
+      __ far_jump(RuntimeAddress(SharedRuntime::get_ic_miss_stub()));
+    __ bind(skip);
+
+  } else {
+    // Unpack value type args passed as oop and then jump to
+    // the verified entry point (skipping the unverified entry).
+    __ unpack_value_args(ra_->C, _receiver_only);
+    __ b(*_verified_entry);
+  }
+}
+
+
+uint MachVEPNode::size(PhaseRegAlloc* ra_) const
+{
+  return MachNode::size(ra_); // too many variables; just compute it the hard way
+}
 
+
+//=============================================================================
 #ifndef PRODUCT
 void MachUEPNode::format(PhaseRegAlloc* ra_, outputStream* st) const
 {
   st->print_cr("# MachUEPNode");
   if (UseCompressedClassPointers) {
@@ -2019,13 +2059,15 @@
 
 void MachUEPNode::emit(CodeBuffer& cbuf, PhaseRegAlloc* ra_) const
 {
   // This is the unverified entry point.
   C2_MacroAssembler _masm(&cbuf);
+  Label skip;
 
+  // UseCompressedClassPointers logic are inside cmp_klass
   __ cmp_klass(j_rarg0, rscratch2, rscratch1);
-  Label skip;
+
   // TODO
   // can we avoid this skip and still use a reloc?
   __ br(Assembler::EQ, skip);
   __ far_jump(RuntimeAddress(SharedRuntime::get_ic_miss_stub()));
   __ bind(skip);
@@ -2428,11 +2470,10 @@
 }
 
 void Compile::reshape_address(AddPNode* addp) {
 }
 
-
 #define MOV_VOLATILE(REG, BASE, INDEX, SCALE, DISP, SCRATCH, INSN)      \
   C2_MacroAssembler _masm(&cbuf);                                       \
   {                                                                     \
     guarantee(INDEX == -1, "mode not permitted for volatile");          \
     guarantee(DISP == 0, "mode not permitted for volatile");            \
@@ -8288,10 +8329,25 @@
   %}
 
   ins_pipe(ialu_reg);
 %}
 
+instruct castN2X(iRegLNoSp dst, iRegN src) %{
+  match(Set dst (CastP2X src));
+
+  ins_cost(INSN_COST);
+  format %{ "mov $dst, $src\t# ptr -> long" %}
+
+  ins_encode %{
+    if ($dst$$reg != $src$$reg) {
+      __ mov(as_Register($dst$$reg), as_Register($src$$reg));
+    }
+  %}
+
+  ins_pipe(ialu_reg);
+%}
+
 instruct castP2X(iRegLNoSp dst, iRegP src) %{
   match(Set dst (CastP2X src));
 
   ins_cost(INSN_COST);
   format %{ "mov $dst, $src\t# ptr -> long" %}
@@ -8303,10 +8359,41 @@
   %}
 
   ins_pipe(ialu_reg);
 %}
 
+instruct castN2I(iRegINoSp dst, iRegN src) %{
+  match(Set dst (CastN2I src));
+
+  ins_cost(INSN_COST);
+  format %{ "movw $dst, $src\t# compressed ptr -> int" %}
+
+  ins_encode %{
+    if ($dst$$reg != $src$$reg) {
+      __ movw(as_Register($dst$$reg), as_Register($src$$reg));
+    }
+  %}
+
+  ins_pipe(ialu_reg);
+%}
+
+instruct castI2N(iRegNNoSp dst, iRegI src) %{
+  match(Set dst (CastI2N src));
+
+  ins_cost(INSN_COST);
+  format %{ "movw $dst, $src\t# int -> compressed ptr" %}
+
+  ins_encode %{
+    if ($dst$$reg != $src$$reg) {
+      __ movw(as_Register($dst$$reg), as_Register($src$$reg));
+    }
+  %}
+
+  ins_pipe(ialu_reg);
+%}
+
+
 // Convert oop into int for vectors alignment masking
 instruct convP2I(iRegINoSp dst, iRegP src) %{
   match(Set dst (ConvL2I (CastP2X src)));
 
   ins_cost(INSN_COST);
@@ -13885,37 +13972,20 @@
 %}
 
 // ============================================================================
 // clearing of an array
 
-instruct clearArray_reg_reg(iRegL_R11 cnt, iRegP_R10 base, Universe dummy, rFlagsReg cr)
+instruct clearArray_reg_reg(iRegL_R11 cnt, iRegP_R10 base, iRegL val, Universe dummy, rFlagsReg cr)
 %{
-  match(Set dummy (ClearArray cnt base));
+  match(Set dummy (ClearArray (Binary cnt base) val));
   effect(USE_KILL cnt, USE_KILL base);
 
   ins_cost(4 * INSN_COST);
-  format %{ "ClearArray $cnt, $base" %}
-
-  ins_encode %{
-    __ zero_words($base$$Register, $cnt$$Register);
-  %}
-
-  ins_pipe(pipe_class_memory);
-%}
-
-instruct clearArray_imm_reg(immL cnt, iRegP_R10 base, Universe dummy, rFlagsReg cr)
-%{
-  predicate((u_int64_t)n->in(2)->get_long()
-            < (u_int64_t)(BlockZeroingLowLimit >> LogBytesPerWord));
-  match(Set dummy (ClearArray cnt base));
-  effect(USE_KILL base);
-
-  ins_cost(4 * INSN_COST);
-  format %{ "ClearArray $cnt, $base" %}
+  format %{ "ClearArray $cnt, $base, $val" %}
 
   ins_encode %{
-    __ zero_words($base$$Register, (u_int64_t)$cnt$$constant);
+    __ fill_words($base$$Register, $cnt$$Register, $val$$Register);
   %}
 
   ins_pipe(pipe_class_memory);
 %}
 
diff a/src/hotspot/cpu/aarch64/c1_MacroAssembler_aarch64.cpp b/src/hotspot/cpu/aarch64/c1_MacroAssembler_aarch64.cpp
--- a/src/hotspot/cpu/aarch64/c1_MacroAssembler_aarch64.cpp
+++ b/src/hotspot/cpu/aarch64/c1_MacroAssembler_aarch64.cpp
@@ -1,7 +1,7 @@
 /*
- * Copyright (c) 1999, 2018, Oracle and/or its affiliates. All rights reserved.
+ * Copyright (c) 1999, 2020, Oracle and/or its affiliates. All rights reserved.
  * Copyright (c) 2014, Red Hat Inc. All rights reserved.
  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  *
  * This code is free software; you can redistribute it and/or modify it
  * under the terms of the GNU General Public License version 2 only, as
@@ -27,10 +27,12 @@
 #include "c1/c1_MacroAssembler.hpp"
 #include "c1/c1_Runtime1.hpp"
 #include "classfile/systemDictionary.hpp"
 #include "gc/shared/barrierSetAssembler.hpp"
 #include "gc/shared/collectedHeap.hpp"
+#include "gc/shared/barrierSet.hpp"
+#include "gc/shared/barrierSetAssembler.hpp"
 #include "interpreter/interpreter.hpp"
 #include "oops/arrayOop.hpp"
 #include "oops/markWord.hpp"
 #include "runtime/basicLock.hpp"
 #include "runtime/biasedLocking.hpp"
@@ -82,10 +84,16 @@
 
   // Load object header
   ldr(hdr, Address(obj, hdr_offset));
   // and mark it as unlocked
   orr(hdr, hdr, markWord::unlocked_value);
+
+  if (EnableValhalla && !UseBiasedLocking) {
+    // Mask always_locked bit such that we go to the slow path if object is a value type
+    andr(hdr, hdr, ~markWord::biased_lock_bit_in_place);
+  }
+
   // save unlocked object header into the displaced header location on the stack
   str(hdr, Address(disp_hdr, 0));
   // test if object header is still the same (i.e. unlocked), and if so, store the
   // displaced header address in the object header - if it is not the same, get the
   // object header instead
@@ -329,35 +337,133 @@
 
   cmp_klass(receiver, iCache, rscratch1);
 }
 
 
-void C1_MacroAssembler::build_frame(int framesize, int bang_size_in_bytes) {
+void C1_MacroAssembler::build_frame(int framesize, int bang_size_in_bytes, bool needs_stack_repair, Label* verified_value_entry_label) {
   assert(bang_size_in_bytes >= framesize, "stack bang size incorrect");
   // Make sure there is enough stack space for this method's activation.
   // Note that we do this before doing an enter().
   generate_stack_overflow_check(bang_size_in_bytes);
+
+  guarantee(needs_stack_repair == false, "Stack repair should not be true");
+  if (verified_value_entry_label != NULL) {
+    bind(*verified_value_entry_label);
+  }
+
   MacroAssembler::build_frame(framesize + 2 * wordSize);
 
   // Insert nmethod entry barrier into frame.
   BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();
   bs->nmethod_entry_barrier(this);
 }
 
-void C1_MacroAssembler::remove_frame(int framesize) {
+void C1_MacroAssembler::remove_frame(int framesize, bool needs_stack_repair) {
+
+  guarantee(needs_stack_repair == false, "Stack repair should not be true");
+
   MacroAssembler::remove_frame(framesize + 2 * wordSize);
 }
 
+void C1_MacroAssembler::verified_value_entry() {
+  if (C1Breakpoint || VerifyFPU || !UseStackBanging) {
+    // Verified Entry first instruction should be 5 bytes long for correct
+    // patching by patch_verified_entry().
+    //
+    // C1Breakpoint and VerifyFPU have one byte first instruction.
+    // Also first instruction will be one byte "push(rbp)" if stack banging
+    // code is not generated (see build_frame() above).
+    // For all these cases generate long instruction first.
+    nop();
+  }
+
+  nop();
+  // build frame
+  // verify_FPU(0, "method_entry");
+}
+
+int C1_MacroAssembler::scalarized_entry(const CompiledEntrySignature *ces, int frame_size_in_bytes, int bang_size_in_bytes, Label& verified_value_entry_label, bool is_value_ro_entry) {
+  // This function required to support for InlineTypePassFieldsAsArgs
+  if (C1Breakpoint || VerifyFPU || !UseStackBanging) {
+    // Verified Entry first instruction should be 5 bytes long for correct
+    // patching by patch_verified_entry().
+    //
+    // C1Breakpoint and VerifyFPU have one byte first instruction.
+    // Also first instruction will be one byte "push(rbp)" if stack banging
+    // code is not generated (see build_frame() above).
+    // For all these cases generate long instruction first.
+    nop();
+  }
 
-void C1_MacroAssembler::verified_entry() {
-  // If we have to make this method not-entrant we'll overwrite its
-  // first instruction with a jump.  For this action to be legal we
-  // must ensure that this first instruction is a B, BL, NOP, BKPT,
-  // SVC, HVC, or SMC.  Make it a NOP.
+  nop();
+  // verify_FPU(0, "method_entry");
+
+  assert(InlineTypePassFieldsAsArgs, "sanity");
+
+  GrowableArray<SigEntry>* sig   = &ces->sig();
+  GrowableArray<SigEntry>* sig_cc = is_value_ro_entry ? &ces->sig_cc_ro() : &ces->sig_cc();
+  VMRegPair* regs      = ces->regs();
+  VMRegPair* regs_cc   = is_value_ro_entry ? ces->regs_cc_ro() : ces->regs_cc();
+  int args_on_stack    = ces->args_on_stack();
+  int args_on_stack_cc = is_value_ro_entry ? ces->args_on_stack_cc_ro() : ces->args_on_stack_cc();
+
+  assert(sig->length() <= sig_cc->length(), "Zero-sized value class not allowed!");
+  BasicType* sig_bt = NEW_RESOURCE_ARRAY(BasicType, sig_cc->length());
+  int args_passed = sig->length();
+  int args_passed_cc = SigEntry::fill_sig_bt(sig_cc, sig_bt);
+
+  int extra_stack_offset = wordSize; // tos is return address.
+
+  // Create a temp frame so we can call into runtime. It must be properly set up to accomodate GC.
+  int sp_inc = (args_on_stack - args_on_stack_cc) * VMRegImpl::stack_slot_size;
+  if (sp_inc > 0) {
+    sp_inc = align_up(sp_inc, StackAlignmentInBytes);
+    sub(sp, sp, sp_inc);
+  } else {
+    sp_inc = 0;
+  }
+
+  sub(sp, sp, frame_size_in_bytes);
+  if (sp_inc > 0) {
+    int real_frame_size = frame_size_in_bytes +
+           + wordSize  // pushed rbp
+           + wordSize  // returned address pushed by the stack extension code
+           + sp_inc;   // stack extension
+    mov(rscratch1, real_frame_size);
+    str(rscratch1, Address(sp, frame_size_in_bytes - wordSize));
+  }
+
+  // FIXME -- call runtime only if we cannot in-line allocate all the incoming value args.
+  mov(r1, (intptr_t) ces->method());
+  if (is_value_ro_entry) {
+    far_call(RuntimeAddress(Runtime1::entry_for(Runtime1::buffer_value_args_no_receiver_id)));
+  } else {
+    far_call(RuntimeAddress(Runtime1::entry_for(Runtime1::buffer_value_args_id)));
+  }
+  int rt_call_offset = offset();
+
+  // Remove the temp frame
+  add(sp, sp, frame_size_in_bytes);
+
+  int n = shuffle_value_args(true, is_value_ro_entry, extra_stack_offset, sig_bt, sig_cc,
+                             args_passed_cc, args_on_stack_cc, regs_cc, // from
+                             args_passed, args_on_stack, regs);         // to
+  assert(sp_inc == n, "must be");
+
+  if (sp_inc != 0) {
+    // Do the stack banging here, and skip over the stack repair code in the
+    // verified_value_entry (which has a different real_frame_size).
+    assert(sp_inc > 0, "stack should not shrink");
+    generate_stack_overflow_check(bang_size_in_bytes);
+    decrement(sp, frame_size_in_bytes);
+  }
+
+  b(verified_value_entry_label);
   nop();
 }
 
+
 void C1_MacroAssembler::load_parameter(int offset_in_words, Register reg) {
   // rbp, + 0: link
   //     + 1: return address
   //     + 2: argument with offset 0
   //     + 3: argument with offset 1
diff a/src/hotspot/cpu/aarch64/gc/shared/barrierSetAssembler_aarch64.cpp b/src/hotspot/cpu/aarch64/gc/shared/barrierSetAssembler_aarch64.cpp
--- a/src/hotspot/cpu/aarch64/gc/shared/barrierSetAssembler_aarch64.cpp
+++ b/src/hotspot/cpu/aarch64/gc/shared/barrierSetAssembler_aarch64.cpp
@@ -76,29 +76,42 @@
   default: Unimplemented();
   }
 }
 
 void BarrierSetAssembler::store_at(MacroAssembler* masm, DecoratorSet decorators, BasicType type,
-                                   Address dst, Register val, Register tmp1, Register tmp2) {
+                                   Address dst, Register val, Register tmp1, Register tmp2, Register tmp3) {
   bool in_heap = (decorators & IN_HEAP) != 0;
   bool in_native = (decorators & IN_NATIVE) != 0;
+  bool is_not_null = (decorators & IS_NOT_NULL) != 0;
+
   switch (type) {
   case T_OBJECT:
   case T_ARRAY: {
-    val = val == noreg ? zr : val;
-    if (in_heap) {
-      if (UseCompressedOops) {
-        assert(!dst.uses(val), "not enough registers");
-        if (val != zr) {
-          __ encode_heap_oop(val);
+   if (in_heap) {
+      if (val == noreg) {
+        assert(!is_not_null, "inconsistent access");
+        if (UseCompressedOops) {
+          __ strw(zr, dst);
+        } else {
+          __ str(zr, dst);
         }
-        __ strw(val, dst);
-      } else {
+      } else {
+        if (UseCompressedOops) {
+          assert(!dst.uses(val), "not enough registers");
+          if (is_not_null) {
+            __ encode_heap_oop_not_null(val);
+          } else {
+            __ encode_heap_oop(val);
+          }
+          __ strw(val, dst);
+        } else {
+          __ str(val, dst);
         __ str(val, dst);
       }
     } else {
       assert(in_native, "why else?");
+      assert(val != noreg, "not supported");
       __ str(val, dst);
     }
     break;
   }
   case T_BOOLEAN:
diff a/src/hotspot/cpu/aarch64/gc/shared/barrierSetAssembler_aarch64.hpp b/src/hotspot/cpu/aarch64/gc/shared/barrierSetAssembler_aarch64.hpp
--- a/src/hotspot/cpu/aarch64/gc/shared/barrierSetAssembler_aarch64.hpp
+++ b/src/hotspot/cpu/aarch64/gc/shared/barrierSetAssembler_aarch64.hpp
@@ -43,11 +43,11 @@
   virtual void arraycopy_epilogue(MacroAssembler* masm, DecoratorSet decorators, bool is_oop,
                                   Register start, Register end, Register tmp, RegSet saved_regs) {}
   virtual void load_at(MacroAssembler* masm, DecoratorSet decorators, BasicType type,
                        Register dst, Address src, Register tmp1, Register tmp_thread);
   virtual void store_at(MacroAssembler* masm, DecoratorSet decorators, BasicType type,
-                        Address dst, Register val, Register tmp1, Register tmp2);
+                        Address dst, Register val, Register tmp1, Register tmp2, Register tmp3 = noreg);
 
   virtual void obj_equals(MacroAssembler* masm,
                           Register obj1, Register obj2);
 
   virtual void resolve(MacroAssembler* masm, DecoratorSet decorators, Register obj) {
diff a/src/hotspot/cpu/aarch64/macroAssembler_aarch64.cpp b/src/hotspot/cpu/aarch64/macroAssembler_aarch64.cpp
--- a/src/hotspot/cpu/aarch64/macroAssembler_aarch64.cpp
+++ b/src/hotspot/cpu/aarch64/macroAssembler_aarch64.cpp
@@ -44,10 +44,11 @@
 #include "runtime/biasedLocking.hpp"
 #include "runtime/icache.hpp"
 #include "runtime/interfaceSupport.inline.hpp"
 #include "runtime/jniHandles.inline.hpp"
 #include "runtime/sharedRuntime.hpp"
+#include "runtime/signature_cc.hpp"
 #include "runtime/thread.hpp"
 #include "utilities/powerOfTwo.hpp"
 #ifdef COMPILER1
 #include "c1/c1_LIRAssembler.hpp"
 #endif
@@ -1314,11 +1315,15 @@
     Unimplemented();
   }
 }
 
 void MacroAssembler::verify_oop(Register reg, const char* s) {
-  if (!VerifyOops) return;
+  if (!VerifyOops || VerifyAdapterSharing) {
+    // Below address of the code string confuses VerifyAdapterSharing
+    // because it may differ between otherwise equivalent adapters.
+    return;
+  }
 
   // Pass register number to verify_oop_subroutine
   const char* b = NULL;
   {
     ResourceMark rm;
@@ -1344,11 +1349,15 @@
 
   BLOCK_COMMENT("} verify_oop");
 }
 
 void MacroAssembler::verify_oop_addr(Address addr, const char* s) {
-  if (!VerifyOops) return;
+  if (!VerifyOops || VerifyAdapterSharing) {
+    // Below address of the code string confuses VerifyAdapterSharing
+    // because it may differ between otherwise equivalent adapters.
+    return;
+  }
 
   const char* b = NULL;
   {
     ResourceMark rm;
     stringStream ss;
@@ -1437,10 +1446,14 @@
   pass_arg1(this, arg_1);
   pass_arg2(this, arg_2);
   call_VM_leaf_base(entry_point, 3);
 }
 
+void MacroAssembler::super_call_VM_leaf(address entry_point) {
+  MacroAssembler::call_VM_leaf_base(entry_point, 1);
+}
+
 void MacroAssembler::super_call_VM_leaf(address entry_point, Register arg_0) {
   pass_arg0(this, arg_0);
   MacroAssembler::call_VM_leaf_base(entry_point, 1);
 }
 
@@ -1486,10 +1499,43 @@
     // nothing to do, (later) access of M[reg + offset]
     // will provoke OS NULL exception if reg = NULL
   }
 }
 
+void MacroAssembler::test_klass_is_value(Register klass, Register temp_reg, Label& is_value) {
+  ldrw(temp_reg, Address(klass, Klass::access_flags_offset()));
+  andr(temp_reg, temp_reg, JVM_ACC_VALUE);
+  cbnz(temp_reg, is_value);
+}
+
+void MacroAssembler::test_field_is_flattenable(Register flags, Register temp_reg, Label& is_flattenable) {
+  (void) temp_reg; // keep signature uniform with x86
+  tbnz(flags, ConstantPoolCacheEntry::is_flattenable_field_shift, is_flattenable);
+}
+
+void MacroAssembler::test_field_is_not_flattenable(Register flags, Register temp_reg, Label& not_flattenable) {
+  (void) temp_reg; // keep signature uniform with x86
+  tbz(flags, ConstantPoolCacheEntry::is_flattenable_field_shift, not_flattenable);
+}
+
+void MacroAssembler::test_field_is_flattened(Register flags, Register temp_reg, Label& is_flattened) {
+  (void) temp_reg; // keep signature uniform with x86
+  tbnz(flags, ConstantPoolCacheEntry::is_flattened_field_shift, is_flattened);
+}
+
+void MacroAssembler::test_flattened_array_oop(Register oop, Register temp_reg, Label& is_flattened_array) {
+  load_storage_props(temp_reg, oop);
+  andr(temp_reg, temp_reg, ArrayStorageProperties::flattened_value);
+  cbnz(temp_reg, is_flattened_array);
+}
+
+void MacroAssembler::test_null_free_array_oop(Register oop, Register temp_reg, Label& is_null_free_array) {
+  load_storage_props(temp_reg, oop);
+  andr(temp_reg, temp_reg, ArrayStorageProperties::null_free_value);
+  cbnz(temp_reg, is_null_free_array);
+}
+
 // MacroAssembler protected routines needed to implement
 // public methods
 
 void MacroAssembler::mov(Register r, Address dest) {
   code_section()->relocate(pc(), dest.rspec());
@@ -3702,19 +3748,28 @@
   ldr(holder, Address(method, Method::const_offset()));                      // ConstMethod*
   ldr(holder, Address(holder, ConstMethod::constants_offset()));             // ConstantPool*
   ldr(holder, Address(holder, ConstantPool::pool_holder_offset_in_bytes())); // InstanceKlass*
 }
 
-void MacroAssembler::load_klass(Register dst, Register src) {
+void MacroAssembler::load_metadata(Register dst, Register src) {
   if (UseCompressedClassPointers) {
     ldrw(dst, Address(src, oopDesc::klass_offset_in_bytes()));
-    decode_klass_not_null(dst);
   } else {
     ldr(dst, Address(src, oopDesc::klass_offset_in_bytes()));
   }
 }
 
+void MacroAssembler::load_klass(Register dst, Register src) {
+  load_metadata(dst, src);
+  if (UseCompressedClassPointers) {
+    andr(dst, dst, oopDesc::compressed_klass_mask());
+    decode_klass_not_null(dst);
+  } else {
+    ubfm(dst, dst, 0, 63 - oopDesc::storage_props_nof_bits);
+  }
+}
+
 // ((OopHandle)result).resolve();
 void MacroAssembler::resolve_oop_handle(Register result, Register tmp) {
   // OopHandle::resolve is an indirection.
   access_load_at(T_OBJECT, IN_NATIVE, result, Address(result, 0), tmp, noreg);
 }
@@ -3742,10 +3797,19 @@
   ldr(dst, Address(dst, ConstantPool::pool_holder_offset_in_bytes()));
   ldr(dst, Address(dst, mirror_offset));
   resolve_oop_handle(dst, tmp);
 }
 
+void MacroAssembler::load_storage_props(Register dst, Register src) {
+  load_metadata(dst, src);
+  if (UseCompressedClassPointers) {
+    asrw(dst, dst, oopDesc::narrow_storage_props_shift);
+  } else {
+    asr(dst, dst, oopDesc::wide_storage_props_shift);
+  }
+}
+
 void MacroAssembler::cmp_klass(Register oop, Register trial_klass, Register tmp) {
   if (UseCompressedClassPointers) {
     ldrw(tmp, Address(oop, oopDesc::klass_offset_in_bytes()));
     if (CompressedKlassPointers::base() == NULL) {
       cmp(trial_klass, tmp, LSL, CompressedKlassPointers::shift());
@@ -4079,18 +4143,19 @@
   }
 }
 
 void MacroAssembler::access_store_at(BasicType type, DecoratorSet decorators,
                                      Address dst, Register src,
-                                     Register tmp1, Register thread_tmp) {
+                                     Register tmp1, Register thread_tmp, Register tmp3) {
+
   BarrierSetAssembler *bs = BarrierSet::barrier_set()->barrier_set_assembler();
   decorators = AccessInternal::decorator_fixup(decorators);
   bool as_raw = (decorators & AS_RAW) != 0;
   if (as_raw) {
-    bs->BarrierSetAssembler::store_at(this, decorators, type, dst, src, tmp1, thread_tmp);
+    bs->BarrierSetAssembler::store_at(this, decorators, type, dst, src, tmp1, thread_tmp, tmp3);
   } else {
-    bs->store_at(this, decorators, type, dst, src, tmp1, thread_tmp);
+    bs->store_at(this, decorators, type, dst, src, tmp1, thread_tmp, tmp3);
   }
 }
 
 void MacroAssembler::resolve(DecoratorSet decorators, Register obj) {
   // Use stronger ACCESS_WRITE|ACCESS_READ by default.
@@ -4110,17 +4175,17 @@
                                             Register thread_tmp, DecoratorSet decorators) {
   access_load_at(T_OBJECT, IN_HEAP | IS_NOT_NULL | decorators, dst, src, tmp1, thread_tmp);
 }
 
 void MacroAssembler::store_heap_oop(Address dst, Register src, Register tmp1,
-                                    Register thread_tmp, DecoratorSet decorators) {
-  access_store_at(T_OBJECT, IN_HEAP | decorators, dst, src, tmp1, thread_tmp);
+                                    Register thread_tmp, Register tmp3, DecoratorSet decorators) {
+  access_store_at(T_OBJECT, IN_HEAP | decorators, dst, src, tmp1, thread_tmp, tmp3);
 }
 
 // Used for storing NULLs.
 void MacroAssembler::store_heap_oop_null(Address dst) {
-  access_store_at(T_OBJECT, IN_HEAP, dst, noreg, noreg, noreg);
+  access_store_at(T_OBJECT, IN_HEAP, dst, noreg, noreg, noreg, noreg);
 }
 
 Address MacroAssembler::allocate_metadata_address(Metadata* obj) {
   assert(oop_recorder() != NULL, "this assembler needs a Recorder");
   int index = oop_recorder()->allocate_metadata_index(obj);
@@ -5189,10 +5254,400 @@
   }
 
   pop(saved_regs, sp);
 }
 
+// C2 compiled method's prolog code
+// Moved here from aarch64.ad to support Valhalla code belows
+void MacroAssembler::verified_entry(Compile* C, int sp_inc) {
+
+// n.b. frame size includes space for return pc and rfp
+  const long framesize = C->frame_size_in_bytes();
+  assert(framesize % (2 * wordSize) == 0, "must preserve 2 * wordSize alignment");
+
+  // insert a nop at the start of the prolog so we can patch in a
+  // branch if we need to invalidate the method later
+  nop();
+
+  int bangsize = C->bang_size_in_bytes();
+  if (C->need_stack_bang(bangsize) && UseStackBanging)
+     generate_stack_overflow_check(bangsize);
+
+  build_frame(framesize);
+
+  if (VerifyStackAtCalls) {
+    Unimplemented();
+  }
+}
+
+int MacroAssembler::store_value_type_fields_to_buf(ciValueKlass* vk, bool from_interpreter) {
+  // A value type might be returned. If fields are in registers we
+  // need to allocate a value type instance and initialize it with
+  // the value of the fields.
+  Label skip;
+  // We only need a new buffered value if a new one is not returned
+  cmp(r0, (u1) 1);
+  br(Assembler::EQ, skip);
+  int call_offset = -1;
+
+  Label slow_case;
+
+  // Try to allocate a new buffered value (from the heap)
+  if (UseTLAB) {
+
+    if (vk != NULL) {
+      // Called from C1, where the return type is statically known.
+      mov(r1, (intptr_t)vk->get_ValueKlass());
+      jint lh = vk->layout_helper();
+      assert(lh != Klass::_lh_neutral_value, "inline class in return type must have been resolved");
+      mov(r14, lh);
+    } else {
+       // Call from interpreter. R0 contains ((the ValueKlass* of the return type) | 0x01)
+       andr(r1, r0, -2);
+       // get obj size
+       ldrw(r14, Address(rscratch1 /*klass*/, Klass::layout_helper_offset()));
+    }
+
+     ldr(r13, Address(rthread, in_bytes(JavaThread::tlab_top_offset())));
+
+     // check whether we have space in TLAB,
+     // rscratch1 contains pointer to just allocated obj
+      lea(r14, Address(r13, r14));
+      ldr(rscratch1, Address(rthread, in_bytes(JavaThread::tlab_end_offset())));
+
+      cmp(r14, rscratch1);
+      br(Assembler::GT, slow_case);
+
+      // OK we have room in TLAB,
+      // Set new TLAB top
+      str(r14, Address(rthread, in_bytes(JavaThread::tlab_top_offset())));
+
+      // Set new class always locked
+      mov(rscratch1, (uint64_t) markWord::always_locked_prototype().value());
+      str(rscratch1, Address(r13, oopDesc::mark_offset_in_bytes()));
+
+      store_klass_gap(r13, zr);  // zero klass gap for compressed oops
+      if (vk == NULL) {
+        // store_klass corrupts rbx, so save it in rax for later use (interpreter case only).
+         mov(r0, r1);
+      }
+
+      store_klass(r13, r1);  // klass
+
+      if (vk != NULL) {
+        // FIXME -- do the packing in-line to avoid the runtime call
+        mov(r0, r13);
+        far_call(RuntimeAddress(vk->pack_handler())); // no need for call info as this will not safepoint.
+      } else {
+
+        // We have our new buffered value, initialize its fields with a
+        // value class specific handler
+        ldr(r1, Address(r0, InstanceKlass::adr_valueklass_fixed_block_offset()));
+        ldr(r1, Address(r1, ValueKlass::pack_handler_offset()));
+
+        // Mov new class to r0 and call pack_handler
+        mov(r0, r13);
+        blr(r1);
+      }
+      b(skip);
+  }
+
+  bind(slow_case);
+  // We failed to allocate a new value, fall back to a runtime
+  // call. Some oop field may be live in some registers but we can't
+  // tell. That runtime call will take care of preserving them
+  // across a GC if there's one.
+
+
+  if (from_interpreter) {
+    super_call_VM_leaf(StubRoutines::store_value_type_fields_to_buf());
+  } else {
+    ldr(rscratch1, RuntimeAddress(StubRoutines::store_value_type_fields_to_buf()));
+    blr(rscratch1);
+    call_offset = offset();
+  }
+
+  bind(skip);
+  return call_offset;
+}
+
+// Move a value between registers/stack slots and update the reg_state
+bool MacroAssembler::move_helper(VMReg from, VMReg to, BasicType bt, RegState reg_state[], int ret_off, int extra_stack_offset) {
+  if (reg_state[to->value()] == reg_written) {
+    return true; // Already written
+  }
+
+  if (from != to && bt != T_VOID) {
+    if (reg_state[to->value()] == reg_readonly) {
+      return false; // Not yet writable
+    }
+    if (from->is_reg()) {
+      if (to->is_reg()) {
+        mov(to->as_Register(), from->as_Register());
+      } else {
+        int st_off = to->reg2stack() * VMRegImpl::stack_slot_size + extra_stack_offset;
+        Address to_addr = Address(sp, st_off);
+        if (from->is_FloatRegister()) {
+          if (bt == T_DOUBLE) {
+             strd(from->as_FloatRegister(), to_addr);
+          } else {
+             assert(bt == T_FLOAT, "must be float");
+             strs(from->as_FloatRegister(), to_addr);
+          }
+        } else {
+          str(from->as_Register(), to_addr);
+        }
+      }
+    } else {
+      Address from_addr = Address(sp, from->reg2stack() * VMRegImpl::stack_slot_size + extra_stack_offset);
+      if (to->is_reg()) {
+        if (to->is_FloatRegister()) {
+          if (bt == T_DOUBLE) {
+             ldrd(to->as_FloatRegister(), from_addr);
+          } else {
+            assert(bt == T_FLOAT, "must be float");
+            ldrs(to->as_FloatRegister(), from_addr);
+          }
+        } else {
+          ldr(to->as_Register(), from_addr);
+        }
+      } else {
+        int st_off = to->reg2stack() * VMRegImpl::stack_slot_size + extra_stack_offset;
+        ldr(rscratch1, from_addr);
+        str(rscratch1, Address(sp, st_off));
+      }
+    }
+  }
+
+  // Update register states
+  reg_state[from->value()] = reg_writable;
+  reg_state[to->value()] = reg_written;
+  return true;
+}
+
+// Read all fields from a value type oop and store the values in registers/stack slots
+bool MacroAssembler::unpack_value_helper(const GrowableArray<SigEntry>* sig, int& sig_index, VMReg from, VMRegPair* regs_to,
+                                         int& to_index, RegState reg_state[], int ret_off, int extra_stack_offset) {
+  Register fromReg = from->is_reg() ? from->as_Register() : noreg;
+  assert(sig->at(sig_index)._bt == T_VOID, "should be at end delimiter");
+
+
+  int vt = 1;
+  bool done = true;
+  bool mark_done = true;
+  do {
+    sig_index--;
+    BasicType bt = sig->at(sig_index)._bt;
+    if (bt == T_VALUETYPE) {
+      vt--;
+    } else if (bt == T_VOID &&
+               sig->at(sig_index-1)._bt != T_LONG &&
+               sig->at(sig_index-1)._bt != T_DOUBLE) {
+      vt++;
+    } else if (SigEntry::is_reserved_entry(sig, sig_index)) {
+      to_index--; // Ignore this
+    } else {
+      assert(to_index >= 0, "invalid to_index");
+      VMRegPair pair_to = regs_to[to_index--];
+      VMReg to = pair_to.first();
+
+      if (bt == T_VOID) continue;
+
+      int idx = (int) to->value();
+      if (reg_state[idx] == reg_readonly) {
+         if (idx != from->value()) {
+           mark_done = false;
+         }
+         done = false;
+         continue;
+      } else if (reg_state[idx] == reg_written) {
+        continue;
+      } else {
+        assert(reg_state[idx] == reg_writable, "must be writable");
+        reg_state[idx] = reg_written;
+      }
+
+      if (fromReg == noreg) {
+        int st_off = from->reg2stack() * VMRegImpl::stack_slot_size + extra_stack_offset;
+        ldr(rscratch2, Address(sp, st_off));
+        fromReg = rscratch2;
+      }
+
+      int off = sig->at(sig_index)._offset;
+      assert(off > 0, "offset in object should be positive");
+      bool is_oop = (bt == T_OBJECT || bt == T_ARRAY);
+
+      Address fromAddr = Address(fromReg, off);
+      bool is_signed = (bt != T_CHAR) && (bt != T_BOOLEAN);
+
+      if (!to->is_FloatRegister()) {
+
+        Register dst = to->is_stack() ? rscratch1 : to->as_Register();
+
+        if (is_oop) {
+          load_heap_oop(dst, fromAddr);
+        } else {
+          load_sized_value(dst, fromAddr, type2aelembytes(bt), is_signed);
+        }
+        if (to->is_stack()) {
+          int st_off = to->reg2stack() * VMRegImpl::stack_slot_size + extra_stack_offset;
+          str(dst, Address(sp, st_off));
+        }
+      } else {
+        if (bt == T_DOUBLE) {
+          ldrd(to->as_FloatRegister(), fromAddr);
+        } else {
+          assert(bt == T_FLOAT, "must be float");
+          ldrs(to->as_FloatRegister(), fromAddr);
+        }
+     }
+
+    }
+
+  } while (vt != 0);
+
+  if (mark_done && reg_state[from->value()] != reg_written) {
+    // This is okay because no one else will write to that slot
+    reg_state[from->value()] = reg_writable;
+  }
+  return done;
+}
+
+// Pack fields back into a value type oop
+bool MacroAssembler::pack_value_helper(const GrowableArray<SigEntry>* sig, int& sig_index, int vtarg_index,
+                                       VMReg to, VMRegPair* regs_from, int regs_from_count, int& from_index, RegState reg_state[],
+                                       int ret_off, int extra_stack_offset) {
+  assert(sig->at(sig_index)._bt == T_VALUETYPE, "should be at end delimiter");
+  assert(to->is_valid(), "must be");
+
+  if (reg_state[to->value()] == reg_written) {
+    skip_unpacked_fields(sig, sig_index, regs_from, regs_from_count, from_index);
+    return true; // Already written
+  }
+
+  Register val_array = r0;
+  Register val_obj_tmp = r11;
+  Register from_reg_tmp = r10;
+  Register tmp1 = r14;
+  Register tmp2 = r13;
+  Register tmp3 = r1;
+  Register val_obj = to->is_stack() ? val_obj_tmp : to->as_Register();
+
+  if (reg_state[to->value()] == reg_readonly) {
+    if (!is_reg_in_unpacked_fields(sig, sig_index, to, regs_from, regs_from_count, from_index)) {
+      skip_unpacked_fields(sig, sig_index, regs_from, regs_from_count, from_index);
+      return false; // Not yet writable
+    }
+    val_obj = val_obj_tmp;
+  }
+
+  int index = arrayOopDesc::base_offset_in_bytes(T_OBJECT) + vtarg_index * type2aelembytes(T_VALUETYPE);
+  load_heap_oop(val_obj, Address(val_array, index));
+
+  ScalarizedValueArgsStream stream(sig, sig_index, regs_from, regs_from_count, from_index);
+  VMRegPair from_pair;
+  BasicType bt;
+
+  while (stream.next(from_pair, bt)) {
+    int off = sig->at(stream.sig_cc_index())._offset;
+    assert(off > 0, "offset in object should be positive");
+    bool is_oop = (bt == T_OBJECT || bt == T_ARRAY);
+    size_t size_in_bytes = is_java_primitive(bt) ? type2aelembytes(bt) : wordSize;
+
+    VMReg from_r1 = from_pair.first();
+    VMReg from_r2 = from_pair.second();
+
+    // Pack the scalarized field into the value object.
+    Address dst(val_obj, off);
+
+    if (!from_r1->is_FloatRegister()) {
+      Register from_reg;
+      if (from_r1->is_stack()) {
+        from_reg = from_reg_tmp;
+        int ld_off = from_r1->reg2stack() * VMRegImpl::stack_slot_size + extra_stack_offset;
+        load_sized_value(from_reg, Address(sp, ld_off), size_in_bytes, /* is_signed */ false);
+      } else {
+        from_reg = from_r1->as_Register();
+      }
+
+      if (is_oop) {
+        DecoratorSet decorators = IN_HEAP | ACCESS_WRITE;
+        store_heap_oop(dst, from_reg, tmp1, tmp2, tmp3, decorators);
+      } else {
+        store_sized_value(dst, from_reg, size_in_bytes);
+      }
+    } else {
+      if (from_r2->is_valid()) {
+        strd(from_r1->as_FloatRegister(), dst);
+      } else {
+        strs(from_r1->as_FloatRegister(), dst);
+      }
+    }
+
+    reg_state[from_r1->value()] = reg_writable;
+  }
+  sig_index = stream.sig_cc_index();
+  from_index = stream.regs_cc_index();
+
+  assert(reg_state[to->value()] == reg_writable, "must have already been read");
+  bool success = move_helper(val_obj->as_VMReg(), to, T_OBJECT, reg_state, ret_off, extra_stack_offset);
+  assert(success, "to register must be writeable");
+
+  return true;
+}
+
+// Unpack all value type arguments passed as oops
+void MacroAssembler::unpack_value_args(Compile* C, bool receiver_only) {
+  int sp_inc = unpack_value_args_common(C, receiver_only);
+  // Emit code for verified entry and save increment for stack repair on return
+  verified_entry(C, sp_inc);
+}
+
+int MacroAssembler::shuffle_value_args(bool is_packing, bool receiver_only, int extra_stack_offset,
+                                       BasicType* sig_bt, const GrowableArray<SigEntry>* sig_cc,
+                                       int args_passed, int args_on_stack, VMRegPair* regs,            // from
+                                       int args_passed_to, int args_on_stack_to, VMRegPair* regs_to) { // to
+  // Check if we need to extend the stack for packing/unpacking
+  int sp_inc = (args_on_stack_to - args_on_stack) * VMRegImpl::stack_slot_size;
+  if (sp_inc > 0) {
+    sp_inc = align_up(sp_inc, StackAlignmentInBytes);
+    if (!is_packing) {
+      // Save the return address, adjust the stack (make sure it is properly
+      // 16-byte aligned) and copy the return address to the new top of the stack.
+      // (Note: C1 does this in C1_MacroAssembler::scalarized_entry).
+      // FIXME: We need not to preserve return address on aarch64
+      pop(rscratch1);
+      sub(sp, sp, sp_inc);
+      push(rscratch1);
+    }
+  } else {
+    // The scalarized calling convention needs less stack space than the unscalarized one.
+    // No need to extend the stack, the caller will take care of these adjustments.
+    sp_inc = 0;
+  }
+
+  int ret_off; // make sure we don't overwrite the return address
+  if (is_packing) {
+    // For C1 code, the VVEP doesn't have reserved slots, so we store the returned address at
+    // rsp[0] during shuffling.
+    ret_off = 0;
+  } else {
+    // C2 code ensures that sp_inc is a reserved slot.
+    ret_off = sp_inc;
+  }
+
+  return shuffle_value_args_common(is_packing, receiver_only, extra_stack_offset,
+                                   sig_bt, sig_cc,
+                                   args_passed, args_on_stack, regs,
+                                   args_passed_to, args_on_stack_to, regs_to,
+                                   sp_inc, ret_off);
+}
+
+VMReg MacroAssembler::spill_reg_for(VMReg reg) {
+  return (reg->is_FloatRegister()) ? v0->as_VMReg() : r14->as_VMReg();
+}
+
 void MacroAssembler::cache_wb(Address line) {
   assert(line.getMode() == Address::base_plus_offset, "mode should be base_plus_offset");
   assert(line.index() == noreg, "index should be noreg");
   assert(line.offset() == 0, "offset should be 0");
   // would like to assert this
diff a/src/hotspot/cpu/aarch64/macroAssembler_aarch64.hpp b/src/hotspot/cpu/aarch64/macroAssembler_aarch64.hpp
--- a/src/hotspot/cpu/aarch64/macroAssembler_aarch64.hpp
+++ b/src/hotspot/cpu/aarch64/macroAssembler_aarch64.hpp
@@ -26,11 +26,16 @@
 #ifndef CPU_AARCH64_MACROASSEMBLER_AARCH64_HPP
 #define CPU_AARCH64_MACROASSEMBLER_AARCH64_HPP
 
 #include "asm/assembler.hpp"
 #include "oops/compressedOops.hpp"
+#include "utilities/macros.hpp"
 #include "utilities/powerOfTwo.hpp"
+#include "runtime/signature.hpp"
+
+
+class ciValueKlass;
 
 // MacroAssembler extends Assembler by frequently used macros.
 //
 // Instructions for which a 'better' code sequence exists depending
 // on arguments should also go in here.
@@ -606,10 +611,20 @@
 
   virtual void null_check(Register reg, int offset = -1);
   static bool needs_explicit_null_check(intptr_t offset);
   static bool uses_implicit_null_check(void* address);
 
+  void test_klass_is_value(Register klass, Register temp_reg, Label& is_value);
+
+  void test_field_is_flattenable(Register flags, Register temp_reg, Label& is_flattenable);
+  void test_field_is_not_flattenable(Register flags, Register temp_reg, Label& notFlattenable);
+  void test_field_is_flattened(Register flags, Register temp_reg, Label& is_flattened);
+
+  // Check klass/oops is flat value type array (oop->_klass->_layout_helper & vt_bit)
+  void test_flattened_array_oop(Register klass, Register temp_reg, Label& is_flattened_array);
+  void test_null_free_array_oop(Register oop, Register temp_reg, Label& is_null_free_array);
+
   static address target_addr_for_insn(address insn_addr, unsigned insn);
   static address target_addr_for_insn(address insn_addr) {
     unsigned insn = *(unsigned*)insn_addr;
     return target_addr_for_insn(insn_addr, insn);
   }
@@ -813,10 +828,13 @@
 
   void load_method_holder_cld(Register rresult, Register rmethod);
   void load_method_holder(Register holder, Register method);
 
   // oop manipulations
+  void load_metadata(Register dst, Register src);
+  void load_storage_props(Register dst, Register src);
+
   void load_klass(Register dst, Register src);
   void store_klass(Register dst, Register src);
   void cmp_klass(Register oop, Register trial_klass, Register tmp);
 
   void resolve_weak_handle(Register result, Register tmp);
@@ -825,11 +843,11 @@
 
   void access_load_at(BasicType type, DecoratorSet decorators, Register dst, Address src,
                       Register tmp1, Register tmp_thread);
 
   void access_store_at(BasicType type, DecoratorSet decorators, Address dst, Register src,
-                       Register tmp1, Register tmp_thread);
+                       Register tmp1, Register tmp_thread, Register tmp3 = noreg);
 
   // Resolves obj for access. Result is placed in the same register.
   // All other registers are preserved.
   void resolve(DecoratorSet decorators, Register obj);
 
@@ -837,11 +855,11 @@
                      Register thread_tmp = noreg, DecoratorSet decorators = 0);
 
   void load_heap_oop_not_null(Register dst, Address src, Register tmp1 = noreg,
                               Register thread_tmp = noreg, DecoratorSet decorators = 0);
   void store_heap_oop(Address dst, Register src, Register tmp1 = noreg,
-                      Register tmp_thread = noreg, DecoratorSet decorators = 0);
+                      Register tmp_thread = noreg, Register tmp3 = noreg, DecoratorSet decorators = 0);
 
   // currently unimplemented
   // Used for storing NULL. All other oop constants should be
   // stored using routines that take a jobject.
   void store_heap_oop_null(Address dst);
@@ -1173,10 +1191,41 @@
   void sub(Register Rd, Register Rn, RegisterOrConstant decrement);
   void subw(Register Rd, Register Rn, RegisterOrConstant decrement);
 
   void adrp(Register reg1, const Address &dest, unsigned long &byte_offset);
 
+
+  enum RegState {
+     reg_readonly,
+     reg_writable,
+     reg_written
+  };
+
+  void verified_entry(Compile* C, int sp_inc);
+
+  int store_value_type_fields_to_buf(ciValueKlass* vk, bool from_interpreter = true);
+
+// Unpack all value type arguments passed as oops
+  void unpack_value_args(Compile* C, bool receiver_only);
+  bool move_helper(VMReg from, VMReg to, BasicType bt, RegState reg_state[], int ret_off, int extra_stack_offset);
+  bool unpack_value_helper(const GrowableArray<SigEntry>* sig, int& sig_index, VMReg from, VMRegPair* regs_to, int& to_index,
+                           RegState reg_state[], int ret_off, int extra_stack_offset);
+  bool pack_value_helper(const GrowableArray<SigEntry>* sig, int& sig_index, int vtarg_index,
+                         VMReg to, VMRegPair* regs_from, int regs_from_count, int& from_index, RegState reg_state[],
+                         int ret_off, int extra_stack_offset);
+  void restore_stack(Compile* C);
+
+  int shuffle_value_args(bool is_packing, bool receiver_only, int extra_stack_offset,
+                         BasicType* sig_bt, const GrowableArray<SigEntry>* sig_cc,
+                         int args_passed, int args_on_stack, VMRegPair* regs,
+                         int args_passed_to, int args_on_stack_to, VMRegPair* regs_to);
+  bool shuffle_value_args_spill(bool is_packing,  const GrowableArray<SigEntry>* sig_cc, int sig_cc_index,
+                                VMRegPair* regs_from, int from_index, int regs_from_count,
+                                RegState* reg_state, int sp_inc, int extra_stack_offset);
+  VMReg spill_reg_for(VMReg reg);
+
+
   void tableswitch(Register index, jint lowbound, jint highbound,
                    Label &jumptable, Label &jumptable_end, int stride = 1) {
     adr(rscratch1, jumptable);
     subsw(rscratch2, index, lowbound);
     subsw(zr, rscratch2, highbound - lowbound);
@@ -1238,10 +1287,12 @@
 
   void string_equals(Register a1, Register a2, Register result, Register cnt1,
                      int elem_size);
 
   void fill_words(Register base, Register cnt, Register value);
+  void fill_words(Register base, u_int64_t cnt, Register value);
+
   void zero_words(Register base, u_int64_t cnt);
   void zero_words(Register ptr, Register cnt);
   void zero_dcache_blocks(Register base, Register cnt);
 
   static const int zero_words_block_size;
@@ -1359,10 +1410,13 @@
     }
   }
 
   void cache_wb(Address line);
   void cache_wbsync(bool is_pre);
+
+  #include "asm/macroAssembler_common.hpp"
+
 };
 
 #ifdef ASSERT
 inline bool AbstractAssembler::pd_check_instruction_mark() { return false; }
 #endif
diff a/src/hotspot/cpu/aarch64/sharedRuntime_aarch64.cpp b/src/hotspot/cpu/aarch64/sharedRuntime_aarch64.cpp
--- a/src/hotspot/cpu/aarch64/sharedRuntime_aarch64.cpp
+++ b/src/hotspot/cpu/aarch64/sharedRuntime_aarch64.cpp
@@ -24,10 +24,11 @@
  */
 
 #include "precompiled.hpp"
 #include "asm/macroAssembler.hpp"
 #include "asm/macroAssembler.inline.hpp"
+#include "classfile/symbolTable.hpp"
 #include "code/debugInfoRec.hpp"
 #include "code/icBuffer.hpp"
 #include "code/vtableStubs.hpp"
 #include "gc/shared/barrierSetAssembler.hpp"
 #include "interpreter/interpreter.hpp"
@@ -288,10 +289,11 @@
       assert((i + 1) < total_args_passed && sig_bt[i + 1] == T_VOID, "expecting half");
       // fall through
     case T_OBJECT:
     case T_ARRAY:
     case T_ADDRESS:
+    case T_VALUETYPE:
       if (int_args < Argument::n_int_register_parameters_j) {
         regs[i].set2(INT_ArgReg[int_args++]->as_VMReg());
       } else {
         regs[i].set2(VMRegImpl::stack2reg(stk_args));
         stk_args += 2;
@@ -321,10 +323,94 @@
   }
 
   return align_up(stk_args, 2);
 }
 
+
+// const uint SharedRuntime::java_return_convention_max_int = Argument::n_int_register_parameters_j+1;
+const uint SharedRuntime::java_return_convention_max_int = 6;
+const uint SharedRuntime::java_return_convention_max_float = Argument::n_float_register_parameters_j;
+
+int SharedRuntime::java_return_convention(const BasicType *sig_bt, VMRegPair *regs, int total_args_passed) {
+
+  // Create the mapping between argument positions and
+  // registers.
+  // r1, r2 used to address klasses and states, exclude it from return convention to avoid colision
+
+  static const Register INT_ArgReg[java_return_convention_max_int] = {
+     r0 /* j_rarg7 */, j_rarg6, j_rarg5, j_rarg4, j_rarg3, j_rarg2
+  };
+
+  static const FloatRegister FP_ArgReg[java_return_convention_max_float] = {
+    j_farg0, j_farg1, j_farg2, j_farg3, j_farg4, j_farg5, j_farg6, j_farg7
+  };
+
+  uint int_args = 0;
+  uint fp_args = 0;
+
+  for (int i = 0; i < total_args_passed; i++) {
+    switch (sig_bt[i]) {
+    case T_BOOLEAN:
+    case T_CHAR:
+    case T_BYTE:
+    case T_SHORT:
+    case T_INT:
+      if (int_args < SharedRuntime::java_return_convention_max_int) {
+        regs[i].set1(INT_ArgReg[int_args]->as_VMReg());
+        int_args ++;
+      } else {
+        // Should we have gurantee here?
+        return -1;
+      }
+      break;
+    case T_VOID:
+      // halves of T_LONG or T_DOUBLE
+      assert(i != 0 && (sig_bt[i - 1] == T_LONG || sig_bt[i - 1] == T_DOUBLE), "expecting half");
+      regs[i].set_bad();
+      break;
+    case T_LONG:
+      assert((i + 1) < total_args_passed && sig_bt[i + 1] == T_VOID, "expecting half");
+      // fall through
+    case T_OBJECT:
+    case T_ARRAY:
+    case T_ADDRESS:
+      // Should T_METADATA be added to java_calling_convention as well ?
+    case T_METADATA:
+    case T_VALUETYPE:
+      if (int_args < SharedRuntime::java_return_convention_max_int) {
+        regs[i].set2(INT_ArgReg[int_args]->as_VMReg());
+        int_args ++;
+      } else {
+        return -1;
+      }
+      break;
+    case T_FLOAT:
+      if (fp_args < SharedRuntime::java_return_convention_max_float) {
+        regs[i].set1(FP_ArgReg[fp_args]->as_VMReg());
+        fp_args ++;
+      } else {
+        return -1;
+      }
+      break;
+    case T_DOUBLE:
+      assert((i + 1) < total_args_passed && sig_bt[i + 1] == T_VOID, "expecting half");
+      if (fp_args < Argument::n_float_register_parameters_j) {
+        regs[i].set2(FP_ArgReg[fp_args]->as_VMReg());
+        fp_args ++;
+      } else {
+        return -1;
+      }
+      break;
+    default:
+      ShouldNotReachHere();
+      break;
+    }
+  }
+
+  return int_args + fp_args;
+}
+
 // Patch the callers callsite with entry to compiled code if it exists.
 static void patch_callers_callsite(MacroAssembler *masm) {
   Label L;
   __ ldr(rscratch1, Address(rmethod, in_bytes(Method::code_offset())));
   __ cbz(rscratch1, L);
@@ -351,50 +437,60 @@
   // restore sp
   __ leave();
   __ bind(L);
 }
 
-static void gen_c2i_adapter(MacroAssembler *masm,
-                            int total_args_passed,
-                            int comp_args_on_stack,
-                            const BasicType *sig_bt,
-                            const VMRegPair *regs,
-                            Label& skip_fixup) {
-  // Before we get into the guts of the C2I adapter, see if we should be here
-  // at all.  We've come from compiled code and are attempting to jump to the
-  // interpreter, which means the caller made a static call to get here
-  // (vcalls always get a compiled target if there is one).  Check for a
-  // compiled target.  If there is one, we need to patch the caller's call.
-  patch_callers_callsite(masm);
-
-  __ bind(skip_fixup);
-
-  int words_pushed = 0;
-
-  // Since all args are passed on the stack, total_args_passed *
-  // Interpreter::stackElementSize is the space we need.
+// For each value type argument, sig includes the list of fields of
+// the value type. This utility function computes the number of
+// arguments for the call if value types are passed by reference (the
+// calling convention the interpreter expects).
+static int compute_total_args_passed_int(const GrowableArray<SigEntry>* sig_extended) {
+  int total_args_passed = 0;
+  if (InlineTypePassFieldsAsArgs) {
+     for (int i = 0; i < sig_extended->length(); i++) {
+       BasicType bt = sig_extended->at(i)._bt;
+       if (SigEntry::is_reserved_entry(sig_extended, i)) {
+         // Ignore reserved entry
+       } else if (bt == T_VALUETYPE) {
+         // In sig_extended, a value type argument starts with:
+         // T_VALUETYPE, followed by the types of the fields of the
+         // value type and T_VOID to mark the end of the value
+         // type. Value types are flattened so, for instance, in the
+         // case of a value type with an int field and a value type
+         // field that itself has 2 fields, an int and a long:
+         // T_VALUETYPE T_INT T_VALUETYPE T_INT T_LONG T_VOID (second
+         // slot for the T_LONG) T_VOID (inner T_VALUETYPE) T_VOID
+         // (outer T_VALUETYPE)
+         total_args_passed++;
+         int vt = 1;
+         do {
+           i++;
+           BasicType bt = sig_extended->at(i)._bt;
+           BasicType prev_bt = sig_extended->at(i-1)._bt;
+           if (bt == T_VALUETYPE) {
+             vt++;
+           } else if (bt == T_VOID &&
+                      prev_bt != T_LONG &&
+                      prev_bt != T_DOUBLE) {
+             vt--;
+           }
+         } while (vt != 0);
+       } else {
+         total_args_passed++;
+       }
+     }
+  } else {
+    total_args_passed = sig_extended->length();
+  }
 
-  int extraspace = total_args_passed * Interpreter::stackElementSize;
+  return total_args_passed;
+}
 
-  __ mov(r13, sp);
-
-  // stack is aligned, keep it that way
+
   extraspace = align_up(extraspace, 2*wordSize);
 
-  if (extraspace)
-    __ sub(sp, sp, extraspace);
-
-  // Now write the args into the outgoing interpreter space
-  for (int i = 0; i < total_args_passed; i++) {
-    if (sig_bt[i] == T_VOID) {
-      assert(i > 0 && (sig_bt[i-1] == T_LONG || sig_bt[i-1] == T_DOUBLE), "missing half");
-      continue;
-    }
-
-    // offset to start parameters
-    int st_off   = (total_args_passed - i - 1) * Interpreter::stackElementSize;
-    int next_off = st_off - Interpreter::stackElementSize;
+    assert(bt != T_VALUETYPE || !InlineTypePassFieldsAsArgs, "no inline type here");
 
     // Say 4 args:
     // i   st_off
     // 0   32 T_LONG
     // 1   24 T_VOID
@@ -405,94 +501,222 @@
     // However to make thing extra confusing. Because we can fit a long/double in
     // a single slot on a 64 bt vm and it would be silly to break them up, the interpreter
     // leaves one slot empty and only stores to a single slot. In this case the
     // slot that is occupied is the T_VOID slot. See I said it was confusing.
 
-    VMReg r_1 = regs[i].first();
-    VMReg r_2 = regs[i].second();
+    // int next_off = st_off - Interpreter::stackElementSize;
+
+    VMReg r_1 = reg_pair.first();
+    VMReg r_2 = reg_pair.second();
+
     if (!r_1->is_valid()) {
       assert(!r_2->is_valid(), "");
-      continue;
+      return;
     }
+
     if (r_1->is_stack()) {
       // memory to memory use rscratch1
-      int ld_off = (r_1->reg2stack() * VMRegImpl::stack_slot_size
-                    + extraspace
-                    + words_pushed * wordSize);
+      // words_pushed is always 0 so we don't use it.
+      int ld_off = (r_1->reg2stack() * VMRegImpl::stack_slot_size + extraspace /* + word_pushed * wordSize */);
       if (!r_2->is_valid()) {
         // sign extend??
         __ ldrw(rscratch1, Address(sp, ld_off));
-        __ str(rscratch1, Address(sp, st_off));
+        __ str(rscratch1, to);
 
       } else {
-
-        __ ldr(rscratch1, Address(sp, ld_off));
-
-        // Two VMREgs|OptoRegs can be T_OBJECT, T_ADDRESS, T_DOUBLE, T_LONG
-        // T_DOUBLE and T_LONG use two slots in the interpreter
-        if ( sig_bt[i] == T_LONG || sig_bt[i] == T_DOUBLE) {
-          // ld_off == LSW, ld_off+wordSize == MSW
-          // st_off == MSW, next_off == LSW
-          __ str(rscratch1, Address(sp, next_off));
-#ifdef ASSERT
-          // Overwrite the unused slot with known junk
-          __ mov(rscratch1, 0xdeadffffdeadaaaaul);
-          __ str(rscratch1, Address(sp, st_off));
-#endif /* ASSERT */
-        } else {
-          __ str(rscratch1, Address(sp, st_off));
+        __ ldr(rscratch1, Address(sp, ld_off));
         }
       }
     } else if (r_1->is_Register()) {
       Register r = r_1->as_Register();
-      if (!r_2->is_valid()) {
-        // must be only an int (or less ) so move only 32bits to slot
-        // why not sign extend??
-        __ str(r, Address(sp, st_off));
-      } else {
-        // Two VMREgs|OptoRegs can be T_OBJECT, T_ADDRESS, T_DOUBLE, T_LONG
-        // T_DOUBLE and T_LONG use two slots in the interpreter
-        if ( sig_bt[i] == T_LONG || sig_bt[i] == T_DOUBLE) {
-          // long/double in gpr
-#ifdef ASSERT
-          // Overwrite the unused slot with known junk
-          __ mov(rscratch1, 0xdeadffffdeadaaabul);
-          __ str(rscratch1, Address(sp, st_off));
-#endif /* ASSERT */
-          __ str(r, Address(sp, next_off));
-        } else {
-          __ str(r, Address(sp, st_off));
-        }
-      }
+      __ str(r, to);
     } else {
       assert(r_1->is_FloatRegister(), "");
       if (!r_2->is_valid()) {
         // only a float use just part of the slot
-        __ strs(r_1->as_FloatRegister(), Address(sp, st_off));
+        __ strs(r_1->as_FloatRegister(), to);
       } else {
-#ifdef ASSERT
-        // Overwrite the unused slot with known junk
-        __ mov(rscratch1, 0xdeadffffdeadaaacul);
-        __ str(rscratch1, Address(sp, st_off));
-#endif /* ASSERT */
-        __ strd(r_1->as_FloatRegister(), Address(sp, next_off));
+        __ strd(r_1->as_FloatRegister(), to);
       }
+   }
+}
+
+static void gen_c2i_adapter(MacroAssembler *masm,
+                            const GrowableArray<SigEntry>* sig_extended,
+                            const VMRegPair *regs,
+                            Label& skip_fixup,
+                            address start,
+                            OopMapSet* oop_maps,
+                            int& frame_complete,
+                            int& frame_size_in_words,
+                            bool alloc_value_receiver) {
+
+  // Before we get into the guts of the C2I adapter, see if we should be here
+  // at all.  We've come from compiled code and are attempting to jump to the
+  // interpreter, which means the caller made a static call to get here
+  // (vcalls always get a compiled target if there is one).  Check for a
+  // compiled target.  If there is one, we need to patch the caller's call.
+  patch_callers_callsite(masm);
+
+  __ bind(skip_fixup);
+
+  bool has_value_argument = false;
+
+  if (InlineTypePassFieldsAsArgs) {
+      // Is there an inline type argument?
+     for (int i = 0; i < sig_extended->length() && !has_value_argument; i++) {
+       has_value_argument = (sig_extended->at(i)._bt == T_VALUETYPE);
+     }
+     if (has_value_argument) {
+      // There is at least a value type argument: we're coming from
+      // compiled code so we have no buffers to back the value
+      // types. Allocate the buffers here with a runtime call.
+      OopMap* map = RegisterSaver::save_live_registers(masm, 0, &frame_size_in_words);
+
+      frame_complete = __ offset();
+      address the_pc = __ pc();
+
+      __ set_last_Java_frame(noreg, noreg, the_pc, rscratch1);
+
+      __ mov(c_rarg0, rthread);
+      __ mov(c_rarg1, r1);
+      __ mov(c_rarg2, (int64_t)alloc_value_receiver);
+
+      __ lea(rscratch1, RuntimeAddress(CAST_FROM_FN_PTR(address, SharedRuntime::allocate_value_types)));
+      __ blr(rscratch1);
+
+      oop_maps->add_gc_map((int)(__ pc() - start), map);
+      __ reset_last_Java_frame(false);
+
+      RegisterSaver::restore_live_registers(masm);
+
+      Label no_exception;
+      __ ldr(r0, Address(rthread, Thread::pending_exception_offset()));
+      __ cbz(r0, no_exception);
+
+      __ str(zr, Address(rthread, JavaThread::vm_result_offset()));
+      __ ldr(r0, Address(rthread, Thread::pending_exception_offset()));
+      __ b(RuntimeAddress(StubRoutines::forward_exception_entry()));
+
+      __ bind(no_exception);
+
+      // We get an array of objects from the runtime call
+      __ get_vm_result(r10, rthread);
+      __ get_vm_result_2(r1, rthread); // TODO: required to keep the callee Method live?
+    }
+  }
+
+  int words_pushed = 0;
+
+  // Since all args are passed on the stack, total_args_passed *
+  // Interpreter::stackElementSize is the space we need.
+
+  int total_args_passed = compute_total_args_passed_int(sig_extended);
+  int extraspace = (total_args_passed * Interpreter::stackElementSize) + wordSize;
+
+  // stack is aligned, keep it that way
+  extraspace = align_up(extraspace, 2 * wordSize);
+
+  __ mov(r13, sp);
+
+  if (extraspace)
+    __ sub(sp, sp, extraspace);
+
+  // Now write the args into the outgoing interpreter space
+
+  int ignored = 0, next_vt_arg = 0, next_arg_int = 0;
+  bool has_oop_field = false;
+
+  for (int next_arg_comp = 0; next_arg_comp < total_args_passed; next_arg_comp++) {
+    BasicType bt = sig_extended->at(next_arg_comp)._bt;
+    // offset to start parameters
+    int st_off   = (total_args_passed - next_arg_int - 1) * Interpreter::stackElementSize;
+
+    if (!InlineTypePassFieldsAsArgs || bt != T_VALUETYPE) {
+
+            if (SigEntry::is_reserved_entry(sig_extended, next_arg_comp)) {
+               continue; // Ignore reserved entry
+            }
+
+            if (bt == T_VOID) {
+               assert(next_arg_comp > 0 && (sig_extended->at(next_arg_comp - 1)._bt == T_LONG || sig_extended->at(next_arg_comp - 1)._bt == T_DOUBLE), "missing half");
+               next_arg_int ++;
+               continue;
+             }
+
+             int next_off = st_off - Interpreter::stackElementSize;
+             int offset = (bt == T_LONG || bt == T_DOUBLE) ? next_off : st_off;
+
+             gen_c2i_adapter_helper(masm, bt, regs[next_arg_comp], extraspace, Address(sp, offset));
+             next_arg_int ++;
+   } else {
+       ignored++;
+      // get the buffer from the just allocated pool of buffers
+      int index = arrayOopDesc::base_offset_in_bytes(T_OBJECT) + next_vt_arg * type2aelembytes(T_VALUETYPE);
+      __ load_heap_oop(rscratch1, Address(r10, index));
+      next_vt_arg++;
+      next_arg_int++;
+      int vt = 1;
+      // write fields we get from compiled code in registers/stack
+      // slots to the buffer: we know we are done with that value type
+      // argument when we hit the T_VOID that acts as an end of value
+      // type delimiter for this value type. Value types are flattened
+      // so we might encounter embedded value types. Each entry in
+      // sig_extended contains a field offset in the buffer.
+      do {
+        next_arg_comp++;
+        BasicType bt = sig_extended->at(next_arg_comp)._bt;
+        BasicType prev_bt = sig_extended->at(next_arg_comp - 1)._bt;
+        if (bt == T_VALUETYPE) {
+          vt++;
+          ignored++;
+        } else if (bt == T_VOID && prev_bt != T_LONG && prev_bt != T_DOUBLE) {
+          vt--;
+          ignored++;
+        } else if (SigEntry::is_reserved_entry(sig_extended, next_arg_comp)) {
+          // Ignore reserved entry
+        } else {
+          int off = sig_extended->at(next_arg_comp)._offset;
+          assert(off > 0, "offset in object should be positive");
+
+          bool is_oop = (bt == T_OBJECT || bt == T_ARRAY);
+          has_oop_field = has_oop_field || is_oop;
+
+          gen_c2i_adapter_helper(masm, bt, regs[next_arg_comp - ignored], extraspace, Address(r11, off));
+        }
+      } while (vt != 0);
+      // pass the buffer to the interpreter
+      __ str(rscratch1, Address(sp, st_off));
+   }
+
+  }
+
+// If a value type was allocated and initialized, apply post barrier to all oop fields
+  if (has_value_argument && has_oop_field) {
+    __ push(r13); // save senderSP
+    __ push(r1); // save callee
+    // Allocate argument register save area
+    if (frame::arg_reg_save_area_bytes != 0) {
+      __ sub(sp, sp, frame::arg_reg_save_area_bytes);
     }
+    __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::apply_post_barriers), rthread, r10);
+    // De-allocate argument register save area
+    if (frame::arg_reg_save_area_bytes != 0) {
+      __ add(sp, sp, frame::arg_reg_save_area_bytes);
+    }
+    __ pop(r1); // restore callee
+    __ pop(r13); // restore sender SP
   }
 
   __ mov(esp, sp); // Interp expects args on caller's expression stack
 
   __ ldr(rscratch1, Address(rmethod, in_bytes(Method::interpreter_entry_offset())));
   __ br(rscratch1);
 }
 
+void SharedRuntime::gen_i2c_adapter(MacroAssembler *masm, int comp_args_on_stack, const GrowableArray<SigEntry>* sig, const VMRegPair *regs) {
 
-void SharedRuntime::gen_i2c_adapter(MacroAssembler *masm,
-                                    int total_args_passed,
-                                    int comp_args_on_stack,
-                                    const BasicType *sig_bt,
-                                    const VMRegPair *regs) {
 
   // Note: r13 contains the senderSP on entry. We must preserve it since
   // we may do a i2c -> c2i transition if we lose a race where compiled
   // code goes non-entrant while we get args ready.
 
@@ -548,14 +772,15 @@
     __ block_comment("} verify_i2ce ");
 #endif
   }
 
   // Cut-out for having no stack args.
-  int comp_words_on_stack = align_up(comp_args_on_stack*VMRegImpl::stack_slot_size, wordSize)>>LogBytesPerWord;
+  int comp_words_on_stack = 0;
   if (comp_args_on_stack) {
-    __ sub(rscratch1, sp, comp_words_on_stack * wordSize);
-    __ andr(sp, rscratch1, -16);
+     comp_words_on_stack = align_up(comp_args_on_stack * VMRegImpl::stack_slot_size, wordSize) >> LogBytesPerWord;
+     __ sub(rscratch1, sp, comp_words_on_stack * wordSize);
+     __ andr(sp, rscratch1, -16);
   }
 
   // Will jump to the compiled code just as if compiled code was doing it.
   // Pre-load the register-jump target early, to schedule it better.
   __ ldr(rscratch1, Address(rmethod, in_bytes(Method::from_compiled_offset())));
@@ -570,22 +795,26 @@
     __ str(zr, Address(rthread, in_bytes(JavaThread::jvmci_alternate_call_target_offset())));
     __ bind(no_alternative_target);
   }
 #endif // INCLUDE_JVMCI
 
+  int total_args_passed = sig->length();
+
   // Now generate the shuffle code.
   for (int i = 0; i < total_args_passed; i++) {
-    if (sig_bt[i] == T_VOID) {
-      assert(i > 0 && (sig_bt[i-1] == T_LONG || sig_bt[i-1] == T_DOUBLE), "missing half");
+    BasicType bt = sig->at(i)._bt;
+
+    assert(bt != T_VALUETYPE, "i2c adapter doesn't unpack value args");
+    if (bt == T_VOID) {
+      assert(i > 0 && (sig->at(i - 1)._bt == T_LONG || sig->at(i - 1)._bt == T_DOUBLE), "missing half");
       continue;
     }
 
     // Pick up 0, 1 or 2 words from SP+offset.
+    assert(!regs[i].second()->is_valid() || regs[i].first()->next() == regs[i].second(), "scrambled load targets?");
 
-    assert(!regs[i].second()->is_valid() || regs[i].first()->next() == regs[i].second(),
-            "scrambled load targets?");
-    // Load in argument order going down.
+    // Load in argument order going down.
     int ld_off = (total_args_passed - i - 1)*Interpreter::stackElementSize;
     // Point to interpreter value (vs. tag)
     int next_off = ld_off - Interpreter::stackElementSize;
     //
     //
@@ -596,11 +825,11 @@
       assert(!r_2->is_valid(), "");
       continue;
     }
     if (r_1->is_stack()) {
       // Convert stack slot to an SP offset (+ wordSize to account for return address )
-      int st_off = regs[i].first()->reg2stack()*VMRegImpl::stack_slot_size;
+      int st_off = regs[i].first()->reg2stack() * VMRegImpl::stack_slot_size;
       if (!r_2->is_valid()) {
         // sign extend???
         __ ldrsw(rscratch2, Address(esp, ld_off));
         __ str(rscratch2, Address(sp, st_off));
       } else {
@@ -613,43 +842,42 @@
         //
         // Interpreter local[n] == MSW, local[n+1] == LSW however locals
         // are accessed as negative so LSW is at LOW address
 
         // ld_off is MSW so get LSW
-        const int offset = (sig_bt[i]==T_LONG||sig_bt[i]==T_DOUBLE)?
-                           next_off : ld_off;
+        const int offset = (bt == T_LONG || bt == T_DOUBLE) ? next_off : ld_off;
         __ ldr(rscratch2, Address(esp, offset));
         // st_off is LSW (i.e. reg.first())
-        __ str(rscratch2, Address(sp, st_off));
-      }
-    } else if (r_1->is_Register()) {  // Register argument
-      Register r = r_1->as_Register();
-      if (r_2->is_valid()) {
-        //
-        // We are using two VMRegs. This can be either T_OBJECT,
-        // T_ADDRESS, T_LONG, or T_DOUBLE the interpreter allocates
-        // two slots but only uses one for thr T_LONG or T_DOUBLE case
-        // So we must adjust where to pick up the data to match the
-        // interpreter.
+         __ str(rscratch2, Address(sp, st_off));
+       }
+     } else if (r_1->is_Register()) {  // Register argument
+       Register r = r_1->as_Register();
+       if (r_2->is_valid()) {
+         //
+         // We are using two VMRegs. This can be either T_OBJECT,
+         // T_ADDRESS, T_LONG, or T_DOUBLE the interpreter allocates
+         // two slots but only uses one for thr T_LONG or T_DOUBLE case
+         // So we must adjust where to pick up the data to match the
+         // interpreter.
+
+        const int offset = (bt == T_LONG || bt == T_DOUBLE) ? next_off : ld_off;
+
+         // this can be a misaligned move
+         __ ldr(r, Address(esp, offset));
+       } else {
+         // sign extend and use a full word?
+         __ ldrw(r, Address(esp, ld_off));
+       }
+     } else {
+       if (!r_2->is_valid()) {
+         __ ldrs(r_1->as_FloatRegister(), Address(esp, ld_off));
+       } else {
+         __ ldrd(r_1->as_FloatRegister(), Address(esp, next_off));
+       }
+     }
+   }
 
-        const int offset = (sig_bt[i]==T_LONG||sig_bt[i]==T_DOUBLE)?
-                           next_off : ld_off;
-
-        // this can be a misaligned move
-        __ ldr(r, Address(esp, offset));
-      } else {
-        // sign extend and use a full word?
-        __ ldrw(r, Address(esp, ld_off));
-      }
-    } else {
-      if (!r_2->is_valid()) {
-        __ ldrs(r_1->as_FloatRegister(), Address(esp, ld_off));
-      } else {
-        __ ldrd(r_1->as_FloatRegister(), Address(esp, next_off));
-      }
-    }
-  }
 
   // 6243940 We might end up in handle_wrong_method if
   // the callee is deoptimized as we race thru here. If that
   // happens we don't want to take a safepoint because the
   // caller frame will look interpreted and arguments are now
@@ -658,27 +886,14 @@
   // we try and find the callee by normal means a safepoint
   // is possible. So we stash the desired callee in the thread
   // and the vm will find there should this case occur.
 
   __ str(rmethod, Address(rthread, JavaThread::callee_target_offset()));
-
   __ br(rscratch1);
 }
 
-// ---------------------------------------------------------------
-AdapterHandlerEntry* SharedRuntime::generate_i2c2i_adapters(MacroAssembler *masm,
-                                                            int total_args_passed,
-                                                            int comp_args_on_stack,
-                                                            const BasicType *sig_bt,
-                                                            const VMRegPair *regs,
-                                                            AdapterFingerPrint* fingerprint) {
-  address i2c_entry = __ pc();
-
-  gen_i2c_adapter(masm, total_args_passed, comp_args_on_stack, sig_bt, regs);
-
-  address c2i_unverified_entry = __ pc();
-  Label skip_fixup;
+static void gen_inline_cache_check(MacroAssembler *masm, Label& skip_fixup) {
 
   Label ok;
 
   Register holder = rscratch2;
   Register receiver = j_rarg0;
@@ -709,39 +924,99 @@
     __ ldr(rscratch1, Address(rmethod, in_bytes(Method::code_offset())));
     __ cbz(rscratch1, skip_fixup);
     __ far_jump(RuntimeAddress(SharedRuntime::get_ic_miss_stub()));
     __ block_comment("} c2i_unverified_entry");
   }
+}
+
+
+// ---------------------------------------------------------------
+AdapterHandlerEntry* SharedRuntime::generate_i2c2i_adapters(MacroAssembler *masm,
+                                                            int comp_args_on_stack,
+                                                            const GrowableArray<SigEntry>* sig,
+                                                            const VMRegPair* regs,
+                                                            const GrowableArray<SigEntry>* sig_cc,
+                                                            const VMRegPair* regs_cc,
+                                                            const GrowableArray<SigEntry>* sig_cc_ro,
+                                                            const VMRegPair* regs_cc_ro,
+                                                            AdapterFingerPrint* fingerprint,
+                                                            AdapterBlob*& new_adapter) {
+
+  address i2c_entry = __ pc();
+  gen_i2c_adapter(masm, comp_args_on_stack, sig, regs);
+
+  address c2i_unverified_entry = __ pc();
+  Label skip_fixup;
+
+  gen_inline_cache_check(masm, skip_fixup);
+
+  OopMapSet* oop_maps = new OopMapSet();
+  int frame_complete = CodeOffsets::frame_never_safe;
+  int frame_size_in_words = 0;
+
+  // Scalarized c2i adapter with non-scalarized receiver (i.e., don't pack receiver)
+  address c2i_value_ro_entry = __ pc();
+  if (regs_cc != regs_cc_ro) {
+    Label unused;
+    gen_c2i_adapter(masm, sig_cc_ro, regs_cc_ro, skip_fixup, i2c_entry, oop_maps, frame_complete, frame_size_in_words, false);
+    skip_fixup = unused;
+  }
 
+  // Scalarized c2i adapter
   address c2i_entry = __ pc();
 
   // Class initialization barrier for static methods
   address c2i_no_clinit_check_entry = NULL;
+
   if (VM_Version::supports_fast_class_init_checks()) {
     Label L_skip_barrier;
-
-    { // Bypass the barrier for non-static methods
-      __ ldrw(rscratch1, Address(rmethod, Method::access_flags_offset()));
-      __ andsw(zr, rscratch1, JVM_ACC_STATIC);
+    { // Bypass the barrier for non-static methods
+        Register flags  = rscratch1;
+      __ ldrw(flags, Address(rmethod, Method::access_flags_offset()));
+      __ tst(flags, JVM_ACC_STATIC);
       __ br(Assembler::EQ, L_skip_barrier); // non-static
     }
 
-    __ load_method_holder(rscratch2, rmethod);
-    __ clinit_barrier(rscratch2, rscratch1, &L_skip_barrier);
-    __ far_jump(RuntimeAddress(SharedRuntime::get_handle_wrong_method_stub()));
+    Register klass = rscratch1;
+    __ load_method_holder(klass, rmethod);
+    // We pass rthread to this function on x86
+    __ clinit_barrier(klass, rscratch2, &L_skip_barrier /*L_fast_path*/);
+
+    __ far_jump(RuntimeAddress(SharedRuntime::get_handle_wrong_method_stub())); // slow path
 
     __ bind(L_skip_barrier);
     c2i_no_clinit_check_entry = __ pc();
   }
 
   BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();
   bs->c2i_entry_barrier(masm);
 
   gen_c2i_adapter(masm, total_args_passed, comp_args_on_stack, sig_bt, regs, skip_fixup);
 
+  address c2i_unverified_value_entry = c2i_unverified_entry;
+
+ // Non-scalarized c2i adapter
+  address c2i_value_entry = c2i_entry;
+  if (regs != regs_cc) {
+    Label value_entry_skip_fixup;
+    c2i_unverified_value_entry = __ pc();
+    gen_inline_cache_check(masm, value_entry_skip_fixup);
+
+    c2i_value_entry = __ pc();
+    Label unused;
+    gen_c2i_adapter(masm, sig, regs, value_entry_skip_fixup, i2c_entry, oop_maps, frame_complete, frame_size_in_words, false);
+  }
+
   __ flush();
-  return AdapterHandlerLibrary::new_entry(fingerprint, i2c_entry, c2i_entry, c2i_unverified_entry, c2i_no_clinit_check_entry);
+
+  // The c2i adapter might safepoint and trigger a GC. The caller must make sure that
+  // the GC knows about the location of oop argument locations passed to the c2i adapter.
+
+  bool caller_must_gc_arguments = (regs != regs_cc);
+  new_adapter = AdapterBlob::create(masm->code(), frame_complete, frame_size_in_words + 10, oop_maps, caller_must_gc_arguments);
+
+  return AdapterHandlerLibrary::new_entry(fingerprint, i2c_entry, c2i_entry, c2i_value_entry, c2i_value_ro_entry, c2i_unverified_entry, c2i_unverified_value_entry, c2i_no_clinit_check_entry);
 }
 
 int SharedRuntime::c_calling_convention(const BasicType *sig_bt,
                                          VMRegPair *regs,
                                          VMRegPair *regs2,
@@ -780,10 +1055,11 @@
       case T_LONG:
         assert((i + 1) < total_args_passed && sig_bt[i + 1] == T_VOID, "expecting half");
         // fall through
       case T_OBJECT:
       case T_ARRAY:
+      case T_VALUETYPE:
       case T_ADDRESS:
       case T_METADATA:
         if (int_args < Argument::n_int_register_parameters_c) {
           regs[i].set2(INT_ArgReg[int_args++]->as_VMReg());
         } else {
@@ -1633,10 +1909,11 @@
           }
 #endif
           int_args++;
           break;
         }
+      case T_VALUETYPE:
       case T_OBJECT:
         assert(!is_critical_native, "no oop arguments");
         object_move(masm, map, oop_handle_offset, stack_slots, in_regs[i], out_regs[c_arg],
                     ((i == 0) && (!is_static)),
                     &receiver_offset);
@@ -1814,10 +2091,11 @@
     case T_INT:
     case T_BOOLEAN:
     case T_LONG:
       return_type = 1; break;
     case T_ARRAY:
+    case T_VALUETYPE:
     case T_OBJECT:
       return_type = 1; break;
     case T_FLOAT:
       return_type = 2; break;
     case T_DOUBLE:
@@ -1846,10 +2124,11 @@
   case T_DOUBLE :
   case T_FLOAT  :
     // Result is in v0 we'll save as needed
     break;
   case T_ARRAY:                 // Really a handle
+  case T_VALUETYPE:
   case T_OBJECT:                // Really a handle
       break; // can't de-handlize until after safepoint check
   case T_VOID: break;
   case T_LONG: break;
   default       : ShouldNotReachHere();
@@ -3071,6 +3350,111 @@
   masm->flush();
 
   // Set exception blob
   _exception_blob =  ExceptionBlob::create(&buffer, oop_maps, SimpleRuntimeFrame::framesize >> 1);
 }
+
+BufferedValueTypeBlob* SharedRuntime::generate_buffered_value_type_adapter(const ValueKlass* vk) {
+  BufferBlob* buf = BufferBlob::create("value types pack/unpack", 16 * K);
+  CodeBuffer buffer(buf);
+  short buffer_locs[20];
+  buffer.insts()->initialize_shared_locs((relocInfo*)buffer_locs,
+                                         sizeof(buffer_locs)/sizeof(relocInfo));
+
+  MacroAssembler _masm(&buffer);
+  MacroAssembler* masm = &_masm;
+
+  const Array<SigEntry>* sig_vk = vk->extended_sig();
+  const Array<VMRegPair>* regs = vk->return_regs();
+
+  int pack_fields_off = __ offset();
+
+  int j = 1;
+  for (int i = 0; i < sig_vk->length(); i++) {
+    BasicType bt = sig_vk->at(i)._bt;
+    if (bt == T_VALUETYPE) {
+      continue;
+    }
+    if (bt == T_VOID) {
+      if (sig_vk->at(i-1)._bt == T_LONG ||
+          sig_vk->at(i-1)._bt == T_DOUBLE) {
+        j++;
+      }
+      continue;
+    }
+    int off = sig_vk->at(i)._offset;
+    VMRegPair pair = regs->at(j);
+    VMReg r_1 = pair.first();
+    VMReg r_2 = pair.second();
+    Address to(r0, off);
+    if (bt == T_FLOAT) {
+      __ strs(r_1->as_FloatRegister(), to);
+    } else if (bt == T_DOUBLE) {
+      __ strd(r_1->as_FloatRegister(), to);
+    } else if (bt == T_OBJECT || bt == T_ARRAY) {
+      Register val = r_1->as_Register();
+      assert_different_registers(r0, val);
+      // We don't need barriers because the destination is a newly allocated object.
+      // Also, we cannot use store_heap_oop(to, val) because it uses r8 as tmp.
+      if (UseCompressedOops) {
+        __ encode_heap_oop(val);
+        __ str(val, to);
+      } else {
+        __ str(val, to);
+      }
+    } else {
+      assert(is_java_primitive(bt), "unexpected basic type");
+      assert_different_registers(r0, r_1->as_Register());
+      size_t size_in_bytes = type2aelembytes(bt);
+      __ store_sized_value(to, r_1->as_Register(), size_in_bytes);
+    }
+    j++;
+  }
+  assert(j == regs->length(), "missed a field?");
+
+  __ ret(lr);
+
+  int unpack_fields_off = __ offset();
+
+  j = 1;
+  for (int i = 0; i < sig_vk->length(); i++) {
+    BasicType bt = sig_vk->at(i)._bt;
+    if (bt == T_VALUETYPE) {
+      continue;
+    }
+    if (bt == T_VOID) {
+      if (sig_vk->at(i-1)._bt == T_LONG ||
+          sig_vk->at(i-1)._bt == T_DOUBLE) {
+        j++;
+      }
+      continue;
+    }
+    int off = sig_vk->at(i)._offset;
+    VMRegPair pair = regs->at(j);
+    VMReg r_1 = pair.first();
+    VMReg r_2 = pair.second();
+    Address from(r0, off);
+    if (bt == T_FLOAT) {
+      __ ldrs(r_1->as_FloatRegister(), from);
+    } else if (bt == T_DOUBLE) {
+      __ ldrd(r_1->as_FloatRegister(), from);
+    } else if (bt == T_OBJECT || bt == T_ARRAY) {
+       assert_different_registers(r0, r_1->as_Register());
+       __ load_heap_oop(r_1->as_Register(), from);
+    } else {
+      assert(is_java_primitive(bt), "unexpected basic type");
+      assert_different_registers(r0, r_1->as_Register());
+
+      size_t size_in_bytes = type2aelembytes(bt);
+      __ load_sized_value(r_1->as_Register(), from, size_in_bytes, bt != T_CHAR && bt != T_BOOLEAN);
+    }
+    j++;
+  }
+  assert(j == regs->length(), "missed a field?");
+
+  __ ret(lr);
+
+  __ flush();
+
+  return BufferedValueTypeBlob::create(&buffer, pack_fields_off, unpack_fields_off);
+}
 #endif // COMPILER2
diff a/src/hotspot/cpu/aarch64/stubGenerator_aarch64.cpp b/src/hotspot/cpu/aarch64/stubGenerator_aarch64.cpp
--- a/src/hotspot/cpu/aarch64/stubGenerator_aarch64.cpp
+++ b/src/hotspot/cpu/aarch64/stubGenerator_aarch64.cpp
@@ -302,18 +302,20 @@
     // save current address for use by exception handling code
 
     return_address = __ pc();
 
     // store result depending on type (everything that is not
-    // T_OBJECT, T_LONG, T_FLOAT or T_DOUBLE is treated as T_INT)
+    // T_OBJECT, T_VALUETYPE, T_LONG, T_FLOAT or T_DOUBLE is treated as T_INT)
     // n.b. this assumes Java returns an integral result in r0
     // and a floating result in j_farg0
     __ ldr(j_rarg2, result);
-    Label is_long, is_float, is_double, exit;
+    Label is_long, is_float, is_double, is_value, exit;
     __ ldr(j_rarg1, result_type);
     __ cmp(j_rarg1, (u1)T_OBJECT);
     __ br(Assembler::EQ, is_long);
+    __ cmp(j_rarg1, (u1)T_VALUETYPE);
+    __ br(Assembler::EQ, is_value);
     __ cmp(j_rarg1, (u1)T_LONG);
     __ br(Assembler::EQ, is_long);
     __ cmp(j_rarg1, (u1)T_FLOAT);
     __ br(Assembler::EQ, is_float);
     __ cmp(j_rarg1, (u1)T_DOUBLE);
@@ -364,10 +366,23 @@
     // leave frame and return to caller
     __ leave();
     __ ret(lr);
 
     // handle return types different from T_INT
+    __ BIND(is_value);
+    if (InlineTypeReturnedAsFields) {
+      // Check for flattened return value
+      __ cbz(r0, is_long);
+      // Initialize pre-allocated buffer
+      __ mov(r1, r0);
+      __ andr(r1, r1, -2);
+      __ ldr(r1, Address(r1, InstanceKlass::adr_valueklass_fixed_block_offset()));
+      __ ldr(r1, Address(r1, ValueKlass::pack_handler_offset()));
+      __ ldr(r0, Address(j_rarg2, 0));
+      __ blr(r1);
+      __ b(exit);
+    }
 
     __ BIND(is_long);
     __ str(r0, Address(j_rarg2, 0));
     __ br(Assembler::AL, exit);
 
@@ -1797,11 +1812,11 @@
     //     store_heap_oop(to++, copied_oop);
     //   }
     __ align(OptoLoopAlignment);
 
     __ BIND(L_store_element);
-    __ store_heap_oop(__ post(to, UseCompressedOops ? 4 : 8), copied_oop, noreg, noreg, AS_RAW);  // store the oop
+    __ store_heap_oop(__ post(to, UseCompressedOops ? 4 : 8), copied_oop, noreg, noreg, noreg, AS_RAW);  // store the oop
     __ sub(count, count, 1);
     __ cbz(count, L_do_card_marks);
 
     // ======== loop entry is here ========
     __ BIND(L_load_element);
@@ -5697,10 +5712,188 @@
     //     t0 = sub(Pm_base, Pn_base, t0, len);
     // }
   };
 
 
+  // Call here from the interpreter or compiled code to either load
+  // multiple returned values from the value type instance being
+  // returned to registers or to store returned values to a newly
+  // allocated value type instance.
+  address generate_return_value_stub(address destination, const char* name, bool has_res) {
+
+    // Information about frame layout at time of blocking runtime call.
+    // Note that we only have to preserve callee-saved registers since
+    // the compilers are responsible for supplying a continuation point
+    // if they expect all registers to be preserved.
+    // n.b. aarch64 asserts that frame::arg_reg_save_area_bytes == 0
+    enum layout {
+      rfp_off = 0, rfp_off2,
+
+      j_rarg7_off, j_rarg7_2,
+      j_rarg6_off, j_rarg6_2,
+      j_rarg5_off, j_rarg5_2,
+      j_rarg4_off, j_rarg4_2,
+      j_rarg3_off, j_rarg3_2,
+      j_rarg2_off, j_rarg2_2,
+      j_rarg1_off, j_rarg1_2,
+      j_rarg0_off, j_rarg0_2,
+
+      j_farg0_off, j_farg0_2,
+      j_farg1_off, j_farg1_2,
+      j_farg2_off, j_farg2_2,
+      j_farg3_off, j_farg3_2,
+      j_farg4_off, j_farg4_2,
+      j_farg5_off, j_farg5_2,
+      j_farg6_off, j_farg6_2,
+      j_farg7_off, j_farg7_2,
+
+      return_off, return_off2,
+      framesize // inclusive of return address
+    };
+
+    int insts_size = 512;
+    int locs_size  = 64;
+
+    CodeBuffer code(name, insts_size, locs_size);
+    OopMapSet* oop_maps  = new OopMapSet();
+    MacroAssembler* masm = new MacroAssembler(&code);
+
+    address start = __ pc();
+
+    const Address f7_save       (rfp, j_farg7_off * wordSize);
+    const Address f6_save       (rfp, j_farg6_off * wordSize);
+    const Address f5_save       (rfp, j_farg5_off * wordSize);
+    const Address f4_save       (rfp, j_farg4_off * wordSize);
+    const Address f3_save       (rfp, j_farg3_off * wordSize);
+    const Address f2_save       (rfp, j_farg2_off * wordSize);
+    const Address f1_save       (rfp, j_farg1_off * wordSize);
+    const Address f0_save       (rfp, j_farg0_off * wordSize);
+
+    const Address r0_save      (rfp, j_rarg0_off * wordSize);
+    const Address r1_save      (rfp, j_rarg1_off * wordSize);
+    const Address r2_save      (rfp, j_rarg2_off * wordSize);
+    const Address r3_save      (rfp, j_rarg3_off * wordSize);
+    const Address r4_save      (rfp, j_rarg4_off * wordSize);
+    const Address r5_save      (rfp, j_rarg5_off * wordSize);
+    const Address r6_save      (rfp, j_rarg6_off * wordSize);
+    const Address r7_save      (rfp, j_rarg7_off * wordSize);
+
+    // Generate oop map
+    OopMap* map = new OopMap(framesize, 0);
+
+    map->set_callee_saved(VMRegImpl::stack2reg(rfp_off), rfp->as_VMReg());
+    map->set_callee_saved(VMRegImpl::stack2reg(j_rarg7_off), j_rarg7->as_VMReg());
+    map->set_callee_saved(VMRegImpl::stack2reg(j_rarg6_off), j_rarg6->as_VMReg());
+    map->set_callee_saved(VMRegImpl::stack2reg(j_rarg5_off), j_rarg5->as_VMReg());
+    map->set_callee_saved(VMRegImpl::stack2reg(j_rarg4_off), j_rarg4->as_VMReg());
+    map->set_callee_saved(VMRegImpl::stack2reg(j_rarg3_off), j_rarg3->as_VMReg());
+    map->set_callee_saved(VMRegImpl::stack2reg(j_rarg2_off), j_rarg2->as_VMReg());
+    map->set_callee_saved(VMRegImpl::stack2reg(j_rarg1_off), j_rarg1->as_VMReg());
+    map->set_callee_saved(VMRegImpl::stack2reg(j_rarg0_off), j_rarg0->as_VMReg());
+
+    map->set_callee_saved(VMRegImpl::stack2reg(j_farg0_off), j_farg0->as_VMReg());
+    map->set_callee_saved(VMRegImpl::stack2reg(j_farg1_off), j_farg1->as_VMReg());
+    map->set_callee_saved(VMRegImpl::stack2reg(j_farg2_off), j_farg2->as_VMReg());
+    map->set_callee_saved(VMRegImpl::stack2reg(j_farg3_off), j_farg3->as_VMReg());
+    map->set_callee_saved(VMRegImpl::stack2reg(j_farg4_off), j_farg4->as_VMReg());
+    map->set_callee_saved(VMRegImpl::stack2reg(j_farg5_off), j_farg5->as_VMReg());
+    map->set_callee_saved(VMRegImpl::stack2reg(j_farg6_off), j_farg6->as_VMReg());
+    map->set_callee_saved(VMRegImpl::stack2reg(j_farg7_off), j_farg7->as_VMReg());
+
+    // This is an inlined and slightly modified version of call_VM
+    // which has the ability to fetch the return PC out of
+    // thread-local storage and also sets up last_Java_sp slightly
+    // differently than the real call_VM
+
+    __ enter(); // Save FP and LR before call
+
+    assert(is_even(framesize/2), "sp not 16-byte aligned");
+
+    // lr and fp are already in place
+    __ sub(sp, rfp, ((unsigned)framesize - 4) << LogBytesPerInt); // prolog
+
+    __ strd(j_farg7, f7_save);
+    __ strd(j_farg6, f6_save);
+    __ strd(j_farg5, f5_save);
+    __ strd(j_farg4, f4_save);
+    __ strd(j_farg3, f3_save);
+    __ strd(j_farg2, f2_save);
+    __ strd(j_farg1, f1_save);
+    __ strd(j_farg0, f0_save);
+
+    __ str(j_rarg0, r0_save);
+    __ str(j_rarg1, r1_save);
+    __ str(j_rarg2, r2_save);
+    __ str(j_rarg3, r3_save);
+    __ str(j_rarg4, r4_save);
+    __ str(j_rarg5, r5_save);
+    __ str(j_rarg6, r6_save);
+    __ str(j_rarg7, r7_save);
+
+    int frame_complete = __ pc() - start;
+
+    // Set up last_Java_sp and last_Java_fp
+    address the_pc = __ pc();
+    __ set_last_Java_frame(sp, rfp, the_pc, rscratch1);
+
+    // Call runtime
+    __ mov(c_rarg0, rthread);
+    __ mov(c_rarg1, r0);
+
+    BLOCK_COMMENT("call runtime_entry");
+    __ mov(rscratch1, destination);
+    __ blr(rscratch1);
+
+    oop_maps->add_gc_map(the_pc - start, map);
+
+    __ reset_last_Java_frame(false);
+    __ maybe_isb();
+
+    __ ldrd(j_farg7, f7_save);
+    __ ldrd(j_farg6, f6_save);
+    __ ldrd(j_farg5, f5_save);
+    __ ldrd(j_farg4, f4_save);
+    __ ldrd(j_farg3, f3_save);
+    __ ldrd(j_farg3, f2_save);
+    __ ldrd(j_farg1, f1_save);
+    __ ldrd(j_farg0, f0_save);
+
+    __ ldr(j_rarg0, r0_save);
+    __ ldr(j_rarg1, r1_save);
+    __ ldr(j_rarg2, r2_save);
+    __ ldr(j_rarg3, r3_save);
+    __ ldr(j_rarg4, r4_save);
+    __ ldr(j_rarg5, r5_save);
+    __ ldr(j_rarg6, r6_save);
+    __ ldr(j_rarg7, r7_save);
+
+    __ leave();
+
+    // check for pending exceptions
+    Label pending;
+    __ ldr(rscratch1, Address(rthread, in_bytes(Thread::pending_exception_offset())));
+    __ cmp(rscratch1, (u1)NULL_WORD);
+    __ br(Assembler::NE, pending);
+
+    if (has_res) {
+      __ get_vm_result(r0, rthread);
+    }
+    __ ret(lr);
+
+    __ bind(pending);
+    __ ldr(r0, Address(rthread, in_bytes(Thread::pending_exception_offset())));
+    __ far_jump(RuntimeAddress(StubRoutines::forward_exception_entry()));
+
+
+    // codeBlob framesize is in words (not VMRegImpl::slot_size)
+    int frame_size_in_words = (framesize >> (LogBytesPerWord - LogBytesPerInt));
+    RuntimeStub* stub =
+      RuntimeStub::new_runtime_stub(name, &code, frame_complete, frame_size_in_words, oop_maps, false);
+
+    return stub->entry_point();
+  }
+
   // Initialization
   void generate_initial() {
     // Generate initial stubs and initializes the entry points
 
     // entry points that exist in all platforms Note: This is code
@@ -5746,10 +5939,16 @@
     }
 
     if (vmIntrinsics::is_intrinsic_available(vmIntrinsics::_dcos)) {
       StubRoutines::_dcos = generate_dsin_dcos(/* isCos = */ true);
     }
+
+
+    StubRoutines::_load_value_type_fields_in_regs =
+         generate_return_value_stub(CAST_FROM_FN_PTR(address, SharedRuntime::load_value_type_fields_in_regs), "load_value_type_fields_in_regs", false);
+    StubRoutines::_store_value_type_fields_to_buf =
+         generate_return_value_stub(CAST_FROM_FN_PTR(address, SharedRuntime::store_value_type_fields_to_buf), "store_value_type_fields_to_buf", true);
   }
 
   void generate_all() {
     // support for verify_oop (must happen after universe_init)
     StubRoutines::_verify_oop_subroutine_entry     = generate_verify_oop();
diff a/src/hotspot/cpu/x86/templateTable_x86.cpp b/src/hotspot/cpu/x86/templateTable_x86.cpp
--- a/src/hotspot/cpu/x86/templateTable_x86.cpp
+++ b/src/hotspot/cpu/x86/templateTable_x86.cpp
@@ -31,10 +31,11 @@
 #include "interpreter/templateTable.hpp"
 #include "memory/universe.hpp"
 #include "oops/methodData.hpp"
 #include "oops/objArrayKlass.hpp"
 #include "oops/oop.inline.hpp"
+#include "oops/valueKlass.hpp"
 #include "prims/methodHandles.hpp"
 #include "runtime/frame.inline.hpp"
 #include "runtime/safepointMechanism.hpp"
 #include "runtime/sharedRuntime.hpp"
 #include "runtime/stubRoutines.hpp"
@@ -152,11 +153,11 @@
 static void do_oop_store(InterpreterMacroAssembler* _masm,
                          Address dst,
                          Register val,
                          DecoratorSet decorators = 0) {
   assert(val == noreg || val == rax, "parameter is just for looks");
-  __ store_heap_oop(dst, val, rdx, rbx, decorators);
+  __ store_heap_oop(dst, val, rdx, rbx, noreg, decorators);
 }
 
 static void do_oop_load(InterpreterMacroAssembler* _masm,
                         Address src,
                         Register dst,
@@ -175,10 +176,11 @@
                                    int byte_no) {
   if (!RewriteBytecodes)  return;
   Label L_patch_done;
 
   switch (bc) {
+  case Bytecodes::_fast_qputfield:
   case Bytecodes::_fast_aputfield:
   case Bytecodes::_fast_bputfield:
   case Bytecodes::_fast_zputfield:
   case Bytecodes::_fast_cputfield:
   case Bytecodes::_fast_dputfield:
@@ -367,10 +369,11 @@
   const int base_offset = ConstantPool::header_size() * wordSize;
   const int tags_offset = Array<u1>::base_offset_in_bytes();
 
   // get type
   __ movzbl(rdx, Address(rax, rbx, Address::times_1, tags_offset));
+  __ andl(rdx, ~JVM_CONSTANT_QDescBit);
 
   // unresolved class - get the resolved class
   __ cmpl(rdx, JVM_CONSTANT_UnresolvedClass);
   __ jccb(Assembler::equal, call_ldc);
 
@@ -817,19 +820,37 @@
                     noreg, noreg);
 }
 
 void TemplateTable::aaload() {
   transition(itos, atos);
-  // rax: index
-  // rdx: array
-  index_check(rdx, rax); // kills rbx
-  do_oop_load(_masm,
-              Address(rdx, rax,
-                      UseCompressedOops ? Address::times_4 : Address::times_ptr,
-                      arrayOopDesc::base_offset_in_bytes(T_OBJECT)),
-              rax,
-              IS_ARRAY);
+  Register array = rdx;
+  Register index = rax;
+
+  index_check(array, index); // kills rbx
+  __ profile_array(rbx, array, rcx);
+  if (ValueArrayFlatten) {
+    Label is_flat_array, done;
+    __ test_flattened_array_oop(array, rbx, is_flat_array);
+    do_oop_load(_masm,
+                Address(array, index,
+                        UseCompressedOops ? Address::times_4 : Address::times_ptr,
+                        arrayOopDesc::base_offset_in_bytes(T_OBJECT)),
+                rax,
+                IS_ARRAY);
+    __ jmp(done);
+    __ bind(is_flat_array);
+    __ read_flattened_element(array, index, rbx, rcx, rax);
+    __ bind(done);
+  } else {
+    do_oop_load(_masm,
+                Address(array, index,
+                        UseCompressedOops ? Address::times_4 : Address::times_ptr,
+                        arrayOopDesc::base_offset_in_bytes(T_OBJECT)),
+                rax,
+                IS_ARRAY);
+  }
+  __ profile_element(rbx, rax, rcx);
 }
 
 void TemplateTable::baload() {
   transition(itos, itos);
   // rax: index
@@ -1111,11 +1132,11 @@
                              arrayOopDesc::base_offset_in_bytes(T_DOUBLE)),
                      noreg /* dtos */, noreg, noreg);
 }
 
 void TemplateTable::aastore() {
-  Label is_null, ok_is_subtype, done;
+  Label is_null, is_flat_array, ok_is_subtype, done;
   transition(vtos, vtos);
   // stack: ..., array, index, value
   __ movptr(rax, at_tos());    // value
   __ movl(rcx, at_tos_p1()); // index
   __ movptr(rdx, at_tos_p2()); // array
@@ -1123,23 +1144,34 @@
   Address element_address(rdx, rcx,
                           UseCompressedOops? Address::times_4 : Address::times_ptr,
                           arrayOopDesc::base_offset_in_bytes(T_OBJECT));
 
   index_check_without_pop(rdx, rcx);     // kills rbx
+
+  __ profile_array(rdi, rdx, rbx);
+  __ profile_element(rdi, rax, rbx);
+
   __ testptr(rax, rax);
   __ jcc(Assembler::zero, is_null);
 
+  // Move array class to rdi
+  __ load_klass(rdi, rdx);
+  if (ValueArrayFlatten) {
+    __ movl(rbx, Address(rdi, Klass::layout_helper_offset()));
+    __ test_flattened_array_layout(rbx, is_flat_array);
+  }
+
   // Move subklass into rbx
   __ load_klass(rbx, rax);
-  // Move superklass into rax
-  __ load_klass(rax, rdx);
-  __ movptr(rax, Address(rax,
+  // Move array element superklass into rax
+  __ movptr(rax, Address(rdi,
                          ObjArrayKlass::element_klass_offset()));
 
   // Generate subtype check.  Blows rcx, rdi
   // Superklass in rax.  Subklass in rbx.
-  __ gen_subtype_check(rbx, ok_is_subtype);
+  // is "rbx <: rax" ? (value subclass <: array element superclass)
+  __ gen_subtype_check(rbx, ok_is_subtype, false);
 
   // Come here on failure
   // object is at TOS
   __ jump(ExternalAddress(Interpreter::_throw_ArrayStoreException_entry));
 
@@ -1153,15 +1185,59 @@
   do_oop_store(_masm, element_address, rax, IS_ARRAY);
   __ jmp(done);
 
   // Have a NULL in rax, rdx=array, ecx=index.  Store NULL at ary[idx]
   __ bind(is_null);
-  __ profile_null_seen(rbx);
+  if (EnableValhalla) {
+    Label is_null_into_value_array_npe, store_null;
 
+    // No way to store null in null-free array
+    __ test_null_free_array_oop(rdx, rbx, is_null_into_value_array_npe);
+    __ jmp(store_null);
+
+    __ bind(is_null_into_value_array_npe);
+    __ jump(ExternalAddress(Interpreter::_throw_NullPointerException_entry));
+
+    __ bind(store_null);
+  }
   // Store a NULL
   do_oop_store(_masm, element_address, noreg, IS_ARRAY);
+  __ jmp(done);
+
+  if (EnableValhalla) {
+    Label is_type_ok;
+    __ bind(is_flat_array); // Store non-null value to flat
+
+    // Simplistic type check...
 
+    // Profile the not-null value's klass.
+    __ load_klass(rbx, rax);
+    // Move element klass into rax
+    __ movptr(rax, Address(rdi, ArrayKlass::element_klass_offset()));
+    // flat value array needs exact type match
+    // is "rax == rbx" (value subclass == array element superclass)
+    __ cmpptr(rax, rbx);
+    __ jccb(Assembler::equal, is_type_ok);
+
+    __ jump(ExternalAddress(Interpreter::_throw_ArrayStoreException_entry));
+
+    __ bind(is_type_ok);
+    // rbx: value's klass
+    // rdx: array
+    // rdi: array klass
+    __ test_klass_is_empty_value(rbx, rax, done);
+
+    // calc dst for copy
+    __ movl(rax, at_tos_p1()); // index
+    __ data_for_value_array_index(rdx, rdi, rax, rax);
+
+    // ...and src for copy
+    __ movptr(rcx, at_tos());  // value
+    __ data_for_oop(rcx, rcx, rbx);
+
+    __ access_value_copy(IN_HEAP, rcx, rax, rbx);
+  }
   // Pop stack arguments
   __ bind(done);
   __ addptr(rsp, 3 * Interpreter::stackElementSize);
 }
 
@@ -2403,19 +2479,65 @@
 }
 
 void TemplateTable::if_acmp(Condition cc) {
   transition(atos, vtos);
   // assume branch is more often taken than not (loops use backward branches)
-  Label not_taken;
+  Label taken, not_taken;
   __ pop_ptr(rdx);
+
+  const int is_value_mask = markWord::always_locked_pattern;
+  if (EnableValhalla) {
+    __ cmpoop(rdx, rax);
+    __ jcc(Assembler::equal, (cc == equal) ? taken : not_taken);
+
+    // might be substitutable, test if either rax or rdx is null
+    __ movptr(rbx, rdx);
+    __ andptr(rbx, rax);
+    __ testptr(rbx, rbx);
+    __ jcc(Assembler::zero, (cc == equal) ? not_taken : taken);
+
+    // and both are values ?
+    __ movptr(rbx, Address(rdx, oopDesc::mark_offset_in_bytes()));
+    __ andptr(rbx, is_value_mask);
+    __ movptr(rcx, Address(rax, oopDesc::mark_offset_in_bytes()));
+    __ andptr(rbx, is_value_mask);
+    __ andptr(rbx, rcx);
+    __ cmpl(rbx, is_value_mask);
+    __ jcc(Assembler::notEqual, (cc == equal) ? not_taken : taken);
+
+    // same value klass ?
+    __ load_metadata(rbx, rdx);
+    __ load_metadata(rcx, rax);
+    __ cmpptr(rbx, rcx);
+    __ jcc(Assembler::notEqual, (cc == equal) ? not_taken : taken);
+
+    // Know both are the same type, let's test for substitutability...
+    if (cc == equal) {
+      invoke_is_substitutable(rax, rdx, taken, not_taken);
+    } else {
+      invoke_is_substitutable(rax, rdx, not_taken, taken);
+    }
+    __ stop("Not reachable");
+  }
+
   __ cmpoop(rdx, rax);
   __ jcc(j_not(cc), not_taken);
+  __ bind(taken);
   branch(false, false);
   __ bind(not_taken);
   __ profile_not_taken_branch(rax);
 }
 
+void TemplateTable::invoke_is_substitutable(Register aobj, Register bobj,
+                                            Label& is_subst, Label& not_subst) {
+  __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::is_substitutable), aobj, bobj);
+  // Restored...rax answer, jmp to outcome...
+  __ testl(rax, rax);
+  __ jcc(Assembler::zero, not_subst);
+  __ jmp(is_subst);
+}
+
 void TemplateTable::ret() {
   transition(vtos, vtos);
   locals_index(rbx);
   LP64_ONLY(__ movslq(rbx, iaddress(rbx))); // get return bci, compute return bcp
   NOT_LP64(__ movptr(rbx, iaddress(rbx)));
@@ -2677,11 +2799,12 @@
   // Need to narrow in the return bytecode rather than in generate_return_entry
   // since compiled code callers expect the result to already be narrowed.
   if (state == itos) {
     __ narrow(rax);
   }
-  __ remove_activation(state, rbcp);
+
+  __ remove_activation(state, rbcp, true, true, true);
 
   __ jmp(rbcp);
 }
 
 // ----------------------------------------------------------------------------
@@ -2875,41 +2998,50 @@
   const Register index = rdx;
   const Register obj   = LP64_ONLY(c_rarg3) NOT_LP64(rcx);
   const Register off   = rbx;
   const Register flags = rax;
   const Register bc    = LP64_ONLY(c_rarg3) NOT_LP64(rcx); // uses same reg as obj, so don't mix them
+  const Register flags2 = rdx;
 
   resolve_cache_and_index(byte_no, cache, index, sizeof(u2));
   jvmti_post_field_access(cache, index, is_static, false);
   load_field_cp_cache_entry(obj, cache, index, off, flags, is_static);
 
-  if (!is_static) pop_and_check_object(obj);
-
   const Address field(obj, off, Address::times_1, 0*wordSize);
 
-  Label Done, notByte, notBool, notInt, notShort, notChar, notLong, notFloat, notObj;
+  Label Done, notByte, notBool, notInt, notShort, notChar, notLong, notFloat, notObj, notValueType;
+
+  if (!is_static) {
+    __ movptr(rcx, Address(cache, index, Address::times_ptr,
+                           in_bytes(ConstantPoolCache::base_offset() +
+                                    ConstantPoolCacheEntry::f1_offset())));
+  }
+
+  __ movl(flags2, flags);
 
   __ shrl(flags, ConstantPoolCacheEntry::tos_state_shift);
   // Make sure we don't need to mask edx after the above shift
   assert(btos == 0, "change code, btos != 0");
 
   __ andl(flags, ConstantPoolCacheEntry::tos_state_mask);
 
   __ jcc(Assembler::notZero, notByte);
   // btos
+  if (!is_static) pop_and_check_object(obj);
   __ access_load_at(T_BYTE, IN_HEAP, rax, field, noreg, noreg);
   __ push(btos);
   // Rewrite bytecode to be faster
   if (!is_static && rc == may_rewrite) {
     patch_bytecode(Bytecodes::_fast_bgetfield, bc, rbx);
   }
   __ jmp(Done);
 
   __ bind(notByte);
+
   __ cmpl(flags, ztos);
   __ jcc(Assembler::notEqual, notBool);
-
+   if (!is_static) pop_and_check_object(obj);
   // ztos (same code as btos)
   __ access_load_at(T_BOOLEAN, IN_HEAP, rax, field, noreg, noreg);
   __ push(ztos);
   // Rewrite bytecode to be faster
   if (!is_static && rc == may_rewrite) {
@@ -2920,18 +3052,97 @@
 
   __ bind(notBool);
   __ cmpl(flags, atos);
   __ jcc(Assembler::notEqual, notObj);
   // atos
-  do_oop_load(_masm, field, rax);
-  __ push(atos);
-  if (!is_static && rc == may_rewrite) {
-    patch_bytecode(Bytecodes::_fast_agetfield, bc, rbx);
+  if (!EnableValhalla) {
+    if (!is_static) pop_and_check_object(obj);
+    do_oop_load(_masm, field, rax);
+    __ push(atos);
+    if (!is_static && rc == may_rewrite) {
+      patch_bytecode(Bytecodes::_fast_agetfield, bc, rbx);
+    }
+    __ jmp(Done);
+  } else {
+    if (is_static) {
+      __ load_heap_oop(rax, field);
+      Label isFlattenable, uninitialized;
+      // Issue below if the static field has not been initialized yet
+      __ test_field_is_flattenable(flags2, rscratch1, isFlattenable);
+        // Not flattenable case
+        __ push(atos);
+        __ jmp(Done);
+      // Flattenable case, must not return null even if uninitialized
+      __ bind(isFlattenable);
+        __ testptr(rax, rax);
+        __ jcc(Assembler::zero, uninitialized);
+          __ push(atos);
+          __ jmp(Done);
+        __ bind(uninitialized);
+          __ andl(flags2, ConstantPoolCacheEntry::field_index_mask);
+#ifdef _LP64
+          Label slow_case, finish;
+          __ cmpb(Address(rcx, InstanceKlass::init_state_offset()), InstanceKlass::fully_initialized);
+          __ jcc(Assembler::notEqual, slow_case);
+        __ get_default_value_oop(rcx, off, rax);
+        __ jmp(finish);
+        __ bind(slow_case);
+#endif // LP64
+          __ call_VM(rax, CAST_FROM_FN_PTR(address, InterpreterRuntime::uninitialized_static_value_field),
+                 obj, flags2);
+#ifdef _LP64
+          __ bind(finish);
+#endif // _LP64
+          __ verify_oop(rax);
+          __ push(atos);
+          __ jmp(Done);
+    } else {
+      Label isFlattened, nonnull, isFlattenable, rewriteFlattenable;
+      __ test_field_is_flattenable(flags2, rscratch1, isFlattenable);
+        // Non-flattenable field case, also covers the object case
+        pop_and_check_object(obj);
+        __ load_heap_oop(rax, field);
+        __ push(atos);
+        if (rc == may_rewrite) {
+          patch_bytecode(Bytecodes::_fast_agetfield, bc, rbx);
+        }
+        __ jmp(Done);
+      __ bind(isFlattenable);
+        __ test_field_is_flattened(flags2, rscratch1, isFlattened);
+          // Non-flattened field case
+          __ movptr(rax, rcx);  // small dance required to preserve the klass_holder somewhere
+          pop_and_check_object(obj);
+          __ push(rax);
+          __ load_heap_oop(rax, field);
+          __ pop(rcx);
+          __ testptr(rax, rax);
+          __ jcc(Assembler::notZero, nonnull);
+            __ andl(flags2, ConstantPoolCacheEntry::field_index_mask);
+            __ get_value_field_klass(rcx, flags2, rbx);
+            __ get_default_value_oop(rbx, rcx, rax);
+          __ bind(nonnull);
+          __ verify_oop(rax);
+          __ push(atos);
+          __ jmp(rewriteFlattenable);
+        __ bind(isFlattened);
+          __ andl(flags2, ConstantPoolCacheEntry::field_index_mask);
+          pop_and_check_object(rax);
+          __ read_flattened_field(rcx, flags2, rbx, rax);
+          __ verify_oop(rax);
+          __ push(atos);
+      __ bind(rewriteFlattenable);
+      if (rc == may_rewrite) {
+        patch_bytecode(Bytecodes::_fast_qgetfield, bc, rbx);
+      }
+      __ jmp(Done);
+    }
   }
-  __ jmp(Done);
 
   __ bind(notObj);
+
+  if (!is_static) pop_and_check_object(obj);
+
   __ cmpl(flags, itos);
   __ jcc(Assembler::notEqual, notInt);
   // itos
   __ access_load_at(T_INT, IN_HEAP, rax, field, noreg, noreg);
   __ push(itos);
@@ -3027,10 +3238,25 @@
 
 void TemplateTable::getstatic(int byte_no) {
   getfield_or_static(byte_no, true);
 }
 
+void TemplateTable::withfield() {
+  transition(vtos, atos);
+
+  Register cache = LP64_ONLY(c_rarg1) NOT_LP64(rcx);
+  Register index = LP64_ONLY(c_rarg2) NOT_LP64(rdx);
+
+  resolve_cache_and_index(f2_byte, cache, index, sizeof(u2));
+
+  call_VM(rbx, CAST_FROM_FN_PTR(address, InterpreterRuntime::withfield), cache);
+  // new value type is returned in rbx
+  // stack adjustement is returned in rax
+  __ verify_oop(rbx);
+  __ addptr(rsp, rax);
+  __ movptr(rax, rbx);
+}
 
 // The registers cache and index expected to be set before call.
 // The function may destroy various registers, just not the cache and index registers.
 void TemplateTable::jvmti_post_field_mod(Register cache, Register index, bool is_static) {
 
@@ -3122,10 +3348,11 @@
   const Register cache = rcx;
   const Register index = rdx;
   const Register obj   = rcx;
   const Register off   = rbx;
   const Register flags = rax;
+  const Register flags2 = rdx;
 
   resolve_cache_and_index(byte_no, cache, index, sizeof(u2));
   jvmti_post_field_mod(cache, index, is_static);
   load_field_cp_cache_entry(obj, cache, index, off, flags, is_static);
 
@@ -3138,32 +3365,33 @@
   __ shrl(rdx, ConstantPoolCacheEntry::is_volatile_shift);
   __ andl(rdx, 0x1);
 
   // Check for volatile store
   __ testl(rdx, rdx);
+  __ movl(flags2, flags);
   __ jcc(Assembler::zero, notVolatile);
 
-  putfield_or_static_helper(byte_no, is_static, rc, obj, off, flags);
+  putfield_or_static_helper(byte_no, is_static, rc, obj, off, flags, flags2);
   volatile_barrier(Assembler::Membar_mask_bits(Assembler::StoreLoad |
                                                Assembler::StoreStore));
   __ jmp(Done);
   __ bind(notVolatile);
 
-  putfield_or_static_helper(byte_no, is_static, rc, obj, off, flags);
+  putfield_or_static_helper(byte_no, is_static, rc, obj, off, flags, flags2);
 
   __ bind(Done);
 }
 
 void TemplateTable::putfield_or_static_helper(int byte_no, bool is_static, RewriteControl rc,
-                                              Register obj, Register off, Register flags) {
+                                              Register obj, Register off, Register flags, Register flags2) {
 
   // field addresses
   const Address field(obj, off, Address::times_1, 0*wordSize);
   NOT_LP64( const Address hi(obj, off, Address::times_1, 1*wordSize);)
 
   Label notByte, notBool, notInt, notShort, notChar,
-        notLong, notFloat, notObj;
+        notLong, notFloat, notObj, notValueType;
   Label Done;
 
   const Register bc    = LP64_ONLY(c_rarg3) NOT_LP64(rcx);
 
   __ shrl(flags, ConstantPoolCacheEntry::tos_state_shift);
@@ -3202,18 +3430,63 @@
   __ cmpl(flags, atos);
   __ jcc(Assembler::notEqual, notObj);
 
   // atos
   {
-    __ pop(atos);
-    if (!is_static) pop_and_check_object(obj);
-    // Store into the field
-    do_oop_store(_masm, field, rax);
-    if (!is_static && rc == may_rewrite) {
-      patch_bytecode(Bytecodes::_fast_aputfield, bc, rbx, true, byte_no);
+    if (!EnableValhalla) {
+      __ pop(atos);
+      if (!is_static) pop_and_check_object(obj);
+      // Store into the field
+      do_oop_store(_masm, field, rax);
+      if (!is_static && rc == may_rewrite) {
+        patch_bytecode(Bytecodes::_fast_aputfield, bc, rbx, true, byte_no);
+      }
+      __ jmp(Done);
+    } else {
+      __ pop(atos);
+      if (is_static) {
+        Label notFlattenable, notBuffered;
+        __ test_field_is_not_flattenable(flags2, rscratch1, notFlattenable);
+        __ null_check(rax);
+        __ bind(notFlattenable);
+        do_oop_store(_masm, field, rax);
+        __ jmp(Done);
+      } else {
+        Label isFlattenable, isFlattened, notBuffered, notBuffered2, rewriteNotFlattenable, rewriteFlattenable;
+        __ test_field_is_flattenable(flags2, rscratch1, isFlattenable);
+        // Not flattenable case, covers not flattenable values and objects
+        pop_and_check_object(obj);
+        // Store into the field
+        do_oop_store(_masm, field, rax);
+        __ bind(rewriteNotFlattenable);
+        if (rc == may_rewrite) {
+          patch_bytecode(Bytecodes::_fast_aputfield, bc, rbx, true, byte_no);
+        }
+        __ jmp(Done);
+        // Implementation of the flattenable semantic
+        __ bind(isFlattenable);
+        __ null_check(rax);
+        __ test_field_is_flattened(flags2, rscratch1, isFlattened);
+        // Not flattened case
+        pop_and_check_object(obj);
+        // Store into the field
+        do_oop_store(_masm, field, rax);
+        __ jmp(rewriteFlattenable);
+        __ bind(isFlattened);
+        pop_and_check_object(obj);
+        assert_different_registers(rax, rdx, obj, off);
+        __ load_klass(rdx, rax);
+        __ data_for_oop(rax, rax, rdx);
+        __ addptr(obj, off);
+        __ access_value_copy(IN_HEAP, rax, obj, rdx);
+        __ bind(rewriteFlattenable);
+        if (rc == may_rewrite) {
+          patch_bytecode(Bytecodes::_fast_qputfield, bc, rbx, true, byte_no);
+        }
+        __ jmp(Done);
+      }
     }
-    __ jmp(Done);
   }
 
   __ bind(notObj);
   __ cmpl(flags, itos);
   __ jcc(Assembler::notEqual, notInt);
@@ -3348,10 +3621,11 @@
     __ push_ptr(rbx);                 // put the object pointer back on tos
     // Save tos values before call_VM() clobbers them. Since we have
     // to do it for every data type, we use the saved values as the
     // jvalue object.
     switch (bytecode()) {          // load values into the jvalue object
+    case Bytecodes::_fast_qputfield: //fall through
     case Bytecodes::_fast_aputfield: __ push_ptr(rax); break;
     case Bytecodes::_fast_bputfield: // fall through
     case Bytecodes::_fast_zputfield: // fall through
     case Bytecodes::_fast_sputfield: // fall through
     case Bytecodes::_fast_cputfield: // fall through
@@ -3373,10 +3647,11 @@
     // c_rarg3: jvalue object on the stack
     LP64_ONLY(__ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::post_field_modification), rbx, c_rarg2, c_rarg3));
     NOT_LP64(__ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::post_field_modification), rbx, rax, rcx));
 
     switch (bytecode()) {             // restore tos values
+    case Bytecodes::_fast_qputfield: // fall through
     case Bytecodes::_fast_aputfield: __ pop_ptr(rax); break;
     case Bytecodes::_fast_bputfield: // fall through
     case Bytecodes::_fast_zputfield: // fall through
     case Bytecodes::_fast_sputfield: // fall through
     case Bytecodes::_fast_cputfield: // fall through
@@ -3412,10 +3687,14 @@
   // [jk] not needed currently
   // volatile_barrier(Assembler::Membar_mask_bits(Assembler::LoadStore |
   //                                              Assembler::StoreStore));
 
   Label notVolatile, Done;
+  if (bytecode() == Bytecodes::_fast_qputfield) {
+    __ movl(rscratch2, rdx);  // saving flags for isFlattened test
+  }
+
   __ shrl(rdx, ConstantPoolCacheEntry::is_volatile_shift);
   __ andl(rdx, 0x1);
 
   // Get object from stack
   pop_and_check_object(rcx);
@@ -3425,27 +3704,52 @@
 
   // Check for volatile store
   __ testl(rdx, rdx);
   __ jcc(Assembler::zero, notVolatile);
 
-  fast_storefield_helper(field, rax);
+  if (bytecode() == Bytecodes::_fast_qputfield) {
+    __ movl(rdx, rscratch2);  // restoring flags for isFlattened test
+  }
+  fast_storefield_helper(field, rax, rdx);
   volatile_barrier(Assembler::Membar_mask_bits(Assembler::StoreLoad |
                                                Assembler::StoreStore));
   __ jmp(Done);
   __ bind(notVolatile);
 
-  fast_storefield_helper(field, rax);
+  if (bytecode() == Bytecodes::_fast_qputfield) {
+    __ movl(rdx, rscratch2);  // restoring flags for isFlattened test
+  }
+  fast_storefield_helper(field, rax, rdx);
 
   __ bind(Done);
 }
 
-void TemplateTable::fast_storefield_helper(Address field, Register rax) {
+void TemplateTable::fast_storefield_helper(Address field, Register rax, Register flags) {
 
   // access field
   switch (bytecode()) {
+  case Bytecodes::_fast_qputfield:
+    {
+      Label isFlattened, done;
+      __ null_check(rax);
+      __ test_field_is_flattened(flags, rscratch1, isFlattened);
+      // No Flattened case
+      do_oop_store(_masm, field, rax);
+      __ jmp(done);
+      __ bind(isFlattened);
+      // Flattened case
+      __ load_klass(rdx, rax);
+      __ data_for_oop(rax, rax, rdx);
+      __ lea(rcx, field);
+      __ access_value_copy(IN_HEAP, rax, rcx, rdx);
+      __ bind(done);
+    }
+    break;
   case Bytecodes::_fast_aputfield:
-    do_oop_store(_masm, field, rax);
+    {
+      do_oop_store(_masm, field, rax);
+    }
     break;
   case Bytecodes::_fast_lputfield:
 #ifdef _LP64
     __ access_store_at(T_LONG, IN_HEAP, field, noreg /* ltos */, noreg, noreg);
 #else
@@ -3511,21 +3815,59 @@
   //                      in_bytes(ConstantPoolCache::base_offset() +
   //                               ConstantPoolCacheEntry::flags_offset())));
   // __ shrl(rdx, ConstantPoolCacheEntry::is_volatile_shift);
   // __ andl(rdx, 0x1);
   //
-  __ movptr(rbx, Address(rcx, rbx, Address::times_ptr,
+  __ movptr(rdx, Address(rcx, rbx, Address::times_ptr,
                          in_bytes(ConstantPoolCache::base_offset() +
                                   ConstantPoolCacheEntry::f2_offset())));
 
   // rax: object
   __ verify_oop(rax);
   __ null_check(rax);
-  Address field(rax, rbx, Address::times_1);
+  Address field(rax, rdx, Address::times_1);
 
   // access field
   switch (bytecode()) {
+  case Bytecodes::_fast_qgetfield:
+    {
+      Label isFlattened, nonnull, Done;
+      __ movptr(rscratch1, Address(rcx, rbx, Address::times_ptr,
+                                   in_bytes(ConstantPoolCache::base_offset() +
+                                            ConstantPoolCacheEntry::flags_offset())));
+      __ test_field_is_flattened(rscratch1, rscratch2, isFlattened);
+        // Non-flattened field case
+        __ load_heap_oop(rax, field);
+        __ testptr(rax, rax);
+        __ jcc(Assembler::notZero, nonnull);
+          __ movl(rdx, Address(rcx, rbx, Address::times_ptr,
+                             in_bytes(ConstantPoolCache::base_offset() +
+                                      ConstantPoolCacheEntry::flags_offset())));
+          __ andl(rdx, ConstantPoolCacheEntry::field_index_mask);
+          __ movptr(rcx, Address(rcx, rbx, Address::times_ptr,
+                                       in_bytes(ConstantPoolCache::base_offset() +
+                                                ConstantPoolCacheEntry::f1_offset())));
+          __ get_value_field_klass(rcx, rdx, rbx);
+          __ get_default_value_oop(rbx, rcx, rax);
+        __ bind(nonnull);
+        __ verify_oop(rax);
+        __ jmp(Done);
+      __ bind(isFlattened);
+        __ push(rdx); // save offset
+        __ movl(rdx, Address(rcx, rbx, Address::times_ptr,
+                           in_bytes(ConstantPoolCache::base_offset() +
+                                    ConstantPoolCacheEntry::flags_offset())));
+        __ andl(rdx, ConstantPoolCacheEntry::field_index_mask);
+        __ movptr(rcx, Address(rcx, rbx, Address::times_ptr,
+                                     in_bytes(ConstantPoolCache::base_offset() +
+                                              ConstantPoolCacheEntry::f1_offset())));
+        __ pop(rbx); // restore offset
+        __ read_flattened_field(rcx, rdx, rbx, rax);
+      __ bind(Done);
+      __ verify_oop(rax);
+    }
+    break;
   case Bytecodes::_fast_agetfield:
     do_oop_load(_masm, field, rax);
     __ verify_oop(rax);
     break;
   case Bytecodes::_fast_lgetfield:
@@ -3993,156 +4335,102 @@
 
 void TemplateTable::_new() {
   transition(vtos, atos);
   __ get_unsigned_2_byte_index_at_bcp(rdx, 1);
   Label slow_case;
-  Label slow_case_no_pop;
-  Label done;
-  Label initialize_header;
+  Label done;
   Label initialize_object;  // including clearing the fields
 
   __ get_cpool_and_tags(rcx, rax);
 
   // Make sure the class we're about to instantiate has been resolved.
   // This is done before loading InstanceKlass to be consistent with the order
   // how Constant Pool is updated (see ConstantPool::klass_at_put)
   const int tags_offset = Array<u1>::base_offset_in_bytes();
   __ cmpb(Address(rax, rdx, Address::times_1, tags_offset), JVM_CONSTANT_Class);
-  __ jcc(Assembler::notEqual, slow_case_no_pop);
+  __ jcc(Assembler::notEqual, slow_case);
 
   // get InstanceKlass
   __ load_resolved_klass_at_index(rcx, rcx, rdx);
-  __ push(rcx);  // save the contexts of klass for initializing the header
+
+  __ cmpb(Address(rcx, InstanceKlass::kind_offset()), InstanceKlass::_kind_inline_type);
+  __ jcc(Assembler::notEqual, is_not_value);
+
+  __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::throw_InstantiationError));
+
+  __ bind(is_not_value);
 
   // make sure klass is initialized & doesn't have finalizer
-  // make sure klass is fully initialized
   __ cmpb(Address(rcx, InstanceKlass::init_state_offset()), InstanceKlass::fully_initialized);
   __ jcc(Assembler::notEqual, slow_case);
 
-  // get instance_size in InstanceKlass (scaled to a count of bytes)
-  __ movl(rdx, Address(rcx, Klass::layout_helper_offset()));
-  // test to see if it has a finalizer or is malformed in some way
-  __ testl(rdx, Klass::_lh_instance_slow_path_bit);
-  __ jcc(Assembler::notZero, slow_case);
-
-  // Allocate the instance:
-  //  If TLAB is enabled:
-  //    Try to allocate in the TLAB.
-  //    If fails, go to the slow path.
-  //  Else If inline contiguous allocations are enabled:
-  //    Try to allocate in eden.
-  //    If fails due to heap end, go to slow path.
-  //
-  //  If TLAB is enabled OR inline contiguous is enabled:
-  //    Initialize the allocation.
-  //    Exit.
-  //
-  //  Go to slow path.
+  __ allocate_instance(rcx, rax, rdx, rbx, true, slow_case);
+  __ jmp(done);
 
-  const bool allow_shared_alloc =
-    Universe::heap()->supports_inline_contig_alloc();
+  // slow case
+  __ bind(slow_case);
 
-  const Register thread = LP64_ONLY(r15_thread) NOT_LP64(rcx);
-#ifndef _LP64
-  if (UseTLAB || allow_shared_alloc) {
-    __ get_thread(thread);
-  }
-#endif // _LP64
+  Register rarg1 = LP64_ONLY(c_rarg1) NOT_LP64(rax);
+  Register rarg2 = LP64_ONLY(c_rarg2) NOT_LP64(rdx);
 
-  if (UseTLAB) {
-    __ tlab_allocate(thread, rax, rdx, 0, rcx, rbx, slow_case);
-    if (ZeroTLAB) {
-      // the fields have been already cleared
-      __ jmp(initialize_header);
-    } else {
-      // initialize both the header and fields
-      __ jmp(initialize_object);
-    }
-  } else {
-    // Allocation in the shared Eden, if allowed.
-    //
-    // rdx: instance size in bytes
-    __ eden_allocate(thread, rax, rdx, 0, rbx, slow_case);
-  }
+  __ get_constant_pool(rarg1);
+  __ get_unsigned_2_byte_index_at_bcp(rarg2, 1);
+  call_VM(rax, CAST_FROM_FN_PTR(address, InterpreterRuntime::_new), rarg1, rarg2);
+   __ verify_oop(rax);
 
-  // If UseTLAB or allow_shared_alloc are true, the object is created above and
-  // there is an initialize need. Otherwise, skip and go to the slow path.
-  if (UseTLAB || allow_shared_alloc) {
-    // The object is initialized before the header.  If the object size is
-    // zero, go directly to the header initialization.
-    __ bind(initialize_object);
-    __ decrement(rdx, sizeof(oopDesc));
-    __ jcc(Assembler::zero, initialize_header);
-
-    // Initialize topmost object field, divide rdx by 8, check if odd and
-    // test if zero.
-    __ xorl(rcx, rcx);    // use zero reg to clear memory (shorter code)
-    __ shrl(rdx, LogBytesPerLong); // divide by 2*oopSize and set carry flag if odd
-
-    // rdx must have been multiple of 8
-#ifdef ASSERT
-    // make sure rdx was multiple of 8
-    Label L;
-    // Ignore partial flag stall after shrl() since it is debug VM
-    __ jcc(Assembler::carryClear, L);
-    __ stop("object size is not multiple of 2 - adjust this code");
-    __ bind(L);
-    // rdx must be > 0, no extra check needed here
-#endif
+  // continue
+  __ bind(done);
+}
 
-    // initialize remaining object fields: rdx was a multiple of 8
-    { Label loop;
-    __ bind(loop);
-    __ movptr(Address(rax, rdx, Address::times_8, sizeof(oopDesc) - 1*oopSize), rcx);
-    NOT_LP64(__ movptr(Address(rax, rdx, Address::times_8, sizeof(oopDesc) - 2*oopSize), rcx));
-    __ decrement(rdx);
-    __ jcc(Assembler::notZero, loop);
-    }
+void TemplateTable::defaultvalue() {
+  transition(vtos, atos);
 
-    // initialize object header only.
-    __ bind(initialize_header);
-    if (UseBiasedLocking) {
-      __ pop(rcx);   // get saved klass back in the register.
-      __ movptr(rbx, Address(rcx, Klass::prototype_header_offset()));
-      __ movptr(Address(rax, oopDesc::mark_offset_in_bytes ()), rbx);
-    } else {
-      __ movptr(Address(rax, oopDesc::mark_offset_in_bytes ()),
-                (intptr_t)markWord::prototype().value()); // header
-      __ pop(rcx);   // get saved klass back in the register.
-    }
-#ifdef _LP64
-    __ xorl(rsi, rsi); // use zero reg to clear memory (shorter code)
-    __ store_klass_gap(rax, rsi);  // zero klass gap for compressed oops
-#endif
-    __ store_klass(rax, rcx);  // klass
+  Label slow_case;
+  Label done;
+  Label is_value;
 
-    {
-      SkipIfEqual skip_if(_masm, &DTraceAllocProbes, 0);
-      // Trigger dtrace event for fastpath
-      __ push(atos);
-      __ call_VM_leaf(
-           CAST_FROM_FN_PTR(address, SharedRuntime::dtrace_object_alloc), rax);
-      __ pop(atos);
-    }
+  __ get_unsigned_2_byte_index_at_bcp(rdx, 1);
+  __ get_cpool_and_tags(rcx, rax);
 
-    __ jmp(done);
-  }
+  // Make sure the class we're about to instantiate has been resolved.
+  // This is done before loading InstanceKlass to be consistent with the order
+  // how Constant Pool is updated (see ConstantPool::klass_at_put)
+  const int tags_offset = Array<u1>::base_offset_in_bytes();
+  __ cmpb(Address(rax, rdx, Address::times_1, tags_offset), JVM_CONSTANT_Class);
+  __ jcc(Assembler::notEqual, slow_case);
+
+  // get InstanceKlass
+  __ load_resolved_klass_at_index(rcx, rcx, rdx);
+
+  __ cmpb(Address(rcx, InstanceKlass::kind_offset()), InstanceKlass::_kind_inline_type);
+  __ jcc(Assembler::equal, is_value);
+
+  // in the future, defaultvalue will just return null instead of throwing an exception
+  __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::throw_IncompatibleClassChangeError));
+
+  __ bind(is_value);
+
+  // make sure klass is fully initialized
+  __ cmpb(Address(rcx, InstanceKlass::init_state_offset()), InstanceKlass::fully_initialized);
+  __ jcc(Assembler::notEqual, slow_case);
+
+  // have a resolved ValueKlass in rcx, return the default value oop from it
+  __ get_default_value_oop(rcx, rdx, rax);
+  __ jmp(done);
 
-  // slow case
-  __ bind(slow_case);
-  __ pop(rcx);   // restore stack pointer to what it was when we came in.
-  __ bind(slow_case_no_pop);
+  __ bind(slow_case);
 
   Register rarg1 = LP64_ONLY(c_rarg1) NOT_LP64(rax);
   Register rarg2 = LP64_ONLY(c_rarg2) NOT_LP64(rdx);
 
-  __ get_constant_pool(rarg1);
-  __ get_unsigned_2_byte_index_at_bcp(rarg2, 1);
-  call_VM(rax, CAST_FROM_FN_PTR(address, InterpreterRuntime::_new), rarg1, rarg2);
+  __ get_unsigned_2_byte_index_at_bcp(rarg2, 1);
+  __ get_constant_pool(rarg1);
+
+  call_VM(rax, CAST_FROM_FN_PTR(address, InterpreterRuntime::defaultvalue),
    __ verify_oop(rax);
 
-  // continue
+  __ bind(done);
   __ bind(done);
 }
 
 void TemplateTable::newarray() {
   transition(itos, atos);
@@ -4178,14 +4466,15 @@
 
   // Get cpool & tags index
   __ get_cpool_and_tags(rcx, rdx); // rcx=cpool, rdx=tags array
   __ get_unsigned_2_byte_index_at_bcp(rbx, 1); // rbx=index
   // See if bytecode has already been quicked
-  __ cmpb(Address(rdx, rbx,
-                  Address::times_1,
-                  Array<u1>::base_offset_in_bytes()),
-          JVM_CONSTANT_Class);
+  __ movzbl(rdx, Address(rdx, rbx,
+      Address::times_1,
+      Array<u1>::base_offset_in_bytes()));
+  __ andl (rdx, ~JVM_CONSTANT_QDescBit);
+  __ cmpl(rdx, JVM_CONSTANT_Class);
   __ jcc(Assembler::equal, quicked);
   __ push(atos); // save receiver for result, and for GC
   call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::quicken_io_cc));
 
   // vm_result_2 has metadata result
@@ -4219,17 +4508,31 @@
   __ jump(ExternalAddress(Interpreter::_throw_ClassCastException_entry));
 
   // Come here on success
   __ bind(ok_is_subtype);
   __ mov(rax, rdx); // Restore object in rdx
+  __ jmp(done);
+
+  __ bind(is_null);
 
   // Collect counts on whether this check-cast sees NULLs a lot or not.
   if (ProfileInterpreter) {
-    __ jmp(done);
-    __ bind(is_null);
-    __ profile_null_seen(rcx);
-  } else {
+    __ profile_null_seen(rcx);
+  }
+
+  if (EnableValhalla) {
+    // Get cpool & tags index
+    __ get_cpool_and_tags(rcx, rdx); // rcx=cpool, rdx=tags array
+    __ get_unsigned_2_byte_index_at_bcp(rbx, 1); // rbx=index
+    // See if CP entry is a Q-descriptor
+    __ movzbl(rcx, Address(rdx, rbx,
+        Address::times_1,
+        Array<u1>::base_offset_in_bytes()));
+    __ andl (rcx, JVM_CONSTANT_QDescBit);
+    __ cmpl(rcx, JVM_CONSTANT_QDescBit);
+    __ jcc(Assembler::notEqual, done);
+    __ jump(ExternalAddress(Interpreter::_throw_NullPointerException_entry));
     __ bind(is_null);   // same as 'done'
   }
   __ bind(done);
 }
 
@@ -4241,14 +4544,15 @@
 
   // Get cpool & tags index
   __ get_cpool_and_tags(rcx, rdx); // rcx=cpool, rdx=tags array
   __ get_unsigned_2_byte_index_at_bcp(rbx, 1); // rbx=index
   // See if bytecode has already been quicked
-  __ cmpb(Address(rdx, rbx,
-                  Address::times_1,
-                  Array<u1>::base_offset_in_bytes()),
-          JVM_CONSTANT_Class);
+  __ movzbl(rdx, Address(rdx, rbx,
+        Address::times_1,
+        Array<u1>::base_offset_in_bytes()));
+  __ andl (rdx, ~JVM_CONSTANT_QDescBit);
+  __ cmpl(rdx, JVM_CONSTANT_Class);
   __ jcc(Assembler::equal, quicked);
 
   __ push(atos); // save receiver for result, and for GC
   call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::quicken_io_cc));
   // vm_result_2 has metadata result
@@ -4296,11 +4600,10 @@
   __ bind(done);
   // rax = 0: obj == NULL or  obj is not an instanceof the specified klass
   // rax = 1: obj != NULL and obj is     an instanceof the specified klass
 }
 
-
 //----------------------------------------------------------------------------------------------------
 // Breakpoints
 void TemplateTable::_breakpoint() {
   // Note: We get here even if we are single stepping..
   // jbug insists on setting breakpoints at every bytecode
@@ -4360,10 +4663,21 @@
   // check for NULL object
   __ null_check(rax);
 
   __ resolve(IS_NOT_NULL, rax);
 
+  const int is_value_mask = markWord::always_locked_pattern;
+  Label has_identity;
+  __ movptr(rbx, Address(rax, oopDesc::mark_offset_in_bytes()));
+  __ andptr(rbx, is_value_mask);
+  __ cmpl(rbx, is_value_mask);
+  __ jcc(Assembler::notEqual, has_identity);
+  __ call_VM(noreg, CAST_FROM_FN_PTR(address,
+                     InterpreterRuntime::throw_illegal_monitor_state_exception));
+  __ should_not_reach_here();
+  __ bind(has_identity);
+
   const Address monitor_block_top(
         rbp, frame::interpreter_frame_monitor_block_top_offset * wordSize);
   const Address monitor_block_bot(
         rbp, frame::interpreter_frame_initial_sp_offset * wordSize);
   const int entry_size = frame::interpreter_frame_monitor_size() * wordSize;
@@ -4459,10 +4773,21 @@
   // check for NULL object
   __ null_check(rax);
 
   __ resolve(IS_NOT_NULL, rax);
 
+  const int is_value_mask = markWord::always_locked_pattern;
+  Label has_identity;
+  __ movptr(rbx, Address(rax, oopDesc::mark_offset_in_bytes()));
+  __ andptr(rbx, is_value_mask);
+  __ cmpl(rbx, is_value_mask);
+  __ jcc(Assembler::notEqual, has_identity);
+  __ call_VM(noreg, CAST_FROM_FN_PTR(address,
+                     InterpreterRuntime::throw_illegal_monitor_state_exception));
+  __ should_not_reach_here();
+  __ bind(has_identity);
+
   const Address monitor_block_top(
         rbp, frame::interpreter_frame_monitor_block_top_offset * wordSize);
   const Address monitor_block_bot(
         rbp, frame::interpreter_frame_initial_sp_offset * wordSize);
   const int entry_size = frame::interpreter_frame_monitor_size() * wordSize;
diff a/src/hotspot/share/classfile/bytecodeAssembler.cpp b/src/hotspot/share/classfile/bytecodeAssembler.cpp
--- a/src/hotspot/share/classfile/bytecodeAssembler.cpp
+++ b/src/hotspot/share/classfile/bytecodeAssembler.cpp
@@ -184,10 +184,11 @@
     case T_SHORT:
     case T_INT:     iload(index); break;
     case T_FLOAT:   fload(index); break;
     case T_DOUBLE:  dload(index); break;
     case T_LONG:    lload(index); break;
+    case T_VALUETYPE:
     default:
       if (is_reference_type(bt)) {
                     aload(index);
                     break;
       }
@@ -253,10 +254,11 @@
     case T_SHORT:
     case T_INT:     ireturn(); break;
     case T_FLOAT:   freturn(); break;
     case T_DOUBLE:  dreturn(); break;
     case T_LONG:    lreturn(); break;
+    case T_VALUETYPE:
     case T_VOID:    _return(); break;
     default:
       if (is_reference_type(bt)) {
                     areturn();
                     break;
diff a/src/hotspot/share/classfile/classLoaderData.cpp b/src/hotspot/share/classfile/classLoaderData.cpp
--- a/src/hotspot/share/classfile/classLoaderData.cpp
+++ b/src/hotspot/share/classfile/classLoaderData.cpp
@@ -61,10 +61,11 @@
 #include "memory/metadataFactory.hpp"
 #include "memory/resourceArea.hpp"
 #include "oops/access.inline.hpp"
 #include "oops/oop.inline.hpp"
 #include "oops/oopHandle.inline.hpp"
+#include "oops/valueKlass.inline.hpp"
 #include "oops/weakHandle.inline.hpp"
 #include "runtime/atomic.hpp"
 #include "runtime/handles.inline.hpp"
 #include "runtime/mutex.hpp"
 #include "runtime/safepoint.hpp"
@@ -371,10 +372,20 @@
     }
     assert(k != k->next_link(), "no loops!");
   }
 }
 
+void ClassLoaderData::value_classes_do(void f(ValueKlass*)) {
+  // Lock-free access requires load_acquire
+  for (Klass* k = Atomic::load_acquire(&_klasses); k != NULL; k = k->next_link()) {
+    if (k->is_value()) {
+      f(ValueKlass::cast(k));
+    }
+    assert(k != k->next_link(), "no loops!");
+  }
+}
+
 void ClassLoaderData::modules_do(void f(ModuleEntry*)) {
   assert_locked_or_safepoint(Module_lock);
   if (_unnamed_module != NULL) {
     f(_unnamed_module);
   }
@@ -537,10 +548,12 @@
 
   // Some items on the _deallocate_list need to free their C heap structures
   // if they are not already on the _klasses list.
   free_deallocate_list_C_heap_structures();
 
+  value_classes_do(ValueKlass::cleanup);
+
   // Clean up class dependencies and tell serviceability tools
   // these classes are unloading.  Must be called
   // after erroneous classes are released.
   classes_do(InstanceKlass::unload_class);
 
@@ -831,11 +844,15 @@
       if (m->is_method()) {
         MetadataFactory::free_metadata(this, (Method*)m);
       } else if (m->is_constantPool()) {
         MetadataFactory::free_metadata(this, (ConstantPool*)m);
       } else if (m->is_klass()) {
-        MetadataFactory::free_metadata(this, (InstanceKlass*)m);
+        if (!((Klass*)m)->is_value()) {
+          MetadataFactory::free_metadata(this, (InstanceKlass*)m);
+        } else {
+          MetadataFactory::free_metadata(this, (ValueKlass*)m);
+        }
       } else {
         ShouldNotReachHere();
       }
     } else {
       // Metadata is alive.
diff a/src/hotspot/share/classfile/vmSymbols.hpp b/src/hotspot/share/classfile/vmSymbols.hpp
--- a/src/hotspot/share/classfile/vmSymbols.hpp
+++ b/src/hotspot/share/classfile/vmSymbols.hpp
@@ -52,19 +52,21 @@
 #define VM_SYMBOLS_DO(template, do_alias)                                                         \
   /* commonly used class, package, module names */                                                \
   template(java_base,                                 "java.base")                                \
   template(java_lang_System,                          "java/lang/System")                         \
   template(java_lang_Object,                          "java/lang/Object")                         \
+  template(java_lang_IdentityObject,                  "java/lang/IdentityObject")                 \
   template(java_lang_Class,                           "java/lang/Class")                          \
   template(java_lang_Package,                         "java/lang/Package")                        \
   template(java_lang_Module,                          "java/lang/Module")                         \
   template(java_lang_String,                          "java/lang/String")                         \
   template(java_lang_StringLatin1,                    "java/lang/StringLatin1")                   \
   template(java_lang_StringUTF16,                     "java/lang/StringUTF16")                    \
   template(java_lang_Thread,                          "java/lang/Thread")                         \
   template(java_lang_ThreadGroup,                     "java/lang/ThreadGroup")                    \
   template(java_lang_Cloneable,                       "java/lang/Cloneable")                      \
+  template(java_lang_NonTearable,                     "java/lang/NonTearable")                    \
   template(java_lang_Throwable,                       "java/lang/Throwable")                      \
   template(java_lang_ClassLoader,                     "java/lang/ClassLoader")                    \
   template(java_lang_ThreadDeath,                     "java/lang/ThreadDeath")                    \
   template(java_lang_Boolean,                         "java/lang/Boolean")                        \
   template(java_lang_Character,                       "java/lang/Character")                      \
@@ -325,12 +327,13 @@
   template(setTargetVolatile_name,                    "setTargetVolatile")                        \
   template(setTarget_signature,                       "(Ljava/lang/invoke/MethodHandle;)V")       \
   template(DEFAULT_CONTEXT_name,                      "DEFAULT_CONTEXT")                          \
   NOT_LP64(  do_alias(intptr_signature,               int_signature)  )                           \
   LP64_ONLY( do_alias(intptr_signature,               long_signature) )                           \
-                                                                                                                                      \
-  /* Support for JVMCI */                                                                                                             \
+                                                                                                  \
+                                                                                                  \
+  /* Support for JVMCI */                                                                         \
   JVMCI_VM_SYMBOLS_DO(template, do_alias)                                                         \
                                                                                                   \
   template(java_lang_StackWalker,                     "java/lang/StackWalker")                    \
   template(java_lang_StackFrameInfo,                  "java/lang/StackFrameInfo")                 \
   template(java_lang_LiveStackFrameInfo,              "java/lang/LiveStackFrameInfo")             \
@@ -450,10 +453,12 @@
   template(java_lang_Boolean_signature,               "Ljava/lang/Boolean;")                      \
   template(url_code_signer_array_void_signature,      "(Ljava/net/URL;[Ljava/security/CodeSigner;)V") \
   template(module_entry_name,                         "module_entry")                             \
   template(resolved_references_name,                  "<resolved_references>")                    \
   template(init_lock_name,                            "<init_lock>")                              \
+  template(default_value_name,                        ".default")                                 \
+  template(empty_marker_name,                         ".empty")                                   \
   template(address_size_name,                         "ADDRESS_SIZE0")                            \
   template(page_size_name,                            "PAGE_SIZE")                                \
   template(big_endian_name,                           "BIG_ENDIAN")                               \
   template(use_unaligned_access_name,                 "UNALIGNED_ACCESS")                         \
   template(data_cache_line_flush_size_name,           "DATA_CACHE_LINE_FLUSH_SIZE")               \
@@ -519,10 +524,11 @@
   template(thread_void_signature,                     "(Ljava/lang/Thread;)V")                                    \
   template(threadgroup_runnable_void_signature,       "(Ljava/lang/ThreadGroup;Ljava/lang/Runnable;)V")           \
   template(threadgroup_string_void_signature,         "(Ljava/lang/ThreadGroup;Ljava/lang/String;)V")             \
   template(string_class_signature,                    "(Ljava/lang/String;)Ljava/lang/Class;")                    \
   template(object_object_object_signature,            "(Ljava/lang/Object;Ljava/lang/Object;)Ljava/lang/Object;") \
+  template(object_object_boolean_signature,           "(Ljava/lang/Object;Ljava/lang/Object;)Z") \
   template(string_string_string_signature,            "(Ljava/lang/String;Ljava/lang/String;)Ljava/lang/String;") \
   template(string_string_signature,                   "(Ljava/lang/String;)Ljava/lang/String;")                   \
   template(classloader_string_long_signature,         "(Ljava/lang/ClassLoader;Ljava/lang/String;)J")             \
   template(byte_array_void_signature,                 "([B)V")                                                    \
   template(char_array_void_signature,                 "([C)V")                                                    \
@@ -669,10 +675,14 @@
   template(jdk_internal_loader_ClassLoaders,       "jdk/internal/loader/ClassLoaders")                            \
   template(toFileURL_name,                         "toFileURL")                                                   \
   template(toFileURL_signature,                    "(Ljava/lang/String;)Ljava/net/URL;")                          \
   template(url_void_signature,                     "(Ljava/net/URL;)V")                                           \
                                                                                                                   \
+  template(java_lang_invoke_ValueBootstrapMethods, "java/lang/invoke/ValueBootstrapMethods")                      \
+  template(isSubstitutable_name,                   "isSubstitutable0")                                            \
+                                                                                                                  \
+  template(jdk_internal_vm_jni_SubElementSelector, "jdk/internal/vm/jni/SubElementSelector")                      \
   /*end*/
 
 // Here are all the intrinsics known to the runtime and the CI.
 // Each intrinsic consists of a public enum name (like _hashCode),
 // followed by a specification of its klass, name, and signature:
@@ -1160,39 +1170,49 @@
   do_signature(putLong_signature,         "(Ljava/lang/Object;JJ)V")                                                    \
   do_signature(getFloat_signature,        "(Ljava/lang/Object;J)F")                                                     \
   do_signature(putFloat_signature,        "(Ljava/lang/Object;JF)V")                                                    \
   do_signature(getDouble_signature,       "(Ljava/lang/Object;J)D")                                                     \
   do_signature(putDouble_signature,       "(Ljava/lang/Object;JD)V")                                                    \
+  do_signature(getValue_signature,        "(Ljava/lang/Object;JLjava/lang/Class;)Ljava/lang/Object;")                   \
+  do_signature(putValue_signature,        "(Ljava/lang/Object;JLjava/lang/Class;Ljava/lang/Object;)V")                  \
                                                                                                                         \
   do_name(getReference_name,"getReference")     do_name(putReference_name,"putReference")                               \
   do_name(getBoolean_name,"getBoolean")         do_name(putBoolean_name,"putBoolean")                                   \
   do_name(getByte_name,"getByte")               do_name(putByte_name,"putByte")                                         \
   do_name(getShort_name,"getShort")             do_name(putShort_name,"putShort")                                       \
   do_name(getChar_name,"getChar")               do_name(putChar_name,"putChar")                                         \
   do_name(getInt_name,"getInt")                 do_name(putInt_name,"putInt")                                           \
   do_name(getLong_name,"getLong")               do_name(putLong_name,"putLong")                                         \
   do_name(getFloat_name,"getFloat")             do_name(putFloat_name,"putFloat")                                       \
   do_name(getDouble_name,"getDouble")           do_name(putDouble_name,"putDouble")                                     \
+  do_name(getValue_name,"getValue")             do_name(putValue_name,"putValue")                                       \
+  do_name(makePrivateBuffer_name,"makePrivateBuffer")                                                                   \
+  do_name(finishPrivateBuffer_name,"finishPrivateBuffer")                                                               \
                                                                                                                         \
   do_intrinsic(_getReference,       jdk_internal_misc_Unsafe,     getReference_name, getReference_signature,     F_RN)  \
   do_intrinsic(_getBoolean,         jdk_internal_misc_Unsafe,     getBoolean_name, getBoolean_signature,         F_RN)  \
   do_intrinsic(_getByte,            jdk_internal_misc_Unsafe,     getByte_name, getByte_signature,               F_RN)  \
   do_intrinsic(_getShort,           jdk_internal_misc_Unsafe,     getShort_name, getShort_signature,             F_RN)  \
   do_intrinsic(_getChar,            jdk_internal_misc_Unsafe,     getChar_name, getChar_signature,               F_RN)  \
   do_intrinsic(_getInt,             jdk_internal_misc_Unsafe,     getInt_name, getInt_signature,                 F_RN)  \
   do_intrinsic(_getLong,            jdk_internal_misc_Unsafe,     getLong_name, getLong_signature,               F_RN)  \
   do_intrinsic(_getFloat,           jdk_internal_misc_Unsafe,     getFloat_name, getFloat_signature,             F_RN)  \
   do_intrinsic(_getDouble,          jdk_internal_misc_Unsafe,     getDouble_name, getDouble_signature,           F_RN)  \
+  do_intrinsic(_getValue,           jdk_internal_misc_Unsafe,     getValue_name, getValue_signature,             F_RN)  \
   do_intrinsic(_putReference,       jdk_internal_misc_Unsafe,     putReference_name, putReference_signature,     F_RN)  \
   do_intrinsic(_putBoolean,         jdk_internal_misc_Unsafe,     putBoolean_name, putBoolean_signature,         F_RN)  \
   do_intrinsic(_putByte,            jdk_internal_misc_Unsafe,     putByte_name, putByte_signature,               F_RN)  \
   do_intrinsic(_putShort,           jdk_internal_misc_Unsafe,     putShort_name, putShort_signature,             F_RN)  \
   do_intrinsic(_putChar,            jdk_internal_misc_Unsafe,     putChar_name, putChar_signature,               F_RN)  \
   do_intrinsic(_putInt,             jdk_internal_misc_Unsafe,     putInt_name, putInt_signature,                 F_RN)  \
   do_intrinsic(_putLong,            jdk_internal_misc_Unsafe,     putLong_name, putLong_signature,               F_RN)  \
   do_intrinsic(_putFloat,           jdk_internal_misc_Unsafe,     putFloat_name, putFloat_signature,             F_RN)  \
   do_intrinsic(_putDouble,          jdk_internal_misc_Unsafe,     putDouble_name, putDouble_signature,           F_RN)  \
+  do_intrinsic(_putValue,           jdk_internal_misc_Unsafe,     putValue_name, putValue_signature,             F_RN)  \
+                                                                                                                        \
+  do_intrinsic(_makePrivateBuffer,  jdk_internal_misc_Unsafe,     makePrivateBuffer_name, object_object_signature, F_RN)   \
+  do_intrinsic(_finishPrivateBuffer,  jdk_internal_misc_Unsafe,   finishPrivateBuffer_name, object_object_signature, F_RN) \
                                                                                                                         \
   do_name(getReferenceVolatile_name,"getReferenceVolatile")   do_name(putReferenceVolatile_name,"putReferenceVolatile") \
   do_name(getBooleanVolatile_name,"getBooleanVolatile")       do_name(putBooleanVolatile_name,"putBooleanVolatile")     \
   do_name(getByteVolatile_name,"getByteVolatile")             do_name(putByteVolatile_name,"putByteVolatile")           \
   do_name(getShortVolatile_name,"getShortVolatile")           do_name(putShortVolatile_name,"putShortVolatile")         \
@@ -1533,11 +1553,11 @@
     #undef VM_ALIAS_ENUM
 
     FIRST_SID = NO_SID + 1
   };
   enum {
-    log2_SID_LIMIT = 10         // checked by an assert at start-up
+    log2_SID_LIMIT = 11         // checked by an assert at start-up
   };
 
  private:
   // The symbol array
   static Symbol* _symbols[];
diff a/src/hotspot/share/jvmci/vmStructs_jvmci.cpp b/src/hotspot/share/jvmci/vmStructs_jvmci.cpp
--- a/src/hotspot/share/jvmci/vmStructs_jvmci.cpp
+++ b/src/hotspot/share/jvmci/vmStructs_jvmci.cpp
@@ -156,11 +156,11 @@
                                                                                                                                      \
   nonstatic_field(InstanceKlass,               _fields,                                       Array<u2>*)                            \
   nonstatic_field(InstanceKlass,               _constants,                                    ConstantPool*)                         \
   nonstatic_field(InstanceKlass,               _init_state,                                   u1)                                    \
   nonstatic_field(InstanceKlass,               _init_thread,                                  Thread*)                               \
-  nonstatic_field(InstanceKlass,               _misc_flags,                                   u2)                                    \
+  nonstatic_field(InstanceKlass,               _misc_flags,                                   u4)                                    \
   nonstatic_field(InstanceKlass,               _annotations,                                  Annotations*)                          \
                                                                                                                                      \
   volatile_nonstatic_field(JavaFrameAnchor,    _last_Java_sp,                                 intptr_t*)                             \
   volatile_nonstatic_field(JavaFrameAnchor,    _last_Java_pc,                                 address)                               \
                                                                                                                                      \
@@ -504,10 +504,11 @@
   declare_constant(DataLayout::arg_info_data_tag)                         \
   declare_constant(DataLayout::call_type_data_tag)                        \
   declare_constant(DataLayout::virtual_call_type_data_tag)                \
   declare_constant(DataLayout::parameters_type_data_tag)                  \
   declare_constant(DataLayout::speculative_trap_data_tag)                 \
+  declare_constant(DataLayout::array_load_store_data_tag)                 \
                                                                           \
   declare_constant(Deoptimization::Unpack_deopt)                          \
   declare_constant(Deoptimization::Unpack_exception)                      \
   declare_constant(Deoptimization::Unpack_uncommon_trap)                  \
   declare_constant(Deoptimization::Unpack_reexecute)                      \
diff a/src/hotspot/share/oops/arrayKlass.cpp b/src/hotspot/share/oops/arrayKlass.cpp
--- a/src/hotspot/share/oops/arrayKlass.cpp
+++ b/src/hotspot/share/oops/arrayKlass.cpp
@@ -23,18 +23,20 @@
  */
 
 #include "precompiled.hpp"
 #include "classfile/javaClasses.hpp"
 #include "classfile/moduleEntry.hpp"
+#include "classfile/symbolTable.hpp"
 #include "classfile/systemDictionary.hpp"
 #include "classfile/vmSymbols.hpp"
 #include "gc/shared/collectedHeap.inline.hpp"
 #include "jvmtifiles/jvmti.h"
 #include "memory/metaspaceClosure.hpp"
 #include "memory/resourceArea.hpp"
 #include "memory/universe.hpp"
 #include "oops/arrayKlass.hpp"
+#include "oops/objArrayKlass.hpp"
 #include "oops/arrayOop.hpp"
 #include "oops/instanceKlass.hpp"
 #include "oops/objArrayOop.hpp"
 #include "oops/oop.inline.hpp"
 #include "runtime/handles.inline.hpp"
@@ -95,10 +97,34 @@
     set_layout_helper(Klass::_lh_neutral_value);
     set_is_cloneable(); // All arrays are considered to be cloneable (See JLS 20.1.5)
     JFR_ONLY(INIT_ID(this);)
 }
 
+Symbol* ArrayKlass::create_element_klass_array_name(Klass* element_klass, TRAPS) {
+  ResourceMark rm(THREAD);
+  Symbol* name = NULL;
+  bool is_qtype = element_klass->is_value();
+  char *name_str = element_klass->name()->as_C_string();
+  int len = element_klass->name()->utf8_length();
+  char *new_str = NEW_RESOURCE_ARRAY(char, len + 4);
+  int idx = 0;
+  new_str[idx++] = JVM_SIGNATURE_ARRAY;
+  if (element_klass->is_instance_klass()) { // it could be an array or simple type
+    if (is_qtype) {
+      new_str[idx++] = JVM_SIGNATURE_VALUETYPE;
+    } else {
+      new_str[idx++] = JVM_SIGNATURE_CLASS;
+    }
+  }
+  memcpy(&new_str[idx], name_str, len * sizeof(char));
+  idx += len;
+  if (element_klass->is_instance_klass()) {
+    new_str[idx++] = JVM_SIGNATURE_ENDCLASS;
+  }
+  new_str[idx++] = '\0';
+  return SymbolTable::new_symbol(new_str);
+}
 
 // Initialization of vtables and mirror object is done separatly from base_create_array_klass,
 // since a GC can happen. At this point all instance variables of the ArrayKlass must be setup.
 void ArrayKlass::complete_create_array_klass(ArrayKlass* k, Klass* super_klass, ModuleEntry* module_entry, TRAPS) {
   k->initialize_supers(super_klass, NULL, CHECK);
@@ -132,28 +158,23 @@
                                                                 /* do_zero */ true, CHECK_NULL);
   // initialization to NULL not necessary, area already cleared
   return o;
 }
 
-void ArrayKlass::array_klasses_do(void f(Klass* k, TRAPS), TRAPS) {
-  Klass* k = this;
-  // Iterate over this array klass and all higher dimensions
-  while (k != NULL) {
-    f(k, CHECK);
-    k = ArrayKlass::cast(k)->higher_dimension();
-  }
-}
-
 void ArrayKlass::array_klasses_do(void f(Klass* k)) {
   Klass* k = this;
   // Iterate over this array klass and all higher dimensions
   while (k != NULL) {
     f(k);
     k = ArrayKlass::cast(k)->higher_dimension();
   }
 }
 
+oop ArrayKlass::component_mirror() const {
+  return java_lang_Class::component_mirror(java_mirror());
+}
+
 // JVM support
 
 jint ArrayKlass::compute_modifier_flags(TRAPS) const {
   return JVM_ACC_ABSTRACT | JVM_ACC_FINAL | JVM_ACC_PUBLIC;
 }
diff a/src/hotspot/share/oops/constantPool.cpp b/src/hotspot/share/oops/constantPool.cpp
--- a/src/hotspot/share/oops/constantPool.cpp
+++ b/src/hotspot/share/oops/constantPool.cpp
@@ -48,10 +48,11 @@
 #include "oops/instanceKlass.hpp"
 #include "oops/objArrayKlass.hpp"
 #include "oops/objArrayOop.inline.hpp"
 #include "oops/oop.inline.hpp"
 #include "oops/typeArrayOop.inline.hpp"
+#include "oops/valueArrayKlass.hpp"
 #include "runtime/atomic.hpp"
 #include "runtime/handles.inline.hpp"
 #include "runtime/init.hpp"
 #include "runtime/javaCalls.hpp"
 #include "runtime/signature.hpp"
@@ -225,11 +226,11 @@
       break;
 #ifndef PRODUCT
     case JVM_CONSTANT_Class:
     case JVM_CONSTANT_UnresolvedClass:
     case JVM_CONSTANT_UnresolvedClassInError:
-      // All of these should have been reverted back to ClassIndex before calling
+      // All of these should have been reverted back to Unresolved before calling
       // this function.
       ShouldNotReachHere();
 #endif
     }
   }
@@ -249,14 +250,15 @@
   Klass** adr = resolved_klasses()->adr_at(resolved_klass_index);
   Atomic::release_store(adr, k);
 
   // The interpreter assumes when the tag is stored, the klass is resolved
   // and the Klass* non-NULL, so we need hardware store ordering here.
+  jbyte qdesc_bit = (name->is_Q_signature()) ? (jbyte) JVM_CONSTANT_QDescBit : 0;
   if (k != NULL) {
-    release_tag_at_put(class_index, JVM_CONSTANT_Class);
+    release_tag_at_put(class_index, JVM_CONSTANT_Class | qdesc_bit);
   } else {
-    release_tag_at_put(class_index, JVM_CONSTANT_UnresolvedClass);
+    release_tag_at_put(class_index, JVM_CONSTANT_UnresolvedClass | qdesc_bit);
   }
 }
 
 // Unsafe anonymous class support:
 void ConstantPool::klass_at_put(int class_index, Klass* k) {
@@ -266,10 +268,11 @@
   Klass** adr = resolved_klasses()->adr_at(resolved_klass_index);
   Atomic::release_store(adr, k);
 
   // The interpreter assumes when the tag is stored, the klass is resolved
   // and the Klass* non-NULL, so we need hardware store ordering here.
+  assert(!k->name()->is_Q_signature(), "Q-type without JVM_CONSTANT_QDescBit");
   release_tag_at_put(class_index, JVM_CONSTANT_Class);
 }
 
 #if INCLUDE_CDS_JAVA_HEAP
 // Archive the resolved references
@@ -466,10 +469,16 @@
                  k->external_name());
     }
   }
 }
 
+void check_is_inline_type(Klass* k, TRAPS) {
+  if (!k->is_value()) {
+    THROW(vmSymbols::java_lang_IncompatibleClassChangeError());
+  }
+}
+
 Klass* ConstantPool::klass_at_impl(const constantPoolHandle& this_cp, int which,
                                    bool save_resolution_error, TRAPS) {
   assert(THREAD->is_Java_thread(), "must be a Java thread");
   JavaThread* javaThread = (JavaThread*)THREAD;
 
@@ -500,27 +509,51 @@
     ShouldNotReachHere();
   }
 
   Handle mirror_handle;
   Symbol* name = this_cp->symbol_at(name_index);
+  bool inline_type_signature = false;
+  if (name->is_Q_signature()) {
+    name = name->fundamental_name(THREAD);
+    inline_type_signature = true;
+  }
   Handle loader (THREAD, this_cp->pool_holder()->class_loader());
   Handle protection_domain (THREAD, this_cp->pool_holder()->protection_domain());
 
   Klass* k;
   {
     // Turn off the single stepping while doing class resolution
     JvmtiHideSingleStepping jhss(javaThread);
     k = SystemDictionary::resolve_or_fail(name, loader, protection_domain, true, THREAD);
   } //  JvmtiHideSingleStepping jhss(javaThread);
+  if (inline_type_signature) {
+    name->decrement_refcount();
+  }
 
   if (!HAS_PENDING_EXCEPTION) {
     // preserve the resolved klass from unloading
     mirror_handle = Handle(THREAD, k->java_mirror());
     // Do access check for klasses
     verify_constant_pool_resolve(this_cp, k, THREAD);
   }
 
+  if (!HAS_PENDING_EXCEPTION && inline_type_signature) {
+    check_is_inline_type(k, THREAD);
+  }
+
+  if (!HAS_PENDING_EXCEPTION) {
+    Klass* bottom_klass = NULL;
+    if (k->is_objArray_klass()) {
+      bottom_klass = ObjArrayKlass::cast(k)->bottom_klass();
+      assert(bottom_klass != NULL, "Should be set");
+      assert(bottom_klass->is_instance_klass() || bottom_klass->is_typeArray_klass(), "Sanity check");
+    } else if (k->is_valueArray_klass()) {
+      bottom_klass = ValueArrayKlass::cast(k)->element_klass();
+      assert(bottom_klass != NULL, "Should be set");
+    }
+  }
+
   // Failed to resolve class. We must record the errors so that subsequent attempts
   // to resolve this constant pool entry fail with the same error (JVMS 5.4.3).
   if (HAS_PENDING_EXCEPTION) {
     if (save_resolution_error) {
       save_and_throw_exception(this_cp, which, constantTag(JVM_CONSTANT_UnresolvedClass), CHECK_NULL);
@@ -542,11 +575,15 @@
   Klass** adr = this_cp->resolved_klasses()->adr_at(resolved_klass_index);
   Atomic::release_store(adr, k);
   // The interpreter assumes when the tag is stored, the klass is resolved
   // and the Klass* stored in _resolved_klasses is non-NULL, so we need
   // hardware store ordering here.
-  this_cp->release_tag_at_put(which, JVM_CONSTANT_Class);
+  jbyte tag = JVM_CONSTANT_Class;
+  if (this_cp->tag_at(which).is_Qdescriptor_klass()) {
+    tag |= JVM_CONSTANT_QDescBit;
+  }
+  this_cp->release_tag_at_put(which, tag);
   return k;
 }
 
 
 // Does not update ConstantPool* - to avoid any exception throwing. Used
@@ -1866,10 +1903,16 @@
         idx1 = Bytes::get_Java_u2(bytes);
         printf("class        #%03d", idx1);
         ent_size = 2;
         break;
       }
+      case (JVM_CONSTANT_Class | JVM_CONSTANT_QDescBit): {
+        idx1 = Bytes::get_Java_u2(bytes);
+        printf("qclass        #%03d", idx1);
+        ent_size = 2;
+        break;
+      }
       case JVM_CONSTANT_String: {
         idx1 = Bytes::get_Java_u2(bytes);
         printf("String       #%03d", idx1);
         ent_size = 2;
         break;
@@ -1908,10 +1951,14 @@
       }
       case JVM_CONSTANT_UnresolvedClass: {
         printf("UnresolvedClass: %s", WARN_MSG);
         break;
       }
+      case (JVM_CONSTANT_UnresolvedClass | JVM_CONSTANT_QDescBit): {
+        printf("UnresolvedQClass: %s", WARN_MSG);
+        break;
+      }
       case JVM_CONSTANT_UnresolvedClassInError: {
         printf("UnresolvedClassInErr: %s", WARN_MSG);
         break;
       }
       case JVM_CONSTANT_StringIndex: {
@@ -2079,10 +2126,11 @@
         break;
       }
       case JVM_CONSTANT_Class:
       case JVM_CONSTANT_UnresolvedClass:
       case JVM_CONSTANT_UnresolvedClassInError: {
+        assert(!tag_at(idx).is_Qdescriptor_klass(), "Failed to encode QDesc");
         *bytes = JVM_CONSTANT_Class;
         Symbol* sym = klass_name_at(idx);
         idx1 = tbl->symbol_to_value(sym);
         assert(idx1 != 0, "Have not found a hashtable entry");
         Bytes::put_Java_u2((address) (bytes+1), idx1);
diff a/src/hotspot/share/oops/constantPool.hpp b/src/hotspot/share/oops/constantPool.hpp
--- a/src/hotspot/share/oops/constantPool.hpp
+++ b/src/hotspot/share/oops/constantPool.hpp
@@ -302,20 +302,29 @@
   static int pool_holder_offset_in_bytes()  { return offset_of(ConstantPool, _pool_holder); }
   static int resolved_klasses_offset_in_bytes()    { return offset_of(ConstantPool, _resolved_klasses); }
 
   // Storing constants
 
-  // For temporary use while constructing constant pool
+  // For temporary use while constructing constant pool. Used during a retransform/class redefinition as well.
   void klass_index_at_put(int which, int name_index) {
     tag_at_put(which, JVM_CONSTANT_ClassIndex);
     *int_at_addr(which) = name_index;
   }
 
   // Unsafe anonymous class support:
   void klass_at_put(int class_index, int name_index, int resolved_klass_index, Klass* k, Symbol* name);
   void klass_at_put(int class_index, Klass* k);
 
+  void unresolved_qdescriptor_at_put(int which, int name_index, int resolved_klass_index) {
+      release_tag_at_put(which, JVM_CONSTANT_UnresolvedClass | (jbyte) JVM_CONSTANT_QDescBit);
+
+      assert((name_index & 0xffff0000) == 0, "must be");
+      assert((resolved_klass_index & 0xffff0000) == 0, "must be");
+      *int_at_addr(which) =
+        build_int_from_shorts((jushort)resolved_klass_index, (jushort)name_index);
+    }
+
   void unresolved_klass_at_put(int which, int name_index, int resolved_klass_index) {
     release_tag_at_put(which, JVM_CONSTANT_UnresolvedClass);
 
     assert((name_index & 0xffff0000) == 0, "must be");
     assert((resolved_klass_index & 0xffff0000) == 0, "must be");
diff a/src/hotspot/share/oops/instanceKlass.cpp b/src/hotspot/share/oops/instanceKlass.cpp
--- a/src/hotspot/share/oops/instanceKlass.cpp
+++ b/src/hotspot/share/oops/instanceKlass.cpp
@@ -63,10 +63,11 @@
 #include "oops/klass.inline.hpp"
 #include "oops/method.hpp"
 #include "oops/oop.inline.hpp"
 #include "oops/recordComponent.hpp"
 #include "oops/symbol.hpp"
+#include "oops/valueKlass.hpp"
 #include "prims/jvmtiExport.hpp"
 #include "prims/jvmtiRedefineClasses.hpp"
 #include "prims/jvmtiThreadState.hpp"
 #include "prims/methodComparator.hpp"
 #include "runtime/atomic.hpp"
@@ -420,11 +421,13 @@
   const int size = InstanceKlass::size(parser.vtable_size(),
                                        parser.itable_size(),
                                        nonstatic_oop_map_size(parser.total_oop_map_count()),
                                        parser.is_interface(),
                                        parser.is_unsafe_anonymous(),
-                                       should_store_fingerprint(is_hidden_or_anonymous));
+                                       should_store_fingerprint(is_hidden_or_anonymous),
+                                       parser.has_flattenable_fields() ? parser.java_fields_count() : 0,
+                                       parser.is_inline_type());
 
   const Symbol* const class_name = parser.class_name();
   assert(class_name != NULL, "invariant");
   ClassLoaderData* loader_data = parser.loader_data();
   assert(loader_data != NULL, "invariant");
@@ -434,14 +437,16 @@
   // Allocation
   if (REF_NONE == parser.reference_type()) {
     if (class_name == vmSymbols::java_lang_Class()) {
       // mirror
       ik = new (loader_data, size, THREAD) InstanceMirrorKlass(parser);
-    }
-    else if (is_class_loader(class_name, parser)) {
+    } else if (is_class_loader(class_name, parser)) {
       // class loader
       ik = new (loader_data, size, THREAD) InstanceClassLoaderKlass(parser);
+    } else if (parser.is_inline_type()) {
+      // inline type
+      ik = new (loader_data, size, THREAD) ValueKlass(parser);
     } else {
       // normal
       ik = new (loader_data, size, THREAD) InstanceKlass(parser, InstanceKlass::_kind_other);
     }
   } else {
@@ -453,13 +458,43 @@
   // class count.  Can get OOM here.
   if (HAS_PENDING_EXCEPTION) {
     return NULL;
   }
 
+#ifdef ASSERT
+  assert(ik->size() == size, "");
+  ik->bounds_check((address) ik->start_of_vtable(), false, size);
+  ik->bounds_check((address) ik->start_of_itable(), false, size);
+  ik->bounds_check((address) ik->end_of_itable(), true, size);
+  ik->bounds_check((address) ik->end_of_nonstatic_oop_maps(), true, size);
+#endif //ASSERT
   return ik;
 }
 
+#ifndef PRODUCT
+bool InstanceKlass::bounds_check(address addr, bool edge_ok, intptr_t size_in_bytes) const {
+  const char* bad = NULL;
+  address end = NULL;
+  if (addr < (address)this) {
+    bad = "before";
+  } else if (addr == (address)this) {
+    if (edge_ok)  return true;
+    bad = "just before";
+  } else if (addr == (end = (address)this + sizeof(intptr_t) * (size_in_bytes < 0 ? size() : size_in_bytes))) {
+    if (edge_ok)  return true;
+    bad = "just after";
+  } else if (addr > end) {
+    bad = "after";
+  } else {
+    return true;
+  }
+  tty->print_cr("%s object bounds: " INTPTR_FORMAT " [" INTPTR_FORMAT ".." INTPTR_FORMAT "]",
+      bad, (intptr_t)addr, (intptr_t)this, (intptr_t)end);
+  Verbose = WizardMode = true; this->print(); //@@
+  return false;
+}
+#endif //PRODUCT
 
 // copy method ordering from resource area to Metaspace
 void InstanceKlass::copy_method_ordering(const intArray* m, TRAPS) {
   if (m != NULL) {
     // allocate a new array and copy contents (memcpy?)
@@ -489,33 +524,42 @@
   _nonstatic_oop_map_size(nonstatic_oop_map_size(parser.total_oop_map_count())),
   _itable_len(parser.itable_size()),
   _nest_host_index(0),
   _init_state(allocated),
   _reference_type(parser.reference_type()),
-  _init_thread(NULL)
+  _init_thread(NULL),
+  _value_field_klasses(NULL),
+  _adr_valueklass_fixed_block(NULL)
 {
   set_vtable_length(parser.vtable_size());
   set_kind(kind);
   set_access_flags(parser.access_flags());
   if (parser.is_hidden()) set_is_hidden();
   set_is_unsafe_anonymous(parser.is_unsafe_anonymous());
   set_layout_helper(Klass::instance_layout_helper(parser.layout_size(),
                                                     false));
+    if (parser.has_flattenable_fields()) {
+      set_has_inline_fields();
+    }
+    _java_fields_count = parser.java_fields_count();
 
-  assert(NULL == _methods, "underlying memory not zeroed?");
-  assert(is_instance_klass(), "is layout incorrect?");
-  assert(size_helper() == parser.layout_size(), "incorrect size_helper?");
+    assert(NULL == _methods, "underlying memory not zeroed?");
+    assert(is_instance_klass(), "is layout incorrect?");
+    assert(size_helper() == parser.layout_size(), "incorrect size_helper?");
 
   if (Arguments::is_dumping_archive()) {
-    SystemDictionaryShared::init_dumptime_info(this);
-  }
+      SystemDictionaryShared::init_dumptime_info(this);
+    }
 
   // Set biased locking bit for all instances of this class; it will be
   // cleared if revocation occurs too often for this type
   if (UseBiasedLocking && BiasedLocking::enabled()) {
     set_prototype_header(markWord::biased_locking_prototype());
   }
+  if (has_inline_fields()) {
+    _value_field_klasses = (const Klass**) adr_value_fields_klasses();
+  }
 }
 
 void InstanceKlass::deallocate_methods(ClassLoaderData* loader_data,
                                        Array<Method*>* methods) {
   if (methods != NULL && methods != Universe::the_empty_method_array() &&
@@ -541,18 +585,20 @@
   Array<InstanceKlass*>* ti = transitive_interfaces;
   if (ti != Universe::the_empty_instance_klass_array() && ti != local_interfaces) {
     // check that the interfaces don't come from super class
     Array<InstanceKlass*>* sti = (super_klass == NULL) ? NULL :
                     InstanceKlass::cast(super_klass)->transitive_interfaces();
-    if (ti != sti && ti != NULL && !ti->is_shared()) {
+    if (ti != sti && ti != NULL && !ti->is_shared() &&
+        ti != Universe::the_single_IdentityObject_klass_array()) {
       MetadataFactory::free_array<InstanceKlass*>(loader_data, ti);
     }
   }
 
   // local interfaces can be empty
   if (local_interfaces != Universe::the_empty_instance_klass_array() &&
-      local_interfaces != NULL && !local_interfaces->is_shared()) {
+      local_interfaces != NULL && !local_interfaces->is_shared() &&
+      local_interfaces != Universe::the_single_IdentityObject_klass_array()) {
     MetadataFactory::free_array<InstanceKlass*>(loader_data, local_interfaces);
   }
 }
 
 void InstanceKlass::deallocate_record_components(ClassLoaderData* loader_data,
@@ -863,10 +909,66 @@
   for (int index = 0; index < num_interfaces; index++) {
     InstanceKlass* interk = interfaces->at(index);
     interk->link_class_impl(CHECK_false);
   }
 
+
+  // If a class declares a method that uses an inline class as an argument
+  // type or return inline type, this inline class must be loaded during the
+  // linking of this class because size and properties of the inline class
+  // must be known in order to be able to perform inline type optimizations.
+  // The implementation below is an approximation of this rule, the code
+  // iterates over all methods of the current class (including overridden
+  // methods), not only the methods declared by this class. This
+  // approximation makes the code simpler, and doesn't change the semantic
+  // because classes declaring methods overridden by the current class are
+  // linked (and have performed their own pre-loading) before the linking
+  // of the current class.
+
+
+  // Note:
+  // Inline class types used for flattenable fields are loaded during
+  // the loading phase (see ClassFileParser::post_process_parsed_stream()).
+  // Inline class types used as element types for array creation
+  // are not pre-loaded. Their loading is triggered by either anewarray
+  // or multianewarray bytecodes.
+
+  // Could it be possible to do the following processing only if the
+  // class uses inline types?
+  {
+    ResourceMark rm(THREAD);
+    for (int i = 0; i < methods()->length(); i++) {
+      Method* m = methods()->at(i);
+      for (SignatureStream ss(m->signature()); !ss.is_done(); ss.next()) {
+        if (ss.is_reference()) {
+          if (ss.is_array()) {
+            ss.skip_array_prefix();
+          }
+          if (ss.type() == T_VALUETYPE) {
+            Symbol* symb = ss.as_symbol();
+
+            oop loader = class_loader();
+            oop protection_domain = this->protection_domain();
+            Klass* klass = SystemDictionary::resolve_or_fail(symb,
+                                                             Handle(THREAD, loader), Handle(THREAD, protection_domain), true,
+                                                             CHECK_false);
+            if (klass == NULL) {
+              THROW_(vmSymbols::java_lang_LinkageError(), false);
+            }
+            if (!klass->is_value()) {
+              Exceptions::fthrow(
+                THREAD_AND_LOCATION,
+                vmSymbols::java_lang_IncompatibleClassChangeError(),
+                "class %s is not an inline type",
+                klass->external_name());
+            }
+          }
+        }
+      }
+    }
+  }
+
   // in case the class is linked in the process of linking its superclasses
   if (is_linked()) {
     return true;
   }
 
@@ -932,10 +1034,11 @@
         vtable().verify(tty, true);
         // In case itable verification is ever added.
         // itable().verify(tty, true);
       }
 #endif
+
       set_init_state(linked);
       if (JvmtiExport::should_post_class_prepare()) {
         Thread *thread = THREAD;
         assert(thread->is_Java_thread(), "thread->is_Java_thread()");
         JvmtiExport::post_class_prepare((JavaThread *) thread, this);
@@ -1085,15 +1188,46 @@
       DTRACE_CLASSINIT_PROBE_WAIT(super__failed, -1, wait);
       THROW_OOP(e());
     }
   }
 
+  // Step 8
+  // Initialize classes of flattenable fields
+  {
+    for (AllFieldStream fs(this); !fs.done(); fs.next()) {
+      if (fs.is_flattenable()) {
+        Klass* klass = this->get_value_field_klass_or_null(fs.index());
+        if (klass == NULL) {
+          assert(fs.access_flags().is_static() && fs.access_flags().is_flattenable(),
+              "Otherwise should have been pre-loaded");
+          klass = SystemDictionary::resolve_or_fail(field_signature(fs.index())->fundamental_name(THREAD),
+              Handle(THREAD, class_loader()),
+              Handle(THREAD, protection_domain()),
+              true, CHECK);
+          if (klass == NULL) {
+            THROW(vmSymbols::java_lang_NoClassDefFoundError());
+          }
+          if (!klass->is_value()) {
+            THROW(vmSymbols::java_lang_IncompatibleClassChangeError());
+          }
+          this->set_value_field_klass(fs.index(), klass);
+        }
+        InstanceKlass::cast(klass)->initialize(CHECK);
+        if (fs.access_flags().is_static()) {
+          if (java_mirror()->obj_field(fs.offset()) == NULL) {
+            java_mirror()->obj_field_put(fs.offset(), ValueKlass::cast(klass)->default_value());
+          }
+        }
+      }
+    }
+  }
+
 
   // Look for aot compiled methods for this klass, including class initializer.
   AOTLoader::load_for_klass(this, THREAD);
 
-  // Step 8
+  // Step 9
   {
     DTRACE_CLASSINIT_PROBE_WAIT(clinit, -1, wait);
     // Timer includes any side effects of class initialization (resolution,
     // etc), but not recursive entry into call_class_initializer().
     PerfClassTraceTime timer(ClassLoader::perf_class_init_time(),
@@ -1103,19 +1237,19 @@
                              jt->get_thread_stat()->perf_timers_addr(),
                              PerfClassTraceTime::CLASS_CLINIT);
     call_class_initializer(THREAD);
   }
 
-  // Step 9
+  // Step 10
   if (!HAS_PENDING_EXCEPTION) {
     set_initialization_state_and_notify(fully_initialized, CHECK);
     {
       debug_only(vtable().verify(tty, true);)
     }
   }
   else {
-    // Step 10 and 11
+    // Step 11 and 12
     Handle e(THREAD, PENDING_EXCEPTION);
     CLEAR_PENDING_EXCEPTION;
     // JVMTI has already reported the pending exception
     // JVMTI internal flag reset is needed in order to report ExceptionInInitializerError
     JvmtiExport::clear_detected_exception(jt);
@@ -1376,11 +1510,11 @@
       // Atomic creation of array_klasses
       MutexLocker ma(THREAD, MultiArray_lock);
 
       // Check if update has already taken place
       if (array_klasses() == NULL) {
-        Klass*    k = ObjArrayKlass::allocate_objArray_klass(class_loader_data(), 1, this, CHECK_NULL);
+        Klass*    k = ObjArrayKlass::allocate_objArray_klass(1, this, CHECK_NULL);
         // use 'release' to pair with lock-free load
         release_set_array_klasses(k);
       }
     }
   }
@@ -1399,11 +1533,11 @@
 static int call_class_initializer_counter = 0;   // for debugging
 
 Method* InstanceKlass::class_initializer() const {
   Method* clinit = find_method(
       vmSymbols::class_initializer_name(), vmSymbols::void_method_signature());
-  if (clinit != NULL && clinit->has_valid_initializer_flags()) {
+  if (clinit != NULL && clinit->is_class_initializer()) {
     return clinit;
   }
   return NULL;
 }
 
@@ -1437,11 +1571,11 @@
   InterpreterOopMap* entry_for) {
   // Lazily create the _oop_map_cache at first request
   // Lock-free access requires load_acquire.
   OopMapCache* oop_map_cache = Atomic::load_acquire(&_oop_map_cache);
   if (oop_map_cache == NULL) {
-    MutexLocker x(OopMapCacheAlloc_lock);
+    MutexLocker x(OopMapCacheAlloc_lock,  Mutex::_no_safepoint_check_flag);
     // Check if _oop_map_cache was allocated while we were waiting for this lock
     if ((oop_map_cache = _oop_map_cache) == NULL) {
       oop_map_cache = new OopMapCache();
       // Ensure _oop_map_cache is stable, since it is examined without a lock
       Atomic::release_store(&_oop_map_cache, oop_map_cache);
@@ -1449,15 +1583,10 @@
   }
   // _oop_map_cache is constant after init; lookup below does its own locking.
   oop_map_cache->lookup(method, bci, entry_for);
 }
 
-bool InstanceKlass::contains_field_offset(int offset) {
-  fieldDescriptor fd;
-  return find_field_from_offset(offset, false, &fd);
-}
-
 bool InstanceKlass::find_local_field(Symbol* name, Symbol* sig, fieldDescriptor* fd) const {
   for (JavaFieldStream fs(this); !fs.done(); fs.next()) {
     Symbol* f_name = fs.name();
     Symbol* f_sig  = fs.signature();
     if (f_name == name && f_sig == sig) {
@@ -1524,10 +1653,19 @@
   }
   // 4) otherwise field lookup fails
   return NULL;
 }
 
+bool InstanceKlass::contains_field_offset(int offset) {
+  if (this->is_value()) {
+    ValueKlass* vk = ValueKlass::cast(this);
+    return offset >= vk->first_field_offset() && offset < (vk->first_field_offset() + vk->get_exact_size_in_bytes());
+  } else {
+    fieldDescriptor fd;
+    return find_field_from_offset(offset, false, &fd);
+  }
+}
 
 bool InstanceKlass::find_local_field_from_offset(int offset, bool is_static, fieldDescriptor* fd) const {
   for (JavaFieldStream fs(this); !fs.done(); fs.next()) {
     if (fs.offset() == offset) {
       fd->reinitialize(const_cast<InstanceKlass*>(this), fs.index());
@@ -1620,15 +1758,10 @@
   }
   FREE_C_HEAP_ARRAY(int, fields_sorted);
 }
 
 
-void InstanceKlass::array_klasses_do(void f(Klass* k, TRAPS), TRAPS) {
-  if (array_klasses() != NULL)
-    ArrayKlass::cast(array_klasses())->array_klasses_do(f, THREAD);
-}
-
 void InstanceKlass::array_klasses_do(void f(Klass* k)) {
   if (array_klasses() != NULL)
     ArrayKlass::cast(array_klasses())->array_klasses_do(f);
 }
 
@@ -1908,10 +2041,13 @@
                                                                         find_static,
                                                                         private_mode);
     if (method != NULL) {
       return method;
     }
+    if (name == vmSymbols::object_initializer_name()) {
+      break;  // <init> is never inherited, not even as a static factory
+    }
     klass = klass->super();
     overpass_local_mode = skip_overpass;   // Always ignore overpass methods in superclasses
   }
   return NULL;
 }
@@ -2489,10 +2625,14 @@
   // sure the current state is <loaded.
   assert(!is_loaded(), "invalid init state");
   set_package(loader_data, pkg_entry, CHECK);
   Klass::restore_unshareable_info(loader_data, protection_domain, CHECK);
 
+  if (is_value()) {
+    ValueKlass::cast(this)->initialize_calling_convention(CHECK);
+  }
+
   Array<Method*>* methods = this->methods();
   int num_methods = methods->length();
   for (int index = 0; index < num_methods; ++index) {
     methods->at(index)->restore_unshareable_info(CHECK);
   }
@@ -2514,11 +2654,11 @@
     // --> see ArrayKlass::complete_create_array_klass()
     ArrayKlass::cast(array_klasses())->restore_unshareable_info(ClassLoaderData::the_null_class_loader_data(), Handle(), CHECK);
   }
 
   // Initialize current biased locking state.
-  if (UseBiasedLocking && BiasedLocking::enabled()) {
+  if (UseBiasedLocking && BiasedLocking::enabled() && !is_value()) {
     set_prototype_header(markWord::biased_locking_prototype());
   }
 }
 
 void InstanceKlass::set_shared_class_loader_type(s2 loader_type) {
@@ -2666,13 +2806,13 @@
   const char* src = (const char*) (name()->as_C_string());
   const int src_length = (int)strlen(src);
 
   char* dest = NEW_RESOURCE_ARRAY(char, src_length + hash_len + 3);
 
-  // Add L as type indicator
+  // Add L or Q as type indicator
   int dest_index = 0;
-  dest[dest_index++] = JVM_SIGNATURE_CLASS;
+  dest[dest_index++] = is_value() ? JVM_SIGNATURE_VALUETYPE : JVM_SIGNATURE_CLASS;
 
   // Add the actual class name
   for (int src_index = 0; src_index < src_length; ) {
     dest[dest_index++] = src[src_index++];
   }
@@ -3228,33 +3368,69 @@
 
 static const char* state_names[] = {
   "allocated", "loaded", "linked", "being_initialized", "fully_initialized", "initialization_error"
 };
 
-static void print_vtable(intptr_t* start, int len, outputStream* st) {
+static void print_vtable(address self, intptr_t* start, int len, outputStream* st) {
+  ResourceMark rm;
+  int* forward_refs = NEW_RESOURCE_ARRAY(int, len);
+  for (int i = 0; i < len; i++)  forward_refs[i] = 0;
   for (int i = 0; i < len; i++) {
     intptr_t e = start[i];
     st->print("%d : " INTPTR_FORMAT, i, e);
+    if (forward_refs[i] != 0) {
+      int from = forward_refs[i];
+      int off = (int) start[from];
+      st->print(" (offset %d <= [%d])", off, from);
+    }
     if (MetaspaceObj::is_valid((Metadata*)e)) {
       st->print(" ");
       ((Metadata*)e)->print_value_on(st);
+    } else if (self != NULL && e > 0 && e < 0x10000) {
+      address location = self + e;
+      int index = (int)((intptr_t*)location - start);
+      st->print(" (offset %d => [%d])", (int)e, index);
+      if (index >= 0 && index < len)
+        forward_refs[index] = i;
     }
     st->cr();
   }
 }
 
 static void print_vtable(vtableEntry* start, int len, outputStream* st) {
-  return print_vtable(reinterpret_cast<intptr_t*>(start), len, st);
+  return print_vtable(NULL, reinterpret_cast<intptr_t*>(start), len, st);
+}
+
+template<typename T>
+ static void print_array_on(outputStream* st, Array<T>* array) {
+   if (array == NULL) { st->print_cr("NULL"); return; }
+   array->print_value_on(st); st->cr();
+   if (Verbose || WizardMode) {
+     for (int i = 0; i < array->length(); i++) {
+       st->print("%d : ", i); array->at(i)->print_value_on(st); st->cr();
+     }
+   }
+ }
+
+static void print_array_on(outputStream* st, Array<int>* array) {
+  if (array == NULL) { st->print_cr("NULL"); return; }
+  array->print_value_on(st); st->cr();
+  if (Verbose || WizardMode) {
+    for (int i = 0; i < array->length(); i++) {
+      st->print("%d : %d", i, array->at(i)); st->cr();
+    }
+  }
 }
 
 void InstanceKlass::print_on(outputStream* st) const {
   assert(is_klass(), "must be klass");
   Klass::print_on(st);
 
   st->print(BULLET"instance size:     %d", size_helper());                        st->cr();
   st->print(BULLET"klass size:        %d", size());                               st->cr();
   st->print(BULLET"access:            "); access_flags().print_on(st);            st->cr();
+  st->print(BULLET"misc flags:        0x%x", _misc_flags);                        st->cr();
   st->print(BULLET"state:             "); st->print_cr("%s", state_names[_init_state]);
   st->print(BULLET"name:              "); name()->print_value_on(st);             st->cr();
   st->print(BULLET"super:             "); Metadata::print_value_on_maybe_null(st, super()); st->cr();
   st->print(BULLET"sub:               ");
   Klass* sub = subklass();
@@ -3277,30 +3453,18 @@
       st->cr();
     }
   }
 
   st->print(BULLET"arrays:            "); Metadata::print_value_on_maybe_null(st, array_klasses()); st->cr();
-  st->print(BULLET"methods:           "); methods()->print_value_on(st);                  st->cr();
-  if (Verbose || WizardMode) {
-    Array<Method*>* method_array = methods();
-    for (int i = 0; i < method_array->length(); i++) {
-      st->print("%d : ", i); method_array->at(i)->print_value(); st->cr();
-    }
-  }
-  st->print(BULLET"method ordering:   "); method_ordering()->print_value_on(st);      st->cr();
-  st->print(BULLET"default_methods:   "); default_methods()->print_value_on(st);      st->cr();
-  if (Verbose && default_methods() != NULL) {
-    Array<Method*>* method_array = default_methods();
-    for (int i = 0; i < method_array->length(); i++) {
-      st->print("%d : ", i); method_array->at(i)->print_value(); st->cr();
-    }
-  }
+  st->print(BULLET"methods:           "); print_array_on(st, methods());
+  st->print(BULLET"method ordering:   "); print_array_on(st, method_ordering());
+  st->print(BULLET"default_methods:   "); print_array_on(st, default_methods());
   if (default_vtable_indices() != NULL) {
-    st->print(BULLET"default vtable indices:   "); default_vtable_indices()->print_value_on(st);       st->cr();
+    st->print(BULLET"default vtable indices:   "); print_array_on(st, default_vtable_indices());
   }
-  st->print(BULLET"local interfaces:  "); local_interfaces()->print_value_on(st);      st->cr();
-  st->print(BULLET"trans. interfaces: "); transitive_interfaces()->print_value_on(st); st->cr();
+  st->print(BULLET"local interfaces:  "); print_array_on(st, local_interfaces());
+  st->print(BULLET"trans. interfaces: "); print_array_on(st, transitive_interfaces());
   st->print(BULLET"constants:         "); constants()->print_value_on(st);         st->cr();
   if (class_loader_data() != NULL) {
     st->print(BULLET"class loader data:  ");
     class_loader_data()->print_value_on(st);
     st->cr();
@@ -3352,11 +3516,11 @@
     st->print_cr(BULLET"java mirror:       NULL");
   }
   st->print(BULLET"vtable length      %d  (start addr: " INTPTR_FORMAT ")", vtable_length(), p2i(start_of_vtable())); st->cr();
   if (vtable_length() > 0 && (Verbose || WizardMode))  print_vtable(start_of_vtable(), vtable_length(), st);
   st->print(BULLET"itable length      %d (start addr: " INTPTR_FORMAT ")", itable_length(), p2i(start_of_itable())); st->cr();
-  if (itable_length() > 0 && (Verbose || WizardMode))  print_vtable(start_of_itable(), itable_length(), st);
+  if (itable_length() > 0 && (Verbose || WizardMode))  print_vtable(NULL, start_of_itable(), itable_length(), st);
   st->print_cr(BULLET"---- static fields (%d words):", static_field_size());
   FieldPrinter print_static_field(st);
   ((InstanceKlass*)this)->do_local_static_fields(&print_static_field);
   st->print_cr(BULLET"---- non-static fields (%d words):", nonstatic_field_size());
   FieldPrinter print_nonstatic_field(st);
@@ -4091,5 +4255,10 @@
 
 unsigned char * InstanceKlass::get_cached_class_file_bytes() {
   return VM_RedefineClasses::get_cached_class_file_bytes(_cached_class_file);
 }
 #endif
+
+#define THROW_DVT_ERROR(s) \
+  Exceptions::fthrow(THREAD_AND_LOCATION, vmSymbols::java_lang_IncompatibleClassChangeError(), \
+      "ValueCapableClass class '%s' %s", external_name(),(s)); \
+      return
diff a/src/hotspot/share/oops/instanceKlass.hpp b/src/hotspot/share/oops/instanceKlass.hpp
--- a/src/hotspot/share/oops/instanceKlass.hpp
+++ b/src/hotspot/share/oops/instanceKlass.hpp
@@ -24,10 +24,11 @@
 
 #ifndef SHARE_OOPS_INSTANCEKLASS_HPP
 #define SHARE_OOPS_INSTANCEKLASS_HPP
 
 #include "classfile/classLoaderData.hpp"
+#include "code/vmreg.hpp"
 #include "memory/referenceType.hpp"
 #include "oops/annotations.hpp"
 #include "oops/constMethod.hpp"
 #include "oops/fieldInfo.hpp"
 #include "oops/instanceOop.hpp"
@@ -52,10 +53,11 @@
 //      The embedded nonstatic oop-map blocks are short pairs (offset, length)
 //      indicating where oops are located in instances of this klass.
 //    [EMBEDDED implementor of the interface] only exist for interface
 //    [EMBEDDED unsafe_anonymous_host klass] only exist for an unsafe anonymous class (JSR 292 enabled)
 //    [EMBEDDED fingerprint       ] only if should_store_fingerprint()==true
+//    [EMBEDDED ValueKlassFixedBlock] only if is a ValueKlass instance
 
 
 // forward declaration for class -- see below for definition
 #if INCLUDE_JVMTI
 class BreakpointInfo;
@@ -68,10 +70,11 @@
 class jniIdMapBase;
 class JNIid;
 class JvmtiCachedClassFieldMap;
 class nmethodBucket;
 class OopMapCache;
+class BufferedValueTypeBlob;
 class InterpreterOopMap;
 class PackageEntry;
 class ModuleEntry;
 
 // This is used in iterators below.
@@ -130,15 +133,39 @@
   uint _count;
 };
 
 struct JvmtiCachedClassFileData;
 
+class SigEntry;
+
+class ValueKlassFixedBlock {
+  Array<SigEntry>** _extended_sig;
+  Array<VMRegPair>** _return_regs;
+  address* _pack_handler;
+  address* _pack_handler_jobject;
+  address* _unpack_handler;
+  int* _default_value_offset;
+  Klass** _value_array_klass;
+  int _alignment;
+  int _first_field_offset;
+  int _exact_size_in_bytes;
+
+  friend class ValueKlass;
+};
+
+class InlineTypes {
+public:
+  u2 _class_info_index;
+  Symbol* _class_name;
+};
+
 class InstanceKlass: public Klass {
   friend class VMStructs;
   friend class JVMCIVMStructs;
   friend class ClassFileParser;
   friend class CompileReplay;
+  friend class TemplateTable;
 
  public:
   static const KlassID ID = InstanceKlassID;
 
  protected:
@@ -152,11 +179,11 @@
   enum ClassState {
     allocated,                          // allocated (but not yet linked)
     loaded,                             // loaded and inserted in class hierarchy (but not linked yet)
     linked,                             // successfully linked/verified (but not initialized yet)
     being_initialized,                  // currently running class initializer
-    fully_initialized,                  // initialized (successfull final state)
+    fully_initialized,                  // initialized (successful final state)
     initialization_error                // error happened during initialization
   };
 
  private:
   static InstanceKlass* allocate_instance_klass(const ClassFileParser& parser, TRAPS);
@@ -196,10 +223,12 @@
   // nest-host. Can also be set directly by JDK API's that establish nest
   // relationships.
   // By always being set it makes nest-member access checks simpler.
   InstanceKlass* _nest_host;
 
+  Array<InlineTypes>* _inline_types;
+
   // The contents of the Record attribute.
   Array<RecordComponent*>* _record_components;
 
   // the source debug extension for this klass, NULL if not specified.
   // Specified as UTF-8 string without terminating zero byte in the classfile,
@@ -231,16 +260,17 @@
   // Class states are defined as ClassState (see above).
   // Place the _init_state here to utilize the unused 2-byte after
   // _idnum_allocated_count.
   u1              _init_state;                    // state of class
 
-  // This can be used to quickly discriminate among the four kinds of
+  // This can be used to quickly discriminate among the five kinds of
   // InstanceKlass. This should be an enum (?)
   static const unsigned _kind_other        = 0; // concrete InstanceKlass
   static const unsigned _kind_reference    = 1; // InstanceRefKlass
   static const unsigned _kind_class_loader = 2; // InstanceClassLoaderKlass
   static const unsigned _kind_mirror       = 3; // InstanceMirrorKlass
+  static const unsigned _kind_inline_type  = 4; // InlineKlass
 
   u1              _reference_type;                // reference type
   u1              _kind;                          // kind of InstanceKlass
 
   enum {
@@ -258,16 +288,23 @@
     _misc_is_shared_boot_class                = 1 << 10, // defining class loader is boot class loader
     _misc_is_shared_platform_class            = 1 << 11, // defining class loader is platform class loader
     _misc_is_shared_app_class                 = 1 << 12, // defining class loader is app class loader
     _misc_has_resolved_methods                = 1 << 13, // resolved methods table entries added for this class
     _misc_is_being_redefined                  = 1 << 14, // used for locking redefinition
-    _misc_has_contended_annotations           = 1 << 15  // has @Contended annotation
+    _misc_has_contended_annotations           = 1 << 15,  // has @Contended annotation
+    _misc_has_inline_fields                   = 1 << 16, // has inline fields and related embedded section is not empty
+    _misc_is_empty_inline_type                = 1 << 17, // empty inline type
+    _misc_is_naturally_atomic                 = 1 << 18, // loaded/stored in one instruction
+    _misc_is_declared_atomic                  = 1 << 19, // implements jl.NonTearable
+    _misc_invalid_inline_super                = 1 << 20, // invalid super type for an inline type
+    _misc_invalid_identity_super              = 1 << 21, // invalid super type for an identity type
+    _misc_has_injected_identityObject         = 1 << 22  // IdentityObject has been injected by the JVM
   };
   u2 shared_loader_type_bits() const {
     return _misc_is_shared_boot_class|_misc_is_shared_platform_class|_misc_is_shared_app_class;
   }
-  u2              _misc_flags;           // There is more space in access_flags for more flags.
+  u4              _misc_flags;           // There is more space in access_flags for more flags.
 
   Thread*         _init_thread;          // Pointer to current thread doing initialization (to handle recursive initialization)
   OopMapCache*    volatile _oop_map_cache;   // OopMapCache for all methods in the klass (allocated lazily)
   JNIid*          _jni_ids;              // First JNI identifier for static fields in this class
   jmethodID*      volatile _methods_jmethod_ids;  // jmethodIDs corresponding to method_idnum, or NULL if none
@@ -315,10 +352,13 @@
   // fn: [access, name index, sig index, initial value index, low_offset, high_offset]
   //     [generic signature index]
   //     [generic signature index]
   //     ...
   Array<u2>*      _fields;
+  const Klass**   _value_field_klasses; // For "inline class" fields, NULL if none present
+
+  const ValueKlassFixedBlock* _adr_valueklass_fixed_block;
 
   // embedded Java vtable follows here
   // embedded Java itables follows here
   // embedded static fields follows here
   // embedded nonstatic oop-map blocks follows here
@@ -373,10 +413,75 @@
     } else {
       _misc_flags &= ~_misc_has_nonstatic_fields;
     }
   }
 
+  bool has_inline_fields() const          {
+    return (_misc_flags & _misc_has_inline_fields) != 0;
+  }
+  void set_has_inline_fields()  {
+    _misc_flags |= _misc_has_inline_fields;
+  }
+
+  bool is_empty_inline_type() const {
+    return (_misc_flags & _misc_is_empty_inline_type) != 0;
+  }
+  void set_is_empty_inline_type() {
+    _misc_flags |= _misc_is_empty_inline_type;
+  }
+
+  // Note:  The naturally_atomic property only applies to
+  // inline classes; it is never true on identity classes.
+  // The bit is placed on instanceKlass for convenience.
+
+  // Query if h/w provides atomic load/store for instances.
+  bool is_naturally_atomic() const {
+    return (_misc_flags & _misc_is_naturally_atomic) != 0;
+  }
+  // Initialized in the class file parser, not changed later.
+  void set_is_naturally_atomic() {
+    _misc_flags |= _misc_is_naturally_atomic;
+  }
+
+  // Query if this class implements jl.NonTearable or was
+  // mentioned in the JVM option AlwaysAtomicValueTypes.
+  // This bit can occur anywhere, but is only significant
+  // for inline classes *and* their super types.
+  // It inherits from supers along with NonTearable.
+  bool is_declared_atomic() const {
+    return (_misc_flags & _misc_is_declared_atomic) != 0;
+  }
+  // Initialized in the class file parser, not changed later.
+  void set_is_declared_atomic() {
+    _misc_flags |= _misc_is_declared_atomic;
+  }
+
+  // Query if class is an invalid super class for an inline type.
+  bool invalid_inline_super() const {
+    return (_misc_flags & _misc_invalid_inline_super) != 0;
+  }
+  // Initialized in the class file parser, not changed later.
+  void set_invalid_inline_super() {
+    _misc_flags |= _misc_invalid_inline_super;
+  }
+  // Query if class is an invalid super class for an identity type.
+  bool invalid_identity_super() const {
+    return (_misc_flags & _misc_invalid_identity_super) != 0;
+  }
+  // Initialized in the class file parser, not changed later.
+  void set_invalid_identity_super() {
+    _misc_flags |= _misc_invalid_identity_super;
+  }
+
+  bool has_injected_identityObject() const {
+    return (_misc_flags & _misc_has_injected_identityObject);
+  }
+
+  void set_has_injected_identityObject() {
+    _misc_flags |= _misc_has_injected_identityObject;
+  }
+
   // field sizes
   int nonstatic_field_size() const         { return _nonstatic_field_size; }
   void set_nonstatic_field_size(int size)  { _nonstatic_field_size = size; }
 
   int static_field_size() const            { return _static_field_size; }
@@ -435,10 +540,12 @@
  public:
   int     field_offset      (int index) const { return field(index)->offset(); }
   int     field_access_flags(int index) const { return field(index)->access_flags(); }
   Symbol* field_name        (int index) const { return field(index)->name(constants()); }
   Symbol* field_signature   (int index) const { return field(index)->signature(constants()); }
+  bool    field_is_flattened(int index) const { return field(index)->is_flattened(); }
+  bool    field_is_flattenable(int index) const { return field(index)->is_flattenable(); }
 
   // Number of Java declared fields
   int java_fields_count() const           { return (int)_java_fields_count; }
 
   Array<u2>* fields() const            { return _fields; }
@@ -555,10 +662,14 @@
 
   // marking
   bool is_marked_dependent() const         { return _is_marked_dependent; }
   void set_is_marked_dependent(bool value) { _is_marked_dependent = value; }
 
+  static ByteSize kind_offset() { return in_ByteSize(offset_of(InstanceKlass, _kind)); }
+  static ByteSize misc_flags_offset() { return in_ByteSize(offset_of(InstanceKlass, _misc_flags)); }
+  static u4 misc_flags_is_empty_inline_type() { return _misc_is_empty_inline_type; }
+
   // initialization (virtuals from Klass)
   bool should_be_initialized() const;  // means that initialize should be called
   void initialize(TRAPS);
   void link_class(TRAPS);
   bool link_class_or_fail(TRAPS); // returns false on failure
@@ -754,12 +865,13 @@
     }
   }
 
 #if INCLUDE_JVMTI
   // Redefinition locking.  Class can only be redefined by one thread at a time.
+
   bool is_being_redefined() const          {
-    return ((_misc_flags & _misc_is_being_redefined) != 0);
+    return (_misc_flags & _misc_is_being_redefined);
   }
   void set_is_being_redefined(bool value)  {
     if (value) {
       _misc_flags |= _misc_is_being_redefined;
     } else {
@@ -840,10 +952,11 @@
   // Other is anything that is not one of the more specialized kinds of InstanceKlass.
   bool is_other_instance_klass() const        { return is_kind(_kind_other); }
   bool is_reference_instance_klass() const    { return is_kind(_kind_reference); }
   bool is_mirror_instance_klass() const       { return is_kind(_kind_mirror); }
   bool is_class_loader_instance_klass() const { return is_kind(_kind_class_loader); }
+  bool is_inline_type_klass()           const { return is_kind(_kind_inline_type); }
 
 #if INCLUDE_JVMTI
 
   void init_previous_versions() {
     _previous_versions = NULL;
@@ -1008,10 +1121,13 @@
   // support for stub routines
   static ByteSize init_state_offset()  { return in_ByteSize(offset_of(InstanceKlass, _init_state)); }
   JFR_ONLY(DEFINE_KLASS_TRACE_ID_OFFSET;)
   static ByteSize init_thread_offset() { return in_ByteSize(offset_of(InstanceKlass, _init_thread)); }
 
+  static ByteSize value_field_klasses_offset() { return in_ByteSize(offset_of(InstanceKlass, _value_field_klasses)); }
+  static ByteSize adr_valueklass_fixed_block_offset() { return in_ByteSize(offset_of(InstanceKlass, _adr_valueklass_fixed_block)); }
+
   // subclass/subinterface checks
   bool implements_interface(Klass* k) const;
   bool is_same_or_direct_interface(Klass* k) const;
 
 #ifdef ASSERT
@@ -1042,12 +1158,11 @@
   void do_local_static_fields(FieldClosure* cl);
   void do_nonstatic_fields(FieldClosure* cl); // including inherited fields
   void do_local_static_fields(void f(fieldDescriptor*, Handle, TRAPS), Handle, TRAPS);
 
   void methods_do(void f(Method* method));
-  void array_klasses_do(void f(Klass* k));
-  void array_klasses_do(void f(Klass* k, TRAPS), TRAPS);
+  virtual void array_klasses_do(void f(Klass* k));
 
   static InstanceKlass* cast(Klass* k) {
     return const_cast<InstanceKlass*>(cast(const_cast<const Klass*>(k)));
   }
 
@@ -1064,34 +1179,41 @@
   // Sizing (in words)
   static int header_size()            { return sizeof(InstanceKlass)/wordSize; }
 
   static int size(int vtable_length, int itable_length,
                   int nonstatic_oop_map_size,
-                  bool is_interface, bool is_unsafe_anonymous, bool has_stored_fingerprint) {
+                  bool is_interface, bool is_unsafe_anonymous, bool has_stored_fingerprint,
+                  int java_fields, bool is_inline_type) {
     return align_metadata_size(header_size() +
            vtable_length +
            itable_length +
            nonstatic_oop_map_size +
            (is_interface ? (int)sizeof(Klass*)/wordSize : 0) +
            (is_unsafe_anonymous ? (int)sizeof(Klass*)/wordSize : 0) +
-           (has_stored_fingerprint ? (int)sizeof(uint64_t*)/wordSize : 0));
+           (has_stored_fingerprint ? (int)sizeof(uint64_t*)/wordSize : 0) +
+           (java_fields * (int)sizeof(Klass*)/wordSize) +
+           (is_inline_type ? (int)sizeof(ValueKlassFixedBlock) : 0));
   }
   int size() const                    { return size(vtable_length(),
                                                itable_length(),
                                                nonstatic_oop_map_size(),
                                                is_interface(),
                                                is_unsafe_anonymous(),
-                                               has_stored_fingerprint());
+                                               has_stored_fingerprint(),
+                                               has_inline_fields() ? java_fields_count() : 0,
+                                               is_value());
   }
 
   intptr_t* start_of_itable()   const { return (intptr_t*)start_of_vtable() + vtable_length(); }
   intptr_t* end_of_itable()     const { return start_of_itable() + itable_length(); }
 
   int  itable_offset_in_words() const { return start_of_itable() - (intptr_t*)this; }
 
   oop static_field_base_raw() { return java_mirror(); }
 
+  bool bounds_check(address addr, bool edge_ok = false, intptr_t size_in_bytes = -1) const PRODUCT_RETURN0;
+
   OopMapBlock* start_of_nonstatic_oop_maps() const {
     return (OopMapBlock*)(start_of_itable() + itable_length());
   }
 
   Klass** end_of_nonstatic_oop_maps() const {
@@ -1136,12 +1258,57 @@
     } else {
       return NULL;
     }
   }
 
+  address adr_value_fields_klasses() const {
+    if (has_inline_fields()) {
+      address adr_fing = adr_fingerprint();
+      if (adr_fing != NULL) {
+        return adr_fingerprint() + sizeof(u8);
+      }
+
+      InstanceKlass** adr_host = adr_unsafe_anonymous_host();
+      if (adr_host != NULL) {
+        return (address)(adr_host + 1);
+      }
+
+      Klass* volatile* adr_impl = adr_implementor();
+      if (adr_impl != NULL) {
+        return (address)(adr_impl + 1);
+      }
+
+      return (address)end_of_nonstatic_oop_maps();
+    } else {
+      return NULL;
+    }
+  }
+
+  Klass* get_value_field_klass(int idx) const {
+    assert(has_inline_fields(), "Sanity checking");
+    Klass* k = ((Klass**)adr_value_fields_klasses())[idx];
+    assert(k != NULL, "Should always be set before being read");
+    assert(k->is_value(), "Must be a inline type");
+    return k;
+  }
+
+  Klass* get_value_field_klass_or_null(int idx) const {
+    assert(has_inline_fields(), "Sanity checking");
+    Klass* k = ((Klass**)adr_value_fields_klasses())[idx];
+    assert(k == NULL || k->is_value(), "Must be a inline type");
+    return k;
+  }
+
+  void set_value_field_klass(int idx, Klass* k) {
+    assert(has_inline_fields(), "Sanity checking");
+    assert(k != NULL, "Should not be set to NULL");
+    assert(((Klass**)adr_value_fields_klasses())[idx] == NULL, "Should not be set twice");
+    ((Klass**)adr_value_fields_klasses())[idx] = k;
+  }
+
   // Use this to return the size of an instance in heap words:
-  int size_helper() const {
+  virtual int size_helper() const {
     return layout_helper_to_size_helper(layout_helper());
   }
 
   // This bit is initialized in classFileParser.cpp.
   // It is false under any of the following conditions:
@@ -1274,16 +1441,18 @@
   void initialize_impl                           (TRAPS);
   void initialize_super_interfaces               (TRAPS);
   void eager_initialize_impl                     ();
   /* jni_id_for_impl for jfieldID only */
   JNIid* jni_id_for_impl                         (int offset);
-
+protected:
   // Returns the array class for the n'th dimension
-  Klass* array_klass_impl(bool or_null, int n, TRAPS);
+  virtual Klass* array_klass_impl(bool or_null, int n, TRAPS);
 
   // Returns the array class with this class as element type
-  Klass* array_klass_impl(bool or_null, TRAPS);
+  virtual Klass* array_klass_impl(bool or_null, TRAPS);
+
+private:
 
   // find a local method (returns NULL if not found)
   Method* find_method_impl(const Symbol* name,
                            const Symbol* signature,
                            OverpassLookupMode overpass_mode,
@@ -1307,11 +1476,11 @@
 #endif
 public:
   // CDS support - remove and restore oops from metadata. Oops are not shared.
   virtual void remove_unshareable_info();
   virtual void remove_java_mirror();
-  void restore_unshareable_info(ClassLoaderData* loader_data, Handle protection_domain, PackageEntry* pkg_entry, TRAPS);
+  virtual void restore_unshareable_info(ClassLoaderData* loader_data, Handle protection_domain, PackageEntry* pkg_entry, TRAPS);
 
   // jvm support
   jint compute_modifier_flags(TRAPS) const;
 
 public:
diff a/src/hotspot/share/oops/klass.cpp b/src/hotspot/share/oops/klass.cpp
--- a/src/hotspot/share/oops/klass.cpp
+++ b/src/hotspot/share/oops/klass.cpp
@@ -207,11 +207,11 @@
   // Note that T_ARRAY is not allowed here.
   int  hsize = arrayOopDesc::base_offset_in_bytes(etype);
   int  esize = type2aelembytes(etype);
   bool isobj = (etype == T_OBJECT);
   int  tag   =  isobj ? _lh_array_tag_obj_value : _lh_array_tag_type_value;
-  int lh = array_layout_helper(tag, hsize, etype, exact_log2(esize));
+  int lh = array_layout_helper(tag, false, hsize, etype, exact_log2(esize));
 
   assert(lh < (int)_lh_neutral_value, "must look like an array layout");
   assert(layout_helper_is_array(lh), "correct kind");
   assert(layout_helper_is_objArray(lh) == isobj, "correct kind");
   assert(layout_helper_is_typeArray(lh) == !isobj, "correct kind");
diff a/src/hotspot/share/oops/klass.hpp b/src/hotspot/share/oops/klass.hpp
--- a/src/hotspot/share/oops/klass.hpp
+++ b/src/hotspot/share/oops/klass.hpp
@@ -43,14 +43,15 @@
   InstanceKlassID,
   InstanceRefKlassID,
   InstanceMirrorKlassID,
   InstanceClassLoaderKlassID,
   TypeArrayKlassID,
+  ValueArrayKlassID,
   ObjArrayKlassID
 };
 
-const uint KLASS_ID_COUNT = 6;
+const uint KLASS_ID_COUNT = 7;
 
 //
 // A Klass provides:
 //  1: language level class object (method dictionary etc.)
 //  2: provide vm dispatch behavior for the object
@@ -96,11 +97,11 @@
   //
   // For arrays, layout helper is a negative number, containing four
   // distinct bytes, as follows:
   //    MSB:[tag, hsz, ebt, log2(esz)]:LSB
   // where:
-  //    tag is 0x80 if the elements are oops, 0xC0 if non-oops
+  //    tag is 0x80 if the elements are oops, 0xC0 if non-oops, 0xA0 if value types
   //    hsz is array header size in bytes (i.e., offset of first element)
   //    ebt is the BasicType of the elements
   //    esz is the element size in bytes
   // This packed word is arranged so as to be quickly unpacked by the
   // various fast paths that use the various subfields.
@@ -346,14 +347,22 @@
   static const int _lh_log2_element_size_mask  = BitsPerLong-1;
   static const int _lh_element_type_shift      = BitsPerByte*1;
   static const int _lh_element_type_mask       = right_n_bits(BitsPerByte);  // shifted mask
   static const int _lh_header_size_shift       = BitsPerByte*2;
   static const int _lh_header_size_mask        = right_n_bits(BitsPerByte);  // shifted mask
-  static const int _lh_array_tag_bits          = 2;
+  static const int _lh_array_tag_bits          = 3;
   static const int _lh_array_tag_shift         = BitsPerInt - _lh_array_tag_bits;
-  static const int _lh_array_tag_obj_value     = ~0x01;   // 0x80000000 >> 30
-
+
+  static const unsigned int _lh_array_tag_type_value = 0Xfffffffc;
+  static const unsigned int _lh_array_tag_vt_value   = 0Xfffffffd;
+  static const unsigned int _lh_array_tag_obj_value  = 0Xfffffffe;
+
+  // null-free array flag bit under the array tag bits, shift one more to get array tag value
+  static const int _lh_null_free_shift = _lh_array_tag_shift - 1;
+  static const int _lh_null_free_mask  = 1;
+
+  static const jint _lh_array_tag_vt_value_bit_inplace = (jint) (1 << _lh_array_tag_shift);
   static const unsigned int _lh_array_tag_type_value = 0Xffffffff; // ~0x00,  // 0xC0000000 >> 30
 
   static int layout_helper_size_in_bytes(jint lh) {
     assert(lh > (jint)_lh_neutral_value, "must be instance");
     return (int) lh & ~_lh_instance_slow_path_bit;
@@ -367,27 +376,37 @@
   }
   static bool layout_helper_is_array(jint lh) {
     return (jint)lh < (jint)_lh_neutral_value;
   }
   static bool layout_helper_is_typeArray(jint lh) {
-    // _lh_array_tag_type_value == (lh >> _lh_array_tag_shift);
-    return (juint)lh >= (juint)(_lh_array_tag_type_value << _lh_array_tag_shift);
+    return (juint) _lh_array_tag_type_value == (juint)(lh >> _lh_array_tag_shift);
   }
   static bool layout_helper_is_objArray(jint lh) {
-    // _lh_array_tag_obj_value == (lh >> _lh_array_tag_shift);
-    return (jint)lh < (jint)(_lh_array_tag_type_value << _lh_array_tag_shift);
+    return (juint)_lh_array_tag_obj_value == (juint)(lh >> _lh_array_tag_shift);
+  }
+  static bool layout_helper_is_valueArray(jint lh) {
+    return (juint)_lh_array_tag_vt_value == (juint)(lh >> _lh_array_tag_shift);
+  }
+  static bool layout_helper_is_null_free(jint lh) {
+    assert(layout_helper_is_valueArray(lh) || layout_helper_is_objArray(lh), "must be array of inline types");
+    return ((lh >> _lh_null_free_shift) & _lh_null_free_mask);
+  }
+  static jint layout_helper_set_null_free(jint lh) {
+    lh |= (_lh_null_free_mask << _lh_null_free_shift);
+    assert(layout_helper_is_null_free(lh), "Bad encoding");
+    return lh;
   }
   static int layout_helper_header_size(jint lh) {
     assert(lh < (jint)_lh_neutral_value, "must be array");
     int hsize = (lh >> _lh_header_size_shift) & _lh_header_size_mask;
     assert(hsize > 0 && hsize < (int)sizeof(oopDesc)*3, "sanity");
     return hsize;
   }
   static BasicType layout_helper_element_type(jint lh) {
     assert(lh < (jint)_lh_neutral_value, "must be array");
     int btvalue = (lh >> _lh_element_type_shift) & _lh_element_type_mask;
-    assert(btvalue >= T_BOOLEAN && btvalue <= T_OBJECT, "sanity");
+    assert((btvalue >= T_BOOLEAN && btvalue <= T_OBJECT) || btvalue == T_VALUETYPE, "sanity");
     return (BasicType) btvalue;
   }
 
   // Want a pattern to quickly diff against layout header in register
   // find something less clever!
@@ -404,16 +423,17 @@
   }
 
   static int layout_helper_log2_element_size(jint lh) {
     assert(lh < (jint)_lh_neutral_value, "must be array");
     int l2esz = (lh >> _lh_log2_element_size_shift) & _lh_log2_element_size_mask;
-    assert(l2esz <= LogBytesPerLong,
+    assert(layout_helper_element_type(lh) == T_VALUETYPE || l2esz <= LogBytesPerLong,
            "sanity. l2esz: 0x%x for lh: 0x%x", (uint)l2esz, (uint)lh);
     return l2esz;
   }
-  static jint array_layout_helper(jint tag, int hsize, BasicType etype, int log2_esize) {
+  static jint array_layout_helper(jint tag, bool null_free, int hsize, BasicType etype, int log2_esize) {
     return (tag        << _lh_array_tag_shift)
+      |    ((null_free ? 1 : 0) <<  _lh_null_free_shift)
       |    (hsize      << _lh_header_size_shift)
       |    ((int)etype << _lh_element_type_shift)
       |    (log2_esize << _lh_log2_element_size_shift);
   }
   static jint instance_layout_helper(jint size, bool slow_path_flag) {
@@ -550,10 +570,12 @@
   // Returns the name for a class (Resource allocated) as the class
   // would appear in a signature.
   // For arrays, this returns the name of the element with a leading '['.
   // For classes, this returns the name with a leading 'L' and a trailing ';'
   //     and the package separators as '/'.
+  // For value classes, this returns the name with a leading 'Q' and a trailing ';'
+  //     and the package separators as '/'.
   virtual const char* signature_name() const;
 
   const char* joint_in_module_of_loader(const Klass* class2, bool include_parent_loader = false) const;
   const char* class_in_module_of_loader(bool use_are = false, bool include_parent_loader = false) const;
 
@@ -565,11 +587,14 @@
  protected:
   virtual bool is_instance_klass_slow()     const { return false; }
   virtual bool is_array_klass_slow()        const { return false; }
   virtual bool is_objArray_klass_slow()     const { return false; }
   virtual bool is_typeArray_klass_slow()    const { return false; }
+  virtual bool is_valueArray_klass_slow()   const { return false; }
 #endif // ASSERT
+  // current implementation uses this method even in non debug builds
+  virtual bool is_value_slow()          const { return false; }
  public:
 
   // Fast non-virtual versions
   #ifndef ASSERT
   #define assert_same_query(xval, xcheck) xval
@@ -591,12 +616,19 @@
                                                     layout_helper_is_objArray(layout_helper()),
                                                     is_objArray_klass_slow()); }
   inline  bool is_typeArray_klass()           const { return assert_same_query(
                                                     layout_helper_is_typeArray(layout_helper()),
                                                     is_typeArray_klass_slow()); }
+  inline  bool is_value()                     const { return is_value_slow(); } //temporary hack
+  inline  bool is_valueArray_klass()          const { return assert_same_query(
+                                                    layout_helper_is_valueArray(layout_helper()),
+                                                    is_valueArray_klass_slow()); }
+
   #undef assert_same_query
 
+  inline bool is_null_free_array_klass()      const { return layout_helper_is_null_free(layout_helper()); }
+
   // Access flags
   AccessFlags access_flags() const         { return _access_flags;  }
   void set_access_flags(AccessFlags flags) { _access_flags = flags; }
 
   bool is_public() const                { return _access_flags.is_public(); }
@@ -626,11 +658,15 @@
 
   // Biased locking support
   // Note: the prototype header is always set up to be at least the
   // prototype markWord. If biased locking is enabled it may further be
   // biasable and have an epoch.
-  markWord prototype_header() const      { return _prototype_header; }
+  markWord prototype_header() const     { return _prototype_header; }
+  static inline markWord default_prototype_header(Klass* k) {
+    return (k == NULL) ? markWord::prototype() : k->prototype_header();
+  }
+
   // NOTE: once instances of this klass are floating around in the
   // system, this header must only be updated at a safepoint.
   // NOTE 2: currently we only ever set the prototype header to the
   // biasable prototype for instanceKlasses. There is no technical
   // reason why it could not be done for arrayKlasses aside from
diff a/src/hotspot/share/oops/method.cpp b/src/hotspot/share/oops/method.cpp
--- a/src/hotspot/share/oops/method.cpp
+++ b/src/hotspot/share/oops/method.cpp
@@ -52,10 +52,11 @@
 #include "oops/methodData.hpp"
 #include "oops/objArrayKlass.hpp"
 #include "oops/objArrayOop.inline.hpp"
 #include "oops/oop.inline.hpp"
 #include "oops/symbol.hpp"
+#include "oops/valueKlass.inline.hpp"
 #include "prims/jvmtiExport.hpp"
 #include "prims/methodHandles.hpp"
 #include "prims/nativeLookup.hpp"
 #include "runtime/arguments.hpp"
 #include "runtime/atomic.hpp"
@@ -111,11 +112,10 @@
 
   if (access_flags.is_native()) {
     clear_native_function();
     set_signature_handler(NULL);
   }
-
   NOT_PRODUCT(set_compiled_invocation_count(0);)
 }
 
 // Release Method*.  The nmethod will be gone when we get here because
 // we've walked the code cache.
@@ -148,15 +148,25 @@
 address Method::get_c2i_entry() {
   assert(adapter() != NULL, "must have");
   return adapter()->get_c2i_entry();
 }
 
+address Method::get_c2i_value_entry() {
+  assert(adapter() != NULL, "must have");
+  return adapter()->get_c2i_value_entry();
+}
+
 address Method::get_c2i_unverified_entry() {
   assert(adapter() != NULL, "must have");
   return adapter()->get_c2i_unverified_entry();
 }
 
+address Method::get_c2i_unverified_value_entry() {
+  assert(adapter() != NULL, "must have");
+  return adapter()->get_c2i_unverified_value_entry();
+}
+
 address Method::get_c2i_no_clinit_check_entry() {
   assert(VM_Version::supports_fast_class_init_checks(), "");
   assert(adapter() != NULL, "must have");
   return adapter()->get_c2i_no_clinit_check_entry();
 }
@@ -345,10 +355,12 @@
   it->push(&_method_counters);
 
   Method* this_ptr = this;
   it->push_method_entry(&this_ptr, (intptr_t*)&_i2i_entry);
   it->push_method_entry(&this_ptr, (intptr_t*)&_from_compiled_entry);
+  it->push_method_entry(&this_ptr, (intptr_t*)&_from_compiled_value_ro_entry);
+  it->push_method_entry(&this_ptr, (intptr_t*)&_from_compiled_value_entry);
   it->push_method_entry(&this_ptr, (intptr_t*)&_from_interpreted_entry);
 }
 
 // Attempt to return method oop to original state.  Clear any pointers
 // (to objects outside the shared spaces).  We won't be able to predict
@@ -588,21 +600,39 @@
   set_size_of_parameters(fp.size_of_parameters());
   constMethod()->set_result_type(fp.return_type());
   constMethod()->set_fingerprint(fp.fingerprint());
 }
 
+// ValueKlass the method is declared to return. This must not
+// safepoint as it is called with references live on the stack at
+// locations the GC is unaware of.
+ValueKlass* Method::returned_value_type(Thread* thread) const {
+  SignatureStream ss(signature());
+  while (!ss.at_return_type()) {
+    ss.next();
+  }
+  Handle class_loader(thread, method_holder()->class_loader());
+  Handle protection_domain(thread, method_holder()->protection_domain());
+  Klass* k = NULL;
+  {
+    NoSafepointVerifier nsv;
+    k = ss.as_klass(class_loader, protection_domain, SignatureStream::ReturnNull, thread);
+  }
+  assert(k != NULL && !thread->has_pending_exception(), "can't resolve klass");
+  return ValueKlass::cast(k);
+}
 bool Method::is_empty_method() const {
   return  code_size() == 1
       && *code_base() == Bytecodes::_return;
 }
 
 bool Method::is_vanilla_constructor() const {
   // Returns true if this method is a vanilla constructor, i.e. an "<init>" "()V" method
   // which only calls the superclass vanilla constructor and possibly does stores of
   // zero constants to local fields:
   //
-  //   aload_0
+  //   aload_0, _fast_aload_0, or _nofast_aload_0
   //   invokespecial
   //   indexbyte1
   //   indexbyte2
   //
   // followed by an (optional) sequence of:
@@ -622,11 +652,12 @@
   int size = code_size();
   // Check if size match
   if (size == 0 || size % 5 != 0) return false;
   address cb = code_base();
   int last = size - 1;
-  if (cb[0] != Bytecodes::_aload_0 || cb[1] != Bytecodes::_invokespecial || cb[last] != Bytecodes::_return) {
+  if ((cb[0] != Bytecodes::_aload_0 && cb[0] != Bytecodes::_fast_aload_0 && cb[0] != Bytecodes::_nofast_aload_0) ||
+       cb[1] != Bytecodes::_invokespecial || cb[last] != Bytecodes::_return) {
     // Does not call superclass default constructor
     return false;
   }
   // Check optional sequence
   for (int i = 4; i < last; i += 5) {
@@ -774,29 +805,31 @@
           Bytecodes::is_const(java_code_at(0)) &&
           Bytecodes::length_for(java_code_at(0)) == last_index &&
           Bytecodes::is_return(java_code_at(last_index)));
 }
 
-bool Method::is_initializer() const {
-  return is_object_initializer() || is_static_initializer();
-}
-
-bool Method::has_valid_initializer_flags() const {
-  return (is_static() ||
-          method_holder()->major_version() < 51);
+bool Method::is_object_constructor_or_class_initializer() const {
+  return (is_object_constructor() || is_class_initializer());
 }
 
-bool Method::is_static_initializer() const {
+bool Method::is_class_initializer() const {
   // For classfiles version 51 or greater, ensure that the clinit method is
   // static.  Non-static methods with the name "<clinit>" are not static
   // initializers. (older classfiles exempted for backward compatibility)
-  return name() == vmSymbols::class_initializer_name() &&
-         has_valid_initializer_flags();
+  return (name() == vmSymbols::class_initializer_name() &&
+          (is_static() ||
+           method_holder()->major_version() < 51));
+}
+
+// A method named <init>, if non-static, is a classic object constructor.
+bool Method::is_object_constructor() const {
+   return name() == vmSymbols::object_initializer_name() && !is_static();
 }
 
-bool Method::is_object_initializer() const {
-   return name() == vmSymbols::object_initializer_name();
+// A static method named <init> is a factory for an inline class.
+bool Method::is_static_init_factory() const {
+   return name() == vmSymbols::object_initializer_name() && is_static();
 }
 
 bool Method::needs_clinit_barrier() const {
   return is_static() && !method_holder()->is_initialized();
 }
@@ -850,11 +883,11 @@
   return best_line;
 }
 
 
 bool Method::is_klass_loaded_by_klass_index(int klass_index) const {
-  if( constants()->tag_at(klass_index).is_unresolved_klass() ) {
+  if( constants()->tag_at(klass_index).is_unresolved_klass()) {
     Thread *thread = Thread::current();
     Symbol* klass_name = constants()->klass_name_at(klass_index);
     Handle loader(thread, method_holder()->class_loader());
     Handle prot  (thread, method_holder()->protection_domain());
     return SystemDictionary::find(klass_name, loader, prot, thread) != NULL;
@@ -866,11 +899,13 @@
 
 bool Method::is_klass_loaded(int refinfo_index, bool must_be_resolved) const {
   int klass_index = constants()->klass_ref_index_at(refinfo_index);
   if (must_be_resolved) {
     // Make sure klass is resolved in constantpool.
-    if (constants()->tag_at(klass_index).is_unresolved_klass()) return false;
+    if (constants()->tag_at(klass_index).is_unresolved_klass()) {
+      return false;
+    }
   }
   return is_klass_loaded_by_klass_index(klass_index);
 }
 
 
@@ -1035,12 +1070,16 @@
 void Method::clear_code() {
   // this may be NULL if c2i adapters have not been made yet
   // Only should happen at allocate time.
   if (adapter() == NULL) {
     _from_compiled_entry    = NULL;
+    _from_compiled_value_entry = NULL;
+    _from_compiled_value_ro_entry = NULL;
   } else {
     _from_compiled_entry    = adapter()->get_c2i_entry();
+    _from_compiled_value_entry = adapter()->get_c2i_value_entry();
+    _from_compiled_value_ro_entry = adapter()->get_c2i_value_ro_entry();
   }
   OrderAccess::storestore();
   _from_interpreted_entry = _i2i_entry;
   OrderAccess::storestore();
   _code = NULL;
@@ -1081,13 +1120,22 @@
   } else {
     // TODO: Simplify the adapter trampoline allocation for static archiving.
     //       Remove the use of CDSAdapterHandlerEntry.
     CDSAdapterHandlerEntry* cds_adapter = (CDSAdapterHandlerEntry*)adapter();
     constMethod()->set_adapter_trampoline(cds_adapter->get_adapter_trampoline());
+
     _from_compiled_entry = cds_adapter->get_c2i_entry_trampoline();
     assert(*((int*)_from_compiled_entry) == 0,
-           "must be NULL during dump time, to be initialized at run time");
+           "instructions must be zeros during dump time, to be initialized at run time");
+
+    _from_compiled_value_ro_entry = cds_adapter->get_c2i_value_ro_entry_trampoline();
+    assert(*((int*)_from_compiled_value_ro_entry) == 0,
+           "instructions must be zeros during dump time, to be initialized at run time");
+
+    _from_compiled_value_entry = cds_adapter->get_c2i_value_entry_trampoline();
+    assert(*((int*)_from_compiled_value_entry) == 0,
+           "instructions must be zeros during dump time, to be initialized at run time");
   }
 
   if (is_native()) {
     *native_function_addr() = NULL;
     set_signature_handler(NULL);
@@ -1240,34 +1288,61 @@
   }
 
   if (mh->is_shared()) {
     assert(mh->adapter() == adapter, "must be");
     assert(mh->_from_compiled_entry != NULL, "must be");
+    assert(mh->_from_compiled_value_entry != NULL, "must be");
+    assert(mh->_from_compiled_value_ro_entry != NULL, "must be");
   } else {
     mh->set_adapter_entry(adapter);
     mh->_from_compiled_entry = adapter->get_c2i_entry();
+    mh->_from_compiled_value_entry = adapter->get_c2i_value_entry();
+    mh->_from_compiled_value_ro_entry = adapter->get_c2i_value_ro_entry();
   }
   return adapter->get_c2i_entry();
 }
 
 void Method::restore_unshareable_info(TRAPS) {
   assert(is_method() && is_valid_method(this), "ensure C++ vtable is restored");
 
+#if 0
+  /*
+   * CDS:TODO --
+   * "Q" classes in the method signature must be resolved during link_method.
+   * However, at this point we are still inside method_holder()->restore_unshareable_info.
+   * If we try to resolve method_holder(), or multually dependent classes, it will
+   * cause deadlock and other ill effects.
+   *
+   * For now, lets do method linking inside InstanceKlass::link_class(). Optimization
+   * may be possible if we know that resolution will never happen.
+   */
+
   // Since restore_unshareable_info can be called more than once for a method, don't
   // redo any work.
   if (adapter() == NULL) {
     methodHandle mh(THREAD, this);
     link_method(mh, CHECK);
   }
+#endif
 }
 
-address Method::from_compiled_entry_no_trampoline() const {
+address Method::from_compiled_entry_no_trampoline(bool caller_is_c1) const {
   CompiledMethod *code = Atomic::load_acquire(&_code);
-  if (code) {
-    return code->verified_entry_point();
+  if (caller_is_c1) {
+    // C1 - value arguments are passed as objects
+    if (code) {
+      return code->verified_value_entry_point();
+    } else {
+      return adapter()->get_c2i_value_entry();
+    }
   } else {
-    return adapter()->get_c2i_entry();
+    // C2 - value arguments may be passed as fields
+    if (code) {
+      return code->verified_entry_point();
+    } else {
+      return adapter()->get_c2i_entry();
+    }
   }
 }
 
 // The verified_code_entry() must be called when a invoke is resolved
 // on this method.
@@ -1280,10 +1355,22 @@
   debug_only(NoSafepointVerifier nsv;)
   assert(_from_compiled_entry != NULL, "must be set");
   return _from_compiled_entry;
 }
 
+address Method::verified_value_code_entry() {
+  debug_only(NoSafepointVerifier nsv;)
+  assert(_from_compiled_value_entry != NULL, "must be set");
+  return _from_compiled_value_entry;
+}
+
+address Method::verified_value_ro_code_entry() {
+  debug_only(NoSafepointVerifier nsv;)
+  assert(_from_compiled_value_ro_entry != NULL, "must be set");
+  return _from_compiled_value_ro_entry;
+}
+
 // Check that if an nmethod ref exists, it has a backlink to this or no backlink at all
 // (could be racing a deopt).
 // Not inline to avoid circular ref.
 bool Method::check_code() const {
   // cached in a register or local.  There's a race on the value of the field.
@@ -1311,10 +1398,12 @@
     mh->set_highest_comp_level(comp_level);
   }
 
   OrderAccess::storestore();
   mh->_from_compiled_entry = code->verified_entry_point();
+  mh->_from_compiled_value_entry = code->verified_value_entry_point();
+  mh->_from_compiled_value_ro_entry = code->verified_value_ro_entry_point();
   OrderAccess::storestore();
   // Instantly compiled code can execute.
   if (!mh->is_method_handle_intrinsic())
     mh->_from_interpreted_entry = mh->get_i2c_entry();
 }
@@ -2342,10 +2431,12 @@
   if (intrinsic_id() != vmIntrinsics::_none)
     st->print_cr(" - intrinsic id:      %d %s", intrinsic_id(), vmIntrinsics::name_at(intrinsic_id()));
   if (highest_comp_level() != CompLevel_none)
     st->print_cr(" - highest level:     %d", highest_comp_level());
   st->print_cr(" - vtable index:      %d",   _vtable_index);
+  if (valid_itable_index())
+    st->print_cr(" - itable index:      %d",   itable_index());
   st->print_cr(" - i2i entry:         " INTPTR_FORMAT, p2i(interpreter_entry()));
   st->print(   " - adapters:          ");
   AdapterHandlerEntry* a = ((Method*)this)->adapter();
   if (a == NULL)
     st->print_cr(INTPTR_FORMAT, p2i(a));
@@ -2419,10 +2510,11 @@
 void Method::print_value_on(outputStream* st) const {
   assert(is_method(), "must be method");
   st->print("%s", internal_name());
   print_address_on(st);
   st->print(" ");
+  if (WizardMode) access_flags().print_on(st);
   name()->print_value_on(st);
   st->print(" ");
   signature()->print_value_on(st);
   st->print(" in ");
   method_holder()->print_value_on(st);
diff a/src/hotspot/share/oops/objArrayKlass.cpp b/src/hotspot/share/oops/objArrayKlass.cpp
--- a/src/hotspot/share/oops/objArrayKlass.cpp
+++ b/src/hotspot/share/oops/objArrayKlass.cpp
@@ -30,10 +30,11 @@
 #include "classfile/vmSymbols.hpp"
 #include "gc/shared/collectedHeap.inline.hpp"
 #include "memory/iterator.inline.hpp"
 #include "memory/metadataFactory.hpp"
 #include "memory/metaspaceClosure.hpp"
+#include "memory/oopFactory.hpp"
 #include "memory/resourceArea.hpp"
 #include "memory/universe.hpp"
 #include "oops/arrayKlass.inline.hpp"
 #include "oops/instanceKlass.hpp"
 #include "oops/klass.inline.hpp"
@@ -52,13 +53,11 @@
   int size = ArrayKlass::static_size(ObjArrayKlass::header_size());
 
   return new (loader_data, size, THREAD) ObjArrayKlass(n, k, name);
 }
 
-Klass* ObjArrayKlass::allocate_objArray_klass(ClassLoaderData* loader_data,
-                                                int n, Klass* element_klass, TRAPS) {
-
+Klass* ObjArrayKlass::allocate_objArray_klass(int n, Klass* element_klass, TRAPS) {
   // Eagerly allocate the direct array supertype.
   Klass* super_klass = NULL;
   if (!Universe::is_bootstrapping() || SystemDictionary::Object_klass_loaded()) {
     Klass* element_super = element_klass->super();
     if (element_super != NULL) {
@@ -95,31 +94,14 @@
       super_klass = SystemDictionary::Object_klass();
     }
   }
 
   // Create type name for klass.
-  Symbol* name = NULL;
-  {
-    ResourceMark rm(THREAD);
-    char *name_str = element_klass->name()->as_C_string();
-    int len = element_klass->name()->utf8_length();
-    char *new_str = NEW_RESOURCE_ARRAY(char, len + 4);
-    int idx = 0;
-    new_str[idx++] = JVM_SIGNATURE_ARRAY;
-    if (element_klass->is_instance_klass()) { // it could be an array or simple type
-      new_str[idx++] = JVM_SIGNATURE_CLASS;
-    }
-    memcpy(&new_str[idx], name_str, len * sizeof(char));
-    idx += len;
-    if (element_klass->is_instance_klass()) {
-      new_str[idx++] = JVM_SIGNATURE_ENDCLASS;
-    }
-    new_str[idx++] = '\0';
-    name = SymbolTable::new_symbol(new_str);
-  }
+  Symbol* name = ArrayKlass::create_element_klass_array_name(element_klass, CHECK_NULL);
 
   // Initialize instance variables
+  ClassLoaderData* loader_data = element_klass->class_loader_data();
   ObjArrayKlass* oak = ObjArrayKlass::allocate(loader_data, n, element_klass, name, CHECK_NULL);
 
   ModuleEntry* module = oak->module();
   assert(module != NULL, "No module entry for array");
 
@@ -141,18 +123,24 @@
   set_element_klass(element_klass);
 
   Klass* bk;
   if (element_klass->is_objArray_klass()) {
     bk = ObjArrayKlass::cast(element_klass)->bottom_klass();
+  } else if (element_klass->is_valueArray_klass()) {
+    bk = ValueArrayKlass::cast(element_klass)->element_klass();
   } else {
     bk = element_klass;
   }
   assert(bk != NULL && (bk->is_instance_klass() || bk->is_typeArray_klass()), "invalid bottom klass");
   set_bottom_klass(bk);
   set_class_loader_data(bk->class_loader_data());
 
-  set_layout_helper(array_layout_helper(T_OBJECT));
+  jint lh = array_layout_helper(T_OBJECT);
+  if (element_klass->is_value()) {
+    lh = layout_helper_set_null_free(lh);
+  }
+  set_layout_helper(lh);
   assert(is_array_klass(), "sanity");
   assert(is_objArray_klass(), "sanity");
 }
 
 int ObjArrayKlass::oop_size(oop obj) const {
@@ -161,40 +149,59 @@
 }
 
 objArrayOop ObjArrayKlass::allocate(int length, TRAPS) {
   check_array_allocation_length(length, arrayOopDesc::max_array_length(T_OBJECT), CHECK_NULL);
   int size = objArrayOopDesc::object_size(length);
-  return (objArrayOop)Universe::heap()->array_allocate(this, size, length,
+  bool populate_null_free = is_null_free_array_klass();
+  objArrayOop array =  (objArrayOop)Universe::heap()->array_allocate(this, size, length,
                                                        /* do_zero */ true, THREAD);
+  if (populate_null_free) {
+    assert(dimension() == 1, "Can only populate the final dimension");
+    assert(element_klass()->is_value(), "Unexpected");
+    assert(!element_klass()->is_array_klass(), "ArrayKlass unexpected here");
+    assert(!ValueKlass::cast(element_klass())->flatten_array(), "Expected valueArrayOop allocation");
+    element_klass()->initialize(CHECK_NULL);
+    // Populate default values...
+    objArrayHandle array_h(THREAD, array);
+    instanceOop value = (instanceOop) ValueKlass::cast(element_klass())->default_value();
+    for (int i = 0; i < length; i++) {
+      array_h->obj_at_put(i, value);
+    }
+  }
+  return array;
 }
 
-static int multi_alloc_counter = 0;
-
 oop ObjArrayKlass::multi_allocate(int rank, jint* sizes, TRAPS) {
   int length = *sizes;
+  if (rank == 1) { // last dim may be valueArray, check if we have any special storage requirements
+    if (element_klass()->is_value()) {
+      return oopFactory::new_valueArray(element_klass(), length, CHECK_NULL);
+    } else {
+      return oopFactory::new_objArray(element_klass(), length, CHECK_NULL);
+    }
+  }
+  guarantee(rank > 1, "Rank below 1");
   // Call to lower_dimension uses this pointer, so most be called before a
   // possible GC
   Klass* ld_klass = lower_dimension();
   // If length < 0 allocate will throw an exception.
   objArrayOop array = allocate(length, CHECK_NULL);
   objArrayHandle h_array (THREAD, array);
-  if (rank > 1) {
-    if (length != 0) {
-      for (int index = 0; index < length; index++) {
-        ArrayKlass* ak = ArrayKlass::cast(ld_klass);
-        oop sub_array = ak->multi_allocate(rank-1, &sizes[1], CHECK_NULL);
-        h_array->obj_at_put(index, sub_array);
-      }
-    } else {
-      // Since this array dimension has zero length, nothing will be
-      // allocated, however the lower dimension values must be checked
-      // for illegal values.
-      for (int i = 0; i < rank - 1; ++i) {
-        sizes += 1;
-        if (*sizes < 0) {
-          THROW_MSG_0(vmSymbols::java_lang_NegativeArraySizeException(), err_msg("%d", *sizes));
-        }
+  if (length != 0) {
+    for (int index = 0; index < length; index++) {
+      ArrayKlass* ak = ArrayKlass::cast(ld_klass);
+      oop sub_array = ak->multi_allocate(rank-1, &sizes[1], CHECK_NULL);
+      h_array->obj_at_put(index, sub_array);
+    }
+  } else {
+    // Since this array dimension has zero length, nothing will be
+    // allocated, however the lower dimension values must be checked
+    // for illegal values.
+    for (int i = 0; i < rank - 1; ++i) {
+      sizes += 1;
+      if (*sizes < 0) {
+        THROW_MSG_0(vmSymbols::java_lang_NegativeArraySizeException(), err_msg("%d", *sizes));
       }
     }
   }
   return h_array();
 }
@@ -208,39 +215,40 @@
     ArrayAccess<>::oop_arraycopy(s, src_offset, d, dst_offset, length);
   } else {
     // We have to make sure all elements conform to the destination array
     Klass* bound = ObjArrayKlass::cast(d->klass())->element_klass();
     Klass* stype = ObjArrayKlass::cast(s->klass())->element_klass();
+    // Perform null check if dst is null-free but src has no such guarantee
+    bool null_check = ((!s->klass()->is_null_free_array_klass()) &&
+        d->klass()->is_null_free_array_klass());
     if (stype == bound || stype->is_subtype_of(bound)) {
-      // elements are guaranteed to be subtypes, so no check necessary
-      ArrayAccess<ARRAYCOPY_DISJOINT>::oop_arraycopy(s, src_offset, d, dst_offset, length);
+      if (null_check) {
+        ArrayAccess<ARRAYCOPY_DISJOINT | ARRAYCOPY_NOTNULL>::oop_arraycopy(s, src_offset, d, dst_offset, length);
+      } else {
+        ArrayAccess<ARRAYCOPY_DISJOINT>::oop_arraycopy(s, src_offset, d, dst_offset, length);
+      }
     } else {
-      // slow case: need individual subtype checks
-      // note: don't use obj_at_put below because it includes a redundant store check
-      if (!ArrayAccess<ARRAYCOPY_DISJOINT | ARRAYCOPY_CHECKCAST>::oop_arraycopy(s, src_offset, d, dst_offset, length)) {
-        ResourceMark rm(THREAD);
-        stringStream ss;
-        if (!bound->is_subtype_of(stype)) {
-          ss.print("arraycopy: type mismatch: can not copy %s[] into %s[]",
-                   stype->external_name(), bound->external_name());
-        } else {
-          // oop_arraycopy should return the index in the source array that
-          // contains the problematic oop.
-          ss.print("arraycopy: element type mismatch: can not cast one of the elements"
-                   " of %s[] to the type of the destination array, %s",
-                   stype->external_name(), bound->external_name());
-        }
-        THROW_MSG(vmSymbols::java_lang_ArrayStoreException(), ss.as_string());
+      if (null_check) {
+        ArrayAccess<ARRAYCOPY_DISJOINT | ARRAYCOPY_CHECKCAST | ARRAYCOPY_NOTNULL>::oop_arraycopy(s, src_offset, d, dst_offset, length);
+      } else {
+        ArrayAccess<ARRAYCOPY_DISJOINT | ARRAYCOPY_CHECKCAST>::oop_arraycopy(s, src_offset, d, dst_offset, length);
       }
     }
   }
 }
 
 void ObjArrayKlass::copy_array(arrayOop s, int src_pos, arrayOop d,
                                int dst_pos, int length, TRAPS) {
   assert(s->is_objArray(), "must be obj array");
 
+  if (EnableValhalla) {
+    if (d->is_valueArray()) {
+      ValueArrayKlass::cast(d->klass())->copy_array(s, src_pos, d, dst_pos, length, THREAD);
+      return;
+    }
+  }
+
   if (!d->is_objArray()) {
     ResourceMark rm(THREAD);
     stringStream ss;
     if (d->is_typeArray()) {
       ss.print("arraycopy: type mismatch: can not copy object array[] into %s[]",
@@ -309,31 +317,28 @@
   }
 }
 
 
 Klass* ObjArrayKlass::array_klass_impl(bool or_null, int n, TRAPS) {
-
   assert(dimension() <= n, "check order of chain");
   int dim = dimension();
   if (dim == n) return this;
 
   // lock-free read needs acquire semantics
   if (higher_dimension_acquire() == NULL) {
     if (or_null) return NULL;
 
     ResourceMark rm;
-    JavaThread *jt = (JavaThread *)THREAD;
     {
       // Ensure atomic creation of higher dimensions
       MutexLocker mu(THREAD, MultiArray_lock);
 
       // Check if another thread beat us
       if (higher_dimension() == NULL) {
 
         // Create multi-dim klass object and link them together
-        Klass* k =
-          ObjArrayKlass::allocate_objArray_klass(class_loader_data(), dim + 1, this, CHECK_NULL);
+        Klass* k = ObjArrayKlass::allocate_objArray_klass(dim + 1, this, CHECK_NULL);
         ObjArrayKlass* ak = ObjArrayKlass::cast(k);
         ak->set_lower_dimension(this);
         // use 'release' to pair with lock-free load
         release_set_higher_dimension(ak);
         assert(ak->is_objArray_klass(), "incorrect initialization of ObjArrayKlass");
@@ -371,13 +376,14 @@
   if (num_secondaries == 2) {
     // Must share this for correct bootstrapping!
     set_secondary_supers(Universe::the_array_interfaces_array());
     return NULL;
   } else {
-    GrowableArray<Klass*>* secondaries = new GrowableArray<Klass*>(num_elem_supers+2);
+    GrowableArray<Klass*>* secondaries = new GrowableArray<Klass*>(num_elem_supers+3);
     secondaries->push(SystemDictionary::Cloneable_klass());
     secondaries->push(SystemDictionary::Serializable_klass());
+    secondaries->push(SystemDictionary::IdentityObject_klass());
     for (int i = 0; i < num_elem_supers; i++) {
       Klass* elem_super = elem_supers->at(i);
       Klass* array_super = elem_super->array_klass_or_null();
       assert(array_super != NULL, "must already have been created");
       secondaries->push(array_super);
@@ -425,11 +431,11 @@
 // Printing
 
 void ObjArrayKlass::print_on(outputStream* st) const {
 #ifndef PRODUCT
   Klass::print_on(st);
-  st->print(" - instance klass: ");
+  st->print(" - element klass: ");
   element_klass()->print_value_on(st);
   st->cr();
 #endif //PRODUCT
 }
 
@@ -487,11 +493,12 @@
 void ObjArrayKlass::verify_on(outputStream* st) {
   ArrayKlass::verify_on(st);
   guarantee(element_klass()->is_klass(), "should be klass");
   guarantee(bottom_klass()->is_klass(), "should be klass");
   Klass* bk = bottom_klass();
-  guarantee(bk->is_instance_klass() || bk->is_typeArray_klass(),  "invalid bottom klass");
+  guarantee(bk->is_instance_klass() || bk->is_typeArray_klass() || bk->is_valueArray_klass(),
+            "invalid bottom klass");
 }
 
 void ObjArrayKlass::oop_verify_on(oop obj, outputStream* st) {
   ArrayKlass::oop_verify_on(obj, st);
   guarantee(obj->is_objArray(), "must be objArray");
diff a/src/hotspot/share/oops/valueKlass.cpp b/src/hotspot/share/oops/valueKlass.cpp
--- /dev/null
+++ b/src/hotspot/share/oops/valueKlass.cpp
@@ -0,0 +1,569 @@
+/*
+ * Copyright (c) 2017, 2020, Oracle and/or its affiliates. All rights reserved.
+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
+ *
+ * This code is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 only, as
+ * published by the Free Software Foundation.
+ *
+ * This code is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
+ * version 2 for more details (a copy is included in the LICENSE file that
+ * accompanied this code).
+ *
+ * You should have received a copy of the GNU General Public License version
+ * 2 along with this work; if not, write to the Free Software Foundation,
+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
+ *
+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
+ * or visit www.oracle.com if you need additional information or have any
+ * questions.
+ *
+ */
+
+#include "precompiled.hpp"
+#include "gc/shared/barrierSet.hpp"
+#include "gc/shared/collectedHeap.inline.hpp"
+#include "gc/shared/gcLocker.inline.hpp"
+#include "interpreter/interpreter.hpp"
+#include "logging/log.hpp"
+#include "memory/metaspaceClosure.hpp"
+#include "memory/metadataFactory.hpp"
+#include "oops/access.hpp"
+#include "oops/compressedOops.inline.hpp"
+#include "oops/fieldStreams.inline.hpp"
+#include "oops/instanceKlass.inline.hpp"
+#include "oops/method.hpp"
+#include "oops/oop.inline.hpp"
+#include "oops/objArrayKlass.hpp"
+#include "oops/valueKlass.inline.hpp"
+#include "oops/valueArrayKlass.hpp"
+#include "runtime/fieldDescriptor.inline.hpp"
+#include "runtime/handles.inline.hpp"
+#include "runtime/safepointVerifiers.hpp"
+#include "runtime/sharedRuntime.hpp"
+#include "runtime/signature.hpp"
+#include "runtime/thread.inline.hpp"
+#include "utilities/copy.hpp"
+
+  // Constructor
+ValueKlass::ValueKlass(const ClassFileParser& parser)
+    : InstanceKlass(parser, InstanceKlass::_kind_inline_type, InstanceKlass::ID) {
+  _adr_valueklass_fixed_block = valueklass_static_block();
+  // Addresses used for value type calling convention
+  *((Array<SigEntry>**)adr_extended_sig()) = NULL;
+  *((Array<VMRegPair>**)adr_return_regs()) = NULL;
+  *((address*)adr_pack_handler()) = NULL;
+  *((address*)adr_pack_handler_jobject()) = NULL;
+  *((address*)adr_unpack_handler()) = NULL;
+  assert(pack_handler() == NULL, "pack handler not null");
+  *((int*)adr_default_value_offset()) = 0;
+  *((Klass**)adr_value_array_klass()) = NULL;
+  set_prototype_header(markWord::always_locked_prototype());
+  assert(is_inline_type_klass(), "invariant");
+}
+
+oop ValueKlass::default_value() {
+  oop val = java_mirror()->obj_field_acquire(default_value_offset());
+  assert(oopDesc::is_oop(val), "Sanity check");
+  assert(val->is_value(), "Sanity check");
+  assert(val->klass() == this, "sanity check");
+  return val;
+}
+
+int ValueKlass::first_field_offset_old() {
+#ifdef ASSERT
+  int first_offset = INT_MAX;
+  for (AllFieldStream fs(this); !fs.done(); fs.next()) {
+    if (fs.offset() < first_offset) first_offset= fs.offset();
+  }
+#endif
+  int base_offset = instanceOopDesc::base_offset_in_bytes();
+  // The first field of value types is aligned on a long boundary
+  base_offset = align_up(base_offset, BytesPerLong);
+  assert(base_offset == first_offset, "inconsistent offsets");
+  return base_offset;
+}
+
+int ValueKlass::raw_value_byte_size() {
+  int heapOopAlignedSize = nonstatic_field_size() << LogBytesPerHeapOop;
+  // If bigger than 64 bits or needs oop alignment, then use jlong aligned
+  // which for values should be jlong aligned, asserts in raw_field_copy otherwise
+  if (heapOopAlignedSize >= longSize || contains_oops()) {
+    return heapOopAlignedSize;
+  }
+  // Small primitives...
+  // If a few small basic type fields, return the actual size, i.e.
+  // 1 byte = 1
+  // 2 byte = 2
+  // 3 byte = 4, because pow2 needed for element stores
+  int first_offset = first_field_offset();
+  int last_offset  = 0; // find the last offset, add basic type size
+  int last_tsz     = 0;
+  for (AllFieldStream fs(this); !fs.done(); fs.next()) {
+    if (fs.access_flags().is_static()) {
+      continue;
+    } else if (fs.offset() > last_offset) {
+      BasicType type = Signature::basic_type(fs.signature());
+      if (is_java_primitive(type)) {
+        last_tsz = type2aelembytes(type);
+      } else if (type == T_VALUETYPE) {
+        // Not just primitives. Layout aligns embedded value, so use jlong aligned it is
+        return heapOopAlignedSize;
+      } else {
+        guarantee(0, "Unknown type %d", type);
+      }
+      assert(last_tsz != 0, "Invariant");
+      last_offset = fs.offset();
+    }
+  }
+  // Assumes VT with no fields are meaningless and illegal
+  last_offset += last_tsz;
+  assert(last_offset > first_offset && last_tsz, "Invariant");
+  return 1 << upper_log2(last_offset - first_offset);
+}
+
+instanceOop ValueKlass::allocate_instance(TRAPS) {
+  int size = size_helper();  // Query before forming handle.
+
+  instanceOop oop = (instanceOop)Universe::heap()->obj_allocate(this, size, CHECK_NULL);
+  assert(oop->mark().is_always_locked(), "Unlocked value type");
+  return oop;
+}
+
+instanceOop ValueKlass::allocate_instance_buffer(TRAPS) {
+  int size = size_helper();  // Query before forming handle.
+
+  instanceOop oop = (instanceOop)Universe::heap()->obj_buffer_allocate(this, size, CHECK_NULL);
+  assert(oop->mark().is_always_locked(), "Unlocked value type");
+  return oop;
+}
+
+int ValueKlass::nonstatic_oop_count() {
+  int oops = 0;
+  int map_count = nonstatic_oop_map_count();
+  OopMapBlock* block = start_of_nonstatic_oop_maps();
+  OopMapBlock* end = block + map_count;
+  while (block != end) {
+    oops += block->count();
+    block++;
+  }
+  return oops;
+}
+
+oop ValueKlass::read_flattened_field(oop obj, int offset, TRAPS) {
+  oop res = NULL;
+  this->initialize(CHECK_NULL); // will throw an exception if in error state
+  if (is_empty_inline_type()) {
+    res = (instanceOop)default_value();
+  } else {
+    Handle obj_h(THREAD, obj);
+    res = allocate_instance_buffer(CHECK_NULL);
+    value_copy_payload_to_new_oop(((char*)(oopDesc*)obj_h()) + offset, res);
+  }
+  assert(res != NULL, "Must be set in one of two paths above");
+  return res;
+}
+
+void ValueKlass::write_flattened_field(oop obj, int offset, oop value, TRAPS) {
+  if (value == NULL) {
+    THROW(vmSymbols::java_lang_NullPointerException());
+  }
+  if (!is_empty_inline_type()) {
+    value_copy_oop_to_payload(value, ((char*)(oopDesc*)obj) + offset);
+  }
+}
+
+// Arrays of...
+
+bool ValueKlass::flatten_array() {
+  if (!ValueArrayFlatten) {
+    return false;
+  }
+  // Too big
+  int elem_bytes = raw_value_byte_size();
+  if ((InlineArrayElemMaxFlatSize >= 0) && (elem_bytes > InlineArrayElemMaxFlatSize)) {
+    return false;
+  }
+  // Too many embedded oops
+  if ((InlineArrayElemMaxFlatOops >= 0) && (nonstatic_oop_count() > InlineArrayElemMaxFlatOops)) {
+    return false;
+  }
+  // Declared atomic but not naturally atomic.
+  if (is_declared_atomic() && !is_naturally_atomic()) {
+    return false;
+  }
+  // VM enforcing InlineArrayAtomicAccess only...
+  if (InlineArrayAtomicAccess && (!is_naturally_atomic())) {
+    return false;
+  }
+  return true;
+}
+
+void ValueKlass::remove_unshareable_info() {
+  InstanceKlass::remove_unshareable_info();
+
+  *((Array<SigEntry>**)adr_extended_sig()) = NULL;
+  *((Array<VMRegPair>**)adr_return_regs()) = NULL;
+  *((address*)adr_pack_handler()) = NULL;
+  *((address*)adr_pack_handler_jobject()) = NULL;
+  *((address*)adr_unpack_handler()) = NULL;
+  assert(pack_handler() == NULL, "pack handler not null");
+  *((Klass**)adr_value_array_klass()) = NULL;
+}
+
+void ValueKlass::restore_unshareable_info(ClassLoaderData* loader_data, Handle protection_domain, PackageEntry* pkg_entry, TRAPS) {
+  InstanceKlass::restore_unshareable_info(loader_data, protection_domain, pkg_entry, CHECK);
+  oop val = allocate_instance(CHECK);
+  set_default_value(val);
+}
+
+
+Klass* ValueKlass::array_klass_impl(bool or_null, int n, TRAPS) {
+  if (flatten_array()) {
+    return value_array_klass(or_null, n, THREAD);
+  } else {
+    return InstanceKlass::array_klass_impl(or_null, n, THREAD);
+  }
+}
+
+Klass* ValueKlass::array_klass_impl(bool or_null, TRAPS) {
+  return array_klass_impl(or_null, 1, THREAD);
+}
+
+Klass* ValueKlass::value_array_klass(bool or_null, int rank, TRAPS) {
+  Klass* vak = acquire_value_array_klass();
+  if (vak == NULL) {
+    if (or_null) return NULL;
+    ResourceMark rm;
+    {
+      // Atomic creation of array_klasses
+      MutexLocker ma(THREAD, MultiArray_lock);
+      if (get_value_array_klass() == NULL) {
+        vak = allocate_value_array_klass(CHECK_NULL);
+        Atomic::release_store((Klass**)adr_value_array_klass(), vak);
+      }
+    }
+  }
+  if (or_null) {
+    return vak->array_klass_or_null(rank);
+  }
+  return vak->array_klass(rank, THREAD);
+}
+
+Klass* ValueKlass::allocate_value_array_klass(TRAPS) {
+  if (flatten_array()) {
+    return ValueArrayKlass::allocate_klass(this, THREAD);
+  }
+  return ObjArrayKlass::allocate_objArray_klass(1, this, THREAD);
+}
+
+void ValueKlass::array_klasses_do(void f(Klass* k)) {
+  InstanceKlass::array_klasses_do(f);
+  if (get_value_array_klass() != NULL)
+    ArrayKlass::cast(get_value_array_klass())->array_klasses_do(f);
+}
+
+// Value type arguments are not passed by reference, instead each
+// field of the value type is passed as an argument. This helper
+// function collects the fields of the value types (including embedded
+// value type's fields) in a list. Included with the field's type is
+// the offset of each field in the value type: i2c and c2i adapters
+// need that to load or store fields. Finally, the list of fields is
+// sorted in order of increasing offsets: the adapters and the
+// compiled code need to agree upon the order of fields.
+//
+// The list of basic types that is returned starts with a T_VALUETYPE
+// and ends with an extra T_VOID. T_VALUETYPE/T_VOID pairs are used as
+// delimiters. Every entry between the two is a field of the value
+// type. If there's an embedded value type in the list, it also starts
+// with a T_VALUETYPE and ends with a T_VOID. This is so we can
+// generate a unique fingerprint for the method's adapters and we can
+// generate the list of basic types from the interpreter point of view
+// (value types passed as reference: iterate on the list until a
+// T_VALUETYPE, drop everything until and including the closing
+// T_VOID) or the compiler point of view (each field of the value
+// types is an argument: drop all T_VALUETYPE/T_VOID from the list).
+int ValueKlass::collect_fields(GrowableArray<SigEntry>* sig, int base_off) {
+  int count = 0;
+  SigEntry::add_entry(sig, T_VALUETYPE, base_off);
+  for (AllFieldStream fs(this); !fs.done(); fs.next()) {
+    if (fs.access_flags().is_static()) continue;
+    int offset = base_off + fs.offset() - (base_off > 0 ? first_field_offset() : 0);
+    if (fs.is_flattened()) {
+      // Resolve klass of flattened value type field and recursively collect fields
+      Klass* vk = get_value_field_klass(fs.index());
+      count += ValueKlass::cast(vk)->collect_fields(sig, offset);
+    } else {
+      BasicType bt = Signature::basic_type(fs.signature());
+      if (bt == T_VALUETYPE) {
+        bt = T_OBJECT;
+      }
+      SigEntry::add_entry(sig, bt, offset);
+      count += type2size[bt];
+    }
+  }
+  int offset = base_off + size_helper()*HeapWordSize - (base_off > 0 ? first_field_offset() : 0);
+  SigEntry::add_entry(sig, T_VOID, offset);
+  if (base_off == 0) {
+    sig->sort(SigEntry::compare);
+  }
+  assert(sig->at(0)._bt == T_VALUETYPE && sig->at(sig->length()-1)._bt == T_VOID, "broken structure");
+  return count;
+}
+
+void ValueKlass::initialize_calling_convention(TRAPS) {
+  // Because the pack and unpack handler addresses need to be loadable from generated code,
+  // they are stored at a fixed offset in the klass metadata. Since value type klasses do
+  // not have a vtable, the vtable offset is used to store these addresses.
+  if (is_scalarizable() && (InlineTypeReturnedAsFields || InlineTypePassFieldsAsArgs)) {
+    ResourceMark rm;
+    GrowableArray<SigEntry> sig_vk;
+    int nb_fields = collect_fields(&sig_vk);
+    Array<SigEntry>* extended_sig = MetadataFactory::new_array<SigEntry>(class_loader_data(), sig_vk.length(), CHECK);
+    *((Array<SigEntry>**)adr_extended_sig()) = extended_sig;
+    for (int i = 0; i < sig_vk.length(); i++) {
+      extended_sig->at_put(i, sig_vk.at(i));
+    }
+
+    if (InlineTypeReturnedAsFields) {
+      nb_fields++;
+      BasicType* sig_bt = NEW_RESOURCE_ARRAY(BasicType, nb_fields);
+      sig_bt[0] = T_METADATA;
+      SigEntry::fill_sig_bt(&sig_vk, sig_bt+1);
+      VMRegPair* regs = NEW_RESOURCE_ARRAY(VMRegPair, nb_fields);
+      int total = SharedRuntime::java_return_convention(sig_bt, regs, nb_fields);
+
+      if (total > 0) {
+        Array<VMRegPair>* return_regs = MetadataFactory::new_array<VMRegPair>(class_loader_data(), nb_fields, CHECK);
+        *((Array<VMRegPair>**)adr_return_regs()) = return_regs;
+        for (int i = 0; i < nb_fields; i++) {
+          return_regs->at_put(i, regs[i]);
+        }
+
+        BufferedValueTypeBlob* buffered_blob = SharedRuntime::generate_buffered_value_type_adapter(this);
+        *((address*)adr_pack_handler()) = buffered_blob->pack_fields();
+        *((address*)adr_pack_handler_jobject()) = buffered_blob->pack_fields_jobject();
+        *((address*)adr_unpack_handler()) = buffered_blob->unpack_fields();
+        assert(CodeCache::find_blob(pack_handler()) == buffered_blob, "lost track of blob");
+      }
+    }
+  }
+}
+
+void ValueKlass::deallocate_contents(ClassLoaderData* loader_data) {
+  if (extended_sig() != NULL) {
+    MetadataFactory::free_array<SigEntry>(loader_data, extended_sig());
+  }
+  if (return_regs() != NULL) {
+    MetadataFactory::free_array<VMRegPair>(loader_data, return_regs());
+  }
+  cleanup_blobs();
+  InstanceKlass::deallocate_contents(loader_data);
+}
+
+void ValueKlass::cleanup(ValueKlass* ik) {
+  ik->cleanup_blobs();
+}
+
+void ValueKlass::cleanup_blobs() {
+  if (pack_handler() != NULL) {
+    CodeBlob* buffered_blob = CodeCache::find_blob(pack_handler());
+    assert(buffered_blob->is_buffered_value_type_blob(), "bad blob type");
+    BufferBlob::free((BufferBlob*)buffered_blob);
+    *((address*)adr_pack_handler()) = NULL;
+    *((address*)adr_pack_handler_jobject()) = NULL;
+    *((address*)adr_unpack_handler()) = NULL;
+  }
+}
+
+// Can this inline type be scalarized?
+bool ValueKlass::is_scalarizable() const {
+  return ScalarizeInlineTypes;
+}
+
+// Can this value type be returned as multiple values?
+bool ValueKlass::can_be_returned_as_fields() const {
+  return return_regs() != NULL;
+}
+
+// Create handles for all oop fields returned in registers that are going to be live across a safepoint
+void ValueKlass::save_oop_fields(const RegisterMap& reg_map, GrowableArray<Handle>& handles) const {
+  Thread* thread = Thread::current();
+  const Array<SigEntry>* sig_vk = extended_sig();
+  const Array<VMRegPair>* regs = return_regs();
+  int j = 1;
+
+  for (int i = 0; i < sig_vk->length(); i++) {
+    BasicType bt = sig_vk->at(i)._bt;
+    if (bt == T_OBJECT || bt == T_ARRAY) {
+      VMRegPair pair = regs->at(j);
+      address loc = reg_map.location(pair.first());
+      oop v = *(oop*)loc;
+      assert(v == NULL || oopDesc::is_oop(v), "not an oop?");
+      assert(Universe::heap()->is_in_or_null(v), "must be heap pointer");
+      handles.push(Handle(thread, v));
+    }
+    if (bt == T_VALUETYPE) {
+      continue;
+    }
+    if (bt == T_VOID &&
+        sig_vk->at(i-1)._bt != T_LONG &&
+        sig_vk->at(i-1)._bt != T_DOUBLE) {
+      continue;
+    }
+    j++;
+  }
+  assert(j == regs->length(), "missed a field?");
+}
+
+// Update oop fields in registers from handles after a safepoint
+void ValueKlass::restore_oop_results(RegisterMap& reg_map, GrowableArray<Handle>& handles) const {
+  assert(InlineTypeReturnedAsFields, "inconsistent");
+  const Array<SigEntry>* sig_vk = extended_sig();
+  const Array<VMRegPair>* regs = return_regs();
+  assert(regs != NULL, "inconsistent");
+
+  int j = 1;
+  for (int i = 0, k = 0; i < sig_vk->length(); i++) {
+    BasicType bt = sig_vk->at(i)._bt;
+    if (bt == T_OBJECT || bt == T_ARRAY) {
+      VMRegPair pair = regs->at(j);
+      address loc = reg_map.location(pair.first());
+      *(oop*)loc = handles.at(k++)();
+    }
+    if (bt == T_VALUETYPE) {
+      continue;
+    }
+    if (bt == T_VOID &&
+        sig_vk->at(i-1)._bt != T_LONG &&
+        sig_vk->at(i-1)._bt != T_DOUBLE) {
+      continue;
+    }
+    j++;
+  }
+  assert(j == regs->length(), "missed a field?");
+}
+
+// Fields are in registers. Create an instance of the value type and
+// initialize it with the values of the fields.
+oop ValueKlass::realloc_result(const RegisterMap& reg_map, const GrowableArray<Handle>& handles, TRAPS) {
+  oop new_vt = allocate_instance(CHECK_NULL);
+  const Array<SigEntry>* sig_vk = extended_sig();
+  const Array<VMRegPair>* regs = return_regs();
+
+  int j = 1;
+  int k = 0;
+  for (int i = 0; i < sig_vk->length(); i++) {
+    BasicType bt = sig_vk->at(i)._bt;
+    if (bt == T_VALUETYPE) {
+      continue;
+    }
+    if (bt == T_VOID) {
+      if (sig_vk->at(i-1)._bt == T_LONG ||
+          sig_vk->at(i-1)._bt == T_DOUBLE) {
+        j++;
+      }
+      continue;
+    }
+    int off = sig_vk->at(i)._offset;
+    assert(off > 0, "offset in object should be positive");
+    VMRegPair pair = regs->at(j);
+    address loc = reg_map.location(pair.first());
+    switch(bt) {
+    case T_BOOLEAN: {
+      new_vt->bool_field_put(off, *(jboolean*)loc);
+      break;
+    }
+    case T_CHAR: {
+      new_vt->char_field_put(off, *(jchar*)loc);
+      break;
+    }
+    case T_BYTE: {
+      new_vt->byte_field_put(off, *(jbyte*)loc);
+      break;
+    }
+    case T_SHORT: {
+      new_vt->short_field_put(off, *(jshort*)loc);
+      break;
+    }
+    case T_INT: {
+      new_vt->int_field_put(off, *(jint*)loc);
+      break;
+    }
+    case T_LONG: {
+#ifdef _LP64
+      new_vt->double_field_put(off,  *(jdouble*)loc);
+#else
+      Unimplemented();
+#endif
+      break;
+    }
+    case T_OBJECT:
+    case T_ARRAY: {
+      Handle handle = handles.at(k++);
+      new_vt->obj_field_put(off, handle());
+      break;
+    }
+    case T_FLOAT: {
+      new_vt->float_field_put(off,  *(jfloat*)loc);
+      break;
+    }
+    case T_DOUBLE: {
+      new_vt->double_field_put(off, *(jdouble*)loc);
+      break;
+    }
+    default:
+      ShouldNotReachHere();
+    }
+    *(intptr_t*)loc = 0xDEAD;
+    j++;
+  }
+  assert(j == regs->length(), "missed a field?");
+  assert(k == handles.length(), "missed an oop?");
+  return new_vt;
+}
+
+// Check the return register for a ValueKlass oop
+ValueKlass* ValueKlass::returned_value_klass(const RegisterMap& map) {
+  BasicType bt = T_METADATA;
+  VMRegPair pair;
+  int nb = SharedRuntime::java_return_convention(&bt, &pair, 1);
+  assert(nb == 1, "broken");
+
+  address loc = map.location(pair.first());
+  intptr_t ptr = *(intptr_t*)loc;
+  if (is_set_nth_bit(ptr, 0)) {
+    // Oop is tagged, must be a ValueKlass oop
+    clear_nth_bit(ptr, 0);
+    assert(Metaspace::contains((void*)ptr), "should be klass");
+    ValueKlass* vk = (ValueKlass*)ptr;
+    assert(vk->can_be_returned_as_fields(), "must be able to return as fields");
+    return vk;
+  }
+#ifdef ASSERT
+  // Oop is not tagged, must be a valid oop
+  if (VerifyOops) {
+    oopDesc::verify(oop((HeapWord*)ptr));
+  }
+#endif
+  return NULL;
+}
+
+void ValueKlass::verify_on(outputStream* st) {
+  InstanceKlass::verify_on(st);
+  guarantee(prototype_header().is_always_locked(), "Prototype header is not always locked");
+}
+
+void ValueKlass::oop_verify_on(oop obj, outputStream* st) {
+  InstanceKlass::oop_verify_on(obj, st);
+  guarantee(obj->mark().is_always_locked(), "Header is not always locked");
+}
+
+void ValueKlass::metaspace_pointers_do(MetaspaceClosure* it) {
+  InstanceKlass::metaspace_pointers_do(it);
+
+  ValueKlass* this_ptr = this;
+  it->push_internal_pointer(&this_ptr, (intptr_t*)&_adr_valueklass_fixed_block);
+}
diff a/src/hotspot/share/opto/cfgnode.cpp b/src/hotspot/share/opto/cfgnode.cpp
--- a/src/hotspot/share/opto/cfgnode.cpp
+++ b/src/hotspot/share/opto/cfgnode.cpp
@@ -41,10 +41,11 @@
 #include "opto/mulnode.hpp"
 #include "opto/phaseX.hpp"
 #include "opto/regmask.hpp"
 #include "opto/runtime.hpp"
 #include "opto/subnode.hpp"
+#include "opto/valuetypenode.hpp"
 #include "utilities/vmError.hpp"
 
 // Portions of code courtesy of Clifford Click
 
 // Optimization - Graph Style
@@ -370,11 +371,11 @@
   }
 
   return true; // The Region node is unreachable - it is dead.
 }
 
-bool RegionNode::try_clean_mem_phi(PhaseGVN *phase) {
+Node* PhiNode::try_clean_mem_phi(PhaseGVN *phase) {
   // Incremental inlining + PhaseStringOpts sometimes produce:
   //
   // cmpP with 1 top input
   //           |
   //          If
@@ -390,31 +391,30 @@
   // the Region stays in the graph. The top input from the cmpP is
   // propagated forward and a subgraph that is useful goes away. The
   // code below replaces the Phi with the MergeMem so that the Region
   // is simplified.
 
-  PhiNode* phi = has_unique_phi();
-  if (phi && phi->type() == Type::MEMORY && req() == 3 && phi->is_diamond_phi(true)) {
+  if (type() == Type::MEMORY && is_diamond_phi(true)) {
     MergeMemNode* m = NULL;
-    assert(phi->req() == 3, "same as region");
+    assert(req() == 3, "same as region");
+    Node* r = in(0);
     for (uint i = 1; i < 3; ++i) {
-      Node *mem = phi->in(i);
-      if (mem && mem->is_MergeMem() && in(i)->outcnt() == 1) {
+      Node *mem = in(i);
+      if (mem && mem->is_MergeMem() && r->in(i)->outcnt() == 1) {
         // Nothing is control-dependent on path #i except the region itself.
         m = mem->as_MergeMem();
         uint j = 3 - i;
-        Node* other = phi->in(j);
+        Node* other = in(j);
         if (other && other == m->base_memory()) {
           // m is a successor memory to other, and is not pinned inside the diamond, so push it out.
           // This will allow the diamond to collapse completely.
-          phase->is_IterGVN()->replace_node(phi, m);
-          return true;
+          return m;
         }
       }
     }
   }
-  return false;
+  return NULL;
 }
 
 //------------------------------Ideal------------------------------------------
 // Return a node which is more "ideal" than the current node.  Must preserve
 // the CFG, but we can still strip out dead paths.
@@ -425,12 +425,19 @@
   // Check for RegionNode with no Phi users and both inputs come from either
   // arm of the same IF.  If found, then the control-flow split is useless.
   bool has_phis = false;
   if (can_reshape) {            // Need DU info to check for Phi users
     has_phis = (has_phi() != NULL);       // Cache result
-    if (has_phis && try_clean_mem_phi(phase)) {
-      has_phis = false;
+    if (has_phis) {
+      PhiNode* phi = has_unique_phi();
+      if (phi != NULL) {
+        Node* m = phi->try_clean_mem_phi(phase);
+        if (m != NULL) {
+          phase->is_IterGVN()->replace_node(phi, m);
+          has_phis = false;
+        }
+      }
     }
 
     if (!has_phis) {            // No Phi users?  Nothing merging?
       for (uint i = 1; i < req()-1; i++) {
         Node *if1 = in(i);
@@ -894,11 +901,11 @@
 
 //----------------------------make---------------------------------------------
 // create a new phi with edges matching r and set (initially) to x
 PhiNode* PhiNode::make(Node* r, Node* x, const Type *t, const TypePtr* at) {
   uint preds = r->req();   // Number of predecessor paths
-  assert(t != Type::MEMORY || at == flatten_phi_adr_type(at), "flatten at");
+  assert(t != Type::MEMORY || at == flatten_phi_adr_type(at) || (flatten_phi_adr_type(at) == TypeAryPtr::VALUES && Compile::current()->flattened_accesses_share_alias()), "flatten at");
   PhiNode* p = new PhiNode(r, t, at);
   for (uint j = 1; j < preds; j++) {
     // Fill in all inputs, except those which the region does not yet have
     if (r->in(j) != NULL)
       p->init_req(j, x);
@@ -1104,19 +1111,14 @@
   // convert the one to the other.
   const TypePtr* ttp = _type->make_ptr();
   const TypeInstPtr* ttip = (ttp != NULL) ? ttp->isa_instptr() : NULL;
   const TypeKlassPtr* ttkp = (ttp != NULL) ? ttp->isa_klassptr() : NULL;
   bool is_intf = false;
-  if (ttip != NULL) {
-    ciKlass* k = ttip->klass();
-    if (k->is_loaded() && k->is_interface())
-      is_intf = true;
-  }
-  if (ttkp != NULL) {
-    ciKlass* k = ttkp->klass();
-    if (k->is_loaded() && k->is_interface())
-      is_intf = true;
+  if (ttip != NULL && ttip->is_loaded() && ttip->klass()->is_interface()) {
+    is_intf = true;
+  } else if (ttkp != NULL && ttkp->is_loaded() && ttkp->klass()->is_interface()) {
+    is_intf = true;
   }
 
   // Default case: merge all inputs
   const Type *t = Type::TOP;        // Merged type starting value
   for (uint i = 1; i < req(); ++i) {// For all paths in
@@ -1169,13 +1171,13 @@
     // both implement interface I, but their meet is at 'j/l/O' which
     // doesn't implement I, we have no way to tell if the result should
     // be 'I' or 'j/l/O'.  Thus we'll pick 'j/l/O'.  If this then flows
     // into a Phi which "knows" it's an Interface type we'll have to
     // uplift the type.
-    if (!t->empty() && ttip && ttip->is_loaded() && ttip->klass()->is_interface()) {
+    if (!t->empty() && ttip != NULL && ttip->is_loaded() && ttip->klass()->is_interface()) {
       assert(ft == _type, ""); // Uplift to interface
-    } else if (!t->empty() && ttkp && ttkp->is_loaded() && ttkp->klass()->is_interface()) {
+    } else if (!t->empty() && ttkp != NULL && ttkp->is_loaded() && ttkp->klass()->is_interface()) {
       assert(ft == _type, ""); // Uplift to interface
     } else {
       // We also have to handle 'evil cases' of interface- vs. class-arrays
       Type::get_arrays_base_elements(jt, _type, NULL, &ttip);
       if (!t->empty() && ttip != NULL && ttip->is_loaded() && ttip->klass()->is_interface()) {
@@ -1333,10 +1335,18 @@
   if (true_path != 0) {
     Node* id = is_cmove_id(phase, true_path);
     if (id != NULL)  return id;
   }
 
+  if (phase->is_IterGVN()) {
+    Node* m = try_clean_mem_phi(phase);
+    if (m != NULL) {
+      return m;
+    }
+  }
+
+
   return this;                     // No identity
 }
 
 //-----------------------------unique_input------------------------------------
 // Find the unique value, discounting top, self-loops, and casts.
@@ -1828,10 +1838,28 @@
   // Note: During parsing, phis are often transformed before their regions.
   // This means we have to use type_or_null to defend against untyped regions.
   if( phase->type_or_null(r) == Type::TOP ) // Dead code?
     return NULL;                // No change
 
+  // If all inputs are value types of the same type, push the value type node down
+  // through the phi because value type nodes should be merged through their input values.
+  if (req() > 2 && in(1) != NULL && in(1)->is_ValueTypeBase() && (can_reshape || in(1)->is_ValueType())) {
+    int opcode = in(1)->Opcode();
+    uint i = 2;
+    // Check if inputs are values of the same type
+    for (; i < req() && in(i) && in(i)->is_ValueTypeBase() && in(i)->cmp(*in(1)); i++) {
+      assert(in(i)->Opcode() == opcode, "mixing pointers and values?");
+    }
+    if (i == req()) {
+      ValueTypeBaseNode* vt = in(1)->as_ValueTypeBase()->clone_with_phis(phase, in(0));
+      for (uint i = 2; i < req(); ++i) {
+        vt->merge_with(phase, in(i)->as_ValueTypeBase(), i, i == (req()-1));
+      }
+      return vt;
+    }
+  }
+
   Node *top = phase->C->top();
   bool new_phi = (outcnt() == 0); // transforming new Phi
   // No change for igvn if new phi is not hooked
   if (new_phi && can_reshape)
     return NULL;
@@ -2571,10 +2599,16 @@
   if( phase->type(in(1)) == Type::TOP ) return in(1);
   if( phase->type(in(0)) == Type::TOP ) return in(0);
   // We only come from CatchProj, unless the CatchProj goes away.
   // If the CatchProj is optimized away, then we just carry the
   // exception oop through.
+
+  // CheckCastPPNode::Ideal() for value types reuses the exception
+  // paths of a call to perform an allocation: we can see a Phi here.
+  if (in(1)->is_Phi()) {
+    return this;
+  }
   CallNode *call = in(1)->in(0)->as_Call();
 
   return ( in(0)->is_CatchProj() && in(0)->in(0)->in(1) == in(1) )
     ? this
     : call->in(TypeFunc::Parms);
diff a/src/hotspot/share/prims/jvmtiRedefineClasses.cpp b/src/hotspot/share/prims/jvmtiRedefineClasses.cpp
--- a/src/hotspot/share/prims/jvmtiRedefineClasses.cpp
+++ b/src/hotspot/share/prims/jvmtiRedefineClasses.cpp
@@ -561,12 +561,11 @@
     case JVM_CONSTANT_Invalid: // fall through
 
     // At this stage, String could be here, but not StringIndex
     case JVM_CONSTANT_StringIndex: // fall through
 
-    // At this stage JVM_CONSTANT_UnresolvedClassInError should not be
-    // here
+    // At this stage JVM_CONSTANT_UnresolvedClassInError should not be here
     case JVM_CONSTANT_UnresolvedClassInError: // fall through
 
     default:
     {
       // leave a breadcrumb
diff a/src/hotspot/share/runtime/globals.hpp b/src/hotspot/share/runtime/globals.hpp
--- a/src/hotspot/share/runtime/globals.hpp
+++ b/src/hotspot/share/runtime/globals.hpp
@@ -763,10 +763,28 @@
           "Use SSE2 MOVQ instruction for Arraycopy")                        \
                                                                             \
   notproduct(bool, PrintFieldLayout, false,                                 \
           "Print field layout for each class")                              \
                                                                             \
+  notproduct(bool, PrintInlineLayout, false,                                \
+          "Print field layout for each inline type")                        \
+                                                                            \
+  notproduct(bool, PrintInlineArrayLayout, false,                           \
+          "Print array layout for each inline type array")                  \
+                                                                            \
+  product(intx, InlineArrayElemMaxFlatSize, -1,                             \
+          "Max size for flattening inline array elements, <0 no limit")     \
+                                                                            \
+  product(intx, InlineFieldMaxFlatSize, 128,                                \
+          "Max size for flattening inline type fields, <0 no limit")        \
+                                                                            \
+  product(intx, InlineArrayElemMaxFlatOops, 4,                              \
+          "Max nof embedded object references in an inline type to flatten, <0 no limit")  \
+                                                                            \
+  product(bool, InlineArrayAtomicAccess, false,                             \
+          "Atomic inline array accesses by-default, for all inline arrays") \
+                                                                            \
   /* Need to limit the extent of the padding to reasonable size.          */\
   /* 8K is well beyond the reasonable HW cache line size, even with       */\
   /* aggressive prefetching, while still leaving the room for segregating */\
   /* among the distinct pages.                                            */\
   product(intx, ContendedPaddingWidth, 128,                                 \
@@ -2475,19 +2493,47 @@
           "Start flight recording with options"))                           \
                                                                             \
   experimental(bool, UseFastUnorderedTimeStamps, false,                     \
           "Use platform unstable time where supported for timestamps only") \
                                                                             \
+  product(bool, EnableValhalla, true,                                       \
+          "Enable experimental Valhalla features")                          \
+                                                                            \
+  product_pd(bool, InlineTypePassFieldsAsArgs,                              \
+          "Pass each inline type field as an argument at calls")            \
+                                                                            \
+  product_pd(bool, InlineTypeReturnedAsFields,                              \
+          "Return fields instead of an inline type reference")              \
+                                                                            \
+  develop(bool, StressInlineTypeReturnedAsFields, false,                    \
+          "Stress return of fields instead of an inline type reference")    \
+                                                                            \
+  develop(bool, ScalarizeInlineTypes, true,                                 \
+          "Scalarize inline types in compiled code")                        \
+                                                                            \
+  diagnostic(ccstrlist, ForceNonTearable, "",                               \
+          "List of inline classes which are forced to be atomic "           \
+          "(whitespace and commas separate names, "                         \
+          "and leading and trailing stars '*' are wildcards)")              \
+                                                                            \
+  product(bool, PrintNewLayout, false,                                      \
+               "Print layout compute by new algorithm")                     \
+                                                                            \
+  product(bool, PrintFlattenableLayouts, false,                             \
+                "Print layout of inline classes and classes with "          \
+                "flattenable fields")                                       \
+                                                                            \
   product(bool, UseNewFieldLayout, true,                                    \
-               "(Deprecated) Use new algorithm to compute field layouts")   \
+                "(Deprecated) Use new algorithm to compute field layouts")  \
                                                                             \
   product(bool, UseEmptySlotsInSupers, true,                                \
                 "Allow allocating fields in empty slots of super-classes")  \
                                                                             \
   diagnostic(bool, DeoptimizeNMethodBarriersALot, false,                    \
                 "Make nmethod barriers deoptimise a lot.")                  \
 
+
 // Interface macros
 #define DECLARE_PRODUCT_FLAG(type, name, value, doc)      extern "C" type name;
 #define DECLARE_PD_PRODUCT_FLAG(type, name, doc)          extern "C" type name;
 #define DECLARE_DIAGNOSTIC_FLAG(type, name, value, doc)   extern "C" type name;
 #define DECLARE_PD_DIAGNOSTIC_FLAG(type, name, doc)       extern "C" type name;
diff a/src/hotspot/share/runtime/sharedRuntime.cpp b/src/hotspot/share/runtime/sharedRuntime.cpp
--- a/src/hotspot/share/runtime/sharedRuntime.cpp
+++ b/src/hotspot/share/runtime/sharedRuntime.cpp
@@ -42,16 +42,21 @@
 #include "interpreter/interpreter.hpp"
 #include "interpreter/interpreterRuntime.hpp"
 #include "jfr/jfrEvents.hpp"
 #include "logging/log.hpp"
 #include "memory/metaspaceShared.hpp"
+#include "memory/oopFactory.hpp"
 #include "memory/resourceArea.hpp"
 #include "memory/universe.hpp"
+#include "oops/access.hpp"
+#include "oops/fieldStreams.inline.hpp"
 #include "oops/klass.hpp"
 #include "oops/method.inline.hpp"
 #include "oops/objArrayKlass.hpp"
+#include "oops/objArrayOop.inline.hpp"
 #include "oops/oop.inline.hpp"
+#include "oops/valueKlass.inline.hpp"
 #include "prims/forte.hpp"
 #include "prims/jvmtiExport.hpp"
 #include "prims/methodHandles.hpp"
 #include "prims/nativeLookup.hpp"
 #include "runtime/arguments.hpp"
@@ -82,11 +87,10 @@
 RuntimeStub*        SharedRuntime::_wrong_method_abstract_blob;
 RuntimeStub*        SharedRuntime::_ic_miss_blob;
 RuntimeStub*        SharedRuntime::_resolve_opt_virtual_call_blob;
 RuntimeStub*        SharedRuntime::_resolve_virtual_call_blob;
 RuntimeStub*        SharedRuntime::_resolve_static_call_blob;
-address             SharedRuntime::_resolve_static_call_entry;
 
 DeoptimizationBlob* SharedRuntime::_deopt_blob;
 SafepointBlob*      SharedRuntime::_polling_page_vectors_safepoint_handler_blob;
 SafepointBlob*      SharedRuntime::_polling_page_safepoint_handler_blob;
 SafepointBlob*      SharedRuntime::_polling_page_return_handler_blob;
@@ -102,11 +106,10 @@
   _wrong_method_abstract_blob          = generate_resolve_blob(CAST_FROM_FN_PTR(address, SharedRuntime::handle_wrong_method_abstract), "wrong_method_abstract_stub");
   _ic_miss_blob                        = generate_resolve_blob(CAST_FROM_FN_PTR(address, SharedRuntime::handle_wrong_method_ic_miss),  "ic_miss_stub");
   _resolve_opt_virtual_call_blob       = generate_resolve_blob(CAST_FROM_FN_PTR(address, SharedRuntime::resolve_opt_virtual_call_C),   "resolve_opt_virtual_call");
   _resolve_virtual_call_blob           = generate_resolve_blob(CAST_FROM_FN_PTR(address, SharedRuntime::resolve_virtual_call_C),       "resolve_virtual_call");
   _resolve_static_call_blob            = generate_resolve_blob(CAST_FROM_FN_PTR(address, SharedRuntime::resolve_static_call_C),        "resolve_static_call");
-  _resolve_static_call_entry           = _resolve_static_call_blob->entry_point();
 
 #if COMPILER2_OR_JVMCI
   // Vectors are generated only by C2 and JVMCI.
   bool support_wide = is_wide_vector(MaxVectorSize);
   if (support_wide) {
@@ -1049,10 +1052,25 @@
 
   // Find caller and bci from vframe
   methodHandle caller(THREAD, vfst.method());
   int          bci   = vfst.bci();
 
+  // Substitutability test implementation piggy backs on static call resolution
+  Bytecodes::Code code = caller->java_code_at(bci);
+  if (code == Bytecodes::_if_acmpeq || code == Bytecodes::_if_acmpne) {
+    bc = Bytecodes::_invokestatic;
+    methodHandle attached_method(THREAD, extract_attached_method(vfst));
+    assert(attached_method.not_null(), "must have attached method");
+    SystemDictionary::ValueBootstrapMethods_klass()->initialize(CHECK_NH);
+    LinkResolver::resolve_invoke(callinfo, receiver, attached_method, bc, false, CHECK_NH);
+#ifdef ASSERT
+    Method* is_subst = SystemDictionary::ValueBootstrapMethods_klass()->find_method(vmSymbols::isSubstitutable_name(), vmSymbols::object_object_boolean_signature());
+    assert(callinfo.selected_method() == is_subst, "must be isSubstitutable method");
+#endif
+    return receiver;
+  }
+
   Bytecode_invoke bytecode(caller, bci);
   int bytecode_index = bytecode.index();
   bc = bytecode.invoke_code();
 
   methodHandle attached_method(THREAD, extract_attached_method(vfst));
@@ -1084,56 +1102,77 @@
           }
           break;
         default:
           break;
       }
+    } else {
+      assert(attached_method->has_scalarized_args(), "invalid use of attached method");
+      if (!attached_method->method_holder()->is_value()) {
+        // Ignore the attached method in this case to not confuse below code
+        attached_method = methodHandle(thread, NULL);
+      }
     }
   }
 
   assert(bc != Bytecodes::_illegal, "not initialized");
 
   bool has_receiver = bc != Bytecodes::_invokestatic &&
                       bc != Bytecodes::_invokedynamic &&
                       bc != Bytecodes::_invokehandle;
+  bool check_null_and_abstract = true;
 
   // Find receiver for non-static call
   if (has_receiver) {
     // This register map must be update since we need to find the receiver for
     // compiled frames. The receiver might be in a register.
     RegisterMap reg_map2(thread);
     frame stubFrame   = thread->last_frame();
     // Caller-frame is a compiled frame
     frame callerFrame = stubFrame.sender(&reg_map2);
+    bool caller_is_c1 = false;
 
-    if (attached_method.is_null()) {
-      Method* callee = bytecode.static_target(CHECK_NH);
+    if (callerFrame.is_compiled_frame() && !callerFrame.is_deoptimized_frame()) {
+      caller_is_c1 = callerFrame.cb()->is_compiled_by_c1();
+    }
+
+    Method* callee = attached_method();
+    if (callee == NULL) {
+      callee = bytecode.static_target(CHECK_NH);
       if (callee == NULL) {
         THROW_(vmSymbols::java_lang_NoSuchMethodException(), nullHandle);
       }
     }
-
-    // Retrieve from a compiled argument list
-    receiver = Handle(THREAD, callerFrame.retrieve_receiver(&reg_map2));
-
-    if (receiver.is_null()) {
-      THROW_(vmSymbols::java_lang_NullPointerException(), nullHandle);
+    if (!caller_is_c1 && callee->has_scalarized_args() && callee->method_holder()->is_value()) {
+      // If the receiver is a value type that is passed as fields, no oop is available.
+      // Resolve the call without receiver null checking.
+      assert(attached_method.not_null() && !attached_method->is_abstract(), "must have non-abstract attached method");
+      if (bc == Bytecodes::_invokeinterface) {
+        bc = Bytecodes::_invokevirtual; // C2 optimistically replaces interface calls by virtual calls
+      }
+      check_null_and_abstract = false;
+    } else {
+      // Retrieve from a compiled argument list
+      receiver = Handle(THREAD, callerFrame.retrieve_receiver(&reg_map2));
+      if (receiver.is_null()) {
+        THROW_(vmSymbols::java_lang_NullPointerException(), nullHandle);
+      }
     }
   }
 
   // Resolve method
   if (attached_method.not_null()) {
     // Parameterized by attached method.
-    LinkResolver::resolve_invoke(callinfo, receiver, attached_method, bc, CHECK_NH);
+    LinkResolver::resolve_invoke(callinfo, receiver, attached_method, bc, check_null_and_abstract, CHECK_NH);
   } else {
     // Parameterized by bytecode.
     constantPoolHandle constants(THREAD, caller->constants());
     LinkResolver::resolve_invoke(callinfo, receiver, constants, bytecode_index, bc, CHECK_NH);
   }
 
 #ifdef ASSERT
   // Check that the receiver klass is of the right subtype and that it is initialized for virtual calls
-  if (has_receiver) {
+  if (has_receiver && check_null_and_abstract) {
     assert(receiver.not_null(), "should have thrown exception");
     Klass* receiver_klass = receiver->klass();
     Klass* rk = NULL;
     if (attached_method.not_null()) {
       // In case there's resolved method attached, use its holder during the check.
@@ -1188,13 +1227,14 @@
 }
 
 // Resolves a call.
 methodHandle SharedRuntime::resolve_helper(JavaThread *thread,
                                            bool is_virtual,
-                                           bool is_optimized, TRAPS) {
+                                           bool is_optimized,
+                                           bool* caller_is_c1, TRAPS) {
   methodHandle callee_method;
-  callee_method = resolve_sub_helper(thread, is_virtual, is_optimized, THREAD);
+  callee_method = resolve_sub_helper(thread, is_virtual, is_optimized, caller_is_c1, THREAD);
   if (JvmtiExport::can_hotswap_or_post_breakpoint()) {
     int retry_count = 0;
     while (!HAS_PENDING_EXCEPTION && callee_method->is_old() &&
            callee_method->method_holder() != SystemDictionary::Object_klass()) {
       // If has a pending exception then there is no need to re-try to
@@ -1207,11 +1247,11 @@
       // in the middle of resolve. If it is looping here more than 100 times
       // means then there could be a bug here.
       guarantee((retry_count++ < 100),
                 "Could not resolve to latest version of redefined method");
       // method is redefined in the middle of resolve so re-try.
-      callee_method = resolve_sub_helper(thread, is_virtual, is_optimized, THREAD);
+      callee_method = resolve_sub_helper(thread, is_virtual, is_optimized, caller_is_c1, THREAD);
     }
   }
   return callee_method;
 }
 
@@ -1238,21 +1278,28 @@
 #ifdef ASSERT
   address dest_entry_point = callee == NULL ? 0 : callee->entry_point(); // used below
 #endif
 
   bool is_nmethod = caller_nm->is_nmethod();
+  bool caller_is_c1 = caller_nm->is_compiled_by_c1();
 
   if (is_virtual) {
-    assert(receiver.not_null() || invoke_code == Bytecodes::_invokehandle, "sanity check");
+    Klass* receiver_klass = NULL;
+    if (InlineTypePassFieldsAsArgs && !caller_is_c1 && callee_method->method_holder()->is_value()) {
+      // If the receiver is an inline type that is passed as fields, no oop is available
+      receiver_klass = callee_method->method_holder();
+    } else {
+      assert(receiver.not_null() || invoke_code == Bytecodes::_invokehandle, "sanity check");
+      receiver_klass = invoke_code == Bytecodes::_invokehandle ? NULL : receiver->klass();
+    }
     bool static_bound = call_info.resolved_method()->can_be_statically_bound();
-    Klass* klass = invoke_code == Bytecodes::_invokehandle ? NULL : receiver->klass();
-    CompiledIC::compute_monomorphic_entry(callee_method, klass,
-                     is_optimized, static_bound, is_nmethod, virtual_call_info,
+    CompiledIC::compute_monomorphic_entry(callee_method, receiver_klass,
+                     is_optimized, static_bound, is_nmethod, caller_is_c1, virtual_call_info,
                      CHECK_false);
   } else {
     // static call
-    CompiledStaticCall::compute_entry(callee_method, is_nmethod, static_call_info);
+    CompiledStaticCall::compute_entry(callee_method, caller_nm, static_call_info);
   }
 
   // grab lock, check for deoptimization and potentially patch caller
   {
     CompiledICLocker ml(caller_nm);
@@ -1300,19 +1347,21 @@
 
 // Resolves a call.  The compilers generate code for calls that go here
 // and are patched with the real destination of the call.
 methodHandle SharedRuntime::resolve_sub_helper(JavaThread *thread,
                                                bool is_virtual,
-                                               bool is_optimized, TRAPS) {
+                                               bool is_optimized,
+                                               bool* caller_is_c1, TRAPS) {
 
   ResourceMark rm(thread);
   RegisterMap cbl_map(thread, false);
   frame caller_frame = thread->last_frame().sender(&cbl_map);
 
   CodeBlob* caller_cb = caller_frame.cb();
   guarantee(caller_cb != NULL && caller_cb->is_compiled(), "must be called from compiled method");
   CompiledMethod* caller_nm = caller_cb->as_compiled_method_or_null();
+  *caller_is_c1 = caller_nm->is_compiled_by_c1();
 
   // make sure caller is not getting deoptimized
   // and removed before we are done with it.
   // CLEANUP - with lazy deopt shouldn't need this lock
   nmethodLocker caller_lock(caller_nm);
@@ -1410,18 +1459,19 @@
   frame caller_frame = stub_frame.sender(&reg_map);
   assert(!caller_frame.is_interpreted_frame() && !caller_frame.is_entry_frame(), "unexpected frame");
 #endif /* ASSERT */
 
   methodHandle callee_method;
+  bool is_optimized = false;
+  bool caller_is_c1 = false;
   JRT_BLOCK
-    callee_method = SharedRuntime::handle_ic_miss_helper(thread, CHECK_NULL);
+    callee_method = SharedRuntime::handle_ic_miss_helper(thread, is_optimized, caller_is_c1, CHECK_NULL);
     // Return Method* through TLS
     thread->set_vm_result_2(callee_method());
   JRT_BLOCK_END
   // return compiled code entry point after potential safepoints
-  assert(callee_method->verified_code_entry() != NULL, " Jump to zero!");
-  return callee_method->verified_code_entry();
+  return entry_for_handle_wrong_method(callee_method, false, is_optimized, caller_is_c1);
 JRT_END
 
 
 // Handle call site that has been made non-entrant
 JRT_BLOCK_ENTRY(address, SharedRuntime::handle_wrong_method(JavaThread* thread))
@@ -1460,18 +1510,20 @@
     }
   }
 
   // Must be compiled to compiled path which is safe to stackwalk
   methodHandle callee_method;
+  bool is_static_call = false;
+  bool is_optimized = false;
+  bool caller_is_c1 = false;
   JRT_BLOCK
     // Force resolving of caller (if we called from compiled frame)
-    callee_method = SharedRuntime::reresolve_call_site(thread, CHECK_NULL);
+    callee_method = SharedRuntime::reresolve_call_site(thread, is_static_call, is_optimized, caller_is_c1, CHECK_NULL);
     thread->set_vm_result_2(callee_method());
   JRT_BLOCK_END
   // return compiled code entry point after potential safepoints
-  assert(callee_method->verified_code_entry() != NULL, " Jump to zero!");
-  return callee_method->verified_code_entry();
+  return entry_for_handle_wrong_method(callee_method, is_static_call, is_optimized, caller_is_c1);
 JRT_END
 
 // Handle abstract method call
 JRT_BLOCK_ENTRY(address, SharedRuntime::handle_wrong_method_abstract(JavaThread* thread))
   // Verbose error message for AbstractMethodError.
@@ -1505,65 +1557,75 @@
 
 
 // resolve a static call and patch code
 JRT_BLOCK_ENTRY(address, SharedRuntime::resolve_static_call_C(JavaThread *thread ))
   methodHandle callee_method;
+  bool caller_is_c1;
   JRT_BLOCK
-    callee_method = SharedRuntime::resolve_helper(thread, false, false, CHECK_NULL);
+    callee_method = SharedRuntime::resolve_helper(thread, false, false, &caller_is_c1, CHECK_NULL);
     thread->set_vm_result_2(callee_method());
   JRT_BLOCK_END
   // return compiled code entry point after potential safepoints
-  assert(callee_method->verified_code_entry() != NULL, " Jump to zero!");
-  return callee_method->verified_code_entry();
+  address entry = caller_is_c1 ?
+    callee_method->verified_value_code_entry() : callee_method->verified_code_entry();
+  assert(entry != NULL, "Jump to zero!");
+  return entry;
 JRT_END
 
 
 // resolve virtual call and update inline cache to monomorphic
 JRT_BLOCK_ENTRY(address, SharedRuntime::resolve_virtual_call_C(JavaThread *thread ))
   methodHandle callee_method;
+  bool caller_is_c1;
   JRT_BLOCK
-    callee_method = SharedRuntime::resolve_helper(thread, true, false, CHECK_NULL);
+    callee_method = SharedRuntime::resolve_helper(thread, true, false, &caller_is_c1, CHECK_NULL);
     thread->set_vm_result_2(callee_method());
   JRT_BLOCK_END
   // return compiled code entry point after potential safepoints
-  assert(callee_method->verified_code_entry() != NULL, " Jump to zero!");
-  return callee_method->verified_code_entry();
+  address entry = caller_is_c1 ?
+    callee_method->verified_value_code_entry() : callee_method->verified_value_ro_code_entry();
+  assert(entry != NULL, "Jump to zero!");
+  return entry;
 JRT_END
 
 
 // Resolve a virtual call that can be statically bound (e.g., always
 // monomorphic, so it has no inline cache).  Patch code to resolved target.
 JRT_BLOCK_ENTRY(address, SharedRuntime::resolve_opt_virtual_call_C(JavaThread *thread))
   methodHandle callee_method;
+  bool caller_is_c1;
   JRT_BLOCK
-    callee_method = SharedRuntime::resolve_helper(thread, true, true, CHECK_NULL);
+    callee_method = SharedRuntime::resolve_helper(thread, true, true, &caller_is_c1, CHECK_NULL);
     thread->set_vm_result_2(callee_method());
   JRT_BLOCK_END
   // return compiled code entry point after potential safepoints
-  assert(callee_method->verified_code_entry() != NULL, " Jump to zero!");
-  return callee_method->verified_code_entry();
+  address entry = caller_is_c1 ?
+    callee_method->verified_value_code_entry() : callee_method->verified_code_entry();
+  assert(entry != NULL, "Jump to zero!");
+  return entry;
 JRT_END
 
 // The handle_ic_miss_helper_internal function returns false if it failed due
 // to either running out of vtable stubs or ic stubs due to IC transitions
 // to transitional states. The needs_ic_stub_refill value will be set if
 // the failure was due to running out of IC stubs, in which case handle_ic_miss_helper
 // refills the IC stubs and tries again.
 bool SharedRuntime::handle_ic_miss_helper_internal(Handle receiver, CompiledMethod* caller_nm,
                                                    const frame& caller_frame, methodHandle callee_method,
                                                    Bytecodes::Code bc, CallInfo& call_info,
-                                                   bool& needs_ic_stub_refill, TRAPS) {
+                                                   bool& needs_ic_stub_refill, bool& is_optimized, bool caller_is_c1, TRAPS) {
   CompiledICLocker ml(caller_nm);
   CompiledIC* inline_cache = CompiledIC_before(caller_nm, caller_frame.pc());
   bool should_be_mono = false;
   if (inline_cache->is_optimized()) {
     if (TraceCallFixup) {
       ResourceMark rm(THREAD);
       tty->print("OPTIMIZED IC miss (%s) call to", Bytecodes::name(bc));
       callee_method->print_short_name(tty);
       tty->print_cr(" code: " INTPTR_FORMAT, p2i(callee_method->code()));
     }
+    is_optimized = true;
     should_be_mono = true;
   } else if (inline_cache->is_icholder_call()) {
     CompiledICHolder* ic_oop = inline_cache->cached_icholder();
     if (ic_oop != NULL) {
       if (!ic_oop->is_loader_alive()) {
@@ -1597,19 +1659,20 @@
     Klass* receiver_klass = receiver()->klass();
     inline_cache->compute_monomorphic_entry(callee_method,
                                             receiver_klass,
                                             inline_cache->is_optimized(),
                                             false, caller_nm->is_nmethod(),
+                                            caller_nm->is_compiled_by_c1(),
                                             info, CHECK_false);
     if (!inline_cache->set_to_monomorphic(info)) {
       needs_ic_stub_refill = true;
       return false;
     }
   } else if (!inline_cache->is_megamorphic() && !inline_cache->is_clean()) {
     // Potential change to megamorphic
 
-    bool successful = inline_cache->set_to_megamorphic(&call_info, bc, needs_ic_stub_refill, CHECK_false);
+    bool successful = inline_cache->set_to_megamorphic(&call_info, bc, needs_ic_stub_refill, caller_is_c1, CHECK_false);
     if (needs_ic_stub_refill) {
       return false;
     }
     if (!successful) {
       if (!inline_cache->set_to_clean()) {
@@ -1621,11 +1684,11 @@
     // Either clean or megamorphic
   }
   return true;
 }
 
-methodHandle SharedRuntime::handle_ic_miss_helper(JavaThread *thread, TRAPS) {
+methodHandle SharedRuntime::handle_ic_miss_helper(JavaThread *thread, bool& is_optimized, bool& caller_is_c1, TRAPS) {
   ResourceMark rm(thread);
   CallInfo call_info;
   Bytecodes::Code bc;
 
   // receiver is NULL for static calls. An exception is thrown for NULL
@@ -1641,11 +1704,13 @@
   // plain ic_miss) and the site will be converted to an optimized virtual call site
   // never to miss again. I don't believe C2 will produce code like this but if it
   // did this would still be the correct thing to do for it too, hence no ifdef.
   //
   if (call_info.resolved_method()->can_be_statically_bound()) {
-    methodHandle callee_method = SharedRuntime::reresolve_call_site(thread, CHECK_(methodHandle()));
+    bool is_static_call = false;
+    methodHandle callee_method = SharedRuntime::reresolve_call_site(thread, is_static_call, is_optimized, caller_is_c1, CHECK_(methodHandle()));
+    assert(!is_static_call, "IC miss at static call?");
     if (TraceCallFixup) {
       RegisterMap reg_map(thread, false);
       frame caller_frame = thread->last_frame().sender(&reg_map);
       ResourceMark rm(thread);
       tty->print("converting IC miss to reresolve (%s) call to", Bytecodes::name(bc));
@@ -1691,16 +1756,17 @@
   // that refills them.
   RegisterMap reg_map(thread, false);
   frame caller_frame = thread->last_frame().sender(&reg_map);
   CodeBlob* cb = caller_frame.cb();
   CompiledMethod* caller_nm = cb->as_compiled_method();
+  caller_is_c1 = caller_nm->is_compiled_by_c1();
 
   for (;;) {
     ICRefillVerifier ic_refill_verifier;
     bool needs_ic_stub_refill = false;
     bool successful = handle_ic_miss_helper_internal(receiver, caller_nm, caller_frame, callee_method,
-                                                     bc, call_info, needs_ic_stub_refill, CHECK_(methodHandle()));
+                                                     bc, call_info, needs_ic_stub_refill, is_optimized, caller_is_c1, CHECK_(methodHandle()));
     if (successful || !needs_ic_stub_refill) {
       return callee_method;
     } else {
       InlineCacheBuffer::refill_ic_stubs();
     }
@@ -1728,11 +1794,11 @@
 // Resets a call-site in compiled code so it will get resolved again.
 // This routines handles both virtual call sites, optimized virtual call
 // sites, and static call sites. Typically used to change a call sites
 // destination from compiled to interpreted.
 //
-methodHandle SharedRuntime::reresolve_call_site(JavaThread *thread, TRAPS) {
+methodHandle SharedRuntime::reresolve_call_site(JavaThread *thread, bool& is_static_call, bool& is_optimized, bool& caller_is_c1, TRAPS) {
   ResourceMark rm(thread);
   RegisterMap reg_map(thread, false);
   frame stub_frame = thread->last_frame();
   assert(stub_frame.is_runtime_frame(), "must be a runtimeStub");
   frame caller = stub_frame.sender(&reg_map);
@@ -1744,11 +1810,11 @@
   if (caller.is_compiled_frame() && !caller.is_deoptimized_frame()) {
 
     address pc = caller.pc();
 
     // Check for static or virtual call
-    bool is_static_call = false;
+    CompiledMethod* caller_nm = CodeCache::find_compiled(pc);
     CompiledMethod* caller_nm = CodeCache::find_compiled(pc);
 
     // Default call_addr is the location of the "basic" call.
     // Determine the address of the call we a reresolving. With
     // Inline Caches we will always find a recognizable call.
@@ -1789,10 +1855,11 @@
           is_static_call = true;
         } else {
           assert(iter.type() == relocInfo::virtual_call_type ||
                  iter.type() == relocInfo::opt_virtual_call_type
                 , "unexpected relocInfo. type");
+          is_optimized = (iter.type() == relocInfo::opt_virtual_call_type);
         }
       } else {
         assert(!UseInlineCaches, "relocation info. must exist for this address");
       }
 
@@ -1814,11 +1881,10 @@
     }
   }
 
   methodHandle callee_method = find_callee_method(thread, CHECK_(methodHandle()));
 
-
 #ifndef PRODUCT
   Atomic::inc(&_wrong_method_ctr);
 
   if (TraceCallFixup) {
     ResourceMark rm(thread);
@@ -1909,12 +1975,10 @@
 // interpreted. If the caller is compiled we attempt to patch the caller
 // so he no longer calls into the interpreter.
 JRT_LEAF(void, SharedRuntime::fixup_callers_callsite(Method* method, address caller_pc))
   Method* moop(method);
 
-  address entry_point = moop->from_compiled_entry_no_trampoline();
-
   // It's possible that deoptimization can occur at a call site which hasn't
   // been resolved yet, in which case this function will be called from
   // an nmethod that has been patched for deopt and we can ignore the
   // request for a fixup.
   // Also it is possible that we lost a race in that from_compiled_entry
@@ -1922,11 +1986,15 @@
   // we did we'd leap into space because the callsite needs to use
   // "to interpreter" stub in order to load up the Method*. Don't
   // ask me how I know this...
 
   CodeBlob* cb = CodeCache::find_blob(caller_pc);
-  if (cb == NULL || !cb->is_compiled() || entry_point == moop->get_c2i_entry()) {
+  if (cb == NULL || !cb->is_compiled()) {
+    return;
+  }
+  address entry_point = moop->from_compiled_entry_no_trampoline(cb->is_compiled_by_c1());
+  if (entry_point == moop->get_c2i_entry()) {
     return;
   }
 
   // The check above makes sure this is a nmethod.
   CompiledMethod* nm = cb->as_compiled_method_or_null();
@@ -2283,18 +2351,31 @@
                // Otherwise _value._fingerprint is the array.
 
   // Remap BasicTypes that are handled equivalently by the adapters.
   // These are correct for the current system but someday it might be
   // necessary to make this mapping platform dependent.
-  static int adapter_encoding(BasicType in) {
+  static int adapter_encoding(BasicType in, bool is_valuetype) {
     switch (in) {
       case T_BOOLEAN:
       case T_BYTE:
       case T_SHORT:
-      case T_CHAR:
-        // There are all promoted to T_INT in the calling convention
-        return T_INT;
+      case T_CHAR: {
+        if (is_valuetype) {
+          // Do not widen inline type field types
+          assert(InlineTypePassFieldsAsArgs, "must be enabled");
+          return in;
+        } else {
+          // They are all promoted to T_INT in the calling convention
+          return T_INT;
+        }
+      }
+
+      case T_VALUETYPE: {
+        // If inline types are passed as fields, return 'in' to differentiate
+        // between a T_VALUETYPE and a T_OBJECT in the signature.
+        return InlineTypePassFieldsAsArgs ? in : adapter_encoding(T_OBJECT, false);
+      }
 
       case T_OBJECT:
       case T_ARRAY:
         // In other words, we assume that any register good enough for
         // an int or long is good enough for a managed pointer.
@@ -2316,13 +2397,14 @@
         return T_CONFLICT;
     }
   }
 
  public:
-  AdapterFingerPrint(int total_args_passed, BasicType* sig_bt) {
+  AdapterFingerPrint(const GrowableArray<SigEntry>* sig, bool has_ro_adapter = false) {
     // The fingerprint is based on the BasicType signature encoded
     // into an array of ints with eight entries per int.
+    int total_args_passed = (sig != NULL) ? sig->length() : 0;
     int* ptr;
     int len = (total_args_passed + (_basic_types_per_int-1)) / _basic_types_per_int;
     if (len <= _compact_int_count) {
       assert(_compact_int_count == 3, "else change next line");
       _value._compact[0] = _value._compact[1] = _value._compact[2] = 0;
@@ -2336,21 +2418,41 @@
       ptr = _value._fingerprint;
     }
 
     // Now pack the BasicTypes with 8 per int
     int sig_index = 0;
+    BasicType prev_sbt = T_ILLEGAL;
+    int vt_count = 0;
     for (int index = 0; index < len; index++) {
       int value = 0;
       for (int byte = 0; byte < _basic_types_per_int; byte++) {
-        int bt = ((sig_index < total_args_passed)
-                  ? adapter_encoding(sig_bt[sig_index++])
-                  : 0);
+        int bt = 0;
+        if (sig_index < total_args_passed) {
+          BasicType sbt = sig->at(sig_index++)._bt;
+          if (InlineTypePassFieldsAsArgs && sbt == T_VALUETYPE) {
+            // Found start of inline type in signature
+            vt_count++;
+            if (sig_index == 1 && has_ro_adapter) {
+              // With a ro_adapter, replace receiver value type delimiter by T_VOID to prevent matching
+              // with other adapters that have the same value type as first argument and no receiver.
+              sbt = T_VOID;
+            }
+          } else if (InlineTypePassFieldsAsArgs && sbt == T_VOID &&
+                     prev_sbt != T_LONG && prev_sbt != T_DOUBLE) {
+            // Found end of inline type in signature
+            vt_count--;
+            assert(vt_count >= 0, "invalid vt_count");
+          }
+          bt = adapter_encoding(sbt, vt_count > 0);
+          prev_sbt = sbt;
+        }
         assert((bt & _basic_type_mask) == bt, "must fit in 4 bits");
         value = (value << _basic_type_bits) | bt;
       }
       ptr[index] = value;
     }
+    assert(vt_count == 0, "invalid vt_count");
   }
 
   ~AdapterFingerPrint() {
     if (_length > 0) {
       FREE_C_HEAP_ARRAY(int, _value._fingerprint);
@@ -2432,13 +2534,16 @@
  public:
   AdapterHandlerTable()
     : BasicHashtable<mtCode>(293, (DumpSharedSpaces ? sizeof(CDSAdapterHandlerEntry) : sizeof(AdapterHandlerEntry))) { }
 
   // Create a new entry suitable for insertion in the table
-  AdapterHandlerEntry* new_entry(AdapterFingerPrint* fingerprint, address i2c_entry, address c2i_entry, address c2i_unverified_entry, address c2i_no_clinit_check_entry) {
+  AdapterHandlerEntry* new_entry(AdapterFingerPrint* fingerprint, address i2c_entry, address c2i_entry,
+                                 address c2i_value_entry, address c2i_value_ro_entry,
+                                 address c2i_unverified_entry, address c2i_unverified_value_entry, address c2i_no_clinit_check_entry) {
     AdapterHandlerEntry* entry = (AdapterHandlerEntry*)BasicHashtable<mtCode>::new_entry(fingerprint->compute_hash());
-    entry->init(fingerprint, i2c_entry, c2i_entry, c2i_unverified_entry, c2i_no_clinit_check_entry);
+    entry->init(fingerprint, i2c_entry, c2i_entry, c2i_value_entry, c2i_value_ro_entry,
+                c2i_unverified_entry, c2i_unverified_value_entry, c2i_no_clinit_check_entry);
     if (DumpSharedSpaces) {
       ((CDSAdapterHandlerEntry*)entry)->init();
     }
     return entry;
   }
@@ -2453,13 +2558,13 @@
     entry->deallocate();
     BasicHashtable<mtCode>::free_entry(entry);
   }
 
   // Find a entry with the same fingerprint if it exists
-  AdapterHandlerEntry* lookup(int total_args_passed, BasicType* sig_bt) {
+  AdapterHandlerEntry* lookup(const GrowableArray<SigEntry>* sig, bool has_ro_adapter = false) {
     NOT_PRODUCT(_lookups++);
-    AdapterFingerPrint fp(total_args_passed, sig_bt);
+    AdapterFingerPrint fp(sig, has_ro_adapter);
     unsigned int hash = fp.compute_hash();
     int index = hash_to_index(hash);
     for (AdapterHandlerEntry* e = bucket(index); e != NULL; e = e->next()) {
       NOT_PRODUCT(_buckets++);
       if (e->hash() == hash) {
@@ -2551,11 +2656,11 @@
 
 // ---------------------------------------------------------------------------
 // Implementation of AdapterHandlerLibrary
 AdapterHandlerTable* AdapterHandlerLibrary::_adapters = NULL;
 AdapterHandlerEntry* AdapterHandlerLibrary::_abstract_method_handler = NULL;
-const int AdapterHandlerLibrary_size = 16*K;
+const int AdapterHandlerLibrary_size = 32*K;
 BufferBlob* AdapterHandlerLibrary::_buffer = NULL;
 
 BufferBlob* AdapterHandlerLibrary::buffer_blob() {
   // Should be called only when AdapterHandlerLibrary_lock is active.
   if (_buffer == NULL) // Initialize lazily
@@ -2575,86 +2680,309 @@
   // are never compiled so an i2c entry is somewhat meaningless, but
   // throw AbstractMethodError just in case.
   // Pass wrong_method_abstract for the c2i transitions to return
   // AbstractMethodError for invalid invocations.
   address wrong_method_abstract = SharedRuntime::get_handle_wrong_method_abstract_stub();
-  _abstract_method_handler = AdapterHandlerLibrary::new_entry(new AdapterFingerPrint(0, NULL),
+  _abstract_method_handler = AdapterHandlerLibrary::new_entry(new AdapterFingerPrint(NULL),
                                                               StubRoutines::throw_AbstractMethodError_entry(),
+                                                              wrong_method_abstract, wrong_method_abstract, wrong_method_abstract,
                                                               wrong_method_abstract, wrong_method_abstract);
 }
 
 AdapterHandlerEntry* AdapterHandlerLibrary::new_entry(AdapterFingerPrint* fingerprint,
                                                       address i2c_entry,
                                                       address c2i_entry,
+                                                      address c2i_value_entry,
+                                                      address c2i_value_ro_entry,
                                                       address c2i_unverified_entry,
+                                                      address c2i_unverified_value_entry,
                                                       address c2i_no_clinit_check_entry) {
-  return _adapters->new_entry(fingerprint, i2c_entry, c2i_entry, c2i_unverified_entry, c2i_no_clinit_check_entry);
+  return _adapters->new_entry(fingerprint, i2c_entry, c2i_entry, c2i_value_entry, c2i_value_ro_entry, c2i_unverified_entry,
+                              c2i_unverified_value_entry, c2i_no_clinit_check_entry);
+}
+
+static void generate_trampoline(address trampoline, address destination) {
+  if (*(int*)trampoline == 0) {
+    CodeBuffer buffer(trampoline, (int)SharedRuntime::trampoline_size());
+    MacroAssembler _masm(&buffer);
+    SharedRuntime::generate_trampoline(&_masm, destination);
+    assert(*(int*)trampoline != 0, "Instruction(s) for trampoline must not be encoded as zeros.");
+      _masm.flush();
+
+    if (PrintInterpreter) {
+      Disassembler::decode(buffer.insts_begin(), buffer.insts_end());
+    }
+  }
 }
 
 AdapterHandlerEntry* AdapterHandlerLibrary::get_adapter(const methodHandle& method) {
   AdapterHandlerEntry* entry = get_adapter0(method);
   if (entry != NULL && method->is_shared()) {
     // See comments around Method::link_method()
     MutexLocker mu(AdapterHandlerLibrary_lock);
     if (method->adapter() == NULL) {
       method->update_adapter_trampoline(entry);
     }
-    address trampoline = method->from_compiled_entry();
-    if (*(int*)trampoline == 0) {
-      CodeBuffer buffer(trampoline, (int)SharedRuntime::trampoline_size());
-      MacroAssembler _masm(&buffer);
-      SharedRuntime::generate_trampoline(&_masm, entry->get_c2i_entry());
-      assert(*(int*)trampoline != 0, "Instruction(s) for trampoline must not be encoded as zeros.");
-      _masm.flush();
+    generate_trampoline(method->from_compiled_entry(),          entry->get_c2i_entry());
+    generate_trampoline(method->from_compiled_value_ro_entry(), entry->get_c2i_value_ro_entry());
+    generate_trampoline(method->from_compiled_value_entry(),    entry->get_c2i_value_entry());
+  }
+
+  return entry;
+}
+
 
-      if (PrintInterpreter) {
-        Disassembler::decode(buffer.insts_begin(), buffer.insts_end());
+CompiledEntrySignature::CompiledEntrySignature(Method* method) :
+  _method(method), _num_value_args(0), _has_value_recv(false),
+  _sig_cc(NULL), _sig_cc_ro(NULL), _regs(NULL), _regs_cc(NULL), _regs_cc_ro(NULL),
+  _args_on_stack(0), _args_on_stack_cc(0), _args_on_stack_cc_ro(0),
+  _c1_needs_stack_repair(false), _c2_needs_stack_repair(false), _has_scalarized_args(false) {
+  _has_reserved_entries = false;
+  _sig = new GrowableArray<SigEntry>(method->size_of_parameters());
+
+}
+
+int CompiledEntrySignature::compute_scalarized_cc(GrowableArray<SigEntry>*& sig_cc, VMRegPair*& regs_cc, bool scalar_receiver) {
+  InstanceKlass* holder = _method->method_holder();
+  sig_cc = new GrowableArray<SigEntry>(_method->size_of_parameters());
+  if (!_method->is_static()) {
+    if (holder->is_value() && scalar_receiver && ValueKlass::cast(holder)->is_scalarizable()) {
+      sig_cc->appendAll(ValueKlass::cast(holder)->extended_sig());
+    } else {
+      SigEntry::add_entry(sig_cc, T_OBJECT);
+    }
+  }
+  Thread* THREAD = Thread::current();
+  for (SignatureStream ss(_method->signature()); !ss.at_return_type(); ss.next()) {
+    if (ss.type() == T_VALUETYPE) {
+      ValueKlass* vk = ss.as_value_klass(holder);
+      if (vk->is_scalarizable()) {
+        sig_cc->appendAll(vk->extended_sig());
+      } else {
+        SigEntry::add_entry(sig_cc, T_OBJECT);
       }
+    } else {
+      SigEntry::add_entry(sig_cc, ss.type());
     }
   }
+  regs_cc = NEW_RESOURCE_ARRAY(VMRegPair, sig_cc->length() + 2);
+  return SharedRuntime::java_calling_convention(sig_cc, regs_cc);
+}
 
-  return entry;
+int CompiledEntrySignature::insert_reserved_entry(int ret_off) {
+  // Find index in signature that belongs to return address slot
+  BasicType bt = T_ILLEGAL;
+  int i = 0;
+  for (uint off = 0; i < _sig_cc->length(); ++i) {
+    if (SigEntry::skip_value_delimiters(_sig_cc, i)) {
+      VMReg first = _regs_cc[off++].first();
+      if (first->is_valid() && first->is_stack()) {
+        // Select a type for the reserved entry that will end up on the stack
+        bt = _sig_cc->at(i)._bt;
+        if (((int)first->reg2stack() + VMRegImpl::slots_per_word) == ret_off) {
+          break; // Index of the return address found
+        }
+      }
+    }
+  }
+  // Insert reserved entry and re-compute calling convention
+  SigEntry::insert_reserved_entry(_sig_cc, i, bt);
+  return SharedRuntime::java_calling_convention(_sig_cc, _regs_cc);
+}
+
+// See if we can save space by sharing the same entry for VVEP and VVEP(RO),
+// or the same entry for VEP and VVEP(RO).
+CodeOffsets::Entries CompiledEntrySignature::c1_value_ro_entry_type() const {
+  if (!has_scalarized_args()) {
+    // VEP/VVEP/VVEP(RO) all share the same entry. There's no packing.
+    return CodeOffsets::Verified_Entry;
+  }
+  if (_method->is_static()) {
+    // Static methods don't need VVEP(RO)
+    return CodeOffsets::Verified_Entry;
+  }
+
+  if (has_value_recv()) {
+    if (num_value_args() == 1) {
+      // Share same entry for VVEP and VVEP(RO).
+      // This is quite common: we have an instance method in a ValueKlass that has
+      // no value args other than <this>.
+      return CodeOffsets::Verified_Value_Entry;
+    } else {
+      assert(num_value_args() > 1, "must be");
+      // No sharing:
+      //   VVEP(RO) -- <this> is passed as object
+      //   VEP      -- <this> is passed as fields
+      return CodeOffsets::Verified_Value_Entry_RO;
+    }
+  }
+
+  // Either a static method, or <this> is not a value type
+  if (args_on_stack_cc() != args_on_stack_cc_ro() || _has_reserved_entries) {
+    // No sharing:
+    // Some arguments are passed on the stack, and we have inserted reserved entries
+    // into the VEP, but we never insert reserved entries into the VVEP(RO).
+    return CodeOffsets::Verified_Value_Entry_RO;
+  } else {
+    // Share same entry for VEP and VVEP(RO).
+    return CodeOffsets::Verified_Entry;
+  }
+}
+
+
+void CompiledEntrySignature::compute_calling_conventions() {
+  // Get the (non-scalarized) signature and check for value type arguments
+  if (!_method->is_static()) {
+    if (_method->method_holder()->is_value() && ValueKlass::cast(_method->method_holder())->is_scalarizable()) {
+      _has_value_recv = true;
+      _num_value_args++;
+    }
+    SigEntry::add_entry(_sig, T_OBJECT);
+  }
+  for (SignatureStream ss(_method->signature()); !ss.at_return_type(); ss.next()) {
+    BasicType bt = ss.type();
+    if (bt == T_VALUETYPE) {
+      if (ss.as_value_klass(_method->method_holder())->is_scalarizable()) {
+        _num_value_args++;
+      }
+      bt = T_OBJECT;
+    }
+    SigEntry::add_entry(_sig, bt);
+  }
+  if (_method->is_abstract() && !(InlineTypePassFieldsAsArgs && has_value_arg())) {
+    return;
+  }
+
+  // Get a description of the compiled java calling convention and the largest used (VMReg) stack slot usage
+  _regs = NEW_RESOURCE_ARRAY(VMRegPair, _sig->length());
+  _args_on_stack = SharedRuntime::java_calling_convention(_sig, _regs);
+
+  // Now compute the scalarized calling convention if there are value types in the signature
+  _sig_cc = _sig;
+  _sig_cc_ro = _sig;
+  _regs_cc = _regs;
+  _regs_cc_ro = _regs;
+  _args_on_stack_cc = _args_on_stack;
+  _args_on_stack_cc_ro = _args_on_stack;
+
+  if (InlineTypePassFieldsAsArgs && has_value_arg() && !_method->is_native()) {
+    _args_on_stack_cc = compute_scalarized_cc(_sig_cc, _regs_cc, /* scalar_receiver = */ true);
+
+    _sig_cc_ro = _sig_cc;
+    _regs_cc_ro = _regs_cc;
+    _args_on_stack_cc_ro = _args_on_stack_cc;
+    if (_has_value_recv || _args_on_stack_cc > _args_on_stack) {
+      // For interface calls, we need another entry point / adapter to unpack the receiver
+      _args_on_stack_cc_ro = compute_scalarized_cc(_sig_cc_ro, _regs_cc_ro, /* scalar_receiver = */ false);
+    }
+
+    // Compute the stack extension that is required to convert between the calling conventions.
+    // The stack slots at these offsets are occupied by the return address with the unscalarized
+    // calling convention. Don't use them for arguments with the scalarized calling convention.
+    int ret_off    = _args_on_stack_cc - _args_on_stack;
+    int ret_off_ro = _args_on_stack_cc - _args_on_stack_cc_ro;
+    assert(ret_off_ro <= 0 || ret_off > 0, "receiver unpacking requires more stack space than expected");
+
+    if (ret_off > 0) {
+      // Make sure the stack of the scalarized calling convention with the reserved
+      // entries (2 slots each) remains 16-byte (4 slots) aligned after stack extension.
+      int alignment = StackAlignmentInBytes / VMRegImpl::stack_slot_size;
+      if (ret_off_ro != ret_off && ret_off_ro >= 0) {
+        ret_off    += 4; // Account for two reserved entries (4 slots)
+        ret_off_ro += 4;
+        ret_off     = align_up(ret_off, alignment);
+        ret_off_ro  = align_up(ret_off_ro, alignment);
+        // TODO can we avoid wasting a stack slot here?
+        //assert(ret_off != ret_off_ro, "fail");
+        if (ret_off > ret_off_ro) {
+          swap(ret_off, ret_off_ro); // Sort by offset
+        }
+        _args_on_stack_cc = insert_reserved_entry(ret_off);
+        _args_on_stack_cc = insert_reserved_entry(ret_off_ro);
+      } else {
+        ret_off += 2; // Account for one reserved entry (2 slots)
+        ret_off = align_up(ret_off, alignment);
+        _args_on_stack_cc = insert_reserved_entry(ret_off);
+      }
+
+      _has_reserved_entries = true;
+    }
+
+    // Upper bound on stack arguments to avoid hitting the argument limit and
+    // bailing out of compilation ("unsupported incoming calling sequence").
+    // TODO we need a reasonable limit (flag?) here
+    if (_args_on_stack_cc > 50) {
+      // Don't scalarize value type arguments
+      _sig_cc = _sig;
+      _sig_cc_ro = _sig;
+      _regs_cc = _regs;
+      _regs_cc_ro = _regs;
+      _args_on_stack_cc = _args_on_stack;
+      _args_on_stack_cc_ro = _args_on_stack;
+    } else {
+      _c1_needs_stack_repair = (_args_on_stack_cc < _args_on_stack) || (_args_on_stack_cc_ro < _args_on_stack);
+      _c2_needs_stack_repair = (_args_on_stack_cc > _args_on_stack) || (_args_on_stack_cc > _args_on_stack_cc_ro);
+      _has_scalarized_args = true;
+    }
+  }
 }
 
 AdapterHandlerEntry* AdapterHandlerLibrary::get_adapter0(const methodHandle& method) {
   // Use customized signature handler.  Need to lock around updates to
   // the AdapterHandlerTable (it is not safe for concurrent readers
   // and a single writer: this could be fixed if it becomes a
   // problem).
 
   ResourceMark rm;
 
-  NOT_PRODUCT(int insts_size);
+  NOT_PRODUCT(int insts_size = 0);
   AdapterBlob* new_adapter = NULL;
   AdapterHandlerEntry* entry = NULL;
   AdapterFingerPrint* fingerprint = NULL;
+
   {
     MutexLocker mu(AdapterHandlerLibrary_lock);
     // make sure data structure is initialized
     initialize();
 
-    if (method->is_abstract()) {
-      return _abstract_method_handler;
+    CompiledEntrySignature ces(method());
+    {
+       MutexUnlocker mul(AdapterHandlerLibrary_lock);
+       ces.compute_calling_conventions();
     }
+    GrowableArray<SigEntry>& sig       = ces.sig();
+    GrowableArray<SigEntry>& sig_cc    = ces.sig_cc();
+    GrowableArray<SigEntry>& sig_cc_ro = ces.sig_cc_ro();
+    VMRegPair* regs         = ces.regs();
+    VMRegPair* regs_cc      = ces.regs_cc();
+    VMRegPair* regs_cc_ro   = ces.regs_cc_ro();
 
-    // Fill in the signature array, for the calling-convention call.
-    int total_args_passed = method->size_of_parameters(); // All args on stack
+    if (ces.has_scalarized_args()) {
+      method->set_has_scalarized_args(true);
+      method->set_c1_needs_stack_repair(ces.c1_needs_stack_repair());
+      method->set_c2_needs_stack_repair(ces.c2_needs_stack_repair());
+    }
 
-    BasicType* sig_bt = NEW_RESOURCE_ARRAY(BasicType, total_args_passed);
-    VMRegPair* regs   = NEW_RESOURCE_ARRAY(VMRegPair, total_args_passed);
-    int i = 0;
-    if (!method->is_static())  // Pass in receiver first
-      sig_bt[i++] = T_OBJECT;
-    for (SignatureStream ss(method->signature()); !ss.at_return_type(); ss.next()) {
-      sig_bt[i++] = ss.type();  // Collect remaining bits of signature
-      if (ss.type() == T_LONG || ss.type() == T_DOUBLE)
-        sig_bt[i++] = T_VOID;   // Longs & doubles take 2 Java slots
+    if (method->is_abstract()) {
+      if (ces.has_scalarized_args()) {
+        // Save a C heap allocated version of the signature for abstract methods with scalarized value type arguments
+        address wrong_method_abstract = SharedRuntime::get_handle_wrong_method_abstract_stub();
+        entry = AdapterHandlerLibrary::new_entry(new AdapterFingerPrint(NULL),
+                                                 StubRoutines::throw_AbstractMethodError_entry(),
+                                                 wrong_method_abstract, wrong_method_abstract, wrong_method_abstract,
+                                                 wrong_method_abstract, wrong_method_abstract);
+        GrowableArray<SigEntry>* heap_sig = new (ResourceObj::C_HEAP, mtInternal) GrowableArray<SigEntry>(sig_cc_ro.length(), true);
+        heap_sig->appendAll(&sig_cc_ro);
+        entry->set_sig_cc(heap_sig);
+        return entry;
+      } else {
+        return _abstract_method_handler;
+      }
     }
-    assert(i == total_args_passed, "");
 
     // Lookup method signature's fingerprint
-    entry = _adapters->lookup(total_args_passed, sig_bt);
+    entry = _adapters->lookup(&sig_cc, regs_cc != regs_cc_ro);
 
 #ifdef ASSERT
     AdapterHandlerEntry* shared_entry = NULL;
     // Start adapter sharing verification only after the VM is booted.
     if (VerifyAdapterSharing && (entry != NULL)) {
@@ -2665,14 +2993,11 @@
 
     if (entry != NULL) {
       return entry;
     }
 
-    // Get a description of the compiled java calling convention and the largest used (VMReg) stack slot usage
-    int comp_args_on_stack = SharedRuntime::java_calling_convention(sig_bt, regs, total_args_passed, false);
-
-    // Make a C heap allocated version of the fingerprint to store in the adapter
+    // Make a C heap allocated version of the fingerprint to store in the adapter
     fingerprint = new AdapterFingerPrint(total_args_passed, sig_bt);
 
     // StubRoutines::code2() is initialized after this function can be called. As a result,
     // VerifyAdapterCalls and VerifyAdapterSharing can fail if we re-use code that generated
     // prior to StubRoutines::code2() being set. Checks refer to checks generated in an I2C
@@ -2687,29 +3012,43 @@
       buffer.insts()->initialize_shared_locs((relocInfo*)buffer_locs,
                                              sizeof(buffer_locs)/sizeof(relocInfo));
 
       MacroAssembler _masm(&buffer);
       entry = SharedRuntime::generate_i2c2i_adapters(&_masm,
-                                                     total_args_passed,
-                                                     comp_args_on_stack,
-                                                     sig_bt,
+                                                     ces.args_on_stack(),
+                                                     &sig,
                                                      regs,
-                                                     fingerprint);
+                                                     &sig_cc,
+                                                     regs_cc,
+                                                     &sig_cc_ro,
+                                                     regs_cc_ro,
+                                                     fingerprint,
+                                                     new_adapter);
+
+      if (ces.has_scalarized_args()) {
+        // Save a C heap allocated version of the scalarized signature and store it in the adapter
+        GrowableArray<SigEntry>* heap_sig = new (ResourceObj::C_HEAP, mtInternal) GrowableArray<SigEntry>(sig_cc.length(), true);
+        heap_sig->appendAll(&sig_cc);
+        entry->set_sig_cc(heap_sig);
+      }
+
 #ifdef ASSERT
       if (VerifyAdapterSharing) {
         if (shared_entry != NULL) {
+          if (!shared_entry->compare_code(buf->code_begin(), buffer.insts_size())) {
+            method->print();
+          }
           assert(shared_entry->compare_code(buf->code_begin(), buffer.insts_size()), "code must match");
           // Release the one just created and return the original
           _adapters->free_entry(entry);
           return shared_entry;
         } else  {
           entry->save_code(buf->code_begin(), buffer.insts_size());
         }
       }
 #endif
 
-      new_adapter = AdapterBlob::create(&buffer);
       NOT_PRODUCT(insts_size = buffer.insts_size());
     }
     if (new_adapter == NULL) {
       // CodeCache is full, disable compilation
       // Ought to log this but compile log is only per compile thread
@@ -2761,11 +3100,14 @@
 
 address AdapterHandlerEntry::base_address() {
   address base = _i2c_entry;
   if (base == NULL)  base = _c2i_entry;
   assert(base <= _c2i_entry || _c2i_entry == NULL, "");
+  assert(base <= _c2i_value_entry || _c2i_value_entry == NULL, "");
+  assert(base <= _c2i_value_ro_entry || _c2i_value_ro_entry == NULL, "");
   assert(base <= _c2i_unverified_entry || _c2i_unverified_entry == NULL, "");
+  assert(base <= _c2i_unverified_value_entry || _c2i_unverified_value_entry == NULL, "");
   assert(base <= _c2i_no_clinit_check_entry || _c2i_no_clinit_check_entry == NULL, "");
   return base;
 }
 
 void AdapterHandlerEntry::relocate(address new_base) {
@@ -2774,20 +3116,29 @@
   ptrdiff_t delta = new_base - old_base;
   if (_i2c_entry != NULL)
     _i2c_entry += delta;
   if (_c2i_entry != NULL)
     _c2i_entry += delta;
+  if (_c2i_value_entry != NULL)
+    _c2i_value_entry += delta;
+  if (_c2i_value_ro_entry != NULL)
+    _c2i_value_ro_entry += delta;
   if (_c2i_unverified_entry != NULL)
     _c2i_unverified_entry += delta;
+  if (_c2i_unverified_value_entry != NULL)
+    _c2i_unverified_value_entry += delta;
   if (_c2i_no_clinit_check_entry != NULL)
     _c2i_no_clinit_check_entry += delta;
   assert(base_address() == new_base, "");
 }
 
 
 void AdapterHandlerEntry::deallocate() {
   delete _fingerprint;
+  if (_sig_cc != NULL) {
+    delete _sig_cc;
+  }
 #ifdef ASSERT
   FREE_C_HEAP_ARRAY(unsigned char, _saved_code);
 #endif
 }
 
@@ -2867,11 +3218,12 @@
       int i=0;
       if (!method->is_static())  // Pass in receiver first
         sig_bt[i++] = T_OBJECT;
       SignatureStream ss(method->signature());
       for (; !ss.at_return_type(); ss.next()) {
-        sig_bt[i++] = ss.type();  // Collect remaining bits of signature
+        BasicType bt = ss.type();
+        sig_bt[i++] = bt;  // Collect remaining bits of signature
         if (ss.type() == T_LONG || ss.type() == T_DOUBLE)
           sig_bt[i++] = T_VOID;   // Longs & doubles take 2 Java slots
       }
       assert(i == total_args_passed, "");
       BasicType ret_type = ss.type();
@@ -3115,12 +3467,21 @@
     st->print(" i2c: " INTPTR_FORMAT, p2i(get_i2c_entry()));
   }
   if (get_c2i_entry() != NULL) {
     st->print(" c2i: " INTPTR_FORMAT, p2i(get_c2i_entry()));
   }
+  if (get_c2i_entry() != NULL) {
+    st->print(" c2iVE: " INTPTR_FORMAT, p2i(get_c2i_value_entry()));
+  }
+  if (get_c2i_entry() != NULL) {
+    st->print(" c2iVROE: " INTPTR_FORMAT, p2i(get_c2i_value_ro_entry()));
+  }
   if (get_c2i_unverified_entry() != NULL) {
-    st->print(" c2iUV: " INTPTR_FORMAT, p2i(get_c2i_unverified_entry()));
+    st->print(" c2iUE: " INTPTR_FORMAT, p2i(get_c2i_unverified_entry()));
+  }
+  if (get_c2i_unverified_entry() != NULL) {
+    st->print(" c2iUVE: " INTPTR_FORMAT, p2i(get_c2i_unverified_value_entry()));
   }
   if (get_c2i_no_clinit_check_entry() != NULL) {
     st->print(" c2iNCI: " INTPTR_FORMAT, p2i(get_c2i_no_clinit_check_entry()));
   }
   st->cr();
@@ -3129,10 +3490,12 @@
 #if INCLUDE_CDS
 
 void CDSAdapterHandlerEntry::init() {
   assert(DumpSharedSpaces, "used during dump time only");
   _c2i_entry_trampoline = (address)MetaspaceShared::misc_code_space_alloc(SharedRuntime::trampoline_size());
+  _c2i_value_ro_entry_trampoline = (address)MetaspaceShared::misc_code_space_alloc(SharedRuntime::trampoline_size());
+  _c2i_value_entry_trampoline = (address)MetaspaceShared::misc_code_space_alloc(SharedRuntime::trampoline_size());
   _adapter_trampoline = (AdapterHandlerEntry**)MetaspaceShared::misc_code_space_alloc(sizeof(AdapterHandlerEntry*));
 };
 
 #endif // INCLUDE_CDS
 
@@ -3214,5 +3577,211 @@
   if (new_obj == NULL) return;
 
   BarrierSet *bs = BarrierSet::barrier_set();
   bs->on_slowpath_allocation_exit(thread, new_obj);
 }
+
+// We are at a compiled code to interpreter call. We need backing
+// buffers for all value type arguments. Allocate an object array to
+// hold them (convenient because once we're done with it we don't have
+// to worry about freeing it).
+oop SharedRuntime::allocate_value_types_impl(JavaThread* thread, methodHandle callee, bool allocate_receiver, TRAPS) {
+  assert(InlineTypePassFieldsAsArgs, "no reason to call this");
+  ResourceMark rm;
+
+  int nb_slots = 0;
+  InstanceKlass* holder = callee->method_holder();
+  allocate_receiver &= !callee->is_static() && holder->is_value();
+  if (allocate_receiver) {
+    nb_slots++;
+  }
+  for (SignatureStream ss(callee->signature()); !ss.at_return_type(); ss.next()) {
+    if (ss.type() == T_VALUETYPE) {
+      nb_slots++;
+    }
+  }
+  objArrayOop array_oop = oopFactory::new_objectArray(nb_slots, CHECK_NULL);
+  objArrayHandle array(THREAD, array_oop);
+  int i = 0;
+  if (allocate_receiver) {
+    ValueKlass* vk = ValueKlass::cast(holder);
+    oop res = vk->allocate_instance(CHECK_NULL);
+    array->obj_at_put(i, res);
+    i++;
+  }
+  for (SignatureStream ss(callee->signature()); !ss.at_return_type(); ss.next()) {
+    if (ss.type() == T_VALUETYPE) {
+      ValueKlass* vk = ss.as_value_klass(holder);
+      oop res = vk->allocate_instance(CHECK_NULL);
+      array->obj_at_put(i, res);
+      i++;
+    }
+  }
+  return array();
+}
+
+JRT_ENTRY(void, SharedRuntime::allocate_value_types(JavaThread* thread, Method* callee_method, bool allocate_receiver))
+  methodHandle callee(thread, callee_method);
+  oop array = SharedRuntime::allocate_value_types_impl(thread, callee, allocate_receiver, CHECK);
+  thread->set_vm_result(array);
+  thread->set_vm_result_2(callee()); // TODO: required to keep callee live?
+JRT_END
+
+// TODO remove this once the AARCH64 dependency is gone
+// Iterate over the array of heap allocated value types and apply the GC post barrier to all reference fields.
+// This is called from the C2I adapter after value type arguments are heap allocated and initialized.
+JRT_LEAF(void, SharedRuntime::apply_post_barriers(JavaThread* thread, objArrayOopDesc* array))
+{
+  assert(InlineTypePassFieldsAsArgs, "no reason to call this");
+  assert(oopDesc::is_oop(array), "should be oop");
+  for (int i = 0; i < array->length(); ++i) {
+    instanceOop valueOop = (instanceOop)array->obj_at(i);
+    ValueKlass* vk = ValueKlass::cast(valueOop->klass());
+    if (vk->contains_oops()) {
+      const address dst_oop_addr = ((address) (void*) valueOop);
+      OopMapBlock* map = vk->start_of_nonstatic_oop_maps();
+      OopMapBlock* const end = map + vk->nonstatic_oop_map_count();
+      while (map != end) {
+        address doop_address = dst_oop_addr + map->offset();
+        barrier_set_cast<ModRefBarrierSet>(BarrierSet::barrier_set())->
+          write_ref_array((HeapWord*) doop_address, map->count());
+        map++;
+      }
+    }
+  }
+}
+JRT_END
+
+// We're returning from an interpreted method: load each field into a
+// register following the calling convention
+JRT_LEAF(void, SharedRuntime::load_value_type_fields_in_regs(JavaThread* thread, oopDesc* res))
+{
+  assert(res->klass()->is_value(), "only value types here");
+  ResourceMark rm;
+  RegisterMap reg_map(thread);
+  frame stubFrame = thread->last_frame();
+  frame callerFrame = stubFrame.sender(&reg_map);
+  assert(callerFrame.is_interpreted_frame(), "should be coming from interpreter");
+
+  ValueKlass* vk = ValueKlass::cast(res->klass());
+
+  const Array<SigEntry>* sig_vk = vk->extended_sig();
+  const Array<VMRegPair>* regs = vk->return_regs();
+
+  if (regs == NULL) {
+    // The fields of the value klass don't fit in registers, bail out
+    return;
+  }
+
+  int j = 1;
+  for (int i = 0; i < sig_vk->length(); i++) {
+    BasicType bt = sig_vk->at(i)._bt;
+    if (bt == T_VALUETYPE) {
+      continue;
+    }
+    if (bt == T_VOID) {
+      if (sig_vk->at(i-1)._bt == T_LONG ||
+          sig_vk->at(i-1)._bt == T_DOUBLE) {
+        j++;
+      }
+      continue;
+    }
+    int off = sig_vk->at(i)._offset;
+    assert(off > 0, "offset in object should be positive");
+    VMRegPair pair = regs->at(j);
+    address loc = reg_map.location(pair.first());
+    switch(bt) {
+    case T_BOOLEAN:
+      *(jboolean*)loc = res->bool_field(off);
+      break;
+    case T_CHAR:
+      *(jchar*)loc = res->char_field(off);
+      break;
+    case T_BYTE:
+      *(jbyte*)loc = res->byte_field(off);
+      break;
+    case T_SHORT:
+      *(jshort*)loc = res->short_field(off);
+      break;
+    case T_INT: {
+      *(jint*)loc = res->int_field(off);
+      break;
+    }
+    case T_LONG:
+#ifdef _LP64
+      *(intptr_t*)loc = res->long_field(off);
+#else
+      Unimplemented();
+#endif
+      break;
+    case T_OBJECT:
+    case T_ARRAY: {
+      *(oop*)loc = res->obj_field(off);
+      break;
+    }
+    case T_FLOAT:
+      *(jfloat*)loc = res->float_field(off);
+      break;
+    case T_DOUBLE:
+      *(jdouble*)loc = res->double_field(off);
+      break;
+    default:
+      ShouldNotReachHere();
+    }
+    j++;
+  }
+  assert(j == regs->length(), "missed a field?");
+
+#ifdef ASSERT
+  VMRegPair pair = regs->at(0);
+  address loc = reg_map.location(pair.first());
+  assert(*(oopDesc**)loc == res, "overwritten object");
+#endif
+
+  thread->set_vm_result(res);
+}
+JRT_END
+
+// We've returned to an interpreted method, the interpreter needs a
+// reference to a value type instance. Allocate it and initialize it
+// from field's values in registers.
+JRT_BLOCK_ENTRY(void, SharedRuntime::store_value_type_fields_to_buf(JavaThread* thread, intptr_t res))
+{
+  ResourceMark rm;
+  RegisterMap reg_map(thread);
+  frame stubFrame = thread->last_frame();
+  frame callerFrame = stubFrame.sender(&reg_map);
+
+#ifdef ASSERT
+  ValueKlass* verif_vk = ValueKlass::returned_value_klass(reg_map);
+#endif
+
+  if (!is_set_nth_bit(res, 0)) {
+    // We're not returning with value type fields in registers (the
+    // calling convention didn't allow it for this value klass)
+    assert(!Metaspace::contains((void*)res), "should be oop or pointer in buffer area");
+    thread->set_vm_result((oopDesc*)res);
+    assert(verif_vk == NULL, "broken calling convention");
+    return;
+  }
+
+  clear_nth_bit(res, 0);
+  ValueKlass* vk = (ValueKlass*)res;
+  assert(verif_vk == vk, "broken calling convention");
+  assert(Metaspace::contains((void*)res), "should be klass");
+
+  // Allocate handles for every oop field so they are safe in case of
+  // a safepoint when allocating
+  GrowableArray<Handle> handles;
+  vk->save_oop_fields(reg_map, handles);
+
+  // It's unsafe to safepoint until we are here
+  JRT_BLOCK;
+  {
+    Thread* THREAD = thread;
+    oop vt = vk->realloc_result(reg_map, handles, CHECK);
+    thread->set_vm_result(vt);
+  }
+  JRT_BLOCK_END;
+}
+JRT_END
+
diff a/src/hotspot/share/runtime/vmStructs.cpp b/src/hotspot/share/runtime/vmStructs.cpp
--- a/src/hotspot/share/runtime/vmStructs.cpp
+++ b/src/hotspot/share/runtime/vmStructs.cpp
@@ -231,11 +231,11 @@
   nonstatic_field(InstanceKlass,               _nonstatic_field_size,                         int)                                   \
   nonstatic_field(InstanceKlass,               _static_field_size,                            int)                                   \
   nonstatic_field(InstanceKlass,               _static_oop_field_count,                       u2)                                    \
   nonstatic_field(InstanceKlass,               _nonstatic_oop_map_size,                       int)                                   \
   nonstatic_field(InstanceKlass,               _is_marked_dependent,                          bool)                                  \
-  nonstatic_field(InstanceKlass,               _misc_flags,                                   u2)                                    \
+  nonstatic_field(InstanceKlass,               _misc_flags,                                   u4)                                    \
   nonstatic_field(InstanceKlass,               _init_state,                                   u1)                                    \
   nonstatic_field(InstanceKlass,               _init_thread,                                  Thread*)                               \
   nonstatic_field(InstanceKlass,               _itable_len,                                   int)                                   \
   nonstatic_field(InstanceKlass,               _reference_type,                               u1)                                    \
   volatile_nonstatic_field(InstanceKlass,      _oop_map_cache,                                OopMapCache*)                          \
@@ -1593,13 +1593,15 @@
   declare_c2_type(ConvF2INode, Node)                                      \
   declare_c2_type(ConvF2LNode, Node)                                      \
   declare_c2_type(ConvI2DNode, Node)                                      \
   declare_c2_type(ConvI2FNode, Node)                                      \
   declare_c2_type(ConvI2LNode, TypeNode)                                  \
+  declare_c2_type(CastI2NNode, TypeNode)                                  \
   declare_c2_type(ConvL2DNode, Node)                                      \
   declare_c2_type(ConvL2FNode, Node)                                      \
   declare_c2_type(ConvL2INode, Node)                                      \
+  declare_c2_type(CastN2INode, Node)                                      \
   declare_c2_type(CastX2PNode, Node)                                      \
   declare_c2_type(CastP2XNode, Node)                                      \
   declare_c2_type(SetVectMaskINode, Node)                                 \
   declare_c2_type(MemBarNode, MultiNode)                                  \
   declare_c2_type(MemBarAcquireNode, MemBarNode)                          \
@@ -1636,10 +1638,11 @@
   declare_c2_type(MachNode, Node)                                         \
   declare_c2_type(MachIdealNode, MachNode)                                \
   declare_c2_type(MachTypeNode, MachNode)                                 \
   declare_c2_type(MachBreakpointNode, MachIdealNode)                      \
   declare_c2_type(MachUEPNode, MachIdealNode)                             \
+  declare_c2_type(MachVEPNode, MachIdealNode)                             \
   declare_c2_type(MachPrologNode, MachIdealNode)                          \
   declare_c2_type(MachEpilogNode, MachIdealNode)                          \
   declare_c2_type(MachNopNode, MachIdealNode)                             \
   declare_c2_type(MachSpillCopyNode, MachIdealNode)                       \
   declare_c2_type(MachNullCheckNode, MachIdealNode)                       \
@@ -2295,10 +2298,12 @@
   declare_constant(InstanceKlass::_misc_has_passed_fingerprint_check)     \
   declare_constant(InstanceKlass::_misc_is_scratch_class)                 \
   declare_constant(InstanceKlass::_misc_is_shared_boot_class)             \
   declare_constant(InstanceKlass::_misc_is_shared_platform_class)         \
   declare_constant(InstanceKlass::_misc_is_shared_app_class)              \
+  declare_constant(InstanceKlass::_misc_invalid_inline_super)             \
+  declare_constant(InstanceKlass::_misc_invalid_identity_super)           \
                                                                           \
   /*********************************/                                     \
   /* Symbol* - symbol max length */                                       \
   /*********************************/                                     \
                                                                           \
diff a/src/java.base/share/classes/java/lang/invoke/InvokerBytecodeGenerator.java b/src/java.base/share/classes/java/lang/invoke/InvokerBytecodeGenerator.java
--- a/src/java.base/share/classes/java/lang/invoke/InvokerBytecodeGenerator.java
+++ b/src/java.base/share/classes/java/lang/invoke/InvokerBytecodeGenerator.java
@@ -1009,11 +1009,12 @@
         return isStaticallyInvocable(name.function.member());
     }
 
     static boolean isStaticallyInvocable(MemberName member) {
         if (member == null)  return false;
-        if (member.isConstructor())  return false;
+        if (member.isObjectConstructorOrStaticInitMethod())  return false;
+
         Class<?> cls = member.getDeclaringClass();
         // Fast-path non-private members declared by MethodHandles, which is a common
         // case
         if (MethodHandle.class.isAssignableFrom(cls) && !member.isPrivate()) {
             assert(isStaticallyInvocableType(member.getMethodOrFieldType()));
diff a/src/java.base/share/classes/java/lang/invoke/MethodHandles.java b/src/java.base/share/classes/java/lang/invoke/MethodHandles.java
--- a/src/java.base/share/classes/java/lang/invoke/MethodHandles.java
+++ b/src/java.base/share/classes/java/lang/invoke/MethodHandles.java
@@ -2329,10 +2329,16 @@
          *                              <a href="MethodHandles.Lookup.html#secmgr">refuses access</a>
          * @throws NullPointerException if any argument is null
          */
         public MethodHandle findStatic(Class<?> refc, String name, MethodType type) throws NoSuchMethodException, IllegalAccessException {
             MemberName method = resolveOrFail(REF_invokeStatic, refc, name, type);
+            // resolveOrFail could return a non-static <init> method if present
+            // detect and throw NSME before producing a MethodHandle
+            if (!method.isStatic() && name.equals("<init>")) {
+                throw new NoSuchMethodException("illegal method name: " + name);
+            }
+
             return getDirectMethod(REF_invokeStatic, refc, method, findBoundCallerLookup(method));
         }
 
         /**
          * Produces a method handle for a virtual method.
@@ -2474,10 +2480,17 @@
   ProcessBuilder.class, methodType(void.class, String[].class));
 ProcessBuilder pb = (ProcessBuilder)
   MH_newProcessBuilder.invoke("x", "y", "z");
 assertEquals("[x, y, z]", pb.command().toString());
          * }</pre></blockquote>
+         *
+         * @apiNote
+         * This method does not find a static {@code <init>} factory method as it is invoked
+         * via {@code invokestatic} bytecode as opposed to {@code invokespecial} for an
+         * object constructor.  To look up static {@code <init>} factory method, use
+         * the {@link #findStatic(Class, String, MethodType) findStatic} method.
+         *
          * @param refc the class or interface from which the method is accessed
          * @param type the type of the method, with the receiver argument omitted, and a void return type
          * @return the desired method handle
          * @throws NoSuchMethodException if the constructor does not exist
          * @throws IllegalAccessException if access checking fails
@@ -2489,10 +2502,13 @@
          */
         public MethodHandle findConstructor(Class<?> refc, MethodType type) throws NoSuchMethodException, IllegalAccessException {
             if (refc.isArray()) {
                 throw new NoSuchMethodException("no constructor for array class: " + refc.getName());
             }
+            if (type.returnType() != void.class) {
+                throw new NoSuchMethodException("Constructors must have void return type: " + refc.getName());
+            }
             String name = "<init>";
             MemberName ctor = resolveOrFail(REF_newInvokeSpecial, refc, name, type);
             return getDirectConstructor(refc, ctor);
         }
 
@@ -3106,14 +3122,22 @@
          *                                is set and {@code asVarargsCollector} fails
          * @throws NullPointerException if the argument is null
          */
         public MethodHandle unreflectConstructor(Constructor<?> c) throws IllegalAccessException {
             MemberName ctor = new MemberName(c);
-            assert(ctor.isConstructor());
+            assert(ctor.isObjectConstructorOrStaticInitMethod());
             @SuppressWarnings("deprecation")
             Lookup lookup = c.isAccessible() ? IMPL_LOOKUP : this;
-            return lookup.getDirectConstructorNoSecurityManager(ctor.getDeclaringClass(), ctor);
+            if (ctor.isObjectConstructor()) {
+                assert(ctor.getReturnType() == void.class);
+                return lookup.getDirectConstructorNoSecurityManager(ctor.getDeclaringClass(), ctor);
+            } else {
+                // static init factory is a static method
+                assert(ctor.isMethod() && ctor.getReturnType() == ctor.getDeclaringClass() && ctor.getReferenceKind() == REF_invokeStatic);
+                assert(!MethodHandleNatives.isCallerSensitive(ctor));  // must not be caller-sensitive
+                return lookup.getDirectMethodNoSecurityManager(ctor.getReferenceKind(), ctor.getDeclaringClass(), ctor, lookup);
+            }
         }
 
         /**
          * Produces a method handle giving read access to a reflected field.
          * The type of the method handle will have a return type of the field's
@@ -3363,15 +3387,17 @@
             return caller == null || VerifyAccess.isClassAccessible(refc, caller, prevLookupClass, allowedModes);
         }
 
         /** Check name for an illegal leading "&lt;" character. */
         void checkMethodName(byte refKind, String name) throws NoSuchMethodException {
-            if (name.startsWith("<") && refKind != REF_newInvokeSpecial)
-                throw new NoSuchMethodException("illegal method name: "+name);
+            // "<init>" can only be invoked via invokespecial or it's a static init factory
+            if (name.startsWith("<") && refKind != REF_newInvokeSpecial &&
+                    !(refKind == REF_invokeStatic && name.equals("<init>"))) {
+                    throw new NoSuchMethodException("illegal method name: " + name);
+            }
         }
 
-
         /**
          * Find my trustable caller class if m is a caller sensitive method.
          * If this lookup object has full privilege access, then the caller class is the lookupClass.
          * Otherwise, if m is caller-sensitive, throw IllegalAccessException.
          */
@@ -3452,11 +3478,11 @@
         }
 
         void checkMethod(byte refKind, Class<?> refc, MemberName m) throws IllegalAccessException {
             boolean wantStatic = (refKind == REF_invokeStatic);
             String message;
-            if (m.isConstructor())
+            if (m.isObjectConstructor())
                 message = "expected a method, not a constructor";
             else if (!m.isMethod())
                 message = "expected a method";
             else if (wantStatic != m.isStatic())
                 message = wantStatic ? "expected a static method" : "expected a non-static method";
@@ -3751,11 +3777,11 @@
             return getDirectConstructorCommon(refc, ctor, checkSecurity);
         }
         /** Common code for all constructors; do not call directly except from immediately above. */
         private MethodHandle getDirectConstructorCommon(Class<?> refc, MemberName ctor,
                                                   boolean checkSecurity) throws IllegalAccessException {
-            assert(ctor.isConstructor());
+            assert(ctor.isObjectConstructor());
             checkAccess(REF_newInvokeSpecial, refc, ctor);
             // Optionally check with the security manager; this isn't needed for unreflect* calls.
             if (checkSecurity)
                 checkSecurityManager(refc, ctor);
             assert(!MethodHandleNatives.isCallerSensitive(ctor));  // maybeBindCaller not relevant here
@@ -3941,10 +3967,13 @@
      * @throws NullPointerException if the argument is null
      * @throws IllegalArgumentException if arrayClass is not an array type
      * @jvms 6.5 {@code aastore} Instruction
      */
     public static MethodHandle arrayElementSetter(Class<?> arrayClass) throws IllegalArgumentException {
+        if (arrayClass.isInlineClass()) {
+            throw new UnsupportedOperationException();
+        }
         return MethodHandleImpl.makeArrayElementAccessor(arrayClass, MethodHandleImpl.ArrayAccess.SET);
     }
 
     /**
      * Produces a VarHandle giving access to elements of an array of type
@@ -4701,11 +4730,17 @@
      * @see MethodHandles#explicitCastArguments
      * @since 9
      */
     public static MethodHandle zero(Class<?> type) {
         Objects.requireNonNull(type);
-        return type.isPrimitive() ?  zero(Wrapper.forPrimitiveType(type), type) : zero(Wrapper.OBJECT, type);
+        if (type.isPrimitive()) {
+            return zero(Wrapper.forPrimitiveType(type), type);
+        } else if (type.isInlineClass()) {
+            throw new UnsupportedOperationException();
+        } else {
+            return zero(Wrapper.OBJECT, type);
+        }
     }
 
     private static MethodHandle identityOrVoid(Class<?> type) {
         return type == void.class ? zero(type) : identity(type);
     }
@@ -4731,11 +4766,11 @@
         return dropArguments(zero(type.returnType()), 0, type.parameterList());
     }
 
     private static final MethodHandle[] IDENTITY_MHS = new MethodHandle[Wrapper.COUNT];
     private static MethodHandle makeIdentity(Class<?> ptype) {
-        MethodType mtype = methodType(ptype, ptype);
+        MethodType mtype = MethodType.methodType(ptype, ptype);
         LambdaForm lform = LambdaForm.identityForm(BasicType.basicType(ptype));
         return MethodHandleImpl.makeIntrinsic(mtype, lform, Intrinsic.IDENTITY);
     }
 
     private static MethodHandle zero(Wrapper btw, Class<?> rtype) {
diff a/src/jdk.internal.vm.ci/share/classes/jdk.vm.ci.hotspot/src/jdk/vm/ci/hotspot/HotSpotResolvedObjectTypeImpl.java b/src/jdk.internal.vm.ci/share/classes/jdk.vm.ci.hotspot/src/jdk/vm/ci/hotspot/HotSpotResolvedObjectTypeImpl.java
--- a/src/jdk.internal.vm.ci/share/classes/jdk.vm.ci.hotspot/src/jdk/vm/ci/hotspot/HotSpotResolvedObjectTypeImpl.java
+++ b/src/jdk.internal.vm.ci/share/classes/jdk.vm.ci.hotspot/src/jdk/vm/ci/hotspot/HotSpotResolvedObjectTypeImpl.java
@@ -275,13 +275,14 @@
 
     @Override
     public HotSpotResolvedObjectTypeImpl[] getInterfaces() {
         if (interfaces == null) {
             if (isArray()) {
-                HotSpotResolvedObjectTypeImpl[] types = new HotSpotResolvedObjectTypeImpl[2];
+                HotSpotResolvedObjectTypeImpl[] types = new HotSpotResolvedObjectTypeImpl[3];
                 types[0] = runtime().getJavaLangCloneable();
                 types[1] = runtime().getJavaLangSerializable();
+                types[2] = runtime().getJavaLangIdentityObject();
                 this.interfaces = types;
             } else {
                 interfaces = runtime().compilerToVm.getInterfaces(this);
             }
         }
diff a/src/jdk.internal.vm.ci/share/classes/jdk.vm.ci.hotspot/src/jdk/vm/ci/hotspot/HotSpotVMConfig.java b/src/jdk.internal.vm.ci/share/classes/jdk.vm.ci.hotspot/src/jdk/vm/ci/hotspot/HotSpotVMConfig.java
--- a/src/jdk.internal.vm.ci/share/classes/jdk.vm.ci.hotspot/src/jdk/vm/ci/hotspot/HotSpotVMConfig.java
+++ b/src/jdk.internal.vm.ci/share/classes/jdk.vm.ci.hotspot/src/jdk/vm/ci/hotspot/HotSpotVMConfig.java
@@ -308,10 +308,11 @@
     final int dataLayoutArgInfoDataTag = getConstant("DataLayout::arg_info_data_tag", Integer.class);
     final int dataLayoutCallTypeDataTag = getConstant("DataLayout::call_type_data_tag", Integer.class);
     final int dataLayoutVirtualCallTypeDataTag = getConstant("DataLayout::virtual_call_type_data_tag", Integer.class);
     final int dataLayoutParametersTypeDataTag = getConstant("DataLayout::parameters_type_data_tag", Integer.class);
     final int dataLayoutSpeculativeTrapDataTag = getConstant("DataLayout::speculative_trap_data_tag", Integer.class);
+    final int dataLayoutArrayLoadStoreDataTag = getConstant("DataLayout::array_load_store_data_tag", Integer.class);
 
     final int bciProfileWidth = getFlag("BciProfileWidth", Integer.class);
     final int typeProfileWidth = getFlag("TypeProfileWidth", Integer.class);
     final int methodProfileWidth = getFlag("MethodProfileWidth", Integer.class, 0);
 
diff a/test/hotspot/jtreg/ProblemList.txt b/test/hotspot/jtreg/ProblemList.txt
--- a/test/hotspot/jtreg/ProblemList.txt
+++ b/test/hotspot/jtreg/ProblemList.txt
@@ -69,10 +69,87 @@
 compiler/rtm/locking/TestRTMSpinLoopCount.java 8183263 generic-x64
 compiler/rtm/locking/TestUseRTMDeopt.java 8183263 generic-x64
 compiler/rtm/locking/TestUseRTMXendForLockBusy.java 8183263 generic-x64
 compiler/rtm/print/TestPrintPreciseRTMLockingStatistics.java 8183263 generic-x64
 
+# Valhalla
+compiler/arguments/CheckCICompilerCount.java                        8205030 generic-all
+compiler/arguments/CheckCompileThresholdScaling.java                8205030 generic-all
+compiler/codecache/CheckSegmentedCodeCache.java                     8205030 generic-all
+compiler/codecache/cli/TestSegmentedCodeCacheOption.java            8205030 generic-all
+compiler/codecache/cli/codeheapsize/TestCodeHeapSizeOptions.java    8205030 generic-all
+compiler/codecache/cli/printcodecache/TestPrintCodeCacheOption.java 8205030 generic-all
+compiler/whitebox/OSRFailureLevel4Test.java                         8205030 generic-all
+
+compiler/aot/cli/DisabledAOTWithLibraryTest.java 8226295 generic-all
+compiler/aot/cli/SingleAOTOptionTest.java 8226295 generic-all
+compiler/aot/cli/MultipleAOTLibraryTest.java 8226295 generic-all
+compiler/aot/cli/jaotc/CompileClassWithDebugTest.java 8226295 generic-all
+compiler/aot/cli/jaotc/CompileModuleTest.java 8226295 generic-all
+compiler/aot/cli/jaotc/AtFileTest.java 8226295 generic-all
+compiler/aot/cli/jaotc/ListOptionWrongFileTest.java 8226295 generic-all
+compiler/aot/cli/jaotc/ClasspathOptionUnknownClassTest.java 8226295 generic-all
+compiler/aot/cli/jaotc/CompileDirectoryTest.java 8226295 generic-all
+compiler/aot/cli/jaotc/ListOptionTest.java 8226295 generic-all
+compiler/aot/cli/jaotc/ListOptionNotExistingTest.java 8226295 generic-all
+compiler/aot/cli/jaotc/CompileClassTest.java 8226295 generic-all
+compiler/aot/cli/jaotc/CompileJarTest.java 8226295 generic-all
+compiler/aot/cli/jaotc/IgnoreErrorsTest.java 8226295 generic-all
+compiler/aot/cli/jaotc/CompileAbsoluteDirectoryTest.java 8226295 generic-all
+compiler/aot/cli/NonExistingAOTLibraryTest.java 8226295 generic-all
+compiler/aot/cli/SingleAOTLibraryTest.java 8226295 generic-all
+compiler/aot/cli/IncorrectAOTLibraryTest.java 8226295 generic-all
+compiler/aot/RecompilationTest.java 8226295 generic-all
+compiler/aot/SharedUsageTest.java 8226295 generic-all
+compiler/aot/jdk.tools.jaotc.test/src/jdk/tools/jaotc/test/collect/ClassSearchTest.java 8226295 generic-all
+compiler/aot/jdk.tools.jaotc.test/src/jdk/tools/jaotc/test/collect/SearchPathTest.java 8226295 generic-all
+compiler/aot/jdk.tools.jaotc.test/src/jdk/tools/jaotc/test/collect/module/ModuleSourceProviderTest.java 8226295 generic-all
+compiler/aot/jdk.tools.jaotc.test/src/jdk/tools/jaotc/test/collect/ClassSourceTest.java 8226295 generic-all
+compiler/aot/jdk.tools.jaotc.test/src/jdk/tools/jaotc/test/collect/directory/DirectorySourceProviderTest.java 8226295 generic-all
+compiler/aot/jdk.tools.jaotc.test/src/jdk/tools/jaotc/test/collect/jar/JarSourceProviderTest.java 8226295 generic-all
+compiler/aot/jdk.tools.jaotc.test/src/jdk/tools/jaotc/test/NativeOrderOutputStreamTest.java 8226295 generic-all
+compiler/aot/verification/vmflags/TrackedFlagTest.java 8226295 generic-all
+compiler/aot/verification/vmflags/NotTrackedFlagTest.java 8226295 generic-all
+compiler/aot/verification/ClassAndLibraryNotMatchTest.java 8226295 generic-all
+compiler/aot/DeoptimizationTest.java 8226295 generic-all
+compiler/aot/calls/fromAot/AotInvokeVirtual2NativeTest.java 8226295 generic-all
+compiler/aot/calls/fromAot/AotInvokeStatic2CompiledTest.java 8226295 generic-all
+compiler/aot/calls/fromAot/AotInvokeSpecial2CompiledTest.java 8226295 generic-all
+compiler/aot/calls/fromAot/AotInvokeVirtual2CompiledTest.java 8226295 generic-all
+compiler/aot/calls/fromAot/AotInvokeVirtual2InterpretedTest.java 8226295 generic-all
+compiler/aot/calls/fromAot/AotInvokeDynamic2NativeTest.java 8226295 generic-all
+compiler/aot/calls/fromAot/AotInvokeDynamic2InterpretedTest.java 8226295 generic-all
+compiler/aot/calls/fromAot/AotInvokeStatic2InterpretedTest.java 8226295 generic-all
+compiler/aot/calls/fromAot/AotInvokeInterface2InterpretedTest.java 8226295 generic-all
+compiler/aot/calls/fromAot/AotInvokeVirtual2AotTest.java 8226295 generic-all
+compiler/aot/calls/fromAot/AotInvokeInterface2AotTest.java 8226295 generic-all
+compiler/aot/calls/fromAot/AotInvokeSpecial2NativeTest.java 8226295 generic-all
+compiler/aot/calls/fromAot/AotInvokeDynamic2AotTest.java 8226295 generic-all
+compiler/aot/calls/fromAot/AotInvokeInterface2NativeTest.java 8226295 generic-all
+compiler/aot/calls/fromAot/AotInvokeStatic2AotTest.java 8226295 generic-all
+compiler/aot/calls/fromAot/AotInvokeInterface2CompiledTest.java 8226295 generic-all
+compiler/aot/calls/fromAot/AotInvokeDynamic2CompiledTest.java 8226295 generic-all
+compiler/aot/calls/fromAot/AotInvokeStatic2NativeTest.java 8226295 generic-all
+compiler/aot/calls/fromAot/AotInvokeSpecial2AotTest.java 8226295 generic-all
+compiler/aot/calls/fromAot/AotInvokeSpecial2InterpretedTest.java 8226295 generic-all
+compiler/aot/calls/fromNative/NativeInvokeStatic2AotTest.java 8226295 generic-all
+compiler/aot/calls/fromNative/NativeInvokeSpecial2AotTest.java 8226295 generic-all
+compiler/aot/calls/fromNative/NativeInvokeVirtual2AotTest.java 8226295 generic-all
+compiler/aot/calls/fromCompiled/CompiledInvokeInterface2AotTest.java 8226295 generic-all
+compiler/aot/calls/fromCompiled/CompiledInvokeVirtual2AotTest.java 8226295 generic-all
+compiler/aot/calls/fromCompiled/CompiledInvokeDynamic2AotTest.java 8226295 generic-all
+compiler/aot/calls/fromCompiled/CompiledInvokeStatic2AotTest.java 8226295 generic-all
+compiler/aot/calls/fromCompiled/CompiledInvokeSpecial2AotTest.java 8226295 generic-all
+compiler/aot/calls/fromInterpreted/InterpretedInvokeDynamic2AotTest.java 8226295 generic-all
+compiler/aot/calls/fromInterpreted/InterpretedInvokeStatic2AotTest.java 8226295 generic-all
+compiler/aot/calls/fromInterpreted/InterpretedInvokeVirtual2AotTest.java 8226295 generic-all
+compiler/aot/calls/fromInterpreted/InterpretedInvokeSpecial2AotTest.java 8226295 generic-all
+compiler/aot/calls/fromInterpreted/InterpretedInvokeInterface2AotTest.java 8226295 generic-all
+compiler/aot/fingerprint/SelfChanged.java 8226295 generic-all
+compiler/aot/fingerprint/SelfChangedCDS.java 8226295 generic-all
+compiler/aot/fingerprint/SuperChanged.java 8226295 generic-all
+
 compiler/c2/Test8004741.java 8235801 generic-all
 
 compiler/jsr292/CreatesInterfaceDotEqualsCallInfo.java 8242923 generic-all
 
 #############################################################################
@@ -93,10 +170,32 @@
 
 # :hotspot_runtime
 
 runtime/jni/terminatedThread/TestTerminatedThread.java 8219652 aix-ppc64
 runtime/ReservedStack/ReservedStackTest.java 8231031 generic-all
+
+# Valhalla TODO:
+runtime/CompressedOops/CompressedClassPointers.java 8210258 generic-all
+runtime/RedefineTests/RedefineLeak.java 8205032 generic-all
+runtime/SharedArchiveFile/BootAppendTests.java 8210258 generic-all
+runtime/SharedArchiveFile/CdsDifferentCompactStrings.java 8210258 generic-all
+runtime/SharedArchiveFile/CdsDifferentObjectAlignment.java 8210258 generic-all
+runtime/SharedArchiveFile/NonBootLoaderClasses.java 8210258 generic-all
+runtime/SharedArchiveFile/PrintSharedArchiveAndExit.java 8210258 generic-all
+runtime/SharedArchiveFile/SharedArchiveFile.java 8210258 generic-all
+runtime/SharedArchiveFile/SharedStringsDedup.java 8210258 generic-all
+runtime/SharedArchiveFile/SharedStringsRunAuto.java 8210258 generic-all
+runtime/SharedArchiveFile/SharedSymbolTableBucketSize.java 8210258 generic-all
+runtime/SharedArchiveFile/SpaceUtilizationCheck.java 8210258 generic-all
+runtime/SharedArchiveFile/TestInterpreterMethodEntries.java 8210258 generic-all
+runtime/SharedArchiveFile/serviceability/transformRelatedClasses/TransformInterfaceAndImplementor.java 8210258 generic-all
+runtime/SharedArchiveFile/serviceability/transformRelatedClasses/TransformSuperAndSubClasses.java 8210258 generic-all
+runtime/SharedArchiveFile/serviceability/transformRelatedClasses/TransformSuperSubTwoPckgs.java 8210258 generic-all
+runtime/appcds/ClassLoaderTest.java 8210258 generic-all
+runtime/appcds/HelloTest.java 8210258 generic-all
+runtime/appcds/sharedStrings/SharedStringsBasic.java 8210258 generic-all
+
 
 #############################################################################
 
 # :hotspot_serviceability
 
@@ -106,10 +205,36 @@
 serviceability/sa/TestRevPtrsForInvokeDynamic.java 8241235 generic-all
 
 serviceability/jvmti/HeapMonitor/MyPackage/HeapMonitorStatIntervalTest.java 8214032 generic-all
 serviceability/jvmti/HeapMonitor/MyPackage/HeapMonitorStatArrayCorrectnessTest.java 8224150 generic-all
 
+# Valhalla TODO:
+serviceability/sa/ClhsdbCDSCore.java 8190936 generic-all
+serviceability/sa/ClhsdbCDSJstackPrintAll.java 8190936 generic-all
+serviceability/sa/ClhsdbFindPC.java 8190936 generic-all
+serviceability/sa/ClhsdbInspect.java 8190936 generic-all
+serviceability/sa/ClhsdbJdis.java 8190936 generic-all
+serviceability/sa/ClhsdbJstack.java 8190936 generic-all
+serviceability/sa/ClhsdbPrintAll.java 8190936 generic-all
+serviceability/sa/ClhsdbPrintAs.java 8190936 generic-all
+serviceability/sa/ClhsdbPrintStatics.java 8190936 generic-all
+serviceability/sa/ClhsdbSource.java 8190936 generic-all
+serviceability/sa/ClhsdbSymbol.java 8190936 generic-all
+serviceability/sa/ClhsdbWhere.java 8190936 generic-all
+serviceability/sa/JhsdbThreadInfoTest.java 8190936 generic-all
+serviceability/sa/TestClassDump.java 8190936 generic-all
+serviceability/sa/TestClhsdbJstackLock.java 8190936 generic-all
+serviceability/sa/TestCpoolForInvokeDynamic.java 8190936 generic-all
+serviceability/sa/TestHeapDumpForInvokeDynamic.java 8190936 generic-all
+serviceability/sa/TestHeapDumpForLargeArray.java 8190936 generic-all
+serviceability/sa/TestIntConstant.java 8190936 generic-all
+serviceability/sa/TestJhsdbJstackLock.java 8190936 generic-all
+serviceability/sa/TestJmapCore.java 8190936 generic-all
+serviceability/sa/TestJmapCoreMetaspace.java 8190936 generic-all
+serviceability/sa/TestPrintMdo.java 8190936 generic-all
+serviceability/sa/jmap-hprof/JMapHProfLargeHeapTest.java 8190936 generic-all
+
 #############################################################################
 
 # :hotspot_misc
 
 #############################################################################
diff a/test/hotspot/jtreg/compiler/jvmci/jdk.vm.ci.runtime.test/src/jdk/vm/ci/runtime/test/TestResolvedJavaType.java b/test/hotspot/jtreg/compiler/jvmci/jdk.vm.ci.runtime.test/src/jdk/vm/ci/runtime/test/TestResolvedJavaType.java
--- a/test/hotspot/jtreg/compiler/jvmci/jdk.vm.ci.runtime.test/src/jdk/vm/ci/runtime/test/TestResolvedJavaType.java
+++ b/test/hotspot/jtreg/compiler/jvmci/jdk.vm.ci.runtime.test/src/jdk/vm/ci/runtime/test/TestResolvedJavaType.java
@@ -271,11 +271,13 @@
     public void getInterfacesTest() {
         for (Class<?> c : classes) {
             ResolvedJavaType type = metaAccess.lookupJavaType(c);
             Class<?>[] expected = c.getInterfaces();
             ResolvedJavaType[] actual = type.getInterfaces();
-            assertEquals(expected.length, actual.length);
+            // With injection of the IdentityObject interface by the JVM, the number of
+            // interfaces visible through reflection and through JVMCI could differ by one
+            assertTrue(expected.length == actual.length || (actual.length - expected.length) == 1);
             for (int i = 0; i < expected.length; i++) {
                 assertTrue(actual[i].equals(metaAccess.lookupJavaType(expected[i])));
             }
         }
     }
diff a/test/hotspot/jtreg/compiler/tiered/ConstantGettersTransitionsTest.java b/test/hotspot/jtreg/compiler/tiered/ConstantGettersTransitionsTest.java
--- a/test/hotspot/jtreg/compiler/tiered/ConstantGettersTransitionsTest.java
+++ b/test/hotspot/jtreg/compiler/tiered/ConstantGettersTransitionsTest.java
@@ -22,10 +22,11 @@
  */
 
 /**
  * @test ConstantGettersTransitionsTest
  * @summary Test the correctness of compilation level transitions for constant getters methods
+ * @requires vm.opt.final.TieredCompilation
  * @library /test/lib /
  * @modules java.base/jdk.internal.misc
  *          java.management
  *
  * @build sun.hotspot.WhiteBox
diff a/test/hotspot/jtreg/compiler/types/TestMeetIncompatibleInterfaceArrays.java b/test/hotspot/jtreg/compiler/types/TestMeetIncompatibleInterfaceArrays.java
--- a/test/hotspot/jtreg/compiler/types/TestMeetIncompatibleInterfaceArrays.java
+++ b/test/hotspot/jtreg/compiler/types/TestMeetIncompatibleInterfaceArrays.java
@@ -23,10 +23,11 @@
 
 /*
  * @test
  * @bug 8141551
  * @summary C2 can not handle returns with inccompatible interface arrays
+ * @requires vm.opt.final.TieredCompilation
  * @requires vm.compMode == "Xmixed" & vm.flavor == "server"
  * @modules java.base/jdk.internal.org.objectweb.asm
  *          java.base/jdk.internal.misc
  * @library /test/lib /
  *
diff a/test/hotspot/jtreg/testlibrary_tests/TestPlatformIsTieredSupported.java b/test/hotspot/jtreg/testlibrary_tests/TestPlatformIsTieredSupported.java
--- a/test/hotspot/jtreg/testlibrary_tests/TestPlatformIsTieredSupported.java
+++ b/test/hotspot/jtreg/testlibrary_tests/TestPlatformIsTieredSupported.java
@@ -26,10 +26,11 @@
 import sun.hotspot.WhiteBox;
 
 /**
  * @test
  * @summary Verifies that Platform::isTieredSupported returns correct value.
+ * @requires vm.opt.final.TieredCompilation
  * @library /test/lib
  * @modules java.base/jdk.internal.misc
  *          java.management
  * @build sun.hotspot.WhiteBox
  * @run driver ClassFileInstaller sun.hotspot.WhiteBox
