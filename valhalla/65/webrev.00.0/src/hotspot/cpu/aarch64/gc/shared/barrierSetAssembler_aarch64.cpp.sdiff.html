<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/cpu/aarch64/gc/shared/barrierSetAssembler_aarch64.cpp</title>
    <link rel="stylesheet" href="../../../../../../style.css" />
  </head>
<body>
<center><a href="../../c1_MacroAssembler_aarch64.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../index.html" target="_top">index</a> <a href="barrierSetAssembler_aarch64.hpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/cpu/aarch64/gc/shared/barrierSetAssembler_aarch64.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
  1 /*
<span class="line-modified">  2  * Copyright (c) 2018, Oracle and/or its affiliates. All rights reserved.</span>
  3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  4  *
  5  * This code is free software; you can redistribute it and/or modify it
  6  * under the terms of the GNU General Public License version 2 only, as
  7  * published by the Free Software Foundation.
  8  *
  9  * This code is distributed in the hope that it will be useful, but WITHOUT
 10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
 11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
 12  * version 2 for more details (a copy is included in the LICENSE file that
 13  * accompanied this code).
 14  *
 15  * You should have received a copy of the GNU General Public License version
 16  * 2 along with this work; if not, write to the Free Software Foundation,
 17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 18  *
 19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
 20  * or visit www.oracle.com if you need additional information or have any
 21  * questions.
 22  *
 23  */
 24 
 25 #include &quot;precompiled.hpp&quot;

 26 #include &quot;gc/shared/barrierSetAssembler.hpp&quot;

 27 #include &quot;gc/shared/collectedHeap.hpp&quot;

 28 #include &quot;memory/universe.hpp&quot;
 29 #include &quot;runtime/jniHandles.hpp&quot;

 30 #include &quot;runtime/thread.hpp&quot;
 31 

 32 #define __ masm-&gt;
 33 
 34 void BarrierSetAssembler::load_at(MacroAssembler* masm, DecoratorSet decorators, BasicType type,
 35                                   Register dst, Address src, Register tmp1, Register tmp_thread) {
 36 
 37   // LR is live.  It must be saved around calls.
 38 
 39   bool in_heap = (decorators &amp; IN_HEAP) != 0;
 40   bool in_native = (decorators &amp; IN_NATIVE) != 0;
 41   bool is_not_null = (decorators &amp; IS_NOT_NULL) != 0;
 42   switch (type) {
 43   case T_OBJECT:
 44   case T_ARRAY: {
 45     if (in_heap) {
 46       if (UseCompressedOops) {
 47         __ ldrw(dst, src);
 48         if (is_not_null) {
 49           __ decode_heap_oop_not_null(dst);
 50         } else {
 51           __ decode_heap_oop(dst);
</pre>
<hr />
<pre>
 79   bool is_not_null = (decorators &amp; IS_NOT_NULL) != 0;
 80 
 81   switch (type) {
 82   case T_OBJECT:
 83   case T_ARRAY: {
 84    if (in_heap) {
 85       if (val == noreg) {
 86         assert(!is_not_null, &quot;inconsistent access&quot;);
 87         if (UseCompressedOops) {
 88           __ strw(zr, dst);
 89         } else {
 90           __ str(zr, dst);
 91         }
 92       } else {
 93         if (UseCompressedOops) {
 94           assert(!dst.uses(val), &quot;not enough registers&quot;);
 95           if (is_not_null) {
 96             __ encode_heap_oop_not_null(val);
 97           } else {
 98             __ encode_heap_oop(val);
<span class="line-modified"> 99           }</span>
100           __ strw(val, dst); 
101         } else {
102           __ str(val, dst);
103         }
104       }
105     } else {
106       assert(in_native, &quot;why else?&quot;);
107       assert(val != noreg, &quot;not supported&quot;);
108       __ str(val, dst);
109     }
110     break;
111   }
112   case T_BOOLEAN:
113     __ andw(val, val, 0x1);  // boolean is true if LSB is 1
114     __ strb(val, dst);
115     break;
116   case T_BYTE:    __ strb(val, dst); break;
117   case T_CHAR:    __ strh(val, dst); break;
118   case T_SHORT:   __ strh(val, dst); break;
119   case T_INT:     __ strw(val, dst); break;
</pre>
<hr />
<pre>
226 
227     incr_allocated_bytes(masm, var_size_in_bytes, con_size_in_bytes, t1);
228   }
229 }
230 
231 void BarrierSetAssembler::incr_allocated_bytes(MacroAssembler* masm,
232                                                Register var_size_in_bytes,
233                                                int con_size_in_bytes,
234                                                Register t1) {
235   assert(t1-&gt;is_valid(), &quot;need temp reg&quot;);
236 
237   __ ldr(t1, Address(rthread, in_bytes(JavaThread::allocated_bytes_offset())));
238   if (var_size_in_bytes-&gt;is_valid()) {
239     __ add(t1, t1, var_size_in_bytes);
240   } else {
241     __ add(t1, t1, con_size_in_bytes);
242   }
243   __ str(t1, Address(rthread, in_bytes(JavaThread::allocated_bytes_offset())));
244 }
245 
<span class="line-modified">246 void BarrierSetAssembler::nmethod_entry_barrier(MacroAssembler* masm)  {</span>
<span class="line-modified">247 // FIXME: 8210498: nmethod entry barriers is not implemented</span>
<span class="line-modified">248 #if 0</span>
<span class="line-removed">249  BarrierSetNMethod* bs_nm = BarrierSet::barrier_set()-&gt;barrier_set_nmethod();</span>
250   if (bs_nm == NULL) {
251     return;
252   }
<span class="line-modified">253   Label continuation;</span>
<span class="line-modified">254   Address disarmed_addr(rthread, in_bytes(bs_nm-&gt;thread_disarmed_offset()));</span>
<span class="line-modified">255   __ align(8);</span>
<span class="line-modified">256   __ ldr(rscratch1, disarmed_addr);</span>
<span class="line-modified">257   __ cbz(rscratch1, continuation);</span>
<span class="line-modified">258   __ blr(RuntimeAddress(StubRoutines::aarch64::method_entry_barrier()));</span>
<span class="line-modified">259   __ bind(continuation);</span>
<span class="line-modified">260 #endif</span>















































261 }
262 
</pre>
</td>
<td>
<hr />
<pre>
  1 /*
<span class="line-modified">  2  * Copyright (c) 2018, 2020, Oracle and/or its affiliates. All rights reserved.</span>
  3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  4  *
  5  * This code is free software; you can redistribute it and/or modify it
  6  * under the terms of the GNU General Public License version 2 only, as
  7  * published by the Free Software Foundation.
  8  *
  9  * This code is distributed in the hope that it will be useful, but WITHOUT
 10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
 11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
 12  * version 2 for more details (a copy is included in the LICENSE file that
 13  * accompanied this code).
 14  *
 15  * You should have received a copy of the GNU General Public License version
 16  * 2 along with this work; if not, write to the Free Software Foundation,
 17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 18  *
 19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
 20  * or visit www.oracle.com if you need additional information or have any
 21  * questions.
 22  *
 23  */
 24 
 25 #include &quot;precompiled.hpp&quot;
<span class="line-added"> 26 #include &quot;gc/shared/barrierSet.hpp&quot;</span>
 27 #include &quot;gc/shared/barrierSetAssembler.hpp&quot;
<span class="line-added"> 28 #include &quot;gc/shared/barrierSetNMethod.hpp&quot;</span>
 29 #include &quot;gc/shared/collectedHeap.hpp&quot;
<span class="line-added"> 30 #include &quot;interpreter/interp_masm.hpp&quot;</span>
 31 #include &quot;memory/universe.hpp&quot;
 32 #include &quot;runtime/jniHandles.hpp&quot;
<span class="line-added"> 33 #include &quot;runtime/sharedRuntime.hpp&quot;</span>
 34 #include &quot;runtime/thread.hpp&quot;
 35 
<span class="line-added"> 36 </span>
 37 #define __ masm-&gt;
 38 
 39 void BarrierSetAssembler::load_at(MacroAssembler* masm, DecoratorSet decorators, BasicType type,
 40                                   Register dst, Address src, Register tmp1, Register tmp_thread) {
 41 
 42   // LR is live.  It must be saved around calls.
 43 
 44   bool in_heap = (decorators &amp; IN_HEAP) != 0;
 45   bool in_native = (decorators &amp; IN_NATIVE) != 0;
 46   bool is_not_null = (decorators &amp; IS_NOT_NULL) != 0;
 47   switch (type) {
 48   case T_OBJECT:
 49   case T_ARRAY: {
 50     if (in_heap) {
 51       if (UseCompressedOops) {
 52         __ ldrw(dst, src);
 53         if (is_not_null) {
 54           __ decode_heap_oop_not_null(dst);
 55         } else {
 56           __ decode_heap_oop(dst);
</pre>
<hr />
<pre>
 84   bool is_not_null = (decorators &amp; IS_NOT_NULL) != 0;
 85 
 86   switch (type) {
 87   case T_OBJECT:
 88   case T_ARRAY: {
 89    if (in_heap) {
 90       if (val == noreg) {
 91         assert(!is_not_null, &quot;inconsistent access&quot;);
 92         if (UseCompressedOops) {
 93           __ strw(zr, dst);
 94         } else {
 95           __ str(zr, dst);
 96         }
 97       } else {
 98         if (UseCompressedOops) {
 99           assert(!dst.uses(val), &quot;not enough registers&quot;);
100           if (is_not_null) {
101             __ encode_heap_oop_not_null(val);
102           } else {
103             __ encode_heap_oop(val);
<span class="line-modified">104           }</span>
105           __ strw(val, dst);
106         } else {
107           __ str(val, dst);
108         }
109       }
110     } else {
111       assert(in_native, &quot;why else?&quot;);
112       assert(val != noreg, &quot;not supported&quot;);
113       __ str(val, dst);
114     }
115     break;
116   }
117   case T_BOOLEAN:
118     __ andw(val, val, 0x1);  // boolean is true if LSB is 1
119     __ strb(val, dst);
120     break;
121   case T_BYTE:    __ strb(val, dst); break;
122   case T_CHAR:    __ strh(val, dst); break;
123   case T_SHORT:   __ strh(val, dst); break;
124   case T_INT:     __ strw(val, dst); break;
</pre>
<hr />
<pre>
231 
232     incr_allocated_bytes(masm, var_size_in_bytes, con_size_in_bytes, t1);
233   }
234 }
235 
236 void BarrierSetAssembler::incr_allocated_bytes(MacroAssembler* masm,
237                                                Register var_size_in_bytes,
238                                                int con_size_in_bytes,
239                                                Register t1) {
240   assert(t1-&gt;is_valid(), &quot;need temp reg&quot;);
241 
242   __ ldr(t1, Address(rthread, in_bytes(JavaThread::allocated_bytes_offset())));
243   if (var_size_in_bytes-&gt;is_valid()) {
244     __ add(t1, t1, var_size_in_bytes);
245   } else {
246     __ add(t1, t1, con_size_in_bytes);
247   }
248   __ str(t1, Address(rthread, in_bytes(JavaThread::allocated_bytes_offset())));
249 }
250 
<span class="line-modified">251 void BarrierSetAssembler::nmethod_entry_barrier(MacroAssembler* masm) {</span>
<span class="line-modified">252   BarrierSetNMethod* bs_nm = BarrierSet::barrier_set()-&gt;barrier_set_nmethod();</span>
<span class="line-modified">253 </span>

254   if (bs_nm == NULL) {
255     return;
256   }
<span class="line-modified">257 </span>
<span class="line-modified">258   Label skip, guard;</span>
<span class="line-modified">259   Address thread_disarmed_addr(rthread, in_bytes(bs_nm-&gt;thread_disarmed_offset()));</span>
<span class="line-modified">260 </span>
<span class="line-modified">261   __ ldrw(rscratch1, guard);</span>
<span class="line-modified">262 </span>
<span class="line-modified">263   // Subsequent loads of oops must occur after load of guard value.</span>
<span class="line-modified">264   // BarrierSetNMethod::disarm sets guard with release semantics.</span>
<span class="line-added">265   __ membar(__ LoadLoad);</span>
<span class="line-added">266   __ ldrw(rscratch2, thread_disarmed_addr);</span>
<span class="line-added">267   __ cmpw(rscratch1, rscratch2);</span>
<span class="line-added">268   __ br(Assembler::EQ, skip);</span>
<span class="line-added">269 </span>
<span class="line-added">270   __ mov(rscratch1, StubRoutines::aarch64::method_entry_barrier());</span>
<span class="line-added">271   __ blr(rscratch1);</span>
<span class="line-added">272   __ b(skip);</span>
<span class="line-added">273 </span>
<span class="line-added">274   __ bind(guard);</span>
<span class="line-added">275 </span>
<span class="line-added">276   __ emit_int32(0);   // nmethod guard value. Skipped over in common case.</span>
<span class="line-added">277 </span>
<span class="line-added">278   __ bind(skip);</span>
<span class="line-added">279 }</span>
<span class="line-added">280 </span>
<span class="line-added">281 void BarrierSetAssembler::c2i_entry_barrier(MacroAssembler* masm) {</span>
<span class="line-added">282   BarrierSetNMethod* bs = BarrierSet::barrier_set()-&gt;barrier_set_nmethod();</span>
<span class="line-added">283   if (bs == NULL) {</span>
<span class="line-added">284     return;</span>
<span class="line-added">285   }</span>
<span class="line-added">286 </span>
<span class="line-added">287   Label bad_call;</span>
<span class="line-added">288   __ cbz(rmethod, bad_call);</span>
<span class="line-added">289 </span>
<span class="line-added">290   // Pointer chase to the method holder to find out if the method is concurrently unloading.</span>
<span class="line-added">291   Label method_live;</span>
<span class="line-added">292   __ load_method_holder_cld(rscratch1, rmethod);</span>
<span class="line-added">293 </span>
<span class="line-added">294   // Is it a strong CLD?</span>
<span class="line-added">295   __ ldr(rscratch2, Address(rscratch1, ClassLoaderData::keep_alive_offset()));</span>
<span class="line-added">296   __ cbnz(rscratch2, method_live);</span>
<span class="line-added">297 </span>
<span class="line-added">298   // Is it a weak but alive CLD?</span>
<span class="line-added">299   __ stp(r10, r11, Address(__ pre(sp, -2 * wordSize)));</span>
<span class="line-added">300   __ ldr(r10, Address(rscratch1, ClassLoaderData::holder_offset()));</span>
<span class="line-added">301 </span>
<span class="line-added">302   // Uses rscratch1 &amp; rscratch2, so we must pass new temporaries.</span>
<span class="line-added">303   __ resolve_weak_handle(r10, r11);</span>
<span class="line-added">304   __ mov(rscratch1, r10);</span>
<span class="line-added">305   __ ldp(r10, r11, Address(__ post(sp, 2 * wordSize)));</span>
<span class="line-added">306   __ cbnz(rscratch1, method_live);</span>
<span class="line-added">307 </span>
<span class="line-added">308   __ bind(bad_call);</span>
<span class="line-added">309 </span>
<span class="line-added">310   __ far_jump(RuntimeAddress(SharedRuntime::get_handle_wrong_method_stub()));</span>
<span class="line-added">311   __ bind(method_live);</span>
312 }
313 
</pre>
</td>
</tr>
</table>
<center><a href="../../c1_MacroAssembler_aarch64.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../index.html" target="_top">index</a> <a href="barrierSetAssembler_aarch64.hpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>