<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Frames src/hotspot/cpu/aarch64/stubGenerator_aarch64.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
    <script type="text/javascript" src="../../../../navigation.js"></script>
  </head>
<body onkeypress="keypress(event);">
<a name="0"></a>
<hr />
<pre>   1 /*
   2  * Copyright (c) 2003, 2020, Oracle and/or its affiliates. All rights reserved.
<a name="1" id="anc1"></a><span class="line-modified">   3  * Copyright (c) 2014, 2019, Red Hat Inc. All rights reserved.</span>
   4  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   5  *
   6  * This code is free software; you can redistribute it and/or modify it
   7  * under the terms of the GNU General Public License version 2 only, as
   8  * published by the Free Software Foundation.
   9  *
  10  * This code is distributed in the hope that it will be useful, but WITHOUT
  11  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  12  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  13  * version 2 for more details (a copy is included in the LICENSE file that
  14  * accompanied this code).
  15  *
  16  * You should have received a copy of the GNU General Public License version
  17  * 2 along with this work; if not, write to the Free Software Foundation,
  18  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  19  *
  20  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  21  * or visit www.oracle.com if you need additional information or have any
  22  * questions.
  23  *
  24  */
  25 
  26 #include &quot;precompiled.hpp&quot;
  27 #include &quot;asm/macroAssembler.hpp&quot;
  28 #include &quot;asm/macroAssembler.inline.hpp&quot;
  29 #include &quot;gc/shared/barrierSet.hpp&quot;
  30 #include &quot;gc/shared/barrierSetAssembler.hpp&quot;
  31 #include &quot;interpreter/interpreter.hpp&quot;
  32 #include &quot;memory/universe.hpp&quot;
  33 #include &quot;nativeInst_aarch64.hpp&quot;
  34 #include &quot;oops/instanceOop.hpp&quot;
  35 #include &quot;oops/method.hpp&quot;
  36 #include &quot;oops/objArrayKlass.hpp&quot;
  37 #include &quot;oops/oop.inline.hpp&quot;
  38 #include &quot;prims/methodHandles.hpp&quot;
  39 #include &quot;runtime/frame.inline.hpp&quot;
  40 #include &quot;runtime/handles.inline.hpp&quot;
  41 #include &quot;runtime/sharedRuntime.hpp&quot;
  42 #include &quot;runtime/stubCodeGenerator.hpp&quot;
  43 #include &quot;runtime/stubRoutines.hpp&quot;
  44 #include &quot;runtime/thread.inline.hpp&quot;
  45 #include &quot;utilities/align.hpp&quot;
  46 #include &quot;utilities/powerOfTwo.hpp&quot;
  47 #ifdef COMPILER2
  48 #include &quot;opto/runtime.hpp&quot;
  49 #endif
  50 #if INCLUDE_ZGC
  51 #include &quot;gc/z/zThreadLocalData.hpp&quot;
  52 #endif
  53 
  54 // Declaration and definition of StubGenerator (no .hpp file).
  55 // For a more detailed description of the stub routine structure
  56 // see the comment in stubRoutines.hpp
  57 
  58 #undef __
  59 #define __ _masm-&gt;
  60 #define TIMES_OOP Address::sxtw(exact_log2(UseCompressedOops ? 4 : 8))
  61 
  62 #ifdef PRODUCT
  63 #define BLOCK_COMMENT(str) /* nothing */
  64 #else
  65 #define BLOCK_COMMENT(str) __ block_comment(str)
  66 #endif
  67 
  68 #define BIND(label) bind(label); BLOCK_COMMENT(#label &quot;:&quot;)
  69 
  70 // Stub Code definitions
  71 
  72 class StubGenerator: public StubCodeGenerator {
  73  private:
  74 
  75 #ifdef PRODUCT
  76 #define inc_counter_np(counter) ((void)0)
  77 #else
  78   void inc_counter_np_(int&amp; counter) {
  79     __ lea(rscratch2, ExternalAddress((address)&amp;counter));
  80     __ ldrw(rscratch1, Address(rscratch2));
  81     __ addw(rscratch1, rscratch1, 1);
  82     __ strw(rscratch1, Address(rscratch2));
  83   }
  84 #define inc_counter_np(counter) \
  85   BLOCK_COMMENT(&quot;inc_counter &quot; #counter); \
  86   inc_counter_np_(counter);
  87 #endif
  88 
  89   // Call stubs are used to call Java from C
  90   //
  91   // Arguments:
  92   //    c_rarg0:   call wrapper address                   address
  93   //    c_rarg1:   result                                 address
  94   //    c_rarg2:   result type                            BasicType
  95   //    c_rarg3:   method                                 Method*
  96   //    c_rarg4:   (interpreter) entry point              address
  97   //    c_rarg5:   parameters                             intptr_t*
  98   //    c_rarg6:   parameter size (in words)              int
  99   //    c_rarg7:   thread                                 Thread*
 100   //
 101   // There is no return from the stub itself as any Java result
 102   // is written to result
 103   //
 104   // we save r30 (lr) as the return PC at the base of the frame and
 105   // link r29 (fp) below it as the frame pointer installing sp (r31)
 106   // into fp.
 107   //
 108   // we save r0-r7, which accounts for all the c arguments.
 109   //
 110   // TODO: strictly do we need to save them all? they are treated as
 111   // volatile by C so could we omit saving the ones we are going to
 112   // place in global registers (thread? method?) or those we only use
 113   // during setup of the Java call?
 114   //
 115   // we don&#39;t need to save r8 which C uses as an indirect result location
 116   // return register.
 117   //
 118   // we don&#39;t need to save r9-r15 which both C and Java treat as
 119   // volatile
 120   //
 121   // we don&#39;t need to save r16-18 because Java does not use them
 122   //
 123   // we save r19-r28 which Java uses as scratch registers and C
 124   // expects to be callee-save
 125   //
 126   // we save the bottom 64 bits of each value stored in v8-v15; it is
 127   // the responsibility of the caller to preserve larger values.
 128   //
 129   // so the stub frame looks like this when we enter Java code
 130   //
 131   //     [ return_from_Java     ] &lt;--- sp
 132   //     [ argument word n      ]
 133   //      ...
 134   // -27 [ argument word 1      ]
 135   // -26 [ saved v15            ] &lt;--- sp_after_call
 136   // -25 [ saved v14            ]
 137   // -24 [ saved v13            ]
 138   // -23 [ saved v12            ]
 139   // -22 [ saved v11            ]
 140   // -21 [ saved v10            ]
 141   // -20 [ saved v9             ]
 142   // -19 [ saved v8             ]
 143   // -18 [ saved r28            ]
 144   // -17 [ saved r27            ]
 145   // -16 [ saved r26            ]
 146   // -15 [ saved r25            ]
 147   // -14 [ saved r24            ]
 148   // -13 [ saved r23            ]
 149   // -12 [ saved r22            ]
 150   // -11 [ saved r21            ]
 151   // -10 [ saved r20            ]
 152   //  -9 [ saved r19            ]
 153   //  -8 [ call wrapper    (r0) ]
 154   //  -7 [ result          (r1) ]
 155   //  -6 [ result type     (r2) ]
 156   //  -5 [ method          (r3) ]
 157   //  -4 [ entry point     (r4) ]
 158   //  -3 [ parameters      (r5) ]
 159   //  -2 [ parameter size  (r6) ]
 160   //  -1 [ thread (r7)          ]
 161   //   0 [ saved fp       (r29) ] &lt;--- fp == saved sp (r31)
 162   //   1 [ saved lr       (r30) ]
 163 
 164   // Call stub stack layout word offsets from fp
 165   enum call_stub_layout {
 166     sp_after_call_off = -26,
 167 
 168     d15_off            = -26,
 169     d13_off            = -24,
 170     d11_off            = -22,
 171     d9_off             = -20,
 172 
 173     r28_off            = -18,
 174     r26_off            = -16,
 175     r24_off            = -14,
 176     r22_off            = -12,
 177     r20_off            = -10,
 178     call_wrapper_off   =  -8,
 179     result_off         =  -7,
 180     result_type_off    =  -6,
 181     method_off         =  -5,
 182     entry_point_off    =  -4,
 183     parameter_size_off =  -2,
 184     thread_off         =  -1,
 185     fp_f               =   0,
 186     retaddr_off        =   1,
 187   };
 188 
 189   address generate_call_stub(address&amp; return_address) {
 190     assert((int)frame::entry_frame_after_call_words == -(int)sp_after_call_off + 1 &amp;&amp;
 191            (int)frame::entry_frame_call_wrapper_offset == (int)call_wrapper_off,
 192            &quot;adjust this code&quot;);
 193 
 194     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;call_stub&quot;);
 195     address start = __ pc();
 196 
 197     const Address sp_after_call(rfp, sp_after_call_off * wordSize);
 198 
 199     const Address call_wrapper  (rfp, call_wrapper_off   * wordSize);
 200     const Address result        (rfp, result_off         * wordSize);
 201     const Address result_type   (rfp, result_type_off    * wordSize);
 202     const Address method        (rfp, method_off         * wordSize);
 203     const Address entry_point   (rfp, entry_point_off    * wordSize);
 204     const Address parameter_size(rfp, parameter_size_off * wordSize);
 205 
 206     const Address thread        (rfp, thread_off         * wordSize);
 207 
 208     const Address d15_save      (rfp, d15_off * wordSize);
 209     const Address d13_save      (rfp, d13_off * wordSize);
 210     const Address d11_save      (rfp, d11_off * wordSize);
 211     const Address d9_save       (rfp, d9_off * wordSize);
 212 
 213     const Address r28_save      (rfp, r28_off * wordSize);
 214     const Address r26_save      (rfp, r26_off * wordSize);
 215     const Address r24_save      (rfp, r24_off * wordSize);
 216     const Address r22_save      (rfp, r22_off * wordSize);
 217     const Address r20_save      (rfp, r20_off * wordSize);
 218 
 219     // stub code
 220 
 221     address aarch64_entry = __ pc();
 222 
 223     // set up frame and move sp to end of save area
 224     __ enter();
 225     __ sub(sp, rfp, -sp_after_call_off * wordSize);
 226 
 227     // save register parameters and Java scratch/global registers
 228     // n.b. we save thread even though it gets installed in
 229     // rthread because we want to sanity check rthread later
 230     __ str(c_rarg7,  thread);
 231     __ strw(c_rarg6, parameter_size);
 232     __ stp(c_rarg4, c_rarg5,  entry_point);
 233     __ stp(c_rarg2, c_rarg3,  result_type);
 234     __ stp(c_rarg0, c_rarg1,  call_wrapper);
 235 
 236     __ stp(r20, r19,   r20_save);
 237     __ stp(r22, r21,   r22_save);
 238     __ stp(r24, r23,   r24_save);
 239     __ stp(r26, r25,   r26_save);
 240     __ stp(r28, r27,   r28_save);
 241 
 242     __ stpd(v9,  v8,   d9_save);
 243     __ stpd(v11, v10,  d11_save);
 244     __ stpd(v13, v12,  d13_save);
 245     __ stpd(v15, v14,  d15_save);
 246 
 247     // install Java thread in global register now we have saved
 248     // whatever value it held
 249     __ mov(rthread, c_rarg7);
 250     // And method
 251     __ mov(rmethod, c_rarg3);
 252 
 253     // set up the heapbase register
 254     __ reinit_heapbase();
 255 
 256 #ifdef ASSERT
 257     // make sure we have no pending exceptions
 258     {
 259       Label L;
 260       __ ldr(rscratch1, Address(rthread, in_bytes(Thread::pending_exception_offset())));
 261       __ cmp(rscratch1, (u1)NULL_WORD);
 262       __ br(Assembler::EQ, L);
 263       __ stop(&quot;StubRoutines::call_stub: entered with pending exception&quot;);
 264       __ BIND(L);
 265     }
 266 #endif
 267     // pass parameters if any
 268     __ mov(esp, sp);
 269     __ sub(rscratch1, sp, c_rarg6, ext::uxtw, LogBytesPerWord); // Move SP out of the way
 270     __ andr(sp, rscratch1, -2 * wordSize);
 271 
 272     BLOCK_COMMENT(&quot;pass parameters if any&quot;);
 273     Label parameters_done;
 274     // parameter count is still in c_rarg6
 275     // and parameter pointer identifying param 1 is in c_rarg5
 276     __ cbzw(c_rarg6, parameters_done);
 277 
 278     address loop = __ pc();
 279     __ ldr(rscratch1, Address(__ post(c_rarg5, wordSize)));
 280     __ subsw(c_rarg6, c_rarg6, 1);
 281     __ push(rscratch1);
 282     __ br(Assembler::GT, loop);
 283 
 284     __ BIND(parameters_done);
 285 
 286     // call Java entry -- passing methdoOop, and current sp
 287     //      rmethod: Method*
 288     //      r13: sender sp
 289     BLOCK_COMMENT(&quot;call Java function&quot;);
 290     __ mov(r13, sp);
 291     __ blr(c_rarg4);
 292 
 293     // we do this here because the notify will already have been done
 294     // if we get to the next instruction via an exception
 295     //
 296     // n.b. adding this instruction here affects the calculation of
 297     // whether or not a routine returns to the call stub (used when
 298     // doing stack walks) since the normal test is to check the return
 299     // pc against the address saved below. so we may need to allow for
 300     // this extra instruction in the check.
 301 
 302     // save current address for use by exception handling code
 303 
 304     return_address = __ pc();
 305 
 306     // store result depending on type (everything that is not
 307     // T_OBJECT, T_VALUETYPE, T_LONG, T_FLOAT or T_DOUBLE is treated as T_INT)
 308     // n.b. this assumes Java returns an integral result in r0
 309     // and a floating result in j_farg0
 310     __ ldr(j_rarg2, result);
 311     Label is_long, is_float, is_double, is_value, exit;
 312     __ ldr(j_rarg1, result_type);
 313     __ cmp(j_rarg1, (u1)T_OBJECT);
 314     __ br(Assembler::EQ, is_long);
 315     __ cmp(j_rarg1, (u1)T_VALUETYPE);
 316     __ br(Assembler::EQ, is_value);
 317     __ cmp(j_rarg1, (u1)T_LONG);
 318     __ br(Assembler::EQ, is_long);
 319     __ cmp(j_rarg1, (u1)T_FLOAT);
 320     __ br(Assembler::EQ, is_float);
 321     __ cmp(j_rarg1, (u1)T_DOUBLE);
 322     __ br(Assembler::EQ, is_double);
 323 
 324     // handle T_INT case
 325     __ strw(r0, Address(j_rarg2));
 326 
 327     __ BIND(exit);
 328 
 329     // pop parameters
 330     __ sub(esp, rfp, -sp_after_call_off * wordSize);
 331 
 332 #ifdef ASSERT
 333     // verify that threads correspond
 334     {
 335       Label L, S;
 336       __ ldr(rscratch1, thread);
 337       __ cmp(rthread, rscratch1);
 338       __ br(Assembler::NE, S);
 339       __ get_thread(rscratch1);
 340       __ cmp(rthread, rscratch1);
 341       __ br(Assembler::EQ, L);
 342       __ BIND(S);
 343       __ stop(&quot;StubRoutines::call_stub: threads must correspond&quot;);
 344       __ BIND(L);
 345     }
 346 #endif
 347 
 348     // restore callee-save registers
 349     __ ldpd(v15, v14,  d15_save);
 350     __ ldpd(v13, v12,  d13_save);
 351     __ ldpd(v11, v10,  d11_save);
 352     __ ldpd(v9,  v8,   d9_save);
 353 
 354     __ ldp(r28, r27,   r28_save);
 355     __ ldp(r26, r25,   r26_save);
 356     __ ldp(r24, r23,   r24_save);
 357     __ ldp(r22, r21,   r22_save);
 358     __ ldp(r20, r19,   r20_save);
 359 
 360     __ ldp(c_rarg0, c_rarg1,  call_wrapper);
 361     __ ldrw(c_rarg2, result_type);
 362     __ ldr(c_rarg3,  method);
 363     __ ldp(c_rarg4, c_rarg5,  entry_point);
 364     __ ldp(c_rarg6, c_rarg7,  parameter_size);
 365 
 366     // leave frame and return to caller
 367     __ leave();
 368     __ ret(lr);
 369 
 370     // handle return types different from T_INT
 371     __ BIND(is_value);
 372     if (InlineTypeReturnedAsFields) {
 373       // Check for flattened return value
 374       __ cbz(r0, is_long);
 375       // Initialize pre-allocated buffer
 376       __ mov(r1, r0);
 377       __ andr(r1, r1, -2);
 378       __ ldr(r1, Address(r1, InstanceKlass::adr_valueklass_fixed_block_offset()));
 379       __ ldr(r1, Address(r1, ValueKlass::pack_handler_offset()));
 380       __ ldr(r0, Address(j_rarg2, 0));
 381       __ blr(r1);
 382       __ b(exit);
 383     }
 384 
 385     __ BIND(is_long);
 386     __ str(r0, Address(j_rarg2, 0));
 387     __ br(Assembler::AL, exit);
 388 
 389     __ BIND(is_float);
 390     __ strs(j_farg0, Address(j_rarg2, 0));
 391     __ br(Assembler::AL, exit);
 392 
 393     __ BIND(is_double);
 394     __ strd(j_farg0, Address(j_rarg2, 0));
 395     __ br(Assembler::AL, exit);
 396 
 397     return start;
 398   }
 399 
 400   // Return point for a Java call if there&#39;s an exception thrown in
 401   // Java code.  The exception is caught and transformed into a
 402   // pending exception stored in JavaThread that can be tested from
 403   // within the VM.
 404   //
 405   // Note: Usually the parameters are removed by the callee. In case
 406   // of an exception crossing an activation frame boundary, that is
 407   // not the case if the callee is compiled code =&gt; need to setup the
 408   // rsp.
 409   //
 410   // r0: exception oop
 411 
 412   address generate_catch_exception() {
 413     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;catch_exception&quot;);
 414     address start = __ pc();
 415 
 416     // same as in generate_call_stub():
 417     const Address sp_after_call(rfp, sp_after_call_off * wordSize);
 418     const Address thread        (rfp, thread_off         * wordSize);
 419 
 420 #ifdef ASSERT
 421     // verify that threads correspond
 422     {
 423       Label L, S;
 424       __ ldr(rscratch1, thread);
 425       __ cmp(rthread, rscratch1);
 426       __ br(Assembler::NE, S);
 427       __ get_thread(rscratch1);
 428       __ cmp(rthread, rscratch1);
 429       __ br(Assembler::EQ, L);
 430       __ bind(S);
 431       __ stop(&quot;StubRoutines::catch_exception: threads must correspond&quot;);
 432       __ bind(L);
 433     }
 434 #endif
 435 
 436     // set pending exception
 437     __ verify_oop(r0);
 438 
 439     __ str(r0, Address(rthread, Thread::pending_exception_offset()));
 440     __ mov(rscratch1, (address)__FILE__);
 441     __ str(rscratch1, Address(rthread, Thread::exception_file_offset()));
 442     __ movw(rscratch1, (int)__LINE__);
 443     __ strw(rscratch1, Address(rthread, Thread::exception_line_offset()));
 444 
 445     // complete return to VM
 446     assert(StubRoutines::_call_stub_return_address != NULL,
 447            &quot;_call_stub_return_address must have been generated before&quot;);
 448     __ b(StubRoutines::_call_stub_return_address);
 449 
 450     return start;
 451   }
 452 
 453   // Continuation point for runtime calls returning with a pending
 454   // exception.  The pending exception check happened in the runtime
 455   // or native call stub.  The pending exception in Thread is
 456   // converted into a Java-level exception.
 457   //
 458   // Contract with Java-level exception handlers:
 459   // r0: exception
 460   // r3: throwing pc
 461   //
 462   // NOTE: At entry of this stub, exception-pc must be in LR !!
 463 
 464   // NOTE: this is always used as a jump target within generated code
 465   // so it just needs to be generated code wiht no x86 prolog
 466 
 467   address generate_forward_exception() {
 468     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;forward exception&quot;);
 469     address start = __ pc();
 470 
 471     // Upon entry, LR points to the return address returning into
 472     // Java (interpreted or compiled) code; i.e., the return address
 473     // becomes the throwing pc.
 474     //
 475     // Arguments pushed before the runtime call are still on the stack
 476     // but the exception handler will reset the stack pointer -&gt;
 477     // ignore them.  A potential result in registers can be ignored as
 478     // well.
 479 
 480 #ifdef ASSERT
 481     // make sure this code is only executed if there is a pending exception
 482     {
 483       Label L;
 484       __ ldr(rscratch1, Address(rthread, Thread::pending_exception_offset()));
 485       __ cbnz(rscratch1, L);
 486       __ stop(&quot;StubRoutines::forward exception: no pending exception (1)&quot;);
 487       __ bind(L);
 488     }
 489 #endif
 490 
 491     // compute exception handler into r19
 492 
 493     // call the VM to find the handler address associated with the
 494     // caller address. pass thread in r0 and caller pc (ret address)
 495     // in r1. n.b. the caller pc is in lr, unlike x86 where it is on
 496     // the stack.
 497     __ mov(c_rarg1, lr);
 498     // lr will be trashed by the VM call so we move it to R19
 499     // (callee-saved) because we also need to pass it to the handler
 500     // returned by this call.
 501     __ mov(r19, lr);
 502     BLOCK_COMMENT(&quot;call exception_handler_for_return_address&quot;);
 503     __ call_VM_leaf(CAST_FROM_FN_PTR(address,
 504                          SharedRuntime::exception_handler_for_return_address),
 505                     rthread, c_rarg1);
 506     // we should not really care that lr is no longer the callee
 507     // address. we saved the value the handler needs in r19 so we can
 508     // just copy it to r3. however, the C2 handler will push its own
 509     // frame and then calls into the VM and the VM code asserts that
 510     // the PC for the frame above the handler belongs to a compiled
 511     // Java method. So, we restore lr here to satisfy that assert.
 512     __ mov(lr, r19);
 513     // setup r0 &amp; r3 &amp; clear pending exception
 514     __ mov(r3, r19);
 515     __ mov(r19, r0);
 516     __ ldr(r0, Address(rthread, Thread::pending_exception_offset()));
 517     __ str(zr, Address(rthread, Thread::pending_exception_offset()));
 518 
 519 #ifdef ASSERT
 520     // make sure exception is set
 521     {
 522       Label L;
 523       __ cbnz(r0, L);
 524       __ stop(&quot;StubRoutines::forward exception: no pending exception (2)&quot;);
 525       __ bind(L);
 526     }
 527 #endif
 528 
 529     // continue at exception handler
 530     // r0: exception
 531     // r3: throwing pc
 532     // r19: exception handler
 533     __ verify_oop(r0);
 534     __ br(r19);
 535 
 536     return start;
 537   }
 538 
 539   // Non-destructive plausibility checks for oops
 540   //
 541   // Arguments:
 542   //    r0: oop to verify
 543   //    rscratch1: error message
 544   //
 545   // Stack after saving c_rarg3:
 546   //    [tos + 0]: saved c_rarg3
 547   //    [tos + 1]: saved c_rarg2
 548   //    [tos + 2]: saved lr
 549   //    [tos + 3]: saved rscratch2
 550   //    [tos + 4]: saved r0
 551   //    [tos + 5]: saved rscratch1
 552   address generate_verify_oop() {
 553 
 554     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;verify_oop&quot;);
 555     address start = __ pc();
 556 
 557     Label exit, error;
 558 
 559     // save c_rarg2 and c_rarg3
 560     __ stp(c_rarg3, c_rarg2, Address(__ pre(sp, -16)));
 561 
 562     // __ incrementl(ExternalAddress((address) StubRoutines::verify_oop_count_addr()));
 563     __ lea(c_rarg2, ExternalAddress((address) StubRoutines::verify_oop_count_addr()));
 564     __ ldr(c_rarg3, Address(c_rarg2));
 565     __ add(c_rarg3, c_rarg3, 1);
 566     __ str(c_rarg3, Address(c_rarg2));
 567 
 568     // object is in r0
 569     // make sure object is &#39;reasonable&#39;
 570     __ cbz(r0, exit); // if obj is NULL it is OK
 571 
 572 #if INCLUDE_ZGC
 573     if (UseZGC) {
 574       // Check if mask is good.
 575       // verifies that ZAddressBadMask &amp; r0 == 0
 576       __ ldr(c_rarg3, Address(rthread, ZThreadLocalData::address_bad_mask_offset()));
 577       __ andr(c_rarg2, r0, c_rarg3);
 578       __ cbnz(c_rarg2, error);
 579     }
 580 #endif
 581 
 582     // Check if the oop is in the right area of memory
 583     __ mov(c_rarg3, (intptr_t) Universe::verify_oop_mask());
 584     __ andr(c_rarg2, r0, c_rarg3);
 585     __ mov(c_rarg3, (intptr_t) Universe::verify_oop_bits());
 586 
 587     // Compare c_rarg2 and c_rarg3.  We don&#39;t use a compare
 588     // instruction here because the flags register is live.
 589     __ eor(c_rarg2, c_rarg2, c_rarg3);
 590     __ cbnz(c_rarg2, error);
 591 
 592     // make sure klass is &#39;reasonable&#39;, which is not zero.
 593     __ load_klass(r0, r0);  // get klass
 594     __ cbz(r0, error);      // if klass is NULL it is broken
 595 
 596     // return if everything seems ok
 597     __ bind(exit);
 598 
 599     __ ldp(c_rarg3, c_rarg2, Address(__ post(sp, 16)));
 600     __ ret(lr);
 601 
 602     // handle errors
 603     __ bind(error);
 604     __ ldp(c_rarg3, c_rarg2, Address(__ post(sp, 16)));
 605 
 606     __ push(RegSet::range(r0, r29), sp);
 607     // debug(char* msg, int64_t pc, int64_t regs[])
 608     __ mov(c_rarg0, rscratch1);      // pass address of error message
 609     __ mov(c_rarg1, lr);             // pass return address
 610     __ mov(c_rarg2, sp);             // pass address of regs on stack
 611 #ifndef PRODUCT
 612     assert(frame::arg_reg_save_area_bytes == 0, &quot;not expecting frame reg save area&quot;);
 613 #endif
 614     BLOCK_COMMENT(&quot;call MacroAssembler::debug&quot;);
 615     __ mov(rscratch1, CAST_FROM_FN_PTR(address, MacroAssembler::debug64));
 616     __ blr(rscratch1);
 617     __ hlt(0);
 618 
 619     return start;
 620   }
 621 
 622   void array_overlap_test(Label&amp; L_no_overlap, Address::sxtw sf) { __ b(L_no_overlap); }
 623 
 624   // The inner part of zero_words().  This is the bulk operation,
 625   // zeroing words in blocks, possibly using DC ZVA to do it.  The
 626   // caller is responsible for zeroing the last few words.
 627   //
 628   // Inputs:
 629   // r10: the HeapWord-aligned base address of an array to zero.
 630   // r11: the count in HeapWords, r11 &gt; 0.
 631   //
 632   // Returns r10 and r11, adjusted for the caller to clear.
 633   // r10: the base address of the tail of words left to clear.
 634   // r11: the number of words in the tail.
 635   //      r11 &lt; MacroAssembler::zero_words_block_size.
 636 
 637   address generate_zero_blocks() {
 638     Label done;
 639     Label base_aligned;
 640 
 641     Register base = r10, cnt = r11;
 642 
 643     __ align(CodeEntryAlignment);
 644     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;zero_blocks&quot;);
 645     address start = __ pc();
 646 
 647     if (UseBlockZeroing) {
 648       int zva_length = VM_Version::zva_length();
 649 
 650       // Ensure ZVA length can be divided by 16. This is required by
 651       // the subsequent operations.
 652       assert (zva_length % 16 == 0, &quot;Unexpected ZVA Length&quot;);
 653 
 654       __ tbz(base, 3, base_aligned);
 655       __ str(zr, Address(__ post(base, 8)));
 656       __ sub(cnt, cnt, 1);
 657       __ bind(base_aligned);
 658 
 659       // Ensure count &gt;= zva_length * 2 so that it still deserves a zva after
 660       // alignment.
 661       Label small;
 662       int low_limit = MAX2(zva_length * 2, (int)BlockZeroingLowLimit);
 663       __ subs(rscratch1, cnt, low_limit &gt;&gt; 3);
 664       __ br(Assembler::LT, small);
 665       __ zero_dcache_blocks(base, cnt);
 666       __ bind(small);
 667     }
 668 
 669     {
 670       // Number of stp instructions we&#39;ll unroll
 671       const int unroll =
 672         MacroAssembler::zero_words_block_size / 2;
 673       // Clear the remaining blocks.
 674       Label loop;
 675       __ subs(cnt, cnt, unroll * 2);
 676       __ br(Assembler::LT, done);
 677       __ bind(loop);
 678       for (int i = 0; i &lt; unroll; i++)
 679         __ stp(zr, zr, __ post(base, 16));
 680       __ subs(cnt, cnt, unroll * 2);
 681       __ br(Assembler::GE, loop);
 682       __ bind(done);
 683       __ add(cnt, cnt, unroll * 2);
 684     }
 685 
 686     __ ret(lr);
 687 
 688     return start;
 689   }
 690 
 691 
 692   typedef enum {
 693     copy_forwards = 1,
 694     copy_backwards = -1
 695   } copy_direction;
 696 
 697   // Bulk copy of blocks of 8 words.
 698   //
 699   // count is a count of words.
 700   //
 701   // Precondition: count &gt;= 8
 702   //
 703   // Postconditions:
 704   //
 705   // The least significant bit of count contains the remaining count
 706   // of words to copy.  The rest of count is trash.
 707   //
 708   // s and d are adjusted to point to the remaining words to copy
 709   //
 710   void generate_copy_longs(Label &amp;start, Register s, Register d, Register count,
 711                            copy_direction direction) {
 712     int unit = wordSize * direction;
 713     int bias = (UseSIMDForMemoryOps ? 4:2) * wordSize;
 714 
 715     int offset;
 716     const Register t0 = r3, t1 = r4, t2 = r5, t3 = r6,
 717       t4 = r7, t5 = r10, t6 = r11, t7 = r12;
 718     const Register stride = r13;
 719 
 720     assert_different_registers(rscratch1, t0, t1, t2, t3, t4, t5, t6, t7);
 721     assert_different_registers(s, d, count, rscratch1);
 722 
 723     Label again, drain;
 724     const char *stub_name;
 725     if (direction == copy_forwards)
 726       stub_name = &quot;forward_copy_longs&quot;;
 727     else
 728       stub_name = &quot;backward_copy_longs&quot;;
 729 
 730     __ align(CodeEntryAlignment);
 731 
 732     StubCodeMark mark(this, &quot;StubRoutines&quot;, stub_name);
 733 
 734     __ bind(start);
 735 
 736     Label unaligned_copy_long;
 737     if (AvoidUnalignedAccesses) {
 738       __ tbnz(d, 3, unaligned_copy_long);
 739     }
 740 
 741     if (direction == copy_forwards) {
 742       __ sub(s, s, bias);
 743       __ sub(d, d, bias);
 744     }
 745 
 746 #ifdef ASSERT
 747     // Make sure we are never given &lt; 8 words
 748     {
 749       Label L;
 750       __ cmp(count, (u1)8);
 751       __ br(Assembler::GE, L);
 752       __ stop(&quot;genrate_copy_longs called with &lt; 8 words&quot;);
 753       __ bind(L);
 754     }
 755 #endif
 756 
 757     // Fill 8 registers
 758     if (UseSIMDForMemoryOps) {
 759       __ ldpq(v0, v1, Address(s, 4 * unit));
 760       __ ldpq(v2, v3, Address(__ pre(s, 8 * unit)));
 761     } else {
 762       __ ldp(t0, t1, Address(s, 2 * unit));
 763       __ ldp(t2, t3, Address(s, 4 * unit));
 764       __ ldp(t4, t5, Address(s, 6 * unit));
 765       __ ldp(t6, t7, Address(__ pre(s, 8 * unit)));
 766     }
 767 
 768     __ subs(count, count, 16);
 769     __ br(Assembler::LO, drain);
 770 
 771     int prefetch = PrefetchCopyIntervalInBytes;
 772     bool use_stride = false;
 773     if (direction == copy_backwards) {
 774        use_stride = prefetch &gt; 256;
 775        prefetch = -prefetch;
 776        if (use_stride) __ mov(stride, prefetch);
 777     }
 778 
 779     __ bind(again);
 780 
 781     if (PrefetchCopyIntervalInBytes &gt; 0)
 782       __ prfm(use_stride ? Address(s, stride) : Address(s, prefetch), PLDL1KEEP);
 783 
 784     if (UseSIMDForMemoryOps) {
 785       __ stpq(v0, v1, Address(d, 4 * unit));
 786       __ ldpq(v0, v1, Address(s, 4 * unit));
 787       __ stpq(v2, v3, Address(__ pre(d, 8 * unit)));
 788       __ ldpq(v2, v3, Address(__ pre(s, 8 * unit)));
 789     } else {
 790       __ stp(t0, t1, Address(d, 2 * unit));
 791       __ ldp(t0, t1, Address(s, 2 * unit));
 792       __ stp(t2, t3, Address(d, 4 * unit));
 793       __ ldp(t2, t3, Address(s, 4 * unit));
 794       __ stp(t4, t5, Address(d, 6 * unit));
 795       __ ldp(t4, t5, Address(s, 6 * unit));
 796       __ stp(t6, t7, Address(__ pre(d, 8 * unit)));
 797       __ ldp(t6, t7, Address(__ pre(s, 8 * unit)));
 798     }
 799 
 800     __ subs(count, count, 8);
 801     __ br(Assembler::HS, again);
 802 
 803     // Drain
 804     __ bind(drain);
 805     if (UseSIMDForMemoryOps) {
 806       __ stpq(v0, v1, Address(d, 4 * unit));
 807       __ stpq(v2, v3, Address(__ pre(d, 8 * unit)));
 808     } else {
 809       __ stp(t0, t1, Address(d, 2 * unit));
 810       __ stp(t2, t3, Address(d, 4 * unit));
 811       __ stp(t4, t5, Address(d, 6 * unit));
 812       __ stp(t6, t7, Address(__ pre(d, 8 * unit)));
 813     }
 814 
 815     {
 816       Label L1, L2;
 817       __ tbz(count, exact_log2(4), L1);
 818       if (UseSIMDForMemoryOps) {
 819         __ ldpq(v0, v1, Address(__ pre(s, 4 * unit)));
 820         __ stpq(v0, v1, Address(__ pre(d, 4 * unit)));
 821       } else {
 822         __ ldp(t0, t1, Address(s, 2 * unit));
 823         __ ldp(t2, t3, Address(__ pre(s, 4 * unit)));
 824         __ stp(t0, t1, Address(d, 2 * unit));
 825         __ stp(t2, t3, Address(__ pre(d, 4 * unit)));
 826       }
 827       __ bind(L1);
 828 
 829       if (direction == copy_forwards) {
 830         __ add(s, s, bias);
 831         __ add(d, d, bias);
 832       }
 833 
 834       __ tbz(count, 1, L2);
 835       __ ldp(t0, t1, Address(__ adjust(s, 2 * unit, direction == copy_backwards)));
 836       __ stp(t0, t1, Address(__ adjust(d, 2 * unit, direction == copy_backwards)));
 837       __ bind(L2);
 838     }
 839 
 840     __ ret(lr);
 841 
 842     if (AvoidUnalignedAccesses) {
 843       Label drain, again;
 844       // Register order for storing. Order is different for backward copy.
 845 
 846       __ bind(unaligned_copy_long);
 847 
 848       // source address is even aligned, target odd aligned
 849       //
 850       // when forward copying word pairs we read long pairs at offsets
 851       // {0, 2, 4, 6} (in long words). when backwards copying we read
 852       // long pairs at offsets {-2, -4, -6, -8}. We adjust the source
 853       // address by -2 in the forwards case so we can compute the
 854       // source offsets for both as {2, 4, 6, 8} * unit where unit = 1
 855       // or -1.
 856       //
 857       // when forward copying we need to store 1 word, 3 pairs and
 858       // then 1 word at offsets {0, 1, 3, 5, 7}. Rather thna use a
 859       // zero offset We adjust the destination by -1 which means we
 860       // have to use offsets { 1, 2, 4, 6, 8} * unit for the stores.
 861       //
 862       // When backwards copyng we need to store 1 word, 3 pairs and
 863       // then 1 word at offsets {-1, -3, -5, -7, -8} i.e. we use
 864       // offsets {1, 3, 5, 7, 8} * unit.
 865 
 866       if (direction == copy_forwards) {
 867         __ sub(s, s, 16);
 868         __ sub(d, d, 8);
 869       }
 870 
 871       // Fill 8 registers
 872       //
 873       // for forwards copy s was offset by -16 from the original input
 874       // value of s so the register contents are at these offsets
 875       // relative to the 64 bit block addressed by that original input
 876       // and so on for each successive 64 byte block when s is updated
 877       //
 878       // t0 at offset 0,  t1 at offset 8
 879       // t2 at offset 16, t3 at offset 24
 880       // t4 at offset 32, t5 at offset 40
 881       // t6 at offset 48, t7 at offset 56
 882 
 883       // for backwards copy s was not offset so the register contents
 884       // are at these offsets into the preceding 64 byte block
 885       // relative to that original input and so on for each successive
 886       // preceding 64 byte block when s is updated. this explains the
 887       // slightly counter-intuitive looking pattern of register usage
 888       // in the stp instructions for backwards copy.
 889       //
 890       // t0 at offset -16, t1 at offset -8
 891       // t2 at offset -32, t3 at offset -24
 892       // t4 at offset -48, t5 at offset -40
 893       // t6 at offset -64, t7 at offset -56
 894 
 895       __ ldp(t0, t1, Address(s, 2 * unit));
 896       __ ldp(t2, t3, Address(s, 4 * unit));
 897       __ ldp(t4, t5, Address(s, 6 * unit));
 898       __ ldp(t6, t7, Address(__ pre(s, 8 * unit)));
 899 
 900       __ subs(count, count, 16);
 901       __ br(Assembler::LO, drain);
 902 
 903       int prefetch = PrefetchCopyIntervalInBytes;
 904       bool use_stride = false;
 905       if (direction == copy_backwards) {
 906          use_stride = prefetch &gt; 256;
 907          prefetch = -prefetch;
 908          if (use_stride) __ mov(stride, prefetch);
 909       }
 910 
 911       __ bind(again);
 912 
 913       if (PrefetchCopyIntervalInBytes &gt; 0)
 914         __ prfm(use_stride ? Address(s, stride) : Address(s, prefetch), PLDL1KEEP);
 915 
 916       if (direction == copy_forwards) {
 917        // allowing for the offset of -8 the store instructions place
 918        // registers into the target 64 bit block at the following
 919        // offsets
 920        //
 921        // t0 at offset 0
 922        // t1 at offset 8,  t2 at offset 16
 923        // t3 at offset 24, t4 at offset 32
 924        // t5 at offset 40, t6 at offset 48
 925        // t7 at offset 56
 926 
 927         __ str(t0, Address(d, 1 * unit));
 928         __ stp(t1, t2, Address(d, 2 * unit));
 929         __ ldp(t0, t1, Address(s, 2 * unit));
 930         __ stp(t3, t4, Address(d, 4 * unit));
 931         __ ldp(t2, t3, Address(s, 4 * unit));
 932         __ stp(t5, t6, Address(d, 6 * unit));
 933         __ ldp(t4, t5, Address(s, 6 * unit));
 934         __ str(t7, Address(__ pre(d, 8 * unit)));
 935         __ ldp(t6, t7, Address(__ pre(s, 8 * unit)));
 936       } else {
 937        // d was not offset when we started so the registers are
 938        // written into the 64 bit block preceding d with the following
 939        // offsets
 940        //
 941        // t1 at offset -8
 942        // t3 at offset -24, t0 at offset -16
 943        // t5 at offset -48, t2 at offset -32
 944        // t7 at offset -56, t4 at offset -48
 945        //                   t6 at offset -64
 946        //
 947        // note that this matches the offsets previously noted for the
 948        // loads
 949 
 950         __ str(t1, Address(d, 1 * unit));
 951         __ stp(t3, t0, Address(d, 3 * unit));
 952         __ ldp(t0, t1, Address(s, 2 * unit));
 953         __ stp(t5, t2, Address(d, 5 * unit));
 954         __ ldp(t2, t3, Address(s, 4 * unit));
 955         __ stp(t7, t4, Address(d, 7 * unit));
 956         __ ldp(t4, t5, Address(s, 6 * unit));
 957         __ str(t6, Address(__ pre(d, 8 * unit)));
 958         __ ldp(t6, t7, Address(__ pre(s, 8 * unit)));
 959       }
 960 
 961       __ subs(count, count, 8);
 962       __ br(Assembler::HS, again);
 963 
 964       // Drain
 965       //
 966       // this uses the same pattern of offsets and register arguments
 967       // as above
 968       __ bind(drain);
 969       if (direction == copy_forwards) {
 970         __ str(t0, Address(d, 1 * unit));
 971         __ stp(t1, t2, Address(d, 2 * unit));
 972         __ stp(t3, t4, Address(d, 4 * unit));
 973         __ stp(t5, t6, Address(d, 6 * unit));
 974         __ str(t7, Address(__ pre(d, 8 * unit)));
 975       } else {
 976         __ str(t1, Address(d, 1 * unit));
 977         __ stp(t3, t0, Address(d, 3 * unit));
 978         __ stp(t5, t2, Address(d, 5 * unit));
 979         __ stp(t7, t4, Address(d, 7 * unit));
 980         __ str(t6, Address(__ pre(d, 8 * unit)));
 981       }
 982       // now we need to copy any remaining part block which may
 983       // include a 4 word block subblock and/or a 2 word subblock.
 984       // bits 2 and 1 in the count are the tell-tale for whetehr we
 985       // have each such subblock
 986       {
 987         Label L1, L2;
 988         __ tbz(count, exact_log2(4), L1);
 989        // this is the same as above but copying only 4 longs hence
 990        // with ony one intervening stp between the str instructions
 991        // but note that the offsets and registers still follow the
 992        // same pattern
 993         __ ldp(t0, t1, Address(s, 2 * unit));
 994         __ ldp(t2, t3, Address(__ pre(s, 4 * unit)));
 995         if (direction == copy_forwards) {
 996           __ str(t0, Address(d, 1 * unit));
 997           __ stp(t1, t2, Address(d, 2 * unit));
 998           __ str(t3, Address(__ pre(d, 4 * unit)));
 999         } else {
1000           __ str(t1, Address(d, 1 * unit));
1001           __ stp(t3, t0, Address(d, 3 * unit));
1002           __ str(t2, Address(__ pre(d, 4 * unit)));
1003         }
1004         __ bind(L1);
1005 
1006         __ tbz(count, 1, L2);
1007        // this is the same as above but copying only 2 longs hence
1008        // there is no intervening stp between the str instructions
1009        // but note that the offset and register patterns are still
1010        // the same
1011         __ ldp(t0, t1, Address(__ pre(s, 2 * unit)));
1012         if (direction == copy_forwards) {
1013           __ str(t0, Address(d, 1 * unit));
1014           __ str(t1, Address(__ pre(d, 2 * unit)));
1015         } else {
1016           __ str(t1, Address(d, 1 * unit));
1017           __ str(t0, Address(__ pre(d, 2 * unit)));
1018         }
1019         __ bind(L2);
1020 
1021        // for forwards copy we need to re-adjust the offsets we
1022        // applied so that s and d are follow the last words written
1023 
1024        if (direction == copy_forwards) {
1025          __ add(s, s, 16);
1026          __ add(d, d, 8);
1027        }
1028 
1029       }
1030 
1031       __ ret(lr);
1032       }
1033   }
1034 
1035   // Small copy: less than 16 bytes.
1036   //
1037   // NB: Ignores all of the bits of count which represent more than 15
1038   // bytes, so a caller doesn&#39;t have to mask them.
1039 
1040   void copy_memory_small(Register s, Register d, Register count, Register tmp, int step) {
1041     bool is_backwards = step &lt; 0;
1042     size_t granularity = uabs(step);
1043     int direction = is_backwards ? -1 : 1;
1044     int unit = wordSize * direction;
1045 
1046     Label Lword, Lint, Lshort, Lbyte;
1047 
1048     assert(granularity
1049            &amp;&amp; granularity &lt;= sizeof (jlong), &quot;Impossible granularity in copy_memory_small&quot;);
1050 
1051     const Register t0 = r3, t1 = r4, t2 = r5, t3 = r6;
1052 
1053     // ??? I don&#39;t know if this bit-test-and-branch is the right thing
1054     // to do.  It does a lot of jumping, resulting in several
1055     // mispredicted branches.  It might make more sense to do this
1056     // with something like Duff&#39;s device with a single computed branch.
1057 
1058     __ tbz(count, 3 - exact_log2(granularity), Lword);
1059     __ ldr(tmp, Address(__ adjust(s, unit, is_backwards)));
1060     __ str(tmp, Address(__ adjust(d, unit, is_backwards)));
1061     __ bind(Lword);
1062 
1063     if (granularity &lt;= sizeof (jint)) {
1064       __ tbz(count, 2 - exact_log2(granularity), Lint);
1065       __ ldrw(tmp, Address(__ adjust(s, sizeof (jint) * direction, is_backwards)));
1066       __ strw(tmp, Address(__ adjust(d, sizeof (jint) * direction, is_backwards)));
1067       __ bind(Lint);
1068     }
1069 
1070     if (granularity &lt;= sizeof (jshort)) {
1071       __ tbz(count, 1 - exact_log2(granularity), Lshort);
1072       __ ldrh(tmp, Address(__ adjust(s, sizeof (jshort) * direction, is_backwards)));
1073       __ strh(tmp, Address(__ adjust(d, sizeof (jshort) * direction, is_backwards)));
1074       __ bind(Lshort);
1075     }
1076 
1077     if (granularity &lt;= sizeof (jbyte)) {
1078       __ tbz(count, 0, Lbyte);
1079       __ ldrb(tmp, Address(__ adjust(s, sizeof (jbyte) * direction, is_backwards)));
1080       __ strb(tmp, Address(__ adjust(d, sizeof (jbyte) * direction, is_backwards)));
1081       __ bind(Lbyte);
1082     }
1083   }
1084 
1085   Label copy_f, copy_b;
1086 
1087   // All-singing all-dancing memory copy.
1088   //
1089   // Copy count units of memory from s to d.  The size of a unit is
1090   // step, which can be positive or negative depending on the direction
1091   // of copy.  If is_aligned is false, we align the source address.
1092   //
1093 
1094   void copy_memory(bool is_aligned, Register s, Register d,
1095                    Register count, Register tmp, int step) {
1096     copy_direction direction = step &lt; 0 ? copy_backwards : copy_forwards;
1097     bool is_backwards = step &lt; 0;
1098     int granularity = uabs(step);
1099     const Register t0 = r3, t1 = r4;
1100 
1101     // &lt;= 96 bytes do inline. Direction doesn&#39;t matter because we always
1102     // load all the data before writing anything
1103     Label copy4, copy8, copy16, copy32, copy80, copy_big, finish;
1104     const Register t2 = r5, t3 = r6, t4 = r7, t5 = r8;
1105     const Register t6 = r9, t7 = r10, t8 = r11, t9 = r12;
1106     const Register send = r17, dend = r18;
1107 
1108     if (PrefetchCopyIntervalInBytes &gt; 0)
1109       __ prfm(Address(s, 0), PLDL1KEEP);
1110     __ cmp(count, u1((UseSIMDForMemoryOps ? 96:80)/granularity));
1111     __ br(Assembler::HI, copy_big);
1112 
1113     __ lea(send, Address(s, count, Address::lsl(exact_log2(granularity))));
1114     __ lea(dend, Address(d, count, Address::lsl(exact_log2(granularity))));
1115 
1116     __ cmp(count, u1(16/granularity));
1117     __ br(Assembler::LS, copy16);
1118 
1119     __ cmp(count, u1(64/granularity));
1120     __ br(Assembler::HI, copy80);
1121 
1122     __ cmp(count, u1(32/granularity));
1123     __ br(Assembler::LS, copy32);
1124 
1125     // 33..64 bytes
1126     if (UseSIMDForMemoryOps) {
1127       __ ldpq(v0, v1, Address(s, 0));
1128       __ ldpq(v2, v3, Address(send, -32));
1129       __ stpq(v0, v1, Address(d, 0));
1130       __ stpq(v2, v3, Address(dend, -32));
1131     } else {
1132       __ ldp(t0, t1, Address(s, 0));
1133       __ ldp(t2, t3, Address(s, 16));
1134       __ ldp(t4, t5, Address(send, -32));
1135       __ ldp(t6, t7, Address(send, -16));
1136 
1137       __ stp(t0, t1, Address(d, 0));
1138       __ stp(t2, t3, Address(d, 16));
1139       __ stp(t4, t5, Address(dend, -32));
1140       __ stp(t6, t7, Address(dend, -16));
1141     }
1142     __ b(finish);
1143 
1144     // 17..32 bytes
1145     __ bind(copy32);
1146     __ ldp(t0, t1, Address(s, 0));
1147     __ ldp(t2, t3, Address(send, -16));
1148     __ stp(t0, t1, Address(d, 0));
1149     __ stp(t2, t3, Address(dend, -16));
1150     __ b(finish);
1151 
1152     // 65..80/96 bytes
1153     // (96 bytes if SIMD because we do 32 byes per instruction)
1154     __ bind(copy80);
1155     if (UseSIMDForMemoryOps) {
1156       __ ld4(v0, v1, v2, v3, __ T16B, Address(s, 0));
1157       __ ldpq(v4, v5, Address(send, -32));
1158       __ st4(v0, v1, v2, v3, __ T16B, Address(d, 0));
1159       __ stpq(v4, v5, Address(dend, -32));
1160     } else {
1161       __ ldp(t0, t1, Address(s, 0));
1162       __ ldp(t2, t3, Address(s, 16));
1163       __ ldp(t4, t5, Address(s, 32));
1164       __ ldp(t6, t7, Address(s, 48));
1165       __ ldp(t8, t9, Address(send, -16));
1166 
1167       __ stp(t0, t1, Address(d, 0));
1168       __ stp(t2, t3, Address(d, 16));
1169       __ stp(t4, t5, Address(d, 32));
1170       __ stp(t6, t7, Address(d, 48));
1171       __ stp(t8, t9, Address(dend, -16));
1172     }
1173     __ b(finish);
1174 
1175     // 0..16 bytes
1176     __ bind(copy16);
1177     __ cmp(count, u1(8/granularity));
1178     __ br(Assembler::LO, copy8);
1179 
1180     // 8..16 bytes
1181     __ ldr(t0, Address(s, 0));
1182     __ ldr(t1, Address(send, -8));
1183     __ str(t0, Address(d, 0));
1184     __ str(t1, Address(dend, -8));
1185     __ b(finish);
1186 
1187     if (granularity &lt; 8) {
1188       // 4..7 bytes
1189       __ bind(copy8);
1190       __ tbz(count, 2 - exact_log2(granularity), copy4);
1191       __ ldrw(t0, Address(s, 0));
1192       __ ldrw(t1, Address(send, -4));
1193       __ strw(t0, Address(d, 0));
1194       __ strw(t1, Address(dend, -4));
1195       __ b(finish);
1196       if (granularity &lt; 4) {
1197         // 0..3 bytes
1198         __ bind(copy4);
1199         __ cbz(count, finish); // get rid of 0 case
1200         if (granularity == 2) {
1201           __ ldrh(t0, Address(s, 0));
1202           __ strh(t0, Address(d, 0));
1203         } else { // granularity == 1
1204           // Now 1..3 bytes. Handle the 1 and 2 byte case by copying
1205           // the first and last byte.
1206           // Handle the 3 byte case by loading and storing base + count/2
1207           // (count == 1 (s+0)-&gt;(d+0), count == 2,3 (s+1) -&gt; (d+1))
1208           // This does means in the 1 byte case we load/store the same
1209           // byte 3 times.
1210           __ lsr(count, count, 1);
1211           __ ldrb(t0, Address(s, 0));
1212           __ ldrb(t1, Address(send, -1));
1213           __ ldrb(t2, Address(s, count));
1214           __ strb(t0, Address(d, 0));
1215           __ strb(t1, Address(dend, -1));
1216           __ strb(t2, Address(d, count));
1217         }
1218         __ b(finish);
1219       }
1220     }
1221 
1222     __ bind(copy_big);
1223     if (is_backwards) {
1224       __ lea(s, Address(s, count, Address::lsl(exact_log2(-step))));
1225       __ lea(d, Address(d, count, Address::lsl(exact_log2(-step))));
1226     }
1227 
1228     // Now we&#39;ve got the small case out of the way we can align the
1229     // source address on a 2-word boundary.
1230 
1231     Label aligned;
1232 
1233     if (is_aligned) {
1234       // We may have to adjust by 1 word to get s 2-word-aligned.
1235       __ tbz(s, exact_log2(wordSize), aligned);
1236       __ ldr(tmp, Address(__ adjust(s, direction * wordSize, is_backwards)));
1237       __ str(tmp, Address(__ adjust(d, direction * wordSize, is_backwards)));
1238       __ sub(count, count, wordSize/granularity);
1239     } else {
1240       if (is_backwards) {
1241         __ andr(rscratch2, s, 2 * wordSize - 1);
1242       } else {
1243         __ neg(rscratch2, s);
1244         __ andr(rscratch2, rscratch2, 2 * wordSize - 1);
1245       }
1246       // rscratch2 is the byte adjustment needed to align s.
1247       __ cbz(rscratch2, aligned);
1248       int shift = exact_log2(granularity);
1249       if (shift)  __ lsr(rscratch2, rscratch2, shift);
1250       __ sub(count, count, rscratch2);
1251 
1252 #if 0
1253       // ?? This code is only correct for a disjoint copy.  It may or
1254       // may not make sense to use it in that case.
1255 
1256       // Copy the first pair; s and d may not be aligned.
1257       __ ldp(t0, t1, Address(s, is_backwards ? -2 * wordSize : 0));
1258       __ stp(t0, t1, Address(d, is_backwards ? -2 * wordSize : 0));
1259 
1260       // Align s and d, adjust count
1261       if (is_backwards) {
1262         __ sub(s, s, rscratch2);
1263         __ sub(d, d, rscratch2);
1264       } else {
1265         __ add(s, s, rscratch2);
1266         __ add(d, d, rscratch2);
1267       }
1268 #else
1269       copy_memory_small(s, d, rscratch2, rscratch1, step);
1270 #endif
1271     }
1272 
1273     __ bind(aligned);
1274 
1275     // s is now 2-word-aligned.
1276 
1277     // We have a count of units and some trailing bytes.  Adjust the
1278     // count and do a bulk copy of words.
1279     __ lsr(rscratch2, count, exact_log2(wordSize/granularity));
1280     if (direction == copy_forwards)
1281       __ bl(copy_f);
1282     else
1283       __ bl(copy_b);
1284 
1285     // And the tail.
1286     copy_memory_small(s, d, count, tmp, step);
1287 
1288     if (granularity &gt;= 8) __ bind(copy8);
1289     if (granularity &gt;= 4) __ bind(copy4);
1290     __ bind(finish);
1291   }
1292 
1293 
1294   void clobber_registers() {
1295 #ifdef ASSERT
1296     __ mov(rscratch1, (uint64_t)0xdeadbeef);
1297     __ orr(rscratch1, rscratch1, rscratch1, Assembler::LSL, 32);
1298     for (Register r = r3; r &lt;= r18; r++)
1299       if (r != rscratch1) __ mov(r, rscratch1);
1300 #endif
1301   }
1302 
1303   // Scan over array at a for count oops, verifying each one.
1304   // Preserves a and count, clobbers rscratch1 and rscratch2.
1305   void verify_oop_array (size_t size, Register a, Register count, Register temp) {
1306     Label loop, end;
1307     __ mov(rscratch1, a);
1308     __ mov(rscratch2, zr);
1309     __ bind(loop);
1310     __ cmp(rscratch2, count);
1311     __ br(Assembler::HS, end);
1312     if (size == (size_t)wordSize) {
1313       __ ldr(temp, Address(a, rscratch2, Address::lsl(exact_log2(size))));
1314       __ verify_oop(temp);
1315     } else {
1316       __ ldrw(r16, Address(a, rscratch2, Address::lsl(exact_log2(size))));
1317       __ decode_heap_oop(temp); // calls verify_oop
1318     }
1319     __ add(rscratch2, rscratch2, size);
1320     __ b(loop);
1321     __ bind(end);
1322   }
1323 
1324   // Arguments:
1325   //   aligned - true =&gt; Input and output aligned on a HeapWord == 8-byte boundary
1326   //             ignored
1327   //   is_oop  - true =&gt; oop array, so generate store check code
1328   //   name    - stub name string
1329   //
1330   // Inputs:
1331   //   c_rarg0   - source array address
1332   //   c_rarg1   - destination array address
1333   //   c_rarg2   - element count, treated as ssize_t, can be zero
1334   //
1335   // If &#39;from&#39; and/or &#39;to&#39; are aligned on 4-byte boundaries, we let
1336   // the hardware handle it.  The two dwords within qwords that span
1337   // cache line boundaries will still be loaded and stored atomicly.
1338   //
1339   // Side Effects:
1340   //   disjoint_int_copy_entry is set to the no-overlap entry point
1341   //   used by generate_conjoint_int_oop_copy().
1342   //
1343   address generate_disjoint_copy(size_t size, bool aligned, bool is_oop, address *entry,
1344                                   const char *name, bool dest_uninitialized = false) {
1345     Register s = c_rarg0, d = c_rarg1, count = c_rarg2;
1346     RegSet saved_reg = RegSet::of(s, d, count);
1347     __ align(CodeEntryAlignment);
1348     StubCodeMark mark(this, &quot;StubRoutines&quot;, name);
1349     address start = __ pc();
1350     __ enter();
1351 
1352     if (entry != NULL) {
1353       *entry = __ pc();
1354       // caller can pass a 64-bit byte count here (from Unsafe.copyMemory)
1355       BLOCK_COMMENT(&quot;Entry:&quot;);
1356     }
1357 
1358     DecoratorSet decorators = IN_HEAP | IS_ARRAY | ARRAYCOPY_DISJOINT;
1359     if (dest_uninitialized) {
1360       decorators |= IS_DEST_UNINITIALIZED;
1361     }
1362     if (aligned) {
1363       decorators |= ARRAYCOPY_ALIGNED;
1364     }
1365 
1366     BarrierSetAssembler *bs = BarrierSet::barrier_set()-&gt;barrier_set_assembler();
1367     bs-&gt;arraycopy_prologue(_masm, decorators, is_oop, s, d, count, saved_reg);
1368 
1369     if (is_oop) {
1370       // save regs before copy_memory
1371       __ push(RegSet::of(d, count), sp);
1372     }
1373     {
1374       // UnsafeCopyMemory page error: continue after ucm
1375       bool add_entry = !is_oop &amp;&amp; (!aligned || sizeof(jlong) == size);
1376       UnsafeCopyMemoryMark ucmm(this, add_entry, true);
1377       copy_memory(aligned, s, d, count, rscratch1, size);
1378     }
1379 
1380     if (is_oop) {
1381       __ pop(RegSet::of(d, count), sp);
1382       if (VerifyOops)
1383         verify_oop_array(size, d, count, r16);
1384     }
1385 
1386     bs-&gt;arraycopy_epilogue(_masm, decorators, is_oop, d, count, rscratch1, RegSet());
1387 
1388     __ leave();
1389     __ mov(r0, zr); // return 0
1390     __ ret(lr);
1391     return start;
1392   }
1393 
1394   // Arguments:
1395   //   aligned - true =&gt; Input and output aligned on a HeapWord == 8-byte boundary
1396   //             ignored
1397   //   is_oop  - true =&gt; oop array, so generate store check code
1398   //   name    - stub name string
1399   //
1400   // Inputs:
1401   //   c_rarg0   - source array address
1402   //   c_rarg1   - destination array address
1403   //   c_rarg2   - element count, treated as ssize_t, can be zero
1404   //
1405   // If &#39;from&#39; and/or &#39;to&#39; are aligned on 4-byte boundaries, we let
1406   // the hardware handle it.  The two dwords within qwords that span
1407   // cache line boundaries will still be loaded and stored atomicly.
1408   //
1409   address generate_conjoint_copy(size_t size, bool aligned, bool is_oop, address nooverlap_target,
1410                                  address *entry, const char *name,
1411                                  bool dest_uninitialized = false) {
1412     Register s = c_rarg0, d = c_rarg1, count = c_rarg2;
1413     RegSet saved_regs = RegSet::of(s, d, count);
1414     StubCodeMark mark(this, &quot;StubRoutines&quot;, name);
1415     address start = __ pc();
1416     __ enter();
1417 
1418     if (entry != NULL) {
1419       *entry = __ pc();
1420       // caller can pass a 64-bit byte count here (from Unsafe.copyMemory)
1421       BLOCK_COMMENT(&quot;Entry:&quot;);
1422     }
1423 
1424     // use fwd copy when (d-s) above_equal (count*size)
1425     __ sub(rscratch1, d, s);
1426     __ cmp(rscratch1, count, Assembler::LSL, exact_log2(size));
1427     __ br(Assembler::HS, nooverlap_target);
1428 
1429     DecoratorSet decorators = IN_HEAP | IS_ARRAY;
1430     if (dest_uninitialized) {
1431       decorators |= IS_DEST_UNINITIALIZED;
1432     }
1433     if (aligned) {
1434       decorators |= ARRAYCOPY_ALIGNED;
1435     }
1436 
1437     BarrierSetAssembler *bs = BarrierSet::barrier_set()-&gt;barrier_set_assembler();
1438     bs-&gt;arraycopy_prologue(_masm, decorators, is_oop, s, d, count, saved_regs);
1439 
1440     if (is_oop) {
1441       // save regs before copy_memory
1442       __ push(RegSet::of(d, count), sp);
1443     }
1444     {
1445       // UnsafeCopyMemory page error: continue after ucm
1446       bool add_entry = !is_oop &amp;&amp; (!aligned || sizeof(jlong) == size);
1447       UnsafeCopyMemoryMark ucmm(this, add_entry, true);
1448       copy_memory(aligned, s, d, count, rscratch1, -size);
1449     }
1450     if (is_oop) {
1451       __ pop(RegSet::of(d, count), sp);
1452       if (VerifyOops)
1453         verify_oop_array(size, d, count, r16);
1454     }
1455     bs-&gt;arraycopy_epilogue(_masm, decorators, is_oop, d, count, rscratch1, RegSet());
1456     __ leave();
1457     __ mov(r0, zr); // return 0
1458     __ ret(lr);
1459     return start;
1460 }
1461 
1462   // Arguments:
1463   //   aligned - true =&gt; Input and output aligned on a HeapWord == 8-byte boundary
1464   //             ignored
1465   //   name    - stub name string
1466   //
1467   // Inputs:
1468   //   c_rarg0   - source array address
1469   //   c_rarg1   - destination array address
1470   //   c_rarg2   - element count, treated as ssize_t, can be zero
1471   //
1472   // If &#39;from&#39; and/or &#39;to&#39; are aligned on 4-, 2-, or 1-byte boundaries,
1473   // we let the hardware handle it.  The one to eight bytes within words,
1474   // dwords or qwords that span cache line boundaries will still be loaded
1475   // and stored atomically.
1476   //
1477   // Side Effects:
1478   //   disjoint_byte_copy_entry is set to the no-overlap entry point  //
1479   // If &#39;from&#39; and/or &#39;to&#39; are aligned on 4-, 2-, or 1-byte boundaries,
1480   // we let the hardware handle it.  The one to eight bytes within words,
1481   // dwords or qwords that span cache line boundaries will still be loaded
1482   // and stored atomically.
1483   //
1484   // Side Effects:
1485   //   disjoint_byte_copy_entry is set to the no-overlap entry point
1486   //   used by generate_conjoint_byte_copy().
1487   //
1488   address generate_disjoint_byte_copy(bool aligned, address* entry, const char *name) {
1489     const bool not_oop = false;
1490     return generate_disjoint_copy(sizeof (jbyte), aligned, not_oop, entry, name);
1491   }
1492 
1493   // Arguments:
1494   //   aligned - true =&gt; Input and output aligned on a HeapWord == 8-byte boundary
1495   //             ignored
1496   //   name    - stub name string
1497   //
1498   // Inputs:
1499   //   c_rarg0   - source array address
1500   //   c_rarg1   - destination array address
1501   //   c_rarg2   - element count, treated as ssize_t, can be zero
1502   //
1503   // If &#39;from&#39; and/or &#39;to&#39; are aligned on 4-, 2-, or 1-byte boundaries,
1504   // we let the hardware handle it.  The one to eight bytes within words,
1505   // dwords or qwords that span cache line boundaries will still be loaded
1506   // and stored atomically.
1507   //
1508   address generate_conjoint_byte_copy(bool aligned, address nooverlap_target,
1509                                       address* entry, const char *name) {
1510     const bool not_oop = false;
1511     return generate_conjoint_copy(sizeof (jbyte), aligned, not_oop, nooverlap_target, entry, name);
1512   }
1513 
1514   // Arguments:
1515   //   aligned - true =&gt; Input and output aligned on a HeapWord == 8-byte boundary
1516   //             ignored
1517   //   name    - stub name string
1518   //
1519   // Inputs:
1520   //   c_rarg0   - source array address
1521   //   c_rarg1   - destination array address
1522   //   c_rarg2   - element count, treated as ssize_t, can be zero
1523   //
1524   // If &#39;from&#39; and/or &#39;to&#39; are aligned on 4- or 2-byte boundaries, we
1525   // let the hardware handle it.  The two or four words within dwords
1526   // or qwords that span cache line boundaries will still be loaded
1527   // and stored atomically.
1528   //
1529   // Side Effects:
1530   //   disjoint_short_copy_entry is set to the no-overlap entry point
1531   //   used by generate_conjoint_short_copy().
1532   //
1533   address generate_disjoint_short_copy(bool aligned,
1534                                        address* entry, const char *name) {
1535     const bool not_oop = false;
1536     return generate_disjoint_copy(sizeof (jshort), aligned, not_oop, entry, name);
1537   }
1538 
1539   // Arguments:
1540   //   aligned - true =&gt; Input and output aligned on a HeapWord == 8-byte boundary
1541   //             ignored
1542   //   name    - stub name string
1543   //
1544   // Inputs:
1545   //   c_rarg0   - source array address
1546   //   c_rarg1   - destination array address
1547   //   c_rarg2   - element count, treated as ssize_t, can be zero
1548   //
1549   // If &#39;from&#39; and/or &#39;to&#39; are aligned on 4- or 2-byte boundaries, we
1550   // let the hardware handle it.  The two or four words within dwords
1551   // or qwords that span cache line boundaries will still be loaded
1552   // and stored atomically.
1553   //
1554   address generate_conjoint_short_copy(bool aligned, address nooverlap_target,
1555                                        address *entry, const char *name) {
1556     const bool not_oop = false;
1557     return generate_conjoint_copy(sizeof (jshort), aligned, not_oop, nooverlap_target, entry, name);
1558 
1559   }
1560   // Arguments:
1561   //   aligned - true =&gt; Input and output aligned on a HeapWord == 8-byte boundary
1562   //             ignored
1563   //   name    - stub name string
1564   //
1565   // Inputs:
1566   //   c_rarg0   - source array address
1567   //   c_rarg1   - destination array address
1568   //   c_rarg2   - element count, treated as ssize_t, can be zero
1569   //
1570   // If &#39;from&#39; and/or &#39;to&#39; are aligned on 4-byte boundaries, we let
1571   // the hardware handle it.  The two dwords within qwords that span
1572   // cache line boundaries will still be loaded and stored atomicly.
1573   //
1574   // Side Effects:
1575   //   disjoint_int_copy_entry is set to the no-overlap entry point
1576   //   used by generate_conjoint_int_oop_copy().
1577   //
1578   address generate_disjoint_int_copy(bool aligned, address *entry,
1579                                          const char *name, bool dest_uninitialized = false) {
1580     const bool not_oop = false;
1581     return generate_disjoint_copy(sizeof (jint), aligned, not_oop, entry, name);
1582   }
1583 
1584   // Arguments:
1585   //   aligned - true =&gt; Input and output aligned on a HeapWord == 8-byte boundary
1586   //             ignored
1587   //   name    - stub name string
1588   //
1589   // Inputs:
1590   //   c_rarg0   - source array address
1591   //   c_rarg1   - destination array address
1592   //   c_rarg2   - element count, treated as ssize_t, can be zero
1593   //
1594   // If &#39;from&#39; and/or &#39;to&#39; are aligned on 4-byte boundaries, we let
1595   // the hardware handle it.  The two dwords within qwords that span
1596   // cache line boundaries will still be loaded and stored atomicly.
1597   //
1598   address generate_conjoint_int_copy(bool aligned, address nooverlap_target,
1599                                      address *entry, const char *name,
1600                                      bool dest_uninitialized = false) {
1601     const bool not_oop = false;
1602     return generate_conjoint_copy(sizeof (jint), aligned, not_oop, nooverlap_target, entry, name);
1603   }
1604 
1605 
1606   // Arguments:
1607   //   aligned - true =&gt; Input and output aligned on a HeapWord boundary == 8 bytes
1608   //             ignored
1609   //   name    - stub name string
1610   //
1611   // Inputs:
1612   //   c_rarg0   - source array address
1613   //   c_rarg1   - destination array address
1614   //   c_rarg2   - element count, treated as size_t, can be zero
1615   //
1616   // Side Effects:
1617   //   disjoint_oop_copy_entry or disjoint_long_copy_entry is set to the
1618   //   no-overlap entry point used by generate_conjoint_long_oop_copy().
1619   //
1620   address generate_disjoint_long_copy(bool aligned, address *entry,
1621                                           const char *name, bool dest_uninitialized = false) {
1622     const bool not_oop = false;
1623     return generate_disjoint_copy(sizeof (jlong), aligned, not_oop, entry, name);
1624   }
1625 
1626   // Arguments:
1627   //   aligned - true =&gt; Input and output aligned on a HeapWord boundary == 8 bytes
1628   //             ignored
1629   //   name    - stub name string
1630   //
1631   // Inputs:
1632   //   c_rarg0   - source array address
1633   //   c_rarg1   - destination array address
1634   //   c_rarg2   - element count, treated as size_t, can be zero
1635   //
1636   address generate_conjoint_long_copy(bool aligned,
1637                                       address nooverlap_target, address *entry,
1638                                       const char *name, bool dest_uninitialized = false) {
1639     const bool not_oop = false;
1640     return generate_conjoint_copy(sizeof (jlong), aligned, not_oop, nooverlap_target, entry, name);
1641   }
1642 
1643   // Arguments:
1644   //   aligned - true =&gt; Input and output aligned on a HeapWord boundary == 8 bytes
1645   //             ignored
1646   //   name    - stub name string
1647   //
1648   // Inputs:
1649   //   c_rarg0   - source array address
1650   //   c_rarg1   - destination array address
1651   //   c_rarg2   - element count, treated as size_t, can be zero
1652   //
1653   // Side Effects:
1654   //   disjoint_oop_copy_entry or disjoint_long_copy_entry is set to the
1655   //   no-overlap entry point used by generate_conjoint_long_oop_copy().
1656   //
1657   address generate_disjoint_oop_copy(bool aligned, address *entry,
1658                                      const char *name, bool dest_uninitialized) {
1659     const bool is_oop = true;
1660     const size_t size = UseCompressedOops ? sizeof (jint) : sizeof (jlong);
1661     return generate_disjoint_copy(size, aligned, is_oop, entry, name, dest_uninitialized);
1662   }
1663 
1664   // Arguments:
1665   //   aligned - true =&gt; Input and output aligned on a HeapWord boundary == 8 bytes
1666   //             ignored
1667   //   name    - stub name string
1668   //
1669   // Inputs:
1670   //   c_rarg0   - source array address
1671   //   c_rarg1   - destination array address
1672   //   c_rarg2   - element count, treated as size_t, can be zero
1673   //
1674   address generate_conjoint_oop_copy(bool aligned,
1675                                      address nooverlap_target, address *entry,
1676                                      const char *name, bool dest_uninitialized) {
1677     const bool is_oop = true;
1678     const size_t size = UseCompressedOops ? sizeof (jint) : sizeof (jlong);
1679     return generate_conjoint_copy(size, aligned, is_oop, nooverlap_target, entry,
1680                                   name, dest_uninitialized);
1681   }
1682 
1683 
1684   // Helper for generating a dynamic type check.
1685   // Smashes rscratch1, rscratch2.
1686   void generate_type_check(Register sub_klass,
1687                            Register super_check_offset,
1688                            Register super_klass,
1689                            Label&amp; L_success) {
1690     assert_different_registers(sub_klass, super_check_offset, super_klass);
1691 
1692     BLOCK_COMMENT(&quot;type_check:&quot;);
1693 
1694     Label L_miss;
1695 
1696     __ check_klass_subtype_fast_path(sub_klass, super_klass, noreg,        &amp;L_success, &amp;L_miss, NULL,
1697                                      super_check_offset);
1698     __ check_klass_subtype_slow_path(sub_klass, super_klass, noreg, noreg, &amp;L_success, NULL);
1699 
1700     // Fall through on failure!
1701     __ BIND(L_miss);
1702   }
1703 
1704   //
1705   //  Generate checkcasting array copy stub
1706   //
1707   //  Input:
1708   //    c_rarg0   - source array address
1709   //    c_rarg1   - destination array address
1710   //    c_rarg2   - element count, treated as ssize_t, can be zero
1711   //    c_rarg3   - size_t ckoff (super_check_offset)
1712   //    c_rarg4   - oop ckval (super_klass)
1713   //
1714   //  Output:
1715   //    r0 ==  0  -  success
1716   //    r0 == -1^K - failure, where K is partial transfer count
1717   //
1718   address generate_checkcast_copy(const char *name, address *entry,
1719                                   bool dest_uninitialized = false) {
1720 
1721     Label L_load_element, L_store_element, L_do_card_marks, L_done, L_done_pop;
1722 
1723     // Input registers (after setup_arg_regs)
1724     const Register from        = c_rarg0;   // source array address
1725     const Register to          = c_rarg1;   // destination array address
1726     const Register count       = c_rarg2;   // elementscount
1727     const Register ckoff       = c_rarg3;   // super_check_offset
1728     const Register ckval       = c_rarg4;   // super_klass
1729 
1730     RegSet wb_pre_saved_regs = RegSet::range(c_rarg0, c_rarg4);
1731     RegSet wb_post_saved_regs = RegSet::of(count);
1732 
1733     // Registers used as temps (r18, r19, r20 are save-on-entry)
1734     const Register count_save  = r21;       // orig elementscount
1735     const Register start_to    = r20;       // destination array start address
1736     const Register copied_oop  = r18;       // actual oop copied
1737     const Register r19_klass   = r19;       // oop._klass
1738 
1739     //---------------------------------------------------------------
1740     // Assembler stub will be used for this call to arraycopy
1741     // if the two arrays are subtypes of Object[] but the
1742     // destination array type is not equal to or a supertype
1743     // of the source type.  Each element must be separately
1744     // checked.
1745 
1746     assert_different_registers(from, to, count, ckoff, ckval, start_to,
1747                                copied_oop, r19_klass, count_save);
1748 
1749     __ align(CodeEntryAlignment);
1750     StubCodeMark mark(this, &quot;StubRoutines&quot;, name);
1751     address start = __ pc();
1752 
1753     __ enter(); // required for proper stackwalking of RuntimeStub frame
1754 
1755 #ifdef ASSERT
1756     // caller guarantees that the arrays really are different
1757     // otherwise, we would have to make conjoint checks
1758     { Label L;
1759       array_overlap_test(L, TIMES_OOP);
1760       __ stop(&quot;checkcast_copy within a single array&quot;);
1761       __ bind(L);
1762     }
1763 #endif //ASSERT
1764 
1765     // Caller of this entry point must set up the argument registers.
1766     if (entry != NULL) {
1767       *entry = __ pc();
1768       BLOCK_COMMENT(&quot;Entry:&quot;);
1769     }
1770 
1771      // Empty array:  Nothing to do.
1772     __ cbz(count, L_done);
1773 
1774     __ push(RegSet::of(r18, r19, r20, r21), sp);
1775 
1776 #ifdef ASSERT
1777     BLOCK_COMMENT(&quot;assert consistent ckoff/ckval&quot;);
1778     // The ckoff and ckval must be mutually consistent,
1779     // even though caller generates both.
1780     { Label L;
1781       int sco_offset = in_bytes(Klass::super_check_offset_offset());
1782       __ ldrw(start_to, Address(ckval, sco_offset));
1783       __ cmpw(ckoff, start_to);
1784       __ br(Assembler::EQ, L);
1785       __ stop(&quot;super_check_offset inconsistent&quot;);
1786       __ bind(L);
1787     }
1788 #endif //ASSERT
1789 
1790     DecoratorSet decorators = IN_HEAP | IS_ARRAY | ARRAYCOPY_CHECKCAST | ARRAYCOPY_DISJOINT;
1791     bool is_oop = true;
1792     if (dest_uninitialized) {
1793       decorators |= IS_DEST_UNINITIALIZED;
1794     }
1795 
1796     BarrierSetAssembler *bs = BarrierSet::barrier_set()-&gt;barrier_set_assembler();
1797     bs-&gt;arraycopy_prologue(_masm, decorators, is_oop, from, to, count, wb_pre_saved_regs);
1798 
1799     // save the original count
1800     __ mov(count_save, count);
1801 
1802     // Copy from low to high addresses
1803     __ mov(start_to, to);              // Save destination array start address
1804     __ b(L_load_element);
1805 
1806     // ======== begin loop ========
1807     // (Loop is rotated; its entry is L_load_element.)
1808     // Loop control:
1809     //   for (; count != 0; count--) {
1810     //     copied_oop = load_heap_oop(from++);
1811     //     ... generate_type_check ...;
1812     //     store_heap_oop(to++, copied_oop);
1813     //   }
1814     __ align(OptoLoopAlignment);
1815 
1816     __ BIND(L_store_element);
1817     __ store_heap_oop(__ post(to, UseCompressedOops ? 4 : 8), copied_oop, noreg, noreg, noreg, AS_RAW);  // store the oop
1818     __ sub(count, count, 1);
1819     __ cbz(count, L_do_card_marks);
1820 
1821     // ======== loop entry is here ========
1822     __ BIND(L_load_element);
1823     __ load_heap_oop(copied_oop, __ post(from, UseCompressedOops ? 4 : 8), noreg, noreg, AS_RAW); // load the oop
1824     __ cbz(copied_oop, L_store_element);
1825 
1826     __ load_klass(r19_klass, copied_oop);// query the object klass
1827     generate_type_check(r19_klass, ckoff, ckval, L_store_element);
1828     // ======== end loop ========
1829 
1830     // It was a real error; we must depend on the caller to finish the job.
1831     // Register count = remaining oops, count_orig = total oops.
1832     // Emit GC store barriers for the oops we have copied and report
1833     // their number to the caller.
1834 
1835     __ subs(count, count_save, count);     // K = partially copied oop count
1836     __ eon(count, count, zr);                   // report (-1^K) to caller
1837     __ br(Assembler::EQ, L_done_pop);
1838 
1839     __ BIND(L_do_card_marks);
1840     bs-&gt;arraycopy_epilogue(_masm, decorators, is_oop, start_to, count_save, rscratch1, wb_post_saved_regs);
1841 
1842     __ bind(L_done_pop);
1843     __ pop(RegSet::of(r18, r19, r20, r21), sp);
1844     inc_counter_np(SharedRuntime::_checkcast_array_copy_ctr);
1845 
1846     __ bind(L_done);
1847     __ mov(r0, count);
1848     __ leave();
1849     __ ret(lr);
1850 
1851     return start;
1852   }
1853 
1854   // Perform range checks on the proposed arraycopy.
1855   // Kills temp, but nothing else.
1856   // Also, clean the sign bits of src_pos and dst_pos.
1857   void arraycopy_range_checks(Register src,     // source array oop (c_rarg0)
1858                               Register src_pos, // source position (c_rarg1)
1859                               Register dst,     // destination array oo (c_rarg2)
1860                               Register dst_pos, // destination position (c_rarg3)
1861                               Register length,
1862                               Register temp,
1863                               Label&amp; L_failed) {
1864     BLOCK_COMMENT(&quot;arraycopy_range_checks:&quot;);
1865 
1866     assert_different_registers(rscratch1, temp);
1867 
1868     //  if (src_pos + length &gt; arrayOop(src)-&gt;length())  FAIL;
1869     __ ldrw(rscratch1, Address(src, arrayOopDesc::length_offset_in_bytes()));
1870     __ addw(temp, length, src_pos);
1871     __ cmpw(temp, rscratch1);
1872     __ br(Assembler::HI, L_failed);
1873 
1874     //  if (dst_pos + length &gt; arrayOop(dst)-&gt;length())  FAIL;
1875     __ ldrw(rscratch1, Address(dst, arrayOopDesc::length_offset_in_bytes()));
1876     __ addw(temp, length, dst_pos);
1877     __ cmpw(temp, rscratch1);
1878     __ br(Assembler::HI, L_failed);
1879 
1880     // Have to clean up high 32 bits of &#39;src_pos&#39; and &#39;dst_pos&#39;.
1881     __ movw(src_pos, src_pos);
1882     __ movw(dst_pos, dst_pos);
1883 
1884     BLOCK_COMMENT(&quot;arraycopy_range_checks done&quot;);
1885   }
1886 
1887   // These stubs get called from some dumb test routine.
1888   // I&#39;ll write them properly when they&#39;re called from
1889   // something that&#39;s actually doing something.
1890   static void fake_arraycopy_stub(address src, address dst, int count) {
1891     assert(count == 0, &quot;huh?&quot;);
1892   }
1893 
1894 
1895   //
1896   //  Generate &#39;unsafe&#39; array copy stub
1897   //  Though just as safe as the other stubs, it takes an unscaled
1898   //  size_t argument instead of an element count.
1899   //
1900   //  Input:
1901   //    c_rarg0   - source array address
1902   //    c_rarg1   - destination array address
1903   //    c_rarg2   - byte count, treated as ssize_t, can be zero
1904   //
1905   // Examines the alignment of the operands and dispatches
1906   // to a long, int, short, or byte copy loop.
1907   //
1908   address generate_unsafe_copy(const char *name,
1909                                address byte_copy_entry,
1910                                address short_copy_entry,
1911                                address int_copy_entry,
1912                                address long_copy_entry) {
1913     Label L_long_aligned, L_int_aligned, L_short_aligned;
1914     Register s = c_rarg0, d = c_rarg1, count = c_rarg2;
1915 
1916     __ align(CodeEntryAlignment);
1917     StubCodeMark mark(this, &quot;StubRoutines&quot;, name);
1918     address start = __ pc();
1919     __ enter(); // required for proper stackwalking of RuntimeStub frame
1920 
1921     // bump this on entry, not on exit:
1922     inc_counter_np(SharedRuntime::_unsafe_array_copy_ctr);
1923 
1924     __ orr(rscratch1, s, d);
1925     __ orr(rscratch1, rscratch1, count);
1926 
1927     __ andr(rscratch1, rscratch1, BytesPerLong-1);
1928     __ cbz(rscratch1, L_long_aligned);
1929     __ andr(rscratch1, rscratch1, BytesPerInt-1);
1930     __ cbz(rscratch1, L_int_aligned);
1931     __ tbz(rscratch1, 0, L_short_aligned);
1932     __ b(RuntimeAddress(byte_copy_entry));
1933 
1934     __ BIND(L_short_aligned);
1935     __ lsr(count, count, LogBytesPerShort);  // size =&gt; short_count
1936     __ b(RuntimeAddress(short_copy_entry));
1937     __ BIND(L_int_aligned);
1938     __ lsr(count, count, LogBytesPerInt);    // size =&gt; int_count
1939     __ b(RuntimeAddress(int_copy_entry));
1940     __ BIND(L_long_aligned);
1941     __ lsr(count, count, LogBytesPerLong);   // size =&gt; long_count
1942     __ b(RuntimeAddress(long_copy_entry));
1943 
1944     return start;
1945   }
1946 
1947   //
1948   //  Generate generic array copy stubs
1949   //
1950   //  Input:
1951   //    c_rarg0    -  src oop
1952   //    c_rarg1    -  src_pos (32-bits)
1953   //    c_rarg2    -  dst oop
1954   //    c_rarg3    -  dst_pos (32-bits)
1955   //    c_rarg4    -  element count (32-bits)
1956   //
1957   //  Output:
1958   //    r0 ==  0  -  success
1959   //    r0 == -1^K - failure, where K is partial transfer count
1960   //
1961   address generate_generic_copy(const char *name,
1962                                 address byte_copy_entry, address short_copy_entry,
1963                                 address int_copy_entry, address oop_copy_entry,
1964                                 address long_copy_entry, address checkcast_copy_entry) {
1965 
1966     Label L_failed, L_objArray;
1967     Label L_copy_bytes, L_copy_shorts, L_copy_ints, L_copy_longs;
1968 
1969     // Input registers
1970     const Register src        = c_rarg0;  // source array oop
1971     const Register src_pos    = c_rarg1;  // source position
1972     const Register dst        = c_rarg2;  // destination array oop
1973     const Register dst_pos    = c_rarg3;  // destination position
1974     const Register length     = c_rarg4;
1975 
1976 
1977     // Registers used as temps
1978     const Register dst_klass  = c_rarg5;
1979 
1980     __ align(CodeEntryAlignment);
1981 
1982     StubCodeMark mark(this, &quot;StubRoutines&quot;, name);
1983 
1984     address start = __ pc();
1985 
1986     __ enter(); // required for proper stackwalking of RuntimeStub frame
1987 
1988     // bump this on entry, not on exit:
1989     inc_counter_np(SharedRuntime::_generic_array_copy_ctr);
1990 
1991     //-----------------------------------------------------------------------
1992     // Assembler stub will be used for this call to arraycopy
1993     // if the following conditions are met:
1994     //
1995     // (1) src and dst must not be null.
1996     // (2) src_pos must not be negative.
1997     // (3) dst_pos must not be negative.
1998     // (4) length  must not be negative.
1999     // (5) src klass and dst klass should be the same and not NULL.
2000     // (6) src and dst should be arrays.
2001     // (7) src_pos + length must not exceed length of src.
2002     // (8) dst_pos + length must not exceed length of dst.
2003     //
2004 
2005     //  if (src == NULL) return -1;
2006     __ cbz(src, L_failed);
2007 
2008     //  if (src_pos &lt; 0) return -1;
2009     __ tbnz(src_pos, 31, L_failed);  // i.e. sign bit set
2010 
2011     //  if (dst == NULL) return -1;
2012     __ cbz(dst, L_failed);
2013 
2014     //  if (dst_pos &lt; 0) return -1;
2015     __ tbnz(dst_pos, 31, L_failed);  // i.e. sign bit set
2016 
2017     // registers used as temp
2018     const Register scratch_length    = r16; // elements count to copy
2019     const Register scratch_src_klass = r17; // array klass
2020     const Register lh                = r18; // layout helper
2021 
2022     //  if (length &lt; 0) return -1;
2023     __ movw(scratch_length, length);        // length (elements count, 32-bits value)
2024     __ tbnz(scratch_length, 31, L_failed);  // i.e. sign bit set
2025 
2026     __ load_klass(scratch_src_klass, src);
2027 #ifdef ASSERT
2028     //  assert(src-&gt;klass() != NULL);
2029     {
2030       BLOCK_COMMENT(&quot;assert klasses not null {&quot;);
2031       Label L1, L2;
2032       __ cbnz(scratch_src_klass, L2);   // it is broken if klass is NULL
2033       __ bind(L1);
2034       __ stop(&quot;broken null klass&quot;);
2035       __ bind(L2);
2036       __ load_klass(rscratch1, dst);
2037       __ cbz(rscratch1, L1);     // this would be broken also
2038       BLOCK_COMMENT(&quot;} assert klasses not null done&quot;);
2039     }
2040 #endif
2041 
2042     // Load layout helper (32-bits)
2043     //
2044     //  |array_tag|     | header_size | element_type |     |log2_element_size|
2045     // 32        30    24            16              8     2                 0
2046     //
2047     //   array_tag: typeArray = 0x3, objArray = 0x2, non-array = 0x0
2048     //
2049 
2050     const int lh_offset = in_bytes(Klass::layout_helper_offset());
2051 
2052     // Handle objArrays completely differently...
2053     const jint objArray_lh = Klass::array_layout_helper(T_OBJECT);
2054     __ ldrw(lh, Address(scratch_src_klass, lh_offset));
2055     __ movw(rscratch1, objArray_lh);
2056     __ eorw(rscratch2, lh, rscratch1);
2057     __ cbzw(rscratch2, L_objArray);
2058 
2059     //  if (src-&gt;klass() != dst-&gt;klass()) return -1;
2060     __ load_klass(rscratch2, dst);
2061     __ eor(rscratch2, rscratch2, scratch_src_klass);
2062     __ cbnz(rscratch2, L_failed);
2063 
2064     //  if (!src-&gt;is_Array()) return -1;
2065     __ tbz(lh, 31, L_failed);  // i.e. (lh &gt;= 0)
2066 
2067     // At this point, it is known to be a typeArray (array_tag 0x3).
2068 #ifdef ASSERT
2069     {
2070       BLOCK_COMMENT(&quot;assert primitive array {&quot;);
2071       Label L;
2072       __ movw(rscratch2, Klass::_lh_array_tag_type_value &lt;&lt; Klass::_lh_array_tag_shift);
2073       __ cmpw(lh, rscratch2);
2074       __ br(Assembler::GE, L);
2075       __ stop(&quot;must be a primitive array&quot;);
2076       __ bind(L);
2077       BLOCK_COMMENT(&quot;} assert primitive array done&quot;);
2078     }
2079 #endif
2080 
2081     arraycopy_range_checks(src, src_pos, dst, dst_pos, scratch_length,
2082                            rscratch2, L_failed);
2083 
2084     // TypeArrayKlass
2085     //
2086     // src_addr = (src + array_header_in_bytes()) + (src_pos &lt;&lt; log2elemsize);
2087     // dst_addr = (dst + array_header_in_bytes()) + (dst_pos &lt;&lt; log2elemsize);
2088     //
2089 
2090     const Register rscratch1_offset = rscratch1;    // array offset
2091     const Register r18_elsize = lh; // element size
2092 
2093     __ ubfx(rscratch1_offset, lh, Klass::_lh_header_size_shift,
2094            exact_log2(Klass::_lh_header_size_mask+1));   // array_offset
2095     __ add(src, src, rscratch1_offset);           // src array offset
2096     __ add(dst, dst, rscratch1_offset);           // dst array offset
2097     BLOCK_COMMENT(&quot;choose copy loop based on element size&quot;);
2098 
2099     // next registers should be set before the jump to corresponding stub
2100     const Register from     = c_rarg0;  // source array address
2101     const Register to       = c_rarg1;  // destination array address
2102     const Register count    = c_rarg2;  // elements count
2103 
2104     // &#39;from&#39;, &#39;to&#39;, &#39;count&#39; registers should be set in such order
2105     // since they are the same as &#39;src&#39;, &#39;src_pos&#39;, &#39;dst&#39;.
2106 
2107     assert(Klass::_lh_log2_element_size_shift == 0, &quot;fix this code&quot;);
2108 
2109     // The possible values of elsize are 0-3, i.e. exact_log2(element
2110     // size in bytes).  We do a simple bitwise binary search.
2111   __ BIND(L_copy_bytes);
2112     __ tbnz(r18_elsize, 1, L_copy_ints);
2113     __ tbnz(r18_elsize, 0, L_copy_shorts);
2114     __ lea(from, Address(src, src_pos));// src_addr
2115     __ lea(to,   Address(dst, dst_pos));// dst_addr
2116     __ movw(count, scratch_length); // length
2117     __ b(RuntimeAddress(byte_copy_entry));
2118 
2119   __ BIND(L_copy_shorts);
2120     __ lea(from, Address(src, src_pos, Address::lsl(1)));// src_addr
2121     __ lea(to,   Address(dst, dst_pos, Address::lsl(1)));// dst_addr
2122     __ movw(count, scratch_length); // length
2123     __ b(RuntimeAddress(short_copy_entry));
2124 
2125   __ BIND(L_copy_ints);
2126     __ tbnz(r18_elsize, 0, L_copy_longs);
2127     __ lea(from, Address(src, src_pos, Address::lsl(2)));// src_addr
2128     __ lea(to,   Address(dst, dst_pos, Address::lsl(2)));// dst_addr
2129     __ movw(count, scratch_length); // length
2130     __ b(RuntimeAddress(int_copy_entry));
2131 
2132   __ BIND(L_copy_longs);
2133 #ifdef ASSERT
2134     {
2135       BLOCK_COMMENT(&quot;assert long copy {&quot;);
2136       Label L;
2137       __ andw(lh, lh, Klass::_lh_log2_element_size_mask); // lh -&gt; r18_elsize
2138       __ cmpw(r18_elsize, LogBytesPerLong);
2139       __ br(Assembler::EQ, L);
2140       __ stop(&quot;must be long copy, but elsize is wrong&quot;);
2141       __ bind(L);
2142       BLOCK_COMMENT(&quot;} assert long copy done&quot;);
2143     }
2144 #endif
2145     __ lea(from, Address(src, src_pos, Address::lsl(3)));// src_addr
2146     __ lea(to,   Address(dst, dst_pos, Address::lsl(3)));// dst_addr
2147     __ movw(count, scratch_length); // length
2148     __ b(RuntimeAddress(long_copy_entry));
2149 
2150     // ObjArrayKlass
2151   __ BIND(L_objArray);
2152     // live at this point:  scratch_src_klass, scratch_length, src[_pos], dst[_pos]
2153 
2154     Label L_plain_copy, L_checkcast_copy;
2155     //  test array classes for subtyping
2156     __ load_klass(r18, dst);
2157     __ cmp(scratch_src_klass, r18); // usual case is exact equality
2158     __ br(Assembler::NE, L_checkcast_copy);
2159 
2160     // Identically typed arrays can be copied without element-wise checks.
2161     arraycopy_range_checks(src, src_pos, dst, dst_pos, scratch_length,
2162                            rscratch2, L_failed);
2163 
2164     __ lea(from, Address(src, src_pos, Address::lsl(LogBytesPerHeapOop)));
2165     __ add(from, from, arrayOopDesc::base_offset_in_bytes(T_OBJECT));
2166     __ lea(to, Address(dst, dst_pos, Address::lsl(LogBytesPerHeapOop)));
2167     __ add(to, to, arrayOopDesc::base_offset_in_bytes(T_OBJECT));
2168     __ movw(count, scratch_length); // length
2169   __ BIND(L_plain_copy);
2170     __ b(RuntimeAddress(oop_copy_entry));
2171 
2172   __ BIND(L_checkcast_copy);
2173     // live at this point:  scratch_src_klass, scratch_length, r18 (dst_klass)
2174     {
2175       // Before looking at dst.length, make sure dst is also an objArray.
2176       __ ldrw(rscratch1, Address(r18, lh_offset));
2177       __ movw(rscratch2, objArray_lh);
2178       __ eorw(rscratch1, rscratch1, rscratch2);
2179       __ cbnzw(rscratch1, L_failed);
2180 
2181       // It is safe to examine both src.length and dst.length.
2182       arraycopy_range_checks(src, src_pos, dst, dst_pos, scratch_length,
2183                              r18, L_failed);
2184 
2185       __ load_klass(dst_klass, dst); // reload
2186 
2187       // Marshal the base address arguments now, freeing registers.
2188       __ lea(from, Address(src, src_pos, Address::lsl(LogBytesPerHeapOop)));
2189       __ add(from, from, arrayOopDesc::base_offset_in_bytes(T_OBJECT));
2190       __ lea(to, Address(dst, dst_pos, Address::lsl(LogBytesPerHeapOop)));
2191       __ add(to, to, arrayOopDesc::base_offset_in_bytes(T_OBJECT));
2192       __ movw(count, length);           // length (reloaded)
2193       Register sco_temp = c_rarg3;      // this register is free now
2194       assert_different_registers(from, to, count, sco_temp,
2195                                  dst_klass, scratch_src_klass);
2196       // assert_clean_int(count, sco_temp);
2197 
2198       // Generate the type check.
2199       const int sco_offset = in_bytes(Klass::super_check_offset_offset());
2200       __ ldrw(sco_temp, Address(dst_klass, sco_offset));
2201 
2202       // Smashes rscratch1, rscratch2
2203       generate_type_check(scratch_src_klass, sco_temp, dst_klass, L_plain_copy);
2204 
2205       // Fetch destination element klass from the ObjArrayKlass header.
2206       int ek_offset = in_bytes(ObjArrayKlass::element_klass_offset());
2207       __ ldr(dst_klass, Address(dst_klass, ek_offset));
2208       __ ldrw(sco_temp, Address(dst_klass, sco_offset));
2209 
2210       // the checkcast_copy loop needs two extra arguments:
2211       assert(c_rarg3 == sco_temp, &quot;#3 already in place&quot;);
2212       // Set up arguments for checkcast_copy_entry.
2213       __ mov(c_rarg4, dst_klass);  // dst.klass.element_klass
2214       __ b(RuntimeAddress(checkcast_copy_entry));
2215     }
2216 
2217   __ BIND(L_failed);
2218     __ mov(r0, -1);
2219     __ leave();   // required for proper stackwalking of RuntimeStub frame
2220     __ ret(lr);
2221 
2222     return start;
2223   }
2224 
2225   //
2226   // Generate stub for array fill. If &quot;aligned&quot; is true, the
2227   // &quot;to&quot; address is assumed to be heapword aligned.
2228   //
2229   // Arguments for generated stub:
2230   //   to:    c_rarg0
2231   //   value: c_rarg1
2232   //   count: c_rarg2 treated as signed
2233   //
2234   address generate_fill(BasicType t, bool aligned, const char *name) {
2235     __ align(CodeEntryAlignment);
2236     StubCodeMark mark(this, &quot;StubRoutines&quot;, name);
2237     address start = __ pc();
2238 
2239     BLOCK_COMMENT(&quot;Entry:&quot;);
2240 
2241     const Register to        = c_rarg0;  // source array address
2242     const Register value     = c_rarg1;  // value
2243     const Register count     = c_rarg2;  // elements count
2244 
2245     const Register bz_base = r10;        // base for block_zero routine
2246     const Register cnt_words = r11;      // temp register
2247 
2248     __ enter();
2249 
2250     Label L_fill_elements, L_exit1;
2251 
2252     int shift = -1;
2253     switch (t) {
2254       case T_BYTE:
2255         shift = 0;
2256         __ cmpw(count, 8 &gt;&gt; shift); // Short arrays (&lt; 8 bytes) fill by element
2257         __ bfi(value, value, 8, 8);   // 8 bit -&gt; 16 bit
2258         __ bfi(value, value, 16, 16); // 16 bit -&gt; 32 bit
2259         __ br(Assembler::LO, L_fill_elements);
2260         break;
2261       case T_SHORT:
2262         shift = 1;
2263         __ cmpw(count, 8 &gt;&gt; shift); // Short arrays (&lt; 8 bytes) fill by element
2264         __ bfi(value, value, 16, 16); // 16 bit -&gt; 32 bit
2265         __ br(Assembler::LO, L_fill_elements);
2266         break;
2267       case T_INT:
2268         shift = 2;
2269         __ cmpw(count, 8 &gt;&gt; shift); // Short arrays (&lt; 8 bytes) fill by element
2270         __ br(Assembler::LO, L_fill_elements);
2271         break;
2272       default: ShouldNotReachHere();
2273     }
2274 
2275     // Align source address at 8 bytes address boundary.
2276     Label L_skip_align1, L_skip_align2, L_skip_align4;
2277     if (!aligned) {
2278       switch (t) {
2279         case T_BYTE:
2280           // One byte misalignment happens only for byte arrays.
2281           __ tbz(to, 0, L_skip_align1);
2282           __ strb(value, Address(__ post(to, 1)));
2283           __ subw(count, count, 1);
2284           __ bind(L_skip_align1);
2285           // Fallthrough
2286         case T_SHORT:
2287           // Two bytes misalignment happens only for byte and short (char) arrays.
2288           __ tbz(to, 1, L_skip_align2);
2289           __ strh(value, Address(__ post(to, 2)));
2290           __ subw(count, count, 2 &gt;&gt; shift);
2291           __ bind(L_skip_align2);
2292           // Fallthrough
2293         case T_INT:
2294           // Align to 8 bytes, we know we are 4 byte aligned to start.
2295           __ tbz(to, 2, L_skip_align4);
2296           __ strw(value, Address(__ post(to, 4)));
2297           __ subw(count, count, 4 &gt;&gt; shift);
2298           __ bind(L_skip_align4);
2299           break;
2300         default: ShouldNotReachHere();
2301       }
2302     }
2303 
2304     //
2305     //  Fill large chunks
2306     //
2307     __ lsrw(cnt_words, count, 3 - shift); // number of words
2308     __ bfi(value, value, 32, 32);         // 32 bit -&gt; 64 bit
2309     __ subw(count, count, cnt_words, Assembler::LSL, 3 - shift);
2310     if (UseBlockZeroing) {
2311       Label non_block_zeroing, rest;
2312       // If the fill value is zero we can use the fast zero_words().
2313       __ cbnz(value, non_block_zeroing);
2314       __ mov(bz_base, to);
2315       __ add(to, to, cnt_words, Assembler::LSL, LogBytesPerWord);
2316       __ zero_words(bz_base, cnt_words);
2317       __ b(rest);
2318       __ bind(non_block_zeroing);
2319       __ fill_words(to, cnt_words, value);
2320       __ bind(rest);
2321     } else {
2322       __ fill_words(to, cnt_words, value);
2323     }
2324 
2325     // Remaining count is less than 8 bytes. Fill it by a single store.
2326     // Note that the total length is no less than 8 bytes.
2327     if (t == T_BYTE || t == T_SHORT) {
2328       Label L_exit1;
2329       __ cbzw(count, L_exit1);
2330       __ add(to, to, count, Assembler::LSL, shift); // points to the end
2331       __ str(value, Address(to, -8));    // overwrite some elements
2332       __ bind(L_exit1);
2333       __ leave();
2334       __ ret(lr);
2335     }
2336 
2337     // Handle copies less than 8 bytes.
2338     Label L_fill_2, L_fill_4, L_exit2;
2339     __ bind(L_fill_elements);
2340     switch (t) {
2341       case T_BYTE:
2342         __ tbz(count, 0, L_fill_2);
2343         __ strb(value, Address(__ post(to, 1)));
2344         __ bind(L_fill_2);
2345         __ tbz(count, 1, L_fill_4);
2346         __ strh(value, Address(__ post(to, 2)));
2347         __ bind(L_fill_4);
2348         __ tbz(count, 2, L_exit2);
2349         __ strw(value, Address(to));
2350         break;
2351       case T_SHORT:
2352         __ tbz(count, 0, L_fill_4);
2353         __ strh(value, Address(__ post(to, 2)));
2354         __ bind(L_fill_4);
2355         __ tbz(count, 1, L_exit2);
2356         __ strw(value, Address(to));
2357         break;
2358       case T_INT:
2359         __ cbzw(count, L_exit2);
2360         __ strw(value, Address(to));
2361         break;
2362       default: ShouldNotReachHere();
2363     }
2364     __ bind(L_exit2);
2365     __ leave();
2366     __ ret(lr);
2367     return start;
2368   }
2369 
2370   address generate_data_cache_writeback() {
2371     const Register line        = c_rarg0;  // address of line to write back
2372 
2373     __ align(CodeEntryAlignment);
2374 
2375     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;_data_cache_writeback&quot;);
2376 
2377     address start = __ pc();
2378     __ enter();
2379     __ cache_wb(Address(line, 0));
2380     __ leave();
2381     __ ret(lr);
2382 
2383     return start;
2384   }
2385 
2386   address generate_data_cache_writeback_sync() {
2387     const Register is_pre     = c_rarg0;  // pre or post sync
2388 
2389     __ align(CodeEntryAlignment);
2390 
2391     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;_data_cache_writeback_sync&quot;);
2392 
2393     // pre wbsync is a no-op
2394     // post wbsync translates to an sfence
2395 
2396     Label skip;
2397     address start = __ pc();
2398     __ enter();
2399     __ cbnz(is_pre, skip);
2400     __ cache_wbsync(false);
2401     __ bind(skip);
2402     __ leave();
2403     __ ret(lr);
2404 
2405     return start;
2406   }
2407 
2408   void generate_arraycopy_stubs() {
2409     address entry;
2410     address entry_jbyte_arraycopy;
2411     address entry_jshort_arraycopy;
2412     address entry_jint_arraycopy;
2413     address entry_oop_arraycopy;
2414     address entry_jlong_arraycopy;
2415     address entry_checkcast_arraycopy;
2416 
2417     generate_copy_longs(copy_f, r0, r1, rscratch2, copy_forwards);
2418     generate_copy_longs(copy_b, r0, r1, rscratch2, copy_backwards);
2419 
2420     StubRoutines::aarch64::_zero_blocks = generate_zero_blocks();
2421 
2422     //*** jbyte
2423     // Always need aligned and unaligned versions
2424     StubRoutines::_jbyte_disjoint_arraycopy         = generate_disjoint_byte_copy(false, &amp;entry,
2425                                                                                   &quot;jbyte_disjoint_arraycopy&quot;);
2426     StubRoutines::_jbyte_arraycopy                  = generate_conjoint_byte_copy(false, entry,
2427                                                                                   &amp;entry_jbyte_arraycopy,
2428                                                                                   &quot;jbyte_arraycopy&quot;);
2429     StubRoutines::_arrayof_jbyte_disjoint_arraycopy = generate_disjoint_byte_copy(true, &amp;entry,
2430                                                                                   &quot;arrayof_jbyte_disjoint_arraycopy&quot;);
2431     StubRoutines::_arrayof_jbyte_arraycopy          = generate_conjoint_byte_copy(true, entry, NULL,
2432                                                                                   &quot;arrayof_jbyte_arraycopy&quot;);
2433 
2434     //*** jshort
2435     // Always need aligned and unaligned versions
2436     StubRoutines::_jshort_disjoint_arraycopy         = generate_disjoint_short_copy(false, &amp;entry,
2437                                                                                     &quot;jshort_disjoint_arraycopy&quot;);
2438     StubRoutines::_jshort_arraycopy                  = generate_conjoint_short_copy(false, entry,
2439                                                                                     &amp;entry_jshort_arraycopy,
2440                                                                                     &quot;jshort_arraycopy&quot;);
2441     StubRoutines::_arrayof_jshort_disjoint_arraycopy = generate_disjoint_short_copy(true, &amp;entry,
2442                                                                                     &quot;arrayof_jshort_disjoint_arraycopy&quot;);
2443     StubRoutines::_arrayof_jshort_arraycopy          = generate_conjoint_short_copy(true, entry, NULL,
2444                                                                                     &quot;arrayof_jshort_arraycopy&quot;);
2445 
2446     //*** jint
2447     // Aligned versions
2448     StubRoutines::_arrayof_jint_disjoint_arraycopy = generate_disjoint_int_copy(true, &amp;entry,
2449                                                                                 &quot;arrayof_jint_disjoint_arraycopy&quot;);
2450     StubRoutines::_arrayof_jint_arraycopy          = generate_conjoint_int_copy(true, entry, &amp;entry_jint_arraycopy,
2451                                                                                 &quot;arrayof_jint_arraycopy&quot;);
2452     // In 64 bit we need both aligned and unaligned versions of jint arraycopy.
2453     // entry_jint_arraycopy always points to the unaligned version
2454     StubRoutines::_jint_disjoint_arraycopy         = generate_disjoint_int_copy(false, &amp;entry,
2455                                                                                 &quot;jint_disjoint_arraycopy&quot;);
2456     StubRoutines::_jint_arraycopy                  = generate_conjoint_int_copy(false, entry,
2457                                                                                 &amp;entry_jint_arraycopy,
2458                                                                                 &quot;jint_arraycopy&quot;);
2459 
2460     //*** jlong
2461     // It is always aligned
2462     StubRoutines::_arrayof_jlong_disjoint_arraycopy = generate_disjoint_long_copy(true, &amp;entry,
2463                                                                                   &quot;arrayof_jlong_disjoint_arraycopy&quot;);
2464     StubRoutines::_arrayof_jlong_arraycopy          = generate_conjoint_long_copy(true, entry, &amp;entry_jlong_arraycopy,
2465                                                                                   &quot;arrayof_jlong_arraycopy&quot;);
2466     StubRoutines::_jlong_disjoint_arraycopy         = StubRoutines::_arrayof_jlong_disjoint_arraycopy;
2467     StubRoutines::_jlong_arraycopy                  = StubRoutines::_arrayof_jlong_arraycopy;
2468 
2469     //*** oops
2470     {
2471       // With compressed oops we need unaligned versions; notice that
2472       // we overwrite entry_oop_arraycopy.
2473       bool aligned = !UseCompressedOops;
2474 
2475       StubRoutines::_arrayof_oop_disjoint_arraycopy
2476         = generate_disjoint_oop_copy(aligned, &amp;entry, &quot;arrayof_oop_disjoint_arraycopy&quot;,
2477                                      /*dest_uninitialized*/false);
2478       StubRoutines::_arrayof_oop_arraycopy
2479         = generate_conjoint_oop_copy(aligned, entry, &amp;entry_oop_arraycopy, &quot;arrayof_oop_arraycopy&quot;,
2480                                      /*dest_uninitialized*/false);
2481       // Aligned versions without pre-barriers
2482       StubRoutines::_arrayof_oop_disjoint_arraycopy_uninit
2483         = generate_disjoint_oop_copy(aligned, &amp;entry, &quot;arrayof_oop_disjoint_arraycopy_uninit&quot;,
2484                                      /*dest_uninitialized*/true);
2485       StubRoutines::_arrayof_oop_arraycopy_uninit
2486         = generate_conjoint_oop_copy(aligned, entry, NULL, &quot;arrayof_oop_arraycopy_uninit&quot;,
2487                                      /*dest_uninitialized*/true);
2488     }
2489 
2490     StubRoutines::_oop_disjoint_arraycopy            = StubRoutines::_arrayof_oop_disjoint_arraycopy;
2491     StubRoutines::_oop_arraycopy                     = StubRoutines::_arrayof_oop_arraycopy;
2492     StubRoutines::_oop_disjoint_arraycopy_uninit     = StubRoutines::_arrayof_oop_disjoint_arraycopy_uninit;
2493     StubRoutines::_oop_arraycopy_uninit              = StubRoutines::_arrayof_oop_arraycopy_uninit;
2494 
2495     StubRoutines::_checkcast_arraycopy        = generate_checkcast_copy(&quot;checkcast_arraycopy&quot;, &amp;entry_checkcast_arraycopy);
2496     StubRoutines::_checkcast_arraycopy_uninit = generate_checkcast_copy(&quot;checkcast_arraycopy_uninit&quot;, NULL,
2497                                                                         /*dest_uninitialized*/true);
2498 
2499     StubRoutines::_unsafe_arraycopy    = generate_unsafe_copy(&quot;unsafe_arraycopy&quot;,
2500                                                               entry_jbyte_arraycopy,
2501                                                               entry_jshort_arraycopy,
2502                                                               entry_jint_arraycopy,
2503                                                               entry_jlong_arraycopy);
2504 
2505     StubRoutines::_generic_arraycopy   = generate_generic_copy(&quot;generic_arraycopy&quot;,
2506                                                                entry_jbyte_arraycopy,
2507                                                                entry_jshort_arraycopy,
2508                                                                entry_jint_arraycopy,
2509                                                                entry_oop_arraycopy,
2510                                                                entry_jlong_arraycopy,
2511                                                                entry_checkcast_arraycopy);
2512 
2513     StubRoutines::_jbyte_fill = generate_fill(T_BYTE, false, &quot;jbyte_fill&quot;);
2514     StubRoutines::_jshort_fill = generate_fill(T_SHORT, false, &quot;jshort_fill&quot;);
2515     StubRoutines::_jint_fill = generate_fill(T_INT, false, &quot;jint_fill&quot;);
2516     StubRoutines::_arrayof_jbyte_fill = generate_fill(T_BYTE, true, &quot;arrayof_jbyte_fill&quot;);
2517     StubRoutines::_arrayof_jshort_fill = generate_fill(T_SHORT, true, &quot;arrayof_jshort_fill&quot;);
2518     StubRoutines::_arrayof_jint_fill = generate_fill(T_INT, true, &quot;arrayof_jint_fill&quot;);
2519   }
2520 
2521   void generate_math_stubs() { Unimplemented(); }
2522 
2523   // Arguments:
2524   //
2525   // Inputs:
2526   //   c_rarg0   - source byte array address
2527   //   c_rarg1   - destination byte array address
2528   //   c_rarg2   - K (key) in little endian int array
2529   //
2530   address generate_aescrypt_encryptBlock() {
2531     __ align(CodeEntryAlignment);
2532     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;aescrypt_encryptBlock&quot;);
2533 
2534     Label L_doLast;
2535 
2536     const Register from        = c_rarg0;  // source array address
2537     const Register to          = c_rarg1;  // destination array address
2538     const Register key         = c_rarg2;  // key array address
2539     const Register keylen      = rscratch1;
2540 
2541     address start = __ pc();
2542     __ enter();
2543 
2544     __ ldrw(keylen, Address(key, arrayOopDesc::length_offset_in_bytes() - arrayOopDesc::base_offset_in_bytes(T_INT)));
2545 
2546     __ ld1(v0, __ T16B, from); // get 16 bytes of input
2547 
2548     __ ld1(v1, v2, v3, v4, __ T16B, __ post(key, 64));
2549     __ rev32(v1, __ T16B, v1);
2550     __ rev32(v2, __ T16B, v2);
2551     __ rev32(v3, __ T16B, v3);
2552     __ rev32(v4, __ T16B, v4);
2553     __ aese(v0, v1);
2554     __ aesmc(v0, v0);
2555     __ aese(v0, v2);
2556     __ aesmc(v0, v0);
2557     __ aese(v0, v3);
2558     __ aesmc(v0, v0);
2559     __ aese(v0, v4);
2560     __ aesmc(v0, v0);
2561 
2562     __ ld1(v1, v2, v3, v4, __ T16B, __ post(key, 64));
2563     __ rev32(v1, __ T16B, v1);
2564     __ rev32(v2, __ T16B, v2);
2565     __ rev32(v3, __ T16B, v3);
2566     __ rev32(v4, __ T16B, v4);
2567     __ aese(v0, v1);
2568     __ aesmc(v0, v0);
2569     __ aese(v0, v2);
2570     __ aesmc(v0, v0);
2571     __ aese(v0, v3);
2572     __ aesmc(v0, v0);
2573     __ aese(v0, v4);
2574     __ aesmc(v0, v0);
2575 
2576     __ ld1(v1, v2, __ T16B, __ post(key, 32));
2577     __ rev32(v1, __ T16B, v1);
2578     __ rev32(v2, __ T16B, v2);
2579 
2580     __ cmpw(keylen, 44);
2581     __ br(Assembler::EQ, L_doLast);
2582 
2583     __ aese(v0, v1);
2584     __ aesmc(v0, v0);
2585     __ aese(v0, v2);
2586     __ aesmc(v0, v0);
2587 
2588     __ ld1(v1, v2, __ T16B, __ post(key, 32));
2589     __ rev32(v1, __ T16B, v1);
2590     __ rev32(v2, __ T16B, v2);
2591 
2592     __ cmpw(keylen, 52);
2593     __ br(Assembler::EQ, L_doLast);
2594 
2595     __ aese(v0, v1);
2596     __ aesmc(v0, v0);
2597     __ aese(v0, v2);
2598     __ aesmc(v0, v0);
2599 
2600     __ ld1(v1, v2, __ T16B, __ post(key, 32));
2601     __ rev32(v1, __ T16B, v1);
2602     __ rev32(v2, __ T16B, v2);
2603 
2604     __ BIND(L_doLast);
2605 
2606     __ aese(v0, v1);
2607     __ aesmc(v0, v0);
2608     __ aese(v0, v2);
2609 
2610     __ ld1(v1, __ T16B, key);
2611     __ rev32(v1, __ T16B, v1);
2612     __ eor(v0, __ T16B, v0, v1);
2613 
2614     __ st1(v0, __ T16B, to);
2615 
2616     __ mov(r0, 0);
2617 
2618     __ leave();
2619     __ ret(lr);
2620 
2621     return start;
2622   }
2623 
2624   // Arguments:
2625   //
2626   // Inputs:
2627   //   c_rarg0   - source byte array address
2628   //   c_rarg1   - destination byte array address
2629   //   c_rarg2   - K (key) in little endian int array
2630   //
2631   address generate_aescrypt_decryptBlock() {
2632     assert(UseAES, &quot;need AES instructions and misaligned SSE support&quot;);
2633     __ align(CodeEntryAlignment);
2634     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;aescrypt_decryptBlock&quot;);
2635     Label L_doLast;
2636 
2637     const Register from        = c_rarg0;  // source array address
2638     const Register to          = c_rarg1;  // destination array address
2639     const Register key         = c_rarg2;  // key array address
2640     const Register keylen      = rscratch1;
2641 
2642     address start = __ pc();
2643     __ enter(); // required for proper stackwalking of RuntimeStub frame
2644 
2645     __ ldrw(keylen, Address(key, arrayOopDesc::length_offset_in_bytes() - arrayOopDesc::base_offset_in_bytes(T_INT)));
2646 
2647     __ ld1(v0, __ T16B, from); // get 16 bytes of input
2648 
2649     __ ld1(v5, __ T16B, __ post(key, 16));
2650     __ rev32(v5, __ T16B, v5);
2651 
2652     __ ld1(v1, v2, v3, v4, __ T16B, __ post(key, 64));
2653     __ rev32(v1, __ T16B, v1);
2654     __ rev32(v2, __ T16B, v2);
2655     __ rev32(v3, __ T16B, v3);
2656     __ rev32(v4, __ T16B, v4);
2657     __ aesd(v0, v1);
2658     __ aesimc(v0, v0);
2659     __ aesd(v0, v2);
2660     __ aesimc(v0, v0);
2661     __ aesd(v0, v3);
2662     __ aesimc(v0, v0);
2663     __ aesd(v0, v4);
2664     __ aesimc(v0, v0);
2665 
2666     __ ld1(v1, v2, v3, v4, __ T16B, __ post(key, 64));
2667     __ rev32(v1, __ T16B, v1);
2668     __ rev32(v2, __ T16B, v2);
2669     __ rev32(v3, __ T16B, v3);
2670     __ rev32(v4, __ T16B, v4);
2671     __ aesd(v0, v1);
2672     __ aesimc(v0, v0);
2673     __ aesd(v0, v2);
2674     __ aesimc(v0, v0);
2675     __ aesd(v0, v3);
2676     __ aesimc(v0, v0);
2677     __ aesd(v0, v4);
2678     __ aesimc(v0, v0);
2679 
2680     __ ld1(v1, v2, __ T16B, __ post(key, 32));
2681     __ rev32(v1, __ T16B, v1);
2682     __ rev32(v2, __ T16B, v2);
2683 
2684     __ cmpw(keylen, 44);
2685     __ br(Assembler::EQ, L_doLast);
2686 
2687     __ aesd(v0, v1);
2688     __ aesimc(v0, v0);
2689     __ aesd(v0, v2);
2690     __ aesimc(v0, v0);
2691 
2692     __ ld1(v1, v2, __ T16B, __ post(key, 32));
2693     __ rev32(v1, __ T16B, v1);
2694     __ rev32(v2, __ T16B, v2);
2695 
2696     __ cmpw(keylen, 52);
2697     __ br(Assembler::EQ, L_doLast);
2698 
2699     __ aesd(v0, v1);
2700     __ aesimc(v0, v0);
2701     __ aesd(v0, v2);
2702     __ aesimc(v0, v0);
2703 
2704     __ ld1(v1, v2, __ T16B, __ post(key, 32));
2705     __ rev32(v1, __ T16B, v1);
2706     __ rev32(v2, __ T16B, v2);
2707 
2708     __ BIND(L_doLast);
2709 
2710     __ aesd(v0, v1);
2711     __ aesimc(v0, v0);
2712     __ aesd(v0, v2);
2713 
2714     __ eor(v0, __ T16B, v0, v5);
2715 
2716     __ st1(v0, __ T16B, to);
2717 
2718     __ mov(r0, 0);
2719 
2720     __ leave();
2721     __ ret(lr);
2722 
2723     return start;
2724   }
2725 
2726   // Arguments:
2727   //
2728   // Inputs:
2729   //   c_rarg0   - source byte array address
2730   //   c_rarg1   - destination byte array address
2731   //   c_rarg2   - K (key) in little endian int array
2732   //   c_rarg3   - r vector byte array address
2733   //   c_rarg4   - input length
2734   //
2735   // Output:
2736   //   x0        - input length
2737   //
2738   address generate_cipherBlockChaining_encryptAESCrypt() {
2739     assert(UseAES, &quot;need AES instructions and misaligned SSE support&quot;);
2740     __ align(CodeEntryAlignment);
2741     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;cipherBlockChaining_encryptAESCrypt&quot;);
2742 
2743     Label L_loadkeys_44, L_loadkeys_52, L_aes_loop, L_rounds_44, L_rounds_52;
2744 
2745     const Register from        = c_rarg0;  // source array address
2746     const Register to          = c_rarg1;  // destination array address
2747     const Register key         = c_rarg2;  // key array address
2748     const Register rvec        = c_rarg3;  // r byte array initialized from initvector array address
2749                                            // and left with the results of the last encryption block
2750     const Register len_reg     = c_rarg4;  // src len (must be multiple of blocksize 16)
2751     const Register keylen      = rscratch1;
2752 
2753     address start = __ pc();
2754 
2755       __ enter();
2756 
2757       __ movw(rscratch2, len_reg);
2758 
2759       __ ldrw(keylen, Address(key, arrayOopDesc::length_offset_in_bytes() - arrayOopDesc::base_offset_in_bytes(T_INT)));
2760 
2761       __ ld1(v0, __ T16B, rvec);
2762 
2763       __ cmpw(keylen, 52);
2764       __ br(Assembler::CC, L_loadkeys_44);
2765       __ br(Assembler::EQ, L_loadkeys_52);
2766 
2767       __ ld1(v17, v18, __ T16B, __ post(key, 32));
2768       __ rev32(v17, __ T16B, v17);
2769       __ rev32(v18, __ T16B, v18);
2770     __ BIND(L_loadkeys_52);
2771       __ ld1(v19, v20, __ T16B, __ post(key, 32));
2772       __ rev32(v19, __ T16B, v19);
2773       __ rev32(v20, __ T16B, v20);
2774     __ BIND(L_loadkeys_44);
2775       __ ld1(v21, v22, v23, v24, __ T16B, __ post(key, 64));
2776       __ rev32(v21, __ T16B, v21);
2777       __ rev32(v22, __ T16B, v22);
2778       __ rev32(v23, __ T16B, v23);
2779       __ rev32(v24, __ T16B, v24);
2780       __ ld1(v25, v26, v27, v28, __ T16B, __ post(key, 64));
2781       __ rev32(v25, __ T16B, v25);
2782       __ rev32(v26, __ T16B, v26);
2783       __ rev32(v27, __ T16B, v27);
2784       __ rev32(v28, __ T16B, v28);
2785       __ ld1(v29, v30, v31, __ T16B, key);
2786       __ rev32(v29, __ T16B, v29);
2787       __ rev32(v30, __ T16B, v30);
2788       __ rev32(v31, __ T16B, v31);
2789 
2790     __ BIND(L_aes_loop);
2791       __ ld1(v1, __ T16B, __ post(from, 16));
2792       __ eor(v0, __ T16B, v0, v1);
2793 
2794       __ br(Assembler::CC, L_rounds_44);
2795       __ br(Assembler::EQ, L_rounds_52);
2796 
2797       __ aese(v0, v17); __ aesmc(v0, v0);
2798       __ aese(v0, v18); __ aesmc(v0, v0);
2799     __ BIND(L_rounds_52);
2800       __ aese(v0, v19); __ aesmc(v0, v0);
2801       __ aese(v0, v20); __ aesmc(v0, v0);
2802     __ BIND(L_rounds_44);
2803       __ aese(v0, v21); __ aesmc(v0, v0);
2804       __ aese(v0, v22); __ aesmc(v0, v0);
2805       __ aese(v0, v23); __ aesmc(v0, v0);
2806       __ aese(v0, v24); __ aesmc(v0, v0);
2807       __ aese(v0, v25); __ aesmc(v0, v0);
2808       __ aese(v0, v26); __ aesmc(v0, v0);
2809       __ aese(v0, v27); __ aesmc(v0, v0);
2810       __ aese(v0, v28); __ aesmc(v0, v0);
2811       __ aese(v0, v29); __ aesmc(v0, v0);
2812       __ aese(v0, v30);
2813       __ eor(v0, __ T16B, v0, v31);
2814 
2815       __ st1(v0, __ T16B, __ post(to, 16));
2816 
2817       __ subw(len_reg, len_reg, 16);
2818       __ cbnzw(len_reg, L_aes_loop);
2819 
2820       __ st1(v0, __ T16B, rvec);
2821 
2822       __ mov(r0, rscratch2);
2823 
2824       __ leave();
2825       __ ret(lr);
2826 
2827       return start;
2828   }
2829 
2830   // Arguments:
2831   //
2832   // Inputs:
2833   //   c_rarg0   - source byte array address
2834   //   c_rarg1   - destination byte array address
2835   //   c_rarg2   - K (key) in little endian int array
2836   //   c_rarg3   - r vector byte array address
2837   //   c_rarg4   - input length
2838   //
2839   // Output:
2840   //   r0        - input length
2841   //
2842   address generate_cipherBlockChaining_decryptAESCrypt() {
2843     assert(UseAES, &quot;need AES instructions and misaligned SSE support&quot;);
2844     __ align(CodeEntryAlignment);
2845     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;cipherBlockChaining_decryptAESCrypt&quot;);
2846 
2847     Label L_loadkeys_44, L_loadkeys_52, L_aes_loop, L_rounds_44, L_rounds_52;
2848 
2849     const Register from        = c_rarg0;  // source array address
2850     const Register to          = c_rarg1;  // destination array address
2851     const Register key         = c_rarg2;  // key array address
2852     const Register rvec        = c_rarg3;  // r byte array initialized from initvector array address
2853                                            // and left with the results of the last encryption block
2854     const Register len_reg     = c_rarg4;  // src len (must be multiple of blocksize 16)
2855     const Register keylen      = rscratch1;
2856 
2857     address start = __ pc();
2858 
2859       __ enter();
2860 
2861       __ movw(rscratch2, len_reg);
2862 
2863       __ ldrw(keylen, Address(key, arrayOopDesc::length_offset_in_bytes() - arrayOopDesc::base_offset_in_bytes(T_INT)));
2864 
2865       __ ld1(v2, __ T16B, rvec);
2866 
2867       __ ld1(v31, __ T16B, __ post(key, 16));
2868       __ rev32(v31, __ T16B, v31);
2869 
2870       __ cmpw(keylen, 52);
2871       __ br(Assembler::CC, L_loadkeys_44);
2872       __ br(Assembler::EQ, L_loadkeys_52);
2873 
2874       __ ld1(v17, v18, __ T16B, __ post(key, 32));
2875       __ rev32(v17, __ T16B, v17);
2876       __ rev32(v18, __ T16B, v18);
2877     __ BIND(L_loadkeys_52);
2878       __ ld1(v19, v20, __ T16B, __ post(key, 32));
2879       __ rev32(v19, __ T16B, v19);
2880       __ rev32(v20, __ T16B, v20);
2881     __ BIND(L_loadkeys_44);
2882       __ ld1(v21, v22, v23, v24, __ T16B, __ post(key, 64));
2883       __ rev32(v21, __ T16B, v21);
2884       __ rev32(v22, __ T16B, v22);
2885       __ rev32(v23, __ T16B, v23);
2886       __ rev32(v24, __ T16B, v24);
2887       __ ld1(v25, v26, v27, v28, __ T16B, __ post(key, 64));
2888       __ rev32(v25, __ T16B, v25);
2889       __ rev32(v26, __ T16B, v26);
2890       __ rev32(v27, __ T16B, v27);
2891       __ rev32(v28, __ T16B, v28);
2892       __ ld1(v29, v30, __ T16B, key);
2893       __ rev32(v29, __ T16B, v29);
2894       __ rev32(v30, __ T16B, v30);
2895 
2896     __ BIND(L_aes_loop);
2897       __ ld1(v0, __ T16B, __ post(from, 16));
2898       __ orr(v1, __ T16B, v0, v0);
2899 
2900       __ br(Assembler::CC, L_rounds_44);
2901       __ br(Assembler::EQ, L_rounds_52);
2902 
2903       __ aesd(v0, v17); __ aesimc(v0, v0);
2904       __ aesd(v0, v18); __ aesimc(v0, v0);
2905     __ BIND(L_rounds_52);
2906       __ aesd(v0, v19); __ aesimc(v0, v0);
2907       __ aesd(v0, v20); __ aesimc(v0, v0);
2908     __ BIND(L_rounds_44);
2909       __ aesd(v0, v21); __ aesimc(v0, v0);
2910       __ aesd(v0, v22); __ aesimc(v0, v0);
2911       __ aesd(v0, v23); __ aesimc(v0, v0);
2912       __ aesd(v0, v24); __ aesimc(v0, v0);
2913       __ aesd(v0, v25); __ aesimc(v0, v0);
2914       __ aesd(v0, v26); __ aesimc(v0, v0);
2915       __ aesd(v0, v27); __ aesimc(v0, v0);
2916       __ aesd(v0, v28); __ aesimc(v0, v0);
2917       __ aesd(v0, v29); __ aesimc(v0, v0);
2918       __ aesd(v0, v30);
2919       __ eor(v0, __ T16B, v0, v31);
2920       __ eor(v0, __ T16B, v0, v2);
2921 
2922       __ st1(v0, __ T16B, __ post(to, 16));
2923       __ orr(v2, __ T16B, v1, v1);
2924 
2925       __ subw(len_reg, len_reg, 16);
2926       __ cbnzw(len_reg, L_aes_loop);
2927 
2928       __ st1(v2, __ T16B, rvec);
2929 
2930       __ mov(r0, rscratch2);
2931 
2932       __ leave();
2933       __ ret(lr);
2934 
2935     return start;
2936   }
2937 
2938   // Arguments:
2939   //
2940   // Inputs:
2941   //   c_rarg0   - byte[]  source+offset
2942   //   c_rarg1   - int[]   SHA.state
2943   //   c_rarg2   - int     offset
2944   //   c_rarg3   - int     limit
2945   //
2946   address generate_sha1_implCompress(bool multi_block, const char *name) {
2947     __ align(CodeEntryAlignment);
2948     StubCodeMark mark(this, &quot;StubRoutines&quot;, name);
2949     address start = __ pc();
2950 
2951     Register buf   = c_rarg0;
2952     Register state = c_rarg1;
2953     Register ofs   = c_rarg2;
2954     Register limit = c_rarg3;
2955 
2956     Label keys;
2957     Label sha1_loop;
2958 
2959     // load the keys into v0..v3
2960     __ adr(rscratch1, keys);
2961     __ ld4r(v0, v1, v2, v3, __ T4S, Address(rscratch1));
2962     // load 5 words state into v6, v7
2963     __ ldrq(v6, Address(state, 0));
2964     __ ldrs(v7, Address(state, 16));
2965 
2966 
2967     __ BIND(sha1_loop);
2968     // load 64 bytes of data into v16..v19
2969     __ ld1(v16, v17, v18, v19, __ T4S, multi_block ? __ post(buf, 64) : buf);
2970     __ rev32(v16, __ T16B, v16);
2971     __ rev32(v17, __ T16B, v17);
2972     __ rev32(v18, __ T16B, v18);
2973     __ rev32(v19, __ T16B, v19);
2974 
2975     // do the sha1
2976     __ addv(v4, __ T4S, v16, v0);
2977     __ orr(v20, __ T16B, v6, v6);
2978 
2979     FloatRegister d0 = v16;
2980     FloatRegister d1 = v17;
2981     FloatRegister d2 = v18;
2982     FloatRegister d3 = v19;
2983 
2984     for (int round = 0; round &lt; 20; round++) {
2985       FloatRegister tmp1 = (round &amp; 1) ? v4 : v5;
2986       FloatRegister tmp2 = (round &amp; 1) ? v21 : v22;
2987       FloatRegister tmp3 = round ? ((round &amp; 1) ? v22 : v21) : v7;
2988       FloatRegister tmp4 = (round &amp; 1) ? v5 : v4;
2989       FloatRegister key = (round &lt; 4) ? v0 : ((round &lt; 9) ? v1 : ((round &lt; 14) ? v2 : v3));
2990 
2991       if (round &lt; 16) __ sha1su0(d0, __ T4S, d1, d2);
2992       if (round &lt; 19) __ addv(tmp1, __ T4S, d1, key);
2993       __ sha1h(tmp2, __ T4S, v20);
2994       if (round &lt; 5)
2995         __ sha1c(v20, __ T4S, tmp3, tmp4);
2996       else if (round &lt; 10 || round &gt;= 15)
2997         __ sha1p(v20, __ T4S, tmp3, tmp4);
2998       else
2999         __ sha1m(v20, __ T4S, tmp3, tmp4);
3000       if (round &lt; 16) __ sha1su1(d0, __ T4S, d3);
3001 
3002       tmp1 = d0; d0 = d1; d1 = d2; d2 = d3; d3 = tmp1;
3003     }
3004 
3005     __ addv(v7, __ T2S, v7, v21);
3006     __ addv(v6, __ T4S, v6, v20);
3007 
3008     if (multi_block) {
3009       __ add(ofs, ofs, 64);
3010       __ cmp(ofs, limit);
3011       __ br(Assembler::LE, sha1_loop);
3012       __ mov(c_rarg0, ofs); // return ofs
3013     }
3014 
3015     __ strq(v6, Address(state, 0));
3016     __ strs(v7, Address(state, 16));
3017 
3018     __ ret(lr);
3019 
3020     __ bind(keys);
3021     __ emit_int32(0x5a827999);
3022     __ emit_int32(0x6ed9eba1);
3023     __ emit_int32(0x8f1bbcdc);
3024     __ emit_int32(0xca62c1d6);
3025 
3026     return start;
3027   }
3028 
3029 
3030   // Arguments:
3031   //
3032   // Inputs:
3033   //   c_rarg0   - byte[]  source+offset
3034   //   c_rarg1   - int[]   SHA.state
3035   //   c_rarg2   - int     offset
3036   //   c_rarg3   - int     limit
3037   //
3038   address generate_sha256_implCompress(bool multi_block, const char *name) {
3039     static const uint32_t round_consts[64] = {
3040       0x428a2f98, 0x71374491, 0xb5c0fbcf, 0xe9b5dba5,
3041       0x3956c25b, 0x59f111f1, 0x923f82a4, 0xab1c5ed5,
3042       0xd807aa98, 0x12835b01, 0x243185be, 0x550c7dc3,
3043       0x72be5d74, 0x80deb1fe, 0x9bdc06a7, 0xc19bf174,
3044       0xe49b69c1, 0xefbe4786, 0x0fc19dc6, 0x240ca1cc,
3045       0x2de92c6f, 0x4a7484aa, 0x5cb0a9dc, 0x76f988da,
3046       0x983e5152, 0xa831c66d, 0xb00327c8, 0xbf597fc7,
3047       0xc6e00bf3, 0xd5a79147, 0x06ca6351, 0x14292967,
3048       0x27b70a85, 0x2e1b2138, 0x4d2c6dfc, 0x53380d13,
3049       0x650a7354, 0x766a0abb, 0x81c2c92e, 0x92722c85,
3050       0xa2bfe8a1, 0xa81a664b, 0xc24b8b70, 0xc76c51a3,
3051       0xd192e819, 0xd6990624, 0xf40e3585, 0x106aa070,
3052       0x19a4c116, 0x1e376c08, 0x2748774c, 0x34b0bcb5,
3053       0x391c0cb3, 0x4ed8aa4a, 0x5b9cca4f, 0x682e6ff3,
3054       0x748f82ee, 0x78a5636f, 0x84c87814, 0x8cc70208,
3055       0x90befffa, 0xa4506ceb, 0xbef9a3f7, 0xc67178f2,
3056     };
3057     __ align(CodeEntryAlignment);
3058     StubCodeMark mark(this, &quot;StubRoutines&quot;, name);
3059     address start = __ pc();
3060 
3061     Register buf   = c_rarg0;
3062     Register state = c_rarg1;
3063     Register ofs   = c_rarg2;
3064     Register limit = c_rarg3;
3065 
3066     Label sha1_loop;
3067 
3068     __ stpd(v8, v9, __ pre(sp, -32));
3069     __ stpd(v10, v11, Address(sp, 16));
3070 
3071 // dga == v0
3072 // dgb == v1
3073 // dg0 == v2
3074 // dg1 == v3
3075 // dg2 == v4
3076 // t0 == v6
3077 // t1 == v7
3078 
3079     // load 16 keys to v16..v31
3080     __ lea(rscratch1, ExternalAddress((address)round_consts));
3081     __ ld1(v16, v17, v18, v19, __ T4S, __ post(rscratch1, 64));
3082     __ ld1(v20, v21, v22, v23, __ T4S, __ post(rscratch1, 64));
3083     __ ld1(v24, v25, v26, v27, __ T4S, __ post(rscratch1, 64));
3084     __ ld1(v28, v29, v30, v31, __ T4S, rscratch1);
3085 
3086     // load 8 words (256 bits) state
3087     __ ldpq(v0, v1, state);
3088 
3089     __ BIND(sha1_loop);
3090     // load 64 bytes of data into v8..v11
3091     __ ld1(v8, v9, v10, v11, __ T4S, multi_block ? __ post(buf, 64) : buf);
3092     __ rev32(v8, __ T16B, v8);
3093     __ rev32(v9, __ T16B, v9);
3094     __ rev32(v10, __ T16B, v10);
3095     __ rev32(v11, __ T16B, v11);
3096 
3097     __ addv(v6, __ T4S, v8, v16);
3098     __ orr(v2, __ T16B, v0, v0);
3099     __ orr(v3, __ T16B, v1, v1);
3100 
3101     FloatRegister d0 = v8;
3102     FloatRegister d1 = v9;
3103     FloatRegister d2 = v10;
3104     FloatRegister d3 = v11;
3105 
3106 
3107     for (int round = 0; round &lt; 16; round++) {
3108       FloatRegister tmp1 = (round &amp; 1) ? v6 : v7;
3109       FloatRegister tmp2 = (round &amp; 1) ? v7 : v6;
3110       FloatRegister tmp3 = (round &amp; 1) ? v2 : v4;
3111       FloatRegister tmp4 = (round &amp; 1) ? v4 : v2;
3112 
3113       if (round &lt; 12) __ sha256su0(d0, __ T4S, d1);
3114        __ orr(v4, __ T16B, v2, v2);
3115       if (round &lt; 15)
3116         __ addv(tmp1, __ T4S, d1, as_FloatRegister(round + 17));
3117       __ sha256h(v2, __ T4S, v3, tmp2);
3118       __ sha256h2(v3, __ T4S, v4, tmp2);
3119       if (round &lt; 12) __ sha256su1(d0, __ T4S, d2, d3);
3120 
3121       tmp1 = d0; d0 = d1; d1 = d2; d2 = d3; d3 = tmp1;
3122     }
3123 
3124     __ addv(v0, __ T4S, v0, v2);
3125     __ addv(v1, __ T4S, v1, v3);
3126 
3127     if (multi_block) {
3128       __ add(ofs, ofs, 64);
3129       __ cmp(ofs, limit);
3130       __ br(Assembler::LE, sha1_loop);
3131       __ mov(c_rarg0, ofs); // return ofs
3132     }
3133 
3134     __ ldpd(v10, v11, Address(sp, 16));
3135     __ ldpd(v8, v9, __ post(sp, 32));
3136 
3137     __ stpq(v0, v1, state);
3138 
3139     __ ret(lr);
3140 
3141     return start;
3142   }
3143 
3144   // Safefetch stubs.
3145   void generate_safefetch(const char* name, int size, address* entry,
3146                           address* fault_pc, address* continuation_pc) {
3147     // safefetch signatures:
3148     //   int      SafeFetch32(int*      adr, int      errValue);
3149     //   intptr_t SafeFetchN (intptr_t* adr, intptr_t errValue);
3150     //
3151     // arguments:
3152     //   c_rarg0 = adr
3153     //   c_rarg1 = errValue
3154     //
3155     // result:
3156     //   PPC_RET  = *adr or errValue
3157 
3158     StubCodeMark mark(this, &quot;StubRoutines&quot;, name);
3159 
3160     // Entry point, pc or function descriptor.
3161     *entry = __ pc();
3162 
3163     // Load *adr into c_rarg1, may fault.
3164     *fault_pc = __ pc();
3165     switch (size) {
3166       case 4:
3167         // int32_t
3168         __ ldrw(c_rarg1, Address(c_rarg0, 0));
3169         break;
3170       case 8:
3171         // int64_t
3172         __ ldr(c_rarg1, Address(c_rarg0, 0));
3173         break;
3174       default:
3175         ShouldNotReachHere();
3176     }
3177 
3178     // return errValue or *adr
3179     *continuation_pc = __ pc();
3180     __ mov(r0, c_rarg1);
3181     __ ret(lr);
3182   }
3183 
3184   /**
3185    *  Arguments:
3186    *
3187    * Inputs:
3188    *   c_rarg0   - int crc
3189    *   c_rarg1   - byte* buf
3190    *   c_rarg2   - int length
3191    *
3192    * Ouput:
3193    *       rax   - int crc result
3194    */
3195   address generate_updateBytesCRC32() {
3196     assert(UseCRC32Intrinsics, &quot;what are we doing here?&quot;);
3197 
3198     __ align(CodeEntryAlignment);
3199     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;updateBytesCRC32&quot;);
3200 
3201     address start = __ pc();
3202 
3203     const Register crc   = c_rarg0;  // crc
3204     const Register buf   = c_rarg1;  // source java byte array address
3205     const Register len   = c_rarg2;  // length
3206     const Register table0 = c_rarg3; // crc_table address
3207     const Register table1 = c_rarg4;
3208     const Register table2 = c_rarg5;
3209     const Register table3 = c_rarg6;
3210     const Register tmp3 = c_rarg7;
3211 
3212     BLOCK_COMMENT(&quot;Entry:&quot;);
3213     __ enter(); // required for proper stackwalking of RuntimeStub frame
3214 
3215     __ kernel_crc32(crc, buf, len,
3216               table0, table1, table2, table3, rscratch1, rscratch2, tmp3);
3217 
3218     __ leave(); // required for proper stackwalking of RuntimeStub frame
3219     __ ret(lr);
3220 
3221     return start;
3222   }
3223 
3224   /**
3225    *  Arguments:
3226    *
3227    * Inputs:
3228    *   c_rarg0   - int crc
3229    *   c_rarg1   - byte* buf
3230    *   c_rarg2   - int length
3231    *   c_rarg3   - int* table
3232    *
3233    * Ouput:
3234    *       r0   - int crc result
3235    */
3236   address generate_updateBytesCRC32C() {
3237     assert(UseCRC32CIntrinsics, &quot;what are we doing here?&quot;);
3238 
3239     __ align(CodeEntryAlignment);
3240     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;updateBytesCRC32C&quot;);
3241 
3242     address start = __ pc();
3243 
3244     const Register crc   = c_rarg0;  // crc
3245     const Register buf   = c_rarg1;  // source java byte array address
3246     const Register len   = c_rarg2;  // length
3247     const Register table0 = c_rarg3; // crc_table address
3248     const Register table1 = c_rarg4;
3249     const Register table2 = c_rarg5;
3250     const Register table3 = c_rarg6;
3251     const Register tmp3 = c_rarg7;
3252 
3253     BLOCK_COMMENT(&quot;Entry:&quot;);
3254     __ enter(); // required for proper stackwalking of RuntimeStub frame
3255 
3256     __ kernel_crc32c(crc, buf, len,
3257               table0, table1, table2, table3, rscratch1, rscratch2, tmp3);
3258 
3259     __ leave(); // required for proper stackwalking of RuntimeStub frame
3260     __ ret(lr);
3261 
3262     return start;
3263   }
3264 
3265   /***
3266    *  Arguments:
3267    *
3268    *  Inputs:
3269    *   c_rarg0   - int   adler
3270    *   c_rarg1   - byte* buff
3271    *   c_rarg2   - int   len
3272    *
3273    * Output:
3274    *   c_rarg0   - int adler result
3275    */
3276   address generate_updateBytesAdler32() {
3277     __ align(CodeEntryAlignment);
3278     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;updateBytesAdler32&quot;);
3279     address start = __ pc();
3280 
3281     Label L_simple_by1_loop, L_nmax, L_nmax_loop, L_by16, L_by16_loop, L_by1_loop, L_do_mod, L_combine, L_by1;
3282 
3283     // Aliases
3284     Register adler  = c_rarg0;
3285     Register s1     = c_rarg0;
3286     Register s2     = c_rarg3;
3287     Register buff   = c_rarg1;
3288     Register len    = c_rarg2;
3289     Register nmax  = r4;
3290     Register base  = r5;
3291     Register count = r6;
3292     Register temp0 = rscratch1;
3293     Register temp1 = rscratch2;
3294     FloatRegister vbytes = v0;
3295     FloatRegister vs1acc = v1;
3296     FloatRegister vs2acc = v2;
3297     FloatRegister vtable = v3;
3298 
3299     // Max number of bytes we can process before having to take the mod
3300     // 0x15B0 is 5552 in decimal, the largest n such that 255n(n+1)/2 + (n+1)(BASE-1) &lt;= 2^32-1
3301     unsigned long BASE = 0xfff1;
3302     unsigned long NMAX = 0x15B0;
3303 
3304     __ mov(base, BASE);
3305     __ mov(nmax, NMAX);
3306 
3307     // Load accumulation coefficients for the upper 16 bits
3308     __ lea(temp0, ExternalAddress((address) StubRoutines::aarch64::_adler_table));
3309     __ ld1(vtable, __ T16B, Address(temp0));
3310 
3311     // s1 is initialized to the lower 16 bits of adler
3312     // s2 is initialized to the upper 16 bits of adler
3313     __ ubfx(s2, adler, 16, 16);  // s2 = ((adler &gt;&gt; 16) &amp; 0xffff)
3314     __ uxth(s1, adler);          // s1 = (adler &amp; 0xffff)
3315 
3316     // The pipelined loop needs at least 16 elements for 1 iteration
3317     // It does check this, but it is more effective to skip to the cleanup loop
3318     __ cmp(len, (u1)16);
3319     __ br(Assembler::HS, L_nmax);
3320     __ cbz(len, L_combine);
3321 
3322     __ bind(L_simple_by1_loop);
3323     __ ldrb(temp0, Address(__ post(buff, 1)));
3324     __ add(s1, s1, temp0);
3325     __ add(s2, s2, s1);
3326     __ subs(len, len, 1);
3327     __ br(Assembler::HI, L_simple_by1_loop);
3328 
3329     // s1 = s1 % BASE
3330     __ subs(temp0, s1, base);
3331     __ csel(s1, temp0, s1, Assembler::HS);
3332 
3333     // s2 = s2 % BASE
3334     __ lsr(temp0, s2, 16);
3335     __ lsl(temp1, temp0, 4);
3336     __ sub(temp1, temp1, temp0);
3337     __ add(s2, temp1, s2, ext::uxth);
3338 
3339     __ subs(temp0, s2, base);
3340     __ csel(s2, temp0, s2, Assembler::HS);
3341 
3342     __ b(L_combine);
3343 
3344     __ bind(L_nmax);
3345     __ subs(len, len, nmax);
3346     __ sub(count, nmax, 16);
3347     __ br(Assembler::LO, L_by16);
3348 
3349     __ bind(L_nmax_loop);
3350 
3351     generate_updateBytesAdler32_accum(s1, s2, buff, temp0, temp1,
3352                                       vbytes, vs1acc, vs2acc, vtable);
3353 
3354     __ subs(count, count, 16);
3355     __ br(Assembler::HS, L_nmax_loop);
3356 
3357     // s1 = s1 % BASE
3358     __ lsr(temp0, s1, 16);
3359     __ lsl(temp1, temp0, 4);
3360     __ sub(temp1, temp1, temp0);
3361     __ add(temp1, temp1, s1, ext::uxth);
3362 
3363     __ lsr(temp0, temp1, 16);
3364     __ lsl(s1, temp0, 4);
3365     __ sub(s1, s1, temp0);
3366     __ add(s1, s1, temp1, ext:: uxth);
3367 
3368     __ subs(temp0, s1, base);
3369     __ csel(s1, temp0, s1, Assembler::HS);
3370 
3371     // s2 = s2 % BASE
3372     __ lsr(temp0, s2, 16);
3373     __ lsl(temp1, temp0, 4);
3374     __ sub(temp1, temp1, temp0);
3375     __ add(temp1, temp1, s2, ext::uxth);
3376 
3377     __ lsr(temp0, temp1, 16);
3378     __ lsl(s2, temp0, 4);
3379     __ sub(s2, s2, temp0);
3380     __ add(s2, s2, temp1, ext:: uxth);
3381 
3382     __ subs(temp0, s2, base);
3383     __ csel(s2, temp0, s2, Assembler::HS);
3384 
3385     __ subs(len, len, nmax);
3386     __ sub(count, nmax, 16);
3387     __ br(Assembler::HS, L_nmax_loop);
3388 
3389     __ bind(L_by16);
3390     __ adds(len, len, count);
3391     __ br(Assembler::LO, L_by1);
3392 
3393     __ bind(L_by16_loop);
3394 
3395     generate_updateBytesAdler32_accum(s1, s2, buff, temp0, temp1,
3396                                       vbytes, vs1acc, vs2acc, vtable);
3397 
3398     __ subs(len, len, 16);
3399     __ br(Assembler::HS, L_by16_loop);
3400 
3401     __ bind(L_by1);
3402     __ adds(len, len, 15);
3403     __ br(Assembler::LO, L_do_mod);
3404 
3405     __ bind(L_by1_loop);
3406     __ ldrb(temp0, Address(__ post(buff, 1)));
3407     __ add(s1, temp0, s1);
3408     __ add(s2, s2, s1);
3409     __ subs(len, len, 1);
3410     __ br(Assembler::HS, L_by1_loop);
3411 
3412     __ bind(L_do_mod);
3413     // s1 = s1 % BASE
3414     __ lsr(temp0, s1, 16);
3415     __ lsl(temp1, temp0, 4);
3416     __ sub(temp1, temp1, temp0);
3417     __ add(temp1, temp1, s1, ext::uxth);
3418 
3419     __ lsr(temp0, temp1, 16);
3420     __ lsl(s1, temp0, 4);
3421     __ sub(s1, s1, temp0);
3422     __ add(s1, s1, temp1, ext:: uxth);
3423 
3424     __ subs(temp0, s1, base);
3425     __ csel(s1, temp0, s1, Assembler::HS);
3426 
3427     // s2 = s2 % BASE
3428     __ lsr(temp0, s2, 16);
3429     __ lsl(temp1, temp0, 4);
3430     __ sub(temp1, temp1, temp0);
3431     __ add(temp1, temp1, s2, ext::uxth);
3432 
3433     __ lsr(temp0, temp1, 16);
3434     __ lsl(s2, temp0, 4);
3435     __ sub(s2, s2, temp0);
3436     __ add(s2, s2, temp1, ext:: uxth);
3437 
3438     __ subs(temp0, s2, base);
3439     __ csel(s2, temp0, s2, Assembler::HS);
3440 
3441     // Combine lower bits and higher bits
3442     __ bind(L_combine);
3443     __ orr(s1, s1, s2, Assembler::LSL, 16); // adler = s1 | (s2 &lt;&lt; 16)
3444 
3445     __ ret(lr);
3446 
3447     return start;
3448   }
3449 
3450   void generate_updateBytesAdler32_accum(Register s1, Register s2, Register buff,
3451           Register temp0, Register temp1, FloatRegister vbytes,
3452           FloatRegister vs1acc, FloatRegister vs2acc, FloatRegister vtable) {
3453     // Below is a vectorized implementation of updating s1 and s2 for 16 bytes.
3454     // We use b1, b2, ..., b16 to denote the 16 bytes loaded in each iteration.
3455     // In non-vectorized code, we update s1 and s2 as:
3456     //   s1 &lt;- s1 + b1
3457     //   s2 &lt;- s2 + s1
3458     //   s1 &lt;- s1 + b2
3459     //   s2 &lt;- s2 + b1
3460     //   ...
3461     //   s1 &lt;- s1 + b16
3462     //   s2 &lt;- s2 + s1
3463     // Putting above assignments together, we have:
3464     //   s1_new = s1 + b1 + b2 + ... + b16
3465     //   s2_new = s2 + (s1 + b1) + (s1 + b1 + b2) + ... + (s1 + b1 + b2 + ... + b16)
3466     //          = s2 + s1 * 16 + (b1 * 16 + b2 * 15 + ... + b16 * 1)
3467     //          = s2 + s1 * 16 + (b1, b2, ... b16) dot (16, 15, ... 1)
3468     __ ld1(vbytes, __ T16B, Address(__ post(buff, 16)));
3469 
3470     // s2 = s2 + s1 * 16
3471     __ add(s2, s2, s1, Assembler::LSL, 4);
3472 
3473     // vs1acc = b1 + b2 + b3 + ... + b16
3474     // vs2acc = (b1 * 16) + (b2 * 15) + (b3 * 14) + ... + (b16 * 1)
3475     __ umullv(vs2acc, __ T8B, vtable, vbytes);
3476     __ umlalv(vs2acc, __ T16B, vtable, vbytes);
3477     __ uaddlv(vs1acc, __ T16B, vbytes);
3478     __ uaddlv(vs2acc, __ T8H, vs2acc);
3479 
3480     // s1 = s1 + vs1acc, s2 = s2 + vs2acc
3481     __ fmovd(temp0, vs1acc);
3482     __ fmovd(temp1, vs2acc);
3483     __ add(s1, s1, temp0);
3484     __ add(s2, s2, temp1);
3485   }
3486 
3487   /**
3488    *  Arguments:
3489    *
3490    *  Input:
3491    *    c_rarg0   - x address
3492    *    c_rarg1   - x length
3493    *    c_rarg2   - y address
3494    *    c_rarg3   - y lenth
3495    *    c_rarg4   - z address
3496    *    c_rarg5   - z length
3497    */
3498   address generate_multiplyToLen() {
3499     __ align(CodeEntryAlignment);
3500     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;multiplyToLen&quot;);
3501 
3502     address start = __ pc();
3503     const Register x     = r0;
3504     const Register xlen  = r1;
3505     const Register y     = r2;
3506     const Register ylen  = r3;
3507     const Register z     = r4;
3508     const Register zlen  = r5;
3509 
3510     const Register tmp1  = r10;
3511     const Register tmp2  = r11;
3512     const Register tmp3  = r12;
3513     const Register tmp4  = r13;
3514     const Register tmp5  = r14;
3515     const Register tmp6  = r15;
3516     const Register tmp7  = r16;
3517 
3518     BLOCK_COMMENT(&quot;Entry:&quot;);
3519     __ enter(); // required for proper stackwalking of RuntimeStub frame
3520     __ multiply_to_len(x, xlen, y, ylen, z, zlen, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7);
3521     __ leave(); // required for proper stackwalking of RuntimeStub frame
3522     __ ret(lr);
3523 
3524     return start;
3525   }
3526 
3527   address generate_squareToLen() {
3528     // squareToLen algorithm for sizes 1..127 described in java code works
3529     // faster than multiply_to_len on some CPUs and slower on others, but
3530     // multiply_to_len shows a bit better overall results
3531     __ align(CodeEntryAlignment);
3532     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;squareToLen&quot;);
3533     address start = __ pc();
3534 
3535     const Register x     = r0;
3536     const Register xlen  = r1;
3537     const Register z     = r2;
3538     const Register zlen  = r3;
3539     const Register y     = r4; // == x
3540     const Register ylen  = r5; // == xlen
3541 
3542     const Register tmp1  = r10;
3543     const Register tmp2  = r11;
3544     const Register tmp3  = r12;
3545     const Register tmp4  = r13;
3546     const Register tmp5  = r14;
3547     const Register tmp6  = r15;
3548     const Register tmp7  = r16;
3549 
3550     RegSet spilled_regs = RegSet::of(y, ylen);
3551     BLOCK_COMMENT(&quot;Entry:&quot;);
3552     __ enter();
3553     __ push(spilled_regs, sp);
3554     __ mov(y, x);
3555     __ mov(ylen, xlen);
3556     __ multiply_to_len(x, xlen, y, ylen, z, zlen, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7);
3557     __ pop(spilled_regs, sp);
3558     __ leave();
3559     __ ret(lr);
3560     return start;
3561   }
3562 
3563   address generate_mulAdd() {
3564     __ align(CodeEntryAlignment);
3565     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;mulAdd&quot;);
3566 
3567     address start = __ pc();
3568 
3569     const Register out     = r0;
3570     const Register in      = r1;
3571     const Register offset  = r2;
3572     const Register len     = r3;
3573     const Register k       = r4;
3574 
3575     BLOCK_COMMENT(&quot;Entry:&quot;);
3576     __ enter();
3577     __ mul_add(out, in, offset, len, k);
3578     __ leave();
3579     __ ret(lr);
3580 
3581     return start;
3582   }
3583 
3584   void ghash_multiply(FloatRegister result_lo, FloatRegister result_hi,
3585                       FloatRegister a, FloatRegister b, FloatRegister a1_xor_a0,
3586                       FloatRegister tmp1, FloatRegister tmp2, FloatRegister tmp3, FloatRegister tmp4) {
3587     // Karatsuba multiplication performs a 128*128 -&gt; 256-bit
3588     // multiplication in three 128-bit multiplications and a few
3589     // additions.
3590     //
3591     // (C1:C0) = A1*B1, (D1:D0) = A0*B0, (E1:E0) = (A0+A1)(B0+B1)
3592     // (A1:A0)(B1:B0) = C1:(C0+C1+D1+E1):(D1+C0+D0+E0):D0
3593     //
3594     // Inputs:
3595     //
3596     // A0 in a.d[0]     (subkey)
3597     // A1 in a.d[1]
3598     // (A1+A0) in a1_xor_a0.d[0]
3599     //
3600     // B0 in b.d[0]     (state)
3601     // B1 in b.d[1]
3602 
3603     __ ext(tmp1, __ T16B, b, b, 0x08);
3604     __ pmull2(result_hi, __ T1Q, b, a, __ T2D);  // A1*B1
3605     __ eor(tmp1, __ T16B, tmp1, b);            // (B1+B0)
3606     __ pmull(result_lo,  __ T1Q, b, a, __ T1D);  // A0*B0
3607     __ pmull(tmp2, __ T1Q, tmp1, a1_xor_a0, __ T1D); // (A1+A0)(B1+B0)
3608 
3609     __ ext(tmp4, __ T16B, result_lo, result_hi, 0x08);
3610     __ eor(tmp3, __ T16B, result_hi, result_lo); // A1*B1+A0*B0
3611     __ eor(tmp2, __ T16B, tmp2, tmp4);
3612     __ eor(tmp2, __ T16B, tmp2, tmp3);
3613 
3614     // Register pair &lt;result_hi:result_lo&gt; holds the result of carry-less multiplication
3615     __ ins(result_hi, __ D, tmp2, 0, 1);
3616     __ ins(result_lo, __ D, tmp2, 1, 0);
3617   }
3618 
3619   void ghash_reduce(FloatRegister result, FloatRegister lo, FloatRegister hi,
3620                     FloatRegister p, FloatRegister z, FloatRegister t1) {
3621     const FloatRegister t0 = result;
3622 
3623     // The GCM field polynomial f is z^128 + p(z), where p =
3624     // z^7+z^2+z+1.
3625     //
3626     //    z^128 === -p(z)  (mod (z^128 + p(z)))
3627     //
3628     // so, given that the product we&#39;re reducing is
3629     //    a == lo + hi * z^128
3630     // substituting,
3631     //      === lo - hi * p(z)  (mod (z^128 + p(z)))
3632     //
3633     // we reduce by multiplying hi by p(z) and subtracting the result
3634     // from (i.e. XORing it with) lo.  Because p has no nonzero high
3635     // bits we can do this with two 64-bit multiplications, lo*p and
3636     // hi*p.
3637 
3638     __ pmull2(t0, __ T1Q, hi, p, __ T2D);
3639     __ ext(t1, __ T16B, t0, z, 8);
3640     __ eor(hi, __ T16B, hi, t1);
3641     __ ext(t1, __ T16B, z, t0, 8);
3642     __ eor(lo, __ T16B, lo, t1);
3643     __ pmull(t0, __ T1Q, hi, p, __ T1D);
3644     __ eor(result, __ T16B, lo, t0);
3645   }
3646 
3647   address generate_has_negatives(address &amp;has_negatives_long) {
3648     const u1 large_loop_size = 64;
3649     const uint64_t UPPER_BIT_MASK=0x8080808080808080;
3650     int dcache_line = VM_Version::dcache_line_size();
3651 
3652     Register ary1 = r1, len = r2, result = r0;
3653 
3654     __ align(CodeEntryAlignment);
3655 
3656     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;has_negatives&quot;);
3657 
3658     address entry = __ pc();
3659 
3660     __ enter();
3661 
3662   Label RET_TRUE, RET_TRUE_NO_POP, RET_FALSE, ALIGNED, LOOP16, CHECK_16, DONE,
3663         LARGE_LOOP, POST_LOOP16, LEN_OVER_15, LEN_OVER_8, POST_LOOP16_LOAD_TAIL;
3664 
3665   __ cmp(len, (u1)15);
3666   __ br(Assembler::GT, LEN_OVER_15);
3667   // The only case when execution falls into this code is when pointer is near
3668   // the end of memory page and we have to avoid reading next page
3669   __ add(ary1, ary1, len);
3670   __ subs(len, len, 8);
3671   __ br(Assembler::GT, LEN_OVER_8);
3672   __ ldr(rscratch2, Address(ary1, -8));
3673   __ sub(rscratch1, zr, len, __ LSL, 3);  // LSL 3 is to get bits from bytes.
3674   __ lsrv(rscratch2, rscratch2, rscratch1);
3675   __ tst(rscratch2, UPPER_BIT_MASK);
3676   __ cset(result, Assembler::NE);
3677   __ leave();
3678   __ ret(lr);
3679   __ bind(LEN_OVER_8);
3680   __ ldp(rscratch1, rscratch2, Address(ary1, -16));
3681   __ sub(len, len, 8); // no data dep., then sub can be executed while loading
3682   __ tst(rscratch2, UPPER_BIT_MASK);
3683   __ br(Assembler::NE, RET_TRUE_NO_POP);
3684   __ sub(rscratch2, zr, len, __ LSL, 3); // LSL 3 is to get bits from bytes
3685   __ lsrv(rscratch1, rscratch1, rscratch2);
3686   __ tst(rscratch1, UPPER_BIT_MASK);
3687   __ cset(result, Assembler::NE);
3688   __ leave();
3689   __ ret(lr);
3690 
3691   Register tmp1 = r3, tmp2 = r4, tmp3 = r5, tmp4 = r6, tmp5 = r7, tmp6 = r10;
3692   const RegSet spilled_regs = RegSet::range(tmp1, tmp5) + tmp6;
3693 
3694   has_negatives_long = __ pc(); // 2nd entry point
3695 
3696   __ enter();
3697 
3698   __ bind(LEN_OVER_15);
3699     __ push(spilled_regs, sp);
3700     __ andr(rscratch2, ary1, 15); // check pointer for 16-byte alignment
3701     __ cbz(rscratch2, ALIGNED);
3702     __ ldp(tmp6, tmp1, Address(ary1));
3703     __ mov(tmp5, 16);
3704     __ sub(rscratch1, tmp5, rscratch2); // amount of bytes until aligned address
3705     __ add(ary1, ary1, rscratch1);
3706     __ sub(len, len, rscratch1);
3707     __ orr(tmp6, tmp6, tmp1);
3708     __ tst(tmp6, UPPER_BIT_MASK);
3709     __ br(Assembler::NE, RET_TRUE);
3710 
3711   __ bind(ALIGNED);
3712     __ cmp(len, large_loop_size);
3713     __ br(Assembler::LT, CHECK_16);
3714     // Perform 16-byte load as early return in pre-loop to handle situation
3715     // when initially aligned large array has negative values at starting bytes,
3716     // so LARGE_LOOP would do 4 reads instead of 1 (in worst case), which is
3717     // slower. Cases with negative bytes further ahead won&#39;t be affected that
3718     // much. In fact, it&#39;ll be faster due to early loads, less instructions and
3719     // less branches in LARGE_LOOP.
3720     __ ldp(tmp6, tmp1, Address(__ post(ary1, 16)));
3721     __ sub(len, len, 16);
3722     __ orr(tmp6, tmp6, tmp1);
3723     __ tst(tmp6, UPPER_BIT_MASK);
3724     __ br(Assembler::NE, RET_TRUE);
3725     __ cmp(len, large_loop_size);
3726     __ br(Assembler::LT, CHECK_16);
3727 
3728     if (SoftwarePrefetchHintDistance &gt;= 0
3729         &amp;&amp; SoftwarePrefetchHintDistance &gt;= dcache_line) {
3730       // initial prefetch
3731       __ prfm(Address(ary1, SoftwarePrefetchHintDistance - dcache_line));
3732     }
3733   __ bind(LARGE_LOOP);
3734     if (SoftwarePrefetchHintDistance &gt;= 0) {
3735       __ prfm(Address(ary1, SoftwarePrefetchHintDistance));
3736     }
3737     // Issue load instructions first, since it can save few CPU/MEM cycles, also
3738     // instead of 4 triples of &quot;orr(...), addr(...);cbnz(...);&quot; (for each ldp)
3739     // better generate 7 * orr(...) + 1 andr(...) + 1 cbnz(...) which saves 3
3740     // instructions per cycle and have less branches, but this approach disables
3741     // early return, thus, all 64 bytes are loaded and checked every time.
3742     __ ldp(tmp2, tmp3, Address(ary1));
3743     __ ldp(tmp4, tmp5, Address(ary1, 16));
3744     __ ldp(rscratch1, rscratch2, Address(ary1, 32));
3745     __ ldp(tmp6, tmp1, Address(ary1, 48));
3746     __ add(ary1, ary1, large_loop_size);
3747     __ sub(len, len, large_loop_size);
3748     __ orr(tmp2, tmp2, tmp3);
3749     __ orr(tmp4, tmp4, tmp5);
3750     __ orr(rscratch1, rscratch1, rscratch2);
3751     __ orr(tmp6, tmp6, tmp1);
3752     __ orr(tmp2, tmp2, tmp4);
3753     __ orr(rscratch1, rscratch1, tmp6);
3754     __ orr(tmp2, tmp2, rscratch1);
3755     __ tst(tmp2, UPPER_BIT_MASK);
3756     __ br(Assembler::NE, RET_TRUE);
3757     __ cmp(len, large_loop_size);
3758     __ br(Assembler::GE, LARGE_LOOP);
3759 
3760   __ bind(CHECK_16); // small 16-byte load pre-loop
3761     __ cmp(len, (u1)16);
3762     __ br(Assembler::LT, POST_LOOP16);
3763 
3764   __ bind(LOOP16); // small 16-byte load loop
3765     __ ldp(tmp2, tmp3, Address(__ post(ary1, 16)));
3766     __ sub(len, len, 16);
3767     __ orr(tmp2, tmp2, tmp3);
3768     __ tst(tmp2, UPPER_BIT_MASK);
3769     __ br(Assembler::NE, RET_TRUE);
3770     __ cmp(len, (u1)16);
3771     __ br(Assembler::GE, LOOP16); // 16-byte load loop end
3772 
3773   __ bind(POST_LOOP16); // 16-byte aligned, so we can read unconditionally
3774     __ cmp(len, (u1)8);
3775     __ br(Assembler::LE, POST_LOOP16_LOAD_TAIL);
3776     __ ldr(tmp3, Address(__ post(ary1, 8)));
3777     __ sub(len, len, 8);
3778     __ tst(tmp3, UPPER_BIT_MASK);
3779     __ br(Assembler::NE, RET_TRUE);
3780 
3781   __ bind(POST_LOOP16_LOAD_TAIL);
3782     __ cbz(len, RET_FALSE); // Can&#39;t shift left by 64 when len==0
3783     __ ldr(tmp1, Address(ary1));
3784     __ mov(tmp2, 64);
3785     __ sub(tmp4, tmp2, len, __ LSL, 3);
3786     __ lslv(tmp1, tmp1, tmp4);
3787     __ tst(tmp1, UPPER_BIT_MASK);
3788     __ br(Assembler::NE, RET_TRUE);
3789     // Fallthrough
3790 
3791   __ bind(RET_FALSE);
3792     __ pop(spilled_regs, sp);
3793     __ leave();
3794     __ mov(result, zr);
3795     __ ret(lr);
3796 
3797   __ bind(RET_TRUE);
3798     __ pop(spilled_regs, sp);
3799   __ bind(RET_TRUE_NO_POP);
3800     __ leave();
3801     __ mov(result, 1);
3802     __ ret(lr);
3803 
3804   __ bind(DONE);
3805     __ pop(spilled_regs, sp);
3806     __ leave();
3807     __ ret(lr);
3808     return entry;
3809   }
3810 
3811   void generate_large_array_equals_loop_nonsimd(int loopThreshold,
3812         bool usePrefetch, Label &amp;NOT_EQUAL) {
3813     Register a1 = r1, a2 = r2, result = r0, cnt1 = r10, tmp1 = rscratch1,
3814         tmp2 = rscratch2, tmp3 = r3, tmp4 = r4, tmp5 = r5, tmp6 = r11,
3815         tmp7 = r12, tmp8 = r13;
3816     Label LOOP;
3817 
3818     __ ldp(tmp1, tmp3, Address(__ post(a1, 2 * wordSize)));
3819     __ ldp(tmp2, tmp4, Address(__ post(a2, 2 * wordSize)));
3820     __ bind(LOOP);
3821     if (usePrefetch) {
3822       __ prfm(Address(a1, SoftwarePrefetchHintDistance));
3823       __ prfm(Address(a2, SoftwarePrefetchHintDistance));
3824     }
3825     __ ldp(tmp5, tmp7, Address(__ post(a1, 2 * wordSize)));
3826     __ eor(tmp1, tmp1, tmp2);
3827     __ eor(tmp3, tmp3, tmp4);
3828     __ ldp(tmp6, tmp8, Address(__ post(a2, 2 * wordSize)));
3829     __ orr(tmp1, tmp1, tmp3);
3830     __ cbnz(tmp1, NOT_EQUAL);
3831     __ ldp(tmp1, tmp3, Address(__ post(a1, 2 * wordSize)));
3832     __ eor(tmp5, tmp5, tmp6);
3833     __ eor(tmp7, tmp7, tmp8);
3834     __ ldp(tmp2, tmp4, Address(__ post(a2, 2 * wordSize)));
3835     __ orr(tmp5, tmp5, tmp7);
3836     __ cbnz(tmp5, NOT_EQUAL);
3837     __ ldp(tmp5, tmp7, Address(__ post(a1, 2 * wordSize)));
3838     __ eor(tmp1, tmp1, tmp2);
3839     __ eor(tmp3, tmp3, tmp4);
3840     __ ldp(tmp6, tmp8, Address(__ post(a2, 2 * wordSize)));
3841     __ orr(tmp1, tmp1, tmp3);
3842     __ cbnz(tmp1, NOT_EQUAL);
3843     __ ldp(tmp1, tmp3, Address(__ post(a1, 2 * wordSize)));
3844     __ eor(tmp5, tmp5, tmp6);
3845     __ sub(cnt1, cnt1, 8 * wordSize);
3846     __ eor(tmp7, tmp7, tmp8);
3847     __ ldp(tmp2, tmp4, Address(__ post(a2, 2 * wordSize)));
3848     // tmp6 is not used. MacroAssembler::subs is used here (rather than
3849     // cmp) because subs allows an unlimited range of immediate operand.
3850     __ subs(tmp6, cnt1, loopThreshold);
3851     __ orr(tmp5, tmp5, tmp7);
3852     __ cbnz(tmp5, NOT_EQUAL);
3853     __ br(__ GE, LOOP);
3854     // post-loop
3855     __ eor(tmp1, tmp1, tmp2);
3856     __ eor(tmp3, tmp3, tmp4);
3857     __ orr(tmp1, tmp1, tmp3);
3858     __ sub(cnt1, cnt1, 2 * wordSize);
3859     __ cbnz(tmp1, NOT_EQUAL);
3860   }
3861 
3862   void generate_large_array_equals_loop_simd(int loopThreshold,
3863         bool usePrefetch, Label &amp;NOT_EQUAL) {
3864     Register a1 = r1, a2 = r2, result = r0, cnt1 = r10, tmp1 = rscratch1,
3865         tmp2 = rscratch2;
3866     Label LOOP;
3867 
3868     __ bind(LOOP);
3869     if (usePrefetch) {
3870       __ prfm(Address(a1, SoftwarePrefetchHintDistance));
3871       __ prfm(Address(a2, SoftwarePrefetchHintDistance));
3872     }
3873     __ ld1(v0, v1, v2, v3, __ T2D, Address(__ post(a1, 4 * 2 * wordSize)));
3874     __ sub(cnt1, cnt1, 8 * wordSize);
3875     __ ld1(v4, v5, v6, v7, __ T2D, Address(__ post(a2, 4 * 2 * wordSize)));
3876     __ subs(tmp1, cnt1, loopThreshold);
3877     __ eor(v0, __ T16B, v0, v4);
3878     __ eor(v1, __ T16B, v1, v5);
3879     __ eor(v2, __ T16B, v2, v6);
3880     __ eor(v3, __ T16B, v3, v7);
3881     __ orr(v0, __ T16B, v0, v1);
3882     __ orr(v1, __ T16B, v2, v3);
3883     __ orr(v0, __ T16B, v0, v1);
3884     __ umov(tmp1, v0, __ D, 0);
3885     __ umov(tmp2, v0, __ D, 1);
3886     __ orr(tmp1, tmp1, tmp2);
3887     __ cbnz(tmp1, NOT_EQUAL);
3888     __ br(__ GE, LOOP);
3889   }
3890 
3891   // a1 = r1 - array1 address
3892   // a2 = r2 - array2 address
3893   // result = r0 - return value. Already contains &quot;false&quot;
3894   // cnt1 = r10 - amount of elements left to check, reduced by wordSize
3895   // r3-r5 are reserved temporary registers
3896   address generate_large_array_equals() {
3897     Register a1 = r1, a2 = r2, result = r0, cnt1 = r10, tmp1 = rscratch1,
3898         tmp2 = rscratch2, tmp3 = r3, tmp4 = r4, tmp5 = r5, tmp6 = r11,
3899         tmp7 = r12, tmp8 = r13;
3900     Label TAIL, NOT_EQUAL, EQUAL, NOT_EQUAL_NO_POP, NO_PREFETCH_LARGE_LOOP,
3901         SMALL_LOOP, POST_LOOP;
3902     const int PRE_LOOP_SIZE = UseSIMDForArrayEquals ? 0 : 16;
3903     // calculate if at least 32 prefetched bytes are used
3904     int prefetchLoopThreshold = SoftwarePrefetchHintDistance + 32;
3905     int nonPrefetchLoopThreshold = (64 + PRE_LOOP_SIZE);
3906     RegSet spilled_regs = RegSet::range(tmp6, tmp8);
3907     assert_different_registers(a1, a2, result, cnt1, tmp1, tmp2, tmp3, tmp4,
3908         tmp5, tmp6, tmp7, tmp8);
3909 
3910     __ align(CodeEntryAlignment);
3911 
3912     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;large_array_equals&quot;);
3913 
3914     address entry = __ pc();
3915     __ enter();
3916     __ sub(cnt1, cnt1, wordSize);  // first 8 bytes were loaded outside of stub
3917     // also advance pointers to use post-increment instead of pre-increment
3918     __ add(a1, a1, wordSize);
3919     __ add(a2, a2, wordSize);
3920     if (AvoidUnalignedAccesses) {
3921       // both implementations (SIMD/nonSIMD) are using relatively large load
3922       // instructions (ld1/ldp), which has huge penalty (up to x2 exec time)
3923       // on some CPUs in case of address is not at least 16-byte aligned.
3924       // Arrays are 8-byte aligned currently, so, we can make additional 8-byte
3925       // load if needed at least for 1st address and make if 16-byte aligned.
3926       Label ALIGNED16;
3927       __ tbz(a1, 3, ALIGNED16);
3928       __ ldr(tmp1, Address(__ post(a1, wordSize)));
3929       __ ldr(tmp2, Address(__ post(a2, wordSize)));
3930       __ sub(cnt1, cnt1, wordSize);
3931       __ eor(tmp1, tmp1, tmp2);
3932       __ cbnz(tmp1, NOT_EQUAL_NO_POP);
3933       __ bind(ALIGNED16);
3934     }
3935     if (UseSIMDForArrayEquals) {
3936       if (SoftwarePrefetchHintDistance &gt;= 0) {
3937         __ subs(tmp1, cnt1, prefetchLoopThreshold);
3938         __ br(__ LE, NO_PREFETCH_LARGE_LOOP);
3939         generate_large_array_equals_loop_simd(prefetchLoopThreshold,
3940             /* prfm = */ true, NOT_EQUAL);
3941         __ subs(zr, cnt1, nonPrefetchLoopThreshold);
3942         __ br(__ LT, TAIL);
3943       }
3944       __ bind(NO_PREFETCH_LARGE_LOOP);
3945       generate_large_array_equals_loop_simd(nonPrefetchLoopThreshold,
3946           /* prfm = */ false, NOT_EQUAL);
3947     } else {
3948       __ push(spilled_regs, sp);
3949       if (SoftwarePrefetchHintDistance &gt;= 0) {
3950         __ subs(tmp1, cnt1, prefetchLoopThreshold);
3951         __ br(__ LE, NO_PREFETCH_LARGE_LOOP);
3952         generate_large_array_equals_loop_nonsimd(prefetchLoopThreshold,
3953             /* prfm = */ true, NOT_EQUAL);
3954         __ subs(zr, cnt1, nonPrefetchLoopThreshold);
3955         __ br(__ LT, TAIL);
3956       }
3957       __ bind(NO_PREFETCH_LARGE_LOOP);
3958       generate_large_array_equals_loop_nonsimd(nonPrefetchLoopThreshold,
3959           /* prfm = */ false, NOT_EQUAL);
3960     }
3961     __ bind(TAIL);
3962       __ cbz(cnt1, EQUAL);
3963       __ subs(cnt1, cnt1, wordSize);
3964       __ br(__ LE, POST_LOOP);
3965     __ bind(SMALL_LOOP);
3966       __ ldr(tmp1, Address(__ post(a1, wordSize)));
3967       __ ldr(tmp2, Address(__ post(a2, wordSize)));
3968       __ subs(cnt1, cnt1, wordSize);
3969       __ eor(tmp1, tmp1, tmp2);
3970       __ cbnz(tmp1, NOT_EQUAL);
3971       __ br(__ GT, SMALL_LOOP);
3972     __ bind(POST_LOOP);
3973       __ ldr(tmp1, Address(a1, cnt1));
3974       __ ldr(tmp2, Address(a2, cnt1));
3975       __ eor(tmp1, tmp1, tmp2);
3976       __ cbnz(tmp1, NOT_EQUAL);
3977     __ bind(EQUAL);
3978       __ mov(result, true);
3979     __ bind(NOT_EQUAL);
3980       if (!UseSIMDForArrayEquals) {
3981         __ pop(spilled_regs, sp);
3982       }
3983     __ bind(NOT_EQUAL_NO_POP);
3984     __ leave();
3985     __ ret(lr);
3986     return entry;
3987   }
3988 
3989   address generate_dsin_dcos(bool isCos) {
3990     __ align(CodeEntryAlignment);
3991     StubCodeMark mark(this, &quot;StubRoutines&quot;, isCos ? &quot;libmDcos&quot; : &quot;libmDsin&quot;);
3992     address start = __ pc();
3993     __ generate_dsin_dcos(isCos, (address)StubRoutines::aarch64::_npio2_hw,
3994         (address)StubRoutines::aarch64::_two_over_pi,
3995         (address)StubRoutines::aarch64::_pio2,
3996         (address)StubRoutines::aarch64::_dsin_coef,
3997         (address)StubRoutines::aarch64::_dcos_coef);
3998     return start;
3999   }
4000 
4001   address generate_dlog() {
4002     __ align(CodeEntryAlignment);
4003     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;dlog&quot;);
4004     address entry = __ pc();
4005     FloatRegister vtmp0 = v0, vtmp1 = v1, vtmp2 = v2, vtmp3 = v3, vtmp4 = v4,
4006         vtmp5 = v5, tmpC1 = v16, tmpC2 = v17, tmpC3 = v18, tmpC4 = v19;
4007     Register tmp1 = r0, tmp2 = r1, tmp3 = r2, tmp4 = r3, tmp5 = r4;
4008     __ fast_log(vtmp0, vtmp1, vtmp2, vtmp3, vtmp4, vtmp5, tmpC1, tmpC2, tmpC3,
4009         tmpC4, tmp1, tmp2, tmp3, tmp4, tmp5);
4010     return entry;
4011   }
4012 
4013   // code for comparing 16 bytes of strings with same encoding
4014   void compare_string_16_bytes_same(Label &amp;DIFF1, Label &amp;DIFF2) {
4015     Register result = r0, str1 = r1, cnt1 = r2, str2 = r3, tmp1 = r10, tmp2 = r11;
4016     __ ldr(rscratch1, Address(__ post(str1, 8)));
4017     __ eor(rscratch2, tmp1, tmp2);
4018     __ ldr(cnt1, Address(__ post(str2, 8)));
4019     __ cbnz(rscratch2, DIFF1);
4020     __ ldr(tmp1, Address(__ post(str1, 8)));
4021     __ eor(rscratch2, rscratch1, cnt1);
4022     __ ldr(tmp2, Address(__ post(str2, 8)));
4023     __ cbnz(rscratch2, DIFF2);
4024   }
4025 
4026   // code for comparing 16 characters of strings with Latin1 and Utf16 encoding
4027   void compare_string_16_x_LU(Register tmpL, Register tmpU, Label &amp;DIFF1,
4028       Label &amp;DIFF2) {
4029     Register cnt1 = r2, tmp2 = r11, tmp3 = r12;
4030     FloatRegister vtmp = v1, vtmpZ = v0, vtmp3 = v2;
4031 
4032     __ ldrq(vtmp, Address(__ post(tmp2, 16)));
4033     __ ldr(tmpU, Address(__ post(cnt1, 8)));
4034     __ zip1(vtmp3, __ T16B, vtmp, vtmpZ);
4035     // now we have 32 bytes of characters (converted to U) in vtmp:vtmp3
4036 
4037     __ fmovd(tmpL, vtmp3);
4038     __ eor(rscratch2, tmp3, tmpL);
4039     __ cbnz(rscratch2, DIFF2);
4040 
4041     __ ldr(tmp3, Address(__ post(cnt1, 8)));
4042     __ umov(tmpL, vtmp3, __ D, 1);
4043     __ eor(rscratch2, tmpU, tmpL);
4044     __ cbnz(rscratch2, DIFF1);
4045 
4046     __ zip2(vtmp, __ T16B, vtmp, vtmpZ);
4047     __ ldr(tmpU, Address(__ post(cnt1, 8)));
4048     __ fmovd(tmpL, vtmp);
4049     __ eor(rscratch2, tmp3, tmpL);
4050     __ cbnz(rscratch2, DIFF2);
4051 
4052     __ ldr(tmp3, Address(__ post(cnt1, 8)));
4053     __ umov(tmpL, vtmp, __ D, 1);
4054     __ eor(rscratch2, tmpU, tmpL);
4055     __ cbnz(rscratch2, DIFF1);
4056   }
4057 
4058   // r0  = result
4059   // r1  = str1
4060   // r2  = cnt1
4061   // r3  = str2
4062   // r4  = cnt2
4063   // r10 = tmp1
4064   // r11 = tmp2
4065   address generate_compare_long_string_different_encoding(bool isLU) {
4066     __ align(CodeEntryAlignment);
4067     StubCodeMark mark(this, &quot;StubRoutines&quot;, isLU
4068         ? &quot;compare_long_string_different_encoding LU&quot;
4069         : &quot;compare_long_string_different_encoding UL&quot;);
4070     address entry = __ pc();
4071     Label SMALL_LOOP, TAIL, TAIL_LOAD_16, LOAD_LAST, DIFF1, DIFF2,
4072         DONE, CALCULATE_DIFFERENCE, LARGE_LOOP_PREFETCH, NO_PREFETCH,
4073         LARGE_LOOP_PREFETCH_REPEAT1, LARGE_LOOP_PREFETCH_REPEAT2;
4074     Register result = r0, str1 = r1, cnt1 = r2, str2 = r3, cnt2 = r4,
4075         tmp1 = r10, tmp2 = r11, tmp3 = r12, tmp4 = r14;
4076     FloatRegister vtmpZ = v0, vtmp = v1, vtmp3 = v2;
4077     RegSet spilled_regs = RegSet::of(tmp3, tmp4);
4078 
4079     int prefetchLoopExitCondition = MAX(64, SoftwarePrefetchHintDistance/2);
4080 
4081     __ eor(vtmpZ, __ T16B, vtmpZ, vtmpZ);
4082     // cnt2 == amount of characters left to compare
4083     // Check already loaded first 4 symbols(vtmp and tmp2(LU)/tmp1(UL))
4084     __ zip1(vtmp, __ T8B, vtmp, vtmpZ);
4085     __ add(str1, str1, isLU ? wordSize/2 : wordSize);
4086     __ add(str2, str2, isLU ? wordSize : wordSize/2);
4087     __ fmovd(isLU ? tmp1 : tmp2, vtmp);
4088     __ subw(cnt2, cnt2, 8); // Already loaded 4 symbols. Last 4 is special case.
4089     __ eor(rscratch2, tmp1, tmp2);
4090     __ mov(rscratch1, tmp2);
4091     __ cbnz(rscratch2, CALCULATE_DIFFERENCE);
4092     Register tmpU = isLU ? rscratch1 : tmp1, // where to keep U for comparison
4093              tmpL = isLU ? tmp1 : rscratch1; // where to keep L for comparison
4094     __ push(spilled_regs, sp);
4095     __ mov(tmp2, isLU ? str1 : str2); // init the pointer to L next load
4096     __ mov(cnt1, isLU ? str2 : str1); // init the pointer to U next load
4097 
4098     __ ldr(tmp3, Address(__ post(cnt1, 8)));
4099 
4100     if (SoftwarePrefetchHintDistance &gt;= 0) {
4101       __ subs(rscratch2, cnt2, prefetchLoopExitCondition);
4102       __ br(__ LT, NO_PREFETCH);
4103       __ bind(LARGE_LOOP_PREFETCH);
4104         __ prfm(Address(tmp2, SoftwarePrefetchHintDistance));
4105         __ mov(tmp4, 2);
4106         __ prfm(Address(cnt1, SoftwarePrefetchHintDistance));
4107         __ bind(LARGE_LOOP_PREFETCH_REPEAT1);
4108           compare_string_16_x_LU(tmpL, tmpU, DIFF1, DIFF2);
4109           __ subs(tmp4, tmp4, 1);
4110           __ br(__ GT, LARGE_LOOP_PREFETCH_REPEAT1);
4111           __ prfm(Address(cnt1, SoftwarePrefetchHintDistance));
4112           __ mov(tmp4, 2);
4113         __ bind(LARGE_LOOP_PREFETCH_REPEAT2);
4114           compare_string_16_x_LU(tmpL, tmpU, DIFF1, DIFF2);
4115           __ subs(tmp4, tmp4, 1);
4116           __ br(__ GT, LARGE_LOOP_PREFETCH_REPEAT2);
4117           __ sub(cnt2, cnt2, 64);
4118           __ subs(rscratch2, cnt2, prefetchLoopExitCondition);
4119           __ br(__ GE, LARGE_LOOP_PREFETCH);
4120     }
4121     __ cbz(cnt2, LOAD_LAST); // no characters left except last load
4122     __ bind(NO_PREFETCH);
4123     __ subs(cnt2, cnt2, 16);
4124     __ br(__ LT, TAIL);
4125     __ align(OptoLoopAlignment);
4126     __ bind(SMALL_LOOP); // smaller loop
4127       __ subs(cnt2, cnt2, 16);
4128       compare_string_16_x_LU(tmpL, tmpU, DIFF1, DIFF2);
4129       __ br(__ GE, SMALL_LOOP);
4130       __ cmn(cnt2, (u1)16);
4131       __ br(__ EQ, LOAD_LAST);
4132     __ bind(TAIL); // 1..15 characters left until last load (last 4 characters)
4133       __ add(cnt1, cnt1, cnt2, __ LSL, 1); // Address of 32 bytes before last 4 characters in UTF-16 string
4134       __ add(tmp2, tmp2, cnt2); // Address of 16 bytes before last 4 characters in Latin1 string
4135       __ ldr(tmp3, Address(cnt1, -8));
4136       compare_string_16_x_LU(tmpL, tmpU, DIFF1, DIFF2); // last 16 characters before last load
4137       __ b(LOAD_LAST);
4138     __ bind(DIFF2);
4139       __ mov(tmpU, tmp3);
4140     __ bind(DIFF1);
4141       __ pop(spilled_regs, sp);
4142       __ b(CALCULATE_DIFFERENCE);
4143     __ bind(LOAD_LAST);
4144       // Last 4 UTF-16 characters are already pre-loaded into tmp3 by compare_string_16_x_LU.
4145       // No need to load it again
4146       __ mov(tmpU, tmp3);
4147       __ pop(spilled_regs, sp);
4148 
4149       // tmp2 points to the address of the last 4 Latin1 characters right now
4150       __ ldrs(vtmp, Address(tmp2));
4151       __ zip1(vtmp, __ T8B, vtmp, vtmpZ);
4152       __ fmovd(tmpL, vtmp);
4153 
4154       __ eor(rscratch2, tmpU, tmpL);
4155       __ cbz(rscratch2, DONE);
4156 
4157     // Find the first different characters in the longwords and
4158     // compute their difference.
4159     __ bind(CALCULATE_DIFFERENCE);
4160       __ rev(rscratch2, rscratch2);
4161       __ clz(rscratch2, rscratch2);
4162       __ andr(rscratch2, rscratch2, -16);
4163       __ lsrv(tmp1, tmp1, rscratch2);
4164       __ uxthw(tmp1, tmp1);
4165       __ lsrv(rscratch1, rscratch1, rscratch2);
4166       __ uxthw(rscratch1, rscratch1);
4167       __ subw(result, tmp1, rscratch1);
4168     __ bind(DONE);
4169       __ ret(lr);
4170     return entry;
4171   }
4172 
<a name="2" id="anc2"></a>











































4173   // r0  = result
4174   // r1  = str1
4175   // r2  = cnt1
4176   // r3  = str2
4177   // r4  = cnt2
4178   // r10 = tmp1
4179   // r11 = tmp2
4180   address generate_compare_long_string_same_encoding(bool isLL) {
4181     __ align(CodeEntryAlignment);
4182     StubCodeMark mark(this, &quot;StubRoutines&quot;, isLL
4183         ? &quot;compare_long_string_same_encoding LL&quot;
4184         : &quot;compare_long_string_same_encoding UU&quot;);
4185     address entry = __ pc();
4186     Register result = r0, str1 = r1, cnt1 = r2, str2 = r3, cnt2 = r4,
4187         tmp1 = r10, tmp2 = r11;
4188     Label SMALL_LOOP, LARGE_LOOP_PREFETCH, CHECK_LAST, DIFF2, TAIL,
4189         LENGTH_DIFF, DIFF, LAST_CHECK_AND_LENGTH_DIFF,
4190         DIFF_LAST_POSITION, DIFF_LAST_POSITION2;
4191     // exit from large loop when less than 64 bytes left to read or we&#39;re about
4192     // to prefetch memory behind array border
4193     int largeLoopExitCondition = MAX(64, SoftwarePrefetchHintDistance)/(isLL ? 1 : 2);
4194     // cnt1/cnt2 contains amount of characters to compare. cnt1 can be re-used
4195     // update cnt2 counter with already loaded 8 bytes
4196     __ sub(cnt2, cnt2, wordSize/(isLL ? 1 : 2));
4197     // update pointers, because of previous read
4198     __ add(str1, str1, wordSize);
4199     __ add(str2, str2, wordSize);
4200     if (SoftwarePrefetchHintDistance &gt;= 0) {
4201       __ bind(LARGE_LOOP_PREFETCH);
4202         __ prfm(Address(str1, SoftwarePrefetchHintDistance));
4203         __ prfm(Address(str2, SoftwarePrefetchHintDistance));
4204         compare_string_16_bytes_same(DIFF, DIFF2);
4205         compare_string_16_bytes_same(DIFF, DIFF2);
4206         __ sub(cnt2, cnt2, isLL ? 64 : 32);
4207         compare_string_16_bytes_same(DIFF, DIFF2);
4208         __ subs(rscratch2, cnt2, largeLoopExitCondition);
4209         compare_string_16_bytes_same(DIFF, DIFF2);
4210         __ br(__ GT, LARGE_LOOP_PREFETCH);
4211         __ cbz(cnt2, LAST_CHECK_AND_LENGTH_DIFF); // no more chars left?
4212     }
4213     // less than 16 bytes left?
4214     __ subs(cnt2, cnt2, isLL ? 16 : 8);
4215     __ br(__ LT, TAIL);
4216     __ align(OptoLoopAlignment);
4217     __ bind(SMALL_LOOP);
4218       compare_string_16_bytes_same(DIFF, DIFF2);
4219       __ subs(cnt2, cnt2, isLL ? 16 : 8);
4220       __ br(__ GE, SMALL_LOOP);
4221     __ bind(TAIL);
4222       __ adds(cnt2, cnt2, isLL ? 16 : 8);
4223       __ br(__ EQ, LAST_CHECK_AND_LENGTH_DIFF);
4224       __ subs(cnt2, cnt2, isLL ? 8 : 4);
4225       __ br(__ LE, CHECK_LAST);
4226       __ eor(rscratch2, tmp1, tmp2);
4227       __ cbnz(rscratch2, DIFF);
4228       __ ldr(tmp1, Address(__ post(str1, 8)));
4229       __ ldr(tmp2, Address(__ post(str2, 8)));
4230       __ sub(cnt2, cnt2, isLL ? 8 : 4);
4231     __ bind(CHECK_LAST);
4232       if (!isLL) {
4233         __ add(cnt2, cnt2, cnt2); // now in bytes
4234       }
4235       __ eor(rscratch2, tmp1, tmp2);
4236       __ cbnz(rscratch2, DIFF);
4237       __ ldr(rscratch1, Address(str1, cnt2));
4238       __ ldr(cnt1, Address(str2, cnt2));
4239       __ eor(rscratch2, rscratch1, cnt1);
4240       __ cbz(rscratch2, LENGTH_DIFF);
4241       // Find the first different characters in the longwords and
4242       // compute their difference.
4243     __ bind(DIFF2);
4244       __ rev(rscratch2, rscratch2);
4245       __ clz(rscratch2, rscratch2);
4246       __ andr(rscratch2, rscratch2, isLL ? -8 : -16);
4247       __ lsrv(rscratch1, rscratch1, rscratch2);
4248       if (isLL) {
4249         __ lsrv(cnt1, cnt1, rscratch2);
4250         __ uxtbw(rscratch1, rscratch1);
4251         __ uxtbw(cnt1, cnt1);
4252       } else {
4253         __ lsrv(cnt1, cnt1, rscratch2);
4254         __ uxthw(rscratch1, rscratch1);
4255         __ uxthw(cnt1, cnt1);
4256       }
4257       __ subw(result, rscratch1, cnt1);
4258       __ b(LENGTH_DIFF);
4259     __ bind(DIFF);
4260       __ rev(rscratch2, rscratch2);
4261       __ clz(rscratch2, rscratch2);
4262       __ andr(rscratch2, rscratch2, isLL ? -8 : -16);
4263       __ lsrv(tmp1, tmp1, rscratch2);
4264       if (isLL) {
4265         __ lsrv(tmp2, tmp2, rscratch2);
4266         __ uxtbw(tmp1, tmp1);
4267         __ uxtbw(tmp2, tmp2);
4268       } else {
4269         __ lsrv(tmp2, tmp2, rscratch2);
4270         __ uxthw(tmp1, tmp1);
4271         __ uxthw(tmp2, tmp2);
4272       }
4273       __ subw(result, tmp1, tmp2);
4274       __ b(LENGTH_DIFF);
4275     __ bind(LAST_CHECK_AND_LENGTH_DIFF);
4276       __ eor(rscratch2, tmp1, tmp2);
4277       __ cbnz(rscratch2, DIFF);
4278     __ bind(LENGTH_DIFF);
4279       __ ret(lr);
4280     return entry;
4281   }
4282 
4283   void generate_compare_long_strings() {
4284       StubRoutines::aarch64::_compare_long_string_LL
4285           = generate_compare_long_string_same_encoding(true);
4286       StubRoutines::aarch64::_compare_long_string_UU
4287           = generate_compare_long_string_same_encoding(false);
4288       StubRoutines::aarch64::_compare_long_string_LU
4289           = generate_compare_long_string_different_encoding(true);
4290       StubRoutines::aarch64::_compare_long_string_UL
4291           = generate_compare_long_string_different_encoding(false);
4292   }
4293 
4294   // R0 = result
4295   // R1 = str2
4296   // R2 = cnt1
4297   // R3 = str1
4298   // R4 = cnt2
4299   // This generic linear code use few additional ideas, which makes it faster:
4300   // 1) we can safely keep at least 1st register of pattern(since length &gt;= 8)
4301   // in order to skip initial loading(help in systems with 1 ld pipeline)
4302   // 2) we can use &quot;fast&quot; algorithm of finding single character to search for
4303   // first symbol with less branches(1 branch per each loaded register instead
4304   // of branch for each symbol), so, this is where constants like
4305   // 0x0101...01, 0x00010001...0001, 0x7f7f...7f, 0x7fff7fff...7fff comes from
4306   // 3) after loading and analyzing 1st register of source string, it can be
4307   // used to search for every 1st character entry, saving few loads in
4308   // comparison with &quot;simplier-but-slower&quot; implementation
4309   // 4) in order to avoid lots of push/pop operations, code below is heavily
4310   // re-using/re-initializing/compressing register values, which makes code
4311   // larger and a bit less readable, however, most of extra operations are
4312   // issued during loads or branches, so, penalty is minimal
4313   address generate_string_indexof_linear(bool str1_isL, bool str2_isL) {
4314     const char* stubName = str1_isL
4315         ? (str2_isL ? &quot;indexof_linear_ll&quot; : &quot;indexof_linear_ul&quot;)
4316         : &quot;indexof_linear_uu&quot;;
4317     __ align(CodeEntryAlignment);
4318     StubCodeMark mark(this, &quot;StubRoutines&quot;, stubName);
4319     address entry = __ pc();
4320 
4321     int str1_chr_size = str1_isL ? 1 : 2;
4322     int str2_chr_size = str2_isL ? 1 : 2;
4323     int str1_chr_shift = str1_isL ? 0 : 1;
4324     int str2_chr_shift = str2_isL ? 0 : 1;
4325     bool isL = str1_isL &amp;&amp; str2_isL;
4326    // parameters
4327     Register result = r0, str2 = r1, cnt1 = r2, str1 = r3, cnt2 = r4;
4328     // temporary registers
4329     Register tmp1 = r20, tmp2 = r21, tmp3 = r22, tmp4 = r23;
4330     RegSet spilled_regs = RegSet::range(tmp1, tmp4);
4331     // redefinitions
4332     Register ch1 = rscratch1, ch2 = rscratch2, first = tmp3;
4333 
4334     __ push(spilled_regs, sp);
4335     Label L_LOOP, L_LOOP_PROCEED, L_SMALL, L_HAS_ZERO,
4336         L_HAS_ZERO_LOOP, L_CMP_LOOP, L_CMP_LOOP_NOMATCH, L_SMALL_PROCEED,
4337         L_SMALL_HAS_ZERO_LOOP, L_SMALL_CMP_LOOP_NOMATCH, L_SMALL_CMP_LOOP,
4338         L_POST_LOOP, L_CMP_LOOP_LAST_CMP, L_HAS_ZERO_LOOP_NOMATCH,
4339         L_SMALL_CMP_LOOP_LAST_CMP, L_SMALL_CMP_LOOP_LAST_CMP2,
4340         L_CMP_LOOP_LAST_CMP2, DONE, NOMATCH;
4341     // Read whole register from str1. It is safe, because length &gt;=8 here
4342     __ ldr(ch1, Address(str1));
4343     // Read whole register from str2. It is safe, because length &gt;=8 here
4344     __ ldr(ch2, Address(str2));
4345     __ sub(cnt2, cnt2, cnt1);
4346     __ andr(first, ch1, str1_isL ? 0xFF : 0xFFFF);
4347     if (str1_isL != str2_isL) {
4348       __ eor(v0, __ T16B, v0, v0);
4349     }
4350     __ mov(tmp1, str2_isL ? 0x0101010101010101 : 0x0001000100010001);
4351     __ mul(first, first, tmp1);
4352     // check if we have less than 1 register to check
4353     __ subs(cnt2, cnt2, wordSize/str2_chr_size - 1);
4354     if (str1_isL != str2_isL) {
4355       __ fmovd(v1, ch1);
4356     }
4357     __ br(__ LE, L_SMALL);
4358     __ eor(ch2, first, ch2);
4359     if (str1_isL != str2_isL) {
4360       __ zip1(v1, __ T16B, v1, v0);
4361     }
4362     __ sub(tmp2, ch2, tmp1);
4363     __ orr(ch2, ch2, str2_isL ? 0x7f7f7f7f7f7f7f7f : 0x7fff7fff7fff7fff);
4364     __ bics(tmp2, tmp2, ch2);
4365     if (str1_isL != str2_isL) {
4366       __ fmovd(ch1, v1);
4367     }
4368     __ br(__ NE, L_HAS_ZERO);
4369     __ subs(cnt2, cnt2, wordSize/str2_chr_size);
4370     __ add(result, result, wordSize/str2_chr_size);
4371     __ add(str2, str2, wordSize);
4372     __ br(__ LT, L_POST_LOOP);
4373     __ BIND(L_LOOP);
4374       __ ldr(ch2, Address(str2));
4375       __ eor(ch2, first, ch2);
4376       __ sub(tmp2, ch2, tmp1);
4377       __ orr(ch2, ch2, str2_isL ? 0x7f7f7f7f7f7f7f7f : 0x7fff7fff7fff7fff);
4378       __ bics(tmp2, tmp2, ch2);
4379       __ br(__ NE, L_HAS_ZERO);
4380     __ BIND(L_LOOP_PROCEED);
4381       __ subs(cnt2, cnt2, wordSize/str2_chr_size);
4382       __ add(str2, str2, wordSize);
4383       __ add(result, result, wordSize/str2_chr_size);
4384       __ br(__ GE, L_LOOP);
4385     __ BIND(L_POST_LOOP);
4386       __ subs(zr, cnt2, -wordSize/str2_chr_size); // no extra characters to check
4387       __ br(__ LE, NOMATCH);
4388       __ ldr(ch2, Address(str2));
4389       __ sub(cnt2, zr, cnt2, __ LSL, LogBitsPerByte + str2_chr_shift);
4390       __ eor(ch2, first, ch2);
4391       __ sub(tmp2, ch2, tmp1);
4392       __ orr(ch2, ch2, str2_isL ? 0x7f7f7f7f7f7f7f7f : 0x7fff7fff7fff7fff);
4393       __ mov(tmp4, -1); // all bits set
4394       __ b(L_SMALL_PROCEED);
4395     __ align(OptoLoopAlignment);
4396     __ BIND(L_SMALL);
4397       __ sub(cnt2, zr, cnt2, __ LSL, LogBitsPerByte + str2_chr_shift);
4398       __ eor(ch2, first, ch2);
4399       if (str1_isL != str2_isL) {
4400         __ zip1(v1, __ T16B, v1, v0);
4401       }
4402       __ sub(tmp2, ch2, tmp1);
4403       __ mov(tmp4, -1); // all bits set
4404       __ orr(ch2, ch2, str2_isL ? 0x7f7f7f7f7f7f7f7f : 0x7fff7fff7fff7fff);
4405       if (str1_isL != str2_isL) {
4406         __ fmovd(ch1, v1); // move converted 4 symbols
4407       }
4408     __ BIND(L_SMALL_PROCEED);
4409       __ lsrv(tmp4, tmp4, cnt2); // mask. zeroes on useless bits.
4410       __ bic(tmp2, tmp2, ch2);
4411       __ ands(tmp2, tmp2, tmp4); // clear useless bits and check
4412       __ rbit(tmp2, tmp2);
4413       __ br(__ EQ, NOMATCH);
4414     __ BIND(L_SMALL_HAS_ZERO_LOOP);
4415       __ clz(tmp4, tmp2); // potentially long. Up to 4 cycles on some cpu&#39;s
4416       __ cmp(cnt1, u1(wordSize/str2_chr_size));
4417       __ br(__ LE, L_SMALL_CMP_LOOP_LAST_CMP2);
4418       if (str2_isL) { // LL
4419         __ add(str2, str2, tmp4, __ LSR, LogBitsPerByte); // address of &quot;index&quot;
4420         __ ldr(ch2, Address(str2)); // read whole register of str2. Safe.
4421         __ lslv(tmp2, tmp2, tmp4); // shift off leading zeroes from match info
4422         __ add(result, result, tmp4, __ LSR, LogBitsPerByte);
4423         __ lsl(tmp2, tmp2, 1); // shift off leading &quot;1&quot; from match info
4424       } else {
4425         __ mov(ch2, 0xE); // all bits in byte set except last one
4426         __ andr(ch2, ch2, tmp4, __ LSR, LogBitsPerByte); // byte shift amount
4427         __ ldr(ch2, Address(str2, ch2)); // read whole register of str2. Safe.
4428         __ lslv(tmp2, tmp2, tmp4);
4429         __ add(result, result, tmp4, __ LSR, LogBitsPerByte + str2_chr_shift);
4430         __ add(str2, str2, tmp4, __ LSR, LogBitsPerByte + str2_chr_shift);
4431         __ lsl(tmp2, tmp2, 1); // shift off leading &quot;1&quot; from match info
4432         __ add(str2, str2, tmp4, __ LSR, LogBitsPerByte + str2_chr_shift);
4433       }
4434       __ cmp(ch1, ch2);
4435       __ mov(tmp4, wordSize/str2_chr_size);
4436       __ br(__ NE, L_SMALL_CMP_LOOP_NOMATCH);
4437     __ BIND(L_SMALL_CMP_LOOP);
4438       str1_isL ? __ ldrb(first, Address(str1, tmp4, Address::lsl(str1_chr_shift)))
4439                : __ ldrh(first, Address(str1, tmp4, Address::lsl(str1_chr_shift)));
4440       str2_isL ? __ ldrb(ch2, Address(str2, tmp4, Address::lsl(str2_chr_shift)))
4441                : __ ldrh(ch2, Address(str2, tmp4, Address::lsl(str2_chr_shift)));
4442       __ add(tmp4, tmp4, 1);
4443       __ cmp(tmp4, cnt1);
4444       __ br(__ GE, L_SMALL_CMP_LOOP_LAST_CMP);
4445       __ cmp(first, ch2);
4446       __ br(__ EQ, L_SMALL_CMP_LOOP);
4447     __ BIND(L_SMALL_CMP_LOOP_NOMATCH);
4448       __ cbz(tmp2, NOMATCH); // no more matches. exit
4449       __ clz(tmp4, tmp2);
4450       __ add(result, result, 1); // advance index
4451       __ add(str2, str2, str2_chr_size); // advance pointer
4452       __ b(L_SMALL_HAS_ZERO_LOOP);
4453     __ align(OptoLoopAlignment);
4454     __ BIND(L_SMALL_CMP_LOOP_LAST_CMP);
4455       __ cmp(first, ch2);
4456       __ br(__ NE, L_SMALL_CMP_LOOP_NOMATCH);
4457       __ b(DONE);
4458     __ align(OptoLoopAlignment);
4459     __ BIND(L_SMALL_CMP_LOOP_LAST_CMP2);
4460       if (str2_isL) { // LL
4461         __ add(str2, str2, tmp4, __ LSR, LogBitsPerByte); // address of &quot;index&quot;
4462         __ ldr(ch2, Address(str2)); // read whole register of str2. Safe.
4463         __ lslv(tmp2, tmp2, tmp4); // shift off leading zeroes from match info
4464         __ add(result, result, tmp4, __ LSR, LogBitsPerByte);
4465         __ lsl(tmp2, tmp2, 1); // shift off leading &quot;1&quot; from match info
4466       } else {
4467         __ mov(ch2, 0xE); // all bits in byte set except last one
4468         __ andr(ch2, ch2, tmp4, __ LSR, LogBitsPerByte); // byte shift amount
4469         __ ldr(ch2, Address(str2, ch2)); // read whole register of str2. Safe.
4470         __ lslv(tmp2, tmp2, tmp4);
4471         __ add(result, result, tmp4, __ LSR, LogBitsPerByte + str2_chr_shift);
4472         __ add(str2, str2, tmp4, __ LSR, LogBitsPerByte + str2_chr_shift);
4473         __ lsl(tmp2, tmp2, 1); // shift off leading &quot;1&quot; from match info
4474         __ add(str2, str2, tmp4, __ LSR, LogBitsPerByte + str2_chr_shift);
4475       }
4476       __ cmp(ch1, ch2);
4477       __ br(__ NE, L_SMALL_CMP_LOOP_NOMATCH);
4478       __ b(DONE);
4479     __ align(OptoLoopAlignment);
4480     __ BIND(L_HAS_ZERO);
4481       __ rbit(tmp2, tmp2);
4482       __ clz(tmp4, tmp2); // potentially long. Up to 4 cycles on some CPU&#39;s
4483       // Now, perform compression of counters(cnt2 and cnt1) into one register.
4484       // It&#39;s fine because both counters are 32bit and are not changed in this
4485       // loop. Just restore it on exit. So, cnt1 can be re-used in this loop.
4486       __ orr(cnt2, cnt2, cnt1, __ LSL, BitsPerByte * wordSize / 2);
4487       __ sub(result, result, 1);
4488     __ BIND(L_HAS_ZERO_LOOP);
4489       __ mov(cnt1, wordSize/str2_chr_size);
4490       __ cmp(cnt1, cnt2, __ LSR, BitsPerByte * wordSize / 2);
4491       __ br(__ GE, L_CMP_LOOP_LAST_CMP2); // case of 8 bytes only to compare
4492       if (str2_isL) {
4493         __ lsr(ch2, tmp4, LogBitsPerByte + str2_chr_shift); // char index
4494         __ ldr(ch2, Address(str2, ch2)); // read whole register of str2. Safe.
4495         __ lslv(tmp2, tmp2, tmp4);
4496         __ add(str2, str2, tmp4, __ LSR, LogBitsPerByte + str2_chr_shift);
4497         __ add(tmp4, tmp4, 1);
4498         __ add(result, result, tmp4, __ LSR, LogBitsPerByte + str2_chr_shift);
4499         __ lsl(tmp2, tmp2, 1);
4500         __ mov(tmp4, wordSize/str2_chr_size);
4501       } else {
4502         __ mov(ch2, 0xE);
4503         __ andr(ch2, ch2, tmp4, __ LSR, LogBitsPerByte); // byte shift amount
4504         __ ldr(ch2, Address(str2, ch2)); // read whole register of str2. Safe.
4505         __ lslv(tmp2, tmp2, tmp4);
4506         __ add(tmp4, tmp4, 1);
4507         __ add(result, result, tmp4, __ LSR, LogBitsPerByte + str2_chr_shift);
4508         __ add(str2, str2, tmp4, __ LSR, LogBitsPerByte);
4509         __ lsl(tmp2, tmp2, 1);
4510         __ mov(tmp4, wordSize/str2_chr_size);
4511         __ sub(str2, str2, str2_chr_size);
4512       }
4513       __ cmp(ch1, ch2);
4514       __ mov(tmp4, wordSize/str2_chr_size);
4515       __ br(__ NE, L_CMP_LOOP_NOMATCH);
4516     __ BIND(L_CMP_LOOP);
4517       str1_isL ? __ ldrb(cnt1, Address(str1, tmp4, Address::lsl(str1_chr_shift)))
4518                : __ ldrh(cnt1, Address(str1, tmp4, Address::lsl(str1_chr_shift)));
4519       str2_isL ? __ ldrb(ch2, Address(str2, tmp4, Address::lsl(str2_chr_shift)))
4520                : __ ldrh(ch2, Address(str2, tmp4, Address::lsl(str2_chr_shift)));
4521       __ add(tmp4, tmp4, 1);
4522       __ cmp(tmp4, cnt2, __ LSR, BitsPerByte * wordSize / 2);
4523       __ br(__ GE, L_CMP_LOOP_LAST_CMP);
4524       __ cmp(cnt1, ch2);
4525       __ br(__ EQ, L_CMP_LOOP);
4526     __ BIND(L_CMP_LOOP_NOMATCH);
4527       // here we&#39;re not matched
4528       __ cbz(tmp2, L_HAS_ZERO_LOOP_NOMATCH); // no more matches. Proceed to main loop
4529       __ clz(tmp4, tmp2);
4530       __ add(str2, str2, str2_chr_size); // advance pointer
4531       __ b(L_HAS_ZERO_LOOP);
4532     __ align(OptoLoopAlignment);
4533     __ BIND(L_CMP_LOOP_LAST_CMP);
4534       __ cmp(cnt1, ch2);
4535       __ br(__ NE, L_CMP_LOOP_NOMATCH);
4536       __ b(DONE);
4537     __ align(OptoLoopAlignment);
4538     __ BIND(L_CMP_LOOP_LAST_CMP2);
4539       if (str2_isL) {
4540         __ lsr(ch2, tmp4, LogBitsPerByte + str2_chr_shift); // char index
4541         __ ldr(ch2, Address(str2, ch2)); // read whole register of str2. Safe.
4542         __ lslv(tmp2, tmp2, tmp4);
4543         __ add(str2, str2, tmp4, __ LSR, LogBitsPerByte + str2_chr_shift);
4544         __ add(tmp4, tmp4, 1);
4545         __ add(result, result, tmp4, __ LSR, LogBitsPerByte + str2_chr_shift);
4546         __ lsl(tmp2, tmp2, 1);
4547       } else {
4548         __ mov(ch2, 0xE);
4549         __ andr(ch2, ch2, tmp4, __ LSR, LogBitsPerByte); // byte shift amount
4550         __ ldr(ch2, Address(str2, ch2)); // read whole register of str2. Safe.
4551         __ lslv(tmp2, tmp2, tmp4);
4552         __ add(tmp4, tmp4, 1);
4553         __ add(result, result, tmp4, __ LSR, LogBitsPerByte + str2_chr_shift);
4554         __ add(str2, str2, tmp4, __ LSR, LogBitsPerByte);
4555         __ lsl(tmp2, tmp2, 1);
4556         __ sub(str2, str2, str2_chr_size);
4557       }
4558       __ cmp(ch1, ch2);
4559       __ br(__ NE, L_CMP_LOOP_NOMATCH);
4560       __ b(DONE);
4561     __ align(OptoLoopAlignment);
4562     __ BIND(L_HAS_ZERO_LOOP_NOMATCH);
4563       // 1) Restore &quot;result&quot; index. Index was wordSize/str2_chr_size * N until
4564       // L_HAS_ZERO block. Byte octet was analyzed in L_HAS_ZERO_LOOP,
4565       // so, result was increased at max by wordSize/str2_chr_size - 1, so,
4566       // respective high bit wasn&#39;t changed. L_LOOP_PROCEED will increase
4567       // result by analyzed characters value, so, we can just reset lower bits
4568       // in result here. Clear 2 lower bits for UU/UL and 3 bits for LL
4569       // 2) restore cnt1 and cnt2 values from &quot;compressed&quot; cnt2
4570       // 3) advance str2 value to represent next str2 octet. result &amp; 7/3 is
4571       // index of last analyzed substring inside current octet. So, str2 in at
4572       // respective start address. We need to advance it to next octet
4573       __ andr(tmp2, result, wordSize/str2_chr_size - 1); // symbols analyzed
4574       __ lsr(cnt1, cnt2, BitsPerByte * wordSize / 2);
4575       __ bfm(result, zr, 0, 2 - str2_chr_shift);
4576       __ sub(str2, str2, tmp2, __ LSL, str2_chr_shift); // restore str2
4577       __ movw(cnt2, cnt2);
4578       __ b(L_LOOP_PROCEED);
4579     __ align(OptoLoopAlignment);
4580     __ BIND(NOMATCH);
4581       __ mov(result, -1);
4582     __ BIND(DONE);
4583       __ pop(spilled_regs, sp);
4584       __ ret(lr);
4585     return entry;
4586   }
4587 
4588   void generate_string_indexof_stubs() {
4589     StubRoutines::aarch64::_string_indexof_linear_ll = generate_string_indexof_linear(true, true);
4590     StubRoutines::aarch64::_string_indexof_linear_uu = generate_string_indexof_linear(false, false);
4591     StubRoutines::aarch64::_string_indexof_linear_ul = generate_string_indexof_linear(true, false);
4592   }
4593 
4594   void inflate_and_store_2_fp_registers(bool generatePrfm,
4595       FloatRegister src1, FloatRegister src2) {
4596     Register dst = r1;
4597     __ zip1(v1, __ T16B, src1, v0);
4598     __ zip2(v2, __ T16B, src1, v0);
4599     if (generatePrfm) {
4600       __ prfm(Address(dst, SoftwarePrefetchHintDistance), PSTL1STRM);
4601     }
4602     __ zip1(v3, __ T16B, src2, v0);
4603     __ zip2(v4, __ T16B, src2, v0);
4604     __ st1(v1, v2, v3, v4, __ T16B, Address(__ post(dst, 64)));
4605   }
4606 
4607   // R0 = src
4608   // R1 = dst
4609   // R2 = len
4610   // R3 = len &gt;&gt; 3
4611   // V0 = 0
4612   // v1 = loaded 8 bytes
4613   address generate_large_byte_array_inflate() {
4614     __ align(CodeEntryAlignment);
4615     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;large_byte_array_inflate&quot;);
4616     address entry = __ pc();
4617     Label LOOP, LOOP_START, LOOP_PRFM, LOOP_PRFM_START, DONE;
4618     Register src = r0, dst = r1, len = r2, octetCounter = r3;
4619     const int large_loop_threshold = MAX(64, SoftwarePrefetchHintDistance)/8 + 4;
4620 
4621     // do one more 8-byte read to have address 16-byte aligned in most cases
4622     // also use single store instruction
4623     __ ldrd(v2, __ post(src, 8));
4624     __ sub(octetCounter, octetCounter, 2);
4625     __ zip1(v1, __ T16B, v1, v0);
4626     __ zip1(v2, __ T16B, v2, v0);
4627     __ st1(v1, v2, __ T16B, __ post(dst, 32));
4628     __ ld1(v3, v4, v5, v6, __ T16B, Address(__ post(src, 64)));
4629     __ subs(rscratch1, octetCounter, large_loop_threshold);
4630     __ br(__ LE, LOOP_START);
4631     __ b(LOOP_PRFM_START);
4632     __ bind(LOOP_PRFM);
4633       __ ld1(v3, v4, v5, v6, __ T16B, Address(__ post(src, 64)));
4634     __ bind(LOOP_PRFM_START);
4635       __ prfm(Address(src, SoftwarePrefetchHintDistance));
4636       __ sub(octetCounter, octetCounter, 8);
4637       __ subs(rscratch1, octetCounter, large_loop_threshold);
4638       inflate_and_store_2_fp_registers(true, v3, v4);
4639       inflate_and_store_2_fp_registers(true, v5, v6);
4640       __ br(__ GT, LOOP_PRFM);
4641       __ cmp(octetCounter, (u1)8);
4642       __ br(__ LT, DONE);
4643     __ bind(LOOP);
4644       __ ld1(v3, v4, v5, v6, __ T16B, Address(__ post(src, 64)));
4645       __ bind(LOOP_START);
4646       __ sub(octetCounter, octetCounter, 8);
4647       __ cmp(octetCounter, (u1)8);
4648       inflate_and_store_2_fp_registers(false, v3, v4);
4649       inflate_and_store_2_fp_registers(false, v5, v6);
4650       __ br(__ GE, LOOP);
4651     __ bind(DONE);
4652       __ ret(lr);
4653     return entry;
4654   }
4655 
4656   /**
4657    *  Arguments:
4658    *
4659    *  Input:
4660    *  c_rarg0   - current state address
4661    *  c_rarg1   - H key address
4662    *  c_rarg2   - data address
4663    *  c_rarg3   - number of blocks
4664    *
4665    *  Output:
4666    *  Updated state at c_rarg0
4667    */
4668   address generate_ghash_processBlocks() {
4669     // Bafflingly, GCM uses little-endian for the byte order, but
4670     // big-endian for the bit order.  For example, the polynomial 1 is
4671     // represented as the 16-byte string 80 00 00 00 | 12 bytes of 00.
4672     //
4673     // So, we must either reverse the bytes in each word and do
4674     // everything big-endian or reverse the bits in each byte and do
4675     // it little-endian.  On AArch64 it&#39;s more idiomatic to reverse
4676     // the bits in each byte (we have an instruction, RBIT, to do
4677     // that) and keep the data in little-endian bit order throught the
4678     // calculation, bit-reversing the inputs and outputs.
4679 
4680     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;ghash_processBlocks&quot;);
4681     __ align(wordSize * 2);
4682     address p = __ pc();
4683     __ emit_int64(0x87);  // The low-order bits of the field
4684                           // polynomial (i.e. p = z^7+z^2+z+1)
4685                           // repeated in the low and high parts of a
4686                           // 128-bit vector
4687     __ emit_int64(0x87);
4688 
4689     __ align(CodeEntryAlignment);
4690     address start = __ pc();
4691 
4692     Register state   = c_rarg0;
4693     Register subkeyH = c_rarg1;
4694     Register data    = c_rarg2;
4695     Register blocks  = c_rarg3;
4696 
4697     FloatRegister vzr = v30;
4698     __ eor(vzr, __ T16B, vzr, vzr); // zero register
4699 
4700     __ ldrq(v0, Address(state));
4701     __ ldrq(v1, Address(subkeyH));
4702 
4703     __ rev64(v0, __ T16B, v0);          // Bit-reverse words in state and subkeyH
4704     __ rbit(v0, __ T16B, v0);
4705     __ rev64(v1, __ T16B, v1);
4706     __ rbit(v1, __ T16B, v1);
4707 
4708     __ ldrq(v26, p);
4709 
4710     __ ext(v16, __ T16B, v1, v1, 0x08); // long-swap subkeyH into v1
4711     __ eor(v16, __ T16B, v16, v1);      // xor subkeyH into subkeyL (Karatsuba: (A1+A0))
4712 
4713     {
4714       Label L_ghash_loop;
4715       __ bind(L_ghash_loop);
4716 
4717       __ ldrq(v2, Address(__ post(data, 0x10))); // Load the data, bit
4718                                                  // reversing each byte
4719       __ rbit(v2, __ T16B, v2);
4720       __ eor(v2, __ T16B, v0, v2);   // bit-swapped data ^ bit-swapped state
4721 
4722       // Multiply state in v2 by subkey in v1
4723       ghash_multiply(/*result_lo*/v5, /*result_hi*/v7,
4724                      /*a*/v1, /*b*/v2, /*a1_xor_a0*/v16,
4725                      /*temps*/v6, v20, v18, v21);
4726       // Reduce v7:v5 by the field polynomial
4727       ghash_reduce(v0, v5, v7, v26, vzr, v20);
4728 
4729       __ sub(blocks, blocks, 1);
4730       __ cbnz(blocks, L_ghash_loop);
4731     }
4732 
4733     // The bit-reversed result is at this point in v0
4734     __ rev64(v1, __ T16B, v0);
4735     __ rbit(v1, __ T16B, v1);
4736 
4737     __ st1(v1, __ T16B, state);
4738     __ ret(lr);
4739 
4740     return start;
4741   }
4742 
4743   // Continuation point for throwing of implicit exceptions that are
4744   // not handled in the current activation. Fabricates an exception
4745   // oop and initiates normal exception dispatching in this
4746   // frame. Since we need to preserve callee-saved values (currently
4747   // only for C2, but done for C1 as well) we need a callee-saved oop
4748   // map and therefore have to make these stubs into RuntimeStubs
4749   // rather than BufferBlobs.  If the compiler needs all registers to
4750   // be preserved between the fault point and the exception handler
4751   // then it must assume responsibility for that in
4752   // AbstractCompiler::continuation_for_implicit_null_exception or
4753   // continuation_for_implicit_division_by_zero_exception. All other
4754   // implicit exceptions (e.g., NullPointerException or
4755   // AbstractMethodError on entry) are either at call sites or
4756   // otherwise assume that stack unwinding will be initiated, so
4757   // caller saved registers were assumed volatile in the compiler.
4758 
4759 #undef __
4760 #define __ masm-&gt;
4761 
4762   address generate_throw_exception(const char* name,
4763                                    address runtime_entry,
4764                                    Register arg1 = noreg,
4765                                    Register arg2 = noreg) {
4766     // Information about frame layout at time of blocking runtime call.
4767     // Note that we only have to preserve callee-saved registers since
4768     // the compilers are responsible for supplying a continuation point
4769     // if they expect all registers to be preserved.
4770     // n.b. aarch64 asserts that frame::arg_reg_save_area_bytes == 0
4771     enum layout {
4772       rfp_off = 0,
4773       rfp_off2,
4774       return_off,
4775       return_off2,
4776       framesize // inclusive of return address
4777     };
4778 
4779     int insts_size = 512;
4780     int locs_size  = 64;
4781 
4782     CodeBuffer code(name, insts_size, locs_size);
4783     OopMapSet* oop_maps  = new OopMapSet();
4784     MacroAssembler* masm = new MacroAssembler(&amp;code);
4785 
4786     address start = __ pc();
4787 
4788     // This is an inlined and slightly modified version of call_VM
4789     // which has the ability to fetch the return PC out of
4790     // thread-local storage and also sets up last_Java_sp slightly
4791     // differently than the real call_VM
4792 
4793     __ enter(); // Save FP and LR before call
4794 
4795     assert(is_even(framesize/2), &quot;sp not 16-byte aligned&quot;);
4796 
4797     // lr and fp are already in place
4798     __ sub(sp, rfp, ((unsigned)framesize-4) &lt;&lt; LogBytesPerInt); // prolog
4799 
4800     int frame_complete = __ pc() - start;
4801 
4802     // Set up last_Java_sp and last_Java_fp
4803     address the_pc = __ pc();
4804     __ set_last_Java_frame(sp, rfp, the_pc, rscratch1);
4805 
4806     // Call runtime
4807     if (arg1 != noreg) {
4808       assert(arg2 != c_rarg1, &quot;clobbered&quot;);
4809       __ mov(c_rarg1, arg1);
4810     }
4811     if (arg2 != noreg) {
4812       __ mov(c_rarg2, arg2);
4813     }
4814     __ mov(c_rarg0, rthread);
4815     BLOCK_COMMENT(&quot;call runtime_entry&quot;);
4816     __ mov(rscratch1, runtime_entry);
4817     __ blr(rscratch1);
4818 
4819     // Generate oop map
4820     OopMap* map = new OopMap(framesize, 0);
4821 
4822     oop_maps-&gt;add_gc_map(the_pc - start, map);
4823 
4824     __ reset_last_Java_frame(true);
4825     __ maybe_isb();
4826 
4827     __ leave();
4828 
4829     // check for pending exceptions
4830 #ifdef ASSERT
4831     Label L;
4832     __ ldr(rscratch1, Address(rthread, Thread::pending_exception_offset()));
4833     __ cbnz(rscratch1, L);
4834     __ should_not_reach_here();
4835     __ bind(L);
4836 #endif // ASSERT
4837     __ far_jump(RuntimeAddress(StubRoutines::forward_exception_entry()));
4838 
4839 
4840     // codeBlob framesize is in words (not VMRegImpl::slot_size)
4841     RuntimeStub* stub =
4842       RuntimeStub::new_runtime_stub(name,
4843                                     &amp;code,
4844                                     frame_complete,
4845                                     (framesize &gt;&gt; (LogBytesPerWord - LogBytesPerInt)),
4846                                     oop_maps, false);
4847     return stub-&gt;entry_point();
4848   }
4849 
4850   class MontgomeryMultiplyGenerator : public MacroAssembler {
4851 
4852     Register Pa_base, Pb_base, Pn_base, Pm_base, inv, Rlen, Ra, Rb, Rm, Rn,
4853       Pa, Pb, Pn, Pm, Rhi_ab, Rlo_ab, Rhi_mn, Rlo_mn, t0, t1, t2, Ri, Rj;
4854 
4855     RegSet _toSave;
4856     bool _squaring;
4857 
4858   public:
4859     MontgomeryMultiplyGenerator (Assembler *as, bool squaring)
4860       : MacroAssembler(as-&gt;code()), _squaring(squaring) {
4861 
4862       // Register allocation
4863 
4864       Register reg = c_rarg0;
4865       Pa_base = reg;       // Argument registers
4866       if (squaring)
4867         Pb_base = Pa_base;
4868       else
4869         Pb_base = ++reg;
4870       Pn_base = ++reg;
4871       Rlen= ++reg;
4872       inv = ++reg;
4873       Pm_base = ++reg;
4874 
4875                           // Working registers:
4876       Ra =  ++reg;        // The current digit of a, b, n, and m.
4877       Rb =  ++reg;
4878       Rm =  ++reg;
4879       Rn =  ++reg;
4880 
4881       Pa =  ++reg;        // Pointers to the current/next digit of a, b, n, and m.
4882       Pb =  ++reg;
4883       Pm =  ++reg;
4884       Pn =  ++reg;
4885 
4886       t0 =  ++reg;        // Three registers which form a
4887       t1 =  ++reg;        // triple-precision accumuator.
4888       t2 =  ++reg;
4889 
4890       Ri =  ++reg;        // Inner and outer loop indexes.
4891       Rj =  ++reg;
4892 
4893       Rhi_ab = ++reg;     // Product registers: low and high parts
4894       Rlo_ab = ++reg;     // of a*b and m*n.
4895       Rhi_mn = ++reg;
4896       Rlo_mn = ++reg;
4897 
4898       // r19 and up are callee-saved.
4899       _toSave = RegSet::range(r19, reg) + Pm_base;
4900     }
4901 
4902   private:
4903     void save_regs() {
4904       push(_toSave, sp);
4905     }
4906 
4907     void restore_regs() {
4908       pop(_toSave, sp);
4909     }
4910 
4911     template &lt;typename T&gt;
4912     void unroll_2(Register count, T block) {
4913       Label loop, end, odd;
4914       tbnz(count, 0, odd);
4915       cbz(count, end);
4916       align(16);
4917       bind(loop);
4918       (this-&gt;*block)();
4919       bind(odd);
4920       (this-&gt;*block)();
4921       subs(count, count, 2);
4922       br(Assembler::GT, loop);
4923       bind(end);
4924     }
4925 
4926     template &lt;typename T&gt;
4927     void unroll_2(Register count, T block, Register d, Register s, Register tmp) {
4928       Label loop, end, odd;
4929       tbnz(count, 0, odd);
4930       cbz(count, end);
4931       align(16);
4932       bind(loop);
4933       (this-&gt;*block)(d, s, tmp);
4934       bind(odd);
4935       (this-&gt;*block)(d, s, tmp);
4936       subs(count, count, 2);
4937       br(Assembler::GT, loop);
4938       bind(end);
4939     }
4940 
4941     void pre1(RegisterOrConstant i) {
4942       block_comment(&quot;pre1&quot;);
4943       // Pa = Pa_base;
4944       // Pb = Pb_base + i;
4945       // Pm = Pm_base;
4946       // Pn = Pn_base + i;
4947       // Ra = *Pa;
4948       // Rb = *Pb;
4949       // Rm = *Pm;
4950       // Rn = *Pn;
4951       ldr(Ra, Address(Pa_base));
4952       ldr(Rb, Address(Pb_base, i, Address::uxtw(LogBytesPerWord)));
4953       ldr(Rm, Address(Pm_base));
4954       ldr(Rn, Address(Pn_base, i, Address::uxtw(LogBytesPerWord)));
4955       lea(Pa, Address(Pa_base));
4956       lea(Pb, Address(Pb_base, i, Address::uxtw(LogBytesPerWord)));
4957       lea(Pm, Address(Pm_base));
4958       lea(Pn, Address(Pn_base, i, Address::uxtw(LogBytesPerWord)));
4959 
4960       // Zero the m*n result.
4961       mov(Rhi_mn, zr);
4962       mov(Rlo_mn, zr);
4963     }
4964 
4965     // The core multiply-accumulate step of a Montgomery
4966     // multiplication.  The idea is to schedule operations as a
4967     // pipeline so that instructions with long latencies (loads and
4968     // multiplies) have time to complete before their results are
4969     // used.  This most benefits in-order implementations of the
4970     // architecture but out-of-order ones also benefit.
4971     void step() {
4972       block_comment(&quot;step&quot;);
4973       // MACC(Ra, Rb, t0, t1, t2);
4974       // Ra = *++Pa;
4975       // Rb = *--Pb;
4976       umulh(Rhi_ab, Ra, Rb);
4977       mul(Rlo_ab, Ra, Rb);
4978       ldr(Ra, pre(Pa, wordSize));
4979       ldr(Rb, pre(Pb, -wordSize));
4980       acc(Rhi_mn, Rlo_mn, t0, t1, t2); // The pending m*n from the
4981                                        // previous iteration.
4982       // MACC(Rm, Rn, t0, t1, t2);
4983       // Rm = *++Pm;
4984       // Rn = *--Pn;
4985       umulh(Rhi_mn, Rm, Rn);
4986       mul(Rlo_mn, Rm, Rn);
4987       ldr(Rm, pre(Pm, wordSize));
4988       ldr(Rn, pre(Pn, -wordSize));
4989       acc(Rhi_ab, Rlo_ab, t0, t1, t2);
4990     }
4991 
4992     void post1() {
4993       block_comment(&quot;post1&quot;);
4994 
4995       // MACC(Ra, Rb, t0, t1, t2);
4996       // Ra = *++Pa;
4997       // Rb = *--Pb;
4998       umulh(Rhi_ab, Ra, Rb);
4999       mul(Rlo_ab, Ra, Rb);
5000       acc(Rhi_mn, Rlo_mn, t0, t1, t2);  // The pending m*n
5001       acc(Rhi_ab, Rlo_ab, t0, t1, t2);
5002 
5003       // *Pm = Rm = t0 * inv;
5004       mul(Rm, t0, inv);
5005       str(Rm, Address(Pm));
5006 
5007       // MACC(Rm, Rn, t0, t1, t2);
5008       // t0 = t1; t1 = t2; t2 = 0;
5009       umulh(Rhi_mn, Rm, Rn);
5010 
5011 #ifndef PRODUCT
5012       // assert(m[i] * n[0] + t0 == 0, &quot;broken Montgomery multiply&quot;);
5013       {
5014         mul(Rlo_mn, Rm, Rn);
5015         add(Rlo_mn, t0, Rlo_mn);
5016         Label ok;
5017         cbz(Rlo_mn, ok); {
5018           stop(&quot;broken Montgomery multiply&quot;);
5019         } bind(ok);
5020       }
5021 #endif
5022       // We have very carefully set things up so that
5023       // m[i]*n[0] + t0 == 0 (mod b), so we don&#39;t have to calculate
5024       // the lower half of Rm * Rn because we know the result already:
5025       // it must be -t0.  t0 + (-t0) must generate a carry iff
5026       // t0 != 0.  So, rather than do a mul and an adds we just set
5027       // the carry flag iff t0 is nonzero.
5028       //
5029       // mul(Rlo_mn, Rm, Rn);
5030       // adds(zr, t0, Rlo_mn);
5031       subs(zr, t0, 1); // Set carry iff t0 is nonzero
5032       adcs(t0, t1, Rhi_mn);
5033       adc(t1, t2, zr);
5034       mov(t2, zr);
5035     }
5036 
5037     void pre2(RegisterOrConstant i, RegisterOrConstant len) {
5038       block_comment(&quot;pre2&quot;);
5039       // Pa = Pa_base + i-len;
5040       // Pb = Pb_base + len;
5041       // Pm = Pm_base + i-len;
5042       // Pn = Pn_base + len;
5043 
5044       if (i.is_register()) {
5045         sub(Rj, i.as_register(), len);
5046       } else {
5047         mov(Rj, i.as_constant());
5048         sub(Rj, Rj, len);
5049       }
5050       // Rj == i-len
5051 
5052       lea(Pa, Address(Pa_base, Rj, Address::uxtw(LogBytesPerWord)));
5053       lea(Pb, Address(Pb_base, len, Address::uxtw(LogBytesPerWord)));
5054       lea(Pm, Address(Pm_base, Rj, Address::uxtw(LogBytesPerWord)));
5055       lea(Pn, Address(Pn_base, len, Address::uxtw(LogBytesPerWord)));
5056 
5057       // Ra = *++Pa;
5058       // Rb = *--Pb;
5059       // Rm = *++Pm;
5060       // Rn = *--Pn;
5061       ldr(Ra, pre(Pa, wordSize));
5062       ldr(Rb, pre(Pb, -wordSize));
5063       ldr(Rm, pre(Pm, wordSize));
5064       ldr(Rn, pre(Pn, -wordSize));
5065 
5066       mov(Rhi_mn, zr);
5067       mov(Rlo_mn, zr);
5068     }
5069 
5070     void post2(RegisterOrConstant i, RegisterOrConstant len) {
5071       block_comment(&quot;post2&quot;);
5072       if (i.is_constant()) {
5073         mov(Rj, i.as_constant()-len.as_constant());
5074       } else {
5075         sub(Rj, i.as_register(), len);
5076       }
5077 
5078       adds(t0, t0, Rlo_mn); // The pending m*n, low part
5079 
5080       // As soon as we know the least significant digit of our result,
5081       // store it.
5082       // Pm_base[i-len] = t0;
5083       str(t0, Address(Pm_base, Rj, Address::uxtw(LogBytesPerWord)));
5084 
5085       // t0 = t1; t1 = t2; t2 = 0;
5086       adcs(t0, t1, Rhi_mn); // The pending m*n, high part
5087       adc(t1, t2, zr);
5088       mov(t2, zr);
5089     }
5090 
5091     // A carry in t0 after Montgomery multiplication means that we
5092     // should subtract multiples of n from our result in m.  We&#39;ll
5093     // keep doing that until there is no carry.
5094     void normalize(RegisterOrConstant len) {
5095       block_comment(&quot;normalize&quot;);
5096       // while (t0)
5097       //   t0 = sub(Pm_base, Pn_base, t0, len);
5098       Label loop, post, again;
5099       Register cnt = t1, i = t2; // Re-use registers; we&#39;re done with them now
5100       cbz(t0, post); {
5101         bind(again); {
5102           mov(i, zr);
5103           mov(cnt, len);
5104           ldr(Rm, Address(Pm_base, i, Address::uxtw(LogBytesPerWord)));
5105           ldr(Rn, Address(Pn_base, i, Address::uxtw(LogBytesPerWord)));
5106           subs(zr, zr, zr); // set carry flag, i.e. no borrow
5107           align(16);
5108           bind(loop); {
5109             sbcs(Rm, Rm, Rn);
5110             str(Rm, Address(Pm_base, i, Address::uxtw(LogBytesPerWord)));
5111             add(i, i, 1);
5112             ldr(Rm, Address(Pm_base, i, Address::uxtw(LogBytesPerWord)));
5113             ldr(Rn, Address(Pn_base, i, Address::uxtw(LogBytesPerWord)));
5114             sub(cnt, cnt, 1);
5115           } cbnz(cnt, loop);
5116           sbc(t0, t0, zr);
5117         } cbnz(t0, again);
5118       } bind(post);
5119     }
5120 
5121     // Move memory at s to d, reversing words.
5122     //    Increments d to end of copied memory
5123     //    Destroys tmp1, tmp2
5124     //    Preserves len
5125     //    Leaves s pointing to the address which was in d at start
5126     void reverse(Register d, Register s, Register len, Register tmp1, Register tmp2) {
5127       assert(tmp1 &lt; r19 &amp;&amp; tmp2 &lt; r19, &quot;register corruption&quot;);
5128 
5129       lea(s, Address(s, len, Address::uxtw(LogBytesPerWord)));
5130       mov(tmp1, len);
5131       unroll_2(tmp1, &amp;MontgomeryMultiplyGenerator::reverse1, d, s, tmp2);
5132       sub(s, d, len, ext::uxtw, LogBytesPerWord);
5133     }
5134     // where
5135     void reverse1(Register d, Register s, Register tmp) {
5136       ldr(tmp, pre(s, -wordSize));
5137       ror(tmp, tmp, 32);
5138       str(tmp, post(d, wordSize));
5139     }
5140 
5141     void step_squaring() {
5142       // An extra ACC
5143       step();
5144       acc(Rhi_ab, Rlo_ab, t0, t1, t2);
5145     }
5146 
5147     void last_squaring(RegisterOrConstant i) {
5148       Label dont;
5149       // if ((i &amp; 1) == 0) {
5150       tbnz(i.as_register(), 0, dont); {
5151         // MACC(Ra, Rb, t0, t1, t2);
5152         // Ra = *++Pa;
5153         // Rb = *--Pb;
5154         umulh(Rhi_ab, Ra, Rb);
5155         mul(Rlo_ab, Ra, Rb);
5156         acc(Rhi_ab, Rlo_ab, t0, t1, t2);
5157       } bind(dont);
5158     }
5159 
5160     void extra_step_squaring() {
5161       acc(Rhi_mn, Rlo_mn, t0, t1, t2);  // The pending m*n
5162 
5163       // MACC(Rm, Rn, t0, t1, t2);
5164       // Rm = *++Pm;
5165       // Rn = *--Pn;
5166       umulh(Rhi_mn, Rm, Rn);
5167       mul(Rlo_mn, Rm, Rn);
5168       ldr(Rm, pre(Pm, wordSize));
5169       ldr(Rn, pre(Pn, -wordSize));
5170     }
5171 
5172     void post1_squaring() {
5173       acc(Rhi_mn, Rlo_mn, t0, t1, t2);  // The pending m*n
5174 
5175       // *Pm = Rm = t0 * inv;
5176       mul(Rm, t0, inv);
5177       str(Rm, Address(Pm));
5178 
5179       // MACC(Rm, Rn, t0, t1, t2);
5180       // t0 = t1; t1 = t2; t2 = 0;
5181       umulh(Rhi_mn, Rm, Rn);
5182 
5183 #ifndef PRODUCT
5184       // assert(m[i] * n[0] + t0 == 0, &quot;broken Montgomery multiply&quot;);
5185       {
5186         mul(Rlo_mn, Rm, Rn);
5187         add(Rlo_mn, t0, Rlo_mn);
5188         Label ok;
5189         cbz(Rlo_mn, ok); {
5190           stop(&quot;broken Montgomery multiply&quot;);
5191         } bind(ok);
5192       }
5193 #endif
5194       // We have very carefully set things up so that
5195       // m[i]*n[0] + t0 == 0 (mod b), so we don&#39;t have to calculate
5196       // the lower half of Rm * Rn because we know the result already:
5197       // it must be -t0.  t0 + (-t0) must generate a carry iff
5198       // t0 != 0.  So, rather than do a mul and an adds we just set
5199       // the carry flag iff t0 is nonzero.
5200       //
5201       // mul(Rlo_mn, Rm, Rn);
5202       // adds(zr, t0, Rlo_mn);
5203       subs(zr, t0, 1); // Set carry iff t0 is nonzero
5204       adcs(t0, t1, Rhi_mn);
5205       adc(t1, t2, zr);
5206       mov(t2, zr);
5207     }
5208 
5209     void acc(Register Rhi, Register Rlo,
5210              Register t0, Register t1, Register t2) {
5211       adds(t0, t0, Rlo);
5212       adcs(t1, t1, Rhi);
5213       adc(t2, t2, zr);
5214     }
5215 
5216   public:
5217     /**
5218      * Fast Montgomery multiplication.  The derivation of the
5219      * algorithm is in A Cryptographic Library for the Motorola
5220      * DSP56000, Dusse and Kaliski, Proc. EUROCRYPT 90, pp. 230-237.
5221      *
5222      * Arguments:
5223      *
5224      * Inputs for multiplication:
5225      *   c_rarg0   - int array elements a
5226      *   c_rarg1   - int array elements b
5227      *   c_rarg2   - int array elements n (the modulus)
5228      *   c_rarg3   - int length
5229      *   c_rarg4   - int inv
5230      *   c_rarg5   - int array elements m (the result)
5231      *
5232      * Inputs for squaring:
5233      *   c_rarg0   - int array elements a
5234      *   c_rarg1   - int array elements n (the modulus)
5235      *   c_rarg2   - int length
5236      *   c_rarg3   - int inv
5237      *   c_rarg4   - int array elements m (the result)
5238      *
5239      */
5240     address generate_multiply() {
5241       Label argh, nothing;
5242       bind(argh);
5243       stop(&quot;MontgomeryMultiply total_allocation must be &lt;= 8192&quot;);
5244 
5245       align(CodeEntryAlignment);
5246       address entry = pc();
5247 
5248       cbzw(Rlen, nothing);
5249 
5250       enter();
5251 
5252       // Make room.
5253       cmpw(Rlen, 512);
5254       br(Assembler::HI, argh);
5255       sub(Ra, sp, Rlen, ext::uxtw, exact_log2(4 * sizeof (jint)));
5256       andr(sp, Ra, -2 * wordSize);
5257 
5258       lsrw(Rlen, Rlen, 1);  // length in longwords = len/2
5259 
5260       {
5261         // Copy input args, reversing as we go.  We use Ra as a
5262         // temporary variable.
5263         reverse(Ra, Pa_base, Rlen, t0, t1);
5264         if (!_squaring)
5265           reverse(Ra, Pb_base, Rlen, t0, t1);
5266         reverse(Ra, Pn_base, Rlen, t0, t1);
5267       }
5268 
5269       // Push all call-saved registers and also Pm_base which we&#39;ll need
5270       // at the end.
5271       save_regs();
5272 
5273 #ifndef PRODUCT
5274       // assert(inv * n[0] == -1UL, &quot;broken inverse in Montgomery multiply&quot;);
5275       {
5276         ldr(Rn, Address(Pn_base, 0));
5277         mul(Rlo_mn, Rn, inv);
5278         subs(zr, Rlo_mn, -1);
5279         Label ok;
5280         br(EQ, ok); {
5281           stop(&quot;broken inverse in Montgomery multiply&quot;);
5282         } bind(ok);
5283       }
5284 #endif
5285 
5286       mov(Pm_base, Ra);
5287 
5288       mov(t0, zr);
5289       mov(t1, zr);
5290       mov(t2, zr);
5291 
5292       block_comment(&quot;for (int i = 0; i &lt; len; i++) {&quot;);
5293       mov(Ri, zr); {
5294         Label loop, end;
5295         cmpw(Ri, Rlen);
5296         br(Assembler::GE, end);
5297 
5298         bind(loop);
5299         pre1(Ri);
5300 
5301         block_comment(&quot;  for (j = i; j; j--) {&quot;); {
5302           movw(Rj, Ri);
5303           unroll_2(Rj, &amp;MontgomeryMultiplyGenerator::step);
5304         } block_comment(&quot;  } // j&quot;);
5305 
5306         post1();
5307         addw(Ri, Ri, 1);
5308         cmpw(Ri, Rlen);
5309         br(Assembler::LT, loop);
5310         bind(end);
5311         block_comment(&quot;} // i&quot;);
5312       }
5313 
5314       block_comment(&quot;for (int i = len; i &lt; 2*len; i++) {&quot;);
5315       mov(Ri, Rlen); {
5316         Label loop, end;
5317         cmpw(Ri, Rlen, Assembler::LSL, 1);
5318         br(Assembler::GE, end);
5319 
5320         bind(loop);
5321         pre2(Ri, Rlen);
5322 
5323         block_comment(&quot;  for (j = len*2-i-1; j; j--) {&quot;); {
5324           lslw(Rj, Rlen, 1);
5325           subw(Rj, Rj, Ri);
5326           subw(Rj, Rj, 1);
5327           unroll_2(Rj, &amp;MontgomeryMultiplyGenerator::step);
5328         } block_comment(&quot;  } // j&quot;);
5329 
5330         post2(Ri, Rlen);
5331         addw(Ri, Ri, 1);
5332         cmpw(Ri, Rlen, Assembler::LSL, 1);
5333         br(Assembler::LT, loop);
5334         bind(end);
5335       }
5336       block_comment(&quot;} // i&quot;);
5337 
5338       normalize(Rlen);
5339 
5340       mov(Ra, Pm_base);  // Save Pm_base in Ra
5341       restore_regs();  // Restore caller&#39;s Pm_base
5342 
5343       // Copy our result into caller&#39;s Pm_base
5344       reverse(Pm_base, Ra, Rlen, t0, t1);
5345 
5346       leave();
5347       bind(nothing);
5348       ret(lr);
5349 
5350       return entry;
5351     }
5352     // In C, approximately:
5353 
5354     // void
5355     // montgomery_multiply(unsigned long Pa_base[], unsigned long Pb_base[],
5356     //                     unsigned long Pn_base[], unsigned long Pm_base[],
5357     //                     unsigned long inv, int len) {
5358     //   unsigned long t0 = 0, t1 = 0, t2 = 0; // Triple-precision accumulator
5359     //   unsigned long *Pa, *Pb, *Pn, *Pm;
5360     //   unsigned long Ra, Rb, Rn, Rm;
5361 
5362     //   int i;
5363 
5364     //   assert(inv * Pn_base[0] == -1UL, &quot;broken inverse in Montgomery multiply&quot;);
5365 
5366     //   for (i = 0; i &lt; len; i++) {
5367     //     int j;
5368 
5369     //     Pa = Pa_base;
5370     //     Pb = Pb_base + i;
5371     //     Pm = Pm_base;
5372     //     Pn = Pn_base + i;
5373 
5374     //     Ra = *Pa;
5375     //     Rb = *Pb;
5376     //     Rm = *Pm;
5377     //     Rn = *Pn;
5378 
5379     //     int iters = i;
5380     //     for (j = 0; iters--; j++) {
5381     //       assert(Ra == Pa_base[j] &amp;&amp; Rb == Pb_base[i-j], &quot;must be&quot;);
5382     //       MACC(Ra, Rb, t0, t1, t2);
5383     //       Ra = *++Pa;
5384     //       Rb = *--Pb;
5385     //       assert(Rm == Pm_base[j] &amp;&amp; Rn == Pn_base[i-j], &quot;must be&quot;);
5386     //       MACC(Rm, Rn, t0, t1, t2);
5387     //       Rm = *++Pm;
5388     //       Rn = *--Pn;
5389     //     }
5390 
5391     //     assert(Ra == Pa_base[i] &amp;&amp; Rb == Pb_base[0], &quot;must be&quot;);
5392     //     MACC(Ra, Rb, t0, t1, t2);
5393     //     *Pm = Rm = t0 * inv;
5394     //     assert(Rm == Pm_base[i] &amp;&amp; Rn == Pn_base[0], &quot;must be&quot;);
5395     //     MACC(Rm, Rn, t0, t1, t2);
5396 
5397     //     assert(t0 == 0, &quot;broken Montgomery multiply&quot;);
5398 
5399     //     t0 = t1; t1 = t2; t2 = 0;
5400     //   }
5401 
5402     //   for (i = len; i &lt; 2*len; i++) {
5403     //     int j;
5404 
5405     //     Pa = Pa_base + i-len;
5406     //     Pb = Pb_base + len;
5407     //     Pm = Pm_base + i-len;
5408     //     Pn = Pn_base + len;
5409 
5410     //     Ra = *++Pa;
5411     //     Rb = *--Pb;
5412     //     Rm = *++Pm;
5413     //     Rn = *--Pn;
5414 
5415     //     int iters = len*2-i-1;
5416     //     for (j = i-len+1; iters--; j++) {
5417     //       assert(Ra == Pa_base[j] &amp;&amp; Rb == Pb_base[i-j], &quot;must be&quot;);
5418     //       MACC(Ra, Rb, t0, t1, t2);
5419     //       Ra = *++Pa;
5420     //       Rb = *--Pb;
5421     //       assert(Rm == Pm_base[j] &amp;&amp; Rn == Pn_base[i-j], &quot;must be&quot;);
5422     //       MACC(Rm, Rn, t0, t1, t2);
5423     //       Rm = *++Pm;
5424     //       Rn = *--Pn;
5425     //     }
5426 
5427     //     Pm_base[i-len] = t0;
5428     //     t0 = t1; t1 = t2; t2 = 0;
5429     //   }
5430 
5431     //   while (t0)
5432     //     t0 = sub(Pm_base, Pn_base, t0, len);
5433     // }
5434 
5435     /**
5436      * Fast Montgomery squaring.  This uses asymptotically 25% fewer
5437      * multiplies than Montgomery multiplication so it should be up to
5438      * 25% faster.  However, its loop control is more complex and it
5439      * may actually run slower on some machines.
5440      *
5441      * Arguments:
5442      *
5443      * Inputs:
5444      *   c_rarg0   - int array elements a
5445      *   c_rarg1   - int array elements n (the modulus)
5446      *   c_rarg2   - int length
5447      *   c_rarg3   - int inv
5448      *   c_rarg4   - int array elements m (the result)
5449      *
5450      */
5451     address generate_square() {
5452       Label argh;
5453       bind(argh);
5454       stop(&quot;MontgomeryMultiply total_allocation must be &lt;= 8192&quot;);
5455 
5456       align(CodeEntryAlignment);
5457       address entry = pc();
5458 
5459       enter();
5460 
5461       // Make room.
5462       cmpw(Rlen, 512);
5463       br(Assembler::HI, argh);
5464       sub(Ra, sp, Rlen, ext::uxtw, exact_log2(4 * sizeof (jint)));
5465       andr(sp, Ra, -2 * wordSize);
5466 
5467       lsrw(Rlen, Rlen, 1);  // length in longwords = len/2
5468 
5469       {
5470         // Copy input args, reversing as we go.  We use Ra as a
5471         // temporary variable.
5472         reverse(Ra, Pa_base, Rlen, t0, t1);
5473         reverse(Ra, Pn_base, Rlen, t0, t1);
5474       }
5475 
5476       // Push all call-saved registers and also Pm_base which we&#39;ll need
5477       // at the end.
5478       save_regs();
5479 
5480       mov(Pm_base, Ra);
5481 
5482       mov(t0, zr);
5483       mov(t1, zr);
5484       mov(t2, zr);
5485 
5486       block_comment(&quot;for (int i = 0; i &lt; len; i++) {&quot;);
5487       mov(Ri, zr); {
5488         Label loop, end;
5489         bind(loop);
5490         cmp(Ri, Rlen);
5491         br(Assembler::GE, end);
5492 
5493         pre1(Ri);
5494 
5495         block_comment(&quot;for (j = (i+1)/2; j; j--) {&quot;); {
5496           add(Rj, Ri, 1);
5497           lsr(Rj, Rj, 1);
5498           unroll_2(Rj, &amp;MontgomeryMultiplyGenerator::step_squaring);
5499         } block_comment(&quot;  } // j&quot;);
5500 
5501         last_squaring(Ri);
5502 
5503         block_comment(&quot;  for (j = i/2; j; j--) {&quot;); {
5504           lsr(Rj, Ri, 1);
5505           unroll_2(Rj, &amp;MontgomeryMultiplyGenerator::extra_step_squaring);
5506         } block_comment(&quot;  } // j&quot;);
5507 
5508         post1_squaring();
5509         add(Ri, Ri, 1);
5510         cmp(Ri, Rlen);
5511         br(Assembler::LT, loop);
5512 
5513         bind(end);
5514         block_comment(&quot;} // i&quot;);
5515       }
5516 
5517       block_comment(&quot;for (int i = len; i &lt; 2*len; i++) {&quot;);
5518       mov(Ri, Rlen); {
5519         Label loop, end;
5520         bind(loop);
5521         cmp(Ri, Rlen, Assembler::LSL, 1);
5522         br(Assembler::GE, end);
5523 
5524         pre2(Ri, Rlen);
5525 
5526         block_comment(&quot;  for (j = (2*len-i-1)/2; j; j--) {&quot;); {
5527           lsl(Rj, Rlen, 1);
5528           sub(Rj, Rj, Ri);
5529           sub(Rj, Rj, 1);
5530           lsr(Rj, Rj, 1);
5531           unroll_2(Rj, &amp;MontgomeryMultiplyGenerator::step_squaring);
5532         } block_comment(&quot;  } // j&quot;);
5533 
5534         last_squaring(Ri);
5535 
5536         block_comment(&quot;  for (j = (2*len-i)/2; j; j--) {&quot;); {
5537           lsl(Rj, Rlen, 1);
5538           sub(Rj, Rj, Ri);
5539           lsr(Rj, Rj, 1);
5540           unroll_2(Rj, &amp;MontgomeryMultiplyGenerator::extra_step_squaring);
5541         } block_comment(&quot;  } // j&quot;);
5542 
5543         post2(Ri, Rlen);
5544         add(Ri, Ri, 1);
5545         cmp(Ri, Rlen, Assembler::LSL, 1);
5546 
5547         br(Assembler::LT, loop);
5548         bind(end);
5549         block_comment(&quot;} // i&quot;);
5550       }
5551 
5552       normalize(Rlen);
5553 
5554       mov(Ra, Pm_base);  // Save Pm_base in Ra
5555       restore_regs();  // Restore caller&#39;s Pm_base
5556 
5557       // Copy our result into caller&#39;s Pm_base
5558       reverse(Pm_base, Ra, Rlen, t0, t1);
5559 
5560       leave();
5561       ret(lr);
5562 
5563       return entry;
5564     }
5565     // In C, approximately:
5566 
5567     // void
5568     // montgomery_square(unsigned long Pa_base[], unsigned long Pn_base[],
5569     //                   unsigned long Pm_base[], unsigned long inv, int len) {
5570     //   unsigned long t0 = 0, t1 = 0, t2 = 0; // Triple-precision accumulator
5571     //   unsigned long *Pa, *Pb, *Pn, *Pm;
5572     //   unsigned long Ra, Rb, Rn, Rm;
5573 
5574     //   int i;
5575 
5576     //   assert(inv * Pn_base[0] == -1UL, &quot;broken inverse in Montgomery multiply&quot;);
5577 
5578     //   for (i = 0; i &lt; len; i++) {
5579     //     int j;
5580 
5581     //     Pa = Pa_base;
5582     //     Pb = Pa_base + i;
5583     //     Pm = Pm_base;
5584     //     Pn = Pn_base + i;
5585 
5586     //     Ra = *Pa;
5587     //     Rb = *Pb;
5588     //     Rm = *Pm;
5589     //     Rn = *Pn;
5590 
5591     //     int iters = (i+1)/2;
5592     //     for (j = 0; iters--; j++) {
5593     //       assert(Ra == Pa_base[j] &amp;&amp; Rb == Pa_base[i-j], &quot;must be&quot;);
5594     //       MACC2(Ra, Rb, t0, t1, t2);
5595     //       Ra = *++Pa;
5596     //       Rb = *--Pb;
5597     //       assert(Rm == Pm_base[j] &amp;&amp; Rn == Pn_base[i-j], &quot;must be&quot;);
5598     //       MACC(Rm, Rn, t0, t1, t2);
5599     //       Rm = *++Pm;
5600     //       Rn = *--Pn;
5601     //     }
5602     //     if ((i &amp; 1) == 0) {
5603     //       assert(Ra == Pa_base[j], &quot;must be&quot;);
5604     //       MACC(Ra, Ra, t0, t1, t2);
5605     //     }
5606     //     iters = i/2;
5607     //     assert(iters == i-j, &quot;must be&quot;);
5608     //     for (; iters--; j++) {
5609     //       assert(Rm == Pm_base[j] &amp;&amp; Rn == Pn_base[i-j], &quot;must be&quot;);
5610     //       MACC(Rm, Rn, t0, t1, t2);
5611     //       Rm = *++Pm;
5612     //       Rn = *--Pn;
5613     //     }
5614 
5615     //     *Pm = Rm = t0 * inv;
5616     //     assert(Rm == Pm_base[i] &amp;&amp; Rn == Pn_base[0], &quot;must be&quot;);
5617     //     MACC(Rm, Rn, t0, t1, t2);
5618 
5619     //     assert(t0 == 0, &quot;broken Montgomery multiply&quot;);
5620 
5621     //     t0 = t1; t1 = t2; t2 = 0;
5622     //   }
5623 
5624     //   for (i = len; i &lt; 2*len; i++) {
5625     //     int start = i-len+1;
5626     //     int end = start + (len - start)/2;
5627     //     int j;
5628 
5629     //     Pa = Pa_base + i-len;
5630     //     Pb = Pa_base + len;
5631     //     Pm = Pm_base + i-len;
5632     //     Pn = Pn_base + len;
5633 
5634     //     Ra = *++Pa;
5635     //     Rb = *--Pb;
5636     //     Rm = *++Pm;
5637     //     Rn = *--Pn;
5638 
5639     //     int iters = (2*len-i-1)/2;
5640     //     assert(iters == end-start, &quot;must be&quot;);
5641     //     for (j = start; iters--; j++) {
5642     //       assert(Ra == Pa_base[j] &amp;&amp; Rb == Pa_base[i-j], &quot;must be&quot;);
5643     //       MACC2(Ra, Rb, t0, t1, t2);
5644     //       Ra = *++Pa;
5645     //       Rb = *--Pb;
5646     //       assert(Rm == Pm_base[j] &amp;&amp; Rn == Pn_base[i-j], &quot;must be&quot;);
5647     //       MACC(Rm, Rn, t0, t1, t2);
5648     //       Rm = *++Pm;
5649     //       Rn = *--Pn;
5650     //     }
5651     //     if ((i &amp; 1) == 0) {
5652     //       assert(Ra == Pa_base[j], &quot;must be&quot;);
5653     //       MACC(Ra, Ra, t0, t1, t2);
5654     //     }
5655     //     iters =  (2*len-i)/2;
5656     //     assert(iters == len-j, &quot;must be&quot;);
5657     //     for (; iters--; j++) {
5658     //       assert(Rm == Pm_base[j] &amp;&amp; Rn == Pn_base[i-j], &quot;must be&quot;);
5659     //       MACC(Rm, Rn, t0, t1, t2);
5660     //       Rm = *++Pm;
5661     //       Rn = *--Pn;
5662     //     }
5663     //     Pm_base[i-len] = t0;
5664     //     t0 = t1; t1 = t2; t2 = 0;
5665     //   }
5666 
5667     //   while (t0)
5668     //     t0 = sub(Pm_base, Pn_base, t0, len);
5669     // }
5670   };
5671 
5672 
5673   // Call here from the interpreter or compiled code to either load
5674   // multiple returned values from the value type instance being
5675   // returned to registers or to store returned values to a newly
5676   // allocated value type instance.
5677   address generate_return_value_stub(address destination, const char* name, bool has_res) {
5678 
5679     // Information about frame layout at time of blocking runtime call.
5680     // Note that we only have to preserve callee-saved registers since
5681     // the compilers are responsible for supplying a continuation point
5682     // if they expect all registers to be preserved.
5683     // n.b. aarch64 asserts that frame::arg_reg_save_area_bytes == 0
5684     enum layout {
5685       rfp_off = 0, rfp_off2,
5686 
5687       j_rarg7_off, j_rarg7_2,
5688       j_rarg6_off, j_rarg6_2,
5689       j_rarg5_off, j_rarg5_2,
5690       j_rarg4_off, j_rarg4_2,
5691       j_rarg3_off, j_rarg3_2,
5692       j_rarg2_off, j_rarg2_2,
5693       j_rarg1_off, j_rarg1_2,
5694       j_rarg0_off, j_rarg0_2,
5695 
5696       j_farg0_off, j_farg0_2,
5697       j_farg1_off, j_farg1_2,
5698       j_farg2_off, j_farg2_2,
5699       j_farg3_off, j_farg3_2,
5700       j_farg4_off, j_farg4_2,
5701       j_farg5_off, j_farg5_2,
5702       j_farg6_off, j_farg6_2,
5703       j_farg7_off, j_farg7_2,
5704 
5705       return_off, return_off2,
5706       framesize // inclusive of return address
5707     };
5708 
5709     int insts_size = 512;
5710     int locs_size  = 64;
5711 
5712     CodeBuffer code(name, insts_size, locs_size);
5713     OopMapSet* oop_maps  = new OopMapSet();
5714     MacroAssembler* masm = new MacroAssembler(&amp;code);
5715 
5716     address start = __ pc();
5717 
5718     const Address f7_save       (rfp, j_farg7_off * wordSize);
5719     const Address f6_save       (rfp, j_farg6_off * wordSize);
5720     const Address f5_save       (rfp, j_farg5_off * wordSize);
5721     const Address f4_save       (rfp, j_farg4_off * wordSize);
5722     const Address f3_save       (rfp, j_farg3_off * wordSize);
5723     const Address f2_save       (rfp, j_farg2_off * wordSize);
5724     const Address f1_save       (rfp, j_farg1_off * wordSize);
5725     const Address f0_save       (rfp, j_farg0_off * wordSize);
5726 
5727     const Address r0_save      (rfp, j_rarg0_off * wordSize);
5728     const Address r1_save      (rfp, j_rarg1_off * wordSize);
5729     const Address r2_save      (rfp, j_rarg2_off * wordSize);
5730     const Address r3_save      (rfp, j_rarg3_off * wordSize);
5731     const Address r4_save      (rfp, j_rarg4_off * wordSize);
5732     const Address r5_save      (rfp, j_rarg5_off * wordSize);
5733     const Address r6_save      (rfp, j_rarg6_off * wordSize);
5734     const Address r7_save      (rfp, j_rarg7_off * wordSize);
5735 
5736     // Generate oop map
5737     OopMap* map = new OopMap(framesize, 0);
5738 
5739     map-&gt;set_callee_saved(VMRegImpl::stack2reg(rfp_off), rfp-&gt;as_VMReg());
5740     map-&gt;set_callee_saved(VMRegImpl::stack2reg(j_rarg7_off), j_rarg7-&gt;as_VMReg());
5741     map-&gt;set_callee_saved(VMRegImpl::stack2reg(j_rarg6_off), j_rarg6-&gt;as_VMReg());
5742     map-&gt;set_callee_saved(VMRegImpl::stack2reg(j_rarg5_off), j_rarg5-&gt;as_VMReg());
5743     map-&gt;set_callee_saved(VMRegImpl::stack2reg(j_rarg4_off), j_rarg4-&gt;as_VMReg());
5744     map-&gt;set_callee_saved(VMRegImpl::stack2reg(j_rarg3_off), j_rarg3-&gt;as_VMReg());
5745     map-&gt;set_callee_saved(VMRegImpl::stack2reg(j_rarg2_off), j_rarg2-&gt;as_VMReg());
5746     map-&gt;set_callee_saved(VMRegImpl::stack2reg(j_rarg1_off), j_rarg1-&gt;as_VMReg());
5747     map-&gt;set_callee_saved(VMRegImpl::stack2reg(j_rarg0_off), j_rarg0-&gt;as_VMReg());
5748 
5749     map-&gt;set_callee_saved(VMRegImpl::stack2reg(j_farg0_off), j_farg0-&gt;as_VMReg());
5750     map-&gt;set_callee_saved(VMRegImpl::stack2reg(j_farg1_off), j_farg1-&gt;as_VMReg());
5751     map-&gt;set_callee_saved(VMRegImpl::stack2reg(j_farg2_off), j_farg2-&gt;as_VMReg());
5752     map-&gt;set_callee_saved(VMRegImpl::stack2reg(j_farg3_off), j_farg3-&gt;as_VMReg());
5753     map-&gt;set_callee_saved(VMRegImpl::stack2reg(j_farg4_off), j_farg4-&gt;as_VMReg());
5754     map-&gt;set_callee_saved(VMRegImpl::stack2reg(j_farg5_off), j_farg5-&gt;as_VMReg());
5755     map-&gt;set_callee_saved(VMRegImpl::stack2reg(j_farg6_off), j_farg6-&gt;as_VMReg());
5756     map-&gt;set_callee_saved(VMRegImpl::stack2reg(j_farg7_off), j_farg7-&gt;as_VMReg());
5757 
5758     // This is an inlined and slightly modified version of call_VM
5759     // which has the ability to fetch the return PC out of
5760     // thread-local storage and also sets up last_Java_sp slightly
5761     // differently than the real call_VM
5762 
5763     __ enter(); // Save FP and LR before call
5764 
5765     assert(is_even(framesize/2), &quot;sp not 16-byte aligned&quot;);
5766 
5767     // lr and fp are already in place
5768     __ sub(sp, rfp, ((unsigned)framesize - 4) &lt;&lt; LogBytesPerInt); // prolog
5769 
5770     __ strd(j_farg7, f7_save);
5771     __ strd(j_farg6, f6_save);
5772     __ strd(j_farg5, f5_save);
5773     __ strd(j_farg4, f4_save);
5774     __ strd(j_farg3, f3_save);
5775     __ strd(j_farg2, f2_save);
5776     __ strd(j_farg1, f1_save);
5777     __ strd(j_farg0, f0_save);
5778 
5779     __ str(j_rarg0, r0_save);
5780     __ str(j_rarg1, r1_save);
5781     __ str(j_rarg2, r2_save);
5782     __ str(j_rarg3, r3_save);
5783     __ str(j_rarg4, r4_save);
5784     __ str(j_rarg5, r5_save);
5785     __ str(j_rarg6, r6_save);
5786     __ str(j_rarg7, r7_save);
5787 
5788     int frame_complete = __ pc() - start;
5789 
5790     // Set up last_Java_sp and last_Java_fp
5791     address the_pc = __ pc();
5792     __ set_last_Java_frame(sp, rfp, the_pc, rscratch1);
5793 
5794     // Call runtime
5795     __ mov(c_rarg0, rthread);
5796     __ mov(c_rarg1, r0);
5797 
5798     BLOCK_COMMENT(&quot;call runtime_entry&quot;);
5799     __ mov(rscratch1, destination);
5800     __ blr(rscratch1);
5801 
5802     oop_maps-&gt;add_gc_map(the_pc - start, map);
5803 
5804     __ reset_last_Java_frame(false);
5805     __ maybe_isb();
5806 
5807     __ ldrd(j_farg7, f7_save);
5808     __ ldrd(j_farg6, f6_save);
5809     __ ldrd(j_farg5, f5_save);
5810     __ ldrd(j_farg4, f4_save);
5811     __ ldrd(j_farg3, f3_save);
5812     __ ldrd(j_farg3, f2_save);
5813     __ ldrd(j_farg1, f1_save);
5814     __ ldrd(j_farg0, f0_save);
5815 
5816     __ ldr(j_rarg0, r0_save);
5817     __ ldr(j_rarg1, r1_save);
5818     __ ldr(j_rarg2, r2_save);
5819     __ ldr(j_rarg3, r3_save);
5820     __ ldr(j_rarg4, r4_save);
5821     __ ldr(j_rarg5, r5_save);
5822     __ ldr(j_rarg6, r6_save);
5823     __ ldr(j_rarg7, r7_save);
5824 
5825     __ leave();
5826 
5827     // check for pending exceptions
5828     Label pending;
5829     __ ldr(rscratch1, Address(rthread, in_bytes(Thread::pending_exception_offset())));
5830     __ cmp(rscratch1, (u1)NULL_WORD);
5831     __ br(Assembler::NE, pending);
5832 
5833     if (has_res) {
5834       __ get_vm_result(r0, rthread);
5835     }
5836     __ ret(lr);
5837 
5838     __ bind(pending);
5839     __ ldr(r0, Address(rthread, in_bytes(Thread::pending_exception_offset())));
5840     __ far_jump(RuntimeAddress(StubRoutines::forward_exception_entry()));
5841 
5842 
5843     // codeBlob framesize is in words (not VMRegImpl::slot_size)
5844     int frame_size_in_words = (framesize &gt;&gt; (LogBytesPerWord - LogBytesPerInt));
5845     RuntimeStub* stub =
5846       RuntimeStub::new_runtime_stub(name, &amp;code, frame_complete, frame_size_in_words, oop_maps, false);
5847 
5848     return stub-&gt;entry_point();
5849   }
5850 
5851   // Initialization
5852   void generate_initial() {
5853     // Generate initial stubs and initializes the entry points
5854 
5855     // entry points that exist in all platforms Note: This is code
5856     // that could be shared among different platforms - however the
5857     // benefit seems to be smaller than the disadvantage of having a
5858     // much more complicated generator structure. See also comment in
5859     // stubRoutines.hpp.
5860 
5861     StubRoutines::_forward_exception_entry = generate_forward_exception();
5862 
5863     StubRoutines::_call_stub_entry =
5864       generate_call_stub(StubRoutines::_call_stub_return_address);
5865 
5866     // is referenced by megamorphic call
5867     StubRoutines::_catch_exception_entry = generate_catch_exception();
5868 
5869     // Build this early so it&#39;s available for the interpreter.
5870     StubRoutines::_throw_StackOverflowError_entry =
5871       generate_throw_exception(&quot;StackOverflowError throw_exception&quot;,
5872                                CAST_FROM_FN_PTR(address,
5873                                                 SharedRuntime::throw_StackOverflowError));
5874     StubRoutines::_throw_delayed_StackOverflowError_entry =
5875       generate_throw_exception(&quot;delayed StackOverflowError throw_exception&quot;,
5876                                CAST_FROM_FN_PTR(address,
5877                                                 SharedRuntime::throw_delayed_StackOverflowError));
5878     if (UseCRC32Intrinsics) {
5879       // set table address before stub generation which use it
5880       StubRoutines::_crc_table_adr = (address)StubRoutines::aarch64::_crc_table;
5881       StubRoutines::_updateBytesCRC32 = generate_updateBytesCRC32();
5882     }
5883 
5884     if (UseCRC32CIntrinsics) {
5885       StubRoutines::_updateBytesCRC32C = generate_updateBytesCRC32C();
5886     }
5887 
5888     // Disabled until JDK-8210858 is fixed
5889     // if (vmIntrinsics::is_intrinsic_available(vmIntrinsics::_dlog)) {
5890     //   StubRoutines::_dlog = generate_dlog();
5891     // }
5892 
5893     if (vmIntrinsics::is_intrinsic_available(vmIntrinsics::_dsin)) {
5894       StubRoutines::_dsin = generate_dsin_dcos(/* isCos = */ false);
5895     }
5896 
5897     if (vmIntrinsics::is_intrinsic_available(vmIntrinsics::_dcos)) {
5898       StubRoutines::_dcos = generate_dsin_dcos(/* isCos = */ true);
5899     }
5900 
5901 
5902     StubRoutines::_load_value_type_fields_in_regs =
5903          generate_return_value_stub(CAST_FROM_FN_PTR(address, SharedRuntime::load_value_type_fields_in_regs), &quot;load_value_type_fields_in_regs&quot;, false);
5904     StubRoutines::_store_value_type_fields_to_buf =
5905          generate_return_value_stub(CAST_FROM_FN_PTR(address, SharedRuntime::store_value_type_fields_to_buf), &quot;store_value_type_fields_to_buf&quot;, true);
5906   }
5907 
5908   void generate_all() {
5909     // support for verify_oop (must happen after universe_init)
5910     StubRoutines::_verify_oop_subroutine_entry     = generate_verify_oop();
5911     StubRoutines::_throw_AbstractMethodError_entry =
5912       generate_throw_exception(&quot;AbstractMethodError throw_exception&quot;,
5913                                CAST_FROM_FN_PTR(address,
5914                                                 SharedRuntime::
5915                                                 throw_AbstractMethodError));
5916 
5917     StubRoutines::_throw_IncompatibleClassChangeError_entry =
5918       generate_throw_exception(&quot;IncompatibleClassChangeError throw_exception&quot;,
5919                                CAST_FROM_FN_PTR(address,
5920                                                 SharedRuntime::
5921                                                 throw_IncompatibleClassChangeError));
5922 
5923     StubRoutines::_throw_NullPointerException_at_call_entry =
5924       generate_throw_exception(&quot;NullPointerException at call throw_exception&quot;,
5925                                CAST_FROM_FN_PTR(address,
5926                                                 SharedRuntime::
5927                                                 throw_NullPointerException_at_call));
5928 
5929     // arraycopy stubs used by compilers
5930     generate_arraycopy_stubs();
5931 
5932     // has negatives stub for large arrays.
5933     StubRoutines::aarch64::_has_negatives = generate_has_negatives(StubRoutines::aarch64::_has_negatives_long);
5934 
5935     // array equals stub for large arrays.
5936     if (!UseSimpleArrayEquals) {
5937       StubRoutines::aarch64::_large_array_equals = generate_large_array_equals();
5938     }
5939 
5940     generate_compare_long_strings();
5941 
5942     generate_string_indexof_stubs();
5943 
5944     // byte_array_inflate stub for large arrays.
5945     StubRoutines::aarch64::_large_byte_array_inflate = generate_large_byte_array_inflate();
5946 
<a name="3" id="anc3"></a>



5947 #ifdef COMPILER2
5948     if (UseMultiplyToLenIntrinsic) {
5949       StubRoutines::_multiplyToLen = generate_multiplyToLen();
5950     }
5951 
5952     if (UseSquareToLenIntrinsic) {
5953       StubRoutines::_squareToLen = generate_squareToLen();
5954     }
5955 
5956     if (UseMulAddIntrinsic) {
5957       StubRoutines::_mulAdd = generate_mulAdd();
5958     }
5959 
5960     if (UseMontgomeryMultiplyIntrinsic) {
5961       StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;montgomeryMultiply&quot;);
5962       MontgomeryMultiplyGenerator g(_masm, /*squaring*/false);
5963       StubRoutines::_montgomeryMultiply = g.generate_multiply();
5964     }
5965 
5966     if (UseMontgomerySquareIntrinsic) {
5967       StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;montgomerySquare&quot;);
5968       MontgomeryMultiplyGenerator g(_masm, /*squaring*/true);
5969       // We use generate_multiply() rather than generate_square()
5970       // because it&#39;s faster for the sizes of modulus we care about.
5971       StubRoutines::_montgomerySquare = g.generate_multiply();
5972     }
5973 #endif // COMPILER2
5974 
5975     // generate GHASH intrinsics code
5976     if (UseGHASHIntrinsics) {
5977       StubRoutines::_ghash_processBlocks = generate_ghash_processBlocks();
5978     }
5979 
5980     // data cache line writeback
5981     StubRoutines::_data_cache_writeback = generate_data_cache_writeback();
5982     StubRoutines::_data_cache_writeback_sync = generate_data_cache_writeback_sync();
5983 
5984     if (UseAESIntrinsics) {
5985       StubRoutines::_aescrypt_encryptBlock = generate_aescrypt_encryptBlock();
5986       StubRoutines::_aescrypt_decryptBlock = generate_aescrypt_decryptBlock();
5987       StubRoutines::_cipherBlockChaining_encryptAESCrypt = generate_cipherBlockChaining_encryptAESCrypt();
5988       StubRoutines::_cipherBlockChaining_decryptAESCrypt = generate_cipherBlockChaining_decryptAESCrypt();
5989     }
5990 
5991     if (UseSHA1Intrinsics) {
5992       StubRoutines::_sha1_implCompress     = generate_sha1_implCompress(false,   &quot;sha1_implCompress&quot;);
5993       StubRoutines::_sha1_implCompressMB   = generate_sha1_implCompress(true,    &quot;sha1_implCompressMB&quot;);
5994     }
5995     if (UseSHA256Intrinsics) {
5996       StubRoutines::_sha256_implCompress   = generate_sha256_implCompress(false, &quot;sha256_implCompress&quot;);
5997       StubRoutines::_sha256_implCompressMB = generate_sha256_implCompress(true,  &quot;sha256_implCompressMB&quot;);
5998     }
5999 
6000     // generate Adler32 intrinsics code
6001     if (UseAdler32Intrinsics) {
6002       StubRoutines::_updateBytesAdler32 = generate_updateBytesAdler32();
6003     }
6004 
6005     // Safefetch stubs.
6006     generate_safefetch(&quot;SafeFetch32&quot;, sizeof(int),     &amp;StubRoutines::_safefetch32_entry,
6007                                                        &amp;StubRoutines::_safefetch32_fault_pc,
6008                                                        &amp;StubRoutines::_safefetch32_continuation_pc);
6009     generate_safefetch(&quot;SafeFetchN&quot;, sizeof(intptr_t), &amp;StubRoutines::_safefetchN_entry,
6010                                                        &amp;StubRoutines::_safefetchN_fault_pc,
6011                                                        &amp;StubRoutines::_safefetchN_continuation_pc);
6012     StubRoutines::aarch64::set_completed();
6013   }
6014 
6015  public:
6016   StubGenerator(CodeBuffer* code, bool all) : StubCodeGenerator(code) {
6017     if (all) {
6018       generate_all();
6019     } else {
6020       generate_initial();
6021     }
6022   }
6023 }; // end class declaration
6024 
6025 #define UCM_TABLE_MAX_ENTRIES 8
6026 void StubGenerator_generate(CodeBuffer* code, bool all) {
6027   if (UnsafeCopyMemory::_table == NULL) {
6028     UnsafeCopyMemory::create_table(UCM_TABLE_MAX_ENTRIES);
6029   }
6030   StubGenerator g(code, all);
6031 }
<a name="4" id="anc4"></a><b style="font-size: large; color: red">--- EOF ---</b>
















































































</pre>
<input id="eof" value="4" type="hidden" />
</body>
</html>