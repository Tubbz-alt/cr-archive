<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/share/opto/library_call.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="graphKit.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="macro.cpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/share/opto/library_call.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
 126 
 127   ciMethod*         caller()    const    { return jvms()-&gt;method(); }
 128   int               bci()       const    { return jvms()-&gt;bci(); }
 129   LibraryIntrinsic* intrinsic() const    { return _intrinsic; }
 130   vmIntrinsics::ID  intrinsic_id() const { return _intrinsic-&gt;intrinsic_id(); }
 131   ciMethod*         callee()    const    { return _intrinsic-&gt;method(); }
 132 
 133   bool  try_to_inline(int predicate);
 134   Node* try_to_predicate(int predicate);
 135 
 136   void push_result() {
 137     // Push the result onto the stack.
 138     Node* res = result();
 139     if (!stopped() &amp;&amp; res != NULL) {
 140       BasicType bt = res-&gt;bottom_type()-&gt;basic_type();
 141       if (C-&gt;inlining_incrementally() &amp;&amp; res-&gt;is_ValueType()) {
 142         // The caller expects an oop when incrementally inlining an intrinsic that returns an
 143         // inline type. Make sure the call is re-executed if the allocation triggers a deoptimization.
 144         PreserveReexecuteState preexecs(this);
 145         jvms()-&gt;set_should_reexecute(true);
<span class="line-modified"> 146         res = ValueTypePtrNode::make_from_value_type(this, res-&gt;as_ValueType());</span>
 147       }
 148       push_node(bt, res);
 149     }
 150   }
 151 
 152  private:
 153   void fatal_unexpected_iid(vmIntrinsics::ID iid) {
 154     fatal(&quot;unexpected intrinsic %d: %s&quot;, iid, vmIntrinsics::name_at(iid));
 155   }
 156 
 157   void  set_result(Node* n) { assert(_result == NULL, &quot;only set once&quot;); _result = n; }
 158   void  set_result(RegionNode* region, PhiNode* value);
 159   Node*     result() { return _result; }
 160 
 161   virtual int reexecute_sp() { return _reexecute_sp; }
 162 
 163   // Helper functions to inline natives
 164   Node* generate_guard(Node* test, RegionNode* region, float true_prob);
 165   Node* generate_slow_guard(Node* test, RegionNode* region);
 166   Node* generate_fair_guard(Node* test, RegionNode* region);
</pre>
<hr />
<pre>
2505         }
2506 
2507         ciField* f = vk-&gt;get_non_flattened_field_by_offset((int)off);
2508 
2509         if (f != NULL) {
2510           BasicType bt = f-&gt;layout_type();
2511           if (bt == T_ARRAY || bt == T_NARROWOOP) {
2512             bt = T_OBJECT;
2513           }
2514           if (bt == type) {
2515             if (bt != T_VALUETYPE || f-&gt;type() == value_klass) {
2516               set_result(vt-&gt;field_value_by_offset((int)off, false));
2517               return true;
2518             }
2519           }
2520         }
2521       }
2522       // Re-execute the unsafe access if allocation triggers deoptimization.
2523       PreserveReexecuteState preexecs(this);
2524       jvms()-&gt;set_should_reexecute(true);
<span class="line-modified">2525       vt = vt-&gt;allocate(this)-&gt;as_ValueType();</span>
<span class="line-removed">2526       base = vt-&gt;get_oop();</span>
2527     }
2528   }
2529 
2530   // 32-bit machines ignore the high half!
2531   offset = ConvL2X(offset);
2532   adr = make_unsafe_address(base, offset, is_store ? ACCESS_WRITE : ACCESS_READ, type, kind == Relaxed);
2533 
2534   if (_gvn.type(base)-&gt;isa_ptr() == TypePtr::NULL_PTR) {
2535     if (type != T_OBJECT &amp;&amp; (value_klass == NULL || !value_klass-&gt;has_object_fields())) {
2536       decorators |= IN_NATIVE; // off-heap primitive access
2537     } else {
2538       return false; // off-heap oop accesses are not supported
2539     }
2540   } else {
2541     heap_base_oop = base; // on-heap or mixed access
2542   }
2543 
2544   // Can base be NULL? Otherwise, always on-heap access.
2545   bool can_access_non_heap = TypePtr::NULL_PTR-&gt;higher_equal(_gvn.type(base));
2546 
</pre>
</td>
<td>
<hr />
<pre>
 126 
 127   ciMethod*         caller()    const    { return jvms()-&gt;method(); }
 128   int               bci()       const    { return jvms()-&gt;bci(); }
 129   LibraryIntrinsic* intrinsic() const    { return _intrinsic; }
 130   vmIntrinsics::ID  intrinsic_id() const { return _intrinsic-&gt;intrinsic_id(); }
 131   ciMethod*         callee()    const    { return _intrinsic-&gt;method(); }
 132 
 133   bool  try_to_inline(int predicate);
 134   Node* try_to_predicate(int predicate);
 135 
 136   void push_result() {
 137     // Push the result onto the stack.
 138     Node* res = result();
 139     if (!stopped() &amp;&amp; res != NULL) {
 140       BasicType bt = res-&gt;bottom_type()-&gt;basic_type();
 141       if (C-&gt;inlining_incrementally() &amp;&amp; res-&gt;is_ValueType()) {
 142         // The caller expects an oop when incrementally inlining an intrinsic that returns an
 143         // inline type. Make sure the call is re-executed if the allocation triggers a deoptimization.
 144         PreserveReexecuteState preexecs(this);
 145         jvms()-&gt;set_should_reexecute(true);
<span class="line-modified"> 146         res = res-&gt;as_ValueType()-&gt;buffer(this);</span>
 147       }
 148       push_node(bt, res);
 149     }
 150   }
 151 
 152  private:
 153   void fatal_unexpected_iid(vmIntrinsics::ID iid) {
 154     fatal(&quot;unexpected intrinsic %d: %s&quot;, iid, vmIntrinsics::name_at(iid));
 155   }
 156 
 157   void  set_result(Node* n) { assert(_result == NULL, &quot;only set once&quot;); _result = n; }
 158   void  set_result(RegionNode* region, PhiNode* value);
 159   Node*     result() { return _result; }
 160 
 161   virtual int reexecute_sp() { return _reexecute_sp; }
 162 
 163   // Helper functions to inline natives
 164   Node* generate_guard(Node* test, RegionNode* region, float true_prob);
 165   Node* generate_slow_guard(Node* test, RegionNode* region);
 166   Node* generate_fair_guard(Node* test, RegionNode* region);
</pre>
<hr />
<pre>
2505         }
2506 
2507         ciField* f = vk-&gt;get_non_flattened_field_by_offset((int)off);
2508 
2509         if (f != NULL) {
2510           BasicType bt = f-&gt;layout_type();
2511           if (bt == T_ARRAY || bt == T_NARROWOOP) {
2512             bt = T_OBJECT;
2513           }
2514           if (bt == type) {
2515             if (bt != T_VALUETYPE || f-&gt;type() == value_klass) {
2516               set_result(vt-&gt;field_value_by_offset((int)off, false));
2517               return true;
2518             }
2519           }
2520         }
2521       }
2522       // Re-execute the unsafe access if allocation triggers deoptimization.
2523       PreserveReexecuteState preexecs(this);
2524       jvms()-&gt;set_should_reexecute(true);
<span class="line-modified">2525       base = vt-&gt;buffer(this)-&gt;get_oop();</span>

2526     }
2527   }
2528 
2529   // 32-bit machines ignore the high half!
2530   offset = ConvL2X(offset);
2531   adr = make_unsafe_address(base, offset, is_store ? ACCESS_WRITE : ACCESS_READ, type, kind == Relaxed);
2532 
2533   if (_gvn.type(base)-&gt;isa_ptr() == TypePtr::NULL_PTR) {
2534     if (type != T_OBJECT &amp;&amp; (value_klass == NULL || !value_klass-&gt;has_object_fields())) {
2535       decorators |= IN_NATIVE; // off-heap primitive access
2536     } else {
2537       return false; // off-heap oop accesses are not supported
2538     }
2539   } else {
2540     heap_base_oop = base; // on-heap or mixed access
2541   }
2542 
2543   // Can base be NULL? Otherwise, always on-heap access.
2544   bool can_access_non_heap = TypePtr::NULL_PTR-&gt;higher_equal(_gvn.type(base));
2545 
</pre>
</td>
</tr>
</table>
<center><a href="graphKit.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="macro.cpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>