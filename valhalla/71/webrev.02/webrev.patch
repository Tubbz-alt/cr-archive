diff a/src/hotspot/share/opto/callGenerator.cpp b/src/hotspot/share/opto/callGenerator.cpp
--- a/src/hotspot/share/opto/callGenerator.cpp
+++ b/src/hotspot/share/opto/callGenerator.cpp
@@ -872,18 +872,18 @@
     Node* m = kit.map()->in(i);
     Node* n = slow_map->in(i);
     const Type* t = gvn.type(m)->meet_speculative(gvn.type(n));
     if (m->is_ValueType() && !t->isa_valuetype()) {
       // Allocate value type in fast path
-      m = ValueTypePtrNode::make_from_value_type(&kit, m->as_ValueType());
+      m = m->as_ValueType()->buffer(&kit);
       kit.map()->set_req(i, m);
     }
     if (n->is_ValueType() && !t->isa_valuetype()) {
       // Allocate value type in slow path
       PreserveJVMState pjvms(&kit);
       kit.set_map(slow_map);
-      n = ValueTypePtrNode::make_from_value_type(&kit, n->as_ValueType());
+      n = n->as_ValueType()->buffer(&kit);
       kit.map()->set_req(i, n);
       slow_map = kit.stop();
     }
   }
 
diff a/src/hotspot/share/opto/callnode.cpp b/src/hotspot/share/opto/callnode.cpp
--- a/src/hotspot/share/opto/callnode.cpp
+++ b/src/hotspot/share/opto/callnode.cpp
@@ -1644,65 +1644,10 @@
   if (analyzer->is_arg_stack(0) || analyzer->is_arg_local(0)) {
     _is_allocation_MemBar_redundant = true;
   }
 }
 
-Node* AllocateNode::Ideal(PhaseGVN* phase, bool can_reshape) {
-  // Check for unused value type allocation
-  if (can_reshape && in(AllocateNode::ValueNode) != NULL &&
-      outcnt() != 0 && result_cast() == NULL) {
-    // Remove allocation by replacing the projection nodes with its inputs
-    InitializeNode* init = initialization();
-    PhaseIterGVN* igvn = phase->is_IterGVN();
-    CallProjections* projs = extract_projections(true, false);
-    assert(projs->nb_resproj <= 1, "unexpected number of results");
-    if (projs->fallthrough_catchproj != NULL) {
-      igvn->replace_node(projs->fallthrough_catchproj, in(TypeFunc::Control));
-    }
-    if (projs->fallthrough_memproj != NULL) {
-      igvn->replace_node(projs->fallthrough_memproj, in(TypeFunc::Memory));
-    }
-    if (projs->catchall_memproj != NULL) {
-      igvn->replace_node(projs->catchall_memproj, phase->C->top());
-    }
-    if (projs->fallthrough_ioproj != NULL) {
-      igvn->replace_node(projs->fallthrough_ioproj, in(TypeFunc::I_O));
-    }
-    if (projs->catchall_ioproj != NULL) {
-      igvn->replace_node(projs->catchall_ioproj, phase->C->top());
-    }
-    if (projs->catchall_catchproj != NULL) {
-      igvn->replace_node(projs->catchall_catchproj, phase->C->top());
-    }
-    if (projs->resproj[0] != NULL) {
-      // Remove MemBarStoreStore user as well
-      for (DUIterator_Fast imax, i = projs->resproj[0]->fast_outs(imax); i < imax; i++) {
-        MemBarStoreStoreNode* mb = projs->resproj[0]->fast_out(i)->isa_MemBarStoreStore();
-        if (mb != NULL && mb->outcnt() == 2) {
-          mb->remove(igvn);
-          --i; --imax;
-        }
-      }
-      igvn->replace_node(projs->resproj[0], phase->C->top());
-    }
-    igvn->replace_node(this, phase->C->top());
-    if (init != NULL) {
-      Node* ctrl_proj = init->proj_out_or_null(TypeFunc::Control);
-      Node* mem_proj = init->proj_out_or_null(TypeFunc::Memory);
-      if (ctrl_proj != NULL) {
-        igvn->replace_node(ctrl_proj, init->in(TypeFunc::Control));
-      }
-      if (mem_proj != NULL) {
-        igvn->replace_node(mem_proj, init->in(TypeFunc::Memory));
-      }
-    }
-    return NULL;
-  }
-
-  return CallNode::Ideal(phase, can_reshape);
-}
-
 Node* AllocateNode::make_ideal_mark(PhaseGVN* phase, Node* control, Node* mem) {
   Node* mark_node = NULL;
   // For now only enable fast locking for non-array types
   if ((EnableValhalla || UseBiasedLocking) && Opcode() == Op_Allocate) {
     Node* klass_node = in(AllocateNode::KlassNode);
diff a/src/hotspot/share/opto/callnode.hpp b/src/hotspot/share/opto/callnode.hpp
--- a/src/hotspot/share/opto/callnode.hpp
+++ b/src/hotspot/share/opto/callnode.hpp
@@ -926,12 +926,10 @@
   }
   virtual int Opcode() const;
   virtual uint ideal_reg() const { return Op_RegP; }
   virtual bool        guaranteed_safepoint()  { return false; }
 
-  virtual Node* Ideal(PhaseGVN* phase, bool can_reshape);
-
   // allocations do not modify their arguments
   virtual bool        may_modify(const TypeOopPtr *t_oop, PhaseTransform *phase) { return false;}
 
   // Pattern-match a possible usage of AllocateNode.
   // Return null if no allocation is recognized.
diff a/src/hotspot/share/opto/cfgnode.cpp b/src/hotspot/share/opto/cfgnode.cpp
--- a/src/hotspot/share/opto/cfgnode.cpp
+++ b/src/hotspot/share/opto/cfgnode.cpp
@@ -2154,10 +2154,12 @@
   // (MergeMemNode is not dead_loop_safe - need to check for dead loop.)
   if (progress == NULL && can_reshape && type() == Type::MEMORY) {
     // see if this phi should be sliced
     uint merge_width = 0;
     bool saw_self = false;
+    // TODO revisit this with JDK-8247216
+    bool mergemem_only = true;
     for( uint i=1; i<req(); ++i ) {// For all paths in
       Node *ii = in(i);
       // TOP inputs should not be counted as safe inputs because if the
       // Phi references itself through all other inputs then splitting the
       // Phi through memory merges would create dead loop at later stage.
@@ -2166,15 +2168,17 @@
       }
       if (ii->is_MergeMem()) {
         MergeMemNode* n = ii->as_MergeMem();
         merge_width = MAX2(merge_width, n->req());
         saw_self = saw_self || phase->eqv(n->base_memory(), this);
+      } else {
+        mergemem_only = false;
       }
     }
 
     // This restriction is temporarily necessary to ensure termination:
-    if (!saw_self && adr_type() == TypePtr::BOTTOM)  merge_width = 0;
+    if (!mergemem_only && !saw_self && adr_type() == TypePtr::BOTTOM)  merge_width = 0;
 
     if (merge_width > Compile::AliasIdxRaw) {
       // found at least one non-empty MergeMem
       const TypePtr* at = adr_type();
       if (at != TypePtr::BOTTOM) {
diff a/src/hotspot/share/opto/compile.cpp b/src/hotspot/share/opto/compile.cpp
--- a/src/hotspot/share/opto/compile.cpp
+++ b/src/hotspot/share/opto/compile.cpp
@@ -405,12 +405,15 @@
     if (!useful.member(opaq)) {
       remove_opaque4_node(opaq);
     }
   }
   // Remove useless value type nodes
-  if (_value_type_nodes != NULL) {
-    _value_type_nodes->remove_useless_nodes(useful.member_set());
+  for (int i = _value_type_nodes->length() - 1; i >= 0; i--) {
+    Node* vt = _value_type_nodes->at(i);
+    if (!useful.member(vt)) {
+      _value_type_nodes->remove(vt);
+    }
   }
   BarrierSetC2* bs = BarrierSet::barrier_set()->barrier_set_c2();
   bs->eliminate_useless_gc_barriers(useful, this);
   // clean up the late inline lists
   remove_useless_late_inlines(&_string_late_inlines, useful);
@@ -1018,11 +1021,11 @@
   _macro_nodes = new(comp_arena()) GrowableArray<Node*>(comp_arena(), 8,  0, NULL);
   _predicate_opaqs = new(comp_arena()) GrowableArray<Node*>(comp_arena(), 8,  0, NULL);
   _expensive_nodes = new(comp_arena()) GrowableArray<Node*>(comp_arena(), 8,  0, NULL);
   _range_check_casts = new(comp_arena()) GrowableArray<Node*>(comp_arena(), 8,  0, NULL);
   _opaque4_nodes = new(comp_arena()) GrowableArray<Node*>(comp_arena(), 8,  0, NULL);
-  _value_type_nodes = new (comp_arena()) Unique_Node_List(comp_arena());
+  _value_type_nodes = new(comp_arena()) GrowableArray<Node*>(comp_arena(), 8,  0, NULL);
   register_library_intrinsics();
 #ifdef ASSERT
   _type_verify_symmetry = true;
 #endif
 }
@@ -1874,28 +1877,29 @@
   }
 }
 
 void Compile::remove_value_type(Node* n) {
   assert(n->is_ValueTypeBase(), "unexpected node");
-  if (_value_type_nodes != NULL) {
+  if (_value_type_nodes != NULL && _value_type_nodes->contains(n)) {
     _value_type_nodes->remove(n);
   }
 }
 
-// Does the return value keep otherwise useless value type allocations
-// alive?
+// Does the return value keep otherwise useless value type allocations alive?
 static bool return_val_keeps_allocations_alive(Node* ret_val) {
   ResourceMark rm;
   Unique_Node_List wq;
   wq.push(ret_val);
   bool some_allocations = false;
   for (uint i = 0; i < wq.size(); i++) {
     Node* n = wq.at(i);
-    assert(!n->is_ValueTypeBase(), "chain of value type nodes");
+    assert(!n->is_ValueType(), "chain of value type nodes");
     if (n->outcnt() > 1) {
       // Some other use for the allocation
       return false;
+    } else if (n->is_ValueTypePtr()) {
+      wq.push(n->in(1));
     } else if (n->is_Phi()) {
       for (uint j = 1; j < n->req(); j++) {
         wq.push(n->in(j));
       }
     } else if (n->is_CheckCastPP() &&
@@ -1905,22 +1909,28 @@
     }
   }
   return some_allocations;
 }
 
-void Compile::process_value_types(PhaseIterGVN &igvn) {
+void Compile::process_value_types(PhaseIterGVN &igvn, bool post_ea) {
   // Make value types scalar in safepoints
-  while (_value_type_nodes->size() != 0) {
-    ValueTypeBaseNode* vt = _value_type_nodes->pop()->as_ValueTypeBase();
+  for (int i = _value_type_nodes->length()-1; i >= 0; i--) {
+    ValueTypeBaseNode* vt = _value_type_nodes->at(i)->as_ValueTypeBase();
     vt->make_scalar_in_safepoints(&igvn);
-    if (vt->is_ValueTypePtr()) {
-      igvn.replace_node(vt, vt->get_oop());
-    } else if (vt->outcnt() == 0) {
-      igvn.remove_dead_node(vt);
+  }
+  // Remove ValueTypePtr nodes only after EA to give scalar replacement a chance
+  // to remove buffer allocations. ValueType nodes are kept until loop opts and
+  // removed via ValueTypeNode::remove_redundant_allocations.
+  if (post_ea) {
+    while (_value_type_nodes->length() > 0) {
+      ValueTypeBaseNode* vt = _value_type_nodes->pop()->as_ValueTypeBase();
+      if (vt->is_ValueTypePtr()) {
+        igvn.replace_node(vt, vt->get_oop());
+      }
     }
   }
-  _value_type_nodes = NULL;
+  // Make sure that the return value does not keep an unused allocation alive
   if (tf()->returns_value_type_as_fields()) {
     Node* ret = NULL;
     for (uint i = 1; i < root()->req(); i++){
       Node* in = root()->in(i);
       if (in->Opcode() == Op_Return) {
@@ -2485,11 +2495,11 @@
     set_for_igvn(&new_worklist);
     igvn = PhaseIterGVN(initial_gvn());
     igvn.optimize();
   }
 
-  if (_value_type_nodes->size() > 0) {
+  if (_value_type_nodes->length() > 0) {
     // Do this once all inlining is over to avoid getting inconsistent debug info
     process_value_types(igvn);
   }
 
   adjust_flattened_array_access_aliases(igvn);
@@ -2524,10 +2534,15 @@
 
       if (failing())  return;
     }
   }
 
+  if (_value_type_nodes->length() > 0) {
+    // Process value types again now that EA might have simplified the graph
+    process_value_types(igvn, /* post_ea= */ true);
+  }
+
   // Loop transforms on the ideal graph.  Range Check Elimination,
   // peeling, unrolling, etc.
 
   // Set loop opts counter
   if((_loop_opts_cnt > 0) && (has_loops() || has_split_ifs())) {
diff a/src/hotspot/share/opto/compile.hpp b/src/hotspot/share/opto/compile.hpp
--- a/src/hotspot/share/opto/compile.hpp
+++ b/src/hotspot/share/opto/compile.hpp
@@ -316,11 +316,11 @@
   GrowableArray<Node*>* _macro_nodes;           // List of nodes which need to be expanded before matching.
   GrowableArray<Node*>* _predicate_opaqs;       // List of Opaque1 nodes for the loop predicates.
   GrowableArray<Node*>* _expensive_nodes;       // List of nodes that are expensive to compute and that we'd better not let the GVN freely common
   GrowableArray<Node*>* _range_check_casts;     // List of CastII nodes with a range check dependency
   GrowableArray<Node*>* _opaque4_nodes;         // List of Opaque4 nodes that have a default value
-  Unique_Node_List*     _value_type_nodes;      // List of ValueType nodes
+  GrowableArray<Node*>* _value_type_nodes;      // List of ValueType nodes
   ConnectionGraph*      _congraph;
 #ifndef PRODUCT
   IdealGraphPrinter*    _printer;
   static IdealGraphPrinter* _debug_file_printer;
   static IdealGraphPrinter* _debug_network_printer;
@@ -712,12 +712,11 @@
   void  remove_opaque4_nodes(PhaseIterGVN &igvn);
 
   // Keep track of value type nodes for later processing
   void add_value_type(Node* n);
   void remove_value_type(Node* n);
-  void process_value_types(PhaseIterGVN &igvn);
-  bool can_add_value_type() const { return _value_type_nodes != NULL; }
+  void process_value_types(PhaseIterGVN &igvn, bool post_ea = false);
 
   void adjust_flattened_array_access_aliases(PhaseIterGVN& igvn);
 
   void sort_macro_nodes();
 
diff a/src/hotspot/share/opto/escape.cpp b/src/hotspot/share/opto/escape.cpp
--- a/src/hotspot/share/opto/escape.cpp
+++ b/src/hotspot/share/opto/escape.cpp
@@ -429,10 +429,11 @@
     }
     case Op_CastX2P: {
       map_ideal_node(n, phantom_obj);
       break;
     }
+    case Op_ValueTypePtr:
     case Op_CastPP:
     case Op_CheckCastPP:
     case Op_EncodeP:
     case Op_DecodeN:
     case Op_EncodePKlass:
@@ -600,10 +601,11 @@
       PointsToNode* ptn_base = ptnode_adr(base->_idx);
       assert(ptn_base != NULL, "field's base should be registered");
       add_base(n_ptn->as_Field(), ptn_base);
       break;
     }
+    case Op_ValueTypePtr:
     case Op_CastPP:
     case Op_CheckCastPP:
     case Op_EncodeP:
     case Op_DecodeN:
     case Op_EncodePKlass:
@@ -3256,11 +3258,11 @@
         } else if (!(op == Op_CmpP || op == Op_Conv2B ||
               op == Op_CastP2X || op == Op_StoreCM ||
               op == Op_FastLock || op == Op_AryEq || op == Op_StrComp || op == Op_HasNegatives ||
               op == Op_StrCompressedCopy || op == Op_StrInflatedCopy ||
               op == Op_StrEquals || op == Op_StrIndexOf || op == Op_StrIndexOfChar ||
-              op == Op_SubTypeCheck || op == Op_ValueType ||
+              op == Op_SubTypeCheck || op == Op_ValueType || op == Op_ValueTypePtr ||
               BarrierSet::barrier_set()->barrier_set_c2()->is_gc_barrier_node(use))) {
           n->dump();
           use->dump();
           assert(false, "EA: missing allocation reference path");
         }
diff a/src/hotspot/share/opto/graphKit.cpp b/src/hotspot/share/opto/graphKit.cpp
--- a/src/hotspot/share/opto/graphKit.cpp
+++ b/src/hotspot/share/opto/graphKit.cpp
@@ -1620,11 +1620,11 @@
   if (val->is_ValueType()) {
     // Store to non-flattened field. Buffer the inline type and make sure
     // the store is re-executed if the allocation triggers deoptimization.
     PreserveReexecuteState preexecs(this);
     jvms()->set_should_reexecute(true);
-    val = val->as_ValueType()->allocate(this, safe_for_replace)->get_oop();
+    val = val->as_ValueType()->buffer(this, safe_for_replace);
   }
 
   C2AccessValuePtr addr(adr, adr_type);
   C2AccessValue value(val, val_type);
   C2ParseAccess access(this, decorators | C2_WRITE_ACCESS, bt, obj, addr);
@@ -1813,14 +1813,13 @@
       // For example, see CompiledMethod::preserve_callee_argument_oops().
       call->set_override_symbolic_info(true);
       continue;
     } else if (arg->is_ValueType()) {
       // Pass value type argument via oop to callee
-      if (is_late_inline) {
-        arg = ValueTypePtrNode::make_from_value_type(this, arg->as_ValueType());
-      } else {
-        arg = arg->as_ValueType()->allocate(this)->get_oop();
+      arg = arg->as_ValueType()->buffer(this);
+      if (!is_late_inline) {
+        arg = arg->as_ValueTypePtr()->get_oop();
       }
     }
     call->init_req(idx++, arg);
     // Skip reserved arguments
     BasicType bt = t->basic_type();
@@ -4580,13 +4579,14 @@
   const Type* con_type = Type::make_constant_from_field(field, holder, field->layout_type(),
                                                         /*is_unsigned_load=*/false);
   if (con_type != NULL) {
     Node* con = makecon(con_type);
     assert(!field->is_flattenable() || (field->is_static() && !con_type->is_zero_type()), "sanity");
-    if (field->layout_type() == T_VALUETYPE && field->type()->as_value_klass()->is_scalarizable()) {
+    // Check type of constant which might be more precise
+    if (con_type->is_valuetypeptr() && con_type->value_klass()->is_scalarizable()) {
       // Load value type from constant oop
-      con = ValueTypeNode::make_from_oop(this, con, field->type()->as_value_klass());
+      con = ValueTypeNode::make_from_oop(this, con, con_type->value_klass());
     }
     return con;
   }
   return NULL;
 }
diff a/src/hotspot/share/opto/library_call.cpp b/src/hotspot/share/opto/library_call.cpp
--- a/src/hotspot/share/opto/library_call.cpp
+++ b/src/hotspot/share/opto/library_call.cpp
@@ -141,11 +141,11 @@
       if (C->inlining_incrementally() && res->is_ValueType()) {
         // The caller expects an oop when incrementally inlining an intrinsic that returns an
         // inline type. Make sure the call is re-executed if the allocation triggers a deoptimization.
         PreserveReexecuteState preexecs(this);
         jvms()->set_should_reexecute(true);
-        res = ValueTypePtrNode::make_from_value_type(this, res->as_ValueType());
+        res = res->as_ValueType()->buffer(this);
       }
       push_node(bt, res);
     }
   }
 
@@ -2520,12 +2520,11 @@
         }
       }
       // Re-execute the unsafe access if allocation triggers deoptimization.
       PreserveReexecuteState preexecs(this);
       jvms()->set_should_reexecute(true);
-      vt = vt->allocate(this)->as_ValueType();
-      base = vt->get_oop();
+      base = vt->buffer(this)->get_oop();
     }
   }
 
   // 32-bit machines ignore the high half!
   offset = ConvL2X(offset);
diff a/src/hotspot/share/opto/macro.cpp b/src/hotspot/share/opto/macro.cpp
--- a/src/hotspot/share/opto/macro.cpp
+++ b/src/hotspot/share/opto/macro.cpp
@@ -979,11 +979,11 @@
     igvn.replace_node(mem_proj, n->in(TypeFunc::Memory));
   }
 }
 
 // Process users of eliminated allocation.
-void PhaseMacroExpand::process_users_of_allocation(CallNode *alloc) {
+void PhaseMacroExpand::process_users_of_allocation(CallNode *alloc, bool inline_alloc) {
   Node* res = alloc->result_cast();
   if (res != NULL) {
     for (DUIterator_Last jmin, j = res->last_outs(jmin); j >= jmin; ) {
       Node *use = res->last_out(j);
       uint oc1 = res->outcnt();
@@ -999,11 +999,12 @@
             for (DUIterator_Fast pmax, p = n->fast_outs(pmax);
                                        p < pmax; p++) {
               Node* mb = n->fast_out(p);
               assert(mb->is_Initialize() || !mb->is_MemBar() ||
                      mb->req() <= MemBarNode::Precedent ||
-                     mb->in(MemBarNode::Precedent) != n,
+                     mb->in(MemBarNode::Precedent) != n ||
+                     (inline_alloc && !ReduceInitialCardMarks),
                      "MemBarVolatile should be eliminated for non-escaping object");
             }
 #endif
             _igvn.replace_node(n, n->in(MemNode::Memory));
           } else {
@@ -1086,10 +1087,15 @@
         // Eliminate Initialize node.
         InitializeNode *init = use->as_Initialize();
         assert(init->outcnt() <= 2, "only a control and memory projection expected");
         Node *ctrl_proj = init->proj_out_or_null(TypeFunc::Control);
         if (ctrl_proj != NULL) {
+          // Inline type buffer allocations are followed by a membar
+          Node* membar_after = ctrl_proj->unique_ctrl_out();
+          if (inline_alloc && membar_after->Opcode() == Op_MemBarCPUOrder) {
+            membar_after->as_MemBar()->remove(&_igvn);
+          }
           _igvn.replace_node(ctrl_proj, init->in(TypeFunc::Control));
 #ifdef ASSERT
           Node* tmp = init->in(TypeFunc::Control);
           assert(tmp == _fallthroughcatchproj, "allocation control projection");
 #endif
@@ -1104,10 +1110,14 @@
             assert(mem == _memproj_fallthrough, "allocation memory projection");
           }
 #endif
           _igvn.replace_node(mem_proj, mem);
         }
+      } else if (use->Opcode() == Op_MemBarStoreStore) {
+        // Inline type buffer allocations are followed by a membar
+        assert(inline_alloc, "Unexpected MemBarStoreStore");
+        use->as_MemBar()->remove(&_igvn);
       } else  {
         assert(false, "only Initialize or AddP expected");
       }
       j -= (oc1 - _resproj->outcnt());
     }
@@ -1135,22 +1145,29 @@
 bool PhaseMacroExpand::eliminate_allocate_node(AllocateNode *alloc) {
   // Don't do scalar replacement if the frame can be popped by JVMTI:
   // if reallocation fails during deoptimization we'll pop all
   // interpreter frames for this compiled frame and that won't play
   // nice with JVMTI popframe.
-  if (!EliminateAllocations || JvmtiExport::can_pop_frame() || !alloc->_is_non_escaping) {
+  if (!EliminateAllocations || JvmtiExport::can_pop_frame()) {
     return false;
   }
   Node* klass = alloc->in(AllocateNode::KlassNode);
   const TypeKlassPtr* tklass = _igvn.type(klass)->is_klassptr();
-  Node* res = alloc->result_cast();
+
+  // Attempt to eliminate inline type buffer allocations
+  // regardless of usage and escape/replaceable status.
+  bool inline_alloc = tklass->klass()->is_valuetype();
+  if (!alloc->_is_non_escaping && !inline_alloc) {
+    return false;
+  }
   // Eliminate boxing allocations which are not used
-  // regardless scalar replacable status.
-  bool boxing_alloc = C->eliminate_boxing() &&
-                      tklass->klass()->is_instance_klass()  &&
+  // regardless of scalar replaceable status.
+  Node* res = alloc->result_cast();
+  bool boxing_alloc = (res == NULL) && C->eliminate_boxing() &&
+                      tklass->klass()->is_instance_klass() &&
                       tklass->klass()->as_instance_klass()->is_box_klass();
-  if (!alloc->_is_scalar_replaceable && (!boxing_alloc || (res != NULL))) {
+  if (!alloc->_is_scalar_replaceable && !boxing_alloc && !inline_alloc) {
     return false;
   }
 
   extract_call_projections(alloc);
 
@@ -1158,15 +1175,16 @@
   if (!can_eliminate_allocation(alloc, safepoints)) {
     return false;
   }
 
   if (!alloc->_is_scalar_replaceable) {
-    assert(res == NULL, "sanity");
+    assert(res == NULL || inline_alloc, "sanity");
     // We can only eliminate allocation if all debug info references
     // are already replaced with SafePointScalarObject because
     // we can't search for a fields value without instance_id.
     if (safepoints.length() > 0) {
+      assert(!inline_alloc, "Inline type allocations should not have safepoint uses");
       return false;
     }
   }
 
   if (!scalar_replacement(alloc, safepoints)) {
@@ -1183,11 +1201,11 @@
       p = p->caller();
     }
     log->tail("eliminate_allocation");
   }
 
-  process_users_of_allocation(alloc);
+  process_users_of_allocation(alloc, inline_alloc);
 
 #ifndef PRODUCT
   if (PrintEliminateAllocations) {
     if (alloc->is_AllocateArray())
       tty->print_cr("++++ Eliminated: %d AllocateArray", alloc->_idx);
diff a/src/hotspot/share/opto/macro.hpp b/src/hotspot/share/opto/macro.hpp
--- a/src/hotspot/share/opto/macro.hpp
+++ b/src/hotspot/share/opto/macro.hpp
@@ -108,11 +108,11 @@
 
   bool eliminate_boxing_node(CallStaticJavaNode *boxing);
   bool eliminate_allocate_node(AllocateNode *alloc);
   bool can_eliminate_allocation(AllocateNode *alloc, GrowableArray <SafePointNode *>& safepoints);
   bool scalar_replacement(AllocateNode *alloc, GrowableArray <SafePointNode *>& safepoints_done);
-  void process_users_of_allocation(CallNode *alloc);
+  void process_users_of_allocation(CallNode *alloc, bool inline_alloc = false);
 
   void eliminate_gc_barrier(Node *p2x);
   void mark_eliminated_box(Node* box, Node* obj);
   void mark_eliminated_locking_nodes(AbstractLockNode *alock);
   bool eliminate_locking_node(AbstractLockNode *alock);
diff a/src/hotspot/share/opto/parse1.cpp b/src/hotspot/share/opto/parse1.cpp
--- a/src/hotspot/share/opto/parse1.cpp
+++ b/src/hotspot/share/opto/parse1.cpp
@@ -1723,11 +1723,11 @@
         t = target->stack_type_at(j - tmp_jvms->stkoff());
       }
       if (t != NULL && t != Type::BOTTOM) {
         if (n->is_ValueType() && !t->isa_valuetype()) {
           // Allocate value type in src block to be able to merge it with oop in target block
-          map()->set_req(j, ValueTypePtrNode::make_from_value_type(this, n->as_ValueType()));
+          map()->set_req(j, n->as_ValueType()->buffer(this));
         }
         assert(!t->isa_valuetype() || n->is_ValueType(), "inconsistent typeflow info");
       }
     }
   }
@@ -2363,11 +2363,11 @@
       // Value type is returned as oop, make sure it is buffered and re-execute
       // if allocation triggers deoptimization.
       PreserveReexecuteState preexecs(this);
       jvms()->set_should_reexecute(true);
       inc_sp(1);
-      value = ValueTypePtrNode::make_from_value_type(this, value->as_ValueType());
+      value = value->as_ValueType()->buffer(this);
       if (Compile::current()->inlining_incrementally()) {
         value = value->as_ValueTypeBase()->allocate_fields(this);
       }
     } else if (tr && tr->isa_instptr() && tr->klass()->is_loaded() && tr->klass()->is_interface()) {
       // If returning oops to an interface-return, there is a silent free
diff a/src/hotspot/share/opto/parse2.cpp b/src/hotspot/share/opto/parse2.cpp
--- a/src/hotspot/share/opto/parse2.cpp
+++ b/src/hotspot/share/opto/parse2.cpp
@@ -120,11 +120,11 @@
         Node* casted_adr = array_element_address(cast, idx, T_VALUETYPE, ary_t->size(), control());
         // Re-execute flattened array load if buffering triggers deoptimization
         PreserveReexecuteState preexecs(this);
         jvms()->set_should_reexecute(true);
         inc_sp(2);
-        Node* vt = ValueTypeNode::make_from_flattened(this, vk, cast, casted_adr)->allocate(this, false)->get_oop();
+        Node* vt = ValueTypeNode::make_from_flattened(this, vk, cast, casted_adr)->buffer(this, false);
         ideal.set(res, vt);
         ideal.sync_kit(this);
       } else {
         // Element type is unknown, emit runtime call
         Node* kls = load_object_klass(ary);
@@ -2065,17 +2065,17 @@
   // Allocate value type operands and re-execute on deoptimization
   if (a->is_ValueType()) {
     PreserveReexecuteState preexecs(this);
     inc_sp(2);
     jvms()->set_should_reexecute(true);
-    a = a->as_ValueType()->allocate(this)->get_oop();
+    a = a->as_ValueType()->buffer(this)->get_oop();
   }
   if (b->is_ValueType()) {
     PreserveReexecuteState preexecs(this);
     inc_sp(2);
     jvms()->set_should_reexecute(true);
-    b = b->as_ValueType()->allocate(this)->get_oop();
+    b = b->as_ValueType()->buffer(this)->get_oop();
   }
 
   // First, do a normal pointer comparison
   const TypeOopPtr* ta = _gvn.type(a)->isa_oopptr();
   const TypeOopPtr* tb = _gvn.type(b)->isa_oopptr();
diff a/src/hotspot/share/opto/parseHelper.cpp b/src/hotspot/share/opto/parseHelper.cpp
--- a/src/hotspot/share/opto/parseHelper.cpp
+++ b/src/hotspot/share/opto/parseHelper.cpp
@@ -375,11 +375,11 @@
     // Non-flattenable field value needs to be allocated because it can be merged
     // with an oop. Re-execute withfield if buffering triggers deoptimization.
     PreserveReexecuteState preexecs(this);
     jvms()->set_should_reexecute(true);
     inc_sp(nargs);
-    val = ValueTypePtrNode::make_from_value_type(this, val->as_ValueType());
+    val = val->as_ValueType()->buffer(this);
   }
 
   // Clone the value type node and set the new field value
   ValueTypeNode* new_vt = holder->clone()->as_ValueType();
   new_vt->set_oop(_gvn.zerocon(T_VALUETYPE));
@@ -390,11 +390,11 @@
   if (!holder_klass->is_scalarizable()) {
     // Re-execute withfield if buffering triggers deoptimization
     PreserveReexecuteState preexecs(this);
     jvms()->set_should_reexecute(true);
     inc_sp(nargs);
-    res = new_vt->allocate(this)->get_oop();
+    res = new_vt->buffer(this)->get_oop();
   }
   push(_gvn.transform(res));
 }
 
 #ifndef PRODUCT
diff a/src/hotspot/share/opto/valuetypenode.cpp b/src/hotspot/share/opto/valuetypenode.cpp
--- a/src/hotspot/share/opto/valuetypenode.cpp
+++ b/src/hotspot/share/opto/valuetypenode.cpp
@@ -45,13 +45,13 @@
 
   // Create a PhiNode each for merging the field values
   for (uint i = 0; i < vt->field_count(); ++i) {
     ciType* type = vt->field_type(i);
     Node*  value = vt->field_value(i);
-    if (type->is_valuetype() && value->isa_ValueType()) {
+    if (value->is_ValueTypeBase()) {
       // Handle flattened value type fields recursively
-      value = value->as_ValueType()->clone_with_phis(gvn, region);
+      value = value->as_ValueTypeBase()->clone_with_phis(gvn, region);
     } else {
       phi_type = Type::get_const_type(type);
       value = PhiNode::make(region, value, phi_type);
       gvn->set_type(value, phi_type);
     }
@@ -93,12 +93,12 @@
   }
   // Merge field values
   for (uint i = 0; i < field_count(); ++i) {
     Node* val1 =        field_value(i);
     Node* val2 = other->field_value(i);
-    if (val1->is_ValueType()) {
-      val1->as_ValueType()->merge_with(gvn, val2->as_ValueType(), pnum, transform);
+    if (val1->is_ValueTypeBase()) {
+      val1->as_ValueTypeBase()->merge_with(gvn, val2->as_ValueTypeBase(), pnum, transform);
     } else {
       assert(val1->is_Phi(), "must be a phi node");
       assert(!val2->is_ValueType(), "inconsistent merge values");
       val1->set_req(pnum, val2);
     }
@@ -251,10 +251,11 @@
   // Now scalarize non-flattened fields
   for (uint i = 0; i < worklist.size(); ++i) {
     Node* vt = worklist.at(i);
     vt->as_ValueType()->make_scalar_in_safepoints(igvn);
   }
+  igvn->record_for_igvn(this);
 }
 
 const TypePtr* ValueTypeBaseNode::field_adr_type(Node* base, int offset, ciInstanceKlass* holder, DecoratorSet decorators, PhaseGVN& gvn) const {
   const TypeAryPtr* ary_type = gvn.type(base)->isa_aryptr();
   const TypePtr* adr_type = NULL;
@@ -294,12 +295,14 @@
         assert(field != NULL, "field not found");
         ciConstant constant = constant_oop->as_instance()->field_value(field);
         const Type* con_type = Type::make_from_constant(constant, /*require_const=*/ true);
         assert(con_type != NULL, "type not found");
         value = kit->gvn().transform(kit->makecon(con_type));
-        if (ft->is_valuetype() && !constant.as_object()->is_null_object()) {
+        // Check type of constant which might be more precise
+        if (con_type->is_valuetypeptr() && !con_type->is_zero_type()) {
           // Null-free, treat as flattenable
+          ft = con_type->value_klass();
           is_flattenable = true;
         }
       } else {
         // Load field value from memory
         const TypePtr* adr_type = field_adr_type(base, offset, holder, decorators, kit->gvn());
@@ -362,17 +365,18 @@
       kit->access_store_at(base, adr, adr_type, value, val_type, bt, decorators);
     }
   }
 }
 
-ValueTypeBaseNode* ValueTypeBaseNode::allocate(GraphKit* kit, bool safe_for_replace) {
+ValueTypePtrNode* ValueTypeBaseNode::buffer(GraphKit* kit, bool safe_for_replace) {
+  assert(is_ValueType(), "sanity");
   // Check if value type is already allocated
   Node* null_ctl = kit->top();
   Node* not_null_oop = kit->null_check_oop(get_oop(), &null_ctl);
   if (null_ctl->is_top()) {
     // Value type is allocated
-    return this;
+    return kit->gvn().transform(new ValueTypePtrNode(this))->as_ValueTypePtr();
   }
   assert(!is_allocated(&kit->gvn()), "should not be allocated");
   RegionNode* region = new RegionNode(3);
 
   // Oop is non-NULL, use it
@@ -428,11 +432,11 @@
   }
   // ValueTypeNode::remove_redundant_allocations piggybacks on split if.
   // Make sure it gets a chance to remove this allocation.
   kit->C->set_has_split_ifs(true);
   assert(vt->is_allocated(&kit->gvn()), "must be allocated");
-  return vt;
+  return kit->gvn().transform(new ValueTypePtrNode(vt))->as_ValueTypePtr();
 }
 
 bool ValueTypeBaseNode::is_allocated(PhaseGVN* phase) const {
   Node* oop = get_oop();
   const Type* oop_type = (phase != NULL) ? phase->type(oop) : oop->bottom_type();
@@ -479,11 +483,11 @@
      if (field_is_flattened(i)) {
        // Flattened value type field
        vt->set_field_value(i, value->allocate_fields(kit));
      } else if (value != NULL) {
        // Non-flattened value type field
-       vt->set_field_value(i, value->allocate(kit));
+       vt->set_field_value(i, value->buffer(kit));
      }
   }
   vt = kit->gvn().transform(vt)->as_ValueTypeBase();
   kit->replace_in_map(this, vt);
   return vt;
@@ -716,11 +720,11 @@
     } else {
       if (arg->is_ValueType()) {
         // Non-flattened value type field
         ValueTypeNode* vt = arg->as_ValueType();
         assert(n->Opcode() != Op_Return || vt->is_allocated(&kit->gvn()), "value type field should be allocated on return");
-        arg = vt->allocate(kit)->get_oop();
+        arg = vt->buffer(kit);
       }
       // Initialize call/return arguments
       BasicType bt = field_type(i)->basic_type();
       n->init_req(base_input++, arg);
       if (type2size[bt] == 2) {
@@ -890,20 +894,13 @@
         }
       }
       if (res_dom != res) {
         // Move users to dominating allocation
         igvn->replace_node(res, res_dom);
-        // The result of the dominated allocation is now unused and will be
-        // removed later in AllocateNode::Ideal() to not confuse loop opts.
+        // The result of the dominated allocation is now unused and will be removed
+        // later in PhaseMacroExpand::eliminate_allocate_node to not confuse loop opts.
         igvn->record_for_igvn(alloc);
-#ifdef ASSERT
-        if (PrintEliminateAllocations) {
-          tty->print("++++ Eliminated: %d Allocate ", alloc->_idx);
-          dump_spec(tty);
-          tty->cr();
-        }
-#endif
       }
     }
   }
 
   // Process users
@@ -926,23 +923,5 @@
 #endif
     }
   }
   igvn->remove_dead_node(this);
 }
-
-ValueTypePtrNode* ValueTypePtrNode::make_from_value_type(GraphKit* kit, ValueTypeNode* vt) {
-  Node* oop = vt->allocate(kit)->get_oop();
-  ValueTypePtrNode* vtptr = new ValueTypePtrNode(vt->value_klass(), oop);
-  for (uint i = Oop+1; i < vt->req(); i++) {
-    vtptr->init_req(i, vt->in(i));
-  }
-  return kit->gvn().transform(vtptr)->as_ValueTypePtr();
-}
-
-ValueTypePtrNode* ValueTypePtrNode::make_from_oop(GraphKit* kit, Node* oop) {
-  // Create and initialize a ValueTypePtrNode by loading all field
-  // values from a heap-allocated version and also save the oop.
-  ciValueKlass* vk = kit->gvn().type(oop)->value_klass();
-  ValueTypePtrNode* vtptr = new ValueTypePtrNode(vk, oop);
-  vtptr->load(kit, oop, oop, vk);
-  return kit->gvn().transform(vtptr)->as_ValueTypePtr();
-}
diff a/src/hotspot/share/opto/valuetypenode.hpp b/src/hotspot/share/opto/valuetypenode.hpp
--- a/src/hotspot/share/opto/valuetypenode.hpp
+++ b/src/hotspot/share/opto/valuetypenode.hpp
@@ -84,11 +84,11 @@
   void store(GraphKit* kit, Node* base, Node* ptr, ciInstanceKlass* holder, int holder_offset = 0, DecoratorSet decorators = IN_HEAP | MO_UNORDERED) const;
   // Initialize the value type by loading its field values from memory
   void load(GraphKit* kit, Node* base, Node* ptr, ciInstanceKlass* holder, int holder_offset = 0, DecoratorSet decorators = IN_HEAP | MO_UNORDERED);
 
   // Allocates the value type (if not yet allocated)
-  ValueTypeBaseNode* allocate(GraphKit* kit, bool safe_for_replace = true);
+  ValueTypePtrNode* buffer(GraphKit* kit, bool safe_for_replace = true);
   bool is_allocated(PhaseGVN* phase) const;
 
   void replace_call_results(GraphKit* kit, Node* call, Compile* C);
 
   // Allocate all non-flattened value type fields
@@ -150,24 +150,23 @@
 };
 
 //------------------------------ValueTypePtrNode-------------------------------------
 // Node representing a value type as a pointer in C2 IR
 class ValueTypePtrNode : public ValueTypeBaseNode {
+  friend class ValueTypeBaseNode;
 private:
   const TypeInstPtr* value_ptr() const { return type()->isa_instptr(); }
 
-  ValueTypePtrNode(ciValueKlass* vk, Node* oop)
-    : ValueTypeBaseNode(TypeInstPtr::make(TypePtr::NotNull, vk), Values + vk->nof_declared_nonstatic_fields()) {
+  ValueTypePtrNode(ValueTypeBaseNode* vt)
+    : ValueTypeBaseNode(TypeInstPtr::make(TypePtr::NotNull, vt->type()->value_klass()), vt->req()) {
     init_class_id(Class_ValueTypePtr);
-    init_req(Oop, oop);
+    init_req(Oop, vt->get_oop());
+    for (uint i = Oop+1; i < vt->req(); i++) {
+      init_req(i, vt->in(i));
+    }
   }
 
 public:
-  // Create and initialize with the values of a ValueTypeNode
-  static ValueTypePtrNode* make_from_value_type(GraphKit* kit, ValueTypeNode* vt);
-  // Create and initialize by loading the field values from an oop
-  static ValueTypePtrNode* make_from_oop(GraphKit* kit, Node* oop);
-
   virtual int Opcode() const;
 };
 
 #endif // SHARE_VM_OPTO_VALUETYPENODE_HPP
diff a/test/hotspot/jtreg/compiler/valhalla/valuetypes/TestLWorld.java b/test/hotspot/jtreg/compiler/valhalla/valuetypes/TestLWorld.java
--- a/test/hotspot/jtreg/compiler/valhalla/valuetypes/TestLWorld.java
+++ b/test/hotspot/jtreg/compiler/valhalla/valuetypes/TestLWorld.java
@@ -2405,11 +2405,11 @@
             field = 0x42;
         }
     }
 
     @Warmup(10000)
-    @Test(match = { CLASS_CHECK_TRAP }, matchCount = { 2 }, failOn = LOAD_UNKNOWN_VALUE + ALLOC_G)
+    @Test(match = { CLASS_CHECK_TRAP }, matchCount = { 2 }, failOn = LOAD_UNKNOWN_VALUE + ALLOC_G + MEMBAR)
     public Object test92(Object[] array) {
         // Dummy loops to ensure we run enough passes of split if
         for (int i = 0; i < 2; i++) {
             for (int j = 0; j < 2; j++) {
               for (int k = 0; k < 2; k++) {
@@ -2469,11 +2469,11 @@
             }
         }
     }
 
     @Warmup(10000)
-    @Test(match = { CLASS_CHECK_TRAP, LOOP }, matchCount = { 2, 1 }, failOn = LOAD_UNKNOWN_VALUE + ALLOC_G)
+    @Test(match = { CLASS_CHECK_TRAP, LOOP }, matchCount = { 2, 1 }, failOn = LOAD_UNKNOWN_VALUE + ALLOC_G + MEMBAR)
     public int test94(Object[] array) {
         int res = 0;
         for (int i = 1; i < 4; i *= 2) {
             Object v = array[i];
             res += (Integer)v;
@@ -2738,11 +2738,11 @@
     class MyObject4 extends NoValueImplementors1 {
 
     }
 
     // Loading from an abstract class array does not require a flatness check if the abstract class has a non-static field
-    @Test(failOn = ALLOC_G + ALLOCA_G + LOAD_UNKNOWN_VALUE + STORE_UNKNOWN_VALUE + VALUE_ARRAY_NULL_GUARD)
+    @Test(failOn = ALLOC_G + MEMBAR + ALLOCA_G + LOAD_UNKNOWN_VALUE + STORE_UNKNOWN_VALUE + VALUE_ARRAY_NULL_GUARD)
     public NoValueImplementors1 test103(NoValueImplementors1[] array, int i) {
         return array[i];
     }
 
     @DontCompile
@@ -2803,11 +2803,11 @@
     class MyObject5 extends NoValueImplementors2 {
 
     }
 
     // Loading from an abstract class array does not require a flatness check if the abstract class has no value implementor
-    @Test(failOn = ALLOC_G + ALLOCA_G + LOAD_UNKNOWN_VALUE + STORE_UNKNOWN_VALUE + VALUE_ARRAY_NULL_GUARD)
+    @Test(failOn = ALLOC_G + MEMBAR + ALLOCA_G + LOAD_UNKNOWN_VALUE + STORE_UNKNOWN_VALUE + VALUE_ARRAY_NULL_GUARD)
     public NoValueImplementors2 test105(NoValueImplementors2[] array, int i) {
         return array[i];
     }
 
     @DontCompile
@@ -2902,6 +2902,343 @@
                                         for (int i = 0; i < dst1.length; i++) {
                                             Asserts.assertEquals(dst1[i], testValue2);
                                             Asserts.assertEquals(dst2[i], o1);
                                         } });
     }
+
+    // Escape analysis tests
+
+    static interface WrapperInterface {
+        long value();
+
+        final static WrapperInterface ZERO = new LongWrapper(0);
+
+        static WrapperInterface wrap(long val) {
+            return (val == 0L) ? ZERO : new LongWrapper(val);
+        }
+    }
+
+    static inline class LongWrapper implements WrapperInterface {
+        final static LongWrapper ZERO = new LongWrapper(0);
+        private long val;
+
+        LongWrapper(long val) {
+            this.val = val;
+        }
+
+        static LongWrapper wrap(long val) {
+            return (val == 0L) ? ZERO : new LongWrapper(val);
+        }
+
+        public long value() {
+            return val;
+        }
+    }
+
+    static class InterfaceBox {
+        WrapperInterface content;
+
+        InterfaceBox(WrapperInterface content) {
+            this.content = content;
+        }
+
+        static InterfaceBox box_sharp(long val) {
+            return new InterfaceBox(LongWrapper.wrap(val));
+        }
+
+        static InterfaceBox box(long val) {
+            return new InterfaceBox(WrapperInterface.wrap(val));
+        }
+    }
+
+    static class ObjectBox {
+        Object content;
+
+        ObjectBox(Object content) {
+            this.content = content;
+        }
+
+        static ObjectBox box_sharp(long val) {
+            return new ObjectBox(LongWrapper.wrap(val));
+        }
+
+        static ObjectBox box(long val) {
+            return new ObjectBox(WrapperInterface.wrap(val));
+        }
+    }
+
+    static class RefBox {
+        LongWrapper.ref content;
+
+        RefBox(LongWrapper.ref content) {
+            this.content = content;
+        }
+
+        static RefBox box_sharp(long val) {
+            return new RefBox(LongWrapper.wrap(val));
+        }
+
+        static RefBox box(long val) {
+            return new RefBox((LongWrapper.ref)WrapperInterface.wrap(val));
+        }
+    }
+
+    static class InlineBox {
+        LongWrapper content;
+
+        InlineBox(long val) {
+            this.content = LongWrapper.wrap(val);
+        }
+
+        static InlineBox box(long val) {
+            return new InlineBox(val);
+        }
+    }
+
+    static class GenericBox<T> {
+        T content;
+
+        static GenericBox<LongWrapper.ref> box_sharp(long val) {
+            GenericBox<LongWrapper.ref> res = new GenericBox<>();
+            res.content = LongWrapper.wrap(val);
+            return res;
+        }
+
+        static GenericBox<WrapperInterface> box(long val) {
+            GenericBox<WrapperInterface> res = new GenericBox<>();
+            res.content = WrapperInterface.wrap(val);
+            return res;
+        }
+    }
+
+    long[] lArr = {0L, rL, 0L, rL, 0L, rL, 0L, rL, 0L, rL};
+
+    // Test removal of allocations when inline type instance is wrapped into box object
+    @Warmup(10000) // Make sure interface calls are inlined
+    @Test(failOn = ALLOC_G + MEMBAR, match = { PREDICATE_TRAP }, matchCount = { 1 })
+    public long test109() {
+        long res = 0;
+        for (int i = 0 ; i < lArr.length; i++) {
+            res += InterfaceBox.box(lArr[i]).content.value();
+        }
+        return res;
+    }
+
+    @DontCompile
+    public void test109_verifier(boolean warmup) {
+        long res = test109();
+        Asserts.assertEquals(res, 5*rL);
+    }
+
+    @Warmup(10000) // Make sure interface calls are inlined
+    @Test(failOn = ALLOC_G + MEMBAR, match = { PREDICATE_TRAP }, matchCount = { 1 })
+    public long test109_sharp() {
+        long res = 0;
+        for (int i = 0 ; i < lArr.length; i++) {
+            res += InterfaceBox.box_sharp(lArr[i]).content.value();
+        }
+        return res;
+    }
+
+    @DontCompile
+    public void test109_sharp_verifier(boolean warmup) {
+        long res = test109_sharp();
+        Asserts.assertEquals(res, 5*rL);
+    }
+
+    // Same as test109 but with ObjectBox
+    @Test(failOn = ALLOC_G + MEMBAR, match = { PREDICATE_TRAP }, matchCount = { 1 })
+    @Warmup(10000) // Make sure interface calls are inlined
+    public long test110() {
+        long res = 0;
+        for (int i = 0 ; i < lArr.length; i++) {
+            res += ((WrapperInterface)ObjectBox.box(lArr[i]).content).value();
+        }
+        return res;
+    }
+
+    @DontCompile
+    public void test110_verifier(boolean warmup) {
+        long res = test110();
+        Asserts.assertEquals(res, 5*rL);
+    }
+
+    @Test(failOn = ALLOC_G + MEMBAR, match = { PREDICATE_TRAP }, matchCount = { 1 })
+    @Warmup(10000) // Make sure interface calls are inlined
+    public long test110_sharp() {
+        long res = 0;
+        for (int i = 0 ; i < lArr.length; i++) {
+            res += ((WrapperInterface)ObjectBox.box_sharp(lArr[i]).content).value();
+        }
+        return res;
+    }
+
+    @DontCompile
+    public void test110_sharp_verifier(boolean warmup) {
+        long res = test110_sharp();
+        Asserts.assertEquals(res, 5*rL);
+    }
+
+    // Same as test109 but with RefBox
+    @Test(failOn = ALLOC_G + MEMBAR, match = { PREDICATE_TRAP }, matchCount = { 1 })
+    public long test111() {
+        long res = 0;
+        for (int i = 0 ; i < lArr.length; i++) {
+            res += RefBox.box(lArr[i]).content.value();
+        }
+        return res;
+    }
+
+    @DontCompile
+    public void test111_verifier(boolean warmup) {
+        long res = test111();
+        Asserts.assertEquals(res, 5*rL);
+    }
+
+    @Test(failOn = ALLOC_G + MEMBAR, match = { PREDICATE_TRAP }, matchCount = { 1 })
+    public long test111_sharp() {
+        long res = 0;
+        for (int i = 0 ; i < lArr.length; i++) {
+            res += RefBox.box_sharp(lArr[i]).content.value();
+        }
+        return res;
+    }
+
+    @DontCompile
+    public void test111_sharp_verifier(boolean warmup) {
+        long res = test111_sharp();
+        Asserts.assertEquals(res, 5*rL);
+    }
+
+    // Same as test109 but with InlineBox
+    @Test(failOn = ALLOC_G + MEMBAR, match = { PREDICATE_TRAP }, matchCount = { 1 })
+    public long test112() {
+        long res = 0;
+        for (int i = 0 ; i < lArr.length; i++) {
+            res += InlineBox.box(lArr[i]).content.value();
+        }
+        return res;
+    }
+
+    @DontCompile
+    public void test112_verifier(boolean warmup) {
+        long res = test112();
+        Asserts.assertEquals(res, 5*rL);
+    }
+
+    // Same as test109 but with GenericBox
+    @Test(failOn = ALLOC_G + MEMBAR, match = { PREDICATE_TRAP }, matchCount = { 1 })
+    @Warmup(10000) // Make sure interface calls are inlined
+    public long test113() {
+        long res = 0;
+        for (int i = 0 ; i < lArr.length; i++) {
+            res += GenericBox.box(lArr[i]).content.value();
+        }
+        return res;
+    }
+
+    @DontCompile
+    public void test113_verifier(boolean warmup) {
+        long res = test113();
+        Asserts.assertEquals(res, 5*rL);
+    }
+
+    @Test(failOn = ALLOC_G + MEMBAR, match = { PREDICATE_TRAP }, matchCount = { 1 })
+    @Warmup(10000) // Make sure interface calls are inlined
+    public long test113_sharp() {
+        long res = 0;
+        for (int i = 0 ; i < lArr.length; i++) {
+            res += GenericBox.box_sharp(lArr[i]).content.value();
+        }
+        return res;
+    }
+
+    @DontCompile
+    public void test113_sharp_verifier(boolean warmup) {
+        long res = test113_sharp();
+        Asserts.assertEquals(res, 5*rL);
+    }
+
+    static interface WrapperInterface2 {
+        public long value();
+
+        static final InlineWrapper.ref ZERO = new InlineWrapper(0);
+
+        public static WrapperInterface2 wrap(long val) {
+            return (val == 0) ? ZERO.content : new LongWrapper2(val);
+        }
+
+        public static WrapperInterface2 wrap_default(long val) {
+            return (val == 0) ? LongWrapper2.default : new LongWrapper2(val);
+        }
+    }
+
+    static inline class LongWrapper2 implements WrapperInterface2 {
+        private long val;
+
+        public LongWrapper2(long val) {
+            this.val = val;
+        }
+
+        public long value() {
+            return val;
+        }
+    }
+
+    static inline class InlineWrapper {
+        WrapperInterface2 content;
+
+        public InlineWrapper(long val) {
+            content = new LongWrapper2(val);
+        }
+    }
+
+    static class InterfaceBox2 {
+        WrapperInterface2 content;
+
+        public InterfaceBox2(long val, boolean def) {
+            this.content = def ? WrapperInterface2.wrap_default(val) : WrapperInterface2.wrap(val);
+        }
+
+        static InterfaceBox2 box(long val) {
+            return new InterfaceBox2(val, false);
+        }
+
+        static InterfaceBox2 box_default(long val) {
+            return new InterfaceBox2(val, true);
+        }
+    }
+
+    // Same as tests above but with ZERO hidden in field of another inline type
+    @Test(failOn = ALLOC_G + MEMBAR, match = { PREDICATE_TRAP }, matchCount = { 1 })
+    @Warmup(10000)
+    public long test114() {
+        long res = 0;
+        for (int i = 0; i < lArr.length; i++) {
+            res += InterfaceBox2.box(lArr[i]).content.value();
+        }
+        return res;
+    }
+
+    @DontCompile
+    public void test114_verifier(boolean warmup) {
+        long res = test114();
+        Asserts.assertEquals(res, 5*rL);
+    }
+
+    // Same as test114 but with .default instead of ZERO field
+    @Test(failOn = ALLOC_G + MEMBAR, match = { PREDICATE_TRAP }, matchCount = { 1 })
+    @Warmup(10000)
+    public long test115() {
+        long res = 0;
+        for (int i = 0; i < lArr.length; i++) {
+            res += InterfaceBox2.box_default(lArr[i]).content.value();
+        }
+        return res;
+    }
+
+    @DontCompile
+    public void test115_verifier(boolean warmup) {
+        long res = test115();
+        Asserts.assertEquals(res, 5*rL);
+    }
 }
diff a/test/hotspot/jtreg/compiler/valhalla/valuetypes/ValueTypeTest.java b/test/hotspot/jtreg/compiler/valhalla/valuetypes/ValueTypeTest.java
--- a/test/hotspot/jtreg/compiler/valhalla/valuetypes/ValueTypeTest.java
+++ b/test/hotspot/jtreg/compiler/valhalla/valuetypes/ValueTypeTest.java
@@ -221,10 +221,12 @@
     protected static final String VALUE_ARRAY_NULL_GUARD = "(.*call,static  wrapper for: uncommon_trap.*reason='null_check' action='none'.*" + END;
     protected static final String CLASS_CHECK_TRAP = START + "CallStaticJava" + MID + "uncommon_trap.*class_check" + END;
     protected static final String NULL_CHECK_TRAP = START + "CallStaticJava" + MID + "uncommon_trap.*null_check" + END;
     protected static final String RANGE_CHECK_TRAP = START + "CallStaticJava" + MID + "uncommon_trap.*range_check" + END;
     protected static final String UNHANDLED_TRAP = START + "CallStaticJava" + MID + "uncommon_trap.*unhandled" + END;
+    protected static final String PREDICATE_TRAP = START + "CallStaticJava" + MID + "uncommon_trap.*predicate" + END;
+    protected static final String MEMBAR = START + "MemBar" + MID + END;
 
     public static String[] concat(String prefix[], String... extra) {
         ArrayList<String> list = new ArrayList<String>();
         if (prefix != null) {
             for (String s : prefix) {
diff a/test/micro/org/openjdk/bench/valhalla/lworld/escapeanalysis/TestBoxing.java b/test/micro/org/openjdk/bench/valhalla/lworld/escapeanalysis/TestBoxing.java
--- /dev/null
+++ b/test/micro/org/openjdk/bench/valhalla/lworld/escapeanalysis/TestBoxing.java
@@ -0,0 +1,263 @@
+/*
+ * Copyright (c) 2020, Oracle and/or its affiliates. All rights reserved.
+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
+ *
+ * This code is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 only, as
+ * published by the Free Software Foundation.
+ *
+ * This code is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
+ * version 2 for more details (a copy is included in the LICENSE file that
+ * accompanied this code).
+ *
+ * You should have received a copy of the GNU General Public License version
+ * 2 along with this work; if not, write to the Free Software Foundation,
+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
+ *
+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
+ * or visit www.oracle.com if you need additional information or have any
+ * questions.
+ */
+
+package org.openjdk.bench.valhalla.lworld.escapeanalysis;
+
+import org.openjdk.jmh.annotations.Benchmark;
+import org.openjdk.jmh.annotations.BenchmarkMode;
+import org.openjdk.jmh.annotations.Fork;
+import org.openjdk.jmh.annotations.Measurement;
+import org.openjdk.jmh.annotations.Mode;
+import org.openjdk.jmh.annotations.OutputTimeUnit;
+import org.openjdk.jmh.annotations.Setup;
+import org.openjdk.jmh.annotations.State;
+import org.openjdk.jmh.annotations.Warmup;
+
+import java.util.concurrent.TimeUnit;
+import java.util.stream.LongStream;
+
+@BenchmarkMode(Mode.AverageTime)
+@Warmup(iterations = 5, time = 500, timeUnit = TimeUnit.MILLISECONDS)
+@Measurement(iterations = 10, time = 500, timeUnit = TimeUnit.MILLISECONDS)
+@State(org.openjdk.jmh.annotations.Scope.Thread)
+@OutputTimeUnit(TimeUnit.MILLISECONDS)
+@Fork(3)
+public class TestBoxing {
+    static final int ELEM_SIZE = 1_000_000;
+    long[] arr;
+
+    @Setup
+    public void setup() {
+        arr = LongStream.range(0, ELEM_SIZE).toArray();
+    }
+
+    @Benchmark
+    public int pojo_loop() {
+        int sum = 0;
+        for (int i = 0; i < ELEM_SIZE; i++) {
+            sum += PojoWrapper.from(arr[i]).value();
+        }
+        return sum;
+    }
+
+    @Benchmark
+    public int inline_loop() {
+        int sum = 0;
+        for (int i = 0; i < ELEM_SIZE; i++) {
+            sum += LongWrapper.from(arr[i]).value();
+        }
+        return sum;
+    }
+
+    @Benchmark
+    public int box_inline_loop() {
+        int sum = 0;
+        for (int i = 0; i < ELEM_SIZE; i++) {
+            sum += BoxInline.from(arr[i]).box().value();
+        }
+        return sum;
+    }
+
+    @Benchmark
+    public int box_intf_loop() {
+        int sum = 0;
+        for (int i = 0; i < ELEM_SIZE; i++) {
+            sum += BoxInterface.from(arr[i]).box().value();
+        }
+        return sum;
+    }
+
+    @Benchmark
+    public int box_intf_loop_sharp() {
+        int sum = 0;
+        for (int i = 0; i < ELEM_SIZE; i++) {
+            sum += BoxInterface.from_sharp(arr[i]).box().value();
+        }
+        return sum;
+    }
+
+    @Benchmark
+    public int box_ref_loop() {
+        int sum = 0;
+        for (int i = 0; i < ELEM_SIZE; i++) {
+            sum += BoxRef.from(arr[i]).box().value();
+        }
+        return sum;
+    }
+
+    @Benchmark
+    public int box_ref_loop_sharp() {
+        int sum = 0;
+        for (int i = 0; i < ELEM_SIZE; i++) {
+            sum += BoxRef.from_sharp(arr[i]).box().value();
+        }
+        return sum;
+    }
+
+    @Benchmark
+    public int box_generic_loop() {
+        int sum = 0;
+        for (int i = 0; i < ELEM_SIZE; i++) {
+            sum += BoxGeneric.from(arr[i]).box().value();
+        }
+        return sum;
+    }
+
+    @Benchmark
+    public int box_generic_loop_sharp() {
+        int sum = 0;
+        for (int i = 0; i < ELEM_SIZE; i++) {
+            sum += BoxGeneric.from_sharp(arr[i]).box().value();
+        }
+        return sum;
+    }
+
+    interface ValueBox {
+        long value();
+
+        final static LongWrapper ZERO = new LongWrapper(0);
+
+        static ValueBox from(long i) {
+            return (i == 0L) ? ZERO : new LongWrapper(i);
+        }
+    }
+
+    static class PojoWrapper {
+        final long i;
+
+        PojoWrapper(long i) {
+            this.i = i;
+        }
+
+        public long value() {
+            return i;
+        }
+
+        final static PojoWrapper ZERO = new PojoWrapper(0);
+
+        static PojoWrapper from(long i) {
+            return (i == 0L) ? ZERO : new PojoWrapper(i);
+        }
+    }
+
+    static inline class LongWrapper implements ValueBox {
+        final long i;
+
+        LongWrapper(long i) {
+            this.i = i;
+        }
+
+        public long value() {
+            return i;
+        }
+
+        final static LongWrapper ZERO = new LongWrapper(0);
+
+        static LongWrapper from(long i) {
+            return (i == 0L) ? ZERO : new LongWrapper(i);
+        }
+    }
+
+    static class BoxInterface {
+        final ValueBox inlineBox;
+
+        public BoxInterface(ValueBox inlineBox) {
+            this.inlineBox = inlineBox;
+        }
+
+        ValueBox box() {
+            return inlineBox;
+        }
+
+        static BoxInterface from_sharp(long i) {
+            LongWrapper box = LongWrapper.from(i);
+            return new BoxInterface(box);
+        }
+
+        static BoxInterface from(long i) {
+            ValueBox box = ValueBox.from(i);
+            return new BoxInterface(box);
+        }
+    }
+
+    static class BoxInline {
+        final LongWrapper inlineBox;
+
+        public BoxInline(LongWrapper inlineBox) {
+            this.inlineBox = inlineBox;
+        }
+
+        ValueBox box() {
+            return inlineBox;
+        }
+
+        static BoxInline from(long i) {
+            LongWrapper box = LongWrapper.from(i);
+            return new BoxInline(box);
+        }
+    }
+
+    static class BoxRef {
+        final LongWrapper.ref inlineBox;
+
+        public BoxRef(LongWrapper.ref inlineBox) {
+            this.inlineBox = inlineBox;
+        }
+
+        ValueBox box() {
+            return inlineBox;
+        }
+
+        static BoxRef from_sharp(long i) {
+            LongWrapper box = LongWrapper.from(i);
+            return new BoxRef(box);
+        }
+
+        static BoxRef from(long i) {
+            LongWrapper.ref box = LongWrapper.from(i);
+            return new BoxRef(box);
+        }
+    }
+
+    static class BoxGeneric<T> {
+        final T inlineBox;
+
+        public BoxGeneric(T inlineBox) {
+            this.inlineBox = inlineBox;
+        }
+
+        T box() {
+            return inlineBox;
+        }
+
+        static BoxGeneric<LongWrapper.ref> from_sharp(long i) {
+            LongWrapper box = LongWrapper.from(i);
+            return new BoxGeneric<LongWrapper.ref>(box);
+        }
+
+        static BoxGeneric<ValueBox> from(long i) {
+            ValueBox box = ValueBox.from(i);
+            return new BoxGeneric<ValueBox>(box);
+        }
+    }
+}
