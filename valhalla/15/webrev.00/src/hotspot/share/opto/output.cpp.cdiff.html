<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Cdiff src/hotspot/share/opto/output.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="node.hpp.cdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="output.hpp.cdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/share/opto/output.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<hr />
<pre>
<span class="line-old-header">*** 33,21 ***</span>
<span class="line-new-header">--- 33,25 ---</span>
  #include &quot;compiler/oopMap.hpp&quot;
  #include &quot;gc/shared/barrierSet.hpp&quot;
  #include &quot;gc/shared/c2/barrierSetC2.hpp&quot;
  #include &quot;memory/allocation.inline.hpp&quot;
  #include &quot;opto/ad.hpp&quot;
<span class="line-added">+ #include &quot;opto/block.hpp&quot;</span>
<span class="line-added">+ #include &quot;opto/c2compiler.hpp&quot;</span>
  #include &quot;opto/callnode.hpp&quot;
  #include &quot;opto/cfgnode.hpp&quot;
  #include &quot;opto/locknode.hpp&quot;
  #include &quot;opto/machnode.hpp&quot;
<span class="line-added">+ #include &quot;opto/node.hpp&quot;</span>
  #include &quot;opto/optoreg.hpp&quot;
  #include &quot;opto/output.hpp&quot;
  #include &quot;opto/regalloc.hpp&quot;
  #include &quot;opto/runtime.hpp&quot;
  #include &quot;opto/subnode.hpp&quot;
  #include &quot;opto/type.hpp&quot;
  #include &quot;runtime/handles.inline.hpp&quot;
<span class="line-added">+ #include &quot;runtime/sharedRuntime.hpp&quot;</span>
  #include &quot;utilities/macros.hpp&quot;
  #include &quot;utilities/powerOfTwo.hpp&quot;
  #include &quot;utilities/xmlstream.hpp&quot;
  #ifdef X86
  #include &quot;c2_intelJccErratum_x86.hpp&quot;
</pre>
<hr />
<pre>
<span class="line-old-header">*** 57,172 ***</span>
  #define DEBUG_ARG(x) , x
  #else
  #define DEBUG_ARG(x)
  #endif
  
  // Convert Nodes to instruction bits and pass off to the VM
<span class="line-modified">! void Compile::Output() {</span>
    // RootNode goes
<span class="line-modified">!   assert( _cfg-&gt;get_root_block()-&gt;number_of_nodes() == 0, &quot;&quot; );</span>
  
    // The number of new nodes (mostly MachNop) is proportional to
    // the number of java calls and inner loops which are aligned.
    if ( C-&gt;check_node_count((NodeLimitFudgeFactor + C-&gt;java_calls()*3 +
                              C-&gt;inner_loops()*(OptoLoopAlignment-1)),
                             &quot;out of nodes before code generation&quot; ) ) {
      return;
    }
    // Make sure I can find the Start Node
<span class="line-modified">!   Block *entry = _cfg-&gt;get_block(1);</span>
<span class="line-modified">!   Block *broot = _cfg-&gt;get_root_block();</span>
  
    const StartNode *start = entry-&gt;head()-&gt;as_Start();
  
    // Replace StartNode with prolog
    Label verified_entry;
    MachPrologNode* prolog = new MachPrologNode(&amp;verified_entry);
    entry-&gt;map_node(prolog, 0);
<span class="line-modified">!   _cfg-&gt;map_node_to_block(prolog, entry);</span>
<span class="line-modified">!   _cfg-&gt;unmap_node_from_block(start); // start is no longer in any block</span>
  
    // Virtual methods need an unverified entry point
<span class="line-modified">!   if (is_osr_compilation()) {</span>
      if (PoisonOSREntry) {
        // TODO: Should use a ShouldNotReachHereNode...
<span class="line-modified">!       _cfg-&gt;insert( broot, 0, new MachBreakpointNode() );</span>
      }
    } else {
<span class="line-modified">!     if (_method) {</span>
<span class="line-modified">!       if (_method-&gt;has_scalarized_args()) {</span>
          // Add entry point to unpack all value type arguments
<span class="line-modified">!         _cfg-&gt;insert(broot, 0, new MachVEPNode(&amp;verified_entry, /* verified */ true, /* receiver_only */ false));</span>
<span class="line-modified">!         if (!_method-&gt;is_static()) {</span>
            // Add verified/unverified entry points to only unpack value type receiver at interface calls
<span class="line-modified">!           _cfg-&gt;insert(broot, 0, new MachVEPNode(&amp;verified_entry, /* verified */ false, /* receiver_only */ false));</span>
<span class="line-modified">!           _cfg-&gt;insert(broot, 0, new MachVEPNode(&amp;verified_entry, /* verified */ true,  /* receiver_only */ true));</span>
<span class="line-modified">!           _cfg-&gt;insert(broot, 0, new MachVEPNode(&amp;verified_entry, /* verified */ false, /* receiver_only */ true));</span>
          }
<span class="line-modified">!       } else if (!_method-&gt;is_static()) {</span>
          // Insert unvalidated entry point
<span class="line-modified">!         _cfg-&gt;insert(broot, 0, new MachUEPNode());</span>
        }
      }
    }
  
    // Break before main entry point
<span class="line-modified">!   if ((_method &amp;&amp; C-&gt;directive()-&gt;BreakAtExecuteOption) ||</span>
<span class="line-modified">!       (OptoBreakpoint &amp;&amp; is_method_compilation())       ||</span>
<span class="line-modified">!       (OptoBreakpointOSR &amp;&amp; is_osr_compilation())       ||</span>
<span class="line-modified">!       (OptoBreakpointC2R &amp;&amp; !_method)                   ) {</span>
<span class="line-modified">!     // checking for _method means that OptoBreakpoint does not apply to</span>
      // runtime stubs or frame converters
<span class="line-modified">!     _cfg-&gt;insert( entry, 1, new MachBreakpointNode() );</span>
    }
  
    // Insert epilogs before every return
<span class="line-modified">!   for (uint i = 0; i &lt; _cfg-&gt;number_of_blocks(); i++) {</span>
<span class="line-modified">!     Block* block = _cfg-&gt;get_block(i);</span>
<span class="line-modified">!     if (!block-&gt;is_connector() &amp;&amp; block-&gt;non_connector_successor(0) == _cfg-&gt;get_root_block()) { // Found a program exit point?</span>
        Node* m = block-&gt;end();
        if (m-&gt;is_Mach() &amp;&amp; m-&gt;as_Mach()-&gt;ideal_Opcode() != Op_Halt) {
          MachEpilogNode* epilog = new MachEpilogNode(m-&gt;as_Mach()-&gt;ideal_Opcode() == Op_Return);
          block-&gt;add_inst(epilog);
<span class="line-modified">!         _cfg-&gt;map_node_to_block(epilog, block);</span>
        }
      }
    }
  
    // Keeper of sizing aspects
    BufferSizingData buf_sizes = BufferSizingData();
  
    // Initialize code buffer
    estimate_buffer_size(buf_sizes._const);
<span class="line-modified">!   if (failing()) return;</span>
  
    // Pre-compute the length of blocks and replace
    // long branches with short if machine supports it.
    // Must be done before ScheduleAndBundle due to SPARC delay slots
<span class="line-modified">!   uint* blk_starts = NEW_RESOURCE_ARRAY(uint, _cfg-&gt;number_of_blocks() + 1);</span>
    blk_starts[0] = 0;
    shorten_branches(blk_starts, buf_sizes);
  
<span class="line-modified">!   if (!is_osr_compilation() &amp;&amp; _method &amp;&amp; _method-&gt;has_scalarized_args()) {</span>
      // Compute the offsets of the entry points required by the value type calling convention
<span class="line-modified">!     if (!_method-&gt;is_static()) {</span>
        // We have entries at the beginning of the method, implemented by the first 4 nodes.
        // Entry                     (unverified) @ offset 0
        // Verified_Value_Entry_RO
        // Value_Entry               (unverified)
        // Verified_Value_Entry
        uint offset = 0;
        _code_offsets.set_value(CodeOffsets::Entry, offset);
  
<span class="line-modified">!       offset += ((MachVEPNode*)broot-&gt;get_node(0))-&gt;size(_regalloc);</span>
        _code_offsets.set_value(CodeOffsets::Verified_Value_Entry_RO, offset);
  
<span class="line-modified">!       offset += ((MachVEPNode*)broot-&gt;get_node(1))-&gt;size(_regalloc);</span>
        _code_offsets.set_value(CodeOffsets::Value_Entry, offset);
  
<span class="line-modified">!       offset += ((MachVEPNode*)broot-&gt;get_node(2))-&gt;size(_regalloc);</span>
        _code_offsets.set_value(CodeOffsets::Verified_Value_Entry, offset);
      } else {
        _code_offsets.set_value(CodeOffsets::Entry, -1); // will be patched later
        _code_offsets.set_value(CodeOffsets::Verified_Value_Entry, 0);
      }
    }
  
    ScheduleAndBundle();
<span class="line-modified">!   if (failing()) {</span>
      return;
    }
  
    // Late barrier analysis must be done after schedule and bundle
    // Otherwise liveness based spilling will fail
    BarrierSetC2* bs = BarrierSet::barrier_set()-&gt;barrier_set_c2();
    bs-&gt;late_barrier_analysis();
  
  #ifdef X86
    if (VM_Version::has_intel_jcc_erratum()) {
<span class="line-modified">!     int extra_padding = IntelJccErratum::tag_affected_machnodes(this, _cfg, _regalloc);</span>
      buf_sizes._code += extra_padding;
    }
  #endif
  
    // Complete sizing of codebuffer
    CodeBuffer* cb = init_buffer(buf_sizes);
<span class="line-modified">!   if (cb == NULL || failing()) {</span>
      return;
    }
  
    BuildOopMaps();
  
<span class="line-modified">!   if (failing())  {</span>
      return;
    }
  
    fill_buffer(cb, blk_starts);
  }
  
<span class="line-modified">! bool Compile::need_stack_bang(int frame_size_in_bytes) const {</span>
    // Determine if we need to generate a stack overflow check.
    // Do it if the method is not a stub function and
    // has java calls or has frame size &gt; vm_page_size/8.
    // The debug VM checks that deoptimization doesn&#39;t trigger an
    // unexpected stack overflow (compiled method stack banging should
    // guarantee it doesn&#39;t happen) so we always need the stack bang in
    // a debug VM.
<span class="line-modified">!   return (UseStackBanging &amp;&amp; stub_function() == NULL &amp;&amp;</span>
<span class="line-modified">!           (has_java_calls() || frame_size_in_bytes &gt; os::vm_page_size()&gt;&gt;3</span>
             DEBUG_ONLY(|| true)));
  }
  
<span class="line-modified">! bool Compile::need_register_stack_bang() const {</span>
    // Determine if we need to generate a register stack overflow check.
    // This is only used on architectures which have split register
    // and memory stacks (ie. IA64).
    // Bang if the method is not a stub function and has java calls
<span class="line-modified">!   return (stub_function() == NULL &amp;&amp; has_java_calls());</span>
  }
  
  
  // Compute the size of first NumberOfLoopInstrToAlign instructions at the top
  // of a loop. When aligning a loop we need to provide enough instructions
<span class="line-new-header">--- 61,373 ---</span>
  #define DEBUG_ARG(x) , x
  #else
  #define DEBUG_ARG(x)
  #endif
  
<span class="line-added">+ //------------------------------Scheduling----------------------------------</span>
<span class="line-added">+ // This class contains all the information necessary to implement instruction</span>
<span class="line-added">+ // scheduling and bundling.</span>
<span class="line-added">+ class Scheduling {</span>
<span class="line-added">+ </span>
<span class="line-added">+ private:</span>
<span class="line-added">+   // Arena to use</span>
<span class="line-added">+   Arena *_arena;</span>
<span class="line-added">+ </span>
<span class="line-added">+   // Control-Flow Graph info</span>
<span class="line-added">+   PhaseCFG *_cfg;</span>
<span class="line-added">+ </span>
<span class="line-added">+   // Register Allocation info</span>
<span class="line-added">+   PhaseRegAlloc *_regalloc;</span>
<span class="line-added">+ </span>
<span class="line-added">+   // Number of nodes in the method</span>
<span class="line-added">+   uint _node_bundling_limit;</span>
<span class="line-added">+ </span>
<span class="line-added">+   // List of scheduled nodes. Generated in reverse order</span>
<span class="line-added">+   Node_List _scheduled;</span>
<span class="line-added">+ </span>
<span class="line-added">+   // List of nodes currently available for choosing for scheduling</span>
<span class="line-added">+   Node_List _available;</span>
<span class="line-added">+ </span>
<span class="line-added">+   // For each instruction beginning a bundle, the number of following</span>
<span class="line-added">+   // nodes to be bundled with it.</span>
<span class="line-added">+   Bundle *_node_bundling_base;</span>
<span class="line-added">+ </span>
<span class="line-added">+   // Mapping from register to Node</span>
<span class="line-added">+   Node_List _reg_node;</span>
<span class="line-added">+ </span>
<span class="line-added">+   // Free list for pinch nodes.</span>
<span class="line-added">+   Node_List _pinch_free_list;</span>
<span class="line-added">+ </span>
<span class="line-added">+   // Latency from the beginning of the containing basic block (base 1)</span>
<span class="line-added">+   // for each node.</span>
<span class="line-added">+   unsigned short *_node_latency;</span>
<span class="line-added">+ </span>
<span class="line-added">+   // Number of uses of this node within the containing basic block.</span>
<span class="line-added">+   short *_uses;</span>
<span class="line-added">+ </span>
<span class="line-added">+   // Schedulable portion of current block.  Skips Region/Phi/CreateEx up</span>
<span class="line-added">+   // front, branch+proj at end.  Also skips Catch/CProj (same as</span>
<span class="line-added">+   // branch-at-end), plus just-prior exception-throwing call.</span>
<span class="line-added">+   uint _bb_start, _bb_end;</span>
<span class="line-added">+ </span>
<span class="line-added">+   // Latency from the end of the basic block as scheduled</span>
<span class="line-added">+   unsigned short *_current_latency;</span>
<span class="line-added">+ </span>
<span class="line-added">+   // Remember the next node</span>
<span class="line-added">+   Node *_next_node;</span>
<span class="line-added">+ </span>
<span class="line-added">+   // Use this for an unconditional branch delay slot</span>
<span class="line-added">+   Node *_unconditional_delay_slot;</span>
<span class="line-added">+ </span>
<span class="line-added">+   // Pointer to a Nop</span>
<span class="line-added">+   MachNopNode *_nop;</span>
<span class="line-added">+ </span>
<span class="line-added">+   // Length of the current bundle, in instructions</span>
<span class="line-added">+   uint _bundle_instr_count;</span>
<span class="line-added">+ </span>
<span class="line-added">+   // Current Cycle number, for computing latencies and bundling</span>
<span class="line-added">+   uint _bundle_cycle_number;</span>
<span class="line-added">+ </span>
<span class="line-added">+   // Bundle information</span>
<span class="line-added">+   Pipeline_Use_Element _bundle_use_elements[resource_count];</span>
<span class="line-added">+   Pipeline_Use         _bundle_use;</span>
<span class="line-added">+ </span>
<span class="line-added">+   // Dump the available list</span>
<span class="line-added">+   void dump_available() const;</span>
<span class="line-added">+ </span>
<span class="line-added">+ public:</span>
<span class="line-added">+   Scheduling(Arena *arena, Compile &amp;compile);</span>
<span class="line-added">+ </span>
<span class="line-added">+   // Destructor</span>
<span class="line-added">+   NOT_PRODUCT( ~Scheduling(); )</span>
<span class="line-added">+ </span>
<span class="line-added">+   // Step ahead &quot;i&quot; cycles</span>
<span class="line-added">+   void step(uint i);</span>
<span class="line-added">+ </span>
<span class="line-added">+   // Step ahead 1 cycle, and clear the bundle state (for example,</span>
<span class="line-added">+   // at a branch target)</span>
<span class="line-added">+   void step_and_clear();</span>
<span class="line-added">+ </span>
<span class="line-added">+   Bundle* node_bundling(const Node *n) {</span>
<span class="line-added">+     assert(valid_bundle_info(n), &quot;oob&quot;);</span>
<span class="line-added">+     return (&amp;_node_bundling_base[n-&gt;_idx]);</span>
<span class="line-added">+   }</span>
<span class="line-added">+ </span>
<span class="line-added">+   bool valid_bundle_info(const Node *n) const {</span>
<span class="line-added">+     return (_node_bundling_limit &gt; n-&gt;_idx);</span>
<span class="line-added">+   }</span>
<span class="line-added">+ </span>
<span class="line-added">+   bool starts_bundle(const Node *n) const {</span>
<span class="line-added">+     return (_node_bundling_limit &gt; n-&gt;_idx &amp;&amp; _node_bundling_base[n-&gt;_idx].starts_bundle());</span>
<span class="line-added">+   }</span>
<span class="line-added">+ </span>
<span class="line-added">+   // Do the scheduling</span>
<span class="line-added">+   void DoScheduling();</span>
<span class="line-added">+ </span>
<span class="line-added">+   // Compute the local latencies walking forward over the list of</span>
<span class="line-added">+   // nodes for a basic block</span>
<span class="line-added">+   void ComputeLocalLatenciesForward(const Block *bb);</span>
<span class="line-added">+ </span>
<span class="line-added">+   // Compute the register antidependencies within a basic block</span>
<span class="line-added">+   void ComputeRegisterAntidependencies(Block *bb);</span>
<span class="line-added">+   void verify_do_def( Node *n, OptoReg::Name def, const char *msg );</span>
<span class="line-added">+   void verify_good_schedule( Block *b, const char *msg );</span>
<span class="line-added">+   void anti_do_def( Block *b, Node *def, OptoReg::Name def_reg, int is_def );</span>
<span class="line-added">+   void anti_do_use( Block *b, Node *use, OptoReg::Name use_reg );</span>
<span class="line-added">+ </span>
<span class="line-added">+   // Add a node to the current bundle</span>
<span class="line-added">+   void AddNodeToBundle(Node *n, const Block *bb);</span>
<span class="line-added">+ </span>
<span class="line-added">+   // Add a node to the list of available nodes</span>
<span class="line-added">+   void AddNodeToAvailableList(Node *n);</span>
<span class="line-added">+ </span>
<span class="line-added">+   // Compute the local use count for the nodes in a block, and compute</span>
<span class="line-added">+   // the list of instructions with no uses in the block as available</span>
<span class="line-added">+   void ComputeUseCount(const Block *bb);</span>
<span class="line-added">+ </span>
<span class="line-added">+   // Choose an instruction from the available list to add to the bundle</span>
<span class="line-added">+   Node * ChooseNodeToBundle();</span>
<span class="line-added">+ </span>
<span class="line-added">+   // See if this Node fits into the currently accumulating bundle</span>
<span class="line-added">+   bool NodeFitsInBundle(Node *n);</span>
<span class="line-added">+ </span>
<span class="line-added">+   // Decrement the use count for a node</span>
<span class="line-added">+  void DecrementUseCounts(Node *n, const Block *bb);</span>
<span class="line-added">+ </span>
<span class="line-added">+   // Garbage collect pinch nodes for reuse by other blocks.</span>
<span class="line-added">+   void garbage_collect_pinch_nodes();</span>
<span class="line-added">+   // Clean up a pinch node for reuse (helper for above).</span>
<span class="line-added">+   void cleanup_pinch( Node *pinch );</span>
<span class="line-added">+ </span>
<span class="line-added">+   // Information for statistics gathering</span>
<span class="line-added">+ #ifndef PRODUCT</span>
<span class="line-added">+ private:</span>
<span class="line-added">+   // Gather information on size of nops relative to total</span>
<span class="line-added">+   uint _branches, _unconditional_delays;</span>
<span class="line-added">+ </span>
<span class="line-added">+   static uint _total_nop_size, _total_method_size;</span>
<span class="line-added">+   static uint _total_branches, _total_unconditional_delays;</span>
<span class="line-added">+   static uint _total_instructions_per_bundle[Pipeline::_max_instrs_per_cycle+1];</span>
<span class="line-added">+ </span>
<span class="line-added">+ public:</span>
<span class="line-added">+   static void print_statistics();</span>
<span class="line-added">+ </span>
<span class="line-added">+   static void increment_instructions_per_bundle(uint i) {</span>
<span class="line-added">+     _total_instructions_per_bundle[i]++;</span>
<span class="line-added">+   }</span>
<span class="line-added">+ </span>
<span class="line-added">+   static void increment_nop_size(uint s) {</span>
<span class="line-added">+     _total_nop_size += s;</span>
<span class="line-added">+   }</span>
<span class="line-added">+ </span>
<span class="line-added">+   static void increment_method_size(uint s) {</span>
<span class="line-added">+     _total_method_size += s;</span>
<span class="line-added">+   }</span>
<span class="line-added">+ #endif</span>
<span class="line-added">+ </span>
<span class="line-added">+ };</span>
<span class="line-added">+ </span>
<span class="line-added">+ </span>
<span class="line-added">+ PhaseOutput::PhaseOutput()</span>
<span class="line-added">+   : Phase(Phase::Output),</span>
<span class="line-added">+     _code_buffer(&quot;Compile::Fill_buffer&quot;),</span>
<span class="line-added">+     _first_block_size(0),</span>
<span class="line-added">+     _handler_table(),</span>
<span class="line-added">+     _inc_table(),</span>
<span class="line-added">+     _oop_map_set(NULL),</span>
<span class="line-added">+     _scratch_buffer_blob(NULL),</span>
<span class="line-added">+     _scratch_locs_memory(NULL),</span>
<span class="line-added">+     _scratch_const_size(-1),</span>
<span class="line-added">+     _in_scratch_emit_size(false),</span>
<span class="line-added">+     _frame_slots(0),</span>
<span class="line-added">+     _code_offsets(),</span>
<span class="line-added">+     _node_bundling_limit(0),</span>
<span class="line-added">+     _node_bundling_base(NULL),</span>
<span class="line-added">+     _orig_pc_slot(0),</span>
<span class="line-added">+     _orig_pc_slot_offset_in_bytes(0),</span>
<span class="line-added">+     _sp_inc_slot(0),</span>
<span class="line-added">+     _sp_inc_slot_offset_in_bytes(0) {</span>
<span class="line-added">+   C-&gt;set_output(this);</span>
<span class="line-added">+   if (C-&gt;stub_name() == NULL) {</span>
<span class="line-added">+     int fixed_slots = C-&gt;fixed_slots();</span>
<span class="line-added">+     if (C-&gt;needs_stack_repair()) {</span>
<span class="line-added">+       fixed_slots -= 2;</span>
<span class="line-added">+       _sp_inc_slot = fixed_slots;</span>
<span class="line-added">+     }</span>
<span class="line-added">+     _orig_pc_slot = fixed_slots - (sizeof(address) / VMRegImpl::stack_slot_size);</span>
<span class="line-added">+   }</span>
<span class="line-added">+ }</span>
<span class="line-added">+ </span>
<span class="line-added">+ PhaseOutput::~PhaseOutput() {</span>
<span class="line-added">+   C-&gt;set_output(NULL);</span>
<span class="line-added">+   if (_scratch_buffer_blob != NULL) {</span>
<span class="line-added">+     BufferBlob::free(_scratch_buffer_blob);</span>
<span class="line-added">+   }</span>
<span class="line-added">+ }</span>
<span class="line-added">+ </span>
  // Convert Nodes to instruction bits and pass off to the VM
<span class="line-modified">! void PhaseOutput::Output() {</span>
    // RootNode goes
<span class="line-modified">!   assert( C-&gt;cfg()-&gt;get_root_block()-&gt;number_of_nodes() == 0, &quot;&quot; );</span>
  
    // The number of new nodes (mostly MachNop) is proportional to
    // the number of java calls and inner loops which are aligned.
    if ( C-&gt;check_node_count((NodeLimitFudgeFactor + C-&gt;java_calls()*3 +
                              C-&gt;inner_loops()*(OptoLoopAlignment-1)),
                             &quot;out of nodes before code generation&quot; ) ) {
      return;
    }
    // Make sure I can find the Start Node
<span class="line-modified">!   Block *entry = C-&gt;cfg()-&gt;get_block(1);</span>
<span class="line-modified">!   Block *broot = C-&gt;cfg()-&gt;get_root_block();</span>
  
    const StartNode *start = entry-&gt;head()-&gt;as_Start();
  
    // Replace StartNode with prolog
    Label verified_entry;
    MachPrologNode* prolog = new MachPrologNode(&amp;verified_entry);
    entry-&gt;map_node(prolog, 0);
<span class="line-modified">!   C-&gt;cfg()-&gt;map_node_to_block(prolog, entry);</span>
<span class="line-modified">!   C-&gt;cfg()-&gt;unmap_node_from_block(start); // start is no longer in any block</span>
  
    // Virtual methods need an unverified entry point
<span class="line-modified">!   if (C-&gt;is_osr_compilation()) {</span>
      if (PoisonOSREntry) {
        // TODO: Should use a ShouldNotReachHereNode...
<span class="line-modified">!       C-&gt;cfg()-&gt;insert( broot, 0, new MachBreakpointNode() );</span>
      }
    } else {
<span class="line-modified">!     if (C-&gt;method()) {</span>
<span class="line-modified">!       if (C-&gt;method()-&gt;has_scalarized_args()) {</span>
          // Add entry point to unpack all value type arguments
<span class="line-modified">!         C-&gt;cfg()-&gt;insert(broot, 0, new MachVEPNode(&amp;verified_entry, /* verified */ true, /* receiver_only */ false));</span>
<span class="line-modified">!         if (!C-&gt;method()-&gt;is_static()) {</span>
            // Add verified/unverified entry points to only unpack value type receiver at interface calls
<span class="line-modified">!           C-&gt;cfg()-&gt;insert(broot, 0, new MachVEPNode(&amp;verified_entry, /* verified */ false, /* receiver_only */ false));</span>
<span class="line-modified">!           C-&gt;cfg()-&gt;insert(broot, 0, new MachVEPNode(&amp;verified_entry, /* verified */ true,  /* receiver_only */ true));</span>
<span class="line-modified">!           C-&gt;cfg()-&gt;insert(broot, 0, new MachVEPNode(&amp;verified_entry, /* verified */ false, /* receiver_only */ true));</span>
          }
<span class="line-modified">!       } else if (!C-&gt;method()-&gt;is_static()) {</span>
          // Insert unvalidated entry point
<span class="line-modified">!         C-&gt;cfg()-&gt;insert(broot, 0, new MachUEPNode());</span>
        }
      }
    }
  
    // Break before main entry point
<span class="line-modified">!   if ((C-&gt;method() &amp;&amp; C-&gt;directive()-&gt;BreakAtExecuteOption) ||</span>
<span class="line-modified">!       (OptoBreakpoint &amp;&amp; C-&gt;is_method_compilation())       ||</span>
<span class="line-modified">!       (OptoBreakpointOSR &amp;&amp; C-&gt;is_osr_compilation())       ||</span>
<span class="line-modified">!       (OptoBreakpointC2R &amp;&amp; !C-&gt;method())                   ) {</span>
<span class="line-modified">!     // checking for C-&gt;method() means that OptoBreakpoint does not apply to</span>
      // runtime stubs or frame converters
<span class="line-modified">!     C-&gt;cfg()-&gt;insert( entry, 1, new MachBreakpointNode() );</span>
    }
  
    // Insert epilogs before every return
<span class="line-modified">!   for (uint i = 0; i &lt; C-&gt;cfg()-&gt;number_of_blocks(); i++) {</span>
<span class="line-modified">!     Block* block = C-&gt;cfg()-&gt;get_block(i);</span>
<span class="line-modified">!     if (!block-&gt;is_connector() &amp;&amp; block-&gt;non_connector_successor(0) == C-&gt;cfg()-&gt;get_root_block()) { // Found a program exit point?</span>
        Node* m = block-&gt;end();
        if (m-&gt;is_Mach() &amp;&amp; m-&gt;as_Mach()-&gt;ideal_Opcode() != Op_Halt) {
          MachEpilogNode* epilog = new MachEpilogNode(m-&gt;as_Mach()-&gt;ideal_Opcode() == Op_Return);
          block-&gt;add_inst(epilog);
<span class="line-modified">!         C-&gt;cfg()-&gt;map_node_to_block(epilog, block);</span>
        }
      }
    }
  
    // Keeper of sizing aspects
    BufferSizingData buf_sizes = BufferSizingData();
  
    // Initialize code buffer
    estimate_buffer_size(buf_sizes._const);
<span class="line-modified">!   if (C-&gt;failing()) return;</span>
  
    // Pre-compute the length of blocks and replace
    // long branches with short if machine supports it.
    // Must be done before ScheduleAndBundle due to SPARC delay slots
<span class="line-modified">!   uint* blk_starts = NEW_RESOURCE_ARRAY(uint, C-&gt;cfg()-&gt;number_of_blocks() + 1);</span>
    blk_starts[0] = 0;
    shorten_branches(blk_starts, buf_sizes);
  
<span class="line-modified">!   if (!C-&gt;is_osr_compilation() &amp;&amp; C-&gt;has_scalarized_args()) {</span>
      // Compute the offsets of the entry points required by the value type calling convention
<span class="line-modified">!     if (!C-&gt;method()-&gt;is_static()) {</span>
        // We have entries at the beginning of the method, implemented by the first 4 nodes.
        // Entry                     (unverified) @ offset 0
        // Verified_Value_Entry_RO
        // Value_Entry               (unverified)
        // Verified_Value_Entry
        uint offset = 0;
        _code_offsets.set_value(CodeOffsets::Entry, offset);
  
<span class="line-modified">!       offset += ((MachVEPNode*)broot-&gt;get_node(0))-&gt;size(C-&gt;regalloc());</span>
        _code_offsets.set_value(CodeOffsets::Verified_Value_Entry_RO, offset);
  
<span class="line-modified">!       offset += ((MachVEPNode*)broot-&gt;get_node(1))-&gt;size(C-&gt;regalloc());</span>
        _code_offsets.set_value(CodeOffsets::Value_Entry, offset);
  
<span class="line-modified">!       offset += ((MachVEPNode*)broot-&gt;get_node(2))-&gt;size(C-&gt;regalloc());</span>
        _code_offsets.set_value(CodeOffsets::Verified_Value_Entry, offset);
      } else {
        _code_offsets.set_value(CodeOffsets::Entry, -1); // will be patched later
        _code_offsets.set_value(CodeOffsets::Verified_Value_Entry, 0);
      }
    }
  
    ScheduleAndBundle();
<span class="line-modified">!   if (C-&gt;failing()) {</span>
      return;
    }
  
    // Late barrier analysis must be done after schedule and bundle
    // Otherwise liveness based spilling will fail
    BarrierSetC2* bs = BarrierSet::barrier_set()-&gt;barrier_set_c2();
    bs-&gt;late_barrier_analysis();
  
  #ifdef X86
    if (VM_Version::has_intel_jcc_erratum()) {
<span class="line-modified">!     int extra_padding = IntelJccErratum::tag_affected_machnodes(C, C-&gt;cfg(), C-&gt;regalloc());</span>
      buf_sizes._code += extra_padding;
    }
  #endif
  
    // Complete sizing of codebuffer
    CodeBuffer* cb = init_buffer(buf_sizes);
<span class="line-modified">!   if (cb == NULL || C-&gt;failing()) {</span>
      return;
    }
  
    BuildOopMaps();
  
<span class="line-modified">!   if (C-&gt;failing())  {</span>
      return;
    }
  
    fill_buffer(cb, blk_starts);
  }
  
<span class="line-modified">! bool PhaseOutput::need_stack_bang(int frame_size_in_bytes) const {</span>
    // Determine if we need to generate a stack overflow check.
    // Do it if the method is not a stub function and
    // has java calls or has frame size &gt; vm_page_size/8.
    // The debug VM checks that deoptimization doesn&#39;t trigger an
    // unexpected stack overflow (compiled method stack banging should
    // guarantee it doesn&#39;t happen) so we always need the stack bang in
    // a debug VM.
<span class="line-modified">!   return (UseStackBanging &amp;&amp; C-&gt;stub_function() == NULL &amp;&amp;</span>
<span class="line-modified">!           (C-&gt;has_java_calls() || frame_size_in_bytes &gt; os::vm_page_size()&gt;&gt;3</span>
             DEBUG_ONLY(|| true)));
  }
  
<span class="line-modified">! bool PhaseOutput::need_register_stack_bang() const {</span>
    // Determine if we need to generate a register stack overflow check.
    // This is only used on architectures which have split register
    // and memory stacks (ie. IA64).
    // Bang if the method is not a stub function and has java calls
<span class="line-modified">!   return (C-&gt;stub_function() == NULL &amp;&amp; C-&gt;has_java_calls());</span>
  }
  
  
  // Compute the size of first NumberOfLoopInstrToAlign instructions at the top
  // of a loop. When aligning a loop we need to provide enough instructions
</pre>
<hr />
<pre>
<span class="line-old-header">*** 232,49 ***</span>
  // a loop will be aligned if the size is not reset here.
  //
  // Note: Mach instructions could contain several HW instructions
  // so the size is estimated only.
  //
<span class="line-modified">! void Compile::compute_loop_first_inst_sizes() {</span>
    // The next condition is used to gate the loop alignment optimization.
    // Don&#39;t aligned a loop if there are enough instructions at the head of a loop
    // or alignment padding is larger then MaxLoopPad. By default, MaxLoopPad
    // is equal to OptoLoopAlignment-1 except on new Intel cpus, where it is
    // equal to 11 bytes which is the largest address NOP instruction.
    if (MaxLoopPad &lt; OptoLoopAlignment - 1) {
<span class="line-modified">!     uint last_block = _cfg-&gt;number_of_blocks() - 1;</span>
      for (uint i = 1; i &lt;= last_block; i++) {
<span class="line-modified">!       Block* block = _cfg-&gt;get_block(i);</span>
        // Check the first loop&#39;s block which requires an alignment.
        if (block-&gt;loop_alignment() &gt; (uint)relocInfo::addr_unit()) {
          uint sum_size = 0;
          uint inst_cnt = NumberOfLoopInstrToAlign;
<span class="line-modified">!         inst_cnt = block-&gt;compute_first_inst_size(sum_size, inst_cnt, _regalloc);</span>
  
          // Check subsequent fallthrough blocks if the loop&#39;s first
          // block(s) does not have enough instructions.
          Block *nb = block;
          while(inst_cnt &gt; 0 &amp;&amp;
                i &lt; last_block &amp;&amp;
<span class="line-modified">!               !_cfg-&gt;get_block(i + 1)-&gt;has_loop_alignment() &amp;&amp;</span>
                !nb-&gt;has_successor(block)) {
            i++;
<span class="line-modified">!           nb = _cfg-&gt;get_block(i);</span>
<span class="line-modified">!           inst_cnt  = nb-&gt;compute_first_inst_size(sum_size, inst_cnt, _regalloc);</span>
          } // while( inst_cnt &gt; 0 &amp;&amp; i &lt; last_block  )
  
          block-&gt;set_first_inst_size(sum_size);
        } // f( b-&gt;head()-&gt;is_Loop() )
      } // for( i &lt;= last_block )
    } // if( MaxLoopPad &lt; OptoLoopAlignment-1 )
  }
  
  // The architecture description provides short branch variants for some long
  // branch instructions. Replace eligible long branches with short branches.
<span class="line-modified">! void Compile::shorten_branches(uint* blk_starts, BufferSizingData&amp; buf_sizes) {</span>
    // Compute size of each block, method size, and relocation information size
<span class="line-modified">!   uint nblocks  = _cfg-&gt;number_of_blocks();</span>
  
    uint*      jmp_offset = NEW_RESOURCE_ARRAY(uint,nblocks);
    uint*      jmp_size   = NEW_RESOURCE_ARRAY(uint,nblocks);
    int*       jmp_nidx   = NEW_RESOURCE_ARRAY(int ,nblocks);
  
<span class="line-new-header">--- 437,49 ---</span>
  // a loop will be aligned if the size is not reset here.
  //
  // Note: Mach instructions could contain several HW instructions
  // so the size is estimated only.
  //
<span class="line-modified">! void PhaseOutput::compute_loop_first_inst_sizes() {</span>
    // The next condition is used to gate the loop alignment optimization.
    // Don&#39;t aligned a loop if there are enough instructions at the head of a loop
    // or alignment padding is larger then MaxLoopPad. By default, MaxLoopPad
    // is equal to OptoLoopAlignment-1 except on new Intel cpus, where it is
    // equal to 11 bytes which is the largest address NOP instruction.
    if (MaxLoopPad &lt; OptoLoopAlignment - 1) {
<span class="line-modified">!     uint last_block = C-&gt;cfg()-&gt;number_of_blocks() - 1;</span>
      for (uint i = 1; i &lt;= last_block; i++) {
<span class="line-modified">!       Block* block = C-&gt;cfg()-&gt;get_block(i);</span>
        // Check the first loop&#39;s block which requires an alignment.
        if (block-&gt;loop_alignment() &gt; (uint)relocInfo::addr_unit()) {
          uint sum_size = 0;
          uint inst_cnt = NumberOfLoopInstrToAlign;
<span class="line-modified">!         inst_cnt = block-&gt;compute_first_inst_size(sum_size, inst_cnt, C-&gt;regalloc());</span>
  
          // Check subsequent fallthrough blocks if the loop&#39;s first
          // block(s) does not have enough instructions.
          Block *nb = block;
          while(inst_cnt &gt; 0 &amp;&amp;
                i &lt; last_block &amp;&amp;
<span class="line-modified">!               !C-&gt;cfg()-&gt;get_block(i + 1)-&gt;has_loop_alignment() &amp;&amp;</span>
                !nb-&gt;has_successor(block)) {
            i++;
<span class="line-modified">!           nb = C-&gt;cfg()-&gt;get_block(i);</span>
<span class="line-modified">!           inst_cnt  = nb-&gt;compute_first_inst_size(sum_size, inst_cnt, C-&gt;regalloc());</span>
          } // while( inst_cnt &gt; 0 &amp;&amp; i &lt; last_block  )
  
          block-&gt;set_first_inst_size(sum_size);
        } // f( b-&gt;head()-&gt;is_Loop() )
      } // for( i &lt;= last_block )
    } // if( MaxLoopPad &lt; OptoLoopAlignment-1 )
  }
  
  // The architecture description provides short branch variants for some long
  // branch instructions. Replace eligible long branches with short branches.
<span class="line-modified">! void PhaseOutput::shorten_branches(uint* blk_starts, BufferSizingData&amp; buf_sizes) {</span>
    // Compute size of each block, method size, and relocation information size
<span class="line-modified">!   uint nblocks  = C-&gt;cfg()-&gt;number_of_blocks();</span>
  
    uint*      jmp_offset = NEW_RESOURCE_ARRAY(uint,nblocks);
    uint*      jmp_size   = NEW_RESOURCE_ARRAY(uint,nblocks);
    int*       jmp_nidx   = NEW_RESOURCE_ARRAY(int ,nblocks);
  
</pre>
<hr />
<pre>
<span class="line-old-header">*** 300,13 ***</span>
    // third inserts nops where needed.
  
    // Step one, perform a pessimistic sizing pass.
    uint last_call_adr = max_juint;
    uint last_avoid_back_to_back_adr = max_juint;
<span class="line-modified">!   uint nop_size = (new MachNopNode())-&gt;size(_regalloc);</span>
    for (uint i = 0; i &lt; nblocks; i++) { // For all blocks
<span class="line-modified">!     Block* block = _cfg-&gt;get_block(i);</span>
  
      // During short branch replacement, we store the relative (to blk_starts)
      // offset of jump in jmp_offset, rather than the absolute offset of jump.
      // This is so that we do not need to recompute sizes of all nodes when
      // we compute correct blk_starts in our next sizing pass.
<span class="line-new-header">--- 505,13 ---</span>
    // third inserts nops where needed.
  
    // Step one, perform a pessimistic sizing pass.
    uint last_call_adr = max_juint;
    uint last_avoid_back_to_back_adr = max_juint;
<span class="line-modified">!   uint nop_size = (new MachNopNode())-&gt;size(C-&gt;regalloc());</span>
    for (uint i = 0; i &lt; nblocks; i++) { // For all blocks
<span class="line-modified">!     Block* block = C-&gt;cfg()-&gt;get_block(i);</span>
  
      // During short branch replacement, we store the relative (to blk_starts)
      // offset of jump in jmp_offset, rather than the absolute offset of jump.
      // This is so that we do not need to recompute sizes of all nodes when
      // we compute correct blk_starts in our next sizing pass.
</pre>
<hr />
<pre>
<span class="line-old-header">*** 378,16 ***</span>
  #endif
              Unimplemented();
            }
            assert(jmp_nidx[i] == -1, &quot;block should have only one branch&quot;);
            jmp_offset[i] = blk_size;
<span class="line-modified">!           jmp_size[i]   = nj-&gt;size(_regalloc);</span>
            jmp_nidx[i]   = j;
            has_short_branch_candidate = true;
          }
        }
<span class="line-modified">!       blk_size += nj-&gt;size(_regalloc);</span>
        // Remember end of call offset
        if (nj-&gt;is_MachCall() &amp;&amp; !nj-&gt;is_MachCallLeaf()) {
          last_call_adr = blk_starts[i]+blk_size;
        }
        // Remember end of avoid_back_to_back offset
<span class="line-new-header">--- 583,16 ---</span>
  #endif
              Unimplemented();
            }
            assert(jmp_nidx[i] == -1, &quot;block should have only one branch&quot;);
            jmp_offset[i] = blk_size;
<span class="line-modified">!           jmp_size[i]   = nj-&gt;size(C-&gt;regalloc());</span>
            jmp_nidx[i]   = j;
            has_short_branch_candidate = true;
          }
        }
<span class="line-modified">!       blk_size += nj-&gt;size(C-&gt;regalloc());</span>
        // Remember end of call offset
        if (nj-&gt;is_MachCall() &amp;&amp; !nj-&gt;is_MachCallLeaf()) {
          last_call_adr = blk_starts[i]+blk_size;
        }
        // Remember end of avoid_back_to_back offset
</pre>
<hr />
<pre>
<span class="line-old-header">*** 398,11 ***</span>
  
      // When the next block starts a loop, we may insert pad NOP
      // instructions.  Since we cannot know our future alignment,
      // assume the worst.
      if (i &lt; nblocks - 1) {
<span class="line-modified">!       Block* nb = _cfg-&gt;get_block(i + 1);</span>
        int max_loop_pad = nb-&gt;code_alignment()-relocInfo::addr_unit();
        if (max_loop_pad &gt; 0) {
          assert(is_power_of_2(max_loop_pad+relocInfo::addr_unit()), &quot;&quot;);
          // Adjust last_call_adr and/or last_avoid_back_to_back_adr.
          // If either is the last instruction in this block, bump by
<span class="line-new-header">--- 603,11 ---</span>
  
      // When the next block starts a loop, we may insert pad NOP
      // instructions.  Since we cannot know our future alignment,
      // assume the worst.
      if (i &lt; nblocks - 1) {
<span class="line-modified">!       Block* nb = C-&gt;cfg()-&gt;get_block(i + 1);</span>
        int max_loop_pad = nb-&gt;code_alignment()-relocInfo::addr_unit();
        if (max_loop_pad &gt; 0) {
          assert(is_power_of_2(max_loop_pad+relocInfo::addr_unit()), &quot;&quot;);
          // Adjust last_call_adr and/or last_avoid_back_to_back_adr.
          // If either is the last instruction in this block, bump by
</pre>
<hr />
<pre>
<span class="line-old-header">*** 430,11 ***</span>
    while (has_short_branch_candidate &amp;&amp; progress) {
      progress = false;
      has_short_branch_candidate = false;
      int adjust_block_start = 0;
      for (uint i = 0; i &lt; nblocks; i++) {
<span class="line-modified">!       Block* block = _cfg-&gt;get_block(i);</span>
        int idx = jmp_nidx[i];
        MachNode* mach = (idx == -1) ? NULL: block-&gt;get_node(idx)-&gt;as_Mach();
        if (mach != NULL &amp;&amp; mach-&gt;may_be_short_branch()) {
  #ifdef ASSERT
          assert(jmp_size[i] &gt; 0 &amp;&amp; mach-&gt;is_MachBranch(), &quot;sanity&quot;);
<span class="line-new-header">--- 635,11 ---</span>
    while (has_short_branch_candidate &amp;&amp; progress) {
      progress = false;
      has_short_branch_candidate = false;
      int adjust_block_start = 0;
      for (uint i = 0; i &lt; nblocks; i++) {
<span class="line-modified">!       Block* block = C-&gt;cfg()-&gt;get_block(i);</span>
        int idx = jmp_nidx[i];
        MachNode* mach = (idx == -1) ? NULL: block-&gt;get_node(idx)-&gt;as_Mach();
        if (mach != NULL &amp;&amp; mach-&gt;may_be_short_branch()) {
  #ifdef ASSERT
          assert(jmp_size[i] &gt; 0 &amp;&amp; mach-&gt;is_MachBranch(), &quot;sanity&quot;);
</pre>
<hr />
<pre>
<span class="line-old-header">*** 467,16 ***</span>
          assert(!needs_padding || jmp_offset[i] == 0, &quot;padding only branches at the beginning of block&quot;);
  
          if (needs_padding &amp;&amp; offset &lt;= 0)
            offset -= nop_size;
  
<span class="line-modified">!         if (_matcher-&gt;is_short_branch_offset(mach-&gt;rule(), br_size, offset)) {</span>
            // We&#39;ve got a winner.  Replace this branch.
            MachNode* replacement = mach-&gt;as_MachBranch()-&gt;short_branch_version();
  
            // Update the jmp_size.
<span class="line-modified">!           int new_size = replacement-&gt;size(_regalloc);</span>
            int diff     = br_size - new_size;
            assert(diff &gt;= (int)nop_size, &quot;short_branch size should be smaller&quot;);
            // Conservatively take into account padding between
            // avoid_back_to_back branches. Previous branch could be
            // converted into avoid_back_to_back branch during next
<span class="line-new-header">--- 672,16 ---</span>
          assert(!needs_padding || jmp_offset[i] == 0, &quot;padding only branches at the beginning of block&quot;);
  
          if (needs_padding &amp;&amp; offset &lt;= 0)
            offset -= nop_size;
  
<span class="line-modified">!         if (C-&gt;matcher()-&gt;is_short_branch_offset(mach-&gt;rule(), br_size, offset)) {</span>
            // We&#39;ve got a winner.  Replace this branch.
            MachNode* replacement = mach-&gt;as_MachBranch()-&gt;short_branch_version();
  
            // Update the jmp_size.
<span class="line-modified">!           int new_size = replacement-&gt;size(C-&gt;regalloc());</span>
            int diff     = br_size - new_size;
            assert(diff &gt;= (int)nop_size, &quot;short_branch size should be smaller&quot;);
            // Conservatively take into account padding between
            // avoid_back_to_back branches. Previous branch could be
            // converted into avoid_back_to_back branch during next
</pre>
<hr />
<pre>
<span class="line-old-header">*** 510,14 ***</span>
  #ifdef ASSERT
    for (uint i = 0; i &lt; nblocks; i++) { // For all blocks
      if (jmp_target[i] != 0) {
        int br_size = jmp_size[i];
        int offset = blk_starts[jmp_target[i]]-(blk_starts[i] + jmp_offset[i]);
<span class="line-modified">!       if (!_matcher-&gt;is_short_branch_offset(jmp_rule[i], br_size, offset)) {</span>
          tty-&gt;print_cr(&quot;target (%d) - jmp_offset(%d) = offset (%d), jump_size(%d), jmp_block B%d, target_block B%d&quot;, blk_starts[jmp_target[i]], blk_starts[i] + jmp_offset[i], offset, br_size, i, jmp_target[i]);
        }
<span class="line-modified">!       assert(_matcher-&gt;is_short_branch_offset(jmp_rule[i], br_size, offset), &quot;Displacement too large for short jmp&quot;);</span>
      }
    }
  #endif
  
    // Step 3, compute the offsets of all blocks, will be done in fill_buffer()
<span class="line-new-header">--- 715,14 ---</span>
  #ifdef ASSERT
    for (uint i = 0; i &lt; nblocks; i++) { // For all blocks
      if (jmp_target[i] != 0) {
        int br_size = jmp_size[i];
        int offset = blk_starts[jmp_target[i]]-(blk_starts[i] + jmp_offset[i]);
<span class="line-modified">!       if (!C-&gt;matcher()-&gt;is_short_branch_offset(jmp_rule[i], br_size, offset)) {</span>
          tty-&gt;print_cr(&quot;target (%d) - jmp_offset(%d) = offset (%d), jump_size(%d), jmp_block B%d, target_block B%d&quot;, blk_starts[jmp_target[i]], blk_starts[i] + jmp_offset[i], offset, br_size, i, jmp_target[i]);
        }
<span class="line-modified">!       assert(C-&gt;matcher()-&gt;is_short_branch_offset(jmp_rule[i], br_size, offset), &quot;Displacement too large for short jmp&quot;);</span>
      }
    }
  #endif
  
    // Step 3, compute the offsets of all blocks, will be done in fill_buffer()
</pre>
<hr />
<pre>
<span class="line-old-header">*** 554,11 ***</span>
           : new LocationValue(Location::new_stk_loc(l_type,  ra-&gt;reg2offset(regnum)));
  }
  
  
  ObjectValue*
<span class="line-modified">! Compile::sv_for_node_id(GrowableArray&lt;ScopeValue*&gt; *objs, int id) {</span>
    for (int i = 0; i &lt; objs-&gt;length(); i++) {
      assert(objs-&gt;at(i)-&gt;is_object(), &quot;corrupt object cache&quot;);
      ObjectValue* sv = (ObjectValue*) objs-&gt;at(i);
      if (sv-&gt;id() == id) {
        return sv;
<span class="line-new-header">--- 759,11 ---</span>
           : new LocationValue(Location::new_stk_loc(l_type,  ra-&gt;reg2offset(regnum)));
  }
  
  
  ObjectValue*
<span class="line-modified">! PhaseOutput::sv_for_node_id(GrowableArray&lt;ScopeValue*&gt; *objs, int id) {</span>
    for (int i = 0; i &lt; objs-&gt;length(); i++) {
      assert(objs-&gt;at(i)-&gt;is_object(), &quot;corrupt object cache&quot;);
      ObjectValue* sv = (ObjectValue*) objs-&gt;at(i);
      if (sv-&gt;id() == id) {
        return sv;
</pre>
<hr />
<pre>
<span class="line-old-header">*** 566,48 ***</span>
    }
    // Otherwise..
    return NULL;
  }
  
<span class="line-modified">! void Compile::set_sv_for_object_node(GrowableArray&lt;ScopeValue*&gt; *objs,</span>
                                       ObjectValue* sv ) {
    assert(sv_for_node_id(objs, sv-&gt;id()) == NULL, &quot;Precondition&quot;);
    objs-&gt;append(sv);
  }
  
  
<span class="line-modified">! void Compile::FillLocArray( int idx, MachSafePointNode* sfpt, Node *local,</span>
                              GrowableArray&lt;ScopeValue*&gt; *array,
                              GrowableArray&lt;ScopeValue*&gt; *objs ) {
    assert( local, &quot;use _top instead of null&quot; );
    if (array-&gt;length() != idx) {
      assert(array-&gt;length() == idx + 1, &quot;Unexpected array count&quot;);
      // Old functionality:
      //   return
      // New functionality:
      //   Assert if the local is not top. In product mode let the new node
      //   override the old entry.
<span class="line-modified">!     assert(local == top(), &quot;LocArray collision&quot;);</span>
<span class="line-modified">!     if (local == top()) {</span>
        return;
      }
      array-&gt;pop();
    }
    const Type *t = local-&gt;bottom_type();
  
    // Is it a safepoint scalar object node?
    if (local-&gt;is_SafePointScalarObject()) {
      SafePointScalarObjectNode* spobj = local-&gt;as_SafePointScalarObject();
  
<span class="line-modified">!     ObjectValue* sv = Compile::sv_for_node_id(objs, spobj-&gt;_idx);</span>
      if (sv == NULL) {
        ciKlass* cik = t-&gt;is_oopptr()-&gt;klass();
        assert(cik-&gt;is_instance_klass() ||
               cik-&gt;is_array_klass(), &quot;Not supported allocation.&quot;);
        sv = new ObjectValue(spobj-&gt;_idx,
                             new ConstantOopWriteValue(cik-&gt;java_mirror()-&gt;constant_encoding()));
<span class="line-modified">!       Compile::set_sv_for_object_node(objs, sv);</span>
  
        uint first_ind = spobj-&gt;first_index(sfpt-&gt;jvms());
        for (uint i = 0; i &lt; spobj-&gt;n_fields(); i++) {
          Node* fld_node = sfpt-&gt;in(first_ind+i);
          (void)FillLocArray(sv-&gt;field_values()-&gt;length(), sfpt, fld_node, sv-&gt;field_values(), objs);
<span class="line-new-header">--- 771,48 ---</span>
    }
    // Otherwise..
    return NULL;
  }
  
<span class="line-modified">! void PhaseOutput::set_sv_for_object_node(GrowableArray&lt;ScopeValue*&gt; *objs,</span>
                                       ObjectValue* sv ) {
    assert(sv_for_node_id(objs, sv-&gt;id()) == NULL, &quot;Precondition&quot;);
    objs-&gt;append(sv);
  }
  
  
<span class="line-modified">! void PhaseOutput::FillLocArray( int idx, MachSafePointNode* sfpt, Node *local,</span>
                              GrowableArray&lt;ScopeValue*&gt; *array,
                              GrowableArray&lt;ScopeValue*&gt; *objs ) {
    assert( local, &quot;use _top instead of null&quot; );
    if (array-&gt;length() != idx) {
      assert(array-&gt;length() == idx + 1, &quot;Unexpected array count&quot;);
      // Old functionality:
      //   return
      // New functionality:
      //   Assert if the local is not top. In product mode let the new node
      //   override the old entry.
<span class="line-modified">!     assert(local == C-&gt;top(), &quot;LocArray collision&quot;);</span>
<span class="line-modified">!     if (local == C-&gt;top()) {</span>
        return;
      }
      array-&gt;pop();
    }
    const Type *t = local-&gt;bottom_type();
  
    // Is it a safepoint scalar object node?
    if (local-&gt;is_SafePointScalarObject()) {
      SafePointScalarObjectNode* spobj = local-&gt;as_SafePointScalarObject();
  
<span class="line-modified">!     ObjectValue* sv = sv_for_node_id(objs, spobj-&gt;_idx);</span>
      if (sv == NULL) {
        ciKlass* cik = t-&gt;is_oopptr()-&gt;klass();
        assert(cik-&gt;is_instance_klass() ||
               cik-&gt;is_array_klass(), &quot;Not supported allocation.&quot;);
        sv = new ObjectValue(spobj-&gt;_idx,
                             new ConstantOopWriteValue(cik-&gt;java_mirror()-&gt;constant_encoding()));
<span class="line-modified">!       set_sv_for_object_node(objs, sv);</span>
  
        uint first_ind = spobj-&gt;first_index(sfpt-&gt;jvms());
        for (uint i = 0; i &lt; spobj-&gt;n_fields(); i++) {
          Node* fld_node = sfpt-&gt;in(first_ind+i);
          (void)FillLocArray(sv-&gt;field_values()-&gt;length(), sfpt, fld_node, sv-&gt;field_values(), objs);
</pre>
<hr />
<pre>
<span class="line-old-header">*** 616,11 ***</span>
      array-&gt;append(sv);
      return;
    }
  
    // Grab the register number for the local
<span class="line-modified">!   OptoReg::Name regnum = _regalloc-&gt;get_reg_first(local);</span>
    if( OptoReg::is_valid(regnum) ) {// Got a register/stack?
      // Record the double as two float registers.
      // The register mask for such a value always specifies two adjacent
      // float registers, with the lower register number even.
      // Normally, the allocation of high and low words to these registers
<span class="line-new-header">--- 821,11 ---</span>
      array-&gt;append(sv);
      return;
    }
  
    // Grab the register number for the local
<span class="line-modified">!   OptoReg::Name regnum = C-&gt;regalloc()-&gt;get_reg_first(local);</span>
    if( OptoReg::is_valid(regnum) ) {// Got a register/stack?
      // Record the double as two float registers.
      // The register mask for such a value always specifies two adjacent
      // float registers, with the lower register number even.
      // Normally, the allocation of high and low words to these registers
</pre>
<hr />
<pre>
<span class="line-old-header">*** 637,51 ***</span>
      // memory word.  (Note that register numbers are completely
      // arbitrary, and are not tied to any machine-level encodings.)
  #ifdef _LP64
      if( t-&gt;base() == Type::DoubleBot || t-&gt;base() == Type::DoubleCon ) {
        array-&gt;append(new ConstantIntValue((jint)0));
<span class="line-modified">!       array-&gt;append(new_loc_value( _regalloc, regnum, Location::dbl ));</span>
      } else if ( t-&gt;base() == Type::Long ) {
        array-&gt;append(new ConstantIntValue((jint)0));
<span class="line-modified">!       array-&gt;append(new_loc_value( _regalloc, regnum, Location::lng ));</span>
      } else if ( t-&gt;base() == Type::RawPtr ) {
        // jsr/ret return address which must be restored into a the full
        // width 64-bit stack slot.
<span class="line-modified">!       array-&gt;append(new_loc_value( _regalloc, regnum, Location::lng ));</span>
      }
  #else //_LP64
  #ifdef SPARC
      if (t-&gt;base() == Type::Long &amp;&amp; OptoReg::is_reg(regnum)) {
        // For SPARC we have to swap high and low words for
        // long values stored in a single-register (g0-g7).
<span class="line-modified">!       array-&gt;append(new_loc_value( _regalloc,              regnum   , Location::normal ));</span>
<span class="line-modified">!       array-&gt;append(new_loc_value( _regalloc, OptoReg::add(regnum,1), Location::normal ));</span>
      } else
  #endif //SPARC
      if( t-&gt;base() == Type::DoubleBot || t-&gt;base() == Type::DoubleCon || t-&gt;base() == Type::Long ) {
        // Repack the double/long as two jints.
        // The convention the interpreter uses is that the second local
        // holds the first raw word of the native double representation.
        // This is actually reasonable, since locals and stack arrays
        // grow downwards in all implementations.
        // (If, on some machine, the interpreter&#39;s Java locals or stack
        // were to grow upwards, the embedded doubles would be word-swapped.)
<span class="line-modified">!       array-&gt;append(new_loc_value( _regalloc, OptoReg::add(regnum,1), Location::normal ));</span>
<span class="line-modified">!       array-&gt;append(new_loc_value( _regalloc,              regnum   , Location::normal ));</span>
      }
  #endif //_LP64
      else if( (t-&gt;base() == Type::FloatBot || t-&gt;base() == Type::FloatCon) &amp;&amp;
               OptoReg::is_reg(regnum) ) {
<span class="line-modified">!       array-&gt;append(new_loc_value( _regalloc, regnum, Matcher::float_in_double()</span>
                                                        ? Location::float_in_dbl : Location::normal ));
      } else if( t-&gt;base() == Type::Int &amp;&amp; OptoReg::is_reg(regnum) ) {
<span class="line-modified">!       array-&gt;append(new_loc_value( _regalloc, regnum, Matcher::int_in_long</span>
                                                        ? Location::int_in_long : Location::normal ));
      } else if( t-&gt;base() == Type::NarrowOop ) {
<span class="line-modified">!       array-&gt;append(new_loc_value( _regalloc, regnum, Location::narrowoop ));</span>
      } else {
<span class="line-modified">!       array-&gt;append(new_loc_value( _regalloc, regnum, _regalloc-&gt;is_oop(local) ? Location::oop : Location::normal ));</span>
      }
      return;
    }
  
    // No register.  It must be constant data.
<span class="line-new-header">--- 842,51 ---</span>
      // memory word.  (Note that register numbers are completely
      // arbitrary, and are not tied to any machine-level encodings.)
  #ifdef _LP64
      if( t-&gt;base() == Type::DoubleBot || t-&gt;base() == Type::DoubleCon ) {
        array-&gt;append(new ConstantIntValue((jint)0));
<span class="line-modified">!       array-&gt;append(new_loc_value( C-&gt;regalloc(), regnum, Location::dbl ));</span>
      } else if ( t-&gt;base() == Type::Long ) {
        array-&gt;append(new ConstantIntValue((jint)0));
<span class="line-modified">!       array-&gt;append(new_loc_value( C-&gt;regalloc(), regnum, Location::lng ));</span>
      } else if ( t-&gt;base() == Type::RawPtr ) {
        // jsr/ret return address which must be restored into a the full
        // width 64-bit stack slot.
<span class="line-modified">!       array-&gt;append(new_loc_value( C-&gt;regalloc(), regnum, Location::lng ));</span>
      }
  #else //_LP64
  #ifdef SPARC
      if (t-&gt;base() == Type::Long &amp;&amp; OptoReg::is_reg(regnum)) {
        // For SPARC we have to swap high and low words for
        // long values stored in a single-register (g0-g7).
<span class="line-modified">!       array-&gt;append(new_loc_value( C-&gt;regalloc(),              regnum   , Location::normal ));</span>
<span class="line-modified">!       array-&gt;append(new_loc_value( C-&gt;regalloc(), OptoReg::add(regnum,1), Location::normal ));</span>
      } else
  #endif //SPARC
      if( t-&gt;base() == Type::DoubleBot || t-&gt;base() == Type::DoubleCon || t-&gt;base() == Type::Long ) {
        // Repack the double/long as two jints.
        // The convention the interpreter uses is that the second local
        // holds the first raw word of the native double representation.
        // This is actually reasonable, since locals and stack arrays
        // grow downwards in all implementations.
        // (If, on some machine, the interpreter&#39;s Java locals or stack
        // were to grow upwards, the embedded doubles would be word-swapped.)
<span class="line-modified">!       array-&gt;append(new_loc_value( C-&gt;regalloc(), OptoReg::add(regnum,1), Location::normal ));</span>
<span class="line-modified">!       array-&gt;append(new_loc_value( C-&gt;regalloc(),              regnum   , Location::normal ));</span>
      }
  #endif //_LP64
      else if( (t-&gt;base() == Type::FloatBot || t-&gt;base() == Type::FloatCon) &amp;&amp;
               OptoReg::is_reg(regnum) ) {
<span class="line-modified">!       array-&gt;append(new_loc_value( C-&gt;regalloc(), regnum, Matcher::float_in_double()</span>
                                                        ? Location::float_in_dbl : Location::normal ));
      } else if( t-&gt;base() == Type::Int &amp;&amp; OptoReg::is_reg(regnum) ) {
<span class="line-modified">!       array-&gt;append(new_loc_value( C-&gt;regalloc(), regnum, Matcher::int_in_long</span>
                                                        ? Location::int_in_long : Location::normal ));
      } else if( t-&gt;base() == Type::NarrowOop ) {
<span class="line-modified">!       array-&gt;append(new_loc_value( C-&gt;regalloc(), regnum, Location::narrowoop ));</span>
      } else {
<span class="line-modified">!       array-&gt;append(new_loc_value( C-&gt;regalloc(), regnum, C-&gt;regalloc()-&gt;is_oop(local) ? Location::oop : Location::normal ));</span>
      }
      return;
    }
  
    // No register.  It must be constant data.
</pre>
<hr />
<pre>
<span class="line-old-header">*** 769,18 ***</span>
        break;
    }
  }
  
  // Determine if this node starts a bundle
<span class="line-modified">! bool Compile::starts_bundle(const Node *n) const {</span>
    return (_node_bundling_limit &gt; n-&gt;_idx &amp;&amp;
            _node_bundling_base[n-&gt;_idx].starts_bundle());
  }
  
  //--------------------------Process_OopMap_Node--------------------------------
<span class="line-modified">! void Compile::Process_OopMap_Node(MachNode *mach, int current_offset) {</span>
<span class="line-removed">- </span>
    // Handle special safepoint nodes for synchronization
    MachSafePointNode *sfn   = mach-&gt;as_MachSafePoint();
    MachCallNode      *mcall;
  
    int safepoint_pc_offset = current_offset;
<span class="line-new-header">--- 974,17 ---</span>
        break;
    }
  }
  
  // Determine if this node starts a bundle
<span class="line-modified">! bool PhaseOutput::starts_bundle(const Node *n) const {</span>
    return (_node_bundling_limit &gt; n-&gt;_idx &amp;&amp;
            _node_bundling_base[n-&gt;_idx].starts_bundle());
  }
  
  //--------------------------Process_OopMap_Node--------------------------------
<span class="line-modified">! void PhaseOutput::Process_OopMap_Node(MachNode *mach, int current_offset) {</span>
    // Handle special safepoint nodes for synchronization
    MachSafePointNode *sfn   = mach-&gt;as_MachSafePoint();
    MachCallNode      *mcall;
  
    int safepoint_pc_offset = current_offset;
</pre>
<hr />
<pre>
<span class="line-old-header">*** 789,18 ***</span>
    bool return_vt = false;
  
    // Add the safepoint in the DebugInfoRecorder
    if( !mach-&gt;is_MachCall() ) {
      mcall = NULL;
<span class="line-modified">!     debug_info()-&gt;add_safepoint(safepoint_pc_offset, sfn-&gt;_oop_map);</span>
    } else {
      mcall = mach-&gt;as_MachCall();
  
      // Is the call a MethodHandle call?
      if (mcall-&gt;is_MachCallJava()) {
        if (mcall-&gt;as_MachCallJava()-&gt;_method_handle_invoke) {
<span class="line-modified">!         assert(has_method_handle_invokes(), &quot;must have been set during call generation&quot;);</span>
          is_method_handle_invoke = true;
        }
      }
  
      // Check if a call returns an object.
<span class="line-new-header">--- 993,18 ---</span>
    bool return_vt = false;
  
    // Add the safepoint in the DebugInfoRecorder
    if( !mach-&gt;is_MachCall() ) {
      mcall = NULL;
<span class="line-modified">!     C-&gt;debug_info()-&gt;add_safepoint(safepoint_pc_offset, sfn-&gt;_oop_map);</span>
    } else {
      mcall = mach-&gt;as_MachCall();
  
      // Is the call a MethodHandle call?
      if (mcall-&gt;is_MachCallJava()) {
        if (mcall-&gt;as_MachCallJava()-&gt;_method_handle_invoke) {
<span class="line-modified">!         assert(C-&gt;has_method_handle_invokes(), &quot;must have been set during call generation&quot;);</span>
          is_method_handle_invoke = true;
        }
      }
  
      // Check if a call returns an object.
</pre>
<hr />
<pre>
<span class="line-old-header">*** 809,11 ***</span>
      }
      if (mcall-&gt;returns_vt()) {
        return_vt = true;
      }
      safepoint_pc_offset += mcall-&gt;ret_addr_offset();
<span class="line-modified">!     debug_info()-&gt;add_safepoint(safepoint_pc_offset, mcall-&gt;_oop_map);</span>
    }
  
    // Loop over the JVMState list to add scope information
    // Do not skip safepoints with a NULL method, they need monitor info
    JVMState* youngest_jvms = sfn-&gt;jvms();
<span class="line-new-header">--- 1013,11 ---</span>
      }
      if (mcall-&gt;returns_vt()) {
        return_vt = true;
      }
      safepoint_pc_offset += mcall-&gt;ret_addr_offset();
<span class="line-modified">!     C-&gt;debug_info()-&gt;add_safepoint(safepoint_pc_offset, mcall-&gt;_oop_map);</span>
    }
  
    // Loop over the JVMState list to add scope information
    // Do not skip safepoints with a NULL method, they need monitor info
    JVMState* youngest_jvms = sfn-&gt;jvms();
</pre>
<hr />
<pre>
<span class="line-old-header">*** 871,66 ***</span>
        // Create ScopeValue for object
        ScopeValue *scval = NULL;
  
        if (obj_node-&gt;is_SafePointScalarObject()) {
          SafePointScalarObjectNode* spobj = obj_node-&gt;as_SafePointScalarObject();
<span class="line-modified">!         scval = Compile::sv_for_node_id(objs, spobj-&gt;_idx);</span>
          if (scval == NULL) {
            const Type *t = spobj-&gt;bottom_type();
            ciKlass* cik = t-&gt;is_oopptr()-&gt;klass();
            assert(cik-&gt;is_instance_klass() ||
                   cik-&gt;is_array_klass(), &quot;Not supported allocation.&quot;);
            ObjectValue* sv = new ObjectValue(spobj-&gt;_idx,
                                              new ConstantOopWriteValue(cik-&gt;java_mirror()-&gt;constant_encoding()));
<span class="line-modified">!           Compile::set_sv_for_object_node(objs, sv);</span>
  
            uint first_ind = spobj-&gt;first_index(youngest_jvms);
            for (uint i = 0; i &lt; spobj-&gt;n_fields(); i++) {
              Node* fld_node = sfn-&gt;in(first_ind+i);
              (void)FillLocArray(sv-&gt;field_values()-&gt;length(), sfn, fld_node, sv-&gt;field_values(), objs);
            }
            scval = sv;
          }
        } else if (!obj_node-&gt;is_Con()) {
<span class="line-modified">!         OptoReg::Name obj_reg = _regalloc-&gt;get_reg_first(obj_node);</span>
          if( obj_node-&gt;bottom_type()-&gt;base() == Type::NarrowOop ) {
<span class="line-modified">!           scval = new_loc_value( _regalloc, obj_reg, Location::narrowoop );</span>
          } else {
<span class="line-modified">!           scval = new_loc_value( _regalloc, obj_reg, Location::oop );</span>
          }
        } else {
          const TypePtr *tp = obj_node-&gt;get_ptr_type();
          scval = new ConstantOopWriteValue(tp-&gt;is_oopptr()-&gt;const_oop()-&gt;constant_encoding());
        }
  
        OptoReg::Name box_reg = BoxLockNode::reg(box_node);
<span class="line-modified">!       Location basic_lock = Location::new_stk_loc(Location::normal,_regalloc-&gt;reg2offset(box_reg));</span>
        bool eliminated = (box_node-&gt;is_BoxLock() &amp;&amp; box_node-&gt;as_BoxLock()-&gt;is_eliminated());
        monarray-&gt;append(new MonitorValue(scval, basic_lock, eliminated));
      }
  
      // We dump the object pool first, since deoptimization reads it in first.
<span class="line-modified">!     debug_info()-&gt;dump_object_pool(objs);</span>
  
      // Build first class objects to pass to scope
<span class="line-modified">!     DebugToken *locvals = debug_info()-&gt;create_scope_values(locarray);</span>
<span class="line-modified">!     DebugToken *expvals = debug_info()-&gt;create_scope_values(exparray);</span>
<span class="line-modified">!     DebugToken *monvals = debug_info()-&gt;create_monitor_values(monarray);</span>
  
      // Make method available for all Safepoints
<span class="line-modified">!     ciMethod* scope_method = method ? method : _method;</span>
      // Describe the scope here
      assert(jvms-&gt;bci() &gt;= InvocationEntryBci &amp;&amp; jvms-&gt;bci() &lt;= 0x10000, &quot;must be a valid or entry BCI&quot;);
      assert(!jvms-&gt;should_reexecute() || depth == max_depth, &quot;reexecute allowed only for the youngest&quot;);
      // Now we can describe the scope.
      methodHandle null_mh;
      bool rethrow_exception = false;
<span class="line-modified">!     debug_info()-&gt;describe_scope(safepoint_pc_offset, null_mh, scope_method, jvms-&gt;bci(), jvms-&gt;should_reexecute(), rethrow_exception, is_method_handle_invoke, return_oop, return_vt, locvals, expvals, monvals);</span>
    } // End jvms loop
  
    // Mark the end of the scope set.
<span class="line-modified">!   debug_info()-&gt;end_safepoint(safepoint_pc_offset);</span>
  }
  
  
  
  // A simplified version of Process_OopMap_Node, to handle non-safepoints.
<span class="line-new-header">--- 1075,66 ---</span>
        // Create ScopeValue for object
        ScopeValue *scval = NULL;
  
        if (obj_node-&gt;is_SafePointScalarObject()) {
          SafePointScalarObjectNode* spobj = obj_node-&gt;as_SafePointScalarObject();
<span class="line-modified">!         scval = PhaseOutput::sv_for_node_id(objs, spobj-&gt;_idx);</span>
          if (scval == NULL) {
            const Type *t = spobj-&gt;bottom_type();
            ciKlass* cik = t-&gt;is_oopptr()-&gt;klass();
            assert(cik-&gt;is_instance_klass() ||
                   cik-&gt;is_array_klass(), &quot;Not supported allocation.&quot;);
            ObjectValue* sv = new ObjectValue(spobj-&gt;_idx,
                                              new ConstantOopWriteValue(cik-&gt;java_mirror()-&gt;constant_encoding()));
<span class="line-modified">!           PhaseOutput::set_sv_for_object_node(objs, sv);</span>
  
            uint first_ind = spobj-&gt;first_index(youngest_jvms);
            for (uint i = 0; i &lt; spobj-&gt;n_fields(); i++) {
              Node* fld_node = sfn-&gt;in(first_ind+i);
              (void)FillLocArray(sv-&gt;field_values()-&gt;length(), sfn, fld_node, sv-&gt;field_values(), objs);
            }
            scval = sv;
          }
        } else if (!obj_node-&gt;is_Con()) {
<span class="line-modified">!         OptoReg::Name obj_reg = C-&gt;regalloc()-&gt;get_reg_first(obj_node);</span>
          if( obj_node-&gt;bottom_type()-&gt;base() == Type::NarrowOop ) {
<span class="line-modified">!           scval = new_loc_value( C-&gt;regalloc(), obj_reg, Location::narrowoop );</span>
          } else {
<span class="line-modified">!           scval = new_loc_value( C-&gt;regalloc(), obj_reg, Location::oop );</span>
          }
        } else {
          const TypePtr *tp = obj_node-&gt;get_ptr_type();
          scval = new ConstantOopWriteValue(tp-&gt;is_oopptr()-&gt;const_oop()-&gt;constant_encoding());
        }
  
        OptoReg::Name box_reg = BoxLockNode::reg(box_node);
<span class="line-modified">!       Location basic_lock = Location::new_stk_loc(Location::normal,C-&gt;regalloc()-&gt;reg2offset(box_reg));</span>
        bool eliminated = (box_node-&gt;is_BoxLock() &amp;&amp; box_node-&gt;as_BoxLock()-&gt;is_eliminated());
        monarray-&gt;append(new MonitorValue(scval, basic_lock, eliminated));
      }
  
      // We dump the object pool first, since deoptimization reads it in first.
<span class="line-modified">!     C-&gt;debug_info()-&gt;dump_object_pool(objs);</span>
  
      // Build first class objects to pass to scope
<span class="line-modified">!     DebugToken *locvals = C-&gt;debug_info()-&gt;create_scope_values(locarray);</span>
<span class="line-modified">!     DebugToken *expvals = C-&gt;debug_info()-&gt;create_scope_values(exparray);</span>
<span class="line-modified">!     DebugToken *monvals = C-&gt;debug_info()-&gt;create_monitor_values(monarray);</span>
  
      // Make method available for all Safepoints
<span class="line-modified">!     ciMethod* scope_method = method ? method : C-&gt;method();</span>
      // Describe the scope here
      assert(jvms-&gt;bci() &gt;= InvocationEntryBci &amp;&amp; jvms-&gt;bci() &lt;= 0x10000, &quot;must be a valid or entry BCI&quot;);
      assert(!jvms-&gt;should_reexecute() || depth == max_depth, &quot;reexecute allowed only for the youngest&quot;);
      // Now we can describe the scope.
      methodHandle null_mh;
      bool rethrow_exception = false;
<span class="line-modified">!     C-&gt;debug_info()-&gt;describe_scope(safepoint_pc_offset, null_mh, scope_method, jvms-&gt;bci(), jvms-&gt;should_reexecute(), rethrow_exception, is_method_handle_invoke, return_oop, return_vt, locvals, expvals, monvals);</span>
    } // End jvms loop
  
    // Mark the end of the scope set.
<span class="line-modified">!   C-&gt;debug_info()-&gt;end_safepoint(safepoint_pc_offset);</span>
  }
  
  
  
  // A simplified version of Process_OopMap_Node, to handle non-safepoints.
</pre>
<hr />
<pre>
<span class="line-old-header">*** 1014,31 ***</span>
    // Mark the end of the scope set.
    debug_info-&gt;end_non_safepoint(pc_offset);
  }
  
  //------------------------------init_buffer------------------------------------
<span class="line-modified">! void Compile::estimate_buffer_size(int&amp; const_req) {</span>
  
    // Set the initially allocated size
    const_req = initial_const_capacity;
  
    // The extra spacing after the code is necessary on some platforms.
    // Sometimes we need to patch in a jump after the last instruction,
    // if the nmethod has been deoptimized.  (See 4932387, 4894843.)
  
    // Compute the byte offset where we can store the deopt pc.
<span class="line-modified">!   if (fixed_slots() != 0) {</span>
<span class="line-modified">!     _orig_pc_slot_offset_in_bytes = _regalloc-&gt;reg2offset(OptoReg::stack2reg(_orig_pc_slot));</span>
    }
    if (C-&gt;needs_stack_repair()) {
      // Compute the byte offset of the stack increment value
<span class="line-modified">!     _sp_inc_slot_offset_in_bytes = _regalloc-&gt;reg2offset(OptoReg::stack2reg(_sp_inc_slot));</span>
    }
  
    // Compute prolog code size
    _method_size = 0;
<span class="line-modified">!   _frame_slots = OptoReg::reg2stack(_matcher-&gt;_old_SP) + _regalloc-&gt;_framesize;</span>
  #if defined(IA64) &amp;&amp; !defined(AIX)
    if (save_argument_registers()) {
      // 4815101: this is a stub with implicit and unknown precision fp args.
      // The usual spill mechanism can only generate stfd&#39;s in this case, which
      // doesn&#39;t work if the fp reg to spill contains a single-precision denorm.
<span class="line-new-header">--- 1218,31 ---</span>
    // Mark the end of the scope set.
    debug_info-&gt;end_non_safepoint(pc_offset);
  }
  
  //------------------------------init_buffer------------------------------------
<span class="line-modified">! void PhaseOutput::estimate_buffer_size(int&amp; const_req) {</span>
  
    // Set the initially allocated size
    const_req = initial_const_capacity;
  
    // The extra spacing after the code is necessary on some platforms.
    // Sometimes we need to patch in a jump after the last instruction,
    // if the nmethod has been deoptimized.  (See 4932387, 4894843.)
  
    // Compute the byte offset where we can store the deopt pc.
<span class="line-modified">!   if (C-&gt;fixed_slots() != 0) {</span>
<span class="line-modified">!     _orig_pc_slot_offset_in_bytes = C-&gt;regalloc()-&gt;reg2offset(OptoReg::stack2reg(_orig_pc_slot));</span>
    }
    if (C-&gt;needs_stack_repair()) {
      // Compute the byte offset of the stack increment value
<span class="line-modified">!     _sp_inc_slot_offset_in_bytes = C-&gt;regalloc()-&gt;reg2offset(OptoReg::stack2reg(_sp_inc_slot));</span>
    }
  
    // Compute prolog code size
    _method_size = 0;
<span class="line-modified">!   _frame_slots = OptoReg::reg2stack(C-&gt;matcher()-&gt;_old_SP) + C-&gt;regalloc()-&gt;_framesize;</span>
  #if defined(IA64) &amp;&amp; !defined(AIX)
    if (save_argument_registers()) {
      // 4815101: this is a stub with implicit and unknown precision fp args.
      // The usual spill mechanism can only generate stfd&#39;s in this case, which
      // doesn&#39;t work if the fp reg to spill contains a single-precision denorm.
</pre>
<hr />
<pre>
<span class="line-old-header">*** 1052,16 ***</span>
      _frame_slots += 8*(16/BytesPerInt);
    }
  #endif
    assert(_frame_slots &gt;= 0 &amp;&amp; _frame_slots &lt; 1000000, &quot;sanity check&quot;);
  
<span class="line-modified">!   if (has_mach_constant_base_node()) {</span>
      uint add_size = 0;
      // Fill the constant table.
      // Note:  This must happen before shorten_branches.
<span class="line-modified">!     for (uint i = 0; i &lt; _cfg-&gt;number_of_blocks(); i++) {</span>
<span class="line-modified">!       Block* b = _cfg-&gt;get_block(i);</span>
  
        for (uint j = 0; j &lt; b-&gt;number_of_nodes(); j++) {
          Node* n = b-&gt;get_node(j);
  
          // If the node is a MachConstantNode evaluate the constant
<span class="line-new-header">--- 1256,16 ---</span>
      _frame_slots += 8*(16/BytesPerInt);
    }
  #endif
    assert(_frame_slots &gt;= 0 &amp;&amp; _frame_slots &lt; 1000000, &quot;sanity check&quot;);
  
<span class="line-modified">!   if (C-&gt;has_mach_constant_base_node()) {</span>
      uint add_size = 0;
      // Fill the constant table.
      // Note:  This must happen before shorten_branches.
<span class="line-modified">!     for (uint i = 0; i &lt; C-&gt;cfg()-&gt;number_of_blocks(); i++) {</span>
<span class="line-modified">!       Block* b = C-&gt;cfg()-&gt;get_block(i);</span>
  
        for (uint j = 0; j &lt; b-&gt;number_of_nodes(); j++) {
          Node* n = b-&gt;get_node(j);
  
          // If the node is a MachConstantNode evaluate the constant
</pre>
<hr />
<pre>
<span class="line-old-header">*** 1085,11 ***</span>
    // Initialize the space for the BufferBlob used to find and verify
    // instruction size in MachNode::emit_size()
    init_scratch_buffer_blob(const_req);
  }
  
<span class="line-modified">! CodeBuffer* Compile::init_buffer(BufferSizingData&amp; buf_sizes) {</span>
  
    int stub_req  = buf_sizes._stub;
    int code_req  = buf_sizes._code;
    int const_req = buf_sizes._const;
  
<span class="line-new-header">--- 1289,11 ---</span>
    // Initialize the space for the BufferBlob used to find and verify
    // instruction size in MachNode::emit_size()
    init_scratch_buffer_blob(const_req);
  }
  
<span class="line-modified">! CodeBuffer* PhaseOutput::init_buffer(BufferSizingData&amp; buf_sizes) {</span>
  
    int stub_req  = buf_sizes._stub;
    int code_req  = buf_sizes._code;
    int const_req = buf_sizes._const;
  
</pre>
<hr />
<pre>
<span class="line-old-header">*** 1114,11 ***</span>
            pad_req +
            stub_req +
            exception_handler_req +
            deopt_handler_req;               // deopt handler
  
<span class="line-modified">!   if (has_method_handle_invokes())</span>
      total_req += deopt_handler_req;  // deopt MH handler
  
    CodeBuffer* cb = code_buffer();
    cb-&gt;initialize(total_req, buf_sizes._reloc);
  
<span class="line-new-header">--- 1318,11 ---</span>
            pad_req +
            stub_req +
            exception_handler_req +
            deopt_handler_req;               // deopt handler
  
<span class="line-modified">!   if (C-&gt;has_method_handle_invokes())</span>
      total_req += deopt_handler_req;  // deopt MH handler
  
    CodeBuffer* cb = code_buffer();
    cb-&gt;initialize(total_req, buf_sizes._reloc);
  
</pre>
<hr />
<pre>
<span class="line-old-header">*** 1128,21 ***</span>
      return NULL;
    }
    // Configure the code buffer.
    cb-&gt;initialize_consts_size(const_req);
    cb-&gt;initialize_stubs_size(stub_req);
<span class="line-modified">!   cb-&gt;initialize_oop_recorder(env()-&gt;oop_recorder());</span>
  
    // fill in the nop array for bundling computations
    MachNode *_nop_list[Bundle::_nop_count];
    Bundle::initialize_nops(_nop_list);
  
    return cb;
  }
  
  //------------------------------fill_buffer------------------------------------
<span class="line-modified">! void Compile::fill_buffer(CodeBuffer* cb, uint* blk_starts) {</span>
    // blk_starts[] contains offsets calculated during short branches processing,
    // offsets should not be increased during following steps.
  
    // Compute the size of first NumberOfLoopInstrToAlign instructions at head
    // of a loop. It is used to determine the padding for loop alignment.
<span class="line-new-header">--- 1332,21 ---</span>
      return NULL;
    }
    // Configure the code buffer.
    cb-&gt;initialize_consts_size(const_req);
    cb-&gt;initialize_stubs_size(stub_req);
<span class="line-modified">!   cb-&gt;initialize_oop_recorder(C-&gt;env()-&gt;oop_recorder());</span>
  
    // fill in the nop array for bundling computations
    MachNode *_nop_list[Bundle::_nop_count];
    Bundle::initialize_nops(_nop_list);
  
    return cb;
  }
  
  //------------------------------fill_buffer------------------------------------
<span class="line-modified">! void PhaseOutput::fill_buffer(CodeBuffer* cb, uint* blk_starts) {</span>
    // blk_starts[] contains offsets calculated during short branches processing,
    // offsets should not be increased during following steps.
  
    // Compute the size of first NumberOfLoopInstrToAlign instructions at head
    // of a loop. It is used to determine the padding for loop alignment.
</pre>
<hr />
<pre>
<span class="line-old-header">*** 1150,22 ***</span>
  
    // Create oopmap set.
    _oop_map_set = new OopMapSet();
  
    // !!!!! This preserves old handling of oopmaps for now
<span class="line-modified">!   debug_info()-&gt;set_oopmaps(_oop_map_set);</span>
  
<span class="line-modified">!   uint nblocks  = _cfg-&gt;number_of_blocks();</span>
    // Count and start of implicit null check instructions
    uint inct_cnt = 0;
    uint *inct_starts = NEW_RESOURCE_ARRAY(uint, nblocks+1);
  
    // Count and start of calls
    uint *call_returns = NEW_RESOURCE_ARRAY(uint, nblocks+1);
  
    uint  return_offset = 0;
<span class="line-modified">!   int nop_size = (new MachNopNode())-&gt;size(_regalloc);</span>
  
    int previous_offset = 0;
    int current_offset  = 0;
    int last_call_offset = -1;
    int last_avoid_back_to_back_offset = -1;
<span class="line-new-header">--- 1354,22 ---</span>
  
    // Create oopmap set.
    _oop_map_set = new OopMapSet();
  
    // !!!!! This preserves old handling of oopmaps for now
<span class="line-modified">!   C-&gt;debug_info()-&gt;set_oopmaps(_oop_map_set);</span>
  
<span class="line-modified">!   uint nblocks  = C-&gt;cfg()-&gt;number_of_blocks();</span>
    // Count and start of implicit null check instructions
    uint inct_cnt = 0;
    uint *inct_starts = NEW_RESOURCE_ARRAY(uint, nblocks+1);
  
    // Count and start of calls
    uint *call_returns = NEW_RESOURCE_ARRAY(uint, nblocks+1);
  
    uint  return_offset = 0;
<span class="line-modified">!   int nop_size = (new MachNopNode())-&gt;size(C-&gt;regalloc());</span>
  
    int previous_offset = 0;
    int current_offset  = 0;
    int last_call_offset = -1;
    int last_avoid_back_to_back_offset = -1;
</pre>
<hr />
<pre>
<span class="line-old-header">*** 1177,25 ***</span>
  #endif
  
    // Create an array of unused labels, one for each basic block, if printing is enabled
  #if defined(SUPPORT_OPTO_ASSEMBLY)
    int *node_offsets      = NULL;
<span class="line-modified">!   uint node_offset_limit = unique();</span>
  
<span class="line-modified">!   if (print_assembly()) {</span>
      node_offsets = NEW_RESOURCE_ARRAY(int, node_offset_limit);
    }
    if (node_offsets != NULL) {
      // We need to initialize. Unused array elements may contain garbage and mess up PrintOptoAssembly.
      memset(node_offsets, 0, node_offset_limit*sizeof(int));
    }
  #endif
  
<span class="line-modified">!   NonSafepointEmitter non_safepoints(this);  // emit non-safepoints lazily</span>
  
    // Emit the constant table.
<span class="line-modified">!   if (has_mach_constant_base_node()) {</span>
      constant_table().emit(*cb);
    }
  
    // Create an array of labels, one for each basic block
    Label *blk_labels = NEW_RESOURCE_ARRAY(Label, nblocks+1);
<span class="line-new-header">--- 1381,25 ---</span>
  #endif
  
    // Create an array of unused labels, one for each basic block, if printing is enabled
  #if defined(SUPPORT_OPTO_ASSEMBLY)
    int *node_offsets      = NULL;
<span class="line-modified">!   uint node_offset_limit = C-&gt;unique();</span>
  
<span class="line-modified">!   if (C-&gt;print_assembly()) {</span>
      node_offsets = NEW_RESOURCE_ARRAY(int, node_offset_limit);
    }
    if (node_offsets != NULL) {
      // We need to initialize. Unused array elements may contain garbage and mess up PrintOptoAssembly.
      memset(node_offsets, 0, node_offset_limit*sizeof(int));
    }
  #endif
  
<span class="line-modified">!   NonSafepointEmitter non_safepoints(C);  // emit non-safepoints lazily</span>
  
    // Emit the constant table.
<span class="line-modified">!   if (C-&gt;has_mach_constant_base_node()) {</span>
      constant_table().emit(*cb);
    }
  
    // Create an array of labels, one for each basic block
    Label *blk_labels = NEW_RESOURCE_ARRAY(Label, nblocks+1);
</pre>
<hr />
<pre>
<span class="line-old-header">*** 1206,11 ***</span>
    // ------------------
    // Now fill in the code buffer
    Node *delay_slot = NULL;
  
    for (uint i = 0; i &lt; nblocks; i++) {
<span class="line-modified">!     Block* block = _cfg-&gt;get_block(i);</span>
      Node* head = block-&gt;head();
  
      // If this block needs to start aligned (i.e, can be reached other
      // than by falling-thru from the previous block), then force the
      // start of a new bundle.
<span class="line-new-header">--- 1410,11 ---</span>
    // ------------------
    // Now fill in the code buffer
    Node *delay_slot = NULL;
  
    for (uint i = 0; i &lt; nblocks; i++) {
<span class="line-modified">!     Block* block = C-&gt;cfg()-&gt;get_block(i);</span>
      Node* head = block-&gt;head();
  
      // If this block needs to start aligned (i.e, can be reached other
      // than by falling-thru from the previous block), then force the
      // start of a new bundle.
</pre>
<hr />
<pre>
<span class="line-old-header">*** 1219,11 ***</span>
      }
  
  #ifdef ASSERT
      if (!block-&gt;is_connector()) {
        stringStream st;
<span class="line-modified">!       block-&gt;dump_head(_cfg, &amp;st);</span>
        MacroAssembler(cb).block_comment(st.as_string());
      }
      jmp_target[i] = 0;
      jmp_offset[i] = 0;
      jmp_size[i]   = 0;
<span class="line-new-header">--- 1423,11 ---</span>
      }
  
  #ifdef ASSERT
      if (!block-&gt;is_connector()) {
        stringStream st;
<span class="line-modified">!       block-&gt;dump_head(C-&gt;cfg(), &amp;st);</span>
        MacroAssembler(cb).block_comment(st.as_string());
      }
      jmp_target[i] = 0;
      jmp_offset[i] = 0;
      jmp_size[i]   = 0;
</pre>
<hr />
<pre>
<span class="line-old-header">*** 1242,14 ***</span>
  
        // Get the node
        Node* n = block-&gt;get_node(j);
  
        // See if delay slots are supported
<span class="line-modified">!       if (valid_bundle_info(n) &amp;&amp;</span>
<span class="line-removed">-           node_bundling(n)-&gt;used_in_unconditional_delay()) {</span>
          assert(delay_slot == NULL, &quot;no use of delay slot node&quot;);
<span class="line-modified">!         assert(n-&gt;size(_regalloc) == Pipeline::instr_unit_size(), &quot;delay slot instruction wrong size&quot;);</span>
  
          delay_slot = n;
          continue;
        }
  
<span class="line-new-header">--- 1446,13 ---</span>
  
        // Get the node
        Node* n = block-&gt;get_node(j);
  
        // See if delay slots are supported
<span class="line-modified">!       if (valid_bundle_info(n) &amp;&amp; node_bundling(n)-&gt;used_in_unconditional_delay()) {</span>
          assert(delay_slot == NULL, &quot;no use of delay slot node&quot;);
<span class="line-modified">!         assert(n-&gt;size(C-&gt;regalloc()) == Pipeline::instr_unit_size(), &quot;delay slot instruction wrong size&quot;);</span>
  
          delay_slot = n;
          continue;
        }
  
</pre>
<hr />
<pre>
<span class="line-old-header">*** 1287,28 ***</span>
            padding = nop_size;
          }
  #ifdef X86
          if (mach-&gt;flags() &amp; Node::Flag_intel_jcc_erratum) {
            assert(padding == 0, &quot;can&#39;t have contradicting padding requirements&quot;);
<span class="line-modified">!           padding = IntelJccErratum::compute_padding(current_offset, mach, block, j, _regalloc);</span>
          }
  #endif
  
          if (padding &gt; 0) {
            assert((padding % nop_size) == 0, &quot;padding is not a multiple of NOP size&quot;);
            int nops_cnt = padding / nop_size;
            MachNode *nop = new MachNopNode(nops_cnt);
            block-&gt;insert_node(nop, j++);
            last_inst++;
<span class="line-modified">!           _cfg-&gt;map_node_to_block(nop, block);</span>
            // Ensure enough space.
            cb-&gt;insts()-&gt;maybe_expand_to_ensure_remaining(MAX_inst_size);
            if ((cb-&gt;blob() == NULL) || (!CompileBroker::should_compile_new_jobs())) {
              C-&gt;record_failure(&quot;CodeCache is full&quot;);
              return;
            }
<span class="line-modified">!           nop-&gt;emit(*cb, _regalloc);</span>
            cb-&gt;flush_bundle(true);
            current_offset = cb-&gt;insts_size();
          }
  
          // Remember the start of the last call in a basic block
<span class="line-new-header">--- 1490,28 ---</span>
            padding = nop_size;
          }
  #ifdef X86
          if (mach-&gt;flags() &amp; Node::Flag_intel_jcc_erratum) {
            assert(padding == 0, &quot;can&#39;t have contradicting padding requirements&quot;);
<span class="line-modified">!           padding = IntelJccErratum::compute_padding(current_offset, mach, block, j, C-&gt;regalloc());</span>
          }
  #endif
  
          if (padding &gt; 0) {
            assert((padding % nop_size) == 0, &quot;padding is not a multiple of NOP size&quot;);
            int nops_cnt = padding / nop_size;
            MachNode *nop = new MachNopNode(nops_cnt);
            block-&gt;insert_node(nop, j++);
            last_inst++;
<span class="line-modified">!           C-&gt;cfg()-&gt;map_node_to_block(nop, block);</span>
            // Ensure enough space.
            cb-&gt;insts()-&gt;maybe_expand_to_ensure_remaining(MAX_inst_size);
            if ((cb-&gt;blob() == NULL) || (!CompileBroker::should_compile_new_jobs())) {
              C-&gt;record_failure(&quot;CodeCache is full&quot;);
              return;
            }
<span class="line-modified">!           nop-&gt;emit(*cb, C-&gt;regalloc());</span>
            cb-&gt;flush_bundle(true);
            current_offset = cb-&gt;insts_size();
          }
  
          // Remember the start of the last call in a basic block
</pre>
<hr />
<pre>
<span class="line-old-header">*** 1359,14 ***</span>
  
            // Try to replace long branch if delay slot is not used,
            // it is mostly for back branches since forward branch&#39;s
            // distance is not updated yet.
            bool delay_slot_is_used = valid_bundle_info(n) &amp;&amp;
<span class="line-modified">!                                     node_bundling(n)-&gt;use_unconditional_delay();</span>
            if (!delay_slot_is_used &amp;&amp; mach-&gt;may_be_short_branch()) {
              assert(delay_slot == NULL, &quot;not expecting delay slot node&quot;);
<span class="line-modified">!             int br_size = n-&gt;size(_regalloc);</span>
              int offset = blk_starts[block_num] - current_offset;
              if (block_num &gt;= i) {
                // Current and following block&#39;s offset are not
                // finalized yet, adjust distance by the difference
                // between calculated and final offsets of current block.
<span class="line-new-header">--- 1562,14 ---</span>
  
            // Try to replace long branch if delay slot is not used,
            // it is mostly for back branches since forward branch&#39;s
            // distance is not updated yet.
            bool delay_slot_is_used = valid_bundle_info(n) &amp;&amp;
<span class="line-modified">!                                     C-&gt;output()-&gt;node_bundling(n)-&gt;use_unconditional_delay();</span>
            if (!delay_slot_is_used &amp;&amp; mach-&gt;may_be_short_branch()) {
              assert(delay_slot == NULL, &quot;not expecting delay slot node&quot;);
<span class="line-modified">!             int br_size = n-&gt;size(C-&gt;regalloc());</span>
              int offset = blk_starts[block_num] - current_offset;
              if (block_num &gt;= i) {
                // Current and following block&#39;s offset are not
                // finalized yet, adjust distance by the difference
                // between calculated and final offsets of current block.
</pre>
<hr />
<pre>
<span class="line-old-header">*** 1376,24 ***</span>
              // the branch which will increase the backward distance.
              bool needs_padding = (current_offset == last_avoid_back_to_back_offset);
              if (needs_padding &amp;&amp; offset &lt;= 0)
                offset -= nop_size;
  
<span class="line-modified">!             if (_matcher-&gt;is_short_branch_offset(mach-&gt;rule(), br_size, offset)) {</span>
                // We&#39;ve got a winner.  Replace this branch.
                MachNode* replacement = mach-&gt;as_MachBranch()-&gt;short_branch_version();
  
                // Update the jmp_size.
<span class="line-modified">!               int new_size = replacement-&gt;size(_regalloc);</span>
                assert((br_size - new_size) &gt;= (int)nop_size, &quot;short_branch size should be smaller&quot;);
                // Insert padding between avoid_back_to_back branches.
                if (needs_padding &amp;&amp; replacement-&gt;avoid_back_to_back(MachNode::AVOID_BEFORE)) {
                  MachNode *nop = new MachNopNode();
                  block-&gt;insert_node(nop, j++);
<span class="line-modified">!                 _cfg-&gt;map_node_to_block(nop, block);</span>
                  last_inst++;
<span class="line-modified">!                 nop-&gt;emit(*cb, _regalloc);</span>
                  cb-&gt;flush_bundle(true);
                  current_offset = cb-&gt;insts_size();
                }
  #ifdef ASSERT
                jmp_target[i] = block_num;
<span class="line-new-header">--- 1579,24 ---</span>
              // the branch which will increase the backward distance.
              bool needs_padding = (current_offset == last_avoid_back_to_back_offset);
              if (needs_padding &amp;&amp; offset &lt;= 0)
                offset -= nop_size;
  
<span class="line-modified">!             if (C-&gt;matcher()-&gt;is_short_branch_offset(mach-&gt;rule(), br_size, offset)) {</span>
                // We&#39;ve got a winner.  Replace this branch.
                MachNode* replacement = mach-&gt;as_MachBranch()-&gt;short_branch_version();
  
                // Update the jmp_size.
<span class="line-modified">!               int new_size = replacement-&gt;size(C-&gt;regalloc());</span>
                assert((br_size - new_size) &gt;= (int)nop_size, &quot;short_branch size should be smaller&quot;);
                // Insert padding between avoid_back_to_back branches.
                if (needs_padding &amp;&amp; replacement-&gt;avoid_back_to_back(MachNode::AVOID_BEFORE)) {
                  MachNode *nop = new MachNopNode();
                  block-&gt;insert_node(nop, j++);
<span class="line-modified">!                 C-&gt;cfg()-&gt;map_node_to_block(nop, block);</span>
                  last_inst++;
<span class="line-modified">!                 nop-&gt;emit(*cb, C-&gt;regalloc());</span>
                  cb-&gt;flush_bundle(true);
                  current_offset = cb-&gt;insts_size();
                }
  #ifdef ASSERT
                jmp_target[i] = block_num;
</pre>
<hr />
<pre>
<span class="line-old-header">*** 1471,22 ***</span>
        }
  #endif
  
        // &quot;Normal&quot; instruction case
        DEBUG_ONLY( uint instr_offset = cb-&gt;insts_size(); )
<span class="line-modified">!       n-&gt;emit(*cb, _regalloc);</span>
        current_offset  = cb-&gt;insts_size();
  
        // Above we only verified that there is enough space in the instruction section.
        // However, the instruction may emit stubs that cause code buffer expansion.
        // Bail out here if expansion failed due to a lack of code cache space.
<span class="line-modified">!       if (failing()) {</span>
          return;
        }
  
  #ifdef ASSERT
<span class="line-modified">!       if (n-&gt;size(_regalloc) &lt; (current_offset-instr_offset)) {</span>
          n-&gt;dump();
          assert(false, &quot;wrong size of mach node&quot;);
        }
  #endif
        non_safepoints.observe_instruction(n, current_offset);
<span class="line-new-header">--- 1674,22 ---</span>
        }
  #endif
  
        // &quot;Normal&quot; instruction case
        DEBUG_ONLY( uint instr_offset = cb-&gt;insts_size(); )
<span class="line-modified">!       n-&gt;emit(*cb, C-&gt;regalloc());</span>
        current_offset  = cb-&gt;insts_size();
  
        // Above we only verified that there is enough space in the instruction section.
        // However, the instruction may emit stubs that cause code buffer expansion.
        // Bail out here if expansion failed due to a lack of code cache space.
<span class="line-modified">!       if (C-&gt;failing()) {</span>
          return;
        }
  
  #ifdef ASSERT
<span class="line-modified">!       if (n-&gt;size(C-&gt;regalloc()) &lt; (current_offset-instr_offset)) {</span>
          n-&gt;dump();
          assert(false, &quot;wrong size of mach node&quot;);
        }
  #endif
        non_safepoints.observe_instruction(n, current_offset);
</pre>
<hr />
<pre>
<span class="line-old-header">*** 1537,28 ***</span>
            // Generate an OopMap entry
            Process_OopMap_Node(mach, adjusted_offset);
          }
  
          // Insert the delay slot instruction
<span class="line-modified">!         delay_slot-&gt;emit(*cb, _regalloc);</span>
  
          // Don&#39;t reuse it
          delay_slot = NULL;
        }
  
      } // End for all instructions in block
  
      // If the next block is the top of a loop, pad this block out to align
      // the loop top a little. Helps prevent pipe stalls at loop back branches.
      if (i &lt; nblocks-1) {
<span class="line-modified">!       Block *nb = _cfg-&gt;get_block(i + 1);</span>
        int padding = nb-&gt;alignment_padding(current_offset);
        if( padding &gt; 0 ) {
          MachNode *nop = new MachNopNode(padding / nop_size);
          block-&gt;insert_node(nop, block-&gt;number_of_nodes());
<span class="line-modified">!         _cfg-&gt;map_node_to_block(nop, block);</span>
<span class="line-modified">!         nop-&gt;emit(*cb, _regalloc);</span>
          current_offset = cb-&gt;insts_size();
        }
      }
      // Verify that the distance for generated before forward
      // short branches is still valid.
<span class="line-new-header">--- 1740,28 ---</span>
            // Generate an OopMap entry
            Process_OopMap_Node(mach, adjusted_offset);
          }
  
          // Insert the delay slot instruction
<span class="line-modified">!         delay_slot-&gt;emit(*cb, C-&gt;regalloc());</span>
  
          // Don&#39;t reuse it
          delay_slot = NULL;
        }
  
      } // End for all instructions in block
  
      // If the next block is the top of a loop, pad this block out to align
      // the loop top a little. Helps prevent pipe stalls at loop back branches.
      if (i &lt; nblocks-1) {
<span class="line-modified">!       Block *nb = C-&gt;cfg()-&gt;get_block(i + 1);</span>
        int padding = nb-&gt;alignment_padding(current_offset);
        if( padding &gt; 0 ) {
          MachNode *nop = new MachNopNode(padding / nop_size);
          block-&gt;insert_node(nop, block-&gt;number_of_nodes());
<span class="line-modified">!         C-&gt;cfg()-&gt;map_node_to_block(nop, block);</span>
<span class="line-modified">!         nop-&gt;emit(*cb, C-&gt;regalloc());</span>
          current_offset = cb-&gt;insts_size();
        }
      }
      // Verify that the distance for generated before forward
      // short branches is still valid.
</pre>
<hr />
<pre>
<span class="line-old-header">*** 1570,11 ***</span>
    blk_starts[nblocks] = current_offset;
  
    non_safepoints.flush_at_end();
  
    // Offset too large?
<span class="line-modified">!   if (failing())  return;</span>
  
    // Define a pseudo-label at the end of the code
    MacroAssembler(cb).bind( blk_labels[nblocks] );
  
    // Compute the size of the first block
<span class="line-new-header">--- 1773,11 ---</span>
    blk_starts[nblocks] = current_offset;
  
    non_safepoints.flush_at_end();
  
    // Offset too large?
<span class="line-modified">!   if (C-&gt;failing())  return;</span>
  
    // Define a pseudo-label at the end of the code
    MacroAssembler(cb).bind( blk_labels[nblocks] );
  
    // Compute the size of the first block
</pre>
<hr />
<pre>
<span class="line-old-header">*** 1583,21 ***</span>
  #ifdef ASSERT
    for (uint i = 0; i &lt; nblocks; i++) { // For all blocks
      if (jmp_target[i] != 0) {
        int br_size = jmp_size[i];
        int offset = blk_starts[jmp_target[i]]-(blk_starts[i] + jmp_offset[i]);
<span class="line-modified">!       if (!_matcher-&gt;is_short_branch_offset(jmp_rule[i], br_size, offset)) {</span>
          tty-&gt;print_cr(&quot;target (%d) - jmp_offset(%d) = offset (%d), jump_size(%d), jmp_block B%d, target_block B%d&quot;, blk_starts[jmp_target[i]], blk_starts[i] + jmp_offset[i], offset, br_size, i, jmp_target[i]);
          assert(false, &quot;Displacement too large for short jmp&quot;);
        }
      }
    }
  #endif
  
    BarrierSetC2* bs = BarrierSet::barrier_set()-&gt;barrier_set_c2();
    bs-&gt;emit_stubs(*cb);
<span class="line-modified">!   if (failing())  return;</span>
  
  #ifndef PRODUCT
    // Information on the size of the method, without the extraneous code
    Scheduling::increment_method_size(cb-&gt;insts_size());
  #endif
<span class="line-new-header">--- 1786,21 ---</span>
  #ifdef ASSERT
    for (uint i = 0; i &lt; nblocks; i++) { // For all blocks
      if (jmp_target[i] != 0) {
        int br_size = jmp_size[i];
        int offset = blk_starts[jmp_target[i]]-(blk_starts[i] + jmp_offset[i]);
<span class="line-modified">!       if (!C-&gt;matcher()-&gt;is_short_branch_offset(jmp_rule[i], br_size, offset)) {</span>
          tty-&gt;print_cr(&quot;target (%d) - jmp_offset(%d) = offset (%d), jump_size(%d), jmp_block B%d, target_block B%d&quot;, blk_starts[jmp_target[i]], blk_starts[i] + jmp_offset[i], offset, br_size, i, jmp_target[i]);
          assert(false, &quot;Displacement too large for short jmp&quot;);
        }
      }
    }
  #endif
  
    BarrierSetC2* bs = BarrierSet::barrier_set()-&gt;barrier_set_c2();
    bs-&gt;emit_stubs(*cb);
<span class="line-modified">!   if (C-&gt;failing())  return;</span>
  
  #ifndef PRODUCT
    // Information on the size of the method, without the extraneous code
    Scheduling::increment_method_size(cb-&gt;insts_size());
  #endif
</pre>
<hr />
<pre>
<span class="line-old-header">*** 1606,21 ***</span>
    // Fill in exception table entries.
    FillExceptionTables(inct_cnt, call_returns, inct_starts, blk_labels);
  
    // Only java methods have exception handlers and deopt handlers
    // class HandlerImpl is platform-specific and defined in the *.ad files.
<span class="line-modified">!   if (_method) {</span>
      // Emit the exception handler code.
      _code_offsets.set_value(CodeOffsets::Exceptions, HandlerImpl::emit_exception_handler(*cb));
<span class="line-modified">!     if (failing()) {</span>
        return; // CodeBuffer::expand failed
      }
      // Emit the deopt handler code.
      _code_offsets.set_value(CodeOffsets::Deopt, HandlerImpl::emit_deopt_handler(*cb));
  
      // Emit the MethodHandle deopt handler code (if required).
<span class="line-modified">!     if (has_method_handle_invokes() &amp;&amp; !failing()) {</span>
        // We can use the same code as for the normal deopt handler, we
        // just need a different entry point address.
        _code_offsets.set_value(CodeOffsets::DeoptMH, HandlerImpl::emit_deopt_handler(*cb));
      }
    }
<span class="line-new-header">--- 1809,21 ---</span>
    // Fill in exception table entries.
    FillExceptionTables(inct_cnt, call_returns, inct_starts, blk_labels);
  
    // Only java methods have exception handlers and deopt handlers
    // class HandlerImpl is platform-specific and defined in the *.ad files.
<span class="line-modified">!   if (C-&gt;method()) {</span>
      // Emit the exception handler code.
      _code_offsets.set_value(CodeOffsets::Exceptions, HandlerImpl::emit_exception_handler(*cb));
<span class="line-modified">!     if (C-&gt;failing()) {</span>
        return; // CodeBuffer::expand failed
      }
      // Emit the deopt handler code.
      _code_offsets.set_value(CodeOffsets::Deopt, HandlerImpl::emit_deopt_handler(*cb));
  
      // Emit the MethodHandle deopt handler code (if required).
<span class="line-modified">!     if (C-&gt;has_method_handle_invokes() &amp;&amp; !C-&gt;failing()) {</span>
        // We can use the same code as for the normal deopt handler, we
        // just need a different entry point address.
        _code_offsets.set_value(CodeOffsets::DeoptMH, HandlerImpl::emit_deopt_handler(*cb));
      }
    }
</pre>
<hr />
<pre>
<span class="line-old-header">*** 1630,37 ***</span>
      C-&gt;record_failure(&quot;CodeCache is full&quot;);
      return;
    }
  
  #if defined(SUPPORT_ABSTRACT_ASSEMBLY) || defined(SUPPORT_ASSEMBLY) || defined(SUPPORT_OPTO_ASSEMBLY)
<span class="line-modified">!   if (print_assembly()) {</span>
      tty-&gt;cr();
      tty-&gt;print_cr(&quot;============================= C2-compiled nmethod ==============================&quot;);
    }
  #endif
  
  #if defined(SUPPORT_OPTO_ASSEMBLY)
    // Dump the assembly code, including basic-block numbers
<span class="line-modified">!   if (print_assembly()) {</span>
      ttyLocker ttyl;  // keep the following output all in one block
      if (!VMThread::should_terminate()) {  // test this under the tty lock
        // This output goes directly to the tty, not the compiler log.
        // To enable tools to match it up with the compilation activity,
        // be sure to tag this tty output with the compile ID.
        if (xtty != NULL) {
<span class="line-modified">!         xtty-&gt;head(&quot;opto_assembly compile_id=&#39;%d&#39;%s&quot;, compile_id(),</span>
<span class="line-modified">!                    is_osr_compilation()    ? &quot; compile_kind=&#39;osr&#39;&quot; :</span>
                     &quot;&quot;);
        }
<span class="line-modified">!       if (method() != NULL) {</span>
<span class="line-modified">!         tty-&gt;print_cr(&quot;----------------------- MetaData before Compile_id = %d ------------------------&quot;, compile_id());</span>
<span class="line-modified">!         method()-&gt;print_metadata();</span>
<span class="line-modified">!       } else if (stub_name() != NULL) {</span>
<span class="line-modified">!         tty-&gt;print_cr(&quot;----------------------------- RuntimeStub %s -------------------------------&quot;, stub_name());</span>
        }
        tty-&gt;cr();
<span class="line-modified">!       tty-&gt;print_cr(&quot;------------------------ OptoAssembly for Compile_id = %d -----------------------&quot;, compile_id());</span>
        dump_asm(node_offsets, node_offset_limit);
        tty-&gt;print_cr(&quot;--------------------------------------------------------------------------------&quot;);
        if (xtty != NULL) {
          // print_metadata and dump_asm above may safepoint which makes us loose the ttylock.
          // Retake lock too make sure the end tag is coherent, and that xmlStream-&gt;pop_tag is done
<span class="line-new-header">--- 1833,37 ---</span>
      C-&gt;record_failure(&quot;CodeCache is full&quot;);
      return;
    }
  
  #if defined(SUPPORT_ABSTRACT_ASSEMBLY) || defined(SUPPORT_ASSEMBLY) || defined(SUPPORT_OPTO_ASSEMBLY)
<span class="line-modified">!   if (C-&gt;print_assembly()) {</span>
      tty-&gt;cr();
      tty-&gt;print_cr(&quot;============================= C2-compiled nmethod ==============================&quot;);
    }
  #endif
  
  #if defined(SUPPORT_OPTO_ASSEMBLY)
    // Dump the assembly code, including basic-block numbers
<span class="line-modified">!   if (C-&gt;print_assembly()) {</span>
      ttyLocker ttyl;  // keep the following output all in one block
      if (!VMThread::should_terminate()) {  // test this under the tty lock
        // This output goes directly to the tty, not the compiler log.
        // To enable tools to match it up with the compilation activity,
        // be sure to tag this tty output with the compile ID.
        if (xtty != NULL) {
<span class="line-modified">!         xtty-&gt;head(&quot;opto_assembly compile_id=&#39;%d&#39;%s&quot;, C-&gt;compile_id(),</span>
<span class="line-modified">!                    C-&gt;is_osr_compilation()    ? &quot; compile_kind=&#39;osr&#39;&quot; :</span>
                     &quot;&quot;);
        }
<span class="line-modified">!       if (C-&gt;method() != NULL) {</span>
<span class="line-modified">!         tty-&gt;print_cr(&quot;----------------------- MetaData before Compile_id = %d ------------------------&quot;, C-&gt;compile_id());</span>
<span class="line-modified">!         C-&gt;method()-&gt;print_metadata();</span>
<span class="line-modified">!       } else if (C-&gt;stub_name() != NULL) {</span>
<span class="line-modified">!         tty-&gt;print_cr(&quot;----------------------------- RuntimeStub %s -------------------------------&quot;, C-&gt;stub_name());</span>
        }
        tty-&gt;cr();
<span class="line-modified">!       tty-&gt;print_cr(&quot;------------------------ OptoAssembly for Compile_id = %d -----------------------&quot;, C-&gt;compile_id());</span>
        dump_asm(node_offsets, node_offset_limit);
        tty-&gt;print_cr(&quot;--------------------------------------------------------------------------------&quot;);
        if (xtty != NULL) {
          // print_metadata and dump_asm above may safepoint which makes us loose the ttylock.
          // Retake lock too make sure the end tag is coherent, and that xmlStream-&gt;pop_tag is done
</pre>
<hr />
<pre>
<span class="line-old-header">*** 1671,16 ***</span>
      }
    }
  #endif
  }
  
<span class="line-modified">! void Compile::FillExceptionTables(uint cnt, uint *call_returns, uint *inct_starts, Label *blk_labels) {</span>
    _inc_table.set_size(cnt);
  
    uint inct_cnt = 0;
<span class="line-modified">!   for (uint i = 0; i &lt; _cfg-&gt;number_of_blocks(); i++) {</span>
<span class="line-modified">!     Block* block = _cfg-&gt;get_block(i);</span>
      Node *n = NULL;
      int j;
  
      // Find the branch; ignore trailing NOPs.
      for (j = block-&gt;number_of_nodes() - 1; j &gt;= 0; j--) {
<span class="line-new-header">--- 1874,16 ---</span>
      }
    }
  #endif
  }
  
<span class="line-modified">! void PhaseOutput::FillExceptionTables(uint cnt, uint *call_returns, uint *inct_starts, Label *blk_labels) {</span>
    _inc_table.set_size(cnt);
  
    uint inct_cnt = 0;
<span class="line-modified">!   for (uint i = 0; i &lt; C-&gt;cfg()-&gt;number_of_blocks(); i++) {</span>
<span class="line-modified">!     Block* block = C-&gt;cfg()-&gt;get_block(i);</span>
      Node *n = NULL;
      int j;
  
      // Find the branch; ignore trailing NOPs.
      for (j = block-&gt;number_of_nodes() - 1; j &gt;= 0; j--) {
</pre>
<hr />
<pre>
<span class="line-old-header">*** 1721,11 ***</span>
              const CatchProjNode* p = pk-&gt;as_CatchProj();
              found_p = true;
              // add the corresponding handler bci &amp; pco information
              if (p-&gt;_con != CatchProjNode::fall_through_index) {
                // p leads to an exception handler (and is not fall through)
<span class="line-modified">!               assert(s == _cfg-&gt;get_block(s-&gt;_pre_order), &quot;bad numbering&quot;);</span>
                // no duplicates, please
                if (!handler_bcis.contains(p-&gt;handler_bci())) {
                  uint block_num = s-&gt;non_connector()-&gt;_pre_order;
                  handler_bcis.append(p-&gt;handler_bci());
                  handler_pcos.append(blk_labels[block_num].loc_pos());
<span class="line-new-header">--- 1924,11 ---</span>
              const CatchProjNode* p = pk-&gt;as_CatchProj();
              found_p = true;
              // add the corresponding handler bci &amp; pco information
              if (p-&gt;_con != CatchProjNode::fall_through_index) {
                // p leads to an exception handler (and is not fall through)
<span class="line-modified">!               assert(s == C-&gt;cfg()-&gt;get_block(s-&gt;_pre_order), &quot;bad numbering&quot;);</span>
                // no duplicates, please
                if (!handler_bcis.contains(p-&gt;handler_bci())) {
                  uint block_num = s-&gt;non_connector()-&gt;_pre_order;
                  handler_bcis.append(p-&gt;handler_bci());
                  handler_pcos.append(blk_labels[block_num].loc_pos());
</pre>
<hr />
<pre>
<span class="line-old-header">*** 1793,11 ***</span>
    // Now that the nops are in the array, save the count
    // (but allow entries for the nops)
    _node_bundling_limit = compile.unique();
    uint node_max = _regalloc-&gt;node_regs_max_index();
  
<span class="line-modified">!   compile.set_node_bundling_limit(_node_bundling_limit);</span>
  
    // This one is persistent within the Compile class
    _node_bundling_base = NEW_ARENA_ARRAY(compile.comp_arena(), Bundle, node_max);
  
    // Allocate space for fixed-size arrays
<span class="line-new-header">--- 1996,11 ---</span>
    // Now that the nops are in the array, save the count
    // (but allow entries for the nops)
    _node_bundling_limit = compile.unique();
    uint node_max = _regalloc-&gt;node_regs_max_index();
  
<span class="line-modified">!   compile.output()-&gt;set_node_bundling_limit(_node_bundling_limit);</span>
  
    // This one is persistent within the Compile class
    _node_bundling_base = NEW_ARENA_ARRAY(compile.comp_arena(), Bundle, node_max);
  
    // Allocate space for fixed-size arrays
</pre>
<hr />
<pre>
<span class="line-old-header">*** 1869,42 ***</span>
           sizeof(Pipeline_Use::elaborated_elements));
  }
  
  // Perform instruction scheduling and bundling over the sequence of
  // instructions in backwards order.
<span class="line-modified">! void Compile::ScheduleAndBundle() {</span>
  
    // Don&#39;t optimize this if it isn&#39;t a method
<span class="line-modified">!   if (!_method)</span>
      return;
  
    // Don&#39;t optimize this if scheduling is disabled
<span class="line-modified">!   if (!do_scheduling())</span>
      return;
  
    // Scheduling code works only with pairs (16 bytes) maximum.
<span class="line-modified">!   if (max_vector_size() &gt; 16)</span>
      return;
  
<span class="line-modified">!   TracePhase tp(&quot;isched&quot;, &amp;timers[_t_instrSched]);</span>
  
    // Create a data structure for all the scheduling information
<span class="line-modified">!   Scheduling scheduling(Thread::current()-&gt;resource_area(), *this);</span>
  
    // Walk backwards over each basic block, computing the needed alignment
    // Walk over all the basic blocks
    scheduling.DoScheduling();
  
  #ifndef PRODUCT
<span class="line-modified">!   if (trace_opto_output()) {</span>
      tty-&gt;print(&quot;\n---- After ScheduleAndBundle ----\n&quot;);
<span class="line-modified">!     for (uint i = 0; i &lt; _cfg-&gt;number_of_blocks(); i++) {</span>
        tty-&gt;print(&quot;\nBB#%03d:\n&quot;, i);
<span class="line-modified">!       Block* block = _cfg-&gt;get_block(i);</span>
        for (uint j = 0; j &lt; block-&gt;number_of_nodes(); j++) {
          Node* n = block-&gt;get_node(j);
<span class="line-modified">!         OptoReg::Name reg = _regalloc-&gt;get_reg_first(n);</span>
          tty-&gt;print(&quot; %-6s &quot;, reg &gt;= 0 &amp;&amp; reg &lt; REG_COUNT ? Matcher::regName[reg] : &quot;&quot;);
          n-&gt;dump();
        }
      }
    }
<span class="line-new-header">--- 2072,42 ---</span>
           sizeof(Pipeline_Use::elaborated_elements));
  }
  
  // Perform instruction scheduling and bundling over the sequence of
  // instructions in backwards order.
<span class="line-modified">! void PhaseOutput::ScheduleAndBundle() {</span>
  
    // Don&#39;t optimize this if it isn&#39;t a method
<span class="line-modified">!   if (!C-&gt;method())</span>
      return;
  
    // Don&#39;t optimize this if scheduling is disabled
<span class="line-modified">!   if (!C-&gt;do_scheduling())</span>
      return;
  
    // Scheduling code works only with pairs (16 bytes) maximum.
<span class="line-modified">!   if (C-&gt;max_vector_size() &gt; 16)</span>
      return;
  
<span class="line-modified">!   Compile::TracePhase tp(&quot;isched&quot;, &amp;timers[_t_instrSched]);</span>
  
    // Create a data structure for all the scheduling information
<span class="line-modified">!   Scheduling scheduling(Thread::current()-&gt;resource_area(), *C);</span>
  
    // Walk backwards over each basic block, computing the needed alignment
    // Walk over all the basic blocks
    scheduling.DoScheduling();
  
  #ifndef PRODUCT
<span class="line-modified">!   if (C-&gt;trace_opto_output()) {</span>
      tty-&gt;print(&quot;\n---- After ScheduleAndBundle ----\n&quot;);
<span class="line-modified">!     for (uint i = 0; i &lt; C-&gt;cfg()-&gt;number_of_blocks(); i++) {</span>
        tty-&gt;print(&quot;\nBB#%03d:\n&quot;, i);
<span class="line-modified">!       Block* block = C-&gt;cfg()-&gt;get_block(i);</span>
        for (uint j = 0; j &lt; block-&gt;number_of_nodes(); j++) {
          Node* n = block-&gt;get_node(j);
<span class="line-modified">!         OptoReg::Name reg = C-&gt;regalloc()-&gt;get_reg_first(n);</span>
          tty-&gt;print(&quot; %-6s &quot;, reg &gt;= 0 &amp;&amp; reg &lt; REG_COUNT ? Matcher::regName[reg] : &quot;&quot;);
          n-&gt;dump();
        }
      }
    }
</pre>
<hr />
<pre>
<span class="line-old-header">*** 2442,10 ***</span>
<span class="line-new-header">--- 2645,11 ---</span>
      tty-&gt;print(&quot;# -&gt; DoScheduling\n&quot;);
  #endif
  
    Block *succ_bb = NULL;
    Block *bb;
<span class="line-added">+   Compile* C = Compile::current();</span>
  
    // Walk over all the basic blocks in reverse order
    for (int i = _cfg-&gt;number_of_blocks() - 1; i &gt;= 0; succ_bb = bb, i--) {
      bb = _cfg-&gt;get_block(i);
  
</pre>
<hr />
<pre>
<span class="line-old-header">*** 2529,11 ***</span>
  
      assert( _bb_start &lt;= _bb_end, &quot;inverted block ends&quot; );
  
      // Compute the register antidependencies for the basic block
      ComputeRegisterAntidependencies(bb);
<span class="line-modified">!     if (_cfg-&gt;C-&gt;failing())  return;  // too many D-U pinch points</span>
  
      // Compute intra-bb latencies for the nodes
      ComputeLocalLatenciesForward(bb);
  
      // Compute the usage within the block, and set the list of all nodes
<span class="line-new-header">--- 2733,11 ---</span>
  
      assert( _bb_start &lt;= _bb_end, &quot;inverted block ends&quot; );
  
      // Compute the register antidependencies for the basic block
      ComputeRegisterAntidependencies(bb);
<span class="line-modified">!     if (C-&gt;failing())  return;  // too many D-U pinch points</span>
  
      // Compute intra-bb latencies for the nodes
      ComputeLocalLatenciesForward(bb);
  
      // Compute the usage within the block, and set the list of all nodes
</pre>
<hr />
<pre>
<span class="line-old-header">*** 2589,11 ***</span>
    if (_cfg-&gt;C-&gt;trace_opto_output())
      tty-&gt;print(&quot;# &lt;- DoScheduling\n&quot;);
  #endif
  
    // Record final node-bundling array location
<span class="line-modified">!   _regalloc-&gt;C-&gt;set_node_bundling_base(_node_bundling_base);</span>
  
  } // end DoScheduling
  
  // Verify that no live-range used in the block is killed in the block by a
  // wrong DEF.  This doesn&#39;t verify live-ranges that span blocks.
<span class="line-new-header">--- 2793,11 ---</span>
    if (_cfg-&gt;C-&gt;trace_opto_output())
      tty-&gt;print(&quot;# &lt;- DoScheduling\n&quot;);
  #endif
  
    // Record final node-bundling array location
<span class="line-modified">!   _regalloc-&gt;C-&gt;output()-&gt;set_node_bundling_base(_node_bundling_base);</span>
  
  } // end DoScheduling
  
  // Verify that no live-range used in the block is killed in the block by a
  // wrong DEF.  This doesn&#39;t verify live-ranges that span blocks.
</pre>
<hr />
<pre>
<span class="line-old-header">*** 2696,10 ***</span>
<span class="line-new-header">--- 2900,12 ---</span>
    debug_only( def = (Node*)((intptr_t)0xdeadbeef); )
  
    // After some number of kills there _may_ be a later def
    Node *later_def = NULL;
  
<span class="line-added">+   Compile* C = Compile::current();</span>
<span class="line-added">+ </span>
    // Finding a kill requires a real pinch-point.
    // Check for not already having a pinch-point.
    // Pinch points are Op_Node&#39;s.
    if( pinch-&gt;Opcode() != Op_Node ) { // Or later-def/kill as pinch-point?
      later_def = pinch;            // Must be def/kill as optimistic pinch-point
</pre>
<hr />
<pre>
<span class="line-old-header">*** 2712,13 ***</span>
        _cfg-&gt;C-&gt;record_method_not_compilable(&quot;too many D-U pinch points&quot;);
        return;
      }
      _cfg-&gt;map_node_to_block(pinch, b);      // Pretend it&#39;s valid in this block (lazy init)
      _reg_node.map(def_reg,pinch); // Record pinch-point
<span class="line-modified">!     //_regalloc-&gt;set_bad(pinch-&gt;_idx); // Already initialized this way.</span>
      if( later_def-&gt;outcnt() == 0 || later_def-&gt;ideal_reg() == MachProjNode::fat_proj ) { // Distinguish def from kill
<span class="line-modified">!       pinch-&gt;init_req(0, _cfg-&gt;C-&gt;top());     // set not NULL for the next call</span>
        add_prec_edge_from_to(later_def,pinch); // Add edge from kill to pinch
        later_def = NULL;           // and no later def
      }
      pinch-&gt;set_req(0,later_def);  // Hook later def so we can find it
    } else {                        // Else have valid pinch point
<span class="line-new-header">--- 2918,13 ---</span>
        _cfg-&gt;C-&gt;record_method_not_compilable(&quot;too many D-U pinch points&quot;);
        return;
      }
      _cfg-&gt;map_node_to_block(pinch, b);      // Pretend it&#39;s valid in this block (lazy init)
      _reg_node.map(def_reg,pinch); // Record pinch-point
<span class="line-modified">!     //regalloc()-&gt;set_bad(pinch-&gt;_idx); // Already initialized this way.</span>
      if( later_def-&gt;outcnt() == 0 || later_def-&gt;ideal_reg() == MachProjNode::fat_proj ) { // Distinguish def from kill
<span class="line-modified">!       pinch-&gt;init_req(0, C-&gt;top());     // set not NULL for the next call</span>
        add_prec_edge_from_to(later_def,pinch); // Add edge from kill to pinch
        later_def = NULL;           // and no later def
      }
      pinch-&gt;set_req(0,later_def);  // Hook later def so we can find it
    } else {                        // Else have valid pinch point
</pre>
<hr />
<pre>
<span class="line-old-header">*** 3017,5 ***</span>
<span class="line-new-header">--- 3223,383 ---</span>
    if (total_bundles &gt; 0)
      tty-&gt;print(&quot;Average ILP (excluding nops) is %.2f\n&quot;,
                 ((double)total_instructions) / ((double)total_bundles));
  }
  #endif
<span class="line-added">+ </span>
<span class="line-added">+ //-----------------------init_scratch_buffer_blob------------------------------</span>
<span class="line-added">+ // Construct a temporary BufferBlob and cache it for this compile.</span>
<span class="line-added">+ void PhaseOutput::init_scratch_buffer_blob(int const_size) {</span>
<span class="line-added">+   // If there is already a scratch buffer blob allocated and the</span>
<span class="line-added">+   // constant section is big enough, use it.  Otherwise free the</span>
<span class="line-added">+   // current and allocate a new one.</span>
<span class="line-added">+   BufferBlob* blob = scratch_buffer_blob();</span>
<span class="line-added">+   if ((blob != NULL) &amp;&amp; (const_size &lt;= _scratch_const_size)) {</span>
<span class="line-added">+     // Use the current blob.</span>
<span class="line-added">+   } else {</span>
<span class="line-added">+     if (blob != NULL) {</span>
<span class="line-added">+       BufferBlob::free(blob);</span>
<span class="line-added">+     }</span>
<span class="line-added">+ </span>
<span class="line-added">+     ResourceMark rm;</span>
<span class="line-added">+     _scratch_const_size = const_size;</span>
<span class="line-added">+     int size = C2Compiler::initial_code_buffer_size(const_size);</span>
<span class="line-added">+ #ifdef ASSERT</span>
<span class="line-added">+     if (C-&gt;has_scalarized_args()) {</span>
<span class="line-added">+       // Oop verification for loading object fields from scalarized value types in the new entry point requires lots of space</span>
<span class="line-added">+       size += 5120;</span>
<span class="line-added">+     }</span>
<span class="line-added">+ #endif</span>
<span class="line-added">+     blob = BufferBlob::create(&quot;Compile::scratch_buffer&quot;, size);</span>
<span class="line-added">+     // Record the buffer blob for next time.</span>
<span class="line-added">+     set_scratch_buffer_blob(blob);</span>
<span class="line-added">+     // Have we run out of code space?</span>
<span class="line-added">+     if (scratch_buffer_blob() == NULL) {</span>
<span class="line-added">+       // Let CompilerBroker disable further compilations.</span>
<span class="line-added">+       C-&gt;record_failure(&quot;Not enough space for scratch buffer in CodeCache&quot;);</span>
<span class="line-added">+       return;</span>
<span class="line-added">+     }</span>
<span class="line-added">+   }</span>
<span class="line-added">+ </span>
<span class="line-added">+   // Initialize the relocation buffers</span>
<span class="line-added">+   relocInfo* locs_buf = (relocInfo*) blob-&gt;content_end() - MAX_locs_size;</span>
<span class="line-added">+   set_scratch_locs_memory(locs_buf);</span>
<span class="line-added">+ }</span>
<span class="line-added">+ </span>
<span class="line-added">+ </span>
<span class="line-added">+ //-----------------------scratch_emit_size-------------------------------------</span>
<span class="line-added">+ // Helper function that computes size by emitting code</span>
<span class="line-added">+ uint PhaseOutput::scratch_emit_size(const Node* n) {</span>
<span class="line-added">+   // Start scratch_emit_size section.</span>
<span class="line-added">+   set_in_scratch_emit_size(true);</span>
<span class="line-added">+ </span>
<span class="line-added">+   // Emit into a trash buffer and count bytes emitted.</span>
<span class="line-added">+   // This is a pretty expensive way to compute a size,</span>
<span class="line-added">+   // but it works well enough if seldom used.</span>
<span class="line-added">+   // All common fixed-size instructions are given a size</span>
<span class="line-added">+   // method by the AD file.</span>
<span class="line-added">+   // Note that the scratch buffer blob and locs memory are</span>
<span class="line-added">+   // allocated at the beginning of the compile task, and</span>
<span class="line-added">+   // may be shared by several calls to scratch_emit_size.</span>
<span class="line-added">+   // The allocation of the scratch buffer blob is particularly</span>
<span class="line-added">+   // expensive, since it has to grab the code cache lock.</span>
<span class="line-added">+   BufferBlob* blob = this-&gt;scratch_buffer_blob();</span>
<span class="line-added">+   assert(blob != NULL, &quot;Initialize BufferBlob at start&quot;);</span>
<span class="line-added">+   assert(blob-&gt;size() &gt; MAX_inst_size, &quot;sanity&quot;);</span>
<span class="line-added">+   relocInfo* locs_buf = scratch_locs_memory();</span>
<span class="line-added">+   address blob_begin = blob-&gt;content_begin();</span>
<span class="line-added">+   address blob_end   = (address)locs_buf;</span>
<span class="line-added">+   assert(blob-&gt;contains(blob_end), &quot;sanity&quot;);</span>
<span class="line-added">+   CodeBuffer buf(blob_begin, blob_end - blob_begin);</span>
<span class="line-added">+   buf.initialize_consts_size(_scratch_const_size);</span>
<span class="line-added">+   buf.initialize_stubs_size(MAX_stubs_size);</span>
<span class="line-added">+   assert(locs_buf != NULL, &quot;sanity&quot;);</span>
<span class="line-added">+   int lsize = MAX_locs_size / 3;</span>
<span class="line-added">+   buf.consts()-&gt;initialize_shared_locs(&amp;locs_buf[lsize * 0], lsize);</span>
<span class="line-added">+   buf.insts()-&gt;initialize_shared_locs( &amp;locs_buf[lsize * 1], lsize);</span>
<span class="line-added">+   buf.stubs()-&gt;initialize_shared_locs( &amp;locs_buf[lsize * 2], lsize);</span>
<span class="line-added">+   // Mark as scratch buffer.</span>
<span class="line-added">+   buf.consts()-&gt;set_scratch_emit();</span>
<span class="line-added">+   buf.insts()-&gt;set_scratch_emit();</span>
<span class="line-added">+   buf.stubs()-&gt;set_scratch_emit();</span>
<span class="line-added">+ </span>
<span class="line-added">+   // Do the emission.</span>
<span class="line-added">+ </span>
<span class="line-added">+   Label fakeL; // Fake label for branch instructions.</span>
<span class="line-added">+   Label*   saveL = NULL;</span>
<span class="line-added">+   uint save_bnum = 0;</span>
<span class="line-added">+   bool is_branch = n-&gt;is_MachBranch();</span>
<span class="line-added">+   if (is_branch) {</span>
<span class="line-added">+     MacroAssembler masm(&amp;buf);</span>
<span class="line-added">+     masm.bind(fakeL);</span>
<span class="line-added">+     n-&gt;as_MachBranch()-&gt;save_label(&amp;saveL, &amp;save_bnum);</span>
<span class="line-added">+     n-&gt;as_MachBranch()-&gt;label_set(&amp;fakeL, 0);</span>
<span class="line-added">+   } else if (n-&gt;is_MachProlog()) {</span>
<span class="line-added">+     saveL = ((MachPrologNode*)n)-&gt;_verified_entry;</span>
<span class="line-added">+     ((MachPrologNode*)n)-&gt;_verified_entry = &amp;fakeL;</span>
<span class="line-added">+   } else if (n-&gt;is_MachVEP()) {</span>
<span class="line-added">+     saveL = ((MachVEPNode*)n)-&gt;_verified_entry;</span>
<span class="line-added">+     ((MachVEPNode*)n)-&gt;_verified_entry = &amp;fakeL;</span>
<span class="line-added">+   }</span>
<span class="line-added">+   n-&gt;emit(buf, C-&gt;regalloc());</span>
<span class="line-added">+ </span>
<span class="line-added">+   // Emitting into the scratch buffer should not fail</span>
<span class="line-added">+   assert (!C-&gt;failing(), &quot;Must not have pending failure. Reason is: %s&quot;, C-&gt;failure_reason());</span>
<span class="line-added">+ </span>
<span class="line-added">+   // Restore label.</span>
<span class="line-added">+   if (is_branch) {</span>
<span class="line-added">+     n-&gt;as_MachBranch()-&gt;label_set(saveL, save_bnum);</span>
<span class="line-added">+   } else if (n-&gt;is_MachProlog()) {</span>
<span class="line-added">+     ((MachPrologNode*)n)-&gt;_verified_entry = saveL;</span>
<span class="line-added">+   } else if (n-&gt;is_MachVEP()) {</span>
<span class="line-added">+     ((MachVEPNode*)n)-&gt;_verified_entry = saveL;</span>
<span class="line-added">+   }</span>
<span class="line-added">+ </span>
<span class="line-added">+   // End scratch_emit_size section.</span>
<span class="line-added">+   set_in_scratch_emit_size(false);</span>
<span class="line-added">+ </span>
<span class="line-added">+   return buf.insts_size();</span>
<span class="line-added">+ }</span>
<span class="line-added">+ </span>
<span class="line-added">+ void PhaseOutput::install() {</span>
<span class="line-added">+   if (C-&gt;stub_function() != NULL) {</span>
<span class="line-added">+     install_stub(C-&gt;stub_name(),</span>
<span class="line-added">+                  C-&gt;save_argument_registers());</span>
<span class="line-added">+   } else {</span>
<span class="line-added">+     install_code(C-&gt;method(),</span>
<span class="line-added">+                  C-&gt;entry_bci(),</span>
<span class="line-added">+                  CompileBroker::compiler2(),</span>
<span class="line-added">+                  C-&gt;has_unsafe_access(),</span>
<span class="line-added">+                  SharedRuntime::is_wide_vector(C-&gt;max_vector_size()),</span>
<span class="line-added">+                  C-&gt;rtm_state());</span>
<span class="line-added">+   }</span>
<span class="line-added">+ }</span>
<span class="line-added">+ </span>
<span class="line-added">+ void PhaseOutput::install_code(ciMethod*         target,</span>
<span class="line-added">+                                int               entry_bci,</span>
<span class="line-added">+                                AbstractCompiler* compiler,</span>
<span class="line-added">+                                bool              has_unsafe_access,</span>
<span class="line-added">+                                bool              has_wide_vectors,</span>
<span class="line-added">+                                RTMState          rtm_state) {</span>
<span class="line-added">+   // Check if we want to skip execution of all compiled code.</span>
<span class="line-added">+   {</span>
<span class="line-added">+ #ifndef PRODUCT</span>
<span class="line-added">+     if (OptoNoExecute) {</span>
<span class="line-added">+       C-&gt;record_method_not_compilable(&quot;+OptoNoExecute&quot;);  // Flag as failed</span>
<span class="line-added">+       return;</span>
<span class="line-added">+     }</span>
<span class="line-added">+ #endif</span>
<span class="line-added">+     Compile::TracePhase tp(&quot;install_code&quot;, &amp;timers[_t_registerMethod]);</span>
<span class="line-added">+ </span>
<span class="line-added">+     if (C-&gt;is_osr_compilation()) {</span>
<span class="line-added">+       _code_offsets.set_value(CodeOffsets::Verified_Entry, 0);</span>
<span class="line-added">+       _code_offsets.set_value(CodeOffsets::OSR_Entry, _first_block_size);</span>
<span class="line-added">+     } else {</span>
<span class="line-added">+       _code_offsets.set_value(CodeOffsets::Verified_Entry, _first_block_size);</span>
<span class="line-added">+       if (_code_offsets.value(CodeOffsets::Verified_Value_Entry) == -1) {</span>
<span class="line-added">+         _code_offsets.set_value(CodeOffsets::Verified_Value_Entry, _first_block_size);</span>
<span class="line-added">+       }</span>
<span class="line-added">+       if (_code_offsets.value(CodeOffsets::Verified_Value_Entry_RO) == -1) {</span>
<span class="line-added">+         _code_offsets.set_value(CodeOffsets::Verified_Value_Entry_RO, _first_block_size);</span>
<span class="line-added">+       }</span>
<span class="line-added">+       if (_code_offsets.value(CodeOffsets::Entry) == -1) {</span>
<span class="line-added">+         _code_offsets.set_value(CodeOffsets::Entry, _first_block_size);</span>
<span class="line-added">+       }</span>
<span class="line-added">+       _code_offsets.set_value(CodeOffsets::OSR_Entry, 0);</span>
<span class="line-added">+     }</span>
<span class="line-added">+ </span>
<span class="line-added">+     C-&gt;env()-&gt;register_method(target,</span>
<span class="line-added">+                               entry_bci,</span>
<span class="line-added">+                               &amp;_code_offsets,</span>
<span class="line-added">+                               _orig_pc_slot_offset_in_bytes,</span>
<span class="line-added">+                               code_buffer(),</span>
<span class="line-added">+                               frame_size_in_words(),</span>
<span class="line-added">+                               _oop_map_set,</span>
<span class="line-added">+                               &amp;_handler_table,</span>
<span class="line-added">+                               &amp;_inc_table,</span>
<span class="line-added">+                               compiler,</span>
<span class="line-added">+                               has_unsafe_access,</span>
<span class="line-added">+                               SharedRuntime::is_wide_vector(C-&gt;max_vector_size()),</span>
<span class="line-added">+                               C-&gt;rtm_state());</span>
<span class="line-added">+ </span>
<span class="line-added">+     if (C-&gt;log() != NULL) { // Print code cache state into compiler log</span>
<span class="line-added">+       C-&gt;log()-&gt;code_cache_state();</span>
<span class="line-added">+     }</span>
<span class="line-added">+   }</span>
<span class="line-added">+ }</span>
<span class="line-added">+ void PhaseOutput::install_stub(const char* stub_name,</span>
<span class="line-added">+                                bool        caller_must_gc_arguments) {</span>
<span class="line-added">+   // Entry point will be accessed using stub_entry_point();</span>
<span class="line-added">+   if (code_buffer() == NULL) {</span>
<span class="line-added">+     Matcher::soft_match_failure();</span>
<span class="line-added">+   } else {</span>
<span class="line-added">+     if (PrintAssembly &amp;&amp; (WizardMode || Verbose))</span>
<span class="line-added">+       tty-&gt;print_cr(&quot;### Stub::%s&quot;, stub_name);</span>
<span class="line-added">+ </span>
<span class="line-added">+     if (!C-&gt;failing()) {</span>
<span class="line-added">+       assert(C-&gt;fixed_slots() == 0, &quot;no fixed slots used for runtime stubs&quot;);</span>
<span class="line-added">+ </span>
<span class="line-added">+       // Make the NMethod</span>
<span class="line-added">+       // For now we mark the frame as never safe for profile stackwalking</span>
<span class="line-added">+       RuntimeStub *rs = RuntimeStub::new_runtime_stub(stub_name,</span>
<span class="line-added">+                                                       code_buffer(),</span>
<span class="line-added">+                                                       CodeOffsets::frame_never_safe,</span>
<span class="line-added">+                                                       // _code_offsets.value(CodeOffsets::Frame_Complete),</span>
<span class="line-added">+                                                       frame_size_in_words(),</span>
<span class="line-added">+                                                       oop_map_set(),</span>
<span class="line-added">+                                                       caller_must_gc_arguments);</span>
<span class="line-added">+       assert(rs != NULL &amp;&amp; rs-&gt;is_runtime_stub(), &quot;sanity check&quot;);</span>
<span class="line-added">+ </span>
<span class="line-added">+       C-&gt;set_stub_entry_point(rs-&gt;entry_point());</span>
<span class="line-added">+     }</span>
<span class="line-added">+   }</span>
<span class="line-added">+ }</span>
<span class="line-added">+ </span>
<span class="line-added">+ // Support for bundling info</span>
<span class="line-added">+ Bundle* PhaseOutput::node_bundling(const Node *n) {</span>
<span class="line-added">+   assert(valid_bundle_info(n), &quot;oob&quot;);</span>
<span class="line-added">+   return &amp;_node_bundling_base[n-&gt;_idx];</span>
<span class="line-added">+ }</span>
<span class="line-added">+ </span>
<span class="line-added">+ bool PhaseOutput::valid_bundle_info(const Node *n) {</span>
<span class="line-added">+   return (_node_bundling_limit &gt; n-&gt;_idx);</span>
<span class="line-added">+ }</span>
<span class="line-added">+ </span>
<span class="line-added">+ //------------------------------frame_size_in_words-----------------------------</span>
<span class="line-added">+ // frame_slots in units of words</span>
<span class="line-added">+ int PhaseOutput::frame_size_in_words() const {</span>
<span class="line-added">+   // shift is 0 in LP32 and 1 in LP64</span>
<span class="line-added">+   const int shift = (LogBytesPerWord - LogBytesPerInt);</span>
<span class="line-added">+   int words = _frame_slots &gt;&gt; shift;</span>
<span class="line-added">+   assert( words &lt;&lt; shift == _frame_slots, &quot;frame size must be properly aligned in LP64&quot; );</span>
<span class="line-added">+   return words;</span>
<span class="line-added">+ }</span>
<span class="line-added">+ </span>
<span class="line-added">+ // To bang the stack of this compiled method we use the stack size</span>
<span class="line-added">+ // that the interpreter would need in case of a deoptimization. This</span>
<span class="line-added">+ // removes the need to bang the stack in the deoptimization blob which</span>
<span class="line-added">+ // in turn simplifies stack overflow handling.</span>
<span class="line-added">+ int PhaseOutput::bang_size_in_bytes() const {</span>
<span class="line-added">+   return MAX2(frame_size_in_bytes() + os::extra_bang_size_in_bytes(), C-&gt;interpreter_frame_size());</span>
<span class="line-added">+ }</span>
<span class="line-added">+ </span>
<span class="line-added">+ //------------------------------dump_asm---------------------------------------</span>
<span class="line-added">+ // Dump formatted assembly</span>
<span class="line-added">+ #if defined(SUPPORT_OPTO_ASSEMBLY)</span>
<span class="line-added">+ void PhaseOutput::dump_asm_on(outputStream* st, int* pcs, uint pc_limit) {</span>
<span class="line-added">+ </span>
<span class="line-added">+   int pc_digits = 3; // #chars required for pc</span>
<span class="line-added">+   int sb_chars  = 3; // #chars for &quot;start bundle&quot; indicator</span>
<span class="line-added">+   int tab_size  = 8;</span>
<span class="line-added">+   if (pcs != NULL) {</span>
<span class="line-added">+     int max_pc = 0;</span>
<span class="line-added">+     for (uint i = 0; i &lt; pc_limit; i++) {</span>
<span class="line-added">+       max_pc = (max_pc &lt; pcs[i]) ? pcs[i] : max_pc;</span>
<span class="line-added">+     }</span>
<span class="line-added">+     pc_digits  = ((max_pc &lt; 4096) ? 3 : ((max_pc &lt; 65536) ? 4 : ((max_pc &lt; 65536*256) ? 6 : 8))); // #chars required for pc</span>
<span class="line-added">+   }</span>
<span class="line-added">+   int prefix_len = ((pc_digits + sb_chars + tab_size - 1)/tab_size)*tab_size;</span>
<span class="line-added">+ </span>
<span class="line-added">+   bool cut_short = false;</span>
<span class="line-added">+   st-&gt;print_cr(&quot;#&quot;);</span>
<span class="line-added">+   st-&gt;print(&quot;#  &quot;);  C-&gt;tf()-&gt;dump_on(st);  st-&gt;cr();</span>
<span class="line-added">+   st-&gt;print_cr(&quot;#&quot;);</span>
<span class="line-added">+ </span>
<span class="line-added">+   // For all blocks</span>
<span class="line-added">+   int pc = 0x0;                 // Program counter</span>
<span class="line-added">+   char starts_bundle = &#39; &#39;;</span>
<span class="line-added">+   C-&gt;regalloc()-&gt;dump_frame();</span>
<span class="line-added">+ </span>
<span class="line-added">+   Node *n = NULL;</span>
<span class="line-added">+   for (uint i = 0; i &lt; C-&gt;cfg()-&gt;number_of_blocks(); i++) {</span>
<span class="line-added">+     if (VMThread::should_terminate()) {</span>
<span class="line-added">+       cut_short = true;</span>
<span class="line-added">+       break;</span>
<span class="line-added">+     }</span>
<span class="line-added">+     Block* block = C-&gt;cfg()-&gt;get_block(i);</span>
<span class="line-added">+     if (block-&gt;is_connector() &amp;&amp; !Verbose) {</span>
<span class="line-added">+       continue;</span>
<span class="line-added">+     }</span>
<span class="line-added">+     n = block-&gt;head();</span>
<span class="line-added">+     if ((pcs != NULL) &amp;&amp; (n-&gt;_idx &lt; pc_limit)) {</span>
<span class="line-added">+       pc = pcs[n-&gt;_idx];</span>
<span class="line-added">+       st-&gt;print(&quot;%*.*x&quot;, pc_digits, pc_digits, pc);</span>
<span class="line-added">+     }</span>
<span class="line-added">+     st-&gt;fill_to(prefix_len);</span>
<span class="line-added">+     block-&gt;dump_head(C-&gt;cfg(), st);</span>
<span class="line-added">+     if (block-&gt;is_connector()) {</span>
<span class="line-added">+       st-&gt;fill_to(prefix_len);</span>
<span class="line-added">+       st-&gt;print_cr(&quot;# Empty connector block&quot;);</span>
<span class="line-added">+     } else if (block-&gt;num_preds() == 2 &amp;&amp; block-&gt;pred(1)-&gt;is_CatchProj() &amp;&amp; block-&gt;pred(1)-&gt;as_CatchProj()-&gt;_con == CatchProjNode::fall_through_index) {</span>
<span class="line-added">+       st-&gt;fill_to(prefix_len);</span>
<span class="line-added">+       st-&gt;print_cr(&quot;# Block is sole successor of call&quot;);</span>
<span class="line-added">+     }</span>
<span class="line-added">+ </span>
<span class="line-added">+     // For all instructions</span>
<span class="line-added">+     Node *delay = NULL;</span>
<span class="line-added">+     for (uint j = 0; j &lt; block-&gt;number_of_nodes(); j++) {</span>
<span class="line-added">+       if (VMThread::should_terminate()) {</span>
<span class="line-added">+         cut_short = true;</span>
<span class="line-added">+         break;</span>
<span class="line-added">+       }</span>
<span class="line-added">+       n = block-&gt;get_node(j);</span>
<span class="line-added">+       if (valid_bundle_info(n)) {</span>
<span class="line-added">+         Bundle* bundle = node_bundling(n);</span>
<span class="line-added">+         if (bundle-&gt;used_in_unconditional_delay()) {</span>
<span class="line-added">+           delay = n;</span>
<span class="line-added">+           continue;</span>
<span class="line-added">+         }</span>
<span class="line-added">+         if (bundle-&gt;starts_bundle()) {</span>
<span class="line-added">+           starts_bundle = &#39;+&#39;;</span>
<span class="line-added">+         }</span>
<span class="line-added">+       }</span>
<span class="line-added">+ </span>
<span class="line-added">+       if (WizardMode) {</span>
<span class="line-added">+         n-&gt;dump();</span>
<span class="line-added">+       }</span>
<span class="line-added">+ </span>
<span class="line-added">+       if( !n-&gt;is_Region() &amp;&amp;    // Dont print in the Assembly</span>
<span class="line-added">+           !n-&gt;is_Phi() &amp;&amp;       // a few noisely useless nodes</span>
<span class="line-added">+           !n-&gt;is_Proj() &amp;&amp;</span>
<span class="line-added">+           !n-&gt;is_MachTemp() &amp;&amp;</span>
<span class="line-added">+           !n-&gt;is_SafePointScalarObject() &amp;&amp;</span>
<span class="line-added">+           !n-&gt;is_Catch() &amp;&amp;     // Would be nice to print exception table targets</span>
<span class="line-added">+           !n-&gt;is_MergeMem() &amp;&amp;  // Not very interesting</span>
<span class="line-added">+           !n-&gt;is_top() &amp;&amp;       // Debug info table constants</span>
<span class="line-added">+           !(n-&gt;is_Con() &amp;&amp; !n-&gt;is_Mach())// Debug info table constants</span>
<span class="line-added">+           ) {</span>
<span class="line-added">+         if ((pcs != NULL) &amp;&amp; (n-&gt;_idx &lt; pc_limit)) {</span>
<span class="line-added">+           pc = pcs[n-&gt;_idx];</span>
<span class="line-added">+           st-&gt;print(&quot;%*.*x&quot;, pc_digits, pc_digits, pc);</span>
<span class="line-added">+         } else {</span>
<span class="line-added">+           st-&gt;fill_to(pc_digits);</span>
<span class="line-added">+         }</span>
<span class="line-added">+         st-&gt;print(&quot; %c &quot;, starts_bundle);</span>
<span class="line-added">+         starts_bundle = &#39; &#39;;</span>
<span class="line-added">+         st-&gt;fill_to(prefix_len);</span>
<span class="line-added">+         n-&gt;format(C-&gt;regalloc(), st);</span>
<span class="line-added">+         st-&gt;cr();</span>
<span class="line-added">+       }</span>
<span class="line-added">+ </span>
<span class="line-added">+       // If we have an instruction with a delay slot, and have seen a delay,</span>
<span class="line-added">+       // then back up and print it</span>
<span class="line-added">+       if (valid_bundle_info(n) &amp;&amp; node_bundling(n)-&gt;use_unconditional_delay()) {</span>
<span class="line-added">+         // Coverity finding - Explicit null dereferenced.</span>
<span class="line-added">+         guarantee(delay != NULL, &quot;no unconditional delay instruction&quot;);</span>
<span class="line-added">+         if (WizardMode) delay-&gt;dump();</span>
<span class="line-added">+ </span>
<span class="line-added">+         if (node_bundling(delay)-&gt;starts_bundle())</span>
<span class="line-added">+           starts_bundle = &#39;+&#39;;</span>
<span class="line-added">+         if ((pcs != NULL) &amp;&amp; (n-&gt;_idx &lt; pc_limit)) {</span>
<span class="line-added">+           pc = pcs[n-&gt;_idx];</span>
<span class="line-added">+           st-&gt;print(&quot;%*.*x&quot;, pc_digits, pc_digits, pc);</span>
<span class="line-added">+         } else {</span>
<span class="line-added">+           st-&gt;fill_to(pc_digits);</span>
<span class="line-added">+         }</span>
<span class="line-added">+         st-&gt;print(&quot; %c &quot;, starts_bundle);</span>
<span class="line-added">+         starts_bundle = &#39; &#39;;</span>
<span class="line-added">+         st-&gt;fill_to(prefix_len);</span>
<span class="line-added">+         delay-&gt;format(C-&gt;regalloc(), st);</span>
<span class="line-added">+         st-&gt;cr();</span>
<span class="line-added">+         delay = NULL;</span>
<span class="line-added">+       }</span>
<span class="line-added">+ </span>
<span class="line-added">+       // Dump the exception table as well</span>
<span class="line-added">+       if( n-&gt;is_Catch() &amp;&amp; (Verbose || WizardMode) ) {</span>
<span class="line-added">+         // Print the exception table for this offset</span>
<span class="line-added">+         _handler_table.print_subtable_for(pc);</span>
<span class="line-added">+       }</span>
<span class="line-added">+       st-&gt;bol(); // Make sure we start on a new line</span>
<span class="line-added">+     }</span>
<span class="line-added">+     st-&gt;cr(); // one empty line between blocks</span>
<span class="line-added">+     assert(cut_short || delay == NULL, &quot;no unconditional delay branch&quot;);</span>
<span class="line-added">+   } // End of per-block dump</span>
<span class="line-added">+ </span>
<span class="line-added">+   if (cut_short)  st-&gt;print_cr(&quot;*** disassembly is cut short ***&quot;);</span>
<span class="line-added">+ }</span>
<span class="line-added">+ #endif</span>
<span class="line-added">+ </span>
<span class="line-added">+ #ifndef PRODUCT</span>
<span class="line-added">+ void PhaseOutput::print_statistics() {</span>
<span class="line-added">+   Scheduling::print_statistics();</span>
<span class="line-added">+ }</span>
<span class="line-added">+ #endif</span>
</pre>
<center><a href="node.hpp.cdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="output.hpp.cdiff.html" target="_top">next &gt;</a></center>  </body>
</html>