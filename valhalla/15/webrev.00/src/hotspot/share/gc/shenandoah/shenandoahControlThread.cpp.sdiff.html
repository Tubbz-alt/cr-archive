<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/share/gc/shenandoah/shenandoahControlThread.cpp</title>
    <link rel="stylesheet" href="../../../../../style.css" />
  </head>
<body>
<center><a href="shenandoahConcurrentMark.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../index.html" target="_top">index</a> <a href="shenandoahControlThread.hpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/share/gc/shenandoah/shenandoahControlThread.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
161         cause = default_cause;
162       }
163 
164       // Ask policy if this cycle wants to process references or unload classes
165       heap-&gt;set_process_references(heuristics-&gt;should_process_references());
166       heap-&gt;set_unload_classes(heuristics-&gt;should_unload_classes());
167     }
168 
169     // Blow all soft references on this cycle, if handling allocation failure,
170     // or we are requested to do so unconditionally.
171     if (alloc_failure_pending || ShenandoahAlwaysClearSoftRefs) {
172       heap-&gt;soft_ref_policy()-&gt;set_should_clear_all_soft_refs(true);
173     }
174 
175     bool gc_requested = (mode != none);
176     assert (!gc_requested || cause != GCCause::_last_gc_cause, &quot;GC cause should be set&quot;);
177 
178     if (gc_requested) {
179       heap-&gt;reset_bytes_allocated_since_gc_start();
180 



181       // If GC was requested, we are sampling the counters even without actual triggers
182       // from allocation machinery. This captures GC phases more accurately.
183       set_forced_counters_update(true);
184 
185       // If GC was requested, we better dump freeset data for performance debugging
186       {
187         ShenandoahHeapLocker locker(heap-&gt;lock());
188         heap-&gt;free_set()-&gt;log_status();
189       }
<span class="line-removed">190     }</span>
191 
<span class="line-modified">192     switch (mode) {</span>
<span class="line-modified">193       case none:</span>
<span class="line-modified">194         break;</span>
<span class="line-modified">195       case concurrent_traversal:</span>
<span class="line-modified">196         service_concurrent_traversal_cycle(cause);</span>
<span class="line-modified">197         break;</span>
<span class="line-modified">198       case concurrent_normal:</span>
<span class="line-modified">199         service_concurrent_normal_cycle(cause);</span>
<span class="line-modified">200         break;</span>
<span class="line-modified">201       case stw_degenerated:</span>
<span class="line-modified">202         service_stw_degenerated_cycle(cause, degen_point);</span>
<span class="line-modified">203         break;</span>
<span class="line-modified">204       case stw_full:</span>
<span class="line-modified">205         service_stw_full_cycle(cause);</span>
<span class="line-modified">206         break;</span>
<span class="line-modified">207       default:</span>
<span class="line-removed">208         ShouldNotReachHere();</span>
<span class="line-removed">209     }</span>
210 
<span class="line-removed">211     if (gc_requested) {</span>
212       // If this was the requested GC cycle, notify waiters about it
213       if (explicit_gc_requested || implicit_gc_requested) {
214         notify_gc_waiters();
215       }
216 
217       // If this was the allocation failure GC cycle, notify waiters about it
218       if (alloc_failure_pending) {
219         notify_alloc_failure_waiters();
220       }
221 
222       // Report current free set state at the end of cycle, whether
223       // it is a normal completion, or the abort.
224       {
225         ShenandoahHeapLocker locker(heap-&gt;lock());
226         heap-&gt;free_set()-&gt;log_status();
227 
228         // Notify Universe about new heap usage. This has implications for
229         // global soft refs policy, and we better report it every time heap
230         // usage goes down.
231         Universe::update_heap_info_at_gc();
232       }
233 
234       // Disable forced counters update, and update counters one more time
235       // to capture the state at the end of GC session.
236       handle_force_counters_update();
237       set_forced_counters_update(false);
238 
239       // Retract forceful part of soft refs policy
240       heap-&gt;soft_ref_policy()-&gt;set_should_clear_all_soft_refs(false);
241 
242       // Clear metaspace oom flag, if current cycle unloaded classes
243       if (heap-&gt;unload_classes()) {
244         heuristics-&gt;clear_metaspace_oom();
245       }
246 



247       // GC is over, we are at idle now
248       if (ShenandoahPacing) {
249         heap-&gt;pacer()-&gt;setup_for_idle();
250       }
251     } else {
252       // Allow allocators to know we have seen this much regions
253       if (ShenandoahPacing &amp;&amp; (allocs_seen &gt; 0)) {
254         heap-&gt;pacer()-&gt;report_alloc(allocs_seen);
255       }
256     }
257 
258     double current = os::elapsedTime();
259 
260     if (ShenandoahUncommit &amp;&amp; (explicit_gc_requested || (current - last_shrink_time &gt; shrink_period))) {
261       // Try to uncommit enough stale regions. Explicit GC tries to uncommit everything.
262       // Regular paths uncommit only occasionally.
263       double shrink_before = explicit_gc_requested ?
264                              current :
265                              current - (ShenandoahUncommitDelay / 1000.0);
266       service_uncommit(shrink_before);
</pre>
<hr />
<pre>
299 
300   if (check_cancellation_or_degen(ShenandoahHeap::_degenerated_traversal)) return;
301 
302   heap-&gt;entry_traversal();
303   if (check_cancellation_or_degen(ShenandoahHeap::_degenerated_traversal)) return;
304 
305   heap-&gt;vmop_entry_final_traversal();
306 
307   heap-&gt;entry_cleanup();
308 
309   heap-&gt;heuristics()-&gt;record_success_concurrent();
310   heap-&gt;shenandoah_policy()-&gt;record_success_concurrent();
311 }
312 
313 void ShenandoahControlThread::service_concurrent_normal_cycle(GCCause::Cause cause) {
314   // Normal cycle goes via all concurrent phases. If allocation failure (af) happens during
315   // any of the concurrent phases, it first degrades to Degenerated GC and completes GC there.
316   // If second allocation failure happens during Degenerated GC cycle (for example, when GC
317   // tries to evac something and no memory is available), cycle degrades to Full GC.
318   //
<span class="line-modified">319   // There are also two shortcuts through the normal cycle: a) immediate garbage shortcut, when</span>
320   // heuristics says there are no regions to compact, and all the collection comes from immediately
<span class="line-modified">321   // reclaimable regions; b) coalesced UR shortcut, when heuristics decides to coalesce UR with the</span>
<span class="line-removed">322   // mark from the next cycle.</span>
323   //
324   // ................................................................................................
325   //
326   //                                    (immediate garbage shortcut)                Concurrent GC
327   //                             /-------------------------------------------\
<span class="line-modified">328   //                             |                       (coalesced UR)      v</span>
<span class="line-modified">329   //                             |                  /-----------------------&gt;o</span>
<span class="line-modified">330   //                             |                  |                        |</span>
<span class="line-modified">331   //                             |                  |                        v</span>
332   // [START] ----&gt; Conc Mark ----o----&gt; Conc Evac --o--&gt; Conc Update-Refs ---o----&gt; [END]
333   //                   |                    |                 |              ^
334   //                   | (af)               | (af)            | (af)         |
335   // ..................|....................|.................|..............|.......................
336   //                   |                    |                 |              |
337   //                   |                    |                 |              |      Degenerated GC
338   //                   v                    v                 v              |
339   //               STW Mark ----------&gt; STW Evac ----&gt; STW Update-Refs -----&gt;o
340   //                   |                    |                 |              ^
341   //                   | (af)               | (af)            | (af)         |
342   // ..................|....................|.................|..............|.......................
343   //                   |                    |                 |              |
344   //                   |                    v                 |              |      Full GC
345   //                   \-------------------&gt;o&lt;----------------/              |
346   //                                        |                                |
347   //                                        v                                |
348   //                                      Full GC  --------------------------/
349   //
350   ShenandoahHeap* heap = ShenandoahHeap::heap();
351 
</pre>
<hr />
<pre>
375   // Evacuate concurrent roots
376   heap-&gt;entry_roots();
377 
378   // Final mark might have reclaimed some immediate garbage, kick cleanup to reclaim
379   // the space. This would be the last action if there is nothing to evacuate.
380   heap-&gt;entry_cleanup();
381 
382   {
383     ShenandoahHeapLocker locker(heap-&gt;lock());
384     heap-&gt;free_set()-&gt;log_status();
385   }
386 
387   // Continue the cycle with evacuation and optional update-refs.
388   // This may be skipped if there is nothing to evacuate.
389   // If so, evac_in_progress would be unset by collection set preparation code.
390   if (heap-&gt;is_evacuation_in_progress()) {
391     // Concurrently evacuate
392     heap-&gt;entry_evac();
393     if (check_cancellation_or_degen(ShenandoahHeap::_degenerated_evac)) return;
394 
<span class="line-modified">395     // Perform update-refs phase, if required. This phase can be skipped if heuristics</span>
<span class="line-modified">396     // decides to piggy-back the update-refs on the next marking cycle. On either path,</span>
<span class="line-modified">397     // we need to turn off evacuation: either in init-update-refs, or in final-evac.</span>
<span class="line-modified">398     if (heap-&gt;heuristics()-&gt;should_start_update_refs()) {</span>
<span class="line-removed">399       heap-&gt;vmop_entry_init_updaterefs();</span>
<span class="line-removed">400       heap-&gt;entry_updaterefs();</span>
<span class="line-removed">401       if (check_cancellation_or_degen(ShenandoahHeap::_degenerated_updaterefs)) return;</span>
<span class="line-removed">402 </span>
<span class="line-removed">403       heap-&gt;vmop_entry_final_updaterefs();</span>
404 
<span class="line-modified">405       // Update references freed up collection set, kick the cleanup to reclaim the space.</span>
<span class="line-removed">406       heap-&gt;entry_cleanup();</span>
407 
<span class="line-modified">408     } else {</span>
<span class="line-modified">409       heap-&gt;vmop_entry_final_evac();</span>
<span class="line-removed">410     }</span>
411   }
412 
413   // Cycle is complete
414   heap-&gt;heuristics()-&gt;record_success_concurrent();
415   heap-&gt;shenandoah_policy()-&gt;record_success_concurrent();
416 }
417 
418 bool ShenandoahControlThread::check_cancellation_or_degen(ShenandoahHeap::ShenandoahDegenPoint point) {
419   ShenandoahHeap* heap = ShenandoahHeap::heap();
420   if (heap-&gt;cancelled_gc()) {
421     assert (is_alloc_failure_gc() || in_graceful_shutdown(), &quot;Cancel GC either for alloc failure GC, or gracefully exiting&quot;);
422     if (!in_graceful_shutdown()) {
423       assert (_degen_point == ShenandoahHeap::_degenerated_outside_cycle,
424               &quot;Should not be set yet: %s&quot;, ShenandoahHeap::degen_point_to_string(_degen_point));
425       _degen_point = point;
426     }
427     return true;
428   }
429   return false;
430 }
</pre>
</td>
<td>
<hr />
<pre>
161         cause = default_cause;
162       }
163 
164       // Ask policy if this cycle wants to process references or unload classes
165       heap-&gt;set_process_references(heuristics-&gt;should_process_references());
166       heap-&gt;set_unload_classes(heuristics-&gt;should_unload_classes());
167     }
168 
169     // Blow all soft references on this cycle, if handling allocation failure,
170     // or we are requested to do so unconditionally.
171     if (alloc_failure_pending || ShenandoahAlwaysClearSoftRefs) {
172       heap-&gt;soft_ref_policy()-&gt;set_should_clear_all_soft_refs(true);
173     }
174 
175     bool gc_requested = (mode != none);
176     assert (!gc_requested || cause != GCCause::_last_gc_cause, &quot;GC cause should be set&quot;);
177 
178     if (gc_requested) {
179       heap-&gt;reset_bytes_allocated_since_gc_start();
180 
<span class="line-added">181       // Use default constructor to snapshot the Metaspace state before GC.</span>
<span class="line-added">182       metaspace::MetaspaceSizesSnapshot meta_sizes;</span>
<span class="line-added">183 </span>
184       // If GC was requested, we are sampling the counters even without actual triggers
185       // from allocation machinery. This captures GC phases more accurately.
186       set_forced_counters_update(true);
187 
188       // If GC was requested, we better dump freeset data for performance debugging
189       {
190         ShenandoahHeapLocker locker(heap-&gt;lock());
191         heap-&gt;free_set()-&gt;log_status();
192       }

193 
<span class="line-modified">194       switch (mode) {</span>
<span class="line-modified">195         case concurrent_traversal:</span>
<span class="line-modified">196           service_concurrent_traversal_cycle(cause);</span>
<span class="line-modified">197           break;</span>
<span class="line-modified">198         case concurrent_normal:</span>
<span class="line-modified">199           service_concurrent_normal_cycle(cause);</span>
<span class="line-modified">200           break;</span>
<span class="line-modified">201         case stw_degenerated:</span>
<span class="line-modified">202           service_stw_degenerated_cycle(cause, degen_point);</span>
<span class="line-modified">203           break;</span>
<span class="line-modified">204         case stw_full:</span>
<span class="line-modified">205           service_stw_full_cycle(cause);</span>
<span class="line-modified">206           break;</span>
<span class="line-modified">207         default:</span>
<span class="line-modified">208           ShouldNotReachHere();</span>
<span class="line-modified">209       }</span>


210 

211       // If this was the requested GC cycle, notify waiters about it
212       if (explicit_gc_requested || implicit_gc_requested) {
213         notify_gc_waiters();
214       }
215 
216       // If this was the allocation failure GC cycle, notify waiters about it
217       if (alloc_failure_pending) {
218         notify_alloc_failure_waiters();
219       }
220 
221       // Report current free set state at the end of cycle, whether
222       // it is a normal completion, or the abort.
223       {
224         ShenandoahHeapLocker locker(heap-&gt;lock());
225         heap-&gt;free_set()-&gt;log_status();
226 
227         // Notify Universe about new heap usage. This has implications for
228         // global soft refs policy, and we better report it every time heap
229         // usage goes down.
230         Universe::update_heap_info_at_gc();
231       }
232 
233       // Disable forced counters update, and update counters one more time
234       // to capture the state at the end of GC session.
235       handle_force_counters_update();
236       set_forced_counters_update(false);
237 
238       // Retract forceful part of soft refs policy
239       heap-&gt;soft_ref_policy()-&gt;set_should_clear_all_soft_refs(false);
240 
241       // Clear metaspace oom flag, if current cycle unloaded classes
242       if (heap-&gt;unload_classes()) {
243         heuristics-&gt;clear_metaspace_oom();
244       }
245 
<span class="line-added">246       // Print Metaspace change following GC (if logging is enabled).</span>
<span class="line-added">247       MetaspaceUtils::print_metaspace_change(meta_sizes);</span>
<span class="line-added">248 </span>
249       // GC is over, we are at idle now
250       if (ShenandoahPacing) {
251         heap-&gt;pacer()-&gt;setup_for_idle();
252       }
253     } else {
254       // Allow allocators to know we have seen this much regions
255       if (ShenandoahPacing &amp;&amp; (allocs_seen &gt; 0)) {
256         heap-&gt;pacer()-&gt;report_alloc(allocs_seen);
257       }
258     }
259 
260     double current = os::elapsedTime();
261 
262     if (ShenandoahUncommit &amp;&amp; (explicit_gc_requested || (current - last_shrink_time &gt; shrink_period))) {
263       // Try to uncommit enough stale regions. Explicit GC tries to uncommit everything.
264       // Regular paths uncommit only occasionally.
265       double shrink_before = explicit_gc_requested ?
266                              current :
267                              current - (ShenandoahUncommitDelay / 1000.0);
268       service_uncommit(shrink_before);
</pre>
<hr />
<pre>
301 
302   if (check_cancellation_or_degen(ShenandoahHeap::_degenerated_traversal)) return;
303 
304   heap-&gt;entry_traversal();
305   if (check_cancellation_or_degen(ShenandoahHeap::_degenerated_traversal)) return;
306 
307   heap-&gt;vmop_entry_final_traversal();
308 
309   heap-&gt;entry_cleanup();
310 
311   heap-&gt;heuristics()-&gt;record_success_concurrent();
312   heap-&gt;shenandoah_policy()-&gt;record_success_concurrent();
313 }
314 
315 void ShenandoahControlThread::service_concurrent_normal_cycle(GCCause::Cause cause) {
316   // Normal cycle goes via all concurrent phases. If allocation failure (af) happens during
317   // any of the concurrent phases, it first degrades to Degenerated GC and completes GC there.
318   // If second allocation failure happens during Degenerated GC cycle (for example, when GC
319   // tries to evac something and no memory is available), cycle degrades to Full GC.
320   //
<span class="line-modified">321   // There are also a shortcut through the normal cycle: immediate garbage shortcut, when</span>
322   // heuristics says there are no regions to compact, and all the collection comes from immediately
<span class="line-modified">323   // reclaimable regions.</span>

324   //
325   // ................................................................................................
326   //
327   //                                    (immediate garbage shortcut)                Concurrent GC
328   //                             /-------------------------------------------\
<span class="line-modified">329   //                             |                                           |</span>
<span class="line-modified">330   //                             |                                           |</span>
<span class="line-modified">331   //                             |                                           |</span>
<span class="line-modified">332   //                             |                                           v</span>
333   // [START] ----&gt; Conc Mark ----o----&gt; Conc Evac --o--&gt; Conc Update-Refs ---o----&gt; [END]
334   //                   |                    |                 |              ^
335   //                   | (af)               | (af)            | (af)         |
336   // ..................|....................|.................|..............|.......................
337   //                   |                    |                 |              |
338   //                   |                    |                 |              |      Degenerated GC
339   //                   v                    v                 v              |
340   //               STW Mark ----------&gt; STW Evac ----&gt; STW Update-Refs -----&gt;o
341   //                   |                    |                 |              ^
342   //                   | (af)               | (af)            | (af)         |
343   // ..................|....................|.................|..............|.......................
344   //                   |                    |                 |              |
345   //                   |                    v                 |              |      Full GC
346   //                   \-------------------&gt;o&lt;----------------/              |
347   //                                        |                                |
348   //                                        v                                |
349   //                                      Full GC  --------------------------/
350   //
351   ShenandoahHeap* heap = ShenandoahHeap::heap();
352 
</pre>
<hr />
<pre>
376   // Evacuate concurrent roots
377   heap-&gt;entry_roots();
378 
379   // Final mark might have reclaimed some immediate garbage, kick cleanup to reclaim
380   // the space. This would be the last action if there is nothing to evacuate.
381   heap-&gt;entry_cleanup();
382 
383   {
384     ShenandoahHeapLocker locker(heap-&gt;lock());
385     heap-&gt;free_set()-&gt;log_status();
386   }
387 
388   // Continue the cycle with evacuation and optional update-refs.
389   // This may be skipped if there is nothing to evacuate.
390   // If so, evac_in_progress would be unset by collection set preparation code.
391   if (heap-&gt;is_evacuation_in_progress()) {
392     // Concurrently evacuate
393     heap-&gt;entry_evac();
394     if (check_cancellation_or_degen(ShenandoahHeap::_degenerated_evac)) return;
395 
<span class="line-modified">396     // Perform update-refs phase.</span>
<span class="line-modified">397     heap-&gt;vmop_entry_init_updaterefs();</span>
<span class="line-modified">398     heap-&gt;entry_updaterefs();</span>
<span class="line-modified">399     if (check_cancellation_or_degen(ShenandoahHeap::_degenerated_updaterefs)) return;</span>





400 
<span class="line-modified">401     heap-&gt;vmop_entry_final_updaterefs();</span>

402 
<span class="line-modified">403     // Update references freed up collection set, kick the cleanup to reclaim the space.</span>
<span class="line-modified">404     heap-&gt;entry_cleanup();</span>

405   }
406 
407   // Cycle is complete
408   heap-&gt;heuristics()-&gt;record_success_concurrent();
409   heap-&gt;shenandoah_policy()-&gt;record_success_concurrent();
410 }
411 
412 bool ShenandoahControlThread::check_cancellation_or_degen(ShenandoahHeap::ShenandoahDegenPoint point) {
413   ShenandoahHeap* heap = ShenandoahHeap::heap();
414   if (heap-&gt;cancelled_gc()) {
415     assert (is_alloc_failure_gc() || in_graceful_shutdown(), &quot;Cancel GC either for alloc failure GC, or gracefully exiting&quot;);
416     if (!in_graceful_shutdown()) {
417       assert (_degen_point == ShenandoahHeap::_degenerated_outside_cycle,
418               &quot;Should not be set yet: %s&quot;, ShenandoahHeap::degen_point_to_string(_degen_point));
419       _degen_point = point;
420     }
421     return true;
422   }
423   return false;
424 }
</pre>
</td>
</tr>
</table>
<center><a href="shenandoahConcurrentMark.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../index.html" target="_top">index</a> <a href="shenandoahControlThread.hpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>