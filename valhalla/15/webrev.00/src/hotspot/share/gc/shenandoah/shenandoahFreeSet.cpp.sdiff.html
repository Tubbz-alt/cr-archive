<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/share/gc/shenandoah/shenandoahFreeSet.cpp</title>
    <link rel="stylesheet" href="../../../../../style.css" />
  </head>
<body>
<center><a href="shenandoahForwarding.inline.hpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../index.html" target="_top">index</a> <a href="shenandoahFreeSet.hpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/share/gc/shenandoah/shenandoahFreeSet.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
 25 #include &quot;precompiled.hpp&quot;
 26 
 27 #include &quot;gc/shenandoah/shenandoahFreeSet.hpp&quot;
 28 #include &quot;gc/shenandoah/shenandoahHeap.inline.hpp&quot;
 29 #include &quot;gc/shenandoah/shenandoahHeapRegionSet.hpp&quot;
 30 #include &quot;gc/shenandoah/shenandoahMarkingContext.inline.hpp&quot;
 31 #include &quot;gc/shenandoah/shenandoahTraversalGC.hpp&quot;
 32 #include &quot;logging/logStream.hpp&quot;
 33 #include &quot;runtime/orderAccess.hpp&quot;
 34 
 35 ShenandoahFreeSet::ShenandoahFreeSet(ShenandoahHeap* heap, size_t max_regions) :
 36   _heap(heap),
 37   _mutator_free_bitmap(max_regions, mtGC),
 38   _collector_free_bitmap(max_regions, mtGC),
 39   _max(max_regions)
 40 {
 41   clear_internal();
 42 }
 43 
 44 void ShenandoahFreeSet::increase_used(size_t num_bytes) {
<span class="line-modified"> 45   assert_heaplock_owned_by_current_thread();</span>
 46   _used += num_bytes;
 47 
 48   assert(_used &lt;= _capacity, &quot;must not use more than we have: used: &quot; SIZE_FORMAT
 49          &quot;, capacity: &quot; SIZE_FORMAT &quot;, num_bytes: &quot; SIZE_FORMAT, _used, _capacity, num_bytes);
 50 }
 51 
 52 bool ShenandoahFreeSet::is_mutator_free(size_t idx) const {
 53   assert (idx &lt; _max, &quot;index is sane: &quot; SIZE_FORMAT &quot; &lt; &quot; SIZE_FORMAT &quot; (left: &quot; SIZE_FORMAT &quot;, right: &quot; SIZE_FORMAT &quot;)&quot;,
 54           idx, _max, _mutator_leftmost, _mutator_rightmost);
 55   return _mutator_free_bitmap.at(idx);
 56 }
 57 
 58 bool ShenandoahFreeSet::is_collector_free(size_t idx) const {
 59   assert (idx &lt; _max, &quot;index is sane: &quot; SIZE_FORMAT &quot; &lt; &quot; SIZE_FORMAT &quot; (left: &quot; SIZE_FORMAT &quot;, right: &quot; SIZE_FORMAT &quot;)&quot;,
 60           idx, _max, _collector_leftmost, _collector_rightmost);
 61   return _collector_free_bitmap.at(idx);
 62 }
 63 
 64 HeapWord* ShenandoahFreeSet::allocate_single(ShenandoahAllocRequest&amp; req, bool&amp; in_new_region) {
 65   // Scan the bitmap looking for a first fit.
</pre>
<hr />
<pre>
162   HeapWord* result = NULL;
163   size_t size = req.size();
164 
165   if (ShenandoahElasticTLAB &amp;&amp; req.is_lab_alloc()) {
166     size_t free = align_down(r-&gt;free() &gt;&gt; LogHeapWordSize, MinObjAlignment);
167     if (size &gt; free) {
168       size = free;
169     }
170     if (size &gt;= req.min_size()) {
171       result = r-&gt;allocate(size, req.type());
172       assert (result != NULL, &quot;Allocation must succeed: free &quot; SIZE_FORMAT &quot;, actual &quot; SIZE_FORMAT, free, size);
173     }
174   } else {
175     result = r-&gt;allocate(size, req.type());
176   }
177 
178   if (result != NULL) {
179     // Allocation successful, bump stats:
180     if (req.is_mutator_alloc()) {
181       increase_used(size * HeapWordSize);



182     }
183 
184     // Record actual allocation size
185     req.set_actual_size(size);
186 
<span class="line-modified">187     if (req.is_gc_alloc() &amp;&amp; _heap-&gt;is_concurrent_traversal_in_progress()) {</span>
<span class="line-modified">188       // Traversal needs to traverse through GC allocs. Adjust TAMS to the new top</span>
<span class="line-modified">189       // so that these allocations appear below TAMS, and thus get traversed.</span>
<span class="line-modified">190       // See top of shenandoahTraversal.cpp for an explanation.</span>
<span class="line-modified">191       _heap-&gt;marking_context()-&gt;capture_top_at_mark_start(r);</span>
<span class="line-modified">192       _heap-&gt;traversal_gc()-&gt;traversal_set()-&gt;add_region_check_for_duplicates(r);</span>
<span class="line-modified">193       OrderAccess::fence();</span>



194     }
195   }
196 
197   if (result == NULL || has_no_alloc_capacity(r)) {
198     // Region cannot afford this or future allocations. Retire it.
199     //
200     // While this seems a bit harsh, especially in the case when this large allocation does not
201     // fit, but the next small one would, we are risking to inflate scan times when lots of
202     // almost-full regions precede the fully-empty region where we want allocate the entire TLAB.
203     // TODO: Record first fully-empty region, and use that for large allocations
204 
205     // Record the remainder as allocation waste
206     if (req.is_mutator_alloc()) {
207       size_t waste = r-&gt;free();
208       if (waste &gt; 0) {
209         increase_used(waste);
210         _heap-&gt;notify_mutator_alloc_words(waste &gt;&gt; LogHeapWordSize, true);
211       }
212     }
213 
</pre>
<hr />
<pre>
239 }
240 
241 void ShenandoahFreeSet::adjust_bounds() {
242   // Rewind both mutator bounds until the next bit.
243   while (_mutator_leftmost &lt; _max &amp;&amp; !is_mutator_free(_mutator_leftmost)) {
244     _mutator_leftmost++;
245   }
246   while (_mutator_rightmost &gt; 0 &amp;&amp; !is_mutator_free(_mutator_rightmost)) {
247     _mutator_rightmost--;
248   }
249   // Rewind both collector bounds until the next bit.
250   while (_collector_leftmost &lt; _max &amp;&amp; !is_collector_free(_collector_leftmost)) {
251     _collector_leftmost++;
252   }
253   while (_collector_rightmost &gt; 0 &amp;&amp; !is_collector_free(_collector_rightmost)) {
254     _collector_rightmost--;
255   }
256 }
257 
258 HeapWord* ShenandoahFreeSet::allocate_contiguous(ShenandoahAllocRequest&amp; req) {
<span class="line-modified">259   assert_heaplock_owned_by_current_thread();</span>
260 
261   size_t words_size = req.size();
262   size_t num = ShenandoahHeapRegion::required_regions(words_size * HeapWordSize);
263 
264   // No regions left to satisfy allocation, bye.
265   if (num &gt; mutator_count()) {
266     return NULL;
267   }
268 
269   // Find the continuous interval of $num regions, starting from $beg and ending in $end,
270   // inclusive. Contiguous allocations are biased to the beginning.
271 
272   size_t beg = _mutator_leftmost;
273   size_t end = beg;
274 
275   while (true) {
276     if (end &gt;= _max) {
277       // Hit the end, goodbye
278       return NULL;
279     }
</pre>
<hr />
<pre>
352     // This would be recycled on allocation path
353     return ShenandoahHeapRegion::region_size_bytes();
354   } else {
355     return r-&gt;free();
356   }
357 }
358 
359 bool ShenandoahFreeSet::has_no_alloc_capacity(ShenandoahHeapRegion *r) {
360   return alloc_capacity(r) == 0;
361 }
362 
363 void ShenandoahFreeSet::try_recycle_trashed(ShenandoahHeapRegion *r) {
364   if (r-&gt;is_trash()) {
365     _heap-&gt;decrease_used(r-&gt;used());
366     r-&gt;recycle();
367   }
368 }
369 
370 void ShenandoahFreeSet::recycle_trash() {
371   // lock is not reentrable, check we don&#39;t have it
<span class="line-modified">372   assert_heaplock_not_owned_by_current_thread();</span>
373 
374   for (size_t i = 0; i &lt; _heap-&gt;num_regions(); i++) {
375     ShenandoahHeapRegion* r = _heap-&gt;get_region(i);
376     if (r-&gt;is_trash()) {
377       ShenandoahHeapLocker locker(_heap-&gt;lock());
378       try_recycle_trashed(r);
379     }
380     SpinPause(); // allow allocators to take the lock
381   }
382 }
383 
384 void ShenandoahFreeSet::flip_to_gc(ShenandoahHeapRegion* r) {
385   size_t idx = r-&gt;region_number();
386 
387   assert(_mutator_free_bitmap.at(idx), &quot;Should be in mutator view&quot;);
388   assert(can_allocate_from(r), &quot;Should not be allocated&quot;);
389 
390   _mutator_free_bitmap.clear_bit(idx);
391   _collector_free_bitmap.set_bit(idx);
392   _collector_leftmost = MIN2(idx, _collector_leftmost);
393   _collector_rightmost = MAX2(idx, _collector_rightmost);
394 
395   _capacity -= alloc_capacity(r);
396 
397   if (touches_bounds(idx)) {
398     adjust_bounds();
399   }
400   assert_bounds();
401 }
402 
403 void ShenandoahFreeSet::clear() {
<span class="line-modified">404   assert_heaplock_owned_by_current_thread();</span>
405   clear_internal();
406 }
407 
408 void ShenandoahFreeSet::clear_internal() {
409   _mutator_free_bitmap.clear();
410   _collector_free_bitmap.clear();
411   _mutator_leftmost = _max;
412   _mutator_rightmost = 0;
413   _collector_leftmost = _max;
414   _collector_rightmost = 0;
415   _capacity = 0;
416   _used = 0;
417 }
418 
419 void ShenandoahFreeSet::rebuild() {
<span class="line-modified">420   assert_heaplock_owned_by_current_thread();</span>
421   clear();
422 
423   for (size_t idx = 0; idx &lt; _heap-&gt;num_regions(); idx++) {
424     ShenandoahHeapRegion* region = _heap-&gt;get_region(idx);
425     if (region-&gt;is_alloc_allowed() || region-&gt;is_trash()) {
426       assert(!region-&gt;is_cset(), &quot;Shouldn&#39;t be adding those to the free set&quot;);
427 
428       // Do not add regions that would surely fail allocation
429       if (has_no_alloc_capacity(region)) continue;
430 
431       _capacity += alloc_capacity(region);
432       assert(_used &lt;= _capacity, &quot;must not use more than we have&quot;);
433 
434       assert(!is_mutator_free(idx), &quot;We are about to add it, it shouldn&#39;t be there already&quot;);
435       _mutator_free_bitmap.set_bit(idx);
436     }
437   }
438 
439   // Evac reserve: reserve trailing space for evacuations
440   size_t to_reserve = _heap-&gt;max_capacity() / 100 * ShenandoahEvacReserve;
441   size_t reserved = 0;
442 
443   for (size_t idx = _heap-&gt;num_regions() - 1; idx &gt; 0; idx--) {
444     if (reserved &gt;= to_reserve) break;
445 
446     ShenandoahHeapRegion* region = _heap-&gt;get_region(idx);
447     if (_mutator_free_bitmap.at(idx) &amp;&amp; can_allocate_from(region)) {
448       _mutator_free_bitmap.clear_bit(idx);
449       _collector_free_bitmap.set_bit(idx);
450       size_t ac = alloc_capacity(region);
451       _capacity -= ac;
452       reserved += ac;
453     }
454   }
455 
456   recompute_bounds();
457   assert_bounds();
458 }
459 
460 void ShenandoahFreeSet::log_status() {
<span class="line-modified">461   assert_heaplock_owned_by_current_thread();</span>
462 
463   LogTarget(Info, gc, ergo) lt;
464   if (lt.is_enabled()) {
465     ResourceMark rm;
466     LogStream ls(lt);
467 
468     {
469       size_t last_idx = 0;
470       size_t max = 0;
471       size_t max_contig = 0;
472       size_t empty_contig = 0;
473 
474       size_t total_used = 0;
475       size_t total_free = 0;

476 
477       for (size_t idx = _mutator_leftmost; idx &lt;= _mutator_rightmost; idx++) {
478         if (is_mutator_free(idx)) {
479           ShenandoahHeapRegion *r = _heap-&gt;get_region(idx);
480           size_t free = alloc_capacity(r);
481 
482           max = MAX2(max, free);
483 
<span class="line-modified">484           if (r-&gt;is_empty() &amp;&amp; (last_idx + 1 == idx)) {</span>
<span class="line-modified">485             empty_contig++;</span>





486           } else {
487             empty_contig = 0;
488           }
489 
490           total_used += r-&gt;used();
491           total_free += free;
492 
493           max_contig = MAX2(max_contig, empty_contig);
494           last_idx = idx;
495         }
496       }
497 
498       size_t max_humongous = max_contig * ShenandoahHeapRegion::region_size_bytes();
499       size_t free = capacity() - used();
500 
501       ls.print(&quot;Free: &quot; SIZE_FORMAT &quot;%s (&quot; SIZE_FORMAT &quot; regions), Max regular: &quot; SIZE_FORMAT &quot;%s, Max humongous: &quot; SIZE_FORMAT &quot;%s, &quot;,
502                byte_size_in_proper_unit(total_free),    proper_unit_for_byte_size(total_free),
503                mutator_count(),
504                byte_size_in_proper_unit(max),           proper_unit_for_byte_size(max),
505                byte_size_in_proper_unit(max_humongous), proper_unit_for_byte_size(max_humongous)
506       );
507 
508       size_t frag_ext;
<span class="line-modified">509       if (free &gt; 0) {</span>
<span class="line-modified">510         frag_ext = 100 - (100 * max_humongous / free);</span>
511       } else {
512         frag_ext = 0;
513       }
514       ls.print(&quot;External frag: &quot; SIZE_FORMAT &quot;%%, &quot;, frag_ext);
515 
516       size_t frag_int;
517       if (mutator_count() &gt; 0) {
518         frag_int = (100 * (total_used / mutator_count()) / ShenandoahHeapRegion::region_size_bytes());
519       } else {
520         frag_int = 0;
521       }
522       ls.print(&quot;Internal frag: &quot; SIZE_FORMAT &quot;%%&quot;, frag_int);
523       ls.cr();
524     }
525 
526     {
527       size_t max = 0;
528       size_t total_free = 0;
529 
530       for (size_t idx = _collector_leftmost; idx &lt;= _collector_rightmost; idx++) {
531         if (is_collector_free(idx)) {
532           ShenandoahHeapRegion *r = _heap-&gt;get_region(idx);
533           size_t free = alloc_capacity(r);
534           max = MAX2(max, free);
535           total_free += free;
536         }
537       }
538 
539       ls.print_cr(&quot;Evacuation Reserve: &quot; SIZE_FORMAT &quot;%s (&quot; SIZE_FORMAT &quot; regions), Max regular: &quot; SIZE_FORMAT &quot;%s&quot;,
540                   byte_size_in_proper_unit(total_free), proper_unit_for_byte_size(total_free),
541                   collector_count(),
542                   byte_size_in_proper_unit(max),        proper_unit_for_byte_size(max));
543     }
544   }
545 }
546 
547 HeapWord* ShenandoahFreeSet::allocate(ShenandoahAllocRequest&amp; req, bool&amp; in_new_region) {
<span class="line-modified">548   assert_heaplock_owned_by_current_thread();</span>
549   assert_bounds();
550 
551   if (req.size() &gt; ShenandoahHeapRegion::humongous_threshold_words()) {
552     switch (req.type()) {
553       case ShenandoahAllocRequest::_alloc_shared:
554       case ShenandoahAllocRequest::_alloc_shared_gc:
555         in_new_region = true;
556         return allocate_contiguous(req);
557       case ShenandoahAllocRequest::_alloc_gclab:
558       case ShenandoahAllocRequest::_alloc_tlab:
559         in_new_region = false;
560         assert(false, &quot;Trying to allocate TLAB larger than the humongous threshold: &quot; SIZE_FORMAT &quot; &gt; &quot; SIZE_FORMAT,
561                req.size(), ShenandoahHeapRegion::humongous_threshold_words());
562         return NULL;
563       default:
564         ShouldNotReachHere();
565         return NULL;
566     }
567   } else {
568     return allocate_single(req, in_new_region);
</pre>
<hr />
<pre>
583 
584   // It appears that no regions left
585   return 0;
586 }
587 
588 void ShenandoahFreeSet::print_on(outputStream* out) const {
589   out-&gt;print_cr(&quot;Mutator Free Set: &quot; SIZE_FORMAT &quot;&quot;, mutator_count());
590   for (size_t index = _mutator_leftmost; index &lt;= _mutator_rightmost; index++) {
591     if (is_mutator_free(index)) {
592       _heap-&gt;get_region(index)-&gt;print_on(out);
593     }
594   }
595   out-&gt;print_cr(&quot;Collector Free Set: &quot; SIZE_FORMAT &quot;&quot;, collector_count());
596   for (size_t index = _collector_leftmost; index &lt;= _collector_rightmost; index++) {
597     if (is_collector_free(index)) {
598       _heap-&gt;get_region(index)-&gt;print_on(out);
599     }
600   }
601 }
602 
<span class="line-modified">603 #ifdef ASSERT</span>
<span class="line-modified">604 void ShenandoahFreeSet::assert_heaplock_owned_by_current_thread() const {</span>
<span class="line-modified">605   _heap-&gt;assert_heaplock_owned_by_current_thread();</span>







































606 }
607 
<span class="line-modified">608 void ShenandoahFreeSet::assert_heaplock_not_owned_by_current_thread() const {</span>
<span class="line-modified">609   _heap-&gt;assert_heaplock_not_owned_by_current_thread();</span>










































610 }
611 

612 void ShenandoahFreeSet::assert_bounds() const {
613   // Performance invariants. Failing these would not break the free set, but performance
614   // would suffer.
615   assert (_mutator_leftmost &lt;= _max, &quot;leftmost in bounds: &quot;  SIZE_FORMAT &quot; &lt; &quot; SIZE_FORMAT, _mutator_leftmost,  _max);
616   assert (_mutator_rightmost &lt; _max, &quot;rightmost in bounds: &quot; SIZE_FORMAT &quot; &lt; &quot; SIZE_FORMAT, _mutator_rightmost, _max);
617 
618   assert (_mutator_leftmost == _max || is_mutator_free(_mutator_leftmost),  &quot;leftmost region should be free: &quot; SIZE_FORMAT,  _mutator_leftmost);
619   assert (_mutator_rightmost == 0   || is_mutator_free(_mutator_rightmost), &quot;rightmost region should be free: &quot; SIZE_FORMAT, _mutator_rightmost);
620 
621   size_t beg_off = _mutator_free_bitmap.get_next_one_offset(0);
622   size_t end_off = _mutator_free_bitmap.get_next_one_offset(_mutator_rightmost + 1);
623   assert (beg_off &gt;= _mutator_leftmost, &quot;free regions before the leftmost: &quot; SIZE_FORMAT &quot;, bound &quot; SIZE_FORMAT, beg_off, _mutator_leftmost);
624   assert (end_off == _max,      &quot;free regions past the rightmost: &quot; SIZE_FORMAT &quot;, bound &quot; SIZE_FORMAT,  end_off, _mutator_rightmost);
625 
626   assert (_collector_leftmost &lt;= _max, &quot;leftmost in bounds: &quot;  SIZE_FORMAT &quot; &lt; &quot; SIZE_FORMAT, _collector_leftmost,  _max);
627   assert (_collector_rightmost &lt; _max, &quot;rightmost in bounds: &quot; SIZE_FORMAT &quot; &lt; &quot; SIZE_FORMAT, _collector_rightmost, _max);
628 
629   assert (_collector_leftmost == _max || is_collector_free(_collector_leftmost),  &quot;leftmost region should be free: &quot; SIZE_FORMAT,  _collector_leftmost);
630   assert (_collector_rightmost == 0   || is_collector_free(_collector_rightmost), &quot;rightmost region should be free: &quot; SIZE_FORMAT, _collector_rightmost);
631 
</pre>
</td>
<td>
<hr />
<pre>
 25 #include &quot;precompiled.hpp&quot;
 26 
 27 #include &quot;gc/shenandoah/shenandoahFreeSet.hpp&quot;
 28 #include &quot;gc/shenandoah/shenandoahHeap.inline.hpp&quot;
 29 #include &quot;gc/shenandoah/shenandoahHeapRegionSet.hpp&quot;
 30 #include &quot;gc/shenandoah/shenandoahMarkingContext.inline.hpp&quot;
 31 #include &quot;gc/shenandoah/shenandoahTraversalGC.hpp&quot;
 32 #include &quot;logging/logStream.hpp&quot;
 33 #include &quot;runtime/orderAccess.hpp&quot;
 34 
 35 ShenandoahFreeSet::ShenandoahFreeSet(ShenandoahHeap* heap, size_t max_regions) :
 36   _heap(heap),
 37   _mutator_free_bitmap(max_regions, mtGC),
 38   _collector_free_bitmap(max_regions, mtGC),
 39   _max(max_regions)
 40 {
 41   clear_internal();
 42 }
 43 
 44 void ShenandoahFreeSet::increase_used(size_t num_bytes) {
<span class="line-modified"> 45   shenandoah_assert_heaplocked();</span>
 46   _used += num_bytes;
 47 
 48   assert(_used &lt;= _capacity, &quot;must not use more than we have: used: &quot; SIZE_FORMAT
 49          &quot;, capacity: &quot; SIZE_FORMAT &quot;, num_bytes: &quot; SIZE_FORMAT, _used, _capacity, num_bytes);
 50 }
 51 
 52 bool ShenandoahFreeSet::is_mutator_free(size_t idx) const {
 53   assert (idx &lt; _max, &quot;index is sane: &quot; SIZE_FORMAT &quot; &lt; &quot; SIZE_FORMAT &quot; (left: &quot; SIZE_FORMAT &quot;, right: &quot; SIZE_FORMAT &quot;)&quot;,
 54           idx, _max, _mutator_leftmost, _mutator_rightmost);
 55   return _mutator_free_bitmap.at(idx);
 56 }
 57 
 58 bool ShenandoahFreeSet::is_collector_free(size_t idx) const {
 59   assert (idx &lt; _max, &quot;index is sane: &quot; SIZE_FORMAT &quot; &lt; &quot; SIZE_FORMAT &quot; (left: &quot; SIZE_FORMAT &quot;, right: &quot; SIZE_FORMAT &quot;)&quot;,
 60           idx, _max, _collector_leftmost, _collector_rightmost);
 61   return _collector_free_bitmap.at(idx);
 62 }
 63 
 64 HeapWord* ShenandoahFreeSet::allocate_single(ShenandoahAllocRequest&amp; req, bool&amp; in_new_region) {
 65   // Scan the bitmap looking for a first fit.
</pre>
<hr />
<pre>
162   HeapWord* result = NULL;
163   size_t size = req.size();
164 
165   if (ShenandoahElasticTLAB &amp;&amp; req.is_lab_alloc()) {
166     size_t free = align_down(r-&gt;free() &gt;&gt; LogHeapWordSize, MinObjAlignment);
167     if (size &gt; free) {
168       size = free;
169     }
170     if (size &gt;= req.min_size()) {
171       result = r-&gt;allocate(size, req.type());
172       assert (result != NULL, &quot;Allocation must succeed: free &quot; SIZE_FORMAT &quot;, actual &quot; SIZE_FORMAT, free, size);
173     }
174   } else {
175     result = r-&gt;allocate(size, req.type());
176   }
177 
178   if (result != NULL) {
179     // Allocation successful, bump stats:
180     if (req.is_mutator_alloc()) {
181       increase_used(size * HeapWordSize);
<span class="line-added">182       if (_heap-&gt;is_traversal_mode()) {</span>
<span class="line-added">183         r-&gt;update_seqnum_last_alloc_mutator();</span>
<span class="line-added">184       }</span>
185     }
186 
187     // Record actual allocation size
188     req.set_actual_size(size);
189 
<span class="line-modified">190     if (req.is_gc_alloc()) {</span>
<span class="line-modified">191       r-&gt;set_update_watermark(r-&gt;top());</span>
<span class="line-modified">192       if (_heap-&gt;is_concurrent_traversal_in_progress()) {</span>
<span class="line-modified">193         // Traversal needs to traverse through GC allocs. Adjust TAMS to the new top</span>
<span class="line-modified">194         // so that these allocations appear below TAMS, and thus get traversed.</span>
<span class="line-modified">195         // See top of shenandoahTraversal.cpp for an explanation.</span>
<span class="line-modified">196         _heap-&gt;marking_context()-&gt;capture_top_at_mark_start(r);</span>
<span class="line-added">197         _heap-&gt;traversal_gc()-&gt;traversal_set()-&gt;add_region_check_for_duplicates(r);</span>
<span class="line-added">198         OrderAccess::fence();</span>
<span class="line-added">199       }</span>
200     }
201   }
202 
203   if (result == NULL || has_no_alloc_capacity(r)) {
204     // Region cannot afford this or future allocations. Retire it.
205     //
206     // While this seems a bit harsh, especially in the case when this large allocation does not
207     // fit, but the next small one would, we are risking to inflate scan times when lots of
208     // almost-full regions precede the fully-empty region where we want allocate the entire TLAB.
209     // TODO: Record first fully-empty region, and use that for large allocations
210 
211     // Record the remainder as allocation waste
212     if (req.is_mutator_alloc()) {
213       size_t waste = r-&gt;free();
214       if (waste &gt; 0) {
215         increase_used(waste);
216         _heap-&gt;notify_mutator_alloc_words(waste &gt;&gt; LogHeapWordSize, true);
217       }
218     }
219 
</pre>
<hr />
<pre>
245 }
246 
247 void ShenandoahFreeSet::adjust_bounds() {
248   // Rewind both mutator bounds until the next bit.
249   while (_mutator_leftmost &lt; _max &amp;&amp; !is_mutator_free(_mutator_leftmost)) {
250     _mutator_leftmost++;
251   }
252   while (_mutator_rightmost &gt; 0 &amp;&amp; !is_mutator_free(_mutator_rightmost)) {
253     _mutator_rightmost--;
254   }
255   // Rewind both collector bounds until the next bit.
256   while (_collector_leftmost &lt; _max &amp;&amp; !is_collector_free(_collector_leftmost)) {
257     _collector_leftmost++;
258   }
259   while (_collector_rightmost &gt; 0 &amp;&amp; !is_collector_free(_collector_rightmost)) {
260     _collector_rightmost--;
261   }
262 }
263 
264 HeapWord* ShenandoahFreeSet::allocate_contiguous(ShenandoahAllocRequest&amp; req) {
<span class="line-modified">265   shenandoah_assert_heaplocked();</span>
266 
267   size_t words_size = req.size();
268   size_t num = ShenandoahHeapRegion::required_regions(words_size * HeapWordSize);
269 
270   // No regions left to satisfy allocation, bye.
271   if (num &gt; mutator_count()) {
272     return NULL;
273   }
274 
275   // Find the continuous interval of $num regions, starting from $beg and ending in $end,
276   // inclusive. Contiguous allocations are biased to the beginning.
277 
278   size_t beg = _mutator_leftmost;
279   size_t end = beg;
280 
281   while (true) {
282     if (end &gt;= _max) {
283       // Hit the end, goodbye
284       return NULL;
285     }
</pre>
<hr />
<pre>
358     // This would be recycled on allocation path
359     return ShenandoahHeapRegion::region_size_bytes();
360   } else {
361     return r-&gt;free();
362   }
363 }
364 
365 bool ShenandoahFreeSet::has_no_alloc_capacity(ShenandoahHeapRegion *r) {
366   return alloc_capacity(r) == 0;
367 }
368 
369 void ShenandoahFreeSet::try_recycle_trashed(ShenandoahHeapRegion *r) {
370   if (r-&gt;is_trash()) {
371     _heap-&gt;decrease_used(r-&gt;used());
372     r-&gt;recycle();
373   }
374 }
375 
376 void ShenandoahFreeSet::recycle_trash() {
377   // lock is not reentrable, check we don&#39;t have it
<span class="line-modified">378   shenandoah_assert_not_heaplocked();</span>
379 
380   for (size_t i = 0; i &lt; _heap-&gt;num_regions(); i++) {
381     ShenandoahHeapRegion* r = _heap-&gt;get_region(i);
382     if (r-&gt;is_trash()) {
383       ShenandoahHeapLocker locker(_heap-&gt;lock());
384       try_recycle_trashed(r);
385     }
386     SpinPause(); // allow allocators to take the lock
387   }
388 }
389 
390 void ShenandoahFreeSet::flip_to_gc(ShenandoahHeapRegion* r) {
391   size_t idx = r-&gt;region_number();
392 
393   assert(_mutator_free_bitmap.at(idx), &quot;Should be in mutator view&quot;);
394   assert(can_allocate_from(r), &quot;Should not be allocated&quot;);
395 
396   _mutator_free_bitmap.clear_bit(idx);
397   _collector_free_bitmap.set_bit(idx);
398   _collector_leftmost = MIN2(idx, _collector_leftmost);
399   _collector_rightmost = MAX2(idx, _collector_rightmost);
400 
401   _capacity -= alloc_capacity(r);
402 
403   if (touches_bounds(idx)) {
404     adjust_bounds();
405   }
406   assert_bounds();
407 }
408 
409 void ShenandoahFreeSet::clear() {
<span class="line-modified">410   shenandoah_assert_heaplocked();</span>
411   clear_internal();
412 }
413 
414 void ShenandoahFreeSet::clear_internal() {
415   _mutator_free_bitmap.clear();
416   _collector_free_bitmap.clear();
417   _mutator_leftmost = _max;
418   _mutator_rightmost = 0;
419   _collector_leftmost = _max;
420   _collector_rightmost = 0;
421   _capacity = 0;
422   _used = 0;
423 }
424 
425 void ShenandoahFreeSet::rebuild() {
<span class="line-modified">426   shenandoah_assert_heaplocked();</span>
427   clear();
428 
429   for (size_t idx = 0; idx &lt; _heap-&gt;num_regions(); idx++) {
430     ShenandoahHeapRegion* region = _heap-&gt;get_region(idx);
431     if (region-&gt;is_alloc_allowed() || region-&gt;is_trash()) {
432       assert(!region-&gt;is_cset(), &quot;Shouldn&#39;t be adding those to the free set&quot;);
433 
434       // Do not add regions that would surely fail allocation
435       if (has_no_alloc_capacity(region)) continue;
436 
437       _capacity += alloc_capacity(region);
438       assert(_used &lt;= _capacity, &quot;must not use more than we have&quot;);
439 
440       assert(!is_mutator_free(idx), &quot;We are about to add it, it shouldn&#39;t be there already&quot;);
441       _mutator_free_bitmap.set_bit(idx);
442     }
443   }
444 
445   // Evac reserve: reserve trailing space for evacuations
446   size_t to_reserve = _heap-&gt;max_capacity() / 100 * ShenandoahEvacReserve;
447   size_t reserved = 0;
448 
449   for (size_t idx = _heap-&gt;num_regions() - 1; idx &gt; 0; idx--) {
450     if (reserved &gt;= to_reserve) break;
451 
452     ShenandoahHeapRegion* region = _heap-&gt;get_region(idx);
453     if (_mutator_free_bitmap.at(idx) &amp;&amp; can_allocate_from(region)) {
454       _mutator_free_bitmap.clear_bit(idx);
455       _collector_free_bitmap.set_bit(idx);
456       size_t ac = alloc_capacity(region);
457       _capacity -= ac;
458       reserved += ac;
459     }
460   }
461 
462   recompute_bounds();
463   assert_bounds();
464 }
465 
466 void ShenandoahFreeSet::log_status() {
<span class="line-modified">467   shenandoah_assert_heaplocked();</span>
468 
469   LogTarget(Info, gc, ergo) lt;
470   if (lt.is_enabled()) {
471     ResourceMark rm;
472     LogStream ls(lt);
473 
474     {
475       size_t last_idx = 0;
476       size_t max = 0;
477       size_t max_contig = 0;
478       size_t empty_contig = 0;
479 
480       size_t total_used = 0;
481       size_t total_free = 0;
<span class="line-added">482       size_t total_free_ext = 0;</span>
483 
484       for (size_t idx = _mutator_leftmost; idx &lt;= _mutator_rightmost; idx++) {
485         if (is_mutator_free(idx)) {
486           ShenandoahHeapRegion *r = _heap-&gt;get_region(idx);
487           size_t free = alloc_capacity(r);
488 
489           max = MAX2(max, free);
490 
<span class="line-modified">491           if (r-&gt;is_empty()) {</span>
<span class="line-modified">492             total_free_ext += free;</span>
<span class="line-added">493             if (last_idx + 1 == idx) {</span>
<span class="line-added">494               empty_contig++;</span>
<span class="line-added">495             } else {</span>
<span class="line-added">496               empty_contig = 1;</span>
<span class="line-added">497             }</span>
498           } else {
499             empty_contig = 0;
500           }
501 
502           total_used += r-&gt;used();
503           total_free += free;
504 
505           max_contig = MAX2(max_contig, empty_contig);
506           last_idx = idx;
507         }
508       }
509 
510       size_t max_humongous = max_contig * ShenandoahHeapRegion::region_size_bytes();
511       size_t free = capacity() - used();
512 
513       ls.print(&quot;Free: &quot; SIZE_FORMAT &quot;%s (&quot; SIZE_FORMAT &quot; regions), Max regular: &quot; SIZE_FORMAT &quot;%s, Max humongous: &quot; SIZE_FORMAT &quot;%s, &quot;,
514                byte_size_in_proper_unit(total_free),    proper_unit_for_byte_size(total_free),
515                mutator_count(),
516                byte_size_in_proper_unit(max),           proper_unit_for_byte_size(max),
517                byte_size_in_proper_unit(max_humongous), proper_unit_for_byte_size(max_humongous)
518       );
519 
520       size_t frag_ext;
<span class="line-modified">521       if (total_free_ext &gt; 0) {</span>
<span class="line-modified">522         frag_ext = 100 - (100 * max_humongous / total_free_ext);</span>
523       } else {
524         frag_ext = 0;
525       }
526       ls.print(&quot;External frag: &quot; SIZE_FORMAT &quot;%%, &quot;, frag_ext);
527 
528       size_t frag_int;
529       if (mutator_count() &gt; 0) {
530         frag_int = (100 * (total_used / mutator_count()) / ShenandoahHeapRegion::region_size_bytes());
531       } else {
532         frag_int = 0;
533       }
534       ls.print(&quot;Internal frag: &quot; SIZE_FORMAT &quot;%%&quot;, frag_int);
535       ls.cr();
536     }
537 
538     {
539       size_t max = 0;
540       size_t total_free = 0;
541 
542       for (size_t idx = _collector_leftmost; idx &lt;= _collector_rightmost; idx++) {
543         if (is_collector_free(idx)) {
544           ShenandoahHeapRegion *r = _heap-&gt;get_region(idx);
545           size_t free = alloc_capacity(r);
546           max = MAX2(max, free);
547           total_free += free;
548         }
549       }
550 
551       ls.print_cr(&quot;Evacuation Reserve: &quot; SIZE_FORMAT &quot;%s (&quot; SIZE_FORMAT &quot; regions), Max regular: &quot; SIZE_FORMAT &quot;%s&quot;,
552                   byte_size_in_proper_unit(total_free), proper_unit_for_byte_size(total_free),
553                   collector_count(),
554                   byte_size_in_proper_unit(max),        proper_unit_for_byte_size(max));
555     }
556   }
557 }
558 
559 HeapWord* ShenandoahFreeSet::allocate(ShenandoahAllocRequest&amp; req, bool&amp; in_new_region) {
<span class="line-modified">560   shenandoah_assert_heaplocked();</span>
561   assert_bounds();
562 
563   if (req.size() &gt; ShenandoahHeapRegion::humongous_threshold_words()) {
564     switch (req.type()) {
565       case ShenandoahAllocRequest::_alloc_shared:
566       case ShenandoahAllocRequest::_alloc_shared_gc:
567         in_new_region = true;
568         return allocate_contiguous(req);
569       case ShenandoahAllocRequest::_alloc_gclab:
570       case ShenandoahAllocRequest::_alloc_tlab:
571         in_new_region = false;
572         assert(false, &quot;Trying to allocate TLAB larger than the humongous threshold: &quot; SIZE_FORMAT &quot; &gt; &quot; SIZE_FORMAT,
573                req.size(), ShenandoahHeapRegion::humongous_threshold_words());
574         return NULL;
575       default:
576         ShouldNotReachHere();
577         return NULL;
578     }
579   } else {
580     return allocate_single(req, in_new_region);
</pre>
<hr />
<pre>
595 
596   // It appears that no regions left
597   return 0;
598 }
599 
600 void ShenandoahFreeSet::print_on(outputStream* out) const {
601   out-&gt;print_cr(&quot;Mutator Free Set: &quot; SIZE_FORMAT &quot;&quot;, mutator_count());
602   for (size_t index = _mutator_leftmost; index &lt;= _mutator_rightmost; index++) {
603     if (is_mutator_free(index)) {
604       _heap-&gt;get_region(index)-&gt;print_on(out);
605     }
606   }
607   out-&gt;print_cr(&quot;Collector Free Set: &quot; SIZE_FORMAT &quot;&quot;, collector_count());
608   for (size_t index = _collector_leftmost; index &lt;= _collector_rightmost; index++) {
609     if (is_collector_free(index)) {
610       _heap-&gt;get_region(index)-&gt;print_on(out);
611     }
612   }
613 }
614 
<span class="line-modified">615 /*</span>
<span class="line-modified">616  * Internal fragmentation metric: describes how fragmented the heap regions are.</span>
<span class="line-modified">617  *</span>
<span class="line-added">618  * It is derived as:</span>
<span class="line-added">619  *</span>
<span class="line-added">620  *               sum(used[i]^2, i=0..k)</span>
<span class="line-added">621  *   IF = 1 - ------------------------------</span>
<span class="line-added">622  *              C * sum(used[i], i=0..k)</span>
<span class="line-added">623  *</span>
<span class="line-added">624  * ...where k is the number of regions in computation, C is the region capacity, and</span>
<span class="line-added">625  * used[i] is the used space in the region.</span>
<span class="line-added">626  *</span>
<span class="line-added">627  * The non-linearity causes IF to be lower for the cases where the same total heap</span>
<span class="line-added">628  * used is densely packed. For example:</span>
<span class="line-added">629  *   a) Heap is completely full  =&gt; IF = 0</span>
<span class="line-added">630  *   b) Heap is half full, first 50% regions are completely full =&gt; IF = 0</span>
<span class="line-added">631  *   c) Heap is half full, each region is 50% full =&gt; IF = 1/2</span>
<span class="line-added">632  *   d) Heap is quarter full, first 50% regions are completely full =&gt; IF = 0</span>
<span class="line-added">633  *   e) Heap is quarter full, each region is 25% full =&gt; IF = 3/4</span>
<span class="line-added">634  *   f) Heap has one small object per each region =&gt; IF =~ 1</span>
<span class="line-added">635  */</span>
<span class="line-added">636 double ShenandoahFreeSet::internal_fragmentation() {</span>
<span class="line-added">637   double squared = 0;</span>
<span class="line-added">638   double linear = 0;</span>
<span class="line-added">639   int count = 0;</span>
<span class="line-added">640 </span>
<span class="line-added">641   for (size_t index = _mutator_leftmost; index &lt;= _mutator_rightmost; index++) {</span>
<span class="line-added">642     if (is_mutator_free(index)) {</span>
<span class="line-added">643       ShenandoahHeapRegion* r = _heap-&gt;get_region(index);</span>
<span class="line-added">644       size_t used = r-&gt;used();</span>
<span class="line-added">645       squared += used * used;</span>
<span class="line-added">646       linear += used;</span>
<span class="line-added">647       count++;</span>
<span class="line-added">648     }</span>
<span class="line-added">649   }</span>
<span class="line-added">650 </span>
<span class="line-added">651   if (count &gt; 0) {</span>
<span class="line-added">652     double s = squared / (ShenandoahHeapRegion::region_size_bytes() * linear);</span>
<span class="line-added">653     return 1 - s;</span>
<span class="line-added">654   } else {</span>
<span class="line-added">655     return 0;</span>
<span class="line-added">656   }</span>
657 }
658 
<span class="line-modified">659 /*</span>
<span class="line-modified">660  * External fragmentation metric: describes how fragmented the heap is.</span>
<span class="line-added">661  *</span>
<span class="line-added">662  * It is derived as:</span>
<span class="line-added">663  *</span>
<span class="line-added">664  *   EF = 1 - largest_contiguous_free / total_free</span>
<span class="line-added">665  *</span>
<span class="line-added">666  * For example:</span>
<span class="line-added">667  *   a) Heap is completely empty =&gt; EF = 0</span>
<span class="line-added">668  *   b) Heap is completely full =&gt; EF = 0</span>
<span class="line-added">669  *   c) Heap is first-half full =&gt; EF = 1/2</span>
<span class="line-added">670  *   d) Heap is half full, full and empty regions interleave =&gt; EF =~ 1</span>
<span class="line-added">671  */</span>
<span class="line-added">672 double ShenandoahFreeSet::external_fragmentation() {</span>
<span class="line-added">673   size_t last_idx = 0;</span>
<span class="line-added">674   size_t max_contig = 0;</span>
<span class="line-added">675   size_t empty_contig = 0;</span>
<span class="line-added">676 </span>
<span class="line-added">677   size_t free = 0;</span>
<span class="line-added">678 </span>
<span class="line-added">679   for (size_t index = _mutator_leftmost; index &lt;= _mutator_rightmost; index++) {</span>
<span class="line-added">680     if (is_mutator_free(index)) {</span>
<span class="line-added">681       ShenandoahHeapRegion* r = _heap-&gt;get_region(index);</span>
<span class="line-added">682       if (r-&gt;is_empty()) {</span>
<span class="line-added">683         free += ShenandoahHeapRegion::region_size_bytes();</span>
<span class="line-added">684         if (last_idx + 1 == index) {</span>
<span class="line-added">685           empty_contig++;</span>
<span class="line-added">686         } else {</span>
<span class="line-added">687           empty_contig = 1;</span>
<span class="line-added">688         }</span>
<span class="line-added">689       } else {</span>
<span class="line-added">690         empty_contig = 0;</span>
<span class="line-added">691       }</span>
<span class="line-added">692 </span>
<span class="line-added">693       max_contig = MAX2(max_contig, empty_contig);</span>
<span class="line-added">694       last_idx = index;</span>
<span class="line-added">695     }</span>
<span class="line-added">696   }</span>
<span class="line-added">697 </span>
<span class="line-added">698   if (free &gt; 0) {</span>
<span class="line-added">699     return 1 - (1.0 * max_contig * ShenandoahHeapRegion::region_size_bytes() / free);</span>
<span class="line-added">700   } else {</span>
<span class="line-added">701     return 0;</span>
<span class="line-added">702   }</span>
703 }
704 
<span class="line-added">705 #ifdef ASSERT</span>
706 void ShenandoahFreeSet::assert_bounds() const {
707   // Performance invariants. Failing these would not break the free set, but performance
708   // would suffer.
709   assert (_mutator_leftmost &lt;= _max, &quot;leftmost in bounds: &quot;  SIZE_FORMAT &quot; &lt; &quot; SIZE_FORMAT, _mutator_leftmost,  _max);
710   assert (_mutator_rightmost &lt; _max, &quot;rightmost in bounds: &quot; SIZE_FORMAT &quot; &lt; &quot; SIZE_FORMAT, _mutator_rightmost, _max);
711 
712   assert (_mutator_leftmost == _max || is_mutator_free(_mutator_leftmost),  &quot;leftmost region should be free: &quot; SIZE_FORMAT,  _mutator_leftmost);
713   assert (_mutator_rightmost == 0   || is_mutator_free(_mutator_rightmost), &quot;rightmost region should be free: &quot; SIZE_FORMAT, _mutator_rightmost);
714 
715   size_t beg_off = _mutator_free_bitmap.get_next_one_offset(0);
716   size_t end_off = _mutator_free_bitmap.get_next_one_offset(_mutator_rightmost + 1);
717   assert (beg_off &gt;= _mutator_leftmost, &quot;free regions before the leftmost: &quot; SIZE_FORMAT &quot;, bound &quot; SIZE_FORMAT, beg_off, _mutator_leftmost);
718   assert (end_off == _max,      &quot;free regions past the rightmost: &quot; SIZE_FORMAT &quot;, bound &quot; SIZE_FORMAT,  end_off, _mutator_rightmost);
719 
720   assert (_collector_leftmost &lt;= _max, &quot;leftmost in bounds: &quot;  SIZE_FORMAT &quot; &lt; &quot; SIZE_FORMAT, _collector_leftmost,  _max);
721   assert (_collector_rightmost &lt; _max, &quot;rightmost in bounds: &quot; SIZE_FORMAT &quot; &lt; &quot; SIZE_FORMAT, _collector_rightmost, _max);
722 
723   assert (_collector_leftmost == _max || is_collector_free(_collector_leftmost),  &quot;leftmost region should be free: &quot; SIZE_FORMAT,  _collector_leftmost);
724   assert (_collector_rightmost == 0   || is_collector_free(_collector_rightmost), &quot;rightmost region should be free: &quot; SIZE_FORMAT, _collector_rightmost);
725 
</pre>
</td>
</tr>
</table>
<center><a href="shenandoahForwarding.inline.hpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../index.html" target="_top">index</a> <a href="shenandoahFreeSet.hpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>