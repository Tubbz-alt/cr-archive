diff a/src/hotspot/share/gc/shenandoah/shenandoahFreeSet.cpp b/src/hotspot/share/gc/shenandoah/shenandoahFreeSet.cpp
--- a/src/hotspot/share/gc/shenandoah/shenandoahFreeSet.cpp
+++ b/src/hotspot/share/gc/shenandoah/shenandoahFreeSet.cpp
@@ -40,11 +40,11 @@
 {
   clear_internal();
 }
 
 void ShenandoahFreeSet::increase_used(size_t num_bytes) {
-  assert_heaplock_owned_by_current_thread();
+  shenandoah_assert_heaplocked();
   _used += num_bytes;
 
   assert(_used <= _capacity, "must not use more than we have: used: " SIZE_FORMAT
          ", capacity: " SIZE_FORMAT ", num_bytes: " SIZE_FORMAT, _used, _capacity, num_bytes);
 }
@@ -177,22 +177,28 @@
 
   if (result != NULL) {
     // Allocation successful, bump stats:
     if (req.is_mutator_alloc()) {
       increase_used(size * HeapWordSize);
+      if (_heap->is_traversal_mode()) {
+        r->update_seqnum_last_alloc_mutator();
+      }
     }
 
     // Record actual allocation size
     req.set_actual_size(size);
 
-    if (req.is_gc_alloc() && _heap->is_concurrent_traversal_in_progress()) {
-      // Traversal needs to traverse through GC allocs. Adjust TAMS to the new top
-      // so that these allocations appear below TAMS, and thus get traversed.
-      // See top of shenandoahTraversal.cpp for an explanation.
-      _heap->marking_context()->capture_top_at_mark_start(r);
-      _heap->traversal_gc()->traversal_set()->add_region_check_for_duplicates(r);
-      OrderAccess::fence();
+    if (req.is_gc_alloc()) {
+      r->set_update_watermark(r->top());
+      if (_heap->is_concurrent_traversal_in_progress()) {
+        // Traversal needs to traverse through GC allocs. Adjust TAMS to the new top
+        // so that these allocations appear below TAMS, and thus get traversed.
+        // See top of shenandoahTraversal.cpp for an explanation.
+        _heap->marking_context()->capture_top_at_mark_start(r);
+        _heap->traversal_gc()->traversal_set()->add_region_check_for_duplicates(r);
+        OrderAccess::fence();
+      }
     }
   }
 
   if (result == NULL || has_no_alloc_capacity(r)) {
     // Region cannot afford this or future allocations. Retire it.
@@ -254,11 +260,11 @@
     _collector_rightmost--;
   }
 }
 
 HeapWord* ShenandoahFreeSet::allocate_contiguous(ShenandoahAllocRequest& req) {
-  assert_heaplock_owned_by_current_thread();
+  shenandoah_assert_heaplocked();
 
   size_t words_size = req.size();
   size_t num = ShenandoahHeapRegion::required_regions(words_size * HeapWordSize);
 
   // No regions left to satisfy allocation, bye.
@@ -367,11 +373,11 @@
   }
 }
 
 void ShenandoahFreeSet::recycle_trash() {
   // lock is not reentrable, check we don't have it
-  assert_heaplock_not_owned_by_current_thread();
+  shenandoah_assert_not_heaplocked();
 
   for (size_t i = 0; i < _heap->num_regions(); i++) {
     ShenandoahHeapRegion* r = _heap->get_region(i);
     if (r->is_trash()) {
       ShenandoahHeapLocker locker(_heap->lock());
@@ -399,11 +405,11 @@
   }
   assert_bounds();
 }
 
 void ShenandoahFreeSet::clear() {
-  assert_heaplock_owned_by_current_thread();
+  shenandoah_assert_heaplocked();
   clear_internal();
 }
 
 void ShenandoahFreeSet::clear_internal() {
   _mutator_free_bitmap.clear();
@@ -415,11 +421,11 @@
   _capacity = 0;
   _used = 0;
 }
 
 void ShenandoahFreeSet::rebuild() {
-  assert_heaplock_owned_by_current_thread();
+  shenandoah_assert_heaplocked();
   clear();
 
   for (size_t idx = 0; idx < _heap->num_regions(); idx++) {
     ShenandoahHeapRegion* region = _heap->get_region(idx);
     if (region->is_alloc_allowed() || region->is_trash()) {
@@ -456,11 +462,11 @@
   recompute_bounds();
   assert_bounds();
 }
 
 void ShenandoahFreeSet::log_status() {
-  assert_heaplock_owned_by_current_thread();
+  shenandoah_assert_heaplocked();
 
   LogTarget(Info, gc, ergo) lt;
   if (lt.is_enabled()) {
     ResourceMark rm;
     LogStream ls(lt);
@@ -471,20 +477,26 @@
       size_t max_contig = 0;
       size_t empty_contig = 0;
 
       size_t total_used = 0;
       size_t total_free = 0;
+      size_t total_free_ext = 0;
 
       for (size_t idx = _mutator_leftmost; idx <= _mutator_rightmost; idx++) {
         if (is_mutator_free(idx)) {
           ShenandoahHeapRegion *r = _heap->get_region(idx);
           size_t free = alloc_capacity(r);
 
           max = MAX2(max, free);
 
-          if (r->is_empty() && (last_idx + 1 == idx)) {
-            empty_contig++;
+          if (r->is_empty()) {
+            total_free_ext += free;
+            if (last_idx + 1 == idx) {
+              empty_contig++;
+            } else {
+              empty_contig = 1;
+            }
           } else {
             empty_contig = 0;
           }
 
           total_used += r->used();
@@ -504,12 +516,12 @@
                byte_size_in_proper_unit(max),           proper_unit_for_byte_size(max),
                byte_size_in_proper_unit(max_humongous), proper_unit_for_byte_size(max_humongous)
       );
 
       size_t frag_ext;
-      if (free > 0) {
-        frag_ext = 100 - (100 * max_humongous / free);
+      if (total_free_ext > 0) {
+        frag_ext = 100 - (100 * max_humongous / total_free_ext);
       } else {
         frag_ext = 0;
       }
       ls.print("External frag: " SIZE_FORMAT "%%, ", frag_ext);
 
@@ -543,11 +555,11 @@
     }
   }
 }
 
 HeapWord* ShenandoahFreeSet::allocate(ShenandoahAllocRequest& req, bool& in_new_region) {
-  assert_heaplock_owned_by_current_thread();
+  shenandoah_assert_heaplocked();
   assert_bounds();
 
   if (req.size() > ShenandoahHeapRegion::humongous_threshold_words()) {
     switch (req.type()) {
       case ShenandoahAllocRequest::_alloc_shared:
@@ -598,19 +610,101 @@
       _heap->get_region(index)->print_on(out);
     }
   }
 }
 
-#ifdef ASSERT
-void ShenandoahFreeSet::assert_heaplock_owned_by_current_thread() const {
-  _heap->assert_heaplock_owned_by_current_thread();
+/*
+ * Internal fragmentation metric: describes how fragmented the heap regions are.
+ *
+ * It is derived as:
+ *
+ *               sum(used[i]^2, i=0..k)
+ *   IF = 1 - ------------------------------
+ *              C * sum(used[i], i=0..k)
+ *
+ * ...where k is the number of regions in computation, C is the region capacity, and
+ * used[i] is the used space in the region.
+ *
+ * The non-linearity causes IF to be lower for the cases where the same total heap
+ * used is densely packed. For example:
+ *   a) Heap is completely full  => IF = 0
+ *   b) Heap is half full, first 50% regions are completely full => IF = 0
+ *   c) Heap is half full, each region is 50% full => IF = 1/2
+ *   d) Heap is quarter full, first 50% regions are completely full => IF = 0
+ *   e) Heap is quarter full, each region is 25% full => IF = 3/4
+ *   f) Heap has one small object per each region => IF =~ 1
+ */
+double ShenandoahFreeSet::internal_fragmentation() {
+  double squared = 0;
+  double linear = 0;
+  int count = 0;
+
+  for (size_t index = _mutator_leftmost; index <= _mutator_rightmost; index++) {
+    if (is_mutator_free(index)) {
+      ShenandoahHeapRegion* r = _heap->get_region(index);
+      size_t used = r->used();
+      squared += used * used;
+      linear += used;
+      count++;
+    }
+  }
+
+  if (count > 0) {
+    double s = squared / (ShenandoahHeapRegion::region_size_bytes() * linear);
+    return 1 - s;
+  } else {
+    return 0;
+  }
 }
 
-void ShenandoahFreeSet::assert_heaplock_not_owned_by_current_thread() const {
-  _heap->assert_heaplock_not_owned_by_current_thread();
+/*
+ * External fragmentation metric: describes how fragmented the heap is.
+ *
+ * It is derived as:
+ *
+ *   EF = 1 - largest_contiguous_free / total_free
+ *
+ * For example:
+ *   a) Heap is completely empty => EF = 0
+ *   b) Heap is completely full => EF = 0
+ *   c) Heap is first-half full => EF = 1/2
+ *   d) Heap is half full, full and empty regions interleave => EF =~ 1
+ */
+double ShenandoahFreeSet::external_fragmentation() {
+  size_t last_idx = 0;
+  size_t max_contig = 0;
+  size_t empty_contig = 0;
+
+  size_t free = 0;
+
+  for (size_t index = _mutator_leftmost; index <= _mutator_rightmost; index++) {
+    if (is_mutator_free(index)) {
+      ShenandoahHeapRegion* r = _heap->get_region(index);
+      if (r->is_empty()) {
+        free += ShenandoahHeapRegion::region_size_bytes();
+        if (last_idx + 1 == index) {
+          empty_contig++;
+        } else {
+          empty_contig = 1;
+        }
+      } else {
+        empty_contig = 0;
+      }
+
+      max_contig = MAX2(max_contig, empty_contig);
+      last_idx = index;
+    }
+  }
+
+  if (free > 0) {
+    return 1 - (1.0 * max_contig * ShenandoahHeapRegion::region_size_bytes() / free);
+  } else {
+    return 0;
+  }
 }
 
+#ifdef ASSERT
 void ShenandoahFreeSet::assert_bounds() const {
   // Performance invariants. Failing these would not break the free set, but performance
   // would suffer.
   assert (_mutator_leftmost <= _max, "leftmost in bounds: "  SIZE_FORMAT " < " SIZE_FORMAT, _mutator_leftmost,  _max);
   assert (_mutator_rightmost < _max, "rightmost in bounds: " SIZE_FORMAT " < " SIZE_FORMAT, _mutator_rightmost, _max);
