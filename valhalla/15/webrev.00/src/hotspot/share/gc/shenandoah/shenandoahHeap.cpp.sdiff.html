<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/share/gc/shenandoah/shenandoahHeap.cpp</title>
    <link rel="stylesheet" href="../../../../../style.css" />
  </head>
<body>
<center><a href="shenandoahFreeSet.hpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../index.html" target="_top">index</a> <a href="shenandoahHeap.hpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/share/gc/shenandoah/shenandoahHeap.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #include &quot;precompiled.hpp&quot;
  26 #include &quot;memory/allocation.hpp&quot;
  27 #include &quot;memory/universe.hpp&quot;
  28 
  29 #include &quot;gc/shared/gcArguments.hpp&quot;
  30 #include &quot;gc/shared/gcTimer.hpp&quot;
  31 #include &quot;gc/shared/gcTraceTime.inline.hpp&quot;
  32 #include &quot;gc/shared/locationPrinter.inline.hpp&quot;
  33 #include &quot;gc/shared/memAllocator.hpp&quot;
  34 #include &quot;gc/shared/oopStorageSet.hpp&quot;
  35 #include &quot;gc/shared/plab.hpp&quot;
  36 
<span class="line-removed">  37 #include &quot;gc/shenandoah/shenandoahAllocTracker.hpp&quot;</span>
  38 #include &quot;gc/shenandoah/shenandoahBarrierSet.hpp&quot;
  39 #include &quot;gc/shenandoah/shenandoahClosures.inline.hpp&quot;
  40 #include &quot;gc/shenandoah/shenandoahCollectionSet.hpp&quot;
  41 #include &quot;gc/shenandoah/shenandoahCollectorPolicy.hpp&quot;
  42 #include &quot;gc/shenandoah/shenandoahConcurrentMark.inline.hpp&quot;
  43 #include &quot;gc/shenandoah/shenandoahConcurrentRoots.hpp&quot;
  44 #include &quot;gc/shenandoah/shenandoahControlThread.hpp&quot;
  45 #include &quot;gc/shenandoah/shenandoahFreeSet.hpp&quot;
  46 #include &quot;gc/shenandoah/shenandoahPhaseTimings.hpp&quot;
  47 #include &quot;gc/shenandoah/shenandoahHeap.inline.hpp&quot;
  48 #include &quot;gc/shenandoah/shenandoahHeapRegion.hpp&quot;
  49 #include &quot;gc/shenandoah/shenandoahHeapRegionSet.hpp&quot;
  50 #include &quot;gc/shenandoah/shenandoahMarkCompact.hpp&quot;
  51 #include &quot;gc/shenandoah/shenandoahMarkingContext.inline.hpp&quot;
  52 #include &quot;gc/shenandoah/shenandoahMemoryPool.hpp&quot;
  53 #include &quot;gc/shenandoah/shenandoahMetrics.hpp&quot;
  54 #include &quot;gc/shenandoah/shenandoahMonitoringSupport.hpp&quot;
  55 #include &quot;gc/shenandoah/shenandoahNormalMode.hpp&quot;
  56 #include &quot;gc/shenandoah/shenandoahOopClosures.inline.hpp&quot;
  57 #include &quot;gc/shenandoah/shenandoahPacer.inline.hpp&quot;

  58 #include &quot;gc/shenandoah/shenandoahParallelCleaning.inline.hpp&quot;
  59 #include &quot;gc/shenandoah/shenandoahPassiveMode.hpp&quot;
  60 #include &quot;gc/shenandoah/shenandoahRootProcessor.inline.hpp&quot;
  61 #include &quot;gc/shenandoah/shenandoahStringDedup.hpp&quot;
  62 #include &quot;gc/shenandoah/shenandoahTaskqueue.hpp&quot;
  63 #include &quot;gc/shenandoah/shenandoahTraversalMode.hpp&quot;
  64 #include &quot;gc/shenandoah/shenandoahUtils.hpp&quot;
  65 #include &quot;gc/shenandoah/shenandoahVerifier.hpp&quot;
  66 #include &quot;gc/shenandoah/shenandoahCodeRoots.hpp&quot;
  67 #include &quot;gc/shenandoah/shenandoahVMOperations.hpp&quot;
  68 #include &quot;gc/shenandoah/shenandoahWorkGroup.hpp&quot;
  69 #include &quot;gc/shenandoah/shenandoahWorkerPolicy.hpp&quot;
  70 #if INCLUDE_JFR
  71 #include &quot;gc/shenandoah/shenandoahJfrSupport.hpp&quot;
  72 #endif
  73 
  74 #include &quot;memory/metaspace.hpp&quot;
  75 #include &quot;oops/compressedOops.inline.hpp&quot;
  76 #include &quot;runtime/atomic.hpp&quot;
  77 #include &quot;runtime/globals.hpp&quot;
  78 #include &quot;runtime/interfaceSupport.inline.hpp&quot;
  79 #include &quot;runtime/orderAccess.hpp&quot;
  80 #include &quot;runtime/safepointMechanism.hpp&quot;
  81 #include &quot;runtime/vmThread.hpp&quot;
  82 #include &quot;services/mallocTracker.hpp&quot;
  83 #include &quot;utilities/powerOfTwo.hpp&quot;
  84 


  85 #ifdef ASSERT
  86 template &lt;class T&gt;
  87 void ShenandoahAssertToSpaceClosure::do_oop_work(T* p) {
  88   T o = RawAccess&lt;&gt;::oop_load(p);
  89   if (! CompressedOops::is_null(o)) {
  90     oop obj = CompressedOops::decode_not_null(o);
  91     shenandoah_assert_not_forwarded(p, obj);
  92   }
  93 }
  94 
  95 void ShenandoahAssertToSpaceClosure::do_oop(narrowOop* p) { do_oop_work(p); }
  96 void ShenandoahAssertToSpaceClosure::do_oop(oop* p)       { do_oop_work(p); }
  97 #endif
  98 
  99 class ShenandoahPretouchHeapTask : public AbstractGangTask {
 100 private:
 101   ShenandoahRegionIterator _regions;
 102   const size_t _page_size;
 103 public:
 104   ShenandoahPretouchHeapTask(size_t page_size) :
</pre>
<hr />
<pre>
 268   }
 269 
 270   // Reserve aux bitmap for use in object_iterate(). We don&#39;t commit it here.
 271   ReservedSpace aux_bitmap(_bitmap_size, bitmap_page_size);
 272   MemTracker::record_virtual_memory_type(aux_bitmap.base(), mtGC);
 273   _aux_bitmap_region = MemRegion((HeapWord*) aux_bitmap.base(), aux_bitmap.size() / HeapWordSize);
 274   _aux_bitmap_region_special = aux_bitmap.special();
 275   _aux_bit_map.initialize(_heap_region, _aux_bitmap_region);
 276 
 277   //
 278   // Create regions and region sets
 279   //
 280 
 281   _regions = NEW_C_HEAP_ARRAY(ShenandoahHeapRegion*, _num_regions, mtGC);
 282   _free_set = new ShenandoahFreeSet(this, _num_regions);
 283   _collection_set = new ShenandoahCollectionSet(this, sh_rs.base(), sh_rs.size());
 284 
 285   {
 286     ShenandoahHeapLocker locker(lock());
 287 
<span class="line-removed"> 288     size_t size_words = ShenandoahHeapRegion::region_size_words();</span>
<span class="line-removed"> 289 </span>
 290     for (size_t i = 0; i &lt; _num_regions; i++) {
<span class="line-modified"> 291       HeapWord* start = (HeapWord*)sh_rs.base() + size_words * i;</span>
 292       bool is_committed = i &lt; num_committed_regions;
<span class="line-modified"> 293       ShenandoahHeapRegion* r = new ShenandoahHeapRegion(this, start, size_words, i, is_committed);</span>
 294 
 295       _marking_context-&gt;initialize_top_at_mark_start(r);
 296       _regions[i] = r;
 297       assert(!collection_set()-&gt;is_in(i), &quot;New region should not be in collection set&quot;);
 298     }
 299 
 300     // Initialize to complete
 301     _marking_context-&gt;mark_complete();
 302 
 303     _free_set-&gt;rebuild();
 304   }
 305 
 306   if (ShenandoahAlwaysPreTouch) {
 307     assert(!AlwaysPreTouch, &quot;Should have been overridden&quot;);
 308 
 309     // For NUMA, it is important to pre-touch the storage under bitmaps with worker threads,
 310     // before initialize() below zeroes it with initializing thread. For any given region,
 311     // we touch the region and the corresponding bitmaps from the same thread.
 312     ShenandoahPushWorkerScope scope(workers(), _max_workers, false);
 313 
</pre>
<hr />
<pre>
 344 
 345   _liveness_cache = NEW_C_HEAP_ARRAY(jushort*, _max_workers, mtGC);
 346   for (uint worker = 0; worker &lt; _max_workers; worker++) {
 347     _liveness_cache[worker] = NEW_C_HEAP_ARRAY(jushort, _num_regions, mtGC);
 348     Copy::fill_to_bytes(_liveness_cache[worker], _num_regions * sizeof(jushort));
 349   }
 350 
 351   // There should probably be Shenandoah-specific options for these,
 352   // just as there are G1-specific options.
 353   {
 354     ShenandoahSATBMarkQueueSet&amp; satbqs = ShenandoahBarrierSet::satb_mark_queue_set();
 355     satbqs.set_process_completed_buffers_threshold(20); // G1SATBProcessCompletedThreshold
 356     satbqs.set_buffer_enqueue_threshold_percentage(60); // G1SATBBufferEnqueueingThresholdPercent
 357   }
 358 
 359   _monitoring_support = new ShenandoahMonitoringSupport(this);
 360   _phase_timings = new ShenandoahPhaseTimings();
 361   ShenandoahStringDedup::initialize();
 362   ShenandoahCodeRoots::initialize();
 363 
<span class="line-removed"> 364   if (ShenandoahAllocationTrace) {</span>
<span class="line-removed"> 365     _alloc_tracker = new ShenandoahAllocTracker();</span>
<span class="line-removed"> 366   }</span>
<span class="line-removed"> 367 </span>
 368   if (ShenandoahPacing) {
 369     _pacer = new ShenandoahPacer(this);
 370     _pacer-&gt;setup_for_idle();
 371   } else {
 372     _pacer = NULL;
 373   }
 374 
 375   _traversal_gc = strcmp(ShenandoahGCMode, &quot;traversal&quot;) == 0 ?
<span class="line-modified"> 376                   new ShenandoahTraversalGC(this, _num_regions) :</span>
 377                   NULL;
 378 
 379   _control_thread = new ShenandoahControlThread();
 380 
 381   log_info(gc, init)(&quot;Initialize Shenandoah heap: &quot; SIZE_FORMAT &quot;%s initial, &quot; SIZE_FORMAT &quot;%s min, &quot; SIZE_FORMAT &quot;%s max&quot;,
 382                      byte_size_in_proper_unit(_initial_size),  proper_unit_for_byte_size(_initial_size),
 383                      byte_size_in_proper_unit(_minimum_size),  proper_unit_for_byte_size(_minimum_size),
 384                      byte_size_in_proper_unit(max_capacity()), proper_unit_for_byte_size(max_capacity())
 385   );
 386 
 387   log_info(gc, init)(&quot;Safepointing mechanism: %s&quot;,
 388                      SafepointMechanism::uses_thread_local_poll() ? &quot;thread-local poll&quot; :
 389                      (SafepointMechanism::uses_global_page_poll() ? &quot;global-page poll&quot; : &quot;unknown&quot;));
 390 
 391   return JNI_OK;
 392 }
 393 
 394 void ShenandoahHeap::initialize_heuristics() {
 395   if (ShenandoahGCMode != NULL) {
 396     if (strcmp(ShenandoahGCMode, &quot;traversal&quot;) == 0) {
</pre>
<hr />
<pre>
 432   _initial_size(0),
 433   _used(0),
 434   _committed(0),
 435   _bytes_allocated_since_gc_start(0),
 436   _max_workers(MAX2(ConcGCThreads, ParallelGCThreads)),
 437   _workers(NULL),
 438   _safepoint_workers(NULL),
 439   _heap_region_special(false),
 440   _num_regions(0),
 441   _regions(NULL),
 442   _update_refs_iterator(this),
 443   _control_thread(NULL),
 444   _shenandoah_policy(policy),
 445   _heuristics(NULL),
 446   _free_set(NULL),
 447   _scm(new ShenandoahConcurrentMark()),
 448   _traversal_gc(NULL),
 449   _full_gc(new ShenandoahMarkCompact()),
 450   _pacer(NULL),
 451   _verifier(NULL),
<span class="line-removed"> 452   _alloc_tracker(NULL),</span>
 453   _phase_timings(NULL),
 454   _monitoring_support(NULL),
 455   _memory_pool(NULL),
 456   _stw_memory_manager(&quot;Shenandoah Pauses&quot;, &quot;end of GC pause&quot;),
 457   _cycle_memory_manager(&quot;Shenandoah Cycles&quot;, &quot;end of GC cycle&quot;),
 458   _gc_timer(new (ResourceObj::C_HEAP, mtGC) ConcurrentGCTimer()),
 459   _soft_ref_policy(),
 460   _log_min_obj_alignment_in_bytes(LogMinObjAlignmentInBytes),
 461   _ref_processor(NULL),
 462   _marking_context(NULL),
 463   _bitmap_size(0),
 464   _bitmap_regions_per_slice(0),
 465   _bitmap_bytes_per_slice(0),
 466   _bitmap_region_special(false),
 467   _aux_bitmap_region_special(false),
 468   _liveness_cache(NULL),
 469   _collection_set(NULL)
 470 {


 471   log_info(gc, init)(&quot;GC threads: &quot; UINT32_FORMAT &quot; parallel, &quot; UINT32_FORMAT &quot; concurrent&quot;, ParallelGCThreads, ConcGCThreads);
 472   log_info(gc, init)(&quot;Reference processing: %s&quot;, ParallelRefProcEnabled ? &quot;parallel&quot; : &quot;serial&quot;);
 473 
 474   BarrierSet::set_barrier_set(new ShenandoahBarrierSet(this));
 475 
 476   _max_workers = MAX2(_max_workers, 1U);
 477   _workers = new ShenandoahWorkGang(&quot;Shenandoah GC Threads&quot;, _max_workers,
 478                             /* are_GC_task_threads */ true,
 479                             /* are_ConcurrentGC_threads */ true);
 480   if (_workers == NULL) {
 481     vm_exit_during_initialization(&quot;Failed necessary allocation.&quot;);
 482   } else {
 483     _workers-&gt;initialize_workers();
 484   }
 485 
<span class="line-modified"> 486   if (ShenandoahParallelSafepointThreads &gt; 1) {</span>
 487     _safepoint_workers = new ShenandoahWorkGang(&quot;Safepoint Cleanup Thread&quot;,
<span class="line-modified"> 488                                                 ShenandoahParallelSafepointThreads,</span>
 489                       /* are_GC_task_threads */ false,
 490                  /* are_ConcurrentGC_threads */ false);
 491     _safepoint_workers-&gt;initialize_workers();
 492   }
 493 }
 494 
 495 #ifdef _MSC_VER
 496 #pragma warning( pop )
 497 #endif
 498 
 499 class ShenandoahResetBitmapTask : public AbstractGangTask {
 500 private:
 501   ShenandoahRegionIterator _regions;
 502 
 503 public:
 504   ShenandoahResetBitmapTask() :
 505     AbstractGangTask(&quot;Parallel Reset Bitmap Task&quot;) {}
 506 
 507   void work(uint worker_id) {
 508     ShenandoahHeapRegion* region = _regions.next();
</pre>
<hr />
<pre>
 599   _scm-&gt;initialize(_max_workers);
 600   _full_gc-&gt;initialize(_gc_timer);
 601 
 602   ref_processing_init();
 603 
 604   _heuristics-&gt;initialize();
 605 
 606   JFR_ONLY(ShenandoahJFRSupport::register_jfr_type_serializers());
 607 }
 608 
 609 size_t ShenandoahHeap::used() const {
 610   return Atomic::load_acquire(&amp;_used);
 611 }
 612 
 613 size_t ShenandoahHeap::committed() const {
 614   OrderAccess::acquire();
 615   return _committed;
 616 }
 617 
 618 void ShenandoahHeap::increase_committed(size_t bytes) {
<span class="line-modified"> 619   assert_heaplock_or_safepoint();</span>
 620   _committed += bytes;
 621 }
 622 
 623 void ShenandoahHeap::decrease_committed(size_t bytes) {
<span class="line-modified"> 624   assert_heaplock_or_safepoint();</span>
 625   _committed -= bytes;
 626 }
 627 
 628 void ShenandoahHeap::increase_used(size_t bytes) {
 629   Atomic::add(&amp;_used, bytes);
 630 }
 631 
 632 void ShenandoahHeap::set_used(size_t bytes) {
 633   Atomic::release_store_fence(&amp;_used, bytes);
 634 }
 635 
 636 void ShenandoahHeap::decrease_used(size_t bytes) {
 637   assert(used() &gt;= bytes, &quot;never decrease heap size by more than we&#39;ve left&quot;);
 638   Atomic::sub(&amp;_used, bytes);
 639 }
 640 
 641 void ShenandoahHeap::increase_allocated(size_t bytes) {
 642   Atomic::add(&amp;_bytes_allocated_since_gc_start, bytes);
 643 }
 644 
</pre>
<hr />
<pre>
 767     *actual_size = req.actual_size();
 768   } else {
 769     *actual_size = 0;
 770   }
 771   return res;
 772 }
 773 
 774 HeapWord* ShenandoahHeap::allocate_new_gclab(size_t min_size,
 775                                              size_t word_size,
 776                                              size_t* actual_size) {
 777   ShenandoahAllocRequest req = ShenandoahAllocRequest::for_gclab(min_size, word_size);
 778   HeapWord* res = allocate_memory(req);
 779   if (res != NULL) {
 780     *actual_size = req.actual_size();
 781   } else {
 782     *actual_size = 0;
 783   }
 784   return res;
 785 }
 786 
<span class="line-removed"> 787 ShenandoahHeap* ShenandoahHeap::heap() {</span>
<span class="line-removed"> 788   CollectedHeap* heap = Universe::heap();</span>
<span class="line-removed"> 789   assert(heap != NULL, &quot;Unitialized access to ShenandoahHeap::heap()&quot;);</span>
<span class="line-removed"> 790   assert(heap-&gt;kind() == CollectedHeap::Shenandoah, &quot;not a shenandoah heap&quot;);</span>
<span class="line-removed"> 791   return (ShenandoahHeap*) heap;</span>
<span class="line-removed"> 792 }</span>
<span class="line-removed"> 793 </span>
<span class="line-removed"> 794 ShenandoahHeap* ShenandoahHeap::heap_no_check() {</span>
<span class="line-removed"> 795   CollectedHeap* heap = Universe::heap();</span>
<span class="line-removed"> 796   return (ShenandoahHeap*) heap;</span>
<span class="line-removed"> 797 }</span>
<span class="line-removed"> 798 </span>
 799 HeapWord* ShenandoahHeap::allocate_memory(ShenandoahAllocRequest&amp; req) {
<span class="line-removed"> 800   ShenandoahAllocTrace trace_alloc(req.size(), req.type());</span>
<span class="line-removed"> 801 </span>
 802   intptr_t pacer_epoch = 0;
 803   bool in_new_region = false;
 804   HeapWord* result = NULL;
 805 
 806   if (req.is_mutator_alloc()) {
 807     if (ShenandoahPacing) {
 808       pacer()-&gt;pace_for_alloc(req.size());
 809       pacer_epoch = pacer()-&gt;epoch();
 810     }
 811 
 812     if (!ShenandoahAllocFailureALot || !should_inject_alloc_failure()) {
 813       result = allocate_memory_under_lock(req, in_new_region);
 814     }
 815 
 816     // Allocation failed, block until control thread reacted, then retry allocation.
 817     //
 818     // It might happen that one of the threads requesting allocation would unblock
 819     // way later after GC happened, only to fail the second allocation, because
 820     // other threads have already depleted the free storage. In this case, a better
 821     // strategy is to try again, as long as GC makes progress.
</pre>
<hr />
<pre>
 982     }
 983   }
 984 };
 985 
 986 void ShenandoahHeap::trash_cset_regions() {
 987   ShenandoahHeapLocker locker(lock());
 988 
 989   ShenandoahCollectionSet* set = collection_set();
 990   ShenandoahHeapRegion* r;
 991   set-&gt;clear_current_index();
 992   while ((r = set-&gt;next()) != NULL) {
 993     r-&gt;make_trash();
 994   }
 995   collection_set()-&gt;clear();
 996 }
 997 
 998 void ShenandoahHeap::print_heap_regions_on(outputStream* st) const {
 999   st-&gt;print_cr(&quot;Heap Regions:&quot;);
1000   st-&gt;print_cr(&quot;EU=empty-uncommitted, EC=empty-committed, R=regular, H=humongous start, HC=humongous continuation, CS=collection set, T=trash, P=pinned&quot;);
1001   st-&gt;print_cr(&quot;BTE=bottom/top/end, U=used, T=TLAB allocs, G=GCLAB allocs, S=shared allocs, L=live data&quot;);
<span class="line-modified">1002   st-&gt;print_cr(&quot;R=root, CP=critical pins, TAMS=top-at-mark-start (previous, next)&quot;);</span>
<span class="line-modified">1003   st-&gt;print_cr(&quot;SN=alloc sequence numbers (first mutator, last mutator, first gc, last gc)&quot;);</span>
1004 
1005   for (size_t i = 0; i &lt; num_regions(); i++) {
1006     get_region(i)-&gt;print_on(st);
1007   }
1008 }
1009 
1010 void ShenandoahHeap::trash_humongous_region_at(ShenandoahHeapRegion* start) {
1011   assert(start-&gt;is_humongous_start(), &quot;reclaim regions starting with the first one&quot;);
1012 
1013   oop humongous_obj = oop(start-&gt;bottom());
1014   size_t size = humongous_obj-&gt;size();
1015   size_t required_regions = ShenandoahHeapRegion::required_regions(size * HeapWordSize);
1016   size_t index = start-&gt;region_number() + required_regions - 1;
1017 
1018   assert(!start-&gt;has_live(), &quot;liveness must be zero&quot;);
1019 
1020   for(size_t i = 0; i &lt; required_regions; i++) {
1021     // Reclaim from tail. Otherwise, assertion fails when printing region to trace log,
1022     // as it expects that every region belongs to a humongous region starting with a humongous start region.
1023     ShenandoahHeapRegion* region = get_region(index --);
</pre>
<hr />
<pre>
1118   }
1119 };
1120 
1121 void ShenandoahHeap::retire_and_reset_gclabs() {
1122   ShenandoahRetireAndResetGCLABClosure cl;
1123   for (JavaThreadIteratorWithHandle jtiwh; JavaThread *t = jtiwh.next(); ) {
1124     cl.do_thread(t);
1125   }
1126   workers()-&gt;threads_do(&amp;cl);
1127 }
1128 
1129 void ShenandoahHeap::collect(GCCause::Cause cause) {
1130   control_thread()-&gt;request_gc(cause);
1131 }
1132 
1133 void ShenandoahHeap::do_full_collection(bool clear_all_soft_refs) {
1134   //assert(false, &quot;Shouldn&#39;t need to do full collections&quot;);
1135 }
1136 
1137 HeapWord* ShenandoahHeap::block_start(const void* addr) const {
<span class="line-modified">1138   Space* sp = heap_region_containing(addr);</span>
<span class="line-modified">1139   if (sp != NULL) {</span>
<span class="line-modified">1140     return sp-&gt;block_start(addr);</span>
1141   }
1142   return NULL;
1143 }
1144 
1145 bool ShenandoahHeap::block_is_obj(const HeapWord* addr) const {
<span class="line-modified">1146   Space* sp = heap_region_containing(addr);</span>
<span class="line-modified">1147   return sp-&gt;block_is_obj(addr);</span>
1148 }
1149 
1150 bool ShenandoahHeap::print_location(outputStream* st, void* addr) const {
1151   return BlockLocationPrinter&lt;ShenandoahHeap&gt;::print_location(st, addr);
1152 }
1153 
1154 jlong ShenandoahHeap::millis_since_last_gc() {
1155   double v = heuristics()-&gt;time_since_last_gc() * 1000;
1156   assert(0 &lt;= v &amp;&amp; v &lt;= max_jlong, &quot;value should fit: %f&quot;, v);
1157   return (jlong)v;
1158 }
1159 
1160 void ShenandoahHeap::prepare_for_verify() {
1161   if (SafepointSynchronize::is_at_safepoint() || ! UseTLAB) {
1162     make_parsable(false);
1163   }
1164 }
1165 
1166 void ShenandoahHeap::print_gc_threads_on(outputStream* st) const {
1167   workers()-&gt;print_worker_threads_on(st);
</pre>
<hr />
<pre>
1185   if (lt.is_enabled()) {
1186     ResourceMark rm;
1187     LogStream ls(lt);
1188 
1189     phase_timings()-&gt;print_on(&amp;ls);
1190 
1191     ls.cr();
1192     ls.cr();
1193 
1194     shenandoah_policy()-&gt;print_gc_stats(&amp;ls);
1195 
1196     ls.cr();
1197     ls.cr();
1198 
1199     if (ShenandoahPacing) {
1200       pacer()-&gt;print_on(&amp;ls);
1201     }
1202 
1203     ls.cr();
1204     ls.cr();
<span class="line-removed">1205 </span>
<span class="line-removed">1206     if (ShenandoahAllocationTrace) {</span>
<span class="line-removed">1207       assert(alloc_tracker() != NULL, &quot;Must be&quot;);</span>
<span class="line-removed">1208       alloc_tracker()-&gt;print_on(&amp;ls);</span>
<span class="line-removed">1209     } else {</span>
<span class="line-removed">1210       ls.print_cr(&quot;  Allocation tracing is disabled, use -XX:+ShenandoahAllocationTrace to enable.&quot;);</span>
<span class="line-removed">1211     }</span>
1212   }
1213 }
1214 
1215 void ShenandoahHeap::verify(VerifyOption vo) {
1216   if (ShenandoahSafepoint::is_at_shenandoah_safepoint()) {
1217     if (ShenandoahVerify) {
1218       verifier()-&gt;verify_generic(vo);
1219     } else {
1220       // TODO: Consider allocating verification bitmaps on demand,
1221       // and turn this on unconditionally.
1222     }
1223   }
1224 }
1225 size_t ShenandoahHeap::tlab_capacity(Thread *thr) const {
1226   return _free_set-&gt;capacity();
1227 }
1228 
1229 class ObjectIterateScanRootClosure : public BasicOopIterateClosure {
1230 private:
1231   MarkBitMap* _bitmap;
1232   Stack&lt;oop,mtGC&gt;* _oop_stack;


1233 
1234   template &lt;class T&gt;
1235   void do_oop_work(T* p) {
1236     T o = RawAccess&lt;&gt;::oop_load(p);
1237     if (!CompressedOops::is_null(o)) {
1238       oop obj = CompressedOops::decode_not_null(o);
<span class="line-modified">1239       oop fwd = (oop) ShenandoahForwarding::get_forwardee_raw_unchecked(obj);</span>
<span class="line-modified">1240       if (fwd == NULL) {</span>
<span class="line-modified">1241         // There is an odd interaction with VM_HeapWalkOperation, see jvmtiTagMap.cpp.</span>
<span class="line-removed">1242         //</span>
<span class="line-removed">1243         // That operation walks the reachable objects on its own, storing the marking</span>
<span class="line-removed">1244         // wavefront in the object marks. When it is done, it calls the CollectedHeap</span>
<span class="line-removed">1245         // to iterate over all objects to clean up the mess. When it reaches here,</span>
<span class="line-removed">1246         // the Shenandoah fwdptr resolution code encounters the marked objects with</span>
<span class="line-removed">1247         // NULL forwardee. Trying to act on that would crash the VM. Or fail the</span>
<span class="line-removed">1248         // asserts, should we go for resolve_forwarded_pointer(obj).</span>
<span class="line-removed">1249         //</span>
<span class="line-removed">1250         // Therefore, we have to dodge it by doing the raw access to forwardee, and</span>
<span class="line-removed">1251         // assuming the object had no forwardee, if that thing is NULL.</span>
<span class="line-removed">1252       } else {</span>
<span class="line-removed">1253         obj = fwd;</span>
1254       }


1255       assert(oopDesc::is_oop(obj), &quot;must be a valid oop&quot;);
1256       if (!_bitmap-&gt;is_marked(obj)) {
1257         _bitmap-&gt;mark(obj);
1258         _oop_stack-&gt;push(obj);
1259       }
1260     }
1261   }
1262 public:
1263   ObjectIterateScanRootClosure(MarkBitMap* bitmap, Stack&lt;oop,mtGC&gt;* oop_stack) :
<span class="line-modified">1264     _bitmap(bitmap), _oop_stack(oop_stack) {}</span>

1265   void do_oop(oop* p)       { do_oop_work(p); }
1266   void do_oop(narrowOop* p) { do_oop_work(p); }
1267 };
1268 
1269 /*
1270  * This is public API, used in preparation of object_iterate().
1271  * Since we don&#39;t do linear scan of heap in object_iterate() (see comment below), we don&#39;t
1272  * need to make the heap parsable. For Shenandoah-internal linear heap scans that we can
1273  * control, we call SH::make_tlabs_parsable().
1274  */
1275 void ShenandoahHeap::ensure_parsability(bool retire_tlabs) {
1276   // No-op.
1277 }
1278 
1279 /*
1280  * Iterates objects in the heap. This is public API, used for, e.g., heap dumping.
1281  *
1282  * We cannot safely iterate objects by doing a linear scan at random points in time. Linear
1283  * scanning needs to deal with dead objects, which may have dead Klass* pointers (e.g.
1284  * calling oopDesc::size() would crash) or dangling reference fields (crashes) etc. Linear
</pre>
<hr />
<pre>
1290  * For all those reasons, we implement object iteration as a single marking traversal, reporting
1291  * objects as we mark+traverse through the heap, starting from GC roots. JVMTI IterateThroughHeap
1292  * is allowed to report dead objects, but is not required to do so.
1293  */
1294 void ShenandoahHeap::object_iterate(ObjectClosure* cl) {
1295   assert(SafepointSynchronize::is_at_safepoint(), &quot;safe iteration is only available during safepoints&quot;);
1296   if (!_aux_bitmap_region_special &amp;&amp; !os::commit_memory((char*)_aux_bitmap_region.start(), _aux_bitmap_region.byte_size(), false)) {
1297     log_warning(gc)(&quot;Could not commit native memory for auxiliary marking bitmap for heap iteration&quot;);
1298     return;
1299   }
1300 
1301   // Reset bitmap
1302   _aux_bit_map.clear();
1303 
1304   Stack&lt;oop,mtGC&gt; oop_stack;
1305 
1306   // First, we process GC roots according to current GC cycle. This populates the work stack with initial objects.
1307   ShenandoahHeapIterationRootScanner rp;
1308   ObjectIterateScanRootClosure oops(&amp;_aux_bit_map, &amp;oop_stack);
1309 
<span class="line-modified">1310   // When concurrent root is in progress, weak roots may contain dead oops, they should not be used</span>
<span class="line-removed">1311   // for root scanning.</span>
<span class="line-removed">1312   if (is_concurrent_root_in_progress()) {</span>
<span class="line-removed">1313     rp.strong_roots_do(&amp;oops);</span>
<span class="line-removed">1314   } else {</span>
<span class="line-removed">1315     rp.roots_do(&amp;oops);</span>
<span class="line-removed">1316   }</span>
1317 
1318   // Work through the oop stack to traverse heap.
1319   while (! oop_stack.is_empty()) {
1320     oop obj = oop_stack.pop();
1321     assert(oopDesc::is_oop(obj), &quot;must be a valid oop&quot;);
1322     cl-&gt;do_object(obj);
1323     obj-&gt;oop_iterate(&amp;oops);
1324   }
1325 
1326   assert(oop_stack.is_empty(), &quot;should be empty&quot;);
1327 
1328   if (!_aux_bitmap_region_special &amp;&amp; !os::uncommit_memory((char*)_aux_bitmap_region.start(), _aux_bitmap_region.byte_size())) {
1329     log_warning(gc)(&quot;Could not uncommit native memory for auxiliary marking bitmap for heap iteration&quot;);
1330   }
1331 }
1332 
1333 // Keep alive an object that was loaded with AS_NO_KEEPALIVE.
1334 void ShenandoahHeap::keep_alive(oop obj) {
1335   if (is_concurrent_mark_in_progress()) {
1336     ShenandoahBarrierSet::barrier_set()-&gt;enqueue(obj);
1337   }
1338 }
1339 
1340 void ShenandoahHeap::heap_region_iterate(ShenandoahHeapRegionClosure* blk) const {
1341   for (size_t i = 0; i &lt; num_regions(); i++) {
1342     ShenandoahHeapRegion* current = get_region(i);
1343     blk-&gt;heap_region_do(current);
1344   }
1345 }
1346 
1347 class ShenandoahParallelHeapRegionTask : public AbstractGangTask {
1348 private:
1349   ShenandoahHeap* const _heap;
1350   ShenandoahHeapRegionClosure* const _blk;
1351 
<span class="line-modified">1352   DEFINE_PAD_MINUS_SIZE(0, DEFAULT_CACHE_LINE_SIZE, sizeof(volatile size_t));</span>
1353   volatile size_t _index;
<span class="line-modified">1354   DEFINE_PAD_MINUS_SIZE(1, DEFAULT_CACHE_LINE_SIZE, 0);</span>
1355 
1356 public:
1357   ShenandoahParallelHeapRegionTask(ShenandoahHeapRegionClosure* blk) :
1358           AbstractGangTask(&quot;Parallel Region Task&quot;),
1359           _heap(ShenandoahHeap::heap()), _blk(blk), _index(0) {}
1360 
1361   void work(uint worker_id) {
1362     size_t stride = ShenandoahParallelRegionStride;
1363 
1364     size_t max = _heap-&gt;num_regions();
1365     while (_index &lt; max) {
1366       size_t cur = Atomic::fetch_and_add(&amp;_index, stride);
1367       size_t start = cur;
1368       size_t end = MIN2(cur + stride, max);
1369       if (start &gt;= max) break;
1370 
1371       for (size_t i = cur; i &lt; end; i++) {
1372         ShenandoahHeapRegion* current = _heap-&gt;get_region(i);
1373         _blk-&gt;heap_region_do(current);
1374       }
</pre>
<hr />
<pre>
1395   void heap_region_do(ShenandoahHeapRegion* r) {
1396     if (r-&gt;is_active()) {
1397       r-&gt;clear_live_data();
1398       _ctx-&gt;capture_top_at_mark_start(r);
1399     } else {
1400       assert(!r-&gt;has_live(), &quot;Region &quot; SIZE_FORMAT &quot; should have no live data&quot;, r-&gt;region_number());
1401       assert(_ctx-&gt;top_at_mark_start(r) == r-&gt;top(),
1402              &quot;Region &quot; SIZE_FORMAT &quot; should already have correct TAMS&quot;, r-&gt;region_number());
1403     }
1404   }
1405 
1406   bool is_thread_safe() { return true; }
1407 };
1408 
1409 void ShenandoahHeap::op_init_mark() {
1410   assert(ShenandoahSafepoint::is_at_shenandoah_safepoint(), &quot;Should be at safepoint&quot;);
1411   assert(Thread::current()-&gt;is_VM_thread(), &quot;can only do this in VMThread&quot;);
1412 
1413   assert(marking_context()-&gt;is_bitmap_clear(), &quot;need clear marking bitmap&quot;);
1414   assert(!marking_context()-&gt;is_complete(), &quot;should not be complete&quot;);

1415 
1416   if (ShenandoahVerify) {
1417     verifier()-&gt;verify_before_concmark();
1418   }
1419 
1420   if (VerifyBeforeGC) {
1421     Universe::verify();
1422   }
1423 
1424   set_concurrent_mark_in_progress(true);
1425   // We need to reset all TLABs because we&#39;d lose marks on all objects allocated in them.
1426   {
1427     ShenandoahGCPhase phase(ShenandoahPhaseTimings::make_parsable);
1428     make_parsable(true);
1429   }
1430 
1431   {
1432     ShenandoahGCPhase phase(ShenandoahPhaseTimings::clear_liveness);
1433     ShenandoahClearLivenessClosure clc;
1434     parallel_heap_region_iterate(&amp;clc);
1435   }
1436 
1437   // Make above changes visible to worker threads
1438   OrderAccess::fence();
1439 
1440   concurrent_mark()-&gt;mark_roots(ShenandoahPhaseTimings::scan_roots);
1441 
1442   if (UseTLAB) {
1443     ShenandoahGCPhase phase(ShenandoahPhaseTimings::resize_tlabs);
1444     resize_tlabs();
1445   }
1446 
1447   if (ShenandoahPacing) {
1448     pacer()-&gt;setup_for_mark();
1449   }







1450 }
1451 
1452 void ShenandoahHeap::op_mark() {
1453   concurrent_mark()-&gt;mark_from_roots();
1454 }
1455 
1456 class ShenandoahCompleteLivenessClosure : public ShenandoahHeapRegionClosure {
1457 private:
1458   ShenandoahMarkingContext* const _ctx;
1459 public:
1460   ShenandoahCompleteLivenessClosure() : _ctx(ShenandoahHeap::heap()-&gt;complete_marking_context()) {}
1461 
1462   void heap_region_do(ShenandoahHeapRegion* r) {
1463     if (r-&gt;is_active()) {
1464       HeapWord *tams = _ctx-&gt;top_at_mark_start(r);
1465       HeapWord *top = r-&gt;top();
1466       if (top &gt; tams) {
1467         r-&gt;increase_live_data_alloc_words(pointer_delta(top, tams));
1468       }
1469     } else {
1470       assert(!r-&gt;has_live(), &quot;Region &quot; SIZE_FORMAT &quot; should have no live data&quot;, r-&gt;region_number());
1471       assert(_ctx-&gt;top_at_mark_start(r) == r-&gt;top(),
1472              &quot;Region &quot; SIZE_FORMAT &quot; should have correct TAMS&quot;, r-&gt;region_number());
1473     }
1474   }
1475 
1476   bool is_thread_safe() { return true; }
1477 };
1478 
1479 void ShenandoahHeap::op_final_mark() {
1480   assert(ShenandoahSafepoint::is_at_shenandoah_safepoint(), &quot;Should be at safepoint&quot;);

1481 
1482   // It is critical that we
1483   // evacuate roots right after finishing marking, so that we don&#39;t
1484   // get unmarked objects in the roots.
1485 
1486   if (!cancelled_gc()) {
1487     concurrent_mark()-&gt;finish_mark_from_roots(/* full_gc = */ false);
1488 
1489     // Marking is completed, deactivate SATB barrier
1490     set_concurrent_mark_in_progress(false);
1491     mark_complete_marking_context();
1492 
1493     parallel_cleaning(false /* full gc*/);
1494 
<span class="line-removed">1495     if (has_forwarded_objects()) {</span>
<span class="line-removed">1496       // Degen may be caused by failed evacuation of roots</span>
<span class="line-removed">1497       if (is_degenerated_gc_in_progress()) {</span>
<span class="line-removed">1498         concurrent_mark()-&gt;update_roots(ShenandoahPhaseTimings::degen_gc_update_roots);</span>
<span class="line-removed">1499       } else {</span>
<span class="line-removed">1500         concurrent_mark()-&gt;update_thread_roots(ShenandoahPhaseTimings::update_roots);</span>
<span class="line-removed">1501       }</span>
<span class="line-removed">1502       set_has_forwarded_objects(false);</span>
<span class="line-removed">1503    }</span>
<span class="line-removed">1504 </span>
1505     if (ShenandoahVerify) {
1506       verifier()-&gt;verify_roots_no_forwarded();
1507     }
1508     // All allocations past TAMS are implicitly live, adjust the region data.
1509     // Bitmaps/TAMS are swapped at this point, so we need to poll complete bitmap.
1510     {
1511       ShenandoahGCPhase phase(ShenandoahPhaseTimings::complete_liveness);
1512       ShenandoahCompleteLivenessClosure cl;
1513       parallel_heap_region_iterate(&amp;cl);
1514     }
1515 
1516     // Force the threads to reacquire their TLABs outside the collection set.
1517     {
1518       ShenandoahGCPhase phase(ShenandoahPhaseTimings::retire_tlabs);
1519       make_parsable(true);
1520     }
1521 
1522     // We are about to select the collection set, make sure it knows about
1523     // current pinning status. Also, this allows trashing more regions that
1524     // now have their pinning status dropped.
</pre>
<hr />
<pre>
1542 
1543       heuristics()-&gt;choose_collection_set(_collection_set);
1544 
1545       _free_set-&gt;rebuild();
1546     }
1547 
1548     if (!is_degenerated_gc_in_progress()) {
1549       prepare_concurrent_roots();
1550       prepare_concurrent_unloading();
1551     }
1552 
1553     // If collection set has candidates, start evacuation.
1554     // Otherwise, bypass the rest of the cycle.
1555     if (!collection_set()-&gt;is_empty()) {
1556       ShenandoahGCPhase init_evac(ShenandoahPhaseTimings::init_evac);
1557 
1558       if (ShenandoahVerify) {
1559         verifier()-&gt;verify_before_evacuation();
1560       }
1561 







1562       set_evacuation_in_progress(true);
1563       // From here on, we need to update references.
1564       set_has_forwarded_objects(true);
1565 
1566       if (!is_degenerated_gc_in_progress()) {
1567         if (ShenandoahConcurrentRoots::should_do_concurrent_class_unloading()) {
1568           ShenandoahCodeRoots::arm_nmethods();
1569         }
1570         evacuate_and_update_roots();
1571       }
1572 
1573       if (ShenandoahPacing) {
1574         pacer()-&gt;setup_for_evac();
1575       }
1576 
1577       if (ShenandoahVerify) {
1578         ShenandoahRootVerifier::RootTypes types = ShenandoahRootVerifier::None;
1579         if (ShenandoahConcurrentRoots::should_do_concurrent_roots()) {
1580           types = ShenandoahRootVerifier::combine(ShenandoahRootVerifier::JNIHandleRoots, ShenandoahRootVerifier::WeakRoots);
1581           types = ShenandoahRootVerifier::combine(types, ShenandoahRootVerifier::CLDGRoots);
</pre>
<hr />
<pre>
1597         Universe::verify();
1598       }
1599     }
1600 
1601   } else {
1602     // If this cycle was updating references, we need to keep the has_forwarded_objects
1603     // flag on, for subsequent phases to deal with it.
1604     concurrent_mark()-&gt;cancel();
1605     set_concurrent_mark_in_progress(false);
1606 
1607     if (process_references()) {
1608       // Abandon reference processing right away: pre-cleaning must have failed.
1609       ReferenceProcessor *rp = ref_processor();
1610       rp-&gt;disable_discovery();
1611       rp-&gt;abandon_partial_discovery();
1612       rp-&gt;verify_no_references_recorded();
1613     }
1614   }
1615 }
1616 
<span class="line-removed">1617 void ShenandoahHeap::op_final_evac() {</span>
<span class="line-removed">1618   assert(ShenandoahSafepoint::is_at_shenandoah_safepoint(), &quot;Should be at safepoint&quot;);</span>
<span class="line-removed">1619 </span>
<span class="line-removed">1620   set_evacuation_in_progress(false);</span>
<span class="line-removed">1621 </span>
<span class="line-removed">1622   {</span>
<span class="line-removed">1623     ShenandoahGCPhase phase(ShenandoahPhaseTimings::final_evac_retire_gclabs);</span>
<span class="line-removed">1624     retire_and_reset_gclabs();</span>
<span class="line-removed">1625   }</span>
<span class="line-removed">1626 </span>
<span class="line-removed">1627   if (ShenandoahVerify) {</span>
<span class="line-removed">1628     verifier()-&gt;verify_after_evacuation();</span>
<span class="line-removed">1629   }</span>
<span class="line-removed">1630 </span>
<span class="line-removed">1631   if (VerifyAfterGC) {</span>
<span class="line-removed">1632     Universe::verify();</span>
<span class="line-removed">1633   }</span>
<span class="line-removed">1634 }</span>
<span class="line-removed">1635 </span>
1636 void ShenandoahHeap::op_conc_evac() {
1637   ShenandoahEvacuationTask task(this, _collection_set, true);
1638   workers()-&gt;run_task(&amp;task);
1639 }
1640 
1641 void ShenandoahHeap::op_stw_evac() {
1642   ShenandoahEvacuationTask task(this, _collection_set, false);
1643   workers()-&gt;run_task(&amp;task);
1644 }
1645 
1646 void ShenandoahHeap::op_updaterefs() {
1647   update_heap_references(true);
1648 }
1649 
1650 void ShenandoahHeap::op_cleanup() {
1651   free_set()-&gt;recycle_trash();
1652 }
1653 
1654 class ShenandoahConcurrentRootsEvacUpdateTask : public AbstractGangTask {
1655 private:
</pre>
<hr />
<pre>
1910         cancel_gc(GCCause::_shenandoah_upgrade_to_full_gc);
1911         op_degenerated_fail();
1912         return;
1913       }
1914 
1915       op_reset();
1916 
1917       op_init_mark();
1918       if (cancelled_gc()) {
1919         op_degenerated_fail();
1920         return;
1921       }
1922 
1923     case _degenerated_mark:
1924       op_final_mark();
1925       if (cancelled_gc()) {
1926         op_degenerated_fail();
1927         return;
1928       }
1929 







1930       op_cleanup();
1931 
1932     case _degenerated_evac:
1933       // If heuristics thinks we should do the cycle, this flag would be set,
1934       // and we can do evacuation. Otherwise, it would be the shortcut cycle.
1935       if (is_evacuation_in_progress()) {
1936 
1937         // Degeneration under oom-evac protocol might have left some objects in
1938         // collection set un-evacuated. Restart evacuation from the beginning to
1939         // capture all objects. For all the objects that are already evacuated,
1940         // it would be a simple check, which is supposed to be fast. This is also
1941         // safe to do even without degeneration, as CSet iterator is at beginning
1942         // in preparation for evacuation anyway.
1943         //
1944         // Before doing that, we need to make sure we never had any cset-pinned
1945         // regions. This may happen if allocation failure happened when evacuating
1946         // the about-to-be-pinned object, oom-evac protocol left the object in
1947         // the collection set, and then the pin reached the cset region. If we continue
1948         // the cycle here, we would trash the cset and alive objects in it. To avoid
1949         // it, we fail degeneration right away and slide into Full GC to recover.
</pre>
<hr />
<pre>
2186                             ShenandoahPhaseTimings::purge_cldg);
2187     ClassLoaderDataGraph::purge();
2188   }
2189   // Resize and verify metaspace
2190   MetaspaceGC::compute_new_size();
2191   MetaspaceUtils::verify_metrics();
2192 }
2193 
2194 // Weak roots are either pre-evacuated (final mark) or updated (final updaterefs),
2195 // so they should not have forwarded oops.
2196 // However, we do need to &quot;null&quot; dead oops in the roots, if can not be done
2197 // in concurrent cycles.
2198 void ShenandoahHeap::stw_process_weak_roots(bool full_gc) {
2199   ShenandoahGCPhase root_phase(full_gc ?
2200                                ShenandoahPhaseTimings::full_gc_purge :
2201                                ShenandoahPhaseTimings::purge);
2202   uint num_workers = _workers-&gt;active_workers();
2203   ShenandoahPhaseTimings::Phase timing_phase = full_gc ?
2204                                                ShenandoahPhaseTimings::full_gc_purge_par :
2205                                                ShenandoahPhaseTimings::purge_par;
<span class="line-removed">2206   // Cleanup weak roots</span>
2207   ShenandoahGCPhase phase(timing_phase);
<span class="line-modified">2208   phase_timings()-&gt;record_workers_start(timing_phase);</span>


2209   if (has_forwarded_objects()) {
<span class="line-modified">2210     if (is_traversal_mode()) {</span>
<span class="line-modified">2211       ShenandoahForwardedIsAliveClosure is_alive;</span>
<span class="line-modified">2212       ShenandoahTraversalUpdateRefsClosure keep_alive;</span>
<span class="line-modified">2213       ShenandoahParallelWeakRootsCleaningTask&lt;ShenandoahForwardedIsAliveClosure, ShenandoahTraversalUpdateRefsClosure&gt;</span>
<span class="line-modified">2214         cleaning_task(&amp;is_alive, &amp;keep_alive, num_workers, !ShenandoahConcurrentRoots::should_do_concurrent_class_unloading());</span>
<span class="line-removed">2215       _workers-&gt;run_task(&amp;cleaning_task);</span>
<span class="line-removed">2216     } else {</span>
<span class="line-removed">2217       ShenandoahForwardedIsAliveClosure is_alive;</span>
<span class="line-removed">2218       ShenandoahUpdateRefsClosure keep_alive;</span>
<span class="line-removed">2219       ShenandoahParallelWeakRootsCleaningTask&lt;ShenandoahForwardedIsAliveClosure, ShenandoahUpdateRefsClosure&gt;</span>
<span class="line-removed">2220         cleaning_task(&amp;is_alive, &amp;keep_alive, num_workers, !ShenandoahConcurrentRoots::should_do_concurrent_class_unloading());</span>
<span class="line-removed">2221       _workers-&gt;run_task(&amp;cleaning_task);</span>
<span class="line-removed">2222     }</span>
2223   } else {
2224     ShenandoahIsAliveClosure is_alive;
2225 #ifdef ASSERT
2226     ShenandoahAssertNotForwardedClosure verify_cl;
2227     ShenandoahParallelWeakRootsCleaningTask&lt;ShenandoahIsAliveClosure, ShenandoahAssertNotForwardedClosure&gt;
2228       cleaning_task(&amp;is_alive, &amp;verify_cl, num_workers, !ShenandoahConcurrentRoots::should_do_concurrent_class_unloading());
2229 #else
2230     ShenandoahParallelWeakRootsCleaningTask&lt;ShenandoahIsAliveClosure, DoNothingClosure&gt;
2231       cleaning_task(&amp;is_alive, &amp;do_nothing_cl, num_workers, !ShenandoahConcurrentRoots::should_do_concurrent_class_unloading());
2232 #endif
2233     _workers-&gt;run_task(&amp;cleaning_task);
2234   }
<span class="line-removed">2235   phase_timings()-&gt;record_workers_end(timing_phase);</span>
2236 }
2237 
2238 void ShenandoahHeap::parallel_cleaning(bool full_gc) {
2239   assert(SafepointSynchronize::is_at_safepoint(), &quot;Must be at a safepoint&quot;);
2240   stw_process_weak_roots(full_gc);
2241   if (!ShenandoahConcurrentRoots::should_do_concurrent_class_unloading()) {
2242     stw_unload_classes(full_gc);
2243   }
2244 }
2245 
2246 void ShenandoahHeap::set_has_forwarded_objects(bool cond) {
2247   if (is_traversal_mode()) {
2248     set_gc_state_mask(HAS_FORWARDED | UPDATEREFS, cond);
2249   } else {
2250     set_gc_state_mask(HAS_FORWARDED, cond);
2251   }
2252 
2253 }
2254 
2255 void ShenandoahHeap::set_process_references(bool pr) {
</pre>
<hr />
<pre>
2372 
2373 void ShenandoahHeap::prepare_concurrent_unloading() {
2374   assert(SafepointSynchronize::is_at_safepoint(), &quot;Must be at a safepoint&quot;);
2375   if (ShenandoahConcurrentRoots::should_do_concurrent_class_unloading()) {
2376     _unloader.prepare();
2377   }
2378 }
2379 
2380 void ShenandoahHeap::finish_concurrent_unloading() {
2381   assert(SafepointSynchronize::is_at_safepoint(), &quot;Must be at a safepoint&quot;);
2382   if (ShenandoahConcurrentRoots::should_do_concurrent_class_unloading()) {
2383     _unloader.finish();
2384   }
2385 }
2386 
2387 #ifdef ASSERT
2388 void ShenandoahHeap::assert_gc_workers(uint nworkers) {
2389   assert(nworkers &gt; 0 &amp;&amp; nworkers &lt;= max_workers(), &quot;Sanity&quot;);
2390 
2391   if (ShenandoahSafepoint::is_at_shenandoah_safepoint()) {
<span class="line-modified">2392     if (UseDynamicNumberOfGCThreads ||</span>
<span class="line-removed">2393         (FLAG_IS_DEFAULT(ParallelGCThreads) &amp;&amp; ForceDynamicNumberOfGCThreads)) {</span>
2394       assert(nworkers &lt;= ParallelGCThreads, &quot;Cannot use more than it has&quot;);
2395     } else {
2396       // Use ParallelGCThreads inside safepoints
<span class="line-modified">2397       assert(nworkers == ParallelGCThreads, &quot;Use ParalleGCThreads within safepoints&quot;);</span>
2398     }
2399   } else {
<span class="line-modified">2400     if (UseDynamicNumberOfGCThreads ||</span>
<span class="line-removed">2401         (FLAG_IS_DEFAULT(ConcGCThreads) &amp;&amp; ForceDynamicNumberOfGCThreads)) {</span>
2402       assert(nworkers &lt;= ConcGCThreads, &quot;Cannot use more than it has&quot;);
2403     } else {
2404       // Use ConcGCThreads outside safepoints
2405       assert(nworkers == ConcGCThreads, &quot;Use ConcGCThreads outside safepoints&quot;);
2406     }
2407   }
2408 }
2409 #endif
2410 
2411 ShenandoahVerifier* ShenandoahHeap::verifier() {
2412   guarantee(ShenandoahVerify, &quot;Should be enabled&quot;);
2413   assert (_verifier != NULL, &quot;sanity&quot;);
2414   return _verifier;
2415 }
2416 
2417 template&lt;class T&gt;
2418 class ShenandoahUpdateHeapRefsTask : public AbstractGangTask {
2419 private:
2420   T cl;
2421   ShenandoahHeap* _heap;
</pre>
<hr />
<pre>
2429     _regions(regions),
2430     _concurrent(concurrent) {
2431   }
2432 
2433   void work(uint worker_id) {
2434     if (_concurrent) {
2435       ShenandoahConcurrentWorkerSession worker_session(worker_id);
2436       ShenandoahSuspendibleThreadSetJoiner stsj(ShenandoahSuspendibleWorkers);
2437       do_work();
2438     } else {
2439       ShenandoahParallelWorkerSession worker_session(worker_id);
2440       do_work();
2441     }
2442   }
2443 
2444 private:
2445   void do_work() {
2446     ShenandoahHeapRegion* r = _regions-&gt;next();
2447     ShenandoahMarkingContext* const ctx = _heap-&gt;complete_marking_context();
2448     while (r != NULL) {
<span class="line-modified">2449       HeapWord* top_at_start_ur = r-&gt;concurrent_iteration_safe_limit();</span>
<span class="line-modified">2450       assert (top_at_start_ur &gt;= r-&gt;bottom(), &quot;sanity&quot;);</span>
2451       if (r-&gt;is_active() &amp;&amp; !r-&gt;is_cset()) {
<span class="line-modified">2452         _heap-&gt;marked_object_oop_iterate(r, &amp;cl, top_at_start_ur);</span>
2453       }
2454       if (ShenandoahPacing) {
<span class="line-modified">2455         _heap-&gt;pacer()-&gt;report_updaterefs(pointer_delta(top_at_start_ur, r-&gt;bottom()));</span>
2456       }
2457       if (_heap-&gt;check_cancelled_gc_and_yield(_concurrent)) {
2458         return;
2459       }
2460       r = _regions-&gt;next();
2461     }
2462   }
2463 };
2464 
2465 void ShenandoahHeap::update_heap_references(bool concurrent) {
2466   ShenandoahUpdateHeapRefsTask&lt;ShenandoahUpdateHeapRefsClosure&gt; task(&amp;_update_refs_iterator, concurrent);
2467   workers()-&gt;run_task(&amp;task);
2468 }
2469 
2470 void ShenandoahHeap::op_init_updaterefs() {
2471   assert(ShenandoahSafepoint::is_at_shenandoah_safepoint(), &quot;must be at safepoint&quot;);
2472 
2473   set_evacuation_in_progress(false);
2474 
2475   {
2476     ShenandoahGCPhase phase(ShenandoahPhaseTimings::init_update_refs_retire_gclabs);
2477     retire_and_reset_gclabs();
2478   }
2479 
2480   if (ShenandoahVerify) {
2481     if (!is_degenerated_gc_in_progress()) {
2482       verifier()-&gt;verify_roots_in_to_space_except(ShenandoahRootVerifier::ThreadRoots);
2483     }
2484     verifier()-&gt;verify_before_updaterefs();
2485   }
2486 
2487   set_update_refs_in_progress(true);
2488 
2489   {
2490     ShenandoahGCPhase phase(ShenandoahPhaseTimings::init_update_refs_prepare);
2491 
2492     make_parsable(true);
<span class="line-removed">2493     for (uint i = 0; i &lt; num_regions(); i++) {</span>
<span class="line-removed">2494       ShenandoahHeapRegion* r = get_region(i);</span>
<span class="line-removed">2495       r-&gt;set_concurrent_iteration_safe_limit(r-&gt;top());</span>
<span class="line-removed">2496     }</span>
2497 
2498     // Reset iterator.
2499     _update_refs_iterator.reset();
2500   }
2501 
2502   if (ShenandoahPacing) {
2503     pacer()-&gt;setup_for_updaterefs();
2504   }
2505 }
2506 
2507 void ShenandoahHeap::op_final_updaterefs() {
2508   assert(ShenandoahSafepoint::is_at_shenandoah_safepoint(), &quot;must be at safepoint&quot;);
2509 
2510   finish_concurrent_unloading();
2511 
2512   // Check if there is left-over work, and finish it
2513   if (_update_refs_iterator.has_next()) {
2514     ShenandoahGCPhase phase(ShenandoahPhaseTimings::final_update_refs_finish_work);
2515 
2516     // Finish updating references where we left off.
</pre>
<hr />
<pre>
2552     trash_cset_regions();
2553   }
2554 
2555   set_has_forwarded_objects(false);
2556   set_update_refs_in_progress(false);
2557 
2558   if (ShenandoahVerify) {
2559     verifier()-&gt;verify_after_updaterefs();
2560   }
2561 
2562   if (VerifyAfterGC) {
2563     Universe::verify();
2564   }
2565 
2566   {
2567     ShenandoahHeapLocker locker(lock());
2568     _free_set-&gt;rebuild();
2569   }
2570 }
2571 
<span class="line-removed">2572 #ifdef ASSERT</span>
<span class="line-removed">2573 void ShenandoahHeap::assert_heaplock_owned_by_current_thread() {</span>
<span class="line-removed">2574   _lock.assert_owned_by_current_thread();</span>
<span class="line-removed">2575 }</span>
<span class="line-removed">2576 </span>
<span class="line-removed">2577 void ShenandoahHeap::assert_heaplock_not_owned_by_current_thread() {</span>
<span class="line-removed">2578   _lock.assert_not_owned_by_current_thread();</span>
<span class="line-removed">2579 }</span>
<span class="line-removed">2580 </span>
<span class="line-removed">2581 void ShenandoahHeap::assert_heaplock_or_safepoint() {</span>
<span class="line-removed">2582   _lock.assert_owned_by_current_thread_or_safepoint();</span>
<span class="line-removed">2583 }</span>
<span class="line-removed">2584 #endif</span>
<span class="line-removed">2585 </span>
2586 void ShenandoahHeap::print_extended_on(outputStream *st) const {
2587   print_on(st);
2588   print_heap_regions_on(st);
2589 }
2590 
2591 bool ShenandoahHeap::is_bitmap_slice_committed(ShenandoahHeapRegion* r, bool skip_self) {
2592   size_t slice = r-&gt;region_number() / _bitmap_regions_per_slice;
2593 
2594   size_t regions_from = _bitmap_regions_per_slice * slice;
2595   size_t regions_to   = MIN2(num_regions(), _bitmap_regions_per_slice * (slice + 1));
2596   for (size_t g = regions_from; g &lt; regions_to; g++) {
2597     assert (g / _bitmap_regions_per_slice == slice, &quot;same slice&quot;);
2598     if (skip_self &amp;&amp; g == r-&gt;region_number()) continue;
2599     if (get_region(g)-&gt;is_committed()) {
2600       return true;
2601     }
2602   }
2603   return false;
2604 }
2605 
2606 bool ShenandoahHeap::commit_bitmap_slice(ShenandoahHeapRegion* r) {
<span class="line-modified">2607   assert_heaplock_owned_by_current_thread();</span>
2608 
2609   // Bitmaps in special regions do not need commits
2610   if (_bitmap_region_special) {
2611     return true;
2612   }
2613 
2614   if (is_bitmap_slice_committed(r, true)) {
2615     // Some other region from the group is already committed, meaning the bitmap
2616     // slice is already committed, we exit right away.
2617     return true;
2618   }
2619 
2620   // Commit the bitmap slice:
2621   size_t slice = r-&gt;region_number() / _bitmap_regions_per_slice;
2622   size_t off = _bitmap_bytes_per_slice * slice;
2623   size_t len = _bitmap_bytes_per_slice;
2624   if (!os::commit_memory((char*)_bitmap_region.start() + off, len, false)) {
2625     return false;
2626   }
2627   return true;
2628 }
2629 
2630 bool ShenandoahHeap::uncommit_bitmap_slice(ShenandoahHeapRegion *r) {
<span class="line-modified">2631   assert_heaplock_owned_by_current_thread();</span>
2632 
2633   // Bitmaps in special regions do not need uncommits
2634   if (_bitmap_region_special) {
2635     return true;
2636   }
2637 
2638   if (is_bitmap_slice_committed(r, true)) {
2639     // Some other region from the group is still committed, meaning the bitmap
2640     // slice is should stay committed, exit right away.
2641     return true;
2642   }
2643 
2644   // Uncommit the bitmap slice:
2645   size_t slice = r-&gt;region_number() / _bitmap_regions_per_slice;
2646   size_t off = _bitmap_bytes_per_slice * slice;
2647   size_t len = _bitmap_bytes_per_slice;
2648   if (!os::uncommit_memory((char*)_bitmap_region.start() + off, len)) {
2649     return false;
2650   }
2651   return true;
</pre>
<hr />
<pre>
2666 void ShenandoahHeap::vmop_entry_init_mark() {
2667   TraceCollectorStats tcs(monitoring_support()-&gt;stw_collection_counters());
2668   ShenandoahGCPhase total(ShenandoahPhaseTimings::total_pause_gross);
2669   ShenandoahGCPhase phase(ShenandoahPhaseTimings::init_mark_gross);
2670 
2671   try_inject_alloc_failure();
2672   VM_ShenandoahInitMark op;
2673   VMThread::execute(&amp;op); // jump to entry_init_mark() under safepoint
2674 }
2675 
2676 void ShenandoahHeap::vmop_entry_final_mark() {
2677   TraceCollectorStats tcs(monitoring_support()-&gt;stw_collection_counters());
2678   ShenandoahGCPhase total(ShenandoahPhaseTimings::total_pause_gross);
2679   ShenandoahGCPhase phase(ShenandoahPhaseTimings::final_mark_gross);
2680 
2681   try_inject_alloc_failure();
2682   VM_ShenandoahFinalMarkStartEvac op;
2683   VMThread::execute(&amp;op); // jump to entry_final_mark under safepoint
2684 }
2685 
<span class="line-removed">2686 void ShenandoahHeap::vmop_entry_final_evac() {</span>
<span class="line-removed">2687   TraceCollectorStats tcs(monitoring_support()-&gt;stw_collection_counters());</span>
<span class="line-removed">2688   ShenandoahGCPhase total(ShenandoahPhaseTimings::total_pause_gross);</span>
<span class="line-removed">2689   ShenandoahGCPhase phase(ShenandoahPhaseTimings::final_evac_gross);</span>
<span class="line-removed">2690 </span>
<span class="line-removed">2691   VM_ShenandoahFinalEvac op;</span>
<span class="line-removed">2692   VMThread::execute(&amp;op); // jump to entry_final_evac under safepoint</span>
<span class="line-removed">2693 }</span>
<span class="line-removed">2694 </span>
2695 void ShenandoahHeap::vmop_entry_init_updaterefs() {
2696   TraceCollectorStats tcs(monitoring_support()-&gt;stw_collection_counters());
2697   ShenandoahGCPhase total(ShenandoahPhaseTimings::total_pause_gross);
2698   ShenandoahGCPhase phase(ShenandoahPhaseTimings::init_update_refs_gross);
2699 
2700   try_inject_alloc_failure();
2701   VM_ShenandoahInitUpdateRefs op;
2702   VMThread::execute(&amp;op);
2703 }
2704 
2705 void ShenandoahHeap::vmop_entry_final_updaterefs() {
2706   TraceCollectorStats tcs(monitoring_support()-&gt;stw_collection_counters());
2707   ShenandoahGCPhase total(ShenandoahPhaseTimings::total_pause_gross);
2708   ShenandoahGCPhase phase(ShenandoahPhaseTimings::final_update_refs_gross);
2709 
2710   try_inject_alloc_failure();
2711   VM_ShenandoahFinalUpdateRefs op;
2712   VMThread::execute(&amp;op);
2713 }
2714 
</pre>
<hr />
<pre>
2762                               ShenandoahWorkerPolicy::calc_workers_for_init_marking(),
2763                               &quot;init marking&quot;);
2764 
2765   op_init_mark();
2766 }
2767 
2768 void ShenandoahHeap::entry_final_mark() {
2769   ShenandoahGCPhase total_phase(ShenandoahPhaseTimings::total_pause);
2770   ShenandoahGCPhase phase(ShenandoahPhaseTimings::final_mark);
2771   const char* msg = final_mark_event_message();
2772   GCTraceTime(Info, gc) time(msg, gc_timer());
2773   EventMark em(&quot;%s&quot;, msg);
2774 
2775   ShenandoahWorkerScope scope(workers(),
2776                               ShenandoahWorkerPolicy::calc_workers_for_final_marking(),
2777                               &quot;final marking&quot;);
2778 
2779   op_final_mark();
2780 }
2781 
<span class="line-removed">2782 void ShenandoahHeap::entry_final_evac() {</span>
<span class="line-removed">2783   ShenandoahGCPhase total_phase(ShenandoahPhaseTimings::total_pause);</span>
<span class="line-removed">2784   ShenandoahGCPhase phase(ShenandoahPhaseTimings::final_evac);</span>
<span class="line-removed">2785   static const char* msg = &quot;Pause Final Evac&quot;;</span>
<span class="line-removed">2786   GCTraceTime(Info, gc) time(msg, gc_timer());</span>
<span class="line-removed">2787   EventMark em(&quot;%s&quot;, msg);</span>
<span class="line-removed">2788 </span>
<span class="line-removed">2789   op_final_evac();</span>
<span class="line-removed">2790 }</span>
<span class="line-removed">2791 </span>
2792 void ShenandoahHeap::entry_init_updaterefs() {
2793   ShenandoahGCPhase total_phase(ShenandoahPhaseTimings::total_pause);
2794   ShenandoahGCPhase phase(ShenandoahPhaseTimings::init_update_refs);
2795 
2796   static const char* msg = &quot;Pause Init Update Refs&quot;;
2797   GCTraceTime(Info, gc) time(msg, gc_timer());
2798   EventMark em(&quot;%s&quot;, msg);
2799 
2800   // No workers used in this phase, no setup required
2801 
2802   op_init_updaterefs();
2803 }
2804 
2805 void ShenandoahHeap::entry_final_updaterefs() {
2806   ShenandoahGCPhase total_phase(ShenandoahPhaseTimings::total_pause);
2807   ShenandoahGCPhase phase(ShenandoahPhaseTimings::final_update_refs);
2808 
2809   static const char* msg = &quot;Pause Final Update Refs&quot;;
2810   GCTraceTime(Info, gc) time(msg, gc_timer());
2811   EventMark em(&quot;%s&quot;, msg);
</pre>
<hr />
<pre>
3069   _index = 0;
3070 }
3071 
3072 bool ShenandoahRegionIterator::has_next() const {
3073   return _index &lt; _heap-&gt;num_regions();
3074 }
3075 
3076 char ShenandoahHeap::gc_state() const {
3077   return _gc_state.raw_value();
3078 }
3079 
3080 void ShenandoahHeap::deduplicate_string(oop str) {
3081   assert(java_lang_String::is_instance(str), &quot;invariant&quot;);
3082 
3083   if (ShenandoahStringDedup::is_enabled()) {
3084     ShenandoahStringDedup::deduplicate(str);
3085   }
3086 }
3087 
3088 const char* ShenandoahHeap::init_mark_event_message() const {
<span class="line-modified">3089   bool update_refs = has_forwarded_objects();</span>

3090   bool proc_refs = process_references();
3091   bool unload_cls = unload_classes();
3092 
<span class="line-modified">3093   if (update_refs &amp;&amp; proc_refs &amp;&amp; unload_cls) {</span>
<span class="line-removed">3094     return &quot;Pause Init Mark (update refs) (process weakrefs) (unload classes)&quot;;</span>
<span class="line-removed">3095   } else if (update_refs &amp;&amp; proc_refs) {</span>
<span class="line-removed">3096     return &quot;Pause Init Mark (update refs) (process weakrefs)&quot;;</span>
<span class="line-removed">3097   } else if (update_refs &amp;&amp; unload_cls) {</span>
<span class="line-removed">3098     return &quot;Pause Init Mark (update refs) (unload classes)&quot;;</span>
<span class="line-removed">3099   } else if (proc_refs &amp;&amp; unload_cls) {</span>
3100     return &quot;Pause Init Mark (process weakrefs) (unload classes)&quot;;
<span class="line-removed">3101   } else if (update_refs) {</span>
<span class="line-removed">3102     return &quot;Pause Init Mark (update refs)&quot;;</span>
3103   } else if (proc_refs) {
3104     return &quot;Pause Init Mark (process weakrefs)&quot;;
3105   } else if (unload_cls) {
3106     return &quot;Pause Init Mark (unload classes)&quot;;
3107   } else {
3108     return &quot;Pause Init Mark&quot;;
3109   }
3110 }
3111 
3112 const char* ShenandoahHeap::final_mark_event_message() const {
<span class="line-modified">3113   bool update_refs = has_forwarded_objects();</span>

3114   bool proc_refs = process_references();
3115   bool unload_cls = unload_classes();
3116 
<span class="line-modified">3117   if (update_refs &amp;&amp; proc_refs &amp;&amp; unload_cls) {</span>
<span class="line-removed">3118     return &quot;Pause Final Mark (update refs) (process weakrefs) (unload classes)&quot;;</span>
<span class="line-removed">3119   } else if (update_refs &amp;&amp; proc_refs) {</span>
<span class="line-removed">3120     return &quot;Pause Final Mark (update refs) (process weakrefs)&quot;;</span>
<span class="line-removed">3121   } else if (update_refs &amp;&amp; unload_cls) {</span>
<span class="line-removed">3122     return &quot;Pause Final Mark (update refs) (unload classes)&quot;;</span>
<span class="line-removed">3123   } else if (proc_refs &amp;&amp; unload_cls) {</span>
3124     return &quot;Pause Final Mark (process weakrefs) (unload classes)&quot;;
<span class="line-removed">3125   } else if (update_refs) {</span>
<span class="line-removed">3126     return &quot;Pause Final Mark (update refs)&quot;;</span>
3127   } else if (proc_refs) {
3128     return &quot;Pause Final Mark (process weakrefs)&quot;;
3129   } else if (unload_cls) {
3130     return &quot;Pause Final Mark (unload classes)&quot;;
3131   } else {
3132     return &quot;Pause Final Mark&quot;;
3133   }
3134 }
3135 
3136 const char* ShenandoahHeap::conc_mark_event_message() const {
<span class="line-modified">3137   bool update_refs = has_forwarded_objects();</span>

3138   bool proc_refs = process_references();
3139   bool unload_cls = unload_classes();
3140 
<span class="line-modified">3141   if (update_refs &amp;&amp; proc_refs &amp;&amp; unload_cls) {</span>
<span class="line-removed">3142     return &quot;Concurrent marking (update refs) (process weakrefs) (unload classes)&quot;;</span>
<span class="line-removed">3143   } else if (update_refs &amp;&amp; proc_refs) {</span>
<span class="line-removed">3144     return &quot;Concurrent marking (update refs) (process weakrefs)&quot;;</span>
<span class="line-removed">3145   } else if (update_refs &amp;&amp; unload_cls) {</span>
<span class="line-removed">3146     return &quot;Concurrent marking (update refs) (unload classes)&quot;;</span>
<span class="line-removed">3147   } else if (proc_refs &amp;&amp; unload_cls) {</span>
3148     return &quot;Concurrent marking (process weakrefs) (unload classes)&quot;;
<span class="line-removed">3149   } else if (update_refs) {</span>
<span class="line-removed">3150     return &quot;Concurrent marking (update refs)&quot;;</span>
3151   } else if (proc_refs) {
3152     return &quot;Concurrent marking (process weakrefs)&quot;;
3153   } else if (unload_cls) {
3154     return &quot;Concurrent marking (unload classes)&quot;;
3155   } else {
3156     return &quot;Concurrent marking&quot;;
3157   }
3158 }
3159 
3160 const char* ShenandoahHeap::init_traversal_event_message() const {
3161   bool proc_refs = process_references();
3162   bool unload_cls = unload_classes();
3163 
3164   if (proc_refs &amp;&amp; unload_cls) {
3165     return &quot;Pause Init Traversal (process weakrefs) (unload classes)&quot;;
3166   } else if (proc_refs) {
3167     return &quot;Pause Init Traversal (process weakrefs)&quot;;
3168   } else if (unload_cls) {
3169     return &quot;Pause Init Traversal (unload classes)&quot;;
3170   } else {
</pre>
</td>
<td>
<hr />
<pre>
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #include &quot;precompiled.hpp&quot;
  26 #include &quot;memory/allocation.hpp&quot;
  27 #include &quot;memory/universe.hpp&quot;
  28 
  29 #include &quot;gc/shared/gcArguments.hpp&quot;
  30 #include &quot;gc/shared/gcTimer.hpp&quot;
  31 #include &quot;gc/shared/gcTraceTime.inline.hpp&quot;
  32 #include &quot;gc/shared/locationPrinter.inline.hpp&quot;
  33 #include &quot;gc/shared/memAllocator.hpp&quot;
  34 #include &quot;gc/shared/oopStorageSet.hpp&quot;
  35 #include &quot;gc/shared/plab.hpp&quot;
  36 

  37 #include &quot;gc/shenandoah/shenandoahBarrierSet.hpp&quot;
  38 #include &quot;gc/shenandoah/shenandoahClosures.inline.hpp&quot;
  39 #include &quot;gc/shenandoah/shenandoahCollectionSet.hpp&quot;
  40 #include &quot;gc/shenandoah/shenandoahCollectorPolicy.hpp&quot;
  41 #include &quot;gc/shenandoah/shenandoahConcurrentMark.inline.hpp&quot;
  42 #include &quot;gc/shenandoah/shenandoahConcurrentRoots.hpp&quot;
  43 #include &quot;gc/shenandoah/shenandoahControlThread.hpp&quot;
  44 #include &quot;gc/shenandoah/shenandoahFreeSet.hpp&quot;
  45 #include &quot;gc/shenandoah/shenandoahPhaseTimings.hpp&quot;
  46 #include &quot;gc/shenandoah/shenandoahHeap.inline.hpp&quot;
  47 #include &quot;gc/shenandoah/shenandoahHeapRegion.hpp&quot;
  48 #include &quot;gc/shenandoah/shenandoahHeapRegionSet.hpp&quot;
  49 #include &quot;gc/shenandoah/shenandoahMarkCompact.hpp&quot;
  50 #include &quot;gc/shenandoah/shenandoahMarkingContext.inline.hpp&quot;
  51 #include &quot;gc/shenandoah/shenandoahMemoryPool.hpp&quot;
  52 #include &quot;gc/shenandoah/shenandoahMetrics.hpp&quot;
  53 #include &quot;gc/shenandoah/shenandoahMonitoringSupport.hpp&quot;
  54 #include &quot;gc/shenandoah/shenandoahNormalMode.hpp&quot;
  55 #include &quot;gc/shenandoah/shenandoahOopClosures.inline.hpp&quot;
  56 #include &quot;gc/shenandoah/shenandoahPacer.inline.hpp&quot;
<span class="line-added">  57 #include &quot;gc/shenandoah/shenandoahPadding.hpp&quot;</span>
  58 #include &quot;gc/shenandoah/shenandoahParallelCleaning.inline.hpp&quot;
  59 #include &quot;gc/shenandoah/shenandoahPassiveMode.hpp&quot;
  60 #include &quot;gc/shenandoah/shenandoahRootProcessor.inline.hpp&quot;
  61 #include &quot;gc/shenandoah/shenandoahStringDedup.hpp&quot;
  62 #include &quot;gc/shenandoah/shenandoahTaskqueue.hpp&quot;
  63 #include &quot;gc/shenandoah/shenandoahTraversalMode.hpp&quot;
  64 #include &quot;gc/shenandoah/shenandoahUtils.hpp&quot;
  65 #include &quot;gc/shenandoah/shenandoahVerifier.hpp&quot;
  66 #include &quot;gc/shenandoah/shenandoahCodeRoots.hpp&quot;
  67 #include &quot;gc/shenandoah/shenandoahVMOperations.hpp&quot;
  68 #include &quot;gc/shenandoah/shenandoahWorkGroup.hpp&quot;
  69 #include &quot;gc/shenandoah/shenandoahWorkerPolicy.hpp&quot;
  70 #if INCLUDE_JFR
  71 #include &quot;gc/shenandoah/shenandoahJfrSupport.hpp&quot;
  72 #endif
  73 
  74 #include &quot;memory/metaspace.hpp&quot;
  75 #include &quot;oops/compressedOops.inline.hpp&quot;
  76 #include &quot;runtime/atomic.hpp&quot;
  77 #include &quot;runtime/globals.hpp&quot;
  78 #include &quot;runtime/interfaceSupport.inline.hpp&quot;
  79 #include &quot;runtime/orderAccess.hpp&quot;
  80 #include &quot;runtime/safepointMechanism.hpp&quot;
  81 #include &quot;runtime/vmThread.hpp&quot;
  82 #include &quot;services/mallocTracker.hpp&quot;
  83 #include &quot;utilities/powerOfTwo.hpp&quot;
  84 
<span class="line-added">  85 ShenandoahHeap* ShenandoahHeap::_heap = NULL;</span>
<span class="line-added">  86 </span>
  87 #ifdef ASSERT
  88 template &lt;class T&gt;
  89 void ShenandoahAssertToSpaceClosure::do_oop_work(T* p) {
  90   T o = RawAccess&lt;&gt;::oop_load(p);
  91   if (! CompressedOops::is_null(o)) {
  92     oop obj = CompressedOops::decode_not_null(o);
  93     shenandoah_assert_not_forwarded(p, obj);
  94   }
  95 }
  96 
  97 void ShenandoahAssertToSpaceClosure::do_oop(narrowOop* p) { do_oop_work(p); }
  98 void ShenandoahAssertToSpaceClosure::do_oop(oop* p)       { do_oop_work(p); }
  99 #endif
 100 
 101 class ShenandoahPretouchHeapTask : public AbstractGangTask {
 102 private:
 103   ShenandoahRegionIterator _regions;
 104   const size_t _page_size;
 105 public:
 106   ShenandoahPretouchHeapTask(size_t page_size) :
</pre>
<hr />
<pre>
 270   }
 271 
 272   // Reserve aux bitmap for use in object_iterate(). We don&#39;t commit it here.
 273   ReservedSpace aux_bitmap(_bitmap_size, bitmap_page_size);
 274   MemTracker::record_virtual_memory_type(aux_bitmap.base(), mtGC);
 275   _aux_bitmap_region = MemRegion((HeapWord*) aux_bitmap.base(), aux_bitmap.size() / HeapWordSize);
 276   _aux_bitmap_region_special = aux_bitmap.special();
 277   _aux_bit_map.initialize(_heap_region, _aux_bitmap_region);
 278 
 279   //
 280   // Create regions and region sets
 281   //
 282 
 283   _regions = NEW_C_HEAP_ARRAY(ShenandoahHeapRegion*, _num_regions, mtGC);
 284   _free_set = new ShenandoahFreeSet(this, _num_regions);
 285   _collection_set = new ShenandoahCollectionSet(this, sh_rs.base(), sh_rs.size());
 286 
 287   {
 288     ShenandoahHeapLocker locker(lock());
 289 


 290     for (size_t i = 0; i &lt; _num_regions; i++) {
<span class="line-modified"> 291       HeapWord* start = (HeapWord*)sh_rs.base() + ShenandoahHeapRegion::region_size_words() * i;</span>
 292       bool is_committed = i &lt; num_committed_regions;
<span class="line-modified"> 293       ShenandoahHeapRegion* r = new ShenandoahHeapRegion(start, i, is_committed);</span>
 294 
 295       _marking_context-&gt;initialize_top_at_mark_start(r);
 296       _regions[i] = r;
 297       assert(!collection_set()-&gt;is_in(i), &quot;New region should not be in collection set&quot;);
 298     }
 299 
 300     // Initialize to complete
 301     _marking_context-&gt;mark_complete();
 302 
 303     _free_set-&gt;rebuild();
 304   }
 305 
 306   if (ShenandoahAlwaysPreTouch) {
 307     assert(!AlwaysPreTouch, &quot;Should have been overridden&quot;);
 308 
 309     // For NUMA, it is important to pre-touch the storage under bitmaps with worker threads,
 310     // before initialize() below zeroes it with initializing thread. For any given region,
 311     // we touch the region and the corresponding bitmaps from the same thread.
 312     ShenandoahPushWorkerScope scope(workers(), _max_workers, false);
 313 
</pre>
<hr />
<pre>
 344 
 345   _liveness_cache = NEW_C_HEAP_ARRAY(jushort*, _max_workers, mtGC);
 346   for (uint worker = 0; worker &lt; _max_workers; worker++) {
 347     _liveness_cache[worker] = NEW_C_HEAP_ARRAY(jushort, _num_regions, mtGC);
 348     Copy::fill_to_bytes(_liveness_cache[worker], _num_regions * sizeof(jushort));
 349   }
 350 
 351   // There should probably be Shenandoah-specific options for these,
 352   // just as there are G1-specific options.
 353   {
 354     ShenandoahSATBMarkQueueSet&amp; satbqs = ShenandoahBarrierSet::satb_mark_queue_set();
 355     satbqs.set_process_completed_buffers_threshold(20); // G1SATBProcessCompletedThreshold
 356     satbqs.set_buffer_enqueue_threshold_percentage(60); // G1SATBBufferEnqueueingThresholdPercent
 357   }
 358 
 359   _monitoring_support = new ShenandoahMonitoringSupport(this);
 360   _phase_timings = new ShenandoahPhaseTimings();
 361   ShenandoahStringDedup::initialize();
 362   ShenandoahCodeRoots::initialize();
 363 




 364   if (ShenandoahPacing) {
 365     _pacer = new ShenandoahPacer(this);
 366     _pacer-&gt;setup_for_idle();
 367   } else {
 368     _pacer = NULL;
 369   }
 370 
 371   _traversal_gc = strcmp(ShenandoahGCMode, &quot;traversal&quot;) == 0 ?
<span class="line-modified"> 372                   new ShenandoahTraversalGC(this) :</span>
 373                   NULL;
 374 
 375   _control_thread = new ShenandoahControlThread();
 376 
 377   log_info(gc, init)(&quot;Initialize Shenandoah heap: &quot; SIZE_FORMAT &quot;%s initial, &quot; SIZE_FORMAT &quot;%s min, &quot; SIZE_FORMAT &quot;%s max&quot;,
 378                      byte_size_in_proper_unit(_initial_size),  proper_unit_for_byte_size(_initial_size),
 379                      byte_size_in_proper_unit(_minimum_size),  proper_unit_for_byte_size(_minimum_size),
 380                      byte_size_in_proper_unit(max_capacity()), proper_unit_for_byte_size(max_capacity())
 381   );
 382 
 383   log_info(gc, init)(&quot;Safepointing mechanism: %s&quot;,
 384                      SafepointMechanism::uses_thread_local_poll() ? &quot;thread-local poll&quot; :
 385                      (SafepointMechanism::uses_global_page_poll() ? &quot;global-page poll&quot; : &quot;unknown&quot;));
 386 
 387   return JNI_OK;
 388 }
 389 
 390 void ShenandoahHeap::initialize_heuristics() {
 391   if (ShenandoahGCMode != NULL) {
 392     if (strcmp(ShenandoahGCMode, &quot;traversal&quot;) == 0) {
</pre>
<hr />
<pre>
 428   _initial_size(0),
 429   _used(0),
 430   _committed(0),
 431   _bytes_allocated_since_gc_start(0),
 432   _max_workers(MAX2(ConcGCThreads, ParallelGCThreads)),
 433   _workers(NULL),
 434   _safepoint_workers(NULL),
 435   _heap_region_special(false),
 436   _num_regions(0),
 437   _regions(NULL),
 438   _update_refs_iterator(this),
 439   _control_thread(NULL),
 440   _shenandoah_policy(policy),
 441   _heuristics(NULL),
 442   _free_set(NULL),
 443   _scm(new ShenandoahConcurrentMark()),
 444   _traversal_gc(NULL),
 445   _full_gc(new ShenandoahMarkCompact()),
 446   _pacer(NULL),
 447   _verifier(NULL),

 448   _phase_timings(NULL),
 449   _monitoring_support(NULL),
 450   _memory_pool(NULL),
 451   _stw_memory_manager(&quot;Shenandoah Pauses&quot;, &quot;end of GC pause&quot;),
 452   _cycle_memory_manager(&quot;Shenandoah Cycles&quot;, &quot;end of GC cycle&quot;),
 453   _gc_timer(new (ResourceObj::C_HEAP, mtGC) ConcurrentGCTimer()),
 454   _soft_ref_policy(),
 455   _log_min_obj_alignment_in_bytes(LogMinObjAlignmentInBytes),
 456   _ref_processor(NULL),
 457   _marking_context(NULL),
 458   _bitmap_size(0),
 459   _bitmap_regions_per_slice(0),
 460   _bitmap_bytes_per_slice(0),
 461   _bitmap_region_special(false),
 462   _aux_bitmap_region_special(false),
 463   _liveness_cache(NULL),
 464   _collection_set(NULL)
 465 {
<span class="line-added"> 466   _heap = this;</span>
<span class="line-added"> 467 </span>
 468   log_info(gc, init)(&quot;GC threads: &quot; UINT32_FORMAT &quot; parallel, &quot; UINT32_FORMAT &quot; concurrent&quot;, ParallelGCThreads, ConcGCThreads);
 469   log_info(gc, init)(&quot;Reference processing: %s&quot;, ParallelRefProcEnabled ? &quot;parallel&quot; : &quot;serial&quot;);
 470 
 471   BarrierSet::set_barrier_set(new ShenandoahBarrierSet(this));
 472 
 473   _max_workers = MAX2(_max_workers, 1U);
 474   _workers = new ShenandoahWorkGang(&quot;Shenandoah GC Threads&quot;, _max_workers,
 475                             /* are_GC_task_threads */ true,
 476                             /* are_ConcurrentGC_threads */ true);
 477   if (_workers == NULL) {
 478     vm_exit_during_initialization(&quot;Failed necessary allocation.&quot;);
 479   } else {
 480     _workers-&gt;initialize_workers();
 481   }
 482 
<span class="line-modified"> 483   if (ParallelGCThreads &gt; 1) {</span>
 484     _safepoint_workers = new ShenandoahWorkGang(&quot;Safepoint Cleanup Thread&quot;,
<span class="line-modified"> 485                                                 ParallelGCThreads,</span>
 486                       /* are_GC_task_threads */ false,
 487                  /* are_ConcurrentGC_threads */ false);
 488     _safepoint_workers-&gt;initialize_workers();
 489   }
 490 }
 491 
 492 #ifdef _MSC_VER
 493 #pragma warning( pop )
 494 #endif
 495 
 496 class ShenandoahResetBitmapTask : public AbstractGangTask {
 497 private:
 498   ShenandoahRegionIterator _regions;
 499 
 500 public:
 501   ShenandoahResetBitmapTask() :
 502     AbstractGangTask(&quot;Parallel Reset Bitmap Task&quot;) {}
 503 
 504   void work(uint worker_id) {
 505     ShenandoahHeapRegion* region = _regions.next();
</pre>
<hr />
<pre>
 596   _scm-&gt;initialize(_max_workers);
 597   _full_gc-&gt;initialize(_gc_timer);
 598 
 599   ref_processing_init();
 600 
 601   _heuristics-&gt;initialize();
 602 
 603   JFR_ONLY(ShenandoahJFRSupport::register_jfr_type_serializers());
 604 }
 605 
 606 size_t ShenandoahHeap::used() const {
 607   return Atomic::load_acquire(&amp;_used);
 608 }
 609 
 610 size_t ShenandoahHeap::committed() const {
 611   OrderAccess::acquire();
 612   return _committed;
 613 }
 614 
 615 void ShenandoahHeap::increase_committed(size_t bytes) {
<span class="line-modified"> 616   shenandoah_assert_heaplocked_or_safepoint();</span>
 617   _committed += bytes;
 618 }
 619 
 620 void ShenandoahHeap::decrease_committed(size_t bytes) {
<span class="line-modified"> 621   shenandoah_assert_heaplocked_or_safepoint();</span>
 622   _committed -= bytes;
 623 }
 624 
 625 void ShenandoahHeap::increase_used(size_t bytes) {
 626   Atomic::add(&amp;_used, bytes);
 627 }
 628 
 629 void ShenandoahHeap::set_used(size_t bytes) {
 630   Atomic::release_store_fence(&amp;_used, bytes);
 631 }
 632 
 633 void ShenandoahHeap::decrease_used(size_t bytes) {
 634   assert(used() &gt;= bytes, &quot;never decrease heap size by more than we&#39;ve left&quot;);
 635   Atomic::sub(&amp;_used, bytes);
 636 }
 637 
 638 void ShenandoahHeap::increase_allocated(size_t bytes) {
 639   Atomic::add(&amp;_bytes_allocated_since_gc_start, bytes);
 640 }
 641 
</pre>
<hr />
<pre>
 764     *actual_size = req.actual_size();
 765   } else {
 766     *actual_size = 0;
 767   }
 768   return res;
 769 }
 770 
 771 HeapWord* ShenandoahHeap::allocate_new_gclab(size_t min_size,
 772                                              size_t word_size,
 773                                              size_t* actual_size) {
 774   ShenandoahAllocRequest req = ShenandoahAllocRequest::for_gclab(min_size, word_size);
 775   HeapWord* res = allocate_memory(req);
 776   if (res != NULL) {
 777     *actual_size = req.actual_size();
 778   } else {
 779     *actual_size = 0;
 780   }
 781   return res;
 782 }
 783 












 784 HeapWord* ShenandoahHeap::allocate_memory(ShenandoahAllocRequest&amp; req) {


 785   intptr_t pacer_epoch = 0;
 786   bool in_new_region = false;
 787   HeapWord* result = NULL;
 788 
 789   if (req.is_mutator_alloc()) {
 790     if (ShenandoahPacing) {
 791       pacer()-&gt;pace_for_alloc(req.size());
 792       pacer_epoch = pacer()-&gt;epoch();
 793     }
 794 
 795     if (!ShenandoahAllocFailureALot || !should_inject_alloc_failure()) {
 796       result = allocate_memory_under_lock(req, in_new_region);
 797     }
 798 
 799     // Allocation failed, block until control thread reacted, then retry allocation.
 800     //
 801     // It might happen that one of the threads requesting allocation would unblock
 802     // way later after GC happened, only to fail the second allocation, because
 803     // other threads have already depleted the free storage. In this case, a better
 804     // strategy is to try again, as long as GC makes progress.
</pre>
<hr />
<pre>
 965     }
 966   }
 967 };
 968 
 969 void ShenandoahHeap::trash_cset_regions() {
 970   ShenandoahHeapLocker locker(lock());
 971 
 972   ShenandoahCollectionSet* set = collection_set();
 973   ShenandoahHeapRegion* r;
 974   set-&gt;clear_current_index();
 975   while ((r = set-&gt;next()) != NULL) {
 976     r-&gt;make_trash();
 977   }
 978   collection_set()-&gt;clear();
 979 }
 980 
 981 void ShenandoahHeap::print_heap_regions_on(outputStream* st) const {
 982   st-&gt;print_cr(&quot;Heap Regions:&quot;);
 983   st-&gt;print_cr(&quot;EU=empty-uncommitted, EC=empty-committed, R=regular, H=humongous start, HC=humongous continuation, CS=collection set, T=trash, P=pinned&quot;);
 984   st-&gt;print_cr(&quot;BTE=bottom/top/end, U=used, T=TLAB allocs, G=GCLAB allocs, S=shared allocs, L=live data&quot;);
<span class="line-modified"> 985   st-&gt;print_cr(&quot;R=root, CP=critical pins, TAMS=top-at-mark-start, UWM=update watermark&quot;);</span>
<span class="line-modified"> 986   st-&gt;print_cr(&quot;SN=alloc sequence number&quot;);</span>
 987 
 988   for (size_t i = 0; i &lt; num_regions(); i++) {
 989     get_region(i)-&gt;print_on(st);
 990   }
 991 }
 992 
 993 void ShenandoahHeap::trash_humongous_region_at(ShenandoahHeapRegion* start) {
 994   assert(start-&gt;is_humongous_start(), &quot;reclaim regions starting with the first one&quot;);
 995 
 996   oop humongous_obj = oop(start-&gt;bottom());
 997   size_t size = humongous_obj-&gt;size();
 998   size_t required_regions = ShenandoahHeapRegion::required_regions(size * HeapWordSize);
 999   size_t index = start-&gt;region_number() + required_regions - 1;
1000 
1001   assert(!start-&gt;has_live(), &quot;liveness must be zero&quot;);
1002 
1003   for(size_t i = 0; i &lt; required_regions; i++) {
1004     // Reclaim from tail. Otherwise, assertion fails when printing region to trace log,
1005     // as it expects that every region belongs to a humongous region starting with a humongous start region.
1006     ShenandoahHeapRegion* region = get_region(index --);
</pre>
<hr />
<pre>
1101   }
1102 };
1103 
1104 void ShenandoahHeap::retire_and_reset_gclabs() {
1105   ShenandoahRetireAndResetGCLABClosure cl;
1106   for (JavaThreadIteratorWithHandle jtiwh; JavaThread *t = jtiwh.next(); ) {
1107     cl.do_thread(t);
1108   }
1109   workers()-&gt;threads_do(&amp;cl);
1110 }
1111 
1112 void ShenandoahHeap::collect(GCCause::Cause cause) {
1113   control_thread()-&gt;request_gc(cause);
1114 }
1115 
1116 void ShenandoahHeap::do_full_collection(bool clear_all_soft_refs) {
1117   //assert(false, &quot;Shouldn&#39;t need to do full collections&quot;);
1118 }
1119 
1120 HeapWord* ShenandoahHeap::block_start(const void* addr) const {
<span class="line-modified">1121   ShenandoahHeapRegion* r = heap_region_containing(addr);</span>
<span class="line-modified">1122   if (r != NULL) {</span>
<span class="line-modified">1123     return r-&gt;block_start(addr);</span>
1124   }
1125   return NULL;
1126 }
1127 
1128 bool ShenandoahHeap::block_is_obj(const HeapWord* addr) const {
<span class="line-modified">1129   ShenandoahHeapRegion* r = heap_region_containing(addr);</span>
<span class="line-modified">1130   return r-&gt;block_is_obj(addr);</span>
1131 }
1132 
1133 bool ShenandoahHeap::print_location(outputStream* st, void* addr) const {
1134   return BlockLocationPrinter&lt;ShenandoahHeap&gt;::print_location(st, addr);
1135 }
1136 
1137 jlong ShenandoahHeap::millis_since_last_gc() {
1138   double v = heuristics()-&gt;time_since_last_gc() * 1000;
1139   assert(0 &lt;= v &amp;&amp; v &lt;= max_jlong, &quot;value should fit: %f&quot;, v);
1140   return (jlong)v;
1141 }
1142 
1143 void ShenandoahHeap::prepare_for_verify() {
1144   if (SafepointSynchronize::is_at_safepoint() || ! UseTLAB) {
1145     make_parsable(false);
1146   }
1147 }
1148 
1149 void ShenandoahHeap::print_gc_threads_on(outputStream* st) const {
1150   workers()-&gt;print_worker_threads_on(st);
</pre>
<hr />
<pre>
1168   if (lt.is_enabled()) {
1169     ResourceMark rm;
1170     LogStream ls(lt);
1171 
1172     phase_timings()-&gt;print_on(&amp;ls);
1173 
1174     ls.cr();
1175     ls.cr();
1176 
1177     shenandoah_policy()-&gt;print_gc_stats(&amp;ls);
1178 
1179     ls.cr();
1180     ls.cr();
1181 
1182     if (ShenandoahPacing) {
1183       pacer()-&gt;print_on(&amp;ls);
1184     }
1185 
1186     ls.cr();
1187     ls.cr();







1188   }
1189 }
1190 
1191 void ShenandoahHeap::verify(VerifyOption vo) {
1192   if (ShenandoahSafepoint::is_at_shenandoah_safepoint()) {
1193     if (ShenandoahVerify) {
1194       verifier()-&gt;verify_generic(vo);
1195     } else {
1196       // TODO: Consider allocating verification bitmaps on demand,
1197       // and turn this on unconditionally.
1198     }
1199   }
1200 }
1201 size_t ShenandoahHeap::tlab_capacity(Thread *thr) const {
1202   return _free_set-&gt;capacity();
1203 }
1204 
1205 class ObjectIterateScanRootClosure : public BasicOopIterateClosure {
1206 private:
1207   MarkBitMap* _bitmap;
1208   Stack&lt;oop,mtGC&gt;* _oop_stack;
<span class="line-added">1209   ShenandoahHeap* const _heap;</span>
<span class="line-added">1210   ShenandoahMarkingContext* const _marking_context;</span>
1211 
1212   template &lt;class T&gt;
1213   void do_oop_work(T* p) {
1214     T o = RawAccess&lt;&gt;::oop_load(p);
1215     if (!CompressedOops::is_null(o)) {
1216       oop obj = CompressedOops::decode_not_null(o);
<span class="line-modified">1217       if (_heap-&gt;is_concurrent_root_in_progress() &amp;&amp; !_marking_context-&gt;is_marked(obj)) {</span>
<span class="line-modified">1218         // There may be dead oops in weak roots in concurrent root phase, do not touch them.</span>
<span class="line-modified">1219         return;</span>












1220       }
<span class="line-added">1221       obj = ShenandoahBarrierSet::resolve_forwarded_not_null(obj);</span>
<span class="line-added">1222 </span>
1223       assert(oopDesc::is_oop(obj), &quot;must be a valid oop&quot;);
1224       if (!_bitmap-&gt;is_marked(obj)) {
1225         _bitmap-&gt;mark(obj);
1226         _oop_stack-&gt;push(obj);
1227       }
1228     }
1229   }
1230 public:
1231   ObjectIterateScanRootClosure(MarkBitMap* bitmap, Stack&lt;oop,mtGC&gt;* oop_stack) :
<span class="line-modified">1232     _bitmap(bitmap), _oop_stack(oop_stack), _heap(ShenandoahHeap::heap()),</span>
<span class="line-added">1233     _marking_context(_heap-&gt;marking_context()) {}</span>
1234   void do_oop(oop* p)       { do_oop_work(p); }
1235   void do_oop(narrowOop* p) { do_oop_work(p); }
1236 };
1237 
1238 /*
1239  * This is public API, used in preparation of object_iterate().
1240  * Since we don&#39;t do linear scan of heap in object_iterate() (see comment below), we don&#39;t
1241  * need to make the heap parsable. For Shenandoah-internal linear heap scans that we can
1242  * control, we call SH::make_tlabs_parsable().
1243  */
1244 void ShenandoahHeap::ensure_parsability(bool retire_tlabs) {
1245   // No-op.
1246 }
1247 
1248 /*
1249  * Iterates objects in the heap. This is public API, used for, e.g., heap dumping.
1250  *
1251  * We cannot safely iterate objects by doing a linear scan at random points in time. Linear
1252  * scanning needs to deal with dead objects, which may have dead Klass* pointers (e.g.
1253  * calling oopDesc::size() would crash) or dangling reference fields (crashes) etc. Linear
</pre>
<hr />
<pre>
1259  * For all those reasons, we implement object iteration as a single marking traversal, reporting
1260  * objects as we mark+traverse through the heap, starting from GC roots. JVMTI IterateThroughHeap
1261  * is allowed to report dead objects, but is not required to do so.
1262  */
1263 void ShenandoahHeap::object_iterate(ObjectClosure* cl) {
1264   assert(SafepointSynchronize::is_at_safepoint(), &quot;safe iteration is only available during safepoints&quot;);
1265   if (!_aux_bitmap_region_special &amp;&amp; !os::commit_memory((char*)_aux_bitmap_region.start(), _aux_bitmap_region.byte_size(), false)) {
1266     log_warning(gc)(&quot;Could not commit native memory for auxiliary marking bitmap for heap iteration&quot;);
1267     return;
1268   }
1269 
1270   // Reset bitmap
1271   _aux_bit_map.clear();
1272 
1273   Stack&lt;oop,mtGC&gt; oop_stack;
1274 
1275   // First, we process GC roots according to current GC cycle. This populates the work stack with initial objects.
1276   ShenandoahHeapIterationRootScanner rp;
1277   ObjectIterateScanRootClosure oops(&amp;_aux_bit_map, &amp;oop_stack);
1278 
<span class="line-modified">1279   rp.roots_do(&amp;oops);</span>






1280 
1281   // Work through the oop stack to traverse heap.
1282   while (! oop_stack.is_empty()) {
1283     oop obj = oop_stack.pop();
1284     assert(oopDesc::is_oop(obj), &quot;must be a valid oop&quot;);
1285     cl-&gt;do_object(obj);
1286     obj-&gt;oop_iterate(&amp;oops);
1287   }
1288 
1289   assert(oop_stack.is_empty(), &quot;should be empty&quot;);
1290 
1291   if (!_aux_bitmap_region_special &amp;&amp; !os::uncommit_memory((char*)_aux_bitmap_region.start(), _aux_bitmap_region.byte_size())) {
1292     log_warning(gc)(&quot;Could not uncommit native memory for auxiliary marking bitmap for heap iteration&quot;);
1293   }
1294 }
1295 
1296 // Keep alive an object that was loaded with AS_NO_KEEPALIVE.
1297 void ShenandoahHeap::keep_alive(oop obj) {
1298   if (is_concurrent_mark_in_progress()) {
1299     ShenandoahBarrierSet::barrier_set()-&gt;enqueue(obj);
1300   }
1301 }
1302 
1303 void ShenandoahHeap::heap_region_iterate(ShenandoahHeapRegionClosure* blk) const {
1304   for (size_t i = 0; i &lt; num_regions(); i++) {
1305     ShenandoahHeapRegion* current = get_region(i);
1306     blk-&gt;heap_region_do(current);
1307   }
1308 }
1309 
1310 class ShenandoahParallelHeapRegionTask : public AbstractGangTask {
1311 private:
1312   ShenandoahHeap* const _heap;
1313   ShenandoahHeapRegionClosure* const _blk;
1314 
<span class="line-modified">1315   shenandoah_padding(0);</span>
1316   volatile size_t _index;
<span class="line-modified">1317   shenandoah_padding(1);</span>
1318 
1319 public:
1320   ShenandoahParallelHeapRegionTask(ShenandoahHeapRegionClosure* blk) :
1321           AbstractGangTask(&quot;Parallel Region Task&quot;),
1322           _heap(ShenandoahHeap::heap()), _blk(blk), _index(0) {}
1323 
1324   void work(uint worker_id) {
1325     size_t stride = ShenandoahParallelRegionStride;
1326 
1327     size_t max = _heap-&gt;num_regions();
1328     while (_index &lt; max) {
1329       size_t cur = Atomic::fetch_and_add(&amp;_index, stride);
1330       size_t start = cur;
1331       size_t end = MIN2(cur + stride, max);
1332       if (start &gt;= max) break;
1333 
1334       for (size_t i = cur; i &lt; end; i++) {
1335         ShenandoahHeapRegion* current = _heap-&gt;get_region(i);
1336         _blk-&gt;heap_region_do(current);
1337       }
</pre>
<hr />
<pre>
1358   void heap_region_do(ShenandoahHeapRegion* r) {
1359     if (r-&gt;is_active()) {
1360       r-&gt;clear_live_data();
1361       _ctx-&gt;capture_top_at_mark_start(r);
1362     } else {
1363       assert(!r-&gt;has_live(), &quot;Region &quot; SIZE_FORMAT &quot; should have no live data&quot;, r-&gt;region_number());
1364       assert(_ctx-&gt;top_at_mark_start(r) == r-&gt;top(),
1365              &quot;Region &quot; SIZE_FORMAT &quot; should already have correct TAMS&quot;, r-&gt;region_number());
1366     }
1367   }
1368 
1369   bool is_thread_safe() { return true; }
1370 };
1371 
1372 void ShenandoahHeap::op_init_mark() {
1373   assert(ShenandoahSafepoint::is_at_shenandoah_safepoint(), &quot;Should be at safepoint&quot;);
1374   assert(Thread::current()-&gt;is_VM_thread(), &quot;can only do this in VMThread&quot;);
1375 
1376   assert(marking_context()-&gt;is_bitmap_clear(), &quot;need clear marking bitmap&quot;);
1377   assert(!marking_context()-&gt;is_complete(), &quot;should not be complete&quot;);
<span class="line-added">1378   assert(!has_forwarded_objects(), &quot;No forwarded objects on this path&quot;);</span>
1379 
1380   if (ShenandoahVerify) {
1381     verifier()-&gt;verify_before_concmark();
1382   }
1383 
1384   if (VerifyBeforeGC) {
1385     Universe::verify();
1386   }
1387 
1388   set_concurrent_mark_in_progress(true);
1389   // We need to reset all TLABs because we&#39;d lose marks on all objects allocated in them.
1390   {
1391     ShenandoahGCPhase phase(ShenandoahPhaseTimings::make_parsable);
1392     make_parsable(true);
1393   }
1394 
1395   {
1396     ShenandoahGCPhase phase(ShenandoahPhaseTimings::clear_liveness);
1397     ShenandoahClearLivenessClosure clc;
1398     parallel_heap_region_iterate(&amp;clc);
1399   }
1400 
1401   // Make above changes visible to worker threads
1402   OrderAccess::fence();
1403 
1404   concurrent_mark()-&gt;mark_roots(ShenandoahPhaseTimings::scan_roots);
1405 
1406   if (UseTLAB) {
1407     ShenandoahGCPhase phase(ShenandoahPhaseTimings::resize_tlabs);
1408     resize_tlabs();
1409   }
1410 
1411   if (ShenandoahPacing) {
1412     pacer()-&gt;setup_for_mark();
1413   }
<span class="line-added">1414 </span>
<span class="line-added">1415   // Arm nmethods for concurrent marking. When a nmethod is about to be executed,</span>
<span class="line-added">1416   // we need to make sure that all its metadata are marked. alternative is to remark</span>
<span class="line-added">1417   // thread roots at final mark pause, but it can be potential latency killer.</span>
<span class="line-added">1418   if (ShenandoahConcurrentRoots::should_do_concurrent_class_unloading()) {</span>
<span class="line-added">1419     ShenandoahCodeRoots::arm_nmethods();</span>
<span class="line-added">1420   }</span>
1421 }
1422 
1423 void ShenandoahHeap::op_mark() {
1424   concurrent_mark()-&gt;mark_from_roots();
1425 }
1426 
1427 class ShenandoahCompleteLivenessClosure : public ShenandoahHeapRegionClosure {
1428 private:
1429   ShenandoahMarkingContext* const _ctx;
1430 public:
1431   ShenandoahCompleteLivenessClosure() : _ctx(ShenandoahHeap::heap()-&gt;complete_marking_context()) {}
1432 
1433   void heap_region_do(ShenandoahHeapRegion* r) {
1434     if (r-&gt;is_active()) {
1435       HeapWord *tams = _ctx-&gt;top_at_mark_start(r);
1436       HeapWord *top = r-&gt;top();
1437       if (top &gt; tams) {
1438         r-&gt;increase_live_data_alloc_words(pointer_delta(top, tams));
1439       }
1440     } else {
1441       assert(!r-&gt;has_live(), &quot;Region &quot; SIZE_FORMAT &quot; should have no live data&quot;, r-&gt;region_number());
1442       assert(_ctx-&gt;top_at_mark_start(r) == r-&gt;top(),
1443              &quot;Region &quot; SIZE_FORMAT &quot; should have correct TAMS&quot;, r-&gt;region_number());
1444     }
1445   }
1446 
1447   bool is_thread_safe() { return true; }
1448 };
1449 
1450 void ShenandoahHeap::op_final_mark() {
1451   assert(ShenandoahSafepoint::is_at_shenandoah_safepoint(), &quot;Should be at safepoint&quot;);
<span class="line-added">1452   assert(!has_forwarded_objects(), &quot;No forwarded objects on this path&quot;);</span>
1453 
1454   // It is critical that we
1455   // evacuate roots right after finishing marking, so that we don&#39;t
1456   // get unmarked objects in the roots.
1457 
1458   if (!cancelled_gc()) {
1459     concurrent_mark()-&gt;finish_mark_from_roots(/* full_gc = */ false);
1460 
1461     // Marking is completed, deactivate SATB barrier
1462     set_concurrent_mark_in_progress(false);
1463     mark_complete_marking_context();
1464 
1465     parallel_cleaning(false /* full gc*/);
1466 










1467     if (ShenandoahVerify) {
1468       verifier()-&gt;verify_roots_no_forwarded();
1469     }
1470     // All allocations past TAMS are implicitly live, adjust the region data.
1471     // Bitmaps/TAMS are swapped at this point, so we need to poll complete bitmap.
1472     {
1473       ShenandoahGCPhase phase(ShenandoahPhaseTimings::complete_liveness);
1474       ShenandoahCompleteLivenessClosure cl;
1475       parallel_heap_region_iterate(&amp;cl);
1476     }
1477 
1478     // Force the threads to reacquire their TLABs outside the collection set.
1479     {
1480       ShenandoahGCPhase phase(ShenandoahPhaseTimings::retire_tlabs);
1481       make_parsable(true);
1482     }
1483 
1484     // We are about to select the collection set, make sure it knows about
1485     // current pinning status. Also, this allows trashing more regions that
1486     // now have their pinning status dropped.
</pre>
<hr />
<pre>
1504 
1505       heuristics()-&gt;choose_collection_set(_collection_set);
1506 
1507       _free_set-&gt;rebuild();
1508     }
1509 
1510     if (!is_degenerated_gc_in_progress()) {
1511       prepare_concurrent_roots();
1512       prepare_concurrent_unloading();
1513     }
1514 
1515     // If collection set has candidates, start evacuation.
1516     // Otherwise, bypass the rest of the cycle.
1517     if (!collection_set()-&gt;is_empty()) {
1518       ShenandoahGCPhase init_evac(ShenandoahPhaseTimings::init_evac);
1519 
1520       if (ShenandoahVerify) {
1521         verifier()-&gt;verify_before_evacuation();
1522       }
1523 
<span class="line-added">1524       // Remember limit for updating refs. It&#39;s guaranteed that we get no from-space-refs written</span>
<span class="line-added">1525       // from here on.</span>
<span class="line-added">1526       for (uint i = 0; i &lt; num_regions(); i++) {</span>
<span class="line-added">1527         ShenandoahHeapRegion* r = get_region(i);</span>
<span class="line-added">1528         r-&gt;set_update_watermark(r-&gt;top());</span>
<span class="line-added">1529       }</span>
<span class="line-added">1530 </span>
1531       set_evacuation_in_progress(true);
1532       // From here on, we need to update references.
1533       set_has_forwarded_objects(true);
1534 
1535       if (!is_degenerated_gc_in_progress()) {
1536         if (ShenandoahConcurrentRoots::should_do_concurrent_class_unloading()) {
1537           ShenandoahCodeRoots::arm_nmethods();
1538         }
1539         evacuate_and_update_roots();
1540       }
1541 
1542       if (ShenandoahPacing) {
1543         pacer()-&gt;setup_for_evac();
1544       }
1545 
1546       if (ShenandoahVerify) {
1547         ShenandoahRootVerifier::RootTypes types = ShenandoahRootVerifier::None;
1548         if (ShenandoahConcurrentRoots::should_do_concurrent_roots()) {
1549           types = ShenandoahRootVerifier::combine(ShenandoahRootVerifier::JNIHandleRoots, ShenandoahRootVerifier::WeakRoots);
1550           types = ShenandoahRootVerifier::combine(types, ShenandoahRootVerifier::CLDGRoots);
</pre>
<hr />
<pre>
1566         Universe::verify();
1567       }
1568     }
1569 
1570   } else {
1571     // If this cycle was updating references, we need to keep the has_forwarded_objects
1572     // flag on, for subsequent phases to deal with it.
1573     concurrent_mark()-&gt;cancel();
1574     set_concurrent_mark_in_progress(false);
1575 
1576     if (process_references()) {
1577       // Abandon reference processing right away: pre-cleaning must have failed.
1578       ReferenceProcessor *rp = ref_processor();
1579       rp-&gt;disable_discovery();
1580       rp-&gt;abandon_partial_discovery();
1581       rp-&gt;verify_no_references_recorded();
1582     }
1583   }
1584 }
1585 



















1586 void ShenandoahHeap::op_conc_evac() {
1587   ShenandoahEvacuationTask task(this, _collection_set, true);
1588   workers()-&gt;run_task(&amp;task);
1589 }
1590 
1591 void ShenandoahHeap::op_stw_evac() {
1592   ShenandoahEvacuationTask task(this, _collection_set, false);
1593   workers()-&gt;run_task(&amp;task);
1594 }
1595 
1596 void ShenandoahHeap::op_updaterefs() {
1597   update_heap_references(true);
1598 }
1599 
1600 void ShenandoahHeap::op_cleanup() {
1601   free_set()-&gt;recycle_trash();
1602 }
1603 
1604 class ShenandoahConcurrentRootsEvacUpdateTask : public AbstractGangTask {
1605 private:
</pre>
<hr />
<pre>
1860         cancel_gc(GCCause::_shenandoah_upgrade_to_full_gc);
1861         op_degenerated_fail();
1862         return;
1863       }
1864 
1865       op_reset();
1866 
1867       op_init_mark();
1868       if (cancelled_gc()) {
1869         op_degenerated_fail();
1870         return;
1871       }
1872 
1873     case _degenerated_mark:
1874       op_final_mark();
1875       if (cancelled_gc()) {
1876         op_degenerated_fail();
1877         return;
1878       }
1879 
<span class="line-added">1880       if (!has_forwarded_objects() &amp;&amp; ShenandoahConcurrentRoots::can_do_concurrent_class_unloading()) {</span>
<span class="line-added">1881         // Disarm nmethods that armed for concurrent mark. On normal cycle, it would</span>
<span class="line-added">1882         // be disarmed while conc-roots phase is running.</span>
<span class="line-added">1883         // TODO: Call op_conc_roots() here instead</span>
<span class="line-added">1884         ShenandoahCodeRoots::disarm_nmethods();</span>
<span class="line-added">1885       }</span>
<span class="line-added">1886 </span>
1887       op_cleanup();
1888 
1889     case _degenerated_evac:
1890       // If heuristics thinks we should do the cycle, this flag would be set,
1891       // and we can do evacuation. Otherwise, it would be the shortcut cycle.
1892       if (is_evacuation_in_progress()) {
1893 
1894         // Degeneration under oom-evac protocol might have left some objects in
1895         // collection set un-evacuated. Restart evacuation from the beginning to
1896         // capture all objects. For all the objects that are already evacuated,
1897         // it would be a simple check, which is supposed to be fast. This is also
1898         // safe to do even without degeneration, as CSet iterator is at beginning
1899         // in preparation for evacuation anyway.
1900         //
1901         // Before doing that, we need to make sure we never had any cset-pinned
1902         // regions. This may happen if allocation failure happened when evacuating
1903         // the about-to-be-pinned object, oom-evac protocol left the object in
1904         // the collection set, and then the pin reached the cset region. If we continue
1905         // the cycle here, we would trash the cset and alive objects in it. To avoid
1906         // it, we fail degeneration right away and slide into Full GC to recover.
</pre>
<hr />
<pre>
2143                             ShenandoahPhaseTimings::purge_cldg);
2144     ClassLoaderDataGraph::purge();
2145   }
2146   // Resize and verify metaspace
2147   MetaspaceGC::compute_new_size();
2148   MetaspaceUtils::verify_metrics();
2149 }
2150 
2151 // Weak roots are either pre-evacuated (final mark) or updated (final updaterefs),
2152 // so they should not have forwarded oops.
2153 // However, we do need to &quot;null&quot; dead oops in the roots, if can not be done
2154 // in concurrent cycles.
2155 void ShenandoahHeap::stw_process_weak_roots(bool full_gc) {
2156   ShenandoahGCPhase root_phase(full_gc ?
2157                                ShenandoahPhaseTimings::full_gc_purge :
2158                                ShenandoahPhaseTimings::purge);
2159   uint num_workers = _workers-&gt;active_workers();
2160   ShenandoahPhaseTimings::Phase timing_phase = full_gc ?
2161                                                ShenandoahPhaseTimings::full_gc_purge_par :
2162                                                ShenandoahPhaseTimings::purge_par;

2163   ShenandoahGCPhase phase(timing_phase);
<span class="line-modified">2164   ShenandoahGCWorkerPhase worker_phase(timing_phase);</span>
<span class="line-added">2165 </span>
<span class="line-added">2166   // Cleanup weak roots</span>
2167   if (has_forwarded_objects()) {
<span class="line-modified">2168     ShenandoahForwardedIsAliveClosure is_alive;</span>
<span class="line-modified">2169     ShenandoahUpdateRefsClosure keep_alive;</span>
<span class="line-modified">2170     ShenandoahParallelWeakRootsCleaningTask&lt;ShenandoahForwardedIsAliveClosure, ShenandoahUpdateRefsClosure&gt;</span>
<span class="line-modified">2171       cleaning_task(&amp;is_alive, &amp;keep_alive, num_workers, !ShenandoahConcurrentRoots::should_do_concurrent_class_unloading());</span>
<span class="line-modified">2172     _workers-&gt;run_task(&amp;cleaning_task);</span>








2173   } else {
2174     ShenandoahIsAliveClosure is_alive;
2175 #ifdef ASSERT
2176     ShenandoahAssertNotForwardedClosure verify_cl;
2177     ShenandoahParallelWeakRootsCleaningTask&lt;ShenandoahIsAliveClosure, ShenandoahAssertNotForwardedClosure&gt;
2178       cleaning_task(&amp;is_alive, &amp;verify_cl, num_workers, !ShenandoahConcurrentRoots::should_do_concurrent_class_unloading());
2179 #else
2180     ShenandoahParallelWeakRootsCleaningTask&lt;ShenandoahIsAliveClosure, DoNothingClosure&gt;
2181       cleaning_task(&amp;is_alive, &amp;do_nothing_cl, num_workers, !ShenandoahConcurrentRoots::should_do_concurrent_class_unloading());
2182 #endif
2183     _workers-&gt;run_task(&amp;cleaning_task);
2184   }

2185 }
2186 
2187 void ShenandoahHeap::parallel_cleaning(bool full_gc) {
2188   assert(SafepointSynchronize::is_at_safepoint(), &quot;Must be at a safepoint&quot;);
2189   stw_process_weak_roots(full_gc);
2190   if (!ShenandoahConcurrentRoots::should_do_concurrent_class_unloading()) {
2191     stw_unload_classes(full_gc);
2192   }
2193 }
2194 
2195 void ShenandoahHeap::set_has_forwarded_objects(bool cond) {
2196   if (is_traversal_mode()) {
2197     set_gc_state_mask(HAS_FORWARDED | UPDATEREFS, cond);
2198   } else {
2199     set_gc_state_mask(HAS_FORWARDED, cond);
2200   }
2201 
2202 }
2203 
2204 void ShenandoahHeap::set_process_references(bool pr) {
</pre>
<hr />
<pre>
2321 
2322 void ShenandoahHeap::prepare_concurrent_unloading() {
2323   assert(SafepointSynchronize::is_at_safepoint(), &quot;Must be at a safepoint&quot;);
2324   if (ShenandoahConcurrentRoots::should_do_concurrent_class_unloading()) {
2325     _unloader.prepare();
2326   }
2327 }
2328 
2329 void ShenandoahHeap::finish_concurrent_unloading() {
2330   assert(SafepointSynchronize::is_at_safepoint(), &quot;Must be at a safepoint&quot;);
2331   if (ShenandoahConcurrentRoots::should_do_concurrent_class_unloading()) {
2332     _unloader.finish();
2333   }
2334 }
2335 
2336 #ifdef ASSERT
2337 void ShenandoahHeap::assert_gc_workers(uint nworkers) {
2338   assert(nworkers &gt; 0 &amp;&amp; nworkers &lt;= max_workers(), &quot;Sanity&quot;);
2339 
2340   if (ShenandoahSafepoint::is_at_shenandoah_safepoint()) {
<span class="line-modified">2341     if (UseDynamicNumberOfGCThreads) {</span>

2342       assert(nworkers &lt;= ParallelGCThreads, &quot;Cannot use more than it has&quot;);
2343     } else {
2344       // Use ParallelGCThreads inside safepoints
<span class="line-modified">2345       assert(nworkers == ParallelGCThreads, &quot;Use ParallelGCThreads within safepoints&quot;);</span>
2346     }
2347   } else {
<span class="line-modified">2348     if (UseDynamicNumberOfGCThreads) {</span>

2349       assert(nworkers &lt;= ConcGCThreads, &quot;Cannot use more than it has&quot;);
2350     } else {
2351       // Use ConcGCThreads outside safepoints
2352       assert(nworkers == ConcGCThreads, &quot;Use ConcGCThreads outside safepoints&quot;);
2353     }
2354   }
2355 }
2356 #endif
2357 
2358 ShenandoahVerifier* ShenandoahHeap::verifier() {
2359   guarantee(ShenandoahVerify, &quot;Should be enabled&quot;);
2360   assert (_verifier != NULL, &quot;sanity&quot;);
2361   return _verifier;
2362 }
2363 
2364 template&lt;class T&gt;
2365 class ShenandoahUpdateHeapRefsTask : public AbstractGangTask {
2366 private:
2367   T cl;
2368   ShenandoahHeap* _heap;
</pre>
<hr />
<pre>
2376     _regions(regions),
2377     _concurrent(concurrent) {
2378   }
2379 
2380   void work(uint worker_id) {
2381     if (_concurrent) {
2382       ShenandoahConcurrentWorkerSession worker_session(worker_id);
2383       ShenandoahSuspendibleThreadSetJoiner stsj(ShenandoahSuspendibleWorkers);
2384       do_work();
2385     } else {
2386       ShenandoahParallelWorkerSession worker_session(worker_id);
2387       do_work();
2388     }
2389   }
2390 
2391 private:
2392   void do_work() {
2393     ShenandoahHeapRegion* r = _regions-&gt;next();
2394     ShenandoahMarkingContext* const ctx = _heap-&gt;complete_marking_context();
2395     while (r != NULL) {
<span class="line-modified">2396       HeapWord* update_watermark = r-&gt;get_update_watermark();</span>
<span class="line-modified">2397       assert (update_watermark &gt;= r-&gt;bottom(), &quot;sanity&quot;);</span>
2398       if (r-&gt;is_active() &amp;&amp; !r-&gt;is_cset()) {
<span class="line-modified">2399         _heap-&gt;marked_object_oop_iterate(r, &amp;cl, update_watermark);</span>
2400       }
2401       if (ShenandoahPacing) {
<span class="line-modified">2402         _heap-&gt;pacer()-&gt;report_updaterefs(pointer_delta(update_watermark, r-&gt;bottom()));</span>
2403       }
2404       if (_heap-&gt;check_cancelled_gc_and_yield(_concurrent)) {
2405         return;
2406       }
2407       r = _regions-&gt;next();
2408     }
2409   }
2410 };
2411 
2412 void ShenandoahHeap::update_heap_references(bool concurrent) {
2413   ShenandoahUpdateHeapRefsTask&lt;ShenandoahUpdateHeapRefsClosure&gt; task(&amp;_update_refs_iterator, concurrent);
2414   workers()-&gt;run_task(&amp;task);
2415 }
2416 
2417 void ShenandoahHeap::op_init_updaterefs() {
2418   assert(ShenandoahSafepoint::is_at_shenandoah_safepoint(), &quot;must be at safepoint&quot;);
2419 
2420   set_evacuation_in_progress(false);
2421 
2422   {
2423     ShenandoahGCPhase phase(ShenandoahPhaseTimings::init_update_refs_retire_gclabs);
2424     retire_and_reset_gclabs();
2425   }
2426 
2427   if (ShenandoahVerify) {
2428     if (!is_degenerated_gc_in_progress()) {
2429       verifier()-&gt;verify_roots_in_to_space_except(ShenandoahRootVerifier::ThreadRoots);
2430     }
2431     verifier()-&gt;verify_before_updaterefs();
2432   }
2433 
2434   set_update_refs_in_progress(true);
2435 
2436   {
2437     ShenandoahGCPhase phase(ShenandoahPhaseTimings::init_update_refs_prepare);
2438 
2439     make_parsable(true);




2440 
2441     // Reset iterator.
2442     _update_refs_iterator.reset();
2443   }
2444 
2445   if (ShenandoahPacing) {
2446     pacer()-&gt;setup_for_updaterefs();
2447   }
2448 }
2449 
2450 void ShenandoahHeap::op_final_updaterefs() {
2451   assert(ShenandoahSafepoint::is_at_shenandoah_safepoint(), &quot;must be at safepoint&quot;);
2452 
2453   finish_concurrent_unloading();
2454 
2455   // Check if there is left-over work, and finish it
2456   if (_update_refs_iterator.has_next()) {
2457     ShenandoahGCPhase phase(ShenandoahPhaseTimings::final_update_refs_finish_work);
2458 
2459     // Finish updating references where we left off.
</pre>
<hr />
<pre>
2495     trash_cset_regions();
2496   }
2497 
2498   set_has_forwarded_objects(false);
2499   set_update_refs_in_progress(false);
2500 
2501   if (ShenandoahVerify) {
2502     verifier()-&gt;verify_after_updaterefs();
2503   }
2504 
2505   if (VerifyAfterGC) {
2506     Universe::verify();
2507   }
2508 
2509   {
2510     ShenandoahHeapLocker locker(lock());
2511     _free_set-&gt;rebuild();
2512   }
2513 }
2514 














2515 void ShenandoahHeap::print_extended_on(outputStream *st) const {
2516   print_on(st);
2517   print_heap_regions_on(st);
2518 }
2519 
2520 bool ShenandoahHeap::is_bitmap_slice_committed(ShenandoahHeapRegion* r, bool skip_self) {
2521   size_t slice = r-&gt;region_number() / _bitmap_regions_per_slice;
2522 
2523   size_t regions_from = _bitmap_regions_per_slice * slice;
2524   size_t regions_to   = MIN2(num_regions(), _bitmap_regions_per_slice * (slice + 1));
2525   for (size_t g = regions_from; g &lt; regions_to; g++) {
2526     assert (g / _bitmap_regions_per_slice == slice, &quot;same slice&quot;);
2527     if (skip_self &amp;&amp; g == r-&gt;region_number()) continue;
2528     if (get_region(g)-&gt;is_committed()) {
2529       return true;
2530     }
2531   }
2532   return false;
2533 }
2534 
2535 bool ShenandoahHeap::commit_bitmap_slice(ShenandoahHeapRegion* r) {
<span class="line-modified">2536   shenandoah_assert_heaplocked();</span>
2537 
2538   // Bitmaps in special regions do not need commits
2539   if (_bitmap_region_special) {
2540     return true;
2541   }
2542 
2543   if (is_bitmap_slice_committed(r, true)) {
2544     // Some other region from the group is already committed, meaning the bitmap
2545     // slice is already committed, we exit right away.
2546     return true;
2547   }
2548 
2549   // Commit the bitmap slice:
2550   size_t slice = r-&gt;region_number() / _bitmap_regions_per_slice;
2551   size_t off = _bitmap_bytes_per_slice * slice;
2552   size_t len = _bitmap_bytes_per_slice;
2553   if (!os::commit_memory((char*)_bitmap_region.start() + off, len, false)) {
2554     return false;
2555   }
2556   return true;
2557 }
2558 
2559 bool ShenandoahHeap::uncommit_bitmap_slice(ShenandoahHeapRegion *r) {
<span class="line-modified">2560   shenandoah_assert_heaplocked();</span>
2561 
2562   // Bitmaps in special regions do not need uncommits
2563   if (_bitmap_region_special) {
2564     return true;
2565   }
2566 
2567   if (is_bitmap_slice_committed(r, true)) {
2568     // Some other region from the group is still committed, meaning the bitmap
2569     // slice is should stay committed, exit right away.
2570     return true;
2571   }
2572 
2573   // Uncommit the bitmap slice:
2574   size_t slice = r-&gt;region_number() / _bitmap_regions_per_slice;
2575   size_t off = _bitmap_bytes_per_slice * slice;
2576   size_t len = _bitmap_bytes_per_slice;
2577   if (!os::uncommit_memory((char*)_bitmap_region.start() + off, len)) {
2578     return false;
2579   }
2580   return true;
</pre>
<hr />
<pre>
2595 void ShenandoahHeap::vmop_entry_init_mark() {
2596   TraceCollectorStats tcs(monitoring_support()-&gt;stw_collection_counters());
2597   ShenandoahGCPhase total(ShenandoahPhaseTimings::total_pause_gross);
2598   ShenandoahGCPhase phase(ShenandoahPhaseTimings::init_mark_gross);
2599 
2600   try_inject_alloc_failure();
2601   VM_ShenandoahInitMark op;
2602   VMThread::execute(&amp;op); // jump to entry_init_mark() under safepoint
2603 }
2604 
2605 void ShenandoahHeap::vmop_entry_final_mark() {
2606   TraceCollectorStats tcs(monitoring_support()-&gt;stw_collection_counters());
2607   ShenandoahGCPhase total(ShenandoahPhaseTimings::total_pause_gross);
2608   ShenandoahGCPhase phase(ShenandoahPhaseTimings::final_mark_gross);
2609 
2610   try_inject_alloc_failure();
2611   VM_ShenandoahFinalMarkStartEvac op;
2612   VMThread::execute(&amp;op); // jump to entry_final_mark under safepoint
2613 }
2614 









2615 void ShenandoahHeap::vmop_entry_init_updaterefs() {
2616   TraceCollectorStats tcs(monitoring_support()-&gt;stw_collection_counters());
2617   ShenandoahGCPhase total(ShenandoahPhaseTimings::total_pause_gross);
2618   ShenandoahGCPhase phase(ShenandoahPhaseTimings::init_update_refs_gross);
2619 
2620   try_inject_alloc_failure();
2621   VM_ShenandoahInitUpdateRefs op;
2622   VMThread::execute(&amp;op);
2623 }
2624 
2625 void ShenandoahHeap::vmop_entry_final_updaterefs() {
2626   TraceCollectorStats tcs(monitoring_support()-&gt;stw_collection_counters());
2627   ShenandoahGCPhase total(ShenandoahPhaseTimings::total_pause_gross);
2628   ShenandoahGCPhase phase(ShenandoahPhaseTimings::final_update_refs_gross);
2629 
2630   try_inject_alloc_failure();
2631   VM_ShenandoahFinalUpdateRefs op;
2632   VMThread::execute(&amp;op);
2633 }
2634 
</pre>
<hr />
<pre>
2682                               ShenandoahWorkerPolicy::calc_workers_for_init_marking(),
2683                               &quot;init marking&quot;);
2684 
2685   op_init_mark();
2686 }
2687 
2688 void ShenandoahHeap::entry_final_mark() {
2689   ShenandoahGCPhase total_phase(ShenandoahPhaseTimings::total_pause);
2690   ShenandoahGCPhase phase(ShenandoahPhaseTimings::final_mark);
2691   const char* msg = final_mark_event_message();
2692   GCTraceTime(Info, gc) time(msg, gc_timer());
2693   EventMark em(&quot;%s&quot;, msg);
2694 
2695   ShenandoahWorkerScope scope(workers(),
2696                               ShenandoahWorkerPolicy::calc_workers_for_final_marking(),
2697                               &quot;final marking&quot;);
2698 
2699   op_final_mark();
2700 }
2701 










2702 void ShenandoahHeap::entry_init_updaterefs() {
2703   ShenandoahGCPhase total_phase(ShenandoahPhaseTimings::total_pause);
2704   ShenandoahGCPhase phase(ShenandoahPhaseTimings::init_update_refs);
2705 
2706   static const char* msg = &quot;Pause Init Update Refs&quot;;
2707   GCTraceTime(Info, gc) time(msg, gc_timer());
2708   EventMark em(&quot;%s&quot;, msg);
2709 
2710   // No workers used in this phase, no setup required
2711 
2712   op_init_updaterefs();
2713 }
2714 
2715 void ShenandoahHeap::entry_final_updaterefs() {
2716   ShenandoahGCPhase total_phase(ShenandoahPhaseTimings::total_pause);
2717   ShenandoahGCPhase phase(ShenandoahPhaseTimings::final_update_refs);
2718 
2719   static const char* msg = &quot;Pause Final Update Refs&quot;;
2720   GCTraceTime(Info, gc) time(msg, gc_timer());
2721   EventMark em(&quot;%s&quot;, msg);
</pre>
<hr />
<pre>
2979   _index = 0;
2980 }
2981 
2982 bool ShenandoahRegionIterator::has_next() const {
2983   return _index &lt; _heap-&gt;num_regions();
2984 }
2985 
2986 char ShenandoahHeap::gc_state() const {
2987   return _gc_state.raw_value();
2988 }
2989 
2990 void ShenandoahHeap::deduplicate_string(oop str) {
2991   assert(java_lang_String::is_instance(str), &quot;invariant&quot;);
2992 
2993   if (ShenandoahStringDedup::is_enabled()) {
2994     ShenandoahStringDedup::deduplicate(str);
2995   }
2996 }
2997 
2998 const char* ShenandoahHeap::init_mark_event_message() const {
<span class="line-modified">2999   assert(!has_forwarded_objects(), &quot;Should not have forwarded objects here&quot;);</span>
<span class="line-added">3000 </span>
3001   bool proc_refs = process_references();
3002   bool unload_cls = unload_classes();
3003 
<span class="line-modified">3004   if (proc_refs &amp;&amp; unload_cls) {</span>






3005     return &quot;Pause Init Mark (process weakrefs) (unload classes)&quot;;


3006   } else if (proc_refs) {
3007     return &quot;Pause Init Mark (process weakrefs)&quot;;
3008   } else if (unload_cls) {
3009     return &quot;Pause Init Mark (unload classes)&quot;;
3010   } else {
3011     return &quot;Pause Init Mark&quot;;
3012   }
3013 }
3014 
3015 const char* ShenandoahHeap::final_mark_event_message() const {
<span class="line-modified">3016   assert(!has_forwarded_objects(), &quot;Should not have forwarded objects here&quot;);</span>
<span class="line-added">3017 </span>
3018   bool proc_refs = process_references();
3019   bool unload_cls = unload_classes();
3020 
<span class="line-modified">3021   if (proc_refs &amp;&amp; unload_cls) {</span>






3022     return &quot;Pause Final Mark (process weakrefs) (unload classes)&quot;;


3023   } else if (proc_refs) {
3024     return &quot;Pause Final Mark (process weakrefs)&quot;;
3025   } else if (unload_cls) {
3026     return &quot;Pause Final Mark (unload classes)&quot;;
3027   } else {
3028     return &quot;Pause Final Mark&quot;;
3029   }
3030 }
3031 
3032 const char* ShenandoahHeap::conc_mark_event_message() const {
<span class="line-modified">3033   assert(!has_forwarded_objects(), &quot;Should not have forwarded objects here&quot;);</span>
<span class="line-added">3034 </span>
3035   bool proc_refs = process_references();
3036   bool unload_cls = unload_classes();
3037 
<span class="line-modified">3038   if (proc_refs &amp;&amp; unload_cls) {</span>






3039     return &quot;Concurrent marking (process weakrefs) (unload classes)&quot;;


3040   } else if (proc_refs) {
3041     return &quot;Concurrent marking (process weakrefs)&quot;;
3042   } else if (unload_cls) {
3043     return &quot;Concurrent marking (unload classes)&quot;;
3044   } else {
3045     return &quot;Concurrent marking&quot;;
3046   }
3047 }
3048 
3049 const char* ShenandoahHeap::init_traversal_event_message() const {
3050   bool proc_refs = process_references();
3051   bool unload_cls = unload_classes();
3052 
3053   if (proc_refs &amp;&amp; unload_cls) {
3054     return &quot;Pause Init Traversal (process weakrefs) (unload classes)&quot;;
3055   } else if (proc_refs) {
3056     return &quot;Pause Init Traversal (process weakrefs)&quot;;
3057   } else if (unload_cls) {
3058     return &quot;Pause Init Traversal (unload classes)&quot;;
3059   } else {
</pre>
</td>
</tr>
</table>
<center><a href="shenandoahFreeSet.hpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../index.html" target="_top">index</a> <a href="shenandoahHeap.hpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>