<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Cdiff src/hotspot/share/gc/shenandoah/shenandoahControlThread.cpp</title>
    <link rel="stylesheet" href="../../../../../style.css" />
  </head>
<body>
<center><a href="shenandoahConcurrentMark.cpp.cdiff.html" target="_top">&lt; prev</a> <a href="../../../../../index.html" target="_top">index</a> <a href="shenandoahControlThread.hpp.cdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/share/gc/shenandoah/shenandoahControlThread.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<hr />
<pre>
<span class="line-old-header">*** 176,41 ***</span>
      assert (!gc_requested || cause != GCCause::_last_gc_cause, &quot;GC cause should be set&quot;);
  
      if (gc_requested) {
        heap-&gt;reset_bytes_allocated_since_gc_start();
  
        // If GC was requested, we are sampling the counters even without actual triggers
        // from allocation machinery. This captures GC phases more accurately.
        set_forced_counters_update(true);
  
        // If GC was requested, we better dump freeset data for performance debugging
        {
          ShenandoahHeapLocker locker(heap-&gt;lock());
          heap-&gt;free_set()-&gt;log_status();
        }
<span class="line-removed">-     }</span>
  
<span class="line-modified">!     switch (mode) {</span>
<span class="line-modified">!       case none:</span>
<span class="line-modified">!         break;</span>
<span class="line-modified">!       case concurrent_traversal:</span>
<span class="line-modified">!         service_concurrent_traversal_cycle(cause);</span>
<span class="line-modified">!         break;</span>
<span class="line-modified">!       case concurrent_normal:</span>
<span class="line-modified">!         service_concurrent_normal_cycle(cause);</span>
<span class="line-modified">!         break;</span>
<span class="line-modified">!       case stw_degenerated:</span>
<span class="line-modified">!         service_stw_degenerated_cycle(cause, degen_point);</span>
<span class="line-modified">!         break;</span>
<span class="line-modified">!       case stw_full:</span>
<span class="line-modified">!         service_stw_full_cycle(cause);</span>
<span class="line-modified">!         break;</span>
<span class="line-modified">!       default:</span>
<span class="line-removed">-         ShouldNotReachHere();</span>
<span class="line-removed">-     }</span>
  
<span class="line-removed">-     if (gc_requested) {</span>
        // If this was the requested GC cycle, notify waiters about it
        if (explicit_gc_requested || implicit_gc_requested) {
          notify_gc_waiters();
        }
  
<span class="line-new-header">--- 176,40 ---</span>
      assert (!gc_requested || cause != GCCause::_last_gc_cause, &quot;GC cause should be set&quot;);
  
      if (gc_requested) {
        heap-&gt;reset_bytes_allocated_since_gc_start();
  
<span class="line-added">+       // Use default constructor to snapshot the Metaspace state before GC.</span>
<span class="line-added">+       metaspace::MetaspaceSizesSnapshot meta_sizes;</span>
<span class="line-added">+ </span>
        // If GC was requested, we are sampling the counters even without actual triggers
        // from allocation machinery. This captures GC phases more accurately.
        set_forced_counters_update(true);
  
        // If GC was requested, we better dump freeset data for performance debugging
        {
          ShenandoahHeapLocker locker(heap-&gt;lock());
          heap-&gt;free_set()-&gt;log_status();
        }
  
<span class="line-modified">!       switch (mode) {</span>
<span class="line-modified">!         case concurrent_traversal:</span>
<span class="line-modified">!           service_concurrent_traversal_cycle(cause);</span>
<span class="line-modified">!           break;</span>
<span class="line-modified">!         case concurrent_normal:</span>
<span class="line-modified">!           service_concurrent_normal_cycle(cause);</span>
<span class="line-modified">!           break;</span>
<span class="line-modified">!         case stw_degenerated:</span>
<span class="line-modified">!           service_stw_degenerated_cycle(cause, degen_point);</span>
<span class="line-modified">!           break;</span>
<span class="line-modified">!         case stw_full:</span>
<span class="line-modified">!           service_stw_full_cycle(cause);</span>
<span class="line-modified">!           break;</span>
<span class="line-modified">!         default:</span>
<span class="line-modified">!           ShouldNotReachHere();</span>
<span class="line-modified">!       }</span>
  
        // If this was the requested GC cycle, notify waiters about it
        if (explicit_gc_requested || implicit_gc_requested) {
          notify_gc_waiters();
        }
  
</pre>
<hr />
<pre>
<span class="line-old-header">*** 242,10 ***</span>
<span class="line-new-header">--- 241,13 ---</span>
        // Clear metaspace oom flag, if current cycle unloaded classes
        if (heap-&gt;unload_classes()) {
          heuristics-&gt;clear_metaspace_oom();
        }
  
<span class="line-added">+       // Print Metaspace change following GC (if logging is enabled).</span>
<span class="line-added">+       MetaspaceUtils::print_metaspace_change(meta_sizes);</span>
<span class="line-added">+ </span>
        // GC is over, we are at idle now
        if (ShenandoahPacing) {
          heap-&gt;pacer()-&gt;setup_for_idle();
        }
      } else {
</pre>
<hr />
<pre>
<span class="line-old-header">*** 314,23 ***</span>
    // Normal cycle goes via all concurrent phases. If allocation failure (af) happens during
    // any of the concurrent phases, it first degrades to Degenerated GC and completes GC there.
    // If second allocation failure happens during Degenerated GC cycle (for example, when GC
    // tries to evac something and no memory is available), cycle degrades to Full GC.
    //
<span class="line-modified">!   // There are also two shortcuts through the normal cycle: a) immediate garbage shortcut, when</span>
    // heuristics says there are no regions to compact, and all the collection comes from immediately
<span class="line-modified">!   // reclaimable regions; b) coalesced UR shortcut, when heuristics decides to coalesce UR with the</span>
<span class="line-removed">-   // mark from the next cycle.</span>
    //
    // ................................................................................................
    //
    //                                    (immediate garbage shortcut)                Concurrent GC
    //                             /-------------------------------------------\
<span class="line-modified">!   //                             |                       (coalesced UR)      v</span>
<span class="line-modified">!   //                             |                  /-----------------------&gt;o</span>
<span class="line-modified">!   //                             |                  |                        |</span>
<span class="line-modified">!   //                             |                  |                        v</span>
    // [START] ----&gt; Conc Mark ----o----&gt; Conc Evac --o--&gt; Conc Update-Refs ---o----&gt; [END]
    //                   |                    |                 |              ^
    //                   | (af)               | (af)            | (af)         |
    // ..................|....................|.................|..............|.......................
    //                   |                    |                 |              |
<span class="line-new-header">--- 316,22 ---</span>
    // Normal cycle goes via all concurrent phases. If allocation failure (af) happens during
    // any of the concurrent phases, it first degrades to Degenerated GC and completes GC there.
    // If second allocation failure happens during Degenerated GC cycle (for example, when GC
    // tries to evac something and no memory is available), cycle degrades to Full GC.
    //
<span class="line-modified">!   // There are also a shortcut through the normal cycle: immediate garbage shortcut, when</span>
    // heuristics says there are no regions to compact, and all the collection comes from immediately
<span class="line-modified">!   // reclaimable regions.</span>
    //
    // ................................................................................................
    //
    //                                    (immediate garbage shortcut)                Concurrent GC
    //                             /-------------------------------------------\
<span class="line-modified">!   //                             |                                           |</span>
<span class="line-modified">!   //                             |                                           |</span>
<span class="line-modified">!   //                             |                                           |</span>
<span class="line-modified">!   //                             |                                           v</span>
    // [START] ----&gt; Conc Mark ----o----&gt; Conc Evac --o--&gt; Conc Update-Refs ---o----&gt; [END]
    //                   |                    |                 |              ^
    //                   | (af)               | (af)            | (af)         |
    // ..................|....................|.................|..............|.......................
    //                   |                    |                 |              |
</pre>
<hr />
<pre>
<span class="line-old-header">*** 390,26 ***</span>
    if (heap-&gt;is_evacuation_in_progress()) {
      // Concurrently evacuate
      heap-&gt;entry_evac();
      if (check_cancellation_or_degen(ShenandoahHeap::_degenerated_evac)) return;
  
<span class="line-modified">!     // Perform update-refs phase, if required. This phase can be skipped if heuristics</span>
<span class="line-modified">!     // decides to piggy-back the update-refs on the next marking cycle. On either path,</span>
<span class="line-modified">!     // we need to turn off evacuation: either in init-update-refs, or in final-evac.</span>
<span class="line-modified">!     if (heap-&gt;heuristics()-&gt;should_start_update_refs()) {</span>
<span class="line-removed">-       heap-&gt;vmop_entry_init_updaterefs();</span>
<span class="line-removed">-       heap-&gt;entry_updaterefs();</span>
<span class="line-removed">-       if (check_cancellation_or_degen(ShenandoahHeap::_degenerated_updaterefs)) return;</span>
<span class="line-removed">- </span>
<span class="line-removed">-       heap-&gt;vmop_entry_final_updaterefs();</span>
  
<span class="line-modified">!       // Update references freed up collection set, kick the cleanup to reclaim the space.</span>
<span class="line-removed">-       heap-&gt;entry_cleanup();</span>
  
<span class="line-modified">!     } else {</span>
<span class="line-modified">!       heap-&gt;vmop_entry_final_evac();</span>
<span class="line-removed">-     }</span>
    }
  
    // Cycle is complete
    heap-&gt;heuristics()-&gt;record_success_concurrent();
    heap-&gt;shenandoah_policy()-&gt;record_success_concurrent();
<span class="line-new-header">--- 391,19 ---</span>
    if (heap-&gt;is_evacuation_in_progress()) {
      // Concurrently evacuate
      heap-&gt;entry_evac();
      if (check_cancellation_or_degen(ShenandoahHeap::_degenerated_evac)) return;
  
<span class="line-modified">!     // Perform update-refs phase.</span>
<span class="line-modified">!     heap-&gt;vmop_entry_init_updaterefs();</span>
<span class="line-modified">!     heap-&gt;entry_updaterefs();</span>
<span class="line-modified">!     if (check_cancellation_or_degen(ShenandoahHeap::_degenerated_updaterefs)) return;</span>
  
<span class="line-modified">!     heap-&gt;vmop_entry_final_updaterefs();</span>
  
<span class="line-modified">!     // Update references freed up collection set, kick the cleanup to reclaim the space.</span>
<span class="line-modified">!     heap-&gt;entry_cleanup();</span>
    }
  
    // Cycle is complete
    heap-&gt;heuristics()-&gt;record_success_concurrent();
    heap-&gt;shenandoah_policy()-&gt;record_success_concurrent();
</pre>
<center><a href="shenandoahConcurrentMark.cpp.cdiff.html" target="_top">&lt; prev</a> <a href="../../../../../index.html" target="_top">index</a> <a href="shenandoahControlThread.hpp.cdiff.html" target="_top">next &gt;</a></center>  </body>
</html>