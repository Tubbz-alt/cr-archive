<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/share/gc/g1/g1CollectedHeap.cpp</title>
    <link rel="stylesheet" href="../../../../../style.css" />
  </head>
<body>
<center><a href="g1Arguments.hpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../index.html" target="_top">index</a> <a href="g1CollectedHeap.hpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/share/gc/g1/g1CollectedHeap.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
  49 #include &quot;gc/g1/g1MemoryPool.hpp&quot;
  50 #include &quot;gc/g1/g1OopClosures.inline.hpp&quot;
  51 #include &quot;gc/g1/g1ParallelCleaning.hpp&quot;
  52 #include &quot;gc/g1/g1ParScanThreadState.inline.hpp&quot;
  53 #include &quot;gc/g1/g1Policy.hpp&quot;
  54 #include &quot;gc/g1/g1RedirtyCardsQueue.hpp&quot;
  55 #include &quot;gc/g1/g1RegionToSpaceMapper.hpp&quot;
  56 #include &quot;gc/g1/g1RemSet.hpp&quot;
  57 #include &quot;gc/g1/g1RootClosures.hpp&quot;
  58 #include &quot;gc/g1/g1RootProcessor.hpp&quot;
  59 #include &quot;gc/g1/g1SATBMarkQueueSet.hpp&quot;
  60 #include &quot;gc/g1/g1StringDedup.hpp&quot;
  61 #include &quot;gc/g1/g1ThreadLocalData.hpp&quot;
  62 #include &quot;gc/g1/g1Trace.hpp&quot;
  63 #include &quot;gc/g1/g1YCTypes.hpp&quot;
  64 #include &quot;gc/g1/g1YoungRemSetSamplingThread.hpp&quot;
  65 #include &quot;gc/g1/g1VMOperations.hpp&quot;
  66 #include &quot;gc/g1/heapRegion.inline.hpp&quot;
  67 #include &quot;gc/g1/heapRegionRemSet.hpp&quot;
  68 #include &quot;gc/g1/heapRegionSet.inline.hpp&quot;

  69 #include &quot;gc/shared/gcBehaviours.hpp&quot;
  70 #include &quot;gc/shared/gcHeapSummary.hpp&quot;
  71 #include &quot;gc/shared/gcId.hpp&quot;
  72 #include &quot;gc/shared/gcLocker.hpp&quot;
  73 #include &quot;gc/shared/gcTimer.hpp&quot;
  74 #include &quot;gc/shared/gcTraceTime.inline.hpp&quot;
  75 #include &quot;gc/shared/generationSpec.hpp&quot;
  76 #include &quot;gc/shared/isGCActiveMark.hpp&quot;
  77 #include &quot;gc/shared/locationPrinter.inline.hpp&quot;
  78 #include &quot;gc/shared/oopStorageParState.hpp&quot;
  79 #include &quot;gc/shared/preservedMarks.inline.hpp&quot;
  80 #include &quot;gc/shared/suspendibleThreadSet.hpp&quot;
  81 #include &quot;gc/shared/referenceProcessor.inline.hpp&quot;
  82 #include &quot;gc/shared/taskTerminator.hpp&quot;
  83 #include &quot;gc/shared/taskqueue.inline.hpp&quot;
  84 #include &quot;gc/shared/weakProcessor.inline.hpp&quot;
  85 #include &quot;gc/shared/workerPolicy.hpp&quot;
  86 #include &quot;logging/log.hpp&quot;
  87 #include &quot;memory/allocation.hpp&quot;
  88 #include &quot;memory/iterator.hpp&quot;
</pre>
<hr />
<pre>
1782     HeapWord* end = _hrm-&gt;reserved().end();
1783     size_t granularity = HeapRegion::GrainBytes;
1784 
1785     _region_attr.initialize(start, end, granularity);
1786     _humongous_reclaim_candidates.initialize(start, end, granularity);
1787   }
1788 
1789   _workers = new WorkGang(&quot;GC Thread&quot;, ParallelGCThreads,
1790                           true /* are_GC_task_threads */,
1791                           false /* are_ConcurrentGC_threads */);
1792   if (_workers == NULL) {
1793     return JNI_ENOMEM;
1794   }
1795   _workers-&gt;initialize_workers();
1796 
1797   _numa-&gt;set_region_info(HeapRegion::GrainBytes, page_size);
1798 
1799   // Create the G1ConcurrentMark data structure and thread.
1800   // (Must do this late, so that &quot;max_regions&quot; is defined.)
1801   _cm = new G1ConcurrentMark(this, prev_bitmap_storage, next_bitmap_storage);
<span class="line-removed">1802   if (!_cm-&gt;completed_initialization()) {</span>
<span class="line-removed">1803     vm_shutdown_during_initialization(&quot;Could not initialize G1ConcurrentMark&quot;);</span>
<span class="line-removed">1804     return JNI_ENOMEM;</span>
<span class="line-removed">1805   }</span>
1806   _cm_thread = _cm-&gt;cm_thread();
1807 
1808   // Now expand into the initial heap size.
1809   if (!expand(init_byte_size, _workers)) {
1810     vm_shutdown_during_initialization(&quot;Failed to allocate initial heap.&quot;);
1811     return JNI_ENOMEM;
1812   }
1813 
1814   // Perform any initialization actions delegated to the policy.
1815   policy()-&gt;init(this, &amp;_collection_set);
1816 
1817   jint ecode = initialize_concurrent_refinement();
1818   if (ecode != JNI_OK) {
1819     return ecode;
1820   }
1821 
1822   ecode = initialize_young_gen_sampling_thread();
1823   if (ecode != JNI_OK) {
1824     return ecode;
1825   }
</pre>
<hr />
<pre>
1986 
1987 size_t G1CollectedHeap::recalculate_used() const {
1988   SumUsedClosure blk;
1989   heap_region_iterate(&amp;blk);
1990   return blk.result();
1991 }
1992 
1993 bool  G1CollectedHeap::is_user_requested_concurrent_full_gc(GCCause::Cause cause) {
1994   switch (cause) {
1995     case GCCause::_java_lang_system_gc:                 return ExplicitGCInvokesConcurrent;
1996     case GCCause::_dcmd_gc_run:                         return ExplicitGCInvokesConcurrent;
1997     case GCCause::_wb_conc_mark:                        return true;
1998     default :                                           return false;
1999   }
2000 }
2001 
2002 bool G1CollectedHeap::should_do_concurrent_full_gc(GCCause::Cause cause) {
2003   switch (cause) {
2004     case GCCause::_g1_humongous_allocation: return true;
2005     case GCCause::_g1_periodic_collection:  return G1PeriodicGCInvokesConcurrent;

2006     default:                                return is_user_requested_concurrent_full_gc(cause);
2007   }
2008 }
2009 
2010 bool G1CollectedHeap::should_upgrade_to_full_gc(GCCause::Cause cause) {
2011   if (policy()-&gt;force_upgrade_to_full()) {
2012     return true;
2013   } else if (should_do_concurrent_full_gc(_gc_cause)) {
2014     return false;
2015   } else if (has_regions_left_for_allocation()) {
2016     return false;
2017   } else {
2018     return true;
2019   }
2020 }
2021 
2022 #ifndef PRODUCT
2023 void G1CollectedHeap::allocate_dummy_regions() {
2024   // Let&#39;s fill up most of the region
2025   size_t word_size = HeapRegion::GrainWords - 1024;
</pre>
<hr />
<pre>
2156       return false;
2157     }
2158 
2159     // Lock to get consistent set of values.
2160     uint old_marking_started_after;
2161     uint old_marking_completed_after;
2162     {
2163       MutexLocker ml(Heap_lock);
2164       // Update gc_counter for retrying VMOp if needed. Captured here to be
2165       // consistent with the values we use below for termination tests.  If
2166       // a retry is needed after a possible wait, and another collection
2167       // occurs in the meantime, it will cause our retry to be skipped and
2168       // we&#39;ll recheck for termination with updated conditions from that
2169       // more recent collection.  That&#39;s what we want, rather than having
2170       // our retry possibly perform an unnecessary collection.
2171       gc_counter = total_collections();
2172       old_marking_started_after = _old_marking_cycles_started;
2173       old_marking_completed_after = _old_marking_cycles_completed;
2174     }
2175 
<span class="line-modified">2176     if (!GCCause::is_user_requested_gc(cause)) {</span>
















2177       // For an &quot;automatic&quot; (not user-requested) collection, we just need to
2178       // ensure that progress is made.
2179       //
2180       // Request is finished if any of
2181       // (1) the VMOp successfully performed a GC,
2182       // (2) a concurrent cycle was already in progress,
<span class="line-modified">2183       // (3) a new cycle was started (by this thread or some other), or</span>
<span class="line-modified">2184       // (4) a Full GC was performed.</span>
<span class="line-modified">2185       // Cases (3) and (4) are detected together by a change to</span>

2186       // _old_marking_cycles_started.
2187       //
<span class="line-modified">2188       // Note that (1) does not imply (3).  If we&#39;re still in the mixed</span>
2189       // phase of an earlier concurrent collection, the request to make the
2190       // collection an initial-mark won&#39;t be honored.  If we don&#39;t check for
2191       // both conditions we&#39;ll spin doing back-to-back collections.
2192       if (op.gc_succeeded() ||
2193           op.cycle_already_in_progress() ||

2194           (old_marking_started_before != old_marking_started_after)) {
2195         LOG_COLLECT_CONCURRENTLY_COMPLETE(cause, true);
2196         return true;
2197       }
2198     } else {                    // User-requested GC.
2199       // For a user-requested collection, we want to ensure that a complete
2200       // full collection has been performed before returning, but without
2201       // waiting for more than needed.
2202 
2203       // For user-requested GCs (unlike non-UR), a successful VMOp implies a
2204       // new cycle was started.  That&#39;s good, because it&#39;s not clear what we
2205       // should do otherwise.  Trying again just does back to back GCs.
2206       // Can&#39;t wait for someone else to start a cycle.  And returning fails
2207       // to meet the goal of ensuring a full collection was performed.
2208       assert(!op.gc_succeeded() ||
2209              (old_marking_started_before != old_marking_started_after),
2210              &quot;invariant: succeeded %s, started before %u, started after %u&quot;,
2211              BOOL_TO_STR(op.gc_succeeded()),
2212              old_marking_started_before, old_marking_started_after);
2213 
</pre>
<hr />
<pre>
2227         LOG_COLLECT_CONCURRENTLY(cause, &quot;wait&quot;);
2228         MonitorLocker ml(G1OldGCCount_lock);
2229         while (gc_counter_less_than(_old_marking_cycles_completed,
2230                                     old_marking_started_after)) {
2231           ml.wait();
2232         }
2233         // Request is finished if the collection we just waited for was
2234         // started after this request.
2235         if (old_marking_started_before != old_marking_started_after) {
2236           LOG_COLLECT_CONCURRENTLY(cause, &quot;complete after wait&quot;);
2237           return true;
2238         }
2239       }
2240 
2241       // If VMOp was successful then it started a new cycle that the above
2242       // wait &amp;etc should have recognized as finishing this request.  This
2243       // differs from a non-user-request, where gc_succeeded does not imply
2244       // a new cycle was started.
2245       assert(!op.gc_succeeded(), &quot;invariant&quot;);
2246 
<span class="line-removed">2247       // If VMOp failed because a cycle was already in progress, it is now</span>
<span class="line-removed">2248       // complete.  But it didn&#39;t finish this user-requested GC, so try</span>
<span class="line-removed">2249       // again.</span>
2250       if (op.cycle_already_in_progress()) {



2251         LOG_COLLECT_CONCURRENTLY(cause, &quot;retry after in-progress&quot;);
2252         continue;












2253       }
2254     }
2255 
2256     // Collection failed and should be retried.
2257     assert(op.transient_failure(), &quot;invariant&quot;);
2258 
<span class="line-removed">2259     // If GCLocker is active, wait until clear before retrying.</span>
2260     if (GCLocker::is_active_and_needs_gc()) {

2261       LOG_COLLECT_CONCURRENTLY(cause, &quot;gc-locker stall&quot;);
2262       GCLocker::stall_until_clear();
2263     }
2264 
2265     LOG_COLLECT_CONCURRENTLY(cause, &quot;retry&quot;);
2266   }
2267 }
2268 
2269 bool G1CollectedHeap::try_collect(GCCause::Cause cause) {
2270   assert_heap_not_locked();
2271 
2272   // Lock to get consistent set of values.
2273   uint gc_count_before;
2274   uint full_gc_count_before;
2275   uint old_marking_started_before;
2276   {
2277     MutexLocker ml(Heap_lock);
2278     gc_count_before = total_collections();
2279     full_gc_count_before = total_full_collections();
2280     old_marking_started_before = _old_marking_cycles_started;
</pre>
<hr />
<pre>
2436   }
2437   return ret_val;
2438 }
2439 
2440 void G1CollectedHeap::deduplicate_string(oop str) {
2441   assert(java_lang_String::is_instance(str), &quot;invariant&quot;);
2442 
2443   if (G1StringDedup::is_enabled()) {
2444     G1StringDedup::deduplicate(str);
2445   }
2446 }
2447 
2448 void G1CollectedHeap::prepare_for_verify() {
2449   _verifier-&gt;prepare_for_verify();
2450 }
2451 
2452 void G1CollectedHeap::verify(VerifyOption vo) {
2453   _verifier-&gt;verify(vo);
2454 }
2455 
<span class="line-modified">2456 bool G1CollectedHeap::supports_concurrent_phase_control() const {</span>
2457   return true;
2458 }
2459 
<span class="line-removed">2460 bool G1CollectedHeap::request_concurrent_phase(const char* phase) {</span>
<span class="line-removed">2461   return _cm_thread-&gt;request_concurrent_phase(phase);</span>
<span class="line-removed">2462 }</span>
<span class="line-removed">2463 </span>
2464 bool G1CollectedHeap::is_heterogeneous_heap() const {
2465   return G1Arguments::is_heterogeneous_heap();
2466 }
2467 
2468 class PrintRegionClosure: public HeapRegionClosure {
2469   outputStream* _st;
2470 public:
2471   PrintRegionClosure(outputStream* st) : _st(st) {}
2472   bool do_heap_region(HeapRegion* r) {
2473     r-&gt;print_on(_st);
2474     return false;
2475   }
2476 };
2477 
2478 bool G1CollectedHeap::is_obj_dead_cond(const oop obj,
2479                                        const HeapRegion* hr,
2480                                        const VerifyOption vo) const {
2481   switch (vo) {
2482   case VerifyOption_G1UsePrevMarking: return is_obj_dead(obj, hr);
2483   case VerifyOption_G1UseNextMarking: return is_obj_ill(obj, hr);
</pre>
<hr />
<pre>
3161     // TraceMemoryManagerStats is called) so that the G1 memory pools are updated
3162     // before any GC notifications are raised.
3163     g1mm()-&gt;update_sizes();
3164 
3165     _gc_tracer_stw-&gt;report_evacuation_info(&amp;evacuation_info);
3166     _gc_tracer_stw-&gt;report_tenuring_threshold(_policy-&gt;tenuring_threshold());
3167     _gc_timer_stw-&gt;register_gc_end();
3168     _gc_tracer_stw-&gt;report_gc_end(_gc_timer_stw-&gt;gc_end(), _gc_timer_stw-&gt;time_partitions());
3169   }
3170   // It should now be safe to tell the concurrent mark thread to start
3171   // without its logging output interfering with the logging output
3172   // that came from the pause.
3173 
3174   if (should_start_conc_mark) {
3175     // CAUTION: after the doConcurrentMark() call below, the concurrent marking
3176     // thread(s) could be running concurrently with us. Make sure that anything
3177     // after this point does not assume that we are the only GC thread running.
3178     // Note: of course, the actual marking work will not start until the safepoint
3179     // itself is released in SuspendibleThreadSet::desynchronize().
3180     do_concurrent_mark();

3181   }
3182 }
3183 
3184 void G1CollectedHeap::remove_self_forwarding_pointers(G1RedirtyCardsQueueSet* rdcqs) {
3185   G1ParRemoveSelfForwardPtrsTask rsfp_task(rdcqs);
3186   workers()-&gt;run_task(&amp;rsfp_task);
3187 }
3188 
3189 void G1CollectedHeap::restore_after_evac_failure(G1RedirtyCardsQueueSet* rdcqs) {
3190   double remove_self_forwards_start = os::elapsedTime();
3191 
3192   remove_self_forwarding_pointers(rdcqs);
3193   _preserved_marks_set.restore(workers());
3194 
3195   phase_times()-&gt;record_evac_fail_remove_self_forwards((os::elapsedTime() - remove_self_forwards_start) * 1000.0);
3196 }
3197 
3198 void G1CollectedHeap::preserve_mark_during_evac_failure(uint worker_id, oop obj, markWord m) {
3199   if (!_evacuation_failed) {
3200     _evacuation_failed = true;
</pre>
</td>
<td>
<hr />
<pre>
  49 #include &quot;gc/g1/g1MemoryPool.hpp&quot;
  50 #include &quot;gc/g1/g1OopClosures.inline.hpp&quot;
  51 #include &quot;gc/g1/g1ParallelCleaning.hpp&quot;
  52 #include &quot;gc/g1/g1ParScanThreadState.inline.hpp&quot;
  53 #include &quot;gc/g1/g1Policy.hpp&quot;
  54 #include &quot;gc/g1/g1RedirtyCardsQueue.hpp&quot;
  55 #include &quot;gc/g1/g1RegionToSpaceMapper.hpp&quot;
  56 #include &quot;gc/g1/g1RemSet.hpp&quot;
  57 #include &quot;gc/g1/g1RootClosures.hpp&quot;
  58 #include &quot;gc/g1/g1RootProcessor.hpp&quot;
  59 #include &quot;gc/g1/g1SATBMarkQueueSet.hpp&quot;
  60 #include &quot;gc/g1/g1StringDedup.hpp&quot;
  61 #include &quot;gc/g1/g1ThreadLocalData.hpp&quot;
  62 #include &quot;gc/g1/g1Trace.hpp&quot;
  63 #include &quot;gc/g1/g1YCTypes.hpp&quot;
  64 #include &quot;gc/g1/g1YoungRemSetSamplingThread.hpp&quot;
  65 #include &quot;gc/g1/g1VMOperations.hpp&quot;
  66 #include &quot;gc/g1/heapRegion.inline.hpp&quot;
  67 #include &quot;gc/g1/heapRegionRemSet.hpp&quot;
  68 #include &quot;gc/g1/heapRegionSet.inline.hpp&quot;
<span class="line-added">  69 #include &quot;gc/shared/concurrentGCBreakpoints.hpp&quot;</span>
  70 #include &quot;gc/shared/gcBehaviours.hpp&quot;
  71 #include &quot;gc/shared/gcHeapSummary.hpp&quot;
  72 #include &quot;gc/shared/gcId.hpp&quot;
  73 #include &quot;gc/shared/gcLocker.hpp&quot;
  74 #include &quot;gc/shared/gcTimer.hpp&quot;
  75 #include &quot;gc/shared/gcTraceTime.inline.hpp&quot;
  76 #include &quot;gc/shared/generationSpec.hpp&quot;
  77 #include &quot;gc/shared/isGCActiveMark.hpp&quot;
  78 #include &quot;gc/shared/locationPrinter.inline.hpp&quot;
  79 #include &quot;gc/shared/oopStorageParState.hpp&quot;
  80 #include &quot;gc/shared/preservedMarks.inline.hpp&quot;
  81 #include &quot;gc/shared/suspendibleThreadSet.hpp&quot;
  82 #include &quot;gc/shared/referenceProcessor.inline.hpp&quot;
  83 #include &quot;gc/shared/taskTerminator.hpp&quot;
  84 #include &quot;gc/shared/taskqueue.inline.hpp&quot;
  85 #include &quot;gc/shared/weakProcessor.inline.hpp&quot;
  86 #include &quot;gc/shared/workerPolicy.hpp&quot;
  87 #include &quot;logging/log.hpp&quot;
  88 #include &quot;memory/allocation.hpp&quot;
  89 #include &quot;memory/iterator.hpp&quot;
</pre>
<hr />
<pre>
1783     HeapWord* end = _hrm-&gt;reserved().end();
1784     size_t granularity = HeapRegion::GrainBytes;
1785 
1786     _region_attr.initialize(start, end, granularity);
1787     _humongous_reclaim_candidates.initialize(start, end, granularity);
1788   }
1789 
1790   _workers = new WorkGang(&quot;GC Thread&quot;, ParallelGCThreads,
1791                           true /* are_GC_task_threads */,
1792                           false /* are_ConcurrentGC_threads */);
1793   if (_workers == NULL) {
1794     return JNI_ENOMEM;
1795   }
1796   _workers-&gt;initialize_workers();
1797 
1798   _numa-&gt;set_region_info(HeapRegion::GrainBytes, page_size);
1799 
1800   // Create the G1ConcurrentMark data structure and thread.
1801   // (Must do this late, so that &quot;max_regions&quot; is defined.)
1802   _cm = new G1ConcurrentMark(this, prev_bitmap_storage, next_bitmap_storage);




1803   _cm_thread = _cm-&gt;cm_thread();
1804 
1805   // Now expand into the initial heap size.
1806   if (!expand(init_byte_size, _workers)) {
1807     vm_shutdown_during_initialization(&quot;Failed to allocate initial heap.&quot;);
1808     return JNI_ENOMEM;
1809   }
1810 
1811   // Perform any initialization actions delegated to the policy.
1812   policy()-&gt;init(this, &amp;_collection_set);
1813 
1814   jint ecode = initialize_concurrent_refinement();
1815   if (ecode != JNI_OK) {
1816     return ecode;
1817   }
1818 
1819   ecode = initialize_young_gen_sampling_thread();
1820   if (ecode != JNI_OK) {
1821     return ecode;
1822   }
</pre>
<hr />
<pre>
1983 
1984 size_t G1CollectedHeap::recalculate_used() const {
1985   SumUsedClosure blk;
1986   heap_region_iterate(&amp;blk);
1987   return blk.result();
1988 }
1989 
1990 bool  G1CollectedHeap::is_user_requested_concurrent_full_gc(GCCause::Cause cause) {
1991   switch (cause) {
1992     case GCCause::_java_lang_system_gc:                 return ExplicitGCInvokesConcurrent;
1993     case GCCause::_dcmd_gc_run:                         return ExplicitGCInvokesConcurrent;
1994     case GCCause::_wb_conc_mark:                        return true;
1995     default :                                           return false;
1996   }
1997 }
1998 
1999 bool G1CollectedHeap::should_do_concurrent_full_gc(GCCause::Cause cause) {
2000   switch (cause) {
2001     case GCCause::_g1_humongous_allocation: return true;
2002     case GCCause::_g1_periodic_collection:  return G1PeriodicGCInvokesConcurrent;
<span class="line-added">2003     case GCCause::_wb_breakpoint:           return true;</span>
2004     default:                                return is_user_requested_concurrent_full_gc(cause);
2005   }
2006 }
2007 
2008 bool G1CollectedHeap::should_upgrade_to_full_gc(GCCause::Cause cause) {
2009   if (policy()-&gt;force_upgrade_to_full()) {
2010     return true;
2011   } else if (should_do_concurrent_full_gc(_gc_cause)) {
2012     return false;
2013   } else if (has_regions_left_for_allocation()) {
2014     return false;
2015   } else {
2016     return true;
2017   }
2018 }
2019 
2020 #ifndef PRODUCT
2021 void G1CollectedHeap::allocate_dummy_regions() {
2022   // Let&#39;s fill up most of the region
2023   size_t word_size = HeapRegion::GrainWords - 1024;
</pre>
<hr />
<pre>
2154       return false;
2155     }
2156 
2157     // Lock to get consistent set of values.
2158     uint old_marking_started_after;
2159     uint old_marking_completed_after;
2160     {
2161       MutexLocker ml(Heap_lock);
2162       // Update gc_counter for retrying VMOp if needed. Captured here to be
2163       // consistent with the values we use below for termination tests.  If
2164       // a retry is needed after a possible wait, and another collection
2165       // occurs in the meantime, it will cause our retry to be skipped and
2166       // we&#39;ll recheck for termination with updated conditions from that
2167       // more recent collection.  That&#39;s what we want, rather than having
2168       // our retry possibly perform an unnecessary collection.
2169       gc_counter = total_collections();
2170       old_marking_started_after = _old_marking_cycles_started;
2171       old_marking_completed_after = _old_marking_cycles_completed;
2172     }
2173 
<span class="line-modified">2174     if (cause == GCCause::_wb_breakpoint) {</span>
<span class="line-added">2175       if (op.gc_succeeded()) {</span>
<span class="line-added">2176         LOG_COLLECT_CONCURRENTLY_COMPLETE(cause, true);</span>
<span class="line-added">2177         return true;</span>
<span class="line-added">2178       }</span>
<span class="line-added">2179       // When _wb_breakpoint there can&#39;t be another cycle or deferred.</span>
<span class="line-added">2180       assert(!op.cycle_already_in_progress(), &quot;invariant&quot;);</span>
<span class="line-added">2181       assert(!op.whitebox_attached(), &quot;invariant&quot;);</span>
<span class="line-added">2182       // Concurrent cycle attempt might have been cancelled by some other</span>
<span class="line-added">2183       // collection, so retry.  Unlike other cases below, we want to retry</span>
<span class="line-added">2184       // even if cancelled by a STW full collection, because we really want</span>
<span class="line-added">2185       // to start a concurrent cycle.</span>
<span class="line-added">2186       if (old_marking_started_before != old_marking_started_after) {</span>
<span class="line-added">2187         LOG_COLLECT_CONCURRENTLY(cause, &quot;ignoring STW full GC&quot;);</span>
<span class="line-added">2188         old_marking_started_before = old_marking_started_after;</span>
<span class="line-added">2189       }</span>
<span class="line-added">2190     } else if (!GCCause::is_user_requested_gc(cause)) {</span>
2191       // For an &quot;automatic&quot; (not user-requested) collection, we just need to
2192       // ensure that progress is made.
2193       //
2194       // Request is finished if any of
2195       // (1) the VMOp successfully performed a GC,
2196       // (2) a concurrent cycle was already in progress,
<span class="line-modified">2197       // (3) whitebox is controlling concurrent cycles,</span>
<span class="line-modified">2198       // (4) a new cycle was started (by this thread or some other), or</span>
<span class="line-modified">2199       // (5) a Full GC was performed.</span>
<span class="line-added">2200       // Cases (4) and (5) are detected together by a change to</span>
2201       // _old_marking_cycles_started.
2202       //
<span class="line-modified">2203       // Note that (1) does not imply (4).  If we&#39;re still in the mixed</span>
2204       // phase of an earlier concurrent collection, the request to make the
2205       // collection an initial-mark won&#39;t be honored.  If we don&#39;t check for
2206       // both conditions we&#39;ll spin doing back-to-back collections.
2207       if (op.gc_succeeded() ||
2208           op.cycle_already_in_progress() ||
<span class="line-added">2209           op.whitebox_attached() ||</span>
2210           (old_marking_started_before != old_marking_started_after)) {
2211         LOG_COLLECT_CONCURRENTLY_COMPLETE(cause, true);
2212         return true;
2213       }
2214     } else {                    // User-requested GC.
2215       // For a user-requested collection, we want to ensure that a complete
2216       // full collection has been performed before returning, but without
2217       // waiting for more than needed.
2218 
2219       // For user-requested GCs (unlike non-UR), a successful VMOp implies a
2220       // new cycle was started.  That&#39;s good, because it&#39;s not clear what we
2221       // should do otherwise.  Trying again just does back to back GCs.
2222       // Can&#39;t wait for someone else to start a cycle.  And returning fails
2223       // to meet the goal of ensuring a full collection was performed.
2224       assert(!op.gc_succeeded() ||
2225              (old_marking_started_before != old_marking_started_after),
2226              &quot;invariant: succeeded %s, started before %u, started after %u&quot;,
2227              BOOL_TO_STR(op.gc_succeeded()),
2228              old_marking_started_before, old_marking_started_after);
2229 
</pre>
<hr />
<pre>
2243         LOG_COLLECT_CONCURRENTLY(cause, &quot;wait&quot;);
2244         MonitorLocker ml(G1OldGCCount_lock);
2245         while (gc_counter_less_than(_old_marking_cycles_completed,
2246                                     old_marking_started_after)) {
2247           ml.wait();
2248         }
2249         // Request is finished if the collection we just waited for was
2250         // started after this request.
2251         if (old_marking_started_before != old_marking_started_after) {
2252           LOG_COLLECT_CONCURRENTLY(cause, &quot;complete after wait&quot;);
2253           return true;
2254         }
2255       }
2256 
2257       // If VMOp was successful then it started a new cycle that the above
2258       // wait &amp;etc should have recognized as finishing this request.  This
2259       // differs from a non-user-request, where gc_succeeded does not imply
2260       // a new cycle was started.
2261       assert(!op.gc_succeeded(), &quot;invariant&quot;);
2262 



2263       if (op.cycle_already_in_progress()) {
<span class="line-added">2264         // If VMOp failed because a cycle was already in progress, it</span>
<span class="line-added">2265         // is now complete.  But it didn&#39;t finish this user-requested</span>
<span class="line-added">2266         // GC, so try again.</span>
2267         LOG_COLLECT_CONCURRENTLY(cause, &quot;retry after in-progress&quot;);
2268         continue;
<span class="line-added">2269       } else if (op.whitebox_attached()) {</span>
<span class="line-added">2270         // If WhiteBox wants control, wait for notification of a state</span>
<span class="line-added">2271         // change in the controller, then try again.  Don&#39;t wait for</span>
<span class="line-added">2272         // release of control, since collections may complete while in</span>
<span class="line-added">2273         // control.  Note: This won&#39;t recognize a STW full collection</span>
<span class="line-added">2274         // while waiting; we can&#39;t wait on multiple monitors.</span>
<span class="line-added">2275         LOG_COLLECT_CONCURRENTLY(cause, &quot;whitebox control stall&quot;);</span>
<span class="line-added">2276         MonitorLocker ml(ConcurrentGCBreakpoints::monitor());</span>
<span class="line-added">2277         if (ConcurrentGCBreakpoints::is_controlled()) {</span>
<span class="line-added">2278           ml.wait();</span>
<span class="line-added">2279         }</span>
<span class="line-added">2280         continue;</span>
2281       }
2282     }
2283 
2284     // Collection failed and should be retried.
2285     assert(op.transient_failure(), &quot;invariant&quot;);
2286 

2287     if (GCLocker::is_active_and_needs_gc()) {
<span class="line-added">2288       // If GCLocker is active, wait until clear before retrying.</span>
2289       LOG_COLLECT_CONCURRENTLY(cause, &quot;gc-locker stall&quot;);
2290       GCLocker::stall_until_clear();
2291     }
2292 
2293     LOG_COLLECT_CONCURRENTLY(cause, &quot;retry&quot;);
2294   }
2295 }
2296 
2297 bool G1CollectedHeap::try_collect(GCCause::Cause cause) {
2298   assert_heap_not_locked();
2299 
2300   // Lock to get consistent set of values.
2301   uint gc_count_before;
2302   uint full_gc_count_before;
2303   uint old_marking_started_before;
2304   {
2305     MutexLocker ml(Heap_lock);
2306     gc_count_before = total_collections();
2307     full_gc_count_before = total_full_collections();
2308     old_marking_started_before = _old_marking_cycles_started;
</pre>
<hr />
<pre>
2464   }
2465   return ret_val;
2466 }
2467 
2468 void G1CollectedHeap::deduplicate_string(oop str) {
2469   assert(java_lang_String::is_instance(str), &quot;invariant&quot;);
2470 
2471   if (G1StringDedup::is_enabled()) {
2472     G1StringDedup::deduplicate(str);
2473   }
2474 }
2475 
2476 void G1CollectedHeap::prepare_for_verify() {
2477   _verifier-&gt;prepare_for_verify();
2478 }
2479 
2480 void G1CollectedHeap::verify(VerifyOption vo) {
2481   _verifier-&gt;verify(vo);
2482 }
2483 
<span class="line-modified">2484 bool G1CollectedHeap::supports_concurrent_gc_breakpoints() const {</span>
2485   return true;
2486 }
2487 




2488 bool G1CollectedHeap::is_heterogeneous_heap() const {
2489   return G1Arguments::is_heterogeneous_heap();
2490 }
2491 
2492 class PrintRegionClosure: public HeapRegionClosure {
2493   outputStream* _st;
2494 public:
2495   PrintRegionClosure(outputStream* st) : _st(st) {}
2496   bool do_heap_region(HeapRegion* r) {
2497     r-&gt;print_on(_st);
2498     return false;
2499   }
2500 };
2501 
2502 bool G1CollectedHeap::is_obj_dead_cond(const oop obj,
2503                                        const HeapRegion* hr,
2504                                        const VerifyOption vo) const {
2505   switch (vo) {
2506   case VerifyOption_G1UsePrevMarking: return is_obj_dead(obj, hr);
2507   case VerifyOption_G1UseNextMarking: return is_obj_ill(obj, hr);
</pre>
<hr />
<pre>
3185     // TraceMemoryManagerStats is called) so that the G1 memory pools are updated
3186     // before any GC notifications are raised.
3187     g1mm()-&gt;update_sizes();
3188 
3189     _gc_tracer_stw-&gt;report_evacuation_info(&amp;evacuation_info);
3190     _gc_tracer_stw-&gt;report_tenuring_threshold(_policy-&gt;tenuring_threshold());
3191     _gc_timer_stw-&gt;register_gc_end();
3192     _gc_tracer_stw-&gt;report_gc_end(_gc_timer_stw-&gt;gc_end(), _gc_timer_stw-&gt;time_partitions());
3193   }
3194   // It should now be safe to tell the concurrent mark thread to start
3195   // without its logging output interfering with the logging output
3196   // that came from the pause.
3197 
3198   if (should_start_conc_mark) {
3199     // CAUTION: after the doConcurrentMark() call below, the concurrent marking
3200     // thread(s) could be running concurrently with us. Make sure that anything
3201     // after this point does not assume that we are the only GC thread running.
3202     // Note: of course, the actual marking work will not start until the safepoint
3203     // itself is released in SuspendibleThreadSet::desynchronize().
3204     do_concurrent_mark();
<span class="line-added">3205     ConcurrentGCBreakpoints::notify_idle_to_active();</span>
3206   }
3207 }
3208 
3209 void G1CollectedHeap::remove_self_forwarding_pointers(G1RedirtyCardsQueueSet* rdcqs) {
3210   G1ParRemoveSelfForwardPtrsTask rsfp_task(rdcqs);
3211   workers()-&gt;run_task(&amp;rsfp_task);
3212 }
3213 
3214 void G1CollectedHeap::restore_after_evac_failure(G1RedirtyCardsQueueSet* rdcqs) {
3215   double remove_self_forwards_start = os::elapsedTime();
3216 
3217   remove_self_forwarding_pointers(rdcqs);
3218   _preserved_marks_set.restore(workers());
3219 
3220   phase_times()-&gt;record_evac_fail_remove_self_forwards((os::elapsedTime() - remove_self_forwards_start) * 1000.0);
3221 }
3222 
3223 void G1CollectedHeap::preserve_mark_during_evac_failure(uint worker_id, oop obj, markWord m) {
3224   if (!_evacuation_failed) {
3225     _evacuation_failed = true;
</pre>
</td>
</tr>
</table>
<center><a href="g1Arguments.hpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../index.html" target="_top">index</a> <a href="g1CollectedHeap.hpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>