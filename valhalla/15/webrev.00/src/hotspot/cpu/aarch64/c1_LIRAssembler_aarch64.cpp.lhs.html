<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Frames src/hotspot/cpu/aarch64/c1_LIRAssembler_aarch64.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
    <script type="text/javascript" src="../../../../navigation.js"></script>
  </head>
<body onkeypress="keypress(event);">
<a name="0"></a>
<hr />
<pre>   1 /*
<a name="1" id="anc1"></a><span class="line-modified">   2  * Copyright (c) 2000, 2019, Oracle and/or its affiliates. All rights reserved.</span>
   3  * Copyright (c) 2014, Red Hat Inc. All rights reserved.
   4  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   5  *
   6  * This code is free software; you can redistribute it and/or modify it
   7  * under the terms of the GNU General Public License version 2 only, as
   8  * published by the Free Software Foundation.
   9  *
  10  * This code is distributed in the hope that it will be useful, but WITHOUT
  11  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  12  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  13  * version 2 for more details (a copy is included in the LICENSE file that
  14  * accompanied this code).
  15  *
  16  * You should have received a copy of the GNU General Public License version
  17  * 2 along with this work; if not, write to the Free Software Foundation,
  18  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  19  *
  20  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  21  * or visit www.oracle.com if you need additional information or have any
  22  * questions.
  23  *
  24  */
  25 
  26 #include &quot;precompiled.hpp&quot;
  27 #include &quot;asm/macroAssembler.inline.hpp&quot;
  28 #include &quot;asm/assembler.hpp&quot;
  29 #include &quot;c1/c1_CodeStubs.hpp&quot;
  30 #include &quot;c1/c1_Compilation.hpp&quot;
  31 #include &quot;c1/c1_LIRAssembler.hpp&quot;
  32 #include &quot;c1/c1_MacroAssembler.hpp&quot;
  33 #include &quot;c1/c1_Runtime1.hpp&quot;
  34 #include &quot;c1/c1_ValueStack.hpp&quot;
  35 #include &quot;ci/ciArrayKlass.hpp&quot;
  36 #include &quot;ci/ciInstance.hpp&quot;
  37 #include &quot;ci/ciValueKlass.hpp&quot;
  38 #include &quot;code/compiledIC.hpp&quot;
  39 #include &quot;gc/shared/collectedHeap.hpp&quot;
  40 #include &quot;nativeInst_aarch64.hpp&quot;
  41 #include &quot;oops/objArrayKlass.hpp&quot;
  42 #include &quot;oops/oop.inline.hpp&quot;
  43 #include &quot;runtime/frame.inline.hpp&quot;
  44 #include &quot;runtime/sharedRuntime.hpp&quot;
  45 #include &quot;utilities/powerOfTwo.hpp&quot;
  46 #include &quot;vmreg_aarch64.inline.hpp&quot;
  47 
  48 
  49 #ifndef PRODUCT
  50 #define COMMENT(x)   do { __ block_comment(x); } while (0)
  51 #else
  52 #define COMMENT(x)
  53 #endif
  54 
  55 NEEDS_CLEANUP // remove this definitions ?
  56 const Register IC_Klass    = rscratch2;   // where the IC klass is cached
  57 const Register SYNC_header = r0;   // synchronization header
  58 const Register SHIFT_count = r0;   // where count for shift operations must be
  59 
  60 #define __ _masm-&gt;
  61 
  62 
  63 static void select_different_registers(Register preserve,
  64                                        Register extra,
  65                                        Register &amp;tmp1,
  66                                        Register &amp;tmp2) {
  67   if (tmp1 == preserve) {
  68     assert_different_registers(tmp1, tmp2, extra);
  69     tmp1 = extra;
  70   } else if (tmp2 == preserve) {
  71     assert_different_registers(tmp1, tmp2, extra);
  72     tmp2 = extra;
  73   }
  74   assert_different_registers(preserve, tmp1, tmp2);
  75 }
  76 
  77 
  78 
  79 static void select_different_registers(Register preserve,
  80                                        Register extra,
  81                                        Register &amp;tmp1,
  82                                        Register &amp;tmp2,
  83                                        Register &amp;tmp3) {
  84   if (tmp1 == preserve) {
  85     assert_different_registers(tmp1, tmp2, tmp3, extra);
  86     tmp1 = extra;
  87   } else if (tmp2 == preserve) {
  88     assert_different_registers(tmp1, tmp2, tmp3, extra);
  89     tmp2 = extra;
  90   } else if (tmp3 == preserve) {
  91     assert_different_registers(tmp1, tmp2, tmp3, extra);
  92     tmp3 = extra;
  93   }
  94   assert_different_registers(preserve, tmp1, tmp2, tmp3);
  95 }
  96 
  97 
  98 bool LIR_Assembler::is_small_constant(LIR_Opr opr) { Unimplemented(); return false; }
  99 
 100 
 101 LIR_Opr LIR_Assembler::receiverOpr() {
 102   return FrameMap::receiver_opr;
 103 }
 104 
 105 LIR_Opr LIR_Assembler::osrBufferPointer() {
 106   return FrameMap::as_pointer_opr(receiverOpr()-&gt;as_register());
 107 }
 108 
 109 //--------------fpu register translations-----------------------
 110 
 111 
 112 address LIR_Assembler::float_constant(float f) {
 113   address const_addr = __ float_constant(f);
 114   if (const_addr == NULL) {
 115     bailout(&quot;const section overflow&quot;);
 116     return __ code()-&gt;consts()-&gt;start();
 117   } else {
 118     return const_addr;
 119   }
 120 }
 121 
 122 
 123 address LIR_Assembler::double_constant(double d) {
 124   address const_addr = __ double_constant(d);
 125   if (const_addr == NULL) {
 126     bailout(&quot;const section overflow&quot;);
 127     return __ code()-&gt;consts()-&gt;start();
 128   } else {
 129     return const_addr;
 130   }
 131 }
 132 
 133 address LIR_Assembler::int_constant(jlong n) {
 134   address const_addr = __ long_constant(n);
 135   if (const_addr == NULL) {
 136     bailout(&quot;const section overflow&quot;);
 137     return __ code()-&gt;consts()-&gt;start();
 138   } else {
 139     return const_addr;
 140   }
 141 }
 142 
 143 void LIR_Assembler::breakpoint() { Unimplemented(); }
 144 
 145 void LIR_Assembler::push(LIR_Opr opr) { Unimplemented(); }
 146 
 147 void LIR_Assembler::pop(LIR_Opr opr) { Unimplemented(); }
 148 
 149 bool LIR_Assembler::is_literal_address(LIR_Address* addr) { Unimplemented(); return false; }
 150 //-------------------------------------------
 151 
 152 static Register as_reg(LIR_Opr op) {
 153   return op-&gt;is_double_cpu() ? op-&gt;as_register_lo() : op-&gt;as_register();
 154 }
 155 
 156 static jlong as_long(LIR_Opr data) {
 157   jlong result;
 158   switch (data-&gt;type()) {
 159   case T_INT:
 160     result = (data-&gt;as_jint());
 161     break;
 162   case T_LONG:
 163     result = (data-&gt;as_jlong());
 164     break;
 165   default:
 166     ShouldNotReachHere();
 167     result = 0;  // unreachable
 168   }
 169   return result;
 170 }
 171 
 172 Address LIR_Assembler::as_Address(LIR_Address* addr, Register tmp) {
 173   Register base = addr-&gt;base()-&gt;as_pointer_register();
 174   LIR_Opr opr = addr-&gt;index();
 175   if (opr-&gt;is_cpu_register()) {
 176     Register index;
 177     if (opr-&gt;is_single_cpu())
 178       index = opr-&gt;as_register();
 179     else
 180       index = opr-&gt;as_register_lo();
 181     assert(addr-&gt;disp() == 0, &quot;must be&quot;);
 182     switch(opr-&gt;type()) {
 183       case T_INT:
 184         return Address(base, index, Address::sxtw(addr-&gt;scale()));
 185       case T_LONG:
 186         return Address(base, index, Address::lsl(addr-&gt;scale()));
 187       default:
 188         ShouldNotReachHere();
 189       }
 190   } else  {
 191     intptr_t addr_offset = intptr_t(addr-&gt;disp());
 192     if (Address::offset_ok_for_immed(addr_offset, addr-&gt;scale()))
 193       return Address(base, addr_offset, Address::lsl(addr-&gt;scale()));
 194     else {
 195       __ mov(tmp, addr_offset);
 196       return Address(base, tmp, Address::lsl(addr-&gt;scale()));
 197     }
 198   }
 199   return Address();
 200 }
 201 
 202 Address LIR_Assembler::as_Address_hi(LIR_Address* addr) {
 203   ShouldNotReachHere();
 204   return Address();
 205 }
 206 
 207 Address LIR_Assembler::as_Address(LIR_Address* addr) {
 208   return as_Address(addr, rscratch1);
 209 }
 210 
 211 Address LIR_Assembler::as_Address_lo(LIR_Address* addr) {
 212   return as_Address(addr, rscratch1);  // Ouch
 213   // FIXME: This needs to be much more clever.  See x86.
 214 }
 215 
 216 
 217 void LIR_Assembler::osr_entry() {
 218   offsets()-&gt;set_value(CodeOffsets::OSR_Entry, code_offset());
 219   BlockBegin* osr_entry = compilation()-&gt;hir()-&gt;osr_entry();
 220   ValueStack* entry_state = osr_entry-&gt;state();
 221   int number_of_locks = entry_state-&gt;locks_size();
 222 
 223   // we jump here if osr happens with the interpreter
 224   // state set up to continue at the beginning of the
 225   // loop that triggered osr - in particular, we have
 226   // the following registers setup:
 227   //
 228   // r2: osr buffer
 229   //
 230 
 231   // build frame
 232   ciMethod* m = compilation()-&gt;method();
 233   __ build_frame(initial_frame_size_in_bytes(), bang_size_in_bytes(), needs_stack_repair(), NULL);
 234 
 235   // OSR buffer is
 236   //
 237   // locals[nlocals-1..0]
 238   // monitors[0..number_of_locks]
 239   //
 240   // locals is a direct copy of the interpreter frame so in the osr buffer
 241   // so first slot in the local array is the last local from the interpreter
 242   // and last slot is local[0] (receiver) from the interpreter
 243   //
 244   // Similarly with locks. The first lock slot in the osr buffer is the nth lock
 245   // from the interpreter frame, the nth lock slot in the osr buffer is 0th lock
 246   // in the interpreter frame (the method lock if a sync method)
 247 
 248   // Initialize monitors in the compiled activation.
 249   //   r2: pointer to osr buffer
 250   //
 251   // All other registers are dead at this point and the locals will be
 252   // copied into place by code emitted in the IR.
 253 
 254   Register OSR_buf = osrBufferPointer()-&gt;as_pointer_register();
 255   { assert(frame::interpreter_frame_monitor_size() == BasicObjectLock::size(), &quot;adjust code below&quot;);
 256     int monitor_offset = BytesPerWord * method()-&gt;max_locals() +
 257       (2 * BytesPerWord) * (number_of_locks - 1);
 258     // SharedRuntime::OSR_migration_begin() packs BasicObjectLocks in
 259     // the OSR buffer using 2 word entries: first the lock and then
 260     // the oop.
 261     for (int i = 0; i &lt; number_of_locks; i++) {
 262       int slot_offset = monitor_offset - ((i * 2) * BytesPerWord);
 263 #ifdef ASSERT
 264       // verify the interpreter&#39;s monitor has a non-null object
 265       {
 266         Label L;
 267         __ ldr(rscratch1, Address(OSR_buf, slot_offset + 1*BytesPerWord));
 268         __ cbnz(rscratch1, L);
 269         __ stop(&quot;locked object is NULL&quot;);
 270         __ bind(L);
 271       }
 272 #endif
 273       __ ldr(r19, Address(OSR_buf, slot_offset + 0));
 274       __ str(r19, frame_map()-&gt;address_for_monitor_lock(i));
 275       __ ldr(r19, Address(OSR_buf, slot_offset + 1*BytesPerWord));
 276       __ str(r19, frame_map()-&gt;address_for_monitor_object(i));
 277     }
 278   }
 279 }
 280 
 281 
 282 // inline cache check; done before the frame is built.
 283 int LIR_Assembler::check_icache() {
 284   Register receiver = FrameMap::receiver_opr-&gt;as_register();
 285   Register ic_klass = IC_Klass;
 286   int start_offset = __ offset();
 287   __ inline_cache_check(receiver, ic_klass);
 288 
 289   // if icache check fails, then jump to runtime routine
 290   // Note: RECEIVER must still contain the receiver!
 291   Label dont;
 292   __ br(Assembler::EQ, dont);
 293   __ far_jump(RuntimeAddress(SharedRuntime::get_ic_miss_stub()));
 294 
 295   // We align the verified entry point unless the method body
 296   // (including its inline cache check) will fit in a single 64-byte
 297   // icache line.
 298   if (! method()-&gt;is_accessor() || __ offset() - start_offset &gt; 4 * 4) {
 299     // force alignment after the cache check.
 300     __ align(CodeEntryAlignment);
 301   }
 302 
 303   __ bind(dont);
 304   return start_offset;
 305 }
 306 
 307 void LIR_Assembler::clinit_barrier(ciMethod* method) {
 308   assert(VM_Version::supports_fast_class_init_checks(), &quot;sanity&quot;);
 309   assert(!method-&gt;holder()-&gt;is_not_initialized(), &quot;initialization should have been started&quot;);
 310 
 311   Label L_skip_barrier;
 312 
 313   __ mov_metadata(rscratch2, method-&gt;holder()-&gt;constant_encoding());
 314   __ clinit_barrier(rscratch2, rscratch1, &amp;L_skip_barrier /*L_fast_path*/);
 315   __ far_jump(RuntimeAddress(SharedRuntime::get_handle_wrong_method_stub()));
 316   __ bind(L_skip_barrier);
 317 }
 318 
 319 void LIR_Assembler::jobject2reg(jobject o, Register reg) {
 320   if (o == NULL) {
 321     __ mov(reg, zr);
 322   } else {
 323     __ movoop(reg, o, /*immediate*/true);
 324   }
 325 }
 326 
 327 void LIR_Assembler::deoptimize_trap(CodeEmitInfo *info) {
 328   address target = NULL;
 329   relocInfo::relocType reloc_type = relocInfo::none;
 330 
 331   switch (patching_id(info)) {
 332   case PatchingStub::access_field_id:
 333     target = Runtime1::entry_for(Runtime1::access_field_patching_id);
 334     reloc_type = relocInfo::section_word_type;
 335     break;
 336   case PatchingStub::load_klass_id:
 337     target = Runtime1::entry_for(Runtime1::load_klass_patching_id);
 338     reloc_type = relocInfo::metadata_type;
 339     break;
 340   case PatchingStub::load_mirror_id:
 341     target = Runtime1::entry_for(Runtime1::load_mirror_patching_id);
 342     reloc_type = relocInfo::oop_type;
 343     break;
 344   case PatchingStub::load_appendix_id:
 345     target = Runtime1::entry_for(Runtime1::load_appendix_patching_id);
 346     reloc_type = relocInfo::oop_type;
 347     break;
 348   default: ShouldNotReachHere();
 349   }
 350 
 351   __ far_call(RuntimeAddress(target));
 352   add_call_info_here(info);
 353 }
 354 
 355 void LIR_Assembler::jobject2reg_with_patching(Register reg, CodeEmitInfo *info) {
 356   deoptimize_trap(info);
 357 }
 358 
 359 
 360 // This specifies the rsp decrement needed to build the frame
 361 int LIR_Assembler::initial_frame_size_in_bytes() const {
 362   // if rounding, must let FrameMap know!
 363 
 364   // The frame_map records size in slots (32bit word)
 365 
 366   // subtract two words to account for return address and link
 367   return (frame_map()-&gt;framesize() - (2*VMRegImpl::slots_per_word))  * VMRegImpl::stack_slot_size;
 368 }
 369 
 370 
 371 int LIR_Assembler::emit_exception_handler() {
 372   // if the last instruction is a call (typically to do a throw which
 373   // is coming at the end after block reordering) the return address
 374   // must still point into the code area in order to avoid assertion
 375   // failures when searching for the corresponding bci =&gt; add a nop
 376   // (was bug 5/14/1999 - gri)
 377   __ nop();
 378 
 379   // generate code for exception handler
 380   address handler_base = __ start_a_stub(exception_handler_size());
 381   if (handler_base == NULL) {
 382     // not enough space left for the handler
 383     bailout(&quot;exception handler overflow&quot;);
 384     return -1;
 385   }
 386 
 387   int offset = code_offset();
 388 
 389   // the exception oop and pc are in r0, and r3
 390   // no other registers need to be preserved, so invalidate them
 391   __ invalidate_registers(false, true, true, false, true, true);
 392 
 393   // check that there is really an exception
 394   __ verify_not_null_oop(r0);
 395 
 396   // search an exception handler (r0: exception oop, r3: throwing pc)
 397   __ far_call(RuntimeAddress(Runtime1::entry_for(Runtime1::handle_exception_from_callee_id)));  __ should_not_reach_here();
 398   guarantee(code_offset() - offset &lt;= exception_handler_size(), &quot;overflow&quot;);
 399   __ end_a_stub();
 400 
 401   return offset;
 402 }
 403 
 404 
 405 // Emit the code to remove the frame from the stack in the exception
 406 // unwind path.
 407 int LIR_Assembler::emit_unwind_handler() {
 408 #ifndef PRODUCT
 409   if (CommentedAssembly) {
 410     _masm-&gt;block_comment(&quot;Unwind handler&quot;);
 411   }
 412 #endif
 413 
 414   int offset = code_offset();
 415 
 416   // Fetch the exception from TLS and clear out exception related thread state
 417   __ ldr(r0, Address(rthread, JavaThread::exception_oop_offset()));
 418   __ str(zr, Address(rthread, JavaThread::exception_oop_offset()));
 419   __ str(zr, Address(rthread, JavaThread::exception_pc_offset()));
 420 
 421   __ bind(_unwind_handler_entry);
 422   __ verify_not_null_oop(r0);
 423   if (method()-&gt;is_synchronized() || compilation()-&gt;env()-&gt;dtrace_method_probes()) {
 424     __ mov(r19, r0);  // Preserve the exception
 425   }
 426 
 427   // Preform needed unlocking
 428   MonitorExitStub* stub = NULL;
 429   if (method()-&gt;is_synchronized()) {
 430     monitor_address(0, FrameMap::r0_opr);
 431     stub = new MonitorExitStub(FrameMap::r0_opr, true, 0);
 432     __ unlock_object(r5, r4, r0, *stub-&gt;entry());
 433     __ bind(*stub-&gt;continuation());
 434   }
 435 
 436   if (compilation()-&gt;env()-&gt;dtrace_method_probes()) {
<a name="2" id="anc2"></a><span class="line-modified"> 437     __ call_Unimplemented();</span>
<span class="line-modified"> 438 #if 0</span>
<span class="line-modified"> 439     __ movptr(Address(rsp, 0), rax);</span>
<span class="line-removed"> 440     __ mov_metadata(Address(rsp, sizeof(void*)), method()-&gt;constant_encoding());</span>
<span class="line-removed"> 441     __ call(RuntimeAddress(CAST_FROM_FN_PTR(address, SharedRuntime::dtrace_method_exit)));</span>
<span class="line-removed"> 442 #endif</span>
 443   }
 444 
 445   if (method()-&gt;is_synchronized() || compilation()-&gt;env()-&gt;dtrace_method_probes()) {
 446     __ mov(r0, r19);  // Restore the exception
 447   }
 448 
 449   // remove the activation and dispatch to the unwind handler
 450   __ block_comment(&quot;remove_frame and dispatch to the unwind handler&quot;);
 451   __ remove_frame(initial_frame_size_in_bytes(), needs_stack_repair());
 452   __ far_jump(RuntimeAddress(Runtime1::entry_for(Runtime1::unwind_exception_id)));
 453 
 454   // Emit the slow path assembly
 455   if (stub != NULL) {
 456     stub-&gt;emit_code(this);
 457   }
 458 
 459   return offset;
 460 }
 461 
 462 
 463 int LIR_Assembler::emit_deopt_handler() {
 464   // if the last instruction is a call (typically to do a throw which
 465   // is coming at the end after block reordering) the return address
 466   // must still point into the code area in order to avoid assertion
 467   // failures when searching for the corresponding bci =&gt; add a nop
 468   // (was bug 5/14/1999 - gri)
 469   __ nop();
 470 
 471   // generate code for exception handler
 472   address handler_base = __ start_a_stub(deopt_handler_size());
 473   if (handler_base == NULL) {
 474     // not enough space left for the handler
 475     bailout(&quot;deopt handler overflow&quot;);
 476     return -1;
 477   }
 478 
 479   int offset = code_offset();
 480 
 481   __ adr(lr, pc());
 482   __ far_jump(RuntimeAddress(SharedRuntime::deopt_blob()-&gt;unpack()));
 483   guarantee(code_offset() - offset &lt;= deopt_handler_size(), &quot;overflow&quot;);
 484   __ end_a_stub();
 485 
 486   return offset;
 487 }
 488 
 489 void LIR_Assembler::add_debug_info_for_branch(address adr, CodeEmitInfo* info) {
 490   _masm-&gt;code_section()-&gt;relocate(adr, relocInfo::poll_type);
 491   int pc_offset = code_offset();
 492   flush_debug_info(pc_offset);
 493   info-&gt;record_debug_info(compilation()-&gt;debug_info_recorder(), pc_offset);
 494   if (info-&gt;exception_handlers() != NULL) {
 495     compilation()-&gt;add_exception_handlers_for_pco(pc_offset, info-&gt;exception_handlers());
 496   }
 497 }
 498 
 499 void LIR_Assembler::return_op(LIR_Opr result) {
 500   assert(result-&gt;is_illegal() || !result-&gt;is_single_cpu() || result-&gt;as_register() == r0, &quot;word returns are in r0,&quot;);
 501 
 502   ciMethod* method = compilation()-&gt;method();
 503 
 504   if (ValueTypeReturnedAsFields &amp;&amp; method-&gt;signature()-&gt;returns_never_null()) {
 505     ciType* return_type = method-&gt;return_type();
 506     if (return_type-&gt;is_valuetype()) {
 507       ciValueKlass* vk = return_type-&gt;as_value_klass();
 508       if (vk-&gt;can_be_returned_as_fields()) {
 509         address unpack_handler = vk-&gt;unpack_handler();
 510         assert(unpack_handler != NULL, &quot;must be&quot;);
 511         __ far_call(RuntimeAddress(unpack_handler));
 512         // At this point, rax points to the value object (for interpreter or C1 caller).
 513         // The fields of the object are copied into registers (for C2 caller).
 514       }
 515     }
 516   }
 517 
 518   // Pop the stack before the safepoint code
 519   __ remove_frame(initial_frame_size_in_bytes(), needs_stack_repair());
 520 
 521   if (StackReservedPages &gt; 0 &amp;&amp; compilation()-&gt;has_reserved_stack_access()) {
 522     __ reserved_stack_check();
 523   }
 524 
 525   address polling_page(os::get_polling_page());
 526   __ read_polling_page(rscratch1, polling_page, relocInfo::poll_return_type);
 527   __ ret(lr);
 528 }
 529 
 530 int LIR_Assembler::store_value_type_fields_to_buf(ciValueKlass* vk) {
 531   return (__ store_value_type_fields_to_buf(vk, false));
 532 }
 533 
 534 int LIR_Assembler::safepoint_poll(LIR_Opr tmp, CodeEmitInfo* info) {
 535   address polling_page(os::get_polling_page());
 536   guarantee(info != NULL, &quot;Shouldn&#39;t be NULL&quot;);
 537   assert(os::is_poll_address(polling_page), &quot;should be&quot;);
 538   __ get_polling_page(rscratch1, polling_page, relocInfo::poll_type);
 539   add_debug_info_for_branch(info);  // This isn&#39;t just debug info:
 540                                     // it&#39;s the oop map
 541   __ read_polling_page(rscratch1, relocInfo::poll_type);
 542   return __ offset();
 543 }
 544 
 545 
 546 void LIR_Assembler::move_regs(Register from_reg, Register to_reg) {
 547   if (from_reg == r31_sp)
 548     from_reg = sp;
 549   if (to_reg == r31_sp)
 550     to_reg = sp;
 551   __ mov(to_reg, from_reg);
 552 }
 553 
 554 void LIR_Assembler::swap_reg(Register a, Register b) { Unimplemented(); }
 555 
 556 
 557 void LIR_Assembler::const2reg(LIR_Opr src, LIR_Opr dest, LIR_PatchCode patch_code, CodeEmitInfo* info) {
 558   assert(src-&gt;is_constant(), &quot;should not call otherwise&quot;);
 559   assert(dest-&gt;is_register(), &quot;should not call otherwise&quot;);
 560   LIR_Const* c = src-&gt;as_constant_ptr();
 561 
 562   switch (c-&gt;type()) {
 563     case T_INT: {
 564       assert(patch_code == lir_patch_none, &quot;no patching handled here&quot;);
 565       __ movw(dest-&gt;as_register(), c-&gt;as_jint());
 566       break;
 567     }
 568 
 569     case T_ADDRESS: {
 570       assert(patch_code == lir_patch_none, &quot;no patching handled here&quot;);
 571       __ mov(dest-&gt;as_register(), c-&gt;as_jint());
 572       break;
 573     }
 574 
 575     case T_LONG: {
 576       assert(patch_code == lir_patch_none, &quot;no patching handled here&quot;);
 577       __ mov(dest-&gt;as_register_lo(), (intptr_t)c-&gt;as_jlong());
 578       break;
 579     }
 580 
 581     case T_VALUETYPE:
 582     case T_OBJECT: {
 583         if (patch_code != lir_patch_none) {
 584           jobject2reg_with_patching(dest-&gt;as_register(), info);
 585         } else {
 586           jobject2reg(c-&gt;as_jobject(), dest-&gt;as_register());
 587         }
 588       break;
 589     }
 590 
 591     case T_METADATA: {
 592       if (patch_code != lir_patch_none) {
 593         klass2reg_with_patching(dest-&gt;as_register(), info);
 594       } else {
 595         __ mov_metadata(dest-&gt;as_register(), c-&gt;as_metadata());
 596       }
 597       break;
 598     }
 599 
 600     case T_FLOAT: {
 601       if (__ operand_valid_for_float_immediate(c-&gt;as_jfloat())) {
 602         __ fmovs(dest-&gt;as_float_reg(), (c-&gt;as_jfloat()));
 603       } else {
 604         __ adr(rscratch1, InternalAddress(float_constant(c-&gt;as_jfloat())));
 605         __ ldrs(dest-&gt;as_float_reg(), Address(rscratch1));
 606       }
 607       break;
 608     }
 609 
 610     case T_DOUBLE: {
 611       if (__ operand_valid_for_float_immediate(c-&gt;as_jdouble())) {
 612         __ fmovd(dest-&gt;as_double_reg(), (c-&gt;as_jdouble()));
 613       } else {
 614         __ adr(rscratch1, InternalAddress(double_constant(c-&gt;as_jdouble())));
 615         __ ldrd(dest-&gt;as_double_reg(), Address(rscratch1));
 616       }
 617       break;
 618     }
 619 
 620     default:
 621       ShouldNotReachHere();
 622   }
 623 }
 624 
 625 void LIR_Assembler::const2stack(LIR_Opr src, LIR_Opr dest) {
 626   LIR_Const* c = src-&gt;as_constant_ptr();
 627   switch (c-&gt;type()) {
 628   case T_VALUETYPE:
 629   case T_OBJECT:
 630     {
 631       if (! c-&gt;as_jobject())
 632         __ str(zr, frame_map()-&gt;address_for_slot(dest-&gt;single_stack_ix()));
 633       else {
 634         const2reg(src, FrameMap::rscratch1_opr, lir_patch_none, NULL);
 635         reg2stack(FrameMap::rscratch1_opr, dest, c-&gt;type(), false);
 636       }
 637     }
 638     break;
 639   case T_ADDRESS:
 640     {
 641       const2reg(src, FrameMap::rscratch1_opr, lir_patch_none, NULL);
 642       reg2stack(FrameMap::rscratch1_opr, dest, c-&gt;type(), false);
 643     }
 644   case T_INT:
 645   case T_FLOAT:
 646     {
 647       Register reg = zr;
 648       if (c-&gt;as_jint_bits() == 0)
 649         __ strw(zr, frame_map()-&gt;address_for_slot(dest-&gt;single_stack_ix()));
 650       else {
 651         __ movw(rscratch1, c-&gt;as_jint_bits());
 652         __ strw(rscratch1, frame_map()-&gt;address_for_slot(dest-&gt;single_stack_ix()));
 653       }
 654     }
 655     break;
 656   case T_LONG:
 657   case T_DOUBLE:
 658     {
 659       Register reg = zr;
 660       if (c-&gt;as_jlong_bits() == 0)
 661         __ str(zr, frame_map()-&gt;address_for_slot(dest-&gt;double_stack_ix(),
 662                                                  lo_word_offset_in_bytes));
 663       else {
 664         __ mov(rscratch1, (intptr_t)c-&gt;as_jlong_bits());
 665         __ str(rscratch1, frame_map()-&gt;address_for_slot(dest-&gt;double_stack_ix(),
 666                                                         lo_word_offset_in_bytes));
 667       }
 668     }
 669     break;
 670   default:
 671     ShouldNotReachHere();
 672   }
 673 }
 674 
 675 void LIR_Assembler::const2mem(LIR_Opr src, LIR_Opr dest, BasicType type, CodeEmitInfo* info, bool wide) {
 676   assert(src-&gt;is_constant(), &quot;should not call otherwise&quot;);
 677   LIR_Const* c = src-&gt;as_constant_ptr();
 678   LIR_Address* to_addr = dest-&gt;as_address_ptr();
 679 
 680   void (Assembler::* insn)(Register Rt, const Address &amp;adr);
 681 
 682   switch (type) {
 683   case T_ADDRESS:
 684     assert(c-&gt;as_jint() == 0, &quot;should be&quot;);
 685     insn = &amp;Assembler::str;
 686     break;
 687   case T_LONG:
 688     assert(c-&gt;as_jlong() == 0, &quot;should be&quot;);
 689     insn = &amp;Assembler::str;
 690     break;
 691   case T_INT:
 692     assert(c-&gt;as_jint() == 0, &quot;should be&quot;);
 693     insn = &amp;Assembler::strw;
 694     break;
<a name="3" id="anc3"></a><span class="line-modified"> 695   case T_VALUETYPE: </span>
 696   case T_OBJECT:
 697   case T_ARRAY:
 698     // Non-null case is not handled on aarch64 but handled on x86
 699     // FIXME: do we need to add it here?
 700     assert(c-&gt;as_jobject() == 0, &quot;should be&quot;);
 701     if (UseCompressedOops &amp;&amp; !wide) {
 702       insn = &amp;Assembler::strw;
 703     } else {
 704       insn = &amp;Assembler::str;
 705     }
 706     break;
 707   case T_CHAR:
 708   case T_SHORT:
 709     assert(c-&gt;as_jint() == 0, &quot;should be&quot;);
 710     insn = &amp;Assembler::strh;
 711     break;
 712   case T_BOOLEAN:
 713   case T_BYTE:
 714     assert(c-&gt;as_jint() == 0, &quot;should be&quot;);
 715     insn = &amp;Assembler::strb;
 716     break;
 717   default:
 718     ShouldNotReachHere();
 719     insn = &amp;Assembler::str;  // unreachable
 720   }
 721 
 722   if (info) add_debug_info_for_null_check_here(info);
 723   (_masm-&gt;*insn)(zr, as_Address(to_addr, rscratch1));
 724 }
 725 
 726 void LIR_Assembler::reg2reg(LIR_Opr src, LIR_Opr dest) {
 727   assert(src-&gt;is_register(), &quot;should not call otherwise&quot;);
 728   assert(dest-&gt;is_register(), &quot;should not call otherwise&quot;);
 729 
 730   // move between cpu-registers
 731   if (dest-&gt;is_single_cpu()) {
 732     if (src-&gt;type() == T_LONG) {
 733       // Can do LONG -&gt; OBJECT
 734       move_regs(src-&gt;as_register_lo(), dest-&gt;as_register());
 735       return;
 736     }
 737     assert(src-&gt;is_single_cpu(), &quot;must match&quot;);
 738     if (src-&gt;type() == T_OBJECT || src-&gt;type() == T_VALUETYPE) {
 739       __ verify_oop(src-&gt;as_register());
 740     }
 741     move_regs(src-&gt;as_register(), dest-&gt;as_register());
 742 
 743   } else if (dest-&gt;is_double_cpu()) {
 744     if (is_reference_type(src-&gt;type())) {
 745       // Surprising to me but we can see move of a long to t_object
 746       __ verify_oop(src-&gt;as_register());
 747       move_regs(src-&gt;as_register(), dest-&gt;as_register_lo());
 748       return;
 749     }
 750     assert(src-&gt;is_double_cpu(), &quot;must match&quot;);
 751     Register f_lo = src-&gt;as_register_lo();
 752     Register f_hi = src-&gt;as_register_hi();
 753     Register t_lo = dest-&gt;as_register_lo();
 754     Register t_hi = dest-&gt;as_register_hi();
 755     assert(f_hi == f_lo, &quot;must be same&quot;);
 756     assert(t_hi == t_lo, &quot;must be same&quot;);
 757     move_regs(f_lo, t_lo);
 758 
 759   } else if (dest-&gt;is_single_fpu()) {
 760     __ fmovs(dest-&gt;as_float_reg(), src-&gt;as_float_reg());
 761 
 762   } else if (dest-&gt;is_double_fpu()) {
 763     __ fmovd(dest-&gt;as_double_reg(), src-&gt;as_double_reg());
 764 
 765   } else {
 766     ShouldNotReachHere();
 767   }
 768 }
 769 
 770 void LIR_Assembler::reg2stack(LIR_Opr src, LIR_Opr dest, BasicType type, bool pop_fpu_stack) {
 771   if (src-&gt;is_single_cpu()) {
 772     if (is_reference_type(type)) {
 773       __ str(src-&gt;as_register(), frame_map()-&gt;address_for_slot(dest-&gt;single_stack_ix()));
 774       __ verify_oop(src-&gt;as_register());
 775     } else if (type == T_METADATA || type == T_DOUBLE || type == T_ADDRESS) {
 776       __ str(src-&gt;as_register(), frame_map()-&gt;address_for_slot(dest-&gt;single_stack_ix()));
 777     } else {
 778       __ strw(src-&gt;as_register(), frame_map()-&gt;address_for_slot(dest-&gt;single_stack_ix()));
 779     }
 780 
 781   } else if (src-&gt;is_double_cpu()) {
 782     Address dest_addr_LO = frame_map()-&gt;address_for_slot(dest-&gt;double_stack_ix(), lo_word_offset_in_bytes);
 783     __ str(src-&gt;as_register_lo(), dest_addr_LO);
 784 
 785   } else if (src-&gt;is_single_fpu()) {
 786     Address dest_addr = frame_map()-&gt;address_for_slot(dest-&gt;single_stack_ix());
 787     __ strs(src-&gt;as_float_reg(), dest_addr);
 788 
 789   } else if (src-&gt;is_double_fpu()) {
 790     Address dest_addr = frame_map()-&gt;address_for_slot(dest-&gt;double_stack_ix());
 791     __ strd(src-&gt;as_double_reg(), dest_addr);
 792 
 793   } else {
 794     ShouldNotReachHere();
 795   }
 796 
 797 }
 798 
 799 
 800 void LIR_Assembler::reg2mem(LIR_Opr src, LIR_Opr dest, BasicType type, LIR_PatchCode patch_code, CodeEmitInfo* info, bool pop_fpu_stack, bool wide, bool /* unaligned */) {
 801   LIR_Address* to_addr = dest-&gt;as_address_ptr();
 802   PatchingStub* patch = NULL;
 803   Register compressed_src = rscratch1;
 804 
 805   if (patch_code != lir_patch_none) {
 806     deoptimize_trap(info);
 807     return;
 808   }
 809 
 810   if (is_reference_type(type)) {
 811     __ verify_oop(src-&gt;as_register());
 812 
 813     if (UseCompressedOops &amp;&amp; !wide) {
 814       __ encode_heap_oop(compressed_src, src-&gt;as_register());
 815     } else {
 816       compressed_src = src-&gt;as_register();
 817     }
 818   }
 819 
 820   int null_check_here = code_offset();
 821   switch (type) {
 822     case T_FLOAT: {
 823       __ strs(src-&gt;as_float_reg(), as_Address(to_addr));
 824       break;
 825     }
 826 
 827     case T_DOUBLE: {
 828       __ strd(src-&gt;as_double_reg(), as_Address(to_addr));
 829       break;
 830     }
 831 
 832     case T_VALUETYPE: // fall through
 833     case T_ARRAY:   // fall through
 834     case T_OBJECT:  // fall through
 835       if (UseCompressedOops &amp;&amp; !wide) {
 836         __ strw(compressed_src, as_Address(to_addr, rscratch2));
 837       } else {
 838          __ str(compressed_src, as_Address(to_addr));
 839       }
 840       break;
 841     case T_METADATA:
 842       // We get here to store a method pointer to the stack to pass to
 843       // a dtrace runtime call. This can&#39;t work on 64 bit with
 844       // compressed klass ptrs: T_METADATA can be a compressed klass
 845       // ptr or a 64 bit method pointer.
 846       ShouldNotReachHere();
 847       __ str(src-&gt;as_register(), as_Address(to_addr));
 848       break;
 849     case T_ADDRESS:
 850       __ str(src-&gt;as_register(), as_Address(to_addr));
 851       break;
 852     case T_INT:
 853       __ strw(src-&gt;as_register(), as_Address(to_addr));
 854       break;
 855 
 856     case T_LONG: {
 857       __ str(src-&gt;as_register_lo(), as_Address_lo(to_addr));
 858       break;
 859     }
 860 
 861     case T_BYTE:    // fall through
 862     case T_BOOLEAN: {
 863       __ strb(src-&gt;as_register(), as_Address(to_addr));
 864       break;
 865     }
 866 
 867     case T_CHAR:    // fall through
 868     case T_SHORT:
 869       __ strh(src-&gt;as_register(), as_Address(to_addr));
 870       break;
 871 
 872     default:
 873       ShouldNotReachHere();
 874   }
 875   if (info != NULL) {
 876     add_debug_info_for_null_check(null_check_here, info);
 877   }
 878 }
 879 
 880 
 881 void LIR_Assembler::stack2reg(LIR_Opr src, LIR_Opr dest, BasicType type) {
 882   assert(src-&gt;is_stack(), &quot;should not call otherwise&quot;);
 883   assert(dest-&gt;is_register(), &quot;should not call otherwise&quot;);
 884 
 885   if (dest-&gt;is_single_cpu()) {
 886     if (is_reference_type(type)) {
 887       __ ldr(dest-&gt;as_register(), frame_map()-&gt;address_for_slot(src-&gt;single_stack_ix()));
 888       __ verify_oop(dest-&gt;as_register());
 889     } else if (type == T_METADATA || type == T_ADDRESS) {
 890       __ ldr(dest-&gt;as_register(), frame_map()-&gt;address_for_slot(src-&gt;single_stack_ix()));
 891     } else {
 892       __ ldrw(dest-&gt;as_register(), frame_map()-&gt;address_for_slot(src-&gt;single_stack_ix()));
 893     }
 894 
 895   } else if (dest-&gt;is_double_cpu()) {
 896     Address src_addr_LO = frame_map()-&gt;address_for_slot(src-&gt;double_stack_ix(), lo_word_offset_in_bytes);
 897     __ ldr(dest-&gt;as_register_lo(), src_addr_LO);
 898 
 899   } else if (dest-&gt;is_single_fpu()) {
 900     Address src_addr = frame_map()-&gt;address_for_slot(src-&gt;single_stack_ix());
 901     __ ldrs(dest-&gt;as_float_reg(), src_addr);
 902 
 903   } else if (dest-&gt;is_double_fpu()) {
 904     Address src_addr = frame_map()-&gt;address_for_slot(src-&gt;double_stack_ix());
 905     __ ldrd(dest-&gt;as_double_reg(), src_addr);
 906 
 907   } else {
 908     ShouldNotReachHere();
 909   }
 910 }
 911 
 912 
 913 void LIR_Assembler::klass2reg_with_patching(Register reg, CodeEmitInfo* info) {
 914   address target = NULL;
 915   relocInfo::relocType reloc_type = relocInfo::none;
 916 
 917   switch (patching_id(info)) {
 918   case PatchingStub::access_field_id:
 919     target = Runtime1::entry_for(Runtime1::access_field_patching_id);
 920     reloc_type = relocInfo::section_word_type;
 921     break;
 922   case PatchingStub::load_klass_id:
 923     target = Runtime1::entry_for(Runtime1::load_klass_patching_id);
 924     reloc_type = relocInfo::metadata_type;
 925     break;
 926   case PatchingStub::load_mirror_id:
 927     target = Runtime1::entry_for(Runtime1::load_mirror_patching_id);
 928     reloc_type = relocInfo::oop_type;
 929     break;
 930   case PatchingStub::load_appendix_id:
 931     target = Runtime1::entry_for(Runtime1::load_appendix_patching_id);
 932     reloc_type = relocInfo::oop_type;
 933     break;
 934   default: ShouldNotReachHere();
 935   }
 936 
 937   __ far_call(RuntimeAddress(target));
 938   add_call_info_here(info);
 939 }
 940 
 941 void LIR_Assembler::stack2stack(LIR_Opr src, LIR_Opr dest, BasicType type) {
 942 
 943   LIR_Opr temp;
 944   if (type == T_LONG || type == T_DOUBLE)
 945     temp = FrameMap::rscratch1_long_opr;
 946   else
 947     temp = FrameMap::rscratch1_opr;
 948 
 949   stack2reg(src, temp, src-&gt;type());
 950   reg2stack(temp, dest, dest-&gt;type(), false);
 951 }
 952 
 953 
 954 void LIR_Assembler::mem2reg(LIR_Opr src, LIR_Opr dest, BasicType type, LIR_PatchCode patch_code, CodeEmitInfo* info, bool wide, bool /* unaligned */) {
 955   LIR_Address* addr = src-&gt;as_address_ptr();
 956   LIR_Address* from_addr = src-&gt;as_address_ptr();
 957 
<a name="4" id="anc4"></a><span class="line-modified"> 958   if (addr-&gt;base()-&gt;type() == T_OBJECT || addr-&gt;base()-&gt;type() == T_VALUETYPE) { </span>
 959     __ verify_oop(addr-&gt;base()-&gt;as_pointer_register());
 960   }
 961 
 962   if (patch_code != lir_patch_none) {
 963     deoptimize_trap(info);
 964     return;
 965   }
 966 
 967   if (info != NULL) {
 968     add_debug_info_for_null_check_here(info);
 969   }
 970   int null_check_here = code_offset();
 971   switch (type) {
 972     case T_FLOAT: {
 973       __ ldrs(dest-&gt;as_float_reg(), as_Address(from_addr));
 974       break;
 975     }
 976 
 977     case T_DOUBLE: {
 978       __ ldrd(dest-&gt;as_double_reg(), as_Address(from_addr));
 979       break;
 980     }
 981 
 982     case T_VALUETYPE: // fall through
 983     case T_ARRAY:   // fall through
 984     case T_OBJECT:  // fall through
 985       if (UseCompressedOops &amp;&amp; !wide) {
 986         __ ldrw(dest-&gt;as_register(), as_Address(from_addr));
 987       } else {
 988          __ ldr(dest-&gt;as_register(), as_Address(from_addr));
 989       }
 990       break;
 991     case T_METADATA:
 992       // We get here to store a method pointer to the stack to pass to
 993       // a dtrace runtime call. This can&#39;t work on 64 bit with
 994       // compressed klass ptrs: T_METADATA can be a compressed klass
 995       // ptr or a 64 bit method pointer.
 996       ShouldNotReachHere();
 997       __ ldr(dest-&gt;as_register(), as_Address(from_addr));
 998       break;
 999     case T_ADDRESS:
1000       // FIXME: OMG this is a horrible kludge.  Any offset from an
1001       // address that matches klass_offset_in_bytes() will be loaded
1002       // as a word, not a long.
1003       if (UseCompressedClassPointers &amp;&amp; addr-&gt;disp() == oopDesc::klass_offset_in_bytes()) {
1004         __ ldrw(dest-&gt;as_register(), as_Address(from_addr));
1005       } else {
1006         __ ldr(dest-&gt;as_register(), as_Address(from_addr));
1007       }
1008       break;
1009     case T_INT:
1010       __ ldrw(dest-&gt;as_register(), as_Address(from_addr));
1011       break;
1012 
1013     case T_LONG: {
1014       __ ldr(dest-&gt;as_register_lo(), as_Address_lo(from_addr));
1015       break;
1016     }
1017 
1018     case T_BYTE:
1019       __ ldrsb(dest-&gt;as_register(), as_Address(from_addr));
1020       break;
1021     case T_BOOLEAN: {
1022       __ ldrb(dest-&gt;as_register(), as_Address(from_addr));
1023       break;
1024     }
1025 
1026     case T_CHAR:
1027       __ ldrh(dest-&gt;as_register(), as_Address(from_addr));
1028       break;
1029     case T_SHORT:
1030       __ ldrsh(dest-&gt;as_register(), as_Address(from_addr));
1031       break;
1032 
1033     default:
1034       ShouldNotReachHere();
1035   }
1036 
1037   if (is_reference_type(type)) {
1038     if (UseCompressedOops &amp;&amp; !wide) {
1039       __ decode_heap_oop(dest-&gt;as_register());
1040     }
1041 
1042     if (!UseZGC) {
1043       // Load barrier has not yet been applied, so ZGC can&#39;t verify the oop here
1044       __ verify_oop(dest-&gt;as_register());
1045     }
1046   } else if (type == T_ADDRESS &amp;&amp; addr-&gt;disp() == oopDesc::klass_offset_in_bytes()) {
1047     if (UseCompressedClassPointers) {
1048       __ andr(dest-&gt;as_register(), dest-&gt;as_register(), oopDesc::compressed_klass_mask());
1049       __ decode_klass_not_null(dest-&gt;as_register());
1050     } else {
1051       __   ubfm(dest-&gt;as_register(), dest-&gt;as_register(), 0, 63 - oopDesc::storage_props_nof_bits);
1052     }
1053   }
1054 }
1055 
1056 void LIR_Assembler::move(LIR_Opr src, LIR_Opr dst) {
1057   assert(dst-&gt;is_cpu_register(), &quot;must be&quot;);
1058   assert(dst-&gt;type() == src-&gt;type(), &quot;must be&quot;);
1059 
1060   if (src-&gt;is_cpu_register()) {
1061     reg2reg(src, dst);
1062   } else if (src-&gt;is_stack()) {
1063     stack2reg(src, dst, dst-&gt;type());
1064   } else if (src-&gt;is_constant()) {
1065     const2reg(src, dst, lir_patch_none, NULL);
1066   } else {
1067     ShouldNotReachHere();
1068   }
1069 }
1070 
1071 int LIR_Assembler::array_element_size(BasicType type) const {
1072   int elem_size = type2aelembytes(type);
1073   return exact_log2(elem_size);
1074 }
1075 
1076 
1077 void LIR_Assembler::emit_op3(LIR_Op3* op) {
1078   switch (op-&gt;code()) {
1079   case lir_idiv:
1080   case lir_irem:
1081     arithmetic_idiv(op-&gt;code(),
1082                     op-&gt;in_opr1(),
1083                     op-&gt;in_opr2(),
1084                     op-&gt;in_opr3(),
1085                     op-&gt;result_opr(),
1086                     op-&gt;info());
1087     break;
1088   case lir_fmad:
1089     __ fmaddd(op-&gt;result_opr()-&gt;as_double_reg(),
1090               op-&gt;in_opr1()-&gt;as_double_reg(),
1091               op-&gt;in_opr2()-&gt;as_double_reg(),
1092               op-&gt;in_opr3()-&gt;as_double_reg());
1093     break;
1094   case lir_fmaf:
1095     __ fmadds(op-&gt;result_opr()-&gt;as_float_reg(),
1096               op-&gt;in_opr1()-&gt;as_float_reg(),
1097               op-&gt;in_opr2()-&gt;as_float_reg(),
1098               op-&gt;in_opr3()-&gt;as_float_reg());
1099     break;
1100   default:      ShouldNotReachHere(); break;
1101   }
1102 }
1103 
1104 void LIR_Assembler::emit_opBranch(LIR_OpBranch* op) {
1105 #ifdef ASSERT
1106   assert(op-&gt;block() == NULL || op-&gt;block()-&gt;label() == op-&gt;label(), &quot;wrong label&quot;);
1107   if (op-&gt;block() != NULL)  _branch_target_blocks.append(op-&gt;block());
1108   if (op-&gt;ublock() != NULL) _branch_target_blocks.append(op-&gt;ublock());
1109 #endif
1110 
1111   if (op-&gt;cond() == lir_cond_always) {
1112     if (op-&gt;info() != NULL) add_debug_info_for_branch(op-&gt;info());
1113     __ b(*(op-&gt;label()));
1114   } else {
1115     Assembler::Condition acond;
1116     if (op-&gt;code() == lir_cond_float_branch) {
1117       bool is_unordered = (op-&gt;ublock() == op-&gt;block());
1118       // Assembler::EQ does not permit unordered branches, so we add
1119       // another branch here.  Likewise, Assembler::NE does not permit
1120       // ordered branches.
1121       if ((is_unordered &amp;&amp; op-&gt;cond() == lir_cond_equal)
1122           || (!is_unordered &amp;&amp; op-&gt;cond() == lir_cond_notEqual))
1123         __ br(Assembler::VS, *(op-&gt;ublock()-&gt;label()));
1124       switch(op-&gt;cond()) {
1125       case lir_cond_equal:        acond = Assembler::EQ; break;
1126       case lir_cond_notEqual:     acond = Assembler::NE; break;
1127       case lir_cond_less:         acond = (is_unordered ? Assembler::LT : Assembler::LO); break;
1128       case lir_cond_lessEqual:    acond = (is_unordered ? Assembler::LE : Assembler::LS); break;
1129       case lir_cond_greaterEqual: acond = (is_unordered ? Assembler::HS : Assembler::GE); break;
1130       case lir_cond_greater:      acond = (is_unordered ? Assembler::HI : Assembler::GT); break;
1131       default:                    ShouldNotReachHere();
1132         acond = Assembler::EQ;  // unreachable
1133       }
1134     } else {
1135       switch (op-&gt;cond()) {
1136         case lir_cond_equal:        acond = Assembler::EQ; break;
1137         case lir_cond_notEqual:     acond = Assembler::NE; break;
1138         case lir_cond_less:         acond = Assembler::LT; break;
1139         case lir_cond_lessEqual:    acond = Assembler::LE; break;
1140         case lir_cond_greaterEqual: acond = Assembler::GE; break;
1141         case lir_cond_greater:      acond = Assembler::GT; break;
1142         case lir_cond_belowEqual:   acond = Assembler::LS; break;
1143         case lir_cond_aboveEqual:   acond = Assembler::HS; break;
1144         default:                    ShouldNotReachHere();
1145           acond = Assembler::EQ;  // unreachable
1146       }
1147     }
1148     __ br(acond,*(op-&gt;label()));
1149   }
1150 }
1151 
1152 
1153 
1154 void LIR_Assembler::emit_opConvert(LIR_OpConvert* op) {
1155   LIR_Opr src  = op-&gt;in_opr();
1156   LIR_Opr dest = op-&gt;result_opr();
1157 
1158   switch (op-&gt;bytecode()) {
1159     case Bytecodes::_i2f:
1160       {
1161         __ scvtfws(dest-&gt;as_float_reg(), src-&gt;as_register());
1162         break;
1163       }
1164     case Bytecodes::_i2d:
1165       {
1166         __ scvtfwd(dest-&gt;as_double_reg(), src-&gt;as_register());
1167         break;
1168       }
1169     case Bytecodes::_l2d:
1170       {
1171         __ scvtfd(dest-&gt;as_double_reg(), src-&gt;as_register_lo());
1172         break;
1173       }
1174     case Bytecodes::_l2f:
1175       {
1176         __ scvtfs(dest-&gt;as_float_reg(), src-&gt;as_register_lo());
1177         break;
1178       }
1179     case Bytecodes::_f2d:
1180       {
1181         __ fcvts(dest-&gt;as_double_reg(), src-&gt;as_float_reg());
1182         break;
1183       }
1184     case Bytecodes::_d2f:
1185       {
1186         __ fcvtd(dest-&gt;as_float_reg(), src-&gt;as_double_reg());
1187         break;
1188       }
1189     case Bytecodes::_i2c:
1190       {
1191         __ ubfx(dest-&gt;as_register(), src-&gt;as_register(), 0, 16);
1192         break;
1193       }
1194     case Bytecodes::_i2l:
1195       {
1196         __ sxtw(dest-&gt;as_register_lo(), src-&gt;as_register());
1197         break;
1198       }
1199     case Bytecodes::_i2s:
1200       {
1201         __ sxth(dest-&gt;as_register(), src-&gt;as_register());
1202         break;
1203       }
1204     case Bytecodes::_i2b:
1205       {
1206         __ sxtb(dest-&gt;as_register(), src-&gt;as_register());
1207         break;
1208       }
1209     case Bytecodes::_l2i:
1210       {
1211         _masm-&gt;block_comment(&quot;FIXME: This could be a no-op&quot;);
1212         __ uxtw(dest-&gt;as_register(), src-&gt;as_register_lo());
1213         break;
1214       }
1215     case Bytecodes::_d2l:
1216       {
1217         __ fcvtzd(dest-&gt;as_register_lo(), src-&gt;as_double_reg());
1218         break;
1219       }
1220     case Bytecodes::_f2i:
1221       {
1222         __ fcvtzsw(dest-&gt;as_register(), src-&gt;as_float_reg());
1223         break;
1224       }
1225     case Bytecodes::_f2l:
1226       {
1227         __ fcvtzs(dest-&gt;as_register_lo(), src-&gt;as_float_reg());
1228         break;
1229       }
1230     case Bytecodes::_d2i:
1231       {
1232         __ fcvtzdw(dest-&gt;as_register(), src-&gt;as_double_reg());
1233         break;
1234       }
1235     default: ShouldNotReachHere();
1236   }
1237 }
1238 
1239 void LIR_Assembler::emit_alloc_obj(LIR_OpAllocObj* op) {
1240   if (op-&gt;init_check()) {
1241     __ ldrb(rscratch1, Address(op-&gt;klass()-&gt;as_register(),
1242                                InstanceKlass::init_state_offset()));
1243     __ cmpw(rscratch1, InstanceKlass::fully_initialized);
1244     add_debug_info_for_null_check_here(op-&gt;stub()-&gt;info());
1245     __ br(Assembler::NE, *op-&gt;stub()-&gt;entry());
1246   }
1247   __ allocate_object(op-&gt;obj()-&gt;as_register(),
1248                      op-&gt;tmp1()-&gt;as_register(),
1249                      op-&gt;tmp2()-&gt;as_register(),
1250                      op-&gt;header_size(),
1251                      op-&gt;object_size(),
1252                      op-&gt;klass()-&gt;as_register(),
1253                      *op-&gt;stub()-&gt;entry());
1254   __ bind(*op-&gt;stub()-&gt;continuation());
1255 }
1256 
1257 void LIR_Assembler::emit_alloc_array(LIR_OpAllocArray* op) {
1258   Register len =  op-&gt;len()-&gt;as_register();
1259   __ uxtw(len, len);
1260 
1261   if (UseSlowPath || op-&gt;type() == T_VALUETYPE ||
1262       (!UseFastNewObjectArray &amp;&amp; is_reference_type(op-&gt;type())) ||
1263       (!UseFastNewTypeArray   &amp;&amp; !is_reference_type(op-&gt;type()))) {
1264     __ b(*op-&gt;stub()-&gt;entry());
1265   } else {
1266     Register tmp1 = op-&gt;tmp1()-&gt;as_register();
1267     Register tmp2 = op-&gt;tmp2()-&gt;as_register();
1268     Register tmp3 = op-&gt;tmp3()-&gt;as_register();
1269     if (len == tmp1) {
1270       tmp1 = tmp3;
1271     } else if (len == tmp2) {
1272       tmp2 = tmp3;
1273     } else if (len == tmp3) {
1274       // everything is ok
1275     } else {
1276       __ mov(tmp3, len);
1277     }
1278     __ allocate_array(op-&gt;obj()-&gt;as_register(),
1279                       len,
1280                       tmp1,
1281                       tmp2,
1282                       arrayOopDesc::header_size(op-&gt;type()),
1283                       array_element_size(op-&gt;type()),
1284                       op-&gt;klass()-&gt;as_register(),
1285                       *op-&gt;stub()-&gt;entry());
1286   }
1287   __ bind(*op-&gt;stub()-&gt;continuation());
1288 }
1289 
1290 void LIR_Assembler::type_profile_helper(Register mdo,
1291                                         ciMethodData *md, ciProfileData *data,
1292                                         Register recv, Label* update_done) {
1293   for (uint i = 0; i &lt; ReceiverTypeData::row_limit(); i++) {
1294     Label next_test;
1295     // See if the receiver is receiver[n].
1296     __ lea(rscratch2, Address(mdo, md-&gt;byte_offset_of_slot(data, ReceiverTypeData::receiver_offset(i))));
1297     __ ldr(rscratch1, Address(rscratch2));
1298     __ cmp(recv, rscratch1);
1299     __ br(Assembler::NE, next_test);
1300     Address data_addr(mdo, md-&gt;byte_offset_of_slot(data, ReceiverTypeData::receiver_count_offset(i)));
1301     __ addptr(data_addr, DataLayout::counter_increment);
1302     __ b(*update_done);
1303     __ bind(next_test);
1304   }
1305 
1306   // Didn&#39;t find receiver; find next empty slot and fill it in
1307   for (uint i = 0; i &lt; ReceiverTypeData::row_limit(); i++) {
1308     Label next_test;
1309     __ lea(rscratch2,
1310            Address(mdo, md-&gt;byte_offset_of_slot(data, ReceiverTypeData::receiver_offset(i))));
1311     Address recv_addr(rscratch2);
1312     __ ldr(rscratch1, recv_addr);
1313     __ cbnz(rscratch1, next_test);
1314     __ str(recv, recv_addr);
1315     __ mov(rscratch1, DataLayout::counter_increment);
1316     __ lea(rscratch2, Address(mdo, md-&gt;byte_offset_of_slot(data, ReceiverTypeData::receiver_count_offset(i))));
1317     __ str(rscratch1, Address(rscratch2));
1318     __ b(*update_done);
1319     __ bind(next_test);
1320   }
1321 }
1322 
1323 void LIR_Assembler::emit_typecheck_helper(LIR_OpTypeCheck *op, Label* success, Label* failure, Label* obj_is_null) {
1324   // we always need a stub for the failure case.
1325   CodeStub* stub = op-&gt;stub();
1326   Register obj = op-&gt;object()-&gt;as_register();
1327   Register k_RInfo = op-&gt;tmp1()-&gt;as_register();
1328   Register klass_RInfo = op-&gt;tmp2()-&gt;as_register();
1329   Register dst = op-&gt;result_opr()-&gt;as_register();
1330   ciKlass* k = op-&gt;klass();
1331   Register Rtmp1 = noreg;
1332 
1333   // check if it needs to be profiled
1334   ciMethodData* md;
1335   ciProfileData* data;
1336 
1337   const bool should_profile = op-&gt;should_profile();
1338 
1339   if (should_profile) {
1340     ciMethod* method = op-&gt;profiled_method();
1341     assert(method != NULL, &quot;Should have method&quot;);
1342     int bci = op-&gt;profiled_bci();
1343     md = method-&gt;method_data_or_null();
1344     assert(md != NULL, &quot;Sanity&quot;);
1345     data = md-&gt;bci_to_data(bci);
1346     assert(data != NULL,                &quot;need data for type check&quot;);
1347     assert(data-&gt;is_ReceiverTypeData(), &quot;need ReceiverTypeData for type check&quot;);
1348   }
1349   Label profile_cast_success, profile_cast_failure;
1350   Label *success_target = should_profile ? &amp;profile_cast_success : success;
1351   Label *failure_target = should_profile ? &amp;profile_cast_failure : failure;
1352 
1353   if (obj == k_RInfo) {
1354     k_RInfo = dst;
1355   } else if (obj == klass_RInfo) {
1356     klass_RInfo = dst;
1357   }
1358   if (k-&gt;is_loaded() &amp;&amp; !UseCompressedClassPointers) {
1359     select_different_registers(obj, dst, k_RInfo, klass_RInfo);
1360   } else {
1361     Rtmp1 = op-&gt;tmp3()-&gt;as_register();
1362     select_different_registers(obj, dst, k_RInfo, klass_RInfo, Rtmp1);
1363   }
1364 
1365   assert_different_registers(obj, k_RInfo, klass_RInfo);
1366 
1367     if (should_profile) {
1368       Label not_null;
1369       __ cbnz(obj, not_null);
1370       // Object is null; update MDO and exit
1371       Register mdo  = klass_RInfo;
1372       __ mov_metadata(mdo, md-&gt;constant_encoding());
1373       Address data_addr
1374         = __ form_address(rscratch2, mdo,
1375                           md-&gt;byte_offset_of_slot(data, DataLayout::flags_offset()),
1376                           0);
1377       __ ldrb(rscratch1, data_addr);
1378       __ orr(rscratch1, rscratch1, BitData::null_seen_byte_constant());
1379       __ strb(rscratch1, data_addr);
1380       __ b(*obj_is_null);
1381       __ bind(not_null);
1382     } else {
1383       __ cbz(obj, *obj_is_null);
1384     }
1385 
1386   if (!k-&gt;is_loaded()) {
1387     klass2reg_with_patching(k_RInfo, op-&gt;info_for_patch());
1388   } else {
1389     __ mov_metadata(k_RInfo, k-&gt;constant_encoding());
1390   }
1391   __ verify_oop(obj);
1392 
1393   if (op-&gt;fast_check()) {
1394     // get object class
1395     // not a safepoint as obj null check happens earlier
1396     __ load_klass(rscratch1, obj);
1397     __ cmp( rscratch1, k_RInfo);
1398 
1399     __ br(Assembler::NE, *failure_target);
1400     // successful cast, fall through to profile or jump
1401   } else {
1402     // get object class
1403     // not a safepoint as obj null check happens earlier
1404     __ load_klass(klass_RInfo, obj);
1405     if (k-&gt;is_loaded()) {
1406       // See if we get an immediate positive hit
1407       __ ldr(rscratch1, Address(klass_RInfo, long(k-&gt;super_check_offset())));
1408       __ cmp(k_RInfo, rscratch1);
1409       if ((juint)in_bytes(Klass::secondary_super_cache_offset()) != k-&gt;super_check_offset()) {
1410         __ br(Assembler::NE, *failure_target);
1411         // successful cast, fall through to profile or jump
1412       } else {
1413         // See if we get an immediate positive hit
1414         __ br(Assembler::EQ, *success_target);
1415         // check for self
1416         __ cmp(klass_RInfo, k_RInfo);
1417         __ br(Assembler::EQ, *success_target);
1418 
1419         __ stp(klass_RInfo, k_RInfo, Address(__ pre(sp, -2 * wordSize)));
1420         __ far_call(RuntimeAddress(Runtime1::entry_for(Runtime1::slow_subtype_check_id)));
1421         __ ldr(klass_RInfo, Address(__ post(sp, 2 * wordSize)));
1422         // result is a boolean
1423         __ cbzw(klass_RInfo, *failure_target);
1424         // successful cast, fall through to profile or jump
1425       }
1426     } else {
1427       // perform the fast part of the checking logic
1428       __ check_klass_subtype_fast_path(klass_RInfo, k_RInfo, Rtmp1, success_target, failure_target, NULL);
1429       // call out-of-line instance of __ check_klass_subtype_slow_path(...):
1430       __ stp(klass_RInfo, k_RInfo, Address(__ pre(sp, -2 * wordSize)));
1431       __ far_call(RuntimeAddress(Runtime1::entry_for(Runtime1::slow_subtype_check_id)));
1432       __ ldp(k_RInfo, klass_RInfo, Address(__ post(sp, 2 * wordSize)));
1433       // result is a boolean
1434       __ cbz(k_RInfo, *failure_target);
1435       // successful cast, fall through to profile or jump
1436     }
1437   }
1438   if (should_profile) {
1439     Register mdo  = klass_RInfo, recv = k_RInfo;
1440     __ bind(profile_cast_success);
1441     __ mov_metadata(mdo, md-&gt;constant_encoding());
1442     __ load_klass(recv, obj);
1443     Label update_done;
1444     type_profile_helper(mdo, md, data, recv, success);
1445     __ b(*success);
1446 
1447     __ bind(profile_cast_failure);
1448     __ mov_metadata(mdo, md-&gt;constant_encoding());
1449     Address counter_addr
1450       = __ form_address(rscratch2, mdo,
1451                         md-&gt;byte_offset_of_slot(data, CounterData::count_offset()),
1452                         0);
1453     __ ldr(rscratch1, counter_addr);
1454     __ sub(rscratch1, rscratch1, DataLayout::counter_increment);
1455     __ str(rscratch1, counter_addr);
1456     __ b(*failure);
1457   }
1458   __ b(*success);
1459 }
1460 
1461 
1462 void LIR_Assembler::emit_opTypeCheck(LIR_OpTypeCheck* op) {
1463   const bool should_profile = op-&gt;should_profile();
1464 
1465   LIR_Code code = op-&gt;code();
1466   if (code == lir_store_check) {
1467     Register value = op-&gt;object()-&gt;as_register();
1468     Register array = op-&gt;array()-&gt;as_register();
1469     Register k_RInfo = op-&gt;tmp1()-&gt;as_register();
1470     Register klass_RInfo = op-&gt;tmp2()-&gt;as_register();
1471     Register Rtmp1 = op-&gt;tmp3()-&gt;as_register();
1472 
1473     CodeStub* stub = op-&gt;stub();
1474 
1475     // check if it needs to be profiled
1476     ciMethodData* md;
1477     ciProfileData* data;
1478 
1479     if (should_profile) {
1480       ciMethod* method = op-&gt;profiled_method();
1481       assert(method != NULL, &quot;Should have method&quot;);
1482       int bci = op-&gt;profiled_bci();
1483       md = method-&gt;method_data_or_null();
1484       assert(md != NULL, &quot;Sanity&quot;);
1485       data = md-&gt;bci_to_data(bci);
1486       assert(data != NULL,                &quot;need data for type check&quot;);
1487       assert(data-&gt;is_ReceiverTypeData(), &quot;need ReceiverTypeData for type check&quot;);
1488     }
1489     Label profile_cast_success, profile_cast_failure, done;
1490     Label *success_target = should_profile ? &amp;profile_cast_success : &amp;done;
1491     Label *failure_target = should_profile ? &amp;profile_cast_failure : stub-&gt;entry();
1492 
1493     if (should_profile) {
1494       Label not_null;
1495       __ cbnz(value, not_null);
1496       // Object is null; update MDO and exit
1497       Register mdo  = klass_RInfo;
1498       __ mov_metadata(mdo, md-&gt;constant_encoding());
1499       Address data_addr
1500         = __ form_address(rscratch2, mdo,
1501                           md-&gt;byte_offset_of_slot(data, DataLayout::flags_offset()),
1502                           0);
1503       __ ldrb(rscratch1, data_addr);
1504       __ orr(rscratch1, rscratch1, BitData::null_seen_byte_constant());
1505       __ strb(rscratch1, data_addr);
1506       __ b(done);
1507       __ bind(not_null);
1508     } else {
1509       __ cbz(value, done);
1510     }
1511 
1512     add_debug_info_for_null_check_here(op-&gt;info_for_exception());
1513     __ load_klass(k_RInfo, array);
1514     __ load_klass(klass_RInfo, value);
1515 
1516     // get instance klass (it&#39;s already uncompressed)
1517     __ ldr(k_RInfo, Address(k_RInfo, ObjArrayKlass::element_klass_offset()));
1518     // perform the fast part of the checking logic
1519     __ check_klass_subtype_fast_path(klass_RInfo, k_RInfo, Rtmp1, success_target, failure_target, NULL);
1520     // call out-of-line instance of __ check_klass_subtype_slow_path(...):
1521     __ stp(klass_RInfo, k_RInfo, Address(__ pre(sp, -2 * wordSize)));
1522     __ far_call(RuntimeAddress(Runtime1::entry_for(Runtime1::slow_subtype_check_id)));
1523     __ ldp(k_RInfo, klass_RInfo, Address(__ post(sp, 2 * wordSize)));
1524     // result is a boolean
1525     __ cbzw(k_RInfo, *failure_target);
1526     // fall through to the success case
1527 
1528     if (should_profile) {
1529       Register mdo  = klass_RInfo, recv = k_RInfo;
1530       __ bind(profile_cast_success);
1531       __ mov_metadata(mdo, md-&gt;constant_encoding());
1532       __ load_klass(recv, value);
1533       Label update_done;
1534       type_profile_helper(mdo, md, data, recv, &amp;done);
1535       __ b(done);
1536 
1537       __ bind(profile_cast_failure);
1538       __ mov_metadata(mdo, md-&gt;constant_encoding());
1539       Address counter_addr(mdo, md-&gt;byte_offset_of_slot(data, CounterData::count_offset()));
1540       __ lea(rscratch2, counter_addr);
1541       __ ldr(rscratch1, Address(rscratch2));
1542       __ sub(rscratch1, rscratch1, DataLayout::counter_increment);
1543       __ str(rscratch1, Address(rscratch2));
1544       __ b(*stub-&gt;entry());
1545     }
1546 
1547     __ bind(done);
1548   } else if (code == lir_checkcast) {
1549     Register obj = op-&gt;object()-&gt;as_register();
1550     Register dst = op-&gt;result_opr()-&gt;as_register();
1551     Label success;
1552     emit_typecheck_helper(op, &amp;success, op-&gt;stub()-&gt;entry(), &amp;success);
1553     __ bind(success);
1554     if (dst != obj) {
1555       __ mov(dst, obj);
1556     }
1557   } else if (code == lir_instanceof) {
1558     Register obj = op-&gt;object()-&gt;as_register();
1559     Register dst = op-&gt;result_opr()-&gt;as_register();
1560     Label success, failure, done;
1561     emit_typecheck_helper(op, &amp;success, &amp;failure, &amp;failure);
1562     __ bind(failure);
1563     __ mov(dst, zr);
1564     __ b(done);
1565     __ bind(success);
1566     __ mov(dst, 1);
1567     __ bind(done);
1568   } else {
1569     ShouldNotReachHere();
1570   }
1571 }
1572 
1573 void LIR_Assembler::emit_opFlattenedArrayCheck(LIR_OpFlattenedArrayCheck* op) {
1574   // We are loading/storing an array that *may* be a flattened array (the declared type
1575   // Object[], interface[], or VT?[]). If this array is flattened, take slow path.
1576 
1577   __ load_storage_props(op-&gt;tmp()-&gt;as_register(), op-&gt;array()-&gt;as_register());
1578   __ tst(op-&gt;tmp()-&gt;as_register(), ArrayStorageProperties::flattened_value);
1579   __ br(Assembler::NE, *op-&gt;stub()-&gt;entry());
1580   if (!op-&gt;value()-&gt;is_illegal()) {
1581     // We are storing into the array.
1582     Label skip;
1583     __ tst(op-&gt;tmp()-&gt;as_register(), ArrayStorageProperties::null_free_value);
1584     __ br(Assembler::EQ, skip);
1585     // The array is not flattened, but it is null_free. If we are storing
1586     // a null, take the slow path (which will throw NPE).
1587     __ cbz(op-&gt;value()-&gt;as_register(), *op-&gt;stub()-&gt;entry());
1588     __ bind(skip);
1589   }
1590 
1591 }
1592 
1593 void LIR_Assembler::emit_opNullFreeArrayCheck(LIR_OpNullFreeArrayCheck* op) {
1594   // This is called when we use aastore into a an array declared as &quot;[LVT;&quot;,
1595   // where we know VT is not flattenable (due to ValueArrayElemMaxFlatOops, etc).
1596   // However, we need to do a NULL check if the actual array is a &quot;[QVT;&quot;.
1597 
1598   __ load_storage_props(op-&gt;tmp()-&gt;as_register(), op-&gt;array()-&gt;as_register());
1599   __ mov(rscratch1, (uint64_t) ArrayStorageProperties::null_free_value);
1600   __ cmp(op-&gt;tmp()-&gt;as_register(), rscratch1);
1601 }
1602 
1603 void LIR_Assembler::emit_opSubstitutabilityCheck(LIR_OpSubstitutabilityCheck* op) {
1604   Label L_oops_equal;
1605   Label L_oops_not_equal;
1606   Label L_end;
1607 
1608   Register left  = op-&gt;left()-&gt;as_register();
1609   Register right = op-&gt;right()-&gt;as_register();
1610 
1611   __ cmp(left, right);
1612   __ br(Assembler::EQ, L_oops_equal);
1613 
1614   // (1) Null check -- if one of the operands is null, the other must not be null (because
1615   //     the two references are not equal), so they are not substitutable,
1616   //     FIXME: do null check only if the operand is nullable
1617   {
1618     __ cbz(left, L_oops_not_equal);
1619     __ cbz(right, L_oops_not_equal);
1620   }
1621 
1622 
1623   ciKlass* left_klass = op-&gt;left_klass();
1624   ciKlass* right_klass = op-&gt;right_klass();
1625 
1626   // (2) Value object check -- if either of the operands is not a value object,
1627   //     they are not substitutable. We do this only if we are not sure that the
1628   //     operands are value objects
1629   if ((left_klass == NULL || right_klass == NULL) ||// The klass is still unloaded, or came from a Phi node.
1630       !left_klass-&gt;is_valuetype() || !right_klass-&gt;is_valuetype()) {
1631     Register tmp1  = rscratch1; /* op-&gt;tmp1()-&gt;as_register(); */
1632     Register tmp2  = rscratch2; /* op-&gt;tmp2()-&gt;as_register(); */
1633 
1634     __ mov(tmp1, (intptr_t)markWord::always_locked_pattern);
1635 
1636     __ ldr(tmp2, Address(left, oopDesc::mark_offset_in_bytes()));
1637     __ andr(tmp1, tmp1, tmp2);
1638 
1639     __ ldr(tmp2, Address(right, oopDesc::mark_offset_in_bytes()));
<a name="5" id="anc5"></a><span class="line-modified">1640     __ andr(tmp1, tmp1, tmp2); </span>
1641 
1642     __ mov(tmp2, (intptr_t)markWord::always_locked_pattern);
<a name="6" id="anc6"></a><span class="line-modified">1643     __ cmp(tmp1, tmp2); </span>
1644     __ br(Assembler::NE, L_oops_not_equal);
1645   }
1646 
1647   // (3) Same klass check: if the operands are of different klasses, they are not substitutable.
1648   if (left_klass != NULL &amp;&amp; left_klass-&gt;is_valuetype() &amp;&amp; left_klass == right_klass) {
1649     // No need to load klass -- the operands are statically known to be the same value klass.
1650     __ b(*op-&gt;stub()-&gt;entry());
1651   } else {
1652     Register left_klass_op = op-&gt;left_klass_op()-&gt;as_register();
1653     Register right_klass_op = op-&gt;right_klass_op()-&gt;as_register();
1654 
1655     if (UseCompressedOops) {
1656       __ ldrw(left_klass_op,  Address(left,  oopDesc::klass_offset_in_bytes()));
1657       __ ldrw(right_klass_op, Address(right, oopDesc::klass_offset_in_bytes()));
1658       __ cmpw(left_klass_op, right_klass_op);
1659     } else {
1660       __ ldr(left_klass_op,  Address(left,  oopDesc::klass_offset_in_bytes()));
1661       __ ldr(right_klass_op, Address(right, oopDesc::klass_offset_in_bytes()));
1662       __ cmp(left_klass_op, right_klass_op);
1663     }
1664 
1665     __ br(Assembler::EQ, *op-&gt;stub()-&gt;entry()); // same klass -&gt; do slow check
1666     // fall through to L_oops_not_equal
1667   }
1668 
1669   __ bind(L_oops_not_equal);
1670   move(op-&gt;not_equal_result(), op-&gt;result_opr());
1671   __ b(L_end);
1672 
1673   __ bind(L_oops_equal);
1674   move(op-&gt;equal_result(), op-&gt;result_opr());
1675   __ b(L_end);
1676 
1677   // We&#39;ve returned from the stub. op-&gt;result_opr() contains 0x0 IFF the two
1678   // operands are not substitutable. (Don&#39;t compare against 0x1 in case the
1679   // C compiler is naughty)
1680   __ bind(*op-&gt;stub()-&gt;continuation());
1681 
1682   if (op-&gt;result_opr()-&gt;type() == T_LONG) {
1683     __ cbzw(op-&gt;result_opr()-&gt;as_register(), L_oops_not_equal); // (call_stub() == 0x0) -&gt; not_equal
1684   } else {
1685     __ cbz(op-&gt;result_opr()-&gt;as_register(), L_oops_not_equal); // (call_stub() == 0x0) -&gt; not_equal
1686   }
1687 
1688   move(op-&gt;equal_result(), op-&gt;result_opr()); // (call_stub() != 0x0) -&gt; equal
1689   // fall-through
1690   __ bind(L_end);
1691 
1692 }
1693 
1694 
1695 void LIR_Assembler::casw(Register addr, Register newval, Register cmpval) {
1696   __ cmpxchg(addr, cmpval, newval, Assembler::word, /* acquire*/ true, /* release*/ true, /* weak*/ false, rscratch1);
1697   __ cset(rscratch1, Assembler::NE);
1698   __ membar(__ AnyAny);
1699 }
1700 
1701 void LIR_Assembler::casl(Register addr, Register newval, Register cmpval) {
1702   __ cmpxchg(addr, cmpval, newval, Assembler::xword, /* acquire*/ true, /* release*/ true, /* weak*/ false, rscratch1);
1703   __ cset(rscratch1, Assembler::NE);
1704   __ membar(__ AnyAny);
1705 }
1706 
1707 
1708 void LIR_Assembler::emit_compare_and_swap(LIR_OpCompareAndSwap* op) {
1709   assert(VM_Version::supports_cx8(), &quot;wrong machine&quot;);
1710   Register addr;
1711   if (op-&gt;addr()-&gt;is_register()) {
1712     addr = as_reg(op-&gt;addr());
1713   } else {
1714     assert(op-&gt;addr()-&gt;is_address(), &quot;what else?&quot;);
1715     LIR_Address* addr_ptr = op-&gt;addr()-&gt;as_address_ptr();
1716     assert(addr_ptr-&gt;disp() == 0, &quot;need 0 disp&quot;);
1717     assert(addr_ptr-&gt;index() == LIR_OprDesc::illegalOpr(), &quot;need 0 index&quot;);
1718     addr = as_reg(addr_ptr-&gt;base());
1719   }
1720   Register newval = as_reg(op-&gt;new_value());
1721   Register cmpval = as_reg(op-&gt;cmp_value());
1722 
1723   if (op-&gt;code() == lir_cas_obj) {
1724     if (UseCompressedOops) {
1725       Register t1 = op-&gt;tmp1()-&gt;as_register();
1726       assert(op-&gt;tmp1()-&gt;is_valid(), &quot;must be&quot;);
1727       __ encode_heap_oop(t1, cmpval);
1728       cmpval = t1;
1729       __ encode_heap_oop(rscratch2, newval);
1730       newval = rscratch2;
1731       casw(addr, newval, cmpval);
1732     } else {
1733       casl(addr, newval, cmpval);
1734     }
1735   } else if (op-&gt;code() == lir_cas_int) {
1736     casw(addr, newval, cmpval);
1737   } else {
1738     casl(addr, newval, cmpval);
1739   }
1740 }
1741 
1742 
1743 void LIR_Assembler::cmove(LIR_Condition condition, LIR_Opr opr1, LIR_Opr opr2, LIR_Opr result, BasicType type) {
1744 
1745   Assembler::Condition acond, ncond;
1746   switch (condition) {
1747   case lir_cond_equal:        acond = Assembler::EQ; ncond = Assembler::NE; break;
1748   case lir_cond_notEqual:     acond = Assembler::NE; ncond = Assembler::EQ; break;
1749   case lir_cond_less:         acond = Assembler::LT; ncond = Assembler::GE; break;
1750   case lir_cond_lessEqual:    acond = Assembler::LE; ncond = Assembler::GT; break;
1751   case lir_cond_greaterEqual: acond = Assembler::GE; ncond = Assembler::LT; break;
1752   case lir_cond_greater:      acond = Assembler::GT; ncond = Assembler::LE; break;
1753   case lir_cond_belowEqual:
1754   case lir_cond_aboveEqual:
1755   default:                    ShouldNotReachHere();
1756     acond = Assembler::EQ; ncond = Assembler::NE;  // unreachable
1757   }
1758 
1759   assert(result-&gt;is_single_cpu() || result-&gt;is_double_cpu(),
1760          &quot;expect single register for result&quot;);
1761   if (opr1-&gt;is_constant() &amp;&amp; opr2-&gt;is_constant()
1762       &amp;&amp; opr1-&gt;type() == T_INT &amp;&amp; opr2-&gt;type() == T_INT) {
1763     jint val1 = opr1-&gt;as_jint();
1764     jint val2 = opr2-&gt;as_jint();
1765     if (val1 == 0 &amp;&amp; val2 == 1) {
1766       __ cset(result-&gt;as_register(), ncond);
1767       return;
1768     } else if (val1 == 1 &amp;&amp; val2 == 0) {
1769       __ cset(result-&gt;as_register(), acond);
1770       return;
1771     }
1772   }
1773 
1774   if (opr1-&gt;is_constant() &amp;&amp; opr2-&gt;is_constant()
1775       &amp;&amp; opr1-&gt;type() == T_LONG &amp;&amp; opr2-&gt;type() == T_LONG) {
1776     jlong val1 = opr1-&gt;as_jlong();
1777     jlong val2 = opr2-&gt;as_jlong();
1778     if (val1 == 0 &amp;&amp; val2 == 1) {
1779       __ cset(result-&gt;as_register_lo(), ncond);
1780       return;
1781     } else if (val1 == 1 &amp;&amp; val2 == 0) {
1782       __ cset(result-&gt;as_register_lo(), acond);
1783       return;
1784     }
1785   }
1786 
1787   if (opr1-&gt;is_stack()) {
1788     stack2reg(opr1, FrameMap::rscratch1_opr, result-&gt;type());
1789     opr1 = FrameMap::rscratch1_opr;
1790   } else if (opr1-&gt;is_constant()) {
1791     LIR_Opr tmp
1792       = opr1-&gt;type() == T_LONG ? FrameMap::rscratch1_long_opr : FrameMap::rscratch1_opr;
1793     const2reg(opr1, tmp, lir_patch_none, NULL);
1794     opr1 = tmp;
1795   }
1796 
1797   if (opr2-&gt;is_stack()) {
1798     stack2reg(opr2, FrameMap::rscratch2_opr, result-&gt;type());
1799     opr2 = FrameMap::rscratch2_opr;
1800   } else if (opr2-&gt;is_constant()) {
1801     LIR_Opr tmp
1802       = opr2-&gt;type() == T_LONG ? FrameMap::rscratch2_long_opr : FrameMap::rscratch2_opr;
1803     const2reg(opr2, tmp, lir_patch_none, NULL);
1804     opr2 = tmp;
1805   }
1806 
1807   if (result-&gt;type() == T_LONG)
1808     __ csel(result-&gt;as_register_lo(), opr1-&gt;as_register_lo(), opr2-&gt;as_register_lo(), acond);
1809   else
1810     __ csel(result-&gt;as_register(), opr1-&gt;as_register(), opr2-&gt;as_register(), acond);
1811 }
1812 
1813 void LIR_Assembler::arith_op(LIR_Code code, LIR_Opr left, LIR_Opr right, LIR_Opr dest, CodeEmitInfo* info, bool pop_fpu_stack) {
1814   assert(info == NULL, &quot;should never be used, idiv/irem and ldiv/lrem not handled by this method&quot;);
1815 
1816   if (left-&gt;is_single_cpu()) {
1817     Register lreg = left-&gt;as_register();
1818     Register dreg = as_reg(dest);
1819 
1820     if (right-&gt;is_single_cpu()) {
1821       // cpu register - cpu register
1822 
1823       assert(left-&gt;type() == T_INT &amp;&amp; right-&gt;type() == T_INT &amp;&amp; dest-&gt;type() == T_INT,
1824              &quot;should be&quot;);
1825       Register rreg = right-&gt;as_register();
1826       switch (code) {
1827       case lir_add: __ addw (dest-&gt;as_register(), lreg, rreg); break;
1828       case lir_sub: __ subw (dest-&gt;as_register(), lreg, rreg); break;
1829       case lir_mul: __ mulw (dest-&gt;as_register(), lreg, rreg); break;
1830       default:      ShouldNotReachHere();
1831       }
1832 
1833     } else if (right-&gt;is_double_cpu()) {
1834       Register rreg = right-&gt;as_register_lo();
1835       // single_cpu + double_cpu: can happen with obj+long
1836       assert(code == lir_add || code == lir_sub, &quot;mismatched arithmetic op&quot;);
1837       switch (code) {
1838       case lir_add: __ add(dreg, lreg, rreg); break;
1839       case lir_sub: __ sub(dreg, lreg, rreg); break;
1840       default: ShouldNotReachHere();
1841       }
1842     } else if (right-&gt;is_constant()) {
1843       // cpu register - constant
1844       jlong c;
1845 
1846       // FIXME.  This is fugly: we really need to factor all this logic.
1847       switch(right-&gt;type()) {
1848       case T_LONG:
1849         c = right-&gt;as_constant_ptr()-&gt;as_jlong();
1850         break;
1851       case T_INT:
1852       case T_ADDRESS:
1853         c = right-&gt;as_constant_ptr()-&gt;as_jint();
1854         break;
1855       default:
1856         ShouldNotReachHere();
1857         c = 0;  // unreachable
1858         break;
1859       }
1860 
1861       assert(code == lir_add || code == lir_sub, &quot;mismatched arithmetic op&quot;);
1862       if (c == 0 &amp;&amp; dreg == lreg) {
1863         COMMENT(&quot;effective nop elided&quot;);
1864         return;
1865       }
1866       switch(left-&gt;type()) {
1867       case T_INT:
1868         switch (code) {
1869         case lir_add: __ addw(dreg, lreg, c); break;
1870         case lir_sub: __ subw(dreg, lreg, c); break;
1871         default: ShouldNotReachHere();
1872         }
1873         break;
1874       case T_OBJECT:
1875       case T_ADDRESS:
1876         switch (code) {
1877         case lir_add: __ add(dreg, lreg, c); break;
1878         case lir_sub: __ sub(dreg, lreg, c); break;
1879         default: ShouldNotReachHere();
1880         }
1881         break;
1882       default:
1883         ShouldNotReachHere();
1884       }
1885     } else {
1886       ShouldNotReachHere();
1887     }
1888 
1889   } else if (left-&gt;is_double_cpu()) {
1890     Register lreg_lo = left-&gt;as_register_lo();
1891 
1892     if (right-&gt;is_double_cpu()) {
1893       // cpu register - cpu register
1894       Register rreg_lo = right-&gt;as_register_lo();
1895       switch (code) {
1896       case lir_add: __ add (dest-&gt;as_register_lo(), lreg_lo, rreg_lo); break;
1897       case lir_sub: __ sub (dest-&gt;as_register_lo(), lreg_lo, rreg_lo); break;
1898       case lir_mul: __ mul (dest-&gt;as_register_lo(), lreg_lo, rreg_lo); break;
1899       case lir_div: __ corrected_idivq(dest-&gt;as_register_lo(), lreg_lo, rreg_lo, false, rscratch1); break;
1900       case lir_rem: __ corrected_idivq(dest-&gt;as_register_lo(), lreg_lo, rreg_lo, true, rscratch1); break;
1901       default:
1902         ShouldNotReachHere();
1903       }
1904 
1905     } else if (right-&gt;is_constant()) {
1906       jlong c = right-&gt;as_constant_ptr()-&gt;as_jlong();
1907       Register dreg = as_reg(dest);
1908       switch (code) {
1909         case lir_add:
1910         case lir_sub:
1911           if (c == 0 &amp;&amp; dreg == lreg_lo) {
1912             COMMENT(&quot;effective nop elided&quot;);
1913             return;
1914           }
1915           code == lir_add ? __ add(dreg, lreg_lo, c) : __ sub(dreg, lreg_lo, c);
1916           break;
1917         case lir_div:
1918           assert(c &gt; 0 &amp;&amp; is_power_of_2(c), &quot;divisor must be power-of-2 constant&quot;);
1919           if (c == 1) {
1920             // move lreg_lo to dreg if divisor is 1
1921             __ mov(dreg, lreg_lo);
1922           } else {
1923             unsigned int shift = exact_log2_long(c);
1924             // use rscratch1 as intermediate result register
1925             __ asr(rscratch1, lreg_lo, 63);
1926             __ add(rscratch1, lreg_lo, rscratch1, Assembler::LSR, 64 - shift);
1927             __ asr(dreg, rscratch1, shift);
1928           }
1929           break;
1930         case lir_rem:
1931           assert(c &gt; 0 &amp;&amp; is_power_of_2(c), &quot;divisor must be power-of-2 constant&quot;);
1932           if (c == 1) {
1933             // move 0 to dreg if divisor is 1
1934             __ mov(dreg, zr);
1935           } else {
1936             // use rscratch1 as intermediate result register
1937             __ negs(rscratch1, lreg_lo);
1938             __ andr(dreg, lreg_lo, c - 1);
1939             __ andr(rscratch1, rscratch1, c - 1);
1940             __ csneg(dreg, dreg, rscratch1, Assembler::MI);
1941           }
1942           break;
1943         default:
1944           ShouldNotReachHere();
1945       }
1946     } else {
1947       ShouldNotReachHere();
1948     }
1949   } else if (left-&gt;is_single_fpu()) {
1950     assert(right-&gt;is_single_fpu(), &quot;right hand side of float arithmetics needs to be float register&quot;);
1951     switch (code) {
1952     case lir_add: __ fadds (dest-&gt;as_float_reg(), left-&gt;as_float_reg(), right-&gt;as_float_reg()); break;
1953     case lir_sub: __ fsubs (dest-&gt;as_float_reg(), left-&gt;as_float_reg(), right-&gt;as_float_reg()); break;
1954     case lir_mul_strictfp: // fall through
1955     case lir_mul: __ fmuls (dest-&gt;as_float_reg(), left-&gt;as_float_reg(), right-&gt;as_float_reg()); break;
1956     case lir_div_strictfp: // fall through
1957     case lir_div: __ fdivs (dest-&gt;as_float_reg(), left-&gt;as_float_reg(), right-&gt;as_float_reg()); break;
1958     default:
1959       ShouldNotReachHere();
1960     }
1961   } else if (left-&gt;is_double_fpu()) {
1962     if (right-&gt;is_double_fpu()) {
1963       // fpu register - fpu register
1964       switch (code) {
1965       case lir_add: __ faddd (dest-&gt;as_double_reg(), left-&gt;as_double_reg(), right-&gt;as_double_reg()); break;
1966       case lir_sub: __ fsubd (dest-&gt;as_double_reg(), left-&gt;as_double_reg(), right-&gt;as_double_reg()); break;
1967       case lir_mul_strictfp: // fall through
1968       case lir_mul: __ fmuld (dest-&gt;as_double_reg(), left-&gt;as_double_reg(), right-&gt;as_double_reg()); break;
1969       case lir_div_strictfp: // fall through
1970       case lir_div: __ fdivd (dest-&gt;as_double_reg(), left-&gt;as_double_reg(), right-&gt;as_double_reg()); break;
1971       default:
1972         ShouldNotReachHere();
1973       }
1974     } else {
1975       if (right-&gt;is_constant()) {
1976         ShouldNotReachHere();
1977       }
1978       ShouldNotReachHere();
1979     }
1980   } else if (left-&gt;is_single_stack() || left-&gt;is_address()) {
1981     assert(left == dest, &quot;left and dest must be equal&quot;);
1982     ShouldNotReachHere();
1983   } else {
1984     ShouldNotReachHere();
1985   }
1986 }
1987 
1988 void LIR_Assembler::arith_fpu_implementation(LIR_Code code, int left_index, int right_index, int dest_index, bool pop_fpu_stack) { Unimplemented(); }
1989 
1990 
1991 void LIR_Assembler::intrinsic_op(LIR_Code code, LIR_Opr value, LIR_Opr unused, LIR_Opr dest, LIR_Op* op) {
1992   switch(code) {
1993   case lir_abs : __ fabsd(dest-&gt;as_double_reg(), value-&gt;as_double_reg()); break;
1994   case lir_sqrt: __ fsqrtd(dest-&gt;as_double_reg(), value-&gt;as_double_reg()); break;
1995   default      : ShouldNotReachHere();
1996   }
1997 }
1998 
1999 void LIR_Assembler::logic_op(LIR_Code code, LIR_Opr left, LIR_Opr right, LIR_Opr dst) {
2000 
2001   assert(left-&gt;is_single_cpu() || left-&gt;is_double_cpu(), &quot;expect single or double register&quot;);
2002   Register Rleft = left-&gt;is_single_cpu() ? left-&gt;as_register() :
2003                                            left-&gt;as_register_lo();
2004    if (dst-&gt;is_single_cpu()) {
2005      Register Rdst = dst-&gt;as_register();
2006      if (right-&gt;is_constant()) {
2007        switch (code) {
2008          case lir_logic_and: __ andw (Rdst, Rleft, right-&gt;as_jint()); break;
2009          case lir_logic_or:  __ orrw (Rdst, Rleft, right-&gt;as_jint()); break;
2010          case lir_logic_xor: __ eorw (Rdst, Rleft, right-&gt;as_jint()); break;
2011          default: ShouldNotReachHere(); break;
2012        }
2013      } else {
2014        Register Rright = right-&gt;is_single_cpu() ? right-&gt;as_register() :
2015                                                   right-&gt;as_register_lo();
2016        switch (code) {
2017          case lir_logic_and: __ andw (Rdst, Rleft, Rright); break;
2018          case lir_logic_or:  __ orrw (Rdst, Rleft, Rright); break;
2019          case lir_logic_xor: __ eorw (Rdst, Rleft, Rright); break;
2020          default: ShouldNotReachHere(); break;
2021        }
2022      }
2023    } else {
2024      Register Rdst = dst-&gt;as_register_lo();
2025      if (right-&gt;is_constant()) {
2026        switch (code) {
2027          case lir_logic_and: __ andr (Rdst, Rleft, right-&gt;as_jlong()); break;
2028          case lir_logic_or:  __ orr (Rdst, Rleft, right-&gt;as_jlong()); break;
2029          case lir_logic_xor: __ eor (Rdst, Rleft, right-&gt;as_jlong()); break;
2030          default: ShouldNotReachHere(); break;
2031        }
2032      } else {
2033        Register Rright = right-&gt;is_single_cpu() ? right-&gt;as_register() :
2034                                                   right-&gt;as_register_lo();
2035        switch (code) {
2036          case lir_logic_and: __ andr (Rdst, Rleft, Rright); break;
2037          case lir_logic_or:  __ orr (Rdst, Rleft, Rright); break;
2038          case lir_logic_xor: __ eor (Rdst, Rleft, Rright); break;
2039          default: ShouldNotReachHere(); break;
2040        }
2041      }
2042    }
2043 }
2044 
2045 
2046 
2047 void LIR_Assembler::arithmetic_idiv(LIR_Code code, LIR_Opr left, LIR_Opr right, LIR_Opr illegal, LIR_Opr result, CodeEmitInfo* info) {
2048 
2049   // opcode check
2050   assert((code == lir_idiv) || (code == lir_irem), &quot;opcode must be idiv or irem&quot;);
2051   bool is_irem = (code == lir_irem);
2052 
2053   // operand check
2054   assert(left-&gt;is_single_cpu(),   &quot;left must be register&quot;);
2055   assert(right-&gt;is_single_cpu() || right-&gt;is_constant(),  &quot;right must be register or constant&quot;);
2056   assert(result-&gt;is_single_cpu(), &quot;result must be register&quot;);
2057   Register lreg = left-&gt;as_register();
2058   Register dreg = result-&gt;as_register();
2059 
2060   // power-of-2 constant check and codegen
2061   if (right-&gt;is_constant()) {
2062     int c = right-&gt;as_constant_ptr()-&gt;as_jint();
2063     assert(c &gt; 0 &amp;&amp; is_power_of_2(c), &quot;divisor must be power-of-2 constant&quot;);
2064     if (is_irem) {
2065       if (c == 1) {
2066         // move 0 to dreg if divisor is 1
2067         __ movw(dreg, zr);
2068       } else {
2069         // use rscratch1 as intermediate result register
2070         __ negsw(rscratch1, lreg);
2071         __ andw(dreg, lreg, c - 1);
2072         __ andw(rscratch1, rscratch1, c - 1);
2073         __ csnegw(dreg, dreg, rscratch1, Assembler::MI);
2074       }
2075     } else {
2076       if (c == 1) {
2077         // move lreg to dreg if divisor is 1
2078         __ movw(dreg, lreg);
2079       } else {
2080         unsigned int shift = exact_log2(c);
2081         // use rscratch1 as intermediate result register
2082         __ asrw(rscratch1, lreg, 31);
2083         __ addw(rscratch1, lreg, rscratch1, Assembler::LSR, 32 - shift);
2084         __ asrw(dreg, rscratch1, shift);
2085       }
2086     }
2087   } else {
2088     Register rreg = right-&gt;as_register();
2089     __ corrected_idivl(dreg, lreg, rreg, is_irem, rscratch1);
2090   }
2091 }
2092 
2093 
2094 void LIR_Assembler::comp_op(LIR_Condition condition, LIR_Opr opr1, LIR_Opr opr2, LIR_Op2* op) {
2095   if (opr1-&gt;is_constant() &amp;&amp; opr2-&gt;is_single_cpu()) {
2096     // tableswitch
2097     Register reg = as_reg(opr2);
2098     struct tableswitch &amp;table = switches[opr1-&gt;as_constant_ptr()-&gt;as_jint()];
2099     __ tableswitch(reg, table._first_key, table._last_key, table._branches, table._after);
2100   } else if (opr1-&gt;is_single_cpu() || opr1-&gt;is_double_cpu()) {
2101     Register reg1 = as_reg(opr1);
2102     if (opr2-&gt;is_single_cpu()) {
2103       // cpu register - cpu register
2104       Register reg2 = opr2-&gt;as_register();
2105       if (is_reference_type(opr1-&gt;type())) {
2106         __ cmpoop(reg1, reg2);
2107       } else {
2108         assert(!is_reference_type(opr2-&gt;type()), &quot;cmp int, oop?&quot;);
2109         __ cmpw(reg1, reg2);
2110       }
2111       return;
2112     }
2113     if (opr2-&gt;is_double_cpu()) {
2114       // cpu register - cpu register
2115       Register reg2 = opr2-&gt;as_register_lo();
2116       __ cmp(reg1, reg2);
2117       return;
2118     }
2119 
2120     if (opr2-&gt;is_constant()) {
2121       bool is_32bit = false; // width of register operand
2122       jlong imm;
2123 
2124       switch(opr2-&gt;type()) {
2125       case T_INT:
2126         imm = opr2-&gt;as_constant_ptr()-&gt;as_jint();
2127         is_32bit = true;
2128         break;
2129       case T_LONG:
2130         imm = opr2-&gt;as_constant_ptr()-&gt;as_jlong();
2131         break;
2132       case T_ADDRESS:
2133         imm = opr2-&gt;as_constant_ptr()-&gt;as_jint();
2134         break;
2135       case T_METADATA:
2136         imm = (intptr_t)(opr2-&gt;as_constant_ptr()-&gt;as_metadata());
2137         break;
2138       case T_VALUETYPE:
2139       case T_OBJECT:
2140       case T_ARRAY:
2141         jobject2reg(opr2-&gt;as_constant_ptr()-&gt;as_jobject(), rscratch1);
2142         __ cmpoop(reg1, rscratch1);
2143         return;
2144       default:
2145         ShouldNotReachHere();
2146         imm = 0;  // unreachable
2147         break;
2148       }
2149 
2150       if (Assembler::operand_valid_for_add_sub_immediate(imm)) {
2151         if (is_32bit)
2152           __ cmpw(reg1, imm);
2153         else
2154           __ subs(zr, reg1, imm);
2155         return;
2156       } else {
2157         __ mov(rscratch1, imm);
2158         if (is_32bit)
2159           __ cmpw(reg1, rscratch1);
2160         else
2161           __ cmp(reg1, rscratch1);
2162         return;
2163       }
2164     } else
2165       ShouldNotReachHere();
2166   } else if (opr1-&gt;is_single_fpu()) {
2167     FloatRegister reg1 = opr1-&gt;as_float_reg();
2168     assert(opr2-&gt;is_single_fpu(), &quot;expect single float register&quot;);
2169     FloatRegister reg2 = opr2-&gt;as_float_reg();
2170     __ fcmps(reg1, reg2);
2171   } else if (opr1-&gt;is_double_fpu()) {
2172     FloatRegister reg1 = opr1-&gt;as_double_reg();
2173     assert(opr2-&gt;is_double_fpu(), &quot;expect double float register&quot;);
2174     FloatRegister reg2 = opr2-&gt;as_double_reg();
2175     __ fcmpd(reg1, reg2);
2176   } else {
2177     ShouldNotReachHere();
2178   }
2179 }
2180 
2181 void LIR_Assembler::comp_fl2i(LIR_Code code, LIR_Opr left, LIR_Opr right, LIR_Opr dst, LIR_Op2* op){
2182   if (code == lir_cmp_fd2i || code == lir_ucmp_fd2i) {
2183     bool is_unordered_less = (code == lir_ucmp_fd2i);
2184     if (left-&gt;is_single_fpu()) {
2185       __ float_cmp(true, is_unordered_less ? -1 : 1, left-&gt;as_float_reg(), right-&gt;as_float_reg(), dst-&gt;as_register());
2186     } else if (left-&gt;is_double_fpu()) {
2187       __ float_cmp(false, is_unordered_less ? -1 : 1, left-&gt;as_double_reg(), right-&gt;as_double_reg(), dst-&gt;as_register());
2188     } else {
2189       ShouldNotReachHere();
2190     }
2191   } else if (code == lir_cmp_l2i) {
2192     Label done;
2193     __ cmp(left-&gt;as_register_lo(), right-&gt;as_register_lo());
2194     __ mov(dst-&gt;as_register(), (u_int64_t)-1L);
2195     __ br(Assembler::LT, done);
2196     __ csinc(dst-&gt;as_register(), zr, zr, Assembler::EQ);
2197     __ bind(done);
2198   } else {
2199     ShouldNotReachHere();
2200   }
2201 }
2202 
2203 
2204 void LIR_Assembler::align_call(LIR_Code code) {  }
2205 
2206 
2207 void LIR_Assembler::call(LIR_OpJavaCall* op, relocInfo::relocType rtype) {
2208   address call = __ trampoline_call(Address(op-&gt;addr(), rtype));
2209   if (call == NULL) {
2210     bailout(&quot;trampoline stub overflow&quot;);
2211     return;
2212   }
2213   add_call_info(code_offset(), op-&gt;info());
2214 }
2215 
2216 
2217 void LIR_Assembler::ic_call(LIR_OpJavaCall* op) {
2218   address call = __ ic_call(op-&gt;addr());
2219   if (call == NULL) {
2220     bailout(&quot;trampoline stub overflow&quot;);
2221     return;
2222   }
2223   add_call_info(code_offset(), op-&gt;info());
2224 }
2225 
2226 
2227 /* Currently, vtable-dispatch is only enabled for sparc platforms */
2228 void LIR_Assembler::vtable_call(LIR_OpJavaCall* op) {
2229   ShouldNotReachHere();
2230 }
2231 
2232 
2233 void LIR_Assembler::emit_static_call_stub() {
2234   address call_pc = __ pc();
2235   address stub = __ start_a_stub(call_stub_size());
2236   if (stub == NULL) {
2237     bailout(&quot;static call stub overflow&quot;);
2238     return;
2239   }
2240 
2241   int start = __ offset();
2242 
2243   __ relocate(static_stub_Relocation::spec(call_pc));
2244   __ emit_static_call_stub();
2245 
2246   assert(__ offset() - start + CompiledStaticCall::to_trampoline_stub_size()
2247         &lt;= call_stub_size(), &quot;stub too big&quot;);
2248   __ end_a_stub();
2249 }
2250 
2251 
2252 void LIR_Assembler::throw_op(LIR_Opr exceptionPC, LIR_Opr exceptionOop, CodeEmitInfo* info) {
2253   assert(exceptionOop-&gt;as_register() == r0, &quot;must match&quot;);
2254   assert(exceptionPC-&gt;as_register() == r3, &quot;must match&quot;);
2255 
2256   // exception object is not added to oop map by LinearScan
2257   // (LinearScan assumes that no oops are in fixed registers)
2258   info-&gt;add_register_oop(exceptionOop);
2259   Runtime1::StubID unwind_id;
2260 
2261   // get current pc information
2262   // pc is only needed if the method has an exception handler, the unwind code does not need it.
2263   int pc_for_athrow_offset = __ offset();
2264   InternalAddress pc_for_athrow(__ pc());
2265   __ adr(exceptionPC-&gt;as_register(), pc_for_athrow);
2266   add_call_info(pc_for_athrow_offset, info); // for exception handler
2267 
2268   __ verify_not_null_oop(r0);
2269   // search an exception handler (r0: exception oop, r3: throwing pc)
2270   if (compilation()-&gt;has_fpu_code()) {
2271     unwind_id = Runtime1::handle_exception_id;
2272   } else {
2273     unwind_id = Runtime1::handle_exception_nofpu_id;
2274   }
2275   __ far_call(RuntimeAddress(Runtime1::entry_for(unwind_id)));
2276 
2277   // FIXME: enough room for two byte trap   ????
2278   __ nop();
2279 }
2280 
2281 
2282 void LIR_Assembler::unwind_op(LIR_Opr exceptionOop) {
2283   assert(exceptionOop-&gt;as_register() == r0, &quot;must match&quot;);
2284 
2285   __ b(_unwind_handler_entry);
2286 }
2287 
2288 
2289 void LIR_Assembler::shift_op(LIR_Code code, LIR_Opr left, LIR_Opr count, LIR_Opr dest, LIR_Opr tmp) {
2290   Register lreg = left-&gt;is_single_cpu() ? left-&gt;as_register() : left-&gt;as_register_lo();
2291   Register dreg = dest-&gt;is_single_cpu() ? dest-&gt;as_register() : dest-&gt;as_register_lo();
2292 
2293   switch (left-&gt;type()) {
2294     case T_INT: {
2295       switch (code) {
2296       case lir_shl:  __ lslvw (dreg, lreg, count-&gt;as_register()); break;
2297       case lir_shr:  __ asrvw (dreg, lreg, count-&gt;as_register()); break;
2298       case lir_ushr: __ lsrvw (dreg, lreg, count-&gt;as_register()); break;
2299       default:
2300         ShouldNotReachHere();
2301         break;
2302       }
2303       break;
2304     case T_LONG:
<a name="7" id="anc7"></a><span class="line-modified">2305     case T_VALUETYPE: </span>
2306     case T_ADDRESS:
2307     case T_OBJECT:
2308       switch (code) {
2309       case lir_shl:  __ lslv (dreg, lreg, count-&gt;as_register()); break;
2310       case lir_shr:  __ asrv (dreg, lreg, count-&gt;as_register()); break;
2311       case lir_ushr: __ lsrv (dreg, lreg, count-&gt;as_register()); break;
2312       default:
2313         ShouldNotReachHere();
2314         break;
2315       }
2316       break;
2317     default:
2318       ShouldNotReachHere();
2319       break;
2320     }
2321   }
2322 }
2323 
2324 
2325 void LIR_Assembler::shift_op(LIR_Code code, LIR_Opr left, jint count, LIR_Opr dest) {
2326   Register dreg = dest-&gt;is_single_cpu() ? dest-&gt;as_register() : dest-&gt;as_register_lo();
2327   Register lreg = left-&gt;is_single_cpu() ? left-&gt;as_register() : left-&gt;as_register_lo();
2328 
2329   switch (left-&gt;type()) {
2330     case T_INT: {
2331       switch (code) {
2332       case lir_shl:  __ lslw (dreg, lreg, count); break;
2333       case lir_shr:  __ asrw (dreg, lreg, count); break;
2334       case lir_ushr: __ lsrw (dreg, lreg, count); break;
2335       default:
2336         ShouldNotReachHere();
2337         break;
2338       }
2339       break;
2340     case T_LONG:
2341     case T_ADDRESS:
2342     case T_VALUETYPE:
2343     case T_OBJECT:
2344       switch (code) {
2345       case lir_shl:  __ lsl (dreg, lreg, count); break;
2346       case lir_shr:  __ asr (dreg, lreg, count); break;
2347       case lir_ushr: __ lsr (dreg, lreg, count); break;
2348       default:
2349         ShouldNotReachHere();
2350         break;
2351       }
2352       break;
2353     default:
2354       ShouldNotReachHere();
2355       break;
2356     }
2357   }
2358 }
2359 
2360 
2361 void LIR_Assembler::store_parameter(Register r, int offset_from_rsp_in_words) {
2362   assert(offset_from_rsp_in_words &gt;= 0, &quot;invalid offset from rsp&quot;);
2363   int offset_from_rsp_in_bytes = offset_from_rsp_in_words * BytesPerWord;
2364   assert(offset_from_rsp_in_bytes &lt; frame_map()-&gt;reserved_argument_area_size(), &quot;invalid offset&quot;);
2365   __ str (r, Address(sp, offset_from_rsp_in_bytes));
2366 }
2367 
2368 
2369 void LIR_Assembler::store_parameter(jint c,     int offset_from_rsp_in_words) {
2370   assert(offset_from_rsp_in_words &gt;= 0, &quot;invalid offset from rsp&quot;);
2371   int offset_from_rsp_in_bytes = offset_from_rsp_in_words * BytesPerWord;
2372   assert(offset_from_rsp_in_bytes &lt; frame_map()-&gt;reserved_argument_area_size(), &quot;invalid offset&quot;);
2373   __ mov (rscratch1, c);
2374   __ str (rscratch1, Address(sp, offset_from_rsp_in_bytes));
2375 }
2376 
2377 
2378 void LIR_Assembler::store_parameter(jobject o,  int offset_from_rsp_in_words) {
2379   ShouldNotReachHere();
2380   assert(offset_from_rsp_in_words &gt;= 0, &quot;invalid offset from rsp&quot;);
2381   int offset_from_rsp_in_bytes = offset_from_rsp_in_words * BytesPerWord;
2382   assert(offset_from_rsp_in_bytes &lt; frame_map()-&gt;reserved_argument_area_size(), &quot;invalid offset&quot;);
2383   __ lea(rscratch1, __ constant_oop_address(o));
2384   __ str(rscratch1, Address(sp, offset_from_rsp_in_bytes));
2385 }
2386 
2387 void LIR_Assembler::arraycopy_valuetype_check(Register obj, Register tmp, CodeStub* slow_path, bool is_dest) {
2388   __ load_storage_props(tmp, obj);
2389   if (is_dest) {
2390     // We also take slow path if it&#39;s a null_free destination array, just in case the source array
2391     // contains NULLs.
2392     __ tst(tmp, ArrayStorageProperties::flattened_value | ArrayStorageProperties::null_free_value);
2393   } else {
2394     __ tst(tmp, ArrayStorageProperties::flattened_value);
2395   }
2396   __ br(Assembler::NE, *slow_path-&gt;entry());
2397 }
2398 
2399 
2400 
2401 // This code replaces a call to arraycopy; no exception may
2402 // be thrown in this code, they must be thrown in the System.arraycopy
2403 // activation frame; we could save some checks if this would not be the case
2404 void LIR_Assembler::emit_arraycopy(LIR_OpArrayCopy* op) {
2405   ciArrayKlass* default_type = op-&gt;expected_type();
2406   Register src = op-&gt;src()-&gt;as_register();
2407   Register dst = op-&gt;dst()-&gt;as_register();
2408   Register src_pos = op-&gt;src_pos()-&gt;as_register();
2409   Register dst_pos = op-&gt;dst_pos()-&gt;as_register();
2410   Register length  = op-&gt;length()-&gt;as_register();
2411   Register tmp = op-&gt;tmp()-&gt;as_register();
2412 
2413   __ resolve(ACCESS_READ, src);
2414   __ resolve(ACCESS_WRITE, dst);
2415 
2416   CodeStub* stub = op-&gt;stub();
2417   int flags = op-&gt;flags();
2418   BasicType basic_type = default_type != NULL ? default_type-&gt;element_type()-&gt;basic_type() : T_ILLEGAL;
2419   if (is_reference_type(basic_type)) basic_type = T_OBJECT;
2420 
2421   if (flags &amp; LIR_OpArrayCopy::always_slow_path) {
2422     __ b(*stub-&gt;entry());
2423     __ bind(*stub-&gt;continuation());
2424     return;
2425   }
2426 
2427   if (flags &amp; LIR_OpArrayCopy::src_valuetype_check) {
2428     arraycopy_valuetype_check(src, tmp, stub, false);
2429   }
2430 
2431   if (flags &amp; LIR_OpArrayCopy::dst_valuetype_check) {
2432     arraycopy_valuetype_check(dst, tmp, stub, true);
2433   }
2434 
2435 
2436 
2437   // if we don&#39;t know anything, just go through the generic arraycopy
2438   if (default_type == NULL // || basic_type == T_OBJECT
2439       ) {
2440     Label done;
2441     assert(src == r1 &amp;&amp; src_pos == r2, &quot;mismatch in calling convention&quot;);
2442 
2443     // Save the arguments in case the generic arraycopy fails and we
2444     // have to fall back to the JNI stub
2445     __ stp(dst,     dst_pos, Address(sp, 0*BytesPerWord));
2446     __ stp(length,  src_pos, Address(sp, 2*BytesPerWord));
2447     __ str(src,              Address(sp, 4*BytesPerWord));
2448 
2449     address copyfunc_addr = StubRoutines::generic_arraycopy();
2450     assert(copyfunc_addr != NULL, &quot;generic arraycopy stub required&quot;);
2451 
2452     // The arguments are in java calling convention so we shift them
2453     // to C convention
2454     assert_different_registers(c_rarg0, j_rarg1, j_rarg2, j_rarg3, j_rarg4);
2455     __ mov(c_rarg0, j_rarg0);
2456     assert_different_registers(c_rarg1, j_rarg2, j_rarg3, j_rarg4);
2457     __ mov(c_rarg1, j_rarg1);
2458     assert_different_registers(c_rarg2, j_rarg3, j_rarg4);
2459     __ mov(c_rarg2, j_rarg2);
2460     assert_different_registers(c_rarg3, j_rarg4);
2461     __ mov(c_rarg3, j_rarg3);
2462     __ mov(c_rarg4, j_rarg4);
2463 #ifndef PRODUCT
2464     if (PrintC1Statistics) {
2465       __ incrementw(ExternalAddress((address)&amp;Runtime1::_generic_arraycopystub_cnt));
2466     }
2467 #endif
2468     __ far_call(RuntimeAddress(copyfunc_addr));
2469 
2470     __ cbz(r0, *stub-&gt;continuation());
2471 
2472     // Reload values from the stack so they are where the stub
2473     // expects them.
2474     __ ldp(dst,     dst_pos, Address(sp, 0*BytesPerWord));
2475     __ ldp(length,  src_pos, Address(sp, 2*BytesPerWord));
2476     __ ldr(src,              Address(sp, 4*BytesPerWord));
2477 
2478     // r0 is -1^K where K == partial copied count
2479     __ eonw(rscratch1, r0, zr);
2480     // adjust length down and src/end pos up by partial copied count
2481     __ subw(length, length, rscratch1);
2482     __ addw(src_pos, src_pos, rscratch1);
2483     __ addw(dst_pos, dst_pos, rscratch1);
2484     __ b(*stub-&gt;entry());
2485 
2486     __ bind(*stub-&gt;continuation());
2487     return;
2488   }
2489 
2490   assert(default_type != NULL &amp;&amp; default_type-&gt;is_array_klass() &amp;&amp; default_type-&gt;is_loaded(), &quot;must be true at this point&quot;);
2491 
2492   int elem_size = type2aelembytes(basic_type);
2493   int shift_amount;
2494   int scale = exact_log2(elem_size);
2495 
2496   Address src_length_addr = Address(src, arrayOopDesc::length_offset_in_bytes());
2497   Address dst_length_addr = Address(dst, arrayOopDesc::length_offset_in_bytes());
2498   Address src_klass_addr = Address(src, oopDesc::klass_offset_in_bytes());
2499   Address dst_klass_addr = Address(dst, oopDesc::klass_offset_in_bytes());
2500 
2501   // test for NULL
2502   if (flags &amp; LIR_OpArrayCopy::src_null_check) {
2503     __ cbz(src, *stub-&gt;entry());
2504   }
2505   if (flags &amp; LIR_OpArrayCopy::dst_null_check) {
2506     __ cbz(dst, *stub-&gt;entry());
2507   }
2508 
2509   // If the compiler was not able to prove that exact type of the source or the destination
2510   // of the arraycopy is an array type, check at runtime if the source or the destination is
2511   // an instance type.
2512   if (flags &amp; LIR_OpArrayCopy::type_check) {
2513     if (!(flags &amp; LIR_OpArrayCopy::LIR_OpArrayCopy::dst_objarray)) {
2514       __ load_klass(tmp, dst);
2515       __ ldrw(rscratch1, Address(tmp, in_bytes(Klass::layout_helper_offset())));
2516       __ cmpw(rscratch1, Klass::_lh_neutral_value);
2517       __ br(Assembler::GE, *stub-&gt;entry());
2518     }
2519 
2520     if (!(flags &amp; LIR_OpArrayCopy::LIR_OpArrayCopy::src_objarray)) {
2521       __ load_klass(tmp, src);
2522       __ ldrw(rscratch1, Address(tmp, in_bytes(Klass::layout_helper_offset())));
2523       __ cmpw(rscratch1, Klass::_lh_neutral_value);
2524       __ br(Assembler::GE, *stub-&gt;entry());
2525     }
2526   }
2527 
2528   // check if negative
2529   if (flags &amp; LIR_OpArrayCopy::src_pos_positive_check) {
2530     __ cmpw(src_pos, 0);
2531     __ br(Assembler::LT, *stub-&gt;entry());
2532   }
2533   if (flags &amp; LIR_OpArrayCopy::dst_pos_positive_check) {
2534     __ cmpw(dst_pos, 0);
2535     __ br(Assembler::LT, *stub-&gt;entry());
2536   }
2537 
2538   if (flags &amp; LIR_OpArrayCopy::length_positive_check) {
2539     __ cmpw(length, 0);
2540     __ br(Assembler::LT, *stub-&gt;entry());
2541   }
2542 
2543   if (flags &amp; LIR_OpArrayCopy::src_range_check) {
2544     __ addw(tmp, src_pos, length);
2545     __ ldrw(rscratch1, src_length_addr);
2546     __ cmpw(tmp, rscratch1);
2547     __ br(Assembler::HI, *stub-&gt;entry());
2548   }
2549   if (flags &amp; LIR_OpArrayCopy::dst_range_check) {
2550     __ addw(tmp, dst_pos, length);
2551     __ ldrw(rscratch1, dst_length_addr);
2552     __ cmpw(tmp, rscratch1);
2553     __ br(Assembler::HI, *stub-&gt;entry());
2554   }
2555 
2556   if (flags &amp; LIR_OpArrayCopy::type_check) {
2557     // We don&#39;t know the array types are compatible
2558     if (basic_type != T_OBJECT) {
2559       // Simple test for basic type arrays
2560       if (UseCompressedClassPointers) {
2561         __ ldrw(tmp, src_klass_addr);
2562         __ ldrw(rscratch1, dst_klass_addr);
2563         __ cmpw(tmp, rscratch1);
2564       } else {
2565         __ ldr(tmp, src_klass_addr);
2566         __ ldr(rscratch1, dst_klass_addr);
2567         __ cmp(tmp, rscratch1);
2568       }
2569       __ br(Assembler::NE, *stub-&gt;entry());
2570     } else {
2571       // For object arrays, if src is a sub class of dst then we can
2572       // safely do the copy.
2573       Label cont, slow;
2574 
2575 #define PUSH(r1, r2)                                    \
2576       stp(r1, r2, __ pre(sp, -2 * wordSize));
2577 
2578 #define POP(r1, r2)                                     \
2579       ldp(r1, r2, __ post(sp, 2 * wordSize));
2580 
2581       __ PUSH(src, dst);
2582 
2583       __ load_klass(src, src);
2584       __ load_klass(dst, dst);
2585 
2586       __ check_klass_subtype_fast_path(src, dst, tmp, &amp;cont, &amp;slow, NULL);
2587 
2588       __ PUSH(src, dst);
2589       __ far_call(RuntimeAddress(Runtime1::entry_for(Runtime1::slow_subtype_check_id)));
2590       __ POP(src, dst);
2591 
2592       __ cbnz(src, cont);
2593 
2594       __ bind(slow);
2595       __ POP(src, dst);
2596 
2597       address copyfunc_addr = StubRoutines::checkcast_arraycopy();
2598       if (copyfunc_addr != NULL) { // use stub if available
2599         // src is not a sub class of dst so we have to do a
2600         // per-element check.
2601 
2602         int mask = LIR_OpArrayCopy::src_objarray|LIR_OpArrayCopy::dst_objarray;
2603         if ((flags &amp; mask) != mask) {
2604           // Check that at least both of them object arrays.
2605           assert(flags &amp; mask, &quot;one of the two should be known to be an object array&quot;);
2606 
2607           if (!(flags &amp; LIR_OpArrayCopy::src_objarray)) {
2608             __ load_klass(tmp, src);
2609           } else if (!(flags &amp; LIR_OpArrayCopy::dst_objarray)) {
2610             __ load_klass(tmp, dst);
2611           }
2612           int lh_offset = in_bytes(Klass::layout_helper_offset());
2613           Address klass_lh_addr(tmp, lh_offset);
2614           jint objArray_lh = Klass::array_layout_helper(T_OBJECT);
2615           __ ldrw(rscratch1, klass_lh_addr);
2616           __ mov(rscratch2, objArray_lh);
2617           __ eorw(rscratch1, rscratch1, rscratch2);
2618           __ cbnzw(rscratch1, *stub-&gt;entry());
2619         }
2620 
2621        // Spill because stubs can use any register they like and it&#39;s
2622        // easier to restore just those that we care about.
2623         __ stp(dst,     dst_pos, Address(sp, 0*BytesPerWord));
2624         __ stp(length,  src_pos, Address(sp, 2*BytesPerWord));
2625         __ str(src,              Address(sp, 4*BytesPerWord));
2626 
2627         __ lea(c_rarg0, Address(src, src_pos, Address::uxtw(scale)));
2628         __ add(c_rarg0, c_rarg0, arrayOopDesc::base_offset_in_bytes(basic_type));
2629         assert_different_registers(c_rarg0, dst, dst_pos, length);
2630         __ lea(c_rarg1, Address(dst, dst_pos, Address::uxtw(scale)));
2631         __ add(c_rarg1, c_rarg1, arrayOopDesc::base_offset_in_bytes(basic_type));
2632         assert_different_registers(c_rarg1, dst, length);
2633         __ uxtw(c_rarg2, length);
2634         assert_different_registers(c_rarg2, dst);
2635 
2636         __ load_klass(c_rarg4, dst);
2637         __ ldr(c_rarg4, Address(c_rarg4, ObjArrayKlass::element_klass_offset()));
2638         __ ldrw(c_rarg3, Address(c_rarg4, Klass::super_check_offset_offset()));
2639         __ far_call(RuntimeAddress(copyfunc_addr));
2640 
2641 #ifndef PRODUCT
2642         if (PrintC1Statistics) {
2643           Label failed;
2644           __ cbnz(r0, failed);
2645           __ incrementw(ExternalAddress((address)&amp;Runtime1::_arraycopy_checkcast_cnt));
2646           __ bind(failed);
2647         }
2648 #endif
2649 
2650         __ cbz(r0, *stub-&gt;continuation());
2651 
2652 #ifndef PRODUCT
2653         if (PrintC1Statistics) {
2654           __ incrementw(ExternalAddress((address)&amp;Runtime1::_arraycopy_checkcast_attempt_cnt));
2655         }
2656 #endif
2657         assert_different_registers(dst, dst_pos, length, src_pos, src, r0, rscratch1);
2658 
2659         // Restore previously spilled arguments
2660         __ ldp(dst,     dst_pos, Address(sp, 0*BytesPerWord));
2661         __ ldp(length,  src_pos, Address(sp, 2*BytesPerWord));
2662         __ ldr(src,              Address(sp, 4*BytesPerWord));
2663 
2664         // return value is -1^K where K is partial copied count
2665         __ eonw(rscratch1, r0, zr);
2666         // adjust length down and src/end pos up by partial copied count
2667         __ subw(length, length, rscratch1);
2668         __ addw(src_pos, src_pos, rscratch1);
2669         __ addw(dst_pos, dst_pos, rscratch1);
2670       }
2671 
2672       __ b(*stub-&gt;entry());
2673 
2674       __ bind(cont);
2675       __ POP(src, dst);
2676     }
2677   }
2678 
2679 #ifdef ASSERT
2680   if (basic_type != T_OBJECT || !(flags &amp; LIR_OpArrayCopy::type_check)) {
2681     // Sanity check the known type with the incoming class.  For the
2682     // primitive case the types must match exactly with src.klass and
2683     // dst.klass each exactly matching the default type.  For the
2684     // object array case, if no type check is needed then either the
2685     // dst type is exactly the expected type and the src type is a
2686     // subtype which we can&#39;t check or src is the same array as dst
2687     // but not necessarily exactly of type default_type.
2688     Label known_ok, halt;
2689     __ mov_metadata(tmp, default_type-&gt;constant_encoding());
2690     if (UseCompressedClassPointers) {
2691       __ encode_klass_not_null(tmp);
2692     }
2693 
2694     if (basic_type != T_OBJECT) {
2695 
2696       if (UseCompressedClassPointers) {
2697         __ ldrw(rscratch1, dst_klass_addr);
2698         __ cmpw(tmp, rscratch1);
2699       } else {
2700         __ ldr(rscratch1, dst_klass_addr);
2701         __ cmp(tmp, rscratch1);
2702       }
2703       __ br(Assembler::NE, halt);
2704       if (UseCompressedClassPointers) {
2705         __ ldrw(rscratch1, src_klass_addr);
2706         __ cmpw(tmp, rscratch1);
2707       } else {
2708         __ ldr(rscratch1, src_klass_addr);
2709         __ cmp(tmp, rscratch1);
2710       }
2711       __ br(Assembler::EQ, known_ok);
2712     } else {
2713       if (UseCompressedClassPointers) {
2714         __ ldrw(rscratch1, dst_klass_addr);
2715         __ cmpw(tmp, rscratch1);
2716       } else {
2717         __ ldr(rscratch1, dst_klass_addr);
2718         __ cmp(tmp, rscratch1);
2719       }
2720       __ br(Assembler::EQ, known_ok);
2721       __ cmp(src, dst);
2722       __ br(Assembler::EQ, known_ok);
2723     }
2724     __ bind(halt);
2725     __ stop(&quot;incorrect type information in arraycopy&quot;);
2726     __ bind(known_ok);
2727   }
2728 #endif
2729 
2730 #ifndef PRODUCT
2731   if (PrintC1Statistics) {
2732     __ incrementw(ExternalAddress(Runtime1::arraycopy_count_address(basic_type)));
2733   }
2734 #endif
2735 
2736   __ lea(c_rarg0, Address(src, src_pos, Address::uxtw(scale)));
2737   __ add(c_rarg0, c_rarg0, arrayOopDesc::base_offset_in_bytes(basic_type));
2738   assert_different_registers(c_rarg0, dst, dst_pos, length);
2739   __ lea(c_rarg1, Address(dst, dst_pos, Address::uxtw(scale)));
2740   __ add(c_rarg1, c_rarg1, arrayOopDesc::base_offset_in_bytes(basic_type));
2741   assert_different_registers(c_rarg1, dst, length);
2742   __ uxtw(c_rarg2, length);
2743   assert_different_registers(c_rarg2, dst);
2744 
2745   bool disjoint = (flags &amp; LIR_OpArrayCopy::overlapping) == 0;
2746   bool aligned = (flags &amp; LIR_OpArrayCopy::unaligned) == 0;
2747   const char *name;
2748   address entry = StubRoutines::select_arraycopy_function(basic_type, aligned, disjoint, name, false);
2749 
2750  CodeBlob *cb = CodeCache::find_blob(entry);
2751  if (cb) {
2752    __ far_call(RuntimeAddress(entry));
2753  } else {
2754    __ call_VM_leaf(entry, 3);
2755  }
2756 
2757   __ bind(*stub-&gt;continuation());
2758 }
2759 
2760 
2761 
2762 
2763 void LIR_Assembler::emit_lock(LIR_OpLock* op) {
2764   Register obj = op-&gt;obj_opr()-&gt;as_register();  // may not be an oop
2765   Register hdr = op-&gt;hdr_opr()-&gt;as_register();
2766   Register lock = op-&gt;lock_opr()-&gt;as_register();
2767   if (!UseFastLocking) {
2768     __ b(*op-&gt;stub()-&gt;entry());
2769   } else if (op-&gt;code() == lir_lock) {
2770     Register scratch = noreg;
2771     if (UseBiasedLocking) {
2772       scratch = op-&gt;scratch_opr()-&gt;as_register();
2773     }
2774     assert(BasicLock::displaced_header_offset_in_bytes() == 0, &quot;lock_reg must point to the displaced header&quot;);
2775     __ resolve(ACCESS_READ | ACCESS_WRITE, obj);
2776     // add debug info for NullPointerException only if one is possible
2777     int null_check_offset = __ lock_object(hdr, obj, lock, scratch, *op-&gt;stub()-&gt;entry());
2778     if (op-&gt;info() != NULL) {
2779       add_debug_info_for_null_check(null_check_offset, op-&gt;info());
2780     }
2781     // done
2782   } else if (op-&gt;code() == lir_unlock) {
2783     assert(BasicLock::displaced_header_offset_in_bytes() == 0, &quot;lock_reg must point to the displaced header&quot;);
2784     __ unlock_object(hdr, obj, lock, *op-&gt;stub()-&gt;entry());
2785   } else {
2786     Unimplemented();
2787   }
2788   __ bind(*op-&gt;stub()-&gt;continuation());
2789 }
2790 
2791 
2792 void LIR_Assembler::emit_profile_call(LIR_OpProfileCall* op) {
2793   ciMethod* method = op-&gt;profiled_method();
2794   int bci          = op-&gt;profiled_bci();
2795   ciMethod* callee = op-&gt;profiled_callee();
2796 
2797   // Update counter for all call types
2798   ciMethodData* md = method-&gt;method_data_or_null();
2799   assert(md != NULL, &quot;Sanity&quot;);
2800   ciProfileData* data = md-&gt;bci_to_data(bci);
2801   assert(data != NULL &amp;&amp; data-&gt;is_CounterData(), &quot;need CounterData for calls&quot;);
2802   assert(op-&gt;mdo()-&gt;is_single_cpu(),  &quot;mdo must be allocated&quot;);
2803   Register mdo  = op-&gt;mdo()-&gt;as_register();
2804   __ mov_metadata(mdo, md-&gt;constant_encoding());
2805   Address counter_addr(mdo, md-&gt;byte_offset_of_slot(data, CounterData::count_offset()));
2806   // Perform additional virtual call profiling for invokevirtual and
2807   // invokeinterface bytecodes
2808   if (op-&gt;should_profile_receiver_type()) {
2809     assert(op-&gt;recv()-&gt;is_single_cpu(), &quot;recv must be allocated&quot;);
2810     Register recv = op-&gt;recv()-&gt;as_register();
2811     assert_different_registers(mdo, recv);
2812     assert(data-&gt;is_VirtualCallData(), &quot;need VirtualCallData for virtual calls&quot;);
2813     ciKlass* known_klass = op-&gt;known_holder();
2814     if (C1OptimizeVirtualCallProfiling &amp;&amp; known_klass != NULL) {
2815       // We know the type that will be seen at this call site; we can
2816       // statically update the MethodData* rather than needing to do
2817       // dynamic tests on the receiver type
2818 
2819       // NOTE: we should probably put a lock around this search to
2820       // avoid collisions by concurrent compilations
2821       ciVirtualCallData* vc_data = (ciVirtualCallData*) data;
2822       uint i;
2823       for (i = 0; i &lt; VirtualCallData::row_limit(); i++) {
2824         ciKlass* receiver = vc_data-&gt;receiver(i);
2825         if (known_klass-&gt;equals(receiver)) {
2826           Address data_addr(mdo, md-&gt;byte_offset_of_slot(data, VirtualCallData::receiver_count_offset(i)));
2827           __ addptr(data_addr, DataLayout::counter_increment);
2828           return;
2829         }
2830       }
2831 
2832       // Receiver type not found in profile data; select an empty slot
2833 
2834       // Note that this is less efficient than it should be because it
2835       // always does a write to the receiver part of the
2836       // VirtualCallData rather than just the first time
2837       for (i = 0; i &lt; VirtualCallData::row_limit(); i++) {
2838         ciKlass* receiver = vc_data-&gt;receiver(i);
2839         if (receiver == NULL) {
2840           Address recv_addr(mdo, md-&gt;byte_offset_of_slot(data, VirtualCallData::receiver_offset(i)));
2841           __ mov_metadata(rscratch1, known_klass-&gt;constant_encoding());
2842           __ lea(rscratch2, recv_addr);
2843           __ str(rscratch1, Address(rscratch2));
2844           Address data_addr(mdo, md-&gt;byte_offset_of_slot(data, VirtualCallData::receiver_count_offset(i)));
2845           __ addptr(data_addr, DataLayout::counter_increment);
2846           return;
2847         }
2848       }
2849     } else {
2850       __ load_klass(recv, recv);
2851       Label update_done;
2852       type_profile_helper(mdo, md, data, recv, &amp;update_done);
2853       // Receiver did not match any saved receiver and there is no empty row for it.
2854       // Increment total counter to indicate polymorphic case.
2855       __ addptr(counter_addr, DataLayout::counter_increment);
2856 
2857       __ bind(update_done);
2858     }
2859   } else {
2860     // Static call
2861     __ addptr(counter_addr, DataLayout::counter_increment);
2862   }
2863 }
2864 
2865 
2866 void LIR_Assembler::emit_delay(LIR_OpDelay*) {
2867   Unimplemented();
2868 }
2869 
2870 
2871 void LIR_Assembler::monitor_address(int monitor_no, LIR_Opr dst) {
2872   __ lea(dst-&gt;as_register(), frame_map()-&gt;address_for_monitor_lock(monitor_no));
2873 }
2874 
2875 void LIR_Assembler::emit_updatecrc32(LIR_OpUpdateCRC32* op) {
2876   assert(op-&gt;crc()-&gt;is_single_cpu(),  &quot;crc must be register&quot;);
2877   assert(op-&gt;val()-&gt;is_single_cpu(),  &quot;byte value must be register&quot;);
2878   assert(op-&gt;result_opr()-&gt;is_single_cpu(), &quot;result must be register&quot;);
2879   Register crc = op-&gt;crc()-&gt;as_register();
2880   Register val = op-&gt;val()-&gt;as_register();
2881   Register res = op-&gt;result_opr()-&gt;as_register();
2882 
2883   assert_different_registers(val, crc, res);
2884   unsigned long offset;
2885   __ adrp(res, ExternalAddress(StubRoutines::crc_table_addr()), offset);
2886   if (offset) __ add(res, res, offset);
2887 
2888   __ mvnw(crc, crc); // ~crc
2889   __ update_byte_crc32(crc, val, res);
2890   __ mvnw(res, crc); // ~crc
2891 }
2892 
2893 void LIR_Assembler::emit_profile_type(LIR_OpProfileType* op) {
2894   COMMENT(&quot;emit_profile_type {&quot;);
2895   Register obj = op-&gt;obj()-&gt;as_register();
2896   Register tmp = op-&gt;tmp()-&gt;as_pointer_register();
2897   Address mdo_addr = as_Address(op-&gt;mdp()-&gt;as_address_ptr());
2898   ciKlass* exact_klass = op-&gt;exact_klass();
2899   intptr_t current_klass = op-&gt;current_klass();
2900   bool not_null = op-&gt;not_null();
2901   bool no_conflict = op-&gt;no_conflict();
2902 
2903   Label update, next, none;
2904 
2905   bool do_null = !not_null;
2906   bool exact_klass_set = exact_klass != NULL &amp;&amp; ciTypeEntries::valid_ciklass(current_klass) == exact_klass;
2907   bool do_update = !TypeEntries::is_type_unknown(current_klass) &amp;&amp; !exact_klass_set;
2908 
2909   assert(do_null || do_update, &quot;why are we here?&quot;);
2910   assert(!TypeEntries::was_null_seen(current_klass) || do_update, &quot;why are we here?&quot;);
2911   assert(mdo_addr.base() != rscratch1, &quot;wrong register&quot;);
2912 
2913   __ verify_oop(obj);
2914 
2915   if (tmp != obj) {
2916     __ mov(tmp, obj);
2917   }
2918   if (do_null) {
2919     __ cbnz(tmp, update);
2920     if (!TypeEntries::was_null_seen(current_klass)) {
2921       __ ldr(rscratch2, mdo_addr);
2922       __ orr(rscratch2, rscratch2, TypeEntries::null_seen);
2923       __ str(rscratch2, mdo_addr);
2924     }
2925     if (do_update) {
2926 #ifndef ASSERT
2927       __ b(next);
2928     }
2929 #else
2930       __ b(next);
2931     }
2932   } else {
2933     __ cbnz(tmp, update);
2934     __ stop(&quot;unexpected null obj&quot;);
2935 #endif
2936   }
2937 
2938   __ bind(update);
2939 
2940   if (do_update) {
2941 #ifdef ASSERT
2942     if (exact_klass != NULL) {
2943       Label ok;
2944       __ load_klass(tmp, tmp);
2945       __ mov_metadata(rscratch1, exact_klass-&gt;constant_encoding());
2946       __ eor(rscratch1, tmp, rscratch1);
2947       __ cbz(rscratch1, ok);
2948       __ stop(&quot;exact klass and actual klass differ&quot;);
2949       __ bind(ok);
2950     }
2951 #endif
2952     if (!no_conflict) {
2953       if (exact_klass == NULL || TypeEntries::is_type_none(current_klass)) {
2954         if (exact_klass != NULL) {
2955           __ mov_metadata(tmp, exact_klass-&gt;constant_encoding());
2956         } else {
2957           __ load_klass(tmp, tmp);
2958         }
2959 
2960         __ ldr(rscratch2, mdo_addr);
2961         __ eor(tmp, tmp, rscratch2);
2962         __ andr(rscratch1, tmp, TypeEntries::type_klass_mask);
2963         // klass seen before, nothing to do. The unknown bit may have been
2964         // set already but no need to check.
2965         __ cbz(rscratch1, next);
2966 
2967         __ tbnz(tmp, exact_log2(TypeEntries::type_unknown), next); // already unknown. Nothing to do anymore.
2968 
2969         if (TypeEntries::is_type_none(current_klass)) {
2970           __ cbz(rscratch2, none);
2971           __ cmp(rscratch2, (u1)TypeEntries::null_seen);
2972           __ br(Assembler::EQ, none);
2973           // There is a chance that the checks above (re-reading profiling
2974           // data from memory) fail if another thread has just set the
2975           // profiling to this obj&#39;s klass
2976           __ dmb(Assembler::ISHLD);
2977           __ ldr(rscratch2, mdo_addr);
2978           __ eor(tmp, tmp, rscratch2);
2979           __ andr(rscratch1, tmp, TypeEntries::type_klass_mask);
2980           __ cbz(rscratch1, next);
2981         }
2982       } else {
2983         assert(ciTypeEntries::valid_ciklass(current_klass) != NULL &amp;&amp;
2984                ciTypeEntries::valid_ciklass(current_klass) != exact_klass, &quot;conflict only&quot;);
2985 
2986         __ ldr(tmp, mdo_addr);
2987         __ tbnz(tmp, exact_log2(TypeEntries::type_unknown), next); // already unknown. Nothing to do anymore.
2988       }
2989 
2990       // different than before. Cannot keep accurate profile.
2991       __ ldr(rscratch2, mdo_addr);
2992       __ orr(rscratch2, rscratch2, TypeEntries::type_unknown);
2993       __ str(rscratch2, mdo_addr);
2994 
2995       if (TypeEntries::is_type_none(current_klass)) {
2996         __ b(next);
2997 
2998         __ bind(none);
2999         // first time here. Set profile type.
3000         __ str(tmp, mdo_addr);
3001       }
3002     } else {
3003       // There&#39;s a single possible klass at this profile point
3004       assert(exact_klass != NULL, &quot;should be&quot;);
3005       if (TypeEntries::is_type_none(current_klass)) {
3006         __ mov_metadata(tmp, exact_klass-&gt;constant_encoding());
3007         __ ldr(rscratch2, mdo_addr);
3008         __ eor(tmp, tmp, rscratch2);
3009         __ andr(rscratch1, tmp, TypeEntries::type_klass_mask);
3010         __ cbz(rscratch1, next);
3011 #ifdef ASSERT
3012         {
3013           Label ok;
3014           __ ldr(rscratch1, mdo_addr);
3015           __ cbz(rscratch1, ok);
3016           __ cmp(rscratch1, (u1)TypeEntries::null_seen);
3017           __ br(Assembler::EQ, ok);
3018           // may have been set by another thread
3019           __ dmb(Assembler::ISHLD);
3020           __ mov_metadata(rscratch1, exact_klass-&gt;constant_encoding());
3021           __ ldr(rscratch2, mdo_addr);
3022           __ eor(rscratch2, rscratch1, rscratch2);
3023           __ andr(rscratch2, rscratch2, TypeEntries::type_mask);
3024           __ cbz(rscratch2, ok);
3025 
3026           __ stop(&quot;unexpected profiling mismatch&quot;);
3027           __ bind(ok);
3028         }
3029 #endif
3030         // first time here. Set profile type.
3031         __ ldr(tmp, mdo_addr);
3032       } else {
3033         assert(ciTypeEntries::valid_ciklass(current_klass) != NULL &amp;&amp;
3034                ciTypeEntries::valid_ciklass(current_klass) != exact_klass, &quot;inconsistent&quot;);
3035 
3036         __ ldr(tmp, mdo_addr);
3037         __ tbnz(tmp, exact_log2(TypeEntries::type_unknown), next); // already unknown. Nothing to do anymore.
3038 
3039         __ orr(tmp, tmp, TypeEntries::type_unknown);
3040         __ str(tmp, mdo_addr);
3041         // FIXME: Write barrier needed here?
3042       }
3043     }
3044 
3045     __ bind(next);
3046   }
3047   COMMENT(&quot;} emit_profile_type&quot;);
3048 }
3049 
3050 
3051 void LIR_Assembler::align_backward_branch_target() {
3052 }
3053 
3054 
3055 void LIR_Assembler::negate(LIR_Opr left, LIR_Opr dest, LIR_Opr tmp) {
3056   // tmp must be unused
3057   assert(tmp-&gt;is_illegal(), &quot;wasting a register if tmp is allocated&quot;);
3058 
3059   if (left-&gt;is_single_cpu()) {
3060     assert(dest-&gt;is_single_cpu(), &quot;expect single result reg&quot;);
3061     __ negw(dest-&gt;as_register(), left-&gt;as_register());
3062   } else if (left-&gt;is_double_cpu()) {
3063     assert(dest-&gt;is_double_cpu(), &quot;expect double result reg&quot;);
3064     __ neg(dest-&gt;as_register_lo(), left-&gt;as_register_lo());
3065   } else if (left-&gt;is_single_fpu()) {
3066     assert(dest-&gt;is_single_fpu(), &quot;expect single float result reg&quot;);
3067     __ fnegs(dest-&gt;as_float_reg(), left-&gt;as_float_reg());
3068   } else {
3069     assert(left-&gt;is_double_fpu(), &quot;expect double float operand reg&quot;);
3070     assert(dest-&gt;is_double_fpu(), &quot;expect double float result reg&quot;);
3071     __ fnegd(dest-&gt;as_double_reg(), left-&gt;as_double_reg());
3072   }
3073 }
3074 
3075 
3076 void LIR_Assembler::leal(LIR_Opr addr, LIR_Opr dest, LIR_PatchCode patch_code, CodeEmitInfo* info) {
3077   if (patch_code != lir_patch_none) {
3078     deoptimize_trap(info);
3079     return;
3080   }
3081 
3082   __ lea(dest-&gt;as_register_lo(), as_Address(addr-&gt;as_address_ptr()));
3083 }
3084 
3085 
3086 void LIR_Assembler::rt_call(LIR_Opr result, address dest, const LIR_OprList* args, LIR_Opr tmp, CodeEmitInfo* info) {
3087   assert(!tmp-&gt;is_valid(), &quot;don&#39;t need temporary&quot;);
3088 
3089   CodeBlob *cb = CodeCache::find_blob(dest);
3090   if (cb) {
3091     __ far_call(RuntimeAddress(dest));
3092   } else {
3093     __ mov(rscratch1, RuntimeAddress(dest));
3094     __ blr(rscratch1);
3095   }
3096 
3097   if (info != NULL) {
3098     add_call_info_here(info);
3099   }
3100   __ maybe_isb();
3101 }
3102 
3103 void LIR_Assembler::volatile_move_op(LIR_Opr src, LIR_Opr dest, BasicType type, CodeEmitInfo* info) {
3104   if (dest-&gt;is_address() || src-&gt;is_address()) {
3105     move_op(src, dest, type, lir_patch_none, info,
3106             /*pop_fpu_stack*/false, /*unaligned*/false, /*wide*/false);
3107   } else {
3108     ShouldNotReachHere();
3109   }
3110 }
3111 
3112 #ifdef ASSERT
3113 // emit run-time assertion
3114 void LIR_Assembler::emit_assert(LIR_OpAssert* op) {
3115   assert(op-&gt;code() == lir_assert, &quot;must be&quot;);
3116 
3117   if (op-&gt;in_opr1()-&gt;is_valid()) {
3118     assert(op-&gt;in_opr2()-&gt;is_valid(), &quot;both operands must be valid&quot;);
3119     comp_op(op-&gt;condition(), op-&gt;in_opr1(), op-&gt;in_opr2(), op);
3120   } else {
3121     assert(op-&gt;in_opr2()-&gt;is_illegal(), &quot;both operands must be illegal&quot;);
3122     assert(op-&gt;condition() == lir_cond_always, &quot;no other conditions allowed&quot;);
3123   }
3124 
3125   Label ok;
3126   if (op-&gt;condition() != lir_cond_always) {
3127     Assembler::Condition acond = Assembler::AL;
3128     switch (op-&gt;condition()) {
3129       case lir_cond_equal:        acond = Assembler::EQ;  break;
3130       case lir_cond_notEqual:     acond = Assembler::NE;  break;
3131       case lir_cond_less:         acond = Assembler::LT;  break;
3132       case lir_cond_lessEqual:    acond = Assembler::LE;  break;
3133       case lir_cond_greaterEqual: acond = Assembler::GE;  break;
3134       case lir_cond_greater:      acond = Assembler::GT;  break;
3135       case lir_cond_belowEqual:   acond = Assembler::LS;  break;
3136       case lir_cond_aboveEqual:   acond = Assembler::HS;  break;
3137       default:                    ShouldNotReachHere();
3138     }
3139     __ br(acond, ok);
3140   }
3141   if (op-&gt;halt()) {
3142     const char* str = __ code_string(op-&gt;msg());
3143     __ stop(str);
3144   } else {
3145     breakpoint();
3146   }
3147   __ bind(ok);
3148 }
3149 #endif
3150 
3151 #ifndef PRODUCT
3152 #define COMMENT(x)   do { __ block_comment(x); } while (0)
3153 #else
3154 #define COMMENT(x)
3155 #endif
3156 
3157 void LIR_Assembler::membar() {
3158   COMMENT(&quot;membar&quot;);
3159   __ membar(MacroAssembler::AnyAny);
3160 }
3161 
3162 void LIR_Assembler::membar_acquire() {
3163   __ membar(Assembler::LoadLoad|Assembler::LoadStore);
3164 }
3165 
3166 void LIR_Assembler::membar_release() {
3167   __ membar(Assembler::LoadStore|Assembler::StoreStore);
3168 }
3169 
3170 void LIR_Assembler::membar_loadload() {
3171   __ membar(Assembler::LoadLoad);
3172 }
3173 
3174 void LIR_Assembler::membar_storestore() {
3175   __ membar(MacroAssembler::StoreStore);
3176 }
3177 
3178 void LIR_Assembler::membar_loadstore() { __ membar(MacroAssembler::LoadStore); }
3179 
3180 void LIR_Assembler::membar_storeload() { __ membar(MacroAssembler::StoreLoad); }
3181 
3182 void LIR_Assembler::on_spin_wait() {
3183   Unimplemented();
3184 }
3185 
3186 void LIR_Assembler::get_thread(LIR_Opr result_reg) {
3187   __ mov(result_reg-&gt;as_register(), rthread);
3188 }
3189 
3190 
3191 void LIR_Assembler::peephole(LIR_List *lir) {
3192 #if 0
3193   if (tableswitch_count &gt;= max_tableswitches)
3194     return;
3195 
3196   /*
3197     This finite-state automaton recognizes sequences of compare-and-
3198     branch instructions.  We will turn them into a tableswitch.  You
3199     could argue that C1 really shouldn&#39;t be doing this sort of
3200     optimization, but without it the code is really horrible.
3201   */
3202 
3203   enum { start_s, cmp1_s, beq_s, cmp_s } state;
3204   int first_key, last_key = -2147483648;
3205   int next_key = 0;
3206   int start_insn = -1;
3207   int last_insn = -1;
3208   Register reg = noreg;
3209   LIR_Opr reg_opr;
3210   state = start_s;
3211 
3212   LIR_OpList* inst = lir-&gt;instructions_list();
3213   for (int i = 0; i &lt; inst-&gt;length(); i++) {
3214     LIR_Op* op = inst-&gt;at(i);
3215     switch (state) {
3216     case start_s:
3217       first_key = -1;
3218       start_insn = i;
3219       switch (op-&gt;code()) {
3220       case lir_cmp:
3221         LIR_Opr opr1 = op-&gt;as_Op2()-&gt;in_opr1();
3222         LIR_Opr opr2 = op-&gt;as_Op2()-&gt;in_opr2();
3223         if (opr1-&gt;is_cpu_register() &amp;&amp; opr1-&gt;is_single_cpu()
3224             &amp;&amp; opr2-&gt;is_constant()
3225             &amp;&amp; opr2-&gt;type() == T_INT) {
3226           reg_opr = opr1;
3227           reg = opr1-&gt;as_register();
3228           first_key = opr2-&gt;as_constant_ptr()-&gt;as_jint();
3229           next_key = first_key + 1;
3230           state = cmp_s;
3231           goto next_state;
3232         }
3233         break;
3234       }
3235       break;
3236     case cmp_s:
3237       switch (op-&gt;code()) {
3238       case lir_branch:
3239         if (op-&gt;as_OpBranch()-&gt;cond() == lir_cond_equal) {
3240           state = beq_s;
3241           last_insn = i;
3242           goto next_state;
3243         }
3244       }
3245       state = start_s;
3246       break;
3247     case beq_s:
3248       switch (op-&gt;code()) {
3249       case lir_cmp: {
3250         LIR_Opr opr1 = op-&gt;as_Op2()-&gt;in_opr1();
3251         LIR_Opr opr2 = op-&gt;as_Op2()-&gt;in_opr2();
3252         if (opr1-&gt;is_cpu_register() &amp;&amp; opr1-&gt;is_single_cpu()
3253             &amp;&amp; opr1-&gt;as_register() == reg
3254             &amp;&amp; opr2-&gt;is_constant()
3255             &amp;&amp; opr2-&gt;type() == T_INT
3256             &amp;&amp; opr2-&gt;as_constant_ptr()-&gt;as_jint() == next_key) {
3257           last_key = next_key;
3258           next_key++;
3259           state = cmp_s;
3260           goto next_state;
3261         }
3262       }
3263       }
3264       last_key = next_key;
3265       state = start_s;
3266       break;
3267     default:
3268       assert(false, &quot;impossible state&quot;);
3269     }
3270     if (state == start_s) {
3271       if (first_key &lt; last_key - 5L &amp;&amp; reg != noreg) {
3272         {
3273           // printf(&quot;found run register %d starting at insn %d low value %d high value %d\n&quot;,
3274           //        reg-&gt;encoding(),
3275           //        start_insn, first_key, last_key);
3276           //   for (int i = 0; i &lt; inst-&gt;length(); i++) {
3277           //     inst-&gt;at(i)-&gt;print();
3278           //     tty-&gt;print(&quot;\n&quot;);
3279           //   }
3280           //   tty-&gt;print(&quot;\n&quot;);
3281         }
3282 
3283         struct tableswitch *sw = &amp;switches[tableswitch_count];
3284         sw-&gt;_insn_index = start_insn, sw-&gt;_first_key = first_key,
3285           sw-&gt;_last_key = last_key, sw-&gt;_reg = reg;
3286         inst-&gt;insert_before(last_insn + 1, new LIR_OpLabel(&amp;sw-&gt;_after));
3287         {
3288           // Insert the new table of branches
3289           int offset = last_insn;
3290           for (int n = first_key; n &lt; last_key; n++) {
3291             inst-&gt;insert_before
3292               (last_insn + 1,
3293                new LIR_OpBranch(lir_cond_always, T_ILLEGAL,
3294                                 inst-&gt;at(offset)-&gt;as_OpBranch()-&gt;label()));
3295             offset -= 2, i++;
3296           }
3297         }
3298         // Delete all the old compare-and-branch instructions
3299         for (int n = first_key; n &lt; last_key; n++) {
3300           inst-&gt;remove_at(start_insn);
3301           inst-&gt;remove_at(start_insn);
3302         }
3303         // Insert the tableswitch instruction
3304         inst-&gt;insert_before(start_insn,
3305                             new LIR_Op2(lir_cmp, lir_cond_always,
3306                                         LIR_OprFact::intConst(tableswitch_count),
3307                                         reg_opr));
3308         inst-&gt;insert_before(start_insn + 1, new LIR_OpLabel(&amp;sw-&gt;_branches));
3309         tableswitch_count++;
3310       }
3311       reg = noreg;
3312       last_key = -2147483648;
3313     }
3314   next_state:
3315     ;
3316   }
3317 #endif
3318 }
3319 
3320 void LIR_Assembler::atomic_op(LIR_Code code, LIR_Opr src, LIR_Opr data, LIR_Opr dest, LIR_Opr tmp_op) {
3321   Address addr = as_Address(src-&gt;as_address_ptr());
3322   BasicType type = src-&gt;type();
3323   bool is_oop = is_reference_type(type);
3324 
3325   void (MacroAssembler::* add)(Register prev, RegisterOrConstant incr, Register addr);
3326   void (MacroAssembler::* xchg)(Register prev, Register newv, Register addr);
3327 
3328   switch(type) {
3329   case T_INT:
3330     xchg = &amp;MacroAssembler::atomic_xchgalw;
3331     add = &amp;MacroAssembler::atomic_addalw;
3332     break;
3333   case T_LONG:
3334     xchg = &amp;MacroAssembler::atomic_xchgal;
3335     add = &amp;MacroAssembler::atomic_addal;
3336     break;
3337   case T_VALUETYPE:
3338   case T_OBJECT:
3339   case T_ARRAY:
3340     if (UseCompressedOops) {
3341       xchg = &amp;MacroAssembler::atomic_xchgalw;
3342       add = &amp;MacroAssembler::atomic_addalw;
3343     } else {
3344       xchg = &amp;MacroAssembler::atomic_xchgal;
3345       add = &amp;MacroAssembler::atomic_addal;
3346     }
3347     break;
3348   default:
3349     ShouldNotReachHere();
3350     xchg = &amp;MacroAssembler::atomic_xchgal;
3351     add = &amp;MacroAssembler::atomic_addal; // unreachable
3352   }
3353 
3354   switch (code) {
3355   case lir_xadd:
3356     {
3357       RegisterOrConstant inc;
3358       Register tmp = as_reg(tmp_op);
3359       Register dst = as_reg(dest);
3360       if (data-&gt;is_constant()) {
3361         inc = RegisterOrConstant(as_long(data));
3362         assert_different_registers(dst, addr.base(), tmp,
3363                                    rscratch1, rscratch2);
3364       } else {
3365         inc = RegisterOrConstant(as_reg(data));
3366         assert_different_registers(inc.as_register(), dst, addr.base(), tmp,
3367                                    rscratch1, rscratch2);
3368       }
3369       __ lea(tmp, addr);
3370       (_masm-&gt;*add)(dst, inc, tmp);
3371       break;
3372     }
3373   case lir_xchg:
3374     {
3375       Register tmp = tmp_op-&gt;as_register();
3376       Register obj = as_reg(data);
3377       Register dst = as_reg(dest);
3378       if (is_oop &amp;&amp; UseCompressedOops) {
3379         __ encode_heap_oop(rscratch2, obj);
3380         obj = rscratch2;
3381       }
3382       assert_different_registers(obj, addr.base(), tmp, rscratch1, dst);
3383       __ lea(tmp, addr);
3384       (_masm-&gt;*xchg)(dst, obj, tmp);
3385       if (is_oop &amp;&amp; UseCompressedOops) {
3386         __ decode_heap_oop(dst);
3387       }
3388     }
3389     break;
3390   default:
3391     ShouldNotReachHere();
3392   }
3393   __ membar(__ AnyAny);
3394 }
3395 
3396 #undef __
<a name="8" id="anc8"></a><b style="font-size: large; color: red">--- EOF ---</b>
















































































</pre>
<input id="eof" value="8" type="hidden" />
</body>
</html>