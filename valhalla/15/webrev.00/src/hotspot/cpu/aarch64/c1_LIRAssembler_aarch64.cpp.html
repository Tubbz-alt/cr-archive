<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>New src/hotspot/cpu/aarch64/c1_LIRAssembler_aarch64.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
  <body>
    <pre>
   1 /*
   2  * Copyright (c) 2000, 2020, Oracle and/or its affiliates. All rights reserved.
   3  * Copyright (c) 2014, Red Hat Inc. All rights reserved.
   4  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   5  *
   6  * This code is free software; you can redistribute it and/or modify it
   7  * under the terms of the GNU General Public License version 2 only, as
   8  * published by the Free Software Foundation.
   9  *
  10  * This code is distributed in the hope that it will be useful, but WITHOUT
  11  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  12  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  13  * version 2 for more details (a copy is included in the LICENSE file that
  14  * accompanied this code).
  15  *
  16  * You should have received a copy of the GNU General Public License version
  17  * 2 along with this work; if not, write to the Free Software Foundation,
  18  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  19  *
  20  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  21  * or visit www.oracle.com if you need additional information or have any
  22  * questions.
  23  *
  24  */
  25 
  26 #include &quot;precompiled.hpp&quot;
  27 #include &quot;asm/macroAssembler.inline.hpp&quot;
  28 #include &quot;asm/assembler.hpp&quot;
  29 #include &quot;c1/c1_CodeStubs.hpp&quot;
  30 #include &quot;c1/c1_Compilation.hpp&quot;
  31 #include &quot;c1/c1_LIRAssembler.hpp&quot;
  32 #include &quot;c1/c1_MacroAssembler.hpp&quot;
  33 #include &quot;c1/c1_Runtime1.hpp&quot;
  34 #include &quot;c1/c1_ValueStack.hpp&quot;
  35 #include &quot;ci/ciArrayKlass.hpp&quot;
  36 #include &quot;ci/ciInstance.hpp&quot;
  37 #include &quot;ci/ciValueKlass.hpp&quot;
  38 #include &quot;code/compiledIC.hpp&quot;
  39 #include &quot;gc/shared/collectedHeap.hpp&quot;
  40 #include &quot;nativeInst_aarch64.hpp&quot;
  41 #include &quot;oops/objArrayKlass.hpp&quot;
  42 #include &quot;oops/oop.inline.hpp&quot;
  43 #include &quot;runtime/frame.inline.hpp&quot;
  44 #include &quot;runtime/sharedRuntime.hpp&quot;
  45 #include &quot;utilities/powerOfTwo.hpp&quot;
  46 #include &quot;vmreg_aarch64.inline.hpp&quot;
  47 
  48 
  49 #ifndef PRODUCT
  50 #define COMMENT(x)   do { __ block_comment(x); } while (0)
  51 #else
  52 #define COMMENT(x)
  53 #endif
  54 
  55 NEEDS_CLEANUP // remove this definitions ?
  56 const Register IC_Klass    = rscratch2;   // where the IC klass is cached
  57 const Register SYNC_header = r0;   // synchronization header
  58 const Register SHIFT_count = r0;   // where count for shift operations must be
  59 
  60 #define __ _masm-&gt;
  61 
  62 
  63 static void select_different_registers(Register preserve,
  64                                        Register extra,
  65                                        Register &amp;tmp1,
  66                                        Register &amp;tmp2) {
  67   if (tmp1 == preserve) {
  68     assert_different_registers(tmp1, tmp2, extra);
  69     tmp1 = extra;
  70   } else if (tmp2 == preserve) {
  71     assert_different_registers(tmp1, tmp2, extra);
  72     tmp2 = extra;
  73   }
  74   assert_different_registers(preserve, tmp1, tmp2);
  75 }
  76 
  77 
  78 
  79 static void select_different_registers(Register preserve,
  80                                        Register extra,
  81                                        Register &amp;tmp1,
  82                                        Register &amp;tmp2,
  83                                        Register &amp;tmp3) {
  84   if (tmp1 == preserve) {
  85     assert_different_registers(tmp1, tmp2, tmp3, extra);
  86     tmp1 = extra;
  87   } else if (tmp2 == preserve) {
  88     assert_different_registers(tmp1, tmp2, tmp3, extra);
  89     tmp2 = extra;
  90   } else if (tmp3 == preserve) {
  91     assert_different_registers(tmp1, tmp2, tmp3, extra);
  92     tmp3 = extra;
  93   }
  94   assert_different_registers(preserve, tmp1, tmp2, tmp3);
  95 }
  96 
  97 
  98 bool LIR_Assembler::is_small_constant(LIR_Opr opr) { Unimplemented(); return false; }
  99 
 100 
 101 LIR_Opr LIR_Assembler::receiverOpr() {
 102   return FrameMap::receiver_opr;
 103 }
 104 
 105 LIR_Opr LIR_Assembler::osrBufferPointer() {
 106   return FrameMap::as_pointer_opr(receiverOpr()-&gt;as_register());
 107 }
 108 
 109 //--------------fpu register translations-----------------------
 110 
 111 
 112 address LIR_Assembler::float_constant(float f) {
 113   address const_addr = __ float_constant(f);
 114   if (const_addr == NULL) {
 115     bailout(&quot;const section overflow&quot;);
 116     return __ code()-&gt;consts()-&gt;start();
 117   } else {
 118     return const_addr;
 119   }
 120 }
 121 
 122 
 123 address LIR_Assembler::double_constant(double d) {
 124   address const_addr = __ double_constant(d);
 125   if (const_addr == NULL) {
 126     bailout(&quot;const section overflow&quot;);
 127     return __ code()-&gt;consts()-&gt;start();
 128   } else {
 129     return const_addr;
 130   }
 131 }
 132 
 133 address LIR_Assembler::int_constant(jlong n) {
 134   address const_addr = __ long_constant(n);
 135   if (const_addr == NULL) {
 136     bailout(&quot;const section overflow&quot;);
 137     return __ code()-&gt;consts()-&gt;start();
 138   } else {
 139     return const_addr;
 140   }
 141 }
 142 
 143 void LIR_Assembler::breakpoint() { Unimplemented(); }
 144 
 145 void LIR_Assembler::push(LIR_Opr opr) { Unimplemented(); }
 146 
 147 void LIR_Assembler::pop(LIR_Opr opr) { Unimplemented(); }
 148 
 149 bool LIR_Assembler::is_literal_address(LIR_Address* addr) { Unimplemented(); return false; }
 150 //-------------------------------------------
 151 
 152 static Register as_reg(LIR_Opr op) {
 153   return op-&gt;is_double_cpu() ? op-&gt;as_register_lo() : op-&gt;as_register();
 154 }
 155 
 156 static jlong as_long(LIR_Opr data) {
 157   jlong result;
 158   switch (data-&gt;type()) {
 159   case T_INT:
 160     result = (data-&gt;as_jint());
 161     break;
 162   case T_LONG:
 163     result = (data-&gt;as_jlong());
 164     break;
 165   default:
 166     ShouldNotReachHere();
 167     result = 0;  // unreachable
 168   }
 169   return result;
 170 }
 171 
 172 Address LIR_Assembler::as_Address(LIR_Address* addr, Register tmp) {
 173   Register base = addr-&gt;base()-&gt;as_pointer_register();
 174   LIR_Opr opr = addr-&gt;index();
 175   if (opr-&gt;is_cpu_register()) {
 176     Register index;
 177     if (opr-&gt;is_single_cpu())
 178       index = opr-&gt;as_register();
 179     else
 180       index = opr-&gt;as_register_lo();
 181     assert(addr-&gt;disp() == 0, &quot;must be&quot;);
 182     switch(opr-&gt;type()) {
 183       case T_INT:
 184         return Address(base, index, Address::sxtw(addr-&gt;scale()));
 185       case T_LONG:
 186         return Address(base, index, Address::lsl(addr-&gt;scale()));
 187       default:
 188         ShouldNotReachHere();
 189       }
 190   } else  {
 191     intptr_t addr_offset = intptr_t(addr-&gt;disp());
 192     if (Address::offset_ok_for_immed(addr_offset, addr-&gt;scale()))
 193       return Address(base, addr_offset, Address::lsl(addr-&gt;scale()));
 194     else {
 195       __ mov(tmp, addr_offset);
 196       return Address(base, tmp, Address::lsl(addr-&gt;scale()));
 197     }
 198   }
 199   return Address();
 200 }
 201 
 202 Address LIR_Assembler::as_Address_hi(LIR_Address* addr) {
 203   ShouldNotReachHere();
 204   return Address();
 205 }
 206 
 207 Address LIR_Assembler::as_Address(LIR_Address* addr) {
 208   return as_Address(addr, rscratch1);
 209 }
 210 
 211 Address LIR_Assembler::as_Address_lo(LIR_Address* addr) {
 212   return as_Address(addr, rscratch1);  // Ouch
 213   // FIXME: This needs to be much more clever.  See x86.
 214 }
 215 
 216 
 217 void LIR_Assembler::osr_entry() {
 218   offsets()-&gt;set_value(CodeOffsets::OSR_Entry, code_offset());
 219   BlockBegin* osr_entry = compilation()-&gt;hir()-&gt;osr_entry();
 220   ValueStack* entry_state = osr_entry-&gt;state();
 221   int number_of_locks = entry_state-&gt;locks_size();
 222 
 223   // we jump here if osr happens with the interpreter
 224   // state set up to continue at the beginning of the
 225   // loop that triggered osr - in particular, we have
 226   // the following registers setup:
 227   //
 228   // r2: osr buffer
 229   //
 230 
 231   // build frame
 232   ciMethod* m = compilation()-&gt;method();
 233   __ build_frame(initial_frame_size_in_bytes(), bang_size_in_bytes(), needs_stack_repair(), NULL);
 234 
 235   // OSR buffer is
 236   //
 237   // locals[nlocals-1..0]
 238   // monitors[0..number_of_locks]
 239   //
 240   // locals is a direct copy of the interpreter frame so in the osr buffer
 241   // so first slot in the local array is the last local from the interpreter
 242   // and last slot is local[0] (receiver) from the interpreter
 243   //
 244   // Similarly with locks. The first lock slot in the osr buffer is the nth lock
 245   // from the interpreter frame, the nth lock slot in the osr buffer is 0th lock
 246   // in the interpreter frame (the method lock if a sync method)
 247 
 248   // Initialize monitors in the compiled activation.
 249   //   r2: pointer to osr buffer
 250   //
 251   // All other registers are dead at this point and the locals will be
 252   // copied into place by code emitted in the IR.
 253 
 254   Register OSR_buf = osrBufferPointer()-&gt;as_pointer_register();
 255   { assert(frame::interpreter_frame_monitor_size() == BasicObjectLock::size(), &quot;adjust code below&quot;);
 256     int monitor_offset = BytesPerWord * method()-&gt;max_locals() +
 257       (2 * BytesPerWord) * (number_of_locks - 1);
 258     // SharedRuntime::OSR_migration_begin() packs BasicObjectLocks in
 259     // the OSR buffer using 2 word entries: first the lock and then
 260     // the oop.
 261     for (int i = 0; i &lt; number_of_locks; i++) {
 262       int slot_offset = monitor_offset - ((i * 2) * BytesPerWord);
 263 #ifdef ASSERT
 264       // verify the interpreter&#39;s monitor has a non-null object
 265       {
 266         Label L;
 267         __ ldr(rscratch1, Address(OSR_buf, slot_offset + 1*BytesPerWord));
 268         __ cbnz(rscratch1, L);
 269         __ stop(&quot;locked object is NULL&quot;);
 270         __ bind(L);
 271       }
 272 #endif
 273       __ ldr(r19, Address(OSR_buf, slot_offset + 0));
 274       __ str(r19, frame_map()-&gt;address_for_monitor_lock(i));
 275       __ ldr(r19, Address(OSR_buf, slot_offset + 1*BytesPerWord));
 276       __ str(r19, frame_map()-&gt;address_for_monitor_object(i));
 277     }
 278   }
 279 }
 280 
 281 
 282 // inline cache check; done before the frame is built.
 283 int LIR_Assembler::check_icache() {
 284   Register receiver = FrameMap::receiver_opr-&gt;as_register();
 285   Register ic_klass = IC_Klass;
 286   int start_offset = __ offset();
 287   __ inline_cache_check(receiver, ic_klass);
 288 
 289   // if icache check fails, then jump to runtime routine
 290   // Note: RECEIVER must still contain the receiver!
 291   Label dont;
 292   __ br(Assembler::EQ, dont);
 293   __ far_jump(RuntimeAddress(SharedRuntime::get_ic_miss_stub()));
 294 
 295   // We align the verified entry point unless the method body
 296   // (including its inline cache check) will fit in a single 64-byte
 297   // icache line.
 298   if (! method()-&gt;is_accessor() || __ offset() - start_offset &gt; 4 * 4) {
 299     // force alignment after the cache check.
 300     __ align(CodeEntryAlignment);
 301   }
 302 
 303   __ bind(dont);
 304   return start_offset;
 305 }
 306 
 307 void LIR_Assembler::clinit_barrier(ciMethod* method) {
 308   assert(VM_Version::supports_fast_class_init_checks(), &quot;sanity&quot;);
 309   assert(!method-&gt;holder()-&gt;is_not_initialized(), &quot;initialization should have been started&quot;);
 310 
 311   Label L_skip_barrier;
 312 
 313   __ mov_metadata(rscratch2, method-&gt;holder()-&gt;constant_encoding());
 314   __ clinit_barrier(rscratch2, rscratch1, &amp;L_skip_barrier /*L_fast_path*/);
 315   __ far_jump(RuntimeAddress(SharedRuntime::get_handle_wrong_method_stub()));
 316   __ bind(L_skip_barrier);
 317 }
 318 
 319 void LIR_Assembler::jobject2reg(jobject o, Register reg) {
 320   if (o == NULL) {
 321     __ mov(reg, zr);
 322   } else {
 323     __ movoop(reg, o, /*immediate*/true);
 324   }
 325 }
 326 
 327 void LIR_Assembler::deoptimize_trap(CodeEmitInfo *info) {
 328   address target = NULL;
 329   relocInfo::relocType reloc_type = relocInfo::none;
 330 
 331   switch (patching_id(info)) {
 332   case PatchingStub::access_field_id:
 333     target = Runtime1::entry_for(Runtime1::access_field_patching_id);
 334     reloc_type = relocInfo::section_word_type;
 335     break;
 336   case PatchingStub::load_klass_id:
 337     target = Runtime1::entry_for(Runtime1::load_klass_patching_id);
 338     reloc_type = relocInfo::metadata_type;
 339     break;
 340   case PatchingStub::load_mirror_id:
 341     target = Runtime1::entry_for(Runtime1::load_mirror_patching_id);
 342     reloc_type = relocInfo::oop_type;
 343     break;
 344   case PatchingStub::load_appendix_id:
 345     target = Runtime1::entry_for(Runtime1::load_appendix_patching_id);
 346     reloc_type = relocInfo::oop_type;
 347     break;
 348   default: ShouldNotReachHere();
 349   }
 350 
 351   __ far_call(RuntimeAddress(target));
 352   add_call_info_here(info);
 353 }
 354 
 355 void LIR_Assembler::jobject2reg_with_patching(Register reg, CodeEmitInfo *info) {
 356   deoptimize_trap(info);
 357 }
 358 
 359 
 360 // This specifies the rsp decrement needed to build the frame
 361 int LIR_Assembler::initial_frame_size_in_bytes() const {
 362   // if rounding, must let FrameMap know!
 363 
 364   // The frame_map records size in slots (32bit word)
 365 
 366   // subtract two words to account for return address and link
 367   return (frame_map()-&gt;framesize() - (2*VMRegImpl::slots_per_word))  * VMRegImpl::stack_slot_size;
 368 }
 369 
 370 
 371 int LIR_Assembler::emit_exception_handler() {
 372   // if the last instruction is a call (typically to do a throw which
 373   // is coming at the end after block reordering) the return address
 374   // must still point into the code area in order to avoid assertion
 375   // failures when searching for the corresponding bci =&gt; add a nop
 376   // (was bug 5/14/1999 - gri)
 377   __ nop();
 378 
 379   // generate code for exception handler
 380   address handler_base = __ start_a_stub(exception_handler_size());
 381   if (handler_base == NULL) {
 382     // not enough space left for the handler
 383     bailout(&quot;exception handler overflow&quot;);
 384     return -1;
 385   }
 386 
 387   int offset = code_offset();
 388 
 389   // the exception oop and pc are in r0, and r3
 390   // no other registers need to be preserved, so invalidate them
 391   __ invalidate_registers(false, true, true, false, true, true);
 392 
 393   // check that there is really an exception
 394   __ verify_not_null_oop(r0);
 395 
 396   // search an exception handler (r0: exception oop, r3: throwing pc)
 397   __ far_call(RuntimeAddress(Runtime1::entry_for(Runtime1::handle_exception_from_callee_id)));  __ should_not_reach_here();
 398   guarantee(code_offset() - offset &lt;= exception_handler_size(), &quot;overflow&quot;);
 399   __ end_a_stub();
 400 
 401   return offset;
 402 }
 403 
 404 
 405 // Emit the code to remove the frame from the stack in the exception
 406 // unwind path.
 407 int LIR_Assembler::emit_unwind_handler() {
 408 #ifndef PRODUCT
 409   if (CommentedAssembly) {
 410     _masm-&gt;block_comment(&quot;Unwind handler&quot;);
 411   }
 412 #endif
 413 
 414   int offset = code_offset();
 415 
 416   // Fetch the exception from TLS and clear out exception related thread state
 417   __ ldr(r0, Address(rthread, JavaThread::exception_oop_offset()));
 418   __ str(zr, Address(rthread, JavaThread::exception_oop_offset()));
 419   __ str(zr, Address(rthread, JavaThread::exception_pc_offset()));
 420 
 421   __ bind(_unwind_handler_entry);
 422   __ verify_not_null_oop(r0);
 423   if (method()-&gt;is_synchronized() || compilation()-&gt;env()-&gt;dtrace_method_probes()) {
 424     __ mov(r19, r0);  // Preserve the exception
 425   }
 426 
 427   // Preform needed unlocking
 428   MonitorExitStub* stub = NULL;
 429   if (method()-&gt;is_synchronized()) {
 430     monitor_address(0, FrameMap::r0_opr);
 431     stub = new MonitorExitStub(FrameMap::r0_opr, true, 0);
 432     __ unlock_object(r5, r4, r0, *stub-&gt;entry());
 433     __ bind(*stub-&gt;continuation());
 434   }
 435 
 436   if (compilation()-&gt;env()-&gt;dtrace_method_probes()) {
 437     __ mov(c_rarg0, rthread);
 438     __ mov_metadata(c_rarg1, method()-&gt;constant_encoding());
 439     __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::dtrace_method_exit), c_rarg0, c_rarg1);
 440   }
 441 
 442   if (method()-&gt;is_synchronized() || compilation()-&gt;env()-&gt;dtrace_method_probes()) {
 443     __ mov(r0, r19);  // Restore the exception
 444   }
 445 
 446   // remove the activation and dispatch to the unwind handler
 447   __ block_comment(&quot;remove_frame and dispatch to the unwind handler&quot;);
 448   __ remove_frame(initial_frame_size_in_bytes(), needs_stack_repair());
 449   __ far_jump(RuntimeAddress(Runtime1::entry_for(Runtime1::unwind_exception_id)));
 450 
 451   // Emit the slow path assembly
 452   if (stub != NULL) {
 453     stub-&gt;emit_code(this);
 454   }
 455 
 456   return offset;
 457 }
 458 
 459 
 460 int LIR_Assembler::emit_deopt_handler() {
 461   // if the last instruction is a call (typically to do a throw which
 462   // is coming at the end after block reordering) the return address
 463   // must still point into the code area in order to avoid assertion
 464   // failures when searching for the corresponding bci =&gt; add a nop
 465   // (was bug 5/14/1999 - gri)
 466   __ nop();
 467 
 468   // generate code for exception handler
 469   address handler_base = __ start_a_stub(deopt_handler_size());
 470   if (handler_base == NULL) {
 471     // not enough space left for the handler
 472     bailout(&quot;deopt handler overflow&quot;);
 473     return -1;
 474   }
 475 
 476   int offset = code_offset();
 477 
 478   __ adr(lr, pc());
 479   __ far_jump(RuntimeAddress(SharedRuntime::deopt_blob()-&gt;unpack()));
 480   guarantee(code_offset() - offset &lt;= deopt_handler_size(), &quot;overflow&quot;);
 481   __ end_a_stub();
 482 
 483   return offset;
 484 }
 485 
 486 void LIR_Assembler::add_debug_info_for_branch(address adr, CodeEmitInfo* info) {
 487   _masm-&gt;code_section()-&gt;relocate(adr, relocInfo::poll_type);
 488   int pc_offset = code_offset();
 489   flush_debug_info(pc_offset);
 490   info-&gt;record_debug_info(compilation()-&gt;debug_info_recorder(), pc_offset);
 491   if (info-&gt;exception_handlers() != NULL) {
 492     compilation()-&gt;add_exception_handlers_for_pco(pc_offset, info-&gt;exception_handlers());
 493   }
 494 }
 495 
 496 void LIR_Assembler::return_op(LIR_Opr result) {
 497   assert(result-&gt;is_illegal() || !result-&gt;is_single_cpu() || result-&gt;as_register() == r0, &quot;word returns are in r0,&quot;);
 498 
 499   ciMethod* method = compilation()-&gt;method();
 500 
 501   if (ValueTypeReturnedAsFields &amp;&amp; method-&gt;signature()-&gt;returns_never_null()) {
 502     ciType* return_type = method-&gt;return_type();
 503     if (return_type-&gt;is_valuetype()) {
 504       ciValueKlass* vk = return_type-&gt;as_value_klass();
 505       if (vk-&gt;can_be_returned_as_fields()) {
 506         address unpack_handler = vk-&gt;unpack_handler();
 507         assert(unpack_handler != NULL, &quot;must be&quot;);
 508         __ far_call(RuntimeAddress(unpack_handler));
 509         // At this point, rax points to the value object (for interpreter or C1 caller).
 510         // The fields of the object are copied into registers (for C2 caller).
 511       }
 512     }
 513   }
 514 
 515   // Pop the stack before the safepoint code
 516   __ remove_frame(initial_frame_size_in_bytes(), needs_stack_repair());
 517 
 518   if (StackReservedPages &gt; 0 &amp;&amp; compilation()-&gt;has_reserved_stack_access()) {
 519     __ reserved_stack_check();
 520   }
 521 
 522   address polling_page(os::get_polling_page());
 523   __ read_polling_page(rscratch1, polling_page, relocInfo::poll_return_type);
 524   __ ret(lr);
 525 }
 526 
 527 int LIR_Assembler::store_value_type_fields_to_buf(ciValueKlass* vk) {
 528   return (__ store_value_type_fields_to_buf(vk, false));
 529 }
 530 
 531 int LIR_Assembler::safepoint_poll(LIR_Opr tmp, CodeEmitInfo* info) {
 532   address polling_page(os::get_polling_page());
 533   guarantee(info != NULL, &quot;Shouldn&#39;t be NULL&quot;);
 534   assert(os::is_poll_address(polling_page), &quot;should be&quot;);
 535   __ get_polling_page(rscratch1, polling_page, relocInfo::poll_type);
 536   add_debug_info_for_branch(info);  // This isn&#39;t just debug info:
 537                                     // it&#39;s the oop map
 538   __ read_polling_page(rscratch1, relocInfo::poll_type);
 539   return __ offset();
 540 }
 541 
 542 
 543 void LIR_Assembler::move_regs(Register from_reg, Register to_reg) {
 544   if (from_reg == r31_sp)
 545     from_reg = sp;
 546   if (to_reg == r31_sp)
 547     to_reg = sp;
 548   __ mov(to_reg, from_reg);
 549 }
 550 
 551 void LIR_Assembler::swap_reg(Register a, Register b) { Unimplemented(); }
 552 
 553 
 554 void LIR_Assembler::const2reg(LIR_Opr src, LIR_Opr dest, LIR_PatchCode patch_code, CodeEmitInfo* info) {
 555   assert(src-&gt;is_constant(), &quot;should not call otherwise&quot;);
 556   assert(dest-&gt;is_register(), &quot;should not call otherwise&quot;);
 557   LIR_Const* c = src-&gt;as_constant_ptr();
 558 
 559   switch (c-&gt;type()) {
 560     case T_INT: {
 561       assert(patch_code == lir_patch_none, &quot;no patching handled here&quot;);
 562       __ movw(dest-&gt;as_register(), c-&gt;as_jint());
 563       break;
 564     }
 565 
 566     case T_ADDRESS: {
 567       assert(patch_code == lir_patch_none, &quot;no patching handled here&quot;);
 568       __ mov(dest-&gt;as_register(), c-&gt;as_jint());
 569       break;
 570     }
 571 
 572     case T_LONG: {
 573       assert(patch_code == lir_patch_none, &quot;no patching handled here&quot;);
 574       __ mov(dest-&gt;as_register_lo(), (intptr_t)c-&gt;as_jlong());
 575       break;
 576     }
 577 
 578     case T_VALUETYPE:
 579     case T_OBJECT: {
 580         if (patch_code != lir_patch_none) {
 581           jobject2reg_with_patching(dest-&gt;as_register(), info);
 582         } else {
 583           jobject2reg(c-&gt;as_jobject(), dest-&gt;as_register());
 584         }
 585       break;
 586     }
 587 
 588     case T_METADATA: {
 589       if (patch_code != lir_patch_none) {
 590         klass2reg_with_patching(dest-&gt;as_register(), info);
 591       } else {
 592         __ mov_metadata(dest-&gt;as_register(), c-&gt;as_metadata());
 593       }
 594       break;
 595     }
 596 
 597     case T_FLOAT: {
 598       if (__ operand_valid_for_float_immediate(c-&gt;as_jfloat())) {
 599         __ fmovs(dest-&gt;as_float_reg(), (c-&gt;as_jfloat()));
 600       } else {
 601         __ adr(rscratch1, InternalAddress(float_constant(c-&gt;as_jfloat())));
 602         __ ldrs(dest-&gt;as_float_reg(), Address(rscratch1));
 603       }
 604       break;
 605     }
 606 
 607     case T_DOUBLE: {
 608       if (__ operand_valid_for_float_immediate(c-&gt;as_jdouble())) {
 609         __ fmovd(dest-&gt;as_double_reg(), (c-&gt;as_jdouble()));
 610       } else {
 611         __ adr(rscratch1, InternalAddress(double_constant(c-&gt;as_jdouble())));
 612         __ ldrd(dest-&gt;as_double_reg(), Address(rscratch1));
 613       }
 614       break;
 615     }
 616 
 617     default:
 618       ShouldNotReachHere();
 619   }
 620 }
 621 
 622 void LIR_Assembler::const2stack(LIR_Opr src, LIR_Opr dest) {
 623   LIR_Const* c = src-&gt;as_constant_ptr();
 624   switch (c-&gt;type()) {
 625   case T_VALUETYPE:
 626   case T_OBJECT:
 627     {
 628       if (! c-&gt;as_jobject())
 629         __ str(zr, frame_map()-&gt;address_for_slot(dest-&gt;single_stack_ix()));
 630       else {
 631         const2reg(src, FrameMap::rscratch1_opr, lir_patch_none, NULL);
 632         reg2stack(FrameMap::rscratch1_opr, dest, c-&gt;type(), false);
 633       }
 634     }
 635     break;
 636   case T_ADDRESS:
 637     {
 638       const2reg(src, FrameMap::rscratch1_opr, lir_patch_none, NULL);
 639       reg2stack(FrameMap::rscratch1_opr, dest, c-&gt;type(), false);
 640     }
 641   case T_INT:
 642   case T_FLOAT:
 643     {
 644       Register reg = zr;
 645       if (c-&gt;as_jint_bits() == 0)
 646         __ strw(zr, frame_map()-&gt;address_for_slot(dest-&gt;single_stack_ix()));
 647       else {
 648         __ movw(rscratch1, c-&gt;as_jint_bits());
 649         __ strw(rscratch1, frame_map()-&gt;address_for_slot(dest-&gt;single_stack_ix()));
 650       }
 651     }
 652     break;
 653   case T_LONG:
 654   case T_DOUBLE:
 655     {
 656       Register reg = zr;
 657       if (c-&gt;as_jlong_bits() == 0)
 658         __ str(zr, frame_map()-&gt;address_for_slot(dest-&gt;double_stack_ix(),
 659                                                  lo_word_offset_in_bytes));
 660       else {
 661         __ mov(rscratch1, (intptr_t)c-&gt;as_jlong_bits());
 662         __ str(rscratch1, frame_map()-&gt;address_for_slot(dest-&gt;double_stack_ix(),
 663                                                         lo_word_offset_in_bytes));
 664       }
 665     }
 666     break;
 667   default:
 668     ShouldNotReachHere();
 669   }
 670 }
 671 
 672 void LIR_Assembler::const2mem(LIR_Opr src, LIR_Opr dest, BasicType type, CodeEmitInfo* info, bool wide) {
 673   assert(src-&gt;is_constant(), &quot;should not call otherwise&quot;);
 674   LIR_Const* c = src-&gt;as_constant_ptr();
 675   LIR_Address* to_addr = dest-&gt;as_address_ptr();
 676 
 677   void (Assembler::* insn)(Register Rt, const Address &amp;adr);
 678 
 679   switch (type) {
 680   case T_ADDRESS:
 681     assert(c-&gt;as_jint() == 0, &quot;should be&quot;);
 682     insn = &amp;Assembler::str;
 683     break;
 684   case T_LONG:
 685     assert(c-&gt;as_jlong() == 0, &quot;should be&quot;);
 686     insn = &amp;Assembler::str;
 687     break;
 688   case T_INT:
 689     assert(c-&gt;as_jint() == 0, &quot;should be&quot;);
 690     insn = &amp;Assembler::strw;
 691     break;
 692   case T_VALUETYPE:
 693   case T_OBJECT:
 694   case T_ARRAY:
 695     // Non-null case is not handled on aarch64 but handled on x86
 696     // FIXME: do we need to add it here?
 697     assert(c-&gt;as_jobject() == 0, &quot;should be&quot;);
 698     if (UseCompressedOops &amp;&amp; !wide) {
 699       insn = &amp;Assembler::strw;
 700     } else {
 701       insn = &amp;Assembler::str;
 702     }
 703     break;
 704   case T_CHAR:
 705   case T_SHORT:
 706     assert(c-&gt;as_jint() == 0, &quot;should be&quot;);
 707     insn = &amp;Assembler::strh;
 708     break;
 709   case T_BOOLEAN:
 710   case T_BYTE:
 711     assert(c-&gt;as_jint() == 0, &quot;should be&quot;);
 712     insn = &amp;Assembler::strb;
 713     break;
 714   default:
 715     ShouldNotReachHere();
 716     insn = &amp;Assembler::str;  // unreachable
 717   }
 718 
 719   if (info) add_debug_info_for_null_check_here(info);
 720   (_masm-&gt;*insn)(zr, as_Address(to_addr, rscratch1));
 721 }
 722 
 723 void LIR_Assembler::reg2reg(LIR_Opr src, LIR_Opr dest) {
 724   assert(src-&gt;is_register(), &quot;should not call otherwise&quot;);
 725   assert(dest-&gt;is_register(), &quot;should not call otherwise&quot;);
 726 
 727   // move between cpu-registers
 728   if (dest-&gt;is_single_cpu()) {
 729     if (src-&gt;type() == T_LONG) {
 730       // Can do LONG -&gt; OBJECT
 731       move_regs(src-&gt;as_register_lo(), dest-&gt;as_register());
 732       return;
 733     }
 734     assert(src-&gt;is_single_cpu(), &quot;must match&quot;);
 735     if (src-&gt;type() == T_OBJECT || src-&gt;type() == T_VALUETYPE) {
 736       __ verify_oop(src-&gt;as_register());
 737     }
 738     move_regs(src-&gt;as_register(), dest-&gt;as_register());
 739 
 740   } else if (dest-&gt;is_double_cpu()) {
 741     if (is_reference_type(src-&gt;type())) {
 742       // Surprising to me but we can see move of a long to t_object
 743       __ verify_oop(src-&gt;as_register());
 744       move_regs(src-&gt;as_register(), dest-&gt;as_register_lo());
 745       return;
 746     }
 747     assert(src-&gt;is_double_cpu(), &quot;must match&quot;);
 748     Register f_lo = src-&gt;as_register_lo();
 749     Register f_hi = src-&gt;as_register_hi();
 750     Register t_lo = dest-&gt;as_register_lo();
 751     Register t_hi = dest-&gt;as_register_hi();
 752     assert(f_hi == f_lo, &quot;must be same&quot;);
 753     assert(t_hi == t_lo, &quot;must be same&quot;);
 754     move_regs(f_lo, t_lo);
 755 
 756   } else if (dest-&gt;is_single_fpu()) {
 757     __ fmovs(dest-&gt;as_float_reg(), src-&gt;as_float_reg());
 758 
 759   } else if (dest-&gt;is_double_fpu()) {
 760     __ fmovd(dest-&gt;as_double_reg(), src-&gt;as_double_reg());
 761 
 762   } else {
 763     ShouldNotReachHere();
 764   }
 765 }
 766 
 767 void LIR_Assembler::reg2stack(LIR_Opr src, LIR_Opr dest, BasicType type, bool pop_fpu_stack) {
 768   if (src-&gt;is_single_cpu()) {
 769     if (is_reference_type(type)) {
 770       __ str(src-&gt;as_register(), frame_map()-&gt;address_for_slot(dest-&gt;single_stack_ix()));
 771       __ verify_oop(src-&gt;as_register());
 772     } else if (type == T_METADATA || type == T_DOUBLE || type == T_ADDRESS) {
 773       __ str(src-&gt;as_register(), frame_map()-&gt;address_for_slot(dest-&gt;single_stack_ix()));
 774     } else {
 775       __ strw(src-&gt;as_register(), frame_map()-&gt;address_for_slot(dest-&gt;single_stack_ix()));
 776     }
 777 
 778   } else if (src-&gt;is_double_cpu()) {
 779     Address dest_addr_LO = frame_map()-&gt;address_for_slot(dest-&gt;double_stack_ix(), lo_word_offset_in_bytes);
 780     __ str(src-&gt;as_register_lo(), dest_addr_LO);
 781 
 782   } else if (src-&gt;is_single_fpu()) {
 783     Address dest_addr = frame_map()-&gt;address_for_slot(dest-&gt;single_stack_ix());
 784     __ strs(src-&gt;as_float_reg(), dest_addr);
 785 
 786   } else if (src-&gt;is_double_fpu()) {
 787     Address dest_addr = frame_map()-&gt;address_for_slot(dest-&gt;double_stack_ix());
 788     __ strd(src-&gt;as_double_reg(), dest_addr);
 789 
 790   } else {
 791     ShouldNotReachHere();
 792   }
 793 
 794 }
 795 
 796 
 797 void LIR_Assembler::reg2mem(LIR_Opr src, LIR_Opr dest, BasicType type, LIR_PatchCode patch_code, CodeEmitInfo* info, bool pop_fpu_stack, bool wide, bool /* unaligned */) {
 798   LIR_Address* to_addr = dest-&gt;as_address_ptr();
 799   PatchingStub* patch = NULL;
 800   Register compressed_src = rscratch1;
 801 
 802   if (patch_code != lir_patch_none) {
 803     deoptimize_trap(info);
 804     return;
 805   }
 806 
 807   if (is_reference_type(type)) {
 808     __ verify_oop(src-&gt;as_register());
 809 
 810     if (UseCompressedOops &amp;&amp; !wide) {
 811       __ encode_heap_oop(compressed_src, src-&gt;as_register());
 812     } else {
 813       compressed_src = src-&gt;as_register();
 814     }
 815   }
 816 
 817   int null_check_here = code_offset();
 818   switch (type) {
 819     case T_FLOAT: {
 820       __ strs(src-&gt;as_float_reg(), as_Address(to_addr));
 821       break;
 822     }
 823 
 824     case T_DOUBLE: {
 825       __ strd(src-&gt;as_double_reg(), as_Address(to_addr));
 826       break;
 827     }
 828 
 829     case T_VALUETYPE: // fall through
 830     case T_ARRAY:   // fall through
 831     case T_OBJECT:  // fall through
 832       if (UseCompressedOops &amp;&amp; !wide) {
 833         __ strw(compressed_src, as_Address(to_addr, rscratch2));
 834       } else {
 835          __ str(compressed_src, as_Address(to_addr));
 836       }
 837       break;
 838     case T_METADATA:
 839       // We get here to store a method pointer to the stack to pass to
 840       // a dtrace runtime call. This can&#39;t work on 64 bit with
 841       // compressed klass ptrs: T_METADATA can be a compressed klass
 842       // ptr or a 64 bit method pointer.
 843       ShouldNotReachHere();
 844       __ str(src-&gt;as_register(), as_Address(to_addr));
 845       break;
 846     case T_ADDRESS:
 847       __ str(src-&gt;as_register(), as_Address(to_addr));
 848       break;
 849     case T_INT:
 850       __ strw(src-&gt;as_register(), as_Address(to_addr));
 851       break;
 852 
 853     case T_LONG: {
 854       __ str(src-&gt;as_register_lo(), as_Address_lo(to_addr));
 855       break;
 856     }
 857 
 858     case T_BYTE:    // fall through
 859     case T_BOOLEAN: {
 860       __ strb(src-&gt;as_register(), as_Address(to_addr));
 861       break;
 862     }
 863 
 864     case T_CHAR:    // fall through
 865     case T_SHORT:
 866       __ strh(src-&gt;as_register(), as_Address(to_addr));
 867       break;
 868 
 869     default:
 870       ShouldNotReachHere();
 871   }
 872   if (info != NULL) {
 873     add_debug_info_for_null_check(null_check_here, info);
 874   }
 875 }
 876 
 877 
 878 void LIR_Assembler::stack2reg(LIR_Opr src, LIR_Opr dest, BasicType type) {
 879   assert(src-&gt;is_stack(), &quot;should not call otherwise&quot;);
 880   assert(dest-&gt;is_register(), &quot;should not call otherwise&quot;);
 881 
 882   if (dest-&gt;is_single_cpu()) {
 883     if (is_reference_type(type)) {
 884       __ ldr(dest-&gt;as_register(), frame_map()-&gt;address_for_slot(src-&gt;single_stack_ix()));
 885       __ verify_oop(dest-&gt;as_register());
 886     } else if (type == T_METADATA || type == T_ADDRESS) {
 887       __ ldr(dest-&gt;as_register(), frame_map()-&gt;address_for_slot(src-&gt;single_stack_ix()));
 888     } else {
 889       __ ldrw(dest-&gt;as_register(), frame_map()-&gt;address_for_slot(src-&gt;single_stack_ix()));
 890     }
 891 
 892   } else if (dest-&gt;is_double_cpu()) {
 893     Address src_addr_LO = frame_map()-&gt;address_for_slot(src-&gt;double_stack_ix(), lo_word_offset_in_bytes);
 894     __ ldr(dest-&gt;as_register_lo(), src_addr_LO);
 895 
 896   } else if (dest-&gt;is_single_fpu()) {
 897     Address src_addr = frame_map()-&gt;address_for_slot(src-&gt;single_stack_ix());
 898     __ ldrs(dest-&gt;as_float_reg(), src_addr);
 899 
 900   } else if (dest-&gt;is_double_fpu()) {
 901     Address src_addr = frame_map()-&gt;address_for_slot(src-&gt;double_stack_ix());
 902     __ ldrd(dest-&gt;as_double_reg(), src_addr);
 903 
 904   } else {
 905     ShouldNotReachHere();
 906   }
 907 }
 908 
 909 
 910 void LIR_Assembler::klass2reg_with_patching(Register reg, CodeEmitInfo* info) {
 911   address target = NULL;
 912   relocInfo::relocType reloc_type = relocInfo::none;
 913 
 914   switch (patching_id(info)) {
 915   case PatchingStub::access_field_id:
 916     target = Runtime1::entry_for(Runtime1::access_field_patching_id);
 917     reloc_type = relocInfo::section_word_type;
 918     break;
 919   case PatchingStub::load_klass_id:
 920     target = Runtime1::entry_for(Runtime1::load_klass_patching_id);
 921     reloc_type = relocInfo::metadata_type;
 922     break;
 923   case PatchingStub::load_mirror_id:
 924     target = Runtime1::entry_for(Runtime1::load_mirror_patching_id);
 925     reloc_type = relocInfo::oop_type;
 926     break;
 927   case PatchingStub::load_appendix_id:
 928     target = Runtime1::entry_for(Runtime1::load_appendix_patching_id);
 929     reloc_type = relocInfo::oop_type;
 930     break;
 931   default: ShouldNotReachHere();
 932   }
 933 
 934   __ far_call(RuntimeAddress(target));
 935   add_call_info_here(info);
 936 }
 937 
 938 void LIR_Assembler::stack2stack(LIR_Opr src, LIR_Opr dest, BasicType type) {
 939 
 940   LIR_Opr temp;
 941   if (type == T_LONG || type == T_DOUBLE)
 942     temp = FrameMap::rscratch1_long_opr;
 943   else
 944     temp = FrameMap::rscratch1_opr;
 945 
 946   stack2reg(src, temp, src-&gt;type());
 947   reg2stack(temp, dest, dest-&gt;type(), false);
 948 }
 949 
 950 
 951 void LIR_Assembler::mem2reg(LIR_Opr src, LIR_Opr dest, BasicType type, LIR_PatchCode patch_code, CodeEmitInfo* info, bool wide, bool /* unaligned */) {
 952   LIR_Address* addr = src-&gt;as_address_ptr();
 953   LIR_Address* from_addr = src-&gt;as_address_ptr();
 954 
 955   if (addr-&gt;base()-&gt;type() == T_OBJECT || addr-&gt;base()-&gt;type() == T_VALUETYPE) {
 956     __ verify_oop(addr-&gt;base()-&gt;as_pointer_register());
 957   }
 958 
 959   if (patch_code != lir_patch_none) {
 960     deoptimize_trap(info);
 961     return;
 962   }
 963 
 964   if (info != NULL) {
 965     add_debug_info_for_null_check_here(info);
 966   }
 967   int null_check_here = code_offset();
 968   switch (type) {
 969     case T_FLOAT: {
 970       __ ldrs(dest-&gt;as_float_reg(), as_Address(from_addr));
 971       break;
 972     }
 973 
 974     case T_DOUBLE: {
 975       __ ldrd(dest-&gt;as_double_reg(), as_Address(from_addr));
 976       break;
 977     }
 978 
 979     case T_VALUETYPE: // fall through
 980     case T_ARRAY:   // fall through
 981     case T_OBJECT:  // fall through
 982       if (UseCompressedOops &amp;&amp; !wide) {
 983         __ ldrw(dest-&gt;as_register(), as_Address(from_addr));
 984       } else {
 985          __ ldr(dest-&gt;as_register(), as_Address(from_addr));
 986       }
 987       break;
 988     case T_METADATA:
 989       // We get here to store a method pointer to the stack to pass to
 990       // a dtrace runtime call. This can&#39;t work on 64 bit with
 991       // compressed klass ptrs: T_METADATA can be a compressed klass
 992       // ptr or a 64 bit method pointer.
 993       ShouldNotReachHere();
 994       __ ldr(dest-&gt;as_register(), as_Address(from_addr));
 995       break;
 996     case T_ADDRESS:
 997       // FIXME: OMG this is a horrible kludge.  Any offset from an
 998       // address that matches klass_offset_in_bytes() will be loaded
 999       // as a word, not a long.
1000       if (UseCompressedClassPointers &amp;&amp; addr-&gt;disp() == oopDesc::klass_offset_in_bytes()) {
1001         __ ldrw(dest-&gt;as_register(), as_Address(from_addr));
1002       } else {
1003         __ ldr(dest-&gt;as_register(), as_Address(from_addr));
1004       }
1005       break;
1006     case T_INT:
1007       __ ldrw(dest-&gt;as_register(), as_Address(from_addr));
1008       break;
1009 
1010     case T_LONG: {
1011       __ ldr(dest-&gt;as_register_lo(), as_Address_lo(from_addr));
1012       break;
1013     }
1014 
1015     case T_BYTE:
1016       __ ldrsb(dest-&gt;as_register(), as_Address(from_addr));
1017       break;
1018     case T_BOOLEAN: {
1019       __ ldrb(dest-&gt;as_register(), as_Address(from_addr));
1020       break;
1021     }
1022 
1023     case T_CHAR:
1024       __ ldrh(dest-&gt;as_register(), as_Address(from_addr));
1025       break;
1026     case T_SHORT:
1027       __ ldrsh(dest-&gt;as_register(), as_Address(from_addr));
1028       break;
1029 
1030     default:
1031       ShouldNotReachHere();
1032   }
1033 
1034   if (is_reference_type(type)) {
1035     if (UseCompressedOops &amp;&amp; !wide) {
1036       __ decode_heap_oop(dest-&gt;as_register());
1037     }
1038 
1039     if (!UseZGC) {
1040       // Load barrier has not yet been applied, so ZGC can&#39;t verify the oop here
1041       __ verify_oop(dest-&gt;as_register());
1042     }
1043   } else if (type == T_ADDRESS &amp;&amp; addr-&gt;disp() == oopDesc::klass_offset_in_bytes()) {
1044     if (UseCompressedClassPointers) {
1045       __ andr(dest-&gt;as_register(), dest-&gt;as_register(), oopDesc::compressed_klass_mask());
1046       __ decode_klass_not_null(dest-&gt;as_register());
1047     } else {
1048       __   ubfm(dest-&gt;as_register(), dest-&gt;as_register(), 0, 63 - oopDesc::storage_props_nof_bits);
1049     }
1050   }
1051 }
1052 
1053 void LIR_Assembler::move(LIR_Opr src, LIR_Opr dst) {
1054   assert(dst-&gt;is_cpu_register(), &quot;must be&quot;);
1055   assert(dst-&gt;type() == src-&gt;type(), &quot;must be&quot;);
1056 
1057   if (src-&gt;is_cpu_register()) {
1058     reg2reg(src, dst);
1059   } else if (src-&gt;is_stack()) {
1060     stack2reg(src, dst, dst-&gt;type());
1061   } else if (src-&gt;is_constant()) {
1062     const2reg(src, dst, lir_patch_none, NULL);
1063   } else {
1064     ShouldNotReachHere();
1065   }
1066 }
1067 
1068 int LIR_Assembler::array_element_size(BasicType type) const {
1069   int elem_size = type2aelembytes(type);
1070   return exact_log2(elem_size);
1071 }
1072 
1073 
1074 void LIR_Assembler::emit_op3(LIR_Op3* op) {
1075   switch (op-&gt;code()) {
1076   case lir_idiv:
1077   case lir_irem:
1078     arithmetic_idiv(op-&gt;code(),
1079                     op-&gt;in_opr1(),
1080                     op-&gt;in_opr2(),
1081                     op-&gt;in_opr3(),
1082                     op-&gt;result_opr(),
1083                     op-&gt;info());
1084     break;
1085   case lir_fmad:
1086     __ fmaddd(op-&gt;result_opr()-&gt;as_double_reg(),
1087               op-&gt;in_opr1()-&gt;as_double_reg(),
1088               op-&gt;in_opr2()-&gt;as_double_reg(),
1089               op-&gt;in_opr3()-&gt;as_double_reg());
1090     break;
1091   case lir_fmaf:
1092     __ fmadds(op-&gt;result_opr()-&gt;as_float_reg(),
1093               op-&gt;in_opr1()-&gt;as_float_reg(),
1094               op-&gt;in_opr2()-&gt;as_float_reg(),
1095               op-&gt;in_opr3()-&gt;as_float_reg());
1096     break;
1097   default:      ShouldNotReachHere(); break;
1098   }
1099 }
1100 
1101 void LIR_Assembler::emit_opBranch(LIR_OpBranch* op) {
1102 #ifdef ASSERT
1103   assert(op-&gt;block() == NULL || op-&gt;block()-&gt;label() == op-&gt;label(), &quot;wrong label&quot;);
1104   if (op-&gt;block() != NULL)  _branch_target_blocks.append(op-&gt;block());
1105   if (op-&gt;ublock() != NULL) _branch_target_blocks.append(op-&gt;ublock());
1106 #endif
1107 
1108   if (op-&gt;cond() == lir_cond_always) {
1109     if (op-&gt;info() != NULL) add_debug_info_for_branch(op-&gt;info());
1110     __ b(*(op-&gt;label()));
1111   } else {
1112     Assembler::Condition acond;
1113     if (op-&gt;code() == lir_cond_float_branch) {
1114       bool is_unordered = (op-&gt;ublock() == op-&gt;block());
1115       // Assembler::EQ does not permit unordered branches, so we add
1116       // another branch here.  Likewise, Assembler::NE does not permit
1117       // ordered branches.
1118       if ((is_unordered &amp;&amp; op-&gt;cond() == lir_cond_equal)
1119           || (!is_unordered &amp;&amp; op-&gt;cond() == lir_cond_notEqual))
1120         __ br(Assembler::VS, *(op-&gt;ublock()-&gt;label()));
1121       switch(op-&gt;cond()) {
1122       case lir_cond_equal:        acond = Assembler::EQ; break;
1123       case lir_cond_notEqual:     acond = Assembler::NE; break;
1124       case lir_cond_less:         acond = (is_unordered ? Assembler::LT : Assembler::LO); break;
1125       case lir_cond_lessEqual:    acond = (is_unordered ? Assembler::LE : Assembler::LS); break;
1126       case lir_cond_greaterEqual: acond = (is_unordered ? Assembler::HS : Assembler::GE); break;
1127       case lir_cond_greater:      acond = (is_unordered ? Assembler::HI : Assembler::GT); break;
1128       default:                    ShouldNotReachHere();
1129         acond = Assembler::EQ;  // unreachable
1130       }
1131     } else {
1132       switch (op-&gt;cond()) {
1133         case lir_cond_equal:        acond = Assembler::EQ; break;
1134         case lir_cond_notEqual:     acond = Assembler::NE; break;
1135         case lir_cond_less:         acond = Assembler::LT; break;
1136         case lir_cond_lessEqual:    acond = Assembler::LE; break;
1137         case lir_cond_greaterEqual: acond = Assembler::GE; break;
1138         case lir_cond_greater:      acond = Assembler::GT; break;
1139         case lir_cond_belowEqual:   acond = Assembler::LS; break;
1140         case lir_cond_aboveEqual:   acond = Assembler::HS; break;
1141         default:                    ShouldNotReachHere();
1142           acond = Assembler::EQ;  // unreachable
1143       }
1144     }
1145     __ br(acond,*(op-&gt;label()));
1146   }
1147 }
1148 
1149 
1150 
1151 void LIR_Assembler::emit_opConvert(LIR_OpConvert* op) {
1152   LIR_Opr src  = op-&gt;in_opr();
1153   LIR_Opr dest = op-&gt;result_opr();
1154 
1155   switch (op-&gt;bytecode()) {
1156     case Bytecodes::_i2f:
1157       {
1158         __ scvtfws(dest-&gt;as_float_reg(), src-&gt;as_register());
1159         break;
1160       }
1161     case Bytecodes::_i2d:
1162       {
1163         __ scvtfwd(dest-&gt;as_double_reg(), src-&gt;as_register());
1164         break;
1165       }
1166     case Bytecodes::_l2d:
1167       {
1168         __ scvtfd(dest-&gt;as_double_reg(), src-&gt;as_register_lo());
1169         break;
1170       }
1171     case Bytecodes::_l2f:
1172       {
1173         __ scvtfs(dest-&gt;as_float_reg(), src-&gt;as_register_lo());
1174         break;
1175       }
1176     case Bytecodes::_f2d:
1177       {
1178         __ fcvts(dest-&gt;as_double_reg(), src-&gt;as_float_reg());
1179         break;
1180       }
1181     case Bytecodes::_d2f:
1182       {
1183         __ fcvtd(dest-&gt;as_float_reg(), src-&gt;as_double_reg());
1184         break;
1185       }
1186     case Bytecodes::_i2c:
1187       {
1188         __ ubfx(dest-&gt;as_register(), src-&gt;as_register(), 0, 16);
1189         break;
1190       }
1191     case Bytecodes::_i2l:
1192       {
1193         __ sxtw(dest-&gt;as_register_lo(), src-&gt;as_register());
1194         break;
1195       }
1196     case Bytecodes::_i2s:
1197       {
1198         __ sxth(dest-&gt;as_register(), src-&gt;as_register());
1199         break;
1200       }
1201     case Bytecodes::_i2b:
1202       {
1203         __ sxtb(dest-&gt;as_register(), src-&gt;as_register());
1204         break;
1205       }
1206     case Bytecodes::_l2i:
1207       {
1208         _masm-&gt;block_comment(&quot;FIXME: This could be a no-op&quot;);
1209         __ uxtw(dest-&gt;as_register(), src-&gt;as_register_lo());
1210         break;
1211       }
1212     case Bytecodes::_d2l:
1213       {
1214         __ fcvtzd(dest-&gt;as_register_lo(), src-&gt;as_double_reg());
1215         break;
1216       }
1217     case Bytecodes::_f2i:
1218       {
1219         __ fcvtzsw(dest-&gt;as_register(), src-&gt;as_float_reg());
1220         break;
1221       }
1222     case Bytecodes::_f2l:
1223       {
1224         __ fcvtzs(dest-&gt;as_register_lo(), src-&gt;as_float_reg());
1225         break;
1226       }
1227     case Bytecodes::_d2i:
1228       {
1229         __ fcvtzdw(dest-&gt;as_register(), src-&gt;as_double_reg());
1230         break;
1231       }
1232     default: ShouldNotReachHere();
1233   }
1234 }
1235 
1236 void LIR_Assembler::emit_alloc_obj(LIR_OpAllocObj* op) {
1237   if (op-&gt;init_check()) {
1238     __ ldrb(rscratch1, Address(op-&gt;klass()-&gt;as_register(),
1239                                InstanceKlass::init_state_offset()));
1240     __ cmpw(rscratch1, InstanceKlass::fully_initialized);
1241     add_debug_info_for_null_check_here(op-&gt;stub()-&gt;info());
1242     __ br(Assembler::NE, *op-&gt;stub()-&gt;entry());
1243   }
1244   __ allocate_object(op-&gt;obj()-&gt;as_register(),
1245                      op-&gt;tmp1()-&gt;as_register(),
1246                      op-&gt;tmp2()-&gt;as_register(),
1247                      op-&gt;header_size(),
1248                      op-&gt;object_size(),
1249                      op-&gt;klass()-&gt;as_register(),
1250                      *op-&gt;stub()-&gt;entry());
1251   __ bind(*op-&gt;stub()-&gt;continuation());
1252 }
1253 
1254 void LIR_Assembler::emit_alloc_array(LIR_OpAllocArray* op) {
1255   Register len =  op-&gt;len()-&gt;as_register();
1256   __ uxtw(len, len);
1257 
1258   if (UseSlowPath || op-&gt;type() == T_VALUETYPE ||
1259       (!UseFastNewObjectArray &amp;&amp; is_reference_type(op-&gt;type())) ||
1260       (!UseFastNewTypeArray   &amp;&amp; !is_reference_type(op-&gt;type()))) {
1261     __ b(*op-&gt;stub()-&gt;entry());
1262   } else {
1263     Register tmp1 = op-&gt;tmp1()-&gt;as_register();
1264     Register tmp2 = op-&gt;tmp2()-&gt;as_register();
1265     Register tmp3 = op-&gt;tmp3()-&gt;as_register();
1266     if (len == tmp1) {
1267       tmp1 = tmp3;
1268     } else if (len == tmp2) {
1269       tmp2 = tmp3;
1270     } else if (len == tmp3) {
1271       // everything is ok
1272     } else {
1273       __ mov(tmp3, len);
1274     }
1275     __ allocate_array(op-&gt;obj()-&gt;as_register(),
1276                       len,
1277                       tmp1,
1278                       tmp2,
1279                       arrayOopDesc::header_size(op-&gt;type()),
1280                       array_element_size(op-&gt;type()),
1281                       op-&gt;klass()-&gt;as_register(),
1282                       *op-&gt;stub()-&gt;entry());
1283   }
1284   __ bind(*op-&gt;stub()-&gt;continuation());
1285 }
1286 
1287 void LIR_Assembler::type_profile_helper(Register mdo,
1288                                         ciMethodData *md, ciProfileData *data,
1289                                         Register recv, Label* update_done) {
1290   for (uint i = 0; i &lt; ReceiverTypeData::row_limit(); i++) {
1291     Label next_test;
1292     // See if the receiver is receiver[n].
1293     __ lea(rscratch2, Address(mdo, md-&gt;byte_offset_of_slot(data, ReceiverTypeData::receiver_offset(i))));
1294     __ ldr(rscratch1, Address(rscratch2));
1295     __ cmp(recv, rscratch1);
1296     __ br(Assembler::NE, next_test);
1297     Address data_addr(mdo, md-&gt;byte_offset_of_slot(data, ReceiverTypeData::receiver_count_offset(i)));
1298     __ addptr(data_addr, DataLayout::counter_increment);
1299     __ b(*update_done);
1300     __ bind(next_test);
1301   }
1302 
1303   // Didn&#39;t find receiver; find next empty slot and fill it in
1304   for (uint i = 0; i &lt; ReceiverTypeData::row_limit(); i++) {
1305     Label next_test;
1306     __ lea(rscratch2,
1307            Address(mdo, md-&gt;byte_offset_of_slot(data, ReceiverTypeData::receiver_offset(i))));
1308     Address recv_addr(rscratch2);
1309     __ ldr(rscratch1, recv_addr);
1310     __ cbnz(rscratch1, next_test);
1311     __ str(recv, recv_addr);
1312     __ mov(rscratch1, DataLayout::counter_increment);
1313     __ lea(rscratch2, Address(mdo, md-&gt;byte_offset_of_slot(data, ReceiverTypeData::receiver_count_offset(i))));
1314     __ str(rscratch1, Address(rscratch2));
1315     __ b(*update_done);
1316     __ bind(next_test);
1317   }
1318 }
1319 
1320 void LIR_Assembler::emit_typecheck_helper(LIR_OpTypeCheck *op, Label* success, Label* failure, Label* obj_is_null) {
1321   // we always need a stub for the failure case.
1322   CodeStub* stub = op-&gt;stub();
1323   Register obj = op-&gt;object()-&gt;as_register();
1324   Register k_RInfo = op-&gt;tmp1()-&gt;as_register();
1325   Register klass_RInfo = op-&gt;tmp2()-&gt;as_register();
1326   Register dst = op-&gt;result_opr()-&gt;as_register();
1327   ciKlass* k = op-&gt;klass();
1328   Register Rtmp1 = noreg;
1329 
1330   // check if it needs to be profiled
1331   ciMethodData* md;
1332   ciProfileData* data;
1333 
1334   const bool should_profile = op-&gt;should_profile();
1335 
1336   if (should_profile) {
1337     ciMethod* method = op-&gt;profiled_method();
1338     assert(method != NULL, &quot;Should have method&quot;);
1339     int bci = op-&gt;profiled_bci();
1340     md = method-&gt;method_data_or_null();
1341     assert(md != NULL, &quot;Sanity&quot;);
1342     data = md-&gt;bci_to_data(bci);
1343     assert(data != NULL,                &quot;need data for type check&quot;);
1344     assert(data-&gt;is_ReceiverTypeData(), &quot;need ReceiverTypeData for type check&quot;);
1345   }
1346   Label profile_cast_success, profile_cast_failure;
1347   Label *success_target = should_profile ? &amp;profile_cast_success : success;
1348   Label *failure_target = should_profile ? &amp;profile_cast_failure : failure;
1349 
1350   if (obj == k_RInfo) {
1351     k_RInfo = dst;
1352   } else if (obj == klass_RInfo) {
1353     klass_RInfo = dst;
1354   }
1355   if (k-&gt;is_loaded() &amp;&amp; !UseCompressedClassPointers) {
1356     select_different_registers(obj, dst, k_RInfo, klass_RInfo);
1357   } else {
1358     Rtmp1 = op-&gt;tmp3()-&gt;as_register();
1359     select_different_registers(obj, dst, k_RInfo, klass_RInfo, Rtmp1);
1360   }
1361 
1362   assert_different_registers(obj, k_RInfo, klass_RInfo);
1363 
1364     if (should_profile) {
1365       Label not_null;
1366       __ cbnz(obj, not_null);
1367       // Object is null; update MDO and exit
1368       Register mdo  = klass_RInfo;
1369       __ mov_metadata(mdo, md-&gt;constant_encoding());
1370       Address data_addr
1371         = __ form_address(rscratch2, mdo,
1372                           md-&gt;byte_offset_of_slot(data, DataLayout::flags_offset()),
1373                           0);
1374       __ ldrb(rscratch1, data_addr);
1375       __ orr(rscratch1, rscratch1, BitData::null_seen_byte_constant());
1376       __ strb(rscratch1, data_addr);
1377       __ b(*obj_is_null);
1378       __ bind(not_null);
1379     } else {
1380       __ cbz(obj, *obj_is_null);
1381     }
1382 
1383   if (!k-&gt;is_loaded()) {
1384     klass2reg_with_patching(k_RInfo, op-&gt;info_for_patch());
1385   } else {
1386     __ mov_metadata(k_RInfo, k-&gt;constant_encoding());
1387   }
1388   __ verify_oop(obj);
1389 
1390   if (op-&gt;fast_check()) {
1391     // get object class
1392     // not a safepoint as obj null check happens earlier
1393     __ load_klass(rscratch1, obj);
1394     __ cmp( rscratch1, k_RInfo);
1395 
1396     __ br(Assembler::NE, *failure_target);
1397     // successful cast, fall through to profile or jump
1398   } else {
1399     // get object class
1400     // not a safepoint as obj null check happens earlier
1401     __ load_klass(klass_RInfo, obj);
1402     if (k-&gt;is_loaded()) {
1403       // See if we get an immediate positive hit
1404       __ ldr(rscratch1, Address(klass_RInfo, long(k-&gt;super_check_offset())));
1405       __ cmp(k_RInfo, rscratch1);
1406       if ((juint)in_bytes(Klass::secondary_super_cache_offset()) != k-&gt;super_check_offset()) {
1407         __ br(Assembler::NE, *failure_target);
1408         // successful cast, fall through to profile or jump
1409       } else {
1410         // See if we get an immediate positive hit
1411         __ br(Assembler::EQ, *success_target);
1412         // check for self
1413         __ cmp(klass_RInfo, k_RInfo);
1414         __ br(Assembler::EQ, *success_target);
1415 
1416         __ stp(klass_RInfo, k_RInfo, Address(__ pre(sp, -2 * wordSize)));
1417         __ far_call(RuntimeAddress(Runtime1::entry_for(Runtime1::slow_subtype_check_id)));
1418         __ ldr(klass_RInfo, Address(__ post(sp, 2 * wordSize)));
1419         // result is a boolean
1420         __ cbzw(klass_RInfo, *failure_target);
1421         // successful cast, fall through to profile or jump
1422       }
1423     } else {
1424       // perform the fast part of the checking logic
1425       __ check_klass_subtype_fast_path(klass_RInfo, k_RInfo, Rtmp1, success_target, failure_target, NULL);
1426       // call out-of-line instance of __ check_klass_subtype_slow_path(...):
1427       __ stp(klass_RInfo, k_RInfo, Address(__ pre(sp, -2 * wordSize)));
1428       __ far_call(RuntimeAddress(Runtime1::entry_for(Runtime1::slow_subtype_check_id)));
1429       __ ldp(k_RInfo, klass_RInfo, Address(__ post(sp, 2 * wordSize)));
1430       // result is a boolean
1431       __ cbz(k_RInfo, *failure_target);
1432       // successful cast, fall through to profile or jump
1433     }
1434   }
1435   if (should_profile) {
1436     Register mdo  = klass_RInfo, recv = k_RInfo;
1437     __ bind(profile_cast_success);
1438     __ mov_metadata(mdo, md-&gt;constant_encoding());
1439     __ load_klass(recv, obj);
1440     Label update_done;
1441     type_profile_helper(mdo, md, data, recv, success);
1442     __ b(*success);
1443 
1444     __ bind(profile_cast_failure);
1445     __ mov_metadata(mdo, md-&gt;constant_encoding());
1446     Address counter_addr
1447       = __ form_address(rscratch2, mdo,
1448                         md-&gt;byte_offset_of_slot(data, CounterData::count_offset()),
1449                         0);
1450     __ ldr(rscratch1, counter_addr);
1451     __ sub(rscratch1, rscratch1, DataLayout::counter_increment);
1452     __ str(rscratch1, counter_addr);
1453     __ b(*failure);
1454   }
1455   __ b(*success);
1456 }
1457 
1458 
1459 void LIR_Assembler::emit_opTypeCheck(LIR_OpTypeCheck* op) {
1460   const bool should_profile = op-&gt;should_profile();
1461 
1462   LIR_Code code = op-&gt;code();
1463   if (code == lir_store_check) {
1464     Register value = op-&gt;object()-&gt;as_register();
1465     Register array = op-&gt;array()-&gt;as_register();
1466     Register k_RInfo = op-&gt;tmp1()-&gt;as_register();
1467     Register klass_RInfo = op-&gt;tmp2()-&gt;as_register();
1468     Register Rtmp1 = op-&gt;tmp3()-&gt;as_register();
1469 
1470     CodeStub* stub = op-&gt;stub();
1471 
1472     // check if it needs to be profiled
1473     ciMethodData* md;
1474     ciProfileData* data;
1475 
1476     if (should_profile) {
1477       ciMethod* method = op-&gt;profiled_method();
1478       assert(method != NULL, &quot;Should have method&quot;);
1479       int bci = op-&gt;profiled_bci();
1480       md = method-&gt;method_data_or_null();
1481       assert(md != NULL, &quot;Sanity&quot;);
1482       data = md-&gt;bci_to_data(bci);
1483       assert(data != NULL,                &quot;need data for type check&quot;);
1484       assert(data-&gt;is_ReceiverTypeData(), &quot;need ReceiverTypeData for type check&quot;);
1485     }
1486     Label profile_cast_success, profile_cast_failure, done;
1487     Label *success_target = should_profile ? &amp;profile_cast_success : &amp;done;
1488     Label *failure_target = should_profile ? &amp;profile_cast_failure : stub-&gt;entry();
1489 
1490     if (should_profile) {
1491       Label not_null;
1492       __ cbnz(value, not_null);
1493       // Object is null; update MDO and exit
1494       Register mdo  = klass_RInfo;
1495       __ mov_metadata(mdo, md-&gt;constant_encoding());
1496       Address data_addr
1497         = __ form_address(rscratch2, mdo,
1498                           md-&gt;byte_offset_of_slot(data, DataLayout::flags_offset()),
1499                           0);
1500       __ ldrb(rscratch1, data_addr);
1501       __ orr(rscratch1, rscratch1, BitData::null_seen_byte_constant());
1502       __ strb(rscratch1, data_addr);
1503       __ b(done);
1504       __ bind(not_null);
1505     } else {
1506       __ cbz(value, done);
1507     }
1508 
1509     add_debug_info_for_null_check_here(op-&gt;info_for_exception());
1510     __ load_klass(k_RInfo, array);
1511     __ load_klass(klass_RInfo, value);
1512 
1513     // get instance klass (it&#39;s already uncompressed)
1514     __ ldr(k_RInfo, Address(k_RInfo, ObjArrayKlass::element_klass_offset()));
1515     // perform the fast part of the checking logic
1516     __ check_klass_subtype_fast_path(klass_RInfo, k_RInfo, Rtmp1, success_target, failure_target, NULL);
1517     // call out-of-line instance of __ check_klass_subtype_slow_path(...):
1518     __ stp(klass_RInfo, k_RInfo, Address(__ pre(sp, -2 * wordSize)));
1519     __ far_call(RuntimeAddress(Runtime1::entry_for(Runtime1::slow_subtype_check_id)));
1520     __ ldp(k_RInfo, klass_RInfo, Address(__ post(sp, 2 * wordSize)));
1521     // result is a boolean
1522     __ cbzw(k_RInfo, *failure_target);
1523     // fall through to the success case
1524 
1525     if (should_profile) {
1526       Register mdo  = klass_RInfo, recv = k_RInfo;
1527       __ bind(profile_cast_success);
1528       __ mov_metadata(mdo, md-&gt;constant_encoding());
1529       __ load_klass(recv, value);
1530       Label update_done;
1531       type_profile_helper(mdo, md, data, recv, &amp;done);
1532       __ b(done);
1533 
1534       __ bind(profile_cast_failure);
1535       __ mov_metadata(mdo, md-&gt;constant_encoding());
1536       Address counter_addr(mdo, md-&gt;byte_offset_of_slot(data, CounterData::count_offset()));
1537       __ lea(rscratch2, counter_addr);
1538       __ ldr(rscratch1, Address(rscratch2));
1539       __ sub(rscratch1, rscratch1, DataLayout::counter_increment);
1540       __ str(rscratch1, Address(rscratch2));
1541       __ b(*stub-&gt;entry());
1542     }
1543 
1544     __ bind(done);
1545   } else if (code == lir_checkcast) {
1546     Register obj = op-&gt;object()-&gt;as_register();
1547     Register dst = op-&gt;result_opr()-&gt;as_register();
1548     Label success;
1549     emit_typecheck_helper(op, &amp;success, op-&gt;stub()-&gt;entry(), &amp;success);
1550     __ bind(success);
1551     if (dst != obj) {
1552       __ mov(dst, obj);
1553     }
1554   } else if (code == lir_instanceof) {
1555     Register obj = op-&gt;object()-&gt;as_register();
1556     Register dst = op-&gt;result_opr()-&gt;as_register();
1557     Label success, failure, done;
1558     emit_typecheck_helper(op, &amp;success, &amp;failure, &amp;failure);
1559     __ bind(failure);
1560     __ mov(dst, zr);
1561     __ b(done);
1562     __ bind(success);
1563     __ mov(dst, 1);
1564     __ bind(done);
1565   } else {
1566     ShouldNotReachHere();
1567   }
1568 }
1569 
1570 void LIR_Assembler::emit_opFlattenedArrayCheck(LIR_OpFlattenedArrayCheck* op) {
1571   // We are loading/storing an array that *may* be a flattened array (the declared type
1572   // Object[], interface[], or VT?[]). If this array is flattened, take slow path.
1573 
1574   __ load_storage_props(op-&gt;tmp()-&gt;as_register(), op-&gt;array()-&gt;as_register());
1575   __ tst(op-&gt;tmp()-&gt;as_register(), ArrayStorageProperties::flattened_value);
1576   __ br(Assembler::NE, *op-&gt;stub()-&gt;entry());
1577   if (!op-&gt;value()-&gt;is_illegal()) {
1578     // We are storing into the array.
1579     Label skip;
1580     __ tst(op-&gt;tmp()-&gt;as_register(), ArrayStorageProperties::null_free_value);
1581     __ br(Assembler::EQ, skip);
1582     // The array is not flattened, but it is null_free. If we are storing
1583     // a null, take the slow path (which will throw NPE).
1584     __ cbz(op-&gt;value()-&gt;as_register(), *op-&gt;stub()-&gt;entry());
1585     __ bind(skip);
1586   }
1587 
1588 }
1589 
1590 void LIR_Assembler::emit_opNullFreeArrayCheck(LIR_OpNullFreeArrayCheck* op) {
1591   // This is called when we use aastore into a an array declared as &quot;[LVT;&quot;,
1592   // where we know VT is not flattenable (due to ValueArrayElemMaxFlatOops, etc).
1593   // However, we need to do a NULL check if the actual array is a &quot;[QVT;&quot;.
1594 
1595   __ load_storage_props(op-&gt;tmp()-&gt;as_register(), op-&gt;array()-&gt;as_register());
1596   __ mov(rscratch1, (uint64_t) ArrayStorageProperties::null_free_value);
1597   __ cmp(op-&gt;tmp()-&gt;as_register(), rscratch1);
1598 }
1599 
1600 void LIR_Assembler::emit_opSubstitutabilityCheck(LIR_OpSubstitutabilityCheck* op) {
1601   Label L_oops_equal;
1602   Label L_oops_not_equal;
1603   Label L_end;
1604 
1605   Register left  = op-&gt;left()-&gt;as_register();
1606   Register right = op-&gt;right()-&gt;as_register();
1607 
1608   __ cmp(left, right);
1609   __ br(Assembler::EQ, L_oops_equal);
1610 
1611   // (1) Null check -- if one of the operands is null, the other must not be null (because
1612   //     the two references are not equal), so they are not substitutable,
1613   //     FIXME: do null check only if the operand is nullable
1614   {
1615     __ cbz(left, L_oops_not_equal);
1616     __ cbz(right, L_oops_not_equal);
1617   }
1618 
1619 
1620   ciKlass* left_klass = op-&gt;left_klass();
1621   ciKlass* right_klass = op-&gt;right_klass();
1622 
1623   // (2) Value object check -- if either of the operands is not a value object,
1624   //     they are not substitutable. We do this only if we are not sure that the
1625   //     operands are value objects
1626   if ((left_klass == NULL || right_klass == NULL) ||// The klass is still unloaded, or came from a Phi node.
1627       !left_klass-&gt;is_valuetype() || !right_klass-&gt;is_valuetype()) {
1628     Register tmp1  = rscratch1; /* op-&gt;tmp1()-&gt;as_register(); */
1629     Register tmp2  = rscratch2; /* op-&gt;tmp2()-&gt;as_register(); */
1630 
1631     __ mov(tmp1, (intptr_t)markWord::always_locked_pattern);
1632 
1633     __ ldr(tmp2, Address(left, oopDesc::mark_offset_in_bytes()));
1634     __ andr(tmp1, tmp1, tmp2);
1635 
1636     __ ldr(tmp2, Address(right, oopDesc::mark_offset_in_bytes()));
1637     __ andr(tmp1, tmp1, tmp2);
1638 
1639     __ mov(tmp2, (intptr_t)markWord::always_locked_pattern);
1640     __ cmp(tmp1, tmp2);
1641     __ br(Assembler::NE, L_oops_not_equal);
1642   }
1643 
1644   // (3) Same klass check: if the operands are of different klasses, they are not substitutable.
1645   if (left_klass != NULL &amp;&amp; left_klass-&gt;is_valuetype() &amp;&amp; left_klass == right_klass) {
1646     // No need to load klass -- the operands are statically known to be the same value klass.
1647     __ b(*op-&gt;stub()-&gt;entry());
1648   } else {
1649     Register left_klass_op = op-&gt;left_klass_op()-&gt;as_register();
1650     Register right_klass_op = op-&gt;right_klass_op()-&gt;as_register();
1651 
1652     if (UseCompressedOops) {
1653       __ ldrw(left_klass_op,  Address(left,  oopDesc::klass_offset_in_bytes()));
1654       __ ldrw(right_klass_op, Address(right, oopDesc::klass_offset_in_bytes()));
1655       __ cmpw(left_klass_op, right_klass_op);
1656     } else {
1657       __ ldr(left_klass_op,  Address(left,  oopDesc::klass_offset_in_bytes()));
1658       __ ldr(right_klass_op, Address(right, oopDesc::klass_offset_in_bytes()));
1659       __ cmp(left_klass_op, right_klass_op);
1660     }
1661 
1662     __ br(Assembler::EQ, *op-&gt;stub()-&gt;entry()); // same klass -&gt; do slow check
1663     // fall through to L_oops_not_equal
1664   }
1665 
1666   __ bind(L_oops_not_equal);
1667   move(op-&gt;not_equal_result(), op-&gt;result_opr());
1668   __ b(L_end);
1669 
1670   __ bind(L_oops_equal);
1671   move(op-&gt;equal_result(), op-&gt;result_opr());
1672   __ b(L_end);
1673 
1674   // We&#39;ve returned from the stub. op-&gt;result_opr() contains 0x0 IFF the two
1675   // operands are not substitutable. (Don&#39;t compare against 0x1 in case the
1676   // C compiler is naughty)
1677   __ bind(*op-&gt;stub()-&gt;continuation());
1678 
1679   if (op-&gt;result_opr()-&gt;type() == T_LONG) {
1680     __ cbzw(op-&gt;result_opr()-&gt;as_register(), L_oops_not_equal); // (call_stub() == 0x0) -&gt; not_equal
1681   } else {
1682     __ cbz(op-&gt;result_opr()-&gt;as_register(), L_oops_not_equal); // (call_stub() == 0x0) -&gt; not_equal
1683   }
1684 
1685   move(op-&gt;equal_result(), op-&gt;result_opr()); // (call_stub() != 0x0) -&gt; equal
1686   // fall-through
1687   __ bind(L_end);
1688 
1689 }
1690 
1691 
1692 void LIR_Assembler::casw(Register addr, Register newval, Register cmpval) {
1693   __ cmpxchg(addr, cmpval, newval, Assembler::word, /* acquire*/ true, /* release*/ true, /* weak*/ false, rscratch1);
1694   __ cset(rscratch1, Assembler::NE);
1695   __ membar(__ AnyAny);
1696 }
1697 
1698 void LIR_Assembler::casl(Register addr, Register newval, Register cmpval) {
1699   __ cmpxchg(addr, cmpval, newval, Assembler::xword, /* acquire*/ true, /* release*/ true, /* weak*/ false, rscratch1);
1700   __ cset(rscratch1, Assembler::NE);
1701   __ membar(__ AnyAny);
1702 }
1703 
1704 
1705 void LIR_Assembler::emit_compare_and_swap(LIR_OpCompareAndSwap* op) {
1706   assert(VM_Version::supports_cx8(), &quot;wrong machine&quot;);
1707   Register addr;
1708   if (op-&gt;addr()-&gt;is_register()) {
1709     addr = as_reg(op-&gt;addr());
1710   } else {
1711     assert(op-&gt;addr()-&gt;is_address(), &quot;what else?&quot;);
1712     LIR_Address* addr_ptr = op-&gt;addr()-&gt;as_address_ptr();
1713     assert(addr_ptr-&gt;disp() == 0, &quot;need 0 disp&quot;);
1714     assert(addr_ptr-&gt;index() == LIR_OprDesc::illegalOpr(), &quot;need 0 index&quot;);
1715     addr = as_reg(addr_ptr-&gt;base());
1716   }
1717   Register newval = as_reg(op-&gt;new_value());
1718   Register cmpval = as_reg(op-&gt;cmp_value());
1719 
1720   if (op-&gt;code() == lir_cas_obj) {
1721     if (UseCompressedOops) {
1722       Register t1 = op-&gt;tmp1()-&gt;as_register();
1723       assert(op-&gt;tmp1()-&gt;is_valid(), &quot;must be&quot;);
1724       __ encode_heap_oop(t1, cmpval);
1725       cmpval = t1;
1726       __ encode_heap_oop(rscratch2, newval);
1727       newval = rscratch2;
1728       casw(addr, newval, cmpval);
1729     } else {
1730       casl(addr, newval, cmpval);
1731     }
1732   } else if (op-&gt;code() == lir_cas_int) {
1733     casw(addr, newval, cmpval);
1734   } else {
1735     casl(addr, newval, cmpval);
1736   }
1737 }
1738 
1739 
1740 void LIR_Assembler::cmove(LIR_Condition condition, LIR_Opr opr1, LIR_Opr opr2, LIR_Opr result, BasicType type) {
1741 
1742   Assembler::Condition acond, ncond;
1743   switch (condition) {
1744   case lir_cond_equal:        acond = Assembler::EQ; ncond = Assembler::NE; break;
1745   case lir_cond_notEqual:     acond = Assembler::NE; ncond = Assembler::EQ; break;
1746   case lir_cond_less:         acond = Assembler::LT; ncond = Assembler::GE; break;
1747   case lir_cond_lessEqual:    acond = Assembler::LE; ncond = Assembler::GT; break;
1748   case lir_cond_greaterEqual: acond = Assembler::GE; ncond = Assembler::LT; break;
1749   case lir_cond_greater:      acond = Assembler::GT; ncond = Assembler::LE; break;
1750   case lir_cond_belowEqual:
1751   case lir_cond_aboveEqual:
1752   default:                    ShouldNotReachHere();
1753     acond = Assembler::EQ; ncond = Assembler::NE;  // unreachable
1754   }
1755 
1756   assert(result-&gt;is_single_cpu() || result-&gt;is_double_cpu(),
1757          &quot;expect single register for result&quot;);
1758   if (opr1-&gt;is_constant() &amp;&amp; opr2-&gt;is_constant()
1759       &amp;&amp; opr1-&gt;type() == T_INT &amp;&amp; opr2-&gt;type() == T_INT) {
1760     jint val1 = opr1-&gt;as_jint();
1761     jint val2 = opr2-&gt;as_jint();
1762     if (val1 == 0 &amp;&amp; val2 == 1) {
1763       __ cset(result-&gt;as_register(), ncond);
1764       return;
1765     } else if (val1 == 1 &amp;&amp; val2 == 0) {
1766       __ cset(result-&gt;as_register(), acond);
1767       return;
1768     }
1769   }
1770 
1771   if (opr1-&gt;is_constant() &amp;&amp; opr2-&gt;is_constant()
1772       &amp;&amp; opr1-&gt;type() == T_LONG &amp;&amp; opr2-&gt;type() == T_LONG) {
1773     jlong val1 = opr1-&gt;as_jlong();
1774     jlong val2 = opr2-&gt;as_jlong();
1775     if (val1 == 0 &amp;&amp; val2 == 1) {
1776       __ cset(result-&gt;as_register_lo(), ncond);
1777       return;
1778     } else if (val1 == 1 &amp;&amp; val2 == 0) {
1779       __ cset(result-&gt;as_register_lo(), acond);
1780       return;
1781     }
1782   }
1783 
1784   if (opr1-&gt;is_stack()) {
1785     stack2reg(opr1, FrameMap::rscratch1_opr, result-&gt;type());
1786     opr1 = FrameMap::rscratch1_opr;
1787   } else if (opr1-&gt;is_constant()) {
1788     LIR_Opr tmp
1789       = opr1-&gt;type() == T_LONG ? FrameMap::rscratch1_long_opr : FrameMap::rscratch1_opr;
1790     const2reg(opr1, tmp, lir_patch_none, NULL);
1791     opr1 = tmp;
1792   }
1793 
1794   if (opr2-&gt;is_stack()) {
1795     stack2reg(opr2, FrameMap::rscratch2_opr, result-&gt;type());
1796     opr2 = FrameMap::rscratch2_opr;
1797   } else if (opr2-&gt;is_constant()) {
1798     LIR_Opr tmp
1799       = opr2-&gt;type() == T_LONG ? FrameMap::rscratch2_long_opr : FrameMap::rscratch2_opr;
1800     const2reg(opr2, tmp, lir_patch_none, NULL);
1801     opr2 = tmp;
1802   }
1803 
1804   if (result-&gt;type() == T_LONG)
1805     __ csel(result-&gt;as_register_lo(), opr1-&gt;as_register_lo(), opr2-&gt;as_register_lo(), acond);
1806   else
1807     __ csel(result-&gt;as_register(), opr1-&gt;as_register(), opr2-&gt;as_register(), acond);
1808 }
1809 
1810 void LIR_Assembler::arith_op(LIR_Code code, LIR_Opr left, LIR_Opr right, LIR_Opr dest, CodeEmitInfo* info, bool pop_fpu_stack) {
1811   assert(info == NULL, &quot;should never be used, idiv/irem and ldiv/lrem not handled by this method&quot;);
1812 
1813   if (left-&gt;is_single_cpu()) {
1814     Register lreg = left-&gt;as_register();
1815     Register dreg = as_reg(dest);
1816 
1817     if (right-&gt;is_single_cpu()) {
1818       // cpu register - cpu register
1819 
1820       assert(left-&gt;type() == T_INT &amp;&amp; right-&gt;type() == T_INT &amp;&amp; dest-&gt;type() == T_INT,
1821              &quot;should be&quot;);
1822       Register rreg = right-&gt;as_register();
1823       switch (code) {
1824       case lir_add: __ addw (dest-&gt;as_register(), lreg, rreg); break;
1825       case lir_sub: __ subw (dest-&gt;as_register(), lreg, rreg); break;
1826       case lir_mul: __ mulw (dest-&gt;as_register(), lreg, rreg); break;
1827       default:      ShouldNotReachHere();
1828       }
1829 
1830     } else if (right-&gt;is_double_cpu()) {
1831       Register rreg = right-&gt;as_register_lo();
1832       // single_cpu + double_cpu: can happen with obj+long
1833       assert(code == lir_add || code == lir_sub, &quot;mismatched arithmetic op&quot;);
1834       switch (code) {
1835       case lir_add: __ add(dreg, lreg, rreg); break;
1836       case lir_sub: __ sub(dreg, lreg, rreg); break;
1837       default: ShouldNotReachHere();
1838       }
1839     } else if (right-&gt;is_constant()) {
1840       // cpu register - constant
1841       jlong c;
1842 
1843       // FIXME.  This is fugly: we really need to factor all this logic.
1844       switch(right-&gt;type()) {
1845       case T_LONG:
1846         c = right-&gt;as_constant_ptr()-&gt;as_jlong();
1847         break;
1848       case T_INT:
1849       case T_ADDRESS:
1850         c = right-&gt;as_constant_ptr()-&gt;as_jint();
1851         break;
1852       default:
1853         ShouldNotReachHere();
1854         c = 0;  // unreachable
1855         break;
1856       }
1857 
1858       assert(code == lir_add || code == lir_sub, &quot;mismatched arithmetic op&quot;);
1859       if (c == 0 &amp;&amp; dreg == lreg) {
1860         COMMENT(&quot;effective nop elided&quot;);
1861         return;
1862       }
1863       switch(left-&gt;type()) {
1864       case T_INT:
1865         switch (code) {
1866         case lir_add: __ addw(dreg, lreg, c); break;
1867         case lir_sub: __ subw(dreg, lreg, c); break;
1868         default: ShouldNotReachHere();
1869         }
1870         break;
1871       case T_OBJECT:
1872       case T_ADDRESS:
1873         switch (code) {
1874         case lir_add: __ add(dreg, lreg, c); break;
1875         case lir_sub: __ sub(dreg, lreg, c); break;
1876         default: ShouldNotReachHere();
1877         }
1878         break;
1879       default:
1880         ShouldNotReachHere();
1881       }
1882     } else {
1883       ShouldNotReachHere();
1884     }
1885 
1886   } else if (left-&gt;is_double_cpu()) {
1887     Register lreg_lo = left-&gt;as_register_lo();
1888 
1889     if (right-&gt;is_double_cpu()) {
1890       // cpu register - cpu register
1891       Register rreg_lo = right-&gt;as_register_lo();
1892       switch (code) {
1893       case lir_add: __ add (dest-&gt;as_register_lo(), lreg_lo, rreg_lo); break;
1894       case lir_sub: __ sub (dest-&gt;as_register_lo(), lreg_lo, rreg_lo); break;
1895       case lir_mul: __ mul (dest-&gt;as_register_lo(), lreg_lo, rreg_lo); break;
1896       case lir_div: __ corrected_idivq(dest-&gt;as_register_lo(), lreg_lo, rreg_lo, false, rscratch1); break;
1897       case lir_rem: __ corrected_idivq(dest-&gt;as_register_lo(), lreg_lo, rreg_lo, true, rscratch1); break;
1898       default:
1899         ShouldNotReachHere();
1900       }
1901 
1902     } else if (right-&gt;is_constant()) {
1903       jlong c = right-&gt;as_constant_ptr()-&gt;as_jlong();
1904       Register dreg = as_reg(dest);
1905       switch (code) {
1906         case lir_add:
1907         case lir_sub:
1908           if (c == 0 &amp;&amp; dreg == lreg_lo) {
1909             COMMENT(&quot;effective nop elided&quot;);
1910             return;
1911           }
1912           code == lir_add ? __ add(dreg, lreg_lo, c) : __ sub(dreg, lreg_lo, c);
1913           break;
1914         case lir_div:
1915           assert(c &gt; 0 &amp;&amp; is_power_of_2(c), &quot;divisor must be power-of-2 constant&quot;);
1916           if (c == 1) {
1917             // move lreg_lo to dreg if divisor is 1
1918             __ mov(dreg, lreg_lo);
1919           } else {
1920             unsigned int shift = exact_log2_long(c);
1921             // use rscratch1 as intermediate result register
1922             __ asr(rscratch1, lreg_lo, 63);
1923             __ add(rscratch1, lreg_lo, rscratch1, Assembler::LSR, 64 - shift);
1924             __ asr(dreg, rscratch1, shift);
1925           }
1926           break;
1927         case lir_rem:
1928           assert(c &gt; 0 &amp;&amp; is_power_of_2(c), &quot;divisor must be power-of-2 constant&quot;);
1929           if (c == 1) {
1930             // move 0 to dreg if divisor is 1
1931             __ mov(dreg, zr);
1932           } else {
1933             // use rscratch1 as intermediate result register
1934             __ negs(rscratch1, lreg_lo);
1935             __ andr(dreg, lreg_lo, c - 1);
1936             __ andr(rscratch1, rscratch1, c - 1);
1937             __ csneg(dreg, dreg, rscratch1, Assembler::MI);
1938           }
1939           break;
1940         default:
1941           ShouldNotReachHere();
1942       }
1943     } else {
1944       ShouldNotReachHere();
1945     }
1946   } else if (left-&gt;is_single_fpu()) {
1947     assert(right-&gt;is_single_fpu(), &quot;right hand side of float arithmetics needs to be float register&quot;);
1948     switch (code) {
1949     case lir_add: __ fadds (dest-&gt;as_float_reg(), left-&gt;as_float_reg(), right-&gt;as_float_reg()); break;
1950     case lir_sub: __ fsubs (dest-&gt;as_float_reg(), left-&gt;as_float_reg(), right-&gt;as_float_reg()); break;
1951     case lir_mul_strictfp: // fall through
1952     case lir_mul: __ fmuls (dest-&gt;as_float_reg(), left-&gt;as_float_reg(), right-&gt;as_float_reg()); break;
1953     case lir_div_strictfp: // fall through
1954     case lir_div: __ fdivs (dest-&gt;as_float_reg(), left-&gt;as_float_reg(), right-&gt;as_float_reg()); break;
1955     default:
1956       ShouldNotReachHere();
1957     }
1958   } else if (left-&gt;is_double_fpu()) {
1959     if (right-&gt;is_double_fpu()) {
1960       // fpu register - fpu register
1961       switch (code) {
1962       case lir_add: __ faddd (dest-&gt;as_double_reg(), left-&gt;as_double_reg(), right-&gt;as_double_reg()); break;
1963       case lir_sub: __ fsubd (dest-&gt;as_double_reg(), left-&gt;as_double_reg(), right-&gt;as_double_reg()); break;
1964       case lir_mul_strictfp: // fall through
1965       case lir_mul: __ fmuld (dest-&gt;as_double_reg(), left-&gt;as_double_reg(), right-&gt;as_double_reg()); break;
1966       case lir_div_strictfp: // fall through
1967       case lir_div: __ fdivd (dest-&gt;as_double_reg(), left-&gt;as_double_reg(), right-&gt;as_double_reg()); break;
1968       default:
1969         ShouldNotReachHere();
1970       }
1971     } else {
1972       if (right-&gt;is_constant()) {
1973         ShouldNotReachHere();
1974       }
1975       ShouldNotReachHere();
1976     }
1977   } else if (left-&gt;is_single_stack() || left-&gt;is_address()) {
1978     assert(left == dest, &quot;left and dest must be equal&quot;);
1979     ShouldNotReachHere();
1980   } else {
1981     ShouldNotReachHere();
1982   }
1983 }
1984 
1985 void LIR_Assembler::arith_fpu_implementation(LIR_Code code, int left_index, int right_index, int dest_index, bool pop_fpu_stack) { Unimplemented(); }
1986 
1987 
1988 void LIR_Assembler::intrinsic_op(LIR_Code code, LIR_Opr value, LIR_Opr unused, LIR_Opr dest, LIR_Op* op) {
1989   switch(code) {
1990   case lir_abs : __ fabsd(dest-&gt;as_double_reg(), value-&gt;as_double_reg()); break;
1991   case lir_sqrt: __ fsqrtd(dest-&gt;as_double_reg(), value-&gt;as_double_reg()); break;
1992   default      : ShouldNotReachHere();
1993   }
1994 }
1995 
1996 void LIR_Assembler::logic_op(LIR_Code code, LIR_Opr left, LIR_Opr right, LIR_Opr dst) {
1997 
1998   assert(left-&gt;is_single_cpu() || left-&gt;is_double_cpu(), &quot;expect single or double register&quot;);
1999   Register Rleft = left-&gt;is_single_cpu() ? left-&gt;as_register() :
2000                                            left-&gt;as_register_lo();
2001    if (dst-&gt;is_single_cpu()) {
2002      Register Rdst = dst-&gt;as_register();
2003      if (right-&gt;is_constant()) {
2004        switch (code) {
2005          case lir_logic_and: __ andw (Rdst, Rleft, right-&gt;as_jint()); break;
2006          case lir_logic_or:  __ orrw (Rdst, Rleft, right-&gt;as_jint()); break;
2007          case lir_logic_xor: __ eorw (Rdst, Rleft, right-&gt;as_jint()); break;
2008          default: ShouldNotReachHere(); break;
2009        }
2010      } else {
2011        Register Rright = right-&gt;is_single_cpu() ? right-&gt;as_register() :
2012                                                   right-&gt;as_register_lo();
2013        switch (code) {
2014          case lir_logic_and: __ andw (Rdst, Rleft, Rright); break;
2015          case lir_logic_or:  __ orrw (Rdst, Rleft, Rright); break;
2016          case lir_logic_xor: __ eorw (Rdst, Rleft, Rright); break;
2017          default: ShouldNotReachHere(); break;
2018        }
2019      }
2020    } else {
2021      Register Rdst = dst-&gt;as_register_lo();
2022      if (right-&gt;is_constant()) {
2023        switch (code) {
2024          case lir_logic_and: __ andr (Rdst, Rleft, right-&gt;as_jlong()); break;
2025          case lir_logic_or:  __ orr (Rdst, Rleft, right-&gt;as_jlong()); break;
2026          case lir_logic_xor: __ eor (Rdst, Rleft, right-&gt;as_jlong()); break;
2027          default: ShouldNotReachHere(); break;
2028        }
2029      } else {
2030        Register Rright = right-&gt;is_single_cpu() ? right-&gt;as_register() :
2031                                                   right-&gt;as_register_lo();
2032        switch (code) {
2033          case lir_logic_and: __ andr (Rdst, Rleft, Rright); break;
2034          case lir_logic_or:  __ orr (Rdst, Rleft, Rright); break;
2035          case lir_logic_xor: __ eor (Rdst, Rleft, Rright); break;
2036          default: ShouldNotReachHere(); break;
2037        }
2038      }
2039    }
2040 }
2041 
2042 
2043 
2044 void LIR_Assembler::arithmetic_idiv(LIR_Code code, LIR_Opr left, LIR_Opr right, LIR_Opr illegal, LIR_Opr result, CodeEmitInfo* info) {
2045 
2046   // opcode check
2047   assert((code == lir_idiv) || (code == lir_irem), &quot;opcode must be idiv or irem&quot;);
2048   bool is_irem = (code == lir_irem);
2049 
2050   // operand check
2051   assert(left-&gt;is_single_cpu(),   &quot;left must be register&quot;);
2052   assert(right-&gt;is_single_cpu() || right-&gt;is_constant(),  &quot;right must be register or constant&quot;);
2053   assert(result-&gt;is_single_cpu(), &quot;result must be register&quot;);
2054   Register lreg = left-&gt;as_register();
2055   Register dreg = result-&gt;as_register();
2056 
2057   // power-of-2 constant check and codegen
2058   if (right-&gt;is_constant()) {
2059     int c = right-&gt;as_constant_ptr()-&gt;as_jint();
2060     assert(c &gt; 0 &amp;&amp; is_power_of_2(c), &quot;divisor must be power-of-2 constant&quot;);
2061     if (is_irem) {
2062       if (c == 1) {
2063         // move 0 to dreg if divisor is 1
2064         __ movw(dreg, zr);
2065       } else {
2066         // use rscratch1 as intermediate result register
2067         __ negsw(rscratch1, lreg);
2068         __ andw(dreg, lreg, c - 1);
2069         __ andw(rscratch1, rscratch1, c - 1);
2070         __ csnegw(dreg, dreg, rscratch1, Assembler::MI);
2071       }
2072     } else {
2073       if (c == 1) {
2074         // move lreg to dreg if divisor is 1
2075         __ movw(dreg, lreg);
2076       } else {
2077         unsigned int shift = exact_log2(c);
2078         // use rscratch1 as intermediate result register
2079         __ asrw(rscratch1, lreg, 31);
2080         __ addw(rscratch1, lreg, rscratch1, Assembler::LSR, 32 - shift);
2081         __ asrw(dreg, rscratch1, shift);
2082       }
2083     }
2084   } else {
2085     Register rreg = right-&gt;as_register();
2086     __ corrected_idivl(dreg, lreg, rreg, is_irem, rscratch1);
2087   }
2088 }
2089 
2090 
2091 void LIR_Assembler::comp_op(LIR_Condition condition, LIR_Opr opr1, LIR_Opr opr2, LIR_Op2* op) {
2092   if (opr1-&gt;is_constant() &amp;&amp; opr2-&gt;is_single_cpu()) {
2093     // tableswitch
2094     Register reg = as_reg(opr2);
2095     struct tableswitch &amp;table = switches[opr1-&gt;as_constant_ptr()-&gt;as_jint()];
2096     __ tableswitch(reg, table._first_key, table._last_key, table._branches, table._after);
2097   } else if (opr1-&gt;is_single_cpu() || opr1-&gt;is_double_cpu()) {
2098     Register reg1 = as_reg(opr1);
2099     if (opr2-&gt;is_single_cpu()) {
2100       // cpu register - cpu register
2101       Register reg2 = opr2-&gt;as_register();
2102       if (is_reference_type(opr1-&gt;type())) {
2103         __ cmpoop(reg1, reg2);
2104       } else {
2105         assert(!is_reference_type(opr2-&gt;type()), &quot;cmp int, oop?&quot;);
2106         __ cmpw(reg1, reg2);
2107       }
2108       return;
2109     }
2110     if (opr2-&gt;is_double_cpu()) {
2111       // cpu register - cpu register
2112       Register reg2 = opr2-&gt;as_register_lo();
2113       __ cmp(reg1, reg2);
2114       return;
2115     }
2116 
2117     if (opr2-&gt;is_constant()) {
2118       bool is_32bit = false; // width of register operand
2119       jlong imm;
2120 
2121       switch(opr2-&gt;type()) {
2122       case T_INT:
2123         imm = opr2-&gt;as_constant_ptr()-&gt;as_jint();
2124         is_32bit = true;
2125         break;
2126       case T_LONG:
2127         imm = opr2-&gt;as_constant_ptr()-&gt;as_jlong();
2128         break;
2129       case T_ADDRESS:
2130         imm = opr2-&gt;as_constant_ptr()-&gt;as_jint();
2131         break;
2132       case T_METADATA:
2133         imm = (intptr_t)(opr2-&gt;as_constant_ptr()-&gt;as_metadata());
2134         break;
2135       case T_VALUETYPE:
2136       case T_OBJECT:
2137       case T_ARRAY:
2138         jobject2reg(opr2-&gt;as_constant_ptr()-&gt;as_jobject(), rscratch1);
2139         __ cmpoop(reg1, rscratch1);
2140         return;
2141       default:
2142         ShouldNotReachHere();
2143         imm = 0;  // unreachable
2144         break;
2145       }
2146 
2147       if (Assembler::operand_valid_for_add_sub_immediate(imm)) {
2148         if (is_32bit)
2149           __ cmpw(reg1, imm);
2150         else
2151           __ subs(zr, reg1, imm);
2152         return;
2153       } else {
2154         __ mov(rscratch1, imm);
2155         if (is_32bit)
2156           __ cmpw(reg1, rscratch1);
2157         else
2158           __ cmp(reg1, rscratch1);
2159         return;
2160       }
2161     } else
2162       ShouldNotReachHere();
2163   } else if (opr1-&gt;is_single_fpu()) {
2164     FloatRegister reg1 = opr1-&gt;as_float_reg();
2165     assert(opr2-&gt;is_single_fpu(), &quot;expect single float register&quot;);
2166     FloatRegister reg2 = opr2-&gt;as_float_reg();
2167     __ fcmps(reg1, reg2);
2168   } else if (opr1-&gt;is_double_fpu()) {
2169     FloatRegister reg1 = opr1-&gt;as_double_reg();
2170     assert(opr2-&gt;is_double_fpu(), &quot;expect double float register&quot;);
2171     FloatRegister reg2 = opr2-&gt;as_double_reg();
2172     __ fcmpd(reg1, reg2);
2173   } else {
2174     ShouldNotReachHere();
2175   }
2176 }
2177 
2178 void LIR_Assembler::comp_fl2i(LIR_Code code, LIR_Opr left, LIR_Opr right, LIR_Opr dst, LIR_Op2* op){
2179   if (code == lir_cmp_fd2i || code == lir_ucmp_fd2i) {
2180     bool is_unordered_less = (code == lir_ucmp_fd2i);
2181     if (left-&gt;is_single_fpu()) {
2182       __ float_cmp(true, is_unordered_less ? -1 : 1, left-&gt;as_float_reg(), right-&gt;as_float_reg(), dst-&gt;as_register());
2183     } else if (left-&gt;is_double_fpu()) {
2184       __ float_cmp(false, is_unordered_less ? -1 : 1, left-&gt;as_double_reg(), right-&gt;as_double_reg(), dst-&gt;as_register());
2185     } else {
2186       ShouldNotReachHere();
2187     }
2188   } else if (code == lir_cmp_l2i) {
2189     Label done;
2190     __ cmp(left-&gt;as_register_lo(), right-&gt;as_register_lo());
2191     __ mov(dst-&gt;as_register(), (u_int64_t)-1L);
2192     __ br(Assembler::LT, done);
2193     __ csinc(dst-&gt;as_register(), zr, zr, Assembler::EQ);
2194     __ bind(done);
2195   } else {
2196     ShouldNotReachHere();
2197   }
2198 }
2199 
2200 
2201 void LIR_Assembler::align_call(LIR_Code code) {  }
2202 
2203 
2204 void LIR_Assembler::call(LIR_OpJavaCall* op, relocInfo::relocType rtype) {
2205   address call = __ trampoline_call(Address(op-&gt;addr(), rtype));
2206   if (call == NULL) {
2207     bailout(&quot;trampoline stub overflow&quot;);
2208     return;
2209   }
2210   add_call_info(code_offset(), op-&gt;info());
2211 }
2212 
2213 
2214 void LIR_Assembler::ic_call(LIR_OpJavaCall* op) {
2215   address call = __ ic_call(op-&gt;addr());
2216   if (call == NULL) {
2217     bailout(&quot;trampoline stub overflow&quot;);
2218     return;
2219   }
2220   add_call_info(code_offset(), op-&gt;info());
2221 }
2222 
2223 
2224 /* Currently, vtable-dispatch is only enabled for sparc platforms */
2225 void LIR_Assembler::vtable_call(LIR_OpJavaCall* op) {
2226   ShouldNotReachHere();
2227 }
2228 
2229 
2230 void LIR_Assembler::emit_static_call_stub() {
2231   address call_pc = __ pc();
2232   address stub = __ start_a_stub(call_stub_size());
2233   if (stub == NULL) {
2234     bailout(&quot;static call stub overflow&quot;);
2235     return;
2236   }
2237 
2238   int start = __ offset();
2239 
2240   __ relocate(static_stub_Relocation::spec(call_pc));
2241   __ emit_static_call_stub();
2242 
2243   assert(__ offset() - start + CompiledStaticCall::to_trampoline_stub_size()
2244         &lt;= call_stub_size(), &quot;stub too big&quot;);
2245   __ end_a_stub();
2246 }
2247 
2248 
2249 void LIR_Assembler::throw_op(LIR_Opr exceptionPC, LIR_Opr exceptionOop, CodeEmitInfo* info) {
2250   assert(exceptionOop-&gt;as_register() == r0, &quot;must match&quot;);
2251   assert(exceptionPC-&gt;as_register() == r3, &quot;must match&quot;);
2252 
2253   // exception object is not added to oop map by LinearScan
2254   // (LinearScan assumes that no oops are in fixed registers)
2255   info-&gt;add_register_oop(exceptionOop);
2256   Runtime1::StubID unwind_id;
2257 
2258   // get current pc information
2259   // pc is only needed if the method has an exception handler, the unwind code does not need it.
2260   int pc_for_athrow_offset = __ offset();
2261   InternalAddress pc_for_athrow(__ pc());
2262   __ adr(exceptionPC-&gt;as_register(), pc_for_athrow);
2263   add_call_info(pc_for_athrow_offset, info); // for exception handler
2264 
2265   __ verify_not_null_oop(r0);
2266   // search an exception handler (r0: exception oop, r3: throwing pc)
2267   if (compilation()-&gt;has_fpu_code()) {
2268     unwind_id = Runtime1::handle_exception_id;
2269   } else {
2270     unwind_id = Runtime1::handle_exception_nofpu_id;
2271   }
2272   __ far_call(RuntimeAddress(Runtime1::entry_for(unwind_id)));
2273 
2274   // FIXME: enough room for two byte trap   ????
2275   __ nop();
2276 }
2277 
2278 
2279 void LIR_Assembler::unwind_op(LIR_Opr exceptionOop) {
2280   assert(exceptionOop-&gt;as_register() == r0, &quot;must match&quot;);
2281 
2282   __ b(_unwind_handler_entry);
2283 }
2284 
2285 
2286 void LIR_Assembler::shift_op(LIR_Code code, LIR_Opr left, LIR_Opr count, LIR_Opr dest, LIR_Opr tmp) {
2287   Register lreg = left-&gt;is_single_cpu() ? left-&gt;as_register() : left-&gt;as_register_lo();
2288   Register dreg = dest-&gt;is_single_cpu() ? dest-&gt;as_register() : dest-&gt;as_register_lo();
2289 
2290   switch (left-&gt;type()) {
2291     case T_INT: {
2292       switch (code) {
2293       case lir_shl:  __ lslvw (dreg, lreg, count-&gt;as_register()); break;
2294       case lir_shr:  __ asrvw (dreg, lreg, count-&gt;as_register()); break;
2295       case lir_ushr: __ lsrvw (dreg, lreg, count-&gt;as_register()); break;
2296       default:
2297         ShouldNotReachHere();
2298         break;
2299       }
2300       break;
2301     case T_LONG:
2302     case T_VALUETYPE:
2303     case T_ADDRESS:
2304     case T_OBJECT:
2305       switch (code) {
2306       case lir_shl:  __ lslv (dreg, lreg, count-&gt;as_register()); break;
2307       case lir_shr:  __ asrv (dreg, lreg, count-&gt;as_register()); break;
2308       case lir_ushr: __ lsrv (dreg, lreg, count-&gt;as_register()); break;
2309       default:
2310         ShouldNotReachHere();
2311         break;
2312       }
2313       break;
2314     default:
2315       ShouldNotReachHere();
2316       break;
2317     }
2318   }
2319 }
2320 
2321 
2322 void LIR_Assembler::shift_op(LIR_Code code, LIR_Opr left, jint count, LIR_Opr dest) {
2323   Register dreg = dest-&gt;is_single_cpu() ? dest-&gt;as_register() : dest-&gt;as_register_lo();
2324   Register lreg = left-&gt;is_single_cpu() ? left-&gt;as_register() : left-&gt;as_register_lo();
2325 
2326   switch (left-&gt;type()) {
2327     case T_INT: {
2328       switch (code) {
2329       case lir_shl:  __ lslw (dreg, lreg, count); break;
2330       case lir_shr:  __ asrw (dreg, lreg, count); break;
2331       case lir_ushr: __ lsrw (dreg, lreg, count); break;
2332       default:
2333         ShouldNotReachHere();
2334         break;
2335       }
2336       break;
2337     case T_LONG:
2338     case T_ADDRESS:
2339     case T_VALUETYPE:
2340     case T_OBJECT:
2341       switch (code) {
2342       case lir_shl:  __ lsl (dreg, lreg, count); break;
2343       case lir_shr:  __ asr (dreg, lreg, count); break;
2344       case lir_ushr: __ lsr (dreg, lreg, count); break;
2345       default:
2346         ShouldNotReachHere();
2347         break;
2348       }
2349       break;
2350     default:
2351       ShouldNotReachHere();
2352       break;
2353     }
2354   }
2355 }
2356 
2357 
2358 void LIR_Assembler::store_parameter(Register r, int offset_from_rsp_in_words) {
2359   assert(offset_from_rsp_in_words &gt;= 0, &quot;invalid offset from rsp&quot;);
2360   int offset_from_rsp_in_bytes = offset_from_rsp_in_words * BytesPerWord;
2361   assert(offset_from_rsp_in_bytes &lt; frame_map()-&gt;reserved_argument_area_size(), &quot;invalid offset&quot;);
2362   __ str (r, Address(sp, offset_from_rsp_in_bytes));
2363 }
2364 
2365 
2366 void LIR_Assembler::store_parameter(jint c,     int offset_from_rsp_in_words) {
2367   assert(offset_from_rsp_in_words &gt;= 0, &quot;invalid offset from rsp&quot;);
2368   int offset_from_rsp_in_bytes = offset_from_rsp_in_words * BytesPerWord;
2369   assert(offset_from_rsp_in_bytes &lt; frame_map()-&gt;reserved_argument_area_size(), &quot;invalid offset&quot;);
2370   __ mov (rscratch1, c);
2371   __ str (rscratch1, Address(sp, offset_from_rsp_in_bytes));
2372 }
2373 
2374 
2375 void LIR_Assembler::store_parameter(jobject o,  int offset_from_rsp_in_words) {
2376   ShouldNotReachHere();
2377   assert(offset_from_rsp_in_words &gt;= 0, &quot;invalid offset from rsp&quot;);
2378   int offset_from_rsp_in_bytes = offset_from_rsp_in_words * BytesPerWord;
2379   assert(offset_from_rsp_in_bytes &lt; frame_map()-&gt;reserved_argument_area_size(), &quot;invalid offset&quot;);
2380   __ lea(rscratch1, __ constant_oop_address(o));
2381   __ str(rscratch1, Address(sp, offset_from_rsp_in_bytes));
2382 }
2383 
2384 void LIR_Assembler::arraycopy_valuetype_check(Register obj, Register tmp, CodeStub* slow_path, bool is_dest) {
2385   __ load_storage_props(tmp, obj);
2386   if (is_dest) {
2387     // We also take slow path if it&#39;s a null_free destination array, just in case the source array
2388     // contains NULLs.
2389     __ tst(tmp, ArrayStorageProperties::flattened_value | ArrayStorageProperties::null_free_value);
2390   } else {
2391     __ tst(tmp, ArrayStorageProperties::flattened_value);
2392   }
2393   __ br(Assembler::NE, *slow_path-&gt;entry());
2394 }
2395 
2396 
2397 
2398 // This code replaces a call to arraycopy; no exception may
2399 // be thrown in this code, they must be thrown in the System.arraycopy
2400 // activation frame; we could save some checks if this would not be the case
2401 void LIR_Assembler::emit_arraycopy(LIR_OpArrayCopy* op) {
2402   ciArrayKlass* default_type = op-&gt;expected_type();
2403   Register src = op-&gt;src()-&gt;as_register();
2404   Register dst = op-&gt;dst()-&gt;as_register();
2405   Register src_pos = op-&gt;src_pos()-&gt;as_register();
2406   Register dst_pos = op-&gt;dst_pos()-&gt;as_register();
2407   Register length  = op-&gt;length()-&gt;as_register();
2408   Register tmp = op-&gt;tmp()-&gt;as_register();
2409 
2410   __ resolve(ACCESS_READ, src);
2411   __ resolve(ACCESS_WRITE, dst);
2412 
2413   CodeStub* stub = op-&gt;stub();
2414   int flags = op-&gt;flags();
2415   BasicType basic_type = default_type != NULL ? default_type-&gt;element_type()-&gt;basic_type() : T_ILLEGAL;
2416   if (is_reference_type(basic_type)) basic_type = T_OBJECT;
2417 
2418   if (flags &amp; LIR_OpArrayCopy::always_slow_path) {
2419     __ b(*stub-&gt;entry());
2420     __ bind(*stub-&gt;continuation());
2421     return;
2422   }
2423 
2424   if (flags &amp; LIR_OpArrayCopy::src_valuetype_check) {
2425     arraycopy_valuetype_check(src, tmp, stub, false);
2426   }
2427 
2428   if (flags &amp; LIR_OpArrayCopy::dst_valuetype_check) {
2429     arraycopy_valuetype_check(dst, tmp, stub, true);
2430   }
2431 
2432 
2433 
2434   // if we don&#39;t know anything, just go through the generic arraycopy
2435   if (default_type == NULL // || basic_type == T_OBJECT
2436       ) {
2437     Label done;
2438     assert(src == r1 &amp;&amp; src_pos == r2, &quot;mismatch in calling convention&quot;);
2439 
2440     // Save the arguments in case the generic arraycopy fails and we
2441     // have to fall back to the JNI stub
2442     __ stp(dst,     dst_pos, Address(sp, 0*BytesPerWord));
2443     __ stp(length,  src_pos, Address(sp, 2*BytesPerWord));
2444     __ str(src,              Address(sp, 4*BytesPerWord));
2445 
2446     address copyfunc_addr = StubRoutines::generic_arraycopy();
2447     assert(copyfunc_addr != NULL, &quot;generic arraycopy stub required&quot;);
2448 
2449     // The arguments are in java calling convention so we shift them
2450     // to C convention
2451     assert_different_registers(c_rarg0, j_rarg1, j_rarg2, j_rarg3, j_rarg4);
2452     __ mov(c_rarg0, j_rarg0);
2453     assert_different_registers(c_rarg1, j_rarg2, j_rarg3, j_rarg4);
2454     __ mov(c_rarg1, j_rarg1);
2455     assert_different_registers(c_rarg2, j_rarg3, j_rarg4);
2456     __ mov(c_rarg2, j_rarg2);
2457     assert_different_registers(c_rarg3, j_rarg4);
2458     __ mov(c_rarg3, j_rarg3);
2459     __ mov(c_rarg4, j_rarg4);
2460 #ifndef PRODUCT
2461     if (PrintC1Statistics) {
2462       __ incrementw(ExternalAddress((address)&amp;Runtime1::_generic_arraycopystub_cnt));
2463     }
2464 #endif
2465     __ far_call(RuntimeAddress(copyfunc_addr));
2466 
2467     __ cbz(r0, *stub-&gt;continuation());
2468 
2469     // Reload values from the stack so they are where the stub
2470     // expects them.
2471     __ ldp(dst,     dst_pos, Address(sp, 0*BytesPerWord));
2472     __ ldp(length,  src_pos, Address(sp, 2*BytesPerWord));
2473     __ ldr(src,              Address(sp, 4*BytesPerWord));
2474 
2475     // r0 is -1^K where K == partial copied count
2476     __ eonw(rscratch1, r0, zr);
2477     // adjust length down and src/end pos up by partial copied count
2478     __ subw(length, length, rscratch1);
2479     __ addw(src_pos, src_pos, rscratch1);
2480     __ addw(dst_pos, dst_pos, rscratch1);
2481     __ b(*stub-&gt;entry());
2482 
2483     __ bind(*stub-&gt;continuation());
2484     return;
2485   }
2486 
2487   assert(default_type != NULL &amp;&amp; default_type-&gt;is_array_klass() &amp;&amp; default_type-&gt;is_loaded(), &quot;must be true at this point&quot;);
2488 
2489   int elem_size = type2aelembytes(basic_type);
2490   int shift_amount;
2491   int scale = exact_log2(elem_size);
2492 
2493   Address src_length_addr = Address(src, arrayOopDesc::length_offset_in_bytes());
2494   Address dst_length_addr = Address(dst, arrayOopDesc::length_offset_in_bytes());
2495   Address src_klass_addr = Address(src, oopDesc::klass_offset_in_bytes());
2496   Address dst_klass_addr = Address(dst, oopDesc::klass_offset_in_bytes());
2497 
2498   // test for NULL
2499   if (flags &amp; LIR_OpArrayCopy::src_null_check) {
2500     __ cbz(src, *stub-&gt;entry());
2501   }
2502   if (flags &amp; LIR_OpArrayCopy::dst_null_check) {
2503     __ cbz(dst, *stub-&gt;entry());
2504   }
2505 
2506   // If the compiler was not able to prove that exact type of the source or the destination
2507   // of the arraycopy is an array type, check at runtime if the source or the destination is
2508   // an instance type.
2509   if (flags &amp; LIR_OpArrayCopy::type_check) {
2510     if (!(flags &amp; LIR_OpArrayCopy::LIR_OpArrayCopy::dst_objarray)) {
2511       __ load_klass(tmp, dst);
2512       __ ldrw(rscratch1, Address(tmp, in_bytes(Klass::layout_helper_offset())));
2513       __ cmpw(rscratch1, Klass::_lh_neutral_value);
2514       __ br(Assembler::GE, *stub-&gt;entry());
2515     }
2516 
2517     if (!(flags &amp; LIR_OpArrayCopy::LIR_OpArrayCopy::src_objarray)) {
2518       __ load_klass(tmp, src);
2519       __ ldrw(rscratch1, Address(tmp, in_bytes(Klass::layout_helper_offset())));
2520       __ cmpw(rscratch1, Klass::_lh_neutral_value);
2521       __ br(Assembler::GE, *stub-&gt;entry());
2522     }
2523   }
2524 
2525   // check if negative
2526   if (flags &amp; LIR_OpArrayCopy::src_pos_positive_check) {
2527     __ cmpw(src_pos, 0);
2528     __ br(Assembler::LT, *stub-&gt;entry());
2529   }
2530   if (flags &amp; LIR_OpArrayCopy::dst_pos_positive_check) {
2531     __ cmpw(dst_pos, 0);
2532     __ br(Assembler::LT, *stub-&gt;entry());
2533   }
2534 
2535   if (flags &amp; LIR_OpArrayCopy::length_positive_check) {
2536     __ cmpw(length, 0);
2537     __ br(Assembler::LT, *stub-&gt;entry());
2538   }
2539 
2540   if (flags &amp; LIR_OpArrayCopy::src_range_check) {
2541     __ addw(tmp, src_pos, length);
2542     __ ldrw(rscratch1, src_length_addr);
2543     __ cmpw(tmp, rscratch1);
2544     __ br(Assembler::HI, *stub-&gt;entry());
2545   }
2546   if (flags &amp; LIR_OpArrayCopy::dst_range_check) {
2547     __ addw(tmp, dst_pos, length);
2548     __ ldrw(rscratch1, dst_length_addr);
2549     __ cmpw(tmp, rscratch1);
2550     __ br(Assembler::HI, *stub-&gt;entry());
2551   }
2552 
2553   if (flags &amp; LIR_OpArrayCopy::type_check) {
2554     // We don&#39;t know the array types are compatible
2555     if (basic_type != T_OBJECT) {
2556       // Simple test for basic type arrays
2557       if (UseCompressedClassPointers) {
2558         __ ldrw(tmp, src_klass_addr);
2559         __ ldrw(rscratch1, dst_klass_addr);
2560         __ cmpw(tmp, rscratch1);
2561       } else {
2562         __ ldr(tmp, src_klass_addr);
2563         __ ldr(rscratch1, dst_klass_addr);
2564         __ cmp(tmp, rscratch1);
2565       }
2566       __ br(Assembler::NE, *stub-&gt;entry());
2567     } else {
2568       // For object arrays, if src is a sub class of dst then we can
2569       // safely do the copy.
2570       Label cont, slow;
2571 
2572 #define PUSH(r1, r2)                                    \
2573       stp(r1, r2, __ pre(sp, -2 * wordSize));
2574 
2575 #define POP(r1, r2)                                     \
2576       ldp(r1, r2, __ post(sp, 2 * wordSize));
2577 
2578       __ PUSH(src, dst);
2579 
2580       __ load_klass(src, src);
2581       __ load_klass(dst, dst);
2582 
2583       __ check_klass_subtype_fast_path(src, dst, tmp, &amp;cont, &amp;slow, NULL);
2584 
2585       __ PUSH(src, dst);
2586       __ far_call(RuntimeAddress(Runtime1::entry_for(Runtime1::slow_subtype_check_id)));
2587       __ POP(src, dst);
2588 
2589       __ cbnz(src, cont);
2590 
2591       __ bind(slow);
2592       __ POP(src, dst);
2593 
2594       address copyfunc_addr = StubRoutines::checkcast_arraycopy();
2595       if (copyfunc_addr != NULL) { // use stub if available
2596         // src is not a sub class of dst so we have to do a
2597         // per-element check.
2598 
2599         int mask = LIR_OpArrayCopy::src_objarray|LIR_OpArrayCopy::dst_objarray;
2600         if ((flags &amp; mask) != mask) {
2601           // Check that at least both of them object arrays.
2602           assert(flags &amp; mask, &quot;one of the two should be known to be an object array&quot;);
2603 
2604           if (!(flags &amp; LIR_OpArrayCopy::src_objarray)) {
2605             __ load_klass(tmp, src);
2606           } else if (!(flags &amp; LIR_OpArrayCopy::dst_objarray)) {
2607             __ load_klass(tmp, dst);
2608           }
2609           int lh_offset = in_bytes(Klass::layout_helper_offset());
2610           Address klass_lh_addr(tmp, lh_offset);
2611           jint objArray_lh = Klass::array_layout_helper(T_OBJECT);
2612           __ ldrw(rscratch1, klass_lh_addr);
2613           __ mov(rscratch2, objArray_lh);
2614           __ eorw(rscratch1, rscratch1, rscratch2);
2615           __ cbnzw(rscratch1, *stub-&gt;entry());
2616         }
2617 
2618        // Spill because stubs can use any register they like and it&#39;s
2619        // easier to restore just those that we care about.
2620         __ stp(dst,     dst_pos, Address(sp, 0*BytesPerWord));
2621         __ stp(length,  src_pos, Address(sp, 2*BytesPerWord));
2622         __ str(src,              Address(sp, 4*BytesPerWord));
2623 
2624         __ lea(c_rarg0, Address(src, src_pos, Address::uxtw(scale)));
2625         __ add(c_rarg0, c_rarg0, arrayOopDesc::base_offset_in_bytes(basic_type));
2626         assert_different_registers(c_rarg0, dst, dst_pos, length);
2627         __ lea(c_rarg1, Address(dst, dst_pos, Address::uxtw(scale)));
2628         __ add(c_rarg1, c_rarg1, arrayOopDesc::base_offset_in_bytes(basic_type));
2629         assert_different_registers(c_rarg1, dst, length);
2630         __ uxtw(c_rarg2, length);
2631         assert_different_registers(c_rarg2, dst);
2632 
2633         __ load_klass(c_rarg4, dst);
2634         __ ldr(c_rarg4, Address(c_rarg4, ObjArrayKlass::element_klass_offset()));
2635         __ ldrw(c_rarg3, Address(c_rarg4, Klass::super_check_offset_offset()));
2636         __ far_call(RuntimeAddress(copyfunc_addr));
2637 
2638 #ifndef PRODUCT
2639         if (PrintC1Statistics) {
2640           Label failed;
2641           __ cbnz(r0, failed);
2642           __ incrementw(ExternalAddress((address)&amp;Runtime1::_arraycopy_checkcast_cnt));
2643           __ bind(failed);
2644         }
2645 #endif
2646 
2647         __ cbz(r0, *stub-&gt;continuation());
2648 
2649 #ifndef PRODUCT
2650         if (PrintC1Statistics) {
2651           __ incrementw(ExternalAddress((address)&amp;Runtime1::_arraycopy_checkcast_attempt_cnt));
2652         }
2653 #endif
2654         assert_different_registers(dst, dst_pos, length, src_pos, src, r0, rscratch1);
2655 
2656         // Restore previously spilled arguments
2657         __ ldp(dst,     dst_pos, Address(sp, 0*BytesPerWord));
2658         __ ldp(length,  src_pos, Address(sp, 2*BytesPerWord));
2659         __ ldr(src,              Address(sp, 4*BytesPerWord));
2660 
2661         // return value is -1^K where K is partial copied count
2662         __ eonw(rscratch1, r0, zr);
2663         // adjust length down and src/end pos up by partial copied count
2664         __ subw(length, length, rscratch1);
2665         __ addw(src_pos, src_pos, rscratch1);
2666         __ addw(dst_pos, dst_pos, rscratch1);
2667       }
2668 
2669       __ b(*stub-&gt;entry());
2670 
2671       __ bind(cont);
2672       __ POP(src, dst);
2673     }
2674   }
2675 
2676 #ifdef ASSERT
2677   if (basic_type != T_OBJECT || !(flags &amp; LIR_OpArrayCopy::type_check)) {
2678     // Sanity check the known type with the incoming class.  For the
2679     // primitive case the types must match exactly with src.klass and
2680     // dst.klass each exactly matching the default type.  For the
2681     // object array case, if no type check is needed then either the
2682     // dst type is exactly the expected type and the src type is a
2683     // subtype which we can&#39;t check or src is the same array as dst
2684     // but not necessarily exactly of type default_type.
2685     Label known_ok, halt;
2686     __ mov_metadata(tmp, default_type-&gt;constant_encoding());
2687     if (UseCompressedClassPointers) {
2688       __ encode_klass_not_null(tmp);
2689     }
2690 
2691     if (basic_type != T_OBJECT) {
2692 
2693       if (UseCompressedClassPointers) {
2694         __ ldrw(rscratch1, dst_klass_addr);
2695         __ cmpw(tmp, rscratch1);
2696       } else {
2697         __ ldr(rscratch1, dst_klass_addr);
2698         __ cmp(tmp, rscratch1);
2699       }
2700       __ br(Assembler::NE, halt);
2701       if (UseCompressedClassPointers) {
2702         __ ldrw(rscratch1, src_klass_addr);
2703         __ cmpw(tmp, rscratch1);
2704       } else {
2705         __ ldr(rscratch1, src_klass_addr);
2706         __ cmp(tmp, rscratch1);
2707       }
2708       __ br(Assembler::EQ, known_ok);
2709     } else {
2710       if (UseCompressedClassPointers) {
2711         __ ldrw(rscratch1, dst_klass_addr);
2712         __ cmpw(tmp, rscratch1);
2713       } else {
2714         __ ldr(rscratch1, dst_klass_addr);
2715         __ cmp(tmp, rscratch1);
2716       }
2717       __ br(Assembler::EQ, known_ok);
2718       __ cmp(src, dst);
2719       __ br(Assembler::EQ, known_ok);
2720     }
2721     __ bind(halt);
2722     __ stop(&quot;incorrect type information in arraycopy&quot;);
2723     __ bind(known_ok);
2724   }
2725 #endif
2726 
2727 #ifndef PRODUCT
2728   if (PrintC1Statistics) {
2729     __ incrementw(ExternalAddress(Runtime1::arraycopy_count_address(basic_type)));
2730   }
2731 #endif
2732 
2733   __ lea(c_rarg0, Address(src, src_pos, Address::uxtw(scale)));
2734   __ add(c_rarg0, c_rarg0, arrayOopDesc::base_offset_in_bytes(basic_type));
2735   assert_different_registers(c_rarg0, dst, dst_pos, length);
2736   __ lea(c_rarg1, Address(dst, dst_pos, Address::uxtw(scale)));
2737   __ add(c_rarg1, c_rarg1, arrayOopDesc::base_offset_in_bytes(basic_type));
2738   assert_different_registers(c_rarg1, dst, length);
2739   __ uxtw(c_rarg2, length);
2740   assert_different_registers(c_rarg2, dst);
2741 
2742   bool disjoint = (flags &amp; LIR_OpArrayCopy::overlapping) == 0;
2743   bool aligned = (flags &amp; LIR_OpArrayCopy::unaligned) == 0;
2744   const char *name;
2745   address entry = StubRoutines::select_arraycopy_function(basic_type, aligned, disjoint, name, false);
2746 
2747  CodeBlob *cb = CodeCache::find_blob(entry);
2748  if (cb) {
2749    __ far_call(RuntimeAddress(entry));
2750  } else {
2751    __ call_VM_leaf(entry, 3);
2752  }
2753 
2754   __ bind(*stub-&gt;continuation());
2755 }
2756 
2757 
2758 
2759 
2760 void LIR_Assembler::emit_lock(LIR_OpLock* op) {
2761   Register obj = op-&gt;obj_opr()-&gt;as_register();  // may not be an oop
2762   Register hdr = op-&gt;hdr_opr()-&gt;as_register();
2763   Register lock = op-&gt;lock_opr()-&gt;as_register();
2764   if (!UseFastLocking) {
2765     __ b(*op-&gt;stub()-&gt;entry());
2766   } else if (op-&gt;code() == lir_lock) {
2767     Register scratch = noreg;
2768     if (UseBiasedLocking) {
2769       scratch = op-&gt;scratch_opr()-&gt;as_register();
2770     }
2771     assert(BasicLock::displaced_header_offset_in_bytes() == 0, &quot;lock_reg must point to the displaced header&quot;);
2772     __ resolve(ACCESS_READ | ACCESS_WRITE, obj);
2773     // add debug info for NullPointerException only if one is possible
2774     int null_check_offset = __ lock_object(hdr, obj, lock, scratch, *op-&gt;stub()-&gt;entry());
2775     if (op-&gt;info() != NULL) {
2776       add_debug_info_for_null_check(null_check_offset, op-&gt;info());
2777     }
2778     // done
2779   } else if (op-&gt;code() == lir_unlock) {
2780     assert(BasicLock::displaced_header_offset_in_bytes() == 0, &quot;lock_reg must point to the displaced header&quot;);
2781     __ unlock_object(hdr, obj, lock, *op-&gt;stub()-&gt;entry());
2782   } else {
2783     Unimplemented();
2784   }
2785   __ bind(*op-&gt;stub()-&gt;continuation());
2786 }
2787 
2788 
2789 void LIR_Assembler::emit_profile_call(LIR_OpProfileCall* op) {
2790   ciMethod* method = op-&gt;profiled_method();
2791   int bci          = op-&gt;profiled_bci();
2792   ciMethod* callee = op-&gt;profiled_callee();
2793 
2794   // Update counter for all call types
2795   ciMethodData* md = method-&gt;method_data_or_null();
2796   assert(md != NULL, &quot;Sanity&quot;);
2797   ciProfileData* data = md-&gt;bci_to_data(bci);
2798   assert(data != NULL &amp;&amp; data-&gt;is_CounterData(), &quot;need CounterData for calls&quot;);
2799   assert(op-&gt;mdo()-&gt;is_single_cpu(),  &quot;mdo must be allocated&quot;);
2800   Register mdo  = op-&gt;mdo()-&gt;as_register();
2801   __ mov_metadata(mdo, md-&gt;constant_encoding());
2802   Address counter_addr(mdo, md-&gt;byte_offset_of_slot(data, CounterData::count_offset()));
2803   // Perform additional virtual call profiling for invokevirtual and
2804   // invokeinterface bytecodes
2805   if (op-&gt;should_profile_receiver_type()) {
2806     assert(op-&gt;recv()-&gt;is_single_cpu(), &quot;recv must be allocated&quot;);
2807     Register recv = op-&gt;recv()-&gt;as_register();
2808     assert_different_registers(mdo, recv);
2809     assert(data-&gt;is_VirtualCallData(), &quot;need VirtualCallData for virtual calls&quot;);
2810     ciKlass* known_klass = op-&gt;known_holder();
2811     if (C1OptimizeVirtualCallProfiling &amp;&amp; known_klass != NULL) {
2812       // We know the type that will be seen at this call site; we can
2813       // statically update the MethodData* rather than needing to do
2814       // dynamic tests on the receiver type
2815 
2816       // NOTE: we should probably put a lock around this search to
2817       // avoid collisions by concurrent compilations
2818       ciVirtualCallData* vc_data = (ciVirtualCallData*) data;
2819       uint i;
2820       for (i = 0; i &lt; VirtualCallData::row_limit(); i++) {
2821         ciKlass* receiver = vc_data-&gt;receiver(i);
2822         if (known_klass-&gt;equals(receiver)) {
2823           Address data_addr(mdo, md-&gt;byte_offset_of_slot(data, VirtualCallData::receiver_count_offset(i)));
2824           __ addptr(data_addr, DataLayout::counter_increment);
2825           return;
2826         }
2827       }
2828 
2829       // Receiver type not found in profile data; select an empty slot
2830 
2831       // Note that this is less efficient than it should be because it
2832       // always does a write to the receiver part of the
2833       // VirtualCallData rather than just the first time
2834       for (i = 0; i &lt; VirtualCallData::row_limit(); i++) {
2835         ciKlass* receiver = vc_data-&gt;receiver(i);
2836         if (receiver == NULL) {
2837           Address recv_addr(mdo, md-&gt;byte_offset_of_slot(data, VirtualCallData::receiver_offset(i)));
2838           __ mov_metadata(rscratch1, known_klass-&gt;constant_encoding());
2839           __ lea(rscratch2, recv_addr);
2840           __ str(rscratch1, Address(rscratch2));
2841           Address data_addr(mdo, md-&gt;byte_offset_of_slot(data, VirtualCallData::receiver_count_offset(i)));
2842           __ addptr(data_addr, DataLayout::counter_increment);
2843           return;
2844         }
2845       }
2846     } else {
2847       __ load_klass(recv, recv);
2848       Label update_done;
2849       type_profile_helper(mdo, md, data, recv, &amp;update_done);
2850       // Receiver did not match any saved receiver and there is no empty row for it.
2851       // Increment total counter to indicate polymorphic case.
2852       __ addptr(counter_addr, DataLayout::counter_increment);
2853 
2854       __ bind(update_done);
2855     }
2856   } else {
2857     // Static call
2858     __ addptr(counter_addr, DataLayout::counter_increment);
2859   }
2860 }
2861 
2862 
2863 void LIR_Assembler::emit_delay(LIR_OpDelay*) {
2864   Unimplemented();
2865 }
2866 
2867 
2868 void LIR_Assembler::monitor_address(int monitor_no, LIR_Opr dst) {
2869   __ lea(dst-&gt;as_register(), frame_map()-&gt;address_for_monitor_lock(monitor_no));
2870 }
2871 
2872 void LIR_Assembler::emit_updatecrc32(LIR_OpUpdateCRC32* op) {
2873   assert(op-&gt;crc()-&gt;is_single_cpu(),  &quot;crc must be register&quot;);
2874   assert(op-&gt;val()-&gt;is_single_cpu(),  &quot;byte value must be register&quot;);
2875   assert(op-&gt;result_opr()-&gt;is_single_cpu(), &quot;result must be register&quot;);
2876   Register crc = op-&gt;crc()-&gt;as_register();
2877   Register val = op-&gt;val()-&gt;as_register();
2878   Register res = op-&gt;result_opr()-&gt;as_register();
2879 
2880   assert_different_registers(val, crc, res);
2881   unsigned long offset;
2882   __ adrp(res, ExternalAddress(StubRoutines::crc_table_addr()), offset);
2883   if (offset) __ add(res, res, offset);
2884 
2885   __ mvnw(crc, crc); // ~crc
2886   __ update_byte_crc32(crc, val, res);
2887   __ mvnw(res, crc); // ~crc
2888 }
2889 
2890 void LIR_Assembler::emit_profile_type(LIR_OpProfileType* op) {
2891   COMMENT(&quot;emit_profile_type {&quot;);
2892   Register obj = op-&gt;obj()-&gt;as_register();
2893   Register tmp = op-&gt;tmp()-&gt;as_pointer_register();
2894   Address mdo_addr = as_Address(op-&gt;mdp()-&gt;as_address_ptr());
2895   ciKlass* exact_klass = op-&gt;exact_klass();
2896   intptr_t current_klass = op-&gt;current_klass();
2897   bool not_null = op-&gt;not_null();
2898   bool no_conflict = op-&gt;no_conflict();
2899 
2900   Label update, next, none;
2901 
2902   bool do_null = !not_null;
2903   bool exact_klass_set = exact_klass != NULL &amp;&amp; ciTypeEntries::valid_ciklass(current_klass) == exact_klass;
2904   bool do_update = !TypeEntries::is_type_unknown(current_klass) &amp;&amp; !exact_klass_set;
2905 
2906   assert(do_null || do_update, &quot;why are we here?&quot;);
2907   assert(!TypeEntries::was_null_seen(current_klass) || do_update, &quot;why are we here?&quot;);
2908   assert(mdo_addr.base() != rscratch1, &quot;wrong register&quot;);
2909 
2910   __ verify_oop(obj);
2911 
2912   if (tmp != obj) {
2913     __ mov(tmp, obj);
2914   }
2915   if (do_null) {
2916     __ cbnz(tmp, update);
2917     if (!TypeEntries::was_null_seen(current_klass)) {
2918       __ ldr(rscratch2, mdo_addr);
2919       __ orr(rscratch2, rscratch2, TypeEntries::null_seen);
2920       __ str(rscratch2, mdo_addr);
2921     }
2922     if (do_update) {
2923 #ifndef ASSERT
2924       __ b(next);
2925     }
2926 #else
2927       __ b(next);
2928     }
2929   } else {
2930     __ cbnz(tmp, update);
2931     __ stop(&quot;unexpected null obj&quot;);
2932 #endif
2933   }
2934 
2935   __ bind(update);
2936 
2937   if (do_update) {
2938 #ifdef ASSERT
2939     if (exact_klass != NULL) {
2940       Label ok;
2941       __ load_klass(tmp, tmp);
2942       __ mov_metadata(rscratch1, exact_klass-&gt;constant_encoding());
2943       __ eor(rscratch1, tmp, rscratch1);
2944       __ cbz(rscratch1, ok);
2945       __ stop(&quot;exact klass and actual klass differ&quot;);
2946       __ bind(ok);
2947     }
2948 #endif
2949     if (!no_conflict) {
2950       if (exact_klass == NULL || TypeEntries::is_type_none(current_klass)) {
2951         if (exact_klass != NULL) {
2952           __ mov_metadata(tmp, exact_klass-&gt;constant_encoding());
2953         } else {
2954           __ load_klass(tmp, tmp);
2955         }
2956 
2957         __ ldr(rscratch2, mdo_addr);
2958         __ eor(tmp, tmp, rscratch2);
2959         __ andr(rscratch1, tmp, TypeEntries::type_klass_mask);
2960         // klass seen before, nothing to do. The unknown bit may have been
2961         // set already but no need to check.
2962         __ cbz(rscratch1, next);
2963 
2964         __ tbnz(tmp, exact_log2(TypeEntries::type_unknown), next); // already unknown. Nothing to do anymore.
2965 
2966         if (TypeEntries::is_type_none(current_klass)) {
2967           __ cbz(rscratch2, none);
2968           __ cmp(rscratch2, (u1)TypeEntries::null_seen);
2969           __ br(Assembler::EQ, none);
2970           // There is a chance that the checks above (re-reading profiling
2971           // data from memory) fail if another thread has just set the
2972           // profiling to this obj&#39;s klass
2973           __ dmb(Assembler::ISHLD);
2974           __ ldr(rscratch2, mdo_addr);
2975           __ eor(tmp, tmp, rscratch2);
2976           __ andr(rscratch1, tmp, TypeEntries::type_klass_mask);
2977           __ cbz(rscratch1, next);
2978         }
2979       } else {
2980         assert(ciTypeEntries::valid_ciklass(current_klass) != NULL &amp;&amp;
2981                ciTypeEntries::valid_ciklass(current_klass) != exact_klass, &quot;conflict only&quot;);
2982 
2983         __ ldr(tmp, mdo_addr);
2984         __ tbnz(tmp, exact_log2(TypeEntries::type_unknown), next); // already unknown. Nothing to do anymore.
2985       }
2986 
2987       // different than before. Cannot keep accurate profile.
2988       __ ldr(rscratch2, mdo_addr);
2989       __ orr(rscratch2, rscratch2, TypeEntries::type_unknown);
2990       __ str(rscratch2, mdo_addr);
2991 
2992       if (TypeEntries::is_type_none(current_klass)) {
2993         __ b(next);
2994 
2995         __ bind(none);
2996         // first time here. Set profile type.
2997         __ str(tmp, mdo_addr);
2998       }
2999     } else {
3000       // There&#39;s a single possible klass at this profile point
3001       assert(exact_klass != NULL, &quot;should be&quot;);
3002       if (TypeEntries::is_type_none(current_klass)) {
3003         __ mov_metadata(tmp, exact_klass-&gt;constant_encoding());
3004         __ ldr(rscratch2, mdo_addr);
3005         __ eor(tmp, tmp, rscratch2);
3006         __ andr(rscratch1, tmp, TypeEntries::type_klass_mask);
3007         __ cbz(rscratch1, next);
3008 #ifdef ASSERT
3009         {
3010           Label ok;
3011           __ ldr(rscratch1, mdo_addr);
3012           __ cbz(rscratch1, ok);
3013           __ cmp(rscratch1, (u1)TypeEntries::null_seen);
3014           __ br(Assembler::EQ, ok);
3015           // may have been set by another thread
3016           __ dmb(Assembler::ISHLD);
3017           __ mov_metadata(rscratch1, exact_klass-&gt;constant_encoding());
3018           __ ldr(rscratch2, mdo_addr);
3019           __ eor(rscratch2, rscratch1, rscratch2);
3020           __ andr(rscratch2, rscratch2, TypeEntries::type_mask);
3021           __ cbz(rscratch2, ok);
3022 
3023           __ stop(&quot;unexpected profiling mismatch&quot;);
3024           __ bind(ok);
3025         }
3026 #endif
3027         // first time here. Set profile type.
3028         __ ldr(tmp, mdo_addr);
3029       } else {
3030         assert(ciTypeEntries::valid_ciklass(current_klass) != NULL &amp;&amp;
3031                ciTypeEntries::valid_ciklass(current_klass) != exact_klass, &quot;inconsistent&quot;);
3032 
3033         __ ldr(tmp, mdo_addr);
3034         __ tbnz(tmp, exact_log2(TypeEntries::type_unknown), next); // already unknown. Nothing to do anymore.
3035 
3036         __ orr(tmp, tmp, TypeEntries::type_unknown);
3037         __ str(tmp, mdo_addr);
3038         // FIXME: Write barrier needed here?
3039       }
3040     }
3041 
3042     __ bind(next);
3043   }
3044   COMMENT(&quot;} emit_profile_type&quot;);
3045 }
3046 
3047 
3048 void LIR_Assembler::align_backward_branch_target() {
3049 }
3050 
3051 
3052 void LIR_Assembler::negate(LIR_Opr left, LIR_Opr dest, LIR_Opr tmp) {
3053   // tmp must be unused
3054   assert(tmp-&gt;is_illegal(), &quot;wasting a register if tmp is allocated&quot;);
3055 
3056   if (left-&gt;is_single_cpu()) {
3057     assert(dest-&gt;is_single_cpu(), &quot;expect single result reg&quot;);
3058     __ negw(dest-&gt;as_register(), left-&gt;as_register());
3059   } else if (left-&gt;is_double_cpu()) {
3060     assert(dest-&gt;is_double_cpu(), &quot;expect double result reg&quot;);
3061     __ neg(dest-&gt;as_register_lo(), left-&gt;as_register_lo());
3062   } else if (left-&gt;is_single_fpu()) {
3063     assert(dest-&gt;is_single_fpu(), &quot;expect single float result reg&quot;);
3064     __ fnegs(dest-&gt;as_float_reg(), left-&gt;as_float_reg());
3065   } else {
3066     assert(left-&gt;is_double_fpu(), &quot;expect double float operand reg&quot;);
3067     assert(dest-&gt;is_double_fpu(), &quot;expect double float result reg&quot;);
3068     __ fnegd(dest-&gt;as_double_reg(), left-&gt;as_double_reg());
3069   }
3070 }
3071 
3072 
3073 void LIR_Assembler::leal(LIR_Opr addr, LIR_Opr dest, LIR_PatchCode patch_code, CodeEmitInfo* info) {
3074   if (patch_code != lir_patch_none) {
3075     deoptimize_trap(info);
3076     return;
3077   }
3078 
3079   __ lea(dest-&gt;as_register_lo(), as_Address(addr-&gt;as_address_ptr()));
3080 }
3081 
3082 
3083 void LIR_Assembler::rt_call(LIR_Opr result, address dest, const LIR_OprList* args, LIR_Opr tmp, CodeEmitInfo* info) {
3084   assert(!tmp-&gt;is_valid(), &quot;don&#39;t need temporary&quot;);
3085 
3086   CodeBlob *cb = CodeCache::find_blob(dest);
3087   if (cb) {
3088     __ far_call(RuntimeAddress(dest));
3089   } else {
3090     __ mov(rscratch1, RuntimeAddress(dest));
3091     __ blr(rscratch1);
3092   }
3093 
3094   if (info != NULL) {
3095     add_call_info_here(info);
3096   }
3097   __ maybe_isb();
3098 }
3099 
3100 void LIR_Assembler::volatile_move_op(LIR_Opr src, LIR_Opr dest, BasicType type, CodeEmitInfo* info) {
3101   if (dest-&gt;is_address() || src-&gt;is_address()) {
3102     move_op(src, dest, type, lir_patch_none, info,
3103             /*pop_fpu_stack*/false, /*unaligned*/false, /*wide*/false);
3104   } else {
3105     ShouldNotReachHere();
3106   }
3107 }
3108 
3109 #ifdef ASSERT
3110 // emit run-time assertion
3111 void LIR_Assembler::emit_assert(LIR_OpAssert* op) {
3112   assert(op-&gt;code() == lir_assert, &quot;must be&quot;);
3113 
3114   if (op-&gt;in_opr1()-&gt;is_valid()) {
3115     assert(op-&gt;in_opr2()-&gt;is_valid(), &quot;both operands must be valid&quot;);
3116     comp_op(op-&gt;condition(), op-&gt;in_opr1(), op-&gt;in_opr2(), op);
3117   } else {
3118     assert(op-&gt;in_opr2()-&gt;is_illegal(), &quot;both operands must be illegal&quot;);
3119     assert(op-&gt;condition() == lir_cond_always, &quot;no other conditions allowed&quot;);
3120   }
3121 
3122   Label ok;
3123   if (op-&gt;condition() != lir_cond_always) {
3124     Assembler::Condition acond = Assembler::AL;
3125     switch (op-&gt;condition()) {
3126       case lir_cond_equal:        acond = Assembler::EQ;  break;
3127       case lir_cond_notEqual:     acond = Assembler::NE;  break;
3128       case lir_cond_less:         acond = Assembler::LT;  break;
3129       case lir_cond_lessEqual:    acond = Assembler::LE;  break;
3130       case lir_cond_greaterEqual: acond = Assembler::GE;  break;
3131       case lir_cond_greater:      acond = Assembler::GT;  break;
3132       case lir_cond_belowEqual:   acond = Assembler::LS;  break;
3133       case lir_cond_aboveEqual:   acond = Assembler::HS;  break;
3134       default:                    ShouldNotReachHere();
3135     }
3136     __ br(acond, ok);
3137   }
3138   if (op-&gt;halt()) {
3139     const char* str = __ code_string(op-&gt;msg());
3140     __ stop(str);
3141   } else {
3142     breakpoint();
3143   }
3144   __ bind(ok);
3145 }
3146 #endif
3147 
3148 #ifndef PRODUCT
3149 #define COMMENT(x)   do { __ block_comment(x); } while (0)
3150 #else
3151 #define COMMENT(x)
3152 #endif
3153 
3154 void LIR_Assembler::membar() {
3155   COMMENT(&quot;membar&quot;);
3156   __ membar(MacroAssembler::AnyAny);
3157 }
3158 
3159 void LIR_Assembler::membar_acquire() {
3160   __ membar(Assembler::LoadLoad|Assembler::LoadStore);
3161 }
3162 
3163 void LIR_Assembler::membar_release() {
3164   __ membar(Assembler::LoadStore|Assembler::StoreStore);
3165 }
3166 
3167 void LIR_Assembler::membar_loadload() {
3168   __ membar(Assembler::LoadLoad);
3169 }
3170 
3171 void LIR_Assembler::membar_storestore() {
3172   __ membar(MacroAssembler::StoreStore);
3173 }
3174 
3175 void LIR_Assembler::membar_loadstore() { __ membar(MacroAssembler::LoadStore); }
3176 
3177 void LIR_Assembler::membar_storeload() { __ membar(MacroAssembler::StoreLoad); }
3178 
3179 void LIR_Assembler::on_spin_wait() {
3180   Unimplemented();
3181 }
3182 
3183 void LIR_Assembler::get_thread(LIR_Opr result_reg) {
3184   __ mov(result_reg-&gt;as_register(), rthread);
3185 }
3186 
3187 
3188 void LIR_Assembler::peephole(LIR_List *lir) {
3189 #if 0
3190   if (tableswitch_count &gt;= max_tableswitches)
3191     return;
3192 
3193   /*
3194     This finite-state automaton recognizes sequences of compare-and-
3195     branch instructions.  We will turn them into a tableswitch.  You
3196     could argue that C1 really shouldn&#39;t be doing this sort of
3197     optimization, but without it the code is really horrible.
3198   */
3199 
3200   enum { start_s, cmp1_s, beq_s, cmp_s } state;
3201   int first_key, last_key = -2147483648;
3202   int next_key = 0;
3203   int start_insn = -1;
3204   int last_insn = -1;
3205   Register reg = noreg;
3206   LIR_Opr reg_opr;
3207   state = start_s;
3208 
3209   LIR_OpList* inst = lir-&gt;instructions_list();
3210   for (int i = 0; i &lt; inst-&gt;length(); i++) {
3211     LIR_Op* op = inst-&gt;at(i);
3212     switch (state) {
3213     case start_s:
3214       first_key = -1;
3215       start_insn = i;
3216       switch (op-&gt;code()) {
3217       case lir_cmp:
3218         LIR_Opr opr1 = op-&gt;as_Op2()-&gt;in_opr1();
3219         LIR_Opr opr2 = op-&gt;as_Op2()-&gt;in_opr2();
3220         if (opr1-&gt;is_cpu_register() &amp;&amp; opr1-&gt;is_single_cpu()
3221             &amp;&amp; opr2-&gt;is_constant()
3222             &amp;&amp; opr2-&gt;type() == T_INT) {
3223           reg_opr = opr1;
3224           reg = opr1-&gt;as_register();
3225           first_key = opr2-&gt;as_constant_ptr()-&gt;as_jint();
3226           next_key = first_key + 1;
3227           state = cmp_s;
3228           goto next_state;
3229         }
3230         break;
3231       }
3232       break;
3233     case cmp_s:
3234       switch (op-&gt;code()) {
3235       case lir_branch:
3236         if (op-&gt;as_OpBranch()-&gt;cond() == lir_cond_equal) {
3237           state = beq_s;
3238           last_insn = i;
3239           goto next_state;
3240         }
3241       }
3242       state = start_s;
3243       break;
3244     case beq_s:
3245       switch (op-&gt;code()) {
3246       case lir_cmp: {
3247         LIR_Opr opr1 = op-&gt;as_Op2()-&gt;in_opr1();
3248         LIR_Opr opr2 = op-&gt;as_Op2()-&gt;in_opr2();
3249         if (opr1-&gt;is_cpu_register() &amp;&amp; opr1-&gt;is_single_cpu()
3250             &amp;&amp; opr1-&gt;as_register() == reg
3251             &amp;&amp; opr2-&gt;is_constant()
3252             &amp;&amp; opr2-&gt;type() == T_INT
3253             &amp;&amp; opr2-&gt;as_constant_ptr()-&gt;as_jint() == next_key) {
3254           last_key = next_key;
3255           next_key++;
3256           state = cmp_s;
3257           goto next_state;
3258         }
3259       }
3260       }
3261       last_key = next_key;
3262       state = start_s;
3263       break;
3264     default:
3265       assert(false, &quot;impossible state&quot;);
3266     }
3267     if (state == start_s) {
3268       if (first_key &lt; last_key - 5L &amp;&amp; reg != noreg) {
3269         {
3270           // printf(&quot;found run register %d starting at insn %d low value %d high value %d\n&quot;,
3271           //        reg-&gt;encoding(),
3272           //        start_insn, first_key, last_key);
3273           //   for (int i = 0; i &lt; inst-&gt;length(); i++) {
3274           //     inst-&gt;at(i)-&gt;print();
3275           //     tty-&gt;print(&quot;\n&quot;);
3276           //   }
3277           //   tty-&gt;print(&quot;\n&quot;);
3278         }
3279 
3280         struct tableswitch *sw = &amp;switches[tableswitch_count];
3281         sw-&gt;_insn_index = start_insn, sw-&gt;_first_key = first_key,
3282           sw-&gt;_last_key = last_key, sw-&gt;_reg = reg;
3283         inst-&gt;insert_before(last_insn + 1, new LIR_OpLabel(&amp;sw-&gt;_after));
3284         {
3285           // Insert the new table of branches
3286           int offset = last_insn;
3287           for (int n = first_key; n &lt; last_key; n++) {
3288             inst-&gt;insert_before
3289               (last_insn + 1,
3290                new LIR_OpBranch(lir_cond_always, T_ILLEGAL,
3291                                 inst-&gt;at(offset)-&gt;as_OpBranch()-&gt;label()));
3292             offset -= 2, i++;
3293           }
3294         }
3295         // Delete all the old compare-and-branch instructions
3296         for (int n = first_key; n &lt; last_key; n++) {
3297           inst-&gt;remove_at(start_insn);
3298           inst-&gt;remove_at(start_insn);
3299         }
3300         // Insert the tableswitch instruction
3301         inst-&gt;insert_before(start_insn,
3302                             new LIR_Op2(lir_cmp, lir_cond_always,
3303                                         LIR_OprFact::intConst(tableswitch_count),
3304                                         reg_opr));
3305         inst-&gt;insert_before(start_insn + 1, new LIR_OpLabel(&amp;sw-&gt;_branches));
3306         tableswitch_count++;
3307       }
3308       reg = noreg;
3309       last_key = -2147483648;
3310     }
3311   next_state:
3312     ;
3313   }
3314 #endif
3315 }
3316 
3317 void LIR_Assembler::atomic_op(LIR_Code code, LIR_Opr src, LIR_Opr data, LIR_Opr dest, LIR_Opr tmp_op) {
3318   Address addr = as_Address(src-&gt;as_address_ptr());
3319   BasicType type = src-&gt;type();
3320   bool is_oop = is_reference_type(type);
3321 
3322   void (MacroAssembler::* add)(Register prev, RegisterOrConstant incr, Register addr);
3323   void (MacroAssembler::* xchg)(Register prev, Register newv, Register addr);
3324 
3325   switch(type) {
3326   case T_INT:
3327     xchg = &amp;MacroAssembler::atomic_xchgalw;
3328     add = &amp;MacroAssembler::atomic_addalw;
3329     break;
3330   case T_LONG:
3331     xchg = &amp;MacroAssembler::atomic_xchgal;
3332     add = &amp;MacroAssembler::atomic_addal;
3333     break;
3334   case T_VALUETYPE:
3335   case T_OBJECT:
3336   case T_ARRAY:
3337     if (UseCompressedOops) {
3338       xchg = &amp;MacroAssembler::atomic_xchgalw;
3339       add = &amp;MacroAssembler::atomic_addalw;
3340     } else {
3341       xchg = &amp;MacroAssembler::atomic_xchgal;
3342       add = &amp;MacroAssembler::atomic_addal;
3343     }
3344     break;
3345   default:
3346     ShouldNotReachHere();
3347     xchg = &amp;MacroAssembler::atomic_xchgal;
3348     add = &amp;MacroAssembler::atomic_addal; // unreachable
3349   }
3350 
3351   switch (code) {
3352   case lir_xadd:
3353     {
3354       RegisterOrConstant inc;
3355       Register tmp = as_reg(tmp_op);
3356       Register dst = as_reg(dest);
3357       if (data-&gt;is_constant()) {
3358         inc = RegisterOrConstant(as_long(data));
3359         assert_different_registers(dst, addr.base(), tmp,
3360                                    rscratch1, rscratch2);
3361       } else {
3362         inc = RegisterOrConstant(as_reg(data));
3363         assert_different_registers(inc.as_register(), dst, addr.base(), tmp,
3364                                    rscratch1, rscratch2);
3365       }
3366       __ lea(tmp, addr);
3367       (_masm-&gt;*add)(dst, inc, tmp);
3368       break;
3369     }
3370   case lir_xchg:
3371     {
3372       Register tmp = tmp_op-&gt;as_register();
3373       Register obj = as_reg(data);
3374       Register dst = as_reg(dest);
3375       if (is_oop &amp;&amp; UseCompressedOops) {
3376         __ encode_heap_oop(rscratch2, obj);
3377         obj = rscratch2;
3378       }
3379       assert_different_registers(obj, addr.base(), tmp, rscratch1, dst);
3380       __ lea(tmp, addr);
3381       (_masm-&gt;*xchg)(dst, obj, tmp);
3382       if (is_oop &amp;&amp; UseCompressedOops) {
3383         __ decode_heap_oop(dst);
3384       }
3385     }
3386     break;
3387   default:
3388     ShouldNotReachHere();
3389   }
3390   __ membar(__ AnyAny);
3391 }
3392 
3393 #undef __
    </pre>
  </body>
</html>