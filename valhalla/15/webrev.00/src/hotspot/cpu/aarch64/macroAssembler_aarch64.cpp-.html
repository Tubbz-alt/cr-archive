<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Old src/hotspot/cpu/aarch64/macroAssembler_aarch64.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
  <body>
    <pre>
   1 /*
   2  * Copyright (c) 1997, 2020, Oracle and/or its affiliates. All rights reserved.
   3  * Copyright (c) 2014, 2020, Red Hat Inc. All rights reserved.
   4  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   5  *
   6  * This code is free software; you can redistribute it and/or modify it
   7  * under the terms of the GNU General Public License version 2 only, as
   8  * published by the Free Software Foundation.
   9  *
  10  * This code is distributed in the hope that it will be useful, but WITHOUT
  11  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  12  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  13  * version 2 for more details (a copy is included in the LICENSE file that
  14  * accompanied this code).
  15  *
  16  * You should have received a copy of the GNU General Public License version
  17  * 2 along with this work; if not, write to the Free Software Foundation,
  18  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  19  *
  20  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  21  * or visit www.oracle.com if you need additional information or have any
  22  * questions.
  23  *
  24  */
  25 
  26 #include &lt;sys/types.h&gt;
  27 
  28 #include &quot;precompiled.hpp&quot;
  29 #include &quot;jvm.h&quot;
  30 #include &quot;asm/assembler.hpp&quot;
  31 #include &quot;asm/assembler.inline.hpp&quot;
  32 #include &quot;gc/shared/barrierSet.hpp&quot;
  33 #include &quot;gc/shared/cardTable.hpp&quot;
  34 #include &quot;gc/shared/barrierSetAssembler.hpp&quot;
  35 #include &quot;gc/shared/cardTableBarrierSet.hpp&quot;
  36 #include &quot;interpreter/interpreter.hpp&quot;
  37 #include &quot;compiler/disassembler.hpp&quot;
  38 #include &quot;memory/resourceArea.hpp&quot;
  39 #include &quot;memory/universe.hpp&quot;
  40 #include &quot;nativeInst_aarch64.hpp&quot;
  41 #include &quot;oops/accessDecorators.hpp&quot;
  42 #include &quot;oops/compressedOops.inline.hpp&quot;
  43 #include &quot;oops/klass.inline.hpp&quot;
  44 #include &quot;runtime/biasedLocking.hpp&quot;
  45 #include &quot;runtime/icache.hpp&quot;
  46 #include &quot;runtime/interfaceSupport.inline.hpp&quot;
  47 #include &quot;runtime/jniHandles.inline.hpp&quot;
  48 #include &quot;runtime/sharedRuntime.hpp&quot;
  49 #include &quot;runtime/signature_cc.hpp&quot;
  50 #include &quot;runtime/thread.hpp&quot;
  51 #include &quot;utilities/powerOfTwo.hpp&quot;
  52 #ifdef COMPILER1
  53 #include &quot;c1/c1_LIRAssembler.hpp&quot;
  54 #endif
  55 #ifdef COMPILER2
  56 #include &quot;oops/oop.hpp&quot;
  57 #include &quot;opto/compile.hpp&quot;
  58 #include &quot;opto/intrinsicnode.hpp&quot;
  59 #include &quot;opto/node.hpp&quot;
  60 #endif
  61 
  62 #ifdef PRODUCT
  63 #define BLOCK_COMMENT(str) /* nothing */
  64 #define STOP(error) stop(error)
  65 #else
  66 #define BLOCK_COMMENT(str) block_comment(str)
  67 #define STOP(error) block_comment(error); stop(error)
  68 #endif
  69 
  70 #define BIND(label) bind(label); BLOCK_COMMENT(#label &quot;:&quot;)
  71 
  72 // Patch any kind of instruction; there may be several instructions.
  73 // Return the total length (in bytes) of the instructions.
  74 int MacroAssembler::pd_patch_instruction_size(address branch, address target) {
  75   int instructions = 1;
  76   assert((uint64_t)target &lt; (1ul &lt;&lt; 48), &quot;48-bit overflow in address constant&quot;);
  77   long offset = (target - branch) &gt;&gt; 2;
  78   unsigned insn = *(unsigned*)branch;
  79   if ((Instruction_aarch64::extract(insn, 29, 24) &amp; 0b111011) == 0b011000) {
  80     // Load register (literal)
  81     Instruction_aarch64::spatch(branch, 23, 5, offset);
  82   } else if (Instruction_aarch64::extract(insn, 30, 26) == 0b00101) {
  83     // Unconditional branch (immediate)
  84     Instruction_aarch64::spatch(branch, 25, 0, offset);
  85   } else if (Instruction_aarch64::extract(insn, 31, 25) == 0b0101010) {
  86     // Conditional branch (immediate)
  87     Instruction_aarch64::spatch(branch, 23, 5, offset);
  88   } else if (Instruction_aarch64::extract(insn, 30, 25) == 0b011010) {
  89     // Compare &amp; branch (immediate)
  90     Instruction_aarch64::spatch(branch, 23, 5, offset);
  91   } else if (Instruction_aarch64::extract(insn, 30, 25) == 0b011011) {
  92     // Test &amp; branch (immediate)
  93     Instruction_aarch64::spatch(branch, 18, 5, offset);
  94   } else if (Instruction_aarch64::extract(insn, 28, 24) == 0b10000) {
  95     // PC-rel. addressing
  96     offset = target-branch;
  97     int shift = Instruction_aarch64::extract(insn, 31, 31);
  98     if (shift) {
  99       u_int64_t dest = (u_int64_t)target;
 100       uint64_t pc_page = (uint64_t)branch &gt;&gt; 12;
 101       uint64_t adr_page = (uint64_t)target &gt;&gt; 12;
 102       unsigned offset_lo = dest &amp; 0xfff;
 103       offset = adr_page - pc_page;
 104 
 105       // We handle 4 types of PC relative addressing
 106       //   1 - adrp    Rx, target_page
 107       //       ldr/str Ry, [Rx, #offset_in_page]
 108       //   2 - adrp    Rx, target_page
 109       //       add     Ry, Rx, #offset_in_page
 110       //   3 - adrp    Rx, target_page (page aligned reloc, offset == 0)
 111       //       movk    Rx, #imm16&lt;&lt;32
 112       //   4 - adrp    Rx, target_page (page aligned reloc, offset == 0)
 113       // In the first 3 cases we must check that Rx is the same in the adrp and the
 114       // subsequent ldr/str, add or movk instruction. Otherwise we could accidentally end
 115       // up treating a type 4 relocation as a type 1, 2 or 3 just because it happened
 116       // to be followed by a random unrelated ldr/str, add or movk instruction.
 117       //
 118       unsigned insn2 = ((unsigned*)branch)[1];
 119       if (Instruction_aarch64::extract(insn2, 29, 24) == 0b111001 &amp;&amp;
 120                 Instruction_aarch64::extract(insn, 4, 0) ==
 121                         Instruction_aarch64::extract(insn2, 9, 5)) {
 122         // Load/store register (unsigned immediate)
 123         unsigned size = Instruction_aarch64::extract(insn2, 31, 30);
 124         Instruction_aarch64::patch(branch + sizeof (unsigned),
 125                                     21, 10, offset_lo &gt;&gt; size);
 126         guarantee(((dest &gt;&gt; size) &lt;&lt; size) == dest, &quot;misaligned target&quot;);
 127         instructions = 2;
 128       } else if (Instruction_aarch64::extract(insn2, 31, 22) == 0b1001000100 &amp;&amp;
 129                 Instruction_aarch64::extract(insn, 4, 0) ==
 130                         Instruction_aarch64::extract(insn2, 4, 0)) {
 131         // add (immediate)
 132         Instruction_aarch64::patch(branch + sizeof (unsigned),
 133                                    21, 10, offset_lo);
 134         instructions = 2;
 135       } else if (Instruction_aarch64::extract(insn2, 31, 21) == 0b11110010110 &amp;&amp;
 136                    Instruction_aarch64::extract(insn, 4, 0) ==
 137                      Instruction_aarch64::extract(insn2, 4, 0)) {
 138         // movk #imm16&lt;&lt;32
 139         Instruction_aarch64::patch(branch + 4, 20, 5, (uint64_t)target &gt;&gt; 32);
 140         long dest = ((long)target &amp; 0xffffffffL) | ((long)branch &amp; 0xffff00000000L);
 141         long pc_page = (long)branch &gt;&gt; 12;
 142         long adr_page = (long)dest &gt;&gt; 12;
 143         offset = adr_page - pc_page;
 144         instructions = 2;
 145       }
 146     }
 147     int offset_lo = offset &amp; 3;
 148     offset &gt;&gt;= 2;
 149     Instruction_aarch64::spatch(branch, 23, 5, offset);
 150     Instruction_aarch64::patch(branch, 30, 29, offset_lo);
 151   } else if (Instruction_aarch64::extract(insn, 31, 21) == 0b11010010100) {
 152     u_int64_t dest = (u_int64_t)target;
 153     // Move wide constant
 154     assert(nativeInstruction_at(branch+4)-&gt;is_movk(), &quot;wrong insns in patch&quot;);
 155     assert(nativeInstruction_at(branch+8)-&gt;is_movk(), &quot;wrong insns in patch&quot;);
 156     Instruction_aarch64::patch(branch, 20, 5, dest &amp; 0xffff);
 157     Instruction_aarch64::patch(branch+4, 20, 5, (dest &gt;&gt;= 16) &amp; 0xffff);
 158     Instruction_aarch64::patch(branch+8, 20, 5, (dest &gt;&gt;= 16) &amp; 0xffff);
 159     assert(target_addr_for_insn(branch) == target, &quot;should be&quot;);
 160     instructions = 3;
 161   } else if (Instruction_aarch64::extract(insn, 31, 22) == 0b1011100101 &amp;&amp;
 162              Instruction_aarch64::extract(insn, 4, 0) == 0b11111) {
 163     // nothing to do
 164     assert(target == 0, &quot;did not expect to relocate target for polling page load&quot;);
 165   } else {
 166     ShouldNotReachHere();
 167   }
 168   return instructions * NativeInstruction::instruction_size;
 169 }
 170 
 171 int MacroAssembler::patch_oop(address insn_addr, address o) {
 172   int instructions;
 173   unsigned insn = *(unsigned*)insn_addr;
 174   assert(nativeInstruction_at(insn_addr+4)-&gt;is_movk(), &quot;wrong insns in patch&quot;);
 175 
 176   // OOPs are either narrow (32 bits) or wide (48 bits).  We encode
 177   // narrow OOPs by setting the upper 16 bits in the first
 178   // instruction.
 179   if (Instruction_aarch64::extract(insn, 31, 21) == 0b11010010101) {
 180     // Move narrow OOP
 181     narrowOop n = CompressedOops::encode((oop)o);
 182     Instruction_aarch64::patch(insn_addr, 20, 5, n &gt;&gt; 16);
 183     Instruction_aarch64::patch(insn_addr+4, 20, 5, n &amp; 0xffff);
 184     instructions = 2;
 185   } else {
 186     // Move wide OOP
 187     assert(nativeInstruction_at(insn_addr+8)-&gt;is_movk(), &quot;wrong insns in patch&quot;);
 188     uintptr_t dest = (uintptr_t)o;
 189     Instruction_aarch64::patch(insn_addr, 20, 5, dest &amp; 0xffff);
 190     Instruction_aarch64::patch(insn_addr+4, 20, 5, (dest &gt;&gt;= 16) &amp; 0xffff);
 191     Instruction_aarch64::patch(insn_addr+8, 20, 5, (dest &gt;&gt;= 16) &amp; 0xffff);
 192     instructions = 3;
 193   }
 194   return instructions * NativeInstruction::instruction_size;
 195 }
 196 
 197 int MacroAssembler::patch_narrow_klass(address insn_addr, narrowKlass n) {
 198   // Metatdata pointers are either narrow (32 bits) or wide (48 bits).
 199   // We encode narrow ones by setting the upper 16 bits in the first
 200   // instruction.
 201   NativeInstruction *insn = nativeInstruction_at(insn_addr);
 202   assert(Instruction_aarch64::extract(insn-&gt;encoding(), 31, 21) == 0b11010010101 &amp;&amp;
 203          nativeInstruction_at(insn_addr+4)-&gt;is_movk(), &quot;wrong insns in patch&quot;);
 204 
 205   Instruction_aarch64::patch(insn_addr, 20, 5, n &gt;&gt; 16);
 206   Instruction_aarch64::patch(insn_addr+4, 20, 5, n &amp; 0xffff);
 207   return 2 * NativeInstruction::instruction_size;
 208 }
 209 
 210 address MacroAssembler::target_addr_for_insn(address insn_addr, unsigned insn) {
 211   long offset = 0;
 212   if ((Instruction_aarch64::extract(insn, 29, 24) &amp; 0b011011) == 0b00011000) {
 213     // Load register (literal)
 214     offset = Instruction_aarch64::sextract(insn, 23, 5);
 215     return address(((uint64_t)insn_addr + (offset &lt;&lt; 2)));
 216   } else if (Instruction_aarch64::extract(insn, 30, 26) == 0b00101) {
 217     // Unconditional branch (immediate)
 218     offset = Instruction_aarch64::sextract(insn, 25, 0);
 219   } else if (Instruction_aarch64::extract(insn, 31, 25) == 0b0101010) {
 220     // Conditional branch (immediate)
 221     offset = Instruction_aarch64::sextract(insn, 23, 5);
 222   } else if (Instruction_aarch64::extract(insn, 30, 25) == 0b011010) {
 223     // Compare &amp; branch (immediate)
 224     offset = Instruction_aarch64::sextract(insn, 23, 5);
 225    } else if (Instruction_aarch64::extract(insn, 30, 25) == 0b011011) {
 226     // Test &amp; branch (immediate)
 227     offset = Instruction_aarch64::sextract(insn, 18, 5);
 228   } else if (Instruction_aarch64::extract(insn, 28, 24) == 0b10000) {
 229     // PC-rel. addressing
 230     offset = Instruction_aarch64::extract(insn, 30, 29);
 231     offset |= Instruction_aarch64::sextract(insn, 23, 5) &lt;&lt; 2;
 232     int shift = Instruction_aarch64::extract(insn, 31, 31) ? 12 : 0;
 233     if (shift) {
 234       offset &lt;&lt;= shift;
 235       uint64_t target_page = ((uint64_t)insn_addr) + offset;
 236       target_page &amp;= ((uint64_t)-1) &lt;&lt; shift;
 237       // Return the target address for the following sequences
 238       //   1 - adrp    Rx, target_page
 239       //       ldr/str Ry, [Rx, #offset_in_page]
 240       //   2 - adrp    Rx, target_page
 241       //       add     Ry, Rx, #offset_in_page
 242       //   3 - adrp    Rx, target_page (page aligned reloc, offset == 0)
 243       //       movk    Rx, #imm12&lt;&lt;32
 244       //   4 - adrp    Rx, target_page (page aligned reloc, offset == 0)
 245       //
 246       // In the first two cases  we check that the register is the same and
 247       // return the target_page + the offset within the page.
 248       // Otherwise we assume it is a page aligned relocation and return
 249       // the target page only.
 250       //
 251       unsigned insn2 = ((unsigned*)insn_addr)[1];
 252       if (Instruction_aarch64::extract(insn2, 29, 24) == 0b111001 &amp;&amp;
 253                 Instruction_aarch64::extract(insn, 4, 0) ==
 254                         Instruction_aarch64::extract(insn2, 9, 5)) {
 255         // Load/store register (unsigned immediate)
 256         unsigned int byte_offset = Instruction_aarch64::extract(insn2, 21, 10);
 257         unsigned int size = Instruction_aarch64::extract(insn2, 31, 30);
 258         return address(target_page + (byte_offset &lt;&lt; size));
 259       } else if (Instruction_aarch64::extract(insn2, 31, 22) == 0b1001000100 &amp;&amp;
 260                 Instruction_aarch64::extract(insn, 4, 0) ==
 261                         Instruction_aarch64::extract(insn2, 4, 0)) {
 262         // add (immediate)
 263         unsigned int byte_offset = Instruction_aarch64::extract(insn2, 21, 10);
 264         return address(target_page + byte_offset);
 265       } else {
 266         if (Instruction_aarch64::extract(insn2, 31, 21) == 0b11110010110  &amp;&amp;
 267                Instruction_aarch64::extract(insn, 4, 0) ==
 268                  Instruction_aarch64::extract(insn2, 4, 0)) {
 269           target_page = (target_page &amp; 0xffffffff) |
 270                          ((uint64_t)Instruction_aarch64::extract(insn2, 20, 5) &lt;&lt; 32);
 271         }
 272         return (address)target_page;
 273       }
 274     } else {
 275       ShouldNotReachHere();
 276     }
 277   } else if (Instruction_aarch64::extract(insn, 31, 23) == 0b110100101) {
 278     u_int32_t *insns = (u_int32_t *)insn_addr;
 279     // Move wide constant: movz, movk, movk.  See movptr().
 280     assert(nativeInstruction_at(insns+1)-&gt;is_movk(), &quot;wrong insns in patch&quot;);
 281     assert(nativeInstruction_at(insns+2)-&gt;is_movk(), &quot;wrong insns in patch&quot;);
 282     return address(u_int64_t(Instruction_aarch64::extract(insns[0], 20, 5))
 283                    + (u_int64_t(Instruction_aarch64::extract(insns[1], 20, 5)) &lt;&lt; 16)
 284                    + (u_int64_t(Instruction_aarch64::extract(insns[2], 20, 5)) &lt;&lt; 32));
 285   } else if (Instruction_aarch64::extract(insn, 31, 22) == 0b1011100101 &amp;&amp;
 286              Instruction_aarch64::extract(insn, 4, 0) == 0b11111) {
 287     return 0;
 288   } else {
 289     ShouldNotReachHere();
 290   }
 291   return address(((uint64_t)insn_addr + (offset &lt;&lt; 2)));
 292 }
 293 
 294 void MacroAssembler::safepoint_poll(Label&amp; slow_path) {
 295   if (SafepointMechanism::uses_thread_local_poll()) {
 296     ldr(rscratch1, Address(rthread, Thread::polling_page_offset()));
 297     tbnz(rscratch1, exact_log2(SafepointMechanism::poll_bit()), slow_path);
 298   } else {
 299     unsigned long offset;
 300     adrp(rscratch1, ExternalAddress(SafepointSynchronize::address_of_state()), offset);
 301     ldrw(rscratch1, Address(rscratch1, offset));
 302     assert(SafepointSynchronize::_not_synchronized == 0, &quot;rewrite this code&quot;);
 303     cbnz(rscratch1, slow_path);
 304   }
 305 }
 306 
 307 // Just like safepoint_poll, but use an acquiring load for thread-
 308 // local polling.
 309 //
 310 // We need an acquire here to ensure that any subsequent load of the
 311 // global SafepointSynchronize::_state flag is ordered after this load
 312 // of the local Thread::_polling page.  We don&#39;t want this poll to
 313 // return false (i.e. not safepointing) and a later poll of the global
 314 // SafepointSynchronize::_state spuriously to return true.
 315 //
 316 // This is to avoid a race when we&#39;re in a native-&gt;Java transition
 317 // racing the code which wakes up from a safepoint.
 318 //
 319 void MacroAssembler::safepoint_poll_acquire(Label&amp; slow_path) {
 320   if (SafepointMechanism::uses_thread_local_poll()) {
 321     lea(rscratch1, Address(rthread, Thread::polling_page_offset()));
 322     ldar(rscratch1, rscratch1);
 323     tbnz(rscratch1, exact_log2(SafepointMechanism::poll_bit()), slow_path);
 324   } else {
 325     safepoint_poll(slow_path);
 326   }
 327 }
 328 
 329 void MacroAssembler::reset_last_Java_frame(bool clear_fp) {
 330   // we must set sp to zero to clear frame
 331   str(zr, Address(rthread, JavaThread::last_Java_sp_offset()));
 332 
 333   // must clear fp, so that compiled frames are not confused; it is
 334   // possible that we need it only for debugging
 335   if (clear_fp) {
 336     str(zr, Address(rthread, JavaThread::last_Java_fp_offset()));
 337   }
 338 
 339   // Always clear the pc because it could have been set by make_walkable()
 340   str(zr, Address(rthread, JavaThread::last_Java_pc_offset()));
 341 }
 342 
 343 // Calls to C land
 344 //
 345 // When entering C land, the rfp, &amp; resp of the last Java frame have to be recorded
 346 // in the (thread-local) JavaThread object. When leaving C land, the last Java fp
 347 // has to be reset to 0. This is required to allow proper stack traversal.
 348 void MacroAssembler::set_last_Java_frame(Register last_java_sp,
 349                                          Register last_java_fp,
 350                                          Register last_java_pc,
 351                                          Register scratch) {
 352 
 353   if (last_java_pc-&gt;is_valid()) {
 354       str(last_java_pc, Address(rthread,
 355                                 JavaThread::frame_anchor_offset()
 356                                 + JavaFrameAnchor::last_Java_pc_offset()));
 357     }
 358 
 359   // determine last_java_sp register
 360   if (last_java_sp == sp) {
 361     mov(scratch, sp);
 362     last_java_sp = scratch;
 363   } else if (!last_java_sp-&gt;is_valid()) {
 364     last_java_sp = esp;
 365   }
 366 
 367   str(last_java_sp, Address(rthread, JavaThread::last_Java_sp_offset()));
 368 
 369   // last_java_fp is optional
 370   if (last_java_fp-&gt;is_valid()) {
 371     str(last_java_fp, Address(rthread, JavaThread::last_Java_fp_offset()));
 372   }
 373 }
 374 
 375 void MacroAssembler::set_last_Java_frame(Register last_java_sp,
 376                                          Register last_java_fp,
 377                                          address  last_java_pc,
 378                                          Register scratch) {
 379   assert(last_java_pc != NULL, &quot;must provide a valid PC&quot;);
 380 
 381   adr(scratch, last_java_pc);
 382   str(scratch, Address(rthread,
 383                        JavaThread::frame_anchor_offset()
 384                        + JavaFrameAnchor::last_Java_pc_offset()));
 385 
 386   set_last_Java_frame(last_java_sp, last_java_fp, noreg, scratch);
 387 }
 388 
 389 void MacroAssembler::set_last_Java_frame(Register last_java_sp,
 390                                          Register last_java_fp,
 391                                          Label &amp;L,
 392                                          Register scratch) {
 393   if (L.is_bound()) {
 394     set_last_Java_frame(last_java_sp, last_java_fp, target(L), scratch);
 395   } else {
 396     InstructionMark im(this);
 397     L.add_patch_at(code(), locator());
 398     set_last_Java_frame(last_java_sp, last_java_fp, pc() /* Patched later */, scratch);
 399   }
 400 }
 401 
 402 void MacroAssembler::far_call(Address entry, CodeBuffer *cbuf, Register tmp) {
 403   assert(ReservedCodeCacheSize &lt; 4*G, &quot;branch out of range&quot;);
 404   assert(CodeCache::find_blob(entry.target()) != NULL,
 405          &quot;destination of far call not found in code cache&quot;);
 406   if (far_branches()) {
 407     unsigned long offset;
 408     // We can use ADRP here because we know that the total size of
 409     // the code cache cannot exceed 2Gb.
 410     adrp(tmp, entry, offset);
 411     add(tmp, tmp, offset);
 412     if (cbuf) cbuf-&gt;set_insts_mark();
 413     blr(tmp);
 414   } else {
 415     if (cbuf) cbuf-&gt;set_insts_mark();
 416     bl(entry);
 417   }
 418 }
 419 
 420 void MacroAssembler::far_jump(Address entry, CodeBuffer *cbuf, Register tmp) {
 421   assert(ReservedCodeCacheSize &lt; 4*G, &quot;branch out of range&quot;);
 422   assert(CodeCache::find_blob(entry.target()) != NULL,
 423          &quot;destination of far call not found in code cache&quot;);
 424   if (far_branches()) {
 425     unsigned long offset;
 426     // We can use ADRP here because we know that the total size of
 427     // the code cache cannot exceed 2Gb.
 428     adrp(tmp, entry, offset);
 429     add(tmp, tmp, offset);
 430     if (cbuf) cbuf-&gt;set_insts_mark();
 431     br(tmp);
 432   } else {
 433     if (cbuf) cbuf-&gt;set_insts_mark();
 434     b(entry);
 435   }
 436 }
 437 
 438 void MacroAssembler::reserved_stack_check() {
 439     // testing if reserved zone needs to be enabled
 440     Label no_reserved_zone_enabling;
 441 
 442     ldr(rscratch1, Address(rthread, JavaThread::reserved_stack_activation_offset()));
 443     cmp(sp, rscratch1);
 444     br(Assembler::LO, no_reserved_zone_enabling);
 445 
 446     enter();   // LR and FP are live.
 447     lea(rscratch1, CAST_FROM_FN_PTR(address, SharedRuntime::enable_stack_reserved_zone));
 448     mov(c_rarg0, rthread);
 449     blr(rscratch1);
 450     leave();
 451 
 452     // We have already removed our own frame.
 453     // throw_delayed_StackOverflowError will think that it&#39;s been
 454     // called by our caller.
 455     lea(rscratch1, RuntimeAddress(StubRoutines::throw_delayed_StackOverflowError_entry()));
 456     br(rscratch1);
 457     should_not_reach_here();
 458 
 459     bind(no_reserved_zone_enabling);
 460 }
 461 
 462 int MacroAssembler::biased_locking_enter(Register lock_reg,
 463                                          Register obj_reg,
 464                                          Register swap_reg,
 465                                          Register tmp_reg,
 466                                          bool swap_reg_contains_mark,
 467                                          Label&amp; done,
 468                                          Label* slow_case,
 469                                          BiasedLockingCounters* counters) {
 470   assert(UseBiasedLocking, &quot;why call this otherwise?&quot;);
 471   assert_different_registers(lock_reg, obj_reg, swap_reg);
 472 
 473   if (PrintBiasedLockingStatistics &amp;&amp; counters == NULL)
 474     counters = BiasedLocking::counters();
 475 
 476   assert_different_registers(lock_reg, obj_reg, swap_reg, tmp_reg, rscratch1, rscratch2, noreg);
 477   assert(markWord::age_shift == markWord::lock_bits + markWord::biased_lock_bits, &quot;biased locking makes assumptions about bit layout&quot;);
 478   Address mark_addr      (obj_reg, oopDesc::mark_offset_in_bytes());
 479   Address klass_addr     (obj_reg, oopDesc::klass_offset_in_bytes());
 480   Address saved_mark_addr(lock_reg, 0);
 481 
 482   // Biased locking
 483   // See whether the lock is currently biased toward our thread and
 484   // whether the epoch is still valid
 485   // Note that the runtime guarantees sufficient alignment of JavaThread
 486   // pointers to allow age to be placed into low bits
 487   // First check to see whether biasing is even enabled for this object
 488   Label cas_label;
 489   int null_check_offset = -1;
 490   if (!swap_reg_contains_mark) {
 491     null_check_offset = offset();
 492     ldr(swap_reg, mark_addr);
 493   }
 494   andr(tmp_reg, swap_reg, markWord::biased_lock_mask_in_place);
 495   cmp(tmp_reg, (u1)markWord::biased_lock_pattern);
 496   br(Assembler::NE, cas_label);
 497   // The bias pattern is present in the object&#39;s header. Need to check
 498   // whether the bias owner and the epoch are both still current.
 499   load_prototype_header(tmp_reg, obj_reg);
 500   orr(tmp_reg, tmp_reg, rthread);
 501   eor(tmp_reg, swap_reg, tmp_reg);
 502   andr(tmp_reg, tmp_reg, ~((int) markWord::age_mask_in_place));
 503   if (counters != NULL) {
 504     Label around;
 505     cbnz(tmp_reg, around);
 506     atomic_incw(Address((address)counters-&gt;biased_lock_entry_count_addr()), tmp_reg, rscratch1, rscratch2);
 507     b(done);
 508     bind(around);
 509   } else {
 510     cbz(tmp_reg, done);
 511   }
 512 
 513   Label try_revoke_bias;
 514   Label try_rebias;
 515 
 516   // At this point we know that the header has the bias pattern and
 517   // that we are not the bias owner in the current epoch. We need to
 518   // figure out more details about the state of the header in order to
 519   // know what operations can be legally performed on the object&#39;s
 520   // header.
 521 
 522   // If the low three bits in the xor result aren&#39;t clear, that means
 523   // the prototype header is no longer biased and we have to revoke
 524   // the bias on this object.
 525   andr(rscratch1, tmp_reg, markWord::biased_lock_mask_in_place);
 526   cbnz(rscratch1, try_revoke_bias);
 527 
 528   // Biasing is still enabled for this data type. See whether the
 529   // epoch of the current bias is still valid, meaning that the epoch
 530   // bits of the mark word are equal to the epoch bits of the
 531   // prototype header. (Note that the prototype header&#39;s epoch bits
 532   // only change at a safepoint.) If not, attempt to rebias the object
 533   // toward the current thread. Note that we must be absolutely sure
 534   // that the current epoch is invalid in order to do this because
 535   // otherwise the manipulations it performs on the mark word are
 536   // illegal.
 537   andr(rscratch1, tmp_reg, markWord::epoch_mask_in_place);
 538   cbnz(rscratch1, try_rebias);
 539 
 540   // The epoch of the current bias is still valid but we know nothing
 541   // about the owner; it might be set or it might be clear. Try to
 542   // acquire the bias of the object using an atomic operation. If this
 543   // fails we will go in to the runtime to revoke the object&#39;s bias.
 544   // Note that we first construct the presumed unbiased header so we
 545   // don&#39;t accidentally blow away another thread&#39;s valid bias.
 546   {
 547     Label here;
 548     mov(rscratch1, markWord::biased_lock_mask_in_place | markWord::age_mask_in_place | markWord::epoch_mask_in_place);
 549     andr(swap_reg, swap_reg, rscratch1);
 550     orr(tmp_reg, swap_reg, rthread);
 551     cmpxchg_obj_header(swap_reg, tmp_reg, obj_reg, rscratch1, here, slow_case);
 552     // If the biasing toward our thread failed, this means that
 553     // another thread succeeded in biasing it toward itself and we
 554     // need to revoke that bias. The revocation will occur in the
 555     // interpreter runtime in the slow case.
 556     bind(here);
 557     if (counters != NULL) {
 558       atomic_incw(Address((address)counters-&gt;anonymously_biased_lock_entry_count_addr()),
 559                   tmp_reg, rscratch1, rscratch2);
 560     }
 561   }
 562   b(done);
 563 
 564   bind(try_rebias);
 565   // At this point we know the epoch has expired, meaning that the
 566   // current &quot;bias owner&quot;, if any, is actually invalid. Under these
 567   // circumstances _only_, we are allowed to use the current header&#39;s
 568   // value as the comparison value when doing the cas to acquire the
 569   // bias in the current epoch. In other words, we allow transfer of
 570   // the bias from one thread to another directly in this situation.
 571   //
 572   // FIXME: due to a lack of registers we currently blow away the age
 573   // bits in this situation. Should attempt to preserve them.
 574   {
 575     Label here;
 576     load_prototype_header(tmp_reg, obj_reg);
 577     orr(tmp_reg, rthread, tmp_reg);
 578     cmpxchg_obj_header(swap_reg, tmp_reg, obj_reg, rscratch1, here, slow_case);
 579     // If the biasing toward our thread failed, then another thread
 580     // succeeded in biasing it toward itself and we need to revoke that
 581     // bias. The revocation will occur in the runtime in the slow case.
 582     bind(here);
 583     if (counters != NULL) {
 584       atomic_incw(Address((address)counters-&gt;rebiased_lock_entry_count_addr()),
 585                   tmp_reg, rscratch1, rscratch2);
 586     }
 587   }
 588   b(done);
 589 
 590   bind(try_revoke_bias);
 591   // The prototype mark in the klass doesn&#39;t have the bias bit set any
 592   // more, indicating that objects of this data type are not supposed
 593   // to be biased any more. We are going to try to reset the mark of
 594   // this object to the prototype value and fall through to the
 595   // CAS-based locking scheme. Note that if our CAS fails, it means
 596   // that another thread raced us for the privilege of revoking the
 597   // bias of this particular object, so it&#39;s okay to continue in the
 598   // normal locking code.
 599   //
 600   // FIXME: due to a lack of registers we currently blow away the age
 601   // bits in this situation. Should attempt to preserve them.
 602   {
 603     Label here, nope;
 604     load_prototype_header(tmp_reg, obj_reg);
 605     cmpxchg_obj_header(swap_reg, tmp_reg, obj_reg, rscratch1, here, &amp;nope);
 606     bind(here);
 607 
 608     // Fall through to the normal CAS-based lock, because no matter what
 609     // the result of the above CAS, some thread must have succeeded in
 610     // removing the bias bit from the object&#39;s header.
 611     if (counters != NULL) {
 612       atomic_incw(Address((address)counters-&gt;revoked_lock_entry_count_addr()), tmp_reg,
 613                   rscratch1, rscratch2);
 614     }
 615     bind(nope);
 616   }
 617 
 618   bind(cas_label);
 619 
 620   return null_check_offset;
 621 }
 622 
 623 void MacroAssembler::biased_locking_exit(Register obj_reg, Register temp_reg, Label&amp; done) {
 624   assert(UseBiasedLocking, &quot;why call this otherwise?&quot;);
 625 
 626   // Check for biased locking unlock case, which is a no-op
 627   // Note: we do not have to check the thread ID for two reasons.
 628   // First, the interpreter checks for IllegalMonitorStateException at
 629   // a higher level. Second, if the bias was revoked while we held the
 630   // lock, the object could not be rebiased toward another thread, so
 631   // the bias bit would be clear.
 632   ldr(temp_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));
 633   andr(temp_reg, temp_reg, markWord::biased_lock_mask_in_place);
 634   cmp(temp_reg, (u1)markWord::biased_lock_pattern);
 635   br(Assembler::EQ, done);
 636 }
 637 
 638 static void pass_arg0(MacroAssembler* masm, Register arg) {
 639   if (c_rarg0 != arg ) {
 640     masm-&gt;mov(c_rarg0, arg);
 641   }
 642 }
 643 
 644 static void pass_arg1(MacroAssembler* masm, Register arg) {
 645   if (c_rarg1 != arg ) {
 646     masm-&gt;mov(c_rarg1, arg);
 647   }
 648 }
 649 
 650 static void pass_arg2(MacroAssembler* masm, Register arg) {
 651   if (c_rarg2 != arg ) {
 652     masm-&gt;mov(c_rarg2, arg);
 653   }
 654 }
 655 
 656 static void pass_arg3(MacroAssembler* masm, Register arg) {
 657   if (c_rarg3 != arg ) {
 658     masm-&gt;mov(c_rarg3, arg);
 659   }
 660 }
 661 
 662 void MacroAssembler::call_VM_base(Register oop_result,
 663                                   Register java_thread,
 664                                   Register last_java_sp,
 665                                   address  entry_point,
 666                                   int      number_of_arguments,
 667                                   bool     check_exceptions) {
 668    // determine java_thread register
 669   if (!java_thread-&gt;is_valid()) {
 670     java_thread = rthread;
 671   }
 672 
 673   // determine last_java_sp register
 674   if (!last_java_sp-&gt;is_valid()) {
 675     last_java_sp = esp;
 676   }
 677 
 678   // debugging support
 679   assert(number_of_arguments &gt;= 0   , &quot;cannot have negative number of arguments&quot;);
 680   assert(java_thread == rthread, &quot;unexpected register&quot;);
 681 #ifdef ASSERT
 682   // TraceBytecodes does not use r12 but saves it over the call, so don&#39;t verify
 683   // if ((UseCompressedOops || UseCompressedClassPointers) &amp;&amp; !TraceBytecodes) verify_heapbase(&quot;call_VM_base: heap base corrupted?&quot;);
 684 #endif // ASSERT
 685 
 686   assert(java_thread != oop_result  , &quot;cannot use the same register for java_thread &amp; oop_result&quot;);
 687   assert(java_thread != last_java_sp, &quot;cannot use the same register for java_thread &amp; last_java_sp&quot;);
 688 
 689   // push java thread (becomes first argument of C function)
 690 
 691   mov(c_rarg0, java_thread);
 692 
 693   // set last Java frame before call
 694   assert(last_java_sp != rfp, &quot;can&#39;t use rfp&quot;);
 695 
 696   Label l;
 697   set_last_Java_frame(last_java_sp, rfp, l, rscratch1);
 698 
 699   // do the call, remove parameters
 700   MacroAssembler::call_VM_leaf_base(entry_point, number_of_arguments, &amp;l);
 701 
 702   // reset last Java frame
 703   // Only interpreter should have to clear fp
 704   reset_last_Java_frame(true);
 705 
 706    // C++ interp handles this in the interpreter
 707   check_and_handle_popframe(java_thread);
 708   check_and_handle_earlyret(java_thread);
 709 
 710   if (check_exceptions) {
 711     // check for pending exceptions (java_thread is set upon return)
 712     ldr(rscratch1, Address(java_thread, in_bytes(Thread::pending_exception_offset())));
 713     Label ok;
 714     cbz(rscratch1, ok);
 715     lea(rscratch1, RuntimeAddress(StubRoutines::forward_exception_entry()));
 716     br(rscratch1);
 717     bind(ok);
 718   }
 719 
 720   // get oop result if there is one and reset the value in the thread
 721   if (oop_result-&gt;is_valid()) {
 722     get_vm_result(oop_result, java_thread);
 723   }
 724 }
 725 
 726 void MacroAssembler::call_VM_helper(Register oop_result, address entry_point, int number_of_arguments, bool check_exceptions) {
 727   call_VM_base(oop_result, noreg, noreg, entry_point, number_of_arguments, check_exceptions);
 728 }
 729 
 730 // Maybe emit a call via a trampoline.  If the code cache is small
 731 // trampolines won&#39;t be emitted.
 732 
 733 address MacroAssembler::trampoline_call(Address entry, CodeBuffer *cbuf) {
 734   assert(JavaThread::current()-&gt;is_Compiler_thread(), &quot;just checking&quot;);
 735   assert(entry.rspec().type() == relocInfo::runtime_call_type
 736          || entry.rspec().type() == relocInfo::opt_virtual_call_type
 737          || entry.rspec().type() == relocInfo::static_call_type
 738          || entry.rspec().type() == relocInfo::virtual_call_type, &quot;wrong reloc type&quot;);
 739 
 740   // We need a trampoline if branches are far.
 741   if (far_branches()) {
 742     bool in_scratch_emit_size = false;
 743 #ifdef COMPILER2
 744     // We don&#39;t want to emit a trampoline if C2 is generating dummy
 745     // code during its branch shortening phase.
 746     CompileTask* task = ciEnv::current()-&gt;task();
 747     in_scratch_emit_size =
 748       (task != NULL &amp;&amp; is_c2_compile(task-&gt;comp_level()) &amp;&amp;
 749        Compile::current()-&gt;in_scratch_emit_size());
 750 #endif
 751     if (!in_scratch_emit_size) {
 752       address stub = emit_trampoline_stub(offset(), entry.target());
 753       if (stub == NULL) {
 754         return NULL; // CodeCache is full
 755       }
 756     }
 757   }
 758 
 759   if (cbuf) cbuf-&gt;set_insts_mark();
 760   relocate(entry.rspec());
 761   if (!far_branches()) {
 762     bl(entry.target());
 763   } else {
 764     bl(pc());
 765   }
 766   // just need to return a non-null address
 767   return pc();
 768 }
 769 
 770 
 771 // Emit a trampoline stub for a call to a target which is too far away.
 772 //
 773 // code sequences:
 774 //
 775 // call-site:
 776 //   branch-and-link to &lt;destination&gt; or &lt;trampoline stub&gt;
 777 //
 778 // Related trampoline stub for this call site in the stub section:
 779 //   load the call target from the constant pool
 780 //   branch (LR still points to the call site above)
 781 
 782 address MacroAssembler::emit_trampoline_stub(int insts_call_instruction_offset,
 783                                              address dest) {
 784   // Max stub size: alignment nop, TrampolineStub.
 785   address stub = start_a_stub(NativeInstruction::instruction_size
 786                    + NativeCallTrampolineStub::instruction_size);
 787   if (stub == NULL) {
 788     return NULL;  // CodeBuffer::expand failed
 789   }
 790 
 791   // Create a trampoline stub relocation which relates this trampoline stub
 792   // with the call instruction at insts_call_instruction_offset in the
 793   // instructions code-section.
 794   align(wordSize);
 795   relocate(trampoline_stub_Relocation::spec(code()-&gt;insts()-&gt;start()
 796                                             + insts_call_instruction_offset));
 797   const int stub_start_offset = offset();
 798 
 799   // Now, create the trampoline stub&#39;s code:
 800   // - load the call
 801   // - call
 802   Label target;
 803   ldr(rscratch1, target);
 804   br(rscratch1);
 805   bind(target);
 806   assert(offset() - stub_start_offset == NativeCallTrampolineStub::data_offset,
 807          &quot;should be&quot;);
 808   emit_int64((int64_t)dest);
 809 
 810   const address stub_start_addr = addr_at(stub_start_offset);
 811 
 812   assert(is_NativeCallTrampolineStub_at(stub_start_addr), &quot;doesn&#39;t look like a trampoline&quot;);
 813 
 814   end_a_stub();
 815   return stub_start_addr;
 816 }
 817 
 818 void MacroAssembler::emit_static_call_stub() {
 819   // CompiledDirectStaticCall::set_to_interpreted knows the
 820   // exact layout of this stub.
 821 
 822   isb();
 823   mov_metadata(rmethod, (Metadata*)NULL);
 824 
 825   // Jump to the entry point of the i2c stub.
 826   movptr(rscratch1, 0);
 827   br(rscratch1);
 828 }
 829 
 830 void MacroAssembler::c2bool(Register x) {
 831   // implements x == 0 ? 0 : 1
 832   // note: must only look at least-significant byte of x
 833   //       since C-style booleans are stored in one byte
 834   //       only! (was bug)
 835   tst(x, 0xff);
 836   cset(x, Assembler::NE);
 837 }
 838 
 839 address MacroAssembler::ic_call(address entry, jint method_index) {
 840   RelocationHolder rh = virtual_call_Relocation::spec(pc(), method_index);
 841   // address const_ptr = long_constant((jlong)Universe::non_oop_word());
 842   // unsigned long offset;
 843   // ldr_constant(rscratch2, const_ptr);
 844   movptr(rscratch2, (uintptr_t)Universe::non_oop_word());
 845   return trampoline_call(Address(entry, rh));
 846 }
 847 
 848 // Implementation of call_VM versions
 849 
 850 void MacroAssembler::call_VM(Register oop_result,
 851                              address entry_point,
 852                              bool check_exceptions) {
 853   call_VM_helper(oop_result, entry_point, 0, check_exceptions);
 854 }
 855 
 856 void MacroAssembler::call_VM(Register oop_result,
 857                              address entry_point,
 858                              Register arg_1,
 859                              bool check_exceptions) {
 860   pass_arg1(this, arg_1);
 861   call_VM_helper(oop_result, entry_point, 1, check_exceptions);
 862 }
 863 
 864 void MacroAssembler::call_VM(Register oop_result,
 865                              address entry_point,
 866                              Register arg_1,
 867                              Register arg_2,
 868                              bool check_exceptions) {
 869   assert(arg_1 != c_rarg2, &quot;smashed arg&quot;);
 870   pass_arg2(this, arg_2);
 871   pass_arg1(this, arg_1);
 872   call_VM_helper(oop_result, entry_point, 2, check_exceptions);
 873 }
 874 
 875 void MacroAssembler::call_VM(Register oop_result,
 876                              address entry_point,
 877                              Register arg_1,
 878                              Register arg_2,
 879                              Register arg_3,
 880                              bool check_exceptions) {
 881   assert(arg_1 != c_rarg3, &quot;smashed arg&quot;);
 882   assert(arg_2 != c_rarg3, &quot;smashed arg&quot;);
 883   pass_arg3(this, arg_3);
 884 
 885   assert(arg_1 != c_rarg2, &quot;smashed arg&quot;);
 886   pass_arg2(this, arg_2);
 887 
 888   pass_arg1(this, arg_1);
 889   call_VM_helper(oop_result, entry_point, 3, check_exceptions);
 890 }
 891 
 892 void MacroAssembler::call_VM(Register oop_result,
 893                              Register last_java_sp,
 894                              address entry_point,
 895                              int number_of_arguments,
 896                              bool check_exceptions) {
 897   call_VM_base(oop_result, rthread, last_java_sp, entry_point, number_of_arguments, check_exceptions);
 898 }
 899 
 900 void MacroAssembler::call_VM(Register oop_result,
 901                              Register last_java_sp,
 902                              address entry_point,
 903                              Register arg_1,
 904                              bool check_exceptions) {
 905   pass_arg1(this, arg_1);
 906   call_VM(oop_result, last_java_sp, entry_point, 1, check_exceptions);
 907 }
 908 
 909 void MacroAssembler::call_VM(Register oop_result,
 910                              Register last_java_sp,
 911                              address entry_point,
 912                              Register arg_1,
 913                              Register arg_2,
 914                              bool check_exceptions) {
 915 
 916   assert(arg_1 != c_rarg2, &quot;smashed arg&quot;);
 917   pass_arg2(this, arg_2);
 918   pass_arg1(this, arg_1);
 919   call_VM(oop_result, last_java_sp, entry_point, 2, check_exceptions);
 920 }
 921 
 922 void MacroAssembler::call_VM(Register oop_result,
 923                              Register last_java_sp,
 924                              address entry_point,
 925                              Register arg_1,
 926                              Register arg_2,
 927                              Register arg_3,
 928                              bool check_exceptions) {
 929   assert(arg_1 != c_rarg3, &quot;smashed arg&quot;);
 930   assert(arg_2 != c_rarg3, &quot;smashed arg&quot;);
 931   pass_arg3(this, arg_3);
 932   assert(arg_1 != c_rarg2, &quot;smashed arg&quot;);
 933   pass_arg2(this, arg_2);
 934   pass_arg1(this, arg_1);
 935   call_VM(oop_result, last_java_sp, entry_point, 3, check_exceptions);
 936 }
 937 
 938 
 939 void MacroAssembler::get_vm_result(Register oop_result, Register java_thread) {
 940   ldr(oop_result, Address(java_thread, JavaThread::vm_result_offset()));
 941   str(zr, Address(java_thread, JavaThread::vm_result_offset()));
 942   verify_oop(oop_result, &quot;broken oop in call_VM_base&quot;);
 943 }
 944 
 945 void MacroAssembler::get_vm_result_2(Register metadata_result, Register java_thread) {
 946   ldr(metadata_result, Address(java_thread, JavaThread::vm_result_2_offset()));
 947   str(zr, Address(java_thread, JavaThread::vm_result_2_offset()));
 948 }
 949 
 950 void MacroAssembler::align(int modulus) {
 951   while (offset() % modulus != 0) nop();
 952 }
 953 
 954 // these are no-ops overridden by InterpreterMacroAssembler
 955 
 956 void MacroAssembler::check_and_handle_earlyret(Register java_thread) { }
 957 
 958 void MacroAssembler::check_and_handle_popframe(Register java_thread) { }
 959 
 960 
 961 RegisterOrConstant MacroAssembler::delayed_value_impl(intptr_t* delayed_value_addr,
 962                                                       Register tmp,
 963                                                       int offset) {
 964   intptr_t value = *delayed_value_addr;
 965   if (value != 0)
 966     return RegisterOrConstant(value + offset);
 967 
 968   // load indirectly to solve generation ordering problem
 969   ldr(tmp, ExternalAddress((address) delayed_value_addr));
 970 
 971   if (offset != 0)
 972     add(tmp, tmp, offset);
 973 
 974   return RegisterOrConstant(tmp);
 975 }
 976 
 977 // Look up the method for a megamorphic invokeinterface call.
 978 // The target method is determined by &lt;intf_klass, itable_index&gt;.
 979 // The receiver klass is in recv_klass.
 980 // On success, the result will be in method_result, and execution falls through.
 981 // On failure, execution transfers to the given label.
 982 void MacroAssembler::lookup_interface_method(Register recv_klass,
 983                                              Register intf_klass,
 984                                              RegisterOrConstant itable_index,
 985                                              Register method_result,
 986                                              Register scan_temp,
 987                                              Label&amp; L_no_such_interface,
 988                          bool return_method) {
 989   assert_different_registers(recv_klass, intf_klass, scan_temp);
 990   assert_different_registers(method_result, intf_klass, scan_temp);
 991   assert(recv_klass != method_result || !return_method,
 992      &quot;recv_klass can be destroyed when method isn&#39;t needed&quot;);
 993   assert(itable_index.is_constant() || itable_index.as_register() == method_result,
 994          &quot;caller must use same register for non-constant itable index as for method&quot;);
 995 
 996   // Compute start of first itableOffsetEntry (which is at the end of the vtable)
 997   int vtable_base = in_bytes(Klass::vtable_start_offset());
 998   int itentry_off = itableMethodEntry::method_offset_in_bytes();
 999   int scan_step   = itableOffsetEntry::size() * wordSize;
1000   int vte_size    = vtableEntry::size_in_bytes();
1001   assert(vte_size == wordSize, &quot;else adjust times_vte_scale&quot;);
1002 
1003   ldrw(scan_temp, Address(recv_klass, Klass::vtable_length_offset()));
1004 
1005   // %%% Could store the aligned, prescaled offset in the klassoop.
1006   // lea(scan_temp, Address(recv_klass, scan_temp, times_vte_scale, vtable_base));
1007   lea(scan_temp, Address(recv_klass, scan_temp, Address::lsl(3)));
1008   add(scan_temp, scan_temp, vtable_base);
1009 
1010   if (return_method) {
1011     // Adjust recv_klass by scaled itable_index, so we can free itable_index.
1012     assert(itableMethodEntry::size() * wordSize == wordSize, &quot;adjust the scaling in the code below&quot;);
1013     // lea(recv_klass, Address(recv_klass, itable_index, Address::times_ptr, itentry_off));
1014     lea(recv_klass, Address(recv_klass, itable_index, Address::lsl(3)));
1015     if (itentry_off)
1016       add(recv_klass, recv_klass, itentry_off);
1017   }
1018 
1019   // for (scan = klass-&gt;itable(); scan-&gt;interface() != NULL; scan += scan_step) {
1020   //   if (scan-&gt;interface() == intf) {
1021   //     result = (klass + scan-&gt;offset() + itable_index);
1022   //   }
1023   // }
1024   Label search, found_method;
1025 
1026   for (int peel = 1; peel &gt;= 0; peel--) {
1027     ldr(method_result, Address(scan_temp, itableOffsetEntry::interface_offset_in_bytes()));
1028     cmp(intf_klass, method_result);
1029 
1030     if (peel) {
1031       br(Assembler::EQ, found_method);
1032     } else {
1033       br(Assembler::NE, search);
1034       // (invert the test to fall through to found_method...)
1035     }
1036 
1037     if (!peel)  break;
1038 
1039     bind(search);
1040 
1041     // Check that the previous entry is non-null.  A null entry means that
1042     // the receiver class doesn&#39;t implement the interface, and wasn&#39;t the
1043     // same as when the caller was compiled.
1044     cbz(method_result, L_no_such_interface);
1045     add(scan_temp, scan_temp, scan_step);
1046   }
1047 
1048   bind(found_method);
1049 
1050   // Got a hit.
1051   if (return_method) {
1052     ldrw(scan_temp, Address(scan_temp, itableOffsetEntry::offset_offset_in_bytes()));
1053     ldr(method_result, Address(recv_klass, scan_temp, Address::uxtw(0)));
1054   }
1055 }
1056 
1057 // virtual method calling
1058 void MacroAssembler::lookup_virtual_method(Register recv_klass,
1059                                            RegisterOrConstant vtable_index,
1060                                            Register method_result) {
1061   const int base = in_bytes(Klass::vtable_start_offset());
1062   assert(vtableEntry::size() * wordSize == 8,
1063          &quot;adjust the scaling in the code below&quot;);
1064   int vtable_offset_in_bytes = base + vtableEntry::method_offset_in_bytes();
1065 
1066   if (vtable_index.is_register()) {
1067     lea(method_result, Address(recv_klass,
1068                                vtable_index.as_register(),
1069                                Address::lsl(LogBytesPerWord)));
1070     ldr(method_result, Address(method_result, vtable_offset_in_bytes));
1071   } else {
1072     vtable_offset_in_bytes += vtable_index.as_constant() * wordSize;
1073     ldr(method_result,
1074         form_address(rscratch1, recv_klass, vtable_offset_in_bytes, 0));
1075   }
1076 }
1077 
1078 void MacroAssembler::check_klass_subtype(Register sub_klass,
1079                            Register super_klass,
1080                            Register temp_reg,
1081                            Label&amp; L_success) {
1082   Label L_failure;
1083   check_klass_subtype_fast_path(sub_klass, super_klass, temp_reg,        &amp;L_success, &amp;L_failure, NULL);
1084   check_klass_subtype_slow_path(sub_klass, super_klass, temp_reg, noreg, &amp;L_success, NULL);
1085   bind(L_failure);
1086 }
1087 
1088 
1089 void MacroAssembler::check_klass_subtype_fast_path(Register sub_klass,
1090                                                    Register super_klass,
1091                                                    Register temp_reg,
1092                                                    Label* L_success,
1093                                                    Label* L_failure,
1094                                                    Label* L_slow_path,
1095                                         RegisterOrConstant super_check_offset) {
1096   assert_different_registers(sub_klass, super_klass, temp_reg);
1097   bool must_load_sco = (super_check_offset.constant_or_zero() == -1);
1098   if (super_check_offset.is_register()) {
1099     assert_different_registers(sub_klass, super_klass,
1100                                super_check_offset.as_register());
1101   } else if (must_load_sco) {
1102     assert(temp_reg != noreg, &quot;supply either a temp or a register offset&quot;);
1103   }
1104 
1105   Label L_fallthrough;
1106   int label_nulls = 0;
1107   if (L_success == NULL)   { L_success   = &amp;L_fallthrough; label_nulls++; }
1108   if (L_failure == NULL)   { L_failure   = &amp;L_fallthrough; label_nulls++; }
1109   if (L_slow_path == NULL) { L_slow_path = &amp;L_fallthrough; label_nulls++; }
1110   assert(label_nulls &lt;= 1, &quot;at most one NULL in the batch&quot;);
1111 
1112   int sc_offset = in_bytes(Klass::secondary_super_cache_offset());
1113   int sco_offset = in_bytes(Klass::super_check_offset_offset());
1114   Address super_check_offset_addr(super_klass, sco_offset);
1115 
1116   // Hacked jmp, which may only be used just before L_fallthrough.
1117 #define final_jmp(label)                                                \
1118   if (&amp;(label) == &amp;L_fallthrough) { /*do nothing*/ }                    \
1119   else                            b(label)                /*omit semi*/
1120 
1121   // If the pointers are equal, we are done (e.g., String[] elements).
1122   // This self-check enables sharing of secondary supertype arrays among
1123   // non-primary types such as array-of-interface.  Otherwise, each such
1124   // type would need its own customized SSA.
1125   // We move this check to the front of the fast path because many
1126   // type checks are in fact trivially successful in this manner,
1127   // so we get a nicely predicted branch right at the start of the check.
1128   cmp(sub_klass, super_klass);
1129   br(Assembler::EQ, *L_success);
1130 
1131   // Check the supertype display:
1132   if (must_load_sco) {
1133     ldrw(temp_reg, super_check_offset_addr);
1134     super_check_offset = RegisterOrConstant(temp_reg);
1135   }
1136   Address super_check_addr(sub_klass, super_check_offset);
1137   ldr(rscratch1, super_check_addr);
1138   cmp(super_klass, rscratch1); // load displayed supertype
1139 
1140   // This check has worked decisively for primary supers.
1141   // Secondary supers are sought in the super_cache (&#39;super_cache_addr&#39;).
1142   // (Secondary supers are interfaces and very deeply nested subtypes.)
1143   // This works in the same check above because of a tricky aliasing
1144   // between the super_cache and the primary super display elements.
1145   // (The &#39;super_check_addr&#39; can address either, as the case requires.)
1146   // Note that the cache is updated below if it does not help us find
1147   // what we need immediately.
1148   // So if it was a primary super, we can just fail immediately.
1149   // Otherwise, it&#39;s the slow path for us (no success at this point).
1150 
1151   if (super_check_offset.is_register()) {
1152     br(Assembler::EQ, *L_success);
1153     subs(zr, super_check_offset.as_register(), sc_offset);
1154     if (L_failure == &amp;L_fallthrough) {
1155       br(Assembler::EQ, *L_slow_path);
1156     } else {
1157       br(Assembler::NE, *L_failure);
1158       final_jmp(*L_slow_path);
1159     }
1160   } else if (super_check_offset.as_constant() == sc_offset) {
1161     // Need a slow path; fast failure is impossible.
1162     if (L_slow_path == &amp;L_fallthrough) {
1163       br(Assembler::EQ, *L_success);
1164     } else {
1165       br(Assembler::NE, *L_slow_path);
1166       final_jmp(*L_success);
1167     }
1168   } else {
1169     // No slow path; it&#39;s a fast decision.
1170     if (L_failure == &amp;L_fallthrough) {
1171       br(Assembler::EQ, *L_success);
1172     } else {
1173       br(Assembler::NE, *L_failure);
1174       final_jmp(*L_success);
1175     }
1176   }
1177 
1178   bind(L_fallthrough);
1179 
1180 #undef final_jmp
1181 }
1182 
1183 // These two are taken from x86, but they look generally useful
1184 
1185 // scans count pointer sized words at [addr] for occurence of value,
1186 // generic
1187 void MacroAssembler::repne_scan(Register addr, Register value, Register count,
1188                                 Register scratch) {
1189   Label Lloop, Lexit;
1190   cbz(count, Lexit);
1191   bind(Lloop);
1192   ldr(scratch, post(addr, wordSize));
1193   cmp(value, scratch);
1194   br(EQ, Lexit);
1195   sub(count, count, 1);
1196   cbnz(count, Lloop);
1197   bind(Lexit);
1198 }
1199 
1200 // scans count 4 byte words at [addr] for occurence of value,
1201 // generic
1202 void MacroAssembler::repne_scanw(Register addr, Register value, Register count,
1203                                 Register scratch) {
1204   Label Lloop, Lexit;
1205   cbz(count, Lexit);
1206   bind(Lloop);
1207   ldrw(scratch, post(addr, wordSize));
1208   cmpw(value, scratch);
1209   br(EQ, Lexit);
1210   sub(count, count, 1);
1211   cbnz(count, Lloop);
1212   bind(Lexit);
1213 }
1214 
1215 void MacroAssembler::check_klass_subtype_slow_path(Register sub_klass,
1216                                                    Register super_klass,
1217                                                    Register temp_reg,
1218                                                    Register temp2_reg,
1219                                                    Label* L_success,
1220                                                    Label* L_failure,
1221                                                    bool set_cond_codes) {
1222   assert_different_registers(sub_klass, super_klass, temp_reg);
1223   if (temp2_reg != noreg)
1224     assert_different_registers(sub_klass, super_klass, temp_reg, temp2_reg, rscratch1);
1225 #define IS_A_TEMP(reg) ((reg) == temp_reg || (reg) == temp2_reg)
1226 
1227   Label L_fallthrough;
1228   int label_nulls = 0;
1229   if (L_success == NULL)   { L_success   = &amp;L_fallthrough; label_nulls++; }
1230   if (L_failure == NULL)   { L_failure   = &amp;L_fallthrough; label_nulls++; }
1231   assert(label_nulls &lt;= 1, &quot;at most one NULL in the batch&quot;);
1232 
1233   // a couple of useful fields in sub_klass:
1234   int ss_offset = in_bytes(Klass::secondary_supers_offset());
1235   int sc_offset = in_bytes(Klass::secondary_super_cache_offset());
1236   Address secondary_supers_addr(sub_klass, ss_offset);
1237   Address super_cache_addr(     sub_klass, sc_offset);
1238 
1239   BLOCK_COMMENT(&quot;check_klass_subtype_slow_path&quot;);
1240 
1241   // Do a linear scan of the secondary super-klass chain.
1242   // This code is rarely used, so simplicity is a virtue here.
1243   // The repne_scan instruction uses fixed registers, which we must spill.
1244   // Don&#39;t worry too much about pre-existing connections with the input regs.
1245 
1246   assert(sub_klass != r0, &quot;killed reg&quot;); // killed by mov(r0, super)
1247   assert(sub_klass != r2, &quot;killed reg&quot;); // killed by lea(r2, &amp;pst_counter)
1248 
1249   RegSet pushed_registers;
1250   if (!IS_A_TEMP(r2))    pushed_registers += r2;
1251   if (!IS_A_TEMP(r5))    pushed_registers += r5;
1252 
1253   if (super_klass != r0 || UseCompressedOops) {
1254     if (!IS_A_TEMP(r0))   pushed_registers += r0;
1255   }
1256 
1257   push(pushed_registers, sp);
1258 
1259   // Get super_klass value into r0 (even if it was in r5 or r2).
1260   if (super_klass != r0) {
1261     mov(r0, super_klass);
1262   }
1263 
1264 #ifndef PRODUCT
1265   mov(rscratch2, (address)&amp;SharedRuntime::_partial_subtype_ctr);
1266   Address pst_counter_addr(rscratch2);
1267   ldr(rscratch1, pst_counter_addr);
1268   add(rscratch1, rscratch1, 1);
1269   str(rscratch1, pst_counter_addr);
1270 #endif //PRODUCT
1271 
1272   // We will consult the secondary-super array.
1273   ldr(r5, secondary_supers_addr);
1274   // Load the array length.
1275   ldrw(r2, Address(r5, Array&lt;Klass*&gt;::length_offset_in_bytes()));
1276   // Skip to start of data.
1277   add(r5, r5, Array&lt;Klass*&gt;::base_offset_in_bytes());
1278 
1279   cmp(sp, zr); // Clear Z flag; SP is never zero
1280   // Scan R2 words at [R5] for an occurrence of R0.
1281   // Set NZ/Z based on last compare.
1282   repne_scan(r5, r0, r2, rscratch1);
1283 
1284   // Unspill the temp. registers:
1285   pop(pushed_registers, sp);
1286 
1287   br(Assembler::NE, *L_failure);
1288 
1289   // Success.  Cache the super we found and proceed in triumph.
1290   str(super_klass, super_cache_addr);
1291 
1292   if (L_success != &amp;L_fallthrough) {
1293     b(*L_success);
1294   }
1295 
1296 #undef IS_A_TEMP
1297 
1298   bind(L_fallthrough);
1299 }
1300 
1301 void MacroAssembler::clinit_barrier(Register klass, Register scratch, Label* L_fast_path, Label* L_slow_path) {
1302   assert(L_fast_path != NULL || L_slow_path != NULL, &quot;at least one is required&quot;);
1303   assert_different_registers(klass, rthread, scratch);
1304 
1305   Label L_fallthrough, L_tmp;
1306   if (L_fast_path == NULL) {
1307     L_fast_path = &amp;L_fallthrough;
1308   } else if (L_slow_path == NULL) {
1309     L_slow_path = &amp;L_fallthrough;
1310   }
1311   // Fast path check: class is fully initialized
1312   ldrb(scratch, Address(klass, InstanceKlass::init_state_offset()));
1313   subs(zr, scratch, InstanceKlass::fully_initialized);
1314   br(Assembler::EQ, *L_fast_path);
1315 
1316   // Fast path check: current thread is initializer thread
1317   ldr(scratch, Address(klass, InstanceKlass::init_thread_offset()));
1318   cmp(rthread, scratch);
1319 
1320   if (L_slow_path == &amp;L_fallthrough) {
1321     br(Assembler::EQ, *L_fast_path);
1322     bind(*L_slow_path);
1323   } else if (L_fast_path == &amp;L_fallthrough) {
1324     br(Assembler::NE, *L_slow_path);
1325     bind(*L_fast_path);
1326   } else {
1327     Unimplemented();
1328   }
1329 }
1330 
1331 void MacroAssembler::verify_oop(Register reg, const char* s) {
1332   if (!VerifyOops || VerifyAdapterSharing) {
1333     // Below address of the code string confuses VerifyAdapterSharing
1334     // because it may differ between otherwise equivalent adapters.
1335     return;
1336   }
1337 
1338   // Pass register number to verify_oop_subroutine
1339   const char* b = NULL;
1340   {
1341     ResourceMark rm;
1342     stringStream ss;
1343     ss.print(&quot;verify_oop: %s: %s&quot;, reg-&gt;name(), s);
1344     b = code_string(ss.as_string());
1345   }
1346   BLOCK_COMMENT(&quot;verify_oop {&quot;);
1347 
1348   stp(r0, rscratch1, Address(pre(sp, -2 * wordSize)));
1349   stp(rscratch2, lr, Address(pre(sp, -2 * wordSize)));
1350 
1351   mov(r0, reg);
1352   mov(rscratch1, (address)b);
1353 
1354   // call indirectly to solve generation ordering problem
1355   lea(rscratch2, ExternalAddress(StubRoutines::verify_oop_subroutine_entry_address()));
1356   ldr(rscratch2, Address(rscratch2));
1357   blr(rscratch2);
1358 
1359   ldp(rscratch2, lr, Address(post(sp, 2 * wordSize)));
1360   ldp(r0, rscratch1, Address(post(sp, 2 * wordSize)));
1361 
1362   BLOCK_COMMENT(&quot;} verify_oop&quot;);
1363 }
1364 
1365 void MacroAssembler::verify_oop_addr(Address addr, const char* s) {
1366   if (!VerifyOops || VerifyAdapterSharing) {
1367     // Below address of the code string confuses VerifyAdapterSharing
1368     // because it may differ between otherwise equivalent adapters.
1369     return;
1370   }
1371 
1372   const char* b = NULL;
1373   {
1374     ResourceMark rm;
1375     stringStream ss;
1376     ss.print(&quot;verify_oop_addr: %s&quot;, s);
1377     b = code_string(ss.as_string());
1378   }
1379   BLOCK_COMMENT(&quot;verify_oop_addr {&quot;);
1380 
1381   stp(r0, rscratch1, Address(pre(sp, -2 * wordSize)));
1382   stp(rscratch2, lr, Address(pre(sp, -2 * wordSize)));
1383 
1384   // addr may contain sp so we will have to adjust it based on the
1385   // pushes that we just did.
1386   if (addr.uses(sp)) {
1387     lea(r0, addr);
1388     ldr(r0, Address(r0, 4 * wordSize));
1389   } else {
1390     ldr(r0, addr);
1391   }
1392   mov(rscratch1, (address)b);
1393 
1394   // call indirectly to solve generation ordering problem
1395   lea(rscratch2, ExternalAddress(StubRoutines::verify_oop_subroutine_entry_address()));
1396   ldr(rscratch2, Address(rscratch2));
1397   blr(rscratch2);
1398 
1399   ldp(rscratch2, lr, Address(post(sp, 2 * wordSize)));
1400   ldp(r0, rscratch1, Address(post(sp, 2 * wordSize)));
1401 
1402   BLOCK_COMMENT(&quot;} verify_oop_addr&quot;);
1403 }
1404 
1405 Address MacroAssembler::argument_address(RegisterOrConstant arg_slot,
1406                                          int extra_slot_offset) {
1407   // cf. TemplateTable::prepare_invoke(), if (load_receiver).
1408   int stackElementSize = Interpreter::stackElementSize;
1409   int offset = Interpreter::expr_offset_in_bytes(extra_slot_offset+0);
1410 #ifdef ASSERT
1411   int offset1 = Interpreter::expr_offset_in_bytes(extra_slot_offset+1);
1412   assert(offset1 - offset == stackElementSize, &quot;correct arithmetic&quot;);
1413 #endif
1414   if (arg_slot.is_constant()) {
1415     return Address(esp, arg_slot.as_constant() * stackElementSize
1416                    + offset);
1417   } else {
1418     add(rscratch1, esp, arg_slot.as_register(),
1419         ext::uxtx, exact_log2(stackElementSize));
1420     return Address(rscratch1, offset);
1421   }
1422 }
1423 
1424 void MacroAssembler::call_VM_leaf_base(address entry_point,
1425                                        int number_of_arguments,
1426                                        Label *retaddr) {
1427   Label E, L;
1428 
1429   stp(rscratch1, rmethod, Address(pre(sp, -2 * wordSize)));
1430 
1431   mov(rscratch1, entry_point);
1432   blr(rscratch1);
1433   if (retaddr)
1434     bind(*retaddr);
1435 
1436   ldp(rscratch1, rmethod, Address(post(sp, 2 * wordSize)));
1437   maybe_isb();
1438 }
1439 
1440 void MacroAssembler::call_VM_leaf(address entry_point, int number_of_arguments) {
1441   call_VM_leaf_base(entry_point, number_of_arguments);
1442 }
1443 
1444 void MacroAssembler::call_VM_leaf(address entry_point, Register arg_0) {
1445   pass_arg0(this, arg_0);
1446   call_VM_leaf_base(entry_point, 1);
1447 }
1448 
1449 void MacroAssembler::call_VM_leaf(address entry_point, Register arg_0, Register arg_1) {
1450   pass_arg0(this, arg_0);
1451   pass_arg1(this, arg_1);
1452   call_VM_leaf_base(entry_point, 2);
1453 }
1454 
1455 void MacroAssembler::call_VM_leaf(address entry_point, Register arg_0,
1456                                   Register arg_1, Register arg_2) {
1457   pass_arg0(this, arg_0);
1458   pass_arg1(this, arg_1);
1459   pass_arg2(this, arg_2);
1460   call_VM_leaf_base(entry_point, 3);
1461 }
1462 
1463 void MacroAssembler::super_call_VM_leaf(address entry_point) {
1464   MacroAssembler::call_VM_leaf_base(entry_point, 1);
1465 }
1466 
1467 void MacroAssembler::super_call_VM_leaf(address entry_point, Register arg_0) {
1468   pass_arg0(this, arg_0);
1469   MacroAssembler::call_VM_leaf_base(entry_point, 1);
1470 }
1471 
1472 void MacroAssembler::super_call_VM_leaf(address entry_point, Register arg_0, Register arg_1) {
1473 
1474   assert(arg_0 != c_rarg1, &quot;smashed arg&quot;);
1475   pass_arg1(this, arg_1);
1476   pass_arg0(this, arg_0);
1477   MacroAssembler::call_VM_leaf_base(entry_point, 2);
1478 }
1479 
1480 void MacroAssembler::super_call_VM_leaf(address entry_point, Register arg_0, Register arg_1, Register arg_2) {
1481   assert(arg_0 != c_rarg2, &quot;smashed arg&quot;);
1482   assert(arg_1 != c_rarg2, &quot;smashed arg&quot;);
1483   pass_arg2(this, arg_2);
1484   assert(arg_0 != c_rarg1, &quot;smashed arg&quot;);
1485   pass_arg1(this, arg_1);
1486   pass_arg0(this, arg_0);
1487   MacroAssembler::call_VM_leaf_base(entry_point, 3);
1488 }
1489 
1490 void MacroAssembler::super_call_VM_leaf(address entry_point, Register arg_0, Register arg_1, Register arg_2, Register arg_3) {
1491   assert(arg_0 != c_rarg3, &quot;smashed arg&quot;);
1492   assert(arg_1 != c_rarg3, &quot;smashed arg&quot;);
1493   assert(arg_2 != c_rarg3, &quot;smashed arg&quot;);
1494   pass_arg3(this, arg_3);
1495   assert(arg_0 != c_rarg2, &quot;smashed arg&quot;);
1496   assert(arg_1 != c_rarg2, &quot;smashed arg&quot;);
1497   pass_arg2(this, arg_2);
1498   assert(arg_0 != c_rarg1, &quot;smashed arg&quot;);
1499   pass_arg1(this, arg_1);
1500   pass_arg0(this, arg_0);
1501   MacroAssembler::call_VM_leaf_base(entry_point, 4);
1502 }
1503 
1504 void MacroAssembler::null_check(Register reg, int offset) {
1505   if (needs_explicit_null_check(offset)) {
1506     // provoke OS NULL exception if reg = NULL by
1507     // accessing M[reg] w/o changing any registers
1508     // NOTE: this is plenty to provoke a segv
1509     ldr(zr, Address(reg));
1510   } else {
1511     // nothing to do, (later) access of M[reg + offset]
1512     // will provoke OS NULL exception if reg = NULL
1513   }
1514 }
1515 
1516 void MacroAssembler::test_klass_is_value(Register klass, Register temp_reg, Label&amp; is_value) {
1517   ldrw(temp_reg, Address(klass, Klass::access_flags_offset()));
1518   andr(temp_reg, temp_reg, JVM_ACC_VALUE);
1519   cbnz(temp_reg, is_value);
1520 }
1521 
1522 void MacroAssembler::test_field_is_flattenable(Register flags, Register temp_reg, Label&amp; is_flattenable) {
1523   (void) temp_reg; // keep signature uniform with x86
1524   tbnz(flags, ConstantPoolCacheEntry::is_flattenable_field_shift, is_flattenable);
1525 }
1526 
1527 void MacroAssembler::test_field_is_not_flattenable(Register flags, Register temp_reg, Label&amp; not_flattenable) {
1528   (void) temp_reg; // keep signature uniform with x86
1529   tbz(flags, ConstantPoolCacheEntry::is_flattenable_field_shift, not_flattenable);
1530 }
1531 
1532 void MacroAssembler::test_field_is_flattened(Register flags, Register temp_reg, Label&amp; is_flattened) {
1533   (void) temp_reg; // keep signature uniform with x86
1534   tbnz(flags, ConstantPoolCacheEntry::is_flattened_field_shift, is_flattened);
1535 }
1536 
1537 void MacroAssembler::test_flattened_array_oop(Register oop, Register temp_reg, Label&amp; is_flattened_array) {
1538   load_storage_props(temp_reg, oop);
1539   andr(temp_reg, temp_reg, ArrayStorageProperties::flattened_value);
1540   cbnz(temp_reg, is_flattened_array);
1541 }
1542 
1543 void MacroAssembler::test_null_free_array_oop(Register oop, Register temp_reg, Label&amp; is_null_free_array) {
1544   load_storage_props(temp_reg, oop);
1545   andr(temp_reg, temp_reg, ArrayStorageProperties::null_free_value);
1546   cbnz(temp_reg, is_null_free_array);
1547 }
1548 
1549 // MacroAssembler protected routines needed to implement
1550 // public methods
1551 
1552 void MacroAssembler::mov(Register r, Address dest) {
1553   code_section()-&gt;relocate(pc(), dest.rspec());
1554   u_int64_t imm64 = (u_int64_t)dest.target();
1555   movptr(r, imm64);
1556 }
1557 
1558 // Move a constant pointer into r.  In AArch64 mode the virtual
1559 // address space is 48 bits in size, so we only need three
1560 // instructions to create a patchable instruction sequence that can
1561 // reach anywhere.
1562 void MacroAssembler::movptr(Register r, uintptr_t imm64) {
1563 #ifndef PRODUCT
1564   {
1565     char buffer[64];
1566     snprintf(buffer, sizeof(buffer), &quot;0x%&quot; PRIX64, imm64);
1567     block_comment(buffer);
1568   }
1569 #endif
1570   assert(imm64 &lt; (1ul &lt;&lt; 48), &quot;48-bit overflow in address constant&quot;);
1571   movz(r, imm64 &amp; 0xffff);
1572   imm64 &gt;&gt;= 16;
1573   movk(r, imm64 &amp; 0xffff, 16);
1574   imm64 &gt;&gt;= 16;
1575   movk(r, imm64 &amp; 0xffff, 32);
1576 }
1577 
1578 // Macro to mov replicated immediate to vector register.
1579 //  Vd will get the following values for different arrangements in T
1580 //   imm32 == hex 000000gh  T8B:  Vd = ghghghghghghghgh
1581 //   imm32 == hex 000000gh  T16B: Vd = ghghghghghghghghghghghghghghghgh
1582 //   imm32 == hex 0000efgh  T4H:  Vd = efghefghefghefgh
1583 //   imm32 == hex 0000efgh  T8H:  Vd = efghefghefghefghefghefghefghefgh
1584 //   imm32 == hex abcdefgh  T2S:  Vd = abcdefghabcdefgh
1585 //   imm32 == hex abcdefgh  T4S:  Vd = abcdefghabcdefghabcdefghabcdefgh
1586 //   T1D/T2D: invalid
1587 void MacroAssembler::mov(FloatRegister Vd, SIMD_Arrangement T, u_int32_t imm32) {
1588   assert(T != T1D &amp;&amp; T != T2D, &quot;invalid arrangement&quot;);
1589   if (T == T8B || T == T16B) {
1590     assert((imm32 &amp; ~0xff) == 0, &quot;extraneous bits in unsigned imm32 (T8B/T16B)&quot;);
1591     movi(Vd, T, imm32 &amp; 0xff, 0);
1592     return;
1593   }
1594   u_int32_t nimm32 = ~imm32;
1595   if (T == T4H || T == T8H) {
1596     assert((imm32  &amp; ~0xffff) == 0, &quot;extraneous bits in unsigned imm32 (T4H/T8H)&quot;);
1597     imm32 &amp;= 0xffff;
1598     nimm32 &amp;= 0xffff;
1599   }
1600   u_int32_t x = imm32;
1601   int movi_cnt = 0;
1602   int movn_cnt = 0;
1603   while (x) { if (x &amp; 0xff) movi_cnt++; x &gt;&gt;= 8; }
1604   x = nimm32;
1605   while (x) { if (x &amp; 0xff) movn_cnt++; x &gt;&gt;= 8; }
1606   if (movn_cnt &lt; movi_cnt) imm32 = nimm32;
1607   unsigned lsl = 0;
1608   while (imm32 &amp;&amp; (imm32 &amp; 0xff) == 0) { lsl += 8; imm32 &gt;&gt;= 8; }
1609   if (movn_cnt &lt; movi_cnt)
1610     mvni(Vd, T, imm32 &amp; 0xff, lsl);
1611   else
1612     movi(Vd, T, imm32 &amp; 0xff, lsl);
1613   imm32 &gt;&gt;= 8; lsl += 8;
1614   while (imm32) {
1615     while ((imm32 &amp; 0xff) == 0) { lsl += 8; imm32 &gt;&gt;= 8; }
1616     if (movn_cnt &lt; movi_cnt)
1617       bici(Vd, T, imm32 &amp; 0xff, lsl);
1618     else
1619       orri(Vd, T, imm32 &amp; 0xff, lsl);
1620     lsl += 8; imm32 &gt;&gt;= 8;
1621   }
1622 }
1623 
1624 void MacroAssembler::mov_immediate64(Register dst, u_int64_t imm64)
1625 {
1626 #ifndef PRODUCT
1627   {
1628     char buffer[64];
1629     snprintf(buffer, sizeof(buffer), &quot;0x%&quot; PRIX64, imm64);
1630     block_comment(buffer);
1631   }
1632 #endif
1633   if (operand_valid_for_logical_immediate(false, imm64)) {
1634     orr(dst, zr, imm64);
1635   } else {
1636     // we can use a combination of MOVZ or MOVN with
1637     // MOVK to build up the constant
1638     u_int64_t imm_h[4];
1639     int zero_count = 0;
1640     int neg_count = 0;
1641     int i;
1642     for (i = 0; i &lt; 4; i++) {
1643       imm_h[i] = ((imm64 &gt;&gt; (i * 16)) &amp; 0xffffL);
1644       if (imm_h[i] == 0) {
1645         zero_count++;
1646       } else if (imm_h[i] == 0xffffL) {
1647         neg_count++;
1648       }
1649     }
1650     if (zero_count == 4) {
1651       // one MOVZ will do
1652       movz(dst, 0);
1653     } else if (neg_count == 4) {
1654       // one MOVN will do
1655       movn(dst, 0);
1656     } else if (zero_count == 3) {
1657       for (i = 0; i &lt; 4; i++) {
1658         if (imm_h[i] != 0L) {
1659           movz(dst, (u_int32_t)imm_h[i], (i &lt;&lt; 4));
1660           break;
1661         }
1662       }
1663     } else if (neg_count == 3) {
1664       // one MOVN will do
1665       for (int i = 0; i &lt; 4; i++) {
1666         if (imm_h[i] != 0xffffL) {
1667           movn(dst, (u_int32_t)imm_h[i] ^ 0xffffL, (i &lt;&lt; 4));
1668           break;
1669         }
1670       }
1671     } else if (zero_count == 2) {
1672       // one MOVZ and one MOVK will do
1673       for (i = 0; i &lt; 3; i++) {
1674         if (imm_h[i] != 0L) {
1675           movz(dst, (u_int32_t)imm_h[i], (i &lt;&lt; 4));
1676           i++;
1677           break;
1678         }
1679       }
1680       for (;i &lt; 4; i++) {
1681         if (imm_h[i] != 0L) {
1682           movk(dst, (u_int32_t)imm_h[i], (i &lt;&lt; 4));
1683         }
1684       }
1685     } else if (neg_count == 2) {
1686       // one MOVN and one MOVK will do
1687       for (i = 0; i &lt; 4; i++) {
1688         if (imm_h[i] != 0xffffL) {
1689           movn(dst, (u_int32_t)imm_h[i] ^ 0xffffL, (i &lt;&lt; 4));
1690           i++;
1691           break;
1692         }
1693       }
1694       for (;i &lt; 4; i++) {
1695         if (imm_h[i] != 0xffffL) {
1696           movk(dst, (u_int32_t)imm_h[i], (i &lt;&lt; 4));
1697         }
1698       }
1699     } else if (zero_count == 1) {
1700       // one MOVZ and two MOVKs will do
1701       for (i = 0; i &lt; 4; i++) {
1702         if (imm_h[i] != 0L) {
1703           movz(dst, (u_int32_t)imm_h[i], (i &lt;&lt; 4));
1704           i++;
1705           break;
1706         }
1707       }
1708       for (;i &lt; 4; i++) {
1709         if (imm_h[i] != 0x0L) {
1710           movk(dst, (u_int32_t)imm_h[i], (i &lt;&lt; 4));
1711         }
1712       }
1713     } else if (neg_count == 1) {
1714       // one MOVN and two MOVKs will do
1715       for (i = 0; i &lt; 4; i++) {
1716         if (imm_h[i] != 0xffffL) {
1717           movn(dst, (u_int32_t)imm_h[i] ^ 0xffffL, (i &lt;&lt; 4));
1718           i++;
1719           break;
1720         }
1721       }
1722       for (;i &lt; 4; i++) {
1723         if (imm_h[i] != 0xffffL) {
1724           movk(dst, (u_int32_t)imm_h[i], (i &lt;&lt; 4));
1725         }
1726       }
1727     } else {
1728       // use a MOVZ and 3 MOVKs (makes it easier to debug)
1729       movz(dst, (u_int32_t)imm_h[0], 0);
1730       for (i = 1; i &lt; 4; i++) {
1731         movk(dst, (u_int32_t)imm_h[i], (i &lt;&lt; 4));
1732       }
1733     }
1734   }
1735 }
1736 
1737 void MacroAssembler::mov_immediate32(Register dst, u_int32_t imm32)
1738 {
1739 #ifndef PRODUCT
1740     {
1741       char buffer[64];
1742       snprintf(buffer, sizeof(buffer), &quot;0x%&quot; PRIX32, imm32);
1743       block_comment(buffer);
1744     }
1745 #endif
1746   if (operand_valid_for_logical_immediate(true, imm32)) {
1747     orrw(dst, zr, imm32);
1748   } else {
1749     // we can use MOVZ, MOVN or two calls to MOVK to build up the
1750     // constant
1751     u_int32_t imm_h[2];
1752     imm_h[0] = imm32 &amp; 0xffff;
1753     imm_h[1] = ((imm32 &gt;&gt; 16) &amp; 0xffff);
1754     if (imm_h[0] == 0) {
1755       movzw(dst, imm_h[1], 16);
1756     } else if (imm_h[0] == 0xffff) {
1757       movnw(dst, imm_h[1] ^ 0xffff, 16);
1758     } else if (imm_h[1] == 0) {
1759       movzw(dst, imm_h[0], 0);
1760     } else if (imm_h[1] == 0xffff) {
1761       movnw(dst, imm_h[0] ^ 0xffff, 0);
1762     } else {
1763       // use a MOVZ and MOVK (makes it easier to debug)
1764       movzw(dst, imm_h[0], 0);
1765       movkw(dst, imm_h[1], 16);
1766     }
1767   }
1768 }
1769 
1770 // Form an address from base + offset in Rd.  Rd may or may
1771 // not actually be used: you must use the Address that is returned.
1772 // It is up to you to ensure that the shift provided matches the size
1773 // of your data.
1774 Address MacroAssembler::form_address(Register Rd, Register base, long byte_offset, int shift) {
1775   if (Address::offset_ok_for_immed(byte_offset, shift))
1776     // It fits; no need for any heroics
1777     return Address(base, byte_offset);
1778 
1779   // Don&#39;t do anything clever with negative or misaligned offsets
1780   unsigned mask = (1 &lt;&lt; shift) - 1;
1781   if (byte_offset &lt; 0 || byte_offset &amp; mask) {
1782     mov(Rd, byte_offset);
1783     add(Rd, base, Rd);
1784     return Address(Rd);
1785   }
1786 
1787   // See if we can do this with two 12-bit offsets
1788   {
1789     unsigned long word_offset = byte_offset &gt;&gt; shift;
1790     unsigned long masked_offset = word_offset &amp; 0xfff000;
1791     if (Address::offset_ok_for_immed(word_offset - masked_offset, 0)
1792         &amp;&amp; Assembler::operand_valid_for_add_sub_immediate(masked_offset &lt;&lt; shift)) {
1793       add(Rd, base, masked_offset &lt;&lt; shift);
1794       word_offset -= masked_offset;
1795       return Address(Rd, word_offset &lt;&lt; shift);
1796     }
1797   }
1798 
1799   // Do it the hard way
1800   mov(Rd, byte_offset);
1801   add(Rd, base, Rd);
1802   return Address(Rd);
1803 }
1804 
1805 void MacroAssembler::atomic_incw(Register counter_addr, Register tmp, Register tmp2) {
1806   if (UseLSE) {
1807     mov(tmp, 1);
1808     ldadd(Assembler::word, tmp, zr, counter_addr);
1809     return;
1810   }
1811   Label retry_load;
1812   if ((VM_Version::features() &amp; VM_Version::CPU_STXR_PREFETCH))
1813     prfm(Address(counter_addr), PSTL1STRM);
1814   bind(retry_load);
1815   // flush and load exclusive from the memory location
1816   ldxrw(tmp, counter_addr);
1817   addw(tmp, tmp, 1);
1818   // if we store+flush with no intervening write tmp wil be zero
1819   stxrw(tmp2, tmp, counter_addr);
1820   cbnzw(tmp2, retry_load);
1821 }
1822 
1823 
1824 int MacroAssembler::corrected_idivl(Register result, Register ra, Register rb,
1825                                     bool want_remainder, Register scratch)
1826 {
1827   // Full implementation of Java idiv and irem.  The function
1828   // returns the (pc) offset of the div instruction - may be needed
1829   // for implicit exceptions.
1830   //
1831   // constraint : ra/rb =/= scratch
1832   //         normal case
1833   //
1834   // input : ra: dividend
1835   //         rb: divisor
1836   //
1837   // result: either
1838   //         quotient  (= ra idiv rb)
1839   //         remainder (= ra irem rb)
1840 
1841   assert(ra != scratch &amp;&amp; rb != scratch, &quot;reg cannot be scratch&quot;);
1842 
1843   int idivl_offset = offset();
1844   if (! want_remainder) {
1845     sdivw(result, ra, rb);
1846   } else {
1847     sdivw(scratch, ra, rb);
1848     Assembler::msubw(result, scratch, rb, ra);
1849   }
1850 
1851   return idivl_offset;
1852 }
1853 
1854 int MacroAssembler::corrected_idivq(Register result, Register ra, Register rb,
1855                                     bool want_remainder, Register scratch)
1856 {
1857   // Full implementation of Java ldiv and lrem.  The function
1858   // returns the (pc) offset of the div instruction - may be needed
1859   // for implicit exceptions.
1860   //
1861   // constraint : ra/rb =/= scratch
1862   //         normal case
1863   //
1864   // input : ra: dividend
1865   //         rb: divisor
1866   //
1867   // result: either
1868   //         quotient  (= ra idiv rb)
1869   //         remainder (= ra irem rb)
1870 
1871   assert(ra != scratch &amp;&amp; rb != scratch, &quot;reg cannot be scratch&quot;);
1872 
1873   int idivq_offset = offset();
1874   if (! want_remainder) {
1875     sdiv(result, ra, rb);
1876   } else {
1877     sdiv(scratch, ra, rb);
1878     Assembler::msub(result, scratch, rb, ra);
1879   }
1880 
1881   return idivq_offset;
1882 }
1883 
1884 void MacroAssembler::membar(Membar_mask_bits order_constraint) {
1885   address prev = pc() - NativeMembar::instruction_size;
1886   address last = code()-&gt;last_insn();
1887   if (last != NULL &amp;&amp; nativeInstruction_at(last)-&gt;is_Membar() &amp;&amp; prev == last) {
1888     NativeMembar *bar = NativeMembar_at(prev);
1889     // We are merging two memory barrier instructions.  On AArch64 we
1890     // can do this simply by ORing them together.
1891     bar-&gt;set_kind(bar-&gt;get_kind() | order_constraint);
1892     BLOCK_COMMENT(&quot;merged membar&quot;);
1893   } else {
1894     code()-&gt;set_last_insn(pc());
1895     dmb(Assembler::barrier(order_constraint));
1896   }
1897 }
1898 
1899 bool MacroAssembler::try_merge_ldst(Register rt, const Address &amp;adr, size_t size_in_bytes, bool is_store) {
1900   if (ldst_can_merge(rt, adr, size_in_bytes, is_store)) {
1901     merge_ldst(rt, adr, size_in_bytes, is_store);
1902     code()-&gt;clear_last_insn();
1903     return true;
1904   } else {
1905     assert(size_in_bytes == 8 || size_in_bytes == 4, &quot;only 8 bytes or 4 bytes load/store is supported.&quot;);
1906     const unsigned mask = size_in_bytes - 1;
1907     if (adr.getMode() == Address::base_plus_offset &amp;&amp;
1908         (adr.offset() &amp; mask) == 0) { // only supports base_plus_offset.
1909       code()-&gt;set_last_insn(pc());
1910     }
1911     return false;
1912   }
1913 }
1914 
1915 void MacroAssembler::ldr(Register Rx, const Address &amp;adr) {
1916   // We always try to merge two adjacent loads into one ldp.
1917   if (!try_merge_ldst(Rx, adr, 8, false)) {
1918     Assembler::ldr(Rx, adr);
1919   }
1920 }
1921 
1922 void MacroAssembler::ldrw(Register Rw, const Address &amp;adr) {
1923   // We always try to merge two adjacent loads into one ldp.
1924   if (!try_merge_ldst(Rw, adr, 4, false)) {
1925     Assembler::ldrw(Rw, adr);
1926   }
1927 }
1928 
1929 void MacroAssembler::str(Register Rx, const Address &amp;adr) {
1930   // We always try to merge two adjacent stores into one stp.
1931   if (!try_merge_ldst(Rx, adr, 8, true)) {
1932     Assembler::str(Rx, adr);
1933   }
1934 }
1935 
1936 void MacroAssembler::strw(Register Rw, const Address &amp;adr) {
1937   // We always try to merge two adjacent stores into one stp.
1938   if (!try_merge_ldst(Rw, adr, 4, true)) {
1939     Assembler::strw(Rw, adr);
1940   }
1941 }
1942 
1943 // MacroAssembler routines found actually to be needed
1944 
1945 void MacroAssembler::push(Register src)
1946 {
1947   str(src, Address(pre(esp, -1 * wordSize)));
1948 }
1949 
1950 void MacroAssembler::pop(Register dst)
1951 {
1952   ldr(dst, Address(post(esp, 1 * wordSize)));
1953 }
1954 
1955 // Note: load_unsigned_short used to be called load_unsigned_word.
1956 int MacroAssembler::load_unsigned_short(Register dst, Address src) {
1957   int off = offset();
1958   ldrh(dst, src);
1959   return off;
1960 }
1961 
1962 int MacroAssembler::load_unsigned_byte(Register dst, Address src) {
1963   int off = offset();
1964   ldrb(dst, src);
1965   return off;
1966 }
1967 
1968 int MacroAssembler::load_signed_short(Register dst, Address src) {
1969   int off = offset();
1970   ldrsh(dst, src);
1971   return off;
1972 }
1973 
1974 int MacroAssembler::load_signed_byte(Register dst, Address src) {
1975   int off = offset();
1976   ldrsb(dst, src);
1977   return off;
1978 }
1979 
1980 int MacroAssembler::load_signed_short32(Register dst, Address src) {
1981   int off = offset();
1982   ldrshw(dst, src);
1983   return off;
1984 }
1985 
1986 int MacroAssembler::load_signed_byte32(Register dst, Address src) {
1987   int off = offset();
1988   ldrsbw(dst, src);
1989   return off;
1990 }
1991 
1992 void MacroAssembler::load_sized_value(Register dst, Address src, size_t size_in_bytes, bool is_signed, Register dst2) {
1993   switch (size_in_bytes) {
1994   case  8:  ldr(dst, src); break;
1995   case  4:  ldrw(dst, src); break;
1996   case  2:  is_signed ? load_signed_short(dst, src) : load_unsigned_short(dst, src); break;
1997   case  1:  is_signed ? load_signed_byte( dst, src) : load_unsigned_byte( dst, src); break;
1998   default:  ShouldNotReachHere();
1999   }
2000 }
2001 
2002 void MacroAssembler::store_sized_value(Address dst, Register src, size_t size_in_bytes, Register src2) {
2003   switch (size_in_bytes) {
2004   case  8:  str(src, dst); break;
2005   case  4:  strw(src, dst); break;
2006   case  2:  strh(src, dst); break;
2007   case  1:  strb(src, dst); break;
2008   default:  ShouldNotReachHere();
2009   }
2010 }
2011 
2012 void MacroAssembler::decrementw(Register reg, int value)
2013 {
2014   if (value &lt; 0)  { incrementw(reg, -value);      return; }
2015   if (value == 0) {                               return; }
2016   if (value &lt; (1 &lt;&lt; 12)) { subw(reg, reg, value); return; }
2017   /* else */ {
2018     guarantee(reg != rscratch2, &quot;invalid dst for register decrement&quot;);
2019     movw(rscratch2, (unsigned)value);
2020     subw(reg, reg, rscratch2);
2021   }
2022 }
2023 
2024 void MacroAssembler::decrement(Register reg, int value)
2025 {
2026   if (value &lt; 0)  { increment(reg, -value);      return; }
2027   if (value == 0) {                              return; }
2028   if (value &lt; (1 &lt;&lt; 12)) { sub(reg, reg, value); return; }
2029   /* else */ {
2030     assert(reg != rscratch2, &quot;invalid dst for register decrement&quot;);
2031     mov(rscratch2, (unsigned long)value);
2032     sub(reg, reg, rscratch2);
2033   }
2034 }
2035 
2036 void MacroAssembler::decrementw(Address dst, int value)
2037 {
2038   assert(!dst.uses(rscratch1), &quot;invalid dst for address decrement&quot;);
2039   if (dst.getMode() == Address::literal) {
2040     assert(abs(value) &lt; (1 &lt;&lt; 12), &quot;invalid value and address mode combination&quot;);
2041     lea(rscratch2, dst);
2042     dst = Address(rscratch2);
2043   }
2044   ldrw(rscratch1, dst);
2045   decrementw(rscratch1, value);
2046   strw(rscratch1, dst);
2047 }
2048 
2049 void MacroAssembler::decrement(Address dst, int value)
2050 {
2051   assert(!dst.uses(rscratch1), &quot;invalid address for decrement&quot;);
2052   if (dst.getMode() == Address::literal) {
2053     assert(abs(value) &lt; (1 &lt;&lt; 12), &quot;invalid value and address mode combination&quot;);
2054     lea(rscratch2, dst);
2055     dst = Address(rscratch2);
2056   }
2057   ldr(rscratch1, dst);
2058   decrement(rscratch1, value);
2059   str(rscratch1, dst);
2060 }
2061 
2062 void MacroAssembler::incrementw(Register reg, int value)
2063 {
2064   if (value &lt; 0)  { decrementw(reg, -value);      return; }
2065   if (value == 0) {                               return; }
2066   if (value &lt; (1 &lt;&lt; 12)) { addw(reg, reg, value); return; }
2067   /* else */ {
2068     assert(reg != rscratch2, &quot;invalid dst for register increment&quot;);
2069     movw(rscratch2, (unsigned)value);
2070     addw(reg, reg, rscratch2);
2071   }
2072 }
2073 
2074 void MacroAssembler::increment(Register reg, int value)
2075 {
2076   if (value &lt; 0)  { decrement(reg, -value);      return; }
2077   if (value == 0) {                              return; }
2078   if (value &lt; (1 &lt;&lt; 12)) { add(reg, reg, value); return; }
2079   /* else */ {
2080     assert(reg != rscratch2, &quot;invalid dst for register increment&quot;);
2081     movw(rscratch2, (unsigned)value);
2082     add(reg, reg, rscratch2);
2083   }
2084 }
2085 
2086 void MacroAssembler::incrementw(Address dst, int value)
2087 {
2088   assert(!dst.uses(rscratch1), &quot;invalid dst for address increment&quot;);
2089   if (dst.getMode() == Address::literal) {
2090     assert(abs(value) &lt; (1 &lt;&lt; 12), &quot;invalid value and address mode combination&quot;);
2091     lea(rscratch2, dst);
2092     dst = Address(rscratch2);
2093   }
2094   ldrw(rscratch1, dst);
2095   incrementw(rscratch1, value);
2096   strw(rscratch1, dst);
2097 }
2098 
2099 void MacroAssembler::increment(Address dst, int value)
2100 {
2101   assert(!dst.uses(rscratch1), &quot;invalid dst for address increment&quot;);
2102   if (dst.getMode() == Address::literal) {
2103     assert(abs(value) &lt; (1 &lt;&lt; 12), &quot;invalid value and address mode combination&quot;);
2104     lea(rscratch2, dst);
2105     dst = Address(rscratch2);
2106   }
2107   ldr(rscratch1, dst);
2108   increment(rscratch1, value);
2109   str(rscratch1, dst);
2110 }
2111 
2112 
2113 void MacroAssembler::pusha() {
2114   push(0x7fffffff, sp);
2115 }
2116 
2117 void MacroAssembler::popa() {
2118   pop(0x7fffffff, sp);
2119 }
2120 
2121 // Push lots of registers in the bit set supplied.  Don&#39;t push sp.
2122 // Return the number of words pushed
2123 int MacroAssembler::push(unsigned int bitset, Register stack) {
2124   int words_pushed = 0;
2125 
2126   // Scan bitset to accumulate register pairs
2127   unsigned char regs[32];
2128   int count = 0;
2129   for (int reg = 0; reg &lt;= 30; reg++) {
2130     if (1 &amp; bitset)
2131       regs[count++] = reg;
2132     bitset &gt;&gt;= 1;
2133   }
2134   regs[count++] = zr-&gt;encoding_nocheck();
2135   count &amp;= ~1;  // Only push an even nuber of regs
2136 
2137   if (count) {
2138     stp(as_Register(regs[0]), as_Register(regs[1]),
2139        Address(pre(stack, -count * wordSize)));
2140     words_pushed += 2;
2141   }
2142   for (int i = 2; i &lt; count; i += 2) {
2143     stp(as_Register(regs[i]), as_Register(regs[i+1]),
2144        Address(stack, i * wordSize));
2145     words_pushed += 2;
2146   }
2147 
2148   assert(words_pushed == count, &quot;oops, pushed != count&quot;);
2149 
2150   return count;
2151 }
2152 
2153 int MacroAssembler::pop(unsigned int bitset, Register stack) {
2154   int words_pushed = 0;
2155 
2156   // Scan bitset to accumulate register pairs
2157   unsigned char regs[32];
2158   int count = 0;
2159   for (int reg = 0; reg &lt;= 30; reg++) {
2160     if (1 &amp; bitset)
2161       regs[count++] = reg;
2162     bitset &gt;&gt;= 1;
2163   }
2164   regs[count++] = zr-&gt;encoding_nocheck();
2165   count &amp;= ~1;
2166 
2167   for (int i = 2; i &lt; count; i += 2) {
2168     ldp(as_Register(regs[i]), as_Register(regs[i+1]),
2169        Address(stack, i * wordSize));
2170     words_pushed += 2;
2171   }
2172   if (count) {
2173     ldp(as_Register(regs[0]), as_Register(regs[1]),
2174        Address(post(stack, count * wordSize)));
2175     words_pushed += 2;
2176   }
2177 
2178   assert(words_pushed == count, &quot;oops, pushed != count&quot;);
2179 
2180   return count;
2181 }
2182 
2183 // Push lots of registers in the bit set supplied.  Don&#39;t push sp.
2184 // Return the number of words pushed
2185 int MacroAssembler::push_fp(unsigned int bitset, Register stack) {
2186   int words_pushed = 0;
2187 
2188   // Scan bitset to accumulate register pairs
2189   unsigned char regs[32];
2190   int count = 0;
2191   for (int reg = 0; reg &lt;= 31; reg++) {
2192     if (1 &amp; bitset)
2193       regs[count++] = reg;
2194     bitset &gt;&gt;= 1;
2195   }
2196   regs[count++] = zr-&gt;encoding_nocheck();
2197   count &amp;= ~1;  // Only push an even number of regs
2198 
2199   // Always pushing full 128 bit registers.
2200   if (count) {
2201     stpq(as_FloatRegister(regs[0]), as_FloatRegister(regs[1]), Address(pre(stack, -count * wordSize * 2)));
2202     words_pushed += 2;
2203   }
2204   for (int i = 2; i &lt; count; i += 2) {
2205     stpq(as_FloatRegister(regs[i]), as_FloatRegister(regs[i+1]), Address(stack, i * wordSize * 2));
2206     words_pushed += 2;
2207   }
2208 
2209   assert(words_pushed == count, &quot;oops, pushed != count&quot;);
2210   return count;
2211 }
2212 
2213 int MacroAssembler::pop_fp(unsigned int bitset, Register stack) {
2214   int words_pushed = 0;
2215 
2216   // Scan bitset to accumulate register pairs
2217   unsigned char regs[32];
2218   int count = 0;
2219   for (int reg = 0; reg &lt;= 31; reg++) {
2220     if (1 &amp; bitset)
2221       regs[count++] = reg;
2222     bitset &gt;&gt;= 1;
2223   }
2224   regs[count++] = zr-&gt;encoding_nocheck();
2225   count &amp;= ~1;
2226 
2227   for (int i = 2; i &lt; count; i += 2) {
2228     ldpq(as_FloatRegister(regs[i]), as_FloatRegister(regs[i+1]), Address(stack, i * wordSize * 2));
2229     words_pushed += 2;
2230   }
2231   if (count) {
2232     ldpq(as_FloatRegister(regs[0]), as_FloatRegister(regs[1]), Address(post(stack, count * wordSize * 2)));
2233     words_pushed += 2;
2234   }
2235 
2236   assert(words_pushed == count, &quot;oops, pushed != count&quot;);
2237 
2238   return count;
2239 }
2240 
2241 #ifdef ASSERT
2242 void MacroAssembler::verify_heapbase(const char* msg) {
2243 #if 0
2244   assert (UseCompressedOops || UseCompressedClassPointers, &quot;should be compressed&quot;);
2245   assert (Universe::heap() != NULL, &quot;java heap should be initialized&quot;);
2246   if (CheckCompressedOops) {
2247     Label ok;
2248     push(1 &lt;&lt; rscratch1-&gt;encoding(), sp); // cmpptr trashes rscratch1
2249     cmpptr(rheapbase, ExternalAddress((address)CompressedOops::ptrs_base_addr()));
2250     br(Assembler::EQ, ok);
2251     stop(msg);
2252     bind(ok);
2253     pop(1 &lt;&lt; rscratch1-&gt;encoding(), sp);
2254   }
2255 #endif
2256 }
2257 #endif
2258 
2259 void MacroAssembler::resolve_jobject(Register value, Register thread, Register tmp) {
2260   Label done, not_weak;
2261   cbz(value, done);           // Use NULL as-is.
2262 
2263   STATIC_ASSERT(JNIHandles::weak_tag_mask == 1u);
2264   tbz(r0, 0, not_weak);    // Test for jweak tag.
2265 
2266   // Resolve jweak.
2267   access_load_at(T_OBJECT, IN_NATIVE | ON_PHANTOM_OOP_REF, value,
2268                  Address(value, -JNIHandles::weak_tag_value), tmp, thread);
2269   verify_oop(value);
2270   b(done);
2271 
2272   bind(not_weak);
2273   // Resolve (untagged) jobject.
2274   access_load_at(T_OBJECT, IN_NATIVE, value, Address(value, 0), tmp, thread);
2275   verify_oop(value);
2276   bind(done);
2277 }
2278 
2279 void MacroAssembler::stop(const char* msg) {
2280   address ip = pc();
2281   pusha();
2282   mov(c_rarg0, (address)msg);
2283   mov(c_rarg1, (address)ip);
2284   mov(c_rarg2, sp);
2285   mov(c_rarg3, CAST_FROM_FN_PTR(address, MacroAssembler::debug64));
2286   blr(c_rarg3);
2287   hlt(0);
2288 }
2289 
2290 void MacroAssembler::warn(const char* msg) {
2291   pusha();
2292   mov(c_rarg0, (address)msg);
2293   mov(lr, CAST_FROM_FN_PTR(address, warning));
2294   blr(lr);
2295   popa();
2296 }
2297 
2298 void MacroAssembler::unimplemented(const char* what) {
2299   const char* buf = NULL;
2300   {
2301     ResourceMark rm;
2302     stringStream ss;
2303     ss.print(&quot;unimplemented: %s&quot;, what);
2304     buf = code_string(ss.as_string());
2305   }
2306   stop(buf);
2307 }
2308 
2309 // If a constant does not fit in an immediate field, generate some
2310 // number of MOV instructions and then perform the operation.
2311 void MacroAssembler::wrap_add_sub_imm_insn(Register Rd, Register Rn, unsigned imm,
2312                                            add_sub_imm_insn insn1,
2313                                            add_sub_reg_insn insn2) {
2314   assert(Rd != zr, &quot;Rd = zr and not setting flags?&quot;);
2315   if (operand_valid_for_add_sub_immediate((int)imm)) {
2316     (this-&gt;*insn1)(Rd, Rn, imm);
2317   } else {
2318     if (uabs(imm) &lt; (1 &lt;&lt; 24)) {
2319        (this-&gt;*insn1)(Rd, Rn, imm &amp; -(1 &lt;&lt; 12));
2320        (this-&gt;*insn1)(Rd, Rd, imm &amp; ((1 &lt;&lt; 12)-1));
2321     } else {
2322        assert_different_registers(Rd, Rn);
2323        mov(Rd, (uint64_t)imm);
2324        (this-&gt;*insn2)(Rd, Rn, Rd, LSL, 0);
2325     }
2326   }
2327 }
2328 
2329 // Seperate vsn which sets the flags. Optimisations are more restricted
2330 // because we must set the flags correctly.
2331 void MacroAssembler::wrap_adds_subs_imm_insn(Register Rd, Register Rn, unsigned imm,
2332                                            add_sub_imm_insn insn1,
2333                                            add_sub_reg_insn insn2) {
2334   if (operand_valid_for_add_sub_immediate((int)imm)) {
2335     (this-&gt;*insn1)(Rd, Rn, imm);
2336   } else {
2337     assert_different_registers(Rd, Rn);
2338     assert(Rd != zr, &quot;overflow in immediate operand&quot;);
2339     mov(Rd, (uint64_t)imm);
2340     (this-&gt;*insn2)(Rd, Rn, Rd, LSL, 0);
2341   }
2342 }
2343 
2344 
2345 void MacroAssembler::add(Register Rd, Register Rn, RegisterOrConstant increment) {
2346   if (increment.is_register()) {
2347     add(Rd, Rn, increment.as_register());
2348   } else {
2349     add(Rd, Rn, increment.as_constant());
2350   }
2351 }
2352 
2353 void MacroAssembler::addw(Register Rd, Register Rn, RegisterOrConstant increment) {
2354   if (increment.is_register()) {
2355     addw(Rd, Rn, increment.as_register());
2356   } else {
2357     addw(Rd, Rn, increment.as_constant());
2358   }
2359 }
2360 
2361 void MacroAssembler::sub(Register Rd, Register Rn, RegisterOrConstant decrement) {
2362   if (decrement.is_register()) {
2363     sub(Rd, Rn, decrement.as_register());
2364   } else {
2365     sub(Rd, Rn, decrement.as_constant());
2366   }
2367 }
2368 
2369 void MacroAssembler::subw(Register Rd, Register Rn, RegisterOrConstant decrement) {
2370   if (decrement.is_register()) {
2371     subw(Rd, Rn, decrement.as_register());
2372   } else {
2373     subw(Rd, Rn, decrement.as_constant());
2374   }
2375 }
2376 
2377 void MacroAssembler::reinit_heapbase()
2378 {
2379   if (UseCompressedOops) {
2380     if (Universe::is_fully_initialized()) {
2381       mov(rheapbase, CompressedOops::ptrs_base());
2382     } else {
2383       lea(rheapbase, ExternalAddress((address)CompressedOops::ptrs_base_addr()));
2384       ldr(rheapbase, Address(rheapbase));
2385     }
2386   }
2387 }
2388 
2389 // this simulates the behaviour of the x86 cmpxchg instruction using a
2390 // load linked/store conditional pair. we use the acquire/release
2391 // versions of these instructions so that we flush pending writes as
2392 // per Java semantics.
2393 
2394 // n.b the x86 version assumes the old value to be compared against is
2395 // in rax and updates rax with the value located in memory if the
2396 // cmpxchg fails. we supply a register for the old value explicitly
2397 
2398 // the aarch64 load linked/store conditional instructions do not
2399 // accept an offset. so, unlike x86, we must provide a plain register
2400 // to identify the memory word to be compared/exchanged rather than a
2401 // register+offset Address.
2402 
2403 void MacroAssembler::cmpxchgptr(Register oldv, Register newv, Register addr, Register tmp,
2404                                 Label &amp;succeed, Label *fail) {
2405   // oldv holds comparison value
2406   // newv holds value to write in exchange
2407   // addr identifies memory word to compare against/update
2408   if (UseLSE) {
2409     mov(tmp, oldv);
2410     casal(Assembler::xword, oldv, newv, addr);
2411     cmp(tmp, oldv);
2412     br(Assembler::EQ, succeed);
2413     membar(AnyAny);
2414   } else {
2415     Label retry_load, nope;
2416     if ((VM_Version::features() &amp; VM_Version::CPU_STXR_PREFETCH))
2417       prfm(Address(addr), PSTL1STRM);
2418     bind(retry_load);
2419     // flush and load exclusive from the memory location
2420     // and fail if it is not what we expect
2421     ldaxr(tmp, addr);
2422     cmp(tmp, oldv);
2423     br(Assembler::NE, nope);
2424     // if we store+flush with no intervening write tmp wil be zero
2425     stlxr(tmp, newv, addr);
2426     cbzw(tmp, succeed);
2427     // retry so we only ever return after a load fails to compare
2428     // ensures we don&#39;t return a stale value after a failed write.
2429     b(retry_load);
2430     // if the memory word differs we return it in oldv and signal a fail
2431     bind(nope);
2432     membar(AnyAny);
2433     mov(oldv, tmp);
2434   }
2435   if (fail)
2436     b(*fail);
2437 }
2438 
2439 void MacroAssembler::cmpxchg_obj_header(Register oldv, Register newv, Register obj, Register tmp,
2440                                         Label &amp;succeed, Label *fail) {
2441   assert(oopDesc::mark_offset_in_bytes() == 0, &quot;assumption&quot;);
2442   cmpxchgptr(oldv, newv, obj, tmp, succeed, fail);
2443 }
2444 
2445 void MacroAssembler::cmpxchgw(Register oldv, Register newv, Register addr, Register tmp,
2446                                 Label &amp;succeed, Label *fail) {
2447   // oldv holds comparison value
2448   // newv holds value to write in exchange
2449   // addr identifies memory word to compare against/update
2450   // tmp returns 0/1 for success/failure
2451   if (UseLSE) {
2452     mov(tmp, oldv);
2453     casal(Assembler::word, oldv, newv, addr);
2454     cmp(tmp, oldv);
2455     br(Assembler::EQ, succeed);
2456     membar(AnyAny);
2457   } else {
2458     Label retry_load, nope;
2459     if ((VM_Version::features() &amp; VM_Version::CPU_STXR_PREFETCH))
2460       prfm(Address(addr), PSTL1STRM);
2461     bind(retry_load);
2462     // flush and load exclusive from the memory location
2463     // and fail if it is not what we expect
2464     ldaxrw(tmp, addr);
2465     cmp(tmp, oldv);
2466     br(Assembler::NE, nope);
2467     // if we store+flush with no intervening write tmp wil be zero
2468     stlxrw(tmp, newv, addr);
2469     cbzw(tmp, succeed);
2470     // retry so we only ever return after a load fails to compare
2471     // ensures we don&#39;t return a stale value after a failed write.
2472     b(retry_load);
2473     // if the memory word differs we return it in oldv and signal a fail
2474     bind(nope);
2475     membar(AnyAny);
2476     mov(oldv, tmp);
2477   }
2478   if (fail)
2479     b(*fail);
2480 }
2481 
2482 // A generic CAS; success or failure is in the EQ flag.  A weak CAS
2483 // doesn&#39;t retry and may fail spuriously.  If the oldval is wanted,
2484 // Pass a register for the result, otherwise pass noreg.
2485 
2486 // Clobbers rscratch1
2487 void MacroAssembler::cmpxchg(Register addr, Register expected,
2488                              Register new_val,
2489                              enum operand_size size,
2490                              bool acquire, bool release,
2491                              bool weak,
2492                              Register result) {
2493   if (result == noreg)  result = rscratch1;
2494   BLOCK_COMMENT(&quot;cmpxchg {&quot;);
2495   if (UseLSE) {
2496     mov(result, expected);
2497     lse_cas(result, new_val, addr, size, acquire, release, /*not_pair*/ true);
2498     compare_eq(result, expected, size);
2499   } else {
2500     Label retry_load, done;
2501     if ((VM_Version::features() &amp; VM_Version::CPU_STXR_PREFETCH))
2502       prfm(Address(addr), PSTL1STRM);
2503     bind(retry_load);
2504     load_exclusive(result, addr, size, acquire);
2505     compare_eq(result, expected, size);
2506     br(Assembler::NE, done);
2507     store_exclusive(rscratch1, new_val, addr, size, release);
2508     if (weak) {
2509       cmpw(rscratch1, 0u);  // If the store fails, return NE to our caller.
2510     } else {
2511       cbnzw(rscratch1, retry_load);
2512     }
2513     bind(done);
2514   }
2515   BLOCK_COMMENT(&quot;} cmpxchg&quot;);
2516 }
2517 
2518 // A generic comparison. Only compares for equality, clobbers rscratch1.
2519 void MacroAssembler::compare_eq(Register rm, Register rn, enum operand_size size) {
2520   if (size == xword) {
2521     cmp(rm, rn);
2522   } else if (size == word) {
2523     cmpw(rm, rn);
2524   } else if (size == halfword) {
2525     eorw(rscratch1, rm, rn);
2526     ands(zr, rscratch1, 0xffff);
2527   } else if (size == byte) {
2528     eorw(rscratch1, rm, rn);
2529     ands(zr, rscratch1, 0xff);
2530   } else {
2531     ShouldNotReachHere();
2532   }
2533 }
2534 
2535 
2536 static bool different(Register a, RegisterOrConstant b, Register c) {
2537   if (b.is_constant())
2538     return a != c;
2539   else
2540     return a != b.as_register() &amp;&amp; a != c &amp;&amp; b.as_register() != c;
2541 }
2542 
2543 #define ATOMIC_OP(NAME, LDXR, OP, IOP, AOP, STXR, sz)                   \
2544 void MacroAssembler::atomic_##NAME(Register prev, RegisterOrConstant incr, Register addr) { \
2545   if (UseLSE) {                                                         \
2546     prev = prev-&gt;is_valid() ? prev : zr;                                \
2547     if (incr.is_register()) {                                           \
2548       AOP(sz, incr.as_register(), prev, addr);                          \
2549     } else {                                                            \
2550       mov(rscratch2, incr.as_constant());                               \
2551       AOP(sz, rscratch2, prev, addr);                                   \
2552     }                                                                   \
2553     return;                                                             \
2554   }                                                                     \
2555   Register result = rscratch2;                                          \
2556   if (prev-&gt;is_valid())                                                 \
2557     result = different(prev, incr, addr) ? prev : rscratch2;            \
2558                                                                         \
2559   Label retry_load;                                                     \
2560   if ((VM_Version::features() &amp; VM_Version::CPU_STXR_PREFETCH))         \
2561     prfm(Address(addr), PSTL1STRM);                                     \
2562   bind(retry_load);                                                     \
2563   LDXR(result, addr);                                                   \
2564   OP(rscratch1, result, incr);                                          \
2565   STXR(rscratch2, rscratch1, addr);                                     \
2566   cbnzw(rscratch2, retry_load);                                         \
2567   if (prev-&gt;is_valid() &amp;&amp; prev != result) {                             \
2568     IOP(prev, rscratch1, incr);                                         \
2569   }                                                                     \
2570 }
2571 
2572 ATOMIC_OP(add, ldxr, add, sub, ldadd, stxr, Assembler::xword)
2573 ATOMIC_OP(addw, ldxrw, addw, subw, ldadd, stxrw, Assembler::word)
2574 ATOMIC_OP(addal, ldaxr, add, sub, ldaddal, stlxr, Assembler::xword)
2575 ATOMIC_OP(addalw, ldaxrw, addw, subw, ldaddal, stlxrw, Assembler::word)
2576 
2577 #undef ATOMIC_OP
2578 
2579 #define ATOMIC_XCHG(OP, AOP, LDXR, STXR, sz)                            \
2580 void MacroAssembler::atomic_##OP(Register prev, Register newv, Register addr) { \
2581   if (UseLSE) {                                                         \
2582     prev = prev-&gt;is_valid() ? prev : zr;                                \
2583     AOP(sz, newv, prev, addr);                                          \
2584     return;                                                             \
2585   }                                                                     \
2586   Register result = rscratch2;                                          \
2587   if (prev-&gt;is_valid())                                                 \
2588     result = different(prev, newv, addr) ? prev : rscratch2;            \
2589                                                                         \
2590   Label retry_load;                                                     \
2591   if ((VM_Version::features() &amp; VM_Version::CPU_STXR_PREFETCH))         \
2592     prfm(Address(addr), PSTL1STRM);                                     \
2593   bind(retry_load);                                                     \
2594   LDXR(result, addr);                                                   \
2595   STXR(rscratch1, newv, addr);                                          \
2596   cbnzw(rscratch1, retry_load);                                         \
2597   if (prev-&gt;is_valid() &amp;&amp; prev != result)                               \
2598     mov(prev, result);                                                  \
2599 }
2600 
2601 ATOMIC_XCHG(xchg, swp, ldxr, stxr, Assembler::xword)
2602 ATOMIC_XCHG(xchgw, swp, ldxrw, stxrw, Assembler::word)
2603 ATOMIC_XCHG(xchgal, swpal, ldaxr, stlxr, Assembler::xword)
2604 ATOMIC_XCHG(xchgalw, swpal, ldaxrw, stlxrw, Assembler::word)
2605 
2606 #undef ATOMIC_XCHG
2607 
2608 #ifndef PRODUCT
2609 extern &quot;C&quot; void findpc(intptr_t x);
2610 #endif
2611 
2612 void MacroAssembler::debug64(char* msg, int64_t pc, int64_t regs[])
2613 {
2614   // In order to get locks to work, we need to fake a in_VM state
2615   if (ShowMessageBoxOnError ) {
2616     JavaThread* thread = JavaThread::current();
2617     JavaThreadState saved_state = thread-&gt;thread_state();
2618     thread-&gt;set_thread_state(_thread_in_vm);
2619 #ifndef PRODUCT
2620     if (CountBytecodes || TraceBytecodes || StopInterpreterAt) {
2621       ttyLocker ttyl;
2622       BytecodeCounter::print();
2623     }
2624 #endif
2625     if (os::message_box(msg, &quot;Execution stopped, print registers?&quot;)) {
2626       ttyLocker ttyl;
2627       tty-&gt;print_cr(&quot; pc = 0x%016lx&quot;, pc);
2628 #ifndef PRODUCT
2629       tty-&gt;cr();
2630       findpc(pc);
2631       tty-&gt;cr();
2632 #endif
2633       tty-&gt;print_cr(&quot; r0 = 0x%016lx&quot;, regs[0]);
2634       tty-&gt;print_cr(&quot; r1 = 0x%016lx&quot;, regs[1]);
2635       tty-&gt;print_cr(&quot; r2 = 0x%016lx&quot;, regs[2]);
2636       tty-&gt;print_cr(&quot; r3 = 0x%016lx&quot;, regs[3]);
2637       tty-&gt;print_cr(&quot; r4 = 0x%016lx&quot;, regs[4]);
2638       tty-&gt;print_cr(&quot; r5 = 0x%016lx&quot;, regs[5]);
2639       tty-&gt;print_cr(&quot; r6 = 0x%016lx&quot;, regs[6]);
2640       tty-&gt;print_cr(&quot; r7 = 0x%016lx&quot;, regs[7]);
2641       tty-&gt;print_cr(&quot; r8 = 0x%016lx&quot;, regs[8]);
2642       tty-&gt;print_cr(&quot; r9 = 0x%016lx&quot;, regs[9]);
2643       tty-&gt;print_cr(&quot;r10 = 0x%016lx&quot;, regs[10]);
2644       tty-&gt;print_cr(&quot;r11 = 0x%016lx&quot;, regs[11]);
2645       tty-&gt;print_cr(&quot;r12 = 0x%016lx&quot;, regs[12]);
2646       tty-&gt;print_cr(&quot;r13 = 0x%016lx&quot;, regs[13]);
2647       tty-&gt;print_cr(&quot;r14 = 0x%016lx&quot;, regs[14]);
2648       tty-&gt;print_cr(&quot;r15 = 0x%016lx&quot;, regs[15]);
2649       tty-&gt;print_cr(&quot;r16 = 0x%016lx&quot;, regs[16]);
2650       tty-&gt;print_cr(&quot;r17 = 0x%016lx&quot;, regs[17]);
2651       tty-&gt;print_cr(&quot;r18 = 0x%016lx&quot;, regs[18]);
2652       tty-&gt;print_cr(&quot;r19 = 0x%016lx&quot;, regs[19]);
2653       tty-&gt;print_cr(&quot;r20 = 0x%016lx&quot;, regs[20]);
2654       tty-&gt;print_cr(&quot;r21 = 0x%016lx&quot;, regs[21]);
2655       tty-&gt;print_cr(&quot;r22 = 0x%016lx&quot;, regs[22]);
2656       tty-&gt;print_cr(&quot;r23 = 0x%016lx&quot;, regs[23]);
2657       tty-&gt;print_cr(&quot;r24 = 0x%016lx&quot;, regs[24]);
2658       tty-&gt;print_cr(&quot;r25 = 0x%016lx&quot;, regs[25]);
2659       tty-&gt;print_cr(&quot;r26 = 0x%016lx&quot;, regs[26]);
2660       tty-&gt;print_cr(&quot;r27 = 0x%016lx&quot;, regs[27]);
2661       tty-&gt;print_cr(&quot;r28 = 0x%016lx&quot;, regs[28]);
2662       tty-&gt;print_cr(&quot;r30 = 0x%016lx&quot;, regs[30]);
2663       tty-&gt;print_cr(&quot;r31 = 0x%016lx&quot;, regs[31]);
2664       BREAKPOINT;
2665     }
2666   }
2667   fatal(&quot;DEBUG MESSAGE: %s&quot;, msg);
2668 }
2669 
2670 void MacroAssembler::push_call_clobbered_registers() {
2671   int step = 4 * wordSize;
2672   push(RegSet::range(r0, r18) - RegSet::of(rscratch1, rscratch2), sp);
2673   sub(sp, sp, step);
2674   mov(rscratch1, -step);
2675   // Push v0-v7, v16-v31.
2676   for (int i = 31; i&gt;= 4; i -= 4) {
2677     if (i &lt;= v7-&gt;encoding() || i &gt;= v16-&gt;encoding())
2678       st1(as_FloatRegister(i-3), as_FloatRegister(i-2), as_FloatRegister(i-1),
2679           as_FloatRegister(i), T1D, Address(post(sp, rscratch1)));
2680   }
2681   st1(as_FloatRegister(0), as_FloatRegister(1), as_FloatRegister(2),
2682       as_FloatRegister(3), T1D, Address(sp));
2683 }
2684 
2685 void MacroAssembler::pop_call_clobbered_registers() {
2686   for (int i = 0; i &lt; 32; i += 4) {
2687     if (i &lt;= v7-&gt;encoding() || i &gt;= v16-&gt;encoding())
2688       ld1(as_FloatRegister(i), as_FloatRegister(i+1), as_FloatRegister(i+2),
2689           as_FloatRegister(i+3), T1D, Address(post(sp, 4 * wordSize)));
2690   }
2691 
2692   pop(RegSet::range(r0, r18) - RegSet::of(rscratch1, rscratch2), sp);
2693 }
2694 
2695 void MacroAssembler::push_CPU_state(bool save_vectors) {
2696   int step = (save_vectors ? 8 : 4) * wordSize;
2697   push(0x3fffffff, sp);         // integer registers except lr &amp; sp
2698   mov(rscratch1, -step);
2699   sub(sp, sp, step);
2700   for (int i = 28; i &gt;= 4; i -= 4) {
2701     st1(as_FloatRegister(i), as_FloatRegister(i+1), as_FloatRegister(i+2),
2702         as_FloatRegister(i+3), save_vectors ? T2D : T1D, Address(post(sp, rscratch1)));
2703   }
2704   st1(v0, v1, v2, v3, save_vectors ? T2D : T1D, sp);
2705 }
2706 
2707 void MacroAssembler::pop_CPU_state(bool restore_vectors) {
2708   int step = (restore_vectors ? 8 : 4) * wordSize;
2709   for (int i = 0; i &lt;= 28; i += 4)
2710     ld1(as_FloatRegister(i), as_FloatRegister(i+1), as_FloatRegister(i+2),
2711         as_FloatRegister(i+3), restore_vectors ? T2D : T1D, Address(post(sp, step)));
2712   pop(0x3fffffff, sp);         // integer registers except lr &amp; sp
2713 }
2714 
2715 /**
2716  * Helpers for multiply_to_len().
2717  */
2718 void MacroAssembler::add2_with_carry(Register final_dest_hi, Register dest_hi, Register dest_lo,
2719                                      Register src1, Register src2) {
2720   adds(dest_lo, dest_lo, src1);
2721   adc(dest_hi, dest_hi, zr);
2722   adds(dest_lo, dest_lo, src2);
2723   adc(final_dest_hi, dest_hi, zr);
2724 }
2725 
2726 // Generate an address from (r + r1 extend offset).  &quot;size&quot; is the
2727 // size of the operand.  The result may be in rscratch2.
2728 Address MacroAssembler::offsetted_address(Register r, Register r1,
2729                                           Address::extend ext, int offset, int size) {
2730   if (offset || (ext.shift() % size != 0)) {
2731     lea(rscratch2, Address(r, r1, ext));
2732     return Address(rscratch2, offset);
2733   } else {
2734     return Address(r, r1, ext);
2735   }
2736 }
2737 
2738 Address MacroAssembler::spill_address(int size, int offset, Register tmp)
2739 {
2740   assert(offset &gt;= 0, &quot;spill to negative address?&quot;);
2741   // Offset reachable ?
2742   //   Not aligned - 9 bits signed offset
2743   //   Aligned - 12 bits unsigned offset shifted
2744   Register base = sp;
2745   if ((offset &amp; (size-1)) &amp;&amp; offset &gt;= (1&lt;&lt;8)) {
2746     add(tmp, base, offset &amp; ((1&lt;&lt;12)-1));
2747     base = tmp;
2748     offset &amp;= -1u&lt;&lt;12;
2749   }
2750 
2751   if (offset &gt;= (1&lt;&lt;12) * size) {
2752     add(tmp, base, offset &amp; (((1&lt;&lt;12)-1)&lt;&lt;12));
2753     base = tmp;
2754     offset &amp;= ~(((1&lt;&lt;12)-1)&lt;&lt;12);
2755   }
2756 
2757   return Address(base, offset);
2758 }
2759 
2760 // Checks whether offset is aligned.
2761 // Returns true if it is, else false.
2762 bool MacroAssembler::merge_alignment_check(Register base,
2763                                            size_t size,
2764                                            long cur_offset,
2765                                            long prev_offset) const {
2766   if (AvoidUnalignedAccesses) {
2767     if (base == sp) {
2768       // Checks whether low offset if aligned to pair of registers.
2769       long pair_mask = size * 2 - 1;
2770       long offset = prev_offset &gt; cur_offset ? cur_offset : prev_offset;
2771       return (offset &amp; pair_mask) == 0;
2772     } else { // If base is not sp, we can&#39;t guarantee the access is aligned.
2773       return false;
2774     }
2775   } else {
2776     long mask = size - 1;
2777     // Load/store pair instruction only supports element size aligned offset.
2778     return (cur_offset &amp; mask) == 0 &amp;&amp; (prev_offset &amp; mask) == 0;
2779   }
2780 }
2781 
2782 // Checks whether current and previous loads/stores can be merged.
2783 // Returns true if it can be merged, else false.
2784 bool MacroAssembler::ldst_can_merge(Register rt,
2785                                     const Address &amp;adr,
2786                                     size_t cur_size_in_bytes,
2787                                     bool is_store) const {
2788   address prev = pc() - NativeInstruction::instruction_size;
2789   address last = code()-&gt;last_insn();
2790 
2791   if (last == NULL || !nativeInstruction_at(last)-&gt;is_Imm_LdSt()) {
2792     return false;
2793   }
2794 
2795   if (adr.getMode() != Address::base_plus_offset || prev != last) {
2796     return false;
2797   }
2798 
2799   NativeLdSt* prev_ldst = NativeLdSt_at(prev);
2800   size_t prev_size_in_bytes = prev_ldst-&gt;size_in_bytes();
2801 
2802   assert(prev_size_in_bytes == 4 || prev_size_in_bytes == 8, &quot;only supports 64/32bit merging.&quot;);
2803   assert(cur_size_in_bytes == 4 || cur_size_in_bytes == 8, &quot;only supports 64/32bit merging.&quot;);
2804 
2805   if (cur_size_in_bytes != prev_size_in_bytes || is_store != prev_ldst-&gt;is_store()) {
2806     return false;
2807   }
2808 
2809   long max_offset = 63 * prev_size_in_bytes;
2810   long min_offset = -64 * prev_size_in_bytes;
2811 
2812   assert(prev_ldst-&gt;is_not_pre_post_index(), &quot;pre-index or post-index is not supported to be merged.&quot;);
2813 
2814   // Only same base can be merged.
2815   if (adr.base() != prev_ldst-&gt;base()) {
2816     return false;
2817   }
2818 
2819   long cur_offset = adr.offset();
2820   long prev_offset = prev_ldst-&gt;offset();
2821   size_t diff = abs(cur_offset - prev_offset);
2822   if (diff != prev_size_in_bytes) {
2823     return false;
2824   }
2825 
2826   // Following cases can not be merged:
2827   // ldr x2, [x2, #8]
2828   // ldr x3, [x2, #16]
2829   // or:
2830   // ldr x2, [x3, #8]
2831   // ldr x2, [x3, #16]
2832   // If t1 and t2 is the same in &quot;ldp t1, t2, [xn, #imm]&quot;, we&#39;ll get SIGILL.
2833   if (!is_store &amp;&amp; (adr.base() == prev_ldst-&gt;target() || rt == prev_ldst-&gt;target())) {
2834     return false;
2835   }
2836 
2837   long low_offset = prev_offset &gt; cur_offset ? cur_offset : prev_offset;
2838   // Offset range must be in ldp/stp instruction&#39;s range.
2839   if (low_offset &gt; max_offset || low_offset &lt; min_offset) {
2840     return false;
2841   }
2842 
2843   if (merge_alignment_check(adr.base(), prev_size_in_bytes, cur_offset, prev_offset)) {
2844     return true;
2845   }
2846 
2847   return false;
2848 }
2849 
2850 // Merge current load/store with previous load/store into ldp/stp.
2851 void MacroAssembler::merge_ldst(Register rt,
2852                                 const Address &amp;adr,
2853                                 size_t cur_size_in_bytes,
2854                                 bool is_store) {
2855 
2856   assert(ldst_can_merge(rt, adr, cur_size_in_bytes, is_store) == true, &quot;cur and prev must be able to be merged.&quot;);
2857 
2858   Register rt_low, rt_high;
2859   address prev = pc() - NativeInstruction::instruction_size;
2860   NativeLdSt* prev_ldst = NativeLdSt_at(prev);
2861 
2862   long offset;
2863 
2864   if (adr.offset() &lt; prev_ldst-&gt;offset()) {
2865     offset = adr.offset();
2866     rt_low = rt;
2867     rt_high = prev_ldst-&gt;target();
2868   } else {
2869     offset = prev_ldst-&gt;offset();
2870     rt_low = prev_ldst-&gt;target();
2871     rt_high = rt;
2872   }
2873 
2874   Address adr_p = Address(prev_ldst-&gt;base(), offset);
2875   // Overwrite previous generated binary.
2876   code_section()-&gt;set_end(prev);
2877 
2878   const int sz = prev_ldst-&gt;size_in_bytes();
2879   assert(sz == 8 || sz == 4, &quot;only supports 64/32bit merging.&quot;);
2880   if (!is_store) {
2881     BLOCK_COMMENT(&quot;merged ldr pair&quot;);
2882     if (sz == 8) {
2883       ldp(rt_low, rt_high, adr_p);
2884     } else {
2885       ldpw(rt_low, rt_high, adr_p);
2886     }
2887   } else {
2888     BLOCK_COMMENT(&quot;merged str pair&quot;);
2889     if (sz == 8) {
2890       stp(rt_low, rt_high, adr_p);
2891     } else {
2892       stpw(rt_low, rt_high, adr_p);
2893     }
2894   }
2895 }
2896 
2897 /**
2898  * Multiply 64 bit by 64 bit first loop.
2899  */
2900 void MacroAssembler::multiply_64_x_64_loop(Register x, Register xstart, Register x_xstart,
2901                                            Register y, Register y_idx, Register z,
2902                                            Register carry, Register product,
2903                                            Register idx, Register kdx) {
2904   //
2905   //  jlong carry, x[], y[], z[];
2906   //  for (int idx=ystart, kdx=ystart+1+xstart; idx &gt;= 0; idx-, kdx--) {
2907   //    huge_128 product = y[idx] * x[xstart] + carry;
2908   //    z[kdx] = (jlong)product;
2909   //    carry  = (jlong)(product &gt;&gt;&gt; 64);
2910   //  }
2911   //  z[xstart] = carry;
2912   //
2913 
2914   Label L_first_loop, L_first_loop_exit;
2915   Label L_one_x, L_one_y, L_multiply;
2916 
2917   subsw(xstart, xstart, 1);
2918   br(Assembler::MI, L_one_x);
2919 
2920   lea(rscratch1, Address(x, xstart, Address::lsl(LogBytesPerInt)));
2921   ldr(x_xstart, Address(rscratch1));
2922   ror(x_xstart, x_xstart, 32); // convert big-endian to little-endian
2923 
2924   bind(L_first_loop);
2925   subsw(idx, idx, 1);
2926   br(Assembler::MI, L_first_loop_exit);
2927   subsw(idx, idx, 1);
2928   br(Assembler::MI, L_one_y);
2929   lea(rscratch1, Address(y, idx, Address::uxtw(LogBytesPerInt)));
2930   ldr(y_idx, Address(rscratch1));
2931   ror(y_idx, y_idx, 32); // convert big-endian to little-endian
2932   bind(L_multiply);
2933 
2934   // AArch64 has a multiply-accumulate instruction that we can&#39;t use
2935   // here because it has no way to process carries, so we have to use
2936   // separate add and adc instructions.  Bah.
2937   umulh(rscratch1, x_xstart, y_idx); // x_xstart * y_idx -&gt; rscratch1:product
2938   mul(product, x_xstart, y_idx);
2939   adds(product, product, carry);
2940   adc(carry, rscratch1, zr);   // x_xstart * y_idx + carry -&gt; carry:product
2941 
2942   subw(kdx, kdx, 2);
2943   ror(product, product, 32); // back to big-endian
2944   str(product, offsetted_address(z, kdx, Address::uxtw(LogBytesPerInt), 0, BytesPerLong));
2945 
2946   b(L_first_loop);
2947 
2948   bind(L_one_y);
2949   ldrw(y_idx, Address(y,  0));
2950   b(L_multiply);
2951 
2952   bind(L_one_x);
2953   ldrw(x_xstart, Address(x,  0));
2954   b(L_first_loop);
2955 
2956   bind(L_first_loop_exit);
2957 }
2958 
2959 /**
2960  * Multiply 128 bit by 128. Unrolled inner loop.
2961  *
2962  */
2963 void MacroAssembler::multiply_128_x_128_loop(Register y, Register z,
2964                                              Register carry, Register carry2,
2965                                              Register idx, Register jdx,
2966                                              Register yz_idx1, Register yz_idx2,
2967                                              Register tmp, Register tmp3, Register tmp4,
2968                                              Register tmp6, Register product_hi) {
2969 
2970   //   jlong carry, x[], y[], z[];
2971   //   int kdx = ystart+1;
2972   //   for (int idx=ystart-2; idx &gt;= 0; idx -= 2) { // Third loop
2973   //     huge_128 tmp3 = (y[idx+1] * product_hi) + z[kdx+idx+1] + carry;
2974   //     jlong carry2  = (jlong)(tmp3 &gt;&gt;&gt; 64);
2975   //     huge_128 tmp4 = (y[idx]   * product_hi) + z[kdx+idx] + carry2;
2976   //     carry  = (jlong)(tmp4 &gt;&gt;&gt; 64);
2977   //     z[kdx+idx+1] = (jlong)tmp3;
2978   //     z[kdx+idx] = (jlong)tmp4;
2979   //   }
2980   //   idx += 2;
2981   //   if (idx &gt; 0) {
2982   //     yz_idx1 = (y[idx] * product_hi) + z[kdx+idx] + carry;
2983   //     z[kdx+idx] = (jlong)yz_idx1;
2984   //     carry  = (jlong)(yz_idx1 &gt;&gt;&gt; 64);
2985   //   }
2986   //
2987 
2988   Label L_third_loop, L_third_loop_exit, L_post_third_loop_done;
2989 
2990   lsrw(jdx, idx, 2);
2991 
2992   bind(L_third_loop);
2993 
2994   subsw(jdx, jdx, 1);
2995   br(Assembler::MI, L_third_loop_exit);
2996   subw(idx, idx, 4);
2997 
2998   lea(rscratch1, Address(y, idx, Address::uxtw(LogBytesPerInt)));
2999 
3000   ldp(yz_idx2, yz_idx1, Address(rscratch1, 0));
3001 
3002   lea(tmp6, Address(z, idx, Address::uxtw(LogBytesPerInt)));
3003 
3004   ror(yz_idx1, yz_idx1, 32); // convert big-endian to little-endian
3005   ror(yz_idx2, yz_idx2, 32);
3006 
3007   ldp(rscratch2, rscratch1, Address(tmp6, 0));
3008 
3009   mul(tmp3, product_hi, yz_idx1);  //  yz_idx1 * product_hi -&gt; tmp4:tmp3
3010   umulh(tmp4, product_hi, yz_idx1);
3011 
3012   ror(rscratch1, rscratch1, 32); // convert big-endian to little-endian
3013   ror(rscratch2, rscratch2, 32);
3014 
3015   mul(tmp, product_hi, yz_idx2);   //  yz_idx2 * product_hi -&gt; carry2:tmp
3016   umulh(carry2, product_hi, yz_idx2);
3017 
3018   // propagate sum of both multiplications into carry:tmp4:tmp3
3019   adds(tmp3, tmp3, carry);
3020   adc(tmp4, tmp4, zr);
3021   adds(tmp3, tmp3, rscratch1);
3022   adcs(tmp4, tmp4, tmp);
3023   adc(carry, carry2, zr);
3024   adds(tmp4, tmp4, rscratch2);
3025   adc(carry, carry, zr);
3026 
3027   ror(tmp3, tmp3, 32); // convert little-endian to big-endian
3028   ror(tmp4, tmp4, 32);
3029   stp(tmp4, tmp3, Address(tmp6, 0));
3030 
3031   b(L_third_loop);
3032   bind (L_third_loop_exit);
3033 
3034   andw (idx, idx, 0x3);
3035   cbz(idx, L_post_third_loop_done);
3036 
3037   Label L_check_1;
3038   subsw(idx, idx, 2);
3039   br(Assembler::MI, L_check_1);
3040 
3041   lea(rscratch1, Address(y, idx, Address::uxtw(LogBytesPerInt)));
3042   ldr(yz_idx1, Address(rscratch1, 0));
3043   ror(yz_idx1, yz_idx1, 32);
3044   mul(tmp3, product_hi, yz_idx1);  //  yz_idx1 * product_hi -&gt; tmp4:tmp3
3045   umulh(tmp4, product_hi, yz_idx1);
3046   lea(rscratch1, Address(z, idx, Address::uxtw(LogBytesPerInt)));
3047   ldr(yz_idx2, Address(rscratch1, 0));
3048   ror(yz_idx2, yz_idx2, 32);
3049 
3050   add2_with_carry(carry, tmp4, tmp3, carry, yz_idx2);
3051 
3052   ror(tmp3, tmp3, 32);
3053   str(tmp3, Address(rscratch1, 0));
3054 
3055   bind (L_check_1);
3056 
3057   andw (idx, idx, 0x1);
3058   subsw(idx, idx, 1);
3059   br(Assembler::MI, L_post_third_loop_done);
3060   ldrw(tmp4, Address(y, idx, Address::uxtw(LogBytesPerInt)));
3061   mul(tmp3, tmp4, product_hi);  //  tmp4 * product_hi -&gt; carry2:tmp3
3062   umulh(carry2, tmp4, product_hi);
3063   ldrw(tmp4, Address(z, idx, Address::uxtw(LogBytesPerInt)));
3064 
3065   add2_with_carry(carry2, tmp3, tmp4, carry);
3066 
3067   strw(tmp3, Address(z, idx, Address::uxtw(LogBytesPerInt)));
3068   extr(carry, carry2, tmp3, 32);
3069 
3070   bind(L_post_third_loop_done);
3071 }
3072 
3073 /**
3074  * Code for BigInteger::multiplyToLen() instrinsic.
3075  *
3076  * r0: x
3077  * r1: xlen
3078  * r2: y
3079  * r3: ylen
3080  * r4:  z
3081  * r5: zlen
3082  * r10: tmp1
3083  * r11: tmp2
3084  * r12: tmp3
3085  * r13: tmp4
3086  * r14: tmp5
3087  * r15: tmp6
3088  * r16: tmp7
3089  *
3090  */
3091 void MacroAssembler::multiply_to_len(Register x, Register xlen, Register y, Register ylen,
3092                                      Register z, Register zlen,
3093                                      Register tmp1, Register tmp2, Register tmp3, Register tmp4,
3094                                      Register tmp5, Register tmp6, Register product_hi) {
3095 
3096   assert_different_registers(x, xlen, y, ylen, z, zlen, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6);
3097 
3098   const Register idx = tmp1;
3099   const Register kdx = tmp2;
3100   const Register xstart = tmp3;
3101 
3102   const Register y_idx = tmp4;
3103   const Register carry = tmp5;
3104   const Register product  = xlen;
3105   const Register x_xstart = zlen;  // reuse register
3106 
3107   // First Loop.
3108   //
3109   //  final static long LONG_MASK = 0xffffffffL;
3110   //  int xstart = xlen - 1;
3111   //  int ystart = ylen - 1;
3112   //  long carry = 0;
3113   //  for (int idx=ystart, kdx=ystart+1+xstart; idx &gt;= 0; idx-, kdx--) {
3114   //    long product = (y[idx] &amp; LONG_MASK) * (x[xstart] &amp; LONG_MASK) + carry;
3115   //    z[kdx] = (int)product;
3116   //    carry = product &gt;&gt;&gt; 32;
3117   //  }
3118   //  z[xstart] = (int)carry;
3119   //
3120 
3121   movw(idx, ylen);      // idx = ylen;
3122   movw(kdx, zlen);      // kdx = xlen+ylen;
3123   mov(carry, zr);       // carry = 0;
3124 
3125   Label L_done;
3126 
3127   movw(xstart, xlen);
3128   subsw(xstart, xstart, 1);
3129   br(Assembler::MI, L_done);
3130 
3131   multiply_64_x_64_loop(x, xstart, x_xstart, y, y_idx, z, carry, product, idx, kdx);
3132 
3133   Label L_second_loop;
3134   cbzw(kdx, L_second_loop);
3135 
3136   Label L_carry;
3137   subw(kdx, kdx, 1);
3138   cbzw(kdx, L_carry);
3139 
3140   strw(carry, Address(z, kdx, Address::uxtw(LogBytesPerInt)));
3141   lsr(carry, carry, 32);
3142   subw(kdx, kdx, 1);
3143 
3144   bind(L_carry);
3145   strw(carry, Address(z, kdx, Address::uxtw(LogBytesPerInt)));
3146 
3147   // Second and third (nested) loops.
3148   //
3149   // for (int i = xstart-1; i &gt;= 0; i--) { // Second loop
3150   //   carry = 0;
3151   //   for (int jdx=ystart, k=ystart+1+i; jdx &gt;= 0; jdx--, k--) { // Third loop
3152   //     long product = (y[jdx] &amp; LONG_MASK) * (x[i] &amp; LONG_MASK) +
3153   //                    (z[k] &amp; LONG_MASK) + carry;
3154   //     z[k] = (int)product;
3155   //     carry = product &gt;&gt;&gt; 32;
3156   //   }
3157   //   z[i] = (int)carry;
3158   // }
3159   //
3160   // i = xlen, j = tmp1, k = tmp2, carry = tmp5, x[i] = product_hi
3161 
3162   const Register jdx = tmp1;
3163 
3164   bind(L_second_loop);
3165   mov(carry, zr);                // carry = 0;
3166   movw(jdx, ylen);               // j = ystart+1
3167 
3168   subsw(xstart, xstart, 1);      // i = xstart-1;
3169   br(Assembler::MI, L_done);
3170 
3171   str(z, Address(pre(sp, -4 * wordSize)));
3172 
3173   Label L_last_x;
3174   lea(z, offsetted_address(z, xstart, Address::uxtw(LogBytesPerInt), 4, BytesPerInt)); // z = z + k - j
3175   subsw(xstart, xstart, 1);       // i = xstart-1;
3176   br(Assembler::MI, L_last_x);
3177 
3178   lea(rscratch1, Address(x, xstart, Address::uxtw(LogBytesPerInt)));
3179   ldr(product_hi, Address(rscratch1));
3180   ror(product_hi, product_hi, 32);  // convert big-endian to little-endian
3181 
3182   Label L_third_loop_prologue;
3183   bind(L_third_loop_prologue);
3184 
3185   str(ylen, Address(sp, wordSize));
3186   stp(x, xstart, Address(sp, 2 * wordSize));
3187   multiply_128_x_128_loop(y, z, carry, x, jdx, ylen, product,
3188                           tmp2, x_xstart, tmp3, tmp4, tmp6, product_hi);
3189   ldp(z, ylen, Address(post(sp, 2 * wordSize)));
3190   ldp(x, xlen, Address(post(sp, 2 * wordSize)));   // copy old xstart -&gt; xlen
3191 
3192   addw(tmp3, xlen, 1);
3193   strw(carry, Address(z, tmp3, Address::uxtw(LogBytesPerInt)));
3194   subsw(tmp3, tmp3, 1);
3195   br(Assembler::MI, L_done);
3196 
3197   lsr(carry, carry, 32);
3198   strw(carry, Address(z, tmp3, Address::uxtw(LogBytesPerInt)));
3199   b(L_second_loop);
3200 
3201   // Next infrequent code is moved outside loops.
3202   bind(L_last_x);
3203   ldrw(product_hi, Address(x,  0));
3204   b(L_third_loop_prologue);
3205 
3206   bind(L_done);
3207 }
3208 
3209 // Code for BigInteger::mulAdd instrinsic
3210 // out     = r0
3211 // in      = r1
3212 // offset  = r2  (already out.length-offset)
3213 // len     = r3
3214 // k       = r4
3215 //
3216 // pseudo code from java implementation:
3217 // carry = 0;
3218 // offset = out.length-offset - 1;
3219 // for (int j=len-1; j &gt;= 0; j--) {
3220 //     product = (in[j] &amp; LONG_MASK) * kLong + (out[offset] &amp; LONG_MASK) + carry;
3221 //     out[offset--] = (int)product;
3222 //     carry = product &gt;&gt;&gt; 32;
3223 // }
3224 // return (int)carry;
3225 void MacroAssembler::mul_add(Register out, Register in, Register offset,
3226       Register len, Register k) {
3227     Label LOOP, END;
3228     // pre-loop
3229     cmp(len, zr); // cmp, not cbz/cbnz: to use condition twice =&gt; less branches
3230     csel(out, zr, out, Assembler::EQ);
3231     br(Assembler::EQ, END);
3232     add(in, in, len, LSL, 2); // in[j+1] address
3233     add(offset, out, offset, LSL, 2); // out[offset + 1] address
3234     mov(out, zr); // used to keep carry now
3235     BIND(LOOP);
3236     ldrw(rscratch1, Address(pre(in, -4)));
3237     madd(rscratch1, rscratch1, k, out);
3238     ldrw(rscratch2, Address(pre(offset, -4)));
3239     add(rscratch1, rscratch1, rscratch2);
3240     strw(rscratch1, Address(offset));
3241     lsr(out, rscratch1, 32);
3242     subs(len, len, 1);
3243     br(Assembler::NE, LOOP);
3244     BIND(END);
3245 }
3246 
3247 /**
3248  * Emits code to update CRC-32 with a byte value according to constants in table
3249  *
3250  * @param [in,out]crc   Register containing the crc.
3251  * @param [in]val       Register containing the byte to fold into the CRC.
3252  * @param [in]table     Register containing the table of crc constants.
3253  *
3254  * uint32_t crc;
3255  * val = crc_table[(val ^ crc) &amp; 0xFF];
3256  * crc = val ^ (crc &gt;&gt; 8);
3257  *
3258  */
3259 void MacroAssembler::update_byte_crc32(Register crc, Register val, Register table) {
3260   eor(val, val, crc);
3261   andr(val, val, 0xff);
3262   ldrw(val, Address(table, val, Address::lsl(2)));
3263   eor(crc, val, crc, Assembler::LSR, 8);
3264 }
3265 
3266 /**
3267  * Emits code to update CRC-32 with a 32-bit value according to tables 0 to 3
3268  *
3269  * @param [in,out]crc   Register containing the crc.
3270  * @param [in]v         Register containing the 32-bit to fold into the CRC.
3271  * @param [in]table0    Register containing table 0 of crc constants.
3272  * @param [in]table1    Register containing table 1 of crc constants.
3273  * @param [in]table2    Register containing table 2 of crc constants.
3274  * @param [in]table3    Register containing table 3 of crc constants.
3275  *
3276  * uint32_t crc;
3277  *   v = crc ^ v
3278  *   crc = table3[v&amp;0xff]^table2[(v&gt;&gt;8)&amp;0xff]^table1[(v&gt;&gt;16)&amp;0xff]^table0[v&gt;&gt;24]
3279  *
3280  */
3281 void MacroAssembler::update_word_crc32(Register crc, Register v, Register tmp,
3282         Register table0, Register table1, Register table2, Register table3,
3283         bool upper) {
3284   eor(v, crc, v, upper ? LSR:LSL, upper ? 32:0);
3285   uxtb(tmp, v);
3286   ldrw(crc, Address(table3, tmp, Address::lsl(2)));
3287   ubfx(tmp, v, 8, 8);
3288   ldrw(tmp, Address(table2, tmp, Address::lsl(2)));
3289   eor(crc, crc, tmp);
3290   ubfx(tmp, v, 16, 8);
3291   ldrw(tmp, Address(table1, tmp, Address::lsl(2)));
3292   eor(crc, crc, tmp);
3293   ubfx(tmp, v, 24, 8);
3294   ldrw(tmp, Address(table0, tmp, Address::lsl(2)));
3295   eor(crc, crc, tmp);
3296 }
3297 
3298 void MacroAssembler::kernel_crc32_using_crc32(Register crc, Register buf,
3299         Register len, Register tmp0, Register tmp1, Register tmp2,
3300         Register tmp3) {
3301     Label CRC_by64_loop, CRC_by4_loop, CRC_by1_loop, CRC_less64, CRC_by64_pre, CRC_by32_loop, CRC_less32, L_exit;
3302     assert_different_registers(crc, buf, len, tmp0, tmp1, tmp2, tmp3);
3303 
3304     mvnw(crc, crc);
3305 
3306     subs(len, len, 128);
3307     br(Assembler::GE, CRC_by64_pre);
3308   BIND(CRC_less64);
3309     adds(len, len, 128-32);
3310     br(Assembler::GE, CRC_by32_loop);
3311   BIND(CRC_less32);
3312     adds(len, len, 32-4);
3313     br(Assembler::GE, CRC_by4_loop);
3314     adds(len, len, 4);
3315     br(Assembler::GT, CRC_by1_loop);
3316     b(L_exit);
3317 
3318   BIND(CRC_by32_loop);
3319     ldp(tmp0, tmp1, Address(post(buf, 16)));
3320     subs(len, len, 32);
3321     crc32x(crc, crc, tmp0);
3322     ldr(tmp2, Address(post(buf, 8)));
3323     crc32x(crc, crc, tmp1);
3324     ldr(tmp3, Address(post(buf, 8)));
3325     crc32x(crc, crc, tmp2);
3326     crc32x(crc, crc, tmp3);
3327     br(Assembler::GE, CRC_by32_loop);
3328     cmn(len, 32);
3329     br(Assembler::NE, CRC_less32);
3330     b(L_exit);
3331 
3332   BIND(CRC_by4_loop);
3333     ldrw(tmp0, Address(post(buf, 4)));
3334     subs(len, len, 4);
3335     crc32w(crc, crc, tmp0);
3336     br(Assembler::GE, CRC_by4_loop);
3337     adds(len, len, 4);
3338     br(Assembler::LE, L_exit);
3339   BIND(CRC_by1_loop);
3340     ldrb(tmp0, Address(post(buf, 1)));
3341     subs(len, len, 1);
3342     crc32b(crc, crc, tmp0);
3343     br(Assembler::GT, CRC_by1_loop);
3344     b(L_exit);
3345 
3346   BIND(CRC_by64_pre);
3347     sub(buf, buf, 8);
3348     ldp(tmp0, tmp1, Address(buf, 8));
3349     crc32x(crc, crc, tmp0);
3350     ldr(tmp2, Address(buf, 24));
3351     crc32x(crc, crc, tmp1);
3352     ldr(tmp3, Address(buf, 32));
3353     crc32x(crc, crc, tmp2);
3354     ldr(tmp0, Address(buf, 40));
3355     crc32x(crc, crc, tmp3);
3356     ldr(tmp1, Address(buf, 48));
3357     crc32x(crc, crc, tmp0);
3358     ldr(tmp2, Address(buf, 56));
3359     crc32x(crc, crc, tmp1);
3360     ldr(tmp3, Address(pre(buf, 64)));
3361 
3362     b(CRC_by64_loop);
3363 
3364     align(CodeEntryAlignment);
3365   BIND(CRC_by64_loop);
3366     subs(len, len, 64);
3367     crc32x(crc, crc, tmp2);
3368     ldr(tmp0, Address(buf, 8));
3369     crc32x(crc, crc, tmp3);
3370     ldr(tmp1, Address(buf, 16));
3371     crc32x(crc, crc, tmp0);
3372     ldr(tmp2, Address(buf, 24));
3373     crc32x(crc, crc, tmp1);
3374     ldr(tmp3, Address(buf, 32));
3375     crc32x(crc, crc, tmp2);
3376     ldr(tmp0, Address(buf, 40));
3377     crc32x(crc, crc, tmp3);
3378     ldr(tmp1, Address(buf, 48));
3379     crc32x(crc, crc, tmp0);
3380     ldr(tmp2, Address(buf, 56));
3381     crc32x(crc, crc, tmp1);
3382     ldr(tmp3, Address(pre(buf, 64)));
3383     br(Assembler::GE, CRC_by64_loop);
3384 
3385     // post-loop
3386     crc32x(crc, crc, tmp2);
3387     crc32x(crc, crc, tmp3);
3388 
3389     sub(len, len, 64);
3390     add(buf, buf, 8);
3391     cmn(len, 128);
3392     br(Assembler::NE, CRC_less64);
3393   BIND(L_exit);
3394     mvnw(crc, crc);
3395 }
3396 
3397 /**
3398  * @param crc   register containing existing CRC (32-bit)
3399  * @param buf   register pointing to input byte buffer (byte*)
3400  * @param len   register containing number of bytes
3401  * @param table register that will contain address of CRC table
3402  * @param tmp   scratch register
3403  */
3404 void MacroAssembler::kernel_crc32(Register crc, Register buf, Register len,
3405         Register table0, Register table1, Register table2, Register table3,
3406         Register tmp, Register tmp2, Register tmp3) {
3407   Label L_by16, L_by16_loop, L_by4, L_by4_loop, L_by1, L_by1_loop, L_exit;
3408   unsigned long offset;
3409 
3410   if (UseCRC32) {
3411       kernel_crc32_using_crc32(crc, buf, len, table0, table1, table2, table3);
3412       return;
3413   }
3414 
3415     mvnw(crc, crc);
3416 
3417     adrp(table0, ExternalAddress(StubRoutines::crc_table_addr()), offset);
3418     if (offset) add(table0, table0, offset);
3419     add(table1, table0, 1*256*sizeof(juint));
3420     add(table2, table0, 2*256*sizeof(juint));
3421     add(table3, table0, 3*256*sizeof(juint));
3422 
3423   if (UseNeon) {
3424       cmp(len, (u1)64);
3425       br(Assembler::LT, L_by16);
3426       eor(v16, T16B, v16, v16);
3427 
3428     Label L_fold;
3429 
3430       add(tmp, table0, 4*256*sizeof(juint)); // Point at the Neon constants
3431 
3432       ld1(v0, v1, T2D, post(buf, 32));
3433       ld1r(v4, T2D, post(tmp, 8));
3434       ld1r(v5, T2D, post(tmp, 8));
3435       ld1r(v6, T2D, post(tmp, 8));
3436       ld1r(v7, T2D, post(tmp, 8));
3437       mov(v16, T4S, 0, crc);
3438 
3439       eor(v0, T16B, v0, v16);
3440       sub(len, len, 64);
3441 
3442     BIND(L_fold);
3443       pmull(v22, T8H, v0, v5, T8B);
3444       pmull(v20, T8H, v0, v7, T8B);
3445       pmull(v23, T8H, v0, v4, T8B);
3446       pmull(v21, T8H, v0, v6, T8B);
3447 
3448       pmull2(v18, T8H, v0, v5, T16B);
3449       pmull2(v16, T8H, v0, v7, T16B);
3450       pmull2(v19, T8H, v0, v4, T16B);
3451       pmull2(v17, T8H, v0, v6, T16B);
3452 
3453       uzp1(v24, T8H, v20, v22);
3454       uzp2(v25, T8H, v20, v22);
3455       eor(v20, T16B, v24, v25);
3456 
3457       uzp1(v26, T8H, v16, v18);
3458       uzp2(v27, T8H, v16, v18);
3459       eor(v16, T16B, v26, v27);
3460 
3461       ushll2(v22, T4S, v20, T8H, 8);
3462       ushll(v20, T4S, v20, T4H, 8);
3463 
3464       ushll2(v18, T4S, v16, T8H, 8);
3465       ushll(v16, T4S, v16, T4H, 8);
3466 
3467       eor(v22, T16B, v23, v22);
3468       eor(v18, T16B, v19, v18);
3469       eor(v20, T16B, v21, v20);
3470       eor(v16, T16B, v17, v16);
3471 
3472       uzp1(v17, T2D, v16, v20);
3473       uzp2(v21, T2D, v16, v20);
3474       eor(v17, T16B, v17, v21);
3475 
3476       ushll2(v20, T2D, v17, T4S, 16);
3477       ushll(v16, T2D, v17, T2S, 16);
3478 
3479       eor(v20, T16B, v20, v22);
3480       eor(v16, T16B, v16, v18);
3481 
3482       uzp1(v17, T2D, v20, v16);
3483       uzp2(v21, T2D, v20, v16);
3484       eor(v28, T16B, v17, v21);
3485 
3486       pmull(v22, T8H, v1, v5, T8B);
3487       pmull(v20, T8H, v1, v7, T8B);
3488       pmull(v23, T8H, v1, v4, T8B);
3489       pmull(v21, T8H, v1, v6, T8B);
3490 
3491       pmull2(v18, T8H, v1, v5, T16B);
3492       pmull2(v16, T8H, v1, v7, T16B);
3493       pmull2(v19, T8H, v1, v4, T16B);
3494       pmull2(v17, T8H, v1, v6, T16B);
3495 
3496       ld1(v0, v1, T2D, post(buf, 32));
3497 
3498       uzp1(v24, T8H, v20, v22);
3499       uzp2(v25, T8H, v20, v22);
3500       eor(v20, T16B, v24, v25);
3501 
3502       uzp1(v26, T8H, v16, v18);
3503       uzp2(v27, T8H, v16, v18);
3504       eor(v16, T16B, v26, v27);
3505 
3506       ushll2(v22, T4S, v20, T8H, 8);
3507       ushll(v20, T4S, v20, T4H, 8);
3508 
3509       ushll2(v18, T4S, v16, T8H, 8);
3510       ushll(v16, T4S, v16, T4H, 8);
3511 
3512       eor(v22, T16B, v23, v22);
3513       eor(v18, T16B, v19, v18);
3514       eor(v20, T16B, v21, v20);
3515       eor(v16, T16B, v17, v16);
3516 
3517       uzp1(v17, T2D, v16, v20);
3518       uzp2(v21, T2D, v16, v20);
3519       eor(v16, T16B, v17, v21);
3520 
3521       ushll2(v20, T2D, v16, T4S, 16);
3522       ushll(v16, T2D, v16, T2S, 16);
3523 
3524       eor(v20, T16B, v22, v20);
3525       eor(v16, T16B, v16, v18);
3526 
3527       uzp1(v17, T2D, v20, v16);
3528       uzp2(v21, T2D, v20, v16);
3529       eor(v20, T16B, v17, v21);
3530 
3531       shl(v16, T2D, v28, 1);
3532       shl(v17, T2D, v20, 1);
3533 
3534       eor(v0, T16B, v0, v16);
3535       eor(v1, T16B, v1, v17);
3536 
3537       subs(len, len, 32);
3538       br(Assembler::GE, L_fold);
3539 
3540       mov(crc, 0);
3541       mov(tmp, v0, T1D, 0);
3542       update_word_crc32(crc, tmp, tmp2, table0, table1, table2, table3, false);
3543       update_word_crc32(crc, tmp, tmp2, table0, table1, table2, table3, true);
3544       mov(tmp, v0, T1D, 1);
3545       update_word_crc32(crc, tmp, tmp2, table0, table1, table2, table3, false);
3546       update_word_crc32(crc, tmp, tmp2, table0, table1, table2, table3, true);
3547       mov(tmp, v1, T1D, 0);
3548       update_word_crc32(crc, tmp, tmp2, table0, table1, table2, table3, false);
3549       update_word_crc32(crc, tmp, tmp2, table0, table1, table2, table3, true);
3550       mov(tmp, v1, T1D, 1);
3551       update_word_crc32(crc, tmp, tmp2, table0, table1, table2, table3, false);
3552       update_word_crc32(crc, tmp, tmp2, table0, table1, table2, table3, true);
3553 
3554       add(len, len, 32);
3555   }
3556 
3557   BIND(L_by16);
3558     subs(len, len, 16);
3559     br(Assembler::GE, L_by16_loop);
3560     adds(len, len, 16-4);
3561     br(Assembler::GE, L_by4_loop);
3562     adds(len, len, 4);
3563     br(Assembler::GT, L_by1_loop);
3564     b(L_exit);
3565 
3566   BIND(L_by4_loop);
3567     ldrw(tmp, Address(post(buf, 4)));
3568     update_word_crc32(crc, tmp, tmp2, table0, table1, table2, table3);
3569     subs(len, len, 4);
3570     br(Assembler::GE, L_by4_loop);
3571     adds(len, len, 4);
3572     br(Assembler::LE, L_exit);
3573   BIND(L_by1_loop);
3574     subs(len, len, 1);
3575     ldrb(tmp, Address(post(buf, 1)));
3576     update_byte_crc32(crc, tmp, table0);
3577     br(Assembler::GT, L_by1_loop);
3578     b(L_exit);
3579 
3580     align(CodeEntryAlignment);
3581   BIND(L_by16_loop);
3582     subs(len, len, 16);
3583     ldp(tmp, tmp3, Address(post(buf, 16)));
3584     update_word_crc32(crc, tmp, tmp2, table0, table1, table2, table3, false);
3585     update_word_crc32(crc, tmp, tmp2, table0, table1, table2, table3, true);
3586     update_word_crc32(crc, tmp3, tmp2, table0, table1, table2, table3, false);
3587     update_word_crc32(crc, tmp3, tmp2, table0, table1, table2, table3, true);
3588     br(Assembler::GE, L_by16_loop);
3589     adds(len, len, 16-4);
3590     br(Assembler::GE, L_by4_loop);
3591     adds(len, len, 4);
3592     br(Assembler::GT, L_by1_loop);
3593   BIND(L_exit);
3594     mvnw(crc, crc);
3595 }
3596 
3597 void MacroAssembler::kernel_crc32c_using_crc32c(Register crc, Register buf,
3598         Register len, Register tmp0, Register tmp1, Register tmp2,
3599         Register tmp3) {
3600     Label CRC_by64_loop, CRC_by4_loop, CRC_by1_loop, CRC_less64, CRC_by64_pre, CRC_by32_loop, CRC_less32, L_exit;
3601     assert_different_registers(crc, buf, len, tmp0, tmp1, tmp2, tmp3);
3602 
3603     subs(len, len, 128);
3604     br(Assembler::GE, CRC_by64_pre);
3605   BIND(CRC_less64);
3606     adds(len, len, 128-32);
3607     br(Assembler::GE, CRC_by32_loop);
3608   BIND(CRC_less32);
3609     adds(len, len, 32-4);
3610     br(Assembler::GE, CRC_by4_loop);
3611     adds(len, len, 4);
3612     br(Assembler::GT, CRC_by1_loop);
3613     b(L_exit);
3614 
3615   BIND(CRC_by32_loop);
3616     ldp(tmp0, tmp1, Address(post(buf, 16)));
3617     subs(len, len, 32);
3618     crc32cx(crc, crc, tmp0);
3619     ldr(tmp2, Address(post(buf, 8)));
3620     crc32cx(crc, crc, tmp1);
3621     ldr(tmp3, Address(post(buf, 8)));
3622     crc32cx(crc, crc, tmp2);
3623     crc32cx(crc, crc, tmp3);
3624     br(Assembler::GE, CRC_by32_loop);
3625     cmn(len, 32);
3626     br(Assembler::NE, CRC_less32);
3627     b(L_exit);
3628 
3629   BIND(CRC_by4_loop);
3630     ldrw(tmp0, Address(post(buf, 4)));
3631     subs(len, len, 4);
3632     crc32cw(crc, crc, tmp0);
3633     br(Assembler::GE, CRC_by4_loop);
3634     adds(len, len, 4);
3635     br(Assembler::LE, L_exit);
3636   BIND(CRC_by1_loop);
3637     ldrb(tmp0, Address(post(buf, 1)));
3638     subs(len, len, 1);
3639     crc32cb(crc, crc, tmp0);
3640     br(Assembler::GT, CRC_by1_loop);
3641     b(L_exit);
3642 
3643   BIND(CRC_by64_pre);
3644     sub(buf, buf, 8);
3645     ldp(tmp0, tmp1, Address(buf, 8));
3646     crc32cx(crc, crc, tmp0);
3647     ldr(tmp2, Address(buf, 24));
3648     crc32cx(crc, crc, tmp1);
3649     ldr(tmp3, Address(buf, 32));
3650     crc32cx(crc, crc, tmp2);
3651     ldr(tmp0, Address(buf, 40));
3652     crc32cx(crc, crc, tmp3);
3653     ldr(tmp1, Address(buf, 48));
3654     crc32cx(crc, crc, tmp0);
3655     ldr(tmp2, Address(buf, 56));
3656     crc32cx(crc, crc, tmp1);
3657     ldr(tmp3, Address(pre(buf, 64)));
3658 
3659     b(CRC_by64_loop);
3660 
3661     align(CodeEntryAlignment);
3662   BIND(CRC_by64_loop);
3663     subs(len, len, 64);
3664     crc32cx(crc, crc, tmp2);
3665     ldr(tmp0, Address(buf, 8));
3666     crc32cx(crc, crc, tmp3);
3667     ldr(tmp1, Address(buf, 16));
3668     crc32cx(crc, crc, tmp0);
3669     ldr(tmp2, Address(buf, 24));
3670     crc32cx(crc, crc, tmp1);
3671     ldr(tmp3, Address(buf, 32));
3672     crc32cx(crc, crc, tmp2);
3673     ldr(tmp0, Address(buf, 40));
3674     crc32cx(crc, crc, tmp3);
3675     ldr(tmp1, Address(buf, 48));
3676     crc32cx(crc, crc, tmp0);
3677     ldr(tmp2, Address(buf, 56));
3678     crc32cx(crc, crc, tmp1);
3679     ldr(tmp3, Address(pre(buf, 64)));
3680     br(Assembler::GE, CRC_by64_loop);
3681 
3682     // post-loop
3683     crc32cx(crc, crc, tmp2);
3684     crc32cx(crc, crc, tmp3);
3685 
3686     sub(len, len, 64);
3687     add(buf, buf, 8);
3688     cmn(len, 128);
3689     br(Assembler::NE, CRC_less64);
3690   BIND(L_exit);
3691 }
3692 
3693 /**
3694  * @param crc   register containing existing CRC (32-bit)
3695  * @param buf   register pointing to input byte buffer (byte*)
3696  * @param len   register containing number of bytes
3697  * @param table register that will contain address of CRC table
3698  * @param tmp   scratch register
3699  */
3700 void MacroAssembler::kernel_crc32c(Register crc, Register buf, Register len,
3701         Register table0, Register table1, Register table2, Register table3,
3702         Register tmp, Register tmp2, Register tmp3) {
3703   kernel_crc32c_using_crc32c(crc, buf, len, table0, table1, table2, table3);
3704 }
3705 
3706 
3707 SkipIfEqual::SkipIfEqual(
3708     MacroAssembler* masm, const bool* flag_addr, bool value) {
3709   _masm = masm;
3710   unsigned long offset;
3711   _masm-&gt;adrp(rscratch1, ExternalAddress((address)flag_addr), offset);
3712   _masm-&gt;ldrb(rscratch1, Address(rscratch1, offset));
3713   _masm-&gt;cbzw(rscratch1, _label);
3714 }
3715 
3716 SkipIfEqual::~SkipIfEqual() {
3717   _masm-&gt;bind(_label);
3718 }
3719 
3720 void MacroAssembler::addptr(const Address &amp;dst, int32_t src) {
3721   Address adr;
3722   switch(dst.getMode()) {
3723   case Address::base_plus_offset:
3724     // This is the expected mode, although we allow all the other
3725     // forms below.
3726     adr = form_address(rscratch2, dst.base(), dst.offset(), LogBytesPerWord);
3727     break;
3728   default:
3729     lea(rscratch2, dst);
3730     adr = Address(rscratch2);
3731     break;
3732   }
3733   ldr(rscratch1, adr);
3734   add(rscratch1, rscratch1, src);
3735   str(rscratch1, adr);
3736 }
3737 
3738 void MacroAssembler::cmpptr(Register src1, Address src2) {
3739   unsigned long offset;
3740   adrp(rscratch1, src2, offset);
3741   ldr(rscratch1, Address(rscratch1, offset));
3742   cmp(src1, rscratch1);
3743 }
3744 
3745 void MacroAssembler::cmpoop(Register obj1, Register obj2) {
3746   BarrierSetAssembler* bs = BarrierSet::barrier_set()-&gt;barrier_set_assembler();
3747   bs-&gt;obj_equals(this, obj1, obj2);
3748 }
3749 
3750 void MacroAssembler::load_method_holder(Register holder, Register method) {
3751   ldr(holder, Address(method, Method::const_offset()));                      // ConstMethod*
3752   ldr(holder, Address(holder, ConstMethod::constants_offset()));             // ConstantPool*
3753   ldr(holder, Address(holder, ConstantPool::pool_holder_offset_in_bytes())); // InstanceKlass*
3754 }
3755 
3756 void MacroAssembler::load_metadata(Register dst, Register src) {
3757   if (UseCompressedClassPointers) {
3758     ldrw(dst, Address(src, oopDesc::klass_offset_in_bytes()));
3759   } else {
3760     ldr(dst, Address(src, oopDesc::klass_offset_in_bytes()));
3761   }
3762 }
3763 
3764 void MacroAssembler::load_klass(Register dst, Register src) {
3765   load_metadata(dst, src);
3766   if (UseCompressedClassPointers) {
3767     andr(dst, dst, oopDesc::compressed_klass_mask());
3768     decode_klass_not_null(dst);
3769   } else {
3770     ubfm(dst, dst, 0, 63 - oopDesc::storage_props_nof_bits);
3771   }
3772 }
3773 
3774 // ((OopHandle)result).resolve();
3775 void MacroAssembler::resolve_oop_handle(Register result, Register tmp) {
3776   // OopHandle::resolve is an indirection.
3777   access_load_at(T_OBJECT, IN_NATIVE, result, Address(result, 0), tmp, noreg);
3778 }
3779 
3780 void MacroAssembler::load_mirror(Register dst, Register method, Register tmp) {
3781   const int mirror_offset = in_bytes(Klass::java_mirror_offset());
3782   ldr(dst, Address(rmethod, Method::const_offset()));
3783   ldr(dst, Address(dst, ConstMethod::constants_offset()));
3784   ldr(dst, Address(dst, ConstantPool::pool_holder_offset_in_bytes()));
3785   ldr(dst, Address(dst, mirror_offset));
3786   resolve_oop_handle(dst, tmp);
3787 }
3788 
3789 void MacroAssembler::load_storage_props(Register dst, Register src) {
3790   load_metadata(dst, src);
3791   if (UseCompressedClassPointers) {
3792     asrw(dst, dst, oopDesc::narrow_storage_props_shift);
3793   } else {
3794     asr(dst, dst, oopDesc::wide_storage_props_shift);
3795   }
3796 }
3797 
3798 void MacroAssembler::cmp_klass(Register oop, Register trial_klass, Register tmp) {
3799   if (UseCompressedClassPointers) {
3800     ldrw(tmp, Address(oop, oopDesc::klass_offset_in_bytes()));
3801     if (CompressedKlassPointers::base() == NULL) {
3802       cmp(trial_klass, tmp, LSL, CompressedKlassPointers::shift());
3803       return;
3804     } else if (((uint64_t)CompressedKlassPointers::base() &amp; 0xffffffff) == 0
3805                &amp;&amp; CompressedKlassPointers::shift() == 0) {
3806       // Only the bottom 32 bits matter
3807       cmpw(trial_klass, tmp);
3808       return;
3809     }
3810     decode_klass_not_null(tmp);
3811   } else {
3812     ldr(tmp, Address(oop, oopDesc::klass_offset_in_bytes()));
3813   }
3814   cmp(trial_klass, tmp);
3815 }
3816 
3817 void MacroAssembler::load_prototype_header(Register dst, Register src) {
3818   load_klass(dst, src);
3819   ldr(dst, Address(dst, Klass::prototype_header_offset()));
3820 }
3821 
3822 void MacroAssembler::store_klass(Register dst, Register src) {
3823   // FIXME: Should this be a store release?  concurrent gcs assumes
3824   // klass length is valid if klass field is not null.
3825   if (UseCompressedClassPointers) {
3826     encode_klass_not_null(src);
3827     strw(src, Address(dst, oopDesc::klass_offset_in_bytes()));
3828   } else {
3829     str(src, Address(dst, oopDesc::klass_offset_in_bytes()));
3830   }
3831 }
3832 
3833 void MacroAssembler::store_klass_gap(Register dst, Register src) {
3834   if (UseCompressedClassPointers) {
3835     // Store to klass gap in destination
3836     strw(src, Address(dst, oopDesc::klass_gap_offset_in_bytes()));
3837   }
3838 }
3839 
3840 // Algorithm must match CompressedOops::encode.
3841 void MacroAssembler::encode_heap_oop(Register d, Register s) {
3842 #ifdef ASSERT
3843   verify_heapbase(&quot;MacroAssembler::encode_heap_oop: heap base corrupted?&quot;);
3844 #endif
3845   verify_oop(s, &quot;broken oop in encode_heap_oop&quot;);
3846   if (CompressedOops::base() == NULL) {
3847     if (CompressedOops::shift() != 0) {
3848       assert (LogMinObjAlignmentInBytes == CompressedOops::shift(), &quot;decode alg wrong&quot;);
3849       lsr(d, s, LogMinObjAlignmentInBytes);
3850     } else {
3851       mov(d, s);
3852     }
3853   } else {
3854     subs(d, s, rheapbase);
3855     csel(d, d, zr, Assembler::HS);
3856     lsr(d, d, LogMinObjAlignmentInBytes);
3857 
3858     /*  Old algorithm: is this any worse?
3859     Label nonnull;
3860     cbnz(r, nonnull);
3861     sub(r, r, rheapbase);
3862     bind(nonnull);
3863     lsr(r, r, LogMinObjAlignmentInBytes);
3864     */
3865   }
3866 }
3867 
3868 void MacroAssembler::encode_heap_oop_not_null(Register r) {
3869 #ifdef ASSERT
3870   verify_heapbase(&quot;MacroAssembler::encode_heap_oop_not_null: heap base corrupted?&quot;);
3871   if (CheckCompressedOops) {
3872     Label ok;
3873     cbnz(r, ok);
3874     stop(&quot;null oop passed to encode_heap_oop_not_null&quot;);
3875     bind(ok);
3876   }
3877 #endif
3878   verify_oop(r, &quot;broken oop in encode_heap_oop_not_null&quot;);
3879   if (CompressedOops::base() != NULL) {
3880     sub(r, r, rheapbase);
3881   }
3882   if (CompressedOops::shift() != 0) {
3883     assert (LogMinObjAlignmentInBytes == CompressedOops::shift(), &quot;decode alg wrong&quot;);
3884     lsr(r, r, LogMinObjAlignmentInBytes);
3885   }
3886 }
3887 
3888 void MacroAssembler::encode_heap_oop_not_null(Register dst, Register src) {
3889 #ifdef ASSERT
3890   verify_heapbase(&quot;MacroAssembler::encode_heap_oop_not_null2: heap base corrupted?&quot;);
3891   if (CheckCompressedOops) {
3892     Label ok;
3893     cbnz(src, ok);
3894     stop(&quot;null oop passed to encode_heap_oop_not_null2&quot;);
3895     bind(ok);
3896   }
3897 #endif
3898   verify_oop(src, &quot;broken oop in encode_heap_oop_not_null2&quot;);
3899 
3900   Register data = src;
3901   if (CompressedOops::base() != NULL) {
3902     sub(dst, src, rheapbase);
3903     data = dst;
3904   }
3905   if (CompressedOops::shift() != 0) {
3906     assert (LogMinObjAlignmentInBytes == CompressedOops::shift(), &quot;decode alg wrong&quot;);
3907     lsr(dst, data, LogMinObjAlignmentInBytes);
3908     data = dst;
3909   }
3910   if (data == src)
3911     mov(dst, src);
3912 }
3913 
3914 void  MacroAssembler::decode_heap_oop(Register d, Register s) {
3915 #ifdef ASSERT
3916   verify_heapbase(&quot;MacroAssembler::decode_heap_oop: heap base corrupted?&quot;);
3917 #endif
3918   if (CompressedOops::base() == NULL) {
3919     if (CompressedOops::shift() != 0 || d != s) {
3920       lsl(d, s, CompressedOops::shift());
3921     }
3922   } else {
3923     Label done;
3924     if (d != s)
3925       mov(d, s);
3926     cbz(s, done);
3927     add(d, rheapbase, s, Assembler::LSL, LogMinObjAlignmentInBytes);
3928     bind(done);
3929   }
3930   verify_oop(d, &quot;broken oop in decode_heap_oop&quot;);
3931 }
3932 
3933 void  MacroAssembler::decode_heap_oop_not_null(Register r) {
3934   assert (UseCompressedOops, &quot;should only be used for compressed headers&quot;);
3935   assert (Universe::heap() != NULL, &quot;java heap should be initialized&quot;);
3936   // Cannot assert, unverified entry point counts instructions (see .ad file)
3937   // vtableStubs also counts instructions in pd_code_size_limit.
3938   // Also do not verify_oop as this is called by verify_oop.
3939   if (CompressedOops::shift() != 0) {
3940     assert(LogMinObjAlignmentInBytes == CompressedOops::shift(), &quot;decode alg wrong&quot;);
3941     if (CompressedOops::base() != NULL) {
3942       add(r, rheapbase, r, Assembler::LSL, LogMinObjAlignmentInBytes);
3943     } else {
3944       add(r, zr, r, Assembler::LSL, LogMinObjAlignmentInBytes);
3945     }
3946   } else {
3947     assert (CompressedOops::base() == NULL, &quot;sanity&quot;);
3948   }
3949 }
3950 
3951 void  MacroAssembler::decode_heap_oop_not_null(Register dst, Register src) {
3952   assert (UseCompressedOops, &quot;should only be used for compressed headers&quot;);
3953   assert (Universe::heap() != NULL, &quot;java heap should be initialized&quot;);
3954   // Cannot assert, unverified entry point counts instructions (see .ad file)
3955   // vtableStubs also counts instructions in pd_code_size_limit.
3956   // Also do not verify_oop as this is called by verify_oop.
3957   if (CompressedOops::shift() != 0) {
3958     assert(LogMinObjAlignmentInBytes == CompressedOops::shift(), &quot;decode alg wrong&quot;);
3959     if (CompressedOops::base() != NULL) {
3960       add(dst, rheapbase, src, Assembler::LSL, LogMinObjAlignmentInBytes);
3961     } else {
3962       add(dst, zr, src, Assembler::LSL, LogMinObjAlignmentInBytes);
3963     }
3964   } else {
3965     assert (CompressedOops::base() == NULL, &quot;sanity&quot;);
3966     if (dst != src) {
3967       mov(dst, src);
3968     }
3969   }
3970 }
3971 
3972 MacroAssembler::KlassDecodeMode MacroAssembler::_klass_decode_mode(KlassDecodeNone);
3973 
3974 MacroAssembler::KlassDecodeMode MacroAssembler::klass_decode_mode() {
3975   assert(UseCompressedClassPointers, &quot;not using compressed class pointers&quot;);
3976   assert(Metaspace::initialized(), &quot;metaspace not initialized yet&quot;);
3977 
3978   if (_klass_decode_mode != KlassDecodeNone) {
3979     return _klass_decode_mode;
3980   }
3981 
3982   assert(LogKlassAlignmentInBytes == CompressedKlassPointers::shift()
3983          || 0 == CompressedKlassPointers::shift(), &quot;decode alg wrong&quot;);
3984 
3985   if (CompressedKlassPointers::base() == NULL) {
3986     return (_klass_decode_mode = KlassDecodeZero);
3987   }
3988 
3989   if (operand_valid_for_logical_immediate(
3990         /*is32*/false, (uint64_t)CompressedKlassPointers::base())) {
3991     const uint64_t range_mask =
3992       (1UL &lt;&lt; log2_intptr(CompressedKlassPointers::range())) - 1;
3993     if (((uint64_t)CompressedKlassPointers::base() &amp; range_mask) == 0) {
3994       return (_klass_decode_mode = KlassDecodeXor);
3995     }
3996   }
3997 
3998   const uint64_t shifted_base =
3999     (uint64_t)CompressedKlassPointers::base() &gt;&gt; CompressedKlassPointers::shift();
4000   guarantee((shifted_base &amp; 0xffff0000ffffffff) == 0,
4001             &quot;compressed class base bad alignment&quot;);
4002 
4003   return (_klass_decode_mode = KlassDecodeMovk);
4004 }
4005 
4006 void MacroAssembler::encode_klass_not_null(Register dst, Register src) {
4007   switch (klass_decode_mode()) {
4008   case KlassDecodeZero:
4009     if (CompressedKlassPointers::shift() != 0) {
4010       lsr(dst, src, LogKlassAlignmentInBytes);
4011     } else {
4012       if (dst != src) mov(dst, src);
4013     }
4014     break;
4015 
4016   case KlassDecodeXor:
4017     if (CompressedKlassPointers::shift() != 0) {
4018       eor(dst, src, (uint64_t)CompressedKlassPointers::base());
4019       lsr(dst, dst, LogKlassAlignmentInBytes);
4020     } else {
4021       eor(dst, src, (uint64_t)CompressedKlassPointers::base());
4022     }
4023     break;
4024 
4025   case KlassDecodeMovk:
4026     if (CompressedKlassPointers::shift() != 0) {
4027       ubfx(dst, src, LogKlassAlignmentInBytes, 32);
4028     } else {
4029       movw(dst, src);
4030     }
4031     break;
4032 
4033   case KlassDecodeNone:
4034     ShouldNotReachHere();
4035     break;
4036   }
4037 }
4038 
4039 void MacroAssembler::encode_klass_not_null(Register r) {
4040   encode_klass_not_null(r, r);
4041 }
4042 
4043 void  MacroAssembler::decode_klass_not_null(Register dst, Register src) {
4044   assert (UseCompressedClassPointers, &quot;should only be used for compressed headers&quot;);
4045 
4046   switch (klass_decode_mode()) {
4047   case KlassDecodeZero:
4048     if (CompressedKlassPointers::shift() != 0) {
4049       lsl(dst, src, LogKlassAlignmentInBytes);
4050     } else {
4051       if (dst != src) mov(dst, src);
4052     }
4053     break;
4054 
4055   case KlassDecodeXor:
4056     if (CompressedKlassPointers::shift() != 0) {
4057       lsl(dst, src, LogKlassAlignmentInBytes);
4058       eor(dst, dst, (uint64_t)CompressedKlassPointers::base());
4059     } else {
4060       eor(dst, src, (uint64_t)CompressedKlassPointers::base());
4061     }
4062     break;
4063 
4064   case KlassDecodeMovk: {
4065     const uint64_t shifted_base =
4066       (uint64_t)CompressedKlassPointers::base() &gt;&gt; CompressedKlassPointers::shift();
4067 
4068     if (dst != src) movw(dst, src);
4069     movk(dst, shifted_base &gt;&gt; 32, 32);
4070 
4071     if (CompressedKlassPointers::shift() != 0) {
4072       lsl(dst, dst, LogKlassAlignmentInBytes);
4073     }
4074 
4075     break;
4076   }
4077 
4078   case KlassDecodeNone:
4079     ShouldNotReachHere();
4080     break;
4081   }
4082 }
4083 
4084 void  MacroAssembler::decode_klass_not_null(Register r) {
4085   decode_klass_not_null(r, r);
4086 }
4087 
4088 void  MacroAssembler::set_narrow_oop(Register dst, jobject obj) {
4089 #ifdef ASSERT
4090   {
4091     ThreadInVMfromUnknown tiv;
4092     assert (UseCompressedOops, &quot;should only be used for compressed oops&quot;);
4093     assert (Universe::heap() != NULL, &quot;java heap should be initialized&quot;);
4094     assert (oop_recorder() != NULL, &quot;this assembler needs an OopRecorder&quot;);
4095     assert(Universe::heap()-&gt;is_in(JNIHandles::resolve(obj)), &quot;should be real oop&quot;);
4096   }
4097 #endif
4098   int oop_index = oop_recorder()-&gt;find_index(obj);
4099   InstructionMark im(this);
4100   RelocationHolder rspec = oop_Relocation::spec(oop_index);
4101   code_section()-&gt;relocate(inst_mark(), rspec);
4102   movz(dst, 0xDEAD, 16);
4103   movk(dst, 0xBEEF);
4104 }
4105 
4106 void  MacroAssembler::set_narrow_klass(Register dst, Klass* k) {
4107   assert (UseCompressedClassPointers, &quot;should only be used for compressed headers&quot;);
4108   assert (oop_recorder() != NULL, &quot;this assembler needs an OopRecorder&quot;);
4109   int index = oop_recorder()-&gt;find_index(k);
4110   assert(! Universe::heap()-&gt;is_in(k), &quot;should not be an oop&quot;);
4111 
4112   InstructionMark im(this);
4113   RelocationHolder rspec = metadata_Relocation::spec(index);
4114   code_section()-&gt;relocate(inst_mark(), rspec);
4115   narrowKlass nk = CompressedKlassPointers::encode(k);
4116   movz(dst, (nk &gt;&gt; 16), 16);
4117   movk(dst, nk &amp; 0xffff);
4118 }
4119 
4120 void MacroAssembler::access_load_at(BasicType type, DecoratorSet decorators,
4121                                     Register dst, Address src,
4122                                     Register tmp1, Register thread_tmp) {
4123   BarrierSetAssembler *bs = BarrierSet::barrier_set()-&gt;barrier_set_assembler();
4124   decorators = AccessInternal::decorator_fixup(decorators);
4125   bool as_raw = (decorators &amp; AS_RAW) != 0;
4126   if (as_raw) {
4127     bs-&gt;BarrierSetAssembler::load_at(this, decorators, type, dst, src, tmp1, thread_tmp);
4128   } else {
4129     bs-&gt;load_at(this, decorators, type, dst, src, tmp1, thread_tmp);
4130   }
4131 }
4132 
4133 void MacroAssembler::access_store_at(BasicType type, DecoratorSet decorators,
4134                                      Address dst, Register src,
4135                                      Register tmp1, Register thread_tmp, Register tmp3) {
4136 
4137   BarrierSetAssembler *bs = BarrierSet::barrier_set()-&gt;barrier_set_assembler();
4138   decorators = AccessInternal::decorator_fixup(decorators);
4139   bool as_raw = (decorators &amp; AS_RAW) != 0;
4140   if (as_raw) {
4141     bs-&gt;BarrierSetAssembler::store_at(this, decorators, type, dst, src, tmp1, thread_tmp, tmp3);
4142   } else {
4143     bs-&gt;store_at(this, decorators, type, dst, src, tmp1, thread_tmp, tmp3);
4144   }
4145 }
4146 
4147 void MacroAssembler::resolve(DecoratorSet decorators, Register obj) {
4148   // Use stronger ACCESS_WRITE|ACCESS_READ by default.
4149   if ((decorators &amp; (ACCESS_READ | ACCESS_WRITE)) == 0) {
4150     decorators |= ACCESS_READ | ACCESS_WRITE;
4151   }
4152   BarrierSetAssembler* bs = BarrierSet::barrier_set()-&gt;barrier_set_assembler();
4153   return bs-&gt;resolve(this, decorators, obj);
4154 }
4155 
4156 void MacroAssembler::load_heap_oop(Register dst, Address src, Register tmp1,
4157                                    Register thread_tmp, DecoratorSet decorators) {
4158   access_load_at(T_OBJECT, IN_HEAP | decorators, dst, src, tmp1, thread_tmp);
4159 }
4160 
4161 void MacroAssembler::load_heap_oop_not_null(Register dst, Address src, Register tmp1,
4162                                             Register thread_tmp, DecoratorSet decorators) {
4163   access_load_at(T_OBJECT, IN_HEAP | IS_NOT_NULL | decorators, dst, src, tmp1, thread_tmp);
4164 }
4165 
4166 void MacroAssembler::store_heap_oop(Address dst, Register src, Register tmp1,
4167                                     Register thread_tmp, Register tmp3, DecoratorSet decorators) {
4168   access_store_at(T_OBJECT, IN_HEAP | decorators, dst, src, tmp1, thread_tmp, tmp3);
4169 }
4170 
4171 // Used for storing NULLs.
4172 void MacroAssembler::store_heap_oop_null(Address dst) {
4173   access_store_at(T_OBJECT, IN_HEAP, dst, noreg, noreg, noreg, noreg);
4174 }
4175 
4176 Address MacroAssembler::allocate_metadata_address(Metadata* obj) {
4177   assert(oop_recorder() != NULL, &quot;this assembler needs a Recorder&quot;);
4178   int index = oop_recorder()-&gt;allocate_metadata_index(obj);
4179   RelocationHolder rspec = metadata_Relocation::spec(index);
4180   return Address((address)obj, rspec);
4181 }
4182 
4183 // Move an oop into a register.  immediate is true if we want
4184 // immediate instrcutions, i.e. we are not going to patch this
4185 // instruction while the code is being executed by another thread.  In
4186 // that case we can use move immediates rather than the constant pool.
4187 void MacroAssembler::movoop(Register dst, jobject obj, bool immediate) {
4188   int oop_index;
4189   if (obj == NULL) {
4190     oop_index = oop_recorder()-&gt;allocate_oop_index(obj);
4191   } else {
4192 #ifdef ASSERT
4193     {
4194       ThreadInVMfromUnknown tiv;
4195       assert(Universe::heap()-&gt;is_in(JNIHandles::resolve(obj)), &quot;should be real oop&quot;);
4196     }
4197 #endif
4198     oop_index = oop_recorder()-&gt;find_index(obj);
4199   }
4200   RelocationHolder rspec = oop_Relocation::spec(oop_index);
4201   if (! immediate) {
4202     address dummy = address(uintptr_t(pc()) &amp; -wordSize); // A nearby aligned address
4203     ldr_constant(dst, Address(dummy, rspec));
4204   } else
4205     mov(dst, Address((address)obj, rspec));
4206 }
4207 
4208 // Move a metadata address into a register.
4209 void MacroAssembler::mov_metadata(Register dst, Metadata* obj) {
4210   int oop_index;
4211   if (obj == NULL) {
4212     oop_index = oop_recorder()-&gt;allocate_metadata_index(obj);
4213   } else {
4214     oop_index = oop_recorder()-&gt;find_index(obj);
4215   }
4216   RelocationHolder rspec = metadata_Relocation::spec(oop_index);
4217   mov(dst, Address((address)obj, rspec));
4218 }
4219 
4220 Address MacroAssembler::constant_oop_address(jobject obj) {
4221 #ifdef ASSERT
4222   {
4223     ThreadInVMfromUnknown tiv;
4224     assert(oop_recorder() != NULL, &quot;this assembler needs an OopRecorder&quot;);
4225     assert(Universe::heap()-&gt;is_in(JNIHandles::resolve(obj)), &quot;not an oop&quot;);
4226   }
4227 #endif
4228   int oop_index = oop_recorder()-&gt;find_index(obj);
4229   return Address((address)obj, oop_Relocation::spec(oop_index));
4230 }
4231 
4232 // Defines obj, preserves var_size_in_bytes, okay for t2 == var_size_in_bytes.
4233 void MacroAssembler::tlab_allocate(Register obj,
4234                                    Register var_size_in_bytes,
4235                                    int con_size_in_bytes,
4236                                    Register t1,
4237                                    Register t2,
4238                                    Label&amp; slow_case) {
4239   BarrierSetAssembler *bs = BarrierSet::barrier_set()-&gt;barrier_set_assembler();
4240   bs-&gt;tlab_allocate(this, obj, var_size_in_bytes, con_size_in_bytes, t1, t2, slow_case);
4241 }
4242 
4243 // Defines obj, preserves var_size_in_bytes
4244 void MacroAssembler::eden_allocate(Register obj,
4245                                    Register var_size_in_bytes,
4246                                    int con_size_in_bytes,
4247                                    Register t1,
4248                                    Label&amp; slow_case) {
4249   BarrierSetAssembler *bs = BarrierSet::barrier_set()-&gt;barrier_set_assembler();
4250   bs-&gt;eden_allocate(this, obj, var_size_in_bytes, con_size_in_bytes, t1, slow_case);
4251 }
4252 
4253 // Zero words; len is in bytes
4254 // Destroys all registers except addr
4255 // len must be a nonzero multiple of wordSize
4256 void MacroAssembler::zero_memory(Register addr, Register len, Register t1) {
4257   assert_different_registers(addr, len, t1, rscratch1, rscratch2);
4258 
4259 #ifdef ASSERT
4260   { Label L;
4261     tst(len, BytesPerWord - 1);
4262     br(Assembler::EQ, L);
4263     stop(&quot;len is not a multiple of BytesPerWord&quot;);
4264     bind(L);
4265   }
4266 #endif
4267 
4268 #ifndef PRODUCT
4269   block_comment(&quot;zero memory&quot;);
4270 #endif
4271 
4272   Label loop;
4273   Label entry;
4274 
4275 //  Algorithm:
4276 //
4277 //    scratch1 = cnt &amp; 7;
4278 //    cnt -= scratch1;
4279 //    p += scratch1;
4280 //    switch (scratch1) {
4281 //      do {
4282 //        cnt -= 8;
4283 //          p[-8] = 0;
4284 //        case 7:
4285 //          p[-7] = 0;
4286 //        case 6:
4287 //          p[-6] = 0;
4288 //          // ...
4289 //        case 1:
4290 //          p[-1] = 0;
4291 //        case 0:
4292 //          p += 8;
4293 //      } while (cnt);
4294 //    }
4295 
4296   const int unroll = 8; // Number of str(zr) instructions we&#39;ll unroll
4297 
4298   lsr(len, len, LogBytesPerWord);
4299   andr(rscratch1, len, unroll - 1);  // tmp1 = cnt % unroll
4300   sub(len, len, rscratch1);      // cnt -= unroll
4301   // t1 always points to the end of the region we&#39;re about to zero
4302   add(t1, addr, rscratch1, Assembler::LSL, LogBytesPerWord);
4303   adr(rscratch2, entry);
4304   sub(rscratch2, rscratch2, rscratch1, Assembler::LSL, 2);
4305   br(rscratch2);
4306   bind(loop);
4307   sub(len, len, unroll);
4308   for (int i = -unroll; i &lt; 0; i++)
4309     Assembler::str(zr, Address(t1, i * wordSize));
4310   bind(entry);
4311   add(t1, t1, unroll * wordSize);
4312   cbnz(len, loop);
4313 }
4314 
4315 void MacroAssembler::verify_tlab() {
4316 #ifdef ASSERT
4317   if (UseTLAB &amp;&amp; VerifyOops) {
4318     Label next, ok;
4319 
4320     stp(rscratch2, rscratch1, Address(pre(sp, -16)));
4321 
4322     ldr(rscratch2, Address(rthread, in_bytes(JavaThread::tlab_top_offset())));
4323     ldr(rscratch1, Address(rthread, in_bytes(JavaThread::tlab_start_offset())));
4324     cmp(rscratch2, rscratch1);
4325     br(Assembler::HS, next);
4326     STOP(&quot;assert(top &gt;= start)&quot;);
4327     should_not_reach_here();
4328 
4329     bind(next);
4330     ldr(rscratch2, Address(rthread, in_bytes(JavaThread::tlab_end_offset())));
4331     ldr(rscratch1, Address(rthread, in_bytes(JavaThread::tlab_top_offset())));
4332     cmp(rscratch2, rscratch1);
4333     br(Assembler::HS, ok);
4334     STOP(&quot;assert(top &lt;= end)&quot;);
4335     should_not_reach_here();
4336 
4337     bind(ok);
4338     ldp(rscratch2, rscratch1, Address(post(sp, 16)));
4339   }
4340 #endif
4341 }
4342 
4343 // Writes to stack successive pages until offset reached to check for
4344 // stack overflow + shadow pages.  This clobbers tmp.
4345 void MacroAssembler::bang_stack_size(Register size, Register tmp) {
4346   assert_different_registers(tmp, size, rscratch1);
4347   mov(tmp, sp);
4348   // Bang stack for total size given plus shadow page size.
4349   // Bang one page at a time because large size can bang beyond yellow and
4350   // red zones.
4351   Label loop;
4352   mov(rscratch1, os::vm_page_size());
4353   bind(loop);
4354   lea(tmp, Address(tmp, -os::vm_page_size()));
4355   subsw(size, size, rscratch1);
4356   str(size, Address(tmp));
4357   br(Assembler::GT, loop);
4358 
4359   // Bang down shadow pages too.
4360   // At this point, (tmp-0) is the last address touched, so don&#39;t
4361   // touch it again.  (It was touched as (tmp-pagesize) but then tmp
4362   // was post-decremented.)  Skip this address by starting at i=1, and
4363   // touch a few more pages below.  N.B.  It is important to touch all
4364   // the way down to and including i=StackShadowPages.
4365   for (int i = 0; i &lt; (int)(JavaThread::stack_shadow_zone_size() / os::vm_page_size()) - 1; i++) {
4366     // this could be any sized move but this is can be a debugging crumb
4367     // so the bigger the better.
4368     lea(tmp, Address(tmp, -os::vm_page_size()));
4369     str(size, Address(tmp));
4370   }
4371 }
4372 
4373 
4374 // Move the address of the polling page into dest.
4375 void MacroAssembler::get_polling_page(Register dest, address page, relocInfo::relocType rtype) {
4376   if (SafepointMechanism::uses_thread_local_poll()) {
4377     ldr(dest, Address(rthread, Thread::polling_page_offset()));
4378   } else {
4379     unsigned long off;
4380     adrp(dest, Address(page, rtype), off);
4381     assert(off == 0, &quot;polling page must be page aligned&quot;);
4382   }
4383 }
4384 
4385 // Move the address of the polling page into r, then read the polling
4386 // page.
4387 address MacroAssembler::read_polling_page(Register r, address page, relocInfo::relocType rtype) {
4388   get_polling_page(r, page, rtype);
4389   return read_polling_page(r, rtype);
4390 }
4391 
4392 // Read the polling page.  The address of the polling page must
4393 // already be in r.
4394 address MacroAssembler::read_polling_page(Register r, relocInfo::relocType rtype) {
4395   InstructionMark im(this);
4396   code_section()-&gt;relocate(inst_mark(), rtype);
4397   ldrw(zr, Address(r, 0));
4398   return inst_mark();
4399 }
4400 
4401 void MacroAssembler::adrp(Register reg1, const Address &amp;dest, unsigned long &amp;byte_offset) {
4402   relocInfo::relocType rtype = dest.rspec().reloc()-&gt;type();
4403   unsigned long low_page = (unsigned long)CodeCache::low_bound() &gt;&gt; 12;
4404   unsigned long high_page = (unsigned long)(CodeCache::high_bound()-1) &gt;&gt; 12;
4405   unsigned long dest_page = (unsigned long)dest.target() &gt;&gt; 12;
4406   long offset_low = dest_page - low_page;
4407   long offset_high = dest_page - high_page;
4408 
4409   assert(is_valid_AArch64_address(dest.target()), &quot;bad address&quot;);
4410   assert(dest.getMode() == Address::literal, &quot;ADRP must be applied to a literal address&quot;);
4411 
4412   InstructionMark im(this);
4413   code_section()-&gt;relocate(inst_mark(), dest.rspec());
4414   // 8143067: Ensure that the adrp can reach the dest from anywhere within
4415   // the code cache so that if it is relocated we know it will still reach
4416   if (offset_high &gt;= -(1&lt;&lt;20) &amp;&amp; offset_low &lt; (1&lt;&lt;20)) {
4417     _adrp(reg1, dest.target());
4418   } else {
4419     unsigned long target = (unsigned long)dest.target();
4420     unsigned long adrp_target
4421       = (target &amp; 0xffffffffUL) | ((unsigned long)pc() &amp; 0xffff00000000UL);
4422 
4423     _adrp(reg1, (address)adrp_target);
4424     movk(reg1, target &gt;&gt; 32, 32);
4425   }
4426   byte_offset = (unsigned long)dest.target() &amp; 0xfff;
4427 }
4428 
4429 void MacroAssembler::load_byte_map_base(Register reg) {
4430   CardTable::CardValue* byte_map_base =
4431     ((CardTableBarrierSet*)(BarrierSet::barrier_set()))-&gt;card_table()-&gt;byte_map_base();
4432 
4433   if (is_valid_AArch64_address((address)byte_map_base)) {
4434     // Strictly speaking the byte_map_base isn&#39;t an address at all,
4435     // and it might even be negative.
4436     unsigned long offset;
4437     adrp(reg, ExternalAddress((address)byte_map_base), offset);
4438     // We expect offset to be zero with most collectors.
4439     if (offset != 0) {
4440       add(reg, reg, offset);
4441     }
4442   } else {
4443     mov(reg, (uint64_t)byte_map_base);
4444   }
4445 }
4446 
4447 void MacroAssembler::build_frame(int framesize) {
4448   assert(framesize &gt; 0, &quot;framesize must be &gt; 0&quot;);
4449   if (framesize &lt; ((1 &lt;&lt; 9) + 2 * wordSize)) {
4450     sub(sp, sp, framesize);
4451     stp(rfp, lr, Address(sp, framesize - 2 * wordSize));
4452     if (PreserveFramePointer) add(rfp, sp, framesize - 2 * wordSize);
4453   } else {
4454     stp(rfp, lr, Address(pre(sp, -2 * wordSize)));
4455     if (PreserveFramePointer) mov(rfp, sp);
4456     if (framesize &lt; ((1 &lt;&lt; 12) + 2 * wordSize))
4457       sub(sp, sp, framesize - 2 * wordSize);
4458     else {
4459       mov(rscratch1, framesize - 2 * wordSize);
4460       sub(sp, sp, rscratch1);
4461     }
4462   }
4463 }
4464 
4465 void MacroAssembler::remove_frame(int framesize) {
4466   assert(framesize &gt; 0, &quot;framesize must be &gt; 0&quot;);
4467   if (framesize &lt; ((1 &lt;&lt; 9) + 2 * wordSize)) {
4468     ldp(rfp, lr, Address(sp, framesize - 2 * wordSize));
4469     add(sp, sp, framesize);
4470   } else {
4471     if (framesize &lt; ((1 &lt;&lt; 12) + 2 * wordSize))
4472       add(sp, sp, framesize - 2 * wordSize);
4473     else {
4474       mov(rscratch1, framesize - 2 * wordSize);
4475       add(sp, sp, rscratch1);
4476     }
4477     ldp(rfp, lr, Address(post(sp, 2 * wordSize)));
4478   }
4479 }
4480 
4481 #ifdef COMPILER2
4482 typedef void (MacroAssembler::* chr_insn)(Register Rt, const Address &amp;adr);
4483 
4484 // Search for str1 in str2 and return index or -1
4485 void MacroAssembler::string_indexof(Register str2, Register str1,
4486                                     Register cnt2, Register cnt1,
4487                                     Register tmp1, Register tmp2,
4488                                     Register tmp3, Register tmp4,
4489                                     Register tmp5, Register tmp6,
4490                                     int icnt1, Register result, int ae) {
4491   // NOTE: tmp5, tmp6 can be zr depending on specific method version
4492   Label LINEARSEARCH, LINEARSTUB, LINEAR_MEDIUM, DONE, NOMATCH, MATCH;
4493 
4494   Register ch1 = rscratch1;
4495   Register ch2 = rscratch2;
4496   Register cnt1tmp = tmp1;
4497   Register cnt2tmp = tmp2;
4498   Register cnt1_neg = cnt1;
4499   Register cnt2_neg = cnt2;
4500   Register result_tmp = tmp4;
4501 
4502   bool isL = ae == StrIntrinsicNode::LL;
4503 
4504   bool str1_isL = ae == StrIntrinsicNode::LL || ae == StrIntrinsicNode::UL;
4505   bool str2_isL = ae == StrIntrinsicNode::LL || ae == StrIntrinsicNode::LU;
4506   int str1_chr_shift = str1_isL ? 0:1;
4507   int str2_chr_shift = str2_isL ? 0:1;
4508   int str1_chr_size = str1_isL ? 1:2;
4509   int str2_chr_size = str2_isL ? 1:2;
4510   chr_insn str1_load_1chr = str1_isL ? (chr_insn)&amp;MacroAssembler::ldrb :
4511                                       (chr_insn)&amp;MacroAssembler::ldrh;
4512   chr_insn str2_load_1chr = str2_isL ? (chr_insn)&amp;MacroAssembler::ldrb :
4513                                       (chr_insn)&amp;MacroAssembler::ldrh;
4514   chr_insn load_2chr = isL ? (chr_insn)&amp;MacroAssembler::ldrh : (chr_insn)&amp;MacroAssembler::ldrw;
4515   chr_insn load_4chr = isL ? (chr_insn)&amp;MacroAssembler::ldrw : (chr_insn)&amp;MacroAssembler::ldr;
4516 
4517   // Note, inline_string_indexOf() generates checks:
4518   // if (substr.count &gt; string.count) return -1;
4519   // if (substr.count == 0) return 0;
4520 
4521   // We have two strings, a source string in str2, cnt2 and a pattern string
4522   // in str1, cnt1. Find the 1st occurence of pattern in source or return -1.
4523 
4524   // For larger pattern and source we use a simplified Boyer Moore algorithm.
4525   // With a small pattern and source we use linear scan.
4526 
4527   if (icnt1 == -1) {
4528     sub(result_tmp, cnt2, cnt1);
4529     cmp(cnt1, (u1)8);             // Use Linear Scan if cnt1 &lt; 8 || cnt1 &gt;= 256
4530     br(LT, LINEARSEARCH);
4531     dup(v0, T16B, cnt1); // done in separate FPU pipeline. Almost no penalty
4532     subs(zr, cnt1, 256);
4533     lsr(tmp1, cnt2, 2);
4534     ccmp(cnt1, tmp1, 0b0000, LT); // Source must be 4 * pattern for BM
4535     br(GE, LINEARSTUB);
4536   }
4537 
4538 // The Boyer Moore alogorithm is based on the description here:-
4539 //
4540 // http://en.wikipedia.org/wiki/Boyer%E2%80%93Moore_string_search_algorithm
4541 //
4542 // This describes and algorithm with 2 shift rules. The &#39;Bad Character&#39; rule
4543 // and the &#39;Good Suffix&#39; rule.
4544 //
4545 // These rules are essentially heuristics for how far we can shift the
4546 // pattern along the search string.
4547 //
4548 // The implementation here uses the &#39;Bad Character&#39; rule only because of the
4549 // complexity of initialisation for the &#39;Good Suffix&#39; rule.
4550 //
4551 // This is also known as the Boyer-Moore-Horspool algorithm:-
4552 //
4553 // http://en.wikipedia.org/wiki/Boyer-Moore-Horspool_algorithm
4554 //
4555 // This particular implementation has few java-specific optimizations.
4556 //
4557 // #define ASIZE 256
4558 //
4559 //    int bm(unsigned char *x, int m, unsigned char *y, int n) {
4560 //       int i, j;
4561 //       unsigned c;
4562 //       unsigned char bc[ASIZE];
4563 //
4564 //       /* Preprocessing */
4565 //       for (i = 0; i &lt; ASIZE; ++i)
4566 //          bc[i] = m;
4567 //       for (i = 0; i &lt; m - 1; ) {
4568 //          c = x[i];
4569 //          ++i;
4570 //          // c &lt; 256 for Latin1 string, so, no need for branch
4571 //          #ifdef PATTERN_STRING_IS_LATIN1
4572 //          bc[c] = m - i;
4573 //          #else
4574 //          if (c &lt; ASIZE) bc[c] = m - i;
4575 //          #endif
4576 //       }
4577 //
4578 //       /* Searching */
4579 //       j = 0;
4580 //       while (j &lt;= n - m) {
4581 //          c = y[i+j];
4582 //          if (x[m-1] == c)
4583 //            for (i = m - 2; i &gt;= 0 &amp;&amp; x[i] == y[i + j]; --i);
4584 //          if (i &lt; 0) return j;
4585 //          // c &lt; 256 for Latin1 string, so, no need for branch
4586 //          #ifdef SOURCE_STRING_IS_LATIN1
4587 //          // LL case: (c&lt; 256) always true. Remove branch
4588 //          j += bc[y[j+m-1]];
4589 //          #endif
4590 //          #ifndef PATTERN_STRING_IS_UTF
4591 //          // UU case: need if (c&lt;ASIZE) check. Skip 1 character if not.
4592 //          if (c &lt; ASIZE)
4593 //            j += bc[y[j+m-1]];
4594 //          else
4595 //            j += 1
4596 //          #endif
4597 //          #ifdef PATTERN_IS_LATIN1_AND_SOURCE_IS_UTF
4598 //          // UL case: need if (c&lt;ASIZE) check. Skip &lt;pattern length&gt; if not.
4599 //          if (c &lt; ASIZE)
4600 //            j += bc[y[j+m-1]];
4601 //          else
4602 //            j += m
4603 //          #endif
4604 //       }
4605 //    }
4606 
4607   if (icnt1 == -1) {
4608     Label BCLOOP, BCSKIP, BMLOOPSTR2, BMLOOPSTR1, BMSKIP, BMADV, BMMATCH,
4609         BMLOOPSTR1_LASTCMP, BMLOOPSTR1_CMP, BMLOOPSTR1_AFTER_LOAD, BM_INIT_LOOP;
4610     Register cnt1end = tmp2;
4611     Register str2end = cnt2;
4612     Register skipch = tmp2;
4613 
4614     // str1 length is &gt;=8, so, we can read at least 1 register for cases when
4615     // UTF-&gt;Latin1 conversion is not needed(8 LL or 4UU) and half register for
4616     // UL case. We&#39;ll re-read last character in inner pre-loop code to have
4617     // single outer pre-loop load
4618     const int firstStep = isL ? 7 : 3;
4619 
4620     const int ASIZE = 256;
4621     const int STORED_BYTES = 32; // amount of bytes stored per instruction
4622     sub(sp, sp, ASIZE);
4623     mov(tmp5, ASIZE/STORED_BYTES); // loop iterations
4624     mov(ch1, sp);
4625     BIND(BM_INIT_LOOP);
4626       stpq(v0, v0, Address(post(ch1, STORED_BYTES)));
4627       subs(tmp5, tmp5, 1);
4628       br(GT, BM_INIT_LOOP);
4629 
4630       sub(cnt1tmp, cnt1, 1);
4631       mov(tmp5, str2);
4632       add(str2end, str2, result_tmp, LSL, str2_chr_shift);
4633       sub(ch2, cnt1, 1);
4634       mov(tmp3, str1);
4635     BIND(BCLOOP);
4636       (this-&gt;*str1_load_1chr)(ch1, Address(post(tmp3, str1_chr_size)));
4637       if (!str1_isL) {
4638         subs(zr, ch1, ASIZE);
4639         br(HS, BCSKIP);
4640       }
4641       strb(ch2, Address(sp, ch1));
4642     BIND(BCSKIP);
4643       subs(ch2, ch2, 1);
4644       br(GT, BCLOOP);
4645 
4646       add(tmp6, str1, cnt1, LSL, str1_chr_shift); // address after str1
4647       if (str1_isL == str2_isL) {
4648         // load last 8 bytes (8LL/4UU symbols)
4649         ldr(tmp6, Address(tmp6, -wordSize));
4650       } else {
4651         ldrw(tmp6, Address(tmp6, -wordSize/2)); // load last 4 bytes(4 symbols)
4652         // convert Latin1 to UTF. We&#39;ll have to wait until load completed, but
4653         // it&#39;s still faster than per-character loads+checks
4654         lsr(tmp3, tmp6, BitsPerByte * (wordSize/2 - str1_chr_size)); // str1[N-1]
4655         ubfx(ch1, tmp6, 8, 8); // str1[N-2]
4656         ubfx(ch2, tmp6, 16, 8); // str1[N-3]
4657         andr(tmp6, tmp6, 0xFF); // str1[N-4]
4658         orr(ch2, ch1, ch2, LSL, 16);
4659         orr(tmp6, tmp6, tmp3, LSL, 48);
4660         orr(tmp6, tmp6, ch2, LSL, 16);
4661       }
4662     BIND(BMLOOPSTR2);
4663       (this-&gt;*str2_load_1chr)(skipch, Address(str2, cnt1tmp, Address::lsl(str2_chr_shift)));
4664       sub(cnt1tmp, cnt1tmp, firstStep); // cnt1tmp is positive here, because cnt1 &gt;= 8
4665       if (str1_isL == str2_isL) {
4666         // re-init tmp3. It&#39;s for free because it&#39;s executed in parallel with
4667         // load above. Alternative is to initialize it before loop, but it&#39;ll
4668         // affect performance on in-order systems with 2 or more ld/st pipelines
4669         lsr(tmp3, tmp6, BitsPerByte * (wordSize - str1_chr_size));
4670       }
4671       if (!isL) { // UU/UL case
4672         lsl(ch2, cnt1tmp, 1); // offset in bytes
4673       }
4674       cmp(tmp3, skipch);
4675       br(NE, BMSKIP);
4676       ldr(ch2, Address(str2, isL ? cnt1tmp : ch2));
4677       mov(ch1, tmp6);
4678       if (isL) {
4679         b(BMLOOPSTR1_AFTER_LOAD);
4680       } else {
4681         sub(cnt1tmp, cnt1tmp, 1); // no need to branch for UU/UL case. cnt1 &gt;= 8
4682         b(BMLOOPSTR1_CMP);
4683       }
4684     BIND(BMLOOPSTR1);
4685       (this-&gt;*str1_load_1chr)(ch1, Address(str1, cnt1tmp, Address::lsl(str1_chr_shift)));
4686       (this-&gt;*str2_load_1chr)(ch2, Address(str2, cnt1tmp, Address::lsl(str2_chr_shift)));
4687     BIND(BMLOOPSTR1_AFTER_LOAD);
4688       subs(cnt1tmp, cnt1tmp, 1);
4689       br(LT, BMLOOPSTR1_LASTCMP);
4690     BIND(BMLOOPSTR1_CMP);
4691       cmp(ch1, ch2);
4692       br(EQ, BMLOOPSTR1);
4693     BIND(BMSKIP);
4694       if (!isL) {
4695         // if we&#39;ve met UTF symbol while searching Latin1 pattern, then we can
4696         // skip cnt1 symbols
4697         if (str1_isL != str2_isL) {
4698           mov(result_tmp, cnt1);
4699         } else {
4700           mov(result_tmp, 1);
4701         }
4702         subs(zr, skipch, ASIZE);
4703         br(HS, BMADV);
4704       }
4705       ldrb(result_tmp, Address(sp, skipch)); // load skip distance
4706     BIND(BMADV);
4707       sub(cnt1tmp, cnt1, 1);
4708       add(str2, str2, result_tmp, LSL, str2_chr_shift);
4709       cmp(str2, str2end);
4710       br(LE, BMLOOPSTR2);
4711       add(sp, sp, ASIZE);
4712       b(NOMATCH);
4713     BIND(BMLOOPSTR1_LASTCMP);
4714       cmp(ch1, ch2);
4715       br(NE, BMSKIP);
4716     BIND(BMMATCH);
4717       sub(result, str2, tmp5);
4718       if (!str2_isL) lsr(result, result, 1);
4719       add(sp, sp, ASIZE);
4720       b(DONE);
4721 
4722     BIND(LINEARSTUB);
4723     cmp(cnt1, (u1)16); // small patterns still should be handled by simple algorithm
4724     br(LT, LINEAR_MEDIUM);
4725     mov(result, zr);
4726     RuntimeAddress stub = NULL;
4727     if (isL) {
4728       stub = RuntimeAddress(StubRoutines::aarch64::string_indexof_linear_ll());
4729       assert(stub.target() != NULL, &quot;string_indexof_linear_ll stub has not been generated&quot;);
4730     } else if (str1_isL) {
4731       stub = RuntimeAddress(StubRoutines::aarch64::string_indexof_linear_ul());
4732        assert(stub.target() != NULL, &quot;string_indexof_linear_ul stub has not been generated&quot;);
4733     } else {
4734       stub = RuntimeAddress(StubRoutines::aarch64::string_indexof_linear_uu());
4735       assert(stub.target() != NULL, &quot;string_indexof_linear_uu stub has not been generated&quot;);
4736     }
4737     trampoline_call(stub);
4738     b(DONE);
4739   }
4740 
4741   BIND(LINEARSEARCH);
4742   {
4743     Label DO1, DO2, DO3;
4744 
4745     Register str2tmp = tmp2;
4746     Register first = tmp3;
4747 
4748     if (icnt1 == -1)
4749     {
4750         Label DOSHORT, FIRST_LOOP, STR2_NEXT, STR1_LOOP, STR1_NEXT;
4751 
4752         cmp(cnt1, u1(str1_isL == str2_isL ? 4 : 2));
4753         br(LT, DOSHORT);
4754       BIND(LINEAR_MEDIUM);
4755         (this-&gt;*str1_load_1chr)(first, Address(str1));
4756         lea(str1, Address(str1, cnt1, Address::lsl(str1_chr_shift)));
4757         sub(cnt1_neg, zr, cnt1, LSL, str1_chr_shift);
4758         lea(str2, Address(str2, result_tmp, Address::lsl(str2_chr_shift)));
4759         sub(cnt2_neg, zr, result_tmp, LSL, str2_chr_shift);
4760 
4761       BIND(FIRST_LOOP);
4762         (this-&gt;*str2_load_1chr)(ch2, Address(str2, cnt2_neg));
4763         cmp(first, ch2);
4764         br(EQ, STR1_LOOP);
4765       BIND(STR2_NEXT);
4766         adds(cnt2_neg, cnt2_neg, str2_chr_size);
4767         br(LE, FIRST_LOOP);
4768         b(NOMATCH);
4769 
4770       BIND(STR1_LOOP);
4771         adds(cnt1tmp, cnt1_neg, str1_chr_size);
4772         add(cnt2tmp, cnt2_neg, str2_chr_size);
4773         br(GE, MATCH);
4774 
4775       BIND(STR1_NEXT);
4776         (this-&gt;*str1_load_1chr)(ch1, Address(str1, cnt1tmp));
4777         (this-&gt;*str2_load_1chr)(ch2, Address(str2, cnt2tmp));
4778         cmp(ch1, ch2);
4779         br(NE, STR2_NEXT);
4780         adds(cnt1tmp, cnt1tmp, str1_chr_size);
4781         add(cnt2tmp, cnt2tmp, str2_chr_size);
4782         br(LT, STR1_NEXT);
4783         b(MATCH);
4784 
4785       BIND(DOSHORT);
4786       if (str1_isL == str2_isL) {
4787         cmp(cnt1, (u1)2);
4788         br(LT, DO1);
4789         br(GT, DO3);
4790       }
4791     }
4792 
4793     if (icnt1 == 4) {
4794       Label CH1_LOOP;
4795 
4796         (this-&gt;*load_4chr)(ch1, str1);
4797         sub(result_tmp, cnt2, 4);
4798         lea(str2, Address(str2, result_tmp, Address::lsl(str2_chr_shift)));
4799         sub(cnt2_neg, zr, result_tmp, LSL, str2_chr_shift);
4800 
4801       BIND(CH1_LOOP);
4802         (this-&gt;*load_4chr)(ch2, Address(str2, cnt2_neg));
4803         cmp(ch1, ch2);
4804         br(EQ, MATCH);
4805         adds(cnt2_neg, cnt2_neg, str2_chr_size);
4806         br(LE, CH1_LOOP);
4807         b(NOMATCH);
4808       }
4809 
4810     if ((icnt1 == -1 &amp;&amp; str1_isL == str2_isL) || icnt1 == 2) {
4811       Label CH1_LOOP;
4812 
4813       BIND(DO2);
4814         (this-&gt;*load_2chr)(ch1, str1);
4815         if (icnt1 == 2) {
4816           sub(result_tmp, cnt2, 2);
4817         }
4818         lea(str2, Address(str2, result_tmp, Address::lsl(str2_chr_shift)));
4819         sub(cnt2_neg, zr, result_tmp, LSL, str2_chr_shift);
4820       BIND(CH1_LOOP);
4821         (this-&gt;*load_2chr)(ch2, Address(str2, cnt2_neg));
4822         cmp(ch1, ch2);
4823         br(EQ, MATCH);
4824         adds(cnt2_neg, cnt2_neg, str2_chr_size);
4825         br(LE, CH1_LOOP);
4826         b(NOMATCH);
4827     }
4828 
4829     if ((icnt1 == -1 &amp;&amp; str1_isL == str2_isL) || icnt1 == 3) {
4830       Label FIRST_LOOP, STR2_NEXT, STR1_LOOP;
4831 
4832       BIND(DO3);
4833         (this-&gt;*load_2chr)(first, str1);
4834         (this-&gt;*str1_load_1chr)(ch1, Address(str1, 2*str1_chr_size));
4835         if (icnt1 == 3) {
4836           sub(result_tmp, cnt2, 3);
4837         }
4838         lea(str2, Address(str2, result_tmp, Address::lsl(str2_chr_shift)));
4839         sub(cnt2_neg, zr, result_tmp, LSL, str2_chr_shift);
4840       BIND(FIRST_LOOP);
4841         (this-&gt;*load_2chr)(ch2, Address(str2, cnt2_neg));
4842         cmpw(first, ch2);
4843         br(EQ, STR1_LOOP);
4844       BIND(STR2_NEXT);
4845         adds(cnt2_neg, cnt2_neg, str2_chr_size);
4846         br(LE, FIRST_LOOP);
4847         b(NOMATCH);
4848 
4849       BIND(STR1_LOOP);
4850         add(cnt2tmp, cnt2_neg, 2*str2_chr_size);
4851         (this-&gt;*str2_load_1chr)(ch2, Address(str2, cnt2tmp));
4852         cmp(ch1, ch2);
4853         br(NE, STR2_NEXT);
4854         b(MATCH);
4855     }
4856 
4857     if (icnt1 == -1 || icnt1 == 1) {
4858       Label CH1_LOOP, HAS_ZERO, DO1_SHORT, DO1_LOOP;
4859 
4860       BIND(DO1);
4861         (this-&gt;*str1_load_1chr)(ch1, str1);
4862         cmp(cnt2, (u1)8);
4863         br(LT, DO1_SHORT);
4864 
4865         sub(result_tmp, cnt2, 8/str2_chr_size);
4866         sub(cnt2_neg, zr, result_tmp, LSL, str2_chr_shift);
4867         mov(tmp3, str2_isL ? 0x0101010101010101 : 0x0001000100010001);
4868         lea(str2, Address(str2, result_tmp, Address::lsl(str2_chr_shift)));
4869 
4870         if (str2_isL) {
4871           orr(ch1, ch1, ch1, LSL, 8);
4872         }
4873         orr(ch1, ch1, ch1, LSL, 16);
4874         orr(ch1, ch1, ch1, LSL, 32);
4875       BIND(CH1_LOOP);
4876         ldr(ch2, Address(str2, cnt2_neg));
4877         eor(ch2, ch1, ch2);
4878         sub(tmp1, ch2, tmp3);
4879         orr(tmp2, ch2, str2_isL ? 0x7f7f7f7f7f7f7f7f : 0x7fff7fff7fff7fff);
4880         bics(tmp1, tmp1, tmp2);
4881         br(NE, HAS_ZERO);
4882         adds(cnt2_neg, cnt2_neg, 8);
4883         br(LT, CH1_LOOP);
4884 
4885         cmp(cnt2_neg, (u1)8);
4886         mov(cnt2_neg, 0);
4887         br(LT, CH1_LOOP);
4888         b(NOMATCH);
4889 
4890       BIND(HAS_ZERO);
4891         rev(tmp1, tmp1);
4892         clz(tmp1, tmp1);
4893         add(cnt2_neg, cnt2_neg, tmp1, LSR, 3);
4894         b(MATCH);
4895 
4896       BIND(DO1_SHORT);
4897         mov(result_tmp, cnt2);
4898         lea(str2, Address(str2, cnt2, Address::lsl(str2_chr_shift)));
4899         sub(cnt2_neg, zr, cnt2, LSL, str2_chr_shift);
4900       BIND(DO1_LOOP);
4901         (this-&gt;*str2_load_1chr)(ch2, Address(str2, cnt2_neg));
4902         cmpw(ch1, ch2);
4903         br(EQ, MATCH);
4904         adds(cnt2_neg, cnt2_neg, str2_chr_size);
4905         br(LT, DO1_LOOP);
4906     }
4907   }
4908   BIND(NOMATCH);
4909     mov(result, -1);
4910     b(DONE);
4911   BIND(MATCH);
4912     add(result, result_tmp, cnt2_neg, ASR, str2_chr_shift);
4913   BIND(DONE);
4914 }
4915 
4916 typedef void (MacroAssembler::* chr_insn)(Register Rt, const Address &amp;adr);
4917 typedef void (MacroAssembler::* uxt_insn)(Register Rd, Register Rn);
4918 
4919 void MacroAssembler::string_indexof_char(Register str1, Register cnt1,
4920                                          Register ch, Register result,
4921                                          Register tmp1, Register tmp2, Register tmp3)
4922 {
4923   Label CH1_LOOP, HAS_ZERO, DO1_SHORT, DO1_LOOP, MATCH, NOMATCH, DONE;
4924   Register cnt1_neg = cnt1;
4925   Register ch1 = rscratch1;
4926   Register result_tmp = rscratch2;
4927 
4928   cmp(cnt1, (u1)4);
4929   br(LT, DO1_SHORT);
4930 
4931   orr(ch, ch, ch, LSL, 16);
4932   orr(ch, ch, ch, LSL, 32);
4933 
4934   sub(cnt1, cnt1, 4);
4935   mov(result_tmp, cnt1);
4936   lea(str1, Address(str1, cnt1, Address::uxtw(1)));
4937   sub(cnt1_neg, zr, cnt1, LSL, 1);
4938 
4939   mov(tmp3, 0x0001000100010001);
4940 
4941   BIND(CH1_LOOP);
4942     ldr(ch1, Address(str1, cnt1_neg));
4943     eor(ch1, ch, ch1);
4944     sub(tmp1, ch1, tmp3);
4945     orr(tmp2, ch1, 0x7fff7fff7fff7fff);
4946     bics(tmp1, tmp1, tmp2);
4947     br(NE, HAS_ZERO);
4948     adds(cnt1_neg, cnt1_neg, 8);
4949     br(LT, CH1_LOOP);
4950 
4951     cmp(cnt1_neg, (u1)8);
4952     mov(cnt1_neg, 0);
4953     br(LT, CH1_LOOP);
4954     b(NOMATCH);
4955 
4956   BIND(HAS_ZERO);
4957     rev(tmp1, tmp1);
4958     clz(tmp1, tmp1);
4959     add(cnt1_neg, cnt1_neg, tmp1, LSR, 3);
4960     b(MATCH);
4961 
4962   BIND(DO1_SHORT);
4963     mov(result_tmp, cnt1);
4964     lea(str1, Address(str1, cnt1, Address::uxtw(1)));
4965     sub(cnt1_neg, zr, cnt1, LSL, 1);
4966   BIND(DO1_LOOP);
4967     ldrh(ch1, Address(str1, cnt1_neg));
4968     cmpw(ch, ch1);
4969     br(EQ, MATCH);
4970     adds(cnt1_neg, cnt1_neg, 2);
4971     br(LT, DO1_LOOP);
4972   BIND(NOMATCH);
4973     mov(result, -1);
4974     b(DONE);
4975   BIND(MATCH);
4976     add(result, result_tmp, cnt1_neg, ASR, 1);
4977   BIND(DONE);
4978 }
4979 
4980 // Compare strings.
4981 void MacroAssembler::string_compare(Register str1, Register str2,
4982     Register cnt1, Register cnt2, Register result, Register tmp1, Register tmp2,
4983     FloatRegister vtmp1, FloatRegister vtmp2, FloatRegister vtmp3, int ae) {
4984   Label DONE, SHORT_LOOP, SHORT_STRING, SHORT_LAST, TAIL, STUB,
4985       DIFFERENCE, NEXT_WORD, SHORT_LOOP_TAIL, SHORT_LAST2, SHORT_LAST_INIT,
4986       SHORT_LOOP_START, TAIL_CHECK;
4987 
4988   bool isLL = ae == StrIntrinsicNode::LL;
4989   bool isLU = ae == StrIntrinsicNode::LU;
4990   bool isUL = ae == StrIntrinsicNode::UL;
4991 
4992   // The stub threshold for LL strings is: 72 (64 + 8) chars
4993   // UU: 36 chars, or 72 bytes (valid for the 64-byte large loop with prefetch)
4994   // LU/UL: 24 chars, or 48 bytes (valid for the 16-character loop at least)
4995   const u1 stub_threshold = isLL ? 72 : ((isLU || isUL) ? 24 : 36);
4996 
4997   bool str1_isL = isLL || isLU;
4998   bool str2_isL = isLL || isUL;
4999 
5000   int str1_chr_shift = str1_isL ? 0 : 1;
5001   int str2_chr_shift = str2_isL ? 0 : 1;
5002   int str1_chr_size = str1_isL ? 1 : 2;
5003   int str2_chr_size = str2_isL ? 1 : 2;
5004   int minCharsInWord = isLL ? wordSize : wordSize/2;
5005 
5006   FloatRegister vtmpZ = vtmp1, vtmp = vtmp2;
5007   chr_insn str1_load_chr = str1_isL ? (chr_insn)&amp;MacroAssembler::ldrb :
5008                                       (chr_insn)&amp;MacroAssembler::ldrh;
5009   chr_insn str2_load_chr = str2_isL ? (chr_insn)&amp;MacroAssembler::ldrb :
5010                                       (chr_insn)&amp;MacroAssembler::ldrh;
5011   uxt_insn ext_chr = isLL ? (uxt_insn)&amp;MacroAssembler::uxtbw :
5012                             (uxt_insn)&amp;MacroAssembler::uxthw;
5013 
5014   BLOCK_COMMENT(&quot;string_compare {&quot;);
5015 
5016   // Bizzarely, the counts are passed in bytes, regardless of whether they
5017   // are L or U strings, however the result is always in characters.
5018   if (!str1_isL) asrw(cnt1, cnt1, 1);
5019   if (!str2_isL) asrw(cnt2, cnt2, 1);
5020 
5021   // Compute the minimum of the string lengths and save the difference.
5022   subsw(result, cnt1, cnt2);
5023   cselw(cnt2, cnt1, cnt2, Assembler::LE); // min
5024 
5025   // A very short string
5026   cmpw(cnt2, minCharsInWord);
5027   br(Assembler::LE, SHORT_STRING);
5028 
5029   // Compare longwords
5030   // load first parts of strings and finish initialization while loading
5031   {
5032     if (str1_isL == str2_isL) { // LL or UU
5033       ldr(tmp1, Address(str1));
5034       cmp(str1, str2);
5035       br(Assembler::EQ, DONE);
5036       ldr(tmp2, Address(str2));
5037       cmp(cnt2, stub_threshold);
5038       br(GE, STUB);
5039       subsw(cnt2, cnt2, minCharsInWord);
5040       br(EQ, TAIL_CHECK);
5041       lea(str2, Address(str2, cnt2, Address::uxtw(str2_chr_shift)));
5042       lea(str1, Address(str1, cnt2, Address::uxtw(str1_chr_shift)));
5043       sub(cnt2, zr, cnt2, LSL, str2_chr_shift);
5044     } else if (isLU) {
5045       ldrs(vtmp, Address(str1));
5046       ldr(tmp2, Address(str2));
5047       cmp(cnt2, stub_threshold);
5048       br(GE, STUB);
5049       subw(cnt2, cnt2, 4);
5050       eor(vtmpZ, T16B, vtmpZ, vtmpZ);
5051       lea(str1, Address(str1, cnt2, Address::uxtw(str1_chr_shift)));
5052       lea(str2, Address(str2, cnt2, Address::uxtw(str2_chr_shift)));
5053       zip1(vtmp, T8B, vtmp, vtmpZ);
5054       sub(cnt1, zr, cnt2, LSL, str1_chr_shift);
5055       sub(cnt2, zr, cnt2, LSL, str2_chr_shift);
5056       add(cnt1, cnt1, 4);
5057       fmovd(tmp1, vtmp);
5058     } else { // UL case
5059       ldr(tmp1, Address(str1));
5060       ldrs(vtmp, Address(str2));
5061       cmp(cnt2, stub_threshold);
5062       br(GE, STUB);
5063       subw(cnt2, cnt2, 4);
5064       lea(str1, Address(str1, cnt2, Address::uxtw(str1_chr_shift)));
5065       eor(vtmpZ, T16B, vtmpZ, vtmpZ);
5066       lea(str2, Address(str2, cnt2, Address::uxtw(str2_chr_shift)));
5067       sub(cnt1, zr, cnt2, LSL, str1_chr_shift);
5068       zip1(vtmp, T8B, vtmp, vtmpZ);
5069       sub(cnt2, zr, cnt2, LSL, str2_chr_shift);
5070       add(cnt1, cnt1, 8);
5071       fmovd(tmp2, vtmp);
5072     }
5073     adds(cnt2, cnt2, isUL ? 4 : 8);
5074     br(GE, TAIL);
5075     eor(rscratch2, tmp1, tmp2);
5076     cbnz(rscratch2, DIFFERENCE);
5077     // main loop
5078     bind(NEXT_WORD);
5079     if (str1_isL == str2_isL) {
5080       ldr(tmp1, Address(str1, cnt2));
5081       ldr(tmp2, Address(str2, cnt2));
5082       adds(cnt2, cnt2, 8);
5083     } else if (isLU) {
5084       ldrs(vtmp, Address(str1, cnt1));
5085       ldr(tmp2, Address(str2, cnt2));
5086       add(cnt1, cnt1, 4);
5087       zip1(vtmp, T8B, vtmp, vtmpZ);
5088       fmovd(tmp1, vtmp);
5089       adds(cnt2, cnt2, 8);
5090     } else { // UL
5091       ldrs(vtmp, Address(str2, cnt2));
5092       ldr(tmp1, Address(str1, cnt1));
5093       zip1(vtmp, T8B, vtmp, vtmpZ);
5094       add(cnt1, cnt1, 8);
5095       fmovd(tmp2, vtmp);
5096       adds(cnt2, cnt2, 4);
5097     }
5098     br(GE, TAIL);
5099 
5100     eor(rscratch2, tmp1, tmp2);
5101     cbz(rscratch2, NEXT_WORD);
5102     b(DIFFERENCE);
5103     bind(TAIL);
5104     eor(rscratch2, tmp1, tmp2);
5105     cbnz(rscratch2, DIFFERENCE);
5106     // Last longword.  In the case where length == 4 we compare the
5107     // same longword twice, but that&#39;s still faster than another
5108     // conditional branch.
5109     if (str1_isL == str2_isL) {
5110       ldr(tmp1, Address(str1));
5111       ldr(tmp2, Address(str2));
5112     } else if (isLU) {
5113       ldrs(vtmp, Address(str1));
5114       ldr(tmp2, Address(str2));
5115       zip1(vtmp, T8B, vtmp, vtmpZ);
5116       fmovd(tmp1, vtmp);
5117     } else { // UL
5118       ldrs(vtmp, Address(str2));
5119       ldr(tmp1, Address(str1));
5120       zip1(vtmp, T8B, vtmp, vtmpZ);
5121       fmovd(tmp2, vtmp);
5122     }
5123     bind(TAIL_CHECK);
5124     eor(rscratch2, tmp1, tmp2);
5125     cbz(rscratch2, DONE);
5126 
5127     // Find the first different characters in the longwords and
5128     // compute their difference.
5129     bind(DIFFERENCE);
5130     rev(rscratch2, rscratch2);
5131     clz(rscratch2, rscratch2);
5132     andr(rscratch2, rscratch2, isLL ? -8 : -16);
5133     lsrv(tmp1, tmp1, rscratch2);
5134     (this-&gt;*ext_chr)(tmp1, tmp1);
5135     lsrv(tmp2, tmp2, rscratch2);
5136     (this-&gt;*ext_chr)(tmp2, tmp2);
5137     subw(result, tmp1, tmp2);
5138     b(DONE);
5139   }
5140 
5141   bind(STUB);
5142     RuntimeAddress stub = NULL;
5143     switch(ae) {
5144       case StrIntrinsicNode::LL:
5145         stub = RuntimeAddress(StubRoutines::aarch64::compare_long_string_LL());
5146         break;
5147       case StrIntrinsicNode::UU:
5148         stub = RuntimeAddress(StubRoutines::aarch64::compare_long_string_UU());
5149         break;
5150       case StrIntrinsicNode::LU:
5151         stub = RuntimeAddress(StubRoutines::aarch64::compare_long_string_LU());
5152         break;
5153       case StrIntrinsicNode::UL:
5154         stub = RuntimeAddress(StubRoutines::aarch64::compare_long_string_UL());
5155         break;
5156       default:
5157         ShouldNotReachHere();
5158      }
5159     assert(stub.target() != NULL, &quot;compare_long_string stub has not been generated&quot;);
5160     trampoline_call(stub);
5161     b(DONE);
5162 
5163   bind(SHORT_STRING);
5164   // Is the minimum length zero?
5165   cbz(cnt2, DONE);
5166   // arrange code to do most branches while loading and loading next characters
5167   // while comparing previous
5168   (this-&gt;*str1_load_chr)(tmp1, Address(post(str1, str1_chr_size)));
5169   subs(cnt2, cnt2, 1);
5170   br(EQ, SHORT_LAST_INIT);
5171   (this-&gt;*str2_load_chr)(cnt1, Address(post(str2, str2_chr_size)));
5172   b(SHORT_LOOP_START);
5173   bind(SHORT_LOOP);
5174   subs(cnt2, cnt2, 1);
5175   br(EQ, SHORT_LAST);
5176   bind(SHORT_LOOP_START);
5177   (this-&gt;*str1_load_chr)(tmp2, Address(post(str1, str1_chr_size)));
5178   (this-&gt;*str2_load_chr)(rscratch1, Address(post(str2, str2_chr_size)));
5179   cmp(tmp1, cnt1);
5180   br(NE, SHORT_LOOP_TAIL);
5181   subs(cnt2, cnt2, 1);
5182   br(EQ, SHORT_LAST2);
5183   (this-&gt;*str1_load_chr)(tmp1, Address(post(str1, str1_chr_size)));
5184   (this-&gt;*str2_load_chr)(cnt1, Address(post(str2, str2_chr_size)));
5185   cmp(tmp2, rscratch1);
5186   br(EQ, SHORT_LOOP);
5187   sub(result, tmp2, rscratch1);
5188   b(DONE);
5189   bind(SHORT_LOOP_TAIL);
5190   sub(result, tmp1, cnt1);
5191   b(DONE);
5192   bind(SHORT_LAST2);
5193   cmp(tmp2, rscratch1);
5194   br(EQ, DONE);
5195   sub(result, tmp2, rscratch1);
5196 
5197   b(DONE);
5198   bind(SHORT_LAST_INIT);
5199   (this-&gt;*str2_load_chr)(cnt1, Address(post(str2, str2_chr_size)));
5200   bind(SHORT_LAST);
5201   cmp(tmp1, cnt1);
5202   br(EQ, DONE);
5203   sub(result, tmp1, cnt1);
5204 
5205   bind(DONE);
5206 
5207   BLOCK_COMMENT(&quot;} string_compare&quot;);
5208 }
5209 #endif // COMPILER2
5210 
5211 // This method checks if provided byte array contains byte with highest bit set.
5212 void MacroAssembler::has_negatives(Register ary1, Register len, Register result) {
5213     // Simple and most common case of aligned small array which is not at the
5214     // end of memory page is placed here. All other cases are in stub.
5215     Label LOOP, END, STUB, STUB_LONG, SET_RESULT, DONE;
5216     const uint64_t UPPER_BIT_MASK=0x8080808080808080;
5217     assert_different_registers(ary1, len, result);
5218 
5219     cmpw(len, 0);
5220     br(LE, SET_RESULT);
5221     cmpw(len, 4 * wordSize);
5222     br(GE, STUB_LONG); // size &gt; 32 then go to stub
5223 
5224     int shift = 64 - exact_log2(os::vm_page_size());
5225     lsl(rscratch1, ary1, shift);
5226     mov(rscratch2, (size_t)(4 * wordSize) &lt;&lt; shift);
5227     adds(rscratch2, rscratch1, rscratch2);  // At end of page?
5228     br(CS, STUB); // at the end of page then go to stub
5229     subs(len, len, wordSize);
5230     br(LT, END);
5231 
5232   BIND(LOOP);
5233     ldr(rscratch1, Address(post(ary1, wordSize)));
5234     tst(rscratch1, UPPER_BIT_MASK);
5235     br(NE, SET_RESULT);
5236     subs(len, len, wordSize);
5237     br(GE, LOOP);
5238     cmpw(len, -wordSize);
5239     br(EQ, SET_RESULT);
5240 
5241   BIND(END);
5242     ldr(result, Address(ary1));
5243     sub(len, zr, len, LSL, 3); // LSL 3 is to get bits from bytes
5244     lslv(result, result, len);
5245     tst(result, UPPER_BIT_MASK);
5246     b(SET_RESULT);
5247 
5248   BIND(STUB);
5249     RuntimeAddress has_neg =  RuntimeAddress(StubRoutines::aarch64::has_negatives());
5250     assert(has_neg.target() != NULL, &quot;has_negatives stub has not been generated&quot;);
5251     trampoline_call(has_neg);
5252     b(DONE);
5253 
5254   BIND(STUB_LONG);
5255     RuntimeAddress has_neg_long =  RuntimeAddress(
5256             StubRoutines::aarch64::has_negatives_long());
5257     assert(has_neg_long.target() != NULL, &quot;has_negatives stub has not been generated&quot;);
5258     trampoline_call(has_neg_long);
5259     b(DONE);
5260 
5261   BIND(SET_RESULT);
5262     cset(result, NE); // set true or false
5263 
5264   BIND(DONE);
5265 }
5266 
5267 void MacroAssembler::arrays_equals(Register a1, Register a2, Register tmp3,
5268                                    Register tmp4, Register tmp5, Register result,
5269                                    Register cnt1, int elem_size) {
5270   Label DONE, SAME;
5271   Register tmp1 = rscratch1;
5272   Register tmp2 = rscratch2;
5273   Register cnt2 = tmp2;  // cnt2 only used in array length compare
5274   int elem_per_word = wordSize/elem_size;
5275   int log_elem_size = exact_log2(elem_size);
5276   int length_offset = arrayOopDesc::length_offset_in_bytes();
5277   int base_offset
5278     = arrayOopDesc::base_offset_in_bytes(elem_size == 2 ? T_CHAR : T_BYTE);
5279   int stubBytesThreshold = 3 * 64 + (UseSIMDForArrayEquals ? 0 : 16);
5280 
5281   assert(elem_size == 1 || elem_size == 2, &quot;must be char or byte&quot;);
5282   assert_different_registers(a1, a2, result, cnt1, rscratch1, rscratch2);
5283 
5284 #ifndef PRODUCT
5285   {
5286     const char kind = (elem_size == 2) ? &#39;U&#39; : &#39;L&#39;;
5287     char comment[64];
5288     snprintf(comment, sizeof comment, &quot;array_equals%c{&quot;, kind);
5289     BLOCK_COMMENT(comment);
5290   }
5291 #endif
5292 
5293   // if (a1 == a2)
5294   //     return true;
5295   cmpoop(a1, a2); // May have read barriers for a1 and a2.
5296   br(EQ, SAME);
5297 
5298   if (UseSimpleArrayEquals) {
5299     Label NEXT_WORD, SHORT, TAIL03, TAIL01, A_MIGHT_BE_NULL, A_IS_NOT_NULL;
5300     // if (a1 == null || a2 == null)
5301     //     return false;
5302     // a1 &amp; a2 == 0 means (some-pointer is null) or
5303     // (very-rare-or-even-probably-impossible-pointer-values)
5304     // so, we can save one branch in most cases
5305     tst(a1, a2);
5306     mov(result, false);
5307     br(EQ, A_MIGHT_BE_NULL);
5308     // if (a1.length != a2.length)
5309     //      return false;
5310     bind(A_IS_NOT_NULL);
5311     ldrw(cnt1, Address(a1, length_offset));
5312     ldrw(cnt2, Address(a2, length_offset));
5313     eorw(tmp5, cnt1, cnt2);
5314     cbnzw(tmp5, DONE);
5315     lea(a1, Address(a1, base_offset));
5316     lea(a2, Address(a2, base_offset));
5317     // Check for short strings, i.e. smaller than wordSize.
5318     subs(cnt1, cnt1, elem_per_word);
5319     br(Assembler::LT, SHORT);
5320     // Main 8 byte comparison loop.
5321     bind(NEXT_WORD); {
5322       ldr(tmp1, Address(post(a1, wordSize)));
5323       ldr(tmp2, Address(post(a2, wordSize)));
5324       subs(cnt1, cnt1, elem_per_word);
5325       eor(tmp5, tmp1, tmp2);
5326       cbnz(tmp5, DONE);
5327     } br(GT, NEXT_WORD);
5328     // Last longword.  In the case where length == 4 we compare the
5329     // same longword twice, but that&#39;s still faster than another
5330     // conditional branch.
5331     // cnt1 could be 0, -1, -2, -3, -4 for chars; -4 only happens when
5332     // length == 4.
5333     if (log_elem_size &gt; 0)
5334       lsl(cnt1, cnt1, log_elem_size);
5335     ldr(tmp3, Address(a1, cnt1));
5336     ldr(tmp4, Address(a2, cnt1));
5337     eor(tmp5, tmp3, tmp4);
5338     cbnz(tmp5, DONE);
5339     b(SAME);
5340     bind(A_MIGHT_BE_NULL);
5341     // in case both a1 and a2 are not-null, proceed with loads
5342     cbz(a1, DONE);
5343     cbz(a2, DONE);
5344     b(A_IS_NOT_NULL);
5345     bind(SHORT);
5346 
5347     tbz(cnt1, 2 - log_elem_size, TAIL03); // 0-7 bytes left.
5348     {
5349       ldrw(tmp1, Address(post(a1, 4)));
5350       ldrw(tmp2, Address(post(a2, 4)));
5351       eorw(tmp5, tmp1, tmp2);
5352       cbnzw(tmp5, DONE);
5353     }
5354     bind(TAIL03);
5355     tbz(cnt1, 1 - log_elem_size, TAIL01); // 0-3 bytes left.
5356     {
5357       ldrh(tmp3, Address(post(a1, 2)));
5358       ldrh(tmp4, Address(post(a2, 2)));
5359       eorw(tmp5, tmp3, tmp4);
5360       cbnzw(tmp5, DONE);
5361     }
5362     bind(TAIL01);
5363     if (elem_size == 1) { // Only needed when comparing byte arrays.
5364       tbz(cnt1, 0, SAME); // 0-1 bytes left.
5365       {
5366         ldrb(tmp1, a1);
5367         ldrb(tmp2, a2);
5368         eorw(tmp5, tmp1, tmp2);
5369         cbnzw(tmp5, DONE);
5370       }
5371     }
5372   } else {
5373     Label NEXT_DWORD, SHORT, TAIL, TAIL2, STUB, EARLY_OUT,
5374         CSET_EQ, LAST_CHECK;
5375     mov(result, false);
5376     cbz(a1, DONE);
5377     ldrw(cnt1, Address(a1, length_offset));
5378     cbz(a2, DONE);
5379     ldrw(cnt2, Address(a2, length_offset));
5380     // on most CPUs a2 is still &quot;locked&quot;(surprisingly) in ldrw and it&#39;s
5381     // faster to perform another branch before comparing a1 and a2
5382     cmp(cnt1, (u1)elem_per_word);
5383     br(LE, SHORT); // short or same
5384     ldr(tmp3, Address(pre(a1, base_offset)));
5385     subs(zr, cnt1, stubBytesThreshold);
5386     br(GE, STUB);
5387     ldr(tmp4, Address(pre(a2, base_offset)));
5388     sub(tmp5, zr, cnt1, LSL, 3 + log_elem_size);
5389     cmp(cnt2, cnt1);
5390     br(NE, DONE);
5391 
5392     // Main 16 byte comparison loop with 2 exits
5393     bind(NEXT_DWORD); {
5394       ldr(tmp1, Address(pre(a1, wordSize)));
5395       ldr(tmp2, Address(pre(a2, wordSize)));
5396       subs(cnt1, cnt1, 2 * elem_per_word);
5397       br(LE, TAIL);
5398       eor(tmp4, tmp3, tmp4);
5399       cbnz(tmp4, DONE);
5400       ldr(tmp3, Address(pre(a1, wordSize)));
5401       ldr(tmp4, Address(pre(a2, wordSize)));
5402       cmp(cnt1, (u1)elem_per_word);
5403       br(LE, TAIL2);
5404       cmp(tmp1, tmp2);
5405     } br(EQ, NEXT_DWORD);
5406     b(DONE);
5407 
5408     bind(TAIL);
5409     eor(tmp4, tmp3, tmp4);
5410     eor(tmp2, tmp1, tmp2);
5411     lslv(tmp2, tmp2, tmp5);
5412     orr(tmp5, tmp4, tmp2);
5413     cmp(tmp5, zr);
5414     b(CSET_EQ);
5415 
5416     bind(TAIL2);
5417     eor(tmp2, tmp1, tmp2);
5418     cbnz(tmp2, DONE);
5419     b(LAST_CHECK);
5420 
5421     bind(STUB);
5422     ldr(tmp4, Address(pre(a2, base_offset)));
5423     cmp(cnt2, cnt1);
5424     br(NE, DONE);
5425     if (elem_size == 2) { // convert to byte counter
5426       lsl(cnt1, cnt1, 1);
5427     }
5428     eor(tmp5, tmp3, tmp4);
5429     cbnz(tmp5, DONE);
5430     RuntimeAddress stub = RuntimeAddress(StubRoutines::aarch64::large_array_equals());
5431     assert(stub.target() != NULL, &quot;array_equals_long stub has not been generated&quot;);
5432     trampoline_call(stub);
5433     b(DONE);
5434 
5435     bind(EARLY_OUT);
5436     // (a1 != null &amp;&amp; a2 == null) || (a1 != null &amp;&amp; a2 != null &amp;&amp; a1 == a2)
5437     // so, if a2 == null =&gt; return false(0), else return true, so we can return a2
5438     mov(result, a2);
5439     b(DONE);
5440     bind(SHORT);
5441     cmp(cnt2, cnt1);
5442     br(NE, DONE);
5443     cbz(cnt1, SAME);
5444     sub(tmp5, zr, cnt1, LSL, 3 + log_elem_size);
5445     ldr(tmp3, Address(a1, base_offset));
5446     ldr(tmp4, Address(a2, base_offset));
5447     bind(LAST_CHECK);
5448     eor(tmp4, tmp3, tmp4);
5449     lslv(tmp5, tmp4, tmp5);
5450     cmp(tmp5, zr);
5451     bind(CSET_EQ);
5452     cset(result, EQ);
5453     b(DONE);
5454   }
5455 
5456   bind(SAME);
5457   mov(result, true);
5458   // That&#39;s it.
5459   bind(DONE);
5460 
5461   BLOCK_COMMENT(&quot;} array_equals&quot;);
5462 }
5463 
5464 // Compare Strings
5465 
5466 // For Strings we&#39;re passed the address of the first characters in a1
5467 // and a2 and the length in cnt1.
5468 // elem_size is the element size in bytes: either 1 or 2.
5469 // There are two implementations.  For arrays &gt;= 8 bytes, all
5470 // comparisons (including the final one, which may overlap) are
5471 // performed 8 bytes at a time.  For strings &lt; 8 bytes, we compare a
5472 // halfword, then a short, and then a byte.
5473 
5474 void MacroAssembler::string_equals(Register a1, Register a2,
5475                                    Register result, Register cnt1, int elem_size)
5476 {
5477   Label SAME, DONE, SHORT, NEXT_WORD;
5478   Register tmp1 = rscratch1;
5479   Register tmp2 = rscratch2;
5480   Register cnt2 = tmp2;  // cnt2 only used in array length compare
5481 
5482   assert(elem_size == 1 || elem_size == 2, &quot;must be 2 or 1 byte&quot;);
5483   assert_different_registers(a1, a2, result, cnt1, rscratch1, rscratch2);
5484 
5485 #ifndef PRODUCT
5486   {
5487     const char kind = (elem_size == 2) ? &#39;U&#39; : &#39;L&#39;;
5488     char comment[64];
5489     snprintf(comment, sizeof comment, &quot;{string_equals%c&quot;, kind);
5490     BLOCK_COMMENT(comment);
5491   }
5492 #endif
5493 
5494   mov(result, false);
5495 
5496   // Check for short strings, i.e. smaller than wordSize.
5497   subs(cnt1, cnt1, wordSize);
5498   br(Assembler::LT, SHORT);
5499   // Main 8 byte comparison loop.
5500   bind(NEXT_WORD); {
5501     ldr(tmp1, Address(post(a1, wordSize)));
5502     ldr(tmp2, Address(post(a2, wordSize)));
5503     subs(cnt1, cnt1, wordSize);
5504     eor(tmp1, tmp1, tmp2);
5505     cbnz(tmp1, DONE);
5506   } br(GT, NEXT_WORD);
5507   // Last longword.  In the case where length == 4 we compare the
5508   // same longword twice, but that&#39;s still faster than another
5509   // conditional branch.
5510   // cnt1 could be 0, -1, -2, -3, -4 for chars; -4 only happens when
5511   // length == 4.
5512   ldr(tmp1, Address(a1, cnt1));
5513   ldr(tmp2, Address(a2, cnt1));
5514   eor(tmp2, tmp1, tmp2);
5515   cbnz(tmp2, DONE);
5516   b(SAME);
5517 
5518   bind(SHORT);
5519   Label TAIL03, TAIL01;
5520 
5521   tbz(cnt1, 2, TAIL03); // 0-7 bytes left.
5522   {
5523     ldrw(tmp1, Address(post(a1, 4)));
5524     ldrw(tmp2, Address(post(a2, 4)));
5525     eorw(tmp1, tmp1, tmp2);
5526     cbnzw(tmp1, DONE);
5527   }
5528   bind(TAIL03);
5529   tbz(cnt1, 1, TAIL01); // 0-3 bytes left.
5530   {
5531     ldrh(tmp1, Address(post(a1, 2)));
5532     ldrh(tmp2, Address(post(a2, 2)));
5533     eorw(tmp1, tmp1, tmp2);
5534     cbnzw(tmp1, DONE);
5535   }
5536   bind(TAIL01);
5537   if (elem_size == 1) { // Only needed when comparing 1-byte elements
5538     tbz(cnt1, 0, SAME); // 0-1 bytes left.
5539     {
5540       ldrb(tmp1, a1);
5541       ldrb(tmp2, a2);
5542       eorw(tmp1, tmp1, tmp2);
5543       cbnzw(tmp1, DONE);
5544     }
5545   }
5546   // Arrays are equal.
5547   bind(SAME);
5548   mov(result, true);
5549 
5550   // That&#39;s it.
5551   bind(DONE);
5552   BLOCK_COMMENT(&quot;} string_equals&quot;);
5553 }
5554 
5555 
5556 // The size of the blocks erased by the zero_blocks stub.  We must
5557 // handle anything smaller than this ourselves in zero_words().
5558 const int MacroAssembler::zero_words_block_size = 8;
5559 
5560 // zero_words() is used by C2 ClearArray patterns.  It is as small as
5561 // possible, handling small word counts locally and delegating
5562 // anything larger to the zero_blocks stub.  It is expanded many times
5563 // in compiled code, so it is important to keep it short.
5564 
5565 // ptr:   Address of a buffer to be zeroed.
5566 // cnt:   Count in HeapWords.
5567 //
5568 // ptr, cnt, rscratch1, and rscratch2 are clobbered.
5569 void MacroAssembler::zero_words(Register ptr, Register cnt)
5570 {
5571   assert(is_power_of_2(zero_words_block_size), &quot;adjust this&quot;);
5572   assert(ptr == r10 &amp;&amp; cnt == r11, &quot;mismatch in register usage&quot;);
5573 
5574   BLOCK_COMMENT(&quot;zero_words {&quot;);
5575   cmp(cnt, (u1)zero_words_block_size);
5576   Label around;
5577   br(LO, around);
5578   {
5579     RuntimeAddress zero_blocks =  RuntimeAddress(StubRoutines::aarch64::zero_blocks());
5580     assert(zero_blocks.target() != NULL, &quot;zero_blocks stub has not been generated&quot;);
5581     if (StubRoutines::aarch64::complete()) {
5582       trampoline_call(zero_blocks);
5583     } else {
5584       bl(zero_blocks);
5585     }
5586   }
5587   bind(around);
5588   for (int i = zero_words_block_size &gt;&gt; 1; i &gt; 1; i &gt;&gt;= 1) {
5589     Label l;
5590     tbz(cnt, exact_log2(i), l);
5591     for (int j = 0; j &lt; i; j += 2) {
5592       stp(zr, zr, post(ptr, 16));
5593     }
5594     bind(l);
5595   }
5596   {
5597     Label l;
5598     tbz(cnt, 0, l);
5599     str(zr, Address(ptr));
5600     bind(l);
5601   }
5602   BLOCK_COMMENT(&quot;} zero_words&quot;);
5603 }
5604 
5605 // base:         Address of a buffer to be zeroed, 8 bytes aligned.
5606 // cnt:          Immediate count in HeapWords.
5607 #define SmallArraySize (18 * BytesPerLong)
5608 void MacroAssembler::zero_words(Register base, u_int64_t cnt)
5609 {
5610   BLOCK_COMMENT(&quot;zero_words {&quot;);
5611   int i = cnt &amp; 1;  // store any odd word to start
5612   if (i) str(zr, Address(base));
5613 
5614   if (cnt &lt;= SmallArraySize / BytesPerLong) {
5615     for (; i &lt; (int)cnt; i += 2)
5616       stp(zr, zr, Address(base, i * wordSize));
5617   } else {
5618     const int unroll = 4; // Number of stp(zr, zr) instructions we&#39;ll unroll
5619     int remainder = cnt % (2 * unroll);
5620     for (; i &lt; remainder; i += 2)
5621       stp(zr, zr, Address(base, i * wordSize));
5622 
5623     Label loop;
5624     Register cnt_reg = rscratch1;
5625     Register loop_base = rscratch2;
5626     cnt = cnt - remainder;
5627     mov(cnt_reg, cnt);
5628     // adjust base and prebias by -2 * wordSize so we can pre-increment
5629     add(loop_base, base, (remainder - 2) * wordSize);
5630     bind(loop);
5631     sub(cnt_reg, cnt_reg, 2 * unroll);
5632     for (i = 1; i &lt; unroll; i++)
5633       stp(zr, zr, Address(loop_base, 2 * i * wordSize));
5634     stp(zr, zr, Address(pre(loop_base, 2 * unroll * wordSize)));
5635     cbnz(cnt_reg, loop);
5636   }
5637   BLOCK_COMMENT(&quot;} zero_words&quot;);
5638 }
5639 
5640 // Zero blocks of memory by using DC ZVA.
5641 //
5642 // Aligns the base address first sufficently for DC ZVA, then uses
5643 // DC ZVA repeatedly for every full block.  cnt is the size to be
5644 // zeroed in HeapWords.  Returns the count of words left to be zeroed
5645 // in cnt.
5646 //
5647 // NOTE: This is intended to be used in the zero_blocks() stub.  If
5648 // you want to use it elsewhere, note that cnt must be &gt;= 2*zva_length.
5649 void MacroAssembler::zero_dcache_blocks(Register base, Register cnt) {
5650   Register tmp = rscratch1;
5651   Register tmp2 = rscratch2;
5652   int zva_length = VM_Version::zva_length();
5653   Label initial_table_end, loop_zva;
5654   Label fini;
5655 
5656   // Base must be 16 byte aligned. If not just return and let caller handle it
5657   tst(base, 0x0f);
5658   br(Assembler::NE, fini);
5659   // Align base with ZVA length.
5660   neg(tmp, base);
5661   andr(tmp, tmp, zva_length - 1);
5662 
5663   // tmp: the number of bytes to be filled to align the base with ZVA length.
5664   add(base, base, tmp);
5665   sub(cnt, cnt, tmp, Assembler::ASR, 3);
5666   adr(tmp2, initial_table_end);
5667   sub(tmp2, tmp2, tmp, Assembler::LSR, 2);
5668   br(tmp2);
5669 
5670   for (int i = -zva_length + 16; i &lt; 0; i += 16)
5671     stp(zr, zr, Address(base, i));
5672   bind(initial_table_end);
5673 
5674   sub(cnt, cnt, zva_length &gt;&gt; 3);
5675   bind(loop_zva);
5676   dc(Assembler::ZVA, base);
5677   subs(cnt, cnt, zva_length &gt;&gt; 3);
5678   add(base, base, zva_length);
5679   br(Assembler::GE, loop_zva);
5680   add(cnt, cnt, zva_length &gt;&gt; 3); // count not zeroed by DC ZVA
5681   bind(fini);
5682 }
5683 
5684 // base:   Address of a buffer to be filled, 8 bytes aligned.
5685 // cnt:    Count in 8-byte unit.
5686 // value:  Value to be filled with.
5687 // base will point to the end of the buffer after filling.
5688 void MacroAssembler::fill_words(Register base, Register cnt, Register value)
5689 {
5690 //  Algorithm:
5691 //
5692 //    scratch1 = cnt &amp; 7;
5693 //    cnt -= scratch1;
5694 //    p += scratch1;
5695 //    switch (scratch1) {
5696 //      do {
5697 //        cnt -= 8;
5698 //          p[-8] = v;
5699 //        case 7:
5700 //          p[-7] = v;
5701 //        case 6:
5702 //          p[-6] = v;
5703 //          // ...
5704 //        case 1:
5705 //          p[-1] = v;
5706 //        case 0:
5707 //          p += 8;
5708 //      } while (cnt);
5709 //    }
5710 
5711   assert_different_registers(base, cnt, value, rscratch1, rscratch2);
5712 
5713   Label fini, skip, entry, loop;
5714   const int unroll = 8; // Number of stp instructions we&#39;ll unroll
5715 
5716   cbz(cnt, fini);
5717   tbz(base, 3, skip);
5718   str(value, Address(post(base, 8)));
5719   sub(cnt, cnt, 1);
5720   bind(skip);
5721 
5722   andr(rscratch1, cnt, (unroll-1) * 2);
5723   sub(cnt, cnt, rscratch1);
5724   add(base, base, rscratch1, Assembler::LSL, 3);
5725   adr(rscratch2, entry);
5726   sub(rscratch2, rscratch2, rscratch1, Assembler::LSL, 1);
5727   br(rscratch2);
5728 
5729   bind(loop);
5730   add(base, base, unroll * 16);
5731   for (int i = -unroll; i &lt; 0; i++)
5732     stp(value, value, Address(base, i * 16));
5733   bind(entry);
5734   subs(cnt, cnt, unroll * 2);
5735   br(Assembler::GE, loop);
5736 
5737   tbz(cnt, 0, fini);
5738   str(value, Address(post(base, 8)));
5739   bind(fini);
5740 }
5741 
5742 // Intrinsic for sun/nio/cs/ISO_8859_1$Encoder.implEncodeISOArray and
5743 // java/lang/StringUTF16.compress.
5744 void MacroAssembler::encode_iso_array(Register src, Register dst,
5745                       Register len, Register result,
5746                       FloatRegister Vtmp1, FloatRegister Vtmp2,
5747                       FloatRegister Vtmp3, FloatRegister Vtmp4)
5748 {
5749     Label DONE, SET_RESULT, NEXT_32, NEXT_32_PRFM, LOOP_8, NEXT_8, LOOP_1, NEXT_1,
5750         NEXT_32_START, NEXT_32_PRFM_START;
5751     Register tmp1 = rscratch1, tmp2 = rscratch2;
5752 
5753       mov(result, len); // Save initial len
5754 
5755       cmp(len, (u1)8); // handle shortest strings first
5756       br(LT, LOOP_1);
5757       cmp(len, (u1)32);
5758       br(LT, NEXT_8);
5759       // The following code uses the SIMD &#39;uzp1&#39; and &#39;uzp2&#39; instructions
5760       // to convert chars to bytes
5761       if (SoftwarePrefetchHintDistance &gt;= 0) {
5762         ld1(Vtmp1, Vtmp2, Vtmp3, Vtmp4, T8H, src);
5763         subs(tmp2, len, SoftwarePrefetchHintDistance/2 + 16);
5764         br(LE, NEXT_32_START);
5765         b(NEXT_32_PRFM_START);
5766         BIND(NEXT_32_PRFM);
5767           ld1(Vtmp1, Vtmp2, Vtmp3, Vtmp4, T8H, src);
5768         BIND(NEXT_32_PRFM_START);
5769           prfm(Address(src, SoftwarePrefetchHintDistance));
5770           orr(v4, T16B, Vtmp1, Vtmp2);
5771           orr(v5, T16B, Vtmp3, Vtmp4);
5772           uzp1(Vtmp1, T16B, Vtmp1, Vtmp2);
5773           uzp1(Vtmp3, T16B, Vtmp3, Vtmp4);
5774           uzp2(v5, T16B, v4, v5); // high bytes
5775           umov(tmp2, v5, D, 1);
5776           fmovd(tmp1, v5);
5777           orr(tmp1, tmp1, tmp2);
5778           cbnz(tmp1, LOOP_8);
5779           stpq(Vtmp1, Vtmp3, dst);
5780           sub(len, len, 32);
5781           add(dst, dst, 32);
5782           add(src, src, 64);
5783           subs(tmp2, len, SoftwarePrefetchHintDistance/2 + 16);
5784           br(GE, NEXT_32_PRFM);
5785           cmp(len, (u1)32);
5786           br(LT, LOOP_8);
5787         BIND(NEXT_32);
5788           ld1(Vtmp1, Vtmp2, Vtmp3, Vtmp4, T8H, src);
5789         BIND(NEXT_32_START);
5790       } else {
5791         BIND(NEXT_32);
5792           ld1(Vtmp1, Vtmp2, Vtmp3, Vtmp4, T8H, src);
5793       }
5794       prfm(Address(src, SoftwarePrefetchHintDistance));
5795       uzp1(v4, T16B, Vtmp1, Vtmp2);
5796       uzp1(v5, T16B, Vtmp3, Vtmp4);
5797       orr(Vtmp1, T16B, Vtmp1, Vtmp2);
5798       orr(Vtmp3, T16B, Vtmp3, Vtmp4);
5799       uzp2(Vtmp1, T16B, Vtmp1, Vtmp3); // high bytes
5800       umov(tmp2, Vtmp1, D, 1);
5801       fmovd(tmp1, Vtmp1);
5802       orr(tmp1, tmp1, tmp2);
5803       cbnz(tmp1, LOOP_8);
5804       stpq(v4, v5, dst);
5805       sub(len, len, 32);
5806       add(dst, dst, 32);
5807       add(src, src, 64);
5808       cmp(len, (u1)32);
5809       br(GE, NEXT_32);
5810       cbz(len, DONE);
5811 
5812     BIND(LOOP_8);
5813       cmp(len, (u1)8);
5814       br(LT, LOOP_1);
5815     BIND(NEXT_8);
5816       ld1(Vtmp1, T8H, src);
5817       uzp1(Vtmp2, T16B, Vtmp1, Vtmp1); // low bytes
5818       uzp2(Vtmp3, T16B, Vtmp1, Vtmp1); // high bytes
5819       fmovd(tmp1, Vtmp3);
5820       cbnz(tmp1, NEXT_1);
5821       strd(Vtmp2, dst);
5822 
5823       sub(len, len, 8);
5824       add(dst, dst, 8);
5825       add(src, src, 16);
5826       cmp(len, (u1)8);
5827       br(GE, NEXT_8);
5828 
5829     BIND(LOOP_1);
5830 
5831     cbz(len, DONE);
5832     BIND(NEXT_1);
5833       ldrh(tmp1, Address(post(src, 2)));
5834       tst(tmp1, 0xff00);
5835       br(NE, SET_RESULT);
5836       strb(tmp1, Address(post(dst, 1)));
5837       subs(len, len, 1);
5838       br(GT, NEXT_1);
5839 
5840     BIND(SET_RESULT);
5841       sub(result, result, len); // Return index where we stopped
5842                                 // Return len == 0 if we processed all
5843                                 // characters
5844     BIND(DONE);
5845 }
5846 
5847 
5848 // Inflate byte[] array to char[].
5849 void MacroAssembler::byte_array_inflate(Register src, Register dst, Register len,
5850                                         FloatRegister vtmp1, FloatRegister vtmp2, FloatRegister vtmp3,
5851                                         Register tmp4) {
5852   Label big, done, after_init, to_stub;
5853 
5854   assert_different_registers(src, dst, len, tmp4, rscratch1);
5855 
5856   fmovd(vtmp1, zr);
5857   lsrw(tmp4, len, 3);
5858   bind(after_init);
5859   cbnzw(tmp4, big);
5860   // Short string: less than 8 bytes.
5861   {
5862     Label loop, tiny;
5863 
5864     cmpw(len, 4);
5865     br(LT, tiny);
5866     // Use SIMD to do 4 bytes.
5867     ldrs(vtmp2, post(src, 4));
5868     zip1(vtmp3, T8B, vtmp2, vtmp1);
5869     subw(len, len, 4);
5870     strd(vtmp3, post(dst, 8));
5871 
5872     cbzw(len, done);
5873 
5874     // Do the remaining bytes by steam.
5875     bind(loop);
5876     ldrb(tmp4, post(src, 1));
5877     strh(tmp4, post(dst, 2));
5878     subw(len, len, 1);
5879 
5880     bind(tiny);
5881     cbnz(len, loop);
5882 
5883     b(done);
5884   }
5885 
5886   if (SoftwarePrefetchHintDistance &gt;= 0) {
5887     bind(to_stub);
5888       RuntimeAddress stub =  RuntimeAddress(StubRoutines::aarch64::large_byte_array_inflate());
5889       assert(stub.target() != NULL, &quot;large_byte_array_inflate stub has not been generated&quot;);
5890       trampoline_call(stub);
5891       b(after_init);
5892   }
5893 
5894   // Unpack the bytes 8 at a time.
5895   bind(big);
5896   {
5897     Label loop, around, loop_last, loop_start;
5898 
5899     if (SoftwarePrefetchHintDistance &gt;= 0) {
5900       const int large_loop_threshold = (64 + 16)/8;
5901       ldrd(vtmp2, post(src, 8));
5902       andw(len, len, 7);
5903       cmp(tmp4, (u1)large_loop_threshold);
5904       br(GE, to_stub);
5905       b(loop_start);
5906 
5907       bind(loop);
5908       ldrd(vtmp2, post(src, 8));
5909       bind(loop_start);
5910       subs(tmp4, tmp4, 1);
5911       br(EQ, loop_last);
5912       zip1(vtmp2, T16B, vtmp2, vtmp1);
5913       ldrd(vtmp3, post(src, 8));
5914       st1(vtmp2, T8H, post(dst, 16));
5915       subs(tmp4, tmp4, 1);
5916       zip1(vtmp3, T16B, vtmp3, vtmp1);
5917       st1(vtmp3, T8H, post(dst, 16));
5918       br(NE, loop);
5919       b(around);
5920       bind(loop_last);
5921       zip1(vtmp2, T16B, vtmp2, vtmp1);
5922       st1(vtmp2, T8H, post(dst, 16));
5923       bind(around);
5924       cbz(len, done);
5925     } else {
5926       andw(len, len, 7);
5927       bind(loop);
5928       ldrd(vtmp2, post(src, 8));
5929       sub(tmp4, tmp4, 1);
5930       zip1(vtmp3, T16B, vtmp2, vtmp1);
5931       st1(vtmp3, T8H, post(dst, 16));
5932       cbnz(tmp4, loop);
5933     }
5934   }
5935 
5936   // Do the tail of up to 8 bytes.
5937   add(src, src, len);
5938   ldrd(vtmp3, Address(src, -8));
5939   add(dst, dst, len, ext::uxtw, 1);
5940   zip1(vtmp3, T16B, vtmp3, vtmp1);
5941   strq(vtmp3, Address(dst, -16));
5942 
5943   bind(done);
5944 }
5945 
5946 // Compress char[] array to byte[].
5947 void MacroAssembler::char_array_compress(Register src, Register dst, Register len,
5948                                          FloatRegister tmp1Reg, FloatRegister tmp2Reg,
5949                                          FloatRegister tmp3Reg, FloatRegister tmp4Reg,
5950                                          Register result) {
5951   encode_iso_array(src, dst, len, result,
5952                    tmp1Reg, tmp2Reg, tmp3Reg, tmp4Reg);
5953   cmp(len, zr);
5954   csel(result, result, zr, EQ);
5955 }
5956 
5957 // get_thread() can be called anywhere inside generated code so we
5958 // need to save whatever non-callee save context might get clobbered
5959 // by the call to JavaThread::aarch64_get_thread_helper() or, indeed,
5960 // the call setup code.
5961 //
5962 // aarch64_get_thread_helper() clobbers only r0, r1, and flags.
5963 //
5964 void MacroAssembler::get_thread(Register dst) {
5965   RegSet saved_regs = RegSet::range(r0, r1) + lr - dst;
5966   push(saved_regs, sp);
5967 
5968   mov(lr, CAST_FROM_FN_PTR(address, JavaThread::aarch64_get_thread_helper));
5969   blr(lr);
5970   if (dst != c_rarg0) {
5971     mov(dst, c_rarg0);
5972   }
5973 
5974   pop(saved_regs, sp);
5975 }
5976 
5977 // C2 compiled method&#39;s prolog code
5978 // Moved here from aarch64.ad to support Valhalla code belows
5979 void MacroAssembler::verified_entry(Compile* C, int sp_inc) {
5980 
5981 // n.b. frame size includes space for return pc and rfp
5982   const long framesize = C-&gt;frame_size_in_bytes();
5983   assert(framesize % (2 * wordSize) == 0, &quot;must preserve 2 * wordSize alignment&quot;);
5984 
5985   // insert a nop at the start of the prolog so we can patch in a
5986   // branch if we need to invalidate the method later
5987   nop();
5988 
5989   int bangsize = C-&gt;bang_size_in_bytes();
5990   if (C-&gt;need_stack_bang(bangsize) &amp;&amp; UseStackBanging)
5991      generate_stack_overflow_check(bangsize);
5992 
5993   build_frame(framesize);
5994 
5995   if (VerifyStackAtCalls) {
5996     Unimplemented();
5997   }
5998 }
5999 
6000 int MacroAssembler::store_value_type_fields_to_buf(ciValueKlass* vk, bool from_interpreter) {
6001   // A value type might be returned. If fields are in registers we
6002   // need to allocate a value type instance and initialize it with
6003   // the value of the fields.
6004   Label skip;
6005   // We only need a new buffered value if a new one is not returned
6006   cmp(r0, (u1) 1);
6007   br(Assembler::EQ, skip);
6008   int call_offset = -1;
6009 
6010   Label slow_case;
6011 
6012   // Try to allocate a new buffered value (from the heap)
6013   if (UseTLAB) {
6014 
6015     if (vk != NULL) {
6016       // Called from C1, where the return type is statically known.
6017       mov(r1, (intptr_t)vk-&gt;get_ValueKlass());
6018       jint lh = vk-&gt;layout_helper();
6019       assert(lh != Klass::_lh_neutral_value, &quot;inline class in return type must have been resolved&quot;);
6020       mov(r14, lh);
6021     } else {
6022        // Call from interpreter. R0 contains ((the ValueKlass* of the return type) | 0x01)
6023        andr(r1, r0, -2);
6024        // get obj size
6025        ldrw(r14, Address(rscratch1 /*klass*/, Klass::layout_helper_offset()));
6026     }
6027 
6028      ldr(r13, Address(rthread, in_bytes(JavaThread::tlab_top_offset())));
6029 
6030      // check whether we have space in TLAB,
6031      // rscratch1 contains pointer to just allocated obj
6032       lea(r14, Address(r13, r14));
6033       ldr(rscratch1, Address(rthread, in_bytes(JavaThread::tlab_end_offset())));
6034 
6035       cmp(r14, rscratch1);
6036       br(Assembler::GT, slow_case);
6037 
6038       // OK we have room in TLAB,
6039       // Set new TLAB top
6040       str(r14, Address(rthread, in_bytes(JavaThread::tlab_top_offset())));
6041 
6042       // Set new class always locked
6043       mov(rscratch1, (uint64_t) markWord::always_locked_prototype().value());
6044       str(rscratch1, Address(r13, oopDesc::mark_offset_in_bytes()));
6045 
6046       store_klass_gap(r13, zr);  // zero klass gap for compressed oops
6047       if (vk == NULL) {
6048         // store_klass corrupts rbx, so save it in rax for later use (interpreter case only).
6049          mov(r0, r1);
6050       }
6051 
6052       store_klass(r13, r1);  // klass
6053 
6054       if (vk != NULL) {
6055         // FIXME -- do the packing in-line to avoid the runtime call
6056         mov(r0, r13);
6057         far_call(RuntimeAddress(vk-&gt;pack_handler())); // no need for call info as this will not safepoint.
6058       } else {
6059 
6060         // We have our new buffered value, initialize its fields with a
6061         // value class specific handler
6062         ldr(r1, Address(r0, InstanceKlass::adr_valueklass_fixed_block_offset()));
6063         ldr(r1, Address(r1, ValueKlass::pack_handler_offset()));
6064 
6065         // Mov new class to r0 and call pack_handler
6066         mov(r0, r13);
6067         blr(r1);
6068       }
6069       b(skip);
6070   }
6071 
6072   bind(slow_case);
6073   // We failed to allocate a new value, fall back to a runtime
6074   // call. Some oop field may be live in some registers but we can&#39;t
6075   // tell. That runtime call will take care of preserving them
6076   // across a GC if there&#39;s one.
6077 
6078 
6079   if (from_interpreter) {
6080     super_call_VM_leaf(StubRoutines::store_value_type_fields_to_buf());
6081   } else {
6082     ldr(rscratch1, RuntimeAddress(StubRoutines::store_value_type_fields_to_buf()));
6083     blr(rscratch1);
6084     call_offset = offset();
6085   }
6086 
6087   bind(skip);
6088   return call_offset;
6089 }
6090 
6091 // Move a value between registers/stack slots and update the reg_state
6092 bool MacroAssembler::move_helper(VMReg from, VMReg to, BasicType bt, RegState reg_state[], int ret_off, int extra_stack_offset) {
6093   if (reg_state[to-&gt;value()] == reg_written) {
6094     return true; // Already written
6095   }
6096 
6097   if (from != to &amp;&amp; bt != T_VOID) {
6098     if (reg_state[to-&gt;value()] == reg_readonly) {
6099       return false; // Not yet writable
6100     }
6101     if (from-&gt;is_reg()) {
6102       if (to-&gt;is_reg()) {
6103         mov(to-&gt;as_Register(), from-&gt;as_Register());
6104       } else {
6105         int st_off = to-&gt;reg2stack() * VMRegImpl::stack_slot_size + extra_stack_offset;
6106         Address to_addr = Address(sp, st_off);
6107         if (from-&gt;is_FloatRegister()) {
6108           if (bt == T_DOUBLE) {
6109              strd(from-&gt;as_FloatRegister(), to_addr);
6110           } else {
6111              assert(bt == T_FLOAT, &quot;must be float&quot;);
6112              strs(from-&gt;as_FloatRegister(), to_addr);
6113           }
6114         } else {
6115           str(from-&gt;as_Register(), to_addr);
6116         }
6117       }
6118     } else {
6119       Address from_addr = Address(sp, from-&gt;reg2stack() * VMRegImpl::stack_slot_size + extra_stack_offset);
6120       if (to-&gt;is_reg()) {
6121         if (to-&gt;is_FloatRegister()) {
6122           if (bt == T_DOUBLE) {
6123              ldrd(to-&gt;as_FloatRegister(), from_addr);
6124           } else {
6125             assert(bt == T_FLOAT, &quot;must be float&quot;);
6126             ldrs(to-&gt;as_FloatRegister(), from_addr);
6127           }
6128         } else {
6129           ldr(to-&gt;as_Register(), from_addr);
6130         }
6131       } else {
6132         int st_off = to-&gt;reg2stack() * VMRegImpl::stack_slot_size + extra_stack_offset;
6133         ldr(rscratch1, from_addr);
6134         str(rscratch1, Address(sp, st_off));
6135       }
6136     }
6137   }
6138 
6139   // Update register states
6140   reg_state[from-&gt;value()] = reg_writable;
6141   reg_state[to-&gt;value()] = reg_written;
6142   return true;
6143 }
6144 
6145 // Read all fields from a value type oop and store the values in registers/stack slots
6146 bool MacroAssembler::unpack_value_helper(const GrowableArray&lt;SigEntry&gt;* sig, int&amp; sig_index, VMReg from, VMRegPair* regs_to,
6147                                          int&amp; to_index, RegState reg_state[], int ret_off, int extra_stack_offset) {
6148   Register fromReg = from-&gt;is_reg() ? from-&gt;as_Register() : noreg;
6149   assert(sig-&gt;at(sig_index)._bt == T_VOID, &quot;should be at end delimiter&quot;);
6150 
6151 
6152   int vt = 1;
6153   bool done = true;
6154   bool mark_done = true;
6155   do {
6156     sig_index--;
6157     BasicType bt = sig-&gt;at(sig_index)._bt;
6158     if (bt == T_VALUETYPE) {
6159       vt--;
6160     } else if (bt == T_VOID &amp;&amp;
6161                sig-&gt;at(sig_index-1)._bt != T_LONG &amp;&amp;
6162                sig-&gt;at(sig_index-1)._bt != T_DOUBLE) {
6163       vt++;
6164     } else if (SigEntry::is_reserved_entry(sig, sig_index)) {
6165       to_index--; // Ignore this
6166     } else {
6167       assert(to_index &gt;= 0, &quot;invalid to_index&quot;);
6168       VMRegPair pair_to = regs_to[to_index--];
6169       VMReg to = pair_to.first();
6170 
6171       if (bt == T_VOID) continue;
6172 
6173       int idx = (int) to-&gt;value();
6174       if (reg_state[idx] == reg_readonly) {
6175          if (idx != from-&gt;value()) {
6176            mark_done = false;
6177          }
6178          done = false;
6179          continue;
6180       } else if (reg_state[idx] == reg_written) {
6181         continue;
6182       } else {
6183         assert(reg_state[idx] == reg_writable, &quot;must be writable&quot;);
6184         reg_state[idx] = reg_written;
6185       }
6186 
6187       if (fromReg == noreg) {
6188         int st_off = from-&gt;reg2stack() * VMRegImpl::stack_slot_size + extra_stack_offset;
6189         ldr(rscratch2, Address(sp, st_off));
6190         fromReg = rscratch2;
6191       }
6192 
6193       int off = sig-&gt;at(sig_index)._offset;
6194       assert(off &gt; 0, &quot;offset in object should be positive&quot;);
6195       bool is_oop = (bt == T_OBJECT || bt == T_ARRAY);
6196 
6197       Address fromAddr = Address(fromReg, off);
6198       bool is_signed = (bt != T_CHAR) &amp;&amp; (bt != T_BOOLEAN);
6199 
6200       if (!to-&gt;is_FloatRegister()) {
6201 
6202         Register dst = to-&gt;is_stack() ? rscratch1 : to-&gt;as_Register();
6203 
6204         if (is_oop) {
6205           load_heap_oop(dst, fromAddr);
6206         } else {
6207           load_sized_value(dst, fromAddr, type2aelembytes(bt), is_signed);
6208         }
6209         if (to-&gt;is_stack()) {
6210           int st_off = to-&gt;reg2stack() * VMRegImpl::stack_slot_size + extra_stack_offset;
6211           str(dst, Address(sp, st_off));
6212         }
6213       } else {
6214         if (bt == T_DOUBLE) {
6215           ldrd(to-&gt;as_FloatRegister(), fromAddr);
6216         } else {
6217           assert(bt == T_FLOAT, &quot;must be float&quot;);
6218           ldrs(to-&gt;as_FloatRegister(), fromAddr);
6219         }
6220      }
6221 
6222     }
6223 
6224   } while (vt != 0);
6225 
6226   if (mark_done &amp;&amp; reg_state[from-&gt;value()] != reg_written) {
6227     // This is okay because no one else will write to that slot
6228     reg_state[from-&gt;value()] = reg_writable;
6229   }
6230   return done;
6231 }
6232 
6233 // Pack fields back into a value type oop
6234 bool MacroAssembler::pack_value_helper(const GrowableArray&lt;SigEntry&gt;* sig, int&amp; sig_index, int vtarg_index,
6235                                        VMReg to, VMRegPair* regs_from, int regs_from_count, int&amp; from_index, RegState reg_state[],
6236                                        int ret_off, int extra_stack_offset) {
6237   assert(sig-&gt;at(sig_index)._bt == T_VALUETYPE, &quot;should be at end delimiter&quot;);
6238   assert(to-&gt;is_valid(), &quot;must be&quot;);
6239 
6240   if (reg_state[to-&gt;value()] == reg_written) {
6241     skip_unpacked_fields(sig, sig_index, regs_from, regs_from_count, from_index);
6242     return true; // Already written
6243   }
6244 
6245   Register val_array = r0;
6246   Register val_obj_tmp = r11;
6247   Register from_reg_tmp = r10;
6248   Register tmp1 = r14;
6249   Register tmp2 = r13;
6250   Register tmp3 = r1;
6251   Register val_obj = to-&gt;is_stack() ? val_obj_tmp : to-&gt;as_Register();
6252 
6253   if (reg_state[to-&gt;value()] == reg_readonly) {
6254     if (!is_reg_in_unpacked_fields(sig, sig_index, to, regs_from, regs_from_count, from_index)) {
6255       skip_unpacked_fields(sig, sig_index, regs_from, regs_from_count, from_index);
6256       return false; // Not yet writable
6257     }
6258     val_obj = val_obj_tmp;
6259   }
6260 
6261   int index = arrayOopDesc::base_offset_in_bytes(T_OBJECT) + vtarg_index * type2aelembytes(T_VALUETYPE);
6262   load_heap_oop(val_obj, Address(val_array, index));
6263 
6264   ScalarizedValueArgsStream stream(sig, sig_index, regs_from, regs_from_count, from_index);
6265   VMRegPair from_pair;
6266   BasicType bt;
6267 
6268   while (stream.next(from_pair, bt)) {
6269     int off = sig-&gt;at(stream.sig_cc_index())._offset;
6270     assert(off &gt; 0, &quot;offset in object should be positive&quot;);
6271     bool is_oop = (bt == T_OBJECT || bt == T_ARRAY);
6272     size_t size_in_bytes = is_java_primitive(bt) ? type2aelembytes(bt) : wordSize;
6273 
6274     VMReg from_r1 = from_pair.first();
6275     VMReg from_r2 = from_pair.second();
6276 
6277     // Pack the scalarized field into the value object.
6278     Address dst(val_obj, off);
6279 
6280     if (!from_r1-&gt;is_FloatRegister()) {
6281       Register from_reg;
6282       if (from_r1-&gt;is_stack()) {
6283         from_reg = from_reg_tmp;
6284         int ld_off = from_r1-&gt;reg2stack() * VMRegImpl::stack_slot_size + extra_stack_offset;
6285         load_sized_value(from_reg, Address(sp, ld_off), size_in_bytes, /* is_signed */ false);
6286       } else {
6287         from_reg = from_r1-&gt;as_Register();
6288       }
6289 
6290       if (is_oop) {
6291         DecoratorSet decorators = IN_HEAP | ACCESS_WRITE;
6292         store_heap_oop(dst, from_reg, tmp1, tmp2, tmp3, decorators);
6293       } else {
6294         store_sized_value(dst, from_reg, size_in_bytes);
6295       }
6296     } else {
6297       if (from_r2-&gt;is_valid()) {
6298         strd(from_r1-&gt;as_FloatRegister(), dst);
6299       } else {
6300         strs(from_r1-&gt;as_FloatRegister(), dst);
6301       }
6302     }
6303 
6304     reg_state[from_r1-&gt;value()] = reg_writable;
6305   }
6306   sig_index = stream.sig_cc_index();
6307   from_index = stream.regs_cc_index();
6308 
6309   assert(reg_state[to-&gt;value()] == reg_writable, &quot;must have already been read&quot;);
6310   bool success = move_helper(val_obj-&gt;as_VMReg(), to, T_OBJECT, reg_state, ret_off, extra_stack_offset);
6311   assert(success, &quot;to register must be writeable&quot;);
6312 
6313   return true;
6314 }
6315 
6316 // Unpack all value type arguments passed as oops
6317 void MacroAssembler::unpack_value_args(Compile* C, bool receiver_only) {
6318   int sp_inc = unpack_value_args_common(C, receiver_only);
6319   // Emit code for verified entry and save increment for stack repair on return
6320   verified_entry(C, sp_inc);
6321 }
6322 
6323 int MacroAssembler::shuffle_value_args(bool is_packing, bool receiver_only, int extra_stack_offset,
6324                                        BasicType* sig_bt, const GrowableArray&lt;SigEntry&gt;* sig_cc,
6325                                        int args_passed, int args_on_stack, VMRegPair* regs,            // from
6326                                        int args_passed_to, int args_on_stack_to, VMRegPair* regs_to) { // to
6327   // Check if we need to extend the stack for packing/unpacking
6328   int sp_inc = (args_on_stack_to - args_on_stack) * VMRegImpl::stack_slot_size;
6329   if (sp_inc &gt; 0) {
6330     sp_inc = align_up(sp_inc, StackAlignmentInBytes);
6331     if (!is_packing) {
6332       // Save the return address, adjust the stack (make sure it is properly
6333       // 16-byte aligned) and copy the return address to the new top of the stack.
6334       // (Note: C1 does this in C1_MacroAssembler::scalarized_entry).
6335       // FIXME: We need not to preserve return address on aarch64
6336       pop(rscratch1);
6337       sub(sp, sp, sp_inc);
6338       push(rscratch1);
6339     }
6340   } else {
6341     // The scalarized calling convention needs less stack space than the unscalarized one.
6342     // No need to extend the stack, the caller will take care of these adjustments.
6343     sp_inc = 0;
6344   }
6345 
6346   int ret_off; // make sure we don&#39;t overwrite the return address
6347   if (is_packing) {
6348     // For C1 code, the VVEP doesn&#39;t have reserved slots, so we store the returned address at
6349     // rsp[0] during shuffling.
6350     ret_off = 0;
6351   } else {
6352     // C2 code ensures that sp_inc is a reserved slot.
6353     ret_off = sp_inc;
6354   }
6355 
6356   return shuffle_value_args_common(is_packing, receiver_only, extra_stack_offset,
6357                                    sig_bt, sig_cc,
6358                                    args_passed, args_on_stack, regs,
6359                                    args_passed_to, args_on_stack_to, regs_to,
6360                                    sp_inc, ret_off);
6361 }
6362 
6363 VMReg MacroAssembler::spill_reg_for(VMReg reg) {
6364   return (reg-&gt;is_FloatRegister()) ? v0-&gt;as_VMReg() : r14-&gt;as_VMReg();
6365 }
6366 
6367 void MacroAssembler::cache_wb(Address line) {
6368   assert(line.getMode() == Address::base_plus_offset, &quot;mode should be base_plus_offset&quot;);
6369   assert(line.index() == noreg, &quot;index should be noreg&quot;);
6370   assert(line.offset() == 0, &quot;offset should be 0&quot;);
6371   // would like to assert this
6372   // assert(line._ext.shift == 0, &quot;shift should be zero&quot;);
6373   if (VM_Version::supports_dcpop()) {
6374     // writeback using clear virtual address to point of persistence
6375     dc(Assembler::CVAP, line.base());
6376   } else {
6377     // no need to generate anything as Unsafe.writebackMemory should
6378     // never invoke this stub
6379   }
6380 }
6381 
6382 void MacroAssembler::cache_wbsync(bool is_pre) {
6383   // we only need a barrier post sync
6384   if (!is_pre) {
6385     membar(Assembler::AnyAny);
6386   }
6387 }
    </pre>
  </body>
</html>