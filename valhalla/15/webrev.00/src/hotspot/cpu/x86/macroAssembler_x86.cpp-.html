<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Old src/hotspot/cpu/x86/macroAssembler_x86.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
  <body>
    <pre>
    1 /*
    2  * Copyright (c) 1997, 2020, Oracle and/or its affiliates. All rights reserved.
    3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
    4  *
    5  * This code is free software; you can redistribute it and/or modify it
    6  * under the terms of the GNU General Public License version 2 only, as
    7  * published by the Free Software Foundation.
    8  *
    9  * This code is distributed in the hope that it will be useful, but WITHOUT
   10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
   11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
   12  * version 2 for more details (a copy is included in the LICENSE file that
   13  * accompanied this code).
   14  *
   15  * You should have received a copy of the GNU General Public License version
   16  * 2 along with this work; if not, write to the Free Software Foundation,
   17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
   18  *
   19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
   20  * or visit www.oracle.com if you need additional information or have any
   21  * questions.
   22  *
   23  */
   24 
   25 #include &quot;precompiled.hpp&quot;
   26 #include &quot;jvm.h&quot;
   27 #include &quot;asm/assembler.hpp&quot;
   28 #include &quot;asm/assembler.inline.hpp&quot;
   29 #include &quot;compiler/disassembler.hpp&quot;
   30 #include &quot;gc/shared/barrierSet.hpp&quot;
   31 #include &quot;gc/shared/barrierSetAssembler.hpp&quot;
   32 #include &quot;gc/shared/collectedHeap.inline.hpp&quot;
   33 #include &quot;interpreter/interpreter.hpp&quot;
   34 #include &quot;memory/resourceArea.hpp&quot;
   35 #include &quot;memory/universe.hpp&quot;
   36 #include &quot;oops/accessDecorators.hpp&quot;
   37 #include &quot;oops/compressedOops.inline.hpp&quot;
   38 #include &quot;oops/klass.inline.hpp&quot;
   39 #include &quot;prims/methodHandles.hpp&quot;
   40 #include &quot;runtime/biasedLocking.hpp&quot;
   41 #include &quot;runtime/flags/flagSetting.hpp&quot;
   42 #include &quot;runtime/interfaceSupport.inline.hpp&quot;
   43 #include &quot;runtime/objectMonitor.hpp&quot;
   44 #include &quot;runtime/os.hpp&quot;
   45 #include &quot;runtime/safepoint.hpp&quot;
   46 #include &quot;runtime/safepointMechanism.hpp&quot;
   47 #include &quot;runtime/sharedRuntime.hpp&quot;
   48 #include &quot;runtime/signature_cc.hpp&quot;
   49 #include &quot;runtime/stubRoutines.hpp&quot;
   50 #include &quot;runtime/thread.hpp&quot;
   51 #include &quot;utilities/macros.hpp&quot;
   52 #include &quot;vmreg_x86.inline.hpp&quot;
   53 #include &quot;crc32c.h&quot;
   54 #ifdef COMPILER2
   55 #include &quot;opto/intrinsicnode.hpp&quot;
   56 #endif
   57 
   58 #ifdef PRODUCT
   59 #define BLOCK_COMMENT(str) /* nothing */
   60 #define STOP(error) stop(error)
   61 #else
   62 #define BLOCK_COMMENT(str) block_comment(str)
   63 #define STOP(error) block_comment(error); stop(error)
   64 #endif
   65 
   66 #define BIND(label) bind(label); BLOCK_COMMENT(#label &quot;:&quot;)
   67 
   68 #ifdef ASSERT
   69 bool AbstractAssembler::pd_check_instruction_mark() { return true; }
   70 #endif
   71 
   72 static Assembler::Condition reverse[] = {
   73     Assembler::noOverflow     /* overflow      = 0x0 */ ,
   74     Assembler::overflow       /* noOverflow    = 0x1 */ ,
   75     Assembler::aboveEqual     /* carrySet      = 0x2, below         = 0x2 */ ,
   76     Assembler::below          /* aboveEqual    = 0x3, carryClear    = 0x3 */ ,
   77     Assembler::notZero        /* zero          = 0x4, equal         = 0x4 */ ,
   78     Assembler::zero           /* notZero       = 0x5, notEqual      = 0x5 */ ,
   79     Assembler::above          /* belowEqual    = 0x6 */ ,
   80     Assembler::belowEqual     /* above         = 0x7 */ ,
   81     Assembler::positive       /* negative      = 0x8 */ ,
   82     Assembler::negative       /* positive      = 0x9 */ ,
   83     Assembler::noParity       /* parity        = 0xa */ ,
   84     Assembler::parity         /* noParity      = 0xb */ ,
   85     Assembler::greaterEqual   /* less          = 0xc */ ,
   86     Assembler::less           /* greaterEqual  = 0xd */ ,
   87     Assembler::greater        /* lessEqual     = 0xe */ ,
   88     Assembler::lessEqual      /* greater       = 0xf, */
   89 
   90 };
   91 
   92 
   93 // Implementation of MacroAssembler
   94 
   95 // First all the versions that have distinct versions depending on 32/64 bit
   96 // Unless the difference is trivial (1 line or so).
   97 
   98 #ifndef _LP64
   99 
  100 // 32bit versions
  101 
  102 Address MacroAssembler::as_Address(AddressLiteral adr) {
  103   return Address(adr.target(), adr.rspec());
  104 }
  105 
  106 Address MacroAssembler::as_Address(ArrayAddress adr) {
  107   return Address::make_array(adr);
  108 }
  109 
  110 void MacroAssembler::call_VM_leaf_base(address entry_point,
  111                                        int number_of_arguments) {
  112   call(RuntimeAddress(entry_point));
  113   increment(rsp, number_of_arguments * wordSize);
  114 }
  115 
  116 void MacroAssembler::cmpklass(Address src1, Metadata* obj) {
  117   cmp_literal32(src1, (int32_t)obj, metadata_Relocation::spec_for_immediate());
  118 }
  119 
  120 void MacroAssembler::cmpklass(Register src1, Metadata* obj) {
  121   cmp_literal32(src1, (int32_t)obj, metadata_Relocation::spec_for_immediate());
  122 }
  123 
  124 void MacroAssembler::cmpoop_raw(Address src1, jobject obj) {
  125   cmp_literal32(src1, (int32_t)obj, oop_Relocation::spec_for_immediate());
  126 }
  127 
  128 void MacroAssembler::cmpoop_raw(Register src1, jobject obj) {
  129   cmp_literal32(src1, (int32_t)obj, oop_Relocation::spec_for_immediate());
  130 }
  131 
  132 void MacroAssembler::cmpoop(Address src1, jobject obj) {
  133   BarrierSetAssembler* bs = BarrierSet::barrier_set()-&gt;barrier_set_assembler();
  134   bs-&gt;obj_equals(this, src1, obj);
  135 }
  136 
  137 void MacroAssembler::cmpoop(Register src1, jobject obj) {
  138   BarrierSetAssembler* bs = BarrierSet::barrier_set()-&gt;barrier_set_assembler();
  139   bs-&gt;obj_equals(this, src1, obj);
  140 }
  141 
  142 void MacroAssembler::extend_sign(Register hi, Register lo) {
  143   // According to Intel Doc. AP-526, &quot;Integer Divide&quot;, p.18.
  144   if (VM_Version::is_P6() &amp;&amp; hi == rdx &amp;&amp; lo == rax) {
  145     cdql();
  146   } else {
  147     movl(hi, lo);
  148     sarl(hi, 31);
  149   }
  150 }
  151 
  152 void MacroAssembler::jC2(Register tmp, Label&amp; L) {
  153   // set parity bit if FPU flag C2 is set (via rax)
  154   save_rax(tmp);
  155   fwait(); fnstsw_ax();
  156   sahf();
  157   restore_rax(tmp);
  158   // branch
  159   jcc(Assembler::parity, L);
  160 }
  161 
  162 void MacroAssembler::jnC2(Register tmp, Label&amp; L) {
  163   // set parity bit if FPU flag C2 is set (via rax)
  164   save_rax(tmp);
  165   fwait(); fnstsw_ax();
  166   sahf();
  167   restore_rax(tmp);
  168   // branch
  169   jcc(Assembler::noParity, L);
  170 }
  171 
  172 // 32bit can do a case table jump in one instruction but we no longer allow the base
  173 // to be installed in the Address class
  174 void MacroAssembler::jump(ArrayAddress entry) {
  175   jmp(as_Address(entry));
  176 }
  177 
  178 // Note: y_lo will be destroyed
  179 void MacroAssembler::lcmp2int(Register x_hi, Register x_lo, Register y_hi, Register y_lo) {
  180   // Long compare for Java (semantics as described in JVM spec.)
  181   Label high, low, done;
  182 
  183   cmpl(x_hi, y_hi);
  184   jcc(Assembler::less, low);
  185   jcc(Assembler::greater, high);
  186   // x_hi is the return register
  187   xorl(x_hi, x_hi);
  188   cmpl(x_lo, y_lo);
  189   jcc(Assembler::below, low);
  190   jcc(Assembler::equal, done);
  191 
  192   bind(high);
  193   xorl(x_hi, x_hi);
  194   increment(x_hi);
  195   jmp(done);
  196 
  197   bind(low);
  198   xorl(x_hi, x_hi);
  199   decrementl(x_hi);
  200 
  201   bind(done);
  202 }
  203 
  204 void MacroAssembler::lea(Register dst, AddressLiteral src) {
  205     mov_literal32(dst, (int32_t)src.target(), src.rspec());
  206 }
  207 
  208 void MacroAssembler::lea(Address dst, AddressLiteral adr) {
  209   // leal(dst, as_Address(adr));
  210   // see note in movl as to why we must use a move
  211   mov_literal32(dst, (int32_t) adr.target(), adr.rspec());
  212 }
  213 
  214 void MacroAssembler::leave() {
  215   mov(rsp, rbp);
  216   pop(rbp);
  217 }
  218 
  219 void MacroAssembler::lmul(int x_rsp_offset, int y_rsp_offset) {
  220   // Multiplication of two Java long values stored on the stack
  221   // as illustrated below. Result is in rdx:rax.
  222   //
  223   // rsp ---&gt; [  ??  ] \               \
  224   //            ....    | y_rsp_offset  |
  225   //          [ y_lo ] /  (in bytes)    | x_rsp_offset
  226   //          [ y_hi ]                  | (in bytes)
  227   //            ....                    |
  228   //          [ x_lo ]                 /
  229   //          [ x_hi ]
  230   //            ....
  231   //
  232   // Basic idea: lo(result) = lo(x_lo * y_lo)
  233   //             hi(result) = hi(x_lo * y_lo) + lo(x_hi * y_lo) + lo(x_lo * y_hi)
  234   Address x_hi(rsp, x_rsp_offset + wordSize); Address x_lo(rsp, x_rsp_offset);
  235   Address y_hi(rsp, y_rsp_offset + wordSize); Address y_lo(rsp, y_rsp_offset);
  236   Label quick;
  237   // load x_hi, y_hi and check if quick
  238   // multiplication is possible
  239   movl(rbx, x_hi);
  240   movl(rcx, y_hi);
  241   movl(rax, rbx);
  242   orl(rbx, rcx);                                 // rbx, = 0 &lt;=&gt; x_hi = 0 and y_hi = 0
  243   jcc(Assembler::zero, quick);                   // if rbx, = 0 do quick multiply
  244   // do full multiplication
  245   // 1st step
  246   mull(y_lo);                                    // x_hi * y_lo
  247   movl(rbx, rax);                                // save lo(x_hi * y_lo) in rbx,
  248   // 2nd step
  249   movl(rax, x_lo);
  250   mull(rcx);                                     // x_lo * y_hi
  251   addl(rbx, rax);                                // add lo(x_lo * y_hi) to rbx,
  252   // 3rd step
  253   bind(quick);                                   // note: rbx, = 0 if quick multiply!
  254   movl(rax, x_lo);
  255   mull(y_lo);                                    // x_lo * y_lo
  256   addl(rdx, rbx);                                // correct hi(x_lo * y_lo)
  257 }
  258 
  259 void MacroAssembler::lneg(Register hi, Register lo) {
  260   negl(lo);
  261   adcl(hi, 0);
  262   negl(hi);
  263 }
  264 
  265 void MacroAssembler::lshl(Register hi, Register lo) {
  266   // Java shift left long support (semantics as described in JVM spec., p.305)
  267   // (basic idea for shift counts s &gt;= n: x &lt;&lt; s == (x &lt;&lt; n) &lt;&lt; (s - n))
  268   // shift value is in rcx !
  269   assert(hi != rcx, &quot;must not use rcx&quot;);
  270   assert(lo != rcx, &quot;must not use rcx&quot;);
  271   const Register s = rcx;                        // shift count
  272   const int      n = BitsPerWord;
  273   Label L;
  274   andl(s, 0x3f);                                 // s := s &amp; 0x3f (s &lt; 0x40)
  275   cmpl(s, n);                                    // if (s &lt; n)
  276   jcc(Assembler::less, L);                       // else (s &gt;= n)
  277   movl(hi, lo);                                  // x := x &lt;&lt; n
  278   xorl(lo, lo);
  279   // Note: subl(s, n) is not needed since the Intel shift instructions work rcx mod n!
  280   bind(L);                                       // s (mod n) &lt; n
  281   shldl(hi, lo);                                 // x := x &lt;&lt; s
  282   shll(lo);
  283 }
  284 
  285 
  286 void MacroAssembler::lshr(Register hi, Register lo, bool sign_extension) {
  287   // Java shift right long support (semantics as described in JVM spec., p.306 &amp; p.310)
  288   // (basic idea for shift counts s &gt;= n: x &gt;&gt; s == (x &gt;&gt; n) &gt;&gt; (s - n))
  289   assert(hi != rcx, &quot;must not use rcx&quot;);
  290   assert(lo != rcx, &quot;must not use rcx&quot;);
  291   const Register s = rcx;                        // shift count
  292   const int      n = BitsPerWord;
  293   Label L;
  294   andl(s, 0x3f);                                 // s := s &amp; 0x3f (s &lt; 0x40)
  295   cmpl(s, n);                                    // if (s &lt; n)
  296   jcc(Assembler::less, L);                       // else (s &gt;= n)
  297   movl(lo, hi);                                  // x := x &gt;&gt; n
  298   if (sign_extension) sarl(hi, 31);
  299   else                xorl(hi, hi);
  300   // Note: subl(s, n) is not needed since the Intel shift instructions work rcx mod n!
  301   bind(L);                                       // s (mod n) &lt; n
  302   shrdl(lo, hi);                                 // x := x &gt;&gt; s
  303   if (sign_extension) sarl(hi);
  304   else                shrl(hi);
  305 }
  306 
  307 void MacroAssembler::movoop(Register dst, jobject obj) {
  308   mov_literal32(dst, (int32_t)obj, oop_Relocation::spec_for_immediate());
  309 }
  310 
  311 void MacroAssembler::movoop(Address dst, jobject obj) {
  312   mov_literal32(dst, (int32_t)obj, oop_Relocation::spec_for_immediate());
  313 }
  314 
  315 void MacroAssembler::mov_metadata(Register dst, Metadata* obj) {
  316   mov_literal32(dst, (int32_t)obj, metadata_Relocation::spec_for_immediate());
  317 }
  318 
  319 void MacroAssembler::mov_metadata(Address dst, Metadata* obj) {
  320   mov_literal32(dst, (int32_t)obj, metadata_Relocation::spec_for_immediate());
  321 }
  322 
  323 void MacroAssembler::movptr(Register dst, AddressLiteral src, Register scratch) {
  324   // scratch register is not used,
  325   // it is defined to match parameters of 64-bit version of this method.
  326   if (src.is_lval()) {
  327     mov_literal32(dst, (intptr_t)src.target(), src.rspec());
  328   } else {
  329     movl(dst, as_Address(src));
  330   }
  331 }
  332 
  333 void MacroAssembler::movptr(ArrayAddress dst, Register src) {
  334   movl(as_Address(dst), src);
  335 }
  336 
  337 void MacroAssembler::movptr(Register dst, ArrayAddress src) {
  338   movl(dst, as_Address(src));
  339 }
  340 
  341 // src should NEVER be a real pointer. Use AddressLiteral for true pointers
  342 void MacroAssembler::movptr(Address dst, intptr_t src) {
  343   movl(dst, src);
  344 }
  345 
  346 
  347 void MacroAssembler::pop_callee_saved_registers() {
  348   pop(rcx);
  349   pop(rdx);
  350   pop(rdi);
  351   pop(rsi);
  352 }
  353 
  354 void MacroAssembler::push_callee_saved_registers() {
  355   push(rsi);
  356   push(rdi);
  357   push(rdx);
  358   push(rcx);
  359 }
  360 
  361 void MacroAssembler::pushoop(jobject obj) {
  362   push_literal32((int32_t)obj, oop_Relocation::spec_for_immediate());
  363 }
  364 
  365 void MacroAssembler::pushklass(Metadata* obj) {
  366   push_literal32((int32_t)obj, metadata_Relocation::spec_for_immediate());
  367 }
  368 
  369 void MacroAssembler::pushptr(AddressLiteral src) {
  370   if (src.is_lval()) {
  371     push_literal32((int32_t)src.target(), src.rspec());
  372   } else {
  373     pushl(as_Address(src));
  374   }
  375 }
  376 
  377 void MacroAssembler::set_word_if_not_zero(Register dst) {
  378   xorl(dst, dst);
  379   set_byte_if_not_zero(dst);
  380 }
  381 
  382 static void pass_arg0(MacroAssembler* masm, Register arg) {
  383   masm-&gt;push(arg);
  384 }
  385 
  386 static void pass_arg1(MacroAssembler* masm, Register arg) {
  387   masm-&gt;push(arg);
  388 }
  389 
  390 static void pass_arg2(MacroAssembler* masm, Register arg) {
  391   masm-&gt;push(arg);
  392 }
  393 
  394 static void pass_arg3(MacroAssembler* masm, Register arg) {
  395   masm-&gt;push(arg);
  396 }
  397 
  398 #ifndef PRODUCT
  399 extern &quot;C&quot; void findpc(intptr_t x);
  400 #endif
  401 
  402 void MacroAssembler::debug32(int rdi, int rsi, int rbp, int rsp, int rbx, int rdx, int rcx, int rax, int eip, char* msg) {
  403   // In order to get locks to work, we need to fake a in_VM state
  404   JavaThread* thread = JavaThread::current();
  405   JavaThreadState saved_state = thread-&gt;thread_state();
  406   thread-&gt;set_thread_state(_thread_in_vm);
  407   if (ShowMessageBoxOnError) {
  408     JavaThread* thread = JavaThread::current();
  409     JavaThreadState saved_state = thread-&gt;thread_state();
  410     thread-&gt;set_thread_state(_thread_in_vm);
  411     if (CountBytecodes || TraceBytecodes || StopInterpreterAt) {
  412       ttyLocker ttyl;
  413       BytecodeCounter::print();
  414     }
  415     // To see where a verify_oop failed, get $ebx+40/X for this frame.
  416     // This is the value of eip which points to where verify_oop will return.
  417     if (os::message_box(msg, &quot;Execution stopped, print registers?&quot;)) {
  418       print_state32(rdi, rsi, rbp, rsp, rbx, rdx, rcx, rax, eip);
  419       BREAKPOINT;
  420     }
  421   }
  422   fatal(&quot;DEBUG MESSAGE: %s&quot;, msg);
  423 }
  424 
  425 void MacroAssembler::print_state32(int rdi, int rsi, int rbp, int rsp, int rbx, int rdx, int rcx, int rax, int eip) {
  426   ttyLocker ttyl;
  427   FlagSetting fs(Debugging, true);
  428   tty-&gt;print_cr(&quot;eip = 0x%08x&quot;, eip);
  429 #ifndef PRODUCT
  430   if ((WizardMode || Verbose) &amp;&amp; PrintMiscellaneous) {
  431     tty-&gt;cr();
  432     findpc(eip);
  433     tty-&gt;cr();
  434   }
  435 #endif
  436 #define PRINT_REG(rax) \
  437   { tty-&gt;print(&quot;%s = &quot;, #rax); os::print_location(tty, rax); }
  438   PRINT_REG(rax);
  439   PRINT_REG(rbx);
  440   PRINT_REG(rcx);
  441   PRINT_REG(rdx);
  442   PRINT_REG(rdi);
  443   PRINT_REG(rsi);
  444   PRINT_REG(rbp);
  445   PRINT_REG(rsp);
  446 #undef PRINT_REG
  447   // Print some words near top of staack.
  448   int* dump_sp = (int*) rsp;
  449   for (int col1 = 0; col1 &lt; 8; col1++) {
  450     tty-&gt;print(&quot;(rsp+0x%03x) 0x%08x: &quot;, (int)((intptr_t)dump_sp - (intptr_t)rsp), (intptr_t)dump_sp);
  451     os::print_location(tty, *dump_sp++);
  452   }
  453   for (int row = 0; row &lt; 16; row++) {
  454     tty-&gt;print(&quot;(rsp+0x%03x) 0x%08x: &quot;, (int)((intptr_t)dump_sp - (intptr_t)rsp), (intptr_t)dump_sp);
  455     for (int col = 0; col &lt; 8; col++) {
  456       tty-&gt;print(&quot; 0x%08x&quot;, *dump_sp++);
  457     }
  458     tty-&gt;cr();
  459   }
  460   // Print some instructions around pc:
  461   Disassembler::decode((address)eip-64, (address)eip);
  462   tty-&gt;print_cr(&quot;--------&quot;);
  463   Disassembler::decode((address)eip, (address)eip+32);
  464 }
  465 
  466 void MacroAssembler::stop(const char* msg) {
  467   ExternalAddress message((address)msg);
  468   // push address of message
  469   pushptr(message.addr());
  470   { Label L; call(L, relocInfo::none); bind(L); }     // push eip
  471   pusha();                                            // push registers
  472   call(RuntimeAddress(CAST_FROM_FN_PTR(address, MacroAssembler::debug32)));
  473   hlt();
  474 }
  475 
  476 void MacroAssembler::warn(const char* msg) {
  477   push_CPU_state();
  478 
  479   ExternalAddress message((address) msg);
  480   // push address of message
  481   pushptr(message.addr());
  482 
  483   call(RuntimeAddress(CAST_FROM_FN_PTR(address, warning)));
  484   addl(rsp, wordSize);       // discard argument
  485   pop_CPU_state();
  486 }
  487 
  488 void MacroAssembler::print_state() {
  489   { Label L; call(L, relocInfo::none); bind(L); }     // push eip
  490   pusha();                                            // push registers
  491 
  492   push_CPU_state();
  493   call(RuntimeAddress(CAST_FROM_FN_PTR(address, MacroAssembler::print_state32)));
  494   pop_CPU_state();
  495 
  496   popa();
  497   addl(rsp, wordSize);
  498 }
  499 
  500 #else // _LP64
  501 
  502 // 64 bit versions
  503 
  504 Address MacroAssembler::as_Address(AddressLiteral adr) {
  505   // amd64 always does this as a pc-rel
  506   // we can be absolute or disp based on the instruction type
  507   // jmp/call are displacements others are absolute
  508   assert(!adr.is_lval(), &quot;must be rval&quot;);
  509   assert(reachable(adr), &quot;must be&quot;);
  510   return Address((int32_t)(intptr_t)(adr.target() - pc()), adr.target(), adr.reloc());
  511 
  512 }
  513 
  514 Address MacroAssembler::as_Address(ArrayAddress adr) {
  515   AddressLiteral base = adr.base();
  516   lea(rscratch1, base);
  517   Address index = adr.index();
  518   assert(index._disp == 0, &quot;must not have disp&quot;); // maybe it can?
  519   Address array(rscratch1, index._index, index._scale, index._disp);
  520   return array;
  521 }
  522 
  523 void MacroAssembler::call_VM_leaf_base(address entry_point, int num_args) {
  524   Label L, E;
  525 
  526 #ifdef _WIN64
  527   // Windows always allocates space for it&#39;s register args
  528   assert(num_args &lt;= 4, &quot;only register arguments supported&quot;);
  529   subq(rsp,  frame::arg_reg_save_area_bytes);
  530 #endif
  531 
  532   // Align stack if necessary
  533   testl(rsp, 15);
  534   jcc(Assembler::zero, L);
  535 
  536   subq(rsp, 8);
  537   {
  538     call(RuntimeAddress(entry_point));
  539   }
  540   addq(rsp, 8);
  541   jmp(E);
  542 
  543   bind(L);
  544   {
  545     call(RuntimeAddress(entry_point));
  546   }
  547 
  548   bind(E);
  549 
  550 #ifdef _WIN64
  551   // restore stack pointer
  552   addq(rsp, frame::arg_reg_save_area_bytes);
  553 #endif
  554 
  555 }
  556 
  557 void MacroAssembler::cmp64(Register src1, AddressLiteral src2) {
  558   assert(!src2.is_lval(), &quot;should use cmpptr&quot;);
  559 
  560   if (reachable(src2)) {
  561     cmpq(src1, as_Address(src2));
  562   } else {
  563     lea(rscratch1, src2);
  564     Assembler::cmpq(src1, Address(rscratch1, 0));
  565   }
  566 }
  567 
  568 int MacroAssembler::corrected_idivq(Register reg) {
  569   // Full implementation of Java ldiv and lrem; checks for special
  570   // case as described in JVM spec., p.243 &amp; p.271.  The function
  571   // returns the (pc) offset of the idivl instruction - may be needed
  572   // for implicit exceptions.
  573   //
  574   //         normal case                           special case
  575   //
  576   // input : rax: dividend                         min_long
  577   //         reg: divisor   (may not be eax/edx)   -1
  578   //
  579   // output: rax: quotient  (= rax idiv reg)       min_long
  580   //         rdx: remainder (= rax irem reg)       0
  581   assert(reg != rax &amp;&amp; reg != rdx, &quot;reg cannot be rax or rdx register&quot;);
  582   static const int64_t min_long = 0x8000000000000000;
  583   Label normal_case, special_case;
  584 
  585   // check for special case
  586   cmp64(rax, ExternalAddress((address) &amp;min_long));
  587   jcc(Assembler::notEqual, normal_case);
  588   xorl(rdx, rdx); // prepare rdx for possible special case (where
  589                   // remainder = 0)
  590   cmpq(reg, -1);
  591   jcc(Assembler::equal, special_case);
  592 
  593   // handle normal case
  594   bind(normal_case);
  595   cdqq();
  596   int idivq_offset = offset();
  597   idivq(reg);
  598 
  599   // normal and special case exit
  600   bind(special_case);
  601 
  602   return idivq_offset;
  603 }
  604 
  605 void MacroAssembler::decrementq(Register reg, int value) {
  606   if (value == min_jint) { subq(reg, value); return; }
  607   if (value &lt;  0) { incrementq(reg, -value); return; }
  608   if (value == 0) {                        ; return; }
  609   if (value == 1 &amp;&amp; UseIncDec) { decq(reg) ; return; }
  610   /* else */      { subq(reg, value)       ; return; }
  611 }
  612 
  613 void MacroAssembler::decrementq(Address dst, int value) {
  614   if (value == min_jint) { subq(dst, value); return; }
  615   if (value &lt;  0) { incrementq(dst, -value); return; }
  616   if (value == 0) {                        ; return; }
  617   if (value == 1 &amp;&amp; UseIncDec) { decq(dst) ; return; }
  618   /* else */      { subq(dst, value)       ; return; }
  619 }
  620 
  621 void MacroAssembler::incrementq(AddressLiteral dst) {
  622   if (reachable(dst)) {
  623     incrementq(as_Address(dst));
  624   } else {
  625     lea(rscratch1, dst);
  626     incrementq(Address(rscratch1, 0));
  627   }
  628 }
  629 
  630 void MacroAssembler::incrementq(Register reg, int value) {
  631   if (value == min_jint) { addq(reg, value); return; }
  632   if (value &lt;  0) { decrementq(reg, -value); return; }
  633   if (value == 0) {                        ; return; }
  634   if (value == 1 &amp;&amp; UseIncDec) { incq(reg) ; return; }
  635   /* else */      { addq(reg, value)       ; return; }
  636 }
  637 
  638 void MacroAssembler::incrementq(Address dst, int value) {
  639   if (value == min_jint) { addq(dst, value); return; }
  640   if (value &lt;  0) { decrementq(dst, -value); return; }
  641   if (value == 0) {                        ; return; }
  642   if (value == 1 &amp;&amp; UseIncDec) { incq(dst) ; return; }
  643   /* else */      { addq(dst, value)       ; return; }
  644 }
  645 
  646 // 32bit can do a case table jump in one instruction but we no longer allow the base
  647 // to be installed in the Address class
  648 void MacroAssembler::jump(ArrayAddress entry) {
  649   lea(rscratch1, entry.base());
  650   Address dispatch = entry.index();
  651   assert(dispatch._base == noreg, &quot;must be&quot;);
  652   dispatch._base = rscratch1;
  653   jmp(dispatch);
  654 }
  655 
  656 void MacroAssembler::lcmp2int(Register x_hi, Register x_lo, Register y_hi, Register y_lo) {
  657   ShouldNotReachHere(); // 64bit doesn&#39;t use two regs
  658   cmpq(x_lo, y_lo);
  659 }
  660 
  661 void MacroAssembler::lea(Register dst, AddressLiteral src) {
  662     mov_literal64(dst, (intptr_t)src.target(), src.rspec());
  663 }
  664 
  665 void MacroAssembler::lea(Address dst, AddressLiteral adr) {
  666   mov_literal64(rscratch1, (intptr_t)adr.target(), adr.rspec());
  667   movptr(dst, rscratch1);
  668 }
  669 
  670 void MacroAssembler::leave() {
  671   // %%% is this really better? Why not on 32bit too?
  672   emit_int8((unsigned char)0xC9); // LEAVE
  673 }
  674 
  675 void MacroAssembler::lneg(Register hi, Register lo) {
  676   ShouldNotReachHere(); // 64bit doesn&#39;t use two regs
  677   negq(lo);
  678 }
  679 
  680 void MacroAssembler::movoop(Register dst, jobject obj) {
  681   mov_literal64(dst, (intptr_t)obj, oop_Relocation::spec_for_immediate());
  682 }
  683 
  684 void MacroAssembler::movoop(Address dst, jobject obj) {
  685   mov_literal64(rscratch1, (intptr_t)obj, oop_Relocation::spec_for_immediate());
  686   movq(dst, rscratch1);
  687 }
  688 
  689 void MacroAssembler::mov_metadata(Register dst, Metadata* obj) {
  690   mov_literal64(dst, (intptr_t)obj, metadata_Relocation::spec_for_immediate());
  691 }
  692 
  693 void MacroAssembler::mov_metadata(Address dst, Metadata* obj) {
  694   mov_literal64(rscratch1, (intptr_t)obj, metadata_Relocation::spec_for_immediate());
  695   movq(dst, rscratch1);
  696 }
  697 
  698 void MacroAssembler::movptr(Register dst, AddressLiteral src, Register scratch) {
  699   if (src.is_lval()) {
  700     mov_literal64(dst, (intptr_t)src.target(), src.rspec());
  701   } else {
  702     if (reachable(src)) {
  703       movq(dst, as_Address(src));
  704     } else {
  705       lea(scratch, src);
  706       movq(dst, Address(scratch, 0));
  707     }
  708   }
  709 }
  710 
  711 void MacroAssembler::movptr(ArrayAddress dst, Register src) {
  712   movq(as_Address(dst), src);
  713 }
  714 
  715 void MacroAssembler::movptr(Register dst, ArrayAddress src) {
  716   movq(dst, as_Address(src));
  717 }
  718 
  719 // src should NEVER be a real pointer. Use AddressLiteral for true pointers
  720 void MacroAssembler::movptr(Address dst, intptr_t src) {
  721   mov64(rscratch1, src);
  722   movq(dst, rscratch1);
  723 }
  724 
  725 // These are mostly for initializing NULL
  726 void MacroAssembler::movptr(Address dst, int32_t src) {
  727   movslq(dst, src);
  728 }
  729 
  730 void MacroAssembler::movptr(Register dst, int32_t src) {
  731   mov64(dst, (intptr_t)src);
  732 }
  733 
  734 void MacroAssembler::pushoop(jobject obj) {
  735   movoop(rscratch1, obj);
  736   push(rscratch1);
  737 }
  738 
  739 void MacroAssembler::pushklass(Metadata* obj) {
  740   mov_metadata(rscratch1, obj);
  741   push(rscratch1);
  742 }
  743 
  744 void MacroAssembler::pushptr(AddressLiteral src) {
  745   lea(rscratch1, src);
  746   if (src.is_lval()) {
  747     push(rscratch1);
  748   } else {
  749     pushq(Address(rscratch1, 0));
  750   }
  751 }
  752 
  753 void MacroAssembler::reset_last_Java_frame(bool clear_fp) {
  754   // we must set sp to zero to clear frame
  755   movptr(Address(r15_thread, JavaThread::last_Java_sp_offset()), NULL_WORD);
  756   // must clear fp, so that compiled frames are not confused; it is
  757   // possible that we need it only for debugging
  758   if (clear_fp) {
  759     movptr(Address(r15_thread, JavaThread::last_Java_fp_offset()), NULL_WORD);
  760   }
  761 
  762   // Always clear the pc because it could have been set by make_walkable()
  763   movptr(Address(r15_thread, JavaThread::last_Java_pc_offset()), NULL_WORD);
  764   vzeroupper();
  765 }
  766 
  767 void MacroAssembler::set_last_Java_frame(Register last_java_sp,
  768                                          Register last_java_fp,
  769                                          address  last_java_pc) {
  770   vzeroupper();
  771   // determine last_java_sp register
  772   if (!last_java_sp-&gt;is_valid()) {
  773     last_java_sp = rsp;
  774   }
  775 
  776   // last_java_fp is optional
  777   if (last_java_fp-&gt;is_valid()) {
  778     movptr(Address(r15_thread, JavaThread::last_Java_fp_offset()),
  779            last_java_fp);
  780   }
  781 
  782   // last_java_pc is optional
  783   if (last_java_pc != NULL) {
  784     Address java_pc(r15_thread,
  785                     JavaThread::frame_anchor_offset() + JavaFrameAnchor::last_Java_pc_offset());
  786     lea(rscratch1, InternalAddress(last_java_pc));
  787     movptr(java_pc, rscratch1);
  788   }
  789 
  790   movptr(Address(r15_thread, JavaThread::last_Java_sp_offset()), last_java_sp);
  791 }
  792 
  793 static void pass_arg0(MacroAssembler* masm, Register arg) {
  794   if (c_rarg0 != arg ) {
  795     masm-&gt;mov(c_rarg0, arg);
  796   }
  797 }
  798 
  799 static void pass_arg1(MacroAssembler* masm, Register arg) {
  800   if (c_rarg1 != arg ) {
  801     masm-&gt;mov(c_rarg1, arg);
  802   }
  803 }
  804 
  805 static void pass_arg2(MacroAssembler* masm, Register arg) {
  806   if (c_rarg2 != arg ) {
  807     masm-&gt;mov(c_rarg2, arg);
  808   }
  809 }
  810 
  811 static void pass_arg3(MacroAssembler* masm, Register arg) {
  812   if (c_rarg3 != arg ) {
  813     masm-&gt;mov(c_rarg3, arg);
  814   }
  815 }
  816 
  817 void MacroAssembler::stop(const char* msg) {
  818   if (ShowMessageBoxOnError) {
  819     address rip = pc();
  820     pusha(); // get regs on stack
  821     lea(c_rarg1, InternalAddress(rip));
  822     movq(c_rarg2, rsp); // pass pointer to regs array
  823   }
  824   lea(c_rarg0, ExternalAddress((address) msg));
  825   andq(rsp, -16); // align stack as required by ABI
  826   call(RuntimeAddress(CAST_FROM_FN_PTR(address, MacroAssembler::debug64)));
  827   hlt();
  828 }
  829 
  830 void MacroAssembler::warn(const char* msg) {
  831   push(rbp);
  832   movq(rbp, rsp);
  833   andq(rsp, -16);     // align stack as required by push_CPU_state and call
  834   push_CPU_state();   // keeps alignment at 16 bytes
  835   lea(c_rarg0, ExternalAddress((address) msg));
  836   lea(rax, ExternalAddress(CAST_FROM_FN_PTR(address, warning)));
  837   call(rax);
  838   pop_CPU_state();
  839   mov(rsp, rbp);
  840   pop(rbp);
  841 }
  842 
  843 void MacroAssembler::print_state() {
  844   address rip = pc();
  845   pusha();            // get regs on stack
  846   push(rbp);
  847   movq(rbp, rsp);
  848   andq(rsp, -16);     // align stack as required by push_CPU_state and call
  849   push_CPU_state();   // keeps alignment at 16 bytes
  850 
  851   lea(c_rarg0, InternalAddress(rip));
  852   lea(c_rarg1, Address(rbp, wordSize)); // pass pointer to regs array
  853   call_VM_leaf(CAST_FROM_FN_PTR(address, MacroAssembler::print_state64), c_rarg0, c_rarg1);
  854 
  855   pop_CPU_state();
  856   mov(rsp, rbp);
  857   pop(rbp);
  858   popa();
  859 }
  860 
  861 #ifndef PRODUCT
  862 extern &quot;C&quot; void findpc(intptr_t x);
  863 #endif
  864 
  865 void MacroAssembler::debug64(char* msg, int64_t pc, int64_t regs[]) {
  866   // In order to get locks to work, we need to fake a in_VM state
  867   if (ShowMessageBoxOnError) {
  868     JavaThread* thread = JavaThread::current();
  869     JavaThreadState saved_state = thread-&gt;thread_state();
  870     thread-&gt;set_thread_state(_thread_in_vm);
  871 #ifndef PRODUCT
  872     if (CountBytecodes || TraceBytecodes || StopInterpreterAt) {
  873       ttyLocker ttyl;
  874       BytecodeCounter::print();
  875     }
  876 #endif
  877     // To see where a verify_oop failed, get $ebx+40/X for this frame.
  878     // XXX correct this offset for amd64
  879     // This is the value of eip which points to where verify_oop will return.
  880     if (os::message_box(msg, &quot;Execution stopped, print registers?&quot;)) {
  881       print_state64(pc, regs);
  882       BREAKPOINT;
  883     }
  884   }
  885   fatal(&quot;DEBUG MESSAGE: %s&quot;, msg);
  886 }
  887 
  888 void MacroAssembler::print_state64(int64_t pc, int64_t regs[]) {
  889   ttyLocker ttyl;
  890   FlagSetting fs(Debugging, true);
  891   tty-&gt;print_cr(&quot;rip = 0x%016lx&quot;, (intptr_t)pc);
  892 #ifndef PRODUCT
  893   tty-&gt;cr();
  894   findpc(pc);
  895   tty-&gt;cr();
  896 #endif
  897 #define PRINT_REG(rax, value) \
  898   { tty-&gt;print(&quot;%s = &quot;, #rax); os::print_location(tty, value); }
  899   PRINT_REG(rax, regs[15]);
  900   PRINT_REG(rbx, regs[12]);
  901   PRINT_REG(rcx, regs[14]);
  902   PRINT_REG(rdx, regs[13]);
  903   PRINT_REG(rdi, regs[8]);
  904   PRINT_REG(rsi, regs[9]);
  905   PRINT_REG(rbp, regs[10]);
  906   PRINT_REG(rsp, regs[11]);
  907   PRINT_REG(r8 , regs[7]);
  908   PRINT_REG(r9 , regs[6]);
  909   PRINT_REG(r10, regs[5]);
  910   PRINT_REG(r11, regs[4]);
  911   PRINT_REG(r12, regs[3]);
  912   PRINT_REG(r13, regs[2]);
  913   PRINT_REG(r14, regs[1]);
  914   PRINT_REG(r15, regs[0]);
  915 #undef PRINT_REG
  916   // Print some words near top of staack.
  917   int64_t* rsp = (int64_t*) regs[11];
  918   int64_t* dump_sp = rsp;
  919   for (int col1 = 0; col1 &lt; 8; col1++) {
  920     tty-&gt;print(&quot;(rsp+0x%03x) 0x%016lx: &quot;, (int)((intptr_t)dump_sp - (intptr_t)rsp), (intptr_t)dump_sp);
  921     os::print_location(tty, *dump_sp++);
  922   }
  923   for (int row = 0; row &lt; 25; row++) {
  924     tty-&gt;print(&quot;(rsp+0x%03x) 0x%016lx: &quot;, (int)((intptr_t)dump_sp - (intptr_t)rsp), (intptr_t)dump_sp);
  925     for (int col = 0; col &lt; 4; col++) {
  926       tty-&gt;print(&quot; 0x%016lx&quot;, (intptr_t)*dump_sp++);
  927     }
  928     tty-&gt;cr();
  929   }
  930   // Print some instructions around pc:
  931   Disassembler::decode((address)pc-64, (address)pc);
  932   tty-&gt;print_cr(&quot;--------&quot;);
  933   Disassembler::decode((address)pc, (address)pc+32);
  934 }
  935 
  936 #endif // _LP64
  937 
  938 // Now versions that are common to 32/64 bit
  939 
  940 void MacroAssembler::addptr(Register dst, int32_t imm32) {
  941   LP64_ONLY(addq(dst, imm32)) NOT_LP64(addl(dst, imm32));
  942 }
  943 
  944 void MacroAssembler::addptr(Register dst, Register src) {
  945   LP64_ONLY(addq(dst, src)) NOT_LP64(addl(dst, src));
  946 }
  947 
  948 void MacroAssembler::addptr(Address dst, Register src) {
  949   LP64_ONLY(addq(dst, src)) NOT_LP64(addl(dst, src));
  950 }
  951 
  952 void MacroAssembler::addsd(XMMRegister dst, AddressLiteral src) {
  953   if (reachable(src)) {
  954     Assembler::addsd(dst, as_Address(src));
  955   } else {
  956     lea(rscratch1, src);
  957     Assembler::addsd(dst, Address(rscratch1, 0));
  958   }
  959 }
  960 
  961 void MacroAssembler::addss(XMMRegister dst, AddressLiteral src) {
  962   if (reachable(src)) {
  963     addss(dst, as_Address(src));
  964   } else {
  965     lea(rscratch1, src);
  966     addss(dst, Address(rscratch1, 0));
  967   }
  968 }
  969 
  970 void MacroAssembler::addpd(XMMRegister dst, AddressLiteral src) {
  971   if (reachable(src)) {
  972     Assembler::addpd(dst, as_Address(src));
  973   } else {
  974     lea(rscratch1, src);
  975     Assembler::addpd(dst, Address(rscratch1, 0));
  976   }
  977 }
  978 
  979 void MacroAssembler::align(int modulus) {
  980   align(modulus, offset());
  981 }
  982 
  983 void MacroAssembler::align(int modulus, int target) {
  984   if (target % modulus != 0) {
  985     nop(modulus - (target % modulus));
  986   }
  987 }
  988 
  989 void MacroAssembler::andpd(XMMRegister dst, AddressLiteral src, Register scratch_reg) {
  990   // Used in sign-masking with aligned address.
  991   assert((UseAVX &gt; 0) || (((intptr_t)src.target() &amp; 15) == 0), &quot;SSE mode requires address alignment 16 bytes&quot;);
  992   if (reachable(src)) {
  993     Assembler::andpd(dst, as_Address(src));
  994   } else {
  995     lea(scratch_reg, src);
  996     Assembler::andpd(dst, Address(scratch_reg, 0));
  997   }
  998 }
  999 
 1000 void MacroAssembler::andps(XMMRegister dst, AddressLiteral src, Register scratch_reg) {
 1001   // Used in sign-masking with aligned address.
 1002   assert((UseAVX &gt; 0) || (((intptr_t)src.target() &amp; 15) == 0), &quot;SSE mode requires address alignment 16 bytes&quot;);
 1003   if (reachable(src)) {
 1004     Assembler::andps(dst, as_Address(src));
 1005   } else {
 1006     lea(scratch_reg, src);
 1007     Assembler::andps(dst, Address(scratch_reg, 0));
 1008   }
 1009 }
 1010 
 1011 void MacroAssembler::andptr(Register dst, int32_t imm32) {
 1012   LP64_ONLY(andq(dst, imm32)) NOT_LP64(andl(dst, imm32));
 1013 }
 1014 
 1015 void MacroAssembler::atomic_incl(Address counter_addr) {
 1016   lock();
 1017   incrementl(counter_addr);
 1018 }
 1019 
 1020 void MacroAssembler::atomic_incl(AddressLiteral counter_addr, Register scr) {
 1021   if (reachable(counter_addr)) {
 1022     atomic_incl(as_Address(counter_addr));
 1023   } else {
 1024     lea(scr, counter_addr);
 1025     atomic_incl(Address(scr, 0));
 1026   }
 1027 }
 1028 
 1029 #ifdef _LP64
 1030 void MacroAssembler::atomic_incq(Address counter_addr) {
 1031   lock();
 1032   incrementq(counter_addr);
 1033 }
 1034 
 1035 void MacroAssembler::atomic_incq(AddressLiteral counter_addr, Register scr) {
 1036   if (reachable(counter_addr)) {
 1037     atomic_incq(as_Address(counter_addr));
 1038   } else {
 1039     lea(scr, counter_addr);
 1040     atomic_incq(Address(scr, 0));
 1041   }
 1042 }
 1043 #endif
 1044 
 1045 // Writes to stack successive pages until offset reached to check for
 1046 // stack overflow + shadow pages.  This clobbers tmp.
 1047 void MacroAssembler::bang_stack_size(Register size, Register tmp) {
 1048   movptr(tmp, rsp);
 1049   // Bang stack for total size given plus shadow page size.
 1050   // Bang one page at a time because large size can bang beyond yellow and
 1051   // red zones.
 1052   Label loop;
 1053   bind(loop);
 1054   movl(Address(tmp, (-os::vm_page_size())), size );
 1055   subptr(tmp, os::vm_page_size());
 1056   subl(size, os::vm_page_size());
 1057   jcc(Assembler::greater, loop);
 1058 
 1059   // Bang down shadow pages too.
 1060   // At this point, (tmp-0) is the last address touched, so don&#39;t
 1061   // touch it again.  (It was touched as (tmp-pagesize) but then tmp
 1062   // was post-decremented.)  Skip this address by starting at i=1, and
 1063   // touch a few more pages below.  N.B.  It is important to touch all
 1064   // the way down including all pages in the shadow zone.
 1065   for (int i = 1; i &lt; ((int)JavaThread::stack_shadow_zone_size() / os::vm_page_size()); i++) {
 1066     // this could be any sized move but this is can be a debugging crumb
 1067     // so the bigger the better.
 1068     movptr(Address(tmp, (-i*os::vm_page_size())), size );
 1069   }
 1070 }
 1071 
 1072 void MacroAssembler::reserved_stack_check() {
 1073     // testing if reserved zone needs to be enabled
 1074     Label no_reserved_zone_enabling;
 1075     Register thread = NOT_LP64(rsi) LP64_ONLY(r15_thread);
 1076     NOT_LP64(get_thread(rsi);)
 1077 
 1078     cmpptr(rsp, Address(thread, JavaThread::reserved_stack_activation_offset()));
 1079     jcc(Assembler::below, no_reserved_zone_enabling);
 1080 
 1081     call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::enable_stack_reserved_zone), thread);
 1082     jump(RuntimeAddress(StubRoutines::throw_delayed_StackOverflowError_entry()));
 1083     should_not_reach_here();
 1084 
 1085     bind(no_reserved_zone_enabling);
 1086 }
 1087 
 1088 int MacroAssembler::biased_locking_enter(Register lock_reg,
 1089                                          Register obj_reg,
 1090                                          Register swap_reg,
 1091                                          Register tmp_reg,
 1092                                          bool swap_reg_contains_mark,
 1093                                          Label&amp; done,
 1094                                          Label* slow_case,
 1095                                          BiasedLockingCounters* counters) {
 1096   assert(UseBiasedLocking, &quot;why call this otherwise?&quot;);
 1097   assert(swap_reg == rax, &quot;swap_reg must be rax for cmpxchgq&quot;);
 1098   assert(tmp_reg != noreg, &quot;tmp_reg must be supplied&quot;);
 1099   assert_different_registers(lock_reg, obj_reg, swap_reg, tmp_reg);
 1100   assert(markWord::age_shift == markWord::lock_bits + markWord::biased_lock_bits, &quot;biased locking makes assumptions about bit layout&quot;);
 1101   Address mark_addr      (obj_reg, oopDesc::mark_offset_in_bytes());
 1102   NOT_LP64( Address saved_mark_addr(lock_reg, 0); )
 1103 
 1104   if (PrintBiasedLockingStatistics &amp;&amp; counters == NULL) {
 1105     counters = BiasedLocking::counters();
 1106   }
 1107   // Biased locking
 1108   // See whether the lock is currently biased toward our thread and
 1109   // whether the epoch is still valid
 1110   // Note that the runtime guarantees sufficient alignment of JavaThread
 1111   // pointers to allow age to be placed into low bits
 1112   // First check to see whether biasing is even enabled for this object
 1113   Label cas_label;
 1114   int null_check_offset = -1;
 1115   if (!swap_reg_contains_mark) {
 1116     null_check_offset = offset();
 1117     movptr(swap_reg, mark_addr);
 1118   }
 1119   movptr(tmp_reg, swap_reg);
 1120   andptr(tmp_reg, markWord::biased_lock_mask_in_place);
 1121   cmpptr(tmp_reg, markWord::biased_lock_pattern);
 1122   jcc(Assembler::notEqual, cas_label);
 1123   // The bias pattern is present in the object&#39;s header. Need to check
 1124   // whether the bias owner and the epoch are both still current.
 1125 #ifndef _LP64
 1126   // Note that because there is no current thread register on x86_32 we
 1127   // need to store off the mark word we read out of the object to
 1128   // avoid reloading it and needing to recheck invariants below. This
 1129   // store is unfortunate but it makes the overall code shorter and
 1130   // simpler.
 1131   movptr(saved_mark_addr, swap_reg);
 1132 #endif
 1133   if (swap_reg_contains_mark) {
 1134     null_check_offset = offset();
 1135   }
 1136   load_prototype_header(tmp_reg, obj_reg);
 1137 #ifdef _LP64
 1138   orptr(tmp_reg, r15_thread);
 1139   xorptr(tmp_reg, swap_reg);
 1140   Register header_reg = tmp_reg;
 1141 #else
 1142   xorptr(tmp_reg, swap_reg);
 1143   get_thread(swap_reg);
 1144   xorptr(swap_reg, tmp_reg);
 1145   Register header_reg = swap_reg;
 1146 #endif
 1147   andptr(header_reg, ~((int) markWord::age_mask_in_place));
 1148   if (counters != NULL) {
 1149     cond_inc32(Assembler::zero,
 1150                ExternalAddress((address) counters-&gt;biased_lock_entry_count_addr()));
 1151   }
 1152   jcc(Assembler::equal, done);
 1153 
 1154   Label try_revoke_bias;
 1155   Label try_rebias;
 1156 
 1157   // At this point we know that the header has the bias pattern and
 1158   // that we are not the bias owner in the current epoch. We need to
 1159   // figure out more details about the state of the header in order to
 1160   // know what operations can be legally performed on the object&#39;s
 1161   // header.
 1162 
 1163   // If the low three bits in the xor result aren&#39;t clear, that means
 1164   // the prototype header is no longer biased and we have to revoke
 1165   // the bias on this object.
 1166   testptr(header_reg, markWord::biased_lock_mask_in_place);
 1167   jccb(Assembler::notZero, try_revoke_bias);
 1168 
 1169   // Biasing is still enabled for this data type. See whether the
 1170   // epoch of the current bias is still valid, meaning that the epoch
 1171   // bits of the mark word are equal to the epoch bits of the
 1172   // prototype header. (Note that the prototype header&#39;s epoch bits
 1173   // only change at a safepoint.) If not, attempt to rebias the object
 1174   // toward the current thread. Note that we must be absolutely sure
 1175   // that the current epoch is invalid in order to do this because
 1176   // otherwise the manipulations it performs on the mark word are
 1177   // illegal.
 1178   testptr(header_reg, markWord::epoch_mask_in_place);
 1179   jccb(Assembler::notZero, try_rebias);
 1180 
 1181   // The epoch of the current bias is still valid but we know nothing
 1182   // about the owner; it might be set or it might be clear. Try to
 1183   // acquire the bias of the object using an atomic operation. If this
 1184   // fails we will go in to the runtime to revoke the object&#39;s bias.
 1185   // Note that we first construct the presumed unbiased header so we
 1186   // don&#39;t accidentally blow away another thread&#39;s valid bias.
 1187   NOT_LP64( movptr(swap_reg, saved_mark_addr); )
 1188   andptr(swap_reg,
 1189          markWord::biased_lock_mask_in_place | markWord::age_mask_in_place | markWord::epoch_mask_in_place);
 1190 #ifdef _LP64
 1191   movptr(tmp_reg, swap_reg);
 1192   orptr(tmp_reg, r15_thread);
 1193 #else
 1194   get_thread(tmp_reg);
 1195   orptr(tmp_reg, swap_reg);
 1196 #endif
 1197   lock();
 1198   cmpxchgptr(tmp_reg, mark_addr); // compare tmp_reg and swap_reg
 1199   // If the biasing toward our thread failed, this means that
 1200   // another thread succeeded in biasing it toward itself and we
 1201   // need to revoke that bias. The revocation will occur in the
 1202   // interpreter runtime in the slow case.
 1203   if (counters != NULL) {
 1204     cond_inc32(Assembler::zero,
 1205                ExternalAddress((address) counters-&gt;anonymously_biased_lock_entry_count_addr()));
 1206   }
 1207   if (slow_case != NULL) {
 1208     jcc(Assembler::notZero, *slow_case);
 1209   }
 1210   jmp(done);
 1211 
 1212   bind(try_rebias);
 1213   // At this point we know the epoch has expired, meaning that the
 1214   // current &quot;bias owner&quot;, if any, is actually invalid. Under these
 1215   // circumstances _only_, we are allowed to use the current header&#39;s
 1216   // value as the comparison value when doing the cas to acquire the
 1217   // bias in the current epoch. In other words, we allow transfer of
 1218   // the bias from one thread to another directly in this situation.
 1219   //
 1220   // FIXME: due to a lack of registers we currently blow away the age
 1221   // bits in this situation. Should attempt to preserve them.
 1222   load_prototype_header(tmp_reg, obj_reg);
 1223 #ifdef _LP64
 1224   orptr(tmp_reg, r15_thread);
 1225 #else
 1226   get_thread(swap_reg);
 1227   orptr(tmp_reg, swap_reg);
 1228   movptr(swap_reg, saved_mark_addr);
 1229 #endif
 1230   lock();
 1231   cmpxchgptr(tmp_reg, mark_addr); // compare tmp_reg and swap_reg
 1232   // If the biasing toward our thread failed, then another thread
 1233   // succeeded in biasing it toward itself and we need to revoke that
 1234   // bias. The revocation will occur in the runtime in the slow case.
 1235   if (counters != NULL) {
 1236     cond_inc32(Assembler::zero,
 1237                ExternalAddress((address) counters-&gt;rebiased_lock_entry_count_addr()));
 1238   }
 1239   if (slow_case != NULL) {
 1240     jcc(Assembler::notZero, *slow_case);
 1241   }
 1242   jmp(done);
 1243 
 1244   bind(try_revoke_bias);
 1245   // The prototype mark in the klass doesn&#39;t have the bias bit set any
 1246   // more, indicating that objects of this data type are not supposed
 1247   // to be biased any more. We are going to try to reset the mark of
 1248   // this object to the prototype value and fall through to the
 1249   // CAS-based locking scheme. Note that if our CAS fails, it means
 1250   // that another thread raced us for the privilege of revoking the
 1251   // bias of this particular object, so it&#39;s okay to continue in the
 1252   // normal locking code.
 1253   //
 1254   // FIXME: due to a lack of registers we currently blow away the age
 1255   // bits in this situation. Should attempt to preserve them.
 1256   NOT_LP64( movptr(swap_reg, saved_mark_addr); )
 1257   load_prototype_header(tmp_reg, obj_reg);
 1258   lock();
 1259   cmpxchgptr(tmp_reg, mark_addr); // compare tmp_reg and swap_reg
 1260   // Fall through to the normal CAS-based lock, because no matter what
 1261   // the result of the above CAS, some thread must have succeeded in
 1262   // removing the bias bit from the object&#39;s header.
 1263   if (counters != NULL) {
 1264     cond_inc32(Assembler::zero,
 1265                ExternalAddress((address) counters-&gt;revoked_lock_entry_count_addr()));
 1266   }
 1267 
 1268   bind(cas_label);
 1269 
 1270   return null_check_offset;
 1271 }
 1272 
 1273 void MacroAssembler::biased_locking_exit(Register obj_reg, Register temp_reg, Label&amp; done) {
 1274   assert(UseBiasedLocking, &quot;why call this otherwise?&quot;);
 1275 
 1276   // Check for biased locking unlock case, which is a no-op
 1277   // Note: we do not have to check the thread ID for two reasons.
 1278   // First, the interpreter checks for IllegalMonitorStateException at
 1279   // a higher level. Second, if the bias was revoked while we held the
 1280   // lock, the object could not be rebiased toward another thread, so
 1281   // the bias bit would be clear.
 1282   movptr(temp_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));
 1283   andptr(temp_reg, markWord::biased_lock_mask_in_place);
 1284   cmpptr(temp_reg, markWord::biased_lock_pattern);
 1285   jcc(Assembler::equal, done);
 1286 }
 1287 
 1288 #ifdef COMPILER2
 1289 
 1290 #if INCLUDE_RTM_OPT
 1291 
 1292 // Update rtm_counters based on abort status
 1293 // input: abort_status
 1294 //        rtm_counters (RTMLockingCounters*)
 1295 // flags are killed
 1296 void MacroAssembler::rtm_counters_update(Register abort_status, Register rtm_counters) {
 1297 
 1298   atomic_incptr(Address(rtm_counters, RTMLockingCounters::abort_count_offset()));
 1299   if (PrintPreciseRTMLockingStatistics) {
 1300     for (int i = 0; i &lt; RTMLockingCounters::ABORT_STATUS_LIMIT; i++) {
 1301       Label check_abort;
 1302       testl(abort_status, (1&lt;&lt;i));
 1303       jccb(Assembler::equal, check_abort);
 1304       atomic_incptr(Address(rtm_counters, RTMLockingCounters::abortX_count_offset() + (i * sizeof(uintx))));
 1305       bind(check_abort);
 1306     }
 1307   }
 1308 }
 1309 
 1310 // Branch if (random &amp; (count-1) != 0), count is 2^n
 1311 // tmp, scr and flags are killed
 1312 void MacroAssembler::branch_on_random_using_rdtsc(Register tmp, Register scr, int count, Label&amp; brLabel) {
 1313   assert(tmp == rax, &quot;&quot;);
 1314   assert(scr == rdx, &quot;&quot;);
 1315   rdtsc(); // modifies EDX:EAX
 1316   andptr(tmp, count-1);
 1317   jccb(Assembler::notZero, brLabel);
 1318 }
 1319 
 1320 // Perform abort ratio calculation, set no_rtm bit if high ratio
 1321 // input:  rtm_counters_Reg (RTMLockingCounters* address)
 1322 // tmpReg, rtm_counters_Reg and flags are killed
 1323 void MacroAssembler::rtm_abort_ratio_calculation(Register tmpReg,
 1324                                                  Register rtm_counters_Reg,
 1325                                                  RTMLockingCounters* rtm_counters,
 1326                                                  Metadata* method_data) {
 1327   Label L_done, L_check_always_rtm1, L_check_always_rtm2;
 1328 
 1329   if (RTMLockingCalculationDelay &gt; 0) {
 1330     // Delay calculation
 1331     movptr(tmpReg, ExternalAddress((address) RTMLockingCounters::rtm_calculation_flag_addr()), tmpReg);
 1332     testptr(tmpReg, tmpReg);
 1333     jccb(Assembler::equal, L_done);
 1334   }
 1335   // Abort ratio calculation only if abort_count &gt; RTMAbortThreshold
 1336   //   Aborted transactions = abort_count * 100
 1337   //   All transactions = total_count *  RTMTotalCountIncrRate
 1338   //   Set no_rtm bit if (Aborted transactions &gt;= All transactions * RTMAbortRatio)
 1339 
 1340   movptr(tmpReg, Address(rtm_counters_Reg, RTMLockingCounters::abort_count_offset()));
 1341   cmpptr(tmpReg, RTMAbortThreshold);
 1342   jccb(Assembler::below, L_check_always_rtm2);
 1343   imulptr(tmpReg, tmpReg, 100);
 1344 
 1345   Register scrReg = rtm_counters_Reg;
 1346   movptr(scrReg, Address(rtm_counters_Reg, RTMLockingCounters::total_count_offset()));
 1347   imulptr(scrReg, scrReg, RTMTotalCountIncrRate);
 1348   imulptr(scrReg, scrReg, RTMAbortRatio);
 1349   cmpptr(tmpReg, scrReg);
 1350   jccb(Assembler::below, L_check_always_rtm1);
 1351   if (method_data != NULL) {
 1352     // set rtm_state to &quot;no rtm&quot; in MDO
 1353     mov_metadata(tmpReg, method_data);
 1354     lock();
 1355     orl(Address(tmpReg, MethodData::rtm_state_offset_in_bytes()), NoRTM);
 1356   }
 1357   jmpb(L_done);
 1358   bind(L_check_always_rtm1);
 1359   // Reload RTMLockingCounters* address
 1360   lea(rtm_counters_Reg, ExternalAddress((address)rtm_counters));
 1361   bind(L_check_always_rtm2);
 1362   movptr(tmpReg, Address(rtm_counters_Reg, RTMLockingCounters::total_count_offset()));
 1363   cmpptr(tmpReg, RTMLockingThreshold / RTMTotalCountIncrRate);
 1364   jccb(Assembler::below, L_done);
 1365   if (method_data != NULL) {
 1366     // set rtm_state to &quot;always rtm&quot; in MDO
 1367     mov_metadata(tmpReg, method_data);
 1368     lock();
 1369     orl(Address(tmpReg, MethodData::rtm_state_offset_in_bytes()), UseRTM);
 1370   }
 1371   bind(L_done);
 1372 }
 1373 
 1374 // Update counters and perform abort ratio calculation
 1375 // input:  abort_status_Reg
 1376 // rtm_counters_Reg, flags are killed
 1377 void MacroAssembler::rtm_profiling(Register abort_status_Reg,
 1378                                    Register rtm_counters_Reg,
 1379                                    RTMLockingCounters* rtm_counters,
 1380                                    Metadata* method_data,
 1381                                    bool profile_rtm) {
 1382 
 1383   assert(rtm_counters != NULL, &quot;should not be NULL when profiling RTM&quot;);
 1384   // update rtm counters based on rax value at abort
 1385   // reads abort_status_Reg, updates flags
 1386   lea(rtm_counters_Reg, ExternalAddress((address)rtm_counters));
 1387   rtm_counters_update(abort_status_Reg, rtm_counters_Reg);
 1388   if (profile_rtm) {
 1389     // Save abort status because abort_status_Reg is used by following code.
 1390     if (RTMRetryCount &gt; 0) {
 1391       push(abort_status_Reg);
 1392     }
 1393     assert(rtm_counters != NULL, &quot;should not be NULL when profiling RTM&quot;);
 1394     rtm_abort_ratio_calculation(abort_status_Reg, rtm_counters_Reg, rtm_counters, method_data);
 1395     // restore abort status
 1396     if (RTMRetryCount &gt; 0) {
 1397       pop(abort_status_Reg);
 1398     }
 1399   }
 1400 }
 1401 
 1402 // Retry on abort if abort&#39;s status is 0x6: can retry (0x2) | memory conflict (0x4)
 1403 // inputs: retry_count_Reg
 1404 //       : abort_status_Reg
 1405 // output: retry_count_Reg decremented by 1
 1406 // flags are killed
 1407 void MacroAssembler::rtm_retry_lock_on_abort(Register retry_count_Reg, Register abort_status_Reg, Label&amp; retryLabel) {
 1408   Label doneRetry;
 1409   assert(abort_status_Reg == rax, &quot;&quot;);
 1410   // The abort reason bits are in eax (see all states in rtmLocking.hpp)
 1411   // 0x6 = conflict on which we can retry (0x2) | memory conflict (0x4)
 1412   // if reason is in 0x6 and retry count != 0 then retry
 1413   andptr(abort_status_Reg, 0x6);
 1414   jccb(Assembler::zero, doneRetry);
 1415   testl(retry_count_Reg, retry_count_Reg);
 1416   jccb(Assembler::zero, doneRetry);
 1417   pause();
 1418   decrementl(retry_count_Reg);
 1419   jmp(retryLabel);
 1420   bind(doneRetry);
 1421 }
 1422 
 1423 // Spin and retry if lock is busy,
 1424 // inputs: box_Reg (monitor address)
 1425 //       : retry_count_Reg
 1426 // output: retry_count_Reg decremented by 1
 1427 //       : clear z flag if retry count exceeded
 1428 // tmp_Reg, scr_Reg, flags are killed
 1429 void MacroAssembler::rtm_retry_lock_on_busy(Register retry_count_Reg, Register box_Reg,
 1430                                             Register tmp_Reg, Register scr_Reg, Label&amp; retryLabel) {
 1431   Label SpinLoop, SpinExit, doneRetry;
 1432   int owner_offset = OM_OFFSET_NO_MONITOR_VALUE_TAG(owner);
 1433 
 1434   testl(retry_count_Reg, retry_count_Reg);
 1435   jccb(Assembler::zero, doneRetry);
 1436   decrementl(retry_count_Reg);
 1437   movptr(scr_Reg, RTMSpinLoopCount);
 1438 
 1439   bind(SpinLoop);
 1440   pause();
 1441   decrementl(scr_Reg);
 1442   jccb(Assembler::lessEqual, SpinExit);
 1443   movptr(tmp_Reg, Address(box_Reg, owner_offset));
 1444   testptr(tmp_Reg, tmp_Reg);
 1445   jccb(Assembler::notZero, SpinLoop);
 1446 
 1447   bind(SpinExit);
 1448   jmp(retryLabel);
 1449   bind(doneRetry);
 1450   incrementl(retry_count_Reg); // clear z flag
 1451 }
 1452 
 1453 // Use RTM for normal stack locks
 1454 // Input: objReg (object to lock)
 1455 void MacroAssembler::rtm_stack_locking(Register objReg, Register tmpReg, Register scrReg,
 1456                                        Register retry_on_abort_count_Reg,
 1457                                        RTMLockingCounters* stack_rtm_counters,
 1458                                        Metadata* method_data, bool profile_rtm,
 1459                                        Label&amp; DONE_LABEL, Label&amp; IsInflated) {
 1460   assert(UseRTMForStackLocks, &quot;why call this otherwise?&quot;);
 1461   assert(!UseBiasedLocking, &quot;Biased locking is not supported with RTM locking&quot;);
 1462   assert(tmpReg == rax, &quot;&quot;);
 1463   assert(scrReg == rdx, &quot;&quot;);
 1464   Label L_rtm_retry, L_decrement_retry, L_on_abort;
 1465 
 1466   if (RTMRetryCount &gt; 0) {
 1467     movl(retry_on_abort_count_Reg, RTMRetryCount); // Retry on abort
 1468     bind(L_rtm_retry);
 1469   }
 1470   movptr(tmpReg, Address(objReg, oopDesc::mark_offset_in_bytes()));
 1471   testptr(tmpReg, markWord::monitor_value);  // inflated vs stack-locked|neutral|biased
 1472   jcc(Assembler::notZero, IsInflated);
 1473 
 1474   if (PrintPreciseRTMLockingStatistics || profile_rtm) {
 1475     Label L_noincrement;
 1476     if (RTMTotalCountIncrRate &gt; 1) {
 1477       // tmpReg, scrReg and flags are killed
 1478       branch_on_random_using_rdtsc(tmpReg, scrReg, RTMTotalCountIncrRate, L_noincrement);
 1479     }
 1480     assert(stack_rtm_counters != NULL, &quot;should not be NULL when profiling RTM&quot;);
 1481     atomic_incptr(ExternalAddress((address)stack_rtm_counters-&gt;total_count_addr()), scrReg);
 1482     bind(L_noincrement);
 1483   }
 1484   xbegin(L_on_abort);
 1485   movptr(tmpReg, Address(objReg, oopDesc::mark_offset_in_bytes()));       // fetch markword
 1486   andptr(tmpReg, markWord::biased_lock_mask_in_place); // look at 3 lock bits
 1487   cmpptr(tmpReg, markWord::unlocked_value);            // bits = 001 unlocked
 1488   jcc(Assembler::equal, DONE_LABEL);        // all done if unlocked
 1489 
 1490   Register abort_status_Reg = tmpReg; // status of abort is stored in RAX
 1491   if (UseRTMXendForLockBusy) {
 1492     xend();
 1493     movptr(abort_status_Reg, 0x2);   // Set the abort status to 2 (so we can retry)
 1494     jmp(L_decrement_retry);
 1495   }
 1496   else {
 1497     xabort(0);
 1498   }
 1499   bind(L_on_abort);
 1500   if (PrintPreciseRTMLockingStatistics || profile_rtm) {
 1501     rtm_profiling(abort_status_Reg, scrReg, stack_rtm_counters, method_data, profile_rtm);
 1502   }
 1503   bind(L_decrement_retry);
 1504   if (RTMRetryCount &gt; 0) {
 1505     // retry on lock abort if abort status is &#39;can retry&#39; (0x2) or &#39;memory conflict&#39; (0x4)
 1506     rtm_retry_lock_on_abort(retry_on_abort_count_Reg, abort_status_Reg, L_rtm_retry);
 1507   }
 1508 }
 1509 
 1510 // Use RTM for inflating locks
 1511 // inputs: objReg (object to lock)
 1512 //         boxReg (on-stack box address (displaced header location) - KILLED)
 1513 //         tmpReg (ObjectMonitor address + markWord::monitor_value)
 1514 void MacroAssembler::rtm_inflated_locking(Register objReg, Register boxReg, Register tmpReg,
 1515                                           Register scrReg, Register retry_on_busy_count_Reg,
 1516                                           Register retry_on_abort_count_Reg,
 1517                                           RTMLockingCounters* rtm_counters,
 1518                                           Metadata* method_data, bool profile_rtm,
 1519                                           Label&amp; DONE_LABEL) {
 1520   assert(UseRTMLocking, &quot;why call this otherwise?&quot;);
 1521   assert(tmpReg == rax, &quot;&quot;);
 1522   assert(scrReg == rdx, &quot;&quot;);
 1523   Label L_rtm_retry, L_decrement_retry, L_on_abort;
 1524   int owner_offset = OM_OFFSET_NO_MONITOR_VALUE_TAG(owner);
 1525 
 1526   // Without cast to int32_t this style of movptr will destroy r10 which is typically obj.
 1527   movptr(Address(boxReg, 0), (int32_t)intptr_t(markWord::unused_mark().value()));
 1528   movptr(boxReg, tmpReg); // Save ObjectMonitor address
 1529 
 1530   if (RTMRetryCount &gt; 0) {
 1531     movl(retry_on_busy_count_Reg, RTMRetryCount);  // Retry on lock busy
 1532     movl(retry_on_abort_count_Reg, RTMRetryCount); // Retry on abort
 1533     bind(L_rtm_retry);
 1534   }
 1535   if (PrintPreciseRTMLockingStatistics || profile_rtm) {
 1536     Label L_noincrement;
 1537     if (RTMTotalCountIncrRate &gt; 1) {
 1538       // tmpReg, scrReg and flags are killed
 1539       branch_on_random_using_rdtsc(tmpReg, scrReg, RTMTotalCountIncrRate, L_noincrement);
 1540     }
 1541     assert(rtm_counters != NULL, &quot;should not be NULL when profiling RTM&quot;);
 1542     atomic_incptr(ExternalAddress((address)rtm_counters-&gt;total_count_addr()), scrReg);
 1543     bind(L_noincrement);
 1544   }
 1545   xbegin(L_on_abort);
 1546   movptr(tmpReg, Address(objReg, oopDesc::mark_offset_in_bytes()));
 1547   movptr(tmpReg, Address(tmpReg, owner_offset));
 1548   testptr(tmpReg, tmpReg);
 1549   jcc(Assembler::zero, DONE_LABEL);
 1550   if (UseRTMXendForLockBusy) {
 1551     xend();
 1552     jmp(L_decrement_retry);
 1553   }
 1554   else {
 1555     xabort(0);
 1556   }
 1557   bind(L_on_abort);
 1558   Register abort_status_Reg = tmpReg; // status of abort is stored in RAX
 1559   if (PrintPreciseRTMLockingStatistics || profile_rtm) {
 1560     rtm_profiling(abort_status_Reg, scrReg, rtm_counters, method_data, profile_rtm);
 1561   }
 1562   if (RTMRetryCount &gt; 0) {
 1563     // retry on lock abort if abort status is &#39;can retry&#39; (0x2) or &#39;memory conflict&#39; (0x4)
 1564     rtm_retry_lock_on_abort(retry_on_abort_count_Reg, abort_status_Reg, L_rtm_retry);
 1565   }
 1566 
 1567   movptr(tmpReg, Address(boxReg, owner_offset)) ;
 1568   testptr(tmpReg, tmpReg) ;
 1569   jccb(Assembler::notZero, L_decrement_retry) ;
 1570 
 1571   // Appears unlocked - try to swing _owner from null to non-null.
 1572   // Invariant: tmpReg == 0.  tmpReg is EAX which is the implicit cmpxchg comparand.
 1573 #ifdef _LP64
 1574   Register threadReg = r15_thread;
 1575 #else
 1576   get_thread(scrReg);
 1577   Register threadReg = scrReg;
 1578 #endif
 1579   lock();
 1580   cmpxchgptr(threadReg, Address(boxReg, owner_offset)); // Updates tmpReg
 1581 
 1582   if (RTMRetryCount &gt; 0) {
 1583     // success done else retry
 1584     jccb(Assembler::equal, DONE_LABEL) ;
 1585     bind(L_decrement_retry);
 1586     // Spin and retry if lock is busy.
 1587     rtm_retry_lock_on_busy(retry_on_busy_count_Reg, boxReg, tmpReg, scrReg, L_rtm_retry);
 1588   }
 1589   else {
 1590     bind(L_decrement_retry);
 1591   }
 1592 }
 1593 
 1594 #endif //  INCLUDE_RTM_OPT
 1595 
 1596 // fast_lock and fast_unlock used by C2
 1597 
 1598 // Because the transitions from emitted code to the runtime
 1599 // monitorenter/exit helper stubs are so slow it&#39;s critical that
 1600 // we inline both the stack-locking fast path and the inflated fast path.
 1601 //
 1602 // See also: cmpFastLock and cmpFastUnlock.
 1603 //
 1604 // What follows is a specialized inline transliteration of the code
 1605 // in enter() and exit(). If we&#39;re concerned about I$ bloat another
 1606 // option would be to emit TrySlowEnter and TrySlowExit methods
 1607 // at startup-time.  These methods would accept arguments as
 1608 // (rax,=Obj, rbx=Self, rcx=box, rdx=Scratch) and return success-failure
 1609 // indications in the icc.ZFlag.  fast_lock and fast_unlock would simply
 1610 // marshal the arguments and emit calls to TrySlowEnter and TrySlowExit.
 1611 // In practice, however, the # of lock sites is bounded and is usually small.
 1612 // Besides the call overhead, TrySlowEnter and TrySlowExit might suffer
 1613 // if the processor uses simple bimodal branch predictors keyed by EIP
 1614 // Since the helper routines would be called from multiple synchronization
 1615 // sites.
 1616 //
 1617 // An even better approach would be write &quot;MonitorEnter()&quot; and &quot;MonitorExit()&quot;
 1618 // in java - using j.u.c and unsafe - and just bind the lock and unlock sites
 1619 // to those specialized methods.  That&#39;d give us a mostly platform-independent
 1620 // implementation that the JITs could optimize and inline at their pleasure.
 1621 // Done correctly, the only time we&#39;d need to cross to native could would be
 1622 // to park() or unpark() threads.  We&#39;d also need a few more unsafe operators
 1623 // to (a) prevent compiler-JIT reordering of non-volatile accesses, and
 1624 // (b) explicit barriers or fence operations.
 1625 //
 1626 // TODO:
 1627 //
 1628 // *  Arrange for C2 to pass &quot;Self&quot; into fast_lock and fast_unlock in one of the registers (scr).
 1629 //    This avoids manifesting the Self pointer in the fast_lock and fast_unlock terminals.
 1630 //    Given TLAB allocation, Self is usually manifested in a register, so passing it into
 1631 //    the lock operators would typically be faster than reifying Self.
 1632 //
 1633 // *  Ideally I&#39;d define the primitives as:
 1634 //       fast_lock   (nax Obj, nax box, EAX tmp, nax scr) where box, tmp and scr are KILLED.
 1635 //       fast_unlock (nax Obj, EAX box, nax tmp) where box and tmp are KILLED
 1636 //    Unfortunately ADLC bugs prevent us from expressing the ideal form.
 1637 //    Instead, we&#39;re stuck with a rather awkward and brittle register assignments below.
 1638 //    Furthermore the register assignments are overconstrained, possibly resulting in
 1639 //    sub-optimal code near the synchronization site.
 1640 //
 1641 // *  Eliminate the sp-proximity tests and just use &quot;== Self&quot; tests instead.
 1642 //    Alternately, use a better sp-proximity test.
 1643 //
 1644 // *  Currently ObjectMonitor._Owner can hold either an sp value or a (THREAD *) value.
 1645 //    Either one is sufficient to uniquely identify a thread.
 1646 //    TODO: eliminate use of sp in _owner and use get_thread(tr) instead.
 1647 //
 1648 // *  Intrinsify notify() and notifyAll() for the common cases where the
 1649 //    object is locked by the calling thread but the waitlist is empty.
 1650 //    avoid the expensive JNI call to JVM_Notify() and JVM_NotifyAll().
 1651 //
 1652 // *  use jccb and jmpb instead of jcc and jmp to improve code density.
 1653 //    But beware of excessive branch density on AMD Opterons.
 1654 //
 1655 // *  Both fast_lock and fast_unlock set the ICC.ZF to indicate success
 1656 //    or failure of the fast path.  If the fast path fails then we pass
 1657 //    control to the slow path, typically in C.  In fast_lock and
 1658 //    fast_unlock we often branch to DONE_LABEL, just to find that C2
 1659 //    will emit a conditional branch immediately after the node.
 1660 //    So we have branches to branches and lots of ICC.ZF games.
 1661 //    Instead, it might be better to have C2 pass a &quot;FailureLabel&quot;
 1662 //    into fast_lock and fast_unlock.  In the case of success, control
 1663 //    will drop through the node.  ICC.ZF is undefined at exit.
 1664 //    In the case of failure, the node will branch directly to the
 1665 //    FailureLabel
 1666 
 1667 
 1668 // obj: object to lock
 1669 // box: on-stack box address (displaced header location) - KILLED
 1670 // rax,: tmp -- KILLED
 1671 // scr: tmp -- KILLED
 1672 void MacroAssembler::fast_lock(Register objReg, Register boxReg, Register tmpReg,
 1673                                Register scrReg, Register cx1Reg, Register cx2Reg,
 1674                                BiasedLockingCounters* counters,
 1675                                RTMLockingCounters* rtm_counters,
 1676                                RTMLockingCounters* stack_rtm_counters,
 1677                                Metadata* method_data,
 1678                                bool use_rtm, bool profile_rtm) {
 1679   // Ensure the register assignments are disjoint
 1680   assert(tmpReg == rax, &quot;&quot;);
 1681 
 1682   if (use_rtm) {
 1683     assert_different_registers(objReg, boxReg, tmpReg, scrReg, cx1Reg, cx2Reg);
 1684   } else {
 1685     assert(cx1Reg == noreg, &quot;&quot;);
 1686     assert(cx2Reg == noreg, &quot;&quot;);
 1687     assert_different_registers(objReg, boxReg, tmpReg, scrReg);
 1688   }
 1689 
 1690   if (counters != NULL) {
 1691     atomic_incl(ExternalAddress((address)counters-&gt;total_entry_count_addr()), scrReg);
 1692   }
 1693 
 1694   // Possible cases that we&#39;ll encounter in fast_lock
 1695   // ------------------------------------------------
 1696   // * Inflated
 1697   //    -- unlocked
 1698   //    -- Locked
 1699   //       = by self
 1700   //       = by other
 1701   // * biased
 1702   //    -- by Self
 1703   //    -- by other
 1704   // * neutral
 1705   // * stack-locked
 1706   //    -- by self
 1707   //       = sp-proximity test hits
 1708   //       = sp-proximity test generates false-negative
 1709   //    -- by other
 1710   //
 1711 
 1712   Label IsInflated, DONE_LABEL;
 1713 
 1714   // it&#39;s stack-locked, biased or neutral
 1715   // TODO: optimize away redundant LDs of obj-&gt;mark and improve the markword triage
 1716   // order to reduce the number of conditional branches in the most common cases.
 1717   // Beware -- there&#39;s a subtle invariant that fetch of the markword
 1718   // at [FETCH], below, will never observe a biased encoding (*101b).
 1719   // If this invariant is not held we risk exclusion (safety) failure.
 1720   if (UseBiasedLocking &amp;&amp; !UseOptoBiasInlining) {
 1721     biased_locking_enter(boxReg, objReg, tmpReg, scrReg, false, DONE_LABEL, NULL, counters);
 1722   }
 1723 
 1724 #if INCLUDE_RTM_OPT
 1725   if (UseRTMForStackLocks &amp;&amp; use_rtm) {
 1726     rtm_stack_locking(objReg, tmpReg, scrReg, cx2Reg,
 1727                       stack_rtm_counters, method_data, profile_rtm,
 1728                       DONE_LABEL, IsInflated);
 1729   }
 1730 #endif // INCLUDE_RTM_OPT
 1731 
 1732   movptr(tmpReg, Address(objReg, oopDesc::mark_offset_in_bytes()));          // [FETCH]
 1733   testptr(tmpReg, markWord::monitor_value); // inflated vs stack-locked|neutral|biased
 1734   jccb(Assembler::notZero, IsInflated);
 1735 
 1736   // Attempt stack-locking ...
 1737   orptr (tmpReg, markWord::unlocked_value);
 1738   if (EnableValhalla &amp;&amp; !UseBiasedLocking) {
 1739     // Mask always_locked bit such that we go to the slow path if object is a value type
 1740     andptr(tmpReg, ~((int) markWord::biased_lock_bit_in_place));
 1741   }
 1742   movptr(Address(boxReg, 0), tmpReg);          // Anticipate successful CAS
 1743   lock();
 1744   cmpxchgptr(boxReg, Address(objReg, oopDesc::mark_offset_in_bytes()));      // Updates tmpReg
 1745   if (counters != NULL) {
 1746     cond_inc32(Assembler::equal,
 1747                ExternalAddress((address)counters-&gt;fast_path_entry_count_addr()));
 1748   }
 1749   jcc(Assembler::equal, DONE_LABEL);           // Success
 1750 
 1751   // Recursive locking.
 1752   // The object is stack-locked: markword contains stack pointer to BasicLock.
 1753   // Locked by current thread if difference with current SP is less than one page.
 1754   subptr(tmpReg, rsp);
 1755   // Next instruction set ZFlag == 1 (Success) if difference is less then one page.
 1756   andptr(tmpReg, (int32_t) (NOT_LP64(0xFFFFF003) LP64_ONLY(7 - os::vm_page_size())) );
 1757   movptr(Address(boxReg, 0), tmpReg);
 1758   if (counters != NULL) {
 1759     cond_inc32(Assembler::equal,
 1760                ExternalAddress((address)counters-&gt;fast_path_entry_count_addr()));
 1761   }
 1762   jmp(DONE_LABEL);
 1763 
 1764   bind(IsInflated);
 1765   // The object is inflated. tmpReg contains pointer to ObjectMonitor* + markWord::monitor_value
 1766 
 1767 #if INCLUDE_RTM_OPT
 1768   // Use the same RTM locking code in 32- and 64-bit VM.
 1769   if (use_rtm) {
 1770     rtm_inflated_locking(objReg, boxReg, tmpReg, scrReg, cx1Reg, cx2Reg,
 1771                          rtm_counters, method_data, profile_rtm, DONE_LABEL);
 1772   } else {
 1773 #endif // INCLUDE_RTM_OPT
 1774 
 1775 #ifndef _LP64
 1776   // The object is inflated.
 1777 
 1778   // boxReg refers to the on-stack BasicLock in the current frame.
 1779   // We&#39;d like to write:
 1780   //   set box-&gt;_displaced_header = markWord::unused_mark().  Any non-0 value suffices.
 1781   // This is convenient but results a ST-before-CAS penalty.  The following CAS suffers
 1782   // additional latency as we have another ST in the store buffer that must drain.
 1783 
 1784   // avoid ST-before-CAS
 1785   // register juggle because we need tmpReg for cmpxchgptr below
 1786   movptr(scrReg, boxReg);
 1787   movptr(boxReg, tmpReg);                   // consider: LEA box, [tmp-2]
 1788 
 1789   // Optimistic form: consider XORL tmpReg,tmpReg
 1790   movptr(tmpReg, NULL_WORD);
 1791 
 1792   // Appears unlocked - try to swing _owner from null to non-null.
 1793   // Ideally, I&#39;d manifest &quot;Self&quot; with get_thread and then attempt
 1794   // to CAS the register containing Self into m-&gt;Owner.
 1795   // But we don&#39;t have enough registers, so instead we can either try to CAS
 1796   // rsp or the address of the box (in scr) into &amp;m-&gt;owner.  If the CAS succeeds
 1797   // we later store &quot;Self&quot; into m-&gt;Owner.  Transiently storing a stack address
 1798   // (rsp or the address of the box) into  m-&gt;owner is harmless.
 1799   // Invariant: tmpReg == 0.  tmpReg is EAX which is the implicit cmpxchg comparand.
 1800   lock();
 1801   cmpxchgptr(scrReg, Address(boxReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)));
 1802   movptr(Address(scrReg, 0), 3);          // box-&gt;_displaced_header = 3
 1803   // If we weren&#39;t able to swing _owner from NULL to the BasicLock
 1804   // then take the slow path.
 1805   jccb  (Assembler::notZero, DONE_LABEL);
 1806   // update _owner from BasicLock to thread
 1807   get_thread (scrReg);                    // beware: clobbers ICCs
 1808   movptr(Address(boxReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)), scrReg);
 1809   xorptr(boxReg, boxReg);                 // set icc.ZFlag = 1 to indicate success
 1810 
 1811   // If the CAS fails we can either retry or pass control to the slow path.
 1812   // We use the latter tactic.
 1813   // Pass the CAS result in the icc.ZFlag into DONE_LABEL
 1814   // If the CAS was successful ...
 1815   //   Self has acquired the lock
 1816   //   Invariant: m-&gt;_recursions should already be 0, so we don&#39;t need to explicitly set it.
 1817   // Intentional fall-through into DONE_LABEL ...
 1818 #else // _LP64
 1819   // It&#39;s inflated and we use scrReg for ObjectMonitor* in this section.
 1820   movq(scrReg, tmpReg);
 1821   xorq(tmpReg, tmpReg);
 1822   lock();
 1823   cmpxchgptr(r15_thread, Address(scrReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)));
 1824   // Unconditionally set box-&gt;_displaced_header = markWord::unused_mark().
 1825   // Without cast to int32_t this style of movptr will destroy r10 which is typically obj.
 1826   movptr(Address(boxReg, 0), (int32_t)intptr_t(markWord::unused_mark().value()));
 1827   // Intentional fall-through into DONE_LABEL ...
 1828   // Propagate ICC.ZF from CAS above into DONE_LABEL.
 1829 #endif // _LP64
 1830 #if INCLUDE_RTM_OPT
 1831   } // use_rtm()
 1832 #endif
 1833   // DONE_LABEL is a hot target - we&#39;d really like to place it at the
 1834   // start of cache line by padding with NOPs.
 1835   // See the AMD and Intel software optimization manuals for the
 1836   // most efficient &quot;long&quot; NOP encodings.
 1837   // Unfortunately none of our alignment mechanisms suffice.
 1838   bind(DONE_LABEL);
 1839 
 1840   // At DONE_LABEL the icc ZFlag is set as follows ...
 1841   // fast_unlock uses the same protocol.
 1842   // ZFlag == 1 -&gt; Success
 1843   // ZFlag == 0 -&gt; Failure - force control through the slow path
 1844 }
 1845 
 1846 // obj: object to unlock
 1847 // box: box address (displaced header location), killed.  Must be EAX.
 1848 // tmp: killed, cannot be obj nor box.
 1849 //
 1850 // Some commentary on balanced locking:
 1851 //
 1852 // fast_lock and fast_unlock are emitted only for provably balanced lock sites.
 1853 // Methods that don&#39;t have provably balanced locking are forced to run in the
 1854 // interpreter - such methods won&#39;t be compiled to use fast_lock and fast_unlock.
 1855 // The interpreter provides two properties:
 1856 // I1:  At return-time the interpreter automatically and quietly unlocks any
 1857 //      objects acquired the current activation (frame).  Recall that the
 1858 //      interpreter maintains an on-stack list of locks currently held by
 1859 //      a frame.
 1860 // I2:  If a method attempts to unlock an object that is not held by the
 1861 //      the frame the interpreter throws IMSX.
 1862 //
 1863 // Lets say A(), which has provably balanced locking, acquires O and then calls B().
 1864 // B() doesn&#39;t have provably balanced locking so it runs in the interpreter.
 1865 // Control returns to A() and A() unlocks O.  By I1 and I2, above, we know that O
 1866 // is still locked by A().
 1867 //
 1868 // The only other source of unbalanced locking would be JNI.  The &quot;Java Native Interface:
 1869 // Programmer&#39;s Guide and Specification&quot; claims that an object locked by jni_monitorenter
 1870 // should not be unlocked by &quot;normal&quot; java-level locking and vice-versa.  The specification
 1871 // doesn&#39;t specify what will occur if a program engages in such mixed-mode locking, however.
 1872 // Arguably given that the spec legislates the JNI case as undefined our implementation
 1873 // could reasonably *avoid* checking owner in fast_unlock().
 1874 // In the interest of performance we elide m-&gt;Owner==Self check in unlock.
 1875 // A perfectly viable alternative is to elide the owner check except when
 1876 // Xcheck:jni is enabled.
 1877 
 1878 void MacroAssembler::fast_unlock(Register objReg, Register boxReg, Register tmpReg, bool use_rtm) {
 1879   assert(boxReg == rax, &quot;&quot;);
 1880   assert_different_registers(objReg, boxReg, tmpReg);
 1881 
 1882   Label DONE_LABEL, Stacked, CheckSucc;
 1883 
 1884   // Critically, the biased locking test must have precedence over
 1885   // and appear before the (box-&gt;dhw == 0) recursive stack-lock test.
 1886   if (UseBiasedLocking &amp;&amp; !UseOptoBiasInlining) {
 1887     biased_locking_exit(objReg, tmpReg, DONE_LABEL);
 1888   }
 1889 
 1890 #if INCLUDE_RTM_OPT
 1891   if (UseRTMForStackLocks &amp;&amp; use_rtm) {
 1892     assert(!UseBiasedLocking, &quot;Biased locking is not supported with RTM locking&quot;);
 1893     Label L_regular_unlock;
 1894     movptr(tmpReg, Address(objReg, oopDesc::mark_offset_in_bytes())); // fetch markword
 1895     andptr(tmpReg, markWord::biased_lock_mask_in_place);              // look at 3 lock bits
 1896     cmpptr(tmpReg, markWord::unlocked_value);                         // bits = 001 unlocked
 1897     jccb(Assembler::notEqual, L_regular_unlock);                      // if !HLE RegularLock
 1898     xend();                                                           // otherwise end...
 1899     jmp(DONE_LABEL);                                                  // ... and we&#39;re done
 1900     bind(L_regular_unlock);
 1901   }
 1902 #endif
 1903 
 1904   cmpptr(Address(boxReg, 0), (int32_t)NULL_WORD);                   // Examine the displaced header
 1905   jcc   (Assembler::zero, DONE_LABEL);                              // 0 indicates recursive stack-lock
 1906   movptr(tmpReg, Address(objReg, oopDesc::mark_offset_in_bytes())); // Examine the object&#39;s markword
 1907   testptr(tmpReg, markWord::monitor_value);                         // Inflated?
 1908   jccb  (Assembler::zero, Stacked);
 1909 
 1910   // It&#39;s inflated.
 1911 #if INCLUDE_RTM_OPT
 1912   if (use_rtm) {
 1913     Label L_regular_inflated_unlock;
 1914     int owner_offset = OM_OFFSET_NO_MONITOR_VALUE_TAG(owner);
 1915     movptr(boxReg, Address(tmpReg, owner_offset));
 1916     testptr(boxReg, boxReg);
 1917     jccb(Assembler::notZero, L_regular_inflated_unlock);
 1918     xend();
 1919     jmpb(DONE_LABEL);
 1920     bind(L_regular_inflated_unlock);
 1921   }
 1922 #endif
 1923 
 1924   // Despite our balanced locking property we still check that m-&gt;_owner == Self
 1925   // as java routines or native JNI code called by this thread might
 1926   // have released the lock.
 1927   // Refer to the comments in synchronizer.cpp for how we might encode extra
 1928   // state in _succ so we can avoid fetching EntryList|cxq.
 1929   //
 1930   // I&#39;d like to add more cases in fast_lock() and fast_unlock() --
 1931   // such as recursive enter and exit -- but we have to be wary of
 1932   // I$ bloat, T$ effects and BP$ effects.
 1933   //
 1934   // If there&#39;s no contention try a 1-0 exit.  That is, exit without
 1935   // a costly MEMBAR or CAS.  See synchronizer.cpp for details on how
 1936   // we detect and recover from the race that the 1-0 exit admits.
 1937   //
 1938   // Conceptually fast_unlock() must execute a STST|LDST &quot;release&quot; barrier
 1939   // before it STs null into _owner, releasing the lock.  Updates
 1940   // to data protected by the critical section must be visible before
 1941   // we drop the lock (and thus before any other thread could acquire
 1942   // the lock and observe the fields protected by the lock).
 1943   // IA32&#39;s memory-model is SPO, so STs are ordered with respect to
 1944   // each other and there&#39;s no need for an explicit barrier (fence).
 1945   // See also http://gee.cs.oswego.edu/dl/jmm/cookbook.html.
 1946 #ifndef _LP64
 1947   get_thread (boxReg);
 1948 
 1949   // Note that we could employ various encoding schemes to reduce
 1950   // the number of loads below (currently 4) to just 2 or 3.
 1951   // Refer to the comments in synchronizer.cpp.
 1952   // In practice the chain of fetches doesn&#39;t seem to impact performance, however.
 1953   xorptr(boxReg, boxReg);
 1954   orptr(boxReg, Address(tmpReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(recursions)));
 1955   jccb  (Assembler::notZero, DONE_LABEL);
 1956   movptr(boxReg, Address(tmpReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(EntryList)));
 1957   orptr(boxReg, Address(tmpReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(cxq)));
 1958   jccb  (Assembler::notZero, CheckSucc);
 1959   movptr(Address(tmpReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)), NULL_WORD);
 1960   jmpb  (DONE_LABEL);
 1961 
 1962   bind (Stacked);
 1963   // It&#39;s not inflated and it&#39;s not recursively stack-locked and it&#39;s not biased.
 1964   // It must be stack-locked.
 1965   // Try to reset the header to displaced header.
 1966   // The &quot;box&quot; value on the stack is stable, so we can reload
 1967   // and be assured we observe the same value as above.
 1968   movptr(tmpReg, Address(boxReg, 0));
 1969   lock();
 1970   cmpxchgptr(tmpReg, Address(objReg, oopDesc::mark_offset_in_bytes())); // Uses RAX which is box
 1971   // Intention fall-thru into DONE_LABEL
 1972 
 1973   // DONE_LABEL is a hot target - we&#39;d really like to place it at the
 1974   // start of cache line by padding with NOPs.
 1975   // See the AMD and Intel software optimization manuals for the
 1976   // most efficient &quot;long&quot; NOP encodings.
 1977   // Unfortunately none of our alignment mechanisms suffice.
 1978   bind (CheckSucc);
 1979 #else // _LP64
 1980   // It&#39;s inflated
 1981   xorptr(boxReg, boxReg);
 1982   orptr(boxReg, Address(tmpReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(recursions)));
 1983   jccb  (Assembler::notZero, DONE_LABEL);
 1984   movptr(boxReg, Address(tmpReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(cxq)));
 1985   orptr(boxReg, Address(tmpReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(EntryList)));
 1986   jccb  (Assembler::notZero, CheckSucc);
 1987   // Without cast to int32_t this style of movptr will destroy r10 which is typically obj.
 1988   movptr(Address(tmpReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)), (int32_t)NULL_WORD);
 1989   jmpb  (DONE_LABEL);
 1990 
 1991   // Try to avoid passing control into the slow_path ...
 1992   Label LSuccess, LGoSlowPath ;
 1993   bind  (CheckSucc);
 1994 
 1995   // The following optional optimization can be elided if necessary
 1996   // Effectively: if (succ == null) goto slow path
 1997   // The code reduces the window for a race, however,
 1998   // and thus benefits performance.
 1999   cmpptr(Address(tmpReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(succ)), (int32_t)NULL_WORD);
 2000   jccb  (Assembler::zero, LGoSlowPath);
 2001 
 2002   xorptr(boxReg, boxReg);
 2003   // Without cast to int32_t this style of movptr will destroy r10 which is typically obj.
 2004   movptr(Address(tmpReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)), (int32_t)NULL_WORD);
 2005 
 2006   // Memory barrier/fence
 2007   // Dekker pivot point -- fulcrum : ST Owner; MEMBAR; LD Succ
 2008   // Instead of MFENCE we use a dummy locked add of 0 to the top-of-stack.
 2009   // This is faster on Nehalem and AMD Shanghai/Barcelona.
 2010   // See https://blogs.oracle.com/dave/entry/instruction_selection_for_volatile_fences
 2011   // We might also restructure (ST Owner=0;barrier;LD _Succ) to
 2012   // (mov box,0; xchgq box, &amp;m-&gt;Owner; LD _succ) .
 2013   lock(); addl(Address(rsp, 0), 0);
 2014 
 2015   cmpptr(Address(tmpReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(succ)), (int32_t)NULL_WORD);
 2016   jccb  (Assembler::notZero, LSuccess);
 2017 
 2018   // Rare inopportune interleaving - race.
 2019   // The successor vanished in the small window above.
 2020   // The lock is contended -- (cxq|EntryList) != null -- and there&#39;s no apparent successor.
 2021   // We need to ensure progress and succession.
 2022   // Try to reacquire the lock.
 2023   // If that fails then the new owner is responsible for succession and this
 2024   // thread needs to take no further action and can exit via the fast path (success).
 2025   // If the re-acquire succeeds then pass control into the slow path.
 2026   // As implemented, this latter mode is horrible because we generated more
 2027   // coherence traffic on the lock *and* artifically extended the critical section
 2028   // length while by virtue of passing control into the slow path.
 2029 
 2030   // box is really RAX -- the following CMPXCHG depends on that binding
 2031   // cmpxchg R,[M] is equivalent to rax = CAS(M,rax,R)
 2032   lock();
 2033   cmpxchgptr(r15_thread, Address(tmpReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)));
 2034   // There&#39;s no successor so we tried to regrab the lock.
 2035   // If that didn&#39;t work, then another thread grabbed the
 2036   // lock so we&#39;re done (and exit was a success).
 2037   jccb  (Assembler::notEqual, LSuccess);
 2038   // Intentional fall-through into slow path
 2039 
 2040   bind  (LGoSlowPath);
 2041   orl   (boxReg, 1);                      // set ICC.ZF=0 to indicate failure
 2042   jmpb  (DONE_LABEL);
 2043 
 2044   bind  (LSuccess);
 2045   testl (boxReg, 0);                      // set ICC.ZF=1 to indicate success
 2046   jmpb  (DONE_LABEL);
 2047 
 2048   bind  (Stacked);
 2049   movptr(tmpReg, Address (boxReg, 0));      // re-fetch
 2050   lock();
 2051   cmpxchgptr(tmpReg, Address(objReg, oopDesc::mark_offset_in_bytes())); // Uses RAX which is box
 2052 
 2053 #endif
 2054   bind(DONE_LABEL);
 2055 }
 2056 #endif // COMPILER2
 2057 
 2058 void MacroAssembler::c2bool(Register x) {
 2059   // implements x == 0 ? 0 : 1
 2060   // note: must only look at least-significant byte of x
 2061   //       since C-style booleans are stored in one byte
 2062   //       only! (was bug)
 2063   andl(x, 0xFF);
 2064   setb(Assembler::notZero, x);
 2065 }
 2066 
 2067 // Wouldn&#39;t need if AddressLiteral version had new name
 2068 void MacroAssembler::call(Label&amp; L, relocInfo::relocType rtype) {
 2069   Assembler::call(L, rtype);
 2070 }
 2071 
 2072 void MacroAssembler::call(Register entry) {
 2073   Assembler::call(entry);
 2074 }
 2075 
 2076 void MacroAssembler::call(AddressLiteral entry) {
 2077   if (reachable(entry)) {
 2078     Assembler::call_literal(entry.target(), entry.rspec());
 2079   } else {
 2080     lea(rscratch1, entry);
 2081     Assembler::call(rscratch1);
 2082   }
 2083 }
 2084 
 2085 void MacroAssembler::ic_call(address entry, jint method_index) {
 2086   RelocationHolder rh = virtual_call_Relocation::spec(pc(), method_index);
 2087   movptr(rax, (intptr_t)Universe::non_oop_word());
 2088   call(AddressLiteral(entry, rh));
 2089 }
 2090 
 2091 // Implementation of call_VM versions
 2092 
 2093 void MacroAssembler::call_VM(Register oop_result,
 2094                              address entry_point,
 2095                              bool check_exceptions) {
 2096   Label C, E;
 2097   call(C, relocInfo::none);
 2098   jmp(E);
 2099 
 2100   bind(C);
 2101   call_VM_helper(oop_result, entry_point, 0, check_exceptions);
 2102   ret(0);
 2103 
 2104   bind(E);
 2105 }
 2106 
 2107 void MacroAssembler::call_VM(Register oop_result,
 2108                              address entry_point,
 2109                              Register arg_1,
 2110                              bool check_exceptions) {
 2111   Label C, E;
 2112   call(C, relocInfo::none);
 2113   jmp(E);
 2114 
 2115   bind(C);
 2116   pass_arg1(this, arg_1);
 2117   call_VM_helper(oop_result, entry_point, 1, check_exceptions);
 2118   ret(0);
 2119 
 2120   bind(E);
 2121 }
 2122 
 2123 void MacroAssembler::call_VM(Register oop_result,
 2124                              address entry_point,
 2125                              Register arg_1,
 2126                              Register arg_2,
 2127                              bool check_exceptions) {
 2128   Label C, E;
 2129   call(C, relocInfo::none);
 2130   jmp(E);
 2131 
 2132   bind(C);
 2133 
 2134   LP64_ONLY(assert(arg_1 != c_rarg2, &quot;smashed arg&quot;));
 2135 
 2136   pass_arg2(this, arg_2);
 2137   pass_arg1(this, arg_1);
 2138   call_VM_helper(oop_result, entry_point, 2, check_exceptions);
 2139   ret(0);
 2140 
 2141   bind(E);
 2142 }
 2143 
 2144 void MacroAssembler::call_VM(Register oop_result,
 2145                              address entry_point,
 2146                              Register arg_1,
 2147                              Register arg_2,
 2148                              Register arg_3,
 2149                              bool check_exceptions) {
 2150   Label C, E;
 2151   call(C, relocInfo::none);
 2152   jmp(E);
 2153 
 2154   bind(C);
 2155 
 2156   LP64_ONLY(assert(arg_1 != c_rarg3, &quot;smashed arg&quot;));
 2157   LP64_ONLY(assert(arg_2 != c_rarg3, &quot;smashed arg&quot;));
 2158   pass_arg3(this, arg_3);
 2159 
 2160   LP64_ONLY(assert(arg_1 != c_rarg2, &quot;smashed arg&quot;));
 2161   pass_arg2(this, arg_2);
 2162 
 2163   pass_arg1(this, arg_1);
 2164   call_VM_helper(oop_result, entry_point, 3, check_exceptions);
 2165   ret(0);
 2166 
 2167   bind(E);
 2168 }
 2169 
 2170 void MacroAssembler::call_VM(Register oop_result,
 2171                              Register last_java_sp,
 2172                              address entry_point,
 2173                              int number_of_arguments,
 2174                              bool check_exceptions) {
 2175   Register thread = LP64_ONLY(r15_thread) NOT_LP64(noreg);
 2176   call_VM_base(oop_result, thread, last_java_sp, entry_point, number_of_arguments, check_exceptions);
 2177 }
 2178 
 2179 void MacroAssembler::call_VM(Register oop_result,
 2180                              Register last_java_sp,
 2181                              address entry_point,
 2182                              Register arg_1,
 2183                              bool check_exceptions) {
 2184   pass_arg1(this, arg_1);
 2185   call_VM(oop_result, last_java_sp, entry_point, 1, check_exceptions);
 2186 }
 2187 
 2188 void MacroAssembler::call_VM(Register oop_result,
 2189                              Register last_java_sp,
 2190                              address entry_point,
 2191                              Register arg_1,
 2192                              Register arg_2,
 2193                              bool check_exceptions) {
 2194 
 2195   LP64_ONLY(assert(arg_1 != c_rarg2, &quot;smashed arg&quot;));
 2196   pass_arg2(this, arg_2);
 2197   pass_arg1(this, arg_1);
 2198   call_VM(oop_result, last_java_sp, entry_point, 2, check_exceptions);
 2199 }
 2200 
 2201 void MacroAssembler::call_VM(Register oop_result,
 2202                              Register last_java_sp,
 2203                              address entry_point,
 2204                              Register arg_1,
 2205                              Register arg_2,
 2206                              Register arg_3,
 2207                              bool check_exceptions) {
 2208   LP64_ONLY(assert(arg_1 != c_rarg3, &quot;smashed arg&quot;));
 2209   LP64_ONLY(assert(arg_2 != c_rarg3, &quot;smashed arg&quot;));
 2210   pass_arg3(this, arg_3);
 2211   LP64_ONLY(assert(arg_1 != c_rarg2, &quot;smashed arg&quot;));
 2212   pass_arg2(this, arg_2);
 2213   pass_arg1(this, arg_1);
 2214   call_VM(oop_result, last_java_sp, entry_point, 3, check_exceptions);
 2215 }
 2216 
 2217 void MacroAssembler::super_call_VM(Register oop_result,
 2218                                    Register last_java_sp,
 2219                                    address entry_point,
 2220                                    int number_of_arguments,
 2221                                    bool check_exceptions) {
 2222   Register thread = LP64_ONLY(r15_thread) NOT_LP64(noreg);
 2223   MacroAssembler::call_VM_base(oop_result, thread, last_java_sp, entry_point, number_of_arguments, check_exceptions);
 2224 }
 2225 
 2226 void MacroAssembler::super_call_VM(Register oop_result,
 2227                                    Register last_java_sp,
 2228                                    address entry_point,
 2229                                    Register arg_1,
 2230                                    bool check_exceptions) {
 2231   pass_arg1(this, arg_1);
 2232   super_call_VM(oop_result, last_java_sp, entry_point, 1, check_exceptions);
 2233 }
 2234 
 2235 void MacroAssembler::super_call_VM(Register oop_result,
 2236                                    Register last_java_sp,
 2237                                    address entry_point,
 2238                                    Register arg_1,
 2239                                    Register arg_2,
 2240                                    bool check_exceptions) {
 2241 
 2242   LP64_ONLY(assert(arg_1 != c_rarg2, &quot;smashed arg&quot;));
 2243   pass_arg2(this, arg_2);
 2244   pass_arg1(this, arg_1);
 2245   super_call_VM(oop_result, last_java_sp, entry_point, 2, check_exceptions);
 2246 }
 2247 
 2248 void MacroAssembler::super_call_VM(Register oop_result,
 2249                                    Register last_java_sp,
 2250                                    address entry_point,
 2251                                    Register arg_1,
 2252                                    Register arg_2,
 2253                                    Register arg_3,
 2254                                    bool check_exceptions) {
 2255   LP64_ONLY(assert(arg_1 != c_rarg3, &quot;smashed arg&quot;));
 2256   LP64_ONLY(assert(arg_2 != c_rarg3, &quot;smashed arg&quot;));
 2257   pass_arg3(this, arg_3);
 2258   LP64_ONLY(assert(arg_1 != c_rarg2, &quot;smashed arg&quot;));
 2259   pass_arg2(this, arg_2);
 2260   pass_arg1(this, arg_1);
 2261   super_call_VM(oop_result, last_java_sp, entry_point, 3, check_exceptions);
 2262 }
 2263 
 2264 void MacroAssembler::call_VM_base(Register oop_result,
 2265                                   Register java_thread,
 2266                                   Register last_java_sp,
 2267                                   address  entry_point,
 2268                                   int      number_of_arguments,
 2269                                   bool     check_exceptions) {
 2270   // determine java_thread register
 2271   if (!java_thread-&gt;is_valid()) {
 2272 #ifdef _LP64
 2273     java_thread = r15_thread;
 2274 #else
 2275     java_thread = rdi;
 2276     get_thread(java_thread);
 2277 #endif // LP64
 2278   }
 2279   // determine last_java_sp register
 2280   if (!last_java_sp-&gt;is_valid()) {
 2281     last_java_sp = rsp;
 2282   }
 2283   // debugging support
 2284   assert(number_of_arguments &gt;= 0   , &quot;cannot have negative number of arguments&quot;);
 2285   LP64_ONLY(assert(java_thread == r15_thread, &quot;unexpected register&quot;));
 2286 #ifdef ASSERT
 2287   // TraceBytecodes does not use r12 but saves it over the call, so don&#39;t verify
 2288   // r12 is the heapbase.
 2289   LP64_ONLY(if ((UseCompressedOops || UseCompressedClassPointers) &amp;&amp; !TraceBytecodes) verify_heapbase(&quot;call_VM_base: heap base corrupted?&quot;);)
 2290 #endif // ASSERT
 2291 
 2292   assert(java_thread != oop_result  , &quot;cannot use the same register for java_thread &amp; oop_result&quot;);
 2293   assert(java_thread != last_java_sp, &quot;cannot use the same register for java_thread &amp; last_java_sp&quot;);
 2294 
 2295   // push java thread (becomes first argument of C function)
 2296 
 2297   NOT_LP64(push(java_thread); number_of_arguments++);
 2298   LP64_ONLY(mov(c_rarg0, r15_thread));
 2299 
 2300   // set last Java frame before call
 2301   assert(last_java_sp != rbp, &quot;can&#39;t use ebp/rbp&quot;);
 2302 
 2303   // Only interpreter should have to set fp
 2304   set_last_Java_frame(java_thread, last_java_sp, rbp, NULL);
 2305 
 2306   // do the call, remove parameters
 2307   MacroAssembler::call_VM_leaf_base(entry_point, number_of_arguments);
 2308 
 2309   // restore the thread (cannot use the pushed argument since arguments
 2310   // may be overwritten by C code generated by an optimizing compiler);
 2311   // however can use the register value directly if it is callee saved.
 2312   if (LP64_ONLY(true ||) java_thread == rdi || java_thread == rsi) {
 2313     // rdi &amp; rsi (also r15) are callee saved -&gt; nothing to do
 2314 #ifdef ASSERT
 2315     guarantee(java_thread != rax, &quot;change this code&quot;);
 2316     push(rax);
 2317     { Label L;
 2318       get_thread(rax);
 2319       cmpptr(java_thread, rax);
 2320       jcc(Assembler::equal, L);
 2321       STOP(&quot;MacroAssembler::call_VM_base: rdi not callee saved?&quot;);
 2322       bind(L);
 2323     }
 2324     pop(rax);
 2325 #endif
 2326   } else {
 2327     get_thread(java_thread);
 2328   }
 2329   // reset last Java frame
 2330   // Only interpreter should have to clear fp
 2331   reset_last_Java_frame(java_thread, true);
 2332 
 2333    // C++ interp handles this in the interpreter
 2334   check_and_handle_popframe(java_thread);
 2335   check_and_handle_earlyret(java_thread);
 2336 
 2337   if (check_exceptions) {
 2338     // check for pending exceptions (java_thread is set upon return)
 2339     cmpptr(Address(java_thread, Thread::pending_exception_offset()), (int32_t) NULL_WORD);
 2340 #ifndef _LP64
 2341     jump_cc(Assembler::notEqual,
 2342             RuntimeAddress(StubRoutines::forward_exception_entry()));
 2343 #else
 2344     // This used to conditionally jump to forward_exception however it is
 2345     // possible if we relocate that the branch will not reach. So we must jump
 2346     // around so we can always reach
 2347 
 2348     Label ok;
 2349     jcc(Assembler::equal, ok);
 2350     jump(RuntimeAddress(StubRoutines::forward_exception_entry()));
 2351     bind(ok);
 2352 #endif // LP64
 2353   }
 2354 
 2355   // get oop result if there is one and reset the value in the thread
 2356   if (oop_result-&gt;is_valid()) {
 2357     get_vm_result(oop_result, java_thread);
 2358   }
 2359 }
 2360 
 2361 void MacroAssembler::call_VM_helper(Register oop_result, address entry_point, int number_of_arguments, bool check_exceptions) {
 2362 
 2363   // Calculate the value for last_Java_sp
 2364   // somewhat subtle. call_VM does an intermediate call
 2365   // which places a return address on the stack just under the
 2366   // stack pointer as the user finsihed with it. This allows
 2367   // use to retrieve last_Java_pc from last_Java_sp[-1].
 2368   // On 32bit we then have to push additional args on the stack to accomplish
 2369   // the actual requested call. On 64bit call_VM only can use register args
 2370   // so the only extra space is the return address that call_VM created.
 2371   // This hopefully explains the calculations here.
 2372 
 2373 #ifdef _LP64
 2374   // We&#39;ve pushed one address, correct last_Java_sp
 2375   lea(rax, Address(rsp, wordSize));
 2376 #else
 2377   lea(rax, Address(rsp, (1 + number_of_arguments) * wordSize));
 2378 #endif // LP64
 2379 
 2380   call_VM_base(oop_result, noreg, rax, entry_point, number_of_arguments, check_exceptions);
 2381 
 2382 }
 2383 
 2384 // Use this method when MacroAssembler version of call_VM_leaf_base() should be called from Interpreter.
 2385 void MacroAssembler::call_VM_leaf0(address entry_point) {
 2386   MacroAssembler::call_VM_leaf_base(entry_point, 0);
 2387 }
 2388 
 2389 void MacroAssembler::call_VM_leaf(address entry_point, int number_of_arguments) {
 2390   call_VM_leaf_base(entry_point, number_of_arguments);
 2391 }
 2392 
 2393 void MacroAssembler::call_VM_leaf(address entry_point, Register arg_0) {
 2394   pass_arg0(this, arg_0);
 2395   call_VM_leaf(entry_point, 1);
 2396 }
 2397 
 2398 void MacroAssembler::call_VM_leaf(address entry_point, Register arg_0, Register arg_1) {
 2399 
 2400   LP64_ONLY(assert(arg_0 != c_rarg1, &quot;smashed arg&quot;));
 2401   pass_arg1(this, arg_1);
 2402   pass_arg0(this, arg_0);
 2403   call_VM_leaf(entry_point, 2);
 2404 }
 2405 
 2406 void MacroAssembler::call_VM_leaf(address entry_point, Register arg_0, Register arg_1, Register arg_2) {
 2407   LP64_ONLY(assert(arg_0 != c_rarg2, &quot;smashed arg&quot;));
 2408   LP64_ONLY(assert(arg_1 != c_rarg2, &quot;smashed arg&quot;));
 2409   pass_arg2(this, arg_2);
 2410   LP64_ONLY(assert(arg_0 != c_rarg1, &quot;smashed arg&quot;));
 2411   pass_arg1(this, arg_1);
 2412   pass_arg0(this, arg_0);
 2413   call_VM_leaf(entry_point, 3);
 2414 }
 2415 
 2416 void MacroAssembler::super_call_VM_leaf(address entry_point) {
 2417   MacroAssembler::call_VM_leaf_base(entry_point, 1);
 2418 }
 2419 
 2420 void MacroAssembler::super_call_VM_leaf(address entry_point, Register arg_0) {
 2421   pass_arg0(this, arg_0);
 2422   MacroAssembler::call_VM_leaf_base(entry_point, 1);
 2423 }
 2424 
 2425 void MacroAssembler::super_call_VM_leaf(address entry_point, Register arg_0, Register arg_1) {
 2426 
 2427   LP64_ONLY(assert(arg_0 != c_rarg1, &quot;smashed arg&quot;));
 2428   pass_arg1(this, arg_1);
 2429   pass_arg0(this, arg_0);
 2430   MacroAssembler::call_VM_leaf_base(entry_point, 2);
 2431 }
 2432 
 2433 void MacroAssembler::super_call_VM_leaf(address entry_point, Register arg_0, Register arg_1, Register arg_2) {
 2434   LP64_ONLY(assert(arg_0 != c_rarg2, &quot;smashed arg&quot;));
 2435   LP64_ONLY(assert(arg_1 != c_rarg2, &quot;smashed arg&quot;));
 2436   pass_arg2(this, arg_2);
 2437   LP64_ONLY(assert(arg_0 != c_rarg1, &quot;smashed arg&quot;));
 2438   pass_arg1(this, arg_1);
 2439   pass_arg0(this, arg_0);
 2440   MacroAssembler::call_VM_leaf_base(entry_point, 3);
 2441 }
 2442 
 2443 void MacroAssembler::super_call_VM_leaf(address entry_point, Register arg_0, Register arg_1, Register arg_2, Register arg_3) {
 2444   LP64_ONLY(assert(arg_0 != c_rarg3, &quot;smashed arg&quot;));
 2445   LP64_ONLY(assert(arg_1 != c_rarg3, &quot;smashed arg&quot;));
 2446   LP64_ONLY(assert(arg_2 != c_rarg3, &quot;smashed arg&quot;));
 2447   pass_arg3(this, arg_3);
 2448   LP64_ONLY(assert(arg_0 != c_rarg2, &quot;smashed arg&quot;));
 2449   LP64_ONLY(assert(arg_1 != c_rarg2, &quot;smashed arg&quot;));
 2450   pass_arg2(this, arg_2);
 2451   LP64_ONLY(assert(arg_0 != c_rarg1, &quot;smashed arg&quot;));
 2452   pass_arg1(this, arg_1);
 2453   pass_arg0(this, arg_0);
 2454   MacroAssembler::call_VM_leaf_base(entry_point, 4);
 2455 }
 2456 
 2457 void MacroAssembler::get_vm_result(Register oop_result, Register java_thread) {
 2458   movptr(oop_result, Address(java_thread, JavaThread::vm_result_offset()));
 2459   movptr(Address(java_thread, JavaThread::vm_result_offset()), NULL_WORD);
 2460   verify_oop_msg(oop_result, &quot;broken oop in call_VM_base&quot;);
 2461 }
 2462 
 2463 void MacroAssembler::get_vm_result_2(Register metadata_result, Register java_thread) {
 2464   movptr(metadata_result, Address(java_thread, JavaThread::vm_result_2_offset()));
 2465   movptr(Address(java_thread, JavaThread::vm_result_2_offset()), NULL_WORD);
 2466 }
 2467 
 2468 void MacroAssembler::check_and_handle_earlyret(Register java_thread) {
 2469 }
 2470 
 2471 void MacroAssembler::check_and_handle_popframe(Register java_thread) {
 2472 }
 2473 
 2474 void MacroAssembler::cmp32(AddressLiteral src1, int32_t imm) {
 2475   if (reachable(src1)) {
 2476     cmpl(as_Address(src1), imm);
 2477   } else {
 2478     lea(rscratch1, src1);
 2479     cmpl(Address(rscratch1, 0), imm);
 2480   }
 2481 }
 2482 
 2483 void MacroAssembler::cmp32(Register src1, AddressLiteral src2) {
 2484   assert(!src2.is_lval(), &quot;use cmpptr&quot;);
 2485   if (reachable(src2)) {
 2486     cmpl(src1, as_Address(src2));
 2487   } else {
 2488     lea(rscratch1, src2);
 2489     cmpl(src1, Address(rscratch1, 0));
 2490   }
 2491 }
 2492 
 2493 void MacroAssembler::cmp32(Register src1, int32_t imm) {
 2494   Assembler::cmpl(src1, imm);
 2495 }
 2496 
 2497 void MacroAssembler::cmp32(Register src1, Address src2) {
 2498   Assembler::cmpl(src1, src2);
 2499 }
 2500 
 2501 void MacroAssembler::cmpsd2int(XMMRegister opr1, XMMRegister opr2, Register dst, bool unordered_is_less) {
 2502   ucomisd(opr1, opr2);
 2503 
 2504   Label L;
 2505   if (unordered_is_less) {
 2506     movl(dst, -1);
 2507     jcc(Assembler::parity, L);
 2508     jcc(Assembler::below , L);
 2509     movl(dst, 0);
 2510     jcc(Assembler::equal , L);
 2511     increment(dst);
 2512   } else { // unordered is greater
 2513     movl(dst, 1);
 2514     jcc(Assembler::parity, L);
 2515     jcc(Assembler::above , L);
 2516     movl(dst, 0);
 2517     jcc(Assembler::equal , L);
 2518     decrementl(dst);
 2519   }
 2520   bind(L);
 2521 }
 2522 
 2523 void MacroAssembler::cmpss2int(XMMRegister opr1, XMMRegister opr2, Register dst, bool unordered_is_less) {
 2524   ucomiss(opr1, opr2);
 2525 
 2526   Label L;
 2527   if (unordered_is_less) {
 2528     movl(dst, -1);
 2529     jcc(Assembler::parity, L);
 2530     jcc(Assembler::below , L);
 2531     movl(dst, 0);
 2532     jcc(Assembler::equal , L);
 2533     increment(dst);
 2534   } else { // unordered is greater
 2535     movl(dst, 1);
 2536     jcc(Assembler::parity, L);
 2537     jcc(Assembler::above , L);
 2538     movl(dst, 0);
 2539     jcc(Assembler::equal , L);
 2540     decrementl(dst);
 2541   }
 2542   bind(L);
 2543 }
 2544 
 2545 
 2546 void MacroAssembler::cmp8(AddressLiteral src1, int imm) {
 2547   if (reachable(src1)) {
 2548     cmpb(as_Address(src1), imm);
 2549   } else {
 2550     lea(rscratch1, src1);
 2551     cmpb(Address(rscratch1, 0), imm);
 2552   }
 2553 }
 2554 
 2555 void MacroAssembler::cmpptr(Register src1, AddressLiteral src2) {
 2556 #ifdef _LP64
 2557   if (src2.is_lval()) {
 2558     movptr(rscratch1, src2);
 2559     Assembler::cmpq(src1, rscratch1);
 2560   } else if (reachable(src2)) {
 2561     cmpq(src1, as_Address(src2));
 2562   } else {
 2563     lea(rscratch1, src2);
 2564     Assembler::cmpq(src1, Address(rscratch1, 0));
 2565   }
 2566 #else
 2567   if (src2.is_lval()) {
 2568     cmp_literal32(src1, (int32_t) src2.target(), src2.rspec());
 2569   } else {
 2570     cmpl(src1, as_Address(src2));
 2571   }
 2572 #endif // _LP64
 2573 }
 2574 
 2575 void MacroAssembler::cmpptr(Address src1, AddressLiteral src2) {
 2576   assert(src2.is_lval(), &quot;not a mem-mem compare&quot;);
 2577 #ifdef _LP64
 2578   // moves src2&#39;s literal address
 2579   movptr(rscratch1, src2);
 2580   Assembler::cmpq(src1, rscratch1);
 2581 #else
 2582   cmp_literal32(src1, (int32_t) src2.target(), src2.rspec());
 2583 #endif // _LP64
 2584 }
 2585 
 2586 void MacroAssembler::cmpoop(Register src1, Register src2) {
 2587   BarrierSetAssembler* bs = BarrierSet::barrier_set()-&gt;barrier_set_assembler();
 2588   bs-&gt;obj_equals(this, src1, src2);
 2589 }
 2590 
 2591 void MacroAssembler::cmpoop(Register src1, Address src2) {
 2592   BarrierSetAssembler* bs = BarrierSet::barrier_set()-&gt;barrier_set_assembler();
 2593   bs-&gt;obj_equals(this, src1, src2);
 2594 }
 2595 
 2596 #ifdef _LP64
 2597 void MacroAssembler::cmpoop(Register src1, jobject src2) {
 2598   movoop(rscratch1, src2);
 2599   BarrierSetAssembler* bs = BarrierSet::barrier_set()-&gt;barrier_set_assembler();
 2600   bs-&gt;obj_equals(this, src1, rscratch1);
 2601 }
 2602 #endif
 2603 
 2604 void MacroAssembler::locked_cmpxchgptr(Register reg, AddressLiteral adr) {
 2605   if (reachable(adr)) {
 2606     lock();
 2607     cmpxchgptr(reg, as_Address(adr));
 2608   } else {
 2609     lea(rscratch1, adr);
 2610     lock();
 2611     cmpxchgptr(reg, Address(rscratch1, 0));
 2612   }
 2613 }
 2614 
 2615 void MacroAssembler::cmpxchgptr(Register reg, Address adr) {
 2616   LP64_ONLY(cmpxchgq(reg, adr)) NOT_LP64(cmpxchgl(reg, adr));
 2617 }
 2618 
 2619 void MacroAssembler::comisd(XMMRegister dst, AddressLiteral src) {
 2620   if (reachable(src)) {
 2621     Assembler::comisd(dst, as_Address(src));
 2622   } else {
 2623     lea(rscratch1, src);
 2624     Assembler::comisd(dst, Address(rscratch1, 0));
 2625   }
 2626 }
 2627 
 2628 void MacroAssembler::comiss(XMMRegister dst, AddressLiteral src) {
 2629   if (reachable(src)) {
 2630     Assembler::comiss(dst, as_Address(src));
 2631   } else {
 2632     lea(rscratch1, src);
 2633     Assembler::comiss(dst, Address(rscratch1, 0));
 2634   }
 2635 }
 2636 
 2637 
 2638 void MacroAssembler::cond_inc32(Condition cond, AddressLiteral counter_addr) {
 2639   Condition negated_cond = negate_condition(cond);
 2640   Label L;
 2641   jcc(negated_cond, L);
 2642   pushf(); // Preserve flags
 2643   atomic_incl(counter_addr);
 2644   popf();
 2645   bind(L);
 2646 }
 2647 
 2648 int MacroAssembler::corrected_idivl(Register reg) {
 2649   // Full implementation of Java idiv and irem; checks for
 2650   // special case as described in JVM spec., p.243 &amp; p.271.
 2651   // The function returns the (pc) offset of the idivl
 2652   // instruction - may be needed for implicit exceptions.
 2653   //
 2654   //         normal case                           special case
 2655   //
 2656   // input : rax,: dividend                         min_int
 2657   //         reg: divisor   (may not be rax,/rdx)   -1
 2658   //
 2659   // output: rax,: quotient  (= rax, idiv reg)       min_int
 2660   //         rdx: remainder (= rax, irem reg)       0
 2661   assert(reg != rax &amp;&amp; reg != rdx, &quot;reg cannot be rax, or rdx register&quot;);
 2662   const int min_int = 0x80000000;
 2663   Label normal_case, special_case;
 2664 
 2665   // check for special case
 2666   cmpl(rax, min_int);
 2667   jcc(Assembler::notEqual, normal_case);
 2668   xorl(rdx, rdx); // prepare rdx for possible special case (where remainder = 0)
 2669   cmpl(reg, -1);
 2670   jcc(Assembler::equal, special_case);
 2671 
 2672   // handle normal case
 2673   bind(normal_case);
 2674   cdql();
 2675   int idivl_offset = offset();
 2676   idivl(reg);
 2677 
 2678   // normal and special case exit
 2679   bind(special_case);
 2680 
 2681   return idivl_offset;
 2682 }
 2683 
 2684 
 2685 
 2686 void MacroAssembler::decrementl(Register reg, int value) {
 2687   if (value == min_jint) {subl(reg, value) ; return; }
 2688   if (value &lt;  0) { incrementl(reg, -value); return; }
 2689   if (value == 0) {                        ; return; }
 2690   if (value == 1 &amp;&amp; UseIncDec) { decl(reg) ; return; }
 2691   /* else */      { subl(reg, value)       ; return; }
 2692 }
 2693 
 2694 void MacroAssembler::decrementl(Address dst, int value) {
 2695   if (value == min_jint) {subl(dst, value) ; return; }
 2696   if (value &lt;  0) { incrementl(dst, -value); return; }
 2697   if (value == 0) {                        ; return; }
 2698   if (value == 1 &amp;&amp; UseIncDec) { decl(dst) ; return; }
 2699   /* else */      { subl(dst, value)       ; return; }
 2700 }
 2701 
 2702 void MacroAssembler::division_with_shift (Register reg, int shift_value) {
 2703   assert (shift_value &gt; 0, &quot;illegal shift value&quot;);
 2704   Label _is_positive;
 2705   testl (reg, reg);
 2706   jcc (Assembler::positive, _is_positive);
 2707   int offset = (1 &lt;&lt; shift_value) - 1 ;
 2708 
 2709   if (offset == 1) {
 2710     incrementl(reg);
 2711   } else {
 2712     addl(reg, offset);
 2713   }
 2714 
 2715   bind (_is_positive);
 2716   sarl(reg, shift_value);
 2717 }
 2718 
 2719 void MacroAssembler::divsd(XMMRegister dst, AddressLiteral src) {
 2720   if (reachable(src)) {
 2721     Assembler::divsd(dst, as_Address(src));
 2722   } else {
 2723     lea(rscratch1, src);
 2724     Assembler::divsd(dst, Address(rscratch1, 0));
 2725   }
 2726 }
 2727 
 2728 void MacroAssembler::divss(XMMRegister dst, AddressLiteral src) {
 2729   if (reachable(src)) {
 2730     Assembler::divss(dst, as_Address(src));
 2731   } else {
 2732     lea(rscratch1, src);
 2733     Assembler::divss(dst, Address(rscratch1, 0));
 2734   }
 2735 }
 2736 
 2737 #ifndef _LP64
 2738 void MacroAssembler::empty_FPU_stack() {
 2739   if (VM_Version::supports_mmx()) {
 2740     emms();
 2741   } else {
 2742     for (int i = 8; i-- &gt; 0; ) ffree(i);
 2743   }
 2744 }
 2745 #endif // !LP64
 2746 
 2747 
 2748 void MacroAssembler::enter() {
 2749   push(rbp);
 2750   mov(rbp, rsp);
 2751 }
 2752 
 2753 // A 5 byte nop that is safe for patching (see patch_verified_entry)
 2754 void MacroAssembler::fat_nop() {
 2755   if (UseAddressNop) {
 2756     addr_nop_5();
 2757   } else {
 2758     emit_int8(0x26); // es:
 2759     emit_int8(0x2e); // cs:
 2760     emit_int8(0x64); // fs:
 2761     emit_int8(0x65); // gs:
 2762     emit_int8((unsigned char)0x90);
 2763   }
 2764 }
 2765 
 2766 #if !defined(_LP64)
 2767 void MacroAssembler::fcmp(Register tmp) {
 2768   fcmp(tmp, 1, true, true);
 2769 }
 2770 
 2771 void MacroAssembler::fcmp(Register tmp, int index, bool pop_left, bool pop_right) {
 2772   assert(!pop_right || pop_left, &quot;usage error&quot;);
 2773   if (VM_Version::supports_cmov()) {
 2774     assert(tmp == noreg, &quot;unneeded temp&quot;);
 2775     if (pop_left) {
 2776       fucomip(index);
 2777     } else {
 2778       fucomi(index);
 2779     }
 2780     if (pop_right) {
 2781       fpop();
 2782     }
 2783   } else {
 2784     assert(tmp != noreg, &quot;need temp&quot;);
 2785     if (pop_left) {
 2786       if (pop_right) {
 2787         fcompp();
 2788       } else {
 2789         fcomp(index);
 2790       }
 2791     } else {
 2792       fcom(index);
 2793     }
 2794     // convert FPU condition into eflags condition via rax,
 2795     save_rax(tmp);
 2796     fwait(); fnstsw_ax();
 2797     sahf();
 2798     restore_rax(tmp);
 2799   }
 2800   // condition codes set as follows:
 2801   //
 2802   // CF (corresponds to C0) if x &lt; y
 2803   // PF (corresponds to C2) if unordered
 2804   // ZF (corresponds to C3) if x = y
 2805 }
 2806 
 2807 void MacroAssembler::fcmp2int(Register dst, bool unordered_is_less) {
 2808   fcmp2int(dst, unordered_is_less, 1, true, true);
 2809 }
 2810 
 2811 void MacroAssembler::fcmp2int(Register dst, bool unordered_is_less, int index, bool pop_left, bool pop_right) {
 2812   fcmp(VM_Version::supports_cmov() ? noreg : dst, index, pop_left, pop_right);
 2813   Label L;
 2814   if (unordered_is_less) {
 2815     movl(dst, -1);
 2816     jcc(Assembler::parity, L);
 2817     jcc(Assembler::below , L);
 2818     movl(dst, 0);
 2819     jcc(Assembler::equal , L);
 2820     increment(dst);
 2821   } else { // unordered is greater
 2822     movl(dst, 1);
 2823     jcc(Assembler::parity, L);
 2824     jcc(Assembler::above , L);
 2825     movl(dst, 0);
 2826     jcc(Assembler::equal , L);
 2827     decrementl(dst);
 2828   }
 2829   bind(L);
 2830 }
 2831 
 2832 void MacroAssembler::fld_d(AddressLiteral src) {
 2833   fld_d(as_Address(src));
 2834 }
 2835 
 2836 void MacroAssembler::fld_s(AddressLiteral src) {
 2837   fld_s(as_Address(src));
 2838 }
 2839 
 2840 void MacroAssembler::fld_x(AddressLiteral src) {
 2841   Assembler::fld_x(as_Address(src));
 2842 }
 2843 
 2844 void MacroAssembler::fldcw(AddressLiteral src) {
 2845   Assembler::fldcw(as_Address(src));
 2846 }
 2847 
 2848 void MacroAssembler::fpop() {
 2849   ffree();
 2850   fincstp();
 2851 }
 2852 
 2853 void MacroAssembler::fremr(Register tmp) {
 2854   save_rax(tmp);
 2855   { Label L;
 2856     bind(L);
 2857     fprem();
 2858     fwait(); fnstsw_ax();
 2859     sahf();
 2860     jcc(Assembler::parity, L);
 2861   }
 2862   restore_rax(tmp);
 2863   // Result is in ST0.
 2864   // Note: fxch &amp; fpop to get rid of ST1
 2865   // (otherwise FPU stack could overflow eventually)
 2866   fxch(1);
 2867   fpop();
 2868 }
 2869 #endif // !LP64
 2870 
 2871 void MacroAssembler::mulpd(XMMRegister dst, AddressLiteral src) {
 2872   if (reachable(src)) {
 2873     Assembler::mulpd(dst, as_Address(src));
 2874   } else {
 2875     lea(rscratch1, src);
 2876     Assembler::mulpd(dst, Address(rscratch1, 0));
 2877   }
 2878 }
 2879 
 2880 void MacroAssembler::load_float(Address src) {
 2881   if (UseSSE &gt;= 1) {
 2882     movflt(xmm0, src);
 2883   } else {
 2884     LP64_ONLY(ShouldNotReachHere());
 2885     NOT_LP64(fld_s(src));
 2886   }
 2887 }
 2888 
 2889 void MacroAssembler::store_float(Address dst) {
 2890   if (UseSSE &gt;= 1) {
 2891     movflt(dst, xmm0);
 2892   } else {
 2893     LP64_ONLY(ShouldNotReachHere());
 2894     NOT_LP64(fstp_s(dst));
 2895   }
 2896 }
 2897 
 2898 void MacroAssembler::load_double(Address src) {
 2899   if (UseSSE &gt;= 2) {
 2900     movdbl(xmm0, src);
 2901   } else {
 2902     LP64_ONLY(ShouldNotReachHere());
 2903     NOT_LP64(fld_d(src));
 2904   }
 2905 }
 2906 
 2907 void MacroAssembler::store_double(Address dst) {
 2908   if (UseSSE &gt;= 2) {
 2909     movdbl(dst, xmm0);
 2910   } else {
 2911     LP64_ONLY(ShouldNotReachHere());
 2912     NOT_LP64(fstp_d(dst));
 2913   }
 2914 }
 2915 
 2916 // dst = c = a * b + c
 2917 void MacroAssembler::fmad(XMMRegister dst, XMMRegister a, XMMRegister b, XMMRegister c) {
 2918   Assembler::vfmadd231sd(c, a, b);
 2919   if (dst != c) {
 2920     movdbl(dst, c);
 2921   }
 2922 }
 2923 
 2924 // dst = c = a * b + c
 2925 void MacroAssembler::fmaf(XMMRegister dst, XMMRegister a, XMMRegister b, XMMRegister c) {
 2926   Assembler::vfmadd231ss(c, a, b);
 2927   if (dst != c) {
 2928     movflt(dst, c);
 2929   }
 2930 }
 2931 
 2932 // dst = c = a * b + c
 2933 void MacroAssembler::vfmad(XMMRegister dst, XMMRegister a, XMMRegister b, XMMRegister c, int vector_len) {
 2934   Assembler::vfmadd231pd(c, a, b, vector_len);
 2935   if (dst != c) {
 2936     vmovdqu(dst, c);
 2937   }
 2938 }
 2939 
 2940 // dst = c = a * b + c
 2941 void MacroAssembler::vfmaf(XMMRegister dst, XMMRegister a, XMMRegister b, XMMRegister c, int vector_len) {
 2942   Assembler::vfmadd231ps(c, a, b, vector_len);
 2943   if (dst != c) {
 2944     vmovdqu(dst, c);
 2945   }
 2946 }
 2947 
 2948 // dst = c = a * b + c
 2949 void MacroAssembler::vfmad(XMMRegister dst, XMMRegister a, Address b, XMMRegister c, int vector_len) {
 2950   Assembler::vfmadd231pd(c, a, b, vector_len);
 2951   if (dst != c) {
 2952     vmovdqu(dst, c);
 2953   }
 2954 }
 2955 
 2956 // dst = c = a * b + c
 2957 void MacroAssembler::vfmaf(XMMRegister dst, XMMRegister a, Address b, XMMRegister c, int vector_len) {
 2958   Assembler::vfmadd231ps(c, a, b, vector_len);
 2959   if (dst != c) {
 2960     vmovdqu(dst, c);
 2961   }
 2962 }
 2963 
 2964 void MacroAssembler::incrementl(AddressLiteral dst) {
 2965   if (reachable(dst)) {
 2966     incrementl(as_Address(dst));
 2967   } else {
 2968     lea(rscratch1, dst);
 2969     incrementl(Address(rscratch1, 0));
 2970   }
 2971 }
 2972 
 2973 void MacroAssembler::incrementl(ArrayAddress dst) {
 2974   incrementl(as_Address(dst));
 2975 }
 2976 
 2977 void MacroAssembler::incrementl(Register reg, int value) {
 2978   if (value == min_jint) {addl(reg, value) ; return; }
 2979   if (value &lt;  0) { decrementl(reg, -value); return; }
 2980   if (value == 0) {                        ; return; }
 2981   if (value == 1 &amp;&amp; UseIncDec) { incl(reg) ; return; }
 2982   /* else */      { addl(reg, value)       ; return; }
 2983 }
 2984 
 2985 void MacroAssembler::incrementl(Address dst, int value) {
 2986   if (value == min_jint) {addl(dst, value) ; return; }
 2987   if (value &lt;  0) { decrementl(dst, -value); return; }
 2988   if (value == 0) {                        ; return; }
 2989   if (value == 1 &amp;&amp; UseIncDec) { incl(dst) ; return; }
 2990   /* else */      { addl(dst, value)       ; return; }
 2991 }
 2992 
 2993 void MacroAssembler::jump(AddressLiteral dst) {
 2994   if (reachable(dst)) {
 2995     jmp_literal(dst.target(), dst.rspec());
 2996   } else {
 2997     lea(rscratch1, dst);
 2998     jmp(rscratch1);
 2999   }
 3000 }
 3001 
 3002 void MacroAssembler::jump_cc(Condition cc, AddressLiteral dst) {
 3003   if (reachable(dst)) {
 3004     InstructionMark im(this);
 3005     relocate(dst.reloc());
 3006     const int short_size = 2;
 3007     const int long_size = 6;
 3008     int offs = (intptr_t)dst.target() - ((intptr_t)pc());
 3009     if (dst.reloc() == relocInfo::none &amp;&amp; is8bit(offs - short_size)) {
 3010       // 0111 tttn #8-bit disp
 3011       emit_int8(0x70 | cc);
 3012       emit_int8((offs - short_size) &amp; 0xFF);
 3013     } else {
 3014       // 0000 1111 1000 tttn #32-bit disp
 3015       emit_int8(0x0F);
 3016       emit_int8((unsigned char)(0x80 | cc));
 3017       emit_int32(offs - long_size);
 3018     }
 3019   } else {
 3020 #ifdef ASSERT
 3021     warning(&quot;reversing conditional branch&quot;);
 3022 #endif /* ASSERT */
 3023     Label skip;
 3024     jccb(reverse[cc], skip);
 3025     lea(rscratch1, dst);
 3026     Assembler::jmp(rscratch1);
 3027     bind(skip);
 3028   }
 3029 }
 3030 
 3031 void MacroAssembler::ldmxcsr(AddressLiteral src) {
 3032   if (reachable(src)) {
 3033     Assembler::ldmxcsr(as_Address(src));
 3034   } else {
 3035     lea(rscratch1, src);
 3036     Assembler::ldmxcsr(Address(rscratch1, 0));
 3037   }
 3038 }
 3039 
 3040 int MacroAssembler::load_signed_byte(Register dst, Address src) {
 3041   int off;
 3042   if (LP64_ONLY(true ||) VM_Version::is_P6()) {
 3043     off = offset();
 3044     movsbl(dst, src); // movsxb
 3045   } else {
 3046     off = load_unsigned_byte(dst, src);
 3047     shll(dst, 24);
 3048     sarl(dst, 24);
 3049   }
 3050   return off;
 3051 }
 3052 
 3053 // Note: load_signed_short used to be called load_signed_word.
 3054 // Although the &#39;w&#39; in x86 opcodes refers to the term &quot;word&quot; in the assembler
 3055 // manual, which means 16 bits, that usage is found nowhere in HotSpot code.
 3056 // The term &quot;word&quot; in HotSpot means a 32- or 64-bit machine word.
 3057 int MacroAssembler::load_signed_short(Register dst, Address src) {
 3058   int off;
 3059   if (LP64_ONLY(true ||) VM_Version::is_P6()) {
 3060     // This is dubious to me since it seems safe to do a signed 16 =&gt; 64 bit
 3061     // version but this is what 64bit has always done. This seems to imply
 3062     // that users are only using 32bits worth.
 3063     off = offset();
 3064     movswl(dst, src); // movsxw
 3065   } else {
 3066     off = load_unsigned_short(dst, src);
 3067     shll(dst, 16);
 3068     sarl(dst, 16);
 3069   }
 3070   return off;
 3071 }
 3072 
 3073 int MacroAssembler::load_unsigned_byte(Register dst, Address src) {
 3074   // According to Intel Doc. AP-526, &quot;Zero-Extension of Short&quot;, p.16,
 3075   // and &quot;3.9 Partial Register Penalties&quot;, p. 22).
 3076   int off;
 3077   if (LP64_ONLY(true || ) VM_Version::is_P6() || src.uses(dst)) {
 3078     off = offset();
 3079     movzbl(dst, src); // movzxb
 3080   } else {
 3081     xorl(dst, dst);
 3082     off = offset();
 3083     movb(dst, src);
 3084   }
 3085   return off;
 3086 }
 3087 
 3088 // Note: load_unsigned_short used to be called load_unsigned_word.
 3089 int MacroAssembler::load_unsigned_short(Register dst, Address src) {
 3090   // According to Intel Doc. AP-526, &quot;Zero-Extension of Short&quot;, p.16,
 3091   // and &quot;3.9 Partial Register Penalties&quot;, p. 22).
 3092   int off;
 3093   if (LP64_ONLY(true ||) VM_Version::is_P6() || src.uses(dst)) {
 3094     off = offset();
 3095     movzwl(dst, src); // movzxw
 3096   } else {
 3097     xorl(dst, dst);
 3098     off = offset();
 3099     movw(dst, src);
 3100   }
 3101   return off;
 3102 }
 3103 
 3104 void MacroAssembler::load_sized_value(Register dst, Address src, size_t size_in_bytes, bool is_signed, Register dst2) {
 3105   switch (size_in_bytes) {
 3106 #ifndef _LP64
 3107   case  8:
 3108     assert(dst2 != noreg, &quot;second dest register required&quot;);
 3109     movl(dst,  src);
 3110     movl(dst2, src.plus_disp(BytesPerInt));
 3111     break;
 3112 #else
 3113   case  8:  movq(dst, src); break;
 3114 #endif
 3115   case  4:  movl(dst, src); break;
 3116   case  2:  is_signed ? load_signed_short(dst, src) : load_unsigned_short(dst, src); break;
 3117   case  1:  is_signed ? load_signed_byte( dst, src) : load_unsigned_byte( dst, src); break;
 3118   default:  ShouldNotReachHere();
 3119   }
 3120 }
 3121 
 3122 void MacroAssembler::store_sized_value(Address dst, Register src, size_t size_in_bytes, Register src2) {
 3123   switch (size_in_bytes) {
 3124 #ifndef _LP64
 3125   case  8:
 3126     assert(src2 != noreg, &quot;second source register required&quot;);
 3127     movl(dst,                        src);
 3128     movl(dst.plus_disp(BytesPerInt), src2);
 3129     break;
 3130 #else
 3131   case  8:  movq(dst, src); break;
 3132 #endif
 3133   case  4:  movl(dst, src); break;
 3134   case  2:  movw(dst, src); break;
 3135   case  1:  movb(dst, src); break;
 3136   default:  ShouldNotReachHere();
 3137   }
 3138 }
 3139 
 3140 void MacroAssembler::mov32(AddressLiteral dst, Register src) {
 3141   if (reachable(dst)) {
 3142     movl(as_Address(dst), src);
 3143   } else {
 3144     lea(rscratch1, dst);
 3145     movl(Address(rscratch1, 0), src);
 3146   }
 3147 }
 3148 
 3149 void MacroAssembler::mov32(Register dst, AddressLiteral src) {
 3150   if (reachable(src)) {
 3151     movl(dst, as_Address(src));
 3152   } else {
 3153     lea(rscratch1, src);
 3154     movl(dst, Address(rscratch1, 0));
 3155   }
 3156 }
 3157 
 3158 // C++ bool manipulation
 3159 
 3160 void MacroAssembler::movbool(Register dst, Address src) {
 3161   if(sizeof(bool) == 1)
 3162     movb(dst, src);
 3163   else if(sizeof(bool) == 2)
 3164     movw(dst, src);
 3165   else if(sizeof(bool) == 4)
 3166     movl(dst, src);
 3167   else
 3168     // unsupported
 3169     ShouldNotReachHere();
 3170 }
 3171 
 3172 void MacroAssembler::movbool(Address dst, bool boolconst) {
 3173   if(sizeof(bool) == 1)
 3174     movb(dst, (int) boolconst);
 3175   else if(sizeof(bool) == 2)
 3176     movw(dst, (int) boolconst);
 3177   else if(sizeof(bool) == 4)
 3178     movl(dst, (int) boolconst);
 3179   else
 3180     // unsupported
 3181     ShouldNotReachHere();
 3182 }
 3183 
 3184 void MacroAssembler::movbool(Address dst, Register src) {
 3185   if(sizeof(bool) == 1)
 3186     movb(dst, src);
 3187   else if(sizeof(bool) == 2)
 3188     movw(dst, src);
 3189   else if(sizeof(bool) == 4)
 3190     movl(dst, src);
 3191   else
 3192     // unsupported
 3193     ShouldNotReachHere();
 3194 }
 3195 
 3196 void MacroAssembler::movbyte(ArrayAddress dst, int src) {
 3197   movb(as_Address(dst), src);
 3198 }
 3199 
 3200 void MacroAssembler::movdl(XMMRegister dst, AddressLiteral src) {
 3201   if (reachable(src)) {
 3202     movdl(dst, as_Address(src));
 3203   } else {
 3204     lea(rscratch1, src);
 3205     movdl(dst, Address(rscratch1, 0));
 3206   }
 3207 }
 3208 
 3209 void MacroAssembler::movq(XMMRegister dst, AddressLiteral src) {
 3210   if (reachable(src)) {
 3211     movq(dst, as_Address(src));
 3212   } else {
 3213     lea(rscratch1, src);
 3214     movq(dst, Address(rscratch1, 0));
 3215   }
 3216 }
 3217 
 3218 #ifdef COMPILER2
 3219 void MacroAssembler::setvectmask(Register dst, Register src) {
 3220   guarantee(PostLoopMultiversioning, &quot;must be&quot;);
 3221   Assembler::movl(dst, 1);
 3222   Assembler::shlxl(dst, dst, src);
 3223   Assembler::decl(dst);
 3224   Assembler::kmovdl(k1, dst);
 3225   Assembler::movl(dst, src);
 3226 }
 3227 
 3228 void MacroAssembler::restorevectmask() {
 3229   guarantee(PostLoopMultiversioning, &quot;must be&quot;);
 3230   Assembler::knotwl(k1, k0);
 3231 }
 3232 #endif // COMPILER2
 3233 
 3234 void MacroAssembler::movdbl(XMMRegister dst, AddressLiteral src) {
 3235   if (reachable(src)) {
 3236     if (UseXmmLoadAndClearUpper) {
 3237       movsd (dst, as_Address(src));
 3238     } else {
 3239       movlpd(dst, as_Address(src));
 3240     }
 3241   } else {
 3242     lea(rscratch1, src);
 3243     if (UseXmmLoadAndClearUpper) {
 3244       movsd (dst, Address(rscratch1, 0));
 3245     } else {
 3246       movlpd(dst, Address(rscratch1, 0));
 3247     }
 3248   }
 3249 }
 3250 
 3251 void MacroAssembler::movflt(XMMRegister dst, AddressLiteral src) {
 3252   if (reachable(src)) {
 3253     movss(dst, as_Address(src));
 3254   } else {
 3255     lea(rscratch1, src);
 3256     movss(dst, Address(rscratch1, 0));
 3257   }
 3258 }
 3259 
 3260 void MacroAssembler::movptr(Register dst, Register src) {
 3261   LP64_ONLY(movq(dst, src)) NOT_LP64(movl(dst, src));
 3262 }
 3263 
 3264 void MacroAssembler::movptr(Register dst, Address src) {
 3265   LP64_ONLY(movq(dst, src)) NOT_LP64(movl(dst, src));
 3266 }
 3267 
 3268 // src should NEVER be a real pointer. Use AddressLiteral for true pointers
 3269 void MacroAssembler::movptr(Register dst, intptr_t src) {
 3270   LP64_ONLY(mov64(dst, src)) NOT_LP64(movl(dst, src));
 3271 }
 3272 
 3273 void MacroAssembler::movptr(Address dst, Register src) {
 3274   LP64_ONLY(movq(dst, src)) NOT_LP64(movl(dst, src));
 3275 }
 3276 
 3277 void MacroAssembler::movdqu(Address dst, XMMRegister src) {
 3278     assert(((src-&gt;encoding() &lt; 16) || VM_Version::supports_avx512vl()),&quot;XMM register should be 0-15&quot;);
 3279     Assembler::movdqu(dst, src);
 3280 }
 3281 
 3282 void MacroAssembler::movdqu(XMMRegister dst, Address src) {
 3283     assert(((dst-&gt;encoding() &lt; 16) || VM_Version::supports_avx512vl()),&quot;XMM register should be 0-15&quot;);
 3284     Assembler::movdqu(dst, src);
 3285 }
 3286 
 3287 void MacroAssembler::movdqu(XMMRegister dst, XMMRegister src) {
 3288     assert(((dst-&gt;encoding() &lt; 16  &amp;&amp; src-&gt;encoding() &lt; 16) || VM_Version::supports_avx512vl()),&quot;XMM register should be 0-15&quot;);
 3289     Assembler::movdqu(dst, src);
 3290 }
 3291 
 3292 void MacroAssembler::movdqu(XMMRegister dst, AddressLiteral src, Register scratchReg) {
 3293   if (reachable(src)) {
 3294     movdqu(dst, as_Address(src));
 3295   } else {
 3296     lea(scratchReg, src);
 3297     movdqu(dst, Address(scratchReg, 0));
 3298   }
 3299 }
 3300 
 3301 void MacroAssembler::vmovdqu(Address dst, XMMRegister src) {
 3302     assert(((src-&gt;encoding() &lt; 16) || VM_Version::supports_avx512vl()),&quot;XMM register should be 0-15&quot;);
 3303     Assembler::vmovdqu(dst, src);
 3304 }
 3305 
 3306 void MacroAssembler::vmovdqu(XMMRegister dst, Address src) {
 3307     assert(((dst-&gt;encoding() &lt; 16) || VM_Version::supports_avx512vl()),&quot;XMM register should be 0-15&quot;);
 3308     Assembler::vmovdqu(dst, src);
 3309 }
 3310 
 3311 void MacroAssembler::vmovdqu(XMMRegister dst, XMMRegister src) {
 3312     assert(((dst-&gt;encoding() &lt; 16  &amp;&amp; src-&gt;encoding() &lt; 16) || VM_Version::supports_avx512vl()),&quot;XMM register should be 0-15&quot;);
 3313     Assembler::vmovdqu(dst, src);
 3314 }
 3315 
 3316 void MacroAssembler::vmovdqu(XMMRegister dst, AddressLiteral src, Register scratch_reg) {
 3317   if (reachable(src)) {
 3318     vmovdqu(dst, as_Address(src));
 3319   }
 3320   else {
 3321     lea(scratch_reg, src);
 3322     vmovdqu(dst, Address(scratch_reg, 0));
 3323   }
 3324 }
 3325 
 3326 void MacroAssembler::evmovdquq(XMMRegister dst, AddressLiteral src, int vector_len, Register rscratch) {
 3327   if (reachable(src)) {
 3328     Assembler::evmovdquq(dst, as_Address(src), vector_len);
 3329   } else {
 3330     lea(rscratch, src);
 3331     Assembler::evmovdquq(dst, Address(rscratch, 0), vector_len);
 3332   }
 3333 }
 3334 
 3335 void MacroAssembler::movdqa(XMMRegister dst, AddressLiteral src) {
 3336   if (reachable(src)) {
 3337     Assembler::movdqa(dst, as_Address(src));
 3338   } else {
 3339     lea(rscratch1, src);
 3340     Assembler::movdqa(dst, Address(rscratch1, 0));
 3341   }
 3342 }
 3343 
 3344 void MacroAssembler::movsd(XMMRegister dst, AddressLiteral src) {
 3345   if (reachable(src)) {
 3346     Assembler::movsd(dst, as_Address(src));
 3347   } else {
 3348     lea(rscratch1, src);
 3349     Assembler::movsd(dst, Address(rscratch1, 0));
 3350   }
 3351 }
 3352 
 3353 void MacroAssembler::movss(XMMRegister dst, AddressLiteral src) {
 3354   if (reachable(src)) {
 3355     Assembler::movss(dst, as_Address(src));
 3356   } else {
 3357     lea(rscratch1, src);
 3358     Assembler::movss(dst, Address(rscratch1, 0));
 3359   }
 3360 }
 3361 
 3362 void MacroAssembler::mulsd(XMMRegister dst, AddressLiteral src) {
 3363   if (reachable(src)) {
 3364     Assembler::mulsd(dst, as_Address(src));
 3365   } else {
 3366     lea(rscratch1, src);
 3367     Assembler::mulsd(dst, Address(rscratch1, 0));
 3368   }
 3369 }
 3370 
 3371 void MacroAssembler::mulss(XMMRegister dst, AddressLiteral src) {
 3372   if (reachable(src)) {
 3373     Assembler::mulss(dst, as_Address(src));
 3374   } else {
 3375     lea(rscratch1, src);
 3376     Assembler::mulss(dst, Address(rscratch1, 0));
 3377   }
 3378 }
 3379 
 3380 void MacroAssembler::null_check(Register reg, int offset) {
 3381   if (needs_explicit_null_check(offset)) {
 3382     // provoke OS NULL exception if reg = NULL by
 3383     // accessing M[reg] w/o changing any (non-CC) registers
 3384     // NOTE: cmpl is plenty here to provoke a segv
 3385     cmpptr(rax, Address(reg, 0));
 3386     // Note: should probably use testl(rax, Address(reg, 0));
 3387     //       may be shorter code (however, this version of
 3388     //       testl needs to be implemented first)
 3389   } else {
 3390     // nothing to do, (later) access of M[reg + offset]
 3391     // will provoke OS NULL exception if reg = NULL
 3392   }
 3393 }
 3394 
 3395 void MacroAssembler::test_klass_is_value(Register klass, Register temp_reg, Label&amp; is_value) {
 3396   movl(temp_reg, Address(klass, Klass::access_flags_offset()));
 3397   testl(temp_reg, JVM_ACC_VALUE);
 3398   jcc(Assembler::notZero, is_value);
 3399 }
 3400 
 3401 void MacroAssembler::test_klass_is_empty_value(Register klass, Register temp_reg, Label&amp; is_empty_value) {
 3402 #ifdef ASSERT
 3403   {
 3404     Label done_check;
 3405     test_klass_is_value(klass, temp_reg, done_check);
 3406     stop(&quot;test_klass_is_empty_value with none value klass&quot;);
 3407     bind(done_check);
 3408   }
 3409 #endif
 3410   movl(temp_reg, Address(klass, InstanceKlass::misc_flags_offset()));
 3411   testl(temp_reg, InstanceKlass::misc_flags_is_empty_value());
 3412   jcc(Assembler::notZero, is_empty_value);
 3413 }
 3414 
 3415 void MacroAssembler::test_field_is_flattenable(Register flags, Register temp_reg, Label&amp; is_flattenable) {
 3416   movl(temp_reg, flags);
 3417   shrl(temp_reg, ConstantPoolCacheEntry::is_flattenable_field_shift);
 3418   andl(temp_reg, 0x1);
 3419   testl(temp_reg, temp_reg);
 3420   jcc(Assembler::notZero, is_flattenable);
 3421 }
 3422 
 3423 void MacroAssembler::test_field_is_not_flattenable(Register flags, Register temp_reg, Label&amp; notFlattenable) {
 3424   movl(temp_reg, flags);
 3425   shrl(temp_reg, ConstantPoolCacheEntry::is_flattenable_field_shift);
 3426   andl(temp_reg, 0x1);
 3427   testl(temp_reg, temp_reg);
 3428   jcc(Assembler::zero, notFlattenable);
 3429 }
 3430 
 3431 void MacroAssembler::test_field_is_flattened(Register flags, Register temp_reg, Label&amp; is_flattened) {
 3432   movl(temp_reg, flags);
 3433   shrl(temp_reg, ConstantPoolCacheEntry::is_flattened_field_shift);
 3434   andl(temp_reg, 0x1);
 3435   testl(temp_reg, temp_reg);
 3436   jcc(Assembler::notZero, is_flattened);
 3437 }
 3438 
 3439 void MacroAssembler::test_flattened_array_oop(Register oop, Register temp_reg,
 3440                                               Label&amp;is_flattened_array) {
 3441   load_storage_props(temp_reg, oop);
 3442   testb(temp_reg, ArrayStorageProperties::flattened_value);
 3443   jcc(Assembler::notZero, is_flattened_array);
 3444 }
 3445 
 3446 void MacroAssembler::test_non_flattened_array_oop(Register oop, Register temp_reg,
 3447                                                   Label&amp;is_non_flattened_array) {
 3448   load_storage_props(temp_reg, oop);
 3449   testb(temp_reg, ArrayStorageProperties::flattened_value);
 3450   jcc(Assembler::zero, is_non_flattened_array);
 3451 }
 3452 
 3453 void MacroAssembler::test_null_free_array_oop(Register oop, Register temp_reg, Label&amp;is_null_free_array) {
 3454   load_storage_props(temp_reg, oop);
 3455   testb(temp_reg, ArrayStorageProperties::null_free_value);
 3456   jcc(Assembler::notZero, is_null_free_array);
 3457 }
 3458 
 3459 void MacroAssembler::test_non_null_free_array_oop(Register oop, Register temp_reg, Label&amp;is_non_null_free_array) {
 3460   load_storage_props(temp_reg, oop);
 3461   testb(temp_reg, ArrayStorageProperties::null_free_value);
 3462   jcc(Assembler::zero, is_non_null_free_array);
 3463 }
 3464 
 3465 void MacroAssembler::os_breakpoint() {
 3466   // instead of directly emitting a breakpoint, call os:breakpoint for better debugability
 3467   // (e.g., MSVC can&#39;t call ps() otherwise)
 3468   call(RuntimeAddress(CAST_FROM_FN_PTR(address, os::breakpoint)));
 3469 }
 3470 
 3471 void MacroAssembler::unimplemented(const char* what) {
 3472   const char* buf = NULL;
 3473   {
 3474     ResourceMark rm;
 3475     stringStream ss;
 3476     ss.print(&quot;unimplemented: %s&quot;, what);
 3477     buf = code_string(ss.as_string());
 3478   }
 3479   stop(buf);
 3480 }
 3481 
 3482 #ifdef _LP64
 3483 #define XSTATE_BV 0x200
 3484 #endif
 3485 
 3486 void MacroAssembler::pop_CPU_state() {
 3487   pop_FPU_state();
 3488   pop_IU_state();
 3489 }
 3490 
 3491 void MacroAssembler::pop_FPU_state() {
 3492 #ifndef _LP64
 3493   frstor(Address(rsp, 0));
 3494 #else
 3495   fxrstor(Address(rsp, 0));
 3496 #endif
 3497   addptr(rsp, FPUStateSizeInWords * wordSize);
 3498 }
 3499 
 3500 void MacroAssembler::pop_IU_state() {
 3501   popa();
 3502   LP64_ONLY(addq(rsp, 8));
 3503   popf();
 3504 }
 3505 
 3506 // Save Integer and Float state
 3507 // Warning: Stack must be 16 byte aligned (64bit)
 3508 void MacroAssembler::push_CPU_state() {
 3509   push_IU_state();
 3510   push_FPU_state();
 3511 }
 3512 
 3513 void MacroAssembler::push_FPU_state() {
 3514   subptr(rsp, FPUStateSizeInWords * wordSize);
 3515 #ifndef _LP64
 3516   fnsave(Address(rsp, 0));
 3517   fwait();
 3518 #else
 3519   fxsave(Address(rsp, 0));
 3520 #endif // LP64
 3521 }
 3522 
 3523 void MacroAssembler::push_IU_state() {
 3524   // Push flags first because pusha kills them
 3525   pushf();
 3526   // Make sure rsp stays 16-byte aligned
 3527   LP64_ONLY(subq(rsp, 8));
 3528   pusha();
 3529 }
 3530 
 3531 void MacroAssembler::reset_last_Java_frame(Register java_thread, bool clear_fp) { // determine java_thread register
 3532   if (!java_thread-&gt;is_valid()) {
 3533     java_thread = rdi;
 3534     get_thread(java_thread);
 3535   }
 3536   // we must set sp to zero to clear frame
 3537   movptr(Address(java_thread, JavaThread::last_Java_sp_offset()), NULL_WORD);
 3538   if (clear_fp) {
 3539     movptr(Address(java_thread, JavaThread::last_Java_fp_offset()), NULL_WORD);
 3540   }
 3541 
 3542   // Always clear the pc because it could have been set by make_walkable()
 3543   movptr(Address(java_thread, JavaThread::last_Java_pc_offset()), NULL_WORD);
 3544 
 3545   vzeroupper();
 3546 }
 3547 
 3548 void MacroAssembler::restore_rax(Register tmp) {
 3549   if (tmp == noreg) pop(rax);
 3550   else if (tmp != rax) mov(rax, tmp);
 3551 }
 3552 
 3553 void MacroAssembler::round_to(Register reg, int modulus) {
 3554   addptr(reg, modulus - 1);
 3555   andptr(reg, -modulus);
 3556 }
 3557 
 3558 void MacroAssembler::save_rax(Register tmp) {
 3559   if (tmp == noreg) push(rax);
 3560   else if (tmp != rax) mov(tmp, rax);
 3561 }
 3562 
 3563 void MacroAssembler::safepoint_poll(Label&amp; slow_path, Register thread_reg, Register temp_reg) {
 3564   if (SafepointMechanism::uses_thread_local_poll()) {
 3565 #ifdef _LP64
 3566     assert(thread_reg == r15_thread, &quot;should be&quot;);
 3567 #else
 3568     if (thread_reg == noreg) {
 3569       thread_reg = temp_reg;
 3570       get_thread(thread_reg);
 3571     }
 3572 #endif
 3573     testb(Address(thread_reg, Thread::polling_page_offset()), SafepointMechanism::poll_bit());
 3574     jcc(Assembler::notZero, slow_path); // handshake bit set implies poll
 3575   } else {
 3576     cmp32(ExternalAddress(SafepointSynchronize::address_of_state()),
 3577         SafepointSynchronize::_not_synchronized);
 3578     jcc(Assembler::notEqual, slow_path);
 3579   }
 3580 }
 3581 
 3582 // Calls to C land
 3583 //
 3584 // When entering C land, the rbp, &amp; rsp of the last Java frame have to be recorded
 3585 // in the (thread-local) JavaThread object. When leaving C land, the last Java fp
 3586 // has to be reset to 0. This is required to allow proper stack traversal.
 3587 void MacroAssembler::set_last_Java_frame(Register java_thread,
 3588                                          Register last_java_sp,
 3589                                          Register last_java_fp,
 3590                                          address  last_java_pc) {
 3591   vzeroupper();
 3592   // determine java_thread register
 3593   if (!java_thread-&gt;is_valid()) {
 3594     java_thread = rdi;
 3595     get_thread(java_thread);
 3596   }
 3597   // determine last_java_sp register
 3598   if (!last_java_sp-&gt;is_valid()) {
 3599     last_java_sp = rsp;
 3600   }
 3601 
 3602   // last_java_fp is optional
 3603 
 3604   if (last_java_fp-&gt;is_valid()) {
 3605     movptr(Address(java_thread, JavaThread::last_Java_fp_offset()), last_java_fp);
 3606   }
 3607 
 3608   // last_java_pc is optional
 3609 
 3610   if (last_java_pc != NULL) {
 3611     lea(Address(java_thread,
 3612                  JavaThread::frame_anchor_offset() + JavaFrameAnchor::last_Java_pc_offset()),
 3613         InternalAddress(last_java_pc));
 3614 
 3615   }
 3616   movptr(Address(java_thread, JavaThread::last_Java_sp_offset()), last_java_sp);
 3617 }
 3618 
 3619 void MacroAssembler::shlptr(Register dst, int imm8) {
 3620   LP64_ONLY(shlq(dst, imm8)) NOT_LP64(shll(dst, imm8));
 3621 }
 3622 
 3623 void MacroAssembler::shrptr(Register dst, int imm8) {
 3624   LP64_ONLY(shrq(dst, imm8)) NOT_LP64(shrl(dst, imm8));
 3625 }
 3626 
 3627 void MacroAssembler::sign_extend_byte(Register reg) {
 3628   if (LP64_ONLY(true ||) (VM_Version::is_P6() &amp;&amp; reg-&gt;has_byte_register())) {
 3629     movsbl(reg, reg); // movsxb
 3630   } else {
 3631     shll(reg, 24);
 3632     sarl(reg, 24);
 3633   }
 3634 }
 3635 
 3636 void MacroAssembler::sign_extend_short(Register reg) {
 3637   if (LP64_ONLY(true ||) VM_Version::is_P6()) {
 3638     movswl(reg, reg); // movsxw
 3639   } else {
 3640     shll(reg, 16);
 3641     sarl(reg, 16);
 3642   }
 3643 }
 3644 
 3645 void MacroAssembler::testl(Register dst, AddressLiteral src) {
 3646   assert(reachable(src), &quot;Address should be reachable&quot;);
 3647   testl(dst, as_Address(src));
 3648 }
 3649 
 3650 void MacroAssembler::pcmpeqb(XMMRegister dst, XMMRegister src) {
 3651   assert(((dst-&gt;encoding() &lt; 16 &amp;&amp; src-&gt;encoding() &lt; 16) || VM_Version::supports_avx512vlbw()),&quot;XMM register should be 0-15&quot;);
 3652   Assembler::pcmpeqb(dst, src);
 3653 }
 3654 
 3655 void MacroAssembler::pcmpeqw(XMMRegister dst, XMMRegister src) {
 3656   assert(((dst-&gt;encoding() &lt; 16 &amp;&amp; src-&gt;encoding() &lt; 16) || VM_Version::supports_avx512vlbw()),&quot;XMM register should be 0-15&quot;);
 3657   Assembler::pcmpeqw(dst, src);
 3658 }
 3659 
 3660 void MacroAssembler::pcmpestri(XMMRegister dst, Address src, int imm8) {
 3661   assert((dst-&gt;encoding() &lt; 16),&quot;XMM register should be 0-15&quot;);
 3662   Assembler::pcmpestri(dst, src, imm8);
 3663 }
 3664 
 3665 void MacroAssembler::pcmpestri(XMMRegister dst, XMMRegister src, int imm8) {
 3666   assert((dst-&gt;encoding() &lt; 16 &amp;&amp; src-&gt;encoding() &lt; 16),&quot;XMM register should be 0-15&quot;);
 3667   Assembler::pcmpestri(dst, src, imm8);
 3668 }
 3669 
 3670 void MacroAssembler::pmovzxbw(XMMRegister dst, XMMRegister src) {
 3671   assert(((dst-&gt;encoding() &lt; 16 &amp;&amp; src-&gt;encoding() &lt; 16) || VM_Version::supports_avx512vlbw()),&quot;XMM register should be 0-15&quot;);
 3672   Assembler::pmovzxbw(dst, src);
 3673 }
 3674 
 3675 void MacroAssembler::pmovzxbw(XMMRegister dst, Address src) {
 3676   assert(((dst-&gt;encoding() &lt; 16) || VM_Version::supports_avx512vlbw()),&quot;XMM register should be 0-15&quot;);
 3677   Assembler::pmovzxbw(dst, src);
 3678 }
 3679 
 3680 void MacroAssembler::pmovmskb(Register dst, XMMRegister src) {
 3681   assert((src-&gt;encoding() &lt; 16),&quot;XMM register should be 0-15&quot;);
 3682   Assembler::pmovmskb(dst, src);
 3683 }
 3684 
 3685 void MacroAssembler::ptest(XMMRegister dst, XMMRegister src) {
 3686   assert((dst-&gt;encoding() &lt; 16 &amp;&amp; src-&gt;encoding() &lt; 16),&quot;XMM register should be 0-15&quot;);
 3687   Assembler::ptest(dst, src);
 3688 }
 3689 
 3690 void MacroAssembler::sqrtsd(XMMRegister dst, AddressLiteral src) {
 3691   if (reachable(src)) {
 3692     Assembler::sqrtsd(dst, as_Address(src));
 3693   } else {
 3694     lea(rscratch1, src);
 3695     Assembler::sqrtsd(dst, Address(rscratch1, 0));
 3696   }
 3697 }
 3698 
 3699 void MacroAssembler::sqrtss(XMMRegister dst, AddressLiteral src) {
 3700   if (reachable(src)) {
 3701     Assembler::sqrtss(dst, as_Address(src));
 3702   } else {
 3703     lea(rscratch1, src);
 3704     Assembler::sqrtss(dst, Address(rscratch1, 0));
 3705   }
 3706 }
 3707 
 3708 void MacroAssembler::subsd(XMMRegister dst, AddressLiteral src) {
 3709   if (reachable(src)) {
 3710     Assembler::subsd(dst, as_Address(src));
 3711   } else {
 3712     lea(rscratch1, src);
 3713     Assembler::subsd(dst, Address(rscratch1, 0));
 3714   }
 3715 }
 3716 
 3717 void MacroAssembler::roundsd(XMMRegister dst, AddressLiteral src, int32_t rmode, Register scratch_reg) {
 3718   if (reachable(src)) {
 3719     Assembler::roundsd(dst, as_Address(src), rmode);
 3720   } else {
 3721     lea(scratch_reg, src);
 3722     Assembler::roundsd(dst, Address(scratch_reg, 0), rmode);
 3723   }
 3724 }
 3725 
 3726 void MacroAssembler::subss(XMMRegister dst, AddressLiteral src) {
 3727   if (reachable(src)) {
 3728     Assembler::subss(dst, as_Address(src));
 3729   } else {
 3730     lea(rscratch1, src);
 3731     Assembler::subss(dst, Address(rscratch1, 0));
 3732   }
 3733 }
 3734 
 3735 void MacroAssembler::ucomisd(XMMRegister dst, AddressLiteral src) {
 3736   if (reachable(src)) {
 3737     Assembler::ucomisd(dst, as_Address(src));
 3738   } else {
 3739     lea(rscratch1, src);
 3740     Assembler::ucomisd(dst, Address(rscratch1, 0));
 3741   }
 3742 }
 3743 
 3744 void MacroAssembler::ucomiss(XMMRegister dst, AddressLiteral src) {
 3745   if (reachable(src)) {
 3746     Assembler::ucomiss(dst, as_Address(src));
 3747   } else {
 3748     lea(rscratch1, src);
 3749     Assembler::ucomiss(dst, Address(rscratch1, 0));
 3750   }
 3751 }
 3752 
 3753 void MacroAssembler::xorpd(XMMRegister dst, AddressLiteral src, Register scratch_reg) {
 3754   // Used in sign-bit flipping with aligned address.
 3755   assert((UseAVX &gt; 0) || (((intptr_t)src.target() &amp; 15) == 0), &quot;SSE mode requires address alignment 16 bytes&quot;);
 3756   if (reachable(src)) {
 3757     Assembler::xorpd(dst, as_Address(src));
 3758   } else {
 3759     lea(scratch_reg, src);
 3760     Assembler::xorpd(dst, Address(scratch_reg, 0));
 3761   }
 3762 }
 3763 
 3764 void MacroAssembler::xorpd(XMMRegister dst, XMMRegister src) {
 3765   if (UseAVX &gt; 2 &amp;&amp; !VM_Version::supports_avx512dq() &amp;&amp; (dst-&gt;encoding() == src-&gt;encoding())) {
 3766     Assembler::vpxor(dst, dst, src, Assembler::AVX_512bit);
 3767   }
 3768   else {
 3769     Assembler::xorpd(dst, src);
 3770   }
 3771 }
 3772 
 3773 void MacroAssembler::xorps(XMMRegister dst, XMMRegister src) {
 3774   if (UseAVX &gt; 2 &amp;&amp; !VM_Version::supports_avx512dq() &amp;&amp; (dst-&gt;encoding() == src-&gt;encoding())) {
 3775     Assembler::vpxor(dst, dst, src, Assembler::AVX_512bit);
 3776   } else {
 3777     Assembler::xorps(dst, src);
 3778   }
 3779 }
 3780 
 3781 void MacroAssembler::xorps(XMMRegister dst, AddressLiteral src, Register scratch_reg) {
 3782   // Used in sign-bit flipping with aligned address.
 3783   assert((UseAVX &gt; 0) || (((intptr_t)src.target() &amp; 15) == 0), &quot;SSE mode requires address alignment 16 bytes&quot;);
 3784   if (reachable(src)) {
 3785     Assembler::xorps(dst, as_Address(src));
 3786   } else {
 3787     lea(scratch_reg, src);
 3788     Assembler::xorps(dst, Address(scratch_reg, 0));
 3789   }
 3790 }
 3791 
 3792 void MacroAssembler::pshufb(XMMRegister dst, AddressLiteral src) {
 3793   // Used in sign-bit flipping with aligned address.
 3794   bool aligned_adr = (((intptr_t)src.target() &amp; 15) == 0);
 3795   assert((UseAVX &gt; 0) || aligned_adr, &quot;SSE mode requires address alignment 16 bytes&quot;);
 3796   if (reachable(src)) {
 3797     Assembler::pshufb(dst, as_Address(src));
 3798   } else {
 3799     lea(rscratch1, src);
 3800     Assembler::pshufb(dst, Address(rscratch1, 0));
 3801   }
 3802 }
 3803 
 3804 // AVX 3-operands instructions
 3805 
 3806 void MacroAssembler::vaddsd(XMMRegister dst, XMMRegister nds, AddressLiteral src) {
 3807   if (reachable(src)) {
 3808     vaddsd(dst, nds, as_Address(src));
 3809   } else {
 3810     lea(rscratch1, src);
 3811     vaddsd(dst, nds, Address(rscratch1, 0));
 3812   }
 3813 }
 3814 
 3815 void MacroAssembler::vaddss(XMMRegister dst, XMMRegister nds, AddressLiteral src) {
 3816   if (reachable(src)) {
 3817     vaddss(dst, nds, as_Address(src));
 3818   } else {
 3819     lea(rscratch1, src);
 3820     vaddss(dst, nds, Address(rscratch1, 0));
 3821   }
 3822 }
 3823 
 3824 void MacroAssembler::vpaddd(XMMRegister dst, XMMRegister nds, AddressLiteral src, int vector_len, Register rscratch) {
 3825   assert(UseAVX &gt; 0, &quot;requires some form of AVX&quot;);
 3826   if (reachable(src)) {
 3827     Assembler::vpaddd(dst, nds, as_Address(src), vector_len);
 3828   } else {
 3829     lea(rscratch, src);
 3830     Assembler::vpaddd(dst, nds, Address(rscratch, 0), vector_len);
 3831   }
 3832 }
 3833 
 3834 void MacroAssembler::vabsss(XMMRegister dst, XMMRegister nds, XMMRegister src, AddressLiteral negate_field, int vector_len) {
 3835   assert(((dst-&gt;encoding() &lt; 16 &amp;&amp; src-&gt;encoding() &lt; 16 &amp;&amp; nds-&gt;encoding() &lt; 16) || VM_Version::supports_avx512vldq()),&quot;XMM register should be 0-15&quot;);
 3836   vandps(dst, nds, negate_field, vector_len);
 3837 }
 3838 
 3839 void MacroAssembler::vabssd(XMMRegister dst, XMMRegister nds, XMMRegister src, AddressLiteral negate_field, int vector_len) {
 3840   assert(((dst-&gt;encoding() &lt; 16 &amp;&amp; src-&gt;encoding() &lt; 16 &amp;&amp; nds-&gt;encoding() &lt; 16) || VM_Version::supports_avx512vldq()),&quot;XMM register should be 0-15&quot;);
 3841   vandpd(dst, nds, negate_field, vector_len);
 3842 }
 3843 
 3844 void MacroAssembler::vpaddb(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {
 3845   assert(((dst-&gt;encoding() &lt; 16 &amp;&amp; src-&gt;encoding() &lt; 16 &amp;&amp; nds-&gt;encoding() &lt; 16) || VM_Version::supports_avx512vlbw()),&quot;XMM register should be 0-15&quot;);
 3846   Assembler::vpaddb(dst, nds, src, vector_len);
 3847 }
 3848 
 3849 void MacroAssembler::vpaddb(XMMRegister dst, XMMRegister nds, Address src, int vector_len) {
 3850   assert(((dst-&gt;encoding() &lt; 16 &amp;&amp; nds-&gt;encoding() &lt; 16) || VM_Version::supports_avx512vlbw()),&quot;XMM register should be 0-15&quot;);
 3851   Assembler::vpaddb(dst, nds, src, vector_len);
 3852 }
 3853 
 3854 void MacroAssembler::vpaddw(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {
 3855   assert(((dst-&gt;encoding() &lt; 16 &amp;&amp; src-&gt;encoding() &lt; 16 &amp;&amp; nds-&gt;encoding() &lt; 16) || VM_Version::supports_avx512vlbw()),&quot;XMM register should be 0-15&quot;);
 3856   Assembler::vpaddw(dst, nds, src, vector_len);
 3857 }
 3858 
 3859 void MacroAssembler::vpaddw(XMMRegister dst, XMMRegister nds, Address src, int vector_len) {
 3860   assert(((dst-&gt;encoding() &lt; 16 &amp;&amp; nds-&gt;encoding() &lt; 16) || VM_Version::supports_avx512vlbw()),&quot;XMM register should be 0-15&quot;);
 3861   Assembler::vpaddw(dst, nds, src, vector_len);
 3862 }
 3863 
 3864 void MacroAssembler::vpand(XMMRegister dst, XMMRegister nds, AddressLiteral src, int vector_len, Register scratch_reg) {
 3865   if (reachable(src)) {
 3866     Assembler::vpand(dst, nds, as_Address(src), vector_len);
 3867   } else {
 3868     lea(scratch_reg, src);
 3869     Assembler::vpand(dst, nds, Address(scratch_reg, 0), vector_len);
 3870   }
 3871 }
 3872 
 3873 void MacroAssembler::vpbroadcastw(XMMRegister dst, XMMRegister src, int vector_len) {
 3874   assert(((dst-&gt;encoding() &lt; 16 &amp;&amp; src-&gt;encoding() &lt; 16) || VM_Version::supports_avx512vlbw()),&quot;XMM register should be 0-15&quot;);
 3875   Assembler::vpbroadcastw(dst, src, vector_len);
 3876 }
 3877 
 3878 void MacroAssembler::vpcmpeqb(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {
 3879   assert(((dst-&gt;encoding() &lt; 16 &amp;&amp; src-&gt;encoding() &lt; 16 &amp;&amp; nds-&gt;encoding() &lt; 16) || VM_Version::supports_avx512vlbw()),&quot;XMM register should be 0-15&quot;);
 3880   Assembler::vpcmpeqb(dst, nds, src, vector_len);
 3881 }
 3882 
 3883 void MacroAssembler::vpcmpeqw(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {
 3884   assert(((dst-&gt;encoding() &lt; 16 &amp;&amp; src-&gt;encoding() &lt; 16 &amp;&amp; nds-&gt;encoding() &lt; 16) || VM_Version::supports_avx512vlbw()),&quot;XMM register should be 0-15&quot;);
 3885   Assembler::vpcmpeqw(dst, nds, src, vector_len);
 3886 }
 3887 
 3888 void MacroAssembler::vpmovzxbw(XMMRegister dst, Address src, int vector_len) {
 3889   assert(((dst-&gt;encoding() &lt; 16) || VM_Version::supports_avx512vlbw()),&quot;XMM register should be 0-15&quot;);
 3890   Assembler::vpmovzxbw(dst, src, vector_len);
 3891 }
 3892 
 3893 void MacroAssembler::vpmovmskb(Register dst, XMMRegister src) {
 3894   assert((src-&gt;encoding() &lt; 16),&quot;XMM register should be 0-15&quot;);
 3895   Assembler::vpmovmskb(dst, src);
 3896 }
 3897 
 3898 void MacroAssembler::vpmullw(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {
 3899   assert(((dst-&gt;encoding() &lt; 16 &amp;&amp; src-&gt;encoding() &lt; 16 &amp;&amp; nds-&gt;encoding() &lt; 16) || VM_Version::supports_avx512vlbw()),&quot;XMM register should be 0-15&quot;);
 3900   Assembler::vpmullw(dst, nds, src, vector_len);
 3901 }
 3902 
 3903 void MacroAssembler::vpmullw(XMMRegister dst, XMMRegister nds, Address src, int vector_len) {
 3904   assert(((dst-&gt;encoding() &lt; 16 &amp;&amp; nds-&gt;encoding() &lt; 16) || VM_Version::supports_avx512vlbw()),&quot;XMM register should be 0-15&quot;);
 3905   Assembler::vpmullw(dst, nds, src, vector_len);
 3906 }
 3907 
 3908 void MacroAssembler::vpsubb(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {
 3909   assert(((dst-&gt;encoding() &lt; 16 &amp;&amp; src-&gt;encoding() &lt; 16 &amp;&amp; nds-&gt;encoding() &lt; 16) || VM_Version::supports_avx512vlbw()),&quot;XMM register should be 0-15&quot;);
 3910   Assembler::vpsubb(dst, nds, src, vector_len);
 3911 }
 3912 
 3913 void MacroAssembler::vpsubb(XMMRegister dst, XMMRegister nds, Address src, int vector_len) {
 3914   assert(((dst-&gt;encoding() &lt; 16 &amp;&amp; nds-&gt;encoding() &lt; 16) || VM_Version::supports_avx512vlbw()),&quot;XMM register should be 0-15&quot;);
 3915   Assembler::vpsubb(dst, nds, src, vector_len);
 3916 }
 3917 
 3918 void MacroAssembler::vpsubw(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {
 3919   assert(((dst-&gt;encoding() &lt; 16 &amp;&amp; src-&gt;encoding() &lt; 16 &amp;&amp; nds-&gt;encoding() &lt; 16) || VM_Version::supports_avx512vlbw()),&quot;XMM register should be 0-15&quot;);
 3920   Assembler::vpsubw(dst, nds, src, vector_len);
 3921 }
 3922 
 3923 void MacroAssembler::vpsubw(XMMRegister dst, XMMRegister nds, Address src, int vector_len) {
 3924   assert(((dst-&gt;encoding() &lt; 16 &amp;&amp; nds-&gt;encoding() &lt; 16) || VM_Version::supports_avx512vlbw()),&quot;XMM register should be 0-15&quot;);
 3925   Assembler::vpsubw(dst, nds, src, vector_len);
 3926 }
 3927 
 3928 void MacroAssembler::vpsraw(XMMRegister dst, XMMRegister nds, XMMRegister shift, int vector_len) {
 3929   assert(((dst-&gt;encoding() &lt; 16 &amp;&amp; shift-&gt;encoding() &lt; 16 &amp;&amp; nds-&gt;encoding() &lt; 16) || VM_Version::supports_avx512vlbw()),&quot;XMM register should be 0-15&quot;);
 3930   Assembler::vpsraw(dst, nds, shift, vector_len);
 3931 }
 3932 
 3933 void MacroAssembler::vpsraw(XMMRegister dst, XMMRegister nds, int shift, int vector_len) {
 3934   assert(((dst-&gt;encoding() &lt; 16 &amp;&amp; nds-&gt;encoding() &lt; 16) || VM_Version::supports_avx512vlbw()),&quot;XMM register should be 0-15&quot;);
 3935   Assembler::vpsraw(dst, nds, shift, vector_len);
 3936 }
 3937 
 3938 void MacroAssembler::evpsraq(XMMRegister dst, XMMRegister nds, XMMRegister shift, int vector_len) {
 3939   assert(UseAVX &gt; 2,&quot;&quot;);
 3940   if (!VM_Version::supports_avx512vl() &amp;&amp; vector_len &lt; 2) {
 3941      vector_len = 2;
 3942   }
 3943   Assembler::evpsraq(dst, nds, shift, vector_len);
 3944 }
 3945 
 3946 void MacroAssembler::evpsraq(XMMRegister dst, XMMRegister nds, int shift, int vector_len) {
 3947   assert(UseAVX &gt; 2,&quot;&quot;);
 3948   if (!VM_Version::supports_avx512vl() &amp;&amp; vector_len &lt; 2) {
 3949      vector_len = 2;
 3950   }
 3951   Assembler::evpsraq(dst, nds, shift, vector_len);
 3952 }
 3953 
 3954 void MacroAssembler::vpsrlw(XMMRegister dst, XMMRegister nds, XMMRegister shift, int vector_len) {
 3955   assert(((dst-&gt;encoding() &lt; 16 &amp;&amp; shift-&gt;encoding() &lt; 16 &amp;&amp; nds-&gt;encoding() &lt; 16) || VM_Version::supports_avx512vlbw()),&quot;XMM register should be 0-15&quot;);
 3956   Assembler::vpsrlw(dst, nds, shift, vector_len);
 3957 }
 3958 
 3959 void MacroAssembler::vpsrlw(XMMRegister dst, XMMRegister nds, int shift, int vector_len) {
 3960   assert(((dst-&gt;encoding() &lt; 16 &amp;&amp; nds-&gt;encoding() &lt; 16) || VM_Version::supports_avx512vlbw()),&quot;XMM register should be 0-15&quot;);
 3961   Assembler::vpsrlw(dst, nds, shift, vector_len);
 3962 }
 3963 
 3964 void MacroAssembler::vpsllw(XMMRegister dst, XMMRegister nds, XMMRegister shift, int vector_len) {
 3965   assert(((dst-&gt;encoding() &lt; 16 &amp;&amp; shift-&gt;encoding() &lt; 16 &amp;&amp; nds-&gt;encoding() &lt; 16) || VM_Version::supports_avx512vlbw()),&quot;XMM register should be 0-15&quot;);
 3966   Assembler::vpsllw(dst, nds, shift, vector_len);
 3967 }
 3968 
 3969 void MacroAssembler::vpsllw(XMMRegister dst, XMMRegister nds, int shift, int vector_len) {
 3970   assert(((dst-&gt;encoding() &lt; 16 &amp;&amp; nds-&gt;encoding() &lt; 16) || VM_Version::supports_avx512vlbw()),&quot;XMM register should be 0-15&quot;);
 3971   Assembler::vpsllw(dst, nds, shift, vector_len);
 3972 }
 3973 
 3974 void MacroAssembler::vptest(XMMRegister dst, XMMRegister src) {
 3975   assert((dst-&gt;encoding() &lt; 16 &amp;&amp; src-&gt;encoding() &lt; 16),&quot;XMM register should be 0-15&quot;);
 3976   Assembler::vptest(dst, src);
 3977 }
 3978 
 3979 void MacroAssembler::punpcklbw(XMMRegister dst, XMMRegister src) {
 3980   assert(((dst-&gt;encoding() &lt; 16 &amp;&amp; src-&gt;encoding() &lt; 16) || VM_Version::supports_avx512vlbw()),&quot;XMM register should be 0-15&quot;);
 3981   Assembler::punpcklbw(dst, src);
 3982 }
 3983 
 3984 void MacroAssembler::pshufd(XMMRegister dst, Address src, int mode) {
 3985   assert(((dst-&gt;encoding() &lt; 16) || VM_Version::supports_avx512vl()),&quot;XMM register should be 0-15&quot;);
 3986   Assembler::pshufd(dst, src, mode);
 3987 }
 3988 
 3989 void MacroAssembler::pshuflw(XMMRegister dst, XMMRegister src, int mode) {
 3990   assert(((dst-&gt;encoding() &lt; 16 &amp;&amp; src-&gt;encoding() &lt; 16) || VM_Version::supports_avx512vlbw()),&quot;XMM register should be 0-15&quot;);
 3991   Assembler::pshuflw(dst, src, mode);
 3992 }
 3993 
 3994 void MacroAssembler::vandpd(XMMRegister dst, XMMRegister nds, AddressLiteral src, int vector_len, Register scratch_reg) {
 3995   if (reachable(src)) {
 3996     vandpd(dst, nds, as_Address(src), vector_len);
 3997   } else {
 3998     lea(scratch_reg, src);
 3999     vandpd(dst, nds, Address(scratch_reg, 0), vector_len);
 4000   }
 4001 }
 4002 
 4003 void MacroAssembler::vandps(XMMRegister dst, XMMRegister nds, AddressLiteral src, int vector_len, Register scratch_reg) {
 4004   if (reachable(src)) {
 4005     vandps(dst, nds, as_Address(src), vector_len);
 4006   } else {
 4007     lea(scratch_reg, src);
 4008     vandps(dst, nds, Address(scratch_reg, 0), vector_len);
 4009   }
 4010 }
 4011 
 4012 void MacroAssembler::vdivsd(XMMRegister dst, XMMRegister nds, AddressLiteral src) {
 4013   if (reachable(src)) {
 4014     vdivsd(dst, nds, as_Address(src));
 4015   } else {
 4016     lea(rscratch1, src);
 4017     vdivsd(dst, nds, Address(rscratch1, 0));
 4018   }
 4019 }
 4020 
 4021 void MacroAssembler::vdivss(XMMRegister dst, XMMRegister nds, AddressLiteral src) {
 4022   if (reachable(src)) {
 4023     vdivss(dst, nds, as_Address(src));
 4024   } else {
 4025     lea(rscratch1, src);
 4026     vdivss(dst, nds, Address(rscratch1, 0));
 4027   }
 4028 }
 4029 
 4030 void MacroAssembler::vmulsd(XMMRegister dst, XMMRegister nds, AddressLiteral src) {
 4031   if (reachable(src)) {
 4032     vmulsd(dst, nds, as_Address(src));
 4033   } else {
 4034     lea(rscratch1, src);
 4035     vmulsd(dst, nds, Address(rscratch1, 0));
 4036   }
 4037 }
 4038 
 4039 void MacroAssembler::vmulss(XMMRegister dst, XMMRegister nds, AddressLiteral src) {
 4040   if (reachable(src)) {
 4041     vmulss(dst, nds, as_Address(src));
 4042   } else {
 4043     lea(rscratch1, src);
 4044     vmulss(dst, nds, Address(rscratch1, 0));
 4045   }
 4046 }
 4047 
 4048 void MacroAssembler::vsubsd(XMMRegister dst, XMMRegister nds, AddressLiteral src) {
 4049   if (reachable(src)) {
 4050     vsubsd(dst, nds, as_Address(src));
 4051   } else {
 4052     lea(rscratch1, src);
 4053     vsubsd(dst, nds, Address(rscratch1, 0));
 4054   }
 4055 }
 4056 
 4057 void MacroAssembler::vsubss(XMMRegister dst, XMMRegister nds, AddressLiteral src) {
 4058   if (reachable(src)) {
 4059     vsubss(dst, nds, as_Address(src));
 4060   } else {
 4061     lea(rscratch1, src);
 4062     vsubss(dst, nds, Address(rscratch1, 0));
 4063   }
 4064 }
 4065 
 4066 void MacroAssembler::vnegatess(XMMRegister dst, XMMRegister nds, AddressLiteral src) {
 4067   assert(((dst-&gt;encoding() &lt; 16 &amp;&amp; nds-&gt;encoding() &lt; 16) || VM_Version::supports_avx512vldq()),&quot;XMM register should be 0-15&quot;);
 4068   vxorps(dst, nds, src, Assembler::AVX_128bit);
 4069 }
 4070 
 4071 void MacroAssembler::vnegatesd(XMMRegister dst, XMMRegister nds, AddressLiteral src) {
 4072   assert(((dst-&gt;encoding() &lt; 16 &amp;&amp; nds-&gt;encoding() &lt; 16) || VM_Version::supports_avx512vldq()),&quot;XMM register should be 0-15&quot;);
 4073   vxorpd(dst, nds, src, Assembler::AVX_128bit);
 4074 }
 4075 
 4076 void MacroAssembler::vxorpd(XMMRegister dst, XMMRegister nds, AddressLiteral src, int vector_len, Register scratch_reg) {
 4077   if (reachable(src)) {
 4078     vxorpd(dst, nds, as_Address(src), vector_len);
 4079   } else {
 4080     lea(scratch_reg, src);
 4081     vxorpd(dst, nds, Address(scratch_reg, 0), vector_len);
 4082   }
 4083 }
 4084 
 4085 void MacroAssembler::vxorps(XMMRegister dst, XMMRegister nds, AddressLiteral src, int vector_len, Register scratch_reg) {
 4086   if (reachable(src)) {
 4087     vxorps(dst, nds, as_Address(src), vector_len);
 4088   } else {
 4089     lea(scratch_reg, src);
 4090     vxorps(dst, nds, Address(scratch_reg, 0), vector_len);
 4091   }
 4092 }
 4093 
 4094 void MacroAssembler::vpxor(XMMRegister dst, XMMRegister nds, AddressLiteral src, int vector_len, Register scratch_reg) {
 4095   if (UseAVX &gt; 1 || (vector_len &lt; 1)) {
 4096     if (reachable(src)) {
 4097       Assembler::vpxor(dst, nds, as_Address(src), vector_len);
 4098     } else {
 4099       lea(scratch_reg, src);
 4100       Assembler::vpxor(dst, nds, Address(scratch_reg, 0), vector_len);
 4101     }
 4102   }
 4103   else {
 4104     MacroAssembler::vxorpd(dst, nds, src, vector_len, scratch_reg);
 4105   }
 4106 }
 4107 
 4108 //-------------------------------------------------------------------------------------------
 4109 #ifdef COMPILER2
 4110 // Generic instructions support for use in .ad files C2 code generation
 4111 
 4112 void MacroAssembler::vabsnegd(int opcode, XMMRegister dst, XMMRegister src, Register scr) {
 4113   if (dst != src) {
 4114     movdqu(dst, src);
 4115   }
 4116   if (opcode == Op_AbsVD) {
 4117     andpd(dst, ExternalAddress(StubRoutines::x86::vector_double_sign_mask()), scr);
 4118   } else {
 4119     assert((opcode == Op_NegVD),&quot;opcode should be Op_NegD&quot;);
 4120     xorpd(dst, ExternalAddress(StubRoutines::x86::vector_double_sign_flip()), scr);
 4121   }
 4122 }
 4123 
 4124 void MacroAssembler::vabsnegd(int opcode, XMMRegister dst, XMMRegister src, int vector_len, Register scr) {
 4125   if (opcode == Op_AbsVD) {
 4126     vandpd(dst, src, ExternalAddress(StubRoutines::x86::vector_double_sign_mask()), vector_len, scr);
 4127   } else {
 4128     assert((opcode == Op_NegVD),&quot;opcode should be Op_NegD&quot;);
 4129     vxorpd(dst, src, ExternalAddress(StubRoutines::x86::vector_double_sign_flip()), vector_len, scr);
 4130   }
 4131 }
 4132 
 4133 void MacroAssembler::vabsnegf(int opcode, XMMRegister dst, XMMRegister src, Register scr) {
 4134   if (dst != src) {
 4135     movdqu(dst, src);
 4136   }
 4137   if (opcode == Op_AbsVF) {
 4138     andps(dst, ExternalAddress(StubRoutines::x86::vector_float_sign_mask()), scr);
 4139   } else {
 4140     assert((opcode == Op_NegVF),&quot;opcode should be Op_NegF&quot;);
 4141     xorps(dst, ExternalAddress(StubRoutines::x86::vector_float_sign_flip()), scr);
 4142   }
 4143 }
 4144 
 4145 void MacroAssembler::vabsnegf(int opcode, XMMRegister dst, XMMRegister src, int vector_len, Register scr) {
 4146   if (opcode == Op_AbsVF) {
 4147     vandps(dst, src, ExternalAddress(StubRoutines::x86::vector_float_sign_mask()), vector_len, scr);
 4148   } else {
 4149     assert((opcode == Op_NegVF),&quot;opcode should be Op_NegF&quot;);
 4150     vxorps(dst, src, ExternalAddress(StubRoutines::x86::vector_float_sign_flip()), vector_len, scr);
 4151   }
 4152 }
 4153 
 4154 void MacroAssembler::vextendbw(bool sign, XMMRegister dst, XMMRegister src) {
 4155   if (sign) {
 4156     pmovsxbw(dst, src);
 4157   } else {
 4158     pmovzxbw(dst, src);
 4159   }
 4160 }
 4161 
 4162 void MacroAssembler::vextendbw(bool sign, XMMRegister dst, XMMRegister src, int vector_len) {
 4163   if (sign) {
 4164     vpmovsxbw(dst, src, vector_len);
 4165   } else {
 4166     vpmovzxbw(dst, src, vector_len);
 4167   }
 4168 }
 4169 
 4170 void MacroAssembler::vshiftd(int opcode, XMMRegister dst, XMMRegister src) {
 4171   if (opcode == Op_RShiftVI) {
 4172     psrad(dst, src);
 4173   } else if (opcode == Op_LShiftVI) {
 4174     pslld(dst, src);
 4175   } else {
 4176     assert((opcode == Op_URShiftVI),&quot;opcode should be Op_URShiftVI&quot;);
 4177     psrld(dst, src);
 4178   }
 4179 }
 4180 
 4181 void MacroAssembler::vshiftd(int opcode, XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {
 4182   if (opcode == Op_RShiftVI) {
 4183     vpsrad(dst, nds, src, vector_len);
 4184   } else if (opcode == Op_LShiftVI) {
 4185     vpslld(dst, nds, src, vector_len);
 4186   } else {
 4187     assert((opcode == Op_URShiftVI),&quot;opcode should be Op_URShiftVI&quot;);
 4188     vpsrld(dst, nds, src, vector_len);
 4189   }
 4190 }
 4191 
 4192 void MacroAssembler::vshiftw(int opcode, XMMRegister dst, XMMRegister src) {
 4193   if ((opcode == Op_RShiftVS) || (opcode == Op_RShiftVB)) {
 4194     psraw(dst, src);
 4195   } else if ((opcode == Op_LShiftVS) || (opcode == Op_LShiftVB)) {
 4196     psllw(dst, src);
 4197   } else {
 4198     assert(((opcode == Op_URShiftVS) || (opcode == Op_URShiftVB)),&quot;opcode should be one of Op_URShiftVS or Op_URShiftVB&quot;);
 4199     psrlw(dst, src);
 4200   }
 4201 }
 4202 
 4203 void MacroAssembler::vshiftw(int opcode, XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {
 4204   if ((opcode == Op_RShiftVS) || (opcode == Op_RShiftVB)) {
 4205     vpsraw(dst, nds, src, vector_len);
 4206   } else if ((opcode == Op_LShiftVS) || (opcode == Op_LShiftVB)) {
 4207     vpsllw(dst, nds, src, vector_len);
 4208   } else {
 4209     assert(((opcode == Op_URShiftVS) || (opcode == Op_URShiftVB)),&quot;opcode should be one of Op_URShiftVS or Op_URShiftVB&quot;);
 4210     vpsrlw(dst, nds, src, vector_len);
 4211   }
 4212 }
 4213 
 4214 void MacroAssembler::vshiftq(int opcode, XMMRegister dst, XMMRegister src) {
 4215   if (opcode == Op_RShiftVL) {
 4216     psrlq(dst, src);  // using srl to implement sra on pre-avs512 systems
 4217   } else if (opcode == Op_LShiftVL) {
 4218     psllq(dst, src);
 4219   } else {
 4220     assert((opcode == Op_URShiftVL),&quot;opcode should be Op_URShiftVL&quot;);
 4221     psrlq(dst, src);
 4222   }
 4223 }
 4224 
 4225 void MacroAssembler::vshiftq(int opcode, XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {
 4226   if (opcode == Op_RShiftVL) {
 4227     evpsraq(dst, nds, src, vector_len);
 4228   } else if (opcode == Op_LShiftVL) {
 4229     vpsllq(dst, nds, src, vector_len);
 4230   } else {
 4231     assert((opcode == Op_URShiftVL),&quot;opcode should be Op_URShiftVL&quot;);
 4232     vpsrlq(dst, nds, src, vector_len);
 4233   }
 4234 }
 4235 #endif
 4236 //-------------------------------------------------------------------------------------------
 4237 
 4238 void MacroAssembler::clear_jweak_tag(Register possibly_jweak) {
 4239   const int32_t inverted_jweak_mask = ~static_cast&lt;int32_t&gt;(JNIHandles::weak_tag_mask);
 4240   STATIC_ASSERT(inverted_jweak_mask == -2); // otherwise check this code
 4241   // The inverted mask is sign-extended
 4242   andptr(possibly_jweak, inverted_jweak_mask);
 4243 }
 4244 
 4245 void MacroAssembler::resolve_jobject(Register value,
 4246                                      Register thread,
 4247                                      Register tmp) {
 4248   assert_different_registers(value, thread, tmp);
 4249   Label done, not_weak;
 4250   testptr(value, value);
 4251   jcc(Assembler::zero, done);                // Use NULL as-is.
 4252   testptr(value, JNIHandles::weak_tag_mask); // Test for jweak tag.
 4253   jcc(Assembler::zero, not_weak);
 4254   // Resolve jweak.
 4255   access_load_at(T_OBJECT, IN_NATIVE | ON_PHANTOM_OOP_REF,
 4256                  value, Address(value, -JNIHandles::weak_tag_value), tmp, thread);
 4257   verify_oop(value);
 4258   jmp(done);
 4259   bind(not_weak);
 4260   // Resolve (untagged) jobject.
 4261   access_load_at(T_OBJECT, IN_NATIVE, value, Address(value, 0), tmp, thread);
 4262   verify_oop(value);
 4263   bind(done);
 4264 }
 4265 
 4266 void MacroAssembler::subptr(Register dst, int32_t imm32) {
 4267   LP64_ONLY(subq(dst, imm32)) NOT_LP64(subl(dst, imm32));
 4268 }
 4269 
 4270 // Force generation of a 4 byte immediate value even if it fits into 8bit
 4271 void MacroAssembler::subptr_imm32(Register dst, int32_t imm32) {
 4272   LP64_ONLY(subq_imm32(dst, imm32)) NOT_LP64(subl_imm32(dst, imm32));
 4273 }
 4274 
 4275 void MacroAssembler::subptr(Register dst, Register src) {
 4276   LP64_ONLY(subq(dst, src)) NOT_LP64(subl(dst, src));
 4277 }
 4278 
 4279 // C++ bool manipulation
 4280 void MacroAssembler::testbool(Register dst) {
 4281   if(sizeof(bool) == 1)
 4282     testb(dst, 0xff);
 4283   else if(sizeof(bool) == 2) {
 4284     // testw implementation needed for two byte bools
 4285     ShouldNotReachHere();
 4286   } else if(sizeof(bool) == 4)
 4287     testl(dst, dst);
 4288   else
 4289     // unsupported
 4290     ShouldNotReachHere();
 4291 }
 4292 
 4293 void MacroAssembler::testptr(Register dst, Register src) {
 4294   LP64_ONLY(testq(dst, src)) NOT_LP64(testl(dst, src));
 4295 }
 4296 
 4297 // Object / value buffer allocation...
 4298 //
 4299 // Kills klass and rsi on LP64
 4300 void MacroAssembler::allocate_instance(Register klass, Register new_obj,
 4301                                        Register t1, Register t2,
 4302                                        bool clear_fields, Label&amp; alloc_failed)
 4303 {
 4304   Label done, initialize_header, initialize_object, slow_case, slow_case_no_pop;
 4305   Register layout_size = t1;
 4306   assert(new_obj == rax, &quot;needs to be rax, according to barrier asm eden_allocate&quot;);
 4307   assert_different_registers(klass, new_obj, t1, t2);
 4308 
 4309 #ifdef ASSERT
 4310   {
 4311     Label L;
 4312     cmpb(Address(klass, InstanceKlass::init_state_offset()), InstanceKlass::fully_initialized);
 4313     jcc(Assembler::equal, L);
 4314     stop(&quot;klass not initialized&quot;);
 4315     bind(L);
 4316   }
 4317 #endif
 4318 
 4319   // get instance_size in InstanceKlass (scaled to a count of bytes)
 4320   movl(layout_size, Address(klass, Klass::layout_helper_offset()));
 4321   // test to see if it has a finalizer or is malformed in some way
 4322   testl(layout_size, Klass::_lh_instance_slow_path_bit);
 4323   jcc(Assembler::notZero, slow_case_no_pop);
 4324 
 4325   // Allocate the instance:
 4326   //  If TLAB is enabled:
 4327   //    Try to allocate in the TLAB.
 4328   //    If fails, go to the slow path.
 4329   //  Else If inline contiguous allocations are enabled:
 4330   //    Try to allocate in eden.
 4331   //    If fails due to heap end, go to slow path.
 4332   //
 4333   //  If TLAB is enabled OR inline contiguous is enabled:
 4334   //    Initialize the allocation.
 4335   //    Exit.
 4336   //
 4337   //  Go to slow path.
 4338   const bool allow_shared_alloc =
 4339     Universe::heap()-&gt;supports_inline_contig_alloc();
 4340 
 4341   push(klass);
 4342   const Register thread = LP64_ONLY(r15_thread) NOT_LP64(klass);
 4343 #ifndef _LP64
 4344   if (UseTLAB || allow_shared_alloc) {
 4345     get_thread(thread);
 4346   }
 4347 #endif // _LP64
 4348 
 4349   if (UseTLAB) {
 4350     tlab_allocate(thread, new_obj, layout_size, 0, klass, t2, slow_case);
 4351     if (ZeroTLAB || (!clear_fields)) {
 4352       // the fields have been already cleared
 4353       jmp(initialize_header);
 4354     } else {
 4355       // initialize both the header and fields
 4356       jmp(initialize_object);
 4357     }
 4358   } else {
 4359     // Allocation in the shared Eden, if allowed.
 4360     //
 4361     eden_allocate(thread, new_obj, layout_size, 0, t2, slow_case);
 4362   }
 4363 
 4364   // If UseTLAB or allow_shared_alloc are true, the object is created above and
 4365   // there is an initialize need. Otherwise, skip and go to the slow path.
 4366   if (UseTLAB || allow_shared_alloc) {
 4367     if (clear_fields) {
 4368       // The object is initialized before the header.  If the object size is
 4369       // zero, go directly to the header initialization.
 4370       bind(initialize_object);
 4371       decrement(layout_size, sizeof(oopDesc));
 4372       jcc(Assembler::zero, initialize_header);
 4373 
 4374       // Initialize topmost object field, divide size by 8, check if odd and
 4375       // test if zero.
 4376       Register zero = klass;
 4377       xorl(zero, zero);    // use zero reg to clear memory (shorter code)
 4378       shrl(layout_size, LogBytesPerLong); // divide by 2*oopSize and set carry flag if odd
 4379 
 4380   #ifdef ASSERT
 4381       // make sure instance_size was multiple of 8
 4382       Label L;
 4383       // Ignore partial flag stall after shrl() since it is debug VM
 4384       jcc(Assembler::carryClear, L);
 4385       stop(&quot;object size is not multiple of 2 - adjust this code&quot;);
 4386       bind(L);
 4387       // must be &gt; 0, no extra check needed here
 4388   #endif
 4389 
 4390       // initialize remaining object fields: instance_size was a multiple of 8
 4391       {
 4392         Label loop;
 4393         bind(loop);
 4394         movptr(Address(new_obj, layout_size, Address::times_8, sizeof(oopDesc) - 1*oopSize), zero);
 4395         NOT_LP64(movptr(Address(new_obj, layout_size, Address::times_8, sizeof(oopDesc) - 2*oopSize), zero));
 4396         decrement(layout_size);
 4397         jcc(Assembler::notZero, loop);
 4398       }
 4399     } // clear_fields
 4400 
 4401     // initialize object header only.
 4402     bind(initialize_header);
 4403     pop(klass);
 4404     Register mark_word = t2;
 4405     movptr(mark_word, Address(klass, Klass::prototype_header_offset()));
 4406     movptr(Address(new_obj, oopDesc::mark_offset_in_bytes ()), mark_word);
 4407 #ifdef _LP64
 4408     xorl(rsi, rsi);                 // use zero reg to clear memory (shorter code)
 4409     store_klass_gap(new_obj, rsi);  // zero klass gap for compressed oops
 4410 #endif
 4411     movptr(t2, klass);         // preserve klass
 4412     store_klass(new_obj, t2);  // src klass reg is potentially compressed
 4413 
 4414     jmp(done);
 4415   }
 4416 
 4417   bind(slow_case);
 4418   pop(klass);
 4419   bind(slow_case_no_pop);
 4420   jmp(alloc_failed);
 4421 
 4422   bind(done);
 4423 }
 4424 
 4425 // Defines obj, preserves var_size_in_bytes, okay for t2 == var_size_in_bytes.
 4426 void MacroAssembler::tlab_allocate(Register thread, Register obj,
 4427                                    Register var_size_in_bytes,
 4428                                    int con_size_in_bytes,
 4429                                    Register t1,
 4430                                    Register t2,
 4431                                    Label&amp; slow_case) {
 4432   BarrierSetAssembler* bs = BarrierSet::barrier_set()-&gt;barrier_set_assembler();
 4433   bs-&gt;tlab_allocate(this, thread, obj, var_size_in_bytes, con_size_in_bytes, t1, t2, slow_case);
 4434 }
 4435 
 4436 // Defines obj, preserves var_size_in_bytes
 4437 void MacroAssembler::eden_allocate(Register thread, Register obj,
 4438                                    Register var_size_in_bytes,
 4439                                    int con_size_in_bytes,
 4440                                    Register t1,
 4441                                    Label&amp; slow_case) {
 4442   BarrierSetAssembler* bs = BarrierSet::barrier_set()-&gt;barrier_set_assembler();
 4443   bs-&gt;eden_allocate(this, thread, obj, var_size_in_bytes, con_size_in_bytes, t1, slow_case);
 4444 }
 4445 
 4446 // Preserves the contents of address, destroys the contents length_in_bytes and temp.
 4447 void MacroAssembler::zero_memory(Register address, Register length_in_bytes, int offset_in_bytes, Register temp) {
 4448   assert(address != length_in_bytes &amp;&amp; address != temp &amp;&amp; temp != length_in_bytes, &quot;registers must be different&quot;);
 4449   assert((offset_in_bytes &amp; (BytesPerWord - 1)) == 0, &quot;offset must be a multiple of BytesPerWord&quot;);
 4450   Label done;
 4451 
 4452   testptr(length_in_bytes, length_in_bytes);
 4453   jcc(Assembler::zero, done);
 4454 
 4455   // initialize topmost word, divide index by 2, check if odd and test if zero
 4456   // note: for the remaining code to work, index must be a multiple of BytesPerWord
 4457 #ifdef ASSERT
 4458   {
 4459     Label L;
 4460     testptr(length_in_bytes, BytesPerWord - 1);
 4461     jcc(Assembler::zero, L);
 4462     stop(&quot;length must be a multiple of BytesPerWord&quot;);
 4463     bind(L);
 4464   }
 4465 #endif
 4466   Register index = length_in_bytes;
 4467   xorptr(temp, temp);    // use _zero reg to clear memory (shorter code)
 4468   if (UseIncDec) {
 4469     shrptr(index, 3);  // divide by 8/16 and set carry flag if bit 2 was set
 4470   } else {
 4471     shrptr(index, 2);  // use 2 instructions to avoid partial flag stall
 4472     shrptr(index, 1);
 4473   }
 4474 #ifndef _LP64
 4475   // index could have not been a multiple of 8 (i.e., bit 2 was set)
 4476   {
 4477     Label even;
 4478     // note: if index was a multiple of 8, then it cannot
 4479     //       be 0 now otherwise it must have been 0 before
 4480     //       =&gt; if it is even, we don&#39;t need to check for 0 again
 4481     jcc(Assembler::carryClear, even);
 4482     // clear topmost word (no jump would be needed if conditional assignment worked here)
 4483     movptr(Address(address, index, Address::times_8, offset_in_bytes - 0*BytesPerWord), temp);
 4484     // index could be 0 now, must check again
 4485     jcc(Assembler::zero, done);
 4486     bind(even);
 4487   }
 4488 #endif // !_LP64
 4489   // initialize remaining object fields: index is a multiple of 2 now
 4490   {
 4491     Label loop;
 4492     bind(loop);
 4493     movptr(Address(address, index, Address::times_8, offset_in_bytes - 1*BytesPerWord), temp);
 4494     NOT_LP64(movptr(Address(address, index, Address::times_8, offset_in_bytes - 2*BytesPerWord), temp);)
 4495     decrement(index);
 4496     jcc(Assembler::notZero, loop);
 4497   }
 4498 
 4499   bind(done);
 4500 }
 4501 
 4502 void MacroAssembler::get_value_field_klass(Register klass, Register index, Register value_klass) {
 4503   movptr(value_klass, Address(klass, InstanceKlass::value_field_klasses_offset()));
 4504 #ifdef ASSERT
 4505   {
 4506     Label done;
 4507     cmpptr(value_klass, 0);
 4508     jcc(Assembler::notEqual, done);
 4509     stop(&quot;get_value_field_klass contains no inline klasses&quot;);
 4510     bind(done);
 4511   }
 4512 #endif
 4513   movptr(value_klass, Address(value_klass, index, Address::times_ptr));
 4514 }
 4515 
 4516 void MacroAssembler::get_default_value_oop(Register value_klass, Register temp_reg, Register obj) {
 4517 #ifdef ASSERT
 4518   {
 4519     Label done_check;
 4520     test_klass_is_value(value_klass, temp_reg, done_check);
 4521     stop(&quot;get_default_value_oop from non-value klass&quot;);
 4522     bind(done_check);
 4523   }
 4524 #endif
 4525   Register offset = temp_reg;
 4526   // Getting the offset of the pre-allocated default value
 4527   movptr(offset, Address(value_klass, in_bytes(InstanceKlass::adr_valueklass_fixed_block_offset())));
 4528   movl(offset, Address(offset, in_bytes(ValueKlass::default_value_offset_offset())));
 4529 
 4530   // Getting the mirror
 4531   movptr(obj, Address(value_klass, in_bytes(Klass::java_mirror_offset())));
 4532   resolve_oop_handle(obj, value_klass);
 4533 
 4534   // Getting the pre-allocated default value from the mirror
 4535   Address field(obj, offset, Address::times_1);
 4536   load_heap_oop(obj, field);
 4537 }
 4538 
 4539 void MacroAssembler::get_empty_value_oop(Register value_klass, Register temp_reg, Register obj) {
 4540 #ifdef ASSERT
 4541   {
 4542     Label done_check;
 4543     test_klass_is_empty_value(value_klass, temp_reg, done_check);
 4544     stop(&quot;get_empty_value from non-empty value klass&quot;);
 4545     bind(done_check);
 4546   }
 4547 #endif
 4548   get_default_value_oop(value_klass, temp_reg, obj);
 4549 }
 4550 
 4551 
 4552 // Look up the method for a megamorphic invokeinterface call.
 4553 // The target method is determined by &lt;intf_klass, itable_index&gt;.
 4554 // The receiver klass is in recv_klass.
 4555 // On success, the result will be in method_result, and execution falls through.
 4556 // On failure, execution transfers to the given label.
 4557 void MacroAssembler::lookup_interface_method(Register recv_klass,
 4558                                              Register intf_klass,
 4559                                              RegisterOrConstant itable_index,
 4560                                              Register method_result,
 4561                                              Register scan_temp,
 4562                                              Label&amp; L_no_such_interface,
 4563                                              bool return_method) {
 4564   assert_different_registers(recv_klass, intf_klass, scan_temp);
 4565   assert_different_registers(method_result, intf_klass, scan_temp);
 4566   assert(recv_klass != method_result || !return_method,
 4567          &quot;recv_klass can be destroyed when method isn&#39;t needed&quot;);
 4568 
 4569   assert(itable_index.is_constant() || itable_index.as_register() == method_result,
 4570          &quot;caller must use same register for non-constant itable index as for method&quot;);
 4571 
 4572   // Compute start of first itableOffsetEntry (which is at the end of the vtable)
 4573   int vtable_base = in_bytes(Klass::vtable_start_offset());
 4574   int itentry_off = itableMethodEntry::method_offset_in_bytes();
 4575   int scan_step   = itableOffsetEntry::size() * wordSize;
 4576   int vte_size    = vtableEntry::size_in_bytes();
 4577   Address::ScaleFactor times_vte_scale = Address::times_ptr;
 4578   assert(vte_size == wordSize, &quot;else adjust times_vte_scale&quot;);
 4579 
 4580   movl(scan_temp, Address(recv_klass, Klass::vtable_length_offset()));
 4581 
 4582   // %%% Could store the aligned, prescaled offset in the klassoop.
 4583   lea(scan_temp, Address(recv_klass, scan_temp, times_vte_scale, vtable_base));
 4584 
 4585   if (return_method) {
 4586     // Adjust recv_klass by scaled itable_index, so we can free itable_index.
 4587     assert(itableMethodEntry::size() * wordSize == wordSize, &quot;adjust the scaling in the code below&quot;);
 4588     lea(recv_klass, Address(recv_klass, itable_index, Address::times_ptr, itentry_off));
 4589   }
 4590 
 4591   // for (scan = klass-&gt;itable(); scan-&gt;interface() != NULL; scan += scan_step) {
 4592   //   if (scan-&gt;interface() == intf) {
 4593   //     result = (klass + scan-&gt;offset() + itable_index);
 4594   //   }
 4595   // }
 4596   Label search, found_method;
 4597 
 4598   for (int peel = 1; peel &gt;= 0; peel--) {
 4599     movptr(method_result, Address(scan_temp, itableOffsetEntry::interface_offset_in_bytes()));
 4600     cmpptr(intf_klass, method_result);
 4601 
 4602     if (peel) {
 4603       jccb(Assembler::equal, found_method);
 4604     } else {
 4605       jccb(Assembler::notEqual, search);
 4606       // (invert the test to fall through to found_method...)
 4607     }
 4608 
 4609     if (!peel)  break;
 4610 
 4611     bind(search);
 4612 
 4613     // Check that the previous entry is non-null.  A null entry means that
 4614     // the receiver class doesn&#39;t implement the interface, and wasn&#39;t the
 4615     // same as when the caller was compiled.
 4616     testptr(method_result, method_result);
 4617     jcc(Assembler::zero, L_no_such_interface);
 4618     addptr(scan_temp, scan_step);
 4619   }
 4620 
 4621   bind(found_method);
 4622 
 4623   if (return_method) {
 4624     // Got a hit.
 4625     movl(scan_temp, Address(scan_temp, itableOffsetEntry::offset_offset_in_bytes()));
 4626     movptr(method_result, Address(recv_klass, scan_temp, Address::times_1));
 4627   }
 4628 }
 4629 
 4630 
 4631 // virtual method calling
 4632 void MacroAssembler::lookup_virtual_method(Register recv_klass,
 4633                                            RegisterOrConstant vtable_index,
 4634                                            Register method_result) {
 4635   const int base = in_bytes(Klass::vtable_start_offset());
 4636   assert(vtableEntry::size() * wordSize == wordSize, &quot;else adjust the scaling in the code below&quot;);
 4637   Address vtable_entry_addr(recv_klass,
 4638                             vtable_index, Address::times_ptr,
 4639                             base + vtableEntry::method_offset_in_bytes());
 4640   movptr(method_result, vtable_entry_addr);
 4641 }
 4642 
 4643 
 4644 void MacroAssembler::check_klass_subtype(Register sub_klass,
 4645                            Register super_klass,
 4646                            Register temp_reg,
 4647                            Label&amp; L_success) {
 4648   Label L_failure;
 4649   check_klass_subtype_fast_path(sub_klass, super_klass, temp_reg,        &amp;L_success, &amp;L_failure, NULL);
 4650   check_klass_subtype_slow_path(sub_klass, super_klass, temp_reg, noreg, &amp;L_success, NULL);
 4651   bind(L_failure);
 4652 }
 4653 
 4654 
 4655 void MacroAssembler::check_klass_subtype_fast_path(Register sub_klass,
 4656                                                    Register super_klass,
 4657                                                    Register temp_reg,
 4658                                                    Label* L_success,
 4659                                                    Label* L_failure,
 4660                                                    Label* L_slow_path,
 4661                                         RegisterOrConstant super_check_offset) {
 4662   assert_different_registers(sub_klass, super_klass, temp_reg);
 4663   bool must_load_sco = (super_check_offset.constant_or_zero() == -1);
 4664   if (super_check_offset.is_register()) {
 4665     assert_different_registers(sub_klass, super_klass,
 4666                                super_check_offset.as_register());
 4667   } else if (must_load_sco) {
 4668     assert(temp_reg != noreg, &quot;supply either a temp or a register offset&quot;);
 4669   }
 4670 
 4671   Label L_fallthrough;
 4672   int label_nulls = 0;
 4673   if (L_success == NULL)   { L_success   = &amp;L_fallthrough; label_nulls++; }
 4674   if (L_failure == NULL)   { L_failure   = &amp;L_fallthrough; label_nulls++; }
 4675   if (L_slow_path == NULL) { L_slow_path = &amp;L_fallthrough; label_nulls++; }
 4676   assert(label_nulls &lt;= 1, &quot;at most one NULL in the batch&quot;);
 4677 
 4678   int sc_offset = in_bytes(Klass::secondary_super_cache_offset());
 4679   int sco_offset = in_bytes(Klass::super_check_offset_offset());
 4680   Address super_check_offset_addr(super_klass, sco_offset);
 4681 
 4682   // Hacked jcc, which &quot;knows&quot; that L_fallthrough, at least, is in
 4683   // range of a jccb.  If this routine grows larger, reconsider at
 4684   // least some of these.
 4685 #define local_jcc(assembler_cond, label)                                \
 4686   if (&amp;(label) == &amp;L_fallthrough)  jccb(assembler_cond, label);         \
 4687   else                             jcc( assembler_cond, label) /*omit semi*/
 4688 
 4689   // Hacked jmp, which may only be used just before L_fallthrough.
 4690 #define final_jmp(label)                                                \
 4691   if (&amp;(label) == &amp;L_fallthrough) { /*do nothing*/ }                    \
 4692   else                            jmp(label)                /*omit semi*/
 4693 
 4694   // If the pointers are equal, we are done (e.g., String[] elements).
 4695   // This self-check enables sharing of secondary supertype arrays among
 4696   // non-primary types such as array-of-interface.  Otherwise, each such
 4697   // type would need its own customized SSA.
 4698   // We move this check to the front of the fast path because many
 4699   // type checks are in fact trivially successful in this manner,
 4700   // so we get a nicely predicted branch right at the start of the check.
 4701   cmpptr(sub_klass, super_klass);
 4702   local_jcc(Assembler::equal, *L_success);
 4703 
 4704   // Check the supertype display:
 4705   if (must_load_sco) {
 4706     // Positive movl does right thing on LP64.
 4707     movl(temp_reg, super_check_offset_addr);
 4708     super_check_offset = RegisterOrConstant(temp_reg);
 4709   }
 4710   Address super_check_addr(sub_klass, super_check_offset, Address::times_1, 0);
 4711   cmpptr(super_klass, super_check_addr); // load displayed supertype
 4712 
 4713   // This check has worked decisively for primary supers.
 4714   // Secondary supers are sought in the super_cache (&#39;super_cache_addr&#39;).
 4715   // (Secondary supers are interfaces and very deeply nested subtypes.)
 4716   // This works in the same check above because of a tricky aliasing
 4717   // between the super_cache and the primary super display elements.
 4718   // (The &#39;super_check_addr&#39; can address either, as the case requires.)
 4719   // Note that the cache is updated below if it does not help us find
 4720   // what we need immediately.
 4721   // So if it was a primary super, we can just fail immediately.
 4722   // Otherwise, it&#39;s the slow path for us (no success at this point).
 4723 
 4724   if (super_check_offset.is_register()) {
 4725     local_jcc(Assembler::equal, *L_success);
 4726     cmpl(super_check_offset.as_register(), sc_offset);
 4727     if (L_failure == &amp;L_fallthrough) {
 4728       local_jcc(Assembler::equal, *L_slow_path);
 4729     } else {
 4730       local_jcc(Assembler::notEqual, *L_failure);
 4731       final_jmp(*L_slow_path);
 4732     }
 4733   } else if (super_check_offset.as_constant() == sc_offset) {
 4734     // Need a slow path; fast failure is impossible.
 4735     if (L_slow_path == &amp;L_fallthrough) {
 4736       local_jcc(Assembler::equal, *L_success);
 4737     } else {
 4738       local_jcc(Assembler::notEqual, *L_slow_path);
 4739       final_jmp(*L_success);
 4740     }
 4741   } else {
 4742     // No slow path; it&#39;s a fast decision.
 4743     if (L_failure == &amp;L_fallthrough) {
 4744       local_jcc(Assembler::equal, *L_success);
 4745     } else {
 4746       local_jcc(Assembler::notEqual, *L_failure);
 4747       final_jmp(*L_success);
 4748     }
 4749   }
 4750 
 4751   bind(L_fallthrough);
 4752 
 4753 #undef local_jcc
 4754 #undef final_jmp
 4755 }
 4756 
 4757 
 4758 void MacroAssembler::check_klass_subtype_slow_path(Register sub_klass,
 4759                                                    Register super_klass,
 4760                                                    Register temp_reg,
 4761                                                    Register temp2_reg,
 4762                                                    Label* L_success,
 4763                                                    Label* L_failure,
 4764                                                    bool set_cond_codes) {
 4765   assert_different_registers(sub_klass, super_klass, temp_reg);
 4766   if (temp2_reg != noreg)
 4767     assert_different_registers(sub_klass, super_klass, temp_reg, temp2_reg);
 4768 #define IS_A_TEMP(reg) ((reg) == temp_reg || (reg) == temp2_reg)
 4769 
 4770   Label L_fallthrough;
 4771   int label_nulls = 0;
 4772   if (L_success == NULL)   { L_success   = &amp;L_fallthrough; label_nulls++; }
 4773   if (L_failure == NULL)   { L_failure   = &amp;L_fallthrough; label_nulls++; }
 4774   assert(label_nulls &lt;= 1, &quot;at most one NULL in the batch&quot;);
 4775 
 4776   // a couple of useful fields in sub_klass:
 4777   int ss_offset = in_bytes(Klass::secondary_supers_offset());
 4778   int sc_offset = in_bytes(Klass::secondary_super_cache_offset());
 4779   Address secondary_supers_addr(sub_klass, ss_offset);
 4780   Address super_cache_addr(     sub_klass, sc_offset);
 4781 
 4782   // Do a linear scan of the secondary super-klass chain.
 4783   // This code is rarely used, so simplicity is a virtue here.
 4784   // The repne_scan instruction uses fixed registers, which we must spill.
 4785   // Don&#39;t worry too much about pre-existing connections with the input regs.
 4786 
 4787   assert(sub_klass != rax, &quot;killed reg&quot;); // killed by mov(rax, super)
 4788   assert(sub_klass != rcx, &quot;killed reg&quot;); // killed by lea(rcx, &amp;pst_counter)
 4789 
 4790   // Get super_klass value into rax (even if it was in rdi or rcx).
 4791   bool pushed_rax = false, pushed_rcx = false, pushed_rdi = false;
 4792   if (super_klass != rax || UseCompressedOops) {
 4793     if (!IS_A_TEMP(rax)) { push(rax); pushed_rax = true; }
 4794     mov(rax, super_klass);
 4795   }
 4796   if (!IS_A_TEMP(rcx)) { push(rcx); pushed_rcx = true; }
 4797   if (!IS_A_TEMP(rdi)) { push(rdi); pushed_rdi = true; }
 4798 
 4799 #ifndef PRODUCT
 4800   int* pst_counter = &amp;SharedRuntime::_partial_subtype_ctr;
 4801   ExternalAddress pst_counter_addr((address) pst_counter);
 4802   NOT_LP64(  incrementl(pst_counter_addr) );
 4803   LP64_ONLY( lea(rcx, pst_counter_addr) );
 4804   LP64_ONLY( incrementl(Address(rcx, 0)) );
 4805 #endif //PRODUCT
 4806 
 4807   // We will consult the secondary-super array.
 4808   movptr(rdi, secondary_supers_addr);
 4809   // Load the array length.  (Positive movl does right thing on LP64.)
 4810   movl(rcx, Address(rdi, Array&lt;Klass*&gt;::length_offset_in_bytes()));
 4811   // Skip to start of data.
 4812   addptr(rdi, Array&lt;Klass*&gt;::base_offset_in_bytes());
 4813 
 4814   // Scan RCX words at [RDI] for an occurrence of RAX.
 4815   // Set NZ/Z based on last compare.
 4816   // Z flag value will not be set by &#39;repne&#39; if RCX == 0 since &#39;repne&#39; does
 4817   // not change flags (only scas instruction which is repeated sets flags).
 4818   // Set Z = 0 (not equal) before &#39;repne&#39; to indicate that class was not found.
 4819 
 4820     testptr(rax,rax); // Set Z = 0
 4821     repne_scan();
 4822 
 4823   // Unspill the temp. registers:
 4824   if (pushed_rdi)  pop(rdi);
 4825   if (pushed_rcx)  pop(rcx);
 4826   if (pushed_rax)  pop(rax);
 4827 
 4828   if (set_cond_codes) {
 4829     // Special hack for the AD files:  rdi is guaranteed non-zero.
 4830     assert(!pushed_rdi, &quot;rdi must be left non-NULL&quot;);
 4831     // Also, the condition codes are properly set Z/NZ on succeed/failure.
 4832   }
 4833 
 4834   if (L_failure == &amp;L_fallthrough)
 4835         jccb(Assembler::notEqual, *L_failure);
 4836   else  jcc(Assembler::notEqual, *L_failure);
 4837 
 4838   // Success.  Cache the super we found and proceed in triumph.
 4839   movptr(super_cache_addr, super_klass);
 4840 
 4841   if (L_success != &amp;L_fallthrough) {
 4842     jmp(*L_success);
 4843   }
 4844 
 4845 #undef IS_A_TEMP
 4846 
 4847   bind(L_fallthrough);
 4848 }
 4849 
 4850 void MacroAssembler::clinit_barrier(Register klass, Register thread, Label* L_fast_path, Label* L_slow_path) {
 4851   assert(L_fast_path != NULL || L_slow_path != NULL, &quot;at least one is required&quot;);
 4852 
 4853   Label L_fallthrough;
 4854   if (L_fast_path == NULL) {
 4855     L_fast_path = &amp;L_fallthrough;
 4856   } else if (L_slow_path == NULL) {
 4857     L_slow_path = &amp;L_fallthrough;
 4858   }
 4859 
 4860   // Fast path check: class is fully initialized
 4861   cmpb(Address(klass, InstanceKlass::init_state_offset()), InstanceKlass::fully_initialized);
 4862   jcc(Assembler::equal, *L_fast_path);
 4863 
 4864   // Fast path check: current thread is initializer thread
 4865   cmpptr(thread, Address(klass, InstanceKlass::init_thread_offset()));
 4866   if (L_slow_path == &amp;L_fallthrough) {
 4867     jcc(Assembler::equal, *L_fast_path);
 4868     bind(*L_slow_path);
 4869   } else if (L_fast_path == &amp;L_fallthrough) {
 4870     jcc(Assembler::notEqual, *L_slow_path);
 4871     bind(*L_fast_path);
 4872   } else {
 4873     Unimplemented();
 4874   }
 4875 }
 4876 
 4877 void MacroAssembler::cmov32(Condition cc, Register dst, Address src) {
 4878   if (VM_Version::supports_cmov()) {
 4879     cmovl(cc, dst, src);
 4880   } else {
 4881     Label L;
 4882     jccb(negate_condition(cc), L);
 4883     movl(dst, src);
 4884     bind(L);
 4885   }
 4886 }
 4887 
 4888 void MacroAssembler::cmov32(Condition cc, Register dst, Register src) {
 4889   if (VM_Version::supports_cmov()) {
 4890     cmovl(cc, dst, src);
 4891   } else {
 4892     Label L;
 4893     jccb(negate_condition(cc), L);
 4894     movl(dst, src);
 4895     bind(L);
 4896   }
 4897 }
 4898 
 4899 void MacroAssembler::_verify_oop(Register reg, const char* s, const char* file, int line) {
 4900   if (!VerifyOops || VerifyAdapterSharing) {
 4901     // Below address of the code string confuses VerifyAdapterSharing
 4902     // because it may differ between otherwise equivalent adapters.
 4903     return;
 4904   }
 4905 
 4906   // Pass register number to verify_oop_subroutine
 4907   const char* b = NULL;
 4908   {
 4909     ResourceMark rm;
 4910     stringStream ss;
 4911     ss.print(&quot;verify_oop: %s: %s (%s:%d)&quot;, reg-&gt;name(), s, file, line);
 4912     b = code_string(ss.as_string());
 4913   }
 4914   BLOCK_COMMENT(&quot;verify_oop {&quot;);
 4915 #ifdef _LP64
 4916   push(rscratch1);                    // save r10, trashed by movptr()
 4917 #endif
 4918   push(rax);                          // save rax,
 4919   push(reg);                          // pass register argument
 4920   ExternalAddress buffer((address) b);
 4921   // avoid using pushptr, as it modifies scratch registers
 4922   // and our contract is not to modify anything
 4923   movptr(rax, buffer.addr());
 4924   push(rax);
 4925   // call indirectly to solve generation ordering problem
 4926   movptr(rax, ExternalAddress(StubRoutines::verify_oop_subroutine_entry_address()));
 4927   call(rax);
 4928   // Caller pops the arguments (oop, message) and restores rax, r10
 4929   BLOCK_COMMENT(&quot;} verify_oop&quot;);
 4930 }
 4931 
 4932 
 4933 RegisterOrConstant MacroAssembler::delayed_value_impl(intptr_t* delayed_value_addr,
 4934                                                       Register tmp,
 4935                                                       int offset) {
 4936   intptr_t value = *delayed_value_addr;
 4937   if (value != 0)
 4938     return RegisterOrConstant(value + offset);
 4939 
 4940   // load indirectly to solve generation ordering problem
 4941   movptr(tmp, ExternalAddress((address) delayed_value_addr));
 4942 
 4943 #ifdef ASSERT
 4944   { Label L;
 4945     testptr(tmp, tmp);
 4946     if (WizardMode) {
 4947       const char* buf = NULL;
 4948       {
 4949         ResourceMark rm;
 4950         stringStream ss;
 4951         ss.print(&quot;DelayedValue=&quot; INTPTR_FORMAT, delayed_value_addr[1]);
 4952         buf = code_string(ss.as_string());
 4953       }
 4954       jcc(Assembler::notZero, L);
 4955       STOP(buf);
 4956     } else {
 4957       jccb(Assembler::notZero, L);
 4958       hlt();
 4959     }
 4960     bind(L);
 4961   }
 4962 #endif
 4963 
 4964   if (offset != 0)
 4965     addptr(tmp, offset);
 4966 
 4967   return RegisterOrConstant(tmp);
 4968 }
 4969 
 4970 
 4971 Address MacroAssembler::argument_address(RegisterOrConstant arg_slot,
 4972                                          int extra_slot_offset) {
 4973   // cf. TemplateTable::prepare_invoke(), if (load_receiver).
 4974   int stackElementSize = Interpreter::stackElementSize;
 4975   int offset = Interpreter::expr_offset_in_bytes(extra_slot_offset+0);
 4976 #ifdef ASSERT
 4977   int offset1 = Interpreter::expr_offset_in_bytes(extra_slot_offset+1);
 4978   assert(offset1 - offset == stackElementSize, &quot;correct arithmetic&quot;);
 4979 #endif
 4980   Register             scale_reg    = noreg;
 4981   Address::ScaleFactor scale_factor = Address::no_scale;
 4982   if (arg_slot.is_constant()) {
 4983     offset += arg_slot.as_constant() * stackElementSize;
 4984   } else {
 4985     scale_reg    = arg_slot.as_register();
 4986     scale_factor = Address::times(stackElementSize);
 4987   }
 4988   offset += wordSize;           // return PC is on stack
 4989   return Address(rsp, scale_reg, scale_factor, offset);
 4990 }
 4991 
 4992 
 4993 void MacroAssembler::_verify_oop_addr(Address addr, const char* s, const char* file, int line) {
 4994   if (!VerifyOops || VerifyAdapterSharing) {
 4995     // Below address of the code string confuses VerifyAdapterSharing
 4996     // because it may differ between otherwise equivalent adapters.
 4997     return;
 4998   }
 4999 
 5000   // Address adjust(addr.base(), addr.index(), addr.scale(), addr.disp() + BytesPerWord);
 5001   // Pass register number to verify_oop_subroutine
 5002   const char* b = NULL;
 5003   {
 5004     ResourceMark rm;
 5005     stringStream ss;
 5006     ss.print(&quot;verify_oop_addr: %s (%s:%d)&quot;, s, file, line);
 5007     b = code_string(ss.as_string());
 5008   }
 5009 #ifdef _LP64
 5010   push(rscratch1);                    // save r10, trashed by movptr()
 5011 #endif
 5012   push(rax);                          // save rax,
 5013   // addr may contain rsp so we will have to adjust it based on the push
 5014   // we just did (and on 64 bit we do two pushes)
 5015   // NOTE: 64bit seemed to have had a bug in that it did movq(addr, rax); which
 5016   // stores rax into addr which is backwards of what was intended.
 5017   if (addr.uses(rsp)) {
 5018     lea(rax, addr);
 5019     pushptr(Address(rax, LP64_ONLY(2 *) BytesPerWord));
 5020   } else {
 5021     pushptr(addr);
 5022   }
 5023 
 5024   ExternalAddress buffer((address) b);
 5025   // pass msg argument
 5026   // avoid using pushptr, as it modifies scratch registers
 5027   // and our contract is not to modify anything
 5028   movptr(rax, buffer.addr());
 5029   push(rax);
 5030 
 5031   // call indirectly to solve generation ordering problem
 5032   movptr(rax, ExternalAddress(StubRoutines::verify_oop_subroutine_entry_address()));
 5033   call(rax);
 5034   // Caller pops the arguments (addr, message) and restores rax, r10.
 5035 }
 5036 
 5037 void MacroAssembler::verify_tlab() {
 5038 #ifdef ASSERT
 5039   if (UseTLAB &amp;&amp; VerifyOops) {
 5040     Label next, ok;
 5041     Register t1 = rsi;
 5042     Register thread_reg = NOT_LP64(rbx) LP64_ONLY(r15_thread);
 5043 
 5044     push(t1);
 5045     NOT_LP64(push(thread_reg));
 5046     NOT_LP64(get_thread(thread_reg));
 5047 
 5048     movptr(t1, Address(thread_reg, in_bytes(JavaThread::tlab_top_offset())));
 5049     cmpptr(t1, Address(thread_reg, in_bytes(JavaThread::tlab_start_offset())));
 5050     jcc(Assembler::aboveEqual, next);
 5051     STOP(&quot;assert(top &gt;= start)&quot;);
 5052     should_not_reach_here();
 5053 
 5054     bind(next);
 5055     movptr(t1, Address(thread_reg, in_bytes(JavaThread::tlab_end_offset())));
 5056     cmpptr(t1, Address(thread_reg, in_bytes(JavaThread::tlab_top_offset())));
 5057     jcc(Assembler::aboveEqual, ok);
 5058     STOP(&quot;assert(top &lt;= end)&quot;);
 5059     should_not_reach_here();
 5060 
 5061     bind(ok);
 5062     NOT_LP64(pop(thread_reg));
 5063     pop(t1);
 5064   }
 5065 #endif
 5066 }
 5067 
 5068 class ControlWord {
 5069  public:
 5070   int32_t _value;
 5071 
 5072   int  rounding_control() const        { return  (_value &gt;&gt; 10) &amp; 3      ; }
 5073   int  precision_control() const       { return  (_value &gt;&gt;  8) &amp; 3      ; }
 5074   bool precision() const               { return ((_value &gt;&gt;  5) &amp; 1) != 0; }
 5075   bool underflow() const               { return ((_value &gt;&gt;  4) &amp; 1) != 0; }
 5076   bool overflow() const                { return ((_value &gt;&gt;  3) &amp; 1) != 0; }
 5077   bool zero_divide() const             { return ((_value &gt;&gt;  2) &amp; 1) != 0; }
 5078   bool denormalized() const            { return ((_value &gt;&gt;  1) &amp; 1) != 0; }
 5079   bool invalid() const                 { return ((_value &gt;&gt;  0) &amp; 1) != 0; }
 5080 
 5081   void print() const {
 5082     // rounding control
 5083     const char* rc;
 5084     switch (rounding_control()) {
 5085       case 0: rc = &quot;round near&quot;; break;
 5086       case 1: rc = &quot;round down&quot;; break;
 5087       case 2: rc = &quot;round up  &quot;; break;
 5088       case 3: rc = &quot;chop      &quot;; break;
 5089     };
 5090     // precision control
 5091     const char* pc;
 5092     switch (precision_control()) {
 5093       case 0: pc = &quot;24 bits &quot;; break;
 5094       case 1: pc = &quot;reserved&quot;; break;
 5095       case 2: pc = &quot;53 bits &quot;; break;
 5096       case 3: pc = &quot;64 bits &quot;; break;
 5097     };
 5098     // flags
 5099     char f[9];
 5100     f[0] = &#39; &#39;;
 5101     f[1] = &#39; &#39;;
 5102     f[2] = (precision   ()) ? &#39;P&#39; : &#39;p&#39;;
 5103     f[3] = (underflow   ()) ? &#39;U&#39; : &#39;u&#39;;
 5104     f[4] = (overflow    ()) ? &#39;O&#39; : &#39;o&#39;;
 5105     f[5] = (zero_divide ()) ? &#39;Z&#39; : &#39;z&#39;;
 5106     f[6] = (denormalized()) ? &#39;D&#39; : &#39;d&#39;;
 5107     f[7] = (invalid     ()) ? &#39;I&#39; : &#39;i&#39;;
 5108     f[8] = &#39;\x0&#39;;
 5109     // output
 5110     printf(&quot;%04x  masks = %s, %s, %s&quot;, _value &amp; 0xFFFF, f, rc, pc);
 5111   }
 5112 
 5113 };
 5114 
 5115 class StatusWord {
 5116  public:
 5117   int32_t _value;
 5118 
 5119   bool busy() const                    { return ((_value &gt;&gt; 15) &amp; 1) != 0; }
 5120   bool C3() const                      { return ((_value &gt;&gt; 14) &amp; 1) != 0; }
 5121   bool C2() const                      { return ((_value &gt;&gt; 10) &amp; 1) != 0; }
 5122   bool C1() const                      { return ((_value &gt;&gt;  9) &amp; 1) != 0; }
 5123   bool C0() const                      { return ((_value &gt;&gt;  8) &amp; 1) != 0; }
 5124   int  top() const                     { return  (_value &gt;&gt; 11) &amp; 7      ; }
 5125   bool error_status() const            { return ((_value &gt;&gt;  7) &amp; 1) != 0; }
 5126   bool stack_fault() const             { return ((_value &gt;&gt;  6) &amp; 1) != 0; }
 5127   bool precision() const               { return ((_value &gt;&gt;  5) &amp; 1) != 0; }
 5128   bool underflow() const               { return ((_value &gt;&gt;  4) &amp; 1) != 0; }
 5129   bool overflow() const                { return ((_value &gt;&gt;  3) &amp; 1) != 0; }
 5130   bool zero_divide() const             { return ((_value &gt;&gt;  2) &amp; 1) != 0; }
 5131   bool denormalized() const            { return ((_value &gt;&gt;  1) &amp; 1) != 0; }
 5132   bool invalid() const                 { return ((_value &gt;&gt;  0) &amp; 1) != 0; }
 5133 
 5134   void print() const {
 5135     // condition codes
 5136     char c[5];
 5137     c[0] = (C3()) ? &#39;3&#39; : &#39;-&#39;;
 5138     c[1] = (C2()) ? &#39;2&#39; : &#39;-&#39;;
 5139     c[2] = (C1()) ? &#39;1&#39; : &#39;-&#39;;
 5140     c[3] = (C0()) ? &#39;0&#39; : &#39;-&#39;;
 5141     c[4] = &#39;\x0&#39;;
 5142     // flags
 5143     char f[9];
 5144     f[0] = (error_status()) ? &#39;E&#39; : &#39;-&#39;;
 5145     f[1] = (stack_fault ()) ? &#39;S&#39; : &#39;-&#39;;
 5146     f[2] = (precision   ()) ? &#39;P&#39; : &#39;-&#39;;
 5147     f[3] = (underflow   ()) ? &#39;U&#39; : &#39;-&#39;;
 5148     f[4] = (overflow    ()) ? &#39;O&#39; : &#39;-&#39;;
 5149     f[5] = (zero_divide ()) ? &#39;Z&#39; : &#39;-&#39;;
 5150     f[6] = (denormalized()) ? &#39;D&#39; : &#39;-&#39;;
 5151     f[7] = (invalid     ()) ? &#39;I&#39; : &#39;-&#39;;
 5152     f[8] = &#39;\x0&#39;;
 5153     // output
 5154     printf(&quot;%04x  flags = %s, cc =  %s, top = %d&quot;, _value &amp; 0xFFFF, f, c, top());
 5155   }
 5156 
 5157 };
 5158 
 5159 class TagWord {
 5160  public:
 5161   int32_t _value;
 5162 
 5163   int tag_at(int i) const              { return (_value &gt;&gt; (i*2)) &amp; 3; }
 5164 
 5165   void print() const {
 5166     printf(&quot;%04x&quot;, _value &amp; 0xFFFF);
 5167   }
 5168 
 5169 };
 5170 
 5171 class FPU_Register {
 5172  public:
 5173   int32_t _m0;
 5174   int32_t _m1;
 5175   int16_t _ex;
 5176 
 5177   bool is_indefinite() const           {
 5178     return _ex == -1 &amp;&amp; _m1 == (int32_t)0xC0000000 &amp;&amp; _m0 == 0;
 5179   }
 5180 
 5181   void print() const {
 5182     char  sign = (_ex &lt; 0) ? &#39;-&#39; : &#39;+&#39;;
 5183     const char* kind = (_ex == 0x7FFF || _ex == (int16_t)-1) ? &quot;NaN&quot; : &quot;   &quot;;
 5184     printf(&quot;%c%04hx.%08x%08x  %s&quot;, sign, _ex, _m1, _m0, kind);
 5185   };
 5186 
 5187 };
 5188 
 5189 class FPU_State {
 5190  public:
 5191   enum {
 5192     register_size       = 10,
 5193     number_of_registers =  8,
 5194     register_mask       =  7
 5195   };
 5196 
 5197   ControlWord  _control_word;
 5198   StatusWord   _status_word;
 5199   TagWord      _tag_word;
 5200   int32_t      _error_offset;
 5201   int32_t      _error_selector;
 5202   int32_t      _data_offset;
 5203   int32_t      _data_selector;
 5204   int8_t       _register[register_size * number_of_registers];
 5205 
 5206   int tag_for_st(int i) const          { return _tag_word.tag_at((_status_word.top() + i) &amp; register_mask); }
 5207   FPU_Register* st(int i) const        { return (FPU_Register*)&amp;_register[register_size * i]; }
 5208 
 5209   const char* tag_as_string(int tag) const {
 5210     switch (tag) {
 5211       case 0: return &quot;valid&quot;;
 5212       case 1: return &quot;zero&quot;;
 5213       case 2: return &quot;special&quot;;
 5214       case 3: return &quot;empty&quot;;
 5215     }
 5216     ShouldNotReachHere();
 5217     return NULL;
 5218   }
 5219 
 5220   void print() const {
 5221     // print computation registers
 5222     { int t = _status_word.top();
 5223       for (int i = 0; i &lt; number_of_registers; i++) {
 5224         int j = (i - t) &amp; register_mask;
 5225         printf(&quot;%c r%d = ST%d = &quot;, (j == 0 ? &#39;*&#39; : &#39; &#39;), i, j);
 5226         st(j)-&gt;print();
 5227         printf(&quot; %s\n&quot;, tag_as_string(_tag_word.tag_at(i)));
 5228       }
 5229     }
 5230     printf(&quot;\n&quot;);
 5231     // print control registers
 5232     printf(&quot;ctrl = &quot;); _control_word.print(); printf(&quot;\n&quot;);
 5233     printf(&quot;stat = &quot;); _status_word .print(); printf(&quot;\n&quot;);
 5234     printf(&quot;tags = &quot;); _tag_word    .print(); printf(&quot;\n&quot;);
 5235   }
 5236 
 5237 };
 5238 
 5239 class Flag_Register {
 5240  public:
 5241   int32_t _value;
 5242 
 5243   bool overflow() const                { return ((_value &gt;&gt; 11) &amp; 1) != 0; }
 5244   bool direction() const               { return ((_value &gt;&gt; 10) &amp; 1) != 0; }
 5245   bool sign() const                    { return ((_value &gt;&gt;  7) &amp; 1) != 0; }
 5246   bool zero() const                    { return ((_value &gt;&gt;  6) &amp; 1) != 0; }
 5247   bool auxiliary_carry() const         { return ((_value &gt;&gt;  4) &amp; 1) != 0; }
 5248   bool parity() const                  { return ((_value &gt;&gt;  2) &amp; 1) != 0; }
 5249   bool carry() const                   { return ((_value &gt;&gt;  0) &amp; 1) != 0; }
 5250 
 5251   void print() const {
 5252     // flags
 5253     char f[8];
 5254     f[0] = (overflow       ()) ? &#39;O&#39; : &#39;-&#39;;
 5255     f[1] = (direction      ()) ? &#39;D&#39; : &#39;-&#39;;
 5256     f[2] = (sign           ()) ? &#39;S&#39; : &#39;-&#39;;
 5257     f[3] = (zero           ()) ? &#39;Z&#39; : &#39;-&#39;;
 5258     f[4] = (auxiliary_carry()) ? &#39;A&#39; : &#39;-&#39;;
 5259     f[5] = (parity         ()) ? &#39;P&#39; : &#39;-&#39;;
 5260     f[6] = (carry          ()) ? &#39;C&#39; : &#39;-&#39;;
 5261     f[7] = &#39;\x0&#39;;
 5262     // output
 5263     printf(&quot;%08x  flags = %s&quot;, _value, f);
 5264   }
 5265 
 5266 };
 5267 
 5268 class IU_Register {
 5269  public:
 5270   int32_t _value;
 5271 
 5272   void print() const {
 5273     printf(&quot;%08x  %11d&quot;, _value, _value);
 5274   }
 5275 
 5276 };
 5277 
 5278 class IU_State {
 5279  public:
 5280   Flag_Register _eflags;
 5281   IU_Register   _rdi;
 5282   IU_Register   _rsi;
 5283   IU_Register   _rbp;
 5284   IU_Register   _rsp;
 5285   IU_Register   _rbx;
 5286   IU_Register   _rdx;
 5287   IU_Register   _rcx;
 5288   IU_Register   _rax;
 5289 
 5290   void print() const {
 5291     // computation registers
 5292     printf(&quot;rax,  = &quot;); _rax.print(); printf(&quot;\n&quot;);
 5293     printf(&quot;rbx,  = &quot;); _rbx.print(); printf(&quot;\n&quot;);
 5294     printf(&quot;rcx  = &quot;); _rcx.print(); printf(&quot;\n&quot;);
 5295     printf(&quot;rdx  = &quot;); _rdx.print(); printf(&quot;\n&quot;);
 5296     printf(&quot;rdi  = &quot;); _rdi.print(); printf(&quot;\n&quot;);
 5297     printf(&quot;rsi  = &quot;); _rsi.print(); printf(&quot;\n&quot;);
 5298     printf(&quot;rbp,  = &quot;); _rbp.print(); printf(&quot;\n&quot;);
 5299     printf(&quot;rsp  = &quot;); _rsp.print(); printf(&quot;\n&quot;);
 5300     printf(&quot;\n&quot;);
 5301     // control registers
 5302     printf(&quot;flgs = &quot;); _eflags.print(); printf(&quot;\n&quot;);
 5303   }
 5304 };
 5305 
 5306 
 5307 class CPU_State {
 5308  public:
 5309   FPU_State _fpu_state;
 5310   IU_State  _iu_state;
 5311 
 5312   void print() const {
 5313     printf(&quot;--------------------------------------------------\n&quot;);
 5314     _iu_state .print();
 5315     printf(&quot;\n&quot;);
 5316     _fpu_state.print();
 5317     printf(&quot;--------------------------------------------------\n&quot;);
 5318   }
 5319 
 5320 };
 5321 
 5322 
 5323 static void _print_CPU_state(CPU_State* state) {
 5324   state-&gt;print();
 5325 };
 5326 
 5327 
 5328 void MacroAssembler::print_CPU_state() {
 5329   push_CPU_state();
 5330   push(rsp);                // pass CPU state
 5331   call(RuntimeAddress(CAST_FROM_FN_PTR(address, _print_CPU_state)));
 5332   addptr(rsp, wordSize);       // discard argument
 5333   pop_CPU_state();
 5334 }
 5335 
 5336 
 5337 #ifndef _LP64
 5338 static bool _verify_FPU(int stack_depth, char* s, CPU_State* state) {
 5339   static int counter = 0;
 5340   FPU_State* fs = &amp;state-&gt;_fpu_state;
 5341   counter++;
 5342   // For leaf calls, only verify that the top few elements remain empty.
 5343   // We only need 1 empty at the top for C2 code.
 5344   if( stack_depth &lt; 0 ) {
 5345     if( fs-&gt;tag_for_st(7) != 3 ) {
 5346       printf(&quot;FPR7 not empty\n&quot;);
 5347       state-&gt;print();
 5348       assert(false, &quot;error&quot;);
 5349       return false;
 5350     }
 5351     return true;                // All other stack states do not matter
 5352   }
 5353 
 5354   assert((fs-&gt;_control_word._value &amp; 0xffff) == StubRoutines::_fpu_cntrl_wrd_std,
 5355          &quot;bad FPU control word&quot;);
 5356 
 5357   // compute stack depth
 5358   int i = 0;
 5359   while (i &lt; FPU_State::number_of_registers &amp;&amp; fs-&gt;tag_for_st(i)  &lt; 3) i++;
 5360   int d = i;
 5361   while (i &lt; FPU_State::number_of_registers &amp;&amp; fs-&gt;tag_for_st(i) == 3) i++;
 5362   // verify findings
 5363   if (i != FPU_State::number_of_registers) {
 5364     // stack not contiguous
 5365     printf(&quot;%s: stack not contiguous at ST%d\n&quot;, s, i);
 5366     state-&gt;print();
 5367     assert(false, &quot;error&quot;);
 5368     return false;
 5369   }
 5370   // check if computed stack depth corresponds to expected stack depth
 5371   if (stack_depth &lt; 0) {
 5372     // expected stack depth is -stack_depth or less
 5373     if (d &gt; -stack_depth) {
 5374       // too many elements on the stack
 5375       printf(&quot;%s: &lt;= %d stack elements expected but found %d\n&quot;, s, -stack_depth, d);
 5376       state-&gt;print();
 5377       assert(false, &quot;error&quot;);
 5378       return false;
 5379     }
 5380   } else {
 5381     // expected stack depth is stack_depth
 5382     if (d != stack_depth) {
 5383       // wrong stack depth
 5384       printf(&quot;%s: %d stack elements expected but found %d\n&quot;, s, stack_depth, d);
 5385       state-&gt;print();
 5386       assert(false, &quot;error&quot;);
 5387       return false;
 5388     }
 5389   }
 5390   // everything is cool
 5391   return true;
 5392 }
 5393 
 5394 void MacroAssembler::verify_FPU(int stack_depth, const char* s) {
 5395   if (!VerifyFPU) return;
 5396   push_CPU_state();
 5397   push(rsp);                // pass CPU state
 5398   ExternalAddress msg((address) s);
 5399   // pass message string s
 5400   pushptr(msg.addr());
 5401   push(stack_depth);        // pass stack depth
 5402   call(RuntimeAddress(CAST_FROM_FN_PTR(address, _verify_FPU)));
 5403   addptr(rsp, 3 * wordSize);   // discard arguments
 5404   // check for error
 5405   { Label L;
 5406     testl(rax, rax);
 5407     jcc(Assembler::notZero, L);
 5408     int3();                  // break if error condition
 5409     bind(L);
 5410   }
 5411   pop_CPU_state();
 5412 }
 5413 #endif // _LP64
 5414 
 5415 void MacroAssembler::restore_cpu_control_state_after_jni() {
 5416   // Either restore the MXCSR register after returning from the JNI Call
 5417   // or verify that it wasn&#39;t changed (with -Xcheck:jni flag).
 5418   if (VM_Version::supports_sse()) {
 5419     if (RestoreMXCSROnJNICalls) {
 5420       ldmxcsr(ExternalAddress(StubRoutines::addr_mxcsr_std()));
 5421     } else if (CheckJNICalls) {
 5422       call(RuntimeAddress(StubRoutines::x86::verify_mxcsr_entry()));
 5423     }
 5424   }
 5425   // Clear upper bits of YMM registers to avoid SSE &lt;-&gt; AVX transition penalty.
 5426   vzeroupper();
 5427   // Reset k1 to 0xffff.
 5428 
 5429 #ifdef COMPILER2
 5430   if (PostLoopMultiversioning &amp;&amp; VM_Version::supports_evex()) {
 5431     push(rcx);
 5432     movl(rcx, 0xffff);
 5433     kmovwl(k1, rcx);
 5434     pop(rcx);
 5435   }
 5436 #endif // COMPILER2
 5437 
 5438 #ifndef _LP64
 5439   // Either restore the x87 floating pointer control word after returning
 5440   // from the JNI call or verify that it wasn&#39;t changed.
 5441   if (CheckJNICalls) {
 5442     call(RuntimeAddress(StubRoutines::x86::verify_fpu_cntrl_wrd_entry()));
 5443   }
 5444 #endif // _LP64
 5445 }
 5446 
 5447 // ((OopHandle)result).resolve();
 5448 void MacroAssembler::resolve_oop_handle(Register result, Register tmp) {
 5449   assert_different_registers(result, tmp);
 5450 
 5451   // Only 64 bit platforms support GCs that require a tmp register
 5452   // Only IN_HEAP loads require a thread_tmp register
 5453   // OopHandle::resolve is an indirection like jobject.
 5454   access_load_at(T_OBJECT, IN_NATIVE,
 5455                  result, Address(result, 0), tmp, /*tmp_thread*/noreg);
 5456 }
 5457 
 5458 // ((WeakHandle)result).resolve();
 5459 void MacroAssembler::resolve_weak_handle(Register rresult, Register rtmp) {
 5460   assert_different_registers(rresult, rtmp);
 5461   Label resolved;
 5462 
 5463   // A null weak handle resolves to null.
 5464   cmpptr(rresult, 0);
 5465   jcc(Assembler::equal, resolved);
 5466 
 5467   // Only 64 bit platforms support GCs that require a tmp register
 5468   // Only IN_HEAP loads require a thread_tmp register
 5469   // WeakHandle::resolve is an indirection like jweak.
 5470   access_load_at(T_OBJECT, IN_NATIVE | ON_PHANTOM_OOP_REF,
 5471                  rresult, Address(rresult, 0), rtmp, /*tmp_thread*/noreg);
 5472   bind(resolved);
 5473 }
 5474 
 5475 void MacroAssembler::load_mirror(Register mirror, Register method, Register tmp) {
 5476   // get mirror
 5477   const int mirror_offset = in_bytes(Klass::java_mirror_offset());
 5478   load_method_holder(mirror, method);
 5479   movptr(mirror, Address(mirror, mirror_offset));
 5480   resolve_oop_handle(mirror, tmp);
 5481 }
 5482 
 5483 void MacroAssembler::load_method_holder_cld(Register rresult, Register rmethod) {
 5484   load_method_holder(rresult, rmethod);
 5485   movptr(rresult, Address(rresult, InstanceKlass::class_loader_data_offset()));
 5486 }
 5487 
 5488 void MacroAssembler::load_metadata(Register dst, Register src) {
 5489   if (UseCompressedClassPointers) {
 5490     movl(dst, Address(src, oopDesc::klass_offset_in_bytes()));
 5491   } else {
 5492     movptr(dst, Address(src, oopDesc::klass_offset_in_bytes()));
 5493   }
 5494 }
 5495 
 5496 void MacroAssembler::load_storage_props(Register dst, Register src) {
 5497   load_metadata(dst, src);
 5498   if (UseCompressedClassPointers) {
 5499     shrl(dst, oopDesc::narrow_storage_props_shift);
 5500   } else {
 5501     shrq(dst, oopDesc::wide_storage_props_shift);
 5502   }
 5503 }
 5504 
 5505 void MacroAssembler::load_method_holder(Register holder, Register method) {
 5506   movptr(holder, Address(method, Method::const_offset()));                      // ConstMethod*
 5507   movptr(holder, Address(holder, ConstMethod::constants_offset()));             // ConstantPool*
 5508   movptr(holder, Address(holder, ConstantPool::pool_holder_offset_in_bytes())); // InstanceKlass*
 5509 }
 5510 
 5511 void MacroAssembler::load_klass(Register dst, Register src) {
 5512   load_metadata(dst, src);
 5513 #ifdef _LP64
 5514   if (UseCompressedClassPointers) {
 5515     andl(dst, oopDesc::compressed_klass_mask());
 5516     decode_klass_not_null(dst);
 5517   } else
 5518 #endif
 5519   {
 5520 #ifdef _LP64
 5521     shlq(dst, oopDesc::storage_props_nof_bits);
 5522     shrq(dst, oopDesc::storage_props_nof_bits);
 5523 #else
 5524     andl(dst, oopDesc::wide_klass_mask());
 5525 #endif
 5526   }
 5527 }
 5528 
 5529 void MacroAssembler::load_prototype_header(Register dst, Register src) {
 5530   load_klass(dst, src);
 5531   movptr(dst, Address(dst, Klass::prototype_header_offset()));
 5532 }
 5533 
 5534 void MacroAssembler::store_klass(Register dst, Register src) {
 5535 #ifdef _LP64
 5536   if (UseCompressedClassPointers) {
 5537     encode_klass_not_null(src);
 5538     movl(Address(dst, oopDesc::klass_offset_in_bytes()), src);
 5539   } else
 5540 #endif
 5541     movptr(Address(dst, oopDesc::klass_offset_in_bytes()), src);
 5542 }
 5543 
 5544 void MacroAssembler::access_load_at(BasicType type, DecoratorSet decorators, Register dst, Address src,
 5545                                     Register tmp1, Register thread_tmp) {
 5546   BarrierSetAssembler* bs = BarrierSet::barrier_set()-&gt;barrier_set_assembler();
 5547   decorators = AccessInternal::decorator_fixup(decorators);
 5548   bool as_raw = (decorators &amp; AS_RAW) != 0;
 5549   if (as_raw) {
 5550     bs-&gt;BarrierSetAssembler::load_at(this, decorators, type, dst, src, tmp1, thread_tmp);
 5551   } else {
 5552     bs-&gt;load_at(this, decorators, type, dst, src, tmp1, thread_tmp);
 5553   }
 5554 }
 5555 
 5556 void MacroAssembler::access_store_at(BasicType type, DecoratorSet decorators, Address dst, Register src,
 5557                                      Register tmp1, Register tmp2, Register tmp3) {
 5558   BarrierSetAssembler* bs = BarrierSet::barrier_set()-&gt;barrier_set_assembler();
 5559   decorators = AccessInternal::decorator_fixup(decorators);
 5560   bool as_raw = (decorators &amp; AS_RAW) != 0;
 5561   if (as_raw) {
 5562     bs-&gt;BarrierSetAssembler::store_at(this, decorators, type, dst, src, tmp1, tmp2, tmp3);
 5563   } else {
 5564     bs-&gt;store_at(this, decorators, type, dst, src, tmp1, tmp2, tmp3);
 5565   }
 5566 }
 5567 
 5568 void MacroAssembler::access_value_copy(DecoratorSet decorators, Register src, Register dst,
 5569                                        Register value_klass) {
 5570   BarrierSetAssembler* bs = BarrierSet::barrier_set()-&gt;barrier_set_assembler();
 5571   bs-&gt;value_copy(this, decorators, src, dst, value_klass);
 5572 }
 5573 
 5574 void MacroAssembler::first_field_offset(Register value_klass, Register offset) {
 5575   movptr(offset, Address(value_klass, InstanceKlass::adr_valueklass_fixed_block_offset()));
 5576   movl(offset, Address(offset, ValueKlass::first_field_offset_offset()));
 5577 }
 5578 
 5579 void MacroAssembler::data_for_oop(Register oop, Register data, Register value_klass) {
 5580   // ((address) (void*) o) + vk-&gt;first_field_offset();
 5581   Register offset = (data == oop) ? rscratch1 : data;
 5582   first_field_offset(value_klass, offset);
 5583   if (data == oop) {
 5584     addptr(data, offset);
 5585   } else {
 5586     lea(data, Address(oop, offset));
 5587   }
 5588 }
 5589 
 5590 void MacroAssembler::data_for_value_array_index(Register array, Register array_klass,
 5591                                                 Register index, Register data) {
 5592   assert(index != rcx, &quot;index needs to shift by rcx&quot;);
 5593   assert_different_registers(array, array_klass, index);
 5594   assert_different_registers(rcx, array, index);
 5595 
 5596   // array-&gt;base() + (index &lt;&lt; Klass::layout_helper_log2_element_size(lh));
 5597   movl(rcx, Address(array_klass, Klass::layout_helper_offset()));
 5598 
 5599   // Klass::layout_helper_log2_element_size(lh)
 5600   // (lh &gt;&gt; _lh_log2_element_size_shift) &amp; _lh_log2_element_size_mask;
 5601   shrl(rcx, Klass::_lh_log2_element_size_shift);
 5602   andl(rcx, Klass::_lh_log2_element_size_mask);
 5603   shlptr(index); // index &lt;&lt; rcx
 5604 
 5605   lea(data, Address(array, index, Address::times_1, arrayOopDesc::base_offset_in_bytes(T_VALUETYPE)));
 5606 }
 5607 
 5608 void MacroAssembler::resolve(DecoratorSet decorators, Register obj) {
 5609   // Use stronger ACCESS_WRITE|ACCESS_READ by default.
 5610   if ((decorators &amp; (ACCESS_READ | ACCESS_WRITE)) == 0) {
 5611     decorators |= ACCESS_READ | ACCESS_WRITE;
 5612   }
 5613   BarrierSetAssembler* bs = BarrierSet::barrier_set()-&gt;barrier_set_assembler();
 5614   return bs-&gt;resolve(this, decorators, obj);
 5615 }
 5616 
 5617 void MacroAssembler::load_heap_oop(Register dst, Address src, Register tmp1,
 5618                                    Register thread_tmp, DecoratorSet decorators) {
 5619   access_load_at(T_OBJECT, IN_HEAP | decorators, dst, src, tmp1, thread_tmp);
 5620 }
 5621 
 5622 // Doesn&#39;t do verfication, generates fixed size code
 5623 void MacroAssembler::load_heap_oop_not_null(Register dst, Address src, Register tmp1,
 5624                                             Register thread_tmp, DecoratorSet decorators) {
 5625   access_load_at(T_OBJECT, IN_HEAP | IS_NOT_NULL | decorators, dst, src, tmp1, thread_tmp);
 5626 }
 5627 
 5628 void MacroAssembler::store_heap_oop(Address dst, Register src, Register tmp1,
 5629                                     Register tmp2, Register tmp3, DecoratorSet decorators) {
 5630   access_store_at(T_OBJECT, IN_HEAP | decorators, dst, src, tmp1, tmp2, tmp3);
 5631 }
 5632 
 5633 // Used for storing NULLs.
 5634 void MacroAssembler::store_heap_oop_null(Address dst) {
 5635   access_store_at(T_OBJECT, IN_HEAP, dst, noreg, noreg, noreg, noreg);
 5636 }
 5637 
 5638 #ifdef _LP64
 5639 void MacroAssembler::store_klass_gap(Register dst, Register src) {
 5640   if (UseCompressedClassPointers) {
 5641     // Store to klass gap in destination
 5642     movl(Address(dst, oopDesc::klass_gap_offset_in_bytes()), src);
 5643   }
 5644 }
 5645 
 5646 #ifdef ASSERT
 5647 void MacroAssembler::verify_heapbase(const char* msg) {
 5648   assert (UseCompressedOops, &quot;should be compressed&quot;);
 5649   assert (Universe::heap() != NULL, &quot;java heap should be initialized&quot;);
 5650   if (CheckCompressedOops) {
 5651     Label ok;
 5652     push(rscratch1); // cmpptr trashes rscratch1
 5653     cmpptr(r12_heapbase, ExternalAddress((address)CompressedOops::ptrs_base_addr()));
 5654     jcc(Assembler::equal, ok);
 5655     STOP(msg);
 5656     bind(ok);
 5657     pop(rscratch1);
 5658   }
 5659 }
 5660 #endif
 5661 
 5662 // Algorithm must match oop.inline.hpp encode_heap_oop.
 5663 void MacroAssembler::encode_heap_oop(Register r) {
 5664 #ifdef ASSERT
 5665   verify_heapbase(&quot;MacroAssembler::encode_heap_oop: heap base corrupted?&quot;);
 5666 #endif
 5667   verify_oop_msg(r, &quot;broken oop in encode_heap_oop&quot;);
 5668   if (CompressedOops::base() == NULL) {
 5669     if (CompressedOops::shift() != 0) {
 5670       assert (LogMinObjAlignmentInBytes == CompressedOops::shift(), &quot;decode alg wrong&quot;);
 5671       shrq(r, LogMinObjAlignmentInBytes);
 5672     }
 5673     return;
 5674   }
 5675   testq(r, r);
 5676   cmovq(Assembler::equal, r, r12_heapbase);
 5677   subq(r, r12_heapbase);
 5678   shrq(r, LogMinObjAlignmentInBytes);
 5679 }
 5680 
 5681 void MacroAssembler::encode_heap_oop_not_null(Register r) {
 5682 #ifdef ASSERT
 5683   verify_heapbase(&quot;MacroAssembler::encode_heap_oop_not_null: heap base corrupted?&quot;);
 5684   if (CheckCompressedOops) {
 5685     Label ok;
 5686     testq(r, r);
 5687     jcc(Assembler::notEqual, ok);
 5688     STOP(&quot;null oop passed to encode_heap_oop_not_null&quot;);
 5689     bind(ok);
 5690   }
 5691 #endif
 5692   verify_oop_msg(r, &quot;broken oop in encode_heap_oop_not_null&quot;);
 5693   if (CompressedOops::base() != NULL) {
 5694     subq(r, r12_heapbase);
 5695   }
 5696   if (CompressedOops::shift() != 0) {
 5697     assert (LogMinObjAlignmentInBytes == CompressedOops::shift(), &quot;decode alg wrong&quot;);
 5698     shrq(r, LogMinObjAlignmentInBytes);
 5699   }
 5700 }
 5701 
 5702 void MacroAssembler::encode_heap_oop_not_null(Register dst, Register src) {
 5703 #ifdef ASSERT
 5704   verify_heapbase(&quot;MacroAssembler::encode_heap_oop_not_null2: heap base corrupted?&quot;);
 5705   if (CheckCompressedOops) {
 5706     Label ok;
 5707     testq(src, src);
 5708     jcc(Assembler::notEqual, ok);
 5709     STOP(&quot;null oop passed to encode_heap_oop_not_null2&quot;);
 5710     bind(ok);
 5711   }
 5712 #endif
 5713   verify_oop_msg(src, &quot;broken oop in encode_heap_oop_not_null2&quot;);
 5714   if (dst != src) {
 5715     movq(dst, src);
 5716   }
 5717   if (CompressedOops::base() != NULL) {
 5718     subq(dst, r12_heapbase);
 5719   }
 5720   if (CompressedOops::shift() != 0) {
 5721     assert (LogMinObjAlignmentInBytes == CompressedOops::shift(), &quot;decode alg wrong&quot;);
 5722     shrq(dst, LogMinObjAlignmentInBytes);
 5723   }
 5724 }
 5725 
 5726 void  MacroAssembler::decode_heap_oop(Register r) {
 5727 #ifdef ASSERT
 5728   verify_heapbase(&quot;MacroAssembler::decode_heap_oop: heap base corrupted?&quot;);
 5729 #endif
 5730   if (CompressedOops::base() == NULL) {
 5731     if (CompressedOops::shift() != 0) {
 5732       assert (LogMinObjAlignmentInBytes == CompressedOops::shift(), &quot;decode alg wrong&quot;);
 5733       shlq(r, LogMinObjAlignmentInBytes);
 5734     }
 5735   } else {
 5736     Label done;
 5737     shlq(r, LogMinObjAlignmentInBytes);
 5738     jccb(Assembler::equal, done);
 5739     addq(r, r12_heapbase);
 5740     bind(done);
 5741   }
 5742   verify_oop_msg(r, &quot;broken oop in decode_heap_oop&quot;);
 5743 }
 5744 
 5745 void  MacroAssembler::decode_heap_oop_not_null(Register r) {
 5746   // Note: it will change flags
 5747   assert (UseCompressedOops, &quot;should only be used for compressed headers&quot;);
 5748   assert (Universe::heap() != NULL, &quot;java heap should be initialized&quot;);
 5749   // Cannot assert, unverified entry point counts instructions (see .ad file)
 5750   // vtableStubs also counts instructions in pd_code_size_limit.
 5751   // Also do not verify_oop as this is called by verify_oop.
 5752   if (CompressedOops::shift() != 0) {
 5753     assert(LogMinObjAlignmentInBytes == CompressedOops::shift(), &quot;decode alg wrong&quot;);
 5754     shlq(r, LogMinObjAlignmentInBytes);
 5755     if (CompressedOops::base() != NULL) {
 5756       addq(r, r12_heapbase);
 5757     }
 5758   } else {
 5759     assert (CompressedOops::base() == NULL, &quot;sanity&quot;);
 5760   }
 5761 }
 5762 
 5763 void  MacroAssembler::decode_heap_oop_not_null(Register dst, Register src) {
 5764   // Note: it will change flags
 5765   assert (UseCompressedOops, &quot;should only be used for compressed headers&quot;);
 5766   assert (Universe::heap() != NULL, &quot;java heap should be initialized&quot;);
 5767   // Cannot assert, unverified entry point counts instructions (see .ad file)
 5768   // vtableStubs also counts instructions in pd_code_size_limit.
 5769   // Also do not verify_oop as this is called by verify_oop.
 5770   if (CompressedOops::shift() != 0) {
 5771     assert(LogMinObjAlignmentInBytes == CompressedOops::shift(), &quot;decode alg wrong&quot;);
 5772     if (LogMinObjAlignmentInBytes == Address::times_8) {
 5773       leaq(dst, Address(r12_heapbase, src, Address::times_8, 0));
 5774     } else {
 5775       if (dst != src) {
 5776         movq(dst, src);
 5777       }
 5778       shlq(dst, LogMinObjAlignmentInBytes);
 5779       if (CompressedOops::base() != NULL) {
 5780         addq(dst, r12_heapbase);
 5781       }
 5782     }
 5783   } else {
 5784     assert (CompressedOops::base() == NULL, &quot;sanity&quot;);
 5785     if (dst != src) {
 5786       movq(dst, src);
 5787     }
 5788   }
 5789 }
 5790 
 5791 void MacroAssembler::encode_klass_not_null(Register r) {
 5792   if (CompressedKlassPointers::base() != NULL) {
 5793     // Use r12 as a scratch register in which to temporarily load the narrow_klass_base.
 5794     assert(r != r12_heapbase, &quot;Encoding a klass in r12&quot;);
 5795     mov64(r12_heapbase, (int64_t)CompressedKlassPointers::base());
 5796     subq(r, r12_heapbase);
 5797   }
 5798   if (CompressedKlassPointers::shift() != 0) {
 5799     assert (LogKlassAlignmentInBytes == CompressedKlassPointers::shift(), &quot;decode alg wrong&quot;);
 5800     shrq(r, LogKlassAlignmentInBytes);
 5801   }
 5802   if (CompressedKlassPointers::base() != NULL) {
 5803     reinit_heapbase();
 5804   }
 5805 }
 5806 
 5807 void MacroAssembler::encode_klass_not_null(Register dst, Register src) {
 5808   if (dst == src) {
 5809     encode_klass_not_null(src);
 5810   } else {
 5811     if (CompressedKlassPointers::base() != NULL) {
 5812       mov64(dst, (int64_t)CompressedKlassPointers::base());
 5813       negq(dst);
 5814       addq(dst, src);
 5815     } else {
 5816       movptr(dst, src);
 5817     }
 5818     if (CompressedKlassPointers::shift() != 0) {
 5819       assert (LogKlassAlignmentInBytes == CompressedKlassPointers::shift(), &quot;decode alg wrong&quot;);
 5820       shrq(dst, LogKlassAlignmentInBytes);
 5821     }
 5822   }
 5823 }
 5824 
 5825 // Function instr_size_for_decode_klass_not_null() counts the instructions
 5826 // generated by decode_klass_not_null(register r) and reinit_heapbase(),
 5827 // when (Universe::heap() != NULL).  Hence, if the instructions they
 5828 // generate change, then this method needs to be updated.
 5829 int MacroAssembler::instr_size_for_decode_klass_not_null() {
 5830   assert (UseCompressedClassPointers, &quot;only for compressed klass ptrs&quot;);
 5831   if (CompressedKlassPointers::base() != NULL) {
 5832     // mov64 + addq + shlq? + mov64  (for reinit_heapbase()).
 5833     return (CompressedKlassPointers::shift() == 0 ? 20 : 24);
 5834   } else {
 5835     // longest load decode klass function, mov64, leaq
 5836     return 16;
 5837   }
 5838 }
 5839 
 5840 // !!! If the instructions that get generated here change then function
 5841 // instr_size_for_decode_klass_not_null() needs to get updated.
 5842 void  MacroAssembler::decode_klass_not_null(Register r) {
 5843   // Note: it will change flags
 5844   assert (UseCompressedClassPointers, &quot;should only be used for compressed headers&quot;);
 5845   assert(r != r12_heapbase, &quot;Decoding a klass in r12&quot;);
 5846   // Cannot assert, unverified entry point counts instructions (see .ad file)
 5847   // vtableStubs also counts instructions in pd_code_size_limit.
 5848   // Also do not verify_oop as this is called by verify_oop.
 5849   if (CompressedKlassPointers::shift() != 0) {
 5850     assert(LogKlassAlignmentInBytes == CompressedKlassPointers::shift(), &quot;decode alg wrong&quot;);
 5851     shlq(r, LogKlassAlignmentInBytes);
 5852   }
 5853   // Use r12 as a scratch register in which to temporarily load the narrow_klass_base.
 5854   if (CompressedKlassPointers::base() != NULL) {
 5855     mov64(r12_heapbase, (int64_t)CompressedKlassPointers::base());
 5856     addq(r, r12_heapbase);
 5857     reinit_heapbase();
 5858   }
 5859 }
 5860 
 5861 void  MacroAssembler::decode_klass_not_null(Register dst, Register src) {
 5862   // Note: it will change flags
 5863   assert (UseCompressedClassPointers, &quot;should only be used for compressed headers&quot;);
 5864   if (dst == src) {
 5865     decode_klass_not_null(dst);
 5866   } else {
 5867     // Cannot assert, unverified entry point counts instructions (see .ad file)
 5868     // vtableStubs also counts instructions in pd_code_size_limit.
 5869     // Also do not verify_oop as this is called by verify_oop.
 5870     mov64(dst, (int64_t)CompressedKlassPointers::base());
 5871     if (CompressedKlassPointers::shift() != 0) {
 5872       assert(LogKlassAlignmentInBytes == CompressedKlassPointers::shift(), &quot;decode alg wrong&quot;);
 5873       assert(LogKlassAlignmentInBytes == Address::times_8, &quot;klass not aligned on 64bits?&quot;);
 5874       leaq(dst, Address(dst, src, Address::times_8, 0));
 5875     } else {
 5876       addq(dst, src);
 5877     }
 5878   }
 5879 }
 5880 
 5881 void  MacroAssembler::set_narrow_oop(Register dst, jobject obj) {
 5882   assert (UseCompressedOops, &quot;should only be used for compressed headers&quot;);
 5883   assert (Universe::heap() != NULL, &quot;java heap should be initialized&quot;);
 5884   assert (oop_recorder() != NULL, &quot;this assembler needs an OopRecorder&quot;);
 5885   int oop_index = oop_recorder()-&gt;find_index(obj);
 5886   RelocationHolder rspec = oop_Relocation::spec(oop_index);
 5887   mov_narrow_oop(dst, oop_index, rspec);
 5888 }
 5889 
 5890 void  MacroAssembler::set_narrow_oop(Address dst, jobject obj) {
 5891   assert (UseCompressedOops, &quot;should only be used for compressed headers&quot;);
 5892   assert (Universe::heap() != NULL, &quot;java heap should be initialized&quot;);
 5893   assert (oop_recorder() != NULL, &quot;this assembler needs an OopRecorder&quot;);
 5894   int oop_index = oop_recorder()-&gt;find_index(obj);
 5895   RelocationHolder rspec = oop_Relocation::spec(oop_index);
 5896   mov_narrow_oop(dst, oop_index, rspec);
 5897 }
 5898 
 5899 void  MacroAssembler::set_narrow_klass(Register dst, Klass* k) {
 5900   assert (UseCompressedClassPointers, &quot;should only be used for compressed headers&quot;);
 5901   assert (oop_recorder() != NULL, &quot;this assembler needs an OopRecorder&quot;);
 5902   int klass_index = oop_recorder()-&gt;find_index(k);
 5903   RelocationHolder rspec = metadata_Relocation::spec(klass_index);
 5904   mov_narrow_oop(dst, CompressedKlassPointers::encode(k), rspec);
 5905 }
 5906 
 5907 void  MacroAssembler::set_narrow_klass(Address dst, Klass* k) {
 5908   assert (UseCompressedClassPointers, &quot;should only be used for compressed headers&quot;);
 5909   assert (oop_recorder() != NULL, &quot;this assembler needs an OopRecorder&quot;);
 5910   int klass_index = oop_recorder()-&gt;find_index(k);
 5911   RelocationHolder rspec = metadata_Relocation::spec(klass_index);
 5912   mov_narrow_oop(dst, CompressedKlassPointers::encode(k), rspec);
 5913 }
 5914 
 5915 void  MacroAssembler::cmp_narrow_oop(Register dst, jobject obj) {
 5916   assert (UseCompressedOops, &quot;should only be used for compressed headers&quot;);
 5917   assert (Universe::heap() != NULL, &quot;java heap should be initialized&quot;);
 5918   assert (oop_recorder() != NULL, &quot;this assembler needs an OopRecorder&quot;);
 5919   int oop_index = oop_recorder()-&gt;find_index(obj);
 5920   RelocationHolder rspec = oop_Relocation::spec(oop_index);
 5921   Assembler::cmp_narrow_oop(dst, oop_index, rspec);
 5922 }
 5923 
 5924 void  MacroAssembler::cmp_narrow_oop(Address dst, jobject obj) {
 5925   assert (UseCompressedOops, &quot;should only be used for compressed headers&quot;);
 5926   assert (Universe::heap() != NULL, &quot;java heap should be initialized&quot;);
 5927   assert (oop_recorder() != NULL, &quot;this assembler needs an OopRecorder&quot;);
 5928   int oop_index = oop_recorder()-&gt;find_index(obj);
 5929   RelocationHolder rspec = oop_Relocation::spec(oop_index);
 5930   Assembler::cmp_narrow_oop(dst, oop_index, rspec);
 5931 }
 5932 
 5933 void  MacroAssembler::cmp_narrow_klass(Register dst, Klass* k) {
 5934   assert (UseCompressedClassPointers, &quot;should only be used for compressed headers&quot;);
 5935   assert (oop_recorder() != NULL, &quot;this assembler needs an OopRecorder&quot;);
 5936   int klass_index = oop_recorder()-&gt;find_index(k);
 5937   RelocationHolder rspec = metadata_Relocation::spec(klass_index);
 5938   Assembler::cmp_narrow_oop(dst, CompressedKlassPointers::encode(k), rspec);
 5939 }
 5940 
 5941 void  MacroAssembler::cmp_narrow_klass(Address dst, Klass* k) {
 5942   assert (UseCompressedClassPointers, &quot;should only be used for compressed headers&quot;);
 5943   assert (oop_recorder() != NULL, &quot;this assembler needs an OopRecorder&quot;);
 5944   int klass_index = oop_recorder()-&gt;find_index(k);
 5945   RelocationHolder rspec = metadata_Relocation::spec(klass_index);
 5946   Assembler::cmp_narrow_oop(dst, CompressedKlassPointers::encode(k), rspec);
 5947 }
 5948 
 5949 void MacroAssembler::reinit_heapbase() {
 5950   if (UseCompressedOops || UseCompressedClassPointers) {
 5951     if (Universe::heap() != NULL) {
 5952       if (CompressedOops::base() == NULL) {
 5953         MacroAssembler::xorptr(r12_heapbase, r12_heapbase);
 5954       } else {
 5955         mov64(r12_heapbase, (int64_t)CompressedOops::ptrs_base());
 5956       }
 5957     } else {
 5958       movptr(r12_heapbase, ExternalAddress((address)CompressedOops::ptrs_base_addr()));
 5959     }
 5960   }
 5961 }
 5962 
 5963 #endif // _LP64
 5964 
 5965 // C2 compiled method&#39;s prolog code.
 5966 void MacroAssembler::verified_entry(Compile* C, int sp_inc) {
 5967   int framesize = C-&gt;frame_size_in_bytes();
 5968   int bangsize = C-&gt;bang_size_in_bytes();
 5969   bool fp_mode_24b = false;
 5970   int stack_bang_size = C-&gt;need_stack_bang(bangsize) ? bangsize : 0;
 5971 
 5972   // WARNING: Initial instruction MUST be 5 bytes or longer so that
 5973   // NativeJump::patch_verified_entry will be able to patch out the entry
 5974   // code safely. The push to verify stack depth is ok at 5 bytes,
 5975   // the frame allocation can be either 3 or 6 bytes. So if we don&#39;t do
 5976   // stack bang then we must use the 6 byte frame allocation even if
 5977   // we have no frame. :-(
 5978   assert(stack_bang_size &gt;= framesize || stack_bang_size &lt;= 0, &quot;stack bang size incorrect&quot;);
 5979 
 5980   assert((framesize &amp; (StackAlignmentInBytes-1)) == 0, &quot;frame size not aligned&quot;);
 5981   // Remove word for return addr
 5982   framesize -= wordSize;
 5983   stack_bang_size -= wordSize;
 5984 
 5985   // Calls to C2R adapters often do not accept exceptional returns.
 5986   // We require that their callers must bang for them.  But be careful, because
 5987   // some VM calls (such as call site linkage) can use several kilobytes of
 5988   // stack.  But the stack safety zone should account for that.
 5989   // See bugs 4446381, 4468289, 4497237.
 5990   if (stack_bang_size &gt; 0) {
 5991     generate_stack_overflow_check(stack_bang_size);
 5992 
 5993     // We always push rbp, so that on return to interpreter rbp, will be
 5994     // restored correctly and we can correct the stack.
 5995     push(rbp);
 5996     // Save caller&#39;s stack pointer into RBP if the frame pointer is preserved.
 5997     if (PreserveFramePointer) {
 5998       mov(rbp, rsp);
 5999     }
 6000     // Remove word for ebp
 6001     framesize -= wordSize;
 6002 
 6003     // Create frame
 6004     if (framesize) {
 6005       subptr(rsp, framesize);
 6006     }
 6007   } else {
 6008     // Create frame (force generation of a 4 byte immediate value)
 6009     subptr_imm32(rsp, framesize);
 6010 
 6011     // Save RBP register now.
 6012     framesize -= wordSize;
 6013     movptr(Address(rsp, framesize), rbp);
 6014     // Save caller&#39;s stack pointer into RBP if the frame pointer is preserved.
 6015     if (PreserveFramePointer) {
 6016       movptr(rbp, rsp);
 6017       if (framesize &gt; 0) {
 6018         addptr(rbp, framesize);
 6019       }
 6020     }
 6021   }
 6022 
 6023   if (C-&gt;needs_stack_repair()) {
 6024     // Save stack increment (also account for fixed framesize and rbp)
 6025     assert((sp_inc &amp; (StackAlignmentInBytes-1)) == 0, &quot;stack increment not aligned&quot;);
 6026     movptr(Address(rsp, C-&gt;sp_inc_offset()), sp_inc + framesize + wordSize);
 6027   }
 6028 
 6029   if (VerifyStackAtCalls) { // Majik cookie to verify stack depth
 6030     framesize -= wordSize;
 6031     movptr(Address(rsp, framesize), (int32_t)0xbadb100d);
 6032   }
 6033 
 6034 #ifndef _LP64
 6035   // If method sets FPU control word do it now
 6036   if (fp_mode_24b) {
 6037     fldcw(ExternalAddress(StubRoutines::addr_fpu_cntrl_wrd_24()));
 6038   }
 6039   if (UseSSE &gt;= 2 &amp;&amp; VerifyFPU) {
 6040     verify_FPU(0, &quot;FPU stack must be clean on entry&quot;);
 6041   }
 6042 #endif
 6043 
 6044 #ifdef ASSERT
 6045   if (VerifyStackAtCalls) {
 6046     Label L;
 6047     push(rax);
 6048     mov(rax, rsp);
 6049     andptr(rax, StackAlignmentInBytes-1);
 6050     cmpptr(rax, StackAlignmentInBytes-wordSize);
 6051     pop(rax);
 6052     jcc(Assembler::equal, L);
 6053     STOP(&quot;Stack is not properly aligned!&quot;);
 6054     bind(L);
 6055   }
 6056 #endif
 6057 }
 6058 
 6059 // clear memory of size &#39;cnt&#39; qwords, starting at &#39;base&#39; using XMM/YMM registers
 6060 void MacroAssembler::xmm_clear_mem(Register base, Register cnt, Register val, XMMRegister xtmp) {
 6061   // cnt - number of qwords (8-byte words).
 6062   // base - start address, qword aligned.
 6063   Label L_zero_64_bytes, L_loop, L_sloop, L_tail, L_end;
 6064   movdq(xtmp, val);
 6065   if (UseAVX &gt;= 2) {
 6066     punpcklqdq(xtmp, xtmp);
 6067     vinserti128_high(xtmp, xtmp);
 6068   } else {
 6069     punpcklqdq(xtmp, xtmp);
 6070   }
 6071   jmp(L_zero_64_bytes);
 6072 
 6073   BIND(L_loop);
 6074   if (UseAVX &gt;= 2) {
 6075     vmovdqu(Address(base,  0), xtmp);
 6076     vmovdqu(Address(base, 32), xtmp);
 6077   } else {
 6078     movdqu(Address(base,  0), xtmp);
 6079     movdqu(Address(base, 16), xtmp);
 6080     movdqu(Address(base, 32), xtmp);
 6081     movdqu(Address(base, 48), xtmp);
 6082   }
 6083   addptr(base, 64);
 6084 
 6085   BIND(L_zero_64_bytes);
 6086   subptr(cnt, 8);
 6087   jccb(Assembler::greaterEqual, L_loop);
 6088   addptr(cnt, 4);
 6089   jccb(Assembler::less, L_tail);
 6090   // Copy trailing 32 bytes
 6091   if (UseAVX &gt;= 2) {
 6092     vmovdqu(Address(base, 0), xtmp);
 6093   } else {
 6094     movdqu(Address(base,  0), xtmp);
 6095     movdqu(Address(base, 16), xtmp);
 6096   }
 6097   addptr(base, 32);
 6098   subptr(cnt, 4);
 6099 
 6100   BIND(L_tail);
 6101   addptr(cnt, 4);
 6102   jccb(Assembler::lessEqual, L_end);
 6103   decrement(cnt);
 6104 
 6105   BIND(L_sloop);
 6106   movq(Address(base, 0), xtmp);
 6107   addptr(base, 8);
 6108   decrement(cnt);
 6109   jccb(Assembler::greaterEqual, L_sloop);
 6110   BIND(L_end);
 6111 }
 6112 
 6113 int MacroAssembler::store_value_type_fields_to_buf(ciValueKlass* vk, bool from_interpreter) {
 6114   // A value type might be returned. If fields are in registers we
 6115   // need to allocate a value type instance and initialize it with
 6116   // the value of the fields.
 6117   Label skip;
 6118   // We only need a new buffered value if a new one is not returned
 6119   testptr(rax, 1);
 6120   jcc(Assembler::zero, skip);
 6121   int call_offset = -1;
 6122 
 6123 #ifdef _LP64
 6124   Label slow_case;
 6125 
 6126   // Try to allocate a new buffered value (from the heap)
 6127   if (UseTLAB) {
 6128     // FIXME -- for smaller code, the inline allocation (and the slow case) should be moved inside the pack handler.
 6129     if (vk != NULL) {
 6130       // Called from C1, where the return type is statically known.
 6131       movptr(rbx, (intptr_t)vk-&gt;get_ValueKlass());
 6132       jint lh = vk-&gt;layout_helper();
 6133       assert(lh != Klass::_lh_neutral_value, &quot;inline class in return type must have been resolved&quot;);
 6134       movl(r14, lh);
 6135     } else {
 6136       // Call from interpreter. RAX contains ((the ValueKlass* of the return type) | 0x01)
 6137       mov(rbx, rax);
 6138       andptr(rbx, -2);
 6139       movl(r14, Address(rbx, Klass::layout_helper_offset()));
 6140     }
 6141 
 6142     movptr(r13, Address(r15_thread, in_bytes(JavaThread::tlab_top_offset())));
 6143     lea(r14, Address(r13, r14, Address::times_1));
 6144     cmpptr(r14, Address(r15_thread, in_bytes(JavaThread::tlab_end_offset())));
 6145     jcc(Assembler::above, slow_case);
 6146     movptr(Address(r15_thread, in_bytes(JavaThread::tlab_top_offset())), r14);
 6147     movptr(Address(r13, oopDesc::mark_offset_in_bytes()), (intptr_t)markWord::always_locked_prototype().value());
 6148 
 6149     xorl(rax, rax); // use zero reg to clear memory (shorter code)
 6150     store_klass_gap(r13, rax);  // zero klass gap for compressed oops
 6151 
 6152     if (vk == NULL) {
 6153       // store_klass corrupts rbx, so save it in rax for later use (interpreter case only).
 6154       mov(rax, rbx);
 6155     }
 6156     store_klass(r13, rbx);  // klass
 6157 
 6158     // We have our new buffered value, initialize its fields with a
 6159     // value class specific handler
 6160     if (vk != NULL) {
 6161       // FIXME -- do the packing in-line to avoid the runtime call
 6162       mov(rax, r13);
 6163       call(RuntimeAddress(vk-&gt;pack_handler())); // no need for call info as this will not safepoint.
 6164     } else {
 6165       movptr(rbx, Address(rax, InstanceKlass::adr_valueklass_fixed_block_offset()));
 6166       movptr(rbx, Address(rbx, ValueKlass::pack_handler_offset()));
 6167       mov(rax, r13);
 6168       call(rbx);
 6169     }
 6170     jmp(skip);
 6171   }
 6172 
 6173   bind(slow_case);
 6174   // We failed to allocate a new value, fall back to a runtime
 6175   // call. Some oop field may be live in some registers but we can&#39;t
 6176   // tell. That runtime call will take care of preserving them
 6177   // across a GC if there&#39;s one.
 6178 #endif
 6179 
 6180   if (from_interpreter) {
 6181     super_call_VM_leaf(StubRoutines::store_value_type_fields_to_buf());
 6182   } else {
 6183     call(RuntimeAddress(StubRoutines::store_value_type_fields_to_buf()));
 6184     call_offset = offset();
 6185   }
 6186 
 6187   bind(skip);
 6188   return call_offset;
 6189 }
 6190 
 6191 
 6192 // Move a value between registers/stack slots and update the reg_state
 6193 bool MacroAssembler::move_helper(VMReg from, VMReg to, BasicType bt, RegState reg_state[], int ret_off, int extra_stack_offset) {
 6194   if (reg_state[to-&gt;value()] == reg_written) {
 6195     return true; // Already written
 6196   }
 6197   if (from != to &amp;&amp; bt != T_VOID) {
 6198     if (reg_state[to-&gt;value()] == reg_readonly) {
 6199       return false; // Not yet writable
 6200     }
 6201     if (from-&gt;is_reg()) {
 6202       if (to-&gt;is_reg()) {
 6203         if (from-&gt;is_XMMRegister()) {
 6204           if (bt == T_DOUBLE) {
 6205             movdbl(to-&gt;as_XMMRegister(), from-&gt;as_XMMRegister());
 6206           } else {
 6207             assert(bt == T_FLOAT, &quot;must be float&quot;);
 6208             movflt(to-&gt;as_XMMRegister(), from-&gt;as_XMMRegister());
 6209           }
 6210         } else {
 6211           movq(to-&gt;as_Register(), from-&gt;as_Register());
 6212         }
 6213       } else {
 6214         int st_off = to-&gt;reg2stack() * VMRegImpl::stack_slot_size + extra_stack_offset;
 6215         assert(st_off != ret_off, &quot;overwriting return address at %d&quot;, st_off);
 6216         Address to_addr = Address(rsp, st_off);
 6217         if (from-&gt;is_XMMRegister()) {
 6218           if (bt == T_DOUBLE) {
 6219             movdbl(to_addr, from-&gt;as_XMMRegister());
 6220           } else {
 6221             assert(bt == T_FLOAT, &quot;must be float&quot;);
 6222             movflt(to_addr, from-&gt;as_XMMRegister());
 6223           }
 6224         } else {
 6225           movq(to_addr, from-&gt;as_Register());
 6226         }
 6227       }
 6228     } else {
 6229       Address from_addr = Address(rsp, from-&gt;reg2stack() * VMRegImpl::stack_slot_size + extra_stack_offset);
 6230       if (to-&gt;is_reg()) {
 6231         if (to-&gt;is_XMMRegister()) {
 6232           if (bt == T_DOUBLE) {
 6233             movdbl(to-&gt;as_XMMRegister(), from_addr);
 6234           } else {
 6235             assert(bt == T_FLOAT, &quot;must be float&quot;);
 6236             movflt(to-&gt;as_XMMRegister(), from_addr);
 6237           }
 6238         } else {
 6239           movq(to-&gt;as_Register(), from_addr);
 6240         }
 6241       } else {
 6242         int st_off = to-&gt;reg2stack() * VMRegImpl::stack_slot_size + extra_stack_offset;
 6243         assert(st_off != ret_off, &quot;overwriting return address at %d&quot;, st_off);
 6244         movq(r13, from_addr);
 6245         movq(Address(rsp, st_off), r13);
 6246       }
 6247     }
 6248   }
 6249   // Update register states
 6250   reg_state[from-&gt;value()] = reg_writable;
 6251   reg_state[to-&gt;value()] = reg_written;
 6252   return true;
 6253 }
 6254 
 6255 // Read all fields from a value type oop and store the values in registers/stack slots
 6256 bool MacroAssembler::unpack_value_helper(const GrowableArray&lt;SigEntry&gt;* sig, int&amp; sig_index, VMReg from, VMRegPair* regs_to,
 6257                                          int&amp; to_index, RegState reg_state[], int ret_off, int extra_stack_offset) {
 6258   Register fromReg = from-&gt;is_reg() ? from-&gt;as_Register() : noreg;
 6259   assert(sig-&gt;at(sig_index)._bt == T_VOID, &quot;should be at end delimiter&quot;);
 6260 
 6261   int vt = 1;
 6262   bool done = true;
 6263   bool mark_done = true;
 6264   do {
 6265     sig_index--;
 6266     BasicType bt = sig-&gt;at(sig_index)._bt;
 6267     if (bt == T_VALUETYPE) {
 6268       vt--;
 6269     } else if (bt == T_VOID &amp;&amp;
 6270                sig-&gt;at(sig_index-1)._bt != T_LONG &amp;&amp;
 6271                sig-&gt;at(sig_index-1)._bt != T_DOUBLE) {
 6272       vt++;
 6273     } else if (SigEntry::is_reserved_entry(sig, sig_index)) {
 6274       to_index--; // Ignore this
 6275     } else {
 6276       assert(to_index &gt;= 0, &quot;invalid to_index&quot;);
 6277       VMRegPair pair_to = regs_to[to_index--];
 6278       VMReg to = pair_to.first();
 6279 
 6280       if (bt == T_VOID) continue;
 6281 
 6282       int idx = (int)to-&gt;value();
 6283       if (reg_state[idx] == reg_readonly) {
 6284          if (idx != from-&gt;value()) {
 6285            mark_done = false;
 6286          }
 6287          done = false;
 6288          continue;
 6289       } else if (reg_state[idx] == reg_written) {
 6290         continue;
 6291       } else {
 6292         assert(reg_state[idx] == reg_writable, &quot;must be writable&quot;);
 6293         reg_state[idx] = reg_written;
 6294        }
 6295 
 6296       if (fromReg == noreg) {
 6297         int st_off = from-&gt;reg2stack() * VMRegImpl::stack_slot_size + extra_stack_offset;
 6298         movq(r10, Address(rsp, st_off));
 6299         fromReg = r10;
 6300       }
 6301 
 6302       int off = sig-&gt;at(sig_index)._offset;
 6303       assert(off &gt; 0, &quot;offset in object should be positive&quot;);
 6304       bool is_oop = (bt == T_OBJECT || bt == T_ARRAY);
 6305 
 6306       Address fromAddr = Address(fromReg, off);
 6307       bool is_signed = (bt != T_CHAR) &amp;&amp; (bt != T_BOOLEAN);
 6308       if (!to-&gt;is_XMMRegister()) {
 6309         Register dst = to-&gt;is_stack() ? r13 : to-&gt;as_Register();
 6310         if (is_oop) {
 6311           load_heap_oop(dst, fromAddr);
 6312         } else {
 6313           load_sized_value(dst, fromAddr, type2aelembytes(bt), is_signed);
 6314         }
 6315         if (to-&gt;is_stack()) {
 6316           int st_off = to-&gt;reg2stack() * VMRegImpl::stack_slot_size + extra_stack_offset;
 6317           assert(st_off != ret_off, &quot;overwriting return address at %d&quot;, st_off);
 6318           movq(Address(rsp, st_off), dst);
 6319         }
 6320       } else {
 6321         if (bt == T_DOUBLE) {
 6322           movdbl(to-&gt;as_XMMRegister(), fromAddr);
 6323         } else {
 6324           assert(bt == T_FLOAT, &quot;must be float&quot;);
 6325           movflt(to-&gt;as_XMMRegister(), fromAddr);
 6326         }
 6327       }
 6328     }
 6329   } while (vt != 0);
 6330   if (mark_done &amp;&amp; reg_state[from-&gt;value()] != reg_written) {
 6331     // This is okay because no one else will write to that slot
 6332     reg_state[from-&gt;value()] = reg_writable;
 6333   }
 6334   return done;
 6335 }
 6336 
 6337 // Pack fields back into a value type oop
 6338 bool MacroAssembler::pack_value_helper(const GrowableArray&lt;SigEntry&gt;* sig, int&amp; sig_index, int vtarg_index,
 6339                                        VMReg to, VMRegPair* regs_from, int regs_from_count, int&amp; from_index, RegState reg_state[],
 6340                                        int ret_off, int extra_stack_offset) {
 6341   assert(sig-&gt;at(sig_index)._bt == T_VALUETYPE, &quot;should be at end delimiter&quot;);
 6342   assert(to-&gt;is_valid(), &quot;must be&quot;);
 6343 
 6344   if (reg_state[to-&gt;value()] == reg_written) {
 6345     skip_unpacked_fields(sig, sig_index, regs_from, regs_from_count, from_index);
 6346     return true; // Already written
 6347   }
 6348 
 6349   Register val_array = rax;
 6350   Register val_obj_tmp = r11;
 6351   Register from_reg_tmp = r14; // Be careful with r14 because it&#39;s used for spilling
 6352   Register tmp1 = r10;
 6353   Register tmp2 = r13;
 6354   Register tmp3 = rbx;
 6355   Register val_obj = to-&gt;is_stack() ? val_obj_tmp : to-&gt;as_Register();
 6356 
 6357   if (reg_state[to-&gt;value()] == reg_readonly) {
 6358     if (!is_reg_in_unpacked_fields(sig, sig_index, to, regs_from, regs_from_count, from_index)) {
 6359       skip_unpacked_fields(sig, sig_index, regs_from, regs_from_count, from_index);
 6360       return false; // Not yet writable
 6361     }
 6362     val_obj = val_obj_tmp;
 6363   }
 6364 
 6365   int index = arrayOopDesc::base_offset_in_bytes(T_OBJECT) + vtarg_index * type2aelembytes(T_VALUETYPE);
 6366   load_heap_oop(val_obj, Address(val_array, index));
 6367 
 6368   ScalarizedValueArgsStream stream(sig, sig_index, regs_from, regs_from_count, from_index);
 6369   VMRegPair from_pair;
 6370   BasicType bt;
 6371   while (stream.next(from_pair, bt)) {
 6372     int off = sig-&gt;at(stream.sig_cc_index())._offset;
 6373     assert(off &gt; 0, &quot;offset in object should be positive&quot;);
 6374     bool is_oop = (bt == T_OBJECT || bt == T_ARRAY);
 6375     size_t size_in_bytes = is_java_primitive(bt) ? type2aelembytes(bt) : wordSize;
 6376 
 6377     VMReg from_r1 = from_pair.first();
 6378     VMReg from_r2 = from_pair.second();
 6379 
 6380     // Pack the scalarized field into the value object.
 6381     Address dst(val_obj, off);
 6382     if (!from_r1-&gt;is_XMMRegister()) {
 6383       Register from_reg;
 6384       if (from_r1-&gt;is_stack()) {
 6385         from_reg = from_reg_tmp;
 6386         int ld_off = from_r1-&gt;reg2stack() * VMRegImpl::stack_slot_size + extra_stack_offset;
 6387         load_sized_value(from_reg, Address(rsp, ld_off), size_in_bytes, /* is_signed */ false);
 6388       } else {
 6389         from_reg = from_r1-&gt;as_Register();
 6390       }
 6391       assert_different_registers(dst.base(), from_reg, tmp1, tmp2, tmp3, val_array);
 6392       if (is_oop) {
 6393         store_heap_oop(dst, from_reg, tmp1, tmp2, tmp3, IN_HEAP | ACCESS_WRITE | IS_DEST_UNINITIALIZED);
 6394       } else {
 6395         store_sized_value(dst, from_reg, size_in_bytes);
 6396       }
 6397     } else {
 6398       if (from_r2-&gt;is_valid()) {
 6399         movdbl(dst, from_r1-&gt;as_XMMRegister());
 6400       } else {
 6401         movflt(dst, from_r1-&gt;as_XMMRegister());
 6402       }
 6403     }
 6404     reg_state[from_r1-&gt;value()] = reg_writable;
 6405   }
 6406   sig_index = stream.sig_cc_index();
 6407   from_index = stream.regs_cc_index();
 6408 
 6409   assert(reg_state[to-&gt;value()] == reg_writable, &quot;must have already been read&quot;);
 6410   bool success = move_helper(val_obj-&gt;as_VMReg(), to, T_OBJECT, reg_state, ret_off, extra_stack_offset);
 6411   assert(success, &quot;to register must be writeable&quot;);
 6412 
 6413   return true;
 6414 }
 6415 
 6416 // Unpack all value type arguments passed as oops
 6417 void MacroAssembler::unpack_value_args(Compile* C, bool receiver_only) {
 6418   int sp_inc = unpack_value_args_common(C, receiver_only);
 6419   // Emit code for verified entry and save increment for stack repair on return
 6420   verified_entry(C, sp_inc);
 6421 }
 6422 
 6423 void MacroAssembler::shuffle_value_args(bool is_packing, bool receiver_only, int extra_stack_offset,
 6424                                         BasicType* sig_bt, const GrowableArray&lt;SigEntry&gt;* sig_cc,
 6425                                         int args_passed, int args_on_stack, VMRegPair* regs,
 6426                                         int args_passed_to, int args_on_stack_to, VMRegPair* regs_to, int sp_inc) {
 6427   // Check if we need to extend the stack for packing/unpacking
 6428   if (sp_inc &gt; 0 &amp;&amp; !is_packing) {
 6429     // Save the return address, adjust the stack (make sure it is properly
 6430     // 16-byte aligned) and copy the return address to the new top of the stack.
 6431     // (Note: C1 does this in C1_MacroAssembler::scalarized_entry).
 6432     pop(r13);
 6433     subptr(rsp, sp_inc);
 6434     push(r13);
 6435   }
 6436 
 6437   int ret_off; // make sure we don&#39;t overwrite the return address
 6438   if (is_packing) {
 6439     // For C1 code, the VVEP doesn&#39;t have reserved slots, so we store the returned address at
 6440     // rsp[0] during shuffling.
 6441     ret_off = 0;
 6442   } else {
 6443     // C2 code ensures that sp_inc is a reserved slot.
 6444     ret_off = sp_inc;
 6445   }
 6446 
 6447   shuffle_value_args_common(is_packing, receiver_only, extra_stack_offset,
 6448                             sig_bt, sig_cc,
 6449                             args_passed, args_on_stack, regs,
 6450                             args_passed_to, args_on_stack_to, regs_to,
 6451                             sp_inc, ret_off);
 6452 }
 6453 
 6454 VMReg MacroAssembler::spill_reg_for(VMReg reg) {
 6455   return reg-&gt;is_XMMRegister() ? xmm8-&gt;as_VMReg() : r14-&gt;as_VMReg();
 6456 }
 6457 
 6458 void MacroAssembler::remove_frame(int initial_framesize, bool needs_stack_repair, int sp_inc_offset) {
 6459   assert((initial_framesize &amp; (StackAlignmentInBytes-1)) == 0, &quot;frame size not aligned&quot;);
 6460   if (needs_stack_repair) {
 6461     // Restore rbp and repair rsp by adding the stack increment
 6462     movq(rbp, Address(rsp, initial_framesize));
 6463     addq(rsp, Address(rsp, sp_inc_offset));
 6464   } else {
 6465     if (initial_framesize &gt; 0) {
 6466       addq(rsp, initial_framesize);
 6467     }
 6468     pop(rbp);
 6469   }
 6470 }
 6471 
 6472 void MacroAssembler::clear_mem(Register base, Register cnt, Register val, XMMRegister xtmp, bool is_large, bool word_copy_only) {
 6473   // cnt - number of qwords (8-byte words).
 6474   // base - start address, qword aligned.
 6475   // is_large - if optimizers know cnt is larger than InitArrayShortSize
 6476   assert(base==rdi, &quot;base register must be edi for rep stos&quot;);
 6477   assert(val==rax,   &quot;tmp register must be eax for rep stos&quot;);
 6478   assert(cnt==rcx,   &quot;cnt register must be ecx for rep stos&quot;);
 6479   assert(InitArrayShortSize % BytesPerLong == 0,
 6480     &quot;InitArrayShortSize should be the multiple of BytesPerLong&quot;);
 6481 
 6482   Label DONE;
 6483 
 6484   if (!is_large) {
 6485     Label LOOP, LONG;
 6486     cmpptr(cnt, InitArrayShortSize/BytesPerLong);
 6487     jccb(Assembler::greater, LONG);
 6488 
 6489     NOT_LP64(shlptr(cnt, 1);) // convert to number of 32-bit words for 32-bit VM
 6490 
 6491     decrement(cnt);
 6492     jccb(Assembler::negative, DONE); // Zero length
 6493 
 6494     // Use individual pointer-sized stores for small counts:
 6495     BIND(LOOP);
 6496     movptr(Address(base, cnt, Address::times_ptr), val);
 6497     decrement(cnt);
 6498     jccb(Assembler::greaterEqual, LOOP);
 6499     jmpb(DONE);
 6500 
 6501     BIND(LONG);
 6502   }
 6503 
 6504   // Use longer rep-prefixed ops for non-small counts:
 6505   if (UseFastStosb &amp;&amp; !word_copy_only) {
 6506     shlptr(cnt, 3); // convert to number of bytes
 6507     rep_stosb();
 6508   } else if (UseXMMForObjInit) {
 6509     xmm_clear_mem(base, cnt, val, xtmp);
 6510   } else {
 6511     NOT_LP64(shlptr(cnt, 1);) // convert to number of 32-bit words for 32-bit VM
 6512     rep_stos();
 6513   }
 6514 
 6515   BIND(DONE);
 6516 }
 6517 
 6518 #ifdef COMPILER2
 6519 
 6520 // IndexOf for constant substrings with size &gt;= 8 chars
 6521 // which don&#39;t need to be loaded through stack.
 6522 void MacroAssembler::string_indexofC8(Register str1, Register str2,
 6523                                       Register cnt1, Register cnt2,
 6524                                       int int_cnt2,  Register result,
 6525                                       XMMRegister vec, Register tmp,
 6526                                       int ae) {
 6527   ShortBranchVerifier sbv(this);
 6528   assert(UseSSE42Intrinsics, &quot;SSE4.2 intrinsics are required&quot;);
 6529   assert(ae != StrIntrinsicNode::LU, &quot;Invalid encoding&quot;);
 6530 
 6531   // This method uses the pcmpestri instruction with bound registers
 6532   //   inputs:
 6533   //     xmm - substring
 6534   //     rax - substring length (elements count)
 6535   //     mem - scanned string
 6536   //     rdx - string length (elements count)
 6537   //     0xd - mode: 1100 (substring search) + 01 (unsigned shorts)
 6538   //     0xc - mode: 1100 (substring search) + 00 (unsigned bytes)
 6539   //   outputs:
 6540   //     rcx - matched index in string
 6541   assert(cnt1 == rdx &amp;&amp; cnt2 == rax &amp;&amp; tmp == rcx, &quot;pcmpestri&quot;);
 6542   int mode   = (ae == StrIntrinsicNode::LL) ? 0x0c : 0x0d; // bytes or shorts
 6543   int stride = (ae == StrIntrinsicNode::LL) ? 16 : 8; //UU, UL -&gt; 8
 6544   Address::ScaleFactor scale1 = (ae == StrIntrinsicNode::LL) ? Address::times_1 : Address::times_2;
 6545   Address::ScaleFactor scale2 = (ae == StrIntrinsicNode::UL) ? Address::times_1 : scale1;
 6546 
 6547   Label RELOAD_SUBSTR, SCAN_TO_SUBSTR, SCAN_SUBSTR,
 6548         RET_FOUND, RET_NOT_FOUND, EXIT, FOUND_SUBSTR,
 6549         MATCH_SUBSTR_HEAD, RELOAD_STR, FOUND_CANDIDATE;
 6550 
 6551   // Note, inline_string_indexOf() generates checks:
 6552   // if (substr.count &gt; string.count) return -1;
 6553   // if (substr.count == 0) return 0;
 6554   assert(int_cnt2 &gt;= stride, &quot;this code is used only for cnt2 &gt;= 8 chars&quot;);
 6555 
 6556   // Load substring.
 6557   if (ae == StrIntrinsicNode::UL) {
 6558     pmovzxbw(vec, Address(str2, 0));
 6559   } else {
 6560     movdqu(vec, Address(str2, 0));
 6561   }
 6562   movl(cnt2, int_cnt2);
 6563   movptr(result, str1); // string addr
 6564 
 6565   if (int_cnt2 &gt; stride) {
 6566     jmpb(SCAN_TO_SUBSTR);
 6567 
 6568     // Reload substr for rescan, this code
 6569     // is executed only for large substrings (&gt; 8 chars)
 6570     bind(RELOAD_SUBSTR);
 6571     if (ae == StrIntrinsicNode::UL) {
 6572       pmovzxbw(vec, Address(str2, 0));
 6573     } else {
 6574       movdqu(vec, Address(str2, 0));
 6575     }
 6576     negptr(cnt2); // Jumped here with negative cnt2, convert to positive
 6577 
 6578     bind(RELOAD_STR);
 6579     // We came here after the beginning of the substring was
 6580     // matched but the rest of it was not so we need to search
 6581     // again. Start from the next element after the previous match.
 6582 
 6583     // cnt2 is number of substring reminding elements and
 6584     // cnt1 is number of string reminding elements when cmp failed.
 6585     // Restored cnt1 = cnt1 - cnt2 + int_cnt2
 6586     subl(cnt1, cnt2);
 6587     addl(cnt1, int_cnt2);
 6588     movl(cnt2, int_cnt2); // Now restore cnt2
 6589 
 6590     decrementl(cnt1);     // Shift to next element
 6591     cmpl(cnt1, cnt2);
 6592     jcc(Assembler::negative, RET_NOT_FOUND);  // Left less then substring
 6593 
 6594     addptr(result, (1&lt;&lt;scale1));
 6595 
 6596   } // (int_cnt2 &gt; 8)
 6597 
 6598   // Scan string for start of substr in 16-byte vectors
 6599   bind(SCAN_TO_SUBSTR);
 6600   pcmpestri(vec, Address(result, 0), mode);
 6601   jccb(Assembler::below, FOUND_CANDIDATE);   // CF == 1
 6602   subl(cnt1, stride);
 6603   jccb(Assembler::lessEqual, RET_NOT_FOUND); // Scanned full string
 6604   cmpl(cnt1, cnt2);
 6605   jccb(Assembler::negative, RET_NOT_FOUND);  // Left less then substring
 6606   addptr(result, 16);
 6607   jmpb(SCAN_TO_SUBSTR);
 6608 
 6609   // Found a potential substr
 6610   bind(FOUND_CANDIDATE);
 6611   // Matched whole vector if first element matched (tmp(rcx) == 0).
 6612   if (int_cnt2 == stride) {
 6613     jccb(Assembler::overflow, RET_FOUND);    // OF == 1
 6614   } else { // int_cnt2 &gt; 8
 6615     jccb(Assembler::overflow, FOUND_SUBSTR);
 6616   }
 6617   // After pcmpestri tmp(rcx) contains matched element index
 6618   // Compute start addr of substr
 6619   lea(result, Address(result, tmp, scale1));
 6620 
 6621   // Make sure string is still long enough
 6622   subl(cnt1, tmp);
 6623   cmpl(cnt1, cnt2);
 6624   if (int_cnt2 == stride) {
 6625     jccb(Assembler::greaterEqual, SCAN_TO_SUBSTR);
 6626   } else { // int_cnt2 &gt; 8
 6627     jccb(Assembler::greaterEqual, MATCH_SUBSTR_HEAD);
 6628   }
 6629   // Left less then substring.
 6630 
 6631   bind(RET_NOT_FOUND);
 6632   movl(result, -1);
 6633   jmp(EXIT);
 6634 
 6635   if (int_cnt2 &gt; stride) {
 6636     // This code is optimized for the case when whole substring
 6637     // is matched if its head is matched.
 6638     bind(MATCH_SUBSTR_HEAD);
 6639     pcmpestri(vec, Address(result, 0), mode);
 6640     // Reload only string if does not match
 6641     jcc(Assembler::noOverflow, RELOAD_STR); // OF == 0
 6642 
 6643     Label CONT_SCAN_SUBSTR;
 6644     // Compare the rest of substring (&gt; 8 chars).
 6645     bind(FOUND_SUBSTR);
 6646     // First 8 chars are already matched.
 6647     negptr(cnt2);
 6648     addptr(cnt2, stride);
 6649 
 6650     bind(SCAN_SUBSTR);
 6651     subl(cnt1, stride);
 6652     cmpl(cnt2, -stride); // Do not read beyond substring
 6653     jccb(Assembler::lessEqual, CONT_SCAN_SUBSTR);
 6654     // Back-up strings to avoid reading beyond substring:
 6655     // cnt1 = cnt1 - cnt2 + 8
 6656     addl(cnt1, cnt2); // cnt2 is negative
 6657     addl(cnt1, stride);
 6658     movl(cnt2, stride); negptr(cnt2);
 6659     bind(CONT_SCAN_SUBSTR);
 6660     if (int_cnt2 &lt; (int)G) {
 6661       int tail_off1 = int_cnt2&lt;&lt;scale1;
 6662       int tail_off2 = int_cnt2&lt;&lt;scale2;
 6663       if (ae == StrIntrinsicNode::UL) {
 6664         pmovzxbw(vec, Address(str2, cnt2, scale2, tail_off2));
 6665       } else {
 6666         movdqu(vec, Address(str2, cnt2, scale2, tail_off2));
 6667       }
 6668       pcmpestri(vec, Address(result, cnt2, scale1, tail_off1), mode);
 6669     } else {
 6670       // calculate index in register to avoid integer overflow (int_cnt2*2)
 6671       movl(tmp, int_cnt2);
 6672       addptr(tmp, cnt2);
 6673       if (ae == StrIntrinsicNode::UL) {
 6674         pmovzxbw(vec, Address(str2, tmp, scale2, 0));
 6675       } else {
 6676         movdqu(vec, Address(str2, tmp, scale2, 0));
 6677       }
 6678       pcmpestri(vec, Address(result, tmp, scale1, 0), mode);
 6679     }
 6680     // Need to reload strings pointers if not matched whole vector
 6681     jcc(Assembler::noOverflow, RELOAD_SUBSTR); // OF == 0
 6682     addptr(cnt2, stride);
 6683     jcc(Assembler::negative, SCAN_SUBSTR);
 6684     // Fall through if found full substring
 6685 
 6686   } // (int_cnt2 &gt; 8)
 6687 
 6688   bind(RET_FOUND);
 6689   // Found result if we matched full small substring.
 6690   // Compute substr offset
 6691   subptr(result, str1);
 6692   if (ae == StrIntrinsicNode::UU || ae == StrIntrinsicNode::UL) {
 6693     shrl(result, 1); // index
 6694   }
 6695   bind(EXIT);
 6696 
 6697 } // string_indexofC8
 6698 
 6699 // Small strings are loaded through stack if they cross page boundary.
 6700 void MacroAssembler::string_indexof(Register str1, Register str2,
 6701                                     Register cnt1, Register cnt2,
 6702                                     int int_cnt2,  Register result,
 6703                                     XMMRegister vec, Register tmp,
 6704                                     int ae) {
 6705   ShortBranchVerifier sbv(this);
 6706   assert(UseSSE42Intrinsics, &quot;SSE4.2 intrinsics are required&quot;);
 6707   assert(ae != StrIntrinsicNode::LU, &quot;Invalid encoding&quot;);
 6708 
 6709   //
 6710   // int_cnt2 is length of small (&lt; 8 chars) constant substring
 6711   // or (-1) for non constant substring in which case its length
 6712   // is in cnt2 register.
 6713   //
 6714   // Note, inline_string_indexOf() generates checks:
 6715   // if (substr.count &gt; string.count) return -1;
 6716   // if (substr.count == 0) return 0;
 6717   //
 6718   int stride = (ae == StrIntrinsicNode::LL) ? 16 : 8; //UU, UL -&gt; 8
 6719   assert(int_cnt2 == -1 || (0 &lt; int_cnt2 &amp;&amp; int_cnt2 &lt; stride), &quot;should be != 0&quot;);
 6720   // This method uses the pcmpestri instruction with bound registers
 6721   //   inputs:
 6722   //     xmm - substring
 6723   //     rax - substring length (elements count)
 6724   //     mem - scanned string
 6725   //     rdx - string length (elements count)
 6726   //     0xd - mode: 1100 (substring search) + 01 (unsigned shorts)
 6727   //     0xc - mode: 1100 (substring search) + 00 (unsigned bytes)
 6728   //   outputs:
 6729   //     rcx - matched index in string
 6730   assert(cnt1 == rdx &amp;&amp; cnt2 == rax &amp;&amp; tmp == rcx, &quot;pcmpestri&quot;);
 6731   int mode = (ae == StrIntrinsicNode::LL) ? 0x0c : 0x0d; // bytes or shorts
 6732   Address::ScaleFactor scale1 = (ae == StrIntrinsicNode::LL) ? Address::times_1 : Address::times_2;
 6733   Address::ScaleFactor scale2 = (ae == StrIntrinsicNode::UL) ? Address::times_1 : scale1;
 6734 
 6735   Label RELOAD_SUBSTR, SCAN_TO_SUBSTR, SCAN_SUBSTR, ADJUST_STR,
 6736         RET_FOUND, RET_NOT_FOUND, CLEANUP, FOUND_SUBSTR,
 6737         FOUND_CANDIDATE;
 6738 
 6739   { //========================================================
 6740     // We don&#39;t know where these strings are located
 6741     // and we can&#39;t read beyond them. Load them through stack.
 6742     Label BIG_STRINGS, CHECK_STR, COPY_SUBSTR, COPY_STR;
 6743 
 6744     movptr(tmp, rsp); // save old SP
 6745 
 6746     if (int_cnt2 &gt; 0) {     // small (&lt; 8 chars) constant substring
 6747       if (int_cnt2 == (1&gt;&gt;scale2)) { // One byte
 6748         assert((ae == StrIntrinsicNode::LL || ae == StrIntrinsicNode::UL), &quot;Only possible for latin1 encoding&quot;);
 6749         load_unsigned_byte(result, Address(str2, 0));
 6750         movdl(vec, result); // move 32 bits
 6751       } else if (ae == StrIntrinsicNode::LL &amp;&amp; int_cnt2 == 3) {  // Three bytes
 6752         // Not enough header space in 32-bit VM: 12+3 = 15.
 6753         movl(result, Address(str2, -1));
 6754         shrl(result, 8);
 6755         movdl(vec, result); // move 32 bits
 6756       } else if (ae != StrIntrinsicNode::UL &amp;&amp; int_cnt2 == (2&gt;&gt;scale2)) {  // One char
 6757         load_unsigned_short(result, Address(str2, 0));
 6758         movdl(vec, result); // move 32 bits
 6759       } else if (ae != StrIntrinsicNode::UL &amp;&amp; int_cnt2 == (4&gt;&gt;scale2)) { // Two chars
 6760         movdl(vec, Address(str2, 0)); // move 32 bits
 6761       } else if (ae != StrIntrinsicNode::UL &amp;&amp; int_cnt2 == (8&gt;&gt;scale2)) { // Four chars
 6762         movq(vec, Address(str2, 0));  // move 64 bits
 6763       } else { // cnt2 = { 3, 5, 6, 7 } || (ae == StrIntrinsicNode::UL &amp;&amp; cnt2 ={2, ..., 7})
 6764         // Array header size is 12 bytes in 32-bit VM
 6765         // + 6 bytes for 3 chars == 18 bytes,
 6766         // enough space to load vec and shift.
 6767         assert(HeapWordSize*TypeArrayKlass::header_size() &gt;= 12,&quot;sanity&quot;);
 6768         if (ae == StrIntrinsicNode::UL) {
 6769           int tail_off = int_cnt2-8;
 6770           pmovzxbw(vec, Address(str2, tail_off));
 6771           psrldq(vec, -2*tail_off);
 6772         }
 6773         else {
 6774           int tail_off = int_cnt2*(1&lt;&lt;scale2);
 6775           movdqu(vec, Address(str2, tail_off-16));
 6776           psrldq(vec, 16-tail_off);
 6777         }
 6778       }
 6779     } else { // not constant substring
 6780       cmpl(cnt2, stride);
 6781       jccb(Assembler::aboveEqual, BIG_STRINGS); // Both strings are big enough
 6782 
 6783       // We can read beyond string if srt+16 does not cross page boundary
 6784       // since heaps are aligned and mapped by pages.
 6785       assert(os::vm_page_size() &lt; (int)G, &quot;default page should be small&quot;);
 6786       movl(result, str2); // We need only low 32 bits
 6787       andl(result, (os::vm_page_size()-1));
 6788       cmpl(result, (os::vm_page_size()-16));
 6789       jccb(Assembler::belowEqual, CHECK_STR);
 6790 
 6791       // Move small strings to stack to allow load 16 bytes into vec.
 6792       subptr(rsp, 16);
 6793       int stk_offset = wordSize-(1&lt;&lt;scale2);
 6794       push(cnt2);
 6795 
 6796       bind(COPY_SUBSTR);
 6797       if (ae == StrIntrinsicNode::LL || ae == StrIntrinsicNode::UL) {
 6798         load_unsigned_byte(result, Address(str2, cnt2, scale2, -1));
 6799         movb(Address(rsp, cnt2, scale2, stk_offset), result);
 6800       } else if (ae == StrIntrinsicNode::UU) {
 6801         load_unsigned_short(result, Address(str2, cnt2, scale2, -2));
 6802         movw(Address(rsp, cnt2, scale2, stk_offset), result);
 6803       }
 6804       decrement(cnt2);
 6805       jccb(Assembler::notZero, COPY_SUBSTR);
 6806 
 6807       pop(cnt2);
 6808       movptr(str2, rsp);  // New substring address
 6809     } // non constant
 6810 
 6811     bind(CHECK_STR);
 6812     cmpl(cnt1, stride);
 6813     jccb(Assembler::aboveEqual, BIG_STRINGS);
 6814 
 6815     // Check cross page boundary.
 6816     movl(result, str1); // We need only low 32 bits
 6817     andl(result, (os::vm_page_size()-1));
 6818     cmpl(result, (os::vm_page_size()-16));
 6819     jccb(Assembler::belowEqual, BIG_STRINGS);
 6820 
 6821     subptr(rsp, 16);
 6822     int stk_offset = -(1&lt;&lt;scale1);
 6823     if (int_cnt2 &lt; 0) { // not constant
 6824       push(cnt2);
 6825       stk_offset += wordSize;
 6826     }
 6827     movl(cnt2, cnt1);
 6828 
 6829     bind(COPY_STR);
 6830     if (ae == StrIntrinsicNode::LL) {
 6831       load_unsigned_byte(result, Address(str1, cnt2, scale1, -1));
 6832       movb(Address(rsp, cnt2, scale1, stk_offset), result);
 6833     } else {
 6834       load_unsigned_short(result, Address(str1, cnt2, scale1, -2));
 6835       movw(Address(rsp, cnt2, scale1, stk_offset), result);
 6836     }
 6837     decrement(cnt2);
 6838     jccb(Assembler::notZero, COPY_STR);
 6839 
 6840     if (int_cnt2 &lt; 0) { // not constant
 6841       pop(cnt2);
 6842     }
 6843     movptr(str1, rsp);  // New string address
 6844 
 6845     bind(BIG_STRINGS);
 6846     // Load substring.
 6847     if (int_cnt2 &lt; 0) { // -1
 6848       if (ae == StrIntrinsicNode::UL) {
 6849         pmovzxbw(vec, Address(str2, 0));
 6850       } else {
 6851         movdqu(vec, Address(str2, 0));
 6852       }
 6853       push(cnt2);       // substr count
 6854       push(str2);       // substr addr
 6855       push(str1);       // string addr
 6856     } else {
 6857       // Small (&lt; 8 chars) constant substrings are loaded already.
 6858       movl(cnt2, int_cnt2);
 6859     }
 6860     push(tmp);  // original SP
 6861 
 6862   } // Finished loading
 6863 
 6864   //========================================================
 6865   // Start search
 6866   //
 6867 
 6868   movptr(result, str1); // string addr
 6869 
 6870   if (int_cnt2  &lt; 0) {  // Only for non constant substring
 6871     jmpb(SCAN_TO_SUBSTR);
 6872 
 6873     // SP saved at sp+0
 6874     // String saved at sp+1*wordSize
 6875     // Substr saved at sp+2*wordSize
 6876     // Substr count saved at sp+3*wordSize
 6877 
 6878     // Reload substr for rescan, this code
 6879     // is executed only for large substrings (&gt; 8 chars)
 6880     bind(RELOAD_SUBSTR);
 6881     movptr(str2, Address(rsp, 2*wordSize));
 6882     movl(cnt2, Address(rsp, 3*wordSize));
 6883     if (ae == StrIntrinsicNode::UL) {
 6884       pmovzxbw(vec, Address(str2, 0));
 6885     } else {
 6886       movdqu(vec, Address(str2, 0));
 6887     }
 6888     // We came here after the beginning of the substring was
 6889     // matched but the rest of it was not so we need to search
 6890     // again. Start from the next element after the previous match.
 6891     subptr(str1, result); // Restore counter
 6892     if (ae == StrIntrinsicNode::UU || ae == StrIntrinsicNode::UL) {
 6893       shrl(str1, 1);
 6894     }
 6895     addl(cnt1, str1);
 6896     decrementl(cnt1);   // Shift to next element
 6897     cmpl(cnt1, cnt2);
 6898     jcc(Assembler::negative, RET_NOT_FOUND);  // Left less then substring
 6899 
 6900     addptr(result, (1&lt;&lt;scale1));
 6901   } // non constant
 6902 
 6903   // Scan string for start of substr in 16-byte vectors
 6904   bind(SCAN_TO_SUBSTR);
 6905   assert(cnt1 == rdx &amp;&amp; cnt2 == rax &amp;&amp; tmp == rcx, &quot;pcmpestri&quot;);
 6906   pcmpestri(vec, Address(result, 0), mode);
 6907   jccb(Assembler::below, FOUND_CANDIDATE);   // CF == 1
 6908   subl(cnt1, stride);
 6909   jccb(Assembler::lessEqual, RET_NOT_FOUND); // Scanned full string
 6910   cmpl(cnt1, cnt2);
 6911   jccb(Assembler::negative, RET_NOT_FOUND);  // Left less then substring
 6912   addptr(result, 16);
 6913 
 6914   bind(ADJUST_STR);
 6915   cmpl(cnt1, stride); // Do not read beyond string
 6916   jccb(Assembler::greaterEqual, SCAN_TO_SUBSTR);
 6917   // Back-up string to avoid reading beyond string.
 6918   lea(result, Address(result, cnt1, scale1, -16));
 6919   movl(cnt1, stride);
 6920   jmpb(SCAN_TO_SUBSTR);
 6921 
 6922   // Found a potential substr
 6923   bind(FOUND_CANDIDATE);
 6924   // After pcmpestri tmp(rcx) contains matched element index
 6925 
 6926   // Make sure string is still long enough
 6927   subl(cnt1, tmp);
 6928   cmpl(cnt1, cnt2);
 6929   jccb(Assembler::greaterEqual, FOUND_SUBSTR);
 6930   // Left less then substring.
 6931 
 6932   bind(RET_NOT_FOUND);
 6933   movl(result, -1);
 6934   jmp(CLEANUP);
 6935 
 6936   bind(FOUND_SUBSTR);
 6937   // Compute start addr of substr
 6938   lea(result, Address(result, tmp, scale1));
 6939   if (int_cnt2 &gt; 0) { // Constant substring
 6940     // Repeat search for small substring (&lt; 8 chars)
 6941     // from new point without reloading substring.
 6942     // Have to check that we don&#39;t read beyond string.
 6943     cmpl(tmp, stride-int_cnt2);
 6944     jccb(Assembler::greater, ADJUST_STR);
 6945     // Fall through if matched whole substring.
 6946   } else { // non constant
 6947     assert(int_cnt2 == -1, &quot;should be != 0&quot;);
 6948 
 6949     addl(tmp, cnt2);
 6950     // Found result if we matched whole substring.
 6951     cmpl(tmp, stride);
 6952     jcc(Assembler::lessEqual, RET_FOUND);
 6953 
 6954     // Repeat search for small substring (&lt;= 8 chars)
 6955     // from new point &#39;str1&#39; without reloading substring.
 6956     cmpl(cnt2, stride);
 6957     // Have to check that we don&#39;t read beyond string.
 6958     jccb(Assembler::lessEqual, ADJUST_STR);
 6959 
 6960     Label CHECK_NEXT, CONT_SCAN_SUBSTR, RET_FOUND_LONG;
 6961     // Compare the rest of substring (&gt; 8 chars).
 6962     movptr(str1, result);
 6963 
 6964     cmpl(tmp, cnt2);
 6965     // First 8 chars are already matched.
 6966     jccb(Assembler::equal, CHECK_NEXT);
 6967 
 6968     bind(SCAN_SUBSTR);
 6969     pcmpestri(vec, Address(str1, 0), mode);
 6970     // Need to reload strings pointers if not matched whole vector
 6971     jcc(Assembler::noOverflow, RELOAD_SUBSTR); // OF == 0
 6972 
 6973     bind(CHECK_NEXT);
 6974     subl(cnt2, stride);
 6975     jccb(Assembler::lessEqual, RET_FOUND_LONG); // Found full substring
 6976     addptr(str1, 16);
 6977     if (ae == StrIntrinsicNode::UL) {
 6978       addptr(str2, 8);
 6979     } else {
 6980       addptr(str2, 16);
 6981     }
 6982     subl(cnt1, stride);
 6983     cmpl(cnt2, stride); // Do not read beyond substring
 6984     jccb(Assembler::greaterEqual, CONT_SCAN_SUBSTR);
 6985     // Back-up strings to avoid reading beyond substring.
 6986 
 6987     if (ae == StrIntrinsicNode::UL) {
 6988       lea(str2, Address(str2, cnt2, scale2, -8));
 6989       lea(str1, Address(str1, cnt2, scale1, -16));
 6990     } else {
 6991       lea(str2, Address(str2, cnt2, scale2, -16));
 6992       lea(str1, Address(str1, cnt2, scale1, -16));
 6993     }
 6994     subl(cnt1, cnt2);
 6995     movl(cnt2, stride);
 6996     addl(cnt1, stride);
 6997     bind(CONT_SCAN_SUBSTR);
 6998     if (ae == StrIntrinsicNode::UL) {
 6999       pmovzxbw(vec, Address(str2, 0));
 7000     } else {
 7001       movdqu(vec, Address(str2, 0));
 7002     }
 7003     jmp(SCAN_SUBSTR);
 7004 
 7005     bind(RET_FOUND_LONG);
 7006     movptr(str1, Address(rsp, wordSize));
 7007   } // non constant
 7008 
 7009   bind(RET_FOUND);
 7010   // Compute substr offset
 7011   subptr(result, str1);
 7012   if (ae == StrIntrinsicNode::UU || ae == StrIntrinsicNode::UL) {
 7013     shrl(result, 1); // index
 7014   }
 7015   bind(CLEANUP);
 7016   pop(rsp); // restore SP
 7017 
 7018 } // string_indexof
 7019 
 7020 void MacroAssembler::string_indexof_char(Register str1, Register cnt1, Register ch, Register result,
 7021                                          XMMRegister vec1, XMMRegister vec2, XMMRegister vec3, Register tmp) {
 7022   ShortBranchVerifier sbv(this);
 7023   assert(UseSSE42Intrinsics, &quot;SSE4.2 intrinsics are required&quot;);
 7024 
 7025   int stride = 8;
 7026 
 7027   Label FOUND_CHAR, SCAN_TO_CHAR, SCAN_TO_CHAR_LOOP,
 7028         SCAN_TO_8_CHAR, SCAN_TO_8_CHAR_LOOP, SCAN_TO_16_CHAR_LOOP,
 7029         RET_NOT_FOUND, SCAN_TO_8_CHAR_INIT,
 7030         FOUND_SEQ_CHAR, DONE_LABEL;
 7031 
 7032   movptr(result, str1);
 7033   if (UseAVX &gt;= 2) {
 7034     cmpl(cnt1, stride);
 7035     jcc(Assembler::less, SCAN_TO_CHAR);
 7036     cmpl(cnt1, 2*stride);
 7037     jcc(Assembler::less, SCAN_TO_8_CHAR_INIT);
 7038     movdl(vec1, ch);
 7039     vpbroadcastw(vec1, vec1, Assembler::AVX_256bit);
 7040     vpxor(vec2, vec2);
 7041     movl(tmp, cnt1);
 7042     andl(tmp, 0xFFFFFFF0);  //vector count (in chars)
 7043     andl(cnt1,0x0000000F);  //tail count (in chars)
 7044 
 7045     bind(SCAN_TO_16_CHAR_LOOP);
 7046     vmovdqu(vec3, Address(result, 0));
 7047     vpcmpeqw(vec3, vec3, vec1, 1);
 7048     vptest(vec2, vec3);
 7049     jcc(Assembler::carryClear, FOUND_CHAR);
 7050     addptr(result, 32);
 7051     subl(tmp, 2*stride);
 7052     jcc(Assembler::notZero, SCAN_TO_16_CHAR_LOOP);
 7053     jmp(SCAN_TO_8_CHAR);
 7054     bind(SCAN_TO_8_CHAR_INIT);
 7055     movdl(vec1, ch);
 7056     pshuflw(vec1, vec1, 0x00);
 7057     pshufd(vec1, vec1, 0);
 7058     pxor(vec2, vec2);
 7059   }
 7060   bind(SCAN_TO_8_CHAR);
 7061   cmpl(cnt1, stride);
 7062   jcc(Assembler::less, SCAN_TO_CHAR);
 7063   if (UseAVX &lt; 2) {
 7064     movdl(vec1, ch);
 7065     pshuflw(vec1, vec1, 0x00);
 7066     pshufd(vec1, vec1, 0);
 7067     pxor(vec2, vec2);
 7068   }
 7069   movl(tmp, cnt1);
 7070   andl(tmp, 0xFFFFFFF8);  //vector count (in chars)
 7071   andl(cnt1,0x00000007);  //tail count (in chars)
 7072 
 7073   bind(SCAN_TO_8_CHAR_LOOP);
 7074   movdqu(vec3, Address(result, 0));
 7075   pcmpeqw(vec3, vec1);
 7076   ptest(vec2, vec3);
 7077   jcc(Assembler::carryClear, FOUND_CHAR);
 7078   addptr(result, 16);
 7079   subl(tmp, stride);
 7080   jcc(Assembler::notZero, SCAN_TO_8_CHAR_LOOP);
 7081   bind(SCAN_TO_CHAR);
 7082   testl(cnt1, cnt1);
 7083   jcc(Assembler::zero, RET_NOT_FOUND);
 7084   bind(SCAN_TO_CHAR_LOOP);
 7085   load_unsigned_short(tmp, Address(result, 0));
 7086   cmpl(ch, tmp);
 7087   jccb(Assembler::equal, FOUND_SEQ_CHAR);
 7088   addptr(result, 2);
 7089   subl(cnt1, 1);
 7090   jccb(Assembler::zero, RET_NOT_FOUND);
 7091   jmp(SCAN_TO_CHAR_LOOP);
 7092 
 7093   bind(RET_NOT_FOUND);
 7094   movl(result, -1);
 7095   jmpb(DONE_LABEL);
 7096 
 7097   bind(FOUND_CHAR);
 7098   if (UseAVX &gt;= 2) {
 7099     vpmovmskb(tmp, vec3);
 7100   } else {
 7101     pmovmskb(tmp, vec3);
 7102   }
 7103   bsfl(ch, tmp);
 7104   addl(result, ch);
 7105 
 7106   bind(FOUND_SEQ_CHAR);
 7107   subptr(result, str1);
 7108   shrl(result, 1);
 7109 
 7110   bind(DONE_LABEL);
 7111 } // string_indexof_char
 7112 
 7113 // helper function for string_compare
 7114 void MacroAssembler::load_next_elements(Register elem1, Register elem2, Register str1, Register str2,
 7115                                         Address::ScaleFactor scale, Address::ScaleFactor scale1,
 7116                                         Address::ScaleFactor scale2, Register index, int ae) {
 7117   if (ae == StrIntrinsicNode::LL) {
 7118     load_unsigned_byte(elem1, Address(str1, index, scale, 0));
 7119     load_unsigned_byte(elem2, Address(str2, index, scale, 0));
 7120   } else if (ae == StrIntrinsicNode::UU) {
 7121     load_unsigned_short(elem1, Address(str1, index, scale, 0));
 7122     load_unsigned_short(elem2, Address(str2, index, scale, 0));
 7123   } else {
 7124     load_unsigned_byte(elem1, Address(str1, index, scale1, 0));
 7125     load_unsigned_short(elem2, Address(str2, index, scale2, 0));
 7126   }
 7127 }
 7128 
 7129 // Compare strings, used for char[] and byte[].
 7130 void MacroAssembler::string_compare(Register str1, Register str2,
 7131                                     Register cnt1, Register cnt2, Register result,
 7132                                     XMMRegister vec1, int ae) {
 7133   ShortBranchVerifier sbv(this);
 7134   Label LENGTH_DIFF_LABEL, POP_LABEL, DONE_LABEL, WHILE_HEAD_LABEL;
 7135   Label COMPARE_WIDE_VECTORS_LOOP_FAILED;  // used only _LP64 &amp;&amp; AVX3
 7136   int stride, stride2, adr_stride, adr_stride1, adr_stride2;
 7137   int stride2x2 = 0x40;
 7138   Address::ScaleFactor scale = Address::no_scale;
 7139   Address::ScaleFactor scale1 = Address::no_scale;
 7140   Address::ScaleFactor scale2 = Address::no_scale;
 7141 
 7142   if (ae != StrIntrinsicNode::LL) {
 7143     stride2x2 = 0x20;
 7144   }
 7145 
 7146   if (ae == StrIntrinsicNode::LU || ae == StrIntrinsicNode::UL) {
 7147     shrl(cnt2, 1);
 7148   }
 7149   // Compute the minimum of the string lengths and the
 7150   // difference of the string lengths (stack).
 7151   // Do the conditional move stuff
 7152   movl(result, cnt1);
 7153   subl(cnt1, cnt2);
 7154   push(cnt1);
 7155   cmov32(Assembler::lessEqual, cnt2, result);    // cnt2 = min(cnt1, cnt2)
 7156 
 7157   // Is the minimum length zero?
 7158   testl(cnt2, cnt2);
 7159   jcc(Assembler::zero, LENGTH_DIFF_LABEL);
 7160   if (ae == StrIntrinsicNode::LL) {
 7161     // Load first bytes
 7162     load_unsigned_byte(result, Address(str1, 0));  // result = str1[0]
 7163     load_unsigned_byte(cnt1, Address(str2, 0));    // cnt1   = str2[0]
 7164   } else if (ae == StrIntrinsicNode::UU) {
 7165     // Load first characters
 7166     load_unsigned_short(result, Address(str1, 0));
 7167     load_unsigned_short(cnt1, Address(str2, 0));
 7168   } else {
 7169     load_unsigned_byte(result, Address(str1, 0));
 7170     load_unsigned_short(cnt1, Address(str2, 0));
 7171   }
 7172   subl(result, cnt1);
 7173   jcc(Assembler::notZero,  POP_LABEL);
 7174 
 7175   if (ae == StrIntrinsicNode::UU) {
 7176     // Divide length by 2 to get number of chars
 7177     shrl(cnt2, 1);
 7178   }
 7179   cmpl(cnt2, 1);
 7180   jcc(Assembler::equal, LENGTH_DIFF_LABEL);
 7181 
 7182   // Check if the strings start at the same location and setup scale and stride
 7183   if (ae == StrIntrinsicNode::LL || ae == StrIntrinsicNode::UU) {
 7184     cmpptr(str1, str2);
 7185     jcc(Assembler::equal, LENGTH_DIFF_LABEL);
 7186     if (ae == StrIntrinsicNode::LL) {
 7187       scale = Address::times_1;
 7188       stride = 16;
 7189     } else {
 7190       scale = Address::times_2;
 7191       stride = 8;
 7192     }
 7193   } else {
 7194     scale1 = Address::times_1;
 7195     scale2 = Address::times_2;
 7196     // scale not used
 7197     stride = 8;
 7198   }
 7199 
 7200   if (UseAVX &gt;= 2 &amp;&amp; UseSSE42Intrinsics) {
 7201     Label COMPARE_WIDE_VECTORS, VECTOR_NOT_EQUAL, COMPARE_WIDE_TAIL, COMPARE_SMALL_STR;
 7202     Label COMPARE_WIDE_VECTORS_LOOP, COMPARE_16_CHARS, COMPARE_INDEX_CHAR;
 7203     Label COMPARE_WIDE_VECTORS_LOOP_AVX2;
 7204     Label COMPARE_TAIL_LONG;
 7205     Label COMPARE_WIDE_VECTORS_LOOP_AVX3;  // used only _LP64 &amp;&amp; AVX3
 7206 
 7207     int pcmpmask = 0x19;
 7208     if (ae == StrIntrinsicNode::LL) {
 7209       pcmpmask &amp;= ~0x01;
 7210     }
 7211 
 7212     // Setup to compare 16-chars (32-bytes) vectors,
 7213     // start from first character again because it has aligned address.
 7214     if (ae == StrIntrinsicNode::LL) {
 7215       stride2 = 32;
 7216     } else {
 7217       stride2 = 16;
 7218     }
 7219     if (ae == StrIntrinsicNode::LL || ae == StrIntrinsicNode::UU) {
 7220       adr_stride = stride &lt;&lt; scale;
 7221     } else {
 7222       adr_stride1 = 8;  //stride &lt;&lt; scale1;
 7223       adr_stride2 = 16; //stride &lt;&lt; scale2;
 7224     }
 7225 
 7226     assert(result == rax &amp;&amp; cnt2 == rdx &amp;&amp; cnt1 == rcx, &quot;pcmpestri&quot;);
 7227     // rax and rdx are used by pcmpestri as elements counters
 7228     movl(result, cnt2);
 7229     andl(cnt2, ~(stride2-1));   // cnt2 holds the vector count
 7230     jcc(Assembler::zero, COMPARE_TAIL_LONG);
 7231 
 7232     // fast path : compare first 2 8-char vectors.
 7233     bind(COMPARE_16_CHARS);
 7234     if (ae == StrIntrinsicNode::LL || ae == StrIntrinsicNode::UU) {
 7235       movdqu(vec1, Address(str1, 0));
 7236     } else {
 7237       pmovzxbw(vec1, Address(str1, 0));
 7238     }
 7239     pcmpestri(vec1, Address(str2, 0), pcmpmask);
 7240     jccb(Assembler::below, COMPARE_INDEX_CHAR);
 7241 
 7242     if (ae == StrIntrinsicNode::LL || ae == StrIntrinsicNode::UU) {
 7243       movdqu(vec1, Address(str1, adr_stride));
 7244       pcmpestri(vec1, Address(str2, adr_stride), pcmpmask);
 7245     } else {
 7246       pmovzxbw(vec1, Address(str1, adr_stride1));
 7247       pcmpestri(vec1, Address(str2, adr_stride2), pcmpmask);
 7248     }
 7249     jccb(Assembler::aboveEqual, COMPARE_WIDE_VECTORS);
 7250     addl(cnt1, stride);
 7251 
 7252     // Compare the characters at index in cnt1
 7253     bind(COMPARE_INDEX_CHAR); // cnt1 has the offset of the mismatching character
 7254     load_next_elements(result, cnt2, str1, str2, scale, scale1, scale2, cnt1, ae);
 7255     subl(result, cnt2);
 7256     jmp(POP_LABEL);
 7257 
 7258     // Setup the registers to start vector comparison loop
 7259     bind(COMPARE_WIDE_VECTORS);
 7260     if (ae == StrIntrinsicNode::LL || ae == StrIntrinsicNode::UU) {
 7261       lea(str1, Address(str1, result, scale));
 7262       lea(str2, Address(str2, result, scale));
 7263     } else {
 7264       lea(str1, Address(str1, result, scale1));
 7265       lea(str2, Address(str2, result, scale2));
 7266     }
 7267     subl(result, stride2);
 7268     subl(cnt2, stride2);
 7269     jcc(Assembler::zero, COMPARE_WIDE_TAIL);
 7270     negptr(result);
 7271 
 7272     //  In a loop, compare 16-chars (32-bytes) at once using (vpxor+vptest)
 7273     bind(COMPARE_WIDE_VECTORS_LOOP);
 7274 
 7275 #ifdef _LP64
 7276     if ((AVX3Threshold == 0) &amp;&amp; VM_Version::supports_avx512vlbw()) { // trying 64 bytes fast loop
 7277       cmpl(cnt2, stride2x2);
 7278       jccb(Assembler::below, COMPARE_WIDE_VECTORS_LOOP_AVX2);
 7279       testl(cnt2, stride2x2-1);   // cnt2 holds the vector count
 7280       jccb(Assembler::notZero, COMPARE_WIDE_VECTORS_LOOP_AVX2);   // means we cannot subtract by 0x40
 7281 
 7282       bind(COMPARE_WIDE_VECTORS_LOOP_AVX3); // the hottest loop
 7283       if (ae == StrIntrinsicNode::LL || ae == StrIntrinsicNode::UU) {
 7284         evmovdquq(vec1, Address(str1, result, scale), Assembler::AVX_512bit);
 7285         evpcmpeqb(k7, vec1, Address(str2, result, scale), Assembler::AVX_512bit); // k7 == 11..11, if operands equal, otherwise k7 has some 0
 7286       } else {
 7287         vpmovzxbw(vec1, Address(str1, result, scale1), Assembler::AVX_512bit);
 7288         evpcmpeqb(k7, vec1, Address(str2, result, scale2), Assembler::AVX_512bit); // k7 == 11..11, if operands equal, otherwise k7 has some 0
 7289       }
 7290       kortestql(k7, k7);
 7291       jcc(Assembler::aboveEqual, COMPARE_WIDE_VECTORS_LOOP_FAILED);     // miscompare
 7292       addptr(result, stride2x2);  // update since we already compared at this addr
 7293       subl(cnt2, stride2x2);      // and sub the size too
 7294       jccb(Assembler::notZero, COMPARE_WIDE_VECTORS_LOOP_AVX3);
 7295 
 7296       vpxor(vec1, vec1);
 7297       jmpb(COMPARE_WIDE_TAIL);
 7298     }//if (VM_Version::supports_avx512vlbw())
 7299 #endif // _LP64
 7300 
 7301 
 7302     bind(COMPARE_WIDE_VECTORS_LOOP_AVX2);
 7303     if (ae == StrIntrinsicNode::LL || ae == StrIntrinsicNode::UU) {
 7304       vmovdqu(vec1, Address(str1, result, scale));
 7305       vpxor(vec1, Address(str2, result, scale));
 7306     } else {
 7307       vpmovzxbw(vec1, Address(str1, result, scale1), Assembler::AVX_256bit);
 7308       vpxor(vec1, Address(str2, result, scale2));
 7309     }
 7310     vptest(vec1, vec1);
 7311     jcc(Assembler::notZero, VECTOR_NOT_EQUAL);
 7312     addptr(result, stride2);
 7313     subl(cnt2, stride2);
 7314     jcc(Assembler::notZero, COMPARE_WIDE_VECTORS_LOOP);
 7315     // clean upper bits of YMM registers
 7316     vpxor(vec1, vec1);
 7317 
 7318     // compare wide vectors tail
 7319     bind(COMPARE_WIDE_TAIL);
 7320     testptr(result, result);
 7321     jcc(Assembler::zero, LENGTH_DIFF_LABEL);
 7322 
 7323     movl(result, stride2);
 7324     movl(cnt2, result);
 7325     negptr(result);
 7326     jmp(COMPARE_WIDE_VECTORS_LOOP_AVX2);
 7327 
 7328     // Identifies the mismatching (higher or lower)16-bytes in the 32-byte vectors.
 7329     bind(VECTOR_NOT_EQUAL);
 7330     // clean upper bits of YMM registers
 7331     vpxor(vec1, vec1);
 7332     if (ae == StrIntrinsicNode::LL || ae == StrIntrinsicNode::UU) {
 7333       lea(str1, Address(str1, result, scale));
 7334       lea(str2, Address(str2, result, scale));
 7335     } else {
 7336       lea(str1, Address(str1, result, scale1));
 7337       lea(str2, Address(str2, result, scale2));
 7338     }
 7339     jmp(COMPARE_16_CHARS);
 7340 
 7341     // Compare tail chars, length between 1 to 15 chars
 7342     bind(COMPARE_TAIL_LONG);
 7343     movl(cnt2, result);
 7344     cmpl(cnt2, stride);
 7345     jcc(Assembler::less, COMPARE_SMALL_STR);
 7346 
 7347     if (ae == StrIntrinsicNode::LL || ae == StrIntrinsicNode::UU) {
 7348       movdqu(vec1, Address(str1, 0));
 7349     } else {
 7350       pmovzxbw(vec1, Address(str1, 0));
 7351     }
 7352     pcmpestri(vec1, Address(str2, 0), pcmpmask);
 7353     jcc(Assembler::below, COMPARE_INDEX_CHAR);
 7354     subptr(cnt2, stride);
 7355     jcc(Assembler::zero, LENGTH_DIFF_LABEL);
 7356     if (ae == StrIntrinsicNode::LL || ae == StrIntrinsicNode::UU) {
 7357       lea(str1, Address(str1, result, scale));
 7358       lea(str2, Address(str2, result, scale));
 7359     } else {
 7360       lea(str1, Address(str1, result, scale1));
 7361       lea(str2, Address(str2, result, scale2));
 7362     }
 7363     negptr(cnt2);
 7364     jmpb(WHILE_HEAD_LABEL);
 7365 
 7366     bind(COMPARE_SMALL_STR);
 7367   } else if (UseSSE42Intrinsics) {
 7368     Label COMPARE_WIDE_VECTORS, VECTOR_NOT_EQUAL, COMPARE_TAIL;
 7369     int pcmpmask = 0x19;
 7370     // Setup to compare 8-char (16-byte) vectors,
 7371     // start from first character again because it has aligned address.
 7372     movl(result, cnt2);
 7373     andl(cnt2, ~(stride - 1));   // cnt2 holds the vector count
 7374     if (ae == StrIntrinsicNode::LL) {
 7375       pcmpmask &amp;= ~0x01;
 7376     }
 7377     jcc(Assembler::zero, COMPARE_TAIL);
 7378     if (ae == StrIntrinsicNode::LL || ae == StrIntrinsicNode::UU) {
 7379       lea(str1, Address(str1, result, scale));
 7380       lea(str2, Address(str2, result, scale));
 7381     } else {
 7382       lea(str1, Address(str1, result, scale1));
 7383       lea(str2, Address(str2, result, scale2));
 7384     }
 7385     negptr(result);
 7386 
 7387     // pcmpestri
 7388     //   inputs:
 7389     //     vec1- substring
 7390     //     rax - negative string length (elements count)
 7391     //     mem - scanned string
 7392     //     rdx - string length (elements count)
 7393     //     pcmpmask - cmp mode: 11000 (string compare with negated result)
 7394     //               + 00 (unsigned bytes) or  + 01 (unsigned shorts)
 7395     //   outputs:
 7396     //     rcx - first mismatched element index
 7397     assert(result == rax &amp;&amp; cnt2 == rdx &amp;&amp; cnt1 == rcx, &quot;pcmpestri&quot;);
 7398 
 7399     bind(COMPARE_WIDE_VECTORS);
 7400     if (ae == StrIntrinsicNode::LL || ae == StrIntrinsicNode::UU) {
 7401       movdqu(vec1, Address(str1, result, scale));
 7402       pcmpestri(vec1, Address(str2, result, scale), pcmpmask);
 7403     } else {
 7404       pmovzxbw(vec1, Address(str1, result, scale1));
 7405       pcmpestri(vec1, Address(str2, result, scale2), pcmpmask);
 7406     }
 7407     // After pcmpestri cnt1(rcx) contains mismatched element index
 7408 
 7409     jccb(Assembler::below, VECTOR_NOT_EQUAL);  // CF==1
 7410     addptr(result, stride);
 7411     subptr(cnt2, stride);
 7412     jccb(Assembler::notZero, COMPARE_WIDE_VECTORS);
 7413 
 7414     // compare wide vectors tail
 7415     testptr(result, result);
 7416     jcc(Assembler::zero, LENGTH_DIFF_LABEL);
 7417 
 7418     movl(cnt2, stride);
 7419     movl(result, stride);
 7420     negptr(result);
 7421     if (ae == StrIntrinsicNode::LL || ae == StrIntrinsicNode::UU) {
 7422       movdqu(vec1, Address(str1, result, scale));
 7423       pcmpestri(vec1, Address(str2, result, scale), pcmpmask);
 7424     } else {
 7425       pmovzxbw(vec1, Address(str1, result, scale1));
 7426       pcmpestri(vec1, Address(str2, result, scale2), pcmpmask);
 7427     }
 7428     jccb(Assembler::aboveEqual, LENGTH_DIFF_LABEL);
 7429 
 7430     // Mismatched characters in the vectors
 7431     bind(VECTOR_NOT_EQUAL);
 7432     addptr(cnt1, result);
 7433     load_next_elements(result, cnt2, str1, str2, scale, scale1, scale2, cnt1, ae);
 7434     subl(result, cnt2);
 7435     jmpb(POP_LABEL);
 7436 
 7437     bind(COMPARE_TAIL); // limit is zero
 7438     movl(cnt2, result);
 7439     // Fallthru to tail compare
 7440   }
 7441   // Shift str2 and str1 to the end of the arrays, negate min
 7442   if (ae == StrIntrinsicNode::LL || ae == StrIntrinsicNode::UU) {
 7443     lea(str1, Address(str1, cnt2, scale));
 7444     lea(str2, Address(str2, cnt2, scale));
 7445   } else {
 7446     lea(str1, Address(str1, cnt2, scale1));
 7447     lea(str2, Address(str2, cnt2, scale2));
 7448   }
 7449   decrementl(cnt2);  // first character was compared already
 7450   negptr(cnt2);
 7451 
 7452   // Compare the rest of the elements
 7453   bind(WHILE_HEAD_LABEL);
 7454   load_next_elements(result, cnt1, str1, str2, scale, scale1, scale2, cnt2, ae);
 7455   subl(result, cnt1);
 7456   jccb(Assembler::notZero, POP_LABEL);
 7457   increment(cnt2);
 7458   jccb(Assembler::notZero, WHILE_HEAD_LABEL);
 7459 
 7460   // Strings are equal up to min length.  Return the length difference.
 7461   bind(LENGTH_DIFF_LABEL);
 7462   pop(result);
 7463   if (ae == StrIntrinsicNode::UU) {
 7464     // Divide diff by 2 to get number of chars
 7465     sarl(result, 1);
 7466   }
 7467   jmpb(DONE_LABEL);
 7468 
 7469 #ifdef _LP64
 7470   if (VM_Version::supports_avx512vlbw()) {
 7471 
 7472     bind(COMPARE_WIDE_VECTORS_LOOP_FAILED);
 7473 
 7474     kmovql(cnt1, k7);
 7475     notq(cnt1);
 7476     bsfq(cnt2, cnt1);
 7477     if (ae != StrIntrinsicNode::LL) {
 7478       // Divide diff by 2 to get number of chars
 7479       sarl(cnt2, 1);
 7480     }
 7481     addq(result, cnt2);
 7482     if (ae == StrIntrinsicNode::LL) {
 7483       load_unsigned_byte(cnt1, Address(str2, result));
 7484       load_unsigned_byte(result, Address(str1, result));
 7485     } else if (ae == StrIntrinsicNode::UU) {
 7486       load_unsigned_short(cnt1, Address(str2, result, scale));
 7487       load_unsigned_short(result, Address(str1, result, scale));
 7488     } else {
 7489       load_unsigned_short(cnt1, Address(str2, result, scale2));
 7490       load_unsigned_byte(result, Address(str1, result, scale1));
 7491     }
 7492     subl(result, cnt1);
 7493     jmpb(POP_LABEL);
 7494   }//if (VM_Version::supports_avx512vlbw())
 7495 #endif // _LP64
 7496 
 7497   // Discard the stored length difference
 7498   bind(POP_LABEL);
 7499   pop(cnt1);
 7500 
 7501   // That&#39;s it
 7502   bind(DONE_LABEL);
 7503   if(ae == StrIntrinsicNode::UL) {
 7504     negl(result);
 7505   }
 7506 
 7507 }
 7508 
 7509 // Search for Non-ASCII character (Negative byte value) in a byte array,
 7510 // return true if it has any and false otherwise.
 7511 //   ..\jdk\src\java.base\share\classes\java\lang\StringCoding.java
 7512 //   @HotSpotIntrinsicCandidate
 7513 //   private static boolean hasNegatives(byte[] ba, int off, int len) {
 7514 //     for (int i = off; i &lt; off + len; i++) {
 7515 //       if (ba[i] &lt; 0) {
 7516 //         return true;
 7517 //       }
 7518 //     }
 7519 //     return false;
 7520 //   }
 7521 void MacroAssembler::has_negatives(Register ary1, Register len,
 7522   Register result, Register tmp1,
 7523   XMMRegister vec1, XMMRegister vec2) {
 7524   // rsi: byte array
 7525   // rcx: len
 7526   // rax: result
 7527   ShortBranchVerifier sbv(this);
 7528   assert_different_registers(ary1, len, result, tmp1);
 7529   assert_different_registers(vec1, vec2);
 7530   Label TRUE_LABEL, FALSE_LABEL, DONE, COMPARE_CHAR, COMPARE_VECTORS, COMPARE_BYTE;
 7531 
 7532   // len == 0
 7533   testl(len, len);
 7534   jcc(Assembler::zero, FALSE_LABEL);
 7535 
 7536   if ((AVX3Threshold == 0) &amp;&amp; (UseAVX &gt; 2) &amp;&amp; // AVX512
 7537     VM_Version::supports_avx512vlbw() &amp;&amp;
 7538     VM_Version::supports_bmi2()) {
 7539 
 7540     Label test_64_loop, test_tail;
 7541     Register tmp3_aliased = len;
 7542 
 7543     movl(tmp1, len);
 7544     vpxor(vec2, vec2, vec2, Assembler::AVX_512bit);
 7545 
 7546     andl(tmp1, 64 - 1);   // tail count (in chars) 0x3F
 7547     andl(len, ~(64 - 1));    // vector count (in chars)
 7548     jccb(Assembler::zero, test_tail);
 7549 
 7550     lea(ary1, Address(ary1, len, Address::times_1));
 7551     negptr(len);
 7552 
 7553     bind(test_64_loop);
 7554     // Check whether our 64 elements of size byte contain negatives
 7555     evpcmpgtb(k2, vec2, Address(ary1, len, Address::times_1), Assembler::AVX_512bit);
 7556     kortestql(k2, k2);
 7557     jcc(Assembler::notZero, TRUE_LABEL);
 7558 
 7559     addptr(len, 64);
 7560     jccb(Assembler::notZero, test_64_loop);
 7561 
 7562 
 7563     bind(test_tail);
 7564     // bail out when there is nothing to be done
 7565     testl(tmp1, -1);
 7566     jcc(Assembler::zero, FALSE_LABEL);
 7567 
 7568     // ~(~0 &lt;&lt; len) applied up to two times (for 32-bit scenario)
 7569 #ifdef _LP64
 7570     mov64(tmp3_aliased, 0xFFFFFFFFFFFFFFFF);
 7571     shlxq(tmp3_aliased, tmp3_aliased, tmp1);
 7572     notq(tmp3_aliased);
 7573     kmovql(k3, tmp3_aliased);
 7574 #else
 7575     Label k_init;
 7576     jmp(k_init);
 7577 
 7578     // We could not read 64-bits from a general purpose register thus we move
 7579     // data required to compose 64 1&#39;s to the instruction stream
 7580     // We emit 64 byte wide series of elements from 0..63 which later on would
 7581     // be used as a compare targets with tail count contained in tmp1 register.
 7582     // Result would be a k register having tmp1 consecutive number or 1
 7583     // counting from least significant bit.
 7584     address tmp = pc();
 7585     emit_int64(0x0706050403020100);
 7586     emit_int64(0x0F0E0D0C0B0A0908);
 7587     emit_int64(0x1716151413121110);
 7588     emit_int64(0x1F1E1D1C1B1A1918);
 7589     emit_int64(0x2726252423222120);
 7590     emit_int64(0x2F2E2D2C2B2A2928);
 7591     emit_int64(0x3736353433323130);
 7592     emit_int64(0x3F3E3D3C3B3A3938);
 7593 
 7594     bind(k_init);
 7595     lea(len, InternalAddress(tmp));
 7596     // create mask to test for negative byte inside a vector
 7597     evpbroadcastb(vec1, tmp1, Assembler::AVX_512bit);
 7598     evpcmpgtb(k3, vec1, Address(len, 0), Assembler::AVX_512bit);
 7599 
 7600 #endif
 7601     evpcmpgtb(k2, k3, vec2, Address(ary1, 0), Assembler::AVX_512bit);
 7602     ktestq(k2, k3);
 7603     jcc(Assembler::notZero, TRUE_LABEL);
 7604 
 7605     jmp(FALSE_LABEL);
 7606   } else {
 7607     movl(result, len); // copy
 7608 
 7609     if (UseAVX &gt;= 2 &amp;&amp; UseSSE &gt;= 2) {
 7610       // With AVX2, use 32-byte vector compare
 7611       Label COMPARE_WIDE_VECTORS, COMPARE_TAIL;
 7612 
 7613       // Compare 32-byte vectors
 7614       andl(result, 0x0000001f);  //   tail count (in bytes)
 7615       andl(len, 0xffffffe0);   // vector count (in bytes)
 7616       jccb(Assembler::zero, COMPARE_TAIL);
 7617 
 7618       lea(ary1, Address(ary1, len, Address::times_1));
 7619       negptr(len);
 7620 
 7621       movl(tmp1, 0x80808080);   // create mask to test for Unicode chars in vector
 7622       movdl(vec2, tmp1);
 7623       vpbroadcastd(vec2, vec2, Assembler::AVX_256bit);
 7624 
 7625       bind(COMPARE_WIDE_VECTORS);
 7626       vmovdqu(vec1, Address(ary1, len, Address::times_1));
 7627       vptest(vec1, vec2);
 7628       jccb(Assembler::notZero, TRUE_LABEL);
 7629       addptr(len, 32);
 7630       jcc(Assembler::notZero, COMPARE_WIDE_VECTORS);
 7631 
 7632       testl(result, result);
 7633       jccb(Assembler::zero, FALSE_LABEL);
 7634 
 7635       vmovdqu(vec1, Address(ary1, result, Address::times_1, -32));
 7636       vptest(vec1, vec2);
 7637       jccb(Assembler::notZero, TRUE_LABEL);
 7638       jmpb(FALSE_LABEL);
 7639 
 7640       bind(COMPARE_TAIL); // len is zero
 7641       movl(len, result);
 7642       // Fallthru to tail compare
 7643     } else if (UseSSE42Intrinsics) {
 7644       // With SSE4.2, use double quad vector compare
 7645       Label COMPARE_WIDE_VECTORS, COMPARE_TAIL;
 7646 
 7647       // Compare 16-byte vectors
 7648       andl(result, 0x0000000f);  //   tail count (in bytes)
 7649       andl(len, 0xfffffff0);   // vector count (in bytes)
 7650       jcc(Assembler::zero, COMPARE_TAIL);
 7651 
 7652       lea(ary1, Address(ary1, len, Address::times_1));
 7653       negptr(len);
 7654 
 7655       movl(tmp1, 0x80808080);
 7656       movdl(vec2, tmp1);
 7657       pshufd(vec2, vec2, 0);
 7658 
 7659       bind(COMPARE_WIDE_VECTORS);
 7660       movdqu(vec1, Address(ary1, len, Address::times_1));
 7661       ptest(vec1, vec2);
 7662       jcc(Assembler::notZero, TRUE_LABEL);
 7663       addptr(len, 16);
 7664       jcc(Assembler::notZero, COMPARE_WIDE_VECTORS);
 7665 
 7666       testl(result, result);
 7667       jcc(Assembler::zero, FALSE_LABEL);
 7668 
 7669       movdqu(vec1, Address(ary1, result, Address::times_1, -16));
 7670       ptest(vec1, vec2);
 7671       jccb(Assembler::notZero, TRUE_LABEL);
 7672       jmpb(FALSE_LABEL);
 7673 
 7674       bind(COMPARE_TAIL); // len is zero
 7675       movl(len, result);
 7676       // Fallthru to tail compare
 7677     }
 7678   }
 7679   // Compare 4-byte vectors
 7680   andl(len, 0xfffffffc); // vector count (in bytes)
 7681   jccb(Assembler::zero, COMPARE_CHAR);
 7682 
 7683   lea(ary1, Address(ary1, len, Address::times_1));
 7684   negptr(len);
 7685 
 7686   bind(COMPARE_VECTORS);
 7687   movl(tmp1, Address(ary1, len, Address::times_1));
 7688   andl(tmp1, 0x80808080);
 7689   jccb(Assembler::notZero, TRUE_LABEL);
 7690   addptr(len, 4);
 7691   jcc(Assembler::notZero, COMPARE_VECTORS);
 7692 
 7693   // Compare trailing char (final 2 bytes), if any
 7694   bind(COMPARE_CHAR);
 7695   testl(result, 0x2);   // tail  char
 7696   jccb(Assembler::zero, COMPARE_BYTE);
 7697   load_unsigned_short(tmp1, Address(ary1, 0));
 7698   andl(tmp1, 0x00008080);
 7699   jccb(Assembler::notZero, TRUE_LABEL);
 7700   subptr(result, 2);
 7701   lea(ary1, Address(ary1, 2));
 7702 
 7703   bind(COMPARE_BYTE);
 7704   testl(result, 0x1);   // tail  byte
 7705   jccb(Assembler::zero, FALSE_LABEL);
 7706   load_unsigned_byte(tmp1, Address(ary1, 0));
 7707   andl(tmp1, 0x00000080);
 7708   jccb(Assembler::notEqual, TRUE_LABEL);
 7709   jmpb(FALSE_LABEL);
 7710 
 7711   bind(TRUE_LABEL);
 7712   movl(result, 1);   // return true
 7713   jmpb(DONE);
 7714 
 7715   bind(FALSE_LABEL);
 7716   xorl(result, result); // return false
 7717 
 7718   // That&#39;s it
 7719   bind(DONE);
 7720   if (UseAVX &gt;= 2 &amp;&amp; UseSSE &gt;= 2) {
 7721     // clean upper bits of YMM registers
 7722     vpxor(vec1, vec1);
 7723     vpxor(vec2, vec2);
 7724   }
 7725 }
 7726 // Compare char[] or byte[] arrays aligned to 4 bytes or substrings.
 7727 void MacroAssembler::arrays_equals(bool is_array_equ, Register ary1, Register ary2,
 7728                                    Register limit, Register result, Register chr,
 7729                                    XMMRegister vec1, XMMRegister vec2, bool is_char) {
 7730   ShortBranchVerifier sbv(this);
 7731   Label TRUE_LABEL, FALSE_LABEL, DONE, COMPARE_VECTORS, COMPARE_CHAR, COMPARE_BYTE;
 7732 
 7733   int length_offset  = arrayOopDesc::length_offset_in_bytes();
 7734   int base_offset    = arrayOopDesc::base_offset_in_bytes(is_char ? T_CHAR : T_BYTE);
 7735 
 7736   if (is_array_equ) {
 7737     // Check the input args
 7738     cmpoop(ary1, ary2);
 7739     jcc(Assembler::equal, TRUE_LABEL);
 7740 
 7741     // Need additional checks for arrays_equals.
 7742     testptr(ary1, ary1);
 7743     jcc(Assembler::zero, FALSE_LABEL);
 7744     testptr(ary2, ary2);
 7745     jcc(Assembler::zero, FALSE_LABEL);
 7746 
 7747     // Check the lengths
 7748     movl(limit, Address(ary1, length_offset));
 7749     cmpl(limit, Address(ary2, length_offset));
 7750     jcc(Assembler::notEqual, FALSE_LABEL);
 7751   }
 7752 
 7753   // count == 0
 7754   testl(limit, limit);
 7755   jcc(Assembler::zero, TRUE_LABEL);
 7756 
 7757   if (is_array_equ) {
 7758     // Load array address
 7759     lea(ary1, Address(ary1, base_offset));
 7760     lea(ary2, Address(ary2, base_offset));
 7761   }
 7762 
 7763   if (is_array_equ &amp;&amp; is_char) {
 7764     // arrays_equals when used for char[].
 7765     shll(limit, 1);      // byte count != 0
 7766   }
 7767   movl(result, limit); // copy
 7768 
 7769   if (UseAVX &gt;= 2) {
 7770     // With AVX2, use 32-byte vector compare
 7771     Label COMPARE_WIDE_VECTORS, COMPARE_TAIL;
 7772 
 7773     // Compare 32-byte vectors
 7774     andl(result, 0x0000001f);  //   tail count (in bytes)
 7775     andl(limit, 0xffffffe0);   // vector count (in bytes)
 7776     jcc(Assembler::zero, COMPARE_TAIL);
 7777 
 7778     lea(ary1, Address(ary1, limit, Address::times_1));
 7779     lea(ary2, Address(ary2, limit, Address::times_1));
 7780     negptr(limit);
 7781 
 7782 #ifdef _LP64
 7783     if ((AVX3Threshold == 0) &amp;&amp; VM_Version::supports_avx512vlbw()) { // trying 64 bytes fast loop
 7784       Label COMPARE_WIDE_VECTORS_LOOP_AVX2, COMPARE_WIDE_VECTORS_LOOP_AVX3;
 7785 
 7786       cmpl(limit, -64);
 7787       jcc(Assembler::greater, COMPARE_WIDE_VECTORS_LOOP_AVX2);
 7788 
 7789       bind(COMPARE_WIDE_VECTORS_LOOP_AVX3); // the hottest loop
 7790 
 7791       evmovdquq(vec1, Address(ary1, limit, Address::times_1), Assembler::AVX_512bit);
 7792       evpcmpeqb(k7, vec1, Address(ary2, limit, Address::times_1), Assembler::AVX_512bit);
 7793       kortestql(k7, k7);
 7794       jcc(Assembler::aboveEqual, FALSE_LABEL);     // miscompare
 7795       addptr(limit, 64);  // update since we already compared at this addr
 7796       cmpl(limit, -64);
 7797       jccb(Assembler::lessEqual, COMPARE_WIDE_VECTORS_LOOP_AVX3);
 7798 
 7799       // At this point we may still need to compare -limit+result bytes.
 7800       // We could execute the next two instruction and just continue via non-wide path:
 7801       //  cmpl(limit, 0);
 7802       //  jcc(Assembler::equal, COMPARE_TAIL);  // true
 7803       // But since we stopped at the points ary{1,2}+limit which are
 7804       // not farther than 64 bytes from the ends of arrays ary{1,2}+result
 7805       // (|limit| &lt;= 32 and result &lt; 32),
 7806       // we may just compare the last 64 bytes.
 7807       //
 7808       addptr(result, -64);   // it is safe, bc we just came from this area
 7809       evmovdquq(vec1, Address(ary1, result, Address::times_1), Assembler::AVX_512bit);
 7810       evpcmpeqb(k7, vec1, Address(ary2, result, Address::times_1), Assembler::AVX_512bit);
 7811       kortestql(k7, k7);
 7812       jcc(Assembler::aboveEqual, FALSE_LABEL);     // miscompare
 7813 
 7814       jmp(TRUE_LABEL);
 7815 
 7816       bind(COMPARE_WIDE_VECTORS_LOOP_AVX2);
 7817 
 7818     }//if (VM_Version::supports_avx512vlbw())
 7819 #endif //_LP64
 7820     bind(COMPARE_WIDE_VECTORS);
 7821     vmovdqu(vec1, Address(ary1, limit, Address::times_1));
 7822     vmovdqu(vec2, Address(ary2, limit, Address::times_1));
 7823     vpxor(vec1, vec2);
 7824 
 7825     vptest(vec1, vec1);
 7826     jcc(Assembler::notZero, FALSE_LABEL);
 7827     addptr(limit, 32);
 7828     jcc(Assembler::notZero, COMPARE_WIDE_VECTORS);
 7829 
 7830     testl(result, result);
 7831     jcc(Assembler::zero, TRUE_LABEL);
 7832 
 7833     vmovdqu(vec1, Address(ary1, result, Address::times_1, -32));
 7834     vmovdqu(vec2, Address(ary2, result, Address::times_1, -32));
 7835     vpxor(vec1, vec2);
 7836 
 7837     vptest(vec1, vec1);
 7838     jccb(Assembler::notZero, FALSE_LABEL);
 7839     jmpb(TRUE_LABEL);
 7840 
 7841     bind(COMPARE_TAIL); // limit is zero
 7842     movl(limit, result);
 7843     // Fallthru to tail compare
 7844   } else if (UseSSE42Intrinsics) {
 7845     // With SSE4.2, use double quad vector compare
 7846     Label COMPARE_WIDE_VECTORS, COMPARE_TAIL;
 7847 
 7848     // Compare 16-byte vectors
 7849     andl(result, 0x0000000f);  //   tail count (in bytes)
 7850     andl(limit, 0xfffffff0);   // vector count (in bytes)
 7851     jcc(Assembler::zero, COMPARE_TAIL);
 7852 
 7853     lea(ary1, Address(ary1, limit, Address::times_1));
 7854     lea(ary2, Address(ary2, limit, Address::times_1));
 7855     negptr(limit);
 7856 
 7857     bind(COMPARE_WIDE_VECTORS);
 7858     movdqu(vec1, Address(ary1, limit, Address::times_1));
 7859     movdqu(vec2, Address(ary2, limit, Address::times_1));
 7860     pxor(vec1, vec2);
 7861 
 7862     ptest(vec1, vec1);
 7863     jcc(Assembler::notZero, FALSE_LABEL);
 7864     addptr(limit, 16);
 7865     jcc(Assembler::notZero, COMPARE_WIDE_VECTORS);
 7866 
 7867     testl(result, result);
 7868     jcc(Assembler::zero, TRUE_LABEL);
 7869 
 7870     movdqu(vec1, Address(ary1, result, Address::times_1, -16));
 7871     movdqu(vec2, Address(ary2, result, Address::times_1, -16));
 7872     pxor(vec1, vec2);
 7873 
 7874     ptest(vec1, vec1);
 7875     jccb(Assembler::notZero, FALSE_LABEL);
 7876     jmpb(TRUE_LABEL);
 7877 
 7878     bind(COMPARE_TAIL); // limit is zero
 7879     movl(limit, result);
 7880     // Fallthru to tail compare
 7881   }
 7882 
 7883   // Compare 4-byte vectors
 7884   andl(limit, 0xfffffffc); // vector count (in bytes)
 7885   jccb(Assembler::zero, COMPARE_CHAR);
 7886 
 7887   lea(ary1, Address(ary1, limit, Address::times_1));
 7888   lea(ary2, Address(ary2, limit, Address::times_1));
 7889   negptr(limit);
 7890 
 7891   bind(COMPARE_VECTORS);
 7892   movl(chr, Address(ary1, limit, Address::times_1));
 7893   cmpl(chr, Address(ary2, limit, Address::times_1));
 7894   jccb(Assembler::notEqual, FALSE_LABEL);
 7895   addptr(limit, 4);
 7896   jcc(Assembler::notZero, COMPARE_VECTORS);
 7897 
 7898   // Compare trailing char (final 2 bytes), if any
 7899   bind(COMPARE_CHAR);
 7900   testl(result, 0x2);   // tail  char
 7901   jccb(Assembler::zero, COMPARE_BYTE);
 7902   load_unsigned_short(chr, Address(ary1, 0));
 7903   load_unsigned_short(limit, Address(ary2, 0));
 7904   cmpl(chr, limit);
 7905   jccb(Assembler::notEqual, FALSE_LABEL);
 7906 
 7907   if (is_array_equ &amp;&amp; is_char) {
 7908     bind(COMPARE_BYTE);
 7909   } else {
 7910     lea(ary1, Address(ary1, 2));
 7911     lea(ary2, Address(ary2, 2));
 7912 
 7913     bind(COMPARE_BYTE);
 7914     testl(result, 0x1);   // tail  byte
 7915     jccb(Assembler::zero, TRUE_LABEL);
 7916     load_unsigned_byte(chr, Address(ary1, 0));
 7917     load_unsigned_byte(limit, Address(ary2, 0));
 7918     cmpl(chr, limit);
 7919     jccb(Assembler::notEqual, FALSE_LABEL);
 7920   }
 7921   bind(TRUE_LABEL);
 7922   movl(result, 1);   // return true
 7923   jmpb(DONE);
 7924 
 7925   bind(FALSE_LABEL);
 7926   xorl(result, result); // return false
 7927 
 7928   // That&#39;s it
 7929   bind(DONE);
 7930   if (UseAVX &gt;= 2) {
 7931     // clean upper bits of YMM registers
 7932     vpxor(vec1, vec1);
 7933     vpxor(vec2, vec2);
 7934   }
 7935 }
 7936 
 7937 #endif
 7938 
 7939 void MacroAssembler::generate_fill(BasicType t, bool aligned,
 7940                                    Register to, Register value, Register count,
 7941                                    Register rtmp, XMMRegister xtmp) {
 7942   ShortBranchVerifier sbv(this);
 7943   assert_different_registers(to, value, count, rtmp);
 7944   Label L_exit;
 7945   Label L_fill_2_bytes, L_fill_4_bytes;
 7946 
 7947   int shift = -1;
 7948   switch (t) {
 7949     case T_BYTE:
 7950       shift = 2;
 7951       break;
 7952     case T_SHORT:
 7953       shift = 1;
 7954       break;
 7955     case T_INT:
 7956       shift = 0;
 7957       break;
 7958     default: ShouldNotReachHere();
 7959   }
 7960 
 7961   if (t == T_BYTE) {
 7962     andl(value, 0xff);
 7963     movl(rtmp, value);
 7964     shll(rtmp, 8);
 7965     orl(value, rtmp);
 7966   }
 7967   if (t == T_SHORT) {
 7968     andl(value, 0xffff);
 7969   }
 7970   if (t == T_BYTE || t == T_SHORT) {
 7971     movl(rtmp, value);
 7972     shll(rtmp, 16);
 7973     orl(value, rtmp);
 7974   }
 7975 
 7976   cmpl(count, 2&lt;&lt;shift); // Short arrays (&lt; 8 bytes) fill by element
 7977   jcc(Assembler::below, L_fill_4_bytes); // use unsigned cmp
 7978   if (!UseUnalignedLoadStores &amp;&amp; !aligned &amp;&amp; (t == T_BYTE || t == T_SHORT)) {
 7979     Label L_skip_align2;
 7980     // align source address at 4 bytes address boundary
 7981     if (t == T_BYTE) {
 7982       Label L_skip_align1;
 7983       // One byte misalignment happens only for byte arrays
 7984       testptr(to, 1);
 7985       jccb(Assembler::zero, L_skip_align1);
 7986       movb(Address(to, 0), value);
 7987       increment(to);
 7988       decrement(count);
 7989       BIND(L_skip_align1);
 7990     }
 7991     // Two bytes misalignment happens only for byte and short (char) arrays
 7992     testptr(to, 2);
 7993     jccb(Assembler::zero, L_skip_align2);
 7994     movw(Address(to, 0), value);
 7995     addptr(to, 2);
 7996     subl(count, 1&lt;&lt;(shift-1));
 7997     BIND(L_skip_align2);
 7998   }
 7999   if (UseSSE &lt; 2) {
 8000     Label L_fill_32_bytes_loop, L_check_fill_8_bytes, L_fill_8_bytes_loop, L_fill_8_bytes;
 8001     // Fill 32-byte chunks
 8002     subl(count, 8 &lt;&lt; shift);
 8003     jcc(Assembler::less, L_check_fill_8_bytes);
 8004     align(16);
 8005 
 8006     BIND(L_fill_32_bytes_loop);
 8007 
 8008     for (int i = 0; i &lt; 32; i += 4) {
 8009       movl(Address(to, i), value);
 8010     }
 8011 
 8012     addptr(to, 32);
 8013     subl(count, 8 &lt;&lt; shift);
 8014     jcc(Assembler::greaterEqual, L_fill_32_bytes_loop);
 8015     BIND(L_check_fill_8_bytes);
 8016     addl(count, 8 &lt;&lt; shift);
 8017     jccb(Assembler::zero, L_exit);
 8018     jmpb(L_fill_8_bytes);
 8019 
 8020     //
 8021     // length is too short, just fill qwords
 8022     //
 8023     BIND(L_fill_8_bytes_loop);
 8024     movl(Address(to, 0), value);
 8025     movl(Address(to, 4), value);
 8026     addptr(to, 8);
 8027     BIND(L_fill_8_bytes);
 8028     subl(count, 1 &lt;&lt; (shift + 1));
 8029     jcc(Assembler::greaterEqual, L_fill_8_bytes_loop);
 8030     // fall through to fill 4 bytes
 8031   } else {
 8032     Label L_fill_32_bytes;
 8033     if (!UseUnalignedLoadStores) {
 8034       // align to 8 bytes, we know we are 4 byte aligned to start
 8035       testptr(to, 4);
 8036       jccb(Assembler::zero, L_fill_32_bytes);
 8037       movl(Address(to, 0), value);
 8038       addptr(to, 4);
 8039       subl(count, 1&lt;&lt;shift);
 8040     }
 8041     BIND(L_fill_32_bytes);
 8042     {
 8043       assert( UseSSE &gt;= 2, &quot;supported cpu only&quot; );
 8044       Label L_fill_32_bytes_loop, L_check_fill_8_bytes, L_fill_8_bytes_loop, L_fill_8_bytes;
 8045       movdl(xtmp, value);
 8046       if (UseAVX &gt;= 2 &amp;&amp; UseUnalignedLoadStores) {
 8047         Label L_check_fill_32_bytes;
 8048         if (UseAVX &gt; 2) {
 8049           // Fill 64-byte chunks
 8050           Label L_fill_64_bytes_loop_avx3, L_check_fill_64_bytes_avx2;
 8051 
 8052           // If number of bytes to fill &lt; AVX3Threshold, perform fill using AVX2
 8053           cmpl(count, AVX3Threshold);
 8054           jccb(Assembler::below, L_check_fill_64_bytes_avx2);
 8055 
 8056           vpbroadcastd(xtmp, xtmp, Assembler::AVX_512bit);
 8057 
 8058           subl(count, 16 &lt;&lt; shift);
 8059           jccb(Assembler::less, L_check_fill_32_bytes);
 8060           align(16);
 8061 
 8062           BIND(L_fill_64_bytes_loop_avx3);
 8063           evmovdqul(Address(to, 0), xtmp, Assembler::AVX_512bit);
 8064           addptr(to, 64);
 8065           subl(count, 16 &lt;&lt; shift);
 8066           jcc(Assembler::greaterEqual, L_fill_64_bytes_loop_avx3);
 8067           jmpb(L_check_fill_32_bytes);
 8068 
 8069           BIND(L_check_fill_64_bytes_avx2);
 8070         }
 8071         // Fill 64-byte chunks
 8072         Label L_fill_64_bytes_loop;
 8073         vpbroadcastd(xtmp, xtmp, Assembler::AVX_256bit);
 8074 
 8075         subl(count, 16 &lt;&lt; shift);
 8076         jcc(Assembler::less, L_check_fill_32_bytes);
 8077         align(16);
 8078 
 8079         BIND(L_fill_64_bytes_loop);
 8080         vmovdqu(Address(to, 0), xtmp);
 8081         vmovdqu(Address(to, 32), xtmp);
 8082         addptr(to, 64);
 8083         subl(count, 16 &lt;&lt; shift);
 8084         jcc(Assembler::greaterEqual, L_fill_64_bytes_loop);
 8085 
 8086         BIND(L_check_fill_32_bytes);
 8087         addl(count, 8 &lt;&lt; shift);
 8088         jccb(Assembler::less, L_check_fill_8_bytes);
 8089         vmovdqu(Address(to, 0), xtmp);
 8090         addptr(to, 32);
 8091         subl(count, 8 &lt;&lt; shift);
 8092 
 8093         BIND(L_check_fill_8_bytes);
 8094         // clean upper bits of YMM registers
 8095         movdl(xtmp, value);
 8096         pshufd(xtmp, xtmp, 0);
 8097       } else {
 8098         // Fill 32-byte chunks
 8099         pshufd(xtmp, xtmp, 0);
 8100 
 8101         subl(count, 8 &lt;&lt; shift);
 8102         jcc(Assembler::less, L_check_fill_8_bytes);
 8103         align(16);
 8104 
 8105         BIND(L_fill_32_bytes_loop);
 8106 
 8107         if (UseUnalignedLoadStores) {
 8108           movdqu(Address(to, 0), xtmp);
 8109           movdqu(Address(to, 16), xtmp);
 8110         } else {
 8111           movq(Address(to, 0), xtmp);
 8112           movq(Address(to, 8), xtmp);
 8113           movq(Address(to, 16), xtmp);
 8114           movq(Address(to, 24), xtmp);
 8115         }
 8116 
 8117         addptr(to, 32);
 8118         subl(count, 8 &lt;&lt; shift);
 8119         jcc(Assembler::greaterEqual, L_fill_32_bytes_loop);
 8120 
 8121         BIND(L_check_fill_8_bytes);
 8122       }
 8123       addl(count, 8 &lt;&lt; shift);
 8124       jccb(Assembler::zero, L_exit);
 8125       jmpb(L_fill_8_bytes);
 8126 
 8127       //
 8128       // length is too short, just fill qwords
 8129       //
 8130       BIND(L_fill_8_bytes_loop);
 8131       movq(Address(to, 0), xtmp);
 8132       addptr(to, 8);
 8133       BIND(L_fill_8_bytes);
 8134       subl(count, 1 &lt;&lt; (shift + 1));
 8135       jcc(Assembler::greaterEqual, L_fill_8_bytes_loop);
 8136     }
 8137   }
 8138   // fill trailing 4 bytes
 8139   BIND(L_fill_4_bytes);
 8140   testl(count, 1&lt;&lt;shift);
 8141   jccb(Assembler::zero, L_fill_2_bytes);
 8142   movl(Address(to, 0), value);
 8143   if (t == T_BYTE || t == T_SHORT) {
 8144     Label L_fill_byte;
 8145     addptr(to, 4);
 8146     BIND(L_fill_2_bytes);
 8147     // fill trailing 2 bytes
 8148     testl(count, 1&lt;&lt;(shift-1));
 8149     jccb(Assembler::zero, L_fill_byte);
 8150     movw(Address(to, 0), value);
 8151     if (t == T_BYTE) {
 8152       addptr(to, 2);
 8153       BIND(L_fill_byte);
 8154       // fill trailing byte
 8155       testl(count, 1);
 8156       jccb(Assembler::zero, L_exit);
 8157       movb(Address(to, 0), value);
 8158     } else {
 8159       BIND(L_fill_byte);
 8160     }
 8161   } else {
 8162     BIND(L_fill_2_bytes);
 8163   }
 8164   BIND(L_exit);
 8165 }
 8166 
 8167 // encode char[] to byte[] in ISO_8859_1
 8168    //@HotSpotIntrinsicCandidate
 8169    //private static int implEncodeISOArray(byte[] sa, int sp,
 8170    //byte[] da, int dp, int len) {
 8171    //  int i = 0;
 8172    //  for (; i &lt; len; i++) {
 8173    //    char c = StringUTF16.getChar(sa, sp++);
 8174    //    if (c &gt; &#39;\u00FF&#39;)
 8175    //      break;
 8176    //    da[dp++] = (byte)c;
 8177    //  }
 8178    //  return i;
 8179    //}
 8180 void MacroAssembler::encode_iso_array(Register src, Register dst, Register len,
 8181   XMMRegister tmp1Reg, XMMRegister tmp2Reg,
 8182   XMMRegister tmp3Reg, XMMRegister tmp4Reg,
 8183   Register tmp5, Register result) {
 8184 
 8185   // rsi: src
 8186   // rdi: dst
 8187   // rdx: len
 8188   // rcx: tmp5
 8189   // rax: result
 8190   ShortBranchVerifier sbv(this);
 8191   assert_different_registers(src, dst, len, tmp5, result);
 8192   Label L_done, L_copy_1_char, L_copy_1_char_exit;
 8193 
 8194   // set result
 8195   xorl(result, result);
 8196   // check for zero length
 8197   testl(len, len);
 8198   jcc(Assembler::zero, L_done);
 8199 
 8200   movl(result, len);
 8201 
 8202   // Setup pointers
 8203   lea(src, Address(src, len, Address::times_2)); // char[]
 8204   lea(dst, Address(dst, len, Address::times_1)); // byte[]
 8205   negptr(len);
 8206 
 8207   if (UseSSE42Intrinsics || UseAVX &gt;= 2) {
 8208     Label L_copy_8_chars, L_copy_8_chars_exit;
 8209     Label L_chars_16_check, L_copy_16_chars, L_copy_16_chars_exit;
 8210 
 8211     if (UseAVX &gt;= 2) {
 8212       Label L_chars_32_check, L_copy_32_chars, L_copy_32_chars_exit;
 8213       movl(tmp5, 0xff00ff00);   // create mask to test for Unicode chars in vector
 8214       movdl(tmp1Reg, tmp5);
 8215       vpbroadcastd(tmp1Reg, tmp1Reg, Assembler::AVX_256bit);
 8216       jmp(L_chars_32_check);
 8217 
 8218       bind(L_copy_32_chars);
 8219       vmovdqu(tmp3Reg, Address(src, len, Address::times_2, -64));
 8220       vmovdqu(tmp4Reg, Address(src, len, Address::times_2, -32));
 8221       vpor(tmp2Reg, tmp3Reg, tmp4Reg, /* vector_len */ 1);
 8222       vptest(tmp2Reg, tmp1Reg);       // check for Unicode chars in  vector
 8223       jccb(Assembler::notZero, L_copy_32_chars_exit);
 8224       vpackuswb(tmp3Reg, tmp3Reg, tmp4Reg, /* vector_len */ 1);
 8225       vpermq(tmp4Reg, tmp3Reg, 0xD8, /* vector_len */ 1);
 8226       vmovdqu(Address(dst, len, Address::times_1, -32), tmp4Reg);
 8227 
 8228       bind(L_chars_32_check);
 8229       addptr(len, 32);
 8230       jcc(Assembler::lessEqual, L_copy_32_chars);
 8231 
 8232       bind(L_copy_32_chars_exit);
 8233       subptr(len, 16);
 8234       jccb(Assembler::greater, L_copy_16_chars_exit);
 8235 
 8236     } else if (UseSSE42Intrinsics) {
 8237       movl(tmp5, 0xff00ff00);   // create mask to test for Unicode chars in vector
 8238       movdl(tmp1Reg, tmp5);
 8239       pshufd(tmp1Reg, tmp1Reg, 0);
 8240       jmpb(L_chars_16_check);
 8241     }
 8242 
 8243     bind(L_copy_16_chars);
 8244     if (UseAVX &gt;= 2) {
 8245       vmovdqu(tmp2Reg, Address(src, len, Address::times_2, -32));
 8246       vptest(tmp2Reg, tmp1Reg);
 8247       jcc(Assembler::notZero, L_copy_16_chars_exit);
 8248       vpackuswb(tmp2Reg, tmp2Reg, tmp1Reg, /* vector_len */ 1);
 8249       vpermq(tmp3Reg, tmp2Reg, 0xD8, /* vector_len */ 1);
 8250     } else {
 8251       if (UseAVX &gt; 0) {
 8252         movdqu(tmp3Reg, Address(src, len, Address::times_2, -32));
 8253         movdqu(tmp4Reg, Address(src, len, Address::times_2, -16));
 8254         vpor(tmp2Reg, tmp3Reg, tmp4Reg, /* vector_len */ 0);
 8255       } else {
 8256         movdqu(tmp3Reg, Address(src, len, Address::times_2, -32));
 8257         por(tmp2Reg, tmp3Reg);
 8258         movdqu(tmp4Reg, Address(src, len, Address::times_2, -16));
 8259         por(tmp2Reg, tmp4Reg);
 8260       }
 8261       ptest(tmp2Reg, tmp1Reg);       // check for Unicode chars in  vector
 8262       jccb(Assembler::notZero, L_copy_16_chars_exit);
 8263       packuswb(tmp3Reg, tmp4Reg);
 8264     }
 8265     movdqu(Address(dst, len, Address::times_1, -16), tmp3Reg);
 8266 
 8267     bind(L_chars_16_check);
 8268     addptr(len, 16);
 8269     jcc(Assembler::lessEqual, L_copy_16_chars);
 8270 
 8271     bind(L_copy_16_chars_exit);
 8272     if (UseAVX &gt;= 2) {
 8273       // clean upper bits of YMM registers
 8274       vpxor(tmp2Reg, tmp2Reg);
 8275       vpxor(tmp3Reg, tmp3Reg);
 8276       vpxor(tmp4Reg, tmp4Reg);
 8277       movdl(tmp1Reg, tmp5);
 8278       pshufd(tmp1Reg, tmp1Reg, 0);
 8279     }
 8280     subptr(len, 8);
 8281     jccb(Assembler::greater, L_copy_8_chars_exit);
 8282 
 8283     bind(L_copy_8_chars);
 8284     movdqu(tmp3Reg, Address(src, len, Address::times_2, -16));
 8285     ptest(tmp3Reg, tmp1Reg);
 8286     jccb(Assembler::notZero, L_copy_8_chars_exit);
 8287     packuswb(tmp3Reg, tmp1Reg);
 8288     movq(Address(dst, len, Address::times_1, -8), tmp3Reg);
 8289     addptr(len, 8);
 8290     jccb(Assembler::lessEqual, L_copy_8_chars);
 8291 
 8292     bind(L_copy_8_chars_exit);
 8293     subptr(len, 8);
 8294     jccb(Assembler::zero, L_done);
 8295   }
 8296 
 8297   bind(L_copy_1_char);
 8298   load_unsigned_short(tmp5, Address(src, len, Address::times_2, 0));
 8299   testl(tmp5, 0xff00);      // check if Unicode char
 8300   jccb(Assembler::notZero, L_copy_1_char_exit);
 8301   movb(Address(dst, len, Address::times_1, 0), tmp5);
 8302   addptr(len, 1);
 8303   jccb(Assembler::less, L_copy_1_char);
 8304 
 8305   bind(L_copy_1_char_exit);
 8306   addptr(result, len); // len is negative count of not processed elements
 8307 
 8308   bind(L_done);
 8309 }
 8310 
 8311 #ifdef _LP64
 8312 /**
 8313  * Helper for multiply_to_len().
 8314  */
 8315 void MacroAssembler::add2_with_carry(Register dest_hi, Register dest_lo, Register src1, Register src2) {
 8316   addq(dest_lo, src1);
 8317   adcq(dest_hi, 0);
 8318   addq(dest_lo, src2);
 8319   adcq(dest_hi, 0);
 8320 }
 8321 
 8322 /**
 8323  * Multiply 64 bit by 64 bit first loop.
 8324  */
 8325 void MacroAssembler::multiply_64_x_64_loop(Register x, Register xstart, Register x_xstart,
 8326                                            Register y, Register y_idx, Register z,
 8327                                            Register carry, Register product,
 8328                                            Register idx, Register kdx) {
 8329   //
 8330   //  jlong carry, x[], y[], z[];
 8331   //  for (int idx=ystart, kdx=ystart+1+xstart; idx &gt;= 0; idx-, kdx--) {
 8332   //    huge_128 product = y[idx] * x[xstart] + carry;
 8333   //    z[kdx] = (jlong)product;
 8334   //    carry  = (jlong)(product &gt;&gt;&gt; 64);
 8335   //  }
 8336   //  z[xstart] = carry;
 8337   //
 8338 
 8339   Label L_first_loop, L_first_loop_exit;
 8340   Label L_one_x, L_one_y, L_multiply;
 8341 
 8342   decrementl(xstart);
 8343   jcc(Assembler::negative, L_one_x);
 8344 
 8345   movq(x_xstart, Address(x, xstart, Address::times_4,  0));
 8346   rorq(x_xstart, 32); // convert big-endian to little-endian
 8347 
 8348   bind(L_first_loop);
 8349   decrementl(idx);
 8350   jcc(Assembler::negative, L_first_loop_exit);
 8351   decrementl(idx);
 8352   jcc(Assembler::negative, L_one_y);
 8353   movq(y_idx, Address(y, idx, Address::times_4,  0));
 8354   rorq(y_idx, 32); // convert big-endian to little-endian
 8355   bind(L_multiply);
 8356   movq(product, x_xstart);
 8357   mulq(y_idx); // product(rax) * y_idx -&gt; rdx:rax
 8358   addq(product, carry);
 8359   adcq(rdx, 0);
 8360   subl(kdx, 2);
 8361   movl(Address(z, kdx, Address::times_4,  4), product);
 8362   shrq(product, 32);
 8363   movl(Address(z, kdx, Address::times_4,  0), product);
 8364   movq(carry, rdx);
 8365   jmp(L_first_loop);
 8366 
 8367   bind(L_one_y);
 8368   movl(y_idx, Address(y,  0));
 8369   jmp(L_multiply);
 8370 
 8371   bind(L_one_x);
 8372   movl(x_xstart, Address(x,  0));
 8373   jmp(L_first_loop);
 8374 
 8375   bind(L_first_loop_exit);
 8376 }
 8377 
 8378 /**
 8379  * Multiply 64 bit by 64 bit and add 128 bit.
 8380  */
 8381 void MacroAssembler::multiply_add_128_x_128(Register x_xstart, Register y, Register z,
 8382                                             Register yz_idx, Register idx,
 8383                                             Register carry, Register product, int offset) {
 8384   //     huge_128 product = (y[idx] * x_xstart) + z[kdx] + carry;
 8385   //     z[kdx] = (jlong)product;
 8386 
 8387   movq(yz_idx, Address(y, idx, Address::times_4,  offset));
 8388   rorq(yz_idx, 32); // convert big-endian to little-endian
 8389   movq(product, x_xstart);
 8390   mulq(yz_idx);     // product(rax) * yz_idx -&gt; rdx:product(rax)
 8391   movq(yz_idx, Address(z, idx, Address::times_4,  offset));
 8392   rorq(yz_idx, 32); // convert big-endian to little-endian
 8393 
 8394   add2_with_carry(rdx, product, carry, yz_idx);
 8395 
 8396   movl(Address(z, idx, Address::times_4,  offset+4), product);
 8397   shrq(product, 32);
 8398   movl(Address(z, idx, Address::times_4,  offset), product);
 8399 
 8400 }
 8401 
 8402 /**
 8403  * Multiply 128 bit by 128 bit. Unrolled inner loop.
 8404  */
 8405 void MacroAssembler::multiply_128_x_128_loop(Register x_xstart, Register y, Register z,
 8406                                              Register yz_idx, Register idx, Register jdx,
 8407                                              Register carry, Register product,
 8408                                              Register carry2) {
 8409   //   jlong carry, x[], y[], z[];
 8410   //   int kdx = ystart+1;
 8411   //   for (int idx=ystart-2; idx &gt;= 0; idx -= 2) { // Third loop
 8412   //     huge_128 product = (y[idx+1] * x_xstart) + z[kdx+idx+1] + carry;
 8413   //     z[kdx+idx+1] = (jlong)product;
 8414   //     jlong carry2  = (jlong)(product &gt;&gt;&gt; 64);
 8415   //     product = (y[idx] * x_xstart) + z[kdx+idx] + carry2;
 8416   //     z[kdx+idx] = (jlong)product;
 8417   //     carry  = (jlong)(product &gt;&gt;&gt; 64);
 8418   //   }
 8419   //   idx += 2;
 8420   //   if (idx &gt; 0) {
 8421   //     product = (y[idx] * x_xstart) + z[kdx+idx] + carry;
 8422   //     z[kdx+idx] = (jlong)product;
 8423   //     carry  = (jlong)(product &gt;&gt;&gt; 64);
 8424   //   }
 8425   //
 8426 
 8427   Label L_third_loop, L_third_loop_exit, L_post_third_loop_done;
 8428 
 8429   movl(jdx, idx);
 8430   andl(jdx, 0xFFFFFFFC);
 8431   shrl(jdx, 2);
 8432 
 8433   bind(L_third_loop);
 8434   subl(jdx, 1);
 8435   jcc(Assembler::negative, L_third_loop_exit);
 8436   subl(idx, 4);
 8437 
 8438   multiply_add_128_x_128(x_xstart, y, z, yz_idx, idx, carry, product, 8);
 8439   movq(carry2, rdx);
 8440 
 8441   multiply_add_128_x_128(x_xstart, y, z, yz_idx, idx, carry2, product, 0);
 8442   movq(carry, rdx);
 8443   jmp(L_third_loop);
 8444 
 8445   bind (L_third_loop_exit);
 8446 
 8447   andl (idx, 0x3);
 8448   jcc(Assembler::zero, L_post_third_loop_done);
 8449 
 8450   Label L_check_1;
 8451   subl(idx, 2);
 8452   jcc(Assembler::negative, L_check_1);
 8453 
 8454   multiply_add_128_x_128(x_xstart, y, z, yz_idx, idx, carry, product, 0);
 8455   movq(carry, rdx);
 8456 
 8457   bind (L_check_1);
 8458   addl (idx, 0x2);
 8459   andl (idx, 0x1);
 8460   subl(idx, 1);
 8461   jcc(Assembler::negative, L_post_third_loop_done);
 8462 
 8463   movl(yz_idx, Address(y, idx, Address::times_4,  0));
 8464   movq(product, x_xstart);
 8465   mulq(yz_idx); // product(rax) * yz_idx -&gt; rdx:product(rax)
 8466   movl(yz_idx, Address(z, idx, Address::times_4,  0));
 8467 
 8468   add2_with_carry(rdx, product, yz_idx, carry);
 8469 
 8470   movl(Address(z, idx, Address::times_4,  0), product);
 8471   shrq(product, 32);
 8472 
 8473   shlq(rdx, 32);
 8474   orq(product, rdx);
 8475   movq(carry, product);
 8476 
 8477   bind(L_post_third_loop_done);
 8478 }
 8479 
 8480 /**
 8481  * Multiply 128 bit by 128 bit using BMI2. Unrolled inner loop.
 8482  *
 8483  */
 8484 void MacroAssembler::multiply_128_x_128_bmi2_loop(Register y, Register z,
 8485                                                   Register carry, Register carry2,
 8486                                                   Register idx, Register jdx,
 8487                                                   Register yz_idx1, Register yz_idx2,
 8488                                                   Register tmp, Register tmp3, Register tmp4) {
 8489   assert(UseBMI2Instructions, &quot;should be used only when BMI2 is available&quot;);
 8490 
 8491   //   jlong carry, x[], y[], z[];
 8492   //   int kdx = ystart+1;
 8493   //   for (int idx=ystart-2; idx &gt;= 0; idx -= 2) { // Third loop
 8494   //     huge_128 tmp3 = (y[idx+1] * rdx) + z[kdx+idx+1] + carry;
 8495   //     jlong carry2  = (jlong)(tmp3 &gt;&gt;&gt; 64);
 8496   //     huge_128 tmp4 = (y[idx]   * rdx) + z[kdx+idx] + carry2;
 8497   //     carry  = (jlong)(tmp4 &gt;&gt;&gt; 64);
 8498   //     z[kdx+idx+1] = (jlong)tmp3;
 8499   //     z[kdx+idx] = (jlong)tmp4;
 8500   //   }
 8501   //   idx += 2;
 8502   //   if (idx &gt; 0) {
 8503   //     yz_idx1 = (y[idx] * rdx) + z[kdx+idx] + carry;
 8504   //     z[kdx+idx] = (jlong)yz_idx1;
 8505   //     carry  = (jlong)(yz_idx1 &gt;&gt;&gt; 64);
 8506   //   }
 8507   //
 8508 
 8509   Label L_third_loop, L_third_loop_exit, L_post_third_loop_done;
 8510 
 8511   movl(jdx, idx);
 8512   andl(jdx, 0xFFFFFFFC);
 8513   shrl(jdx, 2);
 8514 
 8515   bind(L_third_loop);
 8516   subl(jdx, 1);
 8517   jcc(Assembler::negative, L_third_loop_exit);
 8518   subl(idx, 4);
 8519 
 8520   movq(yz_idx1,  Address(y, idx, Address::times_4,  8));
 8521   rorxq(yz_idx1, yz_idx1, 32); // convert big-endian to little-endian
 8522   movq(yz_idx2, Address(y, idx, Address::times_4,  0));
 8523   rorxq(yz_idx2, yz_idx2, 32);
 8524 
 8525   mulxq(tmp4, tmp3, yz_idx1);  //  yz_idx1 * rdx -&gt; tmp4:tmp3
 8526   mulxq(carry2, tmp, yz_idx2); //  yz_idx2 * rdx -&gt; carry2:tmp
 8527 
 8528   movq(yz_idx1,  Address(z, idx, Address::times_4,  8));
 8529   rorxq(yz_idx1, yz_idx1, 32);
 8530   movq(yz_idx2, Address(z, idx, Address::times_4,  0));
 8531   rorxq(yz_idx2, yz_idx2, 32);
 8532 
 8533   if (VM_Version::supports_adx()) {
 8534     adcxq(tmp3, carry);
 8535     adoxq(tmp3, yz_idx1);
 8536 
 8537     adcxq(tmp4, tmp);
 8538     adoxq(tmp4, yz_idx2);
 8539 
 8540     movl(carry, 0); // does not affect flags
 8541     adcxq(carry2, carry);
 8542     adoxq(carry2, carry);
 8543   } else {
 8544     add2_with_carry(tmp4, tmp3, carry, yz_idx1);
 8545     add2_with_carry(carry2, tmp4, tmp, yz_idx2);
 8546   }
 8547   movq(carry, carry2);
 8548 
 8549   movl(Address(z, idx, Address::times_4, 12), tmp3);
 8550   shrq(tmp3, 32);
 8551   movl(Address(z, idx, Address::times_4,  8), tmp3);
 8552 
 8553   movl(Address(z, idx, Address::times_4,  4), tmp4);
 8554   shrq(tmp4, 32);
 8555   movl(Address(z, idx, Address::times_4,  0), tmp4);
 8556 
 8557   jmp(L_third_loop);
 8558 
 8559   bind (L_third_loop_exit);
 8560 
 8561   andl (idx, 0x3);
 8562   jcc(Assembler::zero, L_post_third_loop_done);
 8563 
 8564   Label L_check_1;
 8565   subl(idx, 2);
 8566   jcc(Assembler::negative, L_check_1);
 8567 
 8568   movq(yz_idx1, Address(y, idx, Address::times_4,  0));
 8569   rorxq(yz_idx1, yz_idx1, 32);
 8570   mulxq(tmp4, tmp3, yz_idx1); //  yz_idx1 * rdx -&gt; tmp4:tmp3
 8571   movq(yz_idx2, Address(z, idx, Address::times_4,  0));
 8572   rorxq(yz_idx2, yz_idx2, 32);
 8573 
 8574   add2_with_carry(tmp4, tmp3, carry, yz_idx2);
 8575 
 8576   movl(Address(z, idx, Address::times_4,  4), tmp3);
 8577   shrq(tmp3, 32);
 8578   movl(Address(z, idx, Address::times_4,  0), tmp3);
 8579   movq(carry, tmp4);
 8580 
 8581   bind (L_check_1);
 8582   addl (idx, 0x2);
 8583   andl (idx, 0x1);
 8584   subl(idx, 1);
 8585   jcc(Assembler::negative, L_post_third_loop_done);
 8586   movl(tmp4, Address(y, idx, Address::times_4,  0));
 8587   mulxq(carry2, tmp3, tmp4);  //  tmp4 * rdx -&gt; carry2:tmp3
 8588   movl(tmp4, Address(z, idx, Address::times_4,  0));
 8589 
 8590   add2_with_carry(carry2, tmp3, tmp4, carry);
 8591 
 8592   movl(Address(z, idx, Address::times_4,  0), tmp3);
 8593   shrq(tmp3, 32);
 8594 
 8595   shlq(carry2, 32);
 8596   orq(tmp3, carry2);
 8597   movq(carry, tmp3);
 8598 
 8599   bind(L_post_third_loop_done);
 8600 }
 8601 
 8602 /**
 8603  * Code for BigInteger::multiplyToLen() instrinsic.
 8604  *
 8605  * rdi: x
 8606  * rax: xlen
 8607  * rsi: y
 8608  * rcx: ylen
 8609  * r8:  z
 8610  * r11: zlen
 8611  * r12: tmp1
 8612  * r13: tmp2
 8613  * r14: tmp3
 8614  * r15: tmp4
 8615  * rbx: tmp5
 8616  *
 8617  */
 8618 void MacroAssembler::multiply_to_len(Register x, Register xlen, Register y, Register ylen, Register z, Register zlen,
 8619                                      Register tmp1, Register tmp2, Register tmp3, Register tmp4, Register tmp5) {
 8620   ShortBranchVerifier sbv(this);
 8621   assert_different_registers(x, xlen, y, ylen, z, zlen, tmp1, tmp2, tmp3, tmp4, tmp5, rdx);
 8622 
 8623   push(tmp1);
 8624   push(tmp2);
 8625   push(tmp3);
 8626   push(tmp4);
 8627   push(tmp5);
 8628 
 8629   push(xlen);
 8630   push(zlen);
 8631 
 8632   const Register idx = tmp1;
 8633   const Register kdx = tmp2;
 8634   const Register xstart = tmp3;
 8635 
 8636   const Register y_idx = tmp4;
 8637   const Register carry = tmp5;
 8638   const Register product  = xlen;
 8639   const Register x_xstart = zlen;  // reuse register
 8640 
 8641   // First Loop.
 8642   //
 8643   //  final static long LONG_MASK = 0xffffffffL;
 8644   //  int xstart = xlen - 1;
 8645   //  int ystart = ylen - 1;
 8646   //  long carry = 0;
 8647   //  for (int idx=ystart, kdx=ystart+1+xstart; idx &gt;= 0; idx-, kdx--) {
 8648   //    long product = (y[idx] &amp; LONG_MASK) * (x[xstart] &amp; LONG_MASK) + carry;
 8649   //    z[kdx] = (int)product;
 8650   //    carry = product &gt;&gt;&gt; 32;
 8651   //  }
 8652   //  z[xstart] = (int)carry;
 8653   //
 8654 
 8655   movl(idx, ylen);      // idx = ylen;
 8656   movl(kdx, zlen);      // kdx = xlen+ylen;
 8657   xorq(carry, carry);   // carry = 0;
 8658 
 8659   Label L_done;
 8660 
 8661   movl(xstart, xlen);
 8662   decrementl(xstart);
 8663   jcc(Assembler::negative, L_done);
 8664 
 8665   multiply_64_x_64_loop(x, xstart, x_xstart, y, y_idx, z, carry, product, idx, kdx);
 8666 
 8667   Label L_second_loop;
 8668   testl(kdx, kdx);
 8669   jcc(Assembler::zero, L_second_loop);
 8670 
 8671   Label L_carry;
 8672   subl(kdx, 1);
 8673   jcc(Assembler::zero, L_carry);
 8674 
 8675   movl(Address(z, kdx, Address::times_4,  0), carry);
 8676   shrq(carry, 32);
 8677   subl(kdx, 1);
 8678 
 8679   bind(L_carry);
 8680   movl(Address(z, kdx, Address::times_4,  0), carry);
 8681 
 8682   // Second and third (nested) loops.
 8683   //
 8684   // for (int i = xstart-1; i &gt;= 0; i--) { // Second loop
 8685   //   carry = 0;
 8686   //   for (int jdx=ystart, k=ystart+1+i; jdx &gt;= 0; jdx--, k--) { // Third loop
 8687   //     long product = (y[jdx] &amp; LONG_MASK) * (x[i] &amp; LONG_MASK) +
 8688   //                    (z[k] &amp; LONG_MASK) + carry;
 8689   //     z[k] = (int)product;
 8690   //     carry = product &gt;&gt;&gt; 32;
 8691   //   }
 8692   //   z[i] = (int)carry;
 8693   // }
 8694   //
 8695   // i = xlen, j = tmp1, k = tmp2, carry = tmp5, x[i] = rdx
 8696 
 8697   const Register jdx = tmp1;
 8698 
 8699   bind(L_second_loop);
 8700   xorl(carry, carry);    // carry = 0;
 8701   movl(jdx, ylen);       // j = ystart+1
 8702 
 8703   subl(xstart, 1);       // i = xstart-1;
 8704   jcc(Assembler::negative, L_done);
 8705 
 8706   push (z);
 8707 
 8708   Label L_last_x;
 8709   lea(z, Address(z, xstart, Address::times_4, 4)); // z = z + k - j
 8710   subl(xstart, 1);       // i = xstart-1;
 8711   jcc(Assembler::negative, L_last_x);
 8712 
 8713   if (UseBMI2Instructions) {
 8714     movq(rdx,  Address(x, xstart, Address::times_4,  0));
 8715     rorxq(rdx, rdx, 32); // convert big-endian to little-endian
 8716   } else {
 8717     movq(x_xstart, Address(x, xstart, Address::times_4,  0));
 8718     rorq(x_xstart, 32);  // convert big-endian to little-endian
 8719   }
 8720 
 8721   Label L_third_loop_prologue;
 8722   bind(L_third_loop_prologue);
 8723 
 8724   push (x);
 8725   push (xstart);
 8726   push (ylen);
 8727 
 8728 
 8729   if (UseBMI2Instructions) {
 8730     multiply_128_x_128_bmi2_loop(y, z, carry, x, jdx, ylen, product, tmp2, x_xstart, tmp3, tmp4);
 8731   } else { // !UseBMI2Instructions
 8732     multiply_128_x_128_loop(x_xstart, y, z, y_idx, jdx, ylen, carry, product, x);
 8733   }
 8734 
 8735   pop(ylen);
 8736   pop(xlen);
 8737   pop(x);
 8738   pop(z);
 8739 
 8740   movl(tmp3, xlen);
 8741   addl(tmp3, 1);
 8742   movl(Address(z, tmp3, Address::times_4,  0), carry);
 8743   subl(tmp3, 1);
 8744   jccb(Assembler::negative, L_done);
 8745 
 8746   shrq(carry, 32);
 8747   movl(Address(z, tmp3, Address::times_4,  0), carry);
 8748   jmp(L_second_loop);
 8749 
 8750   // Next infrequent code is moved outside loops.
 8751   bind(L_last_x);
 8752   if (UseBMI2Instructions) {
 8753     movl(rdx, Address(x,  0));
 8754   } else {
 8755     movl(x_xstart, Address(x,  0));
 8756   }
 8757   jmp(L_third_loop_prologue);
 8758 
 8759   bind(L_done);
 8760 
 8761   pop(zlen);
 8762   pop(xlen);
 8763 
 8764   pop(tmp5);
 8765   pop(tmp4);
 8766   pop(tmp3);
 8767   pop(tmp2);
 8768   pop(tmp1);
 8769 }
 8770 
 8771 void MacroAssembler::vectorized_mismatch(Register obja, Register objb, Register length, Register log2_array_indxscale,
 8772   Register result, Register tmp1, Register tmp2, XMMRegister rymm0, XMMRegister rymm1, XMMRegister rymm2){
 8773   assert(UseSSE42Intrinsics, &quot;SSE4.2 must be enabled.&quot;);
 8774   Label VECTOR16_LOOP, VECTOR8_LOOP, VECTOR4_LOOP;
 8775   Label VECTOR8_TAIL, VECTOR4_TAIL;
 8776   Label VECTOR32_NOT_EQUAL, VECTOR16_NOT_EQUAL, VECTOR8_NOT_EQUAL, VECTOR4_NOT_EQUAL;
 8777   Label SAME_TILL_END, DONE;
 8778   Label BYTES_LOOP, BYTES_TAIL, BYTES_NOT_EQUAL;
 8779 
 8780   //scale is in rcx in both Win64 and Unix
 8781   ShortBranchVerifier sbv(this);
 8782 
 8783   shlq(length);
 8784   xorq(result, result);
 8785 
 8786   if ((AVX3Threshold == 0) &amp;&amp; (UseAVX &gt; 2) &amp;&amp;
 8787       VM_Version::supports_avx512vlbw()) {
 8788     Label VECTOR64_LOOP, VECTOR64_NOT_EQUAL, VECTOR32_TAIL;
 8789 
 8790     cmpq(length, 64);
 8791     jcc(Assembler::less, VECTOR32_TAIL);
 8792 
 8793     movq(tmp1, length);
 8794     andq(tmp1, 0x3F);      // tail count
 8795     andq(length, ~(0x3F)); //vector count
 8796 
 8797     bind(VECTOR64_LOOP);
 8798     // AVX512 code to compare 64 byte vectors.
 8799     evmovdqub(rymm0, Address(obja, result), Assembler::AVX_512bit);
 8800     evpcmpeqb(k7, rymm0, Address(objb, result), Assembler::AVX_512bit);
 8801     kortestql(k7, k7);
 8802     jcc(Assembler::aboveEqual, VECTOR64_NOT_EQUAL);     // mismatch
 8803     addq(result, 64);
 8804     subq(length, 64);
 8805     jccb(Assembler::notZero, VECTOR64_LOOP);
 8806 
 8807     //bind(VECTOR64_TAIL);
 8808     testq(tmp1, tmp1);
 8809     jcc(Assembler::zero, SAME_TILL_END);
 8810 
 8811     //bind(VECTOR64_TAIL);
 8812     // AVX512 code to compare upto 63 byte vectors.
 8813     mov64(tmp2, 0xFFFFFFFFFFFFFFFF);
 8814     shlxq(tmp2, tmp2, tmp1);
 8815     notq(tmp2);
 8816     kmovql(k3, tmp2);
 8817 
 8818     evmovdqub(rymm0, k3, Address(obja, result), Assembler::AVX_512bit);
 8819     evpcmpeqb(k7, k3, rymm0, Address(objb, result), Assembler::AVX_512bit);
 8820 
 8821     ktestql(k7, k3);
 8822     jcc(Assembler::below, SAME_TILL_END);     // not mismatch
 8823 
 8824     bind(VECTOR64_NOT_EQUAL);
 8825     kmovql(tmp1, k7);
 8826     notq(tmp1);
 8827     tzcntq(tmp1, tmp1);
 8828     addq(result, tmp1);
 8829     shrq(result);
 8830     jmp(DONE);
 8831     bind(VECTOR32_TAIL);
 8832   }
 8833 
 8834   cmpq(length, 8);
 8835   jcc(Assembler::equal, VECTOR8_LOOP);
 8836   jcc(Assembler::less, VECTOR4_TAIL);
 8837 
 8838   if (UseAVX &gt;= 2) {
 8839     Label VECTOR16_TAIL, VECTOR32_LOOP;
 8840 
 8841     cmpq(length, 16);
 8842     jcc(Assembler::equal, VECTOR16_LOOP);
 8843     jcc(Assembler::less, VECTOR8_LOOP);
 8844 
 8845     cmpq(length, 32);
 8846     jccb(Assembler::less, VECTOR16_TAIL);
 8847 
 8848     subq(length, 32);
 8849     bind(VECTOR32_LOOP);
 8850     vmovdqu(rymm0, Address(obja, result));
 8851     vmovdqu(rymm1, Address(objb, result));
 8852     vpxor(rymm2, rymm0, rymm1, Assembler::AVX_256bit);
 8853     vptest(rymm2, rymm2);
 8854     jcc(Assembler::notZero, VECTOR32_NOT_EQUAL);//mismatch found
 8855     addq(result, 32);
 8856     subq(length, 32);
 8857     jcc(Assembler::greaterEqual, VECTOR32_LOOP);
 8858     addq(length, 32);
 8859     jcc(Assembler::equal, SAME_TILL_END);
 8860     //falling through if less than 32 bytes left //close the branch here.
 8861 
 8862     bind(VECTOR16_TAIL);
 8863     cmpq(length, 16);
 8864     jccb(Assembler::less, VECTOR8_TAIL);
 8865     bind(VECTOR16_LOOP);
 8866     movdqu(rymm0, Address(obja, result));
 8867     movdqu(rymm1, Address(objb, result));
 8868     vpxor(rymm2, rymm0, rymm1, Assembler::AVX_128bit);
 8869     ptest(rymm2, rymm2);
 8870     jcc(Assembler::notZero, VECTOR16_NOT_EQUAL);//mismatch found
 8871     addq(result, 16);
 8872     subq(length, 16);
 8873     jcc(Assembler::equal, SAME_TILL_END);
 8874     //falling through if less than 16 bytes left
 8875   } else {//regular intrinsics
 8876 
 8877     cmpq(length, 16);
 8878     jccb(Assembler::less, VECTOR8_TAIL);
 8879 
 8880     subq(length, 16);
 8881     bind(VECTOR16_LOOP);
 8882     movdqu(rymm0, Address(obja, result));
 8883     movdqu(rymm1, Address(objb, result));
 8884     pxor(rymm0, rymm1);
 8885     ptest(rymm0, rymm0);
 8886     jcc(Assembler::notZero, VECTOR16_NOT_EQUAL);//mismatch found
 8887     addq(result, 16);
 8888     subq(length, 16);
 8889     jccb(Assembler::greaterEqual, VECTOR16_LOOP);
 8890     addq(length, 16);
 8891     jcc(Assembler::equal, SAME_TILL_END);
 8892     //falling through if less than 16 bytes left
 8893   }
 8894 
 8895   bind(VECTOR8_TAIL);
 8896   cmpq(length, 8);
 8897   jccb(Assembler::less, VECTOR4_TAIL);
 8898   bind(VECTOR8_LOOP);
 8899   movq(tmp1, Address(obja, result));
 8900   movq(tmp2, Address(objb, result));
 8901   xorq(tmp1, tmp2);
 8902   testq(tmp1, tmp1);
 8903   jcc(Assembler::notZero, VECTOR8_NOT_EQUAL);//mismatch found
 8904   addq(result, 8);
 8905   subq(length, 8);
 8906   jcc(Assembler::equal, SAME_TILL_END);
 8907   //falling through if less than 8 bytes left
 8908 
 8909   bind(VECTOR4_TAIL);
 8910   cmpq(length, 4);
 8911   jccb(Assembler::less, BYTES_TAIL);
 8912   bind(VECTOR4_LOOP);
 8913   movl(tmp1, Address(obja, result));
 8914   xorl(tmp1, Address(objb, result));
 8915   testl(tmp1, tmp1);
 8916   jcc(Assembler::notZero, VECTOR4_NOT_EQUAL);//mismatch found
 8917   addq(result, 4);
 8918   subq(length, 4);
 8919   jcc(Assembler::equal, SAME_TILL_END);
 8920   //falling through if less than 4 bytes left
 8921 
 8922   bind(BYTES_TAIL);
 8923   bind(BYTES_LOOP);
 8924   load_unsigned_byte(tmp1, Address(obja, result));
 8925   load_unsigned_byte(tmp2, Address(objb, result));
 8926   xorl(tmp1, tmp2);
 8927   testl(tmp1, tmp1);
 8928   jcc(Assembler::notZero, BYTES_NOT_EQUAL);//mismatch found
 8929   decq(length);
 8930   jcc(Assembler::zero, SAME_TILL_END);
 8931   incq(result);
 8932   load_unsigned_byte(tmp1, Address(obja, result));
 8933   load_unsigned_byte(tmp2, Address(objb, result));
 8934   xorl(tmp1, tmp2);
 8935   testl(tmp1, tmp1);
 8936   jcc(Assembler::notZero, BYTES_NOT_EQUAL);//mismatch found
 8937   decq(length);
 8938   jcc(Assembler::zero, SAME_TILL_END);
 8939   incq(result);
 8940   load_unsigned_byte(tmp1, Address(obja, result));
 8941   load_unsigned_byte(tmp2, Address(objb, result));
 8942   xorl(tmp1, tmp2);
 8943   testl(tmp1, tmp1);
 8944   jcc(Assembler::notZero, BYTES_NOT_EQUAL);//mismatch found
 8945   jmp(SAME_TILL_END);
 8946 
 8947   if (UseAVX &gt;= 2) {
 8948     bind(VECTOR32_NOT_EQUAL);
 8949     vpcmpeqb(rymm2, rymm2, rymm2, Assembler::AVX_256bit);
 8950     vpcmpeqb(rymm0, rymm0, rymm1, Assembler::AVX_256bit);
 8951     vpxor(rymm0, rymm0, rymm2, Assembler::AVX_256bit);
 8952     vpmovmskb(tmp1, rymm0);
 8953     bsfq(tmp1, tmp1);
 8954     addq(result, tmp1);
 8955     shrq(result);
 8956     jmp(DONE);
 8957   }
 8958 
 8959   bind(VECTOR16_NOT_EQUAL);
 8960   if (UseAVX &gt;= 2) {
 8961     vpcmpeqb(rymm2, rymm2, rymm2, Assembler::AVX_128bit);
 8962     vpcmpeqb(rymm0, rymm0, rymm1, Assembler::AVX_128bit);
 8963     pxor(rymm0, rymm2);
 8964   } else {
 8965     pcmpeqb(rymm2, rymm2);
 8966     pxor(rymm0, rymm1);
 8967     pcmpeqb(rymm0, rymm1);
 8968     pxor(rymm0, rymm2);
 8969   }
 8970   pmovmskb(tmp1, rymm0);
 8971   bsfq(tmp1, tmp1);
 8972   addq(result, tmp1);
 8973   shrq(result);
 8974   jmpb(DONE);
 8975 
 8976   bind(VECTOR8_NOT_EQUAL);
 8977   bind(VECTOR4_NOT_EQUAL);
 8978   bsfq(tmp1, tmp1);
 8979   shrq(tmp1, 3);
 8980   addq(result, tmp1);
 8981   bind(BYTES_NOT_EQUAL);
 8982   shrq(result);
 8983   jmpb(DONE);
 8984 
 8985   bind(SAME_TILL_END);
 8986   mov64(result, -1);
 8987 
 8988   bind(DONE);
 8989 }
 8990 
 8991 //Helper functions for square_to_len()
 8992 
 8993 /**
 8994  * Store the squares of x[], right shifted one bit (divided by 2) into z[]
 8995  * Preserves x and z and modifies rest of the registers.
 8996  */
 8997 void MacroAssembler::square_rshift(Register x, Register xlen, Register z, Register tmp1, Register tmp3, Register tmp4, Register tmp5, Register rdxReg, Register raxReg) {
 8998   // Perform square and right shift by 1
 8999   // Handle odd xlen case first, then for even xlen do the following
 9000   // jlong carry = 0;
 9001   // for (int j=0, i=0; j &lt; xlen; j+=2, i+=4) {
 9002   //     huge_128 product = x[j:j+1] * x[j:j+1];
 9003   //     z[i:i+1] = (carry &lt;&lt; 63) | (jlong)(product &gt;&gt;&gt; 65);
 9004   //     z[i+2:i+3] = (jlong)(product &gt;&gt;&gt; 1);
 9005   //     carry = (jlong)product;
 9006   // }
 9007 
 9008   xorq(tmp5, tmp5);     // carry
 9009   xorq(rdxReg, rdxReg);
 9010   xorl(tmp1, tmp1);     // index for x
 9011   xorl(tmp4, tmp4);     // index for z
 9012 
 9013   Label L_first_loop, L_first_loop_exit;
 9014 
 9015   testl(xlen, 1);
 9016   jccb(Assembler::zero, L_first_loop); //jump if xlen is even
 9017 
 9018   // Square and right shift by 1 the odd element using 32 bit multiply
 9019   movl(raxReg, Address(x, tmp1, Address::times_4, 0));
 9020   imulq(raxReg, raxReg);
 9021   shrq(raxReg, 1);
 9022   adcq(tmp5, 0);
 9023   movq(Address(z, tmp4, Address::times_4, 0), raxReg);
 9024   incrementl(tmp1);
 9025   addl(tmp4, 2);
 9026 
 9027   // Square and  right shift by 1 the rest using 64 bit multiply
 9028   bind(L_first_loop);
 9029   cmpptr(tmp1, xlen);
 9030   jccb(Assembler::equal, L_first_loop_exit);
 9031 
 9032   // Square
 9033   movq(raxReg, Address(x, tmp1, Address::times_4,  0));
 9034   rorq(raxReg, 32);    // convert big-endian to little-endian
 9035   mulq(raxReg);        // 64-bit multiply rax * rax -&gt; rdx:rax
 9036 
 9037   // Right shift by 1 and save carry
 9038   shrq(tmp5, 1);       // rdx:rax:tmp5 = (tmp5:rdx:rax) &gt;&gt;&gt; 1
 9039   rcrq(rdxReg, 1);
 9040   rcrq(raxReg, 1);
 9041   adcq(tmp5, 0);
 9042 
 9043   // Store result in z
 9044   movq(Address(z, tmp4, Address::times_4, 0), rdxReg);
 9045   movq(Address(z, tmp4, Address::times_4, 8), raxReg);
 9046 
 9047   // Update indices for x and z
 9048   addl(tmp1, 2);
 9049   addl(tmp4, 4);
 9050   jmp(L_first_loop);
 9051 
 9052   bind(L_first_loop_exit);
 9053 }
 9054 
 9055 
 9056 /**
 9057  * Perform the following multiply add operation using BMI2 instructions
 9058  * carry:sum = sum + op1*op2 + carry
 9059  * op2 should be in rdx
 9060  * op2 is preserved, all other registers are modified
 9061  */
 9062 void MacroAssembler::multiply_add_64_bmi2(Register sum, Register op1, Register op2, Register carry, Register tmp2) {
 9063   // assert op2 is rdx
 9064   mulxq(tmp2, op1, op1);  //  op1 * op2 -&gt; tmp2:op1
 9065   addq(sum, carry);
 9066   adcq(tmp2, 0);
 9067   addq(sum, op1);
 9068   adcq(tmp2, 0);
 9069   movq(carry, tmp2);
 9070 }
 9071 
 9072 /**
 9073  * Perform the following multiply add operation:
 9074  * carry:sum = sum + op1*op2 + carry
 9075  * Preserves op1, op2 and modifies rest of registers
 9076  */
 9077 void MacroAssembler::multiply_add_64(Register sum, Register op1, Register op2, Register carry, Register rdxReg, Register raxReg) {
 9078   // rdx:rax = op1 * op2
 9079   movq(raxReg, op2);
 9080   mulq(op1);
 9081 
 9082   //  rdx:rax = sum + carry + rdx:rax
 9083   addq(sum, carry);
 9084   adcq(rdxReg, 0);
 9085   addq(sum, raxReg);
 9086   adcq(rdxReg, 0);
 9087 
 9088   // carry:sum = rdx:sum
 9089   movq(carry, rdxReg);
 9090 }
 9091 
 9092 /**
 9093  * Add 64 bit long carry into z[] with carry propogation.
 9094  * Preserves z and carry register values and modifies rest of registers.
 9095  *
 9096  */
 9097 void MacroAssembler::add_one_64(Register z, Register zlen, Register carry, Register tmp1) {
 9098   Label L_fourth_loop, L_fourth_loop_exit;
 9099 
 9100   movl(tmp1, 1);
 9101   subl(zlen, 2);
 9102   addq(Address(z, zlen, Address::times_4, 0), carry);
 9103 
 9104   bind(L_fourth_loop);
 9105   jccb(Assembler::carryClear, L_fourth_loop_exit);
 9106   subl(zlen, 2);
 9107   jccb(Assembler::negative, L_fourth_loop_exit);
 9108   addq(Address(z, zlen, Address::times_4, 0), tmp1);
 9109   jmp(L_fourth_loop);
 9110   bind(L_fourth_loop_exit);
 9111 }
 9112 
 9113 /**
 9114  * Shift z[] left by 1 bit.
 9115  * Preserves x, len, z and zlen registers and modifies rest of the registers.
 9116  *
 9117  */
 9118 void MacroAssembler::lshift_by_1(Register x, Register len, Register z, Register zlen, Register tmp1, Register tmp2, Register tmp3, Register tmp4) {
 9119 
 9120   Label L_fifth_loop, L_fifth_loop_exit;
 9121 
 9122   // Fifth loop
 9123   // Perform primitiveLeftShift(z, zlen, 1)
 9124 
 9125   const Register prev_carry = tmp1;
 9126   const Register new_carry = tmp4;
 9127   const Register value = tmp2;
 9128   const Register zidx = tmp3;
 9129 
 9130   // int zidx, carry;
 9131   // long value;
 9132   // carry = 0;
 9133   // for (zidx = zlen-2; zidx &gt;=0; zidx -= 2) {
 9134   //    (carry:value)  = (z[i] &lt;&lt; 1) | carry ;
 9135   //    z[i] = value;
 9136   // }
 9137 
 9138   movl(zidx, zlen);
 9139   xorl(prev_carry, prev_carry); // clear carry flag and prev_carry register
 9140 
 9141   bind(L_fifth_loop);
 9142   decl(zidx);  // Use decl to preserve carry flag
 9143   decl(zidx);
 9144   jccb(Assembler::negative, L_fifth_loop_exit);
 9145 
 9146   if (UseBMI2Instructions) {
 9147      movq(value, Address(z, zidx, Address::times_4, 0));
 9148      rclq(value, 1);
 9149      rorxq(value, value, 32);
 9150      movq(Address(z, zidx, Address::times_4,  0), value);  // Store back in big endian form
 9151   }
 9152   else {
 9153     // clear new_carry
 9154     xorl(new_carry, new_carry);
 9155 
 9156     // Shift z[i] by 1, or in previous carry and save new carry
 9157     movq(value, Address(z, zidx, Address::times_4, 0));
 9158     shlq(value, 1);
 9159     adcl(new_carry, 0);
 9160 
 9161     orq(value, prev_carry);
 9162     rorq(value, 0x20);
 9163     movq(Address(z, zidx, Address::times_4,  0), value);  // Store back in big endian form
 9164 
 9165     // Set previous carry = new carry
 9166     movl(prev_carry, new_carry);
 9167   }
 9168   jmp(L_fifth_loop);
 9169 
 9170   bind(L_fifth_loop_exit);
 9171 }
 9172 
 9173 
 9174 /**
 9175  * Code for BigInteger::squareToLen() intrinsic
 9176  *
 9177  * rdi: x
 9178  * rsi: len
 9179  * r8:  z
 9180  * rcx: zlen
 9181  * r12: tmp1
 9182  * r13: tmp2
 9183  * r14: tmp3
 9184  * r15: tmp4
 9185  * rbx: tmp5
 9186  *
 9187  */
 9188 void MacroAssembler::square_to_len(Register x, Register len, Register z, Register zlen, Register tmp1, Register tmp2, Register tmp3, Register tmp4, Register tmp5, Register rdxReg, Register raxReg) {
 9189 
 9190   Label L_second_loop, L_second_loop_exit, L_third_loop, L_third_loop_exit, L_last_x, L_multiply;
 9191   push(tmp1);
 9192   push(tmp2);
 9193   push(tmp3);
 9194   push(tmp4);
 9195   push(tmp5);
 9196 
 9197   // First loop
 9198   // Store the squares, right shifted one bit (i.e., divided by 2).
 9199   square_rshift(x, len, z, tmp1, tmp3, tmp4, tmp5, rdxReg, raxReg);
 9200 
 9201   // Add in off-diagonal sums.
 9202   //
 9203   // Second, third (nested) and fourth loops.
 9204   // zlen +=2;
 9205   // for (int xidx=len-2,zidx=zlen-4; xidx &gt; 0; xidx-=2,zidx-=4) {
 9206   //    carry = 0;
 9207   //    long op2 = x[xidx:xidx+1];
 9208   //    for (int j=xidx-2,k=zidx; j &gt;= 0; j-=2) {
 9209   //       k -= 2;
 9210   //       long op1 = x[j:j+1];
 9211   //       long sum = z[k:k+1];
 9212   //       carry:sum = multiply_add_64(sum, op1, op2, carry, tmp_regs);
 9213   //       z[k:k+1] = sum;
 9214   //    }
 9215   //    add_one_64(z, k, carry, tmp_regs);
 9216   // }
 9217 
 9218   const Register carry = tmp5;
 9219   const Register sum = tmp3;
 9220   const Register op1 = tmp4;
 9221   Register op2 = tmp2;
 9222 
 9223   push(zlen);
 9224   push(len);
 9225   addl(zlen,2);
 9226   bind(L_second_loop);
 9227   xorq(carry, carry);
 9228   subl(zlen, 4);
 9229   subl(len, 2);
 9230   push(zlen);
 9231   push(len);
 9232   cmpl(len, 0);
 9233   jccb(Assembler::lessEqual, L_second_loop_exit);
 9234 
 9235   // Multiply an array by one 64 bit long.
 9236   if (UseBMI2Instructions) {
 9237     op2 = rdxReg;
 9238     movq(op2, Address(x, len, Address::times_4,  0));
 9239     rorxq(op2, op2, 32);
 9240   }
 9241   else {
 9242     movq(op2, Address(x, len, Address::times_4,  0));
 9243     rorq(op2, 32);
 9244   }
 9245 
 9246   bind(L_third_loop);
 9247   decrementl(len);
 9248   jccb(Assembler::negative, L_third_loop_exit);
 9249   decrementl(len);
 9250   jccb(Assembler::negative, L_last_x);
 9251 
 9252   movq(op1, Address(x, len, Address::times_4,  0));
 9253   rorq(op1, 32);
 9254 
 9255   bind(L_multiply);
 9256   subl(zlen, 2);
 9257   movq(sum, Address(z, zlen, Address::times_4,  0));
 9258 
 9259   // Multiply 64 bit by 64 bit and add 64 bits lower half and upper 64 bits as carry.
 9260   if (UseBMI2Instructions) {
 9261     multiply_add_64_bmi2(sum, op1, op2, carry, tmp2);
 9262   }
 9263   else {
 9264     multiply_add_64(sum, op1, op2, carry, rdxReg, raxReg);
 9265   }
 9266 
 9267   movq(Address(z, zlen, Address::times_4, 0), sum);
 9268 
 9269   jmp(L_third_loop);
 9270   bind(L_third_loop_exit);
 9271 
 9272   // Fourth loop
 9273   // Add 64 bit long carry into z with carry propogation.
 9274   // Uses offsetted zlen.
 9275   add_one_64(z, zlen, carry, tmp1);
 9276 
 9277   pop(len);
 9278   pop(zlen);
 9279   jmp(L_second_loop);
 9280 
 9281   // Next infrequent code is moved outside loops.
 9282   bind(L_last_x);
 9283   movl(op1, Address(x, 0));
 9284   jmp(L_multiply);
 9285 
 9286   bind(L_second_loop_exit);
 9287   pop(len);
 9288   pop(zlen);
 9289   pop(len);
 9290   pop(zlen);
 9291 
 9292   // Fifth loop
 9293   // Shift z left 1 bit.
 9294   lshift_by_1(x, len, z, zlen, tmp1, tmp2, tmp3, tmp4);
 9295 
 9296   // z[zlen-1] |= x[len-1] &amp; 1;
 9297   movl(tmp3, Address(x, len, Address::times_4, -4));
 9298   andl(tmp3, 1);
 9299   orl(Address(z, zlen, Address::times_4,  -4), tmp3);
 9300 
 9301   pop(tmp5);
 9302   pop(tmp4);
 9303   pop(tmp3);
 9304   pop(tmp2);
 9305   pop(tmp1);
 9306 }
 9307 
 9308 /**
 9309  * Helper function for mul_add()
 9310  * Multiply the in[] by int k and add to out[] starting at offset offs using
 9311  * 128 bit by 32 bit multiply and return the carry in tmp5.
 9312  * Only quad int aligned length of in[] is operated on in this function.
 9313  * k is in rdxReg for BMI2Instructions, for others it is in tmp2.
 9314  * This function preserves out, in and k registers.
 9315  * len and offset point to the appropriate index in &quot;in&quot; &amp; &quot;out&quot; correspondingly
 9316  * tmp5 has the carry.
 9317  * other registers are temporary and are modified.
 9318  *
 9319  */
 9320 void MacroAssembler::mul_add_128_x_32_loop(Register out, Register in,
 9321   Register offset, Register len, Register tmp1, Register tmp2, Register tmp3,
 9322   Register tmp4, Register tmp5, Register rdxReg, Register raxReg) {
 9323 
 9324   Label L_first_loop, L_first_loop_exit;
 9325 
 9326   movl(tmp1, len);
 9327   shrl(tmp1, 2);
 9328 
 9329   bind(L_first_loop);
 9330   subl(tmp1, 1);
 9331   jccb(Assembler::negative, L_first_loop_exit);
 9332 
 9333   subl(len, 4);
 9334   subl(offset, 4);
 9335 
 9336   Register op2 = tmp2;
 9337   const Register sum = tmp3;
 9338   const Register op1 = tmp4;
 9339   const Register carry = tmp5;
 9340 
 9341   if (UseBMI2Instructions) {
 9342     op2 = rdxReg;
 9343   }
 9344 
 9345   movq(op1, Address(in, len, Address::times_4,  8));
 9346   rorq(op1, 32);
 9347   movq(sum, Address(out, offset, Address::times_4,  8));
 9348   rorq(sum, 32);
 9349   if (UseBMI2Instructions) {
 9350     multiply_add_64_bmi2(sum, op1, op2, carry, raxReg);
 9351   }
 9352   else {
 9353     multiply_add_64(sum, op1, op2, carry, rdxReg, raxReg);
 9354   }
 9355   // Store back in big endian from little endian
 9356   rorq(sum, 0x20);
 9357   movq(Address(out, offset, Address::times_4,  8), sum);
 9358 
 9359   movq(op1, Address(in, len, Address::times_4,  0));
 9360   rorq(op1, 32);
 9361   movq(sum, Address(out, offset, Address::times_4,  0));
 9362   rorq(sum, 32);
 9363   if (UseBMI2Instructions) {
 9364     multiply_add_64_bmi2(sum, op1, op2, carry, raxReg);
 9365   }
 9366   else {
 9367     multiply_add_64(sum, op1, op2, carry, rdxReg, raxReg);
 9368   }
 9369   // Store back in big endian from little endian
 9370   rorq(sum, 0x20);
 9371   movq(Address(out, offset, Address::times_4,  0), sum);
 9372 
 9373   jmp(L_first_loop);
 9374   bind(L_first_loop_exit);
 9375 }
 9376 
 9377 /**
 9378  * Code for BigInteger::mulAdd() intrinsic
 9379  *
 9380  * rdi: out
 9381  * rsi: in
 9382  * r11: offs (out.length - offset)
 9383  * rcx: len
 9384  * r8:  k
 9385  * r12: tmp1
 9386  * r13: tmp2
 9387  * r14: tmp3
 9388  * r15: tmp4
 9389  * rbx: tmp5
 9390  * Multiply the in[] by word k and add to out[], return the carry in rax
 9391  */
 9392 void MacroAssembler::mul_add(Register out, Register in, Register offs,
 9393    Register len, Register k, Register tmp1, Register tmp2, Register tmp3,
 9394    Register tmp4, Register tmp5, Register rdxReg, Register raxReg) {
 9395 
 9396   Label L_carry, L_last_in, L_done;
 9397 
 9398 // carry = 0;
 9399 // for (int j=len-1; j &gt;= 0; j--) {
 9400 //    long product = (in[j] &amp; LONG_MASK) * kLong +
 9401 //                   (out[offs] &amp; LONG_MASK) + carry;
 9402 //    out[offs--] = (int)product;
 9403 //    carry = product &gt;&gt;&gt; 32;
 9404 // }
 9405 //
 9406   push(tmp1);
 9407   push(tmp2);
 9408   push(tmp3);
 9409   push(tmp4);
 9410   push(tmp5);
 9411 
 9412   Register op2 = tmp2;
 9413   const Register sum = tmp3;
 9414   const Register op1 = tmp4;
 9415   const Register carry =  tmp5;
 9416 
 9417   if (UseBMI2Instructions) {
 9418     op2 = rdxReg;
 9419     movl(op2, k);
 9420   }
 9421   else {
 9422     movl(op2, k);
 9423   }
 9424 
 9425   xorq(carry, carry);
 9426 
 9427   //First loop
 9428 
 9429   //Multiply in[] by k in a 4 way unrolled loop using 128 bit by 32 bit multiply
 9430   //The carry is in tmp5
 9431   mul_add_128_x_32_loop(out, in, offs, len, tmp1, tmp2, tmp3, tmp4, tmp5, rdxReg, raxReg);
 9432 
 9433   //Multiply the trailing in[] entry using 64 bit by 32 bit, if any
 9434   decrementl(len);
 9435   jccb(Assembler::negative, L_carry);
 9436   decrementl(len);
 9437   jccb(Assembler::negative, L_last_in);
 9438 
 9439   movq(op1, Address(in, len, Address::times_4,  0));
 9440   rorq(op1, 32);
 9441 
 9442   subl(offs, 2);
 9443   movq(sum, Address(out, offs, Address::times_4,  0));
 9444   rorq(sum, 32);
 9445 
 9446   if (UseBMI2Instructions) {
 9447     multiply_add_64_bmi2(sum, op1, op2, carry, raxReg);
 9448   }
 9449   else {
 9450     multiply_add_64(sum, op1, op2, carry, rdxReg, raxReg);
 9451   }
 9452 
 9453   // Store back in big endian from little endian
 9454   rorq(sum, 0x20);
 9455   movq(Address(out, offs, Address::times_4,  0), sum);
 9456 
 9457   testl(len, len);
 9458   jccb(Assembler::zero, L_carry);
 9459 
 9460   //Multiply the last in[] entry, if any
 9461   bind(L_last_in);
 9462   movl(op1, Address(in, 0));
 9463   movl(sum, Address(out, offs, Address::times_4,  -4));
 9464 
 9465   movl(raxReg, k);
 9466   mull(op1); //tmp4 * eax -&gt; edx:eax
 9467   addl(sum, carry);
 9468   adcl(rdxReg, 0);
 9469   addl(sum, raxReg);
 9470   adcl(rdxReg, 0);
 9471   movl(carry, rdxReg);
 9472 
 9473   movl(Address(out, offs, Address::times_4,  -4), sum);
 9474 
 9475   bind(L_carry);
 9476   //return tmp5/carry as carry in rax
 9477   movl(rax, carry);
 9478 
 9479   bind(L_done);
 9480   pop(tmp5);
 9481   pop(tmp4);
 9482   pop(tmp3);
 9483   pop(tmp2);
 9484   pop(tmp1);
 9485 }
 9486 #endif
 9487 
 9488 /**
 9489  * Emits code to update CRC-32 with a byte value according to constants in table
 9490  *
 9491  * @param [in,out]crc   Register containing the crc.
 9492  * @param [in]val       Register containing the byte to fold into the CRC.
 9493  * @param [in]table     Register containing the table of crc constants.
 9494  *
 9495  * uint32_t crc;
 9496  * val = crc_table[(val ^ crc) &amp; 0xFF];
 9497  * crc = val ^ (crc &gt;&gt; 8);
 9498  *
 9499  */
 9500 void MacroAssembler::update_byte_crc32(Register crc, Register val, Register table) {
 9501   xorl(val, crc);
 9502   andl(val, 0xFF);
 9503   shrl(crc, 8); // unsigned shift
 9504   xorl(crc, Address(table, val, Address::times_4, 0));
 9505 }
 9506 
 9507 /**
 9508 * Fold four 128-bit data chunks
 9509 */
 9510 void MacroAssembler::fold_128bit_crc32_avx512(XMMRegister xcrc, XMMRegister xK, XMMRegister xtmp, Register buf, int offset) {
 9511   evpclmulhdq(xtmp, xK, xcrc, Assembler::AVX_512bit); // [123:64]
 9512   evpclmulldq(xcrc, xK, xcrc, Assembler::AVX_512bit); // [63:0]
 9513   evpxorq(xcrc, xcrc, Address(buf, offset), Assembler::AVX_512bit /* vector_len */);
 9514   evpxorq(xcrc, xcrc, xtmp, Assembler::AVX_512bit /* vector_len */);
 9515 }
 9516 
 9517 /**
 9518  * Fold 128-bit data chunk
 9519  */
 9520 void MacroAssembler::fold_128bit_crc32(XMMRegister xcrc, XMMRegister xK, XMMRegister xtmp, Register buf, int offset) {
 9521   if (UseAVX &gt; 0) {
 9522     vpclmulhdq(xtmp, xK, xcrc); // [123:64]
 9523     vpclmulldq(xcrc, xK, xcrc); // [63:0]
 9524     vpxor(xcrc, xcrc, Address(buf, offset), 0 /* vector_len */);
 9525     pxor(xcrc, xtmp);
 9526   } else {
 9527     movdqa(xtmp, xcrc);
 9528     pclmulhdq(xtmp, xK);   // [123:64]
 9529     pclmulldq(xcrc, xK);   // [63:0]
 9530     pxor(xcrc, xtmp);
 9531     movdqu(xtmp, Address(buf, offset));
 9532     pxor(xcrc, xtmp);
 9533   }
 9534 }
 9535 
 9536 void MacroAssembler::fold_128bit_crc32(XMMRegister xcrc, XMMRegister xK, XMMRegister xtmp, XMMRegister xbuf) {
 9537   if (UseAVX &gt; 0) {
 9538     vpclmulhdq(xtmp, xK, xcrc);
 9539     vpclmulldq(xcrc, xK, xcrc);
 9540     pxor(xcrc, xbuf);
 9541     pxor(xcrc, xtmp);
 9542   } else {
 9543     movdqa(xtmp, xcrc);
 9544     pclmulhdq(xtmp, xK);
 9545     pclmulldq(xcrc, xK);
 9546     pxor(xcrc, xbuf);
 9547     pxor(xcrc, xtmp);
 9548   }
 9549 }
 9550 
 9551 /**
 9552  * 8-bit folds to compute 32-bit CRC
 9553  *
 9554  * uint64_t xcrc;
 9555  * timesXtoThe32[xcrc &amp; 0xFF] ^ (xcrc &gt;&gt; 8);
 9556  */
 9557 void MacroAssembler::fold_8bit_crc32(XMMRegister xcrc, Register table, XMMRegister xtmp, Register tmp) {
 9558   movdl(tmp, xcrc);
 9559   andl(tmp, 0xFF);
 9560   movdl(xtmp, Address(table, tmp, Address::times_4, 0));
 9561   psrldq(xcrc, 1); // unsigned shift one byte
 9562   pxor(xcrc, xtmp);
 9563 }
 9564 
 9565 /**
 9566  * uint32_t crc;
 9567  * timesXtoThe32[crc &amp; 0xFF] ^ (crc &gt;&gt; 8);
 9568  */
 9569 void MacroAssembler::fold_8bit_crc32(Register crc, Register table, Register tmp) {
 9570   movl(tmp, crc);
 9571   andl(tmp, 0xFF);
 9572   shrl(crc, 8);
 9573   xorl(crc, Address(table, tmp, Address::times_4, 0));
 9574 }
 9575 
 9576 /**
 9577  * @param crc   register containing existing CRC (32-bit)
 9578  * @param buf   register pointing to input byte buffer (byte*)
 9579  * @param len   register containing number of bytes
 9580  * @param table register that will contain address of CRC table
 9581  * @param tmp   scratch register
 9582  */
 9583 void MacroAssembler::kernel_crc32(Register crc, Register buf, Register len, Register table, Register tmp) {
 9584   assert_different_registers(crc, buf, len, table, tmp, rax);
 9585 
 9586   Label L_tail, L_tail_restore, L_tail_loop, L_exit, L_align_loop, L_aligned;
 9587   Label L_fold_tail, L_fold_128b, L_fold_512b, L_fold_512b_loop, L_fold_tail_loop;
 9588 
 9589   // For EVEX with VL and BW, provide a standard mask, VL = 128 will guide the merge
 9590   // context for the registers used, where all instructions below are using 128-bit mode
 9591   // On EVEX without VL and BW, these instructions will all be AVX.
 9592   lea(table, ExternalAddress(StubRoutines::crc_table_addr()));
 9593   notl(crc); // ~crc
 9594   cmpl(len, 16);
 9595   jcc(Assembler::less, L_tail);
 9596 
 9597   // Align buffer to 16 bytes
 9598   movl(tmp, buf);
 9599   andl(tmp, 0xF);
 9600   jccb(Assembler::zero, L_aligned);
 9601   subl(tmp,  16);
 9602   addl(len, tmp);
 9603 
 9604   align(4);
 9605   BIND(L_align_loop);
 9606   movsbl(rax, Address(buf, 0)); // load byte with sign extension
 9607   update_byte_crc32(crc, rax, table);
 9608   increment(buf);
 9609   incrementl(tmp);
 9610   jccb(Assembler::less, L_align_loop);
 9611 
 9612   BIND(L_aligned);
 9613   movl(tmp, len); // save
 9614   shrl(len, 4);
 9615   jcc(Assembler::zero, L_tail_restore);
 9616 
 9617   // Fold crc into first bytes of vector
 9618   movdqa(xmm1, Address(buf, 0));
 9619   movdl(rax, xmm1);
 9620   xorl(crc, rax);
 9621   if (VM_Version::supports_sse4_1()) {
 9622     pinsrd(xmm1, crc, 0);
 9623   } else {
 9624     pinsrw(xmm1, crc, 0);
 9625     shrl(crc, 16);
 9626     pinsrw(xmm1, crc, 1);
 9627   }
 9628   addptr(buf, 16);
 9629   subl(len, 4); // len &gt; 0
 9630   jcc(Assembler::less, L_fold_tail);
 9631 
 9632   movdqa(xmm2, Address(buf,  0));
 9633   movdqa(xmm3, Address(buf, 16));
 9634   movdqa(xmm4, Address(buf, 32));
 9635   addptr(buf, 48);
 9636   subl(len, 3);
 9637   jcc(Assembler::lessEqual, L_fold_512b);
 9638 
 9639   // Fold total 512 bits of polynomial on each iteration,
 9640   // 128 bits per each of 4 parallel streams.
 9641   movdqu(xmm0, ExternalAddress(StubRoutines::x86::crc_by128_masks_addr() + 32));
 9642 
 9643   align(32);
 9644   BIND(L_fold_512b_loop);
 9645   fold_128bit_crc32(xmm1, xmm0, xmm5, buf,  0);
 9646   fold_128bit_crc32(xmm2, xmm0, xmm5, buf, 16);
 9647   fold_128bit_crc32(xmm3, xmm0, xmm5, buf, 32);
 9648   fold_128bit_crc32(xmm4, xmm0, xmm5, buf, 48);
 9649   addptr(buf, 64);
 9650   subl(len, 4);
 9651   jcc(Assembler::greater, L_fold_512b_loop);
 9652 
 9653   // Fold 512 bits to 128 bits.
 9654   BIND(L_fold_512b);
 9655   movdqu(xmm0, ExternalAddress(StubRoutines::x86::crc_by128_masks_addr() + 16));
 9656   fold_128bit_crc32(xmm1, xmm0, xmm5, xmm2);
 9657   fold_128bit_crc32(xmm1, xmm0, xmm5, xmm3);
 9658   fold_128bit_crc32(xmm1, xmm0, xmm5, xmm4);
 9659 
 9660   // Fold the rest of 128 bits data chunks
 9661   BIND(L_fold_tail);
 9662   addl(len, 3);
 9663   jccb(Assembler::lessEqual, L_fold_128b);
 9664   movdqu(xmm0, ExternalAddress(StubRoutines::x86::crc_by128_masks_addr() + 16));
 9665 
 9666   BIND(L_fold_tail_loop);
 9667   fold_128bit_crc32(xmm1, xmm0, xmm5, buf,  0);
 9668   addptr(buf, 16);
 9669   decrementl(len);
 9670   jccb(Assembler::greater, L_fold_tail_loop);
 9671 
 9672   // Fold 128 bits in xmm1 down into 32 bits in crc register.
 9673   BIND(L_fold_128b);
 9674   movdqu(xmm0, ExternalAddress(StubRoutines::x86::crc_by128_masks_addr()));
 9675   if (UseAVX &gt; 0) {
 9676     vpclmulqdq(xmm2, xmm0, xmm1, 0x1);
 9677     vpand(xmm3, xmm0, xmm2, 0 /* vector_len */);
 9678     vpclmulqdq(xmm0, xmm0, xmm3, 0x1);
 9679   } else {
 9680     movdqa(xmm2, xmm0);
 9681     pclmulqdq(xmm2, xmm1, 0x1);
 9682     movdqa(xmm3, xmm0);
 9683     pand(xmm3, xmm2);
 9684     pclmulqdq(xmm0, xmm3, 0x1);
 9685   }
 9686   psrldq(xmm1, 8);
 9687   psrldq(xmm2, 4);
 9688   pxor(xmm0, xmm1);
 9689   pxor(xmm0, xmm2);
 9690 
 9691   // 8 8-bit folds to compute 32-bit CRC.
 9692   for (int j = 0; j &lt; 4; j++) {
 9693     fold_8bit_crc32(xmm0, table, xmm1, rax);
 9694   }
 9695   movdl(crc, xmm0); // mov 32 bits to general register
 9696   for (int j = 0; j &lt; 4; j++) {
 9697     fold_8bit_crc32(crc, table, rax);
 9698   }
 9699 
 9700   BIND(L_tail_restore);
 9701   movl(len, tmp); // restore
 9702   BIND(L_tail);
 9703   andl(len, 0xf);
 9704   jccb(Assembler::zero, L_exit);
 9705 
 9706   // Fold the rest of bytes
 9707   align(4);
 9708   BIND(L_tail_loop);
 9709   movsbl(rax, Address(buf, 0)); // load byte with sign extension
 9710   update_byte_crc32(crc, rax, table);
 9711   increment(buf);
 9712   decrementl(len);
 9713   jccb(Assembler::greater, L_tail_loop);
 9714 
 9715   BIND(L_exit);
 9716   notl(crc); // ~c
 9717 }
 9718 
 9719 #ifdef _LP64
 9720 // S. Gueron / Information Processing Letters 112 (2012) 184
 9721 // Algorithm 4: Computing carry-less multiplication using a precomputed lookup table.
 9722 // Input: A 32 bit value B = [byte3, byte2, byte1, byte0].
 9723 // Output: the 64-bit carry-less product of B * CONST
 9724 void MacroAssembler::crc32c_ipl_alg4(Register in, uint32_t n,
 9725                                      Register tmp1, Register tmp2, Register tmp3) {
 9726   lea(tmp3, ExternalAddress(StubRoutines::crc32c_table_addr()));
 9727   if (n &gt; 0) {
 9728     addq(tmp3, n * 256 * 8);
 9729   }
 9730   //    Q1 = TABLEExt[n][B &amp; 0xFF];
 9731   movl(tmp1, in);
 9732   andl(tmp1, 0x000000FF);
 9733   shll(tmp1, 3);
 9734   addq(tmp1, tmp3);
 9735   movq(tmp1, Address(tmp1, 0));
 9736 
 9737   //    Q2 = TABLEExt[n][B &gt;&gt; 8 &amp; 0xFF];
 9738   movl(tmp2, in);
 9739   shrl(tmp2, 8);
 9740   andl(tmp2, 0x000000FF);
 9741   shll(tmp2, 3);
 9742   addq(tmp2, tmp3);
 9743   movq(tmp2, Address(tmp2, 0));
 9744 
 9745   shlq(tmp2, 8);
 9746   xorq(tmp1, tmp2);
 9747 
 9748   //    Q3 = TABLEExt[n][B &gt;&gt; 16 &amp; 0xFF];
 9749   movl(tmp2, in);
 9750   shrl(tmp2, 16);
 9751   andl(tmp2, 0x000000FF);
 9752   shll(tmp2, 3);
 9753   addq(tmp2, tmp3);
 9754   movq(tmp2, Address(tmp2, 0));
 9755 
 9756   shlq(tmp2, 16);
 9757   xorq(tmp1, tmp2);
 9758 
 9759   //    Q4 = TABLEExt[n][B &gt;&gt; 24 &amp; 0xFF];
 9760   shrl(in, 24);
 9761   andl(in, 0x000000FF);
 9762   shll(in, 3);
 9763   addq(in, tmp3);
 9764   movq(in, Address(in, 0));
 9765 
 9766   shlq(in, 24);
 9767   xorq(in, tmp1);
 9768   //    return Q1 ^ Q2 &lt;&lt; 8 ^ Q3 &lt;&lt; 16 ^ Q4 &lt;&lt; 24;
 9769 }
 9770 
 9771 void MacroAssembler::crc32c_pclmulqdq(XMMRegister w_xtmp1,
 9772                                       Register in_out,
 9773                                       uint32_t const_or_pre_comp_const_index, bool is_pclmulqdq_supported,
 9774                                       XMMRegister w_xtmp2,
 9775                                       Register tmp1,
 9776                                       Register n_tmp2, Register n_tmp3) {
 9777   if (is_pclmulqdq_supported) {
 9778     movdl(w_xtmp1, in_out); // modified blindly
 9779 
 9780     movl(tmp1, const_or_pre_comp_const_index);
 9781     movdl(w_xtmp2, tmp1);
 9782     pclmulqdq(w_xtmp1, w_xtmp2, 0);
 9783 
 9784     movdq(in_out, w_xtmp1);
 9785   } else {
 9786     crc32c_ipl_alg4(in_out, const_or_pre_comp_const_index, tmp1, n_tmp2, n_tmp3);
 9787   }
 9788 }
 9789 
 9790 // Recombination Alternative 2: No bit-reflections
 9791 // T1 = (CRC_A * U1) &lt;&lt; 1
 9792 // T2 = (CRC_B * U2) &lt;&lt; 1
 9793 // C1 = T1 &gt;&gt; 32
 9794 // C2 = T2 &gt;&gt; 32
 9795 // T1 = T1 &amp; 0xFFFFFFFF
 9796 // T2 = T2 &amp; 0xFFFFFFFF
 9797 // T1 = CRC32(0, T1)
 9798 // T2 = CRC32(0, T2)
 9799 // C1 = C1 ^ T1
 9800 // C2 = C2 ^ T2
 9801 // CRC = C1 ^ C2 ^ CRC_C
 9802 void MacroAssembler::crc32c_rec_alt2(uint32_t const_or_pre_comp_const_index_u1, uint32_t const_or_pre_comp_const_index_u2, bool is_pclmulqdq_supported, Register in_out, Register in1, Register in2,
 9803                                      XMMRegister w_xtmp1, XMMRegister w_xtmp2, XMMRegister w_xtmp3,
 9804                                      Register tmp1, Register tmp2,
 9805                                      Register n_tmp3) {
 9806   crc32c_pclmulqdq(w_xtmp1, in_out, const_or_pre_comp_const_index_u1, is_pclmulqdq_supported, w_xtmp3, tmp1, tmp2, n_tmp3);
 9807   crc32c_pclmulqdq(w_xtmp2, in1, const_or_pre_comp_const_index_u2, is_pclmulqdq_supported, w_xtmp3, tmp1, tmp2, n_tmp3);
 9808   shlq(in_out, 1);
 9809   movl(tmp1, in_out);
 9810   shrq(in_out, 32);
 9811   xorl(tmp2, tmp2);
 9812   crc32(tmp2, tmp1, 4);
 9813   xorl(in_out, tmp2); // we don&#39;t care about upper 32 bit contents here
 9814   shlq(in1, 1);
 9815   movl(tmp1, in1);
 9816   shrq(in1, 32);
 9817   xorl(tmp2, tmp2);
 9818   crc32(tmp2, tmp1, 4);
 9819   xorl(in1, tmp2);
 9820   xorl(in_out, in1);
 9821   xorl(in_out, in2);
 9822 }
 9823 
 9824 // Set N to predefined value
 9825 // Subtract from a lenght of a buffer
 9826 // execute in a loop:
 9827 // CRC_A = 0xFFFFFFFF, CRC_B = 0, CRC_C = 0
 9828 // for i = 1 to N do
 9829 //  CRC_A = CRC32(CRC_A, A[i])
 9830 //  CRC_B = CRC32(CRC_B, B[i])
 9831 //  CRC_C = CRC32(CRC_C, C[i])
 9832 // end for
 9833 // Recombine
 9834 void MacroAssembler::crc32c_proc_chunk(uint32_t size, uint32_t const_or_pre_comp_const_index_u1, uint32_t const_or_pre_comp_const_index_u2, bool is_pclmulqdq_supported,
 9835                                        Register in_out1, Register in_out2, Register in_out3,
 9836                                        Register tmp1, Register tmp2, Register tmp3,
 9837                                        XMMRegister w_xtmp1, XMMRegister w_xtmp2, XMMRegister w_xtmp3,
 9838                                        Register tmp4, Register tmp5,
 9839                                        Register n_tmp6) {
 9840   Label L_processPartitions;
 9841   Label L_processPartition;
 9842   Label L_exit;
 9843 
 9844   bind(L_processPartitions);
 9845   cmpl(in_out1, 3 * size);
 9846   jcc(Assembler::less, L_exit);
 9847     xorl(tmp1, tmp1);
 9848     xorl(tmp2, tmp2);
 9849     movq(tmp3, in_out2);
 9850     addq(tmp3, size);
 9851 
 9852     bind(L_processPartition);
 9853       crc32(in_out3, Address(in_out2, 0), 8);
 9854       crc32(tmp1, Address(in_out2, size), 8);
 9855       crc32(tmp2, Address(in_out2, size * 2), 8);
 9856       addq(in_out2, 8);
 9857       cmpq(in_out2, tmp3);
 9858       jcc(Assembler::less, L_processPartition);
 9859     crc32c_rec_alt2(const_or_pre_comp_const_index_u1, const_or_pre_comp_const_index_u2, is_pclmulqdq_supported, in_out3, tmp1, tmp2,
 9860             w_xtmp1, w_xtmp2, w_xtmp3,
 9861             tmp4, tmp5,
 9862             n_tmp6);
 9863     addq(in_out2, 2 * size);
 9864     subl(in_out1, 3 * size);
 9865     jmp(L_processPartitions);
 9866 
 9867   bind(L_exit);
 9868 }
 9869 #else
 9870 void MacroAssembler::crc32c_ipl_alg4(Register in_out, uint32_t n,
 9871                                      Register tmp1, Register tmp2, Register tmp3,
 9872                                      XMMRegister xtmp1, XMMRegister xtmp2) {
 9873   lea(tmp3, ExternalAddress(StubRoutines::crc32c_table_addr()));
 9874   if (n &gt; 0) {
 9875     addl(tmp3, n * 256 * 8);
 9876   }
 9877   //    Q1 = TABLEExt[n][B &amp; 0xFF];
 9878   movl(tmp1, in_out);
 9879   andl(tmp1, 0x000000FF);
 9880   shll(tmp1, 3);
 9881   addl(tmp1, tmp3);
 9882   movq(xtmp1, Address(tmp1, 0));
 9883 
 9884   //    Q2 = TABLEExt[n][B &gt;&gt; 8 &amp; 0xFF];
 9885   movl(tmp2, in_out);
 9886   shrl(tmp2, 8);
 9887   andl(tmp2, 0x000000FF);
 9888   shll(tmp2, 3);
 9889   addl(tmp2, tmp3);
 9890   movq(xtmp2, Address(tmp2, 0));
 9891 
 9892   psllq(xtmp2, 8);
 9893   pxor(xtmp1, xtmp2);
 9894 
 9895   //    Q3 = TABLEExt[n][B &gt;&gt; 16 &amp; 0xFF];
 9896   movl(tmp2, in_out);
 9897   shrl(tmp2, 16);
 9898   andl(tmp2, 0x000000FF);
 9899   shll(tmp2, 3);
 9900   addl(tmp2, tmp3);
 9901   movq(xtmp2, Address(tmp2, 0));
 9902 
 9903   psllq(xtmp2, 16);
 9904   pxor(xtmp1, xtmp2);
 9905 
 9906   //    Q4 = TABLEExt[n][B &gt;&gt; 24 &amp; 0xFF];
 9907   shrl(in_out, 24);
 9908   andl(in_out, 0x000000FF);
 9909   shll(in_out, 3);
 9910   addl(in_out, tmp3);
 9911   movq(xtmp2, Address(in_out, 0));
 9912 
 9913   psllq(xtmp2, 24);
 9914   pxor(xtmp1, xtmp2); // Result in CXMM
 9915   //    return Q1 ^ Q2 &lt;&lt; 8 ^ Q3 &lt;&lt; 16 ^ Q4 &lt;&lt; 24;
 9916 }
 9917 
 9918 void MacroAssembler::crc32c_pclmulqdq(XMMRegister w_xtmp1,
 9919                                       Register in_out,
 9920                                       uint32_t const_or_pre_comp_const_index, bool is_pclmulqdq_supported,
 9921                                       XMMRegister w_xtmp2,
 9922                                       Register tmp1,
 9923                                       Register n_tmp2, Register n_tmp3) {
 9924   if (is_pclmulqdq_supported) {
 9925     movdl(w_xtmp1, in_out);
 9926 
 9927     movl(tmp1, const_or_pre_comp_const_index);
 9928     movdl(w_xtmp2, tmp1);
 9929     pclmulqdq(w_xtmp1, w_xtmp2, 0);
 9930     // Keep result in XMM since GPR is 32 bit in length
 9931   } else {
 9932     crc32c_ipl_alg4(in_out, const_or_pre_comp_const_index, tmp1, n_tmp2, n_tmp3, w_xtmp1, w_xtmp2);
 9933   }
 9934 }
 9935 
 9936 void MacroAssembler::crc32c_rec_alt2(uint32_t const_or_pre_comp_const_index_u1, uint32_t const_or_pre_comp_const_index_u2, bool is_pclmulqdq_supported, Register in_out, Register in1, Register in2,
 9937                                      XMMRegister w_xtmp1, XMMRegister w_xtmp2, XMMRegister w_xtmp3,
 9938                                      Register tmp1, Register tmp2,
 9939                                      Register n_tmp3) {
 9940   crc32c_pclmulqdq(w_xtmp1, in_out, const_or_pre_comp_const_index_u1, is_pclmulqdq_supported, w_xtmp3, tmp1, tmp2, n_tmp3);
 9941   crc32c_pclmulqdq(w_xtmp2, in1, const_or_pre_comp_const_index_u2, is_pclmulqdq_supported, w_xtmp3, tmp1, tmp2, n_tmp3);
 9942 
 9943   psllq(w_xtmp1, 1);
 9944   movdl(tmp1, w_xtmp1);
 9945   psrlq(w_xtmp1, 32);
 9946   movdl(in_out, w_xtmp1);
 9947 
 9948   xorl(tmp2, tmp2);
 9949   crc32(tmp2, tmp1, 4);
 9950   xorl(in_out, tmp2);
 9951 
 9952   psllq(w_xtmp2, 1);
 9953   movdl(tmp1, w_xtmp2);
 9954   psrlq(w_xtmp2, 32);
 9955   movdl(in1, w_xtmp2);
 9956 
 9957   xorl(tmp2, tmp2);
 9958   crc32(tmp2, tmp1, 4);
 9959   xorl(in1, tmp2);
 9960   xorl(in_out, in1);
 9961   xorl(in_out, in2);
 9962 }
 9963 
 9964 void MacroAssembler::crc32c_proc_chunk(uint32_t size, uint32_t const_or_pre_comp_const_index_u1, uint32_t const_or_pre_comp_const_index_u2, bool is_pclmulqdq_supported,
 9965                                        Register in_out1, Register in_out2, Register in_out3,
 9966                                        Register tmp1, Register tmp2, Register tmp3,
 9967                                        XMMRegister w_xtmp1, XMMRegister w_xtmp2, XMMRegister w_xtmp3,
 9968                                        Register tmp4, Register tmp5,
 9969                                        Register n_tmp6) {
 9970   Label L_processPartitions;
 9971   Label L_processPartition;
 9972   Label L_exit;
 9973 
 9974   bind(L_processPartitions);
 9975   cmpl(in_out1, 3 * size);
 9976   jcc(Assembler::less, L_exit);
 9977     xorl(tmp1, tmp1);
 9978     xorl(tmp2, tmp2);
 9979     movl(tmp3, in_out2);
 9980     addl(tmp3, size);
 9981 
 9982     bind(L_processPartition);
 9983       crc32(in_out3, Address(in_out2, 0), 4);
 9984       crc32(tmp1, Address(in_out2, size), 4);
 9985       crc32(tmp2, Address(in_out2, size*2), 4);
 9986       crc32(in_out3, Address(in_out2, 0+4), 4);
 9987       crc32(tmp1, Address(in_out2, size+4), 4);
 9988       crc32(tmp2, Address(in_out2, size*2+4), 4);
 9989       addl(in_out2, 8);
 9990       cmpl(in_out2, tmp3);
 9991       jcc(Assembler::less, L_processPartition);
 9992 
 9993         push(tmp3);
 9994         push(in_out1);
 9995         push(in_out2);
 9996         tmp4 = tmp3;
 9997         tmp5 = in_out1;
 9998         n_tmp6 = in_out2;
 9999 
10000       crc32c_rec_alt2(const_or_pre_comp_const_index_u1, const_or_pre_comp_const_index_u2, is_pclmulqdq_supported, in_out3, tmp1, tmp2,
10001             w_xtmp1, w_xtmp2, w_xtmp3,
10002             tmp4, tmp5,
10003             n_tmp6);
10004 
10005         pop(in_out2);
10006         pop(in_out1);
10007         pop(tmp3);
10008 
10009     addl(in_out2, 2 * size);
10010     subl(in_out1, 3 * size);
10011     jmp(L_processPartitions);
10012 
10013   bind(L_exit);
10014 }
10015 #endif //LP64
10016 
10017 #ifdef _LP64
10018 // Algorithm 2: Pipelined usage of the CRC32 instruction.
10019 // Input: A buffer I of L bytes.
10020 // Output: the CRC32C value of the buffer.
10021 // Notations:
10022 // Write L = 24N + r, with N = floor (L/24).
10023 // r = L mod 24 (0 &lt;= r &lt; 24).
10024 // Consider I as the concatenation of A|B|C|R, where A, B, C, each,
10025 // N quadwords, and R consists of r bytes.
10026 // A[j] = I [8j+7:8j], j= 0, 1, ..., N-1
10027 // B[j] = I [N + 8j+7:N + 8j], j= 0, 1, ..., N-1
10028 // C[j] = I [2N + 8j+7:2N + 8j], j= 0, 1, ..., N-1
10029 // if r &gt; 0 R[j] = I [3N +j], j= 0, 1, ...,r-1
10030 void MacroAssembler::crc32c_ipl_alg2_alt2(Register in_out, Register in1, Register in2,
10031                                           Register tmp1, Register tmp2, Register tmp3,
10032                                           Register tmp4, Register tmp5, Register tmp6,
10033                                           XMMRegister w_xtmp1, XMMRegister w_xtmp2, XMMRegister w_xtmp3,
10034                                           bool is_pclmulqdq_supported) {
10035   uint32_t const_or_pre_comp_const_index[CRC32C_NUM_PRECOMPUTED_CONSTANTS];
10036   Label L_wordByWord;
10037   Label L_byteByByteProlog;
10038   Label L_byteByByte;
10039   Label L_exit;
10040 
10041   if (is_pclmulqdq_supported ) {
10042     const_or_pre_comp_const_index[1] = *(uint32_t *)StubRoutines::_crc32c_table_addr;
10043     const_or_pre_comp_const_index[0] = *((uint32_t *)StubRoutines::_crc32c_table_addr+1);
10044 
10045     const_or_pre_comp_const_index[3] = *((uint32_t *)StubRoutines::_crc32c_table_addr + 2);
10046     const_or_pre_comp_const_index[2] = *((uint32_t *)StubRoutines::_crc32c_table_addr + 3);
10047 
10048     const_or_pre_comp_const_index[5] = *((uint32_t *)StubRoutines::_crc32c_table_addr + 4);
10049     const_or_pre_comp_const_index[4] = *((uint32_t *)StubRoutines::_crc32c_table_addr + 5);
10050     assert((CRC32C_NUM_PRECOMPUTED_CONSTANTS - 1 ) == 5, &quot;Checking whether you declared all of the constants based on the number of \&quot;chunks\&quot;&quot;);
10051   } else {
10052     const_or_pre_comp_const_index[0] = 1;
10053     const_or_pre_comp_const_index[1] = 0;
10054 
10055     const_or_pre_comp_const_index[2] = 3;
10056     const_or_pre_comp_const_index[3] = 2;
10057 
10058     const_or_pre_comp_const_index[4] = 5;
10059     const_or_pre_comp_const_index[5] = 4;
10060    }
10061   crc32c_proc_chunk(CRC32C_HIGH, const_or_pre_comp_const_index[0], const_or_pre_comp_const_index[1], is_pclmulqdq_supported,
10062                     in2, in1, in_out,
10063                     tmp1, tmp2, tmp3,
10064                     w_xtmp1, w_xtmp2, w_xtmp3,
10065                     tmp4, tmp5,
10066                     tmp6);
10067   crc32c_proc_chunk(CRC32C_MIDDLE, const_or_pre_comp_const_index[2], const_or_pre_comp_const_index[3], is_pclmulqdq_supported,
10068                     in2, in1, in_out,
10069                     tmp1, tmp2, tmp3,
10070                     w_xtmp1, w_xtmp2, w_xtmp3,
10071                     tmp4, tmp5,
10072                     tmp6);
10073   crc32c_proc_chunk(CRC32C_LOW, const_or_pre_comp_const_index[4], const_or_pre_comp_const_index[5], is_pclmulqdq_supported,
10074                     in2, in1, in_out,
10075                     tmp1, tmp2, tmp3,
10076                     w_xtmp1, w_xtmp2, w_xtmp3,
10077                     tmp4, tmp5,
10078                     tmp6);
10079   movl(tmp1, in2);
10080   andl(tmp1, 0x00000007);
10081   negl(tmp1);
10082   addl(tmp1, in2);
10083   addq(tmp1, in1);
10084 
10085   BIND(L_wordByWord);
10086   cmpq(in1, tmp1);
10087   jcc(Assembler::greaterEqual, L_byteByByteProlog);
10088     crc32(in_out, Address(in1, 0), 4);
10089     addq(in1, 4);
10090     jmp(L_wordByWord);
10091 
10092   BIND(L_byteByByteProlog);
10093   andl(in2, 0x00000007);
10094   movl(tmp2, 1);
10095 
10096   BIND(L_byteByByte);
10097   cmpl(tmp2, in2);
10098   jccb(Assembler::greater, L_exit);
10099     crc32(in_out, Address(in1, 0), 1);
10100     incq(in1);
10101     incl(tmp2);
10102     jmp(L_byteByByte);
10103 
10104   BIND(L_exit);
10105 }
10106 #else
10107 void MacroAssembler::crc32c_ipl_alg2_alt2(Register in_out, Register in1, Register in2,
10108                                           Register tmp1, Register  tmp2, Register tmp3,
10109                                           Register tmp4, Register  tmp5, Register tmp6,
10110                                           XMMRegister w_xtmp1, XMMRegister w_xtmp2, XMMRegister w_xtmp3,
10111                                           bool is_pclmulqdq_supported) {
10112   uint32_t const_or_pre_comp_const_index[CRC32C_NUM_PRECOMPUTED_CONSTANTS];
10113   Label L_wordByWord;
10114   Label L_byteByByteProlog;
10115   Label L_byteByByte;
10116   Label L_exit;
10117 
10118   if (is_pclmulqdq_supported) {
10119     const_or_pre_comp_const_index[1] = *(uint32_t *)StubRoutines::_crc32c_table_addr;
10120     const_or_pre_comp_const_index[0] = *((uint32_t *)StubRoutines::_crc32c_table_addr + 1);
10121 
10122     const_or_pre_comp_const_index[3] = *((uint32_t *)StubRoutines::_crc32c_table_addr + 2);
10123     const_or_pre_comp_const_index[2] = *((uint32_t *)StubRoutines::_crc32c_table_addr + 3);
10124 
10125     const_or_pre_comp_const_index[5] = *((uint32_t *)StubRoutines::_crc32c_table_addr + 4);
10126     const_or_pre_comp_const_index[4] = *((uint32_t *)StubRoutines::_crc32c_table_addr + 5);
10127   } else {
10128     const_or_pre_comp_const_index[0] = 1;
10129     const_or_pre_comp_const_index[1] = 0;
10130 
10131     const_or_pre_comp_const_index[2] = 3;
10132     const_or_pre_comp_const_index[3] = 2;
10133 
10134     const_or_pre_comp_const_index[4] = 5;
10135     const_or_pre_comp_const_index[5] = 4;
10136   }
10137   crc32c_proc_chunk(CRC32C_HIGH, const_or_pre_comp_const_index[0], const_or_pre_comp_const_index[1], is_pclmulqdq_supported,
10138                     in2, in1, in_out,
10139                     tmp1, tmp2, tmp3,
10140                     w_xtmp1, w_xtmp2, w_xtmp3,
10141                     tmp4, tmp5,
10142                     tmp6);
10143   crc32c_proc_chunk(CRC32C_MIDDLE, const_or_pre_comp_const_index[2], const_or_pre_comp_const_index[3], is_pclmulqdq_supported,
10144                     in2, in1, in_out,
10145                     tmp1, tmp2, tmp3,
10146                     w_xtmp1, w_xtmp2, w_xtmp3,
10147                     tmp4, tmp5,
10148                     tmp6);
10149   crc32c_proc_chunk(CRC32C_LOW, const_or_pre_comp_const_index[4], const_or_pre_comp_const_index[5], is_pclmulqdq_supported,
10150                     in2, in1, in_out,
10151                     tmp1, tmp2, tmp3,
10152                     w_xtmp1, w_xtmp2, w_xtmp3,
10153                     tmp4, tmp5,
10154                     tmp6);
10155   movl(tmp1, in2);
10156   andl(tmp1, 0x00000007);
10157   negl(tmp1);
10158   addl(tmp1, in2);
10159   addl(tmp1, in1);
10160 
10161   BIND(L_wordByWord);
10162   cmpl(in1, tmp1);
10163   jcc(Assembler::greaterEqual, L_byteByByteProlog);
10164     crc32(in_out, Address(in1,0), 4);
10165     addl(in1, 4);
10166     jmp(L_wordByWord);
10167 
10168   BIND(L_byteByByteProlog);
10169   andl(in2, 0x00000007);
10170   movl(tmp2, 1);
10171 
10172   BIND(L_byteByByte);
10173   cmpl(tmp2, in2);
10174   jccb(Assembler::greater, L_exit);
10175     movb(tmp1, Address(in1, 0));
10176     crc32(in_out, tmp1, 1);
10177     incl(in1);
10178     incl(tmp2);
10179     jmp(L_byteByByte);
10180 
10181   BIND(L_exit);
10182 }
10183 #endif // LP64
10184 #undef BIND
10185 #undef BLOCK_COMMENT
10186 
10187 // Compress char[] array to byte[].
10188 //   ..\jdk\src\java.base\share\classes\java\lang\StringUTF16.java
10189 //   @HotSpotIntrinsicCandidate
10190 //   private static int compress(char[] src, int srcOff, byte[] dst, int dstOff, int len) {
10191 //     for (int i = 0; i &lt; len; i++) {
10192 //       int c = src[srcOff++];
10193 //       if (c &gt;&gt;&gt; 8 != 0) {
10194 //         return 0;
10195 //       }
10196 //       dst[dstOff++] = (byte)c;
10197 //     }
10198 //     return len;
10199 //   }
10200 void MacroAssembler::char_array_compress(Register src, Register dst, Register len,
10201   XMMRegister tmp1Reg, XMMRegister tmp2Reg,
10202   XMMRegister tmp3Reg, XMMRegister tmp4Reg,
10203   Register tmp5, Register result) {
10204   Label copy_chars_loop, return_length, return_zero, done;
10205 
10206   // rsi: src
10207   // rdi: dst
10208   // rdx: len
10209   // rcx: tmp5
10210   // rax: result
10211 
10212   // rsi holds start addr of source char[] to be compressed
10213   // rdi holds start addr of destination byte[]
10214   // rdx holds length
10215 
10216   assert(len != result, &quot;&quot;);
10217 
10218   // save length for return
10219   push(len);
10220 
10221   if ((AVX3Threshold == 0) &amp;&amp; (UseAVX &gt; 2) &amp;&amp; // AVX512
10222     VM_Version::supports_avx512vlbw() &amp;&amp;
10223     VM_Version::supports_bmi2()) {
10224 
10225     Label copy_32_loop, copy_loop_tail, below_threshold;
10226 
10227     // alignment
10228     Label post_alignment;
10229 
10230     // if length of the string is less than 16, handle it in an old fashioned way
10231     testl(len, -32);
10232     jcc(Assembler::zero, below_threshold);
10233 
10234     // First check whether a character is compressable ( &lt;= 0xFF).
10235     // Create mask to test for Unicode chars inside zmm vector
10236     movl(result, 0x00FF);
10237     evpbroadcastw(tmp2Reg, result, Assembler::AVX_512bit);
10238 
10239     testl(len, -64);
10240     jcc(Assembler::zero, post_alignment);
10241 
10242     movl(tmp5, dst);
10243     andl(tmp5, (32 - 1));
10244     negl(tmp5);
10245     andl(tmp5, (32 - 1));
10246 
10247     // bail out when there is nothing to be done
10248     testl(tmp5, 0xFFFFFFFF);
10249     jcc(Assembler::zero, post_alignment);
10250 
10251     // ~(~0 &lt;&lt; len), where len is the # of remaining elements to process
10252     movl(result, 0xFFFFFFFF);
10253     shlxl(result, result, tmp5);
10254     notl(result);
10255     kmovdl(k3, result);
10256 
10257     evmovdquw(tmp1Reg, k3, Address(src, 0), Assembler::AVX_512bit);
10258     evpcmpuw(k2, k3, tmp1Reg, tmp2Reg, Assembler::le, Assembler::AVX_512bit);
10259     ktestd(k2, k3);
10260     jcc(Assembler::carryClear, return_zero);
10261 
10262     evpmovwb(Address(dst, 0), k3, tmp1Reg, Assembler::AVX_512bit);
10263 
10264     addptr(src, tmp5);
10265     addptr(src, tmp5);
10266     addptr(dst, tmp5);
10267     subl(len, tmp5);
10268 
10269     bind(post_alignment);
10270     // end of alignment
10271 
10272     movl(tmp5, len);
10273     andl(tmp5, (32 - 1));    // tail count (in chars)
10274     andl(len, ~(32 - 1));    // vector count (in chars)
10275     jcc(Assembler::zero, copy_loop_tail);
10276 
10277     lea(src, Address(src, len, Address::times_2));
10278     lea(dst, Address(dst, len, Address::times_1));
10279     negptr(len);
10280 
10281     bind(copy_32_loop);
10282     evmovdquw(tmp1Reg, Address(src, len, Address::times_2), Assembler::AVX_512bit);
10283     evpcmpuw(k2, tmp1Reg, tmp2Reg, Assembler::le, Assembler::AVX_512bit);
10284     kortestdl(k2, k2);
10285     jcc(Assembler::carryClear, return_zero);
10286 
10287     // All elements in current processed chunk are valid candidates for
10288     // compression. Write a truncated byte elements to the memory.
10289     evpmovwb(Address(dst, len, Address::times_1), tmp1Reg, Assembler::AVX_512bit);
10290     addptr(len, 32);
10291     jcc(Assembler::notZero, copy_32_loop);
10292 
10293     bind(copy_loop_tail);
10294     // bail out when there is nothing to be done
10295     testl(tmp5, 0xFFFFFFFF);
10296     jcc(Assembler::zero, return_length);
10297 
10298     movl(len, tmp5);
10299 
10300     // ~(~0 &lt;&lt; len), where len is the # of remaining elements to process
10301     movl(result, 0xFFFFFFFF);
10302     shlxl(result, result, len);
10303     notl(result);
10304 
10305     kmovdl(k3, result);
10306 
10307     evmovdquw(tmp1Reg, k3, Address(src, 0), Assembler::AVX_512bit);
10308     evpcmpuw(k2, k3, tmp1Reg, tmp2Reg, Assembler::le, Assembler::AVX_512bit);
10309     ktestd(k2, k3);
10310     jcc(Assembler::carryClear, return_zero);
10311 
10312     evpmovwb(Address(dst, 0), k3, tmp1Reg, Assembler::AVX_512bit);
10313     jmp(return_length);
10314 
10315     bind(below_threshold);
10316   }
10317 
10318   if (UseSSE42Intrinsics) {
10319     Label copy_32_loop, copy_16, copy_tail;
10320 
10321     movl(result, len);
10322 
10323     movl(tmp5, 0xff00ff00);   // create mask to test for Unicode chars in vectors
10324 
10325     // vectored compression
10326     andl(len, 0xfffffff0);    // vector count (in chars)
10327     andl(result, 0x0000000f);    // tail count (in chars)
10328     testl(len, len);
10329     jcc(Assembler::zero, copy_16);
10330 
10331     // compress 16 chars per iter
10332     movdl(tmp1Reg, tmp5);
10333     pshufd(tmp1Reg, tmp1Reg, 0);   // store Unicode mask in tmp1Reg
10334     pxor(tmp4Reg, tmp4Reg);
10335 
10336     lea(src, Address(src, len, Address::times_2));
10337     lea(dst, Address(dst, len, Address::times_1));
10338     negptr(len);
10339 
10340     bind(copy_32_loop);
10341     movdqu(tmp2Reg, Address(src, len, Address::times_2));     // load 1st 8 characters
10342     por(tmp4Reg, tmp2Reg);
10343     movdqu(tmp3Reg, Address(src, len, Address::times_2, 16)); // load next 8 characters
10344     por(tmp4Reg, tmp3Reg);
10345     ptest(tmp4Reg, tmp1Reg);       // check for Unicode chars in next vector
10346     jcc(Assembler::notZero, return_zero);
10347     packuswb(tmp2Reg, tmp3Reg);    // only ASCII chars; compress each to 1 byte
10348     movdqu(Address(dst, len, Address::times_1), tmp2Reg);
10349     addptr(len, 16);
10350     jcc(Assembler::notZero, copy_32_loop);
10351 
10352     // compress next vector of 8 chars (if any)
10353     bind(copy_16);
10354     movl(len, result);
10355     andl(len, 0xfffffff8);    // vector count (in chars)
10356     andl(result, 0x00000007);    // tail count (in chars)
10357     testl(len, len);
10358     jccb(Assembler::zero, copy_tail);
10359 
10360     movdl(tmp1Reg, tmp5);
10361     pshufd(tmp1Reg, tmp1Reg, 0);   // store Unicode mask in tmp1Reg
10362     pxor(tmp3Reg, tmp3Reg);
10363 
10364     movdqu(tmp2Reg, Address(src, 0));
10365     ptest(tmp2Reg, tmp1Reg);       // check for Unicode chars in vector
10366     jccb(Assembler::notZero, return_zero);
10367     packuswb(tmp2Reg, tmp3Reg);    // only LATIN1 chars; compress each to 1 byte
10368     movq(Address(dst, 0), tmp2Reg);
10369     addptr(src, 16);
10370     addptr(dst, 8);
10371 
10372     bind(copy_tail);
10373     movl(len, result);
10374   }
10375   // compress 1 char per iter
10376   testl(len, len);
10377   jccb(Assembler::zero, return_length);
10378   lea(src, Address(src, len, Address::times_2));
10379   lea(dst, Address(dst, len, Address::times_1));
10380   negptr(len);
10381 
10382   bind(copy_chars_loop);
10383   load_unsigned_short(result, Address(src, len, Address::times_2));
10384   testl(result, 0xff00);      // check if Unicode char
10385   jccb(Assembler::notZero, return_zero);
10386   movb(Address(dst, len, Address::times_1), result);  // ASCII char; compress to 1 byte
10387   increment(len);
10388   jcc(Assembler::notZero, copy_chars_loop);
10389 
10390   // if compression succeeded, return length
10391   bind(return_length);
10392   pop(result);
10393   jmpb(done);
10394 
10395   // if compression failed, return 0
10396   bind(return_zero);
10397   xorl(result, result);
10398   addptr(rsp, wordSize);
10399 
10400   bind(done);
10401 }
10402 
10403 // Inflate byte[] array to char[].
10404 //   ..\jdk\src\java.base\share\classes\java\lang\StringLatin1.java
10405 //   @HotSpotIntrinsicCandidate
10406 //   private static void inflate(byte[] src, int srcOff, char[] dst, int dstOff, int len) {
10407 //     for (int i = 0; i &lt; len; i++) {
10408 //       dst[dstOff++] = (char)(src[srcOff++] &amp; 0xff);
10409 //     }
10410 //   }
10411 void MacroAssembler::byte_array_inflate(Register src, Register dst, Register len,
10412   XMMRegister tmp1, Register tmp2) {
10413   Label copy_chars_loop, done, below_threshold, avx3_threshold;
10414   // rsi: src
10415   // rdi: dst
10416   // rdx: len
10417   // rcx: tmp2
10418 
10419   // rsi holds start addr of source byte[] to be inflated
10420   // rdi holds start addr of destination char[]
10421   // rdx holds length
10422   assert_different_registers(src, dst, len, tmp2);
10423   movl(tmp2, len);
10424   if ((UseAVX &gt; 2) &amp;&amp; // AVX512
10425     VM_Version::supports_avx512vlbw() &amp;&amp;
10426     VM_Version::supports_bmi2()) {
10427 
10428     Label copy_32_loop, copy_tail;
10429     Register tmp3_aliased = len;
10430 
10431     // if length of the string is less than 16, handle it in an old fashioned way
10432     testl(len, -16);
10433     jcc(Assembler::zero, below_threshold);
10434 
10435     testl(len, -1 * AVX3Threshold);
10436     jcc(Assembler::zero, avx3_threshold);
10437 
10438     // In order to use only one arithmetic operation for the main loop we use
10439     // this pre-calculation
10440     andl(tmp2, (32 - 1)); // tail count (in chars), 32 element wide loop
10441     andl(len, -32);     // vector count
10442     jccb(Assembler::zero, copy_tail);
10443 
10444     lea(src, Address(src, len, Address::times_1));
10445     lea(dst, Address(dst, len, Address::times_2));
10446     negptr(len);
10447 
10448 
10449     // inflate 32 chars per iter
10450     bind(copy_32_loop);
10451     vpmovzxbw(tmp1, Address(src, len, Address::times_1), Assembler::AVX_512bit);
10452     evmovdquw(Address(dst, len, Address::times_2), tmp1, Assembler::AVX_512bit);
10453     addptr(len, 32);
10454     jcc(Assembler::notZero, copy_32_loop);
10455 
10456     bind(copy_tail);
10457     // bail out when there is nothing to be done
10458     testl(tmp2, -1); // we don&#39;t destroy the contents of tmp2 here
10459     jcc(Assembler::zero, done);
10460 
10461     // ~(~0 &lt;&lt; length), where length is the # of remaining elements to process
10462     movl(tmp3_aliased, -1);
10463     shlxl(tmp3_aliased, tmp3_aliased, tmp2);
10464     notl(tmp3_aliased);
10465     kmovdl(k2, tmp3_aliased);
10466     evpmovzxbw(tmp1, k2, Address(src, 0), Assembler::AVX_512bit);
10467     evmovdquw(Address(dst, 0), k2, tmp1, Assembler::AVX_512bit);
10468 
10469     jmp(done);
10470     bind(avx3_threshold);
10471   }
10472   if (UseSSE42Intrinsics) {
10473     Label copy_16_loop, copy_8_loop, copy_bytes, copy_new_tail, copy_tail;
10474 
10475     if (UseAVX &gt; 1) {
10476       andl(tmp2, (16 - 1));
10477       andl(len, -16);
10478       jccb(Assembler::zero, copy_new_tail);
10479     } else {
10480       andl(tmp2, 0x00000007);   // tail count (in chars)
10481       andl(len, 0xfffffff8);    // vector count (in chars)
10482       jccb(Assembler::zero, copy_tail);
10483     }
10484 
10485     // vectored inflation
10486     lea(src, Address(src, len, Address::times_1));
10487     lea(dst, Address(dst, len, Address::times_2));
10488     negptr(len);
10489 
10490     if (UseAVX &gt; 1) {
10491       bind(copy_16_loop);
10492       vpmovzxbw(tmp1, Address(src, len, Address::times_1), Assembler::AVX_256bit);
10493       vmovdqu(Address(dst, len, Address::times_2), tmp1);
10494       addptr(len, 16);
10495       jcc(Assembler::notZero, copy_16_loop);
10496 
10497       bind(below_threshold);
10498       bind(copy_new_tail);
10499       movl(len, tmp2);
10500       andl(tmp2, 0x00000007);
10501       andl(len, 0xFFFFFFF8);
10502       jccb(Assembler::zero, copy_tail);
10503 
10504       pmovzxbw(tmp1, Address(src, 0));
10505       movdqu(Address(dst, 0), tmp1);
10506       addptr(src, 8);
10507       addptr(dst, 2 * 8);
10508 
10509       jmp(copy_tail, true);
10510     }
10511 
10512     // inflate 8 chars per iter
10513     bind(copy_8_loop);
10514     pmovzxbw(tmp1, Address(src, len, Address::times_1));  // unpack to 8 words
10515     movdqu(Address(dst, len, Address::times_2), tmp1);
10516     addptr(len, 8);
10517     jcc(Assembler::notZero, copy_8_loop);
10518 
10519     bind(copy_tail);
10520     movl(len, tmp2);
10521 
10522     cmpl(len, 4);
10523     jccb(Assembler::less, copy_bytes);
10524 
10525     movdl(tmp1, Address(src, 0));  // load 4 byte chars
10526     pmovzxbw(tmp1, tmp1);
10527     movq(Address(dst, 0), tmp1);
10528     subptr(len, 4);
10529     addptr(src, 4);
10530     addptr(dst, 8);
10531 
10532     bind(copy_bytes);
10533   } else {
10534     bind(below_threshold);
10535   }
10536 
10537   testl(len, len);
10538   jccb(Assembler::zero, done);
10539   lea(src, Address(src, len, Address::times_1));
10540   lea(dst, Address(dst, len, Address::times_2));
10541   negptr(len);
10542 
10543   // inflate 1 char per iter
10544   bind(copy_chars_loop);
10545   load_unsigned_byte(tmp2, Address(src, len, Address::times_1));  // load byte char
10546   movw(Address(dst, len, Address::times_2), tmp2);  // inflate byte char to word
10547   increment(len);
10548   jcc(Assembler::notZero, copy_chars_loop);
10549 
10550   bind(done);
10551 }
10552 
10553 #ifdef _LP64
10554 void MacroAssembler::convert_f2i(Register dst, XMMRegister src) {
10555   Label done;
10556   cvttss2sil(dst, src);
10557   // Conversion instructions do not match JLS for overflow, underflow and NaN -&gt; fixup in stub
10558   cmpl(dst, 0x80000000); // float_sign_flip
10559   jccb(Assembler::notEqual, done);
10560   subptr(rsp, 8);
10561   movflt(Address(rsp, 0), src);
10562   call(RuntimeAddress(CAST_FROM_FN_PTR(address, StubRoutines::x86::f2i_fixup())));
10563   pop(dst);
10564   bind(done);
10565 }
10566 
10567 void MacroAssembler::convert_d2i(Register dst, XMMRegister src) {
10568   Label done;
10569   cvttsd2sil(dst, src);
10570   // Conversion instructions do not match JLS for overflow, underflow and NaN -&gt; fixup in stub
10571   cmpl(dst, 0x80000000); // float_sign_flip
10572   jccb(Assembler::notEqual, done);
10573   subptr(rsp, 8);
10574   movdbl(Address(rsp, 0), src);
10575   call(RuntimeAddress(CAST_FROM_FN_PTR(address, StubRoutines::x86::d2i_fixup())));
10576   pop(dst);
10577   bind(done);
10578 }
10579 
10580 void MacroAssembler::convert_f2l(Register dst, XMMRegister src) {
10581   Label done;
10582   cvttss2siq(dst, src);
10583   cmp64(dst, ExternalAddress((address) StubRoutines::x86::double_sign_flip()));
10584   jccb(Assembler::notEqual, done);
10585   subptr(rsp, 8);
10586   movflt(Address(rsp, 0), src);
10587   call(RuntimeAddress(CAST_FROM_FN_PTR(address, StubRoutines::x86::f2l_fixup())));
10588   pop(dst);
10589   bind(done);
10590 }
10591 
10592 void MacroAssembler::convert_d2l(Register dst, XMMRegister src) {
10593   Label done;
10594   cvttsd2siq(dst, src);
10595   cmp64(dst, ExternalAddress((address) StubRoutines::x86::double_sign_flip()));
10596   jccb(Assembler::notEqual, done);
10597   subptr(rsp, 8);
10598   movdbl(Address(rsp, 0), src);
10599   call(RuntimeAddress(CAST_FROM_FN_PTR(address, StubRoutines::x86::d2l_fixup())));
10600   pop(dst);
10601   bind(done);
10602 }
10603 
10604 void MacroAssembler::cache_wb(Address line)
10605 {
10606   // 64 bit cpus always support clflush
10607   assert(VM_Version::supports_clflush(), &quot;clflush should be available&quot;);
10608   bool optimized = VM_Version::supports_clflushopt();
10609   bool no_evict = VM_Version::supports_clwb();
10610 
10611   // prefer clwb (writeback without evict) otherwise
10612   // prefer clflushopt (potentially parallel writeback with evict)
10613   // otherwise fallback on clflush (serial writeback with evict)
10614 
10615   if (optimized) {
10616     if (no_evict) {
10617       clwb(line);
10618     } else {
10619       clflushopt(line);
10620     }
10621   } else {
10622     // no need for fence when using CLFLUSH
10623     clflush(line);
10624   }
10625 }
10626 
10627 void MacroAssembler::cache_wbsync(bool is_pre)
10628 {
10629   assert(VM_Version::supports_clflush(), &quot;clflush should be available&quot;);
10630   bool optimized = VM_Version::supports_clflushopt();
10631   bool no_evict = VM_Version::supports_clwb();
10632 
10633   // pick the correct implementation
10634 
10635   if (!is_pre &amp;&amp; (optimized || no_evict)) {
10636     // need an sfence for post flush when using clflushopt or clwb
10637     // otherwise no no need for any synchroniaztion
10638 
10639     sfence();
10640   }
10641 }
10642 #endif // _LP64
10643 
10644 Assembler::Condition MacroAssembler::negate_condition(Assembler::Condition cond) {
10645   switch (cond) {
10646     // Note some conditions are synonyms for others
10647     case Assembler::zero:         return Assembler::notZero;
10648     case Assembler::notZero:      return Assembler::zero;
10649     case Assembler::less:         return Assembler::greaterEqual;
10650     case Assembler::lessEqual:    return Assembler::greater;
10651     case Assembler::greater:      return Assembler::lessEqual;
10652     case Assembler::greaterEqual: return Assembler::less;
10653     case Assembler::below:        return Assembler::aboveEqual;
10654     case Assembler::belowEqual:   return Assembler::above;
10655     case Assembler::above:        return Assembler::belowEqual;
10656     case Assembler::aboveEqual:   return Assembler::below;
10657     case Assembler::overflow:     return Assembler::noOverflow;
10658     case Assembler::noOverflow:   return Assembler::overflow;
10659     case Assembler::negative:     return Assembler::positive;
10660     case Assembler::positive:     return Assembler::negative;
10661     case Assembler::parity:       return Assembler::noParity;
10662     case Assembler::noParity:     return Assembler::parity;
10663   }
10664   ShouldNotReachHere(); return Assembler::overflow;
10665 }
10666 
10667 SkipIfEqual::SkipIfEqual(
10668     MacroAssembler* masm, const bool* flag_addr, bool value) {
10669   _masm = masm;
10670   _masm-&gt;cmp8(ExternalAddress((address)flag_addr), value);
10671   _masm-&gt;jcc(Assembler::equal, _label);
10672 }
10673 
10674 SkipIfEqual::~SkipIfEqual() {
10675   _masm-&gt;bind(_label);
10676 }
10677 
10678 // 32-bit Windows has its own fast-path implementation
10679 // of get_thread
10680 #if !defined(WIN32) || defined(_LP64)
10681 
10682 // This is simply a call to Thread::current()
10683 void MacroAssembler::get_thread(Register thread) {
10684   if (thread != rax) {
10685     push(rax);
10686   }
10687   LP64_ONLY(push(rdi);)
10688   LP64_ONLY(push(rsi);)
10689   push(rdx);
10690   push(rcx);
10691 #ifdef _LP64
10692   push(r8);
10693   push(r9);
10694   push(r10);
10695   push(r11);
10696 #endif
10697 
10698   MacroAssembler::call_VM_leaf_base(CAST_FROM_FN_PTR(address, Thread::current), 0);
10699 
10700 #ifdef _LP64
10701   pop(r11);
10702   pop(r10);
10703   pop(r9);
10704   pop(r8);
10705 #endif
10706   pop(rcx);
10707   pop(rdx);
10708   LP64_ONLY(pop(rsi);)
10709   LP64_ONLY(pop(rdi);)
10710   if (thread != rax) {
10711     mov(thread, rax);
10712     pop(rax);
10713   }
10714 }
10715 
10716 #endif // !WIN32 || _LP64
    </pre>
  </body>
</html>