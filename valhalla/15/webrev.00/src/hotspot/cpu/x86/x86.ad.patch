diff a/src/hotspot/cpu/x86/x86.ad b/src/hotspot/cpu/x86/x86.ad
--- a/src/hotspot/cpu/x86/x86.ad
+++ b/src/hotspot/cpu/x86/x86.ad
@@ -1175,11 +1175,11 @@
 // Stuff framesize into a register and call a VM stub routine.
 int HandlerImpl::emit_exception_handler(CodeBuffer& cbuf) {
 
   // Note that the code buffer's insts_mark is always relative to insts.
   // That's why we must use the macroassembler to generate a handler.
-  MacroAssembler _masm(&cbuf);
+  C2_MacroAssembler _masm(&cbuf);
   address base = __ start_a_stub(size_exception_handler());
   if (base == NULL) {
     ciEnv::current()->record_failure("CodeCache is full");
     return 0;  // CodeBuffer::expand failed
   }
@@ -1193,11 +1193,11 @@
 // Emit deopt handler code.
 int HandlerImpl::emit_deopt_handler(CodeBuffer& cbuf) {
 
   // Note that the code buffer's insts_mark is always relative to insts.
   // That's why we must use the macroassembler to generate a handler.
-  MacroAssembler _masm(&cbuf);
+  C2_MacroAssembler _masm(&cbuf);
   address base = __ start_a_stub(size_deopt_handler());
   if (base == NULL) {
     ciEnv::current()->record_failure("CodeCache is full");
     return 0;  // CodeBuffer::expand failed
   }
@@ -1260,11 +1260,11 @@
       if (!UsePopCountInstruction) {
         return false;
       }
       break;
     case Op_PopCountVI:
-      if (!UsePopCountInstruction || !VM_Version::supports_vpopcntdq()) {
+      if (!UsePopCountInstruction || !VM_Version::supports_avx512_vpopcntdq()) {
         return false;
       }
       break;
     case Op_MulVI:
       if ((UseSSE < 4) && (UseAVX < 1)) { // only with SSE4_1 or AVX
@@ -1275,36 +1275,26 @@
     case Op_MulReductionVL:
       if (VM_Version::supports_avx512dq() == false) {
         return false;
       }
       break;
-    case Op_AddReductionVL:
-      if (UseAVX < 3) { // only EVEX : vector connectivity becomes an issue here
-        return false;
-      }
-      break;
     case Op_AbsVB:
     case Op_AbsVS:
     case Op_AbsVI:
     case Op_AddReductionVI:
-      if (UseSSE < 3 || !VM_Version::supports_ssse3()) { // requires at least SSSE3
+    case Op_AndReductionV:
+    case Op_OrReductionV:
+    case Op_XorReductionV:
+      if (UseSSE < 3) { // requires at least SSSE3
         return false;
       }
       break;
     case Op_MulReductionVI:
       if (UseSSE < 4) { // requires at least SSE4
         return false;
       }
       break;
-    case Op_AddReductionVF:
-    case Op_AddReductionVD:
-    case Op_MulReductionVF:
-    case Op_MulReductionVD:
-      if (UseSSE < 1) { // requires at least SSE
-        return false;
-      }
-      break;
     case Op_SqrtVD:
     case Op_SqrtVF:
       if (UseAVX < 1) { // enabled for AVX only
         return false;
       }
@@ -1336,18 +1326,10 @@
     case Op_OnSpinWait:
       if (VM_Version::supports_on_spin_wait() == false) {
         return false;
       }
       break;
-    case Op_MulAddVS2VI:
-    case Op_RShiftVL:
-    case Op_AbsVD:
-    case Op_NegVD:
-      if (UseSSE < 2) {
-        return false;
-      }
-      break;
     case Op_MulVB:
     case Op_LShiftVB:
     case Op_RShiftVB:
     case Op_URShiftVB:
       if (UseSSE < 4) {
@@ -1379,10 +1361,28 @@
     case Op_RoundDoubleModeV:
       if (VM_Version::supports_avx() == false) {
         return false; // 128bit vroundpd is not available
       }
       break;
+#ifndef _LP64
+    case Op_AddReductionVF:
+    case Op_AddReductionVD:
+    case Op_MulReductionVF:
+    case Op_MulReductionVD:
+      if (UseSSE < 1) { // requires at least SSE
+        return false;
+      }
+      break;
+    case Op_MulAddVS2VI:
+    case Op_RShiftVL:
+    case Op_AbsVD:
+    case Op_NegVD:
+      if (UseSSE < 2) {
+        return false;
+      }
+      break;
+#endif // !LP64
   }
   return true;  // Match rules are supported by default.
 }
 
 //------------------------------------------------------------------------
@@ -1671,10 +1671,16 @@
 static inline uint vector_length(const MachNode* n) {
   const TypeVect* vt = n->bottom_type()->is_vect();
   return vt->length();
 }
 
+static inline uint vector_length(const MachNode* use, MachOper* opnd) {
+  uint def_idx = use->operand_index(opnd);
+  Node* def = use->in(def_idx);
+  return def->bottom_type()->is_vect()->length();
+}
+
 static inline uint vector_length_in_bytes(const MachNode* n) {
   const TypeVect* vt = n->bottom_type()->is_vect();
   return vt->length_in_bytes();
 }
 
@@ -1708,11 +1714,11 @@
   assert(ireg == Op_VecS || // 32bit vector
          (src_lo & 1) == 0 && (src_lo + 1) == src_hi &&
          (dst_lo & 1) == 0 && (dst_lo + 1) == dst_hi,
          "no non-adjacent vector moves" );
   if (cbuf) {
-    MacroAssembler _masm(cbuf);
+    C2_MacroAssembler _masm(cbuf);
     int offset = __ offset();
     switch (ireg) {
     case Op_VecS: // copy whole register
     case Op_VecD:
     case Op_VecX:
@@ -1774,11 +1780,11 @@
                      int stack_offset, int reg, uint ireg, outputStream* st) {
   // In 64-bit VM size calculation is very complex. Emitting instructions
   // into scratch buffer is used to get size in 64-bit VM.
   LP64_ONLY( assert(!do_size, "this method calculates size only for 32-bit VM"); )
   if (cbuf) {
-    MacroAssembler _masm(cbuf);
+    C2_MacroAssembler _masm(cbuf);
     int offset = __ offset();
     if (is_load) {
       switch (ireg) {
       case Op_VecS:
         __ movdl(as_XMMRegister(Matcher::_regEncode[reg]), Address(rsp, stack_offset));
@@ -1977,11 +1983,11 @@
     st->print("nop \t# %d bytes pad for loops and calls", _count);
   }
 #endif
 
   void MachNopNode::emit(CodeBuffer &cbuf, PhaseRegAlloc*) const {
-    MacroAssembler _masm(&cbuf);
+    C2_MacroAssembler _masm(&cbuf);
     __ nop(_count);
   }
 
   uint MachNopNode::size(PhaseRegAlloc*) const {
     return _count;
@@ -1992,11 +1998,11 @@
     st->print("# breakpoint");
   }
 #endif
 
   void MachBreakpointNode::emit(CodeBuffer &cbuf, PhaseRegAlloc* ra_) const {
-    MacroAssembler _masm(&cbuf);
+    C2_MacroAssembler _masm(&cbuf);
     __ int3();
   }
 
   uint MachBreakpointNode::size(PhaseRegAlloc* ra_) const {
     return MachNode::size(ra_);
@@ -2008,11 +2014,11 @@
 
   enc_class call_epilog %{
     if (VerifyStackAtCalls) {
       // Check that stack depth is unchanged: find majik cookie on stack
       int framesize = ra_->reg2offset_unchecked(OptoReg::add(ra_->_matcher._old_SP, -3*VMRegImpl::slots_per_word));
-      MacroAssembler _masm(&cbuf);
+      C2_MacroAssembler _masm(&cbuf);
       Label L;
       __ cmpptr(Address(rsp, framesize), (int32_t)0xbadb100d);
       __ jccb(Assembler::equal, L);
       // Die if stack mismatch
       __ int3();
@@ -3120,107 +3126,69 @@
 
 // ====================REPLICATE=======================================
 
 // Replicate byte scalar to be vector
 instruct ReplB_reg(vec dst, rRegI src) %{
-  predicate((n->as_Vector()->length() <= 32) ||
-            (n->as_Vector()->length() == 64 && VM_Version::supports_avx512bw())); // AVX512BW for 512bit byte instructions
   match(Set dst (ReplicateB src));
   format %{ "replicateB $dst,$src" %}
   ins_encode %{
     uint vlen = vector_length(this);
     if (vlen == 64 || VM_Version::supports_avx512vlbw()) { // AVX512VL for <512bit operands
-      assert(VM_Version::supports_avx512bw(), "required");
+      assert(VM_Version::supports_avx512bw(), "required"); // 512-bit byte vectors assume AVX512BW
       int vlen_enc = vector_length_encoding(this);
       __ evpbroadcastb($dst$$XMMRegister, $src$$Register, vlen_enc);
     } else {
       __ movdl($dst$$XMMRegister, $src$$Register);
       __ punpcklbw($dst$$XMMRegister, $dst$$XMMRegister);
       __ pshuflw($dst$$XMMRegister, $dst$$XMMRegister, 0x00);
       if (vlen >= 16) {
         __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
         if (vlen >= 32) {
-          assert(vlen == 32, "sanity"); // vlen == 64 && !AVX512BW is covered by ReplB_reg_leg
+          assert(vlen == 32, "sanity");
           __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
         }
       }
     }
   %}
   ins_pipe( pipe_slow );
 %}
 
-instruct ReplB_reg_leg(legVec dst, rRegI src) %{
-  predicate(n->as_Vector()->length() == 64 && !VM_Version::supports_avx512bw()); // AVX512BW for 512bit byte instructions
-  match(Set dst (ReplicateB src));
-  format %{ "replicateB $dst,$src" %}
-  ins_encode %{
-    assert(UseAVX > 2, "required");
-    __ movdl($dst$$XMMRegister, $src$$Register);
-    __ punpcklbw($dst$$XMMRegister, $dst$$XMMRegister);
-    __ pshuflw($dst$$XMMRegister, $dst$$XMMRegister, 0x00);
-    __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
-    __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
-    __ vinserti64x4($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, 0x1);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
 instruct ReplB_mem(vec dst, memory mem) %{
-  predicate((n->as_Vector()->length() <= 32 && VM_Version::supports_avx512vlbw()) || // AVX512VL for <512bit operands
-            (n->as_Vector()->length() == 64 && VM_Version::supports_avx512bw()));    // AVX512BW for 512bit byte instructions
+  predicate(VM_Version::supports_avx2());
   match(Set dst (ReplicateB (LoadB mem)));
   format %{ "replicateB $dst,$mem" %}
   ins_encode %{
-    assert(UseAVX > 2, "required");
     int vector_len = vector_length_encoding(this);
     __ vpbroadcastb($dst$$XMMRegister, $mem$$Address, vector_len);
   %}
   ins_pipe( pipe_slow );
 %}
 
 instruct ReplB_imm(vec dst, immI con) %{
-  predicate((n->as_Vector()->length() <= 32) ||
-            (n->as_Vector()->length() == 64 && VM_Version::supports_avx512bw())); // AVX512BW for 512bit byte instructions
   match(Set dst (ReplicateB con));
   format %{ "replicateB $dst,$con" %}
   ins_encode %{
     uint vlen = vector_length(this);
     InternalAddress const_addr = $constantaddress(replicate8_imm($con$$constant, 1));
     if (vlen == 4) {
       __ movdl($dst$$XMMRegister, const_addr);
     } else {
       __ movq($dst$$XMMRegister, const_addr);
       if (vlen >= 16) {
-        if (vlen == 64 || VM_Version::supports_avx512vlbw()) { // AVX512VL for <512bit operands
+        if (VM_Version::supports_avx2()) {
           int vlen_enc = vector_length_encoding(this);
-          __ vpbroadcastb($dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);
+          __ vpbroadcastq($dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);
         } else {
+          assert(vlen == 16, "sanity");
           __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
-          if (vlen >= 32) {
-             assert(vlen == 32, "sanity");// vlen == 64 && !AVX512BW is covered by ReplB_imm_leg
-            __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
-          }
         }
       }
     }
   %}
   ins_pipe( pipe_slow );
 %}
 
-instruct ReplB_imm_leg(legVec dst, immI con) %{
-  predicate(n->as_Vector()->length() == 64 && !VM_Version::supports_avx512bw());
-  match(Set dst (ReplicateB con));
-  format %{ "replicateB $dst,$con" %}
-  ins_encode %{
-    __ movq($dst$$XMMRegister, $constantaddress(replicate8_imm($con$$constant, 1)));
-    __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
-    __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
-    __ vinserti64x4($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, 0x1);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
 // Replicate byte scalar zero to be vector
 instruct ReplB_zero(vec dst, immI0 zero) %{
   match(Set dst (ReplicateB zero));
   format %{ "replicateB $dst,$zero" %}
   ins_encode %{
@@ -3237,130 +3205,68 @@
 %}
 
 // ====================ReplicateS=======================================
 
 instruct ReplS_reg(vec dst, rRegI src) %{
-  predicate((n->as_Vector()->length() <= 16) ||
-            (n->as_Vector()->length() == 32 && VM_Version::supports_avx512bw())); // AVX512BW for 512bit instructions on shorts
   match(Set dst (ReplicateS src));
   format %{ "replicateS $dst,$src" %}
   ins_encode %{
     uint vlen = vector_length(this);
     if (vlen == 32 || VM_Version::supports_avx512vlbw()) { // AVX512VL for <512bit operands
-      assert(VM_Version::supports_avx512bw(), "required");
+      assert(VM_Version::supports_avx512bw(), "required"); // 512-bit short vectors assume AVX512BW
       int vlen_enc = vector_length_encoding(this);
       __ evpbroadcastw($dst$$XMMRegister, $src$$Register, vlen_enc);
     } else {
       __ movdl($dst$$XMMRegister, $src$$Register);
       __ pshuflw($dst$$XMMRegister, $dst$$XMMRegister, 0x00);
       if (vlen >= 8) {
         __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
         if (vlen >= 16) {
-          assert(vlen == 16, "sanity"); // vlen == 32 && !AVX512BW is covered by ReplS_reg_leg
+          assert(vlen == 16, "sanity");
           __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
         }
       }
     }
   %}
   ins_pipe( pipe_slow );
 %}
 
-instruct ReplS_reg_leg(legVec dst, rRegI src) %{
-  predicate(n->as_Vector()->length() == 32 && !VM_Version::supports_avx512bw());
-  match(Set dst (ReplicateS src));
-  format %{ "replicateS $dst,$src" %}
-  ins_encode %{
-    __ movdl($dst$$XMMRegister, $src$$Register);
-    __ pshuflw($dst$$XMMRegister, $dst$$XMMRegister, 0x00);
-    __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
-    __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
-    __ vinserti64x4($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, 0x1);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
 instruct ReplS_mem(vec dst, memory mem) %{
-  predicate((n->as_Vector()->length() >= 4  &&
-             n->as_Vector()->length() <= 16 && VM_Version::supports_avx()) ||
-            (n->as_Vector()->length() == 32 && VM_Version::supports_avx512bw())); // AVX512BW for 512bit instructions on shorts
+  predicate(VM_Version::supports_avx2());
   match(Set dst (ReplicateS (LoadS mem)));
   format %{ "replicateS $dst,$mem" %}
   ins_encode %{
-    uint vlen = vector_length(this);
-    if (vlen == 32 || VM_Version::supports_avx512vlbw()) { // AVX512VL for <512bit operands
-      assert(VM_Version::supports_avx512bw(), "required");
-      int vlen_enc = vector_length_encoding(this);
-      __ vpbroadcastw($dst$$XMMRegister, $mem$$Address, vlen_enc);
-    } else {
-      __ pshuflw($dst$$XMMRegister, $mem$$Address, 0x00);
-      if (vlen >= 8) {
-        __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
-        if (vlen >= 16) {
-          assert(vlen == 16, "sanity"); // vlen == 32 && !AVX512BW is covered by ReplS_mem_leg
-          __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
-        }
-      }
-    }
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct ReplS_mem_leg(legVec dst, memory mem) %{
-  predicate(n->as_Vector()->length() == 32 && !VM_Version::supports_avx512bw());
-  match(Set dst (ReplicateS (LoadS mem)));
-  format %{ "replicateS $dst,$mem" %}
-  ins_encode %{
-    __ pshuflw($dst$$XMMRegister, $mem$$Address, 0x00);
-    __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
-    __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
-    __ vinserti64x4($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, 0x1);
+    int vlen_enc = vector_length_encoding(this);
+    __ vpbroadcastw($dst$$XMMRegister, $mem$$Address, vlen_enc);
   %}
   ins_pipe( pipe_slow );
 %}
 
 instruct ReplS_imm(vec dst, immI con) %{
-  predicate((n->as_Vector()->length() <= 16) ||
-            (n->as_Vector()->length() == 32 && VM_Version::supports_avx512bw())); // AVX512BW for 512bit instructions on shorts
   match(Set dst (ReplicateS con));
   format %{ "replicateS $dst,$con" %}
   ins_encode %{
     uint vlen = vector_length(this);
-    InternalAddress constaddr = $constantaddress(replicate8_imm($con$$constant, 2));
+    InternalAddress const_addr = $constantaddress(replicate8_imm($con$$constant, 2));
     if (vlen == 2) {
-      __ movdl($dst$$XMMRegister, constaddr);
+      __ movdl($dst$$XMMRegister, const_addr);
     } else {
-      __ movq($dst$$XMMRegister, constaddr);
-      if (vlen == 32 || VM_Version::supports_avx512vlbw() ) { // AVX512VL for <512bit operands
-        assert(VM_Version::supports_avx512bw(), "required");
-        int vlen_enc = vector_length_encoding(this);
-        __ vpbroadcastw($dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);
-      } else {
-        __ movq($dst$$XMMRegister, constaddr);
-        __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
-        if (vlen >= 16) {
-          assert(vlen == 16, "sanity"); // vlen == 32 && !AVX512BW is covered by ReplS_imm_leg
-          __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
+      __ movq($dst$$XMMRegister, const_addr);
+      if (vlen >= 8) {
+        if (VM_Version::supports_avx2()) {
+          int vlen_enc = vector_length_encoding(this);
+          __ vpbroadcastw($dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);
+        } else {
+          assert(vlen == 8, "sanity");
+          __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
         }
       }
     }
   %}
   ins_pipe( fpu_reg_reg );
 %}
 
-instruct ReplS_imm_leg(legVec dst, immI con) %{
-  predicate(n->as_Vector()->length() == 32 && !VM_Version::supports_avx512bw());
-  match(Set dst (ReplicateS con));
-  format %{ "replicateS $dst,$con" %}
-  ins_encode %{
-    __ movq($dst$$XMMRegister, $constantaddress(replicate8_imm($con$$constant, 2)));
-    __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
-    __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
-    __ vinserti64x4($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, 0x1);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
 instruct ReplS_zero(vec dst, immI0 zero) %{
   match(Set dst (ReplicateS zero));
   format %{ "replicateS $dst,$zero" %}
   ins_encode %{
     uint vlen = vector_length(this);
@@ -3395,48 +3301,42 @@
   %}
   ins_pipe( pipe_slow );
 %}
 
 instruct ReplI_mem(vec dst, memory mem) %{
-  predicate(VM_Version::supports_avx()); // use VEX-encoded pshufd to relax 16-byte alignment restriction on the source
   match(Set dst (ReplicateI (LoadI mem)));
   format %{ "replicateI $dst,$mem" %}
   ins_encode %{
     uint vlen = vector_length(this);
     if (vlen <= 4) {
-      __ pshufd($dst$$XMMRegister, $mem$$Address, 0x00);
-    } else if (vlen == 16 || VM_Version::supports_avx512vl()) { // AVX512VL for <512bit operands
+      __ movdl($dst$$XMMRegister, $mem$$Address);
+      __ pshufd($dst$$XMMRegister, $dst$$XMMRegister, 0x00);
+    } else {
+      assert(VM_Version::supports_avx2(), "sanity");
       int vector_len = vector_length_encoding(this);
       __ vpbroadcastd($dst$$XMMRegister, $mem$$Address, vector_len);
-    } else {
-      assert(vlen == 8, "sanity");
-      __ pshufd($dst$$XMMRegister, $mem$$Address, 0x00);
-      __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
     }
   %}
   ins_pipe( pipe_slow );
 %}
 
 instruct ReplI_imm(vec dst, immI con) %{
   match(Set dst (ReplicateI con));
   format %{ "replicateI $dst,$con" %}
   ins_encode %{
     uint vlen = vector_length(this);
-    InternalAddress constaddr = $constantaddress(replicate8_imm($con$$constant, 4));
-    if (vlen == 2) {
-      __ movq($dst$$XMMRegister, constaddr);
-    } else if (vlen == 16 || VM_Version::supports_avx512vl()) { // AVX512VL for <512bit operands
+    InternalAddress const_addr = $constantaddress(replicate8_imm($con$$constant, 4));
+    if (vlen <= 4) {
+      __ movq($dst$$XMMRegister, const_addr);
+      if (vlen == 4) {
+        __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
+      }
+    } else {
+      assert(VM_Version::supports_avx2(), "sanity");
       int vector_len = vector_length_encoding(this);
-      __ movq($dst$$XMMRegister, constaddr);
+      __ movq($dst$$XMMRegister, const_addr);
       __ vpbroadcastd($dst$$XMMRegister, $dst$$XMMRegister, vector_len);
-    } else {
-      __ movq($dst$$XMMRegister, constaddr);
-      __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
-      if (vlen >= 8) {
-        assert(vlen == 8, "sanity");
-        __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
-      }
     }
   %}
   ins_pipe( pipe_slow );
 %}
 
@@ -3542,18 +3442,14 @@
   ins_encode %{
     uint vlen = vector_length(this);
     if (vlen == 2) {
       __ movq($dst$$XMMRegister, $mem$$Address);
       __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
-    } else if (vlen == 8 || VM_Version::supports_avx512vl()) { // AVX512VL for <512bit operands
+    } else {
+      assert(VM_Version::supports_avx2(), "sanity");
       int vlen_enc = vector_length_encoding(this);
       __ vpbroadcastq($dst$$XMMRegister, $mem$$Address, vlen_enc);
-    } else {
-      assert(vlen == 4, "sanity");
-      __ movq($dst$$XMMRegister, $mem$$Address);
-      __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
-      __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
     }
   %}
   ins_pipe( pipe_slow );
 %}
 
@@ -3565,19 +3461,15 @@
     uint vlen = vector_length(this);
     InternalAddress const_addr = $constantaddress($con);
     if (vlen == 2) {
       __ movq($dst$$XMMRegister, const_addr);
       __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
-    } else if (vlen == 8 || VM_Version::supports_avx512vl()) { // AVX512VL for <512bit operands
+    } else {
+      assert(VM_Version::supports_avx2(), "sanity");
       int vlen_enc = vector_length_encoding(this);
       __ movq($dst$$XMMRegister, const_addr);
       __ vpbroadcastq($dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);
-    } else {
-      assert(vlen == 4, "sanity");
-      __ movq($dst$$XMMRegister, const_addr);
-      __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
-      __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
     }
   %}
   ins_pipe( pipe_slow );
 %}
 
@@ -3603,37 +3495,34 @@
   format %{ "replicateF $dst,$src" %}
   ins_encode %{
     uint vlen = vector_length(this);
     if (vlen <= 4) {
       __ pshufd($dst$$XMMRegister, $src$$XMMRegister, 0x00);
-    } else if (vlen == 16 || VM_Version::supports_avx512vl()) { // AVX512VL for <512bit operands
+   } else if (VM_Version::supports_avx2()) {
       int vector_len = vector_length_encoding(this);
-      __ vbroadcastss($dst$$XMMRegister, $src$$XMMRegister, vector_len);
+      __ vbroadcastss($dst$$XMMRegister, $src$$XMMRegister, vector_len); // reg-to-reg variant requires AVX2
     } else {
       assert(vlen == 8, "sanity");
       __ pshufd($dst$$XMMRegister, $src$$XMMRegister, 0x00);
       __ vinsertf128_high($dst$$XMMRegister, $dst$$XMMRegister);
     }
   %}
   ins_pipe( pipe_slow );
 %}
 
 instruct ReplF_mem(vec dst, memory mem) %{
-  predicate(VM_Version::supports_avx()); // use VEX-encoded pshufd to relax 16-byte alignment restriction on the source
   match(Set dst (ReplicateF (LoadF mem)));
   format %{ "replicateF $dst,$mem" %}
   ins_encode %{
     uint vlen = vector_length(this);
     if (vlen <= 4) {
-      __ pshufd($dst$$XMMRegister, $mem$$Address, 0x00);
-    } else if (vlen == 16 || VM_Version::supports_avx512vl()) { // AVX512VL for <512bit operands
+      __ movdl($dst$$XMMRegister, $mem$$Address);
+      __ pshufd($dst$$XMMRegister, $dst$$XMMRegister, 0x00);
+    } else {
+      assert(VM_Version::supports_avx(), "sanity");
       int vector_len = vector_length_encoding(this);
       __ vbroadcastss($dst$$XMMRegister, $mem$$Address, vector_len);
-    } else {
-      assert(vlen == 8, "sanity");
-      __ pshufd($dst$$XMMRegister, $mem$$Address, 0x00);
-      __ vinsertf128_high($dst$$XMMRegister, $dst$$XMMRegister);
     }
   %}
   ins_pipe( pipe_slow );
 %}
 
@@ -3660,37 +3549,34 @@
   format %{ "replicateD $dst,$src" %}
   ins_encode %{
     uint vlen = vector_length(this);
     if (vlen == 2) {
       __ pshufd($dst$$XMMRegister, $src$$XMMRegister, 0x44);
-    } else if (vlen == 8 || VM_Version::supports_avx512vl()) { // AVX512VL for <512bit operands
+    } else if (VM_Version::supports_avx2()) {
       int vector_len = vector_length_encoding(this);
-      __ vbroadcastsd($dst$$XMMRegister, $src$$XMMRegister, vector_len);
+      __ vbroadcastsd($dst$$XMMRegister, $src$$XMMRegister, vector_len); // reg-to-reg variant requires AVX2
     } else {
       assert(vlen == 4, "sanity");
       __ pshufd($dst$$XMMRegister, $src$$XMMRegister, 0x44);
       __ vinsertf128_high($dst$$XMMRegister, $dst$$XMMRegister);
     }
   %}
   ins_pipe( pipe_slow );
 %}
 
 instruct ReplD_mem(vec dst, memory mem) %{
-  predicate(VM_Version::supports_avx()); // use VEX-encoded pshufd to relax 16-byte alignment restriction on the source
   match(Set dst (ReplicateD (LoadD mem)));
   format %{ "replicateD $dst,$mem" %}
   ins_encode %{
     uint vlen = vector_length(this);
     if (vlen == 2) {
-      __ pshufd($dst$$XMMRegister, $mem$$Address, 0x44);
-    } else if (vlen == 8 || VM_Version::supports_avx512vl()) { // AVX512VL for <512bit operands
+      __ movq($dst$$XMMRegister, $mem$$Address);
+      __ pshufd($dst$$XMMRegister, $dst$$XMMRegister, 0x44);
+    } else {
+      assert(VM_Version::supports_avx(), "sanity");
       int vector_len = vector_length_encoding(this);
       __ vbroadcastsd($dst$$XMMRegister, $mem$$Address, vector_len);
-    } else {
-      assert(vlen == 4, "sanity");
-      __ pshufd($dst$$XMMRegister, $mem$$Address, 0x44);
-      __ vinsertf128_high($dst$$XMMRegister, $dst$$XMMRegister);
     }
   %}
   ins_pipe( pipe_slow );
 %}
 
@@ -3708,713 +3594,172 @@
   %}
   ins_pipe( fpu_reg_reg );
 %}
 
 // ====================REDUCTION ARITHMETIC=======================================
+// =======================Int Reduction==========================================
 
-// =======================AddReductionVI==========================================
-
-instruct vadd2I_reduction_reg(rRegI dst, rRegI src1, vec src2, vec tmp, vec tmp2) %{
-  predicate(n->in(2)->bottom_type()->is_vect()->length() == 2); // vector_length(src2) == 2
-  match(Set dst (AddReductionVI src1 src2));
-  effect(TEMP tmp, TEMP tmp2);
-  format %{ "vector_add2I_reduction $dst,$src1,$src2" %}
-  ins_encode %{
-    if (UseAVX > 2) {
-      int vector_len = Assembler::AVX_128bit;
-      __ pshufd($tmp2$$XMMRegister, $src2$$XMMRegister, 0x1);
-      __ vpaddd($tmp$$XMMRegister, $src2$$XMMRegister, $tmp2$$XMMRegister, vector_len);
-      __ movdl($tmp2$$XMMRegister, $src1$$Register);
-      __ vpaddd($tmp2$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, vector_len);
-      __ movdl($dst$$Register, $tmp2$$XMMRegister);
-    } else if (VM_Version::supports_avxonly()) {
-      int vector_len = Assembler::AVX_128bit;
-      __ vphaddd($tmp$$XMMRegister, $src2$$XMMRegister, $src2$$XMMRegister, vector_len);
-      __ movdl($tmp2$$XMMRegister, $src1$$Register);
-      __ vpaddd($tmp2$$XMMRegister, $tmp2$$XMMRegister, $tmp$$XMMRegister, vector_len);
-      __ movdl($dst$$Register, $tmp2$$XMMRegister);
-    } else {
-      assert(UseSSE > 2, "required");
-      __ movdqu($tmp2$$XMMRegister, $src2$$XMMRegister);
-      __ phaddd($tmp2$$XMMRegister, $tmp2$$XMMRegister);
-      __ movdl($tmp$$XMMRegister, $src1$$Register);
-      __ paddd($tmp$$XMMRegister, $tmp2$$XMMRegister);
-      __ movdl($dst$$Register, $tmp$$XMMRegister);
-    }
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vadd4I_reduction_reg(rRegI dst, rRegI src1, vec src2, vec tmp, vec tmp2) %{
-  predicate(n->in(2)->bottom_type()->is_vect()->length() == 4); // vector_length(src2) == 4
+instruct reductionI(rRegI dst, rRegI src1, vec src2, vec vtmp1, vec vtmp2) %{
+  predicate(n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_INT &&
+            n->in(2)->bottom_type()->is_vect()->length() < 16);
   match(Set dst (AddReductionVI src1 src2));
-  effect(TEMP tmp, TEMP tmp2);
-  format %{ "vector_add4I_reduction $dst,$src1,$src2" %}
-  ins_encode %{
-    if (UseAVX > 2) {
-      int vector_len = Assembler::AVX_128bit;
-      __ pshufd($tmp2$$XMMRegister, $src2$$XMMRegister, 0xE);
-      __ vpaddd($tmp$$XMMRegister, $src2$$XMMRegister, $tmp2$$XMMRegister, vector_len);
-      __ pshufd($tmp2$$XMMRegister, $tmp$$XMMRegister, 0x1);
-      __ vpaddd($tmp$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, vector_len);
-      __ movdl($tmp2$$XMMRegister, $src1$$Register);
-      __ vpaddd($tmp2$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, vector_len);
-      __ movdl($dst$$Register, $tmp2$$XMMRegister);
-    } else if (VM_Version::supports_avxonly()) {
-      int vector_len = Assembler::AVX_128bit;
-      __ vphaddd($tmp$$XMMRegister, $src2$$XMMRegister, $src2$$XMMRegister, vector_len);
-      __ vphaddd($tmp$$XMMRegister, $tmp$$XMMRegister, $tmp$$XMMRegister, vector_len);
-      __ movdl($tmp2$$XMMRegister, $src1$$Register);
-      __ vpaddd($tmp2$$XMMRegister, $tmp2$$XMMRegister, $tmp$$XMMRegister, vector_len);
-      __ movdl($dst$$Register, $tmp2$$XMMRegister);
-    } else {
-      assert(UseSSE > 2, "required");
-      __ movdqu($tmp$$XMMRegister, $src2$$XMMRegister);
-      __ phaddd($tmp$$XMMRegister, $tmp$$XMMRegister);
-      __ phaddd($tmp$$XMMRegister, $tmp$$XMMRegister);
-      __ movdl($tmp2$$XMMRegister, $src1$$Register);
-      __ paddd($tmp2$$XMMRegister, $tmp$$XMMRegister);
-      __ movdl($dst$$Register, $tmp2$$XMMRegister);
-    }
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vadd8I_reduction_reg(rRegI dst, rRegI src1, vec src2, vec tmp, vec tmp2) %{
-  predicate(n->in(2)->bottom_type()->is_vect()->length() == 8); // vector_length(src2) == 8
-  match(Set dst (AddReductionVI src1 src2));
-  effect(TEMP tmp, TEMP tmp2);
-  format %{ "vector_add8I_reduction $dst,$src1,$src2" %}
-  ins_encode %{
-    if (UseAVX > 2) {
-      int vector_len = Assembler::AVX_128bit;
-      __ vextracti128_high($tmp$$XMMRegister, $src2$$XMMRegister);
-      __ vpaddd($tmp$$XMMRegister, $tmp$$XMMRegister, $src2$$XMMRegister, vector_len);
-      __ pshufd($tmp2$$XMMRegister, $tmp$$XMMRegister, 0xE);
-      __ vpaddd($tmp$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, vector_len);
-      __ pshufd($tmp2$$XMMRegister, $tmp$$XMMRegister, 0x1);
-      __ vpaddd($tmp$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, vector_len);
-      __ movdl($tmp2$$XMMRegister, $src1$$Register);
-      __ vpaddd($tmp2$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, vector_len);
-      __ movdl($dst$$Register, $tmp2$$XMMRegister);
-    } else {
-      assert(UseAVX > 0, "");
-      int vector_len = Assembler::AVX_256bit;
-      __ vphaddd($tmp$$XMMRegister, $src2$$XMMRegister, $src2$$XMMRegister, vector_len);
-      __ vphaddd($tmp$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, vector_len);
-      __ vextracti128_high($tmp2$$XMMRegister, $tmp$$XMMRegister);
-      __ vpaddd($tmp$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, 0);
-      __ movdl($tmp2$$XMMRegister, $src1$$Register);
-      __ vpaddd($tmp2$$XMMRegister, $tmp2$$XMMRegister, $tmp$$XMMRegister, 0);
-      __ movdl($dst$$Register, $tmp2$$XMMRegister);
-    }
+  match(Set dst (MulReductionVI src1 src2));
+  match(Set dst (AndReductionV  src1 src2));
+  match(Set dst ( OrReductionV  src1 src2));
+  match(Set dst (XorReductionV  src1 src2));
+  effect(TEMP vtmp1, TEMP vtmp2);
+  format %{ "vector_reduction_int $dst,$src1,$src2 ; using $vtmp1, $vtmp2 as TEMP" %}
+  ins_encode %{
+    int opcode = this->ideal_Opcode();
+    int vlen = vector_length(this, $src2);
+    __ reduceI(opcode, vlen, $dst$$Register, $src1$$Register, $src2$$XMMRegister, $vtmp1$$XMMRegister, $vtmp2$$XMMRegister);
   %}
   ins_pipe( pipe_slow );
 %}
 
-instruct vadd16I_reduction_reg(rRegI dst, rRegI src1, legVec src2, legVec tmp, legVec tmp2, legVec tmp3) %{
-  predicate(n->in(2)->bottom_type()->is_vect()->length() == 16); // vector_length(src2) == 16
+instruct reduction16I(rRegI dst, rRegI src1, legVec src2, legVec vtmp1, legVec vtmp2) %{
+  predicate(n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_INT &&
+            n->in(2)->bottom_type()->is_vect()->length() == 16);
   match(Set dst (AddReductionVI src1 src2));
-  effect(TEMP tmp, TEMP tmp2, TEMP tmp3);
-  format %{ "vector_add16I_reduction $dst,$src1,$src2" %}
+  match(Set dst (MulReductionVI src1 src2));
+  match(Set dst (AndReductionV  src1 src2));
+  match(Set dst ( OrReductionV  src1 src2));
+  match(Set dst (XorReductionV  src1 src2));
+  effect(TEMP vtmp1, TEMP vtmp2);
+  format %{ "vector_reduction_int $dst,$src1,$src2 ; using $vtmp1, $vtmp2 as TEMP" %}
   ins_encode %{
-    __ vextracti64x4_high($tmp3$$XMMRegister, $src2$$XMMRegister);
-    __ vpaddd($tmp3$$XMMRegister, $tmp3$$XMMRegister, $src2$$XMMRegister, 1);
-    __ vextracti128_high($tmp$$XMMRegister, $tmp3$$XMMRegister);
-    __ vpaddd($tmp$$XMMRegister, $tmp$$XMMRegister, $tmp3$$XMMRegister, 0);
-    __ pshufd($tmp2$$XMMRegister, $tmp$$XMMRegister, 0xE);
-    __ vpaddd($tmp$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, 0);
-    __ pshufd($tmp2$$XMMRegister, $tmp$$XMMRegister, 0x1);
-    __ vpaddd($tmp$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, 0);
-    __ movdl($tmp2$$XMMRegister, $src1$$Register);
-    __ vpaddd($tmp2$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, 0);
-    __ movdl($dst$$Register, $tmp2$$XMMRegister);
+    int opcode = this->ideal_Opcode();
+    int vlen = vector_length(this, $src2);
+    __ reduceI(opcode, vlen, $dst$$Register, $src1$$Register, $src2$$XMMRegister, $vtmp1$$XMMRegister, $vtmp2$$XMMRegister);
   %}
   ins_pipe( pipe_slow );
 %}
 
-// =======================AddReductionVL==========================================
+// =======================Long Reduction==========================================
 
 #ifdef _LP64
-instruct vadd2L_reduction_reg(rRegL dst, rRegL src1, vec src2, vec tmp, vec tmp2) %{
-  predicate(n->in(2)->bottom_type()->is_vect()->length() == 2); // vector_length(src2) == 2
+instruct reductionL(rRegL dst, rRegL src1, vec src2, vec vtmp1, vec vtmp2) %{
+  predicate(n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG &&
+            n->in(2)->bottom_type()->is_vect()->length() < 8);
   match(Set dst (AddReductionVL src1 src2));
-  effect(TEMP tmp, TEMP tmp2);
-  format %{ "vector_add2L_reduction $dst,$src1,$src2" %}
-  ins_encode %{
-    assert(UseAVX > 2, "required");
-    __ pshufd($tmp2$$XMMRegister, $src2$$XMMRegister, 0xE);
-    __ vpaddq($tmp$$XMMRegister, $src2$$XMMRegister, $tmp2$$XMMRegister, 0);
-    __ movdq($tmp2$$XMMRegister, $src1$$Register);
-    __ vpaddq($tmp2$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, 0);
-    __ movdq($dst$$Register, $tmp2$$XMMRegister);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vadd4L_reduction_reg(rRegL dst, rRegL src1, vec src2, vec tmp, vec tmp2) %{
-  predicate(n->in(2)->bottom_type()->is_vect()->length() == 4); // vector_length(src2) == 4
-  match(Set dst (AddReductionVL src1 src2));
-  effect(TEMP tmp, TEMP tmp2);
-  format %{ "vector_add4L_reduction $dst,$src1,$src2" %}
+  match(Set dst (MulReductionVL src1 src2));
+  match(Set dst (AndReductionV  src1 src2));
+  match(Set dst ( OrReductionV  src1 src2));
+  match(Set dst (XorReductionV  src1 src2));
+  effect(TEMP vtmp1, TEMP vtmp2);
+  format %{ "vector_reduction_long $dst,$src1,$src2 ; using $vtmp1, $vtmp2 as TEMP" %}
   ins_encode %{
-    assert(UseAVX > 2, "required");
-    __ vextracti128_high($tmp$$XMMRegister, $src2$$XMMRegister);
-    __ vpaddq($tmp2$$XMMRegister, $tmp$$XMMRegister, $src2$$XMMRegister, 0);
-    __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0xE);
-    __ vpaddq($tmp2$$XMMRegister, $tmp2$$XMMRegister, $tmp$$XMMRegister, 0);
-    __ movdq($tmp$$XMMRegister, $src1$$Register);
-    __ vpaddq($tmp2$$XMMRegister, $tmp2$$XMMRegister, $tmp$$XMMRegister, 0);
-    __ movdq($dst$$Register, $tmp2$$XMMRegister);
+    int opcode = this->ideal_Opcode();
+    int vlen = vector_length(this, $src2);
+    __ reduceL(opcode, vlen, $dst$$Register, $src1$$Register, $src2$$XMMRegister, $vtmp1$$XMMRegister, $vtmp2$$XMMRegister);
   %}
   ins_pipe( pipe_slow );
 %}
 
-instruct vadd8L_reduction_reg(rRegL dst, rRegL src1, legVec src2, legVec tmp, legVec tmp2) %{
-  predicate(n->in(2)->bottom_type()->is_vect()->length() == 8); // vector_length(src2) == 8
+instruct reduction8L(rRegL dst, rRegL src1, legVec src2, legVec vtmp1, legVec vtmp2) %{
+  predicate(n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_LONG &&
+            n->in(2)->bottom_type()->is_vect()->length() == 8);
   match(Set dst (AddReductionVL src1 src2));
-  effect(TEMP tmp, TEMP tmp2);
-  format %{ "vector_addL_reduction $dst,$src1,$src2" %}
+  match(Set dst (MulReductionVL src1 src2));
+  match(Set dst (AndReductionV  src1 src2));
+  match(Set dst ( OrReductionV  src1 src2));
+  match(Set dst (XorReductionV  src1 src2));
+  effect(TEMP vtmp1, TEMP vtmp2);
+  format %{ "vector_reduction_long $dst,$src1,$src2 ; using $vtmp1, $vtmp2 as TEMP" %}
   ins_encode %{
-    assert(UseAVX > 2, "required");
-    __ vextracti64x4_high($tmp2$$XMMRegister, $src2$$XMMRegister);
-    __ vpaddq($tmp2$$XMMRegister, $tmp2$$XMMRegister, $src2$$XMMRegister, 1);
-    __ vextracti128_high($tmp$$XMMRegister, $tmp2$$XMMRegister);
-    __ vpaddq($tmp2$$XMMRegister, $tmp2$$XMMRegister, $tmp$$XMMRegister, 0);
-    __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0xE);
-    __ vpaddq($tmp2$$XMMRegister, $tmp2$$XMMRegister, $tmp$$XMMRegister, 0);
-    __ movdq($tmp$$XMMRegister, $src1$$Register);
-    __ vpaddq($tmp2$$XMMRegister, $tmp2$$XMMRegister, $tmp$$XMMRegister, 0);
-    __ movdq($dst$$Register, $tmp2$$XMMRegister);
+    int opcode = this->ideal_Opcode();
+    int vlen = vector_length(this, $src2);
+    __ reduceL(opcode, vlen, $dst$$Register, $src1$$Register, $src2$$XMMRegister, $vtmp1$$XMMRegister, $vtmp2$$XMMRegister);
   %}
   ins_pipe( pipe_slow );
 %}
 #endif // _LP64
 
-// =======================AddReductionVF==========================================
+// =======================Float Reduction==========================================
 
-instruct vadd2F_reduction_reg(regF dst, vec src2, vec tmp) %{
-  predicate(n->in(2)->bottom_type()->is_vect()->length() == 2); // vector_length(src2) == 2
-  match(Set dst (AddReductionVF dst src2));
-  effect(TEMP dst, TEMP tmp);
-  format %{ "vector_add2F_reduction $dst,$dst,$src2" %}
+instruct reductionF128(regF dst, vec src, vec vtmp) %{
+  predicate(n->in(2)->bottom_type()->is_vect()->length() <= 4);
+  match(Set dst (AddReductionVF dst src));
+  match(Set dst (MulReductionVF dst src));
+  effect(TEMP dst, TEMP vtmp);
+  format %{ "vector_reduction_fp  $dst,$src ; using $vtmp as TEMP" %}
   ins_encode %{
-    if (UseAVX > 0) {
-      __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $src2$$XMMRegister);
-      __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x01);
-      __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
-    } else {
-      assert(UseSSE > 0, "required");
-      __ addss($dst$$XMMRegister, $src2$$XMMRegister);
-      __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x01);
-      __ addss($dst$$XMMRegister, $tmp$$XMMRegister);
-    }
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vadd4F_reduction_reg(regF dst, vec src2, vec tmp) %{
-  predicate(n->in(2)->bottom_type()->is_vect()->length() == 4); // vector_length(src2) == 4
-  match(Set dst (AddReductionVF dst src2));
-  effect(TEMP dst, TEMP tmp);
-  format %{ "vector_add4F_reduction $dst,$dst,$src2" %}
-  ins_encode %{
-    if (UseAVX > 0) {
-      __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $src2$$XMMRegister);
-      __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x01);
-      __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
-      __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x02);
-      __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
-      __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x03);
-      __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
-    } else {
-      assert(UseSSE > 0, "required");
-      __ addss($dst$$XMMRegister, $src2$$XMMRegister);
-      __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x01);
-      __ addss($dst$$XMMRegister, $tmp$$XMMRegister);
-      __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x02);
-      __ addss($dst$$XMMRegister, $tmp$$XMMRegister);
-      __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x03);
-      __ addss($dst$$XMMRegister, $tmp$$XMMRegister);
-    }
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-
-instruct vadd8F_reduction_reg(regF dst, vec src2, vec tmp, vec tmp2) %{
-  predicate(n->in(2)->bottom_type()->is_vect()->length() == 8); // vector_length(src2) == 8
-  match(Set dst (AddReductionVF dst src2));
-  effect(TEMP tmp, TEMP dst, TEMP tmp2);
-  format %{ "vector_add8F_reduction $dst,$dst,$src2" %}
-  ins_encode %{
-    assert(UseAVX > 0, "required");
-    __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $src2$$XMMRegister);
-    __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x01);
-    __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
-    __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x02);
-    __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
-    __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x03);
-    __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
-    __ vextractf128_high($tmp2$$XMMRegister, $src2$$XMMRegister);
-    __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister);
-    __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x01);
-    __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
-    __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x02);
-    __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
-    __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x03);
-    __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vadd16F_reduction_reg(regF dst, legVec src2, legVec tmp, legVec tmp2) %{
-  predicate(n->in(2)->bottom_type()->is_vect()->length() == 16); // vector_length(src2) == 16
-  match(Set dst (AddReductionVF dst src2));
-  effect(TEMP tmp, TEMP dst, TEMP tmp2);
-  format %{ "vector_add16F_reduction $dst,$dst,$src2" %}
-  ins_encode %{
-    assert(UseAVX > 2, "required");
-    __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $src2$$XMMRegister);
-    __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x01);
-    __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
-    __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x02);
-    __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
-    __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x03);
-    __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
-    __ vextractf32x4($tmp2$$XMMRegister, $src2$$XMMRegister, 0x1);
-    __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister);
-    __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x01);
-    __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
-    __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x02);
-    __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
-    __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x03);
-    __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
-    __ vextractf32x4($tmp2$$XMMRegister, $src2$$XMMRegister, 0x2);
-    __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister);
-    __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x01);
-    __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
-    __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x02);
-    __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
-    __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x03);
-    __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
-    __ vextractf32x4($tmp2$$XMMRegister, $src2$$XMMRegister, 0x3);
-    __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister);
-    __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x01);
-    __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
-    __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x02);
-    __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
-    __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x03);
-    __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-// =======================AddReductionVD==========================================
-
-instruct vadd2D_reduction_reg(regD dst, vec src2, vec tmp) %{
-  predicate(n->in(2)->bottom_type()->is_vect()->length() == 2); // vector_length(src2) == 2
-  match(Set dst (AddReductionVD dst src2));
-  effect(TEMP tmp, TEMP dst);
-  format %{ "vector_add2D_reduction  $dst,$src2" %}
-  ins_encode %{
-    if (UseAVX > 0) {
-      __ vaddsd($dst$$XMMRegister, $dst$$XMMRegister, $src2$$XMMRegister);
-      __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0xE);
-      __ vaddsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
-    } else {
-      assert(UseSSE > 0, "required");
-      __ addsd($dst$$XMMRegister, $src2$$XMMRegister);
-      __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0xE);
-      __ addsd($dst$$XMMRegister, $tmp$$XMMRegister);
-    }
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vadd4D_reduction_reg(regD dst, vec src2, vec tmp, vec tmp2) %{
-  predicate(n->in(2)->bottom_type()->is_vect()->length() == 4); // vector_length(src2) == 4
-  match(Set dst (AddReductionVD dst src2));
-  effect(TEMP tmp, TEMP dst, TEMP tmp2);
-  format %{ "vector_add4D_reduction $dst,$dst,$src2" %}
-  ins_encode %{
-    assert(UseAVX > 0, "required");
-    __ vaddsd($dst$$XMMRegister, $dst$$XMMRegister, $src2$$XMMRegister);
-    __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0xE);
-    __ vaddsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
-    __ vextractf128($tmp2$$XMMRegister, $src2$$XMMRegister, 0x1);
-    __ vaddsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister);
-    __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0xE);
-    __ vaddsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vadd8D_reduction_reg(regD dst, legVec src2, legVec tmp, legVec tmp2) %{
-  predicate(n->in(2)->bottom_type()->is_vect()->length() == 8); // vector_length(src2) == 8
-  match(Set dst (AddReductionVD dst src2));
-  effect(TEMP tmp, TEMP dst, TEMP tmp2);
-  format %{ "vector_add8D_reduction $dst,$dst,$src2" %}
-  ins_encode %{
-    assert(UseAVX > 2, "required");
-    __ vaddsd($dst$$XMMRegister, $dst$$XMMRegister, $src2$$XMMRegister);
-    __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0xE);
-    __ vaddsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
-    __ vextractf32x4($tmp2$$XMMRegister, $src2$$XMMRegister, 0x1);
-    __ vaddsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister);
-    __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0xE);
-    __ vaddsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
-    __ vextractf32x4($tmp2$$XMMRegister, $src2$$XMMRegister, 0x2);
-    __ vaddsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister);
-    __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0xE);
-    __ vaddsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
-    __ vextractf32x4($tmp2$$XMMRegister, $src2$$XMMRegister, 0x3);
-    __ vaddsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister);
-    __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0xE);
-    __ vaddsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-// =======================MulReductionVI==========================================
-
-instruct vmul2I_reduction_reg(rRegI dst, rRegI src1, vec src2, vec tmp, vec tmp2) %{
-  predicate(n->in(2)->bottom_type()->is_vect()->length() == 2); // vector_length(src2) == 2
-  match(Set dst (MulReductionVI src1 src2));
-  effect(TEMP tmp, TEMP tmp2);
-  format %{ "vector_mul2I_reduction $dst,$src1,$src2" %}
-  ins_encode %{
-    if (UseAVX > 0) {
-      int vector_len = Assembler::AVX_128bit;
-      __ pshufd($tmp2$$XMMRegister, $src2$$XMMRegister, 0x1);
-      __ vpmulld($tmp$$XMMRegister, $src2$$XMMRegister, $tmp2$$XMMRegister, vector_len);
-      __ movdl($tmp2$$XMMRegister, $src1$$Register);
-      __ vpmulld($tmp2$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, vector_len);
-      __ movdl($dst$$Register, $tmp2$$XMMRegister);
-    } else {
-      assert(UseSSE > 3, "required");
-      __ pshufd($tmp2$$XMMRegister, $src2$$XMMRegister, 0x1);
-      __ pmulld($tmp2$$XMMRegister, $src2$$XMMRegister);
-      __ movdl($tmp$$XMMRegister, $src1$$Register);
-      __ pmulld($tmp2$$XMMRegister, $tmp$$XMMRegister);
-      __ movdl($dst$$Register, $tmp2$$XMMRegister);
-    }
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vmul4I_reduction_reg(rRegI dst, rRegI src1, vec src2, vec tmp, vec tmp2) %{
-  predicate(n->in(2)->bottom_type()->is_vect()->length() == 4); // vector_length(src2) == 4
-  match(Set dst (MulReductionVI src1 src2));
-  effect(TEMP tmp, TEMP tmp2);
-  format %{ "vector_mul4I_reduction $dst,$src1,$src2" %}
-  ins_encode %{
-    if (UseAVX > 0) {
-      int vector_len = Assembler::AVX_128bit;
-      __ pshufd($tmp2$$XMMRegister, $src2$$XMMRegister, 0xE);
-      __ vpmulld($tmp$$XMMRegister, $src2$$XMMRegister, $tmp2$$XMMRegister, vector_len);
-      __ pshufd($tmp2$$XMMRegister, $tmp$$XMMRegister, 0x1);
-      __ vpmulld($tmp$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, vector_len);
-      __ movdl($tmp2$$XMMRegister, $src1$$Register);
-      __ vpmulld($tmp2$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, vector_len);
-      __ movdl($dst$$Register, $tmp2$$XMMRegister);
-    } else {
-      assert(UseSSE > 3, "required");
-      __ pshufd($tmp2$$XMMRegister, $src2$$XMMRegister, 0xE);
-      __ pmulld($tmp2$$XMMRegister, $src2$$XMMRegister);
-      __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x1);
-      __ pmulld($tmp2$$XMMRegister, $tmp$$XMMRegister);
-      __ movdl($tmp$$XMMRegister, $src1$$Register);
-      __ pmulld($tmp2$$XMMRegister, $tmp$$XMMRegister);
-      __ movdl($dst$$Register, $tmp2$$XMMRegister);
-    }
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vmul8I_reduction_reg(rRegI dst, rRegI src1, vec src2, vec tmp, vec tmp2) %{
-  predicate(n->in(2)->bottom_type()->is_vect()->length() == 8); // vector_length(src2) == 8
-  match(Set dst (MulReductionVI src1 src2));
-  effect(TEMP tmp, TEMP tmp2);
-  format %{ "vector_mul8I_reduction $dst,$src1,$src2" %}
-  ins_encode %{
-    assert(UseAVX > 1, "required");
-    int vector_len = Assembler::AVX_128bit;
-    __ vextracti128_high($tmp$$XMMRegister, $src2$$XMMRegister);
-    __ vpmulld($tmp$$XMMRegister, $tmp$$XMMRegister, $src2$$XMMRegister, vector_len);
-    __ pshufd($tmp2$$XMMRegister, $tmp$$XMMRegister, 0xE);
-    __ vpmulld($tmp$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, vector_len);
-    __ pshufd($tmp2$$XMMRegister, $tmp$$XMMRegister, 0x1);
-    __ vpmulld($tmp$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, vector_len);
-    __ movdl($tmp2$$XMMRegister, $src1$$Register);
-    __ vpmulld($tmp2$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, vector_len);
-    __ movdl($dst$$Register, $tmp2$$XMMRegister);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vmul16I_reduction_reg(rRegI dst, rRegI src1, legVec src2, legVec tmp, legVec tmp2, legVec tmp3) %{
-  predicate(n->in(2)->bottom_type()->is_vect()->length() == 16); // vector_length(src2) == 16
-  match(Set dst (MulReductionVI src1 src2));
-  effect(TEMP tmp, TEMP tmp2, TEMP tmp3);
-  format %{ "vector_mul16I_reduction $dst,$src1,$src2" %}
-  ins_encode %{
-    assert(UseAVX > 2, "required");
-    __ vextracti64x4_high($tmp3$$XMMRegister, $src2$$XMMRegister);
-    __ vpmulld($tmp3$$XMMRegister, $tmp3$$XMMRegister, $src2$$XMMRegister, 1);
-    __ vextracti128_high($tmp$$XMMRegister, $tmp3$$XMMRegister);
-    __ vpmulld($tmp$$XMMRegister, $tmp$$XMMRegister, $tmp3$$XMMRegister, 0);
-    __ pshufd($tmp2$$XMMRegister, $tmp$$XMMRegister, 0xE);
-    __ vpmulld($tmp$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, 0);
-    __ pshufd($tmp2$$XMMRegister, $tmp$$XMMRegister, 0x1);
-    __ vpmulld($tmp$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, 0);
-    __ movdl($tmp2$$XMMRegister, $src1$$Register);
-    __ vpmulld($tmp2$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, 0);
-    __ movdl($dst$$Register, $tmp2$$XMMRegister);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-// =======================MulReductionVL==========================================
-
-#ifdef _LP64
-instruct vmul2L_reduction_reg(rRegL dst, rRegL src1, vec src2, vec tmp, vec tmp2) %{
-  predicate(n->in(2)->bottom_type()->is_vect()->length() == 2); // vector_length(src2) == 2
-  match(Set dst (MulReductionVL src1 src2));
-  effect(TEMP tmp, TEMP tmp2);
-  format %{ "vector_mul2L_reduction $dst,$src1,$src2" %}
-  ins_encode %{
-    assert(VM_Version::supports_avx512dq(), "required");
-    __ pshufd($tmp2$$XMMRegister, $src2$$XMMRegister, 0xE);
-    __ vpmullq($tmp$$XMMRegister, $src2$$XMMRegister, $tmp2$$XMMRegister, 0);
-    __ movdq($tmp2$$XMMRegister, $src1$$Register);
-    __ vpmullq($tmp2$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, 0);
-    __ movdq($dst$$Register, $tmp2$$XMMRegister);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vmul4L_reduction_reg(rRegL dst, rRegL src1, vec src2, vec tmp, vec tmp2) %{
-  predicate(n->in(2)->bottom_type()->is_vect()->length() == 4); // vector_length(src2) == 4
-  match(Set dst (MulReductionVL src1 src2));
-  effect(TEMP tmp, TEMP tmp2);
-  format %{ "vector_mul4L_reduction $dst,$src1,$src2" %}
-  ins_encode %{
-    assert(VM_Version::supports_avx512dq(), "required");
-    __ vextracti128_high($tmp$$XMMRegister, $src2$$XMMRegister);
-    __ vpmullq($tmp2$$XMMRegister, $tmp$$XMMRegister, $src2$$XMMRegister, 0);
-    __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0xE);
-    __ vpmullq($tmp2$$XMMRegister, $tmp2$$XMMRegister, $tmp$$XMMRegister, 0);
-    __ movdq($tmp$$XMMRegister, $src1$$Register);
-    __ vpmullq($tmp2$$XMMRegister, $tmp2$$XMMRegister, $tmp$$XMMRegister, 0);
-    __ movdq($dst$$Register, $tmp2$$XMMRegister);
+    int opcode = this->ideal_Opcode();
+    int vlen = vector_length(this, $src);
+    __ reduce_fp(opcode, vlen, $dst$$XMMRegister, $src$$XMMRegister, $vtmp$$XMMRegister);
   %}
   ins_pipe( pipe_slow );
 %}
 
-instruct vmul8L_reduction_reg(rRegL dst, rRegL src1, legVec src2, legVec tmp, legVec tmp2) %{
-  predicate(n->in(2)->bottom_type()->is_vect()->length() == 8); // vector_length(src2) == 8
-  match(Set dst (MulReductionVL src1 src2));
-  effect(TEMP tmp, TEMP tmp2);
-  format %{ "vector_mul8L_reduction $dst,$src1,$src2" %}
+instruct reduction8F(regF dst, vec src, vec vtmp1, vec vtmp2) %{
+  predicate(n->in(2)->bottom_type()->is_vect()->length() == 8);
+  match(Set dst (AddReductionVF dst src));
+  match(Set dst (MulReductionVF dst src));
+  effect(TEMP dst, TEMP vtmp1, TEMP vtmp2);
+  format %{ "vector_reduction_float $dst,$src ; using $vtmp1, $vtmp2 as TEMP" %}
   ins_encode %{
-    assert(VM_Version::supports_avx512dq(), "required");
-    __ vextracti64x4_high($tmp2$$XMMRegister, $src2$$XMMRegister);
-    __ vpmullq($tmp2$$XMMRegister, $tmp2$$XMMRegister, $src2$$XMMRegister, 1);
-    __ vextracti128_high($tmp$$XMMRegister, $tmp2$$XMMRegister);
-    __ vpmullq($tmp2$$XMMRegister, $tmp2$$XMMRegister, $tmp$$XMMRegister, 0);
-    __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0xE);
-    __ vpmullq($tmp2$$XMMRegister, $tmp2$$XMMRegister, $tmp$$XMMRegister, 0);
-    __ movdq($tmp$$XMMRegister, $src1$$Register);
-    __ vpmullq($tmp2$$XMMRegister, $tmp2$$XMMRegister, $tmp$$XMMRegister, 0);
-    __ movdq($dst$$Register, $tmp2$$XMMRegister);
+    int opcode = this->ideal_Opcode();
+    int vlen = vector_length(this, $src);
+    __ reduce_fp(opcode, vlen, $dst$$XMMRegister, $src$$XMMRegister, $vtmp1$$XMMRegister, $vtmp2$$XMMRegister);
   %}
   ins_pipe( pipe_slow );
 %}
-#endif
-
-// =======================MulReductionVF==========================================
 
-instruct vmul2F_reduction_reg(regF dst, vec src2, vec tmp) %{
-  predicate(n->in(2)->bottom_type()->is_vect()->length() == 2); // vector_length(src2) == 2
-  match(Set dst (MulReductionVF dst src2));
-  effect(TEMP dst, TEMP tmp);
-  format %{ "vector_mul2F_reduction $dst,$dst,$src2" %}
+instruct reduction16F(regF dst, legVec src, legVec vtmp1, legVec vtmp2) %{
+  predicate(n->in(2)->bottom_type()->is_vect()->length() == 16);
+  match(Set dst (AddReductionVF dst src));
+  match(Set dst (MulReductionVF dst src));
+  effect(TEMP dst, TEMP vtmp1, TEMP vtmp2);
+  format %{ "vector_reduction_float $dst,$src ; using $vtmp1, $vtmp2 as TEMP" %}
   ins_encode %{
-    if (UseAVX > 0) {
-      __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $src2$$XMMRegister);
-      __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x01);
-      __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
-    } else {
-      assert(UseSSE > 0, "required");
-      __ mulss($dst$$XMMRegister, $src2$$XMMRegister);
-      __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x01);
-      __ mulss($dst$$XMMRegister, $tmp$$XMMRegister);
-    }
+    int opcode = this->ideal_Opcode();
+    int vlen = vector_length(this, $src);
+    __ reduce_fp(opcode, vlen, $dst$$XMMRegister, $src$$XMMRegister, $vtmp1$$XMMRegister, $vtmp2$$XMMRegister);
   %}
   ins_pipe( pipe_slow );
 %}
 
-instruct vmul4F_reduction_reg(regF dst, vec src2, vec tmp) %{
-  predicate(n->in(2)->bottom_type()->is_vect()->length() == 4); // vector_length(src2) == 4
-  match(Set dst (MulReductionVF dst src2));
-  effect(TEMP dst, TEMP tmp);
-  format %{ "vector_mul4F_reduction $dst,$dst,$src2" %}
-  ins_encode %{
-    if (UseAVX > 0) {
-      __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $src2$$XMMRegister);
-      __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x01);
-      __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
-      __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x02);
-      __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
-      __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x03);
-      __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
-    } else {
-      assert(UseSSE > 0, "required");
-      __ mulss($dst$$XMMRegister, $src2$$XMMRegister);
-      __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x01);
-      __ mulss($dst$$XMMRegister, $tmp$$XMMRegister);
-      __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x02);
-      __ mulss($dst$$XMMRegister, $tmp$$XMMRegister);
-      __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x03);
-      __ mulss($dst$$XMMRegister, $tmp$$XMMRegister);
-    }
-  %}
-  ins_pipe( pipe_slow );
-%}
+// =======================Double Reduction==========================================
 
-instruct vmul8F_reduction_reg(regF dst, vec src2, vec tmp, vec tmp2) %{
-  predicate(n->in(2)->bottom_type()->is_vect()->length() == 8); // vector_length(src2) == 8
-  match(Set dst (MulReductionVF dst src2));
-  effect(TEMP tmp, TEMP dst, TEMP tmp2);
-  format %{ "vector_mul8F_reduction $dst,$dst,$src2" %}
-  ins_encode %{
-    assert(UseAVX > 0, "required");
-    __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $src2$$XMMRegister);
-    __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x01);
-    __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
-    __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x02);
-    __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
-    __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x03);
-    __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
-    __ vextractf128_high($tmp2$$XMMRegister, $src2$$XMMRegister);
-    __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister);
-    __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x01);
-    __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
-    __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x02);
-    __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
-    __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x03);
-    __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vmul16F_reduction_reg(regF dst, legVec src2, legVec tmp, legVec tmp2) %{
-  predicate(n->in(2)->bottom_type()->is_vect()->length() == 16); // vector_length(src2) == 16
-  match(Set dst (MulReductionVF dst src2));
-  effect(TEMP tmp, TEMP dst, TEMP tmp2);
-  format %{ "vector_mul16F_reduction $dst,$dst,$src2" %}
-  ins_encode %{
-    assert(UseAVX > 2, "required");
-    __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $src2$$XMMRegister);
-    __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x01);
-    __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
-    __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x02);
-    __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
-    __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x03);
-    __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
-    __ vextractf32x4($tmp2$$XMMRegister, $src2$$XMMRegister, 0x1);
-    __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister);
-    __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x01);
-    __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
-    __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x02);
-    __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
-    __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x03);
-    __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
-    __ vextractf32x4($tmp2$$XMMRegister, $src2$$XMMRegister, 0x2);
-    __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister);
-    __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x01);
-    __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
-    __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x02);
-    __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
-    __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x03);
-    __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
-    __ vextractf32x4($tmp2$$XMMRegister, $src2$$XMMRegister, 0x3);
-    __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister);
-    __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x01);
-    __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
-    __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x02);
-    __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
-    __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x03);
-    __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-// =======================MulReductionVD==========================================
-
-instruct vmul2D_reduction_reg(regD dst, vec src2, vec tmp) %{
-  predicate(n->in(2)->bottom_type()->is_vect()->length() == 2); // vector_length(src2) == 2
-  match(Set dst (MulReductionVD dst src2));
-  effect(TEMP dst, TEMP tmp);
-  format %{ "vector_mul2D_reduction $dst,$dst,$src2" %}
+instruct reduction2D(regD dst, vec src, vec vtmp) %{
+  predicate(n->in(2)->bottom_type()->is_vect()->length() == 2);
+  match(Set dst (AddReductionVD dst src));
+  match(Set dst (MulReductionVD dst src));
+  effect(TEMP dst, TEMP vtmp);
+  format %{ "vector_reduction_double $dst,$src ; using $vtmp as TEMP" %}
   ins_encode %{
-    if (UseAVX > 0) {
-      __ vmulsd($dst$$XMMRegister, $dst$$XMMRegister, $src2$$XMMRegister);
-      __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0xE);
-      __ vmulsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
-    } else {
-      assert(UseSSE > 0, "required");
-      __ mulsd($dst$$XMMRegister, $src2$$XMMRegister);
-      __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0xE);
-      __ mulsd($dst$$XMMRegister, $tmp$$XMMRegister);
-    }
+    int opcode = this->ideal_Opcode();
+    int vlen = vector_length(this, $src);
+    __ reduce_fp(opcode, vlen, $dst$$XMMRegister, $src$$XMMRegister, $vtmp$$XMMRegister);
   %}
   ins_pipe( pipe_slow );
 %}
 
-
-instruct vmul4D_reduction_reg(regD dst, vec src2, vec tmp, vec tmp2) %{
-  predicate(n->in(2)->bottom_type()->is_vect()->length() == 4); // vector_length(src2) == 2
-  match(Set dst (MulReductionVD dst src2));
-  effect(TEMP tmp, TEMP dst, TEMP tmp2);
-  format %{ "vector_mul4D_reduction  $dst,$dst,$src2" %}
+instruct reduction4D(regD dst, vec src, vec vtmp1, vec vtmp2) %{
+  predicate(n->in(2)->bottom_type()->is_vect()->length() == 4);
+  match(Set dst (AddReductionVD dst src));
+  match(Set dst (MulReductionVD dst src));
+  effect(TEMP dst, TEMP vtmp1, TEMP vtmp2);
+  format %{ "vector_reduction_double $dst,$src ; using $vtmp1, $vtmp2 as TEMP" %}
   ins_encode %{
-    assert(UseAVX > 0, "required");
-    __ vmulsd($dst$$XMMRegister, $dst$$XMMRegister, $src2$$XMMRegister);
-    __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0xE);
-    __ vmulsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
-    __ vextractf128_high($tmp2$$XMMRegister, $src2$$XMMRegister);
-    __ vmulsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister);
-    __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0xE);
-    __ vmulsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
+    int opcode = this->ideal_Opcode();
+    int vlen = vector_length(this, $src);
+    __ reduce_fp(opcode, vlen, $dst$$XMMRegister, $src$$XMMRegister, $vtmp1$$XMMRegister, $vtmp2$$XMMRegister);
   %}
   ins_pipe( pipe_slow );
 %}
 
-instruct vmul8D_reduction_reg(regD dst, legVec src2, legVec tmp, legVec tmp2) %{
-  predicate(n->in(2)->bottom_type()->is_vect()->length() == 8); // vector_length(src2) == 2
-  match(Set dst (MulReductionVD dst src2));
-  effect(TEMP tmp, TEMP dst, TEMP tmp2);
-  format %{ "vector_mul8D_reduction $dst,$dst,$src2" %}
+instruct reduction8D(regD dst, legVec src, legVec vtmp1, legVec vtmp2) %{
+  predicate(n->in(2)->bottom_type()->is_vect()->length() == 8);
+  match(Set dst (AddReductionVD dst src));
+  match(Set dst (MulReductionVD dst src));
+  effect(TEMP dst, TEMP vtmp1, TEMP vtmp2);
+  format %{ "vector_reduction_double $dst,$src ; using $vtmp1, $vtmp2 as TEMP" %}
   ins_encode %{
-    assert(UseAVX > 0, "required");
-    __ vmulsd($dst$$XMMRegister, $dst$$XMMRegister, $src2$$XMMRegister);
-    __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0xE);
-    __ vmulsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
-    __ vextractf32x4($tmp2$$XMMRegister, $src2$$XMMRegister, 0x1);
-    __ vmulsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister);
-    __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0xE);
-    __ vmulsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
-    __ vextractf32x4($tmp2$$XMMRegister, $src2$$XMMRegister, 0x2);
-    __ vmulsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister);
-    __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0xE);
-    __ vmulsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
-    __ vextractf32x4($tmp2$$XMMRegister, $src2$$XMMRegister, 0x3);
-    __ vmulsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister);
-    __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0xE);
-    __ vmulsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
+    int opcode = this->ideal_Opcode();
+    int vlen = vector_length(this, $src);
+    __ reduce_fp(opcode, vlen, $dst$$XMMRegister, $src$$XMMRegister, $vtmp1$$XMMRegister, $vtmp2$$XMMRegister);
   %}
   ins_pipe( pipe_slow );
 %}
 
 // ====================VECTOR ARITHMETIC=======================================
@@ -5782,11 +5127,11 @@
 %}
 
 // --------------------------------- Vector Multiply Add Add ----------------------------------
 
 instruct vmuladdaddS2I_reg(vec dst, vec src1, vec src2) %{
-  predicate(VM_Version::supports_vnni());
+  predicate(VM_Version::supports_avx512_vnni());
   match(Set dst (AddVI (MulAddVS2VI src1 src2) dst));
   format %{ "evpdpwssd $dst,$src1,$src2\t! muladdadd packedStoI" %}
   ins_encode %{
     assert(UseAVX > 2, "required");
     int vector_len = vector_length_encoding(this);
