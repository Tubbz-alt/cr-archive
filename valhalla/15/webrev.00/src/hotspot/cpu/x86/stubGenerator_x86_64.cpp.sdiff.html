<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/cpu/x86/stubGenerator_x86_64.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="stubGenerator_x86_32.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="templateTable_x86.cpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/cpu/x86/stubGenerator_x86_64.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
4443   __ vaesdeclast(xmm3, xmm3, xmm_reg, Assembler::AVX_512bit);
4444   __ vaesdeclast(xmm4, xmm4, xmm_reg, Assembler::AVX_512bit);
4445   __ vaesdeclast(xmm5, xmm5, xmm_reg, Assembler::AVX_512bit);
4446   __ vaesdeclast(xmm6, xmm6, xmm_reg, Assembler::AVX_512bit);
4447   __ vaesdeclast(xmm7, xmm7, xmm_reg, Assembler::AVX_512bit);
4448   __ vaesdeclast(xmm8, xmm8, xmm_reg, Assembler::AVX_512bit);
4449 }
4450 
4451   void ev_load_key(XMMRegister xmmdst, Register key, int offset, XMMRegister xmm_shuf_mask = NULL) {
4452     __ movdqu(xmmdst, Address(key, offset));
4453     if (xmm_shuf_mask != NULL) {
4454       __ pshufb(xmmdst, xmm_shuf_mask);
4455     } else {
4456       __ pshufb(xmmdst, ExternalAddress(StubRoutines::x86::key_shuffle_mask_addr()));
4457     }
4458     __ evshufi64x2(xmmdst, xmmdst, xmmdst, 0x0, Assembler::AVX_512bit);
4459 
4460   }
4461 
4462 address generate_cipherBlockChaining_decryptVectorAESCrypt() {
<span class="line-modified">4463     assert(VM_Version::supports_vaes(), &quot;need AES instructions and misaligned SSE support&quot;);</span>
4464     __ align(CodeEntryAlignment);
4465     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;cipherBlockChaining_decryptAESCrypt&quot;);
4466     address start = __ pc();
4467 
4468     const Register from = c_rarg0;  // source array address
4469     const Register to = c_rarg1;  // destination array address
4470     const Register key = c_rarg2;  // key array address
4471     const Register rvec = c_rarg3;  // r byte array initialized from initvector array address
4472     // and left with the results of the last encryption block
4473 #ifndef _WIN64
4474     const Register len_reg = c_rarg4;  // src len (must be multiple of blocksize 16)
4475 #else
4476     const Address  len_mem(rbp, 6 * wordSize);  // length is on stack on Win64
4477     const Register len_reg = r11;      // pick the volatile windows register
4478 #endif
4479 
4480     Label Loop, Loop1, L_128, L_256, L_192, KEY_192, KEY_256, Loop2, Lcbc_dec_rem_loop,
4481           Lcbc_dec_rem_last, Lcbc_dec_ret, Lcbc_dec_rem, Lcbc_exit;
4482 
4483     __ enter();
</pre>
<hr />
<pre>
5748 #endif
5749     __ push(tmp5);
5750 
5751     // Rename temps used throughout the code.
5752     const Register idx = tmp1;
5753     const Register nIdx = tmp2;
5754 
5755     __ xorl(idx, idx);
5756 
5757     // Start right shift from end of the array.
5758     // For example, if #iteration = 4 and newIdx = 1
5759     // then dest[4] = src[4] &gt;&gt; shiftCount  | src[3] &lt;&lt;&lt; (shiftCount - 32)
5760     // if #iteration = 4 and newIdx = 0
5761     // then dest[3] = src[4] &gt;&gt; shiftCount  | src[3] &lt;&lt;&lt; (shiftCount - 32)
5762     __ movl(idx, totalNumIter);
5763     __ movl(nIdx, idx);
5764     __ addl(nIdx, newIdx);
5765 
5766     // If vectorization is enabled, check if the number of iterations is at least 64
5767     // If not, then go to ShifTwo processing 2 iterations
<span class="line-modified">5768     if (VM_Version::supports_vbmi2()) {</span>
5769       __ cmpptr(totalNumIter, (AVX3Threshold/64));
5770       __ jcc(Assembler::less, ShiftTwo);
5771 
5772       if (AVX3Threshold &lt; 16 * 64) {
5773         __ cmpl(totalNumIter, 16);
5774         __ jcc(Assembler::less, ShiftTwo);
5775       }
5776       __ evpbroadcastd(x0, shiftCount, Assembler::AVX_512bit);
5777       __ subl(idx, 16);
5778       __ subl(nIdx, 16);
5779       __ BIND(Shift512Loop);
5780       __ evmovdqul(x2, Address(oldArr, idx, Address::times_4, 4), Assembler::AVX_512bit);
5781       __ evmovdqul(x1, Address(oldArr, idx, Address::times_4), Assembler::AVX_512bit);
5782       __ vpshrdvd(x2, x1, x0, Assembler::AVX_512bit);
5783       __ evmovdqul(Address(newArr, nIdx, Address::times_4), x2, Assembler::AVX_512bit);
5784       __ subl(nIdx, 16);
5785       __ subl(idx, 16);
5786       __ jcc(Assembler::greaterEqual, Shift512Loop);
5787       __ addl(idx, 16);
5788       __ addl(nIdx, 16);
</pre>
<hr />
<pre>
5872     // For windows, since last argument is on stack, we need to move it to the appropriate register.
5873     __ movl(totalNumIter, Address(rsp, 6 * wordSize));
5874     // Save callee save registers.
5875     __ push(tmp3);
5876     __ push(tmp4);
5877 #endif
5878     __ push(tmp5);
5879 
5880     // Rename temps used throughout the code
5881     const Register idx = tmp1;
5882     const Register numIterTmp = tmp2;
5883 
5884     // Start idx from zero.
5885     __ xorl(idx, idx);
5886     // Compute interior pointer for new array. We do this so that we can use same index for both old and new arrays.
5887     __ lea(newArr, Address(newArr, newIdx, Address::times_4));
5888     __ movl(numIterTmp, totalNumIter);
5889 
5890     // If vectorization is enabled, check if the number of iterations is at least 64
5891     // If not, then go to ShiftTwo shifting two numbers at a time
<span class="line-modified">5892     if (VM_Version::supports_vbmi2()) {</span>
5893       __ cmpl(totalNumIter, (AVX3Threshold/64));
5894       __ jcc(Assembler::less, ShiftTwo);
5895 
5896       if (AVX3Threshold &lt; 16 * 64) {
5897         __ cmpl(totalNumIter, 16);
5898         __ jcc(Assembler::less, ShiftTwo);
5899       }
5900       __ evpbroadcastd(x0, shiftCount, Assembler::AVX_512bit);
5901       __ subl(numIterTmp, 16);
5902       __ BIND(Shift512Loop);
5903       __ evmovdqul(x1, Address(oldArr, idx, Address::times_4), Assembler::AVX_512bit);
5904       __ evmovdqul(x2, Address(oldArr, idx, Address::times_4, 0x4), Assembler::AVX_512bit);
5905       __ vpshldvd(x1, x2, x0, Assembler::AVX_512bit);
5906       __ evmovdqul(Address(newArr, idx, Address::times_4), x1, Assembler::AVX_512bit);
5907       __ addl(idx, 16);
5908       __ subl(numIterTmp, 16);
5909       __ jcc(Assembler::greaterEqual, Shift512Loop);
5910       __ addl(numIterTmp, 16);
5911     }
5912     __ BIND(ShiftTwo);
</pre>
<hr />
<pre>
6607     StubRoutines::x86::_vector_short_to_byte_mask = generate_vector_mask(&quot;vector_short_to_byte_mask&quot;, 0x00ff00ff00ff00ff);
6608     StubRoutines::x86::_vector_byte_perm_mask = generate_vector_byte_perm_mask(&quot;vector_byte_perm_mask&quot;);
6609     StubRoutines::x86::_vector_long_sign_mask = generate_vector_mask(&quot;vector_long_sign_mask&quot;, 0x8000000000000000);
6610 
6611     // support for verify_oop (must happen after universe_init)
6612     StubRoutines::_verify_oop_subroutine_entry = generate_verify_oop();
6613 
6614     // data cache line writeback
6615     StubRoutines::_data_cache_writeback = generate_data_cache_writeback();
6616     StubRoutines::_data_cache_writeback_sync = generate_data_cache_writeback_sync();
6617 
6618     // arraycopy stubs used by compilers
6619     generate_arraycopy_stubs();
6620 
6621     // don&#39;t bother generating these AES intrinsic stubs unless global flag is set
6622     if (UseAESIntrinsics) {
6623       StubRoutines::x86::_key_shuffle_mask_addr = generate_key_shuffle_mask();  // needed by the others
6624       StubRoutines::_aescrypt_encryptBlock = generate_aescrypt_encryptBlock();
6625       StubRoutines::_aescrypt_decryptBlock = generate_aescrypt_decryptBlock();
6626       StubRoutines::_cipherBlockChaining_encryptAESCrypt = generate_cipherBlockChaining_encryptAESCrypt();
<span class="line-modified">6627       if (VM_Version::supports_vaes() &amp;&amp;  VM_Version::supports_avx512vl() &amp;&amp; VM_Version::supports_avx512dq() ) {</span>
6628         StubRoutines::_cipherBlockChaining_decryptAESCrypt = generate_cipherBlockChaining_decryptVectorAESCrypt();
6629         StubRoutines::_electronicCodeBook_encryptAESCrypt = generate_electronicCodeBook_encryptAESCrypt();
6630         StubRoutines::_electronicCodeBook_decryptAESCrypt = generate_electronicCodeBook_decryptAESCrypt();
6631       } else {
6632         StubRoutines::_cipherBlockChaining_decryptAESCrypt = generate_cipherBlockChaining_decryptAESCrypt_Parallel();
6633       }
6634     }
6635     if (UseAESCTRIntrinsics) {
<span class="line-modified">6636       if (VM_Version::supports_vaes() &amp;&amp; VM_Version::supports_avx512bw() &amp;&amp; VM_Version::supports_avx512vl()) {</span>
6637         StubRoutines::x86::_counter_mask_addr = counter_mask_addr();
6638         StubRoutines::_counterMode_AESCrypt = generate_counterMode_VectorAESCrypt();
6639       } else {
6640         StubRoutines::x86::_counter_shuffle_mask_addr = generate_counter_shuffle_mask();
6641         StubRoutines::_counterMode_AESCrypt = generate_counterMode_AESCrypt_Parallel();
6642       }
6643     }
6644 
6645     if (UseSHA1Intrinsics) {
6646       StubRoutines::x86::_upper_word_mask_addr = generate_upper_word_mask();
6647       StubRoutines::x86::_shuffle_byte_flip_mask_addr = generate_shuffle_byte_flip_mask();
6648       StubRoutines::_sha1_implCompress = generate_sha1_implCompress(false, &quot;sha1_implCompress&quot;);
6649       StubRoutines::_sha1_implCompressMB = generate_sha1_implCompress(true, &quot;sha1_implCompressMB&quot;);
6650     }
6651     if (UseSHA256Intrinsics) {
6652       StubRoutines::x86::_k256_adr = (address)StubRoutines::x86::_k256;
6653       char* dst = (char*)StubRoutines::x86::_k256_W;
6654       char* src = (char*)StubRoutines::x86::_k256;
6655       for (int ii = 0; ii &lt; 16; ++ii) {
6656         memcpy(dst + 32 * ii,      src + 16 * ii, 16);
</pre>
<hr />
<pre>
6697                                                        &amp;StubRoutines::_safefetch32_fault_pc,
6698                                                        &amp;StubRoutines::_safefetch32_continuation_pc);
6699     generate_safefetch(&quot;SafeFetchN&quot;, sizeof(intptr_t), &amp;StubRoutines::_safefetchN_entry,
6700                                                        &amp;StubRoutines::_safefetchN_fault_pc,
6701                                                        &amp;StubRoutines::_safefetchN_continuation_pc);
6702 
6703     BarrierSetNMethod* bs_nm = BarrierSet::barrier_set()-&gt;barrier_set_nmethod();
6704     if (bs_nm != NULL) {
6705       StubRoutines::x86::_method_entry_barrier = generate_method_entry_barrier();
6706     }
6707 #ifdef COMPILER2
6708     if (UseMultiplyToLenIntrinsic) {
6709       StubRoutines::_multiplyToLen = generate_multiplyToLen();
6710     }
6711     if (UseSquareToLenIntrinsic) {
6712       StubRoutines::_squareToLen = generate_squareToLen();
6713     }
6714     if (UseMulAddIntrinsic) {
6715       StubRoutines::_mulAdd = generate_mulAdd();
6716     }
<span class="line-modified">6717     if (VM_Version::supports_vbmi2()) {</span>
6718       StubRoutines::_bigIntegerRightShiftWorker = generate_bigIntegerRightShift();
6719       StubRoutines::_bigIntegerLeftShiftWorker = generate_bigIntegerLeftShift();
6720     }
6721 #ifndef _WINDOWS
6722     if (UseMontgomeryMultiplyIntrinsic) {
6723       StubRoutines::_montgomeryMultiply
6724         = CAST_FROM_FN_PTR(address, SharedRuntime::montgomery_multiply);
6725     }
6726     if (UseMontgomerySquareIntrinsic) {
6727       StubRoutines::_montgomerySquare
6728         = CAST_FROM_FN_PTR(address, SharedRuntime::montgomery_square);
6729     }
6730 #endif // WINDOWS
6731 #endif // COMPILER2
6732 
6733     if (UseVectorizedMismatchIntrinsic) {
6734       StubRoutines::_vectorizedMismatch = generate_vectorizedMismatch();
6735     }
6736   }
6737 
</pre>
</td>
<td>
<hr />
<pre>
4443   __ vaesdeclast(xmm3, xmm3, xmm_reg, Assembler::AVX_512bit);
4444   __ vaesdeclast(xmm4, xmm4, xmm_reg, Assembler::AVX_512bit);
4445   __ vaesdeclast(xmm5, xmm5, xmm_reg, Assembler::AVX_512bit);
4446   __ vaesdeclast(xmm6, xmm6, xmm_reg, Assembler::AVX_512bit);
4447   __ vaesdeclast(xmm7, xmm7, xmm_reg, Assembler::AVX_512bit);
4448   __ vaesdeclast(xmm8, xmm8, xmm_reg, Assembler::AVX_512bit);
4449 }
4450 
4451   void ev_load_key(XMMRegister xmmdst, Register key, int offset, XMMRegister xmm_shuf_mask = NULL) {
4452     __ movdqu(xmmdst, Address(key, offset));
4453     if (xmm_shuf_mask != NULL) {
4454       __ pshufb(xmmdst, xmm_shuf_mask);
4455     } else {
4456       __ pshufb(xmmdst, ExternalAddress(StubRoutines::x86::key_shuffle_mask_addr()));
4457     }
4458     __ evshufi64x2(xmmdst, xmmdst, xmmdst, 0x0, Assembler::AVX_512bit);
4459 
4460   }
4461 
4462 address generate_cipherBlockChaining_decryptVectorAESCrypt() {
<span class="line-modified">4463     assert(VM_Version::supports_avx512_vaes(), &quot;need AES instructions and misaligned SSE support&quot;);</span>
4464     __ align(CodeEntryAlignment);
4465     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;cipherBlockChaining_decryptAESCrypt&quot;);
4466     address start = __ pc();
4467 
4468     const Register from = c_rarg0;  // source array address
4469     const Register to = c_rarg1;  // destination array address
4470     const Register key = c_rarg2;  // key array address
4471     const Register rvec = c_rarg3;  // r byte array initialized from initvector array address
4472     // and left with the results of the last encryption block
4473 #ifndef _WIN64
4474     const Register len_reg = c_rarg4;  // src len (must be multiple of blocksize 16)
4475 #else
4476     const Address  len_mem(rbp, 6 * wordSize);  // length is on stack on Win64
4477     const Register len_reg = r11;      // pick the volatile windows register
4478 #endif
4479 
4480     Label Loop, Loop1, L_128, L_256, L_192, KEY_192, KEY_256, Loop2, Lcbc_dec_rem_loop,
4481           Lcbc_dec_rem_last, Lcbc_dec_ret, Lcbc_dec_rem, Lcbc_exit;
4482 
4483     __ enter();
</pre>
<hr />
<pre>
5748 #endif
5749     __ push(tmp5);
5750 
5751     // Rename temps used throughout the code.
5752     const Register idx = tmp1;
5753     const Register nIdx = tmp2;
5754 
5755     __ xorl(idx, idx);
5756 
5757     // Start right shift from end of the array.
5758     // For example, if #iteration = 4 and newIdx = 1
5759     // then dest[4] = src[4] &gt;&gt; shiftCount  | src[3] &lt;&lt;&lt; (shiftCount - 32)
5760     // if #iteration = 4 and newIdx = 0
5761     // then dest[3] = src[4] &gt;&gt; shiftCount  | src[3] &lt;&lt;&lt; (shiftCount - 32)
5762     __ movl(idx, totalNumIter);
5763     __ movl(nIdx, idx);
5764     __ addl(nIdx, newIdx);
5765 
5766     // If vectorization is enabled, check if the number of iterations is at least 64
5767     // If not, then go to ShifTwo processing 2 iterations
<span class="line-modified">5768     if (VM_Version::supports_avx512_vbmi2()) {</span>
5769       __ cmpptr(totalNumIter, (AVX3Threshold/64));
5770       __ jcc(Assembler::less, ShiftTwo);
5771 
5772       if (AVX3Threshold &lt; 16 * 64) {
5773         __ cmpl(totalNumIter, 16);
5774         __ jcc(Assembler::less, ShiftTwo);
5775       }
5776       __ evpbroadcastd(x0, shiftCount, Assembler::AVX_512bit);
5777       __ subl(idx, 16);
5778       __ subl(nIdx, 16);
5779       __ BIND(Shift512Loop);
5780       __ evmovdqul(x2, Address(oldArr, idx, Address::times_4, 4), Assembler::AVX_512bit);
5781       __ evmovdqul(x1, Address(oldArr, idx, Address::times_4), Assembler::AVX_512bit);
5782       __ vpshrdvd(x2, x1, x0, Assembler::AVX_512bit);
5783       __ evmovdqul(Address(newArr, nIdx, Address::times_4), x2, Assembler::AVX_512bit);
5784       __ subl(nIdx, 16);
5785       __ subl(idx, 16);
5786       __ jcc(Assembler::greaterEqual, Shift512Loop);
5787       __ addl(idx, 16);
5788       __ addl(nIdx, 16);
</pre>
<hr />
<pre>
5872     // For windows, since last argument is on stack, we need to move it to the appropriate register.
5873     __ movl(totalNumIter, Address(rsp, 6 * wordSize));
5874     // Save callee save registers.
5875     __ push(tmp3);
5876     __ push(tmp4);
5877 #endif
5878     __ push(tmp5);
5879 
5880     // Rename temps used throughout the code
5881     const Register idx = tmp1;
5882     const Register numIterTmp = tmp2;
5883 
5884     // Start idx from zero.
5885     __ xorl(idx, idx);
5886     // Compute interior pointer for new array. We do this so that we can use same index for both old and new arrays.
5887     __ lea(newArr, Address(newArr, newIdx, Address::times_4));
5888     __ movl(numIterTmp, totalNumIter);
5889 
5890     // If vectorization is enabled, check if the number of iterations is at least 64
5891     // If not, then go to ShiftTwo shifting two numbers at a time
<span class="line-modified">5892     if (VM_Version::supports_avx512_vbmi2()) {</span>
5893       __ cmpl(totalNumIter, (AVX3Threshold/64));
5894       __ jcc(Assembler::less, ShiftTwo);
5895 
5896       if (AVX3Threshold &lt; 16 * 64) {
5897         __ cmpl(totalNumIter, 16);
5898         __ jcc(Assembler::less, ShiftTwo);
5899       }
5900       __ evpbroadcastd(x0, shiftCount, Assembler::AVX_512bit);
5901       __ subl(numIterTmp, 16);
5902       __ BIND(Shift512Loop);
5903       __ evmovdqul(x1, Address(oldArr, idx, Address::times_4), Assembler::AVX_512bit);
5904       __ evmovdqul(x2, Address(oldArr, idx, Address::times_4, 0x4), Assembler::AVX_512bit);
5905       __ vpshldvd(x1, x2, x0, Assembler::AVX_512bit);
5906       __ evmovdqul(Address(newArr, idx, Address::times_4), x1, Assembler::AVX_512bit);
5907       __ addl(idx, 16);
5908       __ subl(numIterTmp, 16);
5909       __ jcc(Assembler::greaterEqual, Shift512Loop);
5910       __ addl(numIterTmp, 16);
5911     }
5912     __ BIND(ShiftTwo);
</pre>
<hr />
<pre>
6607     StubRoutines::x86::_vector_short_to_byte_mask = generate_vector_mask(&quot;vector_short_to_byte_mask&quot;, 0x00ff00ff00ff00ff);
6608     StubRoutines::x86::_vector_byte_perm_mask = generate_vector_byte_perm_mask(&quot;vector_byte_perm_mask&quot;);
6609     StubRoutines::x86::_vector_long_sign_mask = generate_vector_mask(&quot;vector_long_sign_mask&quot;, 0x8000000000000000);
6610 
6611     // support for verify_oop (must happen after universe_init)
6612     StubRoutines::_verify_oop_subroutine_entry = generate_verify_oop();
6613 
6614     // data cache line writeback
6615     StubRoutines::_data_cache_writeback = generate_data_cache_writeback();
6616     StubRoutines::_data_cache_writeback_sync = generate_data_cache_writeback_sync();
6617 
6618     // arraycopy stubs used by compilers
6619     generate_arraycopy_stubs();
6620 
6621     // don&#39;t bother generating these AES intrinsic stubs unless global flag is set
6622     if (UseAESIntrinsics) {
6623       StubRoutines::x86::_key_shuffle_mask_addr = generate_key_shuffle_mask();  // needed by the others
6624       StubRoutines::_aescrypt_encryptBlock = generate_aescrypt_encryptBlock();
6625       StubRoutines::_aescrypt_decryptBlock = generate_aescrypt_decryptBlock();
6626       StubRoutines::_cipherBlockChaining_encryptAESCrypt = generate_cipherBlockChaining_encryptAESCrypt();
<span class="line-modified">6627       if (VM_Version::supports_avx512_vaes() &amp;&amp;  VM_Version::supports_avx512vl() &amp;&amp; VM_Version::supports_avx512dq() ) {</span>
6628         StubRoutines::_cipherBlockChaining_decryptAESCrypt = generate_cipherBlockChaining_decryptVectorAESCrypt();
6629         StubRoutines::_electronicCodeBook_encryptAESCrypt = generate_electronicCodeBook_encryptAESCrypt();
6630         StubRoutines::_electronicCodeBook_decryptAESCrypt = generate_electronicCodeBook_decryptAESCrypt();
6631       } else {
6632         StubRoutines::_cipherBlockChaining_decryptAESCrypt = generate_cipherBlockChaining_decryptAESCrypt_Parallel();
6633       }
6634     }
6635     if (UseAESCTRIntrinsics) {
<span class="line-modified">6636       if (VM_Version::supports_avx512_vaes() &amp;&amp; VM_Version::supports_avx512bw() &amp;&amp; VM_Version::supports_avx512vl()) {</span>
6637         StubRoutines::x86::_counter_mask_addr = counter_mask_addr();
6638         StubRoutines::_counterMode_AESCrypt = generate_counterMode_VectorAESCrypt();
6639       } else {
6640         StubRoutines::x86::_counter_shuffle_mask_addr = generate_counter_shuffle_mask();
6641         StubRoutines::_counterMode_AESCrypt = generate_counterMode_AESCrypt_Parallel();
6642       }
6643     }
6644 
6645     if (UseSHA1Intrinsics) {
6646       StubRoutines::x86::_upper_word_mask_addr = generate_upper_word_mask();
6647       StubRoutines::x86::_shuffle_byte_flip_mask_addr = generate_shuffle_byte_flip_mask();
6648       StubRoutines::_sha1_implCompress = generate_sha1_implCompress(false, &quot;sha1_implCompress&quot;);
6649       StubRoutines::_sha1_implCompressMB = generate_sha1_implCompress(true, &quot;sha1_implCompressMB&quot;);
6650     }
6651     if (UseSHA256Intrinsics) {
6652       StubRoutines::x86::_k256_adr = (address)StubRoutines::x86::_k256;
6653       char* dst = (char*)StubRoutines::x86::_k256_W;
6654       char* src = (char*)StubRoutines::x86::_k256;
6655       for (int ii = 0; ii &lt; 16; ++ii) {
6656         memcpy(dst + 32 * ii,      src + 16 * ii, 16);
</pre>
<hr />
<pre>
6697                                                        &amp;StubRoutines::_safefetch32_fault_pc,
6698                                                        &amp;StubRoutines::_safefetch32_continuation_pc);
6699     generate_safefetch(&quot;SafeFetchN&quot;, sizeof(intptr_t), &amp;StubRoutines::_safefetchN_entry,
6700                                                        &amp;StubRoutines::_safefetchN_fault_pc,
6701                                                        &amp;StubRoutines::_safefetchN_continuation_pc);
6702 
6703     BarrierSetNMethod* bs_nm = BarrierSet::barrier_set()-&gt;barrier_set_nmethod();
6704     if (bs_nm != NULL) {
6705       StubRoutines::x86::_method_entry_barrier = generate_method_entry_barrier();
6706     }
6707 #ifdef COMPILER2
6708     if (UseMultiplyToLenIntrinsic) {
6709       StubRoutines::_multiplyToLen = generate_multiplyToLen();
6710     }
6711     if (UseSquareToLenIntrinsic) {
6712       StubRoutines::_squareToLen = generate_squareToLen();
6713     }
6714     if (UseMulAddIntrinsic) {
6715       StubRoutines::_mulAdd = generate_mulAdd();
6716     }
<span class="line-modified">6717     if (VM_Version::supports_avx512_vbmi2()) {</span>
6718       StubRoutines::_bigIntegerRightShiftWorker = generate_bigIntegerRightShift();
6719       StubRoutines::_bigIntegerLeftShiftWorker = generate_bigIntegerLeftShift();
6720     }
6721 #ifndef _WINDOWS
6722     if (UseMontgomeryMultiplyIntrinsic) {
6723       StubRoutines::_montgomeryMultiply
6724         = CAST_FROM_FN_PTR(address, SharedRuntime::montgomery_multiply);
6725     }
6726     if (UseMontgomerySquareIntrinsic) {
6727       StubRoutines::_montgomerySquare
6728         = CAST_FROM_FN_PTR(address, SharedRuntime::montgomery_square);
6729     }
6730 #endif // WINDOWS
6731 #endif // COMPILER2
6732 
6733     if (UseVectorizedMismatchIntrinsic) {
6734       StubRoutines::_vectorizedMismatch = generate_vectorizedMismatch();
6735     }
6736   }
6737 
</pre>
</td>
</tr>
</table>
<center><a href="stubGenerator_x86_32.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="templateTable_x86.cpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>