<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/cpu/x86/x86.ad</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="vtableStubs_x86_32.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="x86_32.ad.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/cpu/x86/x86.ad</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
1160     // a call be deoptimization.  (4932387)
1161     // Note that this value is also credited (in output.cpp) to
1162     // the size of the code section.
1163     return 5 + NativeJump::instruction_size; // pushl(); jmp;
1164   }
1165 #endif
1166 };
1167 
1168 %} // end source_hpp
1169 
1170 source %{
1171 
1172 #include &quot;opto/addnode.hpp&quot;
1173 
1174 // Emit exception handler code.
1175 // Stuff framesize into a register and call a VM stub routine.
1176 int HandlerImpl::emit_exception_handler(CodeBuffer&amp; cbuf) {
1177 
1178   // Note that the code buffer&#39;s insts_mark is always relative to insts.
1179   // That&#39;s why we must use the macroassembler to generate a handler.
<span class="line-modified">1180   MacroAssembler _masm(&amp;cbuf);</span>
1181   address base = __ start_a_stub(size_exception_handler());
1182   if (base == NULL) {
1183     ciEnv::current()-&gt;record_failure(&quot;CodeCache is full&quot;);
1184     return 0;  // CodeBuffer::expand failed
1185   }
1186   int offset = __ offset();
1187   __ jump(RuntimeAddress(OptoRuntime::exception_blob()-&gt;entry_point()));
1188   assert(__ offset() - offset &lt;= (int) size_exception_handler(), &quot;overflow&quot;);
1189   __ end_a_stub();
1190   return offset;
1191 }
1192 
1193 // Emit deopt handler code.
1194 int HandlerImpl::emit_deopt_handler(CodeBuffer&amp; cbuf) {
1195 
1196   // Note that the code buffer&#39;s insts_mark is always relative to insts.
1197   // That&#39;s why we must use the macroassembler to generate a handler.
<span class="line-modified">1198   MacroAssembler _masm(&amp;cbuf);</span>
1199   address base = __ start_a_stub(size_deopt_handler());
1200   if (base == NULL) {
1201     ciEnv::current()-&gt;record_failure(&quot;CodeCache is full&quot;);
1202     return 0;  // CodeBuffer::expand failed
1203   }
1204   int offset = __ offset();
1205 
1206 #ifdef _LP64
1207   address the_pc = (address) __ pc();
1208   Label next;
1209   // push a &quot;the_pc&quot; on the stack without destroying any registers
1210   // as they all may be live.
1211 
1212   // push address of &quot;next&quot;
1213   __ call(next, relocInfo::none); // reloc none is fine since it is a disp32
1214   __ bind(next);
1215   // adjust it so it matches &quot;the_pc&quot;
1216   __ subptr(Address(rsp, 0), __ offset() - offset);
1217 #else
1218   InternalAddress here(__ pc());
</pre>
<hr />
<pre>
1245   static address vector_long_sign_mask() { return StubRoutines::x86::vector_long_sign_mask(); }
1246 
1247 //=============================================================================
1248 const bool Matcher::match_rule_supported(int opcode) {
1249   if (!has_match_rule(opcode)) {
1250     return false; // no match rule present
1251   }
1252   switch (opcode) {
1253     case Op_AbsVL:
1254       if (UseAVX &lt; 3) {
1255         return false;
1256       }
1257       break;
1258     case Op_PopCountI:
1259     case Op_PopCountL:
1260       if (!UsePopCountInstruction) {
1261         return false;
1262       }
1263       break;
1264     case Op_PopCountVI:
<span class="line-modified">1265       if (!UsePopCountInstruction || !VM_Version::supports_vpopcntdq()) {</span>
1266         return false;
1267       }
1268       break;
1269     case Op_MulVI:
1270       if ((UseSSE &lt; 4) &amp;&amp; (UseAVX &lt; 1)) { // only with SSE4_1 or AVX
1271         return false;
1272       }
1273       break;
1274     case Op_MulVL:
1275     case Op_MulReductionVL:
1276       if (VM_Version::supports_avx512dq() == false) {
1277         return false;
1278       }
1279       break;
<span class="line-removed">1280     case Op_AddReductionVL:</span>
<span class="line-removed">1281       if (UseAVX &lt; 3) { // only EVEX : vector connectivity becomes an issue here</span>
<span class="line-removed">1282         return false;</span>
<span class="line-removed">1283       }</span>
<span class="line-removed">1284       break;</span>
1285     case Op_AbsVB:
1286     case Op_AbsVS:
1287     case Op_AbsVI:
1288     case Op_AddReductionVI:
<span class="line-modified">1289       if (UseSSE &lt; 3 || !VM_Version::supports_ssse3()) { // requires at least SSSE3</span>



1290         return false;
1291       }
1292       break;
1293     case Op_MulReductionVI:
1294       if (UseSSE &lt; 4) { // requires at least SSE4
1295         return false;
1296       }
1297       break;
<span class="line-removed">1298     case Op_AddReductionVF:</span>
<span class="line-removed">1299     case Op_AddReductionVD:</span>
<span class="line-removed">1300     case Op_MulReductionVF:</span>
<span class="line-removed">1301     case Op_MulReductionVD:</span>
<span class="line-removed">1302       if (UseSSE &lt; 1) { // requires at least SSE</span>
<span class="line-removed">1303         return false;</span>
<span class="line-removed">1304       }</span>
<span class="line-removed">1305       break;</span>
1306     case Op_SqrtVD:
1307     case Op_SqrtVF:
1308       if (UseAVX &lt; 1) { // enabled for AVX only
1309         return false;
1310       }
1311       break;
1312     case Op_CompareAndSwapL:
1313 #ifdef _LP64
1314     case Op_CompareAndSwapP:
1315 #endif
1316       if (!VM_Version::supports_cx8()) {
1317         return false;
1318       }
1319       break;
1320     case Op_CMoveVF:
1321     case Op_CMoveVD:
1322       if (UseAVX &lt; 1 || UseAVX &gt; 2) {
1323         return false;
1324       }
1325       break;
1326     case Op_StrIndexOf:
1327       if (!UseSSE42Intrinsics) {
1328         return false;
1329       }
1330       break;
1331     case Op_StrIndexOfChar:
1332       if (!UseSSE42Intrinsics) {
1333         return false;
1334       }
1335       break;
1336     case Op_OnSpinWait:
1337       if (VM_Version::supports_on_spin_wait() == false) {
1338         return false;
1339       }
1340       break;
<span class="line-removed">1341     case Op_MulAddVS2VI:</span>
<span class="line-removed">1342     case Op_RShiftVL:</span>
<span class="line-removed">1343     case Op_AbsVD:</span>
<span class="line-removed">1344     case Op_NegVD:</span>
<span class="line-removed">1345       if (UseSSE &lt; 2) {</span>
<span class="line-removed">1346         return false;</span>
<span class="line-removed">1347       }</span>
<span class="line-removed">1348       break;</span>
1349     case Op_MulVB:
1350     case Op_LShiftVB:
1351     case Op_RShiftVB:
1352     case Op_URShiftVB:
1353       if (UseSSE &lt; 4) {
1354         return false;
1355       }
1356       break;
1357 #ifdef _LP64
1358     case Op_MaxD:
1359     case Op_MaxF:
1360     case Op_MinD:
1361     case Op_MinF:
1362       if (UseAVX &lt; 1) { // enabled for AVX only
1363         return false;
1364       }
1365       break;
1366 #endif
1367     case Op_CacheWB:
1368     case Op_CacheWBPreSync:
1369     case Op_CacheWBPostSync:
1370       if (!VM_Version::supports_data_cache_line_flush()) {
1371         return false;
1372       }
1373       break;
1374     case Op_RoundDoubleMode:
1375       if (UseSSE &lt; 4) {
1376         return false;
1377       }
1378       break;
1379     case Op_RoundDoubleModeV:
1380       if (VM_Version::supports_avx() == false) {
1381         return false; // 128bit vroundpd is not available
1382       }
1383       break;


















1384   }
1385   return true;  // Match rules are supported by default.
1386 }
1387 
1388 //------------------------------------------------------------------------
1389 
1390 // Identify extra cases that we might want to provide match rules for vector nodes and
1391 // other intrinsics guarded with vector length (vlen) and element type (bt).
1392 const bool Matcher::match_rule_supported_vector(int opcode, int vlen, BasicType bt) {
1393   if (!match_rule_supported(opcode)) {
1394     return false;
1395   }
1396   // Matcher::vector_size_supported() restricts vector sizes in the following way (see Matcher::vector_width_in_bytes):
1397   //   * SSE2 supports 128bit vectors for all types;
1398   //   * AVX1 supports 256bit vectors only for FLOAT and DOUBLE types;
1399   //   * AVX2 supports 256bit vectors for all types;
1400   //   * AVX512F supports 512bit vectors only for INT, FLOAT, and DOUBLE types;
1401   //   * AVX512BW supports 512bit vectors for BYTE, SHORT, and CHAR types.
1402   // There&#39;s also a limit on minimum vector size supported: 2 elements (or 4 bytes for BYTE).
1403   // And MaxVectorSize is taken into account as well.
</pre>
<hr />
<pre>
1656     mstack.push(off, Visit);
1657     mstack.push(m-&gt;in(AddPNode::Base), Pre_Visit);
1658     return true;
1659   } else if (clone_shift(off, this, mstack, address_visited)) {
1660     address_visited.test_set(m-&gt;_idx); // Flag as address_visited
1661     mstack.push(m-&gt;in(AddPNode::Address), Pre_Visit);
1662     mstack.push(m-&gt;in(AddPNode::Base), Pre_Visit);
1663     return true;
1664   }
1665   return false;
1666 }
1667 
1668 void Compile::reshape_address(AddPNode* addp) {
1669 }
1670 
1671 static inline uint vector_length(const MachNode* n) {
1672   const TypeVect* vt = n-&gt;bottom_type()-&gt;is_vect();
1673   return vt-&gt;length();
1674 }
1675 






1676 static inline uint vector_length_in_bytes(const MachNode* n) {
1677   const TypeVect* vt = n-&gt;bottom_type()-&gt;is_vect();
1678   return vt-&gt;length_in_bytes();
1679 }
1680 
1681 static inline uint vector_length_in_bytes(const MachNode* use, MachOper* opnd) {
1682   uint def_idx = use-&gt;operand_index(opnd);
1683   Node* def = use-&gt;in(def_idx);
1684   return def-&gt;bottom_type()-&gt;is_vect()-&gt;length_in_bytes();
1685 }
1686 
1687 static inline Assembler::AvxVectorLen vector_length_encoding(const MachNode* n) {
1688   switch(vector_length_in_bytes(n)) {
1689     case  4: // fall-through
1690     case  8: // fall-through
1691     case 16: return Assembler::AVX_128bit;
1692     case 32: return Assembler::AVX_256bit;
1693     case 64: return Assembler::AVX_512bit;
1694 
1695     default: {
1696       ShouldNotReachHere();
1697       return Assembler::AVX_NoVec;
1698     }
1699   }
1700 }
1701 
1702 // Helper methods for MachSpillCopyNode::implementation().
1703 static int vec_mov_helper(CodeBuffer *cbuf, bool do_size, int src_lo, int dst_lo,
1704                           int src_hi, int dst_hi, uint ireg, outputStream* st) {
1705   // In 64-bit VM size calculation is very complex. Emitting instructions
1706   // into scratch buffer is used to get size in 64-bit VM.
1707   LP64_ONLY( assert(!do_size, &quot;this method calculates size only for 32-bit VM&quot;); )
1708   assert(ireg == Op_VecS || // 32bit vector
1709          (src_lo &amp; 1) == 0 &amp;&amp; (src_lo + 1) == src_hi &amp;&amp;
1710          (dst_lo &amp; 1) == 0 &amp;&amp; (dst_lo + 1) == dst_hi,
1711          &quot;no non-adjacent vector moves&quot; );
1712   if (cbuf) {
<span class="line-modified">1713     MacroAssembler _masm(cbuf);</span>
1714     int offset = __ offset();
1715     switch (ireg) {
1716     case Op_VecS: // copy whole register
1717     case Op_VecD:
1718     case Op_VecX:
1719 #ifndef _LP64
1720       __ movdqu(as_XMMRegister(Matcher::_regEncode[dst_lo]), as_XMMRegister(Matcher::_regEncode[src_lo]));
1721 #else
1722       if ((UseAVX &lt; 3) || VM_Version::supports_avx512vl()) {
1723         __ movdqu(as_XMMRegister(Matcher::_regEncode[dst_lo]), as_XMMRegister(Matcher::_regEncode[src_lo]));
1724       } else {
1725         __ vextractf32x4(as_XMMRegister(Matcher::_regEncode[dst_lo]), as_XMMRegister(Matcher::_regEncode[src_lo]), 0x0);
1726      }
1727 #endif
1728       break;
1729     case Op_VecY:
1730 #ifndef _LP64
1731       __ vmovdqu(as_XMMRegister(Matcher::_regEncode[dst_lo]), as_XMMRegister(Matcher::_regEncode[src_lo]));
1732 #else
1733       if ((UseAVX &lt; 3) || VM_Version::supports_avx512vl()) {
</pre>
<hr />
<pre>
1759       break;
1760     case Op_VecY:
1761     case Op_VecZ:
1762       st-&gt;print(&quot;vmovdqu %s,%s\t# spill&quot;,Matcher::regName[dst_lo],Matcher::regName[src_lo]);
1763       break;
1764     default:
1765       ShouldNotReachHere();
1766     }
1767 #endif
1768   }
1769   // VEX_2bytes prefix is used if UseAVX &gt; 0, and it takes the same 2 bytes as SIMD prefix.
1770   return (UseAVX &gt; 2) ? 6 : 4;
1771 }
1772 
1773 int vec_spill_helper(CodeBuffer *cbuf, bool do_size, bool is_load,
1774                      int stack_offset, int reg, uint ireg, outputStream* st) {
1775   // In 64-bit VM size calculation is very complex. Emitting instructions
1776   // into scratch buffer is used to get size in 64-bit VM.
1777   LP64_ONLY( assert(!do_size, &quot;this method calculates size only for 32-bit VM&quot;); )
1778   if (cbuf) {
<span class="line-modified">1779     MacroAssembler _masm(cbuf);</span>
1780     int offset = __ offset();
1781     if (is_load) {
1782       switch (ireg) {
1783       case Op_VecS:
1784         __ movdl(as_XMMRegister(Matcher::_regEncode[reg]), Address(rsp, stack_offset));
1785         break;
1786       case Op_VecD:
1787         __ movq(as_XMMRegister(Matcher::_regEncode[reg]), Address(rsp, stack_offset));
1788         break;
1789       case Op_VecX:
1790 #ifndef _LP64
1791         __ movdqu(as_XMMRegister(Matcher::_regEncode[reg]), Address(rsp, stack_offset));
1792 #else
1793         if ((UseAVX &lt; 3) || VM_Version::supports_avx512vl()) {
1794           __ movdqu(as_XMMRegister(Matcher::_regEncode[reg]), Address(rsp, stack_offset));
1795         } else {
1796           __ vpxor(as_XMMRegister(Matcher::_regEncode[reg]), as_XMMRegister(Matcher::_regEncode[reg]), as_XMMRegister(Matcher::_regEncode[reg]), 2);
1797           __ vinsertf32x4(as_XMMRegister(Matcher::_regEncode[reg]), as_XMMRegister(Matcher::_regEncode[reg]), Address(rsp, stack_offset),0x0);
1798         }
1799 #endif
</pre>
<hr />
<pre>
1962 static inline jlong replicate8_imm(int con, int width) {
1963   // Load a constant of &quot;width&quot; (in bytes) and replicate it to fill 64bit.
1964   assert(width == 1 || width == 2 || width == 4, &quot;only byte, short or int types here&quot;);
1965   int bit_width = width * 8;
1966   jlong val = con;
1967   val &amp;= (((jlong) 1) &lt;&lt; bit_width) - 1;  // mask off sign bits
1968   while(bit_width &lt; 64) {
1969     val |= (val &lt;&lt; bit_width);
1970     bit_width &lt;&lt;= 1;
1971   }
1972   return val;
1973 }
1974 
1975 #ifndef PRODUCT
1976   void MachNopNode::format(PhaseRegAlloc*, outputStream* st) const {
1977     st-&gt;print(&quot;nop \t# %d bytes pad for loops and calls&quot;, _count);
1978   }
1979 #endif
1980 
1981   void MachNopNode::emit(CodeBuffer &amp;cbuf, PhaseRegAlloc*) const {
<span class="line-modified">1982     MacroAssembler _masm(&amp;cbuf);</span>
1983     __ nop(_count);
1984   }
1985 
1986   uint MachNopNode::size(PhaseRegAlloc*) const {
1987     return _count;
1988   }
1989 
1990 #ifndef PRODUCT
1991   void MachBreakpointNode::format(PhaseRegAlloc*, outputStream* st) const {
1992     st-&gt;print(&quot;# breakpoint&quot;);
1993   }
1994 #endif
1995 
1996   void MachBreakpointNode::emit(CodeBuffer &amp;cbuf, PhaseRegAlloc* ra_) const {
<span class="line-modified">1997     MacroAssembler _masm(&amp;cbuf);</span>
1998     __ int3();
1999   }
2000 
2001   uint MachBreakpointNode::size(PhaseRegAlloc* ra_) const {
2002     return MachNode::size(ra_);
2003   }
2004 
2005 %}
2006 
2007 encode %{
2008 
2009   enc_class call_epilog %{
2010     if (VerifyStackAtCalls) {
2011       // Check that stack depth is unchanged: find majik cookie on stack
2012       int framesize = ra_-&gt;reg2offset_unchecked(OptoReg::add(ra_-&gt;_matcher._old_SP, -3*VMRegImpl::slots_per_word));
<span class="line-modified">2013       MacroAssembler _masm(&amp;cbuf);</span>
2014       Label L;
2015       __ cmpptr(Address(rsp, framesize), (int32_t)0xbadb100d);
2016       __ jccb(Assembler::equal, L);
2017       // Die if stack mismatch
2018       __ int3();
2019       __ bind(L);
2020     }
2021   %}
2022 
2023 %}
2024 
2025 
2026 //----------OPERANDS-----------------------------------------------------------
2027 // Operand definitions must precede instruction definitions for correct parsing
2028 // in the ADLC because operands constitute user defined types which are used in
2029 // instruction definitions.
2030 
2031 // Vectors
2032 
2033 // Dummy generic vector class. Should be used for all vector operands.
</pre>
<hr />
<pre>
3105   match(Set mem (StoreVector mem src));
3106   ins_cost(145);
3107   format %{ &quot;store_vector $mem,$src\n\t&quot; %}
3108   ins_encode %{
3109     switch (vector_length_in_bytes(this, $src)) {
3110       case  4: __ movdl    ($mem$$Address, $src$$XMMRegister); break;
3111       case  8: __ movq     ($mem$$Address, $src$$XMMRegister); break;
3112       case 16: __ movdqu   ($mem$$Address, $src$$XMMRegister); break;
3113       case 32: __ vmovdqu  ($mem$$Address, $src$$XMMRegister); break;
3114       case 64: __ evmovdqul($mem$$Address, $src$$XMMRegister, Assembler::AVX_512bit); break;
3115       default: ShouldNotReachHere();
3116     }
3117   %}
3118   ins_pipe( pipe_slow );
3119 %}
3120 
3121 // ====================REPLICATE=======================================
3122 
3123 // Replicate byte scalar to be vector
3124 instruct ReplB_reg(vec dst, rRegI src) %{
<span class="line-removed">3125   predicate((n-&gt;as_Vector()-&gt;length() &lt;= 32) ||</span>
<span class="line-removed">3126             (n-&gt;as_Vector()-&gt;length() == 64 &amp;&amp; VM_Version::supports_avx512bw())); // AVX512BW for 512bit byte instructions</span>
3127   match(Set dst (ReplicateB src));
3128   format %{ &quot;replicateB $dst,$src&quot; %}
3129   ins_encode %{
3130     uint vlen = vector_length(this);
3131     if (vlen == 64 || VM_Version::supports_avx512vlbw()) { // AVX512VL for &lt;512bit operands
<span class="line-modified">3132       assert(VM_Version::supports_avx512bw(), &quot;required&quot;);</span>
3133       int vlen_enc = vector_length_encoding(this);
3134       __ evpbroadcastb($dst$$XMMRegister, $src$$Register, vlen_enc);
3135     } else {
3136       __ movdl($dst$$XMMRegister, $src$$Register);
3137       __ punpcklbw($dst$$XMMRegister, $dst$$XMMRegister);
3138       __ pshuflw($dst$$XMMRegister, $dst$$XMMRegister, 0x00);
3139       if (vlen &gt;= 16) {
3140         __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
3141         if (vlen &gt;= 32) {
<span class="line-modified">3142           assert(vlen == 32, &quot;sanity&quot;); // vlen == 64 &amp;&amp; !AVX512BW is covered by ReplB_reg_leg</span>
3143           __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
3144         }
3145       }
3146     }
3147   %}
3148   ins_pipe( pipe_slow );
3149 %}
3150 
<span class="line-removed">3151 instruct ReplB_reg_leg(legVec dst, rRegI src) %{</span>
<span class="line-removed">3152   predicate(n-&gt;as_Vector()-&gt;length() == 64 &amp;&amp; !VM_Version::supports_avx512bw()); // AVX512BW for 512bit byte instructions</span>
<span class="line-removed">3153   match(Set dst (ReplicateB src));</span>
<span class="line-removed">3154   format %{ &quot;replicateB $dst,$src&quot; %}</span>
<span class="line-removed">3155   ins_encode %{</span>
<span class="line-removed">3156     assert(UseAVX &gt; 2, &quot;required&quot;);</span>
<span class="line-removed">3157     __ movdl($dst$$XMMRegister, $src$$Register);</span>
<span class="line-removed">3158     __ punpcklbw($dst$$XMMRegister, $dst$$XMMRegister);</span>
<span class="line-removed">3159     __ pshuflw($dst$$XMMRegister, $dst$$XMMRegister, 0x00);</span>
<span class="line-removed">3160     __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);</span>
<span class="line-removed">3161     __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);</span>
<span class="line-removed">3162     __ vinserti64x4($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, 0x1);</span>
<span class="line-removed">3163   %}</span>
<span class="line-removed">3164   ins_pipe( pipe_slow );</span>
<span class="line-removed">3165 %}</span>
<span class="line-removed">3166 </span>
3167 instruct ReplB_mem(vec dst, memory mem) %{
<span class="line-modified">3168   predicate((n-&gt;as_Vector()-&gt;length() &lt;= 32 &amp;&amp; VM_Version::supports_avx512vlbw()) || // AVX512VL for &lt;512bit operands</span>
<span class="line-removed">3169             (n-&gt;as_Vector()-&gt;length() == 64 &amp;&amp; VM_Version::supports_avx512bw()));    // AVX512BW for 512bit byte instructions</span>
3170   match(Set dst (ReplicateB (LoadB mem)));
3171   format %{ &quot;replicateB $dst,$mem&quot; %}
3172   ins_encode %{
<span class="line-removed">3173     assert(UseAVX &gt; 2, &quot;required&quot;);</span>
3174     int vector_len = vector_length_encoding(this);
3175     __ vpbroadcastb($dst$$XMMRegister, $mem$$Address, vector_len);
3176   %}
3177   ins_pipe( pipe_slow );
3178 %}
3179 
3180 instruct ReplB_imm(vec dst, immI con) %{
<span class="line-removed">3181   predicate((n-&gt;as_Vector()-&gt;length() &lt;= 32) ||</span>
<span class="line-removed">3182             (n-&gt;as_Vector()-&gt;length() == 64 &amp;&amp; VM_Version::supports_avx512bw())); // AVX512BW for 512bit byte instructions</span>
3183   match(Set dst (ReplicateB con));
3184   format %{ &quot;replicateB $dst,$con&quot; %}
3185   ins_encode %{
3186     uint vlen = vector_length(this);
3187     InternalAddress const_addr = $constantaddress(replicate8_imm($con$$constant, 1));
3188     if (vlen == 4) {
3189       __ movdl($dst$$XMMRegister, const_addr);
3190     } else {
3191       __ movq($dst$$XMMRegister, const_addr);
3192       if (vlen &gt;= 16) {
<span class="line-modified">3193         if (vlen == 64 || VM_Version::supports_avx512vlbw()) { // AVX512VL for &lt;512bit operands</span>
3194           int vlen_enc = vector_length_encoding(this);
<span class="line-modified">3195           __ vpbroadcastb($dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);</span>
3196         } else {

3197           __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
<span class="line-removed">3198           if (vlen &gt;= 32) {</span>
<span class="line-removed">3199              assert(vlen == 32, &quot;sanity&quot;);// vlen == 64 &amp;&amp; !AVX512BW is covered by ReplB_imm_leg</span>
<span class="line-removed">3200             __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);</span>
<span class="line-removed">3201           }</span>
3202         }
3203       }
3204     }
3205   %}
3206   ins_pipe( pipe_slow );
3207 %}
3208 
<span class="line-removed">3209 instruct ReplB_imm_leg(legVec dst, immI con) %{</span>
<span class="line-removed">3210   predicate(n-&gt;as_Vector()-&gt;length() == 64 &amp;&amp; !VM_Version::supports_avx512bw());</span>
<span class="line-removed">3211   match(Set dst (ReplicateB con));</span>
<span class="line-removed">3212   format %{ &quot;replicateB $dst,$con&quot; %}</span>
<span class="line-removed">3213   ins_encode %{</span>
<span class="line-removed">3214     __ movq($dst$$XMMRegister, $constantaddress(replicate8_imm($con$$constant, 1)));</span>
<span class="line-removed">3215     __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);</span>
<span class="line-removed">3216     __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);</span>
<span class="line-removed">3217     __ vinserti64x4($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, 0x1);</span>
<span class="line-removed">3218   %}</span>
<span class="line-removed">3219   ins_pipe( pipe_slow );</span>
<span class="line-removed">3220 %}</span>
<span class="line-removed">3221 </span>
3222 // Replicate byte scalar zero to be vector
3223 instruct ReplB_zero(vec dst, immI0 zero) %{
3224   match(Set dst (ReplicateB zero));
3225   format %{ &quot;replicateB $dst,$zero&quot; %}
3226   ins_encode %{
3227     uint vlen = vector_length(this);
3228     if (vlen &lt;= 16) {
3229       __ pxor($dst$$XMMRegister, $dst$$XMMRegister);
3230     } else {
3231       // Use vpxor since AVX512F does not have 512bit vxorpd (requires AVX512DQ).
3232       int vlen_enc = vector_length_encoding(this);
3233       __ vpxor($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);
3234     }
3235   %}
3236   ins_pipe( fpu_reg_reg );
3237 %}
3238 
3239 // ====================ReplicateS=======================================
3240 
3241 instruct ReplS_reg(vec dst, rRegI src) %{
<span class="line-removed">3242   predicate((n-&gt;as_Vector()-&gt;length() &lt;= 16) ||</span>
<span class="line-removed">3243             (n-&gt;as_Vector()-&gt;length() == 32 &amp;&amp; VM_Version::supports_avx512bw())); // AVX512BW for 512bit instructions on shorts</span>
3244   match(Set dst (ReplicateS src));
3245   format %{ &quot;replicateS $dst,$src&quot; %}
3246   ins_encode %{
3247     uint vlen = vector_length(this);
3248     if (vlen == 32 || VM_Version::supports_avx512vlbw()) { // AVX512VL for &lt;512bit operands
<span class="line-modified">3249       assert(VM_Version::supports_avx512bw(), &quot;required&quot;);</span>
3250       int vlen_enc = vector_length_encoding(this);
3251       __ evpbroadcastw($dst$$XMMRegister, $src$$Register, vlen_enc);
3252     } else {
3253       __ movdl($dst$$XMMRegister, $src$$Register);
3254       __ pshuflw($dst$$XMMRegister, $dst$$XMMRegister, 0x00);
3255       if (vlen &gt;= 8) {
3256         __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
3257         if (vlen &gt;= 16) {
<span class="line-modified">3258           assert(vlen == 16, &quot;sanity&quot;); // vlen == 32 &amp;&amp; !AVX512BW is covered by ReplS_reg_leg</span>
3259           __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
3260         }
3261       }
3262     }
3263   %}
3264   ins_pipe( pipe_slow );
3265 %}
3266 
<span class="line-removed">3267 instruct ReplS_reg_leg(legVec dst, rRegI src) %{</span>
<span class="line-removed">3268   predicate(n-&gt;as_Vector()-&gt;length() == 32 &amp;&amp; !VM_Version::supports_avx512bw());</span>
<span class="line-removed">3269   match(Set dst (ReplicateS src));</span>
<span class="line-removed">3270   format %{ &quot;replicateS $dst,$src&quot; %}</span>
<span class="line-removed">3271   ins_encode %{</span>
<span class="line-removed">3272     __ movdl($dst$$XMMRegister, $src$$Register);</span>
<span class="line-removed">3273     __ pshuflw($dst$$XMMRegister, $dst$$XMMRegister, 0x00);</span>
<span class="line-removed">3274     __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);</span>
<span class="line-removed">3275     __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);</span>
<span class="line-removed">3276     __ vinserti64x4($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, 0x1);</span>
<span class="line-removed">3277   %}</span>
<span class="line-removed">3278   ins_pipe( pipe_slow );</span>
<span class="line-removed">3279 %}</span>
<span class="line-removed">3280 </span>
3281 instruct ReplS_mem(vec dst, memory mem) %{
<span class="line-modified">3282   predicate((n-&gt;as_Vector()-&gt;length() &gt;= 4  &amp;&amp;</span>
<span class="line-removed">3283              n-&gt;as_Vector()-&gt;length() &lt;= 16 &amp;&amp; VM_Version::supports_avx()) ||</span>
<span class="line-removed">3284             (n-&gt;as_Vector()-&gt;length() == 32 &amp;&amp; VM_Version::supports_avx512bw())); // AVX512BW for 512bit instructions on shorts</span>
3285   match(Set dst (ReplicateS (LoadS mem)));
3286   format %{ &quot;replicateS $dst,$mem&quot; %}
3287   ins_encode %{
<span class="line-modified">3288     uint vlen = vector_length(this);</span>
<span class="line-modified">3289     if (vlen == 32 || VM_Version::supports_avx512vlbw()) { // AVX512VL for &lt;512bit operands</span>
<span class="line-removed">3290       assert(VM_Version::supports_avx512bw(), &quot;required&quot;);</span>
<span class="line-removed">3291       int vlen_enc = vector_length_encoding(this);</span>
<span class="line-removed">3292       __ vpbroadcastw($dst$$XMMRegister, $mem$$Address, vlen_enc);</span>
<span class="line-removed">3293     } else {</span>
<span class="line-removed">3294       __ pshuflw($dst$$XMMRegister, $mem$$Address, 0x00);</span>
<span class="line-removed">3295       if (vlen &gt;= 8) {</span>
<span class="line-removed">3296         __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);</span>
<span class="line-removed">3297         if (vlen &gt;= 16) {</span>
<span class="line-removed">3298           assert(vlen == 16, &quot;sanity&quot;); // vlen == 32 &amp;&amp; !AVX512BW is covered by ReplS_mem_leg</span>
<span class="line-removed">3299           __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);</span>
<span class="line-removed">3300         }</span>
<span class="line-removed">3301       }</span>
<span class="line-removed">3302     }</span>
<span class="line-removed">3303   %}</span>
<span class="line-removed">3304   ins_pipe( pipe_slow );</span>
<span class="line-removed">3305 %}</span>
<span class="line-removed">3306 </span>
<span class="line-removed">3307 instruct ReplS_mem_leg(legVec dst, memory mem) %{</span>
<span class="line-removed">3308   predicate(n-&gt;as_Vector()-&gt;length() == 32 &amp;&amp; !VM_Version::supports_avx512bw());</span>
<span class="line-removed">3309   match(Set dst (ReplicateS (LoadS mem)));</span>
<span class="line-removed">3310   format %{ &quot;replicateS $dst,$mem&quot; %}</span>
<span class="line-removed">3311   ins_encode %{</span>
<span class="line-removed">3312     __ pshuflw($dst$$XMMRegister, $mem$$Address, 0x00);</span>
<span class="line-removed">3313     __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);</span>
<span class="line-removed">3314     __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);</span>
<span class="line-removed">3315     __ vinserti64x4($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, 0x1);</span>
3316   %}
3317   ins_pipe( pipe_slow );
3318 %}
3319 
3320 instruct ReplS_imm(vec dst, immI con) %{
<span class="line-removed">3321   predicate((n-&gt;as_Vector()-&gt;length() &lt;= 16) ||</span>
<span class="line-removed">3322             (n-&gt;as_Vector()-&gt;length() == 32 &amp;&amp; VM_Version::supports_avx512bw())); // AVX512BW for 512bit instructions on shorts</span>
3323   match(Set dst (ReplicateS con));
3324   format %{ &quot;replicateS $dst,$con&quot; %}
3325   ins_encode %{
3326     uint vlen = vector_length(this);
<span class="line-modified">3327     InternalAddress constaddr = $constantaddress(replicate8_imm($con$$constant, 2));</span>
3328     if (vlen == 2) {
<span class="line-modified">3329       __ movdl($dst$$XMMRegister, constaddr);</span>
3330     } else {
<span class="line-modified">3331       __ movq($dst$$XMMRegister, constaddr);</span>
<span class="line-modified">3332       if (vlen == 32 || VM_Version::supports_avx512vlbw() ) { // AVX512VL for &lt;512bit operands</span>
<span class="line-modified">3333         assert(VM_Version::supports_avx512bw(), &quot;required&quot;);</span>
<span class="line-modified">3334         int vlen_enc = vector_length_encoding(this);</span>
<span class="line-modified">3335         __ vpbroadcastw($dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);</span>
<span class="line-modified">3336       } else {</span>
<span class="line-modified">3337         __ movq($dst$$XMMRegister, constaddr);</span>
<span class="line-modified">3338         __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);</span>
<span class="line-removed">3339         if (vlen &gt;= 16) {</span>
<span class="line-removed">3340           assert(vlen == 16, &quot;sanity&quot;); // vlen == 32 &amp;&amp; !AVX512BW is covered by ReplS_imm_leg</span>
<span class="line-removed">3341           __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);</span>
3342         }
3343       }
3344     }
3345   %}
3346   ins_pipe( fpu_reg_reg );
3347 %}
3348 
<span class="line-removed">3349 instruct ReplS_imm_leg(legVec dst, immI con) %{</span>
<span class="line-removed">3350   predicate(n-&gt;as_Vector()-&gt;length() == 32 &amp;&amp; !VM_Version::supports_avx512bw());</span>
<span class="line-removed">3351   match(Set dst (ReplicateS con));</span>
<span class="line-removed">3352   format %{ &quot;replicateS $dst,$con&quot; %}</span>
<span class="line-removed">3353   ins_encode %{</span>
<span class="line-removed">3354     __ movq($dst$$XMMRegister, $constantaddress(replicate8_imm($con$$constant, 2)));</span>
<span class="line-removed">3355     __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);</span>
<span class="line-removed">3356     __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);</span>
<span class="line-removed">3357     __ vinserti64x4($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, 0x1);</span>
<span class="line-removed">3358   %}</span>
<span class="line-removed">3359   ins_pipe( pipe_slow );</span>
<span class="line-removed">3360 %}</span>
<span class="line-removed">3361 </span>
3362 instruct ReplS_zero(vec dst, immI0 zero) %{
3363   match(Set dst (ReplicateS zero));
3364   format %{ &quot;replicateS $dst,$zero&quot; %}
3365   ins_encode %{
3366     uint vlen = vector_length(this);
3367     if (vlen &lt;= 8) {
3368       __ pxor($dst$$XMMRegister, $dst$$XMMRegister);
3369     } else {
3370       int vlen_enc = vector_length_encoding(this);
3371       __ vpxor($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);
3372     }
3373   %}
3374   ins_pipe( fpu_reg_reg );
3375 %}
3376 
3377 // ====================ReplicateI=======================================
3378 
3379 instruct ReplI_reg(vec dst, rRegI src) %{
3380   match(Set dst (ReplicateI src));
3381   format %{ &quot;replicateI $dst,$src&quot; %}
3382   ins_encode %{
3383     uint vlen = vector_length(this);
3384     if (vlen == 16 || VM_Version::supports_avx512vl()) { // AVX512VL for &lt;512bit operands
3385       int vlen_enc = vector_length_encoding(this);
3386       __ evpbroadcastd($dst$$XMMRegister, $src$$Register, vlen_enc);
3387     } else {
3388       __ movdl($dst$$XMMRegister, $src$$Register);
3389       __ pshufd($dst$$XMMRegister, $dst$$XMMRegister, 0x00);
3390       if (vlen &gt;= 8) {
3391         assert(vlen == 8, &quot;sanity&quot;);
3392         __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
3393       }
3394     }
3395   %}
3396   ins_pipe( pipe_slow );
3397 %}
3398 
3399 instruct ReplI_mem(vec dst, memory mem) %{
<span class="line-removed">3400   predicate(VM_Version::supports_avx()); // use VEX-encoded pshufd to relax 16-byte alignment restriction on the source</span>
3401   match(Set dst (ReplicateI (LoadI mem)));
3402   format %{ &quot;replicateI $dst,$mem&quot; %}
3403   ins_encode %{
3404     uint vlen = vector_length(this);
3405     if (vlen &lt;= 4) {
<span class="line-modified">3406       __ pshufd($dst$$XMMRegister, $mem$$Address, 0x00);</span>
<span class="line-modified">3407     } else if (vlen == 16 || VM_Version::supports_avx512vl()) { // AVX512VL for &lt;512bit operands</span>


3408       int vector_len = vector_length_encoding(this);
3409       __ vpbroadcastd($dst$$XMMRegister, $mem$$Address, vector_len);
<span class="line-removed">3410     } else {</span>
<span class="line-removed">3411       assert(vlen == 8, &quot;sanity&quot;);</span>
<span class="line-removed">3412       __ pshufd($dst$$XMMRegister, $mem$$Address, 0x00);</span>
<span class="line-removed">3413       __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);</span>
3414     }
3415   %}
3416   ins_pipe( pipe_slow );
3417 %}
3418 
3419 instruct ReplI_imm(vec dst, immI con) %{
3420   match(Set dst (ReplicateI con));
3421   format %{ &quot;replicateI $dst,$con&quot; %}
3422   ins_encode %{
3423     uint vlen = vector_length(this);
<span class="line-modified">3424     InternalAddress constaddr = $constantaddress(replicate8_imm($con$$constant, 4));</span>
<span class="line-modified">3425     if (vlen == 2) {</span>
<span class="line-modified">3426       __ movq($dst$$XMMRegister, constaddr);</span>
<span class="line-modified">3427     } else if (vlen == 16 || VM_Version::supports_avx512vl()) { // AVX512VL for &lt;512bit operands</span>




3428       int vector_len = vector_length_encoding(this);
<span class="line-modified">3429       __ movq($dst$$XMMRegister, constaddr);</span>
3430       __ vpbroadcastd($dst$$XMMRegister, $dst$$XMMRegister, vector_len);
<span class="line-removed">3431     } else {</span>
<span class="line-removed">3432       __ movq($dst$$XMMRegister, constaddr);</span>
<span class="line-removed">3433       __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);</span>
<span class="line-removed">3434       if (vlen &gt;= 8) {</span>
<span class="line-removed">3435         assert(vlen == 8, &quot;sanity&quot;);</span>
<span class="line-removed">3436         __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);</span>
<span class="line-removed">3437       }</span>
3438     }
3439   %}
3440   ins_pipe( pipe_slow );
3441 %}
3442 
3443 // Replicate integer (4 byte) scalar zero to be vector
3444 instruct ReplI_zero(vec dst, immI0 zero) %{
3445   match(Set dst (ReplicateI zero));
3446   format %{ &quot;replicateI $dst,$zero&quot; %}
3447   ins_encode %{
3448     uint vlen = vector_length(this);
3449     if (vlen &lt;= 4) {
3450       __ pxor($dst$$XMMRegister, $dst$$XMMRegister);
3451     } else {
3452       int vlen_enc = vector_length_encoding(this);
3453       __ vpxor($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);
3454     }
3455   %}
3456   ins_pipe( fpu_reg_reg );
3457 %}
</pre>
<hr />
<pre>
3527     } else {
3528       int vector_len = Assembler::AVX_512bit;
3529       __ movdl($dst$$XMMRegister, $src$$Register);
3530       __ movdl($tmp$$XMMRegister, HIGH_FROM_LOW($src$$Register));
3531       __ punpckldq($dst$$XMMRegister, $tmp$$XMMRegister);
3532       __ vpbroadcastq($dst$$XMMRegister, $dst$$XMMRegister, vector_len);
3533     }
3534   %}
3535   ins_pipe( pipe_slow );
3536 %}
3537 #endif // _LP64
3538 
3539 instruct ReplL_mem(vec dst, memory mem) %{
3540   match(Set dst (ReplicateL (LoadL mem)));
3541   format %{ &quot;replicateL $dst,$mem&quot; %}
3542   ins_encode %{
3543     uint vlen = vector_length(this);
3544     if (vlen == 2) {
3545       __ movq($dst$$XMMRegister, $mem$$Address);
3546       __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
<span class="line-modified">3547     } else if (vlen == 8 || VM_Version::supports_avx512vl()) { // AVX512VL for &lt;512bit operands</span>

3548       int vlen_enc = vector_length_encoding(this);
3549       __ vpbroadcastq($dst$$XMMRegister, $mem$$Address, vlen_enc);
<span class="line-removed">3550     } else {</span>
<span class="line-removed">3551       assert(vlen == 4, &quot;sanity&quot;);</span>
<span class="line-removed">3552       __ movq($dst$$XMMRegister, $mem$$Address);</span>
<span class="line-removed">3553       __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);</span>
<span class="line-removed">3554       __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);</span>
3555     }
3556   %}
3557   ins_pipe( pipe_slow );
3558 %}
3559 
3560 // Replicate long (8 byte) scalar immediate to be vector by loading from const table.
3561 instruct ReplL_imm(vec dst, immL con) %{
3562   match(Set dst (ReplicateL con));
3563   format %{ &quot;replicateL $dst,$con&quot; %}
3564   ins_encode %{
3565     uint vlen = vector_length(this);
3566     InternalAddress const_addr = $constantaddress($con);
3567     if (vlen == 2) {
3568       __ movq($dst$$XMMRegister, const_addr);
3569       __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
<span class="line-modified">3570     } else if (vlen == 8 || VM_Version::supports_avx512vl()) { // AVX512VL for &lt;512bit operands</span>

3571       int vlen_enc = vector_length_encoding(this);
3572       __ movq($dst$$XMMRegister, const_addr);
3573       __ vpbroadcastq($dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);
<span class="line-removed">3574     } else {</span>
<span class="line-removed">3575       assert(vlen == 4, &quot;sanity&quot;);</span>
<span class="line-removed">3576       __ movq($dst$$XMMRegister, const_addr);</span>
<span class="line-removed">3577       __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);</span>
<span class="line-removed">3578       __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);</span>
3579     }
3580   %}
3581   ins_pipe( pipe_slow );
3582 %}
3583 
3584 instruct ReplL_zero(vec dst, immL0 zero) %{
3585   match(Set dst (ReplicateL zero));
3586   format %{ &quot;replicateL $dst,$zero&quot; %}
3587   ins_encode %{
3588     int vlen = vector_length(this);
3589     if (vlen == 2) {
3590       __ pxor($dst$$XMMRegister, $dst$$XMMRegister);
3591     } else {
3592       int vlen_enc = vector_length_encoding(this);
3593       __ vpxor($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);
3594     }
3595   %}
3596   ins_pipe( fpu_reg_reg );
3597 %}
3598 
3599 // ====================ReplicateF=======================================
3600 
3601 instruct ReplF_reg(vec dst, vlRegF src) %{
3602   match(Set dst (ReplicateF src));
3603   format %{ &quot;replicateF $dst,$src&quot; %}
3604   ins_encode %{
3605     uint vlen = vector_length(this);
3606     if (vlen &lt;= 4) {
3607       __ pshufd($dst$$XMMRegister, $src$$XMMRegister, 0x00);
<span class="line-modified">3608     } else if (vlen == 16 || VM_Version::supports_avx512vl()) { // AVX512VL for &lt;512bit operands</span>
3609       int vector_len = vector_length_encoding(this);
<span class="line-modified">3610       __ vbroadcastss($dst$$XMMRegister, $src$$XMMRegister, vector_len);</span>
3611     } else {
3612       assert(vlen == 8, &quot;sanity&quot;);
3613       __ pshufd($dst$$XMMRegister, $src$$XMMRegister, 0x00);
3614       __ vinsertf128_high($dst$$XMMRegister, $dst$$XMMRegister);
3615     }
3616   %}
3617   ins_pipe( pipe_slow );
3618 %}
3619 
3620 instruct ReplF_mem(vec dst, memory mem) %{
<span class="line-removed">3621   predicate(VM_Version::supports_avx()); // use VEX-encoded pshufd to relax 16-byte alignment restriction on the source</span>
3622   match(Set dst (ReplicateF (LoadF mem)));
3623   format %{ &quot;replicateF $dst,$mem&quot; %}
3624   ins_encode %{
3625     uint vlen = vector_length(this);
3626     if (vlen &lt;= 4) {
<span class="line-modified">3627       __ pshufd($dst$$XMMRegister, $mem$$Address, 0x00);</span>
<span class="line-modified">3628     } else if (vlen == 16 || VM_Version::supports_avx512vl()) { // AVX512VL for &lt;512bit operands</span>


3629       int vector_len = vector_length_encoding(this);
3630       __ vbroadcastss($dst$$XMMRegister, $mem$$Address, vector_len);
<span class="line-removed">3631     } else {</span>
<span class="line-removed">3632       assert(vlen == 8, &quot;sanity&quot;);</span>
<span class="line-removed">3633       __ pshufd($dst$$XMMRegister, $mem$$Address, 0x00);</span>
<span class="line-removed">3634       __ vinsertf128_high($dst$$XMMRegister, $dst$$XMMRegister);</span>
3635     }
3636   %}
3637   ins_pipe( pipe_slow );
3638 %}
3639 
3640 instruct ReplF_zero(vec dst, immF0 zero) %{
3641   match(Set dst (ReplicateF zero));
3642   format %{ &quot;replicateF $dst,$zero&quot; %}
3643   ins_encode %{
3644     uint vlen = vector_length(this);
3645     if (vlen &lt;= 4) {
3646       __ xorps($dst$$XMMRegister, $dst$$XMMRegister);
3647     } else {
3648       int vlen_enc = vector_length_encoding(this);
3649       __ vpxor($dst$$XMMRegister,$dst$$XMMRegister, $dst$$XMMRegister, vlen_enc); // 512bit vxorps requires AVX512DQ
3650     }
3651   %}
3652   ins_pipe( fpu_reg_reg );
3653 %}
3654 
3655 // ====================ReplicateD=======================================
3656 
3657 // Replicate double (8 bytes) scalar to be vector
3658 instruct ReplD_reg(vec dst, vlRegD src) %{
3659   match(Set dst (ReplicateD src));
3660   format %{ &quot;replicateD $dst,$src&quot; %}
3661   ins_encode %{
3662     uint vlen = vector_length(this);
3663     if (vlen == 2) {
3664       __ pshufd($dst$$XMMRegister, $src$$XMMRegister, 0x44);
<span class="line-modified">3665     } else if (vlen == 8 || VM_Version::supports_avx512vl()) { // AVX512VL for &lt;512bit operands</span>
3666       int vector_len = vector_length_encoding(this);
<span class="line-modified">3667       __ vbroadcastsd($dst$$XMMRegister, $src$$XMMRegister, vector_len);</span>
3668     } else {
3669       assert(vlen == 4, &quot;sanity&quot;);
3670       __ pshufd($dst$$XMMRegister, $src$$XMMRegister, 0x44);
3671       __ vinsertf128_high($dst$$XMMRegister, $dst$$XMMRegister);
3672     }
3673   %}
3674   ins_pipe( pipe_slow );
3675 %}
3676 
3677 instruct ReplD_mem(vec dst, memory mem) %{
<span class="line-removed">3678   predicate(VM_Version::supports_avx()); // use VEX-encoded pshufd to relax 16-byte alignment restriction on the source</span>
3679   match(Set dst (ReplicateD (LoadD mem)));
3680   format %{ &quot;replicateD $dst,$mem&quot; %}
3681   ins_encode %{
3682     uint vlen = vector_length(this);
3683     if (vlen == 2) {
<span class="line-modified">3684       __ pshufd($dst$$XMMRegister, $mem$$Address, 0x44);</span>
<span class="line-modified">3685     } else if (vlen == 8 || VM_Version::supports_avx512vl()) { // AVX512VL for &lt;512bit operands</span>


3686       int vector_len = vector_length_encoding(this);
3687       __ vbroadcastsd($dst$$XMMRegister, $mem$$Address, vector_len);
<span class="line-removed">3688     } else {</span>
<span class="line-removed">3689       assert(vlen == 4, &quot;sanity&quot;);</span>
<span class="line-removed">3690       __ pshufd($dst$$XMMRegister, $mem$$Address, 0x44);</span>
<span class="line-removed">3691       __ vinsertf128_high($dst$$XMMRegister, $dst$$XMMRegister);</span>
3692     }
3693   %}
3694   ins_pipe( pipe_slow );
3695 %}
3696 
3697 instruct ReplD_zero(vec dst, immD0 zero) %{
3698   match(Set dst (ReplicateD zero));
3699   format %{ &quot;replicateD $dst,$zero&quot; %}
3700   ins_encode %{
3701     uint vlen = vector_length(this);
3702     if (vlen == 2) {
3703       __ xorpd($dst$$XMMRegister, $dst$$XMMRegister);
3704     } else {
3705       int vlen_enc = vector_length_encoding(this);
3706       __ vpxor($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, vlen_enc); // 512bit vxorps requires AVX512DQ
3707     }
3708   %}
3709   ins_pipe( fpu_reg_reg );
3710 %}
3711 
3712 // ====================REDUCTION ARITHMETIC=======================================

3713 
<span class="line-modified">3714 // =======================AddReductionVI==========================================</span>
<span class="line-modified">3715 </span>
<span class="line-modified">3716 instruct vadd2I_reduction_reg(rRegI dst, rRegI src1, vec src2, vec tmp, vec tmp2) %{</span>
<span class="line-removed">3717   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 2); // vector_length(src2) == 2</span>
<span class="line-removed">3718   match(Set dst (AddReductionVI src1 src2));</span>
<span class="line-removed">3719   effect(TEMP tmp, TEMP tmp2);</span>
<span class="line-removed">3720   format %{ &quot;vector_add2I_reduction $dst,$src1,$src2&quot; %}</span>
<span class="line-removed">3721   ins_encode %{</span>
<span class="line-removed">3722     if (UseAVX &gt; 2) {</span>
<span class="line-removed">3723       int vector_len = Assembler::AVX_128bit;</span>
<span class="line-removed">3724       __ pshufd($tmp2$$XMMRegister, $src2$$XMMRegister, 0x1);</span>
<span class="line-removed">3725       __ vpaddd($tmp$$XMMRegister, $src2$$XMMRegister, $tmp2$$XMMRegister, vector_len);</span>
<span class="line-removed">3726       __ movdl($tmp2$$XMMRegister, $src1$$Register);</span>
<span class="line-removed">3727       __ vpaddd($tmp2$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, vector_len);</span>
<span class="line-removed">3728       __ movdl($dst$$Register, $tmp2$$XMMRegister);</span>
<span class="line-removed">3729     } else if (VM_Version::supports_avxonly()) {</span>
<span class="line-removed">3730       int vector_len = Assembler::AVX_128bit;</span>
<span class="line-removed">3731       __ vphaddd($tmp$$XMMRegister, $src2$$XMMRegister, $src2$$XMMRegister, vector_len);</span>
<span class="line-removed">3732       __ movdl($tmp2$$XMMRegister, $src1$$Register);</span>
<span class="line-removed">3733       __ vpaddd($tmp2$$XMMRegister, $tmp2$$XMMRegister, $tmp$$XMMRegister, vector_len);</span>
<span class="line-removed">3734       __ movdl($dst$$Register, $tmp2$$XMMRegister);</span>
<span class="line-removed">3735     } else {</span>
<span class="line-removed">3736       assert(UseSSE &gt; 2, &quot;required&quot;);</span>
<span class="line-removed">3737       __ movdqu($tmp2$$XMMRegister, $src2$$XMMRegister);</span>
<span class="line-removed">3738       __ phaddd($tmp2$$XMMRegister, $tmp2$$XMMRegister);</span>
<span class="line-removed">3739       __ movdl($tmp$$XMMRegister, $src1$$Register);</span>
<span class="line-removed">3740       __ paddd($tmp$$XMMRegister, $tmp2$$XMMRegister);</span>
<span class="line-removed">3741       __ movdl($dst$$Register, $tmp$$XMMRegister);</span>
<span class="line-removed">3742     }</span>
<span class="line-removed">3743   %}</span>
<span class="line-removed">3744   ins_pipe( pipe_slow );</span>
<span class="line-removed">3745 %}</span>
<span class="line-removed">3746 </span>
<span class="line-removed">3747 instruct vadd4I_reduction_reg(rRegI dst, rRegI src1, vec src2, vec tmp, vec tmp2) %{</span>
<span class="line-removed">3748   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 4); // vector_length(src2) == 4</span>
3749   match(Set dst (AddReductionVI src1 src2));
<span class="line-modified">3750   effect(TEMP tmp, TEMP tmp2);</span>
<span class="line-modified">3751   format %{ &quot;vector_add4I_reduction $dst,$src1,$src2&quot; %}</span>
<span class="line-modified">3752   ins_encode %{</span>
<span class="line-modified">3753     if (UseAVX &gt; 2) {</span>
<span class="line-modified">3754       int vector_len = Assembler::AVX_128bit;</span>
<span class="line-modified">3755       __ pshufd($tmp2$$XMMRegister, $src2$$XMMRegister, 0xE);</span>
<span class="line-modified">3756       __ vpaddd($tmp$$XMMRegister, $src2$$XMMRegister, $tmp2$$XMMRegister, vector_len);</span>
<span class="line-modified">3757       __ pshufd($tmp2$$XMMRegister, $tmp$$XMMRegister, 0x1);</span>
<span class="line-modified">3758       __ vpaddd($tmp$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, vector_len);</span>
<span class="line-modified">3759       __ movdl($tmp2$$XMMRegister, $src1$$Register);</span>
<span class="line-removed">3760       __ vpaddd($tmp2$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, vector_len);</span>
<span class="line-removed">3761       __ movdl($dst$$Register, $tmp2$$XMMRegister);</span>
<span class="line-removed">3762     } else if (VM_Version::supports_avxonly()) {</span>
<span class="line-removed">3763       int vector_len = Assembler::AVX_128bit;</span>
<span class="line-removed">3764       __ vphaddd($tmp$$XMMRegister, $src2$$XMMRegister, $src2$$XMMRegister, vector_len);</span>
<span class="line-removed">3765       __ vphaddd($tmp$$XMMRegister, $tmp$$XMMRegister, $tmp$$XMMRegister, vector_len);</span>
<span class="line-removed">3766       __ movdl($tmp2$$XMMRegister, $src1$$Register);</span>
<span class="line-removed">3767       __ vpaddd($tmp2$$XMMRegister, $tmp2$$XMMRegister, $tmp$$XMMRegister, vector_len);</span>
<span class="line-removed">3768       __ movdl($dst$$Register, $tmp2$$XMMRegister);</span>
<span class="line-removed">3769     } else {</span>
<span class="line-removed">3770       assert(UseSSE &gt; 2, &quot;required&quot;);</span>
<span class="line-removed">3771       __ movdqu($tmp$$XMMRegister, $src2$$XMMRegister);</span>
<span class="line-removed">3772       __ phaddd($tmp$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">3773       __ phaddd($tmp$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">3774       __ movdl($tmp2$$XMMRegister, $src1$$Register);</span>
<span class="line-removed">3775       __ paddd($tmp2$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">3776       __ movdl($dst$$Register, $tmp2$$XMMRegister);</span>
<span class="line-removed">3777     }</span>
<span class="line-removed">3778   %}</span>
<span class="line-removed">3779   ins_pipe( pipe_slow );</span>
<span class="line-removed">3780 %}</span>
<span class="line-removed">3781 </span>
<span class="line-removed">3782 instruct vadd8I_reduction_reg(rRegI dst, rRegI src1, vec src2, vec tmp, vec tmp2) %{</span>
<span class="line-removed">3783   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 8); // vector_length(src2) == 8</span>
<span class="line-removed">3784   match(Set dst (AddReductionVI src1 src2));</span>
<span class="line-removed">3785   effect(TEMP tmp, TEMP tmp2);</span>
<span class="line-removed">3786   format %{ &quot;vector_add8I_reduction $dst,$src1,$src2&quot; %}</span>
<span class="line-removed">3787   ins_encode %{</span>
<span class="line-removed">3788     if (UseAVX &gt; 2) {</span>
<span class="line-removed">3789       int vector_len = Assembler::AVX_128bit;</span>
<span class="line-removed">3790       __ vextracti128_high($tmp$$XMMRegister, $src2$$XMMRegister);</span>
<span class="line-removed">3791       __ vpaddd($tmp$$XMMRegister, $tmp$$XMMRegister, $src2$$XMMRegister, vector_len);</span>
<span class="line-removed">3792       __ pshufd($tmp2$$XMMRegister, $tmp$$XMMRegister, 0xE);</span>
<span class="line-removed">3793       __ vpaddd($tmp$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, vector_len);</span>
<span class="line-removed">3794       __ pshufd($tmp2$$XMMRegister, $tmp$$XMMRegister, 0x1);</span>
<span class="line-removed">3795       __ vpaddd($tmp$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, vector_len);</span>
<span class="line-removed">3796       __ movdl($tmp2$$XMMRegister, $src1$$Register);</span>
<span class="line-removed">3797       __ vpaddd($tmp2$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, vector_len);</span>
<span class="line-removed">3798       __ movdl($dst$$Register, $tmp2$$XMMRegister);</span>
<span class="line-removed">3799     } else {</span>
<span class="line-removed">3800       assert(UseAVX &gt; 0, &quot;&quot;);</span>
<span class="line-removed">3801       int vector_len = Assembler::AVX_256bit;</span>
<span class="line-removed">3802       __ vphaddd($tmp$$XMMRegister, $src2$$XMMRegister, $src2$$XMMRegister, vector_len);</span>
<span class="line-removed">3803       __ vphaddd($tmp$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, vector_len);</span>
<span class="line-removed">3804       __ vextracti128_high($tmp2$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">3805       __ vpaddd($tmp$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, 0);</span>
<span class="line-removed">3806       __ movdl($tmp2$$XMMRegister, $src1$$Register);</span>
<span class="line-removed">3807       __ vpaddd($tmp2$$XMMRegister, $tmp2$$XMMRegister, $tmp$$XMMRegister, 0);</span>
<span class="line-removed">3808       __ movdl($dst$$Register, $tmp2$$XMMRegister);</span>
<span class="line-removed">3809     }</span>
3810   %}
3811   ins_pipe( pipe_slow );
3812 %}
3813 
<span class="line-modified">3814 instruct vadd16I_reduction_reg(rRegI dst, rRegI src1, legVec src2, legVec tmp, legVec tmp2, legVec tmp3) %{</span>
<span class="line-modified">3815   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 16); // vector_length(src2) == 16</span>

3816   match(Set dst (AddReductionVI src1 src2));
<span class="line-modified">3817   effect(TEMP tmp, TEMP tmp2, TEMP tmp3);</span>
<span class="line-modified">3818   format %{ &quot;vector_add16I_reduction $dst,$src1,$src2&quot; %}</span>




3819   ins_encode %{
<span class="line-modified">3820     __ vextracti64x4_high($tmp3$$XMMRegister, $src2$$XMMRegister);</span>
<span class="line-modified">3821     __ vpaddd($tmp3$$XMMRegister, $tmp3$$XMMRegister, $src2$$XMMRegister, 1);</span>
<span class="line-modified">3822     __ vextracti128_high($tmp$$XMMRegister, $tmp3$$XMMRegister);</span>
<span class="line-removed">3823     __ vpaddd($tmp$$XMMRegister, $tmp$$XMMRegister, $tmp3$$XMMRegister, 0);</span>
<span class="line-removed">3824     __ pshufd($tmp2$$XMMRegister, $tmp$$XMMRegister, 0xE);</span>
<span class="line-removed">3825     __ vpaddd($tmp$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, 0);</span>
<span class="line-removed">3826     __ pshufd($tmp2$$XMMRegister, $tmp$$XMMRegister, 0x1);</span>
<span class="line-removed">3827     __ vpaddd($tmp$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, 0);</span>
<span class="line-removed">3828     __ movdl($tmp2$$XMMRegister, $src1$$Register);</span>
<span class="line-removed">3829     __ vpaddd($tmp2$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, 0);</span>
<span class="line-removed">3830     __ movdl($dst$$Register, $tmp2$$XMMRegister);</span>
3831   %}
3832   ins_pipe( pipe_slow );
3833 %}
3834 
<span class="line-modified">3835 // =======================AddReductionVL==========================================</span>
3836 
3837 #ifdef _LP64
<span class="line-modified">3838 instruct vadd2L_reduction_reg(rRegL dst, rRegL src1, vec src2, vec tmp, vec tmp2) %{</span>
<span class="line-modified">3839   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 2); // vector_length(src2) == 2</span>

3840   match(Set dst (AddReductionVL src1 src2));
<span class="line-modified">3841   effect(TEMP tmp, TEMP tmp2);</span>
<span class="line-modified">3842   format %{ &quot;vector_add2L_reduction $dst,$src1,$src2&quot; %}</span>
<span class="line-modified">3843   ins_encode %{</span>
<span class="line-modified">3844     assert(UseAVX &gt; 2, &quot;required&quot;);</span>
<span class="line-modified">3845     __ pshufd($tmp2$$XMMRegister, $src2$$XMMRegister, 0xE);</span>
<span class="line-modified">3846     __ vpaddq($tmp$$XMMRegister, $src2$$XMMRegister, $tmp2$$XMMRegister, 0);</span>
<span class="line-removed">3847     __ movdq($tmp2$$XMMRegister, $src1$$Register);</span>
<span class="line-removed">3848     __ vpaddq($tmp2$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, 0);</span>
<span class="line-removed">3849     __ movdq($dst$$Register, $tmp2$$XMMRegister);</span>
<span class="line-removed">3850   %}</span>
<span class="line-removed">3851   ins_pipe( pipe_slow );</span>
<span class="line-removed">3852 %}</span>
<span class="line-removed">3853 </span>
<span class="line-removed">3854 instruct vadd4L_reduction_reg(rRegL dst, rRegL src1, vec src2, vec tmp, vec tmp2) %{</span>
<span class="line-removed">3855   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 4); // vector_length(src2) == 4</span>
<span class="line-removed">3856   match(Set dst (AddReductionVL src1 src2));</span>
<span class="line-removed">3857   effect(TEMP tmp, TEMP tmp2);</span>
<span class="line-removed">3858   format %{ &quot;vector_add4L_reduction $dst,$src1,$src2&quot; %}</span>
3859   ins_encode %{
<span class="line-modified">3860     assert(UseAVX &gt; 2, &quot;required&quot;);</span>
<span class="line-modified">3861     __ vextracti128_high($tmp$$XMMRegister, $src2$$XMMRegister);</span>
<span class="line-modified">3862     __ vpaddq($tmp2$$XMMRegister, $tmp$$XMMRegister, $src2$$XMMRegister, 0);</span>
<span class="line-removed">3863     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0xE);</span>
<span class="line-removed">3864     __ vpaddq($tmp2$$XMMRegister, $tmp2$$XMMRegister, $tmp$$XMMRegister, 0);</span>
<span class="line-removed">3865     __ movdq($tmp$$XMMRegister, $src1$$Register);</span>
<span class="line-removed">3866     __ vpaddq($tmp2$$XMMRegister, $tmp2$$XMMRegister, $tmp$$XMMRegister, 0);</span>
<span class="line-removed">3867     __ movdq($dst$$Register, $tmp2$$XMMRegister);</span>
3868   %}
3869   ins_pipe( pipe_slow );
3870 %}
3871 
<span class="line-modified">3872 instruct vadd8L_reduction_reg(rRegL dst, rRegL src1, legVec src2, legVec tmp, legVec tmp2) %{</span>
<span class="line-modified">3873   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 8); // vector_length(src2) == 8</span>

3874   match(Set dst (AddReductionVL src1 src2));
<span class="line-modified">3875   effect(TEMP tmp, TEMP tmp2);</span>
<span class="line-modified">3876   format %{ &quot;vector_addL_reduction $dst,$src1,$src2&quot; %}</span>




3877   ins_encode %{
<span class="line-modified">3878     assert(UseAVX &gt; 2, &quot;required&quot;);</span>
<span class="line-modified">3879     __ vextracti64x4_high($tmp2$$XMMRegister, $src2$$XMMRegister);</span>
<span class="line-modified">3880     __ vpaddq($tmp2$$XMMRegister, $tmp2$$XMMRegister, $src2$$XMMRegister, 1);</span>
<span class="line-removed">3881     __ vextracti128_high($tmp$$XMMRegister, $tmp2$$XMMRegister);</span>
<span class="line-removed">3882     __ vpaddq($tmp2$$XMMRegister, $tmp2$$XMMRegister, $tmp$$XMMRegister, 0);</span>
<span class="line-removed">3883     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0xE);</span>
<span class="line-removed">3884     __ vpaddq($tmp2$$XMMRegister, $tmp2$$XMMRegister, $tmp$$XMMRegister, 0);</span>
<span class="line-removed">3885     __ movdq($tmp$$XMMRegister, $src1$$Register);</span>
<span class="line-removed">3886     __ vpaddq($tmp2$$XMMRegister, $tmp2$$XMMRegister, $tmp$$XMMRegister, 0);</span>
<span class="line-removed">3887     __ movdq($dst$$Register, $tmp2$$XMMRegister);</span>
3888   %}
3889   ins_pipe( pipe_slow );
3890 %}
3891 #endif // _LP64
3892 
<span class="line-modified">3893 // =======================AddReductionVF==========================================</span>
3894 
<span class="line-modified">3895 instruct vadd2F_reduction_reg(regF dst, vec src2, vec tmp) %{</span>
<span class="line-modified">3896   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 2); // vector_length(src2) == 2</span>
<span class="line-modified">3897   match(Set dst (AddReductionVF dst src2));</span>
<span class="line-modified">3898   effect(TEMP dst, TEMP tmp);</span>
<span class="line-modified">3899   format %{ &quot;vector_add2F_reduction $dst,$dst,$src2&quot; %}</span>

3900   ins_encode %{
<span class="line-modified">3901     if (UseAVX &gt; 0) {</span>
<span class="line-modified">3902       __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $src2$$XMMRegister);</span>
<span class="line-modified">3903       __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x01);</span>
<span class="line-removed">3904       __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">3905     } else {</span>
<span class="line-removed">3906       assert(UseSSE &gt; 0, &quot;required&quot;);</span>
<span class="line-removed">3907       __ addss($dst$$XMMRegister, $src2$$XMMRegister);</span>
<span class="line-removed">3908       __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x01);</span>
<span class="line-removed">3909       __ addss($dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">3910     }</span>
<span class="line-removed">3911   %}</span>
<span class="line-removed">3912   ins_pipe( pipe_slow );</span>
<span class="line-removed">3913 %}</span>
<span class="line-removed">3914 </span>
<span class="line-removed">3915 instruct vadd4F_reduction_reg(regF dst, vec src2, vec tmp) %{</span>
<span class="line-removed">3916   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 4); // vector_length(src2) == 4</span>
<span class="line-removed">3917   match(Set dst (AddReductionVF dst src2));</span>
<span class="line-removed">3918   effect(TEMP dst, TEMP tmp);</span>
<span class="line-removed">3919   format %{ &quot;vector_add4F_reduction $dst,$dst,$src2&quot; %}</span>
<span class="line-removed">3920   ins_encode %{</span>
<span class="line-removed">3921     if (UseAVX &gt; 0) {</span>
<span class="line-removed">3922       __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $src2$$XMMRegister);</span>
<span class="line-removed">3923       __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x01);</span>
<span class="line-removed">3924       __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">3925       __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x02);</span>
<span class="line-removed">3926       __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">3927       __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x03);</span>
<span class="line-removed">3928       __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">3929     } else {</span>
<span class="line-removed">3930       assert(UseSSE &gt; 0, &quot;required&quot;);</span>
<span class="line-removed">3931       __ addss($dst$$XMMRegister, $src2$$XMMRegister);</span>
<span class="line-removed">3932       __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x01);</span>
<span class="line-removed">3933       __ addss($dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">3934       __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x02);</span>
<span class="line-removed">3935       __ addss($dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">3936       __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x03);</span>
<span class="line-removed">3937       __ addss($dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">3938     }</span>
<span class="line-removed">3939   %}</span>
<span class="line-removed">3940   ins_pipe( pipe_slow );</span>
<span class="line-removed">3941 %}</span>
<span class="line-removed">3942 </span>
<span class="line-removed">3943 </span>
<span class="line-removed">3944 instruct vadd8F_reduction_reg(regF dst, vec src2, vec tmp, vec tmp2) %{</span>
<span class="line-removed">3945   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 8); // vector_length(src2) == 8</span>
<span class="line-removed">3946   match(Set dst (AddReductionVF dst src2));</span>
<span class="line-removed">3947   effect(TEMP tmp, TEMP dst, TEMP tmp2);</span>
<span class="line-removed">3948   format %{ &quot;vector_add8F_reduction $dst,$dst,$src2&quot; %}</span>
<span class="line-removed">3949   ins_encode %{</span>
<span class="line-removed">3950     assert(UseAVX &gt; 0, &quot;required&quot;);</span>
<span class="line-removed">3951     __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $src2$$XMMRegister);</span>
<span class="line-removed">3952     __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x01);</span>
<span class="line-removed">3953     __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">3954     __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x02);</span>
<span class="line-removed">3955     __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">3956     __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x03);</span>
<span class="line-removed">3957     __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">3958     __ vextractf128_high($tmp2$$XMMRegister, $src2$$XMMRegister);</span>
<span class="line-removed">3959     __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister);</span>
<span class="line-removed">3960     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x01);</span>
<span class="line-removed">3961     __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">3962     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x02);</span>
<span class="line-removed">3963     __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">3964     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x03);</span>
<span class="line-removed">3965     __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">3966   %}</span>
<span class="line-removed">3967   ins_pipe( pipe_slow );</span>
<span class="line-removed">3968 %}</span>
<span class="line-removed">3969 </span>
<span class="line-removed">3970 instruct vadd16F_reduction_reg(regF dst, legVec src2, legVec tmp, legVec tmp2) %{</span>
<span class="line-removed">3971   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 16); // vector_length(src2) == 16</span>
<span class="line-removed">3972   match(Set dst (AddReductionVF dst src2));</span>
<span class="line-removed">3973   effect(TEMP tmp, TEMP dst, TEMP tmp2);</span>
<span class="line-removed">3974   format %{ &quot;vector_add16F_reduction $dst,$dst,$src2&quot; %}</span>
<span class="line-removed">3975   ins_encode %{</span>
<span class="line-removed">3976     assert(UseAVX &gt; 2, &quot;required&quot;);</span>
<span class="line-removed">3977     __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $src2$$XMMRegister);</span>
<span class="line-removed">3978     __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x01);</span>
<span class="line-removed">3979     __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">3980     __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x02);</span>
<span class="line-removed">3981     __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">3982     __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x03);</span>
<span class="line-removed">3983     __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">3984     __ vextractf32x4($tmp2$$XMMRegister, $src2$$XMMRegister, 0x1);</span>
<span class="line-removed">3985     __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister);</span>
<span class="line-removed">3986     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x01);</span>
<span class="line-removed">3987     __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">3988     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x02);</span>
<span class="line-removed">3989     __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">3990     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x03);</span>
<span class="line-removed">3991     __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">3992     __ vextractf32x4($tmp2$$XMMRegister, $src2$$XMMRegister, 0x2);</span>
<span class="line-removed">3993     __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister);</span>
<span class="line-removed">3994     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x01);</span>
<span class="line-removed">3995     __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">3996     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x02);</span>
<span class="line-removed">3997     __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">3998     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x03);</span>
<span class="line-removed">3999     __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4000     __ vextractf32x4($tmp2$$XMMRegister, $src2$$XMMRegister, 0x3);</span>
<span class="line-removed">4001     __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister);</span>
<span class="line-removed">4002     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x01);</span>
<span class="line-removed">4003     __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4004     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x02);</span>
<span class="line-removed">4005     __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4006     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x03);</span>
<span class="line-removed">4007     __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4008   %}</span>
<span class="line-removed">4009   ins_pipe( pipe_slow );</span>
<span class="line-removed">4010 %}</span>
<span class="line-removed">4011 </span>
<span class="line-removed">4012 // =======================AddReductionVD==========================================</span>
<span class="line-removed">4013 </span>
<span class="line-removed">4014 instruct vadd2D_reduction_reg(regD dst, vec src2, vec tmp) %{</span>
<span class="line-removed">4015   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 2); // vector_length(src2) == 2</span>
<span class="line-removed">4016   match(Set dst (AddReductionVD dst src2));</span>
<span class="line-removed">4017   effect(TEMP tmp, TEMP dst);</span>
<span class="line-removed">4018   format %{ &quot;vector_add2D_reduction  $dst,$src2&quot; %}</span>
<span class="line-removed">4019   ins_encode %{</span>
<span class="line-removed">4020     if (UseAVX &gt; 0) {</span>
<span class="line-removed">4021       __ vaddsd($dst$$XMMRegister, $dst$$XMMRegister, $src2$$XMMRegister);</span>
<span class="line-removed">4022       __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0xE);</span>
<span class="line-removed">4023       __ vaddsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4024     } else {</span>
<span class="line-removed">4025       assert(UseSSE &gt; 0, &quot;required&quot;);</span>
<span class="line-removed">4026       __ addsd($dst$$XMMRegister, $src2$$XMMRegister);</span>
<span class="line-removed">4027       __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0xE);</span>
<span class="line-removed">4028       __ addsd($dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4029     }</span>
<span class="line-removed">4030   %}</span>
<span class="line-removed">4031   ins_pipe( pipe_slow );</span>
<span class="line-removed">4032 %}</span>
<span class="line-removed">4033 </span>
<span class="line-removed">4034 instruct vadd4D_reduction_reg(regD dst, vec src2, vec tmp, vec tmp2) %{</span>
<span class="line-removed">4035   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 4); // vector_length(src2) == 4</span>
<span class="line-removed">4036   match(Set dst (AddReductionVD dst src2));</span>
<span class="line-removed">4037   effect(TEMP tmp, TEMP dst, TEMP tmp2);</span>
<span class="line-removed">4038   format %{ &quot;vector_add4D_reduction $dst,$dst,$src2&quot; %}</span>
<span class="line-removed">4039   ins_encode %{</span>
<span class="line-removed">4040     assert(UseAVX &gt; 0, &quot;required&quot;);</span>
<span class="line-removed">4041     __ vaddsd($dst$$XMMRegister, $dst$$XMMRegister, $src2$$XMMRegister);</span>
<span class="line-removed">4042     __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0xE);</span>
<span class="line-removed">4043     __ vaddsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4044     __ vextractf128($tmp2$$XMMRegister, $src2$$XMMRegister, 0x1);</span>
<span class="line-removed">4045     __ vaddsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister);</span>
<span class="line-removed">4046     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0xE);</span>
<span class="line-removed">4047     __ vaddsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4048   %}</span>
<span class="line-removed">4049   ins_pipe( pipe_slow );</span>
<span class="line-removed">4050 %}</span>
<span class="line-removed">4051 </span>
<span class="line-removed">4052 instruct vadd8D_reduction_reg(regD dst, legVec src2, legVec tmp, legVec tmp2) %{</span>
<span class="line-removed">4053   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 8); // vector_length(src2) == 8</span>
<span class="line-removed">4054   match(Set dst (AddReductionVD dst src2));</span>
<span class="line-removed">4055   effect(TEMP tmp, TEMP dst, TEMP tmp2);</span>
<span class="line-removed">4056   format %{ &quot;vector_add8D_reduction $dst,$dst,$src2&quot; %}</span>
<span class="line-removed">4057   ins_encode %{</span>
<span class="line-removed">4058     assert(UseAVX &gt; 2, &quot;required&quot;);</span>
<span class="line-removed">4059     __ vaddsd($dst$$XMMRegister, $dst$$XMMRegister, $src2$$XMMRegister);</span>
<span class="line-removed">4060     __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0xE);</span>
<span class="line-removed">4061     __ vaddsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4062     __ vextractf32x4($tmp2$$XMMRegister, $src2$$XMMRegister, 0x1);</span>
<span class="line-removed">4063     __ vaddsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister);</span>
<span class="line-removed">4064     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0xE);</span>
<span class="line-removed">4065     __ vaddsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4066     __ vextractf32x4($tmp2$$XMMRegister, $src2$$XMMRegister, 0x2);</span>
<span class="line-removed">4067     __ vaddsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister);</span>
<span class="line-removed">4068     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0xE);</span>
<span class="line-removed">4069     __ vaddsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4070     __ vextractf32x4($tmp2$$XMMRegister, $src2$$XMMRegister, 0x3);</span>
<span class="line-removed">4071     __ vaddsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister);</span>
<span class="line-removed">4072     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0xE);</span>
<span class="line-removed">4073     __ vaddsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4074   %}</span>
<span class="line-removed">4075   ins_pipe( pipe_slow );</span>
<span class="line-removed">4076 %}</span>
<span class="line-removed">4077 </span>
<span class="line-removed">4078 // =======================MulReductionVI==========================================</span>
<span class="line-removed">4079 </span>
<span class="line-removed">4080 instruct vmul2I_reduction_reg(rRegI dst, rRegI src1, vec src2, vec tmp, vec tmp2) %{</span>
<span class="line-removed">4081   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 2); // vector_length(src2) == 2</span>
<span class="line-removed">4082   match(Set dst (MulReductionVI src1 src2));</span>
<span class="line-removed">4083   effect(TEMP tmp, TEMP tmp2);</span>
<span class="line-removed">4084   format %{ &quot;vector_mul2I_reduction $dst,$src1,$src2&quot; %}</span>
<span class="line-removed">4085   ins_encode %{</span>
<span class="line-removed">4086     if (UseAVX &gt; 0) {</span>
<span class="line-removed">4087       int vector_len = Assembler::AVX_128bit;</span>
<span class="line-removed">4088       __ pshufd($tmp2$$XMMRegister, $src2$$XMMRegister, 0x1);</span>
<span class="line-removed">4089       __ vpmulld($tmp$$XMMRegister, $src2$$XMMRegister, $tmp2$$XMMRegister, vector_len);</span>
<span class="line-removed">4090       __ movdl($tmp2$$XMMRegister, $src1$$Register);</span>
<span class="line-removed">4091       __ vpmulld($tmp2$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, vector_len);</span>
<span class="line-removed">4092       __ movdl($dst$$Register, $tmp2$$XMMRegister);</span>
<span class="line-removed">4093     } else {</span>
<span class="line-removed">4094       assert(UseSSE &gt; 3, &quot;required&quot;);</span>
<span class="line-removed">4095       __ pshufd($tmp2$$XMMRegister, $src2$$XMMRegister, 0x1);</span>
<span class="line-removed">4096       __ pmulld($tmp2$$XMMRegister, $src2$$XMMRegister);</span>
<span class="line-removed">4097       __ movdl($tmp$$XMMRegister, $src1$$Register);</span>
<span class="line-removed">4098       __ pmulld($tmp2$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4099       __ movdl($dst$$Register, $tmp2$$XMMRegister);</span>
<span class="line-removed">4100     }</span>
<span class="line-removed">4101   %}</span>
<span class="line-removed">4102   ins_pipe( pipe_slow );</span>
<span class="line-removed">4103 %}</span>
<span class="line-removed">4104 </span>
<span class="line-removed">4105 instruct vmul4I_reduction_reg(rRegI dst, rRegI src1, vec src2, vec tmp, vec tmp2) %{</span>
<span class="line-removed">4106   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 4); // vector_length(src2) == 4</span>
<span class="line-removed">4107   match(Set dst (MulReductionVI src1 src2));</span>
<span class="line-removed">4108   effect(TEMP tmp, TEMP tmp2);</span>
<span class="line-removed">4109   format %{ &quot;vector_mul4I_reduction $dst,$src1,$src2&quot; %}</span>
<span class="line-removed">4110   ins_encode %{</span>
<span class="line-removed">4111     if (UseAVX &gt; 0) {</span>
<span class="line-removed">4112       int vector_len = Assembler::AVX_128bit;</span>
<span class="line-removed">4113       __ pshufd($tmp2$$XMMRegister, $src2$$XMMRegister, 0xE);</span>
<span class="line-removed">4114       __ vpmulld($tmp$$XMMRegister, $src2$$XMMRegister, $tmp2$$XMMRegister, vector_len);</span>
<span class="line-removed">4115       __ pshufd($tmp2$$XMMRegister, $tmp$$XMMRegister, 0x1);</span>
<span class="line-removed">4116       __ vpmulld($tmp$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, vector_len);</span>
<span class="line-removed">4117       __ movdl($tmp2$$XMMRegister, $src1$$Register);</span>
<span class="line-removed">4118       __ vpmulld($tmp2$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, vector_len);</span>
<span class="line-removed">4119       __ movdl($dst$$Register, $tmp2$$XMMRegister);</span>
<span class="line-removed">4120     } else {</span>
<span class="line-removed">4121       assert(UseSSE &gt; 3, &quot;required&quot;);</span>
<span class="line-removed">4122       __ pshufd($tmp2$$XMMRegister, $src2$$XMMRegister, 0xE);</span>
<span class="line-removed">4123       __ pmulld($tmp2$$XMMRegister, $src2$$XMMRegister);</span>
<span class="line-removed">4124       __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x1);</span>
<span class="line-removed">4125       __ pmulld($tmp2$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4126       __ movdl($tmp$$XMMRegister, $src1$$Register);</span>
<span class="line-removed">4127       __ pmulld($tmp2$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4128       __ movdl($dst$$Register, $tmp2$$XMMRegister);</span>
<span class="line-removed">4129     }</span>
<span class="line-removed">4130   %}</span>
<span class="line-removed">4131   ins_pipe( pipe_slow );</span>
<span class="line-removed">4132 %}</span>
<span class="line-removed">4133 </span>
<span class="line-removed">4134 instruct vmul8I_reduction_reg(rRegI dst, rRegI src1, vec src2, vec tmp, vec tmp2) %{</span>
<span class="line-removed">4135   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 8); // vector_length(src2) == 8</span>
<span class="line-removed">4136   match(Set dst (MulReductionVI src1 src2));</span>
<span class="line-removed">4137   effect(TEMP tmp, TEMP tmp2);</span>
<span class="line-removed">4138   format %{ &quot;vector_mul8I_reduction $dst,$src1,$src2&quot; %}</span>
<span class="line-removed">4139   ins_encode %{</span>
<span class="line-removed">4140     assert(UseAVX &gt; 1, &quot;required&quot;);</span>
<span class="line-removed">4141     int vector_len = Assembler::AVX_128bit;</span>
<span class="line-removed">4142     __ vextracti128_high($tmp$$XMMRegister, $src2$$XMMRegister);</span>
<span class="line-removed">4143     __ vpmulld($tmp$$XMMRegister, $tmp$$XMMRegister, $src2$$XMMRegister, vector_len);</span>
<span class="line-removed">4144     __ pshufd($tmp2$$XMMRegister, $tmp$$XMMRegister, 0xE);</span>
<span class="line-removed">4145     __ vpmulld($tmp$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, vector_len);</span>
<span class="line-removed">4146     __ pshufd($tmp2$$XMMRegister, $tmp$$XMMRegister, 0x1);</span>
<span class="line-removed">4147     __ vpmulld($tmp$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, vector_len);</span>
<span class="line-removed">4148     __ movdl($tmp2$$XMMRegister, $src1$$Register);</span>
<span class="line-removed">4149     __ vpmulld($tmp2$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, vector_len);</span>
<span class="line-removed">4150     __ movdl($dst$$Register, $tmp2$$XMMRegister);</span>
<span class="line-removed">4151   %}</span>
<span class="line-removed">4152   ins_pipe( pipe_slow );</span>
<span class="line-removed">4153 %}</span>
<span class="line-removed">4154 </span>
<span class="line-removed">4155 instruct vmul16I_reduction_reg(rRegI dst, rRegI src1, legVec src2, legVec tmp, legVec tmp2, legVec tmp3) %{</span>
<span class="line-removed">4156   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 16); // vector_length(src2) == 16</span>
<span class="line-removed">4157   match(Set dst (MulReductionVI src1 src2));</span>
<span class="line-removed">4158   effect(TEMP tmp, TEMP tmp2, TEMP tmp3);</span>
<span class="line-removed">4159   format %{ &quot;vector_mul16I_reduction $dst,$src1,$src2&quot; %}</span>
<span class="line-removed">4160   ins_encode %{</span>
<span class="line-removed">4161     assert(UseAVX &gt; 2, &quot;required&quot;);</span>
<span class="line-removed">4162     __ vextracti64x4_high($tmp3$$XMMRegister, $src2$$XMMRegister);</span>
<span class="line-removed">4163     __ vpmulld($tmp3$$XMMRegister, $tmp3$$XMMRegister, $src2$$XMMRegister, 1);</span>
<span class="line-removed">4164     __ vextracti128_high($tmp$$XMMRegister, $tmp3$$XMMRegister);</span>
<span class="line-removed">4165     __ vpmulld($tmp$$XMMRegister, $tmp$$XMMRegister, $tmp3$$XMMRegister, 0);</span>
<span class="line-removed">4166     __ pshufd($tmp2$$XMMRegister, $tmp$$XMMRegister, 0xE);</span>
<span class="line-removed">4167     __ vpmulld($tmp$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, 0);</span>
<span class="line-removed">4168     __ pshufd($tmp2$$XMMRegister, $tmp$$XMMRegister, 0x1);</span>
<span class="line-removed">4169     __ vpmulld($tmp$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, 0);</span>
<span class="line-removed">4170     __ movdl($tmp2$$XMMRegister, $src1$$Register);</span>
<span class="line-removed">4171     __ vpmulld($tmp2$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, 0);</span>
<span class="line-removed">4172     __ movdl($dst$$Register, $tmp2$$XMMRegister);</span>
<span class="line-removed">4173   %}</span>
<span class="line-removed">4174   ins_pipe( pipe_slow );</span>
<span class="line-removed">4175 %}</span>
<span class="line-removed">4176 </span>
<span class="line-removed">4177 // =======================MulReductionVL==========================================</span>
<span class="line-removed">4178 </span>
<span class="line-removed">4179 #ifdef _LP64</span>
<span class="line-removed">4180 instruct vmul2L_reduction_reg(rRegL dst, rRegL src1, vec src2, vec tmp, vec tmp2) %{</span>
<span class="line-removed">4181   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 2); // vector_length(src2) == 2</span>
<span class="line-removed">4182   match(Set dst (MulReductionVL src1 src2));</span>
<span class="line-removed">4183   effect(TEMP tmp, TEMP tmp2);</span>
<span class="line-removed">4184   format %{ &quot;vector_mul2L_reduction $dst,$src1,$src2&quot; %}</span>
<span class="line-removed">4185   ins_encode %{</span>
<span class="line-removed">4186     assert(VM_Version::supports_avx512dq(), &quot;required&quot;);</span>
<span class="line-removed">4187     __ pshufd($tmp2$$XMMRegister, $src2$$XMMRegister, 0xE);</span>
<span class="line-removed">4188     __ vpmullq($tmp$$XMMRegister, $src2$$XMMRegister, $tmp2$$XMMRegister, 0);</span>
<span class="line-removed">4189     __ movdq($tmp2$$XMMRegister, $src1$$Register);</span>
<span class="line-removed">4190     __ vpmullq($tmp2$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, 0);</span>
<span class="line-removed">4191     __ movdq($dst$$Register, $tmp2$$XMMRegister);</span>
<span class="line-removed">4192   %}</span>
<span class="line-removed">4193   ins_pipe( pipe_slow );</span>
<span class="line-removed">4194 %}</span>
<span class="line-removed">4195 </span>
<span class="line-removed">4196 instruct vmul4L_reduction_reg(rRegL dst, rRegL src1, vec src2, vec tmp, vec tmp2) %{</span>
<span class="line-removed">4197   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 4); // vector_length(src2) == 4</span>
<span class="line-removed">4198   match(Set dst (MulReductionVL src1 src2));</span>
<span class="line-removed">4199   effect(TEMP tmp, TEMP tmp2);</span>
<span class="line-removed">4200   format %{ &quot;vector_mul4L_reduction $dst,$src1,$src2&quot; %}</span>
<span class="line-removed">4201   ins_encode %{</span>
<span class="line-removed">4202     assert(VM_Version::supports_avx512dq(), &quot;required&quot;);</span>
<span class="line-removed">4203     __ vextracti128_high($tmp$$XMMRegister, $src2$$XMMRegister);</span>
<span class="line-removed">4204     __ vpmullq($tmp2$$XMMRegister, $tmp$$XMMRegister, $src2$$XMMRegister, 0);</span>
<span class="line-removed">4205     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0xE);</span>
<span class="line-removed">4206     __ vpmullq($tmp2$$XMMRegister, $tmp2$$XMMRegister, $tmp$$XMMRegister, 0);</span>
<span class="line-removed">4207     __ movdq($tmp$$XMMRegister, $src1$$Register);</span>
<span class="line-removed">4208     __ vpmullq($tmp2$$XMMRegister, $tmp2$$XMMRegister, $tmp$$XMMRegister, 0);</span>
<span class="line-removed">4209     __ movdq($dst$$Register, $tmp2$$XMMRegister);</span>
4210   %}
4211   ins_pipe( pipe_slow );
4212 %}
4213 
<span class="line-modified">4214 instruct vmul8L_reduction_reg(rRegL dst, rRegL src1, legVec src2, legVec tmp, legVec tmp2) %{</span>
<span class="line-modified">4215   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 8); // vector_length(src2) == 8</span>
<span class="line-modified">4216   match(Set dst (MulReductionVL src1 src2));</span>
<span class="line-modified">4217   effect(TEMP tmp, TEMP tmp2);</span>
<span class="line-modified">4218   format %{ &quot;vector_mul8L_reduction $dst,$src1,$src2&quot; %}</span>

4219   ins_encode %{
<span class="line-modified">4220     assert(VM_Version::supports_avx512dq(), &quot;required&quot;);</span>
<span class="line-modified">4221     __ vextracti64x4_high($tmp2$$XMMRegister, $src2$$XMMRegister);</span>
<span class="line-modified">4222     __ vpmullq($tmp2$$XMMRegister, $tmp2$$XMMRegister, $src2$$XMMRegister, 1);</span>
<span class="line-removed">4223     __ vextracti128_high($tmp$$XMMRegister, $tmp2$$XMMRegister);</span>
<span class="line-removed">4224     __ vpmullq($tmp2$$XMMRegister, $tmp2$$XMMRegister, $tmp$$XMMRegister, 0);</span>
<span class="line-removed">4225     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0xE);</span>
<span class="line-removed">4226     __ vpmullq($tmp2$$XMMRegister, $tmp2$$XMMRegister, $tmp$$XMMRegister, 0);</span>
<span class="line-removed">4227     __ movdq($tmp$$XMMRegister, $src1$$Register);</span>
<span class="line-removed">4228     __ vpmullq($tmp2$$XMMRegister, $tmp2$$XMMRegister, $tmp$$XMMRegister, 0);</span>
<span class="line-removed">4229     __ movdq($dst$$Register, $tmp2$$XMMRegister);</span>
4230   %}
4231   ins_pipe( pipe_slow );
4232 %}
<span class="line-removed">4233 #endif</span>
<span class="line-removed">4234 </span>
<span class="line-removed">4235 // =======================MulReductionVF==========================================</span>
4236 
<span class="line-modified">4237 instruct vmul2F_reduction_reg(regF dst, vec src2, vec tmp) %{</span>
<span class="line-modified">4238   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 2); // vector_length(src2) == 2</span>
<span class="line-modified">4239   match(Set dst (MulReductionVF dst src2));</span>
<span class="line-modified">4240   effect(TEMP dst, TEMP tmp);</span>
<span class="line-modified">4241   format %{ &quot;vector_mul2F_reduction $dst,$dst,$src2&quot; %}</span>

4242   ins_encode %{
<span class="line-modified">4243     if (UseAVX &gt; 0) {</span>
<span class="line-modified">4244       __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $src2$$XMMRegister);</span>
<span class="line-modified">4245       __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x01);</span>
<span class="line-removed">4246       __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4247     } else {</span>
<span class="line-removed">4248       assert(UseSSE &gt; 0, &quot;required&quot;);</span>
<span class="line-removed">4249       __ mulss($dst$$XMMRegister, $src2$$XMMRegister);</span>
<span class="line-removed">4250       __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x01);</span>
<span class="line-removed">4251       __ mulss($dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4252     }</span>
4253   %}
4254   ins_pipe( pipe_slow );
4255 %}
4256 
<span class="line-modified">4257 instruct vmul4F_reduction_reg(regF dst, vec src2, vec tmp) %{</span>
<span class="line-removed">4258   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 4); // vector_length(src2) == 4</span>
<span class="line-removed">4259   match(Set dst (MulReductionVF dst src2));</span>
<span class="line-removed">4260   effect(TEMP dst, TEMP tmp);</span>
<span class="line-removed">4261   format %{ &quot;vector_mul4F_reduction $dst,$dst,$src2&quot; %}</span>
<span class="line-removed">4262   ins_encode %{</span>
<span class="line-removed">4263     if (UseAVX &gt; 0) {</span>
<span class="line-removed">4264       __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $src2$$XMMRegister);</span>
<span class="line-removed">4265       __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x01);</span>
<span class="line-removed">4266       __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4267       __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x02);</span>
<span class="line-removed">4268       __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4269       __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x03);</span>
<span class="line-removed">4270       __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4271     } else {</span>
<span class="line-removed">4272       assert(UseSSE &gt; 0, &quot;required&quot;);</span>
<span class="line-removed">4273       __ mulss($dst$$XMMRegister, $src2$$XMMRegister);</span>
<span class="line-removed">4274       __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x01);</span>
<span class="line-removed">4275       __ mulss($dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4276       __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x02);</span>
<span class="line-removed">4277       __ mulss($dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4278       __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x03);</span>
<span class="line-removed">4279       __ mulss($dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4280     }</span>
<span class="line-removed">4281   %}</span>
<span class="line-removed">4282   ins_pipe( pipe_slow );</span>
<span class="line-removed">4283 %}</span>
4284 
<span class="line-modified">4285 instruct vmul8F_reduction_reg(regF dst, vec src2, vec tmp, vec tmp2) %{</span>
<span class="line-modified">4286   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 8); // vector_length(src2) == 8</span>
<span class="line-modified">4287   match(Set dst (MulReductionVF dst src2));</span>
<span class="line-modified">4288   effect(TEMP tmp, TEMP dst, TEMP tmp2);</span>
<span class="line-modified">4289   format %{ &quot;vector_mul8F_reduction $dst,$dst,$src2&quot; %}</span>
<span class="line-modified">4290   ins_encode %{</span>
<span class="line-removed">4291     assert(UseAVX &gt; 0, &quot;required&quot;);</span>
<span class="line-removed">4292     __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $src2$$XMMRegister);</span>
<span class="line-removed">4293     __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x01);</span>
<span class="line-removed">4294     __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4295     __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x02);</span>
<span class="line-removed">4296     __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4297     __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x03);</span>
<span class="line-removed">4298     __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4299     __ vextractf128_high($tmp2$$XMMRegister, $src2$$XMMRegister);</span>
<span class="line-removed">4300     __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister);</span>
<span class="line-removed">4301     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x01);</span>
<span class="line-removed">4302     __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4303     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x02);</span>
<span class="line-removed">4304     __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4305     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x03);</span>
<span class="line-removed">4306     __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4307   %}</span>
<span class="line-removed">4308   ins_pipe( pipe_slow );</span>
<span class="line-removed">4309 %}</span>
<span class="line-removed">4310 </span>
<span class="line-removed">4311 instruct vmul16F_reduction_reg(regF dst, legVec src2, legVec tmp, legVec tmp2) %{</span>
<span class="line-removed">4312   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 16); // vector_length(src2) == 16</span>
<span class="line-removed">4313   match(Set dst (MulReductionVF dst src2));</span>
<span class="line-removed">4314   effect(TEMP tmp, TEMP dst, TEMP tmp2);</span>
<span class="line-removed">4315   format %{ &quot;vector_mul16F_reduction $dst,$dst,$src2&quot; %}</span>
<span class="line-removed">4316   ins_encode %{</span>
<span class="line-removed">4317     assert(UseAVX &gt; 2, &quot;required&quot;);</span>
<span class="line-removed">4318     __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $src2$$XMMRegister);</span>
<span class="line-removed">4319     __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x01);</span>
<span class="line-removed">4320     __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4321     __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x02);</span>
<span class="line-removed">4322     __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4323     __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x03);</span>
<span class="line-removed">4324     __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4325     __ vextractf32x4($tmp2$$XMMRegister, $src2$$XMMRegister, 0x1);</span>
<span class="line-removed">4326     __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister);</span>
<span class="line-removed">4327     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x01);</span>
<span class="line-removed">4328     __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4329     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x02);</span>
<span class="line-removed">4330     __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4331     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x03);</span>
<span class="line-removed">4332     __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4333     __ vextractf32x4($tmp2$$XMMRegister, $src2$$XMMRegister, 0x2);</span>
<span class="line-removed">4334     __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister);</span>
<span class="line-removed">4335     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x01);</span>
<span class="line-removed">4336     __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4337     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x02);</span>
<span class="line-removed">4338     __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4339     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x03);</span>
<span class="line-removed">4340     __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4341     __ vextractf32x4($tmp2$$XMMRegister, $src2$$XMMRegister, 0x3);</span>
<span class="line-removed">4342     __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister);</span>
<span class="line-removed">4343     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x01);</span>
<span class="line-removed">4344     __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4345     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x02);</span>
<span class="line-removed">4346     __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4347     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x03);</span>
<span class="line-removed">4348     __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4349   %}</span>
<span class="line-removed">4350   ins_pipe( pipe_slow );</span>
<span class="line-removed">4351 %}</span>
<span class="line-removed">4352 </span>
<span class="line-removed">4353 // =======================MulReductionVD==========================================</span>
<span class="line-removed">4354 </span>
<span class="line-removed">4355 instruct vmul2D_reduction_reg(regD dst, vec src2, vec tmp) %{</span>
<span class="line-removed">4356   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 2); // vector_length(src2) == 2</span>
<span class="line-removed">4357   match(Set dst (MulReductionVD dst src2));</span>
<span class="line-removed">4358   effect(TEMP dst, TEMP tmp);</span>
<span class="line-removed">4359   format %{ &quot;vector_mul2D_reduction $dst,$dst,$src2&quot; %}</span>
4360   ins_encode %{
<span class="line-modified">4361     if (UseAVX &gt; 0) {</span>
<span class="line-modified">4362       __ vmulsd($dst$$XMMRegister, $dst$$XMMRegister, $src2$$XMMRegister);</span>
<span class="line-modified">4363       __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0xE);</span>
<span class="line-removed">4364       __ vmulsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4365     } else {</span>
<span class="line-removed">4366       assert(UseSSE &gt; 0, &quot;required&quot;);</span>
<span class="line-removed">4367       __ mulsd($dst$$XMMRegister, $src2$$XMMRegister);</span>
<span class="line-removed">4368       __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0xE);</span>
<span class="line-removed">4369       __ mulsd($dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4370     }</span>
4371   %}
4372   ins_pipe( pipe_slow );
4373 %}
4374 
<span class="line-modified">4375 </span>
<span class="line-modified">4376 instruct vmul4D_reduction_reg(regD dst, vec src2, vec tmp, vec tmp2) %{</span>
<span class="line-modified">4377   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 4); // vector_length(src2) == 2</span>
<span class="line-modified">4378   match(Set dst (MulReductionVD dst src2));</span>
<span class="line-modified">4379   effect(TEMP tmp, TEMP dst, TEMP tmp2);</span>
<span class="line-modified">4380   format %{ &quot;vector_mul4D_reduction  $dst,$dst,$src2&quot; %}</span>
4381   ins_encode %{
<span class="line-modified">4382     assert(UseAVX &gt; 0, &quot;required&quot;);</span>
<span class="line-modified">4383     __ vmulsd($dst$$XMMRegister, $dst$$XMMRegister, $src2$$XMMRegister);</span>
<span class="line-modified">4384     __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0xE);</span>
<span class="line-removed">4385     __ vmulsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4386     __ vextractf128_high($tmp2$$XMMRegister, $src2$$XMMRegister);</span>
<span class="line-removed">4387     __ vmulsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister);</span>
<span class="line-removed">4388     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0xE);</span>
<span class="line-removed">4389     __ vmulsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
4390   %}
4391   ins_pipe( pipe_slow );
4392 %}
4393 
<span class="line-modified">4394 instruct vmul8D_reduction_reg(regD dst, legVec src2, legVec tmp, legVec tmp2) %{</span>
<span class="line-modified">4395   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 8); // vector_length(src2) == 2</span>
<span class="line-modified">4396   match(Set dst (MulReductionVD dst src2));</span>
<span class="line-modified">4397   effect(TEMP tmp, TEMP dst, TEMP tmp2);</span>
<span class="line-modified">4398   format %{ &quot;vector_mul8D_reduction $dst,$dst,$src2&quot; %}</span>

4399   ins_encode %{
<span class="line-modified">4400     assert(UseAVX &gt; 0, &quot;required&quot;);</span>
<span class="line-modified">4401     __ vmulsd($dst$$XMMRegister, $dst$$XMMRegister, $src2$$XMMRegister);</span>
<span class="line-modified">4402     __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0xE);</span>
<span class="line-removed">4403     __ vmulsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4404     __ vextractf32x4($tmp2$$XMMRegister, $src2$$XMMRegister, 0x1);</span>
<span class="line-removed">4405     __ vmulsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister);</span>
<span class="line-removed">4406     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0xE);</span>
<span class="line-removed">4407     __ vmulsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4408     __ vextractf32x4($tmp2$$XMMRegister, $src2$$XMMRegister, 0x2);</span>
<span class="line-removed">4409     __ vmulsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister);</span>
<span class="line-removed">4410     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0xE);</span>
<span class="line-removed">4411     __ vmulsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
<span class="line-removed">4412     __ vextractf32x4($tmp2$$XMMRegister, $src2$$XMMRegister, 0x3);</span>
<span class="line-removed">4413     __ vmulsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister);</span>
<span class="line-removed">4414     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0xE);</span>
<span class="line-removed">4415     __ vmulsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);</span>
4416   %}
4417   ins_pipe( pipe_slow );
4418 %}
4419 
4420 // ====================VECTOR ARITHMETIC=======================================
4421 
4422 // --------------------------------- ADD --------------------------------------
4423 
4424 // Bytes vector add
4425 instruct vaddB(vec dst, vec src) %{
4426   predicate(UseAVX == 0);
4427   match(Set dst (AddVB dst src));
4428   format %{ &quot;paddb   $dst,$src\t! add packedB&quot; %}
4429   ins_encode %{
4430     __ paddb($dst$$XMMRegister, $src$$XMMRegister);
4431   %}
4432   ins_pipe( pipe_slow );
4433 %}
4434 
4435 instruct vaddB_reg(vec dst, vec src1, vec src2) %{
</pre>
<hr />
<pre>
5767   ins_encode %{
5768     __ pmaddwd($dst$$XMMRegister, $src1$$XMMRegister);
5769   %}
5770   ins_pipe( pipe_slow );
5771 %}
5772 
5773 instruct vmuladdS2I_reg_avx(vec dst, vec src1, vec src2) %{
5774   predicate(UseAVX &gt; 0);
5775   match(Set dst (MulAddVS2VI src1 src2));
5776   format %{ &quot;vpmaddwd $dst,$src1,$src2\t! muladd packedStoI&quot; %}
5777   ins_encode %{
5778     int vector_len = vector_length_encoding(this);
5779     __ vpmaddwd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
5780   %}
5781   ins_pipe( pipe_slow );
5782 %}
5783 
5784 // --------------------------------- Vector Multiply Add Add ----------------------------------
5785 
5786 instruct vmuladdaddS2I_reg(vec dst, vec src1, vec src2) %{
<span class="line-modified">5787   predicate(VM_Version::supports_vnni());</span>
5788   match(Set dst (AddVI (MulAddVS2VI src1 src2) dst));
5789   format %{ &quot;evpdpwssd $dst,$src1,$src2\t! muladdadd packedStoI&quot; %}
5790   ins_encode %{
5791     assert(UseAVX &gt; 2, &quot;required&quot;);
5792     int vector_len = vector_length_encoding(this);
5793     __ evpdpwssd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
5794   %}
5795   ins_pipe( pipe_slow );
5796   ins_cost(10);
5797 %}
5798 
5799 // --------------------------------- PopCount --------------------------------------
5800 
5801 instruct vpopcountI(vec dst, vec src) %{
5802   match(Set dst (PopCountVI src));
5803   format %{ &quot;vpopcntd  $dst,$src\t! vector popcount packedI&quot; %}
5804   ins_encode %{
5805     assert(UsePopCountInstruction, &quot;not enabled&quot;);
5806 
5807     int vector_len = vector_length_encoding(this);
</pre>
</td>
<td>
<hr />
<pre>
1160     // a call be deoptimization.  (4932387)
1161     // Note that this value is also credited (in output.cpp) to
1162     // the size of the code section.
1163     return 5 + NativeJump::instruction_size; // pushl(); jmp;
1164   }
1165 #endif
1166 };
1167 
1168 %} // end source_hpp
1169 
1170 source %{
1171 
1172 #include &quot;opto/addnode.hpp&quot;
1173 
1174 // Emit exception handler code.
1175 // Stuff framesize into a register and call a VM stub routine.
1176 int HandlerImpl::emit_exception_handler(CodeBuffer&amp; cbuf) {
1177 
1178   // Note that the code buffer&#39;s insts_mark is always relative to insts.
1179   // That&#39;s why we must use the macroassembler to generate a handler.
<span class="line-modified">1180   C2_MacroAssembler _masm(&amp;cbuf);</span>
1181   address base = __ start_a_stub(size_exception_handler());
1182   if (base == NULL) {
1183     ciEnv::current()-&gt;record_failure(&quot;CodeCache is full&quot;);
1184     return 0;  // CodeBuffer::expand failed
1185   }
1186   int offset = __ offset();
1187   __ jump(RuntimeAddress(OptoRuntime::exception_blob()-&gt;entry_point()));
1188   assert(__ offset() - offset &lt;= (int) size_exception_handler(), &quot;overflow&quot;);
1189   __ end_a_stub();
1190   return offset;
1191 }
1192 
1193 // Emit deopt handler code.
1194 int HandlerImpl::emit_deopt_handler(CodeBuffer&amp; cbuf) {
1195 
1196   // Note that the code buffer&#39;s insts_mark is always relative to insts.
1197   // That&#39;s why we must use the macroassembler to generate a handler.
<span class="line-modified">1198   C2_MacroAssembler _masm(&amp;cbuf);</span>
1199   address base = __ start_a_stub(size_deopt_handler());
1200   if (base == NULL) {
1201     ciEnv::current()-&gt;record_failure(&quot;CodeCache is full&quot;);
1202     return 0;  // CodeBuffer::expand failed
1203   }
1204   int offset = __ offset();
1205 
1206 #ifdef _LP64
1207   address the_pc = (address) __ pc();
1208   Label next;
1209   // push a &quot;the_pc&quot; on the stack without destroying any registers
1210   // as they all may be live.
1211 
1212   // push address of &quot;next&quot;
1213   __ call(next, relocInfo::none); // reloc none is fine since it is a disp32
1214   __ bind(next);
1215   // adjust it so it matches &quot;the_pc&quot;
1216   __ subptr(Address(rsp, 0), __ offset() - offset);
1217 #else
1218   InternalAddress here(__ pc());
</pre>
<hr />
<pre>
1245   static address vector_long_sign_mask() { return StubRoutines::x86::vector_long_sign_mask(); }
1246 
1247 //=============================================================================
1248 const bool Matcher::match_rule_supported(int opcode) {
1249   if (!has_match_rule(opcode)) {
1250     return false; // no match rule present
1251   }
1252   switch (opcode) {
1253     case Op_AbsVL:
1254       if (UseAVX &lt; 3) {
1255         return false;
1256       }
1257       break;
1258     case Op_PopCountI:
1259     case Op_PopCountL:
1260       if (!UsePopCountInstruction) {
1261         return false;
1262       }
1263       break;
1264     case Op_PopCountVI:
<span class="line-modified">1265       if (!UsePopCountInstruction || !VM_Version::supports_avx512_vpopcntdq()) {</span>
1266         return false;
1267       }
1268       break;
1269     case Op_MulVI:
1270       if ((UseSSE &lt; 4) &amp;&amp; (UseAVX &lt; 1)) { // only with SSE4_1 or AVX
1271         return false;
1272       }
1273       break;
1274     case Op_MulVL:
1275     case Op_MulReductionVL:
1276       if (VM_Version::supports_avx512dq() == false) {
1277         return false;
1278       }
1279       break;





1280     case Op_AbsVB:
1281     case Op_AbsVS:
1282     case Op_AbsVI:
1283     case Op_AddReductionVI:
<span class="line-modified">1284     case Op_AndReductionV:</span>
<span class="line-added">1285     case Op_OrReductionV:</span>
<span class="line-added">1286     case Op_XorReductionV:</span>
<span class="line-added">1287       if (UseSSE &lt; 3) { // requires at least SSSE3</span>
1288         return false;
1289       }
1290       break;
1291     case Op_MulReductionVI:
1292       if (UseSSE &lt; 4) { // requires at least SSE4
1293         return false;
1294       }
1295       break;








1296     case Op_SqrtVD:
1297     case Op_SqrtVF:
1298       if (UseAVX &lt; 1) { // enabled for AVX only
1299         return false;
1300       }
1301       break;
1302     case Op_CompareAndSwapL:
1303 #ifdef _LP64
1304     case Op_CompareAndSwapP:
1305 #endif
1306       if (!VM_Version::supports_cx8()) {
1307         return false;
1308       }
1309       break;
1310     case Op_CMoveVF:
1311     case Op_CMoveVD:
1312       if (UseAVX &lt; 1 || UseAVX &gt; 2) {
1313         return false;
1314       }
1315       break;
1316     case Op_StrIndexOf:
1317       if (!UseSSE42Intrinsics) {
1318         return false;
1319       }
1320       break;
1321     case Op_StrIndexOfChar:
1322       if (!UseSSE42Intrinsics) {
1323         return false;
1324       }
1325       break;
1326     case Op_OnSpinWait:
1327       if (VM_Version::supports_on_spin_wait() == false) {
1328         return false;
1329       }
1330       break;








1331     case Op_MulVB:
1332     case Op_LShiftVB:
1333     case Op_RShiftVB:
1334     case Op_URShiftVB:
1335       if (UseSSE &lt; 4) {
1336         return false;
1337       }
1338       break;
1339 #ifdef _LP64
1340     case Op_MaxD:
1341     case Op_MaxF:
1342     case Op_MinD:
1343     case Op_MinF:
1344       if (UseAVX &lt; 1) { // enabled for AVX only
1345         return false;
1346       }
1347       break;
1348 #endif
1349     case Op_CacheWB:
1350     case Op_CacheWBPreSync:
1351     case Op_CacheWBPostSync:
1352       if (!VM_Version::supports_data_cache_line_flush()) {
1353         return false;
1354       }
1355       break;
1356     case Op_RoundDoubleMode:
1357       if (UseSSE &lt; 4) {
1358         return false;
1359       }
1360       break;
1361     case Op_RoundDoubleModeV:
1362       if (VM_Version::supports_avx() == false) {
1363         return false; // 128bit vroundpd is not available
1364       }
1365       break;
<span class="line-added">1366 #ifndef _LP64</span>
<span class="line-added">1367     case Op_AddReductionVF:</span>
<span class="line-added">1368     case Op_AddReductionVD:</span>
<span class="line-added">1369     case Op_MulReductionVF:</span>
<span class="line-added">1370     case Op_MulReductionVD:</span>
<span class="line-added">1371       if (UseSSE &lt; 1) { // requires at least SSE</span>
<span class="line-added">1372         return false;</span>
<span class="line-added">1373       }</span>
<span class="line-added">1374       break;</span>
<span class="line-added">1375     case Op_MulAddVS2VI:</span>
<span class="line-added">1376     case Op_RShiftVL:</span>
<span class="line-added">1377     case Op_AbsVD:</span>
<span class="line-added">1378     case Op_NegVD:</span>
<span class="line-added">1379       if (UseSSE &lt; 2) {</span>
<span class="line-added">1380         return false;</span>
<span class="line-added">1381       }</span>
<span class="line-added">1382       break;</span>
<span class="line-added">1383 #endif // !LP64</span>
1384   }
1385   return true;  // Match rules are supported by default.
1386 }
1387 
1388 //------------------------------------------------------------------------
1389 
1390 // Identify extra cases that we might want to provide match rules for vector nodes and
1391 // other intrinsics guarded with vector length (vlen) and element type (bt).
1392 const bool Matcher::match_rule_supported_vector(int opcode, int vlen, BasicType bt) {
1393   if (!match_rule_supported(opcode)) {
1394     return false;
1395   }
1396   // Matcher::vector_size_supported() restricts vector sizes in the following way (see Matcher::vector_width_in_bytes):
1397   //   * SSE2 supports 128bit vectors for all types;
1398   //   * AVX1 supports 256bit vectors only for FLOAT and DOUBLE types;
1399   //   * AVX2 supports 256bit vectors for all types;
1400   //   * AVX512F supports 512bit vectors only for INT, FLOAT, and DOUBLE types;
1401   //   * AVX512BW supports 512bit vectors for BYTE, SHORT, and CHAR types.
1402   // There&#39;s also a limit on minimum vector size supported: 2 elements (or 4 bytes for BYTE).
1403   // And MaxVectorSize is taken into account as well.
</pre>
<hr />
<pre>
1656     mstack.push(off, Visit);
1657     mstack.push(m-&gt;in(AddPNode::Base), Pre_Visit);
1658     return true;
1659   } else if (clone_shift(off, this, mstack, address_visited)) {
1660     address_visited.test_set(m-&gt;_idx); // Flag as address_visited
1661     mstack.push(m-&gt;in(AddPNode::Address), Pre_Visit);
1662     mstack.push(m-&gt;in(AddPNode::Base), Pre_Visit);
1663     return true;
1664   }
1665   return false;
1666 }
1667 
1668 void Compile::reshape_address(AddPNode* addp) {
1669 }
1670 
1671 static inline uint vector_length(const MachNode* n) {
1672   const TypeVect* vt = n-&gt;bottom_type()-&gt;is_vect();
1673   return vt-&gt;length();
1674 }
1675 
<span class="line-added">1676 static inline uint vector_length(const MachNode* use, MachOper* opnd) {</span>
<span class="line-added">1677   uint def_idx = use-&gt;operand_index(opnd);</span>
<span class="line-added">1678   Node* def = use-&gt;in(def_idx);</span>
<span class="line-added">1679   return def-&gt;bottom_type()-&gt;is_vect()-&gt;length();</span>
<span class="line-added">1680 }</span>
<span class="line-added">1681 </span>
1682 static inline uint vector_length_in_bytes(const MachNode* n) {
1683   const TypeVect* vt = n-&gt;bottom_type()-&gt;is_vect();
1684   return vt-&gt;length_in_bytes();
1685 }
1686 
1687 static inline uint vector_length_in_bytes(const MachNode* use, MachOper* opnd) {
1688   uint def_idx = use-&gt;operand_index(opnd);
1689   Node* def = use-&gt;in(def_idx);
1690   return def-&gt;bottom_type()-&gt;is_vect()-&gt;length_in_bytes();
1691 }
1692 
1693 static inline Assembler::AvxVectorLen vector_length_encoding(const MachNode* n) {
1694   switch(vector_length_in_bytes(n)) {
1695     case  4: // fall-through
1696     case  8: // fall-through
1697     case 16: return Assembler::AVX_128bit;
1698     case 32: return Assembler::AVX_256bit;
1699     case 64: return Assembler::AVX_512bit;
1700 
1701     default: {
1702       ShouldNotReachHere();
1703       return Assembler::AVX_NoVec;
1704     }
1705   }
1706 }
1707 
1708 // Helper methods for MachSpillCopyNode::implementation().
1709 static int vec_mov_helper(CodeBuffer *cbuf, bool do_size, int src_lo, int dst_lo,
1710                           int src_hi, int dst_hi, uint ireg, outputStream* st) {
1711   // In 64-bit VM size calculation is very complex. Emitting instructions
1712   // into scratch buffer is used to get size in 64-bit VM.
1713   LP64_ONLY( assert(!do_size, &quot;this method calculates size only for 32-bit VM&quot;); )
1714   assert(ireg == Op_VecS || // 32bit vector
1715          (src_lo &amp; 1) == 0 &amp;&amp; (src_lo + 1) == src_hi &amp;&amp;
1716          (dst_lo &amp; 1) == 0 &amp;&amp; (dst_lo + 1) == dst_hi,
1717          &quot;no non-adjacent vector moves&quot; );
1718   if (cbuf) {
<span class="line-modified">1719     C2_MacroAssembler _masm(cbuf);</span>
1720     int offset = __ offset();
1721     switch (ireg) {
1722     case Op_VecS: // copy whole register
1723     case Op_VecD:
1724     case Op_VecX:
1725 #ifndef _LP64
1726       __ movdqu(as_XMMRegister(Matcher::_regEncode[dst_lo]), as_XMMRegister(Matcher::_regEncode[src_lo]));
1727 #else
1728       if ((UseAVX &lt; 3) || VM_Version::supports_avx512vl()) {
1729         __ movdqu(as_XMMRegister(Matcher::_regEncode[dst_lo]), as_XMMRegister(Matcher::_regEncode[src_lo]));
1730       } else {
1731         __ vextractf32x4(as_XMMRegister(Matcher::_regEncode[dst_lo]), as_XMMRegister(Matcher::_regEncode[src_lo]), 0x0);
1732      }
1733 #endif
1734       break;
1735     case Op_VecY:
1736 #ifndef _LP64
1737       __ vmovdqu(as_XMMRegister(Matcher::_regEncode[dst_lo]), as_XMMRegister(Matcher::_regEncode[src_lo]));
1738 #else
1739       if ((UseAVX &lt; 3) || VM_Version::supports_avx512vl()) {
</pre>
<hr />
<pre>
1765       break;
1766     case Op_VecY:
1767     case Op_VecZ:
1768       st-&gt;print(&quot;vmovdqu %s,%s\t# spill&quot;,Matcher::regName[dst_lo],Matcher::regName[src_lo]);
1769       break;
1770     default:
1771       ShouldNotReachHere();
1772     }
1773 #endif
1774   }
1775   // VEX_2bytes prefix is used if UseAVX &gt; 0, and it takes the same 2 bytes as SIMD prefix.
1776   return (UseAVX &gt; 2) ? 6 : 4;
1777 }
1778 
1779 int vec_spill_helper(CodeBuffer *cbuf, bool do_size, bool is_load,
1780                      int stack_offset, int reg, uint ireg, outputStream* st) {
1781   // In 64-bit VM size calculation is very complex. Emitting instructions
1782   // into scratch buffer is used to get size in 64-bit VM.
1783   LP64_ONLY( assert(!do_size, &quot;this method calculates size only for 32-bit VM&quot;); )
1784   if (cbuf) {
<span class="line-modified">1785     C2_MacroAssembler _masm(cbuf);</span>
1786     int offset = __ offset();
1787     if (is_load) {
1788       switch (ireg) {
1789       case Op_VecS:
1790         __ movdl(as_XMMRegister(Matcher::_regEncode[reg]), Address(rsp, stack_offset));
1791         break;
1792       case Op_VecD:
1793         __ movq(as_XMMRegister(Matcher::_regEncode[reg]), Address(rsp, stack_offset));
1794         break;
1795       case Op_VecX:
1796 #ifndef _LP64
1797         __ movdqu(as_XMMRegister(Matcher::_regEncode[reg]), Address(rsp, stack_offset));
1798 #else
1799         if ((UseAVX &lt; 3) || VM_Version::supports_avx512vl()) {
1800           __ movdqu(as_XMMRegister(Matcher::_regEncode[reg]), Address(rsp, stack_offset));
1801         } else {
1802           __ vpxor(as_XMMRegister(Matcher::_regEncode[reg]), as_XMMRegister(Matcher::_regEncode[reg]), as_XMMRegister(Matcher::_regEncode[reg]), 2);
1803           __ vinsertf32x4(as_XMMRegister(Matcher::_regEncode[reg]), as_XMMRegister(Matcher::_regEncode[reg]), Address(rsp, stack_offset),0x0);
1804         }
1805 #endif
</pre>
<hr />
<pre>
1968 static inline jlong replicate8_imm(int con, int width) {
1969   // Load a constant of &quot;width&quot; (in bytes) and replicate it to fill 64bit.
1970   assert(width == 1 || width == 2 || width == 4, &quot;only byte, short or int types here&quot;);
1971   int bit_width = width * 8;
1972   jlong val = con;
1973   val &amp;= (((jlong) 1) &lt;&lt; bit_width) - 1;  // mask off sign bits
1974   while(bit_width &lt; 64) {
1975     val |= (val &lt;&lt; bit_width);
1976     bit_width &lt;&lt;= 1;
1977   }
1978   return val;
1979 }
1980 
1981 #ifndef PRODUCT
1982   void MachNopNode::format(PhaseRegAlloc*, outputStream* st) const {
1983     st-&gt;print(&quot;nop \t# %d bytes pad for loops and calls&quot;, _count);
1984   }
1985 #endif
1986 
1987   void MachNopNode::emit(CodeBuffer &amp;cbuf, PhaseRegAlloc*) const {
<span class="line-modified">1988     C2_MacroAssembler _masm(&amp;cbuf);</span>
1989     __ nop(_count);
1990   }
1991 
1992   uint MachNopNode::size(PhaseRegAlloc*) const {
1993     return _count;
1994   }
1995 
1996 #ifndef PRODUCT
1997   void MachBreakpointNode::format(PhaseRegAlloc*, outputStream* st) const {
1998     st-&gt;print(&quot;# breakpoint&quot;);
1999   }
2000 #endif
2001 
2002   void MachBreakpointNode::emit(CodeBuffer &amp;cbuf, PhaseRegAlloc* ra_) const {
<span class="line-modified">2003     C2_MacroAssembler _masm(&amp;cbuf);</span>
2004     __ int3();
2005   }
2006 
2007   uint MachBreakpointNode::size(PhaseRegAlloc* ra_) const {
2008     return MachNode::size(ra_);
2009   }
2010 
2011 %}
2012 
2013 encode %{
2014 
2015   enc_class call_epilog %{
2016     if (VerifyStackAtCalls) {
2017       // Check that stack depth is unchanged: find majik cookie on stack
2018       int framesize = ra_-&gt;reg2offset_unchecked(OptoReg::add(ra_-&gt;_matcher._old_SP, -3*VMRegImpl::slots_per_word));
<span class="line-modified">2019       C2_MacroAssembler _masm(&amp;cbuf);</span>
2020       Label L;
2021       __ cmpptr(Address(rsp, framesize), (int32_t)0xbadb100d);
2022       __ jccb(Assembler::equal, L);
2023       // Die if stack mismatch
2024       __ int3();
2025       __ bind(L);
2026     }
2027   %}
2028 
2029 %}
2030 
2031 
2032 //----------OPERANDS-----------------------------------------------------------
2033 // Operand definitions must precede instruction definitions for correct parsing
2034 // in the ADLC because operands constitute user defined types which are used in
2035 // instruction definitions.
2036 
2037 // Vectors
2038 
2039 // Dummy generic vector class. Should be used for all vector operands.
</pre>
<hr />
<pre>
3111   match(Set mem (StoreVector mem src));
3112   ins_cost(145);
3113   format %{ &quot;store_vector $mem,$src\n\t&quot; %}
3114   ins_encode %{
3115     switch (vector_length_in_bytes(this, $src)) {
3116       case  4: __ movdl    ($mem$$Address, $src$$XMMRegister); break;
3117       case  8: __ movq     ($mem$$Address, $src$$XMMRegister); break;
3118       case 16: __ movdqu   ($mem$$Address, $src$$XMMRegister); break;
3119       case 32: __ vmovdqu  ($mem$$Address, $src$$XMMRegister); break;
3120       case 64: __ evmovdqul($mem$$Address, $src$$XMMRegister, Assembler::AVX_512bit); break;
3121       default: ShouldNotReachHere();
3122     }
3123   %}
3124   ins_pipe( pipe_slow );
3125 %}
3126 
3127 // ====================REPLICATE=======================================
3128 
3129 // Replicate byte scalar to be vector
3130 instruct ReplB_reg(vec dst, rRegI src) %{


3131   match(Set dst (ReplicateB src));
3132   format %{ &quot;replicateB $dst,$src&quot; %}
3133   ins_encode %{
3134     uint vlen = vector_length(this);
3135     if (vlen == 64 || VM_Version::supports_avx512vlbw()) { // AVX512VL for &lt;512bit operands
<span class="line-modified">3136       assert(VM_Version::supports_avx512bw(), &quot;required&quot;); // 512-bit byte vectors assume AVX512BW</span>
3137       int vlen_enc = vector_length_encoding(this);
3138       __ evpbroadcastb($dst$$XMMRegister, $src$$Register, vlen_enc);
3139     } else {
3140       __ movdl($dst$$XMMRegister, $src$$Register);
3141       __ punpcklbw($dst$$XMMRegister, $dst$$XMMRegister);
3142       __ pshuflw($dst$$XMMRegister, $dst$$XMMRegister, 0x00);
3143       if (vlen &gt;= 16) {
3144         __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
3145         if (vlen &gt;= 32) {
<span class="line-modified">3146           assert(vlen == 32, &quot;sanity&quot;);</span>
3147           __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
3148         }
3149       }
3150     }
3151   %}
3152   ins_pipe( pipe_slow );
3153 %}
3154 
















3155 instruct ReplB_mem(vec dst, memory mem) %{
<span class="line-modified">3156   predicate(VM_Version::supports_avx2());</span>

3157   match(Set dst (ReplicateB (LoadB mem)));
3158   format %{ &quot;replicateB $dst,$mem&quot; %}
3159   ins_encode %{

3160     int vector_len = vector_length_encoding(this);
3161     __ vpbroadcastb($dst$$XMMRegister, $mem$$Address, vector_len);
3162   %}
3163   ins_pipe( pipe_slow );
3164 %}
3165 
3166 instruct ReplB_imm(vec dst, immI con) %{


3167   match(Set dst (ReplicateB con));
3168   format %{ &quot;replicateB $dst,$con&quot; %}
3169   ins_encode %{
3170     uint vlen = vector_length(this);
3171     InternalAddress const_addr = $constantaddress(replicate8_imm($con$$constant, 1));
3172     if (vlen == 4) {
3173       __ movdl($dst$$XMMRegister, const_addr);
3174     } else {
3175       __ movq($dst$$XMMRegister, const_addr);
3176       if (vlen &gt;= 16) {
<span class="line-modified">3177         if (VM_Version::supports_avx2()) {</span>
3178           int vlen_enc = vector_length_encoding(this);
<span class="line-modified">3179           __ vpbroadcastq($dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);</span>
3180         } else {
<span class="line-added">3181           assert(vlen == 16, &quot;sanity&quot;);</span>
3182           __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);




3183         }
3184       }
3185     }
3186   %}
3187   ins_pipe( pipe_slow );
3188 %}
3189 













3190 // Replicate byte scalar zero to be vector
3191 instruct ReplB_zero(vec dst, immI0 zero) %{
3192   match(Set dst (ReplicateB zero));
3193   format %{ &quot;replicateB $dst,$zero&quot; %}
3194   ins_encode %{
3195     uint vlen = vector_length(this);
3196     if (vlen &lt;= 16) {
3197       __ pxor($dst$$XMMRegister, $dst$$XMMRegister);
3198     } else {
3199       // Use vpxor since AVX512F does not have 512bit vxorpd (requires AVX512DQ).
3200       int vlen_enc = vector_length_encoding(this);
3201       __ vpxor($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);
3202     }
3203   %}
3204   ins_pipe( fpu_reg_reg );
3205 %}
3206 
3207 // ====================ReplicateS=======================================
3208 
3209 instruct ReplS_reg(vec dst, rRegI src) %{


3210   match(Set dst (ReplicateS src));
3211   format %{ &quot;replicateS $dst,$src&quot; %}
3212   ins_encode %{
3213     uint vlen = vector_length(this);
3214     if (vlen == 32 || VM_Version::supports_avx512vlbw()) { // AVX512VL for &lt;512bit operands
<span class="line-modified">3215       assert(VM_Version::supports_avx512bw(), &quot;required&quot;); // 512-bit short vectors assume AVX512BW</span>
3216       int vlen_enc = vector_length_encoding(this);
3217       __ evpbroadcastw($dst$$XMMRegister, $src$$Register, vlen_enc);
3218     } else {
3219       __ movdl($dst$$XMMRegister, $src$$Register);
3220       __ pshuflw($dst$$XMMRegister, $dst$$XMMRegister, 0x00);
3221       if (vlen &gt;= 8) {
3222         __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
3223         if (vlen &gt;= 16) {
<span class="line-modified">3224           assert(vlen == 16, &quot;sanity&quot;);</span>
3225           __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
3226         }
3227       }
3228     }
3229   %}
3230   ins_pipe( pipe_slow );
3231 %}
3232 














3233 instruct ReplS_mem(vec dst, memory mem) %{
<span class="line-modified">3234   predicate(VM_Version::supports_avx2());</span>


3235   match(Set dst (ReplicateS (LoadS mem)));
3236   format %{ &quot;replicateS $dst,$mem&quot; %}
3237   ins_encode %{
<span class="line-modified">3238     int vlen_enc = vector_length_encoding(this);</span>
<span class="line-modified">3239     __ vpbroadcastw($dst$$XMMRegister, $mem$$Address, vlen_enc);</span>


























3240   %}
3241   ins_pipe( pipe_slow );
3242 %}
3243 
3244 instruct ReplS_imm(vec dst, immI con) %{


3245   match(Set dst (ReplicateS con));
3246   format %{ &quot;replicateS $dst,$con&quot; %}
3247   ins_encode %{
3248     uint vlen = vector_length(this);
<span class="line-modified">3249     InternalAddress const_addr = $constantaddress(replicate8_imm($con$$constant, 2));</span>
3250     if (vlen == 2) {
<span class="line-modified">3251       __ movdl($dst$$XMMRegister, const_addr);</span>
3252     } else {
<span class="line-modified">3253       __ movq($dst$$XMMRegister, const_addr);</span>
<span class="line-modified">3254       if (vlen &gt;= 8) {</span>
<span class="line-modified">3255         if (VM_Version::supports_avx2()) {</span>
<span class="line-modified">3256           int vlen_enc = vector_length_encoding(this);</span>
<span class="line-modified">3257           __ vpbroadcastw($dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);</span>
<span class="line-modified">3258         } else {</span>
<span class="line-modified">3259           assert(vlen == 8, &quot;sanity&quot;);</span>
<span class="line-modified">3260           __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);</span>



3261         }
3262       }
3263     }
3264   %}
3265   ins_pipe( fpu_reg_reg );
3266 %}
3267 













3268 instruct ReplS_zero(vec dst, immI0 zero) %{
3269   match(Set dst (ReplicateS zero));
3270   format %{ &quot;replicateS $dst,$zero&quot; %}
3271   ins_encode %{
3272     uint vlen = vector_length(this);
3273     if (vlen &lt;= 8) {
3274       __ pxor($dst$$XMMRegister, $dst$$XMMRegister);
3275     } else {
3276       int vlen_enc = vector_length_encoding(this);
3277       __ vpxor($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);
3278     }
3279   %}
3280   ins_pipe( fpu_reg_reg );
3281 %}
3282 
3283 // ====================ReplicateI=======================================
3284 
3285 instruct ReplI_reg(vec dst, rRegI src) %{
3286   match(Set dst (ReplicateI src));
3287   format %{ &quot;replicateI $dst,$src&quot; %}
3288   ins_encode %{
3289     uint vlen = vector_length(this);
3290     if (vlen == 16 || VM_Version::supports_avx512vl()) { // AVX512VL for &lt;512bit operands
3291       int vlen_enc = vector_length_encoding(this);
3292       __ evpbroadcastd($dst$$XMMRegister, $src$$Register, vlen_enc);
3293     } else {
3294       __ movdl($dst$$XMMRegister, $src$$Register);
3295       __ pshufd($dst$$XMMRegister, $dst$$XMMRegister, 0x00);
3296       if (vlen &gt;= 8) {
3297         assert(vlen == 8, &quot;sanity&quot;);
3298         __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
3299       }
3300     }
3301   %}
3302   ins_pipe( pipe_slow );
3303 %}
3304 
3305 instruct ReplI_mem(vec dst, memory mem) %{

3306   match(Set dst (ReplicateI (LoadI mem)));
3307   format %{ &quot;replicateI $dst,$mem&quot; %}
3308   ins_encode %{
3309     uint vlen = vector_length(this);
3310     if (vlen &lt;= 4) {
<span class="line-modified">3311       __ movdl($dst$$XMMRegister, $mem$$Address);</span>
<span class="line-modified">3312       __ pshufd($dst$$XMMRegister, $dst$$XMMRegister, 0x00);</span>
<span class="line-added">3313     } else {</span>
<span class="line-added">3314       assert(VM_Version::supports_avx2(), &quot;sanity&quot;);</span>
3315       int vector_len = vector_length_encoding(this);
3316       __ vpbroadcastd($dst$$XMMRegister, $mem$$Address, vector_len);




3317     }
3318   %}
3319   ins_pipe( pipe_slow );
3320 %}
3321 
3322 instruct ReplI_imm(vec dst, immI con) %{
3323   match(Set dst (ReplicateI con));
3324   format %{ &quot;replicateI $dst,$con&quot; %}
3325   ins_encode %{
3326     uint vlen = vector_length(this);
<span class="line-modified">3327     InternalAddress const_addr = $constantaddress(replicate8_imm($con$$constant, 4));</span>
<span class="line-modified">3328     if (vlen &lt;= 4) {</span>
<span class="line-modified">3329       __ movq($dst$$XMMRegister, const_addr);</span>
<span class="line-modified">3330       if (vlen == 4) {</span>
<span class="line-added">3331         __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);</span>
<span class="line-added">3332       }</span>
<span class="line-added">3333     } else {</span>
<span class="line-added">3334       assert(VM_Version::supports_avx2(), &quot;sanity&quot;);</span>
3335       int vector_len = vector_length_encoding(this);
<span class="line-modified">3336       __ movq($dst$$XMMRegister, const_addr);</span>
3337       __ vpbroadcastd($dst$$XMMRegister, $dst$$XMMRegister, vector_len);







3338     }
3339   %}
3340   ins_pipe( pipe_slow );
3341 %}
3342 
3343 // Replicate integer (4 byte) scalar zero to be vector
3344 instruct ReplI_zero(vec dst, immI0 zero) %{
3345   match(Set dst (ReplicateI zero));
3346   format %{ &quot;replicateI $dst,$zero&quot; %}
3347   ins_encode %{
3348     uint vlen = vector_length(this);
3349     if (vlen &lt;= 4) {
3350       __ pxor($dst$$XMMRegister, $dst$$XMMRegister);
3351     } else {
3352       int vlen_enc = vector_length_encoding(this);
3353       __ vpxor($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);
3354     }
3355   %}
3356   ins_pipe( fpu_reg_reg );
3357 %}
</pre>
<hr />
<pre>
3427     } else {
3428       int vector_len = Assembler::AVX_512bit;
3429       __ movdl($dst$$XMMRegister, $src$$Register);
3430       __ movdl($tmp$$XMMRegister, HIGH_FROM_LOW($src$$Register));
3431       __ punpckldq($dst$$XMMRegister, $tmp$$XMMRegister);
3432       __ vpbroadcastq($dst$$XMMRegister, $dst$$XMMRegister, vector_len);
3433     }
3434   %}
3435   ins_pipe( pipe_slow );
3436 %}
3437 #endif // _LP64
3438 
3439 instruct ReplL_mem(vec dst, memory mem) %{
3440   match(Set dst (ReplicateL (LoadL mem)));
3441   format %{ &quot;replicateL $dst,$mem&quot; %}
3442   ins_encode %{
3443     uint vlen = vector_length(this);
3444     if (vlen == 2) {
3445       __ movq($dst$$XMMRegister, $mem$$Address);
3446       __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
<span class="line-modified">3447     } else {</span>
<span class="line-added">3448       assert(VM_Version::supports_avx2(), &quot;sanity&quot;);</span>
3449       int vlen_enc = vector_length_encoding(this);
3450       __ vpbroadcastq($dst$$XMMRegister, $mem$$Address, vlen_enc);





3451     }
3452   %}
3453   ins_pipe( pipe_slow );
3454 %}
3455 
3456 // Replicate long (8 byte) scalar immediate to be vector by loading from const table.
3457 instruct ReplL_imm(vec dst, immL con) %{
3458   match(Set dst (ReplicateL con));
3459   format %{ &quot;replicateL $dst,$con&quot; %}
3460   ins_encode %{
3461     uint vlen = vector_length(this);
3462     InternalAddress const_addr = $constantaddress($con);
3463     if (vlen == 2) {
3464       __ movq($dst$$XMMRegister, const_addr);
3465       __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
<span class="line-modified">3466     } else {</span>
<span class="line-added">3467       assert(VM_Version::supports_avx2(), &quot;sanity&quot;);</span>
3468       int vlen_enc = vector_length_encoding(this);
3469       __ movq($dst$$XMMRegister, const_addr);
3470       __ vpbroadcastq($dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);





3471     }
3472   %}
3473   ins_pipe( pipe_slow );
3474 %}
3475 
3476 instruct ReplL_zero(vec dst, immL0 zero) %{
3477   match(Set dst (ReplicateL zero));
3478   format %{ &quot;replicateL $dst,$zero&quot; %}
3479   ins_encode %{
3480     int vlen = vector_length(this);
3481     if (vlen == 2) {
3482       __ pxor($dst$$XMMRegister, $dst$$XMMRegister);
3483     } else {
3484       int vlen_enc = vector_length_encoding(this);
3485       __ vpxor($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);
3486     }
3487   %}
3488   ins_pipe( fpu_reg_reg );
3489 %}
3490 
3491 // ====================ReplicateF=======================================
3492 
3493 instruct ReplF_reg(vec dst, vlRegF src) %{
3494   match(Set dst (ReplicateF src));
3495   format %{ &quot;replicateF $dst,$src&quot; %}
3496   ins_encode %{
3497     uint vlen = vector_length(this);
3498     if (vlen &lt;= 4) {
3499       __ pshufd($dst$$XMMRegister, $src$$XMMRegister, 0x00);
<span class="line-modified">3500    } else if (VM_Version::supports_avx2()) {</span>
3501       int vector_len = vector_length_encoding(this);
<span class="line-modified">3502       __ vbroadcastss($dst$$XMMRegister, $src$$XMMRegister, vector_len); // reg-to-reg variant requires AVX2</span>
3503     } else {
3504       assert(vlen == 8, &quot;sanity&quot;);
3505       __ pshufd($dst$$XMMRegister, $src$$XMMRegister, 0x00);
3506       __ vinsertf128_high($dst$$XMMRegister, $dst$$XMMRegister);
3507     }
3508   %}
3509   ins_pipe( pipe_slow );
3510 %}
3511 
3512 instruct ReplF_mem(vec dst, memory mem) %{

3513   match(Set dst (ReplicateF (LoadF mem)));
3514   format %{ &quot;replicateF $dst,$mem&quot; %}
3515   ins_encode %{
3516     uint vlen = vector_length(this);
3517     if (vlen &lt;= 4) {
<span class="line-modified">3518       __ movdl($dst$$XMMRegister, $mem$$Address);</span>
<span class="line-modified">3519       __ pshufd($dst$$XMMRegister, $dst$$XMMRegister, 0x00);</span>
<span class="line-added">3520     } else {</span>
<span class="line-added">3521       assert(VM_Version::supports_avx(), &quot;sanity&quot;);</span>
3522       int vector_len = vector_length_encoding(this);
3523       __ vbroadcastss($dst$$XMMRegister, $mem$$Address, vector_len);




3524     }
3525   %}
3526   ins_pipe( pipe_slow );
3527 %}
3528 
3529 instruct ReplF_zero(vec dst, immF0 zero) %{
3530   match(Set dst (ReplicateF zero));
3531   format %{ &quot;replicateF $dst,$zero&quot; %}
3532   ins_encode %{
3533     uint vlen = vector_length(this);
3534     if (vlen &lt;= 4) {
3535       __ xorps($dst$$XMMRegister, $dst$$XMMRegister);
3536     } else {
3537       int vlen_enc = vector_length_encoding(this);
3538       __ vpxor($dst$$XMMRegister,$dst$$XMMRegister, $dst$$XMMRegister, vlen_enc); // 512bit vxorps requires AVX512DQ
3539     }
3540   %}
3541   ins_pipe( fpu_reg_reg );
3542 %}
3543 
3544 // ====================ReplicateD=======================================
3545 
3546 // Replicate double (8 bytes) scalar to be vector
3547 instruct ReplD_reg(vec dst, vlRegD src) %{
3548   match(Set dst (ReplicateD src));
3549   format %{ &quot;replicateD $dst,$src&quot; %}
3550   ins_encode %{
3551     uint vlen = vector_length(this);
3552     if (vlen == 2) {
3553       __ pshufd($dst$$XMMRegister, $src$$XMMRegister, 0x44);
<span class="line-modified">3554     } else if (VM_Version::supports_avx2()) {</span>
3555       int vector_len = vector_length_encoding(this);
<span class="line-modified">3556       __ vbroadcastsd($dst$$XMMRegister, $src$$XMMRegister, vector_len); // reg-to-reg variant requires AVX2</span>
3557     } else {
3558       assert(vlen == 4, &quot;sanity&quot;);
3559       __ pshufd($dst$$XMMRegister, $src$$XMMRegister, 0x44);
3560       __ vinsertf128_high($dst$$XMMRegister, $dst$$XMMRegister);
3561     }
3562   %}
3563   ins_pipe( pipe_slow );
3564 %}
3565 
3566 instruct ReplD_mem(vec dst, memory mem) %{

3567   match(Set dst (ReplicateD (LoadD mem)));
3568   format %{ &quot;replicateD $dst,$mem&quot; %}
3569   ins_encode %{
3570     uint vlen = vector_length(this);
3571     if (vlen == 2) {
<span class="line-modified">3572       __ movq($dst$$XMMRegister, $mem$$Address);</span>
<span class="line-modified">3573       __ pshufd($dst$$XMMRegister, $dst$$XMMRegister, 0x44);</span>
<span class="line-added">3574     } else {</span>
<span class="line-added">3575       assert(VM_Version::supports_avx(), &quot;sanity&quot;);</span>
3576       int vector_len = vector_length_encoding(this);
3577       __ vbroadcastsd($dst$$XMMRegister, $mem$$Address, vector_len);




3578     }
3579   %}
3580   ins_pipe( pipe_slow );
3581 %}
3582 
3583 instruct ReplD_zero(vec dst, immD0 zero) %{
3584   match(Set dst (ReplicateD zero));
3585   format %{ &quot;replicateD $dst,$zero&quot; %}
3586   ins_encode %{
3587     uint vlen = vector_length(this);
3588     if (vlen == 2) {
3589       __ xorpd($dst$$XMMRegister, $dst$$XMMRegister);
3590     } else {
3591       int vlen_enc = vector_length_encoding(this);
3592       __ vpxor($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, vlen_enc); // 512bit vxorps requires AVX512DQ
3593     }
3594   %}
3595   ins_pipe( fpu_reg_reg );
3596 %}
3597 
3598 // ====================REDUCTION ARITHMETIC=======================================
<span class="line-added">3599 // =======================Int Reduction==========================================</span>
3600 
<span class="line-modified">3601 instruct reductionI(rRegI dst, rRegI src1, vec src2, vec vtmp1, vec vtmp2) %{</span>
<span class="line-modified">3602   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;element_basic_type() == T_INT &amp;&amp;</span>
<span class="line-modified">3603             n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() &lt; 16);</span>
































3604   match(Set dst (AddReductionVI src1 src2));
<span class="line-modified">3605   match(Set dst (MulReductionVI src1 src2));</span>
<span class="line-modified">3606   match(Set dst (AndReductionV  src1 src2));</span>
<span class="line-modified">3607   match(Set dst ( OrReductionV  src1 src2));</span>
<span class="line-modified">3608   match(Set dst (XorReductionV  src1 src2));</span>
<span class="line-modified">3609   effect(TEMP vtmp1, TEMP vtmp2);</span>
<span class="line-modified">3610   format %{ &quot;vector_reduction_int $dst,$src1,$src2 ; using $vtmp1, $vtmp2 as TEMP&quot; %}</span>
<span class="line-modified">3611   ins_encode %{</span>
<span class="line-modified">3612     int opcode = this-&gt;ideal_Opcode();</span>
<span class="line-modified">3613     int vlen = vector_length(this, $src2);</span>
<span class="line-modified">3614     __ reduceI(opcode, vlen, $dst$$Register, $src1$$Register, $src2$$XMMRegister, $vtmp1$$XMMRegister, $vtmp2$$XMMRegister);</span>


















































3615   %}
3616   ins_pipe( pipe_slow );
3617 %}
3618 
<span class="line-modified">3619 instruct reduction16I(rRegI dst, rRegI src1, legVec src2, legVec vtmp1, legVec vtmp2) %{</span>
<span class="line-modified">3620   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;element_basic_type() == T_INT &amp;&amp;</span>
<span class="line-added">3621             n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 16);</span>
3622   match(Set dst (AddReductionVI src1 src2));
<span class="line-modified">3623   match(Set dst (MulReductionVI src1 src2));</span>
<span class="line-modified">3624   match(Set dst (AndReductionV  src1 src2));</span>
<span class="line-added">3625   match(Set dst ( OrReductionV  src1 src2));</span>
<span class="line-added">3626   match(Set dst (XorReductionV  src1 src2));</span>
<span class="line-added">3627   effect(TEMP vtmp1, TEMP vtmp2);</span>
<span class="line-added">3628   format %{ &quot;vector_reduction_int $dst,$src1,$src2 ; using $vtmp1, $vtmp2 as TEMP&quot; %}</span>
3629   ins_encode %{
<span class="line-modified">3630     int opcode = this-&gt;ideal_Opcode();</span>
<span class="line-modified">3631     int vlen = vector_length(this, $src2);</span>
<span class="line-modified">3632     __ reduceI(opcode, vlen, $dst$$Register, $src1$$Register, $src2$$XMMRegister, $vtmp1$$XMMRegister, $vtmp2$$XMMRegister);</span>








3633   %}
3634   ins_pipe( pipe_slow );
3635 %}
3636 
<span class="line-modified">3637 // =======================Long Reduction==========================================</span>
3638 
3639 #ifdef _LP64
<span class="line-modified">3640 instruct reductionL(rRegL dst, rRegL src1, vec src2, vec vtmp1, vec vtmp2) %{</span>
<span class="line-modified">3641   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;element_basic_type() == T_LONG &amp;&amp;</span>
<span class="line-added">3642             n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() &lt; 8);</span>
3643   match(Set dst (AddReductionVL src1 src2));
<span class="line-modified">3644   match(Set dst (MulReductionVL src1 src2));</span>
<span class="line-modified">3645   match(Set dst (AndReductionV  src1 src2));</span>
<span class="line-modified">3646   match(Set dst ( OrReductionV  src1 src2));</span>
<span class="line-modified">3647   match(Set dst (XorReductionV  src1 src2));</span>
<span class="line-modified">3648   effect(TEMP vtmp1, TEMP vtmp2);</span>
<span class="line-modified">3649   format %{ &quot;vector_reduction_long $dst,$src1,$src2 ; using $vtmp1, $vtmp2 as TEMP&quot; %}</span>












3650   ins_encode %{
<span class="line-modified">3651     int opcode = this-&gt;ideal_Opcode();</span>
<span class="line-modified">3652     int vlen = vector_length(this, $src2);</span>
<span class="line-modified">3653     __ reduceL(opcode, vlen, $dst$$Register, $src1$$Register, $src2$$XMMRegister, $vtmp1$$XMMRegister, $vtmp2$$XMMRegister);</span>





3654   %}
3655   ins_pipe( pipe_slow );
3656 %}
3657 
<span class="line-modified">3658 instruct reduction8L(rRegL dst, rRegL src1, legVec src2, legVec vtmp1, legVec vtmp2) %{</span>
<span class="line-modified">3659   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;element_basic_type() == T_LONG &amp;&amp;</span>
<span class="line-added">3660             n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 8);</span>
3661   match(Set dst (AddReductionVL src1 src2));
<span class="line-modified">3662   match(Set dst (MulReductionVL src1 src2));</span>
<span class="line-modified">3663   match(Set dst (AndReductionV  src1 src2));</span>
<span class="line-added">3664   match(Set dst ( OrReductionV  src1 src2));</span>
<span class="line-added">3665   match(Set dst (XorReductionV  src1 src2));</span>
<span class="line-added">3666   effect(TEMP vtmp1, TEMP vtmp2);</span>
<span class="line-added">3667   format %{ &quot;vector_reduction_long $dst,$src1,$src2 ; using $vtmp1, $vtmp2 as TEMP&quot; %}</span>
3668   ins_encode %{
<span class="line-modified">3669     int opcode = this-&gt;ideal_Opcode();</span>
<span class="line-modified">3670     int vlen = vector_length(this, $src2);</span>
<span class="line-modified">3671     __ reduceL(opcode, vlen, $dst$$Register, $src1$$Register, $src2$$XMMRegister, $vtmp1$$XMMRegister, $vtmp2$$XMMRegister);</span>







3672   %}
3673   ins_pipe( pipe_slow );
3674 %}
3675 #endif // _LP64
3676 
<span class="line-modified">3677 // =======================Float Reduction==========================================</span>
3678 
<span class="line-modified">3679 instruct reductionF128(regF dst, vec src, vec vtmp) %{</span>
<span class="line-modified">3680   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() &lt;= 4);</span>
<span class="line-modified">3681   match(Set dst (AddReductionVF dst src));</span>
<span class="line-modified">3682   match(Set dst (MulReductionVF dst src));</span>
<span class="line-modified">3683   effect(TEMP dst, TEMP vtmp);</span>
<span class="line-added">3684   format %{ &quot;vector_reduction_fp  $dst,$src ; using $vtmp as TEMP&quot; %}</span>
3685   ins_encode %{
<span class="line-modified">3686     int opcode = this-&gt;ideal_Opcode();</span>
<span class="line-modified">3687     int vlen = vector_length(this, $src);</span>
<span class="line-modified">3688     __ reduce_fp(opcode, vlen, $dst$$XMMRegister, $src$$XMMRegister, $vtmp$$XMMRegister);</span>


















































































































































































































































































































3689   %}
3690   ins_pipe( pipe_slow );
3691 %}
3692 
<span class="line-modified">3693 instruct reduction8F(regF dst, vec src, vec vtmp1, vec vtmp2) %{</span>
<span class="line-modified">3694   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 8);</span>
<span class="line-modified">3695   match(Set dst (AddReductionVF dst src));</span>
<span class="line-modified">3696   match(Set dst (MulReductionVF dst src));</span>
<span class="line-modified">3697   effect(TEMP dst, TEMP vtmp1, TEMP vtmp2);</span>
<span class="line-added">3698   format %{ &quot;vector_reduction_float $dst,$src ; using $vtmp1, $vtmp2 as TEMP&quot; %}</span>
3699   ins_encode %{
<span class="line-modified">3700     int opcode = this-&gt;ideal_Opcode();</span>
<span class="line-modified">3701     int vlen = vector_length(this, $src);</span>
<span class="line-modified">3702     __ reduce_fp(opcode, vlen, $dst$$XMMRegister, $src$$XMMRegister, $vtmp1$$XMMRegister, $vtmp2$$XMMRegister);</span>







3703   %}
3704   ins_pipe( pipe_slow );
3705 %}



3706 
<span class="line-modified">3707 instruct reduction16F(regF dst, legVec src, legVec vtmp1, legVec vtmp2) %{</span>
<span class="line-modified">3708   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 16);</span>
<span class="line-modified">3709   match(Set dst (AddReductionVF dst src));</span>
<span class="line-modified">3710   match(Set dst (MulReductionVF dst src));</span>
<span class="line-modified">3711   effect(TEMP dst, TEMP vtmp1, TEMP vtmp2);</span>
<span class="line-added">3712   format %{ &quot;vector_reduction_float $dst,$src ; using $vtmp1, $vtmp2 as TEMP&quot; %}</span>
3713   ins_encode %{
<span class="line-modified">3714     int opcode = this-&gt;ideal_Opcode();</span>
<span class="line-modified">3715     int vlen = vector_length(this, $src);</span>
<span class="line-modified">3716     __ reduce_fp(opcode, vlen, $dst$$XMMRegister, $src$$XMMRegister, $vtmp1$$XMMRegister, $vtmp2$$XMMRegister);</span>







3717   %}
3718   ins_pipe( pipe_slow );
3719 %}
3720 
<span class="line-modified">3721 // =======================Double Reduction==========================================</span>


























3722 
<span class="line-modified">3723 instruct reduction2D(regD dst, vec src, vec vtmp) %{</span>
<span class="line-modified">3724   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 2);</span>
<span class="line-modified">3725   match(Set dst (AddReductionVD dst src));</span>
<span class="line-modified">3726   match(Set dst (MulReductionVD dst src));</span>
<span class="line-modified">3727   effect(TEMP dst, TEMP vtmp);</span>
<span class="line-modified">3728   format %{ &quot;vector_reduction_double $dst,$src ; using $vtmp as TEMP&quot; %}</span>





































































3729   ins_encode %{
<span class="line-modified">3730     int opcode = this-&gt;ideal_Opcode();</span>
<span class="line-modified">3731     int vlen = vector_length(this, $src);</span>
<span class="line-modified">3732     __ reduce_fp(opcode, vlen, $dst$$XMMRegister, $src$$XMMRegister, $vtmp$$XMMRegister);</span>







3733   %}
3734   ins_pipe( pipe_slow );
3735 %}
3736 
<span class="line-modified">3737 instruct reduction4D(regD dst, vec src, vec vtmp1, vec vtmp2) %{</span>
<span class="line-modified">3738   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 4);</span>
<span class="line-modified">3739   match(Set dst (AddReductionVD dst src));</span>
<span class="line-modified">3740   match(Set dst (MulReductionVD dst src));</span>
<span class="line-modified">3741   effect(TEMP dst, TEMP vtmp1, TEMP vtmp2);</span>
<span class="line-modified">3742   format %{ &quot;vector_reduction_double $dst,$src ; using $vtmp1, $vtmp2 as TEMP&quot; %}</span>
3743   ins_encode %{
<span class="line-modified">3744     int opcode = this-&gt;ideal_Opcode();</span>
<span class="line-modified">3745     int vlen = vector_length(this, $src);</span>
<span class="line-modified">3746     __ reduce_fp(opcode, vlen, $dst$$XMMRegister, $src$$XMMRegister, $vtmp1$$XMMRegister, $vtmp2$$XMMRegister);</span>





3747   %}
3748   ins_pipe( pipe_slow );
3749 %}
3750 
<span class="line-modified">3751 instruct reduction8D(regD dst, legVec src, legVec vtmp1, legVec vtmp2) %{</span>
<span class="line-modified">3752   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 8);</span>
<span class="line-modified">3753   match(Set dst (AddReductionVD dst src));</span>
<span class="line-modified">3754   match(Set dst (MulReductionVD dst src));</span>
<span class="line-modified">3755   effect(TEMP dst, TEMP vtmp1, TEMP vtmp2);</span>
<span class="line-added">3756   format %{ &quot;vector_reduction_double $dst,$src ; using $vtmp1, $vtmp2 as TEMP&quot; %}</span>
3757   ins_encode %{
<span class="line-modified">3758     int opcode = this-&gt;ideal_Opcode();</span>
<span class="line-modified">3759     int vlen = vector_length(this, $src);</span>
<span class="line-modified">3760     __ reduce_fp(opcode, vlen, $dst$$XMMRegister, $src$$XMMRegister, $vtmp1$$XMMRegister, $vtmp2$$XMMRegister);</span>













3761   %}
3762   ins_pipe( pipe_slow );
3763 %}
3764 
3765 // ====================VECTOR ARITHMETIC=======================================
3766 
3767 // --------------------------------- ADD --------------------------------------
3768 
3769 // Bytes vector add
3770 instruct vaddB(vec dst, vec src) %{
3771   predicate(UseAVX == 0);
3772   match(Set dst (AddVB dst src));
3773   format %{ &quot;paddb   $dst,$src\t! add packedB&quot; %}
3774   ins_encode %{
3775     __ paddb($dst$$XMMRegister, $src$$XMMRegister);
3776   %}
3777   ins_pipe( pipe_slow );
3778 %}
3779 
3780 instruct vaddB_reg(vec dst, vec src1, vec src2) %{
</pre>
<hr />
<pre>
5112   ins_encode %{
5113     __ pmaddwd($dst$$XMMRegister, $src1$$XMMRegister);
5114   %}
5115   ins_pipe( pipe_slow );
5116 %}
5117 
5118 instruct vmuladdS2I_reg_avx(vec dst, vec src1, vec src2) %{
5119   predicate(UseAVX &gt; 0);
5120   match(Set dst (MulAddVS2VI src1 src2));
5121   format %{ &quot;vpmaddwd $dst,$src1,$src2\t! muladd packedStoI&quot; %}
5122   ins_encode %{
5123     int vector_len = vector_length_encoding(this);
5124     __ vpmaddwd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
5125   %}
5126   ins_pipe( pipe_slow );
5127 %}
5128 
5129 // --------------------------------- Vector Multiply Add Add ----------------------------------
5130 
5131 instruct vmuladdaddS2I_reg(vec dst, vec src1, vec src2) %{
<span class="line-modified">5132   predicate(VM_Version::supports_avx512_vnni());</span>
5133   match(Set dst (AddVI (MulAddVS2VI src1 src2) dst));
5134   format %{ &quot;evpdpwssd $dst,$src1,$src2\t! muladdadd packedStoI&quot; %}
5135   ins_encode %{
5136     assert(UseAVX &gt; 2, &quot;required&quot;);
5137     int vector_len = vector_length_encoding(this);
5138     __ evpdpwssd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
5139   %}
5140   ins_pipe( pipe_slow );
5141   ins_cost(10);
5142 %}
5143 
5144 // --------------------------------- PopCount --------------------------------------
5145 
5146 instruct vpopcountI(vec dst, vec src) %{
5147   match(Set dst (PopCountVI src));
5148   format %{ &quot;vpopcntd  $dst,$src\t! vector popcount packedI&quot; %}
5149   ins_encode %{
5150     assert(UsePopCountInstruction, &quot;not enabled&quot;);
5151 
5152     int vector_len = vector_length_encoding(this);
</pre>
</td>
</tr>
</table>
<center><a href="vtableStubs_x86_32.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="x86_32.ad.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>