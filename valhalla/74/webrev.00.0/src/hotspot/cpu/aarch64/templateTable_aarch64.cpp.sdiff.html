<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/cpu/aarch64/templateTable_aarch64.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="globals_aarch64.hpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="../ppc/globals_ppc.hpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/cpu/aarch64/templateTable_aarch64.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
   1 /*
<span class="line-modified">   2  * Copyright (c) 2003, 2019, Oracle and/or its affiliates. All rights reserved.</span>
   3  * Copyright (c) 2014, Red Hat Inc. All rights reserved.
   4  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   5  *
   6  * This code is free software; you can redistribute it and/or modify it
   7  * under the terms of the GNU General Public License version 2 only, as
   8  * published by the Free Software Foundation.
   9  *
  10  * This code is distributed in the hope that it will be useful, but WITHOUT
  11  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  12  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  13  * version 2 for more details (a copy is included in the LICENSE file that
  14  * accompanied this code).
  15  *
  16  * You should have received a copy of the GNU General Public License version
  17  * 2 along with this work; if not, write to the Free Software Foundation,
  18  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  19  *
  20  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  21  * or visit www.oracle.com if you need additional information or have any
  22  * questions.
</pre>
<hr />
<pre>
1152 
1153 
1154   // Come here on success
1155   __ bind(ok_is_subtype);
1156 
1157 
1158   // Get the value we will store
1159   __ ldr(r0, at_tos());
1160   // Now store using the appropriate barrier
1161   do_oop_store(_masm, element_address, r0, IS_ARRAY);
1162   __ b(done);
1163 
1164   // Have a NULL in r0, r3=array, r2=index.  Store NULL at ary[idx]
1165   __ bind(is_null);
1166   __ profile_null_seen(r2);
1167 
1168   if (EnableValhalla) {
1169     Label is_null_into_value_array_npe, store_null;
1170 
1171     // No way to store null in flat array
<span class="line-modified">1172     __ test_null_free_array_oop(r3, r8, is_null_into_value_array_npe); </span>
1173     __ b(store_null);
1174 
1175     __ bind(is_null_into_value_array_npe);
1176     __ b(ExternalAddress(Interpreter::_throw_NullPointerException_entry));
1177 
1178     __ bind(store_null);
1179   }
1180 
1181   // Store a NULL
<span class="line-modified">1182   do_oop_store(_masm, element_address, noreg, IS_ARRAY); </span>
1183   __ b(done);
1184 
<span class="line-modified">1185   if (EnableValhalla) { </span>
1186      Label is_type_ok;
1187 
1188     // store non-null value
1189     __ bind(is_flat_array);
1190 
1191     // Simplistic type check...
1192     // r0 - value, r2 - index, r3 - array.
1193 
1194     // Profile the not-null value&#39;s klass.
<span class="line-modified">1195     // Load value class </span>
1196      __ load_klass(r1, r0);
1197      __ profile_typecheck(r2, r1, r0); // blows r2, and r0
1198 
1199     // flat value array needs exact type match
1200     // is &quot;r8 == r0&quot; (value subclass == array element superclass)
1201 
1202     // Move element klass into r0
1203 
1204      __ load_klass(r0, r3);
1205 
<span class="line-modified">1206      __ ldr(r0, Address(r0, ArrayKlass::element_klass_offset())); </span>
1207      __ cmp(r0, r1);
1208      __ br(Assembler::EQ, is_type_ok);
1209 
1210      __ profile_typecheck_failed(r2);
1211      __ b(ExternalAddress(Interpreter::_throw_ArrayStoreException_entry));
1212 
1213      __ bind(is_type_ok);
1214 
1215     // Reload from TOS to be safe, because of profile_typecheck that blows r2 and r0.
1216     // FIXME: Should we really do it?
1217      __ ldr(r1, at_tos());  // value
<span class="line-modified">1218      __ mov(r2, r3); // array, ldr(r2, at_tos_p2()); </span>
1219      __ ldr(r3, at_tos_p1()); // index
1220      __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::value_array_store), r1, r2, r3);
1221   }
1222 
1223 
1224   // Pop stack arguments
1225   __ bind(done);
1226   __ add(esp, esp, 3 * Interpreter::stackElementSize);
1227 }
1228 
1229 void TemplateTable::bastore()
1230 {
1231   transition(itos, vtos);
1232   __ pop_i(r1);
1233   __ pop_ptr(r3);
1234   // r0: value
1235   // r1: index
1236   // r3: array
1237   index_check(r3, r1); // prefer index in r1
1238 
</pre>
<hr />
<pre>
2381     __ tbz(r3, exact_log2(JVM_ACC_HAS_FINALIZER), skip_register_finalizer);
2382 
2383     __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::register_finalizer), c_rarg1);
2384 
2385     __ bind(skip_register_finalizer);
2386   }
2387 
2388   // Issue a StoreStore barrier after all stores but before return
2389   // from any constructor for any class with a final field.  We don&#39;t
2390   // know if this is a finalizer, so we always do so.
2391   if (_desc-&gt;bytecode() == Bytecodes::_return)
2392     __ membar(MacroAssembler::StoreStore);
2393 
2394   // Narrow result if state is itos but result type is smaller.
2395   // Need to narrow in the return bytecode rather than in generate_return_entry
2396   // since compiled code callers expect the result to already be narrowed.
2397   if (state == itos) {
2398     __ narrow(r0);
2399   }
2400 
<span class="line-modified">2401   __ remove_activation(state); </span>
2402   __ ret(lr);
2403 }
2404 
2405 // ----------------------------------------------------------------------------
2406 // Volatile variables demand their effects be made known to all CPU&#39;s
2407 // in order.  Store buffers on most chips allow reads &amp; writes to
2408 // reorder; the JMM&#39;s ReadAfterWrite.java test fails in -Xint mode
2409 // without some kind of memory barrier (i.e., it&#39;s not sufficient that
2410 // the interpreter does not reorder volatile references, the hardware
2411 // also must not reorder them).
2412 //
2413 // According to the new Java Memory Model (JMM):
2414 // (1) All volatiles are serialized wrt to each other.  ALSO reads &amp;
2415 //     writes act as aquire &amp; release, so:
2416 // (2) A read cannot let unrelated NON-volatile memory refs that
2417 //     happen after the read float up to before the read.  It&#39;s OK for
2418 //     non-volatile memory refs that happen before the volatile read to
2419 //     float down below it.
2420 // (3) Similar a volatile write cannot let unrelated NON-volatile
2421 //     memory refs that happen BEFORE the write float down to after the
</pre>
<hr />
<pre>
2590   const Register off   = r19;
2591   const Register flags = r0;
2592   const Register raw_flags = r6;
2593   const Register bc    = r4; // uses same reg as obj, so don&#39;t mix them
2594 
2595   resolve_cache_and_index(byte_no, cache, index, sizeof(u2));
2596   jvmti_post_field_access(cache, index, is_static, false);
2597   load_field_cp_cache_entry(obj, cache, index, off, raw_flags, is_static);
2598 
2599   if (!is_static) {
2600     // obj is on the stack
2601     pop_and_check_object(obj);
2602   }
2603 
2604   // 8179954: We need to make sure that the code generated for
2605   // volatile accesses forms a sequentially-consistent set of
2606   // operations when combined with STLR and LDAR.  Without a leading
2607   // membar it&#39;s possible for a simple Dekker test to fail if loads
2608   // use LDR;DMB but stores use STLR.  This can happen if C2 compiles
2609   // the stores in one method and we interpret the loads in another.
<span class="line-modified">2610   if (! UseBarriersForVolatile) {</span>
2611     Label notVolatile;
2612     __ tbz(raw_flags, ConstantPoolCacheEntry::is_volatile_shift, notVolatile);
2613     __ membar(MacroAssembler::AnyAny);
2614     __ bind(notVolatile);
2615   }
2616 
2617   const Address field(obj, off);
2618 
2619   Label Done, notByte, notBool, notInt, notShort, notChar,
2620               notLong, notFloat, notObj, notDouble;
2621 
2622   // x86 uses a shift and mask or wings it with a shift plus assert
2623   // the mask is not needed. aarch64 just uses bitfield extract
2624   __ ubfxw(flags, raw_flags, ConstantPoolCacheEntry::tos_state_shift, ConstantPoolCacheEntry::tos_state_bits);
2625 
2626   assert(btos == 0, &quot;change code, btos != 0&quot;);
2627   __ cbnz(flags, notByte);
2628 
2629   // Don&#39;t rewrite getstatic, only getfield
2630   if (is_static) rc = may_not_rewrite;
</pre>
<hr />
<pre>
2644 
2645   // ztos (same code as btos)
2646   __ access_load_at(T_BOOLEAN, IN_HEAP, r0, field, noreg, noreg);
2647   __ push(ztos);
2648   // Rewrite bytecode to be faster
2649   if (rc == may_rewrite) {
2650     // use btos rewriting, no truncating to t/f bit is needed for getfield.
2651     patch_bytecode(Bytecodes::_fast_bgetfield, bc, r1);
2652   }
2653   __ b(Done);
2654 
2655   __ bind(notBool);
2656   __ cmp(flags, (u1)atos);
2657   __ br(Assembler::NE, notObj);
2658   // atos
2659   if (!EnableValhalla) {
2660     do_oop_load(_masm, field, r0, IN_HEAP);
2661     __ push(atos);
2662     if (rc == may_rewrite) {
2663       patch_bytecode(Bytecodes::_fast_agetfield, bc, r1);
<span class="line-modified">2664     }  </span>
2665     __ b(Done);
2666   } else { // Valhalla
2667 
2668     if (is_static) {
2669       __ load_heap_oop(r0, field);
2670       Label isFlattenable, isUninitialized;
2671       // Issue below if the static field has not been initialized yet
2672       __ test_field_is_flattenable(raw_flags, r8 /*temp*/, isFlattenable);
2673         // Not flattenable case
2674         __ push(atos);
2675         __ b(Done);
2676       // Flattenable case, must not return null even if uninitialized
2677       __ bind(isFlattenable);
2678         __ cbz(r0, isUninitialized);
2679           __ push(atos);
2680           __ b(Done);
2681         __ bind(isUninitialized);
2682           __ andw(raw_flags, raw_flags, ConstantPoolCacheEntry::field_index_mask);
2683           __ call_VM(r0, CAST_FROM_FN_PTR(address, InterpreterRuntime::uninitialized_static_value_field), obj, raw_flags);
2684           __ verify_oop(r0);
</pre>
<hr />
<pre>
2691         __ load_heap_oop(r0, field);
2692         __ push(atos);
2693         if (rc == may_rewrite) {
2694           patch_bytecode(Bytecodes::_fast_agetfield, bc, r1);
2695         }
2696         __ b(Done);
2697       __ bind(isFlattenable);
2698         __ test_field_is_flattened(raw_flags, r8 /* temp */, isFlattened);
2699          // Non-flattened field case
2700           __ load_heap_oop(r0, field);
2701           __ cbnz(r0, isInitialized);
2702             __ andw(raw_flags, raw_flags, ConstantPoolCacheEntry::field_index_mask);
2703             __ call_VM(r0, CAST_FROM_FN_PTR(address, InterpreterRuntime::uninitialized_instance_value_field), obj, raw_flags);
2704           __ bind(isInitialized);
2705           __ verify_oop(r0);
2706           __ push(atos);
2707           __ b(rewriteFlattenable);
2708         __ bind(isFlattened);
2709           __ ldr(r10, Address(cache, in_bytes(ConstantPoolCache::base_offset() + ConstantPoolCacheEntry::f1_offset())));
2710           __ andw(raw_flags, raw_flags, ConstantPoolCacheEntry::field_index_mask);
<span class="line-modified">2711           call_VM(r0, CAST_FROM_FN_PTR(address, InterpreterRuntime::read_flattened_field), obj, raw_flags, r10); </span>
2712           __ verify_oop(r0);
2713           __ push(atos);
2714       __ bind(rewriteFlattenable);
<span class="line-modified">2715       if (rc == may_rewrite) { </span>
2716          patch_bytecode(Bytecodes::_fast_qgetfield, bc, r1);
2717       }
2718       __ b(Done);
2719     }
2720   }
2721 
2722   __ bind(notObj);
2723   __ cmp(flags, (u1)itos);
2724   __ br(Assembler::NE, notInt);
2725   // itos
2726   __ access_load_at(T_INT, IN_HEAP, r0, field, noreg, noreg);
2727   __ push(itos);
2728   // Rewrite bytecode to be faster
2729   if (rc == may_rewrite) {
2730     patch_bytecode(Bytecodes::_fast_igetfield, bc, r1);
2731   }
2732   __ b(Done);
2733 
2734   __ bind(notInt);
2735   __ cmp(flags, (u1)ctos);
</pre>
<hr />
<pre>
2893   resolve_cache_and_index(byte_no, cache, index, sizeof(u2));
2894   jvmti_post_field_mod(cache, index, is_static);
2895   load_field_cp_cache_entry(obj, cache, index, off, flags, is_static);
2896 
2897   Label Done;
2898   __ mov(r5, flags);
2899 
2900   {
2901     Label notVolatile;
2902     __ tbz(r5, ConstantPoolCacheEntry::is_volatile_shift, notVolatile);
2903     __ membar(MacroAssembler::StoreStore | MacroAssembler::LoadStore);
2904     __ bind(notVolatile);
2905   }
2906 
2907   // field address
2908   const Address field(obj, off);
2909 
2910   Label notByte, notBool, notInt, notShort, notChar,
2911         notLong, notFloat, notObj, notDouble;
2912 
<span class="line-modified">2913   __ mov(flags2, flags); </span>
2914 
2915   // x86 uses a shift and mask or wings it with a shift plus assert
2916   // the mask is not needed. aarch64 just uses bitfield extract
2917   __ ubfxw(flags, flags, ConstantPoolCacheEntry::tos_state_shift,  ConstantPoolCacheEntry::tos_state_bits);
2918 
2919   assert(btos == 0, &quot;change code, btos != 0&quot;);
2920   __ cbnz(flags, notByte);
2921 
2922   // Don&#39;t rewrite putstatic, only putfield
2923   if (is_static) rc = may_not_rewrite;
2924 
2925   // btos
2926   {
2927     __ pop(btos);
2928     if (!is_static) pop_and_check_object(obj);
2929     __ access_store_at(T_BYTE, IN_HEAP, field, r0, noreg, noreg);
2930     if (rc == may_rewrite) {
2931       patch_bytecode(Bytecodes::_fast_bputfield, bc, r1, true, byte_no);
2932     }
2933     __ b(Done);
</pre>
<hr />
<pre>
2954 
2955   // atos
2956   {
2957      if (!EnableValhalla) {
2958       __ pop(atos);
2959       if (!is_static) pop_and_check_object(obj);
2960       // Store into the field
2961       do_oop_store(_masm, field, r0, IN_HEAP);
2962       if (rc == may_rewrite) {
2963         patch_bytecode(Bytecodes::_fast_aputfield, bc, r1, true, byte_no);
2964       }
2965       __ b(Done);
2966      } else { // Valhalla
2967 
2968       __ pop(atos);
2969       if (is_static) {
2970         Label notFlattenable;
2971          __ test_field_is_not_flattenable(flags2, r8 /* temp */, notFlattenable);
2972          __ null_check(r0);
2973          __ bind(notFlattenable);
<span class="line-modified">2974          do_oop_store(_masm, field, r0, IN_HEAP); </span>
2975          __ b(Done);
2976       } else {
2977         Label isFlattenable, isFlattened, notBuffered, notBuffered2, rewriteNotFlattenable, rewriteFlattenable;
2978         __ test_field_is_flattenable(flags2, r8 /*temp*/, isFlattenable);
2979         // Not flattenable case, covers not flattenable values and objects
2980         pop_and_check_object(obj);
2981         // Store into the field
2982         do_oop_store(_masm, field, r0, IN_HEAP);
2983         __ bind(rewriteNotFlattenable);
2984         if (rc == may_rewrite) {
<span class="line-modified">2985           patch_bytecode(Bytecodes::_fast_aputfield, bc, r19, true, byte_no); </span>
2986         }
2987         __ b(Done);
2988         // Implementation of the flattenable semantic
2989         __ bind(isFlattenable);
2990         __ null_check(r0);
2991         __ test_field_is_flattened(flags2, r8 /*temp*/, isFlattened);
2992         // Not flattened case
2993         pop_and_check_object(obj);
2994         // Store into the field
2995         do_oop_store(_masm, field, r0, IN_HEAP);
2996         __ b(rewriteFlattenable);
2997         __ bind(isFlattened);
2998         pop_and_check_object(obj);
2999         call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::write_flattened_value), r0, off, obj);
3000         __ bind(rewriteFlattenable);
3001         if (rc == may_rewrite) {
3002           patch_bytecode(Bytecodes::_fast_qputfield, bc, r19, true, byte_no);
3003         }
3004         __ b(Done);
3005       }
</pre>
<hr />
<pre>
3204   // replace index with field offset from cache entry
3205   __ ldr(r1, Address(r2, in_bytes(base + ConstantPoolCacheEntry::f2_offset())));
3206 
3207   {
3208     Label notVolatile;
3209     __ tbz(r3, ConstantPoolCacheEntry::is_volatile_shift, notVolatile);
3210     __ membar(MacroAssembler::StoreStore | MacroAssembler::LoadStore);
3211     __ bind(notVolatile);
3212   }
3213 
3214   Label notVolatile;
3215 
3216   // Get object from stack
3217   pop_and_check_object(r2);
3218 
3219   // field address
3220   const Address field(r2, r1);
3221 
3222   // access field
3223   switch (bytecode()) {
<span class="line-modified">3224   case Bytecodes::_fast_qputfield: //fall through </span>
3225    {
<span class="line-modified">3226       Label isFlattened, done; </span>
3227       __ null_check(r0);
3228       __ test_field_is_flattened(r3, r8 /* temp */, isFlattened);
3229       // No Flattened case
3230       do_oop_store(_masm, field, r0, IN_HEAP);
3231       __ b(done);
3232       __ bind(isFlattened);
3233       call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::write_flattened_value), r0, r1, r2);
3234       __ bind(done);
3235     }
3236     break;
3237   case Bytecodes::_fast_aputfield:
3238     do_oop_store(_masm, field, r0, IN_HEAP);
3239     break;
3240   case Bytecodes::_fast_lputfield:
3241     __ access_store_at(T_LONG, IN_HEAP, field, r0, noreg, noreg);
3242     break;
3243   case Bytecodes::_fast_iputfield:
3244     __ access_store_at(T_INT, IN_HEAP, field, r0, noreg, noreg);
3245     break;
3246   case Bytecodes::_fast_zputfield:
</pre>
<hr />
<pre>
3301   }
3302 
3303   // access constant pool cache
3304   __ get_cache_and_index_at_bcp(r2, r1, 1);
3305   __ ldr(r1, Address(r2, in_bytes(ConstantPoolCache::base_offset() +
3306                                   ConstantPoolCacheEntry::f2_offset())));
3307   __ ldrw(r3, Address(r2, in_bytes(ConstantPoolCache::base_offset() +
3308                                    ConstantPoolCacheEntry::flags_offset())));
3309 
3310   // r0: object
3311   __ verify_oop(r0);
3312   __ null_check(r0);
3313   const Address field(r0, r1);
3314 
3315   // 8179954: We need to make sure that the code generated for
3316   // volatile accesses forms a sequentially-consistent set of
3317   // operations when combined with STLR and LDAR.  Without a leading
3318   // membar it&#39;s possible for a simple Dekker test to fail if loads
3319   // use LDR;DMB but stores use STLR.  This can happen if C2 compiles
3320   // the stores in one method and we interpret the loads in another.
<span class="line-modified">3321   if (! UseBarriersForVolatile) {</span>
3322     Label notVolatile;
3323     __ tbz(r3, ConstantPoolCacheEntry::is_volatile_shift, notVolatile);
3324     __ membar(MacroAssembler::AnyAny);
3325     __ bind(notVolatile);
3326   }
3327 
3328   // access field
3329   switch (bytecode()) {
<span class="line-modified">3330   case Bytecodes::_fast_qgetfield: </span>
3331     {
3332        Label isFlattened, isInitialized, Done;
3333        // FIXME: We don&#39;t need to reload registers multiple times, but stay close to x86 code
<span class="line-modified">3334        __ ldrw(r9, Address(r2, in_bytes(ConstantPoolCache::base_offset() + ConstantPoolCacheEntry::flags_offset()))); </span>
3335        __ test_field_is_flattened(r9, r8 /* temp */, isFlattened);
3336         // Non-flattened field case
3337         __ mov(r9, r0);
3338         __ load_heap_oop(r0, field);
3339         __ cbnz(r0, isInitialized);
3340           __ mov(r0, r9);
<span class="line-modified">3341           __ ldrw(r9, Address(r2, in_bytes(ConstantPoolCache::base_offset() + ConstantPoolCacheEntry::flags_offset()))); </span>
3342           __ andw(r9, r9, ConstantPoolCacheEntry::field_index_mask);
3343           __ call_VM(r0, CAST_FROM_FN_PTR(address, InterpreterRuntime::uninitialized_instance_value_field), r0, r9);
3344         __ bind(isInitialized);
3345         __ verify_oop(r0);
3346         __ b(Done);
3347       __ bind(isFlattened);
3348         __ ldrw(r9, Address(r2, in_bytes(ConstantPoolCache::base_offset() + ConstantPoolCacheEntry::flags_offset())));
3349         __ andw(r9, r9, ConstantPoolCacheEntry::field_index_mask);
3350         __ ldr(r3, Address(r2, in_bytes(ConstantPoolCache::base_offset() + ConstantPoolCacheEntry::f1_offset())));
3351         call_VM(r0, CAST_FROM_FN_PTR(address, InterpreterRuntime::read_flattened_field), r0, r9, r3);
3352         __ verify_oop(r0);
3353       __ bind(Done);
3354     }
3355     break;
3356   case Bytecodes::_fast_agetfield:
3357     do_oop_load(_masm, field, r0, IN_HEAP);
3358     __ verify_oop(r0);
3359     break;
3360   case Bytecodes::_fast_lgetfield:
3361     __ access_load_at(T_LONG, IN_HEAP, r0, field, noreg, noreg);
</pre>
<hr />
<pre>
3389   }
3390 }
3391 
3392 void TemplateTable::fast_xaccess(TosState state)
3393 {
3394   transition(vtos, state);
3395 
3396   // get receiver
3397   __ ldr(r0, aaddress(0));
3398   // access constant pool cache
3399   __ get_cache_and_index_at_bcp(r2, r3, 2);
3400   __ ldr(r1, Address(r2, in_bytes(ConstantPoolCache::base_offset() +
3401                                   ConstantPoolCacheEntry::f2_offset())));
3402 
3403   // 8179954: We need to make sure that the code generated for
3404   // volatile accesses forms a sequentially-consistent set of
3405   // operations when combined with STLR and LDAR.  Without a leading
3406   // membar it&#39;s possible for a simple Dekker test to fail if loads
3407   // use LDR;DMB but stores use STLR.  This can happen if C2 compiles
3408   // the stores in one method and we interpret the loads in another.
<span class="line-modified">3409   if (! UseBarriersForVolatile) {</span>
3410     Label notVolatile;
3411     __ ldrw(r3, Address(r2, in_bytes(ConstantPoolCache::base_offset() +
3412                                      ConstantPoolCacheEntry::flags_offset())));
3413     __ tbz(r3, ConstantPoolCacheEntry::is_volatile_shift, notVolatile);
3414     __ membar(MacroAssembler::AnyAny);
3415     __ bind(notVolatile);
3416   }
3417 
3418   // make sure exception is reported in correct bcp range (getfield is
3419   // next instruction)
3420   __ increment(rbcp);
3421   __ null_check(r0);
3422   switch (state) {
3423   case itos:
3424     __ access_load_at(T_INT, IN_HEAP, r0, Address(r0, r1, Address::lsl(0)), noreg, noreg);
3425     break;
3426   case atos:
3427     do_oop_load(_masm, Address(r0, r1, Address::lsl(0)), r0, IN_HEAP);
3428     __ verify_oop(r0);
3429     break;
</pre>
<hr />
<pre>
3908   __ membar(Assembler::StoreStore);
3909 }
3910 
3911 void TemplateTable::defaultvalue() {
3912   transition(vtos, atos);
3913   __ get_unsigned_2_byte_index_at_bcp(c_rarg2, 1);
3914   __ get_constant_pool(c_rarg1);
3915   call_VM(r0, CAST_FROM_FN_PTR(address, InterpreterRuntime::defaultvalue),
3916           c_rarg1, c_rarg2);
3917   __ verify_oop(r0);
3918   // Must prevent reordering of stores for object initialization with stores that publish the new object.
3919   __ membar(Assembler::StoreStore);
3920 }
3921 
3922 void TemplateTable::withfield() {
3923   transition(vtos, atos);
3924   resolve_cache_and_index(f2_byte, c_rarg1 /*cache*/, c_rarg2 /*index*/, sizeof(u2));
3925 
3926   // n.b. unlike x86 cache is now rcpool plus the indexed offset
3927   // so using rcpool to meet shared code expectations
<span class="line-modified">3928  </span>
3929   call_VM(r1, CAST_FROM_FN_PTR(address, InterpreterRuntime::withfield), rcpool);
3930   __ verify_oop(r1);
3931   __ add(esp, esp, r0);
3932   __ mov(r0, r1);
3933 }
3934 
3935 void TemplateTable::newarray() {
3936   transition(itos, atos);
3937   __ load_unsigned_byte(c_rarg1, at_bcp(1));
3938   __ mov(c_rarg2, r0);
3939   call_VM(r0, CAST_FROM_FN_PTR(address, InterpreterRuntime::newarray),
3940           c_rarg1, c_rarg2);
3941   // Must prevent reordering of stores for object initialization with stores that publish the new object.
3942   __ membar(Assembler::StoreStore);
3943 }
3944 
3945 void TemplateTable::anewarray() {
3946   transition(itos, atos);
3947   __ get_unsigned_2_byte_index_at_bcp(c_rarg2, 1);
3948   __ get_constant_pool(c_rarg1);
</pre>
<hr />
<pre>
3999   // object is at TOS
4000   __ b(Interpreter::_throw_ClassCastException_entry);
4001 
4002   // Come here on success
4003   __ bind(ok_is_subtype);
4004   __ mov(r0, r3); // Restore object in r3
4005 
4006   __ b(done);
4007   __ bind(is_null);
4008 
4009   // Collect counts on whether this test sees NULLs a lot or not.
4010   if (ProfileInterpreter) {
4011     __ profile_null_seen(r2);
4012   }
4013 
4014   if (EnableValhalla) {
4015     // Get cpool &amp; tags index
4016     __ get_cpool_and_tags(r2, r3); // r2=cpool, r3=tags array
4017     __ get_unsigned_2_byte_index_at_bcp(r19, 1); // r19=index
4018      // See if bytecode has already been quicked
<span class="line-modified">4019     __ add(rscratch1, r3, Array&lt;u1&gt;::base_offset_in_bytes());</span>
4020     __ lea(r1, Address(rscratch1, r19));
4021     __ ldarb(r1, r1); 
4022     // See if CP entry is a Q-descriptor
4023     __ andr (r1, r1, JVM_CONSTANT_QDescBit);
4024     __ cmp(r1, (u1) JVM_CONSTANT_QDescBit);
4025     __ br(Assembler::NE, done);
4026     __ b(ExternalAddress(Interpreter::_throw_NullPointerException_entry));
4027   }
4028 
4029   __ bind(done);
4030 }
4031 
4032 void TemplateTable::instanceof() {
4033   transition(atos, itos);
4034   Label done, is_null, ok_is_subtype, quicked, resolved;
4035   __ cbz(r0, is_null);
4036 
4037   // Get cpool &amp; tags index
4038   __ get_cpool_and_tags(r2, r3); // r2=cpool, r3=tags array
4039   __ get_unsigned_2_byte_index_at_bcp(r19, 1); // r19=index
</pre>
</td>
<td>
<hr />
<pre>
   1 /*
<span class="line-modified">   2  * Copyright (c) 2003, 2020, Oracle and/or its affiliates. All rights reserved.</span>
   3  * Copyright (c) 2014, Red Hat Inc. All rights reserved.
   4  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   5  *
   6  * This code is free software; you can redistribute it and/or modify it
   7  * under the terms of the GNU General Public License version 2 only, as
   8  * published by the Free Software Foundation.
   9  *
  10  * This code is distributed in the hope that it will be useful, but WITHOUT
  11  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  12  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  13  * version 2 for more details (a copy is included in the LICENSE file that
  14  * accompanied this code).
  15  *
  16  * You should have received a copy of the GNU General Public License version
  17  * 2 along with this work; if not, write to the Free Software Foundation,
  18  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  19  *
  20  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  21  * or visit www.oracle.com if you need additional information or have any
  22  * questions.
</pre>
<hr />
<pre>
1152 
1153 
1154   // Come here on success
1155   __ bind(ok_is_subtype);
1156 
1157 
1158   // Get the value we will store
1159   __ ldr(r0, at_tos());
1160   // Now store using the appropriate barrier
1161   do_oop_store(_masm, element_address, r0, IS_ARRAY);
1162   __ b(done);
1163 
1164   // Have a NULL in r0, r3=array, r2=index.  Store NULL at ary[idx]
1165   __ bind(is_null);
1166   __ profile_null_seen(r2);
1167 
1168   if (EnableValhalla) {
1169     Label is_null_into_value_array_npe, store_null;
1170 
1171     // No way to store null in flat array
<span class="line-modified">1172     __ test_null_free_array_oop(r3, r8, is_null_into_value_array_npe);</span>
1173     __ b(store_null);
1174 
1175     __ bind(is_null_into_value_array_npe);
1176     __ b(ExternalAddress(Interpreter::_throw_NullPointerException_entry));
1177 
1178     __ bind(store_null);
1179   }
1180 
1181   // Store a NULL
<span class="line-modified">1182   do_oop_store(_masm, element_address, noreg, IS_ARRAY);</span>
1183   __ b(done);
1184 
<span class="line-modified">1185   if (EnableValhalla) {</span>
1186      Label is_type_ok;
1187 
1188     // store non-null value
1189     __ bind(is_flat_array);
1190 
1191     // Simplistic type check...
1192     // r0 - value, r2 - index, r3 - array.
1193 
1194     // Profile the not-null value&#39;s klass.
<span class="line-modified">1195     // Load value class</span>
1196      __ load_klass(r1, r0);
1197      __ profile_typecheck(r2, r1, r0); // blows r2, and r0
1198 
1199     // flat value array needs exact type match
1200     // is &quot;r8 == r0&quot; (value subclass == array element superclass)
1201 
1202     // Move element klass into r0
1203 
1204      __ load_klass(r0, r3);
1205 
<span class="line-modified">1206      __ ldr(r0, Address(r0, ArrayKlass::element_klass_offset()));</span>
1207      __ cmp(r0, r1);
1208      __ br(Assembler::EQ, is_type_ok);
1209 
1210      __ profile_typecheck_failed(r2);
1211      __ b(ExternalAddress(Interpreter::_throw_ArrayStoreException_entry));
1212 
1213      __ bind(is_type_ok);
1214 
1215     // Reload from TOS to be safe, because of profile_typecheck that blows r2 and r0.
1216     // FIXME: Should we really do it?
1217      __ ldr(r1, at_tos());  // value
<span class="line-modified">1218      __ mov(r2, r3); // array, ldr(r2, at_tos_p2());</span>
1219      __ ldr(r3, at_tos_p1()); // index
1220      __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::value_array_store), r1, r2, r3);
1221   }
1222 
1223 
1224   // Pop stack arguments
1225   __ bind(done);
1226   __ add(esp, esp, 3 * Interpreter::stackElementSize);
1227 }
1228 
1229 void TemplateTable::bastore()
1230 {
1231   transition(itos, vtos);
1232   __ pop_i(r1);
1233   __ pop_ptr(r3);
1234   // r0: value
1235   // r1: index
1236   // r3: array
1237   index_check(r3, r1); // prefer index in r1
1238 
</pre>
<hr />
<pre>
2381     __ tbz(r3, exact_log2(JVM_ACC_HAS_FINALIZER), skip_register_finalizer);
2382 
2383     __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::register_finalizer), c_rarg1);
2384 
2385     __ bind(skip_register_finalizer);
2386   }
2387 
2388   // Issue a StoreStore barrier after all stores but before return
2389   // from any constructor for any class with a final field.  We don&#39;t
2390   // know if this is a finalizer, so we always do so.
2391   if (_desc-&gt;bytecode() == Bytecodes::_return)
2392     __ membar(MacroAssembler::StoreStore);
2393 
2394   // Narrow result if state is itos but result type is smaller.
2395   // Need to narrow in the return bytecode rather than in generate_return_entry
2396   // since compiled code callers expect the result to already be narrowed.
2397   if (state == itos) {
2398     __ narrow(r0);
2399   }
2400 
<span class="line-modified">2401   __ remove_activation(state);</span>
2402   __ ret(lr);
2403 }
2404 
2405 // ----------------------------------------------------------------------------
2406 // Volatile variables demand their effects be made known to all CPU&#39;s
2407 // in order.  Store buffers on most chips allow reads &amp; writes to
2408 // reorder; the JMM&#39;s ReadAfterWrite.java test fails in -Xint mode
2409 // without some kind of memory barrier (i.e., it&#39;s not sufficient that
2410 // the interpreter does not reorder volatile references, the hardware
2411 // also must not reorder them).
2412 //
2413 // According to the new Java Memory Model (JMM):
2414 // (1) All volatiles are serialized wrt to each other.  ALSO reads &amp;
2415 //     writes act as aquire &amp; release, so:
2416 // (2) A read cannot let unrelated NON-volatile memory refs that
2417 //     happen after the read float up to before the read.  It&#39;s OK for
2418 //     non-volatile memory refs that happen before the volatile read to
2419 //     float down below it.
2420 // (3) Similar a volatile write cannot let unrelated NON-volatile
2421 //     memory refs that happen BEFORE the write float down to after the
</pre>
<hr />
<pre>
2590   const Register off   = r19;
2591   const Register flags = r0;
2592   const Register raw_flags = r6;
2593   const Register bc    = r4; // uses same reg as obj, so don&#39;t mix them
2594 
2595   resolve_cache_and_index(byte_no, cache, index, sizeof(u2));
2596   jvmti_post_field_access(cache, index, is_static, false);
2597   load_field_cp_cache_entry(obj, cache, index, off, raw_flags, is_static);
2598 
2599   if (!is_static) {
2600     // obj is on the stack
2601     pop_and_check_object(obj);
2602   }
2603 
2604   // 8179954: We need to make sure that the code generated for
2605   // volatile accesses forms a sequentially-consistent set of
2606   // operations when combined with STLR and LDAR.  Without a leading
2607   // membar it&#39;s possible for a simple Dekker test to fail if loads
2608   // use LDR;DMB but stores use STLR.  This can happen if C2 compiles
2609   // the stores in one method and we interpret the loads in another.
<span class="line-modified">2610   if (!is_c1_or_interpreter_only()){</span>
2611     Label notVolatile;
2612     __ tbz(raw_flags, ConstantPoolCacheEntry::is_volatile_shift, notVolatile);
2613     __ membar(MacroAssembler::AnyAny);
2614     __ bind(notVolatile);
2615   }
2616 
2617   const Address field(obj, off);
2618 
2619   Label Done, notByte, notBool, notInt, notShort, notChar,
2620               notLong, notFloat, notObj, notDouble;
2621 
2622   // x86 uses a shift and mask or wings it with a shift plus assert
2623   // the mask is not needed. aarch64 just uses bitfield extract
2624   __ ubfxw(flags, raw_flags, ConstantPoolCacheEntry::tos_state_shift, ConstantPoolCacheEntry::tos_state_bits);
2625 
2626   assert(btos == 0, &quot;change code, btos != 0&quot;);
2627   __ cbnz(flags, notByte);
2628 
2629   // Don&#39;t rewrite getstatic, only getfield
2630   if (is_static) rc = may_not_rewrite;
</pre>
<hr />
<pre>
2644 
2645   // ztos (same code as btos)
2646   __ access_load_at(T_BOOLEAN, IN_HEAP, r0, field, noreg, noreg);
2647   __ push(ztos);
2648   // Rewrite bytecode to be faster
2649   if (rc == may_rewrite) {
2650     // use btos rewriting, no truncating to t/f bit is needed for getfield.
2651     patch_bytecode(Bytecodes::_fast_bgetfield, bc, r1);
2652   }
2653   __ b(Done);
2654 
2655   __ bind(notBool);
2656   __ cmp(flags, (u1)atos);
2657   __ br(Assembler::NE, notObj);
2658   // atos
2659   if (!EnableValhalla) {
2660     do_oop_load(_masm, field, r0, IN_HEAP);
2661     __ push(atos);
2662     if (rc == may_rewrite) {
2663       patch_bytecode(Bytecodes::_fast_agetfield, bc, r1);
<span class="line-modified">2664     }</span>
2665     __ b(Done);
2666   } else { // Valhalla
2667 
2668     if (is_static) {
2669       __ load_heap_oop(r0, field);
2670       Label isFlattenable, isUninitialized;
2671       // Issue below if the static field has not been initialized yet
2672       __ test_field_is_flattenable(raw_flags, r8 /*temp*/, isFlattenable);
2673         // Not flattenable case
2674         __ push(atos);
2675         __ b(Done);
2676       // Flattenable case, must not return null even if uninitialized
2677       __ bind(isFlattenable);
2678         __ cbz(r0, isUninitialized);
2679           __ push(atos);
2680           __ b(Done);
2681         __ bind(isUninitialized);
2682           __ andw(raw_flags, raw_flags, ConstantPoolCacheEntry::field_index_mask);
2683           __ call_VM(r0, CAST_FROM_FN_PTR(address, InterpreterRuntime::uninitialized_static_value_field), obj, raw_flags);
2684           __ verify_oop(r0);
</pre>
<hr />
<pre>
2691         __ load_heap_oop(r0, field);
2692         __ push(atos);
2693         if (rc == may_rewrite) {
2694           patch_bytecode(Bytecodes::_fast_agetfield, bc, r1);
2695         }
2696         __ b(Done);
2697       __ bind(isFlattenable);
2698         __ test_field_is_flattened(raw_flags, r8 /* temp */, isFlattened);
2699          // Non-flattened field case
2700           __ load_heap_oop(r0, field);
2701           __ cbnz(r0, isInitialized);
2702             __ andw(raw_flags, raw_flags, ConstantPoolCacheEntry::field_index_mask);
2703             __ call_VM(r0, CAST_FROM_FN_PTR(address, InterpreterRuntime::uninitialized_instance_value_field), obj, raw_flags);
2704           __ bind(isInitialized);
2705           __ verify_oop(r0);
2706           __ push(atos);
2707           __ b(rewriteFlattenable);
2708         __ bind(isFlattened);
2709           __ ldr(r10, Address(cache, in_bytes(ConstantPoolCache::base_offset() + ConstantPoolCacheEntry::f1_offset())));
2710           __ andw(raw_flags, raw_flags, ConstantPoolCacheEntry::field_index_mask);
<span class="line-modified">2711           call_VM(r0, CAST_FROM_FN_PTR(address, InterpreterRuntime::read_flattened_field), obj, raw_flags, r10);</span>
2712           __ verify_oop(r0);
2713           __ push(atos);
2714       __ bind(rewriteFlattenable);
<span class="line-modified">2715       if (rc == may_rewrite) {</span>
2716          patch_bytecode(Bytecodes::_fast_qgetfield, bc, r1);
2717       }
2718       __ b(Done);
2719     }
2720   }
2721 
2722   __ bind(notObj);
2723   __ cmp(flags, (u1)itos);
2724   __ br(Assembler::NE, notInt);
2725   // itos
2726   __ access_load_at(T_INT, IN_HEAP, r0, field, noreg, noreg);
2727   __ push(itos);
2728   // Rewrite bytecode to be faster
2729   if (rc == may_rewrite) {
2730     patch_bytecode(Bytecodes::_fast_igetfield, bc, r1);
2731   }
2732   __ b(Done);
2733 
2734   __ bind(notInt);
2735   __ cmp(flags, (u1)ctos);
</pre>
<hr />
<pre>
2893   resolve_cache_and_index(byte_no, cache, index, sizeof(u2));
2894   jvmti_post_field_mod(cache, index, is_static);
2895   load_field_cp_cache_entry(obj, cache, index, off, flags, is_static);
2896 
2897   Label Done;
2898   __ mov(r5, flags);
2899 
2900   {
2901     Label notVolatile;
2902     __ tbz(r5, ConstantPoolCacheEntry::is_volatile_shift, notVolatile);
2903     __ membar(MacroAssembler::StoreStore | MacroAssembler::LoadStore);
2904     __ bind(notVolatile);
2905   }
2906 
2907   // field address
2908   const Address field(obj, off);
2909 
2910   Label notByte, notBool, notInt, notShort, notChar,
2911         notLong, notFloat, notObj, notDouble;
2912 
<span class="line-modified">2913   __ mov(flags2, flags);</span>
2914 
2915   // x86 uses a shift and mask or wings it with a shift plus assert
2916   // the mask is not needed. aarch64 just uses bitfield extract
2917   __ ubfxw(flags, flags, ConstantPoolCacheEntry::tos_state_shift,  ConstantPoolCacheEntry::tos_state_bits);
2918 
2919   assert(btos == 0, &quot;change code, btos != 0&quot;);
2920   __ cbnz(flags, notByte);
2921 
2922   // Don&#39;t rewrite putstatic, only putfield
2923   if (is_static) rc = may_not_rewrite;
2924 
2925   // btos
2926   {
2927     __ pop(btos);
2928     if (!is_static) pop_and_check_object(obj);
2929     __ access_store_at(T_BYTE, IN_HEAP, field, r0, noreg, noreg);
2930     if (rc == may_rewrite) {
2931       patch_bytecode(Bytecodes::_fast_bputfield, bc, r1, true, byte_no);
2932     }
2933     __ b(Done);
</pre>
<hr />
<pre>
2954 
2955   // atos
2956   {
2957      if (!EnableValhalla) {
2958       __ pop(atos);
2959       if (!is_static) pop_and_check_object(obj);
2960       // Store into the field
2961       do_oop_store(_masm, field, r0, IN_HEAP);
2962       if (rc == may_rewrite) {
2963         patch_bytecode(Bytecodes::_fast_aputfield, bc, r1, true, byte_no);
2964       }
2965       __ b(Done);
2966      } else { // Valhalla
2967 
2968       __ pop(atos);
2969       if (is_static) {
2970         Label notFlattenable;
2971          __ test_field_is_not_flattenable(flags2, r8 /* temp */, notFlattenable);
2972          __ null_check(r0);
2973          __ bind(notFlattenable);
<span class="line-modified">2974          do_oop_store(_masm, field, r0, IN_HEAP);</span>
2975          __ b(Done);
2976       } else {
2977         Label isFlattenable, isFlattened, notBuffered, notBuffered2, rewriteNotFlattenable, rewriteFlattenable;
2978         __ test_field_is_flattenable(flags2, r8 /*temp*/, isFlattenable);
2979         // Not flattenable case, covers not flattenable values and objects
2980         pop_and_check_object(obj);
2981         // Store into the field
2982         do_oop_store(_masm, field, r0, IN_HEAP);
2983         __ bind(rewriteNotFlattenable);
2984         if (rc == may_rewrite) {
<span class="line-modified">2985           patch_bytecode(Bytecodes::_fast_aputfield, bc, r19, true, byte_no);</span>
2986         }
2987         __ b(Done);
2988         // Implementation of the flattenable semantic
2989         __ bind(isFlattenable);
2990         __ null_check(r0);
2991         __ test_field_is_flattened(flags2, r8 /*temp*/, isFlattened);
2992         // Not flattened case
2993         pop_and_check_object(obj);
2994         // Store into the field
2995         do_oop_store(_masm, field, r0, IN_HEAP);
2996         __ b(rewriteFlattenable);
2997         __ bind(isFlattened);
2998         pop_and_check_object(obj);
2999         call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::write_flattened_value), r0, off, obj);
3000         __ bind(rewriteFlattenable);
3001         if (rc == may_rewrite) {
3002           patch_bytecode(Bytecodes::_fast_qputfield, bc, r19, true, byte_no);
3003         }
3004         __ b(Done);
3005       }
</pre>
<hr />
<pre>
3204   // replace index with field offset from cache entry
3205   __ ldr(r1, Address(r2, in_bytes(base + ConstantPoolCacheEntry::f2_offset())));
3206 
3207   {
3208     Label notVolatile;
3209     __ tbz(r3, ConstantPoolCacheEntry::is_volatile_shift, notVolatile);
3210     __ membar(MacroAssembler::StoreStore | MacroAssembler::LoadStore);
3211     __ bind(notVolatile);
3212   }
3213 
3214   Label notVolatile;
3215 
3216   // Get object from stack
3217   pop_and_check_object(r2);
3218 
3219   // field address
3220   const Address field(r2, r1);
3221 
3222   // access field
3223   switch (bytecode()) {
<span class="line-modified">3224   case Bytecodes::_fast_qputfield: //fall through</span>
3225    {
<span class="line-modified">3226       Label isFlattened, done;</span>
3227       __ null_check(r0);
3228       __ test_field_is_flattened(r3, r8 /* temp */, isFlattened);
3229       // No Flattened case
3230       do_oop_store(_masm, field, r0, IN_HEAP);
3231       __ b(done);
3232       __ bind(isFlattened);
3233       call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::write_flattened_value), r0, r1, r2);
3234       __ bind(done);
3235     }
3236     break;
3237   case Bytecodes::_fast_aputfield:
3238     do_oop_store(_masm, field, r0, IN_HEAP);
3239     break;
3240   case Bytecodes::_fast_lputfield:
3241     __ access_store_at(T_LONG, IN_HEAP, field, r0, noreg, noreg);
3242     break;
3243   case Bytecodes::_fast_iputfield:
3244     __ access_store_at(T_INT, IN_HEAP, field, r0, noreg, noreg);
3245     break;
3246   case Bytecodes::_fast_zputfield:
</pre>
<hr />
<pre>
3301   }
3302 
3303   // access constant pool cache
3304   __ get_cache_and_index_at_bcp(r2, r1, 1);
3305   __ ldr(r1, Address(r2, in_bytes(ConstantPoolCache::base_offset() +
3306                                   ConstantPoolCacheEntry::f2_offset())));
3307   __ ldrw(r3, Address(r2, in_bytes(ConstantPoolCache::base_offset() +
3308                                    ConstantPoolCacheEntry::flags_offset())));
3309 
3310   // r0: object
3311   __ verify_oop(r0);
3312   __ null_check(r0);
3313   const Address field(r0, r1);
3314 
3315   // 8179954: We need to make sure that the code generated for
3316   // volatile accesses forms a sequentially-consistent set of
3317   // operations when combined with STLR and LDAR.  Without a leading
3318   // membar it&#39;s possible for a simple Dekker test to fail if loads
3319   // use LDR;DMB but stores use STLR.  This can happen if C2 compiles
3320   // the stores in one method and we interpret the loads in another.
<span class="line-modified">3321   if (!is_c1_or_interpreter_only()) {</span>
3322     Label notVolatile;
3323     __ tbz(r3, ConstantPoolCacheEntry::is_volatile_shift, notVolatile);
3324     __ membar(MacroAssembler::AnyAny);
3325     __ bind(notVolatile);
3326   }
3327 
3328   // access field
3329   switch (bytecode()) {
<span class="line-modified">3330   case Bytecodes::_fast_qgetfield:</span>
3331     {
3332        Label isFlattened, isInitialized, Done;
3333        // FIXME: We don&#39;t need to reload registers multiple times, but stay close to x86 code
<span class="line-modified">3334        __ ldrw(r9, Address(r2, in_bytes(ConstantPoolCache::base_offset() + ConstantPoolCacheEntry::flags_offset())));</span>
3335        __ test_field_is_flattened(r9, r8 /* temp */, isFlattened);
3336         // Non-flattened field case
3337         __ mov(r9, r0);
3338         __ load_heap_oop(r0, field);
3339         __ cbnz(r0, isInitialized);
3340           __ mov(r0, r9);
<span class="line-modified">3341           __ ldrw(r9, Address(r2, in_bytes(ConstantPoolCache::base_offset() + ConstantPoolCacheEntry::flags_offset())));</span>
3342           __ andw(r9, r9, ConstantPoolCacheEntry::field_index_mask);
3343           __ call_VM(r0, CAST_FROM_FN_PTR(address, InterpreterRuntime::uninitialized_instance_value_field), r0, r9);
3344         __ bind(isInitialized);
3345         __ verify_oop(r0);
3346         __ b(Done);
3347       __ bind(isFlattened);
3348         __ ldrw(r9, Address(r2, in_bytes(ConstantPoolCache::base_offset() + ConstantPoolCacheEntry::flags_offset())));
3349         __ andw(r9, r9, ConstantPoolCacheEntry::field_index_mask);
3350         __ ldr(r3, Address(r2, in_bytes(ConstantPoolCache::base_offset() + ConstantPoolCacheEntry::f1_offset())));
3351         call_VM(r0, CAST_FROM_FN_PTR(address, InterpreterRuntime::read_flattened_field), r0, r9, r3);
3352         __ verify_oop(r0);
3353       __ bind(Done);
3354     }
3355     break;
3356   case Bytecodes::_fast_agetfield:
3357     do_oop_load(_masm, field, r0, IN_HEAP);
3358     __ verify_oop(r0);
3359     break;
3360   case Bytecodes::_fast_lgetfield:
3361     __ access_load_at(T_LONG, IN_HEAP, r0, field, noreg, noreg);
</pre>
<hr />
<pre>
3389   }
3390 }
3391 
3392 void TemplateTable::fast_xaccess(TosState state)
3393 {
3394   transition(vtos, state);
3395 
3396   // get receiver
3397   __ ldr(r0, aaddress(0));
3398   // access constant pool cache
3399   __ get_cache_and_index_at_bcp(r2, r3, 2);
3400   __ ldr(r1, Address(r2, in_bytes(ConstantPoolCache::base_offset() +
3401                                   ConstantPoolCacheEntry::f2_offset())));
3402 
3403   // 8179954: We need to make sure that the code generated for
3404   // volatile accesses forms a sequentially-consistent set of
3405   // operations when combined with STLR and LDAR.  Without a leading
3406   // membar it&#39;s possible for a simple Dekker test to fail if loads
3407   // use LDR;DMB but stores use STLR.  This can happen if C2 compiles
3408   // the stores in one method and we interpret the loads in another.
<span class="line-modified">3409   if (!is_c1_or_interpreter_only()) {</span>
3410     Label notVolatile;
3411     __ ldrw(r3, Address(r2, in_bytes(ConstantPoolCache::base_offset() +
3412                                      ConstantPoolCacheEntry::flags_offset())));
3413     __ tbz(r3, ConstantPoolCacheEntry::is_volatile_shift, notVolatile);
3414     __ membar(MacroAssembler::AnyAny);
3415     __ bind(notVolatile);
3416   }
3417 
3418   // make sure exception is reported in correct bcp range (getfield is
3419   // next instruction)
3420   __ increment(rbcp);
3421   __ null_check(r0);
3422   switch (state) {
3423   case itos:
3424     __ access_load_at(T_INT, IN_HEAP, r0, Address(r0, r1, Address::lsl(0)), noreg, noreg);
3425     break;
3426   case atos:
3427     do_oop_load(_masm, Address(r0, r1, Address::lsl(0)), r0, IN_HEAP);
3428     __ verify_oop(r0);
3429     break;
</pre>
<hr />
<pre>
3908   __ membar(Assembler::StoreStore);
3909 }
3910 
3911 void TemplateTable::defaultvalue() {
3912   transition(vtos, atos);
3913   __ get_unsigned_2_byte_index_at_bcp(c_rarg2, 1);
3914   __ get_constant_pool(c_rarg1);
3915   call_VM(r0, CAST_FROM_FN_PTR(address, InterpreterRuntime::defaultvalue),
3916           c_rarg1, c_rarg2);
3917   __ verify_oop(r0);
3918   // Must prevent reordering of stores for object initialization with stores that publish the new object.
3919   __ membar(Assembler::StoreStore);
3920 }
3921 
3922 void TemplateTable::withfield() {
3923   transition(vtos, atos);
3924   resolve_cache_and_index(f2_byte, c_rarg1 /*cache*/, c_rarg2 /*index*/, sizeof(u2));
3925 
3926   // n.b. unlike x86 cache is now rcpool plus the indexed offset
3927   // so using rcpool to meet shared code expectations
<span class="line-modified">3928 </span>
3929   call_VM(r1, CAST_FROM_FN_PTR(address, InterpreterRuntime::withfield), rcpool);
3930   __ verify_oop(r1);
3931   __ add(esp, esp, r0);
3932   __ mov(r0, r1);
3933 }
3934 
3935 void TemplateTable::newarray() {
3936   transition(itos, atos);
3937   __ load_unsigned_byte(c_rarg1, at_bcp(1));
3938   __ mov(c_rarg2, r0);
3939   call_VM(r0, CAST_FROM_FN_PTR(address, InterpreterRuntime::newarray),
3940           c_rarg1, c_rarg2);
3941   // Must prevent reordering of stores for object initialization with stores that publish the new object.
3942   __ membar(Assembler::StoreStore);
3943 }
3944 
3945 void TemplateTable::anewarray() {
3946   transition(itos, atos);
3947   __ get_unsigned_2_byte_index_at_bcp(c_rarg2, 1);
3948   __ get_constant_pool(c_rarg1);
</pre>
<hr />
<pre>
3999   // object is at TOS
4000   __ b(Interpreter::_throw_ClassCastException_entry);
4001 
4002   // Come here on success
4003   __ bind(ok_is_subtype);
4004   __ mov(r0, r3); // Restore object in r3
4005 
4006   __ b(done);
4007   __ bind(is_null);
4008 
4009   // Collect counts on whether this test sees NULLs a lot or not.
4010   if (ProfileInterpreter) {
4011     __ profile_null_seen(r2);
4012   }
4013 
4014   if (EnableValhalla) {
4015     // Get cpool &amp; tags index
4016     __ get_cpool_and_tags(r2, r3); // r2=cpool, r3=tags array
4017     __ get_unsigned_2_byte_index_at_bcp(r19, 1); // r19=index
4018      // See if bytecode has already been quicked
<span class="line-modified">4019     __ add(rscratch1, r3, Array&lt;u1&gt;::base_offset_in_bytes());</span>
4020     __ lea(r1, Address(rscratch1, r19));
4021     __ ldarb(r1, r1);
4022     // See if CP entry is a Q-descriptor
4023     __ andr (r1, r1, JVM_CONSTANT_QDescBit);
4024     __ cmp(r1, (u1) JVM_CONSTANT_QDescBit);
4025     __ br(Assembler::NE, done);
4026     __ b(ExternalAddress(Interpreter::_throw_NullPointerException_entry));
4027   }
4028 
4029   __ bind(done);
4030 }
4031 
4032 void TemplateTable::instanceof() {
4033   transition(atos, itos);
4034   Label done, is_null, ok_is_subtype, quicked, resolved;
4035   __ cbz(r0, is_null);
4036 
4037   // Get cpool &amp; tags index
4038   __ get_cpool_and_tags(r2, r3); // r2=cpool, r3=tags array
4039   __ get_unsigned_2_byte_index_at_bcp(r19, 1); // r19=index
</pre>
</td>
</tr>
</table>
<center><a href="globals_aarch64.hpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="../ppc/globals_ppc.hpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>