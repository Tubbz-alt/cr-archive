diff a/src/hotspot/cpu/aarch64/templateTable_aarch64.cpp b/src/hotspot/cpu/aarch64/templateTable_aarch64.cpp
--- a/src/hotspot/cpu/aarch64/templateTable_aarch64.cpp
+++ b/src/hotspot/cpu/aarch64/templateTable_aarch64.cpp
@@ -1,7 +1,7 @@
 /*
- * Copyright (c) 2003, 2019, Oracle and/or its affiliates. All rights reserved.
+ * Copyright (c) 2003, 2020, Oracle and/or its affiliates. All rights reserved.
  * Copyright (c) 2014, Red Hat Inc. All rights reserved.
  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  *
  * This code is free software; you can redistribute it and/or modify it
  * under the terms of the GNU General Public License version 2 only, as
@@ -1167,45 +1167,45 @@
 
   if (EnableValhalla) {
     Label is_null_into_value_array_npe, store_null;
 
     // No way to store null in flat array
-    __ test_null_free_array_oop(r3, r8, is_null_into_value_array_npe); 
+    __ test_null_free_array_oop(r3, r8, is_null_into_value_array_npe);
     __ b(store_null);
 
     __ bind(is_null_into_value_array_npe);
     __ b(ExternalAddress(Interpreter::_throw_NullPointerException_entry));
 
     __ bind(store_null);
   }
 
   // Store a NULL
-  do_oop_store(_masm, element_address, noreg, IS_ARRAY); 
+  do_oop_store(_masm, element_address, noreg, IS_ARRAY);
   __ b(done);
 
-  if (EnableValhalla) { 
+  if (EnableValhalla) {
      Label is_type_ok;
 
     // store non-null value
     __ bind(is_flat_array);
 
     // Simplistic type check...
     // r0 - value, r2 - index, r3 - array.
 
     // Profile the not-null value's klass.
-    // Load value class 
+    // Load value class
      __ load_klass(r1, r0);
      __ profile_typecheck(r2, r1, r0); // blows r2, and r0
 
     // flat value array needs exact type match
     // is "r8 == r0" (value subclass == array element superclass)
 
     // Move element klass into r0
 
      __ load_klass(r0, r3);
 
-     __ ldr(r0, Address(r0, ArrayKlass::element_klass_offset())); 
+     __ ldr(r0, Address(r0, ArrayKlass::element_klass_offset()));
      __ cmp(r0, r1);
      __ br(Assembler::EQ, is_type_ok);
 
      __ profile_typecheck_failed(r2);
      __ b(ExternalAddress(Interpreter::_throw_ArrayStoreException_entry));
@@ -1213,11 +1213,11 @@
      __ bind(is_type_ok);
 
     // Reload from TOS to be safe, because of profile_typecheck that blows r2 and r0.
     // FIXME: Should we really do it?
      __ ldr(r1, at_tos());  // value
-     __ mov(r2, r3); // array, ldr(r2, at_tos_p2()); 
+     __ mov(r2, r3); // array, ldr(r2, at_tos_p2());
      __ ldr(r3, at_tos_p1()); // index
      __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::value_array_store), r1, r2, r3);
   }
 
 
@@ -2396,11 +2396,11 @@
   // since compiled code callers expect the result to already be narrowed.
   if (state == itos) {
     __ narrow(r0);
   }
 
-  __ remove_activation(state); 
+  __ remove_activation(state);
   __ ret(lr);
 }
 
 // ----------------------------------------------------------------------------
 // Volatile variables demand their effects be made known to all CPU's
@@ -2605,11 +2605,11 @@
   // volatile accesses forms a sequentially-consistent set of
   // operations when combined with STLR and LDAR.  Without a leading
   // membar it's possible for a simple Dekker test to fail if loads
   // use LDR;DMB but stores use STLR.  This can happen if C2 compiles
   // the stores in one method and we interpret the loads in another.
-  if (! UseBarriersForVolatile) {
+  if (!is_c1_or_interpreter_only()){
     Label notVolatile;
     __ tbz(raw_flags, ConstantPoolCacheEntry::is_volatile_shift, notVolatile);
     __ membar(MacroAssembler::AnyAny);
     __ bind(notVolatile);
   }
@@ -2659,11 +2659,11 @@
   if (!EnableValhalla) {
     do_oop_load(_masm, field, r0, IN_HEAP);
     __ push(atos);
     if (rc == may_rewrite) {
       patch_bytecode(Bytecodes::_fast_agetfield, bc, r1);
-    }  
+    }
     __ b(Done);
   } else { // Valhalla
 
     if (is_static) {
       __ load_heap_oop(r0, field);
@@ -2706,15 +2706,15 @@
           __ push(atos);
           __ b(rewriteFlattenable);
         __ bind(isFlattened);
           __ ldr(r10, Address(cache, in_bytes(ConstantPoolCache::base_offset() + ConstantPoolCacheEntry::f1_offset())));
           __ andw(raw_flags, raw_flags, ConstantPoolCacheEntry::field_index_mask);
-          call_VM(r0, CAST_FROM_FN_PTR(address, InterpreterRuntime::read_flattened_field), obj, raw_flags, r10); 
+          call_VM(r0, CAST_FROM_FN_PTR(address, InterpreterRuntime::read_flattened_field), obj, raw_flags, r10);
           __ verify_oop(r0);
           __ push(atos);
       __ bind(rewriteFlattenable);
-      if (rc == may_rewrite) { 
+      if (rc == may_rewrite) {
          patch_bytecode(Bytecodes::_fast_qgetfield, bc, r1);
       }
       __ b(Done);
     }
   }
@@ -2908,11 +2908,11 @@
   const Address field(obj, off);
 
   Label notByte, notBool, notInt, notShort, notChar,
         notLong, notFloat, notObj, notDouble;
 
-  __ mov(flags2, flags); 
+  __ mov(flags2, flags);
 
   // x86 uses a shift and mask or wings it with a shift plus assert
   // the mask is not needed. aarch64 just uses bitfield extract
   __ ubfxw(flags, flags, ConstantPoolCacheEntry::tos_state_shift,  ConstantPoolCacheEntry::tos_state_bits);
 
@@ -2969,22 +2969,22 @@
       if (is_static) {
         Label notFlattenable;
          __ test_field_is_not_flattenable(flags2, r8 /* temp */, notFlattenable);
          __ null_check(r0);
          __ bind(notFlattenable);
-         do_oop_store(_masm, field, r0, IN_HEAP); 
+         do_oop_store(_masm, field, r0, IN_HEAP);
          __ b(Done);
       } else {
         Label isFlattenable, isFlattened, notBuffered, notBuffered2, rewriteNotFlattenable, rewriteFlattenable;
         __ test_field_is_flattenable(flags2, r8 /*temp*/, isFlattenable);
         // Not flattenable case, covers not flattenable values and objects
         pop_and_check_object(obj);
         // Store into the field
         do_oop_store(_masm, field, r0, IN_HEAP);
         __ bind(rewriteNotFlattenable);
         if (rc == may_rewrite) {
-          patch_bytecode(Bytecodes::_fast_aputfield, bc, r19, true, byte_no); 
+          patch_bytecode(Bytecodes::_fast_aputfield, bc, r19, true, byte_no);
         }
         __ b(Done);
         // Implementation of the flattenable semantic
         __ bind(isFlattenable);
         __ null_check(r0);
@@ -3219,13 +3219,13 @@
   // field address
   const Address field(r2, r1);
 
   // access field
   switch (bytecode()) {
-  case Bytecodes::_fast_qputfield: //fall through 
+  case Bytecodes::_fast_qputfield: //fall through
    {
-      Label isFlattened, done; 
+      Label isFlattened, done;
       __ null_check(r0);
       __ test_field_is_flattened(r3, r8 /* temp */, isFlattened);
       // No Flattened case
       do_oop_store(_masm, field, r0, IN_HEAP);
       __ b(done);
@@ -3316,31 +3316,31 @@
   // volatile accesses forms a sequentially-consistent set of
   // operations when combined with STLR and LDAR.  Without a leading
   // membar it's possible for a simple Dekker test to fail if loads
   // use LDR;DMB but stores use STLR.  This can happen if C2 compiles
   // the stores in one method and we interpret the loads in another.
-  if (! UseBarriersForVolatile) {
+  if (!is_c1_or_interpreter_only()) {
     Label notVolatile;
     __ tbz(r3, ConstantPoolCacheEntry::is_volatile_shift, notVolatile);
     __ membar(MacroAssembler::AnyAny);
     __ bind(notVolatile);
   }
 
   // access field
   switch (bytecode()) {
-  case Bytecodes::_fast_qgetfield: 
+  case Bytecodes::_fast_qgetfield:
     {
        Label isFlattened, isInitialized, Done;
        // FIXME: We don't need to reload registers multiple times, but stay close to x86 code
-       __ ldrw(r9, Address(r2, in_bytes(ConstantPoolCache::base_offset() + ConstantPoolCacheEntry::flags_offset()))); 
+       __ ldrw(r9, Address(r2, in_bytes(ConstantPoolCache::base_offset() + ConstantPoolCacheEntry::flags_offset())));
        __ test_field_is_flattened(r9, r8 /* temp */, isFlattened);
         // Non-flattened field case
         __ mov(r9, r0);
         __ load_heap_oop(r0, field);
         __ cbnz(r0, isInitialized);
           __ mov(r0, r9);
-          __ ldrw(r9, Address(r2, in_bytes(ConstantPoolCache::base_offset() + ConstantPoolCacheEntry::flags_offset()))); 
+          __ ldrw(r9, Address(r2, in_bytes(ConstantPoolCache::base_offset() + ConstantPoolCacheEntry::flags_offset())));
           __ andw(r9, r9, ConstantPoolCacheEntry::field_index_mask);
           __ call_VM(r0, CAST_FROM_FN_PTR(address, InterpreterRuntime::uninitialized_instance_value_field), r0, r9);
         __ bind(isInitialized);
         __ verify_oop(r0);
         __ b(Done);
@@ -3404,11 +3404,11 @@
   // volatile accesses forms a sequentially-consistent set of
   // operations when combined with STLR and LDAR.  Without a leading
   // membar it's possible for a simple Dekker test to fail if loads
   // use LDR;DMB but stores use STLR.  This can happen if C2 compiles
   // the stores in one method and we interpret the loads in another.
-  if (! UseBarriersForVolatile) {
+  if (!is_c1_or_interpreter_only()) {
     Label notVolatile;
     __ ldrw(r3, Address(r2, in_bytes(ConstantPoolCache::base_offset() +
                                      ConstantPoolCacheEntry::flags_offset())));
     __ tbz(r3, ConstantPoolCacheEntry::is_volatile_shift, notVolatile);
     __ membar(MacroAssembler::AnyAny);
@@ -3923,11 +3923,11 @@
   transition(vtos, atos);
   resolve_cache_and_index(f2_byte, c_rarg1 /*cache*/, c_rarg2 /*index*/, sizeof(u2));
 
   // n.b. unlike x86 cache is now rcpool plus the indexed offset
   // so using rcpool to meet shared code expectations
- 
+
   call_VM(r1, CAST_FROM_FN_PTR(address, InterpreterRuntime::withfield), rcpool);
   __ verify_oop(r1);
   __ add(esp, esp, r0);
   __ mov(r0, r1);
 }
@@ -4014,11 +4014,11 @@
   if (EnableValhalla) {
     // Get cpool & tags index
     __ get_cpool_and_tags(r2, r3); // r2=cpool, r3=tags array
     __ get_unsigned_2_byte_index_at_bcp(r19, 1); // r19=index
      // See if bytecode has already been quicked
-    __ add(rscratch1, r3, Array<u1>::base_offset_in_bytes());
+    __ add(rscratch1, r3, Array<u1>::base_offset_in_bytes());
     __ lea(r1, Address(rscratch1, r19));
     __ ldarb(r1, r1); 
     // See if CP entry is a Q-descriptor
     __ andr (r1, r1, JVM_CONSTANT_QDescBit);
     __ cmp(r1, (u1) JVM_CONSTANT_QDescBit);
