<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Udiff src/hotspot/cpu/aarch64/templateTable_aarch64.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="globals_aarch64.hpp.udiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="../ppc/globals_ppc.hpp.udiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/cpu/aarch64/templateTable_aarch64.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<hr />
<pre>
<span class="line-new-header">@@ -1,7 +1,7 @@</span>
  /*
<span class="udiff-line-modified-removed">-  * Copyright (c) 2003, 2019, Oracle and/or its affiliates. All rights reserved.</span>
<span class="udiff-line-modified-added">+  * Copyright (c) 2003, 2020, Oracle and/or its affiliates. All rights reserved.</span>
   * Copyright (c) 2014, Red Hat Inc. All rights reserved.
   * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   *
   * This code is free software; you can redistribute it and/or modify it
   * under the terms of the GNU General Public License version 2 only, as
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -1167,45 +1167,45 @@</span>
  
    if (EnableValhalla) {
      Label is_null_into_value_array_npe, store_null;
  
      // No way to store null in flat array
<span class="udiff-line-modified-removed">-     __ test_null_free_array_oop(r3, r8, is_null_into_value_array_npe); </span>
<span class="udiff-line-modified-added">+     __ test_null_free_array_oop(r3, r8, is_null_into_value_array_npe);</span>
      __ b(store_null);
  
      __ bind(is_null_into_value_array_npe);
      __ b(ExternalAddress(Interpreter::_throw_NullPointerException_entry));
  
      __ bind(store_null);
    }
  
    // Store a NULL
<span class="udiff-line-modified-removed">-   do_oop_store(_masm, element_address, noreg, IS_ARRAY); </span>
<span class="udiff-line-modified-added">+   do_oop_store(_masm, element_address, noreg, IS_ARRAY);</span>
    __ b(done);
  
<span class="udiff-line-modified-removed">-   if (EnableValhalla) { </span>
<span class="udiff-line-modified-added">+   if (EnableValhalla) {</span>
       Label is_type_ok;
  
      // store non-null value
      __ bind(is_flat_array);
  
      // Simplistic type check...
      // r0 - value, r2 - index, r3 - array.
  
      // Profile the not-null value&#39;s klass.
<span class="udiff-line-modified-removed">-     // Load value class </span>
<span class="udiff-line-modified-added">+     // Load value class</span>
       __ load_klass(r1, r0);
       __ profile_typecheck(r2, r1, r0); // blows r2, and r0
  
      // flat value array needs exact type match
      // is &quot;r8 == r0&quot; (value subclass == array element superclass)
  
      // Move element klass into r0
  
       __ load_klass(r0, r3);
  
<span class="udiff-line-modified-removed">-      __ ldr(r0, Address(r0, ArrayKlass::element_klass_offset())); </span>
<span class="udiff-line-modified-added">+      __ ldr(r0, Address(r0, ArrayKlass::element_klass_offset()));</span>
       __ cmp(r0, r1);
       __ br(Assembler::EQ, is_type_ok);
  
       __ profile_typecheck_failed(r2);
       __ b(ExternalAddress(Interpreter::_throw_ArrayStoreException_entry));
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -1213,11 +1213,11 @@</span>
       __ bind(is_type_ok);
  
      // Reload from TOS to be safe, because of profile_typecheck that blows r2 and r0.
      // FIXME: Should we really do it?
       __ ldr(r1, at_tos());  // value
<span class="udiff-line-modified-removed">-      __ mov(r2, r3); // array, ldr(r2, at_tos_p2()); </span>
<span class="udiff-line-modified-added">+      __ mov(r2, r3); // array, ldr(r2, at_tos_p2());</span>
       __ ldr(r3, at_tos_p1()); // index
       __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::value_array_store), r1, r2, r3);
    }
  
  
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -2396,11 +2396,11 @@</span>
    // since compiled code callers expect the result to already be narrowed.
    if (state == itos) {
      __ narrow(r0);
    }
  
<span class="udiff-line-modified-removed">-   __ remove_activation(state); </span>
<span class="udiff-line-modified-added">+   __ remove_activation(state);</span>
    __ ret(lr);
  }
  
  // ----------------------------------------------------------------------------
  // Volatile variables demand their effects be made known to all CPU&#39;s
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -2605,11 +2605,11 @@</span>
    // volatile accesses forms a sequentially-consistent set of
    // operations when combined with STLR and LDAR.  Without a leading
    // membar it&#39;s possible for a simple Dekker test to fail if loads
    // use LDR;DMB but stores use STLR.  This can happen if C2 compiles
    // the stores in one method and we interpret the loads in another.
<span class="udiff-line-modified-removed">-   if (! UseBarriersForVolatile) {</span>
<span class="udiff-line-modified-added">+   if (!is_c1_or_interpreter_only()){</span>
      Label notVolatile;
      __ tbz(raw_flags, ConstantPoolCacheEntry::is_volatile_shift, notVolatile);
      __ membar(MacroAssembler::AnyAny);
      __ bind(notVolatile);
    }
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -2659,11 +2659,11 @@</span>
    if (!EnableValhalla) {
      do_oop_load(_masm, field, r0, IN_HEAP);
      __ push(atos);
      if (rc == may_rewrite) {
        patch_bytecode(Bytecodes::_fast_agetfield, bc, r1);
<span class="udiff-line-modified-removed">-     }  </span>
<span class="udiff-line-modified-added">+     }</span>
      __ b(Done);
    } else { // Valhalla
  
      if (is_static) {
        __ load_heap_oop(r0, field);
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -2706,15 +2706,15 @@</span>
            __ push(atos);
            __ b(rewriteFlattenable);
          __ bind(isFlattened);
            __ ldr(r10, Address(cache, in_bytes(ConstantPoolCache::base_offset() + ConstantPoolCacheEntry::f1_offset())));
            __ andw(raw_flags, raw_flags, ConstantPoolCacheEntry::field_index_mask);
<span class="udiff-line-modified-removed">-           call_VM(r0, CAST_FROM_FN_PTR(address, InterpreterRuntime::read_flattened_field), obj, raw_flags, r10); </span>
<span class="udiff-line-modified-added">+           call_VM(r0, CAST_FROM_FN_PTR(address, InterpreterRuntime::read_flattened_field), obj, raw_flags, r10);</span>
            __ verify_oop(r0);
            __ push(atos);
        __ bind(rewriteFlattenable);
<span class="udiff-line-modified-removed">-       if (rc == may_rewrite) { </span>
<span class="udiff-line-modified-added">+       if (rc == may_rewrite) {</span>
           patch_bytecode(Bytecodes::_fast_qgetfield, bc, r1);
        }
        __ b(Done);
      }
    }
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -2908,11 +2908,11 @@</span>
    const Address field(obj, off);
  
    Label notByte, notBool, notInt, notShort, notChar,
          notLong, notFloat, notObj, notDouble;
  
<span class="udiff-line-modified-removed">-   __ mov(flags2, flags); </span>
<span class="udiff-line-modified-added">+   __ mov(flags2, flags);</span>
  
    // x86 uses a shift and mask or wings it with a shift plus assert
    // the mask is not needed. aarch64 just uses bitfield extract
    __ ubfxw(flags, flags, ConstantPoolCacheEntry::tos_state_shift,  ConstantPoolCacheEntry::tos_state_bits);
  
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -2969,22 +2969,22 @@</span>
        if (is_static) {
          Label notFlattenable;
           __ test_field_is_not_flattenable(flags2, r8 /* temp */, notFlattenable);
           __ null_check(r0);
           __ bind(notFlattenable);
<span class="udiff-line-modified-removed">-          do_oop_store(_masm, field, r0, IN_HEAP); </span>
<span class="udiff-line-modified-added">+          do_oop_store(_masm, field, r0, IN_HEAP);</span>
           __ b(Done);
        } else {
          Label isFlattenable, isFlattened, notBuffered, notBuffered2, rewriteNotFlattenable, rewriteFlattenable;
          __ test_field_is_flattenable(flags2, r8 /*temp*/, isFlattenable);
          // Not flattenable case, covers not flattenable values and objects
          pop_and_check_object(obj);
          // Store into the field
          do_oop_store(_masm, field, r0, IN_HEAP);
          __ bind(rewriteNotFlattenable);
          if (rc == may_rewrite) {
<span class="udiff-line-modified-removed">-           patch_bytecode(Bytecodes::_fast_aputfield, bc, r19, true, byte_no); </span>
<span class="udiff-line-modified-added">+           patch_bytecode(Bytecodes::_fast_aputfield, bc, r19, true, byte_no);</span>
          }
          __ b(Done);
          // Implementation of the flattenable semantic
          __ bind(isFlattenable);
          __ null_check(r0);
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -3219,13 +3219,13 @@</span>
    // field address
    const Address field(r2, r1);
  
    // access field
    switch (bytecode()) {
<span class="udiff-line-modified-removed">-   case Bytecodes::_fast_qputfield: //fall through </span>
<span class="udiff-line-modified-added">+   case Bytecodes::_fast_qputfield: //fall through</span>
     {
<span class="udiff-line-modified-removed">-       Label isFlattened, done; </span>
<span class="udiff-line-modified-added">+       Label isFlattened, done;</span>
        __ null_check(r0);
        __ test_field_is_flattened(r3, r8 /* temp */, isFlattened);
        // No Flattened case
        do_oop_store(_masm, field, r0, IN_HEAP);
        __ b(done);
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -3316,31 +3316,31 @@</span>
    // volatile accesses forms a sequentially-consistent set of
    // operations when combined with STLR and LDAR.  Without a leading
    // membar it&#39;s possible for a simple Dekker test to fail if loads
    // use LDR;DMB but stores use STLR.  This can happen if C2 compiles
    // the stores in one method and we interpret the loads in another.
<span class="udiff-line-modified-removed">-   if (! UseBarriersForVolatile) {</span>
<span class="udiff-line-modified-added">+   if (!is_c1_or_interpreter_only()) {</span>
      Label notVolatile;
      __ tbz(r3, ConstantPoolCacheEntry::is_volatile_shift, notVolatile);
      __ membar(MacroAssembler::AnyAny);
      __ bind(notVolatile);
    }
  
    // access field
    switch (bytecode()) {
<span class="udiff-line-modified-removed">-   case Bytecodes::_fast_qgetfield: </span>
<span class="udiff-line-modified-added">+   case Bytecodes::_fast_qgetfield:</span>
      {
         Label isFlattened, isInitialized, Done;
         // FIXME: We don&#39;t need to reload registers multiple times, but stay close to x86 code
<span class="udiff-line-modified-removed">-        __ ldrw(r9, Address(r2, in_bytes(ConstantPoolCache::base_offset() + ConstantPoolCacheEntry::flags_offset()))); </span>
<span class="udiff-line-modified-added">+        __ ldrw(r9, Address(r2, in_bytes(ConstantPoolCache::base_offset() + ConstantPoolCacheEntry::flags_offset())));</span>
         __ test_field_is_flattened(r9, r8 /* temp */, isFlattened);
          // Non-flattened field case
          __ mov(r9, r0);
          __ load_heap_oop(r0, field);
          __ cbnz(r0, isInitialized);
            __ mov(r0, r9);
<span class="udiff-line-modified-removed">-           __ ldrw(r9, Address(r2, in_bytes(ConstantPoolCache::base_offset() + ConstantPoolCacheEntry::flags_offset()))); </span>
<span class="udiff-line-modified-added">+           __ ldrw(r9, Address(r2, in_bytes(ConstantPoolCache::base_offset() + ConstantPoolCacheEntry::flags_offset())));</span>
            __ andw(r9, r9, ConstantPoolCacheEntry::field_index_mask);
            __ call_VM(r0, CAST_FROM_FN_PTR(address, InterpreterRuntime::uninitialized_instance_value_field), r0, r9);
          __ bind(isInitialized);
          __ verify_oop(r0);
          __ b(Done);
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -3404,11 +3404,11 @@</span>
    // volatile accesses forms a sequentially-consistent set of
    // operations when combined with STLR and LDAR.  Without a leading
    // membar it&#39;s possible for a simple Dekker test to fail if loads
    // use LDR;DMB but stores use STLR.  This can happen if C2 compiles
    // the stores in one method and we interpret the loads in another.
<span class="udiff-line-modified-removed">-   if (! UseBarriersForVolatile) {</span>
<span class="udiff-line-modified-added">+   if (!is_c1_or_interpreter_only()) {</span>
      Label notVolatile;
      __ ldrw(r3, Address(r2, in_bytes(ConstantPoolCache::base_offset() +
                                       ConstantPoolCacheEntry::flags_offset())));
      __ tbz(r3, ConstantPoolCacheEntry::is_volatile_shift, notVolatile);
      __ membar(MacroAssembler::AnyAny);
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -3923,11 +3923,11 @@</span>
    transition(vtos, atos);
    resolve_cache_and_index(f2_byte, c_rarg1 /*cache*/, c_rarg2 /*index*/, sizeof(u2));
  
    // n.b. unlike x86 cache is now rcpool plus the indexed offset
    // so using rcpool to meet shared code expectations
<span class="udiff-line-modified-removed">-  </span>
<span class="udiff-line-modified-added">+ </span>
    call_VM(r1, CAST_FROM_FN_PTR(address, InterpreterRuntime::withfield), rcpool);
    __ verify_oop(r1);
    __ add(esp, esp, r0);
    __ mov(r0, r1);
  }
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -4014,11 +4014,11 @@</span>
    if (EnableValhalla) {
      // Get cpool &amp; tags index
      __ get_cpool_and_tags(r2, r3); // r2=cpool, r3=tags array
      __ get_unsigned_2_byte_index_at_bcp(r19, 1); // r19=index
       // See if bytecode has already been quicked
<span class="udiff-line-modified-removed">-     __ add(rscratch1, r3, Array&lt;u1&gt;::base_offset_in_bytes());</span>
<span class="udiff-line-modified-added">+     __ add(rscratch1, r3, Array&lt;u1&gt;::base_offset_in_bytes());</span>
      __ lea(r1, Address(rscratch1, r19));
      __ ldarb(r1, r1);
      // See if CP entry is a Q-descriptor
      __ andr (r1, r1, JVM_CONSTANT_QDescBit);
      __ cmp(r1, (u1) JVM_CONSTANT_QDescBit);
</pre>
<center><a href="globals_aarch64.hpp.udiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="../ppc/globals_ppc.hpp.udiff.html" target="_top">next &gt;</a></center>  </body>
</html>