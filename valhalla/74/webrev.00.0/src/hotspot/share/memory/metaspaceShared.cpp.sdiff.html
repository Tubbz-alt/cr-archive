<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/share/memory/metaspaceShared.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="metaspaceClosure.hpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="universe.cpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/share/memory/metaspaceShared.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
  54 #include &quot;oops/instanceClassLoaderKlass.hpp&quot;
  55 #include &quot;oops/instanceMirrorKlass.hpp&quot;
  56 #include &quot;oops/instanceRefKlass.hpp&quot;
  57 #include &quot;oops/methodData.hpp&quot;
  58 #include &quot;oops/objArrayKlass.hpp&quot;
  59 #include &quot;oops/objArrayOop.hpp&quot;
  60 #include &quot;oops/oop.inline.hpp&quot;
  61 #include &quot;oops/typeArrayKlass.hpp&quot;
  62 #include &quot;oops/valueArrayKlass.hpp&quot;
  63 #include &quot;oops/valueKlass.hpp&quot;
  64 #include &quot;prims/jvmtiRedefineClasses.hpp&quot;
  65 #include &quot;runtime/handles.inline.hpp&quot;
  66 #include &quot;runtime/os.hpp&quot;
  67 #include &quot;runtime/safepointVerifiers.hpp&quot;
  68 #include &quot;runtime/signature.hpp&quot;
  69 #include &quot;runtime/timerTrace.hpp&quot;
  70 #include &quot;runtime/vmThread.hpp&quot;
  71 #include &quot;runtime/vmOperations.hpp&quot;
  72 #include &quot;utilities/align.hpp&quot;
  73 #include &quot;utilities/bitMap.inline.hpp&quot;

  74 #include &quot;utilities/defaultStream.hpp&quot;
  75 #include &quot;utilities/hashtable.inline.hpp&quot;
  76 #if INCLUDE_G1GC
  77 #include &quot;gc/g1/g1CollectedHeap.hpp&quot;
  78 #endif
  79 
  80 ReservedSpace MetaspaceShared::_shared_rs;
  81 VirtualSpace MetaspaceShared::_shared_vs;
  82 ReservedSpace MetaspaceShared::_symbol_rs;
  83 VirtualSpace MetaspaceShared::_symbol_vs;
  84 MetaspaceSharedStats MetaspaceShared::_stats;
  85 bool MetaspaceShared::_has_error_classes;
  86 bool MetaspaceShared::_archive_loading_failed = false;
  87 bool MetaspaceShared::_remapped_readwrite = false;
  88 address MetaspaceShared::_i2i_entry_code_buffers = NULL;
  89 size_t MetaspaceShared::_i2i_entry_code_buffers_size = 0;
  90 void* MetaspaceShared::_shared_metaspace_static_top = NULL;
  91 intx MetaspaceShared::_relocation_delta;
  92 
  93 // The CDS archive is divided into the following regions:
</pre>
<hr />
<pre>
 176                  _name, p2i(_base), p2i(_top), int(_end - _base), int(_top - _base));
 177   if (strcmp(_name, failing_region) == 0) {
 178     log_error(cds)(&quot; required = %d&quot;, int(needed_bytes));
 179   }
 180 }
 181 
 182 void DumpRegion::init(ReservedSpace* rs, VirtualSpace* vs) {
 183   _rs = rs;
 184   _vs = vs;
 185   // Start with 0 committed bytes. The memory will be committed as needed by
 186   // MetaspaceShared::commit_to().
 187   if (!_vs-&gt;initialize(*_rs, 0)) {
 188     fatal(&quot;Unable to allocate memory for shared space&quot;);
 189   }
 190   _base = _top = _rs-&gt;base();
 191   _end = _rs-&gt;end();
 192 }
 193 
 194 void DumpRegion::pack(DumpRegion* next) {
 195   assert(!is_packed(), &quot;sanity&quot;);
<span class="line-modified"> 196   _end = (char*)align_up(_top, Metaspace::reserve_alignment());</span>
 197   _is_packed = true;
 198   if (next != NULL) {
 199     next-&gt;_rs = _rs;
 200     next-&gt;_vs = _vs;
 201     next-&gt;_base = next-&gt;_top = this-&gt;_end;
 202     next-&gt;_end = _rs-&gt;end();
 203   }
 204 }
 205 
 206 static DumpRegion _mc_region(&quot;mc&quot;), _ro_region(&quot;ro&quot;), _rw_region(&quot;rw&quot;), _symbol_region(&quot;symbols&quot;);
 207 static size_t _total_closed_archive_region_size = 0, _total_open_archive_region_size = 0;
 208 
 209 void MetaspaceShared::init_shared_dump_space(DumpRegion* first_space) {
 210   first_space-&gt;init(&amp;_shared_rs, &amp;_shared_vs);
 211 }
 212 
 213 DumpRegion* MetaspaceShared::misc_code_dump_space() {
 214   return &amp;_mc_region;
 215 }
 216 
</pre>
<hr />
<pre>
 222   return &amp;_ro_region;
 223 }
 224 
 225 void MetaspaceShared::pack_dump_space(DumpRegion* current, DumpRegion* next,
 226                                       ReservedSpace* rs) {
 227   current-&gt;pack(next);
 228 }
 229 
 230 char* MetaspaceShared::symbol_space_alloc(size_t num_bytes) {
 231   return _symbol_region.allocate(num_bytes);
 232 }
 233 
 234 char* MetaspaceShared::misc_code_space_alloc(size_t num_bytes) {
 235   return _mc_region.allocate(num_bytes);
 236 }
 237 
 238 char* MetaspaceShared::read_only_space_alloc(size_t num_bytes) {
 239   return _ro_region.allocate(num_bytes);
 240 }
 241 
<span class="line-modified"> 242 // When reserving an address range using ReservedSpace, we need an alignment that satisfies both:</span>
<span class="line-removed"> 243 // os::vm_allocation_granularity() -- so that we can sub-divide this range into multiple mmap regions,</span>
<span class="line-removed"> 244 //                                    while keeping the first range at offset 0 of this range.</span>
<span class="line-removed"> 245 // Metaspace::reserve_alignment()  -- so we can pass the region to</span>
<span class="line-removed"> 246 //                                    Metaspace::allocate_metaspace_compressed_klass_ptrs.</span>
<span class="line-removed"> 247 size_t MetaspaceShared::reserved_space_alignment() {</span>
<span class="line-removed"> 248   size_t os_align = os::vm_allocation_granularity();</span>
<span class="line-removed"> 249   size_t ms_align = Metaspace::reserve_alignment();</span>
<span class="line-removed"> 250   if (os_align &gt;= ms_align) {</span>
<span class="line-removed"> 251     assert(os_align % ms_align == 0, &quot;must be a multiple&quot;);</span>
<span class="line-removed"> 252     return os_align;</span>
<span class="line-removed"> 253   } else {</span>
<span class="line-removed"> 254     assert(ms_align % os_align == 0, &quot;must be a multiple&quot;);</span>
<span class="line-removed"> 255     return ms_align;</span>
<span class="line-removed"> 256   }</span>
<span class="line-removed"> 257 }</span>
 258 
<span class="line-modified"> 259 ReservedSpace MetaspaceShared::reserve_shared_space(size_t size, char* requested_address) {</span>
<span class="line-modified"> 260   return Metaspace::reserve_space(size, reserved_space_alignment(),</span>
<span class="line-modified"> 261                                   requested_address, requested_address != NULL);</span>









 262 }

 263 
 264 void MetaspaceShared::initialize_dumptime_shared_and_meta_spaces() {
 265   assert(DumpSharedSpaces, &quot;should be called for dump time only&quot;);
<span class="line-modified"> 266   const size_t reserve_alignment = reserved_space_alignment();</span>





 267   char* shared_base = (char*)align_up((char*)SharedBaseAddress, reserve_alignment);
 268 
 269 #ifdef _LP64
<span class="line-modified"> 270   // On 64-bit VM, the heap and class space layout will be the same as if</span>
<span class="line-modified"> 271   // you&#39;re running in -Xshare:on mode:</span>
<span class="line-modified"> 272   //</span>
<span class="line-modified"> 273   //                              +-- SharedBaseAddress (default = 0x800000000)</span>
<span class="line-removed"> 274   //                              v</span>
<span class="line-removed"> 275   // +-..---------+---------+ ... +----+----+----+--------------------+</span>
<span class="line-removed"> 276   // |    Heap    | Archive |     | MC | RW | RO |    class space     |</span>
<span class="line-removed"> 277   // +-..---------+---------+ ... +----+----+----+--------------------+</span>
<span class="line-removed"> 278   // |&lt;--   MaxHeapSize  --&gt;|     |&lt;-- UnscaledClassSpaceMax = 4GB --&gt;|</span>
<span class="line-removed"> 279   //</span>
 280   const uint64_t UnscaledClassSpaceMax = (uint64_t(max_juint) + 1);
 281   const size_t cds_total = align_down(UnscaledClassSpaceMax, reserve_alignment);
 282 #else
<span class="line-modified"> 283   // We don&#39;t support archives larger than 256MB on 32-bit due to limited virtual address space.</span>

 284   size_t cds_total = align_down(256*M, reserve_alignment);
 285 #endif
 286 

 287   bool use_requested_base = true;





 288   if (ArchiveRelocationMode == 1) {
 289     log_info(cds)(&quot;ArchiveRelocationMode == 1: always allocate class space at an alternative address&quot;);
 290     use_requested_base = false;
 291   }
 292 
 293   // First try to reserve the space at the specified SharedBaseAddress.
 294   assert(!_shared_rs.is_reserved(), &quot;must be&quot;);
 295   if (use_requested_base) {
<span class="line-modified"> 296     _shared_rs = reserve_shared_space(cds_total, shared_base);</span>







 297   }
<span class="line-modified"> 298   if (_shared_rs.is_reserved()) {</span>
<span class="line-modified"> 299     assert(shared_base == 0 || _shared_rs.base() == shared_base, &quot;should match&quot;);</span>
<span class="line-modified"> 300   } else {</span>
<span class="line-modified"> 301     // Get a mmap region anywhere if the SharedBaseAddress fails.</span>
<span class="line-modified"> 302     _shared_rs = reserve_shared_space(cds_total);</span>









 303   }

 304   if (!_shared_rs.is_reserved()) {
 305     vm_exit_during_initialization(&quot;Unable to reserve memory for shared space&quot;,
 306                                   err_msg(SIZE_FORMAT &quot; bytes.&quot;, cds_total));
 307   }
 308 
 309 #ifdef _LP64
<span class="line-removed"> 310   // During dump time, we allocate 4GB (UnscaledClassSpaceMax) of space and split it up:</span>
<span class="line-removed"> 311   // + The upper 1 GB is used as the &quot;temporary compressed class space&quot; -- preload_classes()</span>
<span class="line-removed"> 312   //   will store Klasses into this space.</span>
<span class="line-removed"> 313   // + The lower 3 GB is used for the archive -- when preload_classes() is done,</span>
<span class="line-removed"> 314   //   ArchiveCompactor will copy the class metadata into this space, first the RW parts,</span>
<span class="line-removed"> 315   //   then the RO parts.</span>
<span class="line-removed"> 316 </span>
<span class="line-removed"> 317   size_t max_archive_size = align_down(cds_total * 3 / 4, reserve_alignment);</span>
<span class="line-removed"> 318   ReservedSpace tmp_class_space = _shared_rs.last_part(max_archive_size);</span>
<span class="line-removed"> 319   CompressedClassSpaceSize = align_down(tmp_class_space.size(), reserve_alignment);</span>
<span class="line-removed"> 320   _shared_rs = _shared_rs.first_part(max_archive_size);</span>
 321 
 322   if (UseCompressedClassPointers) {
<span class="line-modified"> 323     // Set up compress class pointers.</span>
<span class="line-modified"> 324     CompressedKlassPointers::set_base((address)_shared_rs.base());</span>
<span class="line-modified"> 325     // Set narrow_klass_shift to be LogKlassAlignmentInBytes. This is consistent</span>
<span class="line-modified"> 326     // with AOT.</span>
<span class="line-modified"> 327     CompressedKlassPointers::set_shift(LogKlassAlignmentInBytes);</span>
<span class="line-modified"> 328     // Set the range of klass addresses to 4GB.</span>
<span class="line-modified"> 329     CompressedKlassPointers::set_range(cds_total);</span>









































 330     Metaspace::initialize_class_space(tmp_class_space);














 331   }
<span class="line-removed"> 332   log_info(cds)(&quot;narrow_klass_base = &quot; PTR_FORMAT &quot;, narrow_klass_shift = %d&quot;,</span>
<span class="line-removed"> 333                 p2i(CompressedKlassPointers::base()), CompressedKlassPointers::shift());</span>
<span class="line-removed"> 334 </span>
<span class="line-removed"> 335   log_info(cds)(&quot;Allocated temporary class space: &quot; SIZE_FORMAT &quot; bytes at &quot; PTR_FORMAT,</span>
 336                 CompressedClassSpaceSize, p2i(tmp_class_space.base()));
 337 #endif
 338 
 339   init_shared_dump_space(&amp;_mc_region);
 340   SharedBaseAddress = (size_t)_shared_rs.base();
 341   log_info(cds)(&quot;Allocated shared space: &quot; SIZE_FORMAT &quot; bytes at &quot; PTR_FORMAT,
 342                 _shared_rs.size(), p2i(_shared_rs.base()));
 343 
 344   size_t symbol_rs_size = LP64_ONLY(3 * G) NOT_LP64(128 * M);
 345   _symbol_rs = ReservedSpace(symbol_rs_size);
 346   if (!_symbol_rs.is_reserved()) {
 347     vm_exit_during_initialization(&quot;Unable to reserve memory for symbols&quot;,
 348                                   err_msg(SIZE_FORMAT &quot; bytes.&quot;, symbol_rs_size));
 349   }
 350   _symbol_region.init(&amp;_symbol_rs, &amp;_symbol_vs);
 351 }
 352 
 353 // Called by universe_post_init()
 354 void MetaspaceShared::post_initialize(TRAPS) {
 355   if (UseSharedSpaces) {
</pre>
<hr />
<pre>
1149                                const char *name, size_t total_size);
1150   void relocate_to_default_base_address(CHeapBitMap* ptrmap);
1151 
1152 public:
1153 
1154   VMOp_Type type() const { return VMOp_PopulateDumpSharedSpace; }
1155   void doit();   // outline because gdb sucks
1156   bool allow_nested_vm_operations() const { return true; }
1157 }; // class VM_PopulateDumpSharedSpace
1158 
1159 class SortedSymbolClosure: public SymbolClosure {
1160   GrowableArray&lt;Symbol*&gt; _symbols;
1161   virtual void do_symbol(Symbol** sym) {
1162     assert((*sym)-&gt;is_permanent(), &quot;archived symbols must be permanent&quot;);
1163     _symbols.append(*sym);
1164   }
1165   static int compare_symbols_by_address(Symbol** a, Symbol** b) {
1166     if (a[0] &lt; b[0]) {
1167       return -1;
1168     } else if (a[0] == b[0]) {


1169       return 0;
1170     } else {
1171       return 1;
1172     }
1173   }
1174 
1175 public:
1176   SortedSymbolClosure() {
1177     SymbolTable::symbols_do(this);
1178     _symbols.sort(compare_symbols_by_address);
1179   }
1180   GrowableArray&lt;Symbol*&gt;* get_sorted_symbols() {
1181     return &amp;_symbols;
1182   }
1183 };
1184 
1185 // ArchiveCompactor --
1186 //
1187 // This class is the central piece of shared archive compaction -- all metaspace data are
1188 // initially allocated outside of the shared regions. ArchiveCompactor copies the
</pre>
<hr />
<pre>
2076 
2077 bool MetaspaceShared::is_in_trampoline_frame(address addr) {
2078   if (UseSharedSpaces &amp;&amp; is_in_shared_region(addr, MetaspaceShared::mc)) {
2079     return true;
2080   }
2081   return false;
2082 }
2083 
2084 bool MetaspaceShared::is_shared_dynamic(void* p) {
2085   if ((p &lt; MetaspaceObj::shared_metaspace_top()) &amp;&amp;
2086       (p &gt;= _shared_metaspace_static_top)) {
2087     return true;
2088   } else {
2089     return false;
2090   }
2091 }
2092 
2093 void MetaspaceShared::initialize_runtime_shared_and_meta_spaces() {
2094   assert(UseSharedSpaces, &quot;Must be called when UseSharedSpaces is enabled&quot;);
2095   MapArchiveResult result = MAP_ARCHIVE_OTHER_FAILURE;

2096   FileMapInfo* static_mapinfo = open_static_archive();
2097   FileMapInfo* dynamic_mapinfo = NULL;
2098 
2099   if (static_mapinfo != NULL) {
2100     dynamic_mapinfo = open_dynamic_archive();
2101 
2102     // First try to map at the requested address
2103     result = map_archives(static_mapinfo, dynamic_mapinfo, true);
2104     if (result == MAP_ARCHIVE_MMAP_FAILURE) {
2105       // Mapping has failed (probably due to ASLR). Let&#39;s map at an address chosen
2106       // by the OS.
2107       log_info(cds)(&quot;Try to map archive(s) at an alternative address&quot;);
2108       result = map_archives(static_mapinfo, dynamic_mapinfo, false);
2109     }
2110   }
2111 
2112   if (result == MAP_ARCHIVE_SUCCESS) {
2113     bool dynamic_mapped = (dynamic_mapinfo != NULL &amp;&amp; dynamic_mapinfo-&gt;is_mapped());
2114     char* cds_base = static_mapinfo-&gt;mapped_base();
2115     char* cds_end =  dynamic_mapped ? dynamic_mapinfo-&gt;mapped_end() : static_mapinfo-&gt;mapped_end();
</pre>
<hr />
<pre>
2152   }
2153   if (Arguments::GetSharedDynamicArchivePath() == NULL) {
2154     return NULL;
2155   }
2156 
2157   FileMapInfo* mapinfo = new FileMapInfo(false);
2158   if (!mapinfo-&gt;initialize()) {
2159     delete(mapinfo);
2160     return NULL;
2161   }
2162   return mapinfo;
2163 }
2164 
2165 // use_requested_addr:
2166 //  true  = map at FileMapHeader::_requested_base_address
2167 //  false = map at an alternative address picked by OS.
2168 MapArchiveResult MetaspaceShared::map_archives(FileMapInfo* static_mapinfo, FileMapInfo* dynamic_mapinfo,
2169                                                bool use_requested_addr) {
2170   PRODUCT_ONLY(if (ArchiveRelocationMode == 1 &amp;&amp; use_requested_addr) {
2171       // For product build only -- this is for benchmarking the cost of doing relocation.
<span class="line-modified">2172       // For debug builds, the check is done in FileMapInfo::map_regions for better test coverage.</span>

2173       log_info(cds)(&quot;ArchiveRelocationMode == 1: always map archive(s) at an alternative address&quot;);
2174       return MAP_ARCHIVE_MMAP_FAILURE;
2175     });
2176 
2177   if (ArchiveRelocationMode == 2 &amp;&amp; !use_requested_addr) {
2178     log_info(cds)(&quot;ArchiveRelocationMode == 2: never map archive(s) at an alternative address&quot;);
2179     return MAP_ARCHIVE_MMAP_FAILURE;
2180   };
2181 
2182   if (dynamic_mapinfo != NULL) {
2183     // Ensure that the OS won&#39;t be able to allocate new memory spaces between the two
2184     // archives, or else it would mess up the simple comparision in MetaspaceObj::is_shared().
2185     assert(static_mapinfo-&gt;mapping_end_offset() == dynamic_mapinfo-&gt;mapping_base_offset(), &quot;no gap&quot;);
2186   }
2187 
<span class="line-modified">2188   ReservedSpace main_rs, archive_space_rs, class_space_rs;</span>
2189   MapArchiveResult result = MAP_ARCHIVE_OTHER_FAILURE;
2190   char* mapped_base_address = reserve_address_space_for_archives(static_mapinfo, dynamic_mapinfo,
<span class="line-modified">2191                                                                  use_requested_addr, main_rs, archive_space_rs,</span>
2192                                                                  class_space_rs);
2193   if (mapped_base_address == NULL) {
2194     result = MAP_ARCHIVE_MMAP_FAILURE;
2195   } else {





















2196     log_debug(cds)(&quot;Reserved archive_space_rs     [&quot; INTPTR_FORMAT &quot; - &quot; INTPTR_FORMAT &quot;] (&quot; SIZE_FORMAT &quot;) bytes&quot;,
2197                    p2i(archive_space_rs.base()), p2i(archive_space_rs.end()), archive_space_rs.size());
2198     log_debug(cds)(&quot;Reserved class_space_rs [&quot; INTPTR_FORMAT &quot; - &quot; INTPTR_FORMAT &quot;] (&quot; SIZE_FORMAT &quot;) bytes&quot;,
2199                    p2i(class_space_rs.base()), p2i(class_space_rs.end()), class_space_rs.size());























2200     MapArchiveResult static_result = map_archive(static_mapinfo, mapped_base_address, archive_space_rs);
2201     MapArchiveResult dynamic_result = (static_result == MAP_ARCHIVE_SUCCESS) ?
2202                                      map_archive(dynamic_mapinfo, mapped_base_address, archive_space_rs) : MAP_ARCHIVE_OTHER_FAILURE;
2203 
2204     DEBUG_ONLY(if (ArchiveRelocationMode == 1 &amp;&amp; use_requested_addr) {
<span class="line-modified">2205       // This is for simulating mmap failures at the requested address. In debug builds, we do it</span>
<span class="line-modified">2206       // here (after all archives have possibly been mapped), so we can thoroughly test the code for</span>
<span class="line-modified">2207       // failure handling (releasing all allocated resource, etc).</span>

2208       log_info(cds)(&quot;ArchiveRelocationMode == 1: always map archive(s) at an alternative address&quot;);
2209       if (static_result == MAP_ARCHIVE_SUCCESS) {
2210         static_result = MAP_ARCHIVE_MMAP_FAILURE;
2211       }
2212       if (dynamic_result == MAP_ARCHIVE_SUCCESS) {
2213         dynamic_result = MAP_ARCHIVE_MMAP_FAILURE;
2214       }
2215     });
2216 
2217     if (static_result == MAP_ARCHIVE_SUCCESS) {
2218       if (dynamic_result == MAP_ARCHIVE_SUCCESS) {
2219         result = MAP_ARCHIVE_SUCCESS;
2220       } else if (dynamic_result == MAP_ARCHIVE_OTHER_FAILURE) {
2221         assert(dynamic_mapinfo != NULL &amp;&amp; !dynamic_mapinfo-&gt;is_mapped(), &quot;must have failed&quot;);
2222         // No need to retry mapping the dynamic archive again, as it will never succeed
2223         // (bad file, etc) -- just keep the base archive.
2224         log_warning(cds, dynamic)(&quot;Unable to use shared archive. The top archive failed to load: %s&quot;,
2225                                   dynamic_mapinfo-&gt;full_path());
2226         result = MAP_ARCHIVE_SUCCESS;
2227         // TODO, we can give the unused space for the dynamic archive to class_space_rs, but there&#39;s no
2228         // easy API to do that right now.
2229       } else {
2230         result = MAP_ARCHIVE_MMAP_FAILURE;
2231       }
2232     } else if (static_result == MAP_ARCHIVE_OTHER_FAILURE) {
2233       result = MAP_ARCHIVE_OTHER_FAILURE;
2234     } else {
2235       result = MAP_ARCHIVE_MMAP_FAILURE;
2236     }
2237   }
2238 
2239   if (result == MAP_ARCHIVE_SUCCESS) {
<span class="line-removed">2240     if (!main_rs.is_reserved() &amp;&amp; class_space_rs.is_reserved()) {</span>
<span class="line-removed">2241       MemTracker::record_virtual_memory_type((address)class_space_rs.base(), mtClass);</span>
<span class="line-removed">2242     }</span>
2243     SharedBaseAddress = (size_t)mapped_base_address;
2244     LP64_ONLY({
2245         if (Metaspace::using_class_space()) {
<span class="line-modified">2246           assert(class_space_rs.is_reserved(), &quot;must be&quot;);</span>
<span class="line-modified">2247           char* cds_base = static_mapinfo-&gt;mapped_base();</span>
<span class="line-modified">2248           Metaspace::allocate_metaspace_compressed_klass_ptrs(class_space_rs, NULL, (address)cds_base);</span>






2249           // map_heap_regions() compares the current narrow oop and klass encodings
2250           // with the archived ones, so it must be done after all encodings are determined.
2251           static_mapinfo-&gt;map_heap_regions();
<span class="line-removed">2252           CompressedKlassPointers::set_range(CompressedClassSpaceSize);</span>
2253         }
2254       });
2255   } else {
2256     unmap_archive(static_mapinfo);
2257     unmap_archive(dynamic_mapinfo);
<span class="line-modified">2258     release_reserved_spaces(main_rs, archive_space_rs, class_space_rs);</span>
2259   }
2260 
2261   return result;
2262 }
2263 






















































2264 char* MetaspaceShared::reserve_address_space_for_archives(FileMapInfo* static_mapinfo,
2265                                                           FileMapInfo* dynamic_mapinfo,
<span class="line-modified">2266                                                           bool use_requested_addr,</span>
<span class="line-removed">2267                                                           ReservedSpace&amp; main_rs,</span>
2268                                                           ReservedSpace&amp; archive_space_rs,
2269                                                           ReservedSpace&amp; class_space_rs) {
<span class="line-modified">2270   const bool use_klass_space = NOT_LP64(false) LP64_ONLY(Metaspace::using_class_space());</span>
<span class="line-modified">2271   const size_t class_space_size = NOT_LP64(0) LP64_ONLY(Metaspace::compressed_class_space_size());</span>
<span class="line-removed">2272 </span>
<span class="line-removed">2273   if (use_klass_space) {</span>
<span class="line-removed">2274     assert(class_space_size &gt; 0, &quot;CompressedClassSpaceSize must have been validated&quot;);</span>
<span class="line-removed">2275   }</span>
<span class="line-removed">2276   if (use_requested_addr &amp;&amp; !is_aligned(static_mapinfo-&gt;requested_base_address(), reserved_space_alignment())) {</span>
<span class="line-removed">2277     return NULL;</span>
2278   }
2279 
2280   // Size and requested location of the archive_space_rs (for both static and dynamic archives)
<span class="line-modified">2281   size_t base_offset = static_mapinfo-&gt;mapping_base_offset();</span>
<span class="line-modified">2282   size_t end_offset  = (dynamic_mapinfo == NULL) ? static_mapinfo-&gt;mapping_end_offset() : dynamic_mapinfo-&gt;mapping_end_offset();</span>
<span class="line-modified">2283   assert(base_offset == 0, &quot;must be&quot;);</span>
<span class="line-modified">2284   assert(is_aligned(end_offset,  os::vm_allocation_granularity()), &quot;must be&quot;);</span>
<span class="line-modified">2285   assert(is_aligned(base_offset, os::vm_allocation_granularity()), &quot;must be&quot;);</span>
<span class="line-modified">2286 </span>
<span class="line-modified">2287   // In case reserved_space_alignment() != os::vm_allocation_granularity()</span>
<span class="line-modified">2288   assert((size_t)os::vm_allocation_granularity() &lt;= reserved_space_alignment(), &quot;must be&quot;);</span>
<span class="line-modified">2289   end_offset = align_up(end_offset, reserved_space_alignment());</span>
<span class="line-modified">2290 </span>
<span class="line-modified">2291   size_t archive_space_size = end_offset - base_offset;</span>
<span class="line-removed">2292 </span>
<span class="line-removed">2293   // Special handling for Windows because it cannot mmap into a reserved space:</span>
<span class="line-removed">2294   //    use_requested_addr: We just map each region individually, and give up if any one of them fails.</span>
<span class="line-removed">2295   //   !use_requested_addr: We reserve the space first, and then os::read in all the regions (instead of mmap).</span>
<span class="line-removed">2296   //                        We&#39;re going to patch all the pointers anyway so there&#39;s no benefit for mmap.</span>
<span class="line-removed">2297 </span>
<span class="line-removed">2298   if (use_requested_addr) {</span>
<span class="line-removed">2299     char* archive_space_base = static_mapinfo-&gt;requested_base_address() + base_offset;</span>
<span class="line-removed">2300     char* archive_space_end  = archive_space_base + archive_space_size;</span>
<span class="line-removed">2301     if (!MetaspaceShared::use_windows_memory_mapping()) {</span>
<span class="line-removed">2302       archive_space_rs = reserve_shared_space(archive_space_size, archive_space_base);</span>
<span class="line-removed">2303       if (!archive_space_rs.is_reserved()) {</span>
<span class="line-removed">2304         return NULL;</span>
<span class="line-removed">2305       }</span>
<span class="line-removed">2306     }</span>
<span class="line-removed">2307     if (use_klass_space) {</span>
<span class="line-removed">2308       // Make sure we can map the klass space immediately following the archive_space space</span>
<span class="line-removed">2309       // Don&#39;t call reserve_shared_space here as that may try to enforce platform-specific</span>
<span class="line-removed">2310       // alignment rules which only apply to the archive base address</span>
<span class="line-removed">2311       char* class_space_base = archive_space_end;</span>
<span class="line-removed">2312       class_space_rs = ReservedSpace(class_space_size, reserved_space_alignment(),</span>
<span class="line-removed">2313                                      false /* large_pages */, class_space_base);</span>
<span class="line-removed">2314       if (!class_space_rs.is_reserved()) {</span>
<span class="line-removed">2315         return NULL;</span>
<span class="line-removed">2316       }</span>
<span class="line-removed">2317     }</span>
<span class="line-removed">2318     return static_mapinfo-&gt;requested_base_address();</span>
<span class="line-removed">2319   } else {</span>
<span class="line-removed">2320     if (use_klass_space) {</span>
<span class="line-removed">2321       main_rs = reserve_shared_space(archive_space_size + class_space_size);</span>
<span class="line-removed">2322       if (main_rs.is_reserved()) {</span>
<span class="line-removed">2323         archive_space_rs = main_rs.first_part(archive_space_size, reserved_space_alignment(), /*split=*/true);</span>
<span class="line-removed">2324         class_space_rs = main_rs.last_part(archive_space_size);</span>
<span class="line-removed">2325       }</span>
<span class="line-removed">2326     } else {</span>
<span class="line-removed">2327       main_rs = reserve_shared_space(archive_space_size);</span>
<span class="line-removed">2328       archive_space_rs = main_rs;</span>
2329     }







2330     if (archive_space_rs.is_reserved()) {


2331       return archive_space_rs.base();
<span class="line-modified">2332     } else {</span>
<span class="line-modified">2333       return NULL;</span>






































2334     }
2335   }































2336 }
2337 
<span class="line-modified">2338 void MetaspaceShared::release_reserved_spaces(ReservedSpace&amp; main_rs,</span>
<span class="line-removed">2339                                               ReservedSpace&amp; archive_space_rs,</span>
2340                                               ReservedSpace&amp; class_space_rs) {
<span class="line-modified">2341   if (main_rs.is_reserved()) {</span>
<span class="line-modified">2342     assert(main_rs.contains(archive_space_rs.base()), &quot;must be&quot;);</span>
<span class="line-modified">2343     assert(main_rs.contains(class_space_rs.base()), &quot;must be&quot;);</span>
<span class="line-modified">2344     log_debug(cds)(&quot;Released shared space (archive+classes) &quot; INTPTR_FORMAT, p2i(main_rs.base()));</span>
<span class="line-modified">2345     main_rs.release();</span>
<span class="line-modified">2346   } else {</span>
<span class="line-modified">2347     if (archive_space_rs.is_reserved()) {</span>
<span class="line-removed">2348       log_debug(cds)(&quot;Released shared space (archive) &quot; INTPTR_FORMAT, p2i(archive_space_rs.base()));</span>
<span class="line-removed">2349       archive_space_rs.release();</span>
<span class="line-removed">2350     }</span>
<span class="line-removed">2351     if (class_space_rs.is_reserved()) {</span>
<span class="line-removed">2352       log_debug(cds)(&quot;Released shared space (classes) &quot; INTPTR_FORMAT, p2i(class_space_rs.base()));</span>
<span class="line-removed">2353       class_space_rs.release();</span>
<span class="line-removed">2354     }</span>
2355   }
2356 }
2357 
2358 static int archive_regions[]  = {MetaspaceShared::mc,
2359                                  MetaspaceShared::rw,
2360                                  MetaspaceShared::ro};
2361 static int archive_regions_count  = 3;
2362 
2363 MapArchiveResult MetaspaceShared::map_archive(FileMapInfo* mapinfo, char* mapped_base_address, ReservedSpace rs) {
2364   assert(UseSharedSpaces, &quot;must be runtime&quot;);
2365   if (mapinfo == NULL) {
2366     return MAP_ARCHIVE_SUCCESS; // The dynamic archive has not been specified. No error has happened -- trivially succeeded.
2367   }
2368 
2369   mapinfo-&gt;set_is_mapped(false);
2370 
2371   if (mapinfo-&gt;alignment() != (size_t)os::vm_allocation_granularity()) {
2372     log_error(cds)(&quot;Unable to map CDS archive -- os::vm_allocation_granularity() expected: &quot; SIZE_FORMAT
2373                    &quot; actual: %d&quot;, mapinfo-&gt;alignment(), os::vm_allocation_granularity());
2374     return MAP_ARCHIVE_OTHER_FAILURE;
</pre>
<hr />
<pre>
2479 }
2480 
2481 void MetaspaceShared::report_out_of_space(const char* name, size_t needed_bytes) {
2482   // This is highly unlikely to happen on 64-bits because we have reserved a 4GB space.
2483   // On 32-bit we reserve only 256MB so you could run out of space with 100,000 classes
2484   // or so.
2485   _mc_region.print_out_of_space_msg(name, needed_bytes);
2486   _rw_region.print_out_of_space_msg(name, needed_bytes);
2487   _ro_region.print_out_of_space_msg(name, needed_bytes);
2488 
2489   vm_exit_during_initialization(err_msg(&quot;Unable to allocate from &#39;%s&#39; region&quot;, name),
2490                                 &quot;Please reduce the number of shared classes.&quot;);
2491 }
2492 
2493 // This is used to relocate the pointers so that the archive can be mapped at
2494 // Arguments::default_SharedBaseAddress() without runtime relocation.
2495 intx MetaspaceShared::final_delta() {
2496   return intx(Arguments::default_SharedBaseAddress())  // We want the archive to be mapped to here at runtime
2497        - intx(SharedBaseAddress);                      // .. but the archive is mapped at here at dump time
2498 }




























</pre>
</td>
<td>
<hr />
<pre>
  54 #include &quot;oops/instanceClassLoaderKlass.hpp&quot;
  55 #include &quot;oops/instanceMirrorKlass.hpp&quot;
  56 #include &quot;oops/instanceRefKlass.hpp&quot;
  57 #include &quot;oops/methodData.hpp&quot;
  58 #include &quot;oops/objArrayKlass.hpp&quot;
  59 #include &quot;oops/objArrayOop.hpp&quot;
  60 #include &quot;oops/oop.inline.hpp&quot;
  61 #include &quot;oops/typeArrayKlass.hpp&quot;
  62 #include &quot;oops/valueArrayKlass.hpp&quot;
  63 #include &quot;oops/valueKlass.hpp&quot;
  64 #include &quot;prims/jvmtiRedefineClasses.hpp&quot;
  65 #include &quot;runtime/handles.inline.hpp&quot;
  66 #include &quot;runtime/os.hpp&quot;
  67 #include &quot;runtime/safepointVerifiers.hpp&quot;
  68 #include &quot;runtime/signature.hpp&quot;
  69 #include &quot;runtime/timerTrace.hpp&quot;
  70 #include &quot;runtime/vmThread.hpp&quot;
  71 #include &quot;runtime/vmOperations.hpp&quot;
  72 #include &quot;utilities/align.hpp&quot;
  73 #include &quot;utilities/bitMap.inline.hpp&quot;
<span class="line-added">  74 #include &quot;utilities/ostream.hpp&quot;</span>
  75 #include &quot;utilities/defaultStream.hpp&quot;
  76 #include &quot;utilities/hashtable.inline.hpp&quot;
  77 #if INCLUDE_G1GC
  78 #include &quot;gc/g1/g1CollectedHeap.hpp&quot;
  79 #endif
  80 
  81 ReservedSpace MetaspaceShared::_shared_rs;
  82 VirtualSpace MetaspaceShared::_shared_vs;
  83 ReservedSpace MetaspaceShared::_symbol_rs;
  84 VirtualSpace MetaspaceShared::_symbol_vs;
  85 MetaspaceSharedStats MetaspaceShared::_stats;
  86 bool MetaspaceShared::_has_error_classes;
  87 bool MetaspaceShared::_archive_loading_failed = false;
  88 bool MetaspaceShared::_remapped_readwrite = false;
  89 address MetaspaceShared::_i2i_entry_code_buffers = NULL;
  90 size_t MetaspaceShared::_i2i_entry_code_buffers_size = 0;
  91 void* MetaspaceShared::_shared_metaspace_static_top = NULL;
  92 intx MetaspaceShared::_relocation_delta;
  93 
  94 // The CDS archive is divided into the following regions:
</pre>
<hr />
<pre>
 177                  _name, p2i(_base), p2i(_top), int(_end - _base), int(_top - _base));
 178   if (strcmp(_name, failing_region) == 0) {
 179     log_error(cds)(&quot; required = %d&quot;, int(needed_bytes));
 180   }
 181 }
 182 
 183 void DumpRegion::init(ReservedSpace* rs, VirtualSpace* vs) {
 184   _rs = rs;
 185   _vs = vs;
 186   // Start with 0 committed bytes. The memory will be committed as needed by
 187   // MetaspaceShared::commit_to().
 188   if (!_vs-&gt;initialize(*_rs, 0)) {
 189     fatal(&quot;Unable to allocate memory for shared space&quot;);
 190   }
 191   _base = _top = _rs-&gt;base();
 192   _end = _rs-&gt;end();
 193 }
 194 
 195 void DumpRegion::pack(DumpRegion* next) {
 196   assert(!is_packed(), &quot;sanity&quot;);
<span class="line-modified"> 197   _end = (char*)align_up(_top, MetaspaceShared::reserved_space_alignment());</span>
 198   _is_packed = true;
 199   if (next != NULL) {
 200     next-&gt;_rs = _rs;
 201     next-&gt;_vs = _vs;
 202     next-&gt;_base = next-&gt;_top = this-&gt;_end;
 203     next-&gt;_end = _rs-&gt;end();
 204   }
 205 }
 206 
 207 static DumpRegion _mc_region(&quot;mc&quot;), _ro_region(&quot;ro&quot;), _rw_region(&quot;rw&quot;), _symbol_region(&quot;symbols&quot;);
 208 static size_t _total_closed_archive_region_size = 0, _total_open_archive_region_size = 0;
 209 
 210 void MetaspaceShared::init_shared_dump_space(DumpRegion* first_space) {
 211   first_space-&gt;init(&amp;_shared_rs, &amp;_shared_vs);
 212 }
 213 
 214 DumpRegion* MetaspaceShared::misc_code_dump_space() {
 215   return &amp;_mc_region;
 216 }
 217 
</pre>
<hr />
<pre>
 223   return &amp;_ro_region;
 224 }
 225 
 226 void MetaspaceShared::pack_dump_space(DumpRegion* current, DumpRegion* next,
 227                                       ReservedSpace* rs) {
 228   current-&gt;pack(next);
 229 }
 230 
 231 char* MetaspaceShared::symbol_space_alloc(size_t num_bytes) {
 232   return _symbol_region.allocate(num_bytes);
 233 }
 234 
 235 char* MetaspaceShared::misc_code_space_alloc(size_t num_bytes) {
 236   return _mc_region.allocate(num_bytes);
 237 }
 238 
 239 char* MetaspaceShared::read_only_space_alloc(size_t num_bytes) {
 240   return _ro_region.allocate(num_bytes);
 241 }
 242 
<span class="line-modified"> 243 size_t MetaspaceShared::reserved_space_alignment() { return os::vm_allocation_granularity(); }</span>















 244 
<span class="line-modified"> 245 #ifdef _LP64</span>
<span class="line-modified"> 246 // Check SharedBaseAddress for validity. At this point, os::init() must</span>
<span class="line-modified"> 247 //  have been ran.</span>
<span class="line-added"> 248 static void check_SharedBaseAddress() {</span>
<span class="line-added"> 249   SharedBaseAddress = align_up(SharedBaseAddress,</span>
<span class="line-added"> 250                                MetaspaceShared::reserved_space_alignment());</span>
<span class="line-added"> 251   if (!CompressedKlassPointers::is_valid_base((address)SharedBaseAddress)) {</span>
<span class="line-added"> 252     log_warning(cds)(&quot;SharedBaseAddress=&quot; PTR_FORMAT &quot; is invalid for this &quot;</span>
<span class="line-added"> 253                      &quot;platform, option will be ignored.&quot;,</span>
<span class="line-added"> 254                      p2i((address)SharedBaseAddress));</span>
<span class="line-added"> 255     SharedBaseAddress = Arguments::default_SharedBaseAddress();</span>
<span class="line-added"> 256   }</span>
 257 }
<span class="line-added"> 258 #endif</span>
 259 
 260 void MetaspaceShared::initialize_dumptime_shared_and_meta_spaces() {
 261   assert(DumpSharedSpaces, &quot;should be called for dump time only&quot;);
<span class="line-modified"> 262 </span>
<span class="line-added"> 263 #ifdef _LP64</span>
<span class="line-added"> 264   check_SharedBaseAddress();</span>
<span class="line-added"> 265 #endif</span>
<span class="line-added"> 266 </span>
<span class="line-added"> 267   const size_t reserve_alignment = MetaspaceShared::reserved_space_alignment();</span>
 268   char* shared_base = (char*)align_up((char*)SharedBaseAddress, reserve_alignment);
 269 
 270 #ifdef _LP64
<span class="line-modified"> 271   assert(CompressedKlassPointers::is_valid_base((address)shared_base), &quot;Sanity&quot;);</span>
<span class="line-modified"> 272   // On 64-bit VM we reserve a 4G range and, if UseCompressedClassPointers=1,</span>
<span class="line-modified"> 273   //  will use that to house both the archives and the ccs. See below for</span>
<span class="line-modified"> 274   //  details.</span>






 275   const uint64_t UnscaledClassSpaceMax = (uint64_t(max_juint) + 1);
 276   const size_t cds_total = align_down(UnscaledClassSpaceMax, reserve_alignment);
 277 #else
<span class="line-modified"> 278   // We don&#39;t support archives larger than 256MB on 32-bit due to limited</span>
<span class="line-added"> 279   //  virtual address space.</span>
 280   size_t cds_total = align_down(256*M, reserve_alignment);
 281 #endif
 282 
<span class="line-added"> 283   // Whether to use SharedBaseAddress as attach address.</span>
 284   bool use_requested_base = true;
<span class="line-added"> 285 </span>
<span class="line-added"> 286   if (shared_base == NULL) {</span>
<span class="line-added"> 287     use_requested_base = false;</span>
<span class="line-added"> 288   }</span>
<span class="line-added"> 289 </span>
 290   if (ArchiveRelocationMode == 1) {
 291     log_info(cds)(&quot;ArchiveRelocationMode == 1: always allocate class space at an alternative address&quot;);
 292     use_requested_base = false;
 293   }
 294 
 295   // First try to reserve the space at the specified SharedBaseAddress.
 296   assert(!_shared_rs.is_reserved(), &quot;must be&quot;);
 297   if (use_requested_base) {
<span class="line-modified"> 298     _shared_rs = ReservedSpace(cds_total, reserve_alignment,</span>
<span class="line-added"> 299                                false /* large */, (char*)shared_base);</span>
<span class="line-added"> 300     if (_shared_rs.is_reserved()) {</span>
<span class="line-added"> 301       assert(_shared_rs.base() == shared_base, &quot;should match&quot;);</span>
<span class="line-added"> 302     } else {</span>
<span class="line-added"> 303       log_info(cds)(&quot;dumptime space reservation: failed to map at &quot;</span>
<span class="line-added"> 304                     &quot;SharedBaseAddress &quot; PTR_FORMAT, p2i(shared_base));</span>
<span class="line-added"> 305     }</span>
 306   }
<span class="line-modified"> 307   if (!_shared_rs.is_reserved()) {</span>
<span class="line-modified"> 308     // Get a reserved space anywhere if attaching at the SharedBaseAddress</span>
<span class="line-modified"> 309     //  fails:</span>
<span class="line-modified"> 310     if (UseCompressedClassPointers) {</span>
<span class="line-modified"> 311       // If we need to reserve class space as well, let the platform handle</span>
<span class="line-added"> 312       //  the reservation.</span>
<span class="line-added"> 313       LP64_ONLY(_shared_rs =</span>
<span class="line-added"> 314                 Metaspace::reserve_address_space_for_compressed_classes(cds_total);)</span>
<span class="line-added"> 315       NOT_LP64(ShouldNotReachHere();)</span>
<span class="line-added"> 316     } else {</span>
<span class="line-added"> 317       // anywhere is fine.</span>
<span class="line-added"> 318       _shared_rs = ReservedSpace(cds_total, reserve_alignment,</span>
<span class="line-added"> 319                                  false /* large */, (char*)NULL);</span>
<span class="line-added"> 320     }</span>
 321   }
<span class="line-added"> 322 </span>
 323   if (!_shared_rs.is_reserved()) {
 324     vm_exit_during_initialization(&quot;Unable to reserve memory for shared space&quot;,
 325                                   err_msg(SIZE_FORMAT &quot; bytes.&quot;, cds_total));
 326   }
 327 
 328 #ifdef _LP64











 329 
 330   if (UseCompressedClassPointers) {
<span class="line-modified"> 331 </span>
<span class="line-modified"> 332     assert(CompressedKlassPointers::is_valid_base((address)_shared_rs.base()), &quot;Sanity&quot;);</span>
<span class="line-modified"> 333 </span>
<span class="line-modified"> 334     // On 64-bit VM, if UseCompressedClassPointers=1, the compressed class space</span>
<span class="line-modified"> 335     //  must be allocated near the cds such as that the compressed Klass pointer</span>
<span class="line-modified"> 336     //  encoding can be used to en/decode pointers from both cds and ccs. Since</span>
<span class="line-modified"> 337     //  Metaspace cannot do this (it knows nothing about cds), we do it for</span>
<span class="line-added"> 338     //  Metaspace here and pass it the space to use for ccs.</span>
<span class="line-added"> 339     //</span>
<span class="line-added"> 340     // We do this by reserving space for the ccs behind the archives. Note</span>
<span class="line-added"> 341     //  however that ccs follows a different alignment</span>
<span class="line-added"> 342     //  (Metaspace::reserve_alignment), so there may be a gap between ccs and</span>
<span class="line-added"> 343     //  cds.</span>
<span class="line-added"> 344     // We use a similar layout at runtime, see reserve_address_space_for_archives().</span>
<span class="line-added"> 345     //</span>
<span class="line-added"> 346     //                              +-- SharedBaseAddress (default = 0x800000000)</span>
<span class="line-added"> 347     //                              v</span>
<span class="line-added"> 348     // +-..---------+---------+ ... +----+----+----+--------+-----------------+</span>
<span class="line-added"> 349     // |    Heap    | Archive |     | MC | RW | RO | [gap]  |    class space  |</span>
<span class="line-added"> 350     // +-..---------+---------+ ... +----+----+----+--------+-----------------+</span>
<span class="line-added"> 351     // |&lt;--   MaxHeapSize  --&gt;|     |&lt;-- UnscaledClassSpaceMax = 4GB --&gt;|</span>
<span class="line-added"> 352     //</span>
<span class="line-added"> 353     // Note: ccs must follow the archives, and the archives must start at the</span>
<span class="line-added"> 354     //  encoding base. However, the exact placement of ccs does not matter as</span>
<span class="line-added"> 355     //  long as it it resides in the encoding range of CompressedKlassPointers</span>
<span class="line-added"> 356     //  and comes after the archive.</span>
<span class="line-added"> 357     //</span>
<span class="line-added"> 358     // We do this by splitting up the allocated 4G into 3G of archive space,</span>
<span class="line-added"> 359     //  followed by 1G for the ccs:</span>
<span class="line-added"> 360     // + The upper 1 GB is used as the &quot;temporary compressed class space&quot;</span>
<span class="line-added"> 361     //   -- preload_classes() will store Klasses into this space.</span>
<span class="line-added"> 362     // + The lower 3 GB is used for the archive -- when preload_classes()</span>
<span class="line-added"> 363     //   is done, ArchiveCompactor will copy the class metadata into this</span>
<span class="line-added"> 364     //   space, first the RW parts, then the RO parts.</span>
<span class="line-added"> 365 </span>
<span class="line-added"> 366     // Starting address of ccs must be aligned to Metaspace::reserve_alignment()...</span>
<span class="line-added"> 367     size_t class_space_size = align_down(_shared_rs.size() / 4, Metaspace::reserve_alignment());</span>
<span class="line-added"> 368     address class_space_start = (address)align_down(_shared_rs.end() - class_space_size, Metaspace::reserve_alignment());</span>
<span class="line-added"> 369     size_t archive_size = class_space_start - (address)_shared_rs.base();</span>
<span class="line-added"> 370 </span>
<span class="line-added"> 371     ReservedSpace tmp_class_space = _shared_rs.last_part(archive_size);</span>
<span class="line-added"> 372     _shared_rs = _shared_rs.first_part(archive_size);</span>
<span class="line-added"> 373 </span>
<span class="line-added"> 374     // ... as does the size of ccs.</span>
<span class="line-added"> 375     tmp_class_space = tmp_class_space.first_part(class_space_size);</span>
<span class="line-added"> 376     CompressedClassSpaceSize = class_space_size;</span>
<span class="line-added"> 377 </span>
<span class="line-added"> 378     // Let Metaspace initialize ccs</span>
 379     Metaspace::initialize_class_space(tmp_class_space);
<span class="line-added"> 380 </span>
<span class="line-added"> 381     // and set up CompressedKlassPointers encoding.</span>
<span class="line-added"> 382     CompressedKlassPointers::initialize((address)_shared_rs.base(), cds_total);</span>
<span class="line-added"> 383 </span>
<span class="line-added"> 384     log_info(cds)(&quot;narrow_klass_base = &quot; PTR_FORMAT &quot;, narrow_klass_shift = %d&quot;,</span>
<span class="line-added"> 385                   p2i(CompressedKlassPointers::base()), CompressedKlassPointers::shift());</span>
<span class="line-added"> 386 </span>
<span class="line-added"> 387     log_info(cds)(&quot;Allocated temporary class space: &quot; SIZE_FORMAT &quot; bytes at &quot; PTR_FORMAT,</span>
<span class="line-added"> 388                   CompressedClassSpaceSize, p2i(tmp_class_space.base()));</span>
<span class="line-added"> 389 </span>
<span class="line-added"> 390     assert(_shared_rs.end() == tmp_class_space.base() &amp;&amp;</span>
<span class="line-added"> 391            is_aligned(_shared_rs.base(), MetaspaceShared::reserved_space_alignment()) &amp;&amp;</span>
<span class="line-added"> 392            is_aligned(tmp_class_space.base(), Metaspace::reserve_alignment()) &amp;&amp;</span>
<span class="line-added"> 393            is_aligned(tmp_class_space.size(), Metaspace::reserve_alignment()), &quot;Sanity&quot;);</span>
 394   }




 395 
 396 #endif
 397 
 398   init_shared_dump_space(&amp;_mc_region);
 399   SharedBaseAddress = (size_t)_shared_rs.base();
 400   log_info(cds)(&quot;Allocated shared space: &quot; SIZE_FORMAT &quot; bytes at &quot; PTR_FORMAT,
 401                 _shared_rs.size(), p2i(_shared_rs.base()));
 402 
 403   size_t symbol_rs_size = LP64_ONLY(3 * G) NOT_LP64(128 * M);
 404   _symbol_rs = ReservedSpace(symbol_rs_size);
 405   if (!_symbol_rs.is_reserved()) {
 406     vm_exit_during_initialization(&quot;Unable to reserve memory for symbols&quot;,
 407                                   err_msg(SIZE_FORMAT &quot; bytes.&quot;, symbol_rs_size));
 408   }
 409   _symbol_region.init(&amp;_symbol_rs, &amp;_symbol_vs);
 410 }
 411 
 412 // Called by universe_post_init()
 413 void MetaspaceShared::post_initialize(TRAPS) {
 414   if (UseSharedSpaces) {
</pre>
<hr />
<pre>
1208                                const char *name, size_t total_size);
1209   void relocate_to_default_base_address(CHeapBitMap* ptrmap);
1210 
1211 public:
1212 
1213   VMOp_Type type() const { return VMOp_PopulateDumpSharedSpace; }
1214   void doit();   // outline because gdb sucks
1215   bool allow_nested_vm_operations() const { return true; }
1216 }; // class VM_PopulateDumpSharedSpace
1217 
1218 class SortedSymbolClosure: public SymbolClosure {
1219   GrowableArray&lt;Symbol*&gt; _symbols;
1220   virtual void do_symbol(Symbol** sym) {
1221     assert((*sym)-&gt;is_permanent(), &quot;archived symbols must be permanent&quot;);
1222     _symbols.append(*sym);
1223   }
1224   static int compare_symbols_by_address(Symbol** a, Symbol** b) {
1225     if (a[0] &lt; b[0]) {
1226       return -1;
1227     } else if (a[0] == b[0]) {
<span class="line-added">1228       ResourceMark rm;</span>
<span class="line-added">1229       log_warning(cds)(&quot;Duplicated symbol %s unexpected&quot;, (*a)-&gt;as_C_string());</span>
1230       return 0;
1231     } else {
1232       return 1;
1233     }
1234   }
1235 
1236 public:
1237   SortedSymbolClosure() {
1238     SymbolTable::symbols_do(this);
1239     _symbols.sort(compare_symbols_by_address);
1240   }
1241   GrowableArray&lt;Symbol*&gt;* get_sorted_symbols() {
1242     return &amp;_symbols;
1243   }
1244 };
1245 
1246 // ArchiveCompactor --
1247 //
1248 // This class is the central piece of shared archive compaction -- all metaspace data are
1249 // initially allocated outside of the shared regions. ArchiveCompactor copies the
</pre>
<hr />
<pre>
2137 
2138 bool MetaspaceShared::is_in_trampoline_frame(address addr) {
2139   if (UseSharedSpaces &amp;&amp; is_in_shared_region(addr, MetaspaceShared::mc)) {
2140     return true;
2141   }
2142   return false;
2143 }
2144 
2145 bool MetaspaceShared::is_shared_dynamic(void* p) {
2146   if ((p &lt; MetaspaceObj::shared_metaspace_top()) &amp;&amp;
2147       (p &gt;= _shared_metaspace_static_top)) {
2148     return true;
2149   } else {
2150     return false;
2151   }
2152 }
2153 
2154 void MetaspaceShared::initialize_runtime_shared_and_meta_spaces() {
2155   assert(UseSharedSpaces, &quot;Must be called when UseSharedSpaces is enabled&quot;);
2156   MapArchiveResult result = MAP_ARCHIVE_OTHER_FAILURE;
<span class="line-added">2157 </span>
2158   FileMapInfo* static_mapinfo = open_static_archive();
2159   FileMapInfo* dynamic_mapinfo = NULL;
2160 
2161   if (static_mapinfo != NULL) {
2162     dynamic_mapinfo = open_dynamic_archive();
2163 
2164     // First try to map at the requested address
2165     result = map_archives(static_mapinfo, dynamic_mapinfo, true);
2166     if (result == MAP_ARCHIVE_MMAP_FAILURE) {
2167       // Mapping has failed (probably due to ASLR). Let&#39;s map at an address chosen
2168       // by the OS.
2169       log_info(cds)(&quot;Try to map archive(s) at an alternative address&quot;);
2170       result = map_archives(static_mapinfo, dynamic_mapinfo, false);
2171     }
2172   }
2173 
2174   if (result == MAP_ARCHIVE_SUCCESS) {
2175     bool dynamic_mapped = (dynamic_mapinfo != NULL &amp;&amp; dynamic_mapinfo-&gt;is_mapped());
2176     char* cds_base = static_mapinfo-&gt;mapped_base();
2177     char* cds_end =  dynamic_mapped ? dynamic_mapinfo-&gt;mapped_end() : static_mapinfo-&gt;mapped_end();
</pre>
<hr />
<pre>
2214   }
2215   if (Arguments::GetSharedDynamicArchivePath() == NULL) {
2216     return NULL;
2217   }
2218 
2219   FileMapInfo* mapinfo = new FileMapInfo(false);
2220   if (!mapinfo-&gt;initialize()) {
2221     delete(mapinfo);
2222     return NULL;
2223   }
2224   return mapinfo;
2225 }
2226 
2227 // use_requested_addr:
2228 //  true  = map at FileMapHeader::_requested_base_address
2229 //  false = map at an alternative address picked by OS.
2230 MapArchiveResult MetaspaceShared::map_archives(FileMapInfo* static_mapinfo, FileMapInfo* dynamic_mapinfo,
2231                                                bool use_requested_addr) {
2232   PRODUCT_ONLY(if (ArchiveRelocationMode == 1 &amp;&amp; use_requested_addr) {
2233       // For product build only -- this is for benchmarking the cost of doing relocation.
<span class="line-modified">2234       // For debug builds, the check is done below, after reserving the space, for better test coverage</span>
<span class="line-added">2235       // (see comment below).</span>
2236       log_info(cds)(&quot;ArchiveRelocationMode == 1: always map archive(s) at an alternative address&quot;);
2237       return MAP_ARCHIVE_MMAP_FAILURE;
2238     });
2239 
2240   if (ArchiveRelocationMode == 2 &amp;&amp; !use_requested_addr) {
2241     log_info(cds)(&quot;ArchiveRelocationMode == 2: never map archive(s) at an alternative address&quot;);
2242     return MAP_ARCHIVE_MMAP_FAILURE;
2243   };
2244 
2245   if (dynamic_mapinfo != NULL) {
2246     // Ensure that the OS won&#39;t be able to allocate new memory spaces between the two
2247     // archives, or else it would mess up the simple comparision in MetaspaceObj::is_shared().
2248     assert(static_mapinfo-&gt;mapping_end_offset() == dynamic_mapinfo-&gt;mapping_base_offset(), &quot;no gap&quot;);
2249   }
2250 
<span class="line-modified">2251   ReservedSpace archive_space_rs, class_space_rs;</span>
2252   MapArchiveResult result = MAP_ARCHIVE_OTHER_FAILURE;
2253   char* mapped_base_address = reserve_address_space_for_archives(static_mapinfo, dynamic_mapinfo,
<span class="line-modified">2254                                                                  use_requested_addr, archive_space_rs,</span>
2255                                                                  class_space_rs);
2256   if (mapped_base_address == NULL) {
2257     result = MAP_ARCHIVE_MMAP_FAILURE;
2258   } else {
<span class="line-added">2259 </span>
<span class="line-added">2260 #ifdef ASSERT</span>
<span class="line-added">2261     // Some sanity checks after reserving address spaces for archives</span>
<span class="line-added">2262     //  and class space.</span>
<span class="line-added">2263     assert(archive_space_rs.is_reserved(), &quot;Sanity&quot;);</span>
<span class="line-added">2264     if (Metaspace::using_class_space()) {</span>
<span class="line-added">2265       // Class space must closely follow the archive space. Both spaces</span>
<span class="line-added">2266       //  must be aligned correctly.</span>
<span class="line-added">2267       assert(class_space_rs.is_reserved(),</span>
<span class="line-added">2268              &quot;A class space should have been reserved&quot;);</span>
<span class="line-added">2269       assert(class_space_rs.base() &gt;= archive_space_rs.end(),</span>
<span class="line-added">2270              &quot;class space should follow the cds archive space&quot;);</span>
<span class="line-added">2271       assert(is_aligned(archive_space_rs.base(),</span>
<span class="line-added">2272                         MetaspaceShared::reserved_space_alignment()),</span>
<span class="line-added">2273              &quot;Archive space misaligned&quot;);</span>
<span class="line-added">2274       assert(is_aligned(class_space_rs.base(),</span>
<span class="line-added">2275                         Metaspace::reserve_alignment()),</span>
<span class="line-added">2276              &quot;class space misaligned&quot;);</span>
<span class="line-added">2277     }</span>
<span class="line-added">2278 #endif // ASSERT</span>
<span class="line-added">2279 </span>
2280     log_debug(cds)(&quot;Reserved archive_space_rs     [&quot; INTPTR_FORMAT &quot; - &quot; INTPTR_FORMAT &quot;] (&quot; SIZE_FORMAT &quot;) bytes&quot;,
2281                    p2i(archive_space_rs.base()), p2i(archive_space_rs.end()), archive_space_rs.size());
2282     log_debug(cds)(&quot;Reserved class_space_rs [&quot; INTPTR_FORMAT &quot; - &quot; INTPTR_FORMAT &quot;] (&quot; SIZE_FORMAT &quot;) bytes&quot;,
2283                    p2i(class_space_rs.base()), p2i(class_space_rs.end()), class_space_rs.size());
<span class="line-added">2284 </span>
<span class="line-added">2285     if (MetaspaceShared::use_windows_memory_mapping()) {</span>
<span class="line-added">2286       // We have now reserved address space for the archives, and will map in</span>
<span class="line-added">2287       //  the archive files into this space.</span>
<span class="line-added">2288       //</span>
<span class="line-added">2289       // Special handling for Windows: on Windows we cannot map a file view</span>
<span class="line-added">2290       //  into an existing memory mapping. So, we unmap the address range we</span>
<span class="line-added">2291       //  just reserved again, which will make it available for mapping the</span>
<span class="line-added">2292       //  archives.</span>
<span class="line-added">2293       // Reserving this range has not been for naught however since it makes</span>
<span class="line-added">2294       //  us reasonably sure the address range is available.</span>
<span class="line-added">2295       //</span>
<span class="line-added">2296       // But still it may fail, since between unmapping the range and mapping</span>
<span class="line-added">2297       //  in the archive someone else may grab the address space. Therefore</span>
<span class="line-added">2298       //  there is a fallback in FileMap::map_region() where we just read in</span>
<span class="line-added">2299       //  the archive files sequentially instead of mapping it in. We couple</span>
<span class="line-added">2300       //  this with use_requested_addr, since we&#39;re going to patch all the</span>
<span class="line-added">2301       //  pointers anyway so there&#39;s no benefit to mmap.</span>
<span class="line-added">2302       if (use_requested_addr) {</span>
<span class="line-added">2303         log_info(cds)(&quot;Windows mmap workaround: releasing archive space.&quot;);</span>
<span class="line-added">2304         archive_space_rs.release();</span>
<span class="line-added">2305       }</span>
<span class="line-added">2306     }</span>
2307     MapArchiveResult static_result = map_archive(static_mapinfo, mapped_base_address, archive_space_rs);
2308     MapArchiveResult dynamic_result = (static_result == MAP_ARCHIVE_SUCCESS) ?
2309                                      map_archive(dynamic_mapinfo, mapped_base_address, archive_space_rs) : MAP_ARCHIVE_OTHER_FAILURE;
2310 
2311     DEBUG_ONLY(if (ArchiveRelocationMode == 1 &amp;&amp; use_requested_addr) {
<span class="line-modified">2312       // This is for simulating mmap failures at the requested address. In</span>
<span class="line-modified">2313       //  debug builds, we do it here (after all archives have possibly been</span>
<span class="line-modified">2314       //  mapped), so we can thoroughly test the code for failure handling</span>
<span class="line-added">2315       //  (releasing all allocated resource, etc).</span>
2316       log_info(cds)(&quot;ArchiveRelocationMode == 1: always map archive(s) at an alternative address&quot;);
2317       if (static_result == MAP_ARCHIVE_SUCCESS) {
2318         static_result = MAP_ARCHIVE_MMAP_FAILURE;
2319       }
2320       if (dynamic_result == MAP_ARCHIVE_SUCCESS) {
2321         dynamic_result = MAP_ARCHIVE_MMAP_FAILURE;
2322       }
2323     });
2324 
2325     if (static_result == MAP_ARCHIVE_SUCCESS) {
2326       if (dynamic_result == MAP_ARCHIVE_SUCCESS) {
2327         result = MAP_ARCHIVE_SUCCESS;
2328       } else if (dynamic_result == MAP_ARCHIVE_OTHER_FAILURE) {
2329         assert(dynamic_mapinfo != NULL &amp;&amp; !dynamic_mapinfo-&gt;is_mapped(), &quot;must have failed&quot;);
2330         // No need to retry mapping the dynamic archive again, as it will never succeed
2331         // (bad file, etc) -- just keep the base archive.
2332         log_warning(cds, dynamic)(&quot;Unable to use shared archive. The top archive failed to load: %s&quot;,
2333                                   dynamic_mapinfo-&gt;full_path());
2334         result = MAP_ARCHIVE_SUCCESS;
2335         // TODO, we can give the unused space for the dynamic archive to class_space_rs, but there&#39;s no
2336         // easy API to do that right now.
2337       } else {
2338         result = MAP_ARCHIVE_MMAP_FAILURE;
2339       }
2340     } else if (static_result == MAP_ARCHIVE_OTHER_FAILURE) {
2341       result = MAP_ARCHIVE_OTHER_FAILURE;
2342     } else {
2343       result = MAP_ARCHIVE_MMAP_FAILURE;
2344     }
2345   }
2346 
2347   if (result == MAP_ARCHIVE_SUCCESS) {



2348     SharedBaseAddress = (size_t)mapped_base_address;
2349     LP64_ONLY({
2350         if (Metaspace::using_class_space()) {
<span class="line-modified">2351           // Set up ccs in metaspace.</span>
<span class="line-modified">2352           Metaspace::initialize_class_space(class_space_rs);</span>
<span class="line-modified">2353 </span>
<span class="line-added">2354           // Set up compressed Klass pointer encoding: the encoding range must</span>
<span class="line-added">2355           //  cover both archive and class space.</span>
<span class="line-added">2356           address cds_base = (address)static_mapinfo-&gt;mapped_base();</span>
<span class="line-added">2357           address ccs_end = (address)class_space_rs.end();</span>
<span class="line-added">2358           CompressedKlassPointers::initialize(cds_base, ccs_end - cds_base);</span>
<span class="line-added">2359 </span>
2360           // map_heap_regions() compares the current narrow oop and klass encodings
2361           // with the archived ones, so it must be done after all encodings are determined.
2362           static_mapinfo-&gt;map_heap_regions();

2363         }
2364       });
2365   } else {
2366     unmap_archive(static_mapinfo);
2367     unmap_archive(dynamic_mapinfo);
<span class="line-modified">2368     release_reserved_spaces(archive_space_rs, class_space_rs);</span>
2369   }
2370 
2371   return result;
2372 }
2373 
<span class="line-added">2374 </span>
<span class="line-added">2375 // This will reserve two address spaces suitable to house Klass structures, one</span>
<span class="line-added">2376 //  for the cds archives (static archive and optionally dynamic archive) and</span>
<span class="line-added">2377 //  optionally one move for ccs.</span>
<span class="line-added">2378 //</span>
<span class="line-added">2379 // Since both spaces must fall within the compressed class pointer encoding</span>
<span class="line-added">2380 //  range, they are allocated close to each other.</span>
<span class="line-added">2381 //</span>
<span class="line-added">2382 // Space for archives will be reserved first, followed by a potential gap,</span>
<span class="line-added">2383 //  followed by the space for ccs:</span>
<span class="line-added">2384 //</span>
<span class="line-added">2385 // +-- Base address             A        B                     End</span>
<span class="line-added">2386 // |                            |        |                      |</span>
<span class="line-added">2387 // v                            v        v                      v</span>
<span class="line-added">2388 // +-------------+--------------+        +----------------------+</span>
<span class="line-added">2389 // | static arc  | [dyn. arch]  | [gap]  | compr. class space   |</span>
<span class="line-added">2390 // +-------------+--------------+        +----------------------+</span>
<span class="line-added">2391 //</span>
<span class="line-added">2392 // (The gap may result from different alignment requirements between metaspace</span>
<span class="line-added">2393 //  and CDS)</span>
<span class="line-added">2394 //</span>
<span class="line-added">2395 // If UseCompressedClassPointers is disabled, only one address space will be</span>
<span class="line-added">2396 //  reserved:</span>
<span class="line-added">2397 //</span>
<span class="line-added">2398 // +-- Base address             End</span>
<span class="line-added">2399 // |                            |</span>
<span class="line-added">2400 // v                            v</span>
<span class="line-added">2401 // +-------------+--------------+</span>
<span class="line-added">2402 // | static arc  | [dyn. arch]  |</span>
<span class="line-added">2403 // +-------------+--------------+</span>
<span class="line-added">2404 //</span>
<span class="line-added">2405 // Base address: If use_archive_base_addr address is true, the Base address is</span>
<span class="line-added">2406 //  determined by the address stored in the static archive. If</span>
<span class="line-added">2407 //  use_archive_base_addr address is false, this base address is determined</span>
<span class="line-added">2408 //  by the platform.</span>
<span class="line-added">2409 //</span>
<span class="line-added">2410 // If UseCompressedClassPointers=1, the range encompassing both spaces will be</span>
<span class="line-added">2411 //  suitable to en/decode narrow Klass pointers: the base will be valid for</span>
<span class="line-added">2412 //  encoding, the range [Base, End) not surpass KlassEncodingMetaspaceMax.</span>
<span class="line-added">2413 //</span>
<span class="line-added">2414 // Return:</span>
<span class="line-added">2415 //</span>
<span class="line-added">2416 // - On success:</span>
<span class="line-added">2417 //    - archive_space_rs will be reserved and large enough to host static and</span>
<span class="line-added">2418 //      if needed dynamic archive: [Base, A).</span>
<span class="line-added">2419 //      archive_space_rs.base and size will be aligned to CDS reserve</span>
<span class="line-added">2420 //      granularity.</span>
<span class="line-added">2421 //    - class_space_rs: If UseCompressedClassPointers=1, class_space_rs will</span>
<span class="line-added">2422 //      be reserved. Its start address will be aligned to metaspace reserve</span>
<span class="line-added">2423 //      alignment, which may differ from CDS alignment. It will follow the cds</span>
<span class="line-added">2424 //      archive space, close enough such that narrow class pointer encoding</span>
<span class="line-added">2425 //      covers both spaces.</span>
<span class="line-added">2426 //      If UseCompressedClassPointers=0, class_space_rs remains unreserved.</span>
<span class="line-added">2427 // - On error: NULL is returned and the spaces remain unreserved.</span>
2428 char* MetaspaceShared::reserve_address_space_for_archives(FileMapInfo* static_mapinfo,
2429                                                           FileMapInfo* dynamic_mapinfo,
<span class="line-modified">2430                                                           bool use_archive_base_addr,</span>

2431                                                           ReservedSpace&amp; archive_space_rs,
2432                                                           ReservedSpace&amp; class_space_rs) {
<span class="line-modified">2433 </span>
<span class="line-modified">2434   address const base_address = (address) (use_archive_base_addr ? static_mapinfo-&gt;requested_base_address() : NULL);</span>






2435   const size_t archive_space_alignment = MetaspaceShared::reserved_space_alignment();
2436 
2437   // Size and requested location of the archive_space_rs (for both static and dynamic archives)
<span class="line-modified">2438   assert(static_mapinfo-&gt;mapping_base_offset() == 0, &quot;Must be&quot;);</span>
<span class="line-modified">2439   size_t archive_end_offset  = (dynamic_mapinfo == NULL) ? static_mapinfo-&gt;mapping_end_offset() : dynamic_mapinfo-&gt;mapping_end_offset();</span>
<span class="line-modified">2440   size_t archive_space_size = align_up(archive_end_offset, archive_space_alignment);</span>
<span class="line-modified">2441 </span>
<span class="line-modified">2442   // If a base address is given, it must have valid alignment and be suitable as encoding base.</span>
<span class="line-modified">2443   if (base_address != NULL) {</span>
<span class="line-modified">2444     assert(is_aligned(base_address, archive_space_alignment),</span>
<span class="line-modified">2445            &quot;Archive base address invalid: &quot; PTR_FORMAT &quot;.&quot;, p2i(base_address));</span>
<span class="line-modified">2446     if (Metaspace::using_class_space()) {</span>
<span class="line-modified">2447       assert(CompressedKlassPointers::is_valid_base(base_address),</span>
<span class="line-modified">2448              &quot;Archive base address invalid: &quot; PTR_FORMAT &quot;.&quot;, p2i(base_address));</span>





































2449     }
<span class="line-added">2450   }</span>
<span class="line-added">2451 </span>
<span class="line-added">2452   if (!Metaspace::using_class_space()) {</span>
<span class="line-added">2453     // Get the simple case out of the way first:</span>
<span class="line-added">2454     // no compressed class space, simple allocation.</span>
<span class="line-added">2455     archive_space_rs = ReservedSpace(archive_space_size, archive_space_alignment,</span>
<span class="line-added">2456                                      false /* bool large */, (char*)base_address);</span>
2457     if (archive_space_rs.is_reserved()) {
<span class="line-added">2458       assert(base_address == NULL ||</span>
<span class="line-added">2459              (address)archive_space_rs.base() == base_address, &quot;Sanity&quot;);</span>
2460       return archive_space_rs.base();
<span class="line-modified">2461     }</span>
<span class="line-modified">2462     return NULL;</span>
<span class="line-added">2463   }</span>
<span class="line-added">2464 </span>
<span class="line-added">2465 #ifdef _LP64</span>
<span class="line-added">2466 </span>
<span class="line-added">2467   // Complex case: two spaces adjacent to each other, both to be addressable</span>
<span class="line-added">2468   //  with narrow class pointers.</span>
<span class="line-added">2469   // We reserve the whole range spanning both spaces, then split that range up.</span>
<span class="line-added">2470 </span>
<span class="line-added">2471   const size_t class_space_alignment = Metaspace::reserve_alignment();</span>
<span class="line-added">2472 </span>
<span class="line-added">2473   // To simplify matters, lets assume that metaspace alignment will always be</span>
<span class="line-added">2474   //  equal or a multiple of archive alignment.</span>
<span class="line-added">2475   assert(is_power_of_2(class_space_alignment) &amp;&amp;</span>
<span class="line-added">2476                        is_power_of_2(archive_space_alignment) &amp;&amp;</span>
<span class="line-added">2477                        class_space_alignment &gt;= archive_space_alignment,</span>
<span class="line-added">2478                        &quot;Sanity&quot;);</span>
<span class="line-added">2479 </span>
<span class="line-added">2480   const size_t class_space_size = CompressedClassSpaceSize;</span>
<span class="line-added">2481   assert(CompressedClassSpaceSize &gt; 0 &amp;&amp;</span>
<span class="line-added">2482          is_aligned(CompressedClassSpaceSize, class_space_alignment),</span>
<span class="line-added">2483          &quot;CompressedClassSpaceSize malformed: &quot;</span>
<span class="line-added">2484          SIZE_FORMAT, CompressedClassSpaceSize);</span>
<span class="line-added">2485 </span>
<span class="line-added">2486   const size_t ccs_begin_offset = align_up(archive_space_size,</span>
<span class="line-added">2487                                            class_space_alignment);</span>
<span class="line-added">2488   const size_t gap_size = ccs_begin_offset - archive_space_size;</span>
<span class="line-added">2489 </span>
<span class="line-added">2490   const size_t total_range_size =</span>
<span class="line-added">2491       align_up(archive_space_size + gap_size + class_space_size,</span>
<span class="line-added">2492                os::vm_allocation_granularity());</span>
<span class="line-added">2493 </span>
<span class="line-added">2494   ReservedSpace total_rs;</span>
<span class="line-added">2495   if (base_address != NULL) {</span>
<span class="line-added">2496     // Reserve at the given archive base address, or not at all.</span>
<span class="line-added">2497     total_rs = ReservedSpace(total_range_size, archive_space_alignment,</span>
<span class="line-added">2498                              false /* bool large */, (char*) base_address);</span>
<span class="line-added">2499   } else {</span>
<span class="line-added">2500     // Reserve at any address, but leave it up to the platform to choose a good one.</span>
2501     total_rs = Metaspace::reserve_address_space_for_compressed_classes(total_range_size);
2502   }
<span class="line-added">2503 </span>
<span class="line-added">2504   if (!total_rs.is_reserved()) {</span>
<span class="line-added">2505     return NULL;</span>
<span class="line-added">2506   }</span>
<span class="line-added">2507 </span>
<span class="line-added">2508   // Paranoid checks:</span>
<span class="line-added">2509   assert(base_address == NULL || (address)total_rs.base() == base_address,</span>
<span class="line-added">2510          &quot;Sanity (&quot; PTR_FORMAT &quot; vs &quot; PTR_FORMAT &quot;)&quot;, p2i(base_address), p2i(total_rs.base()));</span>
<span class="line-added">2511   assert(is_aligned(total_rs.base(), archive_space_alignment), &quot;Sanity&quot;);</span>
<span class="line-added">2512   assert(total_rs.size() == total_range_size, &quot;Sanity&quot;);</span>
<span class="line-added">2513   assert(CompressedKlassPointers::is_valid_base((address)total_rs.base()), &quot;Sanity&quot;);</span>
<span class="line-added">2514 </span>
<span class="line-added">2515   // Now split up the space into ccs and cds archive. For simplicity, just leave</span>
<span class="line-added">2516   //  the gap reserved at the end of the archive space.</span>
<span class="line-added">2517   archive_space_rs = total_rs.first_part(ccs_begin_offset,</span>
<span class="line-added">2518                                          (size_t)os::vm_allocation_granularity(),</span>
<span class="line-added">2519                                          /*split=*/true);</span>
<span class="line-added">2520   class_space_rs = total_rs.last_part(ccs_begin_offset);</span>
<span class="line-added">2521 </span>
<span class="line-added">2522   assert(is_aligned(archive_space_rs.base(), archive_space_alignment), &quot;Sanity&quot;);</span>
<span class="line-added">2523   assert(is_aligned(archive_space_rs.size(), archive_space_alignment), &quot;Sanity&quot;);</span>
<span class="line-added">2524   assert(is_aligned(class_space_rs.base(), class_space_alignment), &quot;Sanity&quot;);</span>
<span class="line-added">2525   assert(is_aligned(class_space_rs.size(), class_space_alignment), &quot;Sanity&quot;);</span>
<span class="line-added">2526 </span>
<span class="line-added">2527   return archive_space_rs.base();</span>
<span class="line-added">2528 </span>
<span class="line-added">2529 #else</span>
<span class="line-added">2530   ShouldNotReachHere();</span>
<span class="line-added">2531   return NULL;</span>
<span class="line-added">2532 #endif</span>
<span class="line-added">2533 </span>
2534 }
2535 
<span class="line-modified">2536 void MetaspaceShared::release_reserved_spaces(ReservedSpace&amp; archive_space_rs,</span>

2537                                               ReservedSpace&amp; class_space_rs) {
<span class="line-modified">2538   if (archive_space_rs.is_reserved()) {</span>
<span class="line-modified">2539     log_debug(cds)(&quot;Released shared space (archive) &quot; INTPTR_FORMAT, p2i(archive_space_rs.base()));</span>
<span class="line-modified">2540     archive_space_rs.release();</span>
<span class="line-modified">2541   }</span>
<span class="line-modified">2542   if (class_space_rs.is_reserved()) {</span>
<span class="line-modified">2543     log_debug(cds)(&quot;Released shared space (classes) &quot; INTPTR_FORMAT, p2i(class_space_rs.base()));</span>
<span class="line-modified">2544     class_space_rs.release();</span>







2545   }
2546 }
2547 
2548 static int archive_regions[]  = {MetaspaceShared::mc,
2549                                  MetaspaceShared::rw,
2550                                  MetaspaceShared::ro};
2551 static int archive_regions_count  = 3;
2552 
2553 MapArchiveResult MetaspaceShared::map_archive(FileMapInfo* mapinfo, char* mapped_base_address, ReservedSpace rs) {
2554   assert(UseSharedSpaces, &quot;must be runtime&quot;);
2555   if (mapinfo == NULL) {
2556     return MAP_ARCHIVE_SUCCESS; // The dynamic archive has not been specified. No error has happened -- trivially succeeded.
2557   }
2558 
2559   mapinfo-&gt;set_is_mapped(false);
2560 
2561   if (mapinfo-&gt;alignment() != (size_t)os::vm_allocation_granularity()) {
2562     log_error(cds)(&quot;Unable to map CDS archive -- os::vm_allocation_granularity() expected: &quot; SIZE_FORMAT
2563                    &quot; actual: %d&quot;, mapinfo-&gt;alignment(), os::vm_allocation_granularity());
2564     return MAP_ARCHIVE_OTHER_FAILURE;
</pre>
<hr />
<pre>
2669 }
2670 
2671 void MetaspaceShared::report_out_of_space(const char* name, size_t needed_bytes) {
2672   // This is highly unlikely to happen on 64-bits because we have reserved a 4GB space.
2673   // On 32-bit we reserve only 256MB so you could run out of space with 100,000 classes
2674   // or so.
2675   _mc_region.print_out_of_space_msg(name, needed_bytes);
2676   _rw_region.print_out_of_space_msg(name, needed_bytes);
2677   _ro_region.print_out_of_space_msg(name, needed_bytes);
2678 
2679   vm_exit_during_initialization(err_msg(&quot;Unable to allocate from &#39;%s&#39; region&quot;, name),
2680                                 &quot;Please reduce the number of shared classes.&quot;);
2681 }
2682 
2683 // This is used to relocate the pointers so that the archive can be mapped at
2684 // Arguments::default_SharedBaseAddress() without runtime relocation.
2685 intx MetaspaceShared::final_delta() {
2686   return intx(Arguments::default_SharedBaseAddress())  // We want the archive to be mapped to here at runtime
2687        - intx(SharedBaseAddress);                      // .. but the archive is mapped at here at dump time
2688 }
<span class="line-added">2689 </span>
<span class="line-added">2690 void MetaspaceShared::print_on(outputStream* st) {</span>
<span class="line-added">2691   if (UseSharedSpaces || DumpSharedSpaces) {</span>
<span class="line-added">2692     st-&gt;print(&quot;CDS archive(s) mapped at: &quot;);</span>
<span class="line-added">2693     address base;</span>
<span class="line-added">2694     address top;</span>
<span class="line-added">2695     if (UseSharedSpaces) { // Runtime</span>
<span class="line-added">2696       base = (address)MetaspaceObj::shared_metaspace_base();</span>
<span class="line-added">2697       address static_top = (address)_shared_metaspace_static_top;</span>
<span class="line-added">2698       top = (address)MetaspaceObj::shared_metaspace_top();</span>
<span class="line-added">2699       st-&gt;print(&quot;[&quot; PTR_FORMAT &quot;-&quot; PTR_FORMAT &quot;-&quot; PTR_FORMAT &quot;), &quot;, p2i(base), p2i(static_top), p2i(top));</span>
<span class="line-added">2700     } else if (DumpSharedSpaces) { // Dump Time</span>
<span class="line-added">2701       base = (address)_shared_rs.base();</span>
<span class="line-added">2702       top = (address)_shared_rs.end();</span>
<span class="line-added">2703       st-&gt;print(&quot;[&quot; PTR_FORMAT &quot;-&quot; PTR_FORMAT &quot;), &quot;, p2i(base), p2i(top));</span>
<span class="line-added">2704     }</span>
<span class="line-added">2705     st-&gt;print(&quot;size &quot; SIZE_FORMAT &quot;, &quot;, top - base);</span>
<span class="line-added">2706     st-&gt;print(&quot;SharedBaseAddress: &quot; PTR_FORMAT &quot;, ArchiveRelocationMode: %d.&quot;, SharedBaseAddress, (int)ArchiveRelocationMode);</span>
<span class="line-added">2707   } else {</span>
<span class="line-added">2708     st-&gt;print(&quot;CDS disabled.&quot;);</span>
<span class="line-added">2709   }</span>
<span class="line-added">2710   st-&gt;cr();</span>
<span class="line-added">2711 }</span>
<span class="line-added">2712 </span>
<span class="line-added">2713 </span>
<span class="line-added">2714 </span>
<span class="line-added">2715 </span>
<span class="line-added">2716 </span>
</pre>
</td>
</tr>
</table>
<center><a href="metaspaceClosure.hpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="universe.cpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>