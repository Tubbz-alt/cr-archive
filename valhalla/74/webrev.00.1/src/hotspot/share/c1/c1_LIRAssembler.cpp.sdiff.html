<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/share/c1/c1_LIRAssembler.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="c1_LIR.hpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="c1_LinearScan.cpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/share/c1/c1_LIRAssembler.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
 14  *
 15  * You should have received a copy of the GNU General Public License version
 16  * 2 along with this work; if not, write to the Free Software Foundation,
 17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 18  *
 19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
 20  * or visit www.oracle.com if you need additional information or have any
 21  * questions.
 22  *
 23  */
 24 
 25 #include &quot;precompiled.hpp&quot;
 26 #include &quot;asm/assembler.inline.hpp&quot;
 27 #include &quot;c1/c1_Compilation.hpp&quot;
 28 #include &quot;c1/c1_Instruction.hpp&quot;
 29 #include &quot;c1/c1_InstructionPrinter.hpp&quot;
 30 #include &quot;c1/c1_LIRAssembler.hpp&quot;
 31 #include &quot;c1/c1_MacroAssembler.hpp&quot;
 32 #include &quot;c1/c1_ValueStack.hpp&quot;
 33 #include &quot;ci/ciInstance.hpp&quot;

 34 #include &quot;gc/shared/barrierSet.hpp&quot;
 35 #include &quot;runtime/os.hpp&quot;

 36 
 37 void LIR_Assembler::patching_epilog(PatchingStub* patch, LIR_PatchCode patch_code, Register obj, CodeEmitInfo* info) {
 38   // We must have enough patching space so that call can be inserted.
 39   // We cannot use fat nops here, since the concurrent code rewrite may transiently
 40   // create the illegal instruction sequence.
 41   while ((intx) _masm-&gt;pc() - (intx) patch-&gt;pc_start() &lt; NativeGeneralJump::instruction_size) {
 42     _masm-&gt;nop();
 43   }
 44   patch-&gt;install(_masm, patch_code, obj, info);
 45   append_code_stub(patch);
 46 
 47 #ifdef ASSERT
 48   Bytecodes::Code code = info-&gt;scope()-&gt;method()-&gt;java_code_at_bci(info-&gt;stack()-&gt;bci());
 49   if (patch-&gt;id() == PatchingStub::access_field_id) {
 50     switch (code) {
 51       case Bytecodes::_putstatic:
 52       case Bytecodes::_getstatic:
 53       case Bytecodes::_putfield:
 54       case Bytecodes::_getfield:
 55         break;
 56       default:
 57         ShouldNotReachHere();
 58     }
 59   } else if (patch-&gt;id() == PatchingStub::load_klass_id) {
 60     switch (code) {
 61       case Bytecodes::_new:

 62       case Bytecodes::_anewarray:
 63       case Bytecodes::_multianewarray:
 64       case Bytecodes::_instanceof:
 65       case Bytecodes::_checkcast:
 66         break;
 67       default:
 68         ShouldNotReachHere();
 69     }
 70   } else if (patch-&gt;id() == PatchingStub::load_mirror_id) {
 71     switch (code) {
 72       case Bytecodes::_putstatic:
 73       case Bytecodes::_getstatic:
 74       case Bytecodes::_ldc:
 75       case Bytecodes::_ldc_w:
 76         break;
 77       default:
 78         ShouldNotReachHere();
 79     }
 80   } else if (patch-&gt;id() == PatchingStub::load_appendix_id) {
 81     Bytecodes::Code bc_raw = info-&gt;scope()-&gt;method()-&gt;raw_code_at_bci(info-&gt;stack()-&gt;bci());
</pre>
<hr />
<pre>
 98 //---------------------------------------------------------------
 99 
100 
101 LIR_Assembler::LIR_Assembler(Compilation* c):
102    _masm(c-&gt;masm())
103  , _bs(BarrierSet::barrier_set())
104  , _compilation(c)
105  , _frame_map(c-&gt;frame_map())
106  , _current_block(NULL)
107  , _pending_non_safepoint(NULL)
108  , _pending_non_safepoint_offset(0)
109 {
110   _slow_case_stubs = new CodeStubList();
111 }
112 
113 
114 LIR_Assembler::~LIR_Assembler() {
115   // The unwind handler label may be unnbound if this destructor is invoked because of a bail-out.
116   // Reset it here to avoid an assertion.
117   _unwind_handler_entry.reset();

118 }
119 
120 
121 void LIR_Assembler::check_codespace() {
122   CodeSection* cs = _masm-&gt;code_section();
123   if (cs-&gt;remaining() &lt; (int)(NOT_LP64(1*K)LP64_ONLY(2*K))) {
124     BAILOUT(&quot;CodeBuffer overflow&quot;);
125   }
126 }
127 
128 
129 void LIR_Assembler::append_code_stub(CodeStub* stub) {
130   _slow_case_stubs-&gt;append(stub);
131 }
132 
133 void LIR_Assembler::emit_stubs(CodeStubList* stub_list) {
134   for (int m = 0; m &lt; stub_list-&gt;length(); m++) {
135     CodeStub* s = stub_list-&gt;at(m);
136 
137     check_codespace();
</pre>
<hr />
<pre>
319       tty-&gt;print_cr(&quot;label of block B%d is not bound&quot;, _branch_target_blocks.at(i)-&gt;block_id());
320       assert(false, &quot;unbound label&quot;);
321     }
322   }
323 }
324 #endif
325 
326 //----------------------------------debug info--------------------------------
327 
328 
329 void LIR_Assembler::add_debug_info_for_branch(CodeEmitInfo* info) {
330   int pc_offset = code_offset();
331   flush_debug_info(pc_offset);
332   info-&gt;record_debug_info(compilation()-&gt;debug_info_recorder(), pc_offset);
333   if (info-&gt;exception_handlers() != NULL) {
334     compilation()-&gt;add_exception_handlers_for_pco(pc_offset, info-&gt;exception_handlers());
335   }
336 }
337 
338 
<span class="line-modified">339 void LIR_Assembler::add_call_info(int pc_offset, CodeEmitInfo* cinfo) {</span>
340   flush_debug_info(pc_offset);
<span class="line-modified">341   cinfo-&gt;record_debug_info(compilation()-&gt;debug_info_recorder(), pc_offset);</span>
342   if (cinfo-&gt;exception_handlers() != NULL) {
343     compilation()-&gt;add_exception_handlers_for_pco(pc_offset, cinfo-&gt;exception_handlers());
344   }
345 }
346 
347 static ValueStack* debug_info(Instruction* ins) {
348   StateSplit* ss = ins-&gt;as_StateSplit();
349   if (ss != NULL) return ss-&gt;state();
350   return ins-&gt;state_before();
351 }
352 
353 void LIR_Assembler::process_debug_info(LIR_Op* op) {
354   Instruction* src = op-&gt;source();
355   if (src == NULL)  return;
356   int pc_offset = code_offset();
357   if (_pending_non_safepoint == src) {
358     _pending_non_safepoint_offset = pc_offset;
359     return;
360   }
361   ValueStack* vstack = debug_info(src);
</pre>
<hr />
<pre>
464   case lir_optvirtual_call:
465     call(op, relocInfo::opt_virtual_call_type);
466     break;
467   case lir_icvirtual_call:
468     ic_call(op);
469     break;
470   case lir_virtual_call:
471     vtable_call(op);
472     break;
473   default:
474     fatal(&quot;unexpected op code: %s&quot;, op-&gt;name());
475     break;
476   }
477 
478   // JSR 292
479   // Record if this method has MethodHandle invokes.
480   if (op-&gt;is_method_handle_invoke()) {
481     compilation()-&gt;set_has_method_handle_invokes(true);
482   }
483 






484 #if defined(IA32) &amp;&amp; defined(TIERED)
485   // C2 leave fpu stack dirty clean it
486   if (UseSSE &lt; 2) {
487     int i;
488     for ( i = 1; i &lt;= 7 ; i++ ) {
489       ffree(i);
490     }
491     if (!op-&gt;result_opr()-&gt;is_float_kind()) {
492       ffree(0);
493     }
494   }
495 #endif // X86 &amp;&amp; TIERED
496 }
497 
498 
499 void LIR_Assembler::emit_opLabel(LIR_OpLabel* op) {
500   _masm-&gt;bind (*(op-&gt;label()));
501 }
502 
503 
</pre>
<hr />
<pre>
565       } else {
566         Unimplemented();
567       }
568       break;
569     }
570 
571     case lir_monaddr:
572       monitor_address(op-&gt;in_opr()-&gt;as_constant_ptr()-&gt;as_jint(), op-&gt;result_opr());
573       break;
574 
575     case lir_unwind:
576       unwind_op(op-&gt;in_opr());
577       break;
578 
579     default:
580       Unimplemented();
581       break;
582   }
583 }
584 







































































































































585 
586 void LIR_Assembler::emit_op0(LIR_Op0* op) {
587   switch (op-&gt;code()) {
588     case lir_nop:
589       assert(op-&gt;info() == NULL, &quot;not supported&quot;);
590       _masm-&gt;nop();
591       break;
592 
593     case lir_label:
594       Unimplemented();
595       break;
596 
597     case lir_std_entry:
<span class="line-modified">598       // init offsets</span>
<span class="line-removed">599       offsets()-&gt;set_value(CodeOffsets::OSR_Entry, _masm-&gt;offset());</span>
<span class="line-removed">600       _masm-&gt;align(CodeEntryAlignment);</span>
<span class="line-removed">601       if (needs_icache(compilation()-&gt;method())) {</span>
<span class="line-removed">602         check_icache();</span>
<span class="line-removed">603       }</span>
<span class="line-removed">604       offsets()-&gt;set_value(CodeOffsets::Verified_Entry, _masm-&gt;offset());</span>
<span class="line-removed">605       _masm-&gt;verified_entry();</span>
<span class="line-removed">606       if (needs_clinit_barrier_on_entry(compilation()-&gt;method())) {</span>
<span class="line-removed">607         clinit_barrier(compilation()-&gt;method());</span>
<span class="line-removed">608       }</span>
<span class="line-removed">609       build_frame();</span>
<span class="line-removed">610       offsets()-&gt;set_value(CodeOffsets::Frame_Complete, _masm-&gt;offset());</span>
611       break;
612 
613     case lir_osr_entry:
614       offsets()-&gt;set_value(CodeOffsets::OSR_Entry, _masm-&gt;offset());
615       osr_entry();
616       break;
617 
618 #ifdef IA32
619     case lir_fpop_raw:
620       fpop();
621       break;
622 #endif // IA32
623 
624     case lir_breakpoint:
625       breakpoint();
626       break;
627 
628     case lir_membar:
629       membar();
630       break;
</pre>
<hr />
<pre>
644     case lir_membar_storestore:
645       membar_storestore();
646       break;
647 
648     case lir_membar_loadstore:
649       membar_loadstore();
650       break;
651 
652     case lir_membar_storeload:
653       membar_storeload();
654       break;
655 
656     case lir_get_thread:
657       get_thread(op-&gt;result_opr());
658       break;
659 
660     case lir_on_spin_wait:
661       on_spin_wait();
662       break;
663 




664     default:
665       ShouldNotReachHere();
666       break;
667   }
668 }
669 
670 
671 void LIR_Assembler::emit_op2(LIR_Op2* op) {
672   switch (op-&gt;code()) {
673     case lir_cmp:
674       if (op-&gt;info() != NULL) {
675         assert(op-&gt;in_opr1()-&gt;is_address() || op-&gt;in_opr2()-&gt;is_address(),
676                &quot;shouldn&#39;t be codeemitinfo for non-address operands&quot;);
677         add_debug_info_for_null_check_here(op-&gt;info()); // exception possible
678       }
679       comp_op(op-&gt;condition(), op-&gt;in_opr1(), op-&gt;in_opr2(), op);
680       break;
681 
682     case lir_cmp_l2i:
683     case lir_cmp_fd2i:
</pre>
<hr />
<pre>
737         op-&gt;result_opr());
738       break;
739 
740     case lir_throw:
741       throw_op(op-&gt;in_opr1(), op-&gt;in_opr2(), op-&gt;info());
742       break;
743 
744     case lir_xadd:
745     case lir_xchg:
746       atomic_op(op-&gt;code(), op-&gt;in_opr1(), op-&gt;in_opr2(), op-&gt;result_opr(), op-&gt;tmp1_opr());
747       break;
748 
749     default:
750       Unimplemented();
751       break;
752   }
753 }
754 
755 
756 void LIR_Assembler::build_frame() {
<span class="line-modified">757   _masm-&gt;build_frame(initial_frame_size_in_bytes(), bang_size_in_bytes());</span>

758 }
759 
760 
761 void LIR_Assembler::roundfp_op(LIR_Opr src, LIR_Opr tmp, LIR_Opr dest, bool pop_fpu_stack) {
762   assert(strict_fp_requires_explicit_rounding, &quot;not required&quot;);
763   assert((src-&gt;is_single_fpu() &amp;&amp; dest-&gt;is_single_stack()) ||
764          (src-&gt;is_double_fpu() &amp;&amp; dest-&gt;is_double_stack()),
765          &quot;round_fp: rounds register -&gt; stack location&quot;);
766 
767   reg2stack (src, dest, src-&gt;type(), pop_fpu_stack);
768 }
769 
770 
771 void LIR_Assembler::move_op(LIR_Opr src, LIR_Opr dest, BasicType type, LIR_PatchCode patch_code, CodeEmitInfo* info, bool pop_fpu_stack, bool unaligned, bool wide) {
772   if (src-&gt;is_register()) {
773     if (dest-&gt;is_register()) {
774       assert(patch_code == lir_patch_none &amp;&amp; info == NULL, &quot;no patching and info allowed here&quot;);
775       reg2reg(src,  dest);
776     } else if (dest-&gt;is_stack()) {
777       assert(patch_code == lir_patch_none &amp;&amp; info == NULL, &quot;no patching and info allowed here&quot;);
</pre>
</td>
<td>
<hr />
<pre>
 14  *
 15  * You should have received a copy of the GNU General Public License version
 16  * 2 along with this work; if not, write to the Free Software Foundation,
 17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 18  *
 19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
 20  * or visit www.oracle.com if you need additional information or have any
 21  * questions.
 22  *
 23  */
 24 
 25 #include &quot;precompiled.hpp&quot;
 26 #include &quot;asm/assembler.inline.hpp&quot;
 27 #include &quot;c1/c1_Compilation.hpp&quot;
 28 #include &quot;c1/c1_Instruction.hpp&quot;
 29 #include &quot;c1/c1_InstructionPrinter.hpp&quot;
 30 #include &quot;c1/c1_LIRAssembler.hpp&quot;
 31 #include &quot;c1/c1_MacroAssembler.hpp&quot;
 32 #include &quot;c1/c1_ValueStack.hpp&quot;
 33 #include &quot;ci/ciInstance.hpp&quot;
<span class="line-added"> 34 #include &quot;ci/ciValueKlass.hpp&quot;</span>
 35 #include &quot;gc/shared/barrierSet.hpp&quot;
 36 #include &quot;runtime/os.hpp&quot;
<span class="line-added"> 37 #include &quot;runtime/sharedRuntime.hpp&quot;</span>
 38 
 39 void LIR_Assembler::patching_epilog(PatchingStub* patch, LIR_PatchCode patch_code, Register obj, CodeEmitInfo* info) {
 40   // We must have enough patching space so that call can be inserted.
 41   // We cannot use fat nops here, since the concurrent code rewrite may transiently
 42   // create the illegal instruction sequence.
 43   while ((intx) _masm-&gt;pc() - (intx) patch-&gt;pc_start() &lt; NativeGeneralJump::instruction_size) {
 44     _masm-&gt;nop();
 45   }
 46   patch-&gt;install(_masm, patch_code, obj, info);
 47   append_code_stub(patch);
 48 
 49 #ifdef ASSERT
 50   Bytecodes::Code code = info-&gt;scope()-&gt;method()-&gt;java_code_at_bci(info-&gt;stack()-&gt;bci());
 51   if (patch-&gt;id() == PatchingStub::access_field_id) {
 52     switch (code) {
 53       case Bytecodes::_putstatic:
 54       case Bytecodes::_getstatic:
 55       case Bytecodes::_putfield:
 56       case Bytecodes::_getfield:
 57         break;
 58       default:
 59         ShouldNotReachHere();
 60     }
 61   } else if (patch-&gt;id() == PatchingStub::load_klass_id) {
 62     switch (code) {
 63       case Bytecodes::_new:
<span class="line-added"> 64       case Bytecodes::_defaultvalue:</span>
 65       case Bytecodes::_anewarray:
 66       case Bytecodes::_multianewarray:
 67       case Bytecodes::_instanceof:
 68       case Bytecodes::_checkcast:
 69         break;
 70       default:
 71         ShouldNotReachHere();
 72     }
 73   } else if (patch-&gt;id() == PatchingStub::load_mirror_id) {
 74     switch (code) {
 75       case Bytecodes::_putstatic:
 76       case Bytecodes::_getstatic:
 77       case Bytecodes::_ldc:
 78       case Bytecodes::_ldc_w:
 79         break;
 80       default:
 81         ShouldNotReachHere();
 82     }
 83   } else if (patch-&gt;id() == PatchingStub::load_appendix_id) {
 84     Bytecodes::Code bc_raw = info-&gt;scope()-&gt;method()-&gt;raw_code_at_bci(info-&gt;stack()-&gt;bci());
</pre>
<hr />
<pre>
101 //---------------------------------------------------------------
102 
103 
104 LIR_Assembler::LIR_Assembler(Compilation* c):
105    _masm(c-&gt;masm())
106  , _bs(BarrierSet::barrier_set())
107  , _compilation(c)
108  , _frame_map(c-&gt;frame_map())
109  , _current_block(NULL)
110  , _pending_non_safepoint(NULL)
111  , _pending_non_safepoint_offset(0)
112 {
113   _slow_case_stubs = new CodeStubList();
114 }
115 
116 
117 LIR_Assembler::~LIR_Assembler() {
118   // The unwind handler label may be unnbound if this destructor is invoked because of a bail-out.
119   // Reset it here to avoid an assertion.
120   _unwind_handler_entry.reset();
<span class="line-added">121   _verified_value_entry.reset();</span>
122 }
123 
124 
125 void LIR_Assembler::check_codespace() {
126   CodeSection* cs = _masm-&gt;code_section();
127   if (cs-&gt;remaining() &lt; (int)(NOT_LP64(1*K)LP64_ONLY(2*K))) {
128     BAILOUT(&quot;CodeBuffer overflow&quot;);
129   }
130 }
131 
132 
133 void LIR_Assembler::append_code_stub(CodeStub* stub) {
134   _slow_case_stubs-&gt;append(stub);
135 }
136 
137 void LIR_Assembler::emit_stubs(CodeStubList* stub_list) {
138   for (int m = 0; m &lt; stub_list-&gt;length(); m++) {
139     CodeStub* s = stub_list-&gt;at(m);
140 
141     check_codespace();
</pre>
<hr />
<pre>
323       tty-&gt;print_cr(&quot;label of block B%d is not bound&quot;, _branch_target_blocks.at(i)-&gt;block_id());
324       assert(false, &quot;unbound label&quot;);
325     }
326   }
327 }
328 #endif
329 
330 //----------------------------------debug info--------------------------------
331 
332 
333 void LIR_Assembler::add_debug_info_for_branch(CodeEmitInfo* info) {
334   int pc_offset = code_offset();
335   flush_debug_info(pc_offset);
336   info-&gt;record_debug_info(compilation()-&gt;debug_info_recorder(), pc_offset);
337   if (info-&gt;exception_handlers() != NULL) {
338     compilation()-&gt;add_exception_handlers_for_pco(pc_offset, info-&gt;exception_handlers());
339   }
340 }
341 
342 
<span class="line-modified">343 void LIR_Assembler::add_call_info(int pc_offset, CodeEmitInfo* cinfo, bool maybe_return_as_fields) {</span>
344   flush_debug_info(pc_offset);
<span class="line-modified">345   cinfo-&gt;record_debug_info(compilation()-&gt;debug_info_recorder(), pc_offset, maybe_return_as_fields);</span>
346   if (cinfo-&gt;exception_handlers() != NULL) {
347     compilation()-&gt;add_exception_handlers_for_pco(pc_offset, cinfo-&gt;exception_handlers());
348   }
349 }
350 
351 static ValueStack* debug_info(Instruction* ins) {
352   StateSplit* ss = ins-&gt;as_StateSplit();
353   if (ss != NULL) return ss-&gt;state();
354   return ins-&gt;state_before();
355 }
356 
357 void LIR_Assembler::process_debug_info(LIR_Op* op) {
358   Instruction* src = op-&gt;source();
359   if (src == NULL)  return;
360   int pc_offset = code_offset();
361   if (_pending_non_safepoint == src) {
362     _pending_non_safepoint_offset = pc_offset;
363     return;
364   }
365   ValueStack* vstack = debug_info(src);
</pre>
<hr />
<pre>
468   case lir_optvirtual_call:
469     call(op, relocInfo::opt_virtual_call_type);
470     break;
471   case lir_icvirtual_call:
472     ic_call(op);
473     break;
474   case lir_virtual_call:
475     vtable_call(op);
476     break;
477   default:
478     fatal(&quot;unexpected op code: %s&quot;, op-&gt;name());
479     break;
480   }
481 
482   // JSR 292
483   // Record if this method has MethodHandle invokes.
484   if (op-&gt;is_method_handle_invoke()) {
485     compilation()-&gt;set_has_method_handle_invokes(true);
486   }
487 
<span class="line-added">488   ciValueKlass* vk;</span>
<span class="line-added">489   if (op-&gt;maybe_return_as_fields(&amp;vk)) {</span>
<span class="line-added">490     int offset = store_value_type_fields_to_buf(vk);</span>
<span class="line-added">491     add_call_info(offset, op-&gt;info(), true);</span>
<span class="line-added">492   }</span>
<span class="line-added">493 </span>
494 #if defined(IA32) &amp;&amp; defined(TIERED)
495   // C2 leave fpu stack dirty clean it
496   if (UseSSE &lt; 2) {
497     int i;
498     for ( i = 1; i &lt;= 7 ; i++ ) {
499       ffree(i);
500     }
501     if (!op-&gt;result_opr()-&gt;is_float_kind()) {
502       ffree(0);
503     }
504   }
505 #endif // X86 &amp;&amp; TIERED
506 }
507 
508 
509 void LIR_Assembler::emit_opLabel(LIR_OpLabel* op) {
510   _masm-&gt;bind (*(op-&gt;label()));
511 }
512 
513 
</pre>
<hr />
<pre>
575       } else {
576         Unimplemented();
577       }
578       break;
579     }
580 
581     case lir_monaddr:
582       monitor_address(op-&gt;in_opr()-&gt;as_constant_ptr()-&gt;as_jint(), op-&gt;result_opr());
583       break;
584 
585     case lir_unwind:
586       unwind_op(op-&gt;in_opr());
587       break;
588 
589     default:
590       Unimplemented();
591       break;
592   }
593 }
594 
<span class="line-added">595 void LIR_Assembler::add_scalarized_entry_info(int pc_offset) {</span>
<span class="line-added">596   flush_debug_info(pc_offset);</span>
<span class="line-added">597   DebugInformationRecorder* debug_info = compilation()-&gt;debug_info_recorder();</span>
<span class="line-added">598   // The VEP and VVEP(RO) of a C1-compiled method call buffer_value_args_xxx()</span>
<span class="line-added">599   // before doing any argument shuffling. This call may cause GC. When GC happens,</span>
<span class="line-added">600   // all the parameters are still as passed by the caller, so we just use</span>
<span class="line-added">601   // map-&gt;set_include_argument_oops() inside frame::sender_for_compiled_frame(RegisterMap* map).</span>
<span class="line-added">602   // There&#39;s no need to build a GC map here.</span>
<span class="line-added">603   OopMap* oop_map = new OopMap(0, 0);</span>
<span class="line-added">604   debug_info-&gt;add_safepoint(pc_offset, oop_map);</span>
<span class="line-added">605   DebugToken* locvals = debug_info-&gt;create_scope_values(NULL); // FIXME is this needed (for Java debugging to work properly??)</span>
<span class="line-added">606   DebugToken* expvals = debug_info-&gt;create_scope_values(NULL); // FIXME is this needed (for Java debugging to work properly??)</span>
<span class="line-added">607   DebugToken* monvals = debug_info-&gt;create_monitor_values(NULL); // FIXME: need testing with synchronized method</span>
<span class="line-added">608   bool reexecute = false;</span>
<span class="line-added">609   bool return_oop = false; // This flag will be ignored since it used only for C2 with escape analysis.</span>
<span class="line-added">610   bool rethrow_exception = false;</span>
<span class="line-added">611   bool is_method_handle_invoke = false;</span>
<span class="line-added">612   debug_info-&gt;describe_scope(pc_offset, methodHandle(), method(), 0, reexecute, rethrow_exception, is_method_handle_invoke, return_oop, false, locvals, expvals, monvals);</span>
<span class="line-added">613   debug_info-&gt;end_safepoint(pc_offset);</span>
<span class="line-added">614 }</span>
<span class="line-added">615 </span>
<span class="line-added">616 // The entries points of C1-compiled methods can have the following types:</span>
<span class="line-added">617 // (1) Methods with no value args</span>
<span class="line-added">618 // (2) Methods with value receiver but no value args</span>
<span class="line-added">619 //     VVEP_RO is the same as VVEP</span>
<span class="line-added">620 // (3) Methods with non-value receiver and some value args</span>
<span class="line-added">621 //     VVEP_RO is the same as VEP</span>
<span class="line-added">622 // (4) Methods with value receiver and other value args</span>
<span class="line-added">623 //     Separate VEP, VVEP and VVEP_RO</span>
<span class="line-added">624 //</span>
<span class="line-added">625 // (1)               (2)                 (3)                    (4)</span>
<span class="line-added">626 // UEP/UVEP:         VEP:                UEP:                   UEP:</span>
<span class="line-added">627 //   check_icache      pack receiver       check_icache           check_icache</span>
<span class="line-added">628 // VEP/VVEP/VVEP_RO    jump to VVEP      VEP/VVEP_RO:           VVEP_RO:</span>
<span class="line-added">629 //   body            UEP/UVEP:             pack value args        pack value args (except receiver)</span>
<span class="line-added">630 //                     check_icache        jump to VVEP           jump to VVEP</span>
<span class="line-added">631 //                   VVEP/VVEP_RO        UVEP:                  VEP:</span>
<span class="line-added">632 //                     body                check_icache           pack all value args</span>
<span class="line-added">633 //                                       VVEP:                    jump to VVEP</span>
<span class="line-added">634 //                                         body                 UVEP:</span>
<span class="line-added">635 //                                                                check_icache</span>
<span class="line-added">636 //                                                              VVEP:</span>
<span class="line-added">637 //                                                                body</span>
<span class="line-added">638 void LIR_Assembler::emit_std_entries() {</span>
<span class="line-added">639   offsets()-&gt;set_value(CodeOffsets::OSR_Entry, _masm-&gt;offset());</span>
<span class="line-added">640 </span>
<span class="line-added">641   _masm-&gt;align(CodeEntryAlignment);</span>
<span class="line-added">642   const CompiledEntrySignature* ces = compilation()-&gt;compiled_entry_signature();</span>
<span class="line-added">643   if (ces-&gt;has_scalarized_args()) {</span>
<span class="line-added">644     assert(InlineTypePassFieldsAsArgs &amp;&amp; method()-&gt;get_Method()-&gt;has_scalarized_args(), &quot;must be&quot;);</span>
<span class="line-added">645     CodeOffsets::Entries ro_entry_type = ces-&gt;c1_value_ro_entry_type();</span>
<span class="line-added">646 </span>
<span class="line-added">647     // UEP: check icache and fall-through</span>
<span class="line-added">648     if (ro_entry_type != CodeOffsets::Verified_Value_Entry) {</span>
<span class="line-added">649       offsets()-&gt;set_value(CodeOffsets::Entry, _masm-&gt;offset());</span>
<span class="line-added">650       if (needs_icache(method())) {</span>
<span class="line-added">651         check_icache();</span>
<span class="line-added">652       }</span>
<span class="line-added">653     }</span>
<span class="line-added">654 </span>
<span class="line-added">655     // VVEP_RO: pack all value parameters, except the receiver</span>
<span class="line-added">656     if (ro_entry_type == CodeOffsets::Verified_Value_Entry_RO) {</span>
<span class="line-added">657       emit_std_entry(CodeOffsets::Verified_Value_Entry_RO, ces);</span>
<span class="line-added">658     }</span>
<span class="line-added">659 </span>
<span class="line-added">660     // VEP: pack all value parameters</span>
<span class="line-added">661     _masm-&gt;align(CodeEntryAlignment);</span>
<span class="line-added">662     emit_std_entry(CodeOffsets::Verified_Entry, ces);</span>
<span class="line-added">663 </span>
<span class="line-added">664     // UVEP: check icache and fall-through</span>
<span class="line-added">665     _masm-&gt;align(CodeEntryAlignment);</span>
<span class="line-added">666     offsets()-&gt;set_value(CodeOffsets::Value_Entry, _masm-&gt;offset());</span>
<span class="line-added">667     if (ro_entry_type == CodeOffsets::Verified_Value_Entry) {</span>
<span class="line-added">668       // Special case if we have VVEP == VVEP(RO):</span>
<span class="line-added">669       // this means UVEP (called by C1) == UEP (called by C2).</span>
<span class="line-added">670       offsets()-&gt;set_value(CodeOffsets::Entry, _masm-&gt;offset());</span>
<span class="line-added">671     }</span>
<span class="line-added">672     if (needs_icache(method())) {</span>
<span class="line-added">673       check_icache();</span>
<span class="line-added">674     }</span>
<span class="line-added">675 </span>
<span class="line-added">676     // VVEP: all value parameters are passed as refs - no packing.</span>
<span class="line-added">677     emit_std_entry(CodeOffsets::Verified_Value_Entry, NULL);</span>
<span class="line-added">678 </span>
<span class="line-added">679     if (ro_entry_type != CodeOffsets::Verified_Value_Entry_RO) {</span>
<span class="line-added">680       // The VVEP(RO) is the same as VEP or VVEP</span>
<span class="line-added">681       assert(ro_entry_type == CodeOffsets::Verified_Entry ||</span>
<span class="line-added">682              ro_entry_type == CodeOffsets::Verified_Value_Entry, &quot;must be&quot;);</span>
<span class="line-added">683       offsets()-&gt;set_value(CodeOffsets::Verified_Value_Entry_RO,</span>
<span class="line-added">684                            offsets()-&gt;value(ro_entry_type));</span>
<span class="line-added">685     }</span>
<span class="line-added">686   } else {</span>
<span class="line-added">687     // All 3 entries are the same (no value-type packing)</span>
<span class="line-added">688     offsets()-&gt;set_value(CodeOffsets::Entry, _masm-&gt;offset());</span>
<span class="line-added">689     offsets()-&gt;set_value(CodeOffsets::Value_Entry, _masm-&gt;offset());</span>
<span class="line-added">690     if (needs_icache(method())) {</span>
<span class="line-added">691       check_icache();</span>
<span class="line-added">692     }</span>
<span class="line-added">693     emit_std_entry(CodeOffsets::Verified_Value_Entry, NULL);</span>
<span class="line-added">694     offsets()-&gt;set_value(CodeOffsets::Verified_Entry, offsets()-&gt;value(CodeOffsets::Verified_Value_Entry));</span>
<span class="line-added">695     offsets()-&gt;set_value(CodeOffsets::Verified_Value_Entry_RO, offsets()-&gt;value(CodeOffsets::Verified_Value_Entry));</span>
<span class="line-added">696   }</span>
<span class="line-added">697 }</span>
<span class="line-added">698 </span>
<span class="line-added">699 void LIR_Assembler::emit_std_entry(CodeOffsets::Entries entry, const CompiledEntrySignature* ces) {</span>
<span class="line-added">700   offsets()-&gt;set_value(entry, _masm-&gt;offset());</span>
<span class="line-added">701   _masm-&gt;verified_entry();</span>
<span class="line-added">702   switch (entry) {</span>
<span class="line-added">703   case CodeOffsets::Verified_Entry: {</span>
<span class="line-added">704     if (needs_clinit_barrier_on_entry(method())) {</span>
<span class="line-added">705       clinit_barrier(method());</span>
<span class="line-added">706     }</span>
<span class="line-added">707     int rt_call_offset = _masm-&gt;verified_entry(ces, initial_frame_size_in_bytes(), bang_size_in_bytes(), in_bytes(frame_map()-&gt;sp_offset_for_orig_pc()), _verified_value_entry);</span>
<span class="line-added">708     add_scalarized_entry_info(rt_call_offset);</span>
<span class="line-added">709     break;</span>
<span class="line-added">710   }</span>
<span class="line-added">711   case CodeOffsets::Verified_Value_Entry_RO: {</span>
<span class="line-added">712     assert(!needs_clinit_barrier_on_entry(method()), &quot;can&#39;t be static&quot;);</span>
<span class="line-added">713     int rt_call_offset = _masm-&gt;verified_value_ro_entry(ces, initial_frame_size_in_bytes(), bang_size_in_bytes(), in_bytes(frame_map()-&gt;sp_offset_for_orig_pc()), _verified_value_entry);</span>
<span class="line-added">714     add_scalarized_entry_info(rt_call_offset);</span>
<span class="line-added">715     break;</span>
<span class="line-added">716   }</span>
<span class="line-added">717   case CodeOffsets::Verified_Value_Entry: {</span>
<span class="line-added">718     if (needs_clinit_barrier_on_entry(method())) {</span>
<span class="line-added">719       clinit_barrier(method());</span>
<span class="line-added">720     }</span>
<span class="line-added">721     build_frame();</span>
<span class="line-added">722     offsets()-&gt;set_value(CodeOffsets::Frame_Complete, _masm-&gt;offset());</span>
<span class="line-added">723     break;</span>
<span class="line-added">724   }</span>
<span class="line-added">725   default:</span>
<span class="line-added">726     ShouldNotReachHere();</span>
<span class="line-added">727     break;</span>
<span class="line-added">728   }</span>
<span class="line-added">729 }</span>
730 
731 void LIR_Assembler::emit_op0(LIR_Op0* op) {
732   switch (op-&gt;code()) {
733     case lir_nop:
734       assert(op-&gt;info() == NULL, &quot;not supported&quot;);
735       _masm-&gt;nop();
736       break;
737 
738     case lir_label:
739       Unimplemented();
740       break;
741 
742     case lir_std_entry:
<span class="line-modified">743       emit_std_entries();</span>












744       break;
745 
746     case lir_osr_entry:
747       offsets()-&gt;set_value(CodeOffsets::OSR_Entry, _masm-&gt;offset());
748       osr_entry();
749       break;
750 
751 #ifdef IA32
752     case lir_fpop_raw:
753       fpop();
754       break;
755 #endif // IA32
756 
757     case lir_breakpoint:
758       breakpoint();
759       break;
760 
761     case lir_membar:
762       membar();
763       break;
</pre>
<hr />
<pre>
777     case lir_membar_storestore:
778       membar_storestore();
779       break;
780 
781     case lir_membar_loadstore:
782       membar_loadstore();
783       break;
784 
785     case lir_membar_storeload:
786       membar_storeload();
787       break;
788 
789     case lir_get_thread:
790       get_thread(op-&gt;result_opr());
791       break;
792 
793     case lir_on_spin_wait:
794       on_spin_wait();
795       break;
796 
<span class="line-added">797     case lir_check_orig_pc:</span>
<span class="line-added">798       check_orig_pc();</span>
<span class="line-added">799       break;</span>
<span class="line-added">800 </span>
801     default:
802       ShouldNotReachHere();
803       break;
804   }
805 }
806 
807 
808 void LIR_Assembler::emit_op2(LIR_Op2* op) {
809   switch (op-&gt;code()) {
810     case lir_cmp:
811       if (op-&gt;info() != NULL) {
812         assert(op-&gt;in_opr1()-&gt;is_address() || op-&gt;in_opr2()-&gt;is_address(),
813                &quot;shouldn&#39;t be codeemitinfo for non-address operands&quot;);
814         add_debug_info_for_null_check_here(op-&gt;info()); // exception possible
815       }
816       comp_op(op-&gt;condition(), op-&gt;in_opr1(), op-&gt;in_opr2(), op);
817       break;
818 
819     case lir_cmp_l2i:
820     case lir_cmp_fd2i:
</pre>
<hr />
<pre>
874         op-&gt;result_opr());
875       break;
876 
877     case lir_throw:
878       throw_op(op-&gt;in_opr1(), op-&gt;in_opr2(), op-&gt;info());
879       break;
880 
881     case lir_xadd:
882     case lir_xchg:
883       atomic_op(op-&gt;code(), op-&gt;in_opr1(), op-&gt;in_opr2(), op-&gt;result_opr(), op-&gt;tmp1_opr());
884       break;
885 
886     default:
887       Unimplemented();
888       break;
889   }
890 }
891 
892 
893 void LIR_Assembler::build_frame() {
<span class="line-modified">894   _masm-&gt;build_frame(initial_frame_size_in_bytes(), bang_size_in_bytes(), in_bytes(frame_map()-&gt;sp_offset_for_orig_pc()),</span>
<span class="line-added">895                      needs_stack_repair(), method()-&gt;has_scalarized_args(), &amp;_verified_value_entry);</span>
896 }
897 
898 
899 void LIR_Assembler::roundfp_op(LIR_Opr src, LIR_Opr tmp, LIR_Opr dest, bool pop_fpu_stack) {
900   assert(strict_fp_requires_explicit_rounding, &quot;not required&quot;);
901   assert((src-&gt;is_single_fpu() &amp;&amp; dest-&gt;is_single_stack()) ||
902          (src-&gt;is_double_fpu() &amp;&amp; dest-&gt;is_double_stack()),
903          &quot;round_fp: rounds register -&gt; stack location&quot;);
904 
905   reg2stack (src, dest, src-&gt;type(), pop_fpu_stack);
906 }
907 
908 
909 void LIR_Assembler::move_op(LIR_Opr src, LIR_Opr dest, BasicType type, LIR_PatchCode patch_code, CodeEmitInfo* info, bool pop_fpu_stack, bool unaligned, bool wide) {
910   if (src-&gt;is_register()) {
911     if (dest-&gt;is_register()) {
912       assert(patch_code == lir_patch_none &amp;&amp; info == NULL, &quot;no patching and info allowed here&quot;);
913       reg2reg(src,  dest);
914     } else if (dest-&gt;is_stack()) {
915       assert(patch_code == lir_patch_none &amp;&amp; info == NULL, &quot;no patching and info allowed here&quot;);
</pre>
</td>
</tr>
</table>
<center><a href="c1_LIR.hpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="c1_LinearScan.cpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>