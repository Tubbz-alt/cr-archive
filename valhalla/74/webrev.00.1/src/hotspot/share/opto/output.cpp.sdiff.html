<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/share/opto/output.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="loopopts.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="subnode.hpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/share/opto/output.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
 224 };
 225 
 226 
 227 PhaseOutput::PhaseOutput()
 228   : Phase(Phase::Output),
 229     _code_buffer(&quot;Compile::Fill_buffer&quot;),
 230     _first_block_size(0),
 231     _handler_table(),
 232     _inc_table(),
 233     _oop_map_set(NULL),
 234     _scratch_buffer_blob(NULL),
 235     _scratch_locs_memory(NULL),
 236     _scratch_const_size(-1),
 237     _in_scratch_emit_size(false),
 238     _frame_slots(0),
 239     _code_offsets(),
 240     _node_bundling_limit(0),
 241     _node_bundling_base(NULL),
 242     _orig_pc_slot(0),
 243     _orig_pc_slot_offset_in_bytes(0),


 244     _buf_sizes(),
 245     _block(NULL),
 246     _index(0) {
 247   C-&gt;set_output(this);
 248   if (C-&gt;stub_name() == NULL) {
<span class="line-modified"> 249     _orig_pc_slot = C-&gt;fixed_slots() - (sizeof(address) / VMRegImpl::stack_slot_size);</span>





 250   }
 251 }
 252 
 253 PhaseOutput::~PhaseOutput() {
 254   C-&gt;set_output(NULL);
 255   if (_scratch_buffer_blob != NULL) {
 256     BufferBlob::free(_scratch_buffer_blob);
 257   }
 258 }
 259 
 260 void PhaseOutput::perform_mach_node_analysis() {
 261   // Late barrier analysis must be done after schedule and bundle
 262   // Otherwise liveness based spilling will fail
 263   BarrierSetC2* bs = BarrierSet::barrier_set()-&gt;barrier_set_c2();
 264   bs-&gt;late_barrier_analysis();
 265 
 266   pd_perform_mach_node_analysis();
 267 }
 268 
 269 // Convert Nodes to instruction bits and pass off to the VM
 270 void PhaseOutput::Output() {
 271   // RootNode goes
 272   assert( C-&gt;cfg()-&gt;get_root_block()-&gt;number_of_nodes() == 0, &quot;&quot; );
 273 
 274   // The number of new nodes (mostly MachNop) is proportional to
 275   // the number of java calls and inner loops which are aligned.
 276   if ( C-&gt;check_node_count((NodeLimitFudgeFactor + C-&gt;java_calls()*3 +
 277                             C-&gt;inner_loops()*(OptoLoopAlignment-1)),
 278                            &quot;out of nodes before code generation&quot; ) ) {
 279     return;
 280   }
 281   // Make sure I can find the Start Node
 282   Block *entry = C-&gt;cfg()-&gt;get_block(1);
 283   Block *broot = C-&gt;cfg()-&gt;get_root_block();
 284 
 285   const StartNode *start = entry-&gt;head()-&gt;as_Start();
 286 
 287   // Replace StartNode with prolog
<span class="line-modified"> 288   MachPrologNode *prolog = new MachPrologNode();</span>

 289   entry-&gt;map_node(prolog, 0);
 290   C-&gt;cfg()-&gt;map_node_to_block(prolog, entry);
 291   C-&gt;cfg()-&gt;unmap_node_from_block(start); // start is no longer in any block
 292 
 293   // Virtual methods need an unverified entry point
<span class="line-modified"> 294 </span>
<span class="line-modified"> 295   if( C-&gt;is_osr_compilation() ) {</span>
<span class="line-removed"> 296     if( PoisonOSREntry ) {</span>
 297       // TODO: Should use a ShouldNotReachHereNode...
 298       C-&gt;cfg()-&gt;insert( broot, 0, new MachBreakpointNode() );
 299     }
 300   } else {
<span class="line-modified"> 301     if( C-&gt;method() &amp;&amp; !C-&gt;method()-&gt;flags().is_static() ) {</span>
<span class="line-modified"> 302       // Insert unvalidated entry point</span>
<span class="line-modified"> 303       C-&gt;cfg()-&gt;insert( broot, 0, new MachUEPNode() );</span>











 304     }
<span class="line-removed"> 305 </span>
 306   }
 307 
 308   // Break before main entry point
 309   if ((C-&gt;method() &amp;&amp; C-&gt;directive()-&gt;BreakAtExecuteOption) ||
 310       (OptoBreakpoint &amp;&amp; C-&gt;is_method_compilation())       ||
 311       (OptoBreakpointOSR &amp;&amp; C-&gt;is_osr_compilation())       ||
 312       (OptoBreakpointC2R &amp;&amp; !C-&gt;method())                   ) {
 313     // checking for C-&gt;method() means that OptoBreakpoint does not apply to
 314     // runtime stubs or frame converters
 315     C-&gt;cfg()-&gt;insert( entry, 1, new MachBreakpointNode() );
 316   }
 317 
 318   // Insert epilogs before every return
 319   for (uint i = 0; i &lt; C-&gt;cfg()-&gt;number_of_blocks(); i++) {
 320     Block* block = C-&gt;cfg()-&gt;get_block(i);
 321     if (!block-&gt;is_connector() &amp;&amp; block-&gt;non_connector_successor(0) == C-&gt;cfg()-&gt;get_root_block()) { // Found a program exit point?
 322       Node* m = block-&gt;end();
 323       if (m-&gt;is_Mach() &amp;&amp; m-&gt;as_Mach()-&gt;ideal_Opcode() != Op_Halt) {
 324         MachEpilogNode* epilog = new MachEpilogNode(m-&gt;as_Mach()-&gt;ideal_Opcode() == Op_Return);
 325         block-&gt;add_inst(epilog);
 326         C-&gt;cfg()-&gt;map_node_to_block(epilog, block);
 327       }
 328     }
 329   }
 330 
 331   // Keeper of sizing aspects
 332   _buf_sizes = BufferSizingData();
 333 
 334   // Initialize code buffer
 335   estimate_buffer_size(_buf_sizes._const);
 336   if (C-&gt;failing()) return;
 337 
 338   // Pre-compute the length of blocks and replace
 339   // long branches with short if machine supports it.
 340   // Must be done before ScheduleAndBundle due to SPARC delay slots
 341   uint* blk_starts = NEW_RESOURCE_ARRAY(uint, C-&gt;cfg()-&gt;number_of_blocks() + 1);
 342   blk_starts[0] = 0;
 343   shorten_branches(blk_starts);
 344 

























 345   ScheduleAndBundle();
 346   if (C-&gt;failing()) {
 347     return;
 348   }
 349 
 350   perform_mach_node_analysis();
 351 
 352   // Complete sizing of codebuffer
 353   CodeBuffer* cb = init_buffer();
 354   if (cb == NULL || C-&gt;failing()) {
 355     return;
 356   }
 357 
 358   BuildOopMaps();
 359 
 360   if (C-&gt;failing())  {
 361     return;
 362   }
 363 
 364   fill_buffer(cb, blk_starts);
</pre>
<hr />
<pre>
 482     // Sum all instruction sizes to compute block size
 483     uint last_inst = block-&gt;number_of_nodes();
 484     uint blk_size = 0;
 485     for (uint j = 0; j &lt; last_inst; j++) {
 486       _index = j;
 487       Node* nj = block-&gt;get_node(_index);
 488       // Handle machine instruction nodes
 489       if (nj-&gt;is_Mach()) {
 490         MachNode* mach = nj-&gt;as_Mach();
 491         blk_size += (mach-&gt;alignment_required() - 1) * relocInfo::addr_unit(); // assume worst case padding
 492         reloc_size += mach-&gt;reloc();
 493         if (mach-&gt;is_MachCall()) {
 494           // add size information for trampoline stub
 495           // class CallStubImpl is platform-specific and defined in the *.ad files.
 496           stub_size  += CallStubImpl::size_call_trampoline();
 497           reloc_size += CallStubImpl::reloc_call_trampoline();
 498 
 499           MachCallNode *mcall = mach-&gt;as_MachCall();
 500           // This destination address is NOT PC-relative
 501 
<span class="line-modified"> 502           mcall-&gt;method_set((intptr_t)mcall-&gt;entry_point());</span>


 503 
 504           if (mcall-&gt;is_MachCallJava() &amp;&amp; mcall-&gt;as_MachCallJava()-&gt;_method) {
 505             stub_size  += CompiledStaticCall::to_interp_stub_size();
 506             reloc_size += CompiledStaticCall::reloc_to_interp_stub();
 507 #if INCLUDE_AOT
 508             stub_size  += CompiledStaticCall::to_aot_stub_size();
 509             reloc_size += CompiledStaticCall::reloc_to_aot_stub();
 510 #endif
 511           }
 512         } else if (mach-&gt;is_MachSafePoint()) {
 513           // If call/safepoint are adjacent, account for possible
 514           // nop to disambiguate the two safepoints.
 515           // ScheduleAndBundle() can rearrange nodes in a block,
 516           // check for all offsets inside this block.
 517           if (last_call_adr &gt;= blk_starts[i]) {
 518             blk_size += nop_size;
 519           }
 520         }
 521         if (mach-&gt;avoid_back_to_back(MachNode::AVOID_BEFORE)) {
 522           // Nop is inserted between &quot;avoid back to back&quot; instructions.
</pre>
<hr />
<pre>
 915       ShouldNotReachHere();
 916       break;
 917   }
 918 }
 919 
 920 // Determine if this node starts a bundle
 921 bool PhaseOutput::starts_bundle(const Node *n) const {
 922   return (_node_bundling_limit &gt; n-&gt;_idx &amp;&amp;
 923           _node_bundling_base[n-&gt;_idx].starts_bundle());
 924 }
 925 
 926 //--------------------------Process_OopMap_Node--------------------------------
 927 void PhaseOutput::Process_OopMap_Node(MachNode *mach, int current_offset) {
 928   // Handle special safepoint nodes for synchronization
 929   MachSafePointNode *sfn   = mach-&gt;as_MachSafePoint();
 930   MachCallNode      *mcall;
 931 
 932   int safepoint_pc_offset = current_offset;
 933   bool is_method_handle_invoke = false;
 934   bool return_oop = false;

 935 
 936   // Add the safepoint in the DebugInfoRecorder
 937   if( !mach-&gt;is_MachCall() ) {
 938     mcall = NULL;
 939     C-&gt;debug_info()-&gt;add_safepoint(safepoint_pc_offset, sfn-&gt;_oop_map);
 940   } else {
 941     mcall = mach-&gt;as_MachCall();
 942 
 943     // Is the call a MethodHandle call?
 944     if (mcall-&gt;is_MachCallJava()) {
 945       if (mcall-&gt;as_MachCallJava()-&gt;_method_handle_invoke) {
 946         assert(C-&gt;has_method_handle_invokes(), &quot;must have been set during call generation&quot;);
 947         is_method_handle_invoke = true;
 948       }
 949     }
 950 
 951     // Check if a call returns an object.
<span class="line-modified"> 952     if (mcall-&gt;returns_pointer()) {</span>
 953       return_oop = true;
 954     }



 955     safepoint_pc_offset += mcall-&gt;ret_addr_offset();
 956     C-&gt;debug_info()-&gt;add_safepoint(safepoint_pc_offset, mcall-&gt;_oop_map);
 957   }
 958 
 959   // Loop over the JVMState list to add scope information
 960   // Do not skip safepoints with a NULL method, they need monitor info
 961   JVMState* youngest_jvms = sfn-&gt;jvms();
 962   int max_depth = youngest_jvms-&gt;depth();
 963 
 964   // Allocate the object pool for scalar-replaced objects -- the map from
 965   // small-integer keys (which can be recorded in the local and ostack
 966   // arrays) to descriptions of the object state.
 967   GrowableArray&lt;ScopeValue*&gt; *objs = new GrowableArray&lt;ScopeValue*&gt;();
 968 
 969   // Visit scopes from oldest to youngest.
 970   for (int depth = 1; depth &lt;= max_depth; depth++) {
 971     JVMState* jvms = youngest_jvms-&gt;of_depth(depth);
 972     int idx;
 973     ciMethod* method = jvms-&gt;has_method() ? jvms-&gt;method() : NULL;
 974     // Safepoints that do not have method() set only provide oop-map and monitor info
</pre>
<hr />
<pre>
1049       bool eliminated = (box_node-&gt;is_BoxLock() &amp;&amp; box_node-&gt;as_BoxLock()-&gt;is_eliminated());
1050       monarray-&gt;append(new MonitorValue(scval, basic_lock, eliminated));
1051     }
1052 
1053     // We dump the object pool first, since deoptimization reads it in first.
1054     C-&gt;debug_info()-&gt;dump_object_pool(objs);
1055 
1056     // Build first class objects to pass to scope
1057     DebugToken *locvals = C-&gt;debug_info()-&gt;create_scope_values(locarray);
1058     DebugToken *expvals = C-&gt;debug_info()-&gt;create_scope_values(exparray);
1059     DebugToken *monvals = C-&gt;debug_info()-&gt;create_monitor_values(monarray);
1060 
1061     // Make method available for all Safepoints
1062     ciMethod* scope_method = method ? method : C-&gt;method();
1063     // Describe the scope here
1064     assert(jvms-&gt;bci() &gt;= InvocationEntryBci &amp;&amp; jvms-&gt;bci() &lt;= 0x10000, &quot;must be a valid or entry BCI&quot;);
1065     assert(!jvms-&gt;should_reexecute() || depth == max_depth, &quot;reexecute allowed only for the youngest&quot;);
1066     // Now we can describe the scope.
1067     methodHandle null_mh;
1068     bool rethrow_exception = false;
<span class="line-modified">1069     C-&gt;debug_info()-&gt;describe_scope(safepoint_pc_offset, null_mh, scope_method, jvms-&gt;bci(), jvms-&gt;should_reexecute(), rethrow_exception, is_method_handle_invoke, return_oop, locvals, expvals, monvals);</span>
1070   } // End jvms loop
1071 
1072   // Mark the end of the scope set.
1073   C-&gt;debug_info()-&gt;end_safepoint(safepoint_pc_offset);
1074 }
1075 
1076 
1077 
1078 // A simplified version of Process_OopMap_Node, to handle non-safepoints.
1079 class NonSafepointEmitter {
1080     Compile*  C;
1081     JVMState* _pending_jvms;
1082     int       _pending_offset;
1083 
1084     void emit_non_safepoint();
1085 
1086  public:
1087     NonSafepointEmitter(Compile* compile) {
1088       this-&gt;C = compile;
1089       _pending_jvms = NULL;
</pre>
<hr />
<pre>
1154   }
1155 
1156   // Mark the end of the scope set.
1157   debug_info-&gt;end_non_safepoint(pc_offset);
1158 }
1159 
1160 //------------------------------init_buffer------------------------------------
1161 void PhaseOutput::estimate_buffer_size(int&amp; const_req) {
1162 
1163   // Set the initially allocated size
1164   const_req = initial_const_capacity;
1165 
1166   // The extra spacing after the code is necessary on some platforms.
1167   // Sometimes we need to patch in a jump after the last instruction,
1168   // if the nmethod has been deoptimized.  (See 4932387, 4894843.)
1169 
1170   // Compute the byte offset where we can store the deopt pc.
1171   if (C-&gt;fixed_slots() != 0) {
1172     _orig_pc_slot_offset_in_bytes = C-&gt;regalloc()-&gt;reg2offset(OptoReg::stack2reg(_orig_pc_slot));
1173   }




1174 
1175   // Compute prolog code size
1176   _method_size = 0;
1177   _frame_slots = OptoReg::reg2stack(C-&gt;matcher()-&gt;_old_SP) + C-&gt;regalloc()-&gt;_framesize;
1178 #if defined(IA64) &amp;&amp; !defined(AIX)
1179   if (save_argument_registers()) {
1180     // 4815101: this is a stub with implicit and unknown precision fp args.
1181     // The usual spill mechanism can only generate stfd&#39;s in this case, which
1182     // doesn&#39;t work if the fp reg to spill contains a single-precision denorm.
1183     // Instead, we hack around the normal spill mechanism using stfspill&#39;s and
1184     // ldffill&#39;s in the MachProlog and MachEpilog emit methods.  We allocate
1185     // space here for the fp arg regs (f8-f15) we&#39;re going to thusly spill.
1186     //
1187     // If we ever implement 16-byte &#39;registers&#39; == stack slots, we can
1188     // get rid of this hack and have SpillCopy generate stfspill/ldffill
1189     // instead of stfd/stfs/ldfd/ldfs.
1190     _frame_slots += 8*(16/BytesPerInt);
1191   }
1192 #endif
1193   assert(_frame_slots &gt;= 0 &amp;&amp; _frame_slots &lt; 1000000, &quot;sanity check&quot;);
</pre>
<hr />
<pre>
1430           int nops_cnt = padding / nop_size;
1431           MachNode *nop = new MachNopNode(nops_cnt);
1432           block-&gt;insert_node(nop, j++);
1433           last_inst++;
1434           C-&gt;cfg()-&gt;map_node_to_block(nop, block);
1435           // Ensure enough space.
1436           cb-&gt;insts()-&gt;maybe_expand_to_ensure_remaining(MAX_inst_size);
1437           if ((cb-&gt;blob() == NULL) || (!CompileBroker::should_compile_new_jobs())) {
1438             C-&gt;record_failure(&quot;CodeCache is full&quot;);
1439             return;
1440           }
1441           nop-&gt;emit(*cb, C-&gt;regalloc());
1442           cb-&gt;flush_bundle(true);
1443           current_offset = cb-&gt;insts_size();
1444         }
1445 
1446         // Remember the start of the last call in a basic block
1447         if (is_mcall) {
1448           MachCallNode *mcall = mach-&gt;as_MachCall();
1449 
<span class="line-modified">1450           // This destination address is NOT PC-relative</span>
<span class="line-modified">1451           mcall-&gt;method_set((intptr_t)mcall-&gt;entry_point());</span>


1452 
1453           // Save the return address
1454           call_returns[block-&gt;_pre_order] = current_offset + mcall-&gt;ret_addr_offset();
1455 
1456           if (mcall-&gt;is_MachCallLeaf()) {
1457             is_mcall = false;
1458             is_sfn = false;
1459           }
1460         }
1461 
1462         // sfn will be valid whenever mcall is valid now because of inheritance
1463         if (is_sfn || is_mcall) {
1464 
1465           // Handle special safepoint nodes for synchronization
1466           if (!is_mcall) {
1467             MachSafePointNode *sfn = mach-&gt;as_MachSafePoint();
1468             // !!!!! Stubs only need an oopmap right now, so bail out
1469             if (sfn-&gt;jvms()-&gt;method() == NULL) {
1470               // Write the oopmap directly to the code blob??!!
1471               continue;
</pre>
<hr />
<pre>
3153 }
3154 #endif
3155 
3156 //-----------------------init_scratch_buffer_blob------------------------------
3157 // Construct a temporary BufferBlob and cache it for this compile.
3158 void PhaseOutput::init_scratch_buffer_blob(int const_size) {
3159   // If there is already a scratch buffer blob allocated and the
3160   // constant section is big enough, use it.  Otherwise free the
3161   // current and allocate a new one.
3162   BufferBlob* blob = scratch_buffer_blob();
3163   if ((blob != NULL) &amp;&amp; (const_size &lt;= _scratch_const_size)) {
3164     // Use the current blob.
3165   } else {
3166     if (blob != NULL) {
3167       BufferBlob::free(blob);
3168     }
3169 
3170     ResourceMark rm;
3171     _scratch_const_size = const_size;
3172     int size = C2Compiler::initial_code_buffer_size(const_size);






3173     blob = BufferBlob::create(&quot;Compile::scratch_buffer&quot;, size);
3174     // Record the buffer blob for next time.
3175     set_scratch_buffer_blob(blob);
3176     // Have we run out of code space?
3177     if (scratch_buffer_blob() == NULL) {
3178       // Let CompilerBroker disable further compilations.
3179       C-&gt;record_failure(&quot;Not enough space for scratch buffer in CodeCache&quot;);
3180       return;
3181     }
3182   }
3183 
3184   // Initialize the relocation buffers
3185   relocInfo* locs_buf = (relocInfo*) blob-&gt;content_end() - MAX_locs_size;
3186   set_scratch_locs_memory(locs_buf);
3187 }
3188 
3189 
3190 //-----------------------scratch_emit_size-------------------------------------
3191 // Helper function that computes size by emitting code
3192 uint PhaseOutput::scratch_emit_size(const Node* n) {
</pre>
<hr />
<pre>
3217   int lsize = MAX_locs_size / 3;
3218   buf.consts()-&gt;initialize_shared_locs(&amp;locs_buf[lsize * 0], lsize);
3219   buf.insts()-&gt;initialize_shared_locs( &amp;locs_buf[lsize * 1], lsize);
3220   buf.stubs()-&gt;initialize_shared_locs( &amp;locs_buf[lsize * 2], lsize);
3221   // Mark as scratch buffer.
3222   buf.consts()-&gt;set_scratch_emit();
3223   buf.insts()-&gt;set_scratch_emit();
3224   buf.stubs()-&gt;set_scratch_emit();
3225 
3226   // Do the emission.
3227 
3228   Label fakeL; // Fake label for branch instructions.
3229   Label*   saveL = NULL;
3230   uint save_bnum = 0;
3231   bool is_branch = n-&gt;is_MachBranch();
3232   if (is_branch) {
3233     MacroAssembler masm(&amp;buf);
3234     masm.bind(fakeL);
3235     n-&gt;as_MachBranch()-&gt;save_label(&amp;saveL, &amp;save_bnum);
3236     n-&gt;as_MachBranch()-&gt;label_set(&amp;fakeL, 0);






3237   }
3238   n-&gt;emit(buf, C-&gt;regalloc());
3239 
3240   // Emitting into the scratch buffer should not fail
3241   assert (!C-&gt;failing(), &quot;Must not have pending failure. Reason is: %s&quot;, C-&gt;failure_reason());
3242 
<span class="line-modified">3243   if (is_branch) // Restore label.</span>

3244     n-&gt;as_MachBranch()-&gt;label_set(saveL, save_bnum);





3245 
3246   // End scratch_emit_size section.
3247   set_in_scratch_emit_size(false);
3248 
3249   return buf.insts_size();
3250 }
3251 
3252 void PhaseOutput::install() {
3253   if (C-&gt;stub_function() != NULL) {
3254     install_stub(C-&gt;stub_name(),
3255                  C-&gt;save_argument_registers());
3256   } else {
3257     install_code(C-&gt;method(),
3258                  C-&gt;entry_bci(),
3259                  CompileBroker::compiler2(),
3260                  C-&gt;has_unsafe_access(),
3261                  SharedRuntime::is_wide_vector(C-&gt;max_vector_size()),
3262                  C-&gt;rtm_state());
3263   }
3264 }
</pre>
<hr />
<pre>
3267                                int               entry_bci,
3268                                AbstractCompiler* compiler,
3269                                bool              has_unsafe_access,
3270                                bool              has_wide_vectors,
3271                                RTMState          rtm_state) {
3272   // Check if we want to skip execution of all compiled code.
3273   {
3274 #ifndef PRODUCT
3275     if (OptoNoExecute) {
3276       C-&gt;record_method_not_compilable(&quot;+OptoNoExecute&quot;);  // Flag as failed
3277       return;
3278     }
3279 #endif
3280     Compile::TracePhase tp(&quot;install_code&quot;, &amp;timers[_t_registerMethod]);
3281 
3282     if (C-&gt;is_osr_compilation()) {
3283       _code_offsets.set_value(CodeOffsets::Verified_Entry, 0);
3284       _code_offsets.set_value(CodeOffsets::OSR_Entry, _first_block_size);
3285     } else {
3286       _code_offsets.set_value(CodeOffsets::Verified_Entry, _first_block_size);









3287       _code_offsets.set_value(CodeOffsets::OSR_Entry, 0);
3288     }
3289 
3290     C-&gt;env()-&gt;register_method(target,
<span class="line-modified">3291                                      entry_bci,</span>
<span class="line-modified">3292                                      &amp;_code_offsets,</span>
<span class="line-modified">3293                                      _orig_pc_slot_offset_in_bytes,</span>
<span class="line-modified">3294                                      code_buffer(),</span>
<span class="line-modified">3295                                      frame_size_in_words(),</span>
<span class="line-modified">3296                                      oop_map_set(),</span>
<span class="line-modified">3297                                      &amp;_handler_table,</span>
<span class="line-modified">3298                                      inc_table(),</span>
<span class="line-modified">3299                                      compiler,</span>
<span class="line-modified">3300                                      has_unsafe_access,</span>
<span class="line-modified">3301                                      SharedRuntime::is_wide_vector(C-&gt;max_vector_size()),</span>
<span class="line-modified">3302                                      C-&gt;rtm_state());</span>
3303 
3304     if (C-&gt;log() != NULL) { // Print code cache state into compiler log
3305       C-&gt;log()-&gt;code_cache_state();
3306     }
3307   }
3308 }
3309 void PhaseOutput::install_stub(const char* stub_name,
3310                                bool        caller_must_gc_arguments) {
3311   // Entry point will be accessed using stub_entry_point();
3312   if (code_buffer() == NULL) {
3313     Matcher::soft_match_failure();
3314   } else {
3315     if (PrintAssembly &amp;&amp; (WizardMode || Verbose))
3316       tty-&gt;print_cr(&quot;### Stub::%s&quot;, stub_name);
3317 
3318     if (!C-&gt;failing()) {
3319       assert(C-&gt;fixed_slots() == 0, &quot;no fixed slots used for runtime stubs&quot;);
3320 
3321       // Make the NMethod
3322       // For now we mark the frame as never safe for profile stackwalking
</pre>
</td>
<td>
<hr />
<pre>
 224 };
 225 
 226 
 227 PhaseOutput::PhaseOutput()
 228   : Phase(Phase::Output),
 229     _code_buffer(&quot;Compile::Fill_buffer&quot;),
 230     _first_block_size(0),
 231     _handler_table(),
 232     _inc_table(),
 233     _oop_map_set(NULL),
 234     _scratch_buffer_blob(NULL),
 235     _scratch_locs_memory(NULL),
 236     _scratch_const_size(-1),
 237     _in_scratch_emit_size(false),
 238     _frame_slots(0),
 239     _code_offsets(),
 240     _node_bundling_limit(0),
 241     _node_bundling_base(NULL),
 242     _orig_pc_slot(0),
 243     _orig_pc_slot_offset_in_bytes(0),
<span class="line-added"> 244     _sp_inc_slot(0),</span>
<span class="line-added"> 245     _sp_inc_slot_offset_in_bytes(0),</span>
 246     _buf_sizes(),
 247     _block(NULL),
 248     _index(0) {
 249   C-&gt;set_output(this);
 250   if (C-&gt;stub_name() == NULL) {
<span class="line-modified"> 251     int fixed_slots = C-&gt;fixed_slots();</span>
<span class="line-added"> 252     if (C-&gt;needs_stack_repair()) {</span>
<span class="line-added"> 253       fixed_slots -= 2;</span>
<span class="line-added"> 254       _sp_inc_slot = fixed_slots;</span>
<span class="line-added"> 255     }</span>
<span class="line-added"> 256     _orig_pc_slot = fixed_slots - (sizeof(address) / VMRegImpl::stack_slot_size);</span>
 257   }
 258 }
 259 
 260 PhaseOutput::~PhaseOutput() {
 261   C-&gt;set_output(NULL);
 262   if (_scratch_buffer_blob != NULL) {
 263     BufferBlob::free(_scratch_buffer_blob);
 264   }
 265 }
 266 
 267 void PhaseOutput::perform_mach_node_analysis() {
 268   // Late barrier analysis must be done after schedule and bundle
 269   // Otherwise liveness based spilling will fail
 270   BarrierSetC2* bs = BarrierSet::barrier_set()-&gt;barrier_set_c2();
 271   bs-&gt;late_barrier_analysis();
 272 
 273   pd_perform_mach_node_analysis();
 274 }
 275 
 276 // Convert Nodes to instruction bits and pass off to the VM
 277 void PhaseOutput::Output() {
 278   // RootNode goes
 279   assert( C-&gt;cfg()-&gt;get_root_block()-&gt;number_of_nodes() == 0, &quot;&quot; );
 280 
 281   // The number of new nodes (mostly MachNop) is proportional to
 282   // the number of java calls and inner loops which are aligned.
 283   if ( C-&gt;check_node_count((NodeLimitFudgeFactor + C-&gt;java_calls()*3 +
 284                             C-&gt;inner_loops()*(OptoLoopAlignment-1)),
 285                            &quot;out of nodes before code generation&quot; ) ) {
 286     return;
 287   }
 288   // Make sure I can find the Start Node
 289   Block *entry = C-&gt;cfg()-&gt;get_block(1);
 290   Block *broot = C-&gt;cfg()-&gt;get_root_block();
 291 
 292   const StartNode *start = entry-&gt;head()-&gt;as_Start();
 293 
 294   // Replace StartNode with prolog
<span class="line-modified"> 295   Label verified_entry;</span>
<span class="line-added"> 296   MachPrologNode* prolog = new MachPrologNode(&amp;verified_entry);</span>
 297   entry-&gt;map_node(prolog, 0);
 298   C-&gt;cfg()-&gt;map_node_to_block(prolog, entry);
 299   C-&gt;cfg()-&gt;unmap_node_from_block(start); // start is no longer in any block
 300 
 301   // Virtual methods need an unverified entry point
<span class="line-modified"> 302   if (C-&gt;is_osr_compilation()) {</span>
<span class="line-modified"> 303     if (PoisonOSREntry) {</span>

 304       // TODO: Should use a ShouldNotReachHereNode...
 305       C-&gt;cfg()-&gt;insert( broot, 0, new MachBreakpointNode() );
 306     }
 307   } else {
<span class="line-modified"> 308     if (C-&gt;method()) {</span>
<span class="line-modified"> 309       if (C-&gt;method()-&gt;has_scalarized_args()) {</span>
<span class="line-modified"> 310         // Add entry point to unpack all value type arguments</span>
<span class="line-added"> 311         C-&gt;cfg()-&gt;insert(broot, 0, new MachVEPNode(&amp;verified_entry, /* verified */ true, /* receiver_only */ false));</span>
<span class="line-added"> 312         if (!C-&gt;method()-&gt;is_static()) {</span>
<span class="line-added"> 313           // Add verified/unverified entry points to only unpack value type receiver at interface calls</span>
<span class="line-added"> 314           C-&gt;cfg()-&gt;insert(broot, 0, new MachVEPNode(&amp;verified_entry, /* verified */ false, /* receiver_only */ false));</span>
<span class="line-added"> 315           C-&gt;cfg()-&gt;insert(broot, 0, new MachVEPNode(&amp;verified_entry, /* verified */ true,  /* receiver_only */ true));</span>
<span class="line-added"> 316           C-&gt;cfg()-&gt;insert(broot, 0, new MachVEPNode(&amp;verified_entry, /* verified */ false, /* receiver_only */ true));</span>
<span class="line-added"> 317         }</span>
<span class="line-added"> 318       } else if (!C-&gt;method()-&gt;is_static()) {</span>
<span class="line-added"> 319         // Insert unvalidated entry point</span>
<span class="line-added"> 320         C-&gt;cfg()-&gt;insert(broot, 0, new MachUEPNode());</span>
<span class="line-added"> 321       }</span>
 322     }

 323   }
 324 
 325   // Break before main entry point
 326   if ((C-&gt;method() &amp;&amp; C-&gt;directive()-&gt;BreakAtExecuteOption) ||
 327       (OptoBreakpoint &amp;&amp; C-&gt;is_method_compilation())       ||
 328       (OptoBreakpointOSR &amp;&amp; C-&gt;is_osr_compilation())       ||
 329       (OptoBreakpointC2R &amp;&amp; !C-&gt;method())                   ) {
 330     // checking for C-&gt;method() means that OptoBreakpoint does not apply to
 331     // runtime stubs or frame converters
 332     C-&gt;cfg()-&gt;insert( entry, 1, new MachBreakpointNode() );
 333   }
 334 
 335   // Insert epilogs before every return
 336   for (uint i = 0; i &lt; C-&gt;cfg()-&gt;number_of_blocks(); i++) {
 337     Block* block = C-&gt;cfg()-&gt;get_block(i);
 338     if (!block-&gt;is_connector() &amp;&amp; block-&gt;non_connector_successor(0) == C-&gt;cfg()-&gt;get_root_block()) { // Found a program exit point?
 339       Node* m = block-&gt;end();
 340       if (m-&gt;is_Mach() &amp;&amp; m-&gt;as_Mach()-&gt;ideal_Opcode() != Op_Halt) {
 341         MachEpilogNode* epilog = new MachEpilogNode(m-&gt;as_Mach()-&gt;ideal_Opcode() == Op_Return);
 342         block-&gt;add_inst(epilog);
 343         C-&gt;cfg()-&gt;map_node_to_block(epilog, block);
 344       }
 345     }
 346   }
 347 
 348   // Keeper of sizing aspects
 349   _buf_sizes = BufferSizingData();
 350 
 351   // Initialize code buffer
 352   estimate_buffer_size(_buf_sizes._const);
 353   if (C-&gt;failing()) return;
 354 
 355   // Pre-compute the length of blocks and replace
 356   // long branches with short if machine supports it.
 357   // Must be done before ScheduleAndBundle due to SPARC delay slots
 358   uint* blk_starts = NEW_RESOURCE_ARRAY(uint, C-&gt;cfg()-&gt;number_of_blocks() + 1);
 359   blk_starts[0] = 0;
 360   shorten_branches(blk_starts);
 361 
<span class="line-added"> 362   if (!C-&gt;is_osr_compilation() &amp;&amp; C-&gt;has_scalarized_args()) {</span>
<span class="line-added"> 363     // Compute the offsets of the entry points required by the value type calling convention</span>
<span class="line-added"> 364     if (!C-&gt;method()-&gt;is_static()) {</span>
<span class="line-added"> 365       // We have entries at the beginning of the method, implemented by the first 4 nodes.</span>
<span class="line-added"> 366       // Entry                     (unverified) @ offset 0</span>
<span class="line-added"> 367       // Verified_Value_Entry_RO</span>
<span class="line-added"> 368       // Value_Entry               (unverified)</span>
<span class="line-added"> 369       // Verified_Value_Entry</span>
<span class="line-added"> 370       uint offset = 0;</span>
<span class="line-added"> 371       _code_offsets.set_value(CodeOffsets::Entry, offset);</span>
<span class="line-added"> 372 </span>
<span class="line-added"> 373       offset += ((MachVEPNode*)broot-&gt;get_node(0))-&gt;size(C-&gt;regalloc());</span>
<span class="line-added"> 374       _code_offsets.set_value(CodeOffsets::Verified_Value_Entry_RO, offset);</span>
<span class="line-added"> 375 </span>
<span class="line-added"> 376       offset += ((MachVEPNode*)broot-&gt;get_node(1))-&gt;size(C-&gt;regalloc());</span>
<span class="line-added"> 377       _code_offsets.set_value(CodeOffsets::Value_Entry, offset);</span>
<span class="line-added"> 378 </span>
<span class="line-added"> 379       offset += ((MachVEPNode*)broot-&gt;get_node(2))-&gt;size(C-&gt;regalloc());</span>
<span class="line-added"> 380       _code_offsets.set_value(CodeOffsets::Verified_Value_Entry, offset);</span>
<span class="line-added"> 381     } else {</span>
<span class="line-added"> 382       _code_offsets.set_value(CodeOffsets::Entry, -1); // will be patched later</span>
<span class="line-added"> 383       _code_offsets.set_value(CodeOffsets::Verified_Value_Entry, 0);</span>
<span class="line-added"> 384     }</span>
<span class="line-added"> 385   }</span>
<span class="line-added"> 386 </span>
 387   ScheduleAndBundle();
 388   if (C-&gt;failing()) {
 389     return;
 390   }
 391 
 392   perform_mach_node_analysis();
 393 
 394   // Complete sizing of codebuffer
 395   CodeBuffer* cb = init_buffer();
 396   if (cb == NULL || C-&gt;failing()) {
 397     return;
 398   }
 399 
 400   BuildOopMaps();
 401 
 402   if (C-&gt;failing())  {
 403     return;
 404   }
 405 
 406   fill_buffer(cb, blk_starts);
</pre>
<hr />
<pre>
 524     // Sum all instruction sizes to compute block size
 525     uint last_inst = block-&gt;number_of_nodes();
 526     uint blk_size = 0;
 527     for (uint j = 0; j &lt; last_inst; j++) {
 528       _index = j;
 529       Node* nj = block-&gt;get_node(_index);
 530       // Handle machine instruction nodes
 531       if (nj-&gt;is_Mach()) {
 532         MachNode* mach = nj-&gt;as_Mach();
 533         blk_size += (mach-&gt;alignment_required() - 1) * relocInfo::addr_unit(); // assume worst case padding
 534         reloc_size += mach-&gt;reloc();
 535         if (mach-&gt;is_MachCall()) {
 536           // add size information for trampoline stub
 537           // class CallStubImpl is platform-specific and defined in the *.ad files.
 538           stub_size  += CallStubImpl::size_call_trampoline();
 539           reloc_size += CallStubImpl::reloc_call_trampoline();
 540 
 541           MachCallNode *mcall = mach-&gt;as_MachCall();
 542           // This destination address is NOT PC-relative
 543 
<span class="line-modified"> 544           if (mcall-&gt;entry_point() != NULL) {</span>
<span class="line-added"> 545             mcall-&gt;method_set((intptr_t)mcall-&gt;entry_point());</span>
<span class="line-added"> 546           }</span>
 547 
 548           if (mcall-&gt;is_MachCallJava() &amp;&amp; mcall-&gt;as_MachCallJava()-&gt;_method) {
 549             stub_size  += CompiledStaticCall::to_interp_stub_size();
 550             reloc_size += CompiledStaticCall::reloc_to_interp_stub();
 551 #if INCLUDE_AOT
 552             stub_size  += CompiledStaticCall::to_aot_stub_size();
 553             reloc_size += CompiledStaticCall::reloc_to_aot_stub();
 554 #endif
 555           }
 556         } else if (mach-&gt;is_MachSafePoint()) {
 557           // If call/safepoint are adjacent, account for possible
 558           // nop to disambiguate the two safepoints.
 559           // ScheduleAndBundle() can rearrange nodes in a block,
 560           // check for all offsets inside this block.
 561           if (last_call_adr &gt;= blk_starts[i]) {
 562             blk_size += nop_size;
 563           }
 564         }
 565         if (mach-&gt;avoid_back_to_back(MachNode::AVOID_BEFORE)) {
 566           // Nop is inserted between &quot;avoid back to back&quot; instructions.
</pre>
<hr />
<pre>
 959       ShouldNotReachHere();
 960       break;
 961   }
 962 }
 963 
 964 // Determine if this node starts a bundle
 965 bool PhaseOutput::starts_bundle(const Node *n) const {
 966   return (_node_bundling_limit &gt; n-&gt;_idx &amp;&amp;
 967           _node_bundling_base[n-&gt;_idx].starts_bundle());
 968 }
 969 
 970 //--------------------------Process_OopMap_Node--------------------------------
 971 void PhaseOutput::Process_OopMap_Node(MachNode *mach, int current_offset) {
 972   // Handle special safepoint nodes for synchronization
 973   MachSafePointNode *sfn   = mach-&gt;as_MachSafePoint();
 974   MachCallNode      *mcall;
 975 
 976   int safepoint_pc_offset = current_offset;
 977   bool is_method_handle_invoke = false;
 978   bool return_oop = false;
<span class="line-added"> 979   bool return_vt = false;</span>
 980 
 981   // Add the safepoint in the DebugInfoRecorder
 982   if( !mach-&gt;is_MachCall() ) {
 983     mcall = NULL;
 984     C-&gt;debug_info()-&gt;add_safepoint(safepoint_pc_offset, sfn-&gt;_oop_map);
 985   } else {
 986     mcall = mach-&gt;as_MachCall();
 987 
 988     // Is the call a MethodHandle call?
 989     if (mcall-&gt;is_MachCallJava()) {
 990       if (mcall-&gt;as_MachCallJava()-&gt;_method_handle_invoke) {
 991         assert(C-&gt;has_method_handle_invokes(), &quot;must have been set during call generation&quot;);
 992         is_method_handle_invoke = true;
 993       }
 994     }
 995 
 996     // Check if a call returns an object.
<span class="line-modified"> 997     if (mcall-&gt;returns_pointer() || mcall-&gt;returns_vt()) {</span>
 998       return_oop = true;
 999     }
<span class="line-added">1000     if (mcall-&gt;returns_vt()) {</span>
<span class="line-added">1001       return_vt = true;</span>
<span class="line-added">1002     }</span>
1003     safepoint_pc_offset += mcall-&gt;ret_addr_offset();
1004     C-&gt;debug_info()-&gt;add_safepoint(safepoint_pc_offset, mcall-&gt;_oop_map);
1005   }
1006 
1007   // Loop over the JVMState list to add scope information
1008   // Do not skip safepoints with a NULL method, they need monitor info
1009   JVMState* youngest_jvms = sfn-&gt;jvms();
1010   int max_depth = youngest_jvms-&gt;depth();
1011 
1012   // Allocate the object pool for scalar-replaced objects -- the map from
1013   // small-integer keys (which can be recorded in the local and ostack
1014   // arrays) to descriptions of the object state.
1015   GrowableArray&lt;ScopeValue*&gt; *objs = new GrowableArray&lt;ScopeValue*&gt;();
1016 
1017   // Visit scopes from oldest to youngest.
1018   for (int depth = 1; depth &lt;= max_depth; depth++) {
1019     JVMState* jvms = youngest_jvms-&gt;of_depth(depth);
1020     int idx;
1021     ciMethod* method = jvms-&gt;has_method() ? jvms-&gt;method() : NULL;
1022     // Safepoints that do not have method() set only provide oop-map and monitor info
</pre>
<hr />
<pre>
1097       bool eliminated = (box_node-&gt;is_BoxLock() &amp;&amp; box_node-&gt;as_BoxLock()-&gt;is_eliminated());
1098       monarray-&gt;append(new MonitorValue(scval, basic_lock, eliminated));
1099     }
1100 
1101     // We dump the object pool first, since deoptimization reads it in first.
1102     C-&gt;debug_info()-&gt;dump_object_pool(objs);
1103 
1104     // Build first class objects to pass to scope
1105     DebugToken *locvals = C-&gt;debug_info()-&gt;create_scope_values(locarray);
1106     DebugToken *expvals = C-&gt;debug_info()-&gt;create_scope_values(exparray);
1107     DebugToken *monvals = C-&gt;debug_info()-&gt;create_monitor_values(monarray);
1108 
1109     // Make method available for all Safepoints
1110     ciMethod* scope_method = method ? method : C-&gt;method();
1111     // Describe the scope here
1112     assert(jvms-&gt;bci() &gt;= InvocationEntryBci &amp;&amp; jvms-&gt;bci() &lt;= 0x10000, &quot;must be a valid or entry BCI&quot;);
1113     assert(!jvms-&gt;should_reexecute() || depth == max_depth, &quot;reexecute allowed only for the youngest&quot;);
1114     // Now we can describe the scope.
1115     methodHandle null_mh;
1116     bool rethrow_exception = false;
<span class="line-modified">1117     C-&gt;debug_info()-&gt;describe_scope(safepoint_pc_offset, null_mh, scope_method, jvms-&gt;bci(), jvms-&gt;should_reexecute(), rethrow_exception, is_method_handle_invoke, return_oop, return_vt, locvals, expvals, monvals);</span>
1118   } // End jvms loop
1119 
1120   // Mark the end of the scope set.
1121   C-&gt;debug_info()-&gt;end_safepoint(safepoint_pc_offset);
1122 }
1123 
1124 
1125 
1126 // A simplified version of Process_OopMap_Node, to handle non-safepoints.
1127 class NonSafepointEmitter {
1128     Compile*  C;
1129     JVMState* _pending_jvms;
1130     int       _pending_offset;
1131 
1132     void emit_non_safepoint();
1133 
1134  public:
1135     NonSafepointEmitter(Compile* compile) {
1136       this-&gt;C = compile;
1137       _pending_jvms = NULL;
</pre>
<hr />
<pre>
1202   }
1203 
1204   // Mark the end of the scope set.
1205   debug_info-&gt;end_non_safepoint(pc_offset);
1206 }
1207 
1208 //------------------------------init_buffer------------------------------------
1209 void PhaseOutput::estimate_buffer_size(int&amp; const_req) {
1210 
1211   // Set the initially allocated size
1212   const_req = initial_const_capacity;
1213 
1214   // The extra spacing after the code is necessary on some platforms.
1215   // Sometimes we need to patch in a jump after the last instruction,
1216   // if the nmethod has been deoptimized.  (See 4932387, 4894843.)
1217 
1218   // Compute the byte offset where we can store the deopt pc.
1219   if (C-&gt;fixed_slots() != 0) {
1220     _orig_pc_slot_offset_in_bytes = C-&gt;regalloc()-&gt;reg2offset(OptoReg::stack2reg(_orig_pc_slot));
1221   }
<span class="line-added">1222   if (C-&gt;needs_stack_repair()) {</span>
<span class="line-added">1223     // Compute the byte offset of the stack increment value</span>
<span class="line-added">1224     _sp_inc_slot_offset_in_bytes = C-&gt;regalloc()-&gt;reg2offset(OptoReg::stack2reg(_sp_inc_slot));</span>
<span class="line-added">1225   }</span>
1226 
1227   // Compute prolog code size
1228   _method_size = 0;
1229   _frame_slots = OptoReg::reg2stack(C-&gt;matcher()-&gt;_old_SP) + C-&gt;regalloc()-&gt;_framesize;
1230 #if defined(IA64) &amp;&amp; !defined(AIX)
1231   if (save_argument_registers()) {
1232     // 4815101: this is a stub with implicit and unknown precision fp args.
1233     // The usual spill mechanism can only generate stfd&#39;s in this case, which
1234     // doesn&#39;t work if the fp reg to spill contains a single-precision denorm.
1235     // Instead, we hack around the normal spill mechanism using stfspill&#39;s and
1236     // ldffill&#39;s in the MachProlog and MachEpilog emit methods.  We allocate
1237     // space here for the fp arg regs (f8-f15) we&#39;re going to thusly spill.
1238     //
1239     // If we ever implement 16-byte &#39;registers&#39; == stack slots, we can
1240     // get rid of this hack and have SpillCopy generate stfspill/ldffill
1241     // instead of stfd/stfs/ldfd/ldfs.
1242     _frame_slots += 8*(16/BytesPerInt);
1243   }
1244 #endif
1245   assert(_frame_slots &gt;= 0 &amp;&amp; _frame_slots &lt; 1000000, &quot;sanity check&quot;);
</pre>
<hr />
<pre>
1482           int nops_cnt = padding / nop_size;
1483           MachNode *nop = new MachNopNode(nops_cnt);
1484           block-&gt;insert_node(nop, j++);
1485           last_inst++;
1486           C-&gt;cfg()-&gt;map_node_to_block(nop, block);
1487           // Ensure enough space.
1488           cb-&gt;insts()-&gt;maybe_expand_to_ensure_remaining(MAX_inst_size);
1489           if ((cb-&gt;blob() == NULL) || (!CompileBroker::should_compile_new_jobs())) {
1490             C-&gt;record_failure(&quot;CodeCache is full&quot;);
1491             return;
1492           }
1493           nop-&gt;emit(*cb, C-&gt;regalloc());
1494           cb-&gt;flush_bundle(true);
1495           current_offset = cb-&gt;insts_size();
1496         }
1497 
1498         // Remember the start of the last call in a basic block
1499         if (is_mcall) {
1500           MachCallNode *mcall = mach-&gt;as_MachCall();
1501 
<span class="line-modified">1502           if (mcall-&gt;entry_point() != NULL) {</span>
<span class="line-modified">1503             // This destination address is NOT PC-relative</span>
<span class="line-added">1504             mcall-&gt;method_set((intptr_t)mcall-&gt;entry_point());</span>
<span class="line-added">1505           }</span>
1506 
1507           // Save the return address
1508           call_returns[block-&gt;_pre_order] = current_offset + mcall-&gt;ret_addr_offset();
1509 
1510           if (mcall-&gt;is_MachCallLeaf()) {
1511             is_mcall = false;
1512             is_sfn = false;
1513           }
1514         }
1515 
1516         // sfn will be valid whenever mcall is valid now because of inheritance
1517         if (is_sfn || is_mcall) {
1518 
1519           // Handle special safepoint nodes for synchronization
1520           if (!is_mcall) {
1521             MachSafePointNode *sfn = mach-&gt;as_MachSafePoint();
1522             // !!!!! Stubs only need an oopmap right now, so bail out
1523             if (sfn-&gt;jvms()-&gt;method() == NULL) {
1524               // Write the oopmap directly to the code blob??!!
1525               continue;
</pre>
<hr />
<pre>
3207 }
3208 #endif
3209 
3210 //-----------------------init_scratch_buffer_blob------------------------------
3211 // Construct a temporary BufferBlob and cache it for this compile.
3212 void PhaseOutput::init_scratch_buffer_blob(int const_size) {
3213   // If there is already a scratch buffer blob allocated and the
3214   // constant section is big enough, use it.  Otherwise free the
3215   // current and allocate a new one.
3216   BufferBlob* blob = scratch_buffer_blob();
3217   if ((blob != NULL) &amp;&amp; (const_size &lt;= _scratch_const_size)) {
3218     // Use the current blob.
3219   } else {
3220     if (blob != NULL) {
3221       BufferBlob::free(blob);
3222     }
3223 
3224     ResourceMark rm;
3225     _scratch_const_size = const_size;
3226     int size = C2Compiler::initial_code_buffer_size(const_size);
<span class="line-added">3227 #ifdef ASSERT</span>
<span class="line-added">3228     if (C-&gt;has_scalarized_args()) {</span>
<span class="line-added">3229       // Oop verification for loading object fields from scalarized value types in the new entry point requires lots of space</span>
<span class="line-added">3230       size += 5120;</span>
<span class="line-added">3231     }</span>
<span class="line-added">3232 #endif</span>
3233     blob = BufferBlob::create(&quot;Compile::scratch_buffer&quot;, size);
3234     // Record the buffer blob for next time.
3235     set_scratch_buffer_blob(blob);
3236     // Have we run out of code space?
3237     if (scratch_buffer_blob() == NULL) {
3238       // Let CompilerBroker disable further compilations.
3239       C-&gt;record_failure(&quot;Not enough space for scratch buffer in CodeCache&quot;);
3240       return;
3241     }
3242   }
3243 
3244   // Initialize the relocation buffers
3245   relocInfo* locs_buf = (relocInfo*) blob-&gt;content_end() - MAX_locs_size;
3246   set_scratch_locs_memory(locs_buf);
3247 }
3248 
3249 
3250 //-----------------------scratch_emit_size-------------------------------------
3251 // Helper function that computes size by emitting code
3252 uint PhaseOutput::scratch_emit_size(const Node* n) {
</pre>
<hr />
<pre>
3277   int lsize = MAX_locs_size / 3;
3278   buf.consts()-&gt;initialize_shared_locs(&amp;locs_buf[lsize * 0], lsize);
3279   buf.insts()-&gt;initialize_shared_locs( &amp;locs_buf[lsize * 1], lsize);
3280   buf.stubs()-&gt;initialize_shared_locs( &amp;locs_buf[lsize * 2], lsize);
3281   // Mark as scratch buffer.
3282   buf.consts()-&gt;set_scratch_emit();
3283   buf.insts()-&gt;set_scratch_emit();
3284   buf.stubs()-&gt;set_scratch_emit();
3285 
3286   // Do the emission.
3287 
3288   Label fakeL; // Fake label for branch instructions.
3289   Label*   saveL = NULL;
3290   uint save_bnum = 0;
3291   bool is_branch = n-&gt;is_MachBranch();
3292   if (is_branch) {
3293     MacroAssembler masm(&amp;buf);
3294     masm.bind(fakeL);
3295     n-&gt;as_MachBranch()-&gt;save_label(&amp;saveL, &amp;save_bnum);
3296     n-&gt;as_MachBranch()-&gt;label_set(&amp;fakeL, 0);
<span class="line-added">3297   } else if (n-&gt;is_MachProlog()) {</span>
<span class="line-added">3298     saveL = ((MachPrologNode*)n)-&gt;_verified_entry;</span>
<span class="line-added">3299     ((MachPrologNode*)n)-&gt;_verified_entry = &amp;fakeL;</span>
<span class="line-added">3300   } else if (n-&gt;is_MachVEP()) {</span>
<span class="line-added">3301     saveL = ((MachVEPNode*)n)-&gt;_verified_entry;</span>
<span class="line-added">3302     ((MachVEPNode*)n)-&gt;_verified_entry = &amp;fakeL;</span>
3303   }
3304   n-&gt;emit(buf, C-&gt;regalloc());
3305 
3306   // Emitting into the scratch buffer should not fail
3307   assert (!C-&gt;failing(), &quot;Must not have pending failure. Reason is: %s&quot;, C-&gt;failure_reason());
3308 
<span class="line-modified">3309   // Restore label.</span>
<span class="line-added">3310   if (is_branch) {</span>
3311     n-&gt;as_MachBranch()-&gt;label_set(saveL, save_bnum);
<span class="line-added">3312   } else if (n-&gt;is_MachProlog()) {</span>
<span class="line-added">3313     ((MachPrologNode*)n)-&gt;_verified_entry = saveL;</span>
<span class="line-added">3314   } else if (n-&gt;is_MachVEP()) {</span>
<span class="line-added">3315     ((MachVEPNode*)n)-&gt;_verified_entry = saveL;</span>
<span class="line-added">3316   }</span>
3317 
3318   // End scratch_emit_size section.
3319   set_in_scratch_emit_size(false);
3320 
3321   return buf.insts_size();
3322 }
3323 
3324 void PhaseOutput::install() {
3325   if (C-&gt;stub_function() != NULL) {
3326     install_stub(C-&gt;stub_name(),
3327                  C-&gt;save_argument_registers());
3328   } else {
3329     install_code(C-&gt;method(),
3330                  C-&gt;entry_bci(),
3331                  CompileBroker::compiler2(),
3332                  C-&gt;has_unsafe_access(),
3333                  SharedRuntime::is_wide_vector(C-&gt;max_vector_size()),
3334                  C-&gt;rtm_state());
3335   }
3336 }
</pre>
<hr />
<pre>
3339                                int               entry_bci,
3340                                AbstractCompiler* compiler,
3341                                bool              has_unsafe_access,
3342                                bool              has_wide_vectors,
3343                                RTMState          rtm_state) {
3344   // Check if we want to skip execution of all compiled code.
3345   {
3346 #ifndef PRODUCT
3347     if (OptoNoExecute) {
3348       C-&gt;record_method_not_compilable(&quot;+OptoNoExecute&quot;);  // Flag as failed
3349       return;
3350     }
3351 #endif
3352     Compile::TracePhase tp(&quot;install_code&quot;, &amp;timers[_t_registerMethod]);
3353 
3354     if (C-&gt;is_osr_compilation()) {
3355       _code_offsets.set_value(CodeOffsets::Verified_Entry, 0);
3356       _code_offsets.set_value(CodeOffsets::OSR_Entry, _first_block_size);
3357     } else {
3358       _code_offsets.set_value(CodeOffsets::Verified_Entry, _first_block_size);
<span class="line-added">3359       if (_code_offsets.value(CodeOffsets::Verified_Value_Entry) == -1) {</span>
<span class="line-added">3360         _code_offsets.set_value(CodeOffsets::Verified_Value_Entry, _first_block_size);</span>
<span class="line-added">3361       }</span>
<span class="line-added">3362       if (_code_offsets.value(CodeOffsets::Verified_Value_Entry_RO) == -1) {</span>
<span class="line-added">3363         _code_offsets.set_value(CodeOffsets::Verified_Value_Entry_RO, _first_block_size);</span>
<span class="line-added">3364       }</span>
<span class="line-added">3365       if (_code_offsets.value(CodeOffsets::Entry) == -1) {</span>
<span class="line-added">3366         _code_offsets.set_value(CodeOffsets::Entry, _first_block_size);</span>
<span class="line-added">3367       }</span>
3368       _code_offsets.set_value(CodeOffsets::OSR_Entry, 0);
3369     }
3370 
3371     C-&gt;env()-&gt;register_method(target,
<span class="line-modified">3372                               entry_bci,</span>
<span class="line-modified">3373                               &amp;_code_offsets,</span>
<span class="line-modified">3374                               _orig_pc_slot_offset_in_bytes,</span>
<span class="line-modified">3375                               code_buffer(),</span>
<span class="line-modified">3376                               frame_size_in_words(),</span>
<span class="line-modified">3377                               _oop_map_set,</span>
<span class="line-modified">3378                               &amp;_handler_table,</span>
<span class="line-modified">3379                               &amp;_inc_table,</span>
<span class="line-modified">3380                               compiler,</span>
<span class="line-modified">3381                               has_unsafe_access,</span>
<span class="line-modified">3382                               SharedRuntime::is_wide_vector(C-&gt;max_vector_size()),</span>
<span class="line-modified">3383                               C-&gt;rtm_state());</span>
3384 
3385     if (C-&gt;log() != NULL) { // Print code cache state into compiler log
3386       C-&gt;log()-&gt;code_cache_state();
3387     }
3388   }
3389 }
3390 void PhaseOutput::install_stub(const char* stub_name,
3391                                bool        caller_must_gc_arguments) {
3392   // Entry point will be accessed using stub_entry_point();
3393   if (code_buffer() == NULL) {
3394     Matcher::soft_match_failure();
3395   } else {
3396     if (PrintAssembly &amp;&amp; (WizardMode || Verbose))
3397       tty-&gt;print_cr(&quot;### Stub::%s&quot;, stub_name);
3398 
3399     if (!C-&gt;failing()) {
3400       assert(C-&gt;fixed_slots() == 0, &quot;no fixed slots used for runtime stubs&quot;);
3401 
3402       // Make the NMethod
3403       // For now we mark the frame as never safe for profile stackwalking
</pre>
</td>
</tr>
</table>
<center><a href="loopopts.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="subnode.hpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>