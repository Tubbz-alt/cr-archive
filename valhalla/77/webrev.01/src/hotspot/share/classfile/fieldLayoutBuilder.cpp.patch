diff a/src/hotspot/share/classfile/fieldLayoutBuilder.cpp b/src/hotspot/share/classfile/fieldLayoutBuilder.cpp
--- a/src/hotspot/share/classfile/fieldLayoutBuilder.cpp
+++ b/src/hotspot/share/classfile/fieldLayoutBuilder.cpp
@@ -58,11 +58,11 @@
  _offset(-1),
  _alignment(alignment),
  _size(size),
  _field_index(index),
  _is_reference(is_reference) {
-  assert(kind == REGULAR || kind == FLATTENED || kind == INHERITED,
+  assert(kind == REGULAR || kind == ALLOCATED_INLINE || kind == INHERITED,
          "Other kind do not have a field index");
   assert(size > 0, "Sanity check");
   assert(alignment > 0, "Sanity check");
 }
 
@@ -76,11 +76,11 @@
 
 FieldGroup::FieldGroup(int contended_group) :
   _next(NULL),
   _primitive_fields(NULL),
   _oop_fields(NULL),
-  _flattened_fields(NULL),
+  _fields_allocated_inline(NULL),
   _contended_group(contended_group),  // -1 means no contended group, 0 means default contended group
   _oop_count(0) {}
 
 void FieldGroup::add_primitive_field(AllFieldStream fs, BasicType type) {
   int size = type2aelembytes(type);
@@ -99,26 +99,26 @@
   }
   _oop_fields->append(block);
   _oop_count++;
 }
 
-void FieldGroup::add_flattened_field(AllFieldStream fs, ValueKlass* vk) {
-  // _flattened_fields list might be merged with the _primitive_fields list in the future
-  LayoutRawBlock* block = new LayoutRawBlock(fs.index(), LayoutRawBlock::FLATTENED, vk->get_exact_size_in_bytes(), vk->get_alignment(), false);
+void FieldGroup::add_field_allocated_inline(AllFieldStream fs, ValueKlass* vk) {
+  // _fields_allocated_inline list might be merged with the _primitive_fields list in the future
+  LayoutRawBlock* block = new LayoutRawBlock(fs.index(), LayoutRawBlock::ALLOCATED_INLINE, vk->get_exact_size_in_bytes(), vk->get_alignment(), false);
   block->set_value_klass(vk);
-  if (_flattened_fields == NULL) {
-    _flattened_fields = new(ResourceObj::RESOURCE_AREA, mtInternal) GrowableArray<LayoutRawBlock*>(INITIAL_LIST_SIZE);
+  if (_fields_allocated_inline == NULL) {
+    _fields_allocated_inline = new(ResourceObj::RESOURCE_AREA, mtInternal) GrowableArray<LayoutRawBlock*>(INITIAL_LIST_SIZE);
   }
-  _flattened_fields->append(block);
+  _fields_allocated_inline->append(block);
 }
 
 void FieldGroup::sort_by_size() {
   if (_primitive_fields != NULL) {
     _primitive_fields->sort(LayoutRawBlock::compare_size_inverted);
   }
-  if (_flattened_fields != NULL) {
-    _flattened_fields->sort(LayoutRawBlock::compare_size_inverted);
+  if (_fields_allocated_inline != NULL) {
+    _fields_allocated_inline->sort(LayoutRawBlock::compare_size_inverted);
   }
 }
 
 FieldLayout::FieldLayout(Array<u2>* fields, ConstantPool* cp) :
   _fields(fields),
@@ -164,11 +164,11 @@
 LayoutRawBlock* FieldLayout::first_field_block() {
   LayoutRawBlock* block = _blocks;
   while (block != NULL
          && block->kind() != LayoutRawBlock::INHERITED
          && block->kind() != LayoutRawBlock::REGULAR
-         && block->kind() != LayoutRawBlock::FLATTENED) {
+         && block->kind() != LayoutRawBlock::ALLOCATED_INLINE) {
     block = block->next_block();
   }
   return block;
 }
 
@@ -457,19 +457,19 @@
                        b->size(),
                        b->alignment(),
                        "REGULAR");
       break;
     }
-    case LayoutRawBlock::FLATTENED: {
+    case LayoutRawBlock::ALLOCATED_INLINE: {
       FieldInfo* fi = FieldInfo::from_field_array(_fields, b->field_index());
       output->print_cr(" @%d \"%s\" %s %d/%d %s",
                        b->offset(),
                        fi->name(_cp)->as_C_string(),
                        fi->signature(_cp)->as_C_string(),
                        b->size(),
                        b->alignment(),
-                       "FLATTENED");
+                       "ALLOCATED INLINE");
       break;
     }
     case LayoutRawBlock::RESERVED: {
       output->print_cr(" @%d %d/- %s",
                        b->offset(),
@@ -535,10 +535,11 @@
   _nonstatic_oopmap_count(0),
   _alignment(-1),
   _first_field_offset(-1),
   _exact_size_in_bytes(-1),
   _has_nonstatic_fields(false),
+  _has_inline_type_fields(false),
   _is_contended(is_contended),
   _is_inline_type(is_inline_type),
   _has_flattening_information(is_inline_type),
   _has_nonatomic_values(false),
   _atomic_field_count(0)
@@ -612,22 +613,23 @@
     case T_ARRAY:
       if (group != _static_fields) _nonstatic_oopmap_count++;
       group->add_oop_field(fs);
       break;
     case T_VALUETYPE:
+//      fs.set_inline(true);
+      _has_inline_type_fields = true;
       if (group == _static_fields) {
-        // static fields are never flattened
+        // static fields are never allocated inline
         group->add_oop_field(fs);
       } else {
         _has_flattening_information = true;
         // Flattening decision to be taken here
-        // This code assumes all verification have been performed before
-        // (field is a flattenable field, field's type has been loaded
-        // and it is an inline klass
+        // This code assumes all verification already have been performed
+        // (field's type has been loaded and it is an inline klass)
         Thread* THREAD = Thread::current();
         Klass* klass =
-            SystemDictionary::resolve_flattenable_field_or_fail(&fs,
+            SystemDictionary::resolve_inline_type_field_or_fail(&fs,
                                                                 Handle(THREAD, _class_loader_data->class_loader()),
                                                                 _protection_domain, true, THREAD);
         assert(klass != NULL, "Sanity check");
         ValueKlass* vk = ValueKlass::cast(klass);
         bool too_big_to_flatten = (InlineFieldMaxFlatSize >= 0 &&
@@ -635,16 +637,16 @@
         bool too_atomic_to_flatten = vk->is_declared_atomic();
         bool too_volatile_to_flatten = fs.access_flags().is_volatile();
         if (vk->is_naturally_atomic()) {
           too_atomic_to_flatten = false;
           //too_volatile_to_flatten = false; //FIXME
-          // volatile fields are currently never flattened, this could change in the future
+          // volatile fields are currently never allocated inline, this could change in the future
         }
         if (!(too_big_to_flatten | too_atomic_to_flatten | too_volatile_to_flatten)) {
-          group->add_flattened_field(fs, vk);
+          group->add_field_allocated_inline(fs, vk);
           _nonstatic_oopmap_count += vk->nonstatic_oop_map_count();
-          fs.set_flattened(true);
+          fs.set_allocated_inline(true);
           if (!vk->is_atomic()) {  // flat and non-atomic: take note
             _has_nonatomic_values = true;
             _atomic_field_count--;  // every other field is atomic but this one
           }
         } else {
@@ -672,11 +674,11 @@
  *     sharing issue)
  *   - this method also records the alignment of the field with the most
  *     constraining alignment, this value is then used as the alignment
  *     constraint when flattening this inline type into another container
  *   - field flattening decisions are taken in this method (those decisions are
- *     currently only based in the size of the fields to be flattened, the size
+ *     currently only based in the size of the fields to be allocated inline, the size
  *     of the resulting instance is not considered)
  */
 void FieldLayoutBuilder::inline_class_field_sorting(TRAPS) {
   assert(_is_inline_type, "Should only be used for inline classes");
   int alignment = 1;
@@ -713,21 +715,22 @@
         field_alignment = type2aelembytes(type); // alignment == size for oops
       }
       group->add_oop_field(fs);
       break;
     case T_VALUETYPE: {
+//      fs.set_inline(true);
+      _has_inline_type_fields = true;
       if (group == _static_fields) {
-        // static fields are never flattened
+        // static fields are never allocated inline
         group->add_oop_field(fs);
       } else {
         // Flattening decision to be taken here
-        // This code assumes all verifications have been performed before
-        // (field is a flattenable field, field's type has been loaded
-        // and it is an inline klass
+        // This code assumes all verifications have already been performed
+        // (field's type has been loaded and it is an inline klass)
         Thread* THREAD = Thread::current();
         Klass* klass =
-            SystemDictionary::resolve_flattenable_field_or_fail(&fs,
+            SystemDictionary::resolve_inline_type_field_or_fail(&fs,
                 Handle(THREAD, _class_loader_data->class_loader()),
                 _protection_domain, true, CHECK);
         assert(klass != NULL, "Sanity check");
         ValueKlass* vk = ValueKlass::cast(klass);
         bool too_big_to_flatten = (InlineFieldMaxFlatSize >= 0 &&
@@ -735,17 +738,17 @@
         bool too_atomic_to_flatten = vk->is_declared_atomic();
         bool too_volatile_to_flatten = fs.access_flags().is_volatile();
         if (vk->is_naturally_atomic()) {
           too_atomic_to_flatten = false;
           //too_volatile_to_flatten = false; //FIXME
-          // volatile fields are currently never flattened, this could change in the future
+          // volatile fields are currently never allocated inline, this could change in the future
         }
         if (!(too_big_to_flatten | too_atomic_to_flatten | too_volatile_to_flatten)) {
-          group->add_flattened_field(fs, vk);
+          group->add_field_allocated_inline(fs, vk);
           _nonstatic_oopmap_count += vk->nonstatic_oop_map_count();
           field_alignment = vk->get_alignment();
-          fs.set_flattened(true);
+          fs.set_allocated_inline(true);
           if (!vk->is_atomic()) {  // flat and non-atomic: take note
             _has_nonatomic_values = true;
             _atomic_field_count--;  // every other field is atomic but this one
           }
         } else {
@@ -778,14 +781,14 @@
   }
 }
 
 /* Computation of regular classes layout is an evolution of the previous default layout
  * (FieldAllocationStyle 1):
- *   - flattened fields are allocated first (because they have potentially the
+ *   - fields allocated inline are processed first (because they have potentially the
  *     least regular shapes, and are more likely to create empty slots between them,
  *     which can then be used to allocation primitive or oop fields). Allocation is
- *     performed from the biggest to the smallest flattened field.
+ *     performed from the biggest to the smallest field.
  *   - then primitive fields (from the biggest to the smallest)
  *   - then oop fields are allocated contiguously (to reduce the number of oopmaps
  *     and reduce the work of the GC).
  */
 void FieldLayoutBuilder::compute_regular_layout() {
@@ -797,30 +800,30 @@
     // insertion is currently easy because the current strategy doesn't try to fill holes
     // in super classes layouts => the _start block is by consequence the _last_block
     insert_contended_padding(_layout->start());
     need_tail_padding = true;
   }
-  _layout->add(_root_group->flattened_fields());
+  _layout->add(_root_group->fields_allocated_inline());
   _layout->add(_root_group->primitive_fields());
   _layout->add(_root_group->oop_fields());
 
   if (!_contended_groups.is_empty()) {
     for (int i = 0; i < _contended_groups.length(); i++) {
       FieldGroup* cg = _contended_groups.at(i);
       LayoutRawBlock* start = _layout->last_block();
       insert_contended_padding(start);
-      _layout->add(_root_group->flattened_fields());
+      _layout->add(_root_group->fields_allocated_inline());
       _layout->add(cg->primitive_fields(), start);
       _layout->add(cg->oop_fields(), start);
       need_tail_padding = true;
     }
   }
 
   if (need_tail_padding) {
     insert_contended_padding(_layout->last_block());
   }
-  _static_layout->add(_static_fields->flattened_fields());
+  _static_layout->add(_static_fields->fields_allocated_inline());
   _static_layout->add_contiguously(_static_fields->oop_fields());
   _static_layout->add(_static_fields->primitive_fields());
 
   epilogue();
 }
@@ -830,14 +833,14 @@
  * of the layout to increase GC performances. Unfortunately, this strategy
  * increases the number of empty slots inside an instance. Because the purpose
  * of inline classes is to be embedded into other containers, it is critical
  * to keep their size as small as possible. For this reason, the allocation
  * strategy is:
- *   - flattened fields are allocated first (because they have potentially the
+ *   - fields allocated inline are processed first (because they have potentially the
  *     least regular shapes, and are more likely to create empty slots between them,
  *     which can then be used to allocation primitive or oop fields). Allocation is
- *     performed from the biggest to the smallest flattened field.
+ *     performed from the biggest to the smallest field.
  *   - then oop fields are allocated contiguously (to reduce the number of oopmaps
  *     and reduce the work of the GC)
  *   - then primitive fields (from the biggest to the smallest)
  */
 void FieldLayoutBuilder::compute_inline_class_layout(TRAPS) {
@@ -853,11 +856,11 @@
     LayoutRawBlock* padding = new LayoutRawBlock(LayoutRawBlock::PADDING, _alignment - (first_empty->offset() % _alignment));
     _layout->insert(first_empty, padding);
     _layout->set_start(padding->next_block());
   }
 
-  _layout->add(_root_group->flattened_fields());
+  _layout->add(_root_group->fields_allocated_inline());
   _layout->add(_root_group->oop_fields());
   _layout->add(_root_group->primitive_fields());
 
   LayoutRawBlock* first_field = _layout->first_field_block();
    if (first_field != NULL) {
@@ -868,19 +871,19 @@
      _first_field_offset = _layout->blocks()->size();
      _exact_size_in_bytes = 0;
    }
   _exact_size_in_bytes = _layout->last_block()->offset() - _layout->first_field_block()->offset();
 
-  _static_layout->add(_static_fields->flattened_fields());
+  _static_layout->add(_static_fields->fields_allocated_inline());
   _static_layout->add_contiguously(_static_fields->oop_fields());
   _static_layout->add(_static_fields->primitive_fields());
 
 
   epilogue();
 }
 
-void FieldLayoutBuilder::add_flattened_field_oopmap(OopMapBlocksBuilder* nonstatic_oop_maps,
+void FieldLayoutBuilder::add_field__allocated_inline_oopmap(OopMapBlocksBuilder* nonstatic_oop_maps,
                 ValueKlass* vklass, int offset) {
   int diff = offset - vklass->first_field_offset();
   const OopMapBlock* map = vklass->start_of_nonstatic_oop_maps();
   const OopMapBlock* last_map = map + vklass->nonstatic_oop_map_count();
   while (map < last_map) {
@@ -906,18 +909,18 @@
       LayoutRawBlock* b = _root_group->oop_fields()->at(i);
       nonstatic_oop_maps->add(b->offset(), 1);
     }
   }
 
-  GrowableArray<LayoutRawBlock*>* ff = _root_group->flattened_fields();
+  GrowableArray<LayoutRawBlock*>* ff = _root_group->fields_allocated_inline();
   if (ff != NULL) {
     for (int i = 0; i < ff->length(); i++) {
       LayoutRawBlock* f = ff->at(i);
       ValueKlass* vk = f->value_klass();
       assert(vk != NULL, "Should have been initialized");
       if (vk->contains_oops()) {
-        add_flattened_field_oopmap(nonstatic_oop_maps, vk, f->offset());
+        add_field__allocated_inline_oopmap(nonstatic_oop_maps, vk, f->offset());
       }
     }
   }
 
   if (!_contended_groups.is_empty()) {
@@ -943,10 +946,11 @@
   _info->oop_map_blocks = nonstatic_oop_maps;
   _info->_instance_size = align_object_size(instance_end / wordSize);
   _info->_static_field_size = static_fields_size;
   _info->_nonstatic_field_size = (nonstatic_field_end - instanceOopDesc::base_offset_in_bytes()) / heapOopSize;
   _info->_has_nonstatic_fields = _has_nonstatic_fields;
+  _info->_has_inline_fields = _has_inline_type_fields;
 
   // An inline type is naturally atomic if it has just one field, and
   // that field is simple enough.
   _info->_is_naturally_atomic = (_is_inline_type &&
                                  (_atomic_field_count <= 1) &&
