diff a/src/hotspot/cpu/aarch64/macroAssembler_aarch64.cpp b/src/hotspot/cpu/aarch64/macroAssembler_aarch64.cpp
--- a/src/hotspot/cpu/aarch64/macroAssembler_aarch64.cpp
+++ b/src/hotspot/cpu/aarch64/macroAssembler_aarch64.cpp
@@ -1503,21 +1503,21 @@
   ldrw(temp_reg, Address(klass, Klass::access_flags_offset()));
   andr(temp_reg, temp_reg, JVM_ACC_VALUE);
   cbnz(temp_reg, is_value);
 }
 
-void MacroAssembler::test_field_is_flattenable(Register flags, Register temp_reg, Label& is_flattenable) {
+void MacroAssembler::test_field_is_inline_type(Register flags, Register temp_reg, Label& is_inline) {
   (void) temp_reg; // keep signature uniform with x86
-  tbnz(flags, ConstantPoolCacheEntry::is_flattenable_field_shift, is_flattenable);
+  tbnz(flags, ConstantPoolCacheEntry::is_inline_field_shift, is_inline);
 }
 
-void MacroAssembler::test_field_is_not_flattenable(Register flags, Register temp_reg, Label& not_flattenable) {
+void MacroAssembler::test_field_is_not_inline_type(Register flags, Register temp_reg, Label& not_inline) {
   (void) temp_reg; // keep signature uniform with x86
-  tbz(flags, ConstantPoolCacheEntry::is_flattenable_field_shift, not_flattenable);
+  tbz(flags, ConstantPoolCacheEntry::is_inline_field_shift, not_inline);
 }
 
-void MacroAssembler::test_field_is_flattened(Register flags, Register temp_reg, Label& is_flattened) {
+void MacroAssembler::test_field_is_inlined(Register flags, Register temp_reg, Label& is_flattened) {
   (void) temp_reg; // keep signature uniform with x86
   tbnz(flags, ConstantPoolCacheEntry::is_flattened_field_shift, is_flattened);
 }
 
 void MacroAssembler::test_flattened_array_oop(Register oop, Register temp_reg, Label& is_flattened_array) {
diff a/src/hotspot/cpu/aarch64/macroAssembler_aarch64.hpp b/src/hotspot/cpu/aarch64/macroAssembler_aarch64.hpp
--- a/src/hotspot/cpu/aarch64/macroAssembler_aarch64.hpp
+++ b/src/hotspot/cpu/aarch64/macroAssembler_aarch64.hpp
@@ -613,13 +613,13 @@
   static bool needs_explicit_null_check(intptr_t offset);
   static bool uses_implicit_null_check(void* address);
 
   void test_klass_is_value(Register klass, Register temp_reg, Label& is_value);
 
-  void test_field_is_flattenable(Register flags, Register temp_reg, Label& is_flattenable);
-  void test_field_is_not_flattenable(Register flags, Register temp_reg, Label& notFlattenable);
-  void test_field_is_flattened(Register flags, Register temp_reg, Label& is_flattened);
+  void test_field_is_inline_type(Register flags, Register temp_reg, Label& is_inline);
+  void test_field_is_not_inline_type(Register flags, Register temp_reg, Label& not_inline);
+  void test_field_is_inlined(Register flags, Register temp_reg, Label& is_flattened);
 
   // Check klass/oops is flat value type array (oop->_klass->_layout_helper & vt_bit)
   void test_flattened_array_oop(Register klass, Register temp_reg, Label& is_flattened_array);
   void test_null_free_array_oop(Register oop, Register temp_reg, Label& is_null_free_array);
 
diff a/src/hotspot/cpu/aarch64/templateTable_aarch64.cpp b/src/hotspot/cpu/aarch64/templateTable_aarch64.cpp
--- a/src/hotspot/cpu/aarch64/templateTable_aarch64.cpp
+++ b/src/hotspot/cpu/aarch64/templateTable_aarch64.cpp
@@ -2665,55 +2665,55 @@
     __ b(Done);
   } else { // Valhalla
 
     if (is_static) {
       __ load_heap_oop(r0, field);
-      Label isFlattenable, isUninitialized;
+      Label is_inline, isUninitialized;
       // Issue below if the static field has not been initialized yet
-      __ test_field_is_flattenable(raw_flags, r8 /*temp*/, isFlattenable);
-        // Not flattenable case
+      __ test_field_is_inline_type(raw_flags, r8 /*temp*/, is_inline);
+        // Not inline case
         __ push(atos);
         __ b(Done);
-      // Flattenable case, must not return null even if uninitialized
-      __ bind(isFlattenable);
+      // Inline case, must not return null even if uninitialized
+      __ bind(is_inline);
         __ cbz(r0, isUninitialized);
           __ push(atos);
           __ b(Done);
         __ bind(isUninitialized);
           __ andw(raw_flags, raw_flags, ConstantPoolCacheEntry::field_index_mask);
           __ call_VM(r0, CAST_FROM_FN_PTR(address, InterpreterRuntime::uninitialized_static_value_field), obj, raw_flags);
           __ verify_oop(r0);
           __ push(atos);
           __ b(Done);
     } else {
-      Label isFlattened, isInitialized, isFlattenable, rewriteFlattenable;
-        __ test_field_is_flattenable(raw_flags, r8 /*temp*/, isFlattenable);
-        // Non-flattenable field case, also covers the object case
+      Label isFlattened, isInitialized, is_inline, rewrite_inline;
+        __ test_field_is_inline_type(raw_flags, r8 /*temp*/, is_inline);
+        // Non-inline field case
         __ load_heap_oop(r0, field);
         __ push(atos);
         if (rc == may_rewrite) {
           patch_bytecode(Bytecodes::_fast_agetfield, bc, r1);
         }
         __ b(Done);
-      __ bind(isFlattenable);
-        __ test_field_is_flattened(raw_flags, r8 /* temp */, isFlattened);
-         // Non-flattened field case
+      __ bind(is_inline);
+        __ test_field_is_inlined(raw_flags, r8 /* temp */, isFlattened);
+         // Non-inline field case
           __ load_heap_oop(r0, field);
           __ cbnz(r0, isInitialized);
             __ andw(raw_flags, raw_flags, ConstantPoolCacheEntry::field_index_mask);
             __ call_VM(r0, CAST_FROM_FN_PTR(address, InterpreterRuntime::uninitialized_instance_value_field), obj, raw_flags);
           __ bind(isInitialized);
           __ verify_oop(r0);
           __ push(atos);
-          __ b(rewriteFlattenable);
+          __ b(rewrite_inline);
         __ bind(isFlattened);
           __ ldr(r10, Address(cache, in_bytes(ConstantPoolCache::base_offset() + ConstantPoolCacheEntry::f1_offset())));
           __ andw(raw_flags, raw_flags, ConstantPoolCacheEntry::field_index_mask);
           call_VM(r0, CAST_FROM_FN_PTR(address, InterpreterRuntime::read_flattened_field), obj, raw_flags, r10);
           __ verify_oop(r0);
           __ push(atos);
-      __ bind(rewriteFlattenable);
+      __ bind(rewrite_inline);
       if (rc == may_rewrite) {
          patch_bytecode(Bytecodes::_fast_qgetfield, bc, r1);
       }
       __ b(Done);
     }
@@ -2965,41 +2965,41 @@
       __ b(Done);
      } else { // Valhalla
 
       __ pop(atos);
       if (is_static) {
-        Label notFlattenable;
-         __ test_field_is_not_flattenable(flags2, r8 /* temp */, notFlattenable);
+        Label not_inline;
+         __ test_field_is_not_inline_type(flags2, r8 /* temp */, not_inline);
          __ null_check(r0);
-         __ bind(notFlattenable);
+         __ bind(not_inline);
          do_oop_store(_masm, field, r0, IN_HEAP);
          __ b(Done);
       } else {
-        Label isFlattenable, isFlattened, notBuffered, notBuffered2, rewriteNotFlattenable, rewriteFlattenable;
-        __ test_field_is_flattenable(flags2, r8 /*temp*/, isFlattenable);
-        // Not flattenable case, covers not flattenable values and objects
+        Label is_inline, isFlattened, rewrite_not_inline, rewrite_inline;
+        __ test_field_is_inline_type(flags2, r8 /*temp*/, is_inline);
+        // Not inline case
         pop_and_check_object(obj);
         // Store into the field
         do_oop_store(_masm, field, r0, IN_HEAP);
-        __ bind(rewriteNotFlattenable);
+        __ bind(rewrite_not_inline);
         if (rc == may_rewrite) {
           patch_bytecode(Bytecodes::_fast_aputfield, bc, r19, true, byte_no);
         }
         __ b(Done);
-        // Implementation of the flattenable semantic
-        __ bind(isFlattenable);
+        // Implementation of the inline semantic
+        __ bind(is_inline);
         __ null_check(r0);
-        __ test_field_is_flattened(flags2, r8 /*temp*/, isFlattened);
-        // Not flattened case
+        __ test_field_is_inlined(flags2, r8 /*temp*/, isFlattened);
+        // Not inline case
         pop_and_check_object(obj);
         // Store into the field
         do_oop_store(_masm, field, r0, IN_HEAP);
-        __ b(rewriteFlattenable);
+        __ b(rewrite_inline);
         __ bind(isFlattened);
         pop_and_check_object(obj);
         call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::write_flattened_value), r0, off, obj);
-        __ bind(rewriteFlattenable);
+        __ bind(rewrite_inline);
         if (rc == may_rewrite) {
           patch_bytecode(Bytecodes::_fast_qputfield, bc, r19, true, byte_no);
         }
         __ b(Done);
       }
@@ -3330,11 +3330,11 @@
   case Bytecodes::_fast_qgetfield:
     {
        Label isFlattened, isInitialized, Done;
        // FIXME: We don't need to reload registers multiple times, but stay close to x86 code
        __ ldrw(r9, Address(r2, in_bytes(ConstantPoolCache::base_offset() + ConstantPoolCacheEntry::flags_offset())));
-       __ test_field_is_flattened(r9, r8 /* temp */, isFlattened);
+       __ test_field_is_inlined(r9, r8 /* temp */, isFlattened);
         // Non-flattened field case
         __ mov(r9, r0);
         __ load_heap_oop(r0, field);
         __ cbnz(r0, isInitialized);
           __ mov(r0, r9);
diff a/src/hotspot/cpu/x86/interp_masm_x86.cpp b/src/hotspot/cpu/x86/interp_masm_x86.cpp
--- a/src/hotspot/cpu/x86/interp_masm_x86.cpp
+++ b/src/hotspot/cpu/x86/interp_masm_x86.cpp
@@ -1213,11 +1213,11 @@
     pop(atos);
   }
 }
 
 
-void InterpreterMacroAssembler::read_flattened_field(Register holder_klass,
+void InterpreterMacroAssembler::read_inlined_field(Register holder_klass,
                                                      Register field_index, Register field_offset,
                                                      Register obj) {
   Label alloc_failed, empty_value, done;
   const Register src = field_offset;
   const Register alloc_temp = LP64_ONLY(rscratch1) NOT_LP64(rsi);
@@ -1253,11 +1253,11 @@
   jmp(done);
 
   bind(alloc_failed);
   pop(obj);
   pop(holder_klass);
-  call_VM(obj, CAST_FROM_FN_PTR(address, InterpreterRuntime::read_flattened_field),
+  call_VM(obj, CAST_FROM_FN_PTR(address, InterpreterRuntime::read_inlined_field),
           obj, field_index, holder_klass);
 
   bind(done);
 }
 
diff a/src/hotspot/cpu/x86/interp_masm_x86.hpp b/src/hotspot/cpu/x86/interp_masm_x86.hpp
--- a/src/hotspot/cpu/x86/interp_masm_x86.hpp
+++ b/src/hotspot/cpu/x86/interp_masm_x86.hpp
@@ -225,17 +225,17 @@
 
   // Kills t1 and t2, perserves klass, return allocation in new_obj
   void allocate_instance(Register klass, Register new_obj,
                          Register t1, Register t2,
                          bool clear_fields, Label& alloc_failed);
-  // Allocate value buffer in "obj" and read in flattened field
+  // Allocate instance in "obj" and read in the content of the inline field
   // NOTES:
   //   - input holder object via "obj", which must be rax,
-  //     will return new value buffer obj via the same reg
+  //     will return new instance via the same reg
   //   - assumes holder_klass and valueKlass field klass have both been resolved
   //   - 32 bits: kills rdi and rsi
-  void read_flattened_field(Register holder_klass,
+  void read_inlined_field(Register holder_klass,
                             Register field_index, Register field_offset,
                             Register obj = rax);
 
   // Allocate value buffer in "obj" and read in flattened element at the given index
   // NOTES:
diff a/src/hotspot/cpu/x86/macroAssembler_x86.cpp b/src/hotspot/cpu/x86/macroAssembler_x86.cpp
--- a/src/hotspot/cpu/x86/macroAssembler_x86.cpp
+++ b/src/hotspot/cpu/x86/macroAssembler_x86.cpp
@@ -2635,32 +2635,32 @@
   movl(temp_reg, Address(klass, InstanceKlass::misc_flags_offset()));
   testl(temp_reg, InstanceKlass::misc_flags_is_empty_inline_type());
   jcc(Assembler::notZero, is_empty_value);
 }
 
-void MacroAssembler::test_field_is_flattenable(Register flags, Register temp_reg, Label& is_flattenable) {
+void MacroAssembler::test_field_is_inline_type(Register flags, Register temp_reg, Label& is_inline_type) {
   movl(temp_reg, flags);
-  shrl(temp_reg, ConstantPoolCacheEntry::is_flattenable_field_shift);
+  shrl(temp_reg, ConstantPoolCacheEntry::is_inline_type_shift);
   andl(temp_reg, 0x1);
   testl(temp_reg, temp_reg);
-  jcc(Assembler::notZero, is_flattenable);
+  jcc(Assembler::notZero, is_inline_type);
 }
 
-void MacroAssembler::test_field_is_not_flattenable(Register flags, Register temp_reg, Label& notFlattenable) {
+void MacroAssembler::test_field_is_not_inline_type(Register flags, Register temp_reg, Label& not_inline_type) {
   movl(temp_reg, flags);
-  shrl(temp_reg, ConstantPoolCacheEntry::is_flattenable_field_shift);
+  shrl(temp_reg, ConstantPoolCacheEntry::is_inline_type_shift);
   andl(temp_reg, 0x1);
   testl(temp_reg, temp_reg);
-  jcc(Assembler::zero, notFlattenable);
+  jcc(Assembler::zero, not_inline_type);
 }
 
-void MacroAssembler::test_field_is_flattened(Register flags, Register temp_reg, Label& is_flattened) {
+void MacroAssembler::test_field_is_inlined(Register flags, Register temp_reg, Label& is_inlined) {
   movl(temp_reg, flags);
-  shrl(temp_reg, ConstantPoolCacheEntry::is_flattened_field_shift);
+  shrl(temp_reg, ConstantPoolCacheEntry::is_inlined_shift);
   andl(temp_reg, 0x1);
   testl(temp_reg, temp_reg);
-  jcc(Assembler::notZero, is_flattened);
+  jcc(Assembler::notZero, is_inlined);
 }
 
 void MacroAssembler::test_flattened_array_oop(Register oop, Register temp_reg,
                                               Label&is_flattened_array) {
   Register tmp_load_klass = LP64_ONLY(rscratch1) NOT_LP64(noreg);
diff a/src/hotspot/cpu/x86/macroAssembler_x86.hpp b/src/hotspot/cpu/x86/macroAssembler_x86.hpp
--- a/src/hotspot/cpu/x86/macroAssembler_x86.hpp
+++ b/src/hotspot/cpu/x86/macroAssembler_x86.hpp
@@ -109,13 +109,13 @@
   void get_default_value_oop(Register value_klass, Register temp_reg, Register obj);
   // The empty value oop, for the given ValueKlass ("empty" as in no instance fields)
   // get_default_value_oop with extra assertion for empty value klass
   void get_empty_value_oop(Register value_klass, Register temp_reg, Register obj);
 
-  void test_field_is_flattenable(Register flags, Register temp_reg, Label& is_flattenable);
-  void test_field_is_not_flattenable(Register flags, Register temp_reg, Label& notFlattenable);
-  void test_field_is_flattened(Register flags, Register temp_reg, Label& is_flattened);
+  void test_field_is_inline_type(Register flags, Register temp_reg, Label& is_inline);
+  void test_field_is_not_inline_type(Register flags, Register temp_reg, Label& not_inline);
+  void test_field_is_inlined(Register flags, Register temp_reg, Label& is_inlined);
 
   // Check oops array storage properties, i.e. flattened and/or null-free
   void test_flattened_array_oop(Register oop, Register temp_reg, Label&is_flattened_array);
   void test_non_flattened_array_oop(Register oop, Register temp_reg, Label&is_non_flattened_array);
   void test_null_free_array_oop(Register oop, Register temp_reg, Label&is_null_free_array);
diff a/src/hotspot/cpu/x86/templateTable_x86.cpp b/src/hotspot/cpu/x86/templateTable_x86.cpp
--- a/src/hotspot/cpu/x86/templateTable_x86.cpp
+++ b/src/hotspot/cpu/x86/templateTable_x86.cpp
@@ -3066,18 +3066,18 @@
     }
     __ jmp(Done);
   } else {
     if (is_static) {
       __ load_heap_oop(rax, field);
-      Label isFlattenable, uninitialized;
+      Label is_inline_type, uninitialized;
       // Issue below if the static field has not been initialized yet
-      __ test_field_is_flattenable(flags2, rscratch1, isFlattenable);
-        // Not flattenable case
+      __ test_field_is_inline_type(flags2, rscratch1, is_inline_type);
+        // field is not an inline type
         __ push(atos);
         __ jmp(Done);
-      // Flattenable case, must not return null even if uninitialized
-      __ bind(isFlattenable);
+      // field is an inline type, must not return null even if uninitialized
+      __ bind(is_inline_type);
         __ testptr(rax, rax);
         __ jcc(Assembler::zero, uninitialized);
           __ push(atos);
           __ jmp(Done);
         __ bind(uninitialized);
@@ -3097,23 +3097,23 @@
 #endif // _LP64
           __ verify_oop(rax);
           __ push(atos);
           __ jmp(Done);
     } else {
-      Label isFlattened, nonnull, isFlattenable, rewriteFlattenable;
-      __ test_field_is_flattenable(flags2, rscratch1, isFlattenable);
-        // Non-flattenable field case, also covers the object case
+      Label is_inlined, nonnull, is_inline_type, rewrite_inline;
+      __ test_field_is_inline_type(flags2, rscratch1, is_inline_type);
+        // field is not an inline type
         pop_and_check_object(obj);
         __ load_heap_oop(rax, field);
         __ push(atos);
         if (rc == may_rewrite) {
           patch_bytecode(Bytecodes::_fast_agetfield, bc, rbx);
         }
         __ jmp(Done);
-      __ bind(isFlattenable);
-        __ test_field_is_flattened(flags2, rscratch1, isFlattened);
-          // Non-flattened field case
+      __ bind(is_inline_type);
+        __ test_field_is_inlined(flags2, rscratch1, is_inlined);
+          // field is not inlined
           __ movptr(rax, rcx);  // small dance required to preserve the klass_holder somewhere
           pop_and_check_object(obj);
           __ push(rax);
           __ load_heap_oop(rax, field);
           __ pop(rcx);
@@ -3123,18 +3123,19 @@
             __ get_value_field_klass(rcx, flags2, rbx);
             __ get_default_value_oop(rbx, rcx, rax);
           __ bind(nonnull);
           __ verify_oop(rax);
           __ push(atos);
-          __ jmp(rewriteFlattenable);
-        __ bind(isFlattened);
+          __ jmp(rewrite_inline);
+        __ bind(is_inlined);
+        // field is inlined
           __ andl(flags2, ConstantPoolCacheEntry::field_index_mask);
           pop_and_check_object(rax);
-          __ read_flattened_field(rcx, flags2, rbx, rax);
+          __ read_inlined_field(rcx, flags2, rbx, rax);
           __ verify_oop(rax);
           __ push(atos);
-      __ bind(rewriteFlattenable);
+      __ bind(rewrite_inline);
       if (rc == may_rewrite) {
         patch_bytecode(Bytecodes::_fast_qgetfield, bc, rbx);
       }
       __ jmp(Done);
     }
@@ -3445,45 +3446,46 @@
       }
       __ jmp(Done);
     } else {
       __ pop(atos);
       if (is_static) {
-        Label notFlattenable, notBuffered;
-        __ test_field_is_not_flattenable(flags2, rscratch1, notFlattenable);
+        Label is_inline_type;
+        __ test_field_is_not_inline_type(flags2, rscratch1, is_inline_type);
         __ null_check(rax);
-        __ bind(notFlattenable);
+        __ bind(is_inline_type);
         do_oop_store(_masm, field, rax);
         __ jmp(Done);
       } else {
-        Label isFlattenable, isFlattened, notBuffered, notBuffered2, rewriteNotFlattenable, rewriteFlattenable;
-        __ test_field_is_flattenable(flags2, rscratch1, isFlattenable);
-        // Not flattenable case, covers not flattenable values and objects
+        Label is_inline_type, is_inlined, rewrite_not_inline, rewrite_inline;
+        __ test_field_is_inline_type(flags2, rscratch1, is_inline_type);
+        // Not an inline type
         pop_and_check_object(obj);
         // Store into the field
         do_oop_store(_masm, field, rax);
-        __ bind(rewriteNotFlattenable);
+        __ bind(rewrite_not_inline);
         if (rc == may_rewrite) {
           patch_bytecode(Bytecodes::_fast_aputfield, bc, rbx, true, byte_no);
         }
         __ jmp(Done);
-        // Implementation of the flattenable semantic
-        __ bind(isFlattenable);
+        // Implementation of the inline type semantic
+        __ bind(is_inline_type);
         __ null_check(rax);
-        __ test_field_is_flattened(flags2, rscratch1, isFlattened);
-        // Not flattened case
+        __ test_field_is_inlined(flags2, rscratch1, is_inlined);
+        // field is not inlined
         pop_and_check_object(obj);
         // Store into the field
         do_oop_store(_masm, field, rax);
-        __ jmp(rewriteFlattenable);
-        __ bind(isFlattened);
+        __ jmp(rewrite_inline);
+        __ bind(is_inlined);
+        // field is inlined
         pop_and_check_object(obj);
         assert_different_registers(rax, rdx, obj, off);
         __ load_klass(rdx, rax, rscratch1);
         __ data_for_oop(rax, rax, rdx);
         __ addptr(obj, off);
         __ access_value_copy(IN_HEAP, rax, obj, rdx);
-        __ bind(rewriteFlattenable);
+        __ bind(rewrite_inline);
         if (rc == may_rewrite) {
           patch_bytecode(Bytecodes::_fast_qputfield, bc, rbx, true, byte_no);
         }
         __ jmp(Done);
       }
@@ -3691,11 +3693,11 @@
   // volatile_barrier(Assembler::Membar_mask_bits(Assembler::LoadStore |
   //                                              Assembler::StoreStore));
 
   Label notVolatile, Done;
   if (bytecode() == Bytecodes::_fast_qputfield) {
-    __ movl(rscratch2, rdx);  // saving flags for isFlattened test
+    __ movl(rscratch2, rdx);  // saving flags for is_inlined test
   }
 
   __ shrl(rdx, ConstantPoolCacheEntry::is_volatile_shift);
   __ andl(rdx, 0x1);
 
@@ -3708,20 +3710,20 @@
   // Check for volatile store
   __ testl(rdx, rdx);
   __ jcc(Assembler::zero, notVolatile);
 
   if (bytecode() == Bytecodes::_fast_qputfield) {
-    __ movl(rdx, rscratch2);  // restoring flags for isFlattened test
+    __ movl(rdx, rscratch2);  // restoring flags for is_inlined test
   }
   fast_storefield_helper(field, rax, rdx);
   volatile_barrier(Assembler::Membar_mask_bits(Assembler::StoreLoad |
                                                Assembler::StoreStore));
   __ jmp(Done);
   __ bind(notVolatile);
 
   if (bytecode() == Bytecodes::_fast_qputfield) {
-    __ movl(rdx, rscratch2);  // restoring flags for isFlattened test
+    __ movl(rdx, rscratch2);  // restoring flags for is_inlined test
   }
   fast_storefield_helper(field, rax, rdx);
 
   __ bind(Done);
 }
@@ -3730,18 +3732,18 @@
 
   // access field
   switch (bytecode()) {
   case Bytecodes::_fast_qputfield:
     {
-      Label isFlattened, done;
+      Label is_inlined, done;
       __ null_check(rax);
-      __ test_field_is_flattened(flags, rscratch1, isFlattened);
-      // No Flattened case
+      __ test_field_is_inlined(flags, rscratch1, is_inlined);
+      // field is not inlined
       do_oop_store(_masm, field, rax);
       __ jmp(done);
-      __ bind(isFlattened);
-      // Flattened case
+      __ bind(is_inlined);
+      // field is inlined
       __ load_klass(rdx, rax, rscratch1);
       __ data_for_oop(rax, rax, rdx);
       __ lea(rcx, field);
       __ access_value_copy(IN_HEAP, rax, rcx, rdx);
       __ bind(done);
@@ -3831,16 +3833,16 @@
 
   // access field
   switch (bytecode()) {
   case Bytecodes::_fast_qgetfield:
     {
-      Label isFlattened, nonnull, Done;
+      Label is_inlined, nonnull, Done;
       __ movptr(rscratch1, Address(rcx, rbx, Address::times_ptr,
                                    in_bytes(ConstantPoolCache::base_offset() +
                                             ConstantPoolCacheEntry::flags_offset())));
-      __ test_field_is_flattened(rscratch1, rscratch2, isFlattened);
-        // Non-flattened field case
+      __ test_field_is_inlined(rscratch1, rscratch2, is_inlined);
+        // field is not inlined
         __ load_heap_oop(rax, field);
         __ testptr(rax, rax);
         __ jcc(Assembler::notZero, nonnull);
           __ movl(rdx, Address(rcx, rbx, Address::times_ptr,
                              in_bytes(ConstantPoolCache::base_offset() +
@@ -3852,21 +3854,22 @@
           __ get_value_field_klass(rcx, rdx, rbx);
           __ get_default_value_oop(rbx, rcx, rax);
         __ bind(nonnull);
         __ verify_oop(rax);
         __ jmp(Done);
-      __ bind(isFlattened);
+      __ bind(is_inlined);
+      // fiel is inlined
         __ push(rdx); // save offset
         __ movl(rdx, Address(rcx, rbx, Address::times_ptr,
                            in_bytes(ConstantPoolCache::base_offset() +
                                     ConstantPoolCacheEntry::flags_offset())));
         __ andl(rdx, ConstantPoolCacheEntry::field_index_mask);
         __ movptr(rcx, Address(rcx, rbx, Address::times_ptr,
                                      in_bytes(ConstantPoolCache::base_offset() +
                                               ConstantPoolCacheEntry::f1_offset())));
         __ pop(rbx); // restore offset
-        __ read_flattened_field(rcx, rdx, rbx, rax);
+        __ read_inlined_field(rcx, rdx, rbx, rax);
       __ bind(Done);
       __ verify_oop(rax);
     }
     break;
   case Bytecodes::_fast_agetfield:
diff a/src/hotspot/share/ci/ciField.cpp b/src/hotspot/share/ci/ciField.cpp
--- a/src/hotspot/share/ci/ciField.cpp
+++ b/src/hotspot/share/ci/ciField.cpp
@@ -280,12 +280,12 @@
   _flags = ciFlags(fd->access_flags());
   _offset = fd->offset();
   Klass* field_holder = fd->field_holder();
   assert(field_holder != NULL, "null field_holder");
   _holder = CURRENT_ENV->get_instance_klass(field_holder);
-  _is_flattened = fd->is_flattened();
-  _is_flattenable = fd->is_flattenable();
+  _is_flattened = fd->is_inlined();
+  _is_flattenable = fd->is_inline_type();
 
   // Check to see if the field is constant.
   Klass* k = _holder->get_Klass();
   bool is_stable_field = FoldStableValues && is_stable();
   if ((is_final() && !has_initialized_final_update()) || is_stable_field) {
diff a/src/hotspot/share/ci/ciInstanceKlass.cpp b/src/hotspot/share/ci/ciInstanceKlass.cpp
--- a/src/hotspot/share/ci/ciInstanceKlass.cpp
+++ b/src/hotspot/share/ci/ciInstanceKlass.cpp
@@ -546,11 +546,11 @@
   }
 
   for (JavaFieldStream fs(k); !fs.done(); fs.next()) {
     if (fs.access_flags().is_static())  continue;
     fieldDescriptor& fd = fs.field_descriptor();
-    if (fd.is_flattened() && flatten) {
+    if (fd.is_inlined() && flatten) {
       // Value type fields are embedded
       int field_offset = fd.offset();
       // Get ValueKlass and adjust number of fields
       Klass* k = get_instanceKlass()->get_value_field_klass(fd.index());
       ciValueKlass* vk = CURRENT_ENV->get_klass(k)->as_value_klass();
diff a/src/hotspot/share/ci/ciReplay.cpp b/src/hotspot/share/ci/ciReplay.cpp
--- a/src/hotspot/share/ci/ciReplay.cpp
+++ b/src/hotspot/share/ci/ciReplay.cpp
@@ -851,11 +851,11 @@
         assert(res, "should succeed for arrays & objects");
         break;
       }
       case T_VALUETYPE: {
         ValueKlass* vk = ValueKlass::cast(fd->field_holder()->get_value_field_klass(fd->index()));
-        if (fd->is_flattened()) {
+        if (fd->is_inlined()) {
           int field_offset = fd->offset() - vk->first_field_offset();
           oop obj = (oop)(cast_from_oop<address>(_vt) + field_offset);
           ValueTypeFieldInitializer init_fields(obj, _replay);
           vk->do_nonstatic_fields(&init_fields);
         } else {
diff a/src/hotspot/share/classfile/classFileParser.cpp b/src/hotspot/share/classfile/classFileParser.cpp
--- a/src/hotspot/share/classfile/classFileParser.cpp
+++ b/src/hotspot/share/classfile/classFileParser.cpp
@@ -1516,17 +1516,17 @@
   STATIC_OOP,           // Oops
   STATIC_BYTE,          // Boolean, Byte, char
   STATIC_SHORT,         // shorts
   STATIC_WORD,          // ints
   STATIC_DOUBLE,        // aligned long or double
-  STATIC_FLATTENABLE,   // flattenable field
+  STATIC_INLINE,        // inline type field
   NONSTATIC_OOP,
   NONSTATIC_BYTE,
   NONSTATIC_SHORT,
   NONSTATIC_WORD,
   NONSTATIC_DOUBLE,
-  NONSTATIC_FLATTENABLE,
+  NONSTATIC_INLINE,
   MAX_FIELD_ALLOCATION_TYPE,
   BAD_ALLOCATION_TYPE = -1
 };
 
 static FieldAllocationType _basic_type_to_atype[2 * (T_CONFLICT + 1)] = {
@@ -1572,16 +1572,16 @@
   BAD_ALLOCATION_TYPE, // T_METADATA    = 18,
   BAD_ALLOCATION_TYPE, // T_NARROWKLASS = 19,
   BAD_ALLOCATION_TYPE, // T_CONFLICT    = 20
 };
 
-static FieldAllocationType basic_type_to_atype(bool is_static, BasicType type, bool is_flattenable) {
+static FieldAllocationType basic_type_to_atype(bool is_static, BasicType type, bool is_inline_type) {
   assert(type >= T_BOOLEAN && type < T_VOID, "only allowable values");
   FieldAllocationType result = _basic_type_to_atype[type + (is_static ? (T_CONFLICT + 1) : 0)];
   assert(result != BAD_ALLOCATION_TYPE, "bad type");
-  if (is_flattenable) {
-    result = is_static ? STATIC_FLATTENABLE : NONSTATIC_FLATTENABLE;
+  if (is_inline_type) {
+    result = is_static ? STATIC_INLINE : NONSTATIC_INLINE;
   }
   return result;
 }
 
 class ClassFileParser::FieldAllocationCount : public ResourceObj {
@@ -1592,12 +1592,12 @@
     for (int i = 0; i < MAX_FIELD_ALLOCATION_TYPE; i++) {
       count[i] = 0;
     }
   }
 
-  FieldAllocationType update(bool is_static, BasicType type, bool is_flattenable) {
-    FieldAllocationType atype = basic_type_to_atype(is_static, type, is_flattenable);
+  FieldAllocationType update(bool is_static, BasicType type, bool is_inline_type) {
+    FieldAllocationType atype = basic_type_to_atype(is_static, type, is_inline_type);
     if (atype != BAD_ALLOCATION_TYPE) {
       // Make sure there is no overflow with injected fields.
       assert(count[atype] < 0xFFFF, "More than 65535 fields");
       count[atype]++;
     }
@@ -1689,26 +1689,10 @@
     check_property(valid_symbol_at(signature_index),
       "Invalid constant pool index %u for field signature in class file %s",
       signature_index, CHECK);
     const Symbol* const sig = cp->symbol_at(signature_index);
     verify_legal_field_signature(name, sig, CHECK);
-    assert(!access_flags.is_flattenable(), "ACC_FLATTENABLE should have been filtered out");
-    if (sig->is_Q_signature()) {
-      // assert(_major_version >= CONSTANT_CLASS_DESCRIPTORS, "Q-descriptors are only supported in recent classfiles");
-      access_flags.set_is_flattenable();
-    }
-    if (access_flags.is_flattenable()) {
-      // Array flattenability cannot be specified.  Arrays of value classes are
-      // are always flattenable.  Arrays of other classes are not flattenable.
-      if (sig->utf8_length() > 1 && sig->char_at(0) == '[') {
-        classfile_parse_error(
-            "Field \"%s\" with signature \"%s\" in class file %s is invalid."
-            " ACC_FLATTENABLE cannot be specified for an array",
-            name->as_C_string(), sig->as_klass_external_name(), CHECK);
-      }
-      _has_flattenable_fields = true;
-    }
     if (!access_flags.is_static()) instance_fields_count++;
 
     u2 constantvalue_index = 0;
     bool is_synthetic = false;
     u2 generic_signature_index = 0;
@@ -1765,11 +1749,11 @@
                       signature_index,
                       constantvalue_index);
     const BasicType type = cp->basic_type_for_signature_at(signature_index);
 
     // Remember how many oops we encountered and compute allocation type
-    const FieldAllocationType atype = fac->update(is_static, type, access_flags.is_flattenable());
+    const FieldAllocationType atype = fac->update(is_static, type, type == T_VALUETYPE);
     field->set_allocation_type(atype);
 
     // After field is initialized with type, we can augment it with aux info
     if (parsed_annotations.has_any_annotations()) {
       parsed_annotations.apply_to(field);
@@ -4403,11 +4387,11 @@
 
   // Calculate the starting byte offsets
   int next_static_oop_offset    = InstanceMirrorKlass::offset_of_static_fields();
   // Inline types in static fields are not embedded, they are handled with oops
   int next_static_double_offset = next_static_oop_offset +
-                                  ((fac->count[STATIC_OOP] + fac->count[STATIC_FLATTENABLE]) * heapOopSize);
+                                  ((fac->count[STATIC_OOP] + fac->count[STATIC_INLINE]) * heapOopSize);
   if (fac->count[STATIC_DOUBLE]) {
     next_static_double_offset = align_up(next_static_double_offset, BytesPerLong);
   }
 
   int next_static_word_offset   = next_static_double_offset +
@@ -4420,11 +4404,11 @@
   int nonstatic_fields_start  = instanceOopDesc::base_offset_in_bytes() +
                                 nonstatic_field_size * heapOopSize;
 
   // First field of inline types is aligned on a long boundary in order to ease
   // in-lining of inline types (with header removal) in packed arrays and
-  // flatten inline types
+  // inlined fields
   int initial_inline_type_padding = 0;
   if (is_inline_type()) {
     int old = nonstatic_fields_start;
     nonstatic_fields_start = align_up(nonstatic_fields_start, BytesPerLong);
     initial_inline_type_padding = nonstatic_fields_start - old;
@@ -4460,82 +4444,82 @@
   int static_inline_type_count = 0;
   int nonstatic_inline_type_count = 0;
   int* nonstatic_inline_type_indexes = NULL;
   Klass** nonstatic_inline_type_klasses = NULL;
   unsigned int inline_type_oop_map_count = 0;
-  int not_flattened_inline_types = 0;
+  int inline_types_not_inlined = 0;
   int not_atomic_inline_types = 0;
 
-  int max_nonstatic_inline_type = fac->count[NONSTATIC_FLATTENABLE] + 1;
+  int max_nonstatic_inline_type = fac->count[NONSTATIC_INLINE] + 1;
 
   nonstatic_inline_type_indexes = NEW_RESOURCE_ARRAY_IN_THREAD(THREAD, int,
                                                                max_nonstatic_inline_type);
   for (int i = 0; i < max_nonstatic_inline_type; i++) {
     nonstatic_inline_type_indexes[i] = -1;
   }
   nonstatic_inline_type_klasses = NEW_RESOURCE_ARRAY_IN_THREAD(THREAD, Klass*,
                                                                max_nonstatic_inline_type);
 
   for (AllFieldStream fs(_fields, _cp); !fs.done(); fs.next()) {
-    if (fs.allocation_type() == STATIC_FLATTENABLE) {
+    if (fs.allocation_type() == STATIC_INLINE) {
       ResourceMark rm;
       if (!fs.signature()->is_Q_signature()) {
         THROW(vmSymbols::java_lang_ClassFormatError());
       }
       static_inline_type_count++;
-    } else if (fs.allocation_type() == NONSTATIC_FLATTENABLE) {
-      // Pre-resolve the flattenable field and check for inline type circularity issues.
+    } else if (fs.allocation_type() == NONSTATIC_INLINE) {
+      // Pre-resolve the inline field and check for inline type circularity issues.
       ResourceMark rm;
       if (!fs.signature()->is_Q_signature()) {
         THROW(vmSymbols::java_lang_ClassFormatError());
       }
       Klass* klass =
-        SystemDictionary::resolve_flattenable_field_or_fail(&fs,
+        SystemDictionary::resolve_inline_type_field_or_fail(&fs,
                                                             Handle(THREAD, _loader_data->class_loader()),
                                                             _protection_domain, true, CHECK);
       assert(klass != NULL, "Sanity check");
       if (!klass->access_flags().is_inline_type()) {
         THROW(vmSymbols::java_lang_IncompatibleClassChangeError());
       }
       ValueKlass* vk = ValueKlass::cast(klass);
       // Conditions to apply flattening or not should be defined in a single place
-      bool too_big_to_flatten = (InlineFieldMaxFlatSize >= 0 &&
+      bool too_big_to_allocate_inline = (InlineFieldMaxFlatSize >= 0 &&
                                  (vk->size_helper() * HeapWordSize) > InlineFieldMaxFlatSize);
-      bool too_atomic_to_flatten = vk->is_declared_atomic();
-      bool too_volatile_to_flatten = fs.access_flags().is_volatile();
+      bool too_atomic_to_allocate_inline = vk->is_declared_atomic();
+      bool too_volatile_to_allocate_inline = fs.access_flags().is_volatile();
       if (vk->is_naturally_atomic()) {
-        too_atomic_to_flatten = false;
-        //too_volatile_to_flatten = false; //FIXME
-        // volatile fields are currently never flattened, this could change in the future
+        too_atomic_to_allocate_inline = false;
+        // too_volatile_to_allocate_inline = false; //FIXME
+        // volatile fields are currently never inlined, this could change in the future
       }
-      if (!(too_big_to_flatten | too_atomic_to_flatten | too_volatile_to_flatten)) {
+      if (!(too_big_to_allocate_inline | too_atomic_to_allocate_inline | too_volatile_to_allocate_inline)) {
         nonstatic_inline_type_indexes[nonstatic_inline_type_count] = fs.index();
         nonstatic_inline_type_klasses[nonstatic_inline_type_count] = klass;
         nonstatic_inline_type_count++;
 
         ValueKlass* vklass = ValueKlass::cast(klass);
         if (vklass->contains_oops()) {
           inline_type_oop_map_count += vklass->nonstatic_oop_map_count();
         }
-        fs.set_flattened(true);
+        fs.set_inlined(true);
         if (!vk->is_atomic()) {  // flat and non-atomic: take note
           not_atomic_inline_types++;
         }
       } else {
-        not_flattened_inline_types++;
-        fs.set_flattened(false);
+        inline_types_not_inlined++;
+        fs.set_inlined(false);
       }
     }
   }
 
-  // Adjusting non_static_oop_count to take into account not flattened inline types;
-  nonstatic_oop_count += not_flattened_inline_types;
+  // Adjusting non_static_oop_count to take into account inline types fields not inlined;
+  nonstatic_oop_count += inline_types_not_inlined;
 
   // Total non-static fields count, including every contended field
   unsigned int nonstatic_fields_count = fac->count[NONSTATIC_DOUBLE] + fac->count[NONSTATIC_WORD] +
                                         fac->count[NONSTATIC_SHORT] + fac->count[NONSTATIC_BYTE] +
-                                        fac->count[NONSTATIC_OOP] + fac->count[NONSTATIC_FLATTENABLE];
+                                        fac->count[NONSTATIC_OOP] + fac->count[NONSTATIC_INLINE];
 
   const bool super_has_nonstatic_fields =
           (_super_klass != NULL && _super_klass->has_nonstatic_fields());
   const bool has_nonstatic_fields =
     super_has_nonstatic_fields || (nonstatic_fields_count != 0);
@@ -4558,11 +4542,11 @@
   int super_oop_map_count = (_super_klass == NULL) ? 0 :_super_klass->nonstatic_oop_map_count();
   int max_oop_map_count =
       super_oop_map_count +
       fac->count[NONSTATIC_OOP] +
       inline_type_oop_map_count +
-      not_flattened_inline_types;
+      inline_types_not_inlined;
 
   OopMapBlocksBuilder* nonstatic_oop_maps = new OopMapBlocksBuilder(max_oop_map_count);
   if (super_oop_map_count > 0) {
     nonstatic_oop_maps->initialize_inherited_blocks(_super_klass->start_of_nonstatic_oop_maps(),
                                                     _super_klass->nonstatic_oop_map_count());
@@ -4680,11 +4664,11 @@
     const FieldAllocationType atype = (const FieldAllocationType) fs.allocation_type();
 
     // pack the rest of the fields
     switch (atype) {
       // Inline types in static fields are handled with oops
-      case STATIC_FLATTENABLE:   // Fallthrough
+      case STATIC_INLINE:   // Fallthrough
       case STATIC_OOP:
         real_offset = next_static_oop_offset;
         next_static_oop_offset += heapOopSize;
         break;
       case STATIC_BYTE:
@@ -4701,12 +4685,12 @@
         break;
       case STATIC_DOUBLE:
         real_offset = next_static_double_offset;
         next_static_double_offset += BytesPerLong;
         break;
-      case NONSTATIC_FLATTENABLE:
-        if (fs.is_flattened()) {
+      case NONSTATIC_INLINE:
+        if (fs.is_inlined()) {
           Klass* klass = nonstatic_inline_type_klasses[next_inline_type_index];
           assert(klass != NULL, "Klass should have been loaded and resolved earlier");
           assert(klass->access_flags().is_inline_type(),"Must be an inline type");
           ValueKlass* vklass = ValueKlass::cast(klass);
           real_offset = next_nonstatic_inline_type_offset;
@@ -4845,11 +4829,11 @@
             real_offset = next_nonstatic_padded_offset;
             next_nonstatic_padded_offset += BytesPerLong;
             break;
 
             // Inline types in static fields are handled with oops
-          case NONSTATIC_FLATTENABLE:
+          case NONSTATIC_INLINE:
             throwInlineTypeLimitation(THREAD_AND_LOCATION,
                                       "@Contended annotation not supported for inline types yet", fs.name(), fs.signature());
             return;
 
           case NONSTATIC_OOP:
@@ -4959,10 +4943,11 @@
   info->oop_map_blocks = nonstatic_oop_maps;
   info->_instance_size = instance_size;
   info->_static_field_size = static_field_size;
   info->_nonstatic_field_size = nonstatic_field_size;
   info->_has_nonstatic_fields = has_nonstatic_fields;
+  info->_has_inline_fields = nonstatic_inline_type_count > 0;
 
   // An inline type is naturally atomic if it has just one field, and
   // that field is simple enough.
   info->_is_naturally_atomic = (is_inline_type() &&
                                 !super_has_nonstatic_fields &&
@@ -6276,11 +6261,11 @@
   if (_has_injected_identityObject) {
     ik->set_has_injected_identityObject();
   }
 
   assert(_fac != NULL, "invariant");
-  ik->set_static_oop_field_count(_fac->count[STATIC_OOP] + _fac->count[STATIC_FLATTENABLE]);
+  ik->set_static_oop_field_count(_fac->count[STATIC_OOP] + _fac->count[STATIC_INLINE]);
 
   // this transfers ownership of a lot of arrays from
   // the parser onto the InstanceKlass*
   apply_parsed_class_metadata(ik, _java_fields_count, CHECK);
 
@@ -6443,11 +6428,11 @@
   }
 
   int nfields = ik->java_fields_count();
   if (ik->is_value()) nfields++;
   for (int i = 0; i < nfields; i++) {
-    if (ik->field_is_flattenable(i)) {
+    if (ik->field_is_inline_type(i)) {
       Symbol* klass_name = ik->field_signature(i)->fundamental_name(CHECK);
       // Inline classes for instance fields must have been pre-loaded
       // Inline classes for static fields might not have been loaded yet
       Klass* klass = SystemDictionary::find(klass_name,
           Handle(THREAD, ik->class_loader()),
@@ -6669,11 +6654,11 @@
   _relax_verify(false),
   _has_nonstatic_concrete_methods(false),
   _declares_nonstatic_concrete_methods(false),
   _has_final_method(false),
   _has_contended_fields(false),
-  _has_flattenable_fields(false),
+  _has_inline_type_fields(false),
   _has_nonstatic_fields(false),
   _is_empty_inline_type(false),
   _is_naturally_atomic(false),
   _is_declared_atomic(false),
   _invalid_inline_super(false),
@@ -7290,18 +7275,17 @@
   assert(_fac != NULL, "invariant");
   assert(_parsed_annotations != NULL, "invariant");
 
 
   for (AllFieldStream fs(_fields, cp); !fs.done(); fs.next()) {
-    if (fs.is_flattenable() && !fs.access_flags().is_static()) {
+    if (Signature::basic_type(fs.signature()) == T_VALUETYPE  && !fs.access_flags().is_static()) {
       // Pre-load value class
-      Klass* klass = SystemDictionary::resolve_flattenable_field_or_fail(&fs,
+      Klass* klass = SystemDictionary::resolve_inline_type_field_or_fail(&fs,
           Handle(THREAD, _loader_data->class_loader()),
           _protection_domain, true, CHECK);
       assert(klass != NULL, "Sanity check");
-      assert(klass->access_flags().is_inline_type(), "Inline type expected");
-      _has_flattenable_fields = true;
+      assert(klass->access_flags().is_inline_type(), "Value type expected");
     }
   }
 
   _field_info = new FieldLayoutInfo();
   if (UseNewFieldLayout) {
@@ -7315,10 +7299,11 @@
       _exact_size_in_bytes = lb.get_exact_size_in_byte();
     }
   } else {
     layout_fields(cp, _fac, _parsed_annotations, _field_info, CHECK);
   }
+  _has_inline_type_fields = _field_info->_has_inline_fields;
 
   // Compute reference type
   _rt = (NULL ==_super_klass) ? REF_NONE : _super_klass->reference_type();
 }
 
diff a/src/hotspot/share/classfile/classFileParser.hpp b/src/hotspot/share/classfile/classFileParser.hpp
--- a/src/hotspot/share/classfile/classFileParser.hpp
+++ b/src/hotspot/share/classfile/classFileParser.hpp
@@ -74,10 +74,11 @@
   int _instance_size;
   int _nonstatic_field_size;
   int _static_field_size;
   bool  _has_nonstatic_fields;
   bool  _is_naturally_atomic;
+  bool _has_inline_fields;
 };
 
 // Parser for for .class files
 //
 // The bytes describing the class file structure is read from a Stream object
@@ -201,11 +202,11 @@
   bool _has_nonstatic_concrete_methods;
   bool _declares_nonstatic_concrete_methods;
   bool _has_final_method;
   bool _has_contended_fields;
 
-  bool _has_flattenable_fields;
+  bool _has_inline_type_fields;
   bool _has_nonstatic_fields;
   bool _is_empty_inline_type;
   bool _is_naturally_atomic;
   bool _is_declared_atomic;
   bool _invalid_inline_super;   // if true, invalid super type for an inline type.
@@ -603,11 +604,11 @@
   bool is_unsafe_anonymous() const { return _unsafe_anonymous_host != NULL; }
   bool is_hidden() const { return _is_hidden; }
   bool is_interface() const { return _access_flags.is_interface(); }
   bool is_inline_type() const { return _access_flags.is_inline_type(); }
   bool is_value_capable_class() const;
-  bool has_flattenable_fields() const { return _has_flattenable_fields; }
+  bool has_inline_fields() const { return _has_inline_type_fields; }
   bool invalid_inline_super() const { return _invalid_inline_super; }
   void set_invalid_inline_super() { _invalid_inline_super = true; }
   bool invalid_identity_super() const { return _invalid_identity_super; }
   void set_invalid_identity_super() { _invalid_identity_super = true; }
   bool is_invalid_super_for_inline_type();
diff a/src/hotspot/share/classfile/fieldLayoutBuilder.cpp b/src/hotspot/share/classfile/fieldLayoutBuilder.cpp
--- a/src/hotspot/share/classfile/fieldLayoutBuilder.cpp
+++ b/src/hotspot/share/classfile/fieldLayoutBuilder.cpp
@@ -58,11 +58,11 @@
  _offset(-1),
  _alignment(alignment),
  _size(size),
  _field_index(index),
  _is_reference(is_reference) {
-  assert(kind == REGULAR || kind == FLATTENED || kind == INHERITED,
+  assert(kind == REGULAR || kind == INLINED || kind == INHERITED,
          "Other kind do not have a field index");
   assert(size > 0, "Sanity check");
   assert(alignment > 0, "Sanity check");
 }
 
@@ -76,11 +76,11 @@
 
 FieldGroup::FieldGroup(int contended_group) :
   _next(NULL),
   _primitive_fields(NULL),
   _oop_fields(NULL),
-  _flattened_fields(NULL),
+  _inlined_fields(NULL),
   _contended_group(contended_group),  // -1 means no contended group, 0 means default contended group
   _oop_count(0) {}
 
 void FieldGroup::add_primitive_field(AllFieldStream fs, BasicType type) {
   int size = type2aelembytes(type);
@@ -99,26 +99,26 @@
   }
   _oop_fields->append(block);
   _oop_count++;
 }
 
-void FieldGroup::add_flattened_field(AllFieldStream fs, ValueKlass* vk) {
-  // _flattened_fields list might be merged with the _primitive_fields list in the future
-  LayoutRawBlock* block = new LayoutRawBlock(fs.index(), LayoutRawBlock::FLATTENED, vk->get_exact_size_in_bytes(), vk->get_alignment(), false);
+void FieldGroup::add_inlined_field(AllFieldStream fs, ValueKlass* vk) {
+  // _inlined_fields list might be merged with the _primitive_fields list in the future
+  LayoutRawBlock* block = new LayoutRawBlock(fs.index(), LayoutRawBlock::INLINED, vk->get_exact_size_in_bytes(), vk->get_alignment(), false);
   block->set_value_klass(vk);
-  if (_flattened_fields == NULL) {
-    _flattened_fields = new(ResourceObj::RESOURCE_AREA, mtInternal) GrowableArray<LayoutRawBlock*>(INITIAL_LIST_SIZE);
+  if (_inlined_fields == NULL) {
+    _inlined_fields = new(ResourceObj::RESOURCE_AREA, mtInternal) GrowableArray<LayoutRawBlock*>(INITIAL_LIST_SIZE);
   }
-  _flattened_fields->append(block);
+  _inlined_fields->append(block);
 }
 
 void FieldGroup::sort_by_size() {
   if (_primitive_fields != NULL) {
     _primitive_fields->sort(LayoutRawBlock::compare_size_inverted);
   }
-  if (_flattened_fields != NULL) {
-    _flattened_fields->sort(LayoutRawBlock::compare_size_inverted);
+  if (_inlined_fields != NULL) {
+    _inlined_fields->sort(LayoutRawBlock::compare_size_inverted);
   }
 }
 
 FieldLayout::FieldLayout(Array<u2>* fields, ConstantPool* cp) :
   _fields(fields),
@@ -164,11 +164,11 @@
 LayoutRawBlock* FieldLayout::first_field_block() {
   LayoutRawBlock* block = _blocks;
   while (block != NULL
          && block->kind() != LayoutRawBlock::INHERITED
          && block->kind() != LayoutRawBlock::REGULAR
-         && block->kind() != LayoutRawBlock::FLATTENED) {
+         && block->kind() != LayoutRawBlock::INLINED) {
     block = block->next_block();
   }
   return block;
 }
 
@@ -457,19 +457,19 @@
                        b->size(),
                        b->alignment(),
                        "REGULAR");
       break;
     }
-    case LayoutRawBlock::FLATTENED: {
+    case LayoutRawBlock::INLINED: {
       FieldInfo* fi = FieldInfo::from_field_array(_fields, b->field_index());
       output->print_cr(" @%d \"%s\" %s %d/%d %s",
                        b->offset(),
                        fi->name(_cp)->as_C_string(),
                        fi->signature(_cp)->as_C_string(),
                        b->size(),
                        b->alignment(),
-                       "FLATTENED");
+                       "INLINED");
       break;
     }
     case LayoutRawBlock::RESERVED: {
       output->print_cr(" @%d %d/- %s",
                        b->offset(),
@@ -535,10 +535,11 @@
   _nonstatic_oopmap_count(0),
   _alignment(-1),
   _first_field_offset(-1),
   _exact_size_in_bytes(-1),
   _has_nonstatic_fields(false),
+  _has_inline_type_fields(false),
   _is_contended(is_contended),
   _is_inline_type(is_inline_type),
   _has_flattening_information(is_inline_type),
   _has_nonatomic_values(false),
   _atomic_field_count(0)
@@ -612,22 +613,23 @@
     case T_ARRAY:
       if (group != _static_fields) _nonstatic_oopmap_count++;
       group->add_oop_field(fs);
       break;
     case T_VALUETYPE:
+//      fs.set_inline(true);
+      _has_inline_type_fields = true;
       if (group == _static_fields) {
-        // static fields are never flattened
+        // static fields are never inlined
         group->add_oop_field(fs);
       } else {
         _has_flattening_information = true;
         // Flattening decision to be taken here
-        // This code assumes all verification have been performed before
-        // (field is a flattenable field, field's type has been loaded
-        // and it is an inline klass
+        // This code assumes all verification already have been performed
+        // (field's type has been loaded and it is an inline klass)
         Thread* THREAD = Thread::current();
         Klass* klass =
-            SystemDictionary::resolve_flattenable_field_or_fail(&fs,
+            SystemDictionary::resolve_inline_type_field_or_fail(&fs,
                                                                 Handle(THREAD, _class_loader_data->class_loader()),
                                                                 _protection_domain, true, THREAD);
         assert(klass != NULL, "Sanity check");
         ValueKlass* vk = ValueKlass::cast(klass);
         bool too_big_to_flatten = (InlineFieldMaxFlatSize >= 0 &&
@@ -635,16 +637,16 @@
         bool too_atomic_to_flatten = vk->is_declared_atomic();
         bool too_volatile_to_flatten = fs.access_flags().is_volatile();
         if (vk->is_naturally_atomic()) {
           too_atomic_to_flatten = false;
           //too_volatile_to_flatten = false; //FIXME
-          // volatile fields are currently never flattened, this could change in the future
+          // volatile fields are currently never inlined, this could change in the future
         }
         if (!(too_big_to_flatten | too_atomic_to_flatten | too_volatile_to_flatten)) {
-          group->add_flattened_field(fs, vk);
+          group->add_inlined_field(fs, vk);
           _nonstatic_oopmap_count += vk->nonstatic_oop_map_count();
-          fs.set_flattened(true);
+          fs.set_inlined(true);
           if (!vk->is_atomic()) {  // flat and non-atomic: take note
             _has_nonatomic_values = true;
             _atomic_field_count--;  // every other field is atomic but this one
           }
         } else {
@@ -672,11 +674,11 @@
  *     sharing issue)
  *   - this method also records the alignment of the field with the most
  *     constraining alignment, this value is then used as the alignment
  *     constraint when flattening this inline type into another container
  *   - field flattening decisions are taken in this method (those decisions are
- *     currently only based in the size of the fields to be flattened, the size
+ *     currently only based in the size of the fields to be inlined, the size
  *     of the resulting instance is not considered)
  */
 void FieldLayoutBuilder::inline_class_field_sorting(TRAPS) {
   assert(_is_inline_type, "Should only be used for inline classes");
   int alignment = 1;
@@ -713,21 +715,22 @@
         field_alignment = type2aelembytes(type); // alignment == size for oops
       }
       group->add_oop_field(fs);
       break;
     case T_VALUETYPE: {
+//      fs.set_inline(true);
+      _has_inline_type_fields = true;
       if (group == _static_fields) {
-        // static fields are never flattened
+        // static fields are never inlined
         group->add_oop_field(fs);
       } else {
         // Flattening decision to be taken here
-        // This code assumes all verifications have been performed before
-        // (field is a flattenable field, field's type has been loaded
-        // and it is an inline klass
+        // This code assumes all verifications have already been performed
+        // (field's type has been loaded and it is an inline klass)
         Thread* THREAD = Thread::current();
         Klass* klass =
-            SystemDictionary::resolve_flattenable_field_or_fail(&fs,
+            SystemDictionary::resolve_inline_type_field_or_fail(&fs,
                 Handle(THREAD, _class_loader_data->class_loader()),
                 _protection_domain, true, CHECK);
         assert(klass != NULL, "Sanity check");
         ValueKlass* vk = ValueKlass::cast(klass);
         bool too_big_to_flatten = (InlineFieldMaxFlatSize >= 0 &&
@@ -735,17 +738,17 @@
         bool too_atomic_to_flatten = vk->is_declared_atomic();
         bool too_volatile_to_flatten = fs.access_flags().is_volatile();
         if (vk->is_naturally_atomic()) {
           too_atomic_to_flatten = false;
           //too_volatile_to_flatten = false; //FIXME
-          // volatile fields are currently never flattened, this could change in the future
+          // volatile fields are currently never inlined, this could change in the future
         }
         if (!(too_big_to_flatten | too_atomic_to_flatten | too_volatile_to_flatten)) {
-          group->add_flattened_field(fs, vk);
+          group->add_inlined_field(fs, vk);
           _nonstatic_oopmap_count += vk->nonstatic_oop_map_count();
           field_alignment = vk->get_alignment();
-          fs.set_flattened(true);
+          fs.set_inlined(true);
           if (!vk->is_atomic()) {  // flat and non-atomic: take note
             _has_nonatomic_values = true;
             _atomic_field_count--;  // every other field is atomic but this one
           }
         } else {
@@ -778,14 +781,14 @@
   }
 }
 
 /* Computation of regular classes layout is an evolution of the previous default layout
  * (FieldAllocationStyle 1):
- *   - flattened fields are allocated first (because they have potentially the
+ *   - inlined fields are allocated first (because they have potentially the
  *     least regular shapes, and are more likely to create empty slots between them,
  *     which can then be used to allocation primitive or oop fields). Allocation is
- *     performed from the biggest to the smallest flattened field.
+ *     performed from the biggest to the smallest field.
  *   - then primitive fields (from the biggest to the smallest)
  *   - then oop fields are allocated contiguously (to reduce the number of oopmaps
  *     and reduce the work of the GC).
  */
 void FieldLayoutBuilder::compute_regular_layout() {
@@ -797,30 +800,30 @@
     // insertion is currently easy because the current strategy doesn't try to fill holes
     // in super classes layouts => the _start block is by consequence the _last_block
     insert_contended_padding(_layout->start());
     need_tail_padding = true;
   }
-  _layout->add(_root_group->flattened_fields());
+  _layout->add(_root_group->inlined_fields());
   _layout->add(_root_group->primitive_fields());
   _layout->add(_root_group->oop_fields());
 
   if (!_contended_groups.is_empty()) {
     for (int i = 0; i < _contended_groups.length(); i++) {
       FieldGroup* cg = _contended_groups.at(i);
       LayoutRawBlock* start = _layout->last_block();
       insert_contended_padding(start);
-      _layout->add(_root_group->flattened_fields());
+      _layout->add(_root_group->inlined_fields());
       _layout->add(cg->primitive_fields(), start);
       _layout->add(cg->oop_fields(), start);
       need_tail_padding = true;
     }
   }
 
   if (need_tail_padding) {
     insert_contended_padding(_layout->last_block());
   }
-  _static_layout->add(_static_fields->flattened_fields());
+  _static_layout->add(_static_fields->inlined_fields());
   _static_layout->add_contiguously(_static_fields->oop_fields());
   _static_layout->add(_static_fields->primitive_fields());
 
   epilogue();
 }
@@ -830,14 +833,14 @@
  * of the layout to increase GC performances. Unfortunately, this strategy
  * increases the number of empty slots inside an instance. Because the purpose
  * of inline classes is to be embedded into other containers, it is critical
  * to keep their size as small as possible. For this reason, the allocation
  * strategy is:
- *   - flattened fields are allocated first (because they have potentially the
+ *   - inlined fields are allocated first (because they have potentially the
  *     least regular shapes, and are more likely to create empty slots between them,
  *     which can then be used to allocation primitive or oop fields). Allocation is
- *     performed from the biggest to the smallest flattened field.
+ *     performed from the biggest to the smallest field.
  *   - then oop fields are allocated contiguously (to reduce the number of oopmaps
  *     and reduce the work of the GC)
  *   - then primitive fields (from the biggest to the smallest)
  */
 void FieldLayoutBuilder::compute_inline_class_layout(TRAPS) {
@@ -853,11 +856,11 @@
     LayoutRawBlock* padding = new LayoutRawBlock(LayoutRawBlock::PADDING, _alignment - (first_empty->offset() % _alignment));
     _layout->insert(first_empty, padding);
     _layout->set_start(padding->next_block());
   }
 
-  _layout->add(_root_group->flattened_fields());
+  _layout->add(_root_group->inlined_fields());
   _layout->add(_root_group->oop_fields());
   _layout->add(_root_group->primitive_fields());
 
   LayoutRawBlock* first_field = _layout->first_field_block();
    if (first_field != NULL) {
@@ -868,19 +871,19 @@
      _first_field_offset = _layout->blocks()->size();
      _exact_size_in_bytes = 0;
    }
   _exact_size_in_bytes = _layout->last_block()->offset() - _layout->first_field_block()->offset();
 
-  _static_layout->add(_static_fields->flattened_fields());
+  _static_layout->add(_static_fields->inlined_fields());
   _static_layout->add_contiguously(_static_fields->oop_fields());
   _static_layout->add(_static_fields->primitive_fields());
 
 
   epilogue();
 }
 
-void FieldLayoutBuilder::add_flattened_field_oopmap(OopMapBlocksBuilder* nonstatic_oop_maps,
+void FieldLayoutBuilder::add_inlined_field_oopmap(OopMapBlocksBuilder* nonstatic_oop_maps,
                 ValueKlass* vklass, int offset) {
   int diff = offset - vklass->first_field_offset();
   const OopMapBlock* map = vklass->start_of_nonstatic_oop_maps();
   const OopMapBlock* last_map = map + vklass->nonstatic_oop_map_count();
   while (map < last_map) {
@@ -906,18 +909,18 @@
       LayoutRawBlock* b = _root_group->oop_fields()->at(i);
       nonstatic_oop_maps->add(b->offset(), 1);
     }
   }
 
-  GrowableArray<LayoutRawBlock*>* ff = _root_group->flattened_fields();
+  GrowableArray<LayoutRawBlock*>* ff = _root_group->inlined_fields();
   if (ff != NULL) {
     for (int i = 0; i < ff->length(); i++) {
       LayoutRawBlock* f = ff->at(i);
       ValueKlass* vk = f->value_klass();
       assert(vk != NULL, "Should have been initialized");
       if (vk->contains_oops()) {
-        add_flattened_field_oopmap(nonstatic_oop_maps, vk, f->offset());
+        add_inlined_field_oopmap(nonstatic_oop_maps, vk, f->offset());
       }
     }
   }
 
   if (!_contended_groups.is_empty()) {
@@ -943,10 +946,11 @@
   _info->oop_map_blocks = nonstatic_oop_maps;
   _info->_instance_size = align_object_size(instance_end / wordSize);
   _info->_static_field_size = static_fields_size;
   _info->_nonstatic_field_size = (nonstatic_field_end - instanceOopDesc::base_offset_in_bytes()) / heapOopSize;
   _info->_has_nonstatic_fields = _has_nonstatic_fields;
+  _info->_has_inline_fields = _has_inline_type_fields;
 
   // An inline type is naturally atomic if it has just one field, and
   // that field is simple enough.
   _info->_is_naturally_atomic = (_is_inline_type &&
                                  (_atomic_field_count <= 1) &&
diff a/src/hotspot/share/classfile/fieldLayoutBuilder.hpp b/src/hotspot/share/classfile/fieldLayoutBuilder.hpp
--- a/src/hotspot/share/classfile/fieldLayoutBuilder.hpp
+++ b/src/hotspot/share/classfile/fieldLayoutBuilder.hpp
@@ -51,16 +51,16 @@
 //
 class LayoutRawBlock : public ResourceObj {
  public:
   // Some code relies on the order of values below.
   enum Kind {
-    EMPTY,         // empty slot, space is taken from this to allocate fields
-    RESERVED,      // reserved for JVM usage (for instance object header)
-    PADDING,       // padding (because of alignment constraints or @Contended)
-    REGULAR,       // primitive or oop field (including non-flattened inline fields)
-    FLATTENED,     // flattened field
-    INHERITED      // field(s) inherited from super classes
+    EMPTY,            // empty slot, space is taken from this to allocate fields
+    RESERVED,         // reserved for JVM usage (for instance object header)
+    PADDING,          // padding (because of alignment constraints or @Contended)
+    REGULAR,          // primitive or oop field (including inline type fields not inlined)
+    INLINED,          // field inlined
+    INHERITED         // field(s) inherited from super classes
   };
 
  private:
   LayoutRawBlock* _next_block;
   LayoutRawBlock* _prev_block;
@@ -121,20 +121,20 @@
 };
 
 // A Field group represents a set of fields that have to be allocated together,
 // this is the way the @Contended annotation is supported.
 // Inside a FieldGroup, fields are sorted based on their kind: primitive,
-// oop, or flattened.
+// oop, or inlined.
 //
 class FieldGroup : public ResourceObj {
 
  private:
   FieldGroup* _next;
 
   GrowableArray<LayoutRawBlock*>* _primitive_fields;
   GrowableArray<LayoutRawBlock*>* _oop_fields;
-  GrowableArray<LayoutRawBlock*>* _flattened_fields;
+  GrowableArray<LayoutRawBlock*>* _inlined_fields;
   int _contended_group;
   int _oop_count;
   static const int INITIAL_LIST_SIZE = 16;
 
  public:
@@ -142,17 +142,17 @@
 
   FieldGroup* next() const { return _next; }
   void set_next(FieldGroup* next) { _next = next; }
   GrowableArray<LayoutRawBlock*>* primitive_fields() const { return _primitive_fields; }
   GrowableArray<LayoutRawBlock*>* oop_fields() const { return _oop_fields; }
-  GrowableArray<LayoutRawBlock*>* flattened_fields() const { return _flattened_fields; }
+  GrowableArray<LayoutRawBlock*>* inlined_fields() const { return _inlined_fields; }
   int contended_group() const { return _contended_group; }
   int oop_count() const { return _oop_count; }
 
   void add_primitive_field(AllFieldStream fs, BasicType type);
   void add_oop_field(AllFieldStream fs);
-  void add_flattened_field(AllFieldStream fs, ValueKlass* vk);
+  void add_inlined_field(AllFieldStream fs, ValueKlass* vk);
   void add_block(LayoutRawBlock** list, LayoutRawBlock* block);
   void sort_by_size();
 };
 
 // The FieldLayout class represents a set of fields organized
@@ -251,10 +251,11 @@
   int _nonstatic_oopmap_count;
   int _alignment;
   int _first_field_offset;
   int _exact_size_in_bytes;
   bool _has_nonstatic_fields;
+  bool _has_inline_type_fields;
   bool _is_contended;
   bool _is_inline_type;
   bool _has_flattening_information;
   bool _has_nonatomic_values;
   int _atomic_field_count;
@@ -289,9 +290,9 @@
  protected:
   void prologue();
   void epilogue();
   void regular_field_sorting();
   void inline_class_field_sorting(TRAPS);
-  void add_flattened_field_oopmap(OopMapBlocksBuilder* nonstatic_oop_map, ValueKlass* vk, int offset);
+  void add_inlined_field_oopmap(OopMapBlocksBuilder* nonstatic_oop_map, ValueKlass* vk, int offset);
 };
 
 #endif // SHARE_CLASSFILE_FIELDLAYOUTBUILDER_HPP
diff a/src/hotspot/share/classfile/javaClasses.cpp b/src/hotspot/share/classfile/javaClasses.cpp
--- a/src/hotspot/share/classfile/javaClasses.cpp
+++ b/src/hotspot/share/classfile/javaClasses.cpp
@@ -4762,19 +4762,19 @@
 // jdk_internal_vm_jni_SubElementSelector
 
 int jdk_internal_vm_jni_SubElementSelector::_arrayElementType_offset;
 int jdk_internal_vm_jni_SubElementSelector::_subElementType_offset;
 int jdk_internal_vm_jni_SubElementSelector::_offset_offset;
-int jdk_internal_vm_jni_SubElementSelector::_isFlattened_offset;
-int jdk_internal_vm_jni_SubElementSelector::_isFlattenable_offset;
+int jdk_internal_vm_jni_SubElementSelector::_isInlined_offset;
+int jdk_internal_vm_jni_SubElementSelector::_isInlineType_offset;
 
 #define SUBELEMENT_SELECTOR_FIELDS_DO(macro) \
   macro(_arrayElementType_offset,  k, "arrayElementType", class_signature, false); \
   macro(_subElementType_offset,    k, "subElementType",   class_signature, false); \
   macro(_offset_offset,            k, "offset",           int_signature,   false); \
-  macro(_isFlattened_offset,       k, "isFlattened",      bool_signature,  false); \
-  macro(_isFlattenable_offset,     k, "isFlattenable",    bool_signature,  false);
+  macro(_isInlined_offset,         k, "isInlined",        bool_signature,  false); \
+  macro(_isInlineType_offset,      k, "isInlineType",     bool_signature,  false);
 
 void jdk_internal_vm_jni_SubElementSelector::compute_offsets() {
   InstanceKlass* k = SystemDictionary::jdk_internal_vm_jni_SubElementSelector_klass();
   SUBELEMENT_SELECTOR_FIELDS_DO(FIELD_COMPUTE_OFFSET);
 }
@@ -4812,24 +4812,24 @@
 
 void jdk_internal_vm_jni_SubElementSelector::setOffset(oop obj, int offset) {
   obj->int_field_put(_offset_offset, offset);
 }
 
-bool jdk_internal_vm_jni_SubElementSelector::getIsFlattened(oop obj) {
-  return obj->bool_field(_isFlattened_offset);
+bool jdk_internal_vm_jni_SubElementSelector::getIsInlined(oop obj) {
+  return obj->bool_field(_isInlined_offset);
 }
 
-void jdk_internal_vm_jni_SubElementSelector::setIsFlattened(oop obj, bool b) {
-  obj->bool_field_put(_isFlattened_offset, b);
+void jdk_internal_vm_jni_SubElementSelector::setIsInlined(oop obj, bool b) {
+  obj->bool_field_put(_isInlined_offset, b);
 }
 
-bool jdk_internal_vm_jni_SubElementSelector::getIsFlattenable(oop obj) {
-  return obj->bool_field(_isFlattenable_offset);
+bool jdk_internal_vm_jni_SubElementSelector::getIsInlineType(oop obj) {
+  return obj->bool_field(_isInlineType_offset);
 }
 
-void jdk_internal_vm_jni_SubElementSelector::setIsFlattenable(oop obj, bool b) {
-  obj->bool_field_put(_isFlattenable_offset, b);
+void jdk_internal_vm_jni_SubElementSelector::setIsInlineType(oop obj, bool b) {
+  obj->bool_field_put(_isInlineType_offset, b);
 }
 
 jbyte java_lang_Byte::value(oop obj) {
    jvalue v;
    java_lang_boxing_object::get_value(obj, &v);
diff a/src/hotspot/share/classfile/javaClasses.hpp b/src/hotspot/share/classfile/javaClasses.hpp
--- a/src/hotspot/share/classfile/javaClasses.hpp
+++ b/src/hotspot/share/classfile/javaClasses.hpp
@@ -1654,12 +1654,12 @@
 class jdk_internal_vm_jni_SubElementSelector : AllStatic {
  private:
   static int _arrayElementType_offset;
   static int _subElementType_offset;
   static int _offset_offset;
-  static int _isFlattened_offset;
-  static int _isFlattenable_offset;
+  static int _isInlined_offset;
+  static int _isInlineType_offset;
  public:
   static Symbol* symbol();
   static void compute_offsets();
   static void serialize_offsets(SerializeClosure* f) NOT_CDS_RETURN;
 
@@ -1667,14 +1667,14 @@
   static void setArrayElementType(oop obj, oop type);
   static oop getSubElementType(oop obj);
   static void setSubElementType(oop obj, oop type);
   static int getOffset(oop obj);
   static void setOffset(oop obj, int offset);
-  static bool getIsFlattened(oop obj);
-  static void setIsFlattened(oop obj, bool b);
-  static bool getIsFlattenable(oop obj);
-  static void setIsFlattenable(oop obj, bool b);
+  static bool getIsInlined(oop obj);
+  static void setIsInlined(oop obj, bool b);
+  static bool getIsInlineType(oop obj);
+  static void setIsInlineType(oop obj, bool b);
 };
 
 // Use to declare fields that need to be injected into Java classes
 // for the JVM to use.  The name_index and signature_index are
 // declared in vmSymbols.  The may_be_java flag is used to declare
diff a/src/hotspot/share/classfile/placeholders.cpp b/src/hotspot/share/classfile/placeholders.cpp
--- a/src/hotspot/share/classfile/placeholders.cpp
+++ b/src/hotspot/share/classfile/placeholders.cpp
@@ -42,11 +42,11 @@
   entry->set_havesupername(havesupername);
   entry->set_supername(supername);
   entry->set_superThreadQ(NULL);
   entry->set_loadInstanceThreadQ(NULL);
   entry->set_defineThreadQ(NULL);
-  entry->set_flattenableFieldQ(NULL);
+  entry->set_inlineTypeFieldQ(NULL);
   entry->set_definer(NULL);
   entry->set_instance_klass(NULL);
   return entry;
 }
 
@@ -165,11 +165,11 @@
     if (probe != NULL) {
        probe->remove_seen_thread(thread, action);
        // If no other threads using this entry, and this thread is not using this entry for other states
        if ((probe->superThreadQ() == NULL) && (probe->loadInstanceThreadQ() == NULL)
           && (probe->defineThreadQ() == NULL) && (probe->definer() == NULL)
-          && (probe->flattenableFieldQ() == NULL)) {
+          && (probe->inlineTypeFieldQ() == NULL)) {
          remove_entry(index, hash, name, loader_data);
        }
     }
   }
 
@@ -220,12 +220,12 @@
   superThreadQ()->print_action_queue(st);
   st->cr();
   st->print("defineThreadQ threads:");
   defineThreadQ()->print_action_queue(st);
   st->cr();
-  st->print("flattenableFieldQ threads:");
-  flattenableFieldQ()->print_action_queue(st);
+  st->print("inlineTypeFieldQ threads:");
+  inlineTypeFieldQ()->print_action_queue(st);
   st->cr();
 }
 
 void PlaceholderTable::print_on(outputStream* st) const {
   st->print_cr("Placeholder table (table_size=%d, placeholders=%d)",
diff a/src/hotspot/share/classfile/placeholders.hpp b/src/hotspot/share/classfile/placeholders.hpp
--- a/src/hotspot/share/classfile/placeholders.hpp
+++ b/src/hotspot/share/classfile/placeholders.hpp
@@ -72,16 +72,16 @@
 // LOAD_SUPER needed to check for class circularity
 // DEFINE_CLASS: ultimately define class must be single threaded
 // on a class/classloader basis
 // so the head of that queue owns the token
 // and the rest of the threads return the result the first thread gets
-// FLATTENABLE_FIELD: needed to check for value type flattenable fields circularity
+// INLINE_TYPE_FIELD: needed to check for inline type fields circularity
  enum classloadAction {
     LOAD_INSTANCE = 1,             // calling load_instance_class
     LOAD_SUPER = 2,                // loading superclass for this class
     DEFINE_CLASS = 3,              // find_or_define class
-    FLATTENABLE_FIELD = 4          // flattenable value type fields
+    INLINE_TYPE_FIELD = 4          // inline type fields
  };
 
   // find_and_add returns probe pointer - old or new
   // If no entry exists, add a placeholder entry and push SeenThread for classloadAction
   // If entry exists, reuse entry and push SeenThread for classloadAction
@@ -109,11 +109,11 @@
 // For bootclasssearchpath, set before calling load_instance_class.
 // Defining must be single threaded on a class/classloader basis
 // For DEFINE_CLASS, the head of the queue owns the
 // define token and the rest of the threads wait to return the
 // result the first thread gets.
-// For FLATTENABLE_FIELD, set when loading value type fields for
+// For INLINE_FIELD, set when loading inline type fields for
 // class circularity checking.
 class SeenThread: public CHeapObj<mtInternal> {
 private:
    Thread *_thread;
    SeenThread* _stnext;
@@ -162,11 +162,11 @@
 
   SeenThread*       _defineThreadQ; // queue of Threads trying to define this class
                                     // including _definer
                                     // _definer owns token
                                     // queue waits for and returns results from _definer
-  SeenThread*       _flattenableFieldQ; // queue of value types for circularity checking
+  SeenThread*       _inlineTypeFieldQ;  // queue of inline types for circularity checking
 
  public:
   // Simple accessors, used only by SystemDictionary
   Symbol*            klassname()           const { return literal(); }
 
@@ -195,12 +195,12 @@
   void               set_loadInstanceThreadQ(SeenThread* SeenThread) { _loadInstanceThreadQ = SeenThread; }
 
   SeenThread*        defineThreadQ()        const { return _defineThreadQ; }
   void               set_defineThreadQ(SeenThread* SeenThread) { _defineThreadQ = SeenThread; }
 
-  SeenThread*        flattenableFieldQ()    const { return _flattenableFieldQ; }
-  void               set_flattenableFieldQ(SeenThread* SeenThread) { _flattenableFieldQ = SeenThread; }
+  SeenThread*        inlineTypeFieldQ()    const { return _inlineTypeFieldQ; }
+  void               set_inlineTypeFieldQ(SeenThread* SeenThread) { _inlineTypeFieldQ = SeenThread; }
 
   PlaceholderEntry* next() const {
     return (PlaceholderEntry*)HashtableEntry<Symbol*, mtClass>::next();
   }
 
@@ -224,12 +224,12 @@
          queuehead = _superThreadQ;
          break;
       case PlaceholderTable::DEFINE_CLASS:
 	 queuehead = _defineThreadQ;
 	 break;
-      case PlaceholderTable::FLATTENABLE_FIELD:
-         queuehead = _flattenableFieldQ;
+      case PlaceholderTable::INLINE_TYPE_FIELD:
+         queuehead = _inlineTypeFieldQ;
          break;
       default: Unimplemented();
     }
     return queuehead;
   }
@@ -243,12 +243,12 @@
          _superThreadQ = seenthread;
          break;
       case PlaceholderTable::DEFINE_CLASS:
          _defineThreadQ = seenthread;
          break;
-      case PlaceholderTable::FLATTENABLE_FIELD:
-         _flattenableFieldQ = seenthread;
+      case PlaceholderTable::INLINE_TYPE_FIELD:
+         _inlineTypeFieldQ = seenthread;
          break;
       default: Unimplemented();
     }
     return;
   }
@@ -263,12 +263,12 @@
 
   bool define_class_in_progress() {
     return (_defineThreadQ != NULL);
   }
 
-  bool flattenable_field_in_progress() {
-    return (_flattenableFieldQ != NULL);
+  bool inline_type_field_in_progress() {
+    return (_inlineTypeFieldQ != NULL);
   }
 
 // Doubly-linked list of Threads per action for class/classloader pair
 // Class circularity support: links in thread before loading superclass
 // bootstrapsearchpath support: links in a thread before load_instance_class
diff a/src/hotspot/share/classfile/systemDictionary.cpp b/src/hotspot/share/classfile/systemDictionary.cpp
--- a/src/hotspot/share/classfile/systemDictionary.cpp
+++ b/src/hotspot/share/classfile/systemDictionary.cpp
@@ -509,11 +509,11 @@
   }
 
   return superk;
 }
 
-Klass* SystemDictionary::resolve_flattenable_field_or_fail(AllFieldStream* fs,
+Klass* SystemDictionary::resolve_inline_type_field_or_fail(AllFieldStream* fs,
                                                            Handle class_loader,
                                                            Handle protection_domain,
                                                            bool throw_error,
                                                            TRAPS) {
   Symbol* class_name = fs->signature()->fundamental_name(THREAD);
@@ -526,16 +526,16 @@
 
   {
     MutexLocker mu(THREAD, SystemDictionary_lock);
     oldprobe = placeholders()->get_entry(p_index, p_hash, class_name, loader_data);
     if (oldprobe != NULL &&
-      oldprobe->check_seen_thread(THREAD, PlaceholderTable::FLATTENABLE_FIELD)) {
+      oldprobe->check_seen_thread(THREAD, PlaceholderTable::INLINE_TYPE_FIELD)) {
       throw_circularity_error = true;
 
     } else {
       placeholders()->find_and_add(p_index, p_hash, class_name, loader_data,
-                                   PlaceholderTable::FLATTENABLE_FIELD, NULL, THREAD);
+                                   PlaceholderTable::INLINE_TYPE_FIELD, NULL, THREAD);
     }
   }
 
   Klass* klass = NULL;
   if (!throw_circularity_error) {
@@ -547,11 +547,11 @@
   }
 
   {
     MutexLocker mu(THREAD, SystemDictionary_lock);
     placeholders()->find_and_remove(p_index, p_hash, class_name, loader_data,
-                                    PlaceholderTable::FLATTENABLE_FIELD, THREAD);
+                                    PlaceholderTable::INLINE_TYPE_FIELD, THREAD);
   }
 
   class_name->decrement_refcount();
   return klass;
 }
diff a/src/hotspot/share/classfile/systemDictionary.hpp b/src/hotspot/share/classfile/systemDictionary.hpp
--- a/src/hotspot/share/classfile/systemDictionary.hpp
+++ b/src/hotspot/share/classfile/systemDictionary.hpp
@@ -322,11 +322,11 @@
                                               Handle class_loader,
                                               Handle protection_domain,
                                               bool is_superclass,
                                               TRAPS);
 
-  static Klass* resolve_flattenable_field_or_fail(AllFieldStream* fs,
+  static Klass* resolve_inline_type_field_or_fail(AllFieldStream* fs,
                                                   Handle class_loader,
                                                   Handle protection_domain,
                                                   bool throw_error,
                                                   TRAPS);
 
diff a/src/hotspot/share/interpreter/interpreterRuntime.cpp b/src/hotspot/share/interpreter/interpreterRuntime.cpp
--- a/src/hotspot/share/interpreter/interpreterRuntime.cpp
+++ b/src/hotspot/share/interpreter/interpreterRuntime.cpp
@@ -350,19 +350,19 @@
   if (field_type == T_ARRAY || field_type == T_OBJECT) {
     oop aoop = *(oop*)f.interpreter_frame_expression_stack_at(tos_idx);
     assert(aoop == NULL || oopDesc::is_oop(aoop),"argument must be a reference type");
     new_value_h()->obj_field_put(field_offset, aoop);
   } else if (field_type == T_VALUETYPE) {
-    if (cp_entry->is_flattened()) {
+    if (cp_entry->is_inlined()) {
       oop vt_oop = *(oop*)f.interpreter_frame_expression_stack_at(tos_idx);
       assert(vt_oop != NULL && oopDesc::is_oop(vt_oop) && vt_oop->is_value(),"argument must be a value type");
       ValueKlass* field_vk = ValueKlass::cast(vklass->get_value_field_klass(field_index));
       assert(vt_oop != NULL && field_vk == vt_oop->klass(), "Must match");
-      field_vk->write_flattened_field(new_value_h(), offset, vt_oop, CHECK_(return_offset));
-    } else { // not flattened
+      field_vk->write_inlined_field(new_value_h(), offset, vt_oop, CHECK_(return_offset));
+    } else { // not inlined
       oop voop = *(oop*)f.interpreter_frame_expression_stack_at(tos_idx);
-      if (voop == NULL && cp_entry->is_flattenable()) {
+      if (voop == NULL && cp_entry->is_inline_type()) {
         THROW_(vmSymbols::java_lang_NullPointerException(), return_offset);
       }
       assert(voop == NULL || oopDesc::is_oop(voop),"checking argument");
       new_value_h()->obj_field_put(field_offset, voop);
     }
@@ -375,11 +375,11 @@
   thread->set_vm_result(new_value_h());
   return return_offset;
 JRT_END
 
 JRT_ENTRY(void, InterpreterRuntime::uninitialized_static_value_field(JavaThread* thread, oopDesc* mirror, int index))
-  // The interpreter tries to access a flattenable static field that has not been initialized.
+  // The interpreter tries to access an inline static field that has not been initialized.
   // This situation can happen in different scenarios:
   //   1 - if the load or initialization of the field failed during step 8 of
   //       the initialization of the holder of the field, in this case the access to the field
   //       must fail
   //   2 - it can also happen when the initialization of the holder class triggered the initialization of
@@ -423,24 +423,24 @@
       THROW_MSG(vmSymbols::java_lang_NoClassDefFoundError(), message);
     }
   }
 JRT_END
 
-JRT_ENTRY(void, InterpreterRuntime::read_flattened_field(JavaThread* thread, oopDesc* obj, int index, Klass* field_holder))
+JRT_ENTRY(void, InterpreterRuntime::read_inlined_field(JavaThread* thread, oopDesc* obj, int index, Klass* field_holder))
   Handle obj_h(THREAD, obj);
 
   assert(oopDesc::is_oop(obj), "Sanity check");
 
   assert(field_holder->is_instance_klass(), "Sanity check");
   InstanceKlass* klass = InstanceKlass::cast(field_holder);
 
-  assert(klass->field_is_flattened(index), "Sanity check");
+  assert(klass->field_is_inlined(index), "Sanity check");
 
   ValueKlass* field_vklass = ValueKlass::cast(klass->get_value_field_klass(index));
   assert(field_vklass->is_initialized(), "Must be initialized at this point");
 
-  oop res = field_vklass->read_flattened_field(obj_h(), klass->field_offset(index), CHECK);
+  oop res = field_vklass->read_inlined_field(obj_h(), klass->field_offset(index), CHECK);
   thread->set_vm_result(res);
 JRT_END
 
 JRT_ENTRY(void, InterpreterRuntime::newarray(JavaThread* thread, BasicType type, jint size))
   oop obj = oopFactory::new_typeArray(type, size, CHECK);
@@ -989,12 +989,12 @@
     info.index(),
     info.offset(),
     state,
     info.access_flags().is_final(),
     info.access_flags().is_volatile(),
-    info.is_flattened(),
-    info.is_flattenable(),
+    info.is_inlined(),
+    info.is_inline_type(),
     pool->pool_holder()
   );
 }
 
 
@@ -1467,20 +1467,20 @@
   InstanceKlass* ik = InstanceKlass::cast(cp_entry->f1_as_klass());
   int index = cp_entry->field_index();
   if ((ik->field_access_flags(index) & JVM_ACC_FIELD_ACCESS_WATCHED) == 0) return;
 
   bool is_static = (obj == NULL);
-  bool is_flattened = cp_entry->is_flattened();
+  bool is_inlined = cp_entry->is_inlined();
   HandleMark hm(thread);
 
   Handle h_obj;
   if (!is_static) {
     // non-static field accessors have an object, but we need a handle
     h_obj = Handle(thread, obj);
   }
   InstanceKlass* cp_entry_f1 = InstanceKlass::cast(cp_entry->f1_as_klass());
-  jfieldID fid = jfieldIDWorkaround::to_jfieldID(cp_entry_f1, cp_entry->f2_as_index(), is_static, is_flattened);
+  jfieldID fid = jfieldIDWorkaround::to_jfieldID(cp_entry_f1, cp_entry->f2_as_index(), is_static, is_inlined);
   LastFrameAccessor last_frame(thread);
   JvmtiExport::post_field_access(thread, last_frame.method(), last_frame.bcp(), cp_entry_f1, h_obj, fid);
 JRT_END
 
 JRT_ENTRY(void, InterpreterRuntime::post_field_modification(JavaThread *thread,
@@ -1513,14 +1513,14 @@
   if (cp_entry->flag_state() == atos && ik->field_signature(index)->is_Q_signature()) {
     sig_type = JVM_SIGNATURE_VALUETYPE;
   }
 
   bool is_static = (obj == NULL);
-  bool is_flattened = cp_entry->is_flattened();
+  bool is_inlined = cp_entry->is_inlined();
 
   HandleMark hm(thread);
-  jfieldID fid = jfieldIDWorkaround::to_jfieldID(ik, cp_entry->f2_as_index(), is_static, is_flattened);
+  jfieldID fid = jfieldIDWorkaround::to_jfieldID(ik, cp_entry->f2_as_index(), is_static, is_inlined);
   jvalue fvalue;
 #ifdef _LP64
   fvalue = *value;
 #else
   // Long/double values are stored unaligned and also noncontiguously with
diff a/src/hotspot/share/interpreter/interpreterRuntime.hpp b/src/hotspot/share/interpreter/interpreterRuntime.hpp
--- a/src/hotspot/share/interpreter/interpreterRuntime.hpp
+++ b/src/hotspot/share/interpreter/interpreterRuntime.hpp
@@ -67,11 +67,11 @@
   static void    register_finalizer(JavaThread* thread, oopDesc* obj);
   static void    defaultvalue  (JavaThread* thread, ConstantPool* pool, int index);
   static int     withfield     (JavaThread* thread, ConstantPoolCache* cp_cache);
   static void    uninitialized_static_value_field(JavaThread* thread, oopDesc* mirror, int offset);
   static void    write_heap_copy (JavaThread* thread, oopDesc* value, int offset, oopDesc* rcv);
-  static void    read_flattened_field(JavaThread* thread, oopDesc* value, int index, Klass* field_holder);
+  static void    read_inlined_field(JavaThread* thread, oopDesc* value, int index, Klass* field_holder);
 
   static void value_array_load(JavaThread* thread, arrayOopDesc* array, int index);
   static void value_array_store(JavaThread* thread, void* val, arrayOopDesc* array, int index);
 
   static jboolean is_substitutable(JavaThread* thread, oopDesc* aobj, oopDesc* bobj);
diff a/src/hotspot/share/memory/heapInspection.cpp b/src/hotspot/share/memory/heapInspection.cpp
--- a/src/hotspot/share/memory/heapInspection.cpp
+++ b/src/hotspot/share/memory/heapInspection.cpp
@@ -526,32 +526,32 @@
   const Symbol* signature() { return _signature; }
   const int offset() { return _offset; }
   const int index() { return _index; }
   const InstanceKlass* holder() { return _holder; }
   const AccessFlags& access_flags() { return _access_flags; }
-  const bool is_flattenable() { return _access_flags.is_flattenable(); }
+  const bool is_inline_type() { return Signature::basic_type(_signature) == T_VALUETYPE; }
 };
 
 static int compare_offset(FieldDesc* f1, FieldDesc* f2) {
    return f1->offset() > f2->offset() ? 1 : -1;
 }
 
-static void print_field(outputStream* st, int level, int offset, FieldDesc& fd, bool flattenable, bool flattened ) {
-  const char* flattened_msg = "";
-  if (flattenable) {
-    flattened_msg = flattened ? "and flattened" : "not flattened";
+static void print_field(outputStream* st, int level, int offset, FieldDesc& fd, bool is_inline_type, bool is_inlined ) {
+  const char* inlined_msg = "";
+  if (is_inline_type) {
+    inlined_msg = is_inlined ? "inlined" : "not inlined";
   }
   st->print_cr("  @ %d %*s \"%s\" %s %s %s",
       offset, level * 3, "",
       fd.name()->as_C_string(),
       fd.signature()->as_C_string(),
-      flattenable ? " // flattenable" : "",
-      flattened_msg);
+      is_inline_type ? " // inline type " : "",
+      inlined_msg);
 }
 
-static void print_flattened_field(outputStream* st, int level, int offset, InstanceKlass* klass) {
-  assert(klass->is_value(), "Only value classes can be flattened");
+static void print_inlined_field(outputStream* st, int level, int offset, InstanceKlass* klass) {
+  assert(klass->is_value(), "Only inline types can be inlined");
   ValueKlass* vklass = ValueKlass::cast(klass);
   GrowableArray<FieldDesc>* fields = new (ResourceObj::C_HEAP, mtInternal) GrowableArray<FieldDesc>(100, true);
   for (FieldStream fd(klass, false, false); !fd.eos(); fd.next()) {
     if (!fd.access_flags().is_static()) {
       fields->append(FieldDesc(fd.field_descriptor()));
@@ -560,13 +560,13 @@
   fields->sort(compare_offset);
   for(int i = 0; i < fields->length(); i++) {
     FieldDesc fd = fields->at(i);
     int offset2 = offset + fd.offset() - vklass->first_field_offset();
     print_field(st, level, offset2, fd,
-        fd.is_flattenable(), fd.holder()->field_is_flattened(fd.index()));
-    if (fd.holder()->field_is_flattened(fd.index())) {
-      print_flattened_field(st, level + 1, offset2 ,
+        fd.is_inline_type(), fd.holder()->field_is_inlined(fd.index()));
+    if (fd.holder()->field_is_inlined(fd.index())) {
+      print_inlined_field(st, level + 1, offset2 ,
           InstanceKlass::cast(fd.holder()->get_value_field_klass(fd.index())));
     }
   }
 }
 
@@ -601,13 +601,13 @@
       }
     }
     fields->sort(compare_offset);
     for(int i = 0; i < fields->length(); i++) {
       FieldDesc fd = fields->at(i);
-      print_field(st, 0, fd.offset(), fd, fd.is_flattenable(), fd.holder()->field_is_flattened(fd.index()));
-      if (fd.holder()->field_is_flattened(fd.index())) {
-        print_flattened_field(st, 1, fd.offset(),
+      print_field(st, 0, fd.offset(), fd, fd.is_inline_type(), fd.holder()->field_is_inlined(fd.index()));
+      if (fd.holder()->field_is_inlined(fd.index())) {
+        print_inlined_field(st, 1, fd.offset(),
             InstanceKlass::cast(fd.holder()->get_value_field_klass(fd.index())));
       }
     }
   }
   st->cr();
diff a/src/hotspot/share/oops/access.hpp b/src/hotspot/share/oops/access.hpp
--- a/src/hotspot/share/oops/access.hpp
+++ b/src/hotspot/share/oops/access.hpp
@@ -218,11 +218,11 @@
   static inline void clone(oop src, oop dst, size_t size) {
     verify_decorators<IN_HEAP>();
     AccessInternal::clone<decorators>(src, dst, size);
   }
 
-  // Value type inline heap access (flattened)...
+  // inline type heap access (when inlined)...
 
   // Copy value type data from src to dst
   static inline void value_copy(void* src, void* dst, ValueKlass* md) {
     verify_heap_value_decorators<IN_HEAP>();
     AccessInternal::value_copy<decorators>(src, dst, md);
diff a/src/hotspot/share/oops/cpCache.cpp b/src/hotspot/share/oops/cpCache.cpp
--- a/src/hotspot/share/oops/cpCache.cpp
+++ b/src/hotspot/share/oops/cpCache.cpp
@@ -131,23 +131,23 @@
                                        int field_index,
                                        int field_offset,
                                        TosState field_type,
                                        bool is_final,
                                        bool is_volatile,
-                                       bool is_flattened,
-                                       bool is_flattenable,
+                                       bool is_inlined,
+                                       bool is_inline_type,
                                        Klass* root_klass) {
   set_f1(field_holder);
   set_f2(field_offset);
   assert((field_index & field_index_mask) == field_index,
          "field index does not fit in low flag bits");
-  assert(!is_flattened || is_flattenable, "Sanity check");
+  assert(!is_inlined || is_inline_type, "Sanity check");
   set_field_flags(field_type,
                   ((is_volatile ? 1 : 0) << is_volatile_shift) |
                   ((is_final    ? 1 : 0) << is_final_shift) |
-                  ((is_flattened  ? 1 : 0) << is_flattened_field_shift) |
-                  ((is_flattenable ? 1 : 0) << is_flattenable_field_shift),
+                  ((is_inlined  ? 1 : 0) << is_inlined_shift) |
+                  ((is_inline_type ? 1 : 0) << is_inline_type_shift),
                   field_index);
   set_bytecode_1(get_code);
   set_bytecode_2(put_code);
   NOT_PRODUCT(verify(tty));
 }
diff a/src/hotspot/share/oops/cpCache.hpp b/src/hotspot/share/oops/cpCache.hpp
--- a/src/hotspot/share/oops/cpCache.hpp
+++ b/src/hotspot/share/oops/cpCache.hpp
@@ -49,11 +49,11 @@
 // bit length |-8--|-8--|---16----|
 // --------------------------------
 // _indices   [ b2 | b1 |  index  ]  index = constant_pool_index
 // _f1        [  entry specific   ]  metadata ptr (method or klass)
 // _f2        [  entry specific   ]  vtable or res_ref index, or vfinal method ptr
-// _flags     [tos|0|F=1|0|N|i|f|v|0 |0000|field_index] (for field entries)
+// _flags     [tos|0|F=1|0|I|i|f|v|0 |0000|field_index] (for field entries)
 // bit length [ 4 |1| 1 |1|1|1|1|1|1 |1     |-3-|----16-----]
 // _flags     [tos|0|F=0|S|A|I|f|0|vf|indy_rf|000|00000|psize] (for method entries)
 // bit length [ 4 |1| 1 |1|1|1|1|1|1 |-4--|--8--|--8--]
 
 // --------------------------------
@@ -75,12 +75,12 @@
 // vf     = virtual but final (method entries only: is_vfinal())
 // indy_rf = call site specifier method resolution failed
 //
 // The flags after TosState have the following interpretation:
 // bit 27: 0 for fields, 1 for methods
-// N  flag true if field is marked flattenable (must never be null)
-// i  flag true if field is inlined (flattened)
+// I  flag true if field is an inline type (must never be null)
+// i  flag true if field is inlined
 // f  flag true if field is marked final
 // v  flag true if field is volatile (only for fields)
 // f2 flag true if f2 contains an oop (e.g., virtual final method)
 // fv flag true if invokeinterface used for method in class Object
 //
@@ -184,13 +184,13 @@
     tos_state_shift            = BitsPerInt - tos_state_bits,  // see verify_tos_state_shift below
     // misc. option bits; can be any bit position in [16..27]
     is_field_entry_shift       = 26,  // (F) is it a field or a method?
     has_local_signature_shift  = 25,  // (S) does the call site have a per-site signature (sig-poly methods)?
     has_appendix_shift         = 24,  // (A) does the call site have an appendix argument?
-    is_flattenable_field_shift = 24,  // (N) is the field flattenable (must never be null)
+    is_inline_type_shift       = 24,  // (I) is the type of the field an inline type (must never be null)
     is_forced_virtual_shift    = 23,  // (I) is the interface reference forced to virtual mode?
-    is_flattened_field_shift   = 23,  // (i) is the value field flattened?
+    is_inlined_shift           = 23,  // (i) is the field inlined?
     is_final_shift             = 22,  // (f) is the field or method final?
     is_volatile_shift          = 21,  // (v) is the field volatile?
     is_vfinal_shift            = 20,  // (vf) did the call resolve to a final method?
     indy_resolution_failed_shift= 19, // (indy_rf) did call site specifier resolution fail ?
     // low order bits give field index (for FieldInfo) or method parameter size:
@@ -226,12 +226,12 @@
     int             orig_field_index,            // the original field index in the field holder
     int             field_offset,                // the field offset in words in the field holder
     TosState        field_type,                  // the (machine) field type
     bool            is_final,                    // the field is final
     bool            is_volatile,                 // the field is volatile
-    bool            is_flattened,                // the field is flattened (value field)
-    bool            is_flattenable,              // the field is flattenable (must never be null)
+    bool            is_inlined,                  // the field is inlined
+    bool            is_inline_type,              // the field is an inline type (must never be null)
     Klass*          root_klass                   // needed by the GC to dirty the klass
   );
 
  private:
   void set_direct_or_vtable_call(
@@ -351,21 +351,21 @@
   intx flags_ord() const;
   int  field_index() const                       { assert(is_field_entry(),  ""); return (_flags & field_index_mask); }
   int  parameter_size() const                    { assert(is_method_entry(), ""); return (_flags & parameter_size_mask); }
   bool is_volatile() const                       { return (_flags & (1 << is_volatile_shift))       != 0; }
   bool is_final() const                          { return (_flags & (1 << is_final_shift))          != 0; }
-  bool is_flattened() const                      { return  (_flags & (1 << is_flattened_field_shift))       != 0; }
+  bool is_inlined() const                        { return  (_flags & (1 << is_inlined_shift))       != 0; }
   bool is_forced_virtual() const                 { return (_flags & (1 << is_forced_virtual_shift)) != 0; }
   bool is_vfinal() const                         { return (_flags & (1 << is_vfinal_shift))         != 0; }
   bool indy_resolution_failed() const;
   bool has_appendix() const;
   bool has_local_signature() const;
   bool is_method_entry() const                   { return (_flags & (1 << is_field_entry_shift))    == 0; }
   bool is_field_entry() const                    { return (_flags & (1 << is_field_entry_shift))    != 0; }
   bool is_long() const                           { return flag_state() == ltos; }
   bool is_double() const                         { return flag_state() == dtos; }
-  bool is_flattenable() const                    { return (_flags & (1 << is_flattenable_field_shift))       != 0; }
+  bool is_inline_type() const                    { return (_flags & (1 << is_inline_type_shift))       != 0; }
   TosState flag_state() const                    { assert((uint)number_of_states <= (uint)tos_state_mask+1, "");
                                                    return (TosState)((_flags >> tos_state_shift) & tos_state_mask); }
   void set_indy_resolution_failed();
 
   // Code generation support
diff a/src/hotspot/share/oops/fieldInfo.hpp b/src/hotspot/share/oops/fieldInfo.hpp
--- a/src/hotspot/share/oops/fieldInfo.hpp
+++ b/src/hotspot/share/oops/fieldInfo.hpp
@@ -50,21 +50,21 @@
 #define FIELDINFO_TAG_OFFSET           1
 #define FIELDINFO_TAG_TYPE_PLAIN       2
 #define FIELDINFO_TAG_TYPE_CONTENDED   3
 #define FIELDINFO_TAG_TYPE_MASK        3
 #define FIELDINFO_TAG_MASK             7
-#define FIELDINFO_TAG_FLATTENED        4
+#define FIELDINFO_TAG_INLINED          4
 
   // Packed field has the tag, and can be either of:
   //    hi bits <--------------------------- lo bits
   //   |---------high---------|---------low---------|
   //    ..........................................00  - blank
-  //    [------------------offset---------------]F01  - real field offset
-  //    ......................[-------type------]F10  - plain field with type
-  //    [--contention_group--][-------type------]F11  - contended field with type and contention group
+  //    [------------------offset---------------]I01  - real field offset
+  //    ......................[-------type------]I10  - plain field with type
+  //    [--contention_group--][-------type------]I11  - contended field with type and contention group
   //
-  // Bit F indicates if the field has been flattened (F=1) or nor (F=0)
+  // Bit I indicates if the field has been inlined  (I=1) or nor (I=0)
 
   enum FieldOffset {
     access_flags_offset      = 0,
     name_index_offset        = 1,
     signature_index_offset   = 2,
@@ -198,26 +198,26 @@
   }
 
   void set_access_flags(u2 val)                  { _shorts[access_flags_offset] = val;             }
   void set_offset(u4 val)                        {
     val = val << FIELDINFO_TAG_SIZE; // make room for tag
-    bool flattened = is_flattened();
+    bool inlined = is_inlined();
     _shorts[low_packed_offset] = extract_low_short_from_int(val) | FIELDINFO_TAG_OFFSET;
-    if (flattened) set_flattened(true);
+    if (inlined) set_inlined(true);
     _shorts[high_packed_offset] = extract_high_short_from_int(val);
-    assert(is_flattened() || !flattened, "just checking");
+    assert(is_inlined() || !inlined, "just checking");
   }
 
   void set_allocation_type(int type) {
-    bool b = is_flattened();
+    bool b = is_inlined();
     u2 lo = _shorts[low_packed_offset];
     switch(lo & FIELDINFO_TAG_TYPE_MASK) {
       case FIELDINFO_TAG_BLANK:
         _shorts[low_packed_offset] |= ((type << FIELDINFO_TAG_SIZE)) & 0xFFFF;
         _shorts[low_packed_offset] &= ~FIELDINFO_TAG_TYPE_MASK;
         _shorts[low_packed_offset] |= FIELDINFO_TAG_TYPE_PLAIN;
-        assert(is_flattened() || !b, "Just checking");
+        assert(is_inlined() || !b, "Just checking");
         return;
 #ifndef PRODUCT
       case FIELDINFO_TAG_TYPE_PLAIN:
       case FIELDINFO_TAG_TYPE_CONTENDED:
       case FIELDINFO_TAG_OFFSET:
@@ -225,20 +225,20 @@
 #endif
     }
     ShouldNotReachHere();
   }
 
-  void set_flattened(bool b) {
+  void set_inlined(bool b) {
     if (b) {
-      _shorts[low_packed_offset] |= FIELDINFO_TAG_FLATTENED;
+      _shorts[low_packed_offset] |= FIELDINFO_TAG_INLINED;
     } else {
-      _shorts[low_packed_offset] &= ~FIELDINFO_TAG_FLATTENED;
+      _shorts[low_packed_offset] &= ~FIELDINFO_TAG_INLINED;
     }
   }
 
-  bool is_flattened() {
-    return (_shorts[low_packed_offset] & FIELDINFO_TAG_FLATTENED) != 0;
+  bool is_inlined() {
+    return (_shorts[low_packed_offset] & FIELDINFO_TAG_INLINED) != 0;
   }
 
   void set_contended_group(u2 val) {
     u2 lo = _shorts[low_packed_offset];
     switch(lo & FIELDINFO_TAG_TYPE_MASK) {
@@ -268,14 +268,10 @@
   void set_stable(bool z) {
     if (z) _shorts[access_flags_offset] |=  JVM_ACC_FIELD_STABLE;
     else   _shorts[access_flags_offset] &= ~JVM_ACC_FIELD_STABLE;
   }
 
-  bool is_flattenable() const {
-    return (access_flags() & JVM_ACC_FLATTENABLE) != 0;
-  }
-
   Symbol* lookup_symbol(int symbol_index) const {
     assert(is_internal(), "only internal fields");
     return vmSymbols::symbol_at((vmSymbols::SID)symbol_index);
   }
 };
diff a/src/hotspot/share/oops/fieldStreams.hpp b/src/hotspot/share/oops/fieldStreams.hpp
--- a/src/hotspot/share/oops/fieldStreams.hpp
+++ b/src/hotspot/share/oops/fieldStreams.hpp
@@ -137,22 +137,16 @@
 
   int allocation_type() const {
     return field()->allocation_type();
   }
 
-  bool is_flattened() {
-    return field()->is_flattened();
+  bool is_inlined() {
+    return field()->is_inlined();
   }
 
-  void set_flattened(bool b) {
-    field()->set_flattened(b);
-  }
-
-  bool is_flattenable() const {
-    AccessFlags flags;
-    flags.set_flags(field()->access_flags());
-    return flags.is_flattenable();
+  void set_inlined(bool b) {
+    field()->set_inlined(b);
   }
 
   void set_offset(int offset) {
     field()->set_offset(offset);
   }
diff a/src/hotspot/share/oops/instanceKlass.cpp b/src/hotspot/share/oops/instanceKlass.cpp
--- a/src/hotspot/share/oops/instanceKlass.cpp
+++ b/src/hotspot/share/oops/instanceKlass.cpp
@@ -154,10 +154,12 @@
     }
   }
   return false;
 }
 
+bool InstanceKlass::field_is_inline_type(int index) const { return Signature::basic_type(field(index)->signature(constants())) == T_VALUETYPE; }
+
 // private: called to verify that k is a static member of this nest.
 // We know that k is an instance class in the same package and hence the
 // same classloader.
 bool InstanceKlass::has_nest_member(InstanceKlass* k, TRAPS) const {
   assert(!is_hidden(), "unexpected hidden class");
@@ -473,11 +475,11 @@
                                        parser.itable_size(),
                                        nonstatic_oop_map_size(parser.total_oop_map_count()),
                                        parser.is_interface(),
                                        parser.is_unsafe_anonymous(),
                                        should_store_fingerprint(is_hidden_or_anonymous),
-                                       parser.has_flattenable_fields() ? parser.java_fields_count() : 0,
+                                       parser.has_inline_fields() ? parser.java_fields_count() : 0,
                                        parser.is_inline_type());
 
   const Symbol* const class_name = parser.class_name();
   assert(class_name != NULL, "invariant");
   ClassLoaderData* loader_data = parser.loader_data();
@@ -587,11 +589,11 @@
   set_access_flags(parser.access_flags());
   if (parser.is_hidden()) set_is_hidden();
   set_is_unsafe_anonymous(parser.is_unsafe_anonymous());
   set_layout_helper(Klass::instance_layout_helper(parser.layout_size(),
                                                     false));
-    if (parser.has_flattenable_fields()) {
+    if (parser.has_inline_fields()) {
       set_has_inline_fields();
     }
     _java_fields_count = parser.java_fields_count();
 
     assert(NULL == _methods, "underlying memory not zeroed?");
@@ -985,11 +987,11 @@
   // linked (and have performed their own pre-loading) before the linking
   // of the current class.
 
 
   // Note:
-  // Inline class types used for flattenable fields are loaded during
+  // Inline class types are loaded during
   // the loading phase (see ClassFileParser::post_process_parsed_stream()).
   // Inline class types used as element types for array creation
   // are not pre-loaded. Their loading is triggered by either anewarray
   // or multianewarray bytecodes.
 
@@ -1252,18 +1254,17 @@
       THROW_OOP(e());
     }
   }
 
   // Step 8
-  // Initialize classes of flattenable fields
+  // Initialize classes of inline fields
   {
     for (AllFieldStream fs(this); !fs.done(); fs.next()) {
-      if (fs.is_flattenable()) {
+      if (Signature::basic_type(fs.signature()) == T_VALUETYPE) {
         Klass* klass = this->get_value_field_klass_or_null(fs.index());
         if (klass == NULL) {
-          assert(fs.access_flags().is_static() && fs.access_flags().is_flattenable(),
-              "Otherwise should have been pre-loaded");
+          assert(fs.access_flags().is_static(), "Otherwise should have been pre-loaded");
           klass = SystemDictionary::resolve_or_fail(field_signature(fs.index())->fundamental_name(THREAD),
               Handle(THREAD, class_loader()),
               Handle(THREAD, protection_domain()),
               true, CHECK);
           if (klass == NULL) {
diff a/src/hotspot/share/oops/instanceKlass.hpp b/src/hotspot/share/oops/instanceKlass.hpp
--- a/src/hotspot/share/oops/instanceKlass.hpp
+++ b/src/hotspot/share/oops/instanceKlass.hpp
@@ -546,12 +546,12 @@
  public:
   int     field_offset      (int index) const { return field(index)->offset(); }
   int     field_access_flags(int index) const { return field(index)->access_flags(); }
   Symbol* field_name        (int index) const { return field(index)->name(constants()); }
   Symbol* field_signature   (int index) const { return field(index)->signature(constants()); }
-  bool    field_is_flattened(int index) const { return field(index)->is_flattened(); }
-  bool    field_is_flattenable(int index) const { return field(index)->is_flattenable(); }
+  bool    field_is_inlined(int index) const { return field(index)->is_inlined(); }
+  bool    field_is_inline_type(int index) const;
 
   // Number of Java declared fields
   int java_fields_count() const           { return (int)_java_fields_count; }
 
   Array<u2>* fields() const            { return _fields; }
diff a/src/hotspot/share/oops/valueKlass.cpp b/src/hotspot/share/oops/valueKlass.cpp
--- a/src/hotspot/share/oops/valueKlass.cpp
+++ b/src/hotspot/share/oops/valueKlass.cpp
@@ -150,11 +150,11 @@
     block++;
   }
   return oops;
 }
 
-oop ValueKlass::read_flattened_field(oop obj, int offset, TRAPS) {
+oop ValueKlass::read_inlined_field(oop obj, int offset, TRAPS) {
   oop res = NULL;
   this->initialize(CHECK_NULL); // will throw an exception if in error state
   if (is_empty_inline_type()) {
     res = (instanceOop)default_value();
   } else {
@@ -164,11 +164,11 @@
   }
   assert(res != NULL, "Must be set in one of two paths above");
   return res;
 }
 
-void ValueKlass::write_flattened_field(oop obj, int offset, oop value, TRAPS) {
+void ValueKlass::write_inlined_field(oop obj, int offset, oop value, TRAPS) {
   if (value == NULL) {
     THROW(vmSymbols::java_lang_NullPointerException());
   }
   if (!is_empty_inline_type()) {
     value_copy_oop_to_payload(value, ((char*)(oopDesc*)obj) + offset);
@@ -271,21 +271,21 @@
     ArrayKlass::cast(get_value_array_klass())->array_klasses_do(f);
 }
 
 // Value type arguments are not passed by reference, instead each
 // field of the value type is passed as an argument. This helper
-// function collects the fields of the value types (including embedded
-// value type's fields) in a list. Included with the field's type is
-// the offset of each field in the value type: i2c and c2i adapters
+// function collects the inlined field (recursively)
+// in a list. Included with the field's type is
+// the offset of each field in the inline type: i2c and c2i adapters
 // need that to load or store fields. Finally, the list of fields is
 // sorted in order of increasing offsets: the adapters and the
 // compiled code need to agree upon the order of fields.
 //
 // The list of basic types that is returned starts with a T_VALUETYPE
 // and ends with an extra T_VOID. T_VALUETYPE/T_VOID pairs are used as
 // delimiters. Every entry between the two is a field of the value
-// type. If there's an embedded value type in the list, it also starts
+// type. If there's an embedded inline type in the list, it also starts
 // with a T_VALUETYPE and ends with a T_VOID. This is so we can
 // generate a unique fingerprint for the method's adapters and we can
 // generate the list of basic types from the interpreter point of view
 // (value types passed as reference: iterate on the list until a
 // T_VALUETYPE, drop everything until and including the closing
@@ -295,12 +295,12 @@
   int count = 0;
   SigEntry::add_entry(sig, T_VALUETYPE, base_off);
   for (AllFieldStream fs(this); !fs.done(); fs.next()) {
     if (fs.access_flags().is_static()) continue;
     int offset = base_off + fs.offset() - (base_off > 0 ? first_field_offset() : 0);
-    if (fs.is_flattened()) {
-      // Resolve klass of flattened value type field and recursively collect fields
+    if (fs.is_inlined()) {
+      // Resolve klass of inlined field and recursively collect fields
       Klass* vk = get_value_field_klass(fs.index());
       count += ValueKlass::cast(vk)->collect_fields(sig, offset);
     } else {
       BasicType bt = Signature::basic_type(fs.signature());
       if (bt == T_VALUETYPE) {
diff a/src/hotspot/share/oops/valueKlass.hpp b/src/hotspot/share/oops/valueKlass.hpp
--- a/src/hotspot/share/oops/valueKlass.hpp
+++ b/src/hotspot/share/oops/valueKlass.hpp
@@ -229,12 +229,12 @@
   void value_copy_payload_to_new_oop(void* src, oop dst);
   void value_copy_oop_to_new_oop(oop src, oop dst);
   void value_copy_oop_to_new_payload(oop src, void* dst);
   void value_copy_oop_to_payload(oop src, void* dst);
 
-  oop read_flattened_field(oop obj, int offset, TRAPS);
-  void write_flattened_field(oop obj, int offset, oop value, TRAPS);
+  oop read_inlined_field(oop obj, int offset, TRAPS);
+  void write_inlined_field(oop obj, int offset, oop value, TRAPS);
 
   // oop iterate raw value type data pointer (where oop_addr may not be an oop, but backing/array-element)
   template <typename T, class OopClosureType>
   inline void oop_iterate_specialized(const address oop_addr, OopClosureType* closure);
 
diff a/src/hotspot/share/prims/jni.cpp b/src/hotspot/share/prims/jni.cpp
--- a/src/hotspot/share/prims/jni.cpp
+++ b/src/hotspot/share/prims/jni.cpp
@@ -476,13 +476,13 @@
 
   // The slot is the index of the field description in the field-array
   // The jfieldID is the offset of the field within the object
   // It may also have hash bits for k, if VerifyJNIFields is turned on.
   intptr_t offset = InstanceKlass::cast(k1)->field_offset( slot );
-  bool is_flattened = InstanceKlass::cast(k1)->field_is_flattened(slot);
+  bool is_inlined = InstanceKlass::cast(k1)->field_is_inlined(slot);
   assert(InstanceKlass::cast(k1)->contains_field_offset(offset), "stay within object");
-  ret = jfieldIDWorkaround::to_instance_jfieldID(k1, offset, is_flattened);
+  ret = jfieldIDWorkaround::to_instance_jfieldID(k1, offset, is_inlined);
   return ret;
 JNI_END
 
 
 DT_RETURN_MARK_DECL(ToReflectedMethod, jobject
@@ -1940,11 +1940,11 @@
     THROW_MSG_0(vmSymbols::java_lang_NoSuchFieldError(), err_msg("%s.%s %s", k->external_name(), name, sig));
   }
 
   // A jfieldID for a non-static field is simply the offset of the field within the instanceOop
   // It may also have hash bits for k, if VerifyJNIFields is turned on.
-  ret = jfieldIDWorkaround::to_instance_jfieldID(k, fd.offset(), fd.is_flattened());
+  ret = jfieldIDWorkaround::to_instance_jfieldID(k, fd.offset(), fd.is_inlined());
   return ret;
 JNI_END
 
 
 JNI_ENTRY(jobject, jni_GetObjectField(JNIEnv *env, jobject obj, jfieldID fieldID))
@@ -1957,20 +1957,20 @@
   // Keep JVMTI addition small and only check enabled flag here.
   // jni_GetField_probe() assumes that is okay to create handles.
   if (JvmtiExport::should_post_field_access()) {
     o = JvmtiExport::jni_GetField_probe(thread, obj, o, k, fieldID, false);
   }
-  if (!jfieldIDWorkaround::is_flattened_field(fieldID)) {
+  if (!jfieldIDWorkaround::is_inlined_jfieldID(fieldID)) {
     res = HeapAccess<ON_UNKNOWN_OOP_REF>::oop_load_at(o, offset);
   } else {
-    assert(k->is_instance_klass(), "Only instance can have flattened fields");
+    assert(k->is_instance_klass(), "Only instance can have inlined fields");
     InstanceKlass* ik = InstanceKlass::cast(k);
     fieldDescriptor fd;
     ik->find_field_from_offset(offset, false, &fd);  // performance bottleneck
     InstanceKlass* holder = fd.field_holder();
     ValueKlass* field_vklass = ValueKlass::cast(holder->get_value_field_klass(fd.index()));
-    res = field_vklass->read_flattened_field(o, ik->field_offset(fd.index()), CHECK_NULL);
+    res = field_vklass->read_inlined_field(o, ik->field_offset(fd.index()), CHECK_NULL);
   }
   jobject ret = JNIHandles::make_local(env, res);
   HOTSPOT_JNI_GETOBJECTFIELD_RETURN(ret);
   return ret;
 JNI_END
@@ -2066,21 +2066,21 @@
   if (JvmtiExport::should_post_field_modification()) {
     jvalue field_value;
     field_value.l = value;
     o = JvmtiExport::jni_SetField_probe_nh(thread, obj, o, k, fieldID, false, JVM_SIGNATURE_CLASS, (jvalue *)&field_value);
   }
-  if (!jfieldIDWorkaround::is_flattened_field(fieldID)) {
+  if (!jfieldIDWorkaround::is_inlined_jfieldID(fieldID)) {
     HeapAccess<ON_UNKNOWN_OOP_REF>::oop_store_at(o, offset, JNIHandles::resolve(value));
   } else {
-    assert(k->is_instance_klass(), "Only instances can have flattened fields");
+    assert(k->is_instance_klass(), "Only instances can have inlined fields");
     InstanceKlass* ik = InstanceKlass::cast(k);
     fieldDescriptor fd;
     ik->find_field_from_offset(offset, false, &fd);
     InstanceKlass* holder = fd.field_holder();
     ValueKlass* vklass = ValueKlass::cast(holder->get_value_field_klass(fd.index()));
     oop v = JNIHandles::resolve_non_null(value);
-    vklass->write_flattened_field(o, offset, v, CHECK);
+    vklass->write_inlined_field(o, offset, v, CHECK);
   }
   HOTSPOT_JNI_SETOBJECTFIELD_RETURN();
 JNI_END
 
 
@@ -3439,11 +3439,11 @@
   ValueArrayKlass* vak = ValueArrayKlass::cast(a->klass());
   ValueKlass* vk = vak->element_klass();
   return (jclass) JNIHandles::make_local(vk->java_mirror());
 JNI_END
 
-JNI_ENTRY(jsize, jni_GetFieldOffsetInFlattenedLayout(JNIEnv* env, jclass clazz, const char *name, const char *signature, jboolean* isFlattened))
+JNI_ENTRY(jsize, jni_GetFieldOffsetInFlattenedLayout(JNIEnv* env, jclass clazz, const char *name, const char *signature, jboolean* is_inlined))
   JNIWrapper("jni_GetFieldOffsetInFlattenedLayout");
 
   oop mirror = JNIHandles::resolve_non_null(clazz);
   Klass* k = java_lang_Class::as_Klass(mirror);
   if (!k->is_value()) {
@@ -3467,12 +3467,12 @@
     ResourceMark rm;
     THROW_MSG_0(vmSymbols::java_lang_NoSuchFieldError(), err_msg("%s.%s %s", vk->external_name(), name, signature));
   }
 
   int offset = fd.offset() - vk->first_field_offset();
-  if (isFlattened != NULL) {
-    *isFlattened = fd.is_flattened();
+  if (is_inlined != NULL) {
+    *is_inlined = fd.is_inlined();
   }
   return (jsize)offset;
 JNI_END
 
 JNI_ENTRY(jobject, jni_CreateSubElementSelector(JNIEnv* env, jarray array))
@@ -3493,25 +3493,25 @@
   oop ses = ses_ik->allocate_instance(CHECK_NULL);
   Handle ses_h(THREAD, ses);
   jdk_internal_vm_jni_SubElementSelector::setArrayElementType(ses_h(), elementKlass->java_mirror());
   jdk_internal_vm_jni_SubElementSelector::setSubElementType(ses_h(), elementKlass->java_mirror());
   jdk_internal_vm_jni_SubElementSelector::setOffset(ses_h(), 0);
-  jdk_internal_vm_jni_SubElementSelector::setIsFlattened(ses_h(), true);   // by definition, top element of a flattened array is flattened
-  jdk_internal_vm_jni_SubElementSelector::setIsFlattenable(ses_h(), true); // by definition, top element of a flattened array is flattenable
+  jdk_internal_vm_jni_SubElementSelector::setIsInlined(ses_h(), true);   // by definition, top element of a flattened array is inlined
+  jdk_internal_vm_jni_SubElementSelector::setIsInlineType(ses_h(), true); // by definition, top element of a flattened array is an inline type
   return JNIHandles::make_local(ses_h());
 JNI_END
 
 JNI_ENTRY(jobject, jni_GetSubElementSelector(JNIEnv* env, jobject selector, jfieldID fieldID))
   JNIWrapper("jni_GetSubElementSelector");
 
   oop slct = JNIHandles::resolve_non_null(selector);
   if (slct->klass()->name() != vmSymbols::jdk_internal_vm_jni_SubElementSelector()) {
     THROW_MSG_NULL(vmSymbols::java_lang_IllegalArgumentException(), "Not a SubElementSelector");
   }
-  jboolean isflattened = jdk_internal_vm_jni_SubElementSelector::getIsFlattened(slct);
-  if (!isflattened) {
-    THROW_MSG_NULL(vmSymbols::java_lang_IllegalArgumentException(), "SubElement is not flattened");
+  jboolean is_inlined = jdk_internal_vm_jni_SubElementSelector::getIsInlined(slct);
+  if (!is_inlined) {
+    THROW_MSG_NULL(vmSymbols::java_lang_IllegalArgumentException(), "SubElement is not inlined");
   }
   oop semirror = jdk_internal_vm_jni_SubElementSelector::getSubElementType(slct);
   Klass* k = java_lang_Class::as_Klass(semirror);
   if (!k->is_value()) {
     ResourceMark rm;
@@ -3539,12 +3539,12 @@
     Klass* fieldKlass = SystemDictionary::resolve_or_fail(fd.signature(), Handle(THREAD, holder->class_loader()),
         Handle(THREAD, holder->protection_domain()), true, CHECK_NULL);
     jdk_internal_vm_jni_SubElementSelector::setSubElementType(res_h(),fieldKlass->java_mirror());
   }
   jdk_internal_vm_jni_SubElementSelector::setOffset(res_h(), offset);
-  jdk_internal_vm_jni_SubElementSelector::setIsFlattened(res_h(), fd.is_flattened());
-  jdk_internal_vm_jni_SubElementSelector::setIsFlattenable(res_h(), fd.is_flattenable());
+  jdk_internal_vm_jni_SubElementSelector::setIsInlined(res_h(), fd.is_inlined());
+  jdk_internal_vm_jni_SubElementSelector::setIsInlineType(res_h(), fd.is_inline_type());
   return JNIHandles::make_local(res_h());
 JNI_END
 
 JNI_ENTRY(jobject, jni_GetObjectSubElement(JNIEnv* env, jarray array, jobject selector, int index))
   JNIWrapper("jni_GetObjectSubElement");
@@ -3554,11 +3554,11 @@
   ValueArrayKlass* vak = ValueArrayKlass::cast(ar->klass());
   if (jdk_internal_vm_jni_SubElementSelector::getArrayElementType(slct) != vak->element_klass()->java_mirror()) {
     THROW_MSG_NULL(vmSymbols::java_lang_IllegalArgumentException(), "Array/Selector mismatch");
   }
   oop res = NULL;
-  if (!jdk_internal_vm_jni_SubElementSelector::getIsFlattened(slct)) {
+  if (!jdk_internal_vm_jni_SubElementSelector::getIsInlined(slct)) {
     int offset = (address)ar->base() - cast_from_oop<address>(ar) + index * vak->element_byte_size()
                       + jdk_internal_vm_jni_SubElementSelector::getOffset(slct);
     res = HeapAccess<ON_UNKNOWN_OOP_REF>::oop_load_at(ar, offset);
   } else {
     ValueKlass* fieldKlass = ValueKlass::cast(java_lang_Class::as_Klass(jdk_internal_vm_jni_SubElementSelector::getSubElementType(slct)));
@@ -3581,19 +3581,19 @@
   if (jdk_internal_vm_jni_SubElementSelector::getArrayElementType(slct) != vak->element_klass()->java_mirror()) {
     THROW_MSG(vmSymbols::java_lang_IllegalArgumentException(), "Array/Selector mismatch");
   }
   oop val = JNIHandles::resolve(value);
   if (val == NULL) {
-    if (jdk_internal_vm_jni_SubElementSelector::getIsFlattenable(slct)) {
+    if (jdk_internal_vm_jni_SubElementSelector::getIsInlineType(slct)) {
       THROW_MSG(vmSymbols::java_lang_ArrayStoreException(), "null cannot be stored in a flattened array");
     }
   } else {
     if (!val->is_a(java_lang_Class::as_Klass(jdk_internal_vm_jni_SubElementSelector::getSubElementType(slct)))) {
       THROW_MSG(vmSymbols::java_lang_ArrayStoreException(), "type mismatch");
     }
   }
-  if (!jdk_internal_vm_jni_SubElementSelector::getIsFlattened(slct)) {
+  if (!jdk_internal_vm_jni_SubElementSelector::getIsInlined(slct)) {
     int offset = (address)ar->base() - cast_from_oop<address>(ar) + index * vak->element_byte_size()
                   + jdk_internal_vm_jni_SubElementSelector::getOffset(slct);
     HeapAccess<ON_UNKNOWN_OOP_REF>::oop_store_at(ar, offset, JNIHandles::resolve(value));
   } else {
     ValueKlass* fieldKlass = ValueKlass::cast(java_lang_Class::as_Klass(jdk_internal_vm_jni_SubElementSelector::getSubElementType(slct)));
diff a/src/hotspot/share/prims/jvmtiEnv.cpp b/src/hotspot/share/prims/jvmtiEnv.cpp
--- a/src/hotspot/share/prims/jvmtiEnv.cpp
+++ b/src/hotspot/share/prims/jvmtiEnv.cpp
@@ -2590,11 +2590,11 @@
 
   for (FilteredFieldStream src_st(ik, true, true); !src_st.eos(); src_st.next()) {
     result_list[id_index--] = jfieldIDWorkaround::to_jfieldID(
                                             ik, src_st.offset(),
                                             src_st.access_flags().is_static(),
-                                            src_st.field_descriptor().is_flattened());
+                                            src_st.field_descriptor().is_inlined());
   }
   assert(id_index == -1, "just checking");
   // Fill in the results
   *field_count_ptr = result_count;
   *fields_ptr = result_list;
diff a/src/hotspot/share/prims/methodHandles.cpp b/src/hotspot/share/prims/methodHandles.cpp
--- a/src/hotspot/share/prims/methodHandles.cpp
+++ b/src/hotspot/share/prims/methodHandles.cpp
@@ -339,15 +339,12 @@
 }
 
 oop MethodHandles::init_field_MemberName(Handle mname, fieldDescriptor& fd, bool is_setter) {
   int flags = (jushort)( fd.access_flags().as_short() & JVM_RECOGNIZED_FIELD_MODIFIERS );
   flags |= IS_FIELD | ((fd.is_static() ? JVM_REF_getStatic : JVM_REF_getField) << REFERENCE_KIND_SHIFT);
-  if (fd.is_flattenable()) {
-    flags |= JVM_ACC_FIELD_FLATTENABLE;
-  }
-    if (fd.is_flattened()) {
-    flags |= JVM_ACC_FIELD_FLATTENED;
+  if (fd.is_inlined()) {
+    flags |= JVM_ACC_FIELD_INLINED;
   }
   if (is_setter)  flags += ((JVM_REF_putField - JVM_REF_getField) << REFERENCE_KIND_SHIFT);
   int vmindex        = fd.offset();  // determines the field uniquely when combined with static bit
 
   oop mname_oop = mname();
diff a/src/hotspot/share/prims/unsafe.cpp b/src/hotspot/share/prims/unsafe.cpp
--- a/src/hotspot/share/prims/unsafe.cpp
+++ b/src/hotspot/share/prims/unsafe.cpp
@@ -292,11 +292,11 @@
     assert_field_offset_sane(p, offset);
     fieldDescriptor fd;
     bool found = get_field_descriptor(p, offset, &fd);
     if (found) {
       assert(found, "value field not found");
-      assert(fd.is_flattened(), "field not flat");
+      assert(fd.is_inlined(), "field not flat");
     } else {
       if (log_is_enabled(Trace, valuetypes)) {
         log_trace(valuetypes)("not a field in %s at offset " SIZE_FORMAT_HEX,
                               p->klass()->external_name(), offset);
       }
@@ -365,22 +365,22 @@
   oop base = JNIHandles::resolve(obj);
   Klass* k = java_lang_Class::as_Klass(JNIHandles::resolve_non_null(vc));
   ValueKlass* vk = ValueKlass::cast(k);
   assert_and_log_unsafe_value_access(base, offset, vk);
   Handle base_h(THREAD, base);
-  oop v = vk->read_flattened_field(base_h(), offset, CHECK_NULL);
+  oop v = vk->read_inlined_field(base_h(), offset, CHECK_NULL);
   return JNIHandles::make_local(env, v);
 } UNSAFE_END
 
 UNSAFE_ENTRY(void, Unsafe_PutValue(JNIEnv *env, jobject unsafe, jobject obj, jlong offset, jclass vc, jobject value)) {
   oop base = JNIHandles::resolve(obj);
   Klass* k = java_lang_Class::as_Klass(JNIHandles::resolve_non_null(vc));
   ValueKlass* vk = ValueKlass::cast(k);
   assert(!base->is_value() || base->mark().is_larval_state(), "must be an object instance or a larval value");
   assert_and_log_unsafe_value_access(base, offset, vk);
   oop v = JNIHandles::resolve(value);
-  vk->write_flattened_field(base, offset, v, CHECK);
+  vk->write_inlined_field(base, offset, v, CHECK);
 } UNSAFE_END
 
 UNSAFE_ENTRY(jobject, Unsafe_MakePrivateBuffer(JNIEnv *env, jobject unsafe, jobject value)) {
   oop v = JNIHandles::resolve_non_null(value);
   assert(v->is_value(), "must be a value instance");
diff a/src/hotspot/share/runtime/deoptimization.cpp b/src/hotspot/share/runtime/deoptimization.cpp
--- a/src/hotspot/share/runtime/deoptimization.cpp
+++ b/src/hotspot/share/runtime/deoptimization.cpp
@@ -1282,11 +1282,11 @@
         field._offset = fs.offset();
         field._type = Signature::basic_type(fs.signature());
         if (field._type == T_VALUETYPE) {
           field._type = T_OBJECT;
         }
-        if (fs.is_flattened()) {
+        if (fs.is_inlined()) {
           // Resolve klass of flattened value type field
           Klass* vk = klass->get_value_field_klass(fs.index());
           field._klass = ValueKlass::cast(vk);
           field._type = T_VALUETYPE;
         }
diff a/src/hotspot/share/runtime/fieldDescriptor.cpp b/src/hotspot/share/runtime/fieldDescriptor.cpp
--- a/src/hotspot/share/runtime/fieldDescriptor.cpp
+++ b/src/hotspot/share/runtime/fieldDescriptor.cpp
@@ -189,21 +189,21 @@
     case T_BOOLEAN:
       as_int = obj->bool_field(offset());
       st->print(" %s", obj->bool_field(offset()) ? "true" : "false");
       break;
     case T_VALUETYPE:
-      if (is_flattened()) {
-        // Print fields of flattened value type field
+      if (is_inlined()) {
+        // Print fields of inlined fields (recursively)
         ValueKlass* vk = ValueKlass::cast(field_holder()->get_value_field_klass(index()));
         int field_offset = offset() - vk->first_field_offset();
         obj = (oop)(cast_from_oop<address>(obj) + field_offset);
-        st->print_cr("Flattened value type '%s':", vk->name()->as_C_string());
+        st->print_cr("Inline type field inlined '%s':", vk->name()->as_C_string());
         FieldPrinter print_field(st, obj);
         vk->do_nonstatic_fields(&print_field);
         return; // Do not print underlying representation
       }
-      // Non-flattened field, fall through
+      // inline type field not inlined, fall through
     case T_ARRAY:
     case T_OBJECT:
       st->print(" ");
       NOT_LP64(as_int = obj->int_field(offset()));
       if (obj->obj_field(offset()) != NULL) {
diff a/src/hotspot/share/runtime/fieldDescriptor.hpp b/src/hotspot/share/runtime/fieldDescriptor.hpp
--- a/src/hotspot/share/runtime/fieldDescriptor.hpp
+++ b/src/hotspot/share/runtime/fieldDescriptor.hpp
@@ -91,12 +91,12 @@
   bool is_static()                const    { return access_flags().is_static(); }
   bool is_final()                 const    { return access_flags().is_final(); }
   bool is_stable()                const    { return access_flags().is_stable(); }
   bool is_volatile()              const    { return access_flags().is_volatile(); }
   bool is_transient()             const    { return access_flags().is_transient(); }
-  inline bool is_flattened()      const;
-  inline bool is_flattenable()    const;
+  inline bool is_inlined() const;
+  inline bool is_inline_type()    const;
 
   bool is_synthetic()             const    { return access_flags().is_synthetic(); }
 
   bool is_field_access_watched()  const    { return access_flags().is_field_access_watched(); }
   bool is_field_modification_watched() const
diff a/src/hotspot/share/runtime/fieldDescriptor.inline.hpp b/src/hotspot/share/runtime/fieldDescriptor.inline.hpp
--- a/src/hotspot/share/runtime/fieldDescriptor.inline.hpp
+++ b/src/hotspot/share/runtime/fieldDescriptor.inline.hpp
@@ -77,9 +77,9 @@
 
 inline BasicType fieldDescriptor::field_type() const {
   return Signature::basic_type(signature());
 }
 
-inline bool fieldDescriptor::is_flattened()  const  { return field()->is_flattened(); }
-inline bool fieldDescriptor::is_flattenable() const { return field()->is_flattenable(); }
+inline bool fieldDescriptor::is_inlined()  const  { return field()->is_inlined(); }
+inline bool fieldDescriptor::is_inline_type() const { return Signature::basic_type(field()->signature(_cp())) == T_VALUETYPE; }
 
 #endif // SHARE_RUNTIME_FIELDDESCRIPTOR_INLINE_HPP
diff a/src/hotspot/share/runtime/globals.hpp b/src/hotspot/share/runtime/globals.hpp
--- a/src/hotspot/share/runtime/globals.hpp
+++ b/src/hotspot/share/runtime/globals.hpp
@@ -2486,17 +2486,10 @@
   diagnostic(ccstrlist, ForceNonTearable, "",                               \
           "List of inline classes which are forced to be atomic "           \
           "(whitespace and commas separate names, "                         \
           "and leading and trailing stars '*' are wildcards)")              \
                                                                             \
-  product(bool, PrintNewLayout, false,                                      \
-               "Print layout compute by new algorithm")                     \
-                                                                            \
-  product(bool, PrintFlattenableLayouts, false,                             \
-                "Print layout of inline classes and classes with "          \
-                "flattenable fields")                                       \
-                                                                            \
   product(bool, UseNewFieldLayout, true,                                    \
                 "(Deprecated) Use new algorithm to compute field layouts")  \
                                                                             \
   product(bool, UseEmptySlotsInSupers, true,                                \
                 "Allow allocating fields in empty slots of super-classes")  \
diff a/src/hotspot/share/runtime/jfieldIDWorkaround.hpp b/src/hotspot/share/runtime/jfieldIDWorkaround.hpp
--- a/src/hotspot/share/runtime/jfieldIDWorkaround.hpp
+++ b/src/hotspot/share/runtime/jfieldIDWorkaround.hpp
@@ -51,28 +51,28 @@
 
  private:
   enum {
     checked_bits           = 1,
     instance_bits          = 1,
-    flattened_bits         = 1,
-    address_bits           = BitsPerWord - checked_bits - instance_bits - flattened_bits,
+    inlined_bits           = 1,
+    address_bits           = BitsPerWord - checked_bits - instance_bits - inlined_bits,
 
     large_offset_bits      = address_bits,  // unioned with address
     small_offset_bits      = 7,
     klass_bits             = address_bits - small_offset_bits,
 
     checked_shift          = 0,
     instance_shift         = checked_shift  + checked_bits,
-    flattened_shift        = instance_shift + instance_bits,
-    address_shift          = flattened_shift + flattened_bits,
+    inlined_shift          = instance_shift + instance_bits,
+    address_shift          = inlined_shift + inlined_bits,
 
     offset_shift           = address_shift,  // unioned with address
     klass_shift            = offset_shift + small_offset_bits,
 
     checked_mask_in_place  = right_n_bits(checked_bits)  << checked_shift,
     instance_mask_in_place = right_n_bits(instance_bits) << instance_shift,
-    flattened_mask_in_place = right_n_bits(flattened_bits) << flattened_shift,
+    inlined_mask_in_place  = right_n_bits(inlined_bits) << inlined_shift,
 #ifndef _WIN64
     large_offset_mask      = right_n_bits(large_offset_bits),
     small_offset_mask      = right_n_bits(small_offset_bits),
     klass_mask             = right_n_bits(klass_bits)
 #endif
@@ -111,20 +111,20 @@
   static bool is_static_jfieldID(jfieldID id) {
     uintptr_t as_uint = (uintptr_t) id;
     return ((as_uint & instance_mask_in_place) == 0);
   }
 
-  static bool is_flattened_field(jfieldID id) {
+  static bool is_inlined_jfieldID(jfieldID id) {
     uintptr_t as_uint = (uintptr_t) id;
-    return ((as_uint & flattened_mask_in_place) != 0);
+    return ((as_uint & inlined_mask_in_place) != 0);
   }
 
-  static jfieldID to_instance_jfieldID(Klass* k, int offset, bool flattened) {
+  static jfieldID to_instance_jfieldID(Klass* k, int offset, bool inlined) {
     intptr_t as_uint = ((offset & large_offset_mask) << offset_shift) |
                         instance_mask_in_place;
-    if (flattened) {
-      as_uint |= flattened_mask_in_place;
+    if (inlined) {
+      as_uint |= inlined_mask_in_place;
     }
     if (VerifyJNIFields) {
       as_uint |= encode_klass_hash(k, offset);
     }
     jfieldID result = (jfieldID) as_uint;
@@ -163,17 +163,17 @@
     JNIid* result = (JNIid*) id;
     assert(result->is_static_field_id(), "to_JNIid, but not static field id");
     return result;
   }
 
-  static jfieldID to_jfieldID(InstanceKlass* k, int offset, bool is_static, bool is_flattened) {
+  static jfieldID to_jfieldID(InstanceKlass* k, int offset, bool is_static, bool inlined) {
     if (is_static) {
       JNIid *id = k->jni_id_for(offset);
       debug_only(id->set_is_static_field_id());
       return jfieldIDWorkaround::to_static_jfieldID(id);
     } else {
-      return jfieldIDWorkaround::to_instance_jfieldID(k, offset, is_flattened);
+      return jfieldIDWorkaround::to_instance_jfieldID(k, offset, inlined);
     }
   }
 };
 
 #endif // SHARE_RUNTIME_JFIELDIDWORKAROUND_HPP
diff a/src/hotspot/share/runtime/reflection.cpp b/src/hotspot/share/runtime/reflection.cpp
--- a/src/hotspot/share/runtime/reflection.cpp
+++ b/src/hotspot/share/runtime/reflection.cpp
@@ -901,17 +901,12 @@
   java_lang_reflect_Field::set_slot(rh(), fd->index());
   java_lang_reflect_Field::set_name(rh(), name());
   java_lang_reflect_Field::set_type(rh(), type());
   // Note the ACC_ANNOTATION bit, which is a per-class access flag, is never set here.
   int modifiers = fd->access_flags().as_int() & JVM_RECOGNIZED_FIELD_MODIFIERS;
-  if (fd->is_flattenable()) {
-    modifiers |= JVM_ACC_FIELD_FLATTENABLE;
-    // JVM_ACC_FLATTENABLE should not be set in LWorld.  set_is_flattenable should be re-examined.
-    modifiers &= ~JVM_ACC_FLATTENABLE;
-  }
-  if (fd->is_flattened()) {
-    modifiers |= JVM_ACC_FIELD_FLATTENED;
+  if (fd->is_inlined()) {
+    modifiers |= JVM_ACC_FIELD_INLINED;
   }
   java_lang_reflect_Field::set_modifiers(rh(), modifiers);
   java_lang_reflect_Field::set_override(rh(), false);
   if (fd->has_generic_signature()) {
     Symbol*  gs = fd->generic_signature();
diff a/src/hotspot/share/services/diagnosticCommand.hpp b/src/hotspot/share/services/diagnosticCommand.hpp
--- a/src/hotspot/share/services/diagnosticCommand.hpp
+++ b/src/hotspot/share/services/diagnosticCommand.hpp
@@ -411,11 +411,11 @@
   PrintClassLayoutDCmd(outputStream* output, bool heap);
   static const char* name() {
     return "VM.class_print_layout";
   }
   static const char* description() {
-    return "Print the layout of an instance of a class, including flattened fields. "
+    return "Print the layout of an instance of a class, including inlined fields. "
            "The name of each class is followed by the ClassLoaderData* of its ClassLoader, "
            "or \"null\" if loaded by the bootstrap class loader.";
   }
   static const char* impact() {
       return "Medium: Depends on number of loaded classes.";
diff a/src/hotspot/share/utilities/accessFlags.hpp b/src/hotspot/share/utilities/accessFlags.hpp
--- a/src/hotspot/share/utilities/accessFlags.hpp
+++ b/src/hotspot/share/utilities/accessFlags.hpp
@@ -84,21 +84,18 @@
   JVM_ACC_FIELD_MODIFICATION_WATCHED      = 0x00008000, // field modification is watched by JVMTI
   JVM_ACC_FIELD_INTERNAL                  = 0x00000400, // internal field, same as JVM_ACC_ABSTRACT
   JVM_ACC_FIELD_STABLE                    = 0x00000020, // @Stable field, same as JVM_ACC_SYNCHRONIZED and JVM_ACC_SUPER
   JVM_ACC_FIELD_INITIALIZED_FINAL_UPDATE  = 0x00000200, // (static) final field updated outside (class) initializer, same as JVM_ACC_NATIVE
   JVM_ACC_FIELD_HAS_GENERIC_SIGNATURE     = 0x00000800, // field has generic signature
-  JVM_ACC_FIELD_FLATTENABLE               = 0x00004000, // flattenable field
-  JVM_ACC_FIELD_FLATTENED                 = 0x00008000, // flattened field
+  JVM_ACC_FIELD_INLINED                   = 0x00008000, // field is inlined
 
   JVM_ACC_FIELD_INTERNAL_FLAGS       = JVM_ACC_FIELD_ACCESS_WATCHED |
                                        JVM_ACC_FIELD_MODIFICATION_WATCHED |
                                        JVM_ACC_FIELD_INTERNAL |
                                        JVM_ACC_FIELD_STABLE |
                                        JVM_ACC_FIELD_HAS_GENERIC_SIGNATURE |
-                                       JVM_ACC_FLATTENABLE |
-                                       JVM_ACC_FIELD_FLATTENABLE |
-                                       JVM_ACC_FIELD_FLATTENED,
+                                       JVM_ACC_FIELD_INLINED,
 
                                                     // flags accepted by set_field_flags()
   JVM_ACC_FIELD_FLAGS                = JVM_RECOGNIZED_FIELD_MODIFIERS | JVM_ACC_FIELD_INTERNAL_FLAGS
 
 };
@@ -126,11 +123,10 @@
   bool is_native      () const         { return (_flags & JVM_ACC_NATIVE      ) != 0; }
   bool is_interface   () const         { return (_flags & JVM_ACC_INTERFACE   ) != 0; }
   bool is_abstract    () const         { return (_flags & JVM_ACC_ABSTRACT    ) != 0; }
   bool is_strict      () const         { return (_flags & JVM_ACC_STRICT      ) != 0; }
   bool is_inline_type () const         { return (_flags & JVM_ACC_VALUE       ) != 0; }
-  bool is_flattenable () const         { return (_flags & JVM_ACC_FLATTENABLE ) != 0; }
 
   // Attribute flags
   bool is_synthetic   () const         { return (_flags & JVM_ACC_SYNTHETIC   ) != 0; }
 
   // Method* flags
@@ -217,11 +213,10 @@
   void set_has_jsrs()                  { atomic_set_bits(JVM_ACC_HAS_JSRS);                }
   void set_is_old()                    { atomic_set_bits(JVM_ACC_IS_OLD);                  }
   void set_is_obsolete()               { atomic_set_bits(JVM_ACC_IS_OBSOLETE);             }
   void set_is_deleted()                { atomic_set_bits(JVM_ACC_IS_DELETED);              }
   void set_is_prefixed_native()        { atomic_set_bits(JVM_ACC_IS_PREFIXED_NATIVE);      }
-  void set_is_flattenable()            { atomic_set_bits(JVM_ACC_FLATTENABLE);             }
 
   void clear_not_c1_compilable()       { atomic_clear_bits(JVM_ACC_NOT_C1_COMPILABLE);       }
   void clear_not_c2_compilable()       { atomic_clear_bits(JVM_ACC_NOT_C2_COMPILABLE);       }
   void clear_not_c2_osr_compilable()   { atomic_clear_bits(JVM_ACC_NOT_C2_OSR_COMPILABLE);   }
   // Klass* flags
diff a/src/java.base/share/classes/jdk/internal/vm/jni/SubElementSelector.java b/src/java.base/share/classes/jdk/internal/vm/jni/SubElementSelector.java
--- a/src/java.base/share/classes/jdk/internal/vm/jni/SubElementSelector.java
+++ b/src/java.base/share/classes/jdk/internal/vm/jni/SubElementSelector.java
@@ -27,8 +27,8 @@
 
 public /*inline*/ class SubElementSelector {
     public final Class<?> arrayElementType = null;
     public final Class<?> subElementType = null;
     public final int offset = -1;
-    public final boolean isFlattened = false;
-    public final boolean isFlattenable = false;
+    public final boolean isInlined = false;
+    public final boolean isInlineType = false;
 }
diff a/src/java.base/share/native/include/classfile_constants.h.template b/src/java.base/share/native/include/classfile_constants.h.template
--- a/src/java.base/share/native/include/classfile_constants.h.template
+++ b/src/java.base/share/native/include/classfile_constants.h.template
@@ -47,11 +47,10 @@
     JVM_ACC_VOLATILE      = 0x0040,
     JVM_ACC_BRIDGE        = 0x0040,
     JVM_ACC_TRANSIENT     = 0x0080,
     JVM_ACC_VARARGS       = 0x0080,
     JVM_ACC_VALUE         = 0x0100,
-    JVM_ACC_FLATTENABLE   = 0x0100,
     JVM_ACC_NATIVE        = 0x0100,
     JVM_ACC_INTERFACE     = 0x0200,
     JVM_ACC_ABSTRACT      = 0x0400,
     JVM_ACC_STRICT        = 0x0800,
     JVM_ACC_SYNTHETIC     = 0x1000,
@@ -111,11 +110,11 @@
     JVM_CONSTANT_MethodType             = 16,  // JSR 292
     JVM_CONSTANT_Dynamic                = 17,
     JVM_CONSTANT_InvokeDynamic          = 18,
     JVM_CONSTANT_Module                 = 19,
     JVM_CONSTANT_Package                = 20,
-    JVM_CONSTANT_ExternalMax            = 20 
+    JVM_CONSTANT_ExternalMax            = 20
 };
 
 /* JVM_CONSTANT_MethodHandle subtypes */
 enum {
     JVM_REF_getField                = 1,
diff a/test/hotspot/jtreg/runtime/valhalla/valuetypes/TestJNIArrays.java b/test/hotspot/jtreg/runtime/valhalla/valuetypes/TestJNIArrays.java
--- a/test/hotspot/jtreg/runtime/valhalla/valuetypes/TestJNIArrays.java
+++ b/test/hotspot/jtreg/runtime/valhalla/valuetypes/TestJNIArrays.java
@@ -38,12 +38,12 @@
  * @modules java.base/jdk.internal.misc java.base/jdk.internal.vm.jni
  * @library /testlibrary /test/lib
  * @requires (os.simpleArch == "x64")
  * @requires (os.family == "linux" | os.family == "mac")
  * @compile -XDallowWithFieldOperator TestJNIArrays.java
- * @run main/othervm/native/timeout=3000 -XX:InlineArrayElemMaxFlatSize=128 -XX:+PrintFlattenableLayouts -XX:+UseCompressedOops TestJNIArrays
- * @run main/othervm/native/timeout=3000 -XX:InlineArrayElemMaxFlatSize=128 -XX:+PrintFlattenableLayouts -XX:-UseCompressedOops TestJNIArrays
+ * @run main/othervm/native/timeout=3000 -XX:InlineArrayElemMaxFlatSize=128 -XX:+UseCompressedOops TestJNIArrays
+ * @run main/othervm/native/timeout=3000 -XX:InlineArrayElemMaxFlatSize=128 -XX:-UseCompressedOops TestJNIArrays
  */
 public class TestJNIArrays {
 
     static final Unsafe U = Unsafe.getUnsafe();
 
