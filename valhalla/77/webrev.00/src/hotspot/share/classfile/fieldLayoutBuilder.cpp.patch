diff a/src/hotspot/share/classfile/fieldLayoutBuilder.cpp b/src/hotspot/share/classfile/fieldLayoutBuilder.cpp
--- a/src/hotspot/share/classfile/fieldLayoutBuilder.cpp
+++ b/src/hotspot/share/classfile/fieldLayoutBuilder.cpp
@@ -535,10 +535,11 @@
   _nonstatic_oopmap_count(0),
   _alignment(-1),
   _first_field_offset(-1),
   _exact_size_in_bytes(-1),
   _has_nonstatic_fields(false),
+  _has_inline_fields(false),
   _is_contended(is_contended),
   _is_inline_type(is_inline_type),
   _has_flattening_information(is_inline_type),
   _has_nonatomic_values(false),
   _atomic_field_count(0)
@@ -612,22 +613,23 @@
     case T_ARRAY:
       if (group != _static_fields) _nonstatic_oopmap_count++;
       group->add_oop_field(fs);
       break;
     case T_VALUETYPE:
+//      fs.set_inline(true);
+      _has_inline_fields = true;
       if (group == _static_fields) {
         // static fields are never flattened
         group->add_oop_field(fs);
       } else {
         _has_flattening_information = true;
         // Flattening decision to be taken here
-        // This code assumes all verification have been performed before
-        // (field is a flattenable field, field's type has been loaded
-        // and it is an inline klass
+        // This code assumes all verification already have been performed
+        // (field's type has been loaded and it is an inline klass)
         Thread* THREAD = Thread::current();
         Klass* klass =
-            SystemDictionary::resolve_flattenable_field_or_fail(&fs,
+            SystemDictionary::resolve_inline_field_or_fail(&fs,
                                                                 Handle(THREAD, _class_loader_data->class_loader()),
                                                                 _protection_domain, true, THREAD);
         assert(klass != NULL, "Sanity check");
         ValueKlass* vk = ValueKlass::cast(klass);
         bool too_big_to_flatten = (InlineFieldMaxFlatSize >= 0 &&
@@ -713,21 +715,22 @@
         field_alignment = type2aelembytes(type); // alignment == size for oops
       }
       group->add_oop_field(fs);
       break;
     case T_VALUETYPE: {
+//      fs.set_inline(true);
+      _has_inline_fields = true;
       if (group == _static_fields) {
         // static fields are never flattened
         group->add_oop_field(fs);
       } else {
         // Flattening decision to be taken here
-        // This code assumes all verifications have been performed before
-        // (field is a flattenable field, field's type has been loaded
-        // and it is an inline klass
+        // This code assumes all verifications have already been performed
+        // (field's type has been loaded and it is an inline klass)
         Thread* THREAD = Thread::current();
         Klass* klass =
-            SystemDictionary::resolve_flattenable_field_or_fail(&fs,
+            SystemDictionary::resolve_inline_field_or_fail(&fs,
                 Handle(THREAD, _class_loader_data->class_loader()),
                 _protection_domain, true, CHECK);
         assert(klass != NULL, "Sanity check");
         ValueKlass* vk = ValueKlass::cast(klass);
         bool too_big_to_flatten = (InlineFieldMaxFlatSize >= 0 &&
@@ -943,10 +946,11 @@
   _info->oop_map_blocks = nonstatic_oop_maps;
   _info->_instance_size = align_object_size(instance_end / wordSize);
   _info->_static_field_size = static_fields_size;
   _info->_nonstatic_field_size = (nonstatic_field_end - instanceOopDesc::base_offset_in_bytes()) / heapOopSize;
   _info->_has_nonstatic_fields = _has_nonstatic_fields;
+  _info->_has_inline_fields = _has_inline_fields;
 
   // An inline type is naturally atomic if it has just one field, and
   // that field is simple enough.
   _info->_is_naturally_atomic = (_is_inline_type &&
                                  (_atomic_field_count <= 1) &&
