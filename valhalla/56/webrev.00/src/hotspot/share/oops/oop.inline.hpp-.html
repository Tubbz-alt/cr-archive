<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Old src/hotspot/share/oops/oop.inline.hpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
  <body>
    <pre>
  1 /*
  2  * Copyright (c) 1997, 2019, Oracle and/or its affiliates. All rights reserved.
  3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  4  *
  5  * This code is free software; you can redistribute it and/or modify it
  6  * under the terms of the GNU General Public License version 2 only, as
  7  * published by the Free Software Foundation.
  8  *
  9  * This code is distributed in the hope that it will be useful, but WITHOUT
 10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
 11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
 12  * version 2 for more details (a copy is included in the LICENSE file that
 13  * accompanied this code).
 14  *
 15  * You should have received a copy of the GNU General Public License version
 16  * 2 along with this work; if not, write to the Free Software Foundation,
 17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 18  *
 19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
 20  * or visit www.oracle.com if you need additional information or have any
 21  * questions.
 22  *
 23  */
 24 
 25 #ifndef SHARE_OOPS_OOP_INLINE_HPP
 26 #define SHARE_OOPS_OOP_INLINE_HPP
 27 
 28 #include &quot;gc/shared/collectedHeap.hpp&quot;
 29 #include &quot;memory/universe.hpp&quot;
 30 #include &quot;oops/access.inline.hpp&quot;
 31 #include &quot;oops/arrayKlass.hpp&quot;
 32 #include &quot;oops/arrayOop.hpp&quot;
 33 #include &quot;oops/compressedOops.inline.hpp&quot;
 34 #include &quot;oops/klass.inline.hpp&quot;
 35 #include &quot;oops/markWord.inline.hpp&quot;
 36 #include &quot;oops/oop.hpp&quot;
 37 #include &quot;runtime/atomic.hpp&quot;
 38 #include &quot;runtime/os.hpp&quot;
 39 #include &quot;utilities/align.hpp&quot;
 40 #include &quot;utilities/macros.hpp&quot;
 41 
 42 // Implementation of all inlined member functions defined in oop.hpp
 43 // We need a separate file to avoid circular references
 44 
 45 markWord oopDesc::mark() const {
 46   uintptr_t v = HeapAccess&lt;MO_VOLATILE&gt;::load_at(as_oop(), mark_offset_in_bytes());
 47   return markWord(v);
 48 }
 49 
 50 markWord oopDesc::mark_raw() const {
 51   return Atomic::load(&amp;_mark);
 52 }
 53 
 54 markWord* oopDesc::mark_addr_raw() const {
 55   return (markWord*) &amp;_mark;
 56 }
 57 
 58 void oopDesc::set_mark(markWord m) {
 59   HeapAccess&lt;MO_VOLATILE&gt;::store_at(as_oop(), mark_offset_in_bytes(), m.value());
 60 }
 61 
 62 void oopDesc::set_mark_raw(markWord m) {
 63   Atomic::store(&amp;_mark, m);
 64 }
 65 
 66 void oopDesc::set_mark_raw(HeapWord* mem, markWord m) {
 67   *(markWord*)(((char*)mem) + mark_offset_in_bytes()) = m;
 68 }
 69 
 70 void oopDesc::release_set_mark(markWord m) {
 71   HeapAccess&lt;MO_RELEASE&gt;::store_at(as_oop(), mark_offset_in_bytes(), m.value());
 72 }
 73 
 74 markWord oopDesc::cas_set_mark(markWord new_mark, markWord old_mark) {
 75   uintptr_t v = HeapAccess&lt;&gt;::atomic_cmpxchg_at(as_oop(), mark_offset_in_bytes(), old_mark.value(), new_mark.value());
 76   return markWord(v);
 77 }
 78 
 79 markWord oopDesc::cas_set_mark_raw(markWord new_mark, markWord old_mark, atomic_memory_order order) {
 80   return Atomic::cmpxchg(&amp;_mark, old_mark, new_mark, order);
 81 }
 82 
 83 void oopDesc::init_mark() {
 84   set_mark(markWord::prototype_for_klass(klass()));
 85 }
 86 
 87 void oopDesc::init_mark_raw() {
 88   set_mark_raw(markWord::prototype_for_klass(klass()));
 89 }
 90 
 91 narrowKlass oopDesc::compressed_klass_mask() { return ((narrowKlass) 1 &lt;&lt; narrow_storage_props_shift) - 1; }
 92 uintptr_t   oopDesc::klass_mask()   { return ((uintptr_t) 1 &lt;&lt; wide_storage_props_shift) - 1; }
 93 
 94 narrowKlass oopDesc::compressed_klass_masked(narrowKlass raw) { return raw &amp; compressed_klass_mask(); }
 95 Klass*      oopDesc::klass_masked(uintptr_t raw)     { return reinterpret_cast&lt;Klass*&gt;(raw &amp; klass_mask()); }
 96 
 97 
 98 Klass* oopDesc::klass() const {
 99   if (UseCompressedClassPointers) {
100     return CompressedKlassPointers::decode_not_null(compressed_klass_masked(_metadata._compressed_klass));
101   } else {
102     return klass_masked(_metadata._wide_storage_props);
103   }
104 }
105 
106 Klass* oopDesc::klass_or_null() const volatile {
107   if (UseCompressedClassPointers) {
108     return CompressedKlassPointers::decode(compressed_klass_masked(_metadata._compressed_klass));
109   } else {
110     return klass_masked(_metadata._wide_storage_props);
111   }
112 }
113 
114 Klass* oopDesc::klass_or_null_acquire() const volatile {
115   if (UseCompressedClassPointers) {
116     // Workaround for non-const load_acquire parameter.
117     const volatile narrowKlass* addr = &amp;_metadata._compressed_klass;
118     volatile narrowKlass* xaddr = const_cast&lt;volatile narrowKlass*&gt;(addr);
119     return CompressedKlassPointers::decode(compressed_klass_masked(Atomic::load_acquire(xaddr)));
120   } else {
121     return klass_masked(Atomic::load_acquire(&amp;_metadata._wide_storage_props));
122   }
123 }
124 
125 Klass** oopDesc::klass_addr(HeapWord* mem) {
126   // Only used internally and with CMS and will not work with
127   // UseCompressedOops
128   assert(!UseCompressedClassPointers, &quot;only supported with uncompressed klass pointers&quot;);
129   ByteSize offset = byte_offset_of(oopDesc, _metadata._klass);
130   return (Klass**) (((char*)mem) + in_bytes(offset));
131 }
132 
133 uintptr_t* oopDesc::wide_metadata_addr(HeapWord* mem) {
134   assert(!UseCompressedClassPointers, &quot;only supported with uncompressed klass pointers&quot;);
135   ByteSize offset = byte_offset_of(oopDesc, _metadata._wide_storage_props);
136   return (uintptr_t*) (((char*)mem) + in_bytes(offset));
137 }
138 
139 narrowKlass* oopDesc::compressed_klass_addr(HeapWord* mem) {
140   assert(UseCompressedClassPointers, &quot;only called by compressed klass pointers&quot;);
141   ByteSize offset = byte_offset_of(oopDesc, _metadata._compressed_klass);
142   return (narrowKlass*) (((char*)mem) + in_bytes(offset));
143 }
144 
145 Klass** oopDesc::klass_addr() {
146   return klass_addr((HeapWord*)this);
147 }
148 
149 narrowKlass* oopDesc::compressed_klass_addr() {
150   return compressed_klass_addr((HeapWord*)this);
151 }
152 
153 #define CHECK_SET_KLASS(k)                                                \
154   do {                                                                    \
155     assert(Universe::is_bootstrapping() || k != NULL, &quot;NULL Klass&quot;);      \
156     assert(Universe::is_bootstrapping() || k-&gt;is_klass(), &quot;not a Klass&quot;); \
157     assert(((reinterpret_cast&lt;uintptr_t&gt;(k) &amp; (~ oopDesc::klass_mask())) == 0), \
158       &quot;No room for storage props &quot;); \
159   } while (0)
160 
161 void oopDesc::set_klass(Klass* k) {
162   CHECK_SET_KLASS(k);
163   if (UseCompressedClassPointers) {
164     *compressed_klass_addr() = CompressedKlassPointers::encode_not_null(k);
165   } else {
166     *klass_addr() = k;
167   }
168 }
169 
170 void oopDesc::release_set_klass(HeapWord* mem, Klass* klass) {
171   CHECK_SET_KLASS(klass);
172   if (UseCompressedClassPointers) {
173     Atomic::release_store(compressed_klass_addr(mem),
174                           CompressedKlassPointers::encode_not_null(klass));
175   } else {
176     Atomic::release_store(klass_addr(mem), klass);
177   }
178   assert(((oopDesc*)mem)-&gt;klass() == klass, &quot;failed oopDesc::klass() encode/decode&quot;);
179 }
180 
181 void oopDesc::set_metadata(ArrayStorageProperties storage_props, Klass* klass) {
182   CHECK_SET_KLASS(klass);
183   if (UseCompressedClassPointers) {
184     *compressed_klass_addr() = (CompressedKlassPointers::encode_not_null(klass) | storage_props.encode&lt;narrowKlass&gt;(narrow_storage_props_shift));
185   } else {
186     *wide_metadata_addr((HeapWord*)this) = (reinterpret_cast&lt;uintptr_t&gt;(klass) | storage_props.encode&lt;uintptr_t&gt;(wide_storage_props_shift));
187   }
188 }
189 
190 void oopDesc::release_set_metadata(HeapWord* mem, ArrayStorageProperties storage_props, Klass* klass) {
191   CHECK_SET_KLASS(klass);
192   if (UseCompressedClassPointers) {
193     Atomic::release_store(oopDesc::compressed_klass_addr(mem),
194                                CompressedKlassPointers::encode_not_null(klass) | storage_props.encode&lt;narrowKlass&gt;(narrow_storage_props_shift));
195   } else {
196     Atomic::release_store(oopDesc::wide_metadata_addr(mem),
197                                (reinterpret_cast&lt;uintptr_t&gt;(klass) | storage_props.encode&lt;uintptr_t&gt;(wide_storage_props_shift)));
198   }
199 }
200 #undef CHECK_SET_KLASS
201 
202 
203 ArrayStorageProperties oopDesc::array_storage_properties() const {
204   if (UseCompressedClassPointers) {
205     return ArrayStorageProperties(_metadata._narrow_storage_props &gt;&gt; narrow_storage_props_shift);
206   } else {
207     return ArrayStorageProperties(_metadata._wide_storage_props &gt;&gt; wide_storage_props_shift);
208   }
209 }
210 
211 
212 int oopDesc::klass_gap() const {
213   return *(int*)(((intptr_t)this) + klass_gap_offset_in_bytes());
214 }
215 
216 void oopDesc::set_klass_gap(HeapWord* mem, int v) {
217   if (UseCompressedClassPointers) {
218     *(int*)(((char*)mem) + klass_gap_offset_in_bytes()) = v;
219   }
220 }
221 
222 void oopDesc::set_klass_gap(int v) {
223   set_klass_gap((HeapWord*)this, v);
224 }
225 
226 bool oopDesc::is_a(Klass* k) const {
227   return klass()-&gt;is_subtype_of(k);
228 }
229 
230 int oopDesc::size()  {
231   return size_given_klass(klass());
232 }
233 
234 int oopDesc::size_given_klass(Klass* klass)  {
235   int lh = klass-&gt;layout_helper();
236   int s;
237 
238   // lh is now a value computed at class initialization that may hint
239   // at the size.  For instances, this is positive and equal to the
240   // size.  For arrays, this is negative and provides log2 of the
241   // array element size.  For other oops, it is zero and thus requires
242   // a virtual call.
243   //
244   // We go to all this trouble because the size computation is at the
245   // heart of phase 2 of mark-compaction, and called for every object,
246   // alive or dead.  So the speed here is equal in importance to the
247   // speed of allocation.
248 
249   if (lh &gt; Klass::_lh_neutral_value) {
250     if (!Klass::layout_helper_needs_slow_path(lh)) {
251       s = lh &gt;&gt; LogHeapWordSize;  // deliver size scaled by wordSize
252     } else {
253       s = klass-&gt;oop_size(this);
254     }
255   } else if (lh &lt;= Klass::_lh_neutral_value) {
256     // The most common case is instances; fall through if so.
257     if (lh &lt; Klass::_lh_neutral_value) {
258       // Second most common case is arrays.  We have to fetch the
259       // length of the array, shift (multiply) it appropriately,
260       // up to wordSize, add the header, and align to object size.
261       size_t size_in_bytes;
262       size_t array_length = (size_t) ((arrayOop)this)-&gt;length();
263       size_in_bytes = array_length &lt;&lt; Klass::layout_helper_log2_element_size(lh);
264       size_in_bytes += Klass::layout_helper_header_size(lh);
265 
266       // This code could be simplified, but by keeping array_header_in_bytes
267       // in units of bytes and doing it this way we can round up just once,
268       // skipping the intermediate round to HeapWordSize.
269       s = (int)(align_up(size_in_bytes, MinObjAlignmentInBytes) / HeapWordSize);
270 
271       // UseParallelGC and UseG1GC can change the length field
272       // of an &quot;old copy&quot; of an object array in the young gen so it indicates
273       // the grey portion of an already copied array. This will cause the first
274       // disjunct below to fail if the two comparands are computed across such
275       // a concurrent change.
276       assert((s == klass-&gt;oop_size(this)) ||
277              (Universe::heap()-&gt;is_gc_active() &amp;&amp; is_objArray() &amp;&amp; is_forwarded() &amp;&amp; (UseParallelGC || UseG1GC)),
278              &quot;wrong array object size&quot;);
279     } else {
280       // Must be zero, so bite the bullet and take the virtual call.
281       s = klass-&gt;oop_size(this);
282     }
283   }
284 
285   assert(s &gt; 0, &quot;Oop size must be greater than zero, not %d&quot;, s);
286   assert(is_object_aligned(s), &quot;Oop size is not properly aligned: %d&quot;, s);
287   return s;
288 }
289 
290 bool oopDesc::is_instance()  const { return klass()-&gt;is_instance_klass();  }
291 bool oopDesc::is_array()     const { return klass()-&gt;is_array_klass();     }
292 bool oopDesc::is_objArray()  const { return klass()-&gt;is_objArray_klass();  }
293 bool oopDesc::is_typeArray() const { return klass()-&gt;is_typeArray_klass(); }
294 bool oopDesc::is_value()     const { return klass()-&gt;is_value(); }
295 bool oopDesc::is_valueArray()  const { return klass()-&gt;is_valueArray_klass(); }
296 
297 void*    oopDesc::field_addr_raw(int offset)     const { return reinterpret_cast&lt;void*&gt;(cast_from_oop&lt;intptr_t&gt;(as_oop()) + offset); }
298 void*    oopDesc::field_addr(int offset)         const { return Access&lt;&gt;::resolve(as_oop())-&gt;field_addr_raw(offset); }
299 
300 template &lt;class T&gt;
301 T*       oopDesc::obj_field_addr_raw(int offset) const { return (T*) field_addr_raw(offset); }
302 
303 template &lt;typename T&gt;
304 size_t   oopDesc::field_offset(T* p) const { return pointer_delta((void*)p, (void*)this, 1); }
305 
306 template &lt;DecoratorSet decorators&gt;
307 inline oop  oopDesc::obj_field_access(int offset) const             { return HeapAccess&lt;decorators&gt;::oop_load_at(as_oop(), offset); }
308 inline oop  oopDesc::obj_field(int offset) const                    { return HeapAccess&lt;&gt;::oop_load_at(as_oop(), offset);  }
309 
310 inline void oopDesc::obj_field_put(int offset, oop value)           { HeapAccess&lt;&gt;::oop_store_at(as_oop(), offset, value); }
311 
312 inline jbyte oopDesc::byte_field(int offset) const                  { return HeapAccess&lt;&gt;::load_at(as_oop(), offset);  }
313 inline void  oopDesc::byte_field_put(int offset, jbyte value)       { HeapAccess&lt;&gt;::store_at(as_oop(), offset, value); }
314 
315 inline jchar oopDesc::char_field(int offset) const                  { return HeapAccess&lt;&gt;::load_at(as_oop(), offset);  }
316 inline void  oopDesc::char_field_put(int offset, jchar value)       { HeapAccess&lt;&gt;::store_at(as_oop(), offset, value); }
317 
318 inline jboolean oopDesc::bool_field(int offset) const               { return HeapAccess&lt;&gt;::load_at(as_oop(), offset); }
319 inline void     oopDesc::bool_field_put(int offset, jboolean value) { HeapAccess&lt;&gt;::store_at(as_oop(), offset, jboolean(value &amp; 1)); }
320 inline jboolean oopDesc::bool_field_volatile(int offset) const      { return HeapAccess&lt;MO_SEQ_CST&gt;::load_at(as_oop(), offset); }
321 inline void     oopDesc::bool_field_put_volatile(int offset, jboolean value) { HeapAccess&lt;MO_SEQ_CST&gt;::store_at(as_oop(), offset, jboolean(value &amp; 1)); }
322 inline jshort oopDesc::short_field(int offset) const                { return HeapAccess&lt;&gt;::load_at(as_oop(), offset);  }
323 inline void   oopDesc::short_field_put(int offset, jshort value)    { HeapAccess&lt;&gt;::store_at(as_oop(), offset, value); }
324 
325 inline jint oopDesc::int_field(int offset) const                    { return HeapAccess&lt;&gt;::load_at(as_oop(), offset);  }
326 inline jint oopDesc::int_field_raw(int offset) const                { return RawAccess&lt;&gt;::load_at(as_oop(), offset);   }
327 inline void oopDesc::int_field_put(int offset, jint value)          { HeapAccess&lt;&gt;::store_at(as_oop(), offset, value); }
328 
329 inline jlong oopDesc::long_field(int offset) const                  { return HeapAccess&lt;&gt;::load_at(as_oop(), offset);  }
330 inline void  oopDesc::long_field_put(int offset, jlong value)       { HeapAccess&lt;&gt;::store_at(as_oop(), offset, value); }
331 
332 inline jfloat oopDesc::float_field(int offset) const                { return HeapAccess&lt;&gt;::load_at(as_oop(), offset);  }
333 inline void   oopDesc::float_field_put(int offset, jfloat value)    { HeapAccess&lt;&gt;::store_at(as_oop(), offset, value); }
334 
335 inline jdouble oopDesc::double_field(int offset) const              { return HeapAccess&lt;&gt;::load_at(as_oop(), offset);  }
336 inline void    oopDesc::double_field_put(int offset, jdouble value) { HeapAccess&lt;&gt;::store_at(as_oop(), offset, value); }
337 
338 bool oopDesc::is_locked() const {
339   return mark().is_locked();
340 }
341 
342 bool oopDesc::is_unlocked() const {
343   return mark().is_unlocked();
344 }
345 
346 bool oopDesc::has_bias_pattern() const {
347   return mark().has_bias_pattern();
348 }
349 
350 
351 bool oopDesc::has_bias_pattern_raw() const {
352   return mark_raw().has_bias_pattern();
353 }
354 
355 // Used only for markSweep, scavenging
356 bool oopDesc::is_gc_marked() const {
357   return mark_raw().is_marked();
358 }
359 
360 // Used by scavengers
361 bool oopDesc::is_forwarded() const {
362   // The extra heap check is needed since the obj might be locked, in which case the
363   // mark would point to a stack location and have the sentinel bit cleared
364   return mark_raw().is_marked();
365 }
366 
367 // Used by scavengers
368 void oopDesc::forward_to(oop p) {
369   verify_forwardee(p);
370   markWord m = markWord::encode_pointer_as_mark(p);
371   assert(m.decode_pointer() == p, &quot;encoding must be reversable&quot;);
372   set_mark_raw(m);
373 }
374 
375 // Used by parallel scavengers
376 bool oopDesc::cas_forward_to(oop p, markWord compare, atomic_memory_order order) {
377   verify_forwardee(p);
378   markWord m = markWord::encode_pointer_as_mark(p);
379   assert(m.decode_pointer() == p, &quot;encoding must be reversable&quot;);
380   return cas_set_mark_raw(m, compare, order) == compare;
381 }
382 
383 oop oopDesc::forward_to_atomic(oop p, markWord compare, atomic_memory_order order) {
384   verify_forwardee(p);
385   markWord m = markWord::encode_pointer_as_mark(p);
386   assert(m.decode_pointer() == p, &quot;encoding must be reversable&quot;);
387   markWord old_mark = cas_set_mark_raw(m, compare, order);
388   if (old_mark == compare) {
389     return NULL;
390   } else {
391     return (oop)old_mark.decode_pointer();
392   }
393 }
394 
395 // Note that the forwardee is not the same thing as the displaced_mark.
396 // The forwardee is used when copying during scavenge and mark-sweep.
397 // It does need to clear the low two locking- and GC-related bits.
398 oop oopDesc::forwardee() const {
399   return (oop) mark_raw().decode_pointer();
400 }
401 
402 // Note that the forwardee is not the same thing as the displaced_mark.
403 // The forwardee is used when copying during scavenge and mark-sweep.
404 // It does need to clear the low two locking- and GC-related bits.
405 oop oopDesc::forwardee_acquire() const {
406   return (oop) Atomic::load_acquire(&amp;_mark).decode_pointer();
407 }
408 
409 // The following method needs to be MT safe.
410 uint oopDesc::age() const {
411   assert(!is_forwarded(), &quot;Attempt to read age from forwarded mark&quot;);
412   if (has_displaced_mark_raw()) {
413     return displaced_mark_raw().age();
414   } else {
415     return mark_raw().age();
416   }
417 }
418 
419 void oopDesc::incr_age() {
420   assert(!is_forwarded(), &quot;Attempt to increment age of forwarded mark&quot;);
421   if (has_displaced_mark_raw()) {
422     set_displaced_mark_raw(displaced_mark_raw().incr_age());
423   } else {
424     set_mark_raw(mark_raw().incr_age());
425   }
426 }
427 
428 template &lt;typename OopClosureType&gt;
429 void oopDesc::oop_iterate(OopClosureType* cl) {
430   OopIteratorClosureDispatch::oop_oop_iterate(cl, this, klass());
431 }
432 
433 template &lt;typename OopClosureType&gt;
434 void oopDesc::oop_iterate(OopClosureType* cl, MemRegion mr) {
435   OopIteratorClosureDispatch::oop_oop_iterate(cl, this, klass(), mr);
436 }
437 
438 template &lt;typename OopClosureType&gt;
439 int oopDesc::oop_iterate_size(OopClosureType* cl) {
440   Klass* k = klass();
441   int size = size_given_klass(k);
442   OopIteratorClosureDispatch::oop_oop_iterate(cl, this, k);
443   return size;
444 }
445 
446 template &lt;typename OopClosureType&gt;
447 int oopDesc::oop_iterate_size(OopClosureType* cl, MemRegion mr) {
448   Klass* k = klass();
449   int size = size_given_klass(k);
450   OopIteratorClosureDispatch::oop_oop_iterate(cl, this, k, mr);
451   return size;
452 }
453 
454 template &lt;typename OopClosureType&gt;
455 void oopDesc::oop_iterate_backwards(OopClosureType* cl) {
456   OopIteratorClosureDispatch::oop_oop_iterate_backwards(cl, this, klass());
457 }
458 
459 bool oopDesc::is_instanceof_or_null(oop obj, Klass* klass) {
460   return obj == NULL || obj-&gt;klass()-&gt;is_subtype_of(klass);
461 }
462 
463 intptr_t oopDesc::identity_hash() {
464   // Fast case; if the object is unlocked and the hash value is set, no locking is needed
465   // Note: The mark must be read into local variable to avoid concurrent updates.
466   markWord mrk = mark();
467   if (mrk.is_unlocked() &amp;&amp; !mrk.has_no_hash()) {
468     return mrk.hash();
469   } else if (mrk.is_marked()) {
470     return mrk.hash();
471   } else {
472     return slow_identity_hash();
473   }
474 }
475 
476 bool oopDesc::has_displaced_mark_raw() const {
477   return mark_raw().has_displaced_mark_helper();
478 }
479 
480 markWord oopDesc::displaced_mark_raw() const {
481   return mark_raw().displaced_mark_helper();
482 }
483 
484 void oopDesc::set_displaced_mark_raw(markWord m) {
485   mark_raw().set_displaced_mark_helper(m);
486 }
487 
488 // Supports deferred calling of obj-&gt;klass().
489 class DeferredObjectToKlass {
490   const oopDesc* _obj;
491 
492 public:
493   DeferredObjectToKlass(const oopDesc* obj) : _obj(obj) {}
494 
495   // Implicitly convertible to const Klass*.
496   operator const Klass*() const {
497     return _obj-&gt;klass();
498   }
499 };
500 
501 bool oopDesc::mark_must_be_preserved() const {
502   return mark_must_be_preserved(mark_raw());
503 }
504 
505 bool oopDesc::mark_must_be_preserved(markWord m) const {
506   // There&#39;s a circular dependency between oop.inline.hpp and
507   // markWord.inline.hpp because markWord::must_be_preserved wants to call
508   // oopDesc::klass(). This could be solved by calling klass() here. However,
509   // not all paths inside must_be_preserved calls klass(). Defer the call until
510   // the klass is actually needed.
511   return m.must_be_preserved(DeferredObjectToKlass(this));
512 }
513 
514 bool oopDesc::mark_must_be_preserved_for_promotion_failure(markWord m) const {
515   return m.must_be_preserved_for_promotion_failure(DeferredObjectToKlass(this));
516 }
517 
518 #endif // SHARE_OOPS_OOP_INLINE_HPP
    </pre>
  </body>
</html>