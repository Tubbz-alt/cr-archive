<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/share/opto/parse2.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="memnode.hpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="parse3.cpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/share/opto/parse2.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
  81   // Handle value type arrays
  82   const TypeOopPtr* elemptr = elemtype-&gt;make_oopptr();
  83   const TypeAryPtr* ary_t = _gvn.type(ary)-&gt;is_aryptr();
  84   if (elemtype-&gt;isa_valuetype() != NULL) {
  85     C-&gt;set_flattened_accesses();
  86     // Load from flattened value type array
  87     Node* vt = ValueTypeNode::make_from_flattened(this, elemtype-&gt;value_klass(), ary, adr);
  88     push(vt);
  89     return;
  90   } else if (elemptr != NULL &amp;&amp; elemptr-&gt;is_valuetypeptr() &amp;&amp; !elemptr-&gt;maybe_null()) {
  91     // Load from non-flattened but flattenable value type array (elements can never be null)
  92     bt = T_VALUETYPE;
  93   } else if (!ary_t-&gt;is_not_flat()) {
  94     // Cannot statically determine if array is flattened, emit runtime check
  95     assert(ValueArrayFlatten &amp;&amp; is_reference_type(bt) &amp;&amp; elemptr-&gt;can_be_value_type() &amp;&amp; !ary_t-&gt;klass_is_exact() &amp;&amp; !ary_t-&gt;is_not_null_free() &amp;&amp;
  96            (!elemptr-&gt;is_valuetypeptr() || elemptr-&gt;value_klass()-&gt;flatten_array()), &quot;array can&#39;t be flattened&quot;);
  97     Node* ctl = control();
  98     IdealKit ideal(this);
  99     IdealVariable res(ideal);
 100     ideal.declarations_done();
<span class="line-modified"> 101     Node* flattened = gen_flattened_array_test(ary);</span>
<span class="line-modified"> 102     ideal.if_then(flattened, BoolTest::ne, zerocon(flattened-&gt;bottom_type()-&gt;basic_type())); {</span>








 103       // flattened
 104       sync_kit(ideal);
 105       if (elemptr-&gt;is_valuetypeptr()) {
 106         // Element type is known, cast and load from flattened representation
 107         ciValueKlass* vk = elemptr-&gt;value_klass();
 108         assert(vk-&gt;flatten_array() &amp;&amp; elemptr-&gt;maybe_null(), &quot;must be a flattenable and nullable array&quot;);
<span class="line-modified"> 109         ciArrayKlass* array_klass = ciArrayKlass::make(vk, /* never_null */ true);</span>
 110         const TypeAryPtr* arytype = TypeOopPtr::make_from_klass(array_klass)-&gt;isa_aryptr();
 111         Node* cast = _gvn.transform(new CheckCastPPNode(control(), ary, arytype));
 112         Node* casted_adr = array_element_address(cast, idx, T_VALUETYPE, ary_t-&gt;size(), control());
 113         // Re-execute flattened array load if buffering triggers deoptimization
 114         PreserveReexecuteState preexecs(this);
 115         jvms()-&gt;set_should_reexecute(true);
 116         inc_sp(2);
 117         Node* vt = ValueTypeNode::make_from_flattened(this, vk, cast, casted_adr)-&gt;allocate(this, false)-&gt;get_oop();
 118         ideal.set(res, vt);
 119         ideal.sync_kit(this);
 120       } else {
<span class="line-removed"> 121         Node* kls = load_object_klass(ary);</span>
 122         // Element type is unknown, emit runtime call

 123         Node* k_adr = basic_plus_adr(kls, in_bytes(ArrayKlass::element_klass_offset()));
 124         Node* elem_klass = _gvn.transform(LoadKlassNode::make(_gvn, NULL, immutable_memory(), k_adr, TypeInstPtr::KLASS));
 125         Node* obj_size  = NULL;
 126         kill_dead_locals();
 127         // Re-execute flattened array load if buffering triggers deoptimization
 128         PreserveReexecuteState preexecs(this);
 129         jvms()-&gt;set_bci(_bci);
 130         jvms()-&gt;set_should_reexecute(true);
 131         inc_sp(2);
 132         Node* alloc_obj = new_instance(elem_klass, NULL, &amp;obj_size, /*deoptimize_on_exception=*/true);
 133 
 134         AllocateNode* alloc = AllocateNode::Ideal_allocation(alloc_obj, &amp;_gvn);
 135         assert(alloc-&gt;maybe_set_complete(&amp;_gvn), &quot;&quot;);
 136         alloc-&gt;initialization()-&gt;set_complete_with_arraycopy();
 137 
 138         // This membar keeps this access to an unknown flattened array
 139         // correctly ordered with other unknown and known flattened
 140         // array accesses.
 141         insert_mem_bar_volatile(Op_MemBarCPUOrder, C-&gt;get_alias_index(TypeAryPtr::VALUES));
 142 
</pre>
<hr />
<pre>
 172         // This makes sure no other thread sees a partially initialized buffered value
 173         insert_mem_bar_volatile(Op_MemBarStoreStore, Compile::AliasIdxRaw, alloc-&gt;proj_out_or_null(AllocateNode::RawAddress));
 174 
 175         // Same as MemBarCPUOrder above: keep this unknown flattened
 176         // array access correctly ordered with other flattened array
 177         // access
 178         insert_mem_bar_volatile(Op_MemBarCPUOrder, C-&gt;get_alias_index(TypeAryPtr::VALUES));
 179 
 180         // Prevent any use of the newly allocated value before it is
 181         // fully initialized
 182         alloc_obj = new CastPPNode(alloc_obj, _gvn.type(alloc_obj), true);
 183         alloc_obj-&gt;set_req(0, control());
 184         alloc_obj = _gvn.transform(alloc_obj);
 185 
 186         const Type* unknown_value = elemptr-&gt;is_instptr()-&gt;cast_to_flat_array();
 187         alloc_obj = _gvn.transform(new CheckCastPPNode(control(), alloc_obj, unknown_value));
 188 
 189         ideal.sync_kit(this);
 190         ideal.set(res, alloc_obj);
 191       }
<span class="line-removed"> 192     } ideal.else_(); {</span>
<span class="line-removed"> 193       // non-flattened</span>
<span class="line-removed"> 194       sync_kit(ideal);</span>
<span class="line-removed"> 195       const TypeAryPtr* adr_type = TypeAryPtr::get_array_body_type(bt);</span>
<span class="line-removed"> 196       Node* ld = access_load_at(ary, adr, adr_type, elemptr, bt,</span>
<span class="line-removed"> 197                                 IN_HEAP | IS_ARRAY | C2_CONTROL_DEPENDENT_LOAD, ctl);</span>
<span class="line-removed"> 198       ideal.sync_kit(this);</span>
<span class="line-removed"> 199       ideal.set(res, ld);</span>
 200     } ideal.end_if();
 201     sync_kit(ideal);
 202     Node* ld = _gvn.transform(ideal.value(res));
 203     ld = record_profile_for_speculation_at_array_load(ld);
 204     push_node(bt, ld);
 205     return;
 206   }
 207 
 208   if (elemtype == TypeInt::BOOL) {
 209     bt = T_BOOLEAN;
 210   }
 211   const TypeAryPtr* adr_type = TypeAryPtr::get_array_body_type(bt);
 212   Node* ld = access_load_at(ary, adr, adr_type, elemtype, bt,
 213                             IN_HEAP | IS_ARRAY | C2_CONTROL_DEPENDENT_LOAD);
 214   if (bt == T_VALUETYPE) {
 215     // Loading a non-flattened (but flattenable) value type from an array
 216     assert(!gvn().type(ld)-&gt;maybe_null(), &quot;value type array elements should never be null&quot;);
 217     if (elemptr-&gt;value_klass()-&gt;is_scalarizable()) {
 218       ld = ValueTypeNode::make_from_oop(this, ld, elemptr-&gt;value_klass());
 219     }
</pre>
<hr />
<pre>
 285       }
 286       // Re-execute flattened array store if buffering triggers deoptimization
 287       PreserveReexecuteState preexecs(this);
 288       inc_sp(3);
 289       jvms()-&gt;set_should_reexecute(true);
 290       cast_val-&gt;as_ValueType()-&gt;store_flattened(this, ary, adr, NULL, 0, MO_UNORDERED | IN_HEAP | IS_ARRAY);
 291       return;
 292     } else if (elemtype-&gt;is_valuetypeptr() &amp;&amp; !elemtype-&gt;maybe_null()) {
 293       // Store to non-flattened but flattenable value type array (elements can never be null)
 294       if (!cast_val-&gt;is_ValueType() &amp;&amp; tval-&gt;maybe_null()) {
 295         inc_sp(3);
 296         cast_val = null_check(cast_val);
 297         if (stopped()) return;
 298         dec_sp(3);
 299       }
 300     } else if (!ary_t-&gt;is_not_flat()) {
 301       // Array might be flattened, emit runtime checks
 302       assert(ValueArrayFlatten &amp;&amp; !not_flattenable &amp;&amp; elemtype-&gt;is_oopptr()-&gt;can_be_value_type() &amp;&amp;
 303              !ary_t-&gt;klass_is_exact() &amp;&amp; !ary_t-&gt;is_not_null_free(), &quot;array can&#39;t be flattened&quot;);
 304       IdealKit ideal(this);
<span class="line-modified"> 305       Node* flattened = gen_flattened_array_test(ary);</span>
<span class="line-modified"> 306       ideal.if_then(flattened, BoolTest::ne, zerocon(flattened-&gt;bottom_type()-&gt;basic_type())); {</span>








 307         Node* val = cast_val;
 308         // flattened
 309         if (!val-&gt;is_ValueType() &amp;&amp; tval-&gt;maybe_null()) {
 310           // Add null check
 311           sync_kit(ideal);
 312           Node* null_ctl = top();
 313           val = null_check_oop(val, &amp;null_ctl);
 314           if (null_ctl != top()) {
 315             PreserveJVMState pjvms(this);
 316             inc_sp(3);
 317             set_control(null_ctl);
 318             uncommon_trap(Deoptimization::Reason_null_check, Deoptimization::Action_none);
 319             dec_sp(3);
 320           }
 321           ideal.sync_kit(this);
 322         }
 323         // Try to determine the value klass
 324         ciValueKlass* vk = NULL;
 325         if (tval-&gt;isa_valuetype() || tval-&gt;is_valuetypeptr()) {
 326           vk = tval-&gt;value_klass();
 327         } else if (tval_init-&gt;isa_valuetype() || tval_init-&gt;is_valuetypeptr()) {
 328           vk = tval_init-&gt;value_klass();
 329         } else if (elemtype-&gt;is_valuetypeptr()) {
 330           vk = elemtype-&gt;value_klass();
 331         }
 332         Node* casted_ary = ary;
 333         if (vk != NULL &amp;&amp; !stopped()) {
 334           // Element type is known, cast and store to flattened representation
 335           sync_kit(ideal);
 336           assert(vk-&gt;flatten_array() &amp;&amp; elemtype-&gt;maybe_null(), &quot;must be a flattenable and nullable array&quot;);
<span class="line-modified"> 337           ciArrayKlass* array_klass = ciArrayKlass::make(vk, /* never_null */ true);</span>
 338           const TypeAryPtr* arytype = TypeOopPtr::make_from_klass(array_klass)-&gt;isa_aryptr();
 339           casted_ary = _gvn.transform(new CheckCastPPNode(control(), casted_ary, arytype));
 340           Node* casted_adr = array_element_address(casted_ary, idx, T_OBJECT, arytype-&gt;size(), control());
 341           if (!val-&gt;is_ValueType()) {
 342             assert(!gvn().type(val)-&gt;maybe_null(), &quot;value type array elements should never be null&quot;);
 343             val = ValueTypeNode::make_from_oop(this, val, vk);
 344           }
 345           // Re-execute flattened array store if buffering triggers deoptimization
 346           PreserveReexecuteState preexecs(this);
 347           inc_sp(3);
 348           jvms()-&gt;set_should_reexecute(true);
 349           val-&gt;as_ValueType()-&gt;store_flattened(this, casted_ary, casted_adr, NULL, 0, MO_UNORDERED | IN_HEAP | IS_ARRAY);
 350           ideal.sync_kit(this);
 351         } else if (!ideal.ctrl()-&gt;is_top()) {
 352           // Element type is unknown, emit runtime call
 353           sync_kit(ideal);
 354 
 355           // This membar keeps this access to an unknown flattened
 356           // array correctly ordered with other unknown and known
 357           // flattened array accesses.
 358           insert_mem_bar_volatile(Op_MemBarCPUOrder, C-&gt;get_alias_index(TypeAryPtr::VALUES));
 359           ideal.sync_kit(this);
 360 
 361           ideal.make_leaf_call(OptoRuntime::store_unknown_value_Type(),
 362                                CAST_FROM_FN_PTR(address, OptoRuntime::store_unknown_value),
 363                                &quot;store_unknown_value&quot;,
 364                                val, casted_ary, idx);
 365 
 366           sync_kit(ideal);
 367           // Same as MemBarCPUOrder above: keep this unknown
 368           // flattened array access correctly ordered with other
 369           // flattened array accesses.
 370           insert_mem_bar_volatile(Op_MemBarCPUOrder, C-&gt;get_alias_index(TypeAryPtr::VALUES));
 371           ideal.sync_kit(this);
 372         }
 373       }
<span class="line-removed"> 374       ideal.else_();</span>
<span class="line-removed"> 375       {</span>
<span class="line-removed"> 376         // non-flattened</span>
<span class="line-removed"> 377         sync_kit(ideal);</span>
<span class="line-removed"> 378         gen_value_array_null_guard(ary, cast_val, 3);</span>
<span class="line-removed"> 379         inc_sp(3);</span>
<span class="line-removed"> 380         access_store_at(ary, adr, adr_type, cast_val, elemtype, bt, MO_UNORDERED | IN_HEAP | IS_ARRAY, false);</span>
<span class="line-removed"> 381         dec_sp(3);</span>
<span class="line-removed"> 382         ideal.sync_kit(this);</span>
<span class="line-removed"> 383       }</span>
 384       ideal.end_if();
 385       sync_kit(ideal);
 386       return;
 387     } else if (!ary_t-&gt;is_not_null_free()) {
 388       // Array is not flattened but may be null free
 389       assert(elemtype-&gt;is_oopptr()-&gt;can_be_value_type() &amp;&amp; !ary_t-&gt;klass_is_exact(), &quot;array can&#39;t be null free&quot;);
 390       ary = gen_value_array_null_guard(ary, cast_val, 3, true);
 391     }
 392   }
 393   inc_sp(3);
 394   access_store_at(ary, adr, adr_type, val, elemtype, bt, MO_UNORDERED | IN_HEAP | IS_ARRAY);
 395   dec_sp(3);
 396 }
 397 
 398 
 399 //------------------------------array_addressing-------------------------------
 400 // Pull array and index from the stack.  Compute pointer-to-element.
 401 Node* Parse::array_addressing(BasicType type, int vals, const Type*&amp; elemtype) {
 402   Node *idx   = peek(0+vals);   // Get from stack without popping
 403   Node *ary   = peek(1+vals);   // in case of exception
</pre>
<hr />
<pre>
 555       elemtype-&gt;isa_valuetype() == NULL &amp;&amp;
 556       (elemptr == NULL || !elemptr-&gt;is_valuetypeptr()) &amp;&amp;
 557       UseArrayLoadStoreProfile) {
 558     assert(is_reference_type(type), &quot;&quot;);
 559     bool null_free_array = true;
 560     Deoptimization::DeoptReason reason = Deoptimization::Reason_none;
 561     if (arytype-&gt;speculative() != NULL &amp;&amp;
 562         arytype-&gt;speculative()-&gt;is_aryptr()-&gt;is_not_null_free() &amp;&amp;
 563         !too_many_traps_or_recompiles(Deoptimization::Reason_speculate_class_check)) {
 564       null_free_array = false;
 565       reason = Deoptimization::Reason_speculate_class_check;
 566     } else if (!too_many_traps_or_recompiles(Deoptimization::Reason_class_check)) {
 567       ciKlass* array_type = NULL;
 568       ciKlass* element_type = NULL;
 569       ProfilePtrKind element_ptr = ProfileMaybeNull;
 570       bool flat_array = true;
 571       method()-&gt;array_access_profiled_type(bci(), array_type, element_type, element_ptr, flat_array, null_free_array);
 572       reason = Deoptimization::Reason_class_check;
 573     }
 574     if (!null_free_array) {
<span class="line-modified"> 575       Node* tst = gen_null_free_array_check(ary);</span>
<span class="line-modified"> 576       {</span>
<span class="line-removed"> 577         BuildCutout unless(this, tst, PROB_MAX);</span>
 578         uncommon_trap_exact(reason, Deoptimization::Action_maybe_recompile);
 579       }
 580       Node* better_ary = _gvn.transform(new CheckCastPPNode(control(), ary, arytype-&gt;cast_to_not_null_free()));
 581       replace_in_map(ary, better_ary);
 582       ary = better_ary;
<span class="line-modified"> 583       arytype  = _gvn.type(ary)-&gt;is_aryptr();</span>
 584     }
 585   }
 586 
 587   if (!arytype-&gt;is_not_flat() &amp;&amp; elemtype-&gt;isa_valuetype() == NULL) {
 588     assert(is_reference_type(type), &quot;&quot;);
 589     bool flat_array = true;
 590     Deoptimization::DeoptReason reason = Deoptimization::Reason_none;
 591     if (arytype-&gt;speculative() != NULL &amp;&amp;
 592         arytype-&gt;speculative()-&gt;is_aryptr()-&gt;is_not_flat() &amp;&amp;
 593         !too_many_traps_or_recompiles(Deoptimization::Reason_speculate_class_check)) {
 594       flat_array = false;
 595       reason = Deoptimization::Reason_speculate_class_check;
 596     } else if (UseArrayLoadStoreProfile &amp;&amp; !too_many_traps_or_recompiles(reason)) {
 597       ciKlass* array_type = NULL;
 598       ciKlass* element_type = NULL;
 599       ProfilePtrKind element_ptr = ProfileMaybeNull;
 600       bool null_free_array = true;
 601       method()-&gt;array_access_profiled_type(bci(), array_type, element_type, element_ptr, flat_array, null_free_array);
 602       reason = Deoptimization::Reason_class_check;
 603     }
 604     if (!flat_array) {
<span class="line-modified"> 605       Node* flattened = gen_flattened_array_test(ary);</span>
<span class="line-modified"> 606       Node* chk = NULL;</span>
<span class="line-removed"> 607       if (_gvn.type(flattened)-&gt;isa_int()) {</span>
<span class="line-removed"> 608         chk = _gvn.transform(new CmpINode(flattened, intcon(0)));</span>
<span class="line-removed"> 609       } else {</span>
<span class="line-removed"> 610         assert(_gvn.type(flattened)-&gt;isa_long(), &quot;flattened property is int or long&quot;);</span>
<span class="line-removed"> 611         chk = _gvn.transform(new CmpLNode(flattened, longcon(0)));</span>
<span class="line-removed"> 612       }</span>
<span class="line-removed"> 613       Node* tst = _gvn.transform(new BoolNode(chk, BoolTest::eq));</span>
<span class="line-removed"> 614       {</span>
<span class="line-removed"> 615         BuildCutout unless(this, tst, PROB_MAX);</span>
 616         uncommon_trap_exact(reason, Deoptimization::Action_maybe_recompile);
 617       }
 618       Node* better_ary = _gvn.transform(new CheckCastPPNode(control(), ary, arytype-&gt;cast_to_not_flat()));
 619       replace_in_map(ary, better_ary);
 620       ary = better_ary;
<span class="line-modified"> 621       arytype  = _gvn.type(ary)-&gt;is_aryptr();</span>
 622     }
 623   }
 624 
 625   // Make array address computation control dependent to prevent it
 626   // from floating above the range check during loop optimizations.
 627   Node* ptr = array_element_address(ary, idx, type, sizetype, control());
 628   assert(ptr != top(), &quot;top should go hand-in-hand with stopped&quot;);
 629 
 630   return ptr;
 631 }
 632 
 633 
 634 // returns IfNode
 635 IfNode* Parse::jump_if_fork_int(Node* a, Node* b, BoolTest::mask mask, float prob, float cnt) {
 636   Node   *cmp = _gvn.transform(new CmpINode(a, b)); // two cases: shiftcount &gt; 32 and shiftcount &lt;= 32
 637   Node   *tst = _gvn.transform(new BoolNode(cmp, mask));
 638   IfNode *iff = create_and_map_if(control(), tst, prob, cnt);
 639   return iff;
 640 }
 641 
</pre>
<hr />
<pre>
2130   Node* null_ctl = top();
2131   Node* not_null_a = null_check_oop(a, &amp;null_ctl, !too_many_traps(Deoptimization::Reason_null_check), false, false);
2132   dec_sp(2);
2133   ne_region-&gt;init_req(1, null_ctl);
2134   if (stopped()) {
2135     record_for_igvn(ne_region);
2136     set_control(_gvn.transform(ne_region));
2137     if (btest == BoolTest::ne) {
2138       {
2139         PreserveJVMState pjvms(this);
2140         int target_bci = iter().get_dest();
2141         merge(target_bci);
2142       }
2143       record_for_igvn(eq_region);
2144       set_control(_gvn.transform(eq_region));
2145     }
2146     return;
2147   }
2148 
2149   // First operand is non-null, check if it is a value type
<span class="line-modified">2150   Node* is_value = is_always_locked(not_null_a);</span>
2151   IfNode* is_value_iff = create_and_map_if(control(), is_value, PROB_FAIR, COUNT_UNKNOWN);
2152   Node* not_value = _gvn.transform(new IfFalseNode(is_value_iff));
2153   ne_region-&gt;init_req(2, not_value);
2154   set_control(_gvn.transform(new IfTrueNode(is_value_iff)));
2155 
2156   // The first operand is a value type, check if the second operand is non-null
2157   inc_sp(2);
2158   null_ctl = top();
2159   Node* not_null_b = null_check_oop(b, &amp;null_ctl, !too_many_traps(Deoptimization::Reason_null_check), false, false);
2160   dec_sp(2);
2161   ne_region-&gt;init_req(3, null_ctl);
2162   if (stopped()) {
2163     record_for_igvn(ne_region);
2164     set_control(_gvn.transform(ne_region));
2165     if (btest == BoolTest::ne) {
2166       {
2167         PreserveJVMState pjvms(this);
2168         int target_bci = iter().get_dest();
2169         merge(target_bci);
2170       }
</pre>
</td>
<td>
<hr />
<pre>
  81   // Handle value type arrays
  82   const TypeOopPtr* elemptr = elemtype-&gt;make_oopptr();
  83   const TypeAryPtr* ary_t = _gvn.type(ary)-&gt;is_aryptr();
  84   if (elemtype-&gt;isa_valuetype() != NULL) {
  85     C-&gt;set_flattened_accesses();
  86     // Load from flattened value type array
  87     Node* vt = ValueTypeNode::make_from_flattened(this, elemtype-&gt;value_klass(), ary, adr);
  88     push(vt);
  89     return;
  90   } else if (elemptr != NULL &amp;&amp; elemptr-&gt;is_valuetypeptr() &amp;&amp; !elemptr-&gt;maybe_null()) {
  91     // Load from non-flattened but flattenable value type array (elements can never be null)
  92     bt = T_VALUETYPE;
  93   } else if (!ary_t-&gt;is_not_flat()) {
  94     // Cannot statically determine if array is flattened, emit runtime check
  95     assert(ValueArrayFlatten &amp;&amp; is_reference_type(bt) &amp;&amp; elemptr-&gt;can_be_value_type() &amp;&amp; !ary_t-&gt;klass_is_exact() &amp;&amp; !ary_t-&gt;is_not_null_free() &amp;&amp;
  96            (!elemptr-&gt;is_valuetypeptr() || elemptr-&gt;value_klass()-&gt;flatten_array()), &quot;array can&#39;t be flattened&quot;);
  97     Node* ctl = control();
  98     IdealKit ideal(this);
  99     IdealVariable res(ideal);
 100     ideal.declarations_done();
<span class="line-modified"> 101     ideal.if_then(is_non_flattened_array(ary)); {</span>
<span class="line-modified"> 102       // non-flattened</span>
<span class="line-added"> 103       assert(ideal.ctrl()-&gt;in(0)-&gt;as_If()-&gt;is_non_flattened_array_check(&amp;_gvn), &quot;Should be found&quot;);</span>
<span class="line-added"> 104       sync_kit(ideal);</span>
<span class="line-added"> 105       const TypeAryPtr* adr_type = TypeAryPtr::get_array_body_type(bt);</span>
<span class="line-added"> 106       Node* ld = access_load_at(ary, adr, adr_type, elemptr, bt,</span>
<span class="line-added"> 107                                 IN_HEAP | IS_ARRAY | C2_CONTROL_DEPENDENT_LOAD, ctl);</span>
<span class="line-added"> 108       ideal.sync_kit(this);</span>
<span class="line-added"> 109       ideal.set(res, ld);</span>
<span class="line-added"> 110     } ideal.else_(); {</span>
 111       // flattened
 112       sync_kit(ideal);
 113       if (elemptr-&gt;is_valuetypeptr()) {
 114         // Element type is known, cast and load from flattened representation
 115         ciValueKlass* vk = elemptr-&gt;value_klass();
 116         assert(vk-&gt;flatten_array() &amp;&amp; elemptr-&gt;maybe_null(), &quot;must be a flattenable and nullable array&quot;);
<span class="line-modified"> 117         ciArrayKlass* array_klass = ciArrayKlass::make(vk);</span>
 118         const TypeAryPtr* arytype = TypeOopPtr::make_from_klass(array_klass)-&gt;isa_aryptr();
 119         Node* cast = _gvn.transform(new CheckCastPPNode(control(), ary, arytype));
 120         Node* casted_adr = array_element_address(cast, idx, T_VALUETYPE, ary_t-&gt;size(), control());
 121         // Re-execute flattened array load if buffering triggers deoptimization
 122         PreserveReexecuteState preexecs(this);
 123         jvms()-&gt;set_should_reexecute(true);
 124         inc_sp(2);
 125         Node* vt = ValueTypeNode::make_from_flattened(this, vk, cast, casted_adr)-&gt;allocate(this, false)-&gt;get_oop();
 126         ideal.set(res, vt);
 127         ideal.sync_kit(this);
 128       } else {

 129         // Element type is unknown, emit runtime call
<span class="line-added"> 130         Node* kls = load_object_klass(ary);</span>
 131         Node* k_adr = basic_plus_adr(kls, in_bytes(ArrayKlass::element_klass_offset()));
 132         Node* elem_klass = _gvn.transform(LoadKlassNode::make(_gvn, NULL, immutable_memory(), k_adr, TypeInstPtr::KLASS));
 133         Node* obj_size  = NULL;
 134         kill_dead_locals();
 135         // Re-execute flattened array load if buffering triggers deoptimization
 136         PreserveReexecuteState preexecs(this);
 137         jvms()-&gt;set_bci(_bci);
 138         jvms()-&gt;set_should_reexecute(true);
 139         inc_sp(2);
 140         Node* alloc_obj = new_instance(elem_klass, NULL, &amp;obj_size, /*deoptimize_on_exception=*/true);
 141 
 142         AllocateNode* alloc = AllocateNode::Ideal_allocation(alloc_obj, &amp;_gvn);
 143         assert(alloc-&gt;maybe_set_complete(&amp;_gvn), &quot;&quot;);
 144         alloc-&gt;initialization()-&gt;set_complete_with_arraycopy();
 145 
 146         // This membar keeps this access to an unknown flattened array
 147         // correctly ordered with other unknown and known flattened
 148         // array accesses.
 149         insert_mem_bar_volatile(Op_MemBarCPUOrder, C-&gt;get_alias_index(TypeAryPtr::VALUES));
 150 
</pre>
<hr />
<pre>
 180         // This makes sure no other thread sees a partially initialized buffered value
 181         insert_mem_bar_volatile(Op_MemBarStoreStore, Compile::AliasIdxRaw, alloc-&gt;proj_out_or_null(AllocateNode::RawAddress));
 182 
 183         // Same as MemBarCPUOrder above: keep this unknown flattened
 184         // array access correctly ordered with other flattened array
 185         // access
 186         insert_mem_bar_volatile(Op_MemBarCPUOrder, C-&gt;get_alias_index(TypeAryPtr::VALUES));
 187 
 188         // Prevent any use of the newly allocated value before it is
 189         // fully initialized
 190         alloc_obj = new CastPPNode(alloc_obj, _gvn.type(alloc_obj), true);
 191         alloc_obj-&gt;set_req(0, control());
 192         alloc_obj = _gvn.transform(alloc_obj);
 193 
 194         const Type* unknown_value = elemptr-&gt;is_instptr()-&gt;cast_to_flat_array();
 195         alloc_obj = _gvn.transform(new CheckCastPPNode(control(), alloc_obj, unknown_value));
 196 
 197         ideal.sync_kit(this);
 198         ideal.set(res, alloc_obj);
 199       }








 200     } ideal.end_if();
 201     sync_kit(ideal);
 202     Node* ld = _gvn.transform(ideal.value(res));
 203     ld = record_profile_for_speculation_at_array_load(ld);
 204     push_node(bt, ld);
 205     return;
 206   }
 207 
 208   if (elemtype == TypeInt::BOOL) {
 209     bt = T_BOOLEAN;
 210   }
 211   const TypeAryPtr* adr_type = TypeAryPtr::get_array_body_type(bt);
 212   Node* ld = access_load_at(ary, adr, adr_type, elemtype, bt,
 213                             IN_HEAP | IS_ARRAY | C2_CONTROL_DEPENDENT_LOAD);
 214   if (bt == T_VALUETYPE) {
 215     // Loading a non-flattened (but flattenable) value type from an array
 216     assert(!gvn().type(ld)-&gt;maybe_null(), &quot;value type array elements should never be null&quot;);
 217     if (elemptr-&gt;value_klass()-&gt;is_scalarizable()) {
 218       ld = ValueTypeNode::make_from_oop(this, ld, elemptr-&gt;value_klass());
 219     }
</pre>
<hr />
<pre>
 285       }
 286       // Re-execute flattened array store if buffering triggers deoptimization
 287       PreserveReexecuteState preexecs(this);
 288       inc_sp(3);
 289       jvms()-&gt;set_should_reexecute(true);
 290       cast_val-&gt;as_ValueType()-&gt;store_flattened(this, ary, adr, NULL, 0, MO_UNORDERED | IN_HEAP | IS_ARRAY);
 291       return;
 292     } else if (elemtype-&gt;is_valuetypeptr() &amp;&amp; !elemtype-&gt;maybe_null()) {
 293       // Store to non-flattened but flattenable value type array (elements can never be null)
 294       if (!cast_val-&gt;is_ValueType() &amp;&amp; tval-&gt;maybe_null()) {
 295         inc_sp(3);
 296         cast_val = null_check(cast_val);
 297         if (stopped()) return;
 298         dec_sp(3);
 299       }
 300     } else if (!ary_t-&gt;is_not_flat()) {
 301       // Array might be flattened, emit runtime checks
 302       assert(ValueArrayFlatten &amp;&amp; !not_flattenable &amp;&amp; elemtype-&gt;is_oopptr()-&gt;can_be_value_type() &amp;&amp;
 303              !ary_t-&gt;klass_is_exact() &amp;&amp; !ary_t-&gt;is_not_null_free(), &quot;array can&#39;t be flattened&quot;);
 304       IdealKit ideal(this);
<span class="line-modified"> 305       ideal.if_then(is_non_flattened_array(ary)); {</span>
<span class="line-modified"> 306         // non-flattened</span>
<span class="line-added"> 307         assert(ideal.ctrl()-&gt;in(0)-&gt;as_If()-&gt;is_non_flattened_array_check(&amp;_gvn), &quot;Should be found&quot;);</span>
<span class="line-added"> 308         sync_kit(ideal);</span>
<span class="line-added"> 309         gen_value_array_null_guard(ary, cast_val, 3);</span>
<span class="line-added"> 310         inc_sp(3);</span>
<span class="line-added"> 311         access_store_at(ary, adr, adr_type, cast_val, elemtype, bt, MO_UNORDERED | IN_HEAP | IS_ARRAY, false);</span>
<span class="line-added"> 312         dec_sp(3);</span>
<span class="line-added"> 313         ideal.sync_kit(this);</span>
<span class="line-added"> 314       } ideal.else_(); {</span>
 315         Node* val = cast_val;
 316         // flattened
 317         if (!val-&gt;is_ValueType() &amp;&amp; tval-&gt;maybe_null()) {
 318           // Add null check
 319           sync_kit(ideal);
 320           Node* null_ctl = top();
 321           val = null_check_oop(val, &amp;null_ctl);
 322           if (null_ctl != top()) {
 323             PreserveJVMState pjvms(this);
 324             inc_sp(3);
 325             set_control(null_ctl);
 326             uncommon_trap(Deoptimization::Reason_null_check, Deoptimization::Action_none);
 327             dec_sp(3);
 328           }
 329           ideal.sync_kit(this);
 330         }
 331         // Try to determine the value klass
 332         ciValueKlass* vk = NULL;
 333         if (tval-&gt;isa_valuetype() || tval-&gt;is_valuetypeptr()) {
 334           vk = tval-&gt;value_klass();
 335         } else if (tval_init-&gt;isa_valuetype() || tval_init-&gt;is_valuetypeptr()) {
 336           vk = tval_init-&gt;value_klass();
 337         } else if (elemtype-&gt;is_valuetypeptr()) {
 338           vk = elemtype-&gt;value_klass();
 339         }
 340         Node* casted_ary = ary;
 341         if (vk != NULL &amp;&amp; !stopped()) {
 342           // Element type is known, cast and store to flattened representation
 343           sync_kit(ideal);
 344           assert(vk-&gt;flatten_array() &amp;&amp; elemtype-&gt;maybe_null(), &quot;must be a flattenable and nullable array&quot;);
<span class="line-modified"> 345           ciArrayKlass* array_klass = ciArrayKlass::make(vk);</span>
 346           const TypeAryPtr* arytype = TypeOopPtr::make_from_klass(array_klass)-&gt;isa_aryptr();
 347           casted_ary = _gvn.transform(new CheckCastPPNode(control(), casted_ary, arytype));
 348           Node* casted_adr = array_element_address(casted_ary, idx, T_OBJECT, arytype-&gt;size(), control());
 349           if (!val-&gt;is_ValueType()) {
 350             assert(!gvn().type(val)-&gt;maybe_null(), &quot;value type array elements should never be null&quot;);
 351             val = ValueTypeNode::make_from_oop(this, val, vk);
 352           }
 353           // Re-execute flattened array store if buffering triggers deoptimization
 354           PreserveReexecuteState preexecs(this);
 355           inc_sp(3);
 356           jvms()-&gt;set_should_reexecute(true);
 357           val-&gt;as_ValueType()-&gt;store_flattened(this, casted_ary, casted_adr, NULL, 0, MO_UNORDERED | IN_HEAP | IS_ARRAY);
 358           ideal.sync_kit(this);
 359         } else if (!ideal.ctrl()-&gt;is_top()) {
 360           // Element type is unknown, emit runtime call
 361           sync_kit(ideal);
 362 
 363           // This membar keeps this access to an unknown flattened
 364           // array correctly ordered with other unknown and known
 365           // flattened array accesses.
 366           insert_mem_bar_volatile(Op_MemBarCPUOrder, C-&gt;get_alias_index(TypeAryPtr::VALUES));
 367           ideal.sync_kit(this);
 368 
 369           ideal.make_leaf_call(OptoRuntime::store_unknown_value_Type(),
 370                                CAST_FROM_FN_PTR(address, OptoRuntime::store_unknown_value),
 371                                &quot;store_unknown_value&quot;,
 372                                val, casted_ary, idx);
 373 
 374           sync_kit(ideal);
 375           // Same as MemBarCPUOrder above: keep this unknown
 376           // flattened array access correctly ordered with other
 377           // flattened array accesses.
 378           insert_mem_bar_volatile(Op_MemBarCPUOrder, C-&gt;get_alias_index(TypeAryPtr::VALUES));
 379           ideal.sync_kit(this);
 380         }
 381       }










 382       ideal.end_if();
 383       sync_kit(ideal);
 384       return;
 385     } else if (!ary_t-&gt;is_not_null_free()) {
 386       // Array is not flattened but may be null free
 387       assert(elemtype-&gt;is_oopptr()-&gt;can_be_value_type() &amp;&amp; !ary_t-&gt;klass_is_exact(), &quot;array can&#39;t be null free&quot;);
 388       ary = gen_value_array_null_guard(ary, cast_val, 3, true);
 389     }
 390   }
 391   inc_sp(3);
 392   access_store_at(ary, adr, adr_type, val, elemtype, bt, MO_UNORDERED | IN_HEAP | IS_ARRAY);
 393   dec_sp(3);
 394 }
 395 
 396 
 397 //------------------------------array_addressing-------------------------------
 398 // Pull array and index from the stack.  Compute pointer-to-element.
 399 Node* Parse::array_addressing(BasicType type, int vals, const Type*&amp; elemtype) {
 400   Node *idx   = peek(0+vals);   // Get from stack without popping
 401   Node *ary   = peek(1+vals);   // in case of exception
</pre>
<hr />
<pre>
 553       elemtype-&gt;isa_valuetype() == NULL &amp;&amp;
 554       (elemptr == NULL || !elemptr-&gt;is_valuetypeptr()) &amp;&amp;
 555       UseArrayLoadStoreProfile) {
 556     assert(is_reference_type(type), &quot;&quot;);
 557     bool null_free_array = true;
 558     Deoptimization::DeoptReason reason = Deoptimization::Reason_none;
 559     if (arytype-&gt;speculative() != NULL &amp;&amp;
 560         arytype-&gt;speculative()-&gt;is_aryptr()-&gt;is_not_null_free() &amp;&amp;
 561         !too_many_traps_or_recompiles(Deoptimization::Reason_speculate_class_check)) {
 562       null_free_array = false;
 563       reason = Deoptimization::Reason_speculate_class_check;
 564     } else if (!too_many_traps_or_recompiles(Deoptimization::Reason_class_check)) {
 565       ciKlass* array_type = NULL;
 566       ciKlass* element_type = NULL;
 567       ProfilePtrKind element_ptr = ProfileMaybeNull;
 568       bool flat_array = true;
 569       method()-&gt;array_access_profiled_type(bci(), array_type, element_type, element_ptr, flat_array, null_free_array);
 570       reason = Deoptimization::Reason_class_check;
 571     }
 572     if (!null_free_array) {
<span class="line-modified"> 573       { // Deoptimize if null-free array</span>
<span class="line-modified"> 574         BuildCutout unless(this, is_nullable_array(ary), PROB_MAX);</span>

 575         uncommon_trap_exact(reason, Deoptimization::Action_maybe_recompile);
 576       }
 577       Node* better_ary = _gvn.transform(new CheckCastPPNode(control(), ary, arytype-&gt;cast_to_not_null_free()));
 578       replace_in_map(ary, better_ary);
 579       ary = better_ary;
<span class="line-modified"> 580       arytype = _gvn.type(ary)-&gt;is_aryptr();</span>
 581     }
 582   }
 583 
 584   if (!arytype-&gt;is_not_flat() &amp;&amp; elemtype-&gt;isa_valuetype() == NULL) {
 585     assert(is_reference_type(type), &quot;&quot;);
 586     bool flat_array = true;
 587     Deoptimization::DeoptReason reason = Deoptimization::Reason_none;
 588     if (arytype-&gt;speculative() != NULL &amp;&amp;
 589         arytype-&gt;speculative()-&gt;is_aryptr()-&gt;is_not_flat() &amp;&amp;
 590         !too_many_traps_or_recompiles(Deoptimization::Reason_speculate_class_check)) {
 591       flat_array = false;
 592       reason = Deoptimization::Reason_speculate_class_check;
 593     } else if (UseArrayLoadStoreProfile &amp;&amp; !too_many_traps_or_recompiles(reason)) {
 594       ciKlass* array_type = NULL;
 595       ciKlass* element_type = NULL;
 596       ProfilePtrKind element_ptr = ProfileMaybeNull;
 597       bool null_free_array = true;
 598       method()-&gt;array_access_profiled_type(bci(), array_type, element_type, element_ptr, flat_array, null_free_array);
 599       reason = Deoptimization::Reason_class_check;
 600     }
 601     if (!flat_array) {
<span class="line-modified"> 602       { // Deoptimize if flat array</span>
<span class="line-modified"> 603         BuildCutout unless(this, is_non_flattened_array(ary), PROB_MAX);</span>









 604         uncommon_trap_exact(reason, Deoptimization::Action_maybe_recompile);
 605       }
 606       Node* better_ary = _gvn.transform(new CheckCastPPNode(control(), ary, arytype-&gt;cast_to_not_flat()));
 607       replace_in_map(ary, better_ary);
 608       ary = better_ary;
<span class="line-modified"> 609       arytype = _gvn.type(ary)-&gt;is_aryptr();</span>
 610     }
 611   }
 612 
 613   // Make array address computation control dependent to prevent it
 614   // from floating above the range check during loop optimizations.
 615   Node* ptr = array_element_address(ary, idx, type, sizetype, control());
 616   assert(ptr != top(), &quot;top should go hand-in-hand with stopped&quot;);
 617 
 618   return ptr;
 619 }
 620 
 621 
 622 // returns IfNode
 623 IfNode* Parse::jump_if_fork_int(Node* a, Node* b, BoolTest::mask mask, float prob, float cnt) {
 624   Node   *cmp = _gvn.transform(new CmpINode(a, b)); // two cases: shiftcount &gt; 32 and shiftcount &lt;= 32
 625   Node   *tst = _gvn.transform(new BoolNode(cmp, mask));
 626   IfNode *iff = create_and_map_if(control(), tst, prob, cnt);
 627   return iff;
 628 }
 629 
</pre>
<hr />
<pre>
2118   Node* null_ctl = top();
2119   Node* not_null_a = null_check_oop(a, &amp;null_ctl, !too_many_traps(Deoptimization::Reason_null_check), false, false);
2120   dec_sp(2);
2121   ne_region-&gt;init_req(1, null_ctl);
2122   if (stopped()) {
2123     record_for_igvn(ne_region);
2124     set_control(_gvn.transform(ne_region));
2125     if (btest == BoolTest::ne) {
2126       {
2127         PreserveJVMState pjvms(this);
2128         int target_bci = iter().get_dest();
2129         merge(target_bci);
2130       }
2131       record_for_igvn(eq_region);
2132       set_control(_gvn.transform(eq_region));
2133     }
2134     return;
2135   }
2136 
2137   // First operand is non-null, check if it is a value type
<span class="line-modified">2138   Node* is_value = is_value_type(not_null_a);</span>
2139   IfNode* is_value_iff = create_and_map_if(control(), is_value, PROB_FAIR, COUNT_UNKNOWN);
2140   Node* not_value = _gvn.transform(new IfFalseNode(is_value_iff));
2141   ne_region-&gt;init_req(2, not_value);
2142   set_control(_gvn.transform(new IfTrueNode(is_value_iff)));
2143 
2144   // The first operand is a value type, check if the second operand is non-null
2145   inc_sp(2);
2146   null_ctl = top();
2147   Node* not_null_b = null_check_oop(b, &amp;null_ctl, !too_many_traps(Deoptimization::Reason_null_check), false, false);
2148   dec_sp(2);
2149   ne_region-&gt;init_req(3, null_ctl);
2150   if (stopped()) {
2151     record_for_igvn(ne_region);
2152     set_control(_gvn.transform(ne_region));
2153     if (btest == BoolTest::ne) {
2154       {
2155         PreserveJVMState pjvms(this);
2156         int target_bci = iter().get_dest();
2157         merge(target_bci);
2158       }
</pre>
</td>
</tr>
</table>
<center><a href="memnode.hpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="parse3.cpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>