diff a/src/hotspot/share/opto/output.cpp b/src/hotspot/share/opto/output.cpp
--- a/src/hotspot/share/opto/output.cpp
+++ b/src/hotspot/share/opto/output.cpp
@@ -51,13 +51,10 @@
 #include "runtime/handles.inline.hpp"
 #include "runtime/sharedRuntime.hpp"
 #include "utilities/macros.hpp"
 #include "utilities/powerOfTwo.hpp"
 #include "utilities/xmlstream.hpp"
-#ifdef X86
-#include "c2_intelJccErratum_x86.hpp"
-#endif
 
 #ifndef PRODUCT
 #define DEBUG_ARG(x) , x
 #else
 #define DEBUG_ARG(x)
@@ -243,11 +240,14 @@
     _node_bundling_limit(0),
     _node_bundling_base(NULL),
     _orig_pc_slot(0),
     _orig_pc_slot_offset_in_bytes(0),
     _sp_inc_slot(0),
-    _sp_inc_slot_offset_in_bytes(0) {
+    _sp_inc_slot_offset_in_bytes(0),
+    _buf_sizes(),
+    _block(NULL),
+    _index(0) {
   C->set_output(this);
   if (C->stub_name() == NULL) {
     int fixed_slots = C->fixed_slots();
     if (C->needs_stack_repair()) {
       fixed_slots -= 2;
@@ -262,10 +262,19 @@
   if (_scratch_buffer_blob != NULL) {
     BufferBlob::free(_scratch_buffer_blob);
   }
 }
 
+void PhaseOutput::perform_mach_node_analysis() {
+  // Late barrier analysis must be done after schedule and bundle
+  // Otherwise liveness based spilling will fail
+  BarrierSetC2* bs = BarrierSet::barrier_set()->barrier_set_c2();
+  bs->late_barrier_analysis();
+
+  pd_perform_mach_node_analysis();
+}
+
 // Convert Nodes to instruction bits and pass off to the VM
 void PhaseOutput::Output() {
   // RootNode goes
   assert( C->cfg()->get_root_block()->number_of_nodes() == 0, "" );
 
@@ -335,22 +344,22 @@
       }
     }
   }
 
   // Keeper of sizing aspects
-  BufferSizingData buf_sizes = BufferSizingData();
+  _buf_sizes = BufferSizingData();
 
   // Initialize code buffer
-  estimate_buffer_size(buf_sizes._const);
+  estimate_buffer_size(_buf_sizes._const);
   if (C->failing()) return;
 
   // Pre-compute the length of blocks and replace
   // long branches with short if machine supports it.
   // Must be done before ScheduleAndBundle due to SPARC delay slots
   uint* blk_starts = NEW_RESOURCE_ARRAY(uint, C->cfg()->number_of_blocks() + 1);
   blk_starts[0] = 0;
-  shorten_branches(blk_starts, buf_sizes);
+  shorten_branches(blk_starts);
 
   if (!C->is_osr_compilation() && C->has_scalarized_args()) {
     // Compute the offsets of the entry points required by the value type calling convention
     if (!C->method()->is_static()) {
       // We have entries at the beginning of the method, implemented by the first 4 nodes.
@@ -378,24 +387,14 @@
   ScheduleAndBundle();
   if (C->failing()) {
     return;
   }
 
-  // Late barrier analysis must be done after schedule and bundle
-  // Otherwise liveness based spilling will fail
-  BarrierSetC2* bs = BarrierSet::barrier_set()->barrier_set_c2();
-  bs->late_barrier_analysis();
-
-#ifdef X86
-  if (VM_Version::has_intel_jcc_erratum()) {
-    int extra_padding = IntelJccErratum::tag_affected_machnodes(C, C->cfg(), C->regalloc());
-    buf_sizes._code += extra_padding;
-  }
-#endif
+  perform_mach_node_analysis();
 
   // Complete sizing of codebuffer
-  CodeBuffer* cb = init_buffer(buf_sizes);
+  CodeBuffer* cb = init_buffer();
   if (cb == NULL || C->failing()) {
     return;
   }
 
   BuildOopMaps();
@@ -473,11 +472,11 @@
   } // if( MaxLoopPad < OptoLoopAlignment-1 )
 }
 
 // The architecture description provides short branch variants for some long
 // branch instructions. Replace eligible long branches with short branches.
-void PhaseOutput::shorten_branches(uint* blk_starts, BufferSizingData& buf_sizes) {
+void PhaseOutput::shorten_branches(uint* blk_starts) {
   // Compute size of each block, method size, and relocation information size
   uint nblocks  = C->cfg()->number_of_blocks();
 
   uint*      jmp_offset = NEW_RESOURCE_ARRAY(uint,nblocks);
   uint*      jmp_size   = NEW_RESOURCE_ARRAY(uint,nblocks);
@@ -508,10 +507,11 @@
   uint last_call_adr = max_juint;
   uint last_avoid_back_to_back_adr = max_juint;
   uint nop_size = (new MachNopNode())->size(C->regalloc());
   for (uint i = 0; i < nblocks; i++) { // For all blocks
     Block* block = C->cfg()->get_block(i);
+    _block = block;
 
     // During short branch replacement, we store the relative (to blk_starts)
     // offset of jump in jmp_offset, rather than the absolute offset of jump.
     // This is so that we do not need to recompute sizes of all nodes when
     // we compute correct blk_starts in our next sizing pass.
@@ -523,22 +523,16 @@
 
     // Sum all instruction sizes to compute block size
     uint last_inst = block->number_of_nodes();
     uint blk_size = 0;
     for (uint j = 0; j < last_inst; j++) {
-      Node* nj = block->get_node(j);
+      _index = j;
+      Node* nj = block->get_node(_index);
       // Handle machine instruction nodes
       if (nj->is_Mach()) {
-        MachNode *mach = nj->as_Mach();
+        MachNode* mach = nj->as_Mach();
         blk_size += (mach->alignment_required() - 1) * relocInfo::addr_unit(); // assume worst case padding
-#ifdef X86
-        if (VM_Version::has_intel_jcc_erratum() && IntelJccErratum::is_jcc_erratum_branch(block, mach, j)) {
-          // Conservatively add worst case padding
-          blk_size += IntelJccErratum::largest_jcc_size();
-        }
-#endif
-
         reloc_size += mach->reloc();
         if (mach->is_MachCall()) {
           // add size information for trampoline stub
           // class CallStubImpl is platform-specific and defined in the *.ad files.
           stub_size  += CallStubImpl::size_call_trampoline();
@@ -739,13 +733,13 @@
   // Min is 2 bytes, max is probably 6 or 8, with a tax up to 25% for
   // a relocation index.
   // The CodeBuffer will expand the locs array if this estimate is too low.
   reloc_size *= 10 / sizeof(relocInfo);
 
-  buf_sizes._reloc = reloc_size;
-  buf_sizes._code  = code_size;
-  buf_sizes._stub  = stub_size;
+  _buf_sizes._reloc = reloc_size;
+  _buf_sizes._code  = code_size;
+  _buf_sizes._stub  = stub_size;
 }
 
 //------------------------------FillLocArray-----------------------------------
 // Create a bit of debug info and append it to the array.  The mapping is from
 // Java local or expression stack to constant, register or stack-slot.  For
@@ -1289,15 +1283,14 @@
   // Initialize the space for the BufferBlob used to find and verify
   // instruction size in MachNode::emit_size()
   init_scratch_buffer_blob(const_req);
 }
 
-CodeBuffer* PhaseOutput::init_buffer(BufferSizingData& buf_sizes) {
-
-  int stub_req  = buf_sizes._stub;
-  int code_req  = buf_sizes._code;
-  int const_req = buf_sizes._const;
+CodeBuffer* PhaseOutput::init_buffer() {
+  int stub_req  = _buf_sizes._stub;
+  int code_req  = _buf_sizes._code;
+  int const_req = _buf_sizes._const;
 
   int pad_req   = NativeCall::instruction_size;
 
   BarrierSetC2* bs = BarrierSet::barrier_set()->barrier_set_c2();
   stub_req += bs->estimate_stub_size();
@@ -1322,11 +1315,11 @@
 
   if (C->has_method_handle_invokes())
     total_req += deopt_handler_req;  // deopt MH handler
 
   CodeBuffer* cb = code_buffer();
-  cb->initialize(total_req, buf_sizes._reloc);
+  cb->initialize(total_req, _buf_sizes._reloc);
 
   // Have we run out of code space?
   if ((cb->blob() == NULL) || (!CompileBroker::should_compile_new_jobs())) {
     C->record_failure("CodeCache is full");
     return NULL;
@@ -1411,10 +1404,11 @@
   // Now fill in the code buffer
   Node *delay_slot = NULL;
 
   for (uint i = 0; i < nblocks; i++) {
     Block* block = C->cfg()->get_block(i);
+    _block = block;
     Node* head = block->head();
 
     // If this block needs to start aligned (i.e, can be reached other
     // than by falling-thru from the previous block), then force the
     // start of a new bundle.
@@ -1441,10 +1435,11 @@
     uint last_inst = block->number_of_nodes();
 
     // Emit block normally, except for last instruction.
     // Emit means "dump code bits into code buffer".
     for (uint j = 0; j<last_inst; j++) {
+      _index = j;
 
       // Get the node
       Node* n = block->get_node(j);
 
       // See if delay slots are supported
@@ -1487,16 +1482,10 @@
         if (padding == 0 && mach->avoid_back_to_back(MachNode::AVOID_BEFORE) &&
             current_offset == last_avoid_back_to_back_offset) {
           // Avoid back to back some instructions.
           padding = nop_size;
         }
-#ifdef X86
-        if (mach->flags() & Node::Flag_intel_jcc_erratum) {
-          assert(padding == 0, "can't have contradicting padding requirements");
-          padding = IntelJccErratum::compute_padding(current_offset, mach, block, j, C->regalloc());
-        }
-#endif
 
         if (padding > 0) {
           assert((padding % nop_size) == 0, "padding is not a multiple of NOP size");
           int nops_cnt = padding / nop_size;
           MachNode *nop = new MachNopNode(nops_cnt);
