<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/share/opto/output.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="node.hpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="output.hpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/share/opto/output.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
  36 #include &quot;memory/allocation.inline.hpp&quot;
  37 #include &quot;opto/ad.hpp&quot;
  38 #include &quot;opto/block.hpp&quot;
  39 #include &quot;opto/c2compiler.hpp&quot;
  40 #include &quot;opto/callnode.hpp&quot;
  41 #include &quot;opto/cfgnode.hpp&quot;
  42 #include &quot;opto/locknode.hpp&quot;
  43 #include &quot;opto/machnode.hpp&quot;
  44 #include &quot;opto/node.hpp&quot;
  45 #include &quot;opto/optoreg.hpp&quot;
  46 #include &quot;opto/output.hpp&quot;
  47 #include &quot;opto/regalloc.hpp&quot;
  48 #include &quot;opto/runtime.hpp&quot;
  49 #include &quot;opto/subnode.hpp&quot;
  50 #include &quot;opto/type.hpp&quot;
  51 #include &quot;runtime/handles.inline.hpp&quot;
  52 #include &quot;runtime/sharedRuntime.hpp&quot;
  53 #include &quot;utilities/macros.hpp&quot;
  54 #include &quot;utilities/powerOfTwo.hpp&quot;
  55 #include &quot;utilities/xmlstream.hpp&quot;
<span class="line-removed">  56 #ifdef X86</span>
<span class="line-removed">  57 #include &quot;c2_intelJccErratum_x86.hpp&quot;</span>
<span class="line-removed">  58 #endif</span>
  59 
  60 #ifndef PRODUCT
  61 #define DEBUG_ARG(x) , x
  62 #else
  63 #define DEBUG_ARG(x)
  64 #endif
  65 
  66 //------------------------------Scheduling----------------------------------
  67 // This class contains all the information necessary to implement instruction
  68 // scheduling and bundling.
  69 class Scheduling {
  70 
  71 private:
  72   // Arena to use
  73   Arena *_arena;
  74 
  75   // Control-Flow Graph info
  76   PhaseCFG *_cfg;
  77 
  78   // Register Allocation info
</pre>
<hr />
<pre>
 228 
 229 
 230 PhaseOutput::PhaseOutput()
 231   : Phase(Phase::Output),
 232     _code_buffer(&quot;Compile::Fill_buffer&quot;),
 233     _first_block_size(0),
 234     _handler_table(),
 235     _inc_table(),
 236     _oop_map_set(NULL),
 237     _scratch_buffer_blob(NULL),
 238     _scratch_locs_memory(NULL),
 239     _scratch_const_size(-1),
 240     _in_scratch_emit_size(false),
 241     _frame_slots(0),
 242     _code_offsets(),
 243     _node_bundling_limit(0),
 244     _node_bundling_base(NULL),
 245     _orig_pc_slot(0),
 246     _orig_pc_slot_offset_in_bytes(0),
 247     _sp_inc_slot(0),
<span class="line-modified"> 248     _sp_inc_slot_offset_in_bytes(0) {</span>



 249   C-&gt;set_output(this);
 250   if (C-&gt;stub_name() == NULL) {
 251     int fixed_slots = C-&gt;fixed_slots();
 252     if (C-&gt;needs_stack_repair()) {
 253       fixed_slots -= 2;
 254       _sp_inc_slot = fixed_slots;
 255     }
 256     _orig_pc_slot = fixed_slots - (sizeof(address) / VMRegImpl::stack_slot_size);
 257   }
 258 }
 259 
 260 PhaseOutput::~PhaseOutput() {
 261   C-&gt;set_output(NULL);
 262   if (_scratch_buffer_blob != NULL) {
 263     BufferBlob::free(_scratch_buffer_blob);
 264   }
 265 }
 266 









 267 // Convert Nodes to instruction bits and pass off to the VM
 268 void PhaseOutput::Output() {
 269   // RootNode goes
 270   assert( C-&gt;cfg()-&gt;get_root_block()-&gt;number_of_nodes() == 0, &quot;&quot; );
 271 
 272   // The number of new nodes (mostly MachNop) is proportional to
 273   // the number of java calls and inner loops which are aligned.
 274   if ( C-&gt;check_node_count((NodeLimitFudgeFactor + C-&gt;java_calls()*3 +
 275                             C-&gt;inner_loops()*(OptoLoopAlignment-1)),
 276                            &quot;out of nodes before code generation&quot; ) ) {
 277     return;
 278   }
 279   // Make sure I can find the Start Node
 280   Block *entry = C-&gt;cfg()-&gt;get_block(1);
 281   Block *broot = C-&gt;cfg()-&gt;get_root_block();
 282 
 283   const StartNode *start = entry-&gt;head()-&gt;as_Start();
 284 
 285   // Replace StartNode with prolog
 286   Label verified_entry;
</pre>
<hr />
<pre>
 320       (OptoBreakpointC2R &amp;&amp; !C-&gt;method())                   ) {
 321     // checking for C-&gt;method() means that OptoBreakpoint does not apply to
 322     // runtime stubs or frame converters
 323     C-&gt;cfg()-&gt;insert( entry, 1, new MachBreakpointNode() );
 324   }
 325 
 326   // Insert epilogs before every return
 327   for (uint i = 0; i &lt; C-&gt;cfg()-&gt;number_of_blocks(); i++) {
 328     Block* block = C-&gt;cfg()-&gt;get_block(i);
 329     if (!block-&gt;is_connector() &amp;&amp; block-&gt;non_connector_successor(0) == C-&gt;cfg()-&gt;get_root_block()) { // Found a program exit point?
 330       Node* m = block-&gt;end();
 331       if (m-&gt;is_Mach() &amp;&amp; m-&gt;as_Mach()-&gt;ideal_Opcode() != Op_Halt) {
 332         MachEpilogNode* epilog = new MachEpilogNode(m-&gt;as_Mach()-&gt;ideal_Opcode() == Op_Return);
 333         block-&gt;add_inst(epilog);
 334         C-&gt;cfg()-&gt;map_node_to_block(epilog, block);
 335       }
 336     }
 337   }
 338 
 339   // Keeper of sizing aspects
<span class="line-modified"> 340   BufferSizingData buf_sizes = BufferSizingData();</span>
 341 
 342   // Initialize code buffer
<span class="line-modified"> 343   estimate_buffer_size(buf_sizes._const);</span>
 344   if (C-&gt;failing()) return;
 345 
 346   // Pre-compute the length of blocks and replace
 347   // long branches with short if machine supports it.
 348   // Must be done before ScheduleAndBundle due to SPARC delay slots
 349   uint* blk_starts = NEW_RESOURCE_ARRAY(uint, C-&gt;cfg()-&gt;number_of_blocks() + 1);
 350   blk_starts[0] = 0;
<span class="line-modified"> 351   shorten_branches(blk_starts, buf_sizes);</span>
 352 
 353   if (!C-&gt;is_osr_compilation() &amp;&amp; C-&gt;has_scalarized_args()) {
 354     // Compute the offsets of the entry points required by the value type calling convention
 355     if (!C-&gt;method()-&gt;is_static()) {
 356       // We have entries at the beginning of the method, implemented by the first 4 nodes.
 357       // Entry                     (unverified) @ offset 0
 358       // Verified_Value_Entry_RO
 359       // Value_Entry               (unverified)
 360       // Verified_Value_Entry
 361       uint offset = 0;
 362       _code_offsets.set_value(CodeOffsets::Entry, offset);
 363 
 364       offset += ((MachVEPNode*)broot-&gt;get_node(0))-&gt;size(C-&gt;regalloc());
 365       _code_offsets.set_value(CodeOffsets::Verified_Value_Entry_RO, offset);
 366 
 367       offset += ((MachVEPNode*)broot-&gt;get_node(1))-&gt;size(C-&gt;regalloc());
 368       _code_offsets.set_value(CodeOffsets::Value_Entry, offset);
 369 
 370       offset += ((MachVEPNode*)broot-&gt;get_node(2))-&gt;size(C-&gt;regalloc());
 371       _code_offsets.set_value(CodeOffsets::Verified_Value_Entry, offset);
 372     } else {
 373       _code_offsets.set_value(CodeOffsets::Entry, -1); // will be patched later
 374       _code_offsets.set_value(CodeOffsets::Verified_Value_Entry, 0);
 375     }
 376   }
 377 
 378   ScheduleAndBundle();
 379   if (C-&gt;failing()) {
 380     return;
 381   }
 382 
<span class="line-modified"> 383   // Late barrier analysis must be done after schedule and bundle</span>
<span class="line-removed"> 384   // Otherwise liveness based spilling will fail</span>
<span class="line-removed"> 385   BarrierSetC2* bs = BarrierSet::barrier_set()-&gt;barrier_set_c2();</span>
<span class="line-removed"> 386   bs-&gt;late_barrier_analysis();</span>
<span class="line-removed"> 387 </span>
<span class="line-removed"> 388 #ifdef X86</span>
<span class="line-removed"> 389   if (VM_Version::has_intel_jcc_erratum()) {</span>
<span class="line-removed"> 390     int extra_padding = IntelJccErratum::tag_affected_machnodes(C, C-&gt;cfg(), C-&gt;regalloc());</span>
<span class="line-removed"> 391     buf_sizes._code += extra_padding;</span>
<span class="line-removed"> 392   }</span>
<span class="line-removed"> 393 #endif</span>
 394 
 395   // Complete sizing of codebuffer
<span class="line-modified"> 396   CodeBuffer* cb = init_buffer(buf_sizes);</span>
 397   if (cb == NULL || C-&gt;failing()) {
 398     return;
 399   }
 400 
 401   BuildOopMaps();
 402 
 403   if (C-&gt;failing())  {
 404     return;
 405   }
 406 
 407   fill_buffer(cb, blk_starts);
 408 }
 409 
 410 bool PhaseOutput::need_stack_bang(int frame_size_in_bytes) const {
 411   // Determine if we need to generate a stack overflow check.
 412   // Do it if the method is not a stub function and
 413   // has java calls or has frame size &gt; vm_page_size/8.
 414   // The debug VM checks that deoptimization doesn&#39;t trigger an
 415   // unexpected stack overflow (compiled method stack banging should
 416   // guarantee it doesn&#39;t happen) so we always need the stack bang in
</pre>
<hr />
<pre>
 458         // Check subsequent fallthrough blocks if the loop&#39;s first
 459         // block(s) does not have enough instructions.
 460         Block *nb = block;
 461         while(inst_cnt &gt; 0 &amp;&amp;
 462               i &lt; last_block &amp;&amp;
 463               !C-&gt;cfg()-&gt;get_block(i + 1)-&gt;has_loop_alignment() &amp;&amp;
 464               !nb-&gt;has_successor(block)) {
 465           i++;
 466           nb = C-&gt;cfg()-&gt;get_block(i);
 467           inst_cnt  = nb-&gt;compute_first_inst_size(sum_size, inst_cnt, C-&gt;regalloc());
 468         } // while( inst_cnt &gt; 0 &amp;&amp; i &lt; last_block  )
 469 
 470         block-&gt;set_first_inst_size(sum_size);
 471       } // f( b-&gt;head()-&gt;is_Loop() )
 472     } // for( i &lt;= last_block )
 473   } // if( MaxLoopPad &lt; OptoLoopAlignment-1 )
 474 }
 475 
 476 // The architecture description provides short branch variants for some long
 477 // branch instructions. Replace eligible long branches with short branches.
<span class="line-modified"> 478 void PhaseOutput::shorten_branches(uint* blk_starts, BufferSizingData&amp; buf_sizes) {</span>
 479   // Compute size of each block, method size, and relocation information size
 480   uint nblocks  = C-&gt;cfg()-&gt;number_of_blocks();
 481 
 482   uint*      jmp_offset = NEW_RESOURCE_ARRAY(uint,nblocks);
 483   uint*      jmp_size   = NEW_RESOURCE_ARRAY(uint,nblocks);
 484   int*       jmp_nidx   = NEW_RESOURCE_ARRAY(int ,nblocks);
 485 
 486   // Collect worst case block paddings
 487   int* block_worst_case_pad = NEW_RESOURCE_ARRAY(int, nblocks);
 488   memset(block_worst_case_pad, 0, nblocks * sizeof(int));
 489 
 490   DEBUG_ONLY( uint *jmp_target = NEW_RESOURCE_ARRAY(uint,nblocks); )
 491   DEBUG_ONLY( uint *jmp_rule = NEW_RESOURCE_ARRAY(uint,nblocks); )
 492 
 493   bool has_short_branch_candidate = false;
 494 
 495   // Initialize the sizes to 0
 496   int code_size  = 0;          // Size in bytes of generated code
 497   int stub_size  = 0;          // Size in bytes of all stub entries
 498   // Size in bytes of all relocation entries, including those in local stubs.
 499   // Start with 2-bytes of reloc info for the unvalidated entry point
 500   int reloc_size = 1;          // Number of relocation entries
 501 
 502   // Make three passes.  The first computes pessimistic blk_starts,
 503   // relative jmp_offset and reloc_size information.  The second performs
 504   // short branch substitution using the pessimistic sizing.  The
 505   // third inserts nops where needed.
 506 
 507   // Step one, perform a pessimistic sizing pass.
 508   uint last_call_adr = max_juint;
 509   uint last_avoid_back_to_back_adr = max_juint;
 510   uint nop_size = (new MachNopNode())-&gt;size(C-&gt;regalloc());
 511   for (uint i = 0; i &lt; nblocks; i++) { // For all blocks
 512     Block* block = C-&gt;cfg()-&gt;get_block(i);

 513 
 514     // During short branch replacement, we store the relative (to blk_starts)
 515     // offset of jump in jmp_offset, rather than the absolute offset of jump.
 516     // This is so that we do not need to recompute sizes of all nodes when
 517     // we compute correct blk_starts in our next sizing pass.
 518     jmp_offset[i] = 0;
 519     jmp_size[i]   = 0;
 520     jmp_nidx[i]   = -1;
 521     DEBUG_ONLY( jmp_target[i] = 0; )
 522     DEBUG_ONLY( jmp_rule[i]   = 0; )
 523 
 524     // Sum all instruction sizes to compute block size
 525     uint last_inst = block-&gt;number_of_nodes();
 526     uint blk_size = 0;
 527     for (uint j = 0; j &lt; last_inst; j++) {
<span class="line-modified"> 528       Node* nj = block-&gt;get_node(j);</span>

 529       // Handle machine instruction nodes
 530       if (nj-&gt;is_Mach()) {
<span class="line-modified"> 531         MachNode *mach = nj-&gt;as_Mach();</span>
 532         blk_size += (mach-&gt;alignment_required() - 1) * relocInfo::addr_unit(); // assume worst case padding
<span class="line-removed"> 533 #ifdef X86</span>
<span class="line-removed"> 534         if (VM_Version::has_intel_jcc_erratum() &amp;&amp; IntelJccErratum::is_jcc_erratum_branch(block, mach, j)) {</span>
<span class="line-removed"> 535           // Conservatively add worst case padding</span>
<span class="line-removed"> 536           blk_size += IntelJccErratum::largest_jcc_size();</span>
<span class="line-removed"> 537         }</span>
<span class="line-removed"> 538 #endif</span>
<span class="line-removed"> 539 </span>
 540         reloc_size += mach-&gt;reloc();
 541         if (mach-&gt;is_MachCall()) {
 542           // add size information for trampoline stub
 543           // class CallStubImpl is platform-specific and defined in the *.ad files.
 544           stub_size  += CallStubImpl::size_call_trampoline();
 545           reloc_size += CallStubImpl::reloc_call_trampoline();
 546 
 547           MachCallNode *mcall = mach-&gt;as_MachCall();
 548           // This destination address is NOT PC-relative
 549 
 550           if (mcall-&gt;entry_point() != NULL) {
 551             mcall-&gt;method_set((intptr_t)mcall-&gt;entry_point());
 552           }
 553 
 554           if (mcall-&gt;is_MachCallJava() &amp;&amp; mcall-&gt;as_MachCallJava()-&gt;_method) {
 555             stub_size  += CompiledStaticCall::to_interp_stub_size();
 556             reloc_size += CompiledStaticCall::reloc_to_interp_stub();
 557 #if INCLUDE_AOT
 558             stub_size  += CompiledStaticCall::to_aot_stub_size();
 559             reloc_size += CompiledStaticCall::reloc_to_aot_stub();
</pre>
<hr />
<pre>
 724     }
 725   }
 726 #endif
 727 
 728   // Step 3, compute the offsets of all blocks, will be done in fill_buffer()
 729   // after ScheduleAndBundle().
 730 
 731   // ------------------
 732   // Compute size for code buffer
 733   code_size = blk_starts[nblocks];
 734 
 735   // Relocation records
 736   reloc_size += 1;              // Relo entry for exception handler
 737 
 738   // Adjust reloc_size to number of record of relocation info
 739   // Min is 2 bytes, max is probably 6 or 8, with a tax up to 25% for
 740   // a relocation index.
 741   // The CodeBuffer will expand the locs array if this estimate is too low.
 742   reloc_size *= 10 / sizeof(relocInfo);
 743 
<span class="line-modified"> 744   buf_sizes._reloc = reloc_size;</span>
<span class="line-modified"> 745   buf_sizes._code  = code_size;</span>
<span class="line-modified"> 746   buf_sizes._stub  = stub_size;</span>
 747 }
 748 
 749 //------------------------------FillLocArray-----------------------------------
 750 // Create a bit of debug info and append it to the array.  The mapping is from
 751 // Java local or expression stack to constant, register or stack-slot.  For
 752 // doubles, insert 2 mappings and return 1 (to tell the caller that the next
 753 // entry has been taken care of and caller should skip it).
 754 static LocationValue *new_loc_value( PhaseRegAlloc *ra, OptoReg::Name regnum, Location::Type l_type ) {
 755   // This should never have accepted Bad before
 756   assert(OptoReg::is_valid(regnum), &quot;location must be valid&quot;);
 757   return (OptoReg::is_reg(regnum))
 758          ? new LocationValue(Location::new_reg_loc(l_type, OptoReg::as_VMReg(regnum)) )
 759          : new LocationValue(Location::new_stk_loc(l_type,  ra-&gt;reg2offset(regnum)));
 760 }
 761 
 762 
 763 ObjectValue*
 764 PhaseOutput::sv_for_node_id(GrowableArray&lt;ScopeValue*&gt; *objs, int id) {
 765   for (int i = 0; i &lt; objs-&gt;length(); i++) {
 766     assert(objs-&gt;at(i)-&gt;is_object(), &quot;corrupt object cache&quot;);
</pre>
<hr />
<pre>
1274           MachConstantNode* machcon = n-&gt;as_MachConstant();
1275           machcon-&gt;eval_constant(C);
1276         } else if (n-&gt;is_Mach()) {
1277           // On Power there are more nodes that issue constants.
1278           add_size += (n-&gt;as_Mach()-&gt;ins_num_consts() * 8);
1279         }
1280       }
1281     }
1282 
1283     // Calculate the offsets of the constants and the size of the
1284     // constant table (including the padding to the next section).
1285     constant_table().calculate_offsets_and_size();
1286     const_req = constant_table().size() + add_size;
1287   }
1288 
1289   // Initialize the space for the BufferBlob used to find and verify
1290   // instruction size in MachNode::emit_size()
1291   init_scratch_buffer_blob(const_req);
1292 }
1293 
<span class="line-modified">1294 CodeBuffer* PhaseOutput::init_buffer(BufferSizingData&amp; buf_sizes) {</span>
<span class="line-modified">1295 </span>
<span class="line-modified">1296   int stub_req  = buf_sizes._stub;</span>
<span class="line-modified">1297   int code_req  = buf_sizes._code;</span>
<span class="line-removed">1298   int const_req = buf_sizes._const;</span>
1299 
1300   int pad_req   = NativeCall::instruction_size;
1301 
1302   BarrierSetC2* bs = BarrierSet::barrier_set()-&gt;barrier_set_c2();
1303   stub_req += bs-&gt;estimate_stub_size();
1304 
1305   // nmethod and CodeBuffer count stubs &amp; constants as part of method&#39;s code.
1306   // class HandlerImpl is platform-specific and defined in the *.ad files.
1307   int exception_handler_req = HandlerImpl::size_exception_handler() + MAX_stubs_size; // add marginal slop for handler
1308   int deopt_handler_req     = HandlerImpl::size_deopt_handler()     + MAX_stubs_size; // add marginal slop for handler
1309   stub_req += MAX_stubs_size;   // ensure per-stub margin
1310   code_req += MAX_inst_size;    // ensure per-instruction margin
1311 
1312   if (StressCodeBuffers)
1313     code_req = const_req = stub_req = exception_handler_req = deopt_handler_req = 0x10;  // force expansion
1314 
1315   int total_req =
1316           const_req +
1317           code_req +
1318           pad_req +
1319           stub_req +
1320           exception_handler_req +
1321           deopt_handler_req;               // deopt handler
1322 
1323   if (C-&gt;has_method_handle_invokes())
1324     total_req += deopt_handler_req;  // deopt MH handler
1325 
1326   CodeBuffer* cb = code_buffer();
<span class="line-modified">1327   cb-&gt;initialize(total_req, buf_sizes._reloc);</span>
1328 
1329   // Have we run out of code space?
1330   if ((cb-&gt;blob() == NULL) || (!CompileBroker::should_compile_new_jobs())) {
1331     C-&gt;record_failure(&quot;CodeCache is full&quot;);
1332     return NULL;
1333   }
1334   // Configure the code buffer.
1335   cb-&gt;initialize_consts_size(const_req);
1336   cb-&gt;initialize_stubs_size(stub_req);
1337   cb-&gt;initialize_oop_recorder(C-&gt;env()-&gt;oop_recorder());
1338 
1339   // fill in the nop array for bundling computations
1340   MachNode *_nop_list[Bundle::_nop_count];
1341   Bundle::initialize_nops(_nop_list);
1342 
1343   return cb;
1344 }
1345 
1346 //------------------------------fill_buffer------------------------------------
1347 void PhaseOutput::fill_buffer(CodeBuffer* cb, uint* blk_starts) {
</pre>
<hr />
<pre>
1396 
1397   NonSafepointEmitter non_safepoints(C);  // emit non-safepoints lazily
1398 
1399   // Emit the constant table.
1400   if (C-&gt;has_mach_constant_base_node()) {
1401     constant_table().emit(*cb);
1402   }
1403 
1404   // Create an array of labels, one for each basic block
1405   Label *blk_labels = NEW_RESOURCE_ARRAY(Label, nblocks+1);
1406   for (uint i=0; i &lt;= nblocks; i++) {
1407     blk_labels[i].init();
1408   }
1409 
1410   // ------------------
1411   // Now fill in the code buffer
1412   Node *delay_slot = NULL;
1413 
1414   for (uint i = 0; i &lt; nblocks; i++) {
1415     Block* block = C-&gt;cfg()-&gt;get_block(i);

1416     Node* head = block-&gt;head();
1417 
1418     // If this block needs to start aligned (i.e, can be reached other
1419     // than by falling-thru from the previous block), then force the
1420     // start of a new bundle.
1421     if (Pipeline::requires_bundling() &amp;&amp; starts_bundle(head)) {
1422       cb-&gt;flush_bundle(true);
1423     }
1424 
1425 #ifdef ASSERT
1426     if (!block-&gt;is_connector()) {
1427       stringStream st;
1428       block-&gt;dump_head(C-&gt;cfg(), &amp;st);
1429       MacroAssembler(cb).block_comment(st.as_string());
1430     }
1431     jmp_target[i] = 0;
1432     jmp_offset[i] = 0;
1433     jmp_size[i]   = 0;
1434     jmp_rule[i]   = 0;
1435 #endif
1436     int blk_offset = current_offset;
1437 
1438     // Define the label at the beginning of the basic block
1439     MacroAssembler(cb).bind(blk_labels[block-&gt;_pre_order]);
1440 
1441     uint last_inst = block-&gt;number_of_nodes();
1442 
1443     // Emit block normally, except for last instruction.
1444     // Emit means &quot;dump code bits into code buffer&quot;.
1445     for (uint j = 0; j&lt;last_inst; j++) {

1446 
1447       // Get the node
1448       Node* n = block-&gt;get_node(j);
1449 
1450       // See if delay slots are supported
1451       if (valid_bundle_info(n) &amp;&amp; node_bundling(n)-&gt;used_in_unconditional_delay()) {
1452         assert(delay_slot == NULL, &quot;no use of delay slot node&quot;);
1453         assert(n-&gt;size(C-&gt;regalloc()) == Pipeline::instr_unit_size(), &quot;delay slot instruction wrong size&quot;);
1454 
1455         delay_slot = n;
1456         continue;
1457       }
1458 
1459       // If this starts a new instruction group, then flush the current one
1460       // (but allow split bundles)
1461       if (Pipeline::requires_bundling() &amp;&amp; starts_bundle(n))
1462         cb-&gt;flush_bundle(false);
1463 
1464       // Special handling for SafePoint/Call Nodes
1465       bool is_mcall = false;
</pre>
<hr />
<pre>
1472         if (is_sfn || is_mcall || mach-&gt;alignment_required() != 1) {
1473           cb-&gt;flush_bundle(true);
1474           current_offset = cb-&gt;insts_size();
1475         }
1476 
1477         // A padding may be needed again since a previous instruction
1478         // could be moved to delay slot.
1479 
1480         // align the instruction if necessary
1481         int padding = mach-&gt;compute_padding(current_offset);
1482         // Make sure safepoint node for polling is distinct from a call&#39;s
1483         // return by adding a nop if needed.
1484         if (is_sfn &amp;&amp; !is_mcall &amp;&amp; padding == 0 &amp;&amp; current_offset == last_call_offset) {
1485           padding = nop_size;
1486         }
1487         if (padding == 0 &amp;&amp; mach-&gt;avoid_back_to_back(MachNode::AVOID_BEFORE) &amp;&amp;
1488             current_offset == last_avoid_back_to_back_offset) {
1489           // Avoid back to back some instructions.
1490           padding = nop_size;
1491         }
<span class="line-removed">1492 #ifdef X86</span>
<span class="line-removed">1493         if (mach-&gt;flags() &amp; Node::Flag_intel_jcc_erratum) {</span>
<span class="line-removed">1494           assert(padding == 0, &quot;can&#39;t have contradicting padding requirements&quot;);</span>
<span class="line-removed">1495           padding = IntelJccErratum::compute_padding(current_offset, mach, block, j, C-&gt;regalloc());</span>
<span class="line-removed">1496         }</span>
<span class="line-removed">1497 #endif</span>
1498 
1499         if (padding &gt; 0) {
1500           assert((padding % nop_size) == 0, &quot;padding is not a multiple of NOP size&quot;);
1501           int nops_cnt = padding / nop_size;
1502           MachNode *nop = new MachNopNode(nops_cnt);
1503           block-&gt;insert_node(nop, j++);
1504           last_inst++;
1505           C-&gt;cfg()-&gt;map_node_to_block(nop, block);
1506           // Ensure enough space.
1507           cb-&gt;insts()-&gt;maybe_expand_to_ensure_remaining(MAX_inst_size);
1508           if ((cb-&gt;blob() == NULL) || (!CompileBroker::should_compile_new_jobs())) {
1509             C-&gt;record_failure(&quot;CodeCache is full&quot;);
1510             return;
1511           }
1512           nop-&gt;emit(*cb, C-&gt;regalloc());
1513           cb-&gt;flush_bundle(true);
1514           current_offset = cb-&gt;insts_size();
1515         }
1516 
1517         // Remember the start of the last call in a basic block
</pre>
</td>
<td>
<hr />
<pre>
  36 #include &quot;memory/allocation.inline.hpp&quot;
  37 #include &quot;opto/ad.hpp&quot;
  38 #include &quot;opto/block.hpp&quot;
  39 #include &quot;opto/c2compiler.hpp&quot;
  40 #include &quot;opto/callnode.hpp&quot;
  41 #include &quot;opto/cfgnode.hpp&quot;
  42 #include &quot;opto/locknode.hpp&quot;
  43 #include &quot;opto/machnode.hpp&quot;
  44 #include &quot;opto/node.hpp&quot;
  45 #include &quot;opto/optoreg.hpp&quot;
  46 #include &quot;opto/output.hpp&quot;
  47 #include &quot;opto/regalloc.hpp&quot;
  48 #include &quot;opto/runtime.hpp&quot;
  49 #include &quot;opto/subnode.hpp&quot;
  50 #include &quot;opto/type.hpp&quot;
  51 #include &quot;runtime/handles.inline.hpp&quot;
  52 #include &quot;runtime/sharedRuntime.hpp&quot;
  53 #include &quot;utilities/macros.hpp&quot;
  54 #include &quot;utilities/powerOfTwo.hpp&quot;
  55 #include &quot;utilities/xmlstream.hpp&quot;



  56 
  57 #ifndef PRODUCT
  58 #define DEBUG_ARG(x) , x
  59 #else
  60 #define DEBUG_ARG(x)
  61 #endif
  62 
  63 //------------------------------Scheduling----------------------------------
  64 // This class contains all the information necessary to implement instruction
  65 // scheduling and bundling.
  66 class Scheduling {
  67 
  68 private:
  69   // Arena to use
  70   Arena *_arena;
  71 
  72   // Control-Flow Graph info
  73   PhaseCFG *_cfg;
  74 
  75   // Register Allocation info
</pre>
<hr />
<pre>
 225 
 226 
 227 PhaseOutput::PhaseOutput()
 228   : Phase(Phase::Output),
 229     _code_buffer(&quot;Compile::Fill_buffer&quot;),
 230     _first_block_size(0),
 231     _handler_table(),
 232     _inc_table(),
 233     _oop_map_set(NULL),
 234     _scratch_buffer_blob(NULL),
 235     _scratch_locs_memory(NULL),
 236     _scratch_const_size(-1),
 237     _in_scratch_emit_size(false),
 238     _frame_slots(0),
 239     _code_offsets(),
 240     _node_bundling_limit(0),
 241     _node_bundling_base(NULL),
 242     _orig_pc_slot(0),
 243     _orig_pc_slot_offset_in_bytes(0),
 244     _sp_inc_slot(0),
<span class="line-modified"> 245     _sp_inc_slot_offset_in_bytes(0),</span>
<span class="line-added"> 246     _buf_sizes(),</span>
<span class="line-added"> 247     _block(NULL),</span>
<span class="line-added"> 248     _index(0) {</span>
 249   C-&gt;set_output(this);
 250   if (C-&gt;stub_name() == NULL) {
 251     int fixed_slots = C-&gt;fixed_slots();
 252     if (C-&gt;needs_stack_repair()) {
 253       fixed_slots -= 2;
 254       _sp_inc_slot = fixed_slots;
 255     }
 256     _orig_pc_slot = fixed_slots - (sizeof(address) / VMRegImpl::stack_slot_size);
 257   }
 258 }
 259 
 260 PhaseOutput::~PhaseOutput() {
 261   C-&gt;set_output(NULL);
 262   if (_scratch_buffer_blob != NULL) {
 263     BufferBlob::free(_scratch_buffer_blob);
 264   }
 265 }
 266 
<span class="line-added"> 267 void PhaseOutput::perform_mach_node_analysis() {</span>
<span class="line-added"> 268   // Late barrier analysis must be done after schedule and bundle</span>
<span class="line-added"> 269   // Otherwise liveness based spilling will fail</span>
<span class="line-added"> 270   BarrierSetC2* bs = BarrierSet::barrier_set()-&gt;barrier_set_c2();</span>
<span class="line-added"> 271   bs-&gt;late_barrier_analysis();</span>
<span class="line-added"> 272 </span>
<span class="line-added"> 273   pd_perform_mach_node_analysis();</span>
<span class="line-added"> 274 }</span>
<span class="line-added"> 275 </span>
 276 // Convert Nodes to instruction bits and pass off to the VM
 277 void PhaseOutput::Output() {
 278   // RootNode goes
 279   assert( C-&gt;cfg()-&gt;get_root_block()-&gt;number_of_nodes() == 0, &quot;&quot; );
 280 
 281   // The number of new nodes (mostly MachNop) is proportional to
 282   // the number of java calls and inner loops which are aligned.
 283   if ( C-&gt;check_node_count((NodeLimitFudgeFactor + C-&gt;java_calls()*3 +
 284                             C-&gt;inner_loops()*(OptoLoopAlignment-1)),
 285                            &quot;out of nodes before code generation&quot; ) ) {
 286     return;
 287   }
 288   // Make sure I can find the Start Node
 289   Block *entry = C-&gt;cfg()-&gt;get_block(1);
 290   Block *broot = C-&gt;cfg()-&gt;get_root_block();
 291 
 292   const StartNode *start = entry-&gt;head()-&gt;as_Start();
 293 
 294   // Replace StartNode with prolog
 295   Label verified_entry;
</pre>
<hr />
<pre>
 329       (OptoBreakpointC2R &amp;&amp; !C-&gt;method())                   ) {
 330     // checking for C-&gt;method() means that OptoBreakpoint does not apply to
 331     // runtime stubs or frame converters
 332     C-&gt;cfg()-&gt;insert( entry, 1, new MachBreakpointNode() );
 333   }
 334 
 335   // Insert epilogs before every return
 336   for (uint i = 0; i &lt; C-&gt;cfg()-&gt;number_of_blocks(); i++) {
 337     Block* block = C-&gt;cfg()-&gt;get_block(i);
 338     if (!block-&gt;is_connector() &amp;&amp; block-&gt;non_connector_successor(0) == C-&gt;cfg()-&gt;get_root_block()) { // Found a program exit point?
 339       Node* m = block-&gt;end();
 340       if (m-&gt;is_Mach() &amp;&amp; m-&gt;as_Mach()-&gt;ideal_Opcode() != Op_Halt) {
 341         MachEpilogNode* epilog = new MachEpilogNode(m-&gt;as_Mach()-&gt;ideal_Opcode() == Op_Return);
 342         block-&gt;add_inst(epilog);
 343         C-&gt;cfg()-&gt;map_node_to_block(epilog, block);
 344       }
 345     }
 346   }
 347 
 348   // Keeper of sizing aspects
<span class="line-modified"> 349   _buf_sizes = BufferSizingData();</span>
 350 
 351   // Initialize code buffer
<span class="line-modified"> 352   estimate_buffer_size(_buf_sizes._const);</span>
 353   if (C-&gt;failing()) return;
 354 
 355   // Pre-compute the length of blocks and replace
 356   // long branches with short if machine supports it.
 357   // Must be done before ScheduleAndBundle due to SPARC delay slots
 358   uint* blk_starts = NEW_RESOURCE_ARRAY(uint, C-&gt;cfg()-&gt;number_of_blocks() + 1);
 359   blk_starts[0] = 0;
<span class="line-modified"> 360   shorten_branches(blk_starts);</span>
 361 
 362   if (!C-&gt;is_osr_compilation() &amp;&amp; C-&gt;has_scalarized_args()) {
 363     // Compute the offsets of the entry points required by the value type calling convention
 364     if (!C-&gt;method()-&gt;is_static()) {
 365       // We have entries at the beginning of the method, implemented by the first 4 nodes.
 366       // Entry                     (unverified) @ offset 0
 367       // Verified_Value_Entry_RO
 368       // Value_Entry               (unverified)
 369       // Verified_Value_Entry
 370       uint offset = 0;
 371       _code_offsets.set_value(CodeOffsets::Entry, offset);
 372 
 373       offset += ((MachVEPNode*)broot-&gt;get_node(0))-&gt;size(C-&gt;regalloc());
 374       _code_offsets.set_value(CodeOffsets::Verified_Value_Entry_RO, offset);
 375 
 376       offset += ((MachVEPNode*)broot-&gt;get_node(1))-&gt;size(C-&gt;regalloc());
 377       _code_offsets.set_value(CodeOffsets::Value_Entry, offset);
 378 
 379       offset += ((MachVEPNode*)broot-&gt;get_node(2))-&gt;size(C-&gt;regalloc());
 380       _code_offsets.set_value(CodeOffsets::Verified_Value_Entry, offset);
 381     } else {
 382       _code_offsets.set_value(CodeOffsets::Entry, -1); // will be patched later
 383       _code_offsets.set_value(CodeOffsets::Verified_Value_Entry, 0);
 384     }
 385   }
 386 
 387   ScheduleAndBundle();
 388   if (C-&gt;failing()) {
 389     return;
 390   }
 391 
<span class="line-modified"> 392   perform_mach_node_analysis();</span>










 393 
 394   // Complete sizing of codebuffer
<span class="line-modified"> 395   CodeBuffer* cb = init_buffer();</span>
 396   if (cb == NULL || C-&gt;failing()) {
 397     return;
 398   }
 399 
 400   BuildOopMaps();
 401 
 402   if (C-&gt;failing())  {
 403     return;
 404   }
 405 
 406   fill_buffer(cb, blk_starts);
 407 }
 408 
 409 bool PhaseOutput::need_stack_bang(int frame_size_in_bytes) const {
 410   // Determine if we need to generate a stack overflow check.
 411   // Do it if the method is not a stub function and
 412   // has java calls or has frame size &gt; vm_page_size/8.
 413   // The debug VM checks that deoptimization doesn&#39;t trigger an
 414   // unexpected stack overflow (compiled method stack banging should
 415   // guarantee it doesn&#39;t happen) so we always need the stack bang in
</pre>
<hr />
<pre>
 457         // Check subsequent fallthrough blocks if the loop&#39;s first
 458         // block(s) does not have enough instructions.
 459         Block *nb = block;
 460         while(inst_cnt &gt; 0 &amp;&amp;
 461               i &lt; last_block &amp;&amp;
 462               !C-&gt;cfg()-&gt;get_block(i + 1)-&gt;has_loop_alignment() &amp;&amp;
 463               !nb-&gt;has_successor(block)) {
 464           i++;
 465           nb = C-&gt;cfg()-&gt;get_block(i);
 466           inst_cnt  = nb-&gt;compute_first_inst_size(sum_size, inst_cnt, C-&gt;regalloc());
 467         } // while( inst_cnt &gt; 0 &amp;&amp; i &lt; last_block  )
 468 
 469         block-&gt;set_first_inst_size(sum_size);
 470       } // f( b-&gt;head()-&gt;is_Loop() )
 471     } // for( i &lt;= last_block )
 472   } // if( MaxLoopPad &lt; OptoLoopAlignment-1 )
 473 }
 474 
 475 // The architecture description provides short branch variants for some long
 476 // branch instructions. Replace eligible long branches with short branches.
<span class="line-modified"> 477 void PhaseOutput::shorten_branches(uint* blk_starts) {</span>
 478   // Compute size of each block, method size, and relocation information size
 479   uint nblocks  = C-&gt;cfg()-&gt;number_of_blocks();
 480 
 481   uint*      jmp_offset = NEW_RESOURCE_ARRAY(uint,nblocks);
 482   uint*      jmp_size   = NEW_RESOURCE_ARRAY(uint,nblocks);
 483   int*       jmp_nidx   = NEW_RESOURCE_ARRAY(int ,nblocks);
 484 
 485   // Collect worst case block paddings
 486   int* block_worst_case_pad = NEW_RESOURCE_ARRAY(int, nblocks);
 487   memset(block_worst_case_pad, 0, nblocks * sizeof(int));
 488 
 489   DEBUG_ONLY( uint *jmp_target = NEW_RESOURCE_ARRAY(uint,nblocks); )
 490   DEBUG_ONLY( uint *jmp_rule = NEW_RESOURCE_ARRAY(uint,nblocks); )
 491 
 492   bool has_short_branch_candidate = false;
 493 
 494   // Initialize the sizes to 0
 495   int code_size  = 0;          // Size in bytes of generated code
 496   int stub_size  = 0;          // Size in bytes of all stub entries
 497   // Size in bytes of all relocation entries, including those in local stubs.
 498   // Start with 2-bytes of reloc info for the unvalidated entry point
 499   int reloc_size = 1;          // Number of relocation entries
 500 
 501   // Make three passes.  The first computes pessimistic blk_starts,
 502   // relative jmp_offset and reloc_size information.  The second performs
 503   // short branch substitution using the pessimistic sizing.  The
 504   // third inserts nops where needed.
 505 
 506   // Step one, perform a pessimistic sizing pass.
 507   uint last_call_adr = max_juint;
 508   uint last_avoid_back_to_back_adr = max_juint;
 509   uint nop_size = (new MachNopNode())-&gt;size(C-&gt;regalloc());
 510   for (uint i = 0; i &lt; nblocks; i++) { // For all blocks
 511     Block* block = C-&gt;cfg()-&gt;get_block(i);
<span class="line-added"> 512     _block = block;</span>
 513 
 514     // During short branch replacement, we store the relative (to blk_starts)
 515     // offset of jump in jmp_offset, rather than the absolute offset of jump.
 516     // This is so that we do not need to recompute sizes of all nodes when
 517     // we compute correct blk_starts in our next sizing pass.
 518     jmp_offset[i] = 0;
 519     jmp_size[i]   = 0;
 520     jmp_nidx[i]   = -1;
 521     DEBUG_ONLY( jmp_target[i] = 0; )
 522     DEBUG_ONLY( jmp_rule[i]   = 0; )
 523 
 524     // Sum all instruction sizes to compute block size
 525     uint last_inst = block-&gt;number_of_nodes();
 526     uint blk_size = 0;
 527     for (uint j = 0; j &lt; last_inst; j++) {
<span class="line-modified"> 528       _index = j;</span>
<span class="line-added"> 529       Node* nj = block-&gt;get_node(_index);</span>
 530       // Handle machine instruction nodes
 531       if (nj-&gt;is_Mach()) {
<span class="line-modified"> 532         MachNode* mach = nj-&gt;as_Mach();</span>
 533         blk_size += (mach-&gt;alignment_required() - 1) * relocInfo::addr_unit(); // assume worst case padding







 534         reloc_size += mach-&gt;reloc();
 535         if (mach-&gt;is_MachCall()) {
 536           // add size information for trampoline stub
 537           // class CallStubImpl is platform-specific and defined in the *.ad files.
 538           stub_size  += CallStubImpl::size_call_trampoline();
 539           reloc_size += CallStubImpl::reloc_call_trampoline();
 540 
 541           MachCallNode *mcall = mach-&gt;as_MachCall();
 542           // This destination address is NOT PC-relative
 543 
 544           if (mcall-&gt;entry_point() != NULL) {
 545             mcall-&gt;method_set((intptr_t)mcall-&gt;entry_point());
 546           }
 547 
 548           if (mcall-&gt;is_MachCallJava() &amp;&amp; mcall-&gt;as_MachCallJava()-&gt;_method) {
 549             stub_size  += CompiledStaticCall::to_interp_stub_size();
 550             reloc_size += CompiledStaticCall::reloc_to_interp_stub();
 551 #if INCLUDE_AOT
 552             stub_size  += CompiledStaticCall::to_aot_stub_size();
 553             reloc_size += CompiledStaticCall::reloc_to_aot_stub();
</pre>
<hr />
<pre>
 718     }
 719   }
 720 #endif
 721 
 722   // Step 3, compute the offsets of all blocks, will be done in fill_buffer()
 723   // after ScheduleAndBundle().
 724 
 725   // ------------------
 726   // Compute size for code buffer
 727   code_size = blk_starts[nblocks];
 728 
 729   // Relocation records
 730   reloc_size += 1;              // Relo entry for exception handler
 731 
 732   // Adjust reloc_size to number of record of relocation info
 733   // Min is 2 bytes, max is probably 6 or 8, with a tax up to 25% for
 734   // a relocation index.
 735   // The CodeBuffer will expand the locs array if this estimate is too low.
 736   reloc_size *= 10 / sizeof(relocInfo);
 737 
<span class="line-modified"> 738   _buf_sizes._reloc = reloc_size;</span>
<span class="line-modified"> 739   _buf_sizes._code  = code_size;</span>
<span class="line-modified"> 740   _buf_sizes._stub  = stub_size;</span>
 741 }
 742 
 743 //------------------------------FillLocArray-----------------------------------
 744 // Create a bit of debug info and append it to the array.  The mapping is from
 745 // Java local or expression stack to constant, register or stack-slot.  For
 746 // doubles, insert 2 mappings and return 1 (to tell the caller that the next
 747 // entry has been taken care of and caller should skip it).
 748 static LocationValue *new_loc_value( PhaseRegAlloc *ra, OptoReg::Name regnum, Location::Type l_type ) {
 749   // This should never have accepted Bad before
 750   assert(OptoReg::is_valid(regnum), &quot;location must be valid&quot;);
 751   return (OptoReg::is_reg(regnum))
 752          ? new LocationValue(Location::new_reg_loc(l_type, OptoReg::as_VMReg(regnum)) )
 753          : new LocationValue(Location::new_stk_loc(l_type,  ra-&gt;reg2offset(regnum)));
 754 }
 755 
 756 
 757 ObjectValue*
 758 PhaseOutput::sv_for_node_id(GrowableArray&lt;ScopeValue*&gt; *objs, int id) {
 759   for (int i = 0; i &lt; objs-&gt;length(); i++) {
 760     assert(objs-&gt;at(i)-&gt;is_object(), &quot;corrupt object cache&quot;);
</pre>
<hr />
<pre>
1268           MachConstantNode* machcon = n-&gt;as_MachConstant();
1269           machcon-&gt;eval_constant(C);
1270         } else if (n-&gt;is_Mach()) {
1271           // On Power there are more nodes that issue constants.
1272           add_size += (n-&gt;as_Mach()-&gt;ins_num_consts() * 8);
1273         }
1274       }
1275     }
1276 
1277     // Calculate the offsets of the constants and the size of the
1278     // constant table (including the padding to the next section).
1279     constant_table().calculate_offsets_and_size();
1280     const_req = constant_table().size() + add_size;
1281   }
1282 
1283   // Initialize the space for the BufferBlob used to find and verify
1284   // instruction size in MachNode::emit_size()
1285   init_scratch_buffer_blob(const_req);
1286 }
1287 
<span class="line-modified">1288 CodeBuffer* PhaseOutput::init_buffer() {</span>
<span class="line-modified">1289   int stub_req  = _buf_sizes._stub;</span>
<span class="line-modified">1290   int code_req  = _buf_sizes._code;</span>
<span class="line-modified">1291   int const_req = _buf_sizes._const;</span>

1292 
1293   int pad_req   = NativeCall::instruction_size;
1294 
1295   BarrierSetC2* bs = BarrierSet::barrier_set()-&gt;barrier_set_c2();
1296   stub_req += bs-&gt;estimate_stub_size();
1297 
1298   // nmethod and CodeBuffer count stubs &amp; constants as part of method&#39;s code.
1299   // class HandlerImpl is platform-specific and defined in the *.ad files.
1300   int exception_handler_req = HandlerImpl::size_exception_handler() + MAX_stubs_size; // add marginal slop for handler
1301   int deopt_handler_req     = HandlerImpl::size_deopt_handler()     + MAX_stubs_size; // add marginal slop for handler
1302   stub_req += MAX_stubs_size;   // ensure per-stub margin
1303   code_req += MAX_inst_size;    // ensure per-instruction margin
1304 
1305   if (StressCodeBuffers)
1306     code_req = const_req = stub_req = exception_handler_req = deopt_handler_req = 0x10;  // force expansion
1307 
1308   int total_req =
1309           const_req +
1310           code_req +
1311           pad_req +
1312           stub_req +
1313           exception_handler_req +
1314           deopt_handler_req;               // deopt handler
1315 
1316   if (C-&gt;has_method_handle_invokes())
1317     total_req += deopt_handler_req;  // deopt MH handler
1318 
1319   CodeBuffer* cb = code_buffer();
<span class="line-modified">1320   cb-&gt;initialize(total_req, _buf_sizes._reloc);</span>
1321 
1322   // Have we run out of code space?
1323   if ((cb-&gt;blob() == NULL) || (!CompileBroker::should_compile_new_jobs())) {
1324     C-&gt;record_failure(&quot;CodeCache is full&quot;);
1325     return NULL;
1326   }
1327   // Configure the code buffer.
1328   cb-&gt;initialize_consts_size(const_req);
1329   cb-&gt;initialize_stubs_size(stub_req);
1330   cb-&gt;initialize_oop_recorder(C-&gt;env()-&gt;oop_recorder());
1331 
1332   // fill in the nop array for bundling computations
1333   MachNode *_nop_list[Bundle::_nop_count];
1334   Bundle::initialize_nops(_nop_list);
1335 
1336   return cb;
1337 }
1338 
1339 //------------------------------fill_buffer------------------------------------
1340 void PhaseOutput::fill_buffer(CodeBuffer* cb, uint* blk_starts) {
</pre>
<hr />
<pre>
1389 
1390   NonSafepointEmitter non_safepoints(C);  // emit non-safepoints lazily
1391 
1392   // Emit the constant table.
1393   if (C-&gt;has_mach_constant_base_node()) {
1394     constant_table().emit(*cb);
1395   }
1396 
1397   // Create an array of labels, one for each basic block
1398   Label *blk_labels = NEW_RESOURCE_ARRAY(Label, nblocks+1);
1399   for (uint i=0; i &lt;= nblocks; i++) {
1400     blk_labels[i].init();
1401   }
1402 
1403   // ------------------
1404   // Now fill in the code buffer
1405   Node *delay_slot = NULL;
1406 
1407   for (uint i = 0; i &lt; nblocks; i++) {
1408     Block* block = C-&gt;cfg()-&gt;get_block(i);
<span class="line-added">1409     _block = block;</span>
1410     Node* head = block-&gt;head();
1411 
1412     // If this block needs to start aligned (i.e, can be reached other
1413     // than by falling-thru from the previous block), then force the
1414     // start of a new bundle.
1415     if (Pipeline::requires_bundling() &amp;&amp; starts_bundle(head)) {
1416       cb-&gt;flush_bundle(true);
1417     }
1418 
1419 #ifdef ASSERT
1420     if (!block-&gt;is_connector()) {
1421       stringStream st;
1422       block-&gt;dump_head(C-&gt;cfg(), &amp;st);
1423       MacroAssembler(cb).block_comment(st.as_string());
1424     }
1425     jmp_target[i] = 0;
1426     jmp_offset[i] = 0;
1427     jmp_size[i]   = 0;
1428     jmp_rule[i]   = 0;
1429 #endif
1430     int blk_offset = current_offset;
1431 
1432     // Define the label at the beginning of the basic block
1433     MacroAssembler(cb).bind(blk_labels[block-&gt;_pre_order]);
1434 
1435     uint last_inst = block-&gt;number_of_nodes();
1436 
1437     // Emit block normally, except for last instruction.
1438     // Emit means &quot;dump code bits into code buffer&quot;.
1439     for (uint j = 0; j&lt;last_inst; j++) {
<span class="line-added">1440       _index = j;</span>
1441 
1442       // Get the node
1443       Node* n = block-&gt;get_node(j);
1444 
1445       // See if delay slots are supported
1446       if (valid_bundle_info(n) &amp;&amp; node_bundling(n)-&gt;used_in_unconditional_delay()) {
1447         assert(delay_slot == NULL, &quot;no use of delay slot node&quot;);
1448         assert(n-&gt;size(C-&gt;regalloc()) == Pipeline::instr_unit_size(), &quot;delay slot instruction wrong size&quot;);
1449 
1450         delay_slot = n;
1451         continue;
1452       }
1453 
1454       // If this starts a new instruction group, then flush the current one
1455       // (but allow split bundles)
1456       if (Pipeline::requires_bundling() &amp;&amp; starts_bundle(n))
1457         cb-&gt;flush_bundle(false);
1458 
1459       // Special handling for SafePoint/Call Nodes
1460       bool is_mcall = false;
</pre>
<hr />
<pre>
1467         if (is_sfn || is_mcall || mach-&gt;alignment_required() != 1) {
1468           cb-&gt;flush_bundle(true);
1469           current_offset = cb-&gt;insts_size();
1470         }
1471 
1472         // A padding may be needed again since a previous instruction
1473         // could be moved to delay slot.
1474 
1475         // align the instruction if necessary
1476         int padding = mach-&gt;compute_padding(current_offset);
1477         // Make sure safepoint node for polling is distinct from a call&#39;s
1478         // return by adding a nop if needed.
1479         if (is_sfn &amp;&amp; !is_mcall &amp;&amp; padding == 0 &amp;&amp; current_offset == last_call_offset) {
1480           padding = nop_size;
1481         }
1482         if (padding == 0 &amp;&amp; mach-&gt;avoid_back_to_back(MachNode::AVOID_BEFORE) &amp;&amp;
1483             current_offset == last_avoid_back_to_back_offset) {
1484           // Avoid back to back some instructions.
1485           padding = nop_size;
1486         }






1487 
1488         if (padding &gt; 0) {
1489           assert((padding % nop_size) == 0, &quot;padding is not a multiple of NOP size&quot;);
1490           int nops_cnt = padding / nop_size;
1491           MachNode *nop = new MachNopNode(nops_cnt);
1492           block-&gt;insert_node(nop, j++);
1493           last_inst++;
1494           C-&gt;cfg()-&gt;map_node_to_block(nop, block);
1495           // Ensure enough space.
1496           cb-&gt;insts()-&gt;maybe_expand_to_ensure_remaining(MAX_inst_size);
1497           if ((cb-&gt;blob() == NULL) || (!CompileBroker::should_compile_new_jobs())) {
1498             C-&gt;record_failure(&quot;CodeCache is full&quot;);
1499             return;
1500           }
1501           nop-&gt;emit(*cb, C-&gt;regalloc());
1502           cb-&gt;flush_bundle(true);
1503           current_offset = cb-&gt;insts_size();
1504         }
1505 
1506         // Remember the start of the last call in a basic block
</pre>
</td>
</tr>
</table>
<center><a href="node.hpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="output.hpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>