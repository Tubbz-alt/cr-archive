<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/share/runtime/synchronizer.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="safepoint.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="thread.cpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/share/runtime/synchronizer.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
1297   // Induce STW safepoint to trim monitors
1298   // Ultimately, this results in a call to deflate_idle_monitors() in the near future.
1299   // More precisely, trigger a cleanup safepoint as the number
1300   // of active monitors passes the specified threshold.
1301   // TODO: assert thread state is reasonable
1302 
1303   if (Atomic::xchg(&amp;_forceMonitorScavenge, 1) == 0) {
1304     VMThread::check_for_forced_cleanup();
1305   }
1306 }
1307 
1308 ObjectMonitor* ObjectSynchronizer::om_alloc(Thread* self) {
1309   // A large MAXPRIVATE value reduces both list lock contention
1310   // and list coherency traffic, but also tends to increase the
1311   // number of ObjectMonitors in circulation as well as the STW
1312   // scavenge costs.  As usual, we lean toward time in space-time
1313   // tradeoffs.
1314   const int MAXPRIVATE = 1024;
1315   NoSafepointVerifier nsv;
1316 
<span class="line-removed">1317   stringStream ss;</span>
1318   for (;;) {
1319     ObjectMonitor* m;
1320 
1321     // 1: try to allocate from the thread&#39;s local om_free_list.
1322     // Threads will attempt to allocate first from their local list, then
1323     // from the global list, and only after those attempts fail will the
1324     // thread attempt to instantiate new monitors. Thread-local free lists
1325     // improve allocation latency, as well as reducing coherency traffic
1326     // on the shared global list.
1327     m = take_from_start_of_om_free_list(self);
1328     if (m != NULL) {
1329       guarantee(m-&gt;object() == NULL, &quot;invariant&quot;);
1330       prepend_to_om_in_use_list(self, m);
1331       return m;
1332     }
1333 
1334     // 2: try to allocate from the global om_list_globals._free_list
1335     // If we&#39;re using thread-local free lists then try
1336     // to reprovision the caller&#39;s free list.
1337     if (Atomic::load(&amp;om_list_globals._free_list) != NULL) {
</pre>
<hr />
<pre>
1404 }
1405 
1406 // Place &quot;m&quot; on the caller&#39;s private per-thread om_free_list.
1407 // In practice there&#39;s no need to clamp or limit the number of
1408 // monitors on a thread&#39;s om_free_list as the only non-allocation time
1409 // we&#39;ll call om_release() is to return a monitor to the free list after
1410 // a CAS attempt failed. This doesn&#39;t allow unbounded #s of monitors to
1411 // accumulate on a thread&#39;s free list.
1412 //
1413 // Key constraint: all ObjectMonitors on a thread&#39;s free list and the global
1414 // free list must have their object field set to null. This prevents the
1415 // scavenger -- deflate_monitor_list() -- from reclaiming them while we
1416 // are trying to release them.
1417 
1418 void ObjectSynchronizer::om_release(Thread* self, ObjectMonitor* m,
1419                                     bool from_per_thread_alloc) {
1420   guarantee(m-&gt;header().value() == 0, &quot;invariant&quot;);
1421   guarantee(m-&gt;object() == NULL, &quot;invariant&quot;);
1422   NoSafepointVerifier nsv;
1423 
<span class="line-modified">1424   stringStream ss;</span>
<span class="line-modified">1425   guarantee((m-&gt;is_busy() | m-&gt;_recursions) == 0, &quot;freeing in-use monitor: &quot;</span>
<span class="line-modified">1426             &quot;%s, recursions=&quot; INTX_FORMAT, m-&gt;is_busy_to_string(&amp;ss),</span>
<span class="line-modified">1427             m-&gt;_recursions);</span>

1428   // _next_om is used for both per-thread in-use and free lists so
1429   // we have to remove &#39;m&#39; from the in-use list first (as needed).
1430   if (from_per_thread_alloc) {
1431     // Need to remove &#39;m&#39; from om_in_use_list.
1432     ObjectMonitor* mid = NULL;
1433     ObjectMonitor* next = NULL;
1434 
1435     // This list walk can only race with another list walker since
1436     // deflation can only happen at a safepoint so we don&#39;t have to
1437     // worry about an ObjectMonitor being removed from this list
1438     // while we are walking it.
1439 
1440     // Lock the list head to avoid racing with another list walker.
1441     if ((mid = get_list_head_locked(&amp;self-&gt;om_in_use_list)) == NULL) {
1442       fatal(&quot;thread=&quot; INTPTR_FORMAT &quot; in-use list must not be empty.&quot;, p2i(self));
1443     }
1444     next = unmarked_next(mid);
1445     if (m == mid) {
1446       // First special case:
1447       // &#39;m&#39; matches mid, is the list head and is locked. Switch the list
</pre>
<hr />
<pre>
1570     // be ahead of us; we&#39;ll detect that and delay to let it finish.
1571     //
1572     // The thread is going away. Set &#39;free_tail&#39; to the last per-thread free
1573     // monitor which will be linked to om_list_globals._free_list below.
1574     //
1575     // Account for the free list head before the loop since it is
1576     // already locked (by this thread):
1577     free_tail = free_list;
1578     free_count++;
1579     for (ObjectMonitor* s = unmarked_next(free_list); s != NULL; s = unmarked_next(s)) {
1580       if (is_locked(s)) {
1581         // s is locked so there must be a racing walker thread ahead
1582         // of us so we&#39;ll give it a chance to finish.
1583         while (is_locked(s)) {
1584           os::naked_short_sleep(1);
1585         }
1586       }
1587       free_tail = s;
1588       free_count++;
1589       guarantee(s-&gt;object() == NULL, &quot;invariant&quot;);
<span class="line-modified">1590       stringStream ss;</span>
<span class="line-modified">1591       guarantee(!s-&gt;is_busy(), &quot;must be !is_busy: %s&quot;, s-&gt;is_busy_to_string(&amp;ss));</span>


1592     }
1593     guarantee(free_tail != NULL, &quot;invariant&quot;);
1594     int l_om_free_count = Atomic::load(&amp;self-&gt;om_free_count);
1595     assert(l_om_free_count == free_count, &quot;free counts don&#39;t match: &quot;
1596            &quot;l_om_free_count=%d, free_count=%d&quot;, l_om_free_count, free_count);
1597     Atomic::store(&amp;self-&gt;om_free_count, 0);
1598     Atomic::store(&amp;self-&gt;om_free_list, (ObjectMonitor*)NULL);
1599     om_unlock(free_list);
1600   }
1601 
1602   if (free_tail != NULL) {
1603     prepend_list_to_global_free_list(free_list, free_tail, free_count);
1604   }
1605 
1606   if (in_use_tail != NULL) {
1607     prepend_list_to_global_in_use_list(in_use_list, in_use_tail, in_use_count);
1608   }
1609 
1610   LogStreamHandle(Debug, monitorinflation) lsh_debug;
1611   LogStreamHandle(Info, monitorinflation) lsh_info;
</pre>
</td>
<td>
<hr />
<pre>
1297   // Induce STW safepoint to trim monitors
1298   // Ultimately, this results in a call to deflate_idle_monitors() in the near future.
1299   // More precisely, trigger a cleanup safepoint as the number
1300   // of active monitors passes the specified threshold.
1301   // TODO: assert thread state is reasonable
1302 
1303   if (Atomic::xchg(&amp;_forceMonitorScavenge, 1) == 0) {
1304     VMThread::check_for_forced_cleanup();
1305   }
1306 }
1307 
1308 ObjectMonitor* ObjectSynchronizer::om_alloc(Thread* self) {
1309   // A large MAXPRIVATE value reduces both list lock contention
1310   // and list coherency traffic, but also tends to increase the
1311   // number of ObjectMonitors in circulation as well as the STW
1312   // scavenge costs.  As usual, we lean toward time in space-time
1313   // tradeoffs.
1314   const int MAXPRIVATE = 1024;
1315   NoSafepointVerifier nsv;
1316 

1317   for (;;) {
1318     ObjectMonitor* m;
1319 
1320     // 1: try to allocate from the thread&#39;s local om_free_list.
1321     // Threads will attempt to allocate first from their local list, then
1322     // from the global list, and only after those attempts fail will the
1323     // thread attempt to instantiate new monitors. Thread-local free lists
1324     // improve allocation latency, as well as reducing coherency traffic
1325     // on the shared global list.
1326     m = take_from_start_of_om_free_list(self);
1327     if (m != NULL) {
1328       guarantee(m-&gt;object() == NULL, &quot;invariant&quot;);
1329       prepend_to_om_in_use_list(self, m);
1330       return m;
1331     }
1332 
1333     // 2: try to allocate from the global om_list_globals._free_list
1334     // If we&#39;re using thread-local free lists then try
1335     // to reprovision the caller&#39;s free list.
1336     if (Atomic::load(&amp;om_list_globals._free_list) != NULL) {
</pre>
<hr />
<pre>
1403 }
1404 
1405 // Place &quot;m&quot; on the caller&#39;s private per-thread om_free_list.
1406 // In practice there&#39;s no need to clamp or limit the number of
1407 // monitors on a thread&#39;s om_free_list as the only non-allocation time
1408 // we&#39;ll call om_release() is to return a monitor to the free list after
1409 // a CAS attempt failed. This doesn&#39;t allow unbounded #s of monitors to
1410 // accumulate on a thread&#39;s free list.
1411 //
1412 // Key constraint: all ObjectMonitors on a thread&#39;s free list and the global
1413 // free list must have their object field set to null. This prevents the
1414 // scavenger -- deflate_monitor_list() -- from reclaiming them while we
1415 // are trying to release them.
1416 
1417 void ObjectSynchronizer::om_release(Thread* self, ObjectMonitor* m,
1418                                     bool from_per_thread_alloc) {
1419   guarantee(m-&gt;header().value() == 0, &quot;invariant&quot;);
1420   guarantee(m-&gt;object() == NULL, &quot;invariant&quot;);
1421   NoSafepointVerifier nsv;
1422 
<span class="line-modified">1423   if ((m-&gt;is_busy() | m-&gt;_recursions) != 0) {</span>
<span class="line-modified">1424     stringStream ss;</span>
<span class="line-modified">1425     fatal(&quot;freeing in-use monitor: %s, recursions=&quot; INTX_FORMAT,</span>
<span class="line-modified">1426           m-&gt;is_busy_to_string(&amp;ss), m-&gt;_recursions);</span>
<span class="line-added">1427   }</span>
1428   // _next_om is used for both per-thread in-use and free lists so
1429   // we have to remove &#39;m&#39; from the in-use list first (as needed).
1430   if (from_per_thread_alloc) {
1431     // Need to remove &#39;m&#39; from om_in_use_list.
1432     ObjectMonitor* mid = NULL;
1433     ObjectMonitor* next = NULL;
1434 
1435     // This list walk can only race with another list walker since
1436     // deflation can only happen at a safepoint so we don&#39;t have to
1437     // worry about an ObjectMonitor being removed from this list
1438     // while we are walking it.
1439 
1440     // Lock the list head to avoid racing with another list walker.
1441     if ((mid = get_list_head_locked(&amp;self-&gt;om_in_use_list)) == NULL) {
1442       fatal(&quot;thread=&quot; INTPTR_FORMAT &quot; in-use list must not be empty.&quot;, p2i(self));
1443     }
1444     next = unmarked_next(mid);
1445     if (m == mid) {
1446       // First special case:
1447       // &#39;m&#39; matches mid, is the list head and is locked. Switch the list
</pre>
<hr />
<pre>
1570     // be ahead of us; we&#39;ll detect that and delay to let it finish.
1571     //
1572     // The thread is going away. Set &#39;free_tail&#39; to the last per-thread free
1573     // monitor which will be linked to om_list_globals._free_list below.
1574     //
1575     // Account for the free list head before the loop since it is
1576     // already locked (by this thread):
1577     free_tail = free_list;
1578     free_count++;
1579     for (ObjectMonitor* s = unmarked_next(free_list); s != NULL; s = unmarked_next(s)) {
1580       if (is_locked(s)) {
1581         // s is locked so there must be a racing walker thread ahead
1582         // of us so we&#39;ll give it a chance to finish.
1583         while (is_locked(s)) {
1584           os::naked_short_sleep(1);
1585         }
1586       }
1587       free_tail = s;
1588       free_count++;
1589       guarantee(s-&gt;object() == NULL, &quot;invariant&quot;);
<span class="line-modified">1590       if (s-&gt;is_busy()) {</span>
<span class="line-modified">1591         stringStream ss;</span>
<span class="line-added">1592         fatal(&quot;must be !is_busy: %s&quot;, s-&gt;is_busy_to_string(&amp;ss));</span>
<span class="line-added">1593       }</span>
1594     }
1595     guarantee(free_tail != NULL, &quot;invariant&quot;);
1596     int l_om_free_count = Atomic::load(&amp;self-&gt;om_free_count);
1597     assert(l_om_free_count == free_count, &quot;free counts don&#39;t match: &quot;
1598            &quot;l_om_free_count=%d, free_count=%d&quot;, l_om_free_count, free_count);
1599     Atomic::store(&amp;self-&gt;om_free_count, 0);
1600     Atomic::store(&amp;self-&gt;om_free_list, (ObjectMonitor*)NULL);
1601     om_unlock(free_list);
1602   }
1603 
1604   if (free_tail != NULL) {
1605     prepend_list_to_global_free_list(free_list, free_tail, free_count);
1606   }
1607 
1608   if (in_use_tail != NULL) {
1609     prepend_list_to_global_in_use_list(in_use_list, in_use_tail, in_use_count);
1610   }
1611 
1612   LogStreamHandle(Debug, monitorinflation) lsh_debug;
1613   LogStreamHandle(Info, monitorinflation) lsh_info;
</pre>
</td>
</tr>
</table>
<center><a href="safepoint.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="thread.cpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>