<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/share/c1/c1_LIRAssembler.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="c1_LIR.hpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="../classfile/classFileParser.cpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/share/c1/c1_LIRAssembler.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
  1 /*
<span class="line-modified">  2  * Copyright (c) 2000, 2018, Oracle and/or its affiliates. All rights reserved.</span>
  3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  4  *
  5  * This code is free software; you can redistribute it and/or modify it
  6  * under the terms of the GNU General Public License version 2 only, as
  7  * published by the Free Software Foundation.
  8  *
  9  * This code is distributed in the hope that it will be useful, but WITHOUT
 10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
 11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
 12  * version 2 for more details (a copy is included in the LICENSE file that
 13  * accompanied this code).
 14  *
 15  * You should have received a copy of the GNU General Public License version
 16  * 2 along with this work; if not, write to the Free Software Foundation,
 17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 18  *
 19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
 20  * or visit www.oracle.com if you need additional information or have any
 21  * questions.
 22  *
 23  */
 24 
 25 #include &quot;precompiled.hpp&quot;
 26 #include &quot;asm/assembler.inline.hpp&quot;
 27 #include &quot;c1/c1_Compilation.hpp&quot;
 28 #include &quot;c1/c1_Instruction.hpp&quot;
 29 #include &quot;c1/c1_InstructionPrinter.hpp&quot;
 30 #include &quot;c1/c1_LIRAssembler.hpp&quot;
 31 #include &quot;c1/c1_MacroAssembler.hpp&quot;
 32 #include &quot;c1/c1_ValueStack.hpp&quot;
 33 #include &quot;ci/ciInstance.hpp&quot;

 34 #include &quot;gc/shared/barrierSet.hpp&quot;
 35 #include &quot;runtime/os.hpp&quot;

 36 
 37 void LIR_Assembler::patching_epilog(PatchingStub* patch, LIR_PatchCode patch_code, Register obj, CodeEmitInfo* info) {
 38   // We must have enough patching space so that call can be inserted.
 39   // We cannot use fat nops here, since the concurrent code rewrite may transiently
 40   // create the illegal instruction sequence.
 41   while ((intx) _masm-&gt;pc() - (intx) patch-&gt;pc_start() &lt; NativeGeneralJump::instruction_size) {
 42     _masm-&gt;nop();
 43   }
 44   patch-&gt;install(_masm, patch_code, obj, info);
 45   append_code_stub(patch);
 46 
 47 #ifdef ASSERT
 48   Bytecodes::Code code = info-&gt;scope()-&gt;method()-&gt;java_code_at_bci(info-&gt;stack()-&gt;bci());
 49   if (patch-&gt;id() == PatchingStub::access_field_id) {
 50     switch (code) {
 51       case Bytecodes::_putstatic:
 52       case Bytecodes::_getstatic:
 53       case Bytecodes::_putfield:
 54       case Bytecodes::_getfield:
 55         break;
 56       default:
 57         ShouldNotReachHere();
 58     }
 59   } else if (patch-&gt;id() == PatchingStub::load_klass_id) {
 60     switch (code) {
 61       case Bytecodes::_new:

 62       case Bytecodes::_anewarray:
 63       case Bytecodes::_multianewarray:
 64       case Bytecodes::_instanceof:
 65       case Bytecodes::_checkcast:
 66         break;
 67       default:
 68         ShouldNotReachHere();
 69     }
 70   } else if (patch-&gt;id() == PatchingStub::load_mirror_id) {
 71     switch (code) {
 72       case Bytecodes::_putstatic:
 73       case Bytecodes::_getstatic:
 74       case Bytecodes::_ldc:
 75       case Bytecodes::_ldc_w:
 76         break;
 77       default:
 78         ShouldNotReachHere();
 79     }
 80   } else if (patch-&gt;id() == PatchingStub::load_appendix_id) {
 81     Bytecodes::Code bc_raw = info-&gt;scope()-&gt;method()-&gt;raw_code_at_bci(info-&gt;stack()-&gt;bci());
</pre>
<hr />
<pre>
 98 //---------------------------------------------------------------
 99 
100 
101 LIR_Assembler::LIR_Assembler(Compilation* c):
102    _masm(c-&gt;masm())
103  , _bs(BarrierSet::barrier_set())
104  , _compilation(c)
105  , _frame_map(c-&gt;frame_map())
106  , _current_block(NULL)
107  , _pending_non_safepoint(NULL)
108  , _pending_non_safepoint_offset(0)
109 {
110   _slow_case_stubs = new CodeStubList();
111 }
112 
113 
114 LIR_Assembler::~LIR_Assembler() {
115   // The unwind handler label may be unnbound if this destructor is invoked because of a bail-out.
116   // Reset it here to avoid an assertion.
117   _unwind_handler_entry.reset();

118 }
119 
120 
121 void LIR_Assembler::check_codespace() {
122   CodeSection* cs = _masm-&gt;code_section();
123   if (cs-&gt;remaining() &lt; (int)(NOT_LP64(1*K)LP64_ONLY(2*K))) {
124     BAILOUT(&quot;CodeBuffer overflow&quot;);
125   }
126 }
127 
128 
129 void LIR_Assembler::append_code_stub(CodeStub* stub) {
130   _slow_case_stubs-&gt;append(stub);
131 }
132 
133 void LIR_Assembler::emit_stubs(CodeStubList* stub_list) {
134   for (int m = 0; m &lt; stub_list-&gt;length(); m++) {
135     CodeStub* s = stub_list-&gt;at(m);
136 
137     check_codespace();
</pre>
<hr />
<pre>
319       tty-&gt;print_cr(&quot;label of block B%d is not bound&quot;, _branch_target_blocks.at(i)-&gt;block_id());
320       assert(false, &quot;unbound label&quot;);
321     }
322   }
323 }
324 #endif
325 
326 //----------------------------------debug info--------------------------------
327 
328 
329 void LIR_Assembler::add_debug_info_for_branch(CodeEmitInfo* info) {
330   int pc_offset = code_offset();
331   flush_debug_info(pc_offset);
332   info-&gt;record_debug_info(compilation()-&gt;debug_info_recorder(), pc_offset);
333   if (info-&gt;exception_handlers() != NULL) {
334     compilation()-&gt;add_exception_handlers_for_pco(pc_offset, info-&gt;exception_handlers());
335   }
336 }
337 
338 
<span class="line-modified">339 void LIR_Assembler::add_call_info(int pc_offset, CodeEmitInfo* cinfo) {</span>
340   flush_debug_info(pc_offset);
<span class="line-modified">341   cinfo-&gt;record_debug_info(compilation()-&gt;debug_info_recorder(), pc_offset);</span>
342   if (cinfo-&gt;exception_handlers() != NULL) {
343     compilation()-&gt;add_exception_handlers_for_pco(pc_offset, cinfo-&gt;exception_handlers());
344   }
345 }
346 
347 static ValueStack* debug_info(Instruction* ins) {
348   StateSplit* ss = ins-&gt;as_StateSplit();
349   if (ss != NULL) return ss-&gt;state();
350   return ins-&gt;state_before();
351 }
352 
353 void LIR_Assembler::process_debug_info(LIR_Op* op) {
354   Instruction* src = op-&gt;source();
355   if (src == NULL)  return;
356   int pc_offset = code_offset();
357   if (_pending_non_safepoint == src) {
358     _pending_non_safepoint_offset = pc_offset;
359     return;
360   }
361   ValueStack* vstack = debug_info(src);
</pre>
<hr />
<pre>
464   case lir_optvirtual_call:
465     call(op, relocInfo::opt_virtual_call_type);
466     break;
467   case lir_icvirtual_call:
468     ic_call(op);
469     break;
470   case lir_virtual_call:
471     vtable_call(op);
472     break;
473   default:
474     fatal(&quot;unexpected op code: %s&quot;, op-&gt;name());
475     break;
476   }
477 
478   // JSR 292
479   // Record if this method has MethodHandle invokes.
480   if (op-&gt;is_method_handle_invoke()) {
481     compilation()-&gt;set_has_method_handle_invokes(true);
482   }
483 






484 #if defined(IA32) &amp;&amp; defined(TIERED)
485   // C2 leave fpu stack dirty clean it
486   if (UseSSE &lt; 2) {
487     int i;
488     for ( i = 1; i &lt;= 7 ; i++ ) {
489       ffree(i);
490     }
491     if (!op-&gt;result_opr()-&gt;is_float_kind()) {
492       ffree(0);
493     }
494   }
495 #endif // X86 &amp;&amp; TIERED
496 }
497 
498 
499 void LIR_Assembler::emit_opLabel(LIR_OpLabel* op) {
500   _masm-&gt;bind (*(op-&gt;label()));
501 }
502 
503 
</pre>
<hr />
<pre>
575 #ifdef SPARC
576     case lir_pack64:
577       pack64(op-&gt;in_opr(), op-&gt;result_opr());
578       break;
579 
580     case lir_unpack64:
581       unpack64(op-&gt;in_opr(), op-&gt;result_opr());
582       break;
583 #endif
584 
585     case lir_unwind:
586       unwind_op(op-&gt;in_opr());
587       break;
588 
589     default:
590       Unimplemented();
591       break;
592   }
593 }
594 







































































































































595 
596 void LIR_Assembler::emit_op0(LIR_Op0* op) {
597   switch (op-&gt;code()) {
598     case lir_nop:
599       assert(op-&gt;info() == NULL, &quot;not supported&quot;);
600       _masm-&gt;nop();
601       break;
602 
603     case lir_label:
604       Unimplemented();
605       break;
606 
<span class="line-modified">607     case lir_std_entry:</span>
<span class="line-removed">608       // init offsets</span>
<span class="line-removed">609       offsets()-&gt;set_value(CodeOffsets::OSR_Entry, _masm-&gt;offset());</span>
<span class="line-removed">610       _masm-&gt;align(CodeEntryAlignment);</span>
<span class="line-removed">611       if (needs_icache(compilation()-&gt;method())) {</span>
<span class="line-removed">612         check_icache();</span>
<span class="line-removed">613       }</span>
<span class="line-removed">614       offsets()-&gt;set_value(CodeOffsets::Verified_Entry, _masm-&gt;offset());</span>
<span class="line-removed">615       _masm-&gt;verified_entry();</span>
<span class="line-removed">616       if (needs_clinit_barrier_on_entry(compilation()-&gt;method())) {</span>
<span class="line-removed">617         clinit_barrier(compilation()-&gt;method());</span>
<span class="line-removed">618       }</span>
<span class="line-removed">619       build_frame();</span>
620       offsets()-&gt;set_value(CodeOffsets::Frame_Complete, _masm-&gt;offset());
621       break;
622 
623     case lir_osr_entry:
624       offsets()-&gt;set_value(CodeOffsets::OSR_Entry, _masm-&gt;offset());
625       osr_entry();
626       break;
627 
628 #ifdef IA32
629     case lir_fpop_raw:
630       fpop();
631       break;
632 #endif // IA32
633 
634     case lir_breakpoint:
635       breakpoint();
636       break;
637 
638     case lir_membar:
639       membar();
</pre>
<hr />
<pre>
654     case lir_membar_storestore:
655       membar_storestore();
656       break;
657 
658     case lir_membar_loadstore:
659       membar_loadstore();
660       break;
661 
662     case lir_membar_storeload:
663       membar_storeload();
664       break;
665 
666     case lir_get_thread:
667       get_thread(op-&gt;result_opr());
668       break;
669 
670     case lir_on_spin_wait:
671       on_spin_wait();
672       break;
673 




674     default:
675       ShouldNotReachHere();
676       break;
677   }
678 }
679 
680 
681 void LIR_Assembler::emit_op2(LIR_Op2* op) {
682   switch (op-&gt;code()) {
683     case lir_cmp:
684       if (op-&gt;info() != NULL) {
685         assert(op-&gt;in_opr1()-&gt;is_address() || op-&gt;in_opr2()-&gt;is_address(),
686                &quot;shouldn&#39;t be codeemitinfo for non-address operands&quot;);
687         add_debug_info_for_null_check_here(op-&gt;info()); // exception possible
688       }
689       comp_op(op-&gt;condition(), op-&gt;in_opr1(), op-&gt;in_opr2(), op);
690       break;
691 
692     case lir_cmp_l2i:
693     case lir_cmp_fd2i:
</pre>
<hr />
<pre>
747         op-&gt;result_opr());
748       break;
749 
750     case lir_throw:
751       throw_op(op-&gt;in_opr1(), op-&gt;in_opr2(), op-&gt;info());
752       break;
753 
754     case lir_xadd:
755     case lir_xchg:
756       atomic_op(op-&gt;code(), op-&gt;in_opr1(), op-&gt;in_opr2(), op-&gt;result_opr(), op-&gt;tmp1_opr());
757       break;
758 
759     default:
760       Unimplemented();
761       break;
762   }
763 }
764 
765 
766 void LIR_Assembler::build_frame() {
<span class="line-modified">767   _masm-&gt;build_frame(initial_frame_size_in_bytes(), bang_size_in_bytes());</span>

768 }
769 
770 
771 void LIR_Assembler::roundfp_op(LIR_Opr src, LIR_Opr tmp, LIR_Opr dest, bool pop_fpu_stack) {
772   assert(strict_fp_requires_explicit_rounding, &quot;not required&quot;);
773   assert((src-&gt;is_single_fpu() &amp;&amp; dest-&gt;is_single_stack()) ||
774          (src-&gt;is_double_fpu() &amp;&amp; dest-&gt;is_double_stack()),
775          &quot;round_fp: rounds register -&gt; stack location&quot;);
776 
777   reg2stack (src, dest, src-&gt;type(), pop_fpu_stack);
778 }
779 
780 
781 void LIR_Assembler::move_op(LIR_Opr src, LIR_Opr dest, BasicType type, LIR_PatchCode patch_code, CodeEmitInfo* info, bool pop_fpu_stack, bool unaligned, bool wide) {
782   if (src-&gt;is_register()) {
783     if (dest-&gt;is_register()) {
784       assert(patch_code == lir_patch_none &amp;&amp; info == NULL, &quot;no patching and info allowed here&quot;);
785       reg2reg(src,  dest);
786     } else if (dest-&gt;is_stack()) {
787       assert(patch_code == lir_patch_none &amp;&amp; info == NULL, &quot;no patching and info allowed here&quot;);
</pre>
</td>
<td>
<hr />
<pre>
  1 /*
<span class="line-modified">  2  * Copyright (c) 2000, 2019, Oracle and/or its affiliates. All rights reserved.</span>
  3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  4  *
  5  * This code is free software; you can redistribute it and/or modify it
  6  * under the terms of the GNU General Public License version 2 only, as
  7  * published by the Free Software Foundation.
  8  *
  9  * This code is distributed in the hope that it will be useful, but WITHOUT
 10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
 11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
 12  * version 2 for more details (a copy is included in the LICENSE file that
 13  * accompanied this code).
 14  *
 15  * You should have received a copy of the GNU General Public License version
 16  * 2 along with this work; if not, write to the Free Software Foundation,
 17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 18  *
 19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
 20  * or visit www.oracle.com if you need additional information or have any
 21  * questions.
 22  *
 23  */
 24 
 25 #include &quot;precompiled.hpp&quot;
 26 #include &quot;asm/assembler.inline.hpp&quot;
 27 #include &quot;c1/c1_Compilation.hpp&quot;
 28 #include &quot;c1/c1_Instruction.hpp&quot;
 29 #include &quot;c1/c1_InstructionPrinter.hpp&quot;
 30 #include &quot;c1/c1_LIRAssembler.hpp&quot;
 31 #include &quot;c1/c1_MacroAssembler.hpp&quot;
 32 #include &quot;c1/c1_ValueStack.hpp&quot;
 33 #include &quot;ci/ciInstance.hpp&quot;
<span class="line-added"> 34 #include &quot;ci/ciValueKlass.hpp&quot;</span>
 35 #include &quot;gc/shared/barrierSet.hpp&quot;
 36 #include &quot;runtime/os.hpp&quot;
<span class="line-added"> 37 #include &quot;runtime/sharedRuntime.hpp&quot;</span>
 38 
 39 void LIR_Assembler::patching_epilog(PatchingStub* patch, LIR_PatchCode patch_code, Register obj, CodeEmitInfo* info) {
 40   // We must have enough patching space so that call can be inserted.
 41   // We cannot use fat nops here, since the concurrent code rewrite may transiently
 42   // create the illegal instruction sequence.
 43   while ((intx) _masm-&gt;pc() - (intx) patch-&gt;pc_start() &lt; NativeGeneralJump::instruction_size) {
 44     _masm-&gt;nop();
 45   }
 46   patch-&gt;install(_masm, patch_code, obj, info);
 47   append_code_stub(patch);
 48 
 49 #ifdef ASSERT
 50   Bytecodes::Code code = info-&gt;scope()-&gt;method()-&gt;java_code_at_bci(info-&gt;stack()-&gt;bci());
 51   if (patch-&gt;id() == PatchingStub::access_field_id) {
 52     switch (code) {
 53       case Bytecodes::_putstatic:
 54       case Bytecodes::_getstatic:
 55       case Bytecodes::_putfield:
 56       case Bytecodes::_getfield:
 57         break;
 58       default:
 59         ShouldNotReachHere();
 60     }
 61   } else if (patch-&gt;id() == PatchingStub::load_klass_id) {
 62     switch (code) {
 63       case Bytecodes::_new:
<span class="line-added"> 64       case Bytecodes::_defaultvalue:</span>
 65       case Bytecodes::_anewarray:
 66       case Bytecodes::_multianewarray:
 67       case Bytecodes::_instanceof:
 68       case Bytecodes::_checkcast:
 69         break;
 70       default:
 71         ShouldNotReachHere();
 72     }
 73   } else if (patch-&gt;id() == PatchingStub::load_mirror_id) {
 74     switch (code) {
 75       case Bytecodes::_putstatic:
 76       case Bytecodes::_getstatic:
 77       case Bytecodes::_ldc:
 78       case Bytecodes::_ldc_w:
 79         break;
 80       default:
 81         ShouldNotReachHere();
 82     }
 83   } else if (patch-&gt;id() == PatchingStub::load_appendix_id) {
 84     Bytecodes::Code bc_raw = info-&gt;scope()-&gt;method()-&gt;raw_code_at_bci(info-&gt;stack()-&gt;bci());
</pre>
<hr />
<pre>
101 //---------------------------------------------------------------
102 
103 
104 LIR_Assembler::LIR_Assembler(Compilation* c):
105    _masm(c-&gt;masm())
106  , _bs(BarrierSet::barrier_set())
107  , _compilation(c)
108  , _frame_map(c-&gt;frame_map())
109  , _current_block(NULL)
110  , _pending_non_safepoint(NULL)
111  , _pending_non_safepoint_offset(0)
112 {
113   _slow_case_stubs = new CodeStubList();
114 }
115 
116 
117 LIR_Assembler::~LIR_Assembler() {
118   // The unwind handler label may be unnbound if this destructor is invoked because of a bail-out.
119   // Reset it here to avoid an assertion.
120   _unwind_handler_entry.reset();
<span class="line-added">121   _verified_value_entry.reset();</span>
122 }
123 
124 
125 void LIR_Assembler::check_codespace() {
126   CodeSection* cs = _masm-&gt;code_section();
127   if (cs-&gt;remaining() &lt; (int)(NOT_LP64(1*K)LP64_ONLY(2*K))) {
128     BAILOUT(&quot;CodeBuffer overflow&quot;);
129   }
130 }
131 
132 
133 void LIR_Assembler::append_code_stub(CodeStub* stub) {
134   _slow_case_stubs-&gt;append(stub);
135 }
136 
137 void LIR_Assembler::emit_stubs(CodeStubList* stub_list) {
138   for (int m = 0; m &lt; stub_list-&gt;length(); m++) {
139     CodeStub* s = stub_list-&gt;at(m);
140 
141     check_codespace();
</pre>
<hr />
<pre>
323       tty-&gt;print_cr(&quot;label of block B%d is not bound&quot;, _branch_target_blocks.at(i)-&gt;block_id());
324       assert(false, &quot;unbound label&quot;);
325     }
326   }
327 }
328 #endif
329 
330 //----------------------------------debug info--------------------------------
331 
332 
333 void LIR_Assembler::add_debug_info_for_branch(CodeEmitInfo* info) {
334   int pc_offset = code_offset();
335   flush_debug_info(pc_offset);
336   info-&gt;record_debug_info(compilation()-&gt;debug_info_recorder(), pc_offset);
337   if (info-&gt;exception_handlers() != NULL) {
338     compilation()-&gt;add_exception_handlers_for_pco(pc_offset, info-&gt;exception_handlers());
339   }
340 }
341 
342 
<span class="line-modified">343 void LIR_Assembler::add_call_info(int pc_offset, CodeEmitInfo* cinfo, bool maybe_return_as_fields) {</span>
344   flush_debug_info(pc_offset);
<span class="line-modified">345   cinfo-&gt;record_debug_info(compilation()-&gt;debug_info_recorder(), pc_offset, maybe_return_as_fields);</span>
346   if (cinfo-&gt;exception_handlers() != NULL) {
347     compilation()-&gt;add_exception_handlers_for_pco(pc_offset, cinfo-&gt;exception_handlers());
348   }
349 }
350 
351 static ValueStack* debug_info(Instruction* ins) {
352   StateSplit* ss = ins-&gt;as_StateSplit();
353   if (ss != NULL) return ss-&gt;state();
354   return ins-&gt;state_before();
355 }
356 
357 void LIR_Assembler::process_debug_info(LIR_Op* op) {
358   Instruction* src = op-&gt;source();
359   if (src == NULL)  return;
360   int pc_offset = code_offset();
361   if (_pending_non_safepoint == src) {
362     _pending_non_safepoint_offset = pc_offset;
363     return;
364   }
365   ValueStack* vstack = debug_info(src);
</pre>
<hr />
<pre>
468   case lir_optvirtual_call:
469     call(op, relocInfo::opt_virtual_call_type);
470     break;
471   case lir_icvirtual_call:
472     ic_call(op);
473     break;
474   case lir_virtual_call:
475     vtable_call(op);
476     break;
477   default:
478     fatal(&quot;unexpected op code: %s&quot;, op-&gt;name());
479     break;
480   }
481 
482   // JSR 292
483   // Record if this method has MethodHandle invokes.
484   if (op-&gt;is_method_handle_invoke()) {
485     compilation()-&gt;set_has_method_handle_invokes(true);
486   }
487 
<span class="line-added">488   ciValueKlass* vk;</span>
<span class="line-added">489   if (op-&gt;maybe_return_as_fields(&amp;vk)) {</span>
<span class="line-added">490     int offset = store_value_type_fields_to_buf(vk);</span>
<span class="line-added">491     add_call_info(offset, op-&gt;info(), true);</span>
<span class="line-added">492   }</span>
<span class="line-added">493 </span>
494 #if defined(IA32) &amp;&amp; defined(TIERED)
495   // C2 leave fpu stack dirty clean it
496   if (UseSSE &lt; 2) {
497     int i;
498     for ( i = 1; i &lt;= 7 ; i++ ) {
499       ffree(i);
500     }
501     if (!op-&gt;result_opr()-&gt;is_float_kind()) {
502       ffree(0);
503     }
504   }
505 #endif // X86 &amp;&amp; TIERED
506 }
507 
508 
509 void LIR_Assembler::emit_opLabel(LIR_OpLabel* op) {
510   _masm-&gt;bind (*(op-&gt;label()));
511 }
512 
513 
</pre>
<hr />
<pre>
585 #ifdef SPARC
586     case lir_pack64:
587       pack64(op-&gt;in_opr(), op-&gt;result_opr());
588       break;
589 
590     case lir_unpack64:
591       unpack64(op-&gt;in_opr(), op-&gt;result_opr());
592       break;
593 #endif
594 
595     case lir_unwind:
596       unwind_op(op-&gt;in_opr());
597       break;
598 
599     default:
600       Unimplemented();
601       break;
602   }
603 }
604 
<span class="line-added">605 void LIR_Assembler::add_scalarized_entry_info(int pc_offset) {</span>
<span class="line-added">606   flush_debug_info(pc_offset);</span>
<span class="line-added">607   DebugInformationRecorder* debug_info = compilation()-&gt;debug_info_recorder();</span>
<span class="line-added">608   // The VEP and VVEP(RO) of a C1-compiled method call buffer_value_args_xxx()</span>
<span class="line-added">609   // before doing any argument shuffling. This call may cause GC. When GC happens,</span>
<span class="line-added">610   // all the parameters are still as passed by the caller, so we just use</span>
<span class="line-added">611   // map-&gt;set_include_argument_oops() inside frame::sender_for_compiled_frame(RegisterMap* map).</span>
<span class="line-added">612   // There&#39;s no need to build a GC map here.</span>
<span class="line-added">613   OopMap* oop_map = new OopMap(0, 0);</span>
<span class="line-added">614   debug_info-&gt;add_safepoint(pc_offset, oop_map);</span>
<span class="line-added">615   DebugToken* locvals = debug_info-&gt;create_scope_values(NULL); // FIXME is this needed (for Java debugging to work properly??)</span>
<span class="line-added">616   DebugToken* expvals = debug_info-&gt;create_scope_values(NULL); // FIXME is this needed (for Java debugging to work properly??)</span>
<span class="line-added">617   DebugToken* monvals = debug_info-&gt;create_monitor_values(NULL); // FIXME: need testing with synchronized method</span>
<span class="line-added">618   bool reexecute = false;</span>
<span class="line-added">619   bool return_oop = false; // This flag will be ignored since it used only for C2 with escape analysis.</span>
<span class="line-added">620   bool rethrow_exception = false;</span>
<span class="line-added">621   bool is_method_handle_invoke = false;</span>
<span class="line-added">622   debug_info-&gt;describe_scope(pc_offset, methodHandle(), method(), 0, reexecute, rethrow_exception, is_method_handle_invoke, return_oop, false, locvals, expvals, monvals);</span>
<span class="line-added">623   debug_info-&gt;end_safepoint(pc_offset);</span>
<span class="line-added">624 }</span>
<span class="line-added">625 </span>
<span class="line-added">626 // The entries points of C1-compiled methods can have the following types:</span>
<span class="line-added">627 // (1) Methods with no value args</span>
<span class="line-added">628 // (2) Methods with value receiver but no value args</span>
<span class="line-added">629 //     VVEP_RO is the same as VVEP</span>
<span class="line-added">630 // (3) Methods with non-value receiver and some value args</span>
<span class="line-added">631 //     VVEP_RO is the same as VEP</span>
<span class="line-added">632 // (4) Methods with value receiver and other value args</span>
<span class="line-added">633 //     Separate VEP, VVEP and VVEP_RO</span>
<span class="line-added">634 //</span>
<span class="line-added">635 // (1)               (2)                 (3)                    (4)</span>
<span class="line-added">636 // UEP/UVEP:         VEP:                UEP:                   UEP:</span>
<span class="line-added">637 //   check_icache      pack receiver       check_icache           check_icache</span>
<span class="line-added">638 // VEP/VVEP/VVEP_RO    jump to VVEP      VEP/VVEP_RO:           VVEP_RO:</span>
<span class="line-added">639 //   body            UEP/UVEP:             pack value args        pack value args (except receiver)</span>
<span class="line-added">640 //                     check_icache        jump to VVEP           jump to VVEP</span>
<span class="line-added">641 //                   VVEP/VVEP_RO        UVEP:                  VEP:</span>
<span class="line-added">642 //                     body                check_icache           pack all value args</span>
<span class="line-added">643 //                                       VVEP:                    jump to VVEP</span>
<span class="line-added">644 //                                         body                 UVEP:</span>
<span class="line-added">645 //                                                                check_icache</span>
<span class="line-added">646 //                                                              VVEP:</span>
<span class="line-added">647 //                                                                body</span>
<span class="line-added">648 void LIR_Assembler::emit_std_entries() {</span>
<span class="line-added">649   offsets()-&gt;set_value(CodeOffsets::OSR_Entry, _masm-&gt;offset());</span>
<span class="line-added">650 </span>
<span class="line-added">651   _masm-&gt;align(CodeEntryAlignment);</span>
<span class="line-added">652   const CompiledEntrySignature* ces = compilation()-&gt;compiled_entry_signature();</span>
<span class="line-added">653   if (ces-&gt;has_scalarized_args()) {</span>
<span class="line-added">654     assert(ValueTypePassFieldsAsArgs &amp;&amp; method()-&gt;get_Method()-&gt;has_scalarized_args(), &quot;must be&quot;);</span>
<span class="line-added">655     CodeOffsets::Entries ro_entry_type = ces-&gt;c1_value_ro_entry_type();</span>
<span class="line-added">656 </span>
<span class="line-added">657     // UEP: check icache and fall-through</span>
<span class="line-added">658     if (ro_entry_type != CodeOffsets::Verified_Value_Entry) {</span>
<span class="line-added">659       offsets()-&gt;set_value(CodeOffsets::Entry, _masm-&gt;offset());</span>
<span class="line-added">660       if (needs_icache(method())) {</span>
<span class="line-added">661         check_icache();</span>
<span class="line-added">662       }</span>
<span class="line-added">663     }</span>
<span class="line-added">664 </span>
<span class="line-added">665     // VVEP_RO: pack all value parameters, except the receiver</span>
<span class="line-added">666     if (ro_entry_type == CodeOffsets::Verified_Value_Entry_RO) {</span>
<span class="line-added">667       emit_std_entry(CodeOffsets::Verified_Value_Entry_RO, ces);</span>
<span class="line-added">668     }</span>
<span class="line-added">669 </span>
<span class="line-added">670     // VEP: pack all value parameters</span>
<span class="line-added">671     _masm-&gt;align(CodeEntryAlignment);</span>
<span class="line-added">672     emit_std_entry(CodeOffsets::Verified_Entry, ces);</span>
<span class="line-added">673 </span>
<span class="line-added">674     // UVEP: check icache and fall-through</span>
<span class="line-added">675     _masm-&gt;align(CodeEntryAlignment);</span>
<span class="line-added">676     offsets()-&gt;set_value(CodeOffsets::Value_Entry, _masm-&gt;offset());</span>
<span class="line-added">677     if (ro_entry_type == CodeOffsets::Verified_Value_Entry) {</span>
<span class="line-added">678       // Special case if we have VVEP == VVEP(RO):</span>
<span class="line-added">679       // this means UVEP (called by C1) == UEP (called by C2).</span>
<span class="line-added">680       offsets()-&gt;set_value(CodeOffsets::Entry, _masm-&gt;offset());</span>
<span class="line-added">681     }</span>
<span class="line-added">682     if (needs_icache(method())) {</span>
<span class="line-added">683       check_icache();</span>
<span class="line-added">684     }</span>
<span class="line-added">685 </span>
<span class="line-added">686     // VVEP: all value parameters are passed as refs - no packing.</span>
<span class="line-added">687     emit_std_entry(CodeOffsets::Verified_Value_Entry, NULL);</span>
<span class="line-added">688 </span>
<span class="line-added">689     if (ro_entry_type != CodeOffsets::Verified_Value_Entry_RO) {</span>
<span class="line-added">690       // The VVEP(RO) is the same as VEP or VVEP</span>
<span class="line-added">691       assert(ro_entry_type == CodeOffsets::Verified_Entry ||</span>
<span class="line-added">692              ro_entry_type == CodeOffsets::Verified_Value_Entry, &quot;must be&quot;);</span>
<span class="line-added">693       offsets()-&gt;set_value(CodeOffsets::Verified_Value_Entry_RO,</span>
<span class="line-added">694                            offsets()-&gt;value(ro_entry_type));</span>
<span class="line-added">695     }</span>
<span class="line-added">696   } else {</span>
<span class="line-added">697     // All 3 entries are the same (no value-type packing)</span>
<span class="line-added">698     offsets()-&gt;set_value(CodeOffsets::Entry, _masm-&gt;offset());</span>
<span class="line-added">699     offsets()-&gt;set_value(CodeOffsets::Value_Entry, _masm-&gt;offset());</span>
<span class="line-added">700     if (needs_icache(method())) {</span>
<span class="line-added">701       check_icache();</span>
<span class="line-added">702     }</span>
<span class="line-added">703     emit_std_entry(CodeOffsets::Verified_Value_Entry, NULL);</span>
<span class="line-added">704     offsets()-&gt;set_value(CodeOffsets::Verified_Entry, offsets()-&gt;value(CodeOffsets::Verified_Value_Entry));</span>
<span class="line-added">705     offsets()-&gt;set_value(CodeOffsets::Verified_Value_Entry_RO, offsets()-&gt;value(CodeOffsets::Verified_Value_Entry));</span>
<span class="line-added">706   }</span>
<span class="line-added">707 }</span>
<span class="line-added">708 </span>
<span class="line-added">709 void LIR_Assembler::emit_std_entry(CodeOffsets::Entries entry, const CompiledEntrySignature* ces) {</span>
<span class="line-added">710   offsets()-&gt;set_value(entry, _masm-&gt;offset());</span>
<span class="line-added">711   _masm-&gt;verified_entry();</span>
<span class="line-added">712   switch (entry) {</span>
<span class="line-added">713   case CodeOffsets::Verified_Entry: {</span>
<span class="line-added">714     if (needs_clinit_barrier_on_entry(method())) {</span>
<span class="line-added">715       clinit_barrier(method());</span>
<span class="line-added">716     }</span>
<span class="line-added">717     int rt_call_offset = _masm-&gt;verified_entry(ces, initial_frame_size_in_bytes(), bang_size_in_bytes(), in_bytes(frame_map()-&gt;sp_offset_for_orig_pc()), _verified_value_entry);</span>
<span class="line-added">718     add_scalarized_entry_info(rt_call_offset);</span>
<span class="line-added">719     break;</span>
<span class="line-added">720   }</span>
<span class="line-added">721   case CodeOffsets::Verified_Value_Entry_RO: {</span>
<span class="line-added">722     assert(!needs_clinit_barrier_on_entry(method()), &quot;can&#39;t be static&quot;);</span>
<span class="line-added">723     int rt_call_offset = _masm-&gt;verified_value_ro_entry(ces, initial_frame_size_in_bytes(), bang_size_in_bytes(), in_bytes(frame_map()-&gt;sp_offset_for_orig_pc()), _verified_value_entry);</span>
<span class="line-added">724     add_scalarized_entry_info(rt_call_offset);</span>
<span class="line-added">725     break;</span>
<span class="line-added">726   }</span>
<span class="line-added">727   case CodeOffsets::Verified_Value_Entry: {</span>
<span class="line-added">728     if (needs_clinit_barrier_on_entry(method())) {</span>
<span class="line-added">729       clinit_barrier(method());</span>
<span class="line-added">730     }</span>
<span class="line-added">731     build_frame();</span>
<span class="line-added">732     offsets()-&gt;set_value(CodeOffsets::Frame_Complete, _masm-&gt;offset());</span>
<span class="line-added">733     break;</span>
<span class="line-added">734   }</span>
<span class="line-added">735   default:</span>
<span class="line-added">736     ShouldNotReachHere();</span>
<span class="line-added">737     break;</span>
<span class="line-added">738   }</span>
<span class="line-added">739 }</span>
740 
741 void LIR_Assembler::emit_op0(LIR_Op0* op) {
742   switch (op-&gt;code()) {
743     case lir_nop:
744       assert(op-&gt;info() == NULL, &quot;not supported&quot;);
745       _masm-&gt;nop();
746       break;
747 
748     case lir_label:
749       Unimplemented();
750       break;
751 
<span class="line-modified">752     case lir_std_entry:</span>












753       emit_std_entries();
754       break;
755 
756     case lir_osr_entry:
757       offsets()-&gt;set_value(CodeOffsets::OSR_Entry, _masm-&gt;offset());
758       osr_entry();
759       break;
760 
761 #ifdef IA32
762     case lir_fpop_raw:
763       fpop();
764       break;
765 #endif // IA32
766 
767     case lir_breakpoint:
768       breakpoint();
769       break;
770 
771     case lir_membar:
772       membar();
</pre>
<hr />
<pre>
787     case lir_membar_storestore:
788       membar_storestore();
789       break;
790 
791     case lir_membar_loadstore:
792       membar_loadstore();
793       break;
794 
795     case lir_membar_storeload:
796       membar_storeload();
797       break;
798 
799     case lir_get_thread:
800       get_thread(op-&gt;result_opr());
801       break;
802 
803     case lir_on_spin_wait:
804       on_spin_wait();
805       break;
806 
<span class="line-added">807     case lir_check_orig_pc:</span>
<span class="line-added">808       check_orig_pc();</span>
<span class="line-added">809       break;</span>
<span class="line-added">810 </span>
811     default:
812       ShouldNotReachHere();
813       break;
814   }
815 }
816 
817 
818 void LIR_Assembler::emit_op2(LIR_Op2* op) {
819   switch (op-&gt;code()) {
820     case lir_cmp:
821       if (op-&gt;info() != NULL) {
822         assert(op-&gt;in_opr1()-&gt;is_address() || op-&gt;in_opr2()-&gt;is_address(),
823                &quot;shouldn&#39;t be codeemitinfo for non-address operands&quot;);
824         add_debug_info_for_null_check_here(op-&gt;info()); // exception possible
825       }
826       comp_op(op-&gt;condition(), op-&gt;in_opr1(), op-&gt;in_opr2(), op);
827       break;
828 
829     case lir_cmp_l2i:
830     case lir_cmp_fd2i:
</pre>
<hr />
<pre>
884         op-&gt;result_opr());
885       break;
886 
887     case lir_throw:
888       throw_op(op-&gt;in_opr1(), op-&gt;in_opr2(), op-&gt;info());
889       break;
890 
891     case lir_xadd:
892     case lir_xchg:
893       atomic_op(op-&gt;code(), op-&gt;in_opr1(), op-&gt;in_opr2(), op-&gt;result_opr(), op-&gt;tmp1_opr());
894       break;
895 
896     default:
897       Unimplemented();
898       break;
899   }
900 }
901 
902 
903 void LIR_Assembler::build_frame() {
<span class="line-modified">904   _masm-&gt;build_frame(initial_frame_size_in_bytes(), bang_size_in_bytes(), in_bytes(frame_map()-&gt;sp_offset_for_orig_pc()),</span>
<span class="line-added">905                      needs_stack_repair(), method()-&gt;has_scalarized_args(), &amp;_verified_value_entry);</span>
906 }
907 
908 
909 void LIR_Assembler::roundfp_op(LIR_Opr src, LIR_Opr tmp, LIR_Opr dest, bool pop_fpu_stack) {
910   assert(strict_fp_requires_explicit_rounding, &quot;not required&quot;);
911   assert((src-&gt;is_single_fpu() &amp;&amp; dest-&gt;is_single_stack()) ||
912          (src-&gt;is_double_fpu() &amp;&amp; dest-&gt;is_double_stack()),
913          &quot;round_fp: rounds register -&gt; stack location&quot;);
914 
915   reg2stack (src, dest, src-&gt;type(), pop_fpu_stack);
916 }
917 
918 
919 void LIR_Assembler::move_op(LIR_Opr src, LIR_Opr dest, BasicType type, LIR_PatchCode patch_code, CodeEmitInfo* info, bool pop_fpu_stack, bool unaligned, bool wide) {
920   if (src-&gt;is_register()) {
921     if (dest-&gt;is_register()) {
922       assert(patch_code == lir_patch_none &amp;&amp; info == NULL, &quot;no patching and info allowed here&quot;);
923       reg2reg(src,  dest);
924     } else if (dest-&gt;is_stack()) {
925       assert(patch_code == lir_patch_none &amp;&amp; info == NULL, &quot;no patching and info allowed here&quot;);
</pre>
</td>
</tr>
</table>
<center><a href="c1_LIR.hpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="../classfile/classFileParser.cpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>