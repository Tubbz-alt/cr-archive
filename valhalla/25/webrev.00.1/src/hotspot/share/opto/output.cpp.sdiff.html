<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/share/opto/output.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="node.hpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="output.hpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/share/opto/output.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
 224 };
 225 
 226 
 227 PhaseOutput::PhaseOutput()
 228   : Phase(Phase::Output),
 229     _code_buffer(&quot;Compile::Fill_buffer&quot;),
 230     _first_block_size(0),
 231     _handler_table(),
 232     _inc_table(),
 233     _oop_map_set(NULL),
 234     _scratch_buffer_blob(NULL),
 235     _scratch_locs_memory(NULL),
 236     _scratch_const_size(-1),
 237     _in_scratch_emit_size(false),
 238     _frame_slots(0),
 239     _code_offsets(),
 240     _node_bundling_limit(0),
 241     _node_bundling_base(NULL),
 242     _orig_pc_slot(0),
 243     _orig_pc_slot_offset_in_bytes(0),


 244     _buf_sizes(),
 245     _block(NULL),
 246     _index(0) {
 247   C-&gt;set_output(this);
 248   if (C-&gt;stub_name() == NULL) {
<span class="line-modified"> 249     _orig_pc_slot = C-&gt;fixed_slots() - (sizeof(address) / VMRegImpl::stack_slot_size);</span>





 250   }
 251 }
 252 
 253 PhaseOutput::~PhaseOutput() {
 254   C-&gt;set_output(NULL);
 255   if (_scratch_buffer_blob != NULL) {
 256     BufferBlob::free(_scratch_buffer_blob);
 257   }
 258 }
 259 
 260 void PhaseOutput::perform_mach_node_analysis() {
 261   // Late barrier analysis must be done after schedule and bundle
 262   // Otherwise liveness based spilling will fail
 263   BarrierSetC2* bs = BarrierSet::barrier_set()-&gt;barrier_set_c2();
 264   bs-&gt;late_barrier_analysis();
 265 
 266   pd_perform_mach_node_analysis();
 267 }
 268 
 269 // Convert Nodes to instruction bits and pass off to the VM
 270 void PhaseOutput::Output() {
 271   // RootNode goes
 272   assert( C-&gt;cfg()-&gt;get_root_block()-&gt;number_of_nodes() == 0, &quot;&quot; );
 273 
 274   // The number of new nodes (mostly MachNop) is proportional to
 275   // the number of java calls and inner loops which are aligned.
 276   if ( C-&gt;check_node_count((NodeLimitFudgeFactor + C-&gt;java_calls()*3 +
 277                             C-&gt;inner_loops()*(OptoLoopAlignment-1)),
 278                            &quot;out of nodes before code generation&quot; ) ) {
 279     return;
 280   }
 281   // Make sure I can find the Start Node
 282   Block *entry = C-&gt;cfg()-&gt;get_block(1);
 283   Block *broot = C-&gt;cfg()-&gt;get_root_block();
 284 
 285   const StartNode *start = entry-&gt;head()-&gt;as_Start();
 286 
 287   // Replace StartNode with prolog
<span class="line-modified"> 288   MachPrologNode *prolog = new MachPrologNode();</span>

 289   entry-&gt;map_node(prolog, 0);
 290   C-&gt;cfg()-&gt;map_node_to_block(prolog, entry);
 291   C-&gt;cfg()-&gt;unmap_node_from_block(start); // start is no longer in any block
 292 
 293   // Virtual methods need an unverified entry point
<span class="line-modified"> 294 </span>
<span class="line-modified"> 295   if( C-&gt;is_osr_compilation() ) {</span>
<span class="line-removed"> 296     if( PoisonOSREntry ) {</span>
 297       // TODO: Should use a ShouldNotReachHereNode...
 298       C-&gt;cfg()-&gt;insert( broot, 0, new MachBreakpointNode() );
 299     }
 300   } else {
<span class="line-modified"> 301     if( C-&gt;method() &amp;&amp; !C-&gt;method()-&gt;flags().is_static() ) {</span>
<span class="line-modified"> 302       // Insert unvalidated entry point</span>
<span class="line-modified"> 303       C-&gt;cfg()-&gt;insert( broot, 0, new MachUEPNode() );</span>











 304     }
<span class="line-removed"> 305 </span>
 306   }
 307 
 308   // Break before main entry point
 309   if ((C-&gt;method() &amp;&amp; C-&gt;directive()-&gt;BreakAtExecuteOption) ||
 310       (OptoBreakpoint &amp;&amp; C-&gt;is_method_compilation())       ||
 311       (OptoBreakpointOSR &amp;&amp; C-&gt;is_osr_compilation())       ||
 312       (OptoBreakpointC2R &amp;&amp; !C-&gt;method())                   ) {
 313     // checking for C-&gt;method() means that OptoBreakpoint does not apply to
 314     // runtime stubs or frame converters
 315     C-&gt;cfg()-&gt;insert( entry, 1, new MachBreakpointNode() );
 316   }
 317 
 318   // Insert epilogs before every return
 319   for (uint i = 0; i &lt; C-&gt;cfg()-&gt;number_of_blocks(); i++) {
 320     Block* block = C-&gt;cfg()-&gt;get_block(i);
 321     if (!block-&gt;is_connector() &amp;&amp; block-&gt;non_connector_successor(0) == C-&gt;cfg()-&gt;get_root_block()) { // Found a program exit point?
 322       Node* m = block-&gt;end();
 323       if (m-&gt;is_Mach() &amp;&amp; m-&gt;as_Mach()-&gt;ideal_Opcode() != Op_Halt) {
 324         MachEpilogNode* epilog = new MachEpilogNode(m-&gt;as_Mach()-&gt;ideal_Opcode() == Op_Return);
 325         block-&gt;add_inst(epilog);
 326         C-&gt;cfg()-&gt;map_node_to_block(epilog, block);
 327       }
 328     }
 329   }
 330 
 331   // Keeper of sizing aspects
 332   _buf_sizes = BufferSizingData();
 333 
 334   // Initialize code buffer
 335   estimate_buffer_size(_buf_sizes._const);
 336   if (C-&gt;failing()) return;
 337 
 338   // Pre-compute the length of blocks and replace
 339   // long branches with short if machine supports it.
 340   // Must be done before ScheduleAndBundle due to SPARC delay slots
 341   uint* blk_starts = NEW_RESOURCE_ARRAY(uint, C-&gt;cfg()-&gt;number_of_blocks() + 1);
 342   blk_starts[0] = 0;
 343   shorten_branches(blk_starts);
 344 

























 345   ScheduleAndBundle();
 346   if (C-&gt;failing()) {
 347     return;
 348   }
 349 
 350   perform_mach_node_analysis();
 351 
 352   // Complete sizing of codebuffer
 353   CodeBuffer* cb = init_buffer();
 354   if (cb == NULL || C-&gt;failing()) {
 355     return;
 356   }
 357 
 358   BuildOopMaps();
 359 
 360   if (C-&gt;failing())  {
 361     return;
 362   }
 363 
 364   fill_buffer(cb, blk_starts);
</pre>
<hr />
<pre>
 482     // Sum all instruction sizes to compute block size
 483     uint last_inst = block-&gt;number_of_nodes();
 484     uint blk_size = 0;
 485     for (uint j = 0; j &lt; last_inst; j++) {
 486       _index = j;
 487       Node* nj = block-&gt;get_node(_index);
 488       // Handle machine instruction nodes
 489       if (nj-&gt;is_Mach()) {
 490         MachNode* mach = nj-&gt;as_Mach();
 491         blk_size += (mach-&gt;alignment_required() - 1) * relocInfo::addr_unit(); // assume worst case padding
 492         reloc_size += mach-&gt;reloc();
 493         if (mach-&gt;is_MachCall()) {
 494           // add size information for trampoline stub
 495           // class CallStubImpl is platform-specific and defined in the *.ad files.
 496           stub_size  += CallStubImpl::size_call_trampoline();
 497           reloc_size += CallStubImpl::reloc_call_trampoline();
 498 
 499           MachCallNode *mcall = mach-&gt;as_MachCall();
 500           // This destination address is NOT PC-relative
 501 
<span class="line-modified"> 502           mcall-&gt;method_set((intptr_t)mcall-&gt;entry_point());</span>


 503 
 504           if (mcall-&gt;is_MachCallJava() &amp;&amp; mcall-&gt;as_MachCallJava()-&gt;_method) {
 505             stub_size  += CompiledStaticCall::to_interp_stub_size();
 506             reloc_size += CompiledStaticCall::reloc_to_interp_stub();
 507 #if INCLUDE_AOT
 508             stub_size  += CompiledStaticCall::to_aot_stub_size();
 509             reloc_size += CompiledStaticCall::reloc_to_aot_stub();
 510 #endif
 511           }
 512         } else if (mach-&gt;is_MachSafePoint()) {
 513           // If call/safepoint are adjacent, account for possible
 514           // nop to disambiguate the two safepoints.
 515           // ScheduleAndBundle() can rearrange nodes in a block,
 516           // check for all offsets inside this block.
 517           if (last_call_adr &gt;= blk_starts[i]) {
 518             blk_size += nop_size;
 519           }
 520         }
 521         if (mach-&gt;avoid_back_to_back(MachNode::AVOID_BEFORE)) {
 522           // Nop is inserted between &quot;avoid back to back&quot; instructions.
</pre>
<hr />
<pre>
 923       ShouldNotReachHere();
 924       break;
 925   }
 926 }
 927 
 928 // Determine if this node starts a bundle
 929 bool PhaseOutput::starts_bundle(const Node *n) const {
 930   return (_node_bundling_limit &gt; n-&gt;_idx &amp;&amp;
 931           _node_bundling_base[n-&gt;_idx].starts_bundle());
 932 }
 933 
 934 //--------------------------Process_OopMap_Node--------------------------------
 935 void PhaseOutput::Process_OopMap_Node(MachNode *mach, int current_offset) {
 936   // Handle special safepoint nodes for synchronization
 937   MachSafePointNode *sfn   = mach-&gt;as_MachSafePoint();
 938   MachCallNode      *mcall;
 939 
 940   int safepoint_pc_offset = current_offset;
 941   bool is_method_handle_invoke = false;
 942   bool return_oop = false;

 943 
 944   // Add the safepoint in the DebugInfoRecorder
 945   if( !mach-&gt;is_MachCall() ) {
 946     mcall = NULL;
 947     C-&gt;debug_info()-&gt;add_safepoint(safepoint_pc_offset, sfn-&gt;_oop_map);
 948   } else {
 949     mcall = mach-&gt;as_MachCall();
 950 
 951     // Is the call a MethodHandle call?
 952     if (mcall-&gt;is_MachCallJava()) {
 953       if (mcall-&gt;as_MachCallJava()-&gt;_method_handle_invoke) {
 954         assert(C-&gt;has_method_handle_invokes(), &quot;must have been set during call generation&quot;);
 955         is_method_handle_invoke = true;
 956       }
 957     }
 958 
 959     // Check if a call returns an object.
<span class="line-modified"> 960     if (mcall-&gt;returns_pointer()) {</span>
 961       return_oop = true;
 962     }



 963     safepoint_pc_offset += mcall-&gt;ret_addr_offset();
 964     C-&gt;debug_info()-&gt;add_safepoint(safepoint_pc_offset, mcall-&gt;_oop_map);
 965   }
 966 
 967   // Loop over the JVMState list to add scope information
 968   // Do not skip safepoints with a NULL method, they need monitor info
 969   JVMState* youngest_jvms = sfn-&gt;jvms();
 970   int max_depth = youngest_jvms-&gt;depth();
 971 
 972   // Allocate the object pool for scalar-replaced objects -- the map from
 973   // small-integer keys (which can be recorded in the local and ostack
 974   // arrays) to descriptions of the object state.
 975   GrowableArray&lt;ScopeValue*&gt; *objs = new GrowableArray&lt;ScopeValue*&gt;();
 976 
 977   // Visit scopes from oldest to youngest.
 978   for (int depth = 1; depth &lt;= max_depth; depth++) {
 979     JVMState* jvms = youngest_jvms-&gt;of_depth(depth);
 980     int idx;
 981     ciMethod* method = jvms-&gt;has_method() ? jvms-&gt;method() : NULL;
 982     // Safepoints that do not have method() set only provide oop-map and monitor info
</pre>
<hr />
<pre>
1057       bool eliminated = (box_node-&gt;is_BoxLock() &amp;&amp; box_node-&gt;as_BoxLock()-&gt;is_eliminated());
1058       monarray-&gt;append(new MonitorValue(scval, basic_lock, eliminated));
1059     }
1060 
1061     // We dump the object pool first, since deoptimization reads it in first.
1062     C-&gt;debug_info()-&gt;dump_object_pool(objs);
1063 
1064     // Build first class objects to pass to scope
1065     DebugToken *locvals = C-&gt;debug_info()-&gt;create_scope_values(locarray);
1066     DebugToken *expvals = C-&gt;debug_info()-&gt;create_scope_values(exparray);
1067     DebugToken *monvals = C-&gt;debug_info()-&gt;create_monitor_values(monarray);
1068 
1069     // Make method available for all Safepoints
1070     ciMethod* scope_method = method ? method : C-&gt;method();
1071     // Describe the scope here
1072     assert(jvms-&gt;bci() &gt;= InvocationEntryBci &amp;&amp; jvms-&gt;bci() &lt;= 0x10000, &quot;must be a valid or entry BCI&quot;);
1073     assert(!jvms-&gt;should_reexecute() || depth == max_depth, &quot;reexecute allowed only for the youngest&quot;);
1074     // Now we can describe the scope.
1075     methodHandle null_mh;
1076     bool rethrow_exception = false;
<span class="line-modified">1077     C-&gt;debug_info()-&gt;describe_scope(safepoint_pc_offset, null_mh, scope_method, jvms-&gt;bci(), jvms-&gt;should_reexecute(), rethrow_exception, is_method_handle_invoke, return_oop, locvals, expvals, monvals);</span>
1078   } // End jvms loop
1079 
1080   // Mark the end of the scope set.
1081   C-&gt;debug_info()-&gt;end_safepoint(safepoint_pc_offset);
1082 }
1083 
1084 
1085 
1086 // A simplified version of Process_OopMap_Node, to handle non-safepoints.
1087 class NonSafepointEmitter {
1088     Compile*  C;
1089     JVMState* _pending_jvms;
1090     int       _pending_offset;
1091 
1092     void emit_non_safepoint();
1093 
1094  public:
1095     NonSafepointEmitter(Compile* compile) {
1096       this-&gt;C = compile;
1097       _pending_jvms = NULL;
</pre>
<hr />
<pre>
1162   }
1163 
1164   // Mark the end of the scope set.
1165   debug_info-&gt;end_non_safepoint(pc_offset);
1166 }
1167 
1168 //------------------------------init_buffer------------------------------------
1169 void PhaseOutput::estimate_buffer_size(int&amp; const_req) {
1170 
1171   // Set the initially allocated size
1172   const_req = initial_const_capacity;
1173 
1174   // The extra spacing after the code is necessary on some platforms.
1175   // Sometimes we need to patch in a jump after the last instruction,
1176   // if the nmethod has been deoptimized.  (See 4932387, 4894843.)
1177 
1178   // Compute the byte offset where we can store the deopt pc.
1179   if (C-&gt;fixed_slots() != 0) {
1180     _orig_pc_slot_offset_in_bytes = C-&gt;regalloc()-&gt;reg2offset(OptoReg::stack2reg(_orig_pc_slot));
1181   }




1182 
1183   // Compute prolog code size
1184   _method_size = 0;
1185   _frame_slots = OptoReg::reg2stack(C-&gt;matcher()-&gt;_old_SP) + C-&gt;regalloc()-&gt;_framesize;
1186 #if defined(IA64) &amp;&amp; !defined(AIX)
1187   if (save_argument_registers()) {
1188     // 4815101: this is a stub with implicit and unknown precision fp args.
1189     // The usual spill mechanism can only generate stfd&#39;s in this case, which
1190     // doesn&#39;t work if the fp reg to spill contains a single-precision denorm.
1191     // Instead, we hack around the normal spill mechanism using stfspill&#39;s and
1192     // ldffill&#39;s in the MachProlog and MachEpilog emit methods.  We allocate
1193     // space here for the fp arg regs (f8-f15) we&#39;re going to thusly spill.
1194     //
1195     // If we ever implement 16-byte &#39;registers&#39; == stack slots, we can
1196     // get rid of this hack and have SpillCopy generate stfspill/ldffill
1197     // instead of stfd/stfs/ldfd/ldfs.
1198     _frame_slots += 8*(16/BytesPerInt);
1199   }
1200 #endif
1201   assert(_frame_slots &gt;= 0 &amp;&amp; _frame_slots &lt; 1000000, &quot;sanity check&quot;);
</pre>
<hr />
<pre>
1438           int nops_cnt = padding / nop_size;
1439           MachNode *nop = new MachNopNode(nops_cnt);
1440           block-&gt;insert_node(nop, j++);
1441           last_inst++;
1442           C-&gt;cfg()-&gt;map_node_to_block(nop, block);
1443           // Ensure enough space.
1444           cb-&gt;insts()-&gt;maybe_expand_to_ensure_remaining(MAX_inst_size);
1445           if ((cb-&gt;blob() == NULL) || (!CompileBroker::should_compile_new_jobs())) {
1446             C-&gt;record_failure(&quot;CodeCache is full&quot;);
1447             return;
1448           }
1449           nop-&gt;emit(*cb, C-&gt;regalloc());
1450           cb-&gt;flush_bundle(true);
1451           current_offset = cb-&gt;insts_size();
1452         }
1453 
1454         // Remember the start of the last call in a basic block
1455         if (is_mcall) {
1456           MachCallNode *mcall = mach-&gt;as_MachCall();
1457 
<span class="line-modified">1458           // This destination address is NOT PC-relative</span>
<span class="line-modified">1459           mcall-&gt;method_set((intptr_t)mcall-&gt;entry_point());</span>


1460 
1461           // Save the return address
1462           call_returns[block-&gt;_pre_order] = current_offset + mcall-&gt;ret_addr_offset();
1463 
1464           if (mcall-&gt;is_MachCallLeaf()) {
1465             is_mcall = false;
1466             is_sfn = false;
1467           }
1468         }
1469 
1470         // sfn will be valid whenever mcall is valid now because of inheritance
1471         if (is_sfn || is_mcall) {
1472 
1473           // Handle special safepoint nodes for synchronization
1474           if (!is_mcall) {
1475             MachSafePointNode *sfn = mach-&gt;as_MachSafePoint();
1476             // !!!!! Stubs only need an oopmap right now, so bail out
1477             if (sfn-&gt;jvms()-&gt;method() == NULL) {
1478               // Write the oopmap directly to the code blob??!!
1479               continue;
</pre>
<hr />
<pre>
3161 }
3162 #endif
3163 
3164 //-----------------------init_scratch_buffer_blob------------------------------
3165 // Construct a temporary BufferBlob and cache it for this compile.
3166 void PhaseOutput::init_scratch_buffer_blob(int const_size) {
3167   // If there is already a scratch buffer blob allocated and the
3168   // constant section is big enough, use it.  Otherwise free the
3169   // current and allocate a new one.
3170   BufferBlob* blob = scratch_buffer_blob();
3171   if ((blob != NULL) &amp;&amp; (const_size &lt;= _scratch_const_size)) {
3172     // Use the current blob.
3173   } else {
3174     if (blob != NULL) {
3175       BufferBlob::free(blob);
3176     }
3177 
3178     ResourceMark rm;
3179     _scratch_const_size = const_size;
3180     int size = C2Compiler::initial_code_buffer_size(const_size);






3181     blob = BufferBlob::create(&quot;Compile::scratch_buffer&quot;, size);
3182     // Record the buffer blob for next time.
3183     set_scratch_buffer_blob(blob);
3184     // Have we run out of code space?
3185     if (scratch_buffer_blob() == NULL) {
3186       // Let CompilerBroker disable further compilations.
3187       C-&gt;record_failure(&quot;Not enough space for scratch buffer in CodeCache&quot;);
3188       return;
3189     }
3190   }
3191 
3192   // Initialize the relocation buffers
3193   relocInfo* locs_buf = (relocInfo*) blob-&gt;content_end() - MAX_locs_size;
3194   set_scratch_locs_memory(locs_buf);
3195 }
3196 
3197 
3198 //-----------------------scratch_emit_size-------------------------------------
3199 // Helper function that computes size by emitting code
3200 uint PhaseOutput::scratch_emit_size(const Node* n) {
</pre>
<hr />
<pre>
3225   int lsize = MAX_locs_size / 3;
3226   buf.consts()-&gt;initialize_shared_locs(&amp;locs_buf[lsize * 0], lsize);
3227   buf.insts()-&gt;initialize_shared_locs( &amp;locs_buf[lsize * 1], lsize);
3228   buf.stubs()-&gt;initialize_shared_locs( &amp;locs_buf[lsize * 2], lsize);
3229   // Mark as scratch buffer.
3230   buf.consts()-&gt;set_scratch_emit();
3231   buf.insts()-&gt;set_scratch_emit();
3232   buf.stubs()-&gt;set_scratch_emit();
3233 
3234   // Do the emission.
3235 
3236   Label fakeL; // Fake label for branch instructions.
3237   Label*   saveL = NULL;
3238   uint save_bnum = 0;
3239   bool is_branch = n-&gt;is_MachBranch();
3240   if (is_branch) {
3241     MacroAssembler masm(&amp;buf);
3242     masm.bind(fakeL);
3243     n-&gt;as_MachBranch()-&gt;save_label(&amp;saveL, &amp;save_bnum);
3244     n-&gt;as_MachBranch()-&gt;label_set(&amp;fakeL, 0);






3245   }
3246   n-&gt;emit(buf, C-&gt;regalloc());
3247 
3248   // Emitting into the scratch buffer should not fail
3249   assert (!C-&gt;failing(), &quot;Must not have pending failure. Reason is: %s&quot;, C-&gt;failure_reason());
3250 
<span class="line-modified">3251   if (is_branch) // Restore label.</span>

3252     n-&gt;as_MachBranch()-&gt;label_set(saveL, save_bnum);





3253 
3254   // End scratch_emit_size section.
3255   set_in_scratch_emit_size(false);
3256 
3257   return buf.insts_size();
3258 }
3259 
3260 void PhaseOutput::install() {
3261   if (C-&gt;stub_function() != NULL) {
3262     install_stub(C-&gt;stub_name(),
3263                  C-&gt;save_argument_registers());
3264   } else {
3265     install_code(C-&gt;method(),
3266                  C-&gt;entry_bci(),
3267                  CompileBroker::compiler2(),
3268                  C-&gt;has_unsafe_access(),
3269                  SharedRuntime::is_wide_vector(C-&gt;max_vector_size()),
3270                  C-&gt;rtm_state());
3271   }
3272 }
</pre>
<hr />
<pre>
3275                                int               entry_bci,
3276                                AbstractCompiler* compiler,
3277                                bool              has_unsafe_access,
3278                                bool              has_wide_vectors,
3279                                RTMState          rtm_state) {
3280   // Check if we want to skip execution of all compiled code.
3281   {
3282 #ifndef PRODUCT
3283     if (OptoNoExecute) {
3284       C-&gt;record_method_not_compilable(&quot;+OptoNoExecute&quot;);  // Flag as failed
3285       return;
3286     }
3287 #endif
3288     Compile::TracePhase tp(&quot;install_code&quot;, &amp;timers[_t_registerMethod]);
3289 
3290     if (C-&gt;is_osr_compilation()) {
3291       _code_offsets.set_value(CodeOffsets::Verified_Entry, 0);
3292       _code_offsets.set_value(CodeOffsets::OSR_Entry, _first_block_size);
3293     } else {
3294       _code_offsets.set_value(CodeOffsets::Verified_Entry, _first_block_size);









3295       _code_offsets.set_value(CodeOffsets::OSR_Entry, 0);
3296     }
3297 
3298     C-&gt;env()-&gt;register_method(target,
<span class="line-modified">3299                                      entry_bci,</span>
<span class="line-modified">3300                                      &amp;_code_offsets,</span>
<span class="line-modified">3301                                      _orig_pc_slot_offset_in_bytes,</span>
<span class="line-modified">3302                                      code_buffer(),</span>
<span class="line-modified">3303                                      frame_size_in_words(),</span>
<span class="line-modified">3304                                      oop_map_set(),</span>
<span class="line-modified">3305                                      &amp;_handler_table,</span>
<span class="line-modified">3306                                      inc_table(),</span>
<span class="line-modified">3307                                      compiler,</span>
<span class="line-modified">3308                                      has_unsafe_access,</span>
<span class="line-modified">3309                                      SharedRuntime::is_wide_vector(C-&gt;max_vector_size()),</span>
<span class="line-modified">3310                                      C-&gt;rtm_state());</span>
3311 
3312     if (C-&gt;log() != NULL) { // Print code cache state into compiler log
3313       C-&gt;log()-&gt;code_cache_state();
3314     }
3315   }
3316 }
3317 void PhaseOutput::install_stub(const char* stub_name,
3318                                bool        caller_must_gc_arguments) {
3319   // Entry point will be accessed using stub_entry_point();
3320   if (code_buffer() == NULL) {
3321     Matcher::soft_match_failure();
3322   } else {
3323     if (PrintAssembly &amp;&amp; (WizardMode || Verbose))
3324       tty-&gt;print_cr(&quot;### Stub::%s&quot;, stub_name);
3325 
3326     if (!C-&gt;failing()) {
3327       assert(C-&gt;fixed_slots() == 0, &quot;no fixed slots used for runtime stubs&quot;);
3328 
3329       // Make the NMethod
3330       // For now we mark the frame as never safe for profile stackwalking
</pre>
</td>
<td>
<hr />
<pre>
 224 };
 225 
 226 
 227 PhaseOutput::PhaseOutput()
 228   : Phase(Phase::Output),
 229     _code_buffer(&quot;Compile::Fill_buffer&quot;),
 230     _first_block_size(0),
 231     _handler_table(),
 232     _inc_table(),
 233     _oop_map_set(NULL),
 234     _scratch_buffer_blob(NULL),
 235     _scratch_locs_memory(NULL),
 236     _scratch_const_size(-1),
 237     _in_scratch_emit_size(false),
 238     _frame_slots(0),
 239     _code_offsets(),
 240     _node_bundling_limit(0),
 241     _node_bundling_base(NULL),
 242     _orig_pc_slot(0),
 243     _orig_pc_slot_offset_in_bytes(0),
<span class="line-added"> 244     _sp_inc_slot(0),</span>
<span class="line-added"> 245     _sp_inc_slot_offset_in_bytes(0),</span>
 246     _buf_sizes(),
 247     _block(NULL),
 248     _index(0) {
 249   C-&gt;set_output(this);
 250   if (C-&gt;stub_name() == NULL) {
<span class="line-modified"> 251     int fixed_slots = C-&gt;fixed_slots();</span>
<span class="line-added"> 252     if (C-&gt;needs_stack_repair()) {</span>
<span class="line-added"> 253       fixed_slots -= 2;</span>
<span class="line-added"> 254       _sp_inc_slot = fixed_slots;</span>
<span class="line-added"> 255     }</span>
<span class="line-added"> 256     _orig_pc_slot = fixed_slots - (sizeof(address) / VMRegImpl::stack_slot_size);</span>
 257   }
 258 }
 259 
 260 PhaseOutput::~PhaseOutput() {
 261   C-&gt;set_output(NULL);
 262   if (_scratch_buffer_blob != NULL) {
 263     BufferBlob::free(_scratch_buffer_blob);
 264   }
 265 }
 266 
 267 void PhaseOutput::perform_mach_node_analysis() {
 268   // Late barrier analysis must be done after schedule and bundle
 269   // Otherwise liveness based spilling will fail
 270   BarrierSetC2* bs = BarrierSet::barrier_set()-&gt;barrier_set_c2();
 271   bs-&gt;late_barrier_analysis();
 272 
 273   pd_perform_mach_node_analysis();
 274 }
 275 
 276 // Convert Nodes to instruction bits and pass off to the VM
 277 void PhaseOutput::Output() {
 278   // RootNode goes
 279   assert( C-&gt;cfg()-&gt;get_root_block()-&gt;number_of_nodes() == 0, &quot;&quot; );
 280 
 281   // The number of new nodes (mostly MachNop) is proportional to
 282   // the number of java calls and inner loops which are aligned.
 283   if ( C-&gt;check_node_count((NodeLimitFudgeFactor + C-&gt;java_calls()*3 +
 284                             C-&gt;inner_loops()*(OptoLoopAlignment-1)),
 285                            &quot;out of nodes before code generation&quot; ) ) {
 286     return;
 287   }
 288   // Make sure I can find the Start Node
 289   Block *entry = C-&gt;cfg()-&gt;get_block(1);
 290   Block *broot = C-&gt;cfg()-&gt;get_root_block();
 291 
 292   const StartNode *start = entry-&gt;head()-&gt;as_Start();
 293 
 294   // Replace StartNode with prolog
<span class="line-modified"> 295   Label verified_entry;</span>
<span class="line-added"> 296   MachPrologNode* prolog = new MachPrologNode(&amp;verified_entry);</span>
 297   entry-&gt;map_node(prolog, 0);
 298   C-&gt;cfg()-&gt;map_node_to_block(prolog, entry);
 299   C-&gt;cfg()-&gt;unmap_node_from_block(start); // start is no longer in any block
 300 
 301   // Virtual methods need an unverified entry point
<span class="line-modified"> 302   if (C-&gt;is_osr_compilation()) {</span>
<span class="line-modified"> 303     if (PoisonOSREntry) {</span>

 304       // TODO: Should use a ShouldNotReachHereNode...
 305       C-&gt;cfg()-&gt;insert( broot, 0, new MachBreakpointNode() );
 306     }
 307   } else {
<span class="line-modified"> 308     if (C-&gt;method()) {</span>
<span class="line-modified"> 309       if (C-&gt;method()-&gt;has_scalarized_args()) {</span>
<span class="line-modified"> 310         // Add entry point to unpack all value type arguments</span>
<span class="line-added"> 311         C-&gt;cfg()-&gt;insert(broot, 0, new MachVEPNode(&amp;verified_entry, /* verified */ true, /* receiver_only */ false));</span>
<span class="line-added"> 312         if (!C-&gt;method()-&gt;is_static()) {</span>
<span class="line-added"> 313           // Add verified/unverified entry points to only unpack value type receiver at interface calls</span>
<span class="line-added"> 314           C-&gt;cfg()-&gt;insert(broot, 0, new MachVEPNode(&amp;verified_entry, /* verified */ false, /* receiver_only */ false));</span>
<span class="line-added"> 315           C-&gt;cfg()-&gt;insert(broot, 0, new MachVEPNode(&amp;verified_entry, /* verified */ true,  /* receiver_only */ true));</span>
<span class="line-added"> 316           C-&gt;cfg()-&gt;insert(broot, 0, new MachVEPNode(&amp;verified_entry, /* verified */ false, /* receiver_only */ true));</span>
<span class="line-added"> 317         }</span>
<span class="line-added"> 318       } else if (!C-&gt;method()-&gt;is_static()) {</span>
<span class="line-added"> 319         // Insert unvalidated entry point</span>
<span class="line-added"> 320         C-&gt;cfg()-&gt;insert(broot, 0, new MachUEPNode());</span>
<span class="line-added"> 321       }</span>
 322     }

 323   }
 324 
 325   // Break before main entry point
 326   if ((C-&gt;method() &amp;&amp; C-&gt;directive()-&gt;BreakAtExecuteOption) ||
 327       (OptoBreakpoint &amp;&amp; C-&gt;is_method_compilation())       ||
 328       (OptoBreakpointOSR &amp;&amp; C-&gt;is_osr_compilation())       ||
 329       (OptoBreakpointC2R &amp;&amp; !C-&gt;method())                   ) {
 330     // checking for C-&gt;method() means that OptoBreakpoint does not apply to
 331     // runtime stubs or frame converters
 332     C-&gt;cfg()-&gt;insert( entry, 1, new MachBreakpointNode() );
 333   }
 334 
 335   // Insert epilogs before every return
 336   for (uint i = 0; i &lt; C-&gt;cfg()-&gt;number_of_blocks(); i++) {
 337     Block* block = C-&gt;cfg()-&gt;get_block(i);
 338     if (!block-&gt;is_connector() &amp;&amp; block-&gt;non_connector_successor(0) == C-&gt;cfg()-&gt;get_root_block()) { // Found a program exit point?
 339       Node* m = block-&gt;end();
 340       if (m-&gt;is_Mach() &amp;&amp; m-&gt;as_Mach()-&gt;ideal_Opcode() != Op_Halt) {
 341         MachEpilogNode* epilog = new MachEpilogNode(m-&gt;as_Mach()-&gt;ideal_Opcode() == Op_Return);
 342         block-&gt;add_inst(epilog);
 343         C-&gt;cfg()-&gt;map_node_to_block(epilog, block);
 344       }
 345     }
 346   }
 347 
 348   // Keeper of sizing aspects
 349   _buf_sizes = BufferSizingData();
 350 
 351   // Initialize code buffer
 352   estimate_buffer_size(_buf_sizes._const);
 353   if (C-&gt;failing()) return;
 354 
 355   // Pre-compute the length of blocks and replace
 356   // long branches with short if machine supports it.
 357   // Must be done before ScheduleAndBundle due to SPARC delay slots
 358   uint* blk_starts = NEW_RESOURCE_ARRAY(uint, C-&gt;cfg()-&gt;number_of_blocks() + 1);
 359   blk_starts[0] = 0;
 360   shorten_branches(blk_starts);
 361 
<span class="line-added"> 362   if (!C-&gt;is_osr_compilation() &amp;&amp; C-&gt;has_scalarized_args()) {</span>
<span class="line-added"> 363     // Compute the offsets of the entry points required by the value type calling convention</span>
<span class="line-added"> 364     if (!C-&gt;method()-&gt;is_static()) {</span>
<span class="line-added"> 365       // We have entries at the beginning of the method, implemented by the first 4 nodes.</span>
<span class="line-added"> 366       // Entry                     (unverified) @ offset 0</span>
<span class="line-added"> 367       // Verified_Value_Entry_RO</span>
<span class="line-added"> 368       // Value_Entry               (unverified)</span>
<span class="line-added"> 369       // Verified_Value_Entry</span>
<span class="line-added"> 370       uint offset = 0;</span>
<span class="line-added"> 371       _code_offsets.set_value(CodeOffsets::Entry, offset);</span>
<span class="line-added"> 372 </span>
<span class="line-added"> 373       offset += ((MachVEPNode*)broot-&gt;get_node(0))-&gt;size(C-&gt;regalloc());</span>
<span class="line-added"> 374       _code_offsets.set_value(CodeOffsets::Verified_Value_Entry_RO, offset);</span>
<span class="line-added"> 375 </span>
<span class="line-added"> 376       offset += ((MachVEPNode*)broot-&gt;get_node(1))-&gt;size(C-&gt;regalloc());</span>
<span class="line-added"> 377       _code_offsets.set_value(CodeOffsets::Value_Entry, offset);</span>
<span class="line-added"> 378 </span>
<span class="line-added"> 379       offset += ((MachVEPNode*)broot-&gt;get_node(2))-&gt;size(C-&gt;regalloc());</span>
<span class="line-added"> 380       _code_offsets.set_value(CodeOffsets::Verified_Value_Entry, offset);</span>
<span class="line-added"> 381     } else {</span>
<span class="line-added"> 382       _code_offsets.set_value(CodeOffsets::Entry, -1); // will be patched later</span>
<span class="line-added"> 383       _code_offsets.set_value(CodeOffsets::Verified_Value_Entry, 0);</span>
<span class="line-added"> 384     }</span>
<span class="line-added"> 385   }</span>
<span class="line-added"> 386 </span>
 387   ScheduleAndBundle();
 388   if (C-&gt;failing()) {
 389     return;
 390   }
 391 
 392   perform_mach_node_analysis();
 393 
 394   // Complete sizing of codebuffer
 395   CodeBuffer* cb = init_buffer();
 396   if (cb == NULL || C-&gt;failing()) {
 397     return;
 398   }
 399 
 400   BuildOopMaps();
 401 
 402   if (C-&gt;failing())  {
 403     return;
 404   }
 405 
 406   fill_buffer(cb, blk_starts);
</pre>
<hr />
<pre>
 524     // Sum all instruction sizes to compute block size
 525     uint last_inst = block-&gt;number_of_nodes();
 526     uint blk_size = 0;
 527     for (uint j = 0; j &lt; last_inst; j++) {
 528       _index = j;
 529       Node* nj = block-&gt;get_node(_index);
 530       // Handle machine instruction nodes
 531       if (nj-&gt;is_Mach()) {
 532         MachNode* mach = nj-&gt;as_Mach();
 533         blk_size += (mach-&gt;alignment_required() - 1) * relocInfo::addr_unit(); // assume worst case padding
 534         reloc_size += mach-&gt;reloc();
 535         if (mach-&gt;is_MachCall()) {
 536           // add size information for trampoline stub
 537           // class CallStubImpl is platform-specific and defined in the *.ad files.
 538           stub_size  += CallStubImpl::size_call_trampoline();
 539           reloc_size += CallStubImpl::reloc_call_trampoline();
 540 
 541           MachCallNode *mcall = mach-&gt;as_MachCall();
 542           // This destination address is NOT PC-relative
 543 
<span class="line-modified"> 544           if (mcall-&gt;entry_point() != NULL) {</span>
<span class="line-added"> 545             mcall-&gt;method_set((intptr_t)mcall-&gt;entry_point());</span>
<span class="line-added"> 546           }</span>
 547 
 548           if (mcall-&gt;is_MachCallJava() &amp;&amp; mcall-&gt;as_MachCallJava()-&gt;_method) {
 549             stub_size  += CompiledStaticCall::to_interp_stub_size();
 550             reloc_size += CompiledStaticCall::reloc_to_interp_stub();
 551 #if INCLUDE_AOT
 552             stub_size  += CompiledStaticCall::to_aot_stub_size();
 553             reloc_size += CompiledStaticCall::reloc_to_aot_stub();
 554 #endif
 555           }
 556         } else if (mach-&gt;is_MachSafePoint()) {
 557           // If call/safepoint are adjacent, account for possible
 558           // nop to disambiguate the two safepoints.
 559           // ScheduleAndBundle() can rearrange nodes in a block,
 560           // check for all offsets inside this block.
 561           if (last_call_adr &gt;= blk_starts[i]) {
 562             blk_size += nop_size;
 563           }
 564         }
 565         if (mach-&gt;avoid_back_to_back(MachNode::AVOID_BEFORE)) {
 566           // Nop is inserted between &quot;avoid back to back&quot; instructions.
</pre>
<hr />
<pre>
 967       ShouldNotReachHere();
 968       break;
 969   }
 970 }
 971 
 972 // Determine if this node starts a bundle
 973 bool PhaseOutput::starts_bundle(const Node *n) const {
 974   return (_node_bundling_limit &gt; n-&gt;_idx &amp;&amp;
 975           _node_bundling_base[n-&gt;_idx].starts_bundle());
 976 }
 977 
 978 //--------------------------Process_OopMap_Node--------------------------------
 979 void PhaseOutput::Process_OopMap_Node(MachNode *mach, int current_offset) {
 980   // Handle special safepoint nodes for synchronization
 981   MachSafePointNode *sfn   = mach-&gt;as_MachSafePoint();
 982   MachCallNode      *mcall;
 983 
 984   int safepoint_pc_offset = current_offset;
 985   bool is_method_handle_invoke = false;
 986   bool return_oop = false;
<span class="line-added"> 987   bool return_vt = false;</span>
 988 
 989   // Add the safepoint in the DebugInfoRecorder
 990   if( !mach-&gt;is_MachCall() ) {
 991     mcall = NULL;
 992     C-&gt;debug_info()-&gt;add_safepoint(safepoint_pc_offset, sfn-&gt;_oop_map);
 993   } else {
 994     mcall = mach-&gt;as_MachCall();
 995 
 996     // Is the call a MethodHandle call?
 997     if (mcall-&gt;is_MachCallJava()) {
 998       if (mcall-&gt;as_MachCallJava()-&gt;_method_handle_invoke) {
 999         assert(C-&gt;has_method_handle_invokes(), &quot;must have been set during call generation&quot;);
1000         is_method_handle_invoke = true;
1001       }
1002     }
1003 
1004     // Check if a call returns an object.
<span class="line-modified">1005     if (mcall-&gt;returns_pointer() || mcall-&gt;returns_vt()) {</span>
1006       return_oop = true;
1007     }
<span class="line-added">1008     if (mcall-&gt;returns_vt()) {</span>
<span class="line-added">1009       return_vt = true;</span>
<span class="line-added">1010     }</span>
1011     safepoint_pc_offset += mcall-&gt;ret_addr_offset();
1012     C-&gt;debug_info()-&gt;add_safepoint(safepoint_pc_offset, mcall-&gt;_oop_map);
1013   }
1014 
1015   // Loop over the JVMState list to add scope information
1016   // Do not skip safepoints with a NULL method, they need monitor info
1017   JVMState* youngest_jvms = sfn-&gt;jvms();
1018   int max_depth = youngest_jvms-&gt;depth();
1019 
1020   // Allocate the object pool for scalar-replaced objects -- the map from
1021   // small-integer keys (which can be recorded in the local and ostack
1022   // arrays) to descriptions of the object state.
1023   GrowableArray&lt;ScopeValue*&gt; *objs = new GrowableArray&lt;ScopeValue*&gt;();
1024 
1025   // Visit scopes from oldest to youngest.
1026   for (int depth = 1; depth &lt;= max_depth; depth++) {
1027     JVMState* jvms = youngest_jvms-&gt;of_depth(depth);
1028     int idx;
1029     ciMethod* method = jvms-&gt;has_method() ? jvms-&gt;method() : NULL;
1030     // Safepoints that do not have method() set only provide oop-map and monitor info
</pre>
<hr />
<pre>
1105       bool eliminated = (box_node-&gt;is_BoxLock() &amp;&amp; box_node-&gt;as_BoxLock()-&gt;is_eliminated());
1106       monarray-&gt;append(new MonitorValue(scval, basic_lock, eliminated));
1107     }
1108 
1109     // We dump the object pool first, since deoptimization reads it in first.
1110     C-&gt;debug_info()-&gt;dump_object_pool(objs);
1111 
1112     // Build first class objects to pass to scope
1113     DebugToken *locvals = C-&gt;debug_info()-&gt;create_scope_values(locarray);
1114     DebugToken *expvals = C-&gt;debug_info()-&gt;create_scope_values(exparray);
1115     DebugToken *monvals = C-&gt;debug_info()-&gt;create_monitor_values(monarray);
1116 
1117     // Make method available for all Safepoints
1118     ciMethod* scope_method = method ? method : C-&gt;method();
1119     // Describe the scope here
1120     assert(jvms-&gt;bci() &gt;= InvocationEntryBci &amp;&amp; jvms-&gt;bci() &lt;= 0x10000, &quot;must be a valid or entry BCI&quot;);
1121     assert(!jvms-&gt;should_reexecute() || depth == max_depth, &quot;reexecute allowed only for the youngest&quot;);
1122     // Now we can describe the scope.
1123     methodHandle null_mh;
1124     bool rethrow_exception = false;
<span class="line-modified">1125     C-&gt;debug_info()-&gt;describe_scope(safepoint_pc_offset, null_mh, scope_method, jvms-&gt;bci(), jvms-&gt;should_reexecute(), rethrow_exception, is_method_handle_invoke, return_oop, return_vt, locvals, expvals, monvals);</span>
1126   } // End jvms loop
1127 
1128   // Mark the end of the scope set.
1129   C-&gt;debug_info()-&gt;end_safepoint(safepoint_pc_offset);
1130 }
1131 
1132 
1133 
1134 // A simplified version of Process_OopMap_Node, to handle non-safepoints.
1135 class NonSafepointEmitter {
1136     Compile*  C;
1137     JVMState* _pending_jvms;
1138     int       _pending_offset;
1139 
1140     void emit_non_safepoint();
1141 
1142  public:
1143     NonSafepointEmitter(Compile* compile) {
1144       this-&gt;C = compile;
1145       _pending_jvms = NULL;
</pre>
<hr />
<pre>
1210   }
1211 
1212   // Mark the end of the scope set.
1213   debug_info-&gt;end_non_safepoint(pc_offset);
1214 }
1215 
1216 //------------------------------init_buffer------------------------------------
1217 void PhaseOutput::estimate_buffer_size(int&amp; const_req) {
1218 
1219   // Set the initially allocated size
1220   const_req = initial_const_capacity;
1221 
1222   // The extra spacing after the code is necessary on some platforms.
1223   // Sometimes we need to patch in a jump after the last instruction,
1224   // if the nmethod has been deoptimized.  (See 4932387, 4894843.)
1225 
1226   // Compute the byte offset where we can store the deopt pc.
1227   if (C-&gt;fixed_slots() != 0) {
1228     _orig_pc_slot_offset_in_bytes = C-&gt;regalloc()-&gt;reg2offset(OptoReg::stack2reg(_orig_pc_slot));
1229   }
<span class="line-added">1230   if (C-&gt;needs_stack_repair()) {</span>
<span class="line-added">1231     // Compute the byte offset of the stack increment value</span>
<span class="line-added">1232     _sp_inc_slot_offset_in_bytes = C-&gt;regalloc()-&gt;reg2offset(OptoReg::stack2reg(_sp_inc_slot));</span>
<span class="line-added">1233   }</span>
1234 
1235   // Compute prolog code size
1236   _method_size = 0;
1237   _frame_slots = OptoReg::reg2stack(C-&gt;matcher()-&gt;_old_SP) + C-&gt;regalloc()-&gt;_framesize;
1238 #if defined(IA64) &amp;&amp; !defined(AIX)
1239   if (save_argument_registers()) {
1240     // 4815101: this is a stub with implicit and unknown precision fp args.
1241     // The usual spill mechanism can only generate stfd&#39;s in this case, which
1242     // doesn&#39;t work if the fp reg to spill contains a single-precision denorm.
1243     // Instead, we hack around the normal spill mechanism using stfspill&#39;s and
1244     // ldffill&#39;s in the MachProlog and MachEpilog emit methods.  We allocate
1245     // space here for the fp arg regs (f8-f15) we&#39;re going to thusly spill.
1246     //
1247     // If we ever implement 16-byte &#39;registers&#39; == stack slots, we can
1248     // get rid of this hack and have SpillCopy generate stfspill/ldffill
1249     // instead of stfd/stfs/ldfd/ldfs.
1250     _frame_slots += 8*(16/BytesPerInt);
1251   }
1252 #endif
1253   assert(_frame_slots &gt;= 0 &amp;&amp; _frame_slots &lt; 1000000, &quot;sanity check&quot;);
</pre>
<hr />
<pre>
1490           int nops_cnt = padding / nop_size;
1491           MachNode *nop = new MachNopNode(nops_cnt);
1492           block-&gt;insert_node(nop, j++);
1493           last_inst++;
1494           C-&gt;cfg()-&gt;map_node_to_block(nop, block);
1495           // Ensure enough space.
1496           cb-&gt;insts()-&gt;maybe_expand_to_ensure_remaining(MAX_inst_size);
1497           if ((cb-&gt;blob() == NULL) || (!CompileBroker::should_compile_new_jobs())) {
1498             C-&gt;record_failure(&quot;CodeCache is full&quot;);
1499             return;
1500           }
1501           nop-&gt;emit(*cb, C-&gt;regalloc());
1502           cb-&gt;flush_bundle(true);
1503           current_offset = cb-&gt;insts_size();
1504         }
1505 
1506         // Remember the start of the last call in a basic block
1507         if (is_mcall) {
1508           MachCallNode *mcall = mach-&gt;as_MachCall();
1509 
<span class="line-modified">1510           if (mcall-&gt;entry_point() != NULL) {</span>
<span class="line-modified">1511             // This destination address is NOT PC-relative</span>
<span class="line-added">1512             mcall-&gt;method_set((intptr_t)mcall-&gt;entry_point());</span>
<span class="line-added">1513           }</span>
1514 
1515           // Save the return address
1516           call_returns[block-&gt;_pre_order] = current_offset + mcall-&gt;ret_addr_offset();
1517 
1518           if (mcall-&gt;is_MachCallLeaf()) {
1519             is_mcall = false;
1520             is_sfn = false;
1521           }
1522         }
1523 
1524         // sfn will be valid whenever mcall is valid now because of inheritance
1525         if (is_sfn || is_mcall) {
1526 
1527           // Handle special safepoint nodes for synchronization
1528           if (!is_mcall) {
1529             MachSafePointNode *sfn = mach-&gt;as_MachSafePoint();
1530             // !!!!! Stubs only need an oopmap right now, so bail out
1531             if (sfn-&gt;jvms()-&gt;method() == NULL) {
1532               // Write the oopmap directly to the code blob??!!
1533               continue;
</pre>
<hr />
<pre>
3215 }
3216 #endif
3217 
3218 //-----------------------init_scratch_buffer_blob------------------------------
3219 // Construct a temporary BufferBlob and cache it for this compile.
3220 void PhaseOutput::init_scratch_buffer_blob(int const_size) {
3221   // If there is already a scratch buffer blob allocated and the
3222   // constant section is big enough, use it.  Otherwise free the
3223   // current and allocate a new one.
3224   BufferBlob* blob = scratch_buffer_blob();
3225   if ((blob != NULL) &amp;&amp; (const_size &lt;= _scratch_const_size)) {
3226     // Use the current blob.
3227   } else {
3228     if (blob != NULL) {
3229       BufferBlob::free(blob);
3230     }
3231 
3232     ResourceMark rm;
3233     _scratch_const_size = const_size;
3234     int size = C2Compiler::initial_code_buffer_size(const_size);
<span class="line-added">3235 #ifdef ASSERT</span>
<span class="line-added">3236     if (C-&gt;has_scalarized_args()) {</span>
<span class="line-added">3237       // Oop verification for loading object fields from scalarized value types in the new entry point requires lots of space</span>
<span class="line-added">3238       size += 5120;</span>
<span class="line-added">3239     }</span>
<span class="line-added">3240 #endif</span>
3241     blob = BufferBlob::create(&quot;Compile::scratch_buffer&quot;, size);
3242     // Record the buffer blob for next time.
3243     set_scratch_buffer_blob(blob);
3244     // Have we run out of code space?
3245     if (scratch_buffer_blob() == NULL) {
3246       // Let CompilerBroker disable further compilations.
3247       C-&gt;record_failure(&quot;Not enough space for scratch buffer in CodeCache&quot;);
3248       return;
3249     }
3250   }
3251 
3252   // Initialize the relocation buffers
3253   relocInfo* locs_buf = (relocInfo*) blob-&gt;content_end() - MAX_locs_size;
3254   set_scratch_locs_memory(locs_buf);
3255 }
3256 
3257 
3258 //-----------------------scratch_emit_size-------------------------------------
3259 // Helper function that computes size by emitting code
3260 uint PhaseOutput::scratch_emit_size(const Node* n) {
</pre>
<hr />
<pre>
3285   int lsize = MAX_locs_size / 3;
3286   buf.consts()-&gt;initialize_shared_locs(&amp;locs_buf[lsize * 0], lsize);
3287   buf.insts()-&gt;initialize_shared_locs( &amp;locs_buf[lsize * 1], lsize);
3288   buf.stubs()-&gt;initialize_shared_locs( &amp;locs_buf[lsize * 2], lsize);
3289   // Mark as scratch buffer.
3290   buf.consts()-&gt;set_scratch_emit();
3291   buf.insts()-&gt;set_scratch_emit();
3292   buf.stubs()-&gt;set_scratch_emit();
3293 
3294   // Do the emission.
3295 
3296   Label fakeL; // Fake label for branch instructions.
3297   Label*   saveL = NULL;
3298   uint save_bnum = 0;
3299   bool is_branch = n-&gt;is_MachBranch();
3300   if (is_branch) {
3301     MacroAssembler masm(&amp;buf);
3302     masm.bind(fakeL);
3303     n-&gt;as_MachBranch()-&gt;save_label(&amp;saveL, &amp;save_bnum);
3304     n-&gt;as_MachBranch()-&gt;label_set(&amp;fakeL, 0);
<span class="line-added">3305   } else if (n-&gt;is_MachProlog()) {</span>
<span class="line-added">3306     saveL = ((MachPrologNode*)n)-&gt;_verified_entry;</span>
<span class="line-added">3307     ((MachPrologNode*)n)-&gt;_verified_entry = &amp;fakeL;</span>
<span class="line-added">3308   } else if (n-&gt;is_MachVEP()) {</span>
<span class="line-added">3309     saveL = ((MachVEPNode*)n)-&gt;_verified_entry;</span>
<span class="line-added">3310     ((MachVEPNode*)n)-&gt;_verified_entry = &amp;fakeL;</span>
3311   }
3312   n-&gt;emit(buf, C-&gt;regalloc());
3313 
3314   // Emitting into the scratch buffer should not fail
3315   assert (!C-&gt;failing(), &quot;Must not have pending failure. Reason is: %s&quot;, C-&gt;failure_reason());
3316 
<span class="line-modified">3317   // Restore label.</span>
<span class="line-added">3318   if (is_branch) {</span>
3319     n-&gt;as_MachBranch()-&gt;label_set(saveL, save_bnum);
<span class="line-added">3320   } else if (n-&gt;is_MachProlog()) {</span>
<span class="line-added">3321     ((MachPrologNode*)n)-&gt;_verified_entry = saveL;</span>
<span class="line-added">3322   } else if (n-&gt;is_MachVEP()) {</span>
<span class="line-added">3323     ((MachVEPNode*)n)-&gt;_verified_entry = saveL;</span>
<span class="line-added">3324   }</span>
3325 
3326   // End scratch_emit_size section.
3327   set_in_scratch_emit_size(false);
3328 
3329   return buf.insts_size();
3330 }
3331 
3332 void PhaseOutput::install() {
3333   if (C-&gt;stub_function() != NULL) {
3334     install_stub(C-&gt;stub_name(),
3335                  C-&gt;save_argument_registers());
3336   } else {
3337     install_code(C-&gt;method(),
3338                  C-&gt;entry_bci(),
3339                  CompileBroker::compiler2(),
3340                  C-&gt;has_unsafe_access(),
3341                  SharedRuntime::is_wide_vector(C-&gt;max_vector_size()),
3342                  C-&gt;rtm_state());
3343   }
3344 }
</pre>
<hr />
<pre>
3347                                int               entry_bci,
3348                                AbstractCompiler* compiler,
3349                                bool              has_unsafe_access,
3350                                bool              has_wide_vectors,
3351                                RTMState          rtm_state) {
3352   // Check if we want to skip execution of all compiled code.
3353   {
3354 #ifndef PRODUCT
3355     if (OptoNoExecute) {
3356       C-&gt;record_method_not_compilable(&quot;+OptoNoExecute&quot;);  // Flag as failed
3357       return;
3358     }
3359 #endif
3360     Compile::TracePhase tp(&quot;install_code&quot;, &amp;timers[_t_registerMethod]);
3361 
3362     if (C-&gt;is_osr_compilation()) {
3363       _code_offsets.set_value(CodeOffsets::Verified_Entry, 0);
3364       _code_offsets.set_value(CodeOffsets::OSR_Entry, _first_block_size);
3365     } else {
3366       _code_offsets.set_value(CodeOffsets::Verified_Entry, _first_block_size);
<span class="line-added">3367       if (_code_offsets.value(CodeOffsets::Verified_Value_Entry) == -1) {</span>
<span class="line-added">3368         _code_offsets.set_value(CodeOffsets::Verified_Value_Entry, _first_block_size);</span>
<span class="line-added">3369       }</span>
<span class="line-added">3370       if (_code_offsets.value(CodeOffsets::Verified_Value_Entry_RO) == -1) {</span>
<span class="line-added">3371         _code_offsets.set_value(CodeOffsets::Verified_Value_Entry_RO, _first_block_size);</span>
<span class="line-added">3372       }</span>
<span class="line-added">3373       if (_code_offsets.value(CodeOffsets::Entry) == -1) {</span>
<span class="line-added">3374         _code_offsets.set_value(CodeOffsets::Entry, _first_block_size);</span>
<span class="line-added">3375       }</span>
3376       _code_offsets.set_value(CodeOffsets::OSR_Entry, 0);
3377     }
3378 
3379     C-&gt;env()-&gt;register_method(target,
<span class="line-modified">3380                               entry_bci,</span>
<span class="line-modified">3381                               &amp;_code_offsets,</span>
<span class="line-modified">3382                               _orig_pc_slot_offset_in_bytes,</span>
<span class="line-modified">3383                               code_buffer(),</span>
<span class="line-modified">3384                               frame_size_in_words(),</span>
<span class="line-modified">3385                               _oop_map_set,</span>
<span class="line-modified">3386                               &amp;_handler_table,</span>
<span class="line-modified">3387                               &amp;_inc_table,</span>
<span class="line-modified">3388                               compiler,</span>
<span class="line-modified">3389                               has_unsafe_access,</span>
<span class="line-modified">3390                               SharedRuntime::is_wide_vector(C-&gt;max_vector_size()),</span>
<span class="line-modified">3391                               C-&gt;rtm_state());</span>
3392 
3393     if (C-&gt;log() != NULL) { // Print code cache state into compiler log
3394       C-&gt;log()-&gt;code_cache_state();
3395     }
3396   }
3397 }
3398 void PhaseOutput::install_stub(const char* stub_name,
3399                                bool        caller_must_gc_arguments) {
3400   // Entry point will be accessed using stub_entry_point();
3401   if (code_buffer() == NULL) {
3402     Matcher::soft_match_failure();
3403   } else {
3404     if (PrintAssembly &amp;&amp; (WizardMode || Verbose))
3405       tty-&gt;print_cr(&quot;### Stub::%s&quot;, stub_name);
3406 
3407     if (!C-&gt;failing()) {
3408       assert(C-&gt;fixed_slots() == 0, &quot;no fixed slots used for runtime stubs&quot;);
3409 
3410       // Make the NMethod
3411       // For now we mark the frame as never safe for profile stackwalking
</pre>
</td>
</tr>
</table>
<center><a href="node.hpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="output.hpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>