<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/share/opto/compile.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="classes.hpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="compile.hpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/share/opto/compile.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
  50 #include &quot;opto/divnode.hpp&quot;
  51 #include &quot;opto/escape.hpp&quot;
  52 #include &quot;opto/idealGraphPrinter.hpp&quot;
  53 #include &quot;opto/loopnode.hpp&quot;
  54 #include &quot;opto/machnode.hpp&quot;
  55 #include &quot;opto/macro.hpp&quot;
  56 #include &quot;opto/matcher.hpp&quot;
  57 #include &quot;opto/mathexactnode.hpp&quot;
  58 #include &quot;opto/memnode.hpp&quot;
  59 #include &quot;opto/mulnode.hpp&quot;
  60 #include &quot;opto/narrowptrnode.hpp&quot;
  61 #include &quot;opto/node.hpp&quot;
  62 #include &quot;opto/opcodes.hpp&quot;
  63 #include &quot;opto/output.hpp&quot;
  64 #include &quot;opto/parse.hpp&quot;
  65 #include &quot;opto/phaseX.hpp&quot;
  66 #include &quot;opto/rootnode.hpp&quot;
  67 #include &quot;opto/runtime.hpp&quot;
  68 #include &quot;opto/stringopts.hpp&quot;
  69 #include &quot;opto/type.hpp&quot;

  70 #include &quot;opto/vectornode.hpp&quot;
  71 #include &quot;runtime/arguments.hpp&quot;
  72 #include &quot;runtime/sharedRuntime.hpp&quot;
  73 #include &quot;runtime/signature.hpp&quot;
  74 #include &quot;runtime/stubRoutines.hpp&quot;
  75 #include &quot;runtime/timer.hpp&quot;
  76 #include &quot;utilities/align.hpp&quot;
  77 #include &quot;utilities/copy.hpp&quot;
  78 #include &quot;utilities/macros.hpp&quot;
  79 #include &quot;utilities/resourceHash.hpp&quot;
  80 
  81 
  82 // -------------------- Compile::mach_constant_base_node -----------------------
  83 // Constant table base node singleton.
  84 MachConstantBaseNode* Compile::mach_constant_base_node() {
  85   if (_mach_constant_base_node == NULL) {
  86     _mach_constant_base_node = new MachConstantBaseNode();
  87     _mach_constant_base_node-&gt;add_req(C-&gt;root());
  88   }
  89   return _mach_constant_base_node;
</pre>
<hr />
<pre>
 387   for (int i = range_check_cast_count() - 1; i &gt;= 0; i--) {
 388     Node* cast = range_check_cast_node(i);
 389     if (!useful.member(cast)) {
 390       remove_range_check_cast(cast);
 391     }
 392   }
 393   // Remove useless expensive nodes
 394   for (int i = C-&gt;expensive_count()-1; i &gt;= 0; i--) {
 395     Node* n = C-&gt;expensive_node(i);
 396     if (!useful.member(n)) {
 397       remove_expensive_node(n);
 398     }
 399   }
 400   // Remove useless Opaque4 nodes
 401   for (int i = opaque4_count() - 1; i &gt;= 0; i--) {
 402     Node* opaq = opaque4_node(i);
 403     if (!useful.member(opaq)) {
 404       remove_opaque4_node(opaq);
 405     }
 406   }




 407   BarrierSetC2* bs = BarrierSet::barrier_set()-&gt;barrier_set_c2();
 408   bs-&gt;eliminate_useless_gc_barriers(useful, this);
 409   // clean up the late inline lists
 410   remove_useless_late_inlines(&amp;_string_late_inlines, useful);
 411   remove_useless_late_inlines(&amp;_boxing_late_inlines, useful);
 412   remove_useless_late_inlines(&amp;_late_inlines, useful);
 413   debug_only(verify_graph_edges(true/*check for no_dead_code*/);)
 414 }
 415 
 416 // ============================================================================
 417 //------------------------------CompileWrapper---------------------------------
 418 class CompileWrapper : public StackObj {
 419   Compile *const _compile;
 420  public:
 421   CompileWrapper(Compile* compile);
 422 
 423   ~CompileWrapper();
 424 };
 425 
 426 CompileWrapper::CompileWrapper(Compile* compile) : _compile(compile) {
</pre>
<hr />
<pre>
 619   // Node list that Iterative GVN will start with
 620   Unique_Node_List for_igvn(comp_arena());
 621   set_for_igvn(&amp;for_igvn);
 622 
 623   // GVN that will be run immediately on new nodes
 624   uint estimated_size = method()-&gt;code_size()*4+64;
 625   estimated_size = (estimated_size &lt; MINIMUM_NODE_HASH ? MINIMUM_NODE_HASH : estimated_size);
 626   PhaseGVN gvn(node_arena(), estimated_size);
 627   set_initial_gvn(&amp;gvn);
 628 
 629   print_inlining_init();
 630   { // Scope for timing the parser
 631     TracePhase tp(&quot;parse&quot;, &amp;timers[_t_parser]);
 632 
 633     // Put top into the hash table ASAP.
 634     initial_gvn()-&gt;transform_no_reclaim(top());
 635 
 636     // Set up tf(), start(), and find a CallGenerator.
 637     CallGenerator* cg = NULL;
 638     if (is_osr_compilation()) {
<span class="line-modified"> 639       const TypeTuple *domain = StartOSRNode::osr_domain();</span>
<span class="line-modified"> 640       const TypeTuple *range = TypeTuple::make_range(method()-&gt;signature());</span>
<span class="line-removed"> 641       init_tf(TypeFunc::make(domain, range));</span>
<span class="line-removed"> 642       StartNode* s = new StartOSRNode(root(), domain);</span>
 643       initial_gvn()-&gt;set_type_bottom(s);
 644       init_start(s);
 645       cg = CallGenerator::for_osr(method(), entry_bci());
 646     } else {
 647       // Normal case.
 648       init_tf(TypeFunc::make(method()));
<span class="line-modified"> 649       StartNode* s = new StartNode(root(), tf()-&gt;domain());</span>
 650       initial_gvn()-&gt;set_type_bottom(s);
 651       init_start(s);
 652       if (method()-&gt;intrinsic_id() == vmIntrinsics::_Reference_get) {
 653         // With java.lang.ref.reference.get() we must go through the
 654         // intrinsic - even when get() is the root
 655         // method of the compile - so that, if necessary, the value in
 656         // the referent field of the reference object gets recorded by
 657         // the pre-barrier code.
 658         cg = find_intrinsic(method(), false);
 659       }
 660       if (cg == NULL) {
 661         float past_uses = method()-&gt;interpreter_invocation_count();
 662         float expected_uses = past_uses;
 663         cg = CallGenerator::for_inline(method(), expected_uses);
 664       }
 665     }
 666     if (failing())  return;
 667     if (cg == NULL) {
 668       record_method_not_compilable(&quot;cannot parse method&quot;);
 669       return;
</pre>
<hr />
<pre>
 754     }
 755   }
 756 #endif
 757 
 758 #ifdef ASSERT
 759   BarrierSetC2* bs = BarrierSet::barrier_set()-&gt;barrier_set_c2();
 760   bs-&gt;verify_gc_barriers(this, BarrierSetC2::BeforeCodeGen);
 761 #endif
 762 
 763   // Dump compilation data to replay it.
 764   if (directive-&gt;DumpReplayOption) {
 765     env()-&gt;dump_replay_data(_compile_id);
 766   }
 767   if (directive-&gt;DumpInlineOption &amp;&amp; (ilt() != NULL)) {
 768     env()-&gt;dump_inline_data(_compile_id);
 769   }
 770 
 771   // Now that we know the size of all the monitors we can add a fixed slot
 772   // for the original deopt pc.
 773   int next_slot = fixed_slots() + (sizeof(address) / VMRegImpl::stack_slot_size);




 774   set_fixed_slots(next_slot);
 775 
 776   // Compute when to use implicit null checks. Used by matching trap based
 777   // nodes and NullCheck optimization.
 778   set_allowed_deopt_reasons();
 779 
 780   // Now generate code
 781   Code_Gen();
 782 }
 783 
 784 //------------------------------Compile----------------------------------------
 785 // Compile a runtime stub
 786 Compile::Compile( ciEnv* ci_env,
 787                   TypeFunc_generator generator,
 788                   address stub_function,
 789                   const char *stub_name,
 790                   int is_fancy_jump,
 791                   bool pass_tls,
 792                   bool save_arg_registers,
 793                   bool return_pc,
</pre>
<hr />
<pre>
 909   // Create Debug Information Recorder to record scopes, oopmaps, etc.
 910   env()-&gt;set_oop_recorder(new OopRecorder(env()-&gt;arena()));
 911   env()-&gt;set_debug_info(new DebugInformationRecorder(env()-&gt;oop_recorder()));
 912   env()-&gt;set_dependencies(new Dependencies(env()));
 913 
 914   _fixed_slots = 0;
 915   set_has_split_ifs(false);
 916   set_has_loops(has_method() &amp;&amp; method()-&gt;has_loops()); // first approximation
 917   set_has_stringbuilder(false);
 918   set_has_boxed_value(false);
 919   _trap_can_recompile = false;  // no traps emitted yet
 920   _major_progress = true; // start out assuming good things will happen
 921   set_has_unsafe_access(false);
 922   set_max_vector_size(0);
 923   set_clear_upper_avx(false);  //false as default for clear upper bits of ymm registers
 924   Copy::zero_to_bytes(_trap_hist, sizeof(_trap_hist));
 925   set_decompile_count(0);
 926 
 927   set_do_freq_based_layout(_directive-&gt;BlockLayoutByFrequencyOption);
 928   _loop_opts_cnt = LoopOptsCount;



 929   set_do_inlining(Inline);
 930   set_max_inline_size(MaxInlineSize);
 931   set_freq_inline_size(FreqInlineSize);
 932   set_do_scheduling(OptoScheduling);
 933   set_do_count_invocations(false);
 934   set_do_method_data_update(false);
 935 
 936   set_do_vector_loop(false);
 937 
 938   if (AllowVectorizeOnDemand) {
 939     if (has_method() &amp;&amp; (_directive-&gt;VectorizeOption || _directive-&gt;VectorizeDebugOption)) {
 940       set_do_vector_loop(true);
 941       NOT_PRODUCT(if (do_vector_loop() &amp;&amp; Verbose) {tty-&gt;print(&quot;Compile::Init: do vectorized loops (SIMD like) for method %s\n&quot;,  method()-&gt;name()-&gt;as_quoted_ascii());})
 942     } else if (has_method() &amp;&amp; method()-&gt;name() != 0 &amp;&amp;
 943                method()-&gt;intrinsic_id() == vmIntrinsics::_forEachRemaining) {
 944       set_do_vector_loop(true);
 945     }
 946   }
 947   set_use_cmove(UseCMoveUnconditionally /* || do_vector_loop()*/); //TODO: consider do_vector_loop() mandate use_cmove unconditionally
 948   NOT_PRODUCT(if (use_cmove() &amp;&amp; Verbose &amp;&amp; has_method()) {tty-&gt;print(&quot;Compile::Init: use CMove without profitability tests for method %s\n&quot;,  method()-&gt;name()-&gt;as_quoted_ascii());})
</pre>
<hr />
<pre>
 992   Copy::zero_to_bytes(ats, sizeof(AliasType)*grow_ats);
 993   {
 994     for (int i = 0; i &lt; grow_ats; i++)  _alias_types[i] = &amp;ats[i];
 995   }
 996   // Initialize the first few types.
 997   _alias_types[AliasIdxTop]-&gt;Init(AliasIdxTop, NULL);
 998   _alias_types[AliasIdxBot]-&gt;Init(AliasIdxBot, TypePtr::BOTTOM);
 999   _alias_types[AliasIdxRaw]-&gt;Init(AliasIdxRaw, TypeRawPtr::BOTTOM);
1000   _num_alias_types = AliasIdxRaw+1;
1001   // Zero out the alias type cache.
1002   Copy::zero_to_bytes(_alias_cache, sizeof(_alias_cache));
1003   // A NULL adr_type hits in the cache right away.  Preload the right answer.
1004   probe_alias_cache(NULL)-&gt;_index = AliasIdxTop;
1005 
1006   _intrinsics = NULL;
1007   _macro_nodes = new(comp_arena()) GrowableArray&lt;Node*&gt;(comp_arena(), 8,  0, NULL);
1008   _predicate_opaqs = new(comp_arena()) GrowableArray&lt;Node*&gt;(comp_arena(), 8,  0, NULL);
1009   _expensive_nodes = new(comp_arena()) GrowableArray&lt;Node*&gt;(comp_arena(), 8,  0, NULL);
1010   _range_check_casts = new(comp_arena()) GrowableArray&lt;Node*&gt;(comp_arena(), 8,  0, NULL);
1011   _opaque4_nodes = new(comp_arena()) GrowableArray&lt;Node*&gt;(comp_arena(), 8,  0, NULL);

1012   register_library_intrinsics();
1013 #ifdef ASSERT
1014   _type_verify_symmetry = true;
1015 #endif
1016 }
1017 
1018 //---------------------------init_start----------------------------------------
1019 // Install the StartNode on this compile object.
1020 void Compile::init_start(StartNode* s) {
1021   if (failing())
1022     return; // already failing
1023   assert(s == start(), &quot;&quot;);
1024 }
1025 
1026 /**
1027  * Return the &#39;StartNode&#39;. We must not have a pending failure, since the ideal graph
1028  * can be in an inconsistent state, i.e., we can get segmentation faults when traversing
1029  * the ideal graph.
1030  */
1031 StartNode* Compile::start() const {
</pre>
<hr />
<pre>
1219 bool Compile::allow_range_check_smearing() const {
1220   // If this method has already thrown a range-check,
1221   // assume it was because we already tried range smearing
1222   // and it failed.
1223   uint already_trapped = trap_count(Deoptimization::Reason_range_check);
1224   return !already_trapped;
1225 }
1226 
1227 
1228 //------------------------------flatten_alias_type-----------------------------
1229 const TypePtr *Compile::flatten_alias_type( const TypePtr *tj ) const {
1230   int offset = tj-&gt;offset();
1231   TypePtr::PTR ptr = tj-&gt;ptr();
1232 
1233   // Known instance (scalarizable allocation) alias only with itself.
1234   bool is_known_inst = tj-&gt;isa_oopptr() != NULL &amp;&amp;
1235                        tj-&gt;is_oopptr()-&gt;is_known_instance();
1236 
1237   // Process weird unsafe references.
1238   if (offset == Type::OffsetBot &amp;&amp; (tj-&gt;isa_instptr() /*|| tj-&gt;isa_klassptr()*/)) {
<span class="line-modified">1239     assert(InlineUnsafeOps, &quot;indeterminate pointers come only from unsafe ops&quot;);</span>

1240     assert(!is_known_inst, &quot;scalarizable allocation should not have unsafe references&quot;);
1241     tj = TypeOopPtr::BOTTOM;
1242     ptr = tj-&gt;ptr();
1243     offset = tj-&gt;offset();
1244   }
1245 
1246   // Array pointers need some flattening
1247   const TypeAryPtr *ta = tj-&gt;isa_aryptr();
1248   if (ta &amp;&amp; ta-&gt;is_stable()) {
1249     // Erase stability property for alias analysis.
1250     tj = ta = ta-&gt;cast_to_stable(false);
1251   }









1252   if( ta &amp;&amp; is_known_inst ) {
1253     if ( offset != Type::OffsetBot &amp;&amp;
1254          offset &gt; arrayOopDesc::length_offset_in_bytes() ) {
1255       offset = Type::OffsetBot; // Flatten constant access into array body only
<span class="line-modified">1256       tj = ta = TypeAryPtr::make(ptr, ta-&gt;ary(), ta-&gt;klass(), true, offset, ta-&gt;instance_id());</span>
1257     }
1258   } else if( ta &amp;&amp; _AliasLevel &gt;= 2 ) {
1259     // For arrays indexed by constant indices, we flatten the alias
1260     // space to include all of the array body.  Only the header, klass
1261     // and array length can be accessed un-aliased.


1262     if( offset != Type::OffsetBot ) {
1263       if( ta-&gt;const_oop() ) { // MethodData* or Method*
1264         offset = Type::OffsetBot;   // Flatten constant access into array body
<span class="line-modified">1265         tj = ta = TypeAryPtr::make(ptr,ta-&gt;const_oop(),ta-&gt;ary(),ta-&gt;klass(),false,offset);</span>
1266       } else if( offset == arrayOopDesc::length_offset_in_bytes() ) {
1267         // range is OK as-is.
1268         tj = ta = TypeAryPtr::RANGE;
1269       } else if( offset == oopDesc::klass_offset_in_bytes() ) {
1270         tj = TypeInstPtr::KLASS; // all klass loads look alike
1271         ta = TypeAryPtr::RANGE; // generic ignored junk
1272         ptr = TypePtr::BotPTR;
1273       } else if( offset == oopDesc::mark_offset_in_bytes() ) {
1274         tj = TypeInstPtr::MARK;
1275         ta = TypeAryPtr::RANGE; // generic ignored junk
1276         ptr = TypePtr::BotPTR;
1277       } else {                  // Random constant offset into array body
1278         offset = Type::OffsetBot;   // Flatten constant access into array body
<span class="line-modified">1279         tj = ta = TypeAryPtr::make(ptr,ta-&gt;ary(),ta-&gt;klass(),false,offset);</span>
1280       }
1281     }
1282     // Arrays of fixed size alias with arrays of unknown size.
1283     if (ta-&gt;size() != TypeInt::POS) {
1284       const TypeAry *tary = TypeAry::make(ta-&gt;elem(), TypeInt::POS);
<span class="line-modified">1285       tj = ta = TypeAryPtr::make(ptr,ta-&gt;const_oop(),tary,ta-&gt;klass(),false,offset);</span>
1286     }
1287     // Arrays of known objects become arrays of unknown objects.
1288     if (ta-&gt;elem()-&gt;isa_narrowoop() &amp;&amp; ta-&gt;elem() != TypeNarrowOop::BOTTOM) {
1289       const TypeAry *tary = TypeAry::make(TypeNarrowOop::BOTTOM, ta-&gt;size());
<span class="line-modified">1290       tj = ta = TypeAryPtr::make(ptr,ta-&gt;const_oop(),tary,NULL,false,offset);</span>
1291     }
1292     if (ta-&gt;elem()-&gt;isa_oopptr() &amp;&amp; ta-&gt;elem() != TypeInstPtr::BOTTOM) {
1293       const TypeAry *tary = TypeAry::make(TypeInstPtr::BOTTOM, ta-&gt;size());
<span class="line-modified">1294       tj = ta = TypeAryPtr::make(ptr,ta-&gt;const_oop(),tary,NULL,false,offset);</span>





1295     }
1296     // Arrays of bytes and of booleans both use &#39;bastore&#39; and &#39;baload&#39; so
1297     // cannot be distinguished by bytecode alone.
1298     if (ta-&gt;elem() == TypeInt::BOOL) {
1299       const TypeAry *tary = TypeAry::make(TypeInt::BYTE, ta-&gt;size());
1300       ciKlass* aklass = ciTypeArrayKlass::make(T_BYTE);
<span class="line-modified">1301       tj = ta = TypeAryPtr::make(ptr,ta-&gt;const_oop(),tary,aklass,false,offset);</span>
1302     }
1303     // During the 2nd round of IterGVN, NotNull castings are removed.
1304     // Make sure the Bottom and NotNull variants alias the same.
1305     // Also, make sure exact and non-exact variants alias the same.
1306     if (ptr == TypePtr::NotNull || ta-&gt;klass_is_exact() || ta-&gt;speculative() != NULL) {
<span class="line-modified">1307       tj = ta = TypeAryPtr::make(TypePtr::BotPTR,ta-&gt;ary(),ta-&gt;klass(),false,offset);</span>
1308     }
1309   }
1310 
1311   // Oop pointers need some flattening
1312   const TypeInstPtr *to = tj-&gt;isa_instptr();
1313   if( to &amp;&amp; _AliasLevel &gt;= 2 &amp;&amp; to != TypeOopPtr::BOTTOM ) {
1314     ciInstanceKlass *k = to-&gt;klass()-&gt;as_instance_klass();
1315     if( ptr == TypePtr::Constant ) {
1316       if (to-&gt;klass() != ciEnv::current()-&gt;Class_klass() ||
1317           offset &lt; k-&gt;size_helper() * wordSize) {
1318         // No constant oop pointers (such as Strings); they alias with
1319         // unknown strings.
1320         assert(!is_known_inst, &quot;not scalarizable allocation&quot;);
<span class="line-modified">1321         tj = to = TypeInstPtr::make(TypePtr::BotPTR,to-&gt;klass(),false,0,offset);</span>
1322       }
1323     } else if( is_known_inst ) {
1324       tj = to; // Keep NotNull and klass_is_exact for instance type
1325     } else if( ptr == TypePtr::NotNull || to-&gt;klass_is_exact() ) {
1326       // During the 2nd round of IterGVN, NotNull castings are removed.
1327       // Make sure the Bottom and NotNull variants alias the same.
1328       // Also, make sure exact and non-exact variants alias the same.
<span class="line-modified">1329       tj = to = TypeInstPtr::make(TypePtr::BotPTR,to-&gt;klass(),false,0,offset);</span>
1330     }
1331     if (to-&gt;speculative() != NULL) {
<span class="line-modified">1332       tj = to = TypeInstPtr::make(to-&gt;ptr(),to-&gt;klass(),to-&gt;klass_is_exact(),to-&gt;const_oop(),to-&gt;offset(), to-&gt;instance_id());</span>
1333     }
1334     // Canonicalize the holder of this field
1335     if (offset &gt;= 0 &amp;&amp; offset &lt; instanceOopDesc::base_offset_in_bytes()) {
1336       // First handle header references such as a LoadKlassNode, even if the
1337       // object&#39;s klass is unloaded at compile time (4965979).
1338       if (!is_known_inst) { // Do it only for non-instance types
<span class="line-modified">1339         tj = to = TypeInstPtr::make(TypePtr::BotPTR, env()-&gt;Object_klass(), false, NULL, offset);</span>
1340       }
1341     } else if (offset &lt; 0 || offset &gt;= k-&gt;size_helper() * wordSize) {
1342       // Static fields are in the space above the normal instance
1343       // fields in the java.lang.Class instance.
1344       if (to-&gt;klass() != ciEnv::current()-&gt;Class_klass()) {
1345         to = NULL;
1346         tj = TypeOopPtr::BOTTOM;
1347         offset = tj-&gt;offset();
1348       }
1349     } else {
1350       ciInstanceKlass *canonical_holder = k-&gt;get_canonical_holder(offset);
1351       if (!k-&gt;equals(canonical_holder) || tj-&gt;offset() != offset) {
1352         if( is_known_inst ) {
<span class="line-modified">1353           tj = to = TypeInstPtr::make(to-&gt;ptr(), canonical_holder, true, NULL, offset, to-&gt;instance_id());</span>
1354         } else {
<span class="line-modified">1355           tj = to = TypeInstPtr::make(to-&gt;ptr(), canonical_holder, false, NULL, offset);</span>
1356         }
1357       }
1358     }
1359   }
1360 
1361   // Klass pointers to object array klasses need some flattening
1362   const TypeKlassPtr *tk = tj-&gt;isa_klassptr();
1363   if( tk ) {
1364     // If we are referencing a field within a Klass, we need
1365     // to assume the worst case of an Object.  Both exact and
1366     // inexact types must flatten to the same alias class so
1367     // use NotNull as the PTR.
1368     if ( offset == Type::OffsetBot || (offset &gt;= 0 &amp;&amp; (size_t)offset &lt; sizeof(Klass)) ) {
1369 
1370       tj = tk = TypeKlassPtr::make(TypePtr::NotNull,
1371                                    TypeKlassPtr::OBJECT-&gt;klass(),
<span class="line-modified">1372                                    offset);</span>

1373     }
1374 
1375     ciKlass* klass = tk-&gt;klass();
<span class="line-modified">1376     if( klass-&gt;is_obj_array_klass() ) {</span>
1377       ciKlass* k = TypeAryPtr::OOPS-&gt;klass();
1378       if( !k || !k-&gt;is_loaded() )                  // Only fails for some -Xcomp runs
1379         k = TypeInstPtr::BOTTOM-&gt;klass();
<span class="line-modified">1380       tj = tk = TypeKlassPtr::make( TypePtr::NotNull, k, offset );</span>
1381     }
1382 
1383     // Check for precise loads from the primary supertype array and force them
1384     // to the supertype cache alias index.  Check for generic array loads from
1385     // the primary supertype array and also force them to the supertype cache
1386     // alias index.  Since the same load can reach both, we need to merge
1387     // these 2 disparate memories into the same alias class.  Since the
1388     // primary supertype array is read-only, there&#39;s no chance of confusion
1389     // where we bypass an array load and an array store.
1390     int primary_supers_offset = in_bytes(Klass::primary_supers_offset());
1391     if (offset == Type::OffsetBot ||
1392         (offset &gt;= primary_supers_offset &amp;&amp;
1393          offset &lt; (int)(primary_supers_offset + Klass::primary_super_limit() * wordSize)) ||
1394         offset == (int)in_bytes(Klass::secondary_super_cache_offset())) {
1395       offset = in_bytes(Klass::secondary_super_cache_offset());
<span class="line-modified">1396       tj = tk = TypeKlassPtr::make( TypePtr::NotNull, tk-&gt;klass(), offset );</span>
1397     }
1398   }
1399 
1400   // Flatten all Raw pointers together.
1401   if (tj-&gt;base() == Type::RawPtr)
1402     tj = TypeRawPtr::BOTTOM;
1403 
1404   if (tj-&gt;base() == Type::AnyPtr)
1405     tj = TypePtr::BOTTOM;      // An error, which the caller must check for.
1406 
1407   // Flatten all to bottom for now
1408   switch( _AliasLevel ) {
1409   case 0:
1410     tj = TypePtr::BOTTOM;
1411     break;
1412   case 1:                       // Flatten to: oop, static, field or array
1413     switch (tj-&gt;base()) {
1414     //case Type::AryPtr: tj = TypeAryPtr::RANGE;    break;
1415     case Type::RawPtr:   tj = TypeRawPtr::BOTTOM;   break;
1416     case Type::AryPtr:   // do not distinguish arrays at all
</pre>
<hr />
<pre>
1515   intptr_t key = (intptr_t) adr_type;
1516   key ^= key &gt;&gt; logAliasCacheSize;
1517   return &amp;_alias_cache[key &amp; right_n_bits(logAliasCacheSize)];
1518 }
1519 
1520 
1521 //-----------------------------grow_alias_types--------------------------------
1522 void Compile::grow_alias_types() {
1523   const int old_ats  = _max_alias_types; // how many before?
1524   const int new_ats  = old_ats;          // how many more?
1525   const int grow_ats = old_ats+new_ats;  // how many now?
1526   _max_alias_types = grow_ats;
1527   _alias_types =  REALLOC_ARENA_ARRAY(comp_arena(), AliasType*, _alias_types, old_ats, grow_ats);
1528   AliasType* ats =    NEW_ARENA_ARRAY(comp_arena(), AliasType, new_ats);
1529   Copy::zero_to_bytes(ats, sizeof(AliasType)*new_ats);
1530   for (int i = 0; i &lt; new_ats; i++)  _alias_types[old_ats+i] = &amp;ats[i];
1531 }
1532 
1533 
1534 //--------------------------------find_alias_type------------------------------
<span class="line-modified">1535 Compile::AliasType* Compile::find_alias_type(const TypePtr* adr_type, bool no_create, ciField* original_field) {</span>
1536   if (_AliasLevel == 0)
1537     return alias_type(AliasIdxBot);
1538 
<span class="line-modified">1539   AliasCacheEntry* ace = probe_alias_cache(adr_type);</span>
<span class="line-modified">1540   if (ace-&gt;_adr_type == adr_type) {</span>
<span class="line-modified">1541     return alias_type(ace-&gt;_index);</span>



1542   }
1543 
1544   // Handle special cases.
1545   if (adr_type == NULL)             return alias_type(AliasIdxTop);
1546   if (adr_type == TypePtr::BOTTOM)  return alias_type(AliasIdxBot);
1547 
1548   // Do it the slow way.
1549   const TypePtr* flat = flatten_alias_type(adr_type);
1550 
1551 #ifdef ASSERT
1552   {
1553     ResourceMark rm;
1554     assert(flat == flatten_alias_type(flat), &quot;not idempotent: adr_type = %s; flat = %s =&gt; %s&quot;,
1555            Type::str(adr_type), Type::str(flat), Type::str(flatten_alias_type(flat)));
1556     assert(flat != TypePtr::BOTTOM, &quot;cannot alias-analyze an untyped ptr: adr_type = %s&quot;,
1557            Type::str(adr_type));
1558     if (flat-&gt;isa_oopptr() &amp;&amp; !flat-&gt;isa_klassptr()) {
1559       const TypeOopPtr* foop = flat-&gt;is_oopptr();
1560       // Scalarizable allocations have exact klass always.
1561       bool exact = !foop-&gt;klass_is_exact() || foop-&gt;is_known_instance();
</pre>
<hr />
<pre>
1571     if (alias_type(i)-&gt;adr_type() == flat) {
1572       idx = i;
1573       break;
1574     }
1575   }
1576 
1577   if (idx == AliasIdxTop) {
1578     if (no_create)  return NULL;
1579     // Grow the array if necessary.
1580     if (_num_alias_types == _max_alias_types)  grow_alias_types();
1581     // Add a new alias type.
1582     idx = _num_alias_types++;
1583     _alias_types[idx]-&gt;Init(idx, flat);
1584     if (flat == TypeInstPtr::KLASS)  alias_type(idx)-&gt;set_rewritable(false);
1585     if (flat == TypeAryPtr::RANGE)   alias_type(idx)-&gt;set_rewritable(false);
1586     if (flat-&gt;isa_instptr()) {
1587       if (flat-&gt;offset() == java_lang_Class::klass_offset_in_bytes()
1588           &amp;&amp; flat-&gt;is_instptr()-&gt;klass() == env()-&gt;Class_klass())
1589         alias_type(idx)-&gt;set_rewritable(false);
1590     }

1591     if (flat-&gt;isa_aryptr()) {
1592 #ifdef ASSERT
1593       const int header_size_min  = arrayOopDesc::base_offset_in_bytes(T_BYTE);
1594       // (T_BYTE has the weakest alignment and size restrictions...)
1595       assert(flat-&gt;offset() &lt; header_size_min, &quot;array body reference must be OffsetBot&quot;);
1596 #endif

1597       if (flat-&gt;offset() == TypePtr::OffsetBot) {
<span class="line-modified">1598         alias_type(idx)-&gt;set_element(flat-&gt;is_aryptr()-&gt;elem());</span>








1599       }
1600     }
1601     if (flat-&gt;isa_klassptr()) {
1602       if (flat-&gt;offset() == in_bytes(Klass::super_check_offset_offset()))
1603         alias_type(idx)-&gt;set_rewritable(false);
1604       if (flat-&gt;offset() == in_bytes(Klass::modifier_flags_offset()))
1605         alias_type(idx)-&gt;set_rewritable(false);
1606       if (flat-&gt;offset() == in_bytes(Klass::access_flags_offset()))
1607         alias_type(idx)-&gt;set_rewritable(false);
1608       if (flat-&gt;offset() == in_bytes(Klass::java_mirror_offset()))
1609         alias_type(idx)-&gt;set_rewritable(false);


1610       if (flat-&gt;offset() == in_bytes(Klass::secondary_super_cache_offset()))
1611         alias_type(idx)-&gt;set_rewritable(false);
1612     }
1613     // %%% (We would like to finalize JavaThread::threadObj_offset(),
1614     // but the base pointer type is not distinctive enough to identify
1615     // references into JavaThread.)
1616 
1617     // Check for final fields.
1618     const TypeInstPtr* tinst = flat-&gt;isa_instptr();
1619     if (tinst &amp;&amp; tinst-&gt;offset() &gt;= instanceOopDesc::base_offset_in_bytes()) {
<span class="line-removed">1620       ciField* field;</span>
1621       if (tinst-&gt;const_oop() != NULL &amp;&amp;
1622           tinst-&gt;klass() == ciEnv::current()-&gt;Class_klass() &amp;&amp;
1623           tinst-&gt;offset() &gt;= (tinst-&gt;klass()-&gt;as_instance_klass()-&gt;size_helper() * wordSize)) {
1624         // static field
1625         ciInstanceKlass* k = tinst-&gt;const_oop()-&gt;as_instance()-&gt;java_lang_Class_klass()-&gt;as_instance_klass();
1626         field = k-&gt;get_field_by_offset(tinst-&gt;offset(), true);




1627       } else {
<span class="line-modified">1628         ciInstanceKlass *k = tinst-&gt;klass()-&gt;as_instance_klass();</span>
1629         field = k-&gt;get_field_by_offset(tinst-&gt;offset(), false);
1630       }
<span class="line-modified">1631       assert(field == NULL ||</span>
<span class="line-modified">1632              original_field == NULL ||</span>
<span class="line-modified">1633              (field-&gt;holder() == original_field-&gt;holder() &amp;&amp;</span>
<span class="line-modified">1634               field-&gt;offset() == original_field-&gt;offset() &amp;&amp;</span>
<span class="line-modified">1635               field-&gt;is_static() == original_field-&gt;is_static()), &quot;wrong field?&quot;);</span>
<span class="line-modified">1636       // Set field() and is_rewritable() attributes.</span>
<span class="line-modified">1637       if (field != NULL)  alias_type(idx)-&gt;set_field(field);</span>







1638     }
1639   }
1640 
1641   // Fill the cache for next time.
<span class="line-modified">1642   ace-&gt;_adr_type = adr_type;</span>
<span class="line-modified">1643   ace-&gt;_index    = idx;</span>
<span class="line-modified">1644   assert(alias_type(adr_type) == alias_type(idx),  &quot;type must be installed&quot;);</span>

1645 
<span class="line-modified">1646   // Might as well try to fill the cache for the flattened version, too.</span>
<span class="line-modified">1647   AliasCacheEntry* face = probe_alias_cache(flat);</span>
<span class="line-modified">1648   if (face-&gt;_adr_type == NULL) {</span>
<span class="line-modified">1649     face-&gt;_adr_type = flat;</span>
<span class="line-modified">1650     face-&gt;_index    = idx;</span>
<span class="line-modified">1651     assert(alias_type(flat) == alias_type(idx), &quot;flat type must work too&quot;);</span>

1652   }
1653 
1654   return alias_type(idx);
1655 }
1656 
1657 
1658 Compile::AliasType* Compile::alias_type(ciField* field) {
1659   const TypeOopPtr* t;
1660   if (field-&gt;is_static())
1661     t = TypeInstPtr::make(field-&gt;holder()-&gt;java_mirror());
1662   else
1663     t = TypeOopPtr::make_from_klass_raw(field-&gt;holder());
1664   AliasType* atp = alias_type(t-&gt;add_offset(field-&gt;offset_in_bytes()), field);
1665   assert((field-&gt;is_final() || field-&gt;is_stable()) == !atp-&gt;is_rewritable(), &quot;must get the rewritable bits correct&quot;);
1666   return atp;
1667 }
1668 
1669 
1670 //------------------------------have_alias_type--------------------------------
1671 bool Compile::have_alias_type(const TypePtr* adr_type) {
</pre>
<hr />
<pre>
1793   }
1794   assert(range_check_cast_count() == 0, &quot;should be empty&quot;);
1795 }
1796 
1797 void Compile::add_opaque4_node(Node* n) {
1798   assert(n-&gt;Opcode() == Op_Opaque4, &quot;Opaque4 only&quot;);
1799   assert(!_opaque4_nodes-&gt;contains(n), &quot;duplicate entry in Opaque4 list&quot;);
1800   _opaque4_nodes-&gt;append(n);
1801 }
1802 
1803 // Remove all Opaque4 nodes.
1804 void Compile::remove_opaque4_nodes(PhaseIterGVN &amp;igvn) {
1805   for (int i = opaque4_count(); i &gt; 0; i--) {
1806     Node* opaq = opaque4_node(i-1);
1807     assert(opaq-&gt;Opcode() == Op_Opaque4, &quot;Opaque4 only&quot;);
1808     igvn.replace_node(opaq, opaq-&gt;in(2));
1809   }
1810   assert(opaque4_count() == 0, &quot;should be empty&quot;);
1811 }
1812 





















































































































































































































































































































































1813 // StringOpts and late inlining of string methods
1814 void Compile::inline_string_calls(bool parse_time) {
1815   {
1816     // remove useless nodes to make the usage analysis simpler
1817     ResourceMark rm;
1818     PhaseRemoveUseless pru(initial_gvn(), for_igvn());
1819   }
1820 
1821   {
1822     ResourceMark rm;
1823     print_method(PHASE_BEFORE_STRINGOPTS, 3);
1824     PhaseStringOpts pso(initial_gvn(), for_igvn());
1825     print_method(PHASE_AFTER_STRINGOPTS, 3);
1826   }
1827 
1828   // now inline anything that we skipped the first time around
1829   if (!parse_time) {
1830     _late_inlines_pos = _late_inlines.length();
1831   }
1832 
</pre>
<hr />
<pre>
2072   remove_speculative_types(igvn);
2073 
2074   // No more new expensive nodes will be added to the list from here
2075   // so keep only the actual candidates for optimizations.
2076   cleanup_expensive_nodes(igvn);
2077 
2078   if (!failing() &amp;&amp; RenumberLiveNodes &amp;&amp; live_nodes() + NodeLimitFudgeFactor &lt; unique()) {
2079     Compile::TracePhase tp(&quot;&quot;, &amp;timers[_t_renumberLive]);
2080     initial_gvn()-&gt;replace_with(&amp;igvn);
2081     for_igvn()-&gt;clear();
2082     Unique_Node_List new_worklist(C-&gt;comp_arena());
2083     {
2084       ResourceMark rm;
2085       PhaseRenumberLive prl = PhaseRenumberLive(initial_gvn(), for_igvn(), &amp;new_worklist);
2086     }
2087     set_for_igvn(&amp;new_worklist);
2088     igvn = PhaseIterGVN(initial_gvn());
2089     igvn.optimize();
2090   }
2091 







2092   // Perform escape analysis
2093   if (_do_escape_analysis &amp;&amp; ConnectionGraph::has_candidates(this)) {
2094     if (has_loops()) {
2095       // Cleanup graph (remove dead nodes).
2096       TracePhase tp(&quot;idealLoop&quot;, &amp;timers[_t_idealLoop]);
2097       PhaseIdealLoop::optimize(igvn, LoopOptsMaxUnroll);
2098       if (major_progress()) print_method(PHASE_PHASEIDEAL_BEFORE_EA, 2);
2099       if (failing())  return;
2100     }
2101     ConnectionGraph::do_analysis(this, &amp;igvn);
2102 
2103     if (failing())  return;
2104 
2105     // Optimize out fields loads from scalar replaceable allocations.
2106     igvn.optimize();
2107     print_method(PHASE_ITER_GVN_AFTER_EA, 2);
2108 
2109     if (failing())  return;
2110 
2111     if (congraph() != NULL &amp;&amp; macro_count() &gt; 0) {
</pre>
<hr />
<pre>
2743             // Accumulate any precedence edges
2744             if (mem-&gt;in(i) != NULL) {
2745               n-&gt;add_prec(mem-&gt;in(i));
2746             }
2747           }
2748           // Everything above this point has been processed.
2749           done = true;
2750         }
2751         // Eliminate the previous StoreCM
2752         prev-&gt;set_req(MemNode::Memory, mem-&gt;in(MemNode::Memory));
2753         assert(mem-&gt;outcnt() == 0, &quot;should be dead&quot;);
2754         mem-&gt;disconnect_inputs(NULL, this);
2755       } else {
2756         prev = mem;
2757       }
2758       mem = prev-&gt;in(MemNode::Memory);
2759     }
2760   }
2761 }
2762 

2763 //------------------------------final_graph_reshaping_impl----------------------
2764 // Implement items 1-5 from final_graph_reshaping below.
2765 void Compile::final_graph_reshaping_impl( Node *n, Final_Reshape_Counts &amp;frc) {
2766 
2767   if ( n-&gt;outcnt() == 0 ) return; // dead node
2768   uint nop = n-&gt;Opcode();
2769 
2770   // Check for 2-input instruction with &quot;last use&quot; on right input.
2771   // Swap to left input.  Implements item (2).
2772   if( n-&gt;req() == 3 &amp;&amp;          // two-input instruction
2773       n-&gt;in(1)-&gt;outcnt() &gt; 1 &amp;&amp; // left use is NOT a last use
2774       (!n-&gt;in(1)-&gt;is_Phi() || n-&gt;in(1)-&gt;in(2) != n) &amp;&amp; // it is not data loop
2775       n-&gt;in(2)-&gt;outcnt() == 1 &amp;&amp;// right use IS a last use
2776       !n-&gt;in(2)-&gt;is_Con() ) {   // right use is not a constant
2777     // Check for commutative opcode
2778     switch( nop ) {
2779     case Op_AddI:  case Op_AddF:  case Op_AddD:  case Op_AddL:
2780     case Op_MaxI:  case Op_MinI:
2781     case Op_MulI:  case Op_MulF:  case Op_MulD:  case Op_MulL:
2782     case Op_AndL:  case Op_XorL:  case Op_OrL:
</pre>
<hr />
<pre>
2965   case Op_LoadUS:
2966   case Op_LoadI:
2967   case Op_LoadKlass:
2968   case Op_LoadNKlass:
2969   case Op_LoadL:
2970   case Op_LoadL_unaligned:
2971   case Op_LoadPLocked:
2972   case Op_LoadP:
2973   case Op_LoadN:
2974   case Op_LoadRange:
2975   case Op_LoadS: {
2976   handle_mem:
2977 #ifdef ASSERT
2978     if( VerifyOptoOopOffsets ) {
2979       MemNode* mem  = n-&gt;as_Mem();
2980       // Check to see if address types have grounded out somehow.
2981       const TypeInstPtr *tp = mem-&gt;in(MemNode::Address)-&gt;bottom_type()-&gt;isa_instptr();
2982       assert( !tp || oop_offset_is_sane(tp), &quot;&quot; );
2983     }
2984 #endif











































2985     break;
2986   }
2987 
2988   case Op_AddP: {               // Assert sane base pointers
2989     Node *addp = n-&gt;in(AddPNode::Address);
2990     assert( !addp-&gt;is_AddP() ||
2991             addp-&gt;in(AddPNode::Base)-&gt;is_top() || // Top OK for allocation
2992             addp-&gt;in(AddPNode::Base) == n-&gt;in(AddPNode::Base),
2993             &quot;Base pointers must match (addp %u)&quot;, addp-&gt;_idx );
2994 #ifdef _LP64
2995     if ((UseCompressedOops || UseCompressedClassPointers) &amp;&amp;
2996         addp-&gt;Opcode() == Op_ConP &amp;&amp;
2997         addp == n-&gt;in(AddPNode::Base) &amp;&amp;
2998         n-&gt;in(AddPNode::Offset)-&gt;is_Con()) {
2999       // If the transformation of ConP to ConN+DecodeN is beneficial depends
3000       // on the platform and on the compressed oops mode.
3001       // Use addressing with narrow klass to load with offset on x86.
3002       // Some platforms can use the constant pool to load ConP.
3003       // Do this transformation here since IGVN will convert ConN back to ConP.
3004       const Type* t = addp-&gt;bottom_type();
</pre>
<hr />
<pre>
3481           // Replace all nodes with identical edges as m with m
3482           k-&gt;subsume_by(m, this);
3483         }
3484       }
3485     }
3486     break;
3487   }
3488   case Op_CmpUL: {
3489     if (!Matcher::has_match_rule(Op_CmpUL)) {
3490       // No support for unsigned long comparisons
3491       ConINode* sign_pos = new ConINode(TypeInt::make(BitsPerLong - 1));
3492       Node* sign_bit_mask = new RShiftLNode(n-&gt;in(1), sign_pos);
3493       Node* orl = new OrLNode(n-&gt;in(1), sign_bit_mask);
3494       ConLNode* remove_sign_mask = new ConLNode(TypeLong::make(max_jlong));
3495       Node* andl = new AndLNode(orl, remove_sign_mask);
3496       Node* cmp = new CmpLNode(andl, n-&gt;in(2));
3497       n-&gt;subsume_by(cmp, this);
3498     }
3499     break;
3500   }

























3501   default:
3502     assert(!n-&gt;is_Call(), &quot;&quot;);
3503     assert(!n-&gt;is_Mem(), &quot;&quot;);
3504     assert(nop != Op_ProfileBoolean, &quot;should be eliminated during IGVN&quot;);
3505     break;
3506   }
3507 }
3508 
3509 //------------------------------final_graph_reshaping_walk---------------------
3510 // Replacing Opaque nodes with their input in final_graph_reshaping_impl(),
3511 // requires that the walk visits a node&#39;s inputs before visiting the node.
3512 void Compile::final_graph_reshaping_walk( Node_Stack &amp;nstack, Node *root, Final_Reshape_Counts &amp;frc ) {
3513   ResourceArea *area = Thread::current()-&gt;resource_area();
3514   Unique_Node_List sfpt(area);
3515 
3516   frc._visited.set(root-&gt;_idx); // first, mark node as visited
3517   uint cnt = root-&gt;req();
3518   Node *n = root;
3519   uint  i = 0;
3520   while (true) {
</pre>
<hr />
<pre>
3829   }
3830 }
3831 
3832 bool Compile::needs_clinit_barrier(ciMethod* method, ciMethod* accessing_method) {
3833   return method-&gt;is_static() &amp;&amp; needs_clinit_barrier(method-&gt;holder(), accessing_method);
3834 }
3835 
3836 bool Compile::needs_clinit_barrier(ciField* field, ciMethod* accessing_method) {
3837   return field-&gt;is_static() &amp;&amp; needs_clinit_barrier(field-&gt;holder(), accessing_method);
3838 }
3839 
3840 bool Compile::needs_clinit_barrier(ciInstanceKlass* holder, ciMethod* accessing_method) {
3841   if (holder-&gt;is_initialized()) {
3842     return false;
3843   }
3844   if (holder-&gt;is_being_initialized()) {
3845     if (accessing_method-&gt;holder() == holder) {
3846       // Access inside a class. The barrier can be elided when access happens in &lt;clinit&gt;,
3847       // &lt;init&gt;, or a static method. In all those cases, there was an initialization
3848       // barrier on the holder klass passed.
<span class="line-modified">3849       if (accessing_method-&gt;is_static_initializer() ||</span>
<span class="line-modified">3850           accessing_method-&gt;is_object_initializer() ||</span>
3851           accessing_method-&gt;is_static()) {
3852         return false;
3853       }
3854     } else if (accessing_method-&gt;holder()-&gt;is_subclass_of(holder)) {
3855       // Access from a subclass. The barrier can be elided only when access happens in &lt;clinit&gt;.
3856       // In case of &lt;init&gt; or a static method, the barrier is on the subclass is not enough:
3857       // child class can become fully initialized while its parent class is still being initialized.
<span class="line-modified">3858       if (accessing_method-&gt;is_static_initializer()) {</span>
3859         return false;
3860       }
3861     }
3862     ciMethod* root = method(); // the root method of compilation
3863     if (root != accessing_method) {
3864       return needs_clinit_barrier(holder, root); // check access in the context of compilation root
3865     }
3866   }
3867   return true;
3868 }
3869 
3870 #ifndef PRODUCT
3871 //------------------------------verify_graph_edges---------------------------
3872 // Walk the Graph and verify that there is a one-to-one correspondence
3873 // between Use-Def edges and Def-Use edges in the graph.
3874 void Compile::verify_graph_edges(bool no_dead_code) {
3875   if (VerifyGraphEdges) {
3876     ResourceArea *area = Thread::current()-&gt;resource_area();
3877     Unique_Node_List visited(area);
3878     // Call recursive graph walk to check edges
</pre>
<hr />
<pre>
3960                   _phase_name, C-&gt;unique(), C-&gt;live_nodes(), C-&gt;count_live_nodes_by_graph_walk());
3961   }
3962 
3963   if (VerifyIdealNodeCount) {
3964     Compile::current()-&gt;print_missing_nodes();
3965   }
3966 #endif
3967 
3968   if (_log != NULL) {
3969     _log-&gt;done(&quot;phase name=&#39;%s&#39; nodes=&#39;%d&#39; live=&#39;%d&#39;&quot;, _phase_name, C-&gt;unique(), C-&gt;live_nodes());
3970   }
3971 }
3972 
3973 //----------------------------static_subtype_check-----------------------------
3974 // Shortcut important common cases when superklass is exact:
3975 // (0) superklass is java.lang.Object (can occur in reflective code)
3976 // (1) subklass is already limited to a subtype of superklass =&gt; always ok
3977 // (2) subklass does not overlap with superklass =&gt; always fail
3978 // (3) superklass has NO subtypes and we can check with a simple compare.
3979 int Compile::static_subtype_check(ciKlass* superk, ciKlass* subk) {
<span class="line-modified">3980   if (StressReflectiveCode) {</span>
3981     return SSC_full_test;       // Let caller generate the general case.
3982   }
3983 
3984   if (superk == env()-&gt;Object_klass()) {
3985     return SSC_always_true;     // (0) this test cannot fail
3986   }
3987 
3988   ciType* superelem = superk;
<span class="line-modified">3989   if (superelem-&gt;is_array_klass())</span>

3990     superelem = superelem-&gt;as_array_klass()-&gt;base_element_type();

3991 
3992   if (!subk-&gt;is_interface()) {  // cannot trust static interface types yet
3993     if (subk-&gt;is_subtype_of(superk)) {
3994       return SSC_always_true;   // (1) false path dead; no dynamic test needed
3995     }
3996     if (!(superelem-&gt;is_klass() &amp;&amp; superelem-&gt;as_klass()-&gt;is_interface()) &amp;&amp;
3997         !superk-&gt;is_subtype_of(subk)) {
3998       return SSC_always_false;
3999     }
4000   }
4001 





4002   // If casting to an instance klass, it must have no subtypes
4003   if (superk-&gt;is_interface()) {
4004     // Cannot trust interfaces yet.
4005     // %%% S.B. superk-&gt;nof_implementors() == 1
4006   } else if (superelem-&gt;is_instance_klass()) {
4007     ciInstanceKlass* ik = superelem-&gt;as_instance_klass();
4008     if (!ik-&gt;has_subklass() &amp;&amp; !ik-&gt;is_interface()) {
4009       if (!ik-&gt;is_final()) {
4010         // Add a dependency if there is a chance of a later subclass.
4011         dependencies()-&gt;assert_leaf_type(ik);
4012       }
4013       return SSC_easy_test;     // (3) caller can do a simple ptr comparison
4014     }
4015   } else {
4016     // A primitive array type has no subtypes.
4017     return SSC_easy_test;       // (3) caller can do a simple ptr comparison
4018   }
4019 
4020   return SSC_full_test;
4021 }
</pre>
<hr />
<pre>
4431     for (uint next = 0; next &lt; worklist.size(); ++next) {
4432       Node *n  = worklist.at(next);
4433       const Type* t = igvn.type_or_null(n);
4434       assert((t == NULL) || (t == t-&gt;remove_speculative()), &quot;no more speculative types&quot;);
4435       if (n-&gt;is_Type()) {
4436         t = n-&gt;as_Type()-&gt;type();
4437         assert(t == t-&gt;remove_speculative(), &quot;no more speculative types&quot;);
4438       }
4439       uint max = n-&gt;len();
4440       for( uint i = 0; i &lt; max; ++i ) {
4441         Node *m = n-&gt;in(i);
4442         if (not_a_node(m))  continue;
4443         worklist.push(m);
4444       }
4445     }
4446     igvn.check_no_speculative_types();
4447 #endif
4448   }
4449 }
4450 





















4451 // Auxiliary method to support randomized stressing/fuzzing.
4452 //
4453 // This method can be called the arbitrary number of times, with current count
4454 // as the argument. The logic allows selecting a single candidate from the
4455 // running list of candidates as follows:
4456 //    int count = 0;
4457 //    Cand* selected = null;
4458 //    while(cand = cand-&gt;next()) {
4459 //      if (randomized_select(++count)) {
4460 //        selected = cand;
4461 //      }
4462 //    }
4463 //
4464 // Including count equalizes the chances any candidate is &quot;selected&quot;.
4465 // This is useful when we don&#39;t have the complete list of candidates to choose
4466 // from uniformly. In this case, we need to adjust the randomicity of the
4467 // selection, or else we will end up biasing the selection towards the latter
4468 // candidates.
4469 //
4470 // Quick back-envelope calculation shows that for the list of n candidates
</pre>
</td>
<td>
<hr />
<pre>
  50 #include &quot;opto/divnode.hpp&quot;
  51 #include &quot;opto/escape.hpp&quot;
  52 #include &quot;opto/idealGraphPrinter.hpp&quot;
  53 #include &quot;opto/loopnode.hpp&quot;
  54 #include &quot;opto/machnode.hpp&quot;
  55 #include &quot;opto/macro.hpp&quot;
  56 #include &quot;opto/matcher.hpp&quot;
  57 #include &quot;opto/mathexactnode.hpp&quot;
  58 #include &quot;opto/memnode.hpp&quot;
  59 #include &quot;opto/mulnode.hpp&quot;
  60 #include &quot;opto/narrowptrnode.hpp&quot;
  61 #include &quot;opto/node.hpp&quot;
  62 #include &quot;opto/opcodes.hpp&quot;
  63 #include &quot;opto/output.hpp&quot;
  64 #include &quot;opto/parse.hpp&quot;
  65 #include &quot;opto/phaseX.hpp&quot;
  66 #include &quot;opto/rootnode.hpp&quot;
  67 #include &quot;opto/runtime.hpp&quot;
  68 #include &quot;opto/stringopts.hpp&quot;
  69 #include &quot;opto/type.hpp&quot;
<span class="line-added">  70 #include &quot;opto/valuetypenode.hpp&quot;</span>
  71 #include &quot;opto/vectornode.hpp&quot;
  72 #include &quot;runtime/arguments.hpp&quot;
  73 #include &quot;runtime/sharedRuntime.hpp&quot;
  74 #include &quot;runtime/signature.hpp&quot;
  75 #include &quot;runtime/stubRoutines.hpp&quot;
  76 #include &quot;runtime/timer.hpp&quot;
  77 #include &quot;utilities/align.hpp&quot;
  78 #include &quot;utilities/copy.hpp&quot;
  79 #include &quot;utilities/macros.hpp&quot;
  80 #include &quot;utilities/resourceHash.hpp&quot;
  81 
  82 
  83 // -------------------- Compile::mach_constant_base_node -----------------------
  84 // Constant table base node singleton.
  85 MachConstantBaseNode* Compile::mach_constant_base_node() {
  86   if (_mach_constant_base_node == NULL) {
  87     _mach_constant_base_node = new MachConstantBaseNode();
  88     _mach_constant_base_node-&gt;add_req(C-&gt;root());
  89   }
  90   return _mach_constant_base_node;
</pre>
<hr />
<pre>
 388   for (int i = range_check_cast_count() - 1; i &gt;= 0; i--) {
 389     Node* cast = range_check_cast_node(i);
 390     if (!useful.member(cast)) {
 391       remove_range_check_cast(cast);
 392     }
 393   }
 394   // Remove useless expensive nodes
 395   for (int i = C-&gt;expensive_count()-1; i &gt;= 0; i--) {
 396     Node* n = C-&gt;expensive_node(i);
 397     if (!useful.member(n)) {
 398       remove_expensive_node(n);
 399     }
 400   }
 401   // Remove useless Opaque4 nodes
 402   for (int i = opaque4_count() - 1; i &gt;= 0; i--) {
 403     Node* opaq = opaque4_node(i);
 404     if (!useful.member(opaq)) {
 405       remove_opaque4_node(opaq);
 406     }
 407   }
<span class="line-added"> 408   // Remove useless value type nodes</span>
<span class="line-added"> 409   if (_value_type_nodes != NULL) {</span>
<span class="line-added"> 410     _value_type_nodes-&gt;remove_useless_nodes(useful.member_set());</span>
<span class="line-added"> 411   }</span>
 412   BarrierSetC2* bs = BarrierSet::barrier_set()-&gt;barrier_set_c2();
 413   bs-&gt;eliminate_useless_gc_barriers(useful, this);
 414   // clean up the late inline lists
 415   remove_useless_late_inlines(&amp;_string_late_inlines, useful);
 416   remove_useless_late_inlines(&amp;_boxing_late_inlines, useful);
 417   remove_useless_late_inlines(&amp;_late_inlines, useful);
 418   debug_only(verify_graph_edges(true/*check for no_dead_code*/);)
 419 }
 420 
 421 // ============================================================================
 422 //------------------------------CompileWrapper---------------------------------
 423 class CompileWrapper : public StackObj {
 424   Compile *const _compile;
 425  public:
 426   CompileWrapper(Compile* compile);
 427 
 428   ~CompileWrapper();
 429 };
 430 
 431 CompileWrapper::CompileWrapper(Compile* compile) : _compile(compile) {
</pre>
<hr />
<pre>
 624   // Node list that Iterative GVN will start with
 625   Unique_Node_List for_igvn(comp_arena());
 626   set_for_igvn(&amp;for_igvn);
 627 
 628   // GVN that will be run immediately on new nodes
 629   uint estimated_size = method()-&gt;code_size()*4+64;
 630   estimated_size = (estimated_size &lt; MINIMUM_NODE_HASH ? MINIMUM_NODE_HASH : estimated_size);
 631   PhaseGVN gvn(node_arena(), estimated_size);
 632   set_initial_gvn(&amp;gvn);
 633 
 634   print_inlining_init();
 635   { // Scope for timing the parser
 636     TracePhase tp(&quot;parse&quot;, &amp;timers[_t_parser]);
 637 
 638     // Put top into the hash table ASAP.
 639     initial_gvn()-&gt;transform_no_reclaim(top());
 640 
 641     // Set up tf(), start(), and find a CallGenerator.
 642     CallGenerator* cg = NULL;
 643     if (is_osr_compilation()) {
<span class="line-modified"> 644       init_tf(TypeFunc::make(method(), /* is_osr_compilation = */ true));</span>
<span class="line-modified"> 645       StartNode* s = new StartOSRNode(root(), tf()-&gt;domain_sig());</span>


 646       initial_gvn()-&gt;set_type_bottom(s);
 647       init_start(s);
 648       cg = CallGenerator::for_osr(method(), entry_bci());
 649     } else {
 650       // Normal case.
 651       init_tf(TypeFunc::make(method()));
<span class="line-modified"> 652       StartNode* s = new StartNode(root(), tf()-&gt;domain_cc());</span>
 653       initial_gvn()-&gt;set_type_bottom(s);
 654       init_start(s);
 655       if (method()-&gt;intrinsic_id() == vmIntrinsics::_Reference_get) {
 656         // With java.lang.ref.reference.get() we must go through the
 657         // intrinsic - even when get() is the root
 658         // method of the compile - so that, if necessary, the value in
 659         // the referent field of the reference object gets recorded by
 660         // the pre-barrier code.
 661         cg = find_intrinsic(method(), false);
 662       }
 663       if (cg == NULL) {
 664         float past_uses = method()-&gt;interpreter_invocation_count();
 665         float expected_uses = past_uses;
 666         cg = CallGenerator::for_inline(method(), expected_uses);
 667       }
 668     }
 669     if (failing())  return;
 670     if (cg == NULL) {
 671       record_method_not_compilable(&quot;cannot parse method&quot;);
 672       return;
</pre>
<hr />
<pre>
 757     }
 758   }
 759 #endif
 760 
 761 #ifdef ASSERT
 762   BarrierSetC2* bs = BarrierSet::barrier_set()-&gt;barrier_set_c2();
 763   bs-&gt;verify_gc_barriers(this, BarrierSetC2::BeforeCodeGen);
 764 #endif
 765 
 766   // Dump compilation data to replay it.
 767   if (directive-&gt;DumpReplayOption) {
 768     env()-&gt;dump_replay_data(_compile_id);
 769   }
 770   if (directive-&gt;DumpInlineOption &amp;&amp; (ilt() != NULL)) {
 771     env()-&gt;dump_inline_data(_compile_id);
 772   }
 773 
 774   // Now that we know the size of all the monitors we can add a fixed slot
 775   // for the original deopt pc.
 776   int next_slot = fixed_slots() + (sizeof(address) / VMRegImpl::stack_slot_size);
<span class="line-added"> 777   if (needs_stack_repair()) {</span>
<span class="line-added"> 778     // One extra slot for the special stack increment value</span>
<span class="line-added"> 779     next_slot += 2;</span>
<span class="line-added"> 780   }</span>
 781   set_fixed_slots(next_slot);
 782 
 783   // Compute when to use implicit null checks. Used by matching trap based
 784   // nodes and NullCheck optimization.
 785   set_allowed_deopt_reasons();
 786 
 787   // Now generate code
 788   Code_Gen();
 789 }
 790 
 791 //------------------------------Compile----------------------------------------
 792 // Compile a runtime stub
 793 Compile::Compile( ciEnv* ci_env,
 794                   TypeFunc_generator generator,
 795                   address stub_function,
 796                   const char *stub_name,
 797                   int is_fancy_jump,
 798                   bool pass_tls,
 799                   bool save_arg_registers,
 800                   bool return_pc,
</pre>
<hr />
<pre>
 916   // Create Debug Information Recorder to record scopes, oopmaps, etc.
 917   env()-&gt;set_oop_recorder(new OopRecorder(env()-&gt;arena()));
 918   env()-&gt;set_debug_info(new DebugInformationRecorder(env()-&gt;oop_recorder()));
 919   env()-&gt;set_dependencies(new Dependencies(env()));
 920 
 921   _fixed_slots = 0;
 922   set_has_split_ifs(false);
 923   set_has_loops(has_method() &amp;&amp; method()-&gt;has_loops()); // first approximation
 924   set_has_stringbuilder(false);
 925   set_has_boxed_value(false);
 926   _trap_can_recompile = false;  // no traps emitted yet
 927   _major_progress = true; // start out assuming good things will happen
 928   set_has_unsafe_access(false);
 929   set_max_vector_size(0);
 930   set_clear_upper_avx(false);  //false as default for clear upper bits of ymm registers
 931   Copy::zero_to_bytes(_trap_hist, sizeof(_trap_hist));
 932   set_decompile_count(0);
 933 
 934   set_do_freq_based_layout(_directive-&gt;BlockLayoutByFrequencyOption);
 935   _loop_opts_cnt = LoopOptsCount;
<span class="line-added"> 936   _has_flattened_accesses = false;</span>
<span class="line-added"> 937   _flattened_accesses_share_alias = true;</span>
<span class="line-added"> 938 </span>
 939   set_do_inlining(Inline);
 940   set_max_inline_size(MaxInlineSize);
 941   set_freq_inline_size(FreqInlineSize);
 942   set_do_scheduling(OptoScheduling);
 943   set_do_count_invocations(false);
 944   set_do_method_data_update(false);
 945 
 946   set_do_vector_loop(false);
 947 
 948   if (AllowVectorizeOnDemand) {
 949     if (has_method() &amp;&amp; (_directive-&gt;VectorizeOption || _directive-&gt;VectorizeDebugOption)) {
 950       set_do_vector_loop(true);
 951       NOT_PRODUCT(if (do_vector_loop() &amp;&amp; Verbose) {tty-&gt;print(&quot;Compile::Init: do vectorized loops (SIMD like) for method %s\n&quot;,  method()-&gt;name()-&gt;as_quoted_ascii());})
 952     } else if (has_method() &amp;&amp; method()-&gt;name() != 0 &amp;&amp;
 953                method()-&gt;intrinsic_id() == vmIntrinsics::_forEachRemaining) {
 954       set_do_vector_loop(true);
 955     }
 956   }
 957   set_use_cmove(UseCMoveUnconditionally /* || do_vector_loop()*/); //TODO: consider do_vector_loop() mandate use_cmove unconditionally
 958   NOT_PRODUCT(if (use_cmove() &amp;&amp; Verbose &amp;&amp; has_method()) {tty-&gt;print(&quot;Compile::Init: use CMove without profitability tests for method %s\n&quot;,  method()-&gt;name()-&gt;as_quoted_ascii());})
</pre>
<hr />
<pre>
1002   Copy::zero_to_bytes(ats, sizeof(AliasType)*grow_ats);
1003   {
1004     for (int i = 0; i &lt; grow_ats; i++)  _alias_types[i] = &amp;ats[i];
1005   }
1006   // Initialize the first few types.
1007   _alias_types[AliasIdxTop]-&gt;Init(AliasIdxTop, NULL);
1008   _alias_types[AliasIdxBot]-&gt;Init(AliasIdxBot, TypePtr::BOTTOM);
1009   _alias_types[AliasIdxRaw]-&gt;Init(AliasIdxRaw, TypeRawPtr::BOTTOM);
1010   _num_alias_types = AliasIdxRaw+1;
1011   // Zero out the alias type cache.
1012   Copy::zero_to_bytes(_alias_cache, sizeof(_alias_cache));
1013   // A NULL adr_type hits in the cache right away.  Preload the right answer.
1014   probe_alias_cache(NULL)-&gt;_index = AliasIdxTop;
1015 
1016   _intrinsics = NULL;
1017   _macro_nodes = new(comp_arena()) GrowableArray&lt;Node*&gt;(comp_arena(), 8,  0, NULL);
1018   _predicate_opaqs = new(comp_arena()) GrowableArray&lt;Node*&gt;(comp_arena(), 8,  0, NULL);
1019   _expensive_nodes = new(comp_arena()) GrowableArray&lt;Node*&gt;(comp_arena(), 8,  0, NULL);
1020   _range_check_casts = new(comp_arena()) GrowableArray&lt;Node*&gt;(comp_arena(), 8,  0, NULL);
1021   _opaque4_nodes = new(comp_arena()) GrowableArray&lt;Node*&gt;(comp_arena(), 8,  0, NULL);
<span class="line-added">1022   _value_type_nodes = new (comp_arena()) Unique_Node_List(comp_arena());</span>
1023   register_library_intrinsics();
1024 #ifdef ASSERT
1025   _type_verify_symmetry = true;
1026 #endif
1027 }
1028 
1029 //---------------------------init_start----------------------------------------
1030 // Install the StartNode on this compile object.
1031 void Compile::init_start(StartNode* s) {
1032   if (failing())
1033     return; // already failing
1034   assert(s == start(), &quot;&quot;);
1035 }
1036 
1037 /**
1038  * Return the &#39;StartNode&#39;. We must not have a pending failure, since the ideal graph
1039  * can be in an inconsistent state, i.e., we can get segmentation faults when traversing
1040  * the ideal graph.
1041  */
1042 StartNode* Compile::start() const {
</pre>
<hr />
<pre>
1230 bool Compile::allow_range_check_smearing() const {
1231   // If this method has already thrown a range-check,
1232   // assume it was because we already tried range smearing
1233   // and it failed.
1234   uint already_trapped = trap_count(Deoptimization::Reason_range_check);
1235   return !already_trapped;
1236 }
1237 
1238 
1239 //------------------------------flatten_alias_type-----------------------------
1240 const TypePtr *Compile::flatten_alias_type( const TypePtr *tj ) const {
1241   int offset = tj-&gt;offset();
1242   TypePtr::PTR ptr = tj-&gt;ptr();
1243 
1244   // Known instance (scalarizable allocation) alias only with itself.
1245   bool is_known_inst = tj-&gt;isa_oopptr() != NULL &amp;&amp;
1246                        tj-&gt;is_oopptr()-&gt;is_known_instance();
1247 
1248   // Process weird unsafe references.
1249   if (offset == Type::OffsetBot &amp;&amp; (tj-&gt;isa_instptr() /*|| tj-&gt;isa_klassptr()*/)) {
<span class="line-modified">1250     bool default_value_load = EnableValhalla &amp;&amp; tj-&gt;is_instptr()-&gt;klass() == ciEnv::current()-&gt;Class_klass();</span>
<span class="line-added">1251     assert(InlineUnsafeOps || default_value_load, &quot;indeterminate pointers come only from unsafe ops&quot;);</span>
1252     assert(!is_known_inst, &quot;scalarizable allocation should not have unsafe references&quot;);
1253     tj = TypeOopPtr::BOTTOM;
1254     ptr = tj-&gt;ptr();
1255     offset = tj-&gt;offset();
1256   }
1257 
1258   // Array pointers need some flattening
1259   const TypeAryPtr *ta = tj-&gt;isa_aryptr();
1260   if (ta &amp;&amp; ta-&gt;is_stable()) {
1261     // Erase stability property for alias analysis.
1262     tj = ta = ta-&gt;cast_to_stable(false);
1263   }
<span class="line-added">1264   if (ta &amp;&amp; ta-&gt;is_not_flat()) {</span>
<span class="line-added">1265     // Erase not flat property for alias analysis.</span>
<span class="line-added">1266     tj = ta = ta-&gt;cast_to_not_flat(false);</span>
<span class="line-added">1267   }</span>
<span class="line-added">1268   if (ta &amp;&amp; ta-&gt;is_not_null_free()) {</span>
<span class="line-added">1269     // Erase not null free property for alias analysis.</span>
<span class="line-added">1270     tj = ta = ta-&gt;cast_to_not_null_free(false);</span>
<span class="line-added">1271   }</span>
<span class="line-added">1272 </span>
1273   if( ta &amp;&amp; is_known_inst ) {
1274     if ( offset != Type::OffsetBot &amp;&amp;
1275          offset &gt; arrayOopDesc::length_offset_in_bytes() ) {
1276       offset = Type::OffsetBot; // Flatten constant access into array body only
<span class="line-modified">1277       tj = ta = TypeAryPtr::make(ptr, ta-&gt;ary(), ta-&gt;klass(), true, Type::Offset(offset), ta-&gt;field_offset(), ta-&gt;instance_id());</span>
1278     }
1279   } else if( ta &amp;&amp; _AliasLevel &gt;= 2 ) {
1280     // For arrays indexed by constant indices, we flatten the alias
1281     // space to include all of the array body.  Only the header, klass
1282     // and array length can be accessed un-aliased.
<span class="line-added">1283     // For flattened value type array, each field has its own slice so</span>
<span class="line-added">1284     // we must include the field offset.</span>
1285     if( offset != Type::OffsetBot ) {
1286       if( ta-&gt;const_oop() ) { // MethodData* or Method*
1287         offset = Type::OffsetBot;   // Flatten constant access into array body
<span class="line-modified">1288         tj = ta = TypeAryPtr::make(ptr,ta-&gt;const_oop(),ta-&gt;ary(),ta-&gt;klass(),false,Type::Offset(offset), ta-&gt;field_offset());</span>
1289       } else if( offset == arrayOopDesc::length_offset_in_bytes() ) {
1290         // range is OK as-is.
1291         tj = ta = TypeAryPtr::RANGE;
1292       } else if( offset == oopDesc::klass_offset_in_bytes() ) {
1293         tj = TypeInstPtr::KLASS; // all klass loads look alike
1294         ta = TypeAryPtr::RANGE; // generic ignored junk
1295         ptr = TypePtr::BotPTR;
1296       } else if( offset == oopDesc::mark_offset_in_bytes() ) {
1297         tj = TypeInstPtr::MARK;
1298         ta = TypeAryPtr::RANGE; // generic ignored junk
1299         ptr = TypePtr::BotPTR;
1300       } else {                  // Random constant offset into array body
1301         offset = Type::OffsetBot;   // Flatten constant access into array body
<span class="line-modified">1302         tj = ta = TypeAryPtr::make(ptr,ta-&gt;ary(),ta-&gt;klass(),false,Type::Offset(offset), ta-&gt;field_offset());</span>
1303       }
1304     }
1305     // Arrays of fixed size alias with arrays of unknown size.
1306     if (ta-&gt;size() != TypeInt::POS) {
1307       const TypeAry *tary = TypeAry::make(ta-&gt;elem(), TypeInt::POS);
<span class="line-modified">1308       tj = ta = TypeAryPtr::make(ptr,ta-&gt;const_oop(),tary,ta-&gt;klass(),false,Type::Offset(offset), ta-&gt;field_offset());</span>
1309     }
1310     // Arrays of known objects become arrays of unknown objects.
1311     if (ta-&gt;elem()-&gt;isa_narrowoop() &amp;&amp; ta-&gt;elem() != TypeNarrowOop::BOTTOM) {
1312       const TypeAry *tary = TypeAry::make(TypeNarrowOop::BOTTOM, ta-&gt;size());
<span class="line-modified">1313       tj = ta = TypeAryPtr::make(ptr,ta-&gt;const_oop(),tary,NULL,false,Type::Offset(offset), ta-&gt;field_offset());</span>
1314     }
1315     if (ta-&gt;elem()-&gt;isa_oopptr() &amp;&amp; ta-&gt;elem() != TypeInstPtr::BOTTOM) {
1316       const TypeAry *tary = TypeAry::make(TypeInstPtr::BOTTOM, ta-&gt;size());
<span class="line-modified">1317       tj = ta = TypeAryPtr::make(ptr,ta-&gt;const_oop(),tary,NULL,false,Type::Offset(offset), ta-&gt;field_offset());</span>
<span class="line-added">1318     }</span>
<span class="line-added">1319     // Initially all flattened array accesses share a single slice</span>
<span class="line-added">1320     if (ta-&gt;elem()-&gt;isa_valuetype() &amp;&amp; ta-&gt;elem() != TypeValueType::BOTTOM &amp;&amp; _flattened_accesses_share_alias) {</span>
<span class="line-added">1321       const TypeAry *tary = TypeAry::make(TypeValueType::BOTTOM, ta-&gt;size());</span>
<span class="line-added">1322       tj = ta = TypeAryPtr::make(ptr,ta-&gt;const_oop(),tary,NULL,false,Type::Offset(offset), Type::Offset(Type::OffsetBot));</span>
1323     }
1324     // Arrays of bytes and of booleans both use &#39;bastore&#39; and &#39;baload&#39; so
1325     // cannot be distinguished by bytecode alone.
1326     if (ta-&gt;elem() == TypeInt::BOOL) {
1327       const TypeAry *tary = TypeAry::make(TypeInt::BYTE, ta-&gt;size());
1328       ciKlass* aklass = ciTypeArrayKlass::make(T_BYTE);
<span class="line-modified">1329       tj = ta = TypeAryPtr::make(ptr,ta-&gt;const_oop(),tary,aklass,false,Type::Offset(offset), ta-&gt;field_offset());</span>
1330     }
1331     // During the 2nd round of IterGVN, NotNull castings are removed.
1332     // Make sure the Bottom and NotNull variants alias the same.
1333     // Also, make sure exact and non-exact variants alias the same.
1334     if (ptr == TypePtr::NotNull || ta-&gt;klass_is_exact() || ta-&gt;speculative() != NULL) {
<span class="line-modified">1335       tj = ta = TypeAryPtr::make(TypePtr::BotPTR,ta-&gt;ary(),ta-&gt;klass(),false,Type::Offset(offset), ta-&gt;field_offset());</span>
1336     }
1337   }
1338 
1339   // Oop pointers need some flattening
1340   const TypeInstPtr *to = tj-&gt;isa_instptr();
1341   if( to &amp;&amp; _AliasLevel &gt;= 2 &amp;&amp; to != TypeOopPtr::BOTTOM ) {
1342     ciInstanceKlass *k = to-&gt;klass()-&gt;as_instance_klass();
1343     if( ptr == TypePtr::Constant ) {
1344       if (to-&gt;klass() != ciEnv::current()-&gt;Class_klass() ||
1345           offset &lt; k-&gt;size_helper() * wordSize) {
1346         // No constant oop pointers (such as Strings); they alias with
1347         // unknown strings.
1348         assert(!is_known_inst, &quot;not scalarizable allocation&quot;);
<span class="line-modified">1349         tj = to = TypeInstPtr::make(TypePtr::BotPTR,to-&gt;klass(),false,0,Type::Offset(offset), to-&gt;klass()-&gt;flatten_array());</span>
1350       }
1351     } else if( is_known_inst ) {
1352       tj = to; // Keep NotNull and klass_is_exact for instance type
1353     } else if( ptr == TypePtr::NotNull || to-&gt;klass_is_exact() ) {
1354       // During the 2nd round of IterGVN, NotNull castings are removed.
1355       // Make sure the Bottom and NotNull variants alias the same.
1356       // Also, make sure exact and non-exact variants alias the same.
<span class="line-modified">1357       tj = to = TypeInstPtr::make(TypePtr::BotPTR,to-&gt;klass(),false,0,Type::Offset(offset), to-&gt;klass()-&gt;flatten_array());</span>
1358     }
1359     if (to-&gt;speculative() != NULL) {
<span class="line-modified">1360       tj = to = TypeInstPtr::make(to-&gt;ptr(),to-&gt;klass(),to-&gt;klass_is_exact(),to-&gt;const_oop(),Type::Offset(to-&gt;offset()), to-&gt;klass()-&gt;flatten_array(), to-&gt;instance_id());</span>
1361     }
1362     // Canonicalize the holder of this field
1363     if (offset &gt;= 0 &amp;&amp; offset &lt; instanceOopDesc::base_offset_in_bytes()) {
1364       // First handle header references such as a LoadKlassNode, even if the
1365       // object&#39;s klass is unloaded at compile time (4965979).
1366       if (!is_known_inst) { // Do it only for non-instance types
<span class="line-modified">1367         tj = to = TypeInstPtr::make(TypePtr::BotPTR, env()-&gt;Object_klass(), false, NULL, Type::Offset(offset), false);</span>
1368       }
1369     } else if (offset &lt; 0 || offset &gt;= k-&gt;size_helper() * wordSize) {
1370       // Static fields are in the space above the normal instance
1371       // fields in the java.lang.Class instance.
1372       if (to-&gt;klass() != ciEnv::current()-&gt;Class_klass()) {
1373         to = NULL;
1374         tj = TypeOopPtr::BOTTOM;
1375         offset = tj-&gt;offset();
1376       }
1377     } else {
1378       ciInstanceKlass *canonical_holder = k-&gt;get_canonical_holder(offset);
1379       if (!k-&gt;equals(canonical_holder) || tj-&gt;offset() != offset) {
1380         if( is_known_inst ) {
<span class="line-modified">1381           tj = to = TypeInstPtr::make(to-&gt;ptr(), canonical_holder, true, NULL, Type::Offset(offset), canonical_holder-&gt;flatten_array(), to-&gt;instance_id());</span>
1382         } else {
<span class="line-modified">1383           tj = to = TypeInstPtr::make(to-&gt;ptr(), canonical_holder, false, NULL, Type::Offset(offset), canonical_holder-&gt;flatten_array());</span>
1384         }
1385       }
1386     }
1387   }
1388 
1389   // Klass pointers to object array klasses need some flattening
1390   const TypeKlassPtr *tk = tj-&gt;isa_klassptr();
1391   if( tk ) {
1392     // If we are referencing a field within a Klass, we need
1393     // to assume the worst case of an Object.  Both exact and
1394     // inexact types must flatten to the same alias class so
1395     // use NotNull as the PTR.
1396     if ( offset == Type::OffsetBot || (offset &gt;= 0 &amp;&amp; (size_t)offset &lt; sizeof(Klass)) ) {
1397 
1398       tj = tk = TypeKlassPtr::make(TypePtr::NotNull,
1399                                    TypeKlassPtr::OBJECT-&gt;klass(),
<span class="line-modified">1400                                    Type::Offset(offset),</span>
<span class="line-added">1401                                    false);</span>
1402     }
1403 
1404     ciKlass* klass = tk-&gt;klass();
<span class="line-modified">1405     if (klass != NULL &amp;&amp; klass-&gt;is_obj_array_klass()) {</span>
1406       ciKlass* k = TypeAryPtr::OOPS-&gt;klass();
1407       if( !k || !k-&gt;is_loaded() )                  // Only fails for some -Xcomp runs
1408         k = TypeInstPtr::BOTTOM-&gt;klass();
<span class="line-modified">1409       tj = tk = TypeKlassPtr::make(TypePtr::NotNull, k, Type::Offset(offset), false);</span>
1410     }
1411 
1412     // Check for precise loads from the primary supertype array and force them
1413     // to the supertype cache alias index.  Check for generic array loads from
1414     // the primary supertype array and also force them to the supertype cache
1415     // alias index.  Since the same load can reach both, we need to merge
1416     // these 2 disparate memories into the same alias class.  Since the
1417     // primary supertype array is read-only, there&#39;s no chance of confusion
1418     // where we bypass an array load and an array store.
1419     int primary_supers_offset = in_bytes(Klass::primary_supers_offset());
1420     if (offset == Type::OffsetBot ||
1421         (offset &gt;= primary_supers_offset &amp;&amp;
1422          offset &lt; (int)(primary_supers_offset + Klass::primary_super_limit() * wordSize)) ||
1423         offset == (int)in_bytes(Klass::secondary_super_cache_offset())) {
1424       offset = in_bytes(Klass::secondary_super_cache_offset());
<span class="line-modified">1425       tj = tk = TypeKlassPtr::make(TypePtr::NotNull, tk-&gt;klass(), Type::Offset(offset), tk-&gt;flat_array());</span>
1426     }
1427   }
1428 
1429   // Flatten all Raw pointers together.
1430   if (tj-&gt;base() == Type::RawPtr)
1431     tj = TypeRawPtr::BOTTOM;
1432 
1433   if (tj-&gt;base() == Type::AnyPtr)
1434     tj = TypePtr::BOTTOM;      // An error, which the caller must check for.
1435 
1436   // Flatten all to bottom for now
1437   switch( _AliasLevel ) {
1438   case 0:
1439     tj = TypePtr::BOTTOM;
1440     break;
1441   case 1:                       // Flatten to: oop, static, field or array
1442     switch (tj-&gt;base()) {
1443     //case Type::AryPtr: tj = TypeAryPtr::RANGE;    break;
1444     case Type::RawPtr:   tj = TypeRawPtr::BOTTOM;   break;
1445     case Type::AryPtr:   // do not distinguish arrays at all
</pre>
<hr />
<pre>
1544   intptr_t key = (intptr_t) adr_type;
1545   key ^= key &gt;&gt; logAliasCacheSize;
1546   return &amp;_alias_cache[key &amp; right_n_bits(logAliasCacheSize)];
1547 }
1548 
1549 
1550 //-----------------------------grow_alias_types--------------------------------
1551 void Compile::grow_alias_types() {
1552   const int old_ats  = _max_alias_types; // how many before?
1553   const int new_ats  = old_ats;          // how many more?
1554   const int grow_ats = old_ats+new_ats;  // how many now?
1555   _max_alias_types = grow_ats;
1556   _alias_types =  REALLOC_ARENA_ARRAY(comp_arena(), AliasType*, _alias_types, old_ats, grow_ats);
1557   AliasType* ats =    NEW_ARENA_ARRAY(comp_arena(), AliasType, new_ats);
1558   Copy::zero_to_bytes(ats, sizeof(AliasType)*new_ats);
1559   for (int i = 0; i &lt; new_ats; i++)  _alias_types[old_ats+i] = &amp;ats[i];
1560 }
1561 
1562 
1563 //--------------------------------find_alias_type------------------------------
<span class="line-modified">1564 Compile::AliasType* Compile::find_alias_type(const TypePtr* adr_type, bool no_create, ciField* original_field, bool uncached) {</span>
1565   if (_AliasLevel == 0)
1566     return alias_type(AliasIdxBot);
1567 
<span class="line-modified">1568   AliasCacheEntry* ace = NULL;</span>
<span class="line-modified">1569   if (!uncached) {</span>
<span class="line-modified">1570     ace = probe_alias_cache(adr_type);</span>
<span class="line-added">1571     if (ace-&gt;_adr_type == adr_type) {</span>
<span class="line-added">1572       return alias_type(ace-&gt;_index);</span>
<span class="line-added">1573     }</span>
1574   }
1575 
1576   // Handle special cases.
1577   if (adr_type == NULL)             return alias_type(AliasIdxTop);
1578   if (adr_type == TypePtr::BOTTOM)  return alias_type(AliasIdxBot);
1579 
1580   // Do it the slow way.
1581   const TypePtr* flat = flatten_alias_type(adr_type);
1582 
1583 #ifdef ASSERT
1584   {
1585     ResourceMark rm;
1586     assert(flat == flatten_alias_type(flat), &quot;not idempotent: adr_type = %s; flat = %s =&gt; %s&quot;,
1587            Type::str(adr_type), Type::str(flat), Type::str(flatten_alias_type(flat)));
1588     assert(flat != TypePtr::BOTTOM, &quot;cannot alias-analyze an untyped ptr: adr_type = %s&quot;,
1589            Type::str(adr_type));
1590     if (flat-&gt;isa_oopptr() &amp;&amp; !flat-&gt;isa_klassptr()) {
1591       const TypeOopPtr* foop = flat-&gt;is_oopptr();
1592       // Scalarizable allocations have exact klass always.
1593       bool exact = !foop-&gt;klass_is_exact() || foop-&gt;is_known_instance();
</pre>
<hr />
<pre>
1603     if (alias_type(i)-&gt;adr_type() == flat) {
1604       idx = i;
1605       break;
1606     }
1607   }
1608 
1609   if (idx == AliasIdxTop) {
1610     if (no_create)  return NULL;
1611     // Grow the array if necessary.
1612     if (_num_alias_types == _max_alias_types)  grow_alias_types();
1613     // Add a new alias type.
1614     idx = _num_alias_types++;
1615     _alias_types[idx]-&gt;Init(idx, flat);
1616     if (flat == TypeInstPtr::KLASS)  alias_type(idx)-&gt;set_rewritable(false);
1617     if (flat == TypeAryPtr::RANGE)   alias_type(idx)-&gt;set_rewritable(false);
1618     if (flat-&gt;isa_instptr()) {
1619       if (flat-&gt;offset() == java_lang_Class::klass_offset_in_bytes()
1620           &amp;&amp; flat-&gt;is_instptr()-&gt;klass() == env()-&gt;Class_klass())
1621         alias_type(idx)-&gt;set_rewritable(false);
1622     }
<span class="line-added">1623     ciField* field = NULL;</span>
1624     if (flat-&gt;isa_aryptr()) {
1625 #ifdef ASSERT
1626       const int header_size_min  = arrayOopDesc::base_offset_in_bytes(T_BYTE);
1627       // (T_BYTE has the weakest alignment and size restrictions...)
1628       assert(flat-&gt;offset() &lt; header_size_min, &quot;array body reference must be OffsetBot&quot;);
1629 #endif
<span class="line-added">1630       const Type* elemtype = flat-&gt;is_aryptr()-&gt;elem();</span>
1631       if (flat-&gt;offset() == TypePtr::OffsetBot) {
<span class="line-modified">1632         alias_type(idx)-&gt;set_element(elemtype);</span>
<span class="line-added">1633       }</span>
<span class="line-added">1634       int field_offset = flat-&gt;is_aryptr()-&gt;field_offset().get();</span>
<span class="line-added">1635       if (elemtype-&gt;isa_valuetype() &amp;&amp;</span>
<span class="line-added">1636           elemtype-&gt;value_klass() != NULL &amp;&amp;</span>
<span class="line-added">1637           field_offset != Type::OffsetBot) {</span>
<span class="line-added">1638         ciValueKlass* vk = elemtype-&gt;value_klass();</span>
<span class="line-added">1639         field_offset += vk-&gt;first_field_offset();</span>
<span class="line-added">1640         field = vk-&gt;get_field_by_offset(field_offset, false);</span>
1641       }
1642     }
1643     if (flat-&gt;isa_klassptr()) {
1644       if (flat-&gt;offset() == in_bytes(Klass::super_check_offset_offset()))
1645         alias_type(idx)-&gt;set_rewritable(false);
1646       if (flat-&gt;offset() == in_bytes(Klass::modifier_flags_offset()))
1647         alias_type(idx)-&gt;set_rewritable(false);
1648       if (flat-&gt;offset() == in_bytes(Klass::access_flags_offset()))
1649         alias_type(idx)-&gt;set_rewritable(false);
1650       if (flat-&gt;offset() == in_bytes(Klass::java_mirror_offset()))
1651         alias_type(idx)-&gt;set_rewritable(false);
<span class="line-added">1652       if (flat-&gt;offset() == in_bytes(Klass::layout_helper_offset()))</span>
<span class="line-added">1653         alias_type(idx)-&gt;set_rewritable(false);</span>
1654       if (flat-&gt;offset() == in_bytes(Klass::secondary_super_cache_offset()))
1655         alias_type(idx)-&gt;set_rewritable(false);
1656     }
1657     // %%% (We would like to finalize JavaThread::threadObj_offset(),
1658     // but the base pointer type is not distinctive enough to identify
1659     // references into JavaThread.)
1660 
1661     // Check for final fields.
1662     const TypeInstPtr* tinst = flat-&gt;isa_instptr();
1663     if (tinst &amp;&amp; tinst-&gt;offset() &gt;= instanceOopDesc::base_offset_in_bytes()) {

1664       if (tinst-&gt;const_oop() != NULL &amp;&amp;
1665           tinst-&gt;klass() == ciEnv::current()-&gt;Class_klass() &amp;&amp;
1666           tinst-&gt;offset() &gt;= (tinst-&gt;klass()-&gt;as_instance_klass()-&gt;size_helper() * wordSize)) {
1667         // static field
1668         ciInstanceKlass* k = tinst-&gt;const_oop()-&gt;as_instance()-&gt;java_lang_Class_klass()-&gt;as_instance_klass();
1669         field = k-&gt;get_field_by_offset(tinst-&gt;offset(), true);
<span class="line-added">1670       } else if (tinst-&gt;klass()-&gt;is_valuetype()) {</span>
<span class="line-added">1671         // Value type field</span>
<span class="line-added">1672         ciValueKlass* vk = tinst-&gt;value_klass();</span>
<span class="line-added">1673         field = vk-&gt;get_field_by_offset(tinst-&gt;offset(), false);</span>
1674       } else {
<span class="line-modified">1675         ciInstanceKlass* k = tinst-&gt;klass()-&gt;as_instance_klass();</span>
1676         field = k-&gt;get_field_by_offset(tinst-&gt;offset(), false);
1677       }
<span class="line-modified">1678     }</span>
<span class="line-modified">1679     assert(field == NULL ||</span>
<span class="line-modified">1680            original_field == NULL ||</span>
<span class="line-modified">1681            (field-&gt;holder() == original_field-&gt;holder() &amp;&amp;</span>
<span class="line-modified">1682             field-&gt;offset() == original_field-&gt;offset() &amp;&amp;</span>
<span class="line-modified">1683             field-&gt;is_static() == original_field-&gt;is_static()), &quot;wrong field?&quot;);</span>
<span class="line-modified">1684     // Set field() and is_rewritable() attributes.</span>
<span class="line-added">1685     if (field != NULL) {</span>
<span class="line-added">1686       alias_type(idx)-&gt;set_field(field);</span>
<span class="line-added">1687       if (flat-&gt;isa_aryptr()) {</span>
<span class="line-added">1688         // Fields of flattened inline type arrays are rewritable although they are declared final</span>
<span class="line-added">1689         assert(flat-&gt;is_aryptr()-&gt;elem()-&gt;isa_valuetype(), &quot;must be a flattened value array&quot;);</span>
<span class="line-added">1690         alias_type(idx)-&gt;set_rewritable(true);</span>
<span class="line-added">1691       }</span>
1692     }
1693   }
1694 
1695   // Fill the cache for next time.
<span class="line-modified">1696   if (!uncached) {</span>
<span class="line-modified">1697     ace-&gt;_adr_type = adr_type;</span>
<span class="line-modified">1698     ace-&gt;_index    = idx;</span>
<span class="line-added">1699     assert(alias_type(adr_type) == alias_type(idx),  &quot;type must be installed&quot;);</span>
1700 
<span class="line-modified">1701     // Might as well try to fill the cache for the flattened version, too.</span>
<span class="line-modified">1702     AliasCacheEntry* face = probe_alias_cache(flat);</span>
<span class="line-modified">1703     if (face-&gt;_adr_type == NULL) {</span>
<span class="line-modified">1704       face-&gt;_adr_type = flat;</span>
<span class="line-modified">1705       face-&gt;_index    = idx;</span>
<span class="line-modified">1706       assert(alias_type(flat) == alias_type(idx), &quot;flat type must work too&quot;);</span>
<span class="line-added">1707     }</span>
1708   }
1709 
1710   return alias_type(idx);
1711 }
1712 
1713 
1714 Compile::AliasType* Compile::alias_type(ciField* field) {
1715   const TypeOopPtr* t;
1716   if (field-&gt;is_static())
1717     t = TypeInstPtr::make(field-&gt;holder()-&gt;java_mirror());
1718   else
1719     t = TypeOopPtr::make_from_klass_raw(field-&gt;holder());
1720   AliasType* atp = alias_type(t-&gt;add_offset(field-&gt;offset_in_bytes()), field);
1721   assert((field-&gt;is_final() || field-&gt;is_stable()) == !atp-&gt;is_rewritable(), &quot;must get the rewritable bits correct&quot;);
1722   return atp;
1723 }
1724 
1725 
1726 //------------------------------have_alias_type--------------------------------
1727 bool Compile::have_alias_type(const TypePtr* adr_type) {
</pre>
<hr />
<pre>
1849   }
1850   assert(range_check_cast_count() == 0, &quot;should be empty&quot;);
1851 }
1852 
1853 void Compile::add_opaque4_node(Node* n) {
1854   assert(n-&gt;Opcode() == Op_Opaque4, &quot;Opaque4 only&quot;);
1855   assert(!_opaque4_nodes-&gt;contains(n), &quot;duplicate entry in Opaque4 list&quot;);
1856   _opaque4_nodes-&gt;append(n);
1857 }
1858 
1859 // Remove all Opaque4 nodes.
1860 void Compile::remove_opaque4_nodes(PhaseIterGVN &amp;igvn) {
1861   for (int i = opaque4_count(); i &gt; 0; i--) {
1862     Node* opaq = opaque4_node(i-1);
1863     assert(opaq-&gt;Opcode() == Op_Opaque4, &quot;Opaque4 only&quot;);
1864     igvn.replace_node(opaq, opaq-&gt;in(2));
1865   }
1866   assert(opaque4_count() == 0, &quot;should be empty&quot;);
1867 }
1868 
<span class="line-added">1869 void Compile::add_value_type(Node* n) {</span>
<span class="line-added">1870   assert(n-&gt;is_ValueTypeBase(), &quot;unexpected node&quot;);</span>
<span class="line-added">1871   if (_value_type_nodes != NULL) {</span>
<span class="line-added">1872     _value_type_nodes-&gt;push(n);</span>
<span class="line-added">1873   }</span>
<span class="line-added">1874 }</span>
<span class="line-added">1875 </span>
<span class="line-added">1876 void Compile::remove_value_type(Node* n) {</span>
<span class="line-added">1877   assert(n-&gt;is_ValueTypeBase(), &quot;unexpected node&quot;);</span>
<span class="line-added">1878   if (_value_type_nodes != NULL) {</span>
<span class="line-added">1879     _value_type_nodes-&gt;remove(n);</span>
<span class="line-added">1880   }</span>
<span class="line-added">1881 }</span>
<span class="line-added">1882 </span>
<span class="line-added">1883 // Does the return value keep otherwise useless value type allocations</span>
<span class="line-added">1884 // alive?</span>
<span class="line-added">1885 static bool return_val_keeps_allocations_alive(Node* ret_val) {</span>
<span class="line-added">1886   ResourceMark rm;</span>
<span class="line-added">1887   Unique_Node_List wq;</span>
<span class="line-added">1888   wq.push(ret_val);</span>
<span class="line-added">1889   bool some_allocations = false;</span>
<span class="line-added">1890   for (uint i = 0; i &lt; wq.size(); i++) {</span>
<span class="line-added">1891     Node* n = wq.at(i);</span>
<span class="line-added">1892     assert(!n-&gt;is_ValueTypeBase(), &quot;chain of value type nodes&quot;);</span>
<span class="line-added">1893     if (n-&gt;outcnt() &gt; 1) {</span>
<span class="line-added">1894       // Some other use for the allocation</span>
<span class="line-added">1895       return false;</span>
<span class="line-added">1896     } else if (n-&gt;is_Phi()) {</span>
<span class="line-added">1897       for (uint j = 1; j &lt; n-&gt;req(); j++) {</span>
<span class="line-added">1898         wq.push(n-&gt;in(j));</span>
<span class="line-added">1899       }</span>
<span class="line-added">1900     } else if (n-&gt;is_CheckCastPP() &amp;&amp;</span>
<span class="line-added">1901                n-&gt;in(1)-&gt;is_Proj() &amp;&amp;</span>
<span class="line-added">1902                n-&gt;in(1)-&gt;in(0)-&gt;is_Allocate()) {</span>
<span class="line-added">1903       some_allocations = true;</span>
<span class="line-added">1904     }</span>
<span class="line-added">1905   }</span>
<span class="line-added">1906   return some_allocations;</span>
<span class="line-added">1907 }</span>
<span class="line-added">1908 </span>
<span class="line-added">1909 void Compile::process_value_types(PhaseIterGVN &amp;igvn) {</span>
<span class="line-added">1910   // Make value types scalar in safepoints</span>
<span class="line-added">1911   while (_value_type_nodes-&gt;size() != 0) {</span>
<span class="line-added">1912     ValueTypeBaseNode* vt = _value_type_nodes-&gt;pop()-&gt;as_ValueTypeBase();</span>
<span class="line-added">1913     vt-&gt;make_scalar_in_safepoints(&amp;igvn);</span>
<span class="line-added">1914     if (vt-&gt;is_ValueTypePtr()) {</span>
<span class="line-added">1915       igvn.replace_node(vt, vt-&gt;get_oop());</span>
<span class="line-added">1916     } else if (vt-&gt;outcnt() == 0) {</span>
<span class="line-added">1917       igvn.remove_dead_node(vt);</span>
<span class="line-added">1918     }</span>
<span class="line-added">1919   }</span>
<span class="line-added">1920   _value_type_nodes = NULL;</span>
<span class="line-added">1921   if (tf()-&gt;returns_value_type_as_fields()) {</span>
<span class="line-added">1922     Node* ret = NULL;</span>
<span class="line-added">1923     for (uint i = 1; i &lt; root()-&gt;req(); i++){</span>
<span class="line-added">1924       Node* in = root()-&gt;in(i);</span>
<span class="line-added">1925       if (in-&gt;Opcode() == Op_Return) {</span>
<span class="line-added">1926         assert(ret == NULL, &quot;only one return&quot;);</span>
<span class="line-added">1927         ret = in;</span>
<span class="line-added">1928       }</span>
<span class="line-added">1929     }</span>
<span class="line-added">1930     if (ret != NULL) {</span>
<span class="line-added">1931       Node* ret_val = ret-&gt;in(TypeFunc::Parms);</span>
<span class="line-added">1932       if (igvn.type(ret_val)-&gt;isa_oopptr() &amp;&amp;</span>
<span class="line-added">1933           return_val_keeps_allocations_alive(ret_val)) {</span>
<span class="line-added">1934         igvn.replace_input_of(ret, TypeFunc::Parms, ValueTypeNode::tagged_klass(igvn.type(ret_val)-&gt;value_klass(), igvn));</span>
<span class="line-added">1935         assert(ret_val-&gt;outcnt() == 0, &quot;should be dead now&quot;);</span>
<span class="line-added">1936         igvn.remove_dead_node(ret_val);</span>
<span class="line-added">1937       }</span>
<span class="line-added">1938     }</span>
<span class="line-added">1939   }</span>
<span class="line-added">1940   igvn.optimize();</span>
<span class="line-added">1941 }</span>
<span class="line-added">1942 </span>
<span class="line-added">1943 void Compile::adjust_flattened_array_access_aliases(PhaseIterGVN&amp; igvn) {</span>
<span class="line-added">1944   if (!_has_flattened_accesses) {</span>
<span class="line-added">1945     return;</span>
<span class="line-added">1946   }</span>
<span class="line-added">1947   // Initially, all flattened array accesses share the same slice to</span>
<span class="line-added">1948   // keep dependencies with Object[] array accesses (that could be</span>
<span class="line-added">1949   // to a flattened array) correct. We&#39;re done with parsing so we</span>
<span class="line-added">1950   // now know all flattened array accesses in this compile</span>
<span class="line-added">1951   // unit. Let&#39;s move flattened array accesses to their own slice,</span>
<span class="line-added">1952   // one per element field. This should help memory access</span>
<span class="line-added">1953   // optimizations.</span>
<span class="line-added">1954   ResourceMark rm;</span>
<span class="line-added">1955   Unique_Node_List wq;</span>
<span class="line-added">1956   wq.push(root());</span>
<span class="line-added">1957 </span>
<span class="line-added">1958   Node_List mergememnodes;</span>
<span class="line-added">1959   Node_List memnodes;</span>
<span class="line-added">1960 </span>
<span class="line-added">1961   // Alias index currently shared by all flattened memory accesses</span>
<span class="line-added">1962   int index = get_alias_index(TypeAryPtr::VALUES);</span>
<span class="line-added">1963 </span>
<span class="line-added">1964   // Find MergeMem nodes and flattened array accesses</span>
<span class="line-added">1965   for (uint i = 0; i &lt; wq.size(); i++) {</span>
<span class="line-added">1966     Node* n = wq.at(i);</span>
<span class="line-added">1967     if (n-&gt;is_Mem()) {</span>
<span class="line-added">1968       const TypePtr* adr_type = NULL;</span>
<span class="line-added">1969       if (n-&gt;Opcode() == Op_StoreCM) {</span>
<span class="line-added">1970         adr_type = get_adr_type(get_alias_index(n-&gt;in(MemNode::OopStore)-&gt;adr_type()));</span>
<span class="line-added">1971       } else {</span>
<span class="line-added">1972         adr_type = get_adr_type(get_alias_index(n-&gt;adr_type()));</span>
<span class="line-added">1973       }</span>
<span class="line-added">1974       if (adr_type == TypeAryPtr::VALUES) {</span>
<span class="line-added">1975         memnodes.push(n);</span>
<span class="line-added">1976       }</span>
<span class="line-added">1977     } else if (n-&gt;is_MergeMem()) {</span>
<span class="line-added">1978       MergeMemNode* mm = n-&gt;as_MergeMem();</span>
<span class="line-added">1979       if (mm-&gt;memory_at(index) != mm-&gt;base_memory()) {</span>
<span class="line-added">1980         mergememnodes.push(n);</span>
<span class="line-added">1981       }</span>
<span class="line-added">1982     }</span>
<span class="line-added">1983     for (uint j = 0; j &lt; n-&gt;req(); j++) {</span>
<span class="line-added">1984       Node* m = n-&gt;in(j);</span>
<span class="line-added">1985       if (m != NULL) {</span>
<span class="line-added">1986         wq.push(m);</span>
<span class="line-added">1987       }</span>
<span class="line-added">1988     }</span>
<span class="line-added">1989   }</span>
<span class="line-added">1990 </span>
<span class="line-added">1991   if (memnodes.size() &gt; 0) {</span>
<span class="line-added">1992     _flattened_accesses_share_alias = false;</span>
<span class="line-added">1993 </span>
<span class="line-added">1994     // We are going to change the slice for the flattened array</span>
<span class="line-added">1995     // accesses so we need to clear the cache entries that refer to</span>
<span class="line-added">1996     // them.</span>
<span class="line-added">1997     for (uint i = 0; i &lt; AliasCacheSize; i++) {</span>
<span class="line-added">1998       AliasCacheEntry* ace = &amp;_alias_cache[i];</span>
<span class="line-added">1999       if (ace-&gt;_adr_type != NULL &amp;&amp;</span>
<span class="line-added">2000           ace-&gt;_adr_type-&gt;isa_aryptr() &amp;&amp;</span>
<span class="line-added">2001           ace-&gt;_adr_type-&gt;is_aryptr()-&gt;elem()-&gt;isa_valuetype()) {</span>
<span class="line-added">2002         ace-&gt;_adr_type = NULL;</span>
<span class="line-added">2003         ace-&gt;_index = (i != 0) ? 0 : AliasIdxTop; // Make sure the NULL adr_type resolves to AliasIdxTop</span>
<span class="line-added">2004       }</span>
<span class="line-added">2005     }</span>
<span class="line-added">2006 </span>
<span class="line-added">2007     // Find what aliases we are going to add</span>
<span class="line-added">2008     int start_alias = num_alias_types()-1;</span>
<span class="line-added">2009     int stop_alias = 0;</span>
<span class="line-added">2010 </span>
<span class="line-added">2011     for (uint i = 0; i &lt; memnodes.size(); i++) {</span>
<span class="line-added">2012       Node* m = memnodes.at(i);</span>
<span class="line-added">2013       const TypePtr* adr_type = NULL;</span>
<span class="line-added">2014       if (m-&gt;Opcode() == Op_StoreCM) {</span>
<span class="line-added">2015         adr_type = m-&gt;in(MemNode::OopStore)-&gt;adr_type();</span>
<span class="line-added">2016         Node* clone = new StoreCMNode(m-&gt;in(MemNode::Control), m-&gt;in(MemNode::Memory), m-&gt;in(MemNode::Address),</span>
<span class="line-added">2017                                       m-&gt;adr_type(), m-&gt;in(MemNode::ValueIn), m-&gt;in(MemNode::OopStore),</span>
<span class="line-added">2018                                       get_alias_index(adr_type));</span>
<span class="line-added">2019         igvn.register_new_node_with_optimizer(clone);</span>
<span class="line-added">2020         igvn.replace_node(m, clone);</span>
<span class="line-added">2021       } else {</span>
<span class="line-added">2022         adr_type = m-&gt;adr_type();</span>
<span class="line-added">2023 #ifdef ASSERT</span>
<span class="line-added">2024         m-&gt;as_Mem()-&gt;set_adr_type(adr_type);</span>
<span class="line-added">2025 #endif</span>
<span class="line-added">2026       }</span>
<span class="line-added">2027       int idx = get_alias_index(adr_type);</span>
<span class="line-added">2028       start_alias = MIN2(start_alias, idx);</span>
<span class="line-added">2029       stop_alias = MAX2(stop_alias, idx);</span>
<span class="line-added">2030     }</span>
<span class="line-added">2031 </span>
<span class="line-added">2032     assert(stop_alias &gt;= start_alias, &quot;should have expanded aliases&quot;);</span>
<span class="line-added">2033 </span>
<span class="line-added">2034     Node_Stack stack(0);</span>
<span class="line-added">2035 #ifdef ASSERT</span>
<span class="line-added">2036     VectorSet seen(Thread::current()-&gt;resource_area());</span>
<span class="line-added">2037 #endif</span>
<span class="line-added">2038     // Now let&#39;s fix the memory graph so each flattened array access</span>
<span class="line-added">2039     // is moved to the right slice. Start from the MergeMem nodes.</span>
<span class="line-added">2040     uint last = unique();</span>
<span class="line-added">2041     for (uint i = 0; i &lt; mergememnodes.size(); i++) {</span>
<span class="line-added">2042       MergeMemNode* current = mergememnodes.at(i)-&gt;as_MergeMem();</span>
<span class="line-added">2043       Node* n = current-&gt;memory_at(index);</span>
<span class="line-added">2044       MergeMemNode* mm = NULL;</span>
<span class="line-added">2045       do {</span>
<span class="line-added">2046         // Follow memory edges through memory accesses, phis and</span>
<span class="line-added">2047         // narrow membars and push nodes on the stack. Once we hit</span>
<span class="line-added">2048         // bottom memory, we pop element off the stack one at a</span>
<span class="line-added">2049         // time, in reverse order, and move them to the right slice</span>
<span class="line-added">2050         // by changing their memory edges.</span>
<span class="line-added">2051         if ((n-&gt;is_Phi() &amp;&amp; n-&gt;adr_type() != TypePtr::BOTTOM) || n-&gt;is_Mem() || n-&gt;adr_type() == TypeAryPtr::VALUES) {</span>
<span class="line-added">2052           assert(!seen.test_set(n-&gt;_idx), &quot;&quot;);</span>
<span class="line-added">2053           // Uses (a load for instance) will need to be moved to the</span>
<span class="line-added">2054           // right slice as well and will get a new memory state</span>
<span class="line-added">2055           // that we don&#39;t know yet. The use could also be the</span>
<span class="line-added">2056           // backedge of a loop. We put a place holder node between</span>
<span class="line-added">2057           // the memory node and its uses. We replace that place</span>
<span class="line-added">2058           // holder with the correct memory state once we know it,</span>
<span class="line-added">2059           // i.e. when nodes are popped off the stack. Using the</span>
<span class="line-added">2060           // place holder make the logic work in the presence of</span>
<span class="line-added">2061           // loops.</span>
<span class="line-added">2062           if (n-&gt;outcnt() &gt; 1) {</span>
<span class="line-added">2063             Node* place_holder = NULL;</span>
<span class="line-added">2064             assert(!n-&gt;has_out_with(Op_Node), &quot;&quot;);</span>
<span class="line-added">2065             for (DUIterator k = n-&gt;outs(); n-&gt;has_out(k); k++) {</span>
<span class="line-added">2066               Node* u = n-&gt;out(k);</span>
<span class="line-added">2067               if (u != current &amp;&amp; u-&gt;_idx &lt; last) {</span>
<span class="line-added">2068                 bool success = false;</span>
<span class="line-added">2069                 for (uint l = 0; l &lt; u-&gt;req(); l++) {</span>
<span class="line-added">2070                   if (!stack.is_empty() &amp;&amp; u == stack.node() &amp;&amp; l == stack.index()) {</span>
<span class="line-added">2071                     continue;</span>
<span class="line-added">2072                   }</span>
<span class="line-added">2073                   Node* in = u-&gt;in(l);</span>
<span class="line-added">2074                   if (in == n) {</span>
<span class="line-added">2075                     if (place_holder == NULL) {</span>
<span class="line-added">2076                       place_holder = new Node(1);</span>
<span class="line-added">2077                       place_holder-&gt;init_req(0, n);</span>
<span class="line-added">2078                     }</span>
<span class="line-added">2079                     igvn.replace_input_of(u, l, place_holder);</span>
<span class="line-added">2080                     success = true;</span>
<span class="line-added">2081                   }</span>
<span class="line-added">2082                 }</span>
<span class="line-added">2083                 if (success) {</span>
<span class="line-added">2084                   --k;</span>
<span class="line-added">2085                 }</span>
<span class="line-added">2086               }</span>
<span class="line-added">2087             }</span>
<span class="line-added">2088           }</span>
<span class="line-added">2089           if (n-&gt;is_Phi()) {</span>
<span class="line-added">2090             stack.push(n, 1);</span>
<span class="line-added">2091             n = n-&gt;in(1);</span>
<span class="line-added">2092           } else if (n-&gt;is_Mem()) {</span>
<span class="line-added">2093             stack.push(n, n-&gt;req());</span>
<span class="line-added">2094             n = n-&gt;in(MemNode::Memory);</span>
<span class="line-added">2095           } else {</span>
<span class="line-added">2096             assert(n-&gt;is_Proj() &amp;&amp; n-&gt;in(0)-&gt;Opcode() == Op_MemBarCPUOrder, &quot;&quot;);</span>
<span class="line-added">2097             stack.push(n, n-&gt;req());</span>
<span class="line-added">2098             n = n-&gt;in(0)-&gt;in(TypeFunc::Memory);</span>
<span class="line-added">2099           }</span>
<span class="line-added">2100         } else {</span>
<span class="line-added">2101           assert(n-&gt;adr_type() == TypePtr::BOTTOM || (n-&gt;Opcode() == Op_Node &amp;&amp; n-&gt;_idx &gt;= last) || (n-&gt;is_Proj() &amp;&amp; n-&gt;in(0)-&gt;is_Initialize()), &quot;&quot;);</span>
<span class="line-added">2102           // Build a new MergeMem node to carry the new memory state</span>
<span class="line-added">2103           // as we build it. IGVN should fold extraneous MergeMem</span>
<span class="line-added">2104           // nodes.</span>
<span class="line-added">2105           mm = MergeMemNode::make(n);</span>
<span class="line-added">2106           igvn.register_new_node_with_optimizer(mm);</span>
<span class="line-added">2107           while (stack.size() &gt; 0) {</span>
<span class="line-added">2108             Node* m = stack.node();</span>
<span class="line-added">2109             uint idx = stack.index();</span>
<span class="line-added">2110             if (m-&gt;is_Mem()) {</span>
<span class="line-added">2111               // Move memory node to its new slice</span>
<span class="line-added">2112               const TypePtr* adr_type = m-&gt;adr_type();</span>
<span class="line-added">2113               int alias = get_alias_index(adr_type);</span>
<span class="line-added">2114               Node* prev = mm-&gt;memory_at(alias);</span>
<span class="line-added">2115               igvn.replace_input_of(m, MemNode::Memory, prev);</span>
<span class="line-added">2116               mm-&gt;set_memory_at(alias, m);</span>
<span class="line-added">2117             } else if (m-&gt;is_Phi()) {</span>
<span class="line-added">2118               // We need as many new phis as there are new aliases</span>
<span class="line-added">2119               igvn.replace_input_of(m, idx, mm);</span>
<span class="line-added">2120               if (idx == m-&gt;req()-1) {</span>
<span class="line-added">2121                 Node* r = m-&gt;in(0);</span>
<span class="line-added">2122                 for (uint j = (uint)start_alias; j &lt;= (uint)stop_alias; j++) {</span>
<span class="line-added">2123                   const Type* adr_type = get_adr_type(j);</span>
<span class="line-added">2124                   if (!adr_type-&gt;isa_aryptr() || !adr_type-&gt;is_aryptr()-&gt;elem()-&gt;isa_valuetype()) {</span>
<span class="line-added">2125                     continue;</span>
<span class="line-added">2126                   }</span>
<span class="line-added">2127                   Node* phi = new PhiNode(r, Type::MEMORY, get_adr_type(j));</span>
<span class="line-added">2128                   igvn.register_new_node_with_optimizer(phi);</span>
<span class="line-added">2129                   for (uint k = 1; k &lt; m-&gt;req(); k++) {</span>
<span class="line-added">2130                     phi-&gt;init_req(k, m-&gt;in(k)-&gt;as_MergeMem()-&gt;memory_at(j));</span>
<span class="line-added">2131                   }</span>
<span class="line-added">2132                   mm-&gt;set_memory_at(j, phi);</span>
<span class="line-added">2133                 }</span>
<span class="line-added">2134                 Node* base_phi = new PhiNode(r, Type::MEMORY, TypePtr::BOTTOM);</span>
<span class="line-added">2135                 igvn.register_new_node_with_optimizer(base_phi);</span>
<span class="line-added">2136                 for (uint k = 1; k &lt; m-&gt;req(); k++) {</span>
<span class="line-added">2137                   base_phi-&gt;init_req(k, m-&gt;in(k)-&gt;as_MergeMem()-&gt;base_memory());</span>
<span class="line-added">2138                 }</span>
<span class="line-added">2139                 mm-&gt;set_base_memory(base_phi);</span>
<span class="line-added">2140               }</span>
<span class="line-added">2141             } else {</span>
<span class="line-added">2142               // This is a MemBarCPUOrder node from</span>
<span class="line-added">2143               // Parse::array_load()/Parse::array_store(), in the</span>
<span class="line-added">2144               // branch that handles flattened arrays hidden under</span>
<span class="line-added">2145               // an Object[] array. We also need one new membar per</span>
<span class="line-added">2146               // new alias to keep the unknown access that the</span>
<span class="line-added">2147               // membars protect properly ordered with accesses to</span>
<span class="line-added">2148               // known flattened array.</span>
<span class="line-added">2149               assert(m-&gt;is_Proj(), &quot;projection expected&quot;);</span>
<span class="line-added">2150               Node* ctrl = m-&gt;in(0)-&gt;in(TypeFunc::Control);</span>
<span class="line-added">2151               igvn.replace_input_of(m-&gt;in(0), TypeFunc::Control, top());</span>
<span class="line-added">2152               for (uint j = (uint)start_alias; j &lt;= (uint)stop_alias; j++) {</span>
<span class="line-added">2153                 const Type* adr_type = get_adr_type(j);</span>
<span class="line-added">2154                 if (!adr_type-&gt;isa_aryptr() || !adr_type-&gt;is_aryptr()-&gt;elem()-&gt;isa_valuetype()) {</span>
<span class="line-added">2155                   continue;</span>
<span class="line-added">2156                 }</span>
<span class="line-added">2157                 MemBarNode* mb = new MemBarCPUOrderNode(this, j, NULL);</span>
<span class="line-added">2158                 igvn.register_new_node_with_optimizer(mb);</span>
<span class="line-added">2159                 Node* mem = mm-&gt;memory_at(j);</span>
<span class="line-added">2160                 mb-&gt;init_req(TypeFunc::Control, ctrl);</span>
<span class="line-added">2161                 mb-&gt;init_req(TypeFunc::Memory, mem);</span>
<span class="line-added">2162                 ctrl = new ProjNode(mb, TypeFunc::Control);</span>
<span class="line-added">2163                 igvn.register_new_node_with_optimizer(ctrl);</span>
<span class="line-added">2164                 mem = new ProjNode(mb, TypeFunc::Memory);</span>
<span class="line-added">2165                 igvn.register_new_node_with_optimizer(mem);</span>
<span class="line-added">2166                 mm-&gt;set_memory_at(j, mem);</span>
<span class="line-added">2167               }</span>
<span class="line-added">2168               igvn.replace_node(m-&gt;in(0)-&gt;as_Multi()-&gt;proj_out(TypeFunc::Control), ctrl);</span>
<span class="line-added">2169             }</span>
<span class="line-added">2170             if (idx &lt; m-&gt;req()-1) {</span>
<span class="line-added">2171               idx += 1;</span>
<span class="line-added">2172               stack.set_index(idx);</span>
<span class="line-added">2173               n = m-&gt;in(idx);</span>
<span class="line-added">2174               break;</span>
<span class="line-added">2175             }</span>
<span class="line-added">2176             // Take care of place holder nodes</span>
<span class="line-added">2177             if (m-&gt;has_out_with(Op_Node)) {</span>
<span class="line-added">2178               Node* place_holder = m-&gt;find_out_with(Op_Node);</span>
<span class="line-added">2179               if (place_holder != NULL) {</span>
<span class="line-added">2180                 Node* mm_clone = mm-&gt;clone();</span>
<span class="line-added">2181                 igvn.register_new_node_with_optimizer(mm_clone);</span>
<span class="line-added">2182                 Node* hook = new Node(1);</span>
<span class="line-added">2183                 hook-&gt;init_req(0, mm);</span>
<span class="line-added">2184                 igvn.replace_node(place_holder, mm_clone);</span>
<span class="line-added">2185                 hook-&gt;destruct();</span>
<span class="line-added">2186               }</span>
<span class="line-added">2187               assert(!m-&gt;has_out_with(Op_Node), &quot;place holder should be gone now&quot;);</span>
<span class="line-added">2188             }</span>
<span class="line-added">2189             stack.pop();</span>
<span class="line-added">2190           }</span>
<span class="line-added">2191         }</span>
<span class="line-added">2192       } while(stack.size() &gt; 0);</span>
<span class="line-added">2193       // Fix the memory state at the MergeMem we started from</span>
<span class="line-added">2194       igvn.rehash_node_delayed(current);</span>
<span class="line-added">2195       for (uint j = (uint)start_alias; j &lt;= (uint)stop_alias; j++) {</span>
<span class="line-added">2196         const Type* adr_type = get_adr_type(j);</span>
<span class="line-added">2197         if (!adr_type-&gt;isa_aryptr() || !adr_type-&gt;is_aryptr()-&gt;elem()-&gt;isa_valuetype()) {</span>
<span class="line-added">2198           continue;</span>
<span class="line-added">2199         }</span>
<span class="line-added">2200         current-&gt;set_memory_at(j, mm);</span>
<span class="line-added">2201       }</span>
<span class="line-added">2202       current-&gt;set_memory_at(index, current-&gt;base_memory());</span>
<span class="line-added">2203     }</span>
<span class="line-added">2204     igvn.optimize();</span>
<span class="line-added">2205   }</span>
<span class="line-added">2206   print_method(PHASE_SPLIT_VALUES_ARRAY, 2);</span>
<span class="line-added">2207 }</span>
<span class="line-added">2208 </span>
<span class="line-added">2209 </span>
2210 // StringOpts and late inlining of string methods
2211 void Compile::inline_string_calls(bool parse_time) {
2212   {
2213     // remove useless nodes to make the usage analysis simpler
2214     ResourceMark rm;
2215     PhaseRemoveUseless pru(initial_gvn(), for_igvn());
2216   }
2217 
2218   {
2219     ResourceMark rm;
2220     print_method(PHASE_BEFORE_STRINGOPTS, 3);
2221     PhaseStringOpts pso(initial_gvn(), for_igvn());
2222     print_method(PHASE_AFTER_STRINGOPTS, 3);
2223   }
2224 
2225   // now inline anything that we skipped the first time around
2226   if (!parse_time) {
2227     _late_inlines_pos = _late_inlines.length();
2228   }
2229 
</pre>
<hr />
<pre>
2469   remove_speculative_types(igvn);
2470 
2471   // No more new expensive nodes will be added to the list from here
2472   // so keep only the actual candidates for optimizations.
2473   cleanup_expensive_nodes(igvn);
2474 
2475   if (!failing() &amp;&amp; RenumberLiveNodes &amp;&amp; live_nodes() + NodeLimitFudgeFactor &lt; unique()) {
2476     Compile::TracePhase tp(&quot;&quot;, &amp;timers[_t_renumberLive]);
2477     initial_gvn()-&gt;replace_with(&amp;igvn);
2478     for_igvn()-&gt;clear();
2479     Unique_Node_List new_worklist(C-&gt;comp_arena());
2480     {
2481       ResourceMark rm;
2482       PhaseRenumberLive prl = PhaseRenumberLive(initial_gvn(), for_igvn(), &amp;new_worklist);
2483     }
2484     set_for_igvn(&amp;new_worklist);
2485     igvn = PhaseIterGVN(initial_gvn());
2486     igvn.optimize();
2487   }
2488 
<span class="line-added">2489   if (_value_type_nodes-&gt;size() &gt; 0) {</span>
<span class="line-added">2490     // Do this once all inlining is over to avoid getting inconsistent debug info</span>
<span class="line-added">2491     process_value_types(igvn);</span>
<span class="line-added">2492   }</span>
<span class="line-added">2493 </span>
<span class="line-added">2494   adjust_flattened_array_access_aliases(igvn);</span>
<span class="line-added">2495 </span>
2496   // Perform escape analysis
2497   if (_do_escape_analysis &amp;&amp; ConnectionGraph::has_candidates(this)) {
2498     if (has_loops()) {
2499       // Cleanup graph (remove dead nodes).
2500       TracePhase tp(&quot;idealLoop&quot;, &amp;timers[_t_idealLoop]);
2501       PhaseIdealLoop::optimize(igvn, LoopOptsMaxUnroll);
2502       if (major_progress()) print_method(PHASE_PHASEIDEAL_BEFORE_EA, 2);
2503       if (failing())  return;
2504     }
2505     ConnectionGraph::do_analysis(this, &amp;igvn);
2506 
2507     if (failing())  return;
2508 
2509     // Optimize out fields loads from scalar replaceable allocations.
2510     igvn.optimize();
2511     print_method(PHASE_ITER_GVN_AFTER_EA, 2);
2512 
2513     if (failing())  return;
2514 
2515     if (congraph() != NULL &amp;&amp; macro_count() &gt; 0) {
</pre>
<hr />
<pre>
3147             // Accumulate any precedence edges
3148             if (mem-&gt;in(i) != NULL) {
3149               n-&gt;add_prec(mem-&gt;in(i));
3150             }
3151           }
3152           // Everything above this point has been processed.
3153           done = true;
3154         }
3155         // Eliminate the previous StoreCM
3156         prev-&gt;set_req(MemNode::Memory, mem-&gt;in(MemNode::Memory));
3157         assert(mem-&gt;outcnt() == 0, &quot;should be dead&quot;);
3158         mem-&gt;disconnect_inputs(NULL, this);
3159       } else {
3160         prev = mem;
3161       }
3162       mem = prev-&gt;in(MemNode::Memory);
3163     }
3164   }
3165 }
3166 
<span class="line-added">3167 </span>
3168 //------------------------------final_graph_reshaping_impl----------------------
3169 // Implement items 1-5 from final_graph_reshaping below.
3170 void Compile::final_graph_reshaping_impl( Node *n, Final_Reshape_Counts &amp;frc) {
3171 
3172   if ( n-&gt;outcnt() == 0 ) return; // dead node
3173   uint nop = n-&gt;Opcode();
3174 
3175   // Check for 2-input instruction with &quot;last use&quot; on right input.
3176   // Swap to left input.  Implements item (2).
3177   if( n-&gt;req() == 3 &amp;&amp;          // two-input instruction
3178       n-&gt;in(1)-&gt;outcnt() &gt; 1 &amp;&amp; // left use is NOT a last use
3179       (!n-&gt;in(1)-&gt;is_Phi() || n-&gt;in(1)-&gt;in(2) != n) &amp;&amp; // it is not data loop
3180       n-&gt;in(2)-&gt;outcnt() == 1 &amp;&amp;// right use IS a last use
3181       !n-&gt;in(2)-&gt;is_Con() ) {   // right use is not a constant
3182     // Check for commutative opcode
3183     switch( nop ) {
3184     case Op_AddI:  case Op_AddF:  case Op_AddD:  case Op_AddL:
3185     case Op_MaxI:  case Op_MinI:
3186     case Op_MulI:  case Op_MulF:  case Op_MulD:  case Op_MulL:
3187     case Op_AndL:  case Op_XorL:  case Op_OrL:
</pre>
<hr />
<pre>
3370   case Op_LoadUS:
3371   case Op_LoadI:
3372   case Op_LoadKlass:
3373   case Op_LoadNKlass:
3374   case Op_LoadL:
3375   case Op_LoadL_unaligned:
3376   case Op_LoadPLocked:
3377   case Op_LoadP:
3378   case Op_LoadN:
3379   case Op_LoadRange:
3380   case Op_LoadS: {
3381   handle_mem:
3382 #ifdef ASSERT
3383     if( VerifyOptoOopOffsets ) {
3384       MemNode* mem  = n-&gt;as_Mem();
3385       // Check to see if address types have grounded out somehow.
3386       const TypeInstPtr *tp = mem-&gt;in(MemNode::Address)-&gt;bottom_type()-&gt;isa_instptr();
3387       assert( !tp || oop_offset_is_sane(tp), &quot;&quot; );
3388     }
3389 #endif
<span class="line-added">3390     if (EnableValhalla &amp;&amp;</span>
<span class="line-added">3391         ((nop == Op_LoadKlass &amp;&amp; ((LoadKlassNode*)n)-&gt;clear_prop_bits()) ||</span>
<span class="line-added">3392          (nop == Op_LoadNKlass &amp;&amp; ((LoadNKlassNode*)n)-&gt;clear_prop_bits()))) {</span>
<span class="line-added">3393       const TypeKlassPtr* tk = n-&gt;bottom_type()-&gt;make_ptr()-&gt;is_klassptr();</span>
<span class="line-added">3394       assert(!tk-&gt;klass_is_exact(), &quot;should have been folded&quot;);</span>
<span class="line-added">3395       assert(n-&gt;as_Mem()-&gt;adr_type()-&gt;offset() == oopDesc::klass_offset_in_bytes(), &quot;unexpected LoadKlass&quot;);</span>
<span class="line-added">3396       if (tk-&gt;klass()-&gt;can_be_value_array_klass()) {</span>
<span class="line-added">3397         // Array load klass needs to filter out property bits (but not</span>
<span class="line-added">3398         // GetNullFreePropertyNode or GetFlattenedPropertyNode which</span>
<span class="line-added">3399         // needs to extract the storage property bits)</span>
<span class="line-added">3400         uint last = unique();</span>
<span class="line-added">3401         Node* pointer = NULL;</span>
<span class="line-added">3402         if (nop == Op_LoadKlass) {</span>
<span class="line-added">3403           Node* cast = new CastP2XNode(NULL, n);</span>
<span class="line-added">3404           Node* masked = new LShiftXNode(cast, new ConINode(TypeInt::make(oopDesc::storage_props_nof_bits)));</span>
<span class="line-added">3405           masked = new RShiftXNode(masked, new ConINode(TypeInt::make(oopDesc::storage_props_nof_bits)));</span>
<span class="line-added">3406           pointer = new CastX2PNode(masked);</span>
<span class="line-added">3407           pointer = new CheckCastPPNode(NULL, pointer, n-&gt;bottom_type());</span>
<span class="line-added">3408         } else {</span>
<span class="line-added">3409           Node* cast = new CastN2INode(n);</span>
<span class="line-added">3410           Node* masked = new AndINode(cast, new ConINode(TypeInt::make(oopDesc::compressed_klass_mask())));</span>
<span class="line-added">3411           pointer = new CastI2NNode(masked, n-&gt;bottom_type());</span>
<span class="line-added">3412         }</span>
<span class="line-added">3413         for (DUIterator_Fast imax, i = n-&gt;fast_outs(imax); i &lt; imax; i++) {</span>
<span class="line-added">3414           Node* u = n-&gt;fast_out(i);</span>
<span class="line-added">3415           if (u-&gt;_idx &lt; last &amp;&amp; u-&gt;Opcode() != Op_GetNullFreeProperty &amp;&amp; u-&gt;Opcode() != Op_GetFlattenedProperty) {</span>
<span class="line-added">3416             // If user is a comparison with a klass that can&#39;t be a value type</span>
<span class="line-added">3417             // array klass, we don&#39;t need to clear the storage property bits.</span>
<span class="line-added">3418             Node* cmp = (u-&gt;is_DecodeNKlass() &amp;&amp; u-&gt;outcnt() == 1) ? u-&gt;unique_out() : u;</span>
<span class="line-added">3419             if (cmp-&gt;is_Cmp()) {</span>
<span class="line-added">3420               const TypeKlassPtr* kp1 = cmp-&gt;in(1)-&gt;bottom_type()-&gt;make_ptr()-&gt;isa_klassptr();</span>
<span class="line-added">3421               const TypeKlassPtr* kp2 = cmp-&gt;in(2)-&gt;bottom_type()-&gt;make_ptr()-&gt;isa_klassptr();</span>
<span class="line-added">3422               if ((kp1 != NULL &amp;&amp; !kp1-&gt;klass()-&gt;can_be_value_array_klass()) ||</span>
<span class="line-added">3423                   (kp2 != NULL &amp;&amp; !kp2-&gt;klass()-&gt;can_be_value_array_klass())) {</span>
<span class="line-added">3424                 continue;</span>
<span class="line-added">3425               }</span>
<span class="line-added">3426             }</span>
<span class="line-added">3427             int nb = u-&gt;replace_edge(n, pointer);</span>
<span class="line-added">3428             --i, imax -= nb;</span>
<span class="line-added">3429           }</span>
<span class="line-added">3430         }</span>
<span class="line-added">3431       }</span>
<span class="line-added">3432     }</span>
3433     break;
3434   }
3435 
3436   case Op_AddP: {               // Assert sane base pointers
3437     Node *addp = n-&gt;in(AddPNode::Address);
3438     assert( !addp-&gt;is_AddP() ||
3439             addp-&gt;in(AddPNode::Base)-&gt;is_top() || // Top OK for allocation
3440             addp-&gt;in(AddPNode::Base) == n-&gt;in(AddPNode::Base),
3441             &quot;Base pointers must match (addp %u)&quot;, addp-&gt;_idx );
3442 #ifdef _LP64
3443     if ((UseCompressedOops || UseCompressedClassPointers) &amp;&amp;
3444         addp-&gt;Opcode() == Op_ConP &amp;&amp;
3445         addp == n-&gt;in(AddPNode::Base) &amp;&amp;
3446         n-&gt;in(AddPNode::Offset)-&gt;is_Con()) {
3447       // If the transformation of ConP to ConN+DecodeN is beneficial depends
3448       // on the platform and on the compressed oops mode.
3449       // Use addressing with narrow klass to load with offset on x86.
3450       // Some platforms can use the constant pool to load ConP.
3451       // Do this transformation here since IGVN will convert ConN back to ConP.
3452       const Type* t = addp-&gt;bottom_type();
</pre>
<hr />
<pre>
3929           // Replace all nodes with identical edges as m with m
3930           k-&gt;subsume_by(m, this);
3931         }
3932       }
3933     }
3934     break;
3935   }
3936   case Op_CmpUL: {
3937     if (!Matcher::has_match_rule(Op_CmpUL)) {
3938       // No support for unsigned long comparisons
3939       ConINode* sign_pos = new ConINode(TypeInt::make(BitsPerLong - 1));
3940       Node* sign_bit_mask = new RShiftLNode(n-&gt;in(1), sign_pos);
3941       Node* orl = new OrLNode(n-&gt;in(1), sign_bit_mask);
3942       ConLNode* remove_sign_mask = new ConLNode(TypeLong::make(max_jlong));
3943       Node* andl = new AndLNode(orl, remove_sign_mask);
3944       Node* cmp = new CmpLNode(andl, n-&gt;in(2));
3945       n-&gt;subsume_by(cmp, this);
3946     }
3947     break;
3948   }
<span class="line-added">3949 #ifdef ASSERT</span>
<span class="line-added">3950   case Op_ValueTypePtr:</span>
<span class="line-added">3951   case Op_ValueType: {</span>
<span class="line-added">3952     n-&gt;dump(-1);</span>
<span class="line-added">3953     assert(false, &quot;value type node was not removed&quot;);</span>
<span class="line-added">3954     break;</span>
<span class="line-added">3955   }</span>
<span class="line-added">3956 #endif</span>
<span class="line-added">3957   case Op_GetNullFreeProperty:</span>
<span class="line-added">3958   case Op_GetFlattenedProperty: {</span>
<span class="line-added">3959     // Extract the null free bits</span>
<span class="line-added">3960     uint last = unique();</span>
<span class="line-added">3961     Node* null_free = NULL;</span>
<span class="line-added">3962     int bit = nop == Op_GetNullFreeProperty ? ArrayStorageProperties::null_free_bit : ArrayStorageProperties::flattened_bit;</span>
<span class="line-added">3963     if (n-&gt;in(1)-&gt;Opcode() == Op_LoadKlass) {</span>
<span class="line-added">3964       Node* cast = new CastP2XNode(NULL, n-&gt;in(1));</span>
<span class="line-added">3965       null_free = new AndLNode(cast, new ConLNode(TypeLong::make(((jlong)1)&lt;&lt;(oopDesc::wide_storage_props_shift + bit))));</span>
<span class="line-added">3966     } else {</span>
<span class="line-added">3967       assert(n-&gt;in(1)-&gt;Opcode() == Op_LoadNKlass, &quot;not a compressed klass?&quot;);</span>
<span class="line-added">3968       Node* cast = new CastN2INode(n-&gt;in(1));</span>
<span class="line-added">3969       null_free = new AndINode(cast, new ConINode(TypeInt::make(1&lt;&lt;(oopDesc::narrow_storage_props_shift + bit))));</span>
<span class="line-added">3970     }</span>
<span class="line-added">3971     n-&gt;subsume_by(null_free, this);</span>
<span class="line-added">3972     break;</span>
<span class="line-added">3973   }</span>
3974   default:
3975     assert(!n-&gt;is_Call(), &quot;&quot;);
3976     assert(!n-&gt;is_Mem(), &quot;&quot;);
3977     assert(nop != Op_ProfileBoolean, &quot;should be eliminated during IGVN&quot;);
3978     break;
3979   }
3980 }
3981 
3982 //------------------------------final_graph_reshaping_walk---------------------
3983 // Replacing Opaque nodes with their input in final_graph_reshaping_impl(),
3984 // requires that the walk visits a node&#39;s inputs before visiting the node.
3985 void Compile::final_graph_reshaping_walk( Node_Stack &amp;nstack, Node *root, Final_Reshape_Counts &amp;frc ) {
3986   ResourceArea *area = Thread::current()-&gt;resource_area();
3987   Unique_Node_List sfpt(area);
3988 
3989   frc._visited.set(root-&gt;_idx); // first, mark node as visited
3990   uint cnt = root-&gt;req();
3991   Node *n = root;
3992   uint  i = 0;
3993   while (true) {
</pre>
<hr />
<pre>
4302   }
4303 }
4304 
4305 bool Compile::needs_clinit_barrier(ciMethod* method, ciMethod* accessing_method) {
4306   return method-&gt;is_static() &amp;&amp; needs_clinit_barrier(method-&gt;holder(), accessing_method);
4307 }
4308 
4309 bool Compile::needs_clinit_barrier(ciField* field, ciMethod* accessing_method) {
4310   return field-&gt;is_static() &amp;&amp; needs_clinit_barrier(field-&gt;holder(), accessing_method);
4311 }
4312 
4313 bool Compile::needs_clinit_barrier(ciInstanceKlass* holder, ciMethod* accessing_method) {
4314   if (holder-&gt;is_initialized()) {
4315     return false;
4316   }
4317   if (holder-&gt;is_being_initialized()) {
4318     if (accessing_method-&gt;holder() == holder) {
4319       // Access inside a class. The barrier can be elided when access happens in &lt;clinit&gt;,
4320       // &lt;init&gt;, or a static method. In all those cases, there was an initialization
4321       // barrier on the holder klass passed.
<span class="line-modified">4322       if (accessing_method-&gt;is_class_initializer() ||</span>
<span class="line-modified">4323           accessing_method-&gt;is_object_constructor() ||</span>
4324           accessing_method-&gt;is_static()) {
4325         return false;
4326       }
4327     } else if (accessing_method-&gt;holder()-&gt;is_subclass_of(holder)) {
4328       // Access from a subclass. The barrier can be elided only when access happens in &lt;clinit&gt;.
4329       // In case of &lt;init&gt; or a static method, the barrier is on the subclass is not enough:
4330       // child class can become fully initialized while its parent class is still being initialized.
<span class="line-modified">4331       if (accessing_method-&gt;is_class_initializer()) {</span>
4332         return false;
4333       }
4334     }
4335     ciMethod* root = method(); // the root method of compilation
4336     if (root != accessing_method) {
4337       return needs_clinit_barrier(holder, root); // check access in the context of compilation root
4338     }
4339   }
4340   return true;
4341 }
4342 
4343 #ifndef PRODUCT
4344 //------------------------------verify_graph_edges---------------------------
4345 // Walk the Graph and verify that there is a one-to-one correspondence
4346 // between Use-Def edges and Def-Use edges in the graph.
4347 void Compile::verify_graph_edges(bool no_dead_code) {
4348   if (VerifyGraphEdges) {
4349     ResourceArea *area = Thread::current()-&gt;resource_area();
4350     Unique_Node_List visited(area);
4351     // Call recursive graph walk to check edges
</pre>
<hr />
<pre>
4433                   _phase_name, C-&gt;unique(), C-&gt;live_nodes(), C-&gt;count_live_nodes_by_graph_walk());
4434   }
4435 
4436   if (VerifyIdealNodeCount) {
4437     Compile::current()-&gt;print_missing_nodes();
4438   }
4439 #endif
4440 
4441   if (_log != NULL) {
4442     _log-&gt;done(&quot;phase name=&#39;%s&#39; nodes=&#39;%d&#39; live=&#39;%d&#39;&quot;, _phase_name, C-&gt;unique(), C-&gt;live_nodes());
4443   }
4444 }
4445 
4446 //----------------------------static_subtype_check-----------------------------
4447 // Shortcut important common cases when superklass is exact:
4448 // (0) superklass is java.lang.Object (can occur in reflective code)
4449 // (1) subklass is already limited to a subtype of superklass =&gt; always ok
4450 // (2) subklass does not overlap with superklass =&gt; always fail
4451 // (3) superklass has NO subtypes and we can check with a simple compare.
4452 int Compile::static_subtype_check(ciKlass* superk, ciKlass* subk) {
<span class="line-modified">4453   if (StressReflectiveCode || superk == NULL || subk == NULL) {</span>
4454     return SSC_full_test;       // Let caller generate the general case.
4455   }
4456 
4457   if (superk == env()-&gt;Object_klass()) {
4458     return SSC_always_true;     // (0) this test cannot fail
4459   }
4460 
4461   ciType* superelem = superk;
<span class="line-modified">4462   if (superelem-&gt;is_array_klass()) {</span>
<span class="line-added">4463     ciArrayKlass* ak = superelem-&gt;as_array_klass();</span>
4464     superelem = superelem-&gt;as_array_klass()-&gt;base_element_type();
<span class="line-added">4465   }</span>
4466 
4467   if (!subk-&gt;is_interface()) {  // cannot trust static interface types yet
4468     if (subk-&gt;is_subtype_of(superk)) {
4469       return SSC_always_true;   // (1) false path dead; no dynamic test needed
4470     }
4471     if (!(superelem-&gt;is_klass() &amp;&amp; superelem-&gt;as_klass()-&gt;is_interface()) &amp;&amp;
4472         !superk-&gt;is_subtype_of(subk)) {
4473       return SSC_always_false;
4474     }
4475   }
4476 
<span class="line-added">4477   // Do not fold the subtype check to an array klass pointer comparison for [V? arrays.</span>
<span class="line-added">4478   // [V is a subtype of [V? but the klass for [V is not equal to the klass for [V?. Perform a full test.</span>
<span class="line-added">4479   if (superk-&gt;is_obj_array_klass() &amp;&amp; !superk-&gt;as_array_klass()-&gt;storage_properties().is_null_free() &amp;&amp; superk-&gt;as_array_klass()-&gt;element_klass()-&gt;is_valuetype()) {</span>
<span class="line-added">4480     return SSC_full_test;</span>
<span class="line-added">4481   }</span>
4482   // If casting to an instance klass, it must have no subtypes
4483   if (superk-&gt;is_interface()) {
4484     // Cannot trust interfaces yet.
4485     // %%% S.B. superk-&gt;nof_implementors() == 1
4486   } else if (superelem-&gt;is_instance_klass()) {
4487     ciInstanceKlass* ik = superelem-&gt;as_instance_klass();
4488     if (!ik-&gt;has_subklass() &amp;&amp; !ik-&gt;is_interface()) {
4489       if (!ik-&gt;is_final()) {
4490         // Add a dependency if there is a chance of a later subclass.
4491         dependencies()-&gt;assert_leaf_type(ik);
4492       }
4493       return SSC_easy_test;     // (3) caller can do a simple ptr comparison
4494     }
4495   } else {
4496     // A primitive array type has no subtypes.
4497     return SSC_easy_test;       // (3) caller can do a simple ptr comparison
4498   }
4499 
4500   return SSC_full_test;
4501 }
</pre>
<hr />
<pre>
4911     for (uint next = 0; next &lt; worklist.size(); ++next) {
4912       Node *n  = worklist.at(next);
4913       const Type* t = igvn.type_or_null(n);
4914       assert((t == NULL) || (t == t-&gt;remove_speculative()), &quot;no more speculative types&quot;);
4915       if (n-&gt;is_Type()) {
4916         t = n-&gt;as_Type()-&gt;type();
4917         assert(t == t-&gt;remove_speculative(), &quot;no more speculative types&quot;);
4918       }
4919       uint max = n-&gt;len();
4920       for( uint i = 0; i &lt; max; ++i ) {
4921         Node *m = n-&gt;in(i);
4922         if (not_a_node(m))  continue;
4923         worklist.push(m);
4924       }
4925     }
4926     igvn.check_no_speculative_types();
4927 #endif
4928   }
4929 }
4930 
<span class="line-added">4931 Node* Compile::optimize_acmp(PhaseGVN* phase, Node* a, Node* b) {</span>
<span class="line-added">4932   const TypeInstPtr* ta = phase-&gt;type(a)-&gt;isa_instptr();</span>
<span class="line-added">4933   const TypeInstPtr* tb = phase-&gt;type(b)-&gt;isa_instptr();</span>
<span class="line-added">4934   if (!EnableValhalla || ta == NULL || tb == NULL ||</span>
<span class="line-added">4935       ta-&gt;is_zero_type() || tb-&gt;is_zero_type() ||</span>
<span class="line-added">4936       !ta-&gt;can_be_value_type() || !tb-&gt;can_be_value_type()) {</span>
<span class="line-added">4937     // Use old acmp if one operand is null or not a value type</span>
<span class="line-added">4938     return new CmpPNode(a, b);</span>
<span class="line-added">4939   } else if (ta-&gt;is_valuetypeptr() || tb-&gt;is_valuetypeptr()) {</span>
<span class="line-added">4940     // We know that one operand is a value type. Therefore,</span>
<span class="line-added">4941     // new acmp will only return true if both operands are NULL.</span>
<span class="line-added">4942     // Check if both operands are null by or&#39;ing the oops.</span>
<span class="line-added">4943     a = phase-&gt;transform(new CastP2XNode(NULL, a));</span>
<span class="line-added">4944     b = phase-&gt;transform(new CastP2XNode(NULL, b));</span>
<span class="line-added">4945     a = phase-&gt;transform(new OrXNode(a, b));</span>
<span class="line-added">4946     return new CmpXNode(a, phase-&gt;MakeConX(0));</span>
<span class="line-added">4947   }</span>
<span class="line-added">4948   // Use new acmp</span>
<span class="line-added">4949   return NULL;</span>
<span class="line-added">4950 }</span>
<span class="line-added">4951 </span>
4952 // Auxiliary method to support randomized stressing/fuzzing.
4953 //
4954 // This method can be called the arbitrary number of times, with current count
4955 // as the argument. The logic allows selecting a single candidate from the
4956 // running list of candidates as follows:
4957 //    int count = 0;
4958 //    Cand* selected = null;
4959 //    while(cand = cand-&gt;next()) {
4960 //      if (randomized_select(++count)) {
4961 //        selected = cand;
4962 //      }
4963 //    }
4964 //
4965 // Including count equalizes the chances any candidate is &quot;selected&quot;.
4966 // This is useful when we don&#39;t have the complete list of candidates to choose
4967 // from uniformly. In this case, we need to adjust the randomicity of the
4968 // selection, or else we will end up biasing the selection towards the latter
4969 // candidates.
4970 //
4971 // Quick back-envelope calculation shows that for the list of n candidates
</pre>
</td>
</tr>
</table>
<center><a href="classes.hpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="compile.hpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>