diff a/src/hotspot/cpu/aarch64/interp_masm_aarch64.cpp b/src/hotspot/cpu/aarch64/interp_masm_aarch64.cpp
--- a/src/hotspot/cpu/aarch64/interp_masm_aarch64.cpp
+++ b/src/hotspot/cpu/aarch64/interp_masm_aarch64.cpp
@@ -33,10 +33,11 @@
 #include "logging/log.hpp"
 #include "oops/arrayOop.hpp"
 #include "oops/markWord.hpp"
 #include "oops/method.hpp"
 #include "oops/methodData.hpp"
+#include "oops/valueKlass.hpp"
 #include "prims/jvmtiExport.hpp"
 #include "prims/jvmtiThreadState.hpp"
 #include "runtime/basicLock.hpp"
 #include "runtime/biasedLocking.hpp"
 #include "runtime/frame.inline.hpp"
@@ -665,10 +666,11 @@
 
   // remove activation
   // get sender esp
   ldr(esp,
       Address(rfp, frame::interpreter_frame_sender_sp_offset * wordSize));
+
   if (StackReservedPages > 0) {
     // testing if reserved zone needs to be re-enabled
     Label no_reserved_zone_enabling;
 
     ldr(rscratch1, Address(rthread, JavaThread::reserved_stack_activation_offset()));
@@ -681,10 +683,37 @@
                    InterpreterRuntime::throw_delayed_StackOverflowError));
     should_not_reach_here();
 
     bind(no_reserved_zone_enabling);
   }
+
+
+  if (state == atos && ValueTypeReturnedAsFields) {
+    Label skip;
+    // Test if the return type is a value type
+    ldr(rscratch1, Address(rfp, frame::interpreter_frame_method_offset * wordSize));
+    ldr(rscratch1, Address(rscratch1, Method::const_offset()));
+    ldrb(rscratch1, Address(rscratch1, ConstMethod::result_type_offset()));
+    cmpw(rscratch1, (u1) T_VALUETYPE);
+    br(Assembler::NE, skip);
+
+    // We are returning a value type, load its fields into registers
+    // Load fields from a buffered value with a value class specific handler
+
+    load_klass(rscratch1 /*dst*/, r0 /*src*/);
+    ldr(rscratch1, Address(rscratch1, InstanceKlass::adr_valueklass_fixed_block_offset()));
+    ldr(rscratch1, Address(rscratch1, ValueKlass::unpack_handler_offset()));
+    cbz(rscratch1, skip);
+
+    blr(rscratch1);
+
+    // call above kills the value in r1. Reload it.
+    ldr(r1, Address(rfp, frame::interpreter_frame_sender_sp_offset * wordSize));
+    bind(skip);
+  }
+
+
   // remove frame anchor
   leave();
   // If we're returning to interpreted code we will shortly be
   // adjusting SP to allow some space for ESP.  If we're returning to
   // compiled code the saved sender SP was saved in sender_sp, so this
@@ -734,10 +763,15 @@
     orr(swap_reg, rscratch1, 1);
 
     // Save (object->mark() | 1) into BasicLock's displaced header
     str(swap_reg, Address(lock_reg, mark_offset));
 
+    if (EnableValhalla && !UseBiasedLocking) {
+      // For slow path is_always_locked, using biased, which is never natural for !UseBiasLocking
+      andr(swap_reg, swap_reg, ~((int) markWord::biased_lock_bit_in_place));
+    }
+
     assert(lock_offset == 0,
            "displached header must be first word in BasicObjectLock");
 
     Label fail;
     if (PrintBiasedLockingStatistics) {
@@ -1699,11 +1733,11 @@
       if (MethodData::profile_return()) {
         // We're right after the type profile for the last
         // argument. tmp is the number of cells left in the
         // CallTypeData/VirtualCallTypeData to reach its end. Non null
         // if there's a return to profile.
-        assert(ReturnTypeEntry::static_cell_count() < TypeStackSlotEntries::per_arg_count(), "can't move past ret type");
+        assert(SingleTypeEntry::static_cell_count() < TypeStackSlotEntries::per_arg_count(), "can't move past ret type");
         add(mdp, mdp, tmp, LSL, exact_log2(DataLayout::cell_size));
       }
       str(mdp, Address(rfp, frame::interpreter_frame_mdp_offset * wordSize));
     } else {
       assert(MethodData::profile_return(), "either profile call args or call ret");
@@ -1745,11 +1779,11 @@
       br(Assembler::NE, profile_continue);
 
       bind(do_profile);
     }
 
-    Address mdo_ret_addr(mdp, -in_bytes(ReturnTypeEntry::size()));
+    Address mdo_ret_addr(mdp, -in_bytes(SingleTypeEntry::size()));
     mov(tmp, ret);
     profile_obj_type(tmp, mdo_ret_addr);
 
     bind(profile_continue);
   }
