<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/cpu/x86/c1_LIRAssembler_x86.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="../sparc/interp_masm_sparc.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="c1_LIRGenerator_x86.cpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/cpu/x86/c1_LIRAssembler_x86.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #include &quot;precompiled.hpp&quot;
  26 #include &quot;asm/macroAssembler.hpp&quot;
  27 #include &quot;asm/macroAssembler.inline.hpp&quot;
  28 #include &quot;c1/c1_Compilation.hpp&quot;
  29 #include &quot;c1/c1_LIRAssembler.hpp&quot;
  30 #include &quot;c1/c1_MacroAssembler.hpp&quot;
  31 #include &quot;c1/c1_Runtime1.hpp&quot;
  32 #include &quot;c1/c1_ValueStack.hpp&quot;
  33 #include &quot;ci/ciArrayKlass.hpp&quot;
  34 #include &quot;ci/ciInstance.hpp&quot;

  35 #include &quot;gc/shared/collectedHeap.hpp&quot;
  36 #include &quot;nativeInst_x86.hpp&quot;

  37 #include &quot;oops/objArrayKlass.hpp&quot;
  38 #include &quot;runtime/frame.inline.hpp&quot;
  39 #include &quot;runtime/safepointMechanism.hpp&quot;
  40 #include &quot;runtime/sharedRuntime.hpp&quot;
  41 #include &quot;utilities/powerOfTwo.hpp&quot;
  42 #include &quot;vmreg_x86.inline.hpp&quot;
  43 
  44 
  45 // These masks are used to provide 128-bit aligned bitmasks to the XMM
  46 // instructions, to allow sign-masking or sign-bit flipping.  They allow
  47 // fast versions of NegF/NegD and AbsF/AbsD.
  48 
  49 // Note: &#39;double&#39; and &#39;long long&#39; have 32-bits alignment on x86.
  50 static jlong* double_quadword(jlong *adr, jlong lo, jlong hi) {
  51   // Use the expression (adr)&amp;(~0xF) to provide 128-bits aligned address
  52   // of 128-bits operands for SSE instructions.
  53   jlong *operand = (jlong*)(((intptr_t)adr) &amp; ((intptr_t)(~0xF)));
  54   // Store the value to a 128-bits operand.
  55   operand[0] = lo;
  56   operand[1] = hi;
</pre>
<hr />
<pre>
 174 
 175 void LIR_Assembler::ffree(int i) {
 176   __ ffree(i);
 177 }
 178 #endif // !_LP64
 179 
 180 void LIR_Assembler::breakpoint() {
 181   __ int3();
 182 }
 183 
 184 void LIR_Assembler::push(LIR_Opr opr) {
 185   if (opr-&gt;is_single_cpu()) {
 186     __ push_reg(opr-&gt;as_register());
 187   } else if (opr-&gt;is_double_cpu()) {
 188     NOT_LP64(__ push_reg(opr-&gt;as_register_hi()));
 189     __ push_reg(opr-&gt;as_register_lo());
 190   } else if (opr-&gt;is_stack()) {
 191     __ push_addr(frame_map()-&gt;address_for_slot(opr-&gt;single_stack_ix()));
 192   } else if (opr-&gt;is_constant()) {
 193     LIR_Const* const_opr = opr-&gt;as_constant_ptr();
<span class="line-modified"> 194     if (const_opr-&gt;type() == T_OBJECT) {</span>
 195       __ push_oop(const_opr-&gt;as_jobject());
 196     } else if (const_opr-&gt;type() == T_INT) {
 197       __ push_jint(const_opr-&gt;as_jint());
 198     } else {
 199       ShouldNotReachHere();
 200     }
 201 
 202   } else {
 203     ShouldNotReachHere();
 204   }
 205 }
 206 
 207 void LIR_Assembler::pop(LIR_Opr opr) {
 208   if (opr-&gt;is_single_cpu()) {
 209     __ pop_reg(opr-&gt;as_register());
 210   } else {
 211     ShouldNotReachHere();
 212   }
 213 }
 214 
</pre>
<hr />
<pre>
 461     __ bind(*stub-&gt;continuation());
 462   }
 463 
 464   if (compilation()-&gt;env()-&gt;dtrace_method_probes()) {
 465 #ifdef _LP64
 466     __ mov(rdi, r15_thread);
 467     __ mov_metadata(rsi, method()-&gt;constant_encoding());
 468 #else
 469     __ get_thread(rax);
 470     __ movptr(Address(rsp, 0), rax);
 471     __ mov_metadata(Address(rsp, sizeof(void*)), method()-&gt;constant_encoding());
 472 #endif
 473     __ call(RuntimeAddress(CAST_FROM_FN_PTR(address, SharedRuntime::dtrace_method_exit)));
 474   }
 475 
 476   if (method()-&gt;is_synchronized() || compilation()-&gt;env()-&gt;dtrace_method_probes()) {
 477     __ mov(rax, rbx);  // Restore the exception
 478   }
 479 
 480   // remove the activation and dispatch to the unwind handler
<span class="line-modified"> 481   __ remove_frame(initial_frame_size_in_bytes());</span>

 482   __ jump(RuntimeAddress(Runtime1::entry_for(Runtime1::unwind_exception_id)));
 483 
 484   // Emit the slow path assembly
 485   if (stub != NULL) {
 486     stub-&gt;emit_code(this);
 487   }
 488 
 489   return offset;
 490 }
 491 
 492 
 493 int LIR_Assembler::emit_deopt_handler() {
 494   // if the last instruction is a call (typically to do a throw which
 495   // is coming at the end after block reordering) the return address
 496   // must still point into the code area in order to avoid assertion
 497   // failures when searching for the corresponding bci =&gt; add a nop
 498   // (was bug 5/14/1999 - gri)
 499   __ nop();
 500 
 501   // generate code for exception handler
</pre>
<hr />
<pre>
 507   }
 508 
 509   int offset = code_offset();
 510   InternalAddress here(__ pc());
 511 
 512   __ pushptr(here.addr());
 513   __ jump(RuntimeAddress(SharedRuntime::deopt_blob()-&gt;unpack()));
 514   guarantee(code_offset() - offset &lt;= deopt_handler_size(), &quot;overflow&quot;);
 515   __ end_a_stub();
 516 
 517   return offset;
 518 }
 519 
 520 
 521 void LIR_Assembler::return_op(LIR_Opr result) {
 522   assert(result-&gt;is_illegal() || !result-&gt;is_single_cpu() || result-&gt;as_register() == rax, &quot;word returns are in rax,&quot;);
 523   if (!result-&gt;is_illegal() &amp;&amp; result-&gt;is_float_kind() &amp;&amp; !result-&gt;is_xmm_register()) {
 524     assert(result-&gt;fpu() == 0, &quot;result must already be on TOS&quot;);
 525   }
 526 



















 527   // Pop the stack before the safepoint code
<span class="line-modified"> 528   __ remove_frame(initial_frame_size_in_bytes());</span>

 529 
 530   if (StackReservedPages &gt; 0 &amp;&amp; compilation()-&gt;has_reserved_stack_access()) {
 531     __ reserved_stack_check();
 532   }
 533 
 534   bool result_is_oop = result-&gt;is_valid() ? result-&gt;is_oop() : false;
 535 
 536   // Note: we do not need to round double result; float result has the right precision
 537   // the poll sets the condition code, but no data registers
 538 
 539 #ifdef _LP64
 540   const Register poll_addr = rscratch1;
 541   __ movptr(poll_addr, Address(r15_thread, Thread::polling_page_offset()));
 542 #else
 543   const Register poll_addr = rbx;
 544   assert(FrameMap::is_caller_save_register(poll_addr), &quot;will overwrite&quot;);
 545   __ get_thread(poll_addr);
 546   __ movptr(poll_addr, Address(poll_addr, Thread::polling_page_offset()));
 547 #endif
 548   __ relocate(relocInfo::poll_return_type);
 549   __ testl(rax, Address(poll_addr, 0));
 550   __ ret(0);
 551 }
 552 
 553 




 554 int LIR_Assembler::safepoint_poll(LIR_Opr tmp, CodeEmitInfo* info) {
 555   guarantee(info != NULL, &quot;Shouldn&#39;t be NULL&quot;);
 556   int offset = __ offset();
 557 #ifdef _LP64
 558   const Register poll_addr = rscratch1;
 559   __ movptr(poll_addr, Address(r15_thread, Thread::polling_page_offset()));
 560 #else
 561   assert(tmp-&gt;is_cpu_register(), &quot;needed&quot;);
 562   const Register poll_addr = tmp-&gt;as_register();
 563   __ get_thread(poll_addr);
 564   __ movptr(poll_addr, Address(poll_addr, in_bytes(Thread::polling_page_offset())));
 565 #endif
 566   add_debug_info_for_branch(info);
 567   __ relocate(relocInfo::poll_type);
 568   address pre_pc = __ pc();
 569   __ testl(rax, Address(poll_addr, 0));
 570   address post_pc = __ pc();
 571   guarantee(pointer_delta(post_pc, pre_pc, 1) == 2 LP64_ONLY(+1), &quot;must be exact length&quot;);
 572   return offset;
 573 }
</pre>
<hr />
<pre>
 594       break;
 595     }
 596 
 597     case T_ADDRESS: {
 598       assert(patch_code == lir_patch_none, &quot;no patching handled here&quot;);
 599       __ movptr(dest-&gt;as_register(), c-&gt;as_jint());
 600       break;
 601     }
 602 
 603     case T_LONG: {
 604       assert(patch_code == lir_patch_none, &quot;no patching handled here&quot;);
 605 #ifdef _LP64
 606       __ movptr(dest-&gt;as_register_lo(), (intptr_t)c-&gt;as_jlong());
 607 #else
 608       __ movptr(dest-&gt;as_register_lo(), c-&gt;as_jint_lo());
 609       __ movptr(dest-&gt;as_register_hi(), c-&gt;as_jint_hi());
 610 #endif // _LP64
 611       break;
 612     }
 613 

 614     case T_OBJECT: {
 615       if (patch_code != lir_patch_none) {
 616         jobject2reg_with_patching(dest-&gt;as_register(), info);
 617       } else {
 618         __ movoop(dest-&gt;as_register(), c-&gt;as_jobject());
 619       }
 620       break;
 621     }
 622 
 623     case T_METADATA: {
 624       if (patch_code != lir_patch_none) {
 625         klass2reg_with_patching(dest-&gt;as_register(), info);
 626       } else {
 627         __ mov_metadata(dest-&gt;as_register(), c-&gt;as_metadata());
 628       }
 629       break;
 630     }
 631 
 632     case T_FLOAT: {
 633       if (dest-&gt;is_single_xmm()) {
</pre>
<hr />
<pre>
 684     default:
 685       ShouldNotReachHere();
 686   }
 687 }
 688 
 689 void LIR_Assembler::const2stack(LIR_Opr src, LIR_Opr dest) {
 690   assert(src-&gt;is_constant(), &quot;should not call otherwise&quot;);
 691   assert(dest-&gt;is_stack(), &quot;should not call otherwise&quot;);
 692   LIR_Const* c = src-&gt;as_constant_ptr();
 693 
 694   switch (c-&gt;type()) {
 695     case T_INT:  // fall through
 696     case T_FLOAT:
 697       __ movl(frame_map()-&gt;address_for_slot(dest-&gt;single_stack_ix()), c-&gt;as_jint_bits());
 698       break;
 699 
 700     case T_ADDRESS:
 701       __ movptr(frame_map()-&gt;address_for_slot(dest-&gt;single_stack_ix()), c-&gt;as_jint_bits());
 702       break;
 703 

 704     case T_OBJECT:
 705       __ movoop(frame_map()-&gt;address_for_slot(dest-&gt;single_stack_ix()), c-&gt;as_jobject());
 706       break;
 707 
 708     case T_LONG:  // fall through
 709     case T_DOUBLE:
 710 #ifdef _LP64
 711       __ movptr(frame_map()-&gt;address_for_slot(dest-&gt;double_stack_ix(),
 712                                             lo_word_offset_in_bytes), (intptr_t)c-&gt;as_jlong_bits());
 713 #else
 714       __ movptr(frame_map()-&gt;address_for_slot(dest-&gt;double_stack_ix(),
 715                                               lo_word_offset_in_bytes), c-&gt;as_jint_lo_bits());
 716       __ movptr(frame_map()-&gt;address_for_slot(dest-&gt;double_stack_ix(),
 717                                               hi_word_offset_in_bytes), c-&gt;as_jint_hi_bits());
 718 #endif // _LP64
 719       break;
 720 
 721     default:
 722       ShouldNotReachHere();
 723   }
 724 }
 725 
 726 void LIR_Assembler::const2mem(LIR_Opr src, LIR_Opr dest, BasicType type, CodeEmitInfo* info, bool wide) {
 727   assert(src-&gt;is_constant(), &quot;should not call otherwise&quot;);
 728   assert(dest-&gt;is_address(), &quot;should not call otherwise&quot;);
 729   LIR_Const* c = src-&gt;as_constant_ptr();
 730   LIR_Address* addr = dest-&gt;as_address_ptr();
 731 
 732   int null_check_here = code_offset();
 733   switch (type) {
 734     case T_INT:    // fall through
 735     case T_FLOAT:
 736       __ movl(as_Address(addr), c-&gt;as_jint_bits());
 737       break;
 738 
 739     case T_ADDRESS:
 740       __ movptr(as_Address(addr), c-&gt;as_jint_bits());
 741       break;
 742 

 743     case T_OBJECT:  // fall through
 744     case T_ARRAY:
 745       if (c-&gt;as_jobject() == NULL) {
 746         if (UseCompressedOops &amp;&amp; !wide) {
 747           __ movl(as_Address(addr), (int32_t)NULL_WORD);
 748         } else {
 749 #ifdef _LP64
 750           __ xorptr(rscratch1, rscratch1);
 751           null_check_here = code_offset();
 752           __ movptr(as_Address(addr), rscratch1);
 753 #else
 754           __ movptr(as_Address(addr), NULL_WORD);
 755 #endif
 756         }
 757       } else {
 758         if (is_literal_address(addr)) {
 759           ShouldNotReachHere();
 760           __ movoop(as_Address(addr, noreg), c-&gt;as_jobject());
 761         } else {
 762 #ifdef _LP64
</pre>
<hr />
<pre>
 811   if (info != NULL) {
 812     add_debug_info_for_null_check(null_check_here, info);
 813   }
 814 }
 815 
 816 
 817 void LIR_Assembler::reg2reg(LIR_Opr src, LIR_Opr dest) {
 818   assert(src-&gt;is_register(), &quot;should not call otherwise&quot;);
 819   assert(dest-&gt;is_register(), &quot;should not call otherwise&quot;);
 820 
 821   // move between cpu-registers
 822   if (dest-&gt;is_single_cpu()) {
 823 #ifdef _LP64
 824     if (src-&gt;type() == T_LONG) {
 825       // Can do LONG -&gt; OBJECT
 826       move_regs(src-&gt;as_register_lo(), dest-&gt;as_register());
 827       return;
 828     }
 829 #endif
 830     assert(src-&gt;is_single_cpu(), &quot;must match&quot;);
<span class="line-modified"> 831     if (src-&gt;type() == T_OBJECT) {</span>
 832       __ verify_oop(src-&gt;as_register());
 833     }
 834     move_regs(src-&gt;as_register(), dest-&gt;as_register());
 835 
 836   } else if (dest-&gt;is_double_cpu()) {
 837 #ifdef _LP64
 838     if (is_reference_type(src-&gt;type())) {
 839       // Surprising to me but we can see move of a long to t_object
 840       __ verify_oop(src-&gt;as_register());
 841       move_regs(src-&gt;as_register(), dest-&gt;as_register_lo());
 842       return;
 843     }
 844 #endif
 845     assert(src-&gt;is_double_cpu(), &quot;must match&quot;);
 846     Register f_lo = src-&gt;as_register_lo();
 847     Register f_hi = src-&gt;as_register_hi();
 848     Register t_lo = dest-&gt;as_register_lo();
 849     Register t_hi = dest-&gt;as_register_hi();
 850 #ifdef _LP64
 851     assert(f_hi == f_lo, &quot;must be same&quot;);
</pre>
<hr />
<pre>
 997       break;
 998     }
 999 
1000     case T_DOUBLE: {
1001 #ifdef _LP64
1002       assert(src-&gt;is_double_xmm(), &quot;not a double&quot;);
1003       __ movdbl(as_Address(to_addr), src-&gt;as_xmm_double_reg());
1004 #else
1005       if (src-&gt;is_double_xmm()) {
1006         __ movdbl(as_Address(to_addr), src-&gt;as_xmm_double_reg());
1007       } else {
1008         assert(src-&gt;is_double_fpu(), &quot;must be&quot;);
1009         assert(src-&gt;fpu_regnrLo() == 0, &quot;argument must be on TOS&quot;);
1010         if (pop_fpu_stack)      __ fstp_d(as_Address(to_addr));
1011         else                    __ fst_d (as_Address(to_addr));
1012       }
1013 #endif // _LP64
1014       break;
1015     }
1016 

1017     case T_ARRAY:   // fall through
1018     case T_OBJECT:  // fall through
1019       if (UseCompressedOops &amp;&amp; !wide) {
1020         __ movl(as_Address(to_addr), compressed_src);
1021       } else {
1022         __ movptr(as_Address(to_addr), src-&gt;as_register());
1023       }
1024       break;
1025     case T_METADATA:
1026       // We get here to store a method pointer to the stack to pass to
1027       // a dtrace runtime call. This can&#39;t work on 64 bit with
1028       // compressed klass ptrs: T_METADATA can be a compressed klass
1029       // ptr or a 64 bit method pointer.
1030       LP64_ONLY(ShouldNotReachHere());
1031       __ movptr(as_Address(to_addr), src-&gt;as_register());
1032       break;
1033     case T_ADDRESS:
1034       __ movptr(as_Address(to_addr), src-&gt;as_register());
1035       break;
1036     case T_INT:
</pre>
<hr />
<pre>
1169     __ pushl(frame_map()-&gt;address_for_slot(src -&gt;double_stack_ix(), 0));
1170     // push and pop the part at src + wordSize, adding wordSize for the previous push
1171     __ pushl(frame_map()-&gt;address_for_slot(src -&gt;double_stack_ix(), 2 * wordSize));
1172     __ popl (frame_map()-&gt;address_for_slot(dest-&gt;double_stack_ix(), 2 * wordSize));
1173     __ popl (frame_map()-&gt;address_for_slot(dest-&gt;double_stack_ix(), 0));
1174 #endif // _LP64
1175 
1176   } else {
1177     ShouldNotReachHere();
1178   }
1179 }
1180 
1181 
1182 void LIR_Assembler::mem2reg(LIR_Opr src, LIR_Opr dest, BasicType type, LIR_PatchCode patch_code, CodeEmitInfo* info, bool wide, bool /* unaligned */) {
1183   assert(src-&gt;is_address(), &quot;should not call otherwise&quot;);
1184   assert(dest-&gt;is_register(), &quot;should not call otherwise&quot;);
1185 
1186   LIR_Address* addr = src-&gt;as_address_ptr();
1187   Address from_addr = as_Address(addr);
1188 
<span class="line-modified">1189   if (addr-&gt;base()-&gt;type() == T_OBJECT) {</span>
1190     __ verify_oop(addr-&gt;base()-&gt;as_pointer_register());
1191   }
1192 
1193   switch (type) {
1194     case T_BOOLEAN: // fall through
1195     case T_BYTE:    // fall through
1196     case T_CHAR:    // fall through
1197     case T_SHORT:
1198       if (!VM_Version::is_P6() &amp;&amp; !from_addr.uses(dest-&gt;as_register())) {
1199         // on pre P6 processors we may get partial register stalls
1200         // so blow away the value of to_rinfo before loading a
1201         // partial word into it.  Do it here so that it precedes
1202         // the potential patch point below.
1203         __ xorptr(dest-&gt;as_register(), dest-&gt;as_register());
1204       }
1205       break;
1206    default:
1207      break;
1208   }
1209 
</pre>
<hr />
<pre>
1230 #endif // !LP64
1231       }
1232       break;
1233     }
1234 
1235     case T_DOUBLE: {
1236       if (dest-&gt;is_double_xmm()) {
1237         __ movdbl(dest-&gt;as_xmm_double_reg(), from_addr);
1238       } else {
1239 #ifndef _LP64
1240         assert(dest-&gt;is_double_fpu(), &quot;must be&quot;);
1241         assert(dest-&gt;fpu_regnrLo() == 0, &quot;dest must be TOS&quot;);
1242         __ fld_d(from_addr);
1243 #else
1244         ShouldNotReachHere();
1245 #endif // !LP64
1246       }
1247       break;
1248     }
1249 

1250     case T_OBJECT:  // fall through
1251     case T_ARRAY:   // fall through
1252       if (UseCompressedOops &amp;&amp; !wide) {
1253         __ movl(dest-&gt;as_register(), from_addr);
1254       } else {
1255         __ movptr(dest-&gt;as_register(), from_addr);
1256       }
1257       break;
1258 
1259     case T_ADDRESS:
1260       if (UseCompressedClassPointers &amp;&amp; addr-&gt;disp() == oopDesc::klass_offset_in_bytes()) {
1261         __ movl(dest-&gt;as_register(), from_addr);
1262       } else {
1263         __ movptr(dest-&gt;as_register(), from_addr);
1264       }
1265       break;
1266     case T_INT:
1267       __ movl(dest-&gt;as_register(), from_addr);
1268       break;
1269 
</pre>
<hr />
<pre>
1353   }
1354 
1355   if (patch != NULL) {
1356     patching_epilog(patch, patch_code, addr-&gt;base()-&gt;as_register(), info);
1357   }
1358 
1359   if (is_reference_type(type)) {
1360 #ifdef _LP64
1361     if (UseCompressedOops &amp;&amp; !wide) {
1362       __ decode_heap_oop(dest-&gt;as_register());
1363     }
1364 #endif
1365 
1366     // Load barrier has not yet been applied, so ZGC can&#39;t verify the oop here
1367     if (!UseZGC) {
1368       __ verify_oop(dest-&gt;as_register());
1369     }
1370   } else if (type == T_ADDRESS &amp;&amp; addr-&gt;disp() == oopDesc::klass_offset_in_bytes()) {
1371 #ifdef _LP64
1372     if (UseCompressedClassPointers) {

1373       __ decode_klass_not_null(dest-&gt;as_register());



1374     }


1375 #endif
1376   }
1377 }
1378 
1379 
1380 NEEDS_CLEANUP; // This could be static?
1381 Address::ScaleFactor LIR_Assembler::array_element_size(BasicType type) const {
1382   int elem_size = type2aelembytes(type);
1383   switch (elem_size) {
1384     case 1: return Address::times_1;
1385     case 2: return Address::times_2;
1386     case 4: return Address::times_4;
1387     case 8: return Address::times_8;
1388   }
1389   ShouldNotReachHere();
1390   return Address::no_scale;
1391 }
1392 
1393 
1394 void LIR_Assembler::emit_op3(LIR_Op3* op) {
</pre>
<hr />
<pre>
1616     add_debug_info_for_null_check_here(op-&gt;stub()-&gt;info());
1617     __ cmpb(Address(op-&gt;klass()-&gt;as_register(),
1618                     InstanceKlass::init_state_offset()),
1619                     InstanceKlass::fully_initialized);
1620     __ jcc(Assembler::notEqual, *op-&gt;stub()-&gt;entry());
1621   }
1622   __ allocate_object(op-&gt;obj()-&gt;as_register(),
1623                      op-&gt;tmp1()-&gt;as_register(),
1624                      op-&gt;tmp2()-&gt;as_register(),
1625                      op-&gt;header_size(),
1626                      op-&gt;object_size(),
1627                      op-&gt;klass()-&gt;as_register(),
1628                      *op-&gt;stub()-&gt;entry());
1629   __ bind(*op-&gt;stub()-&gt;continuation());
1630 }
1631 
1632 void LIR_Assembler::emit_alloc_array(LIR_OpAllocArray* op) {
1633   Register len =  op-&gt;len()-&gt;as_register();
1634   LP64_ONLY( __ movslq(len, len); )
1635 
<span class="line-modified">1636   if (UseSlowPath ||</span>
1637       (!UseFastNewObjectArray &amp;&amp; is_reference_type(op-&gt;type())) ||
1638       (!UseFastNewTypeArray   &amp;&amp; !is_reference_type(op-&gt;type()))) {
1639     __ jmp(*op-&gt;stub()-&gt;entry());
1640   } else {
1641     Register tmp1 = op-&gt;tmp1()-&gt;as_register();
1642     Register tmp2 = op-&gt;tmp2()-&gt;as_register();
1643     Register tmp3 = op-&gt;tmp3()-&gt;as_register();
1644     if (len == tmp1) {
1645       tmp1 = tmp3;
1646     } else if (len == tmp2) {
1647       tmp2 = tmp3;
1648     } else if (len == tmp3) {
1649       // everything is ok
1650     } else {
1651       __ mov(tmp3, len);
1652     }
1653     __ allocate_array(op-&gt;obj()-&gt;as_register(),
1654                       len,
1655                       tmp1,
1656                       tmp2,
</pre>
<hr />
<pre>
1714     assert(data-&gt;is_ReceiverTypeData(), &quot;need ReceiverTypeData for type check&quot;);
1715   }
1716   Label profile_cast_success, profile_cast_failure;
1717   Label *success_target = op-&gt;should_profile() ? &amp;profile_cast_success : success;
1718   Label *failure_target = op-&gt;should_profile() ? &amp;profile_cast_failure : failure;
1719 
1720   if (obj == k_RInfo) {
1721     k_RInfo = dst;
1722   } else if (obj == klass_RInfo) {
1723     klass_RInfo = dst;
1724   }
1725   if (k-&gt;is_loaded() &amp;&amp; !UseCompressedClassPointers) {
1726     select_different_registers(obj, dst, k_RInfo, klass_RInfo);
1727   } else {
1728     Rtmp1 = op-&gt;tmp3()-&gt;as_register();
1729     select_different_registers(obj, dst, k_RInfo, klass_RInfo, Rtmp1);
1730   }
1731 
1732   assert_different_registers(obj, k_RInfo, klass_RInfo);
1733 
<span class="line-modified">1734   __ cmpptr(obj, (int32_t)NULL_WORD);</span>
<span class="line-modified">1735   if (op-&gt;should_profile()) {</span>
<span class="line-modified">1736     Label not_null;</span>
<span class="line-modified">1737     __ jccb(Assembler::notEqual, not_null);</span>
<span class="line-modified">1738     // Object is null; update MDO and exit</span>
<span class="line-modified">1739     Register mdo  = klass_RInfo;</span>
<span class="line-modified">1740     __ mov_metadata(mdo, md-&gt;constant_encoding());</span>
<span class="line-modified">1741     Address data_addr(mdo, md-&gt;byte_offset_of_slot(data, DataLayout::flags_offset()));</span>
<span class="line-modified">1742     int header_bits = BitData::null_seen_byte_constant();</span>
<span class="line-modified">1743     __ orb(data_addr, header_bits);</span>
<span class="line-modified">1744     __ jmp(*obj_is_null);</span>
<span class="line-modified">1745     __ bind(not_null);</span>
<span class="line-modified">1746   } else {</span>
<span class="line-modified">1747     __ jcc(Assembler::equal, *obj_is_null);</span>


1748   }
1749 
1750   if (!k-&gt;is_loaded()) {
1751     klass2reg_with_patching(k_RInfo, op-&gt;info_for_patch());
1752   } else {
1753 #ifdef _LP64
1754     __ mov_metadata(k_RInfo, k-&gt;constant_encoding());
1755 #endif // _LP64
1756   }
1757   __ verify_oop(obj);
1758 
1759   if (op-&gt;fast_check()) {
1760     // get object class
1761     // not a safepoint as obj null check happens earlier
1762 #ifdef _LP64
1763     if (UseCompressedClassPointers) {
1764       __ load_klass(Rtmp1, obj);
1765       __ cmpptr(k_RInfo, Rtmp1);
1766     } else {
1767       __ cmpptr(k_RInfo, Address(obj, oopDesc::klass_offset_in_bytes()));
</pre>
<hr />
<pre>
1938         __ mov(dst, obj);
1939       }
1940     } else
1941       if (code == lir_instanceof) {
1942         Register obj = op-&gt;object()-&gt;as_register();
1943         Register dst = op-&gt;result_opr()-&gt;as_register();
1944         Label success, failure, done;
1945         emit_typecheck_helper(op, &amp;success, &amp;failure, &amp;failure);
1946         __ bind(failure);
1947         __ xorptr(dst, dst);
1948         __ jmpb(done);
1949         __ bind(success);
1950         __ movptr(dst, 1);
1951         __ bind(done);
1952       } else {
1953         ShouldNotReachHere();
1954       }
1955 
1956 }
1957 











































































































1958 
1959 void LIR_Assembler::emit_compare_and_swap(LIR_OpCompareAndSwap* op) {
1960   if (LP64_ONLY(false &amp;&amp;) op-&gt;code() == lir_cas_long &amp;&amp; VM_Version::supports_cx8()) {
1961     assert(op-&gt;cmp_value()-&gt;as_register_lo() == rax, &quot;wrong register&quot;);
1962     assert(op-&gt;cmp_value()-&gt;as_register_hi() == rdx, &quot;wrong register&quot;);
1963     assert(op-&gt;new_value()-&gt;as_register_lo() == rbx, &quot;wrong register&quot;);
1964     assert(op-&gt;new_value()-&gt;as_register_hi() == rcx, &quot;wrong register&quot;);
1965     Register addr = op-&gt;addr()-&gt;as_register();
1966     __ lock();
1967     NOT_LP64(__ cmpxchg8(Address(addr, 0)));
1968 
1969   } else if (op-&gt;code() == lir_cas_int || op-&gt;code() == lir_cas_obj ) {
1970     NOT_LP64(assert(op-&gt;addr()-&gt;is_single_cpu(), &quot;must be single&quot;);)
1971     Register addr = (op-&gt;addr()-&gt;is_single_cpu() ? op-&gt;addr()-&gt;as_register() : op-&gt;addr()-&gt;as_register_lo());
1972     Register newval = op-&gt;new_value()-&gt;as_register();
1973     Register cmpval = op-&gt;cmp_value()-&gt;as_register();
1974     assert(cmpval == rax, &quot;wrong register&quot;);
1975     assert(newval != NULL, &quot;new val must be register&quot;);
1976     assert(cmpval != newval, &quot;cmp and new values must be in different registers&quot;);
1977     assert(cmpval != addr, &quot;cmp and addr must be in different registers&quot;);
</pre>
<hr />
<pre>
1998       __ cmpxchgl(newval, Address(addr, 0));
1999     }
2000 #ifdef _LP64
2001   } else if (op-&gt;code() == lir_cas_long) {
2002     Register addr = (op-&gt;addr()-&gt;is_single_cpu() ? op-&gt;addr()-&gt;as_register() : op-&gt;addr()-&gt;as_register_lo());
2003     Register newval = op-&gt;new_value()-&gt;as_register_lo();
2004     Register cmpval = op-&gt;cmp_value()-&gt;as_register_lo();
2005     assert(cmpval == rax, &quot;wrong register&quot;);
2006     assert(newval != NULL, &quot;new val must be register&quot;);
2007     assert(cmpval != newval, &quot;cmp and new values must be in different registers&quot;);
2008     assert(cmpval != addr, &quot;cmp and addr must be in different registers&quot;);
2009     assert(newval != addr, &quot;new value and addr must be in different registers&quot;);
2010     __ lock();
2011     __ cmpxchgq(newval, Address(addr, 0));
2012 #endif // _LP64
2013   } else {
2014     Unimplemented();
2015   }
2016 }
2017 















2018 void LIR_Assembler::cmove(LIR_Condition condition, LIR_Opr opr1, LIR_Opr opr2, LIR_Opr result, BasicType type) {
2019   Assembler::Condition acond, ncond;
2020   switch (condition) {
2021     case lir_cond_equal:        acond = Assembler::equal;        ncond = Assembler::notEqual;     break;
2022     case lir_cond_notEqual:     acond = Assembler::notEqual;     ncond = Assembler::equal;        break;
2023     case lir_cond_less:         acond = Assembler::less;         ncond = Assembler::greaterEqual; break;
2024     case lir_cond_lessEqual:    acond = Assembler::lessEqual;    ncond = Assembler::greater;      break;
2025     case lir_cond_greaterEqual: acond = Assembler::greaterEqual; ncond = Assembler::less;         break;
2026     case lir_cond_greater:      acond = Assembler::greater;      ncond = Assembler::lessEqual;    break;
2027     case lir_cond_belowEqual:   acond = Assembler::belowEqual;   ncond = Assembler::above;        break;
2028     case lir_cond_aboveEqual:   acond = Assembler::aboveEqual;   ncond = Assembler::below;        break;
2029     default:                    acond = Assembler::equal;        ncond = Assembler::notEqual;
2030                                 ShouldNotReachHere();
2031   }
2032 
2033   if (opr1-&gt;is_cpu_register()) {
2034     reg2reg(opr1, result);
2035   } else if (opr1-&gt;is_stack()) {
2036     stack2reg(opr1, result, result-&gt;type());
2037   } else if (opr1-&gt;is_constant()) {
</pre>
<hr />
<pre>
2877   switch (code) {
2878   case lir_static_call:
2879   case lir_optvirtual_call:
2880   case lir_dynamic_call:
2881     offset += NativeCall::displacement_offset;
2882     break;
2883   case lir_icvirtual_call:
2884     offset += NativeCall::displacement_offset + NativeMovConstReg::instruction_size;
2885     break;
2886   case lir_virtual_call:  // currently, sparc-specific for niagara
2887   default: ShouldNotReachHere();
2888   }
2889   __ align(BytesPerWord, offset);
2890 }
2891 
2892 
2893 void LIR_Assembler::call(LIR_OpJavaCall* op, relocInfo::relocType rtype) {
2894   assert((__ offset() + NativeCall::displacement_offset) % BytesPerWord == 0,
2895          &quot;must be aligned&quot;);
2896   __ call(AddressLiteral(op-&gt;addr(), rtype));
<span class="line-modified">2897   add_call_info(code_offset(), op-&gt;info());</span>
2898 }
2899 
2900 
2901 void LIR_Assembler::ic_call(LIR_OpJavaCall* op) {
2902   __ ic_call(op-&gt;addr());
<span class="line-modified">2903   add_call_info(code_offset(), op-&gt;info());</span>
2904   assert((__ offset() - NativeCall::instruction_size + NativeCall::displacement_offset) % BytesPerWord == 0,
2905          &quot;must be aligned&quot;);
2906 }
2907 
2908 
2909 /* Currently, vtable-dispatch is only enabled for sparc platforms */
2910 void LIR_Assembler::vtable_call(LIR_OpJavaCall* op) {
2911   ShouldNotReachHere();
2912 }
2913 
2914 
2915 void LIR_Assembler::emit_static_call_stub() {
2916   address call_pc = __ pc();
2917   address stub = __ start_a_stub(call_stub_size());
2918   if (stub == NULL) {
2919     bailout(&quot;static call stub overflow&quot;);
2920     return;
2921   }
2922 
2923   int start = __ offset();
</pre>
<hr />
<pre>
3079   __ movptr (Address(rsp, offset_from_rsp_in_bytes), c);
3080 }
3081 
3082 
3083 void LIR_Assembler::store_parameter(jobject o,  int offset_from_rsp_in_words) {
3084   assert(offset_from_rsp_in_words &gt;= 0, &quot;invalid offset from rsp&quot;);
3085   int offset_from_rsp_in_bytes = offset_from_rsp_in_words * BytesPerWord;
3086   assert(offset_from_rsp_in_bytes &lt; frame_map()-&gt;reserved_argument_area_size(), &quot;invalid offset&quot;);
3087   __ movoop (Address(rsp, offset_from_rsp_in_bytes), o);
3088 }
3089 
3090 
3091 void LIR_Assembler::store_parameter(Metadata* m,  int offset_from_rsp_in_words) {
3092   assert(offset_from_rsp_in_words &gt;= 0, &quot;invalid offset from rsp&quot;);
3093   int offset_from_rsp_in_bytes = offset_from_rsp_in_words * BytesPerWord;
3094   assert(offset_from_rsp_in_bytes &lt; frame_map()-&gt;reserved_argument_area_size(), &quot;invalid offset&quot;);
3095   __ mov_metadata(Address(rsp, offset_from_rsp_in_bytes), m);
3096 }
3097 
3098 

















3099 // This code replaces a call to arraycopy; no exception may
3100 // be thrown in this code, they must be thrown in the System.arraycopy
3101 // activation frame; we could save some checks if this would not be the case
3102 void LIR_Assembler::emit_arraycopy(LIR_OpArrayCopy* op) {
3103   ciArrayKlass* default_type = op-&gt;expected_type();
3104   Register src = op-&gt;src()-&gt;as_register();
3105   Register dst = op-&gt;dst()-&gt;as_register();
3106   Register src_pos = op-&gt;src_pos()-&gt;as_register();
3107   Register dst_pos = op-&gt;dst_pos()-&gt;as_register();
3108   Register length  = op-&gt;length()-&gt;as_register();
3109   Register tmp = op-&gt;tmp()-&gt;as_register();
3110 
3111   __ resolve(ACCESS_READ, src);
3112   __ resolve(ACCESS_WRITE, dst);
3113 
3114   CodeStub* stub = op-&gt;stub();
3115   int flags = op-&gt;flags();
3116   BasicType basic_type = default_type != NULL ? default_type-&gt;element_type()-&gt;basic_type() : T_ILLEGAL;
3117   if (is_reference_type(basic_type)) basic_type = T_OBJECT;
3118 














3119   // if we don&#39;t know anything, just go through the generic arraycopy
3120   if (default_type == NULL) {
3121     // save outgoing arguments on stack in case call to System.arraycopy is needed
3122     // HACK ALERT. This code used to push the parameters in a hardwired fashion
3123     // for interpreter calling conventions. Now we have to do it in new style conventions.
3124     // For the moment until C1 gets the new register allocator I just force all the
3125     // args to the right place (except the register args) and then on the back side
3126     // reload the register args properly if we go slow path. Yuck
3127 
3128     // These are proper for the calling convention
3129     store_parameter(length, 2);
3130     store_parameter(dst_pos, 1);
3131     store_parameter(dst, 0);
3132 
3133     // these are just temporary placements until we need to reload
3134     store_parameter(src_pos, 3);
3135     store_parameter(src, 4);
3136     NOT_LP64(assert(src == rcx &amp;&amp; src_pos == rdx, &quot;mismatch in calling convention&quot;);)
3137 
3138     address copyfunc_addr = StubRoutines::generic_arraycopy();
</pre>
<hr />
<pre>
4034 }
4035 
4036 void LIR_Assembler::membar_storeload() {
4037   __ membar(Assembler::Membar_mask_bits(Assembler::StoreLoad));
4038 }
4039 
4040 void LIR_Assembler::on_spin_wait() {
4041   __ pause ();
4042 }
4043 
4044 void LIR_Assembler::get_thread(LIR_Opr result_reg) {
4045   assert(result_reg-&gt;is_register(), &quot;check&quot;);
4046 #ifdef _LP64
4047   // __ get_thread(result_reg-&gt;as_register_lo());
4048   __ mov(result_reg-&gt;as_register(), r15_thread);
4049 #else
4050   __ get_thread(result_reg-&gt;as_register());
4051 #endif // _LP64
4052 }
4053 



4054 
4055 void LIR_Assembler::peephole(LIR_List*) {
4056   // do nothing for now
4057 }
4058 
4059 void LIR_Assembler::atomic_op(LIR_Code code, LIR_Opr src, LIR_Opr data, LIR_Opr dest, LIR_Opr tmp) {
4060   assert(data == dest, &quot;xchg/xadd uses only 2 operands&quot;);
4061 
4062   if (data-&gt;type() == T_INT) {
4063     if (code == lir_xadd) {
4064       __ lock();
4065       __ xaddl(as_Address(src-&gt;as_address_ptr()), data-&gt;as_register());
4066     } else {
4067       __ xchgl(data-&gt;as_register(), as_Address(src-&gt;as_address_ptr()));
4068     }
4069   } else if (data-&gt;is_oop()) {
4070     assert (code == lir_xchg, &quot;xadd for oops&quot;);
4071     Register obj = data-&gt;as_register();
4072 #ifdef _LP64
4073     if (UseCompressedOops) {
</pre>
</td>
<td>
<hr />
<pre>
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #include &quot;precompiled.hpp&quot;
  26 #include &quot;asm/macroAssembler.hpp&quot;
  27 #include &quot;asm/macroAssembler.inline.hpp&quot;
  28 #include &quot;c1/c1_Compilation.hpp&quot;
  29 #include &quot;c1/c1_LIRAssembler.hpp&quot;
  30 #include &quot;c1/c1_MacroAssembler.hpp&quot;
  31 #include &quot;c1/c1_Runtime1.hpp&quot;
  32 #include &quot;c1/c1_ValueStack.hpp&quot;
  33 #include &quot;ci/ciArrayKlass.hpp&quot;
  34 #include &quot;ci/ciInstance.hpp&quot;
<span class="line-added">  35 #include &quot;ci/ciValueKlass.hpp&quot;</span>
  36 #include &quot;gc/shared/collectedHeap.hpp&quot;
  37 #include &quot;nativeInst_x86.hpp&quot;
<span class="line-added">  38 #include &quot;oops/oop.inline.hpp&quot;</span>
  39 #include &quot;oops/objArrayKlass.hpp&quot;
  40 #include &quot;runtime/frame.inline.hpp&quot;
  41 #include &quot;runtime/safepointMechanism.hpp&quot;
  42 #include &quot;runtime/sharedRuntime.hpp&quot;
  43 #include &quot;utilities/powerOfTwo.hpp&quot;
  44 #include &quot;vmreg_x86.inline.hpp&quot;
  45 
  46 
  47 // These masks are used to provide 128-bit aligned bitmasks to the XMM
  48 // instructions, to allow sign-masking or sign-bit flipping.  They allow
  49 // fast versions of NegF/NegD and AbsF/AbsD.
  50 
  51 // Note: &#39;double&#39; and &#39;long long&#39; have 32-bits alignment on x86.
  52 static jlong* double_quadword(jlong *adr, jlong lo, jlong hi) {
  53   // Use the expression (adr)&amp;(~0xF) to provide 128-bits aligned address
  54   // of 128-bits operands for SSE instructions.
  55   jlong *operand = (jlong*)(((intptr_t)adr) &amp; ((intptr_t)(~0xF)));
  56   // Store the value to a 128-bits operand.
  57   operand[0] = lo;
  58   operand[1] = hi;
</pre>
<hr />
<pre>
 176 
 177 void LIR_Assembler::ffree(int i) {
 178   __ ffree(i);
 179 }
 180 #endif // !_LP64
 181 
 182 void LIR_Assembler::breakpoint() {
 183   __ int3();
 184 }
 185 
 186 void LIR_Assembler::push(LIR_Opr opr) {
 187   if (opr-&gt;is_single_cpu()) {
 188     __ push_reg(opr-&gt;as_register());
 189   } else if (opr-&gt;is_double_cpu()) {
 190     NOT_LP64(__ push_reg(opr-&gt;as_register_hi()));
 191     __ push_reg(opr-&gt;as_register_lo());
 192   } else if (opr-&gt;is_stack()) {
 193     __ push_addr(frame_map()-&gt;address_for_slot(opr-&gt;single_stack_ix()));
 194   } else if (opr-&gt;is_constant()) {
 195     LIR_Const* const_opr = opr-&gt;as_constant_ptr();
<span class="line-modified"> 196     if (const_opr-&gt;type() == T_OBJECT || const_opr-&gt;type() == T_VALUETYPE) {</span>
 197       __ push_oop(const_opr-&gt;as_jobject());
 198     } else if (const_opr-&gt;type() == T_INT) {
 199       __ push_jint(const_opr-&gt;as_jint());
 200     } else {
 201       ShouldNotReachHere();
 202     }
 203 
 204   } else {
 205     ShouldNotReachHere();
 206   }
 207 }
 208 
 209 void LIR_Assembler::pop(LIR_Opr opr) {
 210   if (opr-&gt;is_single_cpu()) {
 211     __ pop_reg(opr-&gt;as_register());
 212   } else {
 213     ShouldNotReachHere();
 214   }
 215 }
 216 
</pre>
<hr />
<pre>
 463     __ bind(*stub-&gt;continuation());
 464   }
 465 
 466   if (compilation()-&gt;env()-&gt;dtrace_method_probes()) {
 467 #ifdef _LP64
 468     __ mov(rdi, r15_thread);
 469     __ mov_metadata(rsi, method()-&gt;constant_encoding());
 470 #else
 471     __ get_thread(rax);
 472     __ movptr(Address(rsp, 0), rax);
 473     __ mov_metadata(Address(rsp, sizeof(void*)), method()-&gt;constant_encoding());
 474 #endif
 475     __ call(RuntimeAddress(CAST_FROM_FN_PTR(address, SharedRuntime::dtrace_method_exit)));
 476   }
 477 
 478   if (method()-&gt;is_synchronized() || compilation()-&gt;env()-&gt;dtrace_method_probes()) {
 479     __ mov(rax, rbx);  // Restore the exception
 480   }
 481 
 482   // remove the activation and dispatch to the unwind handler
<span class="line-modified"> 483   int initial_framesize = initial_frame_size_in_bytes();</span>
<span class="line-added"> 484   __ remove_frame(initial_framesize, needs_stack_repair(), initial_framesize - wordSize);</span>
 485   __ jump(RuntimeAddress(Runtime1::entry_for(Runtime1::unwind_exception_id)));
 486 
 487   // Emit the slow path assembly
 488   if (stub != NULL) {
 489     stub-&gt;emit_code(this);
 490   }
 491 
 492   return offset;
 493 }
 494 
 495 
 496 int LIR_Assembler::emit_deopt_handler() {
 497   // if the last instruction is a call (typically to do a throw which
 498   // is coming at the end after block reordering) the return address
 499   // must still point into the code area in order to avoid assertion
 500   // failures when searching for the corresponding bci =&gt; add a nop
 501   // (was bug 5/14/1999 - gri)
 502   __ nop();
 503 
 504   // generate code for exception handler
</pre>
<hr />
<pre>
 510   }
 511 
 512   int offset = code_offset();
 513   InternalAddress here(__ pc());
 514 
 515   __ pushptr(here.addr());
 516   __ jump(RuntimeAddress(SharedRuntime::deopt_blob()-&gt;unpack()));
 517   guarantee(code_offset() - offset &lt;= deopt_handler_size(), &quot;overflow&quot;);
 518   __ end_a_stub();
 519 
 520   return offset;
 521 }
 522 
 523 
 524 void LIR_Assembler::return_op(LIR_Opr result) {
 525   assert(result-&gt;is_illegal() || !result-&gt;is_single_cpu() || result-&gt;as_register() == rax, &quot;word returns are in rax,&quot;);
 526   if (!result-&gt;is_illegal() &amp;&amp; result-&gt;is_float_kind() &amp;&amp; !result-&gt;is_xmm_register()) {
 527     assert(result-&gt;fpu() == 0, &quot;result must already be on TOS&quot;);
 528   }
 529 
<span class="line-added"> 530   ciMethod* method = compilation()-&gt;method();</span>
<span class="line-added"> 531   if (ValueTypeReturnedAsFields &amp;&amp; method-&gt;signature()-&gt;returns_never_null()) {</span>
<span class="line-added"> 532     ciType* return_type = method-&gt;return_type();</span>
<span class="line-added"> 533     if (return_type-&gt;is_valuetype()) {</span>
<span class="line-added"> 534       ciValueKlass* vk = return_type-&gt;as_value_klass();</span>
<span class="line-added"> 535       if (vk-&gt;can_be_returned_as_fields()) {</span>
<span class="line-added"> 536 #ifndef _LP64</span>
<span class="line-added"> 537         Unimplemented();</span>
<span class="line-added"> 538 #else</span>
<span class="line-added"> 539         address unpack_handler = vk-&gt;unpack_handler();</span>
<span class="line-added"> 540         assert(unpack_handler != NULL, &quot;must be&quot;);</span>
<span class="line-added"> 541         __ call(RuntimeAddress(unpack_handler));</span>
<span class="line-added"> 542         // At this point, rax points to the value object (for interpreter or C1 caller).</span>
<span class="line-added"> 543         // The fields of the object are copied into registers (for C2 caller).</span>
<span class="line-added"> 544 #endif</span>
<span class="line-added"> 545       }</span>
<span class="line-added"> 546     }</span>
<span class="line-added"> 547   }</span>
<span class="line-added"> 548 </span>
 549   // Pop the stack before the safepoint code
<span class="line-modified"> 550   int initial_framesize = initial_frame_size_in_bytes();</span>
<span class="line-added"> 551   __ remove_frame(initial_framesize, needs_stack_repair(), initial_framesize - wordSize);</span>
 552 
 553   if (StackReservedPages &gt; 0 &amp;&amp; compilation()-&gt;has_reserved_stack_access()) {
 554     __ reserved_stack_check();
 555   }
 556 
 557   bool result_is_oop = result-&gt;is_valid() ? result-&gt;is_oop() : false;
 558 
 559   // Note: we do not need to round double result; float result has the right precision
 560   // the poll sets the condition code, but no data registers
 561 
 562 #ifdef _LP64
 563   const Register poll_addr = rscratch1;
 564   __ movptr(poll_addr, Address(r15_thread, Thread::polling_page_offset()));
 565 #else
 566   const Register poll_addr = rbx;
 567   assert(FrameMap::is_caller_save_register(poll_addr), &quot;will overwrite&quot;);
 568   __ get_thread(poll_addr);
 569   __ movptr(poll_addr, Address(poll_addr, Thread::polling_page_offset()));
 570 #endif
 571   __ relocate(relocInfo::poll_return_type);
 572   __ testl(rax, Address(poll_addr, 0));
 573   __ ret(0);
 574 }
 575 
 576 
<span class="line-added"> 577 int LIR_Assembler::store_value_type_fields_to_buf(ciValueKlass* vk) {</span>
<span class="line-added"> 578   return (__ store_value_type_fields_to_buf(vk, false));</span>
<span class="line-added"> 579 }</span>
<span class="line-added"> 580 </span>
 581 int LIR_Assembler::safepoint_poll(LIR_Opr tmp, CodeEmitInfo* info) {
 582   guarantee(info != NULL, &quot;Shouldn&#39;t be NULL&quot;);
 583   int offset = __ offset();
 584 #ifdef _LP64
 585   const Register poll_addr = rscratch1;
 586   __ movptr(poll_addr, Address(r15_thread, Thread::polling_page_offset()));
 587 #else
 588   assert(tmp-&gt;is_cpu_register(), &quot;needed&quot;);
 589   const Register poll_addr = tmp-&gt;as_register();
 590   __ get_thread(poll_addr);
 591   __ movptr(poll_addr, Address(poll_addr, in_bytes(Thread::polling_page_offset())));
 592 #endif
 593   add_debug_info_for_branch(info);
 594   __ relocate(relocInfo::poll_type);
 595   address pre_pc = __ pc();
 596   __ testl(rax, Address(poll_addr, 0));
 597   address post_pc = __ pc();
 598   guarantee(pointer_delta(post_pc, pre_pc, 1) == 2 LP64_ONLY(+1), &quot;must be exact length&quot;);
 599   return offset;
 600 }
</pre>
<hr />
<pre>
 621       break;
 622     }
 623 
 624     case T_ADDRESS: {
 625       assert(patch_code == lir_patch_none, &quot;no patching handled here&quot;);
 626       __ movptr(dest-&gt;as_register(), c-&gt;as_jint());
 627       break;
 628     }
 629 
 630     case T_LONG: {
 631       assert(patch_code == lir_patch_none, &quot;no patching handled here&quot;);
 632 #ifdef _LP64
 633       __ movptr(dest-&gt;as_register_lo(), (intptr_t)c-&gt;as_jlong());
 634 #else
 635       __ movptr(dest-&gt;as_register_lo(), c-&gt;as_jint_lo());
 636       __ movptr(dest-&gt;as_register_hi(), c-&gt;as_jint_hi());
 637 #endif // _LP64
 638       break;
 639     }
 640 
<span class="line-added"> 641     case T_VALUETYPE: // Fall through</span>
 642     case T_OBJECT: {
 643       if (patch_code != lir_patch_none) {
 644         jobject2reg_with_patching(dest-&gt;as_register(), info);
 645       } else {
 646         __ movoop(dest-&gt;as_register(), c-&gt;as_jobject());
 647       }
 648       break;
 649     }
 650 
 651     case T_METADATA: {
 652       if (patch_code != lir_patch_none) {
 653         klass2reg_with_patching(dest-&gt;as_register(), info);
 654       } else {
 655         __ mov_metadata(dest-&gt;as_register(), c-&gt;as_metadata());
 656       }
 657       break;
 658     }
 659 
 660     case T_FLOAT: {
 661       if (dest-&gt;is_single_xmm()) {
</pre>
<hr />
<pre>
 712     default:
 713       ShouldNotReachHere();
 714   }
 715 }
 716 
 717 void LIR_Assembler::const2stack(LIR_Opr src, LIR_Opr dest) {
 718   assert(src-&gt;is_constant(), &quot;should not call otherwise&quot;);
 719   assert(dest-&gt;is_stack(), &quot;should not call otherwise&quot;);
 720   LIR_Const* c = src-&gt;as_constant_ptr();
 721 
 722   switch (c-&gt;type()) {
 723     case T_INT:  // fall through
 724     case T_FLOAT:
 725       __ movl(frame_map()-&gt;address_for_slot(dest-&gt;single_stack_ix()), c-&gt;as_jint_bits());
 726       break;
 727 
 728     case T_ADDRESS:
 729       __ movptr(frame_map()-&gt;address_for_slot(dest-&gt;single_stack_ix()), c-&gt;as_jint_bits());
 730       break;
 731 
<span class="line-added"> 732     case T_VALUETYPE: // Fall through</span>
 733     case T_OBJECT:
 734       __ movoop(frame_map()-&gt;address_for_slot(dest-&gt;single_stack_ix()), c-&gt;as_jobject());
 735       break;
 736 
 737     case T_LONG:  // fall through
 738     case T_DOUBLE:
 739 #ifdef _LP64
 740       __ movptr(frame_map()-&gt;address_for_slot(dest-&gt;double_stack_ix(),
 741                                             lo_word_offset_in_bytes), (intptr_t)c-&gt;as_jlong_bits());
 742 #else
 743       __ movptr(frame_map()-&gt;address_for_slot(dest-&gt;double_stack_ix(),
 744                                               lo_word_offset_in_bytes), c-&gt;as_jint_lo_bits());
 745       __ movptr(frame_map()-&gt;address_for_slot(dest-&gt;double_stack_ix(),
 746                                               hi_word_offset_in_bytes), c-&gt;as_jint_hi_bits());
 747 #endif // _LP64
 748       break;
 749 
 750     default:
 751       ShouldNotReachHere();
 752   }
 753 }
 754 
 755 void LIR_Assembler::const2mem(LIR_Opr src, LIR_Opr dest, BasicType type, CodeEmitInfo* info, bool wide) {
 756   assert(src-&gt;is_constant(), &quot;should not call otherwise&quot;);
 757   assert(dest-&gt;is_address(), &quot;should not call otherwise&quot;);
 758   LIR_Const* c = src-&gt;as_constant_ptr();
 759   LIR_Address* addr = dest-&gt;as_address_ptr();
 760 
 761   int null_check_here = code_offset();
 762   switch (type) {
 763     case T_INT:    // fall through
 764     case T_FLOAT:
 765       __ movl(as_Address(addr), c-&gt;as_jint_bits());
 766       break;
 767 
 768     case T_ADDRESS:
 769       __ movptr(as_Address(addr), c-&gt;as_jint_bits());
 770       break;
 771 
<span class="line-added"> 772     case T_VALUETYPE: // fall through</span>
 773     case T_OBJECT:  // fall through
 774     case T_ARRAY:
 775       if (c-&gt;as_jobject() == NULL) {
 776         if (UseCompressedOops &amp;&amp; !wide) {
 777           __ movl(as_Address(addr), (int32_t)NULL_WORD);
 778         } else {
 779 #ifdef _LP64
 780           __ xorptr(rscratch1, rscratch1);
 781           null_check_here = code_offset();
 782           __ movptr(as_Address(addr), rscratch1);
 783 #else
 784           __ movptr(as_Address(addr), NULL_WORD);
 785 #endif
 786         }
 787       } else {
 788         if (is_literal_address(addr)) {
 789           ShouldNotReachHere();
 790           __ movoop(as_Address(addr, noreg), c-&gt;as_jobject());
 791         } else {
 792 #ifdef _LP64
</pre>
<hr />
<pre>
 841   if (info != NULL) {
 842     add_debug_info_for_null_check(null_check_here, info);
 843   }
 844 }
 845 
 846 
 847 void LIR_Assembler::reg2reg(LIR_Opr src, LIR_Opr dest) {
 848   assert(src-&gt;is_register(), &quot;should not call otherwise&quot;);
 849   assert(dest-&gt;is_register(), &quot;should not call otherwise&quot;);
 850 
 851   // move between cpu-registers
 852   if (dest-&gt;is_single_cpu()) {
 853 #ifdef _LP64
 854     if (src-&gt;type() == T_LONG) {
 855       // Can do LONG -&gt; OBJECT
 856       move_regs(src-&gt;as_register_lo(), dest-&gt;as_register());
 857       return;
 858     }
 859 #endif
 860     assert(src-&gt;is_single_cpu(), &quot;must match&quot;);
<span class="line-modified"> 861     if (src-&gt;type() == T_OBJECT || src-&gt;type() == T_VALUETYPE) {</span>
 862       __ verify_oop(src-&gt;as_register());
 863     }
 864     move_regs(src-&gt;as_register(), dest-&gt;as_register());
 865 
 866   } else if (dest-&gt;is_double_cpu()) {
 867 #ifdef _LP64
 868     if (is_reference_type(src-&gt;type())) {
 869       // Surprising to me but we can see move of a long to t_object
 870       __ verify_oop(src-&gt;as_register());
 871       move_regs(src-&gt;as_register(), dest-&gt;as_register_lo());
 872       return;
 873     }
 874 #endif
 875     assert(src-&gt;is_double_cpu(), &quot;must match&quot;);
 876     Register f_lo = src-&gt;as_register_lo();
 877     Register f_hi = src-&gt;as_register_hi();
 878     Register t_lo = dest-&gt;as_register_lo();
 879     Register t_hi = dest-&gt;as_register_hi();
 880 #ifdef _LP64
 881     assert(f_hi == f_lo, &quot;must be same&quot;);
</pre>
<hr />
<pre>
1027       break;
1028     }
1029 
1030     case T_DOUBLE: {
1031 #ifdef _LP64
1032       assert(src-&gt;is_double_xmm(), &quot;not a double&quot;);
1033       __ movdbl(as_Address(to_addr), src-&gt;as_xmm_double_reg());
1034 #else
1035       if (src-&gt;is_double_xmm()) {
1036         __ movdbl(as_Address(to_addr), src-&gt;as_xmm_double_reg());
1037       } else {
1038         assert(src-&gt;is_double_fpu(), &quot;must be&quot;);
1039         assert(src-&gt;fpu_regnrLo() == 0, &quot;argument must be on TOS&quot;);
1040         if (pop_fpu_stack)      __ fstp_d(as_Address(to_addr));
1041         else                    __ fst_d (as_Address(to_addr));
1042       }
1043 #endif // _LP64
1044       break;
1045     }
1046 
<span class="line-added">1047     case T_VALUETYPE: // fall through</span>
1048     case T_ARRAY:   // fall through
1049     case T_OBJECT:  // fall through
1050       if (UseCompressedOops &amp;&amp; !wide) {
1051         __ movl(as_Address(to_addr), compressed_src);
1052       } else {
1053         __ movptr(as_Address(to_addr), src-&gt;as_register());
1054       }
1055       break;
1056     case T_METADATA:
1057       // We get here to store a method pointer to the stack to pass to
1058       // a dtrace runtime call. This can&#39;t work on 64 bit with
1059       // compressed klass ptrs: T_METADATA can be a compressed klass
1060       // ptr or a 64 bit method pointer.
1061       LP64_ONLY(ShouldNotReachHere());
1062       __ movptr(as_Address(to_addr), src-&gt;as_register());
1063       break;
1064     case T_ADDRESS:
1065       __ movptr(as_Address(to_addr), src-&gt;as_register());
1066       break;
1067     case T_INT:
</pre>
<hr />
<pre>
1200     __ pushl(frame_map()-&gt;address_for_slot(src -&gt;double_stack_ix(), 0));
1201     // push and pop the part at src + wordSize, adding wordSize for the previous push
1202     __ pushl(frame_map()-&gt;address_for_slot(src -&gt;double_stack_ix(), 2 * wordSize));
1203     __ popl (frame_map()-&gt;address_for_slot(dest-&gt;double_stack_ix(), 2 * wordSize));
1204     __ popl (frame_map()-&gt;address_for_slot(dest-&gt;double_stack_ix(), 0));
1205 #endif // _LP64
1206 
1207   } else {
1208     ShouldNotReachHere();
1209   }
1210 }
1211 
1212 
1213 void LIR_Assembler::mem2reg(LIR_Opr src, LIR_Opr dest, BasicType type, LIR_PatchCode patch_code, CodeEmitInfo* info, bool wide, bool /* unaligned */) {
1214   assert(src-&gt;is_address(), &quot;should not call otherwise&quot;);
1215   assert(dest-&gt;is_register(), &quot;should not call otherwise&quot;);
1216 
1217   LIR_Address* addr = src-&gt;as_address_ptr();
1218   Address from_addr = as_Address(addr);
1219 
<span class="line-modified">1220   if (addr-&gt;base()-&gt;type() == T_OBJECT || addr-&gt;base()-&gt;type() == T_VALUETYPE) {</span>
1221     __ verify_oop(addr-&gt;base()-&gt;as_pointer_register());
1222   }
1223 
1224   switch (type) {
1225     case T_BOOLEAN: // fall through
1226     case T_BYTE:    // fall through
1227     case T_CHAR:    // fall through
1228     case T_SHORT:
1229       if (!VM_Version::is_P6() &amp;&amp; !from_addr.uses(dest-&gt;as_register())) {
1230         // on pre P6 processors we may get partial register stalls
1231         // so blow away the value of to_rinfo before loading a
1232         // partial word into it.  Do it here so that it precedes
1233         // the potential patch point below.
1234         __ xorptr(dest-&gt;as_register(), dest-&gt;as_register());
1235       }
1236       break;
1237    default:
1238      break;
1239   }
1240 
</pre>
<hr />
<pre>
1261 #endif // !LP64
1262       }
1263       break;
1264     }
1265 
1266     case T_DOUBLE: {
1267       if (dest-&gt;is_double_xmm()) {
1268         __ movdbl(dest-&gt;as_xmm_double_reg(), from_addr);
1269       } else {
1270 #ifndef _LP64
1271         assert(dest-&gt;is_double_fpu(), &quot;must be&quot;);
1272         assert(dest-&gt;fpu_regnrLo() == 0, &quot;dest must be TOS&quot;);
1273         __ fld_d(from_addr);
1274 #else
1275         ShouldNotReachHere();
1276 #endif // !LP64
1277       }
1278       break;
1279     }
1280 
<span class="line-added">1281     case T_VALUETYPE: // fall through</span>
1282     case T_OBJECT:  // fall through
1283     case T_ARRAY:   // fall through
1284       if (UseCompressedOops &amp;&amp; !wide) {
1285         __ movl(dest-&gt;as_register(), from_addr);
1286       } else {
1287         __ movptr(dest-&gt;as_register(), from_addr);
1288       }
1289       break;
1290 
1291     case T_ADDRESS:
1292       if (UseCompressedClassPointers &amp;&amp; addr-&gt;disp() == oopDesc::klass_offset_in_bytes()) {
1293         __ movl(dest-&gt;as_register(), from_addr);
1294       } else {
1295         __ movptr(dest-&gt;as_register(), from_addr);
1296       }
1297       break;
1298     case T_INT:
1299       __ movl(dest-&gt;as_register(), from_addr);
1300       break;
1301 
</pre>
<hr />
<pre>
1385   }
1386 
1387   if (patch != NULL) {
1388     patching_epilog(patch, patch_code, addr-&gt;base()-&gt;as_register(), info);
1389   }
1390 
1391   if (is_reference_type(type)) {
1392 #ifdef _LP64
1393     if (UseCompressedOops &amp;&amp; !wide) {
1394       __ decode_heap_oop(dest-&gt;as_register());
1395     }
1396 #endif
1397 
1398     // Load barrier has not yet been applied, so ZGC can&#39;t verify the oop here
1399     if (!UseZGC) {
1400       __ verify_oop(dest-&gt;as_register());
1401     }
1402   } else if (type == T_ADDRESS &amp;&amp; addr-&gt;disp() == oopDesc::klass_offset_in_bytes()) {
1403 #ifdef _LP64
1404     if (UseCompressedClassPointers) {
<span class="line-added">1405       __ andl(dest-&gt;as_register(), oopDesc::compressed_klass_mask());</span>
1406       __ decode_klass_not_null(dest-&gt;as_register());
<span class="line-added">1407     } else {</span>
<span class="line-added">1408       __ shlq(dest-&gt;as_register(), oopDesc::storage_props_nof_bits);</span>
<span class="line-added">1409       __ shrq(dest-&gt;as_register(), oopDesc::storage_props_nof_bits);</span>
1410     }
<span class="line-added">1411 #else</span>
<span class="line-added">1412     __ andl(dest-&gt;as_register(), oopDesc::wide_klass_mask());</span>
1413 #endif
1414   }
1415 }
1416 
1417 
1418 NEEDS_CLEANUP; // This could be static?
1419 Address::ScaleFactor LIR_Assembler::array_element_size(BasicType type) const {
1420   int elem_size = type2aelembytes(type);
1421   switch (elem_size) {
1422     case 1: return Address::times_1;
1423     case 2: return Address::times_2;
1424     case 4: return Address::times_4;
1425     case 8: return Address::times_8;
1426   }
1427   ShouldNotReachHere();
1428   return Address::no_scale;
1429 }
1430 
1431 
1432 void LIR_Assembler::emit_op3(LIR_Op3* op) {
</pre>
<hr />
<pre>
1654     add_debug_info_for_null_check_here(op-&gt;stub()-&gt;info());
1655     __ cmpb(Address(op-&gt;klass()-&gt;as_register(),
1656                     InstanceKlass::init_state_offset()),
1657                     InstanceKlass::fully_initialized);
1658     __ jcc(Assembler::notEqual, *op-&gt;stub()-&gt;entry());
1659   }
1660   __ allocate_object(op-&gt;obj()-&gt;as_register(),
1661                      op-&gt;tmp1()-&gt;as_register(),
1662                      op-&gt;tmp2()-&gt;as_register(),
1663                      op-&gt;header_size(),
1664                      op-&gt;object_size(),
1665                      op-&gt;klass()-&gt;as_register(),
1666                      *op-&gt;stub()-&gt;entry());
1667   __ bind(*op-&gt;stub()-&gt;continuation());
1668 }
1669 
1670 void LIR_Assembler::emit_alloc_array(LIR_OpAllocArray* op) {
1671   Register len =  op-&gt;len()-&gt;as_register();
1672   LP64_ONLY( __ movslq(len, len); )
1673 
<span class="line-modified">1674   if (UseSlowPath || op-&gt;type() == T_VALUETYPE ||</span>
1675       (!UseFastNewObjectArray &amp;&amp; is_reference_type(op-&gt;type())) ||
1676       (!UseFastNewTypeArray   &amp;&amp; !is_reference_type(op-&gt;type()))) {
1677     __ jmp(*op-&gt;stub()-&gt;entry());
1678   } else {
1679     Register tmp1 = op-&gt;tmp1()-&gt;as_register();
1680     Register tmp2 = op-&gt;tmp2()-&gt;as_register();
1681     Register tmp3 = op-&gt;tmp3()-&gt;as_register();
1682     if (len == tmp1) {
1683       tmp1 = tmp3;
1684     } else if (len == tmp2) {
1685       tmp2 = tmp3;
1686     } else if (len == tmp3) {
1687       // everything is ok
1688     } else {
1689       __ mov(tmp3, len);
1690     }
1691     __ allocate_array(op-&gt;obj()-&gt;as_register(),
1692                       len,
1693                       tmp1,
1694                       tmp2,
</pre>
<hr />
<pre>
1752     assert(data-&gt;is_ReceiverTypeData(), &quot;need ReceiverTypeData for type check&quot;);
1753   }
1754   Label profile_cast_success, profile_cast_failure;
1755   Label *success_target = op-&gt;should_profile() ? &amp;profile_cast_success : success;
1756   Label *failure_target = op-&gt;should_profile() ? &amp;profile_cast_failure : failure;
1757 
1758   if (obj == k_RInfo) {
1759     k_RInfo = dst;
1760   } else if (obj == klass_RInfo) {
1761     klass_RInfo = dst;
1762   }
1763   if (k-&gt;is_loaded() &amp;&amp; !UseCompressedClassPointers) {
1764     select_different_registers(obj, dst, k_RInfo, klass_RInfo);
1765   } else {
1766     Rtmp1 = op-&gt;tmp3()-&gt;as_register();
1767     select_different_registers(obj, dst, k_RInfo, klass_RInfo, Rtmp1);
1768   }
1769 
1770   assert_different_registers(obj, k_RInfo, klass_RInfo);
1771 
<span class="line-modified">1772   if (op-&gt;need_null_check()) {</span>
<span class="line-modified">1773     __ cmpptr(obj, (int32_t)NULL_WORD);</span>
<span class="line-modified">1774     if (op-&gt;should_profile()) {</span>
<span class="line-modified">1775       Label not_null;</span>
<span class="line-modified">1776       __ jccb(Assembler::notEqual, not_null);</span>
<span class="line-modified">1777       // Object is null; update MDO and exit</span>
<span class="line-modified">1778       Register mdo  = klass_RInfo;</span>
<span class="line-modified">1779       __ mov_metadata(mdo, md-&gt;constant_encoding());</span>
<span class="line-modified">1780       Address data_addr(mdo, md-&gt;byte_offset_of_slot(data, DataLayout::flags_offset()));</span>
<span class="line-modified">1781       int header_bits = BitData::null_seen_byte_constant();</span>
<span class="line-modified">1782       __ orb(data_addr, header_bits);</span>
<span class="line-modified">1783       __ jmp(*obj_is_null);</span>
<span class="line-modified">1784       __ bind(not_null);</span>
<span class="line-modified">1785     } else {</span>
<span class="line-added">1786       __ jcc(Assembler::equal, *obj_is_null);</span>
<span class="line-added">1787     }</span>
1788   }
1789 
1790   if (!k-&gt;is_loaded()) {
1791     klass2reg_with_patching(k_RInfo, op-&gt;info_for_patch());
1792   } else {
1793 #ifdef _LP64
1794     __ mov_metadata(k_RInfo, k-&gt;constant_encoding());
1795 #endif // _LP64
1796   }
1797   __ verify_oop(obj);
1798 
1799   if (op-&gt;fast_check()) {
1800     // get object class
1801     // not a safepoint as obj null check happens earlier
1802 #ifdef _LP64
1803     if (UseCompressedClassPointers) {
1804       __ load_klass(Rtmp1, obj);
1805       __ cmpptr(k_RInfo, Rtmp1);
1806     } else {
1807       __ cmpptr(k_RInfo, Address(obj, oopDesc::klass_offset_in_bytes()));
</pre>
<hr />
<pre>
1978         __ mov(dst, obj);
1979       }
1980     } else
1981       if (code == lir_instanceof) {
1982         Register obj = op-&gt;object()-&gt;as_register();
1983         Register dst = op-&gt;result_opr()-&gt;as_register();
1984         Label success, failure, done;
1985         emit_typecheck_helper(op, &amp;success, &amp;failure, &amp;failure);
1986         __ bind(failure);
1987         __ xorptr(dst, dst);
1988         __ jmpb(done);
1989         __ bind(success);
1990         __ movptr(dst, 1);
1991         __ bind(done);
1992       } else {
1993         ShouldNotReachHere();
1994       }
1995 
1996 }
1997 
<span class="line-added">1998 void LIR_Assembler::emit_opFlattenedArrayCheck(LIR_OpFlattenedArrayCheck* op) {</span>
<span class="line-added">1999   // We are loading/storing an array that *may* be a flattened array (the declared type</span>
<span class="line-added">2000   // Object[], interface[], or VT?[]). If this array is flattened, take slow path.</span>
<span class="line-added">2001 </span>
<span class="line-added">2002   __ load_storage_props(op-&gt;tmp()-&gt;as_register(), op-&gt;array()-&gt;as_register());</span>
<span class="line-added">2003   __ testb(op-&gt;tmp()-&gt;as_register(), ArrayStorageProperties::flattened_value);</span>
<span class="line-added">2004   __ jcc(Assembler::notZero, *op-&gt;stub()-&gt;entry());</span>
<span class="line-added">2005   if (!op-&gt;value()-&gt;is_illegal()) {</span>
<span class="line-added">2006     // We are storing into the array.</span>
<span class="line-added">2007     Label skip;</span>
<span class="line-added">2008     __ testb(op-&gt;tmp()-&gt;as_register(), ArrayStorageProperties::null_free_value);</span>
<span class="line-added">2009     __ jcc(Assembler::zero, skip);</span>
<span class="line-added">2010     // The array is not flattened, but it is null_free. If we are storing</span>
<span class="line-added">2011     // a null, take the slow path (which will throw NPE).</span>
<span class="line-added">2012     __ cmpptr(op-&gt;value()-&gt;as_register(), (int32_t)NULL_WORD);</span>
<span class="line-added">2013     __ jcc(Assembler::zero, *op-&gt;stub()-&gt;entry());</span>
<span class="line-added">2014     __ bind(skip);</span>
<span class="line-added">2015   }</span>
<span class="line-added">2016 }</span>
<span class="line-added">2017 </span>
<span class="line-added">2018 void LIR_Assembler::emit_opNullFreeArrayCheck(LIR_OpNullFreeArrayCheck* op) {</span>
<span class="line-added">2019   // This is called when we use aastore into a an array declared as &quot;[LVT;&quot;,</span>
<span class="line-added">2020   // where we know VT is not flattenable (due to ValueArrayElemMaxFlatOops, etc).</span>
<span class="line-added">2021   // However, we need to do a NULL check if the actual array is a &quot;[QVT;&quot;.</span>
<span class="line-added">2022 </span>
<span class="line-added">2023   __ load_storage_props(op-&gt;tmp()-&gt;as_register(), op-&gt;array()-&gt;as_register());</span>
<span class="line-added">2024   __ testb(op-&gt;tmp()-&gt;as_register(), ArrayStorageProperties::null_free_value);</span>
<span class="line-added">2025 }</span>
<span class="line-added">2026 </span>
<span class="line-added">2027 void LIR_Assembler::emit_opSubstitutabilityCheck(LIR_OpSubstitutabilityCheck* op) {</span>
<span class="line-added">2028   Label L_oops_equal;</span>
<span class="line-added">2029   Label L_oops_not_equal;</span>
<span class="line-added">2030   Label L_end;</span>
<span class="line-added">2031 </span>
<span class="line-added">2032   Register left  = op-&gt;left()-&gt;as_register();</span>
<span class="line-added">2033   Register right = op-&gt;right()-&gt;as_register();</span>
<span class="line-added">2034 </span>
<span class="line-added">2035   __ cmpptr(left, right);</span>
<span class="line-added">2036   __ jcc(Assembler::equal, L_oops_equal);</span>
<span class="line-added">2037 </span>
<span class="line-added">2038   // (1) Null check -- if one of the operands is null, the other must not be null (because</span>
<span class="line-added">2039   //     the two references are not equal), so they are not substitutable,</span>
<span class="line-added">2040   //     FIXME: do null check only if the operand is nullable</span>
<span class="line-added">2041   {</span>
<span class="line-added">2042     __ cmpptr(left, (int32_t)NULL_WORD);</span>
<span class="line-added">2043     __ jcc(Assembler::equal, L_oops_not_equal);</span>
<span class="line-added">2044 </span>
<span class="line-added">2045     __ cmpptr(right, (int32_t)NULL_WORD);</span>
<span class="line-added">2046     __ jcc(Assembler::equal, L_oops_not_equal);</span>
<span class="line-added">2047   }</span>
<span class="line-added">2048 </span>
<span class="line-added">2049   ciKlass* left_klass = op-&gt;left_klass();</span>
<span class="line-added">2050   ciKlass* right_klass = op-&gt;right_klass();</span>
<span class="line-added">2051 </span>
<span class="line-added">2052   // (2) Value object check -- if either of the operands is not a value object,</span>
<span class="line-added">2053   //     they are not substitutable. We do this only if we are not sure that the</span>
<span class="line-added">2054   //     operands are value objects</span>
<span class="line-added">2055   if ((left_klass == NULL || right_klass == NULL) ||// The klass is still unloaded, or came from a Phi node.</span>
<span class="line-added">2056       !left_klass-&gt;is_valuetype() || !right_klass-&gt;is_valuetype()) {</span>
<span class="line-added">2057     Register tmp1  = op-&gt;tmp1()-&gt;as_register();</span>
<span class="line-added">2058     __ movptr(tmp1, (intptr_t)markWord::always_locked_pattern);</span>
<span class="line-added">2059     __ andl(tmp1, Address(left, oopDesc::mark_offset_in_bytes()));</span>
<span class="line-added">2060     __ andl(tmp1, Address(right, oopDesc::mark_offset_in_bytes()));</span>
<span class="line-added">2061     __ cmpptr(tmp1, (intptr_t)markWord::always_locked_pattern);</span>
<span class="line-added">2062     __ jcc(Assembler::notEqual, L_oops_not_equal);</span>
<span class="line-added">2063   }</span>
<span class="line-added">2064 </span>
<span class="line-added">2065   // (3) Same klass check: if the operands are of different klasses, they are not substitutable.</span>
<span class="line-added">2066   if (left_klass != NULL &amp;&amp; left_klass-&gt;is_valuetype() &amp;&amp; left_klass == right_klass) {</span>
<span class="line-added">2067     // No need to load klass -- the operands are statically known to be the same value klass.</span>
<span class="line-added">2068     __ jmp(*op-&gt;stub()-&gt;entry());</span>
<span class="line-added">2069   } else {</span>
<span class="line-added">2070     Register left_klass_op = op-&gt;left_klass_op()-&gt;as_register();</span>
<span class="line-added">2071     Register right_klass_op = op-&gt;right_klass_op()-&gt;as_register();</span>
<span class="line-added">2072 </span>
<span class="line-added">2073     if (UseCompressedOops) {</span>
<span class="line-added">2074       __ movl(left_klass_op,  Address(left,  oopDesc::klass_offset_in_bytes()));</span>
<span class="line-added">2075       __ movl(right_klass_op, Address(right, oopDesc::klass_offset_in_bytes()));</span>
<span class="line-added">2076       __ cmpl(left_klass_op, right_klass_op);</span>
<span class="line-added">2077     } else {</span>
<span class="line-added">2078       __ movptr(left_klass_op,  Address(left,  oopDesc::klass_offset_in_bytes()));</span>
<span class="line-added">2079       __ movptr(right_klass_op, Address(right, oopDesc::klass_offset_in_bytes()));</span>
<span class="line-added">2080       __ cmpptr(left_klass_op, right_klass_op);</span>
<span class="line-added">2081     }</span>
<span class="line-added">2082 </span>
<span class="line-added">2083     __ jcc(Assembler::equal, *op-&gt;stub()-&gt;entry()); // same klass -&gt; do slow check</span>
<span class="line-added">2084     // fall through to L_oops_not_equal</span>
<span class="line-added">2085   }</span>
<span class="line-added">2086 </span>
<span class="line-added">2087   __ bind(L_oops_not_equal);</span>
<span class="line-added">2088   move(op-&gt;not_equal_result(), op-&gt;result_opr());</span>
<span class="line-added">2089   __ jmp(L_end);</span>
<span class="line-added">2090 </span>
<span class="line-added">2091   __ bind(L_oops_equal);</span>
<span class="line-added">2092   move(op-&gt;equal_result(), op-&gt;result_opr());</span>
<span class="line-added">2093   __ jmp(L_end);</span>
<span class="line-added">2094 </span>
<span class="line-added">2095   // We&#39;ve returned from the stub. RAX contains 0x0 IFF the two</span>
<span class="line-added">2096   // operands are not substitutable. (Don&#39;t compare against 0x1 in case the</span>
<span class="line-added">2097   // C compiler is naughty)</span>
<span class="line-added">2098   __ bind(*op-&gt;stub()-&gt;continuation());</span>
<span class="line-added">2099   __ cmpl(rax, 0);</span>
<span class="line-added">2100   __ jcc(Assembler::equal, L_oops_not_equal); // (call_stub() == 0x0) -&gt; not_equal</span>
<span class="line-added">2101   move(op-&gt;equal_result(), op-&gt;result_opr()); // (call_stub() != 0x0) -&gt; equal</span>
<span class="line-added">2102   // fall-through</span>
<span class="line-added">2103   __ bind(L_end);</span>
<span class="line-added">2104 }</span>
2105 
2106 void LIR_Assembler::emit_compare_and_swap(LIR_OpCompareAndSwap* op) {
2107   if (LP64_ONLY(false &amp;&amp;) op-&gt;code() == lir_cas_long &amp;&amp; VM_Version::supports_cx8()) {
2108     assert(op-&gt;cmp_value()-&gt;as_register_lo() == rax, &quot;wrong register&quot;);
2109     assert(op-&gt;cmp_value()-&gt;as_register_hi() == rdx, &quot;wrong register&quot;);
2110     assert(op-&gt;new_value()-&gt;as_register_lo() == rbx, &quot;wrong register&quot;);
2111     assert(op-&gt;new_value()-&gt;as_register_hi() == rcx, &quot;wrong register&quot;);
2112     Register addr = op-&gt;addr()-&gt;as_register();
2113     __ lock();
2114     NOT_LP64(__ cmpxchg8(Address(addr, 0)));
2115 
2116   } else if (op-&gt;code() == lir_cas_int || op-&gt;code() == lir_cas_obj ) {
2117     NOT_LP64(assert(op-&gt;addr()-&gt;is_single_cpu(), &quot;must be single&quot;);)
2118     Register addr = (op-&gt;addr()-&gt;is_single_cpu() ? op-&gt;addr()-&gt;as_register() : op-&gt;addr()-&gt;as_register_lo());
2119     Register newval = op-&gt;new_value()-&gt;as_register();
2120     Register cmpval = op-&gt;cmp_value()-&gt;as_register();
2121     assert(cmpval == rax, &quot;wrong register&quot;);
2122     assert(newval != NULL, &quot;new val must be register&quot;);
2123     assert(cmpval != newval, &quot;cmp and new values must be in different registers&quot;);
2124     assert(cmpval != addr, &quot;cmp and addr must be in different registers&quot;);
</pre>
<hr />
<pre>
2145       __ cmpxchgl(newval, Address(addr, 0));
2146     }
2147 #ifdef _LP64
2148   } else if (op-&gt;code() == lir_cas_long) {
2149     Register addr = (op-&gt;addr()-&gt;is_single_cpu() ? op-&gt;addr()-&gt;as_register() : op-&gt;addr()-&gt;as_register_lo());
2150     Register newval = op-&gt;new_value()-&gt;as_register_lo();
2151     Register cmpval = op-&gt;cmp_value()-&gt;as_register_lo();
2152     assert(cmpval == rax, &quot;wrong register&quot;);
2153     assert(newval != NULL, &quot;new val must be register&quot;);
2154     assert(cmpval != newval, &quot;cmp and new values must be in different registers&quot;);
2155     assert(cmpval != addr, &quot;cmp and addr must be in different registers&quot;);
2156     assert(newval != addr, &quot;new value and addr must be in different registers&quot;);
2157     __ lock();
2158     __ cmpxchgq(newval, Address(addr, 0));
2159 #endif // _LP64
2160   } else {
2161     Unimplemented();
2162   }
2163 }
2164 
<span class="line-added">2165 void LIR_Assembler::move(LIR_Opr src, LIR_Opr dst) {</span>
<span class="line-added">2166   assert(dst-&gt;is_cpu_register(), &quot;must be&quot;);</span>
<span class="line-added">2167   assert(dst-&gt;type() == src-&gt;type(), &quot;must be&quot;);</span>
<span class="line-added">2168 </span>
<span class="line-added">2169   if (src-&gt;is_cpu_register()) {</span>
<span class="line-added">2170     reg2reg(src, dst);</span>
<span class="line-added">2171   } else if (src-&gt;is_stack()) {</span>
<span class="line-added">2172     stack2reg(src, dst, dst-&gt;type());</span>
<span class="line-added">2173   } else if (src-&gt;is_constant()) {</span>
<span class="line-added">2174     const2reg(src, dst, lir_patch_none, NULL);</span>
<span class="line-added">2175   } else {</span>
<span class="line-added">2176     ShouldNotReachHere();</span>
<span class="line-added">2177   }</span>
<span class="line-added">2178 }</span>
<span class="line-added">2179 </span>
2180 void LIR_Assembler::cmove(LIR_Condition condition, LIR_Opr opr1, LIR_Opr opr2, LIR_Opr result, BasicType type) {
2181   Assembler::Condition acond, ncond;
2182   switch (condition) {
2183     case lir_cond_equal:        acond = Assembler::equal;        ncond = Assembler::notEqual;     break;
2184     case lir_cond_notEqual:     acond = Assembler::notEqual;     ncond = Assembler::equal;        break;
2185     case lir_cond_less:         acond = Assembler::less;         ncond = Assembler::greaterEqual; break;
2186     case lir_cond_lessEqual:    acond = Assembler::lessEqual;    ncond = Assembler::greater;      break;
2187     case lir_cond_greaterEqual: acond = Assembler::greaterEqual; ncond = Assembler::less;         break;
2188     case lir_cond_greater:      acond = Assembler::greater;      ncond = Assembler::lessEqual;    break;
2189     case lir_cond_belowEqual:   acond = Assembler::belowEqual;   ncond = Assembler::above;        break;
2190     case lir_cond_aboveEqual:   acond = Assembler::aboveEqual;   ncond = Assembler::below;        break;
2191     default:                    acond = Assembler::equal;        ncond = Assembler::notEqual;
2192                                 ShouldNotReachHere();
2193   }
2194 
2195   if (opr1-&gt;is_cpu_register()) {
2196     reg2reg(opr1, result);
2197   } else if (opr1-&gt;is_stack()) {
2198     stack2reg(opr1, result, result-&gt;type());
2199   } else if (opr1-&gt;is_constant()) {
</pre>
<hr />
<pre>
3039   switch (code) {
3040   case lir_static_call:
3041   case lir_optvirtual_call:
3042   case lir_dynamic_call:
3043     offset += NativeCall::displacement_offset;
3044     break;
3045   case lir_icvirtual_call:
3046     offset += NativeCall::displacement_offset + NativeMovConstReg::instruction_size;
3047     break;
3048   case lir_virtual_call:  // currently, sparc-specific for niagara
3049   default: ShouldNotReachHere();
3050   }
3051   __ align(BytesPerWord, offset);
3052 }
3053 
3054 
3055 void LIR_Assembler::call(LIR_OpJavaCall* op, relocInfo::relocType rtype) {
3056   assert((__ offset() + NativeCall::displacement_offset) % BytesPerWord == 0,
3057          &quot;must be aligned&quot;);
3058   __ call(AddressLiteral(op-&gt;addr(), rtype));
<span class="line-modified">3059   add_call_info(code_offset(), op-&gt;info(), op-&gt;maybe_return_as_fields());</span>
3060 }
3061 
3062 
3063 void LIR_Assembler::ic_call(LIR_OpJavaCall* op) {
3064   __ ic_call(op-&gt;addr());
<span class="line-modified">3065   add_call_info(code_offset(), op-&gt;info(), op-&gt;maybe_return_as_fields());</span>
3066   assert((__ offset() - NativeCall::instruction_size + NativeCall::displacement_offset) % BytesPerWord == 0,
3067          &quot;must be aligned&quot;);
3068 }
3069 
3070 
3071 /* Currently, vtable-dispatch is only enabled for sparc platforms */
3072 void LIR_Assembler::vtable_call(LIR_OpJavaCall* op) {
3073   ShouldNotReachHere();
3074 }
3075 
3076 
3077 void LIR_Assembler::emit_static_call_stub() {
3078   address call_pc = __ pc();
3079   address stub = __ start_a_stub(call_stub_size());
3080   if (stub == NULL) {
3081     bailout(&quot;static call stub overflow&quot;);
3082     return;
3083   }
3084 
3085   int start = __ offset();
</pre>
<hr />
<pre>
3241   __ movptr (Address(rsp, offset_from_rsp_in_bytes), c);
3242 }
3243 
3244 
3245 void LIR_Assembler::store_parameter(jobject o,  int offset_from_rsp_in_words) {
3246   assert(offset_from_rsp_in_words &gt;= 0, &quot;invalid offset from rsp&quot;);
3247   int offset_from_rsp_in_bytes = offset_from_rsp_in_words * BytesPerWord;
3248   assert(offset_from_rsp_in_bytes &lt; frame_map()-&gt;reserved_argument_area_size(), &quot;invalid offset&quot;);
3249   __ movoop (Address(rsp, offset_from_rsp_in_bytes), o);
3250 }
3251 
3252 
3253 void LIR_Assembler::store_parameter(Metadata* m,  int offset_from_rsp_in_words) {
3254   assert(offset_from_rsp_in_words &gt;= 0, &quot;invalid offset from rsp&quot;);
3255   int offset_from_rsp_in_bytes = offset_from_rsp_in_words * BytesPerWord;
3256   assert(offset_from_rsp_in_bytes &lt; frame_map()-&gt;reserved_argument_area_size(), &quot;invalid offset&quot;);
3257   __ mov_metadata(Address(rsp, offset_from_rsp_in_bytes), m);
3258 }
3259 
3260 
<span class="line-added">3261 void LIR_Assembler::arraycopy_valuetype_check(Register obj, Register tmp, CodeStub* slow_path, bool is_dest, bool null_check) {</span>
<span class="line-added">3262   if (null_check) {</span>
<span class="line-added">3263     __ testptr(obj, obj);</span>
<span class="line-added">3264     __ jcc(Assembler::zero, *slow_path-&gt;entry());</span>
<span class="line-added">3265   }</span>
<span class="line-added">3266   __ load_storage_props(tmp, obj);</span>
<span class="line-added">3267   if (is_dest) {</span>
<span class="line-added">3268     // We also take slow path if it&#39;s a null_free destination array, just in case the source array</span>
<span class="line-added">3269     // contains NULLs.</span>
<span class="line-added">3270     __ testb(tmp, ArrayStorageProperties::flattened_value | ArrayStorageProperties::null_free_value);</span>
<span class="line-added">3271   } else {</span>
<span class="line-added">3272     __ testb(tmp, ArrayStorageProperties::flattened_value);</span>
<span class="line-added">3273   }</span>
<span class="line-added">3274   __ jcc(Assembler::notEqual, *slow_path-&gt;entry());</span>
<span class="line-added">3275 }</span>
<span class="line-added">3276 </span>
<span class="line-added">3277 </span>
3278 // This code replaces a call to arraycopy; no exception may
3279 // be thrown in this code, they must be thrown in the System.arraycopy
3280 // activation frame; we could save some checks if this would not be the case
3281 void LIR_Assembler::emit_arraycopy(LIR_OpArrayCopy* op) {
3282   ciArrayKlass* default_type = op-&gt;expected_type();
3283   Register src = op-&gt;src()-&gt;as_register();
3284   Register dst = op-&gt;dst()-&gt;as_register();
3285   Register src_pos = op-&gt;src_pos()-&gt;as_register();
3286   Register dst_pos = op-&gt;dst_pos()-&gt;as_register();
3287   Register length  = op-&gt;length()-&gt;as_register();
3288   Register tmp = op-&gt;tmp()-&gt;as_register();
3289 
3290   __ resolve(ACCESS_READ, src);
3291   __ resolve(ACCESS_WRITE, dst);
3292 
3293   CodeStub* stub = op-&gt;stub();
3294   int flags = op-&gt;flags();
3295   BasicType basic_type = default_type != NULL ? default_type-&gt;element_type()-&gt;basic_type() : T_ILLEGAL;
3296   if (is_reference_type(basic_type)) basic_type = T_OBJECT;
3297 
<span class="line-added">3298   if (flags &amp; LIR_OpArrayCopy::always_slow_path) {</span>
<span class="line-added">3299     __ jmp(*stub-&gt;entry());</span>
<span class="line-added">3300     __ bind(*stub-&gt;continuation());</span>
<span class="line-added">3301     return;</span>
<span class="line-added">3302   }</span>
<span class="line-added">3303 </span>
<span class="line-added">3304   if (flags &amp; LIR_OpArrayCopy::src_valuetype_check) {</span>
<span class="line-added">3305     arraycopy_valuetype_check(src, tmp, stub, false, (flags &amp; LIR_OpArrayCopy::src_null_check));</span>
<span class="line-added">3306   }</span>
<span class="line-added">3307 </span>
<span class="line-added">3308   if (flags &amp; LIR_OpArrayCopy::dst_valuetype_check) {</span>
<span class="line-added">3309     arraycopy_valuetype_check(dst, tmp, stub, true, (flags &amp; LIR_OpArrayCopy::dst_null_check));</span>
<span class="line-added">3310   }</span>
<span class="line-added">3311 </span>
3312   // if we don&#39;t know anything, just go through the generic arraycopy
3313   if (default_type == NULL) {
3314     // save outgoing arguments on stack in case call to System.arraycopy is needed
3315     // HACK ALERT. This code used to push the parameters in a hardwired fashion
3316     // for interpreter calling conventions. Now we have to do it in new style conventions.
3317     // For the moment until C1 gets the new register allocator I just force all the
3318     // args to the right place (except the register args) and then on the back side
3319     // reload the register args properly if we go slow path. Yuck
3320 
3321     // These are proper for the calling convention
3322     store_parameter(length, 2);
3323     store_parameter(dst_pos, 1);
3324     store_parameter(dst, 0);
3325 
3326     // these are just temporary placements until we need to reload
3327     store_parameter(src_pos, 3);
3328     store_parameter(src, 4);
3329     NOT_LP64(assert(src == rcx &amp;&amp; src_pos == rdx, &quot;mismatch in calling convention&quot;);)
3330 
3331     address copyfunc_addr = StubRoutines::generic_arraycopy();
</pre>
<hr />
<pre>
4227 }
4228 
4229 void LIR_Assembler::membar_storeload() {
4230   __ membar(Assembler::Membar_mask_bits(Assembler::StoreLoad));
4231 }
4232 
4233 void LIR_Assembler::on_spin_wait() {
4234   __ pause ();
4235 }
4236 
4237 void LIR_Assembler::get_thread(LIR_Opr result_reg) {
4238   assert(result_reg-&gt;is_register(), &quot;check&quot;);
4239 #ifdef _LP64
4240   // __ get_thread(result_reg-&gt;as_register_lo());
4241   __ mov(result_reg-&gt;as_register(), r15_thread);
4242 #else
4243   __ get_thread(result_reg-&gt;as_register());
4244 #endif // _LP64
4245 }
4246 
<span class="line-added">4247 void LIR_Assembler::check_orig_pc() {</span>
<span class="line-added">4248   __ cmpptr(frame_map()-&gt;address_for_orig_pc_addr(), (int32_t)NULL_WORD);</span>
<span class="line-added">4249 }</span>
4250 
4251 void LIR_Assembler::peephole(LIR_List*) {
4252   // do nothing for now
4253 }
4254 
4255 void LIR_Assembler::atomic_op(LIR_Code code, LIR_Opr src, LIR_Opr data, LIR_Opr dest, LIR_Opr tmp) {
4256   assert(data == dest, &quot;xchg/xadd uses only 2 operands&quot;);
4257 
4258   if (data-&gt;type() == T_INT) {
4259     if (code == lir_xadd) {
4260       __ lock();
4261       __ xaddl(as_Address(src-&gt;as_address_ptr()), data-&gt;as_register());
4262     } else {
4263       __ xchgl(data-&gt;as_register(), as_Address(src-&gt;as_address_ptr()));
4264     }
4265   } else if (data-&gt;is_oop()) {
4266     assert (code == lir_xchg, &quot;xadd for oops&quot;);
4267     Register obj = data-&gt;as_register();
4268 #ifdef _LP64
4269     if (UseCompressedOops) {
</pre>
</td>
</tr>
</table>
<center><a href="../sparc/interp_masm_sparc.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="c1_LIRGenerator_x86.cpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>