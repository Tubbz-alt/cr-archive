diff a/.hgtags b/.hgtags
--- a/.hgtags
+++ b/.hgtags
@@ -642,5 +642,6 @@
 f143729ca00ec14a98ea5c7f73acba88da97746e jdk-15+23
 497fd9f9129c4928fd5a876dd55e0daf6298b511 jdk-15+24
 58833044988772ca06c97ab2f142474a8627af80 jdk-15+25
 58833044988772ca06c97ab2f142474a8627af80 jdk-15+25
 90b266a84c06f1b3dc0ed8767856793e8c1c357e jdk-15+25
+0a32396f7a690015d22ca3328ac441a358295d90 jdk-15+26
diff a/make/Docs.gmk b/make/Docs.gmk
--- a/make/Docs.gmk
+++ b/make/Docs.gmk
@@ -609,13 +609,13 @@
 
   # Create dynamic man pages from markdown using pandoc. We need
   # PANDOC_HTML_MANPAGE_FILTER, a wrapper around
   # PANDOC_HTML_MANPAGE_FILTER_JAVASCRIPT. This is created by buildtools-jdk.
 
-  # We should also depend on the source javascript filter
-  PANDOC_HTML_MANPAGE_FILTER_JAVASCRIPT := \
-      $(TOPDIR)/make/scripts/pandoc-html-manpage-filter.js
+  # We should also depend on the source code for the filter
+  PANDOC_HTML_MANPAGE_FILTER_SOURCE := $(call FindFiles, \
+      $(TOPDIR)/make/jdk/src/classes/build/tools/pandocfilter)
 
   $(foreach m, $(ALL_MODULES), \
     $(eval MAN_$m := $(call FindModuleManDirs, $m)) \
     $(foreach d, $(MAN_$m), \
       $(foreach f, $(call ApplySpecFilter, $(filter %.md, $(call FindFiles, $d))), \
@@ -631,11 +631,11 @@
 		@@VERSION_SHORT@@ => $(VERSION_SHORT) ; \
 		@@VERSION_SPECIFICATION@@ => $(VERSION_SPECIFICATION), \
             OPTIONS := --toc -V include-before='$(SPECS_TOP)' -V include-after='$(SPECS_BOTTOM_1)', \
             POST_PROCESS := $(TOOL_FIXUPPANDOC), \
             EXTRA_DEPS := $(PANDOC_HTML_MANPAGE_FILTER) \
-                $(PANDOC_HTML_MANPAGE_FILTER_JAVASCRIPT), \
+                $(PANDOC_HTML_MANPAGE_FILTER_SOURCE), \
         )) \
         $(eval JDK_SPECS_TARGETS += $($($m_$f_NAME))) \
       ) \
     ) \
   )
diff a/make/conf/jib-profiles.js b/make/conf/jib-profiles.js
--- a/make/conf/jib-profiles.js
+++ b/make/conf/jib-profiles.js
@@ -424,11 +424,14 @@
         "macosx-x64": {
             target_os: "macosx",
             target_cpu: "x64",
             dependencies: ["devkit", "gtest", "pandoc", "graalunit_lib"],
             configure_args: concat(common.configure_args_64bit, "--with-zlib=system",
-                "--with-macosx-version-max=10.9.0"),
+                "--with-macosx-version-max=10.9.0",
+                // Use system SetFile instead of the one in the devkit as the
+                // devkit one may not work on Catalina.
+                "SETFILE=/usr/bin/SetFile"),
         },
 
         "windows-x64": {
             target_os: "windows",
             target_cpu: "x64",
@@ -602,14 +605,10 @@
             // The prebuilt bootcycle variant modifies the boot jdk argument
             var bootcyclePrebuiltBase = {
                 dependencies: [ name + ".jdk" ],
                 configure_args: [
                     "--with-boot-jdk=" + input.get(name + ".jdk", "home_path"),
-                    // Full docs do not currently work with bootcycle build
-                    // since Nashorn was removed. This negates the
-                    // --enable-full-docs from the main profile.
-                    "--enable-full-docs=auto",
                 ]
             }
             profiles[bootcyclePrebuiltName] = concatObjects(profiles[name],
                 bootcyclePrebuiltBase);
             var bootJdkIndex = profiles[bootcyclePrebuiltName].dependencies.indexOf("boot_jdk");
@@ -686,11 +685,11 @@
             artifacts: {
                 doc_api_spec: {
                     local: "bundles/\\(jdk.*doc-api-spec.tar.gz\\)",
                     remote: [
                         "bundles/common/jdk-" + data.version + "_doc-api-spec.tar.gz",
-                        "bundles/linux-x64/\\1"
+                        "bundles/common/\\1"
                     ],
                 },
             }
         }
     };
@@ -763,11 +762,11 @@
                     = concat(profiles[cmpBaselineName].default_make_targets, "docs");
             }
             profiles[cmpBaselineName].make_args = [ "COMPARE_BUILD=CONF=" ];
             profiles[cmpBaselineName].configure_args = concat(
                 profiles[cmpBaselineName].configure_args,
-                "--with-hotspot-build-time=n/a", 
+                "--with-hotspot-build-time=n/a",
                 "--disable-precompiled-headers");
             // Do not inherit artifact definitions from base profile
             delete profiles[cmpBaselineName].artifacts;
         });
     });
@@ -1044,14 +1043,14 @@
         },
 
         jtreg: {
             server: "jpg",
             product: "jtreg",
-            version: "5.0",
+            version: "5.1",
             build_number: "b01",
             checksum_file: "MD5_VALUES",
-            file: "bundles/jtreg_bin-5.0.zip",
+            file: "bundles/jtreg_bin-5.1.zip",
             environment_name: "JT_HOME",
             environment_path: input.get("jtreg", "home_path") + "/bin",
             configure_args: "--with-jtreg=" + input.get("jtreg", "home_path"),
         },
 
diff a/make/data/jdwp/jdwp.spec b/make/data/jdwp/jdwp.spec
--- a/make/data/jdwp/jdwp.spec
+++ b/make/data/jdwp/jdwp.spec
@@ -391,15 +391,18 @@
             (boolean canGetMonitorInfo
                      "Can the VM get the monitor information for a given object? ")
             (boolean canRedefineClasses
                      "Can the VM redefine classes?")
             (boolean canAddMethod
-                     "Can the VM add methods when redefining "
-                     "classes?")
+                     "Can the VM add methods when redefining classes? "
+                     "<p>@Deprecated(since=\"15\") A JVM TI based JDWP back-end "
+                     "will never set this capability to true.")
             (boolean canUnrestrictedlyRedefineClasses
                      "Can the VM redefine classes "
-                     "in ways that are normally restricted?")
+                     "in ways that are normally restricted?"
+                     "<p>@Deprecated(since=\"15\") A JVM TI based JDWP back-end "
+                     "will never set this capability to true.")
             (boolean canPopFrames
                      "Can the VM pop stack frames?")
             (boolean canUseInstanceFilters
                      "Can the VM filter events by specific object?")
             (boolean canGetSourceDebugExtension
@@ -465,10 +468,11 @@
         "the redefinition must follow the restrictions described in "
         "<a href=\"../jvmti.html#RedefineClasses\">JVM TI RedefineClasses</a>."
         "<p>"
         "Requires canRedefineClasses capability - see "
         "<a href=\"#JDWP_VirtualMachine_CapabilitiesNew\">CapabilitiesNew</a>. "
+        "<p>@Deprecated(since=\"15\")  "
         "In addition to the canRedefineClasses capability, the target VM must "
         "have the canAddMethod capability to add methods when redefining classes, "
         "or the canUnrestrictedlyRedefineClasses capability to redefine classes in ways "
         "that are normally restricted."
         (Out
diff a/src/hotspot/cpu/aarch64/aarch64.ad b/src/hotspot/cpu/aarch64/aarch64.ad
--- a/src/hotspot/cpu/aarch64/aarch64.ad
+++ b/src/hotspot/cpu/aarch64/aarch64.ad
@@ -13193,10 +13193,44 @@
   %}
 
   ins_pipe(fp_uop_d);
 %}
 
+instruct absI_reg(iRegINoSp dst, iRegIorL2I src, rFlagsReg cr)
+%{
+  match(Set dst (AbsI src));
+
+  effect(KILL cr);
+  ins_cost(INSN_COST * 2);
+  format %{ "cmpw  $src, zr\n\t"
+            "cnegw $dst, $src, Assembler::LT\t# int abs"
+  %}
+
+  ins_encode %{
+    __ cmpw(as_Register($src$$reg), zr);
+    __ cnegw(as_Register($dst$$reg), as_Register($src$$reg), Assembler::LT);
+  %}
+  ins_pipe(pipe_class_default);
+%}
+
+instruct absL_reg(iRegLNoSp dst, iRegL src, rFlagsReg cr)
+%{
+  match(Set dst (AbsL src));
+
+  effect(KILL cr);
+  ins_cost(INSN_COST * 2);
+  format %{ "cmp  $src, zr\n\t"
+            "cneg $dst, $src, Assembler::LT\t# long abs"
+  %}
+
+  ins_encode %{
+    __ cmp(as_Register($src$$reg), zr);
+    __ cneg(as_Register($dst$$reg), as_Register($src$$reg), Assembler::LT);
+  %}
+  ins_pipe(pipe_class_default);
+%}
+
 instruct absF_reg(vRegF dst, vRegF src) %{
   match(Set dst (AbsF src));
 
   ins_cost(INSN_COST * 3);
   format %{ "fabss   $dst, $src" %}
@@ -17066,10 +17100,95 @@
   ins_pipe(vsqrt_fp128);
 %}
 
 // --------------------------------- ABS --------------------------------------
 
+instruct vabs8B(vecD dst, vecD src)
+%{
+  predicate(n->as_Vector()->length() == 4 ||
+            n->as_Vector()->length() == 8);
+  match(Set dst (AbsVB src));
+  ins_cost(INSN_COST);
+  format %{ "abs  $dst, $src\t# vector (8B)" %}
+  ins_encode %{
+    __ absr(as_FloatRegister($dst$$reg), __ T8B, as_FloatRegister($src$$reg));
+  %}
+  ins_pipe(vlogical64);
+%}
+
+instruct vabs16B(vecX dst, vecX src)
+%{
+  predicate(n->as_Vector()->length() == 16);
+  match(Set dst (AbsVB src));
+  ins_cost(INSN_COST);
+  format %{ "abs  $dst, $src\t# vector (16B)" %}
+  ins_encode %{
+    __ absr(as_FloatRegister($dst$$reg), __ T16B, as_FloatRegister($src$$reg));
+  %}
+  ins_pipe(vlogical128);
+%}
+
+instruct vabs4S(vecD dst, vecD src)
+%{
+  predicate(n->as_Vector()->length() == 4);
+  match(Set dst (AbsVS src));
+  ins_cost(INSN_COST);
+  format %{ "abs  $dst, $src\t# vector (4H)" %}
+  ins_encode %{
+    __ absr(as_FloatRegister($dst$$reg), __ T4H, as_FloatRegister($src$$reg));
+  %}
+  ins_pipe(vlogical64);
+%}
+
+instruct vabs8S(vecX dst, vecX src)
+%{
+  predicate(n->as_Vector()->length() == 8);
+  match(Set dst (AbsVS src));
+  ins_cost(INSN_COST);
+  format %{ "abs  $dst, $src\t# vector (8H)" %}
+  ins_encode %{
+    __ absr(as_FloatRegister($dst$$reg), __ T8H, as_FloatRegister($src$$reg));
+  %}
+  ins_pipe(vlogical128);
+%}
+
+instruct vabs2I(vecD dst, vecD src)
+%{
+  predicate(n->as_Vector()->length() == 2);
+  match(Set dst (AbsVI src));
+  ins_cost(INSN_COST);
+  format %{ "abs  $dst, $src\t# vector (2S)" %}
+  ins_encode %{
+    __ absr(as_FloatRegister($dst$$reg), __ T2S, as_FloatRegister($src$$reg));
+  %}
+  ins_pipe(vlogical64);
+%}
+
+instruct vabs4I(vecX dst, vecX src)
+%{
+  predicate(n->as_Vector()->length() == 4);
+  match(Set dst (AbsVI src));
+  ins_cost(INSN_COST);
+  format %{ "abs  $dst, $src\t# vector (4S)" %}
+  ins_encode %{
+    __ absr(as_FloatRegister($dst$$reg), __ T4S, as_FloatRegister($src$$reg));
+  %}
+  ins_pipe(vlogical128);
+%}
+
+instruct vabs2L(vecX dst, vecX src)
+%{
+  predicate(n->as_Vector()->length() == 2);
+  match(Set dst (AbsVL src));
+  ins_cost(INSN_COST);
+  format %{ "abs  $dst, $src\t# vector (2D)" %}
+  ins_encode %{
+    __ absr(as_FloatRegister($dst$$reg), __ T2D, as_FloatRegister($src$$reg));
+  %}
+  ins_pipe(vlogical128);
+%}
+
 instruct vabs2F(vecD dst, vecD src)
 %{
   predicate(n->as_Vector()->length() == 2);
   match(Set dst (AbsVF src));
   ins_cost(INSN_COST * 3);
diff a/src/hotspot/cpu/x86/macroAssembler_x86.cpp b/src/hotspot/cpu/x86/macroAssembler_x86.cpp
--- a/src/hotspot/cpu/x86/macroAssembler_x86.cpp
+++ b/src/hotspot/cpu/x86/macroAssembler_x86.cpp
@@ -7176,20 +7176,10 @@
   andl(val, 0xFF);
   shrl(crc, 8); // unsigned shift
   xorl(crc, Address(table, val, Address::times_4, 0));
 }
 
-/**
-* Fold four 128-bit data chunks
-*/
-void MacroAssembler::fold_128bit_crc32_avx512(XMMRegister xcrc, XMMRegister xK, XMMRegister xtmp, Register buf, int offset) {
-  evpclmulhdq(xtmp, xK, xcrc, Assembler::AVX_512bit); // [123:64]
-  evpclmulldq(xcrc, xK, xcrc, Assembler::AVX_512bit); // [63:0]
-  evpxorq(xcrc, xcrc, Address(buf, offset), Assembler::AVX_512bit /* vector_len */);
-  evpxorq(xcrc, xcrc, xtmp, Assembler::AVX_512bit /* vector_len */);
-}
-
 /**
  * Fold 128-bit data chunk
  */
 void MacroAssembler::fold_128bit_crc32(XMMRegister xcrc, XMMRegister xK, XMMRegister xtmp, Register buf, int offset) {
   if (UseAVX > 0) {
@@ -7389,10 +7379,376 @@
   BIND(L_exit);
   notl(crc); // ~c
 }
 
 #ifdef _LP64
+// Helper function for AVX 512 CRC32
+// Fold 512-bit data chunks
+void MacroAssembler::fold512bit_crc32_avx512(XMMRegister xcrc, XMMRegister xK, XMMRegister xtmp, Register buf,
+                                             Register pos, int offset) {
+  evmovdquq(xmm3, Address(buf, pos, Address::times_1, offset), Assembler::AVX_512bit);
+  evpclmulqdq(xtmp, xcrc, xK, 0x10, Assembler::AVX_512bit); // [123:64]
+  evpclmulqdq(xmm2, xcrc, xK, 0x01, Assembler::AVX_512bit); // [63:0]
+  evpxorq(xcrc, xtmp, xmm2, Assembler::AVX_512bit /* vector_len */);
+  evpxorq(xcrc, xcrc, xmm3, Assembler::AVX_512bit /* vector_len */);
+}
+
+// Helper function for AVX 512 CRC32
+// Compute CRC32 for < 256B buffers
+void MacroAssembler::kernel_crc32_avx512_256B(Register crc, Register buf, Register len, Register key, Register pos,
+                                              Register tmp1, Register tmp2, Label& L_barrett, Label& L_16B_reduction_loop,
+                                              Label& L_get_last_two_xmms, Label& L_128_done, Label& L_cleanup) {
+
+  Label L_less_than_32, L_exact_16_left, L_less_than_16_left;
+  Label L_less_than_8_left, L_less_than_4_left, L_less_than_2_left, L_zero_left;
+  Label L_only_less_than_4, L_only_less_than_3, L_only_less_than_2;
+
+  // check if there is enough buffer to be able to fold 16B at a time
+  cmpl(len, 32);
+  jcc(Assembler::less, L_less_than_32);
+
+  // if there is, load the constants
+  movdqu(xmm10, Address(key, 1 * 16));    //rk1 and rk2 in xmm10
+  movdl(xmm0, crc);                        // get the initial crc value
+  movdqu(xmm7, Address(buf, pos, Address::times_1, 0 * 16)); //load the plaintext
+  pxor(xmm7, xmm0);
+
+  // update the buffer pointer
+  addl(pos, 16);
+  //update the counter.subtract 32 instead of 16 to save one instruction from the loop
+  subl(len, 32);
+  jmp(L_16B_reduction_loop);
+
+  bind(L_less_than_32);
+  //mov initial crc to the return value. this is necessary for zero - length buffers.
+  movl(rax, crc);
+  testl(len, len);
+  jcc(Assembler::equal, L_cleanup);
+
+  movdl(xmm0, crc);                        //get the initial crc value
+
+  cmpl(len, 16);
+  jcc(Assembler::equal, L_exact_16_left);
+  jcc(Assembler::less, L_less_than_16_left);
+
+  movdqu(xmm7, Address(buf, pos, Address::times_1, 0 * 16)); //load the plaintext
+  pxor(xmm7, xmm0);                       //xor the initial crc value
+  addl(pos, 16);
+  subl(len, 16);
+  movdqu(xmm10, Address(key, 1 * 16));    // rk1 and rk2 in xmm10
+  jmp(L_get_last_two_xmms);
+
+  bind(L_less_than_16_left);
+  //use stack space to load data less than 16 bytes, zero - out the 16B in memory first.
+  pxor(xmm1, xmm1);
+  movptr(tmp1, rsp);
+  movdqu(Address(tmp1, 0 * 16), xmm1);
+
+  cmpl(len, 4);
+  jcc(Assembler::less, L_only_less_than_4);
+
+  //backup the counter value
+  movl(tmp2, len);
+  cmpl(len, 8);
+  jcc(Assembler::less, L_less_than_8_left);
+
+  //load 8 Bytes
+  movq(rax, Address(buf, pos, Address::times_1, 0 * 16));
+  movq(Address(tmp1, 0 * 16), rax);
+  addptr(tmp1, 8);
+  subl(len, 8);
+  addl(pos, 8);
+
+  bind(L_less_than_8_left);
+  cmpl(len, 4);
+  jcc(Assembler::less, L_less_than_4_left);
+
+  //load 4 Bytes
+  movl(rax, Address(buf, pos, Address::times_1, 0));
+  movl(Address(tmp1, 0 * 16), rax);
+  addptr(tmp1, 4);
+  subl(len, 4);
+  addl(pos, 4);
+
+  bind(L_less_than_4_left);
+  cmpl(len, 2);
+  jcc(Assembler::less, L_less_than_2_left);
+
+  // load 2 Bytes
+  movw(rax, Address(buf, pos, Address::times_1, 0));
+  movl(Address(tmp1, 0 * 16), rax);
+  addptr(tmp1, 2);
+  subl(len, 2);
+  addl(pos, 2);
+
+  bind(L_less_than_2_left);
+  cmpl(len, 1);
+  jcc(Assembler::less, L_zero_left);
+
+  // load 1 Byte
+  movb(rax, Address(buf, pos, Address::times_1, 0));
+  movb(Address(tmp1, 0 * 16), rax);
+
+  bind(L_zero_left);
+  movdqu(xmm7, Address(rsp, 0));
+  pxor(xmm7, xmm0);                       //xor the initial crc value
+
+  lea(rax, ExternalAddress(StubRoutines::x86::shuf_table_crc32_avx512_addr()));
+  movdqu(xmm0, Address(rax, tmp2));
+  pshufb(xmm7, xmm0);
+  jmp(L_128_done);
+
+  bind(L_exact_16_left);
+  movdqu(xmm7, Address(buf, pos, Address::times_1, 0));
+  pxor(xmm7, xmm0);                       //xor the initial crc value
+  jmp(L_128_done);
+
+  bind(L_only_less_than_4);
+  cmpl(len, 3);
+  jcc(Assembler::less, L_only_less_than_3);
+
+  // load 3 Bytes
+  movb(rax, Address(buf, pos, Address::times_1, 0));
+  movb(Address(tmp1, 0), rax);
+
+  movb(rax, Address(buf, pos, Address::times_1, 1));
+  movb(Address(tmp1, 1), rax);
+
+  movb(rax, Address(buf, pos, Address::times_1, 2));
+  movb(Address(tmp1, 2), rax);
+
+  movdqu(xmm7, Address(rsp, 0));
+  pxor(xmm7, xmm0);                     //xor the initial crc value
+
+  pslldq(xmm7, 0x5);
+  jmp(L_barrett);
+  bind(L_only_less_than_3);
+  cmpl(len, 2);
+  jcc(Assembler::less, L_only_less_than_2);
+
+  // load 2 Bytes
+  movb(rax, Address(buf, pos, Address::times_1, 0));
+  movb(Address(tmp1, 0), rax);
+
+  movb(rax, Address(buf, pos, Address::times_1, 1));
+  movb(Address(tmp1, 1), rax);
+
+  movdqu(xmm7, Address(rsp, 0));
+  pxor(xmm7, xmm0);                     //xor the initial crc value
+
+  pslldq(xmm7, 0x6);
+  jmp(L_barrett);
+
+  bind(L_only_less_than_2);
+  //load 1 Byte
+  movb(rax, Address(buf, pos, Address::times_1, 0));
+  movb(Address(tmp1, 0), rax);
+
+  movdqu(xmm7, Address(rsp, 0));
+  pxor(xmm7, xmm0);                     //xor the initial crc value
+
+  pslldq(xmm7, 0x7);
+}
+
+/**
+* Compute CRC32 using AVX512 instructions
+* param crc   register containing existing CRC (32-bit)
+* param buf   register pointing to input byte buffer (byte*)
+* param len   register containing number of bytes
+* param tmp1  scratch register
+* param tmp2  scratch register
+* return rax  result register
+*/
+void MacroAssembler::kernel_crc32_avx512(Register crc, Register buf, Register len, Register key, Register tmp1, Register tmp2) {
+  assert_different_registers(crc, buf, len, key, tmp1, tmp2, rax);
+
+  Label L_tail, L_tail_restore, L_tail_loop, L_exit, L_align_loop, L_aligned;
+  Label L_fold_tail, L_fold_128b, L_fold_512b, L_fold_512b_loop, L_fold_tail_loop;
+  Label L_less_than_256, L_fold_128_B_loop, L_fold_256_B_loop;
+  Label L_fold_128_B_register, L_final_reduction_for_128, L_16B_reduction_loop;
+  Label L_128_done, L_get_last_two_xmms, L_barrett, L_cleanup;
+
+  const Register pos = r12;
+  push(r12);
+  subptr(rsp, 16 * 2 + 8);
+
+  // For EVEX with VL and BW, provide a standard mask, VL = 128 will guide the merge
+  // context for the registers used, where all instructions below are using 128-bit mode
+  // On EVEX without VL and BW, these instructions will all be AVX.
+  lea(key, ExternalAddress(StubRoutines::x86::crc_table_avx512_addr()));
+  notl(crc);
+  movl(pos, 0);
+
+  // check if smaller than 256B
+  cmpl(len, 256);
+  jcc(Assembler::less, L_less_than_256);
+
+  // load the initial crc value
+  movdl(xmm10, crc);
+
+  // receive the initial 64B data, xor the initial crc value
+  evmovdquq(xmm0, Address(buf, pos, Address::times_1, 0 * 64), Assembler::AVX_512bit);
+  evmovdquq(xmm4, Address(buf, pos, Address::times_1, 1 * 64), Assembler::AVX_512bit);
+  evpxorq(xmm0, xmm0, xmm10, Assembler::AVX_512bit);
+  evbroadcasti32x4(xmm10, Address(key, 2 * 16), Assembler::AVX_512bit); //zmm10 has rk3 and rk4
+
+  subl(len, 256);
+  cmpl(len, 256);
+  jcc(Assembler::less, L_fold_128_B_loop);
+
+  evmovdquq(xmm7, Address(buf, pos, Address::times_1, 2 * 64), Assembler::AVX_512bit);
+  evmovdquq(xmm8, Address(buf, pos, Address::times_1, 3 * 64), Assembler::AVX_512bit);
+  evbroadcasti32x4(xmm16, Address(key, 0 * 16), Assembler::AVX_512bit); //zmm16 has rk-1 and rk-2
+  subl(len, 256);
+
+  bind(L_fold_256_B_loop);
+  addl(pos, 256);
+  fold512bit_crc32_avx512(xmm0, xmm16, xmm1, buf, pos, 0 * 64);
+  fold512bit_crc32_avx512(xmm4, xmm16, xmm1, buf, pos, 1 * 64);
+  fold512bit_crc32_avx512(xmm7, xmm16, xmm1, buf, pos, 2 * 64);
+  fold512bit_crc32_avx512(xmm8, xmm16, xmm1, buf, pos, 3 * 64);
+
+  subl(len, 256);
+  jcc(Assembler::greaterEqual, L_fold_256_B_loop);
+
+  // Fold 256 into 128
+  addl(pos, 256);
+  evpclmulqdq(xmm1, xmm0, xmm10, 0x01, Assembler::AVX_512bit);
+  evpclmulqdq(xmm2, xmm0, xmm10, 0x10, Assembler::AVX_512bit);
+  vpternlogq(xmm7, 0x96, xmm1, xmm2, Assembler::AVX_512bit); // xor ABC
+
+  evpclmulqdq(xmm5, xmm4, xmm10, 0x01, Assembler::AVX_512bit);
+  evpclmulqdq(xmm6, xmm4, xmm10, 0x10, Assembler::AVX_512bit);
+  vpternlogq(xmm8, 0x96, xmm5, xmm6, Assembler::AVX_512bit); // xor ABC
+
+  evmovdquq(xmm0, xmm7, Assembler::AVX_512bit);
+  evmovdquq(xmm4, xmm8, Assembler::AVX_512bit);
+
+  addl(len, 128);
+  jmp(L_fold_128_B_register);
+
+  // at this section of the code, there is 128 * x + y(0 <= y<128) bytes of buffer.The fold_128_B_loop
+  // loop will fold 128B at a time until we have 128 + y Bytes of buffer
+
+  // fold 128B at a time.This section of the code folds 8 xmm registers in parallel
+  bind(L_fold_128_B_loop);
+  addl(pos, 128);
+  fold512bit_crc32_avx512(xmm0, xmm10, xmm1, buf, pos, 0 * 64);
+  fold512bit_crc32_avx512(xmm4, xmm10, xmm1, buf, pos, 1 * 64);
+
+  subl(len, 128);
+  jcc(Assembler::greaterEqual, L_fold_128_B_loop);
+
+  addl(pos, 128);
+
+  // at this point, the buffer pointer is pointing at the last y Bytes of the buffer, where 0 <= y < 128
+  // the 128B of folded data is in 8 of the xmm registers : xmm0, xmm1, xmm2, xmm3, xmm4, xmm5, xmm6, xmm7
+  bind(L_fold_128_B_register);
+  evmovdquq(xmm16, Address(key, 5 * 16), Assembler::AVX_512bit); // multiply by rk9-rk16
+  evmovdquq(xmm11, Address(key, 9 * 16), Assembler::AVX_512bit); // multiply by rk17-rk20, rk1,rk2, 0,0
+  evpclmulqdq(xmm1, xmm0, xmm16, 0x01, Assembler::AVX_512bit);
+  evpclmulqdq(xmm2, xmm0, xmm16, 0x10, Assembler::AVX_512bit);
+  // save last that has no multiplicand
+  vextracti64x2(xmm7, xmm4, 3);
+
+  evpclmulqdq(xmm5, xmm4, xmm11, 0x01, Assembler::AVX_512bit);
+  evpclmulqdq(xmm6, xmm4, xmm11, 0x10, Assembler::AVX_512bit);
+  // Needed later in reduction loop
+  movdqu(xmm10, Address(key, 1 * 16));
+  vpternlogq(xmm1, 0x96, xmm2, xmm5, Assembler::AVX_512bit); // xor ABC
+  vpternlogq(xmm1, 0x96, xmm6, xmm7, Assembler::AVX_512bit); // xor ABC
+
+  // Swap 1,0,3,2 - 01 00 11 10
+  evshufi64x2(xmm8, xmm1, xmm1, 0x4e, Assembler::AVX_512bit);
+  evpxorq(xmm8, xmm8, xmm1, Assembler::AVX_256bit);
+  vextracti128(xmm5, xmm8, 1);
+  evpxorq(xmm7, xmm5, xmm8, Assembler::AVX_128bit);
+
+  // instead of 128, we add 128 - 16 to the loop counter to save 1 instruction from the loop
+  // instead of a cmp instruction, we use the negative flag with the jl instruction
+  addl(len, 128 - 16);
+  jcc(Assembler::less, L_final_reduction_for_128);
+
+  bind(L_16B_reduction_loop);
+  vpclmulqdq(xmm8, xmm7, xmm10, 0x1);
+  vpclmulqdq(xmm7, xmm7, xmm10, 0x10);
+  vpxor(xmm7, xmm7, xmm8, Assembler::AVX_128bit);
+  movdqu(xmm0, Address(buf, pos, Address::times_1, 0 * 16));
+  vpxor(xmm7, xmm7, xmm0, Assembler::AVX_128bit);
+  addl(pos, 16);
+  subl(len, 16);
+  jcc(Assembler::greaterEqual, L_16B_reduction_loop);
+
+  bind(L_final_reduction_for_128);
+  addl(len, 16);
+  jcc(Assembler::equal, L_128_done);
+
+  bind(L_get_last_two_xmms);
+  movdqu(xmm2, xmm7);
+  addl(pos, len);
+  movdqu(xmm1, Address(buf, pos, Address::times_1, -16));
+  subl(pos, len);
+
+  // get rid of the extra data that was loaded before
+  // load the shift constant
+  lea(rax, ExternalAddress(StubRoutines::x86::shuf_table_crc32_avx512_addr()));
+  movdqu(xmm0, Address(rax, len));
+  addl(rax, len);
+
+  vpshufb(xmm7, xmm7, xmm0, Assembler::AVX_128bit);
+  //Change mask to 512
+  vpxor(xmm0, xmm0, ExternalAddress(StubRoutines::x86::crc_by128_masks_avx512_addr() + 2 * 16), Assembler::AVX_128bit, tmp2);
+  vpshufb(xmm2, xmm2, xmm0, Assembler::AVX_128bit);
+
+  blendvpb(xmm2, xmm2, xmm1, xmm0, Assembler::AVX_128bit);
+  vpclmulqdq(xmm8, xmm7, xmm10, 0x1);
+  vpclmulqdq(xmm7, xmm7, xmm10, 0x10);
+  vpxor(xmm7, xmm7, xmm8, Assembler::AVX_128bit);
+  vpxor(xmm7, xmm7, xmm2, Assembler::AVX_128bit);
+
+  bind(L_128_done);
+  // compute crc of a 128-bit value
+  movdqu(xmm10, Address(key, 3 * 16));
+  movdqu(xmm0, xmm7);
+
+  // 64b fold
+  vpclmulqdq(xmm7, xmm7, xmm10, 0x0);
+  vpsrldq(xmm0, xmm0, 0x8, Assembler::AVX_128bit);
+  vpxor(xmm7, xmm7, xmm0, Assembler::AVX_128bit);
+
+  // 32b fold
+  movdqu(xmm0, xmm7);
+  vpslldq(xmm7, xmm7, 0x4, Assembler::AVX_128bit);
+  vpclmulqdq(xmm7, xmm7, xmm10, 0x10);
+  vpxor(xmm7, xmm7, xmm0, Assembler::AVX_128bit);
+  jmp(L_barrett);
+
+  bind(L_less_than_256);
+  kernel_crc32_avx512_256B(crc, buf, len, key, pos, tmp1, tmp2, L_barrett, L_16B_reduction_loop, L_get_last_two_xmms, L_128_done, L_cleanup);
+
+  //barrett reduction
+  bind(L_barrett);
+  vpand(xmm7, xmm7, ExternalAddress(StubRoutines::x86::crc_by128_masks_avx512_addr() + 1 * 16), Assembler::AVX_128bit, tmp2);
+  movdqu(xmm1, xmm7);
+  movdqu(xmm2, xmm7);
+  movdqu(xmm10, Address(key, 4 * 16));
+
+  pclmulqdq(xmm7, xmm10, 0x0);
+  pxor(xmm7, xmm2);
+  vpand(xmm7, xmm7, ExternalAddress(StubRoutines::x86::crc_by128_masks_avx512_addr()), Assembler::AVX_128bit, tmp2);
+  movdqu(xmm2, xmm7);
+  pclmulqdq(xmm7, xmm10, 0x10);
+  pxor(xmm7, xmm2);
+  pxor(xmm7, xmm1);
+  pextrd(crc, xmm7, 2);
+
+  bind(L_cleanup);
+  notl(crc); // ~c
+  addptr(rsp, 16 * 2 + 8);
+  pop(r12);
+}
+
 // S. Gueron / Information Processing Letters 112 (2012) 184
 // Algorithm 4: Computing carry-less multiplication using a precomputed lookup table.
 // Input: A 32 bit value B = [byte3, byte2, byte1, byte0].
 // Output: the 64-bit carry-less product of B * CONST
 void MacroAssembler::crc32c_ipl_alg4(Register in, uint32_t n,
diff a/src/hotspot/cpu/x86/macroAssembler_x86.hpp b/src/hotspot/cpu/x86/macroAssembler_x86.hpp
--- a/src/hotspot/cpu/x86/macroAssembler_x86.hpp
+++ b/src/hotspot/cpu/x86/macroAssembler_x86.hpp
@@ -1735,10 +1735,19 @@
 #endif
 
   // CRC32 code for java.util.zip.CRC32::updateBytes() intrinsic.
   void update_byte_crc32(Register crc, Register val, Register table);
   void kernel_crc32(Register crc, Register buf, Register len, Register table, Register tmp);
+
+
+#ifdef _LP64
+  void kernel_crc32_avx512(Register crc, Register buf, Register len, Register table, Register tmp1, Register tmp2);
+  void kernel_crc32_avx512_256B(Register crc, Register buf, Register len, Register key, Register pos,
+                                Register tmp1, Register tmp2, Label& L_barrett, Label& L_16B_reduction_loop,
+                                Label& L_get_last_two_xmms, Label& L_128_done, Label& L_cleanup);
+#endif // _LP64
+
   // CRC32C code for java.util.zip.CRC32C::updateBytes() intrinsic
   // Note on a naming convention:
   // Prefix w = register only used on a Westmere+ architecture
   // Prefix n = register only used on a Nehalem architecture
 #ifdef _LP64
@@ -1771,14 +1780,17 @@
                             XMMRegister w_xtmp1, XMMRegister w_xtmp2, XMMRegister w_xtmp3,
                             bool is_pclmulqdq_supported);
   // Fold 128-bit data chunk
   void fold_128bit_crc32(XMMRegister xcrc, XMMRegister xK, XMMRegister xtmp, Register buf, int offset);
   void fold_128bit_crc32(XMMRegister xcrc, XMMRegister xK, XMMRegister xtmp, XMMRegister xbuf);
+#ifdef _LP64
+  // Fold 512-bit data chunk
+  void fold512bit_crc32_avx512(XMMRegister xcrc, XMMRegister xK, XMMRegister xtmp, Register buf, Register pos, int offset);
+#endif // _LP64
   // Fold 8-bit data
   void fold_8bit_crc32(Register crc, Register table, Register tmp);
   void fold_8bit_crc32(XMMRegister crc, Register table, XMMRegister xtmp, Register tmp);
-  void fold_128bit_crc32_avx512(XMMRegister xcrc, XMMRegister xK, XMMRegister xtmp, Register buf, int offset);
 
   // Compress char[] array to byte[].
   void char_array_compress(Register src, Register dst, Register len,
                            XMMRegister tmp1, XMMRegister tmp2, XMMRegister tmp3,
                            XMMRegister tmp4, Register tmp5, Register result);
diff a/src/hotspot/cpu/x86/stubGenerator_x86_64.cpp b/src/hotspot/cpu/x86/stubGenerator_x86_64.cpp
--- a/src/hotspot/cpu/x86/stubGenerator_x86_64.cpp
+++ b/src/hotspot/cpu/x86/stubGenerator_x86_64.cpp
@@ -5338,17 +5338,24 @@
     // rscratch1: r10
     const Register crc   = c_rarg0;  // crc
     const Register buf   = c_rarg1;  // source java byte array address
     const Register len   = c_rarg2;  // length
     const Register table = c_rarg3;  // crc_table address (reuse register)
-    const Register tmp   = r11;
-    assert_different_registers(crc, buf, len, table, tmp, rax);
+    const Register tmp1   = r11;
+    const Register tmp2   = r10;
+    assert_different_registers(crc, buf, len, table, tmp1, tmp2, rax);
 
     BLOCK_COMMENT("Entry:");
     __ enter(); // required for proper stackwalking of RuntimeStub frame
 
-    __ kernel_crc32(crc, buf, len, table, tmp);
+    if (VM_Version::supports_sse4_1() && VM_Version::supports_avx512_vpclmulqdq() &&
+        VM_Version::supports_avx512bw() &&
+        VM_Version::supports_avx512vl()) {
+      __ kernel_crc32_avx512(crc, buf, len, table, tmp1, tmp2);
+    } else {
+      __ kernel_crc32(crc, buf, len, table, tmp1);
+    }
 
     __ movl(rax, crc);
     __ vzeroupper();
     __ leave(); // required for proper stackwalking of RuntimeStub frame
     __ ret(0);
diff a/src/hotspot/share/classfile/classFileParser.cpp b/src/hotspot/share/classfile/classFileParser.cpp
--- a/src/hotspot/share/classfile/classFileParser.cpp
+++ b/src/hotspot/share/classfile/classFileParser.cpp
@@ -4113,16 +4113,16 @@
   this_klass->set_fields(_fields, java_fields_count);
   this_klass->set_methods(_methods);
   this_klass->set_inner_classes(_inner_classes);
   this_klass->set_nest_members(_nest_members);
   this_klass->set_nest_host_index(_nest_host);
-  this_klass->set_local_interfaces(_local_interfaces);
   this_klass->set_annotations(_combined_annotations);
   this_klass->set_permitted_subclasses(_permitted_subclasses);
   this_klass->set_record_components(_record_components);
-  // Delay the setting of _transitive_interfaces until after initialize_supers() in
-  // fill_instance_klass(). It is because the _transitive_interfaces may be shared with
+  // Delay the setting of _local_interfaces and _transitive_interfaces until after
+  // initialize_supers() in fill_instance_klass(). It is because the _local_interfaces could
+  // be shared with _transitive_interfaces and _transitive_interfaces may be shared with
   // its _super. If an OOM occurs while loading the current klass, its _super field
   // may not have been set. When GC tries to free the klass, the _transitive_interfaces
   // may be deallocated mistakenly in InstanceKlass::deallocate_interfaces(). Subsequent
   // dereferences to the deallocated _transitive_interfaces will result in a crash.
 
@@ -6293,11 +6293,10 @@
   assert(NULL == _cp, "invariant");
   assert(NULL == _fields, "invariant");
   assert(NULL == _methods, "invariant");
   assert(NULL == _inner_classes, "invariant");
   assert(NULL == _nest_members, "invariant");
-  assert(NULL == _local_interfaces, "invariant");
   assert(NULL == _combined_annotations, "invariant");
   assert(NULL == _record_components, "invariant");
   assert(NULL == _permitted_subclasses, "invariant");
 
   if (_has_final_method) {
@@ -6370,11 +6369,13 @@
   }
 
   // Fill in information needed to compute superclasses.
   ik->initialize_supers(const_cast<InstanceKlass*>(_super_klass), _transitive_interfaces, CHECK);
   ik->set_transitive_interfaces(_transitive_interfaces);
+  ik->set_local_interfaces(_local_interfaces);
   _transitive_interfaces = NULL;
+  _local_interfaces = NULL;
 
   // Initialize itable offset tables
   klassItable::setup_itable_offset_table(ik);
 
   // Compute transitive closure of interfaces this class implements
@@ -6746,11 +6747,10 @@
   _fields = NULL;
   _methods = NULL;
   _inner_classes = NULL;
   _nest_members = NULL;
   _permitted_subclasses = NULL;
-  _local_interfaces = NULL;
   _combined_annotations = NULL;
   _class_annotations = _class_type_annotations = NULL;
   _fields_annotations = _fields_type_annotations = NULL;
   _record_components = NULL;
 }
@@ -6814,10 +6814,11 @@
     Annotations::free_contents(_loader_data, _fields_type_annotations);
   }
 
   clear_class_metadata();
   _transitive_interfaces = NULL;
+  _local_interfaces = NULL;
 
   // deallocate the klass if already created.  Don't directly deallocate, but add
   // to the deallocate list so that the klass is removed from the CLD::_klasses list
   // at a safepoint.
   if (_klass_to_deallocate != NULL) {
diff a/src/hotspot/share/classfile/classLoaderData.cpp b/src/hotspot/share/classfile/classLoaderData.cpp
--- a/src/hotspot/share/classfile/classLoaderData.cpp
+++ b/src/hotspot/share/classfile/classLoaderData.cpp
@@ -182,19 +182,19 @@
     delete c;
     c = next;
   }
 }
 
-oop* ClassLoaderData::ChunkedHandleList::add(oop o) {
+OopHandle ClassLoaderData::ChunkedHandleList::add(oop o) {
   if (_head == NULL || _head->_size == Chunk::CAPACITY) {
     Chunk* next = new Chunk(_head);
     Atomic::release_store(&_head, next);
   }
   oop* handle = &_head->_data[_head->_size];
   NativeAccess<IS_DEST_UNINITIALIZED>::oop_store(handle, o);
   Atomic::release_store(&_head->_size, _head->_size + 1);
-  return handle;
+  return OopHandle(handle);
 }
 
 int ClassLoaderData::ChunkedHandleList::count() const {
   int count = 0;
   Chunk* chunk = _head;
@@ -496,11 +496,11 @@
 }
 
 void ClassLoaderData::initialize_holder(Handle loader_or_mirror) {
   if (loader_or_mirror() != NULL) {
     assert(_holder.is_null(), "never replace holders");
-    _holder = WeakHandle<vm_class_loader_data>::create(loader_or_mirror);
+    _holder = WeakHandle<vm_weak_data>::create(loader_or_mirror);
   }
 }
 
 // Remove a klass from the _klasses list for scratch_class during redefinition
 // or parsed class in the case of an error.
@@ -787,11 +787,11 @@
 }
 
 OopHandle ClassLoaderData::add_handle(Handle h) {
   MutexLocker ml(metaspace_lock(),  Mutex::_no_safepoint_check_flag);
   record_modified_oops();
-  return OopHandle(_handles.add(h()));
+  return _handles.add(h());
 }
 
 void ClassLoaderData::remove_handle(OopHandle h) {
   assert(!is_unloading(), "Do not remove a handle for a CLD that is unloading");
   oop* ptr = h.ptr_raw();
diff a/src/hotspot/share/classfile/classLoaderData.hpp b/src/hotspot/share/classfile/classLoaderData.hpp
--- a/src/hotspot/share/classfile/classLoaderData.hpp
+++ b/src/hotspot/share/classfile/classLoaderData.hpp
@@ -88,11 +88,11 @@
     ChunkedHandleList() : _head(NULL) {}
     ~ChunkedHandleList();
 
     // Only one thread at a time can add, guarded by ClassLoaderData::metaspace_lock().
     // However, multiple threads can execute oops_do concurrently with add.
-    oop* add(oop o);
+    OopHandle add(oop o);
     bool contains(oop p);
     NOT_PRODUCT(bool owner_of(oop* p);)
     void oops_do(OopClosure* f);
 
     int count() const;
@@ -107,13 +107,13 @@
   friend class MetaDataFactory;
   friend class Method;
 
   static ClassLoaderData * _the_null_class_loader_data;
 
-  WeakHandle<vm_class_loader_data> _holder; // The oop that determines lifetime of this class loader
-  OopHandle _class_loader;    // The instance of java/lang/ClassLoader associated with
-                              // this ClassLoaderData
+  WeakHandle<vm_weak_data> _holder; // The oop that determines lifetime of this class loader
+  OopHandle _class_loader;          // The instance of java/lang/ClassLoader associated with
+                                    // this ClassLoaderData
 
   ClassLoaderMetaspace * volatile _metaspace;  // Meta-space where meta-data defined by the
                                     // classes in the class loader are allocated.
   Mutex* _metaspace_lock;  // Locks the metaspace for allocations and setup.
   bool _unloading;         // true if this class loader goes away
diff a/src/hotspot/share/classfile/javaClasses.cpp b/src/hotspot/share/classfile/javaClasses.cpp
--- a/src/hotspot/share/classfile/javaClasses.cpp
+++ b/src/hotspot/share/classfile/javaClasses.cpp
@@ -897,11 +897,11 @@
     if (HeapShared::open_archive_heap_region_mapped()) {
       bool present = restore_archived_mirror(k, Handle(), Handle(), Handle(), CHECK);
       assert(present, "Missing archived mirror for %s", k->external_name());
       return;
     } else {
-      k->set_java_mirror_handle(NULL);
+      k->set_java_mirror_handle(OopHandle());
       k->clear_has_raw_archived_mirror();
     }
   }
   create_mirror(k, Handle(), Handle(), Handle(), Handle(), CHECK);
 }
@@ -1225,18 +1225,18 @@
 
     if (!(ik->is_shared_boot_class() || ik->is_shared_platform_class() ||
           ik->is_shared_app_class())) {
       // Archiving mirror for classes from non-builtin loaders is not
       // supported. Clear the _java_mirror within the archived class.
-      k->set_java_mirror_handle(NULL);
+      k->set_java_mirror_handle(OopHandle());
       return NULL;
     }
   }
 
   if (k->is_value()) {
     // Values have a val type mirror and a ref type mirror. Don't handle this for now. TODO:CDS
-    k->set_java_mirror_handle(NULL);
+    k->set_java_mirror_handle(OopHandle());
     return NULL;
   }
 
   // Now start archiving the mirror object
   oop archived_mirror = HeapShared::archive_heap_object(mirror, THREAD);
diff a/src/hotspot/share/classfile/systemDictionary.cpp b/src/hotspot/share/classfile/systemDictionary.cpp
--- a/src/hotspot/share/classfile/systemDictionary.cpp
+++ b/src/hotspot/share/classfile/systemDictionary.cpp
@@ -44,12 +44,10 @@
 #include "classfile/systemDictionary.hpp"
 #include "classfile/vmSymbols.hpp"
 #include "code/codeCache.hpp"
 #include "compiler/compileBroker.hpp"
 #include "gc/shared/gcTraceTime.inline.hpp"
-#include "gc/shared/oopStorage.inline.hpp"
-#include "gc/shared/oopStorageSet.hpp"
 #include "interpreter/bytecodeStream.hpp"
 #include "interpreter/interpreter.hpp"
 #include "jfr/jfrEvents.hpp"
 #include "logging/log.hpp"
 #include "logging/logStream.hpp"
@@ -67,10 +65,11 @@
 #include "oops/method.inline.hpp"
 #include "oops/methodData.hpp"
 #include "oops/objArrayKlass.hpp"
 #include "oops/objArrayOop.inline.hpp"
 #include "oops/oop.inline.hpp"
+#include "oops/oopHandle.inline.hpp"
 #include "oops/symbol.hpp"
 #include "oops/typeArrayKlass.hpp"
 #include "oops/valueKlass.hpp"
 #include "prims/jvmtiExport.hpp"
 #include "prims/methodHandles.hpp"
@@ -99,19 +98,19 @@
 LoaderConstraintTable* SystemDictionary::_loader_constraints  = NULL;
 ResolutionErrorTable*  SystemDictionary::_resolution_errors   = NULL;
 SymbolPropertyTable*   SystemDictionary::_invoke_method_table = NULL;
 ProtectionDomainCacheTable*   SystemDictionary::_pd_cache_table = NULL;
 
-oop         SystemDictionary::_system_loader_lock_obj     =  NULL;
-
 InstanceKlass*      SystemDictionary::_well_known_klasses[SystemDictionary::WKID_LIMIT]
                                                           =  { NULL /*, NULL...*/ };
 
 InstanceKlass*      SystemDictionary::_box_klasses[T_VOID+1]      =  { NULL /*, NULL...*/ };
 
-oop         SystemDictionary::_java_system_loader         =  NULL;
-oop         SystemDictionary::_java_platform_loader       =  NULL;
+
+OopHandle   SystemDictionary::_system_loader_lock_obj;
+OopHandle   SystemDictionary::_java_system_loader;
+OopHandle   SystemDictionary::_java_platform_loader;
 
 // Default ProtectionDomainCacheSize value
 
 const int defaultProtectionDomainCacheSize = 1009;
 
@@ -156,16 +155,20 @@
 }
 
 // ----------------------------------------------------------------------------
 // Java-level SystemLoader and PlatformLoader
 
+oop SystemDictionary::system_loader_lock() {
+  return _system_loader_lock_obj.resolve();
+}
+
 oop SystemDictionary::java_system_loader() {
-  return _java_system_loader;
+  return _java_system_loader.resolve();
 }
 
 oop SystemDictionary::java_platform_loader() {
-  return _java_platform_loader;
+  return _java_platform_loader.resolve();
 }
 
 void SystemDictionary::compute_java_loaders(TRAPS) {
   JavaValue result(T_OBJECT);
   InstanceKlass* class_loader_klass = SystemDictionary::ClassLoader_klass();
@@ -173,19 +176,19 @@
                          class_loader_klass,
                          vmSymbols::getSystemClassLoader_name(),
                          vmSymbols::void_classloader_signature(),
                          CHECK);
 
-  _java_system_loader = (oop)result.get_jobject();
+  _java_system_loader = OopHandle::create((oop)result.get_jobject());
 
   JavaCalls::call_static(&result,
                          class_loader_klass,
                          vmSymbols::getPlatformClassLoader_name(),
                          vmSymbols::void_classloader_signature(),
                          CHECK);
 
-  _java_platform_loader = (oop)result.get_jobject();
+  _java_platform_loader = OopHandle::create((oop)result.get_jobject());
 }
 
 ClassLoaderData* SystemDictionary::register_loader(Handle class_loader, bool create_mirror_cld) {
   if (create_mirror_cld) {
     // Add a new class loader data to the graph.
@@ -220,11 +223,11 @@
 bool SystemDictionary::is_system_class_loader(oop class_loader) {
   if (class_loader == NULL) {
     return false;
   }
   return (class_loader->klass() == SystemDictionary::jdk_internal_loader_ClassLoaders_AppClassLoader_klass() ||
-         class_loader == _java_system_loader);
+         class_loader == _java_system_loader.peek());
 }
 
 // Returns true if the passed class loader is the platform class loader.
 bool SystemDictionary::is_platform_class_loader(oop class_loader) {
   if (class_loader == NULL) {
@@ -648,11 +651,12 @@
   assert_lock_strong(SystemDictionary_lock);
 
   bool calledholdinglock
       = ObjectSynchronizer::current_thread_holds_lock((JavaThread*)THREAD, lockObject);
   assert(calledholdinglock,"must hold lock for notify");
-  assert((lockObject() != _system_loader_lock_obj && !is_parallelCapable(lockObject)), "unexpected double_lock_wait");
+  assert((lockObject() != _system_loader_lock_obj.resolve() &&
+         !is_parallelCapable(lockObject)), "unexpected double_lock_wait");
   ObjectSynchronizer::notifyall(lockObject, THREAD);
   intx recursions =  ObjectSynchronizer::complete_exit(lockObject, THREAD);
   SystemDictionary_lock->wait();
   SystemDictionary_lock->unlock();
   ObjectSynchronizer::reenter(lockObject, recursions, THREAD);
@@ -744,11 +748,11 @@
     }
   }
   return NULL;
 }
 
-static void post_class_load_event(EventClassLoad* event, const InstanceKlass* k, const ClassLoaderData* init_cld) {
+void SystemDictionary::post_class_load_event(EventClassLoad* event, const InstanceKlass* k, const ClassLoaderData* init_cld) {
   assert(event != NULL, "invariant");
   assert(k != NULL, "invariant");
   assert(event->should_commit(), "invariant");
   event->set_loadedClass(k);
   event->set_definingClassLoader(k->class_loader_data());
@@ -1410,10 +1414,41 @@
   }
 
   return true;
 }
 
+InstanceKlass* SystemDictionary::load_shared_lambda_proxy_class(InstanceKlass* ik,
+                                                                Handle class_loader,
+                                                                Handle protection_domain,
+                                                                PackageEntry* pkg_entry,
+                                                                TRAPS) {
+  InstanceKlass* shared_nest_host = SystemDictionaryShared::get_shared_nest_host(ik);
+  assert(shared_nest_host->is_shared(), "nest host must be in CDS archive");
+  Symbol* cn = shared_nest_host->name();
+  Klass *s = resolve_or_fail(cn, class_loader, protection_domain, true, CHECK_NULL);
+  if (s != shared_nest_host) {
+    // The dynamically resolved nest_host is not the same as the one we used during dump time,
+    // so we cannot use ik.
+    return NULL;
+  } else {
+    assert(s->is_shared(), "must be");
+  }
+
+  // The lambda proxy class and its nest host have the same class loader and class loader data,
+  // as verified in SystemDictionaryShared::add_lambda_proxy_class()
+  assert(shared_nest_host->class_loader() == class_loader(), "mismatched class loader");
+  assert(shared_nest_host->class_loader_data() == ClassLoaderData::class_loader_data(class_loader()), "mismatched class loader data");
+  ik->set_nest_host(shared_nest_host, THREAD);
+
+  InstanceKlass* loaded_ik = load_shared_class(ik, class_loader, protection_domain, NULL, pkg_entry, CHECK_NULL);
+
+  assert(shared_nest_host->is_same_class_package(ik),
+         "lambda proxy class and its nest host must be in the same package");
+
+  return loaded_ik;
+}
+
 InstanceKlass* SystemDictionary::load_shared_class(InstanceKlass* ik,
                                                    Handle class_loader,
                                                    Handle protection_domain,
                                                    const ClassFileStream *cfs,
                                                    PackageEntry* pkg_entry,
@@ -1430,12 +1465,17 @@
 
   if (!check_shared_class_super_types(ik, class_loader, protection_domain, THREAD)) {
     return NULL;
   }
 
-  InstanceKlass* new_ik = KlassFactory::check_shared_class_file_load_hook(
+  InstanceKlass* new_ik = NULL;
+  // CFLH check is skipped for VM hidden or anonymous classes (see KlassFactory::create_from_stream).
+  // It will be skipped for shared VM hidden lambda proxy classes.
+  if (!SystemDictionaryShared::is_hidden_lambda_proxy(ik)) {
+    new_ik = KlassFactory::check_shared_class_file_load_hook(
       ik, class_name, class_loader, protection_domain, cfs, CHECK_NULL);
+  }
   if (new_ik != NULL) {
     // The class is changed by CFLH. Return the new class. The shared class is
     // not used.
     return new_ik;
   }
@@ -1865,11 +1905,11 @@
 }
 
 Handle SystemDictionary::compute_loader_lock_object(Handle class_loader, TRAPS) {
   // If class_loader is NULL we synchronize on _system_loader_lock_obj
   if (class_loader.is_null()) {
-    return Handle(THREAD, _system_loader_lock_obj);
+    return Handle(THREAD, _system_loader_lock_obj.resolve());
   } else {
     return class_loader;
   }
 }
 
@@ -1886,11 +1926,11 @@
 
   if (ObjectSynchronizer::query_lock_ownership((JavaThread*)THREAD, loader_lock)
       == ObjectSynchronizer::owner_other) {
     // contention will likely happen, so increment the corresponding
     // contention counter.
-    if (loader_lock() == _system_loader_lock_obj) {
+    if (loader_lock() == _system_loader_lock_obj.resolve()) {
       ClassLoader::sync_systemLoaderLockContentionRate()->inc();
     } else {
       ClassLoader::sync_nonSystemLoaderLockContentionRate()->inc();
     }
   }
@@ -2003,24 +2043,10 @@
   }
 
   return unloading_occurred;
 }
 
-void SystemDictionary::oops_do(OopClosure* f, bool include_handles) {
-  f->do_oop(&_java_system_loader);
-  f->do_oop(&_java_platform_loader);
-  f->do_oop(&_system_loader_lock_obj);
-  CDS_ONLY(SystemDictionaryShared::oops_do(f);)
-
-  // Visit extra methods
-  invoke_method_table()->oops_do(f);
-
-  if (include_handles) {
-    OopStorageSet::vm_global()->oops_do(f);
-  }
-}
-
 // CDS: scan and relocate all classes referenced by _well_known_klasses[].
 void SystemDictionary::well_known_klasses_do(MetaspaceClosure* it) {
   for (int id = FIRST_WKID; id < WKID_LIMIT; id++) {
     it->push(well_known_klass_addr((WKID)id));
   }
@@ -2044,11 +2070,13 @@
   _resolution_errors   = new ResolutionErrorTable(_resolution_error_size);
   _invoke_method_table = new SymbolPropertyTable(_invoke_method_size);
   _pd_cache_table = new ProtectionDomainCacheTable(defaultProtectionDomainCacheSize);
 
   // Allocate private object used as system class loader lock
-  _system_loader_lock_obj = oopFactory::new_intArray(0, CHECK);
+  oop lock_obj = oopFactory::new_intArray(0, CHECK);
+  _system_loader_lock_obj = OopHandle::create(lock_obj);
+
   // Initialize basic classes
   resolve_well_known_classes(CHECK);
 }
 
 // Compact table of directions on the initialization of klasses:
diff a/src/hotspot/share/classfile/systemDictionary.hpp b/src/hotspot/share/classfile/systemDictionary.hpp
--- a/src/hotspot/share/classfile/systemDictionary.hpp
+++ b/src/hotspot/share/classfile/systemDictionary.hpp
@@ -25,10 +25,11 @@
 #ifndef SHARE_CLASSFILE_SYSTEMDICTIONARY_HPP
 #define SHARE_CLASSFILE_SYSTEMDICTIONARY_HPP
 
 #include "classfile/classLoaderData.hpp"
 #include "oops/objArrayOop.hpp"
+#include "oops/oopHandle.hpp"
 #include "oops/symbol.hpp"
 #include "runtime/java.hpp"
 #include "runtime/mutexLocker.hpp"
 #include "runtime/reflectionUtils.hpp"
 #include "runtime/signature.hpp"
@@ -130,10 +131,11 @@
 class ResolutionErrorTable;
 class SymbolPropertyTable;
 class ProtectionDomainCacheTable;
 class ProtectionDomainCacheEntry;
 class GCTimer;
+class EventClassLoad;
 
 #define WK_KLASS_ENUM_NAME(kname)    kname##_knum
 
 // Certain classes, such as java.lang.Object and java.lang.String,
 // are "well-known", in the sense that no class loader is allowed
@@ -389,16 +391,11 @@
 
   // Unload (that is, break root links to) all unmarked classes and
   // loaders.  Returns "true" iff something was unloaded.
   static bool do_unloading(GCTimer* gc_timer);
 
-  // Applies "f->do_oop" to all root oops in the system dictionary.
-  // If include_handles is true (the default), then the handles in the
-  // vm_global OopStorage object are included.
-  static void oops_do(OopClosure* f, bool include_handles = true);
-
-  // System loader lock
+  // System loader lock
   static oop system_loader_lock()           { return _system_loader_lock_obj; }
 
   // Protection Domain Table
   static ProtectionDomainCacheTable* pd_cache_table() { return _pd_cache_table; }
 
@@ -594,11 +591,11 @@
 
   // Hashtable holding placeholders for classes being loaded.
   static PlaceholderTable*       _placeholders;
 
   // Lock object for system class loader
-  static oop                     _system_loader_lock_obj;
+  static OopHandle               _system_loader_lock_obj;
 
   // Constraints on class loaders
   static LoaderConstraintTable*  _loader_constraints;
 
   // Resolution errors
@@ -619,10 +616,11 @@
   friend class TraversePlaceholdersClosure;
   static PlaceholderTable*   placeholders() { return _placeholders; }
   static LoaderConstraintTable* constraints() { return _loader_constraints; }
   static ResolutionErrorTable* resolution_errors() { return _resolution_errors; }
   static SymbolPropertyTable* invoke_method_table() { return _invoke_method_table; }
+  static void post_class_load_event(EventClassLoad* event, const InstanceKlass* k, const ClassLoaderData* init_cld);
 
   // Basic loading operations
   static InstanceKlass* resolve_instance_class_or_null_helper(Symbol* name,
                                                               Handle class_loader,
                                                               Handle protection_domain,
@@ -645,10 +643,15 @@
   static bool check_shared_class_super_type(InstanceKlass* child, InstanceKlass* super,
                                             Handle class_loader,  Handle protection_domain,
                                             bool is_superclass, TRAPS);
   static bool check_shared_class_super_types(InstanceKlass* ik, Handle class_loader,
                                                Handle protection_domain, TRAPS);
+  static InstanceKlass* load_shared_lambda_proxy_class(InstanceKlass* ik,
+                                                       Handle class_loader,
+                                                       Handle protection_domain,
+                                                       PackageEntry* pkg_entry,
+                                                       TRAPS);
   static InstanceKlass* load_shared_class(InstanceKlass* ik,
                                           Handle class_loader,
                                           Handle protection_domain,
                                           const ClassFileStream *cfs,
                                           PackageEntry* pkg_entry,
@@ -712,12 +715,12 @@
 
   // table of box klasses (int_klass, etc.)
   static InstanceKlass* _box_klasses[T_VOID+1];
 
 private:
-  static oop  _java_system_loader;
-  static oop  _java_platform_loader;
+  static OopHandle  _java_system_loader;
+  static OopHandle  _java_platform_loader;
 
 public:
   static TableStatistics placeholders_statistics();
   static TableStatistics loader_constraints_statistics();
   static TableStatistics protection_domain_cache_statistics();
diff a/src/hotspot/share/code/nmethod.cpp b/src/hotspot/share/code/nmethod.cpp
--- a/src/hotspot/share/code/nmethod.cpp
+++ b/src/hotspot/share/code/nmethod.cpp
@@ -1138,11 +1138,13 @@
 bool nmethod::can_convert_to_zombie() {
   // Note that this is called when the sweeper has observed the nmethod to be
   // not_entrant. However, with concurrent code cache unloading, the state
   // might have moved on to unloaded if it is_unloading(), due to racing
   // concurrent GC threads.
-  assert(is_not_entrant() || is_unloading(), "must be a non-entrant method");
+  assert(is_not_entrant() || is_unloading() ||
+         !Thread::current()->is_Code_cache_sweeper_thread(),
+         "must be a non-entrant method if called from sweeper");
 
   // Since the nmethod sweeper only does partial sweep the sweeper's traversal
   // count can be greater than the stack traversal count before it hits the
   // nmethod for the second time.
   // If an is_unloading() nmethod is still not_entrant, then it is not safe to
diff a/src/hotspot/share/compiler/compileBroker.cpp b/src/hotspot/share/compiler/compileBroker.cpp
--- a/src/hotspot/share/compiler/compileBroker.cpp
+++ b/src/hotspot/share/compiler/compileBroker.cpp
@@ -1653,11 +1653,12 @@
 
   methodHandle method(thread, task->method());
   bool free_task;
 #if INCLUDE_JVMCI
   AbstractCompiler* comp = compiler(task->comp_level());
-  if (comp->is_jvmci()) {
+  if (comp->is_jvmci() && !task->should_wait_for_compilation()) {
+    // It may return before compilation is completed.
     free_task = wait_for_jvmci_completion((JVMCICompiler*) comp, task, thread);
   } else
 #endif
   {
     MonitorLocker ml(thread, task->lock());
diff a/src/hotspot/share/gc/parallel/psParallelCompact.cpp b/src/hotspot/share/gc/parallel/psParallelCompact.cpp
--- a/src/hotspot/share/gc/parallel/psParallelCompact.cpp
+++ b/src/hotspot/share/gc/parallel/psParallelCompact.cpp
@@ -47,10 +47,12 @@
 #include "gc/shared/gcLocker.hpp"
 #include "gc/shared/gcTimer.hpp"
 #include "gc/shared/gcTrace.hpp"
 #include "gc/shared/gcTraceTime.inline.hpp"
 #include "gc/shared/isGCActiveMark.hpp"
+#include "gc/shared/oopStorage.inline.hpp"
+#include "gc/shared/oopStorageSet.hpp"
 #include "gc/shared/referencePolicy.hpp"
 #include "gc/shared/referenceProcessor.hpp"
 #include "gc/shared/referenceProcessorPhaseTimes.hpp"
 #include "gc/shared/spaceDecorator.inline.hpp"
 #include "gc/shared/taskTerminator.hpp"
@@ -2028,12 +2030,12 @@
 
     case ParallelRootType::jvmti:
       JvmtiExport::oops_do(&mark_and_push_closure);
       break;
 
-    case ParallelRootType::system_dictionary:
-      SystemDictionary::oops_do(&mark_and_push_closure);
+    case ParallelRootType::vm_global:
+      OopStorageSet::vm_global()->oops_do(&mark_and_push_closure);
       break;
 
     case ParallelRootType::class_loader_data:
       {
         CLDToOopClosure cld_closure(&mark_and_push_closure, ClassLoaderData::_claim_strong);
@@ -2237,11 +2239,11 @@
   JNIHandles::oops_do(&oop_closure);   // Global (strong) JNI handles
   Threads::oops_do(&oop_closure, NULL);
   ObjectSynchronizer::oops_do(&oop_closure);
   Management::oops_do(&oop_closure);
   JvmtiExport::oops_do(&oop_closure);
-  SystemDictionary::oops_do(&oop_closure);
+  OopStorageSet::vm_global()->oops_do(&oop_closure);
   CLDToOopClosure cld_closure(&oop_closure, ClassLoaderData::_claim_strong);
   ClassLoaderDataGraph::cld_do(&cld_closure);
 
   // Now adjust pointers in remaining weak roots.  (All of which should
   // have been cleared if they pointed to non-surviving objects.)
diff a/src/hotspot/share/gc/shared/collectedHeap.hpp b/src/hotspot/share/gc/shared/collectedHeap.hpp
--- a/src/hotspot/share/gc/shared/collectedHeap.hpp
+++ b/src/hotspot/share/gc/shared/collectedHeap.hpp
@@ -428,17 +428,10 @@
   virtual void print_on_error(outputStream* st) const;
 
   // Used to print information about locations in the hs_err file.
   virtual bool print_location(outputStream* st, void* addr) const = 0;
 
-  // Print all GC threads (other than the VM thread)
-  // used by this heap.
-  virtual void print_gc_threads_on(outputStream* st) const = 0;
-  // The default behavior is to call print_gc_threads_on() on tty.
-  void print_gc_threads() {
-    print_gc_threads_on(tty);
-  }
   // Iterator for all GC threads (other than VM thread)
   virtual void gc_threads_do(ThreadClosure* tc) const = 0;
 
   // Print any relevant tracing info that flags imply.
   // Default implementation does nothing.
diff a/src/hotspot/share/memory/dynamicArchive.cpp b/src/hotspot/share/memory/dynamicArchive.cpp
--- a/src/hotspot/share/memory/dynamicArchive.cpp
+++ b/src/hotspot/share/memory/dynamicArchive.cpp
@@ -671,10 +671,15 @@
     verify_estimate_size(_estimated_hashtable_bytes, "Hashtables");
 
     make_trampolines();
     make_klasses_shareable();
 
+    {
+      log_info(cds)("Adjust lambda proxy class dictionary");
+      SystemDictionaryShared::adjust_lambda_proxy_class_dictionary();
+    }
+
     {
       log_info(cds)("Final relocation of pointers ... ");
       ResourceMark rm;
       PointerMarker marker(this);
       iterate_roots(&marker);
@@ -862,20 +867,11 @@
     sort_methods(ik);
   }
 
   for (i = 0; i < count; i++) {
     InstanceKlass* ik = _klasses->at(i);
-    ClassLoaderData *cld = ik->class_loader_data();
-    if (cld->is_boot_class_loader_data()) {
-      ik->set_shared_class_loader_type(ClassLoader::BOOT_LOADER);
-    }
-    else if (cld->is_platform_class_loader_data()) {
-      ik->set_shared_class_loader_type(ClassLoader::PLATFORM_LOADER);
-    }
-    else if (cld->is_system_class_loader_data()) {
-      ik->set_shared_class_loader_type(ClassLoader::APP_LOADER);
-    }
+    ik->assign_class_loader_type();
 
     MetaspaceShared::rewrite_nofast_bytecodes_and_calculate_fingerprints(Thread::current(), ik);
     ik->remove_unshareable_info();
 
     assert(ik->array_klasses() == NULL, "sanity");
diff a/src/hotspot/share/memory/metaspaceShared.cpp b/src/hotspot/share/memory/metaspaceShared.cpp
--- a/src/hotspot/share/memory/metaspaceShared.cpp
+++ b/src/hotspot/share/memory/metaspaceShared.cpp
@@ -2556,10 +2556,14 @@
   assert(is_aligned(archive_space_rs.base(), archive_space_alignment), "Sanity");
   assert(is_aligned(archive_space_rs.size(), archive_space_alignment), "Sanity");
   assert(is_aligned(class_space_rs.base(), class_space_alignment), "Sanity");
   assert(is_aligned(class_space_rs.size(), class_space_alignment), "Sanity");
 
+  // NMT: fix up the space tags
+  MemTracker::record_virtual_memory_type(archive_space_rs.base(), mtClassShared);
+  MemTracker::record_virtual_memory_type(class_space_rs.base(), mtClass);
+
   return archive_space_rs.base();
 
 #else
   ShouldNotReachHere();
   return NULL;
diff a/src/hotspot/share/memory/universe.cpp b/src/hotspot/share/memory/universe.cpp
--- a/src/hotspot/share/memory/universe.cpp
+++ b/src/hotspot/share/memory/universe.cpp
@@ -1119,16 +1119,14 @@
   }
   if (should_verify_subset(Verify_SystemDictionary)) {
     log_debug(gc, verify)("SystemDictionary");
     SystemDictionary::verify();
   }
-#ifndef PRODUCT
   if (should_verify_subset(Verify_ClassLoaderDataGraph)) {
     log_debug(gc, verify)("ClassLoaderDataGraph");
     ClassLoaderDataGraph::verify();
   }
-#endif
   if (should_verify_subset(Verify_MetaspaceUtils)) {
     log_debug(gc, verify)("MetaspaceUtils");
     MetaspaceUtils::verify_free_chunks();
   }
   if (should_verify_subset(Verify_JNIHandles)) {
diff a/src/hotspot/share/oops/constantPool.cpp b/src/hotspot/share/oops/constantPool.cpp
--- a/src/hotspot/share/oops/constantPool.cpp
+++ b/src/hotspot/share/oops/constantPool.cpp
@@ -284,11 +284,11 @@
   InstanceKlass *ik = pool_holder();
   if (!(ik->is_shared_boot_class() || ik->is_shared_platform_class() ||
         ik->is_shared_app_class())) {
     // Archiving resolved references for classes from non-builtin loaders
     // is not yet supported.
-    set_resolved_references(NULL);
+    set_resolved_references(OopHandle());
     return;
   }
 
   objArrayOop rr = resolved_references();
   Array<u2>* ref_map = reference_map();
@@ -316,11 +316,11 @@
     // the 'archived' object is NULL. No need to explicitly check
     // the return value of archive_heap_object here. At runtime, the
     // resolved references will be created using the normal process
     // when there is no archived value.
     _cache->set_archived_references(archived);
-    set_resolved_references(NULL);
+    set_resolved_references(OopHandle());
   }
 }
 
 void ConstantPool::resolve_class_constants(TRAPS) {
   assert(DumpSharedSpaces, "used during dump time only");
@@ -390,11 +390,11 @@
   // Otherwise, it is cleared after the resolved references array is cached
   // (see archive_resolved_references()).
   // If DynamicDumpSharedSpaces is enabled, clear the resolved references also
   // as java objects are not archived in the top layer.
   if (!HeapShared::is_heap_object_archiving_allowed() || DynamicDumpSharedSpaces) {
-    set_resolved_references(NULL);
+    set_resolved_references(OopHandle());
   }
 
   // Shared ConstantPools are in the RO region, so the _flags cannot be modified.
   // The _on_stack flag is used to prevent ConstantPools from deallocation during
   // class redefinition. Since shared ConstantPools cannot be deallocated anyway,
@@ -415,17 +415,25 @@
     if (tag_at(index).is_klass()) {
       // This class was resolved as a side effect of executing Java code
       // during dump time. We need to restore it back to an UnresolvedClass,
       // so that the proper class loading and initialization can happen
       // at runtime.
-      CPKlassSlot kslot = klass_slot_at(index);
-      int resolved_klass_index = kslot.resolved_klass_index();
-      int name_index = kslot.name_index();
-      assert(tag_at(name_index).is_symbol(), "sanity");
-      resolved_klasses()->at_put(resolved_klass_index, NULL);
-      tag_at_put(index, JVM_CONSTANT_UnresolvedClass);
-      assert(klass_name_at(index) == symbol_at(name_index), "sanity");
+      bool clear_it = true;
+      if (pool_holder()->is_hidden() && index == pool_holder()->this_class_index()) {
+        // All references to a hidden class's own field/methods are through this
+        // index. We cannot clear it. See comments in ClassFileParser::fill_instance_klass.
+        clear_it = false;
+      }
+      if (clear_it) {
+        CPKlassSlot kslot = klass_slot_at(index);
+        int resolved_klass_index = kslot.resolved_klass_index();
+        int name_index = kslot.name_index();
+        assert(tag_at(name_index).is_symbol(), "sanity");
+        resolved_klasses()->at_put(resolved_klass_index, NULL);
+        tag_at_put(index, JVM_CONSTANT_UnresolvedClass);
+        assert(klass_name_at(index) == symbol_at(name_index), "sanity");
+      }
     }
   }
   if (cache() != NULL) {
     cache()->remove_unshareable_info();
   }
diff a/src/hotspot/share/oops/cpCache.cpp b/src/hotspot/share/oops/cpCache.cpp
--- a/src/hotspot/share/oops/cpCache.cpp
+++ b/src/hotspot/share/oops/cpCache.cpp
@@ -1,7 +1,7 @@
 /*
- * Copyright (c) 1998, 2019, Oracle and/or its affiliates. All rights reserved.
+ * Copyright (c) 1998, 2020, Oracle and/or its affiliates. All rights reserved.
  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  *
  * This code is free software; you can redistribute it and/or modify it
  * under the terms of the GNU General Public License version 2 only, as
  * published by the Free Software Foundation.
@@ -771,11 +771,11 @@
 }
 
 void ConstantPoolCache::deallocate_contents(ClassLoaderData* data) {
   assert(!is_shared(), "shared caches are not deallocated");
   data->remove_handle(_resolved_references);
-  set_resolved_references(NULL);
+  set_resolved_references(OopHandle());
   MetadataFactory::free_array<u2>(data, _reference_map);
   set_reference_map(NULL);
 }
 
 #if INCLUDE_CDS_JAVA_HEAP
diff a/src/hotspot/share/oops/instanceKlass.cpp b/src/hotspot/share/oops/instanceKlass.cpp
--- a/src/hotspot/share/oops/instanceKlass.cpp
+++ b/src/hotspot/share/oops/instanceKlass.cpp
@@ -2745,10 +2745,23 @@
     ShouldNotReachHere();
     break;
   }
 }
 
+void InstanceKlass::assign_class_loader_type() {
+  ClassLoaderData *cld = class_loader_data();
+  if (cld->is_boot_class_loader_data()) {
+    set_shared_class_loader_type(ClassLoader::BOOT_LOADER);
+  }
+  else if (cld->is_platform_class_loader_data()) {
+    set_shared_class_loader_type(ClassLoader::PLATFORM_LOADER);
+  }
+  else if (cld->is_system_class_loader_data()) {
+    set_shared_class_loader_type(ClassLoader::APP_LOADER);
+  }
+}
+
 #if INCLUDE_JVMTI
 static void clear_all_breakpoints(Method* m) {
   m->clear_all_breakpoints();
 }
 #endif
diff a/src/hotspot/share/oops/instanceKlass.hpp b/src/hotspot/share/oops/instanceKlass.hpp
--- a/src/hotspot/share/oops/instanceKlass.hpp
+++ b/src/hotspot/share/oops/instanceKlass.hpp
@@ -406,10 +406,12 @@
     _misc_flags &= ~shared_loader_type_bits();
   }
 
   void set_shared_class_loader_type(s2 loader_type);
 
+  void assign_class_loader_type();
+
   bool has_nonstatic_fields() const        {
     return (_misc_flags & _misc_has_nonstatic_fields) != 0;
   }
   void set_has_nonstatic_fields(bool b)    {
     if (b) {
diff a/src/hotspot/share/oops/klass.cpp b/src/hotspot/share/oops/klass.cpp
--- a/src/hotspot/share/oops/klass.cpp
+++ b/src/hotspot/share/oops/klass.cpp
@@ -553,11 +553,11 @@
   if (log_is_enabled(Trace, cds, unshareable)) {
     ResourceMark rm;
     log_trace(cds, unshareable)("remove java_mirror: %s", external_name());
   }
   // Just null out the mirror.  The class_loader_data() no longer exists.
-  _java_mirror = NULL;
+  _java_mirror = OopHandle();
 }
 
 void Klass::restore_unshareable_info(ClassLoaderData* loader_data, Handle protection_domain, TRAPS) {
   assert(is_klass(), "ensure C++ vtable is restored");
   assert(is_shared(), "must be set");
@@ -607,11 +607,11 @@
       }
     }
 
     // No archived mirror data
     log_debug(cds, mirror)("No archived mirror data for %s", external_name());
-    _java_mirror = NULL;
+    _java_mirror = OopHandle();
     this->clear_has_raw_archived_mirror();
   }
 
   // Only recreate it if not present.  A previous attempt to restore may have
   // gotten an OOM later but keep the mirror if it was created.
diff a/src/hotspot/share/oops/klass.hpp b/src/hotspot/share/oops/klass.hpp
--- a/src/hotspot/share/oops/klass.hpp
+++ b/src/hotspot/share/oops/klass.hpp
@@ -175,11 +175,12 @@
 
 #if INCLUDE_CDS
   // Flags of the current shared class.
   u2     _shared_class_flags;
   enum {
-    _has_raw_archived_mirror = 1
+    _has_raw_archived_mirror = 1,
+    _archived_lambda_proxy_is_available = 2
   };
 #endif
 
   // The _archived_mirror is set at CDS dump time pointing to the cached mirror
   // in the open archive heap region when archiving java object is supported.
@@ -292,10 +293,11 @@
   InstanceKlass* superklass() const;
   void append_to_sibling_list();           // add newly created receiver to superklass' subklass list
 
   void set_next_link(Klass* k) { _next_link = k; }
   Klass* next_link() const { return _next_link; }   // The next klass defined by the class loader.
+  Klass** next_link_addr() { return &_next_link; }
 
   // class loader data
   ClassLoaderData* class_loader_data() const               { return _class_loader_data; }
   void set_class_loader_data(ClassLoaderData* loader_data) {  _class_loader_data = loader_data; }
 
@@ -316,10 +318,21 @@
   bool has_raw_archived_mirror() const {
     CDS_ONLY(return (_shared_class_flags & _has_raw_archived_mirror) != 0;)
     NOT_CDS(return false;)
   }
 
+  void set_lambda_proxy_is_available() {
+    CDS_ONLY(_shared_class_flags |= _archived_lambda_proxy_is_available;)
+  }
+  void clear_lambda_proxy_is_available() {
+    CDS_ONLY(_shared_class_flags &= ~_archived_lambda_proxy_is_available;)
+  }
+  bool lambda_proxy_is_available() const {
+    CDS_ONLY(return (_shared_class_flags & _archived_lambda_proxy_is_available) != 0;)
+    NOT_CDS(return false;)
+  }
+
   // Obtain the module or package for this class
   virtual ModuleEntry* module() const = 0;
   virtual PackageEntry* package() const = 0;
 
  protected:                                // internal accessors
diff a/src/hotspot/share/opto/arraycopynode.cpp b/src/hotspot/share/opto/arraycopynode.cpp
--- a/src/hotspot/share/opto/arraycopynode.cpp
+++ b/src/hotspot/share/opto/arraycopynode.cpp
@@ -76,15 +76,15 @@
   }
 
   return ac;
 }
 
-void ArrayCopyNode::connect_outputs(GraphKit* kit) {
+void ArrayCopyNode::connect_outputs(GraphKit* kit, bool deoptimize_on_exception) {
   kit->set_all_memory_call(this, true);
   kit->set_control(kit->gvn().transform(new ProjNode(this,TypeFunc::Control)));
   kit->set_i_o(kit->gvn().transform(new ProjNode(this, TypeFunc::I_O)));
-  kit->make_slow_call_ex(this, kit->env()->Throwable_klass(), true);
+  kit->make_slow_call_ex(this, kit->env()->Throwable_klass(), true, deoptimize_on_exception);
   kit->set_all_memory_call(this);
 }
 
 #ifndef PRODUCT
 const char* ArrayCopyNode::_kind_names[] = {"arraycopy", "arraycopy, validated arguments", "clone", "oop array clone", "CopyOf", "CopyOfRange"};
diff a/src/hotspot/share/opto/arraycopynode.hpp b/src/hotspot/share/opto/arraycopynode.hpp
--- a/src/hotspot/share/opto/arraycopynode.hpp
+++ b/src/hotspot/share/opto/arraycopynode.hpp
@@ -144,11 +144,11 @@
                              bool alloc_tightly_coupled,
                              bool has_negative_length_guard,
                              Node* src_klass = NULL, Node* dest_klass = NULL,
                              Node* src_length = NULL, Node* dest_length = NULL);
 
-  void connect_outputs(GraphKit* kit);
+  void connect_outputs(GraphKit* kit, bool deoptimize_on_exception = false);
 
   bool is_arraycopy()             const  { assert(_kind != None, "should bet set"); return _kind == ArrayCopy; }
   bool is_arraycopy_validated()   const  { assert(_kind != None, "should bet set"); return _kind == ArrayCopy && _arguments_validated; }
   bool is_clone_inst()            const  { assert(_kind != None, "should bet set"); return _kind == CloneInst; }
   // is_clone_array - true for all arrays when using GCs that has no barriers
diff a/src/hotspot/share/opto/library_call.cpp b/src/hotspot/share/opto/library_call.cpp
--- a/src/hotspot/share/opto/library_call.cpp
+++ b/src/hotspot/share/opto/library_call.cpp
@@ -4671,11 +4671,11 @@
       }
 
       if (!stopped()) {
         Node* obj_length = load_array_length(obj);
         Node* obj_size  = NULL;
-        Node* alloc_obj = new_array(obj_klass, obj_length, 0, &obj_size);  // no arguments to push
+        Node* alloc_obj = new_array(obj_klass, obj_length, 0, &obj_size, true);  // no arguments to push
 
         BarrierSetC2* bs = BarrierSet::barrier_set()->barrier_set_c2();
         if (bs->array_copy_requires_gc_barriers(true, T_OBJECT, true, BarrierSetC2::Parsing)) {
           // If it is an oop array, it requires very special treatment,
           // because gc barriers are required when accessing the array.
diff a/src/hotspot/share/opto/parse1.cpp b/src/hotspot/share/opto/parse1.cpp
--- a/src/hotspot/share/opto/parse1.cpp
+++ b/src/hotspot/share/opto/parse1.cpp
@@ -1365,13 +1365,15 @@
     ciTypeFlow::Block* tf2 = (i < ns) ? tfs->at(i) : tfe->at(i-ns);
     Block* block2 = outer->rpo_at(tf2->rpo());
     _successors[i] = block2;
 
     // Accumulate pred info for the other block, too.
-    if (i < ns) {
-      block2->_pred_count++;
-    } else {
+    // Note: We also need to set _pred_count for exception blocks since they could
+    // also have normal predecessors (reached without athrow by an explicit jump).
+    // This also means that next_path_num can be called along exception paths.
+    block2->_pred_count++;
+    if (i >= ns) {
       block2->_is_handler = true;
     }
 
     #ifdef ASSERT
     // A block's successors must be distinguishable by BCI.
@@ -1382,14 +1384,10 @@
       if (block1 == block2)  continue;  // duplicates are OK
       assert(block1->start() != block2->start(), "successors have unique bcis");
     }
     #endif
   }
-
-  // Note: We never call next_path_num along exception paths, so they
-  // never get processed as "ready".  Also, the input phis of exception
-  // handlers get specially processed, so that
 }
 
 //---------------------------successor_for_bci---------------------------------
 Parse::Block* Parse::Block::successor_for_bci(int bci) {
   for (int i = 0; i < all_successors(); i++) {
diff a/src/hotspot/share/prims/jvm.cpp b/src/hotspot/share/prims/jvm.cpp
--- a/src/hotspot/share/prims/jvm.cpp
+++ b/src/hotspot/share/prims/jvm.cpp
@@ -24,10 +24,11 @@
 
 #include "precompiled.hpp"
 #include "jvm.h"
 #include "classfile/classFileStream.hpp"
 #include "classfile/classLoader.hpp"
+#include "classfile/classLoaderData.hpp"
 #include "classfile/classLoaderData.inline.hpp"
 #include "classfile/javaAssertions.hpp"
 #include "classfile/javaClasses.inline.hpp"
 #include "classfile/moduleEntry.hpp"
 #include "classfile/modules.hpp"
@@ -39,10 +40,11 @@
 #include "gc/shared/collectedHeap.inline.hpp"
 #include "interpreter/bytecode.hpp"
 #include "interpreter/bytecodeUtils.hpp"
 #include "jfr/jfrEvents.hpp"
 #include "logging/log.hpp"
+#include "memory/dynamicArchive.hpp"
 #include "memory/heapShared.hpp"
 #include "memory/oopFactory.hpp"
 #include "memory/referenceType.hpp"
 #include "memory/resourceArea.hpp"
 #include "memory/universe.hpp"
@@ -3779,10 +3781,117 @@
   Klass* k = java_lang_Class::as_Klass(JNIHandles::resolve(cls));
   assert(k->is_klass(), "just checking");
   HeapShared::initialize_from_archived_subgraph(k);
 JVM_END
 
+JVM_ENTRY(void, JVM_RegisterLambdaProxyClassForArchiving(JNIEnv* env,
+                                              jclass caller,
+                                              jstring invokedName,
+                                              jobject invokedType,
+                                              jobject methodType,
+                                              jobject implMethodMember,
+                                              jobject instantiatedMethodType,
+                                              jclass lambdaProxyClass))
+  JVMWrapper("JVM_RegisterLambdaProxyClassForArchiving");
+  if (!DynamicDumpSharedSpaces) {
+    return;
+  }
+
+  Klass* caller_k = java_lang_Class::as_Klass(JNIHandles::resolve(caller));
+  InstanceKlass* caller_ik = InstanceKlass::cast(caller_k);
+  if (caller_ik->is_hidden() || caller_ik->is_unsafe_anonymous()) {
+    // VM anonymous classes and hidden classes not of type lambda proxy classes are currently not being archived.
+    // If the caller_ik is of one of the above types, the corresponding lambda proxy class won't be
+    // registered for archiving.
+    return;
+  }
+  Klass* lambda_k = java_lang_Class::as_Klass(JNIHandles::resolve(lambdaProxyClass));
+  InstanceKlass* lambda_ik = InstanceKlass::cast(lambda_k);
+  assert(lambda_ik->is_hidden(), "must be a hidden class");
+  assert(!lambda_ik->is_non_strong_hidden(), "expected a strong hidden class");
+
+  Symbol* invoked_name = NULL;
+  if (invokedName != NULL) {
+    invoked_name = java_lang_String::as_symbol(JNIHandles::resolve_non_null(invokedName));
+  }
+  Handle invoked_type_oop(THREAD, JNIHandles::resolve_non_null(invokedType));
+  Symbol* invoked_type = java_lang_invoke_MethodType::as_signature(invoked_type_oop(), true);
+
+  Handle method_type_oop(THREAD, JNIHandles::resolve_non_null(methodType));
+  Symbol* method_type = java_lang_invoke_MethodType::as_signature(method_type_oop(), true);
+
+  Handle impl_method_member_oop(THREAD, JNIHandles::resolve_non_null(implMethodMember));
+  assert(java_lang_invoke_MemberName::is_method(impl_method_member_oop()), "must be");
+  Method* m = java_lang_invoke_MemberName::vmtarget(impl_method_member_oop());
+
+  Handle instantiated_method_type_oop(THREAD, JNIHandles::resolve_non_null(instantiatedMethodType));
+  Symbol* instantiated_method_type = java_lang_invoke_MethodType::as_signature(instantiated_method_type_oop(), true);
+
+  SystemDictionaryShared::add_lambda_proxy_class(caller_ik, lambda_ik, invoked_name, invoked_type,
+                                                 method_type, m, instantiated_method_type);
+
+JVM_END
+
+JVM_ENTRY(jclass, JVM_LookupLambdaProxyClassFromArchive(JNIEnv* env,
+                                                        jclass caller,
+                                                        jstring invokedName,
+                                                        jobject invokedType,
+                                                        jobject methodType,
+                                                        jobject implMethodMember,
+                                                        jobject instantiatedMethodType,
+                                                        jboolean initialize))
+  JVMWrapper("JVM_LookupLambdaProxyClassFromArchive");
+  if (!DynamicArchive::is_mapped()) {
+    return NULL;
+  }
+
+  if (invokedName == NULL || invokedType == NULL || methodType == NULL ||
+      implMethodMember == NULL || instantiatedMethodType == NULL) {
+    THROW_(vmSymbols::java_lang_NullPointerException(), NULL);
+  }
+
+  Klass* caller_k = java_lang_Class::as_Klass(JNIHandles::resolve(caller));
+  InstanceKlass* caller_ik = InstanceKlass::cast(caller_k);
+  if (!caller_ik->is_shared()) {
+    // there won't be a shared lambda class if the caller_ik is not in the shared archive.
+    return NULL;
+  }
+
+  Symbol* invoked_name = java_lang_String::as_symbol(JNIHandles::resolve_non_null(invokedName));
+  Handle invoked_type_oop(THREAD, JNIHandles::resolve_non_null(invokedType));
+  Symbol* invoked_type = java_lang_invoke_MethodType::as_signature(invoked_type_oop(), true);
+
+  Handle method_type_oop(THREAD, JNIHandles::resolve_non_null(methodType));
+  Symbol* method_type = java_lang_invoke_MethodType::as_signature(method_type_oop(), true);
+
+  Handle impl_method_member_oop(THREAD, JNIHandles::resolve_non_null(implMethodMember));
+  assert(java_lang_invoke_MemberName::is_method(impl_method_member_oop()), "must be");
+  Method* m = java_lang_invoke_MemberName::vmtarget(impl_method_member_oop());
+
+  Handle instantiated_method_type_oop(THREAD, JNIHandles::resolve_non_null(instantiatedMethodType));
+  Symbol* instantiated_method_type = java_lang_invoke_MethodType::as_signature(instantiated_method_type_oop(), true);
+
+  InstanceKlass* lambda_ik = SystemDictionaryShared::get_shared_lambda_proxy_class(caller_ik, invoked_name, invoked_type,
+                                                                                   method_type, m, instantiated_method_type);
+  jclass jcls = NULL;
+  if (lambda_ik != NULL) {
+    InstanceKlass* loaded_lambda = SystemDictionaryShared::prepare_shared_lambda_proxy_class(lambda_ik, caller_ik, initialize, THREAD);
+    jcls = loaded_lambda == NULL ? NULL : (jclass) JNIHandles::make_local(env, loaded_lambda->java_mirror());
+  }
+  return jcls;
+JVM_END
+
+JVM_ENTRY(jboolean, JVM_IsCDSDumpingEnabled(JNIEnv* env))
+    JVMWrapper("JVM_IsCDSDumpingEnable");
+    return DynamicDumpSharedSpaces;
+JVM_END
+
+JVM_ENTRY(jboolean, JVM_IsCDSSharingEnabled(JNIEnv* env))
+    JVMWrapper("JVM_IsCDSSharingEnable");
+    return UseSharedSpaces;
+JVM_END
+
 JVM_ENTRY_NO_ENV(jlong, JVM_GetRandomSeedForCDSDump())
   JVMWrapper("JVM_GetRandomSeedForCDSDump");
   if (DumpSharedSpaces) {
     const char* release = Abstract_VM_Version::vm_release();
     const char* dbg_level = Abstract_VM_Version::jdk_debug_level();
diff a/src/hotspot/share/prims/jvmtiRedefineClasses.cpp b/src/hotspot/share/prims/jvmtiRedefineClasses.cpp
--- a/src/hotspot/share/prims/jvmtiRedefineClasses.cpp
+++ b/src/hotspot/share/prims/jvmtiRedefineClasses.cpp
@@ -707,89 +707,97 @@
   char* astr = (*(Symbol**)a)->as_C_string();
   char* bstr = (*(Symbol**)b)->as_C_string();
   return strcmp(astr, bstr);
 }
 
-static jvmtiError check_nest_attributes(InstanceKlass* the_class,
-                                        InstanceKlass* scratch_class) {
-  // Check whether the class NestHost attribute has been changed.
-  Thread* thread = Thread::current();
-  ResourceMark rm(thread);
-  u2 the_nest_host_idx = the_class->nest_host_index();
-  u2 scr_nest_host_idx = scratch_class->nest_host_index();
-
-  if (the_nest_host_idx != 0 && scr_nest_host_idx != 0) {
-    Symbol* the_sym = the_class->constants()->klass_name_at(the_nest_host_idx);
-    Symbol* scr_sym = scratch_class->constants()->klass_name_at(scr_nest_host_idx);
-    if (the_sym != scr_sym) {
-      log_trace(redefine, class, nestmates)
-        ("redefined class %s attribute change error: NestHost class: %s replaced with: %s",
-         the_class->external_name(), the_sym->as_C_string(), scr_sym->as_C_string());
-      return JVMTI_ERROR_UNSUPPORTED_REDEFINITION_CLASS_ATTRIBUTE_CHANGED;
-    }
-  } else if ((the_nest_host_idx == 0) ^ (scr_nest_host_idx == 0)) {
-    const char* action_str = (the_nest_host_idx != 0) ? "removed" : "added";
-    log_trace(redefine, class, nestmates)
-      ("redefined class %s attribute change error: NestHost attribute %s",
-       the_class->external_name(), action_str);
-    return JVMTI_ERROR_UNSUPPORTED_REDEFINITION_CLASS_ATTRIBUTE_CHANGED;
-  }
-
-  // Check whether the class NestMembers attribute has been changed.
-  Array<u2>* the_nest_members = the_class->nest_members();
-  Array<u2>* scr_nest_members = scratch_class->nest_members();
-  bool the_members_exists = the_nest_members != Universe::the_empty_short_array();
-  bool scr_members_exists = scr_nest_members != Universe::the_empty_short_array();
-
-  int members_len = the_nest_members->length();
-  if (the_members_exists && scr_members_exists) {
-    if (members_len != scr_nest_members->length()) {
-      log_trace(redefine, class, nestmates)
-        ("redefined class %s attribute change error: NestMember len=%d changed to len=%d",
-         the_class->external_name(), members_len, scr_nest_members->length());
+// The caller must have an active ResourceMark.
+static jvmtiError check_attribute_arrays(const char* attr_name,
+           InstanceKlass* the_class, InstanceKlass* scratch_class,
+           Array<u2>* the_array, Array<u2>* scr_array) {
+  bool the_array_exists = the_array != Universe::the_empty_short_array();
+  bool scr_array_exists = scr_array != Universe::the_empty_short_array();
+
+  int array_len = the_array->length();
+  if (the_array_exists && scr_array_exists) {
+    if (array_len != scr_array->length()) {
+      log_trace(redefine, class)
+        ("redefined class %s attribute change error: %s len=%d changed to len=%d",
+         the_class->external_name(), attr_name, array_len, scr_array->length());
       return JVMTI_ERROR_UNSUPPORTED_REDEFINITION_CLASS_ATTRIBUTE_CHANGED;
     }
 
-    // The order of entries in the NestMembers array is not specified so we
+    // The order of entries in the attribute array is not specified so we
     // have to explicitly check for the same contents. We do this by copying
     // the referenced symbols into their own arrays, sorting them and then
     // comparing each element pair.
 
-    Symbol** the_syms = NEW_RESOURCE_ARRAY_RETURN_NULL(Symbol*, members_len);
-    Symbol** scr_syms = NEW_RESOURCE_ARRAY_RETURN_NULL(Symbol*, members_len);
+    Symbol** the_syms = NEW_RESOURCE_ARRAY_RETURN_NULL(Symbol*, array_len);
+    Symbol** scr_syms = NEW_RESOURCE_ARRAY_RETURN_NULL(Symbol*, array_len);
 
     if (the_syms == NULL || scr_syms == NULL) {
       return JVMTI_ERROR_OUT_OF_MEMORY;
     }
 
-    for (int i = 0; i < members_len; i++) {
-      int the_cp_index = the_nest_members->at(i);
-      int scr_cp_index = scr_nest_members->at(i);
+    for (int i = 0; i < array_len; i++) {
+      int the_cp_index = the_array->at(i);
+      int scr_cp_index = scr_array->at(i);
       the_syms[i] = the_class->constants()->klass_name_at(the_cp_index);
       scr_syms[i] = scratch_class->constants()->klass_name_at(scr_cp_index);
     }
 
-    qsort(the_syms, members_len, sizeof(Symbol*), symcmp);
-    qsort(scr_syms, members_len, sizeof(Symbol*), symcmp);
+    qsort(the_syms, array_len, sizeof(Symbol*), symcmp);
+    qsort(scr_syms, array_len, sizeof(Symbol*), symcmp);
 
-    for (int i = 0; i < members_len; i++) {
+    for (int i = 0; i < array_len; i++) {
       if (the_syms[i] != scr_syms[i]) {
-        log_trace(redefine, class, nestmates)
-          ("redefined class %s attribute change error: NestMembers[%d]: %s changed to %s",
-           the_class->external_name(), i, the_syms[i]->as_C_string(), scr_syms[i]->as_C_string());
+        log_trace(redefine, class)
+          ("redefined class %s attribute change error: %s[%d]: %s changed to %s",
+           the_class->external_name(), attr_name, i,
+           the_syms[i]->as_C_string(), scr_syms[i]->as_C_string());
         return JVMTI_ERROR_UNSUPPORTED_REDEFINITION_CLASS_ATTRIBUTE_CHANGED;
       }
     }
-  } else if (the_members_exists ^ scr_members_exists) {
-    const char* action_str = (the_members_exists) ? "removed" : "added";
+  } else if (the_array_exists ^ scr_array_exists) {
+    const char* action_str = (the_array_exists) ? "removed" : "added";
+    log_trace(redefine, class)
+      ("redefined class %s attribute change error: %s attribute %s",
+       the_class->external_name(), attr_name, action_str);
+    return JVMTI_ERROR_UNSUPPORTED_REDEFINITION_CLASS_ATTRIBUTE_CHANGED;
+  }
+  return JVMTI_ERROR_NONE;
+}
+
+static jvmtiError check_nest_attributes(InstanceKlass* the_class,
+                                        InstanceKlass* scratch_class) {
+  // Check whether the class NestHost attribute has been changed.
+  Thread* thread = Thread::current();
+  ResourceMark rm(thread);
+  u2 the_nest_host_idx = the_class->nest_host_index();
+  u2 scr_nest_host_idx = scratch_class->nest_host_index();
+
+  if (the_nest_host_idx != 0 && scr_nest_host_idx != 0) {
+    Symbol* the_sym = the_class->constants()->klass_name_at(the_nest_host_idx);
+    Symbol* scr_sym = scratch_class->constants()->klass_name_at(scr_nest_host_idx);
+    if (the_sym != scr_sym) {
+      log_trace(redefine, class, nestmates)
+        ("redefined class %s attribute change error: NestHost class: %s replaced with: %s",
+         the_class->external_name(), the_sym->as_C_string(), scr_sym->as_C_string());
+      return JVMTI_ERROR_UNSUPPORTED_REDEFINITION_CLASS_ATTRIBUTE_CHANGED;
+    }
+  } else if ((the_nest_host_idx == 0) ^ (scr_nest_host_idx == 0)) {
+    const char* action_str = (the_nest_host_idx != 0) ? "removed" : "added";
     log_trace(redefine, class, nestmates)
-      ("redefined class %s attribute change error: NestMembers attribute %s",
+      ("redefined class %s attribute change error: NestHost attribute %s",
        the_class->external_name(), action_str);
     return JVMTI_ERROR_UNSUPPORTED_REDEFINITION_CLASS_ATTRIBUTE_CHANGED;
   }
 
-  return JVMTI_ERROR_NONE;
+  // Check whether the class NestMembers attribute has been changed.
+  return check_attribute_arrays("NestMembers",
+                                the_class, scratch_class,
+                                the_class->nest_members(),
+                                scratch_class->nest_members());
 }
 
 // Return an error status if the class Record attribute was changed.
 static jvmtiError check_record_attribute(InstanceKlass* the_class, InstanceKlass* scratch_class) {
   // Get lists of record components.
@@ -853,64 +861,17 @@
 }
 
 
 static jvmtiError check_permitted_subclasses_attribute(InstanceKlass* the_class,
                                                        InstanceKlass* scratch_class) {
-  // Check whether the class PermittedSubclasses attribute has been changed.
   Thread* thread = Thread::current();
   ResourceMark rm(thread);
-  Array<u2>* the_permitted_subclasses = the_class->permitted_subclasses();
-  Array<u2>* scr_permitted_subclasses = scratch_class->permitted_subclasses();
-  bool the_subclasses_exist = the_permitted_subclasses != Universe::the_empty_short_array();
-  bool scr_subclasses_exist = scr_permitted_subclasses != Universe::the_empty_short_array();
-  int subclasses_len = the_permitted_subclasses->length();
-  if (the_subclasses_exist && scr_subclasses_exist) {
-    if (subclasses_len != scr_permitted_subclasses->length()) {
-      log_trace(redefine, class, sealed)
-        ("redefined class %s attribute change error: PermittedSubclasses len=%d changed to len=%d",
-         the_class->external_name(), subclasses_len, scr_permitted_subclasses->length());
-      return JVMTI_ERROR_UNSUPPORTED_REDEFINITION_CLASS_ATTRIBUTE_CHANGED;
-    }
-
-    // The order of entries in the PermittedSubclasses array is not specified so
-    // we have to explicitly check for the same contents. We do this by copying
-    // the referenced symbols into their own arrays, sorting them and then
-    // comparing each element pair.
-
-    Symbol** the_syms = NEW_RESOURCE_ARRAY_RETURN_NULL(Symbol*, subclasses_len);
-    Symbol** scr_syms = NEW_RESOURCE_ARRAY_RETURN_NULL(Symbol*, subclasses_len);
-
-    if (the_syms == NULL || scr_syms == NULL) {
-      return JVMTI_ERROR_OUT_OF_MEMORY;
-    }
-
-    for (int i = 0; i < subclasses_len; i++) {
-      int the_cp_index = the_permitted_subclasses->at(i);
-      int scr_cp_index = scr_permitted_subclasses->at(i);
-      the_syms[i] = the_class->constants()->klass_name_at(the_cp_index);
-      scr_syms[i] = scratch_class->constants()->klass_name_at(scr_cp_index);
-    }
-
-    qsort(the_syms, subclasses_len, sizeof(Symbol*), symcmp);
-    qsort(scr_syms, subclasses_len, sizeof(Symbol*), symcmp);
-
-    for (int i = 0; i < subclasses_len; i++) {
-      if (the_syms[i] != scr_syms[i]) {
-        log_trace(redefine, class, sealed)
-          ("redefined class %s attribute change error: PermittedSubclasses[%d]: %s changed to %s",
-           the_class->external_name(), i, the_syms[i]->as_C_string(), scr_syms[i]->as_C_string());
-        return JVMTI_ERROR_UNSUPPORTED_REDEFINITION_CLASS_ATTRIBUTE_CHANGED;
-      }
-    }
-  } else if (the_subclasses_exist ^ scr_subclasses_exist) {
-    const char* action_str = (the_subclasses_exist) ? "removed" : "added";
-    log_trace(redefine, class, sealed)
-      ("redefined class %s attribute change error: PermittedSubclasses attribute %s",
-       the_class->external_name(), action_str);
-    return JVMTI_ERROR_UNSUPPORTED_REDEFINITION_CLASS_ATTRIBUTE_CHANGED;
-  }
-
+
+  // Check whether the class PermittedSubclasses attribute has been changed.
+  return check_attribute_arrays("PermittedSubclasses",
+                                the_class, scratch_class,
+                                the_class->permitted_subclasses(),
   return JVMTI_ERROR_NONE;
 }
 
 static bool can_add_or_delete(Method* m) {
       // Compatibility mode
diff a/src/hotspot/share/runtime/deoptimization.cpp b/src/hotspot/share/runtime/deoptimization.cpp
--- a/src/hotspot/share/runtime/deoptimization.cpp
+++ b/src/hotspot/share/runtime/deoptimization.cpp
@@ -687,13 +687,21 @@
   Events::log_deopt_message(thread, "DEOPT UNPACKING pc=" INTPTR_FORMAT " sp=" INTPTR_FORMAT " mode %d",
               p2i(stub_frame.pc()), p2i(stub_frame.sp()), exec_mode);
 
   UnrollBlock* info = array->unroll_block();
 
+  // We set the last_Java frame. But the stack isn't really parsable here. So we
+  // clear it to make sure JFR understands not to try and walk stacks from events
+  // in here.
+  intptr_t* sp = thread->frame_anchor()->last_Java_sp();
+  thread->frame_anchor()->set_last_Java_sp(NULL);
+
   // Unpack the interpreter frames and any adapter frame (c2 only) we might create.
   array->unpack_to_stack(stub_frame, exec_mode, info->caller_actual_parameters());
 
+  thread->frame_anchor()->set_last_Java_sp(sp);
+
   BasicType bt = info->return_type();
 
   // If we have an exception pending, claim that the return type is an oop
   // so the deopt_blob does not overwrite the exception_oop.
 
@@ -825,11 +833,10 @@
       is_top_frame = false;
     }
   }
 #endif /* !PRODUCT */
 
-
   return bt;
 JRT_END
 
 class DeoptimizeMarkedClosure : public HandshakeClosure {
  public:
diff a/src/hotspot/share/runtime/fieldDescriptor.cpp b/src/hotspot/share/runtime/fieldDescriptor.cpp
--- a/src/hotspot/share/runtime/fieldDescriptor.cpp
+++ b/src/hotspot/share/runtime/fieldDescriptor.cpp
@@ -207,11 +207,11 @@
       st->print(" ");
       NOT_LP64(as_int = obj->int_field(offset()));
       if (obj->obj_field(offset()) != NULL) {
         obj->obj_field(offset())->print_value_on(st);
       } else {
-        st->print_cr("NULL");
+        st->print("NULL");
       }
       break;
     default:
       ShouldNotReachHere();
       break;
diff a/src/hotspot/share/runtime/thread.cpp b/src/hotspot/share/runtime/thread.cpp
--- a/src/hotspot/share/runtime/thread.cpp
+++ b/src/hotspot/share/runtime/thread.cpp
@@ -4739,10 +4739,26 @@
 
   // cannot assert on lack of success here; see above comment
   return the_owner;
 }
 
+class PrintOnClosure : public ThreadClosure {
+private:
+  outputStream* _st;
+
+public:
+  PrintOnClosure(outputStream* st) :
+      _st(st) {}
+
+  virtual void do_thread(Thread* thread) {
+    if (thread != NULL) {
+      thread->print_on(_st);
+      _st->cr();
+    }
+  }
+};
+
 // Threads::print_on() is called at safepoint by VM_PrintThreads operation.
 void Threads::print_on(outputStream* st, bool print_stacks,
                        bool internal_format, bool print_concurrent_locks,
                        bool print_extended_info) {
   char buf[32];
@@ -4781,18 +4797,14 @@
       concurrent_locks.print_locks_on(p, st);
     }
 #endif // INCLUDE_SERVICES
   }
 
-  VMThread::vm_thread()->print_on(st);
-  st->cr();
-  Universe::heap()->print_gc_threads_on(st);
-  WatcherThread* wt = WatcherThread::watcher_thread();
-  if (wt != NULL) {
-    wt->print_on(st);
-    st->cr();
-  }
+  PrintOnClosure cl(st);
+  cl.do_thread(VMThread::vm_thread());
+  Universe::heap()->gc_threads_do(&cl);
+  cl.do_thread(WatcherThread::watcher_thread());
 
   st->flush();
 }
 
 void Threads::print_on_error(Thread* this_thread, outputStream* st, Thread* current, char* buf,
diff a/src/hotspot/share/runtime/thread.hpp b/src/hotspot/share/runtime/thread.hpp
--- a/src/hotspot/share/runtime/thread.hpp
+++ b/src/hotspot/share/runtime/thread.hpp
@@ -2037,13 +2037,15 @@
  private:
   JvmtiThreadState *_jvmti_thread_state;
 
   // Used by the interpreter in fullspeed mode for frame pop, method
   // entry, method exit and single stepping support. This field is
-  // only set to non-zero by the VM_EnterInterpOnlyMode VM operation.
-  // It can be set to zero asynchronously (i.e., without a VM operation
-  // or a lock) so we have to be very careful.
+  // only set to non-zero at a safepoint or using a direct handshake
+  // (see EnterInterpOnlyModeClosure).
+  // It can be set to zero asynchronously to this threads execution (i.e., without
+  // safepoint/handshake or a lock) so we have to be very careful.
+  // Accesses by other threads are synchronized using JvmtiThreadState_lock though.
   int               _interp_only_mode;
 
  public:
   // used by the interpreter for fullspeed debugging support (see above)
   static ByteSize interp_only_mode_offset() { return byte_offset_of(JavaThread, _interp_only_mode); }
diff a/src/hotspot/share/runtime/vmOperations.hpp b/src/hotspot/share/runtime/vmOperations.hpp
--- a/src/hotspot/share/runtime/vmOperations.hpp
+++ b/src/hotspot/share/runtime/vmOperations.hpp
@@ -88,11 +88,10 @@
   template(GetFrameCount)                         \
   template(GetFrameLocation)                      \
   template(ChangeBreakpoints)                     \
   template(GetOrSetLocal)                         \
   template(GetCurrentLocation)                    \
-  template(EnterInterpOnlyMode)                   \
   template(ChangeSingleStep)                      \
   template(HeapWalkOperation)                     \
   template(HeapIterateOperation)                  \
   template(ReportJavaOutOfMemory)                 \
   template(JFRCheckpoint)                         \
diff a/src/hotspot/share/runtime/vmStructs.cpp b/src/hotspot/share/runtime/vmStructs.cpp
--- a/src/hotspot/share/runtime/vmStructs.cpp
+++ b/src/hotspot/share/runtime/vmStructs.cpp
@@ -468,20 +468,18 @@
                                                                                                                                      \
   /********************/                                                                                                             \
   /* SystemDictionary */                                                                                                             \
   /********************/                                                                                                             \
                                                                                                                                      \
-     static_field(SystemDictionary,            _system_loader_lock_obj,                       oop)                                   \
      static_field(SystemDictionary,            WK_KLASS(Object_klass),                        InstanceKlass*)                        \
      static_field(SystemDictionary,            WK_KLASS(String_klass),                        InstanceKlass*)                        \
      static_field(SystemDictionary,            WK_KLASS(Class_klass),                         InstanceKlass*)                        \
      static_field(SystemDictionary,            WK_KLASS(ClassLoader_klass),                   InstanceKlass*)                        \
      static_field(SystemDictionary,            WK_KLASS(System_klass),                        InstanceKlass*)                        \
      static_field(SystemDictionary,            WK_KLASS(Thread_klass),                        InstanceKlass*)                        \
      static_field(SystemDictionary,            WK_KLASS(ThreadGroup_klass),                   InstanceKlass*)                        \
      static_field(SystemDictionary,            WK_KLASS(MethodHandle_klass),                  InstanceKlass*)                        \
-     static_field(SystemDictionary,            _java_system_loader,                           oop)                                   \
                                                                                                                                      \
   /*************/                                                                                                                    \
   /* vmSymbols */                                                                                                                    \
   /*************/                                                                                                                    \
                                                                                                                                      \
diff a/src/hotspot/share/services/diagnosticCommand.cpp b/src/hotspot/share/services/diagnosticCommand.cpp
--- a/src/hotspot/share/services/diagnosticCommand.cpp
+++ b/src/hotspot/share/services/diagnosticCommand.cpp
@@ -502,21 +502,36 @@
 #if INCLUDE_SERVICES // Heap dumping/inspection supported
 HeapDumpDCmd::HeapDumpDCmd(outputStream* output, bool heap) :
                            DCmdWithParser(output, heap),
   _filename("filename","Name of the dump file", "STRING",true),
   _all("-all", "Dump all objects, including unreachable objects",
-       "BOOLEAN", false, "false") {
+       "BOOLEAN", false, "false"),
+  _gzip("-gz", "If specified, the heap dump is written in gzipped format "
+               "using the given compression level. 1 (recommended) is the fastest, "
+               "9 the strongest compression.", "INT", false, "1") {
   _dcmdparser.add_dcmd_option(&_all);
   _dcmdparser.add_dcmd_argument(&_filename);
+  _dcmdparser.add_dcmd_option(&_gzip);
 }
 
 void HeapDumpDCmd::execute(DCmdSource source, TRAPS) {
+  jlong level = -1; // -1 means no compression.
+
+  if (_gzip.is_set()) {
+    level = _gzip.value();
+
+    if (level < 1 || level > 9) {
+      output()->print_cr("Compression level out of range (1-9): " JLONG_FORMAT, level);
+      return;
+    }
+  }
+
   // Request a full GC before heap dump if _all is false
   // This helps reduces the amount of unreachable objects in the dump
   // and makes it easier to browse.
   HeapDumper dumper(!_all.value() /* request GC if _all is false*/);
-  dumper.dump(_filename.value(), output());
+  dumper.dump(_filename.value(), output(), (int) level);
 }
 
 int HeapDumpDCmd::num_arguments() {
   ResourceMark rm;
   HeapDumpDCmd* dcmd = new HeapDumpDCmd(NULL, false);
diff a/src/hotspot/share/services/diagnosticCommand.hpp b/src/hotspot/share/services/diagnosticCommand.hpp
--- a/src/hotspot/share/services/diagnosticCommand.hpp
+++ b/src/hotspot/share/services/diagnosticCommand.hpp
@@ -328,10 +328,11 @@
 // See also: dump_heap in attachListener.cpp
 class HeapDumpDCmd : public DCmdWithParser {
 protected:
   DCmdArgument<char*> _filename;
   DCmdArgument<bool>  _all;
+  DCmdArgument<jlong> _gzip;
 public:
   HeapDumpDCmd(outputStream* output, bool heap);
   static const char* name() {
     return "GC.heap_dump";
   }
diff a/src/hotspot/share/services/heapDumper.cpp b/src/hotspot/share/services/heapDumper.cpp
--- a/src/hotspot/share/services/heapDumper.cpp
+++ b/src/hotspot/share/services/heapDumper.cpp
@@ -30,10 +30,11 @@
 #include "classfile/symbolTable.hpp"
 #include "classfile/systemDictionary.hpp"
 #include "classfile/vmSymbols.hpp"
 #include "gc/shared/gcLocker.hpp"
 #include "gc/shared/gcVMOperations.hpp"
+#include "gc/shared/workgroup.hpp"
 #include "jfr/jfrEvents.hpp"
 #include "memory/allocation.inline.hpp"
 #include "memory/resourceArea.hpp"
 #include "memory/universe.hpp"
 #include "oops/objArrayKlass.hpp"
@@ -50,10 +51,11 @@
 #include "runtime/threadSMR.hpp"
 #include "runtime/vframe.hpp"
 #include "runtime/vmThread.hpp"
 #include "runtime/vmOperations.hpp"
 #include "services/heapDumper.hpp"
+#include "services/heapDumperCompression.hpp"
 #include "services/threadService.hpp"
 #include "utilities/macros.hpp"
 #include "utilities/ostream.hpp"
 
 /*
@@ -375,64 +377,58 @@
 enum {
   STACK_TRACE_ID = 1,
   INITIAL_CLASS_COUNT = 200
 };
 
-// Supports I/O operations on a dump file
+// Supports I/O operations for a dump
 
 class DumpWriter : public StackObj {
  private:
   enum {
-    io_buffer_max_size = 8*M,
-    io_buffer_min_size = 64*K,
+    io_buffer_max_size = 1*M,
+    io_buffer_max_waste = 10*K,
     dump_segment_header_size = 9
   };
 
-  int _fd;              // file descriptor (-1 if dump file not open)
-  julong _bytes_written; // number of byte written to dump file
-
   char* _buffer;    // internal buffer
   size_t _size;
   size_t _pos;
 
   bool _in_dump_segment; // Are we currently in a dump segment?
   bool _is_huge_sub_record; // Are we writing a sub-record larger than the buffer size?
   DEBUG_ONLY(size_t _sub_record_left;) // The bytes not written for the current sub-record.
   DEBUG_ONLY(bool _sub_record_ended;) // True if we have called the end_sub_record().
 
-  char* _error;   // error message when I/O fails
-
-  void set_file_descriptor(int fd)              { _fd = fd; }
-  int file_descriptor() const                   { return _fd; }
+  CompressionBackend _backend; // Does the actual writing.
 
-  bool is_open() const                          { return file_descriptor() >= 0; }
   void flush();
 
   char* buffer() const                          { return _buffer; }
   size_t buffer_size() const                    { return _size; }
   size_t position() const                       { return _pos; }
   void set_position(size_t pos)                 { _pos = pos; }
 
-  void set_error(const char* error)             { _error = (char*)os::strdup(error); }
+  // Can be called if we have enough room in the buffer.
+  void write_fast(void* s, size_t len);
 
-  // all I/O go through this function
-  void write_internal(void* s, size_t len);
+  // Returns true if we have enough room in the buffer for 'len' bytes.
+  bool can_write_fast(size_t len);
 
  public:
-  DumpWriter(const char* path);
-  ~DumpWriter();
+  // Takes ownership of the writer and compressor.
+  DumpWriter(AbstractWriter* writer, AbstractCompressor* compressor);
 
-  void close();
+  ~DumpWriter();
 
   // total number of bytes written to the disk
-  julong bytes_written() const          { return _bytes_written; }
+  julong bytes_written() const          { return (julong) _backend.get_written(); }
 
-  char* error() const                   { return _error; }
+  char const* error() const             { return _backend.error(); }
 
   // writer functions
   void write_raw(void* s, size_t len);
-  void write_u1(u1 x)                   { write_raw((void*)&x, 1); }
+  void write_u1(u1 x);
   void write_u2(u2 x);
   void write_u4(u4 x);
   void write_u8(u8 x);
   void write_objectID(oop o);
   void write_symbolID(Symbol* o);
@@ -443,120 +439,95 @@
   void start_sub_record(u1 tag, u4 len);
   // Ends the current sub-record.
   void end_sub_record();
   // Finishes the current dump segment if not already finished.
   void finish_dump_segment();
-};
-
-DumpWriter::DumpWriter(const char* path) : _fd(-1), _bytes_written(0), _pos(0),
-                                           _in_dump_segment(false), _error(NULL) {
-  // try to allocate an I/O buffer of io_buffer_size. If there isn't
-  // sufficient memory then reduce size until we can allocate something.
-  _size = io_buffer_max_size;
-  do {
-    _buffer = (char*)os::malloc(_size, mtInternal);
-    if (_buffer == NULL) {
-      _size = _size >> 1;
-    }
-  } while (_buffer == NULL && _size >= io_buffer_min_size);
-
-  if (_buffer == NULL) {
-    set_error("Could not allocate buffer memory for heap dump");
-  } else {
+
+  // Called by threads used for parallel writing.
+  void writer_loop()                    { _backend.thread_loop(false); }
+  // Called when finished to release the threads.
+  void deactivate()                     { flush(); _backend.deactivate(); }
     _fd = os::create_binary_file(path, false);    // don't replace existing file
 
-    // if the open failed we record the error
-    if (_fd < 0) {
-      set_error(os::strerror(errno));
-    }
-  }
+// Check for error after constructing the object and destroy it in case of an error.
+DumpWriter::DumpWriter(AbstractWriter* writer, AbstractCompressor* compressor) :
+  _buffer(NULL),
+  _size(0),
+  _pos(0),
+  _in_dump_segment(false),
+  _backend(writer, compressor, io_buffer_max_size, io_buffer_max_waste) {
+  flush();
 }
 
 DumpWriter::~DumpWriter() {
-  close();
-  os::free(_buffer);
-  os::free(_error);
+  flush();
 }
 
-// closes dump file (if open)
-void DumpWriter::close() {
-  // flush and close dump file
-  if (is_open()) {
-    flush();
-    os::close(file_descriptor());
-    set_file_descriptor(-1);
-  }
-}
+void DumpWriter::write_fast(void* s, size_t len) {
+  assert(!_in_dump_segment || (_sub_record_left >= len), "sub-record too large");
+  assert(buffer_size() - position() >= len, "Must fit");
+  debug_only(_sub_record_left -= len);
 
-// write directly to the file
-void DumpWriter::write_internal(void* s, size_t len) {
-  if (is_open()) {
-    const char* pos = (char*)s;
-    ssize_t n = 0;
-    while (len > 0) {
-      uint tmp = (uint)MIN2(len, (size_t)INT_MAX);
-      n = os::write(file_descriptor(), pos, tmp);
-
-      if (n < 0) {
-        // EINTR cannot happen here, os::write will take care of that
-        set_error(os::strerror(errno));
-        os::close(file_descriptor());
-        set_file_descriptor(-1);
-        return;
-      }
+  memcpy(buffer() + position(), s, len);
+  set_position(position() + len);
+}
 
-      _bytes_written += n;
-      pos += n;
-      len -= n;
-    }
-  }
+bool DumpWriter::can_write_fast(size_t len) {
+  return buffer_size() - position() >= len;
 }
 
 // write raw bytes
 void DumpWriter::write_raw(void* s, size_t len) {
   assert(!_in_dump_segment || (_sub_record_left >= len), "sub-record too large");
   debug_only(_sub_record_left -= len);
 
-  // flush buffer to make room
-  if (len > buffer_size() - position()) {
-    assert(!_in_dump_segment || _is_huge_sub_record, "Cannot overflow in non-huge sub-record.");
-    flush();
-
-    // If larger than the buffer, just write it directly.
-    if (len > buffer_size()) {
-      write_internal(s, len);
+  // flush buffer to make room.
+  while (len > buffer_size() - position()) {
+    assert(!_in_dump_segment || _is_huge_sub_record,
+           "Cannot overflow in non-huge sub-record.");
 
-      return;
-    }
+    size_t to_write = buffer_size() - position();
+    memcpy(buffer() + position(), s, to_write);
+    s = (void*) ((char*) s + to_write);
+    len -= to_write;
+    set_position(position() + to_write);
+    flush();
   }
 
   memcpy(buffer() + position(), s, len);
   set_position(position() + len);
 }
 
 // flush any buffered bytes to the file
 void DumpWriter::flush() {
-  write_internal(buffer(), position());
-  set_position(0);
+  _backend.get_new_buffer(&_buffer, &_pos, &_size);
+}
+
+// Makes sure we inline the fast write into the write_u* functions. This is a big speedup.
+#define WRITE_KNOWN_TYPE(p, len) do { if (can_write_fast((len))) write_fast((p), (len)); \
+                                      else write_raw((p), (len)); } while (0)
+
+void DumpWriter::write_u1(u1 x) {
+  WRITE_KNOWN_TYPE((void*) &x, 1);
 }
 
 void DumpWriter::write_u2(u2 x) {
   u2 v;
   Bytes::put_Java_u2((address)&v, x);
-  write_raw((void*)&v, 2);
+  WRITE_KNOWN_TYPE((void*)&v, 2);
 }
 
 void DumpWriter::write_u4(u4 x) {
   u4 v;
   Bytes::put_Java_u4((address)&v, x);
-  write_raw((void*)&v, 4);
+  WRITE_KNOWN_TYPE((void*)&v, 4);
 }
 
 void DumpWriter::write_u8(u8 x) {
   u8 v;
   Bytes::put_Java_u8((address)&v, x);
-  write_raw((void*)&v, 8);
+  WRITE_KNOWN_TYPE((void*)&v, 8);
 }
 
 void DumpWriter::write_objectID(oop o) {
   address a = cast_from_oop<address>(o);
 #ifdef _LP64
@@ -595,11 +566,12 @@
 
     // Fix up the dump segment length if we haven't written a huge sub-record last
     // (in which case the segment length was already set to the correct value initially).
     if (!_is_huge_sub_record) {
       assert(position() > dump_segment_header_size, "Dump segment should have some content");
-      Bytes::put_Java_u4((address) (buffer() + 5), (u4) (position() - dump_segment_header_size));
+      Bytes::put_Java_u4((address) (buffer() + 5),
+                         (u4) (position() - dump_segment_header_size));
     }
 
     flush();
     _in_dump_segment = false;
   }
@@ -607,13 +579,14 @@
 
 void DumpWriter::start_sub_record(u1 tag, u4 len) {
   if (!_in_dump_segment) {
     if (position() > 0) {
       flush();
-      assert(position() == 0, "Must be at the start");
     }
 
+    assert(position() == 0, "Must be at the start");
+
     write_u1(HPROF_HEAP_DUMP_SEGMENT);
     write_u4(0); // timestamp
     // Will be fixed up later if we add more sub-records.  If this is a huge sub-record,
     // this is already the correct length, since we don't add more sub-records.
     write_u4(len);
@@ -1501,11 +1474,11 @@
     DumperSupport::dump_prim_array(writer(), typeArrayOop(o));
   }
 }
 
 // The VM operation that performs the heap dump
-class VM_HeapDumper : public VM_GC_Operation {
+class VM_HeapDumper : public VM_GC_Operation, public AbstractGangTask {
  private:
   static VM_HeapDumper* _global_dumper;
   static DumpWriter*    _global_writer;
   DumpWriter*           _local_writer;
   JavaThread*           _oome_thread;
@@ -1557,11 +1530,12 @@
  public:
   VM_HeapDumper(DumpWriter* writer, bool gc_before_heap_dump, bool oome) :
     VM_GC_Operation(0 /* total collections,      dummy, ignored */,
                     GCCause::_heap_dump /* GC Cause */,
                     0 /* total full collections, dummy, ignored */,
-                    gc_before_heap_dump) {
+                    gc_before_heap_dump),
+    AbstractGangTask("dump heap") {
     _local_writer = writer;
     _gc_before_heap_dump = gc_before_heap_dump;
     _klass_map = new (ResourceObj::C_HEAP, mtInternal) GrowableArray<Klass*>(INITIAL_CLASS_COUNT, true);
     _stack_traces = NULL;
     _num_threads = 0;
@@ -1588,12 +1562,14 @@
     delete _klass_map;
   }
 
   VMOp_Type type() const { return VMOp_HeapDumper; }
   void doit();
+  void work(uint worker_id);
 };
 
+
 VM_HeapDumper* VM_HeapDumper::_global_dumper = NULL;
 DumpWriter*    VM_HeapDumper::_global_writer = NULL;
 
 bool VM_HeapDumper::skip_operation() const {
   return false;
@@ -1818,12 +1794,30 @@
   // At this point we should be the only dumper active, so
   // the following should be safe.
   set_global_dumper();
   set_global_writer();
 
+  WorkGang* gang = ch->get_safepoint_workers();
+
+  if (gang == NULL) {
+    work(0);
+  } else {
+    gang->run_task(this, gang->active_workers(), true);
+  }
+
+  // Now we clear the global variables, so that a future dumper can run.
+  clear_global_dumper();
+  clear_global_writer();
+}
+
+void VM_HeapDumper::work(uint worker_id) {
+  if (!Thread::current()->is_VM_thread()) {
+    writer()->writer_loop();
+    return;
+  }
+
   // Write the file header - we always use 1.0.2
-  size_t used = ch->used();
   const char* header = "JAVA PROFILE 1.0.2";
 
   // header is few bytes long - no chance to overflow int
   writer()->write_raw((void*)header, (int)strlen(header));
   writer()->write_u1(0); // terminator
@@ -1882,13 +1876,12 @@
   ClassLoaderData::the_null_class_loader_data()->classes_do(&class_dumper);
 
   // Writes the HPROF_HEAP_DUMP_END record.
   DumperSupport::end_of_dump(writer());
 
-  // Now we clear the global variables, so that a future dumper might run.
-  clear_global_dumper();
-  clear_global_writer();
+  // We are done with writing. Release the worker threads.
+  writer()->deactivate();
 }
 
 void VM_HeapDumper::dump_stack_traces() {
   // write a HPROF_TRACE record without any frames to be referenced as object alloc sites
   DumperSupport::write_header(writer(), HPROF_TRACE, 3*sizeof(u4));
@@ -1900,10 +1893,11 @@
   int frame_serial_num = 0;
   for (JavaThreadIteratorWithHandle jtiwh; JavaThread *thread = jtiwh.next(); ) {
     oop threadObj = thread->threadObj();
     if (threadObj != NULL && !thread->is_exiting() && !thread->is_hidden_from_external_view()) {
       // dump thread stack trace
+      ResourceMark rm;
       ThreadStackTrace* stack_trace = new ThreadStackTrace(thread, false);
       stack_trace->dump_stack_at_safepoint(-1);
       _stack_traces[_num_threads++] = stack_trace;
 
       // write HPROF_FRAME records for this thread's stack trace
@@ -1942,11 +1936,11 @@
     }
   }
 }
 
 // dump the heap to given path.
-int HeapDumper::dump(const char* path, outputStream* out) {
+int HeapDumper::dump(const char* path, outputStream* out, int compression) {
   assert(path != NULL && strlen(path) > 0, "path missing");
 
   // print message in interactive case
   if (out != NULL) {
     out->print_cr("Dumping heap to %s ...", path);
@@ -1954,12 +1948,23 @@
   }
 
   // create JFR event
   EventHeapDump event;
 
-  // create the dump writer. If the file can be opened then bail
-  DumpWriter writer(path);
+  AbstractCompressor* compressor = NULL;
+
+  if (compression > 0) {
+    compressor = new (std::nothrow) GZipCompressor(compression);
+
+    if (compressor == NULL) {
+      set_error("Could not allocate gzip compressor");
+      return -1;
+    }
+  }
+
+  DumpWriter writer(new (std::nothrow) FileWriter(path), compressor);
+
   if (writer.error() != NULL) {
     set_error(writer.error());
     if (out != NULL) {
       out->print_cr("Unable to create %s: %s", path,
         (error() != NULL) ? error() : "reason unknown");
@@ -1974,12 +1979,11 @@
     dumper.doit();
   } else {
     VMThread::execute(&dumper);
   }
 
-  // close dump file and record any error that the writer may have encountered
-  writer.close();
+  // record any error that the writer may have encountered
   set_error(writer.error());
 
   // emit JFR event
   if (error() == NULL) {
     event.set_destination(path);
@@ -2022,11 +2026,11 @@
     return NULL;
   }
 }
 
 // set the error string
-void HeapDumper::set_error(char* error) {
+void HeapDumper::set_error(char const* error) {
   if (_error != NULL) {
     os::free(_error);
   }
   if (error == NULL) {
     _error = NULL;
diff a/src/java.base/share/classes/java/lang/invoke/InnerClassLambdaMetafactory.java b/src/java.base/share/classes/java/lang/invoke/InnerClassLambdaMetafactory.java
--- a/src/java.base/share/classes/java/lang/invoke/InnerClassLambdaMetafactory.java
+++ b/src/java.base/share/classes/java/lang/invoke/InnerClassLambdaMetafactory.java
@@ -1,7 +1,7 @@
 /*
- * Copyright (c) 2012, 2013, Oracle and/or its affiliates. All rights reserved.
+ * Copyright (c) 2012, 2020, Oracle and/or its affiliates. All rights reserved.
  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  *
  * This code is free software; you can redistribute it and/or modify it
  * under the terms of the GNU General Public License version 2 only, as
  * published by the Free Software Foundation.  Oracle designates this
@@ -244,10 +244,51 @@
                 throw new LambdaConversionException("Exception finding constructor", e);
             }
         }
     }
 
+    /**
+     * Spins the lambda proxy class.
+     *
+     * This first checks if a lambda proxy class can be loaded from CDS archive.
+     * Otherwise, generate the lambda proxy class. If CDS dumping is enabled, it
+     * registers the lambda proxy class for including into the CDS archive.
+     */
+    private Class<?> spinInnerClass() throws LambdaConversionException {
+        // include lambda proxy class in CDS archive at dump time
+        if (LambdaProxyClassArchive.isDumpArchive()) {
+            Class<?> innerClass = generateInnerClass();
+            LambdaProxyClassArchive.register(targetClass,
+                                             samMethodName,
+                                             invokedType,
+                                             samMethodType,
+                                             implMethod,
+                                             instantiatedMethodType,
+                                             isSerializable,
+                                             markerInterfaces,
+                                             additionalBridges,
+                                             innerClass);
+            return innerClass;
+        }
+
+        // load from CDS archive if present
+        Class<?> innerClass = LambdaProxyClassArchive.find(targetClass,
+                                                           samMethodName,
+                                                           invokedType,
+                                                           samMethodType,
+                                                           implMethod,
+                                                           instantiatedMethodType,
+                                                           isSerializable,
+                                                           markerInterfaces,
+                                                           additionalBridges,
+                                                           !disableEagerInitialization);
+        if (innerClass == null) {
+            innerClass = generateInnerClass();
+        }
+        return innerClass;
+    }
+
     /**
      * Generate a class file which implements the functional
      * interface, define and return the class.
      *
      * @implNote The class that is generated does not include signature
@@ -259,11 +300,11 @@
      *
      * @return a Class which implements the functional interface
      * @throws LambdaConversionException If properly formed functional interface
      * is not found
      */
-    private Class<?> spinInnerClass() throws LambdaConversionException {
+    private Class<?> generateInnerClass() throws LambdaConversionException {
         String[] interfaces;
         String samIntf = samBase.getName().replace('.', '/');
         boolean accidentallySerializable = !isSerializable && Serializable.class.isAssignableFrom(samBase);
         if (markerInterfaces.length == 0) {
             interfaces = new String[]{samIntf};
diff a/src/java.base/share/classes/java/lang/invoke/LambdaForm.java b/src/java.base/share/classes/java/lang/invoke/LambdaForm.java
--- a/src/java.base/share/classes/java/lang/invoke/LambdaForm.java
+++ b/src/java.base/share/classes/java/lang/invoke/LambdaForm.java
@@ -1,7 +1,7 @@
 /*
- * Copyright (c) 2011, 2018, Oracle and/or its affiliates. All rights reserved.
+ * Copyright (c) 2011, 2020, Oracle and/or its affiliates. All rights reserved.
  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  *
  * This code is free software; you can redistribute it and/or modify it
  * under the terms of the GNU General Public License version 2 only, as
  * published by the Free Software Foundation.  Oracle designates this
@@ -803,11 +803,11 @@
      * not differ in their optimized form.  Repeated versions
      * of machine would have the undesirable overheads of
      * (a) redundant compilation work and (b) extra I$ pressure.
      * To control repeated versions, we need to be ready to
      * erase details from LFs and move them into MH data,
-     * whevener those details are not relevant to significant
+     * whenever those details are not relevant to significant
      * optimization.  "Significant" means optimization of
      * code that is actually hot.
      *
      * Achieving this may require dynamic splitting of MHs, by replacing
      * a generic LF with a more specialized one, on the same MH,
@@ -897,11 +897,11 @@
     // The next few routines are called only from assert expressions
     // They verify that the built-in invokers process the correct raw data types.
     private static boolean argumentTypesMatch(String sig, Object[] av) {
         int arity = signatureArity(sig);
         assert(av.length == arity) : "av.length == arity: av.length=" + av.length + ", arity=" + arity;
-        assert(av[0] instanceof MethodHandle) : "av[0] not instace of MethodHandle: " + av[0];
+        assert(av[0] instanceof MethodHandle) : "av[0] not instance of MethodHandle: " + av[0];
         MethodHandle mh = (MethodHandle) av[0];
         MethodType mt = mh.type();
         assert(mt.parameterCount() == arity-1);
         for (int i = 0; i < av.length; i++) {
             Class<?> pt = (i == 0 ? MethodHandle.class : mt.parameterType(i-1));
diff a/src/java.base/share/classes/java/lang/invoke/MethodHandleNatives.java b/src/java.base/share/classes/java/lang/invoke/MethodHandleNatives.java
--- a/src/java.base/share/classes/java/lang/invoke/MethodHandleNatives.java
+++ b/src/java.base/share/classes/java/lang/invoke/MethodHandleNatives.java
@@ -1,7 +1,7 @@
 /*
- * Copyright (c) 2008, 2019, Oracle and/or its affiliates. All rights reserved.
+ * Copyright (c) 2008, 2020, Oracle and/or its affiliates. All rights reserved.
  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  *
  * This code is free software; you can redistribute it and/or modify it
  * under the terms of the GNU General Public License version 2 only, as
  * published by the Free Software Foundation.  Oracle designates this
@@ -508,11 +508,11 @@
     }
 
     /**
      * Obtain the method to link to the VarHandle operation.
      * This method is located here and not in Invokers to avoid
-     * intializing that and other classes early on in VM bootup.
+     * initializing that and other classes early on in VM bootup.
      */
     private static MemberName varHandleOperationLinkerMethod(String name,
                                                              MethodType mtype,
                                                              Object[] appendixResult) {
         // Get the signature method type
diff a/src/java.base/share/classes/java/lang/invoke/MethodHandles.java b/src/java.base/share/classes/java/lang/invoke/MethodHandles.java
--- a/src/java.base/share/classes/java/lang/invoke/MethodHandles.java
+++ b/src/java.base/share/classes/java/lang/invoke/MethodHandles.java
@@ -25,10 +25,11 @@
 
 package java.lang.invoke;
 
 import jdk.internal.access.JavaLangAccess;
 import jdk.internal.access.SharedSecrets;
+import jdk.internal.misc.Unsafe;
 import jdk.internal.misc.VM;
 import jdk.internal.module.IllegalAccessLogger;
 import jdk.internal.org.objectweb.asm.ClassReader;
 import jdk.internal.org.objectweb.asm.Opcodes;
 import jdk.internal.org.objectweb.asm.Type;
@@ -2241,11 +2242,12 @@
              */
             Class<?> defineClass(boolean initialize, Object classData) {
                 Class<?> lookupClass = lookup.lookupClass();
                 ClassLoader loader = lookupClass.getClassLoader();
                 ProtectionDomain pd = (loader != null) ? lookup.lookupClassProtectionDomain() : null;
-                Class<?> c = JLA.defineClass(loader, lookupClass, name, bytes, pd, initialize, classFlags, classData);
+                Class<?> c = SharedSecrets.getJavaLangAccess()
+                        .defineClass(loader, lookupClass, name, bytes, pd, initialize, classFlags, classData);
                 assert !isNestmate() || c.getNestHost() == lookupClass.getNestHost();
                 return c;
             }
 
             Lookup defineClassAsLookup(boolean initialize, Object classData) {
@@ -2261,11 +2263,11 @@
         }
 
         private ProtectionDomain lookupClassProtectionDomain() {
             ProtectionDomain pd = cachedProtectionDomain;
             if (pd == null) {
-                cachedProtectionDomain = pd = JLA.protectionDomain(lookupClass);
+                cachedProtectionDomain = pd = SharedSecrets.getJavaLangAccess().protectionDomain(lookupClass);
             }
             return pd;
         }
 
         // cached protection domain
@@ -2281,12 +2283,10 @@
          *  It can only be used to create method handles to publicly accessible
          *  members in packages that are exported unconditionally.
          */
         static final Lookup PUBLIC_LOOKUP = new Lookup(Object.class, null, UNCONDITIONAL);
 
-        static final JavaLangAccess JLA = SharedSecrets.getJavaLangAccess();
-
         private static void checkUnprivilegedlookupClass(Class<?> lookupClass) {
             String name = lookupClass.getName();
             if (name.startsWith("java.lang.invoke."))
                 throw newIllegalArgumentException("illegal lookupClass: "+lookupClass);
         }
@@ -2600,10 +2600,47 @@
         public Class<?> findClass(String targetName) throws ClassNotFoundException, IllegalAccessException {
             Class<?> targetClass = Class.forName(targetName, false, lookupClass.getClassLoader());
             return accessClass(targetClass);
         }
 
+        /**
+         * Ensures that {@code targetClass} has been initialized. The class
+         * to be initialized must be {@linkplain #accessClass accessible}
+         * to this {@code Lookup} object.  This method causes {@code targetClass}
+         * to be initialized if it has not been already initialized,
+         * as specified in JVMS {@jvms 5.5}.
+         *
+         * @param targetClass the class to be initialized
+         * @return {@code targetClass} that has been initialized
+         *
+         * @throws  IllegalArgumentException if {@code targetClass} is a primitive type or {@code void}
+         *          or array class
+         * @throws  IllegalAccessException if {@code targetClass} is not
+         *          {@linkplain #accessClass accessible} to this lookup
+         * @throws  ExceptionInInitializerError if the class initialization provoked
+         *          by this method fails
+         * @throws  SecurityException if a security manager is present and it
+         *          <a href="MethodHandles.Lookup.html#secmgr">refuses access</a>
+         * @since 15
+         * @jvms 5.5 Initialization
+         */
+        public Class<?> ensureInitialized(Class<?> targetClass) throws IllegalAccessException {
+            if (targetClass.isPrimitive())
+                throw new IllegalArgumentException(targetClass + " is a primitive class");
+            if (targetClass.isArray())
+                throw new IllegalArgumentException(targetClass + " is an array class");
+
+            if (!VerifyAccess.isClassAccessible(targetClass, lookupClass, prevLookupClass, allowedModes)) {
+                throw new MemberName(targetClass).makeAccessException("access violation", this);
+            }
+            checkSecurityManager(targetClass, null);
+
+            // ensure class initialization
+            Unsafe.getUnsafe().ensureClassInitialized(targetClass);
+            return targetClass;
+        }
+
         /**
          * Determines if a class can be accessed from the lookup context defined by
          * this {@code Lookup} object. The static initializer of the class is not run.
          * <p>
          * If the {@code targetClass} is in the same module as the lookup class,
diff a/src/jdk.compiler/share/classes/com/sun/tools/doclint/Checker.java b/src/jdk.compiler/share/classes/com/sun/tools/doclint/Checker.java
--- a/src/jdk.compiler/share/classes/com/sun/tools/doclint/Checker.java
+++ b/src/jdk.compiler/share/classes/com/sun/tools/doclint/Checker.java
@@ -801,10 +801,11 @@
         return super.visitDocRoot(tree, ignore);
     }
 
     @Override @DefinedBy(Api.COMPILER_TREE)
     public Void visitIndex(IndexTree tree, Void ignore) {
+        markEnclosingTag(Flag.HAS_INLINE_TAG);
         for (TagStackItem tsi : tagStack) {
             if (tsi.tag == HtmlTag.A) {
                 env.messages.warning(HTML, tree, "dc.tag.a.within.a",
                         "{@" + tree.getTagName() + "}");
                 break;
@@ -954,10 +955,11 @@
         return super.visitSince(tree, ignore);
     }
 
     @Override @DefinedBy(Api.COMPILER_TREE)
     public Void visitSummary(SummaryTree node, Void aVoid) {
+        markEnclosingTag(Flag.HAS_INLINE_TAG);
         int idx = env.currDocComment.getFullBody().indexOf(node);
         // Warn if the node is preceded by non-whitespace characters,
         // or other non-text nodes.
         if ((idx == 1 && hasNonWhitespaceText) || idx > 1) {
             env.messages.warning(SYNTAX, node, "dc.invalid.summary", node.getTagName());
@@ -965,10 +967,11 @@
         return super.visitSummary(node, aVoid);
     }
 
     @Override @DefinedBy(Api.COMPILER_TREE)
     public Void visitSystemProperty(SystemPropertyTree tree, Void ignore) {
+        markEnclosingTag(Flag.HAS_INLINE_TAG);
         for (TagStackItem tsi : tagStack) {
             if (tsi.tag == HtmlTag.A) {
                 env.messages.warning(HTML, tree, "dc.tag.a.within.a",
                         "{@" + tree.getTagName() + "}");
                 break;
@@ -1039,10 +1042,11 @@
         return super.visitUnknownBlockTag(tree, ignore);
     }
 
     @Override @DefinedBy(Api.COMPILER_TREE)
     public Void visitUnknownInlineTag(UnknownInlineTagTree tree, Void ignore) {
+        markEnclosingTag(Flag.HAS_INLINE_TAG);
         checkUnknownTag(tree, tree.getTagName());
         return super.visitUnknownInlineTag(tree, ignore);
     }
 
     private void checkUnknownTag(DocTree tree, String tagName) {
diff a/src/jdk.compiler/share/classes/com/sun/tools/javac/code/Types.java b/src/jdk.compiler/share/classes/com/sun/tools/javac/code/Types.java
--- a/src/jdk.compiler/share/classes/com/sun/tools/javac/code/Types.java
+++ b/src/jdk.compiler/share/classes/com/sun/tools/javac/code/Types.java
@@ -2224,26 +2224,35 @@
 
                 // No man may be an island, but the bell tolls for a value.
                 if (isValue(t))
                     return null;
 
-                Type st = supertype(t);
-                if (st.hasTag(CLASS) || st.hasTag(TYPEVAR)) {
-                    Type x = asSuper(st, sym);
-                    if (x != null)
-                        return x;
+                Symbol c = t.tsym;
+                if ((c.flags_field & LOCKED) != 0) {
+                    return null;
                 }
-                if ((sym.flags() & INTERFACE) != 0) {
-                    for (List<Type> l = interfaces(t); l.nonEmpty(); l = l.tail) {
-                        if (!l.head.hasTag(ERROR)) {
-                            Type x = asSuper(l.head, sym);
-                            if (x != null)
-                                return x;
+                try {
+                    c.flags_field |= LOCKED;
+                    Type st = supertype(t);
+                    if (st.hasTag(CLASS) || st.hasTag(TYPEVAR)) {
+                        Type x = asSuper(st, sym);
+                        if (x != null)
+                            return x;
+                    }
+                    if ((sym.flags() & INTERFACE) != 0) {
+                        for (List<Type> l = interfaces(t); l.nonEmpty(); l = l.tail) {
+                            if (!l.head.hasTag(ERROR)) {
+                                Type x = asSuper(l.head, sym);
+                                if (x != null)
+                                    return x;
+                            }
                         }
                     }
+                    return null;
+                } finally {
+                    c.flags_field &= ~LOCKED;
                 }
-                return null;
             }
 
             @Override
             public Type visitArrayType(ArrayType t, Symbol sym) {
                 return isSubtype(t, sym.type) ? sym.type : null;
diff a/src/jdk.compiler/share/classes/com/sun/tools/javac/comp/Attr.java b/src/jdk.compiler/share/classes/com/sun/tools/javac/comp/Attr.java
--- a/src/jdk.compiler/share/classes/com/sun/tools/javac/comp/Attr.java
+++ b/src/jdk.compiler/share/classes/com/sun/tools/javac/comp/Attr.java
@@ -5243,11 +5243,16 @@
                                                   .map(s -> (ClassSymbol) s.tsym)
                                                   .collect(List.collector());
 
             if (sealedSupers.isEmpty()) {
                 if ((c.flags_field & Flags.NON_SEALED) != 0) {
-                    log.error(TreeInfo.diagnosticPositionFor(c, env.tree), Errors.NonSealedWithNoSealedSupertype(c));
+                    boolean hasErrorSuper = types.directSupertypes(c.type)
+                                                 .stream()
+                                                 .anyMatch(s -> s.tsym.kind == Kind.ERR);
+                    if (!hasErrorSuper) {
+                        log.error(TreeInfo.diagnosticPositionFor(c, env.tree), Errors.NonSealedWithNoSealedSupertype(c));
+                    }
                 }
             } else {
                 if (c.isLocal() && !c.isEnum()) {
                     log.error(TreeInfo.diagnosticPositionFor(c, env.tree), Errors.LocalClassesCantExtendSealed);
                 }
diff a/src/jdk.compiler/share/classes/com/sun/tools/javac/comp/TypeEnter.java b/src/jdk.compiler/share/classes/com/sun/tools/javac/comp/TypeEnter.java
--- a/src/jdk.compiler/share/classes/com/sun/tools/javac/comp/TypeEnter.java
+++ b/src/jdk.compiler/share/classes/com/sun/tools/javac/comp/TypeEnter.java
@@ -717,11 +717,10 @@
 
             // Determine permits.
             ListBuffer<Symbol> permittedSubtypeSymbols = new ListBuffer<>();
             List<JCExpression> permittedTrees = tree.permitting;
             for (JCExpression permitted : permittedTrees) {
-                permitted = clearTypeParams(permitted);
                 Type pt = attr.attribBase(permitted, baseEnv, false, false, false);
                 permittedSubtypeSymbols.append(pt.tsym);
             }
 
             if ((sym.flags_field & ANNOTATION) != 0) {
@@ -740,11 +739,17 @@
                     projectedType.interfaces_field = ct.interfaces_field;
                     projectedType.all_interfaces_field = ct.all_interfaces_field;
                 }
             }
 
-            sym.permitted = permittedSubtypeSymbols.toList();
+            /* it could be that there are already some symbols in the permitted list, for the case
+             * where there are subtypes in the same compilation unit but the permits list is empty
+             * so don't overwrite the permitted list if it is not empty
+             */
+            if (!permittedSubtypeSymbols.isEmpty()) {
+                sym.permitted = permittedSubtypeSymbols.toList();
+            }
             sym.isPermittedExplicit = !permittedSubtypeSymbols.isEmpty();
         }
             //where:
             protected JCExpression clearTypeParams(JCExpression superType) {
                 return superType;
diff a/src/jdk.compiler/share/classes/com/sun/tools/javac/parser/JavacParser.java b/src/jdk.compiler/share/classes/com/sun/tools/javac/parser/JavacParser.java
--- a/src/jdk.compiler/share/classes/com/sun/tools/javac/parser/JavacParser.java
+++ b/src/jdk.compiler/share/classes/com/sun/tools/javac/parser/JavacParser.java
@@ -3490,10 +3490,17 @@
                 return Source.JDK15;
             } else if (shouldWarn) {
                 log.warning(pos, Warnings.RestrictedTypeNotAllowedPreview(name, Source.JDK15));
             }
         }
+        if (name == names.permits) {
+            if (allowSealedTypes) {
+                return Source.JDK15;
+            } else if (shouldWarn) {
+                log.warning(pos, Warnings.RestrictedTypeNotAllowedPreview(name, Source.JDK15));
+            }
+        }
         return null;
     }
 
     /** VariableDeclaratorId = Ident BracketsOpt
      */
diff a/src/jdk.internal.vm.compiler/share/classes/org.graalvm.compiler.hotspot.test/src/org/graalvm/compiler/hotspot/test/CheckGraalIntrinsics.java b/src/jdk.internal.vm.compiler/share/classes/org.graalvm.compiler.hotspot.test/src/org/graalvm/compiler/hotspot/test/CheckGraalIntrinsics.java
--- a/src/jdk.internal.vm.compiler/share/classes/org.graalvm.compiler.hotspot.test/src/org/graalvm/compiler/hotspot/test/CheckGraalIntrinsics.java
+++ b/src/jdk.internal.vm.compiler/share/classes/org.graalvm.compiler.hotspot.test/src/org/graalvm/compiler/hotspot/test/CheckGraalIntrinsics.java
@@ -275,12 +275,10 @@
                             "jdk/jfr/internal/JVM.counterTime()J",
                             "jdk/jfr/internal/JVM.getBufferWriter()Ljava/lang/Object;",
                             "jdk/jfr/internal/JVM.getClassId(Ljava/lang/Class;)J");
 
             add(toBeInvestigated,
-                            // Just check if the argument is a compile time constant
-                            "java/lang/invoke/MethodHandleImpl.isCompileConstant(Ljava/lang/Object;)Z",
                             // Only used as a marker for vectorization?
                             "java/util/stream/Streams$RangeIntSpliterator.forEachRemaining(Ljava/util/function/IntConsumer;)V",
                             // Only implemented on non-AMD64 platforms (some logic and runtime call)
                             "java/util/zip/Adler32.updateByteBuffer(IJII)I",
                             // Only implemented on non-AMD64 platforms (some logic and runtime call)
@@ -370,10 +368,14 @@
             }
             if (!config.useFMAIntrinsics) {
                 add(ignore,
                                 "java/lang/Math.fma(DDD)D",
                                 "java/lang/Math.fma(FFF)F");
+            } else if (isSPARC(arch)) {
+                add(toBeInvestigated,
+                                "java/lang/Math.fma(DDD)D",
+                                "java/lang/Math.fma(FFF)F");
             }
         }
 
         if (isJDK10OrHigher()) {
             add(toBeInvestigated,
diff a/src/jdk.javadoc/share/classes/jdk/javadoc/internal/doclets/toolkit/util/Utils.java b/src/jdk.javadoc/share/classes/jdk/javadoc/internal/doclets/toolkit/util/Utils.java
--- a/src/jdk.javadoc/share/classes/jdk/javadoc/internal/doclets/toolkit/util/Utils.java
+++ b/src/jdk.javadoc/share/classes/jdk/javadoc/internal/doclets/toolkit/util/Utils.java
@@ -105,11 +105,11 @@
 import com.sun.source.util.DocTrees;
 import com.sun.source.util.TreePath;
 import com.sun.tools.javac.model.JavacTypes;
 import jdk.javadoc.internal.doclets.toolkit.BaseConfiguration;
 import jdk.javadoc.internal.doclets.toolkit.BaseOptions;
-import jdk.javadoc.internal.doclets.toolkit.CommentUtils.DocCommentDuo;
+import jdk.javadoc.internal.doclets.toolkit.CommentUtils.DocCommentInfo;
 import jdk.javadoc.internal.doclets.toolkit.Resources;
 import jdk.javadoc.internal.doclets.toolkit.WorkArounds;
 import jdk.javadoc.internal.doclets.toolkit.taglets.BaseTaglet;
 import jdk.javadoc.internal.doclets.toolkit.taglets.Taglet;
 import jdk.javadoc.internal.tool.DocEnvImpl;
@@ -2649,87 +2649,95 @@
      * and efficient.
      * @param e an Element
      * @return TreePath
      */
     public TreePath getTreePath(Element e) {
-        DocCommentDuo duo = dcTreeCache.get(e);
-        if (duo != null && duo.treePath != null) {
-            return duo.treePath;
+        DocCommentInfo info = dcTreeCache.get(e);
+        if (info != null && info.treePath != null) {
+            return info.treePath;
         }
-        duo = configuration.cmtUtils.getSyntheticCommentDuo(e);
-        if (duo != null && duo.treePath != null) {
-            return duo.treePath;
+        info = configuration.cmtUtils.getSyntheticCommentInfo(e);
+        if (info != null && info.treePath != null) {
+            return info.treePath;
         }
         Map<Element, TreePath> elementToTreePath = configuration.workArounds.getElementToTreePath();
         TreePath path = elementToTreePath.get(e);
         if (path != null || elementToTreePath.containsKey(e)) {
             // expedite the path and one that is a null
             return path;
         }
         return elementToTreePath.computeIfAbsent(e, docTrees::getPath);
     }
 
-    private final Map<Element, DocCommentDuo> dcTreeCache = new LinkedHashMap<>();
+    /**
+     * A cache of doc comment info objects for elements.
+     * The entries may come from the AST and DocCommentParser, or may be autromatically
+     * generated comments for mandated elements and JavaFX properties.
+     *
+     * @see CommentUtils.dcInfoMap
+     */
+    private final Map<Element, DocCommentInfo> dcTreeCache = new LinkedHashMap<>();
 
     /**
      * Retrieves the doc comments for a given element.
      * @param element
      * @return DocCommentTree for the Element
      */
     public DocCommentTree getDocCommentTree0(Element element) {
 
-        DocCommentDuo duo = null;
+        DocCommentInfo info = null;
 
         ElementKind kind = element.getKind();
         if (kind == ElementKind.PACKAGE || kind == ElementKind.OTHER) {
-            duo = dcTreeCache.get(element); // local cache
-            if (duo == null && kind == ElementKind.PACKAGE) {
+            info = dcTreeCache.get(element); // local cache
+            if (info == null && kind == ElementKind.PACKAGE) {
                 // package-info.java
-                duo = getDocCommentTuple(element);
+                info = getDocCommentInfo(element);
             }
-            if (duo == null) {
+            if (info == null) {
                 // package.html or overview.html
-                duo = configuration.cmtUtils.getHtmlCommentDuo(element); // html source
+                info = configuration.cmtUtils.getHtmlCommentInfo(element); // html source
             }
         } else {
-            duo = configuration.cmtUtils.getSyntheticCommentDuo(element);
-            if (duo == null) {
-                duo = dcTreeCache.get(element); // local cache
+            info = configuration.cmtUtils.getSyntheticCommentInfo(element);
+            if (info == null) {
+                info = dcTreeCache.get(element); // local cache
             }
-            if (duo == null) {
-                duo = getDocCommentTuple(element); // get the real mccoy
+            if (info == null) {
+                info = getDocCommentInfo(element); // get the real mccoy
             }
         }
 
-        DocCommentTree docCommentTree = isValidDuo(duo) ? duo.dcTree : null;
-        TreePath path = isValidDuo(duo) ? duo.treePath : null;
+        DocCommentTree docCommentTree = info == null ? null : info.dcTree;
         if (!dcTreeCache.containsKey(element)) {
-            if (docCommentTree != null && path != null) {
-                if (!configuration.isAllowScriptInComments()) {
+            TreePath path = info == null ? null : info.treePath;
+            if (path != null) {
+                if (docCommentTree != null && !configuration.isAllowScriptInComments()) {
                     try {
                         javaScriptScanner.scan(docCommentTree, path, p -> {
                             throw new JavaScriptScanner.Fault();
                         });
                     } catch (JavaScriptScanner.Fault jsf) {
                         String text = resources.getText("doclet.JavaScript_in_comment");
                         throw new UncheckedDocletException(new SimpleDocletException(text, jsf));
                     }
                 }
+                // run doclint even if docCommentTree is null, to trigger checks for missing comments
                 configuration.workArounds.runDocLint(path);
             }
-            dcTreeCache.put(element, duo);
+            dcTreeCache.put(element, info);
         }
         return docCommentTree;
     }
 
-    private DocCommentDuo getDocCommentTuple(Element element) {
+    private DocCommentInfo getDocCommentInfo(Element element) {
         // prevent nasty things downstream with overview element
         if (element.getKind() != ElementKind.OTHER) {
             TreePath path = getTreePath(element);
             if (path != null) {
                 DocCommentTree docCommentTree = docTrees.getDocCommentTree(path);
-                return new DocCommentDuo(path, docCommentTree);
+                return new DocCommentInfo(path, docCommentTree);
             }
         }
         return null;
     }
 
@@ -2750,14 +2758,10 @@
                 throw new UncheckedDocletException(new SimpleDocletException(text, jsf));
             }
         }
     }
 
-    boolean isValidDuo(DocCommentDuo duo) {
-        return duo != null && duo.dcTree != null;
-    }
-
     public DocCommentTree getDocCommentTree(Element element) {
         CommentHelper ch = commentHelperCache.get(element);
         if (ch != null) {
             return ch.dcTree;
         }
diff a/src/jdk.jdi/share/classes/com/sun/tools/jdi/VirtualMachineImpl.java b/src/jdk.jdi/share/classes/com/sun/tools/jdi/VirtualMachineImpl.java
--- a/src/jdk.jdi/share/classes/com/sun/tools/jdi/VirtualMachineImpl.java
+++ b/src/jdk.jdi/share/classes/com/sun/tools/jdi/VirtualMachineImpl.java
@@ -766,16 +766,18 @@
         validateVM();
         return hasNewCapabilities() &&
             capabilitiesNew().canRedefineClasses;
     }
 
+    @Deprecated(since="15")
     public boolean canAddMethod() {
         validateVM();
         return hasNewCapabilities() &&
             capabilitiesNew().canAddMethod;
     }
 
+    @Deprecated(since="15")
     public boolean canUnrestrictedlyRedefineClasses() {
         validateVM();
         return hasNewCapabilities() &&
             capabilitiesNew().canUnrestrictedlyRedefineClasses;
     }
diff a/src/jdk.unsupported/share/classes/sun/misc/Unsafe.java b/src/jdk.unsupported/share/classes/sun/misc/Unsafe.java
--- a/src/jdk.unsupported/share/classes/sun/misc/Unsafe.java
+++ b/src/jdk.unsupported/share/classes/sun/misc/Unsafe.java
@@ -706,22 +706,37 @@
 
     /**
      * Detects if the given class may need to be initialized. This is often
      * needed in conjunction with obtaining the static field base of a
      * class.
+     *
+     * @deprecated No replacement API for this method.  As multiple threads
+     * may be trying to initialize the same class or interface at the same time.
+     * The only reliable result returned by this method is {@code false}
+     * indicating that the given class has been initialized.  Instead, simply
+     * call {@link java.lang.invoke.MethodHandles.Lookup#ensureInitialized(Class)}
+     * that does nothing if the given class has already been initialized.
+     * This method is subject to removal in a future version of JDK.
+     *
      * @return false only if a call to {@code ensureClassInitialized} would have no effect
+     *
      */
+    @Deprecated(since = "15", forRemoval = true)
     @ForceInline
     public boolean shouldBeInitialized(Class<?> c) {
         return theInternalUnsafe.shouldBeInitialized(c);
     }
 
     /**
      * Ensures the given class has been initialized. This is often
      * needed in conjunction with obtaining the static field base of a
      * class.
+     *
+     * @deprecated Use the {@link java.lang.invoke.MethodHandles.Lookup#ensureInitialized(Class)}
+     * method instead.  This method is subject to removal in a future version of JDK.
      */
+    @Deprecated(since = "15", forRemoval = true)
     @ForceInline
     public void ensureClassInitialized(Class<?> c) {
         theInternalUnsafe.ensureClassInitialized(c);
     }
 
