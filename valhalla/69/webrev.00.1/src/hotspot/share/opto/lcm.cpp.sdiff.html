<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/share/opto/lcm.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="graphKit.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="machnode.hpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/share/opto/lcm.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
 257       const Node* base = mach-&gt;get_base_and_disp(offset, adr_type);
 258       if (base == NULL || base == NodeSentinel) {
 259         // Narrow oop address doesn&#39;t have base, only index.
 260         // Give up if offset is beyond page size or if heap base is not protected.
 261         if (val-&gt;bottom_type()-&gt;isa_narrowoop() &amp;&amp;
 262             (MacroAssembler::needs_explicit_null_check(offset) ||
 263              !CompressedOops::use_implicit_null_checks()))
 264           continue;
 265         // cannot reason about it; is probably not implicit null exception
 266       } else {
 267         const TypePtr* tptr;
 268         if ((UseCompressedOops || UseCompressedClassPointers) &amp;&amp;
 269             (CompressedOops::shift() == 0 || CompressedKlassPointers::shift() == 0)) {
 270           // 32-bits narrow oop can be the base of address expressions
 271           tptr = base-&gt;get_ptr_type();
 272         } else {
 273           // only regular oops are expected here
 274           tptr = base-&gt;bottom_type()-&gt;is_ptr();
 275         }
 276         // Give up if offset is not a compile-time constant.
<span class="line-modified"> 277         if (offset == Type::OffsetBot || tptr-&gt;_offset == Type::OffsetBot)</span>
 278           continue;
<span class="line-modified"> 279         offset += tptr-&gt;_offset; // correct if base is offseted</span>
 280         // Give up if reference is beyond page size.
 281         if (MacroAssembler::needs_explicit_null_check(offset))
 282           continue;
 283         // Give up if base is a decode node and the heap base is not protected.
 284         if (base-&gt;is_Mach() &amp;&amp; base-&gt;as_Mach()-&gt;ideal_Opcode() == Op_DecodeN &amp;&amp;
 285             !CompressedOops::use_implicit_null_checks())
 286           continue;
 287       }
 288     }
 289 
 290     // Check ctrl input to see if the null-check dominates the memory op
 291     Block *cb = get_block_for_node(mach);
 292     cb = cb-&gt;_idom;             // Always hoist at least 1 block
 293     if( !was_store ) {          // Stores can be hoisted only one block
 294       while( cb-&gt;_dom_depth &gt; (block-&gt;_dom_depth + 1))
 295         cb = cb-&gt;_idom;         // Hoist loads as far as we want
 296       // The non-null-block should dominate the memory op, too. Live
 297       // range spilling will insert a spill in the non-null-block if it is
 298       // needs to spill the memory op for an implicit null check.
 299       if (cb-&gt;_dom_depth == (block-&gt;_dom_depth + 1)) {
 300         if (cb != not_null_block) continue;
 301         cb = cb-&gt;_idom;
 302       }
 303     }
 304     if( cb != block ) continue;
 305 
 306     // Found a memory user; see if it can be hoisted to check-block
 307     uint vidx = 0;              // Capture index of value into memop
 308     uint j;
 309     for( j = mach-&gt;req()-1; j &gt; 0; j-- ) {
 310       if( mach-&gt;in(j) == val ) {
 311         vidx = j;
 312         // Ignore DecodeN val which could be hoisted to where needed.
 313         if( is_decoden ) continue;
 314       }
 315       // Block of memory-op input
<span class="line-modified"> 316       Block *inb = get_block_for_node(mach-&gt;in(j));</span>




 317       Block *b = block;          // Start from nul check
 318       while( b != inb &amp;&amp; b-&gt;_dom_depth &gt; inb-&gt;_dom_depth )
 319         b = b-&gt;_idom;           // search upwards for input
 320       // See if input dominates null check
 321       if( b != inb )
 322         break;
 323     }
 324     if( j &gt; 0 )
 325       continue;
 326     Block *mb = get_block_for_node(mach);
 327     // Hoisting stores requires more checks for the anti-dependence case.
 328     // Give up hoisting if we have to move the store past any load.
 329     if( was_store ) {
 330       Block *b = mb;            // Start searching here for a local load
 331       // mach use (faulting) trying to hoist
 332       // n might be blocker to hoisting
 333       while( b != block ) {
 334         uint k;
 335         for( k = 1; k &lt; b-&gt;number_of_nodes(); k++ ) {
 336           Node *n = b-&gt;get_node(k);
</pre>
<hr />
<pre>
 372 
 373   if( is_decoden ) {
 374     // Check if we need to hoist decodeHeapOop_not_null first.
 375     Block *valb = get_block_for_node(val);
 376     if( block != valb &amp;&amp; block-&gt;_dom_depth &lt; valb-&gt;_dom_depth ) {
 377       // Hoist it up to the end of the test block.
 378       valb-&gt;find_remove(val);
 379       block-&gt;add_inst(val);
 380       map_node_to_block(val, block);
 381       // DecodeN on x86 may kill flags. Check for flag-killing projections
 382       // that also need to be hoisted.
 383       for (DUIterator_Fast jmax, j = val-&gt;fast_outs(jmax); j &lt; jmax; j++) {
 384         Node* n = val-&gt;fast_out(j);
 385         if( n-&gt;is_MachProj() ) {
 386           get_block_for_node(n)-&gt;find_remove(n);
 387           block-&gt;add_inst(n);
 388           map_node_to_block(n, block);
 389         }
 390       }
 391     }




















 392   }

 393   // Hoist the memory candidate up to the end of the test block.
 394   Block *old_block = get_block_for_node(best);
 395   old_block-&gt;find_remove(best);
 396   block-&gt;add_inst(best);
 397   map_node_to_block(best, block);
 398 
 399   // Move the control dependence if it is pinned to not-null block.
 400   // Don&#39;t change it in other cases: NULL or dominating control.
 401   if (best-&gt;in(0) == not_null_block-&gt;head()) {
 402     // Set it to control edge of null check.
 403     best-&gt;set_req(0, proj-&gt;in(0)-&gt;in(0));
 404   }
 405 
 406   // Check for flag-killing projections that also need to be hoisted
 407   // Should be DU safe because no edge updates.
 408   for (DUIterator_Fast jmax, j = best-&gt;fast_outs(jmax); j &lt; jmax; j++) {
 409     Node* n = best-&gt;fast_out(j);
 410     if( n-&gt;is_MachProj() ) {
 411       get_block_for_node(n)-&gt;find_remove(n);
 412       block-&gt;add_inst(n);
</pre>
<hr />
<pre>
 824     // Children of projections are now all ready
 825     for (DUIterator_Fast jmax, j = n-&gt;fast_outs(jmax); j &lt; jmax; j++) {
 826       Node* m = n-&gt;fast_out(j); // Get user
 827       if(get_block_for_node(m) != block) {
 828         continue;
 829       }
 830       if( m-&gt;is_Phi() ) continue;
 831       int m_cnt = ready_cnt.at(m-&gt;_idx) - 1;
 832       ready_cnt.at_put(m-&gt;_idx, m_cnt);
 833       if( m_cnt == 0 )
 834         worklist.push(m);
 835     }
 836 
 837   }
 838 
 839   // Act as if the call defines the Frame Pointer.
 840   // Certainly the FP is alive and well after the call.
 841   regs.Insert(_matcher.c_frame_pointer());
 842 
 843   // Set all registers killed and not already defined by the call.
<span class="line-modified"> 844   uint r_cnt = mcall-&gt;tf()-&gt;range()-&gt;cnt();</span>
 845   int op = mcall-&gt;ideal_Opcode();
 846   MachProjNode *proj = new MachProjNode( mcall, r_cnt+1, RegMask::Empty, MachProjNode::fat_proj );
 847   map_node_to_block(proj, block);
 848   block-&gt;insert_node(proj, node_cnt++);
 849 
 850   // Select the right register save policy.
 851   const char *save_policy = NULL;
 852   switch (op) {
 853     case Op_CallRuntime:
 854     case Op_CallLeaf:
 855     case Op_CallLeafNoFP:
 856       // Calling C code so use C calling convention
 857       save_policy = _matcher._c_reg_save_policy;
 858       break;
 859 
 860     case Op_CallStaticJava:
 861     case Op_CallDynamicJava:
 862       // Calling Java code so use Java calling convention
 863       save_policy = _matcher._register_save_policy;
 864       break;
</pre>
</td>
<td>
<hr />
<pre>
 257       const Node* base = mach-&gt;get_base_and_disp(offset, adr_type);
 258       if (base == NULL || base == NodeSentinel) {
 259         // Narrow oop address doesn&#39;t have base, only index.
 260         // Give up if offset is beyond page size or if heap base is not protected.
 261         if (val-&gt;bottom_type()-&gt;isa_narrowoop() &amp;&amp;
 262             (MacroAssembler::needs_explicit_null_check(offset) ||
 263              !CompressedOops::use_implicit_null_checks()))
 264           continue;
 265         // cannot reason about it; is probably not implicit null exception
 266       } else {
 267         const TypePtr* tptr;
 268         if ((UseCompressedOops || UseCompressedClassPointers) &amp;&amp;
 269             (CompressedOops::shift() == 0 || CompressedKlassPointers::shift() == 0)) {
 270           // 32-bits narrow oop can be the base of address expressions
 271           tptr = base-&gt;get_ptr_type();
 272         } else {
 273           // only regular oops are expected here
 274           tptr = base-&gt;bottom_type()-&gt;is_ptr();
 275         }
 276         // Give up if offset is not a compile-time constant.
<span class="line-modified"> 277         if (offset == Type::OffsetBot || tptr-&gt;offset() == Type::OffsetBot)</span>
 278           continue;
<span class="line-modified"> 279         offset += tptr-&gt;offset(); // correct if base is offseted</span>
 280         // Give up if reference is beyond page size.
 281         if (MacroAssembler::needs_explicit_null_check(offset))
 282           continue;
 283         // Give up if base is a decode node and the heap base is not protected.
 284         if (base-&gt;is_Mach() &amp;&amp; base-&gt;as_Mach()-&gt;ideal_Opcode() == Op_DecodeN &amp;&amp;
 285             !CompressedOops::use_implicit_null_checks())
 286           continue;
 287       }
 288     }
 289 
 290     // Check ctrl input to see if the null-check dominates the memory op
 291     Block *cb = get_block_for_node(mach);
 292     cb = cb-&gt;_idom;             // Always hoist at least 1 block
 293     if( !was_store ) {          // Stores can be hoisted only one block
 294       while( cb-&gt;_dom_depth &gt; (block-&gt;_dom_depth + 1))
 295         cb = cb-&gt;_idom;         // Hoist loads as far as we want
 296       // The non-null-block should dominate the memory op, too. Live
 297       // range spilling will insert a spill in the non-null-block if it is
 298       // needs to spill the memory op for an implicit null check.
 299       if (cb-&gt;_dom_depth == (block-&gt;_dom_depth + 1)) {
 300         if (cb != not_null_block) continue;
 301         cb = cb-&gt;_idom;
 302       }
 303     }
 304     if( cb != block ) continue;
 305 
 306     // Found a memory user; see if it can be hoisted to check-block
 307     uint vidx = 0;              // Capture index of value into memop
 308     uint j;
 309     for( j = mach-&gt;req()-1; j &gt; 0; j-- ) {
 310       if( mach-&gt;in(j) == val ) {
 311         vidx = j;
 312         // Ignore DecodeN val which could be hoisted to where needed.
 313         if( is_decoden ) continue;
 314       }
 315       // Block of memory-op input
<span class="line-modified"> 316       Block* inb = get_block_for_node(mach-&gt;in(j));</span>
<span class="line-added"> 317       if (mach-&gt;in(j)-&gt;is_Con() &amp;&amp; inb == get_block_for_node(mach)) {</span>
<span class="line-added"> 318         // Ignore constant loads scheduled in the same block (we can simply hoist them as well)</span>
<span class="line-added"> 319         continue;</span>
<span class="line-added"> 320       }</span>
 321       Block *b = block;          // Start from nul check
 322       while( b != inb &amp;&amp; b-&gt;_dom_depth &gt; inb-&gt;_dom_depth )
 323         b = b-&gt;_idom;           // search upwards for input
 324       // See if input dominates null check
 325       if( b != inb )
 326         break;
 327     }
 328     if( j &gt; 0 )
 329       continue;
 330     Block *mb = get_block_for_node(mach);
 331     // Hoisting stores requires more checks for the anti-dependence case.
 332     // Give up hoisting if we have to move the store past any load.
 333     if( was_store ) {
 334       Block *b = mb;            // Start searching here for a local load
 335       // mach use (faulting) trying to hoist
 336       // n might be blocker to hoisting
 337       while( b != block ) {
 338         uint k;
 339         for( k = 1; k &lt; b-&gt;number_of_nodes(); k++ ) {
 340           Node *n = b-&gt;get_node(k);
</pre>
<hr />
<pre>
 376 
 377   if( is_decoden ) {
 378     // Check if we need to hoist decodeHeapOop_not_null first.
 379     Block *valb = get_block_for_node(val);
 380     if( block != valb &amp;&amp; block-&gt;_dom_depth &lt; valb-&gt;_dom_depth ) {
 381       // Hoist it up to the end of the test block.
 382       valb-&gt;find_remove(val);
 383       block-&gt;add_inst(val);
 384       map_node_to_block(val, block);
 385       // DecodeN on x86 may kill flags. Check for flag-killing projections
 386       // that also need to be hoisted.
 387       for (DUIterator_Fast jmax, j = val-&gt;fast_outs(jmax); j &lt; jmax; j++) {
 388         Node* n = val-&gt;fast_out(j);
 389         if( n-&gt;is_MachProj() ) {
 390           get_block_for_node(n)-&gt;find_remove(n);
 391           block-&gt;add_inst(n);
 392           map_node_to_block(n, block);
 393         }
 394       }
 395     }
<span class="line-added"> 396   } else {</span>
<span class="line-added"> 397     // Hoist constant load inputs as well.</span>
<span class="line-added"> 398     for (uint i = 1; i &lt; best-&gt;req(); ++i) {</span>
<span class="line-added"> 399       Node* n = best-&gt;in(i);</span>
<span class="line-added"> 400       if (n-&gt;is_Con() &amp;&amp; get_block_for_node(n) == get_block_for_node(best)) {</span>
<span class="line-added"> 401         get_block_for_node(n)-&gt;find_remove(n);</span>
<span class="line-added"> 402         block-&gt;add_inst(n);</span>
<span class="line-added"> 403         map_node_to_block(n, block);</span>
<span class="line-added"> 404         // Constant loads may kill flags (for example, when XORing a register).</span>
<span class="line-added"> 405         // Check for flag-killing projections that also need to be hoisted.</span>
<span class="line-added"> 406         for (DUIterator_Fast jmax, j = n-&gt;fast_outs(jmax); j &lt; jmax; j++) {</span>
<span class="line-added"> 407           Node* proj = n-&gt;fast_out(j);</span>
<span class="line-added"> 408           if (proj-&gt;is_MachProj()) {</span>
<span class="line-added"> 409             get_block_for_node(proj)-&gt;find_remove(proj);</span>
<span class="line-added"> 410             block-&gt;add_inst(proj);</span>
<span class="line-added"> 411             map_node_to_block(proj, block);</span>
<span class="line-added"> 412           }</span>
<span class="line-added"> 413         }</span>
<span class="line-added"> 414       }</span>
<span class="line-added"> 415     }</span>
 416   }
<span class="line-added"> 417 </span>
 418   // Hoist the memory candidate up to the end of the test block.
 419   Block *old_block = get_block_for_node(best);
 420   old_block-&gt;find_remove(best);
 421   block-&gt;add_inst(best);
 422   map_node_to_block(best, block);
 423 
 424   // Move the control dependence if it is pinned to not-null block.
 425   // Don&#39;t change it in other cases: NULL or dominating control.
 426   if (best-&gt;in(0) == not_null_block-&gt;head()) {
 427     // Set it to control edge of null check.
 428     best-&gt;set_req(0, proj-&gt;in(0)-&gt;in(0));
 429   }
 430 
 431   // Check for flag-killing projections that also need to be hoisted
 432   // Should be DU safe because no edge updates.
 433   for (DUIterator_Fast jmax, j = best-&gt;fast_outs(jmax); j &lt; jmax; j++) {
 434     Node* n = best-&gt;fast_out(j);
 435     if( n-&gt;is_MachProj() ) {
 436       get_block_for_node(n)-&gt;find_remove(n);
 437       block-&gt;add_inst(n);
</pre>
<hr />
<pre>
 849     // Children of projections are now all ready
 850     for (DUIterator_Fast jmax, j = n-&gt;fast_outs(jmax); j &lt; jmax; j++) {
 851       Node* m = n-&gt;fast_out(j); // Get user
 852       if(get_block_for_node(m) != block) {
 853         continue;
 854       }
 855       if( m-&gt;is_Phi() ) continue;
 856       int m_cnt = ready_cnt.at(m-&gt;_idx) - 1;
 857       ready_cnt.at_put(m-&gt;_idx, m_cnt);
 858       if( m_cnt == 0 )
 859         worklist.push(m);
 860     }
 861 
 862   }
 863 
 864   // Act as if the call defines the Frame Pointer.
 865   // Certainly the FP is alive and well after the call.
 866   regs.Insert(_matcher.c_frame_pointer());
 867 
 868   // Set all registers killed and not already defined by the call.
<span class="line-modified"> 869   uint r_cnt = mcall-&gt;tf()-&gt;range_cc()-&gt;cnt();</span>
 870   int op = mcall-&gt;ideal_Opcode();
 871   MachProjNode *proj = new MachProjNode( mcall, r_cnt+1, RegMask::Empty, MachProjNode::fat_proj );
 872   map_node_to_block(proj, block);
 873   block-&gt;insert_node(proj, node_cnt++);
 874 
 875   // Select the right register save policy.
 876   const char *save_policy = NULL;
 877   switch (op) {
 878     case Op_CallRuntime:
 879     case Op_CallLeaf:
 880     case Op_CallLeafNoFP:
 881       // Calling C code so use C calling convention
 882       save_policy = _matcher._c_reg_save_policy;
 883       break;
 884 
 885     case Op_CallStaticJava:
 886     case Op_CallDynamicJava:
 887       // Calling Java code so use Java calling convention
 888       save_policy = _matcher._register_save_policy;
 889       break;
</pre>
</td>
</tr>
</table>
<center><a href="graphKit.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="machnode.hpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>