<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/cpu/x86/c1_LIRAssembler_x86.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="../aarch64/aarch64.ad.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="c1_MacroAssembler_x86.cpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/cpu/x86/c1_LIRAssembler_x86.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #include &quot;precompiled.hpp&quot;
  26 #include &quot;asm/macroAssembler.hpp&quot;
  27 #include &quot;asm/macroAssembler.inline.hpp&quot;
  28 #include &quot;c1/c1_Compilation.hpp&quot;
  29 #include &quot;c1/c1_LIRAssembler.hpp&quot;
  30 #include &quot;c1/c1_MacroAssembler.hpp&quot;
  31 #include &quot;c1/c1_Runtime1.hpp&quot;
  32 #include &quot;c1/c1_ValueStack.hpp&quot;
  33 #include &quot;ci/ciArrayKlass.hpp&quot;
  34 #include &quot;ci/ciInstance.hpp&quot;

  35 #include &quot;gc/shared/collectedHeap.hpp&quot;
  36 #include &quot;nativeInst_x86.hpp&quot;

  37 #include &quot;oops/objArrayKlass.hpp&quot;
  38 #include &quot;runtime/frame.inline.hpp&quot;
  39 #include &quot;runtime/safepointMechanism.hpp&quot;
  40 #include &quot;runtime/sharedRuntime.hpp&quot;
  41 #include &quot;utilities/powerOfTwo.hpp&quot;
  42 #include &quot;vmreg_x86.inline.hpp&quot;
  43 
  44 
  45 // These masks are used to provide 128-bit aligned bitmasks to the XMM
  46 // instructions, to allow sign-masking or sign-bit flipping.  They allow
  47 // fast versions of NegF/NegD and AbsF/AbsD.
  48 
  49 // Note: &#39;double&#39; and &#39;long long&#39; have 32-bits alignment on x86.
  50 static jlong* double_quadword(jlong *adr, jlong lo, jlong hi) {
  51   // Use the expression (adr)&amp;(~0xF) to provide 128-bits aligned address
  52   // of 128-bits operands for SSE instructions.
  53   jlong *operand = (jlong*)(((intptr_t)adr) &amp; ((intptr_t)(~0xF)));
  54   // Store the value to a 128-bits operand.
  55   operand[0] = lo;
  56   operand[1] = hi;
</pre>
<hr />
<pre>
 174 
 175 void LIR_Assembler::ffree(int i) {
 176   __ ffree(i);
 177 }
 178 #endif // !_LP64
 179 
 180 void LIR_Assembler::breakpoint() {
 181   __ int3();
 182 }
 183 
 184 void LIR_Assembler::push(LIR_Opr opr) {
 185   if (opr-&gt;is_single_cpu()) {
 186     __ push_reg(opr-&gt;as_register());
 187   } else if (opr-&gt;is_double_cpu()) {
 188     NOT_LP64(__ push_reg(opr-&gt;as_register_hi()));
 189     __ push_reg(opr-&gt;as_register_lo());
 190   } else if (opr-&gt;is_stack()) {
 191     __ push_addr(frame_map()-&gt;address_for_slot(opr-&gt;single_stack_ix()));
 192   } else if (opr-&gt;is_constant()) {
 193     LIR_Const* const_opr = opr-&gt;as_constant_ptr();
<span class="line-modified"> 194     if (const_opr-&gt;type() == T_OBJECT) {</span>
 195       __ push_oop(const_opr-&gt;as_jobject());
 196     } else if (const_opr-&gt;type() == T_INT) {
 197       __ push_jint(const_opr-&gt;as_jint());
 198     } else {
 199       ShouldNotReachHere();
 200     }
 201 
 202   } else {
 203     ShouldNotReachHere();
 204   }
 205 }
 206 
 207 void LIR_Assembler::pop(LIR_Opr opr) {
 208   if (opr-&gt;is_single_cpu()) {
 209     __ pop_reg(opr-&gt;as_register());
 210   } else {
 211     ShouldNotReachHere();
 212   }
 213 }
 214 
</pre>
<hr />
<pre>
 461     __ bind(*stub-&gt;continuation());
 462   }
 463 
 464   if (compilation()-&gt;env()-&gt;dtrace_method_probes()) {
 465 #ifdef _LP64
 466     __ mov(rdi, r15_thread);
 467     __ mov_metadata(rsi, method()-&gt;constant_encoding());
 468 #else
 469     __ get_thread(rax);
 470     __ movptr(Address(rsp, 0), rax);
 471     __ mov_metadata(Address(rsp, sizeof(void*)), method()-&gt;constant_encoding());
 472 #endif
 473     __ call(RuntimeAddress(CAST_FROM_FN_PTR(address, SharedRuntime::dtrace_method_exit)));
 474   }
 475 
 476   if (method()-&gt;is_synchronized() || compilation()-&gt;env()-&gt;dtrace_method_probes()) {
 477     __ mov(rax, rbx);  // Restore the exception
 478   }
 479 
 480   // remove the activation and dispatch to the unwind handler
<span class="line-modified"> 481   __ remove_frame(initial_frame_size_in_bytes());</span>

 482   __ jump(RuntimeAddress(Runtime1::entry_for(Runtime1::unwind_exception_id)));
 483 
 484   // Emit the slow path assembly
 485   if (stub != NULL) {
 486     stub-&gt;emit_code(this);
 487   }
 488 
 489   return offset;
 490 }
 491 
 492 
 493 int LIR_Assembler::emit_deopt_handler() {
 494   // if the last instruction is a call (typically to do a throw which
 495   // is coming at the end after block reordering) the return address
 496   // must still point into the code area in order to avoid assertion
 497   // failures when searching for the corresponding bci =&gt; add a nop
 498   // (was bug 5/14/1999 - gri)
 499   __ nop();
 500 
 501   // generate code for exception handler
</pre>
<hr />
<pre>
 507   }
 508 
 509   int offset = code_offset();
 510   InternalAddress here(__ pc());
 511 
 512   __ pushptr(here.addr());
 513   __ jump(RuntimeAddress(SharedRuntime::deopt_blob()-&gt;unpack()));
 514   guarantee(code_offset() - offset &lt;= deopt_handler_size(), &quot;overflow&quot;);
 515   __ end_a_stub();
 516 
 517   return offset;
 518 }
 519 
 520 
 521 void LIR_Assembler::return_op(LIR_Opr result) {
 522   assert(result-&gt;is_illegal() || !result-&gt;is_single_cpu() || result-&gt;as_register() == rax, &quot;word returns are in rax,&quot;);
 523   if (!result-&gt;is_illegal() &amp;&amp; result-&gt;is_float_kind() &amp;&amp; !result-&gt;is_xmm_register()) {
 524     assert(result-&gt;fpu() == 0, &quot;result must already be on TOS&quot;);
 525   }
 526 



















 527   // Pop the stack before the safepoint code
<span class="line-modified"> 528   __ remove_frame(initial_frame_size_in_bytes());</span>

 529 
 530   if (StackReservedPages &gt; 0 &amp;&amp; compilation()-&gt;has_reserved_stack_access()) {
 531     __ reserved_stack_check();
 532   }
 533 
 534   bool result_is_oop = result-&gt;is_valid() ? result-&gt;is_oop() : false;
 535 
 536   // Note: we do not need to round double result; float result has the right precision
 537   // the poll sets the condition code, but no data registers
 538 
 539 #ifdef _LP64
 540   const Register poll_addr = rscratch1;
 541   __ movptr(poll_addr, Address(r15_thread, Thread::polling_page_offset()));
 542 #else
 543   const Register poll_addr = rbx;
 544   assert(FrameMap::is_caller_save_register(poll_addr), &quot;will overwrite&quot;);
 545   __ get_thread(poll_addr);
 546   __ movptr(poll_addr, Address(poll_addr, Thread::polling_page_offset()));
 547 #endif
 548   __ relocate(relocInfo::poll_return_type);
 549   __ testl(rax, Address(poll_addr, 0));
 550   __ ret(0);
 551 }
 552 
 553 




 554 int LIR_Assembler::safepoint_poll(LIR_Opr tmp, CodeEmitInfo* info) {
 555   guarantee(info != NULL, &quot;Shouldn&#39;t be NULL&quot;);
 556   int offset = __ offset();
 557 #ifdef _LP64
 558   const Register poll_addr = rscratch1;
 559   __ movptr(poll_addr, Address(r15_thread, Thread::polling_page_offset()));
 560 #else
 561   assert(tmp-&gt;is_cpu_register(), &quot;needed&quot;);
 562   const Register poll_addr = tmp-&gt;as_register();
 563   __ get_thread(poll_addr);
 564   __ movptr(poll_addr, Address(poll_addr, in_bytes(Thread::polling_page_offset())));
 565 #endif
 566   add_debug_info_for_branch(info);
 567   __ relocate(relocInfo::poll_type);
 568   address pre_pc = __ pc();
 569   __ testl(rax, Address(poll_addr, 0));
 570   address post_pc = __ pc();
 571   guarantee(pointer_delta(post_pc, pre_pc, 1) == 2 LP64_ONLY(+1), &quot;must be exact length&quot;);
 572   return offset;
 573 }
</pre>
<hr />
<pre>
 594       break;
 595     }
 596 
 597     case T_ADDRESS: {
 598       assert(patch_code == lir_patch_none, &quot;no patching handled here&quot;);
 599       __ movptr(dest-&gt;as_register(), c-&gt;as_jint());
 600       break;
 601     }
 602 
 603     case T_LONG: {
 604       assert(patch_code == lir_patch_none, &quot;no patching handled here&quot;);
 605 #ifdef _LP64
 606       __ movptr(dest-&gt;as_register_lo(), (intptr_t)c-&gt;as_jlong());
 607 #else
 608       __ movptr(dest-&gt;as_register_lo(), c-&gt;as_jint_lo());
 609       __ movptr(dest-&gt;as_register_hi(), c-&gt;as_jint_hi());
 610 #endif // _LP64
 611       break;
 612     }
 613 

 614     case T_OBJECT: {
 615       if (patch_code != lir_patch_none) {
 616         jobject2reg_with_patching(dest-&gt;as_register(), info);
 617       } else {
 618         __ movoop(dest-&gt;as_register(), c-&gt;as_jobject());
 619       }
 620       break;
 621     }
 622 
 623     case T_METADATA: {
 624       if (patch_code != lir_patch_none) {
 625         klass2reg_with_patching(dest-&gt;as_register(), info);
 626       } else {
 627         __ mov_metadata(dest-&gt;as_register(), c-&gt;as_metadata());
 628       }
 629       break;
 630     }
 631 
 632     case T_FLOAT: {
 633       if (dest-&gt;is_single_xmm()) {
</pre>
<hr />
<pre>
 684     default:
 685       ShouldNotReachHere();
 686   }
 687 }
 688 
 689 void LIR_Assembler::const2stack(LIR_Opr src, LIR_Opr dest) {
 690   assert(src-&gt;is_constant(), &quot;should not call otherwise&quot;);
 691   assert(dest-&gt;is_stack(), &quot;should not call otherwise&quot;);
 692   LIR_Const* c = src-&gt;as_constant_ptr();
 693 
 694   switch (c-&gt;type()) {
 695     case T_INT:  // fall through
 696     case T_FLOAT:
 697       __ movl(frame_map()-&gt;address_for_slot(dest-&gt;single_stack_ix()), c-&gt;as_jint_bits());
 698       break;
 699 
 700     case T_ADDRESS:
 701       __ movptr(frame_map()-&gt;address_for_slot(dest-&gt;single_stack_ix()), c-&gt;as_jint_bits());
 702       break;
 703 

 704     case T_OBJECT:
 705       __ movoop(frame_map()-&gt;address_for_slot(dest-&gt;single_stack_ix()), c-&gt;as_jobject());
 706       break;
 707 
 708     case T_LONG:  // fall through
 709     case T_DOUBLE:
 710 #ifdef _LP64
 711       __ movptr(frame_map()-&gt;address_for_slot(dest-&gt;double_stack_ix(),
 712                                             lo_word_offset_in_bytes), (intptr_t)c-&gt;as_jlong_bits());
 713 #else
 714       __ movptr(frame_map()-&gt;address_for_slot(dest-&gt;double_stack_ix(),
 715                                               lo_word_offset_in_bytes), c-&gt;as_jint_lo_bits());
 716       __ movptr(frame_map()-&gt;address_for_slot(dest-&gt;double_stack_ix(),
 717                                               hi_word_offset_in_bytes), c-&gt;as_jint_hi_bits());
 718 #endif // _LP64
 719       break;
 720 
 721     default:
 722       ShouldNotReachHere();
 723   }
 724 }
 725 
 726 void LIR_Assembler::const2mem(LIR_Opr src, LIR_Opr dest, BasicType type, CodeEmitInfo* info, bool wide) {
 727   assert(src-&gt;is_constant(), &quot;should not call otherwise&quot;);
 728   assert(dest-&gt;is_address(), &quot;should not call otherwise&quot;);
 729   LIR_Const* c = src-&gt;as_constant_ptr();
 730   LIR_Address* addr = dest-&gt;as_address_ptr();
 731 
 732   int null_check_here = code_offset();
 733   switch (type) {
 734     case T_INT:    // fall through
 735     case T_FLOAT:
 736       __ movl(as_Address(addr), c-&gt;as_jint_bits());
 737       break;
 738 
 739     case T_ADDRESS:
 740       __ movptr(as_Address(addr), c-&gt;as_jint_bits());
 741       break;
 742 

 743     case T_OBJECT:  // fall through
 744     case T_ARRAY:
 745       if (c-&gt;as_jobject() == NULL) {
 746         if (UseCompressedOops &amp;&amp; !wide) {
 747           __ movl(as_Address(addr), (int32_t)NULL_WORD);
 748         } else {
 749 #ifdef _LP64
 750           __ xorptr(rscratch1, rscratch1);
 751           null_check_here = code_offset();
 752           __ movptr(as_Address(addr), rscratch1);
 753 #else
 754           __ movptr(as_Address(addr), NULL_WORD);
 755 #endif
 756         }
 757       } else {
 758         if (is_literal_address(addr)) {
 759           ShouldNotReachHere();
 760           __ movoop(as_Address(addr, noreg), c-&gt;as_jobject());
 761         } else {
 762 #ifdef _LP64
</pre>
<hr />
<pre>
 811   if (info != NULL) {
 812     add_debug_info_for_null_check(null_check_here, info);
 813   }
 814 }
 815 
 816 
 817 void LIR_Assembler::reg2reg(LIR_Opr src, LIR_Opr dest) {
 818   assert(src-&gt;is_register(), &quot;should not call otherwise&quot;);
 819   assert(dest-&gt;is_register(), &quot;should not call otherwise&quot;);
 820 
 821   // move between cpu-registers
 822   if (dest-&gt;is_single_cpu()) {
 823 #ifdef _LP64
 824     if (src-&gt;type() == T_LONG) {
 825       // Can do LONG -&gt; OBJECT
 826       move_regs(src-&gt;as_register_lo(), dest-&gt;as_register());
 827       return;
 828     }
 829 #endif
 830     assert(src-&gt;is_single_cpu(), &quot;must match&quot;);
<span class="line-modified"> 831     if (src-&gt;type() == T_OBJECT) {</span>
 832       __ verify_oop(src-&gt;as_register());
 833     }
 834     move_regs(src-&gt;as_register(), dest-&gt;as_register());
 835 
 836   } else if (dest-&gt;is_double_cpu()) {
 837 #ifdef _LP64
 838     if (is_reference_type(src-&gt;type())) {
 839       // Surprising to me but we can see move of a long to t_object
 840       __ verify_oop(src-&gt;as_register());
 841       move_regs(src-&gt;as_register(), dest-&gt;as_register_lo());
 842       return;
 843     }
 844 #endif
 845     assert(src-&gt;is_double_cpu(), &quot;must match&quot;);
 846     Register f_lo = src-&gt;as_register_lo();
 847     Register f_hi = src-&gt;as_register_hi();
 848     Register t_lo = dest-&gt;as_register_lo();
 849     Register t_hi = dest-&gt;as_register_hi();
 850 #ifdef _LP64
 851     assert(f_hi == f_lo, &quot;must be same&quot;);
</pre>
<hr />
<pre>
 997       break;
 998     }
 999 
1000     case T_DOUBLE: {
1001 #ifdef _LP64
1002       assert(src-&gt;is_double_xmm(), &quot;not a double&quot;);
1003       __ movdbl(as_Address(to_addr), src-&gt;as_xmm_double_reg());
1004 #else
1005       if (src-&gt;is_double_xmm()) {
1006         __ movdbl(as_Address(to_addr), src-&gt;as_xmm_double_reg());
1007       } else {
1008         assert(src-&gt;is_double_fpu(), &quot;must be&quot;);
1009         assert(src-&gt;fpu_regnrLo() == 0, &quot;argument must be on TOS&quot;);
1010         if (pop_fpu_stack)      __ fstp_d(as_Address(to_addr));
1011         else                    __ fst_d (as_Address(to_addr));
1012       }
1013 #endif // _LP64
1014       break;
1015     }
1016 

1017     case T_ARRAY:   // fall through
1018     case T_OBJECT:  // fall through
1019       if (UseCompressedOops &amp;&amp; !wide) {
1020         __ movl(as_Address(to_addr), compressed_src);
1021       } else {
1022         __ movptr(as_Address(to_addr), src-&gt;as_register());
1023       }
1024       break;
1025     case T_METADATA:
1026       // We get here to store a method pointer to the stack to pass to
1027       // a dtrace runtime call. This can&#39;t work on 64 bit with
1028       // compressed klass ptrs: T_METADATA can be a compressed klass
1029       // ptr or a 64 bit method pointer.
1030       LP64_ONLY(ShouldNotReachHere());
1031       __ movptr(as_Address(to_addr), src-&gt;as_register());
1032       break;
1033     case T_ADDRESS:
1034       __ movptr(as_Address(to_addr), src-&gt;as_register());
1035       break;
1036     case T_INT:
</pre>
<hr />
<pre>
1170     // push and pop the part at src + wordSize, adding wordSize for the previous push
1171     __ pushl(frame_map()-&gt;address_for_slot(src -&gt;double_stack_ix(), 2 * wordSize));
1172     __ popl (frame_map()-&gt;address_for_slot(dest-&gt;double_stack_ix(), 2 * wordSize));
1173     __ popl (frame_map()-&gt;address_for_slot(dest-&gt;double_stack_ix(), 0));
1174 #endif // _LP64
1175 
1176   } else {
1177     ShouldNotReachHere();
1178   }
1179 }
1180 
1181 
1182 void LIR_Assembler::mem2reg(LIR_Opr src, LIR_Opr dest, BasicType type, LIR_PatchCode patch_code, CodeEmitInfo* info, bool wide, bool /* unaligned */) {
1183   assert(src-&gt;is_address(), &quot;should not call otherwise&quot;);
1184   assert(dest-&gt;is_register(), &quot;should not call otherwise&quot;);
1185 
1186   LIR_Address* addr = src-&gt;as_address_ptr();
1187   Address from_addr = as_Address(addr);
1188   Register tmp_load_klass = LP64_ONLY(rscratch1) NOT_LP64(noreg);
1189 
<span class="line-modified">1190   if (addr-&gt;base()-&gt;type() == T_OBJECT) {</span>
1191     __ verify_oop(addr-&gt;base()-&gt;as_pointer_register());
1192   }
1193 
1194   switch (type) {
1195     case T_BOOLEAN: // fall through
1196     case T_BYTE:    // fall through
1197     case T_CHAR:    // fall through
1198     case T_SHORT:
1199       if (!VM_Version::is_P6() &amp;&amp; !from_addr.uses(dest-&gt;as_register())) {
1200         // on pre P6 processors we may get partial register stalls
1201         // so blow away the value of to_rinfo before loading a
1202         // partial word into it.  Do it here so that it precedes
1203         // the potential patch point below.
1204         __ xorptr(dest-&gt;as_register(), dest-&gt;as_register());
1205       }
1206       break;
1207    default:
1208      break;
1209   }
1210 
</pre>
<hr />
<pre>
1231 #endif // !LP64
1232       }
1233       break;
1234     }
1235 
1236     case T_DOUBLE: {
1237       if (dest-&gt;is_double_xmm()) {
1238         __ movdbl(dest-&gt;as_xmm_double_reg(), from_addr);
1239       } else {
1240 #ifndef _LP64
1241         assert(dest-&gt;is_double_fpu(), &quot;must be&quot;);
1242         assert(dest-&gt;fpu_regnrLo() == 0, &quot;dest must be TOS&quot;);
1243         __ fld_d(from_addr);
1244 #else
1245         ShouldNotReachHere();
1246 #endif // !LP64
1247       }
1248       break;
1249     }
1250 

1251     case T_OBJECT:  // fall through
1252     case T_ARRAY:   // fall through
1253       if (UseCompressedOops &amp;&amp; !wide) {
1254         __ movl(dest-&gt;as_register(), from_addr);
1255       } else {
1256         __ movptr(dest-&gt;as_register(), from_addr);
1257       }
1258       break;
1259 
1260     case T_ADDRESS:
1261       if (UseCompressedClassPointers &amp;&amp; addr-&gt;disp() == oopDesc::klass_offset_in_bytes()) {
1262         __ movl(dest-&gt;as_register(), from_addr);
1263       } else {
1264         __ movptr(dest-&gt;as_register(), from_addr);
1265       }
1266       break;
1267     case T_INT:
1268       __ movl(dest-&gt;as_register(), from_addr);
1269       break;
1270 
</pre>
<hr />
<pre>
1617     add_debug_info_for_null_check_here(op-&gt;stub()-&gt;info());
1618     __ cmpb(Address(op-&gt;klass()-&gt;as_register(),
1619                     InstanceKlass::init_state_offset()),
1620                     InstanceKlass::fully_initialized);
1621     __ jcc(Assembler::notEqual, *op-&gt;stub()-&gt;entry());
1622   }
1623   __ allocate_object(op-&gt;obj()-&gt;as_register(),
1624                      op-&gt;tmp1()-&gt;as_register(),
1625                      op-&gt;tmp2()-&gt;as_register(),
1626                      op-&gt;header_size(),
1627                      op-&gt;object_size(),
1628                      op-&gt;klass()-&gt;as_register(),
1629                      *op-&gt;stub()-&gt;entry());
1630   __ bind(*op-&gt;stub()-&gt;continuation());
1631 }
1632 
1633 void LIR_Assembler::emit_alloc_array(LIR_OpAllocArray* op) {
1634   Register len =  op-&gt;len()-&gt;as_register();
1635   LP64_ONLY( __ movslq(len, len); )
1636 
<span class="line-modified">1637   if (UseSlowPath ||</span>
1638       (!UseFastNewObjectArray &amp;&amp; is_reference_type(op-&gt;type())) ||
1639       (!UseFastNewTypeArray   &amp;&amp; !is_reference_type(op-&gt;type()))) {
1640     __ jmp(*op-&gt;stub()-&gt;entry());
1641   } else {
1642     Register tmp1 = op-&gt;tmp1()-&gt;as_register();
1643     Register tmp2 = op-&gt;tmp2()-&gt;as_register();
1644     Register tmp3 = op-&gt;tmp3()-&gt;as_register();
1645     if (len == tmp1) {
1646       tmp1 = tmp3;
1647     } else if (len == tmp2) {
1648       tmp2 = tmp3;
1649     } else if (len == tmp3) {
1650       // everything is ok
1651     } else {
1652       __ mov(tmp3, len);
1653     }
1654     __ allocate_array(op-&gt;obj()-&gt;as_register(),
1655                       len,
1656                       tmp1,
1657                       tmp2,
</pre>
<hr />
<pre>
1716     assert(data-&gt;is_ReceiverTypeData(), &quot;need ReceiverTypeData for type check&quot;);
1717   }
1718   Label profile_cast_success, profile_cast_failure;
1719   Label *success_target = op-&gt;should_profile() ? &amp;profile_cast_success : success;
1720   Label *failure_target = op-&gt;should_profile() ? &amp;profile_cast_failure : failure;
1721 
1722   if (obj == k_RInfo) {
1723     k_RInfo = dst;
1724   } else if (obj == klass_RInfo) {
1725     klass_RInfo = dst;
1726   }
1727   if (k-&gt;is_loaded() &amp;&amp; !UseCompressedClassPointers) {
1728     select_different_registers(obj, dst, k_RInfo, klass_RInfo);
1729   } else {
1730     Rtmp1 = op-&gt;tmp3()-&gt;as_register();
1731     select_different_registers(obj, dst, k_RInfo, klass_RInfo, Rtmp1);
1732   }
1733 
1734   assert_different_registers(obj, k_RInfo, klass_RInfo);
1735 
<span class="line-modified">1736   __ cmpptr(obj, (int32_t)NULL_WORD);</span>
<span class="line-modified">1737   if (op-&gt;should_profile()) {</span>
<span class="line-modified">1738     Label not_null;</span>
<span class="line-modified">1739     __ jccb(Assembler::notEqual, not_null);</span>
<span class="line-modified">1740     // Object is null; update MDO and exit</span>
<span class="line-modified">1741     Register mdo  = klass_RInfo;</span>
<span class="line-modified">1742     __ mov_metadata(mdo, md-&gt;constant_encoding());</span>
<span class="line-modified">1743     Address data_addr(mdo, md-&gt;byte_offset_of_slot(data, DataLayout::flags_offset()));</span>
<span class="line-modified">1744     int header_bits = BitData::null_seen_byte_constant();</span>
<span class="line-modified">1745     __ orb(data_addr, header_bits);</span>
<span class="line-modified">1746     __ jmp(*obj_is_null);</span>
<span class="line-modified">1747     __ bind(not_null);</span>
<span class="line-modified">1748   } else {</span>
<span class="line-modified">1749     __ jcc(Assembler::equal, *obj_is_null);</span>


1750   }
1751 
1752   if (!k-&gt;is_loaded()) {
1753     klass2reg_with_patching(k_RInfo, op-&gt;info_for_patch());
1754   } else {
1755 #ifdef _LP64
1756     __ mov_metadata(k_RInfo, k-&gt;constant_encoding());
1757 #endif // _LP64
1758   }
1759   __ verify_oop(obj);
1760 
1761   if (op-&gt;fast_check()) {
1762     // get object class
1763     // not a safepoint as obj null check happens earlier
1764 #ifdef _LP64
1765     if (UseCompressedClassPointers) {
1766       __ load_klass(Rtmp1, obj, tmp_load_klass);
1767       __ cmpptr(k_RInfo, Rtmp1);
1768     } else {
1769       __ cmpptr(k_RInfo, Address(obj, oopDesc::klass_offset_in_bytes()));
</pre>
<hr />
<pre>
1941         __ mov(dst, obj);
1942       }
1943     } else
1944       if (code == lir_instanceof) {
1945         Register obj = op-&gt;object()-&gt;as_register();
1946         Register dst = op-&gt;result_opr()-&gt;as_register();
1947         Label success, failure, done;
1948         emit_typecheck_helper(op, &amp;success, &amp;failure, &amp;failure);
1949         __ bind(failure);
1950         __ xorptr(dst, dst);
1951         __ jmpb(done);
1952         __ bind(success);
1953         __ movptr(dst, 1);
1954         __ bind(done);
1955       } else {
1956         ShouldNotReachHere();
1957       }
1958 
1959 }
1960 














































































































1961 
1962 void LIR_Assembler::emit_compare_and_swap(LIR_OpCompareAndSwap* op) {
1963   if (LP64_ONLY(false &amp;&amp;) op-&gt;code() == lir_cas_long &amp;&amp; VM_Version::supports_cx8()) {
1964     assert(op-&gt;cmp_value()-&gt;as_register_lo() == rax, &quot;wrong register&quot;);
1965     assert(op-&gt;cmp_value()-&gt;as_register_hi() == rdx, &quot;wrong register&quot;);
1966     assert(op-&gt;new_value()-&gt;as_register_lo() == rbx, &quot;wrong register&quot;);
1967     assert(op-&gt;new_value()-&gt;as_register_hi() == rcx, &quot;wrong register&quot;);
1968     Register addr = op-&gt;addr()-&gt;as_register();
1969     __ lock();
1970     NOT_LP64(__ cmpxchg8(Address(addr, 0)));
1971 
1972   } else if (op-&gt;code() == lir_cas_int || op-&gt;code() == lir_cas_obj ) {
1973     NOT_LP64(assert(op-&gt;addr()-&gt;is_single_cpu(), &quot;must be single&quot;);)
1974     Register addr = (op-&gt;addr()-&gt;is_single_cpu() ? op-&gt;addr()-&gt;as_register() : op-&gt;addr()-&gt;as_register_lo());
1975     Register newval = op-&gt;new_value()-&gt;as_register();
1976     Register cmpval = op-&gt;cmp_value()-&gt;as_register();
1977     assert(cmpval == rax, &quot;wrong register&quot;);
1978     assert(newval != NULL, &quot;new val must be register&quot;);
1979     assert(cmpval != newval, &quot;cmp and new values must be in different registers&quot;);
1980     assert(cmpval != addr, &quot;cmp and addr must be in different registers&quot;);
</pre>
<hr />
<pre>
2001       __ cmpxchgl(newval, Address(addr, 0));
2002     }
2003 #ifdef _LP64
2004   } else if (op-&gt;code() == lir_cas_long) {
2005     Register addr = (op-&gt;addr()-&gt;is_single_cpu() ? op-&gt;addr()-&gt;as_register() : op-&gt;addr()-&gt;as_register_lo());
2006     Register newval = op-&gt;new_value()-&gt;as_register_lo();
2007     Register cmpval = op-&gt;cmp_value()-&gt;as_register_lo();
2008     assert(cmpval == rax, &quot;wrong register&quot;);
2009     assert(newval != NULL, &quot;new val must be register&quot;);
2010     assert(cmpval != newval, &quot;cmp and new values must be in different registers&quot;);
2011     assert(cmpval != addr, &quot;cmp and addr must be in different registers&quot;);
2012     assert(newval != addr, &quot;new value and addr must be in different registers&quot;);
2013     __ lock();
2014     __ cmpxchgq(newval, Address(addr, 0));
2015 #endif // _LP64
2016   } else {
2017     Unimplemented();
2018   }
2019 }
2020 















2021 void LIR_Assembler::cmove(LIR_Condition condition, LIR_Opr opr1, LIR_Opr opr2, LIR_Opr result, BasicType type) {
2022   Assembler::Condition acond, ncond;
2023   switch (condition) {
2024     case lir_cond_equal:        acond = Assembler::equal;        ncond = Assembler::notEqual;     break;
2025     case lir_cond_notEqual:     acond = Assembler::notEqual;     ncond = Assembler::equal;        break;
2026     case lir_cond_less:         acond = Assembler::less;         ncond = Assembler::greaterEqual; break;
2027     case lir_cond_lessEqual:    acond = Assembler::lessEqual;    ncond = Assembler::greater;      break;
2028     case lir_cond_greaterEqual: acond = Assembler::greaterEqual; ncond = Assembler::less;         break;
2029     case lir_cond_greater:      acond = Assembler::greater;      ncond = Assembler::lessEqual;    break;
2030     case lir_cond_belowEqual:   acond = Assembler::belowEqual;   ncond = Assembler::above;        break;
2031     case lir_cond_aboveEqual:   acond = Assembler::aboveEqual;   ncond = Assembler::below;        break;
2032     default:                    acond = Assembler::equal;        ncond = Assembler::notEqual;
2033                                 ShouldNotReachHere();
2034   }
2035 
2036   if (opr1-&gt;is_cpu_register()) {
2037     reg2reg(opr1, result);
2038   } else if (opr1-&gt;is_stack()) {
2039     stack2reg(opr1, result, result-&gt;type());
2040   } else if (opr1-&gt;is_constant()) {
</pre>
<hr />
<pre>
2880   switch (code) {
2881   case lir_static_call:
2882   case lir_optvirtual_call:
2883   case lir_dynamic_call:
2884     offset += NativeCall::displacement_offset;
2885     break;
2886   case lir_icvirtual_call:
2887     offset += NativeCall::displacement_offset + NativeMovConstReg::instruction_size;
2888     break;
2889   case lir_virtual_call:  // currently, sparc-specific for niagara
2890   default: ShouldNotReachHere();
2891   }
2892   __ align(BytesPerWord, offset);
2893 }
2894 
2895 
2896 void LIR_Assembler::call(LIR_OpJavaCall* op, relocInfo::relocType rtype) {
2897   assert((__ offset() + NativeCall::displacement_offset) % BytesPerWord == 0,
2898          &quot;must be aligned&quot;);
2899   __ call(AddressLiteral(op-&gt;addr(), rtype));
<span class="line-modified">2900   add_call_info(code_offset(), op-&gt;info());</span>
2901 }
2902 
2903 
2904 void LIR_Assembler::ic_call(LIR_OpJavaCall* op) {
2905   __ ic_call(op-&gt;addr());
<span class="line-modified">2906   add_call_info(code_offset(), op-&gt;info());</span>
2907   assert((__ offset() - NativeCall::instruction_size + NativeCall::displacement_offset) % BytesPerWord == 0,
2908          &quot;must be aligned&quot;);
2909 }
2910 
2911 
2912 /* Currently, vtable-dispatch is only enabled for sparc platforms */
2913 void LIR_Assembler::vtable_call(LIR_OpJavaCall* op) {
2914   ShouldNotReachHere();
2915 }
2916 
2917 
2918 void LIR_Assembler::emit_static_call_stub() {
2919   address call_pc = __ pc();
2920   address stub = __ start_a_stub(call_stub_size());
2921   if (stub == NULL) {
2922     bailout(&quot;static call stub overflow&quot;);
2923     return;
2924   }
2925 
2926   int start = __ offset();
</pre>
<hr />
<pre>
3082   __ movptr (Address(rsp, offset_from_rsp_in_bytes), c);
3083 }
3084 
3085 
3086 void LIR_Assembler::store_parameter(jobject o,  int offset_from_rsp_in_words) {
3087   assert(offset_from_rsp_in_words &gt;= 0, &quot;invalid offset from rsp&quot;);
3088   int offset_from_rsp_in_bytes = offset_from_rsp_in_words * BytesPerWord;
3089   assert(offset_from_rsp_in_bytes &lt; frame_map()-&gt;reserved_argument_area_size(), &quot;invalid offset&quot;);
3090   __ movoop (Address(rsp, offset_from_rsp_in_bytes), o);
3091 }
3092 
3093 
3094 void LIR_Assembler::store_parameter(Metadata* m,  int offset_from_rsp_in_words) {
3095   assert(offset_from_rsp_in_words &gt;= 0, &quot;invalid offset from rsp&quot;);
3096   int offset_from_rsp_in_bytes = offset_from_rsp_in_words * BytesPerWord;
3097   assert(offset_from_rsp_in_bytes &lt; frame_map()-&gt;reserved_argument_area_size(), &quot;invalid offset&quot;);
3098   __ mov_metadata(Address(rsp, offset_from_rsp_in_bytes), m);
3099 }
3100 
3101 



















3102 // This code replaces a call to arraycopy; no exception may
3103 // be thrown in this code, they must be thrown in the System.arraycopy
3104 // activation frame; we could save some checks if this would not be the case
3105 void LIR_Assembler::emit_arraycopy(LIR_OpArrayCopy* op) {
3106   ciArrayKlass* default_type = op-&gt;expected_type();
3107   Register src = op-&gt;src()-&gt;as_register();
3108   Register dst = op-&gt;dst()-&gt;as_register();
3109   Register src_pos = op-&gt;src_pos()-&gt;as_register();
3110   Register dst_pos = op-&gt;dst_pos()-&gt;as_register();
3111   Register length  = op-&gt;length()-&gt;as_register();
3112   Register tmp = op-&gt;tmp()-&gt;as_register();
3113   Register tmp_load_klass = LP64_ONLY(rscratch1) NOT_LP64(noreg);
3114 
3115   __ resolve(ACCESS_READ, src);
3116   __ resolve(ACCESS_WRITE, dst);
3117 
3118   CodeStub* stub = op-&gt;stub();
3119   int flags = op-&gt;flags();
3120   BasicType basic_type = default_type != NULL ? default_type-&gt;element_type()-&gt;basic_type() : T_ILLEGAL;
3121   if (is_reference_type(basic_type)) basic_type = T_OBJECT;
3122 














3123   // if we don&#39;t know anything, just go through the generic arraycopy
3124   if (default_type == NULL) {
3125     // save outgoing arguments on stack in case call to System.arraycopy is needed
3126     // HACK ALERT. This code used to push the parameters in a hardwired fashion
3127     // for interpreter calling conventions. Now we have to do it in new style conventions.
3128     // For the moment until C1 gets the new register allocator I just force all the
3129     // args to the right place (except the register args) and then on the back side
3130     // reload the register args properly if we go slow path. Yuck
3131 
3132     // These are proper for the calling convention
3133     store_parameter(length, 2);
3134     store_parameter(dst_pos, 1);
3135     store_parameter(dst, 0);
3136 
3137     // these are just temporary placements until we need to reload
3138     store_parameter(src_pos, 3);
3139     store_parameter(src, 4);
3140     NOT_LP64(assert(src == rcx &amp;&amp; src_pos == rdx, &quot;mismatch in calling convention&quot;);)
3141 
3142     address copyfunc_addr = StubRoutines::generic_arraycopy();
</pre>
<hr />
<pre>
4040 }
4041 
4042 void LIR_Assembler::membar_storeload() {
4043   __ membar(Assembler::Membar_mask_bits(Assembler::StoreLoad));
4044 }
4045 
4046 void LIR_Assembler::on_spin_wait() {
4047   __ pause ();
4048 }
4049 
4050 void LIR_Assembler::get_thread(LIR_Opr result_reg) {
4051   assert(result_reg-&gt;is_register(), &quot;check&quot;);
4052 #ifdef _LP64
4053   // __ get_thread(result_reg-&gt;as_register_lo());
4054   __ mov(result_reg-&gt;as_register(), r15_thread);
4055 #else
4056   __ get_thread(result_reg-&gt;as_register());
4057 #endif // _LP64
4058 }
4059 



4060 
4061 void LIR_Assembler::peephole(LIR_List*) {
4062   // do nothing for now
4063 }
4064 
4065 void LIR_Assembler::atomic_op(LIR_Code code, LIR_Opr src, LIR_Opr data, LIR_Opr dest, LIR_Opr tmp) {
4066   assert(data == dest, &quot;xchg/xadd uses only 2 operands&quot;);
4067 
4068   if (data-&gt;type() == T_INT) {
4069     if (code == lir_xadd) {
4070       __ lock();
4071       __ xaddl(as_Address(src-&gt;as_address_ptr()), data-&gt;as_register());
4072     } else {
4073       __ xchgl(data-&gt;as_register(), as_Address(src-&gt;as_address_ptr()));
4074     }
4075   } else if (data-&gt;is_oop()) {
4076     assert (code == lir_xchg, &quot;xadd for oops&quot;);
4077     Register obj = data-&gt;as_register();
4078 #ifdef _LP64
4079     if (UseCompressedOops) {
</pre>
</td>
<td>
<hr />
<pre>
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #include &quot;precompiled.hpp&quot;
  26 #include &quot;asm/macroAssembler.hpp&quot;
  27 #include &quot;asm/macroAssembler.inline.hpp&quot;
  28 #include &quot;c1/c1_Compilation.hpp&quot;
  29 #include &quot;c1/c1_LIRAssembler.hpp&quot;
  30 #include &quot;c1/c1_MacroAssembler.hpp&quot;
  31 #include &quot;c1/c1_Runtime1.hpp&quot;
  32 #include &quot;c1/c1_ValueStack.hpp&quot;
  33 #include &quot;ci/ciArrayKlass.hpp&quot;
  34 #include &quot;ci/ciInstance.hpp&quot;
<span class="line-added">  35 #include &quot;ci/ciValueKlass.hpp&quot;</span>
  36 #include &quot;gc/shared/collectedHeap.hpp&quot;
  37 #include &quot;nativeInst_x86.hpp&quot;
<span class="line-added">  38 #include &quot;oops/oop.inline.hpp&quot;</span>
  39 #include &quot;oops/objArrayKlass.hpp&quot;
  40 #include &quot;runtime/frame.inline.hpp&quot;
  41 #include &quot;runtime/safepointMechanism.hpp&quot;
  42 #include &quot;runtime/sharedRuntime.hpp&quot;
  43 #include &quot;utilities/powerOfTwo.hpp&quot;
  44 #include &quot;vmreg_x86.inline.hpp&quot;
  45 
  46 
  47 // These masks are used to provide 128-bit aligned bitmasks to the XMM
  48 // instructions, to allow sign-masking or sign-bit flipping.  They allow
  49 // fast versions of NegF/NegD and AbsF/AbsD.
  50 
  51 // Note: &#39;double&#39; and &#39;long long&#39; have 32-bits alignment on x86.
  52 static jlong* double_quadword(jlong *adr, jlong lo, jlong hi) {
  53   // Use the expression (adr)&amp;(~0xF) to provide 128-bits aligned address
  54   // of 128-bits operands for SSE instructions.
  55   jlong *operand = (jlong*)(((intptr_t)adr) &amp; ((intptr_t)(~0xF)));
  56   // Store the value to a 128-bits operand.
  57   operand[0] = lo;
  58   operand[1] = hi;
</pre>
<hr />
<pre>
 176 
 177 void LIR_Assembler::ffree(int i) {
 178   __ ffree(i);
 179 }
 180 #endif // !_LP64
 181 
 182 void LIR_Assembler::breakpoint() {
 183   __ int3();
 184 }
 185 
 186 void LIR_Assembler::push(LIR_Opr opr) {
 187   if (opr-&gt;is_single_cpu()) {
 188     __ push_reg(opr-&gt;as_register());
 189   } else if (opr-&gt;is_double_cpu()) {
 190     NOT_LP64(__ push_reg(opr-&gt;as_register_hi()));
 191     __ push_reg(opr-&gt;as_register_lo());
 192   } else if (opr-&gt;is_stack()) {
 193     __ push_addr(frame_map()-&gt;address_for_slot(opr-&gt;single_stack_ix()));
 194   } else if (opr-&gt;is_constant()) {
 195     LIR_Const* const_opr = opr-&gt;as_constant_ptr();
<span class="line-modified"> 196     if (const_opr-&gt;type() == T_OBJECT || const_opr-&gt;type() == T_VALUETYPE) {</span>
 197       __ push_oop(const_opr-&gt;as_jobject());
 198     } else if (const_opr-&gt;type() == T_INT) {
 199       __ push_jint(const_opr-&gt;as_jint());
 200     } else {
 201       ShouldNotReachHere();
 202     }
 203 
 204   } else {
 205     ShouldNotReachHere();
 206   }
 207 }
 208 
 209 void LIR_Assembler::pop(LIR_Opr opr) {
 210   if (opr-&gt;is_single_cpu()) {
 211     __ pop_reg(opr-&gt;as_register());
 212   } else {
 213     ShouldNotReachHere();
 214   }
 215 }
 216 
</pre>
<hr />
<pre>
 463     __ bind(*stub-&gt;continuation());
 464   }
 465 
 466   if (compilation()-&gt;env()-&gt;dtrace_method_probes()) {
 467 #ifdef _LP64
 468     __ mov(rdi, r15_thread);
 469     __ mov_metadata(rsi, method()-&gt;constant_encoding());
 470 #else
 471     __ get_thread(rax);
 472     __ movptr(Address(rsp, 0), rax);
 473     __ mov_metadata(Address(rsp, sizeof(void*)), method()-&gt;constant_encoding());
 474 #endif
 475     __ call(RuntimeAddress(CAST_FROM_FN_PTR(address, SharedRuntime::dtrace_method_exit)));
 476   }
 477 
 478   if (method()-&gt;is_synchronized() || compilation()-&gt;env()-&gt;dtrace_method_probes()) {
 479     __ mov(rax, rbx);  // Restore the exception
 480   }
 481 
 482   // remove the activation and dispatch to the unwind handler
<span class="line-modified"> 483   int initial_framesize = initial_frame_size_in_bytes();</span>
<span class="line-added"> 484   __ remove_frame(initial_framesize, needs_stack_repair(), initial_framesize - wordSize);</span>
 485   __ jump(RuntimeAddress(Runtime1::entry_for(Runtime1::unwind_exception_id)));
 486 
 487   // Emit the slow path assembly
 488   if (stub != NULL) {
 489     stub-&gt;emit_code(this);
 490   }
 491 
 492   return offset;
 493 }
 494 
 495 
 496 int LIR_Assembler::emit_deopt_handler() {
 497   // if the last instruction is a call (typically to do a throw which
 498   // is coming at the end after block reordering) the return address
 499   // must still point into the code area in order to avoid assertion
 500   // failures when searching for the corresponding bci =&gt; add a nop
 501   // (was bug 5/14/1999 - gri)
 502   __ nop();
 503 
 504   // generate code for exception handler
</pre>
<hr />
<pre>
 510   }
 511 
 512   int offset = code_offset();
 513   InternalAddress here(__ pc());
 514 
 515   __ pushptr(here.addr());
 516   __ jump(RuntimeAddress(SharedRuntime::deopt_blob()-&gt;unpack()));
 517   guarantee(code_offset() - offset &lt;= deopt_handler_size(), &quot;overflow&quot;);
 518   __ end_a_stub();
 519 
 520   return offset;
 521 }
 522 
 523 
 524 void LIR_Assembler::return_op(LIR_Opr result) {
 525   assert(result-&gt;is_illegal() || !result-&gt;is_single_cpu() || result-&gt;as_register() == rax, &quot;word returns are in rax,&quot;);
 526   if (!result-&gt;is_illegal() &amp;&amp; result-&gt;is_float_kind() &amp;&amp; !result-&gt;is_xmm_register()) {
 527     assert(result-&gt;fpu() == 0, &quot;result must already be on TOS&quot;);
 528   }
 529 
<span class="line-added"> 530   ciMethod* method = compilation()-&gt;method();</span>
<span class="line-added"> 531   if (InlineTypeReturnedAsFields &amp;&amp; method-&gt;signature()-&gt;returns_never_null()) {</span>
<span class="line-added"> 532     ciType* return_type = method-&gt;return_type();</span>
<span class="line-added"> 533     if (return_type-&gt;is_valuetype()) {</span>
<span class="line-added"> 534       ciValueKlass* vk = return_type-&gt;as_value_klass();</span>
<span class="line-added"> 535       if (vk-&gt;can_be_returned_as_fields()) {</span>
<span class="line-added"> 536 #ifndef _LP64</span>
<span class="line-added"> 537         Unimplemented();</span>
<span class="line-added"> 538 #else</span>
<span class="line-added"> 539         address unpack_handler = vk-&gt;unpack_handler();</span>
<span class="line-added"> 540         assert(unpack_handler != NULL, &quot;must be&quot;);</span>
<span class="line-added"> 541         __ call(RuntimeAddress(unpack_handler));</span>
<span class="line-added"> 542         // At this point, rax points to the value object (for interpreter or C1 caller).</span>
<span class="line-added"> 543         // The fields of the object are copied into registers (for C2 caller).</span>
<span class="line-added"> 544 #endif</span>
<span class="line-added"> 545       }</span>
<span class="line-added"> 546     }</span>
<span class="line-added"> 547   }</span>
<span class="line-added"> 548 </span>
 549   // Pop the stack before the safepoint code
<span class="line-modified"> 550   int initial_framesize = initial_frame_size_in_bytes();</span>
<span class="line-added"> 551   __ remove_frame(initial_framesize, needs_stack_repair(), initial_framesize - wordSize);</span>
 552 
 553   if (StackReservedPages &gt; 0 &amp;&amp; compilation()-&gt;has_reserved_stack_access()) {
 554     __ reserved_stack_check();
 555   }
 556 
 557   bool result_is_oop = result-&gt;is_valid() ? result-&gt;is_oop() : false;
 558 
 559   // Note: we do not need to round double result; float result has the right precision
 560   // the poll sets the condition code, but no data registers
 561 
 562 #ifdef _LP64
 563   const Register poll_addr = rscratch1;
 564   __ movptr(poll_addr, Address(r15_thread, Thread::polling_page_offset()));
 565 #else
 566   const Register poll_addr = rbx;
 567   assert(FrameMap::is_caller_save_register(poll_addr), &quot;will overwrite&quot;);
 568   __ get_thread(poll_addr);
 569   __ movptr(poll_addr, Address(poll_addr, Thread::polling_page_offset()));
 570 #endif
 571   __ relocate(relocInfo::poll_return_type);
 572   __ testl(rax, Address(poll_addr, 0));
 573   __ ret(0);
 574 }
 575 
 576 
<span class="line-added"> 577 int LIR_Assembler::store_value_type_fields_to_buf(ciValueKlass* vk) {</span>
<span class="line-added"> 578   return (__ store_value_type_fields_to_buf(vk, false));</span>
<span class="line-added"> 579 }</span>
<span class="line-added"> 580 </span>
 581 int LIR_Assembler::safepoint_poll(LIR_Opr tmp, CodeEmitInfo* info) {
 582   guarantee(info != NULL, &quot;Shouldn&#39;t be NULL&quot;);
 583   int offset = __ offset();
 584 #ifdef _LP64
 585   const Register poll_addr = rscratch1;
 586   __ movptr(poll_addr, Address(r15_thread, Thread::polling_page_offset()));
 587 #else
 588   assert(tmp-&gt;is_cpu_register(), &quot;needed&quot;);
 589   const Register poll_addr = tmp-&gt;as_register();
 590   __ get_thread(poll_addr);
 591   __ movptr(poll_addr, Address(poll_addr, in_bytes(Thread::polling_page_offset())));
 592 #endif
 593   add_debug_info_for_branch(info);
 594   __ relocate(relocInfo::poll_type);
 595   address pre_pc = __ pc();
 596   __ testl(rax, Address(poll_addr, 0));
 597   address post_pc = __ pc();
 598   guarantee(pointer_delta(post_pc, pre_pc, 1) == 2 LP64_ONLY(+1), &quot;must be exact length&quot;);
 599   return offset;
 600 }
</pre>
<hr />
<pre>
 621       break;
 622     }
 623 
 624     case T_ADDRESS: {
 625       assert(patch_code == lir_patch_none, &quot;no patching handled here&quot;);
 626       __ movptr(dest-&gt;as_register(), c-&gt;as_jint());
 627       break;
 628     }
 629 
 630     case T_LONG: {
 631       assert(patch_code == lir_patch_none, &quot;no patching handled here&quot;);
 632 #ifdef _LP64
 633       __ movptr(dest-&gt;as_register_lo(), (intptr_t)c-&gt;as_jlong());
 634 #else
 635       __ movptr(dest-&gt;as_register_lo(), c-&gt;as_jint_lo());
 636       __ movptr(dest-&gt;as_register_hi(), c-&gt;as_jint_hi());
 637 #endif // _LP64
 638       break;
 639     }
 640 
<span class="line-added"> 641     case T_VALUETYPE: // Fall through</span>
 642     case T_OBJECT: {
 643       if (patch_code != lir_patch_none) {
 644         jobject2reg_with_patching(dest-&gt;as_register(), info);
 645       } else {
 646         __ movoop(dest-&gt;as_register(), c-&gt;as_jobject());
 647       }
 648       break;
 649     }
 650 
 651     case T_METADATA: {
 652       if (patch_code != lir_patch_none) {
 653         klass2reg_with_patching(dest-&gt;as_register(), info);
 654       } else {
 655         __ mov_metadata(dest-&gt;as_register(), c-&gt;as_metadata());
 656       }
 657       break;
 658     }
 659 
 660     case T_FLOAT: {
 661       if (dest-&gt;is_single_xmm()) {
</pre>
<hr />
<pre>
 712     default:
 713       ShouldNotReachHere();
 714   }
 715 }
 716 
 717 void LIR_Assembler::const2stack(LIR_Opr src, LIR_Opr dest) {
 718   assert(src-&gt;is_constant(), &quot;should not call otherwise&quot;);
 719   assert(dest-&gt;is_stack(), &quot;should not call otherwise&quot;);
 720   LIR_Const* c = src-&gt;as_constant_ptr();
 721 
 722   switch (c-&gt;type()) {
 723     case T_INT:  // fall through
 724     case T_FLOAT:
 725       __ movl(frame_map()-&gt;address_for_slot(dest-&gt;single_stack_ix()), c-&gt;as_jint_bits());
 726       break;
 727 
 728     case T_ADDRESS:
 729       __ movptr(frame_map()-&gt;address_for_slot(dest-&gt;single_stack_ix()), c-&gt;as_jint_bits());
 730       break;
 731 
<span class="line-added"> 732     case T_VALUETYPE: // Fall through</span>
 733     case T_OBJECT:
 734       __ movoop(frame_map()-&gt;address_for_slot(dest-&gt;single_stack_ix()), c-&gt;as_jobject());
 735       break;
 736 
 737     case T_LONG:  // fall through
 738     case T_DOUBLE:
 739 #ifdef _LP64
 740       __ movptr(frame_map()-&gt;address_for_slot(dest-&gt;double_stack_ix(),
 741                                             lo_word_offset_in_bytes), (intptr_t)c-&gt;as_jlong_bits());
 742 #else
 743       __ movptr(frame_map()-&gt;address_for_slot(dest-&gt;double_stack_ix(),
 744                                               lo_word_offset_in_bytes), c-&gt;as_jint_lo_bits());
 745       __ movptr(frame_map()-&gt;address_for_slot(dest-&gt;double_stack_ix(),
 746                                               hi_word_offset_in_bytes), c-&gt;as_jint_hi_bits());
 747 #endif // _LP64
 748       break;
 749 
 750     default:
 751       ShouldNotReachHere();
 752   }
 753 }
 754 
 755 void LIR_Assembler::const2mem(LIR_Opr src, LIR_Opr dest, BasicType type, CodeEmitInfo* info, bool wide) {
 756   assert(src-&gt;is_constant(), &quot;should not call otherwise&quot;);
 757   assert(dest-&gt;is_address(), &quot;should not call otherwise&quot;);
 758   LIR_Const* c = src-&gt;as_constant_ptr();
 759   LIR_Address* addr = dest-&gt;as_address_ptr();
 760 
 761   int null_check_here = code_offset();
 762   switch (type) {
 763     case T_INT:    // fall through
 764     case T_FLOAT:
 765       __ movl(as_Address(addr), c-&gt;as_jint_bits());
 766       break;
 767 
 768     case T_ADDRESS:
 769       __ movptr(as_Address(addr), c-&gt;as_jint_bits());
 770       break;
 771 
<span class="line-added"> 772     case T_VALUETYPE: // fall through</span>
 773     case T_OBJECT:  // fall through
 774     case T_ARRAY:
 775       if (c-&gt;as_jobject() == NULL) {
 776         if (UseCompressedOops &amp;&amp; !wide) {
 777           __ movl(as_Address(addr), (int32_t)NULL_WORD);
 778         } else {
 779 #ifdef _LP64
 780           __ xorptr(rscratch1, rscratch1);
 781           null_check_here = code_offset();
 782           __ movptr(as_Address(addr), rscratch1);
 783 #else
 784           __ movptr(as_Address(addr), NULL_WORD);
 785 #endif
 786         }
 787       } else {
 788         if (is_literal_address(addr)) {
 789           ShouldNotReachHere();
 790           __ movoop(as_Address(addr, noreg), c-&gt;as_jobject());
 791         } else {
 792 #ifdef _LP64
</pre>
<hr />
<pre>
 841   if (info != NULL) {
 842     add_debug_info_for_null_check(null_check_here, info);
 843   }
 844 }
 845 
 846 
 847 void LIR_Assembler::reg2reg(LIR_Opr src, LIR_Opr dest) {
 848   assert(src-&gt;is_register(), &quot;should not call otherwise&quot;);
 849   assert(dest-&gt;is_register(), &quot;should not call otherwise&quot;);
 850 
 851   // move between cpu-registers
 852   if (dest-&gt;is_single_cpu()) {
 853 #ifdef _LP64
 854     if (src-&gt;type() == T_LONG) {
 855       // Can do LONG -&gt; OBJECT
 856       move_regs(src-&gt;as_register_lo(), dest-&gt;as_register());
 857       return;
 858     }
 859 #endif
 860     assert(src-&gt;is_single_cpu(), &quot;must match&quot;);
<span class="line-modified"> 861     if (src-&gt;type() == T_OBJECT || src-&gt;type() == T_VALUETYPE) {</span>
 862       __ verify_oop(src-&gt;as_register());
 863     }
 864     move_regs(src-&gt;as_register(), dest-&gt;as_register());
 865 
 866   } else if (dest-&gt;is_double_cpu()) {
 867 #ifdef _LP64
 868     if (is_reference_type(src-&gt;type())) {
 869       // Surprising to me but we can see move of a long to t_object
 870       __ verify_oop(src-&gt;as_register());
 871       move_regs(src-&gt;as_register(), dest-&gt;as_register_lo());
 872       return;
 873     }
 874 #endif
 875     assert(src-&gt;is_double_cpu(), &quot;must match&quot;);
 876     Register f_lo = src-&gt;as_register_lo();
 877     Register f_hi = src-&gt;as_register_hi();
 878     Register t_lo = dest-&gt;as_register_lo();
 879     Register t_hi = dest-&gt;as_register_hi();
 880 #ifdef _LP64
 881     assert(f_hi == f_lo, &quot;must be same&quot;);
</pre>
<hr />
<pre>
1027       break;
1028     }
1029 
1030     case T_DOUBLE: {
1031 #ifdef _LP64
1032       assert(src-&gt;is_double_xmm(), &quot;not a double&quot;);
1033       __ movdbl(as_Address(to_addr), src-&gt;as_xmm_double_reg());
1034 #else
1035       if (src-&gt;is_double_xmm()) {
1036         __ movdbl(as_Address(to_addr), src-&gt;as_xmm_double_reg());
1037       } else {
1038         assert(src-&gt;is_double_fpu(), &quot;must be&quot;);
1039         assert(src-&gt;fpu_regnrLo() == 0, &quot;argument must be on TOS&quot;);
1040         if (pop_fpu_stack)      __ fstp_d(as_Address(to_addr));
1041         else                    __ fst_d (as_Address(to_addr));
1042       }
1043 #endif // _LP64
1044       break;
1045     }
1046 
<span class="line-added">1047     case T_VALUETYPE: // fall through</span>
1048     case T_ARRAY:   // fall through
1049     case T_OBJECT:  // fall through
1050       if (UseCompressedOops &amp;&amp; !wide) {
1051         __ movl(as_Address(to_addr), compressed_src);
1052       } else {
1053         __ movptr(as_Address(to_addr), src-&gt;as_register());
1054       }
1055       break;
1056     case T_METADATA:
1057       // We get here to store a method pointer to the stack to pass to
1058       // a dtrace runtime call. This can&#39;t work on 64 bit with
1059       // compressed klass ptrs: T_METADATA can be a compressed klass
1060       // ptr or a 64 bit method pointer.
1061       LP64_ONLY(ShouldNotReachHere());
1062       __ movptr(as_Address(to_addr), src-&gt;as_register());
1063       break;
1064     case T_ADDRESS:
1065       __ movptr(as_Address(to_addr), src-&gt;as_register());
1066       break;
1067     case T_INT:
</pre>
<hr />
<pre>
1201     // push and pop the part at src + wordSize, adding wordSize for the previous push
1202     __ pushl(frame_map()-&gt;address_for_slot(src -&gt;double_stack_ix(), 2 * wordSize));
1203     __ popl (frame_map()-&gt;address_for_slot(dest-&gt;double_stack_ix(), 2 * wordSize));
1204     __ popl (frame_map()-&gt;address_for_slot(dest-&gt;double_stack_ix(), 0));
1205 #endif // _LP64
1206 
1207   } else {
1208     ShouldNotReachHere();
1209   }
1210 }
1211 
1212 
1213 void LIR_Assembler::mem2reg(LIR_Opr src, LIR_Opr dest, BasicType type, LIR_PatchCode patch_code, CodeEmitInfo* info, bool wide, bool /* unaligned */) {
1214   assert(src-&gt;is_address(), &quot;should not call otherwise&quot;);
1215   assert(dest-&gt;is_register(), &quot;should not call otherwise&quot;);
1216 
1217   LIR_Address* addr = src-&gt;as_address_ptr();
1218   Address from_addr = as_Address(addr);
1219   Register tmp_load_klass = LP64_ONLY(rscratch1) NOT_LP64(noreg);
1220 
<span class="line-modified">1221   if (addr-&gt;base()-&gt;type() == T_OBJECT || addr-&gt;base()-&gt;type() == T_VALUETYPE) {</span>
1222     __ verify_oop(addr-&gt;base()-&gt;as_pointer_register());
1223   }
1224 
1225   switch (type) {
1226     case T_BOOLEAN: // fall through
1227     case T_BYTE:    // fall through
1228     case T_CHAR:    // fall through
1229     case T_SHORT:
1230       if (!VM_Version::is_P6() &amp;&amp; !from_addr.uses(dest-&gt;as_register())) {
1231         // on pre P6 processors we may get partial register stalls
1232         // so blow away the value of to_rinfo before loading a
1233         // partial word into it.  Do it here so that it precedes
1234         // the potential patch point below.
1235         __ xorptr(dest-&gt;as_register(), dest-&gt;as_register());
1236       }
1237       break;
1238    default:
1239      break;
1240   }
1241 
</pre>
<hr />
<pre>
1262 #endif // !LP64
1263       }
1264       break;
1265     }
1266 
1267     case T_DOUBLE: {
1268       if (dest-&gt;is_double_xmm()) {
1269         __ movdbl(dest-&gt;as_xmm_double_reg(), from_addr);
1270       } else {
1271 #ifndef _LP64
1272         assert(dest-&gt;is_double_fpu(), &quot;must be&quot;);
1273         assert(dest-&gt;fpu_regnrLo() == 0, &quot;dest must be TOS&quot;);
1274         __ fld_d(from_addr);
1275 #else
1276         ShouldNotReachHere();
1277 #endif // !LP64
1278       }
1279       break;
1280     }
1281 
<span class="line-added">1282     case T_VALUETYPE: // fall through</span>
1283     case T_OBJECT:  // fall through
1284     case T_ARRAY:   // fall through
1285       if (UseCompressedOops &amp;&amp; !wide) {
1286         __ movl(dest-&gt;as_register(), from_addr);
1287       } else {
1288         __ movptr(dest-&gt;as_register(), from_addr);
1289       }
1290       break;
1291 
1292     case T_ADDRESS:
1293       if (UseCompressedClassPointers &amp;&amp; addr-&gt;disp() == oopDesc::klass_offset_in_bytes()) {
1294         __ movl(dest-&gt;as_register(), from_addr);
1295       } else {
1296         __ movptr(dest-&gt;as_register(), from_addr);
1297       }
1298       break;
1299     case T_INT:
1300       __ movl(dest-&gt;as_register(), from_addr);
1301       break;
1302 
</pre>
<hr />
<pre>
1649     add_debug_info_for_null_check_here(op-&gt;stub()-&gt;info());
1650     __ cmpb(Address(op-&gt;klass()-&gt;as_register(),
1651                     InstanceKlass::init_state_offset()),
1652                     InstanceKlass::fully_initialized);
1653     __ jcc(Assembler::notEqual, *op-&gt;stub()-&gt;entry());
1654   }
1655   __ allocate_object(op-&gt;obj()-&gt;as_register(),
1656                      op-&gt;tmp1()-&gt;as_register(),
1657                      op-&gt;tmp2()-&gt;as_register(),
1658                      op-&gt;header_size(),
1659                      op-&gt;object_size(),
1660                      op-&gt;klass()-&gt;as_register(),
1661                      *op-&gt;stub()-&gt;entry());
1662   __ bind(*op-&gt;stub()-&gt;continuation());
1663 }
1664 
1665 void LIR_Assembler::emit_alloc_array(LIR_OpAllocArray* op) {
1666   Register len =  op-&gt;len()-&gt;as_register();
1667   LP64_ONLY( __ movslq(len, len); )
1668 
<span class="line-modified">1669   if (UseSlowPath || op-&gt;type() == T_VALUETYPE ||</span>
1670       (!UseFastNewObjectArray &amp;&amp; is_reference_type(op-&gt;type())) ||
1671       (!UseFastNewTypeArray   &amp;&amp; !is_reference_type(op-&gt;type()))) {
1672     __ jmp(*op-&gt;stub()-&gt;entry());
1673   } else {
1674     Register tmp1 = op-&gt;tmp1()-&gt;as_register();
1675     Register tmp2 = op-&gt;tmp2()-&gt;as_register();
1676     Register tmp3 = op-&gt;tmp3()-&gt;as_register();
1677     if (len == tmp1) {
1678       tmp1 = tmp3;
1679     } else if (len == tmp2) {
1680       tmp2 = tmp3;
1681     } else if (len == tmp3) {
1682       // everything is ok
1683     } else {
1684       __ mov(tmp3, len);
1685     }
1686     __ allocate_array(op-&gt;obj()-&gt;as_register(),
1687                       len,
1688                       tmp1,
1689                       tmp2,
</pre>
<hr />
<pre>
1748     assert(data-&gt;is_ReceiverTypeData(), &quot;need ReceiverTypeData for type check&quot;);
1749   }
1750   Label profile_cast_success, profile_cast_failure;
1751   Label *success_target = op-&gt;should_profile() ? &amp;profile_cast_success : success;
1752   Label *failure_target = op-&gt;should_profile() ? &amp;profile_cast_failure : failure;
1753 
1754   if (obj == k_RInfo) {
1755     k_RInfo = dst;
1756   } else if (obj == klass_RInfo) {
1757     klass_RInfo = dst;
1758   }
1759   if (k-&gt;is_loaded() &amp;&amp; !UseCompressedClassPointers) {
1760     select_different_registers(obj, dst, k_RInfo, klass_RInfo);
1761   } else {
1762     Rtmp1 = op-&gt;tmp3()-&gt;as_register();
1763     select_different_registers(obj, dst, k_RInfo, klass_RInfo, Rtmp1);
1764   }
1765 
1766   assert_different_registers(obj, k_RInfo, klass_RInfo);
1767 
<span class="line-modified">1768   if (op-&gt;need_null_check()) {</span>
<span class="line-modified">1769     __ cmpptr(obj, (int32_t)NULL_WORD);</span>
<span class="line-modified">1770     if (op-&gt;should_profile()) {</span>
<span class="line-modified">1771       Label not_null;</span>
<span class="line-modified">1772       __ jccb(Assembler::notEqual, not_null);</span>
<span class="line-modified">1773       // Object is null; update MDO and exit</span>
<span class="line-modified">1774       Register mdo  = klass_RInfo;</span>
<span class="line-modified">1775       __ mov_metadata(mdo, md-&gt;constant_encoding());</span>
<span class="line-modified">1776       Address data_addr(mdo, md-&gt;byte_offset_of_slot(data, DataLayout::flags_offset()));</span>
<span class="line-modified">1777       int header_bits = BitData::null_seen_byte_constant();</span>
<span class="line-modified">1778       __ orb(data_addr, header_bits);</span>
<span class="line-modified">1779       __ jmp(*obj_is_null);</span>
<span class="line-modified">1780       __ bind(not_null);</span>
<span class="line-modified">1781     } else {</span>
<span class="line-added">1782       __ jcc(Assembler::equal, *obj_is_null);</span>
<span class="line-added">1783     }</span>
1784   }
1785 
1786   if (!k-&gt;is_loaded()) {
1787     klass2reg_with_patching(k_RInfo, op-&gt;info_for_patch());
1788   } else {
1789 #ifdef _LP64
1790     __ mov_metadata(k_RInfo, k-&gt;constant_encoding());
1791 #endif // _LP64
1792   }
1793   __ verify_oop(obj);
1794 
1795   if (op-&gt;fast_check()) {
1796     // get object class
1797     // not a safepoint as obj null check happens earlier
1798 #ifdef _LP64
1799     if (UseCompressedClassPointers) {
1800       __ load_klass(Rtmp1, obj, tmp_load_klass);
1801       __ cmpptr(k_RInfo, Rtmp1);
1802     } else {
1803       __ cmpptr(k_RInfo, Address(obj, oopDesc::klass_offset_in_bytes()));
</pre>
<hr />
<pre>
1975         __ mov(dst, obj);
1976       }
1977     } else
1978       if (code == lir_instanceof) {
1979         Register obj = op-&gt;object()-&gt;as_register();
1980         Register dst = op-&gt;result_opr()-&gt;as_register();
1981         Label success, failure, done;
1982         emit_typecheck_helper(op, &amp;success, &amp;failure, &amp;failure);
1983         __ bind(failure);
1984         __ xorptr(dst, dst);
1985         __ jmpb(done);
1986         __ bind(success);
1987         __ movptr(dst, 1);
1988         __ bind(done);
1989       } else {
1990         ShouldNotReachHere();
1991       }
1992 
1993 }
1994 
<span class="line-added">1995 void LIR_Assembler::emit_opFlattenedArrayCheck(LIR_OpFlattenedArrayCheck* op) {</span>
<span class="line-added">1996   // We are loading/storing from/to an array that *may* be flattened (the</span>
<span class="line-added">1997   // declared type is Object[], abstract[], interface[] or VT.ref[]).</span>
<span class="line-added">1998   // If this array is flattened, take the slow path.</span>
<span class="line-added">1999   Register tmp_load_klass = LP64_ONLY(rscratch1) NOT_LP64(noreg);</span>
<span class="line-added">2000   Register klass = op-&gt;tmp()-&gt;as_register();</span>
<span class="line-added">2001   __ load_klass(klass, op-&gt;array()-&gt;as_register(), tmp_load_klass);</span>
<span class="line-added">2002   __ movl(klass, Address(klass, Klass::layout_helper_offset()));</span>
<span class="line-added">2003   __ testl(klass, Klass::_lh_array_tag_vt_value_bit_inplace);</span>
<span class="line-added">2004   __ jcc(Assembler::notZero, *op-&gt;stub()-&gt;entry());</span>
<span class="line-added">2005   if (!op-&gt;value()-&gt;is_illegal()) {</span>
<span class="line-added">2006     // The array is not flattened, but it might be null-free. If we are storing</span>
<span class="line-added">2007     // a null into a null-free array, take the slow path (which will throw NPE).</span>
<span class="line-added">2008     Label skip;</span>
<span class="line-added">2009     __ cmpptr(op-&gt;value()-&gt;as_register(), (int32_t)NULL_WORD);</span>
<span class="line-added">2010     __ jcc(Assembler::notEqual, skip);</span>
<span class="line-added">2011     __ testl(klass, Klass::_lh_null_free_bit_inplace);</span>
<span class="line-added">2012     __ jcc(Assembler::notZero, *op-&gt;stub()-&gt;entry());</span>
<span class="line-added">2013     __ bind(skip);</span>
<span class="line-added">2014   }</span>
<span class="line-added">2015 }</span>
<span class="line-added">2016 </span>
<span class="line-added">2017 void LIR_Assembler::emit_opNullFreeArrayCheck(LIR_OpNullFreeArrayCheck* op) {</span>
<span class="line-added">2018   // We are storing into an array that *may* be null-free (the declared type is</span>
<span class="line-added">2019   // Object[], abstract[], interface[] or VT.ref[]).</span>
<span class="line-added">2020   Register tmp_load_klass = LP64_ONLY(rscratch1) NOT_LP64(noreg);</span>
<span class="line-added">2021   Register klass = op-&gt;tmp()-&gt;as_register();</span>
<span class="line-added">2022   __ load_klass(klass, op-&gt;array()-&gt;as_register(), tmp_load_klass);</span>
<span class="line-added">2023   __ movl(klass, Address(klass, Klass::layout_helper_offset()));</span>
<span class="line-added">2024   __ testl(klass, Klass::_lh_null_free_bit_inplace);</span>
<span class="line-added">2025 }</span>
<span class="line-added">2026 </span>
<span class="line-added">2027 void LIR_Assembler::emit_opSubstitutabilityCheck(LIR_OpSubstitutabilityCheck* op) {</span>
<span class="line-added">2028   Label L_oops_equal;</span>
<span class="line-added">2029   Label L_oops_not_equal;</span>
<span class="line-added">2030   Label L_end;</span>
<span class="line-added">2031 </span>
<span class="line-added">2032   Register left  = op-&gt;left()-&gt;as_register();</span>
<span class="line-added">2033   Register right = op-&gt;right()-&gt;as_register();</span>
<span class="line-added">2034 </span>
<span class="line-added">2035   __ cmpptr(left, right);</span>
<span class="line-added">2036   __ jcc(Assembler::equal, L_oops_equal);</span>
<span class="line-added">2037 </span>
<span class="line-added">2038   // (1) Null check -- if one of the operands is null, the other must not be null (because</span>
<span class="line-added">2039   //     the two references are not equal), so they are not substitutable,</span>
<span class="line-added">2040   //     FIXME: do null check only if the operand is nullable</span>
<span class="line-added">2041   {</span>
<span class="line-added">2042     __ cmpptr(left, (int32_t)NULL_WORD);</span>
<span class="line-added">2043     __ jcc(Assembler::equal, L_oops_not_equal);</span>
<span class="line-added">2044 </span>
<span class="line-added">2045     __ cmpptr(right, (int32_t)NULL_WORD);</span>
<span class="line-added">2046     __ jcc(Assembler::equal, L_oops_not_equal);</span>
<span class="line-added">2047   }</span>
<span class="line-added">2048 </span>
<span class="line-added">2049   ciKlass* left_klass = op-&gt;left_klass();</span>
<span class="line-added">2050   ciKlass* right_klass = op-&gt;right_klass();</span>
<span class="line-added">2051 </span>
<span class="line-added">2052   // (2) Value object check -- if either of the operands is not a value object,</span>
<span class="line-added">2053   //     they are not substitutable. We do this only if we are not sure that the</span>
<span class="line-added">2054   //     operands are value objects</span>
<span class="line-added">2055   if ((left_klass == NULL || right_klass == NULL) ||// The klass is still unloaded, or came from a Phi node.</span>
<span class="line-added">2056       !left_klass-&gt;is_valuetype() || !right_klass-&gt;is_valuetype()) {</span>
<span class="line-added">2057     Register tmp1  = op-&gt;tmp1()-&gt;as_register();</span>
<span class="line-added">2058     __ movptr(tmp1, (intptr_t)markWord::always_locked_pattern);</span>
<span class="line-added">2059     __ andl(tmp1, Address(left, oopDesc::mark_offset_in_bytes()));</span>
<span class="line-added">2060     __ andl(tmp1, Address(right, oopDesc::mark_offset_in_bytes()));</span>
<span class="line-added">2061     __ cmpptr(tmp1, (intptr_t)markWord::always_locked_pattern);</span>
<span class="line-added">2062     __ jcc(Assembler::notEqual, L_oops_not_equal);</span>
<span class="line-added">2063   }</span>
<span class="line-added">2064 </span>
<span class="line-added">2065   // (3) Same klass check: if the operands are of different klasses, they are not substitutable.</span>
<span class="line-added">2066   if (left_klass != NULL &amp;&amp; left_klass-&gt;is_valuetype() &amp;&amp; left_klass == right_klass) {</span>
<span class="line-added">2067     // No need to load klass -- the operands are statically known to be the same value klass.</span>
<span class="line-added">2068     __ jmp(*op-&gt;stub()-&gt;entry());</span>
<span class="line-added">2069   } else {</span>
<span class="line-added">2070     Register left_klass_op = op-&gt;left_klass_op()-&gt;as_register();</span>
<span class="line-added">2071     Register right_klass_op = op-&gt;right_klass_op()-&gt;as_register();</span>
<span class="line-added">2072 </span>
<span class="line-added">2073     if (UseCompressedOops) {</span>
<span class="line-added">2074       __ movl(left_klass_op,  Address(left,  oopDesc::klass_offset_in_bytes()));</span>
<span class="line-added">2075       __ movl(right_klass_op, Address(right, oopDesc::klass_offset_in_bytes()));</span>
<span class="line-added">2076       __ cmpl(left_klass_op, right_klass_op);</span>
<span class="line-added">2077     } else {</span>
<span class="line-added">2078       __ movptr(left_klass_op,  Address(left,  oopDesc::klass_offset_in_bytes()));</span>
<span class="line-added">2079       __ movptr(right_klass_op, Address(right, oopDesc::klass_offset_in_bytes()));</span>
<span class="line-added">2080       __ cmpptr(left_klass_op, right_klass_op);</span>
<span class="line-added">2081     }</span>
<span class="line-added">2082 </span>
<span class="line-added">2083     __ jcc(Assembler::equal, *op-&gt;stub()-&gt;entry()); // same klass -&gt; do slow check</span>
<span class="line-added">2084     // fall through to L_oops_not_equal</span>
<span class="line-added">2085   }</span>
<span class="line-added">2086 </span>
<span class="line-added">2087   __ bind(L_oops_not_equal);</span>
<span class="line-added">2088   move(op-&gt;not_equal_result(), op-&gt;result_opr());</span>
<span class="line-added">2089   __ jmp(L_end);</span>
<span class="line-added">2090 </span>
<span class="line-added">2091   __ bind(L_oops_equal);</span>
<span class="line-added">2092   move(op-&gt;equal_result(), op-&gt;result_opr());</span>
<span class="line-added">2093   __ jmp(L_end);</span>
<span class="line-added">2094 </span>
<span class="line-added">2095   // We&#39;ve returned from the stub. RAX contains 0x0 IFF the two</span>
<span class="line-added">2096   // operands are not substitutable. (Don&#39;t compare against 0x1 in case the</span>
<span class="line-added">2097   // C compiler is naughty)</span>
<span class="line-added">2098   __ bind(*op-&gt;stub()-&gt;continuation());</span>
<span class="line-added">2099   __ cmpl(rax, 0);</span>
<span class="line-added">2100   __ jcc(Assembler::equal, L_oops_not_equal); // (call_stub() == 0x0) -&gt; not_equal</span>
<span class="line-added">2101   move(op-&gt;equal_result(), op-&gt;result_opr()); // (call_stub() != 0x0) -&gt; equal</span>
<span class="line-added">2102   // fall-through</span>
<span class="line-added">2103   __ bind(L_end);</span>
<span class="line-added">2104 }</span>
2105 
2106 void LIR_Assembler::emit_compare_and_swap(LIR_OpCompareAndSwap* op) {
2107   if (LP64_ONLY(false &amp;&amp;) op-&gt;code() == lir_cas_long &amp;&amp; VM_Version::supports_cx8()) {
2108     assert(op-&gt;cmp_value()-&gt;as_register_lo() == rax, &quot;wrong register&quot;);
2109     assert(op-&gt;cmp_value()-&gt;as_register_hi() == rdx, &quot;wrong register&quot;);
2110     assert(op-&gt;new_value()-&gt;as_register_lo() == rbx, &quot;wrong register&quot;);
2111     assert(op-&gt;new_value()-&gt;as_register_hi() == rcx, &quot;wrong register&quot;);
2112     Register addr = op-&gt;addr()-&gt;as_register();
2113     __ lock();
2114     NOT_LP64(__ cmpxchg8(Address(addr, 0)));
2115 
2116   } else if (op-&gt;code() == lir_cas_int || op-&gt;code() == lir_cas_obj ) {
2117     NOT_LP64(assert(op-&gt;addr()-&gt;is_single_cpu(), &quot;must be single&quot;);)
2118     Register addr = (op-&gt;addr()-&gt;is_single_cpu() ? op-&gt;addr()-&gt;as_register() : op-&gt;addr()-&gt;as_register_lo());
2119     Register newval = op-&gt;new_value()-&gt;as_register();
2120     Register cmpval = op-&gt;cmp_value()-&gt;as_register();
2121     assert(cmpval == rax, &quot;wrong register&quot;);
2122     assert(newval != NULL, &quot;new val must be register&quot;);
2123     assert(cmpval != newval, &quot;cmp and new values must be in different registers&quot;);
2124     assert(cmpval != addr, &quot;cmp and addr must be in different registers&quot;);
</pre>
<hr />
<pre>
2145       __ cmpxchgl(newval, Address(addr, 0));
2146     }
2147 #ifdef _LP64
2148   } else if (op-&gt;code() == lir_cas_long) {
2149     Register addr = (op-&gt;addr()-&gt;is_single_cpu() ? op-&gt;addr()-&gt;as_register() : op-&gt;addr()-&gt;as_register_lo());
2150     Register newval = op-&gt;new_value()-&gt;as_register_lo();
2151     Register cmpval = op-&gt;cmp_value()-&gt;as_register_lo();
2152     assert(cmpval == rax, &quot;wrong register&quot;);
2153     assert(newval != NULL, &quot;new val must be register&quot;);
2154     assert(cmpval != newval, &quot;cmp and new values must be in different registers&quot;);
2155     assert(cmpval != addr, &quot;cmp and addr must be in different registers&quot;);
2156     assert(newval != addr, &quot;new value and addr must be in different registers&quot;);
2157     __ lock();
2158     __ cmpxchgq(newval, Address(addr, 0));
2159 #endif // _LP64
2160   } else {
2161     Unimplemented();
2162   }
2163 }
2164 
<span class="line-added">2165 void LIR_Assembler::move(LIR_Opr src, LIR_Opr dst) {</span>
<span class="line-added">2166   assert(dst-&gt;is_cpu_register(), &quot;must be&quot;);</span>
<span class="line-added">2167   assert(dst-&gt;type() == src-&gt;type(), &quot;must be&quot;);</span>
<span class="line-added">2168 </span>
<span class="line-added">2169   if (src-&gt;is_cpu_register()) {</span>
<span class="line-added">2170     reg2reg(src, dst);</span>
<span class="line-added">2171   } else if (src-&gt;is_stack()) {</span>
<span class="line-added">2172     stack2reg(src, dst, dst-&gt;type());</span>
<span class="line-added">2173   } else if (src-&gt;is_constant()) {</span>
<span class="line-added">2174     const2reg(src, dst, lir_patch_none, NULL);</span>
<span class="line-added">2175   } else {</span>
<span class="line-added">2176     ShouldNotReachHere();</span>
<span class="line-added">2177   }</span>
<span class="line-added">2178 }</span>
<span class="line-added">2179 </span>
2180 void LIR_Assembler::cmove(LIR_Condition condition, LIR_Opr opr1, LIR_Opr opr2, LIR_Opr result, BasicType type) {
2181   Assembler::Condition acond, ncond;
2182   switch (condition) {
2183     case lir_cond_equal:        acond = Assembler::equal;        ncond = Assembler::notEqual;     break;
2184     case lir_cond_notEqual:     acond = Assembler::notEqual;     ncond = Assembler::equal;        break;
2185     case lir_cond_less:         acond = Assembler::less;         ncond = Assembler::greaterEqual; break;
2186     case lir_cond_lessEqual:    acond = Assembler::lessEqual;    ncond = Assembler::greater;      break;
2187     case lir_cond_greaterEqual: acond = Assembler::greaterEqual; ncond = Assembler::less;         break;
2188     case lir_cond_greater:      acond = Assembler::greater;      ncond = Assembler::lessEqual;    break;
2189     case lir_cond_belowEqual:   acond = Assembler::belowEqual;   ncond = Assembler::above;        break;
2190     case lir_cond_aboveEqual:   acond = Assembler::aboveEqual;   ncond = Assembler::below;        break;
2191     default:                    acond = Assembler::equal;        ncond = Assembler::notEqual;
2192                                 ShouldNotReachHere();
2193   }
2194 
2195   if (opr1-&gt;is_cpu_register()) {
2196     reg2reg(opr1, result);
2197   } else if (opr1-&gt;is_stack()) {
2198     stack2reg(opr1, result, result-&gt;type());
2199   } else if (opr1-&gt;is_constant()) {
</pre>
<hr />
<pre>
3039   switch (code) {
3040   case lir_static_call:
3041   case lir_optvirtual_call:
3042   case lir_dynamic_call:
3043     offset += NativeCall::displacement_offset;
3044     break;
3045   case lir_icvirtual_call:
3046     offset += NativeCall::displacement_offset + NativeMovConstReg::instruction_size;
3047     break;
3048   case lir_virtual_call:  // currently, sparc-specific for niagara
3049   default: ShouldNotReachHere();
3050   }
3051   __ align(BytesPerWord, offset);
3052 }
3053 
3054 
3055 void LIR_Assembler::call(LIR_OpJavaCall* op, relocInfo::relocType rtype) {
3056   assert((__ offset() + NativeCall::displacement_offset) % BytesPerWord == 0,
3057          &quot;must be aligned&quot;);
3058   __ call(AddressLiteral(op-&gt;addr(), rtype));
<span class="line-modified">3059   add_call_info(code_offset(), op-&gt;info(), op-&gt;maybe_return_as_fields());</span>
3060 }
3061 
3062 
3063 void LIR_Assembler::ic_call(LIR_OpJavaCall* op) {
3064   __ ic_call(op-&gt;addr());
<span class="line-modified">3065   add_call_info(code_offset(), op-&gt;info(), op-&gt;maybe_return_as_fields());</span>
3066   assert((__ offset() - NativeCall::instruction_size + NativeCall::displacement_offset) % BytesPerWord == 0,
3067          &quot;must be aligned&quot;);
3068 }
3069 
3070 
3071 /* Currently, vtable-dispatch is only enabled for sparc platforms */
3072 void LIR_Assembler::vtable_call(LIR_OpJavaCall* op) {
3073   ShouldNotReachHere();
3074 }
3075 
3076 
3077 void LIR_Assembler::emit_static_call_stub() {
3078   address call_pc = __ pc();
3079   address stub = __ start_a_stub(call_stub_size());
3080   if (stub == NULL) {
3081     bailout(&quot;static call stub overflow&quot;);
3082     return;
3083   }
3084 
3085   int start = __ offset();
</pre>
<hr />
<pre>
3241   __ movptr (Address(rsp, offset_from_rsp_in_bytes), c);
3242 }
3243 
3244 
3245 void LIR_Assembler::store_parameter(jobject o,  int offset_from_rsp_in_words) {
3246   assert(offset_from_rsp_in_words &gt;= 0, &quot;invalid offset from rsp&quot;);
3247   int offset_from_rsp_in_bytes = offset_from_rsp_in_words * BytesPerWord;
3248   assert(offset_from_rsp_in_bytes &lt; frame_map()-&gt;reserved_argument_area_size(), &quot;invalid offset&quot;);
3249   __ movoop (Address(rsp, offset_from_rsp_in_bytes), o);
3250 }
3251 
3252 
3253 void LIR_Assembler::store_parameter(Metadata* m,  int offset_from_rsp_in_words) {
3254   assert(offset_from_rsp_in_words &gt;= 0, &quot;invalid offset from rsp&quot;);
3255   int offset_from_rsp_in_bytes = offset_from_rsp_in_words * BytesPerWord;
3256   assert(offset_from_rsp_in_bytes &lt; frame_map()-&gt;reserved_argument_area_size(), &quot;invalid offset&quot;);
3257   __ mov_metadata(Address(rsp, offset_from_rsp_in_bytes), m);
3258 }
3259 
3260 
<span class="line-added">3261 void LIR_Assembler::arraycopy_valuetype_check(Register obj, Register tmp, CodeStub* slow_path, bool is_dest, bool null_check) {</span>
<span class="line-added">3262   if (null_check) {</span>
<span class="line-added">3263     __ testptr(obj, obj);</span>
<span class="line-added">3264     __ jcc(Assembler::zero, *slow_path-&gt;entry());</span>
<span class="line-added">3265   }</span>
<span class="line-added">3266   Register tmp_load_klass = LP64_ONLY(rscratch1) NOT_LP64(noreg);</span>
<span class="line-added">3267   __ load_klass(tmp, obj, tmp_load_klass);</span>
<span class="line-added">3268   __ movl(tmp, Address(tmp, Klass::layout_helper_offset()));</span>
<span class="line-added">3269   if (is_dest) {</span>
<span class="line-added">3270     // We also take slow path if it&#39;s a null_free destination array, just in case the source array</span>
<span class="line-added">3271     // contains NULLs.</span>
<span class="line-added">3272     __ testl(tmp, Klass::_lh_null_free_bit_inplace);</span>
<span class="line-added">3273   } else {</span>
<span class="line-added">3274     __ testl(tmp, Klass::_lh_array_tag_vt_value_bit_inplace);</span>
<span class="line-added">3275   }</span>
<span class="line-added">3276   __ jcc(Assembler::notZero, *slow_path-&gt;entry());</span>
<span class="line-added">3277 }</span>
<span class="line-added">3278 </span>
<span class="line-added">3279 </span>
3280 // This code replaces a call to arraycopy; no exception may
3281 // be thrown in this code, they must be thrown in the System.arraycopy
3282 // activation frame; we could save some checks if this would not be the case
3283 void LIR_Assembler::emit_arraycopy(LIR_OpArrayCopy* op) {
3284   ciArrayKlass* default_type = op-&gt;expected_type();
3285   Register src = op-&gt;src()-&gt;as_register();
3286   Register dst = op-&gt;dst()-&gt;as_register();
3287   Register src_pos = op-&gt;src_pos()-&gt;as_register();
3288   Register dst_pos = op-&gt;dst_pos()-&gt;as_register();
3289   Register length  = op-&gt;length()-&gt;as_register();
3290   Register tmp = op-&gt;tmp()-&gt;as_register();
3291   Register tmp_load_klass = LP64_ONLY(rscratch1) NOT_LP64(noreg);
3292 
3293   __ resolve(ACCESS_READ, src);
3294   __ resolve(ACCESS_WRITE, dst);
3295 
3296   CodeStub* stub = op-&gt;stub();
3297   int flags = op-&gt;flags();
3298   BasicType basic_type = default_type != NULL ? default_type-&gt;element_type()-&gt;basic_type() : T_ILLEGAL;
3299   if (is_reference_type(basic_type)) basic_type = T_OBJECT;
3300 
<span class="line-added">3301   if (flags &amp; LIR_OpArrayCopy::always_slow_path) {</span>
<span class="line-added">3302     __ jmp(*stub-&gt;entry());</span>
<span class="line-added">3303     __ bind(*stub-&gt;continuation());</span>
<span class="line-added">3304     return;</span>
<span class="line-added">3305   }</span>
<span class="line-added">3306 </span>
<span class="line-added">3307   if (flags &amp; LIR_OpArrayCopy::src_valuetype_check) {</span>
<span class="line-added">3308     arraycopy_valuetype_check(src, tmp, stub, false, (flags &amp; LIR_OpArrayCopy::src_null_check));</span>
<span class="line-added">3309   }</span>
<span class="line-added">3310 </span>
<span class="line-added">3311   if (flags &amp; LIR_OpArrayCopy::dst_valuetype_check) {</span>
<span class="line-added">3312     arraycopy_valuetype_check(dst, tmp, stub, true, (flags &amp; LIR_OpArrayCopy::dst_null_check));</span>
<span class="line-added">3313   }</span>
<span class="line-added">3314 </span>
3315   // if we don&#39;t know anything, just go through the generic arraycopy
3316   if (default_type == NULL) {
3317     // save outgoing arguments on stack in case call to System.arraycopy is needed
3318     // HACK ALERT. This code used to push the parameters in a hardwired fashion
3319     // for interpreter calling conventions. Now we have to do it in new style conventions.
3320     // For the moment until C1 gets the new register allocator I just force all the
3321     // args to the right place (except the register args) and then on the back side
3322     // reload the register args properly if we go slow path. Yuck
3323 
3324     // These are proper for the calling convention
3325     store_parameter(length, 2);
3326     store_parameter(dst_pos, 1);
3327     store_parameter(dst, 0);
3328 
3329     // these are just temporary placements until we need to reload
3330     store_parameter(src_pos, 3);
3331     store_parameter(src, 4);
3332     NOT_LP64(assert(src == rcx &amp;&amp; src_pos == rdx, &quot;mismatch in calling convention&quot;);)
3333 
3334     address copyfunc_addr = StubRoutines::generic_arraycopy();
</pre>
<hr />
<pre>
4232 }
4233 
4234 void LIR_Assembler::membar_storeload() {
4235   __ membar(Assembler::Membar_mask_bits(Assembler::StoreLoad));
4236 }
4237 
4238 void LIR_Assembler::on_spin_wait() {
4239   __ pause ();
4240 }
4241 
4242 void LIR_Assembler::get_thread(LIR_Opr result_reg) {
4243   assert(result_reg-&gt;is_register(), &quot;check&quot;);
4244 #ifdef _LP64
4245   // __ get_thread(result_reg-&gt;as_register_lo());
4246   __ mov(result_reg-&gt;as_register(), r15_thread);
4247 #else
4248   __ get_thread(result_reg-&gt;as_register());
4249 #endif // _LP64
4250 }
4251 
<span class="line-added">4252 void LIR_Assembler::check_orig_pc() {</span>
<span class="line-added">4253   __ cmpptr(frame_map()-&gt;address_for_orig_pc_addr(), (int32_t)NULL_WORD);</span>
<span class="line-added">4254 }</span>
4255 
4256 void LIR_Assembler::peephole(LIR_List*) {
4257   // do nothing for now
4258 }
4259 
4260 void LIR_Assembler::atomic_op(LIR_Code code, LIR_Opr src, LIR_Opr data, LIR_Opr dest, LIR_Opr tmp) {
4261   assert(data == dest, &quot;xchg/xadd uses only 2 operands&quot;);
4262 
4263   if (data-&gt;type() == T_INT) {
4264     if (code == lir_xadd) {
4265       __ lock();
4266       __ xaddl(as_Address(src-&gt;as_address_ptr()), data-&gt;as_register());
4267     } else {
4268       __ xchgl(data-&gt;as_register(), as_Address(src-&gt;as_address_ptr()));
4269     }
4270   } else if (data-&gt;is_oop()) {
4271     assert (code == lir_xchg, &quot;xadd for oops&quot;);
4272     Register obj = data-&gt;as_register();
4273 #ifdef _LP64
4274     if (UseCompressedOops) {
</pre>
</td>
</tr>
</table>
<center><a href="../aarch64/aarch64.ad.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="c1_MacroAssembler_x86.cpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>