<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/cpu/x86/c1_MacroAssembler_x86.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="c1_LIRAssembler_x86.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="c1_Runtime1_x86.cpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/cpu/x86/c1_MacroAssembler_x86.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
  1 /*
<span class="line-modified">  2  * Copyright (c) 1999, 2018, Oracle and/or its affiliates. All rights reserved.</span>
  3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  4  *
  5  * This code is free software; you can redistribute it and/or modify it
  6  * under the terms of the GNU General Public License version 2 only, as
  7  * published by the Free Software Foundation.
  8  *
  9  * This code is distributed in the hope that it will be useful, but WITHOUT
 10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
 11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
 12  * version 2 for more details (a copy is included in the LICENSE file that
 13  * accompanied this code).
 14  *
 15  * You should have received a copy of the GNU General Public License version
 16  * 2 along with this work; if not, write to the Free Software Foundation,
 17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 18  *
 19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
 20  * or visit www.oracle.com if you need additional information or have any
 21  * questions.
 22  *
 23  */
 24 
 25 #include &quot;precompiled.hpp&quot;
 26 #include &quot;c1/c1_MacroAssembler.hpp&quot;
 27 #include &quot;c1/c1_Runtime1.hpp&quot;
 28 #include &quot;classfile/systemDictionary.hpp&quot;
 29 #include &quot;gc/shared/barrierSet.hpp&quot;
 30 #include &quot;gc/shared/barrierSetAssembler.hpp&quot;
 31 #include &quot;gc/shared/collectedHeap.hpp&quot;
 32 #include &quot;interpreter/interpreter.hpp&quot;
 33 #include &quot;oops/arrayOop.hpp&quot;
 34 #include &quot;oops/markWord.hpp&quot;
 35 #include &quot;runtime/basicLock.hpp&quot;
 36 #include &quot;runtime/biasedLocking.hpp&quot;

 37 #include &quot;runtime/os.hpp&quot;
 38 #include &quot;runtime/sharedRuntime.hpp&quot;
 39 #include &quot;runtime/stubRoutines.hpp&quot;
 40 
 41 int C1_MacroAssembler::lock_object(Register hdr, Register obj, Register disp_hdr, Register scratch, Label&amp; slow_case) {
 42   const int aligned_mask = BytesPerWord -1;
 43   const int hdr_offset = oopDesc::mark_offset_in_bytes();
 44   assert(hdr == rax, &quot;hdr must be rax, for the cmpxchg instruction&quot;);
 45   assert(hdr != obj &amp;&amp; hdr != disp_hdr &amp;&amp; obj != disp_hdr, &quot;registers must be different&quot;);
 46   Label done;
 47   int null_check_offset = -1;
 48 
 49   verify_oop(obj);
 50 
 51   // save object being locked into the BasicObjectLock
 52   movptr(Address(disp_hdr, BasicObjectLock::obj_offset_in_bytes()), obj);
 53 
 54   if (UseBiasedLocking) {
 55     assert(scratch != noreg, &quot;should have scratch register at this point&quot;);
 56     Register rklass_decode_tmp = LP64_ONLY(rscratch1) NOT_LP64(noreg);
 57     null_check_offset = biased_locking_enter(disp_hdr, obj, hdr, scratch, rklass_decode_tmp, false, done, &amp;slow_case);
 58   } else {
 59     null_check_offset = offset();
 60   }
 61 
 62   // Load object header
 63   movptr(hdr, Address(obj, hdr_offset));
 64   // and mark it as unlocked
 65   orptr(hdr, markWord::unlocked_value);




 66   // save unlocked object header into the displaced header location on the stack
 67   movptr(Address(disp_hdr, 0), hdr);
 68   // test if object header is still the same (i.e. unlocked), and if so, store the
 69   // displaced header address in the object header - if it is not the same, get the
 70   // object header instead
 71   MacroAssembler::lock(); // must be immediately before cmpxchg!
 72   cmpxchgptr(disp_hdr, Address(obj, hdr_offset));
 73   // if the object header was the same, we&#39;re done
 74   if (PrintBiasedLockingStatistics) {
 75     cond_inc32(Assembler::equal,
 76                ExternalAddress((address)BiasedLocking::fast_path_entry_count_addr()));
 77   }
 78   jcc(Assembler::equal, done);
 79   // if the object header was not the same, it is now in the hdr register
 80   // =&gt; test if it is a stack pointer into the same stack (recursive locking), i.e.:
 81   //
 82   // 1) (hdr &amp; aligned_mask) == 0
 83   // 2) rsp &lt;= hdr
 84   // 3) hdr &lt;= rsp + page_size
 85   //
</pre>
<hr />
<pre>
135   // we do unlocking via runtime call
136   jcc(Assembler::notEqual, slow_case);
137   // done
138   bind(done);
139 }
140 
141 
142 // Defines obj, preserves var_size_in_bytes
143 void C1_MacroAssembler::try_allocate(Register obj, Register var_size_in_bytes, int con_size_in_bytes, Register t1, Register t2, Label&amp; slow_case) {
144   if (UseTLAB) {
145     tlab_allocate(noreg, obj, var_size_in_bytes, con_size_in_bytes, t1, t2, slow_case);
146   } else {
147     eden_allocate(noreg, obj, var_size_in_bytes, con_size_in_bytes, t1, slow_case);
148   }
149 }
150 
151 
152 void C1_MacroAssembler::initialize_header(Register obj, Register klass, Register len, Register t1, Register t2) {
153   assert_different_registers(obj, klass, len);
154   Register tmp_encode_klass = LP64_ONLY(rscratch1) NOT_LP64(noreg);
<span class="line-modified">155   if (UseBiasedLocking &amp;&amp; !len-&gt;is_valid()) {</span>

156     assert_different_registers(obj, klass, len, t1, t2);
157     movptr(t1, Address(klass, Klass::prototype_header_offset()));
158     movptr(Address(obj, oopDesc::mark_offset_in_bytes()), t1);
159   } else {
160     // This assumes that all prototype bits fit in an int32_t
161     movptr(Address(obj, oopDesc::mark_offset_in_bytes ()), (int32_t)(intptr_t)markWord::prototype().value());
162   }
163 #ifdef _LP64
164   if (UseCompressedClassPointers) { // Take care not to kill klass
165     movptr(t1, klass);
166     encode_klass_not_null(t1, tmp_encode_klass);
167     movl(Address(obj, oopDesc::klass_offset_in_bytes()), t1);
168   } else
169 #endif
170   {
171     movptr(Address(obj, oopDesc::klass_offset_in_bytes()), klass);
172   }
173 
174   if (len-&gt;is_valid()) {
175     movl(Address(obj, arrayOopDesc::length_offset_in_bytes()), len);
</pre>
<hr />
<pre>
297   // explicit NULL check not needed since load from [klass_offset] causes a trap
298   // check against inline cache
299   assert(!MacroAssembler::needs_explicit_null_check(oopDesc::klass_offset_in_bytes()), &quot;must add explicit null check&quot;);
300   int start_offset = offset();
301   Register tmp_load_klass = LP64_ONLY(rscratch2) NOT_LP64(noreg);
302 
303   if (UseCompressedClassPointers) {
304     load_klass(rscratch1, receiver, tmp_load_klass);
305     cmpptr(rscratch1, iCache);
306   } else {
307     cmpptr(iCache, Address(receiver, oopDesc::klass_offset_in_bytes()));
308   }
309   // if icache check fails, then jump to runtime routine
310   // Note: RECEIVER must still contain the receiver!
311   jump_cc(Assembler::notEqual,
312           RuntimeAddress(SharedRuntime::get_ic_miss_stub()));
313   const int ic_cmp_size = LP64_ONLY(10) NOT_LP64(9);
314   assert(UseCompressedClassPointers || offset() - start_offset == ic_cmp_size, &quot;check alignment in emit_method_entry&quot;);
315 }
316 




















317 
<span class="line-modified">318 void C1_MacroAssembler::build_frame(int frame_size_in_bytes, int bang_size_in_bytes) {</span>
<span class="line-modified">319   assert(bang_size_in_bytes &gt;= frame_size_in_bytes, &quot;stack bang size incorrect&quot;);</span>






320   // Make sure there is enough stack space for this method&#39;s activation.
321   // Note that we do this before doing an enter(). This matches the
322   // ordering of C2&#39;s stack overflow check / rsp decrement and allows
323   // the SharedRuntime stack overflow handling to be consistent
324   // between the two compilers.

325   generate_stack_overflow_check(bang_size_in_bytes);
326 
<span class="line-modified">327   push(rbp);</span>
<span class="line-removed">328   if (PreserveFramePointer) {</span>
<span class="line-removed">329     mov(rbp, rsp);</span>
<span class="line-removed">330   }</span>
<span class="line-removed">331 #if !defined(_LP64) &amp;&amp; defined(TIERED)</span>
<span class="line-removed">332   if (UseSSE &lt; 2 ) {</span>
<span class="line-removed">333     // c2 leaves fpu stack dirty. Clean it on entry</span>
<span class="line-removed">334     empty_FPU_stack();</span>
<span class="line-removed">335   }</span>
<span class="line-removed">336 #endif // !_LP64 &amp;&amp; TIERED</span>
<span class="line-removed">337   decrement(rsp, frame_size_in_bytes); // does not emit code for frame_size == 0</span>
338 





339   BarrierSetAssembler* bs = BarrierSet::barrier_set()-&gt;barrier_set_assembler();
340   bs-&gt;nmethod_entry_barrier(this);
341 }
342 
<span class="line-removed">343 </span>
<span class="line-removed">344 void C1_MacroAssembler::remove_frame(int frame_size_in_bytes) {</span>
<span class="line-removed">345   increment(rsp, frame_size_in_bytes);  // Does not emit code for frame_size == 0</span>
<span class="line-removed">346   pop(rbp);</span>
<span class="line-removed">347 }</span>
<span class="line-removed">348 </span>
<span class="line-removed">349 </span>
350 void C1_MacroAssembler::verified_entry() {
351   if (C1Breakpoint || VerifyFPU || !UseStackBanging) {
352     // Verified Entry first instruction should be 5 bytes long for correct
353     // patching by patch_verified_entry().
354     //
355     // C1Breakpoint and VerifyFPU have one byte first instruction.
356     // Also first instruction will be one byte &quot;push(rbp)&quot; if stack banging
357     // code is not generated (see build_frame() above).
358     // For all these cases generate long instruction first.
359     fat_nop();
360   }
361   if (C1Breakpoint)int3();
362   // build frame
363   IA32_ONLY( verify_FPU(0, &quot;method_entry&quot;); )
364 }
365 































































366 void C1_MacroAssembler::load_parameter(int offset_in_words, Register reg) {
367   // rbp, + 0: link
368   //     + 1: return address
369   //     + 2: argument with offset 0
370   //     + 3: argument with offset 1
371   //     + 4: ...
372 
373   movptr(reg, Address(rbp, (offset_in_words + 2) * BytesPerWord));
374 }
375 
376 #ifndef PRODUCT
377 
378 void C1_MacroAssembler::verify_stack_oop(int stack_offset) {
379   if (!VerifyOops) return;
380   verify_oop_addr(Address(rsp, stack_offset));
381 }
382 
383 void C1_MacroAssembler::verify_not_null_oop(Register r) {
384   if (!VerifyOops) return;
385   Label not_null;
</pre>
</td>
<td>
<hr />
<pre>
  1 /*
<span class="line-modified">  2  * Copyright (c) 1999, 2020, Oracle and/or its affiliates. All rights reserved.</span>
  3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  4  *
  5  * This code is free software; you can redistribute it and/or modify it
  6  * under the terms of the GNU General Public License version 2 only, as
  7  * published by the Free Software Foundation.
  8  *
  9  * This code is distributed in the hope that it will be useful, but WITHOUT
 10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
 11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
 12  * version 2 for more details (a copy is included in the LICENSE file that
 13  * accompanied this code).
 14  *
 15  * You should have received a copy of the GNU General Public License version
 16  * 2 along with this work; if not, write to the Free Software Foundation,
 17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 18  *
 19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
 20  * or visit www.oracle.com if you need additional information or have any
 21  * questions.
 22  *
 23  */
 24 
 25 #include &quot;precompiled.hpp&quot;
 26 #include &quot;c1/c1_MacroAssembler.hpp&quot;
 27 #include &quot;c1/c1_Runtime1.hpp&quot;
 28 #include &quot;classfile/systemDictionary.hpp&quot;
 29 #include &quot;gc/shared/barrierSet.hpp&quot;
 30 #include &quot;gc/shared/barrierSetAssembler.hpp&quot;
 31 #include &quot;gc/shared/collectedHeap.hpp&quot;
 32 #include &quot;interpreter/interpreter.hpp&quot;
 33 #include &quot;oops/arrayOop.hpp&quot;
 34 #include &quot;oops/markWord.hpp&quot;
 35 #include &quot;runtime/basicLock.hpp&quot;
 36 #include &quot;runtime/biasedLocking.hpp&quot;
<span class="line-added"> 37 #include &quot;runtime/frame.inline.hpp&quot;</span>
 38 #include &quot;runtime/os.hpp&quot;
 39 #include &quot;runtime/sharedRuntime.hpp&quot;
 40 #include &quot;runtime/stubRoutines.hpp&quot;
 41 
 42 int C1_MacroAssembler::lock_object(Register hdr, Register obj, Register disp_hdr, Register scratch, Label&amp; slow_case) {
 43   const int aligned_mask = BytesPerWord -1;
 44   const int hdr_offset = oopDesc::mark_offset_in_bytes();
 45   assert(hdr == rax, &quot;hdr must be rax, for the cmpxchg instruction&quot;);
 46   assert(hdr != obj &amp;&amp; hdr != disp_hdr &amp;&amp; obj != disp_hdr, &quot;registers must be different&quot;);
 47   Label done;
 48   int null_check_offset = -1;
 49 
 50   verify_oop(obj);
 51 
 52   // save object being locked into the BasicObjectLock
 53   movptr(Address(disp_hdr, BasicObjectLock::obj_offset_in_bytes()), obj);
 54 
 55   if (UseBiasedLocking) {
 56     assert(scratch != noreg, &quot;should have scratch register at this point&quot;);
 57     Register rklass_decode_tmp = LP64_ONLY(rscratch1) NOT_LP64(noreg);
 58     null_check_offset = biased_locking_enter(disp_hdr, obj, hdr, scratch, rklass_decode_tmp, false, done, &amp;slow_case);
 59   } else {
 60     null_check_offset = offset();
 61   }
 62 
 63   // Load object header
 64   movptr(hdr, Address(obj, hdr_offset));
 65   // and mark it as unlocked
 66   orptr(hdr, markWord::unlocked_value);
<span class="line-added"> 67   if (EnableValhalla &amp;&amp; !UseBiasedLocking) {</span>
<span class="line-added"> 68     // Mask always_locked bit such that we go to the slow path if object is a value type</span>
<span class="line-added"> 69     andptr(hdr, ~((int) markWord::biased_lock_bit_in_place));</span>
<span class="line-added"> 70   }</span>
 71   // save unlocked object header into the displaced header location on the stack
 72   movptr(Address(disp_hdr, 0), hdr);
 73   // test if object header is still the same (i.e. unlocked), and if so, store the
 74   // displaced header address in the object header - if it is not the same, get the
 75   // object header instead
 76   MacroAssembler::lock(); // must be immediately before cmpxchg!
 77   cmpxchgptr(disp_hdr, Address(obj, hdr_offset));
 78   // if the object header was the same, we&#39;re done
 79   if (PrintBiasedLockingStatistics) {
 80     cond_inc32(Assembler::equal,
 81                ExternalAddress((address)BiasedLocking::fast_path_entry_count_addr()));
 82   }
 83   jcc(Assembler::equal, done);
 84   // if the object header was not the same, it is now in the hdr register
 85   // =&gt; test if it is a stack pointer into the same stack (recursive locking), i.e.:
 86   //
 87   // 1) (hdr &amp; aligned_mask) == 0
 88   // 2) rsp &lt;= hdr
 89   // 3) hdr &lt;= rsp + page_size
 90   //
</pre>
<hr />
<pre>
140   // we do unlocking via runtime call
141   jcc(Assembler::notEqual, slow_case);
142   // done
143   bind(done);
144 }
145 
146 
147 // Defines obj, preserves var_size_in_bytes
148 void C1_MacroAssembler::try_allocate(Register obj, Register var_size_in_bytes, int con_size_in_bytes, Register t1, Register t2, Label&amp; slow_case) {
149   if (UseTLAB) {
150     tlab_allocate(noreg, obj, var_size_in_bytes, con_size_in_bytes, t1, t2, slow_case);
151   } else {
152     eden_allocate(noreg, obj, var_size_in_bytes, con_size_in_bytes, t1, slow_case);
153   }
154 }
155 
156 
157 void C1_MacroAssembler::initialize_header(Register obj, Register klass, Register len, Register t1, Register t2) {
158   assert_different_registers(obj, klass, len);
159   Register tmp_encode_klass = LP64_ONLY(rscratch1) NOT_LP64(noreg);
<span class="line-modified">160   if ((UseBiasedLocking || EnableValhalla) &amp;&amp; !len-&gt;is_valid()) {</span>
<span class="line-added">161     // Need to copy markWord::always_locked_pattern for values.</span>
162     assert_different_registers(obj, klass, len, t1, t2);
163     movptr(t1, Address(klass, Klass::prototype_header_offset()));
164     movptr(Address(obj, oopDesc::mark_offset_in_bytes()), t1);
165   } else {
166     // This assumes that all prototype bits fit in an int32_t
167     movptr(Address(obj, oopDesc::mark_offset_in_bytes ()), (int32_t)(intptr_t)markWord::prototype().value());
168   }
169 #ifdef _LP64
170   if (UseCompressedClassPointers) { // Take care not to kill klass
171     movptr(t1, klass);
172     encode_klass_not_null(t1, tmp_encode_klass);
173     movl(Address(obj, oopDesc::klass_offset_in_bytes()), t1);
174   } else
175 #endif
176   {
177     movptr(Address(obj, oopDesc::klass_offset_in_bytes()), klass);
178   }
179 
180   if (len-&gt;is_valid()) {
181     movl(Address(obj, arrayOopDesc::length_offset_in_bytes()), len);
</pre>
<hr />
<pre>
303   // explicit NULL check not needed since load from [klass_offset] causes a trap
304   // check against inline cache
305   assert(!MacroAssembler::needs_explicit_null_check(oopDesc::klass_offset_in_bytes()), &quot;must add explicit null check&quot;);
306   int start_offset = offset();
307   Register tmp_load_klass = LP64_ONLY(rscratch2) NOT_LP64(noreg);
308 
309   if (UseCompressedClassPointers) {
310     load_klass(rscratch1, receiver, tmp_load_klass);
311     cmpptr(rscratch1, iCache);
312   } else {
313     cmpptr(iCache, Address(receiver, oopDesc::klass_offset_in_bytes()));
314   }
315   // if icache check fails, then jump to runtime routine
316   // Note: RECEIVER must still contain the receiver!
317   jump_cc(Assembler::notEqual,
318           RuntimeAddress(SharedRuntime::get_ic_miss_stub()));
319   const int ic_cmp_size = LP64_ONLY(10) NOT_LP64(9);
320   assert(UseCompressedClassPointers || offset() - start_offset == ic_cmp_size, &quot;check alignment in emit_method_entry&quot;);
321 }
322 
<span class="line-added">323 void C1_MacroAssembler::build_frame_helper(int frame_size_in_bytes, int sp_inc, bool needs_stack_repair) {</span>
<span class="line-added">324   push(rbp);</span>
<span class="line-added">325   if (PreserveFramePointer) {</span>
<span class="line-added">326     mov(rbp, rsp);</span>
<span class="line-added">327   }</span>
<span class="line-added">328   #if !defined(_LP64) &amp;&amp; defined(TIERED)</span>
<span class="line-added">329     if (UseSSE &lt; 2 ) {</span>
<span class="line-added">330       // c2 leaves fpu stack dirty. Clean it on entry</span>
<span class="line-added">331       empty_FPU_stack();</span>
<span class="line-added">332     }</span>
<span class="line-added">333   #endif // !_LP64 &amp;&amp; TIERED</span>
<span class="line-added">334   decrement(rsp, frame_size_in_bytes);</span>
<span class="line-added">335 </span>
<span class="line-added">336   if (needs_stack_repair) {</span>
<span class="line-added">337     // Save stack increment (also account for fixed framesize and rbp)</span>
<span class="line-added">338     assert((sp_inc &amp; (StackAlignmentInBytes-1)) == 0, &quot;stack increment not aligned&quot;);</span>
<span class="line-added">339     int real_frame_size = sp_inc + frame_size_in_bytes + wordSize;</span>
<span class="line-added">340     movptr(Address(rsp, frame_size_in_bytes - wordSize), real_frame_size);</span>
<span class="line-added">341   }</span>
<span class="line-added">342 }</span>
343 
<span class="line-modified">344 void C1_MacroAssembler::build_frame(int frame_size_in_bytes, int bang_size_in_bytes, int sp_offset_for_orig_pc, bool needs_stack_repair, bool has_scalarized_args, Label* verified_value_entry_label) {</span>
<span class="line-modified">345   if (has_scalarized_args) {</span>
<span class="line-added">346     // Initialize orig_pc to detect deoptimization during buffering in the entry points</span>
<span class="line-added">347     movptr(Address(rsp, sp_offset_for_orig_pc - frame_size_in_bytes - wordSize), 0);</span>
<span class="line-added">348   }</span>
<span class="line-added">349   if (!needs_stack_repair &amp;&amp; verified_value_entry_label != NULL) {</span>
<span class="line-added">350     bind(*verified_value_entry_label);</span>
<span class="line-added">351   }</span>
352   // Make sure there is enough stack space for this method&#39;s activation.
353   // Note that we do this before doing an enter(). This matches the
354   // ordering of C2&#39;s stack overflow check / rsp decrement and allows
355   // the SharedRuntime stack overflow handling to be consistent
356   // between the two compilers.
<span class="line-added">357   assert(bang_size_in_bytes &gt;= frame_size_in_bytes, &quot;stack bang size incorrect&quot;);</span>
358   generate_stack_overflow_check(bang_size_in_bytes);
359 
<span class="line-modified">360   build_frame_helper(frame_size_in_bytes, 0, needs_stack_repair);</span>










361 
<span class="line-added">362   if (needs_stack_repair &amp;&amp; verified_value_entry_label != NULL) {</span>
<span class="line-added">363     // Jump here from the scalarized entry points that require additional stack space</span>
<span class="line-added">364     // for packing scalarized arguments and therefore already created the frame.</span>
<span class="line-added">365     bind(*verified_value_entry_label);</span>
<span class="line-added">366   }</span>
367   BarrierSetAssembler* bs = BarrierSet::barrier_set()-&gt;barrier_set_assembler();
368   bs-&gt;nmethod_entry_barrier(this);
369 }
370 







371 void C1_MacroAssembler::verified_entry() {
372   if (C1Breakpoint || VerifyFPU || !UseStackBanging) {
373     // Verified Entry first instruction should be 5 bytes long for correct
374     // patching by patch_verified_entry().
375     //
376     // C1Breakpoint and VerifyFPU have one byte first instruction.
377     // Also first instruction will be one byte &quot;push(rbp)&quot; if stack banging
378     // code is not generated (see build_frame() above).
379     // For all these cases generate long instruction first.
380     fat_nop();
381   }
382   if (C1Breakpoint)int3();
383   // build frame
384   IA32_ONLY( verify_FPU(0, &quot;method_entry&quot;); )
385 }
386 
<span class="line-added">387 int C1_MacroAssembler::scalarized_entry(const CompiledEntrySignature *ces, int frame_size_in_bytes, int bang_size_in_bytes, int sp_offset_for_orig_pc, Label&amp; verified_value_entry_label, bool is_value_ro_entry) {</span>
<span class="line-added">388   assert(InlineTypePassFieldsAsArgs, &quot;sanity&quot;);</span>
<span class="line-added">389   // Make sure there is enough stack space for this method&#39;s activation.</span>
<span class="line-added">390   assert(bang_size_in_bytes &gt;= frame_size_in_bytes, &quot;stack bang size incorrect&quot;);</span>
<span class="line-added">391   generate_stack_overflow_check(bang_size_in_bytes);</span>
<span class="line-added">392 </span>
<span class="line-added">393   GrowableArray&lt;SigEntry&gt;* sig    = &amp;ces-&gt;sig();</span>
<span class="line-added">394   GrowableArray&lt;SigEntry&gt;* sig_cc = is_value_ro_entry ? &amp;ces-&gt;sig_cc_ro() : &amp;ces-&gt;sig_cc();</span>
<span class="line-added">395   VMRegPair* regs      = ces-&gt;regs();</span>
<span class="line-added">396   VMRegPair* regs_cc   = is_value_ro_entry ? ces-&gt;regs_cc_ro() : ces-&gt;regs_cc();</span>
<span class="line-added">397   int args_on_stack    = ces-&gt;args_on_stack();</span>
<span class="line-added">398   int args_on_stack_cc = is_value_ro_entry ? ces-&gt;args_on_stack_cc_ro() : ces-&gt;args_on_stack_cc();</span>
<span class="line-added">399 </span>
<span class="line-added">400   assert(sig-&gt;length() &lt;= sig_cc-&gt;length(), &quot;Zero-sized value class not allowed!&quot;);</span>
<span class="line-added">401   BasicType* sig_bt = NEW_RESOURCE_ARRAY(BasicType, sig_cc-&gt;length());</span>
<span class="line-added">402   int args_passed = sig-&gt;length();</span>
<span class="line-added">403   int args_passed_cc = SigEntry::fill_sig_bt(sig_cc, sig_bt);</span>
<span class="line-added">404   int extra_stack_offset = wordSize; // tos is return address.</span>
<span class="line-added">405 </span>
<span class="line-added">406   // Check if we need to extend the stack for packing</span>
<span class="line-added">407   int sp_inc = 0;</span>
<span class="line-added">408   if (args_on_stack &gt; args_on_stack_cc) {</span>
<span class="line-added">409     // Two additional slots to account for return address</span>
<span class="line-added">410     sp_inc = (args_on_stack + 2) * VMRegImpl::stack_slot_size;</span>
<span class="line-added">411     sp_inc = align_up(sp_inc, StackAlignmentInBytes);</span>
<span class="line-added">412     pop(r13); // Copy return address</span>
<span class="line-added">413     subptr(rsp, sp_inc);</span>
<span class="line-added">414     push(r13);</span>
<span class="line-added">415   }</span>
<span class="line-added">416 </span>
<span class="line-added">417   // Create a temp frame so we can call into the runtime. It must be properly set up to accommodate GC.</span>
<span class="line-added">418   build_frame_helper(frame_size_in_bytes, sp_inc, ces-&gt;c1_needs_stack_repair());</span>
<span class="line-added">419 </span>
<span class="line-added">420   // Initialize orig_pc to detect deoptimization during buffering in below runtime call</span>
<span class="line-added">421   movptr(Address(rsp, sp_offset_for_orig_pc), 0);</span>
<span class="line-added">422 </span>
<span class="line-added">423   // FIXME -- call runtime only if we cannot in-line allocate all the incoming value args.</span>
<span class="line-added">424   movptr(rbx, (intptr_t)(ces-&gt;method()));</span>
<span class="line-added">425   if (is_value_ro_entry) {</span>
<span class="line-added">426     call(RuntimeAddress(Runtime1::entry_for(Runtime1::buffer_value_args_no_receiver_id)));</span>
<span class="line-added">427   } else {</span>
<span class="line-added">428     call(RuntimeAddress(Runtime1::entry_for(Runtime1::buffer_value_args_id)));</span>
<span class="line-added">429   }</span>
<span class="line-added">430   int rt_call_offset = offset();</span>
<span class="line-added">431 </span>
<span class="line-added">432   // Remove the temp frame</span>
<span class="line-added">433   addptr(rsp, frame_size_in_bytes);</span>
<span class="line-added">434   pop(rbp);</span>
<span class="line-added">435 </span>
<span class="line-added">436   shuffle_value_args(true, is_value_ro_entry, extra_stack_offset, sig_bt, sig_cc,</span>
<span class="line-added">437                      args_passed_cc, args_on_stack_cc, regs_cc, // from</span>
<span class="line-added">438                      args_passed, args_on_stack, regs, sp_inc); // to</span>
<span class="line-added">439 </span>
<span class="line-added">440   if (ces-&gt;c1_needs_stack_repair()) {</span>
<span class="line-added">441     // Create the real frame. Below jump will then skip over the stack banging and frame</span>
<span class="line-added">442     // setup code in the verified_value_entry (which has a different real_frame_size).</span>
<span class="line-added">443     build_frame_helper(frame_size_in_bytes, sp_inc, true);</span>
<span class="line-added">444   }</span>
<span class="line-added">445 </span>
<span class="line-added">446   jmp(verified_value_entry_label);</span>
<span class="line-added">447   return rt_call_offset;</span>
<span class="line-added">448 }</span>
<span class="line-added">449 </span>
450 void C1_MacroAssembler::load_parameter(int offset_in_words, Register reg) {
451   // rbp, + 0: link
452   //     + 1: return address
453   //     + 2: argument with offset 0
454   //     + 3: argument with offset 1
455   //     + 4: ...
456 
457   movptr(reg, Address(rbp, (offset_in_words + 2) * BytesPerWord));
458 }
459 
460 #ifndef PRODUCT
461 
462 void C1_MacroAssembler::verify_stack_oop(int stack_offset) {
463   if (!VerifyOops) return;
464   verify_oop_addr(Address(rsp, stack_offset));
465 }
466 
467 void C1_MacroAssembler::verify_not_null_oop(Register r) {
468   if (!VerifyOops) return;
469   Label not_null;
</pre>
</td>
</tr>
</table>
<center><a href="c1_LIRAssembler_x86.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="c1_Runtime1_x86.cpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>