<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Frames src/hotspot/cpu/x86/c2_MacroAssembler_x86.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
    <script type="text/javascript" src="../../../../navigation.js"></script>
  </head>
<body onkeypress="keypress(event);">
<a name="0"></a>
<hr />
<pre>   1 /*
   2  * Copyright (c) 2020, Oracle and/or its affiliates. All rights reserved.
   3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   4  *
   5  * This code is free software; you can redistribute it and/or modify it
   6  * under the terms of the GNU General Public License version 2 only, as
   7  * published by the Free Software Foundation.
   8  *
   9  * This code is distributed in the hope that it will be useful, but WITHOUT
  10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #include &quot;precompiled.hpp&quot;
  26 #include &quot;asm/assembler.hpp&quot;
  27 #include &quot;asm/assembler.inline.hpp&quot;
  28 #include &quot;oops/methodData.hpp&quot;
  29 #include &quot;opto/c2_MacroAssembler.hpp&quot;
  30 #include &quot;opto/intrinsicnode.hpp&quot;
  31 #include &quot;opto/opcodes.hpp&quot;
  32 #include &quot;runtime/biasedLocking.hpp&quot;
  33 #include &quot;runtime/objectMonitor.hpp&quot;
  34 #include &quot;runtime/stubRoutines.hpp&quot;
  35 
  36 void C2_MacroAssembler::setvectmask(Register dst, Register src) {
  37   guarantee(PostLoopMultiversioning, &quot;must be&quot;);
  38   Assembler::movl(dst, 1);
  39   Assembler::shlxl(dst, dst, src);
  40   Assembler::decl(dst);
  41   Assembler::kmovdl(k1, dst);
  42   Assembler::movl(dst, src);
  43 }
  44 
  45 void C2_MacroAssembler::restorevectmask() {
  46   guarantee(PostLoopMultiversioning, &quot;must be&quot;);
  47   Assembler::knotwl(k1, k0);
  48 }
  49 
  50 #if INCLUDE_RTM_OPT
  51 
  52 // Update rtm_counters based on abort status
  53 // input: abort_status
  54 //        rtm_counters (RTMLockingCounters*)
  55 // flags are killed
  56 void C2_MacroAssembler::rtm_counters_update(Register abort_status, Register rtm_counters) {
  57 
  58   atomic_incptr(Address(rtm_counters, RTMLockingCounters::abort_count_offset()));
  59   if (PrintPreciseRTMLockingStatistics) {
  60     for (int i = 0; i &lt; RTMLockingCounters::ABORT_STATUS_LIMIT; i++) {
  61       Label check_abort;
  62       testl(abort_status, (1&lt;&lt;i));
  63       jccb(Assembler::equal, check_abort);
  64       atomic_incptr(Address(rtm_counters, RTMLockingCounters::abortX_count_offset() + (i * sizeof(uintx))));
  65       bind(check_abort);
  66     }
  67   }
  68 }
  69 
  70 // Branch if (random &amp; (count-1) != 0), count is 2^n
  71 // tmp, scr and flags are killed
  72 void C2_MacroAssembler::branch_on_random_using_rdtsc(Register tmp, Register scr, int count, Label&amp; brLabel) {
  73   assert(tmp == rax, &quot;&quot;);
  74   assert(scr == rdx, &quot;&quot;);
  75   rdtsc(); // modifies EDX:EAX
  76   andptr(tmp, count-1);
  77   jccb(Assembler::notZero, brLabel);
  78 }
  79 
  80 // Perform abort ratio calculation, set no_rtm bit if high ratio
  81 // input:  rtm_counters_Reg (RTMLockingCounters* address)
  82 // tmpReg, rtm_counters_Reg and flags are killed
  83 void C2_MacroAssembler::rtm_abort_ratio_calculation(Register tmpReg,
  84                                                     Register rtm_counters_Reg,
  85                                                     RTMLockingCounters* rtm_counters,
  86                                                     Metadata* method_data) {
  87   Label L_done, L_check_always_rtm1, L_check_always_rtm2;
  88 
  89   if (RTMLockingCalculationDelay &gt; 0) {
  90     // Delay calculation
  91     movptr(tmpReg, ExternalAddress((address) RTMLockingCounters::rtm_calculation_flag_addr()), tmpReg);
  92     testptr(tmpReg, tmpReg);
  93     jccb(Assembler::equal, L_done);
  94   }
  95   // Abort ratio calculation only if abort_count &gt; RTMAbortThreshold
  96   //   Aborted transactions = abort_count * 100
  97   //   All transactions = total_count *  RTMTotalCountIncrRate
  98   //   Set no_rtm bit if (Aborted transactions &gt;= All transactions * RTMAbortRatio)
  99 
 100   movptr(tmpReg, Address(rtm_counters_Reg, RTMLockingCounters::abort_count_offset()));
 101   cmpptr(tmpReg, RTMAbortThreshold);
 102   jccb(Assembler::below, L_check_always_rtm2);
 103   imulptr(tmpReg, tmpReg, 100);
 104 
 105   Register scrReg = rtm_counters_Reg;
 106   movptr(scrReg, Address(rtm_counters_Reg, RTMLockingCounters::total_count_offset()));
 107   imulptr(scrReg, scrReg, RTMTotalCountIncrRate);
 108   imulptr(scrReg, scrReg, RTMAbortRatio);
 109   cmpptr(tmpReg, scrReg);
 110   jccb(Assembler::below, L_check_always_rtm1);
 111   if (method_data != NULL) {
 112     // set rtm_state to &quot;no rtm&quot; in MDO
 113     mov_metadata(tmpReg, method_data);
 114     lock();
 115     orl(Address(tmpReg, MethodData::rtm_state_offset_in_bytes()), NoRTM);
 116   }
 117   jmpb(L_done);
 118   bind(L_check_always_rtm1);
 119   // Reload RTMLockingCounters* address
 120   lea(rtm_counters_Reg, ExternalAddress((address)rtm_counters));
 121   bind(L_check_always_rtm2);
 122   movptr(tmpReg, Address(rtm_counters_Reg, RTMLockingCounters::total_count_offset()));
 123   cmpptr(tmpReg, RTMLockingThreshold / RTMTotalCountIncrRate);
 124   jccb(Assembler::below, L_done);
 125   if (method_data != NULL) {
 126     // set rtm_state to &quot;always rtm&quot; in MDO
 127     mov_metadata(tmpReg, method_data);
 128     lock();
 129     orl(Address(tmpReg, MethodData::rtm_state_offset_in_bytes()), UseRTM);
 130   }
 131   bind(L_done);
 132 }
 133 
 134 // Update counters and perform abort ratio calculation
 135 // input:  abort_status_Reg
 136 // rtm_counters_Reg, flags are killed
 137 void C2_MacroAssembler::rtm_profiling(Register abort_status_Reg,
 138                                       Register rtm_counters_Reg,
 139                                       RTMLockingCounters* rtm_counters,
 140                                       Metadata* method_data,
 141                                       bool profile_rtm) {
 142 
 143   assert(rtm_counters != NULL, &quot;should not be NULL when profiling RTM&quot;);
 144   // update rtm counters based on rax value at abort
 145   // reads abort_status_Reg, updates flags
 146   lea(rtm_counters_Reg, ExternalAddress((address)rtm_counters));
 147   rtm_counters_update(abort_status_Reg, rtm_counters_Reg);
 148   if (profile_rtm) {
 149     // Save abort status because abort_status_Reg is used by following code.
 150     if (RTMRetryCount &gt; 0) {
 151       push(abort_status_Reg);
 152     }
 153     assert(rtm_counters != NULL, &quot;should not be NULL when profiling RTM&quot;);
 154     rtm_abort_ratio_calculation(abort_status_Reg, rtm_counters_Reg, rtm_counters, method_data);
 155     // restore abort status
 156     if (RTMRetryCount &gt; 0) {
 157       pop(abort_status_Reg);
 158     }
 159   }
 160 }
 161 
 162 // Retry on abort if abort&#39;s status is 0x6: can retry (0x2) | memory conflict (0x4)
 163 // inputs: retry_count_Reg
 164 //       : abort_status_Reg
 165 // output: retry_count_Reg decremented by 1
 166 // flags are killed
 167 void C2_MacroAssembler::rtm_retry_lock_on_abort(Register retry_count_Reg, Register abort_status_Reg, Label&amp; retryLabel) {
 168   Label doneRetry;
 169   assert(abort_status_Reg == rax, &quot;&quot;);
 170   // The abort reason bits are in eax (see all states in rtmLocking.hpp)
 171   // 0x6 = conflict on which we can retry (0x2) | memory conflict (0x4)
 172   // if reason is in 0x6 and retry count != 0 then retry
 173   andptr(abort_status_Reg, 0x6);
 174   jccb(Assembler::zero, doneRetry);
 175   testl(retry_count_Reg, retry_count_Reg);
 176   jccb(Assembler::zero, doneRetry);
 177   pause();
 178   decrementl(retry_count_Reg);
 179   jmp(retryLabel);
 180   bind(doneRetry);
 181 }
 182 
 183 // Spin and retry if lock is busy,
 184 // inputs: box_Reg (monitor address)
 185 //       : retry_count_Reg
 186 // output: retry_count_Reg decremented by 1
 187 //       : clear z flag if retry count exceeded
 188 // tmp_Reg, scr_Reg, flags are killed
 189 void C2_MacroAssembler::rtm_retry_lock_on_busy(Register retry_count_Reg, Register box_Reg,
 190                                                Register tmp_Reg, Register scr_Reg, Label&amp; retryLabel) {
 191   Label SpinLoop, SpinExit, doneRetry;
 192   int owner_offset = OM_OFFSET_NO_MONITOR_VALUE_TAG(owner);
 193 
 194   testl(retry_count_Reg, retry_count_Reg);
 195   jccb(Assembler::zero, doneRetry);
 196   decrementl(retry_count_Reg);
 197   movptr(scr_Reg, RTMSpinLoopCount);
 198 
 199   bind(SpinLoop);
 200   pause();
 201   decrementl(scr_Reg);
 202   jccb(Assembler::lessEqual, SpinExit);
 203   movptr(tmp_Reg, Address(box_Reg, owner_offset));
 204   testptr(tmp_Reg, tmp_Reg);
 205   jccb(Assembler::notZero, SpinLoop);
 206 
 207   bind(SpinExit);
 208   jmp(retryLabel);
 209   bind(doneRetry);
 210   incrementl(retry_count_Reg); // clear z flag
 211 }
 212 
 213 // Use RTM for normal stack locks
 214 // Input: objReg (object to lock)
 215 void C2_MacroAssembler::rtm_stack_locking(Register objReg, Register tmpReg, Register scrReg,
 216                                          Register retry_on_abort_count_Reg,
 217                                          RTMLockingCounters* stack_rtm_counters,
 218                                          Metadata* method_data, bool profile_rtm,
 219                                          Label&amp; DONE_LABEL, Label&amp; IsInflated) {
 220   assert(UseRTMForStackLocks, &quot;why call this otherwise?&quot;);
 221   assert(!UseBiasedLocking, &quot;Biased locking is not supported with RTM locking&quot;);
 222   assert(tmpReg == rax, &quot;&quot;);
 223   assert(scrReg == rdx, &quot;&quot;);
 224   Label L_rtm_retry, L_decrement_retry, L_on_abort;
 225 
 226   if (RTMRetryCount &gt; 0) {
 227     movl(retry_on_abort_count_Reg, RTMRetryCount); // Retry on abort
 228     bind(L_rtm_retry);
 229   }
 230   movptr(tmpReg, Address(objReg, oopDesc::mark_offset_in_bytes()));
 231   testptr(tmpReg, markWord::monitor_value);  // inflated vs stack-locked|neutral|biased
 232   jcc(Assembler::notZero, IsInflated);
 233 
 234   if (PrintPreciseRTMLockingStatistics || profile_rtm) {
 235     Label L_noincrement;
 236     if (RTMTotalCountIncrRate &gt; 1) {
 237       // tmpReg, scrReg and flags are killed
 238       branch_on_random_using_rdtsc(tmpReg, scrReg, RTMTotalCountIncrRate, L_noincrement);
 239     }
 240     assert(stack_rtm_counters != NULL, &quot;should not be NULL when profiling RTM&quot;);
 241     atomic_incptr(ExternalAddress((address)stack_rtm_counters-&gt;total_count_addr()), scrReg);
 242     bind(L_noincrement);
 243   }
 244   xbegin(L_on_abort);
 245   movptr(tmpReg, Address(objReg, oopDesc::mark_offset_in_bytes()));       // fetch markword
 246   andptr(tmpReg, markWord::biased_lock_mask_in_place); // look at 3 lock bits
 247   cmpptr(tmpReg, markWord::unlocked_value);            // bits = 001 unlocked
 248   jcc(Assembler::equal, DONE_LABEL);        // all done if unlocked
 249 
 250   Register abort_status_Reg = tmpReg; // status of abort is stored in RAX
 251   if (UseRTMXendForLockBusy) {
 252     xend();
 253     movptr(abort_status_Reg, 0x2);   // Set the abort status to 2 (so we can retry)
 254     jmp(L_decrement_retry);
 255   }
 256   else {
 257     xabort(0);
 258   }
 259   bind(L_on_abort);
 260   if (PrintPreciseRTMLockingStatistics || profile_rtm) {
 261     rtm_profiling(abort_status_Reg, scrReg, stack_rtm_counters, method_data, profile_rtm);
 262   }
 263   bind(L_decrement_retry);
 264   if (RTMRetryCount &gt; 0) {
 265     // retry on lock abort if abort status is &#39;can retry&#39; (0x2) or &#39;memory conflict&#39; (0x4)
 266     rtm_retry_lock_on_abort(retry_on_abort_count_Reg, abort_status_Reg, L_rtm_retry);
 267   }
 268 }
 269 
 270 // Use RTM for inflating locks
 271 // inputs: objReg (object to lock)
 272 //         boxReg (on-stack box address (displaced header location) - KILLED)
 273 //         tmpReg (ObjectMonitor address + markWord::monitor_value)
 274 void C2_MacroAssembler::rtm_inflated_locking(Register objReg, Register boxReg, Register tmpReg,
 275                                             Register scrReg, Register retry_on_busy_count_Reg,
 276                                             Register retry_on_abort_count_Reg,
 277                                             RTMLockingCounters* rtm_counters,
 278                                             Metadata* method_data, bool profile_rtm,
 279                                             Label&amp; DONE_LABEL) {
 280   assert(UseRTMLocking, &quot;why call this otherwise?&quot;);
 281   assert(tmpReg == rax, &quot;&quot;);
 282   assert(scrReg == rdx, &quot;&quot;);
 283   Label L_rtm_retry, L_decrement_retry, L_on_abort;
 284   int owner_offset = OM_OFFSET_NO_MONITOR_VALUE_TAG(owner);
 285 
 286   // Without cast to int32_t this style of movptr will destroy r10 which is typically obj.
 287   movptr(Address(boxReg, 0), (int32_t)intptr_t(markWord::unused_mark().value()));
 288   movptr(boxReg, tmpReg); // Save ObjectMonitor address
 289 
 290   if (RTMRetryCount &gt; 0) {
 291     movl(retry_on_busy_count_Reg, RTMRetryCount);  // Retry on lock busy
 292     movl(retry_on_abort_count_Reg, RTMRetryCount); // Retry on abort
 293     bind(L_rtm_retry);
 294   }
 295   if (PrintPreciseRTMLockingStatistics || profile_rtm) {
 296     Label L_noincrement;
 297     if (RTMTotalCountIncrRate &gt; 1) {
 298       // tmpReg, scrReg and flags are killed
 299       branch_on_random_using_rdtsc(tmpReg, scrReg, RTMTotalCountIncrRate, L_noincrement);
 300     }
 301     assert(rtm_counters != NULL, &quot;should not be NULL when profiling RTM&quot;);
 302     atomic_incptr(ExternalAddress((address)rtm_counters-&gt;total_count_addr()), scrReg);
 303     bind(L_noincrement);
 304   }
 305   xbegin(L_on_abort);
 306   movptr(tmpReg, Address(objReg, oopDesc::mark_offset_in_bytes()));
 307   movptr(tmpReg, Address(tmpReg, owner_offset));
 308   testptr(tmpReg, tmpReg);
 309   jcc(Assembler::zero, DONE_LABEL);
 310   if (UseRTMXendForLockBusy) {
 311     xend();
 312     jmp(L_decrement_retry);
 313   }
 314   else {
 315     xabort(0);
 316   }
 317   bind(L_on_abort);
 318   Register abort_status_Reg = tmpReg; // status of abort is stored in RAX
 319   if (PrintPreciseRTMLockingStatistics || profile_rtm) {
 320     rtm_profiling(abort_status_Reg, scrReg, rtm_counters, method_data, profile_rtm);
 321   }
 322   if (RTMRetryCount &gt; 0) {
 323     // retry on lock abort if abort status is &#39;can retry&#39; (0x2) or &#39;memory conflict&#39; (0x4)
 324     rtm_retry_lock_on_abort(retry_on_abort_count_Reg, abort_status_Reg, L_rtm_retry);
 325   }
 326 
 327   movptr(tmpReg, Address(boxReg, owner_offset)) ;
 328   testptr(tmpReg, tmpReg) ;
 329   jccb(Assembler::notZero, L_decrement_retry) ;
 330 
 331   // Appears unlocked - try to swing _owner from null to non-null.
 332   // Invariant: tmpReg == 0.  tmpReg is EAX which is the implicit cmpxchg comparand.
 333 #ifdef _LP64
 334   Register threadReg = r15_thread;
 335 #else
 336   get_thread(scrReg);
 337   Register threadReg = scrReg;
 338 #endif
 339   lock();
 340   cmpxchgptr(threadReg, Address(boxReg, owner_offset)); // Updates tmpReg
 341 
 342   if (RTMRetryCount &gt; 0) {
 343     // success done else retry
 344     jccb(Assembler::equal, DONE_LABEL) ;
 345     bind(L_decrement_retry);
 346     // Spin and retry if lock is busy.
 347     rtm_retry_lock_on_busy(retry_on_busy_count_Reg, boxReg, tmpReg, scrReg, L_rtm_retry);
 348   }
 349   else {
 350     bind(L_decrement_retry);
 351   }
 352 }
 353 
 354 #endif //  INCLUDE_RTM_OPT
 355 
 356 // fast_lock and fast_unlock used by C2
 357 
 358 // Because the transitions from emitted code to the runtime
 359 // monitorenter/exit helper stubs are so slow it&#39;s critical that
 360 // we inline both the stack-locking fast path and the inflated fast path.
 361 //
 362 // See also: cmpFastLock and cmpFastUnlock.
 363 //
 364 // What follows is a specialized inline transliteration of the code
 365 // in enter() and exit(). If we&#39;re concerned about I$ bloat another
 366 // option would be to emit TrySlowEnter and TrySlowExit methods
 367 // at startup-time.  These methods would accept arguments as
 368 // (rax,=Obj, rbx=Self, rcx=box, rdx=Scratch) and return success-failure
 369 // indications in the icc.ZFlag.  fast_lock and fast_unlock would simply
 370 // marshal the arguments and emit calls to TrySlowEnter and TrySlowExit.
 371 // In practice, however, the # of lock sites is bounded and is usually small.
 372 // Besides the call overhead, TrySlowEnter and TrySlowExit might suffer
 373 // if the processor uses simple bimodal branch predictors keyed by EIP
 374 // Since the helper routines would be called from multiple synchronization
 375 // sites.
 376 //
 377 // An even better approach would be write &quot;MonitorEnter()&quot; and &quot;MonitorExit()&quot;
 378 // in java - using j.u.c and unsafe - and just bind the lock and unlock sites
 379 // to those specialized methods.  That&#39;d give us a mostly platform-independent
 380 // implementation that the JITs could optimize and inline at their pleasure.
 381 // Done correctly, the only time we&#39;d need to cross to native could would be
 382 // to park() or unpark() threads.  We&#39;d also need a few more unsafe operators
 383 // to (a) prevent compiler-JIT reordering of non-volatile accesses, and
 384 // (b) explicit barriers or fence operations.
 385 //
 386 // TODO:
 387 //
 388 // *  Arrange for C2 to pass &quot;Self&quot; into fast_lock and fast_unlock in one of the registers (scr).
 389 //    This avoids manifesting the Self pointer in the fast_lock and fast_unlock terminals.
 390 //    Given TLAB allocation, Self is usually manifested in a register, so passing it into
 391 //    the lock operators would typically be faster than reifying Self.
 392 //
 393 // *  Ideally I&#39;d define the primitives as:
 394 //       fast_lock   (nax Obj, nax box, EAX tmp, nax scr) where box, tmp and scr are KILLED.
 395 //       fast_unlock (nax Obj, EAX box, nax tmp) where box and tmp are KILLED
 396 //    Unfortunately ADLC bugs prevent us from expressing the ideal form.
 397 //    Instead, we&#39;re stuck with a rather awkward and brittle register assignments below.
 398 //    Furthermore the register assignments are overconstrained, possibly resulting in
 399 //    sub-optimal code near the synchronization site.
 400 //
 401 // *  Eliminate the sp-proximity tests and just use &quot;== Self&quot; tests instead.
 402 //    Alternately, use a better sp-proximity test.
 403 //
 404 // *  Currently ObjectMonitor._Owner can hold either an sp value or a (THREAD *) value.
 405 //    Either one is sufficient to uniquely identify a thread.
 406 //    TODO: eliminate use of sp in _owner and use get_thread(tr) instead.
 407 //
 408 // *  Intrinsify notify() and notifyAll() for the common cases where the
 409 //    object is locked by the calling thread but the waitlist is empty.
 410 //    avoid the expensive JNI call to JVM_Notify() and JVM_NotifyAll().
 411 //
 412 // *  use jccb and jmpb instead of jcc and jmp to improve code density.
 413 //    But beware of excessive branch density on AMD Opterons.
 414 //
 415 // *  Both fast_lock and fast_unlock set the ICC.ZF to indicate success
 416 //    or failure of the fast path.  If the fast path fails then we pass
 417 //    control to the slow path, typically in C.  In fast_lock and
 418 //    fast_unlock we often branch to DONE_LABEL, just to find that C2
 419 //    will emit a conditional branch immediately after the node.
 420 //    So we have branches to branches and lots of ICC.ZF games.
 421 //    Instead, it might be better to have C2 pass a &quot;FailureLabel&quot;
 422 //    into fast_lock and fast_unlock.  In the case of success, control
 423 //    will drop through the node.  ICC.ZF is undefined at exit.
 424 //    In the case of failure, the node will branch directly to the
 425 //    FailureLabel
 426 
 427 
 428 // obj: object to lock
 429 // box: on-stack box address (displaced header location) - KILLED
 430 // rax,: tmp -- KILLED
 431 // scr: tmp -- KILLED
 432 void C2_MacroAssembler::fast_lock(Register objReg, Register boxReg, Register tmpReg,
 433                                  Register scrReg, Register cx1Reg, Register cx2Reg,
 434                                  BiasedLockingCounters* counters,
 435                                  RTMLockingCounters* rtm_counters,
 436                                  RTMLockingCounters* stack_rtm_counters,
 437                                  Metadata* method_data,
 438                                  bool use_rtm, bool profile_rtm) {
 439   // Ensure the register assignments are disjoint
 440   assert(tmpReg == rax, &quot;&quot;);
 441 
 442   if (use_rtm) {
 443     assert_different_registers(objReg, boxReg, tmpReg, scrReg, cx1Reg, cx2Reg);
 444   } else {
 445     assert(cx2Reg == noreg, &quot;&quot;);
 446     assert_different_registers(objReg, boxReg, tmpReg, scrReg);
 447   }
 448 
 449   if (counters != NULL) {
 450     atomic_incl(ExternalAddress((address)counters-&gt;total_entry_count_addr()), scrReg);
 451   }
 452 
 453   // Possible cases that we&#39;ll encounter in fast_lock
 454   // ------------------------------------------------
 455   // * Inflated
 456   //    -- unlocked
 457   //    -- Locked
 458   //       = by self
 459   //       = by other
 460   // * biased
 461   //    -- by Self
 462   //    -- by other
 463   // * neutral
 464   // * stack-locked
 465   //    -- by self
 466   //       = sp-proximity test hits
 467   //       = sp-proximity test generates false-negative
 468   //    -- by other
 469   //
 470 
 471   Label IsInflated, DONE_LABEL;
 472 
 473   // it&#39;s stack-locked, biased or neutral
 474   // TODO: optimize away redundant LDs of obj-&gt;mark and improve the markword triage
 475   // order to reduce the number of conditional branches in the most common cases.
 476   // Beware -- there&#39;s a subtle invariant that fetch of the markword
 477   // at [FETCH], below, will never observe a biased encoding (*101b).
 478   // If this invariant is not held we risk exclusion (safety) failure.
 479   if (UseBiasedLocking &amp;&amp; !UseOptoBiasInlining) {
 480     biased_locking_enter(boxReg, objReg, tmpReg, scrReg, cx1Reg, false, DONE_LABEL, NULL, counters);
 481   }
 482 
 483 #if INCLUDE_RTM_OPT
 484   if (UseRTMForStackLocks &amp;&amp; use_rtm) {
 485     rtm_stack_locking(objReg, tmpReg, scrReg, cx2Reg,
 486                       stack_rtm_counters, method_data, profile_rtm,
 487                       DONE_LABEL, IsInflated);
 488   }
 489 #endif // INCLUDE_RTM_OPT
 490 
 491   movptr(tmpReg, Address(objReg, oopDesc::mark_offset_in_bytes()));          // [FETCH]
 492   testptr(tmpReg, markWord::monitor_value); // inflated vs stack-locked|neutral|biased
 493   jccb(Assembler::notZero, IsInflated);
 494 
 495   // Attempt stack-locking ...
 496   orptr (tmpReg, markWord::unlocked_value);
<a name="1" id="anc1"></a><span class="line-added"> 497   if (EnableValhalla &amp;&amp; !UseBiasedLocking) {</span>
<span class="line-added"> 498     // Mask always_locked bit such that we go to the slow path if object is a value type</span>
<span class="line-added"> 499     andptr(tmpReg, ~((int) markWord::biased_lock_bit_in_place));</span>
<span class="line-added"> 500   }</span>
 501   movptr(Address(boxReg, 0), tmpReg);          // Anticipate successful CAS
 502   lock();
 503   cmpxchgptr(boxReg, Address(objReg, oopDesc::mark_offset_in_bytes()));      // Updates tmpReg
 504   if (counters != NULL) {
 505     cond_inc32(Assembler::equal,
 506                ExternalAddress((address)counters-&gt;fast_path_entry_count_addr()));
 507   }
 508   jcc(Assembler::equal, DONE_LABEL);           // Success
 509 
 510   // Recursive locking.
 511   // The object is stack-locked: markword contains stack pointer to BasicLock.
 512   // Locked by current thread if difference with current SP is less than one page.
 513   subptr(tmpReg, rsp);
 514   // Next instruction set ZFlag == 1 (Success) if difference is less then one page.
 515   andptr(tmpReg, (int32_t) (NOT_LP64(0xFFFFF003) LP64_ONLY(7 - os::vm_page_size())) );
 516   movptr(Address(boxReg, 0), tmpReg);
 517   if (counters != NULL) {
 518     cond_inc32(Assembler::equal,
 519                ExternalAddress((address)counters-&gt;fast_path_entry_count_addr()));
 520   }
 521   jmp(DONE_LABEL);
 522 
 523   bind(IsInflated);
 524   // The object is inflated. tmpReg contains pointer to ObjectMonitor* + markWord::monitor_value
 525 
 526 #if INCLUDE_RTM_OPT
 527   // Use the same RTM locking code in 32- and 64-bit VM.
 528   if (use_rtm) {
 529     rtm_inflated_locking(objReg, boxReg, tmpReg, scrReg, cx1Reg, cx2Reg,
 530                          rtm_counters, method_data, profile_rtm, DONE_LABEL);
 531   } else {
 532 #endif // INCLUDE_RTM_OPT
 533 
 534 #ifndef _LP64
 535   // The object is inflated.
 536 
 537   // boxReg refers to the on-stack BasicLock in the current frame.
 538   // We&#39;d like to write:
 539   //   set box-&gt;_displaced_header = markWord::unused_mark().  Any non-0 value suffices.
 540   // This is convenient but results a ST-before-CAS penalty.  The following CAS suffers
 541   // additional latency as we have another ST in the store buffer that must drain.
 542 
 543   // avoid ST-before-CAS
 544   // register juggle because we need tmpReg for cmpxchgptr below
 545   movptr(scrReg, boxReg);
 546   movptr(boxReg, tmpReg);                   // consider: LEA box, [tmp-2]
 547 
 548   // Optimistic form: consider XORL tmpReg,tmpReg
 549   movptr(tmpReg, NULL_WORD);
 550 
 551   // Appears unlocked - try to swing _owner from null to non-null.
 552   // Ideally, I&#39;d manifest &quot;Self&quot; with get_thread and then attempt
 553   // to CAS the register containing Self into m-&gt;Owner.
 554   // But we don&#39;t have enough registers, so instead we can either try to CAS
 555   // rsp or the address of the box (in scr) into &amp;m-&gt;owner.  If the CAS succeeds
 556   // we later store &quot;Self&quot; into m-&gt;Owner.  Transiently storing a stack address
 557   // (rsp or the address of the box) into  m-&gt;owner is harmless.
 558   // Invariant: tmpReg == 0.  tmpReg is EAX which is the implicit cmpxchg comparand.
 559   lock();
 560   cmpxchgptr(scrReg, Address(boxReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)));
 561   movptr(Address(scrReg, 0), 3);          // box-&gt;_displaced_header = 3
 562   // If we weren&#39;t able to swing _owner from NULL to the BasicLock
 563   // then take the slow path.
 564   jccb  (Assembler::notZero, DONE_LABEL);
 565   // update _owner from BasicLock to thread
 566   get_thread (scrReg);                    // beware: clobbers ICCs
 567   movptr(Address(boxReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)), scrReg);
 568   xorptr(boxReg, boxReg);                 // set icc.ZFlag = 1 to indicate success
 569 
 570   // If the CAS fails we can either retry or pass control to the slow path.
 571   // We use the latter tactic.
 572   // Pass the CAS result in the icc.ZFlag into DONE_LABEL
 573   // If the CAS was successful ...
 574   //   Self has acquired the lock
 575   //   Invariant: m-&gt;_recursions should already be 0, so we don&#39;t need to explicitly set it.
 576   // Intentional fall-through into DONE_LABEL ...
 577 #else // _LP64
 578   // It&#39;s inflated and we use scrReg for ObjectMonitor* in this section.
 579   movq(scrReg, tmpReg);
 580   xorq(tmpReg, tmpReg);
 581   lock();
 582   cmpxchgptr(r15_thread, Address(scrReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)));
 583   // Unconditionally set box-&gt;_displaced_header = markWord::unused_mark().
 584   // Without cast to int32_t this style of movptr will destroy r10 which is typically obj.
 585   movptr(Address(boxReg, 0), (int32_t)intptr_t(markWord::unused_mark().value()));
 586   // Intentional fall-through into DONE_LABEL ...
 587   // Propagate ICC.ZF from CAS above into DONE_LABEL.
 588 #endif // _LP64
 589 #if INCLUDE_RTM_OPT
 590   } // use_rtm()
 591 #endif
 592   // DONE_LABEL is a hot target - we&#39;d really like to place it at the
 593   // start of cache line by padding with NOPs.
 594   // See the AMD and Intel software optimization manuals for the
 595   // most efficient &quot;long&quot; NOP encodings.
 596   // Unfortunately none of our alignment mechanisms suffice.
 597   bind(DONE_LABEL);
 598 
 599   // At DONE_LABEL the icc ZFlag is set as follows ...
 600   // fast_unlock uses the same protocol.
 601   // ZFlag == 1 -&gt; Success
 602   // ZFlag == 0 -&gt; Failure - force control through the slow path
 603 }
 604 
 605 // obj: object to unlock
 606 // box: box address (displaced header location), killed.  Must be EAX.
 607 // tmp: killed, cannot be obj nor box.
 608 //
 609 // Some commentary on balanced locking:
 610 //
 611 // fast_lock and fast_unlock are emitted only for provably balanced lock sites.
 612 // Methods that don&#39;t have provably balanced locking are forced to run in the
 613 // interpreter - such methods won&#39;t be compiled to use fast_lock and fast_unlock.
 614 // The interpreter provides two properties:
 615 // I1:  At return-time the interpreter automatically and quietly unlocks any
 616 //      objects acquired the current activation (frame).  Recall that the
 617 //      interpreter maintains an on-stack list of locks currently held by
 618 //      a frame.
 619 // I2:  If a method attempts to unlock an object that is not held by the
 620 //      the frame the interpreter throws IMSX.
 621 //
 622 // Lets say A(), which has provably balanced locking, acquires O and then calls B().
 623 // B() doesn&#39;t have provably balanced locking so it runs in the interpreter.
 624 // Control returns to A() and A() unlocks O.  By I1 and I2, above, we know that O
 625 // is still locked by A().
 626 //
 627 // The only other source of unbalanced locking would be JNI.  The &quot;Java Native Interface:
 628 // Programmer&#39;s Guide and Specification&quot; claims that an object locked by jni_monitorenter
 629 // should not be unlocked by &quot;normal&quot; java-level locking and vice-versa.  The specification
 630 // doesn&#39;t specify what will occur if a program engages in such mixed-mode locking, however.
 631 // Arguably given that the spec legislates the JNI case as undefined our implementation
 632 // could reasonably *avoid* checking owner in fast_unlock().
 633 // In the interest of performance we elide m-&gt;Owner==Self check in unlock.
 634 // A perfectly viable alternative is to elide the owner check except when
 635 // Xcheck:jni is enabled.
 636 
 637 void C2_MacroAssembler::fast_unlock(Register objReg, Register boxReg, Register tmpReg, bool use_rtm) {
 638   assert(boxReg == rax, &quot;&quot;);
 639   assert_different_registers(objReg, boxReg, tmpReg);
 640 
 641   Label DONE_LABEL, Stacked, CheckSucc;
 642 
 643   // Critically, the biased locking test must have precedence over
 644   // and appear before the (box-&gt;dhw == 0) recursive stack-lock test.
 645   if (UseBiasedLocking &amp;&amp; !UseOptoBiasInlining) {
 646     biased_locking_exit(objReg, tmpReg, DONE_LABEL);
 647   }
 648 
 649 #if INCLUDE_RTM_OPT
 650   if (UseRTMForStackLocks &amp;&amp; use_rtm) {
 651     assert(!UseBiasedLocking, &quot;Biased locking is not supported with RTM locking&quot;);
 652     Label L_regular_unlock;
 653     movptr(tmpReg, Address(objReg, oopDesc::mark_offset_in_bytes())); // fetch markword
 654     andptr(tmpReg, markWord::biased_lock_mask_in_place);              // look at 3 lock bits
 655     cmpptr(tmpReg, markWord::unlocked_value);                         // bits = 001 unlocked
 656     jccb(Assembler::notEqual, L_regular_unlock);                      // if !HLE RegularLock
 657     xend();                                                           // otherwise end...
 658     jmp(DONE_LABEL);                                                  // ... and we&#39;re done
 659     bind(L_regular_unlock);
 660   }
 661 #endif
 662 
 663   cmpptr(Address(boxReg, 0), (int32_t)NULL_WORD);                   // Examine the displaced header
 664   jcc   (Assembler::zero, DONE_LABEL);                              // 0 indicates recursive stack-lock
 665   movptr(tmpReg, Address(objReg, oopDesc::mark_offset_in_bytes())); // Examine the object&#39;s markword
 666   testptr(tmpReg, markWord::monitor_value);                         // Inflated?
 667   jccb  (Assembler::zero, Stacked);
 668 
 669   // It&#39;s inflated.
 670 #if INCLUDE_RTM_OPT
 671   if (use_rtm) {
 672     Label L_regular_inflated_unlock;
 673     int owner_offset = OM_OFFSET_NO_MONITOR_VALUE_TAG(owner);
 674     movptr(boxReg, Address(tmpReg, owner_offset));
 675     testptr(boxReg, boxReg);
 676     jccb(Assembler::notZero, L_regular_inflated_unlock);
 677     xend();
 678     jmpb(DONE_LABEL);
 679     bind(L_regular_inflated_unlock);
 680   }
 681 #endif
 682 
 683   // Despite our balanced locking property we still check that m-&gt;_owner == Self
 684   // as java routines or native JNI code called by this thread might
 685   // have released the lock.
 686   // Refer to the comments in synchronizer.cpp for how we might encode extra
 687   // state in _succ so we can avoid fetching EntryList|cxq.
 688   //
 689   // I&#39;d like to add more cases in fast_lock() and fast_unlock() --
 690   // such as recursive enter and exit -- but we have to be wary of
 691   // I$ bloat, T$ effects and BP$ effects.
 692   //
 693   // If there&#39;s no contention try a 1-0 exit.  That is, exit without
 694   // a costly MEMBAR or CAS.  See synchronizer.cpp for details on how
 695   // we detect and recover from the race that the 1-0 exit admits.
 696   //
 697   // Conceptually fast_unlock() must execute a STST|LDST &quot;release&quot; barrier
 698   // before it STs null into _owner, releasing the lock.  Updates
 699   // to data protected by the critical section must be visible before
 700   // we drop the lock (and thus before any other thread could acquire
 701   // the lock and observe the fields protected by the lock).
 702   // IA32&#39;s memory-model is SPO, so STs are ordered with respect to
 703   // each other and there&#39;s no need for an explicit barrier (fence).
 704   // See also http://gee.cs.oswego.edu/dl/jmm/cookbook.html.
 705 #ifndef _LP64
 706   get_thread (boxReg);
 707 
 708   // Note that we could employ various encoding schemes to reduce
 709   // the number of loads below (currently 4) to just 2 or 3.
 710   // Refer to the comments in synchronizer.cpp.
 711   // In practice the chain of fetches doesn&#39;t seem to impact performance, however.
 712   xorptr(boxReg, boxReg);
 713   orptr(boxReg, Address(tmpReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(recursions)));
 714   jccb  (Assembler::notZero, DONE_LABEL);
 715   movptr(boxReg, Address(tmpReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(EntryList)));
 716   orptr(boxReg, Address(tmpReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(cxq)));
 717   jccb  (Assembler::notZero, CheckSucc);
 718   movptr(Address(tmpReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)), NULL_WORD);
 719   jmpb  (DONE_LABEL);
 720 
 721   bind (Stacked);
 722   // It&#39;s not inflated and it&#39;s not recursively stack-locked and it&#39;s not biased.
 723   // It must be stack-locked.
 724   // Try to reset the header to displaced header.
 725   // The &quot;box&quot; value on the stack is stable, so we can reload
 726   // and be assured we observe the same value as above.
 727   movptr(tmpReg, Address(boxReg, 0));
 728   lock();
 729   cmpxchgptr(tmpReg, Address(objReg, oopDesc::mark_offset_in_bytes())); // Uses RAX which is box
 730   // Intention fall-thru into DONE_LABEL
 731 
 732   // DONE_LABEL is a hot target - we&#39;d really like to place it at the
 733   // start of cache line by padding with NOPs.
 734   // See the AMD and Intel software optimization manuals for the
 735   // most efficient &quot;long&quot; NOP encodings.
 736   // Unfortunately none of our alignment mechanisms suffice.
 737   bind (CheckSucc);
 738 #else // _LP64
 739   // It&#39;s inflated
 740   xorptr(boxReg, boxReg);
 741   orptr(boxReg, Address(tmpReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(recursions)));
 742   jccb  (Assembler::notZero, DONE_LABEL);
 743   movptr(boxReg, Address(tmpReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(cxq)));
 744   orptr(boxReg, Address(tmpReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(EntryList)));
 745   jccb  (Assembler::notZero, CheckSucc);
 746   // Without cast to int32_t this style of movptr will destroy r10 which is typically obj.
 747   movptr(Address(tmpReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)), (int32_t)NULL_WORD);
 748   jmpb  (DONE_LABEL);
 749 
 750   // Try to avoid passing control into the slow_path ...
 751   Label LSuccess, LGoSlowPath ;
 752   bind  (CheckSucc);
 753 
 754   // The following optional optimization can be elided if necessary
 755   // Effectively: if (succ == null) goto slow path
 756   // The code reduces the window for a race, however,
 757   // and thus benefits performance.
 758   cmpptr(Address(tmpReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(succ)), (int32_t)NULL_WORD);
 759   jccb  (Assembler::zero, LGoSlowPath);
 760 
 761   xorptr(boxReg, boxReg);
 762   // Without cast to int32_t this style of movptr will destroy r10 which is typically obj.
 763   movptr(Address(tmpReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)), (int32_t)NULL_WORD);
 764 
 765   // Memory barrier/fence
 766   // Dekker pivot point -- fulcrum : ST Owner; MEMBAR; LD Succ
 767   // Instead of MFENCE we use a dummy locked add of 0 to the top-of-stack.
 768   // This is faster on Nehalem and AMD Shanghai/Barcelona.
 769   // See https://blogs.oracle.com/dave/entry/instruction_selection_for_volatile_fences
 770   // We might also restructure (ST Owner=0;barrier;LD _Succ) to
 771   // (mov box,0; xchgq box, &amp;m-&gt;Owner; LD _succ) .
 772   lock(); addl(Address(rsp, 0), 0);
 773 
 774   cmpptr(Address(tmpReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(succ)), (int32_t)NULL_WORD);
 775   jccb  (Assembler::notZero, LSuccess);
 776 
 777   // Rare inopportune interleaving - race.
 778   // The successor vanished in the small window above.
 779   // The lock is contended -- (cxq|EntryList) != null -- and there&#39;s no apparent successor.
 780   // We need to ensure progress and succession.
 781   // Try to reacquire the lock.
 782   // If that fails then the new owner is responsible for succession and this
 783   // thread needs to take no further action and can exit via the fast path (success).
 784   // If the re-acquire succeeds then pass control into the slow path.
 785   // As implemented, this latter mode is horrible because we generated more
 786   // coherence traffic on the lock *and* artifically extended the critical section
 787   // length while by virtue of passing control into the slow path.
 788 
 789   // box is really RAX -- the following CMPXCHG depends on that binding
 790   // cmpxchg R,[M] is equivalent to rax = CAS(M,rax,R)
 791   lock();
 792   cmpxchgptr(r15_thread, Address(tmpReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)));
 793   // There&#39;s no successor so we tried to regrab the lock.
 794   // If that didn&#39;t work, then another thread grabbed the
 795   // lock so we&#39;re done (and exit was a success).
 796   jccb  (Assembler::notEqual, LSuccess);
 797   // Intentional fall-through into slow path
 798 
 799   bind  (LGoSlowPath);
 800   orl   (boxReg, 1);                      // set ICC.ZF=0 to indicate failure
 801   jmpb  (DONE_LABEL);
 802 
 803   bind  (LSuccess);
 804   testl (boxReg, 0);                      // set ICC.ZF=1 to indicate success
 805   jmpb  (DONE_LABEL);
 806 
 807   bind  (Stacked);
 808   movptr(tmpReg, Address (boxReg, 0));      // re-fetch
 809   lock();
 810   cmpxchgptr(tmpReg, Address(objReg, oopDesc::mark_offset_in_bytes())); // Uses RAX which is box
 811 
 812 #endif
 813   bind(DONE_LABEL);
 814 }
 815 
 816 //-------------------------------------------------------------------------------------------
 817 // Generic instructions support for use in .ad files C2 code generation
 818 
 819 void C2_MacroAssembler::vabsnegd(int opcode, XMMRegister dst, XMMRegister src, Register scr) {
 820   if (dst != src) {
 821     movdqu(dst, src);
 822   }
 823   if (opcode == Op_AbsVD) {
 824     andpd(dst, ExternalAddress(StubRoutines::x86::vector_double_sign_mask()), scr);
 825   } else {
 826     assert((opcode == Op_NegVD),&quot;opcode should be Op_NegD&quot;);
 827     xorpd(dst, ExternalAddress(StubRoutines::x86::vector_double_sign_flip()), scr);
 828   }
 829 }
 830 
 831 void C2_MacroAssembler::vabsnegd(int opcode, XMMRegister dst, XMMRegister src, int vector_len, Register scr) {
 832   if (opcode == Op_AbsVD) {
 833     vandpd(dst, src, ExternalAddress(StubRoutines::x86::vector_double_sign_mask()), vector_len, scr);
 834   } else {
 835     assert((opcode == Op_NegVD),&quot;opcode should be Op_NegD&quot;);
 836     vxorpd(dst, src, ExternalAddress(StubRoutines::x86::vector_double_sign_flip()), vector_len, scr);
 837   }
 838 }
 839 
 840 void C2_MacroAssembler::vabsnegf(int opcode, XMMRegister dst, XMMRegister src, Register scr) {
 841   if (dst != src) {
 842     movdqu(dst, src);
 843   }
 844   if (opcode == Op_AbsVF) {
 845     andps(dst, ExternalAddress(StubRoutines::x86::vector_float_sign_mask()), scr);
 846   } else {
 847     assert((opcode == Op_NegVF),&quot;opcode should be Op_NegF&quot;);
 848     xorps(dst, ExternalAddress(StubRoutines::x86::vector_float_sign_flip()), scr);
 849   }
 850 }
 851 
 852 void C2_MacroAssembler::vabsnegf(int opcode, XMMRegister dst, XMMRegister src, int vector_len, Register scr) {
 853   if (opcode == Op_AbsVF) {
 854     vandps(dst, src, ExternalAddress(StubRoutines::x86::vector_float_sign_mask()), vector_len, scr);
 855   } else {
 856     assert((opcode == Op_NegVF),&quot;opcode should be Op_NegF&quot;);
 857     vxorps(dst, src, ExternalAddress(StubRoutines::x86::vector_float_sign_flip()), vector_len, scr);
 858   }
 859 }
 860 
 861 void C2_MacroAssembler::vextendbw(bool sign, XMMRegister dst, XMMRegister src) {
 862   if (sign) {
 863     pmovsxbw(dst, src);
 864   } else {
 865     pmovzxbw(dst, src);
 866   }
 867 }
 868 
 869 void C2_MacroAssembler::vextendbw(bool sign, XMMRegister dst, XMMRegister src, int vector_len) {
 870   if (sign) {
 871     vpmovsxbw(dst, src, vector_len);
 872   } else {
 873     vpmovzxbw(dst, src, vector_len);
 874   }
 875 }
 876 
 877 void C2_MacroAssembler::vshiftd(int opcode, XMMRegister dst, XMMRegister src) {
 878   if (opcode == Op_RShiftVI) {
 879     psrad(dst, src);
 880   } else if (opcode == Op_LShiftVI) {
 881     pslld(dst, src);
 882   } else {
 883     assert((opcode == Op_URShiftVI),&quot;opcode should be Op_URShiftVI&quot;);
 884     psrld(dst, src);
 885   }
 886 }
 887 
 888 void C2_MacroAssembler::vshiftd(int opcode, XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {
 889   if (opcode == Op_RShiftVI) {
 890     vpsrad(dst, nds, src, vector_len);
 891   } else if (opcode == Op_LShiftVI) {
 892     vpslld(dst, nds, src, vector_len);
 893   } else {
 894     assert((opcode == Op_URShiftVI),&quot;opcode should be Op_URShiftVI&quot;);
 895     vpsrld(dst, nds, src, vector_len);
 896   }
 897 }
 898 
 899 void C2_MacroAssembler::vshiftw(int opcode, XMMRegister dst, XMMRegister src) {
 900   if ((opcode == Op_RShiftVS) || (opcode == Op_RShiftVB)) {
 901     psraw(dst, src);
 902   } else if ((opcode == Op_LShiftVS) || (opcode == Op_LShiftVB)) {
 903     psllw(dst, src);
 904   } else {
 905     assert(((opcode == Op_URShiftVS) || (opcode == Op_URShiftVB)),&quot;opcode should be one of Op_URShiftVS or Op_URShiftVB&quot;);
 906     psrlw(dst, src);
 907   }
 908 }
 909 
 910 void C2_MacroAssembler::vshiftw(int opcode, XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {
 911   if ((opcode == Op_RShiftVS) || (opcode == Op_RShiftVB)) {
 912     vpsraw(dst, nds, src, vector_len);
 913   } else if ((opcode == Op_LShiftVS) || (opcode == Op_LShiftVB)) {
 914     vpsllw(dst, nds, src, vector_len);
 915   } else {
 916     assert(((opcode == Op_URShiftVS) || (opcode == Op_URShiftVB)),&quot;opcode should be one of Op_URShiftVS or Op_URShiftVB&quot;);
 917     vpsrlw(dst, nds, src, vector_len);
 918   }
 919 }
 920 
 921 void C2_MacroAssembler::vshiftq(int opcode, XMMRegister dst, XMMRegister src) {
 922   if (opcode == Op_RShiftVL) {
 923     psrlq(dst, src);  // using srl to implement sra on pre-avs512 systems
 924   } else if (opcode == Op_LShiftVL) {
 925     psllq(dst, src);
 926   } else {
 927     assert((opcode == Op_URShiftVL),&quot;opcode should be Op_URShiftVL&quot;);
 928     psrlq(dst, src);
 929   }
 930 }
 931 
 932 void C2_MacroAssembler::vshiftq(int opcode, XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {
 933   if (opcode == Op_RShiftVL) {
 934     evpsraq(dst, nds, src, vector_len);
 935   } else if (opcode == Op_LShiftVL) {
 936     vpsllq(dst, nds, src, vector_len);
 937   } else {
 938     assert((opcode == Op_URShiftVL),&quot;opcode should be Op_URShiftVL&quot;);
 939     vpsrlq(dst, nds, src, vector_len);
 940   }
 941 }
 942 
 943 // Reductions for vectors of ints, longs, floats, and doubles.
 944 
 945 void C2_MacroAssembler::reduce_operation_128(int opcode, XMMRegister dst, XMMRegister src) {
 946   int vector_len = Assembler::AVX_128bit;
 947 
 948   switch (opcode) {
 949     case Op_AndReductionV:  pand(dst, src); break;
 950     case Op_OrReductionV:   por (dst, src); break;
 951     case Op_XorReductionV:  pxor(dst, src); break;
 952 
 953     case Op_AddReductionVF: addss(dst, src); break;
 954     case Op_AddReductionVD: addsd(dst, src); break;
 955     case Op_AddReductionVI: paddd(dst, src); break;
 956     case Op_AddReductionVL: paddq(dst, src); break;
 957 
 958     case Op_MulReductionVF: mulss(dst, src); break;
 959     case Op_MulReductionVD: mulsd(dst, src); break;
 960     case Op_MulReductionVI: pmulld(dst, src); break;
 961     case Op_MulReductionVL: vpmullq(dst, dst, src, vector_len); break;
 962 
 963     default: assert(false, &quot;wrong opcode&quot;);
 964   }
 965 }
 966 
 967 void C2_MacroAssembler::reduce_operation_256(int opcode, XMMRegister dst,  XMMRegister src1, XMMRegister src2) {
 968   int vector_len = Assembler::AVX_256bit;
 969 
 970   switch (opcode) {
 971     case Op_AndReductionV:  vpand(dst, src1, src2, vector_len); break;
 972     case Op_OrReductionV:   vpor (dst, src1, src2, vector_len); break;
 973     case Op_XorReductionV:  vpxor(dst, src1, src2, vector_len); break;
 974 
 975     case Op_AddReductionVI: vpaddd(dst, src1, src2, vector_len); break;
 976     case Op_AddReductionVL: vpaddq(dst, src1, src2, vector_len); break;
 977 
 978     case Op_MulReductionVI: vpmulld(dst, src1, src2, vector_len); break;
 979     case Op_MulReductionVL: vpmullq(dst, src1, src2, vector_len); break;
 980 
 981     default: assert(false, &quot;wrong opcode&quot;);
 982   }
 983 }
 984 
 985 void C2_MacroAssembler::reduce_fp(int opcode, int vlen,
 986                                   XMMRegister dst, XMMRegister src,
 987                                   XMMRegister vtmp1, XMMRegister vtmp2) {
 988   switch (opcode) {
 989     case Op_AddReductionVF:
 990     case Op_MulReductionVF:
 991       reduceF(opcode, vlen, dst, src, vtmp1, vtmp2);
 992       break;
 993 
 994     case Op_AddReductionVD:
 995     case Op_MulReductionVD:
 996       reduceD(opcode, vlen, dst, src, vtmp1, vtmp2);
 997       break;
 998 
 999     default: assert(false, &quot;wrong opcode&quot;);
1000   }
1001 }
1002 
1003 void C2_MacroAssembler::reduceI(int opcode, int vlen,
1004                                 Register dst, Register src1, XMMRegister src2,
1005                                 XMMRegister vtmp1, XMMRegister vtmp2) {
1006   switch (vlen) {
1007     case  2: reduce2I (opcode, dst, src1, src2, vtmp1, vtmp2); break;
1008     case  4: reduce4I (opcode, dst, src1, src2, vtmp1, vtmp2); break;
1009     case  8: reduce8I (opcode, dst, src1, src2, vtmp1, vtmp2); break;
1010     case 16: reduce16I(opcode, dst, src1, src2, vtmp1, vtmp2); break;
1011 
1012     default: assert(false, &quot;wrong vector length&quot;);
1013   }
1014 }
1015 
1016 #ifdef _LP64
1017 void C2_MacroAssembler::reduceL(int opcode, int vlen,
1018                                 Register dst, Register src1, XMMRegister src2,
1019                                 XMMRegister vtmp1, XMMRegister vtmp2) {
1020   switch (vlen) {
1021     case 2: reduce2L(opcode, dst, src1, src2, vtmp1, vtmp2); break;
1022     case 4: reduce4L(opcode, dst, src1, src2, vtmp1, vtmp2); break;
1023     case 8: reduce8L(opcode, dst, src1, src2, vtmp1, vtmp2); break;
1024 
1025     default: assert(false, &quot;wrong vector length&quot;);
1026   }
1027 }
1028 #endif // _LP64
1029 
1030 void C2_MacroAssembler::reduceF(int opcode, int vlen, XMMRegister dst, XMMRegister src, XMMRegister vtmp1, XMMRegister vtmp2) {
1031   switch (vlen) {
1032     case 2:
1033       assert(vtmp2 == xnoreg, &quot;&quot;);
1034       reduce2F(opcode, dst, src, vtmp1);
1035       break;
1036     case 4:
1037       assert(vtmp2 == xnoreg, &quot;&quot;);
1038       reduce4F(opcode, dst, src, vtmp1);
1039       break;
1040     case 8:
1041       reduce8F(opcode, dst, src, vtmp1, vtmp2);
1042       break;
1043     case 16:
1044       reduce16F(opcode, dst, src, vtmp1, vtmp2);
1045       break;
1046     default: assert(false, &quot;wrong vector length&quot;);
1047   }
1048 }
1049 
1050 void C2_MacroAssembler::reduceD(int opcode, int vlen, XMMRegister dst, XMMRegister src, XMMRegister vtmp1, XMMRegister vtmp2) {
1051   switch (vlen) {
1052     case 2:
1053       assert(vtmp2 == xnoreg, &quot;&quot;);
1054       reduce2D(opcode, dst, src, vtmp1);
1055       break;
1056     case 4:
1057       reduce4D(opcode, dst, src, vtmp1, vtmp2);
1058       break;
1059     case 8:
1060       reduce8D(opcode, dst, src, vtmp1, vtmp2);
1061       break;
1062     default: assert(false, &quot;wrong vector length&quot;);
1063   }
1064 }
1065 
1066 void C2_MacroAssembler::reduce2I(int opcode, Register dst, Register src1, XMMRegister src2, XMMRegister vtmp1, XMMRegister vtmp2) {
1067   if (opcode == Op_AddReductionVI) {
1068     if (vtmp1 != src2) {
1069       movdqu(vtmp1, src2);
1070     }
1071     phaddd(vtmp1, vtmp1);
1072   } else {
1073     pshufd(vtmp1, src2, 0x1);
1074     reduce_operation_128(opcode, vtmp1, src2);
1075   }
1076   movdl(vtmp2, src1);
1077   reduce_operation_128(opcode, vtmp1, vtmp2);
1078   movdl(dst, vtmp1);
1079 }
1080 
1081 void C2_MacroAssembler::reduce4I(int opcode, Register dst, Register src1, XMMRegister src2, XMMRegister vtmp1, XMMRegister vtmp2) {
1082   if (opcode == Op_AddReductionVI) {
1083     if (vtmp1 != src2) {
1084       movdqu(vtmp1, src2);
1085     }
1086     phaddd(vtmp1, src2);
1087     reduce2I(opcode, dst, src1, vtmp1, vtmp1, vtmp2);
1088   } else {
1089     pshufd(vtmp2, src2, 0xE);
1090     reduce_operation_128(opcode, vtmp2, src2);
1091     reduce2I(opcode, dst, src1, vtmp2, vtmp1, vtmp2);
1092   }
1093 }
1094 
1095 void C2_MacroAssembler::reduce8I(int opcode, Register dst, Register src1, XMMRegister src2, XMMRegister vtmp1, XMMRegister vtmp2) {
1096   if (opcode == Op_AddReductionVI) {
1097     vphaddd(vtmp1, src2, src2, Assembler::AVX_256bit);
1098     vextracti128_high(vtmp2, vtmp1);
1099     vpaddd(vtmp1, vtmp1, vtmp2, Assembler::AVX_128bit);
1100     reduce2I(opcode, dst, src1, vtmp1, vtmp1, vtmp2);
1101   } else {
1102     vextracti128_high(vtmp1, src2);
1103     reduce_operation_128(opcode, vtmp1, src2);
1104     reduce4I(opcode, dst, src1, vtmp1, vtmp1, vtmp2);
1105   }
1106 }
1107 
1108 void C2_MacroAssembler::reduce16I(int opcode, Register dst, Register src1, XMMRegister src2, XMMRegister vtmp1, XMMRegister vtmp2) {
1109   vextracti64x4_high(vtmp2, src2);
1110   reduce_operation_256(opcode, vtmp2, vtmp2, src2);
1111   reduce8I(opcode, dst, src1, vtmp2, vtmp1, vtmp2);
1112 }
1113 
1114 #ifdef _LP64
1115 void C2_MacroAssembler::reduce2L(int opcode, Register dst, Register src1, XMMRegister src2, XMMRegister vtmp1, XMMRegister vtmp2) {
1116   pshufd(vtmp2, src2, 0xE);
1117   reduce_operation_128(opcode, vtmp2, src2);
1118   movdq(vtmp1, src1);
1119   reduce_operation_128(opcode, vtmp1, vtmp2);
1120   movdq(dst, vtmp1);
1121 }
1122 
1123 void C2_MacroAssembler::reduce4L(int opcode, Register dst, Register src1, XMMRegister src2, XMMRegister vtmp1, XMMRegister vtmp2) {
1124   vextracti128_high(vtmp1, src2);
1125   reduce_operation_128(opcode, vtmp1, src2);
1126   reduce2L(opcode, dst, src1, vtmp1, vtmp1, vtmp2);
1127 }
1128 
1129 void C2_MacroAssembler::reduce8L(int opcode, Register dst, Register src1, XMMRegister src2, XMMRegister vtmp1, XMMRegister vtmp2) {
1130   vextracti64x4_high(vtmp2, src2);
1131   reduce_operation_256(opcode, vtmp2, vtmp2, src2);
1132   reduce4L(opcode, dst, src1, vtmp2, vtmp1, vtmp2);
1133 }
1134 #endif // _LP64
1135 
1136 void C2_MacroAssembler::reduce2F(int opcode, XMMRegister dst, XMMRegister src, XMMRegister vtmp) {
1137   reduce_operation_128(opcode, dst, src);
1138   pshufd(vtmp, src, 0x1);
1139   reduce_operation_128(opcode, dst, vtmp);
1140 }
1141 
1142 void C2_MacroAssembler::reduce4F(int opcode, XMMRegister dst, XMMRegister src, XMMRegister vtmp) {
1143   reduce2F(opcode, dst, src, vtmp);
1144   pshufd(vtmp, src, 0x2);
1145   reduce_operation_128(opcode, dst, vtmp);
1146   pshufd(vtmp, src, 0x3);
1147   reduce_operation_128(opcode, dst, vtmp);
1148 }
1149 
1150 void C2_MacroAssembler::reduce8F(int opcode, XMMRegister dst, XMMRegister src, XMMRegister vtmp1, XMMRegister vtmp2) {
1151   reduce4F(opcode, dst, src, vtmp2);
1152   vextractf128_high(vtmp2, src);
1153   reduce4F(opcode, dst, vtmp2, vtmp1);
1154 }
1155 
1156 void C2_MacroAssembler::reduce16F(int opcode, XMMRegister dst, XMMRegister src, XMMRegister vtmp1, XMMRegister vtmp2) {
1157   reduce8F(opcode, dst, src, vtmp1, vtmp2);
1158   vextracti64x4_high(vtmp1, src);
1159   reduce8F(opcode, dst, vtmp1, vtmp1, vtmp2);
1160 }
1161 
1162 void C2_MacroAssembler::reduce2D(int opcode, XMMRegister dst, XMMRegister src, XMMRegister vtmp) {
1163   reduce_operation_128(opcode, dst, src);
1164   pshufd(vtmp, src, 0xE);
1165   reduce_operation_128(opcode, dst, vtmp);
1166 }
1167 
1168 void C2_MacroAssembler::reduce4D(int opcode, XMMRegister dst, XMMRegister src, XMMRegister vtmp1, XMMRegister vtmp2) {
1169   reduce2D(opcode, dst, src, vtmp2);
1170   vextractf128_high(vtmp2, src);
1171   reduce2D(opcode, dst, vtmp2, vtmp1);
1172 }
1173 
1174 void C2_MacroAssembler::reduce8D(int opcode, XMMRegister dst, XMMRegister src, XMMRegister vtmp1, XMMRegister vtmp2) {
1175   reduce4D(opcode, dst, src, vtmp1, vtmp2);
1176   vextracti64x4_high(vtmp1, src);
1177   reduce4D(opcode, dst, vtmp1, vtmp1, vtmp2);
1178 }
1179 
1180 //-------------------------------------------------------------------------------------------
1181 
1182 // IndexOf for constant substrings with size &gt;= 8 chars
1183 // which don&#39;t need to be loaded through stack.
1184 void C2_MacroAssembler::string_indexofC8(Register str1, Register str2,
1185                                          Register cnt1, Register cnt2,
1186                                          int int_cnt2,  Register result,
1187                                          XMMRegister vec, Register tmp,
1188                                          int ae) {
1189   ShortBranchVerifier sbv(this);
1190   assert(UseSSE42Intrinsics, &quot;SSE4.2 intrinsics are required&quot;);
1191   assert(ae != StrIntrinsicNode::LU, &quot;Invalid encoding&quot;);
1192 
1193   // This method uses the pcmpestri instruction with bound registers
1194   //   inputs:
1195   //     xmm - substring
1196   //     rax - substring length (elements count)
1197   //     mem - scanned string
1198   //     rdx - string length (elements count)
1199   //     0xd - mode: 1100 (substring search) + 01 (unsigned shorts)
1200   //     0xc - mode: 1100 (substring search) + 00 (unsigned bytes)
1201   //   outputs:
1202   //     rcx - matched index in string
1203   assert(cnt1 == rdx &amp;&amp; cnt2 == rax &amp;&amp; tmp == rcx, &quot;pcmpestri&quot;);
1204   int mode   = (ae == StrIntrinsicNode::LL) ? 0x0c : 0x0d; // bytes or shorts
1205   int stride = (ae == StrIntrinsicNode::LL) ? 16 : 8; //UU, UL -&gt; 8
1206   Address::ScaleFactor scale1 = (ae == StrIntrinsicNode::LL) ? Address::times_1 : Address::times_2;
1207   Address::ScaleFactor scale2 = (ae == StrIntrinsicNode::UL) ? Address::times_1 : scale1;
1208 
1209   Label RELOAD_SUBSTR, SCAN_TO_SUBSTR, SCAN_SUBSTR,
1210         RET_FOUND, RET_NOT_FOUND, EXIT, FOUND_SUBSTR,
1211         MATCH_SUBSTR_HEAD, RELOAD_STR, FOUND_CANDIDATE;
1212 
1213   // Note, inline_string_indexOf() generates checks:
1214   // if (substr.count &gt; string.count) return -1;
1215   // if (substr.count == 0) return 0;
1216   assert(int_cnt2 &gt;= stride, &quot;this code is used only for cnt2 &gt;= 8 chars&quot;);
1217 
1218   // Load substring.
1219   if (ae == StrIntrinsicNode::UL) {
1220     pmovzxbw(vec, Address(str2, 0));
1221   } else {
1222     movdqu(vec, Address(str2, 0));
1223   }
1224   movl(cnt2, int_cnt2);
1225   movptr(result, str1); // string addr
1226 
1227   if (int_cnt2 &gt; stride) {
1228     jmpb(SCAN_TO_SUBSTR);
1229 
1230     // Reload substr for rescan, this code
1231     // is executed only for large substrings (&gt; 8 chars)
1232     bind(RELOAD_SUBSTR);
1233     if (ae == StrIntrinsicNode::UL) {
1234       pmovzxbw(vec, Address(str2, 0));
1235     } else {
1236       movdqu(vec, Address(str2, 0));
1237     }
1238     negptr(cnt2); // Jumped here with negative cnt2, convert to positive
1239 
1240     bind(RELOAD_STR);
1241     // We came here after the beginning of the substring was
1242     // matched but the rest of it was not so we need to search
1243     // again. Start from the next element after the previous match.
1244 
1245     // cnt2 is number of substring reminding elements and
1246     // cnt1 is number of string reminding elements when cmp failed.
1247     // Restored cnt1 = cnt1 - cnt2 + int_cnt2
1248     subl(cnt1, cnt2);
1249     addl(cnt1, int_cnt2);
1250     movl(cnt2, int_cnt2); // Now restore cnt2
1251 
1252     decrementl(cnt1);     // Shift to next element
1253     cmpl(cnt1, cnt2);
1254     jcc(Assembler::negative, RET_NOT_FOUND);  // Left less then substring
1255 
1256     addptr(result, (1&lt;&lt;scale1));
1257 
1258   } // (int_cnt2 &gt; 8)
1259 
1260   // Scan string for start of substr in 16-byte vectors
1261   bind(SCAN_TO_SUBSTR);
1262   pcmpestri(vec, Address(result, 0), mode);
1263   jccb(Assembler::below, FOUND_CANDIDATE);   // CF == 1
1264   subl(cnt1, stride);
1265   jccb(Assembler::lessEqual, RET_NOT_FOUND); // Scanned full string
1266   cmpl(cnt1, cnt2);
1267   jccb(Assembler::negative, RET_NOT_FOUND);  // Left less then substring
1268   addptr(result, 16);
1269   jmpb(SCAN_TO_SUBSTR);
1270 
1271   // Found a potential substr
1272   bind(FOUND_CANDIDATE);
1273   // Matched whole vector if first element matched (tmp(rcx) == 0).
1274   if (int_cnt2 == stride) {
1275     jccb(Assembler::overflow, RET_FOUND);    // OF == 1
1276   } else { // int_cnt2 &gt; 8
1277     jccb(Assembler::overflow, FOUND_SUBSTR);
1278   }
1279   // After pcmpestri tmp(rcx) contains matched element index
1280   // Compute start addr of substr
1281   lea(result, Address(result, tmp, scale1));
1282 
1283   // Make sure string is still long enough
1284   subl(cnt1, tmp);
1285   cmpl(cnt1, cnt2);
1286   if (int_cnt2 == stride) {
1287     jccb(Assembler::greaterEqual, SCAN_TO_SUBSTR);
1288   } else { // int_cnt2 &gt; 8
1289     jccb(Assembler::greaterEqual, MATCH_SUBSTR_HEAD);
1290   }
1291   // Left less then substring.
1292 
1293   bind(RET_NOT_FOUND);
1294   movl(result, -1);
1295   jmp(EXIT);
1296 
1297   if (int_cnt2 &gt; stride) {
1298     // This code is optimized for the case when whole substring
1299     // is matched if its head is matched.
1300     bind(MATCH_SUBSTR_HEAD);
1301     pcmpestri(vec, Address(result, 0), mode);
1302     // Reload only string if does not match
1303     jcc(Assembler::noOverflow, RELOAD_STR); // OF == 0
1304 
1305     Label CONT_SCAN_SUBSTR;
1306     // Compare the rest of substring (&gt; 8 chars).
1307     bind(FOUND_SUBSTR);
1308     // First 8 chars are already matched.
1309     negptr(cnt2);
1310     addptr(cnt2, stride);
1311 
1312     bind(SCAN_SUBSTR);
1313     subl(cnt1, stride);
1314     cmpl(cnt2, -stride); // Do not read beyond substring
1315     jccb(Assembler::lessEqual, CONT_SCAN_SUBSTR);
1316     // Back-up strings to avoid reading beyond substring:
1317     // cnt1 = cnt1 - cnt2 + 8
1318     addl(cnt1, cnt2); // cnt2 is negative
1319     addl(cnt1, stride);
1320     movl(cnt2, stride); negptr(cnt2);
1321     bind(CONT_SCAN_SUBSTR);
1322     if (int_cnt2 &lt; (int)G) {
1323       int tail_off1 = int_cnt2&lt;&lt;scale1;
1324       int tail_off2 = int_cnt2&lt;&lt;scale2;
1325       if (ae == StrIntrinsicNode::UL) {
1326         pmovzxbw(vec, Address(str2, cnt2, scale2, tail_off2));
1327       } else {
1328         movdqu(vec, Address(str2, cnt2, scale2, tail_off2));
1329       }
1330       pcmpestri(vec, Address(result, cnt2, scale1, tail_off1), mode);
1331     } else {
1332       // calculate index in register to avoid integer overflow (int_cnt2*2)
1333       movl(tmp, int_cnt2);
1334       addptr(tmp, cnt2);
1335       if (ae == StrIntrinsicNode::UL) {
1336         pmovzxbw(vec, Address(str2, tmp, scale2, 0));
1337       } else {
1338         movdqu(vec, Address(str2, tmp, scale2, 0));
1339       }
1340       pcmpestri(vec, Address(result, tmp, scale1, 0), mode);
1341     }
1342     // Need to reload strings pointers if not matched whole vector
1343     jcc(Assembler::noOverflow, RELOAD_SUBSTR); // OF == 0
1344     addptr(cnt2, stride);
1345     jcc(Assembler::negative, SCAN_SUBSTR);
1346     // Fall through if found full substring
1347 
1348   } // (int_cnt2 &gt; 8)
1349 
1350   bind(RET_FOUND);
1351   // Found result if we matched full small substring.
1352   // Compute substr offset
1353   subptr(result, str1);
1354   if (ae == StrIntrinsicNode::UU || ae == StrIntrinsicNode::UL) {
1355     shrl(result, 1); // index
1356   }
1357   bind(EXIT);
1358 
1359 } // string_indexofC8
1360 
1361 // Small strings are loaded through stack if they cross page boundary.
1362 void C2_MacroAssembler::string_indexof(Register str1, Register str2,
1363                                        Register cnt1, Register cnt2,
1364                                        int int_cnt2,  Register result,
1365                                        XMMRegister vec, Register tmp,
1366                                        int ae) {
1367   ShortBranchVerifier sbv(this);
1368   assert(UseSSE42Intrinsics, &quot;SSE4.2 intrinsics are required&quot;);
1369   assert(ae != StrIntrinsicNode::LU, &quot;Invalid encoding&quot;);
1370 
1371   //
1372   // int_cnt2 is length of small (&lt; 8 chars) constant substring
1373   // or (-1) for non constant substring in which case its length
1374   // is in cnt2 register.
1375   //
1376   // Note, inline_string_indexOf() generates checks:
1377   // if (substr.count &gt; string.count) return -1;
1378   // if (substr.count == 0) return 0;
1379   //
1380   int stride = (ae == StrIntrinsicNode::LL) ? 16 : 8; //UU, UL -&gt; 8
1381   assert(int_cnt2 == -1 || (0 &lt; int_cnt2 &amp;&amp; int_cnt2 &lt; stride), &quot;should be != 0&quot;);
1382   // This method uses the pcmpestri instruction with bound registers
1383   //   inputs:
1384   //     xmm - substring
1385   //     rax - substring length (elements count)
1386   //     mem - scanned string
1387   //     rdx - string length (elements count)
1388   //     0xd - mode: 1100 (substring search) + 01 (unsigned shorts)
1389   //     0xc - mode: 1100 (substring search) + 00 (unsigned bytes)
1390   //   outputs:
1391   //     rcx - matched index in string
1392   assert(cnt1 == rdx &amp;&amp; cnt2 == rax &amp;&amp; tmp == rcx, &quot;pcmpestri&quot;);
1393   int mode = (ae == StrIntrinsicNode::LL) ? 0x0c : 0x0d; // bytes or shorts
1394   Address::ScaleFactor scale1 = (ae == StrIntrinsicNode::LL) ? Address::times_1 : Address::times_2;
1395   Address::ScaleFactor scale2 = (ae == StrIntrinsicNode::UL) ? Address::times_1 : scale1;
1396 
1397   Label RELOAD_SUBSTR, SCAN_TO_SUBSTR, SCAN_SUBSTR, ADJUST_STR,
1398         RET_FOUND, RET_NOT_FOUND, CLEANUP, FOUND_SUBSTR,
1399         FOUND_CANDIDATE;
1400 
1401   { //========================================================
1402     // We don&#39;t know where these strings are located
1403     // and we can&#39;t read beyond them. Load them through stack.
1404     Label BIG_STRINGS, CHECK_STR, COPY_SUBSTR, COPY_STR;
1405 
1406     movptr(tmp, rsp); // save old SP
1407 
1408     if (int_cnt2 &gt; 0) {     // small (&lt; 8 chars) constant substring
1409       if (int_cnt2 == (1&gt;&gt;scale2)) { // One byte
1410         assert((ae == StrIntrinsicNode::LL || ae == StrIntrinsicNode::UL), &quot;Only possible for latin1 encoding&quot;);
1411         load_unsigned_byte(result, Address(str2, 0));
1412         movdl(vec, result); // move 32 bits
1413       } else if (ae == StrIntrinsicNode::LL &amp;&amp; int_cnt2 == 3) {  // Three bytes
1414         // Not enough header space in 32-bit VM: 12+3 = 15.
1415         movl(result, Address(str2, -1));
1416         shrl(result, 8);
1417         movdl(vec, result); // move 32 bits
1418       } else if (ae != StrIntrinsicNode::UL &amp;&amp; int_cnt2 == (2&gt;&gt;scale2)) {  // One char
1419         load_unsigned_short(result, Address(str2, 0));
1420         movdl(vec, result); // move 32 bits
1421       } else if (ae != StrIntrinsicNode::UL &amp;&amp; int_cnt2 == (4&gt;&gt;scale2)) { // Two chars
1422         movdl(vec, Address(str2, 0)); // move 32 bits
1423       } else if (ae != StrIntrinsicNode::UL &amp;&amp; int_cnt2 == (8&gt;&gt;scale2)) { // Four chars
1424         movq(vec, Address(str2, 0));  // move 64 bits
1425       } else { // cnt2 = { 3, 5, 6, 7 } || (ae == StrIntrinsicNode::UL &amp;&amp; cnt2 ={2, ..., 7})
1426         // Array header size is 12 bytes in 32-bit VM
1427         // + 6 bytes for 3 chars == 18 bytes,
1428         // enough space to load vec and shift.
1429         assert(HeapWordSize*TypeArrayKlass::header_size() &gt;= 12,&quot;sanity&quot;);
1430         if (ae == StrIntrinsicNode::UL) {
1431           int tail_off = int_cnt2-8;
1432           pmovzxbw(vec, Address(str2, tail_off));
1433           psrldq(vec, -2*tail_off);
1434         }
1435         else {
1436           int tail_off = int_cnt2*(1&lt;&lt;scale2);
1437           movdqu(vec, Address(str2, tail_off-16));
1438           psrldq(vec, 16-tail_off);
1439         }
1440       }
1441     } else { // not constant substring
1442       cmpl(cnt2, stride);
1443       jccb(Assembler::aboveEqual, BIG_STRINGS); // Both strings are big enough
1444 
1445       // We can read beyond string if srt+16 does not cross page boundary
1446       // since heaps are aligned and mapped by pages.
1447       assert(os::vm_page_size() &lt; (int)G, &quot;default page should be small&quot;);
1448       movl(result, str2); // We need only low 32 bits
1449       andl(result, (os::vm_page_size()-1));
1450       cmpl(result, (os::vm_page_size()-16));
1451       jccb(Assembler::belowEqual, CHECK_STR);
1452 
1453       // Move small strings to stack to allow load 16 bytes into vec.
1454       subptr(rsp, 16);
1455       int stk_offset = wordSize-(1&lt;&lt;scale2);
1456       push(cnt2);
1457 
1458       bind(COPY_SUBSTR);
1459       if (ae == StrIntrinsicNode::LL || ae == StrIntrinsicNode::UL) {
1460         load_unsigned_byte(result, Address(str2, cnt2, scale2, -1));
1461         movb(Address(rsp, cnt2, scale2, stk_offset), result);
1462       } else if (ae == StrIntrinsicNode::UU) {
1463         load_unsigned_short(result, Address(str2, cnt2, scale2, -2));
1464         movw(Address(rsp, cnt2, scale2, stk_offset), result);
1465       }
1466       decrement(cnt2);
1467       jccb(Assembler::notZero, COPY_SUBSTR);
1468 
1469       pop(cnt2);
1470       movptr(str2, rsp);  // New substring address
1471     } // non constant
1472 
1473     bind(CHECK_STR);
1474     cmpl(cnt1, stride);
1475     jccb(Assembler::aboveEqual, BIG_STRINGS);
1476 
1477     // Check cross page boundary.
1478     movl(result, str1); // We need only low 32 bits
1479     andl(result, (os::vm_page_size()-1));
1480     cmpl(result, (os::vm_page_size()-16));
1481     jccb(Assembler::belowEqual, BIG_STRINGS);
1482 
1483     subptr(rsp, 16);
1484     int stk_offset = -(1&lt;&lt;scale1);
1485     if (int_cnt2 &lt; 0) { // not constant
1486       push(cnt2);
1487       stk_offset += wordSize;
1488     }
1489     movl(cnt2, cnt1);
1490 
1491     bind(COPY_STR);
1492     if (ae == StrIntrinsicNode::LL) {
1493       load_unsigned_byte(result, Address(str1, cnt2, scale1, -1));
1494       movb(Address(rsp, cnt2, scale1, stk_offset), result);
1495     } else {
1496       load_unsigned_short(result, Address(str1, cnt2, scale1, -2));
1497       movw(Address(rsp, cnt2, scale1, stk_offset), result);
1498     }
1499     decrement(cnt2);
1500     jccb(Assembler::notZero, COPY_STR);
1501 
1502     if (int_cnt2 &lt; 0) { // not constant
1503       pop(cnt2);
1504     }
1505     movptr(str1, rsp);  // New string address
1506 
1507     bind(BIG_STRINGS);
1508     // Load substring.
1509     if (int_cnt2 &lt; 0) { // -1
1510       if (ae == StrIntrinsicNode::UL) {
1511         pmovzxbw(vec, Address(str2, 0));
1512       } else {
1513         movdqu(vec, Address(str2, 0));
1514       }
1515       push(cnt2);       // substr count
1516       push(str2);       // substr addr
1517       push(str1);       // string addr
1518     } else {
1519       // Small (&lt; 8 chars) constant substrings are loaded already.
1520       movl(cnt2, int_cnt2);
1521     }
1522     push(tmp);  // original SP
1523 
1524   } // Finished loading
1525 
1526   //========================================================
1527   // Start search
1528   //
1529 
1530   movptr(result, str1); // string addr
1531 
1532   if (int_cnt2  &lt; 0) {  // Only for non constant substring
1533     jmpb(SCAN_TO_SUBSTR);
1534 
1535     // SP saved at sp+0
1536     // String saved at sp+1*wordSize
1537     // Substr saved at sp+2*wordSize
1538     // Substr count saved at sp+3*wordSize
1539 
1540     // Reload substr for rescan, this code
1541     // is executed only for large substrings (&gt; 8 chars)
1542     bind(RELOAD_SUBSTR);
1543     movptr(str2, Address(rsp, 2*wordSize));
1544     movl(cnt2, Address(rsp, 3*wordSize));
1545     if (ae == StrIntrinsicNode::UL) {
1546       pmovzxbw(vec, Address(str2, 0));
1547     } else {
1548       movdqu(vec, Address(str2, 0));
1549     }
1550     // We came here after the beginning of the substring was
1551     // matched but the rest of it was not so we need to search
1552     // again. Start from the next element after the previous match.
1553     subptr(str1, result); // Restore counter
1554     if (ae == StrIntrinsicNode::UU || ae == StrIntrinsicNode::UL) {
1555       shrl(str1, 1);
1556     }
1557     addl(cnt1, str1);
1558     decrementl(cnt1);   // Shift to next element
1559     cmpl(cnt1, cnt2);
1560     jcc(Assembler::negative, RET_NOT_FOUND);  // Left less then substring
1561 
1562     addptr(result, (1&lt;&lt;scale1));
1563   } // non constant
1564 
1565   // Scan string for start of substr in 16-byte vectors
1566   bind(SCAN_TO_SUBSTR);
1567   assert(cnt1 == rdx &amp;&amp; cnt2 == rax &amp;&amp; tmp == rcx, &quot;pcmpestri&quot;);
1568   pcmpestri(vec, Address(result, 0), mode);
1569   jccb(Assembler::below, FOUND_CANDIDATE);   // CF == 1
1570   subl(cnt1, stride);
1571   jccb(Assembler::lessEqual, RET_NOT_FOUND); // Scanned full string
1572   cmpl(cnt1, cnt2);
1573   jccb(Assembler::negative, RET_NOT_FOUND);  // Left less then substring
1574   addptr(result, 16);
1575 
1576   bind(ADJUST_STR);
1577   cmpl(cnt1, stride); // Do not read beyond string
1578   jccb(Assembler::greaterEqual, SCAN_TO_SUBSTR);
1579   // Back-up string to avoid reading beyond string.
1580   lea(result, Address(result, cnt1, scale1, -16));
1581   movl(cnt1, stride);
1582   jmpb(SCAN_TO_SUBSTR);
1583 
1584   // Found a potential substr
1585   bind(FOUND_CANDIDATE);
1586   // After pcmpestri tmp(rcx) contains matched element index
1587 
1588   // Make sure string is still long enough
1589   subl(cnt1, tmp);
1590   cmpl(cnt1, cnt2);
1591   jccb(Assembler::greaterEqual, FOUND_SUBSTR);
1592   // Left less then substring.
1593 
1594   bind(RET_NOT_FOUND);
1595   movl(result, -1);
1596   jmp(CLEANUP);
1597 
1598   bind(FOUND_SUBSTR);
1599   // Compute start addr of substr
1600   lea(result, Address(result, tmp, scale1));
1601   if (int_cnt2 &gt; 0) { // Constant substring
1602     // Repeat search for small substring (&lt; 8 chars)
1603     // from new point without reloading substring.
1604     // Have to check that we don&#39;t read beyond string.
1605     cmpl(tmp, stride-int_cnt2);
1606     jccb(Assembler::greater, ADJUST_STR);
1607     // Fall through if matched whole substring.
1608   } else { // non constant
1609     assert(int_cnt2 == -1, &quot;should be != 0&quot;);
1610 
1611     addl(tmp, cnt2);
1612     // Found result if we matched whole substring.
1613     cmpl(tmp, stride);
1614     jcc(Assembler::lessEqual, RET_FOUND);
1615 
1616     // Repeat search for small substring (&lt;= 8 chars)
1617     // from new point &#39;str1&#39; without reloading substring.
1618     cmpl(cnt2, stride);
1619     // Have to check that we don&#39;t read beyond string.
1620     jccb(Assembler::lessEqual, ADJUST_STR);
1621 
1622     Label CHECK_NEXT, CONT_SCAN_SUBSTR, RET_FOUND_LONG;
1623     // Compare the rest of substring (&gt; 8 chars).
1624     movptr(str1, result);
1625 
1626     cmpl(tmp, cnt2);
1627     // First 8 chars are already matched.
1628     jccb(Assembler::equal, CHECK_NEXT);
1629 
1630     bind(SCAN_SUBSTR);
1631     pcmpestri(vec, Address(str1, 0), mode);
1632     // Need to reload strings pointers if not matched whole vector
1633     jcc(Assembler::noOverflow, RELOAD_SUBSTR); // OF == 0
1634 
1635     bind(CHECK_NEXT);
1636     subl(cnt2, stride);
1637     jccb(Assembler::lessEqual, RET_FOUND_LONG); // Found full substring
1638     addptr(str1, 16);
1639     if (ae == StrIntrinsicNode::UL) {
1640       addptr(str2, 8);
1641     } else {
1642       addptr(str2, 16);
1643     }
1644     subl(cnt1, stride);
1645     cmpl(cnt2, stride); // Do not read beyond substring
1646     jccb(Assembler::greaterEqual, CONT_SCAN_SUBSTR);
1647     // Back-up strings to avoid reading beyond substring.
1648 
1649     if (ae == StrIntrinsicNode::UL) {
1650       lea(str2, Address(str2, cnt2, scale2, -8));
1651       lea(str1, Address(str1, cnt2, scale1, -16));
1652     } else {
1653       lea(str2, Address(str2, cnt2, scale2, -16));
1654       lea(str1, Address(str1, cnt2, scale1, -16));
1655     }
1656     subl(cnt1, cnt2);
1657     movl(cnt2, stride);
1658     addl(cnt1, stride);
1659     bind(CONT_SCAN_SUBSTR);
1660     if (ae == StrIntrinsicNode::UL) {
1661       pmovzxbw(vec, Address(str2, 0));
1662     } else {
1663       movdqu(vec, Address(str2, 0));
1664     }
1665     jmp(SCAN_SUBSTR);
1666 
1667     bind(RET_FOUND_LONG);
1668     movptr(str1, Address(rsp, wordSize));
1669   } // non constant
1670 
1671   bind(RET_FOUND);
1672   // Compute substr offset
1673   subptr(result, str1);
1674   if (ae == StrIntrinsicNode::UU || ae == StrIntrinsicNode::UL) {
1675     shrl(result, 1); // index
1676   }
1677   bind(CLEANUP);
1678   pop(rsp); // restore SP
1679 
1680 } // string_indexof
1681 
1682 void C2_MacroAssembler::string_indexof_char(Register str1, Register cnt1, Register ch, Register result,
1683                                             XMMRegister vec1, XMMRegister vec2, XMMRegister vec3, Register tmp) {
1684   ShortBranchVerifier sbv(this);
1685   assert(UseSSE42Intrinsics, &quot;SSE4.2 intrinsics are required&quot;);
1686 
1687   int stride = 8;
1688 
1689   Label FOUND_CHAR, SCAN_TO_CHAR, SCAN_TO_CHAR_LOOP,
1690         SCAN_TO_8_CHAR, SCAN_TO_8_CHAR_LOOP, SCAN_TO_16_CHAR_LOOP,
1691         RET_NOT_FOUND, SCAN_TO_8_CHAR_INIT,
1692         FOUND_SEQ_CHAR, DONE_LABEL;
1693 
1694   movptr(result, str1);
1695   if (UseAVX &gt;= 2) {
1696     cmpl(cnt1, stride);
1697     jcc(Assembler::less, SCAN_TO_CHAR);
1698     cmpl(cnt1, 2*stride);
1699     jcc(Assembler::less, SCAN_TO_8_CHAR_INIT);
1700     movdl(vec1, ch);
1701     vpbroadcastw(vec1, vec1, Assembler::AVX_256bit);
1702     vpxor(vec2, vec2);
1703     movl(tmp, cnt1);
1704     andl(tmp, 0xFFFFFFF0);  //vector count (in chars)
1705     andl(cnt1,0x0000000F);  //tail count (in chars)
1706 
1707     bind(SCAN_TO_16_CHAR_LOOP);
1708     vmovdqu(vec3, Address(result, 0));
1709     vpcmpeqw(vec3, vec3, vec1, 1);
1710     vptest(vec2, vec3);
1711     jcc(Assembler::carryClear, FOUND_CHAR);
1712     addptr(result, 32);
1713     subl(tmp, 2*stride);
1714     jcc(Assembler::notZero, SCAN_TO_16_CHAR_LOOP);
1715     jmp(SCAN_TO_8_CHAR);
1716     bind(SCAN_TO_8_CHAR_INIT);
1717     movdl(vec1, ch);
1718     pshuflw(vec1, vec1, 0x00);
1719     pshufd(vec1, vec1, 0);
1720     pxor(vec2, vec2);
1721   }
1722   bind(SCAN_TO_8_CHAR);
1723   cmpl(cnt1, stride);
1724   jcc(Assembler::less, SCAN_TO_CHAR);
1725   if (UseAVX &lt; 2) {
1726     movdl(vec1, ch);
1727     pshuflw(vec1, vec1, 0x00);
1728     pshufd(vec1, vec1, 0);
1729     pxor(vec2, vec2);
1730   }
1731   movl(tmp, cnt1);
1732   andl(tmp, 0xFFFFFFF8);  //vector count (in chars)
1733   andl(cnt1,0x00000007);  //tail count (in chars)
1734 
1735   bind(SCAN_TO_8_CHAR_LOOP);
1736   movdqu(vec3, Address(result, 0));
1737   pcmpeqw(vec3, vec1);
1738   ptest(vec2, vec3);
1739   jcc(Assembler::carryClear, FOUND_CHAR);
1740   addptr(result, 16);
1741   subl(tmp, stride);
1742   jcc(Assembler::notZero, SCAN_TO_8_CHAR_LOOP);
1743   bind(SCAN_TO_CHAR);
1744   testl(cnt1, cnt1);
1745   jcc(Assembler::zero, RET_NOT_FOUND);
1746   bind(SCAN_TO_CHAR_LOOP);
1747   load_unsigned_short(tmp, Address(result, 0));
1748   cmpl(ch, tmp);
1749   jccb(Assembler::equal, FOUND_SEQ_CHAR);
1750   addptr(result, 2);
1751   subl(cnt1, 1);
1752   jccb(Assembler::zero, RET_NOT_FOUND);
1753   jmp(SCAN_TO_CHAR_LOOP);
1754 
1755   bind(RET_NOT_FOUND);
1756   movl(result, -1);
1757   jmpb(DONE_LABEL);
1758 
1759   bind(FOUND_CHAR);
1760   if (UseAVX &gt;= 2) {
1761     vpmovmskb(tmp, vec3);
1762   } else {
1763     pmovmskb(tmp, vec3);
1764   }
1765   bsfl(ch, tmp);
1766   addl(result, ch);
1767 
1768   bind(FOUND_SEQ_CHAR);
1769   subptr(result, str1);
1770   shrl(result, 1);
1771 
1772   bind(DONE_LABEL);
1773 } // string_indexof_char
1774 
1775 // helper function for string_compare
1776 void C2_MacroAssembler::load_next_elements(Register elem1, Register elem2, Register str1, Register str2,
1777                                            Address::ScaleFactor scale, Address::ScaleFactor scale1,
1778                                            Address::ScaleFactor scale2, Register index, int ae) {
1779   if (ae == StrIntrinsicNode::LL) {
1780     load_unsigned_byte(elem1, Address(str1, index, scale, 0));
1781     load_unsigned_byte(elem2, Address(str2, index, scale, 0));
1782   } else if (ae == StrIntrinsicNode::UU) {
1783     load_unsigned_short(elem1, Address(str1, index, scale, 0));
1784     load_unsigned_short(elem2, Address(str2, index, scale, 0));
1785   } else {
1786     load_unsigned_byte(elem1, Address(str1, index, scale1, 0));
1787     load_unsigned_short(elem2, Address(str2, index, scale2, 0));
1788   }
1789 }
1790 
1791 // Compare strings, used for char[] and byte[].
1792 void C2_MacroAssembler::string_compare(Register str1, Register str2,
1793                                        Register cnt1, Register cnt2, Register result,
1794                                        XMMRegister vec1, int ae) {
1795   ShortBranchVerifier sbv(this);
1796   Label LENGTH_DIFF_LABEL, POP_LABEL, DONE_LABEL, WHILE_HEAD_LABEL;
1797   Label COMPARE_WIDE_VECTORS_LOOP_FAILED;  // used only _LP64 &amp;&amp; AVX3
1798   int stride, stride2, adr_stride, adr_stride1, adr_stride2;
1799   int stride2x2 = 0x40;
1800   Address::ScaleFactor scale = Address::no_scale;
1801   Address::ScaleFactor scale1 = Address::no_scale;
1802   Address::ScaleFactor scale2 = Address::no_scale;
1803 
1804   if (ae != StrIntrinsicNode::LL) {
1805     stride2x2 = 0x20;
1806   }
1807 
1808   if (ae == StrIntrinsicNode::LU || ae == StrIntrinsicNode::UL) {
1809     shrl(cnt2, 1);
1810   }
1811   // Compute the minimum of the string lengths and the
1812   // difference of the string lengths (stack).
1813   // Do the conditional move stuff
1814   movl(result, cnt1);
1815   subl(cnt1, cnt2);
1816   push(cnt1);
1817   cmov32(Assembler::lessEqual, cnt2, result);    // cnt2 = min(cnt1, cnt2)
1818 
1819   // Is the minimum length zero?
1820   testl(cnt2, cnt2);
1821   jcc(Assembler::zero, LENGTH_DIFF_LABEL);
1822   if (ae == StrIntrinsicNode::LL) {
1823     // Load first bytes
1824     load_unsigned_byte(result, Address(str1, 0));  // result = str1[0]
1825     load_unsigned_byte(cnt1, Address(str2, 0));    // cnt1   = str2[0]
1826   } else if (ae == StrIntrinsicNode::UU) {
1827     // Load first characters
1828     load_unsigned_short(result, Address(str1, 0));
1829     load_unsigned_short(cnt1, Address(str2, 0));
1830   } else {
1831     load_unsigned_byte(result, Address(str1, 0));
1832     load_unsigned_short(cnt1, Address(str2, 0));
1833   }
1834   subl(result, cnt1);
1835   jcc(Assembler::notZero,  POP_LABEL);
1836 
1837   if (ae == StrIntrinsicNode::UU) {
1838     // Divide length by 2 to get number of chars
1839     shrl(cnt2, 1);
1840   }
1841   cmpl(cnt2, 1);
1842   jcc(Assembler::equal, LENGTH_DIFF_LABEL);
1843 
1844   // Check if the strings start at the same location and setup scale and stride
1845   if (ae == StrIntrinsicNode::LL || ae == StrIntrinsicNode::UU) {
1846     cmpptr(str1, str2);
1847     jcc(Assembler::equal, LENGTH_DIFF_LABEL);
1848     if (ae == StrIntrinsicNode::LL) {
1849       scale = Address::times_1;
1850       stride = 16;
1851     } else {
1852       scale = Address::times_2;
1853       stride = 8;
1854     }
1855   } else {
1856     scale1 = Address::times_1;
1857     scale2 = Address::times_2;
1858     // scale not used
1859     stride = 8;
1860   }
1861 
1862   if (UseAVX &gt;= 2 &amp;&amp; UseSSE42Intrinsics) {
1863     Label COMPARE_WIDE_VECTORS, VECTOR_NOT_EQUAL, COMPARE_WIDE_TAIL, COMPARE_SMALL_STR;
1864     Label COMPARE_WIDE_VECTORS_LOOP, COMPARE_16_CHARS, COMPARE_INDEX_CHAR;
1865     Label COMPARE_WIDE_VECTORS_LOOP_AVX2;
1866     Label COMPARE_TAIL_LONG;
1867     Label COMPARE_WIDE_VECTORS_LOOP_AVX3;  // used only _LP64 &amp;&amp; AVX3
1868 
1869     int pcmpmask = 0x19;
1870     if (ae == StrIntrinsicNode::LL) {
1871       pcmpmask &amp;= ~0x01;
1872     }
1873 
1874     // Setup to compare 16-chars (32-bytes) vectors,
1875     // start from first character again because it has aligned address.
1876     if (ae == StrIntrinsicNode::LL) {
1877       stride2 = 32;
1878     } else {
1879       stride2 = 16;
1880     }
1881     if (ae == StrIntrinsicNode::LL || ae == StrIntrinsicNode::UU) {
1882       adr_stride = stride &lt;&lt; scale;
1883     } else {
1884       adr_stride1 = 8;  //stride &lt;&lt; scale1;
1885       adr_stride2 = 16; //stride &lt;&lt; scale2;
1886     }
1887 
1888     assert(result == rax &amp;&amp; cnt2 == rdx &amp;&amp; cnt1 == rcx, &quot;pcmpestri&quot;);
1889     // rax and rdx are used by pcmpestri as elements counters
1890     movl(result, cnt2);
1891     andl(cnt2, ~(stride2-1));   // cnt2 holds the vector count
1892     jcc(Assembler::zero, COMPARE_TAIL_LONG);
1893 
1894     // fast path : compare first 2 8-char vectors.
1895     bind(COMPARE_16_CHARS);
1896     if (ae == StrIntrinsicNode::LL || ae == StrIntrinsicNode::UU) {
1897       movdqu(vec1, Address(str1, 0));
1898     } else {
1899       pmovzxbw(vec1, Address(str1, 0));
1900     }
1901     pcmpestri(vec1, Address(str2, 0), pcmpmask);
1902     jccb(Assembler::below, COMPARE_INDEX_CHAR);
1903 
1904     if (ae == StrIntrinsicNode::LL || ae == StrIntrinsicNode::UU) {
1905       movdqu(vec1, Address(str1, adr_stride));
1906       pcmpestri(vec1, Address(str2, adr_stride), pcmpmask);
1907     } else {
1908       pmovzxbw(vec1, Address(str1, adr_stride1));
1909       pcmpestri(vec1, Address(str2, adr_stride2), pcmpmask);
1910     }
1911     jccb(Assembler::aboveEqual, COMPARE_WIDE_VECTORS);
1912     addl(cnt1, stride);
1913 
1914     // Compare the characters at index in cnt1
1915     bind(COMPARE_INDEX_CHAR); // cnt1 has the offset of the mismatching character
1916     load_next_elements(result, cnt2, str1, str2, scale, scale1, scale2, cnt1, ae);
1917     subl(result, cnt2);
1918     jmp(POP_LABEL);
1919 
1920     // Setup the registers to start vector comparison loop
1921     bind(COMPARE_WIDE_VECTORS);
1922     if (ae == StrIntrinsicNode::LL || ae == StrIntrinsicNode::UU) {
1923       lea(str1, Address(str1, result, scale));
1924       lea(str2, Address(str2, result, scale));
1925     } else {
1926       lea(str1, Address(str1, result, scale1));
1927       lea(str2, Address(str2, result, scale2));
1928     }
1929     subl(result, stride2);
1930     subl(cnt2, stride2);
1931     jcc(Assembler::zero, COMPARE_WIDE_TAIL);
1932     negptr(result);
1933 
1934     //  In a loop, compare 16-chars (32-bytes) at once using (vpxor+vptest)
1935     bind(COMPARE_WIDE_VECTORS_LOOP);
1936 
1937 #ifdef _LP64
1938     if ((AVX3Threshold == 0) &amp;&amp; VM_Version::supports_avx512vlbw()) { // trying 64 bytes fast loop
1939       cmpl(cnt2, stride2x2);
1940       jccb(Assembler::below, COMPARE_WIDE_VECTORS_LOOP_AVX2);
1941       testl(cnt2, stride2x2-1);   // cnt2 holds the vector count
1942       jccb(Assembler::notZero, COMPARE_WIDE_VECTORS_LOOP_AVX2);   // means we cannot subtract by 0x40
1943 
1944       bind(COMPARE_WIDE_VECTORS_LOOP_AVX3); // the hottest loop
1945       if (ae == StrIntrinsicNode::LL || ae == StrIntrinsicNode::UU) {
1946         evmovdquq(vec1, Address(str1, result, scale), Assembler::AVX_512bit);
1947         evpcmpeqb(k7, vec1, Address(str2, result, scale), Assembler::AVX_512bit); // k7 == 11..11, if operands equal, otherwise k7 has some 0
1948       } else {
1949         vpmovzxbw(vec1, Address(str1, result, scale1), Assembler::AVX_512bit);
1950         evpcmpeqb(k7, vec1, Address(str2, result, scale2), Assembler::AVX_512bit); // k7 == 11..11, if operands equal, otherwise k7 has some 0
1951       }
1952       kortestql(k7, k7);
1953       jcc(Assembler::aboveEqual, COMPARE_WIDE_VECTORS_LOOP_FAILED);     // miscompare
1954       addptr(result, stride2x2);  // update since we already compared at this addr
1955       subl(cnt2, stride2x2);      // and sub the size too
1956       jccb(Assembler::notZero, COMPARE_WIDE_VECTORS_LOOP_AVX3);
1957 
1958       vpxor(vec1, vec1);
1959       jmpb(COMPARE_WIDE_TAIL);
1960     }//if (VM_Version::supports_avx512vlbw())
1961 #endif // _LP64
1962 
1963 
1964     bind(COMPARE_WIDE_VECTORS_LOOP_AVX2);
1965     if (ae == StrIntrinsicNode::LL || ae == StrIntrinsicNode::UU) {
1966       vmovdqu(vec1, Address(str1, result, scale));
1967       vpxor(vec1, Address(str2, result, scale));
1968     } else {
1969       vpmovzxbw(vec1, Address(str1, result, scale1), Assembler::AVX_256bit);
1970       vpxor(vec1, Address(str2, result, scale2));
1971     }
1972     vptest(vec1, vec1);
1973     jcc(Assembler::notZero, VECTOR_NOT_EQUAL);
1974     addptr(result, stride2);
1975     subl(cnt2, stride2);
1976     jcc(Assembler::notZero, COMPARE_WIDE_VECTORS_LOOP);
1977     // clean upper bits of YMM registers
1978     vpxor(vec1, vec1);
1979 
1980     // compare wide vectors tail
1981     bind(COMPARE_WIDE_TAIL);
1982     testptr(result, result);
1983     jcc(Assembler::zero, LENGTH_DIFF_LABEL);
1984 
1985     movl(result, stride2);
1986     movl(cnt2, result);
1987     negptr(result);
1988     jmp(COMPARE_WIDE_VECTORS_LOOP_AVX2);
1989 
1990     // Identifies the mismatching (higher or lower)16-bytes in the 32-byte vectors.
1991     bind(VECTOR_NOT_EQUAL);
1992     // clean upper bits of YMM registers
1993     vpxor(vec1, vec1);
1994     if (ae == StrIntrinsicNode::LL || ae == StrIntrinsicNode::UU) {
1995       lea(str1, Address(str1, result, scale));
1996       lea(str2, Address(str2, result, scale));
1997     } else {
1998       lea(str1, Address(str1, result, scale1));
1999       lea(str2, Address(str2, result, scale2));
2000     }
2001     jmp(COMPARE_16_CHARS);
2002 
2003     // Compare tail chars, length between 1 to 15 chars
2004     bind(COMPARE_TAIL_LONG);
2005     movl(cnt2, result);
2006     cmpl(cnt2, stride);
2007     jcc(Assembler::less, COMPARE_SMALL_STR);
2008 
2009     if (ae == StrIntrinsicNode::LL || ae == StrIntrinsicNode::UU) {
2010       movdqu(vec1, Address(str1, 0));
2011     } else {
2012       pmovzxbw(vec1, Address(str1, 0));
2013     }
2014     pcmpestri(vec1, Address(str2, 0), pcmpmask);
2015     jcc(Assembler::below, COMPARE_INDEX_CHAR);
2016     subptr(cnt2, stride);
2017     jcc(Assembler::zero, LENGTH_DIFF_LABEL);
2018     if (ae == StrIntrinsicNode::LL || ae == StrIntrinsicNode::UU) {
2019       lea(str1, Address(str1, result, scale));
2020       lea(str2, Address(str2, result, scale));
2021     } else {
2022       lea(str1, Address(str1, result, scale1));
2023       lea(str2, Address(str2, result, scale2));
2024     }
2025     negptr(cnt2);
2026     jmpb(WHILE_HEAD_LABEL);
2027 
2028     bind(COMPARE_SMALL_STR);
2029   } else if (UseSSE42Intrinsics) {
2030     Label COMPARE_WIDE_VECTORS, VECTOR_NOT_EQUAL, COMPARE_TAIL;
2031     int pcmpmask = 0x19;
2032     // Setup to compare 8-char (16-byte) vectors,
2033     // start from first character again because it has aligned address.
2034     movl(result, cnt2);
2035     andl(cnt2, ~(stride - 1));   // cnt2 holds the vector count
2036     if (ae == StrIntrinsicNode::LL) {
2037       pcmpmask &amp;= ~0x01;
2038     }
2039     jcc(Assembler::zero, COMPARE_TAIL);
2040     if (ae == StrIntrinsicNode::LL || ae == StrIntrinsicNode::UU) {
2041       lea(str1, Address(str1, result, scale));
2042       lea(str2, Address(str2, result, scale));
2043     } else {
2044       lea(str1, Address(str1, result, scale1));
2045       lea(str2, Address(str2, result, scale2));
2046     }
2047     negptr(result);
2048 
2049     // pcmpestri
2050     //   inputs:
2051     //     vec1- substring
2052     //     rax - negative string length (elements count)
2053     //     mem - scanned string
2054     //     rdx - string length (elements count)
2055     //     pcmpmask - cmp mode: 11000 (string compare with negated result)
2056     //               + 00 (unsigned bytes) or  + 01 (unsigned shorts)
2057     //   outputs:
2058     //     rcx - first mismatched element index
2059     assert(result == rax &amp;&amp; cnt2 == rdx &amp;&amp; cnt1 == rcx, &quot;pcmpestri&quot;);
2060 
2061     bind(COMPARE_WIDE_VECTORS);
2062     if (ae == StrIntrinsicNode::LL || ae == StrIntrinsicNode::UU) {
2063       movdqu(vec1, Address(str1, result, scale));
2064       pcmpestri(vec1, Address(str2, result, scale), pcmpmask);
2065     } else {
2066       pmovzxbw(vec1, Address(str1, result, scale1));
2067       pcmpestri(vec1, Address(str2, result, scale2), pcmpmask);
2068     }
2069     // After pcmpestri cnt1(rcx) contains mismatched element index
2070 
2071     jccb(Assembler::below, VECTOR_NOT_EQUAL);  // CF==1
2072     addptr(result, stride);
2073     subptr(cnt2, stride);
2074     jccb(Assembler::notZero, COMPARE_WIDE_VECTORS);
2075 
2076     // compare wide vectors tail
2077     testptr(result, result);
2078     jcc(Assembler::zero, LENGTH_DIFF_LABEL);
2079 
2080     movl(cnt2, stride);
2081     movl(result, stride);
2082     negptr(result);
2083     if (ae == StrIntrinsicNode::LL || ae == StrIntrinsicNode::UU) {
2084       movdqu(vec1, Address(str1, result, scale));
2085       pcmpestri(vec1, Address(str2, result, scale), pcmpmask);
2086     } else {
2087       pmovzxbw(vec1, Address(str1, result, scale1));
2088       pcmpestri(vec1, Address(str2, result, scale2), pcmpmask);
2089     }
2090     jccb(Assembler::aboveEqual, LENGTH_DIFF_LABEL);
2091 
2092     // Mismatched characters in the vectors
2093     bind(VECTOR_NOT_EQUAL);
2094     addptr(cnt1, result);
2095     load_next_elements(result, cnt2, str1, str2, scale, scale1, scale2, cnt1, ae);
2096     subl(result, cnt2);
2097     jmpb(POP_LABEL);
2098 
2099     bind(COMPARE_TAIL); // limit is zero
2100     movl(cnt2, result);
2101     // Fallthru to tail compare
2102   }
2103   // Shift str2 and str1 to the end of the arrays, negate min
2104   if (ae == StrIntrinsicNode::LL || ae == StrIntrinsicNode::UU) {
2105     lea(str1, Address(str1, cnt2, scale));
2106     lea(str2, Address(str2, cnt2, scale));
2107   } else {
2108     lea(str1, Address(str1, cnt2, scale1));
2109     lea(str2, Address(str2, cnt2, scale2));
2110   }
2111   decrementl(cnt2);  // first character was compared already
2112   negptr(cnt2);
2113 
2114   // Compare the rest of the elements
2115   bind(WHILE_HEAD_LABEL);
2116   load_next_elements(result, cnt1, str1, str2, scale, scale1, scale2, cnt2, ae);
2117   subl(result, cnt1);
2118   jccb(Assembler::notZero, POP_LABEL);
2119   increment(cnt2);
2120   jccb(Assembler::notZero, WHILE_HEAD_LABEL);
2121 
2122   // Strings are equal up to min length.  Return the length difference.
2123   bind(LENGTH_DIFF_LABEL);
2124   pop(result);
2125   if (ae == StrIntrinsicNode::UU) {
2126     // Divide diff by 2 to get number of chars
2127     sarl(result, 1);
2128   }
2129   jmpb(DONE_LABEL);
2130 
2131 #ifdef _LP64
2132   if (VM_Version::supports_avx512vlbw()) {
2133 
2134     bind(COMPARE_WIDE_VECTORS_LOOP_FAILED);
2135 
2136     kmovql(cnt1, k7);
2137     notq(cnt1);
2138     bsfq(cnt2, cnt1);
2139     if (ae != StrIntrinsicNode::LL) {
2140       // Divide diff by 2 to get number of chars
2141       sarl(cnt2, 1);
2142     }
2143     addq(result, cnt2);
2144     if (ae == StrIntrinsicNode::LL) {
2145       load_unsigned_byte(cnt1, Address(str2, result));
2146       load_unsigned_byte(result, Address(str1, result));
2147     } else if (ae == StrIntrinsicNode::UU) {
2148       load_unsigned_short(cnt1, Address(str2, result, scale));
2149       load_unsigned_short(result, Address(str1, result, scale));
2150     } else {
2151       load_unsigned_short(cnt1, Address(str2, result, scale2));
2152       load_unsigned_byte(result, Address(str1, result, scale1));
2153     }
2154     subl(result, cnt1);
2155     jmpb(POP_LABEL);
2156   }//if (VM_Version::supports_avx512vlbw())
2157 #endif // _LP64
2158 
2159   // Discard the stored length difference
2160   bind(POP_LABEL);
2161   pop(cnt1);
2162 
2163   // That&#39;s it
2164   bind(DONE_LABEL);
2165   if(ae == StrIntrinsicNode::UL) {
2166     negl(result);
2167   }
2168 
2169 }
2170 
2171 // Search for Non-ASCII character (Negative byte value) in a byte array,
2172 // return true if it has any and false otherwise.
2173 //   ..\jdk\src\java.base\share\classes\java\lang\StringCoding.java
2174 //   @HotSpotIntrinsicCandidate
2175 //   private static boolean hasNegatives(byte[] ba, int off, int len) {
2176 //     for (int i = off; i &lt; off + len; i++) {
2177 //       if (ba[i] &lt; 0) {
2178 //         return true;
2179 //       }
2180 //     }
2181 //     return false;
2182 //   }
2183 void C2_MacroAssembler::has_negatives(Register ary1, Register len,
2184   Register result, Register tmp1,
2185   XMMRegister vec1, XMMRegister vec2) {
2186   // rsi: byte array
2187   // rcx: len
2188   // rax: result
2189   ShortBranchVerifier sbv(this);
2190   assert_different_registers(ary1, len, result, tmp1);
2191   assert_different_registers(vec1, vec2);
2192   Label TRUE_LABEL, FALSE_LABEL, DONE, COMPARE_CHAR, COMPARE_VECTORS, COMPARE_BYTE;
2193 
2194   // len == 0
2195   testl(len, len);
2196   jcc(Assembler::zero, FALSE_LABEL);
2197 
2198   if ((AVX3Threshold == 0) &amp;&amp; (UseAVX &gt; 2) &amp;&amp; // AVX512
2199     VM_Version::supports_avx512vlbw() &amp;&amp;
2200     VM_Version::supports_bmi2()) {
2201 
2202     Label test_64_loop, test_tail;
2203     Register tmp3_aliased = len;
2204 
2205     movl(tmp1, len);
2206     vpxor(vec2, vec2, vec2, Assembler::AVX_512bit);
2207 
2208     andl(tmp1, 64 - 1);   // tail count (in chars) 0x3F
2209     andl(len, ~(64 - 1));    // vector count (in chars)
2210     jccb(Assembler::zero, test_tail);
2211 
2212     lea(ary1, Address(ary1, len, Address::times_1));
2213     negptr(len);
2214 
2215     bind(test_64_loop);
2216     // Check whether our 64 elements of size byte contain negatives
2217     evpcmpgtb(k2, vec2, Address(ary1, len, Address::times_1), Assembler::AVX_512bit);
2218     kortestql(k2, k2);
2219     jcc(Assembler::notZero, TRUE_LABEL);
2220 
2221     addptr(len, 64);
2222     jccb(Assembler::notZero, test_64_loop);
2223 
2224 
2225     bind(test_tail);
2226     // bail out when there is nothing to be done
2227     testl(tmp1, -1);
2228     jcc(Assembler::zero, FALSE_LABEL);
2229 
2230     // ~(~0 &lt;&lt; len) applied up to two times (for 32-bit scenario)
2231 #ifdef _LP64
2232     mov64(tmp3_aliased, 0xFFFFFFFFFFFFFFFF);
2233     shlxq(tmp3_aliased, tmp3_aliased, tmp1);
2234     notq(tmp3_aliased);
2235     kmovql(k3, tmp3_aliased);
2236 #else
2237     Label k_init;
2238     jmp(k_init);
2239 
2240     // We could not read 64-bits from a general purpose register thus we move
2241     // data required to compose 64 1&#39;s to the instruction stream
2242     // We emit 64 byte wide series of elements from 0..63 which later on would
2243     // be used as a compare targets with tail count contained in tmp1 register.
2244     // Result would be a k register having tmp1 consecutive number or 1
2245     // counting from least significant bit.
2246     address tmp = pc();
2247     emit_int64(0x0706050403020100);
2248     emit_int64(0x0F0E0D0C0B0A0908);
2249     emit_int64(0x1716151413121110);
2250     emit_int64(0x1F1E1D1C1B1A1918);
2251     emit_int64(0x2726252423222120);
2252     emit_int64(0x2F2E2D2C2B2A2928);
2253     emit_int64(0x3736353433323130);
2254     emit_int64(0x3F3E3D3C3B3A3938);
2255 
2256     bind(k_init);
2257     lea(len, InternalAddress(tmp));
2258     // create mask to test for negative byte inside a vector
2259     evpbroadcastb(vec1, tmp1, Assembler::AVX_512bit);
2260     evpcmpgtb(k3, vec1, Address(len, 0), Assembler::AVX_512bit);
2261 
2262 #endif
2263     evpcmpgtb(k2, k3, vec2, Address(ary1, 0), Assembler::AVX_512bit);
2264     ktestq(k2, k3);
2265     jcc(Assembler::notZero, TRUE_LABEL);
2266 
2267     jmp(FALSE_LABEL);
2268   } else {
2269     movl(result, len); // copy
2270 
2271     if (UseAVX &gt;= 2 &amp;&amp; UseSSE &gt;= 2) {
2272       // With AVX2, use 32-byte vector compare
2273       Label COMPARE_WIDE_VECTORS, COMPARE_TAIL;
2274 
2275       // Compare 32-byte vectors
2276       andl(result, 0x0000001f);  //   tail count (in bytes)
2277       andl(len, 0xffffffe0);   // vector count (in bytes)
2278       jccb(Assembler::zero, COMPARE_TAIL);
2279 
2280       lea(ary1, Address(ary1, len, Address::times_1));
2281       negptr(len);
2282 
2283       movl(tmp1, 0x80808080);   // create mask to test for Unicode chars in vector
2284       movdl(vec2, tmp1);
2285       vpbroadcastd(vec2, vec2, Assembler::AVX_256bit);
2286 
2287       bind(COMPARE_WIDE_VECTORS);
2288       vmovdqu(vec1, Address(ary1, len, Address::times_1));
2289       vptest(vec1, vec2);
2290       jccb(Assembler::notZero, TRUE_LABEL);
2291       addptr(len, 32);
2292       jcc(Assembler::notZero, COMPARE_WIDE_VECTORS);
2293 
2294       testl(result, result);
2295       jccb(Assembler::zero, FALSE_LABEL);
2296 
2297       vmovdqu(vec1, Address(ary1, result, Address::times_1, -32));
2298       vptest(vec1, vec2);
2299       jccb(Assembler::notZero, TRUE_LABEL);
2300       jmpb(FALSE_LABEL);
2301 
2302       bind(COMPARE_TAIL); // len is zero
2303       movl(len, result);
2304       // Fallthru to tail compare
2305     } else if (UseSSE42Intrinsics) {
2306       // With SSE4.2, use double quad vector compare
2307       Label COMPARE_WIDE_VECTORS, COMPARE_TAIL;
2308 
2309       // Compare 16-byte vectors
2310       andl(result, 0x0000000f);  //   tail count (in bytes)
2311       andl(len, 0xfffffff0);   // vector count (in bytes)
2312       jcc(Assembler::zero, COMPARE_TAIL);
2313 
2314       lea(ary1, Address(ary1, len, Address::times_1));
2315       negptr(len);
2316 
2317       movl(tmp1, 0x80808080);
2318       movdl(vec2, tmp1);
2319       pshufd(vec2, vec2, 0);
2320 
2321       bind(COMPARE_WIDE_VECTORS);
2322       movdqu(vec1, Address(ary1, len, Address::times_1));
2323       ptest(vec1, vec2);
2324       jcc(Assembler::notZero, TRUE_LABEL);
2325       addptr(len, 16);
2326       jcc(Assembler::notZero, COMPARE_WIDE_VECTORS);
2327 
2328       testl(result, result);
2329       jcc(Assembler::zero, FALSE_LABEL);
2330 
2331       movdqu(vec1, Address(ary1, result, Address::times_1, -16));
2332       ptest(vec1, vec2);
2333       jccb(Assembler::notZero, TRUE_LABEL);
2334       jmpb(FALSE_LABEL);
2335 
2336       bind(COMPARE_TAIL); // len is zero
2337       movl(len, result);
2338       // Fallthru to tail compare
2339     }
2340   }
2341   // Compare 4-byte vectors
2342   andl(len, 0xfffffffc); // vector count (in bytes)
2343   jccb(Assembler::zero, COMPARE_CHAR);
2344 
2345   lea(ary1, Address(ary1, len, Address::times_1));
2346   negptr(len);
2347 
2348   bind(COMPARE_VECTORS);
2349   movl(tmp1, Address(ary1, len, Address::times_1));
2350   andl(tmp1, 0x80808080);
2351   jccb(Assembler::notZero, TRUE_LABEL);
2352   addptr(len, 4);
2353   jcc(Assembler::notZero, COMPARE_VECTORS);
2354 
2355   // Compare trailing char (final 2 bytes), if any
2356   bind(COMPARE_CHAR);
2357   testl(result, 0x2);   // tail  char
2358   jccb(Assembler::zero, COMPARE_BYTE);
2359   load_unsigned_short(tmp1, Address(ary1, 0));
2360   andl(tmp1, 0x00008080);
2361   jccb(Assembler::notZero, TRUE_LABEL);
2362   subptr(result, 2);
2363   lea(ary1, Address(ary1, 2));
2364 
2365   bind(COMPARE_BYTE);
2366   testl(result, 0x1);   // tail  byte
2367   jccb(Assembler::zero, FALSE_LABEL);
2368   load_unsigned_byte(tmp1, Address(ary1, 0));
2369   andl(tmp1, 0x00000080);
2370   jccb(Assembler::notEqual, TRUE_LABEL);
2371   jmpb(FALSE_LABEL);
2372 
2373   bind(TRUE_LABEL);
2374   movl(result, 1);   // return true
2375   jmpb(DONE);
2376 
2377   bind(FALSE_LABEL);
2378   xorl(result, result); // return false
2379 
2380   // That&#39;s it
2381   bind(DONE);
2382   if (UseAVX &gt;= 2 &amp;&amp; UseSSE &gt;= 2) {
2383     // clean upper bits of YMM registers
2384     vpxor(vec1, vec1);
2385     vpxor(vec2, vec2);
2386   }
2387 }
2388 // Compare char[] or byte[] arrays aligned to 4 bytes or substrings.
2389 void C2_MacroAssembler::arrays_equals(bool is_array_equ, Register ary1, Register ary2,
2390                                       Register limit, Register result, Register chr,
2391                                       XMMRegister vec1, XMMRegister vec2, bool is_char) {
2392   ShortBranchVerifier sbv(this);
2393   Label TRUE_LABEL, FALSE_LABEL, DONE, COMPARE_VECTORS, COMPARE_CHAR, COMPARE_BYTE;
2394 
2395   int length_offset  = arrayOopDesc::length_offset_in_bytes();
2396   int base_offset    = arrayOopDesc::base_offset_in_bytes(is_char ? T_CHAR : T_BYTE);
2397 
2398   if (is_array_equ) {
2399     // Check the input args
2400     cmpoop(ary1, ary2);
2401     jcc(Assembler::equal, TRUE_LABEL);
2402 
2403     // Need additional checks for arrays_equals.
2404     testptr(ary1, ary1);
2405     jcc(Assembler::zero, FALSE_LABEL);
2406     testptr(ary2, ary2);
2407     jcc(Assembler::zero, FALSE_LABEL);
2408 
2409     // Check the lengths
2410     movl(limit, Address(ary1, length_offset));
2411     cmpl(limit, Address(ary2, length_offset));
2412     jcc(Assembler::notEqual, FALSE_LABEL);
2413   }
2414 
2415   // count == 0
2416   testl(limit, limit);
2417   jcc(Assembler::zero, TRUE_LABEL);
2418 
2419   if (is_array_equ) {
2420     // Load array address
2421     lea(ary1, Address(ary1, base_offset));
2422     lea(ary2, Address(ary2, base_offset));
2423   }
2424 
2425   if (is_array_equ &amp;&amp; is_char) {
2426     // arrays_equals when used for char[].
2427     shll(limit, 1);      // byte count != 0
2428   }
2429   movl(result, limit); // copy
2430 
2431   if (UseAVX &gt;= 2) {
2432     // With AVX2, use 32-byte vector compare
2433     Label COMPARE_WIDE_VECTORS, COMPARE_TAIL;
2434 
2435     // Compare 32-byte vectors
2436     andl(result, 0x0000001f);  //   tail count (in bytes)
2437     andl(limit, 0xffffffe0);   // vector count (in bytes)
2438     jcc(Assembler::zero, COMPARE_TAIL);
2439 
2440     lea(ary1, Address(ary1, limit, Address::times_1));
2441     lea(ary2, Address(ary2, limit, Address::times_1));
2442     negptr(limit);
2443 
2444 #ifdef _LP64
2445     if ((AVX3Threshold == 0) &amp;&amp; VM_Version::supports_avx512vlbw()) { // trying 64 bytes fast loop
2446       Label COMPARE_WIDE_VECTORS_LOOP_AVX2, COMPARE_WIDE_VECTORS_LOOP_AVX3;
2447 
2448       cmpl(limit, -64);
2449       jcc(Assembler::greater, COMPARE_WIDE_VECTORS_LOOP_AVX2);
2450 
2451       bind(COMPARE_WIDE_VECTORS_LOOP_AVX3); // the hottest loop
2452 
2453       evmovdquq(vec1, Address(ary1, limit, Address::times_1), Assembler::AVX_512bit);
2454       evpcmpeqb(k7, vec1, Address(ary2, limit, Address::times_1), Assembler::AVX_512bit);
2455       kortestql(k7, k7);
2456       jcc(Assembler::aboveEqual, FALSE_LABEL);     // miscompare
2457       addptr(limit, 64);  // update since we already compared at this addr
2458       cmpl(limit, -64);
2459       jccb(Assembler::lessEqual, COMPARE_WIDE_VECTORS_LOOP_AVX3);
2460 
2461       // At this point we may still need to compare -limit+result bytes.
2462       // We could execute the next two instruction and just continue via non-wide path:
2463       //  cmpl(limit, 0);
2464       //  jcc(Assembler::equal, COMPARE_TAIL);  // true
2465       // But since we stopped at the points ary{1,2}+limit which are
2466       // not farther than 64 bytes from the ends of arrays ary{1,2}+result
2467       // (|limit| &lt;= 32 and result &lt; 32),
2468       // we may just compare the last 64 bytes.
2469       //
2470       addptr(result, -64);   // it is safe, bc we just came from this area
2471       evmovdquq(vec1, Address(ary1, result, Address::times_1), Assembler::AVX_512bit);
2472       evpcmpeqb(k7, vec1, Address(ary2, result, Address::times_1), Assembler::AVX_512bit);
2473       kortestql(k7, k7);
2474       jcc(Assembler::aboveEqual, FALSE_LABEL);     // miscompare
2475 
2476       jmp(TRUE_LABEL);
2477 
2478       bind(COMPARE_WIDE_VECTORS_LOOP_AVX2);
2479 
2480     }//if (VM_Version::supports_avx512vlbw())
2481 #endif //_LP64
2482     bind(COMPARE_WIDE_VECTORS);
2483     vmovdqu(vec1, Address(ary1, limit, Address::times_1));
2484     vmovdqu(vec2, Address(ary2, limit, Address::times_1));
2485     vpxor(vec1, vec2);
2486 
2487     vptest(vec1, vec1);
2488     jcc(Assembler::notZero, FALSE_LABEL);
2489     addptr(limit, 32);
2490     jcc(Assembler::notZero, COMPARE_WIDE_VECTORS);
2491 
2492     testl(result, result);
2493     jcc(Assembler::zero, TRUE_LABEL);
2494 
2495     vmovdqu(vec1, Address(ary1, result, Address::times_1, -32));
2496     vmovdqu(vec2, Address(ary2, result, Address::times_1, -32));
2497     vpxor(vec1, vec2);
2498 
2499     vptest(vec1, vec1);
2500     jccb(Assembler::notZero, FALSE_LABEL);
2501     jmpb(TRUE_LABEL);
2502 
2503     bind(COMPARE_TAIL); // limit is zero
2504     movl(limit, result);
2505     // Fallthru to tail compare
2506   } else if (UseSSE42Intrinsics) {
2507     // With SSE4.2, use double quad vector compare
2508     Label COMPARE_WIDE_VECTORS, COMPARE_TAIL;
2509 
2510     // Compare 16-byte vectors
2511     andl(result, 0x0000000f);  //   tail count (in bytes)
2512     andl(limit, 0xfffffff0);   // vector count (in bytes)
2513     jcc(Assembler::zero, COMPARE_TAIL);
2514 
2515     lea(ary1, Address(ary1, limit, Address::times_1));
2516     lea(ary2, Address(ary2, limit, Address::times_1));
2517     negptr(limit);
2518 
2519     bind(COMPARE_WIDE_VECTORS);
2520     movdqu(vec1, Address(ary1, limit, Address::times_1));
2521     movdqu(vec2, Address(ary2, limit, Address::times_1));
2522     pxor(vec1, vec2);
2523 
2524     ptest(vec1, vec1);
2525     jcc(Assembler::notZero, FALSE_LABEL);
2526     addptr(limit, 16);
2527     jcc(Assembler::notZero, COMPARE_WIDE_VECTORS);
2528 
2529     testl(result, result);
2530     jcc(Assembler::zero, TRUE_LABEL);
2531 
2532     movdqu(vec1, Address(ary1, result, Address::times_1, -16));
2533     movdqu(vec2, Address(ary2, result, Address::times_1, -16));
2534     pxor(vec1, vec2);
2535 
2536     ptest(vec1, vec1);
2537     jccb(Assembler::notZero, FALSE_LABEL);
2538     jmpb(TRUE_LABEL);
2539 
2540     bind(COMPARE_TAIL); // limit is zero
2541     movl(limit, result);
2542     // Fallthru to tail compare
2543   }
2544 
2545   // Compare 4-byte vectors
2546   andl(limit, 0xfffffffc); // vector count (in bytes)
2547   jccb(Assembler::zero, COMPARE_CHAR);
2548 
2549   lea(ary1, Address(ary1, limit, Address::times_1));
2550   lea(ary2, Address(ary2, limit, Address::times_1));
2551   negptr(limit);
2552 
2553   bind(COMPARE_VECTORS);
2554   movl(chr, Address(ary1, limit, Address::times_1));
2555   cmpl(chr, Address(ary2, limit, Address::times_1));
2556   jccb(Assembler::notEqual, FALSE_LABEL);
2557   addptr(limit, 4);
2558   jcc(Assembler::notZero, COMPARE_VECTORS);
2559 
2560   // Compare trailing char (final 2 bytes), if any
2561   bind(COMPARE_CHAR);
2562   testl(result, 0x2);   // tail  char
2563   jccb(Assembler::zero, COMPARE_BYTE);
2564   load_unsigned_short(chr, Address(ary1, 0));
2565   load_unsigned_short(limit, Address(ary2, 0));
2566   cmpl(chr, limit);
2567   jccb(Assembler::notEqual, FALSE_LABEL);
2568 
2569   if (is_array_equ &amp;&amp; is_char) {
2570     bind(COMPARE_BYTE);
2571   } else {
2572     lea(ary1, Address(ary1, 2));
2573     lea(ary2, Address(ary2, 2));
2574 
2575     bind(COMPARE_BYTE);
2576     testl(result, 0x1);   // tail  byte
2577     jccb(Assembler::zero, TRUE_LABEL);
2578     load_unsigned_byte(chr, Address(ary1, 0));
2579     load_unsigned_byte(limit, Address(ary2, 0));
2580     cmpl(chr, limit);
2581     jccb(Assembler::notEqual, FALSE_LABEL);
2582   }
2583   bind(TRUE_LABEL);
2584   movl(result, 1);   // return true
2585   jmpb(DONE);
2586 
2587   bind(FALSE_LABEL);
2588   xorl(result, result); // return false
2589 
2590   // That&#39;s it
2591   bind(DONE);
2592   if (UseAVX &gt;= 2) {
2593     // clean upper bits of YMM registers
2594     vpxor(vec1, vec1);
2595     vpxor(vec2, vec2);
2596   }
2597 }
<a name="2" id="anc2"></a><b style="font-size: large; color: red">--- EOF ---</b>
















































































</pre>
<input id="eof" value="2" type="hidden" />
</body>
</html>