<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/cpu/x86/sharedRuntime_x86_32.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="methodHandles_x86.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="sharedRuntime_x86_64.cpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/cpu/x86/sharedRuntime_x86_32.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
 449       else if( freg_arg1 == fltarg_flt_dbl ) freg_arg1 = i;
 450       else // Else double is passed low on the stack to be aligned.
 451         stack += 2;
 452     } else if( sig_bt[i] == T_LONG ) {
 453       stack += 2;
 454     }
 455   }
 456   int dstack = 0;             // Separate counter for placing doubles
 457 
 458   // Now pick where all else goes.
 459   for( i = 0; i &lt; total_args_passed; i++) {
 460     // From the type and the argument number (count) compute the location
 461     switch( sig_bt[i] ) {
 462     case T_SHORT:
 463     case T_CHAR:
 464     case T_BYTE:
 465     case T_BOOLEAN:
 466     case T_INT:
 467     case T_ARRAY:
 468     case T_OBJECT:

 469     case T_ADDRESS:
 470       if( reg_arg0 == 9999 )  {
 471         reg_arg0 = i;
 472         regs[i].set1(rcx-&gt;as_VMReg());
 473       } else if( reg_arg1 == 9999 )  {
 474         reg_arg1 = i;
 475         regs[i].set1(rdx-&gt;as_VMReg());
 476       } else {
 477         regs[i].set1(VMRegImpl::stack2reg(stack++));
 478       }
 479       break;
 480     case T_FLOAT:
 481       if( freg_arg0 == fltarg_flt_dbl || freg_arg0 == fltarg_float_only ) {
 482         freg_arg0 = i;
 483         regs[i].set1(xmm0-&gt;as_VMReg());
 484       } else if( freg_arg1 == fltarg_flt_dbl || freg_arg1 == fltarg_float_only ) {
 485         freg_arg1 = i;
 486         regs[i].set1(xmm1-&gt;as_VMReg());
 487       } else {
 488         regs[i].set1(VMRegImpl::stack2reg(stack++));
</pre>
<hr />
<pre>
 499         regs[i].set2(xmm0-&gt;as_VMReg());
 500       } else if( freg_arg1 == (uint)i ) {
 501         regs[i].set2(xmm1-&gt;as_VMReg());
 502       } else {
 503         regs[i].set2(VMRegImpl::stack2reg(dstack));
 504         dstack += 2;
 505       }
 506       break;
 507     case T_VOID: regs[i].set_bad(); break;
 508       break;
 509     default:
 510       ShouldNotReachHere();
 511       break;
 512     }
 513   }
 514 
 515   // return value can be odd number of VMRegImpl stack slots make multiple of 2
 516   return align_up(stack, 2);
 517 }
 518 









 519 // Patch the callers callsite with entry to compiled code if it exists.
 520 static void patch_callers_callsite(MacroAssembler *masm) {
 521   Label L;
 522   __ cmpptr(Address(rbx, in_bytes(Method::code_offset())), (int32_t)NULL_WORD);
 523   __ jcc(Assembler::equal, L);
 524   // Schedule the branch target address early.
 525   // Call into the VM to patch the caller, then jump to compiled callee
 526   // rax, isn&#39;t live so capture return address while we easily can
 527   __ movptr(rax, Address(rsp, 0));
 528   __ pusha();
 529   __ pushf();
 530 
 531   if (UseSSE == 1) {
 532     __ subptr(rsp, 2*wordSize);
 533     __ movflt(Address(rsp, 0), xmm0);
 534     __ movflt(Address(rsp, wordSize), xmm1);
 535   }
 536   if (UseSSE &gt;= 2) {
 537     __ subptr(rsp, 4*wordSize);
 538     __ movdbl(Address(rsp, 0), xmm0);
</pre>
<hr />
<pre>
 560     __ addptr(rsp, 2*wordSize);
 561   }
 562   if (UseSSE &gt;= 2) {
 563     __ movdbl(xmm0, Address(rsp, 0));
 564     __ movdbl(xmm1, Address(rsp, 2*wordSize));
 565     __ addptr(rsp, 4*wordSize);
 566   }
 567 
 568   __ popf();
 569   __ popa();
 570   __ bind(L);
 571 }
 572 
 573 
 574 static void move_c2i_double(MacroAssembler *masm, XMMRegister r, int st_off) {
 575   int next_off = st_off - Interpreter::stackElementSize;
 576   __ movdbl(Address(rsp, next_off), r);
 577 }
 578 
 579 static void gen_c2i_adapter(MacroAssembler *masm,
<span class="line-modified"> 580                             int total_args_passed,</span>
<span class="line-removed"> 581                             int comp_args_on_stack,</span>
<span class="line-removed"> 582                             const BasicType *sig_bt,</span>
 583                             const VMRegPair *regs,
<span class="line-modified"> 584                             Label&amp; skip_fixup) {</span>




 585   // Before we get into the guts of the C2I adapter, see if we should be here
 586   // at all.  We&#39;ve come from compiled code and are attempting to jump to the
 587   // interpreter, which means the caller made a static call to get here
 588   // (vcalls always get a compiled target if there is one).  Check for a
 589   // compiled target.  If there is one, we need to patch the caller&#39;s call.
 590   patch_callers_callsite(masm);
 591 
 592   __ bind(skip_fixup);
 593 
 594 #ifdef COMPILER2
 595   // C2 may leave the stack dirty if not in SSE2+ mode
 596   if (UseSSE &gt;= 2) {
 597     __ verify_FPU(0, &quot;c2i transition should have clean FPU stack&quot;);
 598   } else {
 599     __ empty_FPU_stack();
 600   }
 601 #endif /* COMPILER2 */
 602 
 603   // Since all args are passed on the stack, total_args_passed * interpreter_
 604   // stack_element_size  is the
 605   // space we need.
<span class="line-modified"> 606   int extraspace = total_args_passed * Interpreter::stackElementSize;</span>
 607 
 608   // Get return address
 609   __ pop(rax);
 610 
 611   // set senderSP value
 612   __ movptr(rsi, rsp);
 613 
 614   __ subptr(rsp, extraspace);
 615 
 616   // Now write the args into the outgoing interpreter space
<span class="line-modified"> 617   for (int i = 0; i &lt; total_args_passed; i++) {</span>
<span class="line-modified"> 618     if (sig_bt[i] == T_VOID) {</span>
<span class="line-modified"> 619       assert(i &gt; 0 &amp;&amp; (sig_bt[i-1] == T_LONG || sig_bt[i-1] == T_DOUBLE), &quot;missing half&quot;);</span>
 620       continue;
 621     }
 622 
 623     // st_off points to lowest address on stack.
<span class="line-modified"> 624     int st_off = ((total_args_passed - 1) - i) * Interpreter::stackElementSize;</span>
 625     int next_off = st_off - Interpreter::stackElementSize;
 626 
 627     // Say 4 args:
 628     // i   st_off
 629     // 0   12 T_LONG
 630     // 1    8 T_VOID
 631     // 2    4 T_OBJECT
 632     // 3    0 T_BOOL
 633     VMReg r_1 = regs[i].first();
 634     VMReg r_2 = regs[i].second();
 635     if (!r_1-&gt;is_valid()) {
 636       assert(!r_2-&gt;is_valid(), &quot;&quot;);
 637       continue;
 638     }
 639 
 640     if (r_1-&gt;is_stack()) {
 641       // memory to memory use fpu stack top
 642       int ld_off = r_1-&gt;reg2stack() * VMRegImpl::stack_slot_size + extraspace;
 643 
 644       if (!r_2-&gt;is_valid()) {
</pre>
<hr />
<pre>
 654 #ifndef _LP64
 655         __ movptr(rdi, Address(rsp, ld_off + wordSize));
 656         __ movptr(Address(rsp, st_off), rdi);
 657 #else
 658 #ifdef ASSERT
 659         // Overwrite the unused slot with known junk
 660         __ mov64(rax, CONST64(0xdeadffffdeadaaaa));
 661         __ movptr(Address(rsp, st_off), rax);
 662 #endif /* ASSERT */
 663 #endif // _LP64
 664       }
 665     } else if (r_1-&gt;is_Register()) {
 666       Register r = r_1-&gt;as_Register();
 667       if (!r_2-&gt;is_valid()) {
 668         __ movl(Address(rsp, st_off), r);
 669       } else {
 670         // long/double in gpr
 671         NOT_LP64(ShouldNotReachHere());
 672         // Two VMRegs can be T_OBJECT, T_ADDRESS, T_DOUBLE, T_LONG
 673         // T_DOUBLE and T_LONG use two slots in the interpreter
<span class="line-modified"> 674         if ( sig_bt[i] == T_LONG || sig_bt[i] == T_DOUBLE) {</span>
 675           // long/double in gpr
 676 #ifdef ASSERT
 677           // Overwrite the unused slot with known junk
 678           LP64_ONLY(__ mov64(rax, CONST64(0xdeadffffdeadaaab)));
 679           __ movptr(Address(rsp, st_off), rax);
 680 #endif /* ASSERT */
 681           __ movptr(Address(rsp, next_off), r);
 682         } else {
 683           __ movptr(Address(rsp, st_off), r);
 684         }
 685       }
 686     } else {
 687       assert(r_1-&gt;is_XMMRegister(), &quot;&quot;);
 688       if (!r_2-&gt;is_valid()) {
 689         __ movflt(Address(rsp, st_off), r_1-&gt;as_XMMRegister());
 690       } else {
<span class="line-modified"> 691         assert(sig_bt[i] == T_DOUBLE || sig_bt[i] == T_LONG, &quot;wrong type&quot;);</span>
 692         move_c2i_double(masm, r_1-&gt;as_XMMRegister(), st_off);
 693       }
 694     }
 695   }
 696 
 697   // Schedule the branch target address early.
 698   __ movptr(rcx, Address(rbx, in_bytes(Method::interpreter_entry_offset())));
 699   // And repush original return address
 700   __ push(rax);
 701   __ jmp(rcx);
 702 }
 703 
 704 
 705 static void move_i2c_double(MacroAssembler *masm, XMMRegister r, Register saved_sp, int ld_off) {
 706   int next_val_off = ld_off - Interpreter::stackElementSize;
 707   __ movdbl(r, Address(saved_sp, next_val_off));
 708 }
 709 
 710 static void range_check(MacroAssembler* masm, Register pc_reg, Register temp_reg,
 711                         address code_start, address code_end,
 712                         Label&amp; L_ok) {
 713   Label L_fail;
 714   __ lea(temp_reg, ExternalAddress(code_start));
 715   __ cmpptr(pc_reg, temp_reg);
 716   __ jcc(Assembler::belowEqual, L_fail);
 717   __ lea(temp_reg, ExternalAddress(code_end));
 718   __ cmpptr(pc_reg, temp_reg);
 719   __ jcc(Assembler::below, L_ok);
 720   __ bind(L_fail);
 721 }
 722 
 723 void SharedRuntime::gen_i2c_adapter(MacroAssembler *masm,
<span class="line-modified"> 724                                     int total_args_passed,</span>
<span class="line-removed"> 725                                     int comp_args_on_stack,</span>
 726                                     const BasicType *sig_bt,
 727                                     const VMRegPair *regs) {

 728   // Note: rsi contains the senderSP on entry. We must preserve it since
 729   // we may do a i2c -&gt; c2i transition if we lose a race where compiled
 730   // code goes non-entrant while we get args ready.
 731 
 732   // Adapters can be frameless because they do not require the caller
 733   // to perform additional cleanup work, such as correcting the stack pointer.
 734   // An i2c adapter is frameless because the *caller* frame, which is interpreted,
 735   // routinely repairs its own stack pointer (from interpreter_frame_last_sp),
 736   // even if a callee has modified the stack pointer.
 737   // A c2i adapter is frameless because the *callee* frame, which is interpreted,
 738   // routinely repairs its caller&#39;s stack pointer (from sender_sp, which is set
 739   // up via the senderSP register).
 740   // In other words, if *either* the caller or callee is interpreted, we can
 741   // get the stack pointer repaired after a call.
 742   // This is why c2i and i2c adapters cannot be indefinitely composed.
 743   // In particular, if a c2i adapter were to somehow call an i2c adapter,
 744   // both caller and callee would be compiled methods, and neither would
 745   // clean up the stack pointer changes performed by the two adapters.
 746   // If this happens, control eventually transfers back to the compiled
 747   // caller, but with an uncorrected stack, causing delayed havoc.
</pre>
<hr />
<pre>
 796   }
 797 
 798   // Align the outgoing SP
 799   __ andptr(rsp, -(StackAlignmentInBytes));
 800 
 801   // push the return address on the stack (note that pushing, rather
 802   // than storing it, yields the correct frame alignment for the callee)
 803   __ push(rax);
 804 
 805   // Put saved SP in another register
 806   const Register saved_sp = rax;
 807   __ movptr(saved_sp, rdi);
 808 
 809 
 810   // Will jump to the compiled code just as if compiled code was doing it.
 811   // Pre-load the register-jump target early, to schedule it better.
 812   __ movptr(rdi, Address(rbx, in_bytes(Method::from_compiled_offset())));
 813 
 814   // Now generate the shuffle code.  Pick up all register args and move the
 815   // rest through the floating point stack top.
<span class="line-modified"> 816   for (int i = 0; i &lt; total_args_passed; i++) {</span>
<span class="line-modified"> 817     if (sig_bt[i] == T_VOID) {</span>
 818       // Longs and doubles are passed in native word order, but misaligned
 819       // in the 32-bit build.
<span class="line-modified"> 820       assert(i &gt; 0 &amp;&amp; (sig_bt[i-1] == T_LONG || sig_bt[i-1] == T_DOUBLE), &quot;missing half&quot;);</span>
 821       continue;
 822     }
 823 
 824     // Pick up 0, 1 or 2 words from SP+offset.
 825 
 826     assert(!regs[i].second()-&gt;is_valid() || regs[i].first()-&gt;next() == regs[i].second(),
 827             &quot;scrambled load targets?&quot;);
 828     // Load in argument order going down.
<span class="line-modified"> 829     int ld_off = (total_args_passed - i) * Interpreter::stackElementSize;</span>
 830     // Point to interpreter value (vs. tag)
 831     int next_off = ld_off - Interpreter::stackElementSize;
 832     //
 833     //
 834     //
 835     VMReg r_1 = regs[i].first();
 836     VMReg r_2 = regs[i].second();
 837     if (!r_1-&gt;is_valid()) {
 838       assert(!r_2-&gt;is_valid(), &quot;&quot;);
 839       continue;
 840     }
 841     if (r_1-&gt;is_stack()) {
 842       // Convert stack slot to an SP offset (+ wordSize to account for return address )
 843       int st_off = regs[i].first()-&gt;reg2stack()*VMRegImpl::stack_slot_size + wordSize;
 844 
 845       // We can use rsi as a temp here because compiled code doesn&#39;t need rsi as an input
 846       // and if we end up going thru a c2i because of a miss a reasonable value of rsi
 847       // we be generated.
 848       if (!r_2-&gt;is_valid()) {
 849         // __ fld_s(Address(saved_sp, ld_off));
 850         // __ fstp_s(Address(rsp, st_off));
 851         __ movl(rsi, Address(saved_sp, ld_off));
 852         __ movptr(Address(rsp, st_off), rsi);
 853       } else {
 854         // Interpreter local[n] == MSW, local[n+1] == LSW however locals
 855         // are accessed as negative so LSW is at LOW address
 856 
 857         // ld_off is MSW so get LSW
 858         // st_off is LSW (i.e. reg.first())
 859         // __ fld_d(Address(saved_sp, next_off));
 860         // __ fstp_d(Address(rsp, st_off));
 861         //
 862         // We are using two VMRegs. This can be either T_OBJECT, T_ADDRESS, T_LONG, or T_DOUBLE
 863         // the interpreter allocates two slots but only uses one for thr T_LONG or T_DOUBLE case
 864         // So we must adjust where to pick up the data to match the interpreter.
 865         //
 866         // Interpreter local[n] == MSW, local[n+1] == LSW however locals
 867         // are accessed as negative so LSW is at LOW address
 868 
 869         // ld_off is MSW so get LSW
<span class="line-modified"> 870         const int offset = (NOT_LP64(true ||) sig_bt[i]==T_LONG||sig_bt[i]==T_DOUBLE)?</span>
 871                            next_off : ld_off;
 872         __ movptr(rsi, Address(saved_sp, offset));
 873         __ movptr(Address(rsp, st_off), rsi);
 874 #ifndef _LP64
 875         __ movptr(rsi, Address(saved_sp, ld_off));
 876         __ movptr(Address(rsp, st_off + wordSize), rsi);
 877 #endif // _LP64
 878       }
 879     } else if (r_1-&gt;is_Register()) {  // Register argument
 880       Register r = r_1-&gt;as_Register();
 881       assert(r != rax, &quot;must be different&quot;);
 882       if (r_2-&gt;is_valid()) {
 883         //
 884         // We are using two VMRegs. This can be either T_OBJECT, T_ADDRESS, T_LONG, or T_DOUBLE
 885         // the interpreter allocates two slots but only uses one for thr T_LONG or T_DOUBLE case
 886         // So we must adjust where to pick up the data to match the interpreter.
 887 
<span class="line-modified"> 888         const int offset = (NOT_LP64(true ||) sig_bt[i]==T_LONG||sig_bt[i]==T_DOUBLE)?</span>
 889                            next_off : ld_off;
 890 
 891         // this can be a misaligned move
 892         __ movptr(r, Address(saved_sp, offset));
 893 #ifndef _LP64
 894         assert(r_2-&gt;as_Register() != rax, &quot;need another temporary register&quot;);
 895         // Remember r_1 is low address (and LSB on x86)
 896         // So r_2 gets loaded from high address regardless of the platform
 897         __ movptr(r_2-&gt;as_Register(), Address(saved_sp, ld_off));
 898 #endif // _LP64
 899       } else {
 900         __ movl(r, Address(saved_sp, ld_off));
 901       }
 902     } else {
 903       assert(r_1-&gt;is_XMMRegister(), &quot;&quot;);
 904       if (!r_2-&gt;is_valid()) {
 905         __ movflt(r_1-&gt;as_XMMRegister(), Address(saved_sp, ld_off));
 906       } else {
 907         move_i2c_double(masm, r_1-&gt;as_XMMRegister(), saved_sp, ld_off);
 908       }
</pre>
<hr />
<pre>
 916   // &quot;compiled&quot; so it is much better to make this transition
 917   // invisible to the stack walking code. Unfortunately if
 918   // we try and find the callee by normal means a safepoint
 919   // is possible. So we stash the desired callee in the thread
 920   // and the vm will find there should this case occur.
 921 
 922   __ get_thread(rax);
 923   __ movptr(Address(rax, JavaThread::callee_target_offset()), rbx);
 924 
 925   // move Method* to rax, in case we end up in an c2i adapter.
 926   // the c2i adapters expect Method* in rax, (c2) because c2&#39;s
 927   // resolve stubs return the result (the method) in rax,.
 928   // I&#39;d love to fix this.
 929   __ mov(rax, rbx);
 930 
 931   __ jmp(rdi);
 932 }
 933 
 934 // ---------------------------------------------------------------
 935 AdapterHandlerEntry* SharedRuntime::generate_i2c2i_adapters(MacroAssembler *masm,
<span class="line-modified"> 936                                                             int total_args_passed,</span>
<span class="line-removed"> 937                                                             int comp_args_on_stack,</span>
 938                                                             const BasicType *sig_bt,
 939                                                             const VMRegPair *regs,
<span class="line-modified"> 940                                                             AdapterFingerPrint* fingerprint) {</span>

 941   address i2c_entry = __ pc();
 942 
<span class="line-modified"> 943   gen_i2c_adapter(masm, total_args_passed, comp_args_on_stack, sig_bt, regs);</span>
 944 
 945   // -------------------------------------------------------------------------
 946   // Generate a C2I adapter.  On entry we know rbx, holds the Method* during calls
 947   // to the interpreter.  The args start out packed in the compiled layout.  They
 948   // need to be unpacked into the interpreter layout.  This will almost always
 949   // require some stack space.  We grow the current (compiled) stack, then repack
 950   // the args.  We  finally end in a jump to the generic interpreter entry point.
 951   // On exit from the interpreter, the interpreter will restore our SP (lest the
 952   // compiled code, which relys solely on SP and not EBP, get sick).
 953 
 954   address c2i_unverified_entry = __ pc();
 955   Label skip_fixup;
 956 
 957   Register holder = rax;
 958   Register receiver = rcx;
 959   Register temp = rbx;
 960 
 961   {
 962 
 963     Label missed;
 964     __ movptr(temp, Address(receiver, oopDesc::klass_offset_in_bytes()));
 965     __ cmpptr(temp, Address(holder, CompiledICHolder::holder_klass_offset()));
 966     __ movptr(rbx, Address(holder, CompiledICHolder::holder_metadata_offset()));
 967     __ jcc(Assembler::notEqual, missed);
 968     // Method might have been compiled since the call site was patched to
 969     // interpreted if that is the case treat it as a miss so we can get
 970     // the call site corrected.
 971     __ cmpptr(Address(rbx, in_bytes(Method::code_offset())), (int32_t)NULL_WORD);
 972     __ jcc(Assembler::equal, skip_fixup);
 973 
 974     __ bind(missed);
 975     __ jump(RuntimeAddress(SharedRuntime::get_ic_miss_stub()));
 976   }
 977 
 978   address c2i_entry = __ pc();
 979 
 980   BarrierSetAssembler* bs = BarrierSet::barrier_set()-&gt;barrier_set_assembler();
 981   bs-&gt;c2i_entry_barrier(masm);
 982 
<span class="line-modified"> 983   gen_c2i_adapter(masm, total_args_passed, comp_args_on_stack, sig_bt, regs, skip_fixup);</span>



 984 
 985   __ flush();

 986   return AdapterHandlerLibrary::new_entry(fingerprint, i2c_entry, c2i_entry, c2i_unverified_entry);
 987 }
 988 
 989 int SharedRuntime::c_calling_convention(const BasicType *sig_bt,
 990                                          VMRegPair *regs,
 991                                          VMRegPair *regs2,
 992                                          int total_args_passed) {
 993   assert(regs2 == NULL, &quot;not needed on x86&quot;);
 994 // We return the amount of VMRegImpl stack slots we need to reserve for all
 995 // the arguments NOT counting out_preserve_stack_slots.
 996 
 997   uint    stack = 0;        // All arguments on stack
 998 
 999   for( int i = 0; i &lt; total_args_passed; i++) {
1000     // From the type and the argument number (count) compute the location
1001     switch( sig_bt[i] ) {
1002     case T_BOOLEAN:
1003     case T_CHAR:
1004     case T_FLOAT:
1005     case T_BYTE:
1006     case T_SHORT:
1007     case T_INT:
1008     case T_OBJECT:

1009     case T_ARRAY:
1010     case T_ADDRESS:
1011     case T_METADATA:
1012       regs[i].set1(VMRegImpl::stack2reg(stack++));
1013       break;
1014     case T_LONG:
1015     case T_DOUBLE: // The stack numbering is reversed from Java
1016       // Since C arguments do not get reversed, the ordering for
1017       // doubles on the stack must be opposite the Java convention
1018       assert((i + 1) &lt; total_args_passed &amp;&amp; sig_bt[i+1] == T_VOID, &quot;missing Half&quot; );
1019       regs[i].set2(VMRegImpl::stack2reg(stack));
1020       stack += 2;
1021       break;
1022     case T_VOID: regs[i].set_bad(); break;
1023     default:
1024       ShouldNotReachHere();
1025       break;
1026     }
1027   }
1028   return stack;
</pre>
<hr />
<pre>
1270       switch (in_sig_bt[i]) {
1271         case T_ARRAY:
1272           if (map != NULL) {
1273             __ movptr(Address(rsp, offset), reg);
1274           } else {
1275             __ movptr(reg, Address(rsp, offset));
1276           }
1277           break;
1278         case T_BOOLEAN:
1279         case T_CHAR:
1280         case T_BYTE:
1281         case T_SHORT:
1282         case T_INT:
1283           if (map != NULL) {
1284             __ movl(Address(rsp, offset), reg);
1285           } else {
1286             __ movl(reg, Address(rsp, offset));
1287           }
1288           break;
1289         case T_OBJECT:

1290         default: ShouldNotReachHere();
1291       }
1292     } else if (in_regs[i].first()-&gt;is_XMMRegister()) {
1293       if (in_sig_bt[i] == T_FLOAT) {
1294         int slot = handle_index++ * VMRegImpl::slots_per_word + arg_save_area;
1295         int offset = slot * VMRegImpl::stack_slot_size;
1296         assert(handle_index &lt;= stack_slots, &quot;overflow&quot;);
1297         if (map != NULL) {
1298           __ movflt(Address(rsp, offset), in_regs[i].first()-&gt;as_XMMRegister());
1299         } else {
1300           __ movflt(in_regs[i].first()-&gt;as_XMMRegister(), Address(rsp, offset));
1301         }
1302       }
1303     } else if (in_regs[i].first()-&gt;is_stack()) {
1304       if (in_sig_bt[i] == T_ARRAY &amp;&amp; map != NULL) {
1305         int offset_in_older_frame = in_regs[i].first()-&gt;reg2stack() + SharedRuntime::out_preserve_stack_slots();
1306         map-&gt;set_oop(VMRegImpl::stack2reg(offset_in_older_frame + stack_slots));
1307       }
1308     }
1309   }
</pre>
<hr />
<pre>
1987             pinned_args.append(i);
1988 
1989             // rax has pinned array
1990             VMRegPair result_reg(rax-&gt;as_VMReg());
1991             if (!in_arg.first()-&gt;is_stack()) {
1992               assert(pinned_slot &lt;= stack_slots, &quot;overflow&quot;);
1993               simple_move32(masm, result_reg, VMRegImpl::stack2reg(pinned_slot));
1994               pinned_slot += VMRegImpl::slots_per_word;
1995             } else {
1996               // Write back pinned value, it will be used to unpin this argument
1997               __ movptr(Address(rbp, reg2offset_in(in_arg.first())), result_reg.first()-&gt;as_Register());
1998             }
1999             // We have the array in register, use it
2000             in_arg = result_reg;
2001           }
2002 
2003           unpack_array_argument(masm, in_arg, in_elem_bt[i], out_regs[c_arg + 1], out_regs[c_arg]);
2004           c_arg++;
2005           break;
2006         }

2007       case T_OBJECT:
2008         assert(!is_critical_native, &quot;no oop arguments&quot;);
2009         object_move(masm, map, oop_handle_offset, stack_slots, in_regs[i], out_regs[c_arg],
2010                     ((i == 0) &amp;&amp; (!is_static)),
2011                     &amp;receiver_offset);
2012         break;
2013       case T_VOID:
2014         break;
2015 
2016       case T_FLOAT:
2017         float_move(masm, in_regs[i], out_regs[c_arg]);
2018           break;
2019 
2020       case T_DOUBLE:
2021         assert( i + 1 &lt; total_in_args &amp;&amp;
2022                 in_sig_bt[i + 1] == T_VOID &amp;&amp;
2023                 out_sig_bt[c_arg+1] == T_VOID, &quot;bad arg list&quot;);
2024         double_move(masm, in_regs[i], out_regs[c_arg]);
2025         break;
2026 
</pre>
<hr />
<pre>
2169   // Verify or restore cpu control state after JNI call
2170   __ restore_cpu_control_state_after_jni();
2171 
2172   // WARNING - on Windows Java Natives use pascal calling convention and pop the
2173   // arguments off of the stack. We could just re-adjust the stack pointer here
2174   // and continue to do SP relative addressing but we instead switch to FP
2175   // relative addressing.
2176 
2177   // Unpack native results.
2178   switch (ret_type) {
2179   case T_BOOLEAN: __ c2bool(rax);            break;
2180   case T_CHAR   : __ andptr(rax, 0xFFFF);    break;
2181   case T_BYTE   : __ sign_extend_byte (rax); break;
2182   case T_SHORT  : __ sign_extend_short(rax); break;
2183   case T_INT    : /* nothing to do */        break;
2184   case T_DOUBLE :
2185   case T_FLOAT  :
2186     // Result is in st0 we&#39;ll save as needed
2187     break;
2188   case T_ARRAY:                 // Really a handle

2189   case T_OBJECT:                // Really a handle
2190       break; // can&#39;t de-handlize until after safepoint check
2191   case T_VOID: break;
2192   case T_LONG: break;
2193   default       : ShouldNotReachHere();
2194   }
2195 
2196   // unpin pinned arguments
2197   pinned_slot = oop_handle_offset;
2198   if (pinned_args.length() &gt; 0) {
2199     // save return value that may be overwritten otherwise.
2200     save_native_result(masm, ret_type, stack_slots);
2201     for (int index = 0; index &lt; pinned_args.length(); index ++) {
2202       int i = pinned_args.at(index);
2203       assert(pinned_slot &lt;= stack_slots, &quot;overflow&quot;);
2204       if (!in_regs[i].first()-&gt;is_stack()) {
2205         int offset = pinned_slot * VMRegImpl::stack_slot_size;
2206         __ movl(in_regs[i].first()-&gt;as_Register(), Address(rsp, offset));
2207         pinned_slot += VMRegImpl::slots_per_word;
2208       }
</pre>
<hr />
<pre>
3287 
3288   __ bind(pending);
3289 
3290   RegisterSaver::restore_live_registers(masm);
3291 
3292   // exception pending =&gt; remove activation and forward to exception handler
3293 
3294   __ get_thread(thread);
3295   __ movptr(Address(thread, JavaThread::vm_result_offset()), NULL_WORD);
3296   __ movptr(rax, Address(thread, Thread::pending_exception_offset()));
3297   __ jump(RuntimeAddress(StubRoutines::forward_exception_entry()));
3298 
3299   // -------------
3300   // make sure all code is generated
3301   masm-&gt;flush();
3302 
3303   // return the  blob
3304   // frame_size_words or bytes??
3305   return RuntimeStub::new_runtime_stub(name, &amp;buffer, frame_complete, frame_size_words, oop_maps, true);
3306 }





</pre>
</td>
<td>
<hr />
<pre>
 449       else if( freg_arg1 == fltarg_flt_dbl ) freg_arg1 = i;
 450       else // Else double is passed low on the stack to be aligned.
 451         stack += 2;
 452     } else if( sig_bt[i] == T_LONG ) {
 453       stack += 2;
 454     }
 455   }
 456   int dstack = 0;             // Separate counter for placing doubles
 457 
 458   // Now pick where all else goes.
 459   for( i = 0; i &lt; total_args_passed; i++) {
 460     // From the type and the argument number (count) compute the location
 461     switch( sig_bt[i] ) {
 462     case T_SHORT:
 463     case T_CHAR:
 464     case T_BYTE:
 465     case T_BOOLEAN:
 466     case T_INT:
 467     case T_ARRAY:
 468     case T_OBJECT:
<span class="line-added"> 469     case T_VALUETYPE:</span>
 470     case T_ADDRESS:
 471       if( reg_arg0 == 9999 )  {
 472         reg_arg0 = i;
 473         regs[i].set1(rcx-&gt;as_VMReg());
 474       } else if( reg_arg1 == 9999 )  {
 475         reg_arg1 = i;
 476         regs[i].set1(rdx-&gt;as_VMReg());
 477       } else {
 478         regs[i].set1(VMRegImpl::stack2reg(stack++));
 479       }
 480       break;
 481     case T_FLOAT:
 482       if( freg_arg0 == fltarg_flt_dbl || freg_arg0 == fltarg_float_only ) {
 483         freg_arg0 = i;
 484         regs[i].set1(xmm0-&gt;as_VMReg());
 485       } else if( freg_arg1 == fltarg_flt_dbl || freg_arg1 == fltarg_float_only ) {
 486         freg_arg1 = i;
 487         regs[i].set1(xmm1-&gt;as_VMReg());
 488       } else {
 489         regs[i].set1(VMRegImpl::stack2reg(stack++));
</pre>
<hr />
<pre>
 500         regs[i].set2(xmm0-&gt;as_VMReg());
 501       } else if( freg_arg1 == (uint)i ) {
 502         regs[i].set2(xmm1-&gt;as_VMReg());
 503       } else {
 504         regs[i].set2(VMRegImpl::stack2reg(dstack));
 505         dstack += 2;
 506       }
 507       break;
 508     case T_VOID: regs[i].set_bad(); break;
 509       break;
 510     default:
 511       ShouldNotReachHere();
 512       break;
 513     }
 514   }
 515 
 516   // return value can be odd number of VMRegImpl stack slots make multiple of 2
 517   return align_up(stack, 2);
 518 }
 519 
<span class="line-added"> 520 const uint SharedRuntime::java_return_convention_max_int = 1;</span>
<span class="line-added"> 521 const uint SharedRuntime::java_return_convention_max_float = 1;</span>
<span class="line-added"> 522 int SharedRuntime::java_return_convention(const BasicType *sig_bt,</span>
<span class="line-added"> 523                                           VMRegPair *regs,</span>
<span class="line-added"> 524                                           int total_args_passed) {</span>
<span class="line-added"> 525   Unimplemented();</span>
<span class="line-added"> 526   return 0;</span>
<span class="line-added"> 527 }</span>
<span class="line-added"> 528 </span>
 529 // Patch the callers callsite with entry to compiled code if it exists.
 530 static void patch_callers_callsite(MacroAssembler *masm) {
 531   Label L;
 532   __ cmpptr(Address(rbx, in_bytes(Method::code_offset())), (int32_t)NULL_WORD);
 533   __ jcc(Assembler::equal, L);
 534   // Schedule the branch target address early.
 535   // Call into the VM to patch the caller, then jump to compiled callee
 536   // rax, isn&#39;t live so capture return address while we easily can
 537   __ movptr(rax, Address(rsp, 0));
 538   __ pusha();
 539   __ pushf();
 540 
 541   if (UseSSE == 1) {
 542     __ subptr(rsp, 2*wordSize);
 543     __ movflt(Address(rsp, 0), xmm0);
 544     __ movflt(Address(rsp, wordSize), xmm1);
 545   }
 546   if (UseSSE &gt;= 2) {
 547     __ subptr(rsp, 4*wordSize);
 548     __ movdbl(Address(rsp, 0), xmm0);
</pre>
<hr />
<pre>
 570     __ addptr(rsp, 2*wordSize);
 571   }
 572   if (UseSSE &gt;= 2) {
 573     __ movdbl(xmm0, Address(rsp, 0));
 574     __ movdbl(xmm1, Address(rsp, 2*wordSize));
 575     __ addptr(rsp, 4*wordSize);
 576   }
 577 
 578   __ popf();
 579   __ popa();
 580   __ bind(L);
 581 }
 582 
 583 
 584 static void move_c2i_double(MacroAssembler *masm, XMMRegister r, int st_off) {
 585   int next_off = st_off - Interpreter::stackElementSize;
 586   __ movdbl(Address(rsp, next_off), r);
 587 }
 588 
 589 static void gen_c2i_adapter(MacroAssembler *masm,
<span class="line-modified"> 590                             const GrowableArray&lt;SigEntry&gt;&amp; sig_extended,</span>


 591                             const VMRegPair *regs,
<span class="line-modified"> 592                             Label&amp; skip_fixup,</span>
<span class="line-added"> 593                             address start,</span>
<span class="line-added"> 594                             OopMapSet*&amp; oop_maps,</span>
<span class="line-added"> 595                             int&amp; frame_complete,</span>
<span class="line-added"> 596                             int&amp; frame_size_in_words) {</span>
 597   // Before we get into the guts of the C2I adapter, see if we should be here
 598   // at all.  We&#39;ve come from compiled code and are attempting to jump to the
 599   // interpreter, which means the caller made a static call to get here
 600   // (vcalls always get a compiled target if there is one).  Check for a
 601   // compiled target.  If there is one, we need to patch the caller&#39;s call.
 602   patch_callers_callsite(masm);
 603 
 604   __ bind(skip_fixup);
 605 
 606 #ifdef COMPILER2
 607   // C2 may leave the stack dirty if not in SSE2+ mode
 608   if (UseSSE &gt;= 2) {
 609     __ verify_FPU(0, &quot;c2i transition should have clean FPU stack&quot;);
 610   } else {
 611     __ empty_FPU_stack();
 612   }
 613 #endif /* COMPILER2 */
 614 
 615   // Since all args are passed on the stack, total_args_passed * interpreter_
 616   // stack_element_size  is the
 617   // space we need.
<span class="line-modified"> 618   int extraspace = sig_extended.length() * Interpreter::stackElementSize;</span>
 619 
 620   // Get return address
 621   __ pop(rax);
 622 
 623   // set senderSP value
 624   __ movptr(rsi, rsp);
 625 
 626   __ subptr(rsp, extraspace);
 627 
 628   // Now write the args into the outgoing interpreter space
<span class="line-modified"> 629   for (int i = 0; i &lt; sig_extended.length(); i++) {</span>
<span class="line-modified"> 630     if (sig_extended.at(i)._bt == T_VOID) {</span>
<span class="line-modified"> 631       assert(i &gt; 0 &amp;&amp; (sig_extended.at(i-1)._bt == T_LONG || sig_extended.at(i-1)._bt == T_DOUBLE), &quot;missing half&quot;);</span>
 632       continue;
 633     }
 634 
 635     // st_off points to lowest address on stack.
<span class="line-modified"> 636     int st_off = ((sig_extended.length() - 1) - i) * Interpreter::stackElementSize;</span>
 637     int next_off = st_off - Interpreter::stackElementSize;
 638 
 639     // Say 4 args:
 640     // i   st_off
 641     // 0   12 T_LONG
 642     // 1    8 T_VOID
 643     // 2    4 T_OBJECT
 644     // 3    0 T_BOOL
 645     VMReg r_1 = regs[i].first();
 646     VMReg r_2 = regs[i].second();
 647     if (!r_1-&gt;is_valid()) {
 648       assert(!r_2-&gt;is_valid(), &quot;&quot;);
 649       continue;
 650     }
 651 
 652     if (r_1-&gt;is_stack()) {
 653       // memory to memory use fpu stack top
 654       int ld_off = r_1-&gt;reg2stack() * VMRegImpl::stack_slot_size + extraspace;
 655 
 656       if (!r_2-&gt;is_valid()) {
</pre>
<hr />
<pre>
 666 #ifndef _LP64
 667         __ movptr(rdi, Address(rsp, ld_off + wordSize));
 668         __ movptr(Address(rsp, st_off), rdi);
 669 #else
 670 #ifdef ASSERT
 671         // Overwrite the unused slot with known junk
 672         __ mov64(rax, CONST64(0xdeadffffdeadaaaa));
 673         __ movptr(Address(rsp, st_off), rax);
 674 #endif /* ASSERT */
 675 #endif // _LP64
 676       }
 677     } else if (r_1-&gt;is_Register()) {
 678       Register r = r_1-&gt;as_Register();
 679       if (!r_2-&gt;is_valid()) {
 680         __ movl(Address(rsp, st_off), r);
 681       } else {
 682         // long/double in gpr
 683         NOT_LP64(ShouldNotReachHere());
 684         // Two VMRegs can be T_OBJECT, T_ADDRESS, T_DOUBLE, T_LONG
 685         // T_DOUBLE and T_LONG use two slots in the interpreter
<span class="line-modified"> 686         if (sig_extended.at(i)._bt == T_LONG || sig_extended.at(i)._bt == T_DOUBLE) {</span>
 687           // long/double in gpr
 688 #ifdef ASSERT
 689           // Overwrite the unused slot with known junk
 690           LP64_ONLY(__ mov64(rax, CONST64(0xdeadffffdeadaaab)));
 691           __ movptr(Address(rsp, st_off), rax);
 692 #endif /* ASSERT */
 693           __ movptr(Address(rsp, next_off), r);
 694         } else {
 695           __ movptr(Address(rsp, st_off), r);
 696         }
 697       }
 698     } else {
 699       assert(r_1-&gt;is_XMMRegister(), &quot;&quot;);
 700       if (!r_2-&gt;is_valid()) {
 701         __ movflt(Address(rsp, st_off), r_1-&gt;as_XMMRegister());
 702       } else {
<span class="line-modified"> 703         assert(sig_extended.at(i)._bt == T_DOUBLE || sig_extended.at(i)._bt == T_LONG, &quot;wrong type&quot;);</span>
 704         move_c2i_double(masm, r_1-&gt;as_XMMRegister(), st_off);
 705       }
 706     }
 707   }
 708 
 709   // Schedule the branch target address early.
 710   __ movptr(rcx, Address(rbx, in_bytes(Method::interpreter_entry_offset())));
 711   // And repush original return address
 712   __ push(rax);
 713   __ jmp(rcx);
 714 }
 715 
 716 
 717 static void move_i2c_double(MacroAssembler *masm, XMMRegister r, Register saved_sp, int ld_off) {
 718   int next_val_off = ld_off - Interpreter::stackElementSize;
 719   __ movdbl(r, Address(saved_sp, next_val_off));
 720 }
 721 
 722 static void range_check(MacroAssembler* masm, Register pc_reg, Register temp_reg,
 723                         address code_start, address code_end,
 724                         Label&amp; L_ok) {
 725   Label L_fail;
 726   __ lea(temp_reg, ExternalAddress(code_start));
 727   __ cmpptr(pc_reg, temp_reg);
 728   __ jcc(Assembler::belowEqual, L_fail);
 729   __ lea(temp_reg, ExternalAddress(code_end));
 730   __ cmpptr(pc_reg, temp_reg);
 731   __ jcc(Assembler::below, L_ok);
 732   __ bind(L_fail);
 733 }
 734 
 735 void SharedRuntime::gen_i2c_adapter(MacroAssembler *masm,
<span class="line-modified"> 736                                     int comp_args_on_stack,</span>

 737                                     const GrowableArray&lt;SigEntry&gt;&amp; sig_extended,
 738                                     const VMRegPair *regs) {
<span class="line-added"> 739 </span>
 740   // Note: rsi contains the senderSP on entry. We must preserve it since
 741   // we may do a i2c -&gt; c2i transition if we lose a race where compiled
 742   // code goes non-entrant while we get args ready.
 743 
 744   // Adapters can be frameless because they do not require the caller
 745   // to perform additional cleanup work, such as correcting the stack pointer.
 746   // An i2c adapter is frameless because the *caller* frame, which is interpreted,
 747   // routinely repairs its own stack pointer (from interpreter_frame_last_sp),
 748   // even if a callee has modified the stack pointer.
 749   // A c2i adapter is frameless because the *callee* frame, which is interpreted,
 750   // routinely repairs its caller&#39;s stack pointer (from sender_sp, which is set
 751   // up via the senderSP register).
 752   // In other words, if *either* the caller or callee is interpreted, we can
 753   // get the stack pointer repaired after a call.
 754   // This is why c2i and i2c adapters cannot be indefinitely composed.
 755   // In particular, if a c2i adapter were to somehow call an i2c adapter,
 756   // both caller and callee would be compiled methods, and neither would
 757   // clean up the stack pointer changes performed by the two adapters.
 758   // If this happens, control eventually transfers back to the compiled
 759   // caller, but with an uncorrected stack, causing delayed havoc.
</pre>
<hr />
<pre>
 808   }
 809 
 810   // Align the outgoing SP
 811   __ andptr(rsp, -(StackAlignmentInBytes));
 812 
 813   // push the return address on the stack (note that pushing, rather
 814   // than storing it, yields the correct frame alignment for the callee)
 815   __ push(rax);
 816 
 817   // Put saved SP in another register
 818   const Register saved_sp = rax;
 819   __ movptr(saved_sp, rdi);
 820 
 821 
 822   // Will jump to the compiled code just as if compiled code was doing it.
 823   // Pre-load the register-jump target early, to schedule it better.
 824   __ movptr(rdi, Address(rbx, in_bytes(Method::from_compiled_offset())));
 825 
 826   // Now generate the shuffle code.  Pick up all register args and move the
 827   // rest through the floating point stack top.
<span class="line-modified"> 828   for (int i = 0; i &lt; sig_extended.length(); i++) {</span>
<span class="line-modified"> 829     if (sig_extended.at(i)._bt == T_VOID) {</span>
 830       // Longs and doubles are passed in native word order, but misaligned
 831       // in the 32-bit build.
<span class="line-modified"> 832       assert(i &gt; 0 &amp;&amp; (sig_extended.at(i-1)._bt == T_LONG || sig_extended.at(i-1)._bt == T_DOUBLE), &quot;missing half&quot;);</span>
 833       continue;
 834     }
 835 
 836     // Pick up 0, 1 or 2 words from SP+offset.
 837 
 838     assert(!regs[i].second()-&gt;is_valid() || regs[i].first()-&gt;next() == regs[i].second(),
 839             &quot;scrambled load targets?&quot;);
 840     // Load in argument order going down.
<span class="line-modified"> 841     int ld_off = (sig_extended.length() - i) * Interpreter::stackElementSize;</span>
 842     // Point to interpreter value (vs. tag)
 843     int next_off = ld_off - Interpreter::stackElementSize;
 844     //
 845     //
 846     //
 847     VMReg r_1 = regs[i].first();
 848     VMReg r_2 = regs[i].second();
 849     if (!r_1-&gt;is_valid()) {
 850       assert(!r_2-&gt;is_valid(), &quot;&quot;);
 851       continue;
 852     }
 853     if (r_1-&gt;is_stack()) {
 854       // Convert stack slot to an SP offset (+ wordSize to account for return address )
 855       int st_off = regs[i].first()-&gt;reg2stack()*VMRegImpl::stack_slot_size + wordSize;
 856 
 857       // We can use rsi as a temp here because compiled code doesn&#39;t need rsi as an input
 858       // and if we end up going thru a c2i because of a miss a reasonable value of rsi
 859       // we be generated.
 860       if (!r_2-&gt;is_valid()) {
 861         // __ fld_s(Address(saved_sp, ld_off));
 862         // __ fstp_s(Address(rsp, st_off));
 863         __ movl(rsi, Address(saved_sp, ld_off));
 864         __ movptr(Address(rsp, st_off), rsi);
 865       } else {
 866         // Interpreter local[n] == MSW, local[n+1] == LSW however locals
 867         // are accessed as negative so LSW is at LOW address
 868 
 869         // ld_off is MSW so get LSW
 870         // st_off is LSW (i.e. reg.first())
 871         // __ fld_d(Address(saved_sp, next_off));
 872         // __ fstp_d(Address(rsp, st_off));
 873         //
 874         // We are using two VMRegs. This can be either T_OBJECT, T_ADDRESS, T_LONG, or T_DOUBLE
 875         // the interpreter allocates two slots but only uses one for thr T_LONG or T_DOUBLE case
 876         // So we must adjust where to pick up the data to match the interpreter.
 877         //
 878         // Interpreter local[n] == MSW, local[n+1] == LSW however locals
 879         // are accessed as negative so LSW is at LOW address
 880 
 881         // ld_off is MSW so get LSW
<span class="line-modified"> 882         const int offset = (NOT_LP64(true ||) sig_extended.at(i)._bt==T_LONG||sig_extended.at(i)._bt==T_DOUBLE)?</span>
 883                            next_off : ld_off;
 884         __ movptr(rsi, Address(saved_sp, offset));
 885         __ movptr(Address(rsp, st_off), rsi);
 886 #ifndef _LP64
 887         __ movptr(rsi, Address(saved_sp, ld_off));
 888         __ movptr(Address(rsp, st_off + wordSize), rsi);
 889 #endif // _LP64
 890       }
 891     } else if (r_1-&gt;is_Register()) {  // Register argument
 892       Register r = r_1-&gt;as_Register();
 893       assert(r != rax, &quot;must be different&quot;);
 894       if (r_2-&gt;is_valid()) {
 895         //
 896         // We are using two VMRegs. This can be either T_OBJECT, T_ADDRESS, T_LONG, or T_DOUBLE
 897         // the interpreter allocates two slots but only uses one for thr T_LONG or T_DOUBLE case
 898         // So we must adjust where to pick up the data to match the interpreter.
 899 
<span class="line-modified"> 900         const int offset = (NOT_LP64(true ||) sig_extended.at(i)._bt==T_LONG||sig_extended.at(i)._bt==T_DOUBLE)?</span>
 901                            next_off : ld_off;
 902 
 903         // this can be a misaligned move
 904         __ movptr(r, Address(saved_sp, offset));
 905 #ifndef _LP64
 906         assert(r_2-&gt;as_Register() != rax, &quot;need another temporary register&quot;);
 907         // Remember r_1 is low address (and LSB on x86)
 908         // So r_2 gets loaded from high address regardless of the platform
 909         __ movptr(r_2-&gt;as_Register(), Address(saved_sp, ld_off));
 910 #endif // _LP64
 911       } else {
 912         __ movl(r, Address(saved_sp, ld_off));
 913       }
 914     } else {
 915       assert(r_1-&gt;is_XMMRegister(), &quot;&quot;);
 916       if (!r_2-&gt;is_valid()) {
 917         __ movflt(r_1-&gt;as_XMMRegister(), Address(saved_sp, ld_off));
 918       } else {
 919         move_i2c_double(masm, r_1-&gt;as_XMMRegister(), saved_sp, ld_off);
 920       }
</pre>
<hr />
<pre>
 928   // &quot;compiled&quot; so it is much better to make this transition
 929   // invisible to the stack walking code. Unfortunately if
 930   // we try and find the callee by normal means a safepoint
 931   // is possible. So we stash the desired callee in the thread
 932   // and the vm will find there should this case occur.
 933 
 934   __ get_thread(rax);
 935   __ movptr(Address(rax, JavaThread::callee_target_offset()), rbx);
 936 
 937   // move Method* to rax, in case we end up in an c2i adapter.
 938   // the c2i adapters expect Method* in rax, (c2) because c2&#39;s
 939   // resolve stubs return the result (the method) in rax,.
 940   // I&#39;d love to fix this.
 941   __ mov(rax, rbx);
 942 
 943   __ jmp(rdi);
 944 }
 945 
 946 // ---------------------------------------------------------------
 947 AdapterHandlerEntry* SharedRuntime::generate_i2c2i_adapters(MacroAssembler *masm,
<span class="line-modified"> 948                                                             int comp_args_on_stack,</span>

 949                                                             const GrowableArray&lt;SigEntry&gt;&amp; sig_extended,
 950                                                             const VMRegPair *regs,
<span class="line-modified"> 951                                                             AdapterFingerPrint* fingerprint,</span>
<span class="line-added"> 952                                                             AdapterBlob*&amp; new_adapter) {</span>
 953   address i2c_entry = __ pc();
 954 
<span class="line-modified"> 955   gen_i2c_adapter(masm, comp_args_on_stack, sig_extended, regs);</span>
 956 
 957   // -------------------------------------------------------------------------
 958   // Generate a C2I adapter.  On entry we know rbx, holds the Method* during calls
 959   // to the interpreter.  The args start out packed in the compiled layout.  They
 960   // need to be unpacked into the interpreter layout.  This will almost always
 961   // require some stack space.  We grow the current (compiled) stack, then repack
 962   // the args.  We  finally end in a jump to the generic interpreter entry point.
 963   // On exit from the interpreter, the interpreter will restore our SP (lest the
 964   // compiled code, which relys solely on SP and not EBP, get sick).
 965 
 966   address c2i_unverified_entry = __ pc();
 967   Label skip_fixup;
 968 
 969   Register holder = rax;
 970   Register receiver = rcx;
 971   Register temp = rbx;
 972 
 973   {
 974 
 975     Label missed;
 976     __ movptr(temp, Address(receiver, oopDesc::klass_offset_in_bytes()));
 977     __ cmpptr(temp, Address(holder, CompiledICHolder::holder_klass_offset()));
 978     __ movptr(rbx, Address(holder, CompiledICHolder::holder_metadata_offset()));
 979     __ jcc(Assembler::notEqual, missed);
 980     // Method might have been compiled since the call site was patched to
 981     // interpreted if that is the case treat it as a miss so we can get
 982     // the call site corrected.
 983     __ cmpptr(Address(rbx, in_bytes(Method::code_offset())), (int32_t)NULL_WORD);
 984     __ jcc(Assembler::equal, skip_fixup);
 985 
 986     __ bind(missed);
 987     __ jump(RuntimeAddress(SharedRuntime::get_ic_miss_stub()));
 988   }
 989 
 990   address c2i_entry = __ pc();
 991 
 992   BarrierSetAssembler* bs = BarrierSet::barrier_set()-&gt;barrier_set_assembler();
 993   bs-&gt;c2i_entry_barrier(masm);
 994 
<span class="line-modified"> 995   OopMapSet* oop_maps = NULL;</span>
<span class="line-added"> 996   int frame_complete = CodeOffsets::frame_never_safe;</span>
<span class="line-added"> 997   int frame_size_in_words = 0;</span>
<span class="line-added"> 998   gen_c2i_adapter(masm, sig_extended, regs, skip_fixup, i2c_entry, oop_maps, frame_complete, frame_size_in_words);</span>
 999 
1000   __ flush();
<span class="line-added">1001   new_adapter = AdapterBlob::create(masm-&gt;code(), frame_complete, frame_size_in_words, oop_maps);</span>
1002   return AdapterHandlerLibrary::new_entry(fingerprint, i2c_entry, c2i_entry, c2i_unverified_entry);
1003 }
1004 
1005 int SharedRuntime::c_calling_convention(const BasicType *sig_bt,
1006                                          VMRegPair *regs,
1007                                          VMRegPair *regs2,
1008                                          int total_args_passed) {
1009   assert(regs2 == NULL, &quot;not needed on x86&quot;);
1010 // We return the amount of VMRegImpl stack slots we need to reserve for all
1011 // the arguments NOT counting out_preserve_stack_slots.
1012 
1013   uint    stack = 0;        // All arguments on stack
1014 
1015   for( int i = 0; i &lt; total_args_passed; i++) {
1016     // From the type and the argument number (count) compute the location
1017     switch( sig_bt[i] ) {
1018     case T_BOOLEAN:
1019     case T_CHAR:
1020     case T_FLOAT:
1021     case T_BYTE:
1022     case T_SHORT:
1023     case T_INT:
1024     case T_OBJECT:
<span class="line-added">1025     case T_VALUETYPE:</span>
1026     case T_ARRAY:
1027     case T_ADDRESS:
1028     case T_METADATA:
1029       regs[i].set1(VMRegImpl::stack2reg(stack++));
1030       break;
1031     case T_LONG:
1032     case T_DOUBLE: // The stack numbering is reversed from Java
1033       // Since C arguments do not get reversed, the ordering for
1034       // doubles on the stack must be opposite the Java convention
1035       assert((i + 1) &lt; total_args_passed &amp;&amp; sig_bt[i+1] == T_VOID, &quot;missing Half&quot; );
1036       regs[i].set2(VMRegImpl::stack2reg(stack));
1037       stack += 2;
1038       break;
1039     case T_VOID: regs[i].set_bad(); break;
1040     default:
1041       ShouldNotReachHere();
1042       break;
1043     }
1044   }
1045   return stack;
</pre>
<hr />
<pre>
1287       switch (in_sig_bt[i]) {
1288         case T_ARRAY:
1289           if (map != NULL) {
1290             __ movptr(Address(rsp, offset), reg);
1291           } else {
1292             __ movptr(reg, Address(rsp, offset));
1293           }
1294           break;
1295         case T_BOOLEAN:
1296         case T_CHAR:
1297         case T_BYTE:
1298         case T_SHORT:
1299         case T_INT:
1300           if (map != NULL) {
1301             __ movl(Address(rsp, offset), reg);
1302           } else {
1303             __ movl(reg, Address(rsp, offset));
1304           }
1305           break;
1306         case T_OBJECT:
<span class="line-added">1307         case T_VALUETYPE:</span>
1308         default: ShouldNotReachHere();
1309       }
1310     } else if (in_regs[i].first()-&gt;is_XMMRegister()) {
1311       if (in_sig_bt[i] == T_FLOAT) {
1312         int slot = handle_index++ * VMRegImpl::slots_per_word + arg_save_area;
1313         int offset = slot * VMRegImpl::stack_slot_size;
1314         assert(handle_index &lt;= stack_slots, &quot;overflow&quot;);
1315         if (map != NULL) {
1316           __ movflt(Address(rsp, offset), in_regs[i].first()-&gt;as_XMMRegister());
1317         } else {
1318           __ movflt(in_regs[i].first()-&gt;as_XMMRegister(), Address(rsp, offset));
1319         }
1320       }
1321     } else if (in_regs[i].first()-&gt;is_stack()) {
1322       if (in_sig_bt[i] == T_ARRAY &amp;&amp; map != NULL) {
1323         int offset_in_older_frame = in_regs[i].first()-&gt;reg2stack() + SharedRuntime::out_preserve_stack_slots();
1324         map-&gt;set_oop(VMRegImpl::stack2reg(offset_in_older_frame + stack_slots));
1325       }
1326     }
1327   }
</pre>
<hr />
<pre>
2005             pinned_args.append(i);
2006 
2007             // rax has pinned array
2008             VMRegPair result_reg(rax-&gt;as_VMReg());
2009             if (!in_arg.first()-&gt;is_stack()) {
2010               assert(pinned_slot &lt;= stack_slots, &quot;overflow&quot;);
2011               simple_move32(masm, result_reg, VMRegImpl::stack2reg(pinned_slot));
2012               pinned_slot += VMRegImpl::slots_per_word;
2013             } else {
2014               // Write back pinned value, it will be used to unpin this argument
2015               __ movptr(Address(rbp, reg2offset_in(in_arg.first())), result_reg.first()-&gt;as_Register());
2016             }
2017             // We have the array in register, use it
2018             in_arg = result_reg;
2019           }
2020 
2021           unpack_array_argument(masm, in_arg, in_elem_bt[i], out_regs[c_arg + 1], out_regs[c_arg]);
2022           c_arg++;
2023           break;
2024         }
<span class="line-added">2025       case T_VALUETYPE:</span>
2026       case T_OBJECT:
2027         assert(!is_critical_native, &quot;no oop arguments&quot;);
2028         object_move(masm, map, oop_handle_offset, stack_slots, in_regs[i], out_regs[c_arg],
2029                     ((i == 0) &amp;&amp; (!is_static)),
2030                     &amp;receiver_offset);
2031         break;
2032       case T_VOID:
2033         break;
2034 
2035       case T_FLOAT:
2036         float_move(masm, in_regs[i], out_regs[c_arg]);
2037           break;
2038 
2039       case T_DOUBLE:
2040         assert( i + 1 &lt; total_in_args &amp;&amp;
2041                 in_sig_bt[i + 1] == T_VOID &amp;&amp;
2042                 out_sig_bt[c_arg+1] == T_VOID, &quot;bad arg list&quot;);
2043         double_move(masm, in_regs[i], out_regs[c_arg]);
2044         break;
2045 
</pre>
<hr />
<pre>
2188   // Verify or restore cpu control state after JNI call
2189   __ restore_cpu_control_state_after_jni();
2190 
2191   // WARNING - on Windows Java Natives use pascal calling convention and pop the
2192   // arguments off of the stack. We could just re-adjust the stack pointer here
2193   // and continue to do SP relative addressing but we instead switch to FP
2194   // relative addressing.
2195 
2196   // Unpack native results.
2197   switch (ret_type) {
2198   case T_BOOLEAN: __ c2bool(rax);            break;
2199   case T_CHAR   : __ andptr(rax, 0xFFFF);    break;
2200   case T_BYTE   : __ sign_extend_byte (rax); break;
2201   case T_SHORT  : __ sign_extend_short(rax); break;
2202   case T_INT    : /* nothing to do */        break;
2203   case T_DOUBLE :
2204   case T_FLOAT  :
2205     // Result is in st0 we&#39;ll save as needed
2206     break;
2207   case T_ARRAY:                 // Really a handle
<span class="line-added">2208   case T_VALUETYPE:             // Really a handle</span>
2209   case T_OBJECT:                // Really a handle
2210       break; // can&#39;t de-handlize until after safepoint check
2211   case T_VOID: break;
2212   case T_LONG: break;
2213   default       : ShouldNotReachHere();
2214   }
2215 
2216   // unpin pinned arguments
2217   pinned_slot = oop_handle_offset;
2218   if (pinned_args.length() &gt; 0) {
2219     // save return value that may be overwritten otherwise.
2220     save_native_result(masm, ret_type, stack_slots);
2221     for (int index = 0; index &lt; pinned_args.length(); index ++) {
2222       int i = pinned_args.at(index);
2223       assert(pinned_slot &lt;= stack_slots, &quot;overflow&quot;);
2224       if (!in_regs[i].first()-&gt;is_stack()) {
2225         int offset = pinned_slot * VMRegImpl::stack_slot_size;
2226         __ movl(in_regs[i].first()-&gt;as_Register(), Address(rsp, offset));
2227         pinned_slot += VMRegImpl::slots_per_word;
2228       }
</pre>
<hr />
<pre>
3307 
3308   __ bind(pending);
3309 
3310   RegisterSaver::restore_live_registers(masm);
3311 
3312   // exception pending =&gt; remove activation and forward to exception handler
3313 
3314   __ get_thread(thread);
3315   __ movptr(Address(thread, JavaThread::vm_result_offset()), NULL_WORD);
3316   __ movptr(rax, Address(thread, Thread::pending_exception_offset()));
3317   __ jump(RuntimeAddress(StubRoutines::forward_exception_entry()));
3318 
3319   // -------------
3320   // make sure all code is generated
3321   masm-&gt;flush();
3322 
3323   // return the  blob
3324   // frame_size_words or bytes??
3325   return RuntimeStub::new_runtime_stub(name, &amp;buffer, frame_complete, frame_size_words, oop_maps, true);
3326 }
<span class="line-added">3327 </span>
<span class="line-added">3328 BufferedValueTypeBlob* SharedRuntime::generate_buffered_value_type_adapter(const ValueKlass* vk) {</span>
<span class="line-added">3329   Unimplemented();</span>
<span class="line-added">3330   return NULL;</span>
<span class="line-added">3331 }</span>
</pre>
</td>
</tr>
</table>
<center><a href="methodHandles_x86.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="sharedRuntime_x86_64.cpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>