diff a/.hgtags b/.hgtags
--- a/.hgtags
+++ b/.hgtags
@@ -636,5 +636,6 @@
 dd5198db2e5b1ebcafe065d987c03ba9fcb50fc3 jdk-15+17
 44aef192b488a48cce12422394691a6b1d16b98e jdk-15+18
 7cc27caabe6e342151e8baf549beb07a9c755ec2 jdk-15+19
 46bca5e5e6fb26efd07245d26fe96a9c3260f51e jdk-15+20
 12b55fad80f30d24b1f8fdb3b947ea6465ef9518 jdk-15+21
+7223c6d610343fd8323af9d07d501e01fa1a7696 jdk-15+22
diff a/src/hotspot/cpu/aarch64/aarch64.ad b/src/hotspot/cpu/aarch64/aarch64.ad
--- a/src/hotspot/cpu/aarch64/aarch64.ad
+++ b/src/hotspot/cpu/aarch64/aarch64.ad
@@ -10659,34 +10659,10 @@
 
   ins_encode(aarch64_enc_divw(dst, src1, src2));
   ins_pipe(idiv_reg_reg);
 %}
 
-instruct signExtract(iRegINoSp dst, iRegIorL2I src1, immI_31 div1, immI_31 div2) %{
-  match(Set dst (URShiftI (RShiftI src1 div1) div2));
-  ins_cost(INSN_COST);
-  format %{ "lsrw $dst, $src1, $div1" %}
-  ins_encode %{
-    __ lsrw(as_Register($dst$$reg), as_Register($src1$$reg), 31);
-  %}
-  ins_pipe(ialu_reg_shift);
-%}
-
-instruct div2Round(iRegINoSp dst, iRegIorL2I src, immI_31 div1, immI_31 div2) %{
-  match(Set dst (AddI src (URShiftI (RShiftI src div1) div2)));
-  ins_cost(INSN_COST);
-  format %{ "addw $dst, $src, LSR $div1" %}
-
-  ins_encode %{
-    __ addw(as_Register($dst$$reg),
-              as_Register($src$$reg),
-              as_Register($src$$reg),
-              Assembler::LSR, 31);
-  %}
-  ins_pipe(ialu_reg);
-%}
-
 // Long Divide
 
 instruct divL(iRegLNoSp dst, iRegL src1, iRegL src2) %{
   match(Set dst (DivL src1 src2));
 
@@ -10695,34 +10671,10 @@
 
   ins_encode(aarch64_enc_div(dst, src1, src2));
   ins_pipe(ldiv_reg_reg);
 %}
 
-instruct signExtractL(iRegLNoSp dst, iRegL src1, immI_63 div1, immI_63 div2) %{
-  match(Set dst (URShiftL (RShiftL src1 div1) div2));
-  ins_cost(INSN_COST);
-  format %{ "lsr $dst, $src1, $div1" %}
-  ins_encode %{
-    __ lsr(as_Register($dst$$reg), as_Register($src1$$reg), 63);
-  %}
-  ins_pipe(ialu_reg_shift);
-%}
-
-instruct div2RoundL(iRegLNoSp dst, iRegL src, immI_63 div1, immI_63 div2) %{
-  match(Set dst (AddL src (URShiftL (RShiftL src div1) div2)));
-  ins_cost(INSN_COST);
-  format %{ "add $dst, $src, $div1" %}
-
-  ins_encode %{
-    __ add(as_Register($dst$$reg),
-              as_Register($src$$reg),
-              as_Register($src$$reg),
-              Assembler::LSR, 63);
-  %}
-  ins_pipe(ialu_reg);
-%}
-
 // Integer Remainder
 
 instruct modI(iRegINoSp dst, iRegIorL2I src1, iRegIorL2I src2) %{
   match(Set dst (ModI src1 src2));
 
@@ -15401,13 +15353,13 @@
 
   ins_cost(CALL_COST);
   format %{ "ShouldNotReachHere" %}
 
   ins_encode %{
-    // +1 so NativeInstruction::is_sigill_zombie_not_entrant() doesn't
-    // return true
-    __ dpcs1(0xdead + 1);
+    if (is_reachable()) {
+      __ dpcs1(0xdead + 1);
+    }
   %}
 
   ins_pipe(pipe_class_default);
 %}
 
diff a/src/hotspot/cpu/x86/c1_LIRAssembler_x86.cpp b/src/hotspot/cpu/x86/c1_LIRAssembler_x86.cpp
--- a/src/hotspot/cpu/x86/c1_LIRAssembler_x86.cpp
+++ b/src/hotspot/cpu/x86/c1_LIRAssembler_x86.cpp
@@ -1214,10 +1214,11 @@
   assert(src->is_address(), "should not call otherwise");
   assert(dest->is_register(), "should not call otherwise");
 
   LIR_Address* addr = src->as_address_ptr();
   Address from_addr = as_Address(addr);
+  Register tmp_load_klass = LP64_ONLY(rscratch1) NOT_LP64(noreg);
 
   if (addr->base()->type() == T_OBJECT || addr->base()->type() == T_VALUETYPE) {
     __ verify_oop(addr->base()->as_pointer_register());
   }
 
@@ -1400,11 +1401,11 @@
       __ verify_oop(dest->as_register());
     }
   } else if (type == T_ADDRESS && addr->disp() == oopDesc::klass_offset_in_bytes()) {
 #ifdef _LP64
     if (UseCompressedClassPointers) {
-      __ decode_klass_not_null(dest->as_register());
+      __ decode_klass_not_null(dest->as_register(), tmp_load_klass);
     }
 #endif
   }
 }
 
@@ -1728,10 +1729,11 @@
   Register k_RInfo = op->tmp1()->as_register();
   Register klass_RInfo = op->tmp2()->as_register();
   Register dst = op->result_opr()->as_register();
   ciKlass* k = op->klass();
   Register Rtmp1 = noreg;
+  Register tmp_load_klass = LP64_ONLY(rscratch1) NOT_LP64(noreg);
 
   // check if it needs to be profiled
   ciMethodData* md = NULL;
   ciProfileData* data = NULL;
 
@@ -1793,11 +1795,11 @@
   if (op->fast_check()) {
     // get object class
     // not a safepoint as obj null check happens earlier
 #ifdef _LP64
     if (UseCompressedClassPointers) {
-      __ load_klass(Rtmp1, obj);
+      __ load_klass(Rtmp1, obj, tmp_load_klass);
       __ cmpptr(k_RInfo, Rtmp1);
     } else {
       __ cmpptr(k_RInfo, Address(obj, oopDesc::klass_offset_in_bytes()));
     }
 #else
@@ -1810,11 +1812,11 @@
     __ jcc(Assembler::notEqual, *failure_target);
     // successful cast, fall through to profile or jump
   } else {
     // get object class
     // not a safepoint as obj null check happens earlier
-    __ load_klass(klass_RInfo, obj);
+    __ load_klass(klass_RInfo, obj, tmp_load_klass);
     if (k->is_loaded()) {
       // See if we get an immediate positive hit
 #ifdef _LP64
       __ cmpptr(k_RInfo, Address(klass_RInfo, k->super_check_offset()));
 #else
@@ -1865,11 +1867,11 @@
   }
   if (op->should_profile()) {
     Register mdo  = klass_RInfo, recv = k_RInfo;
     __ bind(profile_cast_success);
     __ mov_metadata(mdo, md->constant_encoding());
-    __ load_klass(recv, obj);
+    __ load_klass(recv, obj, tmp_load_klass);
     type_profile_helper(mdo, md, data, recv, success);
     __ jmp(*success);
 
     __ bind(profile_cast_failure);
     __ mov_metadata(mdo, md->constant_encoding());
@@ -1880,10 +1882,11 @@
   __ jmp(*success);
 }
 
 
 void LIR_Assembler::emit_opTypeCheck(LIR_OpTypeCheck* op) {
+  Register tmp_load_klass = LP64_ONLY(rscratch1) NOT_LP64(noreg);
   LIR_Code code = op->code();
   if (code == lir_store_check) {
     Register value = op->object()->as_register();
     Register array = op->array()->as_register();
     Register k_RInfo = op->tmp1()->as_register();
@@ -1925,12 +1928,12 @@
     } else {
       __ jcc(Assembler::equal, done);
     }
 
     add_debug_info_for_null_check_here(op->info_for_exception());
-    __ load_klass(k_RInfo, array);
-    __ load_klass(klass_RInfo, value);
+    __ load_klass(k_RInfo, array, tmp_load_klass);
+    __ load_klass(klass_RInfo, value, tmp_load_klass);
 
     // get instance klass (it's already uncompressed)
     __ movptr(k_RInfo, Address(k_RInfo, ObjArrayKlass::element_klass_offset()));
     // perform the fast part of the checking logic
     __ check_klass_subtype_fast_path(klass_RInfo, k_RInfo, Rtmp1, success_target, failure_target, NULL);
@@ -1947,11 +1950,11 @@
 
     if (op->should_profile()) {
       Register mdo  = klass_RInfo, recv = k_RInfo;
       __ bind(profile_cast_success);
       __ mov_metadata(mdo, md->constant_encoding());
-      __ load_klass(recv, value);
+      __ load_klass(recv, value, tmp_load_klass);
       type_profile_helper(mdo, md, data, recv, &done);
       __ jmpb(done);
 
       __ bind(profile_cast_failure);
       __ mov_metadata(mdo, md->constant_encoding());
@@ -1991,12 +1994,13 @@
 
 void LIR_Assembler::emit_opFlattenedArrayCheck(LIR_OpFlattenedArrayCheck* op) {
   // We are loading/storing from/to an array that *may* be flattened (the
   // declared type is Object[], abstract[], interface[] or VT.ref[]).
   // If this array is flattened, take the slow path.
+  Register tmp_load_klass = LP64_ONLY(rscratch1) NOT_LP64(noreg);
   Register klass = op->tmp()->as_register();
-  __ load_klass(klass, op->array()->as_register());
+  __ load_klass(klass, op->array()->as_register(), tmp_load_klass);
   __ movl(klass, Address(klass, Klass::layout_helper_offset()));
   __ testl(klass, Klass::_lh_array_tag_vt_value_bit_inplace);
   __ jcc(Assembler::notZero, *op->stub()->entry());
   if (!op->value()->is_illegal()) {
     // The array is not flattened, but it might be null-free. If we are storing
@@ -2011,12 +2015,13 @@
 }
 
 void LIR_Assembler::emit_opNullFreeArrayCheck(LIR_OpNullFreeArrayCheck* op) {
   // We are storing into an array that *may* be null-free (the declared type is
   // Object[], abstract[], interface[] or VT.ref[]).
+  Register tmp_load_klass = LP64_ONLY(rscratch1) NOT_LP64(noreg);
   Register klass = op->tmp()->as_register();
-  __ load_klass(klass, op->array()->as_register());
+  __ load_klass(klass, op->array()->as_register(), tmp_load_klass);
   __ movl(klass, Address(klass, Klass::layout_helper_offset()));
   __ testl(klass, Klass::_lh_null_free_bit_inplace);
 }
 
 void LIR_Assembler::emit_opSubstitutabilityCheck(LIR_OpSubstitutabilityCheck* op) {
@@ -3256,11 +3261,12 @@
 void LIR_Assembler::arraycopy_valuetype_check(Register obj, Register tmp, CodeStub* slow_path, bool is_dest, bool null_check) {
   if (null_check) {
     __ testptr(obj, obj);
     __ jcc(Assembler::zero, *slow_path->entry());
   }
-  __ load_klass(tmp, obj);
+  Register tmp_load_klass = LP64_ONLY(rscratch1) NOT_LP64(noreg);
+  __ load_klass(tmp, obj, tmp_load_klass);
   __ movl(tmp, Address(tmp, Klass::layout_helper_offset()));
   if (is_dest) {
     // We also take slow path if it's a null_free destination array, just in case the source array
     // contains NULLs.
     __ testl(tmp, Klass::_lh_null_free_bit_inplace);
@@ -3280,10 +3286,11 @@
   Register dst = op->dst()->as_register();
   Register src_pos = op->src_pos()->as_register();
   Register dst_pos = op->dst_pos()->as_register();
   Register length  = op->length()->as_register();
   Register tmp = op->tmp()->as_register();
+  Register tmp_load_klass = LP64_ONLY(rscratch1) NOT_LP64(noreg);
 
   __ resolve(ACCESS_READ, src);
   __ resolve(ACCESS_WRITE, dst);
 
   CodeStub* stub = op->stub();
@@ -3441,17 +3448,17 @@
   // If the compiler was not able to prove that exact type of the source or the destination
   // of the arraycopy is an array type, check at runtime if the source or the destination is
   // an instance type.
   if (flags & LIR_OpArrayCopy::type_check) {
     if (!(flags & LIR_OpArrayCopy::dst_objarray)) {
-      __ load_klass(tmp, dst);
+      __ load_klass(tmp, dst, tmp_load_klass);
       __ cmpl(Address(tmp, in_bytes(Klass::layout_helper_offset())), Klass::_lh_neutral_value);
       __ jcc(Assembler::greaterEqual, *stub->entry());
     }
 
     if (!(flags & LIR_OpArrayCopy::src_objarray)) {
-      __ load_klass(tmp, src);
+      __ load_klass(tmp, src, tmp_load_klass);
       __ cmpl(Address(tmp, in_bytes(Klass::layout_helper_offset())), Klass::_lh_neutral_value);
       __ jcc(Assembler::greaterEqual, *stub->entry());
     }
   }
 
@@ -3504,12 +3511,12 @@
       Label cont, slow;
 
       __ push(src);
       __ push(dst);
 
-      __ load_klass(src, src);
-      __ load_klass(dst, dst);
+      __ load_klass(src, src, tmp_load_klass);
+      __ load_klass(dst, dst, tmp_load_klass);
 
       __ check_klass_subtype_fast_path(src, dst, tmp, &cont, &slow, NULL);
 
       __ push(src);
       __ push(dst);
@@ -3533,13 +3540,13 @@
         if ((flags & mask) != mask) {
           // Check that at least both of them object arrays.
           assert(flags & mask, "one of the two should be known to be an object array");
 
           if (!(flags & LIR_OpArrayCopy::src_objarray)) {
-            __ load_klass(tmp, src);
+            __ load_klass(tmp, src, tmp_load_klass);
           } else if (!(flags & LIR_OpArrayCopy::dst_objarray)) {
-            __ load_klass(tmp, dst);
+            __ load_klass(tmp, dst, tmp_load_klass);
           }
           int lh_offset = in_bytes(Klass::layout_helper_offset());
           Address klass_lh_addr(tmp, lh_offset);
           jint objArray_lh = Klass::array_layout_helper(T_OBJECT);
           __ cmpl(klass_lh_addr, objArray_lh);
@@ -3579,18 +3586,18 @@
         assert_different_registers(c_rarg2, dst);
 
 #ifdef _WIN64
         // Allocate abi space for args but be sure to keep stack aligned
         __ subptr(rsp, 6*wordSize);
-        __ load_klass(c_rarg3, dst);
+        __ load_klass(c_rarg3, dst, tmp_load_klass);
         __ movptr(c_rarg3, Address(c_rarg3, ObjArrayKlass::element_klass_offset()));
         store_parameter(c_rarg3, 4);
         __ movl(c_rarg3, Address(c_rarg3, Klass::super_check_offset_offset()));
         __ call(RuntimeAddress(copyfunc_addr));
         __ addptr(rsp, 6*wordSize);
 #else
-        __ load_klass(c_rarg4, dst);
+        __ load_klass(c_rarg4, dst, tmp_load_klass);
         __ movptr(c_rarg4, Address(c_rarg4, ObjArrayKlass::element_klass_offset()));
         __ movl(c_rarg3, Address(c_rarg4, Klass::super_check_offset_offset()));
         __ call(RuntimeAddress(copyfunc_addr));
 #endif
 
@@ -3651,11 +3658,11 @@
     // but not necessarily exactly of type default_type.
     Label known_ok, halt;
     __ mov_metadata(tmp, default_type->constant_encoding());
 #ifdef _LP64
     if (UseCompressedClassPointers) {
-      __ encode_klass_not_null(tmp);
+      __ encode_klass_not_null(tmp, rscratch1);
     }
 #endif
 
     if (basic_type != T_OBJECT) {
 
@@ -3756,10 +3763,11 @@
 
 void LIR_Assembler::emit_profile_call(LIR_OpProfileCall* op) {
   ciMethod* method = op->profiled_method();
   int bci          = op->profiled_bci();
   ciMethod* callee = op->profiled_callee();
+  Register tmp_load_klass = LP64_ONLY(rscratch1) NOT_LP64(noreg);
 
   // Update counter for all call types
   ciMethodData* md = method->method_data_or_null();
   assert(md != NULL, "Sanity");
   ciProfileData* data = md->bci_to_data(bci);
@@ -3808,11 +3816,11 @@
           __ addptr(data_addr, DataLayout::counter_increment);
           return;
         }
       }
     } else {
-      __ load_klass(recv, recv);
+      __ load_klass(recv, recv, tmp_load_klass);
       Label update_done;
       type_profile_helper(mdo, md, data, recv, &update_done);
       // Receiver did not match any saved receiver and there is no empty row for it.
       // Increment total counter to indicate polymorphic case.
       __ addptr(counter_addr, DataLayout::counter_increment);
@@ -3826,10 +3834,11 @@
 }
 
 void LIR_Assembler::emit_profile_type(LIR_OpProfileType* op) {
   Register obj = op->obj()->as_register();
   Register tmp = op->tmp()->as_pointer_register();
+  Register tmp_load_klass = LP64_ONLY(rscratch1) NOT_LP64(noreg);
   Address mdo_addr = as_Address(op->mdp()->as_address_ptr());
   ciKlass* exact_klass = op->exact_klass();
   intptr_t current_klass = op->current_klass();
   bool not_null = op->not_null();
   bool no_conflict = op->no_conflict();
@@ -3872,11 +3881,11 @@
 
   if (do_update) {
 #ifdef ASSERT
     if (exact_klass != NULL) {
       Label ok;
-      __ load_klass(tmp, tmp);
+      __ load_klass(tmp, tmp, tmp_load_klass);
       __ push(tmp);
       __ mov_metadata(tmp, exact_klass->constant_encoding());
       __ cmpptr(tmp, Address(rsp, 0));
       __ jcc(Assembler::equal, ok);
       __ stop("exact klass and actual klass differ");
@@ -3887,11 +3896,11 @@
     if (!no_conflict) {
       if (exact_klass == NULL || TypeEntries::is_type_none(current_klass)) {
         if (exact_klass != NULL) {
           __ mov_metadata(tmp, exact_klass->constant_encoding());
         } else {
-          __ load_klass(tmp, tmp);
+          __ load_klass(tmp, tmp, tmp_load_klass);
         }
 
         __ xorptr(tmp, mdo_addr);
         __ testptr(tmp, TypeEntries::type_klass_mask);
         // klass seen before, nothing to do. The unknown bit may have been
diff a/src/hotspot/cpu/x86/c1_MacroAssembler_x86.cpp b/src/hotspot/cpu/x86/c1_MacroAssembler_x86.cpp
--- a/src/hotspot/cpu/x86/c1_MacroAssembler_x86.cpp
+++ b/src/hotspot/cpu/x86/c1_MacroAssembler_x86.cpp
@@ -52,11 +52,12 @@
   // save object being locked into the BasicObjectLock
   movptr(Address(disp_hdr, BasicObjectLock::obj_offset_in_bytes()), obj);
 
   if (UseBiasedLocking) {
     assert(scratch != noreg, "should have scratch register at this point");
-    null_check_offset = biased_locking_enter(disp_hdr, obj, hdr, scratch, false, done, &slow_case);
+    Register rklass_decode_tmp = LP64_ONLY(rscratch1) NOT_LP64(noreg);
+    null_check_offset = biased_locking_enter(disp_hdr, obj, hdr, scratch, rklass_decode_tmp, false, done, &slow_case);
   } else {
     null_check_offset = offset();
   }
 
   // Load object header
@@ -153,10 +154,11 @@
 }
 
 
 void C1_MacroAssembler::initialize_header(Register obj, Register klass, Register len, Register t1, Register t2) {
   assert_different_registers(obj, klass, len);
+  Register tmp_encode_klass = LP64_ONLY(rscratch1) NOT_LP64(noreg);
   if ((UseBiasedLocking || EnableValhalla) && !len->is_valid()) {
     // Need to copy markWord::always_locked_pattern for values.
     assert_different_registers(obj, klass, len, t1, t2);
     movptr(t1, Address(klass, Klass::prototype_header_offset()));
     movptr(Address(obj, oopDesc::mark_offset_in_bytes()), t1);
@@ -165,11 +167,11 @@
     movptr(Address(obj, oopDesc::mark_offset_in_bytes ()), (int32_t)(intptr_t)markWord::prototype().value());
   }
 #ifdef _LP64
   if (UseCompressedClassPointers) { // Take care not to kill klass
     movptr(t1, klass);
-    encode_klass_not_null(t1);
+    encode_klass_not_null(t1, tmp_encode_klass);
     movl(Address(obj, oopDesc::klass_offset_in_bytes()), t1);
   } else
 #endif
   {
     movptr(Address(obj, oopDesc::klass_offset_in_bytes()), klass);
@@ -300,13 +302,14 @@
   verify_oop(receiver);
   // explicit NULL check not needed since load from [klass_offset] causes a trap
   // check against inline cache
   assert(!MacroAssembler::needs_explicit_null_check(oopDesc::klass_offset_in_bytes()), "must add explicit null check");
   int start_offset = offset();
+  Register tmp_load_klass = LP64_ONLY(rscratch2) NOT_LP64(noreg);
 
   if (UseCompressedClassPointers) {
-    load_klass(rscratch1, receiver);
+    load_klass(rscratch1, receiver, tmp_load_klass);
     cmpptr(rscratch1, iCache);
   } else {
     cmpptr(iCache, Address(receiver, oopDesc::klass_offset_in_bytes()));
   }
   // if icache check fails, then jump to runtime routine
diff a/src/hotspot/cpu/x86/c1_Runtime1_x86.cpp b/src/hotspot/cpu/x86/c1_Runtime1_x86.cpp
--- a/src/hotspot/cpu/x86/c1_Runtime1_x86.cpp
+++ b/src/hotspot/cpu/x86/c1_Runtime1_x86.cpp
@@ -1346,12 +1346,13 @@
         __ verify_oop(rax);
 #endif // _LP64
 
         // load the klass and check the has finalizer flag
         Label register_finalizer;
+        Register tmp_load_klass = LP64_ONLY(rscratch1) NOT_LP64(noreg);
         Register t = rsi;
-        __ load_klass(t, rax);
+        __ load_klass(t, rax, tmp_load_klass);
         __ movl(t, Address(t, Klass::access_flags_offset()));
         __ testl(t, JVM_ACC_HAS_FINALIZER);
         __ jcc(Assembler::notZero, register_finalizer);
         __ ret(0);
 
diff a/src/hotspot/cpu/x86/c2_MacroAssembler_x86.cpp b/src/hotspot/cpu/x86/c2_MacroAssembler_x86.cpp
--- a/src/hotspot/cpu/x86/c2_MacroAssembler_x86.cpp
+++ b/src/hotspot/cpu/x86/c2_MacroAssembler_x86.cpp
@@ -440,11 +440,10 @@
   assert(tmpReg == rax, "");
 
   if (use_rtm) {
     assert_different_registers(objReg, boxReg, tmpReg, scrReg, cx1Reg, cx2Reg);
   } else {
-    assert(cx1Reg == noreg, "");
     assert(cx2Reg == noreg, "");
     assert_different_registers(objReg, boxReg, tmpReg, scrReg);
   }
 
   if (counters != NULL) {
@@ -476,11 +475,11 @@
   // order to reduce the number of conditional branches in the most common cases.
   // Beware -- there's a subtle invariant that fetch of the markword
   // at [FETCH], below, will never observe a biased encoding (*101b).
   // If this invariant is not held we risk exclusion (safety) failure.
   if (UseBiasedLocking && !UseOptoBiasInlining) {
-    biased_locking_enter(boxReg, objReg, tmpReg, scrReg, false, DONE_LABEL, NULL, counters);
+    biased_locking_enter(boxReg, objReg, tmpReg, scrReg, cx1Reg, false, DONE_LABEL, NULL, counters);
   }
 
 #if INCLUDE_RTM_OPT
   if (UseRTMForStackLocks && use_rtm) {
     rtm_stack_locking(objReg, tmpReg, scrReg, cx2Reg,
diff a/src/hotspot/cpu/x86/gc/shenandoah/shenandoahBarrierSetAssembler_x86.cpp b/src/hotspot/cpu/x86/gc/shenandoah/shenandoahBarrierSetAssembler_x86.cpp
--- a/src/hotspot/cpu/x86/gc/shenandoah/shenandoahBarrierSetAssembler_x86.cpp
+++ b/src/hotspot/cpu/x86/gc/shenandoah/shenandoahBarrierSetAssembler_x86.cpp
@@ -26,13 +26,13 @@
 #include "gc/shenandoah/shenandoahBarrierSet.hpp"
 #include "gc/shenandoah/shenandoahBarrierSetAssembler.hpp"
 #include "gc/shenandoah/shenandoahForwarding.hpp"
 #include "gc/shenandoah/shenandoahHeap.inline.hpp"
 #include "gc/shenandoah/shenandoahHeapRegion.hpp"
-#include "gc/shenandoah/shenandoahHeuristics.hpp"
 #include "gc/shenandoah/shenandoahRuntime.hpp"
 #include "gc/shenandoah/shenandoahThreadLocalData.hpp"
+#include "gc/shenandoah/heuristics/shenandoahHeuristics.hpp"
 #include "interpreter/interpreter.hpp"
 #include "interpreter/interp_masm.hpp"
 #include "runtime/sharedRuntime.hpp"
 #include "runtime/thread.hpp"
 #include "utilities/macros.hpp"
diff a/src/hotspot/cpu/x86/interp_masm_x86.cpp b/src/hotspot/cpu/x86/interp_masm_x86.cpp
--- a/src/hotspot/cpu/x86/interp_masm_x86.cpp
+++ b/src/hotspot/cpu/x86/interp_masm_x86.cpp
@@ -58,11 +58,12 @@
   jccb(Assembler::notZero, update);
   orptr(mdo_addr, TypeEntries::null_seen);
   jmpb(next);
 
   bind(update);
-  load_klass(obj, obj);
+  Register tmp_load_klass = LP64_ONLY(rscratch1) NOT_LP64(noreg);
+  load_klass(obj, obj, tmp_load_klass);
 
   xorptr(obj, mdo_addr);
   testptr(obj, TypeEntries::type_klass_mask);
   jccb(Assembler::zero, next); // klass seen before, nothing to
                                // do. The unknown bit may have been
@@ -1165,11 +1166,12 @@
     // We are returning a value type, load its fields into registers
 #ifndef _LP64
     super_call_VM_leaf(StubRoutines::load_value_type_fields_in_regs());
 #else
     // Load fields from a buffered value with a value class specific handler
-    load_klass(rdi, rax);
+    Register tmp_load_klass = LP64_ONLY(rscratch1) NOT_LP64(noreg);
+    load_klass(rdi, rax, tmp_load_klass);
     movptr(rdi, Address(rdi, InstanceKlass::adr_valueklass_fixed_block_offset()));
     movptr(rdi, Address(rdi, ValueKlass::unpack_handler_offset()));
 
     testptr(rdi, rdi);
     jcc(Assembler::equal, skip);
@@ -1268,11 +1270,12 @@
   const Register elem_klass = t1;
   const Register alloc_temp = LP64_ONLY(rscratch1) NOT_LP64(rsi);
   const Register dst_temp   = LP64_ONLY(rscratch2) NOT_LP64(rdi);
 
   // load in array->klass()->element_klass()
-  load_klass(array_klass, array);
+  Register tmp_load_klass = LP64_ONLY(rscratch1) NOT_LP64(noreg);
+  load_klass(array_klass, array, tmp_load_klass);
   movptr(elem_klass, Address(array_klass, ArrayKlass::element_klass_offset()));
 
   //check for empty value klass
   test_klass_is_empty_value(elem_klass, dst_temp, empty_value);
 
@@ -1337,11 +1340,12 @@
 
     // Load object pointer into obj_reg
     movptr(obj_reg, Address(lock_reg, obj_offset));
 
     if (UseBiasedLocking) {
-      biased_locking_enter(lock_reg, obj_reg, swap_reg, tmp_reg, false, done, &slow_case);
+      Register rklass_decode_tmp = LP64_ONLY(rscratch1) NOT_LP64(noreg);
+      biased_locking_enter(lock_reg, obj_reg, swap_reg, tmp_reg, rklass_decode_tmp, false, done, &slow_case);
     }
 
     // Load immediate 1 into swap_reg %rax
     movl(swap_reg, (int32_t)1);
 
diff a/src/hotspot/cpu/x86/macroAssembler_x86.cpp b/src/hotspot/cpu/x86/macroAssembler_x86.cpp
--- a/src/hotspot/cpu/x86/macroAssembler_x86.cpp
+++ b/src/hotspot/cpu/x86/macroAssembler_x86.cpp
@@ -1087,10 +1087,11 @@
 
 int MacroAssembler::biased_locking_enter(Register lock_reg,
                                          Register obj_reg,
                                          Register swap_reg,
                                          Register tmp_reg,
+                                         Register tmp_reg2,
                                          bool swap_reg_contains_mark,
                                          Label& done,
                                          Label* slow_case,
                                          BiasedLockingCounters* counters) {
   assert(UseBiasedLocking, "why call this otherwise?");
@@ -1131,11 +1132,11 @@
   movptr(saved_mark_addr, swap_reg);
 #endif
   if (swap_reg_contains_mark) {
     null_check_offset = offset();
   }
-  load_prototype_header(tmp_reg, obj_reg);
+  load_prototype_header(tmp_reg, obj_reg, tmp_reg2);
 #ifdef _LP64
   orptr(tmp_reg, r15_thread);
   xorptr(tmp_reg, swap_reg);
   Register header_reg = tmp_reg;
 #else
@@ -1217,11 +1218,11 @@
   // bias in the current epoch. In other words, we allow transfer of
   // the bias from one thread to another directly in this situation.
   //
   // FIXME: due to a lack of registers we currently blow away the age
   // bits in this situation. Should attempt to preserve them.
-  load_prototype_header(tmp_reg, obj_reg);
+  load_prototype_header(tmp_reg, obj_reg, tmp_reg2);
 #ifdef _LP64
   orptr(tmp_reg, r15_thread);
 #else
   get_thread(swap_reg);
   orptr(tmp_reg, swap_reg);
@@ -1252,11 +1253,11 @@
   // normal locking code.
   //
   // FIXME: due to a lack of registers we currently blow away the age
   // bits in this situation. Should attempt to preserve them.
   NOT_LP64( movptr(swap_reg, saved_mark_addr); )
-  load_prototype_header(tmp_reg, obj_reg);
+  load_prototype_header(tmp_reg, obj_reg, tmp_reg2);
   lock();
   cmpxchgptr(tmp_reg, mark_addr); // compare tmp_reg and swap_reg
   // Fall through to the normal CAS-based lock, because no matter what
   // the result of the above CAS, some thread must have succeeded in
   // removing the bias bit from the object's header.
@@ -1514,11 +1515,11 @@
   assert(number_of_arguments >= 0   , "cannot have negative number of arguments");
   LP64_ONLY(assert(java_thread == r15_thread, "unexpected register"));
 #ifdef ASSERT
   // TraceBytecodes does not use r12 but saves it over the call, so don't verify
   // r12 is the heapbase.
-  LP64_ONLY(if ((UseCompressedOops || UseCompressedClassPointers) && !TraceBytecodes) verify_heapbase("call_VM_base: heap base corrupted?");)
+  LP64_ONLY(if (UseCompressedOops && !TraceBytecodes) verify_heapbase("call_VM_base: heap base corrupted?");)
 #endif // ASSERT
 
   assert(java_thread != oop_result  , "cannot use the same register for java_thread & oop_result");
   assert(java_thread != last_java_sp, "cannot use the same register for java_thread & last_java_sp");
 
@@ -2659,30 +2660,34 @@
   jcc(Assembler::notZero, is_flattened);
 }
 
 void MacroAssembler::test_flattened_array_oop(Register oop, Register temp_reg,
                                               Label&is_flattened_array) {
-  load_klass(temp_reg, oop);
+  Register tmp_load_klass = LP64_ONLY(rscratch1) NOT_LP64(noreg);
+  load_klass(temp_reg, oop, tmp_load_klass);
   movl(temp_reg, Address(temp_reg, Klass::layout_helper_offset()));
   test_flattened_array_layout(temp_reg, is_flattened_array);
 }
 
 void MacroAssembler::test_non_flattened_array_oop(Register oop, Register temp_reg,
                                                   Label&is_non_flattened_array) {
-  load_klass(temp_reg, oop);
+  Register tmp_load_klass = LP64_ONLY(rscratch1) NOT_LP64(noreg);
+  load_klass(temp_reg, oop, tmp_load_klass);
   movl(temp_reg, Address(temp_reg, Klass::layout_helper_offset()));
   test_non_flattened_array_layout(temp_reg, is_non_flattened_array);
 }
 
 void MacroAssembler::test_null_free_array_oop(Register oop, Register temp_reg, Label&is_null_free_array) {
-  load_klass(temp_reg, oop);
+  Register tmp_load_klass = LP64_ONLY(rscratch1) NOT_LP64(noreg);
+  load_klass(temp_reg, oop, tmp_load_klass);
   movl(temp_reg, Address(temp_reg, Klass::layout_helper_offset()));
   test_null_free_array_layout(temp_reg, is_null_free_array);
 }
 
 void MacroAssembler::test_non_null_free_array_oop(Register oop, Register temp_reg, Label&is_non_null_free_array) {
-  load_klass(temp_reg, oop);
+  Register tmp_load_klass = LP64_ONLY(rscratch1) NOT_LP64(noreg);
+  load_klass(temp_reg, oop, tmp_load_klass);
   movl(temp_reg, Address(temp_reg, Klass::layout_helper_offset()));
   test_non_null_free_array_layout(temp_reg, is_non_null_free_array);
 }
 
 void MacroAssembler::test_flattened_array_layout(Register lh, Label& is_flattened_array) {
@@ -3516,11 +3521,12 @@
 #ifdef _LP64
     xorl(rsi, rsi);                 // use zero reg to clear memory (shorter code)
     store_klass_gap(new_obj, rsi);  // zero klass gap for compressed oops
 #endif
     movptr(t2, klass);         // preserve klass
-    store_klass(new_obj, t2);  // src klass reg is potentially compressed
+    Register tmp_store_klass = LP64_ONLY(rscratch1) NOT_LP64(noreg);
+    store_klass(new_obj, t2, tmp_store_klass);  // src klass reg is potentially compressed
 
     jmp(done);
   }
 
   bind(slow_case);
@@ -4600,43 +4606,47 @@
 void MacroAssembler::load_method_holder_cld(Register rresult, Register rmethod) {
   load_method_holder(rresult, rmethod);
   movptr(rresult, Address(rresult, InstanceKlass::class_loader_data_offset()));
 }
 
+void MacroAssembler::load_method_holder(Register holder, Register method) {
+  movptr(holder, Address(method, Method::const_offset()));                      // ConstMethod*
+  movptr(holder, Address(holder, ConstMethod::constants_offset()));             // ConstantPool*
+  movptr(holder, Address(holder, ConstantPool::pool_holder_offset_in_bytes())); // InstanceKlass*
+}
+
 void MacroAssembler::load_metadata(Register dst, Register src) {
   if (UseCompressedClassPointers) {
     movl(dst, Address(src, oopDesc::klass_offset_in_bytes()));
   } else {
     movptr(dst, Address(src, oopDesc::klass_offset_in_bytes()));
   }
 }
 
-void MacroAssembler::load_method_holder(Register holder, Register method) {
-  movptr(holder, Address(method, Method::const_offset()));                      // ConstMethod*
-  movptr(holder, Address(holder, ConstMethod::constants_offset()));             // ConstantPool*
-  movptr(holder, Address(holder, ConstantPool::pool_holder_offset_in_bytes())); // InstanceKlass*
-}
-
-void MacroAssembler::load_klass(Register dst, Register src) {
+void MacroAssembler::load_klass(Register dst, Register src, Register tmp) {
+  assert_different_registers(src, tmp);
+  assert_different_registers(dst, tmp);
 #ifdef _LP64
   if (UseCompressedClassPointers) {
     movl(dst, Address(src, oopDesc::klass_offset_in_bytes()));
-    decode_klass_not_null(dst);
+    decode_klass_not_null(dst, tmp);
   } else
 #endif
   movptr(dst, Address(src, oopDesc::klass_offset_in_bytes()));
 }
 
-void MacroAssembler::load_prototype_header(Register dst, Register src) {
-  load_klass(dst, src);
+void MacroAssembler::load_prototype_header(Register dst, Register src, Register tmp) {
+  load_klass(dst, src, tmp);
   movptr(dst, Address(dst, Klass::prototype_header_offset()));
 }
 
-void MacroAssembler::store_klass(Register dst, Register src) {
+void MacroAssembler::store_klass(Register dst, Register src, Register tmp) {
+  assert_different_registers(src, tmp);
+  assert_different_registers(dst, tmp);
 #ifdef _LP64
   if (UseCompressedClassPointers) {
-    encode_klass_not_null(src);
+    encode_klass_not_null(src, tmp);
     movl(Address(dst, oopDesc::klass_offset_in_bytes()), src);
   } else
 #endif
     movptr(Address(dst, oopDesc::klass_offset_in_bytes()), src);
 }
@@ -4886,90 +4896,74 @@
       movq(dst, src);
     }
   }
 }
 
-void MacroAssembler::encode_klass_not_null(Register r) {
+void MacroAssembler::encode_klass_not_null(Register r, Register tmp) {
+  assert_different_registers(r, tmp);
   if (CompressedKlassPointers::base() != NULL) {
-    // Use r12 as a scratch register in which to temporarily load the narrow_klass_base.
-    assert(r != r12_heapbase, "Encoding a klass in r12");
-    mov64(r12_heapbase, (int64_t)CompressedKlassPointers::base());
-    subq(r, r12_heapbase);
+    mov64(tmp, (int64_t)CompressedKlassPointers::base());
+    subq(r, tmp);
   }
   if (CompressedKlassPointers::shift() != 0) {
     assert (LogKlassAlignmentInBytes == CompressedKlassPointers::shift(), "decode alg wrong");
     shrq(r, LogKlassAlignmentInBytes);
   }
-  if (CompressedKlassPointers::base() != NULL) {
-    reinit_heapbase();
-  }
-}
-
-void MacroAssembler::encode_klass_not_null(Register dst, Register src) {
-  if (dst == src) {
-    encode_klass_not_null(src);
-  } else {
-    if (CompressedKlassPointers::base() != NULL) {
-      mov64(dst, (int64_t)CompressedKlassPointers::base());
-      negq(dst);
-      addq(dst, src);
-    } else {
-      movptr(dst, src);
-    }
-    if (CompressedKlassPointers::shift() != 0) {
-      assert (LogKlassAlignmentInBytes == CompressedKlassPointers::shift(), "decode alg wrong");
-      shrq(dst, LogKlassAlignmentInBytes);
-    }
-  }
 }
 
-// Function instr_size_for_decode_klass_not_null() counts the instructions
-// generated by decode_klass_not_null(register r) and reinit_heapbase(),
-// when (Universe::heap() != NULL).  Hence, if the instructions they
-// generate change, then this method needs to be updated.
-int MacroAssembler::instr_size_for_decode_klass_not_null() {
-  assert (UseCompressedClassPointers, "only for compressed klass ptrs");
+void MacroAssembler::encode_and_move_klass_not_null(Register dst, Register src) {
+  assert_different_registers(src, dst);
   if (CompressedKlassPointers::base() != NULL) {
-    // mov64 + addq + shlq? + mov64  (for reinit_heapbase()).
-    return (CompressedKlassPointers::shift() == 0 ? 20 : 24);
+    mov64(dst, -(int64_t)CompressedKlassPointers::base());
+    addq(dst, src);
   } else {
-    // longest load decode klass function, mov64, leaq
-    return 16;
+    movptr(dst, src);
+  }
+  if (CompressedKlassPointers::shift() != 0) {
+    assert (LogKlassAlignmentInBytes == CompressedKlassPointers::shift(), "decode alg wrong");
+    shrq(dst, LogKlassAlignmentInBytes);
   }
 }
 
 // !!! If the instructions that get generated here change then function
 // instr_size_for_decode_klass_not_null() needs to get updated.
-void  MacroAssembler::decode_klass_not_null(Register r) {
+void  MacroAssembler::decode_klass_not_null(Register r, Register tmp) {
+  assert_different_registers(r, tmp);
   // Note: it will change flags
-  assert (UseCompressedClassPointers, "should only be used for compressed headers");
-  assert(r != r12_heapbase, "Decoding a klass in r12");
+  assert(UseCompressedClassPointers, "should only be used for compressed headers");
   // Cannot assert, unverified entry point counts instructions (see .ad file)
   // vtableStubs also counts instructions in pd_code_size_limit.
   // Also do not verify_oop as this is called by verify_oop.
   if (CompressedKlassPointers::shift() != 0) {
     assert(LogKlassAlignmentInBytes == CompressedKlassPointers::shift(), "decode alg wrong");
     shlq(r, LogKlassAlignmentInBytes);
   }
-  // Use r12 as a scratch register in which to temporarily load the narrow_klass_base.
-  if (CompressedKlassPointers::base() != NULL) {
-    mov64(r12_heapbase, (int64_t)CompressedKlassPointers::base());
-    addq(r, r12_heapbase);
+  if (CompressedKlassPointers::base() != NULL) {
+    mov64(tmp, (int64_t)CompressedKlassPointers::base());
     reinit_heapbase();
   }
 }
 
-void  MacroAssembler::decode_klass_not_null(Register dst, Register src) {
+void  MacroAssembler::decode_and_move_klass_not_null(Register dst, Register src) {
+  assert_different_registers(src, dst);
   // Note: it will change flags
   assert (UseCompressedClassPointers, "should only be used for compressed headers");
-  if (dst == src) {
-    decode_klass_not_null(dst);
+  // Cannot assert, unverified entry point counts instructions (see .ad file)
+  // vtableStubs also counts instructions in pd_code_size_limit.
+  // Also do not verify_oop as this is called by verify_oop.
+
+  if (CompressedKlassPointers::base() == NULL &&
+      CompressedKlassPointers::shift() == 0) {
+    // The best case scenario is that there is no base or shift. Then it is already
+    // a pointer that needs nothing but a register rename.
+    movl(dst, src);
   } else {
-    // Cannot assert, unverified entry point counts instructions (see .ad file)
-    // vtableStubs also counts instructions in pd_code_size_limit.
-    // Also do not verify_oop as this is called by verify_oop.
-    mov64(dst, (int64_t)CompressedKlassPointers::base());
+    if (CompressedKlassPointers::base() != NULL) {
+      mov64(dst, (int64_t)CompressedKlassPointers::base());
+    } else {
+      xorq(dst, dst);
+    }
     if (CompressedKlassPointers::shift() != 0) {
       assert(LogKlassAlignmentInBytes == CompressedKlassPointers::shift(), "decode alg wrong");
       assert(LogKlassAlignmentInBytes == Address::times_8, "klass not aligned on 64bits?");
       leaq(dst, Address(dst, src, Address::times_8, 0));
     } else {
@@ -5045,11 +5039,11 @@
   RelocationHolder rspec = metadata_Relocation::spec(klass_index);
   Assembler::cmp_narrow_oop(dst, CompressedKlassPointers::encode(k), rspec);
 }
 
 void MacroAssembler::reinit_heapbase() {
-  if (UseCompressedOops || UseCompressedClassPointers) {
+  if (UseCompressedOops) {
     if (Universe::heap() != NULL) {
       if (CompressedOops::base() == NULL) {
         MacroAssembler::xorptr(r12_heapbase, r12_heapbase);
       } else {
         mov64(r12_heapbase, (int64_t)CompressedOops::ptrs_base());
@@ -5251,11 +5245,12 @@
 
     if (vk == NULL) {
       // store_klass corrupts rbx, so save it in rax for later use (interpreter case only).
       mov(rax, rbx);
     }
-    store_klass(r13, rbx);  // klass
+    Register tmp_store_klass = LP64_ONLY(rscratch1) NOT_LP64(noreg);
+    store_klass(r13, rbx, tmp_store_klass);  // klass
 
     // We have our new buffered value, initialize its fields with a
     // value class specific handler
     if (vk != NULL) {
       // FIXME -- do the packing in-line to avoid the runtime call
diff a/src/hotspot/cpu/x86/macroAssembler_x86.hpp b/src/hotspot/cpu/x86/macroAssembler_x86.hpp
--- a/src/hotspot/cpu/x86/macroAssembler_x86.hpp
+++ b/src/hotspot/cpu/x86/macroAssembler_x86.hpp
@@ -343,12 +343,12 @@
 
   void load_method_holder(Register holder, Register method);
 
   // oop manipulations
   void load_metadata(Register dst, Register src);
-  void load_klass(Register dst, Register src);
-  void store_klass(Register dst, Register src);
+  void load_klass(Register dst, Register src, Register tmp);
+  void store_klass(Register dst, Register src, Register tmp);
 
   void access_load_at(BasicType type, DecoratorSet decorators, Register dst, Address src,
                       Register tmp1, Register thread_tmp);
   void access_store_at(BasicType type, DecoratorSet decorators, Address dst, Register src,
                        Register tmp1, Register tmp2, Register tmp3 = noreg);
@@ -376,11 +376,11 @@
 
   // Used for storing NULL. All other oop constants should be
   // stored using routines that take a jobject.
   void store_heap_oop_null(Address dst);
 
-  void load_prototype_header(Register dst, Register src);
+  void load_prototype_header(Register dst, Register src, Register tmp);
 
 #ifdef _LP64
   void store_klass_gap(Register dst, Register src);
 
   // This dummy is to prevent a call to store_heap_oop from
@@ -399,23 +399,19 @@
   void set_narrow_oop(Register dst, jobject obj);
   void set_narrow_oop(Address dst, jobject obj);
   void cmp_narrow_oop(Register dst, jobject obj);
   void cmp_narrow_oop(Address dst, jobject obj);
 
-  void encode_klass_not_null(Register r);
-  void decode_klass_not_null(Register r);
-  void encode_klass_not_null(Register dst, Register src);
-  void decode_klass_not_null(Register dst, Register src);
+  void encode_klass_not_null(Register r, Register tmp);
+  void decode_klass_not_null(Register r, Register tmp);
+  void encode_and_move_klass_not_null(Register dst, Register src);
+  void decode_and_move_klass_not_null(Register dst, Register src);
   void set_narrow_klass(Register dst, Klass* k);
   void set_narrow_klass(Address dst, Klass* k);
   void cmp_narrow_klass(Register dst, Klass* k);
   void cmp_narrow_klass(Address dst, Klass* k);
 
-  // Returns the byte size of the instructions generated by decode_klass_not_null()
-  // when compressed klass pointers are being used.
-  static int instr_size_for_decode_klass_not_null();
-
   // if heap base register is used - reinit it with the correct value
   void reinit_heapbase();
 
   DEBUG_ONLY(void verify_heapbase(const char* msg);)
 
@@ -721,11 +717,11 @@
   // check info (currently consumed only by C1). If
   // swap_reg_contains_mark is true then returns -1 as it is assumed
   // the calling code has already passed any potential faults.
   int biased_locking_enter(Register lock_reg, Register obj_reg,
                            Register swap_reg, Register tmp_reg,
-                           bool swap_reg_contains_mark,
+                           Register tmp_reg2, bool swap_reg_contains_mark,
                            Label& done, Label* slow_case = NULL,
                            BiasedLockingCounters* counters = NULL);
   void biased_locking_exit (Register obj_reg, Register temp_reg, Label& done);
 
   Condition negate_condition(Condition cond);
diff a/src/hotspot/cpu/x86/methodHandles_x86.cpp b/src/hotspot/cpu/x86/methodHandles_x86.cpp
--- a/src/hotspot/cpu/x86/methodHandles_x86.cpp
+++ b/src/hotspot/cpu/x86/methodHandles_x86.cpp
@@ -72,19 +72,19 @@
                                  const char* error_message) {
   InstanceKlass** klass_addr = SystemDictionary::well_known_klass_addr(klass_id);
   Klass* klass = SystemDictionary::well_known_klass(klass_id);
   Register temp = rdi;
   Register temp2 = noreg;
-  LP64_ONLY(temp2 = rscratch1);  // used by MacroAssembler::cmpptr
+  LP64_ONLY(temp2 = rscratch1);  // used by MacroAssembler::cmpptr and load_klass
   Label L_ok, L_bad;
   BLOCK_COMMENT("verify_klass {");
   __ verify_oop(obj);
   __ testptr(obj, obj);
   __ jcc(Assembler::zero, L_bad);
   __ push(temp); if (temp2 != noreg)  __ push(temp2);
 #define UNPUSH { if (temp2 != noreg)  __ pop(temp2);  __ pop(temp); }
-  __ load_klass(temp, obj);
+  __ load_klass(temp, obj, temp2);
   __ cmpptr(temp, ExternalAddress((address) klass_addr));
   __ jcc(Assembler::equal, L_ok);
   intptr_t super_check_offset = klass->super_check_offset();
   __ movptr(temp, Address(temp, super_check_offset));
   __ cmpptr(temp, ExternalAddress((address) klass_addr));
@@ -354,19 +354,19 @@
         // Don't actually load the klass; just null-check the receiver.
         __ null_check(receiver_reg);
       } else {
         // load receiver klass itself
         __ null_check(receiver_reg, oopDesc::klass_offset_in_bytes());
-        __ load_klass(temp1_recv_klass, receiver_reg);
+        __ load_klass(temp1_recv_klass, receiver_reg, temp2);
         __ verify_klass_ptr(temp1_recv_klass);
       }
       BLOCK_COMMENT("check_receiver {");
       // The receiver for the MemberName must be in receiver_reg.
       // Check the receiver against the MemberName.clazz
       if (VerifyMethodHandles && iid == vmIntrinsics::_linkToSpecial) {
         // Did not load it above...
-        __ load_klass(temp1_recv_klass, receiver_reg);
+        __ load_klass(temp1_recv_klass, receiver_reg, temp2);
         __ verify_klass_ptr(temp1_recv_klass);
       }
       if (VerifyMethodHandles && iid != vmIntrinsics::_linkToInterface) {
         Label L_ok;
         Register temp2_defc = temp2;
diff a/src/hotspot/cpu/x86/sharedRuntime_x86_32.cpp b/src/hotspot/cpu/x86/sharedRuntime_x86_32.cpp
--- a/src/hotspot/cpu/x86/sharedRuntime_x86_32.cpp
+++ b/src/hotspot/cpu/x86/sharedRuntime_x86_32.cpp
@@ -2126,11 +2126,11 @@
     // Load the oop from the handle
     __ movptr(obj_reg, Address(oop_handle_reg, 0));
 
     if (UseBiasedLocking) {
       // Note that oop_handle_reg is trashed during this call
-      __ biased_locking_enter(lock_reg, obj_reg, swap_reg, oop_handle_reg, false, lock_done, &slow_path_lock);
+      __ biased_locking_enter(lock_reg, obj_reg, swap_reg, oop_handle_reg, noreg, false, lock_done, &slow_path_lock);
     }
 
     // Load immediate 1 into swap_reg %rax,
     __ movptr(swap_reg, 1);
 
diff a/src/hotspot/cpu/x86/sharedRuntime_x86_64.cpp b/src/hotspot/cpu/x86/sharedRuntime_x86_64.cpp
--- a/src/hotspot/cpu/x86/sharedRuntime_x86_64.cpp
+++ b/src/hotspot/cpu/x86/sharedRuntime_x86_64.cpp
@@ -1155,11 +1155,11 @@
 
   Register holder = rax;
   Register receiver = j_rarg0;
   Register temp = rbx;
 
-  __ load_klass(temp, receiver);
+  __ load_klass(temp, receiver, rscratch1);
   __ cmpptr(temp, Address(holder, CompiledICHolder::holder_klass_offset()));
   __ movptr(rbx, Address(holder, CompiledICHolder::holder_metadata_offset()));
   __ jcc(Assembler::equal, ok);
   __ jump(RuntimeAddress(SharedRuntime::get_ic_miss_stub()));
 
@@ -2402,11 +2402,11 @@
   Label hit;
   Label exception_pending;
 
   assert_different_registers(ic_reg, receiver, rscratch1);
   __ verify_oop(receiver);
-  __ load_klass(rscratch1, receiver);
+  __ load_klass(rscratch1, receiver, rscratch2);
   __ cmpq(ic_reg, rscratch1);
   __ jcc(Assembler::equal, hit);
 
   __ jump(RuntimeAddress(SharedRuntime::get_ic_miss_stub()));
 
@@ -2747,11 +2747,11 @@
     // Load the oop from the handle
     __ movptr(obj_reg, Address(oop_handle_reg, 0));
 
     __ resolve(IS_NOT_NULL, obj_reg);
     if (UseBiasedLocking) {
-      __ biased_locking_enter(lock_reg, obj_reg, swap_reg, rscratch1, false, lock_done, &slow_path_lock);
+      __ biased_locking_enter(lock_reg, obj_reg, swap_reg, rscratch1, rscratch2, false, lock_done, &slow_path_lock);
     }
 
     // Load immediate 1 into swap_reg %rax
     __ movl(swap_reg, 1);
 
@@ -4421,11 +4421,11 @@
     j++;
   }
   assert(j == regs->length(), "missed a field?");
 
   if (StressInlineTypeReturnedAsFields) {
-    __ load_klass(rax, rax);
+    __ load_klass(rax, rax, rscratch1);
     __ orptr(rax, 1);
   }
 
   __ ret(0);
 
diff a/src/hotspot/cpu/x86/stubGenerator_x86_64.cpp b/src/hotspot/cpu/x86/stubGenerator_x86_64.cpp
--- a/src/hotspot/cpu/x86/stubGenerator_x86_64.cpp
+++ b/src/hotspot/cpu/x86/stubGenerator_x86_64.cpp
@@ -1096,14 +1096,11 @@
     __ andptr(c_rarg2, c_rarg3);
     __ movptr(c_rarg3, (intptr_t) Universe::verify_oop_bits());
     __ cmpptr(c_rarg2, c_rarg3);
     __ jcc(Assembler::notZero, error);
 
-    // set r12 to heapbase for load_klass()
-    __ reinit_heapbase();
-
-    // make sure klass is 'reasonable', which is not zero.
+    // make sure klass is 'reasonable', which is not zero.
     __ load_klass(rax, rax);  // get klass
     __ testptr(rax, rax);
     __ jcc(Assembler::zero, error); // if klass is NULL it is broken
 
     // return if everything seems ok
@@ -2538,11 +2535,11 @@
     __ BIND(L_load_element);
     __ load_heap_oop(rax_oop, from_element_addr, noreg, noreg, AS_RAW); // load the oop
     __ testptr(rax_oop, rax_oop);
     __ jcc(Assembler::zero, L_store_element);
 
-    __ load_klass(r11_klass, rax_oop);// query the object klass
+    __ load_klass(r11_klass, rax_oop, rscratch1);// query the object klass
     generate_type_check(r11_klass, ckoff, ckval, L_store_element);
     // ======== end loop ========
 
     // It was a real error; we must depend on the caller to finish the job.
     // Register rdx = -1 * number of *remaining* oops, r14 = *total* oops.
@@ -2702,12 +2699,14 @@
     const Register src_pos    = c_rarg1;  // source position
     const Register dst        = c_rarg2;  // destination array oop
     const Register dst_pos    = c_rarg3;  // destination position
 #ifndef _WIN64
     const Register length     = c_rarg4;
+    const Register rklass_tmp = r9;  // load_klass
 #else
     const Address  length(rsp, 6 * wordSize);  // elements count is on stack on Win64
+    const Register rklass_tmp = rdi;  // load_klass
 #endif
 
     { int modulus = CodeEntryAlignment;
       int target  = modulus - 5; // 5 = sizeof jmp(L_failed)
       int advance = target - (__ offset() % modulus);
@@ -2776,22 +2775,22 @@
     //  if (length < 0) return -1;
     __ movl(r11_length, length);        // length (elements count, 32-bits value)
     __ testl(r11_length, r11_length);
     __ jccb(Assembler::negative, L_failed_0);
 
-    __ load_klass(r10_src_klass, src);
+    __ load_klass(r10_src_klass, src, rklass_tmp);
 #ifdef ASSERT
     //  assert(src->klass() != NULL);
     {
       BLOCK_COMMENT("assert klasses not null {");
       Label L1, L2;
       __ testptr(r10_src_klass, r10_src_klass);
       __ jcc(Assembler::notZero, L2);   // it is broken if klass is NULL
       __ bind(L1);
       __ stop("broken null klass");
       __ bind(L2);
-      __ load_klass(rax, dst);
+      __ load_klass(rax, dst, rklass_tmp);
       __ cmpq(rax, 0);
       __ jcc(Assembler::equal, L1);     // this would be broken also
       BLOCK_COMMENT("} assert klasses not null done");
     }
 #endif
@@ -2810,11 +2809,11 @@
     const jint objArray_lh = Klass::array_layout_helper(T_OBJECT);
     __ cmpl(Address(r10_src_klass, lh_offset), objArray_lh);
     __ jcc(Assembler::equal, L_objArray);
 
     //  if (src->klass() != dst->klass()) return -1;
-    __ load_klass(rax, dst);
+    __ load_klass(rax, dst, rklass_tmp);
     __ cmpq(r10_src_klass, rax);
     __ jcc(Assembler::notEqual, L_failed);
 
     const Register rax_lh = rax;  // layout helper
     __ movl(rax_lh, Address(r10_src_klass, lh_offset));
@@ -2909,11 +2908,11 @@
   __ BIND(L_objArray);
     // live at this point:  r10_src_klass, r11_length, src[_pos], dst[_pos]
 
     Label L_plain_copy, L_checkcast_copy;
     //  test array classes for subtyping
-    __ load_klass(rax, dst);
+    __ load_klass(rax, dst, rklass_tmp);
     __ cmpq(r10_src_klass, rax); // usual case is exact equality
     __ jcc(Assembler::notEqual, L_checkcast_copy);
 
     // Identically typed arrays can be copied without element-wise checks.
     arraycopy_range_checks(src, src_pos, dst, dst_pos, r11_length,
@@ -2937,11 +2936,11 @@
       // It is safe to examine both src.length and dst.length.
       arraycopy_range_checks(src, src_pos, dst, dst_pos, r11_length,
                              rax, L_failed);
 
       const Register r11_dst_klass = r11;
-      __ load_klass(r11_dst_klass, dst); // reload
+      __ load_klass(r11_dst_klass, dst, rklass_tmp); // reload
 
       // Marshal the base address arguments now, freeing registers.
       __ lea(from, Address(src, src_pos, TIMES_OOP,
                    arrayOopDesc::base_offset_in_bytes(T_OBJECT)));
       __ lea(to,   Address(dst, dst_pos, TIMES_OOP,
diff a/src/hotspot/cpu/x86/templateTable_x86.cpp b/src/hotspot/cpu/x86/templateTable_x86.cpp
--- a/src/hotspot/cpu/x86/templateTable_x86.cpp
+++ b/src/hotspot/cpu/x86/templateTable_x86.cpp
@@ -1152,18 +1152,19 @@
 
   __ testptr(rax, rax);
   __ jcc(Assembler::zero, is_null);
 
   // Move array class to rdi
-  __ load_klass(rdi, rdx);
+  Register tmp_load_klass = LP64_ONLY(rscratch1) NOT_LP64(noreg);
+  __ load_klass(rdi, rdx, tmp_load_klass);
   if (ValueArrayFlatten) {
     __ movl(rbx, Address(rdi, Klass::layout_helper_offset()));
     __ test_flattened_array_layout(rbx, is_flat_array);
   }
 
   // Move subklass into rbx
-  __ load_klass(rbx, rax);
+  __ load_klass(rbx, rax, tmp_load_klass);
   // Move array element superklass into rax
   __ movptr(rax, Address(rdi,
                          ObjArrayKlass::element_klass_offset()));
 
   // Generate subtype check.  Blows rcx, rdi
@@ -1208,11 +1209,11 @@
     __ bind(is_flat_array); // Store non-null value to flat
 
     // Simplistic type check...
 
     // Profile the not-null value's klass.
-    __ load_klass(rbx, rax);
+    __ load_klass(rbx, rax, tmp_load_klass);
     // Move element klass into rax
     __ movptr(rax, Address(rdi, ArrayKlass::element_klass_offset()));
     // flat value array needs exact type match
     // is "rax == rbx" (value subclass == array element superclass)
     __ cmpptr(rax, rbx);
@@ -1248,11 +1249,12 @@
   // rbx: index
   // rdx: array
   index_check(rdx, rbx); // prefer index in rbx
   // Need to check whether array is boolean or byte
   // since both types share the bastore bytecode.
-  __ load_klass(rcx, rdx);
+  Register tmp_load_klass = LP64_ONLY(rscratch1) NOT_LP64(noreg);
+  __ load_klass(rcx, rdx, tmp_load_klass);
   __ movl(rcx, Address(rcx, Klass::layout_helper_offset()));
   int diffbit = Klass::layout_helper_boolean_diffbit();
   __ testl(rcx, diffbit);
   Label L_skip;
   __ jccb(Assembler::zero, L_skip);
@@ -2764,11 +2766,12 @@
 
   if (_desc->bytecode() == Bytecodes::_return_register_finalizer) {
     assert(state == vtos, "only valid state");
     Register robj = LP64_ONLY(c_rarg1) NOT_LP64(rax);
     __ movptr(robj, aaddress(0));
-    __ load_klass(rdi, robj);
+    Register tmp_load_klass = LP64_ONLY(rscratch1) NOT_LP64(noreg);
+    __ load_klass(rdi, robj, tmp_load_klass);
     __ movl(rdi, Address(rdi, Klass::access_flags_offset()));
     __ testl(rdi, JVM_ACC_HAS_FINALIZER);
     Label skip_register_finalizer;
     __ jcc(Assembler::zero, skip_register_finalizer);
 
@@ -3472,11 +3475,11 @@
         do_oop_store(_masm, field, rax);
         __ jmp(rewriteFlattenable);
         __ bind(isFlattened);
         pop_and_check_object(obj);
         assert_different_registers(rax, rdx, obj, off);
-        __ load_klass(rdx, rax);
+        __ load_klass(rdx, rax, rscratch1);
         __ data_for_oop(rax, rax, rdx);
         __ addptr(obj, off);
         __ access_value_copy(IN_HEAP, rax, obj, rdx);
         __ bind(rewriteFlattenable);
         if (rc == may_rewrite) {
@@ -3735,11 +3738,11 @@
       // No Flattened case
       do_oop_store(_masm, field, rax);
       __ jmp(done);
       __ bind(isFlattened);
       // Flattened case
-      __ load_klass(rdx, rax);
+      __ load_klass(rdx, rax, rscratch1);
       __ data_for_oop(rax, rax, rdx);
       __ lea(rcx, field);
       __ access_value_copy(IN_HEAP, rax, rcx, rdx);
       __ bind(done);
     }
@@ -4077,11 +4080,12 @@
 
   __ bind(notFinal);
 
   // get receiver klass
   __ null_check(recv, oopDesc::klass_offset_in_bytes());
-  __ load_klass(rax, recv);
+  Register tmp_load_klass = LP64_ONLY(rscratch1) NOT_LP64(noreg);
+  __ load_klass(rax, recv, tmp_load_klass);
 
   // profile this call
   __ profile_virtual_call(rax, rlocals, rdx);
   // get target Method* & entry point
   __ lookup_virtual_method(rax, index, method);
@@ -4169,11 +4173,12 @@
   __ andl(rlocals, (1 << ConstantPoolCacheEntry::is_vfinal_shift));
   __ jcc(Assembler::zero, notVFinal);
 
   // Get receiver klass into rlocals - also a null check
   __ null_check(rcx, oopDesc::klass_offset_in_bytes());
-  __ load_klass(rlocals, rcx);
+  Register tmp_load_klass = LP64_ONLY(rscratch1) NOT_LP64(noreg);
+  __ load_klass(rlocals, rcx, tmp_load_klass);
 
   Label subtype;
   __ check_klass_subtype(rlocals, rax, rbcp, subtype);
   // If we get here the typecheck failed
   recvKlass = rdx;
@@ -4192,11 +4197,11 @@
   __ bind(notVFinal);
 
   // Get receiver klass into rdx - also a null check
   __ restore_locals();  // restore r14
   __ null_check(rcx, oopDesc::klass_offset_in_bytes());
-  __ load_klass(rdx, rcx);
+  __ load_klass(rdx, rcx, tmp_load_klass);
 
   Label no_such_method;
 
   // Preserve method for throw_AbstractMethodErrorVerbose.
   __ mov(rcx, rbx);
@@ -4494,11 +4499,12 @@
   __ bind(quicked);
   __ mov(rdx, rax); // Save object in rdx; rax needed for subtype check
   __ load_resolved_klass_at_index(rax, rcx, rbx);
 
   __ bind(resolved);
-  __ load_klass(rbx, rdx);
+  Register tmp_load_klass = LP64_ONLY(rscratch1) NOT_LP64(noreg);
+  __ load_klass(rbx, rdx, tmp_load_klass);
 
   // Generate subtype check.  Blows rcx, rdi.  Object in rdx.
   // Superklass in rax.  Subklass in rbx.
   __ gen_subtype_check(rbx, ok_is_subtype);
 
@@ -4566,16 +4572,17 @@
   __ get_vm_result_2(rax, r15_thread);
 #endif
 
   __ pop_ptr(rdx); // restore receiver
   __ verify_oop(rdx);
-  __ load_klass(rdx, rdx);
+  Register tmp_load_klass = LP64_ONLY(rscratch1) NOT_LP64(noreg);
+  __ load_klass(rdx, rdx, tmp_load_klass);
   __ jmpb(resolved);
 
   // Get superklass in rax and subklass in rdx
   __ bind(quicked);
-  __ load_klass(rdx, rax);
+  __ load_klass(rdx, rax, tmp_load_klass);
   __ load_resolved_klass_at_index(rax, rcx, rbx);
 
   __ bind(resolved);
 
   // Generate subtype check.  Blows rcx, rdi
diff a/src/hotspot/cpu/x86/vtableStubs_x86_64.cpp b/src/hotspot/cpu/x86/vtableStubs_x86_64.cpp
--- a/src/hotspot/cpu/x86/vtableStubs_x86_64.cpp
+++ b/src/hotspot/cpu/x86/vtableStubs_x86_64.cpp
@@ -46,10 +46,11 @@
 #endif
 
 VtableStub* VtableStubs::create_vtable_stub(int vtable_index, bool caller_is_c1) {
   // Read "A word on VtableStub sizing" in share/code/vtableStubs.hpp for details on stub sizing.
   const int stub_code_length = code_size_limit(true);
+  Register tmp_load_klass = rscratch1;
   VtableStub* s = new(stub_code_length) VtableStub(true, vtable_index, caller_is_c1);
   // Can be NULL if there is no free space in the code cache.
   if (s == NULL) {
     return NULL;
   }
@@ -79,11 +80,11 @@
 
   // Free registers (non-args) are rax, rbx
 
   // get receiver klass
   address npe_addr = __ pc();
-  __ load_klass(rax, j_rarg0);
+  __ load_klass(rax, j_rarg0, tmp_load_klass);
 
 #ifndef PRODUCT
   if (DebugVtables) {
     Label L;
     start_pc = __ pc();
@@ -186,11 +187,11 @@
   Label L_no_such_interface;
 
   // get receiver klass (also an implicit null-check)
   assert(VtableStub::receiver_location() == j_rarg0->as_VMReg(), "receiver expected in j_rarg0");
   address npe_addr = __ pc();
-  __ load_klass(recv_klass_reg, j_rarg0);
+  __ load_klass(recv_klass_reg, j_rarg0, temp_reg);
 
   start_pc = __ pc();
 
   // Receiver subtype check against REFC.
   // Destroys recv_klass_reg value.
@@ -204,11 +205,11 @@
   const ptrdiff_t  typecheckSize = __ pc() - start_pc;
   start_pc = __ pc();
 
   // Get selected method from declaring class and itable index
   const Register method = rbx;
-  __ load_klass(recv_klass_reg, j_rarg0);   // restore recv_klass_reg
+  __ load_klass(recv_klass_reg, j_rarg0, temp_reg);   // restore recv_klass_reg
   __ lookup_interface_method(// inputs: rec. class, interface, itable index
                              recv_klass_reg, holder_klass_reg, itable_index,
                              // outputs: method, scan temp. reg
                              method, temp_reg,
                              L_no_such_interface);
diff a/src/hotspot/cpu/x86/x86_64.ad b/src/hotspot/cpu/x86/x86_64.ad
--- a/src/hotspot/cpu/x86/x86_64.ad
+++ b/src/hotspot/cpu/x86/x86_64.ad
@@ -355,11 +355,11 @@
 RegMask _STACK_OR_PTR_REG_mask;
 RegMask _STACK_OR_LONG_REG_mask;
 RegMask _STACK_OR_INT_REG_mask;
 
 static bool need_r12_heapbase() {
-  return UseCompressedOops || UseCompressedClassPointers;
+  return UseCompressedOops;
 }
 
 void reg_mask_init() {
   // _ALL_REG_mask is generated by adlc from the all_reg register class below.
   // We derive a number of subsets from it.
@@ -1510,11 +1510,11 @@
 {
   MacroAssembler masm(&cbuf);
   if (!_verified) {  
     uint insts_size = cbuf.insts_size();
     if (UseCompressedClassPointers) {
-      masm.load_klass(rscratch1, j_rarg0);
+      masm.load_klass(rscratch1, j_rarg0, rscratch2);
       masm.cmpptr(rax, rscratch1);
     } else {
       masm.cmpptr(rax, Address(j_rarg0, oopDesc::klass_offset_in_bytes()));
     }
     masm.jump_cc(Assembler::notEqual, RuntimeAddress(SharedRuntime::get_ic_miss_stub()));
@@ -1546,11 +1546,11 @@
 void MachUEPNode::emit(CodeBuffer& cbuf, PhaseRegAlloc* ra_) const
 {
   MacroAssembler masm(&cbuf);
   uint insts_size = cbuf.insts_size();
   if (UseCompressedClassPointers) {
-    masm.load_klass(rscratch1, j_rarg0);
+    masm.load_klass(rscratch1, j_rarg0, rscratch2);
     masm.cmpptr(rax, rscratch1);
   } else {
     masm.cmpptr(rax, Address(j_rarg0, oopDesc::klass_offset_in_bytes()));
   }
 
@@ -5962,11 +5962,11 @@
   ins_pipe(ialu_mem_reg);
 %}
 
 instruct storeImmP0(memory mem, immP0 zero)
 %{
-  predicate(UseCompressedOops && (CompressedOops::base() == NULL) && (CompressedKlassPointers::base() == NULL));
+  predicate(UseCompressedOops && (CompressedOops::base() == NULL));
   match(Set mem (StoreP mem zero));
 
   ins_cost(125); // XXX
   format %{ "movq    $mem, R12\t# ptr (R12_heapbase==0)" %}
   ins_encode %{
@@ -6012,11 +6012,11 @@
   ins_pipe(ialu_mem_reg);
 %}
 
 instruct storeImmN0(memory mem, immN0 zero)
 %{
-  predicate(CompressedOops::base() == NULL && CompressedKlassPointers::base() == NULL);
+  predicate(CompressedOops::base() == NULL);
   match(Set mem (StoreN mem zero));
 
   ins_cost(125); // XXX
   format %{ "movl    $mem, R12\t# compressed ptr (R12_heapbase==0)" %}
   ins_encode %{
@@ -6055,11 +6055,11 @@
 %}
 
 // Store Integer Immediate
 instruct storeImmI0(memory mem, immI0 zero)
 %{
-  predicate(UseCompressedOops && (CompressedOops::base() == NULL) && (CompressedKlassPointers::base() == NULL));
+  predicate(UseCompressedOops && (CompressedOops::base() == NULL));
   match(Set mem (StoreI mem zero));
 
   ins_cost(125); // XXX
   format %{ "movl    $mem, R12\t# int (R12_heapbase==0)" %}
   ins_encode %{
@@ -6080,11 +6080,11 @@
 %}
 
 // Store Long Immediate
 instruct storeImmL0(memory mem, immL0 zero)
 %{
-  predicate(UseCompressedOops && (CompressedOops::base() == NULL) && (CompressedKlassPointers::base() == NULL));
+  predicate(UseCompressedOops && (CompressedOops::base() == NULL));
   match(Set mem (StoreL mem zero));
 
   ins_cost(125); // XXX
   format %{ "movq    $mem, R12\t# long (R12_heapbase==0)" %}
   ins_encode %{
@@ -6105,11 +6105,11 @@
 %}
 
 // Store Short/Char Immediate
 instruct storeImmC0(memory mem, immI0 zero)
 %{
-  predicate(UseCompressedOops && (CompressedOops::base() == NULL) && (CompressedKlassPointers::base() == NULL));
+  predicate(UseCompressedOops && (CompressedOops::base() == NULL));
   match(Set mem (StoreC mem zero));
 
   ins_cost(125); // XXX
   format %{ "movw    $mem, R12\t# short/char (R12_heapbase==0)" %}
   ins_encode %{
@@ -6131,11 +6131,11 @@
 %}
 
 // Store Byte Immediate
 instruct storeImmB0(memory mem, immI0 zero)
 %{
-  predicate(UseCompressedOops && (CompressedOops::base() == NULL) && (CompressedKlassPointers::base() == NULL));
+  predicate(UseCompressedOops && (CompressedOops::base() == NULL));
   match(Set mem (StoreB mem zero));
 
   ins_cost(125); // XXX
   format %{ "movb    $mem, R12\t# short/char (R12_heapbase==0)" %}
   ins_encode %{
@@ -6156,11 +6156,11 @@
 %}
 
 // Store CMS card-mark Immediate
 instruct storeImmCM0_reg(memory mem, immI0 zero)
 %{
-  predicate(UseCompressedOops && (CompressedOops::base() == NULL) && (CompressedKlassPointers::base() == NULL));
+  predicate(UseCompressedOops && (CompressedOops::base() == NULL));
   match(Set mem (StoreCM mem zero));
 
   ins_cost(125); // XXX
   format %{ "movb    $mem, R12\t# CMS card-mark byte 0 (R12_heapbase==0)" %}
   ins_encode %{
@@ -6194,11 +6194,11 @@
 %}
 
 // Store immediate Float value (it is faster than store from XMM register)
 instruct storeF0(memory mem, immF0 zero)
 %{
-  predicate(UseCompressedOops && (CompressedOops::base() == NULL) && (CompressedKlassPointers::base() == NULL));
+  predicate(UseCompressedOops && (CompressedOops::base() == NULL));
   match(Set mem (StoreF mem zero));
 
   ins_cost(25); // XXX
   format %{ "movl    $mem, R12\t# float 0. (R12_heapbase==0)" %}
   ins_encode %{
@@ -6244,11 +6244,11 @@
   ins_pipe(ialu_mem_imm);
 %}
 
 instruct storeD0(memory mem, immD0 zero)
 %{
-  predicate(UseCompressedOops && (CompressedOops::base() == NULL) && (CompressedKlassPointers::base() == NULL));
+  predicate(UseCompressedOops && (CompressedOops::base() == NULL));
   match(Set mem (StoreD mem zero));
 
   ins_cost(25); // XXX
   format %{ "movq    $mem, R12\t# double 0. (R12_heapbase==0)" %}
   ins_encode %{
@@ -6837,35 +6837,28 @@
   ins_pipe(ialu_reg_long);
 %}
 
 instruct encodeKlass_not_null(rRegN dst, rRegP src, rFlagsReg cr) %{
   match(Set dst (EncodePKlass src));
-  effect(KILL cr);
-  format %{ "encode_klass_not_null $dst,$src" %}
+  effect(TEMP dst, KILL cr);
+  format %{ "encode_and_move_klass_not_null $dst,$src" %}
   ins_encode %{
-    __ encode_klass_not_null($dst$$Register, $src$$Register);
+    __ encode_and_move_klass_not_null($dst$$Register, $src$$Register);
   %}
   ins_pipe(ialu_reg_long);
 %}
 
 instruct decodeKlass_not_null(rRegP dst, rRegN src, rFlagsReg cr) %{
   match(Set dst (DecodeNKlass src));
-  effect(KILL cr);
-  format %{ "decode_klass_not_null $dst,$src" %}
+  effect(TEMP dst, KILL cr);
+  format %{ "decode_and_move_klass_not_null $dst,$src" %}
   ins_encode %{
-    Register s = $src$$Register;
-    Register d = $dst$$Register;
-    if (s != d) {
-      __ decode_klass_not_null(d, s);
-    } else {
-      __ decode_klass_not_null(d);
-    }
+    __ decode_and_move_klass_not_null($dst$$Register, $src$$Register);
   %}
   ins_pipe(ialu_reg_long);
 %}
 
-
 //----------Conditional Move---------------------------------------------------
 // Jump
 // dummy instruction for generating temp registers
 instruct jumpXtnd_offset(rRegL switch_val, immI2 shift, rRegI dest) %{
   match(Jump (LShiftL switch_val shift));
@@ -11881,11 +11874,10 @@
 %}
 
 instruct testP_mem_reg0(rFlagsReg cr, memory mem, immP0 zero)
 %{
   predicate(UseCompressedOops && (CompressedOops::base() == NULL) &&
-            (CompressedKlassPointers::base() == NULL) &&
             n->in(1)->as_Load()->barrier_data() == 0);
   match(Set cr (CmpP (LoadP mem) zero));
 
   format %{ "cmpq    R12, $mem\t# ptr (R12_heapbase==0)" %}
   ins_encode %{
@@ -11977,11 +11969,11 @@
   ins_pipe(ialu_cr_reg_mem);
 %}
 
 instruct testN_mem_reg0(rFlagsReg cr, memory mem, immN0 zero)
 %{
-  predicate(CompressedOops::base() == NULL && (CompressedKlassPointers::base() == NULL));
+  predicate(CompressedOops::base() == NULL);
   match(Set cr (CmpN (LoadN mem) zero));
 
   format %{ "cmpl    R12, $mem\t# compressed ptr (R12_heapbase==0)" %}
   ins_encode %{
     __ cmpl(r12, $mem$$Address);
@@ -12635,19 +12627,19 @@
                  true, ra_->C->profile_rtm());
   %}
   ins_pipe(pipe_slow);
 %}
 
-instruct cmpFastLock(rFlagsReg cr, rRegP object, rbx_RegP box, rax_RegI tmp, rRegP scr) %{
+instruct cmpFastLock(rFlagsReg cr, rRegP object, rbx_RegP box, rax_RegI tmp, rRegP scr, rRegP cx1) %{
   predicate(!Compile::current()->use_rtm());
   match(Set cr (FastLock object box));
-  effect(TEMP tmp, TEMP scr, USE_KILL box);
+  effect(TEMP tmp, TEMP scr, TEMP cx1, USE_KILL box);
   ins_cost(300);
   format %{ "fastlock $object,$box\t! kills $box,$tmp,$scr" %}
   ins_encode %{
     __ fast_lock($object$$Register, $box$$Register, $tmp$$Register,
-                 $scr$$Register, noreg, noreg, _counters, NULL, NULL, NULL, false, false);
+                 $scr$$Register, $cx1$$Register, noreg, _counters, NULL, NULL, NULL, false, false);
   %}
   ins_pipe(pipe_slow);
 %}
 
 instruct cmpFastUnlock(rFlagsReg cr, rRegP object, rax_RegP box, rRegP tmp) %{
diff a/src/hotspot/share/c1/c1_Runtime1.cpp b/src/hotspot/share/c1/c1_Runtime1.cpp
--- a/src/hotspot/share/c1/c1_Runtime1.cpp
+++ b/src/hotspot/share/c1/c1_Runtime1.cpp
@@ -28,10 +28,11 @@
 #include "c1/c1_Defs.hpp"
 #include "c1/c1_FrameMap.hpp"
 #include "c1/c1_LIRAssembler.hpp"
 #include "c1/c1_MacroAssembler.hpp"
 #include "c1/c1_Runtime1.hpp"
+#include "classfile/javaClasses.inline.hpp"
 #include "classfile/systemDictionary.hpp"
 #include "classfile/vmSymbols.hpp"
 #include "code/codeBlob.hpp"
 #include "code/compiledIC.hpp"
 #include "code/pcDesc.hpp"
diff a/src/hotspot/share/classfile/classListParser.cpp b/src/hotspot/share/classfile/classListParser.cpp
--- a/src/hotspot/share/classfile/classListParser.cpp
+++ b/src/hotspot/share/classfile/classListParser.cpp
@@ -25,10 +25,11 @@
 #include "precompiled.hpp"
 #include "jvm.h"
 #include "jimage.hpp"
 #include "classfile/classListParser.hpp"
 #include "classfile/classLoaderExt.hpp"
+#include "classfile/javaClasses.inline.hpp"
 #include "classfile/symbolTable.hpp"
 #include "classfile/systemDictionary.hpp"
 #include "classfile/systemDictionaryShared.hpp"
 #include "classfile/vmSymbols.hpp"
 #include "logging/log.hpp"
diff a/src/hotspot/share/classfile/classLoaderData.cpp b/src/hotspot/share/classfile/classLoaderData.cpp
--- a/src/hotspot/share/classfile/classLoaderData.cpp
+++ b/src/hotspot/share/classfile/classLoaderData.cpp
@@ -892,16 +892,10 @@
       remove_class(ik);
     }
   }
 }
 
-// These CLDs are to contain non-strong hidden classes or unsafe anonymous classes used for JSR292
-ClassLoaderData* ClassLoaderData::has_class_mirror_holder_cld(Handle loader) {
-  // Add a new class loader data to the graph.
-  return ClassLoaderDataGraph::add(loader, true);
-}
-
 // Caller needs ResourceMark
 // If the class loader's _name has not been explicitly set, the class loader's
 // qualified class name is returned.
 const char* ClassLoaderData::loader_name() const {
    if (_class_loader_klass == NULL) {
diff a/src/hotspot/share/classfile/classLoaderData.hpp b/src/hotspot/share/classfile/classLoaderData.hpp
--- a/src/hotspot/share/classfile/classLoaderData.hpp
+++ b/src/hotspot/share/classfile/classLoaderData.hpp
@@ -315,11 +315,10 @@
 
   void add_to_deallocate_list(Metadata* m);
 
   static ClassLoaderData* class_loader_data(oop loader);
   static ClassLoaderData* class_loader_data_or_null(oop loader);
-  static ClassLoaderData* has_class_mirror_holder_cld(Handle loader);
 
   // Returns Klass* of associated class loader, or NULL if associated loader is 'bootstrap'.
   // Also works if unloading.
   Klass* class_loader_klass() const { return _class_loader_klass; }
 
diff a/src/hotspot/share/classfile/fieldLayoutBuilder.cpp b/src/hotspot/share/classfile/fieldLayoutBuilder.cpp
--- a/src/hotspot/share/classfile/fieldLayoutBuilder.cpp
+++ b/src/hotspot/share/classfile/fieldLayoutBuilder.cpp
@@ -876,72 +876,10 @@
 
 
   epilogue();
 }
 
-// Compute layout of the java/lang/ref/Reference class according
-// to the hard coded offsets of its fields
-void FieldLayoutBuilder::compute_java_lang_ref_Reference_layout() {
-  prologue();
-  regular_field_sorting();
-
-  assert(_contended_groups.is_empty(), "java.lang.Reference has no @Contended annotations");
-  assert(_root_group->primitive_fields() == NULL, "java.lang.Reference has no nonstatic primitive fields");
-  int field_count = 0;
-  int offset = -1;
-  for (int i = 0; i < _root_group->oop_fields()->length(); i++) {
-    LayoutRawBlock* b = _root_group->oop_fields()->at(i);
-    FieldInfo* fi = FieldInfo::from_field_array(_fields, b->field_index());
-    if (fi->name(_constant_pool)->equals("referent")) {
-      offset = java_lang_ref_Reference::referent_offset;
-    } else if (fi->name(_constant_pool)->equals("queue")) {
-      offset = java_lang_ref_Reference::queue_offset;
-    } else if (fi->name(_constant_pool)->equals("next")) {
-      offset = java_lang_ref_Reference::next_offset;
-    } else if (fi->name(_constant_pool)->equals("discovered")) {
-      offset = java_lang_ref_Reference::discovered_offset;
-    }
-    assert(offset != -1, "Unknown field");
-    _layout->add_field_at_offset(b, offset);
-    field_count++;
-  }
-  assert(field_count == 4, "Wrong number of fields in java.lang.ref.Reference");
-
-  _static_layout->add_contiguously(this->_static_fields->oop_fields());
-  _static_layout->add(this->_static_fields->primitive_fields());
-
-  epilogue();
-}
-
-// Compute layout of the boxing class according
-// to the hard coded offsets of their fields
-void FieldLayoutBuilder::compute_boxing_class_layout() {
-  prologue();
-  regular_field_sorting();
-
-  assert(_contended_groups.is_empty(), "Boxing classes have no @Contended annotations");
-  assert(_root_group->oop_fields() == NULL, "Boxing classes have no nonstatic oops fields");
-  int field_count = 0;
-  int offset = -1;
-  for (int i = 0; i < _root_group->primitive_fields()->length(); i++) {
-    LayoutRawBlock* b = _root_group->primitive_fields()->at(i);
-    FieldInfo* fi = FieldInfo::from_field_array(_fields, b->field_index());
-    assert(fi->name(_constant_pool)->equals("value"), "Boxing classes have a single nonstatic field named 'value'");
-    BasicType type = Signature::basic_type(fi->signature(_constant_pool));
-    offset = java_lang_boxing_object::value_offset_in_bytes(type);
-    assert(offset != -1, "Unknown field");
-    _layout->add_field_at_offset(b, offset);
-    field_count++;
-  }
-  assert(field_count == 1, "Wrong number of fields for a boxing class");
-
-  _static_layout->add_contiguously(this->_static_fields->oop_fields());
-  _static_layout->add(this->_static_fields->primitive_fields());
-
-  epilogue();
-}
-
 void FieldLayoutBuilder::add_flattened_field_oopmap(OopMapBlocksBuilder* nonstatic_oop_maps,
                 ValueKlass* vklass, int offset) {
   int diff = offset - vklass->first_field_offset();
   const OopMapBlock* map = vklass->start_of_nonstatic_oop_maps();
   const OopMapBlock* last_map = map + vklass->nonstatic_oop_map_count();
@@ -1037,22 +975,11 @@
     tty->print_cr("---");
   }
 }
 
 void FieldLayoutBuilder::build_layout(TRAPS) {
-  if (_classname == vmSymbols::java_lang_ref_Reference()) {
-    compute_java_lang_ref_Reference_layout();
-  } else if (_classname == vmSymbols::java_lang_Boolean() ||
-             _classname == vmSymbols::java_lang_Character() ||
-             _classname == vmSymbols::java_lang_Float() ||
-             _classname == vmSymbols::java_lang_Double() ||
-             _classname == vmSymbols::java_lang_Byte() ||
-             _classname == vmSymbols::java_lang_Short() ||
-             _classname == vmSymbols::java_lang_Integer() ||
-             _classname == vmSymbols::java_lang_Long()) {
-      compute_boxing_class_layout();
-  } else if (_is_inline_type) {
+  if (_is_inline_type) {
     compute_inline_class_layout(CHECK);
   } else {
     compute_regular_layout();
   }
 }
diff a/src/hotspot/share/classfile/fieldLayoutBuilder.hpp b/src/hotspot/share/classfile/fieldLayoutBuilder.hpp
--- a/src/hotspot/share/classfile/fieldLayoutBuilder.hpp
+++ b/src/hotspot/share/classfile/fieldLayoutBuilder.hpp
@@ -281,12 +281,10 @@
     return _exact_size_in_bytes;
   }
 
   void build_layout(TRAPS);
   void compute_regular_layout();
-  void compute_java_lang_ref_Reference_layout();
-  void compute_boxing_class_layout();
   void compute_inline_class_layout(TRAPS);
   void insert_contended_padding(LayoutRawBlock* slot);
 
  protected:
   void prologue();
diff a/src/hotspot/share/classfile/javaClasses.cpp b/src/hotspot/share/classfile/javaClasses.cpp
--- a/src/hotspot/share/classfile/javaClasses.cpp
+++ b/src/hotspot/share/classfile/javaClasses.cpp
@@ -1543,18 +1543,10 @@
 #endif
   return java_class;
 }
 
 
-Klass* java_lang_Class::as_Klass(oop java_class) {
-  //%note memory_2
-  assert(java_lang_Class::is_instance(java_class), "must be a Class object");
-  Klass* k = ((Klass*)java_class->metadata_field(_klass_offset));
-  assert(k == NULL || k->is_klass(), "type check");
-  return k;
-}
-
 Klass* java_lang_Class::as_Klass_raw(oop java_class) {
   //%note memory_2
   assert(java_lang_Class::is_instance(java_class), "must be a Class object");
   Klass* k = ((Klass*)java_class->metadata_field_raw(_klass_offset));
   assert(k == NULL || k->is_klass(), "type check");
@@ -4874,12 +4866,14 @@
    jvalue v;
    java_lang_boxing_object::get_value(obj, &v);
    return v.z;
 }
 
-static int member_offset(int hardcoded_offset) {
-  return (hardcoded_offset * heapOopSize) + instanceOopDesc::base_offset_in_bytes();
+// Use with care. This function makes a lot of assumptions about the contents of the object.
+// So naturally, only hardcode offsets if you know what you are doing.
+static int member_offset(int hardcoded_offset, int elementSize) {
+  return align_up((hardcoded_offset * elementSize) + instanceOopDesc::base_offset_in_bytes(), elementSize);
 }
 
 #define RECORDCOMPONENT_FIELDS_DO(macro) \
   macro(clazz_offset,       k, "clazz",       class_signature,  false); \
   macro(name_offset,        k, "name",        string_signature, false); \
@@ -4933,18 +4927,18 @@
 // Invoked before SystemDictionary::initialize, so pre-loaded classes
 // are not available to determine the offset_of_static_fields.
 void JavaClasses::compute_hard_coded_offsets() {
 
   // java_lang_boxing_object
-  java_lang_boxing_object::value_offset      = member_offset(java_lang_boxing_object::hc_value_offset);
-  java_lang_boxing_object::long_value_offset = align_up(member_offset(java_lang_boxing_object::hc_value_offset), BytesPerLong);
+  java_lang_boxing_object::value_offset      = member_offset(java_lang_boxing_object::hc_value_offset, BytesPerInt);
+  java_lang_boxing_object::long_value_offset = member_offset(java_lang_boxing_object::hc_value_offset, BytesPerLong);
 
   // java_lang_ref_Reference
-  java_lang_ref_Reference::referent_offset    = member_offset(java_lang_ref_Reference::hc_referent_offset);
-  java_lang_ref_Reference::queue_offset       = member_offset(java_lang_ref_Reference::hc_queue_offset);
-  java_lang_ref_Reference::next_offset        = member_offset(java_lang_ref_Reference::hc_next_offset);
-  java_lang_ref_Reference::discovered_offset  = member_offset(java_lang_ref_Reference::hc_discovered_offset);
+  java_lang_ref_Reference::referent_offset    = member_offset(java_lang_ref_Reference::hc_referent_offset, heapOopSize);
+  java_lang_ref_Reference::queue_offset       = member_offset(java_lang_ref_Reference::hc_queue_offset, heapOopSize);
+  java_lang_ref_Reference::next_offset        = member_offset(java_lang_ref_Reference::hc_next_offset, heapOopSize);
+  java_lang_ref_Reference::discovered_offset  = member_offset(java_lang_ref_Reference::hc_discovered_offset, heapOopSize);
 }
 
 #define DO_COMPUTE_OFFSETS(k) k::compute_offsets();
 
 // Compute non-hard-coded field offsets of all the classes in this file
diff a/src/hotspot/share/classfile/systemDictionary.cpp b/src/hotspot/share/classfile/systemDictionary.cpp
--- a/src/hotspot/share/classfile/systemDictionary.cpp
+++ b/src/hotspot/share/classfile/systemDictionary.cpp
@@ -183,13 +183,18 @@
                          CHECK);
 
   _java_platform_loader = (oop)result.get_jobject();
 }
 
-ClassLoaderData* SystemDictionary::register_loader(Handle class_loader) {
-  if (class_loader.is_null()) return ClassLoaderData::the_null_class_loader_data();
-  return ClassLoaderDataGraph::find_or_create(class_loader);
+ClassLoaderData* SystemDictionary::register_loader(Handle class_loader, bool create_mirror_cld) {
+  if (create_mirror_cld) {
+    // Add a new class loader data to the graph.
+    return ClassLoaderDataGraph::add(class_loader, true);
+  } else {
+    return (class_loader() == NULL) ? ClassLoaderData::the_null_class_loader_data() :
+                                      ClassLoaderDataGraph::find_or_create(class_loader);
+  }
 }
 
 // ----------------------------------------------------------------------------
 // Parallel class loading check
 
@@ -1076,31 +1081,23 @@
                                               ClassFileStream* st,
                                               const ClassLoadInfo& cl_info,
                                               TRAPS) {
 
   EventClassLoad class_load_start_event;
-
-  ClassLoaderData* loader_data;
 
   bool is_unsafe_anon_class = cl_info.unsafe_anonymous_host() != NULL;
 
-  if (is_unsafe_anon_class) {
-    // - for unsafe anonymous class: create a new CLD whith a class holder that uses
-    //                               the same class loader as the unsafe_anonymous_host.
-    guarantee(cl_info.unsafe_anonymous_host()->class_loader() == class_loader(),
-              "should be the same");
-    loader_data = ClassLoaderData::has_class_mirror_holder_cld(class_loader);
-  } else if (cl_info.is_hidden()) {
-    // - for hidden classes that are not strong: create a new CLD that has a class holder and
-    //                                           whose loader is the Lookup class' loader.
-    // - for hidden class: add the class to the Lookup class' loader's CLD.
-    if (!cl_info.is_strong_hidden()) {
-      loader_data = ClassLoaderData::has_class_mirror_holder_cld(class_loader);
-    } else {
-      // This hidden class goes into the regular CLD pool for this loader.
-      loader_data = register_loader(class_loader);
-    }
+  // - for unsafe anonymous class: create a new CLD whith a class holder that uses
+  //                               the same class loader as the unsafe_anonymous_host.
+  // - for hidden classes that are not strong: create a new CLD that has a class holder and
+  //                                           whose loader is the Lookup class's loader.
+  // - for hidden class: add the class to the Lookup class's loader's CLD.
+  if (is_unsafe_anon_class || cl_info.is_hidden()) {
+    guarantee(!is_unsafe_anon_class || cl_info.unsafe_anonymous_host()->class_loader() == class_loader(),
+              "should be NULL or the same");
+    bool create_mirror_cld = is_unsafe_anon_class || !cl_info.is_strong_hidden();
+    loader_data = register_loader(class_loader, create_mirror_cld);
   } else {
     loader_data = ClassLoaderData::class_loader_data(class_loader());
   }
 
   assert(st != NULL, "invariant");
diff a/src/hotspot/share/classfile/systemDictionary.hpp b/src/hotspot/share/classfile/systemDictionary.hpp
--- a/src/hotspot/share/classfile/systemDictionary.hpp
+++ b/src/hotspot/share/classfile/systemDictionary.hpp
@@ -491,11 +491,11 @@
 
   // Compute the java system and platform loaders
   static void compute_java_loaders(TRAPS);
 
   // Register a new class loader
-  static ClassLoaderData* register_loader(Handle class_loader);
+  static ClassLoaderData* register_loader(Handle class_loader, bool create_mirror_cld = false);
 protected:
   // Mirrors for primitive classes (created eagerly)
   static oop check_mirror(oop m) {
     assert(m != NULL, "mirror not initialized");
     return m;
diff a/src/hotspot/share/gc/shenandoah/c2/shenandoahBarrierSetC2.cpp b/src/hotspot/share/gc/shenandoah/c2/shenandoahBarrierSetC2.cpp
--- a/src/hotspot/share/gc/shenandoah/c2/shenandoahBarrierSetC2.cpp
+++ b/src/hotspot/share/gc/shenandoah/c2/shenandoahBarrierSetC2.cpp
@@ -25,15 +25,15 @@
 #include "precompiled.hpp"
 #include "gc/shared/barrierSet.hpp"
 #include "gc/shenandoah/shenandoahBarrierSet.hpp"
 #include "gc/shenandoah/shenandoahForwarding.hpp"
 #include "gc/shenandoah/shenandoahHeap.hpp"
-#include "gc/shenandoah/shenandoahHeuristics.hpp"
 #include "gc/shenandoah/shenandoahRuntime.hpp"
 #include "gc/shenandoah/shenandoahThreadLocalData.hpp"
 #include "gc/shenandoah/c2/shenandoahBarrierSetC2.hpp"
 #include "gc/shenandoah/c2/shenandoahSupport.hpp"
+#include "gc/shenandoah/heuristics/shenandoahHeuristics.hpp"
 #include "opto/arraycopynode.hpp"
 #include "opto/escape.hpp"
 #include "opto/graphKit.hpp"
 #include "opto/idealKit.hpp"
 #include "opto/macro.hpp"
diff a/src/hotspot/share/gc/shenandoah/c2/shenandoahSupport.cpp b/src/hotspot/share/gc/shenandoah/c2/shenandoahSupport.cpp
--- a/src/hotspot/share/gc/shenandoah/c2/shenandoahSupport.cpp
+++ b/src/hotspot/share/gc/shenandoah/c2/shenandoahSupport.cpp
@@ -66,11 +66,11 @@
     }
   }
   return true;
 }
 
-bool ShenandoahBarrierC2Support::is_heap_state_test(Node* iff, int mask) {
+bool ShenandoahBarrierC2Support::is_gc_state_test(Node* iff, int mask) {
   if (!UseShenandoahGC) {
     return false;
   }
   assert(iff->is_If(), "bad input");
   if (iff->Opcode() != Op_If) {
@@ -100,11 +100,11 @@
 
   return is_gc_state_load(in1);
 }
 
 bool ShenandoahBarrierC2Support::is_heap_stable_test(Node* iff) {
-  return is_heap_state_test(iff, ShenandoahHeap::HAS_FORWARDED);
+  return is_gc_state_test(iff, ShenandoahHeap::HAS_FORWARDED);
 }
 
 bool ShenandoahBarrierC2Support::is_gc_state_load(Node *n) {
   if (!UseShenandoahGC) {
     return false;
@@ -857,56 +857,66 @@
   phase->lazy_replace(outer, new_outer);
   phase->lazy_replace(le, new_le);
   inner->clear_strip_mined();
 }
 
-void ShenandoahBarrierC2Support::test_heap_state(Node*& ctrl, Node* raw_mem, Node*& heap_stable_ctrl,
-                                                 PhaseIdealLoop* phase, int flags) {
-  IdealLoopTree* loop = phase->get_loop(ctrl);
-  Node* thread = new ThreadLocalNode();
-  phase->register_new_node(thread, ctrl);
-  Node* offset = phase->igvn().MakeConX(in_bytes(ShenandoahThreadLocalData::gc_state_offset()));
-  phase->set_ctrl(offset, phase->C->root());
-  Node* gc_state_addr = new AddPNode(phase->C->top(), thread, offset);
-  phase->register_new_node(gc_state_addr, ctrl);
-  uint gc_state_idx = Compile::AliasIdxRaw;
-  const TypePtr* gc_state_adr_type = NULL; // debug-mode-only argument
-  debug_only(gc_state_adr_type = phase->C->get_adr_type(gc_state_idx));
-
-  Node* gc_state = new LoadBNode(ctrl, raw_mem, gc_state_addr, gc_state_adr_type, TypeInt::BYTE, MemNode::unordered);
-  phase->register_new_node(gc_state, ctrl);
-  Node* heap_stable_and = new AndINode(gc_state, phase->igvn().intcon(flags));
-  phase->register_new_node(heap_stable_and, ctrl);
-  Node* heap_stable_cmp = new CmpINode(heap_stable_and, phase->igvn().zerocon(T_INT));
-  phase->register_new_node(heap_stable_cmp, ctrl);
-  Node* heap_stable_test = new BoolNode(heap_stable_cmp, BoolTest::ne);
-  phase->register_new_node(heap_stable_test, ctrl);
-  IfNode* heap_stable_iff = new IfNode(ctrl, heap_stable_test, PROB_UNLIKELY(0.999), COUNT_UNKNOWN);
-  phase->register_control(heap_stable_iff, loop, ctrl);
-
-  heap_stable_ctrl = new IfFalseNode(heap_stable_iff);
-  phase->register_control(heap_stable_ctrl, loop, heap_stable_iff);
-  ctrl = new IfTrueNode(heap_stable_iff);
-  phase->register_control(ctrl, loop, heap_stable_iff);
-
-  assert(is_heap_state_test(heap_stable_iff, flags), "Should match the shape");
+void ShenandoahBarrierC2Support::test_gc_state(Node*& ctrl, Node* raw_mem, Node*& test_fail_ctrl,
+                                               PhaseIdealLoop* phase, int flags) {
+  PhaseIterGVN& igvn = phase->igvn();
+  Node* old_ctrl = ctrl;
+
+  Node* thread          = new ThreadLocalNode();
+  Node* gc_state_offset = igvn.MakeConX(in_bytes(ShenandoahThreadLocalData::gc_state_offset()));
+  Node* gc_state_addr   = new AddPNode(phase->C->top(), thread, gc_state_offset);
+  Node* gc_state        = new LoadBNode(old_ctrl, raw_mem, gc_state_addr,
+                                        DEBUG_ONLY(phase->C->get_adr_type(Compile::AliasIdxRaw)) NOT_DEBUG(NULL),
+                                        TypeInt::BYTE, MemNode::unordered);
+  Node* gc_state_and    = new AndINode(gc_state, igvn.intcon(flags));
+  Node* gc_state_cmp    = new CmpINode(gc_state_and, igvn.zerocon(T_INT));
+  Node* gc_state_bool   = new BoolNode(gc_state_cmp, BoolTest::ne);
+
+  IfNode* gc_state_iff  = new IfNode(old_ctrl, gc_state_bool, PROB_UNLIKELY(0.999), COUNT_UNKNOWN);
+  ctrl                  = new IfTrueNode(gc_state_iff);
+  test_fail_ctrl        = new IfFalseNode(gc_state_iff);
+
+  IdealLoopTree* loop = phase->get_loop(old_ctrl);
+  phase->register_control(gc_state_iff,   loop, old_ctrl);
+  phase->register_control(ctrl,           loop, gc_state_iff);
+  phase->register_control(test_fail_ctrl, loop, gc_state_iff);
+
+  phase->register_new_node(thread,        old_ctrl);
+  phase->register_new_node(gc_state_addr, old_ctrl);
+  phase->register_new_node(gc_state,      old_ctrl);
+  phase->register_new_node(gc_state_and,  old_ctrl);
+  phase->register_new_node(gc_state_cmp,  old_ctrl);
+  phase->register_new_node(gc_state_bool, old_ctrl);
+
+  phase->set_ctrl(gc_state_offset, phase->C->root());
+
+  assert(is_gc_state_test(gc_state_iff, flags), "Should match the shape");
 }
 
 void ShenandoahBarrierC2Support::test_null(Node*& ctrl, Node* val, Node*& null_ctrl, PhaseIdealLoop* phase) {
-  const Type* val_t = phase->igvn().type(val);
+  Node* old_ctrl = ctrl;
+  PhaseIterGVN& igvn = phase->igvn();
+
+  const Type* val_t = igvn.type(val);
   if (val_t->meet(TypePtr::NULL_PTR) == val_t) {
-    IdealLoopTree* loop = phase->get_loop(ctrl);
-    Node* null_cmp = new CmpPNode(val, phase->igvn().zerocon(T_OBJECT));
-    phase->register_new_node(null_cmp, ctrl);
-    Node* null_test = new BoolNode(null_cmp, BoolTest::ne);
-    phase->register_new_node(null_test, ctrl);
-    IfNode* null_iff = new IfNode(ctrl, null_test, PROB_LIKELY(0.999), COUNT_UNKNOWN);
-    phase->register_control(null_iff, loop, ctrl);
-    ctrl = new IfTrueNode(null_iff);
-    phase->register_control(ctrl, loop, null_iff);
-    null_ctrl = new IfFalseNode(null_iff);
+    Node* null_cmp   = new CmpPNode(val, igvn.zerocon(T_OBJECT));
+    Node* null_test  = new BoolNode(null_cmp, BoolTest::ne);
+
+    IfNode* null_iff = new IfNode(old_ctrl, null_test, PROB_LIKELY(0.999), COUNT_UNKNOWN);
+    ctrl             = new IfTrueNode(null_iff);
+    null_ctrl        = new IfFalseNode(null_iff);
+
+    IdealLoopTree* loop = phase->get_loop(old_ctrl);
+    phase->register_control(null_iff,  loop, old_ctrl);
+    phase->register_control(ctrl,      loop, null_iff);
     phase->register_control(null_ctrl, loop, null_iff);
+
+    phase->register_new_node(null_cmp,  old_ctrl);
+    phase->register_new_node(null_test, old_ctrl);
   }
 }
 
 Node* ShenandoahBarrierC2Support::clone_null_check(Node*& c, Node* val, Node* unc_ctrl, PhaseIdealLoop* phase) {
   IdealLoopTree *loop = phase->get_loop(c);
@@ -976,37 +986,41 @@
   phase->igvn().rehash_node_delayed(use);
   int nb = use->replace_edge(proj, new_unc_ctrl);
   assert(nb == 1, "only use expected");
 }
 
-void ShenandoahBarrierC2Support::in_cset_fast_test(Node*& ctrl, Node*& not_cset_ctrl, Node* val, Node* raw_mem, PhaseIdealLoop* phase) {
-  IdealLoopTree *loop = phase->get_loop(ctrl);
-  Node* raw_rbtrue = new CastP2XNode(ctrl, val);
-  phase->register_new_node(raw_rbtrue, ctrl);
-  Node* cset_offset = new URShiftXNode(raw_rbtrue, phase->igvn().intcon(ShenandoahHeapRegion::region_size_bytes_shift_jint()));
-  phase->register_new_node(cset_offset, ctrl);
-  Node* in_cset_fast_test_base_addr = phase->igvn().makecon(TypeRawPtr::make(ShenandoahHeap::in_cset_fast_test_addr()));
-  phase->set_ctrl(in_cset_fast_test_base_addr, phase->C->root());
-  Node* in_cset_fast_test_adr = new AddPNode(phase->C->top(), in_cset_fast_test_base_addr, cset_offset);
-  phase->register_new_node(in_cset_fast_test_adr, ctrl);
-  uint in_cset_fast_test_idx = Compile::AliasIdxRaw;
-  const TypePtr* in_cset_fast_test_adr_type = NULL; // debug-mode-only argument
-  debug_only(in_cset_fast_test_adr_type = phase->C->get_adr_type(in_cset_fast_test_idx));
-  Node* in_cset_fast_test_load = new LoadBNode(ctrl, raw_mem, in_cset_fast_test_adr, in_cset_fast_test_adr_type, TypeInt::BYTE, MemNode::unordered);
-  phase->register_new_node(in_cset_fast_test_load, ctrl);
-  Node* in_cset_fast_test_cmp = new CmpINode(in_cset_fast_test_load, phase->igvn().zerocon(T_INT));
-  phase->register_new_node(in_cset_fast_test_cmp, ctrl);
-  Node* in_cset_fast_test_test = new BoolNode(in_cset_fast_test_cmp, BoolTest::eq);
-  phase->register_new_node(in_cset_fast_test_test, ctrl);
-  IfNode* in_cset_fast_test_iff = new IfNode(ctrl, in_cset_fast_test_test, PROB_UNLIKELY(0.999), COUNT_UNKNOWN);
-  phase->register_control(in_cset_fast_test_iff, loop, ctrl);
-
-  not_cset_ctrl = new IfTrueNode(in_cset_fast_test_iff);
-  phase->register_control(not_cset_ctrl, loop, in_cset_fast_test_iff);
-
-  ctrl = new IfFalseNode(in_cset_fast_test_iff);
-  phase->register_control(ctrl, loop, in_cset_fast_test_iff);
+void ShenandoahBarrierC2Support::test_in_cset(Node*& ctrl, Node*& not_cset_ctrl, Node* val, Node* raw_mem, PhaseIdealLoop* phase) {
+  Node* old_ctrl = ctrl;
+  PhaseIterGVN& igvn = phase->igvn();
+
+  Node* raw_val        = new CastP2XNode(old_ctrl, val);
+  Node* cset_idx       = new URShiftXNode(raw_val, igvn.intcon(ShenandoahHeapRegion::region_size_bytes_shift_jint()));
+  Node* cset_addr      = igvn.makecon(TypeRawPtr::make(ShenandoahHeap::in_cset_fast_test_addr()));
+  Node* cset_load_addr = new AddPNode(phase->C->top(), cset_addr, cset_idx);
+  Node* cset_load      = new LoadBNode(old_ctrl, raw_mem, cset_load_addr,
+                                       DEBUG_ONLY(phase->C->get_adr_type(Compile::AliasIdxRaw)) NOT_DEBUG(NULL),
+                                       TypeInt::BYTE, MemNode::unordered);
+  Node* cset_cmp       = new CmpINode(cset_load, igvn.zerocon(T_INT));
+  Node* cset_bool      = new BoolNode(cset_cmp, BoolTest::ne);
+
+  IfNode* cset_iff     = new IfNode(old_ctrl, cset_bool, PROB_UNLIKELY(0.999), COUNT_UNKNOWN);
+  ctrl                 = new IfTrueNode(cset_iff);
+  not_cset_ctrl        = new IfFalseNode(cset_iff);
+
+  IdealLoopTree *loop = phase->get_loop(old_ctrl);
+  phase->register_control(cset_iff,      loop, old_ctrl);
+  phase->register_control(ctrl,          loop, cset_iff);
+  phase->register_control(not_cset_ctrl, loop, cset_iff);
+
+  phase->set_ctrl(cset_addr, phase->C->root());
+
+  phase->register_new_node(raw_val,        old_ctrl);
+  phase->register_new_node(cset_idx,       old_ctrl);
+  phase->register_new_node(cset_load_addr, old_ctrl);
+  phase->register_new_node(cset_load,      old_ctrl);
+  phase->register_new_node(cset_cmp,       old_ctrl);
+  phase->register_new_node(cset_bool,      old_ctrl);
 }
 
 void ShenandoahBarrierC2Support::call_lrb_stub(Node*& ctrl, Node*& val, Node* load_addr, Node*& result_mem, Node* raw_mem, bool is_native, PhaseIdealLoop* phase) {
   IdealLoopTree*loop = phase->get_loop(ctrl);
   const TypePtr* obj_type = phase->igvn().type(val)->is_oopptr();
@@ -1432,11 +1446,11 @@
     Node* region = new RegionNode(PATH_LIMIT);
     Node* val_phi = new PhiNode(region, uncasted_val->bottom_type()->is_oopptr());
     Node* raw_mem_phi = PhiNode::make(region, raw_mem, Type::MEMORY, TypeRawPtr::BOTTOM);
 
     // Stable path.
-    test_heap_state(ctrl, raw_mem, heap_stable_ctrl, phase, ShenandoahHeap::HAS_FORWARDED);
+    test_gc_state(ctrl, raw_mem, heap_stable_ctrl, phase, ShenandoahHeap::HAS_FORWARDED);
     IfNode* heap_stable_iff = heap_stable_ctrl->in(0)->as_If();
 
     // Heap stable case
     region->init_req(_heap_stable, heap_stable_ctrl);
     val_phi->init_req(_heap_stable, uncasted_val);
@@ -1457,11 +1471,11 @@
     }
 
     // Test for in-cset.
     // Wires !in_cset(obj) to slot 2 of region and phis
     Node* not_cset_ctrl = NULL;
-    in_cset_fast_test(ctrl, not_cset_ctrl, uncasted_val, raw_mem, phase);
+    test_in_cset(ctrl, not_cset_ctrl, uncasted_val, raw_mem, phase);
     if (not_cset_ctrl != NULL) {
       if (reg2_ctrl == NULL) reg2_ctrl = not_cset_ctrl->in(0);
       region->init_req(_not_cset, not_cset_ctrl);
       val_phi->init_req(_not_cset, uncasted_val);
       raw_mem_phi->init_req(_not_cset, raw_mem);
@@ -1603,11 +1617,11 @@
     enum { _fast_path = 1, _slow_path, _null_path, PATH_LIMIT2 };
     Node* region2 = new RegionNode(PATH_LIMIT2);
     Node* phi2 = PhiNode::make(region2, raw_mem, Type::MEMORY, TypeRawPtr::BOTTOM);
 
     // Stable path.
-    test_heap_state(ctrl, raw_mem, heap_stable_ctrl, phase, ShenandoahHeap::MARKING);
+    test_gc_state(ctrl, raw_mem, heap_stable_ctrl, phase, ShenandoahHeap::MARKING);
     region->init_req(_heap_stable, heap_stable_ctrl);
     phi->init_req(_heap_stable, raw_mem);
 
     // Null path
     Node* reg2_ctrl = NULL;
@@ -1785,11 +1799,11 @@
       return phase->igvn().zerocon(T_OBJECT);
   }
 
 }
 
-void ShenandoahBarrierC2Support::move_heap_stable_test_out_of_loop(IfNode* iff, PhaseIdealLoop* phase) {
+void ShenandoahBarrierC2Support::move_gc_state_test_out_of_loop(IfNode* iff, PhaseIdealLoop* phase) {
   IdealLoopTree *loop = phase->get_loop(iff);
   Node* loop_head = loop->_head;
   Node* entry_c = loop_head->in(LoopNode::EntryControl);
 
   Node* bol = iff->in(1);
@@ -1979,11 +1993,11 @@
           if (iff != NULL) {
             Node* bol = iff->in(1);
             if (head->as_Loop()->is_strip_mined()) {
               head->as_Loop()->verify_strip_mined(0);
             }
-            move_heap_stable_test_out_of_loop(iff, phase);
+            move_gc_state_test_out_of_loop(iff, phase);
 
             AutoNodeBudget node_budget(phase);
 
             if (loop->policy_unswitching(phase)) {
               if (head->as_Loop()->is_strip_mined()) {
diff a/src/hotspot/share/jvmci/jvmciCodeInstaller.cpp b/src/hotspot/share/jvmci/jvmciCodeInstaller.cpp
--- a/src/hotspot/share/jvmci/jvmciCodeInstaller.cpp
+++ b/src/hotspot/share/jvmci/jvmciCodeInstaller.cpp
@@ -20,10 +20,11 @@
  * or visit www.oracle.com if you need additional information or have any
  * questions.
  */
 
 #include "precompiled.hpp"
+#include "classfile/javaClasses.inline.hpp"
 #include "code/compiledIC.hpp"
 #include "compiler/compileBroker.hpp"
 #include "jvmci/jvmciCodeInstaller.hpp"
 #include "jvmci/jvmciCompilerToVM.hpp"
 #include "jvmci/jvmciRuntime.hpp"
diff a/src/hotspot/share/memory/metaspace.cpp b/src/hotspot/share/memory/metaspace.cpp
--- a/src/hotspot/share/memory/metaspace.cpp
+++ b/src/hotspot/share/memory/metaspace.cpp
@@ -1246,11 +1246,16 @@
   }
 #endif // INCLUDE_CDS
 
 #ifdef _LP64
   if (using_class_space() && !class_space_inited) {
-    char* base = (char*)align_up(CompressedOops::end(), _reserve_alignment);
+    char* base;
+    if (UseCompressedOops) {
+      base = (char*)align_up(CompressedOops::end(), _reserve_alignment);
+    } else {
+      base = (char*)HeapBaseMinAddress;
+    }
     ReservedSpace dummy;
     allocate_metaspace_compressed_klass_ptrs(dummy, base, 0);
   }
 #endif
 
diff a/src/hotspot/share/oops/instanceKlass.hpp b/src/hotspot/share/oops/instanceKlass.hpp
--- a/src/hotspot/share/oops/instanceKlass.hpp
+++ b/src/hotspot/share/oops/instanceKlass.hpp
@@ -1069,10 +1069,11 @@
   Array<AnnotationArray*>* fields_type_annotations() const {
     return (_annotations != NULL) ? _annotations->fields_type_annotations() : NULL;
   }
   // allocation
   instanceOop allocate_instance(TRAPS);
+  static instanceOop allocate_instance(oop cls, TRAPS);
 
   // additional member function to return a handle
   instanceHandle allocate_instance_handle(TRAPS);
 
   objArrayOop allocate_objArray(int n, int length, TRAPS);
diff a/src/hotspot/share/opto/compile.cpp b/src/hotspot/share/opto/compile.cpp
--- a/src/hotspot/share/opto/compile.cpp
+++ b/src/hotspot/share/opto/compile.cpp
@@ -4979,11 +4979,10 @@
     NodeCloneInfo ni(val);
     ni.dump();
   }
 }
 
-
 // Move Allocate nodes to the start of the list
 void Compile::sort_macro_nodes() {
   int count = macro_count();
   int allocates = 0;
   for (int i = 0; i < count; i++) {
@@ -4996,5 +4995,81 @@
       }
       allocates++;
     }
   }
 }
+
+
+#ifndef PRODUCT
+IdealGraphPrinter* Compile::_debug_file_printer = NULL;
+IdealGraphPrinter* Compile::_debug_network_printer = NULL;
+
+// Called from debugger. Prints method to the default file with the default phase name.
+// This works regardless of any Ideal Graph Visualizer flags set or not.
+void igv_print() {
+  Compile::current()->igv_print_method_to_file();
+}
+
+// Same as igv_print() above but with a specified phase name.
+void igv_print(const char* phase_name) {
+  Compile::current()->igv_print_method_to_file(phase_name);
+}
+
+// Called from debugger. Prints method with the default phase name to the default network or the one specified with
+// the network flags for the Ideal Graph Visualizer, or to the default file depending on the 'network' argument.
+// This works regardless of any Ideal Graph Visualizer flags set or not.
+void igv_print(bool network) {
+  if (network) {
+    Compile::current()->igv_print_method_to_network();
+  } else {
+    Compile::current()->igv_print_method_to_file();
+  }
+}
+
+// Same as igv_print(bool network) above but with a specified phase name.
+void igv_print(bool network, const char* phase_name) {
+  if (network) {
+    Compile::current()->igv_print_method_to_network(phase_name);
+  } else {
+    Compile::current()->igv_print_method_to_file(phase_name);
+  }
+}
+
+// Called from debugger. Normal write to the default _printer. Only works if Ideal Graph Visualizer printing flags are set.
+void igv_print_default() {
+  Compile::current()->print_method(PHASE_DEBUG, 0, 0);
+}
+
+// Called from debugger, especially when replaying a trace in which the program state cannot be altered like with rr replay.
+// A method is appended to an existing default file with the default phase name. This means that igv_append() must follow
+// an earlier igv_print(*) call which sets up the file. This works regardless of any Ideal Graph Visualizer flags set or not.
+void igv_append() {
+  Compile::current()->igv_print_method_to_file("Debug", true);
+}
+
+// Same as igv_append() above but with a specified phase name.
+void igv_append(const char* phase_name) {
+  Compile::current()->igv_print_method_to_file(phase_name, true);
+}
+
+void Compile::igv_print_method_to_file(const char* phase_name, bool append) {
+  const char* file_name = "custom_debug.xml";
+  if (_debug_file_printer == NULL) {
+    _debug_file_printer = new IdealGraphPrinter(C, file_name, append);
+  } else {
+    _debug_file_printer->update_compiled_method(C->method());
+  }
+  tty->print_cr("Method %s to %s", append ? "appended" : "printed", file_name);
+  _debug_file_printer->print_method(phase_name, 0);
+}
+
+void Compile::igv_print_method_to_network(const char* phase_name) {
+  if (_debug_network_printer == NULL) {
+    _debug_network_printer = new IdealGraphPrinter(C);
+  } else {
+    _debug_network_printer->update_compiled_method(C->method());
+  }
+  tty->print_cr("Method printed over network stream to IGV");
+  _debug_network_printer->print_method(phase_name, 0);
+}
+#endif
+
diff a/src/hotspot/share/opto/compile.hpp b/src/hotspot/share/opto/compile.hpp
--- a/src/hotspot/share/opto/compile.hpp
+++ b/src/hotspot/share/opto/compile.hpp
@@ -320,10 +320,12 @@
   GrowableArray<Node*>* _opaque4_nodes;         // List of Opaque4 nodes that have a default value
   Unique_Node_List*     _value_type_nodes;      // List of ValueType nodes
   ConnectionGraph*      _congraph;
 #ifndef PRODUCT
   IdealGraphPrinter*    _printer;
+  static IdealGraphPrinter* _debug_file_printer;
+  static IdealGraphPrinter* _debug_network_printer;
 #endif
 
 
   // Node management
   uint                  _unique;                // Counter for unique Node indices
@@ -649,20 +651,27 @@
 
 #ifndef PRODUCT
     if (should_print(level)) {
       char output[1024];
       if (idx != 0) {
-        sprintf(output, "%s:%d", CompilerPhaseTypeHelper::to_string(cpt), idx);
+        jio_snprintf(output, sizeof(output), "%s:%d", CompilerPhaseTypeHelper::to_string(cpt), idx);
       } else {
-        sprintf(output, "%s", CompilerPhaseTypeHelper::to_string(cpt));
+        jio_snprintf(output, sizeof(output), "%s", CompilerPhaseTypeHelper::to_string(cpt));
       }
       _printer->print_method(output, level);
     }
 #endif
     C->_latest_stage_start_counter.stamp();
   }
 
+#ifndef PRODUCT
+  void igv_print_method_to_file(const char* phase_name = "Debug", bool append = false);
+  void igv_print_method_to_network(const char* phase_name = "Debug");
+  static IdealGraphPrinter* debug_file_printer() { return _debug_file_printer; }
+  static IdealGraphPrinter* debug_network_printer() { return _debug_network_printer; }
+#endif
+
   void end_method(int level = 1) {
     EventCompilerPhase event;
     if (event.should_commit()) {
       CompilerEvent::PhaseEvent::post(event, C->_latest_stage_start_counter, PHASE_END, C->_compile_id, level);
     }
diff a/src/hotspot/share/opto/graphKit.cpp b/src/hotspot/share/opto/graphKit.cpp
--- a/src/hotspot/share/opto/graphKit.cpp
+++ b/src/hotspot/share/opto/graphKit.cpp
@@ -2214,11 +2214,12 @@
 
   call->set_req(TypeFunc::ReturnAdr, returnadr());
   // The debug info is the only real input to this call.
 
   // Halt-and-catch fire here.  The above call should never return!
-  HaltNode* halt = new HaltNode(control(), frameptr(), "uncommon trap returned which should never happen");
+  HaltNode* halt = new HaltNode(control(), frameptr(), "uncommon trap returned which should never happen"
+                                                       PRODUCT_ONLY(COMMA /*reachable*/false));
   _gvn.set_type_bottom(halt);
   root()->add_req(halt);
 
   stop_and_kill_map();
 }
diff a/src/hotspot/share/opto/lcm.cpp b/src/hotspot/share/opto/lcm.cpp
--- a/src/hotspot/share/opto/lcm.cpp
+++ b/src/hotspot/share/opto/lcm.cpp
@@ -263,12 +263,12 @@
              !CompressedOops::use_implicit_null_checks()))
           continue;
         // cannot reason about it; is probably not implicit null exception
       } else {
         const TypePtr* tptr;
-        if (UseCompressedOops && (CompressedOops::shift() == 0 ||
-                                  CompressedKlassPointers::shift() == 0)) {
+        if ((UseCompressedOops || UseCompressedClassPointers) &&
+            (CompressedOops::shift() == 0 || CompressedKlassPointers::shift() == 0)) {
           // 32-bits narrow oop can be the base of address expressions
           tptr = base->get_ptr_type();
         } else {
           // only regular oops are expected here
           tptr = base->bottom_type()->is_ptr();
diff a/src/hotspot/share/opto/machnode.hpp b/src/hotspot/share/opto/machnode.hpp
--- a/src/hotspot/share/opto/machnode.hpp
+++ b/src/hotspot/share/opto/machnode.hpp
@@ -1045,12 +1045,16 @@
 
 //------------------------------MachHaltNode-----------------------------------
 // Machine-specific versions of halt nodes
 class MachHaltNode : public MachReturnNode {
 public:
+  bool _reachable;
   const char* _halt_reason;
   virtual JVMState* jvms() const;
+  bool is_reachable() const {
+    return _reachable;
+  }
 };
 
 class MachMemBarNode : public MachNode {
   virtual uint size_of() const; // Size is bigger
 public:
diff a/src/hotspot/share/opto/mulnode.cpp b/src/hotspot/share/opto/mulnode.cpp
--- a/src/hotspot/share/opto/mulnode.cpp
+++ b/src/hotspot/share/opto/mulnode.cpp
@@ -1,7 +1,7 @@
 /*
- * Copyright (c) 1997, 2017, Oracle and/or its affiliates. All rights reserved.
+ * Copyright (c) 1997, 2020, Oracle and/or its affiliates. All rights reserved.
  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  *
  * This code is free software; you can redistribute it and/or modify it
  * under the terms of the GNU General Public License version 2 only, as
  * published by the Free Software Foundation.
@@ -1183,10 +1183,22 @@
   Node *shl = in(1);
   if( in1_op == Op_LShiftI &&
       phase->type(shl->in(2)) == t2 )
     return new AndINode( shl->in(1), phase->intcon(mask) );
 
+  // Check for (x >> n) >>> 31. Replace with (x >>> 31)
+  Node *shr = in(1);
+  if ( in1_op == Op_RShiftI ) {
+    Node *in11 = shr->in(1);
+    Node *in12 = shr->in(2);
+    const TypeInt *t11 = phase->type(in11)->isa_int();
+    const TypeInt *t12 = phase->type(in12)->isa_int();
+    if ( t11 && t2 && t2->is_con(31) && t12 && t12->is_con() ) {
+      return new URShiftINode(in11, phase->intcon(31));
+    }
+  }
+
   return NULL;
 }
 
 //------------------------------Value------------------------------------------
 // A URShiftINode shifts its input2 right by input1 amount.
@@ -1312,10 +1324,21 @@
   Node *shl = in(1);
   if( shl->Opcode() == Op_LShiftL &&
       phase->type(shl->in(2)) == t2 )
     return new AndLNode( shl->in(1), phase->longcon(mask) );
 
+  // Check for (x >> n) >>> 63. Replace with (x >>> 63)
+  Node *shr = in(1);
+  if ( shr->Opcode() == Op_RShiftL ) {
+    Node *in11 = shr->in(1);
+    Node *in12 = shr->in(2);
+    const TypeLong *t11 = phase->type(in11)->isa_long();
+    const TypeInt *t12 = phase->type(in12)->isa_int();
+    if ( t11 && t2 && t2->is_con(63) && t12 && t12->is_con() ) {
+      return new URShiftLNode(in11, phase->intcon(63));
+    }
+  }
   return NULL;
 }
 
 //------------------------------Value------------------------------------------
 // A URShiftINode shifts its input2 right by input1 amount.
diff a/src/hotspot/share/opto/phasetype.hpp b/src/hotspot/share/opto/phasetype.hpp
--- a/src/hotspot/share/opto/phasetype.hpp
+++ b/src/hotspot/share/opto/phasetype.hpp
@@ -59,11 +59,11 @@
   PHASE_ADD_UNSAFE_BARRIER,
   PHASE_END,
   PHASE_FAILURE,
   PHASE_SPLIT_VALUES_ARRAY,
   PHASE_SPLIT_VALUES_ARRAY_IGVN,
-
+  PHASE_DEBUG,
   PHASE_NUM_TYPES
 };
 
 class CompilerPhaseTypeHelper {
   public:
@@ -102,10 +102,11 @@
       case PHASE_ADD_UNSAFE_BARRIER:         return "Add barrier to unsafe op";
       case PHASE_END:                        return "End";
       case PHASE_FAILURE:                    return "Failure";
       case PHASE_SPLIT_VALUES_ARRAY:         return "Split values array";
       case PHASE_SPLIT_VALUES_ARRAY_IGVN:    return "IGVN after split values array";
+      case PHASE_DEBUG:                      return "Debug";
       default:
         ShouldNotReachHere();
         return NULL;
     }
   }
diff a/src/hotspot/share/opto/subnode.cpp b/src/hotspot/share/opto/subnode.cpp
--- a/src/hotspot/share/opto/subnode.cpp
+++ b/src/hotspot/share/opto/subnode.cpp
@@ -1,7 +1,7 @@
 /*
- * Copyright (c) 1997, 2018, Oracle and/or its affiliates. All rights reserved.
+ * Copyright (c) 1997, 2020, Oracle and/or its affiliates. All rights reserved.
  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  *
  * This code is free software; you can redistribute it and/or modify it
  * under the terms of the GNU General Public License version 2 only, as
  * published by the Free Software Foundation.
@@ -250,10 +250,22 @@
   if( op2 == Op_SubI && in2->outcnt() == 1) {
     Node *add1 = phase->transform( new AddINode( in1, in2->in(2) ) );
     return new SubINode( add1, in2->in(1) );
   }
 
+  // Convert "0-(A>>31)" into "(A>>>31)"
+  if ( op2 == Op_RShiftI ) {
+    Node *in21 = in2->in(1);
+    Node *in22 = in2->in(2);
+    const TypeInt *zero = phase->type(in1)->isa_int();
+    const TypeInt *t21 = phase->type(in21)->isa_int();
+    const TypeInt *t22 = phase->type(in22)->isa_int();
+    if ( t21 && t22 && zero == TypeInt::ZERO && t22->is_con(31) ) {
+      return new URShiftINode(in21, in22);
+    }
+  }
+
   return NULL;
 }
 
 //------------------------------sub--------------------------------------------
 // A subtract node differences it's two inputs.
@@ -359,10 +371,22 @@
   if( op2 == Op_SubL && in2->outcnt() == 1) {
     Node *add1 = phase->transform( new AddLNode( in1, in2->in(2) ) );
     return new SubLNode( add1, in2->in(1) );
   }
 
+  // Convert "0L-(A>>63)" into "(A>>>63)"
+  if ( op2 == Op_RShiftL ) {
+    Node *in21 = in2->in(1);
+    Node *in22 = in2->in(2);
+    const TypeLong *zero = phase->type(in1)->isa_long();
+    const TypeLong *t21 = phase->type(in21)->isa_long();
+    const TypeInt *t22 = phase->type(in22)->isa_int();
+    if ( t21 && t22 && zero == TypeLong::ZERO && t22->is_con(63) ) {
+      return new URShiftLNode(in21, in22);
+    }
+  }
+
   return NULL;
 }
 
 //------------------------------sub--------------------------------------------
 // A subtract node differences it's two inputs.
diff a/src/hotspot/share/prims/jni.cpp b/src/hotspot/share/prims/jni.cpp
--- a/src/hotspot/share/prims/jni.cpp
+++ b/src/hotspot/share/prims/jni.cpp
@@ -47,11 +47,11 @@
 #include "memory/oopFactory.hpp"
 #include "memory/resourceArea.hpp"
 #include "memory/universe.hpp"
 #include "oops/access.inline.hpp"
 #include "oops/arrayOop.inline.hpp"
-#include "oops/instanceKlass.hpp"
+#include "oops/instanceKlass.inline.hpp"
 #include "oops/instanceOop.hpp"
 #include "oops/markWord.hpp"
 #include "oops/method.hpp"
 #include "oops/objArrayKlass.hpp"
 #include "oops/objArrayOop.inline.hpp"
@@ -1066,23 +1066,10 @@
   if (is_reference_type(result->get_type())) {
     result->set_jobject(JNIHandles::make_local(env, (oop) result->get_jobject()));
   }
 }
 
-
-static instanceOop alloc_object(jclass clazz, TRAPS) {
-  Klass* k = java_lang_Class::as_Klass(JNIHandles::resolve_non_null(clazz));
-  if (k == NULL) {
-    ResourceMark rm(THREAD);
-    THROW_(vmSymbols::java_lang_InstantiationException(), NULL);
-  }
-  k->check_valid_for_instantiation(false, CHECK_NULL);
-  k->initialize(CHECK_NULL);
-  instanceOop ih = InstanceKlass::cast(k)->allocate_instance(THREAD);
-  return ih;
-}
-
 DT_RETURN_MARK_DECL(AllocObject, jobject
                     , HOTSPOT_JNI_ALLOCOBJECT_RETURN(_ret_ref));
 
 JNI_ENTRY(jobject, jni_AllocObject(JNIEnv *env, jclass clazz))
   JNIWrapper("AllocObject");
@@ -1090,11 +1077,11 @@
   HOTSPOT_JNI_ALLOCOBJECT_ENTRY(env, clazz);
 
   jobject ret = NULL;
   DT_RETURN_MARK(AllocObject, jobject, (const jobject&)ret);
 
-  instanceOop i = alloc_object(clazz, CHECK_NULL);
+  instanceOop i = InstanceKlass::allocate_instance(JNIHandles::resolve_non_null(clazz), CHECK_NULL);
   ret = JNIHandles::make_local(env, i);
   return ret;
 JNI_END
 
 DT_RETURN_MARK_DECL(NewObjectA, jobject
@@ -1106,18 +1093,19 @@
   HOTSPOT_JNI_NEWOBJECTA_ENTRY(env, clazz, (uintptr_t) methodID);
 
   jobject obj = NULL;
   DT_RETURN_MARK(NewObjectA, jobject, (const jobject)obj);
 
-  Klass* k = java_lang_Class::as_Klass(JNIHandles::resolve_non_null(clazz));
+  oop clazzoop = JNIHandles::resolve_non_null(clazz);
+  Klass* k = java_lang_Class::as_Klass(clazzoop);
   if (k == NULL) {
     ResourceMark rm(THREAD);
     THROW_(vmSymbols::java_lang_InstantiationException(), NULL);
   }
 
   if (!k->is_value()) {
-    instanceOop i = alloc_object(clazz, CHECK_NULL);
+    instanceOop i = InstanceKlass::allocate_instance(clazzoop, CHECK_NULL);
     obj = JNIHandles::make_local(env, i);
     JavaValue jvalue(T_VOID);
     JNI_ArgumentPusherArray ap(methodID, args);
     jni_invoke_nonstatic(env, &jvalue, obj, JNI_NONVIRTUAL, methodID, &ap, CHECK_NULL);
   } else {
@@ -1139,18 +1127,19 @@
   HOTSPOT_JNI_NEWOBJECTV_ENTRY(env, clazz, (uintptr_t) methodID);
 
   jobject obj = NULL;
   DT_RETURN_MARK(NewObjectV, jobject, (const jobject&)obj);
 
-  Klass* k = java_lang_Class::as_Klass(JNIHandles::resolve_non_null(clazz));
+  oop clazzoop = JNIHandles::resolve_non_null(clazz);
+  Klass* k = java_lang_Class::as_Klass(clazzoop);
   if (k == NULL) {
     ResourceMark rm(THREAD);
     THROW_(vmSymbols::java_lang_InstantiationException(), NULL);
   }
 
   if (!k->is_value()) {
-    instanceOop i = alloc_object(clazz, CHECK_NULL);
+    instanceOop i = InstanceKlass::allocate_instance(clazzoop, CHECK_NULL);
     obj = JNIHandles::make_local(env, i);
     JavaValue jvalue(T_VOID);
     JNI_ArgumentPusherVaArg ap(methodID, args);
     jni_invoke_nonstatic(env, &jvalue, obj, JNI_NONVIRTUAL, methodID, &ap, CHECK_NULL);
   } else {
@@ -1172,18 +1161,19 @@
   HOTSPOT_JNI_NEWOBJECT_ENTRY(env, clazz, (uintptr_t) methodID);
 
   jobject obj = NULL;
   DT_RETURN_MARK(NewObject, jobject, (const jobject&)obj);
 
-  Klass* k = java_lang_Class::as_Klass(JNIHandles::resolve_non_null(clazz));
+  oop clazzoop = JNIHandles::resolve_non_null(clazz);
+  Klass* k = java_lang_Class::as_Klass(clazzoop);
   if (k == NULL) {
     ResourceMark rm(THREAD);
     THROW_(vmSymbols::java_lang_InstantiationException(), NULL);
   }
 
   if (!k->is_value()) {
-    instanceOop i = alloc_object(clazz, CHECK_NULL);
+    instanceOop i = InstanceKlass::allocate_instance(clazzoop, CHECK_NULL);
     obj = JNIHandles::make_local(env, i);
     va_list args;
     va_start(args, methodID);
     JavaValue jvalue(T_VOID);
     JNI_ArgumentPusherVaArg ap(methodID, args);
diff a/src/hotspot/share/prims/unsafe.cpp b/src/hotspot/share/prims/unsafe.cpp
--- a/src/hotspot/share/prims/unsafe.cpp
+++ b/src/hotspot/share/prims/unsafe.cpp
@@ -25,18 +25,20 @@
 #include "precompiled.hpp"
 #include "jni.h"
 #include "jvm.h"
 #include "classfile/classFileStream.hpp"
 #include "classfile/classLoader.hpp"
+#include "classfile/javaClasses.inline.hpp"
 #include "classfile/vmSymbols.hpp"
 #include "jfr/jfrEvents.hpp"
 #include "memory/allocation.inline.hpp"
 #include "memory/resourceArea.hpp"
 #include "logging/log.hpp"
 #include "logging/logStream.hpp"
 #include "oops/access.inline.hpp"
 #include "oops/fieldStreams.inline.hpp"
+#include "oops/instanceKlass.inline.hpp"
 #include "oops/objArrayOop.inline.hpp"
 #include "oops/oop.inline.hpp"
 #include "oops/typeArrayOop.inline.hpp"
 #include "oops/valueArrayKlass.hpp"
 #include "oops/valueArrayOop.inline.hpp"
@@ -477,12 +479,12 @@
 } UNSAFE_END
 
 ////// Allocation requests
 
 UNSAFE_ENTRY(jobject, Unsafe_AllocateInstance(JNIEnv *env, jobject unsafe, jclass cls)) {
-  ThreadToNativeFromVM ttnfv(thread);
-  return env->AllocObject(cls);
+  instanceOop i = InstanceKlass::allocate_instance(JNIHandles::resolve_non_null(cls), CHECK_NULL);
+  return JNIHandles::make_local(env, i);
 } UNSAFE_END
 
 UNSAFE_ENTRY(jlong, Unsafe_AllocateMemory0(JNIEnv *env, jobject unsafe, jlong size)) {
   size_t sz = (size_t)size;
 
diff a/src/hotspot/share/prims/whitebox.cpp b/src/hotspot/share/prims/whitebox.cpp
--- a/src/hotspot/share/prims/whitebox.cpp
+++ b/src/hotspot/share/prims/whitebox.cpp
@@ -25,10 +25,11 @@
 #include "precompiled.hpp"
 
 #include <new>
 
 #include "classfile/classLoaderDataGraph.hpp"
+#include "classfile/javaClasses.inline.hpp"
 #include "classfile/modules.hpp"
 #include "classfile/protectionDomainCache.hpp"
 #include "classfile/stringTable.hpp"
 #include "classfile/symbolTable.hpp"
 #include "code/codeCache.hpp"
diff a/src/hotspot/share/runtime/arguments.cpp b/src/hotspot/share/runtime/arguments.cpp
--- a/src/hotspot/share/runtime/arguments.cpp
+++ b/src/hotspot/share/runtime/arguments.cpp
@@ -1664,11 +1664,13 @@
     }
   } else {
     if (UseCompressedOops && !FLAG_IS_DEFAULT(UseCompressedOops)) {
       warning("Max heap size too large for Compressed Oops");
       FLAG_SET_DEFAULT(UseCompressedOops, false);
-      FLAG_SET_DEFAULT(UseCompressedClassPointers, false);
+      if (COMPRESSED_CLASS_POINTERS_DEPENDS_ON_COMPRESSED_OOPS) {
+        FLAG_SET_DEFAULT(UseCompressedClassPointers, false);
+      }
     }
   }
 #endif // _LP64
 #endif // ZERO
 }
@@ -1677,12 +1679,18 @@
 // NOTE: set_use_compressed_klass_ptrs() must be called after calling
 // set_use_compressed_oops().
 void Arguments::set_use_compressed_klass_ptrs() {
 #ifndef ZERO
 #ifdef _LP64
-  // UseCompressedOops must be on for UseCompressedClassPointers to be on.
-  if (!UseCompressedOops) {
+  // On some architectures, the use of UseCompressedClassPointers implies the use of
+  // UseCompressedOops. The reason is that the rheap_base register of said platforms
+  // is reused to perform some optimized spilling, in order to use rheap_base as a
+  // temp register. But by treating it as any other temp register, spilling can typically
+  // be completely avoided instead. So it is better not to perform this trick. And by
+  // not having that reliance, large heaps, or heaps not supporting compressed oops,
+  // can still use compressed class pointers.
+  if (COMPRESSED_CLASS_POINTERS_DEPENDS_ON_COMPRESSED_OOPS && !UseCompressedOops) {
     if (UseCompressedClassPointers) {
       warning("UseCompressedClassPointers requires UseCompressedOops");
     }
     FLAG_SET_DEFAULT(UseCompressedClassPointers, false);
   } else {
@@ -1807,14 +1815,11 @@
       // Limit the heap size to ErgoHeapSizeLimit
       reasonable_max = MIN2(reasonable_max, (julong)ErgoHeapSizeLimit);
     }
 
 #ifdef _LP64
-    if (UseCompressedOops) {
-      // Limit the heap size to the maximum possible when using compressed oops
-      julong max_coop_heap = (julong)max_heap_for_compressed_oops();
-
+    if (UseCompressedOops || UseCompressedClassPointers) {
       // HeapBaseMinAddress can be greater than default but not less than.
       if (!FLAG_IS_DEFAULT(HeapBaseMinAddress)) {
         if (HeapBaseMinAddress < DefaultHeapBaseMinAddress) {
           // matches compressed oops printing flags
           log_debug(gc, heap, coops)("HeapBaseMinAddress must be at least " SIZE_FORMAT
@@ -1823,10 +1828,14 @@
                                      DefaultHeapBaseMinAddress/G,
                                      HeapBaseMinAddress);
           FLAG_SET_ERGO(HeapBaseMinAddress, DefaultHeapBaseMinAddress);
         }
       }
+    }
+    if (UseCompressedOops) {
+      // Limit the heap size to the maximum possible when using compressed oops
+      julong max_coop_heap = (julong)max_heap_for_compressed_oops();
 
       if (HeapBaseMinAddress + MaxHeapSize < max_coop_heap) {
         // Heap should be above HeapBaseMinAddress to get zero based compressed oops
         // but it should be not less than default MaxHeapSize.
         max_coop_heap -= HeapBaseMinAddress;
@@ -1841,11 +1850,13 @@
           log_info(cds)("UseCompressedOops and UseCompressedClassPointers have been disabled due to"
             " max heap " SIZE_FORMAT " > compressed oop heap " SIZE_FORMAT ". "
             "Please check the setting of MaxRAMPercentage %5.2f."
             ,(size_t)reasonable_max, (size_t)max_coop_heap, MaxRAMPercentage);
           FLAG_SET_ERGO(UseCompressedOops, false);
-          FLAG_SET_ERGO(UseCompressedClassPointers, false);
+          if (COMPRESSED_CLASS_POINTERS_DEPENDS_ON_COMPRESSED_OOPS) {
+            FLAG_SET_ERGO(UseCompressedClassPointers, false);
+          }
         } else {
           reasonable_max = MIN2(reasonable_max, max_coop_heap);
         }
       }
     }
diff a/src/hotspot/share/runtime/deoptimization.cpp b/src/hotspot/share/runtime/deoptimization.cpp
--- a/src/hotspot/share/runtime/deoptimization.cpp
+++ b/src/hotspot/share/runtime/deoptimization.cpp
@@ -24,10 +24,11 @@
  *
  */
 
 #include "precompiled.hpp"
 #include "jvm.h"
+#include "classfile/javaClasses.inline.hpp"
 #include "classfile/symbolTable.hpp"
 #include "classfile/systemDictionary.hpp"
 #include "code/codeCache.hpp"
 #include "code/debugInfoRec.hpp"
 #include "code/nmethod.hpp"
diff a/test/hotspot/jtreg/ProblemList.txt b/test/hotspot/jtreg/ProblemList.txt
--- a/test/hotspot/jtreg/ProblemList.txt
+++ b/test/hotspot/jtreg/ProblemList.txt
@@ -170,10 +170,11 @@
 
 # :hotspot_runtime
 
 runtime/jni/terminatedThread/TestTerminatedThread.java 8219652 aix-ppc64
 runtime/ReservedStack/ReservedStackTest.java 8231031 generic-all
+runtime/cds/DeterministicDump.java 8244536 windows-all 
 
 # Valhalla TODO:
 runtime/CompressedOops/CompressedClassPointers.java 8210258 generic-all
 runtime/RedefineTests/RedefineLeak.java 8205032 generic-all
 runtime/SharedArchiveFile/BootAppendTests.java 8210258 generic-all
@@ -204,10 +205,11 @@
 serviceability/sa/TestInstanceKlassSizeForInterface.java 8230664 linux-ppc64le,linux-ppc64
 serviceability/sa/TestRevPtrsForInvokeDynamic.java 8241235 generic-all
 
 serviceability/jvmti/HeapMonitor/MyPackage/HeapMonitorStatIntervalTest.java 8214032 generic-all
 serviceability/jvmti/HeapMonitor/MyPackage/HeapMonitorStatArrayCorrectnessTest.java 8224150 generic-all
+serviceability/jvmti/HiddenClass/P/Q/HiddenClassSigTest.java 8244571 generic-all
 
 # Valhalla TODO:
 serviceability/sa/ClhsdbCDSCore.java 8190936 generic-all
 serviceability/sa/ClhsdbCDSJstackPrintAll.java 8190936 generic-all
 serviceability/sa/ClhsdbFindPC.java 8190936 generic-all
diff a/test/hotspot/jtreg/TEST.groups b/test/hotspot/jtreg/TEST.groups
--- a/test/hotspot/jtreg/TEST.groups
+++ b/test/hotspot/jtreg/TEST.groups
@@ -394,20 +394,18 @@
  -runtime/cds/appcds/jigsaw/classpathtests/EmptyClassInBootClassPath.java \
  -runtime/cds/appcds/jigsaw/JigsawOptionsCombo.java \
  -runtime/cds/appcds/jigsaw/modulepath/MainModuleOnly.java
 
 tier1_serviceability = \
-  serviceability/dcmd/compiler \
+  serviceability/ \
   -serviceability/dcmd/compiler/CompilerQueueTest.java \
-  serviceability/jvmti/RedefineClasses \
   -serviceability/jvmti/RedefineClasses/RedefineLeak.java \
   -serviceability/jvmti/RedefineClasses/RedefinePreviousVersions.java \
   -serviceability/jvmti/RedefineClasses/RedefineRunningMethods.java \
   -serviceability/jvmti/RedefineClasses/RedefineRunningMethodsWithBacktrace.java \
   -serviceability/jvmti/RedefineClasses/TestRedefineObject.java \
-  serviceability/logging \
-  serviceability/sa \
+  -serviceability/sa/ClhsdbScanOops.java \
   -serviceability/sa/ClhsdbScanOops.java \
   -serviceability/sa/TestJmapCore.java \
   -serviceability/sa/TestJmapCoreMetaspace.java
 
 tier1 = \
diff a/test/hotspot/jtreg/runtime/CompressedOops/CompressedClassPointers.java b/test/hotspot/jtreg/runtime/CompressedOops/CompressedClassPointers.java
--- a/test/hotspot/jtreg/runtime/CompressedOops/CompressedClassPointers.java
+++ b/test/hotspot/jtreg/runtime/CompressedOops/CompressedClassPointers.java
@@ -23,11 +23,11 @@
 
 /*
  * @test
  * @bug 8024927
  * @summary Testing address of compressed class pointer space as best as possible.
- * @requires vm.bits == 64 & vm.opt.final.UseCompressedOops == true & os.family != "windows"
+ * @requires vm.bits == 64 & os.family != "windows" & !vm.graal.enabled
  * @library /test/lib
  * @modules java.base/jdk.internal.misc
  *          java.management
  * @run driver CompressedClassPointers
  */
@@ -139,10 +139,127 @@
           output.shouldContain("Unable to use shared archive");
           output.shouldHaveExitValue(1);
         }
     }
 
+    public static void smallHeapTestNoCoop() throws Exception {
+        ProcessBuilder pb = ProcessTools.createJavaProcessBuilder(
+            "-XX:-UseCompressedOops",
+            "-XX:+UseCompressedClassPointers",
+            "-XX:+UnlockDiagnosticVMOptions",
+            "-XX:SharedBaseAddress=8g",
+            "-Xmx128m",
+            "-Xlog:gc+metaspace=trace",
+            "-Xshare:off",
+            "-Xlog:cds=trace",
+            "-XX:+VerifyBeforeGC", "-version");
+        OutputAnalyzer output = new OutputAnalyzer(pb.start());
+        output.shouldContain("Narrow klass base: 0x0000000000000000");
+        output.shouldHaveExitValue(0);
+    }
+
+    public static void smallHeapTestWith1GNoCoop() throws Exception {
+        ProcessBuilder pb = ProcessTools.createJavaProcessBuilder(
+            "-XX:-UseCompressedOops",
+            "-XX:+UseCompressedClassPointers",
+            "-XX:+UnlockDiagnosticVMOptions",
+            "-XX:CompressedClassSpaceSize=1g",
+            "-Xmx128m",
+            "-Xlog:gc+metaspace=trace",
+            "-Xshare:off",
+            "-Xlog:cds=trace",
+            "-XX:+VerifyBeforeGC", "-version");
+        OutputAnalyzer output = new OutputAnalyzer(pb.start());
+        output.shouldContain("Narrow klass base: 0x0000000000000000");
+        output.shouldContain("Narrow klass shift: 0");
+        output.shouldHaveExitValue(0);
+    }
+
+    public static void largeHeapTestNoCoop() throws Exception {
+        ProcessBuilder pb = ProcessTools.createJavaProcessBuilder(
+            "-XX:-UseCompressedOops",
+            "-XX:+UseCompressedClassPointers",
+            "-XX:+UnlockDiagnosticVMOptions",
+            "-XX:+UnlockExperimentalVMOptions",
+            "-Xmx30g",
+            "-XX:-UseAOT", // AOT explicitly set klass shift to 3.
+            "-Xlog:gc+metaspace=trace",
+            "-Xshare:off",
+            "-Xlog:cds=trace",
+            "-XX:+VerifyBeforeGC", "-version");
+        OutputAnalyzer output = new OutputAnalyzer(pb.start());
+        output.shouldContain("Narrow klass base: 0x0000000000000000");
+        output.shouldContain("Narrow klass shift: 0");
+        output.shouldHaveExitValue(0);
+    }
+
+    public static void largePagesTestNoCoop() throws Exception {
+        ProcessBuilder pb = ProcessTools.createJavaProcessBuilder(
+            "-XX:-UseCompressedOops",
+            "-XX:+UseCompressedClassPointers",
+            "-XX:+UnlockDiagnosticVMOptions",
+            "-Xmx128m",
+            "-XX:+UseLargePages",
+            "-Xlog:gc+metaspace=trace",
+            "-XX:+VerifyBeforeGC", "-version");
+        OutputAnalyzer output = new OutputAnalyzer(pb.start());
+        output.shouldContain("Narrow klass base:");
+        output.shouldHaveExitValue(0);
+    }
+
+    public static void heapBaseMinAddressTestNoCoop() throws Exception {
+        ProcessBuilder pb = ProcessTools.createJavaProcessBuilder(
+            "-XX:-UseCompressedOops",
+            "-XX:+UseCompressedClassPointers",
+            "-XX:HeapBaseMinAddress=1m",
+            "-Xlog:gc+heap+coops=debug",
+            "-version");
+        OutputAnalyzer output = new OutputAnalyzer(pb.start());
+        output.shouldContain("HeapBaseMinAddress must be at least");
+        output.shouldHaveExitValue(0);
+    }
+
+    public static void sharingTestNoCoop() throws Exception {
+        // Test small heaps
+        ProcessBuilder pb = ProcessTools.createJavaProcessBuilder(
+            "-XX:-UseCompressedOops",
+            "-XX:+UseCompressedClassPointers",
+            "-XX:+UnlockDiagnosticVMOptions",
+            "-XX:SharedArchiveFile=./CompressedClassPointers.jsa",
+            "-Xmx128m",
+            "-XX:SharedBaseAddress=8g",
+            "-XX:+PrintCompressedOopsMode",
+            "-XX:+VerifyBeforeGC",
+            "-Xshare:dump", "-Xlog:cds");
+        OutputAnalyzer output = new OutputAnalyzer(pb.start());
+        if (output.firstMatch("Shared spaces are not supported in this VM") != null) {
+            return;
+        }
+        try {
+          output.shouldContain("Loading classes to share");
+          output.shouldHaveExitValue(0);
+
+          pb = ProcessTools.createJavaProcessBuilder(
+            "-XX:-UseCompressedOops",
+            "-XX:+UseCompressedClassPointers",
+            "-XX:+UnlockDiagnosticVMOptions",
+            "-XX:SharedArchiveFile=./CompressedClassPointers.jsa",
+            "-Xmx128m",
+            "-XX:SharedBaseAddress=8g",
+            "-XX:+PrintCompressedOopsMode",
+            "-Xshare:on",
+            "-version");
+          output = new OutputAnalyzer(pb.start());
+          output.shouldContain("sharing");
+          output.shouldHaveExitValue(0);
+
+        } catch (RuntimeException e) {
+          output.shouldContain("Unable to use shared archive");
+          output.shouldHaveExitValue(1);
+        }
+    }
+
     public static void main(String[] args) throws Exception {
         if (Platform.isSolaris()) {
              String name = System.getProperty("os.version");
              if (name.equals("5.10")) {
                 throw new SkippedException("Solaris 10 can't mmap compressed oops space without a base");
@@ -152,7 +269,24 @@
         smallHeapTestWith1G();
         largeHeapTest();
         largePagesTest();
         heapBaseMinAddressTest();
         sharingTest();
+
+        boolean ccpRequiresCoop = Platform.isAArch64() || Platform.isSparc();
+
+        if (!ccpRequiresCoop && !Platform.isOSX()) {
+            // Testing compressed class pointers without compressed oops.
+            // This is only possible if the platform supports it. Notably,
+            // on macOS, when compressed oops is disabled and the heap is
+            // given an arbitrary address, that address occasionally collides
+            // with where we would ideally have placed the compressed class
+            // space. Therefore, macOS is omitted for now.
+            smallHeapTestNoCoop();
+            smallHeapTestWith1GNoCoop();
+            largeHeapTestNoCoop();
+            largePagesTestNoCoop();
+            heapBaseMinAddressTestNoCoop();
+            sharingTestNoCoop();
+        }
     }
 }
