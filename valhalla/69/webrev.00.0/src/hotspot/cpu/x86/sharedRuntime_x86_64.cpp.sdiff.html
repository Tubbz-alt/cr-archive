<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/cpu/x86/sharedRuntime_x86_64.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="sharedRuntime_x86_32.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="stubGenerator_x86_64.cpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/cpu/x86/sharedRuntime_x86_64.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
1140   // we try and find the callee by normal means a safepoint
1141   // is possible. So we stash the desired callee in the thread
1142   // and the vm will find there should this case occur.
1143 
1144   __ movptr(Address(r15_thread, JavaThread::callee_target_offset()), rbx);
1145 
1146   // put Method* where a c2i would expect should we end up there
1147   // only needed because of c2 resolve stubs return Method* as a result in
1148   // rax
1149   __ mov(rax, rbx);
1150   __ jmp(r11);
1151 }
1152 
1153 static void gen_inline_cache_check(MacroAssembler *masm, Label&amp; skip_fixup) {
1154   Label ok;
1155 
1156   Register holder = rax;
1157   Register receiver = j_rarg0;
1158   Register temp = rbx;
1159 
<span class="line-modified">1160   __ load_klass(temp, receiver);</span>
1161   __ cmpptr(temp, Address(holder, CompiledICHolder::holder_klass_offset()));
1162   __ movptr(rbx, Address(holder, CompiledICHolder::holder_metadata_offset()));
1163   __ jcc(Assembler::equal, ok);
1164   __ jump(RuntimeAddress(SharedRuntime::get_ic_miss_stub()));
1165 
1166   __ bind(ok);
1167   // Method might have been compiled since the call site was patched to
1168   // interpreted if that is the case treat it as a miss so we can get
1169   // the call site corrected.
1170   __ cmpptr(Address(rbx, in_bytes(Method::code_offset())), (int32_t)NULL_WORD);
1171   __ jcc(Assembler::equal, skip_fixup);
1172   __ jump(RuntimeAddress(SharedRuntime::get_ic_miss_stub()));
1173 }
1174 
1175 // ---------------------------------------------------------------
1176 AdapterHandlerEntry* SharedRuntime::generate_i2c2i_adapters(MacroAssembler *masm,
1177                                                             int comp_args_on_stack,
1178                                                             const GrowableArray&lt;SigEntry&gt;* sig,
1179                                                             const VMRegPair* regs,
1180                                                             const GrowableArray&lt;SigEntry&gt;* sig_cc,
</pre>
<hr />
<pre>
2387   // stack properly aligned.
2388   stack_slots = align_up(stack_slots, StackAlignmentInSlots);
2389 
2390   int stack_size = stack_slots * VMRegImpl::stack_slot_size;
2391 
2392   // First thing make an ic check to see if we should even be here
2393 
2394   // We are free to use all registers as temps without saving them and
2395   // restoring them except rbp. rbp is the only callee save register
2396   // as far as the interpreter and the compiler(s) are concerned.
2397 
2398 
2399   const Register ic_reg = rax;
2400   const Register receiver = j_rarg0;
2401 
2402   Label hit;
2403   Label exception_pending;
2404 
2405   assert_different_registers(ic_reg, receiver, rscratch1);
2406   __ verify_oop(receiver);
<span class="line-modified">2407   __ load_klass(rscratch1, receiver);</span>
2408   __ cmpq(ic_reg, rscratch1);
2409   __ jcc(Assembler::equal, hit);
2410 
2411   __ jump(RuntimeAddress(SharedRuntime::get_ic_miss_stub()));
2412 
2413   // Verified entry point must be aligned
2414   __ align(8);
2415 
2416   __ bind(hit);
2417 
2418   int vep_offset = ((intptr_t)__ pc()) - start;
2419 
2420   if (VM_Version::supports_fast_class_init_checks() &amp;&amp; method-&gt;needs_clinit_barrier()) {
2421     Label L_skip_barrier;
2422     Register klass = r10;
2423     __ mov_metadata(klass, method-&gt;method_holder()); // InstanceKlass*
2424     __ clinit_barrier(klass, r15_thread, &amp;L_skip_barrier /*L_fast_path*/);
2425 
2426     __ jump(RuntimeAddress(SharedRuntime::get_handle_wrong_method_stub())); // slow path
2427 
</pre>
<hr />
<pre>
2732   Label lock_done;
2733 
2734   if (method-&gt;is_synchronized()) {
2735     assert(!is_critical_native, &quot;unhandled&quot;);
2736 
2737 
2738     const int mark_word_offset = BasicLock::displaced_header_offset_in_bytes();
2739 
2740     // Get the handle (the 2nd argument)
2741     __ mov(oop_handle_reg, c_rarg1);
2742 
2743     // Get address of the box
2744 
2745     __ lea(lock_reg, Address(rsp, lock_slot_offset * VMRegImpl::stack_slot_size));
2746 
2747     // Load the oop from the handle
2748     __ movptr(obj_reg, Address(oop_handle_reg, 0));
2749 
2750     __ resolve(IS_NOT_NULL, obj_reg);
2751     if (UseBiasedLocking) {
<span class="line-modified">2752       __ biased_locking_enter(lock_reg, obj_reg, swap_reg, rscratch1, false, lock_done, &amp;slow_path_lock);</span>
2753     }
2754 
2755     // Load immediate 1 into swap_reg %rax
2756     __ movl(swap_reg, 1);
2757 
2758     // Load (object-&gt;mark() | 1) into swap_reg %rax
2759     __ orptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));
2760     if (EnableValhalla &amp;&amp; !UseBiasedLocking) {
2761       // For slow path is_always_locked, using biased, which is never natural for !UseBiasLocking
2762       __ andptr(swap_reg, ~((int) markWord::biased_lock_bit_in_place));
2763     }
2764 
2765     // Save (object-&gt;mark() | 1) into BasicLock&#39;s displaced header
2766     __ movptr(Address(lock_reg, mark_word_offset), swap_reg);
2767 
2768     // src -&gt; dest iff dest == rax else rax &lt;- dest
2769     __ lock();
2770     __ cmpxchgptr(lock_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));
2771     __ jcc(Assembler::equal, lock_done);
2772 
</pre>
<hr />
<pre>
4406     VMReg r_2 = pair.second();
4407     Address from(rax, off);
4408     if (bt == T_FLOAT) {
4409       __ movflt(r_1-&gt;as_XMMRegister(), from);
4410     } else if (bt == T_DOUBLE) {
4411       __ movdbl(r_1-&gt;as_XMMRegister(), from);
4412     } else if (bt == T_OBJECT || bt == T_ARRAY) {
4413       assert_different_registers(rax, r_1-&gt;as_Register());
4414       __ load_heap_oop(r_1-&gt;as_Register(), from);
4415     } else {
4416       assert(is_java_primitive(bt), &quot;unexpected basic type&quot;);
4417       assert_different_registers(rax, r_1-&gt;as_Register());
4418       size_t size_in_bytes = type2aelembytes(bt);
4419       __ load_sized_value(r_1-&gt;as_Register(), from, size_in_bytes, bt != T_CHAR &amp;&amp; bt != T_BOOLEAN);
4420     }
4421     j++;
4422   }
4423   assert(j == regs-&gt;length(), &quot;missed a field?&quot;);
4424 
4425   if (StressInlineTypeReturnedAsFields) {
<span class="line-modified">4426     __ load_klass(rax, rax);</span>
4427     __ orptr(rax, 1);
4428   }
4429 
4430   __ ret(0);
4431 
4432   __ flush();
4433 
4434   return BufferedValueTypeBlob::create(&amp;buffer, pack_fields_off, pack_fields_jobject_off, unpack_fields_off);
4435 }
</pre>
</td>
<td>
<hr />
<pre>
1140   // we try and find the callee by normal means a safepoint
1141   // is possible. So we stash the desired callee in the thread
1142   // and the vm will find there should this case occur.
1143 
1144   __ movptr(Address(r15_thread, JavaThread::callee_target_offset()), rbx);
1145 
1146   // put Method* where a c2i would expect should we end up there
1147   // only needed because of c2 resolve stubs return Method* as a result in
1148   // rax
1149   __ mov(rax, rbx);
1150   __ jmp(r11);
1151 }
1152 
1153 static void gen_inline_cache_check(MacroAssembler *masm, Label&amp; skip_fixup) {
1154   Label ok;
1155 
1156   Register holder = rax;
1157   Register receiver = j_rarg0;
1158   Register temp = rbx;
1159 
<span class="line-modified">1160   __ load_klass(temp, receiver, rscratch1);</span>
1161   __ cmpptr(temp, Address(holder, CompiledICHolder::holder_klass_offset()));
1162   __ movptr(rbx, Address(holder, CompiledICHolder::holder_metadata_offset()));
1163   __ jcc(Assembler::equal, ok);
1164   __ jump(RuntimeAddress(SharedRuntime::get_ic_miss_stub()));
1165 
1166   __ bind(ok);
1167   // Method might have been compiled since the call site was patched to
1168   // interpreted if that is the case treat it as a miss so we can get
1169   // the call site corrected.
1170   __ cmpptr(Address(rbx, in_bytes(Method::code_offset())), (int32_t)NULL_WORD);
1171   __ jcc(Assembler::equal, skip_fixup);
1172   __ jump(RuntimeAddress(SharedRuntime::get_ic_miss_stub()));
1173 }
1174 
1175 // ---------------------------------------------------------------
1176 AdapterHandlerEntry* SharedRuntime::generate_i2c2i_adapters(MacroAssembler *masm,
1177                                                             int comp_args_on_stack,
1178                                                             const GrowableArray&lt;SigEntry&gt;* sig,
1179                                                             const VMRegPair* regs,
1180                                                             const GrowableArray&lt;SigEntry&gt;* sig_cc,
</pre>
<hr />
<pre>
2387   // stack properly aligned.
2388   stack_slots = align_up(stack_slots, StackAlignmentInSlots);
2389 
2390   int stack_size = stack_slots * VMRegImpl::stack_slot_size;
2391 
2392   // First thing make an ic check to see if we should even be here
2393 
2394   // We are free to use all registers as temps without saving them and
2395   // restoring them except rbp. rbp is the only callee save register
2396   // as far as the interpreter and the compiler(s) are concerned.
2397 
2398 
2399   const Register ic_reg = rax;
2400   const Register receiver = j_rarg0;
2401 
2402   Label hit;
2403   Label exception_pending;
2404 
2405   assert_different_registers(ic_reg, receiver, rscratch1);
2406   __ verify_oop(receiver);
<span class="line-modified">2407   __ load_klass(rscratch1, receiver, rscratch2);</span>
2408   __ cmpq(ic_reg, rscratch1);
2409   __ jcc(Assembler::equal, hit);
2410 
2411   __ jump(RuntimeAddress(SharedRuntime::get_ic_miss_stub()));
2412 
2413   // Verified entry point must be aligned
2414   __ align(8);
2415 
2416   __ bind(hit);
2417 
2418   int vep_offset = ((intptr_t)__ pc()) - start;
2419 
2420   if (VM_Version::supports_fast_class_init_checks() &amp;&amp; method-&gt;needs_clinit_barrier()) {
2421     Label L_skip_barrier;
2422     Register klass = r10;
2423     __ mov_metadata(klass, method-&gt;method_holder()); // InstanceKlass*
2424     __ clinit_barrier(klass, r15_thread, &amp;L_skip_barrier /*L_fast_path*/);
2425 
2426     __ jump(RuntimeAddress(SharedRuntime::get_handle_wrong_method_stub())); // slow path
2427 
</pre>
<hr />
<pre>
2732   Label lock_done;
2733 
2734   if (method-&gt;is_synchronized()) {
2735     assert(!is_critical_native, &quot;unhandled&quot;);
2736 
2737 
2738     const int mark_word_offset = BasicLock::displaced_header_offset_in_bytes();
2739 
2740     // Get the handle (the 2nd argument)
2741     __ mov(oop_handle_reg, c_rarg1);
2742 
2743     // Get address of the box
2744 
2745     __ lea(lock_reg, Address(rsp, lock_slot_offset * VMRegImpl::stack_slot_size));
2746 
2747     // Load the oop from the handle
2748     __ movptr(obj_reg, Address(oop_handle_reg, 0));
2749 
2750     __ resolve(IS_NOT_NULL, obj_reg);
2751     if (UseBiasedLocking) {
<span class="line-modified">2752       __ biased_locking_enter(lock_reg, obj_reg, swap_reg, rscratch1, rscratch2, false, lock_done, &amp;slow_path_lock);</span>
2753     }
2754 
2755     // Load immediate 1 into swap_reg %rax
2756     __ movl(swap_reg, 1);
2757 
2758     // Load (object-&gt;mark() | 1) into swap_reg %rax
2759     __ orptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));
2760     if (EnableValhalla &amp;&amp; !UseBiasedLocking) {
2761       // For slow path is_always_locked, using biased, which is never natural for !UseBiasLocking
2762       __ andptr(swap_reg, ~((int) markWord::biased_lock_bit_in_place));
2763     }
2764 
2765     // Save (object-&gt;mark() | 1) into BasicLock&#39;s displaced header
2766     __ movptr(Address(lock_reg, mark_word_offset), swap_reg);
2767 
2768     // src -&gt; dest iff dest == rax else rax &lt;- dest
2769     __ lock();
2770     __ cmpxchgptr(lock_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));
2771     __ jcc(Assembler::equal, lock_done);
2772 
</pre>
<hr />
<pre>
4406     VMReg r_2 = pair.second();
4407     Address from(rax, off);
4408     if (bt == T_FLOAT) {
4409       __ movflt(r_1-&gt;as_XMMRegister(), from);
4410     } else if (bt == T_DOUBLE) {
4411       __ movdbl(r_1-&gt;as_XMMRegister(), from);
4412     } else if (bt == T_OBJECT || bt == T_ARRAY) {
4413       assert_different_registers(rax, r_1-&gt;as_Register());
4414       __ load_heap_oop(r_1-&gt;as_Register(), from);
4415     } else {
4416       assert(is_java_primitive(bt), &quot;unexpected basic type&quot;);
4417       assert_different_registers(rax, r_1-&gt;as_Register());
4418       size_t size_in_bytes = type2aelembytes(bt);
4419       __ load_sized_value(r_1-&gt;as_Register(), from, size_in_bytes, bt != T_CHAR &amp;&amp; bt != T_BOOLEAN);
4420     }
4421     j++;
4422   }
4423   assert(j == regs-&gt;length(), &quot;missed a field?&quot;);
4424 
4425   if (StressInlineTypeReturnedAsFields) {
<span class="line-modified">4426     __ load_klass(rax, rax, rscratch1);</span>
4427     __ orptr(rax, 1);
4428   }
4429 
4430   __ ret(0);
4431 
4432   __ flush();
4433 
4434   return BufferedValueTypeBlob::create(&amp;buffer, pack_fields_off, pack_fields_jobject_off, unpack_fields_off);
4435 }
</pre>
</td>
</tr>
</table>
<center><a href="sharedRuntime_x86_32.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="stubGenerator_x86_64.cpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>