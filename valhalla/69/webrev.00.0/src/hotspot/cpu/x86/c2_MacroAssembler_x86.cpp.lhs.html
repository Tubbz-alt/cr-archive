<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Frames src/hotspot/cpu/x86/c2_MacroAssembler_x86.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
    <script type="text/javascript" src="../../../../navigation.js"></script>
  </head>
<body onkeypress="keypress(event);">
<a name="0"></a>
<hr />
<pre>   1 /*
   2  * Copyright (c) 2020, Oracle and/or its affiliates. All rights reserved.
   3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   4  *
   5  * This code is free software; you can redistribute it and/or modify it
   6  * under the terms of the GNU General Public License version 2 only, as
   7  * published by the Free Software Foundation.
   8  *
   9  * This code is distributed in the hope that it will be useful, but WITHOUT
  10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #include &quot;precompiled.hpp&quot;
  26 #include &quot;asm/assembler.hpp&quot;
  27 #include &quot;asm/assembler.inline.hpp&quot;
  28 #include &quot;oops/methodData.hpp&quot;
  29 #include &quot;opto/c2_MacroAssembler.hpp&quot;
  30 #include &quot;opto/intrinsicnode.hpp&quot;
  31 #include &quot;opto/opcodes.hpp&quot;
  32 #include &quot;runtime/biasedLocking.hpp&quot;
  33 #include &quot;runtime/objectMonitor.hpp&quot;
  34 #include &quot;runtime/stubRoutines.hpp&quot;
  35 
  36 void C2_MacroAssembler::setvectmask(Register dst, Register src) {
  37   guarantee(PostLoopMultiversioning, &quot;must be&quot;);
  38   Assembler::movl(dst, 1);
  39   Assembler::shlxl(dst, dst, src);
  40   Assembler::decl(dst);
  41   Assembler::kmovdl(k1, dst);
  42   Assembler::movl(dst, src);
  43 }
  44 
  45 void C2_MacroAssembler::restorevectmask() {
  46   guarantee(PostLoopMultiversioning, &quot;must be&quot;);
  47   Assembler::knotwl(k1, k0);
  48 }
  49 
  50 #if INCLUDE_RTM_OPT
  51 
  52 // Update rtm_counters based on abort status
  53 // input: abort_status
  54 //        rtm_counters (RTMLockingCounters*)
  55 // flags are killed
  56 void C2_MacroAssembler::rtm_counters_update(Register abort_status, Register rtm_counters) {
  57 
  58   atomic_incptr(Address(rtm_counters, RTMLockingCounters::abort_count_offset()));
  59   if (PrintPreciseRTMLockingStatistics) {
  60     for (int i = 0; i &lt; RTMLockingCounters::ABORT_STATUS_LIMIT; i++) {
  61       Label check_abort;
  62       testl(abort_status, (1&lt;&lt;i));
  63       jccb(Assembler::equal, check_abort);
  64       atomic_incptr(Address(rtm_counters, RTMLockingCounters::abortX_count_offset() + (i * sizeof(uintx))));
  65       bind(check_abort);
  66     }
  67   }
  68 }
  69 
  70 // Branch if (random &amp; (count-1) != 0), count is 2^n
  71 // tmp, scr and flags are killed
  72 void C2_MacroAssembler::branch_on_random_using_rdtsc(Register tmp, Register scr, int count, Label&amp; brLabel) {
  73   assert(tmp == rax, &quot;&quot;);
  74   assert(scr == rdx, &quot;&quot;);
  75   rdtsc(); // modifies EDX:EAX
  76   andptr(tmp, count-1);
  77   jccb(Assembler::notZero, brLabel);
  78 }
  79 
  80 // Perform abort ratio calculation, set no_rtm bit if high ratio
  81 // input:  rtm_counters_Reg (RTMLockingCounters* address)
  82 // tmpReg, rtm_counters_Reg and flags are killed
  83 void C2_MacroAssembler::rtm_abort_ratio_calculation(Register tmpReg,
  84                                                     Register rtm_counters_Reg,
  85                                                     RTMLockingCounters* rtm_counters,
  86                                                     Metadata* method_data) {
  87   Label L_done, L_check_always_rtm1, L_check_always_rtm2;
  88 
  89   if (RTMLockingCalculationDelay &gt; 0) {
  90     // Delay calculation
  91     movptr(tmpReg, ExternalAddress((address) RTMLockingCounters::rtm_calculation_flag_addr()), tmpReg);
  92     testptr(tmpReg, tmpReg);
  93     jccb(Assembler::equal, L_done);
  94   }
  95   // Abort ratio calculation only if abort_count &gt; RTMAbortThreshold
  96   //   Aborted transactions = abort_count * 100
  97   //   All transactions = total_count *  RTMTotalCountIncrRate
  98   //   Set no_rtm bit if (Aborted transactions &gt;= All transactions * RTMAbortRatio)
  99 
 100   movptr(tmpReg, Address(rtm_counters_Reg, RTMLockingCounters::abort_count_offset()));
 101   cmpptr(tmpReg, RTMAbortThreshold);
 102   jccb(Assembler::below, L_check_always_rtm2);
 103   imulptr(tmpReg, tmpReg, 100);
 104 
 105   Register scrReg = rtm_counters_Reg;
 106   movptr(scrReg, Address(rtm_counters_Reg, RTMLockingCounters::total_count_offset()));
 107   imulptr(scrReg, scrReg, RTMTotalCountIncrRate);
 108   imulptr(scrReg, scrReg, RTMAbortRatio);
 109   cmpptr(tmpReg, scrReg);
 110   jccb(Assembler::below, L_check_always_rtm1);
 111   if (method_data != NULL) {
 112     // set rtm_state to &quot;no rtm&quot; in MDO
 113     mov_metadata(tmpReg, method_data);
 114     lock();
 115     orl(Address(tmpReg, MethodData::rtm_state_offset_in_bytes()), NoRTM);
 116   }
 117   jmpb(L_done);
 118   bind(L_check_always_rtm1);
 119   // Reload RTMLockingCounters* address
 120   lea(rtm_counters_Reg, ExternalAddress((address)rtm_counters));
 121   bind(L_check_always_rtm2);
 122   movptr(tmpReg, Address(rtm_counters_Reg, RTMLockingCounters::total_count_offset()));
 123   cmpptr(tmpReg, RTMLockingThreshold / RTMTotalCountIncrRate);
 124   jccb(Assembler::below, L_done);
 125   if (method_data != NULL) {
 126     // set rtm_state to &quot;always rtm&quot; in MDO
 127     mov_metadata(tmpReg, method_data);
 128     lock();
 129     orl(Address(tmpReg, MethodData::rtm_state_offset_in_bytes()), UseRTM);
 130   }
 131   bind(L_done);
 132 }
 133 
 134 // Update counters and perform abort ratio calculation
 135 // input:  abort_status_Reg
 136 // rtm_counters_Reg, flags are killed
 137 void C2_MacroAssembler::rtm_profiling(Register abort_status_Reg,
 138                                       Register rtm_counters_Reg,
 139                                       RTMLockingCounters* rtm_counters,
 140                                       Metadata* method_data,
 141                                       bool profile_rtm) {
 142 
 143   assert(rtm_counters != NULL, &quot;should not be NULL when profiling RTM&quot;);
 144   // update rtm counters based on rax value at abort
 145   // reads abort_status_Reg, updates flags
 146   lea(rtm_counters_Reg, ExternalAddress((address)rtm_counters));
 147   rtm_counters_update(abort_status_Reg, rtm_counters_Reg);
 148   if (profile_rtm) {
 149     // Save abort status because abort_status_Reg is used by following code.
 150     if (RTMRetryCount &gt; 0) {
 151       push(abort_status_Reg);
 152     }
 153     assert(rtm_counters != NULL, &quot;should not be NULL when profiling RTM&quot;);
 154     rtm_abort_ratio_calculation(abort_status_Reg, rtm_counters_Reg, rtm_counters, method_data);
 155     // restore abort status
 156     if (RTMRetryCount &gt; 0) {
 157       pop(abort_status_Reg);
 158     }
 159   }
 160 }
 161 
 162 // Retry on abort if abort&#39;s status is 0x6: can retry (0x2) | memory conflict (0x4)
 163 // inputs: retry_count_Reg
 164 //       : abort_status_Reg
 165 // output: retry_count_Reg decremented by 1
 166 // flags are killed
 167 void C2_MacroAssembler::rtm_retry_lock_on_abort(Register retry_count_Reg, Register abort_status_Reg, Label&amp; retryLabel) {
 168   Label doneRetry;
 169   assert(abort_status_Reg == rax, &quot;&quot;);
 170   // The abort reason bits are in eax (see all states in rtmLocking.hpp)
 171   // 0x6 = conflict on which we can retry (0x2) | memory conflict (0x4)
 172   // if reason is in 0x6 and retry count != 0 then retry
 173   andptr(abort_status_Reg, 0x6);
 174   jccb(Assembler::zero, doneRetry);
 175   testl(retry_count_Reg, retry_count_Reg);
 176   jccb(Assembler::zero, doneRetry);
 177   pause();
 178   decrementl(retry_count_Reg);
 179   jmp(retryLabel);
 180   bind(doneRetry);
 181 }
 182 
 183 // Spin and retry if lock is busy,
 184 // inputs: box_Reg (monitor address)
 185 //       : retry_count_Reg
 186 // output: retry_count_Reg decremented by 1
 187 //       : clear z flag if retry count exceeded
 188 // tmp_Reg, scr_Reg, flags are killed
 189 void C2_MacroAssembler::rtm_retry_lock_on_busy(Register retry_count_Reg, Register box_Reg,
 190                                                Register tmp_Reg, Register scr_Reg, Label&amp; retryLabel) {
 191   Label SpinLoop, SpinExit, doneRetry;
 192   int owner_offset = OM_OFFSET_NO_MONITOR_VALUE_TAG(owner);
 193 
 194   testl(retry_count_Reg, retry_count_Reg);
 195   jccb(Assembler::zero, doneRetry);
 196   decrementl(retry_count_Reg);
 197   movptr(scr_Reg, RTMSpinLoopCount);
 198 
 199   bind(SpinLoop);
 200   pause();
 201   decrementl(scr_Reg);
 202   jccb(Assembler::lessEqual, SpinExit);
 203   movptr(tmp_Reg, Address(box_Reg, owner_offset));
 204   testptr(tmp_Reg, tmp_Reg);
 205   jccb(Assembler::notZero, SpinLoop);
 206 
 207   bind(SpinExit);
 208   jmp(retryLabel);
 209   bind(doneRetry);
 210   incrementl(retry_count_Reg); // clear z flag
 211 }
 212 
 213 // Use RTM for normal stack locks
 214 // Input: objReg (object to lock)
 215 void C2_MacroAssembler::rtm_stack_locking(Register objReg, Register tmpReg, Register scrReg,
 216                                          Register retry_on_abort_count_Reg,
 217                                          RTMLockingCounters* stack_rtm_counters,
 218                                          Metadata* method_data, bool profile_rtm,
 219                                          Label&amp; DONE_LABEL, Label&amp; IsInflated) {
 220   assert(UseRTMForStackLocks, &quot;why call this otherwise?&quot;);
 221   assert(!UseBiasedLocking, &quot;Biased locking is not supported with RTM locking&quot;);
 222   assert(tmpReg == rax, &quot;&quot;);
 223   assert(scrReg == rdx, &quot;&quot;);
 224   Label L_rtm_retry, L_decrement_retry, L_on_abort;
 225 
 226   if (RTMRetryCount &gt; 0) {
 227     movl(retry_on_abort_count_Reg, RTMRetryCount); // Retry on abort
 228     bind(L_rtm_retry);
 229   }
 230   movptr(tmpReg, Address(objReg, oopDesc::mark_offset_in_bytes()));
 231   testptr(tmpReg, markWord::monitor_value);  // inflated vs stack-locked|neutral|biased
 232   jcc(Assembler::notZero, IsInflated);
 233 
 234   if (PrintPreciseRTMLockingStatistics || profile_rtm) {
 235     Label L_noincrement;
 236     if (RTMTotalCountIncrRate &gt; 1) {
 237       // tmpReg, scrReg and flags are killed
 238       branch_on_random_using_rdtsc(tmpReg, scrReg, RTMTotalCountIncrRate, L_noincrement);
 239     }
 240     assert(stack_rtm_counters != NULL, &quot;should not be NULL when profiling RTM&quot;);
 241     atomic_incptr(ExternalAddress((address)stack_rtm_counters-&gt;total_count_addr()), scrReg);
 242     bind(L_noincrement);
 243   }
 244   xbegin(L_on_abort);
 245   movptr(tmpReg, Address(objReg, oopDesc::mark_offset_in_bytes()));       // fetch markword
 246   andptr(tmpReg, markWord::biased_lock_mask_in_place); // look at 3 lock bits
 247   cmpptr(tmpReg, markWord::unlocked_value);            // bits = 001 unlocked
 248   jcc(Assembler::equal, DONE_LABEL);        // all done if unlocked
 249 
 250   Register abort_status_Reg = tmpReg; // status of abort is stored in RAX
 251   if (UseRTMXendForLockBusy) {
 252     xend();
 253     movptr(abort_status_Reg, 0x2);   // Set the abort status to 2 (so we can retry)
 254     jmp(L_decrement_retry);
 255   }
 256   else {
 257     xabort(0);
 258   }
 259   bind(L_on_abort);
 260   if (PrintPreciseRTMLockingStatistics || profile_rtm) {
 261     rtm_profiling(abort_status_Reg, scrReg, stack_rtm_counters, method_data, profile_rtm);
 262   }
 263   bind(L_decrement_retry);
 264   if (RTMRetryCount &gt; 0) {
 265     // retry on lock abort if abort status is &#39;can retry&#39; (0x2) or &#39;memory conflict&#39; (0x4)
 266     rtm_retry_lock_on_abort(retry_on_abort_count_Reg, abort_status_Reg, L_rtm_retry);
 267   }
 268 }
 269 
 270 // Use RTM for inflating locks
 271 // inputs: objReg (object to lock)
 272 //         boxReg (on-stack box address (displaced header location) - KILLED)
 273 //         tmpReg (ObjectMonitor address + markWord::monitor_value)
 274 void C2_MacroAssembler::rtm_inflated_locking(Register objReg, Register boxReg, Register tmpReg,
 275                                             Register scrReg, Register retry_on_busy_count_Reg,
 276                                             Register retry_on_abort_count_Reg,
 277                                             RTMLockingCounters* rtm_counters,
 278                                             Metadata* method_data, bool profile_rtm,
 279                                             Label&amp; DONE_LABEL) {
 280   assert(UseRTMLocking, &quot;why call this otherwise?&quot;);
 281   assert(tmpReg == rax, &quot;&quot;);
 282   assert(scrReg == rdx, &quot;&quot;);
 283   Label L_rtm_retry, L_decrement_retry, L_on_abort;
 284   int owner_offset = OM_OFFSET_NO_MONITOR_VALUE_TAG(owner);
 285 
 286   // Without cast to int32_t this style of movptr will destroy r10 which is typically obj.
 287   movptr(Address(boxReg, 0), (int32_t)intptr_t(markWord::unused_mark().value()));
 288   movptr(boxReg, tmpReg); // Save ObjectMonitor address
 289 
 290   if (RTMRetryCount &gt; 0) {
 291     movl(retry_on_busy_count_Reg, RTMRetryCount);  // Retry on lock busy
 292     movl(retry_on_abort_count_Reg, RTMRetryCount); // Retry on abort
 293     bind(L_rtm_retry);
 294   }
 295   if (PrintPreciseRTMLockingStatistics || profile_rtm) {
 296     Label L_noincrement;
 297     if (RTMTotalCountIncrRate &gt; 1) {
 298       // tmpReg, scrReg and flags are killed
 299       branch_on_random_using_rdtsc(tmpReg, scrReg, RTMTotalCountIncrRate, L_noincrement);
 300     }
 301     assert(rtm_counters != NULL, &quot;should not be NULL when profiling RTM&quot;);
 302     atomic_incptr(ExternalAddress((address)rtm_counters-&gt;total_count_addr()), scrReg);
 303     bind(L_noincrement);
 304   }
 305   xbegin(L_on_abort);
 306   movptr(tmpReg, Address(objReg, oopDesc::mark_offset_in_bytes()));
 307   movptr(tmpReg, Address(tmpReg, owner_offset));
 308   testptr(tmpReg, tmpReg);
 309   jcc(Assembler::zero, DONE_LABEL);
 310   if (UseRTMXendForLockBusy) {
 311     xend();
 312     jmp(L_decrement_retry);
 313   }
 314   else {
 315     xabort(0);
 316   }
 317   bind(L_on_abort);
 318   Register abort_status_Reg = tmpReg; // status of abort is stored in RAX
 319   if (PrintPreciseRTMLockingStatistics || profile_rtm) {
 320     rtm_profiling(abort_status_Reg, scrReg, rtm_counters, method_data, profile_rtm);
 321   }
 322   if (RTMRetryCount &gt; 0) {
 323     // retry on lock abort if abort status is &#39;can retry&#39; (0x2) or &#39;memory conflict&#39; (0x4)
 324     rtm_retry_lock_on_abort(retry_on_abort_count_Reg, abort_status_Reg, L_rtm_retry);
 325   }
 326 
 327   movptr(tmpReg, Address(boxReg, owner_offset)) ;
 328   testptr(tmpReg, tmpReg) ;
 329   jccb(Assembler::notZero, L_decrement_retry) ;
 330 
 331   // Appears unlocked - try to swing _owner from null to non-null.
 332   // Invariant: tmpReg == 0.  tmpReg is EAX which is the implicit cmpxchg comparand.
 333 #ifdef _LP64
 334   Register threadReg = r15_thread;
 335 #else
 336   get_thread(scrReg);
 337   Register threadReg = scrReg;
 338 #endif
 339   lock();
 340   cmpxchgptr(threadReg, Address(boxReg, owner_offset)); // Updates tmpReg
 341 
 342   if (RTMRetryCount &gt; 0) {
 343     // success done else retry
 344     jccb(Assembler::equal, DONE_LABEL) ;
 345     bind(L_decrement_retry);
 346     // Spin and retry if lock is busy.
 347     rtm_retry_lock_on_busy(retry_on_busy_count_Reg, boxReg, tmpReg, scrReg, L_rtm_retry);
 348   }
 349   else {
 350     bind(L_decrement_retry);
 351   }
 352 }
 353 
 354 #endif //  INCLUDE_RTM_OPT
 355 
 356 // fast_lock and fast_unlock used by C2
 357 
 358 // Because the transitions from emitted code to the runtime
 359 // monitorenter/exit helper stubs are so slow it&#39;s critical that
 360 // we inline both the stack-locking fast path and the inflated fast path.
 361 //
 362 // See also: cmpFastLock and cmpFastUnlock.
 363 //
 364 // What follows is a specialized inline transliteration of the code
 365 // in enter() and exit(). If we&#39;re concerned about I$ bloat another
 366 // option would be to emit TrySlowEnter and TrySlowExit methods
 367 // at startup-time.  These methods would accept arguments as
 368 // (rax,=Obj, rbx=Self, rcx=box, rdx=Scratch) and return success-failure
 369 // indications in the icc.ZFlag.  fast_lock and fast_unlock would simply
 370 // marshal the arguments and emit calls to TrySlowEnter and TrySlowExit.
 371 // In practice, however, the # of lock sites is bounded and is usually small.
 372 // Besides the call overhead, TrySlowEnter and TrySlowExit might suffer
 373 // if the processor uses simple bimodal branch predictors keyed by EIP
 374 // Since the helper routines would be called from multiple synchronization
 375 // sites.
 376 //
 377 // An even better approach would be write &quot;MonitorEnter()&quot; and &quot;MonitorExit()&quot;
 378 // in java - using j.u.c and unsafe - and just bind the lock and unlock sites
 379 // to those specialized methods.  That&#39;d give us a mostly platform-independent
 380 // implementation that the JITs could optimize and inline at their pleasure.
 381 // Done correctly, the only time we&#39;d need to cross to native could would be
 382 // to park() or unpark() threads.  We&#39;d also need a few more unsafe operators
 383 // to (a) prevent compiler-JIT reordering of non-volatile accesses, and
 384 // (b) explicit barriers or fence operations.
 385 //
 386 // TODO:
 387 //
 388 // *  Arrange for C2 to pass &quot;Self&quot; into fast_lock and fast_unlock in one of the registers (scr).
 389 //    This avoids manifesting the Self pointer in the fast_lock and fast_unlock terminals.
 390 //    Given TLAB allocation, Self is usually manifested in a register, so passing it into
 391 //    the lock operators would typically be faster than reifying Self.
 392 //
 393 // *  Ideally I&#39;d define the primitives as:
 394 //       fast_lock   (nax Obj, nax box, EAX tmp, nax scr) where box, tmp and scr are KILLED.
 395 //       fast_unlock (nax Obj, EAX box, nax tmp) where box and tmp are KILLED
 396 //    Unfortunately ADLC bugs prevent us from expressing the ideal form.
 397 //    Instead, we&#39;re stuck with a rather awkward and brittle register assignments below.
 398 //    Furthermore the register assignments are overconstrained, possibly resulting in
 399 //    sub-optimal code near the synchronization site.
 400 //
 401 // *  Eliminate the sp-proximity tests and just use &quot;== Self&quot; tests instead.
 402 //    Alternately, use a better sp-proximity test.
 403 //
 404 // *  Currently ObjectMonitor._Owner can hold either an sp value or a (THREAD *) value.
 405 //    Either one is sufficient to uniquely identify a thread.
 406 //    TODO: eliminate use of sp in _owner and use get_thread(tr) instead.
 407 //
 408 // *  Intrinsify notify() and notifyAll() for the common cases where the
 409 //    object is locked by the calling thread but the waitlist is empty.
 410 //    avoid the expensive JNI call to JVM_Notify() and JVM_NotifyAll().
 411 //
 412 // *  use jccb and jmpb instead of jcc and jmp to improve code density.
 413 //    But beware of excessive branch density on AMD Opterons.
 414 //
 415 // *  Both fast_lock and fast_unlock set the ICC.ZF to indicate success
 416 //    or failure of the fast path.  If the fast path fails then we pass
 417 //    control to the slow path, typically in C.  In fast_lock and
 418 //    fast_unlock we often branch to DONE_LABEL, just to find that C2
 419 //    will emit a conditional branch immediately after the node.
 420 //    So we have branches to branches and lots of ICC.ZF games.
 421 //    Instead, it might be better to have C2 pass a &quot;FailureLabel&quot;
 422 //    into fast_lock and fast_unlock.  In the case of success, control
 423 //    will drop through the node.  ICC.ZF is undefined at exit.
 424 //    In the case of failure, the node will branch directly to the
 425 //    FailureLabel
 426 
 427 
 428 // obj: object to lock
 429 // box: on-stack box address (displaced header location) - KILLED
 430 // rax,: tmp -- KILLED
 431 // scr: tmp -- KILLED
 432 void C2_MacroAssembler::fast_lock(Register objReg, Register boxReg, Register tmpReg,
 433                                  Register scrReg, Register cx1Reg, Register cx2Reg,
 434                                  BiasedLockingCounters* counters,
 435                                  RTMLockingCounters* rtm_counters,
 436                                  RTMLockingCounters* stack_rtm_counters,
 437                                  Metadata* method_data,
 438                                  bool use_rtm, bool profile_rtm) {
 439   // Ensure the register assignments are disjoint
 440   assert(tmpReg == rax, &quot;&quot;);
 441 
 442   if (use_rtm) {
 443     assert_different_registers(objReg, boxReg, tmpReg, scrReg, cx1Reg, cx2Reg);
 444   } else {
<a name="1" id="anc1"></a><span class="line-removed"> 445     assert(cx1Reg == noreg, &quot;&quot;);</span>
 446     assert(cx2Reg == noreg, &quot;&quot;);
 447     assert_different_registers(objReg, boxReg, tmpReg, scrReg);
 448   }
 449 
 450   if (counters != NULL) {
 451     atomic_incl(ExternalAddress((address)counters-&gt;total_entry_count_addr()), scrReg);
 452   }
 453 
 454   // Possible cases that we&#39;ll encounter in fast_lock
 455   // ------------------------------------------------
 456   // * Inflated
 457   //    -- unlocked
 458   //    -- Locked
 459   //       = by self
 460   //       = by other
 461   // * biased
 462   //    -- by Self
 463   //    -- by other
 464   // * neutral
 465   // * stack-locked
 466   //    -- by self
 467   //       = sp-proximity test hits
 468   //       = sp-proximity test generates false-negative
 469   //    -- by other
 470   //
 471 
 472   Label IsInflated, DONE_LABEL;
 473 
 474   // it&#39;s stack-locked, biased or neutral
 475   // TODO: optimize away redundant LDs of obj-&gt;mark and improve the markword triage
 476   // order to reduce the number of conditional branches in the most common cases.
 477   // Beware -- there&#39;s a subtle invariant that fetch of the markword
 478   // at [FETCH], below, will never observe a biased encoding (*101b).
 479   // If this invariant is not held we risk exclusion (safety) failure.
 480   if (UseBiasedLocking &amp;&amp; !UseOptoBiasInlining) {
<a name="2" id="anc2"></a><span class="line-modified"> 481     biased_locking_enter(boxReg, objReg, tmpReg, scrReg, false, DONE_LABEL, NULL, counters);</span>
 482   }
 483 
 484 #if INCLUDE_RTM_OPT
 485   if (UseRTMForStackLocks &amp;&amp; use_rtm) {
 486     rtm_stack_locking(objReg, tmpReg, scrReg, cx2Reg,
 487                       stack_rtm_counters, method_data, profile_rtm,
 488                       DONE_LABEL, IsInflated);
 489   }
 490 #endif // INCLUDE_RTM_OPT
 491 
 492   movptr(tmpReg, Address(objReg, oopDesc::mark_offset_in_bytes()));          // [FETCH]
 493   testptr(tmpReg, markWord::monitor_value); // inflated vs stack-locked|neutral|biased
 494   jccb(Assembler::notZero, IsInflated);
 495 
 496   // Attempt stack-locking ...
 497   orptr (tmpReg, markWord::unlocked_value);
 498   if (EnableValhalla &amp;&amp; !UseBiasedLocking) {
 499     // Mask always_locked bit such that we go to the slow path if object is a value type
 500     andptr(tmpReg, ~((int) markWord::biased_lock_bit_in_place));
 501   }
 502   movptr(Address(boxReg, 0), tmpReg);          // Anticipate successful CAS
 503   lock();
 504   cmpxchgptr(boxReg, Address(objReg, oopDesc::mark_offset_in_bytes()));      // Updates tmpReg
 505   if (counters != NULL) {
 506     cond_inc32(Assembler::equal,
 507                ExternalAddress((address)counters-&gt;fast_path_entry_count_addr()));
 508   }
 509   jcc(Assembler::equal, DONE_LABEL);           // Success
 510 
 511   // Recursive locking.
 512   // The object is stack-locked: markword contains stack pointer to BasicLock.
 513   // Locked by current thread if difference with current SP is less than one page.
 514   subptr(tmpReg, rsp);
 515   // Next instruction set ZFlag == 1 (Success) if difference is less then one page.
 516   andptr(tmpReg, (int32_t) (NOT_LP64(0xFFFFF003) LP64_ONLY(7 - os::vm_page_size())) );
 517   movptr(Address(boxReg, 0), tmpReg);
 518   if (counters != NULL) {
 519     cond_inc32(Assembler::equal,
 520                ExternalAddress((address)counters-&gt;fast_path_entry_count_addr()));
 521   }
 522   jmp(DONE_LABEL);
 523 
 524   bind(IsInflated);
 525   // The object is inflated. tmpReg contains pointer to ObjectMonitor* + markWord::monitor_value
 526 
 527 #if INCLUDE_RTM_OPT
 528   // Use the same RTM locking code in 32- and 64-bit VM.
 529   if (use_rtm) {
 530     rtm_inflated_locking(objReg, boxReg, tmpReg, scrReg, cx1Reg, cx2Reg,
 531                          rtm_counters, method_data, profile_rtm, DONE_LABEL);
 532   } else {
 533 #endif // INCLUDE_RTM_OPT
 534 
 535 #ifndef _LP64
 536   // The object is inflated.
 537 
 538   // boxReg refers to the on-stack BasicLock in the current frame.
 539   // We&#39;d like to write:
 540   //   set box-&gt;_displaced_header = markWord::unused_mark().  Any non-0 value suffices.
 541   // This is convenient but results a ST-before-CAS penalty.  The following CAS suffers
 542   // additional latency as we have another ST in the store buffer that must drain.
 543 
 544   // avoid ST-before-CAS
 545   // register juggle because we need tmpReg for cmpxchgptr below
 546   movptr(scrReg, boxReg);
 547   movptr(boxReg, tmpReg);                   // consider: LEA box, [tmp-2]
 548 
 549   // Optimistic form: consider XORL tmpReg,tmpReg
 550   movptr(tmpReg, NULL_WORD);
 551 
 552   // Appears unlocked - try to swing _owner from null to non-null.
 553   // Ideally, I&#39;d manifest &quot;Self&quot; with get_thread and then attempt
 554   // to CAS the register containing Self into m-&gt;Owner.
 555   // But we don&#39;t have enough registers, so instead we can either try to CAS
 556   // rsp or the address of the box (in scr) into &amp;m-&gt;owner.  If the CAS succeeds
 557   // we later store &quot;Self&quot; into m-&gt;Owner.  Transiently storing a stack address
 558   // (rsp or the address of the box) into  m-&gt;owner is harmless.
 559   // Invariant: tmpReg == 0.  tmpReg is EAX which is the implicit cmpxchg comparand.
 560   lock();
 561   cmpxchgptr(scrReg, Address(boxReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)));
 562   movptr(Address(scrReg, 0), 3);          // box-&gt;_displaced_header = 3
 563   // If we weren&#39;t able to swing _owner from NULL to the BasicLock
 564   // then take the slow path.
 565   jccb  (Assembler::notZero, DONE_LABEL);
 566   // update _owner from BasicLock to thread
 567   get_thread (scrReg);                    // beware: clobbers ICCs
 568   movptr(Address(boxReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)), scrReg);
 569   xorptr(boxReg, boxReg);                 // set icc.ZFlag = 1 to indicate success
 570 
 571   // If the CAS fails we can either retry or pass control to the slow path.
 572   // We use the latter tactic.
 573   // Pass the CAS result in the icc.ZFlag into DONE_LABEL
 574   // If the CAS was successful ...
 575   //   Self has acquired the lock
 576   //   Invariant: m-&gt;_recursions should already be 0, so we don&#39;t need to explicitly set it.
 577   // Intentional fall-through into DONE_LABEL ...
 578 #else // _LP64
 579   // It&#39;s inflated and we use scrReg for ObjectMonitor* in this section.
 580   movq(scrReg, tmpReg);
 581   xorq(tmpReg, tmpReg);
 582   lock();
 583   cmpxchgptr(r15_thread, Address(scrReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)));
 584   // Unconditionally set box-&gt;_displaced_header = markWord::unused_mark().
 585   // Without cast to int32_t this style of movptr will destroy r10 which is typically obj.
 586   movptr(Address(boxReg, 0), (int32_t)intptr_t(markWord::unused_mark().value()));
 587   // Intentional fall-through into DONE_LABEL ...
 588   // Propagate ICC.ZF from CAS above into DONE_LABEL.
 589 #endif // _LP64
 590 #if INCLUDE_RTM_OPT
 591   } // use_rtm()
 592 #endif
 593   // DONE_LABEL is a hot target - we&#39;d really like to place it at the
 594   // start of cache line by padding with NOPs.
 595   // See the AMD and Intel software optimization manuals for the
 596   // most efficient &quot;long&quot; NOP encodings.
 597   // Unfortunately none of our alignment mechanisms suffice.
 598   bind(DONE_LABEL);
 599 
 600   // At DONE_LABEL the icc ZFlag is set as follows ...
 601   // fast_unlock uses the same protocol.
 602   // ZFlag == 1 -&gt; Success
 603   // ZFlag == 0 -&gt; Failure - force control through the slow path
 604 }
 605 
 606 // obj: object to unlock
 607 // box: box address (displaced header location), killed.  Must be EAX.
 608 // tmp: killed, cannot be obj nor box.
 609 //
 610 // Some commentary on balanced locking:
 611 //
 612 // fast_lock and fast_unlock are emitted only for provably balanced lock sites.
 613 // Methods that don&#39;t have provably balanced locking are forced to run in the
 614 // interpreter - such methods won&#39;t be compiled to use fast_lock and fast_unlock.
 615 // The interpreter provides two properties:
 616 // I1:  At return-time the interpreter automatically and quietly unlocks any
 617 //      objects acquired the current activation (frame).  Recall that the
 618 //      interpreter maintains an on-stack list of locks currently held by
 619 //      a frame.
 620 // I2:  If a method attempts to unlock an object that is not held by the
 621 //      the frame the interpreter throws IMSX.
 622 //
 623 // Lets say A(), which has provably balanced locking, acquires O and then calls B().
 624 // B() doesn&#39;t have provably balanced locking so it runs in the interpreter.
 625 // Control returns to A() and A() unlocks O.  By I1 and I2, above, we know that O
 626 // is still locked by A().
 627 //
 628 // The only other source of unbalanced locking would be JNI.  The &quot;Java Native Interface:
 629 // Programmer&#39;s Guide and Specification&quot; claims that an object locked by jni_monitorenter
 630 // should not be unlocked by &quot;normal&quot; java-level locking and vice-versa.  The specification
 631 // doesn&#39;t specify what will occur if a program engages in such mixed-mode locking, however.
 632 // Arguably given that the spec legislates the JNI case as undefined our implementation
 633 // could reasonably *avoid* checking owner in fast_unlock().
 634 // In the interest of performance we elide m-&gt;Owner==Self check in unlock.
 635 // A perfectly viable alternative is to elide the owner check except when
 636 // Xcheck:jni is enabled.
 637 
 638 void C2_MacroAssembler::fast_unlock(Register objReg, Register boxReg, Register tmpReg, bool use_rtm) {
 639   assert(boxReg == rax, &quot;&quot;);
 640   assert_different_registers(objReg, boxReg, tmpReg);
 641 
 642   Label DONE_LABEL, Stacked, CheckSucc;
 643 
 644   // Critically, the biased locking test must have precedence over
 645   // and appear before the (box-&gt;dhw == 0) recursive stack-lock test.
 646   if (UseBiasedLocking &amp;&amp; !UseOptoBiasInlining) {
 647     biased_locking_exit(objReg, tmpReg, DONE_LABEL);
 648   }
 649 
 650 #if INCLUDE_RTM_OPT
 651   if (UseRTMForStackLocks &amp;&amp; use_rtm) {
 652     assert(!UseBiasedLocking, &quot;Biased locking is not supported with RTM locking&quot;);
 653     Label L_regular_unlock;
 654     movptr(tmpReg, Address(objReg, oopDesc::mark_offset_in_bytes())); // fetch markword
 655     andptr(tmpReg, markWord::biased_lock_mask_in_place);              // look at 3 lock bits
 656     cmpptr(tmpReg, markWord::unlocked_value);                         // bits = 001 unlocked
 657     jccb(Assembler::notEqual, L_regular_unlock);                      // if !HLE RegularLock
 658     xend();                                                           // otherwise end...
 659     jmp(DONE_LABEL);                                                  // ... and we&#39;re done
 660     bind(L_regular_unlock);
 661   }
 662 #endif
 663 
 664   cmpptr(Address(boxReg, 0), (int32_t)NULL_WORD);                   // Examine the displaced header
 665   jcc   (Assembler::zero, DONE_LABEL);                              // 0 indicates recursive stack-lock
 666   movptr(tmpReg, Address(objReg, oopDesc::mark_offset_in_bytes())); // Examine the object&#39;s markword
 667   testptr(tmpReg, markWord::monitor_value);                         // Inflated?
 668   jccb  (Assembler::zero, Stacked);
 669 
 670   // It&#39;s inflated.
 671 #if INCLUDE_RTM_OPT
 672   if (use_rtm) {
 673     Label L_regular_inflated_unlock;
 674     int owner_offset = OM_OFFSET_NO_MONITOR_VALUE_TAG(owner);
 675     movptr(boxReg, Address(tmpReg, owner_offset));
 676     testptr(boxReg, boxReg);
 677     jccb(Assembler::notZero, L_regular_inflated_unlock);
 678     xend();
 679     jmpb(DONE_LABEL);
 680     bind(L_regular_inflated_unlock);
 681   }
 682 #endif
 683 
 684   // Despite our balanced locking property we still check that m-&gt;_owner == Self
 685   // as java routines or native JNI code called by this thread might
 686   // have released the lock.
 687   // Refer to the comments in synchronizer.cpp for how we might encode extra
 688   // state in _succ so we can avoid fetching EntryList|cxq.
 689   //
 690   // I&#39;d like to add more cases in fast_lock() and fast_unlock() --
 691   // such as recursive enter and exit -- but we have to be wary of
 692   // I$ bloat, T$ effects and BP$ effects.
 693   //
 694   // If there&#39;s no contention try a 1-0 exit.  That is, exit without
 695   // a costly MEMBAR or CAS.  See synchronizer.cpp for details on how
 696   // we detect and recover from the race that the 1-0 exit admits.
 697   //
 698   // Conceptually fast_unlock() must execute a STST|LDST &quot;release&quot; barrier
 699   // before it STs null into _owner, releasing the lock.  Updates
 700   // to data protected by the critical section must be visible before
 701   // we drop the lock (and thus before any other thread could acquire
 702   // the lock and observe the fields protected by the lock).
 703   // IA32&#39;s memory-model is SPO, so STs are ordered with respect to
 704   // each other and there&#39;s no need for an explicit barrier (fence).
 705   // See also http://gee.cs.oswego.edu/dl/jmm/cookbook.html.
 706 #ifndef _LP64
 707   get_thread (boxReg);
 708 
 709   // Note that we could employ various encoding schemes to reduce
 710   // the number of loads below (currently 4) to just 2 or 3.
 711   // Refer to the comments in synchronizer.cpp.
 712   // In practice the chain of fetches doesn&#39;t seem to impact performance, however.
 713   xorptr(boxReg, boxReg);
 714   orptr(boxReg, Address(tmpReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(recursions)));
 715   jccb  (Assembler::notZero, DONE_LABEL);
 716   movptr(boxReg, Address(tmpReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(EntryList)));
 717   orptr(boxReg, Address(tmpReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(cxq)));
 718   jccb  (Assembler::notZero, CheckSucc);
 719   movptr(Address(tmpReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)), NULL_WORD);
 720   jmpb  (DONE_LABEL);
 721 
 722   bind (Stacked);
 723   // It&#39;s not inflated and it&#39;s not recursively stack-locked and it&#39;s not biased.
 724   // It must be stack-locked.
 725   // Try to reset the header to displaced header.
 726   // The &quot;box&quot; value on the stack is stable, so we can reload
 727   // and be assured we observe the same value as above.
 728   movptr(tmpReg, Address(boxReg, 0));
 729   lock();
 730   cmpxchgptr(tmpReg, Address(objReg, oopDesc::mark_offset_in_bytes())); // Uses RAX which is box
 731   // Intention fall-thru into DONE_LABEL
 732 
 733   // DONE_LABEL is a hot target - we&#39;d really like to place it at the
 734   // start of cache line by padding with NOPs.
 735   // See the AMD and Intel software optimization manuals for the
 736   // most efficient &quot;long&quot; NOP encodings.
 737   // Unfortunately none of our alignment mechanisms suffice.
 738   bind (CheckSucc);
 739 #else // _LP64
 740   // It&#39;s inflated
 741   xorptr(boxReg, boxReg);
 742   orptr(boxReg, Address(tmpReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(recursions)));
 743   jccb  (Assembler::notZero, DONE_LABEL);
 744   movptr(boxReg, Address(tmpReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(cxq)));
 745   orptr(boxReg, Address(tmpReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(EntryList)));
 746   jccb  (Assembler::notZero, CheckSucc);
 747   // Without cast to int32_t this style of movptr will destroy r10 which is typically obj.
 748   movptr(Address(tmpReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)), (int32_t)NULL_WORD);
 749   jmpb  (DONE_LABEL);
 750 
 751   // Try to avoid passing control into the slow_path ...
 752   Label LSuccess, LGoSlowPath ;
 753   bind  (CheckSucc);
 754 
 755   // The following optional optimization can be elided if necessary
 756   // Effectively: if (succ == null) goto slow path
 757   // The code reduces the window for a race, however,
 758   // and thus benefits performance.
 759   cmpptr(Address(tmpReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(succ)), (int32_t)NULL_WORD);
 760   jccb  (Assembler::zero, LGoSlowPath);
 761 
 762   xorptr(boxReg, boxReg);
 763   // Without cast to int32_t this style of movptr will destroy r10 which is typically obj.
 764   movptr(Address(tmpReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)), (int32_t)NULL_WORD);
 765 
 766   // Memory barrier/fence
 767   // Dekker pivot point -- fulcrum : ST Owner; MEMBAR; LD Succ
 768   // Instead of MFENCE we use a dummy locked add of 0 to the top-of-stack.
 769   // This is faster on Nehalem and AMD Shanghai/Barcelona.
 770   // See https://blogs.oracle.com/dave/entry/instruction_selection_for_volatile_fences
 771   // We might also restructure (ST Owner=0;barrier;LD _Succ) to
 772   // (mov box,0; xchgq box, &amp;m-&gt;Owner; LD _succ) .
 773   lock(); addl(Address(rsp, 0), 0);
 774 
 775   cmpptr(Address(tmpReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(succ)), (int32_t)NULL_WORD);
 776   jccb  (Assembler::notZero, LSuccess);
 777 
 778   // Rare inopportune interleaving - race.
 779   // The successor vanished in the small window above.
 780   // The lock is contended -- (cxq|EntryList) != null -- and there&#39;s no apparent successor.
 781   // We need to ensure progress and succession.
 782   // Try to reacquire the lock.
 783   // If that fails then the new owner is responsible for succession and this
 784   // thread needs to take no further action and can exit via the fast path (success).
 785   // If the re-acquire succeeds then pass control into the slow path.
 786   // As implemented, this latter mode is horrible because we generated more
 787   // coherence traffic on the lock *and* artifically extended the critical section
 788   // length while by virtue of passing control into the slow path.
 789 
 790   // box is really RAX -- the following CMPXCHG depends on that binding
 791   // cmpxchg R,[M] is equivalent to rax = CAS(M,rax,R)
 792   lock();
 793   cmpxchgptr(r15_thread, Address(tmpReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)));
 794   // There&#39;s no successor so we tried to regrab the lock.
 795   // If that didn&#39;t work, then another thread grabbed the
 796   // lock so we&#39;re done (and exit was a success).
 797   jccb  (Assembler::notEqual, LSuccess);
 798   // Intentional fall-through into slow path
 799 
 800   bind  (LGoSlowPath);
 801   orl   (boxReg, 1);                      // set ICC.ZF=0 to indicate failure
 802   jmpb  (DONE_LABEL);
 803 
 804   bind  (LSuccess);
 805   testl (boxReg, 0);                      // set ICC.ZF=1 to indicate success
 806   jmpb  (DONE_LABEL);
 807 
 808   bind  (Stacked);
 809   movptr(tmpReg, Address (boxReg, 0));      // re-fetch
 810   lock();
 811   cmpxchgptr(tmpReg, Address(objReg, oopDesc::mark_offset_in_bytes())); // Uses RAX which is box
 812 
 813 #endif
 814   bind(DONE_LABEL);
 815 }
 816 
 817 //-------------------------------------------------------------------------------------------
 818 // Generic instructions support for use in .ad files C2 code generation
 819 
 820 void C2_MacroAssembler::vabsnegd(int opcode, XMMRegister dst, XMMRegister src, Register scr) {
 821   if (dst != src) {
 822     movdqu(dst, src);
 823   }
 824   if (opcode == Op_AbsVD) {
 825     andpd(dst, ExternalAddress(StubRoutines::x86::vector_double_sign_mask()), scr);
 826   } else {
 827     assert((opcode == Op_NegVD),&quot;opcode should be Op_NegD&quot;);
 828     xorpd(dst, ExternalAddress(StubRoutines::x86::vector_double_sign_flip()), scr);
 829   }
 830 }
 831 
 832 void C2_MacroAssembler::vabsnegd(int opcode, XMMRegister dst, XMMRegister src, int vector_len, Register scr) {
 833   if (opcode == Op_AbsVD) {
 834     vandpd(dst, src, ExternalAddress(StubRoutines::x86::vector_double_sign_mask()), vector_len, scr);
 835   } else {
 836     assert((opcode == Op_NegVD),&quot;opcode should be Op_NegD&quot;);
 837     vxorpd(dst, src, ExternalAddress(StubRoutines::x86::vector_double_sign_flip()), vector_len, scr);
 838   }
 839 }
 840 
 841 void C2_MacroAssembler::vabsnegf(int opcode, XMMRegister dst, XMMRegister src, Register scr) {
 842   if (dst != src) {
 843     movdqu(dst, src);
 844   }
 845   if (opcode == Op_AbsVF) {
 846     andps(dst, ExternalAddress(StubRoutines::x86::vector_float_sign_mask()), scr);
 847   } else {
 848     assert((opcode == Op_NegVF),&quot;opcode should be Op_NegF&quot;);
 849     xorps(dst, ExternalAddress(StubRoutines::x86::vector_float_sign_flip()), scr);
 850   }
 851 }
 852 
 853 void C2_MacroAssembler::vabsnegf(int opcode, XMMRegister dst, XMMRegister src, int vector_len, Register scr) {
 854   if (opcode == Op_AbsVF) {
 855     vandps(dst, src, ExternalAddress(StubRoutines::x86::vector_float_sign_mask()), vector_len, scr);
 856   } else {
 857     assert((opcode == Op_NegVF),&quot;opcode should be Op_NegF&quot;);
 858     vxorps(dst, src, ExternalAddress(StubRoutines::x86::vector_float_sign_flip()), vector_len, scr);
 859   }
 860 }
 861 
 862 void C2_MacroAssembler::vextendbw(bool sign, XMMRegister dst, XMMRegister src) {
 863   if (sign) {
 864     pmovsxbw(dst, src);
 865   } else {
 866     pmovzxbw(dst, src);
 867   }
 868 }
 869 
 870 void C2_MacroAssembler::vextendbw(bool sign, XMMRegister dst, XMMRegister src, int vector_len) {
 871   if (sign) {
 872     vpmovsxbw(dst, src, vector_len);
 873   } else {
 874     vpmovzxbw(dst, src, vector_len);
 875   }
 876 }
 877 
 878 void C2_MacroAssembler::vshiftd(int opcode, XMMRegister dst, XMMRegister src) {
 879   if (opcode == Op_RShiftVI) {
 880     psrad(dst, src);
 881   } else if (opcode == Op_LShiftVI) {
 882     pslld(dst, src);
 883   } else {
 884     assert((opcode == Op_URShiftVI),&quot;opcode should be Op_URShiftVI&quot;);
 885     psrld(dst, src);
 886   }
 887 }
 888 
 889 void C2_MacroAssembler::vshiftd(int opcode, XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {
 890   if (opcode == Op_RShiftVI) {
 891     vpsrad(dst, nds, src, vector_len);
 892   } else if (opcode == Op_LShiftVI) {
 893     vpslld(dst, nds, src, vector_len);
 894   } else {
 895     assert((opcode == Op_URShiftVI),&quot;opcode should be Op_URShiftVI&quot;);
 896     vpsrld(dst, nds, src, vector_len);
 897   }
 898 }
 899 
 900 void C2_MacroAssembler::vshiftw(int opcode, XMMRegister dst, XMMRegister src) {
 901   if ((opcode == Op_RShiftVS) || (opcode == Op_RShiftVB)) {
 902     psraw(dst, src);
 903   } else if ((opcode == Op_LShiftVS) || (opcode == Op_LShiftVB)) {
 904     psllw(dst, src);
 905   } else {
 906     assert(((opcode == Op_URShiftVS) || (opcode == Op_URShiftVB)),&quot;opcode should be one of Op_URShiftVS or Op_URShiftVB&quot;);
 907     psrlw(dst, src);
 908   }
 909 }
 910 
 911 void C2_MacroAssembler::vshiftw(int opcode, XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {
 912   if ((opcode == Op_RShiftVS) || (opcode == Op_RShiftVB)) {
 913     vpsraw(dst, nds, src, vector_len);
 914   } else if ((opcode == Op_LShiftVS) || (opcode == Op_LShiftVB)) {
 915     vpsllw(dst, nds, src, vector_len);
 916   } else {
 917     assert(((opcode == Op_URShiftVS) || (opcode == Op_URShiftVB)),&quot;opcode should be one of Op_URShiftVS or Op_URShiftVB&quot;);
 918     vpsrlw(dst, nds, src, vector_len);
 919   }
 920 }
 921 
 922 void C2_MacroAssembler::vshiftq(int opcode, XMMRegister dst, XMMRegister src) {
 923   if (opcode == Op_RShiftVL) {
 924     psrlq(dst, src);  // using srl to implement sra on pre-avs512 systems
 925   } else if (opcode == Op_LShiftVL) {
 926     psllq(dst, src);
 927   } else {
 928     assert((opcode == Op_URShiftVL),&quot;opcode should be Op_URShiftVL&quot;);
 929     psrlq(dst, src);
 930   }
 931 }
 932 
 933 void C2_MacroAssembler::vshiftq(int opcode, XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {
 934   if (opcode == Op_RShiftVL) {
 935     evpsraq(dst, nds, src, vector_len);
 936   } else if (opcode == Op_LShiftVL) {
 937     vpsllq(dst, nds, src, vector_len);
 938   } else {
 939     assert((opcode == Op_URShiftVL),&quot;opcode should be Op_URShiftVL&quot;);
 940     vpsrlq(dst, nds, src, vector_len);
 941   }
 942 }
 943 
 944 // Reductions for vectors of ints, longs, floats, and doubles.
 945 
 946 void C2_MacroAssembler::reduce_operation_128(int opcode, XMMRegister dst, XMMRegister src) {
 947   int vector_len = Assembler::AVX_128bit;
 948 
 949   switch (opcode) {
 950     case Op_AndReductionV:  pand(dst, src); break;
 951     case Op_OrReductionV:   por (dst, src); break;
 952     case Op_XorReductionV:  pxor(dst, src); break;
 953 
 954     case Op_AddReductionVF: addss(dst, src); break;
 955     case Op_AddReductionVD: addsd(dst, src); break;
 956     case Op_AddReductionVI: paddd(dst, src); break;
 957     case Op_AddReductionVL: paddq(dst, src); break;
 958 
 959     case Op_MulReductionVF: mulss(dst, src); break;
 960     case Op_MulReductionVD: mulsd(dst, src); break;
 961     case Op_MulReductionVI: pmulld(dst, src); break;
 962     case Op_MulReductionVL: vpmullq(dst, dst, src, vector_len); break;
 963 
 964     default: assert(false, &quot;wrong opcode&quot;);
 965   }
 966 }
 967 
 968 void C2_MacroAssembler::reduce_operation_256(int opcode, XMMRegister dst,  XMMRegister src1, XMMRegister src2) {
 969   int vector_len = Assembler::AVX_256bit;
 970 
 971   switch (opcode) {
 972     case Op_AndReductionV:  vpand(dst, src1, src2, vector_len); break;
 973     case Op_OrReductionV:   vpor (dst, src1, src2, vector_len); break;
 974     case Op_XorReductionV:  vpxor(dst, src1, src2, vector_len); break;
 975 
 976     case Op_AddReductionVI: vpaddd(dst, src1, src2, vector_len); break;
 977     case Op_AddReductionVL: vpaddq(dst, src1, src2, vector_len); break;
 978 
 979     case Op_MulReductionVI: vpmulld(dst, src1, src2, vector_len); break;
 980     case Op_MulReductionVL: vpmullq(dst, src1, src2, vector_len); break;
 981 
 982     default: assert(false, &quot;wrong opcode&quot;);
 983   }
 984 }
 985 
 986 void C2_MacroAssembler::reduce_fp(int opcode, int vlen,
 987                                   XMMRegister dst, XMMRegister src,
 988                                   XMMRegister vtmp1, XMMRegister vtmp2) {
 989   switch (opcode) {
 990     case Op_AddReductionVF:
 991     case Op_MulReductionVF:
 992       reduceF(opcode, vlen, dst, src, vtmp1, vtmp2);
 993       break;
 994 
 995     case Op_AddReductionVD:
 996     case Op_MulReductionVD:
 997       reduceD(opcode, vlen, dst, src, vtmp1, vtmp2);
 998       break;
 999 
1000     default: assert(false, &quot;wrong opcode&quot;);
1001   }
1002 }
1003 
1004 void C2_MacroAssembler::reduceI(int opcode, int vlen,
1005                                 Register dst, Register src1, XMMRegister src2,
1006                                 XMMRegister vtmp1, XMMRegister vtmp2) {
1007   switch (vlen) {
1008     case  2: reduce2I (opcode, dst, src1, src2, vtmp1, vtmp2); break;
1009     case  4: reduce4I (opcode, dst, src1, src2, vtmp1, vtmp2); break;
1010     case  8: reduce8I (opcode, dst, src1, src2, vtmp1, vtmp2); break;
1011     case 16: reduce16I(opcode, dst, src1, src2, vtmp1, vtmp2); break;
1012 
1013     default: assert(false, &quot;wrong vector length&quot;);
1014   }
1015 }
1016 
1017 #ifdef _LP64
1018 void C2_MacroAssembler::reduceL(int opcode, int vlen,
1019                                 Register dst, Register src1, XMMRegister src2,
1020                                 XMMRegister vtmp1, XMMRegister vtmp2) {
1021   switch (vlen) {
1022     case 2: reduce2L(opcode, dst, src1, src2, vtmp1, vtmp2); break;
1023     case 4: reduce4L(opcode, dst, src1, src2, vtmp1, vtmp2); break;
1024     case 8: reduce8L(opcode, dst, src1, src2, vtmp1, vtmp2); break;
1025 
1026     default: assert(false, &quot;wrong vector length&quot;);
1027   }
1028 }
1029 #endif // _LP64
1030 
1031 void C2_MacroAssembler::reduceF(int opcode, int vlen, XMMRegister dst, XMMRegister src, XMMRegister vtmp1, XMMRegister vtmp2) {
1032   switch (vlen) {
1033     case 2:
1034       assert(vtmp2 == xnoreg, &quot;&quot;);
1035       reduce2F(opcode, dst, src, vtmp1);
1036       break;
1037     case 4:
1038       assert(vtmp2 == xnoreg, &quot;&quot;);
1039       reduce4F(opcode, dst, src, vtmp1);
1040       break;
1041     case 8:
1042       reduce8F(opcode, dst, src, vtmp1, vtmp2);
1043       break;
1044     case 16:
1045       reduce16F(opcode, dst, src, vtmp1, vtmp2);
1046       break;
1047     default: assert(false, &quot;wrong vector length&quot;);
1048   }
1049 }
1050 
1051 void C2_MacroAssembler::reduceD(int opcode, int vlen, XMMRegister dst, XMMRegister src, XMMRegister vtmp1, XMMRegister vtmp2) {
1052   switch (vlen) {
1053     case 2:
1054       assert(vtmp2 == xnoreg, &quot;&quot;);
1055       reduce2D(opcode, dst, src, vtmp1);
1056       break;
1057     case 4:
1058       reduce4D(opcode, dst, src, vtmp1, vtmp2);
1059       break;
1060     case 8:
1061       reduce8D(opcode, dst, src, vtmp1, vtmp2);
1062       break;
1063     default: assert(false, &quot;wrong vector length&quot;);
1064   }
1065 }
1066 
1067 void C2_MacroAssembler::reduce2I(int opcode, Register dst, Register src1, XMMRegister src2, XMMRegister vtmp1, XMMRegister vtmp2) {
1068   if (opcode == Op_AddReductionVI) {
1069     if (vtmp1 != src2) {
1070       movdqu(vtmp1, src2);
1071     }
1072     phaddd(vtmp1, vtmp1);
1073   } else {
1074     pshufd(vtmp1, src2, 0x1);
1075     reduce_operation_128(opcode, vtmp1, src2);
1076   }
1077   movdl(vtmp2, src1);
1078   reduce_operation_128(opcode, vtmp1, vtmp2);
1079   movdl(dst, vtmp1);
1080 }
1081 
1082 void C2_MacroAssembler::reduce4I(int opcode, Register dst, Register src1, XMMRegister src2, XMMRegister vtmp1, XMMRegister vtmp2) {
1083   if (opcode == Op_AddReductionVI) {
1084     if (vtmp1 != src2) {
1085       movdqu(vtmp1, src2);
1086     }
1087     phaddd(vtmp1, src2);
1088     reduce2I(opcode, dst, src1, vtmp1, vtmp1, vtmp2);
1089   } else {
1090     pshufd(vtmp2, src2, 0xE);
1091     reduce_operation_128(opcode, vtmp2, src2);
1092     reduce2I(opcode, dst, src1, vtmp2, vtmp1, vtmp2);
1093   }
1094 }
1095 
1096 void C2_MacroAssembler::reduce8I(int opcode, Register dst, Register src1, XMMRegister src2, XMMRegister vtmp1, XMMRegister vtmp2) {
1097   if (opcode == Op_AddReductionVI) {
1098     vphaddd(vtmp1, src2, src2, Assembler::AVX_256bit);
1099     vextracti128_high(vtmp2, vtmp1);
1100     vpaddd(vtmp1, vtmp1, vtmp2, Assembler::AVX_128bit);
1101     reduce2I(opcode, dst, src1, vtmp1, vtmp1, vtmp2);
1102   } else {
1103     vextracti128_high(vtmp1, src2);
1104     reduce_operation_128(opcode, vtmp1, src2);
1105     reduce4I(opcode, dst, src1, vtmp1, vtmp1, vtmp2);
1106   }
1107 }
1108 
1109 void C2_MacroAssembler::reduce16I(int opcode, Register dst, Register src1, XMMRegister src2, XMMRegister vtmp1, XMMRegister vtmp2) {
1110   vextracti64x4_high(vtmp2, src2);
1111   reduce_operation_256(opcode, vtmp2, vtmp2, src2);
1112   reduce8I(opcode, dst, src1, vtmp2, vtmp1, vtmp2);
1113 }
1114 
1115 #ifdef _LP64
1116 void C2_MacroAssembler::reduce2L(int opcode, Register dst, Register src1, XMMRegister src2, XMMRegister vtmp1, XMMRegister vtmp2) {
1117   pshufd(vtmp2, src2, 0xE);
1118   reduce_operation_128(opcode, vtmp2, src2);
1119   movdq(vtmp1, src1);
1120   reduce_operation_128(opcode, vtmp1, vtmp2);
1121   movdq(dst, vtmp1);
1122 }
1123 
1124 void C2_MacroAssembler::reduce4L(int opcode, Register dst, Register src1, XMMRegister src2, XMMRegister vtmp1, XMMRegister vtmp2) {
1125   vextracti128_high(vtmp1, src2);
1126   reduce_operation_128(opcode, vtmp1, src2);
1127   reduce2L(opcode, dst, src1, vtmp1, vtmp1, vtmp2);
1128 }
1129 
1130 void C2_MacroAssembler::reduce8L(int opcode, Register dst, Register src1, XMMRegister src2, XMMRegister vtmp1, XMMRegister vtmp2) {
1131   vextracti64x4_high(vtmp2, src2);
1132   reduce_operation_256(opcode, vtmp2, vtmp2, src2);
1133   reduce4L(opcode, dst, src1, vtmp2, vtmp1, vtmp2);
1134 }
1135 #endif // _LP64
1136 
1137 void C2_MacroAssembler::reduce2F(int opcode, XMMRegister dst, XMMRegister src, XMMRegister vtmp) {
1138   reduce_operation_128(opcode, dst, src);
1139   pshufd(vtmp, src, 0x1);
1140   reduce_operation_128(opcode, dst, vtmp);
1141 }
1142 
1143 void C2_MacroAssembler::reduce4F(int opcode, XMMRegister dst, XMMRegister src, XMMRegister vtmp) {
1144   reduce2F(opcode, dst, src, vtmp);
1145   pshufd(vtmp, src, 0x2);
1146   reduce_operation_128(opcode, dst, vtmp);
1147   pshufd(vtmp, src, 0x3);
1148   reduce_operation_128(opcode, dst, vtmp);
1149 }
1150 
1151 void C2_MacroAssembler::reduce8F(int opcode, XMMRegister dst, XMMRegister src, XMMRegister vtmp1, XMMRegister vtmp2) {
1152   reduce4F(opcode, dst, src, vtmp2);
1153   vextractf128_high(vtmp2, src);
1154   reduce4F(opcode, dst, vtmp2, vtmp1);
1155 }
1156 
1157 void C2_MacroAssembler::reduce16F(int opcode, XMMRegister dst, XMMRegister src, XMMRegister vtmp1, XMMRegister vtmp2) {
1158   reduce8F(opcode, dst, src, vtmp1, vtmp2);
1159   vextracti64x4_high(vtmp1, src);
1160   reduce8F(opcode, dst, vtmp1, vtmp1, vtmp2);
1161 }
1162 
1163 void C2_MacroAssembler::reduce2D(int opcode, XMMRegister dst, XMMRegister src, XMMRegister vtmp) {
1164   reduce_operation_128(opcode, dst, src);
1165   pshufd(vtmp, src, 0xE);
1166   reduce_operation_128(opcode, dst, vtmp);
1167 }
1168 
1169 void C2_MacroAssembler::reduce4D(int opcode, XMMRegister dst, XMMRegister src, XMMRegister vtmp1, XMMRegister vtmp2) {
1170   reduce2D(opcode, dst, src, vtmp2);
1171   vextractf128_high(vtmp2, src);
1172   reduce2D(opcode, dst, vtmp2, vtmp1);
1173 }
1174 
1175 void C2_MacroAssembler::reduce8D(int opcode, XMMRegister dst, XMMRegister src, XMMRegister vtmp1, XMMRegister vtmp2) {
1176   reduce4D(opcode, dst, src, vtmp1, vtmp2);
1177   vextracti64x4_high(vtmp1, src);
1178   reduce4D(opcode, dst, vtmp1, vtmp1, vtmp2);
1179 }
1180 
1181 //-------------------------------------------------------------------------------------------
1182 
1183 // IndexOf for constant substrings with size &gt;= 8 chars
1184 // which don&#39;t need to be loaded through stack.
1185 void C2_MacroAssembler::string_indexofC8(Register str1, Register str2,
1186                                          Register cnt1, Register cnt2,
1187                                          int int_cnt2,  Register result,
1188                                          XMMRegister vec, Register tmp,
1189                                          int ae) {
1190   ShortBranchVerifier sbv(this);
1191   assert(UseSSE42Intrinsics, &quot;SSE4.2 intrinsics are required&quot;);
1192   assert(ae != StrIntrinsicNode::LU, &quot;Invalid encoding&quot;);
1193 
1194   // This method uses the pcmpestri instruction with bound registers
1195   //   inputs:
1196   //     xmm - substring
1197   //     rax - substring length (elements count)
1198   //     mem - scanned string
1199   //     rdx - string length (elements count)
1200   //     0xd - mode: 1100 (substring search) + 01 (unsigned shorts)
1201   //     0xc - mode: 1100 (substring search) + 00 (unsigned bytes)
1202   //   outputs:
1203   //     rcx - matched index in string
1204   assert(cnt1 == rdx &amp;&amp; cnt2 == rax &amp;&amp; tmp == rcx, &quot;pcmpestri&quot;);
1205   int mode   = (ae == StrIntrinsicNode::LL) ? 0x0c : 0x0d; // bytes or shorts
1206   int stride = (ae == StrIntrinsicNode::LL) ? 16 : 8; //UU, UL -&gt; 8
1207   Address::ScaleFactor scale1 = (ae == StrIntrinsicNode::LL) ? Address::times_1 : Address::times_2;
1208   Address::ScaleFactor scale2 = (ae == StrIntrinsicNode::UL) ? Address::times_1 : scale1;
1209 
1210   Label RELOAD_SUBSTR, SCAN_TO_SUBSTR, SCAN_SUBSTR,
1211         RET_FOUND, RET_NOT_FOUND, EXIT, FOUND_SUBSTR,
1212         MATCH_SUBSTR_HEAD, RELOAD_STR, FOUND_CANDIDATE;
1213 
1214   // Note, inline_string_indexOf() generates checks:
1215   // if (substr.count &gt; string.count) return -1;
1216   // if (substr.count == 0) return 0;
1217   assert(int_cnt2 &gt;= stride, &quot;this code is used only for cnt2 &gt;= 8 chars&quot;);
1218 
1219   // Load substring.
1220   if (ae == StrIntrinsicNode::UL) {
1221     pmovzxbw(vec, Address(str2, 0));
1222   } else {
1223     movdqu(vec, Address(str2, 0));
1224   }
1225   movl(cnt2, int_cnt2);
1226   movptr(result, str1); // string addr
1227 
1228   if (int_cnt2 &gt; stride) {
1229     jmpb(SCAN_TO_SUBSTR);
1230 
1231     // Reload substr for rescan, this code
1232     // is executed only for large substrings (&gt; 8 chars)
1233     bind(RELOAD_SUBSTR);
1234     if (ae == StrIntrinsicNode::UL) {
1235       pmovzxbw(vec, Address(str2, 0));
1236     } else {
1237       movdqu(vec, Address(str2, 0));
1238     }
1239     negptr(cnt2); // Jumped here with negative cnt2, convert to positive
1240 
1241     bind(RELOAD_STR);
1242     // We came here after the beginning of the substring was
1243     // matched but the rest of it was not so we need to search
1244     // again. Start from the next element after the previous match.
1245 
1246     // cnt2 is number of substring reminding elements and
1247     // cnt1 is number of string reminding elements when cmp failed.
1248     // Restored cnt1 = cnt1 - cnt2 + int_cnt2
1249     subl(cnt1, cnt2);
1250     addl(cnt1, int_cnt2);
1251     movl(cnt2, int_cnt2); // Now restore cnt2
1252 
1253     decrementl(cnt1);     // Shift to next element
1254     cmpl(cnt1, cnt2);
1255     jcc(Assembler::negative, RET_NOT_FOUND);  // Left less then substring
1256 
1257     addptr(result, (1&lt;&lt;scale1));
1258 
1259   } // (int_cnt2 &gt; 8)
1260 
1261   // Scan string for start of substr in 16-byte vectors
1262   bind(SCAN_TO_SUBSTR);
1263   pcmpestri(vec, Address(result, 0), mode);
1264   jccb(Assembler::below, FOUND_CANDIDATE);   // CF == 1
1265   subl(cnt1, stride);
1266   jccb(Assembler::lessEqual, RET_NOT_FOUND); // Scanned full string
1267   cmpl(cnt1, cnt2);
1268   jccb(Assembler::negative, RET_NOT_FOUND);  // Left less then substring
1269   addptr(result, 16);
1270   jmpb(SCAN_TO_SUBSTR);
1271 
1272   // Found a potential substr
1273   bind(FOUND_CANDIDATE);
1274   // Matched whole vector if first element matched (tmp(rcx) == 0).
1275   if (int_cnt2 == stride) {
1276     jccb(Assembler::overflow, RET_FOUND);    // OF == 1
1277   } else { // int_cnt2 &gt; 8
1278     jccb(Assembler::overflow, FOUND_SUBSTR);
1279   }
1280   // After pcmpestri tmp(rcx) contains matched element index
1281   // Compute start addr of substr
1282   lea(result, Address(result, tmp, scale1));
1283 
1284   // Make sure string is still long enough
1285   subl(cnt1, tmp);
1286   cmpl(cnt1, cnt2);
1287   if (int_cnt2 == stride) {
1288     jccb(Assembler::greaterEqual, SCAN_TO_SUBSTR);
1289   } else { // int_cnt2 &gt; 8
1290     jccb(Assembler::greaterEqual, MATCH_SUBSTR_HEAD);
1291   }
1292   // Left less then substring.
1293 
1294   bind(RET_NOT_FOUND);
1295   movl(result, -1);
1296   jmp(EXIT);
1297 
1298   if (int_cnt2 &gt; stride) {
1299     // This code is optimized for the case when whole substring
1300     // is matched if its head is matched.
1301     bind(MATCH_SUBSTR_HEAD);
1302     pcmpestri(vec, Address(result, 0), mode);
1303     // Reload only string if does not match
1304     jcc(Assembler::noOverflow, RELOAD_STR); // OF == 0
1305 
1306     Label CONT_SCAN_SUBSTR;
1307     // Compare the rest of substring (&gt; 8 chars).
1308     bind(FOUND_SUBSTR);
1309     // First 8 chars are already matched.
1310     negptr(cnt2);
1311     addptr(cnt2, stride);
1312 
1313     bind(SCAN_SUBSTR);
1314     subl(cnt1, stride);
1315     cmpl(cnt2, -stride); // Do not read beyond substring
1316     jccb(Assembler::lessEqual, CONT_SCAN_SUBSTR);
1317     // Back-up strings to avoid reading beyond substring:
1318     // cnt1 = cnt1 - cnt2 + 8
1319     addl(cnt1, cnt2); // cnt2 is negative
1320     addl(cnt1, stride);
1321     movl(cnt2, stride); negptr(cnt2);
1322     bind(CONT_SCAN_SUBSTR);
1323     if (int_cnt2 &lt; (int)G) {
1324       int tail_off1 = int_cnt2&lt;&lt;scale1;
1325       int tail_off2 = int_cnt2&lt;&lt;scale2;
1326       if (ae == StrIntrinsicNode::UL) {
1327         pmovzxbw(vec, Address(str2, cnt2, scale2, tail_off2));
1328       } else {
1329         movdqu(vec, Address(str2, cnt2, scale2, tail_off2));
1330       }
1331       pcmpestri(vec, Address(result, cnt2, scale1, tail_off1), mode);
1332     } else {
1333       // calculate index in register to avoid integer overflow (int_cnt2*2)
1334       movl(tmp, int_cnt2);
1335       addptr(tmp, cnt2);
1336       if (ae == StrIntrinsicNode::UL) {
1337         pmovzxbw(vec, Address(str2, tmp, scale2, 0));
1338       } else {
1339         movdqu(vec, Address(str2, tmp, scale2, 0));
1340       }
1341       pcmpestri(vec, Address(result, tmp, scale1, 0), mode);
1342     }
1343     // Need to reload strings pointers if not matched whole vector
1344     jcc(Assembler::noOverflow, RELOAD_SUBSTR); // OF == 0
1345     addptr(cnt2, stride);
1346     jcc(Assembler::negative, SCAN_SUBSTR);
1347     // Fall through if found full substring
1348 
1349   } // (int_cnt2 &gt; 8)
1350 
1351   bind(RET_FOUND);
1352   // Found result if we matched full small substring.
1353   // Compute substr offset
1354   subptr(result, str1);
1355   if (ae == StrIntrinsicNode::UU || ae == StrIntrinsicNode::UL) {
1356     shrl(result, 1); // index
1357   }
1358   bind(EXIT);
1359 
1360 } // string_indexofC8
1361 
1362 // Small strings are loaded through stack if they cross page boundary.
1363 void C2_MacroAssembler::string_indexof(Register str1, Register str2,
1364                                        Register cnt1, Register cnt2,
1365                                        int int_cnt2,  Register result,
1366                                        XMMRegister vec, Register tmp,
1367                                        int ae) {
1368   ShortBranchVerifier sbv(this);
1369   assert(UseSSE42Intrinsics, &quot;SSE4.2 intrinsics are required&quot;);
1370   assert(ae != StrIntrinsicNode::LU, &quot;Invalid encoding&quot;);
1371 
1372   //
1373   // int_cnt2 is length of small (&lt; 8 chars) constant substring
1374   // or (-1) for non constant substring in which case its length
1375   // is in cnt2 register.
1376   //
1377   // Note, inline_string_indexOf() generates checks:
1378   // if (substr.count &gt; string.count) return -1;
1379   // if (substr.count == 0) return 0;
1380   //
1381   int stride = (ae == StrIntrinsicNode::LL) ? 16 : 8; //UU, UL -&gt; 8
1382   assert(int_cnt2 == -1 || (0 &lt; int_cnt2 &amp;&amp; int_cnt2 &lt; stride), &quot;should be != 0&quot;);
1383   // This method uses the pcmpestri instruction with bound registers
1384   //   inputs:
1385   //     xmm - substring
1386   //     rax - substring length (elements count)
1387   //     mem - scanned string
1388   //     rdx - string length (elements count)
1389   //     0xd - mode: 1100 (substring search) + 01 (unsigned shorts)
1390   //     0xc - mode: 1100 (substring search) + 00 (unsigned bytes)
1391   //   outputs:
1392   //     rcx - matched index in string
1393   assert(cnt1 == rdx &amp;&amp; cnt2 == rax &amp;&amp; tmp == rcx, &quot;pcmpestri&quot;);
1394   int mode = (ae == StrIntrinsicNode::LL) ? 0x0c : 0x0d; // bytes or shorts
1395   Address::ScaleFactor scale1 = (ae == StrIntrinsicNode::LL) ? Address::times_1 : Address::times_2;
1396   Address::ScaleFactor scale2 = (ae == StrIntrinsicNode::UL) ? Address::times_1 : scale1;
1397 
1398   Label RELOAD_SUBSTR, SCAN_TO_SUBSTR, SCAN_SUBSTR, ADJUST_STR,
1399         RET_FOUND, RET_NOT_FOUND, CLEANUP, FOUND_SUBSTR,
1400         FOUND_CANDIDATE;
1401 
1402   { //========================================================
1403     // We don&#39;t know where these strings are located
1404     // and we can&#39;t read beyond them. Load them through stack.
1405     Label BIG_STRINGS, CHECK_STR, COPY_SUBSTR, COPY_STR;
1406 
1407     movptr(tmp, rsp); // save old SP
1408 
1409     if (int_cnt2 &gt; 0) {     // small (&lt; 8 chars) constant substring
1410       if (int_cnt2 == (1&gt;&gt;scale2)) { // One byte
1411         assert((ae == StrIntrinsicNode::LL || ae == StrIntrinsicNode::UL), &quot;Only possible for latin1 encoding&quot;);
1412         load_unsigned_byte(result, Address(str2, 0));
1413         movdl(vec, result); // move 32 bits
1414       } else if (ae == StrIntrinsicNode::LL &amp;&amp; int_cnt2 == 3) {  // Three bytes
1415         // Not enough header space in 32-bit VM: 12+3 = 15.
1416         movl(result, Address(str2, -1));
1417         shrl(result, 8);
1418         movdl(vec, result); // move 32 bits
1419       } else if (ae != StrIntrinsicNode::UL &amp;&amp; int_cnt2 == (2&gt;&gt;scale2)) {  // One char
1420         load_unsigned_short(result, Address(str2, 0));
1421         movdl(vec, result); // move 32 bits
1422       } else if (ae != StrIntrinsicNode::UL &amp;&amp; int_cnt2 == (4&gt;&gt;scale2)) { // Two chars
1423         movdl(vec, Address(str2, 0)); // move 32 bits
1424       } else if (ae != StrIntrinsicNode::UL &amp;&amp; int_cnt2 == (8&gt;&gt;scale2)) { // Four chars
1425         movq(vec, Address(str2, 0));  // move 64 bits
1426       } else { // cnt2 = { 3, 5, 6, 7 } || (ae == StrIntrinsicNode::UL &amp;&amp; cnt2 ={2, ..., 7})
1427         // Array header size is 12 bytes in 32-bit VM
1428         // + 6 bytes for 3 chars == 18 bytes,
1429         // enough space to load vec and shift.
1430         assert(HeapWordSize*TypeArrayKlass::header_size() &gt;= 12,&quot;sanity&quot;);
1431         if (ae == StrIntrinsicNode::UL) {
1432           int tail_off = int_cnt2-8;
1433           pmovzxbw(vec, Address(str2, tail_off));
1434           psrldq(vec, -2*tail_off);
1435         }
1436         else {
1437           int tail_off = int_cnt2*(1&lt;&lt;scale2);
1438           movdqu(vec, Address(str2, tail_off-16));
1439           psrldq(vec, 16-tail_off);
1440         }
1441       }
1442     } else { // not constant substring
1443       cmpl(cnt2, stride);
1444       jccb(Assembler::aboveEqual, BIG_STRINGS); // Both strings are big enough
1445 
1446       // We can read beyond string if srt+16 does not cross page boundary
1447       // since heaps are aligned and mapped by pages.
1448       assert(os::vm_page_size() &lt; (int)G, &quot;default page should be small&quot;);
1449       movl(result, str2); // We need only low 32 bits
1450       andl(result, (os::vm_page_size()-1));
1451       cmpl(result, (os::vm_page_size()-16));
1452       jccb(Assembler::belowEqual, CHECK_STR);
1453 
1454       // Move small strings to stack to allow load 16 bytes into vec.
1455       subptr(rsp, 16);
1456       int stk_offset = wordSize-(1&lt;&lt;scale2);
1457       push(cnt2);
1458 
1459       bind(COPY_SUBSTR);
1460       if (ae == StrIntrinsicNode::LL || ae == StrIntrinsicNode::UL) {
1461         load_unsigned_byte(result, Address(str2, cnt2, scale2, -1));
1462         movb(Address(rsp, cnt2, scale2, stk_offset), result);
1463       } else if (ae == StrIntrinsicNode::UU) {
1464         load_unsigned_short(result, Address(str2, cnt2, scale2, -2));
1465         movw(Address(rsp, cnt2, scale2, stk_offset), result);
1466       }
1467       decrement(cnt2);
1468       jccb(Assembler::notZero, COPY_SUBSTR);
1469 
1470       pop(cnt2);
1471       movptr(str2, rsp);  // New substring address
1472     } // non constant
1473 
1474     bind(CHECK_STR);
1475     cmpl(cnt1, stride);
1476     jccb(Assembler::aboveEqual, BIG_STRINGS);
1477 
1478     // Check cross page boundary.
1479     movl(result, str1); // We need only low 32 bits
1480     andl(result, (os::vm_page_size()-1));
1481     cmpl(result, (os::vm_page_size()-16));
1482     jccb(Assembler::belowEqual, BIG_STRINGS);
1483 
1484     subptr(rsp, 16);
1485     int stk_offset = -(1&lt;&lt;scale1);
1486     if (int_cnt2 &lt; 0) { // not constant
1487       push(cnt2);
1488       stk_offset += wordSize;
1489     }
1490     movl(cnt2, cnt1);
1491 
1492     bind(COPY_STR);
1493     if (ae == StrIntrinsicNode::LL) {
1494       load_unsigned_byte(result, Address(str1, cnt2, scale1, -1));
1495       movb(Address(rsp, cnt2, scale1, stk_offset), result);
1496     } else {
1497       load_unsigned_short(result, Address(str1, cnt2, scale1, -2));
1498       movw(Address(rsp, cnt2, scale1, stk_offset), result);
1499     }
1500     decrement(cnt2);
1501     jccb(Assembler::notZero, COPY_STR);
1502 
1503     if (int_cnt2 &lt; 0) { // not constant
1504       pop(cnt2);
1505     }
1506     movptr(str1, rsp);  // New string address
1507 
1508     bind(BIG_STRINGS);
1509     // Load substring.
1510     if (int_cnt2 &lt; 0) { // -1
1511       if (ae == StrIntrinsicNode::UL) {
1512         pmovzxbw(vec, Address(str2, 0));
1513       } else {
1514         movdqu(vec, Address(str2, 0));
1515       }
1516       push(cnt2);       // substr count
1517       push(str2);       // substr addr
1518       push(str1);       // string addr
1519     } else {
1520       // Small (&lt; 8 chars) constant substrings are loaded already.
1521       movl(cnt2, int_cnt2);
1522     }
1523     push(tmp);  // original SP
1524 
1525   } // Finished loading
1526 
1527   //========================================================
1528   // Start search
1529   //
1530 
1531   movptr(result, str1); // string addr
1532 
1533   if (int_cnt2  &lt; 0) {  // Only for non constant substring
1534     jmpb(SCAN_TO_SUBSTR);
1535 
1536     // SP saved at sp+0
1537     // String saved at sp+1*wordSize
1538     // Substr saved at sp+2*wordSize
1539     // Substr count saved at sp+3*wordSize
1540 
1541     // Reload substr for rescan, this code
1542     // is executed only for large substrings (&gt; 8 chars)
1543     bind(RELOAD_SUBSTR);
1544     movptr(str2, Address(rsp, 2*wordSize));
1545     movl(cnt2, Address(rsp, 3*wordSize));
1546     if (ae == StrIntrinsicNode::UL) {
1547       pmovzxbw(vec, Address(str2, 0));
1548     } else {
1549       movdqu(vec, Address(str2, 0));
1550     }
1551     // We came here after the beginning of the substring was
1552     // matched but the rest of it was not so we need to search
1553     // again. Start from the next element after the previous match.
1554     subptr(str1, result); // Restore counter
1555     if (ae == StrIntrinsicNode::UU || ae == StrIntrinsicNode::UL) {
1556       shrl(str1, 1);
1557     }
1558     addl(cnt1, str1);
1559     decrementl(cnt1);   // Shift to next element
1560     cmpl(cnt1, cnt2);
1561     jcc(Assembler::negative, RET_NOT_FOUND);  // Left less then substring
1562 
1563     addptr(result, (1&lt;&lt;scale1));
1564   } // non constant
1565 
1566   // Scan string for start of substr in 16-byte vectors
1567   bind(SCAN_TO_SUBSTR);
1568   assert(cnt1 == rdx &amp;&amp; cnt2 == rax &amp;&amp; tmp == rcx, &quot;pcmpestri&quot;);
1569   pcmpestri(vec, Address(result, 0), mode);
1570   jccb(Assembler::below, FOUND_CANDIDATE);   // CF == 1
1571   subl(cnt1, stride);
1572   jccb(Assembler::lessEqual, RET_NOT_FOUND); // Scanned full string
1573   cmpl(cnt1, cnt2);
1574   jccb(Assembler::negative, RET_NOT_FOUND);  // Left less then substring
1575   addptr(result, 16);
1576 
1577   bind(ADJUST_STR);
1578   cmpl(cnt1, stride); // Do not read beyond string
1579   jccb(Assembler::greaterEqual, SCAN_TO_SUBSTR);
1580   // Back-up string to avoid reading beyond string.
1581   lea(result, Address(result, cnt1, scale1, -16));
1582   movl(cnt1, stride);
1583   jmpb(SCAN_TO_SUBSTR);
1584 
1585   // Found a potential substr
1586   bind(FOUND_CANDIDATE);
1587   // After pcmpestri tmp(rcx) contains matched element index
1588 
1589   // Make sure string is still long enough
1590   subl(cnt1, tmp);
1591   cmpl(cnt1, cnt2);
1592   jccb(Assembler::greaterEqual, FOUND_SUBSTR);
1593   // Left less then substring.
1594 
1595   bind(RET_NOT_FOUND);
1596   movl(result, -1);
1597   jmp(CLEANUP);
1598 
1599   bind(FOUND_SUBSTR);
1600   // Compute start addr of substr
1601   lea(result, Address(result, tmp, scale1));
1602   if (int_cnt2 &gt; 0) { // Constant substring
1603     // Repeat search for small substring (&lt; 8 chars)
1604     // from new point without reloading substring.
1605     // Have to check that we don&#39;t read beyond string.
1606     cmpl(tmp, stride-int_cnt2);
1607     jccb(Assembler::greater, ADJUST_STR);
1608     // Fall through if matched whole substring.
1609   } else { // non constant
1610     assert(int_cnt2 == -1, &quot;should be != 0&quot;);
1611 
1612     addl(tmp, cnt2);
1613     // Found result if we matched whole substring.
1614     cmpl(tmp, stride);
1615     jcc(Assembler::lessEqual, RET_FOUND);
1616 
1617     // Repeat search for small substring (&lt;= 8 chars)
1618     // from new point &#39;str1&#39; without reloading substring.
1619     cmpl(cnt2, stride);
1620     // Have to check that we don&#39;t read beyond string.
1621     jccb(Assembler::lessEqual, ADJUST_STR);
1622 
1623     Label CHECK_NEXT, CONT_SCAN_SUBSTR, RET_FOUND_LONG;
1624     // Compare the rest of substring (&gt; 8 chars).
1625     movptr(str1, result);
1626 
1627     cmpl(tmp, cnt2);
1628     // First 8 chars are already matched.
1629     jccb(Assembler::equal, CHECK_NEXT);
1630 
1631     bind(SCAN_SUBSTR);
1632     pcmpestri(vec, Address(str1, 0), mode);
1633     // Need to reload strings pointers if not matched whole vector
1634     jcc(Assembler::noOverflow, RELOAD_SUBSTR); // OF == 0
1635 
1636     bind(CHECK_NEXT);
1637     subl(cnt2, stride);
1638     jccb(Assembler::lessEqual, RET_FOUND_LONG); // Found full substring
1639     addptr(str1, 16);
1640     if (ae == StrIntrinsicNode::UL) {
1641       addptr(str2, 8);
1642     } else {
1643       addptr(str2, 16);
1644     }
1645     subl(cnt1, stride);
1646     cmpl(cnt2, stride); // Do not read beyond substring
1647     jccb(Assembler::greaterEqual, CONT_SCAN_SUBSTR);
1648     // Back-up strings to avoid reading beyond substring.
1649 
1650     if (ae == StrIntrinsicNode::UL) {
1651       lea(str2, Address(str2, cnt2, scale2, -8));
1652       lea(str1, Address(str1, cnt2, scale1, -16));
1653     } else {
1654       lea(str2, Address(str2, cnt2, scale2, -16));
1655       lea(str1, Address(str1, cnt2, scale1, -16));
1656     }
1657     subl(cnt1, cnt2);
1658     movl(cnt2, stride);
1659     addl(cnt1, stride);
1660     bind(CONT_SCAN_SUBSTR);
1661     if (ae == StrIntrinsicNode::UL) {
1662       pmovzxbw(vec, Address(str2, 0));
1663     } else {
1664       movdqu(vec, Address(str2, 0));
1665     }
1666     jmp(SCAN_SUBSTR);
1667 
1668     bind(RET_FOUND_LONG);
1669     movptr(str1, Address(rsp, wordSize));
1670   } // non constant
1671 
1672   bind(RET_FOUND);
1673   // Compute substr offset
1674   subptr(result, str1);
1675   if (ae == StrIntrinsicNode::UU || ae == StrIntrinsicNode::UL) {
1676     shrl(result, 1); // index
1677   }
1678   bind(CLEANUP);
1679   pop(rsp); // restore SP
1680 
1681 } // string_indexof
1682 
1683 void C2_MacroAssembler::string_indexof_char(Register str1, Register cnt1, Register ch, Register result,
1684                                             XMMRegister vec1, XMMRegister vec2, XMMRegister vec3, Register tmp) {
1685   ShortBranchVerifier sbv(this);
1686   assert(UseSSE42Intrinsics, &quot;SSE4.2 intrinsics are required&quot;);
1687 
1688   int stride = 8;
1689 
1690   Label FOUND_CHAR, SCAN_TO_CHAR, SCAN_TO_CHAR_LOOP,
1691         SCAN_TO_8_CHAR, SCAN_TO_8_CHAR_LOOP, SCAN_TO_16_CHAR_LOOP,
1692         RET_NOT_FOUND, SCAN_TO_8_CHAR_INIT,
1693         FOUND_SEQ_CHAR, DONE_LABEL;
1694 
1695   movptr(result, str1);
1696   if (UseAVX &gt;= 2) {
1697     cmpl(cnt1, stride);
1698     jcc(Assembler::less, SCAN_TO_CHAR);
1699     cmpl(cnt1, 2*stride);
1700     jcc(Assembler::less, SCAN_TO_8_CHAR_INIT);
1701     movdl(vec1, ch);
1702     vpbroadcastw(vec1, vec1, Assembler::AVX_256bit);
1703     vpxor(vec2, vec2);
1704     movl(tmp, cnt1);
1705     andl(tmp, 0xFFFFFFF0);  //vector count (in chars)
1706     andl(cnt1,0x0000000F);  //tail count (in chars)
1707 
1708     bind(SCAN_TO_16_CHAR_LOOP);
1709     vmovdqu(vec3, Address(result, 0));
1710     vpcmpeqw(vec3, vec3, vec1, 1);
1711     vptest(vec2, vec3);
1712     jcc(Assembler::carryClear, FOUND_CHAR);
1713     addptr(result, 32);
1714     subl(tmp, 2*stride);
1715     jcc(Assembler::notZero, SCAN_TO_16_CHAR_LOOP);
1716     jmp(SCAN_TO_8_CHAR);
1717     bind(SCAN_TO_8_CHAR_INIT);
1718     movdl(vec1, ch);
1719     pshuflw(vec1, vec1, 0x00);
1720     pshufd(vec1, vec1, 0);
1721     pxor(vec2, vec2);
1722   }
1723   bind(SCAN_TO_8_CHAR);
1724   cmpl(cnt1, stride);
1725   jcc(Assembler::less, SCAN_TO_CHAR);
1726   if (UseAVX &lt; 2) {
1727     movdl(vec1, ch);
1728     pshuflw(vec1, vec1, 0x00);
1729     pshufd(vec1, vec1, 0);
1730     pxor(vec2, vec2);
1731   }
1732   movl(tmp, cnt1);
1733   andl(tmp, 0xFFFFFFF8);  //vector count (in chars)
1734   andl(cnt1,0x00000007);  //tail count (in chars)
1735 
1736   bind(SCAN_TO_8_CHAR_LOOP);
1737   movdqu(vec3, Address(result, 0));
1738   pcmpeqw(vec3, vec1);
1739   ptest(vec2, vec3);
1740   jcc(Assembler::carryClear, FOUND_CHAR);
1741   addptr(result, 16);
1742   subl(tmp, stride);
1743   jcc(Assembler::notZero, SCAN_TO_8_CHAR_LOOP);
1744   bind(SCAN_TO_CHAR);
1745   testl(cnt1, cnt1);
1746   jcc(Assembler::zero, RET_NOT_FOUND);
1747   bind(SCAN_TO_CHAR_LOOP);
1748   load_unsigned_short(tmp, Address(result, 0));
1749   cmpl(ch, tmp);
1750   jccb(Assembler::equal, FOUND_SEQ_CHAR);
1751   addptr(result, 2);
1752   subl(cnt1, 1);
1753   jccb(Assembler::zero, RET_NOT_FOUND);
1754   jmp(SCAN_TO_CHAR_LOOP);
1755 
1756   bind(RET_NOT_FOUND);
1757   movl(result, -1);
1758   jmpb(DONE_LABEL);
1759 
1760   bind(FOUND_CHAR);
1761   if (UseAVX &gt;= 2) {
1762     vpmovmskb(tmp, vec3);
1763   } else {
1764     pmovmskb(tmp, vec3);
1765   }
1766   bsfl(ch, tmp);
1767   addl(result, ch);
1768 
1769   bind(FOUND_SEQ_CHAR);
1770   subptr(result, str1);
1771   shrl(result, 1);
1772 
1773   bind(DONE_LABEL);
1774 } // string_indexof_char
1775 
1776 // helper function for string_compare
1777 void C2_MacroAssembler::load_next_elements(Register elem1, Register elem2, Register str1, Register str2,
1778                                            Address::ScaleFactor scale, Address::ScaleFactor scale1,
1779                                            Address::ScaleFactor scale2, Register index, int ae) {
1780   if (ae == StrIntrinsicNode::LL) {
1781     load_unsigned_byte(elem1, Address(str1, index, scale, 0));
1782     load_unsigned_byte(elem2, Address(str2, index, scale, 0));
1783   } else if (ae == StrIntrinsicNode::UU) {
1784     load_unsigned_short(elem1, Address(str1, index, scale, 0));
1785     load_unsigned_short(elem2, Address(str2, index, scale, 0));
1786   } else {
1787     load_unsigned_byte(elem1, Address(str1, index, scale1, 0));
1788     load_unsigned_short(elem2, Address(str2, index, scale2, 0));
1789   }
1790 }
1791 
1792 // Compare strings, used for char[] and byte[].
1793 void C2_MacroAssembler::string_compare(Register str1, Register str2,
1794                                        Register cnt1, Register cnt2, Register result,
1795                                        XMMRegister vec1, int ae) {
1796   ShortBranchVerifier sbv(this);
1797   Label LENGTH_DIFF_LABEL, POP_LABEL, DONE_LABEL, WHILE_HEAD_LABEL;
1798   Label COMPARE_WIDE_VECTORS_LOOP_FAILED;  // used only _LP64 &amp;&amp; AVX3
1799   int stride, stride2, adr_stride, adr_stride1, adr_stride2;
1800   int stride2x2 = 0x40;
1801   Address::ScaleFactor scale = Address::no_scale;
1802   Address::ScaleFactor scale1 = Address::no_scale;
1803   Address::ScaleFactor scale2 = Address::no_scale;
1804 
1805   if (ae != StrIntrinsicNode::LL) {
1806     stride2x2 = 0x20;
1807   }
1808 
1809   if (ae == StrIntrinsicNode::LU || ae == StrIntrinsicNode::UL) {
1810     shrl(cnt2, 1);
1811   }
1812   // Compute the minimum of the string lengths and the
1813   // difference of the string lengths (stack).
1814   // Do the conditional move stuff
1815   movl(result, cnt1);
1816   subl(cnt1, cnt2);
1817   push(cnt1);
1818   cmov32(Assembler::lessEqual, cnt2, result);    // cnt2 = min(cnt1, cnt2)
1819 
1820   // Is the minimum length zero?
1821   testl(cnt2, cnt2);
1822   jcc(Assembler::zero, LENGTH_DIFF_LABEL);
1823   if (ae == StrIntrinsicNode::LL) {
1824     // Load first bytes
1825     load_unsigned_byte(result, Address(str1, 0));  // result = str1[0]
1826     load_unsigned_byte(cnt1, Address(str2, 0));    // cnt1   = str2[0]
1827   } else if (ae == StrIntrinsicNode::UU) {
1828     // Load first characters
1829     load_unsigned_short(result, Address(str1, 0));
1830     load_unsigned_short(cnt1, Address(str2, 0));
1831   } else {
1832     load_unsigned_byte(result, Address(str1, 0));
1833     load_unsigned_short(cnt1, Address(str2, 0));
1834   }
1835   subl(result, cnt1);
1836   jcc(Assembler::notZero,  POP_LABEL);
1837 
1838   if (ae == StrIntrinsicNode::UU) {
1839     // Divide length by 2 to get number of chars
1840     shrl(cnt2, 1);
1841   }
1842   cmpl(cnt2, 1);
1843   jcc(Assembler::equal, LENGTH_DIFF_LABEL);
1844 
1845   // Check if the strings start at the same location and setup scale and stride
1846   if (ae == StrIntrinsicNode::LL || ae == StrIntrinsicNode::UU) {
1847     cmpptr(str1, str2);
1848     jcc(Assembler::equal, LENGTH_DIFF_LABEL);
1849     if (ae == StrIntrinsicNode::LL) {
1850       scale = Address::times_1;
1851       stride = 16;
1852     } else {
1853       scale = Address::times_2;
1854       stride = 8;
1855     }
1856   } else {
1857     scale1 = Address::times_1;
1858     scale2 = Address::times_2;
1859     // scale not used
1860     stride = 8;
1861   }
1862 
1863   if (UseAVX &gt;= 2 &amp;&amp; UseSSE42Intrinsics) {
1864     Label COMPARE_WIDE_VECTORS, VECTOR_NOT_EQUAL, COMPARE_WIDE_TAIL, COMPARE_SMALL_STR;
1865     Label COMPARE_WIDE_VECTORS_LOOP, COMPARE_16_CHARS, COMPARE_INDEX_CHAR;
1866     Label COMPARE_WIDE_VECTORS_LOOP_AVX2;
1867     Label COMPARE_TAIL_LONG;
1868     Label COMPARE_WIDE_VECTORS_LOOP_AVX3;  // used only _LP64 &amp;&amp; AVX3
1869 
1870     int pcmpmask = 0x19;
1871     if (ae == StrIntrinsicNode::LL) {
1872       pcmpmask &amp;= ~0x01;
1873     }
1874 
1875     // Setup to compare 16-chars (32-bytes) vectors,
1876     // start from first character again because it has aligned address.
1877     if (ae == StrIntrinsicNode::LL) {
1878       stride2 = 32;
1879     } else {
1880       stride2 = 16;
1881     }
1882     if (ae == StrIntrinsicNode::LL || ae == StrIntrinsicNode::UU) {
1883       adr_stride = stride &lt;&lt; scale;
1884     } else {
1885       adr_stride1 = 8;  //stride &lt;&lt; scale1;
1886       adr_stride2 = 16; //stride &lt;&lt; scale2;
1887     }
1888 
1889     assert(result == rax &amp;&amp; cnt2 == rdx &amp;&amp; cnt1 == rcx, &quot;pcmpestri&quot;);
1890     // rax and rdx are used by pcmpestri as elements counters
1891     movl(result, cnt2);
1892     andl(cnt2, ~(stride2-1));   // cnt2 holds the vector count
1893     jcc(Assembler::zero, COMPARE_TAIL_LONG);
1894 
1895     // fast path : compare first 2 8-char vectors.
1896     bind(COMPARE_16_CHARS);
1897     if (ae == StrIntrinsicNode::LL || ae == StrIntrinsicNode::UU) {
1898       movdqu(vec1, Address(str1, 0));
1899     } else {
1900       pmovzxbw(vec1, Address(str1, 0));
1901     }
1902     pcmpestri(vec1, Address(str2, 0), pcmpmask);
1903     jccb(Assembler::below, COMPARE_INDEX_CHAR);
1904 
1905     if (ae == StrIntrinsicNode::LL || ae == StrIntrinsicNode::UU) {
1906       movdqu(vec1, Address(str1, adr_stride));
1907       pcmpestri(vec1, Address(str2, adr_stride), pcmpmask);
1908     } else {
1909       pmovzxbw(vec1, Address(str1, adr_stride1));
1910       pcmpestri(vec1, Address(str2, adr_stride2), pcmpmask);
1911     }
1912     jccb(Assembler::aboveEqual, COMPARE_WIDE_VECTORS);
1913     addl(cnt1, stride);
1914 
1915     // Compare the characters at index in cnt1
1916     bind(COMPARE_INDEX_CHAR); // cnt1 has the offset of the mismatching character
1917     load_next_elements(result, cnt2, str1, str2, scale, scale1, scale2, cnt1, ae);
1918     subl(result, cnt2);
1919     jmp(POP_LABEL);
1920 
1921     // Setup the registers to start vector comparison loop
1922     bind(COMPARE_WIDE_VECTORS);
1923     if (ae == StrIntrinsicNode::LL || ae == StrIntrinsicNode::UU) {
1924       lea(str1, Address(str1, result, scale));
1925       lea(str2, Address(str2, result, scale));
1926     } else {
1927       lea(str1, Address(str1, result, scale1));
1928       lea(str2, Address(str2, result, scale2));
1929     }
1930     subl(result, stride2);
1931     subl(cnt2, stride2);
1932     jcc(Assembler::zero, COMPARE_WIDE_TAIL);
1933     negptr(result);
1934 
1935     //  In a loop, compare 16-chars (32-bytes) at once using (vpxor+vptest)
1936     bind(COMPARE_WIDE_VECTORS_LOOP);
1937 
1938 #ifdef _LP64
1939     if ((AVX3Threshold == 0) &amp;&amp; VM_Version::supports_avx512vlbw()) { // trying 64 bytes fast loop
1940       cmpl(cnt2, stride2x2);
1941       jccb(Assembler::below, COMPARE_WIDE_VECTORS_LOOP_AVX2);
1942       testl(cnt2, stride2x2-1);   // cnt2 holds the vector count
1943       jccb(Assembler::notZero, COMPARE_WIDE_VECTORS_LOOP_AVX2);   // means we cannot subtract by 0x40
1944 
1945       bind(COMPARE_WIDE_VECTORS_LOOP_AVX3); // the hottest loop
1946       if (ae == StrIntrinsicNode::LL || ae == StrIntrinsicNode::UU) {
1947         evmovdquq(vec1, Address(str1, result, scale), Assembler::AVX_512bit);
1948         evpcmpeqb(k7, vec1, Address(str2, result, scale), Assembler::AVX_512bit); // k7 == 11..11, if operands equal, otherwise k7 has some 0
1949       } else {
1950         vpmovzxbw(vec1, Address(str1, result, scale1), Assembler::AVX_512bit);
1951         evpcmpeqb(k7, vec1, Address(str2, result, scale2), Assembler::AVX_512bit); // k7 == 11..11, if operands equal, otherwise k7 has some 0
1952       }
1953       kortestql(k7, k7);
1954       jcc(Assembler::aboveEqual, COMPARE_WIDE_VECTORS_LOOP_FAILED);     // miscompare
1955       addptr(result, stride2x2);  // update since we already compared at this addr
1956       subl(cnt2, stride2x2);      // and sub the size too
1957       jccb(Assembler::notZero, COMPARE_WIDE_VECTORS_LOOP_AVX3);
1958 
1959       vpxor(vec1, vec1);
1960       jmpb(COMPARE_WIDE_TAIL);
1961     }//if (VM_Version::supports_avx512vlbw())
1962 #endif // _LP64
1963 
1964 
1965     bind(COMPARE_WIDE_VECTORS_LOOP_AVX2);
1966     if (ae == StrIntrinsicNode::LL || ae == StrIntrinsicNode::UU) {
1967       vmovdqu(vec1, Address(str1, result, scale));
1968       vpxor(vec1, Address(str2, result, scale));
1969     } else {
1970       vpmovzxbw(vec1, Address(str1, result, scale1), Assembler::AVX_256bit);
1971       vpxor(vec1, Address(str2, result, scale2));
1972     }
1973     vptest(vec1, vec1);
1974     jcc(Assembler::notZero, VECTOR_NOT_EQUAL);
1975     addptr(result, stride2);
1976     subl(cnt2, stride2);
1977     jcc(Assembler::notZero, COMPARE_WIDE_VECTORS_LOOP);
1978     // clean upper bits of YMM registers
1979     vpxor(vec1, vec1);
1980 
1981     // compare wide vectors tail
1982     bind(COMPARE_WIDE_TAIL);
1983     testptr(result, result);
1984     jcc(Assembler::zero, LENGTH_DIFF_LABEL);
1985 
1986     movl(result, stride2);
1987     movl(cnt2, result);
1988     negptr(result);
1989     jmp(COMPARE_WIDE_VECTORS_LOOP_AVX2);
1990 
1991     // Identifies the mismatching (higher or lower)16-bytes in the 32-byte vectors.
1992     bind(VECTOR_NOT_EQUAL);
1993     // clean upper bits of YMM registers
1994     vpxor(vec1, vec1);
1995     if (ae == StrIntrinsicNode::LL || ae == StrIntrinsicNode::UU) {
1996       lea(str1, Address(str1, result, scale));
1997       lea(str2, Address(str2, result, scale));
1998     } else {
1999       lea(str1, Address(str1, result, scale1));
2000       lea(str2, Address(str2, result, scale2));
2001     }
2002     jmp(COMPARE_16_CHARS);
2003 
2004     // Compare tail chars, length between 1 to 15 chars
2005     bind(COMPARE_TAIL_LONG);
2006     movl(cnt2, result);
2007     cmpl(cnt2, stride);
2008     jcc(Assembler::less, COMPARE_SMALL_STR);
2009 
2010     if (ae == StrIntrinsicNode::LL || ae == StrIntrinsicNode::UU) {
2011       movdqu(vec1, Address(str1, 0));
2012     } else {
2013       pmovzxbw(vec1, Address(str1, 0));
2014     }
2015     pcmpestri(vec1, Address(str2, 0), pcmpmask);
2016     jcc(Assembler::below, COMPARE_INDEX_CHAR);
2017     subptr(cnt2, stride);
2018     jcc(Assembler::zero, LENGTH_DIFF_LABEL);
2019     if (ae == StrIntrinsicNode::LL || ae == StrIntrinsicNode::UU) {
2020       lea(str1, Address(str1, result, scale));
2021       lea(str2, Address(str2, result, scale));
2022     } else {
2023       lea(str1, Address(str1, result, scale1));
2024       lea(str2, Address(str2, result, scale2));
2025     }
2026     negptr(cnt2);
2027     jmpb(WHILE_HEAD_LABEL);
2028 
2029     bind(COMPARE_SMALL_STR);
2030   } else if (UseSSE42Intrinsics) {
2031     Label COMPARE_WIDE_VECTORS, VECTOR_NOT_EQUAL, COMPARE_TAIL;
2032     int pcmpmask = 0x19;
2033     // Setup to compare 8-char (16-byte) vectors,
2034     // start from first character again because it has aligned address.
2035     movl(result, cnt2);
2036     andl(cnt2, ~(stride - 1));   // cnt2 holds the vector count
2037     if (ae == StrIntrinsicNode::LL) {
2038       pcmpmask &amp;= ~0x01;
2039     }
2040     jcc(Assembler::zero, COMPARE_TAIL);
2041     if (ae == StrIntrinsicNode::LL || ae == StrIntrinsicNode::UU) {
2042       lea(str1, Address(str1, result, scale));
2043       lea(str2, Address(str2, result, scale));
2044     } else {
2045       lea(str1, Address(str1, result, scale1));
2046       lea(str2, Address(str2, result, scale2));
2047     }
2048     negptr(result);
2049 
2050     // pcmpestri
2051     //   inputs:
2052     //     vec1- substring
2053     //     rax - negative string length (elements count)
2054     //     mem - scanned string
2055     //     rdx - string length (elements count)
2056     //     pcmpmask - cmp mode: 11000 (string compare with negated result)
2057     //               + 00 (unsigned bytes) or  + 01 (unsigned shorts)
2058     //   outputs:
2059     //     rcx - first mismatched element index
2060     assert(result == rax &amp;&amp; cnt2 == rdx &amp;&amp; cnt1 == rcx, &quot;pcmpestri&quot;);
2061 
2062     bind(COMPARE_WIDE_VECTORS);
2063     if (ae == StrIntrinsicNode::LL || ae == StrIntrinsicNode::UU) {
2064       movdqu(vec1, Address(str1, result, scale));
2065       pcmpestri(vec1, Address(str2, result, scale), pcmpmask);
2066     } else {
2067       pmovzxbw(vec1, Address(str1, result, scale1));
2068       pcmpestri(vec1, Address(str2, result, scale2), pcmpmask);
2069     }
2070     // After pcmpestri cnt1(rcx) contains mismatched element index
2071 
2072     jccb(Assembler::below, VECTOR_NOT_EQUAL);  // CF==1
2073     addptr(result, stride);
2074     subptr(cnt2, stride);
2075     jccb(Assembler::notZero, COMPARE_WIDE_VECTORS);
2076 
2077     // compare wide vectors tail
2078     testptr(result, result);
2079     jcc(Assembler::zero, LENGTH_DIFF_LABEL);
2080 
2081     movl(cnt2, stride);
2082     movl(result, stride);
2083     negptr(result);
2084     if (ae == StrIntrinsicNode::LL || ae == StrIntrinsicNode::UU) {
2085       movdqu(vec1, Address(str1, result, scale));
2086       pcmpestri(vec1, Address(str2, result, scale), pcmpmask);
2087     } else {
2088       pmovzxbw(vec1, Address(str1, result, scale1));
2089       pcmpestri(vec1, Address(str2, result, scale2), pcmpmask);
2090     }
2091     jccb(Assembler::aboveEqual, LENGTH_DIFF_LABEL);
2092 
2093     // Mismatched characters in the vectors
2094     bind(VECTOR_NOT_EQUAL);
2095     addptr(cnt1, result);
2096     load_next_elements(result, cnt2, str1, str2, scale, scale1, scale2, cnt1, ae);
2097     subl(result, cnt2);
2098     jmpb(POP_LABEL);
2099 
2100     bind(COMPARE_TAIL); // limit is zero
2101     movl(cnt2, result);
2102     // Fallthru to tail compare
2103   }
2104   // Shift str2 and str1 to the end of the arrays, negate min
2105   if (ae == StrIntrinsicNode::LL || ae == StrIntrinsicNode::UU) {
2106     lea(str1, Address(str1, cnt2, scale));
2107     lea(str2, Address(str2, cnt2, scale));
2108   } else {
2109     lea(str1, Address(str1, cnt2, scale1));
2110     lea(str2, Address(str2, cnt2, scale2));
2111   }
2112   decrementl(cnt2);  // first character was compared already
2113   negptr(cnt2);
2114 
2115   // Compare the rest of the elements
2116   bind(WHILE_HEAD_LABEL);
2117   load_next_elements(result, cnt1, str1, str2, scale, scale1, scale2, cnt2, ae);
2118   subl(result, cnt1);
2119   jccb(Assembler::notZero, POP_LABEL);
2120   increment(cnt2);
2121   jccb(Assembler::notZero, WHILE_HEAD_LABEL);
2122 
2123   // Strings are equal up to min length.  Return the length difference.
2124   bind(LENGTH_DIFF_LABEL);
2125   pop(result);
2126   if (ae == StrIntrinsicNode::UU) {
2127     // Divide diff by 2 to get number of chars
2128     sarl(result, 1);
2129   }
2130   jmpb(DONE_LABEL);
2131 
2132 #ifdef _LP64
2133   if (VM_Version::supports_avx512vlbw()) {
2134 
2135     bind(COMPARE_WIDE_VECTORS_LOOP_FAILED);
2136 
2137     kmovql(cnt1, k7);
2138     notq(cnt1);
2139     bsfq(cnt2, cnt1);
2140     if (ae != StrIntrinsicNode::LL) {
2141       // Divide diff by 2 to get number of chars
2142       sarl(cnt2, 1);
2143     }
2144     addq(result, cnt2);
2145     if (ae == StrIntrinsicNode::LL) {
2146       load_unsigned_byte(cnt1, Address(str2, result));
2147       load_unsigned_byte(result, Address(str1, result));
2148     } else if (ae == StrIntrinsicNode::UU) {
2149       load_unsigned_short(cnt1, Address(str2, result, scale));
2150       load_unsigned_short(result, Address(str1, result, scale));
2151     } else {
2152       load_unsigned_short(cnt1, Address(str2, result, scale2));
2153       load_unsigned_byte(result, Address(str1, result, scale1));
2154     }
2155     subl(result, cnt1);
2156     jmpb(POP_LABEL);
2157   }//if (VM_Version::supports_avx512vlbw())
2158 #endif // _LP64
2159 
2160   // Discard the stored length difference
2161   bind(POP_LABEL);
2162   pop(cnt1);
2163 
2164   // That&#39;s it
2165   bind(DONE_LABEL);
2166   if(ae == StrIntrinsicNode::UL) {
2167     negl(result);
2168   }
2169 
2170 }
2171 
2172 // Search for Non-ASCII character (Negative byte value) in a byte array,
2173 // return true if it has any and false otherwise.
2174 //   ..\jdk\src\java.base\share\classes\java\lang\StringCoding.java
2175 //   @HotSpotIntrinsicCandidate
2176 //   private static boolean hasNegatives(byte[] ba, int off, int len) {
2177 //     for (int i = off; i &lt; off + len; i++) {
2178 //       if (ba[i] &lt; 0) {
2179 //         return true;
2180 //       }
2181 //     }
2182 //     return false;
2183 //   }
2184 void C2_MacroAssembler::has_negatives(Register ary1, Register len,
2185   Register result, Register tmp1,
2186   XMMRegister vec1, XMMRegister vec2) {
2187   // rsi: byte array
2188   // rcx: len
2189   // rax: result
2190   ShortBranchVerifier sbv(this);
2191   assert_different_registers(ary1, len, result, tmp1);
2192   assert_different_registers(vec1, vec2);
2193   Label TRUE_LABEL, FALSE_LABEL, DONE, COMPARE_CHAR, COMPARE_VECTORS, COMPARE_BYTE;
2194 
2195   // len == 0
2196   testl(len, len);
2197   jcc(Assembler::zero, FALSE_LABEL);
2198 
2199   if ((AVX3Threshold == 0) &amp;&amp; (UseAVX &gt; 2) &amp;&amp; // AVX512
2200     VM_Version::supports_avx512vlbw() &amp;&amp;
2201     VM_Version::supports_bmi2()) {
2202 
2203     Label test_64_loop, test_tail;
2204     Register tmp3_aliased = len;
2205 
2206     movl(tmp1, len);
2207     vpxor(vec2, vec2, vec2, Assembler::AVX_512bit);
2208 
2209     andl(tmp1, 64 - 1);   // tail count (in chars) 0x3F
2210     andl(len, ~(64 - 1));    // vector count (in chars)
2211     jccb(Assembler::zero, test_tail);
2212 
2213     lea(ary1, Address(ary1, len, Address::times_1));
2214     negptr(len);
2215 
2216     bind(test_64_loop);
2217     // Check whether our 64 elements of size byte contain negatives
2218     evpcmpgtb(k2, vec2, Address(ary1, len, Address::times_1), Assembler::AVX_512bit);
2219     kortestql(k2, k2);
2220     jcc(Assembler::notZero, TRUE_LABEL);
2221 
2222     addptr(len, 64);
2223     jccb(Assembler::notZero, test_64_loop);
2224 
2225 
2226     bind(test_tail);
2227     // bail out when there is nothing to be done
2228     testl(tmp1, -1);
2229     jcc(Assembler::zero, FALSE_LABEL);
2230 
2231     // ~(~0 &lt;&lt; len) applied up to two times (for 32-bit scenario)
2232 #ifdef _LP64
2233     mov64(tmp3_aliased, 0xFFFFFFFFFFFFFFFF);
2234     shlxq(tmp3_aliased, tmp3_aliased, tmp1);
2235     notq(tmp3_aliased);
2236     kmovql(k3, tmp3_aliased);
2237 #else
2238     Label k_init;
2239     jmp(k_init);
2240 
2241     // We could not read 64-bits from a general purpose register thus we move
2242     // data required to compose 64 1&#39;s to the instruction stream
2243     // We emit 64 byte wide series of elements from 0..63 which later on would
2244     // be used as a compare targets with tail count contained in tmp1 register.
2245     // Result would be a k register having tmp1 consecutive number or 1
2246     // counting from least significant bit.
2247     address tmp = pc();
2248     emit_int64(0x0706050403020100);
2249     emit_int64(0x0F0E0D0C0B0A0908);
2250     emit_int64(0x1716151413121110);
2251     emit_int64(0x1F1E1D1C1B1A1918);
2252     emit_int64(0x2726252423222120);
2253     emit_int64(0x2F2E2D2C2B2A2928);
2254     emit_int64(0x3736353433323130);
2255     emit_int64(0x3F3E3D3C3B3A3938);
2256 
2257     bind(k_init);
2258     lea(len, InternalAddress(tmp));
2259     // create mask to test for negative byte inside a vector
2260     evpbroadcastb(vec1, tmp1, Assembler::AVX_512bit);
2261     evpcmpgtb(k3, vec1, Address(len, 0), Assembler::AVX_512bit);
2262 
2263 #endif
2264     evpcmpgtb(k2, k3, vec2, Address(ary1, 0), Assembler::AVX_512bit);
2265     ktestq(k2, k3);
2266     jcc(Assembler::notZero, TRUE_LABEL);
2267 
2268     jmp(FALSE_LABEL);
2269   } else {
2270     movl(result, len); // copy
2271 
2272     if (UseAVX &gt;= 2 &amp;&amp; UseSSE &gt;= 2) {
2273       // With AVX2, use 32-byte vector compare
2274       Label COMPARE_WIDE_VECTORS, COMPARE_TAIL;
2275 
2276       // Compare 32-byte vectors
2277       andl(result, 0x0000001f);  //   tail count (in bytes)
2278       andl(len, 0xffffffe0);   // vector count (in bytes)
2279       jccb(Assembler::zero, COMPARE_TAIL);
2280 
2281       lea(ary1, Address(ary1, len, Address::times_1));
2282       negptr(len);
2283 
2284       movl(tmp1, 0x80808080);   // create mask to test for Unicode chars in vector
2285       movdl(vec2, tmp1);
2286       vpbroadcastd(vec2, vec2, Assembler::AVX_256bit);
2287 
2288       bind(COMPARE_WIDE_VECTORS);
2289       vmovdqu(vec1, Address(ary1, len, Address::times_1));
2290       vptest(vec1, vec2);
2291       jccb(Assembler::notZero, TRUE_LABEL);
2292       addptr(len, 32);
2293       jcc(Assembler::notZero, COMPARE_WIDE_VECTORS);
2294 
2295       testl(result, result);
2296       jccb(Assembler::zero, FALSE_LABEL);
2297 
2298       vmovdqu(vec1, Address(ary1, result, Address::times_1, -32));
2299       vptest(vec1, vec2);
2300       jccb(Assembler::notZero, TRUE_LABEL);
2301       jmpb(FALSE_LABEL);
2302 
2303       bind(COMPARE_TAIL); // len is zero
2304       movl(len, result);
2305       // Fallthru to tail compare
2306     } else if (UseSSE42Intrinsics) {
2307       // With SSE4.2, use double quad vector compare
2308       Label COMPARE_WIDE_VECTORS, COMPARE_TAIL;
2309 
2310       // Compare 16-byte vectors
2311       andl(result, 0x0000000f);  //   tail count (in bytes)
2312       andl(len, 0xfffffff0);   // vector count (in bytes)
2313       jcc(Assembler::zero, COMPARE_TAIL);
2314 
2315       lea(ary1, Address(ary1, len, Address::times_1));
2316       negptr(len);
2317 
2318       movl(tmp1, 0x80808080);
2319       movdl(vec2, tmp1);
2320       pshufd(vec2, vec2, 0);
2321 
2322       bind(COMPARE_WIDE_VECTORS);
2323       movdqu(vec1, Address(ary1, len, Address::times_1));
2324       ptest(vec1, vec2);
2325       jcc(Assembler::notZero, TRUE_LABEL);
2326       addptr(len, 16);
2327       jcc(Assembler::notZero, COMPARE_WIDE_VECTORS);
2328 
2329       testl(result, result);
2330       jcc(Assembler::zero, FALSE_LABEL);
2331 
2332       movdqu(vec1, Address(ary1, result, Address::times_1, -16));
2333       ptest(vec1, vec2);
2334       jccb(Assembler::notZero, TRUE_LABEL);
2335       jmpb(FALSE_LABEL);
2336 
2337       bind(COMPARE_TAIL); // len is zero
2338       movl(len, result);
2339       // Fallthru to tail compare
2340     }
2341   }
2342   // Compare 4-byte vectors
2343   andl(len, 0xfffffffc); // vector count (in bytes)
2344   jccb(Assembler::zero, COMPARE_CHAR);
2345 
2346   lea(ary1, Address(ary1, len, Address::times_1));
2347   negptr(len);
2348 
2349   bind(COMPARE_VECTORS);
2350   movl(tmp1, Address(ary1, len, Address::times_1));
2351   andl(tmp1, 0x80808080);
2352   jccb(Assembler::notZero, TRUE_LABEL);
2353   addptr(len, 4);
2354   jcc(Assembler::notZero, COMPARE_VECTORS);
2355 
2356   // Compare trailing char (final 2 bytes), if any
2357   bind(COMPARE_CHAR);
2358   testl(result, 0x2);   // tail  char
2359   jccb(Assembler::zero, COMPARE_BYTE);
2360   load_unsigned_short(tmp1, Address(ary1, 0));
2361   andl(tmp1, 0x00008080);
2362   jccb(Assembler::notZero, TRUE_LABEL);
2363   subptr(result, 2);
2364   lea(ary1, Address(ary1, 2));
2365 
2366   bind(COMPARE_BYTE);
2367   testl(result, 0x1);   // tail  byte
2368   jccb(Assembler::zero, FALSE_LABEL);
2369   load_unsigned_byte(tmp1, Address(ary1, 0));
2370   andl(tmp1, 0x00000080);
2371   jccb(Assembler::notEqual, TRUE_LABEL);
2372   jmpb(FALSE_LABEL);
2373 
2374   bind(TRUE_LABEL);
2375   movl(result, 1);   // return true
2376   jmpb(DONE);
2377 
2378   bind(FALSE_LABEL);
2379   xorl(result, result); // return false
2380 
2381   // That&#39;s it
2382   bind(DONE);
2383   if (UseAVX &gt;= 2 &amp;&amp; UseSSE &gt;= 2) {
2384     // clean upper bits of YMM registers
2385     vpxor(vec1, vec1);
2386     vpxor(vec2, vec2);
2387   }
2388 }
2389 // Compare char[] or byte[] arrays aligned to 4 bytes or substrings.
2390 void C2_MacroAssembler::arrays_equals(bool is_array_equ, Register ary1, Register ary2,
2391                                       Register limit, Register result, Register chr,
2392                                       XMMRegister vec1, XMMRegister vec2, bool is_char) {
2393   ShortBranchVerifier sbv(this);
2394   Label TRUE_LABEL, FALSE_LABEL, DONE, COMPARE_VECTORS, COMPARE_CHAR, COMPARE_BYTE;
2395 
2396   int length_offset  = arrayOopDesc::length_offset_in_bytes();
2397   int base_offset    = arrayOopDesc::base_offset_in_bytes(is_char ? T_CHAR : T_BYTE);
2398 
2399   if (is_array_equ) {
2400     // Check the input args
2401     cmpoop(ary1, ary2);
2402     jcc(Assembler::equal, TRUE_LABEL);
2403 
2404     // Need additional checks for arrays_equals.
2405     testptr(ary1, ary1);
2406     jcc(Assembler::zero, FALSE_LABEL);
2407     testptr(ary2, ary2);
2408     jcc(Assembler::zero, FALSE_LABEL);
2409 
2410     // Check the lengths
2411     movl(limit, Address(ary1, length_offset));
2412     cmpl(limit, Address(ary2, length_offset));
2413     jcc(Assembler::notEqual, FALSE_LABEL);
2414   }
2415 
2416   // count == 0
2417   testl(limit, limit);
2418   jcc(Assembler::zero, TRUE_LABEL);
2419 
2420   if (is_array_equ) {
2421     // Load array address
2422     lea(ary1, Address(ary1, base_offset));
2423     lea(ary2, Address(ary2, base_offset));
2424   }
2425 
2426   if (is_array_equ &amp;&amp; is_char) {
2427     // arrays_equals when used for char[].
2428     shll(limit, 1);      // byte count != 0
2429   }
2430   movl(result, limit); // copy
2431 
2432   if (UseAVX &gt;= 2) {
2433     // With AVX2, use 32-byte vector compare
2434     Label COMPARE_WIDE_VECTORS, COMPARE_TAIL;
2435 
2436     // Compare 32-byte vectors
2437     andl(result, 0x0000001f);  //   tail count (in bytes)
2438     andl(limit, 0xffffffe0);   // vector count (in bytes)
2439     jcc(Assembler::zero, COMPARE_TAIL);
2440 
2441     lea(ary1, Address(ary1, limit, Address::times_1));
2442     lea(ary2, Address(ary2, limit, Address::times_1));
2443     negptr(limit);
2444 
2445 #ifdef _LP64
2446     if ((AVX3Threshold == 0) &amp;&amp; VM_Version::supports_avx512vlbw()) { // trying 64 bytes fast loop
2447       Label COMPARE_WIDE_VECTORS_LOOP_AVX2, COMPARE_WIDE_VECTORS_LOOP_AVX3;
2448 
2449       cmpl(limit, -64);
2450       jcc(Assembler::greater, COMPARE_WIDE_VECTORS_LOOP_AVX2);
2451 
2452       bind(COMPARE_WIDE_VECTORS_LOOP_AVX3); // the hottest loop
2453 
2454       evmovdquq(vec1, Address(ary1, limit, Address::times_1), Assembler::AVX_512bit);
2455       evpcmpeqb(k7, vec1, Address(ary2, limit, Address::times_1), Assembler::AVX_512bit);
2456       kortestql(k7, k7);
2457       jcc(Assembler::aboveEqual, FALSE_LABEL);     // miscompare
2458       addptr(limit, 64);  // update since we already compared at this addr
2459       cmpl(limit, -64);
2460       jccb(Assembler::lessEqual, COMPARE_WIDE_VECTORS_LOOP_AVX3);
2461 
2462       // At this point we may still need to compare -limit+result bytes.
2463       // We could execute the next two instruction and just continue via non-wide path:
2464       //  cmpl(limit, 0);
2465       //  jcc(Assembler::equal, COMPARE_TAIL);  // true
2466       // But since we stopped at the points ary{1,2}+limit which are
2467       // not farther than 64 bytes from the ends of arrays ary{1,2}+result
2468       // (|limit| &lt;= 32 and result &lt; 32),
2469       // we may just compare the last 64 bytes.
2470       //
2471       addptr(result, -64);   // it is safe, bc we just came from this area
2472       evmovdquq(vec1, Address(ary1, result, Address::times_1), Assembler::AVX_512bit);
2473       evpcmpeqb(k7, vec1, Address(ary2, result, Address::times_1), Assembler::AVX_512bit);
2474       kortestql(k7, k7);
2475       jcc(Assembler::aboveEqual, FALSE_LABEL);     // miscompare
2476 
2477       jmp(TRUE_LABEL);
2478 
2479       bind(COMPARE_WIDE_VECTORS_LOOP_AVX2);
2480 
2481     }//if (VM_Version::supports_avx512vlbw())
2482 #endif //_LP64
2483     bind(COMPARE_WIDE_VECTORS);
2484     vmovdqu(vec1, Address(ary1, limit, Address::times_1));
2485     vmovdqu(vec2, Address(ary2, limit, Address::times_1));
2486     vpxor(vec1, vec2);
2487 
2488     vptest(vec1, vec1);
2489     jcc(Assembler::notZero, FALSE_LABEL);
2490     addptr(limit, 32);
2491     jcc(Assembler::notZero, COMPARE_WIDE_VECTORS);
2492 
2493     testl(result, result);
2494     jcc(Assembler::zero, TRUE_LABEL);
2495 
2496     vmovdqu(vec1, Address(ary1, result, Address::times_1, -32));
2497     vmovdqu(vec2, Address(ary2, result, Address::times_1, -32));
2498     vpxor(vec1, vec2);
2499 
2500     vptest(vec1, vec1);
2501     jccb(Assembler::notZero, FALSE_LABEL);
2502     jmpb(TRUE_LABEL);
2503 
2504     bind(COMPARE_TAIL); // limit is zero
2505     movl(limit, result);
2506     // Fallthru to tail compare
2507   } else if (UseSSE42Intrinsics) {
2508     // With SSE4.2, use double quad vector compare
2509     Label COMPARE_WIDE_VECTORS, COMPARE_TAIL;
2510 
2511     // Compare 16-byte vectors
2512     andl(result, 0x0000000f);  //   tail count (in bytes)
2513     andl(limit, 0xfffffff0);   // vector count (in bytes)
2514     jcc(Assembler::zero, COMPARE_TAIL);
2515 
2516     lea(ary1, Address(ary1, limit, Address::times_1));
2517     lea(ary2, Address(ary2, limit, Address::times_1));
2518     negptr(limit);
2519 
2520     bind(COMPARE_WIDE_VECTORS);
2521     movdqu(vec1, Address(ary1, limit, Address::times_1));
2522     movdqu(vec2, Address(ary2, limit, Address::times_1));
2523     pxor(vec1, vec2);
2524 
2525     ptest(vec1, vec1);
2526     jcc(Assembler::notZero, FALSE_LABEL);
2527     addptr(limit, 16);
2528     jcc(Assembler::notZero, COMPARE_WIDE_VECTORS);
2529 
2530     testl(result, result);
2531     jcc(Assembler::zero, TRUE_LABEL);
2532 
2533     movdqu(vec1, Address(ary1, result, Address::times_1, -16));
2534     movdqu(vec2, Address(ary2, result, Address::times_1, -16));
2535     pxor(vec1, vec2);
2536 
2537     ptest(vec1, vec1);
2538     jccb(Assembler::notZero, FALSE_LABEL);
2539     jmpb(TRUE_LABEL);
2540 
2541     bind(COMPARE_TAIL); // limit is zero
2542     movl(limit, result);
2543     // Fallthru to tail compare
2544   }
2545 
2546   // Compare 4-byte vectors
2547   andl(limit, 0xfffffffc); // vector count (in bytes)
2548   jccb(Assembler::zero, COMPARE_CHAR);
2549 
2550   lea(ary1, Address(ary1, limit, Address::times_1));
2551   lea(ary2, Address(ary2, limit, Address::times_1));
2552   negptr(limit);
2553 
2554   bind(COMPARE_VECTORS);
2555   movl(chr, Address(ary1, limit, Address::times_1));
2556   cmpl(chr, Address(ary2, limit, Address::times_1));
2557   jccb(Assembler::notEqual, FALSE_LABEL);
2558   addptr(limit, 4);
2559   jcc(Assembler::notZero, COMPARE_VECTORS);
2560 
2561   // Compare trailing char (final 2 bytes), if any
2562   bind(COMPARE_CHAR);
2563   testl(result, 0x2);   // tail  char
2564   jccb(Assembler::zero, COMPARE_BYTE);
2565   load_unsigned_short(chr, Address(ary1, 0));
2566   load_unsigned_short(limit, Address(ary2, 0));
2567   cmpl(chr, limit);
2568   jccb(Assembler::notEqual, FALSE_LABEL);
2569 
2570   if (is_array_equ &amp;&amp; is_char) {
2571     bind(COMPARE_BYTE);
2572   } else {
2573     lea(ary1, Address(ary1, 2));
2574     lea(ary2, Address(ary2, 2));
2575 
2576     bind(COMPARE_BYTE);
2577     testl(result, 0x1);   // tail  byte
2578     jccb(Assembler::zero, TRUE_LABEL);
2579     load_unsigned_byte(chr, Address(ary1, 0));
2580     load_unsigned_byte(limit, Address(ary2, 0));
2581     cmpl(chr, limit);
2582     jccb(Assembler::notEqual, FALSE_LABEL);
2583   }
2584   bind(TRUE_LABEL);
2585   movl(result, 1);   // return true
2586   jmpb(DONE);
2587 
2588   bind(FALSE_LABEL);
2589   xorl(result, result); // return false
2590 
2591   // That&#39;s it
2592   bind(DONE);
2593   if (UseAVX &gt;= 2) {
2594     // clean upper bits of YMM registers
2595     vpxor(vec1, vec1);
2596     vpxor(vec2, vec2);
2597   }
2598 }
<a name="3" id="anc3"></a><b style="font-size: large; color: red">--- EOF ---</b>
















































































</pre>
<input id="eof" value="3" type="hidden" />
</body>
</html>