<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/cpu/x86/macroAssembler_x86.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="interp_masm_x86.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="macroAssembler_x86.hpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/cpu/x86/macroAssembler_x86.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
1072 void MacroAssembler::reserved_stack_check() {
1073     // testing if reserved zone needs to be enabled
1074     Label no_reserved_zone_enabling;
1075     Register thread = NOT_LP64(rsi) LP64_ONLY(r15_thread);
1076     NOT_LP64(get_thread(rsi);)
1077 
1078     cmpptr(rsp, Address(thread, JavaThread::reserved_stack_activation_offset()));
1079     jcc(Assembler::below, no_reserved_zone_enabling);
1080 
1081     call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::enable_stack_reserved_zone), thread);
1082     jump(RuntimeAddress(StubRoutines::throw_delayed_StackOverflowError_entry()));
1083     should_not_reach_here();
1084 
1085     bind(no_reserved_zone_enabling);
1086 }
1087 
1088 int MacroAssembler::biased_locking_enter(Register lock_reg,
1089                                          Register obj_reg,
1090                                          Register swap_reg,
1091                                          Register tmp_reg,

1092                                          bool swap_reg_contains_mark,
1093                                          Label&amp; done,
1094                                          Label* slow_case,
1095                                          BiasedLockingCounters* counters) {
1096   assert(UseBiasedLocking, &quot;why call this otherwise?&quot;);
1097   assert(swap_reg == rax, &quot;swap_reg must be rax for cmpxchgq&quot;);
1098   assert(tmp_reg != noreg, &quot;tmp_reg must be supplied&quot;);
1099   assert_different_registers(lock_reg, obj_reg, swap_reg, tmp_reg);
1100   assert(markWord::age_shift == markWord::lock_bits + markWord::biased_lock_bits, &quot;biased locking makes assumptions about bit layout&quot;);
1101   Address mark_addr      (obj_reg, oopDesc::mark_offset_in_bytes());
1102   NOT_LP64( Address saved_mark_addr(lock_reg, 0); )
1103 
1104   if (PrintBiasedLockingStatistics &amp;&amp; counters == NULL) {
1105     counters = BiasedLocking::counters();
1106   }
1107   // Biased locking
1108   // See whether the lock is currently biased toward our thread and
1109   // whether the epoch is still valid
1110   // Note that the runtime guarantees sufficient alignment of JavaThread
1111   // pointers to allow age to be placed into low bits
</pre>
<hr />
<pre>
1116     null_check_offset = offset();
1117     movptr(swap_reg, mark_addr);
1118   }
1119   movptr(tmp_reg, swap_reg);
1120   andptr(tmp_reg, markWord::biased_lock_mask_in_place);
1121   cmpptr(tmp_reg, markWord::biased_lock_pattern);
1122   jcc(Assembler::notEqual, cas_label);
1123   // The bias pattern is present in the object&#39;s header. Need to check
1124   // whether the bias owner and the epoch are both still current.
1125 #ifndef _LP64
1126   // Note that because there is no current thread register on x86_32 we
1127   // need to store off the mark word we read out of the object to
1128   // avoid reloading it and needing to recheck invariants below. This
1129   // store is unfortunate but it makes the overall code shorter and
1130   // simpler.
1131   movptr(saved_mark_addr, swap_reg);
1132 #endif
1133   if (swap_reg_contains_mark) {
1134     null_check_offset = offset();
1135   }
<span class="line-modified">1136   load_prototype_header(tmp_reg, obj_reg);</span>
1137 #ifdef _LP64
1138   orptr(tmp_reg, r15_thread);
1139   xorptr(tmp_reg, swap_reg);
1140   Register header_reg = tmp_reg;
1141 #else
1142   xorptr(tmp_reg, swap_reg);
1143   get_thread(swap_reg);
1144   xorptr(swap_reg, tmp_reg);
1145   Register header_reg = swap_reg;
1146 #endif
1147   andptr(header_reg, ~((int) markWord::age_mask_in_place));
1148   if (counters != NULL) {
1149     cond_inc32(Assembler::zero,
1150                ExternalAddress((address) counters-&gt;biased_lock_entry_count_addr()));
1151   }
1152   jcc(Assembler::equal, done);
1153 
1154   Label try_revoke_bias;
1155   Label try_rebias;
1156 
</pre>
<hr />
<pre>
1202   // interpreter runtime in the slow case.
1203   if (counters != NULL) {
1204     cond_inc32(Assembler::zero,
1205                ExternalAddress((address) counters-&gt;anonymously_biased_lock_entry_count_addr()));
1206   }
1207   if (slow_case != NULL) {
1208     jcc(Assembler::notZero, *slow_case);
1209   }
1210   jmp(done);
1211 
1212   bind(try_rebias);
1213   // At this point we know the epoch has expired, meaning that the
1214   // current &quot;bias owner&quot;, if any, is actually invalid. Under these
1215   // circumstances _only_, we are allowed to use the current header&#39;s
1216   // value as the comparison value when doing the cas to acquire the
1217   // bias in the current epoch. In other words, we allow transfer of
1218   // the bias from one thread to another directly in this situation.
1219   //
1220   // FIXME: due to a lack of registers we currently blow away the age
1221   // bits in this situation. Should attempt to preserve them.
<span class="line-modified">1222   load_prototype_header(tmp_reg, obj_reg);</span>
1223 #ifdef _LP64
1224   orptr(tmp_reg, r15_thread);
1225 #else
1226   get_thread(swap_reg);
1227   orptr(tmp_reg, swap_reg);
1228   movptr(swap_reg, saved_mark_addr);
1229 #endif
1230   lock();
1231   cmpxchgptr(tmp_reg, mark_addr); // compare tmp_reg and swap_reg
1232   // If the biasing toward our thread failed, then another thread
1233   // succeeded in biasing it toward itself and we need to revoke that
1234   // bias. The revocation will occur in the runtime in the slow case.
1235   if (counters != NULL) {
1236     cond_inc32(Assembler::zero,
1237                ExternalAddress((address) counters-&gt;rebiased_lock_entry_count_addr()));
1238   }
1239   if (slow_case != NULL) {
1240     jcc(Assembler::notZero, *slow_case);
1241   }
1242   jmp(done);
1243 
1244   bind(try_revoke_bias);
1245   // The prototype mark in the klass doesn&#39;t have the bias bit set any
1246   // more, indicating that objects of this data type are not supposed
1247   // to be biased any more. We are going to try to reset the mark of
1248   // this object to the prototype value and fall through to the
1249   // CAS-based locking scheme. Note that if our CAS fails, it means
1250   // that another thread raced us for the privilege of revoking the
1251   // bias of this particular object, so it&#39;s okay to continue in the
1252   // normal locking code.
1253   //
1254   // FIXME: due to a lack of registers we currently blow away the age
1255   // bits in this situation. Should attempt to preserve them.
1256   NOT_LP64( movptr(swap_reg, saved_mark_addr); )
<span class="line-modified">1257   load_prototype_header(tmp_reg, obj_reg);</span>
1258   lock();
1259   cmpxchgptr(tmp_reg, mark_addr); // compare tmp_reg and swap_reg
1260   // Fall through to the normal CAS-based lock, because no matter what
1261   // the result of the above CAS, some thread must have succeeded in
1262   // removing the bias bit from the object&#39;s header.
1263   if (counters != NULL) {
1264     cond_inc32(Assembler::zero,
1265                ExternalAddress((address) counters-&gt;revoked_lock_entry_count_addr()));
1266   }
1267 
1268   bind(cas_label);
1269 
1270   return null_check_offset;
1271 }
1272 
1273 void MacroAssembler::biased_locking_exit(Register obj_reg, Register temp_reg, Label&amp; done) {
1274   assert(UseBiasedLocking, &quot;why call this otherwise?&quot;);
1275 
1276   // Check for biased locking unlock case, which is a no-op
1277   // Note: we do not have to check the thread ID for two reasons.
</pre>
<hr />
<pre>
1499                                   bool     check_exceptions) {
1500   // determine java_thread register
1501   if (!java_thread-&gt;is_valid()) {
1502 #ifdef _LP64
1503     java_thread = r15_thread;
1504 #else
1505     java_thread = rdi;
1506     get_thread(java_thread);
1507 #endif // LP64
1508   }
1509   // determine last_java_sp register
1510   if (!last_java_sp-&gt;is_valid()) {
1511     last_java_sp = rsp;
1512   }
1513   // debugging support
1514   assert(number_of_arguments &gt;= 0   , &quot;cannot have negative number of arguments&quot;);
1515   LP64_ONLY(assert(java_thread == r15_thread, &quot;unexpected register&quot;));
1516 #ifdef ASSERT
1517   // TraceBytecodes does not use r12 but saves it over the call, so don&#39;t verify
1518   // r12 is the heapbase.
<span class="line-modified">1519   LP64_ONLY(if ((UseCompressedOops || UseCompressedClassPointers) &amp;&amp; !TraceBytecodes) verify_heapbase(&quot;call_VM_base: heap base corrupted?&quot;);)</span>
1520 #endif // ASSERT
1521 
1522   assert(java_thread != oop_result  , &quot;cannot use the same register for java_thread &amp; oop_result&quot;);
1523   assert(java_thread != last_java_sp, &quot;cannot use the same register for java_thread &amp; last_java_sp&quot;);
1524 
1525   // push java thread (becomes first argument of C function)
1526 
1527   NOT_LP64(push(java_thread); number_of_arguments++);
1528   LP64_ONLY(mov(c_rarg0, r15_thread));
1529 
1530   // set last Java frame before call
1531   assert(last_java_sp != rbp, &quot;can&#39;t use ebp/rbp&quot;);
1532 
1533   // Only interpreter should have to set fp
1534   set_last_Java_frame(java_thread, last_java_sp, rbp, NULL);
1535 
1536   // do the call, remove parameters
1537   MacroAssembler::call_VM_leaf_base(entry_point, number_of_arguments);
1538 
1539   // restore the thread (cannot use the pushed argument since arguments
</pre>
<hr />
<pre>
2644 }
2645 
2646 void MacroAssembler::test_field_is_not_flattenable(Register flags, Register temp_reg, Label&amp; notFlattenable) {
2647   movl(temp_reg, flags);
2648   shrl(temp_reg, ConstantPoolCacheEntry::is_flattenable_field_shift);
2649   andl(temp_reg, 0x1);
2650   testl(temp_reg, temp_reg);
2651   jcc(Assembler::zero, notFlattenable);
2652 }
2653 
2654 void MacroAssembler::test_field_is_flattened(Register flags, Register temp_reg, Label&amp; is_flattened) {
2655   movl(temp_reg, flags);
2656   shrl(temp_reg, ConstantPoolCacheEntry::is_flattened_field_shift);
2657   andl(temp_reg, 0x1);
2658   testl(temp_reg, temp_reg);
2659   jcc(Assembler::notZero, is_flattened);
2660 }
2661 
2662 void MacroAssembler::test_flattened_array_oop(Register oop, Register temp_reg,
2663                                               Label&amp;is_flattened_array) {
<span class="line-modified">2664   load_klass(temp_reg, oop);</span>

2665   movl(temp_reg, Address(temp_reg, Klass::layout_helper_offset()));
2666   test_flattened_array_layout(temp_reg, is_flattened_array);
2667 }
2668 
2669 void MacroAssembler::test_non_flattened_array_oop(Register oop, Register temp_reg,
2670                                                   Label&amp;is_non_flattened_array) {
<span class="line-modified">2671   load_klass(temp_reg, oop);</span>

2672   movl(temp_reg, Address(temp_reg, Klass::layout_helper_offset()));
2673   test_non_flattened_array_layout(temp_reg, is_non_flattened_array);
2674 }
2675 
2676 void MacroAssembler::test_null_free_array_oop(Register oop, Register temp_reg, Label&amp;is_null_free_array) {
<span class="line-modified">2677   load_klass(temp_reg, oop);</span>

2678   movl(temp_reg, Address(temp_reg, Klass::layout_helper_offset()));
2679   test_null_free_array_layout(temp_reg, is_null_free_array);
2680 }
2681 
2682 void MacroAssembler::test_non_null_free_array_oop(Register oop, Register temp_reg, Label&amp;is_non_null_free_array) {
<span class="line-modified">2683   load_klass(temp_reg, oop);</span>

2684   movl(temp_reg, Address(temp_reg, Klass::layout_helper_offset()));
2685   test_non_null_free_array_layout(temp_reg, is_non_null_free_array);
2686 }
2687 
2688 void MacroAssembler::test_flattened_array_layout(Register lh, Label&amp; is_flattened_array) {
2689   testl(lh, Klass::_lh_array_tag_vt_value_bit_inplace);
2690   jcc(Assembler::notZero, is_flattened_array);
2691 }
2692 void MacroAssembler::test_non_flattened_array_layout(Register lh, Label&amp; is_non_flattened_array) {
2693   testl(lh, Klass::_lh_array_tag_vt_value_bit_inplace);
2694   jcc(Assembler::zero, is_non_flattened_array);
2695 }
2696 
2697 void MacroAssembler::test_null_free_array_layout(Register lh, Label&amp; is_null_free_array) {
2698   testl(lh, Klass::_lh_null_free_bit_inplace);
2699   jcc(Assembler::notZero, is_null_free_array);
2700 }
2701 
2702 void MacroAssembler::test_non_null_free_array_layout(Register lh, Label&amp; is_non_null_free_array) {
2703   testl(lh, Klass::_lh_null_free_bit_inplace);
</pre>
<hr />
<pre>
3501         Label loop;
3502         bind(loop);
3503         movptr(Address(new_obj, layout_size, Address::times_8, sizeof(oopDesc) - 1*oopSize), zero);
3504         NOT_LP64(movptr(Address(new_obj, layout_size, Address::times_8, sizeof(oopDesc) - 2*oopSize), zero));
3505         decrement(layout_size);
3506         jcc(Assembler::notZero, loop);
3507       }
3508     } // clear_fields
3509 
3510     // initialize object header only.
3511     bind(initialize_header);
3512     pop(klass);
3513     Register mark_word = t2;
3514     movptr(mark_word, Address(klass, Klass::prototype_header_offset()));
3515     movptr(Address(new_obj, oopDesc::mark_offset_in_bytes ()), mark_word);
3516 #ifdef _LP64
3517     xorl(rsi, rsi);                 // use zero reg to clear memory (shorter code)
3518     store_klass_gap(new_obj, rsi);  // zero klass gap for compressed oops
3519 #endif
3520     movptr(t2, klass);         // preserve klass
<span class="line-modified">3521     store_klass(new_obj, t2);  // src klass reg is potentially compressed</span>

3522 
3523     jmp(done);
3524   }
3525 
3526   bind(slow_case);
3527   pop(klass);
3528   bind(slow_case_no_pop);
3529   jmp(alloc_failed);
3530 
3531   bind(done);
3532 }
3533 
3534 // Defines obj, preserves var_size_in_bytes, okay for t2 == var_size_in_bytes.
3535 void MacroAssembler::tlab_allocate(Register thread, Register obj,
3536                                    Register var_size_in_bytes,
3537                                    int con_size_in_bytes,
3538                                    Register t1,
3539                                    Register t2,
3540                                    Label&amp; slow_case) {
3541   BarrierSetAssembler* bs = BarrierSet::barrier_set()-&gt;barrier_set_assembler();
</pre>
<hr />
<pre>
4585   // Only IN_HEAP loads require a thread_tmp register
4586   // WeakHandle::resolve is an indirection like jweak.
4587   access_load_at(T_OBJECT, IN_NATIVE | ON_PHANTOM_OOP_REF,
4588                  rresult, Address(rresult, 0), rtmp, /*tmp_thread*/noreg);
4589   bind(resolved);
4590 }
4591 
4592 void MacroAssembler::load_mirror(Register mirror, Register method, Register tmp) {
4593   // get mirror
4594   const int mirror_offset = in_bytes(Klass::java_mirror_offset());
4595   load_method_holder(mirror, method);
4596   movptr(mirror, Address(mirror, mirror_offset));
4597   resolve_oop_handle(mirror, tmp);
4598 }
4599 
4600 void MacroAssembler::load_method_holder_cld(Register rresult, Register rmethod) {
4601   load_method_holder(rresult, rmethod);
4602   movptr(rresult, Address(rresult, InstanceKlass::class_loader_data_offset()));
4603 }
4604 






4605 void MacroAssembler::load_metadata(Register dst, Register src) {
4606   if (UseCompressedClassPointers) {
4607     movl(dst, Address(src, oopDesc::klass_offset_in_bytes()));
4608   } else {
4609     movptr(dst, Address(src, oopDesc::klass_offset_in_bytes()));
4610   }
4611 }
4612 
<span class="line-modified">4613 void MacroAssembler::load_method_holder(Register holder, Register method) {</span>
<span class="line-modified">4614   movptr(holder, Address(method, Method::const_offset()));                      // ConstMethod*</span>
<span class="line-modified">4615   movptr(holder, Address(holder, ConstMethod::constants_offset()));             // ConstantPool*</span>
<span class="line-removed">4616   movptr(holder, Address(holder, ConstantPool::pool_holder_offset_in_bytes())); // InstanceKlass*</span>
<span class="line-removed">4617 }</span>
<span class="line-removed">4618 </span>
<span class="line-removed">4619 void MacroAssembler::load_klass(Register dst, Register src) {</span>
4620 #ifdef _LP64
4621   if (UseCompressedClassPointers) {
4622     movl(dst, Address(src, oopDesc::klass_offset_in_bytes()));
<span class="line-modified">4623     decode_klass_not_null(dst);</span>
4624   } else
4625 #endif
4626   movptr(dst, Address(src, oopDesc::klass_offset_in_bytes()));
4627 }
4628 
<span class="line-modified">4629 void MacroAssembler::load_prototype_header(Register dst, Register src) {</span>
<span class="line-modified">4630   load_klass(dst, src);</span>
4631   movptr(dst, Address(dst, Klass::prototype_header_offset()));
4632 }
4633 
<span class="line-modified">4634 void MacroAssembler::store_klass(Register dst, Register src) {</span>


4635 #ifdef _LP64
4636   if (UseCompressedClassPointers) {
<span class="line-modified">4637     encode_klass_not_null(src);</span>
4638     movl(Address(dst, oopDesc::klass_offset_in_bytes()), src);
4639   } else
4640 #endif
4641     movptr(Address(dst, oopDesc::klass_offset_in_bytes()), src);
4642 }
4643 
4644 void MacroAssembler::access_load_at(BasicType type, DecoratorSet decorators, Register dst, Address src,
4645                                     Register tmp1, Register thread_tmp) {
4646   BarrierSetAssembler* bs = BarrierSet::barrier_set()-&gt;barrier_set_assembler();
4647   decorators = AccessInternal::decorator_fixup(decorators);
4648   bool as_raw = (decorators &amp; AS_RAW) != 0;
4649   if (as_raw) {
4650     bs-&gt;BarrierSetAssembler::load_at(this, decorators, type, dst, src, tmp1, thread_tmp);
4651   } else {
4652     bs-&gt;load_at(this, decorators, type, dst, src, tmp1, thread_tmp);
4653   }
4654 }
4655 
4656 void MacroAssembler::access_store_at(BasicType type, DecoratorSet decorators, Address dst, Register src,
4657                                      Register tmp1, Register tmp2, Register tmp3) {
</pre>
<hr />
<pre>
4871     assert(LogMinObjAlignmentInBytes == CompressedOops::shift(), &quot;decode alg wrong&quot;);
4872     if (LogMinObjAlignmentInBytes == Address::times_8) {
4873       leaq(dst, Address(r12_heapbase, src, Address::times_8, 0));
4874     } else {
4875       if (dst != src) {
4876         movq(dst, src);
4877       }
4878       shlq(dst, LogMinObjAlignmentInBytes);
4879       if (CompressedOops::base() != NULL) {
4880         addq(dst, r12_heapbase);
4881       }
4882     }
4883   } else {
4884     assert (CompressedOops::base() == NULL, &quot;sanity&quot;);
4885     if (dst != src) {
4886       movq(dst, src);
4887     }
4888   }
4889 }
4890 
<span class="line-modified">4891 void MacroAssembler::encode_klass_not_null(Register r) {</span>

4892   if (CompressedKlassPointers::base() != NULL) {
<span class="line-modified">4893     // Use r12 as a scratch register in which to temporarily load the narrow_klass_base.</span>
<span class="line-modified">4894     assert(r != r12_heapbase, &quot;Encoding a klass in r12&quot;);</span>
<span class="line-removed">4895     mov64(r12_heapbase, (int64_t)CompressedKlassPointers::base());</span>
<span class="line-removed">4896     subq(r, r12_heapbase);</span>
4897   }
4898   if (CompressedKlassPointers::shift() != 0) {
4899     assert (LogKlassAlignmentInBytes == CompressedKlassPointers::shift(), &quot;decode alg wrong&quot;);
4900     shrq(r, LogKlassAlignmentInBytes);
4901   }
<span class="line-removed">4902   if (CompressedKlassPointers::base() != NULL) {</span>
<span class="line-removed">4903     reinit_heapbase();</span>
<span class="line-removed">4904   }</span>
<span class="line-removed">4905 }</span>
<span class="line-removed">4906 </span>
<span class="line-removed">4907 void MacroAssembler::encode_klass_not_null(Register dst, Register src) {</span>
<span class="line-removed">4908   if (dst == src) {</span>
<span class="line-removed">4909     encode_klass_not_null(src);</span>
<span class="line-removed">4910   } else {</span>
<span class="line-removed">4911     if (CompressedKlassPointers::base() != NULL) {</span>
<span class="line-removed">4912       mov64(dst, (int64_t)CompressedKlassPointers::base());</span>
<span class="line-removed">4913       negq(dst);</span>
<span class="line-removed">4914       addq(dst, src);</span>
<span class="line-removed">4915     } else {</span>
<span class="line-removed">4916       movptr(dst, src);</span>
<span class="line-removed">4917     }</span>
<span class="line-removed">4918     if (CompressedKlassPointers::shift() != 0) {</span>
<span class="line-removed">4919       assert (LogKlassAlignmentInBytes == CompressedKlassPointers::shift(), &quot;decode alg wrong&quot;);</span>
<span class="line-removed">4920       shrq(dst, LogKlassAlignmentInBytes);</span>
<span class="line-removed">4921     }</span>
<span class="line-removed">4922   }</span>
4923 }
4924 
<span class="line-modified">4925 // Function instr_size_for_decode_klass_not_null() counts the instructions</span>
<span class="line-modified">4926 // generated by decode_klass_not_null(register r) and reinit_heapbase(),</span>
<span class="line-removed">4927 // when (Universe::heap() != NULL).  Hence, if the instructions they</span>
<span class="line-removed">4928 // generate change, then this method needs to be updated.</span>
<span class="line-removed">4929 int MacroAssembler::instr_size_for_decode_klass_not_null() {</span>
<span class="line-removed">4930   assert (UseCompressedClassPointers, &quot;only for compressed klass ptrs&quot;);</span>
4931   if (CompressedKlassPointers::base() != NULL) {
<span class="line-modified">4932     // mov64 + addq + shlq? + mov64  (for reinit_heapbase()).</span>
<span class="line-modified">4933     return (CompressedKlassPointers::shift() == 0 ? 20 : 24);</span>
4934   } else {
<span class="line-modified">4935     // longest load decode klass function, mov64, leaq</span>
<span class="line-modified">4936     return 16;</span>



4937   }
4938 }
4939 
4940 // !!! If the instructions that get generated here change then function
4941 // instr_size_for_decode_klass_not_null() needs to get updated.
<span class="line-modified">4942 void  MacroAssembler::decode_klass_not_null(Register r) {</span>

4943   // Note: it will change flags
<span class="line-modified">4944   assert (UseCompressedClassPointers, &quot;should only be used for compressed headers&quot;);</span>
<span class="line-removed">4945   assert(r != r12_heapbase, &quot;Decoding a klass in r12&quot;);</span>
4946   // Cannot assert, unverified entry point counts instructions (see .ad file)
4947   // vtableStubs also counts instructions in pd_code_size_limit.
4948   // Also do not verify_oop as this is called by verify_oop.
4949   if (CompressedKlassPointers::shift() != 0) {
4950     assert(LogKlassAlignmentInBytes == CompressedKlassPointers::shift(), &quot;decode alg wrong&quot;);
4951     shlq(r, LogKlassAlignmentInBytes);
4952   }
<span class="line-modified">4953   // Use r12 as a scratch register in which to temporarily load the narrow_klass_base.</span>
<span class="line-modified">4954   if (CompressedKlassPointers::base() != NULL) {</span>
<span class="line-removed">4955     mov64(r12_heapbase, (int64_t)CompressedKlassPointers::base());</span>
<span class="line-removed">4956     addq(r, r12_heapbase);</span>
4957     reinit_heapbase();
4958   }
4959 }
4960 
<span class="line-modified">4961 void  MacroAssembler::decode_klass_not_null(Register dst, Register src) {</span>

4962   // Note: it will change flags
4963   assert (UseCompressedClassPointers, &quot;should only be used for compressed headers&quot;);
<span class="line-modified">4964   if (dst == src) {</span>
<span class="line-modified">4965     decode_klass_not_null(dst);</span>







4966   } else {
<span class="line-modified">4967     // Cannot assert, unverified entry point counts instructions (see .ad file)</span>
<span class="line-modified">4968     // vtableStubs also counts instructions in pd_code_size_limit.</span>
<span class="line-modified">4969     // Also do not verify_oop as this is called by verify_oop.</span>
<span class="line-modified">4970     mov64(dst, (int64_t)CompressedKlassPointers::base());</span>

4971     if (CompressedKlassPointers::shift() != 0) {
4972       assert(LogKlassAlignmentInBytes == CompressedKlassPointers::shift(), &quot;decode alg wrong&quot;);
4973       assert(LogKlassAlignmentInBytes == Address::times_8, &quot;klass not aligned on 64bits?&quot;);
4974       leaq(dst, Address(dst, src, Address::times_8, 0));
4975     } else {
4976       addq(dst, src);
4977     }
4978   }
4979 }
4980 
4981 void  MacroAssembler::set_narrow_oop(Register dst, jobject obj) {
4982   assert (UseCompressedOops, &quot;should only be used for compressed headers&quot;);
4983   assert (Universe::heap() != NULL, &quot;java heap should be initialized&quot;);
4984   assert (oop_recorder() != NULL, &quot;this assembler needs an OopRecorder&quot;);
4985   int oop_index = oop_recorder()-&gt;find_index(obj);
4986   RelocationHolder rspec = oop_Relocation::spec(oop_index);
4987   mov_narrow_oop(dst, oop_index, rspec);
4988 }
4989 
4990 void  MacroAssembler::set_narrow_oop(Address dst, jobject obj) {
</pre>
<hr />
<pre>
5030   Assembler::cmp_narrow_oop(dst, oop_index, rspec);
5031 }
5032 
5033 void  MacroAssembler::cmp_narrow_klass(Register dst, Klass* k) {
5034   assert (UseCompressedClassPointers, &quot;should only be used for compressed headers&quot;);
5035   assert (oop_recorder() != NULL, &quot;this assembler needs an OopRecorder&quot;);
5036   int klass_index = oop_recorder()-&gt;find_index(k);
5037   RelocationHolder rspec = metadata_Relocation::spec(klass_index);
5038   Assembler::cmp_narrow_oop(dst, CompressedKlassPointers::encode(k), rspec);
5039 }
5040 
5041 void  MacroAssembler::cmp_narrow_klass(Address dst, Klass* k) {
5042   assert (UseCompressedClassPointers, &quot;should only be used for compressed headers&quot;);
5043   assert (oop_recorder() != NULL, &quot;this assembler needs an OopRecorder&quot;);
5044   int klass_index = oop_recorder()-&gt;find_index(k);
5045   RelocationHolder rspec = metadata_Relocation::spec(klass_index);
5046   Assembler::cmp_narrow_oop(dst, CompressedKlassPointers::encode(k), rspec);
5047 }
5048 
5049 void MacroAssembler::reinit_heapbase() {
<span class="line-modified">5050   if (UseCompressedOops || UseCompressedClassPointers) {</span>
5051     if (Universe::heap() != NULL) {
5052       if (CompressedOops::base() == NULL) {
5053         MacroAssembler::xorptr(r12_heapbase, r12_heapbase);
5054       } else {
5055         mov64(r12_heapbase, (int64_t)CompressedOops::ptrs_base());
5056       }
5057     } else {
5058       movptr(r12_heapbase, ExternalAddress((address)CompressedOops::ptrs_base_addr()));
5059     }
5060   }
5061 }
5062 
5063 #endif // _LP64
5064 
5065 // C2 compiled method&#39;s prolog code.
5066 void MacroAssembler::verified_entry(Compile* C, int sp_inc) {
5067   int framesize = C-&gt;output()-&gt;frame_size_in_bytes();
5068   int bangsize = C-&gt;output()-&gt;bang_size_in_bytes();
5069   bool fp_mode_24b = false;
5070   int stack_bang_size = C-&gt;output()-&gt;need_stack_bang(bangsize) ? bangsize : 0;
</pre>
<hr />
<pre>
5236       // Call from interpreter. RAX contains ((the ValueKlass* of the return type) | 0x01)
5237       mov(rbx, rax);
5238       andptr(rbx, -2);
5239       movl(r14, Address(rbx, Klass::layout_helper_offset()));
5240     }
5241 
5242     movptr(r13, Address(r15_thread, in_bytes(JavaThread::tlab_top_offset())));
5243     lea(r14, Address(r13, r14, Address::times_1));
5244     cmpptr(r14, Address(r15_thread, in_bytes(JavaThread::tlab_end_offset())));
5245     jcc(Assembler::above, slow_case);
5246     movptr(Address(r15_thread, in_bytes(JavaThread::tlab_top_offset())), r14);
5247     movptr(Address(r13, oopDesc::mark_offset_in_bytes()), (intptr_t)markWord::always_locked_prototype().value());
5248 
5249     xorl(rax, rax); // use zero reg to clear memory (shorter code)
5250     store_klass_gap(r13, rax);  // zero klass gap for compressed oops
5251 
5252     if (vk == NULL) {
5253       // store_klass corrupts rbx, so save it in rax for later use (interpreter case only).
5254       mov(rax, rbx);
5255     }
<span class="line-modified">5256     store_klass(r13, rbx);  // klass</span>

5257 
5258     // We have our new buffered value, initialize its fields with a
5259     // value class specific handler
5260     if (vk != NULL) {
5261       // FIXME -- do the packing in-line to avoid the runtime call
5262       mov(rax, r13);
5263       call(RuntimeAddress(vk-&gt;pack_handler())); // no need for call info as this will not safepoint.
5264     } else {
5265       movptr(rbx, Address(rax, InstanceKlass::adr_valueklass_fixed_block_offset()));
5266       movptr(rbx, Address(rbx, ValueKlass::pack_handler_offset()));
5267       mov(rax, r13);
5268       call(rbx);
5269     }
5270     jmp(skip);
5271   }
5272 
5273   bind(slow_case);
5274   // We failed to allocate a new value, fall back to a runtime
5275   // call. Some oop field may be live in some registers but we can&#39;t
5276   // tell. That runtime call will take care of preserving them
</pre>
</td>
<td>
<hr />
<pre>
1072 void MacroAssembler::reserved_stack_check() {
1073     // testing if reserved zone needs to be enabled
1074     Label no_reserved_zone_enabling;
1075     Register thread = NOT_LP64(rsi) LP64_ONLY(r15_thread);
1076     NOT_LP64(get_thread(rsi);)
1077 
1078     cmpptr(rsp, Address(thread, JavaThread::reserved_stack_activation_offset()));
1079     jcc(Assembler::below, no_reserved_zone_enabling);
1080 
1081     call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::enable_stack_reserved_zone), thread);
1082     jump(RuntimeAddress(StubRoutines::throw_delayed_StackOverflowError_entry()));
1083     should_not_reach_here();
1084 
1085     bind(no_reserved_zone_enabling);
1086 }
1087 
1088 int MacroAssembler::biased_locking_enter(Register lock_reg,
1089                                          Register obj_reg,
1090                                          Register swap_reg,
1091                                          Register tmp_reg,
<span class="line-added">1092                                          Register tmp_reg2,</span>
1093                                          bool swap_reg_contains_mark,
1094                                          Label&amp; done,
1095                                          Label* slow_case,
1096                                          BiasedLockingCounters* counters) {
1097   assert(UseBiasedLocking, &quot;why call this otherwise?&quot;);
1098   assert(swap_reg == rax, &quot;swap_reg must be rax for cmpxchgq&quot;);
1099   assert(tmp_reg != noreg, &quot;tmp_reg must be supplied&quot;);
1100   assert_different_registers(lock_reg, obj_reg, swap_reg, tmp_reg);
1101   assert(markWord::age_shift == markWord::lock_bits + markWord::biased_lock_bits, &quot;biased locking makes assumptions about bit layout&quot;);
1102   Address mark_addr      (obj_reg, oopDesc::mark_offset_in_bytes());
1103   NOT_LP64( Address saved_mark_addr(lock_reg, 0); )
1104 
1105   if (PrintBiasedLockingStatistics &amp;&amp; counters == NULL) {
1106     counters = BiasedLocking::counters();
1107   }
1108   // Biased locking
1109   // See whether the lock is currently biased toward our thread and
1110   // whether the epoch is still valid
1111   // Note that the runtime guarantees sufficient alignment of JavaThread
1112   // pointers to allow age to be placed into low bits
</pre>
<hr />
<pre>
1117     null_check_offset = offset();
1118     movptr(swap_reg, mark_addr);
1119   }
1120   movptr(tmp_reg, swap_reg);
1121   andptr(tmp_reg, markWord::biased_lock_mask_in_place);
1122   cmpptr(tmp_reg, markWord::biased_lock_pattern);
1123   jcc(Assembler::notEqual, cas_label);
1124   // The bias pattern is present in the object&#39;s header. Need to check
1125   // whether the bias owner and the epoch are both still current.
1126 #ifndef _LP64
1127   // Note that because there is no current thread register on x86_32 we
1128   // need to store off the mark word we read out of the object to
1129   // avoid reloading it and needing to recheck invariants below. This
1130   // store is unfortunate but it makes the overall code shorter and
1131   // simpler.
1132   movptr(saved_mark_addr, swap_reg);
1133 #endif
1134   if (swap_reg_contains_mark) {
1135     null_check_offset = offset();
1136   }
<span class="line-modified">1137   load_prototype_header(tmp_reg, obj_reg, tmp_reg2);</span>
1138 #ifdef _LP64
1139   orptr(tmp_reg, r15_thread);
1140   xorptr(tmp_reg, swap_reg);
1141   Register header_reg = tmp_reg;
1142 #else
1143   xorptr(tmp_reg, swap_reg);
1144   get_thread(swap_reg);
1145   xorptr(swap_reg, tmp_reg);
1146   Register header_reg = swap_reg;
1147 #endif
1148   andptr(header_reg, ~((int) markWord::age_mask_in_place));
1149   if (counters != NULL) {
1150     cond_inc32(Assembler::zero,
1151                ExternalAddress((address) counters-&gt;biased_lock_entry_count_addr()));
1152   }
1153   jcc(Assembler::equal, done);
1154 
1155   Label try_revoke_bias;
1156   Label try_rebias;
1157 
</pre>
<hr />
<pre>
1203   // interpreter runtime in the slow case.
1204   if (counters != NULL) {
1205     cond_inc32(Assembler::zero,
1206                ExternalAddress((address) counters-&gt;anonymously_biased_lock_entry_count_addr()));
1207   }
1208   if (slow_case != NULL) {
1209     jcc(Assembler::notZero, *slow_case);
1210   }
1211   jmp(done);
1212 
1213   bind(try_rebias);
1214   // At this point we know the epoch has expired, meaning that the
1215   // current &quot;bias owner&quot;, if any, is actually invalid. Under these
1216   // circumstances _only_, we are allowed to use the current header&#39;s
1217   // value as the comparison value when doing the cas to acquire the
1218   // bias in the current epoch. In other words, we allow transfer of
1219   // the bias from one thread to another directly in this situation.
1220   //
1221   // FIXME: due to a lack of registers we currently blow away the age
1222   // bits in this situation. Should attempt to preserve them.
<span class="line-modified">1223   load_prototype_header(tmp_reg, obj_reg, tmp_reg2);</span>
1224 #ifdef _LP64
1225   orptr(tmp_reg, r15_thread);
1226 #else
1227   get_thread(swap_reg);
1228   orptr(tmp_reg, swap_reg);
1229   movptr(swap_reg, saved_mark_addr);
1230 #endif
1231   lock();
1232   cmpxchgptr(tmp_reg, mark_addr); // compare tmp_reg and swap_reg
1233   // If the biasing toward our thread failed, then another thread
1234   // succeeded in biasing it toward itself and we need to revoke that
1235   // bias. The revocation will occur in the runtime in the slow case.
1236   if (counters != NULL) {
1237     cond_inc32(Assembler::zero,
1238                ExternalAddress((address) counters-&gt;rebiased_lock_entry_count_addr()));
1239   }
1240   if (slow_case != NULL) {
1241     jcc(Assembler::notZero, *slow_case);
1242   }
1243   jmp(done);
1244 
1245   bind(try_revoke_bias);
1246   // The prototype mark in the klass doesn&#39;t have the bias bit set any
1247   // more, indicating that objects of this data type are not supposed
1248   // to be biased any more. We are going to try to reset the mark of
1249   // this object to the prototype value and fall through to the
1250   // CAS-based locking scheme. Note that if our CAS fails, it means
1251   // that another thread raced us for the privilege of revoking the
1252   // bias of this particular object, so it&#39;s okay to continue in the
1253   // normal locking code.
1254   //
1255   // FIXME: due to a lack of registers we currently blow away the age
1256   // bits in this situation. Should attempt to preserve them.
1257   NOT_LP64( movptr(swap_reg, saved_mark_addr); )
<span class="line-modified">1258   load_prototype_header(tmp_reg, obj_reg, tmp_reg2);</span>
1259   lock();
1260   cmpxchgptr(tmp_reg, mark_addr); // compare tmp_reg and swap_reg
1261   // Fall through to the normal CAS-based lock, because no matter what
1262   // the result of the above CAS, some thread must have succeeded in
1263   // removing the bias bit from the object&#39;s header.
1264   if (counters != NULL) {
1265     cond_inc32(Assembler::zero,
1266                ExternalAddress((address) counters-&gt;revoked_lock_entry_count_addr()));
1267   }
1268 
1269   bind(cas_label);
1270 
1271   return null_check_offset;
1272 }
1273 
1274 void MacroAssembler::biased_locking_exit(Register obj_reg, Register temp_reg, Label&amp; done) {
1275   assert(UseBiasedLocking, &quot;why call this otherwise?&quot;);
1276 
1277   // Check for biased locking unlock case, which is a no-op
1278   // Note: we do not have to check the thread ID for two reasons.
</pre>
<hr />
<pre>
1500                                   bool     check_exceptions) {
1501   // determine java_thread register
1502   if (!java_thread-&gt;is_valid()) {
1503 #ifdef _LP64
1504     java_thread = r15_thread;
1505 #else
1506     java_thread = rdi;
1507     get_thread(java_thread);
1508 #endif // LP64
1509   }
1510   // determine last_java_sp register
1511   if (!last_java_sp-&gt;is_valid()) {
1512     last_java_sp = rsp;
1513   }
1514   // debugging support
1515   assert(number_of_arguments &gt;= 0   , &quot;cannot have negative number of arguments&quot;);
1516   LP64_ONLY(assert(java_thread == r15_thread, &quot;unexpected register&quot;));
1517 #ifdef ASSERT
1518   // TraceBytecodes does not use r12 but saves it over the call, so don&#39;t verify
1519   // r12 is the heapbase.
<span class="line-modified">1520   LP64_ONLY(if (UseCompressedOops &amp;&amp; !TraceBytecodes) verify_heapbase(&quot;call_VM_base: heap base corrupted?&quot;);)</span>
1521 #endif // ASSERT
1522 
1523   assert(java_thread != oop_result  , &quot;cannot use the same register for java_thread &amp; oop_result&quot;);
1524   assert(java_thread != last_java_sp, &quot;cannot use the same register for java_thread &amp; last_java_sp&quot;);
1525 
1526   // push java thread (becomes first argument of C function)
1527 
1528   NOT_LP64(push(java_thread); number_of_arguments++);
1529   LP64_ONLY(mov(c_rarg0, r15_thread));
1530 
1531   // set last Java frame before call
1532   assert(last_java_sp != rbp, &quot;can&#39;t use ebp/rbp&quot;);
1533 
1534   // Only interpreter should have to set fp
1535   set_last_Java_frame(java_thread, last_java_sp, rbp, NULL);
1536 
1537   // do the call, remove parameters
1538   MacroAssembler::call_VM_leaf_base(entry_point, number_of_arguments);
1539 
1540   // restore the thread (cannot use the pushed argument since arguments
</pre>
<hr />
<pre>
2645 }
2646 
2647 void MacroAssembler::test_field_is_not_flattenable(Register flags, Register temp_reg, Label&amp; notFlattenable) {
2648   movl(temp_reg, flags);
2649   shrl(temp_reg, ConstantPoolCacheEntry::is_flattenable_field_shift);
2650   andl(temp_reg, 0x1);
2651   testl(temp_reg, temp_reg);
2652   jcc(Assembler::zero, notFlattenable);
2653 }
2654 
2655 void MacroAssembler::test_field_is_flattened(Register flags, Register temp_reg, Label&amp; is_flattened) {
2656   movl(temp_reg, flags);
2657   shrl(temp_reg, ConstantPoolCacheEntry::is_flattened_field_shift);
2658   andl(temp_reg, 0x1);
2659   testl(temp_reg, temp_reg);
2660   jcc(Assembler::notZero, is_flattened);
2661 }
2662 
2663 void MacroAssembler::test_flattened_array_oop(Register oop, Register temp_reg,
2664                                               Label&amp;is_flattened_array) {
<span class="line-modified">2665   Register tmp_load_klass = LP64_ONLY(rscratch1) NOT_LP64(noreg);</span>
<span class="line-added">2666   load_klass(temp_reg, oop, tmp_load_klass);</span>
2667   movl(temp_reg, Address(temp_reg, Klass::layout_helper_offset()));
2668   test_flattened_array_layout(temp_reg, is_flattened_array);
2669 }
2670 
2671 void MacroAssembler::test_non_flattened_array_oop(Register oop, Register temp_reg,
2672                                                   Label&amp;is_non_flattened_array) {
<span class="line-modified">2673   Register tmp_load_klass = LP64_ONLY(rscratch1) NOT_LP64(noreg);</span>
<span class="line-added">2674   load_klass(temp_reg, oop, tmp_load_klass);</span>
2675   movl(temp_reg, Address(temp_reg, Klass::layout_helper_offset()));
2676   test_non_flattened_array_layout(temp_reg, is_non_flattened_array);
2677 }
2678 
2679 void MacroAssembler::test_null_free_array_oop(Register oop, Register temp_reg, Label&amp;is_null_free_array) {
<span class="line-modified">2680   Register tmp_load_klass = LP64_ONLY(rscratch1) NOT_LP64(noreg);</span>
<span class="line-added">2681   load_klass(temp_reg, oop, tmp_load_klass);</span>
2682   movl(temp_reg, Address(temp_reg, Klass::layout_helper_offset()));
2683   test_null_free_array_layout(temp_reg, is_null_free_array);
2684 }
2685 
2686 void MacroAssembler::test_non_null_free_array_oop(Register oop, Register temp_reg, Label&amp;is_non_null_free_array) {
<span class="line-modified">2687   Register tmp_load_klass = LP64_ONLY(rscratch1) NOT_LP64(noreg);</span>
<span class="line-added">2688   load_klass(temp_reg, oop, tmp_load_klass);</span>
2689   movl(temp_reg, Address(temp_reg, Klass::layout_helper_offset()));
2690   test_non_null_free_array_layout(temp_reg, is_non_null_free_array);
2691 }
2692 
2693 void MacroAssembler::test_flattened_array_layout(Register lh, Label&amp; is_flattened_array) {
2694   testl(lh, Klass::_lh_array_tag_vt_value_bit_inplace);
2695   jcc(Assembler::notZero, is_flattened_array);
2696 }
2697 void MacroAssembler::test_non_flattened_array_layout(Register lh, Label&amp; is_non_flattened_array) {
2698   testl(lh, Klass::_lh_array_tag_vt_value_bit_inplace);
2699   jcc(Assembler::zero, is_non_flattened_array);
2700 }
2701 
2702 void MacroAssembler::test_null_free_array_layout(Register lh, Label&amp; is_null_free_array) {
2703   testl(lh, Klass::_lh_null_free_bit_inplace);
2704   jcc(Assembler::notZero, is_null_free_array);
2705 }
2706 
2707 void MacroAssembler::test_non_null_free_array_layout(Register lh, Label&amp; is_non_null_free_array) {
2708   testl(lh, Klass::_lh_null_free_bit_inplace);
</pre>
<hr />
<pre>
3506         Label loop;
3507         bind(loop);
3508         movptr(Address(new_obj, layout_size, Address::times_8, sizeof(oopDesc) - 1*oopSize), zero);
3509         NOT_LP64(movptr(Address(new_obj, layout_size, Address::times_8, sizeof(oopDesc) - 2*oopSize), zero));
3510         decrement(layout_size);
3511         jcc(Assembler::notZero, loop);
3512       }
3513     } // clear_fields
3514 
3515     // initialize object header only.
3516     bind(initialize_header);
3517     pop(klass);
3518     Register mark_word = t2;
3519     movptr(mark_word, Address(klass, Klass::prototype_header_offset()));
3520     movptr(Address(new_obj, oopDesc::mark_offset_in_bytes ()), mark_word);
3521 #ifdef _LP64
3522     xorl(rsi, rsi);                 // use zero reg to clear memory (shorter code)
3523     store_klass_gap(new_obj, rsi);  // zero klass gap for compressed oops
3524 #endif
3525     movptr(t2, klass);         // preserve klass
<span class="line-modified">3526     Register tmp_store_klass = LP64_ONLY(rscratch1) NOT_LP64(noreg);</span>
<span class="line-added">3527     store_klass(new_obj, t2, tmp_store_klass);  // src klass reg is potentially compressed</span>
3528 
3529     jmp(done);
3530   }
3531 
3532   bind(slow_case);
3533   pop(klass);
3534   bind(slow_case_no_pop);
3535   jmp(alloc_failed);
3536 
3537   bind(done);
3538 }
3539 
3540 // Defines obj, preserves var_size_in_bytes, okay for t2 == var_size_in_bytes.
3541 void MacroAssembler::tlab_allocate(Register thread, Register obj,
3542                                    Register var_size_in_bytes,
3543                                    int con_size_in_bytes,
3544                                    Register t1,
3545                                    Register t2,
3546                                    Label&amp; slow_case) {
3547   BarrierSetAssembler* bs = BarrierSet::barrier_set()-&gt;barrier_set_assembler();
</pre>
<hr />
<pre>
4591   // Only IN_HEAP loads require a thread_tmp register
4592   // WeakHandle::resolve is an indirection like jweak.
4593   access_load_at(T_OBJECT, IN_NATIVE | ON_PHANTOM_OOP_REF,
4594                  rresult, Address(rresult, 0), rtmp, /*tmp_thread*/noreg);
4595   bind(resolved);
4596 }
4597 
4598 void MacroAssembler::load_mirror(Register mirror, Register method, Register tmp) {
4599   // get mirror
4600   const int mirror_offset = in_bytes(Klass::java_mirror_offset());
4601   load_method_holder(mirror, method);
4602   movptr(mirror, Address(mirror, mirror_offset));
4603   resolve_oop_handle(mirror, tmp);
4604 }
4605 
4606 void MacroAssembler::load_method_holder_cld(Register rresult, Register rmethod) {
4607   load_method_holder(rresult, rmethod);
4608   movptr(rresult, Address(rresult, InstanceKlass::class_loader_data_offset()));
4609 }
4610 
<span class="line-added">4611 void MacroAssembler::load_method_holder(Register holder, Register method) {</span>
<span class="line-added">4612   movptr(holder, Address(method, Method::const_offset()));                      // ConstMethod*</span>
<span class="line-added">4613   movptr(holder, Address(holder, ConstMethod::constants_offset()));             // ConstantPool*</span>
<span class="line-added">4614   movptr(holder, Address(holder, ConstantPool::pool_holder_offset_in_bytes())); // InstanceKlass*</span>
<span class="line-added">4615 }</span>
<span class="line-added">4616 </span>
4617 void MacroAssembler::load_metadata(Register dst, Register src) {
4618   if (UseCompressedClassPointers) {
4619     movl(dst, Address(src, oopDesc::klass_offset_in_bytes()));
4620   } else {
4621     movptr(dst, Address(src, oopDesc::klass_offset_in_bytes()));
4622   }
4623 }
4624 
<span class="line-modified">4625 void MacroAssembler::load_klass(Register dst, Register src, Register tmp) {</span>
<span class="line-modified">4626   assert_different_registers(src, tmp);</span>
<span class="line-modified">4627   assert_different_registers(dst, tmp);</span>




4628 #ifdef _LP64
4629   if (UseCompressedClassPointers) {
4630     movl(dst, Address(src, oopDesc::klass_offset_in_bytes()));
<span class="line-modified">4631     decode_klass_not_null(dst, tmp);</span>
4632   } else
4633 #endif
4634   movptr(dst, Address(src, oopDesc::klass_offset_in_bytes()));
4635 }
4636 
<span class="line-modified">4637 void MacroAssembler::load_prototype_header(Register dst, Register src, Register tmp) {</span>
<span class="line-modified">4638   load_klass(dst, src, tmp);</span>
4639   movptr(dst, Address(dst, Klass::prototype_header_offset()));
4640 }
4641 
<span class="line-modified">4642 void MacroAssembler::store_klass(Register dst, Register src, Register tmp) {</span>
<span class="line-added">4643   assert_different_registers(src, tmp);</span>
<span class="line-added">4644   assert_different_registers(dst, tmp);</span>
4645 #ifdef _LP64
4646   if (UseCompressedClassPointers) {
<span class="line-modified">4647     encode_klass_not_null(src, tmp);</span>
4648     movl(Address(dst, oopDesc::klass_offset_in_bytes()), src);
4649   } else
4650 #endif
4651     movptr(Address(dst, oopDesc::klass_offset_in_bytes()), src);
4652 }
4653 
4654 void MacroAssembler::access_load_at(BasicType type, DecoratorSet decorators, Register dst, Address src,
4655                                     Register tmp1, Register thread_tmp) {
4656   BarrierSetAssembler* bs = BarrierSet::barrier_set()-&gt;barrier_set_assembler();
4657   decorators = AccessInternal::decorator_fixup(decorators);
4658   bool as_raw = (decorators &amp; AS_RAW) != 0;
4659   if (as_raw) {
4660     bs-&gt;BarrierSetAssembler::load_at(this, decorators, type, dst, src, tmp1, thread_tmp);
4661   } else {
4662     bs-&gt;load_at(this, decorators, type, dst, src, tmp1, thread_tmp);
4663   }
4664 }
4665 
4666 void MacroAssembler::access_store_at(BasicType type, DecoratorSet decorators, Address dst, Register src,
4667                                      Register tmp1, Register tmp2, Register tmp3) {
</pre>
<hr />
<pre>
4881     assert(LogMinObjAlignmentInBytes == CompressedOops::shift(), &quot;decode alg wrong&quot;);
4882     if (LogMinObjAlignmentInBytes == Address::times_8) {
4883       leaq(dst, Address(r12_heapbase, src, Address::times_8, 0));
4884     } else {
4885       if (dst != src) {
4886         movq(dst, src);
4887       }
4888       shlq(dst, LogMinObjAlignmentInBytes);
4889       if (CompressedOops::base() != NULL) {
4890         addq(dst, r12_heapbase);
4891       }
4892     }
4893   } else {
4894     assert (CompressedOops::base() == NULL, &quot;sanity&quot;);
4895     if (dst != src) {
4896       movq(dst, src);
4897     }
4898   }
4899 }
4900 
<span class="line-modified">4901 void MacroAssembler::encode_klass_not_null(Register r, Register tmp) {</span>
<span class="line-added">4902   assert_different_registers(r, tmp);</span>
4903   if (CompressedKlassPointers::base() != NULL) {
<span class="line-modified">4904     mov64(tmp, (int64_t)CompressedKlassPointers::base());</span>
<span class="line-modified">4905     subq(r, tmp);</span>


4906   }
4907   if (CompressedKlassPointers::shift() != 0) {
4908     assert (LogKlassAlignmentInBytes == CompressedKlassPointers::shift(), &quot;decode alg wrong&quot;);
4909     shrq(r, LogKlassAlignmentInBytes);
4910   }





















4911 }
4912 
<span class="line-modified">4913 void MacroAssembler::encode_and_move_klass_not_null(Register dst, Register src) {</span>
<span class="line-modified">4914   assert_different_registers(src, dst);</span>




4915   if (CompressedKlassPointers::base() != NULL) {
<span class="line-modified">4916     mov64(dst, -(int64_t)CompressedKlassPointers::base());</span>
<span class="line-modified">4917     addq(dst, src);</span>
4918   } else {
<span class="line-modified">4919     movptr(dst, src);</span>
<span class="line-modified">4920   }</span>
<span class="line-added">4921   if (CompressedKlassPointers::shift() != 0) {</span>
<span class="line-added">4922     assert (LogKlassAlignmentInBytes == CompressedKlassPointers::shift(), &quot;decode alg wrong&quot;);</span>
<span class="line-added">4923     shrq(dst, LogKlassAlignmentInBytes);</span>
4924   }
4925 }
4926 
4927 // !!! If the instructions that get generated here change then function
4928 // instr_size_for_decode_klass_not_null() needs to get updated.
<span class="line-modified">4929 void  MacroAssembler::decode_klass_not_null(Register r, Register tmp) {</span>
<span class="line-added">4930   assert_different_registers(r, tmp);</span>
4931   // Note: it will change flags
<span class="line-modified">4932   assert(UseCompressedClassPointers, &quot;should only be used for compressed headers&quot;);</span>

4933   // Cannot assert, unverified entry point counts instructions (see .ad file)
4934   // vtableStubs also counts instructions in pd_code_size_limit.
4935   // Also do not verify_oop as this is called by verify_oop.
4936   if (CompressedKlassPointers::shift() != 0) {
4937     assert(LogKlassAlignmentInBytes == CompressedKlassPointers::shift(), &quot;decode alg wrong&quot;);
4938     shlq(r, LogKlassAlignmentInBytes);
4939   }
<span class="line-modified">4940   if (CompressedKlassPointers::base() != NULL) {</span>
<span class="line-modified">4941     mov64(tmp, (int64_t)CompressedKlassPointers::base());</span>


4942     addq(r, tmp);
4943   }
4944 }
4945 
<span class="line-modified">4946 void  MacroAssembler::decode_and_move_klass_not_null(Register dst, Register src) {</span>
<span class="line-added">4947   assert_different_registers(src, dst);</span>
4948   // Note: it will change flags
4949   assert (UseCompressedClassPointers, &quot;should only be used for compressed headers&quot;);
<span class="line-modified">4950   // Cannot assert, unverified entry point counts instructions (see .ad file)</span>
<span class="line-modified">4951   // vtableStubs also counts instructions in pd_code_size_limit.</span>
<span class="line-added">4952   // Also do not verify_oop as this is called by verify_oop.</span>
<span class="line-added">4953 </span>
<span class="line-added">4954   if (CompressedKlassPointers::base() == NULL &amp;&amp;</span>
<span class="line-added">4955       CompressedKlassPointers::shift() == 0) {</span>
<span class="line-added">4956     // The best case scenario is that there is no base or shift. Then it is already</span>
<span class="line-added">4957     // a pointer that needs nothing but a register rename.</span>
<span class="line-added">4958     movl(dst, src);</span>
4959   } else {
<span class="line-modified">4960     if (CompressedKlassPointers::base() != NULL) {</span>
<span class="line-modified">4961       mov64(dst, (int64_t)CompressedKlassPointers::base());</span>
<span class="line-modified">4962     } else {</span>
<span class="line-modified">4963       xorq(dst, dst);</span>
<span class="line-added">4964     }</span>
4965     if (CompressedKlassPointers::shift() != 0) {
4966       assert(LogKlassAlignmentInBytes == CompressedKlassPointers::shift(), &quot;decode alg wrong&quot;);
4967       assert(LogKlassAlignmentInBytes == Address::times_8, &quot;klass not aligned on 64bits?&quot;);
4968       leaq(dst, Address(dst, src, Address::times_8, 0));
4969     } else {
4970       addq(dst, src);
4971     }
4972   }
4973 }
4974 
4975 void  MacroAssembler::set_narrow_oop(Register dst, jobject obj) {
4976   assert (UseCompressedOops, &quot;should only be used for compressed headers&quot;);
4977   assert (Universe::heap() != NULL, &quot;java heap should be initialized&quot;);
4978   assert (oop_recorder() != NULL, &quot;this assembler needs an OopRecorder&quot;);
4979   int oop_index = oop_recorder()-&gt;find_index(obj);
4980   RelocationHolder rspec = oop_Relocation::spec(oop_index);
4981   mov_narrow_oop(dst, oop_index, rspec);
4982 }
4983 
4984 void  MacroAssembler::set_narrow_oop(Address dst, jobject obj) {
</pre>
<hr />
<pre>
5024   Assembler::cmp_narrow_oop(dst, oop_index, rspec);
5025 }
5026 
5027 void  MacroAssembler::cmp_narrow_klass(Register dst, Klass* k) {
5028   assert (UseCompressedClassPointers, &quot;should only be used for compressed headers&quot;);
5029   assert (oop_recorder() != NULL, &quot;this assembler needs an OopRecorder&quot;);
5030   int klass_index = oop_recorder()-&gt;find_index(k);
5031   RelocationHolder rspec = metadata_Relocation::spec(klass_index);
5032   Assembler::cmp_narrow_oop(dst, CompressedKlassPointers::encode(k), rspec);
5033 }
5034 
5035 void  MacroAssembler::cmp_narrow_klass(Address dst, Klass* k) {
5036   assert (UseCompressedClassPointers, &quot;should only be used for compressed headers&quot;);
5037   assert (oop_recorder() != NULL, &quot;this assembler needs an OopRecorder&quot;);
5038   int klass_index = oop_recorder()-&gt;find_index(k);
5039   RelocationHolder rspec = metadata_Relocation::spec(klass_index);
5040   Assembler::cmp_narrow_oop(dst, CompressedKlassPointers::encode(k), rspec);
5041 }
5042 
5043 void MacroAssembler::reinit_heapbase() {
<span class="line-modified">5044   if (UseCompressedOops) {</span>
5045     if (Universe::heap() != NULL) {
5046       if (CompressedOops::base() == NULL) {
5047         MacroAssembler::xorptr(r12_heapbase, r12_heapbase);
5048       } else {
5049         mov64(r12_heapbase, (int64_t)CompressedOops::ptrs_base());
5050       }
5051     } else {
5052       movptr(r12_heapbase, ExternalAddress((address)CompressedOops::ptrs_base_addr()));
5053     }
5054   }
5055 }
5056 
5057 #endif // _LP64
5058 
5059 // C2 compiled method&#39;s prolog code.
5060 void MacroAssembler::verified_entry(Compile* C, int sp_inc) {
5061   int framesize = C-&gt;output()-&gt;frame_size_in_bytes();
5062   int bangsize = C-&gt;output()-&gt;bang_size_in_bytes();
5063   bool fp_mode_24b = false;
5064   int stack_bang_size = C-&gt;output()-&gt;need_stack_bang(bangsize) ? bangsize : 0;
</pre>
<hr />
<pre>
5230       // Call from interpreter. RAX contains ((the ValueKlass* of the return type) | 0x01)
5231       mov(rbx, rax);
5232       andptr(rbx, -2);
5233       movl(r14, Address(rbx, Klass::layout_helper_offset()));
5234     }
5235 
5236     movptr(r13, Address(r15_thread, in_bytes(JavaThread::tlab_top_offset())));
5237     lea(r14, Address(r13, r14, Address::times_1));
5238     cmpptr(r14, Address(r15_thread, in_bytes(JavaThread::tlab_end_offset())));
5239     jcc(Assembler::above, slow_case);
5240     movptr(Address(r15_thread, in_bytes(JavaThread::tlab_top_offset())), r14);
5241     movptr(Address(r13, oopDesc::mark_offset_in_bytes()), (intptr_t)markWord::always_locked_prototype().value());
5242 
5243     xorl(rax, rax); // use zero reg to clear memory (shorter code)
5244     store_klass_gap(r13, rax);  // zero klass gap for compressed oops
5245 
5246     if (vk == NULL) {
5247       // store_klass corrupts rbx, so save it in rax for later use (interpreter case only).
5248       mov(rax, rbx);
5249     }
<span class="line-modified">5250     Register tmp_store_klass = LP64_ONLY(rscratch1) NOT_LP64(noreg);</span>
<span class="line-added">5251     store_klass(r13, rbx, tmp_store_klass);  // klass</span>
5252 
5253     // We have our new buffered value, initialize its fields with a
5254     // value class specific handler
5255     if (vk != NULL) {
5256       // FIXME -- do the packing in-line to avoid the runtime call
5257       mov(rax, r13);
5258       call(RuntimeAddress(vk-&gt;pack_handler())); // no need for call info as this will not safepoint.
5259     } else {
5260       movptr(rbx, Address(rax, InstanceKlass::adr_valueklass_fixed_block_offset()));
5261       movptr(rbx, Address(rbx, ValueKlass::pack_handler_offset()));
5262       mov(rax, r13);
5263       call(rbx);
5264     }
5265     jmp(skip);
5266   }
5267 
5268   bind(slow_case);
5269   // We failed to allocate a new value, fall back to a runtime
5270   // call. Some oop field may be live in some registers but we can&#39;t
5271   // tell. That runtime call will take care of preserving them
</pre>
</td>
</tr>
</table>
<center><a href="interp_masm_x86.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="macroAssembler_x86.hpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>