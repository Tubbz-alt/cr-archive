<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/share/opto/library_call.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="graphKit.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="macroArrayCopy.cpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/share/opto/library_call.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
 206   };
 207 
 208   Node* generate_hidden_class_guard(Node* kls, RegionNode* region);
 209 
 210   Node* generate_array_guard(Node* kls, RegionNode* region) {
 211     return generate_array_guard_common(kls, region, AnyArray);
 212   }
 213   Node* generate_non_array_guard(Node* kls, RegionNode* region) {
 214     return generate_array_guard_common(kls, region, NonArray);
 215   }
 216   Node* generate_objArray_guard(Node* kls, RegionNode* region) {
 217     return generate_array_guard_common(kls, region, ObjectArray);
 218   }
 219   Node* generate_non_objArray_guard(Node* kls, RegionNode* region) {
 220     return generate_array_guard_common(kls, region, NonObjectArray);
 221   }
 222   Node* generate_typeArray_guard(Node* kls, RegionNode* region) {
 223     return generate_array_guard_common(kls, region, TypeArray);
 224   }
 225   Node* generate_valueArray_guard(Node* kls, RegionNode* region) {
<span class="line-modified"> 226     assert(FlatArrayFlatten, &quot;can never be flattened&quot;);</span>
 227     return generate_array_guard_common(kls, region, ValueArray);
 228   }
 229   Node* generate_array_guard_common(Node* kls, RegionNode* region, ArrayKind kind);
 230   Node* generate_virtual_guard(Node* obj_klass, RegionNode* slow_region);
 231   CallJavaNode* generate_method_call(vmIntrinsics::ID method_id,
 232                                      bool is_virtual = false, bool is_static = false);
 233   CallJavaNode* generate_method_call_static(vmIntrinsics::ID method_id) {
 234     return generate_method_call(method_id, false, true);
 235   }
 236   CallJavaNode* generate_method_call_virtual(vmIntrinsics::ID method_id) {
 237     return generate_method_call(method_id, true, false);
 238   }
 239   Node * load_field_from_object(Node * fromObj, const char * fieldName, const char * fieldTypeString, bool is_exact, bool is_static, ciInstanceKlass * fromKls);
 240   Node * field_address_from_object(Node * fromObj, const char * fieldName, const char * fieldTypeString, bool is_exact, bool is_static, ciInstanceKlass * fromKls);
 241 
 242   Node* make_string_method_node(int opcode, Node* str1_start, Node* cnt1, Node* str2_start, Node* cnt2, StrIntrinsicNode::ArgEnc ae);
 243   bool inline_string_compareTo(StrIntrinsicNode::ArgEnc ae);
 244   bool inline_string_indexOf(StrIntrinsicNode::ArgEnc ae);
 245   bool inline_string_indexOfI(StrIntrinsicNode::ArgEnc ae);
 246   Node* make_indexOf_node(Node* src_start, Node* src_count, Node* tgt_start, Node* tgt_count,
</pre>
<hr />
<pre>
3878 
3879   C-&gt;set_has_split_ifs(true);  // Has chance for split-if optimization
3880   set_result(result);
3881   return true;
3882 }
3883 
3884 //------------------------inline_array_copyOf----------------------------
3885 // public static &lt;T,U&gt; T[] java.util.Arrays.copyOf(     U[] original, int newLength,         Class&lt;? extends T[]&gt; newType);
3886 // public static &lt;T,U&gt; T[] java.util.Arrays.copyOfRange(U[] original, int from,      int to, Class&lt;? extends T[]&gt; newType);
3887 bool LibraryCallKit::inline_array_copyOf(bool is_copyOfRange) {
3888   if (too_many_traps(Deoptimization::Reason_intrinsic))  return false;
3889 
3890   // Get the arguments.
3891   Node* original          = argument(0);
3892   Node* start             = is_copyOfRange? argument(1): intcon(0);
3893   Node* end               = is_copyOfRange? argument(2): argument(1);
3894   Node* array_type_mirror = is_copyOfRange? argument(3): argument(2);
3895 
3896   const TypeAryPtr* original_t = _gvn.type(original)-&gt;isa_aryptr();
3897   const TypeInstPtr* mirror_t = _gvn.type(array_type_mirror)-&gt;isa_instptr();
<span class="line-modified">3898   if (EnableValhalla &amp;&amp; FlatArrayFlatten &amp;&amp;</span>
3899       (original_t == NULL || mirror_t == NULL ||
3900        (mirror_t-&gt;java_mirror_type() == NULL &amp;&amp;
3901         (original_t-&gt;elem()-&gt;isa_valuetype() ||
3902          (original_t-&gt;elem()-&gt;make_oopptr() != NULL &amp;&amp;
3903           original_t-&gt;elem()-&gt;make_oopptr()-&gt;can_be_value_type()))))) {
3904     // We need to know statically if the copy is to a flattened array
3905     // or not but can&#39;t tell.
3906     return false;
3907   }
3908 
3909   Node* newcopy = NULL;
3910 
3911   // Set the original stack and the reexecute bit for the interpreter to reexecute
3912   // the bytecode that invokes Arrays.copyOf if deoptimization happens.
3913   { PreserveReexecuteState preexecs(this);
3914     jvms()-&gt;set_should_reexecute(true);
3915 
3916     array_type_mirror = null_check(array_type_mirror);
3917     original          = null_check(original);
3918 
</pre>
<hr />
<pre>
3948     // loads/stores but it is legal only if we&#39;re sure the
3949     // Arrays.copyOf would succeed. So we need all input arguments
3950     // to the copyOf to be validated, including that the copy to the
3951     // new array won&#39;t trigger an ArrayStoreException. That subtype
3952     // check can be optimized if we know something on the type of
3953     // the input array from type speculation.
3954     if (_gvn.type(klass_node)-&gt;singleton() &amp;&amp; !stopped()) {
3955       ciKlass* subk   = _gvn.type(original_kls)-&gt;is_klassptr()-&gt;klass();
3956       ciKlass* superk = _gvn.type(klass_node)-&gt;is_klassptr()-&gt;klass();
3957 
3958       int test = C-&gt;static_subtype_check(superk, subk);
3959       if (test != Compile::SSC_always_true &amp;&amp; test != Compile::SSC_always_false) {
3960         const TypeOopPtr* t_original = _gvn.type(original)-&gt;is_oopptr();
3961         if (t_original-&gt;speculative_type() != NULL) {
3962           original = maybe_cast_profiled_obj(original, t_original-&gt;speculative_type(), true);
3963           original_kls = load_object_klass(original);
3964         }
3965       }
3966     }
3967 
<span class="line-modified">3968     if (FlatArrayFlatten) {</span>
3969       // Either both or neither new array klass and original array
3970       // klass must be flattened
3971       Node* is_flat = generate_valueArray_guard(klass_node, NULL);
3972       if (!original_t-&gt;is_not_flat()) {
3973         generate_valueArray_guard(original_kls, bailout);
3974       }
3975       if (is_flat != NULL) {
3976         RegionNode* r = new RegionNode(2);
3977         record_for_igvn(r);
3978         r-&gt;init_req(1, control());
3979         set_control(is_flat);
3980         if (!original_t-&gt;is_not_flat()) {
3981           generate_valueArray_guard(original_kls, r);
3982         }
3983         bailout-&gt;add_req(control());
3984         set_control(_gvn.transform(r));
3985       }
3986     }
3987 
3988     // Bail out if either start or end is negative.
</pre>
</td>
<td>
<hr />
<pre>
 206   };
 207 
 208   Node* generate_hidden_class_guard(Node* kls, RegionNode* region);
 209 
 210   Node* generate_array_guard(Node* kls, RegionNode* region) {
 211     return generate_array_guard_common(kls, region, AnyArray);
 212   }
 213   Node* generate_non_array_guard(Node* kls, RegionNode* region) {
 214     return generate_array_guard_common(kls, region, NonArray);
 215   }
 216   Node* generate_objArray_guard(Node* kls, RegionNode* region) {
 217     return generate_array_guard_common(kls, region, ObjectArray);
 218   }
 219   Node* generate_non_objArray_guard(Node* kls, RegionNode* region) {
 220     return generate_array_guard_common(kls, region, NonObjectArray);
 221   }
 222   Node* generate_typeArray_guard(Node* kls, RegionNode* region) {
 223     return generate_array_guard_common(kls, region, TypeArray);
 224   }
 225   Node* generate_valueArray_guard(Node* kls, RegionNode* region) {
<span class="line-modified"> 226     assert(UseFlatArray, &quot;can never be flattened&quot;);</span>
 227     return generate_array_guard_common(kls, region, ValueArray);
 228   }
 229   Node* generate_array_guard_common(Node* kls, RegionNode* region, ArrayKind kind);
 230   Node* generate_virtual_guard(Node* obj_klass, RegionNode* slow_region);
 231   CallJavaNode* generate_method_call(vmIntrinsics::ID method_id,
 232                                      bool is_virtual = false, bool is_static = false);
 233   CallJavaNode* generate_method_call_static(vmIntrinsics::ID method_id) {
 234     return generate_method_call(method_id, false, true);
 235   }
 236   CallJavaNode* generate_method_call_virtual(vmIntrinsics::ID method_id) {
 237     return generate_method_call(method_id, true, false);
 238   }
 239   Node * load_field_from_object(Node * fromObj, const char * fieldName, const char * fieldTypeString, bool is_exact, bool is_static, ciInstanceKlass * fromKls);
 240   Node * field_address_from_object(Node * fromObj, const char * fieldName, const char * fieldTypeString, bool is_exact, bool is_static, ciInstanceKlass * fromKls);
 241 
 242   Node* make_string_method_node(int opcode, Node* str1_start, Node* cnt1, Node* str2_start, Node* cnt2, StrIntrinsicNode::ArgEnc ae);
 243   bool inline_string_compareTo(StrIntrinsicNode::ArgEnc ae);
 244   bool inline_string_indexOf(StrIntrinsicNode::ArgEnc ae);
 245   bool inline_string_indexOfI(StrIntrinsicNode::ArgEnc ae);
 246   Node* make_indexOf_node(Node* src_start, Node* src_count, Node* tgt_start, Node* tgt_count,
</pre>
<hr />
<pre>
3878 
3879   C-&gt;set_has_split_ifs(true);  // Has chance for split-if optimization
3880   set_result(result);
3881   return true;
3882 }
3883 
3884 //------------------------inline_array_copyOf----------------------------
3885 // public static &lt;T,U&gt; T[] java.util.Arrays.copyOf(     U[] original, int newLength,         Class&lt;? extends T[]&gt; newType);
3886 // public static &lt;T,U&gt; T[] java.util.Arrays.copyOfRange(U[] original, int from,      int to, Class&lt;? extends T[]&gt; newType);
3887 bool LibraryCallKit::inline_array_copyOf(bool is_copyOfRange) {
3888   if (too_many_traps(Deoptimization::Reason_intrinsic))  return false;
3889 
3890   // Get the arguments.
3891   Node* original          = argument(0);
3892   Node* start             = is_copyOfRange? argument(1): intcon(0);
3893   Node* end               = is_copyOfRange? argument(2): argument(1);
3894   Node* array_type_mirror = is_copyOfRange? argument(3): argument(2);
3895 
3896   const TypeAryPtr* original_t = _gvn.type(original)-&gt;isa_aryptr();
3897   const TypeInstPtr* mirror_t = _gvn.type(array_type_mirror)-&gt;isa_instptr();
<span class="line-modified">3898   if (EnableValhalla &amp;&amp; UseFlatArray &amp;&amp;</span>
3899       (original_t == NULL || mirror_t == NULL ||
3900        (mirror_t-&gt;java_mirror_type() == NULL &amp;&amp;
3901         (original_t-&gt;elem()-&gt;isa_valuetype() ||
3902          (original_t-&gt;elem()-&gt;make_oopptr() != NULL &amp;&amp;
3903           original_t-&gt;elem()-&gt;make_oopptr()-&gt;can_be_value_type()))))) {
3904     // We need to know statically if the copy is to a flattened array
3905     // or not but can&#39;t tell.
3906     return false;
3907   }
3908 
3909   Node* newcopy = NULL;
3910 
3911   // Set the original stack and the reexecute bit for the interpreter to reexecute
3912   // the bytecode that invokes Arrays.copyOf if deoptimization happens.
3913   { PreserveReexecuteState preexecs(this);
3914     jvms()-&gt;set_should_reexecute(true);
3915 
3916     array_type_mirror = null_check(array_type_mirror);
3917     original          = null_check(original);
3918 
</pre>
<hr />
<pre>
3948     // loads/stores but it is legal only if we&#39;re sure the
3949     // Arrays.copyOf would succeed. So we need all input arguments
3950     // to the copyOf to be validated, including that the copy to the
3951     // new array won&#39;t trigger an ArrayStoreException. That subtype
3952     // check can be optimized if we know something on the type of
3953     // the input array from type speculation.
3954     if (_gvn.type(klass_node)-&gt;singleton() &amp;&amp; !stopped()) {
3955       ciKlass* subk   = _gvn.type(original_kls)-&gt;is_klassptr()-&gt;klass();
3956       ciKlass* superk = _gvn.type(klass_node)-&gt;is_klassptr()-&gt;klass();
3957 
3958       int test = C-&gt;static_subtype_check(superk, subk);
3959       if (test != Compile::SSC_always_true &amp;&amp; test != Compile::SSC_always_false) {
3960         const TypeOopPtr* t_original = _gvn.type(original)-&gt;is_oopptr();
3961         if (t_original-&gt;speculative_type() != NULL) {
3962           original = maybe_cast_profiled_obj(original, t_original-&gt;speculative_type(), true);
3963           original_kls = load_object_klass(original);
3964         }
3965       }
3966     }
3967 
<span class="line-modified">3968     if (UseFlatArray) {</span>
3969       // Either both or neither new array klass and original array
3970       // klass must be flattened
3971       Node* is_flat = generate_valueArray_guard(klass_node, NULL);
3972       if (!original_t-&gt;is_not_flat()) {
3973         generate_valueArray_guard(original_kls, bailout);
3974       }
3975       if (is_flat != NULL) {
3976         RegionNode* r = new RegionNode(2);
3977         record_for_igvn(r);
3978         r-&gt;init_req(1, control());
3979         set_control(is_flat);
3980         if (!original_t-&gt;is_not_flat()) {
3981           generate_valueArray_guard(original_kls, r);
3982         }
3983         bailout-&gt;add_req(control());
3984         set_control(_gvn.transform(r));
3985       }
3986     }
3987 
3988     // Bail out if either start or end is negative.
</pre>
</td>
</tr>
</table>
<center><a href="graphKit.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="macroArrayCopy.cpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>