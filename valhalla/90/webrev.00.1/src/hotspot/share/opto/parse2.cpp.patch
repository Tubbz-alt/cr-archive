diff a/src/hotspot/share/opto/parse2.cpp b/src/hotspot/share/opto/parse2.cpp
--- a/src/hotspot/share/opto/parse2.cpp
+++ b/src/hotspot/share/opto/parse2.cpp
@@ -34,73 +34,364 @@
 #include "opto/addnode.hpp"
 #include "opto/castnode.hpp"
 #include "opto/convertnode.hpp"
 #include "opto/divnode.hpp"
 #include "opto/idealGraphPrinter.hpp"
+#include "opto/idealKit.hpp"
 #include "opto/matcher.hpp"
 #include "opto/memnode.hpp"
 #include "opto/mulnode.hpp"
 #include "opto/opaquenode.hpp"
 #include "opto/parse.hpp"
 #include "opto/runtime.hpp"
+#include "opto/valuetypenode.hpp"
 #include "runtime/deoptimization.hpp"
 #include "runtime/sharedRuntime.hpp"
 
 #ifndef PRODUCT
 extern int explicit_null_checks_inserted,
            explicit_null_checks_elided;
 #endif
 
+Node* Parse::record_profile_for_speculation_at_array_load(Node* ld) {
+  // Feed unused profile data to type speculation
+  if (UseTypeSpeculation && UseArrayLoadStoreProfile) {
+    ciKlass* array_type = NULL;
+    ciKlass* element_type = NULL;
+    ProfilePtrKind element_ptr = ProfileMaybeNull;
+    bool flat_array = true;
+    bool null_free_array = true;
+    method()->array_access_profiled_type(bci(), array_type, element_type, element_ptr, flat_array, null_free_array);
+    if (element_type != NULL || element_ptr != ProfileMaybeNull) {
+      ld = record_profile_for_speculation(ld, element_type, element_ptr);
+    }
+  }
+  return ld;
+}
+
+
 //---------------------------------array_load----------------------------------
 void Parse::array_load(BasicType bt) {
   const Type* elemtype = Type::TOP;
-  bool big_val = bt == T_DOUBLE || bt == T_LONG;
   Node* adr = array_addressing(bt, 0, elemtype);
   if (stopped())  return;     // guaranteed null or range check
 
-  pop();                      // index (already used)
-  Node* array = pop();        // the array itself
+  Node* idx = pop();
+  Node* ary = pop();
+
+  // Handle value type arrays
+  const TypeOopPtr* elemptr = elemtype->make_oopptr();
+  const TypeAryPtr* ary_t = _gvn.type(ary)->is_aryptr();
+  if (elemtype->isa_valuetype() != NULL) {
+    C->set_flattened_accesses();
+    // Load from flattened value type array
+    Node* vt = ValueTypeNode::make_from_flattened(this, elemtype->value_klass(), ary, adr);
+    push(vt);
+    return;
+  } else if (elemptr != NULL && elemptr->is_valuetypeptr() && !elemptr->maybe_null()) {
+    // Load from non-flattened but flattenable value type array (elements can never be null)
+    bt = T_VALUETYPE;
+  } else if (!ary_t->is_not_flat()) {
+    // Cannot statically determine if array is flattened, emit runtime check
+    assert(ValueArrayFlatten && is_reference_type(bt) && elemptr->can_be_value_type() && !ary_t->klass_is_exact() && !ary_t->is_not_null_free() &&
+           (!elemptr->is_valuetypeptr() || elemptr->value_klass()->flatten_array()), "array can't be flattened");
+    IdealKit ideal(this);
+    IdealVariable res(ideal);
+    ideal.declarations_done();
+    ideal.if_then(is_non_flattened_array(ary)); {
+      // non-flattened
+      assert(ideal.ctrl()->in(0)->as_If()->is_non_flattened_array_check(&_gvn), "Should be found");
+      sync_kit(ideal);
+      const TypeAryPtr* adr_type = TypeAryPtr::get_array_body_type(bt);
+      Node* ld = access_load_at(ary, adr, adr_type, elemptr, bt,
+                                IN_HEAP | IS_ARRAY | C2_CONTROL_DEPENDENT_LOAD);
+      ideal.sync_kit(this);
+      ideal.set(res, ld);
+    } ideal.else_(); {
+      // flattened
+      sync_kit(ideal);
+      if (elemptr->is_valuetypeptr()) {
+        // Element type is known, cast and load from flattened representation
+        ciValueKlass* vk = elemptr->value_klass();
+        assert(vk->flatten_array() && elemptr->maybe_null(), "must be a flattenable and nullable array");
+        ciArrayKlass* array_klass = ciArrayKlass::make(vk);
+        const TypeAryPtr* arytype = TypeOopPtr::make_from_klass(array_klass)->isa_aryptr();
+        Node* cast = _gvn.transform(new CheckCastPPNode(control(), ary, arytype));
+        Node* casted_adr = array_element_address(cast, idx, T_VALUETYPE, ary_t->size(), control());
+        // Re-execute flattened array load if buffering triggers deoptimization
+        PreserveReexecuteState preexecs(this);
+        jvms()->set_should_reexecute(true);
+        inc_sp(2);
+        Node* vt = ValueTypeNode::make_from_flattened(this, vk, cast, casted_adr)->buffer(this, false);
+        ideal.set(res, vt);
+        ideal.sync_kit(this);
+      } else {
+        // Element type is unknown, emit runtime call
+        Node* kls = load_object_klass(ary);
+        Node* k_adr = basic_plus_adr(kls, in_bytes(ArrayKlass::element_klass_offset()));
+        Node* elem_klass = _gvn.transform(LoadKlassNode::make(_gvn, NULL, immutable_memory(), k_adr, TypeInstPtr::KLASS));
+        Node* obj_size  = NULL;
+        kill_dead_locals();
+        // Re-execute flattened array load if buffering triggers deoptimization
+        PreserveReexecuteState preexecs(this);
+        jvms()->set_bci(_bci);
+        jvms()->set_should_reexecute(true);
+        inc_sp(2);
+        Node* alloc_obj = new_instance(elem_klass, NULL, &obj_size, /*deoptimize_on_exception=*/true);
+
+        AllocateNode* alloc = AllocateNode::Ideal_allocation(alloc_obj, &_gvn);
+        assert(alloc->maybe_set_complete(&_gvn), "");
+        alloc->initialization()->set_complete_with_arraycopy();
+
+        // This membar keeps this access to an unknown flattened array
+        // correctly ordered with other unknown and known flattened
+        // array accesses.
+        insert_mem_bar_volatile(Op_MemBarCPUOrder, C->get_alias_index(TypeAryPtr::VALUES));
+
+        BarrierSetC2* bs = BarrierSet::barrier_set()->barrier_set_c2();
+        // Unknown value type might contain reference fields
+        if (false && !bs->array_copy_requires_gc_barriers(false, T_OBJECT, false, BarrierSetC2::Parsing)) {
+          // FIXME 8230656 also merge changes from 8238759 in
+          int base_off = sizeof(instanceOopDesc);
+          Node* dst_base = basic_plus_adr(alloc_obj, base_off);
+          Node* countx = obj_size;
+          countx = _gvn.transform(new SubXNode(countx, MakeConX(base_off)));
+          countx = _gvn.transform(new URShiftXNode(countx, intcon(LogBytesPerLong)));
+
+          assert(Klass::_lh_log2_element_size_shift == 0, "use shift in place");
+          Node* lhp = basic_plus_adr(kls, in_bytes(Klass::layout_helper_offset()));
+          Node* elem_shift = make_load(NULL, lhp, TypeInt::INT, T_INT, MemNode::unordered);
+          uint header = arrayOopDesc::base_offset_in_bytes(T_VALUETYPE);
+          Node* base  = basic_plus_adr(ary, header);
+          idx = Compile::conv_I2X_index(&_gvn, idx, TypeInt::POS, control());
+          Node* scale = _gvn.transform(new LShiftXNode(idx, elem_shift));
+          Node* adr = basic_plus_adr(ary, base, scale);
+
+          access_clone(adr, dst_base, countx, false);
+        } else {
+          ideal.sync_kit(this);
+          ideal.make_leaf_call(OptoRuntime::load_unknown_value_Type(),
+                               CAST_FROM_FN_PTR(address, OptoRuntime::load_unknown_value),
+                               "load_unknown_value",
+                               ary, idx, alloc_obj);
+          sync_kit(ideal);
+        }
+
+        // This makes sure no other thread sees a partially initialized buffered value
+        insert_mem_bar_volatile(Op_MemBarStoreStore, Compile::AliasIdxRaw, alloc->proj_out_or_null(AllocateNode::RawAddress));
+
+        // Same as MemBarCPUOrder above: keep this unknown flattened
+        // array access correctly ordered with other flattened array
+        // access
+        insert_mem_bar_volatile(Op_MemBarCPUOrder, C->get_alias_index(TypeAryPtr::VALUES));
+
+        // Prevent any use of the newly allocated value before it is
+        // fully initialized
+        alloc_obj = new CastPPNode(alloc_obj, _gvn.type(alloc_obj), true);
+        alloc_obj->set_req(0, control());
+        alloc_obj = _gvn.transform(alloc_obj);
+
+        const Type* unknown_value = elemptr->is_instptr()->cast_to_flat_array();
+        alloc_obj = _gvn.transform(new CheckCastPPNode(control(), alloc_obj, unknown_value));
+
+        ideal.sync_kit(this);
+        ideal.set(res, alloc_obj);
+      }
+    } ideal.end_if();
+    sync_kit(ideal);
+    Node* ld = _gvn.transform(ideal.value(res));
+    ld = record_profile_for_speculation_at_array_load(ld);
+    push_node(bt, ld);
+    return;
+  }
 
   if (elemtype == TypeInt::BOOL) {
     bt = T_BOOLEAN;
   }
   const TypeAryPtr* adr_type = TypeAryPtr::get_array_body_type(bt);
-
-  Node* ld = access_load_at(array, adr, adr_type, elemtype, bt,
+  Node* ld = access_load_at(ary, adr, adr_type, elemtype, bt,
                             IN_HEAP | IS_ARRAY | C2_CONTROL_DEPENDENT_LOAD);
-  if (big_val) {
-    push_pair(ld);
-  } else {
-    push(ld);
+  if (bt == T_VALUETYPE) {
+    // Loading a non-flattened (but flattenable) value type from an array
+    assert(!gvn().type(ld)->maybe_null(), "value type array elements should never be null");
+    if (elemptr->value_klass()->is_scalarizable()) {
+      ld = ValueTypeNode::make_from_oop(this, ld, elemptr->value_klass());
+    }
   }
+  if (!ld->is_ValueType()) {
+    ld = record_profile_for_speculation_at_array_load(ld);
+  }
+
+  push_node(bt, ld);
 }
 
 
 //--------------------------------array_store----------------------------------
 void Parse::array_store(BasicType bt) {
   const Type* elemtype = Type::TOP;
-  bool big_val = bt == T_DOUBLE || bt == T_LONG;
-  Node* adr = array_addressing(bt, big_val ? 2 : 1, elemtype);
+  Node* adr = array_addressing(bt, type2size[bt], elemtype);
   if (stopped())  return;     // guaranteed null or range check
+  Node* cast_val = NULL;
   if (bt == T_OBJECT) {
-    array_store_check();
+    cast_val = array_store_check();
+    if (stopped()) return;
   }
-  Node* val;                  // Oop to store
-  if (big_val) {
-    val = pop_pair();
-  } else {
-    val = pop();
-  }
-  pop();                      // index (already used)
-  Node* array = pop();        // the array itself
+  Node* val = pop_node(bt); // Value to store
+  Node* idx = pop();        // Index in the array
+  Node* ary = pop();        // The array itself
+
+  const TypeAryPtr* ary_t = _gvn.type(ary)->is_aryptr();
+  const TypeAryPtr* adr_type = TypeAryPtr::get_array_body_type(bt);
 
   if (elemtype == TypeInt::BOOL) {
     bt = T_BOOLEAN;
-  }
-  const TypeAryPtr* adr_type = TypeAryPtr::get_array_body_type(bt);
+  } else if (bt == T_OBJECT) {
+    elemtype = elemtype->make_oopptr();
+    const Type* tval = _gvn.type(cast_val);
+    // We may have lost type information for 'val' here due to the casts
+    // emitted by the array_store_check code (see JDK-6312651)
+    // TODO Remove this code once JDK-6312651 is in.
+    const Type* tval_init = _gvn.type(val);
+    bool can_be_value_type = tval->isa_valuetype() || (tval != TypePtr::NULL_PTR && tval_init->is_oopptr()->can_be_value_type() && tval->is_oopptr()->can_be_value_type());
+    bool not_flattenable = !can_be_value_type || ((tval_init->is_valuetypeptr() || tval_init->isa_valuetype()) && !tval_init->value_klass()->flatten_array());
+
+    if (!ary_t->is_not_null_free() && !can_be_value_type && (!tval->maybe_null() || !tval_init->maybe_null())) {
+      // Storing a non-inline-type, mark array as not null-free.
+      // This is only legal for non-null stores because the array_store_check passes for null.
+      ary_t = ary_t->cast_to_not_null_free();
+      Node* cast = _gvn.transform(new CheckCastPPNode(control(), ary, ary_t));
+      replace_in_map(ary, cast);
+      ary = cast;
+    } else if (!ary_t->is_not_flat() && not_flattenable) {
+      // Storing a non-flattenable value, mark array as not flat.
+      ary_t = ary_t->cast_to_not_flat();
+      if (tval != TypePtr::NULL_PTR) {
+        // For NULL, this transformation is only valid after the null guard below
+        Node* cast = _gvn.transform(new CheckCastPPNode(control(), ary, ary_t));
+        replace_in_map(ary, cast);
+        ary = cast;
+      }
+    }
 
-  access_store_at(array, adr, adr_type, val, elemtype, bt, MO_UNORDERED | IN_HEAP | IS_ARRAY);
+    if (ary_t->elem()->isa_valuetype() != NULL) {
+      // Store to flattened value type array
+      C->set_flattened_accesses();
+      if (!cast_val->is_ValueType()) {
+        inc_sp(3);
+        cast_val = null_check(cast_val);
+        if (stopped()) return;
+        dec_sp(3);
+        cast_val = ValueTypeNode::make_from_oop(this, cast_val, ary_t->elem()->value_klass());
+      }
+      // Re-execute flattened array store if buffering triggers deoptimization
+      PreserveReexecuteState preexecs(this);
+      inc_sp(3);
+      jvms()->set_should_reexecute(true);
+      cast_val->as_ValueType()->store_flattened(this, ary, adr, NULL, 0, MO_UNORDERED | IN_HEAP | IS_ARRAY);
+      return;
+    } else if (elemtype->is_valuetypeptr() && !elemtype->maybe_null()) {
+      // Store to non-flattened but flattenable value type array (elements can never be null)
+      if (!cast_val->is_ValueType() && tval->maybe_null()) {
+        inc_sp(3);
+        cast_val = null_check(cast_val);
+        if (stopped()) return;
+        dec_sp(3);
+      }
+    } else if (!ary_t->is_not_flat()) {
+      // Array might be flattened, emit runtime checks
+      assert(ValueArrayFlatten && !not_flattenable && elemtype->is_oopptr()->can_be_value_type() &&
+             !ary_t->klass_is_exact() && !ary_t->is_not_null_free(), "array can't be flattened");
+      IdealKit ideal(this);
+      ideal.if_then(is_non_flattened_array(ary)); {
+        // non-flattened
+        assert(ideal.ctrl()->in(0)->as_If()->is_non_flattened_array_check(&_gvn), "Should be found");
+        sync_kit(ideal);
+        gen_value_array_null_guard(ary, cast_val, 3);
+        inc_sp(3);
+        access_store_at(ary, adr, adr_type, cast_val, elemtype, bt, MO_UNORDERED | IN_HEAP | IS_ARRAY, false);
+        dec_sp(3);
+        ideal.sync_kit(this);
+      } ideal.else_(); {
+        Node* val = cast_val;
+        // flattened
+        if (!val->is_ValueType() && tval->maybe_null()) {
+          // Add null check
+          sync_kit(ideal);
+          Node* null_ctl = top();
+          val = null_check_oop(val, &null_ctl);
+          if (null_ctl != top()) {
+            PreserveJVMState pjvms(this);
+            inc_sp(3);
+            set_control(null_ctl);
+            uncommon_trap(Deoptimization::Reason_null_check, Deoptimization::Action_none);
+            dec_sp(3);
+          }
+          ideal.sync_kit(this);
+        }
+        // Try to determine the value klass
+        ciValueKlass* vk = NULL;
+        if (tval->isa_valuetype() || tval->is_valuetypeptr()) {
+          vk = tval->value_klass();
+        } else if (tval_init->isa_valuetype() || tval_init->is_valuetypeptr()) {
+          vk = tval_init->value_klass();
+        } else if (elemtype->is_valuetypeptr()) {
+          vk = elemtype->value_klass();
+        }
+        Node* casted_ary = ary;
+        if (vk != NULL && !stopped()) {
+          // Element type is known, cast and store to flattened representation
+          sync_kit(ideal);
+          assert(vk->flatten_array() && elemtype->maybe_null(), "must be a flattenable and nullable array");
+          ciArrayKlass* array_klass = ciArrayKlass::make(vk);
+          const TypeAryPtr* arytype = TypeOopPtr::make_from_klass(array_klass)->isa_aryptr();
+          casted_ary = _gvn.transform(new CheckCastPPNode(control(), casted_ary, arytype));
+          Node* casted_adr = array_element_address(casted_ary, idx, T_OBJECT, arytype->size(), control());
+          if (!val->is_ValueType()) {
+            assert(!gvn().type(val)->maybe_null(), "value type array elements should never be null");
+            val = ValueTypeNode::make_from_oop(this, val, vk);
+          }
+          // Re-execute flattened array store if buffering triggers deoptimization
+          PreserveReexecuteState preexecs(this);
+          inc_sp(3);
+          jvms()->set_should_reexecute(true);
+          val->as_ValueType()->store_flattened(this, casted_ary, casted_adr, NULL, 0, MO_UNORDERED | IN_HEAP | IS_ARRAY);
+          ideal.sync_kit(this);
+        } else if (!ideal.ctrl()->is_top()) {
+          // Element type is unknown, emit runtime call
+          sync_kit(ideal);
+
+          // This membar keeps this access to an unknown flattened
+          // array correctly ordered with other unknown and known
+          // flattened array accesses.
+          insert_mem_bar_volatile(Op_MemBarCPUOrder, C->get_alias_index(TypeAryPtr::VALUES));
+          ideal.sync_kit(this);
+
+          ideal.make_leaf_call(OptoRuntime::store_unknown_value_Type(),
+                               CAST_FROM_FN_PTR(address, OptoRuntime::store_unknown_value),
+                               "store_unknown_value",
+                               val, casted_ary, idx);
+
+          sync_kit(ideal);
+          // Same as MemBarCPUOrder above: keep this unknown
+          // flattened array access correctly ordered with other
+          // flattened array accesses.
+          insert_mem_bar_volatile(Op_MemBarCPUOrder, C->get_alias_index(TypeAryPtr::VALUES));
+          ideal.sync_kit(this);
+        }
+      }
+      ideal.end_if();
+      sync_kit(ideal);
+      return;
+    } else if (!ary_t->is_not_null_free()) {
+      // Array is not flattened but may be null free
+      assert(elemtype->is_oopptr()->can_be_value_type() && !ary_t->klass_is_exact(), "array can't be null free");
+      ary = gen_value_array_null_guard(ary, cast_val, 3, true);
+    }
+  }
+  inc_sp(3);
+  access_store_at(ary, adr, adr_type, val, elemtype, bt, MO_UNORDERED | IN_HEAP | IS_ARRAY);
+  dec_sp(3);
 }
 
 
 //------------------------------array_addressing-------------------------------
 // Pull array and index from the stack.  Compute pointer-to-element.
@@ -196,10 +487,130 @@
     }
   }
   // Check for always knowing you are throwing a range-check exception
   if (stopped())  return top();
 
+  // This could be an access to a value array. We can't tell if it's
+  // flat or not. Speculating it's not leads to a much simpler graph
+  // shape. Check profiling.
+  // For aastore, by the time we're here, the array store check should
+  // have already taken advantage of profiling to cast the array to an
+  // exact type reported by profiling
+  const TypeOopPtr* elemptr = elemtype->make_oopptr();
+  if (elemtype->isa_valuetype() == NULL &&
+      (elemptr == NULL || !elemptr->is_valuetypeptr() || elemptr->maybe_null()) &&
+      !arytype->is_not_flat()) {
+    assert(is_reference_type(type), "Only references");
+    // First check the speculative type
+    Deoptimization::DeoptReason reason = Deoptimization::Reason_speculate_class_check;
+    ciKlass* array_type = arytype->speculative_type();
+    if (too_many_traps_or_recompiles(reason) || array_type == NULL) {
+      // No speculative type, check profile data at this bci
+      array_type = NULL;
+      reason = Deoptimization::Reason_class_check;
+      if (UseArrayLoadStoreProfile && !too_many_traps_or_recompiles(reason)) {
+        ciKlass* element_type = NULL;
+        ProfilePtrKind element_ptr = ProfileMaybeNull;
+        bool flat_array = true;
+        bool null_free_array = true;
+        method()->array_access_profiled_type(bci(), array_type, element_type, element_ptr, flat_array, null_free_array);
+      }
+    }
+    if (array_type != NULL) {
+      // Speculate that this array has the exact type reported by profile data
+      Node* better_ary = NULL;
+      Node* slow_ctl = type_check_receiver(ary, array_type, 1.0, &better_ary);
+      { PreserveJVMState pjvms(this);
+        set_control(slow_ctl);
+        uncommon_trap_exact(reason, Deoptimization::Action_maybe_recompile);
+      }
+      replace_in_map(ary, better_ary);
+      ary = better_ary;
+      arytype  = _gvn.type(ary)->is_aryptr();
+      elemtype = arytype->elem();
+    }
+  } else if (UseTypeSpeculation && UseArrayLoadStoreProfile) {
+    // No need to speculate: feed profile data at this bci for the
+    // array to type speculation
+    ciKlass* array_type = NULL;
+    ciKlass* element_type = NULL;
+    ProfilePtrKind element_ptr = ProfileMaybeNull;
+    bool flat_array = true;
+    bool null_free_array = true;
+    method()->array_access_profiled_type(bci(), array_type, element_type, element_ptr, flat_array, null_free_array);
+    if (array_type != NULL) {
+      record_profile_for_speculation(ary, array_type, ProfileMaybeNull);
+    }
+  }
+
+  // We have no exact array type from profile data. Check profile data
+  // for a non null free or non flat array. Non null free implies non
+  // flat so check this one first. Speculating on a non null free
+  // array doesn't help aaload but could be profitable for a
+  // subsequent aastore.
+  elemptr = elemtype->make_oopptr();
+  if (!arytype->is_not_null_free() &&
+      elemtype->isa_valuetype() == NULL &&
+      (elemptr == NULL || !elemptr->is_valuetypeptr()) &&
+      UseArrayLoadStoreProfile) {
+    assert(is_reference_type(type), "");
+    bool null_free_array = true;
+    Deoptimization::DeoptReason reason = Deoptimization::Reason_none;
+    if (arytype->speculative() != NULL &&
+        arytype->speculative()->is_aryptr()->is_not_null_free() &&
+        !too_many_traps_or_recompiles(Deoptimization::Reason_speculate_class_check)) {
+      null_free_array = false;
+      reason = Deoptimization::Reason_speculate_class_check;
+    } else if (!too_many_traps_or_recompiles(Deoptimization::Reason_class_check)) {
+      ciKlass* array_type = NULL;
+      ciKlass* element_type = NULL;
+      ProfilePtrKind element_ptr = ProfileMaybeNull;
+      bool flat_array = true;
+      method()->array_access_profiled_type(bci(), array_type, element_type, element_ptr, flat_array, null_free_array);
+      reason = Deoptimization::Reason_class_check;
+    }
+    if (!null_free_array) {
+      { // Deoptimize if null-free array
+        BuildCutout unless(this, is_nullable_array(ary), PROB_MAX);
+        uncommon_trap_exact(reason, Deoptimization::Action_maybe_recompile);
+      }
+      Node* better_ary = _gvn.transform(new CheckCastPPNode(control(), ary, arytype->cast_to_not_null_free()));
+      replace_in_map(ary, better_ary);
+      ary = better_ary;
+      arytype = _gvn.type(ary)->is_aryptr();
+    }
+  }
+
+  if (!arytype->is_not_flat() && elemtype->isa_valuetype() == NULL) {
+    assert(is_reference_type(type), "");
+    bool flat_array = true;
+    Deoptimization::DeoptReason reason = Deoptimization::Reason_none;
+    if (arytype->speculative() != NULL &&
+        arytype->speculative()->is_aryptr()->is_not_flat() &&
+        !too_many_traps_or_recompiles(Deoptimization::Reason_speculate_class_check)) {
+      flat_array = false;
+      reason = Deoptimization::Reason_speculate_class_check;
+    } else if (UseArrayLoadStoreProfile && !too_many_traps_or_recompiles(reason)) {
+      ciKlass* array_type = NULL;
+      ciKlass* element_type = NULL;
+      ProfilePtrKind element_ptr = ProfileMaybeNull;
+      bool null_free_array = true;
+      method()->array_access_profiled_type(bci(), array_type, element_type, element_ptr, flat_array, null_free_array);
+      reason = Deoptimization::Reason_class_check;
+    }
+    if (!flat_array) {
+      { // Deoptimize if flat array
+        BuildCutout unless(this, is_non_flattened_array(ary), PROB_MAX);
+        uncommon_trap_exact(reason, Deoptimization::Action_maybe_recompile);
+      }
+      Node* better_ary = _gvn.transform(new CheckCastPPNode(control(), ary, arytype->cast_to_not_flat()));
+      replace_in_map(ary, better_ary);
+      ary = better_ary;
+      arytype = _gvn.type(ary)->is_aryptr();
+    }
+  }
+
   // Make array address computation control dependent to prevent it
   // from floating above the range check during loop optimizations.
   Node* ptr = array_element_address(ary, idx, type, sizetype, control());
   assert(ptr != top(), "top should go hand-in-hand with stopped");
 
@@ -1484,11 +1895,11 @@
         branch_block->next_path_num();
       }
     } else {                    // Path is live.
       // Update method data
       profile_taken_branch(target_bci);
-      adjust_map_after_if(btest, c, prob, branch_block, next_block);
+      adjust_map_after_if(btest, c, prob, branch_block);
       if (!stopped()) {
         merge(target_bci);
       }
     }
   }
@@ -1504,17 +1915,16 @@
       next_block->next_path_num();
     }
   } else  {                     // Path is live.
     // Update method data
     profile_not_taken_branch();
-    adjust_map_after_if(BoolTest(btest).negate(), c, 1.0-prob,
-                        next_block, branch_block);
+    adjust_map_after_if(BoolTest(btest).negate(), c, 1.0-prob, next_block);
   }
 }
 
 //------------------------------------do_if------------------------------------
-void Parse::do_if(BoolTest::mask btest, Node* c) {
+void Parse::do_if(BoolTest::mask btest, Node* c, bool new_path, Node** ctrl_taken) {
   int target_bci = iter().get_dest();
 
   Block* branch_block = successor_for_bci(target_bci);
   Block* next_block   = successor_for_bci(iter().next_bci());
 
@@ -1599,38 +2009,249 @@
   { PreserveJVMState pjvms(this);
     taken_branch = _gvn.transform(taken_branch);
     set_control(taken_branch);
 
     if (stopped()) {
-      if (C->eliminate_boxing()) {
-        // Mark the successor block as parsed
+      if (C->eliminate_boxing() && !new_path) {
+        // Mark the successor block as parsed (if we haven't created a new path)
         branch_block->next_path_num();
       }
     } else {
       // Update method data
       profile_taken_branch(target_bci);
-      adjust_map_after_if(taken_btest, c, prob, branch_block, next_block);
+      adjust_map_after_if(taken_btest, c, prob, branch_block);
       if (!stopped()) {
-        merge(target_bci);
+        if (new_path) {
+          // Merge by using a new path
+          merge_new_path(target_bci);
+        } else if (ctrl_taken != NULL) {
+          // Don't merge but save taken branch to be wired by caller
+          *ctrl_taken = control();
+        } else {
+          merge(target_bci);
+        }
       }
     }
   }
 
   untaken_branch = _gvn.transform(untaken_branch);
   set_control(untaken_branch);
 
   // Branch not taken.
-  if (stopped()) {
+  if (stopped() && ctrl_taken == NULL) {
     if (C->eliminate_boxing()) {
-      // Mark the successor block as parsed
+      // Mark the successor block as parsed (if caller does not re-wire control flow)
       next_block->next_path_num();
     }
   } else {
     // Update method data
     profile_not_taken_branch();
-    adjust_map_after_if(untaken_btest, c, untaken_prob,
-                        next_block, branch_block);
+    adjust_map_after_if(untaken_btest, c, untaken_prob, next_block);
+  }
+}
+
+void Parse::do_acmp(BoolTest::mask btest, Node* a, Node* b) {
+  ciMethod* subst_method = ciEnv::current()->ValueBootstrapMethods_klass()->find_method(ciSymbol::isSubstitutable_name(), ciSymbol::object_object_boolean_signature());
+  // If current method is ValueBootstrapMethods::isSubstitutable(),
+  // compile the acmp as a regular pointer comparison otherwise we
+  // could call ValueBootstrapMethods::isSubstitutable() back
+  if (!EnableValhalla || (method() == subst_method)) {
+    Node* cmp = CmpP(a, b);
+    cmp = optimize_cmp_with_klass(cmp);
+    do_if(btest, cmp);
+    return;
+  }
+
+  // Allocate value type operands and re-execute on deoptimization
+  if (a->is_ValueType()) {
+    PreserveReexecuteState preexecs(this);
+    inc_sp(2);
+    jvms()->set_should_reexecute(true);
+    a = a->as_ValueType()->buffer(this)->get_oop();
+  }
+  if (b->is_ValueType()) {
+    PreserveReexecuteState preexecs(this);
+    inc_sp(2);
+    jvms()->set_should_reexecute(true);
+    b = b->as_ValueType()->buffer(this)->get_oop();
+  }
+
+  // First, do a normal pointer comparison
+  const TypeOopPtr* ta = _gvn.type(a)->isa_oopptr();
+  const TypeOopPtr* tb = _gvn.type(b)->isa_oopptr();
+  Node* cmp = CmpP(a, b);
+  cmp = optimize_cmp_with_klass(cmp);
+  if (ta == NULL || !ta->can_be_value_type() ||
+      tb == NULL || !tb->can_be_value_type()) {
+    // This is sufficient, if one of the operands can't be a value type
+    do_if(btest, cmp);
+    return;
+  }
+  Node* eq_region = NULL;
+  if (btest == BoolTest::eq) {
+    do_if(btest, cmp, true);
+    if (stopped()) {
+      return;
+    }
+  } else {
+    assert(btest == BoolTest::ne, "only eq or ne");
+    Node* is_not_equal = NULL;
+    eq_region = new RegionNode(3);
+    {
+      PreserveJVMState pjvms(this);
+      do_if(btest, cmp, false, &is_not_equal);
+      if (!stopped()) {
+        eq_region->init_req(1, control());
+      }
+    }
+    if (is_not_equal == NULL || is_not_equal->is_top()) {
+      record_for_igvn(eq_region);
+      set_control(_gvn.transform(eq_region));
+      return;
+    }
+    set_control(is_not_equal);
+  }
+
+  // Pointers are not equal, check if first operand is non-null
+  Node* ne_region = new RegionNode(6);
+  inc_sp(2);
+  Node* null_ctl = top();
+  Node* not_null_a = null_check_oop(a, &null_ctl, !too_many_traps(Deoptimization::Reason_null_check), false, false);
+  dec_sp(2);
+  ne_region->init_req(1, null_ctl);
+  if (stopped()) {
+    record_for_igvn(ne_region);
+    set_control(_gvn.transform(ne_region));
+    if (btest == BoolTest::ne) {
+      {
+        PreserveJVMState pjvms(this);
+        int target_bci = iter().get_dest();
+        merge(target_bci);
+      }
+      record_for_igvn(eq_region);
+      set_control(_gvn.transform(eq_region));
+    }
+    return;
+  }
+
+  // First operand is non-null, check if it is a value type
+  Node* is_value = is_value_type(not_null_a);
+  IfNode* is_value_iff = create_and_map_if(control(), is_value, PROB_FAIR, COUNT_UNKNOWN);
+  Node* not_value = _gvn.transform(new IfFalseNode(is_value_iff));
+  ne_region->init_req(2, not_value);
+  set_control(_gvn.transform(new IfTrueNode(is_value_iff)));
+
+  // The first operand is a value type, check if the second operand is non-null
+  inc_sp(2);
+  null_ctl = top();
+  Node* not_null_b = null_check_oop(b, &null_ctl, !too_many_traps(Deoptimization::Reason_null_check), false, false);
+  dec_sp(2);
+  ne_region->init_req(3, null_ctl);
+  if (stopped()) {
+    record_for_igvn(ne_region);
+    set_control(_gvn.transform(ne_region));
+    if (btest == BoolTest::ne) {
+      {
+        PreserveJVMState pjvms(this);
+        int target_bci = iter().get_dest();
+        merge(target_bci);
+      }
+      record_for_igvn(eq_region);
+      set_control(_gvn.transform(eq_region));
+    }
+    return;
+  }
+
+  // Check if both operands are of the same class.
+  Node* kls_a = load_object_klass(not_null_a);
+  Node* kls_b = load_object_klass(not_null_b);
+  Node* kls_cmp = CmpP(kls_a, kls_b);
+  Node* kls_bol = _gvn.transform(new BoolNode(kls_cmp, BoolTest::ne));
+  IfNode* kls_iff = create_and_map_if(control(), kls_bol, PROB_FAIR, COUNT_UNKNOWN);
+  Node* kls_ne = _gvn.transform(new IfTrueNode(kls_iff));
+  set_control(_gvn.transform(new IfFalseNode(kls_iff)));
+  ne_region->init_req(4, kls_ne);
+
+  if (stopped()) {
+    record_for_igvn(ne_region);
+    set_control(_gvn.transform(ne_region));
+    if (btest == BoolTest::ne) {
+      {
+        PreserveJVMState pjvms(this);
+        int target_bci = iter().get_dest();
+        merge(target_bci);
+      }
+      record_for_igvn(eq_region);
+      set_control(_gvn.transform(eq_region));
+    }
+    return;
+  }
+
+  // Both operands are values types of the same class, we need to perform a
+  // substitutability test. Delegate to ValueBootstrapMethods::isSubstitutable().
+  Node* ne_io_phi = PhiNode::make(ne_region, i_o());
+  Node* mem = reset_memory();
+  Node* ne_mem_phi = PhiNode::make(ne_region, mem);
+
+  Node* eq_io_phi = NULL;
+  Node* eq_mem_phi = NULL;
+  if (eq_region != NULL) {
+    eq_io_phi = PhiNode::make(eq_region, i_o());
+    eq_mem_phi = PhiNode::make(eq_region, mem);
+  }
+
+  set_all_memory(mem);
+
+  kill_dead_locals();
+  CallStaticJavaNode *call = new CallStaticJavaNode(C, TypeFunc::make(subst_method), SharedRuntime::get_resolve_static_call_stub(), subst_method, bci());
+  call->set_override_symbolic_info(true);
+  call->init_req(TypeFunc::Parms, not_null_a);
+  call->init_req(TypeFunc::Parms+1, not_null_b);
+  inc_sp(2);
+  set_edges_for_java_call(call, false, false);
+  Node* ret = set_results_for_java_call(call, false, true);
+  dec_sp(2);
+
+  // Test the return value of ValueBootstrapMethods::isSubstitutable()
+  Node* subst_cmp = _gvn.transform(new CmpINode(ret, intcon(1)));
+  Node* ctl = C->top();
+  if (btest == BoolTest::eq) {
+    PreserveJVMState pjvms(this);
+    do_if(btest, subst_cmp);
+    if (!stopped()) {
+      ctl = control();
+    }
+  } else {
+    assert(btest == BoolTest::ne, "only eq or ne");
+    PreserveJVMState pjvms(this);
+    do_if(btest, subst_cmp, false, &ctl);
+    if (!stopped()) {
+      eq_region->init_req(2, control());
+      eq_io_phi->init_req(2, i_o());
+      eq_mem_phi->init_req(2, reset_memory());
+    }
+  }
+  ne_region->init_req(5, ctl);
+  ne_io_phi->init_req(5, i_o());
+  ne_mem_phi->init_req(5, reset_memory());
+
+  record_for_igvn(ne_region);
+  set_control(_gvn.transform(ne_region));
+  set_i_o(_gvn.transform(ne_io_phi));
+  set_all_memory(_gvn.transform(ne_mem_phi));
+
+  if (btest == BoolTest::ne) {
+    {
+      PreserveJVMState pjvms(this);
+      int target_bci = iter().get_dest();
+      merge(target_bci);
+    }
+
+    record_for_igvn(eq_region);
+    set_control(_gvn.transform(eq_region));
+    set_i_o(_gvn.transform(eq_io_phi));
+    set_all_memory(_gvn.transform(eq_mem_phi));
   }
 }
 
 bool Parse::path_is_suitable_for_uncommon_trap(float prob) const {
   // Don't want to speculate on uncommon traps when running with -Xcomp
@@ -1656,12 +2277,11 @@
 // Adjust the JVM state to reflect the result of taking this path.
 // Basically, it means inspecting the CmpNode controlling this
 // branch, seeing how it constrains a tested value, and then
 // deciding if it's worth our while to encode this constraint
 // as graph nodes in the current abstract interpretation map.
-void Parse::adjust_map_after_if(BoolTest::mask btest, Node* c, float prob,
-                                Block* path, Block* other_path) {
+void Parse::adjust_map_after_if(BoolTest::mask btest, Node* c, float prob, Block* path) {
   if (!c->is_Cmp()) {
     maybe_add_predicate_after_if(path);
     return;
   }
 
@@ -1867,10 +2487,14 @@
       if (obj_type->speculative_type_not_null() != NULL) {
         ciKlass* k = obj_type->speculative_type();
         inc_sp(2);
         obj = maybe_cast_profiled_obj(obj, k);
         dec_sp(2);
+        if (obj->is_ValueType()) {
+          assert(obj->as_ValueType()->is_allocated(&_gvn), "must be allocated");
+          obj = obj->as_ValueType()->get_oop();
+        }
         // Make the CmpP use the casted obj
         addp = basic_plus_adr(obj, addp->in(AddPNode::Offset));
         load_klass = load_klass->clone();
         load_klass->set_req(2, addp);
         load_klass = _gvn.transform(load_klass);
@@ -2714,37 +3338,40 @@
   handle_if_null:
     // If this is a backwards branch in the bytecodes, add Safepoint
     maybe_add_safepoint(iter().get_dest());
     a = null();
     b = pop();
-    if (!_gvn.type(b)->speculative_maybe_null() &&
-        !too_many_traps(Deoptimization::Reason_speculate_null_check)) {
-      inc_sp(1);
-      Node* null_ctl = top();
-      b = null_check_oop(b, &null_ctl, true, true, true);
-      assert(null_ctl->is_top(), "no null control here");
-      dec_sp(1);
-    } else if (_gvn.type(b)->speculative_always_null() &&
-               !too_many_traps(Deoptimization::Reason_speculate_null_assert)) {
-      inc_sp(1);
-      b = null_assert(b);
-      dec_sp(1);
-    }
-    c = _gvn.transform( new CmpPNode(b, a) );
+    if (b->is_ValueType()) {
+      // Return constant false because 'b' is always non-null
+      c = _gvn.makecon(TypeInt::CC_GT);
+    } else {
+      if (!_gvn.type(b)->speculative_maybe_null() &&
+          !too_many_traps(Deoptimization::Reason_speculate_null_check)) {
+        inc_sp(1);
+        Node* null_ctl = top();
+        b = null_check_oop(b, &null_ctl, true, true, true);
+        assert(null_ctl->is_top(), "no null control here");
+        dec_sp(1);
+      } else if (_gvn.type(b)->speculative_always_null() &&
+                 !too_many_traps(Deoptimization::Reason_speculate_null_assert)) {
+        inc_sp(1);
+        b = null_assert(b);
+        dec_sp(1);
+      }
+      c = _gvn.transform( new CmpPNode(b, a) );
+    }
     do_ifnull(btest, c);
     break;
 
   case Bytecodes::_if_acmpeq: btest = BoolTest::eq; goto handle_if_acmp;
   case Bytecodes::_if_acmpne: btest = BoolTest::ne; goto handle_if_acmp;
   handle_if_acmp:
     // If this is a backwards branch in the bytecodes, add Safepoint
     maybe_add_safepoint(iter().get_dest());
     a = pop();
     b = pop();
-    c = _gvn.transform( new CmpPNode(b, a) );
-    c = optimize_cmp_with_klass(c);
-    do_if(btest, c);
+    do_acmp(btest, a, b);
     break;
 
   case Bytecodes::_ifeq: btest = BoolTest::eq; goto handle_ifxx;
   case Bytecodes::_ifne: btest = BoolTest::ne; goto handle_ifxx;
   case Bytecodes::_iflt: btest = BoolTest::lt; goto handle_ifxx;
@@ -2795,21 +3422,27 @@
     break;
   case Bytecodes::_instanceof:
     do_instanceof();
     break;
   case Bytecodes::_anewarray:
-    do_anewarray();
+    do_newarray();
     break;
   case Bytecodes::_newarray:
     do_newarray((BasicType)iter().get_index());
     break;
   case Bytecodes::_multianewarray:
     do_multianewarray();
     break;
   case Bytecodes::_new:
     do_new();
     break;
+  case Bytecodes::_defaultvalue:
+    do_defaultvalue();
+    break;
+  case Bytecodes::_withfield:
+    do_withfield();
+    break;
 
   case Bytecodes::_jsr:
   case Bytecodes::_jsr_w:
     do_jsr();
     break;
