<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Old src/hotspot/cpu/x86/macroAssembler_x86.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
  <body>
    <pre>
    1 /*
    2  * Copyright (c) 1997, 2020, Oracle and/or its affiliates. All rights reserved.
    3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
    4  *
    5  * This code is free software; you can redistribute it and/or modify it
    6  * under the terms of the GNU General Public License version 2 only, as
    7  * published by the Free Software Foundation.
    8  *
    9  * This code is distributed in the hope that it will be useful, but WITHOUT
   10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
   11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
   12  * version 2 for more details (a copy is included in the LICENSE file that
   13  * accompanied this code).
   14  *
   15  * You should have received a copy of the GNU General Public License version
   16  * 2 along with this work; if not, write to the Free Software Foundation,
   17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
   18  *
   19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
   20  * or visit www.oracle.com if you need additional information or have any
   21  * questions.
   22  *
   23  */
   24 
   25 #include &quot;precompiled.hpp&quot;
   26 #include &quot;jvm.h&quot;
   27 #include &quot;asm/assembler.hpp&quot;
   28 #include &quot;asm/assembler.inline.hpp&quot;
   29 #include &quot;compiler/disassembler.hpp&quot;
   30 #include &quot;gc/shared/barrierSet.hpp&quot;
   31 #include &quot;gc/shared/barrierSetAssembler.hpp&quot;
   32 #include &quot;gc/shared/collectedHeap.inline.hpp&quot;
   33 #include &quot;interpreter/interpreter.hpp&quot;
   34 #include &quot;memory/resourceArea.hpp&quot;
   35 #include &quot;memory/universe.hpp&quot;
   36 #include &quot;oops/accessDecorators.hpp&quot;
   37 #include &quot;oops/compressedOops.inline.hpp&quot;
   38 #include &quot;oops/klass.inline.hpp&quot;
   39 #include &quot;prims/methodHandles.hpp&quot;
   40 #include &quot;runtime/biasedLocking.hpp&quot;
   41 #include &quot;runtime/flags/flagSetting.hpp&quot;
   42 #include &quot;runtime/interfaceSupport.inline.hpp&quot;
   43 #include &quot;runtime/objectMonitor.hpp&quot;
   44 #include &quot;runtime/os.hpp&quot;
   45 #include &quot;runtime/safepoint.hpp&quot;
   46 #include &quot;runtime/safepointMechanism.hpp&quot;
   47 #include &quot;runtime/sharedRuntime.hpp&quot;
   48 #include &quot;runtime/signature_cc.hpp&quot;
   49 #include &quot;runtime/stubRoutines.hpp&quot;
   50 #include &quot;runtime/thread.hpp&quot;
   51 #include &quot;utilities/macros.hpp&quot;
   52 #include &quot;vmreg_x86.inline.hpp&quot;
   53 #include &quot;crc32c.h&quot;
   54 #ifdef COMPILER2
   55 #include &quot;opto/intrinsicnode.hpp&quot;
   56 #endif
   57 
   58 #ifdef PRODUCT
   59 #define BLOCK_COMMENT(str) /* nothing */
   60 #define STOP(error) stop(error)
   61 #else
   62 #define BLOCK_COMMENT(str) block_comment(str)
   63 #define STOP(error) block_comment(error); stop(error)
   64 #endif
   65 
   66 #define BIND(label) bind(label); BLOCK_COMMENT(#label &quot;:&quot;)
   67 
   68 #ifdef ASSERT
   69 bool AbstractAssembler::pd_check_instruction_mark() { return true; }
   70 #endif
   71 
   72 static Assembler::Condition reverse[] = {
   73     Assembler::noOverflow     /* overflow      = 0x0 */ ,
   74     Assembler::overflow       /* noOverflow    = 0x1 */ ,
   75     Assembler::aboveEqual     /* carrySet      = 0x2, below         = 0x2 */ ,
   76     Assembler::below          /* aboveEqual    = 0x3, carryClear    = 0x3 */ ,
   77     Assembler::notZero        /* zero          = 0x4, equal         = 0x4 */ ,
   78     Assembler::zero           /* notZero       = 0x5, notEqual      = 0x5 */ ,
   79     Assembler::above          /* belowEqual    = 0x6 */ ,
   80     Assembler::belowEqual     /* above         = 0x7 */ ,
   81     Assembler::positive       /* negative      = 0x8 */ ,
   82     Assembler::negative       /* positive      = 0x9 */ ,
   83     Assembler::noParity       /* parity        = 0xa */ ,
   84     Assembler::parity         /* noParity      = 0xb */ ,
   85     Assembler::greaterEqual   /* less          = 0xc */ ,
   86     Assembler::less           /* greaterEqual  = 0xd */ ,
   87     Assembler::greater        /* lessEqual     = 0xe */ ,
   88     Assembler::lessEqual      /* greater       = 0xf, */
   89 
   90 };
   91 
   92 
   93 // Implementation of MacroAssembler
   94 
   95 // First all the versions that have distinct versions depending on 32/64 bit
   96 // Unless the difference is trivial (1 line or so).
   97 
   98 #ifndef _LP64
   99 
  100 // 32bit versions
  101 
  102 Address MacroAssembler::as_Address(AddressLiteral adr) {
  103   return Address(adr.target(), adr.rspec());
  104 }
  105 
  106 Address MacroAssembler::as_Address(ArrayAddress adr) {
  107   return Address::make_array(adr);
  108 }
  109 
  110 void MacroAssembler::call_VM_leaf_base(address entry_point,
  111                                        int number_of_arguments) {
  112   call(RuntimeAddress(entry_point));
  113   increment(rsp, number_of_arguments * wordSize);
  114 }
  115 
  116 void MacroAssembler::cmpklass(Address src1, Metadata* obj) {
  117   cmp_literal32(src1, (int32_t)obj, metadata_Relocation::spec_for_immediate());
  118 }
  119 
  120 void MacroAssembler::cmpklass(Register src1, Metadata* obj) {
  121   cmp_literal32(src1, (int32_t)obj, metadata_Relocation::spec_for_immediate());
  122 }
  123 
  124 void MacroAssembler::cmpoop_raw(Address src1, jobject obj) {
  125   cmp_literal32(src1, (int32_t)obj, oop_Relocation::spec_for_immediate());
  126 }
  127 
  128 void MacroAssembler::cmpoop_raw(Register src1, jobject obj) {
  129   cmp_literal32(src1, (int32_t)obj, oop_Relocation::spec_for_immediate());
  130 }
  131 
  132 void MacroAssembler::cmpoop(Address src1, jobject obj) {
  133   BarrierSetAssembler* bs = BarrierSet::barrier_set()-&gt;barrier_set_assembler();
  134   bs-&gt;obj_equals(this, src1, obj);
  135 }
  136 
  137 void MacroAssembler::cmpoop(Register src1, jobject obj) {
  138   BarrierSetAssembler* bs = BarrierSet::barrier_set()-&gt;barrier_set_assembler();
  139   bs-&gt;obj_equals(this, src1, obj);
  140 }
  141 
  142 void MacroAssembler::extend_sign(Register hi, Register lo) {
  143   // According to Intel Doc. AP-526, &quot;Integer Divide&quot;, p.18.
  144   if (VM_Version::is_P6() &amp;&amp; hi == rdx &amp;&amp; lo == rax) {
  145     cdql();
  146   } else {
  147     movl(hi, lo);
  148     sarl(hi, 31);
  149   }
  150 }
  151 
  152 void MacroAssembler::jC2(Register tmp, Label&amp; L) {
  153   // set parity bit if FPU flag C2 is set (via rax)
  154   save_rax(tmp);
  155   fwait(); fnstsw_ax();
  156   sahf();
  157   restore_rax(tmp);
  158   // branch
  159   jcc(Assembler::parity, L);
  160 }
  161 
  162 void MacroAssembler::jnC2(Register tmp, Label&amp; L) {
  163   // set parity bit if FPU flag C2 is set (via rax)
  164   save_rax(tmp);
  165   fwait(); fnstsw_ax();
  166   sahf();
  167   restore_rax(tmp);
  168   // branch
  169   jcc(Assembler::noParity, L);
  170 }
  171 
  172 // 32bit can do a case table jump in one instruction but we no longer allow the base
  173 // to be installed in the Address class
  174 void MacroAssembler::jump(ArrayAddress entry) {
  175   jmp(as_Address(entry));
  176 }
  177 
  178 // Note: y_lo will be destroyed
  179 void MacroAssembler::lcmp2int(Register x_hi, Register x_lo, Register y_hi, Register y_lo) {
  180   // Long compare for Java (semantics as described in JVM spec.)
  181   Label high, low, done;
  182 
  183   cmpl(x_hi, y_hi);
  184   jcc(Assembler::less, low);
  185   jcc(Assembler::greater, high);
  186   // x_hi is the return register
  187   xorl(x_hi, x_hi);
  188   cmpl(x_lo, y_lo);
  189   jcc(Assembler::below, low);
  190   jcc(Assembler::equal, done);
  191 
  192   bind(high);
  193   xorl(x_hi, x_hi);
  194   increment(x_hi);
  195   jmp(done);
  196 
  197   bind(low);
  198   xorl(x_hi, x_hi);
  199   decrementl(x_hi);
  200 
  201   bind(done);
  202 }
  203 
  204 void MacroAssembler::lea(Register dst, AddressLiteral src) {
  205     mov_literal32(dst, (int32_t)src.target(), src.rspec());
  206 }
  207 
  208 void MacroAssembler::lea(Address dst, AddressLiteral adr) {
  209   // leal(dst, as_Address(adr));
  210   // see note in movl as to why we must use a move
  211   mov_literal32(dst, (int32_t) adr.target(), adr.rspec());
  212 }
  213 
  214 void MacroAssembler::leave() {
  215   mov(rsp, rbp);
  216   pop(rbp);
  217 }
  218 
  219 void MacroAssembler::lmul(int x_rsp_offset, int y_rsp_offset) {
  220   // Multiplication of two Java long values stored on the stack
  221   // as illustrated below. Result is in rdx:rax.
  222   //
  223   // rsp ---&gt; [  ??  ] \               \
  224   //            ....    | y_rsp_offset  |
  225   //          [ y_lo ] /  (in bytes)    | x_rsp_offset
  226   //          [ y_hi ]                  | (in bytes)
  227   //            ....                    |
  228   //          [ x_lo ]                 /
  229   //          [ x_hi ]
  230   //            ....
  231   //
  232   // Basic idea: lo(result) = lo(x_lo * y_lo)
  233   //             hi(result) = hi(x_lo * y_lo) + lo(x_hi * y_lo) + lo(x_lo * y_hi)
  234   Address x_hi(rsp, x_rsp_offset + wordSize); Address x_lo(rsp, x_rsp_offset);
  235   Address y_hi(rsp, y_rsp_offset + wordSize); Address y_lo(rsp, y_rsp_offset);
  236   Label quick;
  237   // load x_hi, y_hi and check if quick
  238   // multiplication is possible
  239   movl(rbx, x_hi);
  240   movl(rcx, y_hi);
  241   movl(rax, rbx);
  242   orl(rbx, rcx);                                 // rbx, = 0 &lt;=&gt; x_hi = 0 and y_hi = 0
  243   jcc(Assembler::zero, quick);                   // if rbx, = 0 do quick multiply
  244   // do full multiplication
  245   // 1st step
  246   mull(y_lo);                                    // x_hi * y_lo
  247   movl(rbx, rax);                                // save lo(x_hi * y_lo) in rbx,
  248   // 2nd step
  249   movl(rax, x_lo);
  250   mull(rcx);                                     // x_lo * y_hi
  251   addl(rbx, rax);                                // add lo(x_lo * y_hi) to rbx,
  252   // 3rd step
  253   bind(quick);                                   // note: rbx, = 0 if quick multiply!
  254   movl(rax, x_lo);
  255   mull(y_lo);                                    // x_lo * y_lo
  256   addl(rdx, rbx);                                // correct hi(x_lo * y_lo)
  257 }
  258 
  259 void MacroAssembler::lneg(Register hi, Register lo) {
  260   negl(lo);
  261   adcl(hi, 0);
  262   negl(hi);
  263 }
  264 
  265 void MacroAssembler::lshl(Register hi, Register lo) {
  266   // Java shift left long support (semantics as described in JVM spec., p.305)
  267   // (basic idea for shift counts s &gt;= n: x &lt;&lt; s == (x &lt;&lt; n) &lt;&lt; (s - n))
  268   // shift value is in rcx !
  269   assert(hi != rcx, &quot;must not use rcx&quot;);
  270   assert(lo != rcx, &quot;must not use rcx&quot;);
  271   const Register s = rcx;                        // shift count
  272   const int      n = BitsPerWord;
  273   Label L;
  274   andl(s, 0x3f);                                 // s := s &amp; 0x3f (s &lt; 0x40)
  275   cmpl(s, n);                                    // if (s &lt; n)
  276   jcc(Assembler::less, L);                       // else (s &gt;= n)
  277   movl(hi, lo);                                  // x := x &lt;&lt; n
  278   xorl(lo, lo);
  279   // Note: subl(s, n) is not needed since the Intel shift instructions work rcx mod n!
  280   bind(L);                                       // s (mod n) &lt; n
  281   shldl(hi, lo);                                 // x := x &lt;&lt; s
  282   shll(lo);
  283 }
  284 
  285 
  286 void MacroAssembler::lshr(Register hi, Register lo, bool sign_extension) {
  287   // Java shift right long support (semantics as described in JVM spec., p.306 &amp; p.310)
  288   // (basic idea for shift counts s &gt;= n: x &gt;&gt; s == (x &gt;&gt; n) &gt;&gt; (s - n))
  289   assert(hi != rcx, &quot;must not use rcx&quot;);
  290   assert(lo != rcx, &quot;must not use rcx&quot;);
  291   const Register s = rcx;                        // shift count
  292   const int      n = BitsPerWord;
  293   Label L;
  294   andl(s, 0x3f);                                 // s := s &amp; 0x3f (s &lt; 0x40)
  295   cmpl(s, n);                                    // if (s &lt; n)
  296   jcc(Assembler::less, L);                       // else (s &gt;= n)
  297   movl(lo, hi);                                  // x := x &gt;&gt; n
  298   if (sign_extension) sarl(hi, 31);
  299   else                xorl(hi, hi);
  300   // Note: subl(s, n) is not needed since the Intel shift instructions work rcx mod n!
  301   bind(L);                                       // s (mod n) &lt; n
  302   shrdl(lo, hi);                                 // x := x &gt;&gt; s
  303   if (sign_extension) sarl(hi);
  304   else                shrl(hi);
  305 }
  306 
  307 void MacroAssembler::movoop(Register dst, jobject obj) {
  308   mov_literal32(dst, (int32_t)obj, oop_Relocation::spec_for_immediate());
  309 }
  310 
  311 void MacroAssembler::movoop(Address dst, jobject obj) {
  312   mov_literal32(dst, (int32_t)obj, oop_Relocation::spec_for_immediate());
  313 }
  314 
  315 void MacroAssembler::mov_metadata(Register dst, Metadata* obj) {
  316   mov_literal32(dst, (int32_t)obj, metadata_Relocation::spec_for_immediate());
  317 }
  318 
  319 void MacroAssembler::mov_metadata(Address dst, Metadata* obj) {
  320   mov_literal32(dst, (int32_t)obj, metadata_Relocation::spec_for_immediate());
  321 }
  322 
  323 void MacroAssembler::movptr(Register dst, AddressLiteral src, Register scratch) {
  324   // scratch register is not used,
  325   // it is defined to match parameters of 64-bit version of this method.
  326   if (src.is_lval()) {
  327     mov_literal32(dst, (intptr_t)src.target(), src.rspec());
  328   } else {
  329     movl(dst, as_Address(src));
  330   }
  331 }
  332 
  333 void MacroAssembler::movptr(ArrayAddress dst, Register src) {
  334   movl(as_Address(dst), src);
  335 }
  336 
  337 void MacroAssembler::movptr(Register dst, ArrayAddress src) {
  338   movl(dst, as_Address(src));
  339 }
  340 
  341 // src should NEVER be a real pointer. Use AddressLiteral for true pointers
  342 void MacroAssembler::movptr(Address dst, intptr_t src) {
  343   movl(dst, src);
  344 }
  345 
  346 
  347 void MacroAssembler::pop_callee_saved_registers() {
  348   pop(rcx);
  349   pop(rdx);
  350   pop(rdi);
  351   pop(rsi);
  352 }
  353 
  354 void MacroAssembler::push_callee_saved_registers() {
  355   push(rsi);
  356   push(rdi);
  357   push(rdx);
  358   push(rcx);
  359 }
  360 
  361 void MacroAssembler::pushoop(jobject obj) {
  362   push_literal32((int32_t)obj, oop_Relocation::spec_for_immediate());
  363 }
  364 
  365 void MacroAssembler::pushklass(Metadata* obj) {
  366   push_literal32((int32_t)obj, metadata_Relocation::spec_for_immediate());
  367 }
  368 
  369 void MacroAssembler::pushptr(AddressLiteral src) {
  370   if (src.is_lval()) {
  371     push_literal32((int32_t)src.target(), src.rspec());
  372   } else {
  373     pushl(as_Address(src));
  374   }
  375 }
  376 
  377 void MacroAssembler::set_word_if_not_zero(Register dst) {
  378   xorl(dst, dst);
  379   set_byte_if_not_zero(dst);
  380 }
  381 
  382 static void pass_arg0(MacroAssembler* masm, Register arg) {
  383   masm-&gt;push(arg);
  384 }
  385 
  386 static void pass_arg1(MacroAssembler* masm, Register arg) {
  387   masm-&gt;push(arg);
  388 }
  389 
  390 static void pass_arg2(MacroAssembler* masm, Register arg) {
  391   masm-&gt;push(arg);
  392 }
  393 
  394 static void pass_arg3(MacroAssembler* masm, Register arg) {
  395   masm-&gt;push(arg);
  396 }
  397 
  398 #ifndef PRODUCT
  399 extern &quot;C&quot; void findpc(intptr_t x);
  400 #endif
  401 
  402 void MacroAssembler::debug32(int rdi, int rsi, int rbp, int rsp, int rbx, int rdx, int rcx, int rax, int eip, char* msg) {
  403   // In order to get locks to work, we need to fake a in_VM state
  404   JavaThread* thread = JavaThread::current();
  405   JavaThreadState saved_state = thread-&gt;thread_state();
  406   thread-&gt;set_thread_state(_thread_in_vm);
  407   if (ShowMessageBoxOnError) {
  408     JavaThread* thread = JavaThread::current();
  409     JavaThreadState saved_state = thread-&gt;thread_state();
  410     thread-&gt;set_thread_state(_thread_in_vm);
  411     if (CountBytecodes || TraceBytecodes || StopInterpreterAt) {
  412       ttyLocker ttyl;
  413       BytecodeCounter::print();
  414     }
  415     // To see where a verify_oop failed, get $ebx+40/X for this frame.
  416     // This is the value of eip which points to where verify_oop will return.
  417     if (os::message_box(msg, &quot;Execution stopped, print registers?&quot;)) {
  418       print_state32(rdi, rsi, rbp, rsp, rbx, rdx, rcx, rax, eip);
  419       BREAKPOINT;
  420     }
  421   }
  422   fatal(&quot;DEBUG MESSAGE: %s&quot;, msg);
  423 }
  424 
  425 void MacroAssembler::print_state32(int rdi, int rsi, int rbp, int rsp, int rbx, int rdx, int rcx, int rax, int eip) {
  426   ttyLocker ttyl;
  427   FlagSetting fs(Debugging, true);
  428   tty-&gt;print_cr(&quot;eip = 0x%08x&quot;, eip);
  429 #ifndef PRODUCT
  430   if ((WizardMode || Verbose) &amp;&amp; PrintMiscellaneous) {
  431     tty-&gt;cr();
  432     findpc(eip);
  433     tty-&gt;cr();
  434   }
  435 #endif
  436 #define PRINT_REG(rax) \
  437   { tty-&gt;print(&quot;%s = &quot;, #rax); os::print_location(tty, rax); }
  438   PRINT_REG(rax);
  439   PRINT_REG(rbx);
  440   PRINT_REG(rcx);
  441   PRINT_REG(rdx);
  442   PRINT_REG(rdi);
  443   PRINT_REG(rsi);
  444   PRINT_REG(rbp);
  445   PRINT_REG(rsp);
  446 #undef PRINT_REG
  447   // Print some words near top of staack.
  448   int* dump_sp = (int*) rsp;
  449   for (int col1 = 0; col1 &lt; 8; col1++) {
  450     tty-&gt;print(&quot;(rsp+0x%03x) 0x%08x: &quot;, (int)((intptr_t)dump_sp - (intptr_t)rsp), (intptr_t)dump_sp);
  451     os::print_location(tty, *dump_sp++);
  452   }
  453   for (int row = 0; row &lt; 16; row++) {
  454     tty-&gt;print(&quot;(rsp+0x%03x) 0x%08x: &quot;, (int)((intptr_t)dump_sp - (intptr_t)rsp), (intptr_t)dump_sp);
  455     for (int col = 0; col &lt; 8; col++) {
  456       tty-&gt;print(&quot; 0x%08x&quot;, *dump_sp++);
  457     }
  458     tty-&gt;cr();
  459   }
  460   // Print some instructions around pc:
  461   Disassembler::decode((address)eip-64, (address)eip);
  462   tty-&gt;print_cr(&quot;--------&quot;);
  463   Disassembler::decode((address)eip, (address)eip+32);
  464 }
  465 
  466 void MacroAssembler::stop(const char* msg) {
  467   ExternalAddress message((address)msg);
  468   // push address of message
  469   pushptr(message.addr());
  470   { Label L; call(L, relocInfo::none); bind(L); }     // push eip
  471   pusha();                                            // push registers
  472   call(RuntimeAddress(CAST_FROM_FN_PTR(address, MacroAssembler::debug32)));
  473   hlt();
  474 }
  475 
  476 void MacroAssembler::warn(const char* msg) {
  477   push_CPU_state();
  478 
  479   ExternalAddress message((address) msg);
  480   // push address of message
  481   pushptr(message.addr());
  482 
  483   call(RuntimeAddress(CAST_FROM_FN_PTR(address, warning)));
  484   addl(rsp, wordSize);       // discard argument
  485   pop_CPU_state();
  486 }
  487 
  488 void MacroAssembler::print_state() {
  489   { Label L; call(L, relocInfo::none); bind(L); }     // push eip
  490   pusha();                                            // push registers
  491 
  492   push_CPU_state();
  493   call(RuntimeAddress(CAST_FROM_FN_PTR(address, MacroAssembler::print_state32)));
  494   pop_CPU_state();
  495 
  496   popa();
  497   addl(rsp, wordSize);
  498 }
  499 
  500 #else // _LP64
  501 
  502 // 64 bit versions
  503 
  504 Address MacroAssembler::as_Address(AddressLiteral adr) {
  505   // amd64 always does this as a pc-rel
  506   // we can be absolute or disp based on the instruction type
  507   // jmp/call are displacements others are absolute
  508   assert(!adr.is_lval(), &quot;must be rval&quot;);
  509   assert(reachable(adr), &quot;must be&quot;);
  510   return Address((int32_t)(intptr_t)(adr.target() - pc()), adr.target(), adr.reloc());
  511 
  512 }
  513 
  514 Address MacroAssembler::as_Address(ArrayAddress adr) {
  515   AddressLiteral base = adr.base();
  516   lea(rscratch1, base);
  517   Address index = adr.index();
  518   assert(index._disp == 0, &quot;must not have disp&quot;); // maybe it can?
  519   Address array(rscratch1, index._index, index._scale, index._disp);
  520   return array;
  521 }
  522 
  523 void MacroAssembler::call_VM_leaf_base(address entry_point, int num_args) {
  524   Label L, E;
  525 
  526 #ifdef _WIN64
  527   // Windows always allocates space for it&#39;s register args
  528   assert(num_args &lt;= 4, &quot;only register arguments supported&quot;);
  529   subq(rsp,  frame::arg_reg_save_area_bytes);
  530 #endif
  531 
  532   // Align stack if necessary
  533   testl(rsp, 15);
  534   jcc(Assembler::zero, L);
  535 
  536   subq(rsp, 8);
  537   {
  538     call(RuntimeAddress(entry_point));
  539   }
  540   addq(rsp, 8);
  541   jmp(E);
  542 
  543   bind(L);
  544   {
  545     call(RuntimeAddress(entry_point));
  546   }
  547 
  548   bind(E);
  549 
  550 #ifdef _WIN64
  551   // restore stack pointer
  552   addq(rsp, frame::arg_reg_save_area_bytes);
  553 #endif
  554 
  555 }
  556 
  557 void MacroAssembler::cmp64(Register src1, AddressLiteral src2) {
  558   assert(!src2.is_lval(), &quot;should use cmpptr&quot;);
  559 
  560   if (reachable(src2)) {
  561     cmpq(src1, as_Address(src2));
  562   } else {
  563     lea(rscratch1, src2);
  564     Assembler::cmpq(src1, Address(rscratch1, 0));
  565   }
  566 }
  567 
  568 int MacroAssembler::corrected_idivq(Register reg) {
  569   // Full implementation of Java ldiv and lrem; checks for special
  570   // case as described in JVM spec., p.243 &amp; p.271.  The function
  571   // returns the (pc) offset of the idivl instruction - may be needed
  572   // for implicit exceptions.
  573   //
  574   //         normal case                           special case
  575   //
  576   // input : rax: dividend                         min_long
  577   //         reg: divisor   (may not be eax/edx)   -1
  578   //
  579   // output: rax: quotient  (= rax idiv reg)       min_long
  580   //         rdx: remainder (= rax irem reg)       0
  581   assert(reg != rax &amp;&amp; reg != rdx, &quot;reg cannot be rax or rdx register&quot;);
  582   static const int64_t min_long = 0x8000000000000000;
  583   Label normal_case, special_case;
  584 
  585   // check for special case
  586   cmp64(rax, ExternalAddress((address) &amp;min_long));
  587   jcc(Assembler::notEqual, normal_case);
  588   xorl(rdx, rdx); // prepare rdx for possible special case (where
  589                   // remainder = 0)
  590   cmpq(reg, -1);
  591   jcc(Assembler::equal, special_case);
  592 
  593   // handle normal case
  594   bind(normal_case);
  595   cdqq();
  596   int idivq_offset = offset();
  597   idivq(reg);
  598 
  599   // normal and special case exit
  600   bind(special_case);
  601 
  602   return idivq_offset;
  603 }
  604 
  605 void MacroAssembler::decrementq(Register reg, int value) {
  606   if (value == min_jint) { subq(reg, value); return; }
  607   if (value &lt;  0) { incrementq(reg, -value); return; }
  608   if (value == 0) {                        ; return; }
  609   if (value == 1 &amp;&amp; UseIncDec) { decq(reg) ; return; }
  610   /* else */      { subq(reg, value)       ; return; }
  611 }
  612 
  613 void MacroAssembler::decrementq(Address dst, int value) {
  614   if (value == min_jint) { subq(dst, value); return; }
  615   if (value &lt;  0) { incrementq(dst, -value); return; }
  616   if (value == 0) {                        ; return; }
  617   if (value == 1 &amp;&amp; UseIncDec) { decq(dst) ; return; }
  618   /* else */      { subq(dst, value)       ; return; }
  619 }
  620 
  621 void MacroAssembler::incrementq(AddressLiteral dst) {
  622   if (reachable(dst)) {
  623     incrementq(as_Address(dst));
  624   } else {
  625     lea(rscratch1, dst);
  626     incrementq(Address(rscratch1, 0));
  627   }
  628 }
  629 
  630 void MacroAssembler::incrementq(Register reg, int value) {
  631   if (value == min_jint) { addq(reg, value); return; }
  632   if (value &lt;  0) { decrementq(reg, -value); return; }
  633   if (value == 0) {                        ; return; }
  634   if (value == 1 &amp;&amp; UseIncDec) { incq(reg) ; return; }
  635   /* else */      { addq(reg, value)       ; return; }
  636 }
  637 
  638 void MacroAssembler::incrementq(Address dst, int value) {
  639   if (value == min_jint) { addq(dst, value); return; }
  640   if (value &lt;  0) { decrementq(dst, -value); return; }
  641   if (value == 0) {                        ; return; }
  642   if (value == 1 &amp;&amp; UseIncDec) { incq(dst) ; return; }
  643   /* else */      { addq(dst, value)       ; return; }
  644 }
  645 
  646 // 32bit can do a case table jump in one instruction but we no longer allow the base
  647 // to be installed in the Address class
  648 void MacroAssembler::jump(ArrayAddress entry) {
  649   lea(rscratch1, entry.base());
  650   Address dispatch = entry.index();
  651   assert(dispatch._base == noreg, &quot;must be&quot;);
  652   dispatch._base = rscratch1;
  653   jmp(dispatch);
  654 }
  655 
  656 void MacroAssembler::lcmp2int(Register x_hi, Register x_lo, Register y_hi, Register y_lo) {
  657   ShouldNotReachHere(); // 64bit doesn&#39;t use two regs
  658   cmpq(x_lo, y_lo);
  659 }
  660 
  661 void MacroAssembler::lea(Register dst, AddressLiteral src) {
  662     mov_literal64(dst, (intptr_t)src.target(), src.rspec());
  663 }
  664 
  665 void MacroAssembler::lea(Address dst, AddressLiteral adr) {
  666   mov_literal64(rscratch1, (intptr_t)adr.target(), adr.rspec());
  667   movptr(dst, rscratch1);
  668 }
  669 
  670 void MacroAssembler::leave() {
  671   // %%% is this really better? Why not on 32bit too?
  672   emit_int8((unsigned char)0xC9); // LEAVE
  673 }
  674 
  675 void MacroAssembler::lneg(Register hi, Register lo) {
  676   ShouldNotReachHere(); // 64bit doesn&#39;t use two regs
  677   negq(lo);
  678 }
  679 
  680 void MacroAssembler::movoop(Register dst, jobject obj) {
  681   mov_literal64(dst, (intptr_t)obj, oop_Relocation::spec_for_immediate());
  682 }
  683 
  684 void MacroAssembler::movoop(Address dst, jobject obj) {
  685   mov_literal64(rscratch1, (intptr_t)obj, oop_Relocation::spec_for_immediate());
  686   movq(dst, rscratch1);
  687 }
  688 
  689 void MacroAssembler::mov_metadata(Register dst, Metadata* obj) {
  690   mov_literal64(dst, (intptr_t)obj, metadata_Relocation::spec_for_immediate());
  691 }
  692 
  693 void MacroAssembler::mov_metadata(Address dst, Metadata* obj) {
  694   mov_literal64(rscratch1, (intptr_t)obj, metadata_Relocation::spec_for_immediate());
  695   movq(dst, rscratch1);
  696 }
  697 
  698 void MacroAssembler::movptr(Register dst, AddressLiteral src, Register scratch) {
  699   if (src.is_lval()) {
  700     mov_literal64(dst, (intptr_t)src.target(), src.rspec());
  701   } else {
  702     if (reachable(src)) {
  703       movq(dst, as_Address(src));
  704     } else {
  705       lea(scratch, src);
  706       movq(dst, Address(scratch, 0));
  707     }
  708   }
  709 }
  710 
  711 void MacroAssembler::movptr(ArrayAddress dst, Register src) {
  712   movq(as_Address(dst), src);
  713 }
  714 
  715 void MacroAssembler::movptr(Register dst, ArrayAddress src) {
  716   movq(dst, as_Address(src));
  717 }
  718 
  719 // src should NEVER be a real pointer. Use AddressLiteral for true pointers
  720 void MacroAssembler::movptr(Address dst, intptr_t src) {
  721   mov64(rscratch1, src);
  722   movq(dst, rscratch1);
  723 }
  724 
  725 // These are mostly for initializing NULL
  726 void MacroAssembler::movptr(Address dst, int32_t src) {
  727   movslq(dst, src);
  728 }
  729 
  730 void MacroAssembler::movptr(Register dst, int32_t src) {
  731   mov64(dst, (intptr_t)src);
  732 }
  733 
  734 void MacroAssembler::pushoop(jobject obj) {
  735   movoop(rscratch1, obj);
  736   push(rscratch1);
  737 }
  738 
  739 void MacroAssembler::pushklass(Metadata* obj) {
  740   mov_metadata(rscratch1, obj);
  741   push(rscratch1);
  742 }
  743 
  744 void MacroAssembler::pushptr(AddressLiteral src) {
  745   lea(rscratch1, src);
  746   if (src.is_lval()) {
  747     push(rscratch1);
  748   } else {
  749     pushq(Address(rscratch1, 0));
  750   }
  751 }
  752 
  753 void MacroAssembler::reset_last_Java_frame(bool clear_fp) {
  754   // we must set sp to zero to clear frame
  755   movptr(Address(r15_thread, JavaThread::last_Java_sp_offset()), NULL_WORD);
  756   // must clear fp, so that compiled frames are not confused; it is
  757   // possible that we need it only for debugging
  758   if (clear_fp) {
  759     movptr(Address(r15_thread, JavaThread::last_Java_fp_offset()), NULL_WORD);
  760   }
  761 
  762   // Always clear the pc because it could have been set by make_walkable()
  763   movptr(Address(r15_thread, JavaThread::last_Java_pc_offset()), NULL_WORD);
  764   vzeroupper();
  765 }
  766 
  767 void MacroAssembler::set_last_Java_frame(Register last_java_sp,
  768                                          Register last_java_fp,
  769                                          address  last_java_pc) {
  770   vzeroupper();
  771   // determine last_java_sp register
  772   if (!last_java_sp-&gt;is_valid()) {
  773     last_java_sp = rsp;
  774   }
  775 
  776   // last_java_fp is optional
  777   if (last_java_fp-&gt;is_valid()) {
  778     movptr(Address(r15_thread, JavaThread::last_Java_fp_offset()),
  779            last_java_fp);
  780   }
  781 
  782   // last_java_pc is optional
  783   if (last_java_pc != NULL) {
  784     Address java_pc(r15_thread,
  785                     JavaThread::frame_anchor_offset() + JavaFrameAnchor::last_Java_pc_offset());
  786     lea(rscratch1, InternalAddress(last_java_pc));
  787     movptr(java_pc, rscratch1);
  788   }
  789 
  790   movptr(Address(r15_thread, JavaThread::last_Java_sp_offset()), last_java_sp);
  791 }
  792 
  793 static void pass_arg0(MacroAssembler* masm, Register arg) {
  794   if (c_rarg0 != arg ) {
  795     masm-&gt;mov(c_rarg0, arg);
  796   }
  797 }
  798 
  799 static void pass_arg1(MacroAssembler* masm, Register arg) {
  800   if (c_rarg1 != arg ) {
  801     masm-&gt;mov(c_rarg1, arg);
  802   }
  803 }
  804 
  805 static void pass_arg2(MacroAssembler* masm, Register arg) {
  806   if (c_rarg2 != arg ) {
  807     masm-&gt;mov(c_rarg2, arg);
  808   }
  809 }
  810 
  811 static void pass_arg3(MacroAssembler* masm, Register arg) {
  812   if (c_rarg3 != arg ) {
  813     masm-&gt;mov(c_rarg3, arg);
  814   }
  815 }
  816 
  817 void MacroAssembler::stop(const char* msg) {
  818   if (ShowMessageBoxOnError) {
  819     address rip = pc();
  820     pusha(); // get regs on stack
  821     lea(c_rarg1, InternalAddress(rip));
  822     movq(c_rarg2, rsp); // pass pointer to regs array
  823   }
  824   lea(c_rarg0, ExternalAddress((address) msg));
  825   andq(rsp, -16); // align stack as required by ABI
  826   call(RuntimeAddress(CAST_FROM_FN_PTR(address, MacroAssembler::debug64)));
  827   hlt();
  828 }
  829 
  830 void MacroAssembler::warn(const char* msg) {
  831   push(rbp);
  832   movq(rbp, rsp);
  833   andq(rsp, -16);     // align stack as required by push_CPU_state and call
  834   push_CPU_state();   // keeps alignment at 16 bytes
  835   lea(c_rarg0, ExternalAddress((address) msg));
  836   lea(rax, ExternalAddress(CAST_FROM_FN_PTR(address, warning)));
  837   call(rax);
  838   pop_CPU_state();
  839   mov(rsp, rbp);
  840   pop(rbp);
  841 }
  842 
  843 void MacroAssembler::print_state() {
  844   address rip = pc();
  845   pusha();            // get regs on stack
  846   push(rbp);
  847   movq(rbp, rsp);
  848   andq(rsp, -16);     // align stack as required by push_CPU_state and call
  849   push_CPU_state();   // keeps alignment at 16 bytes
  850 
  851   lea(c_rarg0, InternalAddress(rip));
  852   lea(c_rarg1, Address(rbp, wordSize)); // pass pointer to regs array
  853   call_VM_leaf(CAST_FROM_FN_PTR(address, MacroAssembler::print_state64), c_rarg0, c_rarg1);
  854 
  855   pop_CPU_state();
  856   mov(rsp, rbp);
  857   pop(rbp);
  858   popa();
  859 }
  860 
  861 #ifndef PRODUCT
  862 extern &quot;C&quot; void findpc(intptr_t x);
  863 #endif
  864 
  865 void MacroAssembler::debug64(char* msg, int64_t pc, int64_t regs[]) {
  866   // In order to get locks to work, we need to fake a in_VM state
  867   if (ShowMessageBoxOnError) {
  868     JavaThread* thread = JavaThread::current();
  869     JavaThreadState saved_state = thread-&gt;thread_state();
  870     thread-&gt;set_thread_state(_thread_in_vm);
  871 #ifndef PRODUCT
  872     if (CountBytecodes || TraceBytecodes || StopInterpreterAt) {
  873       ttyLocker ttyl;
  874       BytecodeCounter::print();
  875     }
  876 #endif
  877     // To see where a verify_oop failed, get $ebx+40/X for this frame.
  878     // XXX correct this offset for amd64
  879     // This is the value of eip which points to where verify_oop will return.
  880     if (os::message_box(msg, &quot;Execution stopped, print registers?&quot;)) {
  881       print_state64(pc, regs);
  882       BREAKPOINT;
  883     }
  884   }
  885   fatal(&quot;DEBUG MESSAGE: %s&quot;, msg);
  886 }
  887 
  888 void MacroAssembler::print_state64(int64_t pc, int64_t regs[]) {
  889   ttyLocker ttyl;
  890   FlagSetting fs(Debugging, true);
  891   tty-&gt;print_cr(&quot;rip = 0x%016lx&quot;, (intptr_t)pc);
  892 #ifndef PRODUCT
  893   tty-&gt;cr();
  894   findpc(pc);
  895   tty-&gt;cr();
  896 #endif
  897 #define PRINT_REG(rax, value) \
  898   { tty-&gt;print(&quot;%s = &quot;, #rax); os::print_location(tty, value); }
  899   PRINT_REG(rax, regs[15]);
  900   PRINT_REG(rbx, regs[12]);
  901   PRINT_REG(rcx, regs[14]);
  902   PRINT_REG(rdx, regs[13]);
  903   PRINT_REG(rdi, regs[8]);
  904   PRINT_REG(rsi, regs[9]);
  905   PRINT_REG(rbp, regs[10]);
  906   PRINT_REG(rsp, regs[11]);
  907   PRINT_REG(r8 , regs[7]);
  908   PRINT_REG(r9 , regs[6]);
  909   PRINT_REG(r10, regs[5]);
  910   PRINT_REG(r11, regs[4]);
  911   PRINT_REG(r12, regs[3]);
  912   PRINT_REG(r13, regs[2]);
  913   PRINT_REG(r14, regs[1]);
  914   PRINT_REG(r15, regs[0]);
  915 #undef PRINT_REG
  916   // Print some words near top of staack.
  917   int64_t* rsp = (int64_t*) regs[11];
  918   int64_t* dump_sp = rsp;
  919   for (int col1 = 0; col1 &lt; 8; col1++) {
  920     tty-&gt;print(&quot;(rsp+0x%03x) 0x%016lx: &quot;, (int)((intptr_t)dump_sp - (intptr_t)rsp), (intptr_t)dump_sp);
  921     os::print_location(tty, *dump_sp++);
  922   }
  923   for (int row = 0; row &lt; 25; row++) {
  924     tty-&gt;print(&quot;(rsp+0x%03x) 0x%016lx: &quot;, (int)((intptr_t)dump_sp - (intptr_t)rsp), (intptr_t)dump_sp);
  925     for (int col = 0; col &lt; 4; col++) {
  926       tty-&gt;print(&quot; 0x%016lx&quot;, (intptr_t)*dump_sp++);
  927     }
  928     tty-&gt;cr();
  929   }
  930   // Print some instructions around pc:
  931   Disassembler::decode((address)pc-64, (address)pc);
  932   tty-&gt;print_cr(&quot;--------&quot;);
  933   Disassembler::decode((address)pc, (address)pc+32);
  934 }
  935 
  936 #endif // _LP64
  937 
  938 // Now versions that are common to 32/64 bit
  939 
  940 void MacroAssembler::addptr(Register dst, int32_t imm32) {
  941   LP64_ONLY(addq(dst, imm32)) NOT_LP64(addl(dst, imm32));
  942 }
  943 
  944 void MacroAssembler::addptr(Register dst, Register src) {
  945   LP64_ONLY(addq(dst, src)) NOT_LP64(addl(dst, src));
  946 }
  947 
  948 void MacroAssembler::addptr(Address dst, Register src) {
  949   LP64_ONLY(addq(dst, src)) NOT_LP64(addl(dst, src));
  950 }
  951 
  952 void MacroAssembler::addsd(XMMRegister dst, AddressLiteral src) {
  953   if (reachable(src)) {
  954     Assembler::addsd(dst, as_Address(src));
  955   } else {
  956     lea(rscratch1, src);
  957     Assembler::addsd(dst, Address(rscratch1, 0));
  958   }
  959 }
  960 
  961 void MacroAssembler::addss(XMMRegister dst, AddressLiteral src) {
  962   if (reachable(src)) {
  963     addss(dst, as_Address(src));
  964   } else {
  965     lea(rscratch1, src);
  966     addss(dst, Address(rscratch1, 0));
  967   }
  968 }
  969 
  970 void MacroAssembler::addpd(XMMRegister dst, AddressLiteral src) {
  971   if (reachable(src)) {
  972     Assembler::addpd(dst, as_Address(src));
  973   } else {
  974     lea(rscratch1, src);
  975     Assembler::addpd(dst, Address(rscratch1, 0));
  976   }
  977 }
  978 
  979 void MacroAssembler::align(int modulus) {
  980   align(modulus, offset());
  981 }
  982 
  983 void MacroAssembler::align(int modulus, int target) {
  984   if (target % modulus != 0) {
  985     nop(modulus - (target % modulus));
  986   }
  987 }
  988 
  989 void MacroAssembler::andpd(XMMRegister dst, AddressLiteral src, Register scratch_reg) {
  990   // Used in sign-masking with aligned address.
  991   assert((UseAVX &gt; 0) || (((intptr_t)src.target() &amp; 15) == 0), &quot;SSE mode requires address alignment 16 bytes&quot;);
  992   if (reachable(src)) {
  993     Assembler::andpd(dst, as_Address(src));
  994   } else {
  995     lea(scratch_reg, src);
  996     Assembler::andpd(dst, Address(scratch_reg, 0));
  997   }
  998 }
  999 
 1000 void MacroAssembler::andps(XMMRegister dst, AddressLiteral src, Register scratch_reg) {
 1001   // Used in sign-masking with aligned address.
 1002   assert((UseAVX &gt; 0) || (((intptr_t)src.target() &amp; 15) == 0), &quot;SSE mode requires address alignment 16 bytes&quot;);
 1003   if (reachable(src)) {
 1004     Assembler::andps(dst, as_Address(src));
 1005   } else {
 1006     lea(scratch_reg, src);
 1007     Assembler::andps(dst, Address(scratch_reg, 0));
 1008   }
 1009 }
 1010 
 1011 void MacroAssembler::andptr(Register dst, int32_t imm32) {
 1012   LP64_ONLY(andq(dst, imm32)) NOT_LP64(andl(dst, imm32));
 1013 }
 1014 
 1015 void MacroAssembler::atomic_incl(Address counter_addr) {
 1016   lock();
 1017   incrementl(counter_addr);
 1018 }
 1019 
 1020 void MacroAssembler::atomic_incl(AddressLiteral counter_addr, Register scr) {
 1021   if (reachable(counter_addr)) {
 1022     atomic_incl(as_Address(counter_addr));
 1023   } else {
 1024     lea(scr, counter_addr);
 1025     atomic_incl(Address(scr, 0));
 1026   }
 1027 }
 1028 
 1029 #ifdef _LP64
 1030 void MacroAssembler::atomic_incq(Address counter_addr) {
 1031   lock();
 1032   incrementq(counter_addr);
 1033 }
 1034 
 1035 void MacroAssembler::atomic_incq(AddressLiteral counter_addr, Register scr) {
 1036   if (reachable(counter_addr)) {
 1037     atomic_incq(as_Address(counter_addr));
 1038   } else {
 1039     lea(scr, counter_addr);
 1040     atomic_incq(Address(scr, 0));
 1041   }
 1042 }
 1043 #endif
 1044 
 1045 // Writes to stack successive pages until offset reached to check for
 1046 // stack overflow + shadow pages.  This clobbers tmp.
 1047 void MacroAssembler::bang_stack_size(Register size, Register tmp) {
 1048   movptr(tmp, rsp);
 1049   // Bang stack for total size given plus shadow page size.
 1050   // Bang one page at a time because large size can bang beyond yellow and
 1051   // red zones.
 1052   Label loop;
 1053   bind(loop);
 1054   movl(Address(tmp, (-os::vm_page_size())), size );
 1055   subptr(tmp, os::vm_page_size());
 1056   subl(size, os::vm_page_size());
 1057   jcc(Assembler::greater, loop);
 1058 
 1059   // Bang down shadow pages too.
 1060   // At this point, (tmp-0) is the last address touched, so don&#39;t
 1061   // touch it again.  (It was touched as (tmp-pagesize) but then tmp
 1062   // was post-decremented.)  Skip this address by starting at i=1, and
 1063   // touch a few more pages below.  N.B.  It is important to touch all
 1064   // the way down including all pages in the shadow zone.
 1065   for (int i = 1; i &lt; ((int)JavaThread::stack_shadow_zone_size() / os::vm_page_size()); i++) {
 1066     // this could be any sized move but this is can be a debugging crumb
 1067     // so the bigger the better.
 1068     movptr(Address(tmp, (-i*os::vm_page_size())), size );
 1069   }
 1070 }
 1071 
 1072 void MacroAssembler::reserved_stack_check() {
 1073     // testing if reserved zone needs to be enabled
 1074     Label no_reserved_zone_enabling;
 1075     Register thread = NOT_LP64(rsi) LP64_ONLY(r15_thread);
 1076     NOT_LP64(get_thread(rsi);)
 1077 
 1078     cmpptr(rsp, Address(thread, JavaThread::reserved_stack_activation_offset()));
 1079     jcc(Assembler::below, no_reserved_zone_enabling);
 1080 
 1081     call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::enable_stack_reserved_zone), thread);
 1082     jump(RuntimeAddress(StubRoutines::throw_delayed_StackOverflowError_entry()));
 1083     should_not_reach_here();
 1084 
 1085     bind(no_reserved_zone_enabling);
 1086 }
 1087 
 1088 int MacroAssembler::biased_locking_enter(Register lock_reg,
 1089                                          Register obj_reg,
 1090                                          Register swap_reg,
 1091                                          Register tmp_reg,
 1092                                          bool swap_reg_contains_mark,
 1093                                          Label&amp; done,
 1094                                          Label* slow_case,
 1095                                          BiasedLockingCounters* counters) {
 1096   assert(UseBiasedLocking, &quot;why call this otherwise?&quot;);
 1097   assert(swap_reg == rax, &quot;swap_reg must be rax for cmpxchgq&quot;);
 1098   assert(tmp_reg != noreg, &quot;tmp_reg must be supplied&quot;);
 1099   assert_different_registers(lock_reg, obj_reg, swap_reg, tmp_reg);
 1100   assert(markWord::age_shift == markWord::lock_bits + markWord::biased_lock_bits, &quot;biased locking makes assumptions about bit layout&quot;);
 1101   Address mark_addr      (obj_reg, oopDesc::mark_offset_in_bytes());
 1102   NOT_LP64( Address saved_mark_addr(lock_reg, 0); )
 1103 
 1104   if (PrintBiasedLockingStatistics &amp;&amp; counters == NULL) {
 1105     counters = BiasedLocking::counters();
 1106   }
 1107   // Biased locking
 1108   // See whether the lock is currently biased toward our thread and
 1109   // whether the epoch is still valid
 1110   // Note that the runtime guarantees sufficient alignment of JavaThread
 1111   // pointers to allow age to be placed into low bits
 1112   // First check to see whether biasing is even enabled for this object
 1113   Label cas_label;
 1114   int null_check_offset = -1;
 1115   if (!swap_reg_contains_mark) {
 1116     null_check_offset = offset();
 1117     movptr(swap_reg, mark_addr);
 1118   }
 1119   movptr(tmp_reg, swap_reg);
 1120   andptr(tmp_reg, markWord::biased_lock_mask_in_place);
 1121   cmpptr(tmp_reg, markWord::biased_lock_pattern);
 1122   jcc(Assembler::notEqual, cas_label);
 1123   // The bias pattern is present in the object&#39;s header. Need to check
 1124   // whether the bias owner and the epoch are both still current.
 1125 #ifndef _LP64
 1126   // Note that because there is no current thread register on x86_32 we
 1127   // need to store off the mark word we read out of the object to
 1128   // avoid reloading it and needing to recheck invariants below. This
 1129   // store is unfortunate but it makes the overall code shorter and
 1130   // simpler.
 1131   movptr(saved_mark_addr, swap_reg);
 1132 #endif
 1133   if (swap_reg_contains_mark) {
 1134     null_check_offset = offset();
 1135   }
 1136   load_prototype_header(tmp_reg, obj_reg);
 1137 #ifdef _LP64
 1138   orptr(tmp_reg, r15_thread);
 1139   xorptr(tmp_reg, swap_reg);
 1140   Register header_reg = tmp_reg;
 1141 #else
 1142   xorptr(tmp_reg, swap_reg);
 1143   get_thread(swap_reg);
 1144   xorptr(swap_reg, tmp_reg);
 1145   Register header_reg = swap_reg;
 1146 #endif
 1147   andptr(header_reg, ~((int) markWord::age_mask_in_place));
 1148   if (counters != NULL) {
 1149     cond_inc32(Assembler::zero,
 1150                ExternalAddress((address) counters-&gt;biased_lock_entry_count_addr()));
 1151   }
 1152   jcc(Assembler::equal, done);
 1153 
 1154   Label try_revoke_bias;
 1155   Label try_rebias;
 1156 
 1157   // At this point we know that the header has the bias pattern and
 1158   // that we are not the bias owner in the current epoch. We need to
 1159   // figure out more details about the state of the header in order to
 1160   // know what operations can be legally performed on the object&#39;s
 1161   // header.
 1162 
 1163   // If the low three bits in the xor result aren&#39;t clear, that means
 1164   // the prototype header is no longer biased and we have to revoke
 1165   // the bias on this object.
 1166   testptr(header_reg, markWord::biased_lock_mask_in_place);
 1167   jccb(Assembler::notZero, try_revoke_bias);
 1168 
 1169   // Biasing is still enabled for this data type. See whether the
 1170   // epoch of the current bias is still valid, meaning that the epoch
 1171   // bits of the mark word are equal to the epoch bits of the
 1172   // prototype header. (Note that the prototype header&#39;s epoch bits
 1173   // only change at a safepoint.) If not, attempt to rebias the object
 1174   // toward the current thread. Note that we must be absolutely sure
 1175   // that the current epoch is invalid in order to do this because
 1176   // otherwise the manipulations it performs on the mark word are
 1177   // illegal.
 1178   testptr(header_reg, markWord::epoch_mask_in_place);
 1179   jccb(Assembler::notZero, try_rebias);
 1180 
 1181   // The epoch of the current bias is still valid but we know nothing
 1182   // about the owner; it might be set or it might be clear. Try to
 1183   // acquire the bias of the object using an atomic operation. If this
 1184   // fails we will go in to the runtime to revoke the object&#39;s bias.
 1185   // Note that we first construct the presumed unbiased header so we
 1186   // don&#39;t accidentally blow away another thread&#39;s valid bias.
 1187   NOT_LP64( movptr(swap_reg, saved_mark_addr); )
 1188   andptr(swap_reg,
 1189          markWord::biased_lock_mask_in_place | markWord::age_mask_in_place | markWord::epoch_mask_in_place);
 1190 #ifdef _LP64
 1191   movptr(tmp_reg, swap_reg);
 1192   orptr(tmp_reg, r15_thread);
 1193 #else
 1194   get_thread(tmp_reg);
 1195   orptr(tmp_reg, swap_reg);
 1196 #endif
 1197   lock();
 1198   cmpxchgptr(tmp_reg, mark_addr); // compare tmp_reg and swap_reg
 1199   // If the biasing toward our thread failed, this means that
 1200   // another thread succeeded in biasing it toward itself and we
 1201   // need to revoke that bias. The revocation will occur in the
 1202   // interpreter runtime in the slow case.
 1203   if (counters != NULL) {
 1204     cond_inc32(Assembler::zero,
 1205                ExternalAddress((address) counters-&gt;anonymously_biased_lock_entry_count_addr()));
 1206   }
 1207   if (slow_case != NULL) {
 1208     jcc(Assembler::notZero, *slow_case);
 1209   }
 1210   jmp(done);
 1211 
 1212   bind(try_rebias);
 1213   // At this point we know the epoch has expired, meaning that the
 1214   // current &quot;bias owner&quot;, if any, is actually invalid. Under these
 1215   // circumstances _only_, we are allowed to use the current header&#39;s
 1216   // value as the comparison value when doing the cas to acquire the
 1217   // bias in the current epoch. In other words, we allow transfer of
 1218   // the bias from one thread to another directly in this situation.
 1219   //
 1220   // FIXME: due to a lack of registers we currently blow away the age
 1221   // bits in this situation. Should attempt to preserve them.
 1222   load_prototype_header(tmp_reg, obj_reg);
 1223 #ifdef _LP64
 1224   orptr(tmp_reg, r15_thread);
 1225 #else
 1226   get_thread(swap_reg);
 1227   orptr(tmp_reg, swap_reg);
 1228   movptr(swap_reg, saved_mark_addr);
 1229 #endif
 1230   lock();
 1231   cmpxchgptr(tmp_reg, mark_addr); // compare tmp_reg and swap_reg
 1232   // If the biasing toward our thread failed, then another thread
 1233   // succeeded in biasing it toward itself and we need to revoke that
 1234   // bias. The revocation will occur in the runtime in the slow case.
 1235   if (counters != NULL) {
 1236     cond_inc32(Assembler::zero,
 1237                ExternalAddress((address) counters-&gt;rebiased_lock_entry_count_addr()));
 1238   }
 1239   if (slow_case != NULL) {
 1240     jcc(Assembler::notZero, *slow_case);
 1241   }
 1242   jmp(done);
 1243 
 1244   bind(try_revoke_bias);
 1245   // The prototype mark in the klass doesn&#39;t have the bias bit set any
 1246   // more, indicating that objects of this data type are not supposed
 1247   // to be biased any more. We are going to try to reset the mark of
 1248   // this object to the prototype value and fall through to the
 1249   // CAS-based locking scheme. Note that if our CAS fails, it means
 1250   // that another thread raced us for the privilege of revoking the
 1251   // bias of this particular object, so it&#39;s okay to continue in the
 1252   // normal locking code.
 1253   //
 1254   // FIXME: due to a lack of registers we currently blow away the age
 1255   // bits in this situation. Should attempt to preserve them.
 1256   NOT_LP64( movptr(swap_reg, saved_mark_addr); )
 1257   load_prototype_header(tmp_reg, obj_reg);
 1258   lock();
 1259   cmpxchgptr(tmp_reg, mark_addr); // compare tmp_reg and swap_reg
 1260   // Fall through to the normal CAS-based lock, because no matter what
 1261   // the result of the above CAS, some thread must have succeeded in
 1262   // removing the bias bit from the object&#39;s header.
 1263   if (counters != NULL) {
 1264     cond_inc32(Assembler::zero,
 1265                ExternalAddress((address) counters-&gt;revoked_lock_entry_count_addr()));
 1266   }
 1267 
 1268   bind(cas_label);
 1269 
 1270   return null_check_offset;
 1271 }
 1272 
 1273 void MacroAssembler::biased_locking_exit(Register obj_reg, Register temp_reg, Label&amp; done) {
 1274   assert(UseBiasedLocking, &quot;why call this otherwise?&quot;);
 1275 
 1276   // Check for biased locking unlock case, which is a no-op
 1277   // Note: we do not have to check the thread ID for two reasons.
 1278   // First, the interpreter checks for IllegalMonitorStateException at
 1279   // a higher level. Second, if the bias was revoked while we held the
 1280   // lock, the object could not be rebiased toward another thread, so
 1281   // the bias bit would be clear.
 1282   movptr(temp_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));
 1283   andptr(temp_reg, markWord::biased_lock_mask_in_place);
 1284   cmpptr(temp_reg, markWord::biased_lock_pattern);
 1285   jcc(Assembler::equal, done);
 1286 }
 1287 
 1288 #ifdef COMPILER2
 1289 
 1290 #if INCLUDE_RTM_OPT
 1291 
 1292 // Update rtm_counters based on abort status
 1293 // input: abort_status
 1294 //        rtm_counters (RTMLockingCounters*)
 1295 // flags are killed
 1296 void MacroAssembler::rtm_counters_update(Register abort_status, Register rtm_counters) {
 1297 
 1298   atomic_incptr(Address(rtm_counters, RTMLockingCounters::abort_count_offset()));
 1299   if (PrintPreciseRTMLockingStatistics) {
 1300     for (int i = 0; i &lt; RTMLockingCounters::ABORT_STATUS_LIMIT; i++) {
 1301       Label check_abort;
 1302       testl(abort_status, (1&lt;&lt;i));
 1303       jccb(Assembler::equal, check_abort);
 1304       atomic_incptr(Address(rtm_counters, RTMLockingCounters::abortX_count_offset() + (i * sizeof(uintx))));
 1305       bind(check_abort);
 1306     }
 1307   }
 1308 }
 1309 
 1310 // Branch if (random &amp; (count-1) != 0), count is 2^n
 1311 // tmp, scr and flags are killed
 1312 void MacroAssembler::branch_on_random_using_rdtsc(Register tmp, Register scr, int count, Label&amp; brLabel) {
 1313   assert(tmp == rax, &quot;&quot;);
 1314   assert(scr == rdx, &quot;&quot;);
 1315   rdtsc(); // modifies EDX:EAX
 1316   andptr(tmp, count-1);
 1317   jccb(Assembler::notZero, brLabel);
 1318 }
 1319 
 1320 // Perform abort ratio calculation, set no_rtm bit if high ratio
 1321 // input:  rtm_counters_Reg (RTMLockingCounters* address)
 1322 // tmpReg, rtm_counters_Reg and flags are killed
 1323 void MacroAssembler::rtm_abort_ratio_calculation(Register tmpReg,
 1324                                                  Register rtm_counters_Reg,
 1325                                                  RTMLockingCounters* rtm_counters,
 1326                                                  Metadata* method_data) {
 1327   Label L_done, L_check_always_rtm1, L_check_always_rtm2;
 1328 
 1329   if (RTMLockingCalculationDelay &gt; 0) {
 1330     // Delay calculation
 1331     movptr(tmpReg, ExternalAddress((address) RTMLockingCounters::rtm_calculation_flag_addr()), tmpReg);
 1332     testptr(tmpReg, tmpReg);
 1333     jccb(Assembler::equal, L_done);
 1334   }
 1335   // Abort ratio calculation only if abort_count &gt; RTMAbortThreshold
 1336   //   Aborted transactions = abort_count * 100
 1337   //   All transactions = total_count *  RTMTotalCountIncrRate
 1338   //   Set no_rtm bit if (Aborted transactions &gt;= All transactions * RTMAbortRatio)
 1339 
 1340   movptr(tmpReg, Address(rtm_counters_Reg, RTMLockingCounters::abort_count_offset()));
 1341   cmpptr(tmpReg, RTMAbortThreshold);
 1342   jccb(Assembler::below, L_check_always_rtm2);
 1343   imulptr(tmpReg, tmpReg, 100);
 1344 
 1345   Register scrReg = rtm_counters_Reg;
 1346   movptr(scrReg, Address(rtm_counters_Reg, RTMLockingCounters::total_count_offset()));
 1347   imulptr(scrReg, scrReg, RTMTotalCountIncrRate);
 1348   imulptr(scrReg, scrReg, RTMAbortRatio);
 1349   cmpptr(tmpReg, scrReg);
 1350   jccb(Assembler::below, L_check_always_rtm1);
 1351   if (method_data != NULL) {
 1352     // set rtm_state to &quot;no rtm&quot; in MDO
 1353     mov_metadata(tmpReg, method_data);
 1354     lock();
 1355     orl(Address(tmpReg, MethodData::rtm_state_offset_in_bytes()), NoRTM);
 1356   }
 1357   jmpb(L_done);
 1358   bind(L_check_always_rtm1);
 1359   // Reload RTMLockingCounters* address
 1360   lea(rtm_counters_Reg, ExternalAddress((address)rtm_counters));
 1361   bind(L_check_always_rtm2);
 1362   movptr(tmpReg, Address(rtm_counters_Reg, RTMLockingCounters::total_count_offset()));
 1363   cmpptr(tmpReg, RTMLockingThreshold / RTMTotalCountIncrRate);
 1364   jccb(Assembler::below, L_done);
 1365   if (method_data != NULL) {
 1366     // set rtm_state to &quot;always rtm&quot; in MDO
 1367     mov_metadata(tmpReg, method_data);
 1368     lock();
 1369     orl(Address(tmpReg, MethodData::rtm_state_offset_in_bytes()), UseRTM);
 1370   }
 1371   bind(L_done);
 1372 }
 1373 
 1374 // Update counters and perform abort ratio calculation
 1375 // input:  abort_status_Reg
 1376 // rtm_counters_Reg, flags are killed
 1377 void MacroAssembler::rtm_profiling(Register abort_status_Reg,
 1378                                    Register rtm_counters_Reg,
 1379                                    RTMLockingCounters* rtm_counters,
 1380                                    Metadata* method_data,
 1381                                    bool profile_rtm) {
 1382 
 1383   assert(rtm_counters != NULL, &quot;should not be NULL when profiling RTM&quot;);
 1384   // update rtm counters based on rax value at abort
 1385   // reads abort_status_Reg, updates flags
 1386   lea(rtm_counters_Reg, ExternalAddress((address)rtm_counters));
 1387   rtm_counters_update(abort_status_Reg, rtm_counters_Reg);
 1388   if (profile_rtm) {
 1389     // Save abort status because abort_status_Reg is used by following code.
 1390     if (RTMRetryCount &gt; 0) {
 1391       push(abort_status_Reg);
 1392     }
 1393     assert(rtm_counters != NULL, &quot;should not be NULL when profiling RTM&quot;);
 1394     rtm_abort_ratio_calculation(abort_status_Reg, rtm_counters_Reg, rtm_counters, method_data);
 1395     // restore abort status
 1396     if (RTMRetryCount &gt; 0) {
 1397       pop(abort_status_Reg);
 1398     }
 1399   }
 1400 }
 1401 
 1402 // Retry on abort if abort&#39;s status is 0x6: can retry (0x2) | memory conflict (0x4)
 1403 // inputs: retry_count_Reg
 1404 //       : abort_status_Reg
 1405 // output: retry_count_Reg decremented by 1
 1406 // flags are killed
 1407 void MacroAssembler::rtm_retry_lock_on_abort(Register retry_count_Reg, Register abort_status_Reg, Label&amp; retryLabel) {
 1408   Label doneRetry;
 1409   assert(abort_status_Reg == rax, &quot;&quot;);
 1410   // The abort reason bits are in eax (see all states in rtmLocking.hpp)
 1411   // 0x6 = conflict on which we can retry (0x2) | memory conflict (0x4)
 1412   // if reason is in 0x6 and retry count != 0 then retry
 1413   andptr(abort_status_Reg, 0x6);
 1414   jccb(Assembler::zero, doneRetry);
 1415   testl(retry_count_Reg, retry_count_Reg);
 1416   jccb(Assembler::zero, doneRetry);
 1417   pause();
 1418   decrementl(retry_count_Reg);
 1419   jmp(retryLabel);
 1420   bind(doneRetry);
 1421 }
 1422 
 1423 // Spin and retry if lock is busy,
 1424 // inputs: box_Reg (monitor address)
 1425 //       : retry_count_Reg
 1426 // output: retry_count_Reg decremented by 1
 1427 //       : clear z flag if retry count exceeded
 1428 // tmp_Reg, scr_Reg, flags are killed
 1429 void MacroAssembler::rtm_retry_lock_on_busy(Register retry_count_Reg, Register box_Reg,
 1430                                             Register tmp_Reg, Register scr_Reg, Label&amp; retryLabel) {
 1431   Label SpinLoop, SpinExit, doneRetry;
 1432   int owner_offset = OM_OFFSET_NO_MONITOR_VALUE_TAG(owner);
 1433 
 1434   testl(retry_count_Reg, retry_count_Reg);
 1435   jccb(Assembler::zero, doneRetry);
 1436   decrementl(retry_count_Reg);
 1437   movptr(scr_Reg, RTMSpinLoopCount);
 1438 
 1439   bind(SpinLoop);
 1440   pause();
 1441   decrementl(scr_Reg);
 1442   jccb(Assembler::lessEqual, SpinExit);
 1443   movptr(tmp_Reg, Address(box_Reg, owner_offset));
 1444   testptr(tmp_Reg, tmp_Reg);
 1445   jccb(Assembler::notZero, SpinLoop);
 1446 
 1447   bind(SpinExit);
 1448   jmp(retryLabel);
 1449   bind(doneRetry);
 1450   incrementl(retry_count_Reg); // clear z flag
 1451 }
 1452 
 1453 // Use RTM for normal stack locks
 1454 // Input: objReg (object to lock)
 1455 void MacroAssembler::rtm_stack_locking(Register objReg, Register tmpReg, Register scrReg,
 1456                                        Register retry_on_abort_count_Reg,
 1457                                        RTMLockingCounters* stack_rtm_counters,
 1458                                        Metadata* method_data, bool profile_rtm,
 1459                                        Label&amp; DONE_LABEL, Label&amp; IsInflated) {
 1460   assert(UseRTMForStackLocks, &quot;why call this otherwise?&quot;);
 1461   assert(!UseBiasedLocking, &quot;Biased locking is not supported with RTM locking&quot;);
 1462   assert(tmpReg == rax, &quot;&quot;);
 1463   assert(scrReg == rdx, &quot;&quot;);
 1464   Label L_rtm_retry, L_decrement_retry, L_on_abort;
 1465 
 1466   if (RTMRetryCount &gt; 0) {
 1467     movl(retry_on_abort_count_Reg, RTMRetryCount); // Retry on abort
 1468     bind(L_rtm_retry);
 1469   }
 1470   movptr(tmpReg, Address(objReg, oopDesc::mark_offset_in_bytes()));
 1471   testptr(tmpReg, markWord::monitor_value);  // inflated vs stack-locked|neutral|biased
 1472   jcc(Assembler::notZero, IsInflated);
 1473 
 1474   if (PrintPreciseRTMLockingStatistics || profile_rtm) {
 1475     Label L_noincrement;
 1476     if (RTMTotalCountIncrRate &gt; 1) {
 1477       // tmpReg, scrReg and flags are killed
 1478       branch_on_random_using_rdtsc(tmpReg, scrReg, RTMTotalCountIncrRate, L_noincrement);
 1479     }
 1480     assert(stack_rtm_counters != NULL, &quot;should not be NULL when profiling RTM&quot;);
 1481     atomic_incptr(ExternalAddress((address)stack_rtm_counters-&gt;total_count_addr()), scrReg);
 1482     bind(L_noincrement);
 1483   }
 1484   xbegin(L_on_abort);
 1485   movptr(tmpReg, Address(objReg, oopDesc::mark_offset_in_bytes()));       // fetch markword
 1486   andptr(tmpReg, markWord::biased_lock_mask_in_place); // look at 3 lock bits
 1487   cmpptr(tmpReg, markWord::unlocked_value);            // bits = 001 unlocked
 1488   jcc(Assembler::equal, DONE_LABEL);        // all done if unlocked
 1489 
 1490   Register abort_status_Reg = tmpReg; // status of abort is stored in RAX
 1491   if (UseRTMXendForLockBusy) {
 1492     xend();
 1493     movptr(abort_status_Reg, 0x2);   // Set the abort status to 2 (so we can retry)
 1494     jmp(L_decrement_retry);
 1495   }
 1496   else {
 1497     xabort(0);
 1498   }
 1499   bind(L_on_abort);
 1500   if (PrintPreciseRTMLockingStatistics || profile_rtm) {
 1501     rtm_profiling(abort_status_Reg, scrReg, stack_rtm_counters, method_data, profile_rtm);
 1502   }
 1503   bind(L_decrement_retry);
 1504   if (RTMRetryCount &gt; 0) {
 1505     // retry on lock abort if abort status is &#39;can retry&#39; (0x2) or &#39;memory conflict&#39; (0x4)
 1506     rtm_retry_lock_on_abort(retry_on_abort_count_Reg, abort_status_Reg, L_rtm_retry);
 1507   }
 1508 }
 1509 
 1510 // Use RTM for inflating locks
 1511 // inputs: objReg (object to lock)
 1512 //         boxReg (on-stack box address (displaced header location) - KILLED)
 1513 //         tmpReg (ObjectMonitor address + markWord::monitor_value)
 1514 void MacroAssembler::rtm_inflated_locking(Register objReg, Register boxReg, Register tmpReg,
 1515                                           Register scrReg, Register retry_on_busy_count_Reg,
 1516                                           Register retry_on_abort_count_Reg,
 1517                                           RTMLockingCounters* rtm_counters,
 1518                                           Metadata* method_data, bool profile_rtm,
 1519                                           Label&amp; DONE_LABEL) {
 1520   assert(UseRTMLocking, &quot;why call this otherwise?&quot;);
 1521   assert(tmpReg == rax, &quot;&quot;);
 1522   assert(scrReg == rdx, &quot;&quot;);
 1523   Label L_rtm_retry, L_decrement_retry, L_on_abort;
 1524   int owner_offset = OM_OFFSET_NO_MONITOR_VALUE_TAG(owner);
 1525 
 1526   // Without cast to int32_t this style of movptr will destroy r10 which is typically obj.
 1527   movptr(Address(boxReg, 0), (int32_t)intptr_t(markWord::unused_mark().value()));
 1528   movptr(boxReg, tmpReg); // Save ObjectMonitor address
 1529 
 1530   if (RTMRetryCount &gt; 0) {
 1531     movl(retry_on_busy_count_Reg, RTMRetryCount);  // Retry on lock busy
 1532     movl(retry_on_abort_count_Reg, RTMRetryCount); // Retry on abort
 1533     bind(L_rtm_retry);
 1534   }
 1535   if (PrintPreciseRTMLockingStatistics || profile_rtm) {
 1536     Label L_noincrement;
 1537     if (RTMTotalCountIncrRate &gt; 1) {
 1538       // tmpReg, scrReg and flags are killed
 1539       branch_on_random_using_rdtsc(tmpReg, scrReg, RTMTotalCountIncrRate, L_noincrement);
 1540     }
 1541     assert(rtm_counters != NULL, &quot;should not be NULL when profiling RTM&quot;);
 1542     atomic_incptr(ExternalAddress((address)rtm_counters-&gt;total_count_addr()), scrReg);
 1543     bind(L_noincrement);
 1544   }
 1545   xbegin(L_on_abort);
 1546   movptr(tmpReg, Address(objReg, oopDesc::mark_offset_in_bytes()));
 1547   movptr(tmpReg, Address(tmpReg, owner_offset));
 1548   testptr(tmpReg, tmpReg);
 1549   jcc(Assembler::zero, DONE_LABEL);
 1550   if (UseRTMXendForLockBusy) {
 1551     xend();
 1552     jmp(L_decrement_retry);
 1553   }
 1554   else {
 1555     xabort(0);
 1556   }
 1557   bind(L_on_abort);
 1558   Register abort_status_Reg = tmpReg; // status of abort is stored in RAX
 1559   if (PrintPreciseRTMLockingStatistics || profile_rtm) {
 1560     rtm_profiling(abort_status_Reg, scrReg, rtm_counters, method_data, profile_rtm);
 1561   }
 1562   if (RTMRetryCount &gt; 0) {
 1563     // retry on lock abort if abort status is &#39;can retry&#39; (0x2) or &#39;memory conflict&#39; (0x4)
 1564     rtm_retry_lock_on_abort(retry_on_abort_count_Reg, abort_status_Reg, L_rtm_retry);
 1565   }
 1566 
 1567   movptr(tmpReg, Address(boxReg, owner_offset)) ;
 1568   testptr(tmpReg, tmpReg) ;
 1569   jccb(Assembler::notZero, L_decrement_retry) ;
 1570 
 1571   // Appears unlocked - try to swing _owner from null to non-null.
 1572   // Invariant: tmpReg == 0.  tmpReg is EAX which is the implicit cmpxchg comparand.
 1573 #ifdef _LP64
 1574   Register threadReg = r15_thread;
 1575 #else
 1576   get_thread(scrReg);
 1577   Register threadReg = scrReg;
 1578 #endif
 1579   lock();
 1580   cmpxchgptr(threadReg, Address(boxReg, owner_offset)); // Updates tmpReg
 1581 
 1582   if (RTMRetryCount &gt; 0) {
 1583     // success done else retry
 1584     jccb(Assembler::equal, DONE_LABEL) ;
 1585     bind(L_decrement_retry);
 1586     // Spin and retry if lock is busy.
 1587     rtm_retry_lock_on_busy(retry_on_busy_count_Reg, boxReg, tmpReg, scrReg, L_rtm_retry);
 1588   }
 1589   else {
 1590     bind(L_decrement_retry);
 1591   }
 1592 }
 1593 
 1594 #endif //  INCLUDE_RTM_OPT
 1595 
 1596 // fast_lock and fast_unlock used by C2
 1597 
 1598 // Because the transitions from emitted code to the runtime
 1599 // monitorenter/exit helper stubs are so slow it&#39;s critical that
 1600 // we inline both the stack-locking fast path and the inflated fast path.
 1601 //
 1602 // See also: cmpFastLock and cmpFastUnlock.
 1603 //
 1604 // What follows is a specialized inline transliteration of the code
 1605 // in enter() and exit(). If we&#39;re concerned about I$ bloat another
 1606 // option would be to emit TrySlowEnter and TrySlowExit methods
 1607 // at startup-time.  These methods would accept arguments as
 1608 // (rax,=Obj, rbx=Self, rcx=box, rdx=Scratch) and return success-failure
 1609 // indications in the icc.ZFlag.  fast_lock and fast_unlock would simply
 1610 // marshal the arguments and emit calls to TrySlowEnter and TrySlowExit.
 1611 // In practice, however, the # of lock sites is bounded and is usually small.
 1612 // Besides the call overhead, TrySlowEnter and TrySlowExit might suffer
 1613 // if the processor uses simple bimodal branch predictors keyed by EIP
 1614 // Since the helper routines would be called from multiple synchronization
 1615 // sites.
 1616 //
 1617 // An even better approach would be write &quot;MonitorEnter()&quot; and &quot;MonitorExit()&quot;
 1618 // in java - using j.u.c and unsafe - and just bind the lock and unlock sites
 1619 // to those specialized methods.  That&#39;d give us a mostly platform-independent
 1620 // implementation that the JITs could optimize and inline at their pleasure.
 1621 // Done correctly, the only time we&#39;d need to cross to native could would be
 1622 // to park() or unpark() threads.  We&#39;d also need a few more unsafe operators
 1623 // to (a) prevent compiler-JIT reordering of non-volatile accesses, and
 1624 // (b) explicit barriers or fence operations.
 1625 //
 1626 // TODO:
 1627 //
 1628 // *  Arrange for C2 to pass &quot;Self&quot; into fast_lock and fast_unlock in one of the registers (scr).
 1629 //    This avoids manifesting the Self pointer in the fast_lock and fast_unlock terminals.
 1630 //    Given TLAB allocation, Self is usually manifested in a register, so passing it into
 1631 //    the lock operators would typically be faster than reifying Self.
 1632 //
 1633 // *  Ideally I&#39;d define the primitives as:
 1634 //       fast_lock   (nax Obj, nax box, EAX tmp, nax scr) where box, tmp and scr are KILLED.
 1635 //       fast_unlock (nax Obj, EAX box, nax tmp) where box and tmp are KILLED
 1636 //    Unfortunately ADLC bugs prevent us from expressing the ideal form.
 1637 //    Instead, we&#39;re stuck with a rather awkward and brittle register assignments below.
 1638 //    Furthermore the register assignments are overconstrained, possibly resulting in
 1639 //    sub-optimal code near the synchronization site.
 1640 //
 1641 // *  Eliminate the sp-proximity tests and just use &quot;== Self&quot; tests instead.
 1642 //    Alternately, use a better sp-proximity test.
 1643 //
 1644 // *  Currently ObjectMonitor._Owner can hold either an sp value or a (THREAD *) value.
 1645 //    Either one is sufficient to uniquely identify a thread.
 1646 //    TODO: eliminate use of sp in _owner and use get_thread(tr) instead.
 1647 //
 1648 // *  Intrinsify notify() and notifyAll() for the common cases where the
 1649 //    object is locked by the calling thread but the waitlist is empty.
 1650 //    avoid the expensive JNI call to JVM_Notify() and JVM_NotifyAll().
 1651 //
 1652 // *  use jccb and jmpb instead of jcc and jmp to improve code density.
 1653 //    But beware of excessive branch density on AMD Opterons.
 1654 //
 1655 // *  Both fast_lock and fast_unlock set the ICC.ZF to indicate success
 1656 //    or failure of the fast path.  If the fast path fails then we pass
 1657 //    control to the slow path, typically in C.  In fast_lock and
 1658 //    fast_unlock we often branch to DONE_LABEL, just to find that C2
 1659 //    will emit a conditional branch immediately after the node.
 1660 //    So we have branches to branches and lots of ICC.ZF games.
 1661 //    Instead, it might be better to have C2 pass a &quot;FailureLabel&quot;
 1662 //    into fast_lock and fast_unlock.  In the case of success, control
 1663 //    will drop through the node.  ICC.ZF is undefined at exit.
 1664 //    In the case of failure, the node will branch directly to the
 1665 //    FailureLabel
 1666 
 1667 
 1668 // obj: object to lock
 1669 // box: on-stack box address (displaced header location) - KILLED
 1670 // rax,: tmp -- KILLED
 1671 // scr: tmp -- KILLED
 1672 void MacroAssembler::fast_lock(Register objReg, Register boxReg, Register tmpReg,
 1673                                Register scrReg, Register cx1Reg, Register cx2Reg,
 1674                                BiasedLockingCounters* counters,
 1675                                RTMLockingCounters* rtm_counters,
 1676                                RTMLockingCounters* stack_rtm_counters,
 1677                                Metadata* method_data,
 1678                                bool use_rtm, bool profile_rtm) {
 1679   // Ensure the register assignments are disjoint
 1680   assert(tmpReg == rax, &quot;&quot;);
 1681 
 1682   if (use_rtm) {
 1683     assert_different_registers(objReg, boxReg, tmpReg, scrReg, cx1Reg, cx2Reg);
 1684   } else {
 1685     assert(cx1Reg == noreg, &quot;&quot;);
 1686     assert(cx2Reg == noreg, &quot;&quot;);
 1687     assert_different_registers(objReg, boxReg, tmpReg, scrReg);
 1688   }
 1689 
 1690   if (counters != NULL) {
 1691     atomic_incl(ExternalAddress((address)counters-&gt;total_entry_count_addr()), scrReg);
 1692   }
 1693 
 1694   // Possible cases that we&#39;ll encounter in fast_lock
 1695   // ------------------------------------------------
 1696   // * Inflated
 1697   //    -- unlocked
 1698   //    -- Locked
 1699   //       = by self
 1700   //       = by other
 1701   // * biased
 1702   //    -- by Self
 1703   //    -- by other
 1704   // * neutral
 1705   // * stack-locked
 1706   //    -- by self
 1707   //       = sp-proximity test hits
 1708   //       = sp-proximity test generates false-negative
 1709   //    -- by other
 1710   //
 1711 
 1712   Label IsInflated, DONE_LABEL;
 1713 
 1714   // it&#39;s stack-locked, biased or neutral
 1715   // TODO: optimize away redundant LDs of obj-&gt;mark and improve the markword triage
 1716   // order to reduce the number of conditional branches in the most common cases.
 1717   // Beware -- there&#39;s a subtle invariant that fetch of the markword
 1718   // at [FETCH], below, will never observe a biased encoding (*101b).
 1719   // If this invariant is not held we risk exclusion (safety) failure.
 1720   if (UseBiasedLocking &amp;&amp; !UseOptoBiasInlining) {
 1721     biased_locking_enter(boxReg, objReg, tmpReg, scrReg, false, DONE_LABEL, NULL, counters);
 1722   }
 1723 
 1724 #if INCLUDE_RTM_OPT
 1725   if (UseRTMForStackLocks &amp;&amp; use_rtm) {
 1726     rtm_stack_locking(objReg, tmpReg, scrReg, cx2Reg,
 1727                       stack_rtm_counters, method_data, profile_rtm,
 1728                       DONE_LABEL, IsInflated);
 1729   }
 1730 #endif // INCLUDE_RTM_OPT
 1731 
 1732   movptr(tmpReg, Address(objReg, oopDesc::mark_offset_in_bytes()));          // [FETCH]
 1733   testptr(tmpReg, markWord::monitor_value); // inflated vs stack-locked|neutral|biased
 1734   jccb(Assembler::notZero, IsInflated);
 1735 
 1736   // Attempt stack-locking ...
 1737   orptr (tmpReg, markWord::unlocked_value);
 1738   if (EnableValhalla &amp;&amp; !UseBiasedLocking) {
 1739     // Mask always_locked bit such that we go to the slow path if object is a value type
 1740     andptr(tmpReg, ~((int) markWord::biased_lock_bit_in_place));
 1741   }
 1742   movptr(Address(boxReg, 0), tmpReg);          // Anticipate successful CAS
 1743   lock();
 1744   cmpxchgptr(boxReg, Address(objReg, oopDesc::mark_offset_in_bytes()));      // Updates tmpReg
 1745   if (counters != NULL) {
 1746     cond_inc32(Assembler::equal,
 1747                ExternalAddress((address)counters-&gt;fast_path_entry_count_addr()));
 1748   }
 1749   jcc(Assembler::equal, DONE_LABEL);           // Success
 1750 
 1751   // Recursive locking.
 1752   // The object is stack-locked: markword contains stack pointer to BasicLock.
 1753   // Locked by current thread if difference with current SP is less than one page.
 1754   subptr(tmpReg, rsp);
 1755   // Next instruction set ZFlag == 1 (Success) if difference is less then one page.
 1756   andptr(tmpReg, (int32_t) (NOT_LP64(0xFFFFF003) LP64_ONLY(7 - os::vm_page_size())) );
 1757   movptr(Address(boxReg, 0), tmpReg);
 1758   if (counters != NULL) {
 1759     cond_inc32(Assembler::equal,
 1760                ExternalAddress((address)counters-&gt;fast_path_entry_count_addr()));
 1761   }
 1762   jmp(DONE_LABEL);
 1763 
 1764   bind(IsInflated);
 1765   // The object is inflated. tmpReg contains pointer to ObjectMonitor* + markWord::monitor_value
 1766 
 1767 #if INCLUDE_RTM_OPT
 1768   // Use the same RTM locking code in 32- and 64-bit VM.
 1769   if (use_rtm) {
 1770     rtm_inflated_locking(objReg, boxReg, tmpReg, scrReg, cx1Reg, cx2Reg,
 1771                          rtm_counters, method_data, profile_rtm, DONE_LABEL);
 1772   } else {
 1773 #endif // INCLUDE_RTM_OPT
 1774 
 1775 #ifndef _LP64
 1776   // The object is inflated.
 1777 
 1778   // boxReg refers to the on-stack BasicLock in the current frame.
 1779   // We&#39;d like to write:
 1780   //   set box-&gt;_displaced_header = markWord::unused_mark().  Any non-0 value suffices.
 1781   // This is convenient but results a ST-before-CAS penalty.  The following CAS suffers
 1782   // additional latency as we have another ST in the store buffer that must drain.
 1783 
 1784   // avoid ST-before-CAS
 1785   // register juggle because we need tmpReg for cmpxchgptr below
 1786   movptr(scrReg, boxReg);
 1787   movptr(boxReg, tmpReg);                   // consider: LEA box, [tmp-2]
 1788 
 1789   // Optimistic form: consider XORL tmpReg,tmpReg
 1790   movptr(tmpReg, NULL_WORD);
 1791 
 1792   // Appears unlocked - try to swing _owner from null to non-null.
 1793   // Ideally, I&#39;d manifest &quot;Self&quot; with get_thread and then attempt
 1794   // to CAS the register containing Self into m-&gt;Owner.
 1795   // But we don&#39;t have enough registers, so instead we can either try to CAS
 1796   // rsp or the address of the box (in scr) into &amp;m-&gt;owner.  If the CAS succeeds
 1797   // we later store &quot;Self&quot; into m-&gt;Owner.  Transiently storing a stack address
 1798   // (rsp or the address of the box) into  m-&gt;owner is harmless.
 1799   // Invariant: tmpReg == 0.  tmpReg is EAX which is the implicit cmpxchg comparand.
 1800   lock();
 1801   cmpxchgptr(scrReg, Address(boxReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)));
 1802   movptr(Address(scrReg, 0), 3);          // box-&gt;_displaced_header = 3
 1803   // If we weren&#39;t able to swing _owner from NULL to the BasicLock
 1804   // then take the slow path.
 1805   jccb  (Assembler::notZero, DONE_LABEL);
 1806   // update _owner from BasicLock to thread
 1807   get_thread (scrReg);                    // beware: clobbers ICCs
 1808   movptr(Address(boxReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)), scrReg);
 1809   xorptr(boxReg, boxReg);                 // set icc.ZFlag = 1 to indicate success
 1810 
 1811   // If the CAS fails we can either retry or pass control to the slow path.
 1812   // We use the latter tactic.
 1813   // Pass the CAS result in the icc.ZFlag into DONE_LABEL
 1814   // If the CAS was successful ...
 1815   //   Self has acquired the lock
 1816   //   Invariant: m-&gt;_recursions should already be 0, so we don&#39;t need to explicitly set it.
 1817   // Intentional fall-through into DONE_LABEL ...
 1818 #else // _LP64
 1819   // It&#39;s inflated and we use scrReg for ObjectMonitor* in this section.
 1820   movq(scrReg, tmpReg);
 1821   xorq(tmpReg, tmpReg);
 1822   lock();
 1823   cmpxchgptr(r15_thread, Address(scrReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)));
 1824   // Unconditionally set box-&gt;_displaced_header = markWord::unused_mark().
 1825   // Without cast to int32_t this style of movptr will destroy r10 which is typically obj.
 1826   movptr(Address(boxReg, 0), (int32_t)intptr_t(markWord::unused_mark().value()));
 1827   // Intentional fall-through into DONE_LABEL ...
 1828   // Propagate ICC.ZF from CAS above into DONE_LABEL.
 1829 #endif // _LP64
 1830 #if INCLUDE_RTM_OPT
 1831   } // use_rtm()
 1832 #endif
 1833   // DONE_LABEL is a hot target - we&#39;d really like to place it at the
 1834   // start of cache line by padding with NOPs.
 1835   // See the AMD and Intel software optimization manuals for the
 1836   // most efficient &quot;long&quot; NOP encodings.
 1837   // Unfortunately none of our alignment mechanisms suffice.
 1838   bind(DONE_LABEL);
 1839 
 1840   // At DONE_LABEL the icc ZFlag is set as follows ...
 1841   // fast_unlock uses the same protocol.
 1842   // ZFlag == 1 -&gt; Success
 1843   // ZFlag == 0 -&gt; Failure - force control through the slow path
 1844 }
 1845 
 1846 // obj: object to unlock
 1847 // box: box address (displaced header location), killed.  Must be EAX.
 1848 // tmp: killed, cannot be obj nor box.
 1849 //
 1850 // Some commentary on balanced locking:
 1851 //
 1852 // fast_lock and fast_unlock are emitted only for provably balanced lock sites.
 1853 // Methods that don&#39;t have provably balanced locking are forced to run in the
 1854 // interpreter - such methods won&#39;t be compiled to use fast_lock and fast_unlock.
 1855 // The interpreter provides two properties:
 1856 // I1:  At return-time the interpreter automatically and quietly unlocks any
 1857 //      objects acquired the current activation (frame).  Recall that the
 1858 //      interpreter maintains an on-stack list of locks currently held by
 1859 //      a frame.
 1860 // I2:  If a method attempts to unlock an object that is not held by the
 1861 //      the frame the interpreter throws IMSX.
 1862 //
 1863 // Lets say A(), which has provably balanced locking, acquires O and then calls B().
 1864 // B() doesn&#39;t have provably balanced locking so it runs in the interpreter.
 1865 // Control returns to A() and A() unlocks O.  By I1 and I2, above, we know that O
 1866 // is still locked by A().
 1867 //
 1868 // The only other source of unbalanced locking would be JNI.  The &quot;Java Native Interface:
 1869 // Programmer&#39;s Guide and Specification&quot; claims that an object locked by jni_monitorenter
 1870 // should not be unlocked by &quot;normal&quot; java-level locking and vice-versa.  The specification
 1871 // doesn&#39;t specify what will occur if a program engages in such mixed-mode locking, however.
 1872 // Arguably given that the spec legislates the JNI case as undefined our implementation
 1873 // could reasonably *avoid* checking owner in fast_unlock().
 1874 // In the interest of performance we elide m-&gt;Owner==Self check in unlock.
 1875 // A perfectly viable alternative is to elide the owner check except when
 1876 // Xcheck:jni is enabled.
 1877 
 1878 void MacroAssembler::fast_unlock(Register objReg, Register boxReg, Register tmpReg, bool use_rtm) {
 1879   assert(boxReg == rax, &quot;&quot;);
 1880   assert_different_registers(objReg, boxReg, tmpReg);
 1881 
 1882   Label DONE_LABEL, Stacked, CheckSucc;
 1883 
 1884   // Critically, the biased locking test must have precedence over
 1885   // and appear before the (box-&gt;dhw == 0) recursive stack-lock test.
 1886   if (UseBiasedLocking &amp;&amp; !UseOptoBiasInlining) {
 1887     biased_locking_exit(objReg, tmpReg, DONE_LABEL);
 1888   }
 1889 
 1890 #if INCLUDE_RTM_OPT
 1891   if (UseRTMForStackLocks &amp;&amp; use_rtm) {
 1892     assert(!UseBiasedLocking, &quot;Biased locking is not supported with RTM locking&quot;);
 1893     Label L_regular_unlock;
 1894     movptr(tmpReg, Address(objReg, oopDesc::mark_offset_in_bytes())); // fetch markword
 1895     andptr(tmpReg, markWord::biased_lock_mask_in_place);              // look at 3 lock bits
 1896     cmpptr(tmpReg, markWord::unlocked_value);                         // bits = 001 unlocked
 1897     jccb(Assembler::notEqual, L_regular_unlock);                      // if !HLE RegularLock
 1898     xend();                                                           // otherwise end...
 1899     jmp(DONE_LABEL);                                                  // ... and we&#39;re done
 1900     bind(L_regular_unlock);
 1901   }
 1902 #endif
 1903 
 1904   cmpptr(Address(boxReg, 0), (int32_t)NULL_WORD);                   // Examine the displaced header
 1905   jcc   (Assembler::zero, DONE_LABEL);                              // 0 indicates recursive stack-lock
 1906   movptr(tmpReg, Address(objReg, oopDesc::mark_offset_in_bytes())); // Examine the object&#39;s markword
 1907   testptr(tmpReg, markWord::monitor_value);                         // Inflated?
 1908   jccb  (Assembler::zero, Stacked);
 1909 
 1910   // It&#39;s inflated.
 1911 #if INCLUDE_RTM_OPT
 1912   if (use_rtm) {
 1913     Label L_regular_inflated_unlock;
 1914     int owner_offset = OM_OFFSET_NO_MONITOR_VALUE_TAG(owner);
 1915     movptr(boxReg, Address(tmpReg, owner_offset));
 1916     testptr(boxReg, boxReg);
 1917     jccb(Assembler::notZero, L_regular_inflated_unlock);
 1918     xend();
 1919     jmpb(DONE_LABEL);
 1920     bind(L_regular_inflated_unlock);
 1921   }
 1922 #endif
 1923 
 1924   // Despite our balanced locking property we still check that m-&gt;_owner == Self
 1925   // as java routines or native JNI code called by this thread might
 1926   // have released the lock.
 1927   // Refer to the comments in synchronizer.cpp for how we might encode extra
 1928   // state in _succ so we can avoid fetching EntryList|cxq.
 1929   //
 1930   // I&#39;d like to add more cases in fast_lock() and fast_unlock() --
 1931   // such as recursive enter and exit -- but we have to be wary of
 1932   // I$ bloat, T$ effects and BP$ effects.
 1933   //
 1934   // If there&#39;s no contention try a 1-0 exit.  That is, exit without
 1935   // a costly MEMBAR or CAS.  See synchronizer.cpp for details on how
 1936   // we detect and recover from the race that the 1-0 exit admits.
 1937   //
 1938   // Conceptually fast_unlock() must execute a STST|LDST &quot;release&quot; barrier
 1939   // before it STs null into _owner, releasing the lock.  Updates
 1940   // to data protected by the critical section must be visible before
 1941   // we drop the lock (and thus before any other thread could acquire
 1942   // the lock and observe the fields protected by the lock).
 1943   // IA32&#39;s memory-model is SPO, so STs are ordered with respect to
 1944   // each other and there&#39;s no need for an explicit barrier (fence).
 1945   // See also http://gee.cs.oswego.edu/dl/jmm/cookbook.html.
 1946 #ifndef _LP64
 1947   get_thread (boxReg);
 1948 
 1949   // Note that we could employ various encoding schemes to reduce
 1950   // the number of loads below (currently 4) to just 2 or 3.
 1951   // Refer to the comments in synchronizer.cpp.
 1952   // In practice the chain of fetches doesn&#39;t seem to impact performance, however.
 1953   xorptr(boxReg, boxReg);
 1954   orptr(boxReg, Address(tmpReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(recursions)));
 1955   jccb  (Assembler::notZero, DONE_LABEL);
 1956   movptr(boxReg, Address(tmpReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(EntryList)));
 1957   orptr(boxReg, Address(tmpReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(cxq)));
 1958   jccb  (Assembler::notZero, CheckSucc);
 1959   movptr(Address(tmpReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)), NULL_WORD);
 1960   jmpb  (DONE_LABEL);
 1961 
 1962   bind (Stacked);
 1963   // It&#39;s not inflated and it&#39;s not recursively stack-locked and it&#39;s not biased.
 1964   // It must be stack-locked.
 1965   // Try to reset the header to displaced header.
 1966   // The &quot;box&quot; value on the stack is stable, so we can reload
 1967   // and be assured we observe the same value as above.
 1968   movptr(tmpReg, Address(boxReg, 0));
 1969   lock();
 1970   cmpxchgptr(tmpReg, Address(objReg, oopDesc::mark_offset_in_bytes())); // Uses RAX which is box
 1971   // Intention fall-thru into DONE_LABEL
 1972 
 1973   // DONE_LABEL is a hot target - we&#39;d really like to place it at the
 1974   // start of cache line by padding with NOPs.
 1975   // See the AMD and Intel software optimization manuals for the
 1976   // most efficient &quot;long&quot; NOP encodings.
 1977   // Unfortunately none of our alignment mechanisms suffice.
 1978   bind (CheckSucc);
 1979 #else // _LP64
 1980   // It&#39;s inflated
 1981   xorptr(boxReg, boxReg);
 1982   orptr(boxReg, Address(tmpReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(recursions)));
 1983   jccb  (Assembler::notZero, DONE_LABEL);
 1984   movptr(boxReg, Address(tmpReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(cxq)));
 1985   orptr(boxReg, Address(tmpReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(EntryList)));
 1986   jccb  (Assembler::notZero, CheckSucc);
 1987   // Without cast to int32_t this style of movptr will destroy r10 which is typically obj.
 1988   movptr(Address(tmpReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)), (int32_t)NULL_WORD);
 1989   jmpb  (DONE_LABEL);
 1990 
 1991   // Try to avoid passing control into the slow_path ...
 1992   Label LSuccess, LGoSlowPath ;
 1993   bind  (CheckSucc);
 1994 
 1995   // The following optional optimization can be elided if necessary
 1996   // Effectively: if (succ == null) goto slow path
 1997   // The code reduces the window for a race, however,
 1998   // and thus benefits performance.
 1999   cmpptr(Address(tmpReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(succ)), (int32_t)NULL_WORD);
 2000   jccb  (Assembler::zero, LGoSlowPath);
 2001 
 2002   xorptr(boxReg, boxReg);
 2003   // Without cast to int32_t this style of movptr will destroy r10 which is typically obj.
 2004   movptr(Address(tmpReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)), (int32_t)NULL_WORD);
 2005 
 2006   // Memory barrier/fence
 2007   // Dekker pivot point -- fulcrum : ST Owner; MEMBAR; LD Succ
 2008   // Instead of MFENCE we use a dummy locked add of 0 to the top-of-stack.
 2009   // This is faster on Nehalem and AMD Shanghai/Barcelona.
 2010   // See https://blogs.oracle.com/dave/entry/instruction_selection_for_volatile_fences
 2011   // We might also restructure (ST Owner=0;barrier;LD _Succ) to
 2012   // (mov box,0; xchgq box, &amp;m-&gt;Owner; LD _succ) .
 2013   lock(); addl(Address(rsp, 0), 0);
 2014 
 2015   cmpptr(Address(tmpReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(succ)), (int32_t)NULL_WORD);
 2016   jccb  (Assembler::notZero, LSuccess);
 2017 
 2018   // Rare inopportune interleaving - race.
 2019   // The successor vanished in the small window above.
 2020   // The lock is contended -- (cxq|EntryList) != null -- and there&#39;s no apparent successor.
 2021   // We need to ensure progress and succession.
 2022   // Try to reacquire the lock.
 2023   // If that fails then the new owner is responsible for succession and this
 2024   // thread needs to take no further action and can exit via the fast path (success).
 2025   // If the re-acquire succeeds then pass control into the slow path.
 2026   // As implemented, this latter mode is horrible because we generated more
 2027   // coherence traffic on the lock *and* artifically extended the critical section
 2028   // length while by virtue of passing control into the slow path.
 2029 
 2030   // box is really RAX -- the following CMPXCHG depends on that binding
 2031   // cmpxchg R,[M] is equivalent to rax = CAS(M,rax,R)
 2032   lock();
 2033   cmpxchgptr(r15_thread, Address(tmpReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)));
 2034   // There&#39;s no successor so we tried to regrab the lock.
 2035   // If that didn&#39;t work, then another thread grabbed the
 2036   // lock so we&#39;re done (and exit was a success).
 2037   jccb  (Assembler::notEqual, LSuccess);
 2038   // Intentional fall-through into slow path
 2039 
 2040   bind  (LGoSlowPath);
 2041   orl   (boxReg, 1);                      // set ICC.ZF=0 to indicate failure
 2042   jmpb  (DONE_LABEL);
 2043 
 2044   bind  (LSuccess);
 2045   testl (boxReg, 0);                      // set ICC.ZF=1 to indicate success
 2046   jmpb  (DONE_LABEL);
 2047 
 2048   bind  (Stacked);
 2049   movptr(tmpReg, Address (boxReg, 0));      // re-fetch
 2050   lock();
 2051   cmpxchgptr(tmpReg, Address(objReg, oopDesc::mark_offset_in_bytes())); // Uses RAX which is box
 2052 
 2053 #endif
 2054   bind(DONE_LABEL);
 2055 }
 2056 #endif // COMPILER2
 2057 
 2058 void MacroAssembler::c2bool(Register x) {
 2059   // implements x == 0 ? 0 : 1
 2060   // note: must only look at least-significant byte of x
 2061   //       since C-style booleans are stored in one byte
 2062   //       only! (was bug)
 2063   andl(x, 0xFF);
 2064   setb(Assembler::notZero, x);
 2065 }
 2066 
 2067 // Wouldn&#39;t need if AddressLiteral version had new name
 2068 void MacroAssembler::call(Label&amp; L, relocInfo::relocType rtype) {
 2069   Assembler::call(L, rtype);
 2070 }
 2071 
 2072 void MacroAssembler::call(Register entry) {
 2073   Assembler::call(entry);
 2074 }
 2075 
 2076 void MacroAssembler::call(AddressLiteral entry) {
 2077   if (reachable(entry)) {
 2078     Assembler::call_literal(entry.target(), entry.rspec());
 2079   } else {
 2080     lea(rscratch1, entry);
 2081     Assembler::call(rscratch1);
 2082   }
 2083 }
 2084 
 2085 void MacroAssembler::ic_call(address entry, jint method_index) {
 2086   RelocationHolder rh = virtual_call_Relocation::spec(pc(), method_index);
 2087   movptr(rax, (intptr_t)Universe::non_oop_word());
 2088   call(AddressLiteral(entry, rh));
 2089 }
 2090 
 2091 // Implementation of call_VM versions
 2092 
 2093 void MacroAssembler::call_VM(Register oop_result,
 2094                              address entry_point,
 2095                              bool check_exceptions) {
 2096   Label C, E;
 2097   call(C, relocInfo::none);
 2098   jmp(E);
 2099 
 2100   bind(C);
 2101   call_VM_helper(oop_result, entry_point, 0, check_exceptions);
 2102   ret(0);
 2103 
 2104   bind(E);
 2105 }
 2106 
 2107 void MacroAssembler::call_VM(Register oop_result,
 2108                              address entry_point,
 2109                              Register arg_1,
 2110                              bool check_exceptions) {
 2111   Label C, E;
 2112   call(C, relocInfo::none);
 2113   jmp(E);
 2114 
 2115   bind(C);
 2116   pass_arg1(this, arg_1);
 2117   call_VM_helper(oop_result, entry_point, 1, check_exceptions);
 2118   ret(0);
 2119 
 2120   bind(E);
 2121 }
 2122 
 2123 void MacroAssembler::call_VM(Register oop_result,
 2124                              address entry_point,
 2125                              Register arg_1,
 2126                              Register arg_2,
 2127                              bool check_exceptions) {
 2128   Label C, E;
 2129   call(C, relocInfo::none);
 2130   jmp(E);
 2131 
 2132   bind(C);
 2133 
 2134   LP64_ONLY(assert(arg_1 != c_rarg2, &quot;smashed arg&quot;));
 2135 
 2136   pass_arg2(this, arg_2);
 2137   pass_arg1(this, arg_1);
 2138   call_VM_helper(oop_result, entry_point, 2, check_exceptions);
 2139   ret(0);
 2140 
 2141   bind(E);
 2142 }
 2143 
 2144 void MacroAssembler::call_VM(Register oop_result,
 2145                              address entry_point,
 2146                              Register arg_1,
 2147                              Register arg_2,
 2148                              Register arg_3,
 2149                              bool check_exceptions) {
 2150   Label C, E;
 2151   call(C, relocInfo::none);
 2152   jmp(E);
 2153 
 2154   bind(C);
 2155 
 2156   LP64_ONLY(assert(arg_1 != c_rarg3, &quot;smashed arg&quot;));
 2157   LP64_ONLY(assert(arg_2 != c_rarg3, &quot;smashed arg&quot;));
 2158   pass_arg3(this, arg_3);
 2159 
 2160   LP64_ONLY(assert(arg_1 != c_rarg2, &quot;smashed arg&quot;));
 2161   pass_arg2(this, arg_2);
 2162 
 2163   pass_arg1(this, arg_1);
 2164   call_VM_helper(oop_result, entry_point, 3, check_exceptions);
 2165   ret(0);
 2166 
 2167   bind(E);
 2168 }
 2169 
 2170 void MacroAssembler::call_VM(Register oop_result,
 2171                              Register last_java_sp,
 2172                              address entry_point,
 2173                              int number_of_arguments,
 2174                              bool check_exceptions) {
 2175   Register thread = LP64_ONLY(r15_thread) NOT_LP64(noreg);
 2176   call_VM_base(oop_result, thread, last_java_sp, entry_point, number_of_arguments, check_exceptions);
 2177 }
 2178 
 2179 void MacroAssembler::call_VM(Register oop_result,
 2180                              Register last_java_sp,
 2181                              address entry_point,
 2182                              Register arg_1,
 2183                              bool check_exceptions) {
 2184   pass_arg1(this, arg_1);
 2185   call_VM(oop_result, last_java_sp, entry_point, 1, check_exceptions);
 2186 }
 2187 
 2188 void MacroAssembler::call_VM(Register oop_result,
 2189                              Register last_java_sp,
 2190                              address entry_point,
 2191                              Register arg_1,
 2192                              Register arg_2,
 2193                              bool check_exceptions) {
 2194 
 2195   LP64_ONLY(assert(arg_1 != c_rarg2, &quot;smashed arg&quot;));
 2196   pass_arg2(this, arg_2);
 2197   pass_arg1(this, arg_1);
 2198   call_VM(oop_result, last_java_sp, entry_point, 2, check_exceptions);
 2199 }
 2200 
 2201 void MacroAssembler::call_VM(Register oop_result,
 2202                              Register last_java_sp,
 2203                              address entry_point,
 2204                              Register arg_1,
 2205                              Register arg_2,
 2206                              Register arg_3,
 2207                              bool check_exceptions) {
 2208   LP64_ONLY(assert(arg_1 != c_rarg3, &quot;smashed arg&quot;));
 2209   LP64_ONLY(assert(arg_2 != c_rarg3, &quot;smashed arg&quot;));
 2210   pass_arg3(this, arg_3);
 2211   LP64_ONLY(assert(arg_1 != c_rarg2, &quot;smashed arg&quot;));
 2212   pass_arg2(this, arg_2);
 2213   pass_arg1(this, arg_1);
 2214   call_VM(oop_result, last_java_sp, entry_point, 3, check_exceptions);
 2215 }
 2216 
 2217 void MacroAssembler::super_call_VM(Register oop_result,
 2218                                    Register last_java_sp,
 2219                                    address entry_point,
 2220                                    int number_of_arguments,
 2221                                    bool check_exceptions) {
 2222   Register thread = LP64_ONLY(r15_thread) NOT_LP64(noreg);
 2223   MacroAssembler::call_VM_base(oop_result, thread, last_java_sp, entry_point, number_of_arguments, check_exceptions);
 2224 }
 2225 
 2226 void MacroAssembler::super_call_VM(Register oop_result,
 2227                                    Register last_java_sp,
 2228                                    address entry_point,
 2229                                    Register arg_1,
 2230                                    bool check_exceptions) {
 2231   pass_arg1(this, arg_1);
 2232   super_call_VM(oop_result, last_java_sp, entry_point, 1, check_exceptions);
 2233 }
 2234 
 2235 void MacroAssembler::super_call_VM(Register oop_result,
 2236                                    Register last_java_sp,
 2237                                    address entry_point,
 2238                                    Register arg_1,
 2239                                    Register arg_2,
 2240                                    bool check_exceptions) {
 2241 
 2242   LP64_ONLY(assert(arg_1 != c_rarg2, &quot;smashed arg&quot;));
 2243   pass_arg2(this, arg_2);
 2244   pass_arg1(this, arg_1);
 2245   super_call_VM(oop_result, last_java_sp, entry_point, 2, check_exceptions);
 2246 }
 2247 
 2248 void MacroAssembler::super_call_VM(Register oop_result,
 2249                                    Register last_java_sp,
 2250                                    address entry_point,
 2251                                    Register arg_1,
 2252                                    Register arg_2,
 2253                                    Register arg_3,
 2254                                    bool check_exceptions) {
 2255   LP64_ONLY(assert(arg_1 != c_rarg3, &quot;smashed arg&quot;));
 2256   LP64_ONLY(assert(arg_2 != c_rarg3, &quot;smashed arg&quot;));
 2257   pass_arg3(this, arg_3);
 2258   LP64_ONLY(assert(arg_1 != c_rarg2, &quot;smashed arg&quot;));
 2259   pass_arg2(this, arg_2);
 2260   pass_arg1(this, arg_1);
 2261   super_call_VM(oop_result, last_java_sp, entry_point, 3, check_exceptions);
 2262 }
 2263 
 2264 void MacroAssembler::call_VM_base(Register oop_result,
 2265                                   Register java_thread,
 2266                                   Register last_java_sp,
 2267                                   address  entry_point,
 2268                                   int      number_of_arguments,
 2269                                   bool     check_exceptions) {
 2270   // determine java_thread register
 2271   if (!java_thread-&gt;is_valid()) {
 2272 #ifdef _LP64
 2273     java_thread = r15_thread;
 2274 #else
 2275     java_thread = rdi;
 2276     get_thread(java_thread);
 2277 #endif // LP64
 2278   }
 2279   // determine last_java_sp register
 2280   if (!last_java_sp-&gt;is_valid()) {
 2281     last_java_sp = rsp;
 2282   }
 2283   // debugging support
 2284   assert(number_of_arguments &gt;= 0   , &quot;cannot have negative number of arguments&quot;);
 2285   LP64_ONLY(assert(java_thread == r15_thread, &quot;unexpected register&quot;));
 2286 #ifdef ASSERT
 2287   // TraceBytecodes does not use r12 but saves it over the call, so don&#39;t verify
 2288   // r12 is the heapbase.
 2289   LP64_ONLY(if ((UseCompressedOops || UseCompressedClassPointers) &amp;&amp; !TraceBytecodes) verify_heapbase(&quot;call_VM_base: heap base corrupted?&quot;);)
 2290 #endif // ASSERT
 2291 
 2292   assert(java_thread != oop_result  , &quot;cannot use the same register for java_thread &amp; oop_result&quot;);
 2293   assert(java_thread != last_java_sp, &quot;cannot use the same register for java_thread &amp; last_java_sp&quot;);
 2294 
 2295   // push java thread (becomes first argument of C function)
 2296 
 2297   NOT_LP64(push(java_thread); number_of_arguments++);
 2298   LP64_ONLY(mov(c_rarg0, r15_thread));
 2299 
 2300   // set last Java frame before call
 2301   assert(last_java_sp != rbp, &quot;can&#39;t use ebp/rbp&quot;);
 2302 
 2303   // Only interpreter should have to set fp
 2304   set_last_Java_frame(java_thread, last_java_sp, rbp, NULL);
 2305 
 2306   // do the call, remove parameters
 2307   MacroAssembler::call_VM_leaf_base(entry_point, number_of_arguments);
 2308 
 2309   // restore the thread (cannot use the pushed argument since arguments
 2310   // may be overwritten by C code generated by an optimizing compiler);
 2311   // however can use the register value directly if it is callee saved.
 2312   if (LP64_ONLY(true ||) java_thread == rdi || java_thread == rsi) {
 2313     // rdi &amp; rsi (also r15) are callee saved -&gt; nothing to do
 2314 #ifdef ASSERT
 2315     guarantee(java_thread != rax, &quot;change this code&quot;);
 2316     push(rax);
 2317     { Label L;
 2318       get_thread(rax);
 2319       cmpptr(java_thread, rax);
 2320       jcc(Assembler::equal, L);
 2321       STOP(&quot;MacroAssembler::call_VM_base: rdi not callee saved?&quot;);
 2322       bind(L);
 2323     }
 2324     pop(rax);
 2325 #endif
 2326   } else {
 2327     get_thread(java_thread);
 2328   }
 2329   // reset last Java frame
 2330   // Only interpreter should have to clear fp
 2331   reset_last_Java_frame(java_thread, true);
 2332 
 2333    // C++ interp handles this in the interpreter
 2334   check_and_handle_popframe(java_thread);
 2335   check_and_handle_earlyret(java_thread);
 2336 
 2337   if (check_exceptions) {
 2338     // check for pending exceptions (java_thread is set upon return)
 2339     cmpptr(Address(java_thread, Thread::pending_exception_offset()), (int32_t) NULL_WORD);
 2340 #ifndef _LP64
 2341     jump_cc(Assembler::notEqual,
 2342             RuntimeAddress(StubRoutines::forward_exception_entry()));
 2343 #else
 2344     // This used to conditionally jump to forward_exception however it is
 2345     // possible if we relocate that the branch will not reach. So we must jump
 2346     // around so we can always reach
 2347 
 2348     Label ok;
 2349     jcc(Assembler::equal, ok);
 2350     jump(RuntimeAddress(StubRoutines::forward_exception_entry()));
 2351     bind(ok);
 2352 #endif // LP64
 2353   }
 2354 
 2355   // get oop result if there is one and reset the value in the thread
 2356   if (oop_result-&gt;is_valid()) {
 2357     get_vm_result(oop_result, java_thread);
 2358   }
 2359 }
 2360 
 2361 void MacroAssembler::call_VM_helper(Register oop_result, address entry_point, int number_of_arguments, bool check_exceptions) {
 2362 
 2363   // Calculate the value for last_Java_sp
 2364   // somewhat subtle. call_VM does an intermediate call
 2365   // which places a return address on the stack just under the
 2366   // stack pointer as the user finsihed with it. This allows
 2367   // use to retrieve last_Java_pc from last_Java_sp[-1].
 2368   // On 32bit we then have to push additional args on the stack to accomplish
 2369   // the actual requested call. On 64bit call_VM only can use register args
 2370   // so the only extra space is the return address that call_VM created.
 2371   // This hopefully explains the calculations here.
 2372 
 2373 #ifdef _LP64
 2374   // We&#39;ve pushed one address, correct last_Java_sp
 2375   lea(rax, Address(rsp, wordSize));
 2376 #else
 2377   lea(rax, Address(rsp, (1 + number_of_arguments) * wordSize));
 2378 #endif // LP64
 2379 
 2380   call_VM_base(oop_result, noreg, rax, entry_point, number_of_arguments, check_exceptions);
 2381 
 2382 }
 2383 
 2384 // Use this method when MacroAssembler version of call_VM_leaf_base() should be called from Interpreter.
 2385 void MacroAssembler::call_VM_leaf0(address entry_point) {
 2386   MacroAssembler::call_VM_leaf_base(entry_point, 0);
 2387 }
 2388 
 2389 void MacroAssembler::call_VM_leaf(address entry_point, int number_of_arguments) {
 2390   call_VM_leaf_base(entry_point, number_of_arguments);
 2391 }
 2392 
 2393 void MacroAssembler::call_VM_leaf(address entry_point, Register arg_0) {
 2394   pass_arg0(this, arg_0);
 2395   call_VM_leaf(entry_point, 1);
 2396 }
 2397 
 2398 void MacroAssembler::call_VM_leaf(address entry_point, Register arg_0, Register arg_1) {
 2399 
 2400   LP64_ONLY(assert(arg_0 != c_rarg1, &quot;smashed arg&quot;));
 2401   pass_arg1(this, arg_1);
 2402   pass_arg0(this, arg_0);
 2403   call_VM_leaf(entry_point, 2);
 2404 }
 2405 
 2406 void MacroAssembler::call_VM_leaf(address entry_point, Register arg_0, Register arg_1, Register arg_2) {
 2407   LP64_ONLY(assert(arg_0 != c_rarg2, &quot;smashed arg&quot;));
 2408   LP64_ONLY(assert(arg_1 != c_rarg2, &quot;smashed arg&quot;));
 2409   pass_arg2(this, arg_2);
 2410   LP64_ONLY(assert(arg_0 != c_rarg1, &quot;smashed arg&quot;));
 2411   pass_arg1(this, arg_1);
 2412   pass_arg0(this, arg_0);
 2413   call_VM_leaf(entry_point, 3);
 2414 }
 2415 
 2416 void MacroAssembler::super_call_VM_leaf(address entry_point) {
 2417   MacroAssembler::call_VM_leaf_base(entry_point, 1);
 2418 }
 2419 
 2420 void MacroAssembler::super_call_VM_leaf(address entry_point, Register arg_0) {
 2421   pass_arg0(this, arg_0);
 2422   MacroAssembler::call_VM_leaf_base(entry_point, 1);
 2423 }
 2424 
 2425 void MacroAssembler::super_call_VM_leaf(address entry_point, Register arg_0, Register arg_1) {
 2426 
 2427   LP64_ONLY(assert(arg_0 != c_rarg1, &quot;smashed arg&quot;));
 2428   pass_arg1(this, arg_1);
 2429   pass_arg0(this, arg_0);
 2430   MacroAssembler::call_VM_leaf_base(entry_point, 2);
 2431 }
 2432 
 2433 void MacroAssembler::super_call_VM_leaf(address entry_point, Register arg_0, Register arg_1, Register arg_2) {
 2434   LP64_ONLY(assert(arg_0 != c_rarg2, &quot;smashed arg&quot;));
 2435   LP64_ONLY(assert(arg_1 != c_rarg2, &quot;smashed arg&quot;));
 2436   pass_arg2(this, arg_2);
 2437   LP64_ONLY(assert(arg_0 != c_rarg1, &quot;smashed arg&quot;));
 2438   pass_arg1(this, arg_1);
 2439   pass_arg0(this, arg_0);
 2440   MacroAssembler::call_VM_leaf_base(entry_point, 3);
 2441 }
 2442 
 2443 void MacroAssembler::super_call_VM_leaf(address entry_point, Register arg_0, Register arg_1, Register arg_2, Register arg_3) {
 2444   LP64_ONLY(assert(arg_0 != c_rarg3, &quot;smashed arg&quot;));
 2445   LP64_ONLY(assert(arg_1 != c_rarg3, &quot;smashed arg&quot;));
 2446   LP64_ONLY(assert(arg_2 != c_rarg3, &quot;smashed arg&quot;));
 2447   pass_arg3(this, arg_3);
 2448   LP64_ONLY(assert(arg_0 != c_rarg2, &quot;smashed arg&quot;));
 2449   LP64_ONLY(assert(arg_1 != c_rarg2, &quot;smashed arg&quot;));
 2450   pass_arg2(this, arg_2);
 2451   LP64_ONLY(assert(arg_0 != c_rarg1, &quot;smashed arg&quot;));
 2452   pass_arg1(this, arg_1);
 2453   pass_arg0(this, arg_0);
 2454   MacroAssembler::call_VM_leaf_base(entry_point, 4);
 2455 }
 2456 
 2457 void MacroAssembler::get_vm_result(Register oop_result, Register java_thread) {
 2458   movptr(oop_result, Address(java_thread, JavaThread::vm_result_offset()));
 2459   movptr(Address(java_thread, JavaThread::vm_result_offset()), NULL_WORD);
 2460   verify_oop_msg(oop_result, &quot;broken oop in call_VM_base&quot;);
 2461 }
 2462 
 2463 void MacroAssembler::get_vm_result_2(Register metadata_result, Register java_thread) {
 2464   movptr(metadata_result, Address(java_thread, JavaThread::vm_result_2_offset()));
 2465   movptr(Address(java_thread, JavaThread::vm_result_2_offset()), NULL_WORD);
 2466 }
 2467 
 2468 void MacroAssembler::check_and_handle_earlyret(Register java_thread) {
 2469 }
 2470 
 2471 void MacroAssembler::check_and_handle_popframe(Register java_thread) {
 2472 }
 2473 
 2474 void MacroAssembler::cmp32(AddressLiteral src1, int32_t imm) {
 2475   if (reachable(src1)) {
 2476     cmpl(as_Address(src1), imm);
 2477   } else {
 2478     lea(rscratch1, src1);
 2479     cmpl(Address(rscratch1, 0), imm);
 2480   }
 2481 }
 2482 
 2483 void MacroAssembler::cmp32(Register src1, AddressLiteral src2) {
 2484   assert(!src2.is_lval(), &quot;use cmpptr&quot;);
 2485   if (reachable(src2)) {
 2486     cmpl(src1, as_Address(src2));
 2487   } else {
 2488     lea(rscratch1, src2);
 2489     cmpl(src1, Address(rscratch1, 0));
 2490   }
 2491 }
 2492 
 2493 void MacroAssembler::cmp32(Register src1, int32_t imm) {
 2494   Assembler::cmpl(src1, imm);
 2495 }
 2496 
 2497 void MacroAssembler::cmp32(Register src1, Address src2) {
 2498   Assembler::cmpl(src1, src2);
 2499 }
 2500 
 2501 void MacroAssembler::cmpsd2int(XMMRegister opr1, XMMRegister opr2, Register dst, bool unordered_is_less) {
 2502   ucomisd(opr1, opr2);
 2503 
 2504   Label L;
 2505   if (unordered_is_less) {
 2506     movl(dst, -1);
 2507     jcc(Assembler::parity, L);
 2508     jcc(Assembler::below , L);
 2509     movl(dst, 0);
 2510     jcc(Assembler::equal , L);
 2511     increment(dst);
 2512   } else { // unordered is greater
 2513     movl(dst, 1);
 2514     jcc(Assembler::parity, L);
 2515     jcc(Assembler::above , L);
 2516     movl(dst, 0);
 2517     jcc(Assembler::equal , L);
 2518     decrementl(dst);
 2519   }
 2520   bind(L);
 2521 }
 2522 
 2523 void MacroAssembler::cmpss2int(XMMRegister opr1, XMMRegister opr2, Register dst, bool unordered_is_less) {
 2524   ucomiss(opr1, opr2);
 2525 
 2526   Label L;
 2527   if (unordered_is_less) {
 2528     movl(dst, -1);
 2529     jcc(Assembler::parity, L);
 2530     jcc(Assembler::below , L);
 2531     movl(dst, 0);
 2532     jcc(Assembler::equal , L);
 2533     increment(dst);
 2534   } else { // unordered is greater
 2535     movl(dst, 1);
 2536     jcc(Assembler::parity, L);
 2537     jcc(Assembler::above , L);
 2538     movl(dst, 0);
 2539     jcc(Assembler::equal , L);
 2540     decrementl(dst);
 2541   }
 2542   bind(L);
 2543 }
 2544 
 2545 
 2546 void MacroAssembler::cmp8(AddressLiteral src1, int imm) {
 2547   if (reachable(src1)) {
 2548     cmpb(as_Address(src1), imm);
 2549   } else {
 2550     lea(rscratch1, src1);
 2551     cmpb(Address(rscratch1, 0), imm);
 2552   }
 2553 }
 2554 
 2555 void MacroAssembler::cmpptr(Register src1, AddressLiteral src2) {
 2556 #ifdef _LP64
 2557   if (src2.is_lval()) {
 2558     movptr(rscratch1, src2);
 2559     Assembler::cmpq(src1, rscratch1);
 2560   } else if (reachable(src2)) {
 2561     cmpq(src1, as_Address(src2));
 2562   } else {
 2563     lea(rscratch1, src2);
 2564     Assembler::cmpq(src1, Address(rscratch1, 0));
 2565   }
 2566 #else
 2567   if (src2.is_lval()) {
 2568     cmp_literal32(src1, (int32_t) src2.target(), src2.rspec());
 2569   } else {
 2570     cmpl(src1, as_Address(src2));
 2571   }
 2572 #endif // _LP64
 2573 }
 2574 
 2575 void MacroAssembler::cmpptr(Address src1, AddressLiteral src2) {
 2576   assert(src2.is_lval(), &quot;not a mem-mem compare&quot;);
 2577 #ifdef _LP64
 2578   // moves src2&#39;s literal address
 2579   movptr(rscratch1, src2);
 2580   Assembler::cmpq(src1, rscratch1);
 2581 #else
 2582   cmp_literal32(src1, (int32_t) src2.target(), src2.rspec());
 2583 #endif // _LP64
 2584 }
 2585 
 2586 void MacroAssembler::cmpoop(Register src1, Register src2) {
 2587   BarrierSetAssembler* bs = BarrierSet::barrier_set()-&gt;barrier_set_assembler();
 2588   bs-&gt;obj_equals(this, src1, src2);
 2589 }
 2590 
 2591 void MacroAssembler::cmpoop(Register src1, Address src2) {
 2592   BarrierSetAssembler* bs = BarrierSet::barrier_set()-&gt;barrier_set_assembler();
 2593   bs-&gt;obj_equals(this, src1, src2);
 2594 }
 2595 
 2596 #ifdef _LP64
 2597 void MacroAssembler::cmpoop(Register src1, jobject src2) {
 2598   movoop(rscratch1, src2);
 2599   BarrierSetAssembler* bs = BarrierSet::barrier_set()-&gt;barrier_set_assembler();
 2600   bs-&gt;obj_equals(this, src1, rscratch1);
 2601 }
 2602 #endif
 2603 
 2604 void MacroAssembler::locked_cmpxchgptr(Register reg, AddressLiteral adr) {
 2605   if (reachable(adr)) {
 2606     lock();
 2607     cmpxchgptr(reg, as_Address(adr));
 2608   } else {
 2609     lea(rscratch1, adr);
 2610     lock();
 2611     cmpxchgptr(reg, Address(rscratch1, 0));
 2612   }
 2613 }
 2614 
 2615 void MacroAssembler::cmpxchgptr(Register reg, Address adr) {
 2616   LP64_ONLY(cmpxchgq(reg, adr)) NOT_LP64(cmpxchgl(reg, adr));
 2617 }
 2618 
 2619 void MacroAssembler::comisd(XMMRegister dst, AddressLiteral src) {
 2620   if (reachable(src)) {
 2621     Assembler::comisd(dst, as_Address(src));
 2622   } else {
 2623     lea(rscratch1, src);
 2624     Assembler::comisd(dst, Address(rscratch1, 0));
 2625   }
 2626 }
 2627 
 2628 void MacroAssembler::comiss(XMMRegister dst, AddressLiteral src) {
 2629   if (reachable(src)) {
 2630     Assembler::comiss(dst, as_Address(src));
 2631   } else {
 2632     lea(rscratch1, src);
 2633     Assembler::comiss(dst, Address(rscratch1, 0));
 2634   }
 2635 }
 2636 
 2637 
 2638 void MacroAssembler::cond_inc32(Condition cond, AddressLiteral counter_addr) {
 2639   Condition negated_cond = negate_condition(cond);
 2640   Label L;
 2641   jcc(negated_cond, L);
 2642   pushf(); // Preserve flags
 2643   atomic_incl(counter_addr);
 2644   popf();
 2645   bind(L);
 2646 }
 2647 
 2648 int MacroAssembler::corrected_idivl(Register reg) {
 2649   // Full implementation of Java idiv and irem; checks for
 2650   // special case as described in JVM spec., p.243 &amp; p.271.
 2651   // The function returns the (pc) offset of the idivl
 2652   // instruction - may be needed for implicit exceptions.
 2653   //
 2654   //         normal case                           special case
 2655   //
 2656   // input : rax,: dividend                         min_int
 2657   //         reg: divisor   (may not be rax,/rdx)   -1
 2658   //
 2659   // output: rax,: quotient  (= rax, idiv reg)       min_int
 2660   //         rdx: remainder (= rax, irem reg)       0
 2661   assert(reg != rax &amp;&amp; reg != rdx, &quot;reg cannot be rax, or rdx register&quot;);
 2662   const int min_int = 0x80000000;
 2663   Label normal_case, special_case;
 2664 
 2665   // check for special case
 2666   cmpl(rax, min_int);
 2667   jcc(Assembler::notEqual, normal_case);
 2668   xorl(rdx, rdx); // prepare rdx for possible special case (where remainder = 0)
 2669   cmpl(reg, -1);
 2670   jcc(Assembler::equal, special_case);
 2671 
 2672   // handle normal case
 2673   bind(normal_case);
 2674   cdql();
 2675   int idivl_offset = offset();
 2676   idivl(reg);
 2677 
 2678   // normal and special case exit
 2679   bind(special_case);
 2680 
 2681   return idivl_offset;
 2682 }
 2683 
 2684 
 2685 
 2686 void MacroAssembler::decrementl(Register reg, int value) {
 2687   if (value == min_jint) {subl(reg, value) ; return; }
 2688   if (value &lt;  0) { incrementl(reg, -value); return; }
 2689   if (value == 0) {                        ; return; }
 2690   if (value == 1 &amp;&amp; UseIncDec) { decl(reg) ; return; }
 2691   /* else */      { subl(reg, value)       ; return; }
 2692 }
 2693 
 2694 void MacroAssembler::decrementl(Address dst, int value) {
 2695   if (value == min_jint) {subl(dst, value) ; return; }
 2696   if (value &lt;  0) { incrementl(dst, -value); return; }
 2697   if (value == 0) {                        ; return; }
 2698   if (value == 1 &amp;&amp; UseIncDec) { decl(dst) ; return; }
 2699   /* else */      { subl(dst, value)       ; return; }
 2700 }
 2701 
 2702 void MacroAssembler::division_with_shift (Register reg, int shift_value) {
 2703   assert (shift_value &gt; 0, &quot;illegal shift value&quot;);
 2704   Label _is_positive;
 2705   testl (reg, reg);
 2706   jcc (Assembler::positive, _is_positive);
 2707   int offset = (1 &lt;&lt; shift_value) - 1 ;
 2708 
 2709   if (offset == 1) {
 2710     incrementl(reg);
 2711   } else {
 2712     addl(reg, offset);
 2713   }
 2714 
 2715   bind (_is_positive);
 2716   sarl(reg, shift_value);
 2717 }
 2718 
 2719 void MacroAssembler::divsd(XMMRegister dst, AddressLiteral src) {
 2720   if (reachable(src)) {
 2721     Assembler::divsd(dst, as_Address(src));
 2722   } else {
 2723     lea(rscratch1, src);
 2724     Assembler::divsd(dst, Address(rscratch1, 0));
 2725   }
 2726 }
 2727 
 2728 void MacroAssembler::divss(XMMRegister dst, AddressLiteral src) {
 2729   if (reachable(src)) {
 2730     Assembler::divss(dst, as_Address(src));
 2731   } else {
 2732     lea(rscratch1, src);
 2733     Assembler::divss(dst, Address(rscratch1, 0));
 2734   }
 2735 }
 2736 
 2737 #ifndef _LP64
 2738 void MacroAssembler::empty_FPU_stack() {
 2739   if (VM_Version::supports_mmx()) {
 2740     emms();
 2741   } else {
 2742     for (int i = 8; i-- &gt; 0; ) ffree(i);
 2743   }
 2744 }
 2745 #endif // !LP64
 2746 
 2747 
 2748 void MacroAssembler::enter() {
 2749   push(rbp);
 2750   mov(rbp, rsp);
 2751 }
 2752 
 2753 // A 5 byte nop that is safe for patching (see patch_verified_entry)
 2754 void MacroAssembler::fat_nop() {
 2755   if (UseAddressNop) {
 2756     addr_nop_5();
 2757   } else {
 2758     emit_int8(0x26); // es:
 2759     emit_int8(0x2e); // cs:
 2760     emit_int8(0x64); // fs:
 2761     emit_int8(0x65); // gs:
 2762     emit_int8((unsigned char)0x90);
 2763   }
 2764 }
 2765 
 2766 #if !defined(_LP64)
 2767 void MacroAssembler::fcmp(Register tmp) {
 2768   fcmp(tmp, 1, true, true);
 2769 }
 2770 
 2771 void MacroAssembler::fcmp(Register tmp, int index, bool pop_left, bool pop_right) {
 2772   assert(!pop_right || pop_left, &quot;usage error&quot;);
 2773   if (VM_Version::supports_cmov()) {
 2774     assert(tmp == noreg, &quot;unneeded temp&quot;);
 2775     if (pop_left) {
 2776       fucomip(index);
 2777     } else {
 2778       fucomi(index);
 2779     }
 2780     if (pop_right) {
 2781       fpop();
 2782     }
 2783   } else {
 2784     assert(tmp != noreg, &quot;need temp&quot;);
 2785     if (pop_left) {
 2786       if (pop_right) {
 2787         fcompp();
 2788       } else {
 2789         fcomp(index);
 2790       }
 2791     } else {
 2792       fcom(index);
 2793     }
 2794     // convert FPU condition into eflags condition via rax,
 2795     save_rax(tmp);
 2796     fwait(); fnstsw_ax();
 2797     sahf();
 2798     restore_rax(tmp);
 2799   }
 2800   // condition codes set as follows:
 2801   //
 2802   // CF (corresponds to C0) if x &lt; y
 2803   // PF (corresponds to C2) if unordered
 2804   // ZF (corresponds to C3) if x = y
 2805 }
 2806 
 2807 void MacroAssembler::fcmp2int(Register dst, bool unordered_is_less) {
 2808   fcmp2int(dst, unordered_is_less, 1, true, true);
 2809 }
 2810 
 2811 void MacroAssembler::fcmp2int(Register dst, bool unordered_is_less, int index, bool pop_left, bool pop_right) {
 2812   fcmp(VM_Version::supports_cmov() ? noreg : dst, index, pop_left, pop_right);
 2813   Label L;
 2814   if (unordered_is_less) {
 2815     movl(dst, -1);
 2816     jcc(Assembler::parity, L);
 2817     jcc(Assembler::below , L);
 2818     movl(dst, 0);
 2819     jcc(Assembler::equal , L);
 2820     increment(dst);
 2821   } else { // unordered is greater
 2822     movl(dst, 1);
 2823     jcc(Assembler::parity, L);
 2824     jcc(Assembler::above , L);
 2825     movl(dst, 0);
 2826     jcc(Assembler::equal , L);
 2827     decrementl(dst);
 2828   }
 2829   bind(L);
 2830 }
 2831 
 2832 void MacroAssembler::fld_d(AddressLiteral src) {
 2833   fld_d(as_Address(src));
 2834 }
 2835 
 2836 void MacroAssembler::fld_s(AddressLiteral src) {
 2837   fld_s(as_Address(src));
 2838 }
 2839 
 2840 void MacroAssembler::fld_x(AddressLiteral src) {
 2841   Assembler::fld_x(as_Address(src));
 2842 }
 2843 
 2844 void MacroAssembler::fldcw(AddressLiteral src) {
 2845   Assembler::fldcw(as_Address(src));
 2846 }
 2847 
 2848 void MacroAssembler::fpop() {
 2849   ffree();
 2850   fincstp();
 2851 }
 2852 
 2853 void MacroAssembler::fremr(Register tmp) {
 2854   save_rax(tmp);
 2855   { Label L;
 2856     bind(L);
 2857     fprem();
 2858     fwait(); fnstsw_ax();
 2859     sahf();
 2860     jcc(Assembler::parity, L);
 2861   }
 2862   restore_rax(tmp);
 2863   // Result is in ST0.
 2864   // Note: fxch &amp; fpop to get rid of ST1
 2865   // (otherwise FPU stack could overflow eventually)
 2866   fxch(1);
 2867   fpop();
 2868 }
 2869 #endif // !LP64
 2870 
 2871 void MacroAssembler::mulpd(XMMRegister dst, AddressLiteral src) {
 2872   if (reachable(src)) {
 2873     Assembler::mulpd(dst, as_Address(src));
 2874   } else {
 2875     lea(rscratch1, src);
 2876     Assembler::mulpd(dst, Address(rscratch1, 0));
 2877   }
 2878 }
 2879 
 2880 void MacroAssembler::load_float(Address src) {
 2881   if (UseSSE &gt;= 1) {
 2882     movflt(xmm0, src);
 2883   } else {
 2884     LP64_ONLY(ShouldNotReachHere());
 2885     NOT_LP64(fld_s(src));
 2886   }
 2887 }
 2888 
 2889 void MacroAssembler::store_float(Address dst) {
 2890   if (UseSSE &gt;= 1) {
 2891     movflt(dst, xmm0);
 2892   } else {
 2893     LP64_ONLY(ShouldNotReachHere());
 2894     NOT_LP64(fstp_s(dst));
 2895   }
 2896 }
 2897 
 2898 void MacroAssembler::load_double(Address src) {
 2899   if (UseSSE &gt;= 2) {
 2900     movdbl(xmm0, src);
 2901   } else {
 2902     LP64_ONLY(ShouldNotReachHere());
 2903     NOT_LP64(fld_d(src));
 2904   }
 2905 }
 2906 
 2907 void MacroAssembler::store_double(Address dst) {
 2908   if (UseSSE &gt;= 2) {
 2909     movdbl(dst, xmm0);
 2910   } else {
 2911     LP64_ONLY(ShouldNotReachHere());
 2912     NOT_LP64(fstp_d(dst));
 2913   }
 2914 }
 2915 
 2916 // dst = c = a * b + c
 2917 void MacroAssembler::fmad(XMMRegister dst, XMMRegister a, XMMRegister b, XMMRegister c) {
 2918   Assembler::vfmadd231sd(c, a, b);
 2919   if (dst != c) {
 2920     movdbl(dst, c);
 2921   }
 2922 }
 2923 
 2924 // dst = c = a * b + c
 2925 void MacroAssembler::fmaf(XMMRegister dst, XMMRegister a, XMMRegister b, XMMRegister c) {
 2926   Assembler::vfmadd231ss(c, a, b);
 2927   if (dst != c) {
 2928     movflt(dst, c);
 2929   }
 2930 }
 2931 
 2932 // dst = c = a * b + c
 2933 void MacroAssembler::vfmad(XMMRegister dst, XMMRegister a, XMMRegister b, XMMRegister c, int vector_len) {
 2934   Assembler::vfmadd231pd(c, a, b, vector_len);
 2935   if (dst != c) {
 2936     vmovdqu(dst, c);
 2937   }
 2938 }
 2939 
 2940 // dst = c = a * b + c
 2941 void MacroAssembler::vfmaf(XMMRegister dst, XMMRegister a, XMMRegister b, XMMRegister c, int vector_len) {
 2942   Assembler::vfmadd231ps(c, a, b, vector_len);
 2943   if (dst != c) {
 2944     vmovdqu(dst, c);
 2945   }
 2946 }
 2947 
 2948 // dst = c = a * b + c
 2949 void MacroAssembler::vfmad(XMMRegister dst, XMMRegister a, Address b, XMMRegister c, int vector_len) {
 2950   Assembler::vfmadd231pd(c, a, b, vector_len);
 2951   if (dst != c) {
 2952     vmovdqu(dst, c);
 2953   }
 2954 }
 2955 
 2956 // dst = c = a * b + c
 2957 void MacroAssembler::vfmaf(XMMRegister dst, XMMRegister a, Address b, XMMRegister c, int vector_len) {
 2958   Assembler::vfmadd231ps(c, a, b, vector_len);
 2959   if (dst != c) {
 2960     vmovdqu(dst, c);
 2961   }
 2962 }
 2963 
 2964 void MacroAssembler::incrementl(AddressLiteral dst) {
 2965   if (reachable(dst)) {
 2966     incrementl(as_Address(dst));
 2967   } else {
 2968     lea(rscratch1, dst);
 2969     incrementl(Address(rscratch1, 0));
 2970   }
 2971 }
 2972 
 2973 void MacroAssembler::incrementl(ArrayAddress dst) {
 2974   incrementl(as_Address(dst));
 2975 }
 2976 
 2977 void MacroAssembler::incrementl(Register reg, int value) {
 2978   if (value == min_jint) {addl(reg, value) ; return; }
 2979   if (value &lt;  0) { decrementl(reg, -value); return; }
 2980   if (value == 0) {                        ; return; }
 2981   if (value == 1 &amp;&amp; UseIncDec) { incl(reg) ; return; }
 2982   /* else */      { addl(reg, value)       ; return; }
 2983 }
 2984 
 2985 void MacroAssembler::incrementl(Address dst, int value) {
 2986   if (value == min_jint) {addl(dst, value) ; return; }
 2987   if (value &lt;  0) { decrementl(dst, -value); return; }
 2988   if (value == 0) {                        ; return; }
 2989   if (value == 1 &amp;&amp; UseIncDec) { incl(dst) ; return; }
 2990   /* else */      { addl(dst, value)       ; return; }
 2991 }
 2992 
 2993 void MacroAssembler::jump(AddressLiteral dst) {
 2994   if (reachable(dst)) {
 2995     jmp_literal(dst.target(), dst.rspec());
 2996   } else {
 2997     lea(rscratch1, dst);
 2998     jmp(rscratch1);
 2999   }
 3000 }
 3001 
 3002 void MacroAssembler::jump_cc(Condition cc, AddressLiteral dst) {
 3003   if (reachable(dst)) {
 3004     InstructionMark im(this);
 3005     relocate(dst.reloc());
 3006     const int short_size = 2;
 3007     const int long_size = 6;
 3008     int offs = (intptr_t)dst.target() - ((intptr_t)pc());
 3009     if (dst.reloc() == relocInfo::none &amp;&amp; is8bit(offs - short_size)) {
 3010       // 0111 tttn #8-bit disp
 3011       emit_int8(0x70 | cc);
 3012       emit_int8((offs - short_size) &amp; 0xFF);
 3013     } else {
 3014       // 0000 1111 1000 tttn #32-bit disp
 3015       emit_int8(0x0F);
 3016       emit_int8((unsigned char)(0x80 | cc));
 3017       emit_int32(offs - long_size);
 3018     }
 3019   } else {
 3020 #ifdef ASSERT
 3021     warning(&quot;reversing conditional branch&quot;);
 3022 #endif /* ASSERT */
 3023     Label skip;
 3024     jccb(reverse[cc], skip);
 3025     lea(rscratch1, dst);
 3026     Assembler::jmp(rscratch1);
 3027     bind(skip);
 3028   }
 3029 }
 3030 
 3031 void MacroAssembler::ldmxcsr(AddressLiteral src) {
 3032   if (reachable(src)) {
 3033     Assembler::ldmxcsr(as_Address(src));
 3034   } else {
 3035     lea(rscratch1, src);
 3036     Assembler::ldmxcsr(Address(rscratch1, 0));
 3037   }
 3038 }
 3039 
 3040 int MacroAssembler::load_signed_byte(Register dst, Address src) {
 3041   int off;
 3042   if (LP64_ONLY(true ||) VM_Version::is_P6()) {
 3043     off = offset();
 3044     movsbl(dst, src); // movsxb
 3045   } else {
 3046     off = load_unsigned_byte(dst, src);
 3047     shll(dst, 24);
 3048     sarl(dst, 24);
 3049   }
 3050   return off;
 3051 }
 3052 
 3053 // Note: load_signed_short used to be called load_signed_word.
 3054 // Although the &#39;w&#39; in x86 opcodes refers to the term &quot;word&quot; in the assembler
 3055 // manual, which means 16 bits, that usage is found nowhere in HotSpot code.
 3056 // The term &quot;word&quot; in HotSpot means a 32- or 64-bit machine word.
 3057 int MacroAssembler::load_signed_short(Register dst, Address src) {
 3058   int off;
 3059   if (LP64_ONLY(true ||) VM_Version::is_P6()) {
 3060     // This is dubious to me since it seems safe to do a signed 16 =&gt; 64 bit
 3061     // version but this is what 64bit has always done. This seems to imply
 3062     // that users are only using 32bits worth.
 3063     off = offset();
 3064     movswl(dst, src); // movsxw
 3065   } else {
 3066     off = load_unsigned_short(dst, src);
 3067     shll(dst, 16);
 3068     sarl(dst, 16);
 3069   }
 3070   return off;
 3071 }
 3072 
 3073 int MacroAssembler::load_unsigned_byte(Register dst, Address src) {
 3074   // According to Intel Doc. AP-526, &quot;Zero-Extension of Short&quot;, p.16,
 3075   // and &quot;3.9 Partial Register Penalties&quot;, p. 22).
 3076   int off;
 3077   if (LP64_ONLY(true || ) VM_Version::is_P6() || src.uses(dst)) {
 3078     off = offset();
 3079     movzbl(dst, src); // movzxb
 3080   } else {
 3081     xorl(dst, dst);
 3082     off = offset();
 3083     movb(dst, src);
 3084   }
 3085   return off;
 3086 }
 3087 
 3088 // Note: load_unsigned_short used to be called load_unsigned_word.
 3089 int MacroAssembler::load_unsigned_short(Register dst, Address src) {
 3090   // According to Intel Doc. AP-526, &quot;Zero-Extension of Short&quot;, p.16,
 3091   // and &quot;3.9 Partial Register Penalties&quot;, p. 22).
 3092   int off;
 3093   if (LP64_ONLY(true ||) VM_Version::is_P6() || src.uses(dst)) {
 3094     off = offset();
 3095     movzwl(dst, src); // movzxw
 3096   } else {
 3097     xorl(dst, dst);
 3098     off = offset();
 3099     movw(dst, src);
 3100   }
 3101   return off;
 3102 }
 3103 
 3104 void MacroAssembler::load_sized_value(Register dst, Address src, size_t size_in_bytes, bool is_signed, Register dst2) {
 3105   switch (size_in_bytes) {
 3106 #ifndef _LP64
 3107   case  8:
 3108     assert(dst2 != noreg, &quot;second dest register required&quot;);
 3109     movl(dst,  src);
 3110     movl(dst2, src.plus_disp(BytesPerInt));
 3111     break;
 3112 #else
 3113   case  8:  movq(dst, src); break;
 3114 #endif
 3115   case  4:  movl(dst, src); break;
 3116   case  2:  is_signed ? load_signed_short(dst, src) : load_unsigned_short(dst, src); break;
 3117   case  1:  is_signed ? load_signed_byte( dst, src) : load_unsigned_byte( dst, src); break;
 3118   default:  ShouldNotReachHere();
 3119   }
 3120 }
 3121 
 3122 void MacroAssembler::store_sized_value(Address dst, Register src, size_t size_in_bytes, Register src2) {
 3123   switch (size_in_bytes) {
 3124 #ifndef _LP64
 3125   case  8:
 3126     assert(src2 != noreg, &quot;second source register required&quot;);
 3127     movl(dst,                        src);
 3128     movl(dst.plus_disp(BytesPerInt), src2);
 3129     break;
 3130 #else
 3131   case  8:  movq(dst, src); break;
 3132 #endif
 3133   case  4:  movl(dst, src); break;
 3134   case  2:  movw(dst, src); break;
 3135   case  1:  movb(dst, src); break;
 3136   default:  ShouldNotReachHere();
 3137   }
 3138 }
 3139 
 3140 void MacroAssembler::mov32(AddressLiteral dst, Register src) {
 3141   if (reachable(dst)) {
 3142     movl(as_Address(dst), src);
 3143   } else {
 3144     lea(rscratch1, dst);
 3145     movl(Address(rscratch1, 0), src);
 3146   }
 3147 }
 3148 
 3149 void MacroAssembler::mov32(Register dst, AddressLiteral src) {
 3150   if (reachable(src)) {
 3151     movl(dst, as_Address(src));
 3152   } else {
 3153     lea(rscratch1, src);
 3154     movl(dst, Address(rscratch1, 0));
 3155   }
 3156 }
 3157 
 3158 // C++ bool manipulation
 3159 
 3160 void MacroAssembler::movbool(Register dst, Address src) {
 3161   if(sizeof(bool) == 1)
 3162     movb(dst, src);
 3163   else if(sizeof(bool) == 2)
 3164     movw(dst, src);
 3165   else if(sizeof(bool) == 4)
 3166     movl(dst, src);
 3167   else
 3168     // unsupported
 3169     ShouldNotReachHere();
 3170 }
 3171 
 3172 void MacroAssembler::movbool(Address dst, bool boolconst) {
 3173   if(sizeof(bool) == 1)
 3174     movb(dst, (int) boolconst);
 3175   else if(sizeof(bool) == 2)
 3176     movw(dst, (int) boolconst);
 3177   else if(sizeof(bool) == 4)
 3178     movl(dst, (int) boolconst);
 3179   else
 3180     // unsupported
 3181     ShouldNotReachHere();
 3182 }
 3183 
 3184 void MacroAssembler::movbool(Address dst, Register src) {
 3185   if(sizeof(bool) == 1)
 3186     movb(dst, src);
 3187   else if(sizeof(bool) == 2)
 3188     movw(dst, src);
 3189   else if(sizeof(bool) == 4)
 3190     movl(dst, src);
 3191   else
 3192     // unsupported
 3193     ShouldNotReachHere();
 3194 }
 3195 
 3196 void MacroAssembler::movbyte(ArrayAddress dst, int src) {
 3197   movb(as_Address(dst), src);
 3198 }
 3199 
 3200 void MacroAssembler::movdl(XMMRegister dst, AddressLiteral src) {
 3201   if (reachable(src)) {
 3202     movdl(dst, as_Address(src));
 3203   } else {
 3204     lea(rscratch1, src);
 3205     movdl(dst, Address(rscratch1, 0));
 3206   }
 3207 }
 3208 
 3209 void MacroAssembler::movq(XMMRegister dst, AddressLiteral src) {
 3210   if (reachable(src)) {
 3211     movq(dst, as_Address(src));
 3212   } else {
 3213     lea(rscratch1, src);
 3214     movq(dst, Address(rscratch1, 0));
 3215   }
 3216 }
 3217 
 3218 #ifdef COMPILER2
 3219 void MacroAssembler::setvectmask(Register dst, Register src) {
 3220   guarantee(PostLoopMultiversioning, &quot;must be&quot;);
 3221   Assembler::movl(dst, 1);
 3222   Assembler::shlxl(dst, dst, src);
 3223   Assembler::decl(dst);
 3224   Assembler::kmovdl(k1, dst);
 3225   Assembler::movl(dst, src);
 3226 }
 3227 
 3228 void MacroAssembler::restorevectmask() {
 3229   guarantee(PostLoopMultiversioning, &quot;must be&quot;);
 3230   Assembler::knotwl(k1, k0);
 3231 }
 3232 #endif // COMPILER2
 3233 
 3234 void MacroAssembler::movdbl(XMMRegister dst, AddressLiteral src) {
 3235   if (reachable(src)) {
 3236     if (UseXmmLoadAndClearUpper) {
 3237       movsd (dst, as_Address(src));
 3238     } else {
 3239       movlpd(dst, as_Address(src));
 3240     }
 3241   } else {
 3242     lea(rscratch1, src);
 3243     if (UseXmmLoadAndClearUpper) {
 3244       movsd (dst, Address(rscratch1, 0));
 3245     } else {
 3246       movlpd(dst, Address(rscratch1, 0));
 3247     }
 3248   }
 3249 }
 3250 
 3251 void MacroAssembler::movflt(XMMRegister dst, AddressLiteral src) {
 3252   if (reachable(src)) {
 3253     movss(dst, as_Address(src));
 3254   } else {
 3255     lea(rscratch1, src);
 3256     movss(dst, Address(rscratch1, 0));
 3257   }
 3258 }
 3259 
 3260 void MacroAssembler::movptr(Register dst, Register src) {
 3261   LP64_ONLY(movq(dst, src)) NOT_LP64(movl(dst, src));
 3262 }
 3263 
 3264 void MacroAssembler::movptr(Register dst, Address src) {
 3265   LP64_ONLY(movq(dst, src)) NOT_LP64(movl(dst, src));
 3266 }
 3267 
 3268 // src should NEVER be a real pointer. Use AddressLiteral for true pointers
 3269 void MacroAssembler::movptr(Register dst, intptr_t src) {
 3270   LP64_ONLY(mov64(dst, src)) NOT_LP64(movl(dst, src));
 3271 }
 3272 
 3273 void MacroAssembler::movptr(Address dst, Register src) {
 3274   LP64_ONLY(movq(dst, src)) NOT_LP64(movl(dst, src));
 3275 }
 3276 
 3277 void MacroAssembler::movdqu(Address dst, XMMRegister src) {
 3278     assert(((src-&gt;encoding() &lt; 16) || VM_Version::supports_avx512vl()),&quot;XMM register should be 0-15&quot;);
 3279     Assembler::movdqu(dst, src);
 3280 }
 3281 
 3282 void MacroAssembler::movdqu(XMMRegister dst, Address src) {
 3283     assert(((dst-&gt;encoding() &lt; 16) || VM_Version::supports_avx512vl()),&quot;XMM register should be 0-15&quot;);
 3284     Assembler::movdqu(dst, src);
 3285 }
 3286 
 3287 void MacroAssembler::movdqu(XMMRegister dst, XMMRegister src) {
 3288     assert(((dst-&gt;encoding() &lt; 16  &amp;&amp; src-&gt;encoding() &lt; 16) || VM_Version::supports_avx512vl()),&quot;XMM register should be 0-15&quot;);
 3289     Assembler::movdqu(dst, src);
 3290 }
 3291 
 3292 void MacroAssembler::movdqu(XMMRegister dst, AddressLiteral src, Register scratchReg) {
 3293   if (reachable(src)) {
 3294     movdqu(dst, as_Address(src));
 3295   } else {
 3296     lea(scratchReg, src);
 3297     movdqu(dst, Address(scratchReg, 0));
 3298   }
 3299 }
 3300 
 3301 void MacroAssembler::vmovdqu(Address dst, XMMRegister src) {
 3302     assert(((src-&gt;encoding() &lt; 16) || VM_Version::supports_avx512vl()),&quot;XMM register should be 0-15&quot;);
 3303     Assembler::vmovdqu(dst, src);
 3304 }
 3305 
 3306 void MacroAssembler::vmovdqu(XMMRegister dst, Address src) {
 3307     assert(((dst-&gt;encoding() &lt; 16) || VM_Version::supports_avx512vl()),&quot;XMM register should be 0-15&quot;);
 3308     Assembler::vmovdqu(dst, src);
 3309 }
 3310 
 3311 void MacroAssembler::vmovdqu(XMMRegister dst, XMMRegister src) {
 3312     assert(((dst-&gt;encoding() &lt; 16  &amp;&amp; src-&gt;encoding() &lt; 16) || VM_Version::supports_avx512vl()),&quot;XMM register should be 0-15&quot;);
 3313     Assembler::vmovdqu(dst, src);
 3314 }
 3315 
 3316 void MacroAssembler::vmovdqu(XMMRegister dst, AddressLiteral src, Register scratch_reg) {
 3317   if (reachable(src)) {
 3318     vmovdqu(dst, as_Address(src));
 3319   }
 3320   else {
 3321     lea(scratch_reg, src);
 3322     vmovdqu(dst, Address(scratch_reg, 0));
 3323   }
 3324 }
 3325 
 3326 void MacroAssembler::evmovdquq(XMMRegister dst, AddressLiteral src, int vector_len, Register rscratch) {
 3327   if (reachable(src)) {
 3328     Assembler::evmovdquq(dst, as_Address(src), vector_len);
 3329   } else {
 3330     lea(rscratch, src);
 3331     Assembler::evmovdquq(dst, Address(rscratch, 0), vector_len);
 3332   }
 3333 }
 3334 
 3335 void MacroAssembler::movdqa(XMMRegister dst, AddressLiteral src) {
 3336   if (reachable(src)) {
 3337     Assembler::movdqa(dst, as_Address(src));
 3338   } else {
 3339     lea(rscratch1, src);
 3340     Assembler::movdqa(dst, Address(rscratch1, 0));
 3341   }
 3342 }
 3343 
 3344 void MacroAssembler::movsd(XMMRegister dst, AddressLiteral src) {
 3345   if (reachable(src)) {
 3346     Assembler::movsd(dst, as_Address(src));
 3347   } else {
 3348     lea(rscratch1, src);
 3349     Assembler::movsd(dst, Address(rscratch1, 0));
 3350   }
 3351 }
 3352 
 3353 void MacroAssembler::movss(XMMRegister dst, AddressLiteral src) {
 3354   if (reachable(src)) {
 3355     Assembler::movss(dst, as_Address(src));
 3356   } else {
 3357     lea(rscratch1, src);
 3358     Assembler::movss(dst, Address(rscratch1, 0));
 3359   }
 3360 }
 3361 
 3362 void MacroAssembler::mulsd(XMMRegister dst, AddressLiteral src) {
 3363   if (reachable(src)) {
 3364     Assembler::mulsd(dst, as_Address(src));
 3365   } else {
 3366     lea(rscratch1, src);
 3367     Assembler::mulsd(dst, Address(rscratch1, 0));
 3368   }
 3369 }
 3370 
 3371 void MacroAssembler::mulss(XMMRegister dst, AddressLiteral src) {
 3372   if (reachable(src)) {
 3373     Assembler::mulss(dst, as_Address(src));
 3374   } else {
 3375     lea(rscratch1, src);
 3376     Assembler::mulss(dst, Address(rscratch1, 0));
 3377   }
 3378 }
 3379 
 3380 void MacroAssembler::null_check(Register reg, int offset) {
 3381   if (needs_explicit_null_check(offset)) {
 3382     // provoke OS NULL exception if reg = NULL by
 3383     // accessing M[reg] w/o changing any (non-CC) registers
 3384     // NOTE: cmpl is plenty here to provoke a segv
 3385     cmpptr(rax, Address(reg, 0));
 3386     // Note: should probably use testl(rax, Address(reg, 0));
 3387     //       may be shorter code (however, this version of
 3388     //       testl needs to be implemented first)
 3389   } else {
 3390     // nothing to do, (later) access of M[reg + offset]
 3391     // will provoke OS NULL exception if reg = NULL
 3392   }
 3393 }
 3394 
 3395 void MacroAssembler::test_klass_is_value(Register klass, Register temp_reg, Label&amp; is_value) {
 3396   movl(temp_reg, Address(klass, Klass::access_flags_offset()));
 3397   testl(temp_reg, JVM_ACC_VALUE);
 3398   jcc(Assembler::notZero, is_value);
 3399 }
 3400 
 3401 void MacroAssembler::test_klass_is_empty_value(Register klass, Register temp_reg, Label&amp; is_empty_value) {
 3402 #ifdef ASSERT
 3403   {
 3404     Label done_check;
 3405     test_klass_is_value(klass, temp_reg, done_check);
 3406     stop(&quot;test_klass_is_empty_value with none value klass&quot;);
 3407     bind(done_check);
 3408   }
 3409 #endif
 3410   movl(temp_reg, Address(klass, InstanceKlass::misc_flags_offset()));
 3411   testl(temp_reg, InstanceKlass::misc_flags_is_empty_value());
 3412   jcc(Assembler::notZero, is_empty_value);
 3413 }
 3414 
 3415 void MacroAssembler::test_field_is_flattenable(Register flags, Register temp_reg, Label&amp; is_flattenable) {
 3416   movl(temp_reg, flags);
 3417   shrl(temp_reg, ConstantPoolCacheEntry::is_flattenable_field_shift);
 3418   andl(temp_reg, 0x1);
 3419   testl(temp_reg, temp_reg);
 3420   jcc(Assembler::notZero, is_flattenable);
 3421 }
 3422 
 3423 void MacroAssembler::test_field_is_not_flattenable(Register flags, Register temp_reg, Label&amp; notFlattenable) {
 3424   movl(temp_reg, flags);
 3425   shrl(temp_reg, ConstantPoolCacheEntry::is_flattenable_field_shift);
 3426   andl(temp_reg, 0x1);
 3427   testl(temp_reg, temp_reg);
 3428   jcc(Assembler::zero, notFlattenable);
 3429 }
 3430 
 3431 void MacroAssembler::test_field_is_flattened(Register flags, Register temp_reg, Label&amp; is_flattened) {
 3432   movl(temp_reg, flags);
 3433   shrl(temp_reg, ConstantPoolCacheEntry::is_flattened_field_shift);
 3434   andl(temp_reg, 0x1);
 3435   testl(temp_reg, temp_reg);
 3436   jcc(Assembler::notZero, is_flattened);
 3437 }
 3438 
 3439 void MacroAssembler::test_flattened_array_oop(Register oop, Register temp_reg,
 3440                                               Label&amp;is_flattened_array) {
 3441   load_storage_props(temp_reg, oop);
 3442   testb(temp_reg, ArrayStorageProperties::flattened_value);
 3443   jcc(Assembler::notZero, is_flattened_array);
 3444 }
 3445 
 3446 void MacroAssembler::test_non_flattened_array_oop(Register oop, Register temp_reg,
 3447                                                   Label&amp;is_non_flattened_array) {
 3448   load_storage_props(temp_reg, oop);
 3449   testb(temp_reg, ArrayStorageProperties::flattened_value);
 3450   jcc(Assembler::zero, is_non_flattened_array);
 3451 }
 3452 
 3453 void MacroAssembler::test_null_free_array_oop(Register oop, Register temp_reg, Label&amp;is_null_free_array) {
 3454   load_storage_props(temp_reg, oop);
 3455   testb(temp_reg, ArrayStorageProperties::null_free_value);
 3456   jcc(Assembler::notZero, is_null_free_array);
 3457 }
 3458 
 3459 void MacroAssembler::test_non_null_free_array_oop(Register oop, Register temp_reg, Label&amp;is_non_null_free_array) {
 3460   load_storage_props(temp_reg, oop);
 3461   testb(temp_reg, ArrayStorageProperties::null_free_value);
 3462   jcc(Assembler::zero, is_non_null_free_array);
 3463 }
 3464 
 3465 void MacroAssembler::os_breakpoint() {
 3466   // instead of directly emitting a breakpoint, call os:breakpoint for better debugability
 3467   // (e.g., MSVC can&#39;t call ps() otherwise)
 3468   call(RuntimeAddress(CAST_FROM_FN_PTR(address, os::breakpoint)));
 3469 }
 3470 
 3471 void MacroAssembler::unimplemented(const char* what) {
 3472   const char* buf = NULL;
 3473   {
 3474     ResourceMark rm;
 3475     stringStream ss;
 3476     ss.print(&quot;unimplemented: %s&quot;, what);
 3477     buf = code_string(ss.as_string());
 3478   }
 3479   stop(buf);
 3480 }
 3481 
 3482 #ifdef _LP64
 3483 #define XSTATE_BV 0x200
 3484 #endif
 3485 
 3486 void MacroAssembler::pop_CPU_state() {
 3487   pop_FPU_state();
 3488   pop_IU_state();
 3489 }
 3490 
 3491 void MacroAssembler::pop_FPU_state() {
 3492 #ifndef _LP64
 3493   frstor(Address(rsp, 0));
 3494 #else
 3495   fxrstor(Address(rsp, 0));
 3496 #endif
 3497   addptr(rsp, FPUStateSizeInWords * wordSize);
 3498 }
 3499 
 3500 void MacroAssembler::pop_IU_state() {
 3501   popa();
 3502   LP64_ONLY(addq(rsp, 8));
 3503   popf();
 3504 }
 3505 
 3506 // Save Integer and Float state
 3507 // Warning: Stack must be 16 byte aligned (64bit)
 3508 void MacroAssembler::push_CPU_state() {
 3509   push_IU_state();
 3510   push_FPU_state();
 3511 }
 3512 
 3513 void MacroAssembler::push_FPU_state() {
 3514   subptr(rsp, FPUStateSizeInWords * wordSize);
 3515 #ifndef _LP64
 3516   fnsave(Address(rsp, 0));
 3517   fwait();
 3518 #else
 3519   fxsave(Address(rsp, 0));
 3520 #endif // LP64
 3521 }
 3522 
 3523 void MacroAssembler::push_IU_state() {
 3524   // Push flags first because pusha kills them
 3525   pushf();
 3526   // Make sure rsp stays 16-byte aligned
 3527   LP64_ONLY(subq(rsp, 8));
 3528   pusha();
 3529 }
 3530 
 3531 void MacroAssembler::reset_last_Java_frame(Register java_thread, bool clear_fp) { // determine java_thread register
 3532   if (!java_thread-&gt;is_valid()) {
 3533     java_thread = rdi;
 3534     get_thread(java_thread);
 3535   }
 3536   // we must set sp to zero to clear frame
 3537   movptr(Address(java_thread, JavaThread::last_Java_sp_offset()), NULL_WORD);
 3538   if (clear_fp) {
 3539     movptr(Address(java_thread, JavaThread::last_Java_fp_offset()), NULL_WORD);
 3540   }
 3541 
 3542   // Always clear the pc because it could have been set by make_walkable()
 3543   movptr(Address(java_thread, JavaThread::last_Java_pc_offset()), NULL_WORD);
 3544 
 3545   vzeroupper();
 3546 }
 3547 
 3548 void MacroAssembler::restore_rax(Register tmp) {
 3549   if (tmp == noreg) pop(rax);
 3550   else if (tmp != rax) mov(rax, tmp);
 3551 }
 3552 
 3553 void MacroAssembler::round_to(Register reg, int modulus) {
 3554   addptr(reg, modulus - 1);
 3555   andptr(reg, -modulus);
 3556 }
 3557 
 3558 void MacroAssembler::save_rax(Register tmp) {
 3559   if (tmp == noreg) push(rax);
 3560   else if (tmp != rax) mov(tmp, rax);
 3561 }
 3562 
 3563 void MacroAssembler::safepoint_poll(Label&amp; slow_path, Register thread_reg, Register temp_reg) {
 3564   if (SafepointMechanism::uses_thread_local_poll()) {
 3565 #ifdef _LP64
 3566     assert(thread_reg == r15_thread, &quot;should be&quot;);
 3567 #else
 3568     if (thread_reg == noreg) {
 3569       thread_reg = temp_reg;
 3570       get_thread(thread_reg);
 3571     }
 3572 #endif
 3573     testb(Address(thread_reg, Thread::polling_page_offset()), SafepointMechanism::poll_bit());
 3574     jcc(Assembler::notZero, slow_path); // handshake bit set implies poll
 3575   } else {
 3576     cmp32(ExternalAddress(SafepointSynchronize::address_of_state()),
 3577         SafepointSynchronize::_not_synchronized);
 3578     jcc(Assembler::notEqual, slow_path);
 3579   }
 3580 }
 3581 
 3582 // Calls to C land
 3583 //
 3584 // When entering C land, the rbp, &amp; rsp of the last Java frame have to be recorded
 3585 // in the (thread-local) JavaThread object. When leaving C land, the last Java fp
 3586 // has to be reset to 0. This is required to allow proper stack traversal.
 3587 void MacroAssembler::set_last_Java_frame(Register java_thread,
 3588                                          Register last_java_sp,
 3589                                          Register last_java_fp,
 3590                                          address  last_java_pc) {
 3591   vzeroupper();
 3592   // determine java_thread register
 3593   if (!java_thread-&gt;is_valid()) {
 3594     java_thread = rdi;
 3595     get_thread(java_thread);
 3596   }
 3597   // determine last_java_sp register
 3598   if (!last_java_sp-&gt;is_valid()) {
 3599     last_java_sp = rsp;
 3600   }
 3601 
 3602   // last_java_fp is optional
 3603 
 3604   if (last_java_fp-&gt;is_valid()) {
 3605     movptr(Address(java_thread, JavaThread::last_Java_fp_offset()), last_java_fp);
 3606   }
 3607 
 3608   // last_java_pc is optional
 3609 
 3610   if (last_java_pc != NULL) {
 3611     lea(Address(java_thread,
 3612                  JavaThread::frame_anchor_offset() + JavaFrameAnchor::last_Java_pc_offset()),
 3613         InternalAddress(last_java_pc));
 3614 
 3615   }
 3616   movptr(Address(java_thread, JavaThread::last_Java_sp_offset()), last_java_sp);
 3617 }
 3618 
 3619 void MacroAssembler::shlptr(Register dst, int imm8) {
 3620   LP64_ONLY(shlq(dst, imm8)) NOT_LP64(shll(dst, imm8));
 3621 }
 3622 
 3623 void MacroAssembler::shrptr(Register dst, int imm8) {
 3624   LP64_ONLY(shrq(dst, imm8)) NOT_LP64(shrl(dst, imm8));
 3625 }
 3626 
 3627 void MacroAssembler::sign_extend_byte(Register reg) {
 3628   if (LP64_ONLY(true ||) (VM_Version::is_P6() &amp;&amp; reg-&gt;has_byte_register())) {
 3629     movsbl(reg, reg); // movsxb
 3630   } else {
 3631     shll(reg, 24);
 3632     sarl(reg, 24);
 3633   }
 3634 }
 3635 
 3636 void MacroAssembler::sign_extend_short(Register reg) {
 3637   if (LP64_ONLY(true ||) VM_Version::is_P6()) {
 3638     movswl(reg, reg); // movsxw
 3639   } else {
 3640     shll(reg, 16);
 3641     sarl(reg, 16);
 3642   }
 3643 }
 3644 
 3645 void MacroAssembler::testl(Register dst, AddressLiteral src) {
 3646   assert(reachable(src), &quot;Address should be reachable&quot;);
 3647   testl(dst, as_Address(src));
 3648 }
 3649 
 3650 void MacroAssembler::pcmpeqb(XMMRegister dst, XMMRegister src) {
 3651   assert(((dst-&gt;encoding() &lt; 16 &amp;&amp; src-&gt;encoding() &lt; 16) || VM_Version::supports_avx512vlbw()),&quot;XMM register should be 0-15&quot;);
 3652   Assembler::pcmpeqb(dst, src);
 3653 }
 3654 
 3655 void MacroAssembler::pcmpeqw(XMMRegister dst, XMMRegister src) {
 3656   assert(((dst-&gt;encoding() &lt; 16 &amp;&amp; src-&gt;encoding() &lt; 16) || VM_Version::supports_avx512vlbw()),&quot;XMM register should be 0-15&quot;);
 3657   Assembler::pcmpeqw(dst, src);
 3658 }
 3659 
 3660 void MacroAssembler::pcmpestri(XMMRegister dst, Address src, int imm8) {
 3661   assert((dst-&gt;encoding() &lt; 16),&quot;XMM register should be 0-15&quot;);
 3662   Assembler::pcmpestri(dst, src, imm8);
 3663 }
 3664 
 3665 void MacroAssembler::pcmpestri(XMMRegister dst, XMMRegister src, int imm8) {
 3666   assert((dst-&gt;encoding() &lt; 16 &amp;&amp; src-&gt;encoding() &lt; 16),&quot;XMM register should be 0-15&quot;);
 3667   Assembler::pcmpestri(dst, src, imm8);
 3668 }
 3669 
 3670 void MacroAssembler::pmovzxbw(XMMRegister dst, XMMRegister src) {
 3671   assert(((dst-&gt;encoding() &lt; 16 &amp;&amp; src-&gt;encoding() &lt; 16) || VM_Version::supports_avx512vlbw()),&quot;XMM register should be 0-15&quot;);
 3672   Assembler::pmovzxbw(dst, src);
 3673 }
 3674 
 3675 void MacroAssembler::pmovzxbw(XMMRegister dst, Address src) {
 3676   assert(((dst-&gt;encoding() &lt; 16) || VM_Version::supports_avx512vlbw()),&quot;XMM register should be 0-15&quot;);
 3677   Assembler::pmovzxbw(dst, src);
 3678 }
 3679 
 3680 void MacroAssembler::pmovmskb(Register dst, XMMRegister src) {
 3681   assert((src-&gt;encoding() &lt; 16),&quot;XMM register should be 0-15&quot;);
 3682   Assembler::pmovmskb(dst, src);
 3683 }
 3684 
 3685 void MacroAssembler::ptest(XMMRegister dst, XMMRegister src) {
 3686   assert((dst-&gt;encoding() &lt; 16 &amp;&amp; src-&gt;encoding() &lt; 16),&quot;XMM register should be 0-15&quot;);
 3687   Assembler::ptest(dst, src);
 3688 }
 3689 
 3690 void MacroAssembler::sqrtsd(XMMRegister dst, AddressLiteral src) {
 3691   if (reachable(src)) {
 3692     Assembler::sqrtsd(dst, as_Address(src));
 3693   } else {
 3694     lea(rscratch1, src);
 3695     Assembler::sqrtsd(dst, Address(rscratch1, 0));
 3696   }
 3697 }
 3698 
 3699 void MacroAssembler::sqrtss(XMMRegister dst, AddressLiteral src) {
 3700   if (reachable(src)) {
 3701     Assembler::sqrtss(dst, as_Address(src));
 3702   } else {
 3703     lea(rscratch1, src);
 3704     Assembler::sqrtss(dst, Address(rscratch1, 0));
 3705   }
 3706 }
 3707 
 3708 void MacroAssembler::subsd(XMMRegister dst, AddressLiteral src) {
 3709   if (reachable(src)) {
 3710     Assembler::subsd(dst, as_Address(src));
 3711   } else {
 3712     lea(rscratch1, src);
 3713     Assembler::subsd(dst, Address(rscratch1, 0));
 3714   }
 3715 }
 3716 
 3717 void MacroAssembler::roundsd(XMMRegister dst, AddressLiteral src, int32_t rmode, Register scratch_reg) {
 3718   if (reachable(src)) {
 3719     Assembler::roundsd(dst, as_Address(src), rmode);
 3720   } else {
 3721     lea(scratch_reg, src);
 3722     Assembler::roundsd(dst, Address(scratch_reg, 0), rmode);
 3723   }
 3724 }
 3725 
 3726 void MacroAssembler::subss(XMMRegister dst, AddressLiteral src) {
 3727   if (reachable(src)) {
 3728     Assembler::subss(dst, as_Address(src));
 3729   } else {
 3730     lea(rscratch1, src);
 3731     Assembler::subss(dst, Address(rscratch1, 0));
 3732   }
 3733 }
 3734 
 3735 void MacroAssembler::ucomisd(XMMRegister dst, AddressLiteral src) {
 3736   if (reachable(src)) {
 3737     Assembler::ucomisd(dst, as_Address(src));
 3738   } else {
 3739     lea(rscratch1, src);
 3740     Assembler::ucomisd(dst, Address(rscratch1, 0));
 3741   }
 3742 }
 3743 
 3744 void MacroAssembler::ucomiss(XMMRegister dst, AddressLiteral src) {
 3745   if (reachable(src)) {
 3746     Assembler::ucomiss(dst, as_Address(src));
 3747   } else {
 3748     lea(rscratch1, src);
 3749     Assembler::ucomiss(dst, Address(rscratch1, 0));
 3750   }
 3751 }
 3752 
 3753 void MacroAssembler::xorpd(XMMRegister dst, AddressLiteral src, Register scratch_reg) {
 3754   // Used in sign-bit flipping with aligned address.
 3755   assert((UseAVX &gt; 0) || (((intptr_t)src.target() &amp; 15) == 0), &quot;SSE mode requires address alignment 16 bytes&quot;);
 3756   if (reachable(src)) {
 3757     Assembler::xorpd(dst, as_Address(src));
 3758   } else {
 3759     lea(scratch_reg, src);
 3760     Assembler::xorpd(dst, Address(scratch_reg, 0));
 3761   }
 3762 }
 3763 
 3764 void MacroAssembler::xorpd(XMMRegister dst, XMMRegister src) {
 3765   if (UseAVX &gt; 2 &amp;&amp; !VM_Version::supports_avx512dq() &amp;&amp; (dst-&gt;encoding() == src-&gt;encoding())) {
 3766     Assembler::vpxor(dst, dst, src, Assembler::AVX_512bit);
 3767   }
 3768   else {
 3769     Assembler::xorpd(dst, src);
 3770   }
 3771 }
 3772 
 3773 void MacroAssembler::xorps(XMMRegister dst, XMMRegister src) {
 3774   if (UseAVX &gt; 2 &amp;&amp; !VM_Version::supports_avx512dq() &amp;&amp; (dst-&gt;encoding() == src-&gt;encoding())) {
 3775     Assembler::vpxor(dst, dst, src, Assembler::AVX_512bit);
 3776   } else {
 3777     Assembler::xorps(dst, src);
 3778   }
 3779 }
 3780 
 3781 void MacroAssembler::xorps(XMMRegister dst, AddressLiteral src, Register scratch_reg) {
 3782   // Used in sign-bit flipping with aligned address.
 3783   assert((UseAVX &gt; 0) || (((intptr_t)src.target() &amp; 15) == 0), &quot;SSE mode requires address alignment 16 bytes&quot;);
 3784   if (reachable(src)) {
 3785     Assembler::xorps(dst, as_Address(src));
 3786   } else {
 3787     lea(scratch_reg, src);
 3788     Assembler::xorps(dst, Address(scratch_reg, 0));
 3789   }
 3790 }
 3791 
 3792 void MacroAssembler::pshufb(XMMRegister dst, AddressLiteral src) {
 3793   // Used in sign-bit flipping with aligned address.
 3794   bool aligned_adr = (((intptr_t)src.target() &amp; 15) == 0);
 3795   assert((UseAVX &gt; 0) || aligned_adr, &quot;SSE mode requires address alignment 16 bytes&quot;);
 3796   if (reachable(src)) {
 3797     Assembler::pshufb(dst, as_Address(src));
 3798   } else {
 3799     lea(rscratch1, src);
 3800     Assembler::pshufb(dst, Address(rscratch1, 0));
 3801   }
 3802 }
 3803 
 3804 // AVX 3-operands instructions
 3805 
 3806 void MacroAssembler::vaddsd(XMMRegister dst, XMMRegister nds, AddressLiteral src) {
 3807   if (reachable(src)) {
 3808     vaddsd(dst, nds, as_Address(src));
 3809   } else {
 3810     lea(rscratch1, src);
 3811     vaddsd(dst, nds, Address(rscratch1, 0));
 3812   }
 3813 }
 3814 
 3815 void MacroAssembler::vaddss(XMMRegister dst, XMMRegister nds, AddressLiteral src) {
 3816   if (reachable(src)) {
 3817     vaddss(dst, nds, as_Address(src));
 3818   } else {
 3819     lea(rscratch1, src);
 3820     vaddss(dst, nds, Address(rscratch1, 0));
 3821   }
 3822 }
 3823 
 3824 void MacroAssembler::vpaddd(XMMRegister dst, XMMRegister nds, AddressLiteral src, int vector_len, Register rscratch) {
 3825   assert(UseAVX &gt; 0, &quot;requires some form of AVX&quot;);
 3826   if (reachable(src)) {
 3827     Assembler::vpaddd(dst, nds, as_Address(src), vector_len);
 3828   } else {
 3829     lea(rscratch, src);
 3830     Assembler::vpaddd(dst, nds, Address(rscratch, 0), vector_len);
 3831   }
 3832 }
 3833 
 3834 void MacroAssembler::vabsss(XMMRegister dst, XMMRegister nds, XMMRegister src, AddressLiteral negate_field, int vector_len) {
 3835   assert(((dst-&gt;encoding() &lt; 16 &amp;&amp; src-&gt;encoding() &lt; 16 &amp;&amp; nds-&gt;encoding() &lt; 16) || VM_Version::supports_avx512vldq()),&quot;XMM register should be 0-15&quot;);
 3836   vandps(dst, nds, negate_field, vector_len);
 3837 }
 3838 
 3839 void MacroAssembler::vabssd(XMMRegister dst, XMMRegister nds, XMMRegister src, AddressLiteral negate_field, int vector_len) {
 3840   assert(((dst-&gt;encoding() &lt; 16 &amp;&amp; src-&gt;encoding() &lt; 16 &amp;&amp; nds-&gt;encoding() &lt; 16) || VM_Version::supports_avx512vldq()),&quot;XMM register should be 0-15&quot;);
 3841   vandpd(dst, nds, negate_field, vector_len);
 3842 }
 3843 
 3844 void MacroAssembler::vpaddb(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {
 3845   assert(((dst-&gt;encoding() &lt; 16 &amp;&amp; src-&gt;encoding() &lt; 16 &amp;&amp; nds-&gt;encoding() &lt; 16) || VM_Version::supports_avx512vlbw()),&quot;XMM register should be 0-15&quot;);
 3846   Assembler::vpaddb(dst, nds, src, vector_len);
 3847 }
 3848 
 3849 void MacroAssembler::vpaddb(XMMRegister dst, XMMRegister nds, Address src, int vector_len) {
 3850   assert(((dst-&gt;encoding() &lt; 16 &amp;&amp; nds-&gt;encoding() &lt; 16) || VM_Version::supports_avx512vlbw()),&quot;XMM register should be 0-15&quot;);
 3851   Assembler::vpaddb(dst, nds, src, vector_len);
 3852 }
 3853 
 3854 void MacroAssembler::vpaddw(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {
 3855   assert(((dst-&gt;encoding() &lt; 16 &amp;&amp; src-&gt;encoding() &lt; 16 &amp;&amp; nds-&gt;encoding() &lt; 16) || VM_Version::supports_avx512vlbw()),&quot;XMM register should be 0-15&quot;);
 3856   Assembler::vpaddw(dst, nds, src, vector_len);
 3857 }
 3858 
 3859 void MacroAssembler::vpaddw(XMMRegister dst, XMMRegister nds, Address src, int vector_len) {
 3860   assert(((dst-&gt;encoding() &lt; 16 &amp;&amp; nds-&gt;encoding() &lt; 16) || VM_Version::supports_avx512vlbw()),&quot;XMM register should be 0-15&quot;);
 3861   Assembler::vpaddw(dst, nds, src, vector_len);
 3862 }
 3863 
 3864 void MacroAssembler::vpand(XMMRegister dst, XMMRegister nds, AddressLiteral src, int vector_len, Register scratch_reg) {
 3865   if (reachable(src)) {
 3866     Assembler::vpand(dst, nds, as_Address(src), vector_len);
 3867   } else {
 3868     lea(scratch_reg, src);
 3869     Assembler::vpand(dst, nds, Address(scratch_reg, 0), vector_len);
 3870   }
 3871 }
 3872 
 3873 void MacroAssembler::vpbroadcastw(XMMRegister dst, XMMRegister src, int vector_len) {
 3874   assert(((dst-&gt;encoding() &lt; 16 &amp;&amp; src-&gt;encoding() &lt; 16) || VM_Version::supports_avx512vlbw()),&quot;XMM register should be 0-15&quot;);
 3875   Assembler::vpbroadcastw(dst, src, vector_len);
 3876 }
 3877 
 3878 void MacroAssembler::vpcmpeqb(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {
 3879   assert(((dst-&gt;encoding() &lt; 16 &amp;&amp; src-&gt;encoding() &lt; 16 &amp;&amp; nds-&gt;encoding() &lt; 16) || VM_Version::supports_avx512vlbw()),&quot;XMM register should be 0-15&quot;);
 3880   Assembler::vpcmpeqb(dst, nds, src, vector_len);
 3881 }
 3882 
 3883 void MacroAssembler::vpcmpeqw(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {
 3884   assert(((dst-&gt;encoding() &lt; 16 &amp;&amp; src-&gt;encoding() &lt; 16 &amp;&amp; nds-&gt;encoding() &lt; 16) || VM_Version::supports_avx512vlbw()),&quot;XMM register should be 0-15&quot;);
 3885   Assembler::vpcmpeqw(dst, nds, src, vector_len);
 3886 }
 3887 
 3888 void MacroAssembler::vpmovzxbw(XMMRegister dst, Address src, int vector_len) {
 3889   assert(((dst-&gt;encoding() &lt; 16) || VM_Version::supports_avx512vlbw()),&quot;XMM register should be 0-15&quot;);
 3890   Assembler::vpmovzxbw(dst, src, vector_len);
 3891 }
 3892 
 3893 void MacroAssembler::vpmovmskb(Register dst, XMMRegister src) {
 3894   assert((src-&gt;encoding() &lt; 16),&quot;XMM register should be 0-15&quot;);
 3895   Assembler::vpmovmskb(dst, src);
 3896 }
 3897 
 3898 void MacroAssembler::vpmullw(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {
 3899   assert(((dst-&gt;encoding() &lt; 16 &amp;&amp; src-&gt;encoding() &lt; 16 &amp;&amp; nds-&gt;encoding() &lt; 16) || VM_Version::supports_avx512vlbw()),&quot;XMM register should be 0-15&quot;);
 3900   Assembler::vpmullw(dst, nds, src, vector_len);
 3901 }
 3902 
 3903 void MacroAssembler::vpmullw(XMMRegister dst, XMMRegister nds, Address src, int vector_len) {
 3904   assert(((dst-&gt;encoding() &lt; 16 &amp;&amp; nds-&gt;encoding() &lt; 16) || VM_Version::supports_avx512vlbw()),&quot;XMM register should be 0-15&quot;);
 3905   Assembler::vpmullw(dst, nds, src, vector_len);
 3906 }
 3907 
 3908 void MacroAssembler::vpsubb(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {
 3909   assert(((dst-&gt;encoding() &lt; 16 &amp;&amp; src-&gt;encoding() &lt; 16 &amp;&amp; nds-&gt;encoding() &lt; 16) || VM_Version::supports_avx512vlbw()),&quot;XMM register should be 0-15&quot;);
 3910   Assembler::vpsubb(dst, nds, src, vector_len);
 3911 }
 3912 
 3913 void MacroAssembler::vpsubb(XMMRegister dst, XMMRegister nds, Address src, int vector_len) {
 3914   assert(((dst-&gt;encoding() &lt; 16 &amp;&amp; nds-&gt;encoding() &lt; 16) || VM_Version::supports_avx512vlbw()),&quot;XMM register should be 0-15&quot;);
 3915   Assembler::vpsubb(dst, nds, src, vector_len);
 3916 }
 3917 
 3918 void MacroAssembler::vpsubw(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {
 3919   assert(((dst-&gt;encoding() &lt; 16 &amp;&amp; src-&gt;encoding() &lt; 16 &amp;&amp; nds-&gt;encoding() &lt; 16) || VM_Version::supports_avx512vlbw()),&quot;XMM register should be 0-15&quot;);
 3920   Assembler::vpsubw(dst, nds, src, vector_len);
 3921 }
 3922 
 3923 void MacroAssembler::vpsubw(XMMRegister dst, XMMRegister nds, Address src, int vector_len) {
 3924   assert(((dst-&gt;encoding() &lt; 16 &amp;&amp; nds-&gt;encoding() &lt; 16) || VM_Version::supports_avx512vlbw()),&quot;XMM register should be 0-15&quot;);
 3925   Assembler::vpsubw(dst, nds, src, vector_len);
 3926 }
 3927 
 3928 void MacroAssembler::vpsraw(XMMRegister dst, XMMRegister nds, XMMRegister shift, int vector_len) {
 3929   assert(((dst-&gt;encoding() &lt; 16 &amp;&amp; shift-&gt;encoding() &lt; 16 &amp;&amp; nds-&gt;encoding() &lt; 16) || VM_Version::supports_avx512vlbw()),&quot;XMM register should be 0-15&quot;);
 3930   Assembler::vpsraw(dst, nds, shift, vector_len);
 3931 }
 3932 
 3933 void MacroAssembler::vpsraw(XMMRegister dst, XMMRegister nds, int shift, int vector_len) {
 3934   assert(((dst-&gt;encoding() &lt; 16 &amp;&amp; nds-&gt;encoding() &lt; 16) || VM_Version::supports_avx512vlbw()),&quot;XMM register should be 0-15&quot;);
 3935   Assembler::vpsraw(dst, nds, shift, vector_len);
 3936 }
 3937 
 3938 void MacroAssembler::evpsraq(XMMRegister dst, XMMRegister nds, XMMRegister shift, int vector_len) {
 3939   assert(UseAVX &gt; 2,&quot;&quot;);
 3940   if (!VM_Version::supports_avx512vl() &amp;&amp; vector_len &lt; 2) {
 3941      vector_len = 2;
 3942   }
 3943   Assembler::evpsraq(dst, nds, shift, vector_len);
 3944 }
 3945 
 3946 void MacroAssembler::evpsraq(XMMRegister dst, XMMRegister nds, int shift, int vector_len) {
 3947   assert(UseAVX &gt; 2,&quot;&quot;);
 3948   if (!VM_Version::supports_avx512vl() &amp;&amp; vector_len &lt; 2) {
 3949      vector_len = 2;
 3950   }
 3951   Assembler::evpsraq(dst, nds, shift, vector_len);
 3952 }
 3953 
 3954 void MacroAssembler::vpsrlw(XMMRegister dst, XMMRegister nds, XMMRegister shift, int vector_len) {
 3955   assert(((dst-&gt;encoding() &lt; 16 &amp;&amp; shift-&gt;encoding() &lt; 16 &amp;&amp; nds-&gt;encoding() &lt; 16) || VM_Version::supports_avx512vlbw()),&quot;XMM register should be 0-15&quot;);
 3956   Assembler::vpsrlw(dst, nds, shift, vector_len);
 3957 }
 3958 
 3959 void MacroAssembler::vpsrlw(XMMRegister dst, XMMRegister nds, int shift, int vector_len) {
 3960   assert(((dst-&gt;encoding() &lt; 16 &amp;&amp; nds-&gt;encoding() &lt; 16) || VM_Version::supports_avx512vlbw()),&quot;XMM register should be 0-15&quot;);
 3961   Assembler::vpsrlw(dst, nds, shift, vector_len);
 3962 }
 3963 
 3964 void MacroAssembler::vpsllw(XMMRegister dst, XMMRegister nds, XMMRegister shift, int vector_len) {
 3965   assert(((dst-&gt;encoding() &lt; 16 &amp;&amp; shift-&gt;encoding() &lt; 16 &amp;&amp; nds-&gt;encoding() &lt; 16) || VM_Version::supports_avx512vlbw()),&quot;XMM register should be 0-15&quot;);
 3966   Assembler::vpsllw(dst, nds, shift, vector_len);
 3967 }
 3968 
 3969 void MacroAssembler::vpsllw(XMMRegister dst, XMMRegister nds, int shift, int vector_len) {
 3970   assert(((dst-&gt;encoding() &lt; 16 &amp;&amp; nds-&gt;encoding() &lt; 16) || VM_Version::supports_avx512vlbw()),&quot;XMM register should be 0-15&quot;);
 3971   Assembler::vpsllw(dst, nds, shift, vector_len);
 3972 }
 3973 
 3974 void MacroAssembler::vptest(XMMRegister dst, XMMRegister src) {
 3975   assert((dst-&gt;encoding() &lt; 16 &amp;&amp; src-&gt;encoding() &lt; 16),&quot;XMM register should be 0-15&quot;);
 3976   Assembler::vptest(dst, src);
 3977 }
 3978 
 3979 void MacroAssembler::punpcklbw(XMMRegister dst, XMMRegister src) {
 3980   assert(((dst-&gt;encoding() &lt; 16 &amp;&amp; src-&gt;encoding() &lt; 16) || VM_Version::supports_avx512vlbw()),&quot;XMM register should be 0-15&quot;);
 3981   Assembler::punpcklbw(dst, src);
 3982 }
 3983 
 3984 void MacroAssembler::pshufd(XMMRegister dst, Address src, int mode) {
 3985   assert(((dst-&gt;encoding() &lt; 16) || VM_Version::supports_avx512vl()),&quot;XMM register should be 0-15&quot;);
 3986   Assembler::pshufd(dst, src, mode);
 3987 }
 3988 
 3989 void MacroAssembler::pshuflw(XMMRegister dst, XMMRegister src, int mode) {
 3990   assert(((dst-&gt;encoding() &lt; 16 &amp;&amp; src-&gt;encoding() &lt; 16) || VM_Version::supports_avx512vlbw()),&quot;XMM register should be 0-15&quot;);
 3991   Assembler::pshuflw(dst, src, mode);
 3992 }
 3993 
 3994 void MacroAssembler::vandpd(XMMRegister dst, XMMRegister nds, AddressLiteral src, int vector_len, Register scratch_reg) {
 3995   if (reachable(src)) {
 3996     vandpd(dst, nds, as_Address(src), vector_len);
 3997   } else {
 3998     lea(scratch_reg, src);
 3999     vandpd(dst, nds, Address(scratch_reg, 0), vector_len);
 4000   }
 4001 }
 4002 
 4003 void MacroAssembler::vandps(XMMRegister dst, XMMRegister nds, AddressLiteral src, int vector_len, Register scratch_reg) {
 4004   if (reachable(src)) {
 4005     vandps(dst, nds, as_Address(src), vector_len);
 4006   } else {
 4007     lea(scratch_reg, src);
 4008     vandps(dst, nds, Address(scratch_reg, 0), vector_len);
 4009   }
 4010 }
 4011 
 4012 void MacroAssembler::vdivsd(XMMRegister dst, XMMRegister nds, AddressLiteral src) {
 4013   if (reachable(src)) {
 4014     vdivsd(dst, nds, as_Address(src));
 4015   } else {
 4016     lea(rscratch1, src);
 4017     vdivsd(dst, nds, Address(rscratch1, 0));
 4018   }
 4019 }
 4020 
 4021 void MacroAssembler::vdivss(XMMRegister dst, XMMRegister nds, AddressLiteral src) {
 4022   if (reachable(src)) {
 4023     vdivss(dst, nds, as_Address(src));
 4024   } else {
 4025     lea(rscratch1, src);
 4026     vdivss(dst, nds, Address(rscratch1, 0));
 4027   }
 4028 }
 4029 
 4030 void MacroAssembler::vmulsd(XMMRegister dst, XMMRegister nds, AddressLiteral src) {
 4031   if (reachable(src)) {
 4032     vmulsd(dst, nds, as_Address(src));
 4033   } else {
 4034     lea(rscratch1, src);
 4035     vmulsd(dst, nds, Address(rscratch1, 0));
 4036   }
 4037 }
 4038 
 4039 void MacroAssembler::vmulss(XMMRegister dst, XMMRegister nds, AddressLiteral src) {
 4040   if (reachable(src)) {
 4041     vmulss(dst, nds, as_Address(src));
 4042   } else {
 4043     lea(rscratch1, src);
 4044     vmulss(dst, nds, Address(rscratch1, 0));
 4045   }
 4046 }
 4047 
 4048 void MacroAssembler::vsubsd(XMMRegister dst, XMMRegister nds, AddressLiteral src) {
 4049   if (reachable(src)) {
 4050     vsubsd(dst, nds, as_Address(src));
 4051   } else {
 4052     lea(rscratch1, src);
 4053     vsubsd(dst, nds, Address(rscratch1, 0));
 4054   }
 4055 }
 4056 
 4057 void MacroAssembler::vsubss(XMMRegister dst, XMMRegister nds, AddressLiteral src) {
 4058   if (reachable(src)) {
 4059     vsubss(dst, nds, as_Address(src));
 4060   } else {
 4061     lea(rscratch1, src);
 4062     vsubss(dst, nds, Address(rscratch1, 0));
 4063   }
 4064 }
 4065 
 4066 void MacroAssembler::vnegatess(XMMRegister dst, XMMRegister nds, AddressLiteral src) {
 4067   assert(((dst-&gt;encoding() &lt; 16 &amp;&amp; nds-&gt;encoding() &lt; 16) || VM_Version::supports_avx512vldq()),&quot;XMM register should be 0-15&quot;);
 4068   vxorps(dst, nds, src, Assembler::AVX_128bit);
 4069 }
 4070 
 4071 void MacroAssembler::vnegatesd(XMMRegister dst, XMMRegister nds, AddressLiteral src) {
 4072   assert(((dst-&gt;encoding() &lt; 16 &amp;&amp; nds-&gt;encoding() &lt; 16) || VM_Version::supports_avx512vldq()),&quot;XMM register should be 0-15&quot;);
 4073   vxorpd(dst, nds, src, Assembler::AVX_128bit);
 4074 }
 4075 
 4076 void MacroAssembler::vxorpd(XMMRegister dst, XMMRegister nds, AddressLiteral src, int vector_len, Register scratch_reg) {
 4077   if (reachable(src)) {
 4078     vxorpd(dst, nds, as_Address(src), vector_len);
 4079   } else {
 4080     lea(scratch_reg, src);
 4081     vxorpd(dst, nds, Address(scratch_reg, 0), vector_len);
 4082   }
 4083 }
 4084 
 4085 void MacroAssembler::vxorps(XMMRegister dst, XMMRegister nds, AddressLiteral src, int vector_len, Register scratch_reg) {
 4086   if (reachable(src)) {
 4087     vxorps(dst, nds, as_Address(src), vector_len);
 4088   } else {
 4089     lea(scratch_reg, src);
 4090     vxorps(dst, nds, Address(scratch_reg, 0), vector_len);
 4091   }
 4092 }
 4093 
 4094 void MacroAssembler::vpxor(XMMRegister dst, XMMRegister nds, AddressLiteral src, int vector_len, Register scratch_reg) {
 4095   if (UseAVX &gt; 1 || (vector_len &lt; 1)) {
 4096     if (reachable(src)) {
 4097       Assembler::vpxor(dst, nds, as_Address(src), vector_len);
 4098     } else {
 4099       lea(scratch_reg, src);
 4100       Assembler::vpxor(dst, nds, Address(scratch_reg, 0), vector_len);
 4101     }
 4102   }
 4103   else {
 4104     MacroAssembler::vxorpd(dst, nds, src, vector_len, scratch_reg);
 4105   }
 4106 }
 4107 
 4108 //-------------------------------------------------------------------------------------------
 4109 #ifdef COMPILER2
 4110 // Generic instructions support for use in .ad files C2 code generation
 4111 
 4112 void MacroAssembler::vabsnegd(int opcode, XMMRegister dst, XMMRegister src, Register scr) {
 4113   if (dst != src) {
 4114     movdqu(dst, src);
 4115   }
 4116   if (opcode == Op_AbsVD) {
 4117     andpd(dst, ExternalAddress(StubRoutines::x86::vector_double_sign_mask()), scr);
 4118   } else {
 4119     assert((opcode == Op_NegVD),&quot;opcode should be Op_NegD&quot;);
 4120     xorpd(dst, ExternalAddress(StubRoutines::x86::vector_double_sign_flip()), scr);
 4121   }
 4122 }
 4123 
 4124 void MacroAssembler::vabsnegd(int opcode, XMMRegister dst, XMMRegister src, int vector_len, Register scr) {
 4125   if (opcode == Op_AbsVD) {
 4126     vandpd(dst, src, ExternalAddress(StubRoutines::x86::vector_double_sign_mask()), vector_len, scr);
 4127   } else {
 4128     assert((opcode == Op_NegVD),&quot;opcode should be Op_NegD&quot;);
 4129     vxorpd(dst, src, ExternalAddress(StubRoutines::x86::vector_double_sign_flip()), vector_len, scr);
 4130   }
 4131 }
 4132 
 4133 void MacroAssembler::vabsnegf(int opcode, XMMRegister dst, XMMRegister src, Register scr) {
 4134   if (dst != src) {
 4135     movdqu(dst, src);
 4136   }
 4137   if (opcode == Op_AbsVF) {
 4138     andps(dst, ExternalAddress(StubRoutines::x86::vector_float_sign_mask()), scr);
 4139   } else {
 4140     assert((opcode == Op_NegVF),&quot;opcode should be Op_NegF&quot;);
 4141     xorps(dst, ExternalAddress(StubRoutines::x86::vector_float_sign_flip()), scr);
 4142   }
 4143 }
 4144 
 4145 void MacroAssembler::vabsnegf(int opcode, XMMRegister dst, XMMRegister src, int vector_len, Register scr) {
 4146   if (opcode == Op_AbsVF) {
 4147     vandps(dst, src, ExternalAddress(StubRoutines::x86::vector_float_sign_mask()), vector_len, scr);
 4148   } else {
 4149     assert((opcode == Op_NegVF),&quot;opcode should be Op_NegF&quot;);
 4150     vxorps(dst, src, ExternalAddress(StubRoutines::x86::vector_float_sign_flip()), vector_len, scr);
 4151   }
 4152 }
 4153 
 4154 void MacroAssembler::vextendbw(bool sign, XMMRegister dst, XMMRegister src) {
 4155   if (sign) {
 4156     pmovsxbw(dst, src);
 4157   } else {
 4158     pmovzxbw(dst, src);
 4159   }
 4160 }
 4161 
 4162 void MacroAssembler::vextendbw(bool sign, XMMRegister dst, XMMRegister src, int vector_len) {
 4163   if (sign) {
 4164     vpmovsxbw(dst, src, vector_len);
 4165   } else {
 4166     vpmovzxbw(dst, src, vector_len);
 4167   }
 4168 }
 4169 
 4170 void MacroAssembler::vshiftd(int opcode, XMMRegister dst, XMMRegister src) {
 4171   if (opcode == Op_RShiftVI) {
 4172     psrad(dst, src);
 4173   } else if (opcode == Op_LShiftVI) {
 4174     pslld(dst, src);
 4175   } else {
 4176     assert((opcode == Op_URShiftVI),&quot;opcode should be Op_URShiftVI&quot;);
 4177     psrld(dst, src);
 4178   }
 4179 }
 4180 
 4181 void MacroAssembler::vshiftd(int opcode, XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {
 4182   if (opcode == Op_RShiftVI) {
 4183     vpsrad(dst, nds, src, vector_len);
 4184   } else if (opcode == Op_LShiftVI) {
 4185     vpslld(dst, nds, src, vector_len);
 4186   } else {
 4187     assert((opcode == Op_URShiftVI),&quot;opcode should be Op_URShiftVI&quot;);
 4188     vpsrld(dst, nds, src, vector_len);
 4189   }
 4190 }
 4191 
 4192 void MacroAssembler::vshiftw(int opcode, XMMRegister dst, XMMRegister src) {
 4193   if ((opcode == Op_RShiftVS) || (opcode == Op_RShiftVB)) {
 4194     psraw(dst, src);
 4195   } else if ((opcode == Op_LShiftVS) || (opcode == Op_LShiftVB)) {
 4196     psllw(dst, src);
 4197   } else {
 4198     assert(((opcode == Op_URShiftVS) || (opcode == Op_URShiftVB)),&quot;opcode should be one of Op_URShiftVS or Op_URShiftVB&quot;);
 4199     psrlw(dst, src);
 4200   }
 4201 }
 4202 
 4203 void MacroAssembler::vshiftw(int opcode, XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {
 4204   if ((opcode == Op_RShiftVS) || (opcode == Op_RShiftVB)) {
 4205     vpsraw(dst, nds, src, vector_len);
 4206   } else if ((opcode == Op_LShiftVS) || (opcode == Op_LShiftVB)) {
 4207     vpsllw(dst, nds, src, vector_len);
 4208   } else {
 4209     assert(((opcode == Op_URShiftVS) || (opcode == Op_URShiftVB)),&quot;opcode should be one of Op_URShiftVS or Op_URShiftVB&quot;);
 4210     vpsrlw(dst, nds, src, vector_len);
 4211   }
 4212 }
 4213 
 4214 void MacroAssembler::vshiftq(int opcode, XMMRegister dst, XMMRegister src) {
 4215   if (opcode == Op_RShiftVL) {
 4216     psrlq(dst, src);  // using srl to implement sra on pre-avs512 systems
 4217   } else if (opcode == Op_LShiftVL) {
 4218     psllq(dst, src);
 4219   } else {
 4220     assert((opcode == Op_URShiftVL),&quot;opcode should be Op_URShiftVL&quot;);
 4221     psrlq(dst, src);
 4222   }
 4223 }
 4224 
 4225 void MacroAssembler::vshiftq(int opcode, XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {
 4226   if (opcode == Op_RShiftVL) {
 4227     evpsraq(dst, nds, src, vector_len);
 4228   } else if (opcode == Op_LShiftVL) {
 4229     vpsllq(dst, nds, src, vector_len);
 4230   } else {
 4231     assert((opcode == Op_URShiftVL),&quot;opcode should be Op_URShiftVL&quot;);
 4232     vpsrlq(dst, nds, src, vector_len);
 4233   }
 4234 }
 4235 #endif
 4236 //-------------------------------------------------------------------------------------------
 4237 
 4238 void MacroAssembler::clear_jweak_tag(Register possibly_jweak) {
 4239   const int32_t inverted_jweak_mask = ~static_cast&lt;int32_t&gt;(JNIHandles::weak_tag_mask);
 4240   STATIC_ASSERT(inverted_jweak_mask == -2); // otherwise check this code
 4241   // The inverted mask is sign-extended
 4242   andptr(possibly_jweak, inverted_jweak_mask);
 4243 }
 4244 
 4245 void MacroAssembler::resolve_jobject(Register value,
 4246                                      Register thread,
 4247                                      Register tmp) {
 4248   assert_different_registers(value, thread, tmp);
 4249   Label done, not_weak;
 4250   testptr(value, value);
 4251   jcc(Assembler::zero, done);                // Use NULL as-is.
 4252   testptr(value, JNIHandles::weak_tag_mask); // Test for jweak tag.
 4253   jcc(Assembler::zero, not_weak);
 4254   // Resolve jweak.
 4255   access_load_at(T_OBJECT, IN_NATIVE | ON_PHANTOM_OOP_REF,
 4256                  value, Address(value, -JNIHandles::weak_tag_value), tmp, thread);
 4257   verify_oop(value);
 4258   jmp(done);
 4259   bind(not_weak);
 4260   // Resolve (untagged) jobject.
 4261   access_load_at(T_OBJECT, IN_NATIVE, value, Address(value, 0), tmp, thread);
 4262   verify_oop(value);
 4263   bind(done);
 4264 }
 4265 
 4266 void MacroAssembler::subptr(Register dst, int32_t imm32) {
 4267   LP64_ONLY(subq(dst, imm32)) NOT_LP64(subl(dst, imm32));
 4268 }
 4269 
 4270 // Force generation of a 4 byte immediate value even if it fits into 8bit
 4271 void MacroAssembler::subptr_imm32(Register dst, int32_t imm32) {
 4272   LP64_ONLY(subq_imm32(dst, imm32)) NOT_LP64(subl_imm32(dst, imm32));
 4273 }
 4274 
 4275 void MacroAssembler::subptr(Register dst, Register src) {
 4276   LP64_ONLY(subq(dst, src)) NOT_LP64(subl(dst, src));
 4277 }
 4278 
 4279 // C++ bool manipulation
 4280 void MacroAssembler::testbool(Register dst) {
 4281   if(sizeof(bool) == 1)
 4282     testb(dst, 0xff);
 4283   else if(sizeof(bool) == 2) {
 4284     // testw implementation needed for two byte bools
 4285     ShouldNotReachHere();
 4286   } else if(sizeof(bool) == 4)
 4287     testl(dst, dst);
 4288   else
 4289     // unsupported
 4290     ShouldNotReachHere();
 4291 }
 4292 
 4293 void MacroAssembler::testptr(Register dst, Register src) {
 4294   LP64_ONLY(testq(dst, src)) NOT_LP64(testl(dst, src));
 4295 }
 4296 
 4297 // Object / value buffer allocation...
 4298 //
 4299 // Kills klass and rsi on LP64
 4300 void MacroAssembler::allocate_instance(Register klass, Register new_obj,
 4301                                        Register t1, Register t2,
 4302                                        bool clear_fields, Label&amp; alloc_failed)
 4303 {
 4304   Label done, initialize_header, initialize_object, slow_case, slow_case_no_pop;
 4305   Register layout_size = t1;
 4306   assert(new_obj == rax, &quot;needs to be rax, according to barrier asm eden_allocate&quot;);
 4307   assert_different_registers(klass, new_obj, t1, t2);
 4308 
 4309 #ifdef ASSERT
 4310   {
 4311     Label L;
 4312     cmpb(Address(klass, InstanceKlass::init_state_offset()), InstanceKlass::fully_initialized);
 4313     jcc(Assembler::equal, L);
 4314     stop(&quot;klass not initialized&quot;);
 4315     bind(L);
 4316   }
 4317 #endif
 4318 
 4319   // get instance_size in InstanceKlass (scaled to a count of bytes)
 4320   movl(layout_size, Address(klass, Klass::layout_helper_offset()));
 4321   // test to see if it has a finalizer or is malformed in some way
 4322   testl(layout_size, Klass::_lh_instance_slow_path_bit);
 4323   jcc(Assembler::notZero, slow_case_no_pop);
 4324 
 4325   // Allocate the instance:
 4326   //  If TLAB is enabled:
 4327   //    Try to allocate in the TLAB.
 4328   //    If fails, go to the slow path.
 4329   //  Else If inline contiguous allocations are enabled:
 4330   //    Try to allocate in eden.
 4331   //    If fails due to heap end, go to slow path.
 4332   //
 4333   //  If TLAB is enabled OR inline contiguous is enabled:
 4334   //    Initialize the allocation.
 4335   //    Exit.
 4336   //
 4337   //  Go to slow path.
 4338   const bool allow_shared_alloc =
 4339     Universe::heap()-&gt;supports_inline_contig_alloc();
 4340 
 4341   push(klass);
 4342   const Register thread = LP64_ONLY(r15_thread) NOT_LP64(klass);
 4343 #ifndef _LP64
 4344   if (UseTLAB || allow_shared_alloc) {
 4345     get_thread(thread);
 4346   }
 4347 #endif // _LP64
 4348 
 4349   if (UseTLAB) {
 4350     tlab_allocate(thread, new_obj, layout_size, 0, klass, t2, slow_case);
 4351     if (ZeroTLAB || (!clear_fields)) {
 4352       // the fields have been already cleared
 4353       jmp(initialize_header);
 4354     } else {
 4355       // initialize both the header and fields
 4356       jmp(initialize_object);
 4357     }
 4358   } else {
 4359     // Allocation in the shared Eden, if allowed.
 4360     //
 4361     eden_allocate(thread, new_obj, layout_size, 0, t2, slow_case);
 4362   }
 4363 
 4364   // If UseTLAB or allow_shared_alloc are true, the object is created above and
 4365   // there is an initialize need. Otherwise, skip and go to the slow path.
 4366   if (UseTLAB || allow_shared_alloc) {
 4367     if (clear_fields) {
 4368       // The object is initialized before the header.  If the object size is
 4369       // zero, go directly to the header initialization.
 4370       bind(initialize_object);
 4371       decrement(layout_size, sizeof(oopDesc));
 4372       jcc(Assembler::zero, initialize_header);
 4373 
 4374       // Initialize topmost object field, divide size by 8, check if odd and
 4375       // test if zero.
 4376       Register zero = klass;
 4377       xorl(zero, zero);    // use zero reg to clear memory (shorter code)
 4378       shrl(layout_size, LogBytesPerLong); // divide by 2*oopSize and set carry flag if odd
 4379 
 4380   #ifdef ASSERT
 4381       // make sure instance_size was multiple of 8
 4382       Label L;
 4383       // Ignore partial flag stall after shrl() since it is debug VM
 4384       jcc(Assembler::carryClear, L);
 4385       stop(&quot;object size is not multiple of 2 - adjust this code&quot;);
 4386       bind(L);
 4387       // must be &gt; 0, no extra check needed here
 4388   #endif
 4389 
 4390       // initialize remaining object fields: instance_size was a multiple of 8
 4391       {
 4392         Label loop;
 4393         bind(loop);
 4394         movptr(Address(new_obj, layout_size, Address::times_8, sizeof(oopDesc) - 1*oopSize), zero);
 4395         NOT_LP64(movptr(Address(new_obj, layout_size, Address::times_8, sizeof(oopDesc) - 2*oopSize), zero));
 4396         decrement(layout_size);
 4397         jcc(Assembler::notZero, loop);
 4398       }
 4399     } // clear_fields
 4400 
 4401     // initialize object header only.
 4402     bind(initialize_header);
 4403     pop(klass);
 4404     Register mark_word = t2;
 4405     movptr(mark_word, Address(klass, Klass::prototype_header_offset()));
 4406     movptr(Address(new_obj, oopDesc::mark_offset_in_bytes ()), mark_word);
 4407 #ifdef _LP64
 4408     xorl(rsi, rsi);                 // use zero reg to clear memory (shorter code)
 4409     store_klass_gap(new_obj, rsi);  // zero klass gap for compressed oops
 4410 #endif
 4411     movptr(t2, klass);         // preserve klass
 4412     store_klass(new_obj, t2);  // src klass reg is potentially compressed
 4413 
 4414     jmp(done);
 4415   }
 4416 
 4417   bind(slow_case);
 4418   pop(klass);
 4419   bind(slow_case_no_pop);
 4420   jmp(alloc_failed);
 4421 
 4422   bind(done);
 4423 }
 4424 
 4425 // Defines obj, preserves var_size_in_bytes, okay for t2 == var_size_in_bytes.
 4426 void MacroAssembler::tlab_allocate(Register thread, Register obj,
 4427                                    Register var_size_in_bytes,
 4428                                    int con_size_in_bytes,
 4429                                    Register t1,
 4430                                    Register t2,
 4431                                    Label&amp; slow_case) {
 4432   BarrierSetAssembler* bs = BarrierSet::barrier_set()-&gt;barrier_set_assembler();
 4433   bs-&gt;tlab_allocate(this, thread, obj, var_size_in_bytes, con_size_in_bytes, t1, t2, slow_case);
 4434 }
 4435 
 4436 // Defines obj, preserves var_size_in_bytes
 4437 void MacroAssembler::eden_allocate(Register thread, Register obj,
 4438                                    Register var_size_in_bytes,
 4439                                    int con_size_in_bytes,
 4440                                    Register t1,
 4441                                    Label&amp; slow_case) {
 4442   BarrierSetAssembler* bs = BarrierSet::barrier_set()-&gt;barrier_set_assembler();
 4443   bs-&gt;eden_allocate(this, thread, obj, var_size_in_bytes, con_size_in_bytes, t1, slow_case);
 4444 }
 4445 
 4446 // Preserves the contents of address, destroys the contents length_in_bytes and temp.
 4447 void MacroAssembler::zero_memory(Register address, Register length_in_bytes, int offset_in_bytes, Register temp) {
 4448   assert(address != length_in_bytes &amp;&amp; address != temp &amp;&amp; temp != length_in_bytes, &quot;registers must be different&quot;);
 4449   assert((offset_in_bytes &amp; (BytesPerWord - 1)) == 0, &quot;offset must be a multiple of BytesPerWord&quot;);
 4450   Label done;
 4451 
 4452   testptr(length_in_bytes, length_in_bytes);
 4453   jcc(Assembler::zero, done);
 4454 
 4455   // initialize topmost word, divide index by 2, check if odd and test if zero
 4456   // note: for the remaining code to work, index must be a multiple of BytesPerWord
 4457 #ifdef ASSERT
 4458   {
 4459     Label L;
 4460     testptr(length_in_bytes, BytesPerWord - 1);
 4461     jcc(Assembler::zero, L);
 4462     stop(&quot;length must be a multiple of BytesPerWord&quot;);
 4463     bind(L);
 4464   }
 4465 #endif
 4466   Register index = length_in_bytes;
 4467   xorptr(temp, temp);    // use _zero reg to clear memory (shorter code)
 4468   if (UseIncDec) {
 4469     shrptr(index, 3);  // divide by 8/16 and set carry flag if bit 2 was set
 4470   } else {
 4471     shrptr(index, 2);  // use 2 instructions to avoid partial flag stall
 4472     shrptr(index, 1);
 4473   }
 4474 #ifndef _LP64
 4475   // index could have not been a multiple of 8 (i.e., bit 2 was set)
 4476   {
 4477     Label even;
 4478     // note: if index was a multiple of 8, then it cannot
 4479     //       be 0 now otherwise it must have been 0 before
 4480     //       =&gt; if it is even, we don&#39;t need to check for 0 again
 4481     jcc(Assembler::carryClear, even);
 4482     // clear topmost word (no jump would be needed if conditional assignment worked here)
 4483     movptr(Address(address, index, Address::times_8, offset_in_bytes - 0*BytesPerWord), temp);
 4484     // index could be 0 now, must check again
 4485     jcc(Assembler::zero, done);
 4486     bind(even);
 4487   }
 4488 #endif // !_LP64
 4489   // initialize remaining object fields: index is a multiple of 2 now
 4490   {
 4491     Label loop;
 4492     bind(loop);
 4493     movptr(Address(address, index, Address::times_8, offset_in_bytes - 1*BytesPerWord), temp);
 4494     NOT_LP64(movptr(Address(address, index, Address::times_8, offset_in_bytes - 2*BytesPerWord), temp);)
 4495     decrement(index);
 4496     jcc(Assembler::notZero, loop);
 4497   }
 4498 
 4499   bind(done);
 4500 }
 4501 
 4502 void MacroAssembler::get_value_field_klass(Register klass, Register index, Register value_klass) {
 4503   movptr(value_klass, Address(klass, InstanceKlass::value_field_klasses_offset()));
 4504 #ifdef ASSERT
 4505   {
 4506     Label done;
 4507     cmpptr(value_klass, 0);
 4508     jcc(Assembler::notEqual, done);
 4509     stop(&quot;get_value_field_klass contains no inline klasses&quot;);
 4510     bind(done);
 4511   }
 4512 #endif
 4513   movptr(value_klass, Address(value_klass, index, Address::times_ptr));
 4514 }
 4515 
 4516 void MacroAssembler::get_default_value_oop(Register value_klass, Register temp_reg, Register obj) {
 4517 #ifdef ASSERT
 4518   {
 4519     Label done_check;
 4520     test_klass_is_value(value_klass, temp_reg, done_check);
 4521     stop(&quot;get_default_value_oop from non-value klass&quot;);
 4522     bind(done_check);
 4523   }
 4524 #endif
 4525   Register offset = temp_reg;
 4526   // Getting the offset of the pre-allocated default value
 4527   movptr(offset, Address(value_klass, in_bytes(InstanceKlass::adr_valueklass_fixed_block_offset())));
 4528   movl(offset, Address(offset, in_bytes(ValueKlass::default_value_offset_offset())));
 4529 
 4530   // Getting the mirror
 4531   movptr(obj, Address(value_klass, in_bytes(Klass::java_mirror_offset())));
 4532   resolve_oop_handle(obj, value_klass);
 4533 
 4534   // Getting the pre-allocated default value from the mirror
 4535   Address field(obj, offset, Address::times_1);
 4536   load_heap_oop(obj, field);
 4537 }
 4538 
 4539 void MacroAssembler::get_empty_value_oop(Register value_klass, Register temp_reg, Register obj) {
 4540 #ifdef ASSERT
 4541   {
 4542     Label done_check;
 4543     test_klass_is_empty_value(value_klass, temp_reg, done_check);
 4544     stop(&quot;get_empty_value from non-empty value klass&quot;);
 4545     bind(done_check);
 4546   }
 4547 #endif
 4548   get_default_value_oop(value_klass, temp_reg, obj);
 4549 }
 4550 
 4551 
 4552 // Look up the method for a megamorphic invokeinterface call.
 4553 // The target method is determined by &lt;intf_klass, itable_index&gt;.
 4554 // The receiver klass is in recv_klass.
 4555 // On success, the result will be in method_result, and execution falls through.
 4556 // On failure, execution transfers to the given label.
 4557 void MacroAssembler::lookup_interface_method(Register recv_klass,
 4558                                              Register intf_klass,
 4559                                              RegisterOrConstant itable_index,
 4560                                              Register method_result,
 4561                                              Register scan_temp,
 4562                                              Label&amp; L_no_such_interface,
 4563                                              bool return_method) {
 4564   assert_different_registers(recv_klass, intf_klass, scan_temp);
 4565   assert_different_registers(method_result, intf_klass, scan_temp);
 4566   assert(recv_klass != method_result || !return_method,
 4567          &quot;recv_klass can be destroyed when method isn&#39;t needed&quot;);
 4568 
 4569   assert(itable_index.is_constant() || itable_index.as_register() == method_result,
 4570          &quot;caller must use same register for non-constant itable index as for method&quot;);
 4571 
 4572   // Compute start of first itableOffsetEntry (which is at the end of the vtable)
 4573   int vtable_base = in_bytes(Klass::vtable_start_offset());
 4574   int itentry_off = itableMethodEntry::method_offset_in_bytes();
 4575   int scan_step   = itableOffsetEntry::size() * wordSize;
 4576   int vte_size    = vtableEntry::size_in_bytes();
 4577   Address::ScaleFactor times_vte_scale = Address::times_ptr;
 4578   assert(vte_size == wordSize, &quot;else adjust times_vte_scale&quot;);
 4579 
 4580   movl(scan_temp, Address(recv_klass, Klass::vtable_length_offset()));
 4581 
 4582   // %%% Could store the aligned, prescaled offset in the klassoop.
 4583   lea(scan_temp, Address(recv_klass, scan_temp, times_vte_scale, vtable_base));
 4584 
 4585   if (return_method) {
 4586     // Adjust recv_klass by scaled itable_index, so we can free itable_index.
 4587     assert(itableMethodEntry::size() * wordSize == wordSize, &quot;adjust the scaling in the code below&quot;);
 4588     lea(recv_klass, Address(recv_klass, itable_index, Address::times_ptr, itentry_off));
 4589   }
 4590 
 4591   // for (scan = klass-&gt;itable(); scan-&gt;interface() != NULL; scan += scan_step) {
 4592   //   if (scan-&gt;interface() == intf) {
 4593   //     result = (klass + scan-&gt;offset() + itable_index);
 4594   //   }
 4595   // }
 4596   Label search, found_method;
 4597 
 4598   for (int peel = 1; peel &gt;= 0; peel--) {
 4599     movptr(method_result, Address(scan_temp, itableOffsetEntry::interface_offset_in_bytes()));
 4600     cmpptr(intf_klass, method_result);
 4601 
 4602     if (peel) {
 4603       jccb(Assembler::equal, found_method);
 4604     } else {
 4605       jccb(Assembler::notEqual, search);
 4606       // (invert the test to fall through to found_method...)
 4607     }
 4608 
 4609     if (!peel)  break;
 4610 
 4611     bind(search);
 4612 
 4613     // Check that the previous entry is non-null.  A null entry means that
 4614     // the receiver class doesn&#39;t implement the interface, and wasn&#39;t the
 4615     // same as when the caller was compiled.
 4616     testptr(method_result, method_result);
 4617     jcc(Assembler::zero, L_no_such_interface);
 4618     addptr(scan_temp, scan_step);
 4619   }
 4620 
 4621   bind(found_method);
 4622 
 4623   if (return_method) {
 4624     // Got a hit.
 4625     movl(scan_temp, Address(scan_temp, itableOffsetEntry::offset_offset_in_bytes()));
 4626     movptr(method_result, Address(recv_klass, scan_temp, Address::times_1));
 4627   }
 4628 }
 4629 
 4630 
 4631 // virtual method calling
 4632 void MacroAssembler::lookup_virtual_method(Register recv_klass,
 4633                                            RegisterOrConstant vtable_index,
 4634                                            Register method_result) {
 4635   const int base = in_bytes(Klass::vtable_start_offset());
 4636   assert(vtableEntry::size() * wordSize == wordSize, &quot;else adjust the scaling in the code below&quot;);
 4637   Address vtable_entry_addr(recv_klass,
 4638                             vtable_index, Address::times_ptr,
 4639                             base + vtableEntry::method_offset_in_bytes());
 4640   movptr(method_result, vtable_entry_addr);
 4641 }
 4642 
 4643 
 4644 void MacroAssembler::check_klass_subtype(Register sub_klass,
 4645                            Register super_klass,
 4646                            Register temp_reg,
 4647                            Label&amp; L_success) {
 4648   Label L_failure;
 4649   check_klass_subtype_fast_path(sub_klass, super_klass, temp_reg,        &amp;L_success, &amp;L_failure, NULL);
 4650   check_klass_subtype_slow_path(sub_klass, super_klass, temp_reg, noreg, &amp;L_success, NULL);
 4651   bind(L_failure);
 4652 }
 4653 
 4654 
 4655 void MacroAssembler::check_klass_subtype_fast_path(Register sub_klass,
 4656                                                    Register super_klass,
 4657                                                    Register temp_reg,
 4658                                                    Label* L_success,
 4659                                                    Label* L_failure,
 4660                                                    Label* L_slow_path,
 4661                                         RegisterOrConstant super_check_offset) {
 4662   assert_different_registers(sub_klass, super_klass, temp_reg);
 4663   bool must_load_sco = (super_check_offset.constant_or_zero() == -1);
 4664   if (super_check_offset.is_register()) {
 4665     assert_different_registers(sub_klass, super_klass,
 4666                                super_check_offset.as_register());
 4667   } else if (must_load_sco) {
 4668     assert(temp_reg != noreg, &quot;supply either a temp or a register offset&quot;);
 4669   }
 4670 
 4671   Label L_fallthrough;
 4672   int label_nulls = 0;
 4673   if (L_success == NULL)   { L_success   = &amp;L_fallthrough; label_nulls++; }
 4674   if (L_failure == NULL)   { L_failure   = &amp;L_fallthrough; label_nulls++; }
 4675   if (L_slow_path == NULL) { L_slow_path = &amp;L_fallthrough; label_nulls++; }
 4676   assert(label_nulls &lt;= 1, &quot;at most one NULL in the batch&quot;);
 4677 
 4678   int sc_offset = in_bytes(Klass::secondary_super_cache_offset());
 4679   int sco_offset = in_bytes(Klass::super_check_offset_offset());
 4680   Address super_check_offset_addr(super_klass, sco_offset);
 4681 
 4682   // Hacked jcc, which &quot;knows&quot; that L_fallthrough, at least, is in
 4683   // range of a jccb.  If this routine grows larger, reconsider at
 4684   // least some of these.
 4685 #define local_jcc(assembler_cond, label)                                \
 4686   if (&amp;(label) == &amp;L_fallthrough)  jccb(assembler_cond, label);         \
 4687   else                             jcc( assembler_cond, label) /*omit semi*/
 4688 
 4689   // Hacked jmp, which may only be used just before L_fallthrough.
 4690 #define final_jmp(label)                                                \
 4691   if (&amp;(label) == &amp;L_fallthrough) { /*do nothing*/ }                    \
 4692   else                            jmp(label)                /*omit semi*/
 4693 
 4694   // If the pointers are equal, we are done (e.g., String[] elements).
 4695   // This self-check enables sharing of secondary supertype arrays among
 4696   // non-primary types such as array-of-interface.  Otherwise, each such
 4697   // type would need its own customized SSA.
 4698   // We move this check to the front of the fast path because many
 4699   // type checks are in fact trivially successful in this manner,
 4700   // so we get a nicely predicted branch right at the start of the check.
 4701   cmpptr(sub_klass, super_klass);
 4702   local_jcc(Assembler::equal, *L_success);
 4703 
 4704   // Check the supertype display:
 4705   if (must_load_sco) {
 4706     // Positive movl does right thing on LP64.
 4707     movl(temp_reg, super_check_offset_addr);
 4708     super_check_offset = RegisterOrConstant(temp_reg);
 4709   }
 4710   Address super_check_addr(sub_klass, super_check_offset, Address::times_1, 0);
 4711   cmpptr(super_klass, super_check_addr); // load displayed supertype
 4712 
 4713   // This check has worked decisively for primary supers.
 4714   // Secondary supers are sought in the super_cache (&#39;super_cache_addr&#39;).
 4715   // (Secondary supers are interfaces and very deeply nested subtypes.)
 4716   // This works in the same check above because of a tricky aliasing
 4717   // between the super_cache and the primary super display elements.
 4718   // (The &#39;super_check_addr&#39; can address either, as the case requires.)
 4719   // Note that the cache is updated below if it does not help us find
 4720   // what we need immediately.
 4721   // So if it was a primary super, we can just fail immediately.
 4722   // Otherwise, it&#39;s the slow path for us (no success at this point).
 4723 
 4724   if (super_check_offset.is_register()) {
 4725     local_jcc(Assembler::equal, *L_success);
 4726     cmpl(super_check_offset.as_register(), sc_offset);
 4727     if (L_failure == &amp;L_fallthrough) {
 4728       local_jcc(Assembler::equal, *L_slow_path);
 4729     } else {
 4730       local_jcc(Assembler::notEqual, *L_failure);
 4731       final_jmp(*L_slow_path);
 4732     }
 4733   } else if (super_check_offset.as_constant() == sc_offset) {
 4734     // Need a slow path; fast failure is impossible.
 4735     if (L_slow_path == &amp;L_fallthrough) {
 4736       local_jcc(Assembler::equal, *L_success);
 4737     } else {
 4738       local_jcc(Assembler::notEqual, *L_slow_path);
 4739       final_jmp(*L_success);
 4740     }
 4741   } else {
 4742     // No slow path; it&#39;s a fast decision.
 4743     if (L_failure == &amp;L_fallthrough) {
 4744       local_jcc(Assembler::equal, *L_success);
 4745     } else {
 4746       local_jcc(Assembler::notEqual, *L_failure);
 4747       final_jmp(*L_success);
 4748     }
 4749   }
 4750 
 4751   bind(L_fallthrough);
 4752 
 4753 #undef local_jcc
 4754 #undef final_jmp
 4755 }
 4756 
 4757 
 4758 void MacroAssembler::check_klass_subtype_slow_path(Register sub_klass,
 4759                                                    Register super_klass,
 4760                                                    Register temp_reg,
 4761                                                    Register temp2_reg,
 4762                                                    Label* L_success,
 4763                                                    Label* L_failure,
 4764                                                    bool set_cond_codes) {
 4765   assert_different_registers(sub_klass, super_klass, temp_reg);
 4766   if (temp2_reg != noreg)
 4767     assert_different_registers(sub_klass, super_klass, temp_reg, temp2_reg);
 4768 #define IS_A_TEMP(reg) ((reg) == temp_reg || (reg) == temp2_reg)
 4769 
 4770   Label L_fallthrough;
 4771   int label_nulls = 0;
 4772   if (L_success == NULL)   { L_success   = &amp;L_fallthrough; label_nulls++; }
 4773   if (L_failure == NULL)   { L_failure   = &amp;L_fallthrough; label_nulls++; }
 4774   assert(label_nulls &lt;= 1, &quot;at most one NULL in the batch&quot;);
 4775 
 4776   // a couple of useful fields in sub_klass:
 4777   int ss_offset = in_bytes(Klass::secondary_supers_offset());
 4778   int sc_offset = in_bytes(Klass::secondary_super_cache_offset());
 4779   Address secondary_supers_addr(sub_klass, ss_offset);
 4780   Address super_cache_addr(     sub_klass, sc_offset);
 4781 
 4782   // Do a linear scan of the secondary super-klass chain.
 4783   // This code is rarely used, so simplicity is a virtue here.
 4784   // The repne_scan instruction uses fixed registers, which we must spill.
 4785   // Don&#39;t worry too much about pre-existing connections with the input regs.
 4786 
 4787   assert(sub_klass != rax, &quot;killed reg&quot;); // killed by mov(rax, super)
 4788   assert(sub_klass != rcx, &quot;killed reg&quot;); // killed by lea(rcx, &amp;pst_counter)
 4789 
 4790   // Get super_klass value into rax (even if it was in rdi or rcx).
 4791   bool pushed_rax = false, pushed_rcx = false, pushed_rdi = false;
 4792   if (super_klass != rax || UseCompressedOops) {
 4793     if (!IS_A_TEMP(rax)) { push(rax); pushed_rax = true; }
 4794     mov(rax, super_klass);
 4795   }
 4796   if (!IS_A_TEMP(rcx)) { push(rcx); pushed_rcx = true; }
 4797   if (!IS_A_TEMP(rdi)) { push(rdi); pushed_rdi = true; }
 4798 
 4799 #ifndef PRODUCT
 4800   int* pst_counter = &amp;SharedRuntime::_partial_subtype_ctr;
 4801   ExternalAddress pst_counter_addr((address) pst_counter);
 4802   NOT_LP64(  incrementl(pst_counter_addr) );
 4803   LP64_ONLY( lea(rcx, pst_counter_addr) );
 4804   LP64_ONLY( incrementl(Address(rcx, 0)) );
 4805 #endif //PRODUCT
 4806 
 4807   // We will consult the secondary-super array.
 4808   movptr(rdi, secondary_supers_addr);
 4809   // Load the array length.  (Positive movl does right thing on LP64.)
 4810   movl(rcx, Address(rdi, Array&lt;Klass*&gt;::length_offset_in_bytes()));
 4811   // Skip to start of data.
 4812   addptr(rdi, Array&lt;Klass*&gt;::base_offset_in_bytes());
 4813 
 4814   // Scan RCX words at [RDI] for an occurrence of RAX.
 4815   // Set NZ/Z based on last compare.
 4816   // Z flag value will not be set by &#39;repne&#39; if RCX == 0 since &#39;repne&#39; does
 4817   // not change flags (only scas instruction which is repeated sets flags).
 4818   // Set Z = 0 (not equal) before &#39;repne&#39; to indicate that class was not found.
 4819 
 4820     testptr(rax,rax); // Set Z = 0
 4821     repne_scan();
 4822 
 4823   // Unspill the temp. registers:
 4824   if (pushed_rdi)  pop(rdi);
 4825   if (pushed_rcx)  pop(rcx);
 4826   if (pushed_rax)  pop(rax);
 4827 
 4828   if (set_cond_codes) {
 4829     // Special hack for the AD files:  rdi is guaranteed non-zero.
 4830     assert(!pushed_rdi, &quot;rdi must be left non-NULL&quot;);
 4831     // Also, the condition codes are properly set Z/NZ on succeed/failure.
 4832   }
 4833 
 4834   if (L_failure == &amp;L_fallthrough)
 4835         jccb(Assembler::notEqual, *L_failure);
 4836   else  jcc(Assembler::notEqual, *L_failure);
 4837 
 4838   // Success.  Cache the super we found and proceed in triumph.
 4839   movptr(super_cache_addr, super_klass);
 4840 
 4841   if (L_success != &amp;L_fallthrough) {
 4842     jmp(*L_success);
 4843   }
 4844 
 4845 #undef IS_A_TEMP
 4846 
 4847   bind(L_fallthrough);
 4848 }
 4849 
 4850 void MacroAssembler::clinit_barrier(Register klass, Register thread, Label* L_fast_path, Label* L_slow_path) {
 4851   assert(L_fast_path != NULL || L_slow_path != NULL, &quot;at least one is required&quot;);
 4852 
 4853   Label L_fallthrough;
 4854   if (L_fast_path == NULL) {
 4855     L_fast_path = &amp;L_fallthrough;
 4856   } else if (L_slow_path == NULL) {
 4857     L_slow_path = &amp;L_fallthrough;
 4858   }
 4859 
 4860   // Fast path check: class is fully initialized
 4861   cmpb(Address(klass, InstanceKlass::init_state_offset()), InstanceKlass::fully_initialized);
 4862   jcc(Assembler::equal, *L_fast_path);
 4863 
 4864   // Fast path check: current thread is initializer thread
 4865   cmpptr(thread, Address(klass, InstanceKlass::init_thread_offset()));
 4866   if (L_slow_path == &amp;L_fallthrough) {
 4867     jcc(Assembler::equal, *L_fast_path);
 4868     bind(*L_slow_path);
 4869   } else if (L_fast_path == &amp;L_fallthrough) {
 4870     jcc(Assembler::notEqual, *L_slow_path);
 4871     bind(*L_fast_path);
 4872   } else {
 4873     Unimplemented();
 4874   }
 4875 }
 4876 
 4877 void MacroAssembler::cmov32(Condition cc, Register dst, Address src) {
 4878   if (VM_Version::supports_cmov()) {
 4879     cmovl(cc, dst, src);
 4880   } else {
 4881     Label L;
 4882     jccb(negate_condition(cc), L);
 4883     movl(dst, src);
 4884     bind(L);
 4885   }
 4886 }
 4887 
 4888 void MacroAssembler::cmov32(Condition cc, Register dst, Register src) {
 4889   if (VM_Version::supports_cmov()) {
 4890     cmovl(cc, dst, src);
 4891   } else {
 4892     Label L;
 4893     jccb(negate_condition(cc), L);
 4894     movl(dst, src);
 4895     bind(L);
 4896   }
 4897 }
 4898 
 4899 void MacroAssembler::_verify_oop(Register reg, const char* s, const char* file, int line) {
 4900   if (!VerifyOops || VerifyAdapterSharing) {
 4901     // Below address of the code string confuses VerifyAdapterSharing
 4902     // because it may differ between otherwise equivalent adapters.
 4903     return;
 4904   }
 4905 
 4906   // Pass register number to verify_oop_subroutine
 4907   const char* b = NULL;
 4908   {
 4909     ResourceMark rm;
 4910     stringStream ss;
 4911     ss.print(&quot;verify_oop: %s: %s (%s:%d)&quot;, reg-&gt;name(), s, file, line);
 4912     b = code_string(ss.as_string());
 4913   }
 4914   BLOCK_COMMENT(&quot;verify_oop {&quot;);
 4915 #ifdef _LP64
 4916   push(rscratch1);                    // save r10, trashed by movptr()
 4917 #endif
 4918   push(rax);                          // save rax,
 4919   push(reg);                          // pass register argument
 4920   ExternalAddress buffer((address) b);
 4921   // avoid using pushptr, as it modifies scratch registers
 4922   // and our contract is not to modify anything
 4923   movptr(rax, buffer.addr());
 4924   push(rax);
 4925   // call indirectly to solve generation ordering problem
 4926   movptr(rax, ExternalAddress(StubRoutines::verify_oop_subroutine_entry_address()));
 4927   call(rax);
 4928   // Caller pops the arguments (oop, message) and restores rax, r10
 4929   BLOCK_COMMENT(&quot;} verify_oop&quot;);
 4930 }
 4931 
 4932 
 4933 RegisterOrConstant MacroAssembler::delayed_value_impl(intptr_t* delayed_value_addr,
 4934                                                       Register tmp,
 4935                                                       int offset) {
 4936   intptr_t value = *delayed_value_addr;
 4937   if (value != 0)
 4938     return RegisterOrConstant(value + offset);
 4939 
 4940   // load indirectly to solve generation ordering problem
 4941   movptr(tmp, ExternalAddress((address) delayed_value_addr));
 4942 
 4943 #ifdef ASSERT
 4944   { Label L;
 4945     testptr(tmp, tmp);
 4946     if (WizardMode) {
 4947       const char* buf = NULL;
 4948       {
 4949         ResourceMark rm;
 4950         stringStream ss;
 4951         ss.print(&quot;DelayedValue=&quot; INTPTR_FORMAT, delayed_value_addr[1]);
 4952         buf = code_string(ss.as_string());
 4953       }
 4954       jcc(Assembler::notZero, L);
 4955       STOP(buf);
 4956     } else {
 4957       jccb(Assembler::notZero, L);
 4958       hlt();
 4959     }
 4960     bind(L);
 4961   }
 4962 #endif
 4963 
 4964   if (offset != 0)
 4965     addptr(tmp, offset);
 4966 
 4967   return RegisterOrConstant(tmp);
 4968 }
 4969 
 4970 
 4971 Address MacroAssembler::argument_address(RegisterOrConstant arg_slot,
 4972                                          int extra_slot_offset) {
 4973   // cf. TemplateTable::prepare_invoke(), if (load_receiver).
 4974   int stackElementSize = Interpreter::stackElementSize;
 4975   int offset = Interpreter::expr_offset_in_bytes(extra_slot_offset+0);
 4976 #ifdef ASSERT
 4977   int offset1 = Interpreter::expr_offset_in_bytes(extra_slot_offset+1);
 4978   assert(offset1 - offset == stackElementSize, &quot;correct arithmetic&quot;);
 4979 #endif
 4980   Register             scale_reg    = noreg;
 4981   Address::ScaleFactor scale_factor = Address::no_scale;
 4982   if (arg_slot.is_constant()) {
 4983     offset += arg_slot.as_constant() * stackElementSize;
 4984   } else {
 4985     scale_reg    = arg_slot.as_register();
 4986     scale_factor = Address::times(stackElementSize);
 4987   }
 4988   offset += wordSize;           // return PC is on stack
 4989   return Address(rsp, scale_reg, scale_factor, offset);
 4990 }
 4991 
 4992 
 4993 void MacroAssembler::_verify_oop_addr(Address addr, const char* s, const char* file, int line) {
 4994   if (!VerifyOops || VerifyAdapterSharing) {
 4995     // Below address of the code string confuses VerifyAdapterSharing
 4996     // because it may differ between otherwise equivalent adapters.
 4997     return;
 4998   }
 4999 
 5000   // Address adjust(addr.base(), addr.index(), addr.scale(), addr.disp() + BytesPerWord);
 5001   // Pass register number to verify_oop_subroutine
 5002   const char* b = NULL;
 5003   {
 5004     ResourceMark rm;
 5005     stringStream ss;
 5006     ss.print(&quot;verify_oop_addr: %s (%s:%d)&quot;, s, file, line);
 5007     b = code_string(ss.as_string());
 5008   }
 5009 #ifdef _LP64
 5010   push(rscratch1);                    // save r10, trashed by movptr()
 5011 #endif
 5012   push(rax);                          // save rax,
 5013   // addr may contain rsp so we will have to adjust it based on the push
 5014   // we just did (and on 64 bit we do two pushes)
 5015   // NOTE: 64bit seemed to have had a bug in that it did movq(addr, rax); which
 5016   // stores rax into addr which is backwards of what was intended.
 5017   if (addr.uses(rsp)) {
 5018     lea(rax, addr);
 5019     pushptr(Address(rax, LP64_ONLY(2 *) BytesPerWord));
 5020   } else {
 5021     pushptr(addr);
 5022   }
 5023 
 5024   ExternalAddress buffer((address) b);
 5025   // pass msg argument
 5026   // avoid using pushptr, as it modifies scratch registers
 5027   // and our contract is not to modify anything
 5028   movptr(rax, buffer.addr());
 5029   push(rax);
 5030 
 5031   // call indirectly to solve generation ordering problem
 5032   movptr(rax, ExternalAddress(StubRoutines::verify_oop_subroutine_entry_address()));
 5033   call(rax);
 5034   // Caller pops the arguments (addr, message) and restores rax, r10.
 5035 }
 5036 
 5037 void MacroAssembler::verify_tlab() {
 5038 #ifdef ASSERT
 5039   if (UseTLAB &amp;&amp; VerifyOops) {
 5040     Label next, ok;
 5041     Register t1 = rsi;
 5042     Register thread_reg = NOT_LP64(rbx) LP64_ONLY(r15_thread);
 5043 
 5044     push(t1);
 5045     NOT_LP64(push(thread_reg));
 5046     NOT_LP64(get_thread(thread_reg));
 5047 
 5048     movptr(t1, Address(thread_reg, in_bytes(JavaThread::tlab_top_offset())));
 5049     cmpptr(t1, Address(thread_reg, in_bytes(JavaThread::tlab_start_offset())));
 5050     jcc(Assembler::aboveEqual, next);
 5051     STOP(&quot;assert(top &gt;= start)&quot;);
 5052     should_not_reach_here();
 5053 
 5054     bind(next);
 5055     movptr(t1, Address(thread_reg, in_bytes(JavaThread::tlab_end_offset())));
 5056     cmpptr(t1, Address(thread_reg, in_bytes(JavaThread::tlab_top_offset())));
 5057     jcc(Assembler::aboveEqual, ok);
 5058     STOP(&quot;assert(top &lt;= end)&quot;);
 5059     should_not_reach_here();
 5060 
 5061     bind(ok);
 5062     NOT_LP64(pop(thread_reg));
 5063     pop(t1);
 5064   }
 5065 #endif
 5066 }
 5067 
 5068 class ControlWord {
 5069  public:
 5070   int32_t _value;
 5071 
 5072   int  rounding_control() const        { return  (_value &gt;&gt; 10) &amp; 3      ; }
 5073   int  precision_control() const       { return  (_value &gt;&gt;  8) &amp; 3      ; }
 5074   bool precision() const               { return ((_value &gt;&gt;  5) &amp; 1) != 0; }
 5075   bool underflow() const               { return ((_value &gt;&gt;  4) &amp; 1) != 0; }
 5076   bool overflow() const                { return ((_value &gt;&gt;  3) &amp; 1) != 0; }
 5077   bool zero_divide() const             { return ((_value &gt;&gt;  2) &amp; 1) != 0; }
 5078   bool denormalized() const            { return ((_value &gt;&gt;  1) &amp; 1) != 0; }
 5079   bool invalid() const                 { return ((_value &gt;&gt;  0) &amp; 1) != 0; }
 5080 
 5081   void print() const {
 5082     // rounding control
 5083     const char* rc;
 5084     switch (rounding_control()) {
 5085       case 0: rc = &quot;round near&quot;; break;
 5086       case 1: rc = &quot;round down&quot;; break;
 5087       case 2: rc = &quot;round up  &quot;; break;
 5088       case 3: rc = &quot;chop      &quot;; break;
 5089     };
 5090     // precision control
 5091     const char* pc;
 5092     switch (precision_control()) {
 5093       case 0: pc = &quot;24 bits &quot;; break;
 5094       case 1: pc = &quot;reserved&quot;; break;
 5095       case 2: pc = &quot;53 bits &quot;; break;
 5096       case 3: pc = &quot;64 bits &quot;; break;
 5097     };
 5098     // flags
 5099     char f[9];
 5100     f[0] = &#39; &#39;;
 5101     f[1] = &#39; &#39;;
 5102     f[2] = (precision   ()) ? &#39;P&#39; : &#39;p&#39;;
 5103     f[3] = (underflow   ()) ? &#39;U&#39; : &#39;u&#39;;
 5104     f[4] = (overflow    ()) ? &#39;O&#39; : &#39;o&#39;;
 5105     f[5] = (zero_divide ()) ? &#39;Z&#39; : &#39;z&#39;;
 5106     f[6] = (denormalized()) ? &#39;D&#39; : &#39;d&#39;;
 5107     f[7] = (invalid     ()) ? &#39;I&#39; : &#39;i&#39;;
 5108     f[8] = &#39;\x0&#39;;
 5109     // output
 5110     printf(&quot;%04x  masks = %s, %s, %s&quot;, _value &amp; 0xFFFF, f, rc, pc);
 5111   }
 5112 
 5113 };
 5114 
 5115 class StatusWord {
 5116  public:
 5117   int32_t _value;
 5118 
 5119   bool busy() const                    { return ((_value &gt;&gt; 15) &amp; 1) != 0; }
 5120   bool C3() const                      { return ((_value &gt;&gt; 14) &amp; 1) != 0; }
 5121   bool C2() const                      { return ((_value &gt;&gt; 10) &amp; 1) != 0; }
 5122   bool C1() const                      { return ((_value &gt;&gt;  9) &amp; 1) != 0; }
 5123   bool C0() const                      { return ((_value &gt;&gt;  8) &amp; 1) != 0; }
 5124   int  top() const                     { return  (_value &gt;&gt; 11) &amp; 7      ; }
 5125   bool error_status() const            { return ((_value &gt;&gt;  7) &amp; 1) != 0; }
 5126   bool stack_fault() const             { return ((_value &gt;&gt;  6) &amp; 1) != 0; }
 5127   bool precision() const               { return ((_value &gt;&gt;  5) &amp; 1) != 0; }
 5128   bool underflow() const               { return ((_value &gt;&gt;  4) &amp; 1) != 0; }
 5129   bool overflow() const                { return ((_value &gt;&gt;  3) &amp; 1) != 0; }
 5130   bool zero_divide() const             { return ((_value &gt;&gt;  2) &amp; 1) != 0; }
 5131   bool denormalized() const            { return ((_value &gt;&gt;  1) &amp; 1) != 0; }
 5132   bool invalid() const                 { return ((_value &gt;&gt;  0) &amp; 1) != 0; }
 5133 
 5134   void print() const {
 5135     // condition codes
 5136     char c[5];
 5137     c[0] = (C3()) ? &#39;3&#39; : &#39;-&#39;;
 5138     c[1] = (C2()) ? &#39;2&#39; : &#39;-&#39;;
 5139     c[2] = (C1()) ? &#39;1&#39; : &#39;-&#39;;
 5140     c[3] = (C0()) ? &#39;0&#39; : &#39;-&#39;;
 5141     c[4] = &#39;\x0&#39;;
 5142     // flags
 5143     char f[9];
 5144     f[0] = (error_status()) ? &#39;E&#39; : &#39;-&#39;;
 5145     f[1] = (stack_fault ()) ? &#39;S&#39; : &#39;-&#39;;
 5146     f[2] = (precision   ()) ? &#39;P&#39; : &#39;-&#39;;
 5147     f[3] = (underflow   ()) ? &#39;U&#39; : &#39;-&#39;;
 5148     f[4] = (overflow    ()) ? &#39;O&#39; : &#39;-&#39;;
 5149     f[5] = (zero_divide ()) ? &#39;Z&#39; : &#39;-&#39;;
 5150     f[6] = (denormalized()) ? &#39;D&#39; : &#39;-&#39;;
 5151     f[7] = (invalid     ()) ? &#39;I&#39; : &#39;-&#39;;
 5152     f[8] = &#39;\x0&#39;;
 5153     // output
 5154     printf(&quot;%04x  flags = %s, cc =  %s, top = %d&quot;, _value &amp; 0xFFFF, f, c, top());
 5155   }
 5156 
 5157 };
 5158 
 5159 class TagWord {
 5160  public:
 5161   int32_t _value;
 5162 
 5163   int tag_at(int i) const              { return (_value &gt;&gt; (i*2)) &amp; 3; }
 5164 
 5165   void print() const {
 5166     printf(&quot;%04x&quot;, _value &amp; 0xFFFF);
 5167   }
 5168 
 5169 };
 5170 
 5171 class FPU_Register {
 5172  public:
 5173   int32_t _m0;
 5174   int32_t _m1;
 5175   int16_t _ex;
 5176 
 5177   bool is_indefinite() const           {
 5178     return _ex == -1 &amp;&amp; _m1 == (int32_t)0xC0000000 &amp;&amp; _m0 == 0;
 5179   }
 5180 
 5181   void print() const {
 5182     char  sign = (_ex &lt; 0) ? &#39;-&#39; : &#39;+&#39;;
 5183     const char* kind = (_ex == 0x7FFF || _ex == (int16_t)-1) ? &quot;NaN&quot; : &quot;   &quot;;
 5184     printf(&quot;%c%04hx.%08x%08x  %s&quot;, sign, _ex, _m1, _m0, kind);
 5185   };
 5186 
 5187 };
 5188 
 5189 class FPU_State {
 5190  public:
 5191   enum {
 5192     register_size       = 10,
 5193     number_of_registers =  8,
 5194     register_mask       =  7
 5195   };
 5196 
 5197   ControlWord  _control_word;
 5198   StatusWord   _status_word;
 5199   TagWord      _tag_word;
 5200   int32_t      _error_offset;
 5201   int32_t      _error_selector;
 5202   int32_t      _data_offset;
 5203   int32_t      _data_selector;
 5204   int8_t       _register[register_size * number_of_registers];
 5205 
 5206   int tag_for_st(int i) const          { return _tag_word.tag_at((_status_word.top() + i) &amp; register_mask); }
 5207   FPU_Register* st(int i) const        { return (FPU_Register*)&amp;_register[register_size * i]; }
 5208 
 5209   const char* tag_as_string(int tag) const {
 5210     switch (tag) {
 5211       case 0: return &quot;valid&quot;;
 5212       case 1: return &quot;zero&quot;;
 5213       case 2: return &quot;special&quot;;
 5214       case 3: return &quot;empty&quot;;
 5215     }
 5216     ShouldNotReachHere();
 5217     return NULL;
 5218   }
 5219 
 5220   void print() const {
 5221     // print computation registers
 5222     { int t = _status_word.top();
 5223       for (int i = 0; i &lt; number_of_registers; i++) {
 5224         int j = (i - t) &amp; register_mask;
 5225         printf(&quot;%c r%d = ST%d = &quot;, (j == 0 ? &#39;*&#39; : &#39; &#39;), i, j);
 5226         st(j)-&gt;print();
 5227         printf(&quot; %s\n&quot;, tag_as_string(_tag_word.tag_at(i)));
 5228       }
 5229     }
 5230     printf(&quot;\n&quot;);
 5231     // print control registers
 5232     printf(&quot;ctrl = &quot;); _control_word.print(); printf(&quot;\n&quot;);
 5233     printf(&quot;stat = &quot;); _status_word .print(); printf(&quot;\n&quot;);
 5234     printf(&quot;tags = &quot;); _tag_word    .print(); printf(&quot;\n&quot;);
 5235   }
 5236 
 5237 };
 5238 
 5239 class Flag_Register {
 5240  public:
 5241   int32_t _value;
 5242 
 5243   bool overflow() const                { return ((_value &gt;&gt; 11) &amp; 1) != 0; }
 5244   bool direction() const               { return ((_value &gt;&gt; 10) &amp; 1) != 0; }
 5245   bool sign() const                    { return ((_value &gt;&gt;  7) &amp; 1) != 0; }
 5246   bool zero() const                    { return ((_value &gt;&gt;  6) &amp; 1) != 0; }
 5247   bool auxiliary_carry() const         { return ((_value &gt;&gt;  4) &amp; 1) != 0; }
 5248   bool parity() const                  { return ((_value &gt;&gt;  2) &amp; 1) != 0; }
 5249   bool carry() const                   { return ((_value &gt;&gt;  0) &amp; 1) != 0; }
 5250 
 5251   void print() const {
 5252     // flags
 5253     char f[8];
 5254     f[0] = (overflow       ()) ? &#39;O&#39; : &#39;-&#39;;
 5255     f[1] = (direction      ()) ? &#39;D&#39; : &#39;-&#39;;
 5256     f[2] = (sign           ()) ? &#39;S&#39; : &#39;-&#39;;
 5257     f[3] = (zero           ()) ? &#39;Z&#39; : &#39;-&#39;;
 5258     f[4] = (auxiliary_carry()) ? &#39;A&#39; : &#39;-&#39;;
 5259     f[5] = (parity         ()) ? &#39;P&#39; : &#39;-&#39;;
 5260     f[6] = (carry          ()) ? &#39;C&#39; : &#39;-&#39;;
 5261     f[7] = &#39;\x0&#39;;
 5262     // output
 5263     printf(&quot;%08x  flags = %s&quot;, _value, f);
 5264   }
 5265 
 5266 };
 5267 
 5268 class IU_Register {
 5269  public:
 5270   int32_t _value;
 5271 
 5272   void print() const {
 5273     printf(&quot;%08x  %11d&quot;, _value, _value);
 5274   }
 5275 
 5276 };
 5277 
 5278 class IU_State {
 5279  public:
 5280   Flag_Register _eflags;
 5281   IU_Register   _rdi;
 5282   IU_Register   _rsi;
 5283   IU_Register   _rbp;
 5284   IU_Register   _rsp;
 5285   IU_Register   _rbx;
 5286   IU_Register   _rdx;
 5287   IU_Register   _rcx;
 5288   IU_Register   _rax;
 5289 
 5290   void print() const {
 5291     // computation registers
 5292     printf(&quot;rax,  = &quot;); _rax.print(); printf(&quot;\n&quot;);
 5293     printf(&quot;rbx,  = &quot;); _rbx.print(); printf(&quot;\n&quot;);
 5294     printf(&quot;rcx  = &quot;); _rcx.print(); printf(&quot;\n&quot;);
 5295     printf(&quot;rdx  = &quot;); _rdx.print(); printf(&quot;\n&quot;);
 5296     printf(&quot;rdi  = &quot;); _rdi.print(); printf(&quot;\n&quot;);
 5297     printf(&quot;rsi  = &quot;); _rsi.print(); printf(&quot;\n&quot;);
 5298     printf(&quot;rbp,  = &quot;); _rbp.print(); printf(&quot;\n&quot;);
 5299     printf(&quot;rsp  = &quot;); _rsp.print(); printf(&quot;\n&quot;);
 5300     printf(&quot;\n&quot;);
 5301     // control registers
 5302     printf(&quot;flgs = &quot;); _eflags.print(); printf(&quot;\n&quot;);
 5303   }
 5304 };
 5305 
 5306 
 5307 class CPU_State {
 5308  public:
 5309   FPU_State _fpu_state;
 5310   IU_State  _iu_state;
 5311 
 5312   void print() const {
 5313     printf(&quot;--------------------------------------------------\n&quot;);
 5314     _iu_state .print();
 5315     printf(&quot;\n&quot;);
 5316     _fpu_state.print();
 5317     printf(&quot;--------------------------------------------------\n&quot;);
 5318   }
 5319 
 5320 };
 5321 
 5322 
 5323 static void _print_CPU_state(CPU_State* state) {
 5324   state-&gt;print();
 5325 };
 5326 
 5327 
 5328 void MacroAssembler::print_CPU_state() {
 5329   push_CPU_state();
 5330   push(rsp);                // pass CPU state
 5331   call(RuntimeAddress(CAST_FROM_FN_PTR(address, _print_CPU_state)));
 5332   addptr(rsp, wordSize);       // discard argument
 5333   pop_CPU_state();
 5334 }
 5335 
 5336 
 5337 #ifndef _LP64
 5338 static bool _verify_FPU(int stack_depth, char* s, CPU_State* state) {
 5339   static int counter = 0;
 5340   FPU_State* fs = &amp;state-&gt;_fpu_state;
 5341   counter++;
 5342   // For leaf calls, only verify that the top few elements remain empty.
 5343   // We only need 1 empty at the top for C2 code.
 5344   if( stack_depth &lt; 0 ) {
 5345     if( fs-&gt;tag_for_st(7) != 3 ) {
 5346       printf(&quot;FPR7 not empty\n&quot;);
 5347       state-&gt;print();
 5348       assert(false, &quot;error&quot;);
 5349       return false;
 5350     }
 5351     return true;                // All other stack states do not matter
 5352   }
 5353 
 5354   assert((fs-&gt;_control_word._value &amp; 0xffff) == StubRoutines::_fpu_cntrl_wrd_std,
 5355          &quot;bad FPU control word&quot;);
 5356 
 5357   // compute stack depth
 5358   int i = 0;
 5359   while (i &lt; FPU_State::number_of_registers &amp;&amp; fs-&gt;tag_for_st(i)  &lt; 3) i++;
 5360   int d = i;
 5361   while (i &lt; FPU_State::number_of_registers &amp;&amp; fs-&gt;tag_for_st(i) == 3) i++;
 5362   // verify findings
 5363   if (i != FPU_State::number_of_registers) {
 5364     // stack not contiguous
 5365     printf(&quot;%s: stack not contiguous at ST%d\n&quot;, s, i);
 5366     state-&gt;print();
 5367     assert(false, &quot;error&quot;);
 5368     return false;
 5369   }
 5370   // check if computed stack depth corresponds to expected stack depth
 5371   if (stack_depth &lt; 0) {
 5372     // expected stack depth is -stack_depth or less
 5373     if (d &gt; -stack_depth) {
 5374       // too many elements on the stack
 5375       printf(&quot;%s: &lt;= %d stack elements expected but found %d\n&quot;, s, -stack_depth, d);
 5376       state-&gt;print();
 5377       assert(false, &quot;error&quot;);
 5378       return false;
 5379     }
 5380   } else {
 5381     // expected stack depth is stack_depth
 5382     if (d != stack_depth) {
 5383       // wrong stack depth
 5384       printf(&quot;%s: %d stack elements expected but found %d\n&quot;, s, stack_depth, d);
 5385       state-&gt;print();
 5386       assert(false, &quot;error&quot;);
 5387       return false;
 5388     }
 5389   }
 5390   // everything is cool
 5391   return true;
 5392 }
 5393 
 5394 void MacroAssembler::verify_FPU(int stack_depth, const char* s) {
 5395   if (!VerifyFPU) return;
 5396   push_CPU_state();
 5397   push(rsp);                // pass CPU state
 5398   ExternalAddress msg((address) s);
 5399   // pass message string s
 5400   pushptr(msg.addr());
 5401   push(stack_depth);        // pass stack depth
 5402   call(RuntimeAddress(CAST_FROM_FN_PTR(address, _verify_FPU)));
 5403   addptr(rsp, 3 * wordSize);   // discard arguments
 5404   // check for error
 5405   { Label L;
 5406     testl(rax, rax);
 5407     jcc(Assembler::notZero, L);
 5408     int3();                  // break if error condition
 5409     bind(L);
 5410   }
 5411   pop_CPU_state();
 5412 }
 5413 #endif // _LP64
 5414 
 5415 void MacroAssembler::restore_cpu_control_state_after_jni() {
 5416   // Either restore the MXCSR register after returning from the JNI Call
 5417   // or verify that it wasn&#39;t changed (with -Xcheck:jni flag).
 5418   if (VM_Version::supports_sse()) {
 5419     if (RestoreMXCSROnJNICalls) {
 5420       ldmxcsr(ExternalAddress(StubRoutines::addr_mxcsr_std()));
 5421     } else if (CheckJNICalls) {
 5422       call(RuntimeAddress(StubRoutines::x86::verify_mxcsr_entry()));
 5423     }
 5424   }
 5425   // Clear upper bits of YMM registers to avoid SSE &lt;-&gt; AVX transition penalty.
 5426   vzeroupper();
 5427   // Reset k1 to 0xffff.
 5428 
 5429 #ifdef COMPILER2
 5430   if (PostLoopMultiversioning &amp;&amp; VM_Version::supports_evex()) {
 5431     push(rcx);
 5432     movl(rcx, 0xffff);
 5433     kmovwl(k1, rcx);
 5434     pop(rcx);
 5435   }
 5436 #endif // COMPILER2
 5437 
 5438 #ifndef _LP64
 5439   // Either restore the x87 floating pointer control word after returning
 5440   // from the JNI call or verify that it wasn&#39;t changed.
 5441   if (CheckJNICalls) {
 5442     call(RuntimeAddress(StubRoutines::x86::verify_fpu_cntrl_wrd_entry()));
 5443   }
 5444 #endif // _LP64
 5445 }
 5446 
 5447 // ((OopHandle)result).resolve();
 5448 void MacroAssembler::resolve_oop_handle(Register result, Register tmp) {
 5449   assert_different_registers(result, tmp);
 5450 
 5451   // Only 64 bit platforms support GCs that require a tmp register
 5452   // Only IN_HEAP loads require a thread_tmp register
 5453   // OopHandle::resolve is an indirection like jobject.
 5454   access_load_at(T_OBJECT, IN_NATIVE,
 5455                  result, Address(result, 0), tmp, /*tmp_thread*/noreg);
 5456 }
 5457 
 5458 // ((WeakHandle)result).resolve();
 5459 void MacroAssembler::resolve_weak_handle(Register rresult, Register rtmp) {
 5460   assert_different_registers(rresult, rtmp);
 5461   Label resolved;
 5462 
 5463   // A null weak handle resolves to null.
 5464   cmpptr(rresult, 0);
 5465   jcc(Assembler::equal, resolved);
 5466 
 5467   // Only 64 bit platforms support GCs that require a tmp register
 5468   // Only IN_HEAP loads require a thread_tmp register
 5469   // WeakHandle::resolve is an indirection like jweak.
 5470   access_load_at(T_OBJECT, IN_NATIVE | ON_PHANTOM_OOP_REF,
 5471                  rresult, Address(rresult, 0), rtmp, /*tmp_thread*/noreg);
 5472   bind(resolved);
 5473 }
 5474 
 5475 void MacroAssembler::load_mirror(Register mirror, Register method, Register tmp) {
 5476   // get mirror
 5477   const int mirror_offset = in_bytes(Klass::java_mirror_offset());
 5478   load_method_holder(mirror, method);
 5479   movptr(mirror, Address(mirror, mirror_offset));
 5480   resolve_oop_handle(mirror, tmp);
 5481 }
 5482 
 5483 void MacroAssembler::load_method_holder_cld(Register rresult, Register rmethod) {
 5484   load_method_holder(rresult, rmethod);
 5485   movptr(rresult, Address(rresult, InstanceKlass::class_loader_data_offset()));
 5486 }
 5487 
 5488 void MacroAssembler::load_metadata(Register dst, Register src) {
 5489   if (UseCompressedClassPointers) {
 5490     movl(dst, Address(src, oopDesc::klass_offset_in_bytes()));
 5491   } else {
 5492     movptr(dst, Address(src, oopDesc::klass_offset_in_bytes()));
 5493   }
 5494 }
 5495 
 5496 void MacroAssembler::load_storage_props(Register dst, Register src) {
 5497   load_metadata(dst, src);
 5498   if (UseCompressedClassPointers) {
 5499     shrl(dst, oopDesc::narrow_storage_props_shift);
 5500   } else {
 5501     shrq(dst, oopDesc::wide_storage_props_shift);
 5502   }
 5503 }
 5504 
 5505 void MacroAssembler::load_method_holder(Register holder, Register method) {
 5506   movptr(holder, Address(method, Method::const_offset()));                      // ConstMethod*
 5507   movptr(holder, Address(holder, ConstMethod::constants_offset()));             // ConstantPool*
 5508   movptr(holder, Address(holder, ConstantPool::pool_holder_offset_in_bytes())); // InstanceKlass*
 5509 }
 5510 
 5511 void MacroAssembler::load_klass(Register dst, Register src) {
 5512   load_metadata(dst, src);
 5513 #ifdef _LP64
 5514   if (UseCompressedClassPointers) {
 5515     andl(dst, oopDesc::compressed_klass_mask());
 5516     decode_klass_not_null(dst);
 5517   } else
 5518 #endif
 5519   {
 5520 #ifdef _LP64
 5521     shlq(dst, oopDesc::storage_props_nof_bits);
 5522     shrq(dst, oopDesc::storage_props_nof_bits);
 5523 #else
 5524     andl(dst, oopDesc::wide_klass_mask());
 5525 #endif
 5526   }
 5527 }
 5528 
 5529 void MacroAssembler::load_prototype_header(Register dst, Register src) {
 5530   load_klass(dst, src);
 5531   movptr(dst, Address(dst, Klass::prototype_header_offset()));
 5532 }
 5533 
 5534 void MacroAssembler::store_klass(Register dst, Register src) {
 5535 #ifdef _LP64
 5536   if (UseCompressedClassPointers) {
 5537     encode_klass_not_null(src);
 5538     movl(Address(dst, oopDesc::klass_offset_in_bytes()), src);
 5539   } else
 5540 #endif
 5541     movptr(Address(dst, oopDesc::klass_offset_in_bytes()), src);
 5542 }
 5543 
 5544 void MacroAssembler::access_load_at(BasicType type, DecoratorSet decorators, Register dst, Address src,
 5545                                     Register tmp1, Register thread_tmp) {
 5546   BarrierSetAssembler* bs = BarrierSet::barrier_set()-&gt;barrier_set_assembler();
 5547   decorators = AccessInternal::decorator_fixup(decorators);
 5548   bool as_raw = (decorators &amp; AS_RAW) != 0;
 5549   if (as_raw) {
 5550     bs-&gt;BarrierSetAssembler::load_at(this, decorators, type, dst, src, tmp1, thread_tmp);
 5551   } else {
 5552     bs-&gt;load_at(this, decorators, type, dst, src, tmp1, thread_tmp);
 5553   }
 5554 }
 5555 
 5556 void MacroAssembler::access_store_at(BasicType type, DecoratorSet decorators, Address dst, Register src,
 5557                                      Register tmp1, Register tmp2, Register tmp3) {
 5558   BarrierSetAssembler* bs = BarrierSet::barrier_set()-&gt;barrier_set_assembler();
 5559   decorators = AccessInternal::decorator_fixup(decorators);
 5560   bool as_raw = (decorators &amp; AS_RAW) != 0;
 5561   if (as_raw) {
 5562     bs-&gt;BarrierSetAssembler::store_at(this, decorators, type, dst, src, tmp1, tmp2, tmp3);
 5563   } else {
 5564     bs-&gt;store_at(this, decorators, type, dst, src, tmp1, tmp2, tmp3);
 5565   }
 5566 }
 5567 
 5568 void MacroAssembler::access_value_copy(DecoratorSet decorators, Register src, Register dst,
 5569                                        Register value_klass) {
 5570   BarrierSetAssembler* bs = BarrierSet::barrier_set()-&gt;barrier_set_assembler();
 5571   bs-&gt;value_copy(this, decorators, src, dst, value_klass);
 5572 }
 5573 
 5574 void MacroAssembler::first_field_offset(Register value_klass, Register offset) {
 5575   movptr(offset, Address(value_klass, InstanceKlass::adr_valueklass_fixed_block_offset()));
 5576   movl(offset, Address(offset, ValueKlass::first_field_offset_offset()));
 5577 }
 5578 
 5579 void MacroAssembler::data_for_oop(Register oop, Register data, Register value_klass) {
 5580   // ((address) (void*) o) + vk-&gt;first_field_offset();
 5581   Register offset = (data == oop) ? rscratch1 : data;
 5582   first_field_offset(value_klass, offset);
 5583   if (data == oop) {
 5584     addptr(data, offset);
 5585   } else {
 5586     lea(data, Address(oop, offset));
 5587   }
 5588 }
 5589 
 5590 void MacroAssembler::data_for_value_array_index(Register array, Register array_klass,
 5591                                                 Register index, Register data) {
 5592   assert(index != rcx, &quot;index needs to shift by rcx&quot;);
 5593   assert_different_registers(array, array_klass, index);
 5594   assert_different_registers(rcx, array, index);
 5595 
 5596   // array-&gt;base() + (index &lt;&lt; Klass::layout_helper_log2_element_size(lh));
 5597   movl(rcx, Address(array_klass, Klass::layout_helper_offset()));
 5598 
 5599   // Klass::layout_helper_log2_element_size(lh)
 5600   // (lh &gt;&gt; _lh_log2_element_size_shift) &amp; _lh_log2_element_size_mask;
 5601   shrl(rcx, Klass::_lh_log2_element_size_shift);
 5602   andl(rcx, Klass::_lh_log2_element_size_mask);
 5603   shlptr(index); // index &lt;&lt; rcx
 5604 
 5605   lea(data, Address(array, index, Address::times_1, arrayOopDesc::base_offset_in_bytes(T_VALUETYPE)));
 5606 }
 5607 
 5608 void MacroAssembler::resolve(DecoratorSet decorators, Register obj) {
 5609   // Use stronger ACCESS_WRITE|ACCESS_READ by default.
 5610   if ((decorators &amp; (ACCESS_READ | ACCESS_WRITE)) == 0) {
 5611     decorators |= ACCESS_READ | ACCESS_WRITE;
 5612   }
 5613   BarrierSetAssembler* bs = BarrierSet::barrier_set()-&gt;barrier_set_assembler();
 5614   return bs-&gt;resolve(this, decorators, obj);
 5615 }
 5616 
 5617 void MacroAssembler::load_heap_oop(Register dst, Address src, Register tmp1,
 5618                                    Register thread_tmp, DecoratorSet decorators) {
 5619   access_load_at(T_OBJECT, IN_HEAP | decorators, dst, src, tmp1, thread_tmp);
 5620 }
 5621 
 5622 // Doesn&#39;t do verfication, generates fixed size code
 5623 void MacroAssembler::load_heap_oop_not_null(Register dst, Address src, Register tmp1,
 5624                                             Register thread_tmp, DecoratorSet decorators) {
 5625   access_load_at(T_OBJECT, IN_HEAP | IS_NOT_NULL | decorators, dst, src, tmp1, thread_tmp);
 5626 }
 5627 
 5628 void MacroAssembler::store_heap_oop(Address dst, Register src, Register tmp1,
 5629                                     Register tmp2, Register tmp3, DecoratorSet decorators) {
 5630   access_store_at(T_OBJECT, IN_HEAP | decorators, dst, src, tmp1, tmp2, tmp3);
 5631 }
 5632 
 5633 // Used for storing NULLs.
 5634 void MacroAssembler::store_heap_oop_null(Address dst) {
 5635   access_store_at(T_OBJECT, IN_HEAP, dst, noreg, noreg, noreg, noreg);
 5636 }
 5637 
 5638 #ifdef _LP64
 5639 void MacroAssembler::store_klass_gap(Register dst, Register src) {
 5640   if (UseCompressedClassPointers) {
 5641     // Store to klass gap in destination
 5642     movl(Address(dst, oopDesc::klass_gap_offset_in_bytes()), src);
 5643   }
 5644 }
 5645 
 5646 #ifdef ASSERT
 5647 void MacroAssembler::verify_heapbase(const char* msg) {
 5648   assert (UseCompressedOops, &quot;should be compressed&quot;);
 5649   assert (Universe::heap() != NULL, &quot;java heap should be initialized&quot;);
 5650   if (CheckCompressedOops) {
 5651     Label ok;
 5652     push(rscratch1); // cmpptr trashes rscratch1
 5653     cmpptr(r12_heapbase, ExternalAddress((address)CompressedOops::ptrs_base_addr()));
 5654     jcc(Assembler::equal, ok);
 5655     STOP(msg);
 5656     bind(ok);
 5657     pop(rscratch1);
 5658   }
 5659 }
 5660 #endif
 5661 
 5662 // Algorithm must match oop.inline.hpp encode_heap_oop.
 5663 void MacroAssembler::encode_heap_oop(Register r) {
 5664 #ifdef ASSERT
 5665   verify_heapbase(&quot;MacroAssembler::encode_heap_oop: heap base corrupted?&quot;);
 5666 #endif
 5667   verify_oop_msg(r, &quot;broken oop in encode_heap_oop&quot;);
 5668   if (CompressedOops::base() == NULL) {
 5669     if (CompressedOops::shift() != 0) {
 5670       assert (LogMinObjAlignmentInBytes == CompressedOops::shift(), &quot;decode alg wrong&quot;);
 5671       shrq(r, LogMinObjAlignmentInBytes);
 5672     }
 5673     return;
 5674   }
 5675   testq(r, r);
 5676   cmovq(Assembler::equal, r, r12_heapbase);
 5677   subq(r, r12_heapbase);
 5678   shrq(r, LogMinObjAlignmentInBytes);
 5679 }
 5680 
 5681 void MacroAssembler::encode_heap_oop_not_null(Register r) {
 5682 #ifdef ASSERT
 5683   verify_heapbase(&quot;MacroAssembler::encode_heap_oop_not_null: heap base corrupted?&quot;);
 5684   if (CheckCompressedOops) {
 5685     Label ok;
 5686     testq(r, r);
 5687     jcc(Assembler::notEqual, ok);
 5688     STOP(&quot;null oop passed to encode_heap_oop_not_null&quot;);
 5689     bind(ok);
 5690   }
 5691 #endif
 5692   verify_oop_msg(r, &quot;broken oop in encode_heap_oop_not_null&quot;);
 5693   if (CompressedOops::base() != NULL) {
 5694     subq(r, r12_heapbase);
 5695   }
 5696   if (CompressedOops::shift() != 0) {
 5697     assert (LogMinObjAlignmentInBytes == CompressedOops::shift(), &quot;decode alg wrong&quot;);
 5698     shrq(r, LogMinObjAlignmentInBytes);
 5699   }
 5700 }
 5701 
 5702 void MacroAssembler::encode_heap_oop_not_null(Register dst, Register src) {
 5703 #ifdef ASSERT
 5704   verify_heapbase(&quot;MacroAssembler::encode_heap_oop_not_null2: heap base corrupted?&quot;);
 5705   if (CheckCompressedOops) {
 5706     Label ok;
 5707     testq(src, src);
 5708     jcc(Assembler::notEqual, ok);
 5709     STOP(&quot;null oop passed to encode_heap_oop_not_null2&quot;);
 5710     bind(ok);
 5711   }
 5712 #endif
 5713   verify_oop_msg(src, &quot;broken oop in encode_heap_oop_not_null2&quot;);
 5714   if (dst != src) {
 5715     movq(dst, src);
 5716   }
 5717   if (CompressedOops::base() != NULL) {
 5718     subq(dst, r12_heapbase);
 5719   }
 5720   if (CompressedOops::shift() != 0) {
 5721     assert (LogMinObjAlignmentInBytes == CompressedOops::shift(), &quot;decode alg wrong&quot;);
 5722     shrq(dst, LogMinObjAlignmentInBytes);
 5723   }
 5724 }
 5725 
 5726 void  MacroAssembler::decode_heap_oop(Register r) {
 5727 #ifdef ASSERT
 5728   verify_heapbase(&quot;MacroAssembler::decode_heap_oop: heap base corrupted?&quot;);
 5729 #endif
 5730   if (CompressedOops::base() == NULL) {
 5731     if (CompressedOops::shift() != 0) {
 5732       assert (LogMinObjAlignmentInBytes == CompressedOops::shift(), &quot;decode alg wrong&quot;);
 5733       shlq(r, LogMinObjAlignmentInBytes);
 5734     }
 5735   } else {
 5736     Label done;
 5737     shlq(r, LogMinObjAlignmentInBytes);
 5738     jccb(Assembler::equal, done);
 5739     addq(r, r12_heapbase);
 5740     bind(done);
 5741   }
 5742   verify_oop_msg(r, &quot;broken oop in decode_heap_oop&quot;);
 5743 }
 5744 
 5745 void  MacroAssembler::decode_heap_oop_not_null(Register r) {
 5746   // Note: it will change flags
 5747   assert (UseCompressedOops, &quot;should only be used for compressed headers&quot;);
 5748   assert (Universe::heap() != NULL, &quot;java heap should be initialized&quot;);
 5749   // Cannot assert, unverified entry point counts instructions (see .ad file)
 5750   // vtableStubs also counts instructions in pd_code_size_limit.
 5751   // Also do not verify_oop as this is called by verify_oop.
 5752   if (CompressedOops::shift() != 0) {
 5753     assert(LogMinObjAlignmentInBytes == CompressedOops::shift(), &quot;decode alg wrong&quot;);
 5754     shlq(r, LogMinObjAlignmentInBytes);
 5755     if (CompressedOops::base() != NULL) {
 5756       addq(r, r12_heapbase);
 5757     }
 5758   } else {
 5759     assert (CompressedOops::base() == NULL, &quot;sanity&quot;);
 5760   }
 5761 }
 5762 
 5763 void  MacroAssembler::decode_heap_oop_not_null(Register dst, Register src) {
 5764   // Note: it will change flags
 5765   assert (UseCompressedOops, &quot;should only be used for compressed headers&quot;);
 5766   assert (Universe::heap() != NULL, &quot;java heap should be initialized&quot;);
 5767   // Cannot assert, unverified entry point counts instructions (see .ad file)
 5768   // vtableStubs also counts instructions in pd_code_size_limit.
 5769   // Also do not verify_oop as this is called by verify_oop.
 5770   if (CompressedOops::shift() != 0) {
 5771     assert(LogMinObjAlignmentInBytes == CompressedOops::shift(), &quot;decode alg wrong&quot;);
 5772     if (LogMinObjAlignmentInBytes == Address::times_8) {
 5773       leaq(dst, Address(r12_heapbase, src, Address::times_8, 0));
 5774     } else {
 5775       if (dst != src) {
 5776         movq(dst, src);
 5777       }
 5778       shlq(dst, LogMinObjAlignmentInBytes);
 5779       if (CompressedOops::base() != NULL) {
 5780         addq(dst, r12_heapbase);
 5781       }
 5782     }
 5783   } else {
 5784     assert (CompressedOops::base() == NULL, &quot;sanity&quot;);
 5785     if (dst != src) {
 5786       movq(dst, src);
 5787     }
 5788   }
 5789 }
 5790 
 5791 void MacroAssembler::encode_klass_not_null(Register r) {
 5792   if (CompressedKlassPointers::base() != NULL) {
 5793     // Use r12 as a scratch register in which to temporarily load the narrow_klass_base.
 5794     assert(r != r12_heapbase, &quot;Encoding a klass in r12&quot;);
 5795     mov64(r12_heapbase, (int64_t)CompressedKlassPointers::base());
 5796     subq(r, r12_heapbase);
 5797   }
 5798   if (CompressedKlassPointers::shift() != 0) {
 5799     assert (LogKlassAlignmentInBytes == CompressedKlassPointers::shift(), &quot;decode alg wrong&quot;);
 5800     shrq(r, LogKlassAlignmentInBytes);
 5801   }
 5802   if (CompressedKlassPointers::base() != NULL) {
 5803     reinit_heapbase();
 5804   }
 5805 }
 5806 
 5807 void MacroAssembler::encode_klass_not_null(Register dst, Register src) {
 5808   if (dst == src) {
 5809     encode_klass_not_null(src);
 5810   } else {
 5811     if (CompressedKlassPointers::base() != NULL) {
 5812       mov64(dst, (int64_t)CompressedKlassPointers::base());
 5813       negq(dst);
 5814       addq(dst, src);
 5815     } else {
 5816       movptr(dst, src);
 5817     }
 5818     if (CompressedKlassPointers::shift() != 0) {
 5819       assert (LogKlassAlignmentInBytes == CompressedKlassPointers::shift(), &quot;decode alg wrong&quot;);
 5820       shrq(dst, LogKlassAlignmentInBytes);
 5821     }
 5822   }
 5823 }
 5824 
 5825 // Function instr_size_for_decode_klass_not_null() counts the instructions
 5826 // generated by decode_klass_not_null(register r) and reinit_heapbase(),
 5827 // when (Universe::heap() != NULL).  Hence, if the instructions they
 5828 // generate change, then this method needs to be updated.
 5829 int MacroAssembler::instr_size_for_decode_klass_not_null() {
 5830   assert (UseCompressedClassPointers, &quot;only for compressed klass ptrs&quot;);
 5831   if (CompressedKlassPointers::base() != NULL) {
 5832     // mov64 + addq + shlq? + mov64  (for reinit_heapbase()).
 5833     return (CompressedKlassPointers::shift() == 0 ? 20 : 24);
 5834   } else {
 5835     // longest load decode klass function, mov64, leaq
 5836     return 16;
 5837   }
 5838 }
 5839 
 5840 // !!! If the instructions that get generated here change then function
 5841 // instr_size_for_decode_klass_not_null() needs to get updated.
 5842 void  MacroAssembler::decode_klass_not_null(Register r) {
 5843   // Note: it will change flags
 5844   assert (UseCompressedClassPointers, &quot;should only be used for compressed headers&quot;);
 5845   assert(r != r12_heapbase, &quot;Decoding a klass in r12&quot;);
 5846   // Cannot assert, unverified entry point counts instructions (see .ad file)
 5847   // vtableStubs also counts instructions in pd_code_size_limit.
 5848   // Also do not verify_oop as this is called by verify_oop.
 5849   if (CompressedKlassPointers::shift() != 0) {
 5850     assert(LogKlassAlignmentInBytes == CompressedKlassPointers::shift(), &quot;decode alg wrong&quot;);
 5851     shlq(r, LogKlassAlignmentInBytes);
 5852   }
 5853   // Use r12 as a scratch register in which to temporarily load the narrow_klass_base.
 5854   if (CompressedKlassPointers::base() != NULL) {
 5855     mov64(r12_heapbase, (int64_t)CompressedKlassPointers::base());
 5856     addq(r, r12_heapbase);
 5857     reinit_heapbase();
 5858   }
 5859 }
 5860 
 5861 void  MacroAssembler::decode_klass_not_null(Register dst, Register src) {
 5862   // Note: it will change flags
 5863   assert (UseCompressedClassPointers, &quot;should only be used for compressed headers&quot;);
 5864   if (dst == src) {
 5865     decode_klass_not_null(dst);
 5866   } else {
 5867     // Cannot assert, unverified entry point counts instructions (see .ad file)
 5868     // vtableStubs also counts instructions in pd_code_size_limit.
 5869     // Also do not verify_oop as this is called by verify_oop.
 5870     mov64(dst, (int64_t)CompressedKlassPointers::base());
 5871     if (CompressedKlassPointers::shift() != 0) {
 5872       assert(LogKlassAlignmentInBytes == CompressedKlassPointers::shift(), &quot;decode alg wrong&quot;);
 5873       assert(LogKlassAlignmentInBytes == Address::times_8, &quot;klass not aligned on 64bits?&quot;);
 5874       leaq(dst, Address(dst, src, Address::times_8, 0));
 5875     } else {
 5876       addq(dst, src);
 5877     }
 5878   }
 5879 }
 5880 
 5881 void  MacroAssembler::set_narrow_oop(Register dst, jobject obj) {
 5882   assert (UseCompressedOops, &quot;should only be used for compressed headers&quot;);
 5883   assert (Universe::heap() != NULL, &quot;java heap should be initialized&quot;);
 5884   assert (oop_recorder() != NULL, &quot;this assembler needs an OopRecorder&quot;);
 5885   int oop_index = oop_recorder()-&gt;find_index(obj);
 5886   RelocationHolder rspec = oop_Relocation::spec(oop_index);
 5887   mov_narrow_oop(dst, oop_index, rspec);
 5888 }
 5889 
 5890 void  MacroAssembler::set_narrow_oop(Address dst, jobject obj) {
 5891   assert (UseCompressedOops, &quot;should only be used for compressed headers&quot;);
 5892   assert (Universe::heap() != NULL, &quot;java heap should be initialized&quot;);
 5893   assert (oop_recorder() != NULL, &quot;this assembler needs an OopRecorder&quot;);
 5894   int oop_index = oop_recorder()-&gt;find_index(obj);
 5895   RelocationHolder rspec = oop_Relocation::spec(oop_index);
 5896   mov_narrow_oop(dst, oop_index, rspec);
 5897 }
 5898 
 5899 void  MacroAssembler::set_narrow_klass(Register dst, Klass* k) {
 5900   assert (UseCompressedClassPointers, &quot;should only be used for compressed headers&quot;);
 5901   assert (oop_recorder() != NULL, &quot;this assembler needs an OopRecorder&quot;);
 5902   int klass_index = oop_recorder()-&gt;find_index(k);
 5903   RelocationHolder rspec = metadata_Relocation::spec(klass_index);
 5904   mov_narrow_oop(dst, CompressedKlassPointers::encode(k), rspec);
 5905 }
 5906 
 5907 void  MacroAssembler::set_narrow_klass(Address dst, Klass* k) {
 5908   assert (UseCompressedClassPointers, &quot;should only be used for compressed headers&quot;);
 5909   assert (oop_recorder() != NULL, &quot;this assembler needs an OopRecorder&quot;);
 5910   int klass_index = oop_recorder()-&gt;find_index(k);
 5911   RelocationHolder rspec = metadata_Relocation::spec(klass_index);
 5912   mov_narrow_oop(dst, CompressedKlassPointers::encode(k), rspec);
 5913 }
 5914 
 5915 void  MacroAssembler::cmp_narrow_oop(Register dst, jobject obj) {
 5916   assert (UseCompressedOops, &quot;should only be used for compressed headers&quot;);
 5917   assert (Universe::heap() != NULL, &quot;java heap should be initialized&quot;);
 5918   assert (oop_recorder() != NULL, &quot;this assembler needs an OopRecorder&quot;);
 5919   int oop_index = oop_recorder()-&gt;find_index(obj);
 5920   RelocationHolder rspec = oop_Relocation::spec(oop_index);
 5921   Assembler::cmp_narrow_oop(dst, oop_index, rspec);
 5922 }
 5923 
 5924 void  MacroAssembler::cmp_narrow_oop(Address dst, jobject obj) {
 5925   assert (UseCompressedOops, &quot;should only be used for compressed headers&quot;);
 5926   assert (Universe::heap() != NULL, &quot;java heap should be initialized&quot;);
 5927   assert (oop_recorder() != NULL, &quot;this assembler needs an OopRecorder&quot;);
 5928   int oop_index = oop_recorder()-&gt;find_index(obj);
 5929   RelocationHolder rspec = oop_Relocation::spec(oop_index);
 5930   Assembler::cmp_narrow_oop(dst, oop_index, rspec);
 5931 }
 5932 
 5933 void  MacroAssembler::cmp_narrow_klass(Register dst, Klass* k) {
 5934   assert (UseCompressedClassPointers, &quot;should only be used for compressed headers&quot;);
 5935   assert (oop_recorder() != NULL, &quot;this assembler needs an OopRecorder&quot;);
 5936   int klass_index = oop_recorder()-&gt;find_index(k);
 5937   RelocationHolder rspec = metadata_Relocation::spec(klass_index);
 5938   Assembler::cmp_narrow_oop(dst, CompressedKlassPointers::encode(k), rspec);
 5939 }
 5940 
 5941 void  MacroAssembler::cmp_narrow_klass(Address dst, Klass* k) {
 5942   assert (UseCompressedClassPointers, &quot;should only be used for compressed headers&quot;);
 5943   assert (oop_recorder() != NULL, &quot;this assembler needs an OopRecorder&quot;);
 5944   int klass_index = oop_recorder()-&gt;find_index(k);
 5945   RelocationHolder rspec = metadata_Relocation::spec(klass_index);
 5946   Assembler::cmp_narrow_oop(dst, CompressedKlassPointers::encode(k), rspec);
 5947 }
 5948 
 5949 void MacroAssembler::reinit_heapbase() {
 5950   if (UseCompressedOops || UseCompressedClassPointers) {
 5951     if (Universe::heap() != NULL) {
 5952       if (CompressedOops::base() == NULL) {
 5953         MacroAssembler::xorptr(r12_heapbase, r12_heapbase);
 5954       } else {
 5955         mov64(r12_heapbase, (int64_t)CompressedOops::ptrs_base());
 5956       }
 5957     } else {
 5958       movptr(r12_heapbase, ExternalAddress((address)CompressedOops::ptrs_base_addr()));
 5959     }
 5960   }
 5961 }
 5962 
 5963 #endif // _LP64
 5964 
 5965 // C2 compiled method&#39;s prolog code.
 5966 void MacroAssembler::verified_entry(Compile* C, int sp_inc) {
 5967   int framesize = C-&gt;frame_size_in_bytes();
 5968   int bangsize = C-&gt;bang_size_in_bytes();
 5969   bool fp_mode_24b = false;
 5970   int stack_bang_size = C-&gt;need_stack_bang(bangsize) ? bangsize : 0;
 5971 
 5972   // WARNING: Initial instruction MUST be 5 bytes or longer so that
 5973   // NativeJump::patch_verified_entry will be able to patch out the entry
 5974   // code safely. The push to verify stack depth is ok at 5 bytes,
 5975   // the frame allocation can be either 3 or 6 bytes. So if we don&#39;t do
 5976   // stack bang then we must use the 6 byte frame allocation even if
 5977   // we have no frame. :-(
 5978   assert(stack_bang_size &gt;= framesize || stack_bang_size &lt;= 0, &quot;stack bang size incorrect&quot;);
 5979 
 5980   assert((framesize &amp; (StackAlignmentInBytes-1)) == 0, &quot;frame size not aligned&quot;);
 5981   // Remove word for return addr
 5982   framesize -= wordSize;
 5983   stack_bang_size -= wordSize;
 5984 
 5985   // Calls to C2R adapters often do not accept exceptional returns.
 5986   // We require that their callers must bang for them.  But be careful, because
 5987   // some VM calls (such as call site linkage) can use several kilobytes of
 5988   // stack.  But the stack safety zone should account for that.
 5989   // See bugs 4446381, 4468289, 4497237.
 5990   if (stack_bang_size &gt; 0) {
 5991     generate_stack_overflow_check(stack_bang_size);
 5992 
 5993     // We always push rbp, so that on return to interpreter rbp, will be
 5994     // restored correctly and we can correct the stack.
 5995     push(rbp);
 5996     // Save caller&#39;s stack pointer into RBP if the frame pointer is preserved.
 5997     if (PreserveFramePointer) {
 5998       mov(rbp, rsp);
 5999     }
 6000     // Remove word for ebp
 6001     framesize -= wordSize;
 6002 
 6003     // Create frame
 6004     if (framesize) {
 6005       subptr(rsp, framesize);
 6006     }
 6007   } else {
 6008     // Create frame (force generation of a 4 byte immediate value)
 6009     subptr_imm32(rsp, framesize);
 6010 
 6011     // Save RBP register now.
 6012     framesize -= wordSize;
 6013     movptr(Address(rsp, framesize), rbp);
 6014     // Save caller&#39;s stack pointer into RBP if the frame pointer is preserved.
 6015     if (PreserveFramePointer) {
 6016       movptr(rbp, rsp);
 6017       if (framesize &gt; 0) {
 6018         addptr(rbp, framesize);
 6019       }
 6020     }
 6021   }
 6022 
 6023   if (C-&gt;needs_stack_repair()) {
 6024     // Save stack increment (also account for fixed framesize and rbp)
 6025     assert((sp_inc &amp; (StackAlignmentInBytes-1)) == 0, &quot;stack increment not aligned&quot;);
 6026     movptr(Address(rsp, C-&gt;sp_inc_offset()), sp_inc + framesize + wordSize);
 6027   }
 6028 
 6029   if (VerifyStackAtCalls) { // Majik cookie to verify stack depth
 6030     framesize -= wordSize;
 6031     movptr(Address(rsp, framesize), (int32_t)0xbadb100d);
 6032   }
 6033 
 6034 #ifndef _LP64
 6035   // If method sets FPU control word do it now
 6036   if (fp_mode_24b) {
 6037     fldcw(ExternalAddress(StubRoutines::addr_fpu_cntrl_wrd_24()));
 6038   }
 6039   if (UseSSE &gt;= 2 &amp;&amp; VerifyFPU) {
 6040     verify_FPU(0, &quot;FPU stack must be clean on entry&quot;);
 6041   }
 6042 #endif
 6043 
 6044 #ifdef ASSERT
 6045   if (VerifyStackAtCalls) {
 6046     Label L;
 6047     push(rax);
 6048     mov(rax, rsp);
 6049     andptr(rax, StackAlignmentInBytes-1);
 6050     cmpptr(rax, StackAlignmentInBytes-wordSize);
 6051     pop(rax);
 6052     jcc(Assembler::equal, L);
 6053     STOP(&quot;Stack is not properly aligned!&quot;);
 6054     bind(L);
 6055   }
 6056 #endif
 6057 }
 6058 
 6059 // clear memory of size &#39;cnt&#39; qwords, starting at &#39;base&#39; using XMM/YMM registers
 6060 void MacroAssembler::xmm_clear_mem(Register base, Register cnt, Register val, XMMRegister xtmp) {
 6061   // cnt - number of qwords (8-byte words).
 6062   // base - start address, qword aligned.
 6063   Label L_zero_64_bytes, L_loop, L_sloop, L_tail, L_end;
 6064   movdq(xtmp, val);
 6065   if (UseAVX &gt;= 2) {
 6066     punpcklqdq(xtmp, xtmp);
 6067     vinserti128_high(xtmp, xtmp);
 6068   } else {
 6069     punpcklqdq(xtmp, xtmp);
 6070   }
 6071   jmp(L_zero_64_bytes);
 6072 
 6073   BIND(L_loop);
 6074   if (UseAVX &gt;= 2) {
 6075     vmovdqu(Address(base,  0), xtmp);
 6076     vmovdqu(Address(base, 32), xtmp);
 6077   } else {
 6078     movdqu(Address(base,  0), xtmp);
 6079     movdqu(Address(base, 16), xtmp);
 6080     movdqu(Address(base, 32), xtmp);
 6081     movdqu(Address(base, 48), xtmp);
 6082   }
 6083   addptr(base, 64);
 6084 
 6085   BIND(L_zero_64_bytes);
 6086   subptr(cnt, 8);
 6087   jccb(Assembler::greaterEqual, L_loop);
 6088   addptr(cnt, 4);
 6089   jccb(Assembler::less, L_tail);
 6090   // Copy trailing 32 bytes
 6091   if (UseAVX &gt;= 2) {
 6092     vmovdqu(Address(base, 0), xtmp);
 6093   } else {
 6094     movdqu(Address(base,  0), xtmp);
 6095     movdqu(Address(base, 16), xtmp);
 6096   }
 6097   addptr(base, 32);
 6098   subptr(cnt, 4);
 6099 
 6100   BIND(L_tail);
 6101   addptr(cnt, 4);
 6102   jccb(Assembler::lessEqual, L_end);
 6103   decrement(cnt);
 6104 
 6105   BIND(L_sloop);
 6106   movq(Address(base, 0), xtmp);
 6107   addptr(base, 8);
 6108   decrement(cnt);
 6109   jccb(Assembler::greaterEqual, L_sloop);
 6110   BIND(L_end);
 6111 }
 6112 
 6113 int MacroAssembler::store_value_type_fields_to_buf(ciValueKlass* vk, bool from_interpreter) {
 6114   // A value type might be returned. If fields are in registers we
 6115   // need to allocate a value type instance and initialize it with
 6116   // the value of the fields.
 6117   Label skip;
 6118   // We only need a new buffered value if a new one is not returned
 6119   testptr(rax, 1);
 6120   jcc(Assembler::zero, skip);
 6121   int call_offset = -1;
 6122 
 6123 #ifdef _LP64
 6124   Label slow_case;
 6125 
 6126   // Try to allocate a new buffered value (from the heap)
 6127   if (UseTLAB) {
 6128     // FIXME -- for smaller code, the inline allocation (and the slow case) should be moved inside the pack handler.
 6129     if (vk != NULL) {
 6130       // Called from C1, where the return type is statically known.
 6131       movptr(rbx, (intptr_t)vk-&gt;get_ValueKlass());
 6132       jint lh = vk-&gt;layout_helper();
 6133       assert(lh != Klass::_lh_neutral_value, &quot;inline class in return type must have been resolved&quot;);
 6134       movl(r14, lh);
 6135     } else {
 6136       // Call from interpreter. RAX contains ((the ValueKlass* of the return type) | 0x01)
 6137       mov(rbx, rax);
 6138       andptr(rbx, -2);
 6139       movl(r14, Address(rbx, Klass::layout_helper_offset()));
 6140     }
 6141 
 6142     movptr(r13, Address(r15_thread, in_bytes(JavaThread::tlab_top_offset())));
 6143     lea(r14, Address(r13, r14, Address::times_1));
 6144     cmpptr(r14, Address(r15_thread, in_bytes(JavaThread::tlab_end_offset())));
 6145     jcc(Assembler::above, slow_case);
 6146     movptr(Address(r15_thread, in_bytes(JavaThread::tlab_top_offset())), r14);
 6147     movptr(Address(r13, oopDesc::mark_offset_in_bytes()), (intptr_t)markWord::always_locked_prototype().value());
 6148 
 6149     xorl(rax, rax); // use zero reg to clear memory (shorter code)
 6150     store_klass_gap(r13, rax);  // zero klass gap for compressed oops
 6151 
 6152     if (vk == NULL) {
 6153       // store_klass corrupts rbx, so save it in rax for later use (interpreter case only).
 6154       mov(rax, rbx);
 6155     }
 6156     store_klass(r13, rbx);  // klass
 6157 
 6158     // We have our new buffered value, initialize its fields with a
 6159     // value class specific handler
 6160     if (vk != NULL) {
 6161       // FIXME -- do the packing in-line to avoid the runtime call
 6162       mov(rax, r13);
 6163       call(RuntimeAddress(vk-&gt;pack_handler())); // no need for call info as this will not safepoint.
 6164     } else {
 6165       movptr(rbx, Address(rax, InstanceKlass::adr_valueklass_fixed_block_offset()));
 6166       movptr(rbx, Address(rbx, ValueKlass::pack_handler_offset()));
 6167       mov(rax, r13);
 6168       call(rbx);
 6169     }
 6170     jmp(skip);
 6171   }
 6172 
 6173   bind(slow_case);
 6174   // We failed to allocate a new value, fall back to a runtime
 6175   // call. Some oop field may be live in some registers but we can&#39;t
 6176   // tell. That runtime call will take care of preserving them
 6177   // across a GC if there&#39;s one.
 6178 #endif
 6179 
 6180   if (from_interpreter) {
 6181     super_call_VM_leaf(StubRoutines::store_value_type_fields_to_buf());
 6182   } else {
 6183     call(RuntimeAddress(StubRoutines::store_value_type_fields_to_buf()));
 6184     call_offset = offset();
 6185   }
 6186 
 6187   bind(skip);
 6188   return call_offset;
 6189 }
 6190 
 6191 
 6192 // Move a value between registers/stack slots and update the reg_state
 6193 bool MacroAssembler::move_helper(VMReg from, VMReg to, BasicType bt, RegState reg_state[], int ret_off, int extra_stack_offset) {
 6194   if (reg_state[to-&gt;value()] == reg_written) {
 6195     return true; // Already written
 6196   }
 6197   if (from != to &amp;&amp; bt != T_VOID) {
 6198     if (reg_state[to-&gt;value()] == reg_readonly) {
 6199       return false; // Not yet writable
 6200     }
 6201     if (from-&gt;is_reg()) {
 6202       if (to-&gt;is_reg()) {
 6203         if (from-&gt;is_XMMRegister()) {
 6204           if (bt == T_DOUBLE) {
 6205             movdbl(to-&gt;as_XMMRegister(), from-&gt;as_XMMRegister());
 6206           } else {
 6207             assert(bt == T_FLOAT, &quot;must be float&quot;);
 6208             movflt(to-&gt;as_XMMRegister(), from-&gt;as_XMMRegister());
 6209           }
 6210         } else {
 6211           movq(to-&gt;as_Register(), from-&gt;as_Register());
 6212         }
 6213       } else {
 6214         int st_off = to-&gt;reg2stack() * VMRegImpl::stack_slot_size + extra_stack_offset;
 6215         assert(st_off != ret_off, &quot;overwriting return address at %d&quot;, st_off);
 6216         Address to_addr = Address(rsp, st_off);
 6217         if (from-&gt;is_XMMRegister()) {
 6218           if (bt == T_DOUBLE) {
 6219             movdbl(to_addr, from-&gt;as_XMMRegister());
 6220           } else {
 6221             assert(bt == T_FLOAT, &quot;must be float&quot;);
 6222             movflt(to_addr, from-&gt;as_XMMRegister());
 6223           }
 6224         } else {
 6225           movq(to_addr, from-&gt;as_Register());
 6226         }
 6227       }
 6228     } else {
 6229       Address from_addr = Address(rsp, from-&gt;reg2stack() * VMRegImpl::stack_slot_size + extra_stack_offset);
 6230       if (to-&gt;is_reg()) {
 6231         if (to-&gt;is_XMMRegister()) {
 6232           if (bt == T_DOUBLE) {
 6233             movdbl(to-&gt;as_XMMRegister(), from_addr);
 6234           } else {
 6235             assert(bt == T_FLOAT, &quot;must be float&quot;);
 6236             movflt(to-&gt;as_XMMRegister(), from_addr);
 6237           }
 6238         } else {
 6239           movq(to-&gt;as_Register(), from_addr);
 6240         }
 6241       } else {
 6242         int st_off = to-&gt;reg2stack() * VMRegImpl::stack_slot_size + extra_stack_offset;
 6243         assert(st_off != ret_off, &quot;overwriting return address at %d&quot;, st_off);
 6244         movq(r13, from_addr);
 6245         movq(Address(rsp, st_off), r13);
 6246       }
 6247     }
 6248   }
 6249   // Update register states
 6250   reg_state[from-&gt;value()] = reg_writable;
 6251   reg_state[to-&gt;value()] = reg_written;
 6252   return true;
 6253 }
 6254 
 6255 // Read all fields from a value type oop and store the values in registers/stack slots
 6256 bool MacroAssembler::unpack_value_helper(const GrowableArray&lt;SigEntry&gt;* sig, int&amp; sig_index, VMReg from, VMRegPair* regs_to,
 6257                                          int&amp; to_index, RegState reg_state[], int ret_off, int extra_stack_offset) {
 6258   Register fromReg = from-&gt;is_reg() ? from-&gt;as_Register() : noreg;
 6259   assert(sig-&gt;at(sig_index)._bt == T_VOID, &quot;should be at end delimiter&quot;);
 6260 
 6261   int vt = 1;
 6262   bool done = true;
 6263   bool mark_done = true;
 6264   do {
 6265     sig_index--;
 6266     BasicType bt = sig-&gt;at(sig_index)._bt;
 6267     if (bt == T_VALUETYPE) {
 6268       vt--;
 6269     } else if (bt == T_VOID &amp;&amp;
 6270                sig-&gt;at(sig_index-1)._bt != T_LONG &amp;&amp;
 6271                sig-&gt;at(sig_index-1)._bt != T_DOUBLE) {
 6272       vt++;
 6273     } else if (SigEntry::is_reserved_entry(sig, sig_index)) {
 6274       to_index--; // Ignore this
 6275     } else {
 6276       assert(to_index &gt;= 0, &quot;invalid to_index&quot;);
 6277       VMRegPair pair_to = regs_to[to_index--];
 6278       VMReg to = pair_to.first();
 6279 
 6280       if (bt == T_VOID) continue;
 6281 
 6282       int idx = (int)to-&gt;value();
 6283       if (reg_state[idx] == reg_readonly) {
 6284          if (idx != from-&gt;value()) {
 6285            mark_done = false;
 6286          }
 6287          done = false;
 6288          continue;
 6289       } else if (reg_state[idx] == reg_written) {
 6290         continue;
 6291       } else {
 6292         assert(reg_state[idx] == reg_writable, &quot;must be writable&quot;);
 6293         reg_state[idx] = reg_written;
 6294        }
 6295 
 6296       if (fromReg == noreg) {
 6297         int st_off = from-&gt;reg2stack() * VMRegImpl::stack_slot_size + extra_stack_offset;
 6298         movq(r10, Address(rsp, st_off));
 6299         fromReg = r10;
 6300       }
 6301 
 6302       int off = sig-&gt;at(sig_index)._offset;
 6303       assert(off &gt; 0, &quot;offset in object should be positive&quot;);
 6304       bool is_oop = (bt == T_OBJECT || bt == T_ARRAY);
 6305 
 6306       Address fromAddr = Address(fromReg, off);
 6307       bool is_signed = (bt != T_CHAR) &amp;&amp; (bt != T_BOOLEAN);
 6308       if (!to-&gt;is_XMMRegister()) {
 6309         Register dst = to-&gt;is_stack() ? r13 : to-&gt;as_Register();
 6310         if (is_oop) {
 6311           load_heap_oop(dst, fromAddr);
 6312         } else {
 6313           load_sized_value(dst, fromAddr, type2aelembytes(bt), is_signed);
 6314         }
 6315         if (to-&gt;is_stack()) {
 6316           int st_off = to-&gt;reg2stack() * VMRegImpl::stack_slot_size + extra_stack_offset;
 6317           assert(st_off != ret_off, &quot;overwriting return address at %d&quot;, st_off);
 6318           movq(Address(rsp, st_off), dst);
 6319         }
 6320       } else {
 6321         if (bt == T_DOUBLE) {
 6322           movdbl(to-&gt;as_XMMRegister(), fromAddr);
 6323         } else {
 6324           assert(bt == T_FLOAT, &quot;must be float&quot;);
 6325           movflt(to-&gt;as_XMMRegister(), fromAddr);
 6326         }
 6327       }
 6328     }
 6329   } while (vt != 0);
 6330   if (mark_done &amp;&amp; reg_state[from-&gt;value()] != reg_written) {
 6331     // This is okay because no one else will write to that slot
 6332     reg_state[from-&gt;value()] = reg_writable;
 6333   }
 6334   return done;
 6335 }
 6336 
 6337 // Pack fields back into a value type oop
 6338 bool MacroAssembler::pack_value_helper(const GrowableArray&lt;SigEntry&gt;* sig, int&amp; sig_index, int vtarg_index,
 6339                                        VMReg to, VMRegPair* regs_from, int regs_from_count, int&amp; from_index, RegState reg_state[],
 6340                                        int ret_off, int extra_stack_offset) {
 6341   assert(sig-&gt;at(sig_index)._bt == T_VALUETYPE, &quot;should be at end delimiter&quot;);
 6342   assert(to-&gt;is_valid(), &quot;must be&quot;);
 6343 
 6344   if (reg_state[to-&gt;value()] == reg_written) {
 6345     skip_unpacked_fields(sig, sig_index, regs_from, regs_from_count, from_index);
 6346     return true; // Already written
 6347   }
 6348 
 6349   Register val_array = rax;
 6350   Register val_obj_tmp = r11;
 6351   Register from_reg_tmp = r14; // Be careful with r14 because it&#39;s used for spilling
 6352   Register tmp1 = r10;
 6353   Register tmp2 = r13;
 6354   Register tmp3 = rbx;
 6355   Register val_obj = to-&gt;is_stack() ? val_obj_tmp : to-&gt;as_Register();
 6356 
 6357   if (reg_state[to-&gt;value()] == reg_readonly) {
 6358     if (!is_reg_in_unpacked_fields(sig, sig_index, to, regs_from, regs_from_count, from_index)) {
 6359       skip_unpacked_fields(sig, sig_index, regs_from, regs_from_count, from_index);
 6360       return false; // Not yet writable
 6361     }
 6362     val_obj = val_obj_tmp;
 6363   }
 6364 
 6365   int index = arrayOopDesc::base_offset_in_bytes(T_OBJECT) + vtarg_index * type2aelembytes(T_VALUETYPE);
 6366   load_heap_oop(val_obj, Address(val_array, index));
 6367 
 6368   ScalarizedValueArgsStream stream(sig, sig_index, regs_from, regs_from_count, from_index);
 6369   VMRegPair from_pair;
 6370   BasicType bt;
 6371   while (stream.next(from_pair, bt)) {
 6372     int off = sig-&gt;at(stream.sig_cc_index())._offset;
 6373     assert(off &gt; 0, &quot;offset in object should be positive&quot;);
 6374     bool is_oop = (bt == T_OBJECT || bt == T_ARRAY);
 6375     size_t size_in_bytes = is_java_primitive(bt) ? type2aelembytes(bt) : wordSize;
 6376 
 6377     VMReg from_r1 = from_pair.first();
 6378     VMReg from_r2 = from_pair.second();
 6379 
 6380     // Pack the scalarized field into the value object.
 6381     Address dst(val_obj, off);
 6382     if (!from_r1-&gt;is_XMMRegister()) {
 6383       Register from_reg;
 6384       if (from_r1-&gt;is_stack()) {
 6385         from_reg = from_reg_tmp;
 6386         int ld_off = from_r1-&gt;reg2stack() * VMRegImpl::stack_slot_size + extra_stack_offset;
 6387         load_sized_value(from_reg, Address(rsp, ld_off), size_in_bytes, /* is_signed */ false);
 6388       } else {
 6389         from_reg = from_r1-&gt;as_Register();
 6390       }
 6391       assert_different_registers(dst.base(), from_reg, tmp1, tmp2, tmp3, val_array);
 6392       if (is_oop) {
 6393         store_heap_oop(dst, from_reg, tmp1, tmp2, tmp3, IN_HEAP | ACCESS_WRITE | IS_DEST_UNINITIALIZED);
 6394       } else {
 6395         store_sized_value(dst, from_reg, size_in_bytes);
 6396       }
 6397     } else {
 6398       if (from_r2-&gt;is_valid()) {
 6399         movdbl(dst, from_r1-&gt;as_XMMRegister());
 6400       } else {
 6401         movflt(dst, from_r1-&gt;as_XMMRegister());
 6402       }
 6403     }
 6404     reg_state[from_r1-&gt;value()] = reg_writable;
 6405   }
 6406   sig_index = stream.sig_cc_index();
 6407   from_index = stream.regs_cc_index();
 6408 
 6409   assert(reg_state[to-&gt;value()] == reg_writable, &quot;must have already been read&quot;);
 6410   bool success = move_helper(val_obj-&gt;as_VMReg(), to, T_OBJECT, reg_state, ret_off, extra_stack_offset);
 6411   assert(success, &quot;to register must be writeable&quot;);
 6412 
 6413   return true;
 6414 }
 6415 
 6416 // Unpack all value type arguments passed as oops
 6417 void MacroAssembler::unpack_value_args(Compile* C, bool receiver_only) {
 6418   int sp_inc = unpack_value_args_common(C, receiver_only);
 6419   // Emit code for verified entry and save increment for stack repair on return
 6420   verified_entry(C, sp_inc);
 6421 }
 6422 
 6423 int MacroAssembler::shuffle_value_args(bool is_packing, bool receiver_only, int extra_stack_offset,
 6424                                        BasicType* sig_bt, const GrowableArray&lt;SigEntry&gt;* sig_cc,
 6425                                        int args_passed, int args_on_stack, VMRegPair* regs,            // from
 6426                                        int args_passed_to, int args_on_stack_to, VMRegPair* regs_to) { // to
 6427   // Check if we need to extend the stack for packing/unpacking
 6428   int sp_inc = (args_on_stack_to - args_on_stack) * VMRegImpl::stack_slot_size;
 6429   if (sp_inc &gt; 0) {
 6430     sp_inc = align_up(sp_inc, StackAlignmentInBytes);
 6431     if (!is_packing) {
 6432       // Save the return address, adjust the stack (make sure it is properly
 6433       // 16-byte aligned) and copy the return address to the new top of the stack.
 6434       // (Note: C1 does this in C1_MacroAssembler::scalarized_entry).
 6435       pop(r13);
 6436       subptr(rsp, sp_inc);
 6437       push(r13);
 6438     }
 6439   } else {
 6440     // The scalarized calling convention needs less stack space than the unscalarized one.
 6441     // No need to extend the stack, the caller will take care of these adjustments.
 6442     sp_inc = 0;
 6443   }
 6444 
 6445   int ret_off; // make sure we don&#39;t overwrite the return address
 6446   if (is_packing) {
 6447     // For C1 code, the VVEP doesn&#39;t have reserved slots, so we store the returned address at
 6448     // rsp[0] during shuffling.
 6449     ret_off = 0;
 6450   } else {
 6451     // C2 code ensures that sp_inc is a reserved slot.
 6452     ret_off = sp_inc;
 6453   }
 6454 
 6455   return shuffle_value_args_common(is_packing, receiver_only, extra_stack_offset,
 6456                                    sig_bt, sig_cc,
 6457                                    args_passed, args_on_stack, regs,
 6458                                    args_passed_to, args_on_stack_to, regs_to,
 6459                                    sp_inc, ret_off);
 6460 }
 6461 
 6462 VMReg MacroAssembler::spill_reg_for(VMReg reg) {
 6463   return reg-&gt;is_XMMRegister() ? xmm8-&gt;as_VMReg() : r14-&gt;as_VMReg();
 6464 }
 6465 
 6466 // Restores the stack on return
 6467 void MacroAssembler::restore_stack(Compile* C) {
 6468   int framesize = C-&gt;frame_size_in_bytes();
 6469   assert((framesize &amp; (StackAlignmentInBytes-1)) == 0, &quot;frame size not aligned&quot;);
 6470   // Remove word for return addr already pushed and RBP
 6471   framesize -= 2*wordSize;
 6472 
 6473   if (C-&gt;needs_stack_repair()) {
 6474     // Restore rbp and repair rsp by adding the stack increment
 6475     movq(rbp, Address(rsp, framesize));
 6476     addq(rsp, Address(rsp, C-&gt;sp_inc_offset()));
 6477   } else {
 6478     if (framesize &gt; 0) {
 6479       addq(rsp, framesize);
 6480     }
 6481     pop(rbp);
 6482   }
 6483 }
 6484 
 6485 void MacroAssembler::clear_mem(Register base, Register cnt, Register val, XMMRegister xtmp, bool is_large, bool word_copy_only) {
 6486   // cnt - number of qwords (8-byte words).
 6487   // base - start address, qword aligned.
 6488   // is_large - if optimizers know cnt is larger than InitArrayShortSize
 6489   assert(base==rdi, &quot;base register must be edi for rep stos&quot;);
 6490   assert(val==rax,   &quot;tmp register must be eax for rep stos&quot;);
 6491   assert(cnt==rcx,   &quot;cnt register must be ecx for rep stos&quot;);
 6492   assert(InitArrayShortSize % BytesPerLong == 0,
 6493     &quot;InitArrayShortSize should be the multiple of BytesPerLong&quot;);
 6494 
 6495   Label DONE;
 6496 
 6497   if (!is_large) {
 6498     Label LOOP, LONG;
 6499     cmpptr(cnt, InitArrayShortSize/BytesPerLong);
 6500     jccb(Assembler::greater, LONG);
 6501 
 6502     NOT_LP64(shlptr(cnt, 1);) // convert to number of 32-bit words for 32-bit VM
 6503 
 6504     decrement(cnt);
 6505     jccb(Assembler::negative, DONE); // Zero length
 6506 
 6507     // Use individual pointer-sized stores for small counts:
 6508     BIND(LOOP);
 6509     movptr(Address(base, cnt, Address::times_ptr), val);
 6510     decrement(cnt);
 6511     jccb(Assembler::greaterEqual, LOOP);
 6512     jmpb(DONE);
 6513 
 6514     BIND(LONG);
 6515   }
 6516 
 6517   // Use longer rep-prefixed ops for non-small counts:
 6518   if (UseFastStosb &amp;&amp; !word_copy_only) {
 6519     shlptr(cnt, 3); // convert to number of bytes
 6520     rep_stosb();
 6521   } else if (UseXMMForObjInit) {
 6522     xmm_clear_mem(base, cnt, val, xtmp);
 6523   } else {
 6524     NOT_LP64(shlptr(cnt, 1);) // convert to number of 32-bit words for 32-bit VM
 6525     rep_stos();
 6526   }
 6527 
 6528   BIND(DONE);
 6529 }
 6530 
 6531 #ifdef COMPILER2
 6532 
 6533 // IndexOf for constant substrings with size &gt;= 8 chars
 6534 // which don&#39;t need to be loaded through stack.
 6535 void MacroAssembler::string_indexofC8(Register str1, Register str2,
 6536                                       Register cnt1, Register cnt2,
 6537                                       int int_cnt2,  Register result,
 6538                                       XMMRegister vec, Register tmp,
 6539                                       int ae) {
 6540   ShortBranchVerifier sbv(this);
 6541   assert(UseSSE42Intrinsics, &quot;SSE4.2 intrinsics are required&quot;);
 6542   assert(ae != StrIntrinsicNode::LU, &quot;Invalid encoding&quot;);
 6543 
 6544   // This method uses the pcmpestri instruction with bound registers
 6545   //   inputs:
 6546   //     xmm - substring
 6547   //     rax - substring length (elements count)
 6548   //     mem - scanned string
 6549   //     rdx - string length (elements count)
 6550   //     0xd - mode: 1100 (substring search) + 01 (unsigned shorts)
 6551   //     0xc - mode: 1100 (substring search) + 00 (unsigned bytes)
 6552   //   outputs:
 6553   //     rcx - matched index in string
 6554   assert(cnt1 == rdx &amp;&amp; cnt2 == rax &amp;&amp; tmp == rcx, &quot;pcmpestri&quot;);
 6555   int mode   = (ae == StrIntrinsicNode::LL) ? 0x0c : 0x0d; // bytes or shorts
 6556   int stride = (ae == StrIntrinsicNode::LL) ? 16 : 8; //UU, UL -&gt; 8
 6557   Address::ScaleFactor scale1 = (ae == StrIntrinsicNode::LL) ? Address::times_1 : Address::times_2;
 6558   Address::ScaleFactor scale2 = (ae == StrIntrinsicNode::UL) ? Address::times_1 : scale1;
 6559 
 6560   Label RELOAD_SUBSTR, SCAN_TO_SUBSTR, SCAN_SUBSTR,
 6561         RET_FOUND, RET_NOT_FOUND, EXIT, FOUND_SUBSTR,
 6562         MATCH_SUBSTR_HEAD, RELOAD_STR, FOUND_CANDIDATE;
 6563 
 6564   // Note, inline_string_indexOf() generates checks:
 6565   // if (substr.count &gt; string.count) return -1;
 6566   // if (substr.count == 0) return 0;
 6567   assert(int_cnt2 &gt;= stride, &quot;this code is used only for cnt2 &gt;= 8 chars&quot;);
 6568 
 6569   // Load substring.
 6570   if (ae == StrIntrinsicNode::UL) {
 6571     pmovzxbw(vec, Address(str2, 0));
 6572   } else {
 6573     movdqu(vec, Address(str2, 0));
 6574   }
 6575   movl(cnt2, int_cnt2);
 6576   movptr(result, str1); // string addr
 6577 
 6578   if (int_cnt2 &gt; stride) {
 6579     jmpb(SCAN_TO_SUBSTR);
 6580 
 6581     // Reload substr for rescan, this code
 6582     // is executed only for large substrings (&gt; 8 chars)
 6583     bind(RELOAD_SUBSTR);
 6584     if (ae == StrIntrinsicNode::UL) {
 6585       pmovzxbw(vec, Address(str2, 0));
 6586     } else {
 6587       movdqu(vec, Address(str2, 0));
 6588     }
 6589     negptr(cnt2); // Jumped here with negative cnt2, convert to positive
 6590 
 6591     bind(RELOAD_STR);
 6592     // We came here after the beginning of the substring was
 6593     // matched but the rest of it was not so we need to search
 6594     // again. Start from the next element after the previous match.
 6595 
 6596     // cnt2 is number of substring reminding elements and
 6597     // cnt1 is number of string reminding elements when cmp failed.
 6598     // Restored cnt1 = cnt1 - cnt2 + int_cnt2
 6599     subl(cnt1, cnt2);
 6600     addl(cnt1, int_cnt2);
 6601     movl(cnt2, int_cnt2); // Now restore cnt2
 6602 
 6603     decrementl(cnt1);     // Shift to next element
 6604     cmpl(cnt1, cnt2);
 6605     jcc(Assembler::negative, RET_NOT_FOUND);  // Left less then substring
 6606 
 6607     addptr(result, (1&lt;&lt;scale1));
 6608 
 6609   } // (int_cnt2 &gt; 8)
 6610 
 6611   // Scan string for start of substr in 16-byte vectors
 6612   bind(SCAN_TO_SUBSTR);
 6613   pcmpestri(vec, Address(result, 0), mode);
 6614   jccb(Assembler::below, FOUND_CANDIDATE);   // CF == 1
 6615   subl(cnt1, stride);
 6616   jccb(Assembler::lessEqual, RET_NOT_FOUND); // Scanned full string
 6617   cmpl(cnt1, cnt2);
 6618   jccb(Assembler::negative, RET_NOT_FOUND);  // Left less then substring
 6619   addptr(result, 16);
 6620   jmpb(SCAN_TO_SUBSTR);
 6621 
 6622   // Found a potential substr
 6623   bind(FOUND_CANDIDATE);
 6624   // Matched whole vector if first element matched (tmp(rcx) == 0).
 6625   if (int_cnt2 == stride) {
 6626     jccb(Assembler::overflow, RET_FOUND);    // OF == 1
 6627   } else { // int_cnt2 &gt; 8
 6628     jccb(Assembler::overflow, FOUND_SUBSTR);
 6629   }
 6630   // After pcmpestri tmp(rcx) contains matched element index
 6631   // Compute start addr of substr
 6632   lea(result, Address(result, tmp, scale1));
 6633 
 6634   // Make sure string is still long enough
 6635   subl(cnt1, tmp);
 6636   cmpl(cnt1, cnt2);
 6637   if (int_cnt2 == stride) {
 6638     jccb(Assembler::greaterEqual, SCAN_TO_SUBSTR);
 6639   } else { // int_cnt2 &gt; 8
 6640     jccb(Assembler::greaterEqual, MATCH_SUBSTR_HEAD);
 6641   }
 6642   // Left less then substring.
 6643 
 6644   bind(RET_NOT_FOUND);
 6645   movl(result, -1);
 6646   jmp(EXIT);
 6647 
 6648   if (int_cnt2 &gt; stride) {
 6649     // This code is optimized for the case when whole substring
 6650     // is matched if its head is matched.
 6651     bind(MATCH_SUBSTR_HEAD);
 6652     pcmpestri(vec, Address(result, 0), mode);
 6653     // Reload only string if does not match
 6654     jcc(Assembler::noOverflow, RELOAD_STR); // OF == 0
 6655 
 6656     Label CONT_SCAN_SUBSTR;
 6657     // Compare the rest of substring (&gt; 8 chars).
 6658     bind(FOUND_SUBSTR);
 6659     // First 8 chars are already matched.
 6660     negptr(cnt2);
 6661     addptr(cnt2, stride);
 6662 
 6663     bind(SCAN_SUBSTR);
 6664     subl(cnt1, stride);
 6665     cmpl(cnt2, -stride); // Do not read beyond substring
 6666     jccb(Assembler::lessEqual, CONT_SCAN_SUBSTR);
 6667     // Back-up strings to avoid reading beyond substring:
 6668     // cnt1 = cnt1 - cnt2 + 8
 6669     addl(cnt1, cnt2); // cnt2 is negative
 6670     addl(cnt1, stride);
 6671     movl(cnt2, stride); negptr(cnt2);
 6672     bind(CONT_SCAN_SUBSTR);
 6673     if (int_cnt2 &lt; (int)G) {
 6674       int tail_off1 = int_cnt2&lt;&lt;scale1;
 6675       int tail_off2 = int_cnt2&lt;&lt;scale2;
 6676       if (ae == StrIntrinsicNode::UL) {
 6677         pmovzxbw(vec, Address(str2, cnt2, scale2, tail_off2));
 6678       } else {
 6679         movdqu(vec, Address(str2, cnt2, scale2, tail_off2));
 6680       }
 6681       pcmpestri(vec, Address(result, cnt2, scale1, tail_off1), mode);
 6682     } else {
 6683       // calculate index in register to avoid integer overflow (int_cnt2*2)
 6684       movl(tmp, int_cnt2);
 6685       addptr(tmp, cnt2);
 6686       if (ae == StrIntrinsicNode::UL) {
 6687         pmovzxbw(vec, Address(str2, tmp, scale2, 0));
 6688       } else {
 6689         movdqu(vec, Address(str2, tmp, scale2, 0));
 6690       }
 6691       pcmpestri(vec, Address(result, tmp, scale1, 0), mode);
 6692     }
 6693     // Need to reload strings pointers if not matched whole vector
 6694     jcc(Assembler::noOverflow, RELOAD_SUBSTR); // OF == 0
 6695     addptr(cnt2, stride);
 6696     jcc(Assembler::negative, SCAN_SUBSTR);
 6697     // Fall through if found full substring
 6698 
 6699   } // (int_cnt2 &gt; 8)
 6700 
 6701   bind(RET_FOUND);
 6702   // Found result if we matched full small substring.
 6703   // Compute substr offset
 6704   subptr(result, str1);
 6705   if (ae == StrIntrinsicNode::UU || ae == StrIntrinsicNode::UL) {
 6706     shrl(result, 1); // index
 6707   }
 6708   bind(EXIT);
 6709 
 6710 } // string_indexofC8
 6711 
 6712 // Small strings are loaded through stack if they cross page boundary.
 6713 void MacroAssembler::string_indexof(Register str1, Register str2,
 6714                                     Register cnt1, Register cnt2,
 6715                                     int int_cnt2,  Register result,
 6716                                     XMMRegister vec, Register tmp,
 6717                                     int ae) {
 6718   ShortBranchVerifier sbv(this);
 6719   assert(UseSSE42Intrinsics, &quot;SSE4.2 intrinsics are required&quot;);
 6720   assert(ae != StrIntrinsicNode::LU, &quot;Invalid encoding&quot;);
 6721 
 6722   //
 6723   // int_cnt2 is length of small (&lt; 8 chars) constant substring
 6724   // or (-1) for non constant substring in which case its length
 6725   // is in cnt2 register.
 6726   //
 6727   // Note, inline_string_indexOf() generates checks:
 6728   // if (substr.count &gt; string.count) return -1;
 6729   // if (substr.count == 0) return 0;
 6730   //
 6731   int stride = (ae == StrIntrinsicNode::LL) ? 16 : 8; //UU, UL -&gt; 8
 6732   assert(int_cnt2 == -1 || (0 &lt; int_cnt2 &amp;&amp; int_cnt2 &lt; stride), &quot;should be != 0&quot;);
 6733   // This method uses the pcmpestri instruction with bound registers
 6734   //   inputs:
 6735   //     xmm - substring
 6736   //     rax - substring length (elements count)
 6737   //     mem - scanned string
 6738   //     rdx - string length (elements count)
 6739   //     0xd - mode: 1100 (substring search) + 01 (unsigned shorts)
 6740   //     0xc - mode: 1100 (substring search) + 00 (unsigned bytes)
 6741   //   outputs:
 6742   //     rcx - matched index in string
 6743   assert(cnt1 == rdx &amp;&amp; cnt2 == rax &amp;&amp; tmp == rcx, &quot;pcmpestri&quot;);
 6744   int mode = (ae == StrIntrinsicNode::LL) ? 0x0c : 0x0d; // bytes or shorts
 6745   Address::ScaleFactor scale1 = (ae == StrIntrinsicNode::LL) ? Address::times_1 : Address::times_2;
 6746   Address::ScaleFactor scale2 = (ae == StrIntrinsicNode::UL) ? Address::times_1 : scale1;
 6747 
 6748   Label RELOAD_SUBSTR, SCAN_TO_SUBSTR, SCAN_SUBSTR, ADJUST_STR,
 6749         RET_FOUND, RET_NOT_FOUND, CLEANUP, FOUND_SUBSTR,
 6750         FOUND_CANDIDATE;
 6751 
 6752   { //========================================================
 6753     // We don&#39;t know where these strings are located
 6754     // and we can&#39;t read beyond them. Load them through stack.
 6755     Label BIG_STRINGS, CHECK_STR, COPY_SUBSTR, COPY_STR;
 6756 
 6757     movptr(tmp, rsp); // save old SP
 6758 
 6759     if (int_cnt2 &gt; 0) {     // small (&lt; 8 chars) constant substring
 6760       if (int_cnt2 == (1&gt;&gt;scale2)) { // One byte
 6761         assert((ae == StrIntrinsicNode::LL || ae == StrIntrinsicNode::UL), &quot;Only possible for latin1 encoding&quot;);
 6762         load_unsigned_byte(result, Address(str2, 0));
 6763         movdl(vec, result); // move 32 bits
 6764       } else if (ae == StrIntrinsicNode::LL &amp;&amp; int_cnt2 == 3) {  // Three bytes
 6765         // Not enough header space in 32-bit VM: 12+3 = 15.
 6766         movl(result, Address(str2, -1));
 6767         shrl(result, 8);
 6768         movdl(vec, result); // move 32 bits
 6769       } else if (ae != StrIntrinsicNode::UL &amp;&amp; int_cnt2 == (2&gt;&gt;scale2)) {  // One char
 6770         load_unsigned_short(result, Address(str2, 0));
 6771         movdl(vec, result); // move 32 bits
 6772       } else if (ae != StrIntrinsicNode::UL &amp;&amp; int_cnt2 == (4&gt;&gt;scale2)) { // Two chars
 6773         movdl(vec, Address(str2, 0)); // move 32 bits
 6774       } else if (ae != StrIntrinsicNode::UL &amp;&amp; int_cnt2 == (8&gt;&gt;scale2)) { // Four chars
 6775         movq(vec, Address(str2, 0));  // move 64 bits
 6776       } else { // cnt2 = { 3, 5, 6, 7 } || (ae == StrIntrinsicNode::UL &amp;&amp; cnt2 ={2, ..., 7})
 6777         // Array header size is 12 bytes in 32-bit VM
 6778         // + 6 bytes for 3 chars == 18 bytes,
 6779         // enough space to load vec and shift.
 6780         assert(HeapWordSize*TypeArrayKlass::header_size() &gt;= 12,&quot;sanity&quot;);
 6781         if (ae == StrIntrinsicNode::UL) {
 6782           int tail_off = int_cnt2-8;
 6783           pmovzxbw(vec, Address(str2, tail_off));
 6784           psrldq(vec, -2*tail_off);
 6785         }
 6786         else {
 6787           int tail_off = int_cnt2*(1&lt;&lt;scale2);
 6788           movdqu(vec, Address(str2, tail_off-16));
 6789           psrldq(vec, 16-tail_off);
 6790         }
 6791       }
 6792     } else { // not constant substring
 6793       cmpl(cnt2, stride);
 6794       jccb(Assembler::aboveEqual, BIG_STRINGS); // Both strings are big enough
 6795 
 6796       // We can read beyond string if srt+16 does not cross page boundary
 6797       // since heaps are aligned and mapped by pages.
 6798       assert(os::vm_page_size() &lt; (int)G, &quot;default page should be small&quot;);
 6799       movl(result, str2); // We need only low 32 bits
 6800       andl(result, (os::vm_page_size()-1));
 6801       cmpl(result, (os::vm_page_size()-16));
 6802       jccb(Assembler::belowEqual, CHECK_STR);
 6803 
 6804       // Move small strings to stack to allow load 16 bytes into vec.
 6805       subptr(rsp, 16);
 6806       int stk_offset = wordSize-(1&lt;&lt;scale2);
 6807       push(cnt2);
 6808 
 6809       bind(COPY_SUBSTR);
 6810       if (ae == StrIntrinsicNode::LL || ae == StrIntrinsicNode::UL) {
 6811         load_unsigned_byte(result, Address(str2, cnt2, scale2, -1));
 6812         movb(Address(rsp, cnt2, scale2, stk_offset), result);
 6813       } else if (ae == StrIntrinsicNode::UU) {
 6814         load_unsigned_short(result, Address(str2, cnt2, scale2, -2));
 6815         movw(Address(rsp, cnt2, scale2, stk_offset), result);
 6816       }
 6817       decrement(cnt2);
 6818       jccb(Assembler::notZero, COPY_SUBSTR);
 6819 
 6820       pop(cnt2);
 6821       movptr(str2, rsp);  // New substring address
 6822     } // non constant
 6823 
 6824     bind(CHECK_STR);
 6825     cmpl(cnt1, stride);
 6826     jccb(Assembler::aboveEqual, BIG_STRINGS);
 6827 
 6828     // Check cross page boundary.
 6829     movl(result, str1); // We need only low 32 bits
 6830     andl(result, (os::vm_page_size()-1));
 6831     cmpl(result, (os::vm_page_size()-16));
 6832     jccb(Assembler::belowEqual, BIG_STRINGS);
 6833 
 6834     subptr(rsp, 16);
 6835     int stk_offset = -(1&lt;&lt;scale1);
 6836     if (int_cnt2 &lt; 0) { // not constant
 6837       push(cnt2);
 6838       stk_offset += wordSize;
 6839     }
 6840     movl(cnt2, cnt1);
 6841 
 6842     bind(COPY_STR);
 6843     if (ae == StrIntrinsicNode::LL) {
 6844       load_unsigned_byte(result, Address(str1, cnt2, scale1, -1));
 6845       movb(Address(rsp, cnt2, scale1, stk_offset), result);
 6846     } else {
 6847       load_unsigned_short(result, Address(str1, cnt2, scale1, -2));
 6848       movw(Address(rsp, cnt2, scale1, stk_offset), result);
 6849     }
 6850     decrement(cnt2);
 6851     jccb(Assembler::notZero, COPY_STR);
 6852 
 6853     if (int_cnt2 &lt; 0) { // not constant
 6854       pop(cnt2);
 6855     }
 6856     movptr(str1, rsp);  // New string address
 6857 
 6858     bind(BIG_STRINGS);
 6859     // Load substring.
 6860     if (int_cnt2 &lt; 0) { // -1
 6861       if (ae == StrIntrinsicNode::UL) {
 6862         pmovzxbw(vec, Address(str2, 0));
 6863       } else {
 6864         movdqu(vec, Address(str2, 0));
 6865       }
 6866       push(cnt2);       // substr count
 6867       push(str2);       // substr addr
 6868       push(str1);       // string addr
 6869     } else {
 6870       // Small (&lt; 8 chars) constant substrings are loaded already.
 6871       movl(cnt2, int_cnt2);
 6872     }
 6873     push(tmp);  // original SP
 6874 
 6875   } // Finished loading
 6876 
 6877   //========================================================
 6878   // Start search
 6879   //
 6880 
 6881   movptr(result, str1); // string addr
 6882 
 6883   if (int_cnt2  &lt; 0) {  // Only for non constant substring
 6884     jmpb(SCAN_TO_SUBSTR);
 6885 
 6886     // SP saved at sp+0
 6887     // String saved at sp+1*wordSize
 6888     // Substr saved at sp+2*wordSize
 6889     // Substr count saved at sp+3*wordSize
 6890 
 6891     // Reload substr for rescan, this code
 6892     // is executed only for large substrings (&gt; 8 chars)
 6893     bind(RELOAD_SUBSTR);
 6894     movptr(str2, Address(rsp, 2*wordSize));
 6895     movl(cnt2, Address(rsp, 3*wordSize));
 6896     if (ae == StrIntrinsicNode::UL) {
 6897       pmovzxbw(vec, Address(str2, 0));
 6898     } else {
 6899       movdqu(vec, Address(str2, 0));
 6900     }
 6901     // We came here after the beginning of the substring was
 6902     // matched but the rest of it was not so we need to search
 6903     // again. Start from the next element after the previous match.
 6904     subptr(str1, result); // Restore counter
 6905     if (ae == StrIntrinsicNode::UU || ae == StrIntrinsicNode::UL) {
 6906       shrl(str1, 1);
 6907     }
 6908     addl(cnt1, str1);
 6909     decrementl(cnt1);   // Shift to next element
 6910     cmpl(cnt1, cnt2);
 6911     jcc(Assembler::negative, RET_NOT_FOUND);  // Left less then substring
 6912 
 6913     addptr(result, (1&lt;&lt;scale1));
 6914   } // non constant
 6915 
 6916   // Scan string for start of substr in 16-byte vectors
 6917   bind(SCAN_TO_SUBSTR);
 6918   assert(cnt1 == rdx &amp;&amp; cnt2 == rax &amp;&amp; tmp == rcx, &quot;pcmpestri&quot;);
 6919   pcmpestri(vec, Address(result, 0), mode);
 6920   jccb(Assembler::below, FOUND_CANDIDATE);   // CF == 1
 6921   subl(cnt1, stride);
 6922   jccb(Assembler::lessEqual, RET_NOT_FOUND); // Scanned full string
 6923   cmpl(cnt1, cnt2);
 6924   jccb(Assembler::negative, RET_NOT_FOUND);  // Left less then substring
 6925   addptr(result, 16);
 6926 
 6927   bind(ADJUST_STR);
 6928   cmpl(cnt1, stride); // Do not read beyond string
 6929   jccb(Assembler::greaterEqual, SCAN_TO_SUBSTR);
 6930   // Back-up string to avoid reading beyond string.
 6931   lea(result, Address(result, cnt1, scale1, -16));
 6932   movl(cnt1, stride);
 6933   jmpb(SCAN_TO_SUBSTR);
 6934 
 6935   // Found a potential substr
 6936   bind(FOUND_CANDIDATE);
 6937   // After pcmpestri tmp(rcx) contains matched element index
 6938 
 6939   // Make sure string is still long enough
 6940   subl(cnt1, tmp);
 6941   cmpl(cnt1, cnt2);
 6942   jccb(Assembler::greaterEqual, FOUND_SUBSTR);
 6943   // Left less then substring.
 6944 
 6945   bind(RET_NOT_FOUND);
 6946   movl(result, -1);
 6947   jmp(CLEANUP);
 6948 
 6949   bind(FOUND_SUBSTR);
 6950   // Compute start addr of substr
 6951   lea(result, Address(result, tmp, scale1));
 6952   if (int_cnt2 &gt; 0) { // Constant substring
 6953     // Repeat search for small substring (&lt; 8 chars)
 6954     // from new point without reloading substring.
 6955     // Have to check that we don&#39;t read beyond string.
 6956     cmpl(tmp, stride-int_cnt2);
 6957     jccb(Assembler::greater, ADJUST_STR);
 6958     // Fall through if matched whole substring.
 6959   } else { // non constant
 6960     assert(int_cnt2 == -1, &quot;should be != 0&quot;);
 6961 
 6962     addl(tmp, cnt2);
 6963     // Found result if we matched whole substring.
 6964     cmpl(tmp, stride);
 6965     jcc(Assembler::lessEqual, RET_FOUND);
 6966 
 6967     // Repeat search for small substring (&lt;= 8 chars)
 6968     // from new point &#39;str1&#39; without reloading substring.
 6969     cmpl(cnt2, stride);
 6970     // Have to check that we don&#39;t read beyond string.
 6971     jccb(Assembler::lessEqual, ADJUST_STR);
 6972 
 6973     Label CHECK_NEXT, CONT_SCAN_SUBSTR, RET_FOUND_LONG;
 6974     // Compare the rest of substring (&gt; 8 chars).
 6975     movptr(str1, result);
 6976 
 6977     cmpl(tmp, cnt2);
 6978     // First 8 chars are already matched.
 6979     jccb(Assembler::equal, CHECK_NEXT);
 6980 
 6981     bind(SCAN_SUBSTR);
 6982     pcmpestri(vec, Address(str1, 0), mode);
 6983     // Need to reload strings pointers if not matched whole vector
 6984     jcc(Assembler::noOverflow, RELOAD_SUBSTR); // OF == 0
 6985 
 6986     bind(CHECK_NEXT);
 6987     subl(cnt2, stride);
 6988     jccb(Assembler::lessEqual, RET_FOUND_LONG); // Found full substring
 6989     addptr(str1, 16);
 6990     if (ae == StrIntrinsicNode::UL) {
 6991       addptr(str2, 8);
 6992     } else {
 6993       addptr(str2, 16);
 6994     }
 6995     subl(cnt1, stride);
 6996     cmpl(cnt2, stride); // Do not read beyond substring
 6997     jccb(Assembler::greaterEqual, CONT_SCAN_SUBSTR);
 6998     // Back-up strings to avoid reading beyond substring.
 6999 
 7000     if (ae == StrIntrinsicNode::UL) {
 7001       lea(str2, Address(str2, cnt2, scale2, -8));
 7002       lea(str1, Address(str1, cnt2, scale1, -16));
 7003     } else {
 7004       lea(str2, Address(str2, cnt2, scale2, -16));
 7005       lea(str1, Address(str1, cnt2, scale1, -16));
 7006     }
 7007     subl(cnt1, cnt2);
 7008     movl(cnt2, stride);
 7009     addl(cnt1, stride);
 7010     bind(CONT_SCAN_SUBSTR);
 7011     if (ae == StrIntrinsicNode::UL) {
 7012       pmovzxbw(vec, Address(str2, 0));
 7013     } else {
 7014       movdqu(vec, Address(str2, 0));
 7015     }
 7016     jmp(SCAN_SUBSTR);
 7017 
 7018     bind(RET_FOUND_LONG);
 7019     movptr(str1, Address(rsp, wordSize));
 7020   } // non constant
 7021 
 7022   bind(RET_FOUND);
 7023   // Compute substr offset
 7024   subptr(result, str1);
 7025   if (ae == StrIntrinsicNode::UU || ae == StrIntrinsicNode::UL) {
 7026     shrl(result, 1); // index
 7027   }
 7028   bind(CLEANUP);
 7029   pop(rsp); // restore SP
 7030 
 7031 } // string_indexof
 7032 
 7033 void MacroAssembler::string_indexof_char(Register str1, Register cnt1, Register ch, Register result,
 7034                                          XMMRegister vec1, XMMRegister vec2, XMMRegister vec3, Register tmp) {
 7035   ShortBranchVerifier sbv(this);
 7036   assert(UseSSE42Intrinsics, &quot;SSE4.2 intrinsics are required&quot;);
 7037 
 7038   int stride = 8;
 7039 
 7040   Label FOUND_CHAR, SCAN_TO_CHAR, SCAN_TO_CHAR_LOOP,
 7041         SCAN_TO_8_CHAR, SCAN_TO_8_CHAR_LOOP, SCAN_TO_16_CHAR_LOOP,
 7042         RET_NOT_FOUND, SCAN_TO_8_CHAR_INIT,
 7043         FOUND_SEQ_CHAR, DONE_LABEL;
 7044 
 7045   movptr(result, str1);
 7046   if (UseAVX &gt;= 2) {
 7047     cmpl(cnt1, stride);
 7048     jcc(Assembler::less, SCAN_TO_CHAR);
 7049     cmpl(cnt1, 2*stride);
 7050     jcc(Assembler::less, SCAN_TO_8_CHAR_INIT);
 7051     movdl(vec1, ch);
 7052     vpbroadcastw(vec1, vec1, Assembler::AVX_256bit);
 7053     vpxor(vec2, vec2);
 7054     movl(tmp, cnt1);
 7055     andl(tmp, 0xFFFFFFF0);  //vector count (in chars)
 7056     andl(cnt1,0x0000000F);  //tail count (in chars)
 7057 
 7058     bind(SCAN_TO_16_CHAR_LOOP);
 7059     vmovdqu(vec3, Address(result, 0));
 7060     vpcmpeqw(vec3, vec3, vec1, 1);
 7061     vptest(vec2, vec3);
 7062     jcc(Assembler::carryClear, FOUND_CHAR);
 7063     addptr(result, 32);
 7064     subl(tmp, 2*stride);
 7065     jcc(Assembler::notZero, SCAN_TO_16_CHAR_LOOP);
 7066     jmp(SCAN_TO_8_CHAR);
 7067     bind(SCAN_TO_8_CHAR_INIT);
 7068     movdl(vec1, ch);
 7069     pshuflw(vec1, vec1, 0x00);
 7070     pshufd(vec1, vec1, 0);
 7071     pxor(vec2, vec2);
 7072   }
 7073   bind(SCAN_TO_8_CHAR);
 7074   cmpl(cnt1, stride);
 7075   jcc(Assembler::less, SCAN_TO_CHAR);
 7076   if (UseAVX &lt; 2) {
 7077     movdl(vec1, ch);
 7078     pshuflw(vec1, vec1, 0x00);
 7079     pshufd(vec1, vec1, 0);
 7080     pxor(vec2, vec2);
 7081   }
 7082   movl(tmp, cnt1);
 7083   andl(tmp, 0xFFFFFFF8);  //vector count (in chars)
 7084   andl(cnt1,0x00000007);  //tail count (in chars)
 7085 
 7086   bind(SCAN_TO_8_CHAR_LOOP);
 7087   movdqu(vec3, Address(result, 0));
 7088   pcmpeqw(vec3, vec1);
 7089   ptest(vec2, vec3);
 7090   jcc(Assembler::carryClear, FOUND_CHAR);
 7091   addptr(result, 16);
 7092   subl(tmp, stride);
 7093   jcc(Assembler::notZero, SCAN_TO_8_CHAR_LOOP);
 7094   bind(SCAN_TO_CHAR);
 7095   testl(cnt1, cnt1);
 7096   jcc(Assembler::zero, RET_NOT_FOUND);
 7097   bind(SCAN_TO_CHAR_LOOP);
 7098   load_unsigned_short(tmp, Address(result, 0));
 7099   cmpl(ch, tmp);
 7100   jccb(Assembler::equal, FOUND_SEQ_CHAR);
 7101   addptr(result, 2);
 7102   subl(cnt1, 1);
 7103   jccb(Assembler::zero, RET_NOT_FOUND);
 7104   jmp(SCAN_TO_CHAR_LOOP);
 7105 
 7106   bind(RET_NOT_FOUND);
 7107   movl(result, -1);
 7108   jmpb(DONE_LABEL);
 7109 
 7110   bind(FOUND_CHAR);
 7111   if (UseAVX &gt;= 2) {
 7112     vpmovmskb(tmp, vec3);
 7113   } else {
 7114     pmovmskb(tmp, vec3);
 7115   }
 7116   bsfl(ch, tmp);
 7117   addl(result, ch);
 7118 
 7119   bind(FOUND_SEQ_CHAR);
 7120   subptr(result, str1);
 7121   shrl(result, 1);
 7122 
 7123   bind(DONE_LABEL);
 7124 } // string_indexof_char
 7125 
 7126 // helper function for string_compare
 7127 void MacroAssembler::load_next_elements(Register elem1, Register elem2, Register str1, Register str2,
 7128                                         Address::ScaleFactor scale, Address::ScaleFactor scale1,
 7129                                         Address::ScaleFactor scale2, Register index, int ae) {
 7130   if (ae == StrIntrinsicNode::LL) {
 7131     load_unsigned_byte(elem1, Address(str1, index, scale, 0));
 7132     load_unsigned_byte(elem2, Address(str2, index, scale, 0));
 7133   } else if (ae == StrIntrinsicNode::UU) {
 7134     load_unsigned_short(elem1, Address(str1, index, scale, 0));
 7135     load_unsigned_short(elem2, Address(str2, index, scale, 0));
 7136   } else {
 7137     load_unsigned_byte(elem1, Address(str1, index, scale1, 0));
 7138     load_unsigned_short(elem2, Address(str2, index, scale2, 0));
 7139   }
 7140 }
 7141 
 7142 // Compare strings, used for char[] and byte[].
 7143 void MacroAssembler::string_compare(Register str1, Register str2,
 7144                                     Register cnt1, Register cnt2, Register result,
 7145                                     XMMRegister vec1, int ae) {
 7146   ShortBranchVerifier sbv(this);
 7147   Label LENGTH_DIFF_LABEL, POP_LABEL, DONE_LABEL, WHILE_HEAD_LABEL;
 7148   Label COMPARE_WIDE_VECTORS_LOOP_FAILED;  // used only _LP64 &amp;&amp; AVX3
 7149   int stride, stride2, adr_stride, adr_stride1, adr_stride2;
 7150   int stride2x2 = 0x40;
 7151   Address::ScaleFactor scale = Address::no_scale;
 7152   Address::ScaleFactor scale1 = Address::no_scale;
 7153   Address::ScaleFactor scale2 = Address::no_scale;
 7154 
 7155   if (ae != StrIntrinsicNode::LL) {
 7156     stride2x2 = 0x20;
 7157   }
 7158 
 7159   if (ae == StrIntrinsicNode::LU || ae == StrIntrinsicNode::UL) {
 7160     shrl(cnt2, 1);
 7161   }
 7162   // Compute the minimum of the string lengths and the
 7163   // difference of the string lengths (stack).
 7164   // Do the conditional move stuff
 7165   movl(result, cnt1);
 7166   subl(cnt1, cnt2);
 7167   push(cnt1);
 7168   cmov32(Assembler::lessEqual, cnt2, result);    // cnt2 = min(cnt1, cnt2)
 7169 
 7170   // Is the minimum length zero?
 7171   testl(cnt2, cnt2);
 7172   jcc(Assembler::zero, LENGTH_DIFF_LABEL);
 7173   if (ae == StrIntrinsicNode::LL) {
 7174     // Load first bytes
 7175     load_unsigned_byte(result, Address(str1, 0));  // result = str1[0]
 7176     load_unsigned_byte(cnt1, Address(str2, 0));    // cnt1   = str2[0]
 7177   } else if (ae == StrIntrinsicNode::UU) {
 7178     // Load first characters
 7179     load_unsigned_short(result, Address(str1, 0));
 7180     load_unsigned_short(cnt1, Address(str2, 0));
 7181   } else {
 7182     load_unsigned_byte(result, Address(str1, 0));
 7183     load_unsigned_short(cnt1, Address(str2, 0));
 7184   }
 7185   subl(result, cnt1);
 7186   jcc(Assembler::notZero,  POP_LABEL);
 7187 
 7188   if (ae == StrIntrinsicNode::UU) {
 7189     // Divide length by 2 to get number of chars
 7190     shrl(cnt2, 1);
 7191   }
 7192   cmpl(cnt2, 1);
 7193   jcc(Assembler::equal, LENGTH_DIFF_LABEL);
 7194 
 7195   // Check if the strings start at the same location and setup scale and stride
 7196   if (ae == StrIntrinsicNode::LL || ae == StrIntrinsicNode::UU) {
 7197     cmpptr(str1, str2);
 7198     jcc(Assembler::equal, LENGTH_DIFF_LABEL);
 7199     if (ae == StrIntrinsicNode::LL) {
 7200       scale = Address::times_1;
 7201       stride = 16;
 7202     } else {
 7203       scale = Address::times_2;
 7204       stride = 8;
 7205     }
 7206   } else {
 7207     scale1 = Address::times_1;
 7208     scale2 = Address::times_2;
 7209     // scale not used
 7210     stride = 8;
 7211   }
 7212 
 7213   if (UseAVX &gt;= 2 &amp;&amp; UseSSE42Intrinsics) {
 7214     Label COMPARE_WIDE_VECTORS, VECTOR_NOT_EQUAL, COMPARE_WIDE_TAIL, COMPARE_SMALL_STR;
 7215     Label COMPARE_WIDE_VECTORS_LOOP, COMPARE_16_CHARS, COMPARE_INDEX_CHAR;
 7216     Label COMPARE_WIDE_VECTORS_LOOP_AVX2;
 7217     Label COMPARE_TAIL_LONG;
 7218     Label COMPARE_WIDE_VECTORS_LOOP_AVX3;  // used only _LP64 &amp;&amp; AVX3
 7219 
 7220     int pcmpmask = 0x19;
 7221     if (ae == StrIntrinsicNode::LL) {
 7222       pcmpmask &amp;= ~0x01;
 7223     }
 7224 
 7225     // Setup to compare 16-chars (32-bytes) vectors,
 7226     // start from first character again because it has aligned address.
 7227     if (ae == StrIntrinsicNode::LL) {
 7228       stride2 = 32;
 7229     } else {
 7230       stride2 = 16;
 7231     }
 7232     if (ae == StrIntrinsicNode::LL || ae == StrIntrinsicNode::UU) {
 7233       adr_stride = stride &lt;&lt; scale;
 7234     } else {
 7235       adr_stride1 = 8;  //stride &lt;&lt; scale1;
 7236       adr_stride2 = 16; //stride &lt;&lt; scale2;
 7237     }
 7238 
 7239     assert(result == rax &amp;&amp; cnt2 == rdx &amp;&amp; cnt1 == rcx, &quot;pcmpestri&quot;);
 7240     // rax and rdx are used by pcmpestri as elements counters
 7241     movl(result, cnt2);
 7242     andl(cnt2, ~(stride2-1));   // cnt2 holds the vector count
 7243     jcc(Assembler::zero, COMPARE_TAIL_LONG);
 7244 
 7245     // fast path : compare first 2 8-char vectors.
 7246     bind(COMPARE_16_CHARS);
 7247     if (ae == StrIntrinsicNode::LL || ae == StrIntrinsicNode::UU) {
 7248       movdqu(vec1, Address(str1, 0));
 7249     } else {
 7250       pmovzxbw(vec1, Address(str1, 0));
 7251     }
 7252     pcmpestri(vec1, Address(str2, 0), pcmpmask);
 7253     jccb(Assembler::below, COMPARE_INDEX_CHAR);
 7254 
 7255     if (ae == StrIntrinsicNode::LL || ae == StrIntrinsicNode::UU) {
 7256       movdqu(vec1, Address(str1, adr_stride));
 7257       pcmpestri(vec1, Address(str2, adr_stride), pcmpmask);
 7258     } else {
 7259       pmovzxbw(vec1, Address(str1, adr_stride1));
 7260       pcmpestri(vec1, Address(str2, adr_stride2), pcmpmask);
 7261     }
 7262     jccb(Assembler::aboveEqual, COMPARE_WIDE_VECTORS);
 7263     addl(cnt1, stride);
 7264 
 7265     // Compare the characters at index in cnt1
 7266     bind(COMPARE_INDEX_CHAR); // cnt1 has the offset of the mismatching character
 7267     load_next_elements(result, cnt2, str1, str2, scale, scale1, scale2, cnt1, ae);
 7268     subl(result, cnt2);
 7269     jmp(POP_LABEL);
 7270 
 7271     // Setup the registers to start vector comparison loop
 7272     bind(COMPARE_WIDE_VECTORS);
 7273     if (ae == StrIntrinsicNode::LL || ae == StrIntrinsicNode::UU) {
 7274       lea(str1, Address(str1, result, scale));
 7275       lea(str2, Address(str2, result, scale));
 7276     } else {
 7277       lea(str1, Address(str1, result, scale1));
 7278       lea(str2, Address(str2, result, scale2));
 7279     }
 7280     subl(result, stride2);
 7281     subl(cnt2, stride2);
 7282     jcc(Assembler::zero, COMPARE_WIDE_TAIL);
 7283     negptr(result);
 7284 
 7285     //  In a loop, compare 16-chars (32-bytes) at once using (vpxor+vptest)
 7286     bind(COMPARE_WIDE_VECTORS_LOOP);
 7287 
 7288 #ifdef _LP64
 7289     if ((AVX3Threshold == 0) &amp;&amp; VM_Version::supports_avx512vlbw()) { // trying 64 bytes fast loop
 7290       cmpl(cnt2, stride2x2);
 7291       jccb(Assembler::below, COMPARE_WIDE_VECTORS_LOOP_AVX2);
 7292       testl(cnt2, stride2x2-1);   // cnt2 holds the vector count
 7293       jccb(Assembler::notZero, COMPARE_WIDE_VECTORS_LOOP_AVX2);   // means we cannot subtract by 0x40
 7294 
 7295       bind(COMPARE_WIDE_VECTORS_LOOP_AVX3); // the hottest loop
 7296       if (ae == StrIntrinsicNode::LL || ae == StrIntrinsicNode::UU) {
 7297         evmovdquq(vec1, Address(str1, result, scale), Assembler::AVX_512bit);
 7298         evpcmpeqb(k7, vec1, Address(str2, result, scale), Assembler::AVX_512bit); // k7 == 11..11, if operands equal, otherwise k7 has some 0
 7299       } else {
 7300         vpmovzxbw(vec1, Address(str1, result, scale1), Assembler::AVX_512bit);
 7301         evpcmpeqb(k7, vec1, Address(str2, result, scale2), Assembler::AVX_512bit); // k7 == 11..11, if operands equal, otherwise k7 has some 0
 7302       }
 7303       kortestql(k7, k7);
 7304       jcc(Assembler::aboveEqual, COMPARE_WIDE_VECTORS_LOOP_FAILED);     // miscompare
 7305       addptr(result, stride2x2);  // update since we already compared at this addr
 7306       subl(cnt2, stride2x2);      // and sub the size too
 7307       jccb(Assembler::notZero, COMPARE_WIDE_VECTORS_LOOP_AVX3);
 7308 
 7309       vpxor(vec1, vec1);
 7310       jmpb(COMPARE_WIDE_TAIL);
 7311     }//if (VM_Version::supports_avx512vlbw())
 7312 #endif // _LP64
 7313 
 7314 
 7315     bind(COMPARE_WIDE_VECTORS_LOOP_AVX2);
 7316     if (ae == StrIntrinsicNode::LL || ae == StrIntrinsicNode::UU) {
 7317       vmovdqu(vec1, Address(str1, result, scale));
 7318       vpxor(vec1, Address(str2, result, scale));
 7319     } else {
 7320       vpmovzxbw(vec1, Address(str1, result, scale1), Assembler::AVX_256bit);
 7321       vpxor(vec1, Address(str2, result, scale2));
 7322     }
 7323     vptest(vec1, vec1);
 7324     jcc(Assembler::notZero, VECTOR_NOT_EQUAL);
 7325     addptr(result, stride2);
 7326     subl(cnt2, stride2);
 7327     jcc(Assembler::notZero, COMPARE_WIDE_VECTORS_LOOP);
 7328     // clean upper bits of YMM registers
 7329     vpxor(vec1, vec1);
 7330 
 7331     // compare wide vectors tail
 7332     bind(COMPARE_WIDE_TAIL);
 7333     testptr(result, result);
 7334     jcc(Assembler::zero, LENGTH_DIFF_LABEL);
 7335 
 7336     movl(result, stride2);
 7337     movl(cnt2, result);
 7338     negptr(result);
 7339     jmp(COMPARE_WIDE_VECTORS_LOOP_AVX2);
 7340 
 7341     // Identifies the mismatching (higher or lower)16-bytes in the 32-byte vectors.
 7342     bind(VECTOR_NOT_EQUAL);
 7343     // clean upper bits of YMM registers
 7344     vpxor(vec1, vec1);
 7345     if (ae == StrIntrinsicNode::LL || ae == StrIntrinsicNode::UU) {
 7346       lea(str1, Address(str1, result, scale));
 7347       lea(str2, Address(str2, result, scale));
 7348     } else {
 7349       lea(str1, Address(str1, result, scale1));
 7350       lea(str2, Address(str2, result, scale2));
 7351     }
 7352     jmp(COMPARE_16_CHARS);
 7353 
 7354     // Compare tail chars, length between 1 to 15 chars
 7355     bind(COMPARE_TAIL_LONG);
 7356     movl(cnt2, result);
 7357     cmpl(cnt2, stride);
 7358     jcc(Assembler::less, COMPARE_SMALL_STR);
 7359 
 7360     if (ae == StrIntrinsicNode::LL || ae == StrIntrinsicNode::UU) {
 7361       movdqu(vec1, Address(str1, 0));
 7362     } else {
 7363       pmovzxbw(vec1, Address(str1, 0));
 7364     }
 7365     pcmpestri(vec1, Address(str2, 0), pcmpmask);
 7366     jcc(Assembler::below, COMPARE_INDEX_CHAR);
 7367     subptr(cnt2, stride);
 7368     jcc(Assembler::zero, LENGTH_DIFF_LABEL);
 7369     if (ae == StrIntrinsicNode::LL || ae == StrIntrinsicNode::UU) {
 7370       lea(str1, Address(str1, result, scale));
 7371       lea(str2, Address(str2, result, scale));
 7372     } else {
 7373       lea(str1, Address(str1, result, scale1));
 7374       lea(str2, Address(str2, result, scale2));
 7375     }
 7376     negptr(cnt2);
 7377     jmpb(WHILE_HEAD_LABEL);
 7378 
 7379     bind(COMPARE_SMALL_STR);
 7380   } else if (UseSSE42Intrinsics) {
 7381     Label COMPARE_WIDE_VECTORS, VECTOR_NOT_EQUAL, COMPARE_TAIL;
 7382     int pcmpmask = 0x19;
 7383     // Setup to compare 8-char (16-byte) vectors,
 7384     // start from first character again because it has aligned address.
 7385     movl(result, cnt2);
 7386     andl(cnt2, ~(stride - 1));   // cnt2 holds the vector count
 7387     if (ae == StrIntrinsicNode::LL) {
 7388       pcmpmask &amp;= ~0x01;
 7389     }
 7390     jcc(Assembler::zero, COMPARE_TAIL);
 7391     if (ae == StrIntrinsicNode::LL || ae == StrIntrinsicNode::UU) {
 7392       lea(str1, Address(str1, result, scale));
 7393       lea(str2, Address(str2, result, scale));
 7394     } else {
 7395       lea(str1, Address(str1, result, scale1));
 7396       lea(str2, Address(str2, result, scale2));
 7397     }
 7398     negptr(result);
 7399 
 7400     // pcmpestri
 7401     //   inputs:
 7402     //     vec1- substring
 7403     //     rax - negative string length (elements count)
 7404     //     mem - scanned string
 7405     //     rdx - string length (elements count)
 7406     //     pcmpmask - cmp mode: 11000 (string compare with negated result)
 7407     //               + 00 (unsigned bytes) or  + 01 (unsigned shorts)
 7408     //   outputs:
 7409     //     rcx - first mismatched element index
 7410     assert(result == rax &amp;&amp; cnt2 == rdx &amp;&amp; cnt1 == rcx, &quot;pcmpestri&quot;);
 7411 
 7412     bind(COMPARE_WIDE_VECTORS);
 7413     if (ae == StrIntrinsicNode::LL || ae == StrIntrinsicNode::UU) {
 7414       movdqu(vec1, Address(str1, result, scale));
 7415       pcmpestri(vec1, Address(str2, result, scale), pcmpmask);
 7416     } else {
 7417       pmovzxbw(vec1, Address(str1, result, scale1));
 7418       pcmpestri(vec1, Address(str2, result, scale2), pcmpmask);
 7419     }
 7420     // After pcmpestri cnt1(rcx) contains mismatched element index
 7421 
 7422     jccb(Assembler::below, VECTOR_NOT_EQUAL);  // CF==1
 7423     addptr(result, stride);
 7424     subptr(cnt2, stride);
 7425     jccb(Assembler::notZero, COMPARE_WIDE_VECTORS);
 7426 
 7427     // compare wide vectors tail
 7428     testptr(result, result);
 7429     jcc(Assembler::zero, LENGTH_DIFF_LABEL);
 7430 
 7431     movl(cnt2, stride);
 7432     movl(result, stride);
 7433     negptr(result);
 7434     if (ae == StrIntrinsicNode::LL || ae == StrIntrinsicNode::UU) {
 7435       movdqu(vec1, Address(str1, result, scale));
 7436       pcmpestri(vec1, Address(str2, result, scale), pcmpmask);
 7437     } else {
 7438       pmovzxbw(vec1, Address(str1, result, scale1));
 7439       pcmpestri(vec1, Address(str2, result, scale2), pcmpmask);
 7440     }
 7441     jccb(Assembler::aboveEqual, LENGTH_DIFF_LABEL);
 7442 
 7443     // Mismatched characters in the vectors
 7444     bind(VECTOR_NOT_EQUAL);
 7445     addptr(cnt1, result);
 7446     load_next_elements(result, cnt2, str1, str2, scale, scale1, scale2, cnt1, ae);
 7447     subl(result, cnt2);
 7448     jmpb(POP_LABEL);
 7449 
 7450     bind(COMPARE_TAIL); // limit is zero
 7451     movl(cnt2, result);
 7452     // Fallthru to tail compare
 7453   }
 7454   // Shift str2 and str1 to the end of the arrays, negate min
 7455   if (ae == StrIntrinsicNode::LL || ae == StrIntrinsicNode::UU) {
 7456     lea(str1, Address(str1, cnt2, scale));
 7457     lea(str2, Address(str2, cnt2, scale));
 7458   } else {
 7459     lea(str1, Address(str1, cnt2, scale1));
 7460     lea(str2, Address(str2, cnt2, scale2));
 7461   }
 7462   decrementl(cnt2);  // first character was compared already
 7463   negptr(cnt2);
 7464 
 7465   // Compare the rest of the elements
 7466   bind(WHILE_HEAD_LABEL);
 7467   load_next_elements(result, cnt1, str1, str2, scale, scale1, scale2, cnt2, ae);
 7468   subl(result, cnt1);
 7469   jccb(Assembler::notZero, POP_LABEL);
 7470   increment(cnt2);
 7471   jccb(Assembler::notZero, WHILE_HEAD_LABEL);
 7472 
 7473   // Strings are equal up to min length.  Return the length difference.
 7474   bind(LENGTH_DIFF_LABEL);
 7475   pop(result);
 7476   if (ae == StrIntrinsicNode::UU) {
 7477     // Divide diff by 2 to get number of chars
 7478     sarl(result, 1);
 7479   }
 7480   jmpb(DONE_LABEL);
 7481 
 7482 #ifdef _LP64
 7483   if (VM_Version::supports_avx512vlbw()) {
 7484 
 7485     bind(COMPARE_WIDE_VECTORS_LOOP_FAILED);
 7486 
 7487     kmovql(cnt1, k7);
 7488     notq(cnt1);
 7489     bsfq(cnt2, cnt1);
 7490     if (ae != StrIntrinsicNode::LL) {
 7491       // Divide diff by 2 to get number of chars
 7492       sarl(cnt2, 1);
 7493     }
 7494     addq(result, cnt2);
 7495     if (ae == StrIntrinsicNode::LL) {
 7496       load_unsigned_byte(cnt1, Address(str2, result));
 7497       load_unsigned_byte(result, Address(str1, result));
 7498     } else if (ae == StrIntrinsicNode::UU) {
 7499       load_unsigned_short(cnt1, Address(str2, result, scale));
 7500       load_unsigned_short(result, Address(str1, result, scale));
 7501     } else {
 7502       load_unsigned_short(cnt1, Address(str2, result, scale2));
 7503       load_unsigned_byte(result, Address(str1, result, scale1));
 7504     }
 7505     subl(result, cnt1);
 7506     jmpb(POP_LABEL);
 7507   }//if (VM_Version::supports_avx512vlbw())
 7508 #endif // _LP64
 7509 
 7510   // Discard the stored length difference
 7511   bind(POP_LABEL);
 7512   pop(cnt1);
 7513 
 7514   // That&#39;s it
 7515   bind(DONE_LABEL);
 7516   if(ae == StrIntrinsicNode::UL) {
 7517     negl(result);
 7518   }
 7519 
 7520 }
 7521 
 7522 // Search for Non-ASCII character (Negative byte value) in a byte array,
 7523 // return true if it has any and false otherwise.
 7524 //   ..\jdk\src\java.base\share\classes\java\lang\StringCoding.java
 7525 //   @HotSpotIntrinsicCandidate
 7526 //   private static boolean hasNegatives(byte[] ba, int off, int len) {
 7527 //     for (int i = off; i &lt; off + len; i++) {
 7528 //       if (ba[i] &lt; 0) {
 7529 //         return true;
 7530 //       }
 7531 //     }
 7532 //     return false;
 7533 //   }
 7534 void MacroAssembler::has_negatives(Register ary1, Register len,
 7535   Register result, Register tmp1,
 7536   XMMRegister vec1, XMMRegister vec2) {
 7537   // rsi: byte array
 7538   // rcx: len
 7539   // rax: result
 7540   ShortBranchVerifier sbv(this);
 7541   assert_different_registers(ary1, len, result, tmp1);
 7542   assert_different_registers(vec1, vec2);
 7543   Label TRUE_LABEL, FALSE_LABEL, DONE, COMPARE_CHAR, COMPARE_VECTORS, COMPARE_BYTE;
 7544 
 7545   // len == 0
 7546   testl(len, len);
 7547   jcc(Assembler::zero, FALSE_LABEL);
 7548 
 7549   if ((AVX3Threshold == 0) &amp;&amp; (UseAVX &gt; 2) &amp;&amp; // AVX512
 7550     VM_Version::supports_avx512vlbw() &amp;&amp;
 7551     VM_Version::supports_bmi2()) {
 7552 
 7553     Label test_64_loop, test_tail;
 7554     Register tmp3_aliased = len;
 7555 
 7556     movl(tmp1, len);
 7557     vpxor(vec2, vec2, vec2, Assembler::AVX_512bit);
 7558 
 7559     andl(tmp1, 64 - 1);   // tail count (in chars) 0x3F
 7560     andl(len, ~(64 - 1));    // vector count (in chars)
 7561     jccb(Assembler::zero, test_tail);
 7562 
 7563     lea(ary1, Address(ary1, len, Address::times_1));
 7564     negptr(len);
 7565 
 7566     bind(test_64_loop);
 7567     // Check whether our 64 elements of size byte contain negatives
 7568     evpcmpgtb(k2, vec2, Address(ary1, len, Address::times_1), Assembler::AVX_512bit);
 7569     kortestql(k2, k2);
 7570     jcc(Assembler::notZero, TRUE_LABEL);
 7571 
 7572     addptr(len, 64);
 7573     jccb(Assembler::notZero, test_64_loop);
 7574 
 7575 
 7576     bind(test_tail);
 7577     // bail out when there is nothing to be done
 7578     testl(tmp1, -1);
 7579     jcc(Assembler::zero, FALSE_LABEL);
 7580 
 7581     // ~(~0 &lt;&lt; len) applied up to two times (for 32-bit scenario)
 7582 #ifdef _LP64
 7583     mov64(tmp3_aliased, 0xFFFFFFFFFFFFFFFF);
 7584     shlxq(tmp3_aliased, tmp3_aliased, tmp1);
 7585     notq(tmp3_aliased);
 7586     kmovql(k3, tmp3_aliased);
 7587 #else
 7588     Label k_init;
 7589     jmp(k_init);
 7590 
 7591     // We could not read 64-bits from a general purpose register thus we move
 7592     // data required to compose 64 1&#39;s to the instruction stream
 7593     // We emit 64 byte wide series of elements from 0..63 which later on would
 7594     // be used as a compare targets with tail count contained in tmp1 register.
 7595     // Result would be a k register having tmp1 consecutive number or 1
 7596     // counting from least significant bit.
 7597     address tmp = pc();
 7598     emit_int64(0x0706050403020100);
 7599     emit_int64(0x0F0E0D0C0B0A0908);
 7600     emit_int64(0x1716151413121110);
 7601     emit_int64(0x1F1E1D1C1B1A1918);
 7602     emit_int64(0x2726252423222120);
 7603     emit_int64(0x2F2E2D2C2B2A2928);
 7604     emit_int64(0x3736353433323130);
 7605     emit_int64(0x3F3E3D3C3B3A3938);
 7606 
 7607     bind(k_init);
 7608     lea(len, InternalAddress(tmp));
 7609     // create mask to test for negative byte inside a vector
 7610     evpbroadcastb(vec1, tmp1, Assembler::AVX_512bit);
 7611     evpcmpgtb(k3, vec1, Address(len, 0), Assembler::AVX_512bit);
 7612 
 7613 #endif
 7614     evpcmpgtb(k2, k3, vec2, Address(ary1, 0), Assembler::AVX_512bit);
 7615     ktestq(k2, k3);
 7616     jcc(Assembler::notZero, TRUE_LABEL);
 7617 
 7618     jmp(FALSE_LABEL);
 7619   } else {
 7620     movl(result, len); // copy
 7621 
 7622     if (UseAVX &gt;= 2 &amp;&amp; UseSSE &gt;= 2) {
 7623       // With AVX2, use 32-byte vector compare
 7624       Label COMPARE_WIDE_VECTORS, COMPARE_TAIL;
 7625 
 7626       // Compare 32-byte vectors
 7627       andl(result, 0x0000001f);  //   tail count (in bytes)
 7628       andl(len, 0xffffffe0);   // vector count (in bytes)
 7629       jccb(Assembler::zero, COMPARE_TAIL);
 7630 
 7631       lea(ary1, Address(ary1, len, Address::times_1));
 7632       negptr(len);
 7633 
 7634       movl(tmp1, 0x80808080);   // create mask to test for Unicode chars in vector
 7635       movdl(vec2, tmp1);
 7636       vpbroadcastd(vec2, vec2, Assembler::AVX_256bit);
 7637 
 7638       bind(COMPARE_WIDE_VECTORS);
 7639       vmovdqu(vec1, Address(ary1, len, Address::times_1));
 7640       vptest(vec1, vec2);
 7641       jccb(Assembler::notZero, TRUE_LABEL);
 7642       addptr(len, 32);
 7643       jcc(Assembler::notZero, COMPARE_WIDE_VECTORS);
 7644 
 7645       testl(result, result);
 7646       jccb(Assembler::zero, FALSE_LABEL);
 7647 
 7648       vmovdqu(vec1, Address(ary1, result, Address::times_1, -32));
 7649       vptest(vec1, vec2);
 7650       jccb(Assembler::notZero, TRUE_LABEL);
 7651       jmpb(FALSE_LABEL);
 7652 
 7653       bind(COMPARE_TAIL); // len is zero
 7654       movl(len, result);
 7655       // Fallthru to tail compare
 7656     } else if (UseSSE42Intrinsics) {
 7657       // With SSE4.2, use double quad vector compare
 7658       Label COMPARE_WIDE_VECTORS, COMPARE_TAIL;
 7659 
 7660       // Compare 16-byte vectors
 7661       andl(result, 0x0000000f);  //   tail count (in bytes)
 7662       andl(len, 0xfffffff0);   // vector count (in bytes)
 7663       jcc(Assembler::zero, COMPARE_TAIL);
 7664 
 7665       lea(ary1, Address(ary1, len, Address::times_1));
 7666       negptr(len);
 7667 
 7668       movl(tmp1, 0x80808080);
 7669       movdl(vec2, tmp1);
 7670       pshufd(vec2, vec2, 0);
 7671 
 7672       bind(COMPARE_WIDE_VECTORS);
 7673       movdqu(vec1, Address(ary1, len, Address::times_1));
 7674       ptest(vec1, vec2);
 7675       jcc(Assembler::notZero, TRUE_LABEL);
 7676       addptr(len, 16);
 7677       jcc(Assembler::notZero, COMPARE_WIDE_VECTORS);
 7678 
 7679       testl(result, result);
 7680       jcc(Assembler::zero, FALSE_LABEL);
 7681 
 7682       movdqu(vec1, Address(ary1, result, Address::times_1, -16));
 7683       ptest(vec1, vec2);
 7684       jccb(Assembler::notZero, TRUE_LABEL);
 7685       jmpb(FALSE_LABEL);
 7686 
 7687       bind(COMPARE_TAIL); // len is zero
 7688       movl(len, result);
 7689       // Fallthru to tail compare
 7690     }
 7691   }
 7692   // Compare 4-byte vectors
 7693   andl(len, 0xfffffffc); // vector count (in bytes)
 7694   jccb(Assembler::zero, COMPARE_CHAR);
 7695 
 7696   lea(ary1, Address(ary1, len, Address::times_1));
 7697   negptr(len);
 7698 
 7699   bind(COMPARE_VECTORS);
 7700   movl(tmp1, Address(ary1, len, Address::times_1));
 7701   andl(tmp1, 0x80808080);
 7702   jccb(Assembler::notZero, TRUE_LABEL);
 7703   addptr(len, 4);
 7704   jcc(Assembler::notZero, COMPARE_VECTORS);
 7705 
 7706   // Compare trailing char (final 2 bytes), if any
 7707   bind(COMPARE_CHAR);
 7708   testl(result, 0x2);   // tail  char
 7709   jccb(Assembler::zero, COMPARE_BYTE);
 7710   load_unsigned_short(tmp1, Address(ary1, 0));
 7711   andl(tmp1, 0x00008080);
 7712   jccb(Assembler::notZero, TRUE_LABEL);
 7713   subptr(result, 2);
 7714   lea(ary1, Address(ary1, 2));
 7715 
 7716   bind(COMPARE_BYTE);
 7717   testl(result, 0x1);   // tail  byte
 7718   jccb(Assembler::zero, FALSE_LABEL);
 7719   load_unsigned_byte(tmp1, Address(ary1, 0));
 7720   andl(tmp1, 0x00000080);
 7721   jccb(Assembler::notEqual, TRUE_LABEL);
 7722   jmpb(FALSE_LABEL);
 7723 
 7724   bind(TRUE_LABEL);
 7725   movl(result, 1);   // return true
 7726   jmpb(DONE);
 7727 
 7728   bind(FALSE_LABEL);
 7729   xorl(result, result); // return false
 7730 
 7731   // That&#39;s it
 7732   bind(DONE);
 7733   if (UseAVX &gt;= 2 &amp;&amp; UseSSE &gt;= 2) {
 7734     // clean upper bits of YMM registers
 7735     vpxor(vec1, vec1);
 7736     vpxor(vec2, vec2);
 7737   }
 7738 }
 7739 // Compare char[] or byte[] arrays aligned to 4 bytes or substrings.
 7740 void MacroAssembler::arrays_equals(bool is_array_equ, Register ary1, Register ary2,
 7741                                    Register limit, Register result, Register chr,
 7742                                    XMMRegister vec1, XMMRegister vec2, bool is_char) {
 7743   ShortBranchVerifier sbv(this);
 7744   Label TRUE_LABEL, FALSE_LABEL, DONE, COMPARE_VECTORS, COMPARE_CHAR, COMPARE_BYTE;
 7745 
 7746   int length_offset  = arrayOopDesc::length_offset_in_bytes();
 7747   int base_offset    = arrayOopDesc::base_offset_in_bytes(is_char ? T_CHAR : T_BYTE);
 7748 
 7749   if (is_array_equ) {
 7750     // Check the input args
 7751     cmpoop(ary1, ary2);
 7752     jcc(Assembler::equal, TRUE_LABEL);
 7753 
 7754     // Need additional checks for arrays_equals.
 7755     testptr(ary1, ary1);
 7756     jcc(Assembler::zero, FALSE_LABEL);
 7757     testptr(ary2, ary2);
 7758     jcc(Assembler::zero, FALSE_LABEL);
 7759 
 7760     // Check the lengths
 7761     movl(limit, Address(ary1, length_offset));
 7762     cmpl(limit, Address(ary2, length_offset));
 7763     jcc(Assembler::notEqual, FALSE_LABEL);
 7764   }
 7765 
 7766   // count == 0
 7767   testl(limit, limit);
 7768   jcc(Assembler::zero, TRUE_LABEL);
 7769 
 7770   if (is_array_equ) {
 7771     // Load array address
 7772     lea(ary1, Address(ary1, base_offset));
 7773     lea(ary2, Address(ary2, base_offset));
 7774   }
 7775 
 7776   if (is_array_equ &amp;&amp; is_char) {
 7777     // arrays_equals when used for char[].
 7778     shll(limit, 1);      // byte count != 0
 7779   }
 7780   movl(result, limit); // copy
 7781 
 7782   if (UseAVX &gt;= 2) {
 7783     // With AVX2, use 32-byte vector compare
 7784     Label COMPARE_WIDE_VECTORS, COMPARE_TAIL;
 7785 
 7786     // Compare 32-byte vectors
 7787     andl(result, 0x0000001f);  //   tail count (in bytes)
 7788     andl(limit, 0xffffffe0);   // vector count (in bytes)
 7789     jcc(Assembler::zero, COMPARE_TAIL);
 7790 
 7791     lea(ary1, Address(ary1, limit, Address::times_1));
 7792     lea(ary2, Address(ary2, limit, Address::times_1));
 7793     negptr(limit);
 7794 
 7795 #ifdef _LP64
 7796     if ((AVX3Threshold == 0) &amp;&amp; VM_Version::supports_avx512vlbw()) { // trying 64 bytes fast loop
 7797       Label COMPARE_WIDE_VECTORS_LOOP_AVX2, COMPARE_WIDE_VECTORS_LOOP_AVX3;
 7798 
 7799       cmpl(limit, -64);
 7800       jcc(Assembler::greater, COMPARE_WIDE_VECTORS_LOOP_AVX2);
 7801 
 7802       bind(COMPARE_WIDE_VECTORS_LOOP_AVX3); // the hottest loop
 7803 
 7804       evmovdquq(vec1, Address(ary1, limit, Address::times_1), Assembler::AVX_512bit);
 7805       evpcmpeqb(k7, vec1, Address(ary2, limit, Address::times_1), Assembler::AVX_512bit);
 7806       kortestql(k7, k7);
 7807       jcc(Assembler::aboveEqual, FALSE_LABEL);     // miscompare
 7808       addptr(limit, 64);  // update since we already compared at this addr
 7809       cmpl(limit, -64);
 7810       jccb(Assembler::lessEqual, COMPARE_WIDE_VECTORS_LOOP_AVX3);
 7811 
 7812       // At this point we may still need to compare -limit+result bytes.
 7813       // We could execute the next two instruction and just continue via non-wide path:
 7814       //  cmpl(limit, 0);
 7815       //  jcc(Assembler::equal, COMPARE_TAIL);  // true
 7816       // But since we stopped at the points ary{1,2}+limit which are
 7817       // not farther than 64 bytes from the ends of arrays ary{1,2}+result
 7818       // (|limit| &lt;= 32 and result &lt; 32),
 7819       // we may just compare the last 64 bytes.
 7820       //
 7821       addptr(result, -64);   // it is safe, bc we just came from this area
 7822       evmovdquq(vec1, Address(ary1, result, Address::times_1), Assembler::AVX_512bit);
 7823       evpcmpeqb(k7, vec1, Address(ary2, result, Address::times_1), Assembler::AVX_512bit);
 7824       kortestql(k7, k7);
 7825       jcc(Assembler::aboveEqual, FALSE_LABEL);     // miscompare
 7826 
 7827       jmp(TRUE_LABEL);
 7828 
 7829       bind(COMPARE_WIDE_VECTORS_LOOP_AVX2);
 7830 
 7831     }//if (VM_Version::supports_avx512vlbw())
 7832 #endif //_LP64
 7833     bind(COMPARE_WIDE_VECTORS);
 7834     vmovdqu(vec1, Address(ary1, limit, Address::times_1));
 7835     vmovdqu(vec2, Address(ary2, limit, Address::times_1));
 7836     vpxor(vec1, vec2);
 7837 
 7838     vptest(vec1, vec1);
 7839     jcc(Assembler::notZero, FALSE_LABEL);
 7840     addptr(limit, 32);
 7841     jcc(Assembler::notZero, COMPARE_WIDE_VECTORS);
 7842 
 7843     testl(result, result);
 7844     jcc(Assembler::zero, TRUE_LABEL);
 7845 
 7846     vmovdqu(vec1, Address(ary1, result, Address::times_1, -32));
 7847     vmovdqu(vec2, Address(ary2, result, Address::times_1, -32));
 7848     vpxor(vec1, vec2);
 7849 
 7850     vptest(vec1, vec1);
 7851     jccb(Assembler::notZero, FALSE_LABEL);
 7852     jmpb(TRUE_LABEL);
 7853 
 7854     bind(COMPARE_TAIL); // limit is zero
 7855     movl(limit, result);
 7856     // Fallthru to tail compare
 7857   } else if (UseSSE42Intrinsics) {
 7858     // With SSE4.2, use double quad vector compare
 7859     Label COMPARE_WIDE_VECTORS, COMPARE_TAIL;
 7860 
 7861     // Compare 16-byte vectors
 7862     andl(result, 0x0000000f);  //   tail count (in bytes)
 7863     andl(limit, 0xfffffff0);   // vector count (in bytes)
 7864     jcc(Assembler::zero, COMPARE_TAIL);
 7865 
 7866     lea(ary1, Address(ary1, limit, Address::times_1));
 7867     lea(ary2, Address(ary2, limit, Address::times_1));
 7868     negptr(limit);
 7869 
 7870     bind(COMPARE_WIDE_VECTORS);
 7871     movdqu(vec1, Address(ary1, limit, Address::times_1));
 7872     movdqu(vec2, Address(ary2, limit, Address::times_1));
 7873     pxor(vec1, vec2);
 7874 
 7875     ptest(vec1, vec1);
 7876     jcc(Assembler::notZero, FALSE_LABEL);
 7877     addptr(limit, 16);
 7878     jcc(Assembler::notZero, COMPARE_WIDE_VECTORS);
 7879 
 7880     testl(result, result);
 7881     jcc(Assembler::zero, TRUE_LABEL);
 7882 
 7883     movdqu(vec1, Address(ary1, result, Address::times_1, -16));
 7884     movdqu(vec2, Address(ary2, result, Address::times_1, -16));
 7885     pxor(vec1, vec2);
 7886 
 7887     ptest(vec1, vec1);
 7888     jccb(Assembler::notZero, FALSE_LABEL);
 7889     jmpb(TRUE_LABEL);
 7890 
 7891     bind(COMPARE_TAIL); // limit is zero
 7892     movl(limit, result);
 7893     // Fallthru to tail compare
 7894   }
 7895 
 7896   // Compare 4-byte vectors
 7897   andl(limit, 0xfffffffc); // vector count (in bytes)
 7898   jccb(Assembler::zero, COMPARE_CHAR);
 7899 
 7900   lea(ary1, Address(ary1, limit, Address::times_1));
 7901   lea(ary2, Address(ary2, limit, Address::times_1));
 7902   negptr(limit);
 7903 
 7904   bind(COMPARE_VECTORS);
 7905   movl(chr, Address(ary1, limit, Address::times_1));
 7906   cmpl(chr, Address(ary2, limit, Address::times_1));
 7907   jccb(Assembler::notEqual, FALSE_LABEL);
 7908   addptr(limit, 4);
 7909   jcc(Assembler::notZero, COMPARE_VECTORS);
 7910 
 7911   // Compare trailing char (final 2 bytes), if any
 7912   bind(COMPARE_CHAR);
 7913   testl(result, 0x2);   // tail  char
 7914   jccb(Assembler::zero, COMPARE_BYTE);
 7915   load_unsigned_short(chr, Address(ary1, 0));
 7916   load_unsigned_short(limit, Address(ary2, 0));
 7917   cmpl(chr, limit);
 7918   jccb(Assembler::notEqual, FALSE_LABEL);
 7919 
 7920   if (is_array_equ &amp;&amp; is_char) {
 7921     bind(COMPARE_BYTE);
 7922   } else {
 7923     lea(ary1, Address(ary1, 2));
 7924     lea(ary2, Address(ary2, 2));
 7925 
 7926     bind(COMPARE_BYTE);
 7927     testl(result, 0x1);   // tail  byte
 7928     jccb(Assembler::zero, TRUE_LABEL);
 7929     load_unsigned_byte(chr, Address(ary1, 0));
 7930     load_unsigned_byte(limit, Address(ary2, 0));
 7931     cmpl(chr, limit);
 7932     jccb(Assembler::notEqual, FALSE_LABEL);
 7933   }
 7934   bind(TRUE_LABEL);
 7935   movl(result, 1);   // return true
 7936   jmpb(DONE);
 7937 
 7938   bind(FALSE_LABEL);
 7939   xorl(result, result); // return false
 7940 
 7941   // That&#39;s it
 7942   bind(DONE);
 7943   if (UseAVX &gt;= 2) {
 7944     // clean upper bits of YMM registers
 7945     vpxor(vec1, vec1);
 7946     vpxor(vec2, vec2);
 7947   }
 7948 }
 7949 
 7950 #endif
 7951 
 7952 void MacroAssembler::generate_fill(BasicType t, bool aligned,
 7953                                    Register to, Register value, Register count,
 7954                                    Register rtmp, XMMRegister xtmp) {
 7955   ShortBranchVerifier sbv(this);
 7956   assert_different_registers(to, value, count, rtmp);
 7957   Label L_exit;
 7958   Label L_fill_2_bytes, L_fill_4_bytes;
 7959 
 7960   int shift = -1;
 7961   switch (t) {
 7962     case T_BYTE:
 7963       shift = 2;
 7964       break;
 7965     case T_SHORT:
 7966       shift = 1;
 7967       break;
 7968     case T_INT:
 7969       shift = 0;
 7970       break;
 7971     default: ShouldNotReachHere();
 7972   }
 7973 
 7974   if (t == T_BYTE) {
 7975     andl(value, 0xff);
 7976     movl(rtmp, value);
 7977     shll(rtmp, 8);
 7978     orl(value, rtmp);
 7979   }
 7980   if (t == T_SHORT) {
 7981     andl(value, 0xffff);
 7982   }
 7983   if (t == T_BYTE || t == T_SHORT) {
 7984     movl(rtmp, value);
 7985     shll(rtmp, 16);
 7986     orl(value, rtmp);
 7987   }
 7988 
 7989   cmpl(count, 2&lt;&lt;shift); // Short arrays (&lt; 8 bytes) fill by element
 7990   jcc(Assembler::below, L_fill_4_bytes); // use unsigned cmp
 7991   if (!UseUnalignedLoadStores &amp;&amp; !aligned &amp;&amp; (t == T_BYTE || t == T_SHORT)) {
 7992     Label L_skip_align2;
 7993     // align source address at 4 bytes address boundary
 7994     if (t == T_BYTE) {
 7995       Label L_skip_align1;
 7996       // One byte misalignment happens only for byte arrays
 7997       testptr(to, 1);
 7998       jccb(Assembler::zero, L_skip_align1);
 7999       movb(Address(to, 0), value);
 8000       increment(to);
 8001       decrement(count);
 8002       BIND(L_skip_align1);
 8003     }
 8004     // Two bytes misalignment happens only for byte and short (char) arrays
 8005     testptr(to, 2);
 8006     jccb(Assembler::zero, L_skip_align2);
 8007     movw(Address(to, 0), value);
 8008     addptr(to, 2);
 8009     subl(count, 1&lt;&lt;(shift-1));
 8010     BIND(L_skip_align2);
 8011   }
 8012   if (UseSSE &lt; 2) {
 8013     Label L_fill_32_bytes_loop, L_check_fill_8_bytes, L_fill_8_bytes_loop, L_fill_8_bytes;
 8014     // Fill 32-byte chunks
 8015     subl(count, 8 &lt;&lt; shift);
 8016     jcc(Assembler::less, L_check_fill_8_bytes);
 8017     align(16);
 8018 
 8019     BIND(L_fill_32_bytes_loop);
 8020 
 8021     for (int i = 0; i &lt; 32; i += 4) {
 8022       movl(Address(to, i), value);
 8023     }
 8024 
 8025     addptr(to, 32);
 8026     subl(count, 8 &lt;&lt; shift);
 8027     jcc(Assembler::greaterEqual, L_fill_32_bytes_loop);
 8028     BIND(L_check_fill_8_bytes);
 8029     addl(count, 8 &lt;&lt; shift);
 8030     jccb(Assembler::zero, L_exit);
 8031     jmpb(L_fill_8_bytes);
 8032 
 8033     //
 8034     // length is too short, just fill qwords
 8035     //
 8036     BIND(L_fill_8_bytes_loop);
 8037     movl(Address(to, 0), value);
 8038     movl(Address(to, 4), value);
 8039     addptr(to, 8);
 8040     BIND(L_fill_8_bytes);
 8041     subl(count, 1 &lt;&lt; (shift + 1));
 8042     jcc(Assembler::greaterEqual, L_fill_8_bytes_loop);
 8043     // fall through to fill 4 bytes
 8044   } else {
 8045     Label L_fill_32_bytes;
 8046     if (!UseUnalignedLoadStores) {
 8047       // align to 8 bytes, we know we are 4 byte aligned to start
 8048       testptr(to, 4);
 8049       jccb(Assembler::zero, L_fill_32_bytes);
 8050       movl(Address(to, 0), value);
 8051       addptr(to, 4);
 8052       subl(count, 1&lt;&lt;shift);
 8053     }
 8054     BIND(L_fill_32_bytes);
 8055     {
 8056       assert( UseSSE &gt;= 2, &quot;supported cpu only&quot; );
 8057       Label L_fill_32_bytes_loop, L_check_fill_8_bytes, L_fill_8_bytes_loop, L_fill_8_bytes;
 8058       movdl(xtmp, value);
 8059       if (UseAVX &gt;= 2 &amp;&amp; UseUnalignedLoadStores) {
 8060         Label L_check_fill_32_bytes;
 8061         if (UseAVX &gt; 2) {
 8062           // Fill 64-byte chunks
 8063           Label L_fill_64_bytes_loop_avx3, L_check_fill_64_bytes_avx2;
 8064 
 8065           // If number of bytes to fill &lt; AVX3Threshold, perform fill using AVX2
 8066           cmpl(count, AVX3Threshold);
 8067           jccb(Assembler::below, L_check_fill_64_bytes_avx2);
 8068 
 8069           vpbroadcastd(xtmp, xtmp, Assembler::AVX_512bit);
 8070 
 8071           subl(count, 16 &lt;&lt; shift);
 8072           jccb(Assembler::less, L_check_fill_32_bytes);
 8073           align(16);
 8074 
 8075           BIND(L_fill_64_bytes_loop_avx3);
 8076           evmovdqul(Address(to, 0), xtmp, Assembler::AVX_512bit);
 8077           addptr(to, 64);
 8078           subl(count, 16 &lt;&lt; shift);
 8079           jcc(Assembler::greaterEqual, L_fill_64_bytes_loop_avx3);
 8080           jmpb(L_check_fill_32_bytes);
 8081 
 8082           BIND(L_check_fill_64_bytes_avx2);
 8083         }
 8084         // Fill 64-byte chunks
 8085         Label L_fill_64_bytes_loop;
 8086         vpbroadcastd(xtmp, xtmp, Assembler::AVX_256bit);
 8087 
 8088         subl(count, 16 &lt;&lt; shift);
 8089         jcc(Assembler::less, L_check_fill_32_bytes);
 8090         align(16);
 8091 
 8092         BIND(L_fill_64_bytes_loop);
 8093         vmovdqu(Address(to, 0), xtmp);
 8094         vmovdqu(Address(to, 32), xtmp);
 8095         addptr(to, 64);
 8096         subl(count, 16 &lt;&lt; shift);
 8097         jcc(Assembler::greaterEqual, L_fill_64_bytes_loop);
 8098 
 8099         BIND(L_check_fill_32_bytes);
 8100         addl(count, 8 &lt;&lt; shift);
 8101         jccb(Assembler::less, L_check_fill_8_bytes);
 8102         vmovdqu(Address(to, 0), xtmp);
 8103         addptr(to, 32);
 8104         subl(count, 8 &lt;&lt; shift);
 8105 
 8106         BIND(L_check_fill_8_bytes);
 8107         // clean upper bits of YMM registers
 8108         movdl(xtmp, value);
 8109         pshufd(xtmp, xtmp, 0);
 8110       } else {
 8111         // Fill 32-byte chunks
 8112         pshufd(xtmp, xtmp, 0);
 8113 
 8114         subl(count, 8 &lt;&lt; shift);
 8115         jcc(Assembler::less, L_check_fill_8_bytes);
 8116         align(16);
 8117 
 8118         BIND(L_fill_32_bytes_loop);
 8119 
 8120         if (UseUnalignedLoadStores) {
 8121           movdqu(Address(to, 0), xtmp);
 8122           movdqu(Address(to, 16), xtmp);
 8123         } else {
 8124           movq(Address(to, 0), xtmp);
 8125           movq(Address(to, 8), xtmp);
 8126           movq(Address(to, 16), xtmp);
 8127           movq(Address(to, 24), xtmp);
 8128         }
 8129 
 8130         addptr(to, 32);
 8131         subl(count, 8 &lt;&lt; shift);
 8132         jcc(Assembler::greaterEqual, L_fill_32_bytes_loop);
 8133 
 8134         BIND(L_check_fill_8_bytes);
 8135       }
 8136       addl(count, 8 &lt;&lt; shift);
 8137       jccb(Assembler::zero, L_exit);
 8138       jmpb(L_fill_8_bytes);
 8139 
 8140       //
 8141       // length is too short, just fill qwords
 8142       //
 8143       BIND(L_fill_8_bytes_loop);
 8144       movq(Address(to, 0), xtmp);
 8145       addptr(to, 8);
 8146       BIND(L_fill_8_bytes);
 8147       subl(count, 1 &lt;&lt; (shift + 1));
 8148       jcc(Assembler::greaterEqual, L_fill_8_bytes_loop);
 8149     }
 8150   }
 8151   // fill trailing 4 bytes
 8152   BIND(L_fill_4_bytes);
 8153   testl(count, 1&lt;&lt;shift);
 8154   jccb(Assembler::zero, L_fill_2_bytes);
 8155   movl(Address(to, 0), value);
 8156   if (t == T_BYTE || t == T_SHORT) {
 8157     Label L_fill_byte;
 8158     addptr(to, 4);
 8159     BIND(L_fill_2_bytes);
 8160     // fill trailing 2 bytes
 8161     testl(count, 1&lt;&lt;(shift-1));
 8162     jccb(Assembler::zero, L_fill_byte);
 8163     movw(Address(to, 0), value);
 8164     if (t == T_BYTE) {
 8165       addptr(to, 2);
 8166       BIND(L_fill_byte);
 8167       // fill trailing byte
 8168       testl(count, 1);
 8169       jccb(Assembler::zero, L_exit);
 8170       movb(Address(to, 0), value);
 8171     } else {
 8172       BIND(L_fill_byte);
 8173     }
 8174   } else {
 8175     BIND(L_fill_2_bytes);
 8176   }
 8177   BIND(L_exit);
 8178 }
 8179 
 8180 // encode char[] to byte[] in ISO_8859_1
 8181    //@HotSpotIntrinsicCandidate
 8182    //private static int implEncodeISOArray(byte[] sa, int sp,
 8183    //byte[] da, int dp, int len) {
 8184    //  int i = 0;
 8185    //  for (; i &lt; len; i++) {
 8186    //    char c = StringUTF16.getChar(sa, sp++);
 8187    //    if (c &gt; &#39;\u00FF&#39;)
 8188    //      break;
 8189    //    da[dp++] = (byte)c;
 8190    //  }
 8191    //  return i;
 8192    //}
 8193 void MacroAssembler::encode_iso_array(Register src, Register dst, Register len,
 8194   XMMRegister tmp1Reg, XMMRegister tmp2Reg,
 8195   XMMRegister tmp3Reg, XMMRegister tmp4Reg,
 8196   Register tmp5, Register result) {
 8197 
 8198   // rsi: src
 8199   // rdi: dst
 8200   // rdx: len
 8201   // rcx: tmp5
 8202   // rax: result
 8203   ShortBranchVerifier sbv(this);
 8204   assert_different_registers(src, dst, len, tmp5, result);
 8205   Label L_done, L_copy_1_char, L_copy_1_char_exit;
 8206 
 8207   // set result
 8208   xorl(result, result);
 8209   // check for zero length
 8210   testl(len, len);
 8211   jcc(Assembler::zero, L_done);
 8212 
 8213   movl(result, len);
 8214 
 8215   // Setup pointers
 8216   lea(src, Address(src, len, Address::times_2)); // char[]
 8217   lea(dst, Address(dst, len, Address::times_1)); // byte[]
 8218   negptr(len);
 8219 
 8220   if (UseSSE42Intrinsics || UseAVX &gt;= 2) {
 8221     Label L_copy_8_chars, L_copy_8_chars_exit;
 8222     Label L_chars_16_check, L_copy_16_chars, L_copy_16_chars_exit;
 8223 
 8224     if (UseAVX &gt;= 2) {
 8225       Label L_chars_32_check, L_copy_32_chars, L_copy_32_chars_exit;
 8226       movl(tmp5, 0xff00ff00);   // create mask to test for Unicode chars in vector
 8227       movdl(tmp1Reg, tmp5);
 8228       vpbroadcastd(tmp1Reg, tmp1Reg, Assembler::AVX_256bit);
 8229       jmp(L_chars_32_check);
 8230 
 8231       bind(L_copy_32_chars);
 8232       vmovdqu(tmp3Reg, Address(src, len, Address::times_2, -64));
 8233       vmovdqu(tmp4Reg, Address(src, len, Address::times_2, -32));
 8234       vpor(tmp2Reg, tmp3Reg, tmp4Reg, /* vector_len */ 1);
 8235       vptest(tmp2Reg, tmp1Reg);       // check for Unicode chars in  vector
 8236       jccb(Assembler::notZero, L_copy_32_chars_exit);
 8237       vpackuswb(tmp3Reg, tmp3Reg, tmp4Reg, /* vector_len */ 1);
 8238       vpermq(tmp4Reg, tmp3Reg, 0xD8, /* vector_len */ 1);
 8239       vmovdqu(Address(dst, len, Address::times_1, -32), tmp4Reg);
 8240 
 8241       bind(L_chars_32_check);
 8242       addptr(len, 32);
 8243       jcc(Assembler::lessEqual, L_copy_32_chars);
 8244 
 8245       bind(L_copy_32_chars_exit);
 8246       subptr(len, 16);
 8247       jccb(Assembler::greater, L_copy_16_chars_exit);
 8248 
 8249     } else if (UseSSE42Intrinsics) {
 8250       movl(tmp5, 0xff00ff00);   // create mask to test for Unicode chars in vector
 8251       movdl(tmp1Reg, tmp5);
 8252       pshufd(tmp1Reg, tmp1Reg, 0);
 8253       jmpb(L_chars_16_check);
 8254     }
 8255 
 8256     bind(L_copy_16_chars);
 8257     if (UseAVX &gt;= 2) {
 8258       vmovdqu(tmp2Reg, Address(src, len, Address::times_2, -32));
 8259       vptest(tmp2Reg, tmp1Reg);
 8260       jcc(Assembler::notZero, L_copy_16_chars_exit);
 8261       vpackuswb(tmp2Reg, tmp2Reg, tmp1Reg, /* vector_len */ 1);
 8262       vpermq(tmp3Reg, tmp2Reg, 0xD8, /* vector_len */ 1);
 8263     } else {
 8264       if (UseAVX &gt; 0) {
 8265         movdqu(tmp3Reg, Address(src, len, Address::times_2, -32));
 8266         movdqu(tmp4Reg, Address(src, len, Address::times_2, -16));
 8267         vpor(tmp2Reg, tmp3Reg, tmp4Reg, /* vector_len */ 0);
 8268       } else {
 8269         movdqu(tmp3Reg, Address(src, len, Address::times_2, -32));
 8270         por(tmp2Reg, tmp3Reg);
 8271         movdqu(tmp4Reg, Address(src, len, Address::times_2, -16));
 8272         por(tmp2Reg, tmp4Reg);
 8273       }
 8274       ptest(tmp2Reg, tmp1Reg);       // check for Unicode chars in  vector
 8275       jccb(Assembler::notZero, L_copy_16_chars_exit);
 8276       packuswb(tmp3Reg, tmp4Reg);
 8277     }
 8278     movdqu(Address(dst, len, Address::times_1, -16), tmp3Reg);
 8279 
 8280     bind(L_chars_16_check);
 8281     addptr(len, 16);
 8282     jcc(Assembler::lessEqual, L_copy_16_chars);
 8283 
 8284     bind(L_copy_16_chars_exit);
 8285     if (UseAVX &gt;= 2) {
 8286       // clean upper bits of YMM registers
 8287       vpxor(tmp2Reg, tmp2Reg);
 8288       vpxor(tmp3Reg, tmp3Reg);
 8289       vpxor(tmp4Reg, tmp4Reg);
 8290       movdl(tmp1Reg, tmp5);
 8291       pshufd(tmp1Reg, tmp1Reg, 0);
 8292     }
 8293     subptr(len, 8);
 8294     jccb(Assembler::greater, L_copy_8_chars_exit);
 8295 
 8296     bind(L_copy_8_chars);
 8297     movdqu(tmp3Reg, Address(src, len, Address::times_2, -16));
 8298     ptest(tmp3Reg, tmp1Reg);
 8299     jccb(Assembler::notZero, L_copy_8_chars_exit);
 8300     packuswb(tmp3Reg, tmp1Reg);
 8301     movq(Address(dst, len, Address::times_1, -8), tmp3Reg);
 8302     addptr(len, 8);
 8303     jccb(Assembler::lessEqual, L_copy_8_chars);
 8304 
 8305     bind(L_copy_8_chars_exit);
 8306     subptr(len, 8);
 8307     jccb(Assembler::zero, L_done);
 8308   }
 8309 
 8310   bind(L_copy_1_char);
 8311   load_unsigned_short(tmp5, Address(src, len, Address::times_2, 0));
 8312   testl(tmp5, 0xff00);      // check if Unicode char
 8313   jccb(Assembler::notZero, L_copy_1_char_exit);
 8314   movb(Address(dst, len, Address::times_1, 0), tmp5);
 8315   addptr(len, 1);
 8316   jccb(Assembler::less, L_copy_1_char);
 8317 
 8318   bind(L_copy_1_char_exit);
 8319   addptr(result, len); // len is negative count of not processed elements
 8320 
 8321   bind(L_done);
 8322 }
 8323 
 8324 #ifdef _LP64
 8325 /**
 8326  * Helper for multiply_to_len().
 8327  */
 8328 void MacroAssembler::add2_with_carry(Register dest_hi, Register dest_lo, Register src1, Register src2) {
 8329   addq(dest_lo, src1);
 8330   adcq(dest_hi, 0);
 8331   addq(dest_lo, src2);
 8332   adcq(dest_hi, 0);
 8333 }
 8334 
 8335 /**
 8336  * Multiply 64 bit by 64 bit first loop.
 8337  */
 8338 void MacroAssembler::multiply_64_x_64_loop(Register x, Register xstart, Register x_xstart,
 8339                                            Register y, Register y_idx, Register z,
 8340                                            Register carry, Register product,
 8341                                            Register idx, Register kdx) {
 8342   //
 8343   //  jlong carry, x[], y[], z[];
 8344   //  for (int idx=ystart, kdx=ystart+1+xstart; idx &gt;= 0; idx-, kdx--) {
 8345   //    huge_128 product = y[idx] * x[xstart] + carry;
 8346   //    z[kdx] = (jlong)product;
 8347   //    carry  = (jlong)(product &gt;&gt;&gt; 64);
 8348   //  }
 8349   //  z[xstart] = carry;
 8350   //
 8351 
 8352   Label L_first_loop, L_first_loop_exit;
 8353   Label L_one_x, L_one_y, L_multiply;
 8354 
 8355   decrementl(xstart);
 8356   jcc(Assembler::negative, L_one_x);
 8357 
 8358   movq(x_xstart, Address(x, xstart, Address::times_4,  0));
 8359   rorq(x_xstart, 32); // convert big-endian to little-endian
 8360 
 8361   bind(L_first_loop);
 8362   decrementl(idx);
 8363   jcc(Assembler::negative, L_first_loop_exit);
 8364   decrementl(idx);
 8365   jcc(Assembler::negative, L_one_y);
 8366   movq(y_idx, Address(y, idx, Address::times_4,  0));
 8367   rorq(y_idx, 32); // convert big-endian to little-endian
 8368   bind(L_multiply);
 8369   movq(product, x_xstart);
 8370   mulq(y_idx); // product(rax) * y_idx -&gt; rdx:rax
 8371   addq(product, carry);
 8372   adcq(rdx, 0);
 8373   subl(kdx, 2);
 8374   movl(Address(z, kdx, Address::times_4,  4), product);
 8375   shrq(product, 32);
 8376   movl(Address(z, kdx, Address::times_4,  0), product);
 8377   movq(carry, rdx);
 8378   jmp(L_first_loop);
 8379 
 8380   bind(L_one_y);
 8381   movl(y_idx, Address(y,  0));
 8382   jmp(L_multiply);
 8383 
 8384   bind(L_one_x);
 8385   movl(x_xstart, Address(x,  0));
 8386   jmp(L_first_loop);
 8387 
 8388   bind(L_first_loop_exit);
 8389 }
 8390 
 8391 /**
 8392  * Multiply 64 bit by 64 bit and add 128 bit.
 8393  */
 8394 void MacroAssembler::multiply_add_128_x_128(Register x_xstart, Register y, Register z,
 8395                                             Register yz_idx, Register idx,
 8396                                             Register carry, Register product, int offset) {
 8397   //     huge_128 product = (y[idx] * x_xstart) + z[kdx] + carry;
 8398   //     z[kdx] = (jlong)product;
 8399 
 8400   movq(yz_idx, Address(y, idx, Address::times_4,  offset));
 8401   rorq(yz_idx, 32); // convert big-endian to little-endian
 8402   movq(product, x_xstart);
 8403   mulq(yz_idx);     // product(rax) * yz_idx -&gt; rdx:product(rax)
 8404   movq(yz_idx, Address(z, idx, Address::times_4,  offset));
 8405   rorq(yz_idx, 32); // convert big-endian to little-endian
 8406 
 8407   add2_with_carry(rdx, product, carry, yz_idx);
 8408 
 8409   movl(Address(z, idx, Address::times_4,  offset+4), product);
 8410   shrq(product, 32);
 8411   movl(Address(z, idx, Address::times_4,  offset), product);
 8412 
 8413 }
 8414 
 8415 /**
 8416  * Multiply 128 bit by 128 bit. Unrolled inner loop.
 8417  */
 8418 void MacroAssembler::multiply_128_x_128_loop(Register x_xstart, Register y, Register z,
 8419                                              Register yz_idx, Register idx, Register jdx,
 8420                                              Register carry, Register product,
 8421                                              Register carry2) {
 8422   //   jlong carry, x[], y[], z[];
 8423   //   int kdx = ystart+1;
 8424   //   for (int idx=ystart-2; idx &gt;= 0; idx -= 2) { // Third loop
 8425   //     huge_128 product = (y[idx+1] * x_xstart) + z[kdx+idx+1] + carry;
 8426   //     z[kdx+idx+1] = (jlong)product;
 8427   //     jlong carry2  = (jlong)(product &gt;&gt;&gt; 64);
 8428   //     product = (y[idx] * x_xstart) + z[kdx+idx] + carry2;
 8429   //     z[kdx+idx] = (jlong)product;
 8430   //     carry  = (jlong)(product &gt;&gt;&gt; 64);
 8431   //   }
 8432   //   idx += 2;
 8433   //   if (idx &gt; 0) {
 8434   //     product = (y[idx] * x_xstart) + z[kdx+idx] + carry;
 8435   //     z[kdx+idx] = (jlong)product;
 8436   //     carry  = (jlong)(product &gt;&gt;&gt; 64);
 8437   //   }
 8438   //
 8439 
 8440   Label L_third_loop, L_third_loop_exit, L_post_third_loop_done;
 8441 
 8442   movl(jdx, idx);
 8443   andl(jdx, 0xFFFFFFFC);
 8444   shrl(jdx, 2);
 8445 
 8446   bind(L_third_loop);
 8447   subl(jdx, 1);
 8448   jcc(Assembler::negative, L_third_loop_exit);
 8449   subl(idx, 4);
 8450 
 8451   multiply_add_128_x_128(x_xstart, y, z, yz_idx, idx, carry, product, 8);
 8452   movq(carry2, rdx);
 8453 
 8454   multiply_add_128_x_128(x_xstart, y, z, yz_idx, idx, carry2, product, 0);
 8455   movq(carry, rdx);
 8456   jmp(L_third_loop);
 8457 
 8458   bind (L_third_loop_exit);
 8459 
 8460   andl (idx, 0x3);
 8461   jcc(Assembler::zero, L_post_third_loop_done);
 8462 
 8463   Label L_check_1;
 8464   subl(idx, 2);
 8465   jcc(Assembler::negative, L_check_1);
 8466 
 8467   multiply_add_128_x_128(x_xstart, y, z, yz_idx, idx, carry, product, 0);
 8468   movq(carry, rdx);
 8469 
 8470   bind (L_check_1);
 8471   addl (idx, 0x2);
 8472   andl (idx, 0x1);
 8473   subl(idx, 1);
 8474   jcc(Assembler::negative, L_post_third_loop_done);
 8475 
 8476   movl(yz_idx, Address(y, idx, Address::times_4,  0));
 8477   movq(product, x_xstart);
 8478   mulq(yz_idx); // product(rax) * yz_idx -&gt; rdx:product(rax)
 8479   movl(yz_idx, Address(z, idx, Address::times_4,  0));
 8480 
 8481   add2_with_carry(rdx, product, yz_idx, carry);
 8482 
 8483   movl(Address(z, idx, Address::times_4,  0), product);
 8484   shrq(product, 32);
 8485 
 8486   shlq(rdx, 32);
 8487   orq(product, rdx);
 8488   movq(carry, product);
 8489 
 8490   bind(L_post_third_loop_done);
 8491 }
 8492 
 8493 /**
 8494  * Multiply 128 bit by 128 bit using BMI2. Unrolled inner loop.
 8495  *
 8496  */
 8497 void MacroAssembler::multiply_128_x_128_bmi2_loop(Register y, Register z,
 8498                                                   Register carry, Register carry2,
 8499                                                   Register idx, Register jdx,
 8500                                                   Register yz_idx1, Register yz_idx2,
 8501                                                   Register tmp, Register tmp3, Register tmp4) {
 8502   assert(UseBMI2Instructions, &quot;should be used only when BMI2 is available&quot;);
 8503 
 8504   //   jlong carry, x[], y[], z[];
 8505   //   int kdx = ystart+1;
 8506   //   for (int idx=ystart-2; idx &gt;= 0; idx -= 2) { // Third loop
 8507   //     huge_128 tmp3 = (y[idx+1] * rdx) + z[kdx+idx+1] + carry;
 8508   //     jlong carry2  = (jlong)(tmp3 &gt;&gt;&gt; 64);
 8509   //     huge_128 tmp4 = (y[idx]   * rdx) + z[kdx+idx] + carry2;
 8510   //     carry  = (jlong)(tmp4 &gt;&gt;&gt; 64);
 8511   //     z[kdx+idx+1] = (jlong)tmp3;
 8512   //     z[kdx+idx] = (jlong)tmp4;
 8513   //   }
 8514   //   idx += 2;
 8515   //   if (idx &gt; 0) {
 8516   //     yz_idx1 = (y[idx] * rdx) + z[kdx+idx] + carry;
 8517   //     z[kdx+idx] = (jlong)yz_idx1;
 8518   //     carry  = (jlong)(yz_idx1 &gt;&gt;&gt; 64);
 8519   //   }
 8520   //
 8521 
 8522   Label L_third_loop, L_third_loop_exit, L_post_third_loop_done;
 8523 
 8524   movl(jdx, idx);
 8525   andl(jdx, 0xFFFFFFFC);
 8526   shrl(jdx, 2);
 8527 
 8528   bind(L_third_loop);
 8529   subl(jdx, 1);
 8530   jcc(Assembler::negative, L_third_loop_exit);
 8531   subl(idx, 4);
 8532 
 8533   movq(yz_idx1,  Address(y, idx, Address::times_4,  8));
 8534   rorxq(yz_idx1, yz_idx1, 32); // convert big-endian to little-endian
 8535   movq(yz_idx2, Address(y, idx, Address::times_4,  0));
 8536   rorxq(yz_idx2, yz_idx2, 32);
 8537 
 8538   mulxq(tmp4, tmp3, yz_idx1);  //  yz_idx1 * rdx -&gt; tmp4:tmp3
 8539   mulxq(carry2, tmp, yz_idx2); //  yz_idx2 * rdx -&gt; carry2:tmp
 8540 
 8541   movq(yz_idx1,  Address(z, idx, Address::times_4,  8));
 8542   rorxq(yz_idx1, yz_idx1, 32);
 8543   movq(yz_idx2, Address(z, idx, Address::times_4,  0));
 8544   rorxq(yz_idx2, yz_idx2, 32);
 8545 
 8546   if (VM_Version::supports_adx()) {
 8547     adcxq(tmp3, carry);
 8548     adoxq(tmp3, yz_idx1);
 8549 
 8550     adcxq(tmp4, tmp);
 8551     adoxq(tmp4, yz_idx2);
 8552 
 8553     movl(carry, 0); // does not affect flags
 8554     adcxq(carry2, carry);
 8555     adoxq(carry2, carry);
 8556   } else {
 8557     add2_with_carry(tmp4, tmp3, carry, yz_idx1);
 8558     add2_with_carry(carry2, tmp4, tmp, yz_idx2);
 8559   }
 8560   movq(carry, carry2);
 8561 
 8562   movl(Address(z, idx, Address::times_4, 12), tmp3);
 8563   shrq(tmp3, 32);
 8564   movl(Address(z, idx, Address::times_4,  8), tmp3);
 8565 
 8566   movl(Address(z, idx, Address::times_4,  4), tmp4);
 8567   shrq(tmp4, 32);
 8568   movl(Address(z, idx, Address::times_4,  0), tmp4);
 8569 
 8570   jmp(L_third_loop);
 8571 
 8572   bind (L_third_loop_exit);
 8573 
 8574   andl (idx, 0x3);
 8575   jcc(Assembler::zero, L_post_third_loop_done);
 8576 
 8577   Label L_check_1;
 8578   subl(idx, 2);
 8579   jcc(Assembler::negative, L_check_1);
 8580 
 8581   movq(yz_idx1, Address(y, idx, Address::times_4,  0));
 8582   rorxq(yz_idx1, yz_idx1, 32);
 8583   mulxq(tmp4, tmp3, yz_idx1); //  yz_idx1 * rdx -&gt; tmp4:tmp3
 8584   movq(yz_idx2, Address(z, idx, Address::times_4,  0));
 8585   rorxq(yz_idx2, yz_idx2, 32);
 8586 
 8587   add2_with_carry(tmp4, tmp3, carry, yz_idx2);
 8588 
 8589   movl(Address(z, idx, Address::times_4,  4), tmp3);
 8590   shrq(tmp3, 32);
 8591   movl(Address(z, idx, Address::times_4,  0), tmp3);
 8592   movq(carry, tmp4);
 8593 
 8594   bind (L_check_1);
 8595   addl (idx, 0x2);
 8596   andl (idx, 0x1);
 8597   subl(idx, 1);
 8598   jcc(Assembler::negative, L_post_third_loop_done);
 8599   movl(tmp4, Address(y, idx, Address::times_4,  0));
 8600   mulxq(carry2, tmp3, tmp4);  //  tmp4 * rdx -&gt; carry2:tmp3
 8601   movl(tmp4, Address(z, idx, Address::times_4,  0));
 8602 
 8603   add2_with_carry(carry2, tmp3, tmp4, carry);
 8604 
 8605   movl(Address(z, idx, Address::times_4,  0), tmp3);
 8606   shrq(tmp3, 32);
 8607 
 8608   shlq(carry2, 32);
 8609   orq(tmp3, carry2);
 8610   movq(carry, tmp3);
 8611 
 8612   bind(L_post_third_loop_done);
 8613 }
 8614 
 8615 /**
 8616  * Code for BigInteger::multiplyToLen() instrinsic.
 8617  *
 8618  * rdi: x
 8619  * rax: xlen
 8620  * rsi: y
 8621  * rcx: ylen
 8622  * r8:  z
 8623  * r11: zlen
 8624  * r12: tmp1
 8625  * r13: tmp2
 8626  * r14: tmp3
 8627  * r15: tmp4
 8628  * rbx: tmp5
 8629  *
 8630  */
 8631 void MacroAssembler::multiply_to_len(Register x, Register xlen, Register y, Register ylen, Register z, Register zlen,
 8632                                      Register tmp1, Register tmp2, Register tmp3, Register tmp4, Register tmp5) {
 8633   ShortBranchVerifier sbv(this);
 8634   assert_different_registers(x, xlen, y, ylen, z, zlen, tmp1, tmp2, tmp3, tmp4, tmp5, rdx);
 8635 
 8636   push(tmp1);
 8637   push(tmp2);
 8638   push(tmp3);
 8639   push(tmp4);
 8640   push(tmp5);
 8641 
 8642   push(xlen);
 8643   push(zlen);
 8644 
 8645   const Register idx = tmp1;
 8646   const Register kdx = tmp2;
 8647   const Register xstart = tmp3;
 8648 
 8649   const Register y_idx = tmp4;
 8650   const Register carry = tmp5;
 8651   const Register product  = xlen;
 8652   const Register x_xstart = zlen;  // reuse register
 8653 
 8654   // First Loop.
 8655   //
 8656   //  final static long LONG_MASK = 0xffffffffL;
 8657   //  int xstart = xlen - 1;
 8658   //  int ystart = ylen - 1;
 8659   //  long carry = 0;
 8660   //  for (int idx=ystart, kdx=ystart+1+xstart; idx &gt;= 0; idx-, kdx--) {
 8661   //    long product = (y[idx] &amp; LONG_MASK) * (x[xstart] &amp; LONG_MASK) + carry;
 8662   //    z[kdx] = (int)product;
 8663   //    carry = product &gt;&gt;&gt; 32;
 8664   //  }
 8665   //  z[xstart] = (int)carry;
 8666   //
 8667 
 8668   movl(idx, ylen);      // idx = ylen;
 8669   movl(kdx, zlen);      // kdx = xlen+ylen;
 8670   xorq(carry, carry);   // carry = 0;
 8671 
 8672   Label L_done;
 8673 
 8674   movl(xstart, xlen);
 8675   decrementl(xstart);
 8676   jcc(Assembler::negative, L_done);
 8677 
 8678   multiply_64_x_64_loop(x, xstart, x_xstart, y, y_idx, z, carry, product, idx, kdx);
 8679 
 8680   Label L_second_loop;
 8681   testl(kdx, kdx);
 8682   jcc(Assembler::zero, L_second_loop);
 8683 
 8684   Label L_carry;
 8685   subl(kdx, 1);
 8686   jcc(Assembler::zero, L_carry);
 8687 
 8688   movl(Address(z, kdx, Address::times_4,  0), carry);
 8689   shrq(carry, 32);
 8690   subl(kdx, 1);
 8691 
 8692   bind(L_carry);
 8693   movl(Address(z, kdx, Address::times_4,  0), carry);
 8694 
 8695   // Second and third (nested) loops.
 8696   //
 8697   // for (int i = xstart-1; i &gt;= 0; i--) { // Second loop
 8698   //   carry = 0;
 8699   //   for (int jdx=ystart, k=ystart+1+i; jdx &gt;= 0; jdx--, k--) { // Third loop
 8700   //     long product = (y[jdx] &amp; LONG_MASK) * (x[i] &amp; LONG_MASK) +
 8701   //                    (z[k] &amp; LONG_MASK) + carry;
 8702   //     z[k] = (int)product;
 8703   //     carry = product &gt;&gt;&gt; 32;
 8704   //   }
 8705   //   z[i] = (int)carry;
 8706   // }
 8707   //
 8708   // i = xlen, j = tmp1, k = tmp2, carry = tmp5, x[i] = rdx
 8709 
 8710   const Register jdx = tmp1;
 8711 
 8712   bind(L_second_loop);
 8713   xorl(carry, carry);    // carry = 0;
 8714   movl(jdx, ylen);       // j = ystart+1
 8715 
 8716   subl(xstart, 1);       // i = xstart-1;
 8717   jcc(Assembler::negative, L_done);
 8718 
 8719   push (z);
 8720 
 8721   Label L_last_x;
 8722   lea(z, Address(z, xstart, Address::times_4, 4)); // z = z + k - j
 8723   subl(xstart, 1);       // i = xstart-1;
 8724   jcc(Assembler::negative, L_last_x);
 8725 
 8726   if (UseBMI2Instructions) {
 8727     movq(rdx,  Address(x, xstart, Address::times_4,  0));
 8728     rorxq(rdx, rdx, 32); // convert big-endian to little-endian
 8729   } else {
 8730     movq(x_xstart, Address(x, xstart, Address::times_4,  0));
 8731     rorq(x_xstart, 32);  // convert big-endian to little-endian
 8732   }
 8733 
 8734   Label L_third_loop_prologue;
 8735   bind(L_third_loop_prologue);
 8736 
 8737   push (x);
 8738   push (xstart);
 8739   push (ylen);
 8740 
 8741 
 8742   if (UseBMI2Instructions) {
 8743     multiply_128_x_128_bmi2_loop(y, z, carry, x, jdx, ylen, product, tmp2, x_xstart, tmp3, tmp4);
 8744   } else { // !UseBMI2Instructions
 8745     multiply_128_x_128_loop(x_xstart, y, z, y_idx, jdx, ylen, carry, product, x);
 8746   }
 8747 
 8748   pop(ylen);
 8749   pop(xlen);
 8750   pop(x);
 8751   pop(z);
 8752 
 8753   movl(tmp3, xlen);
 8754   addl(tmp3, 1);
 8755   movl(Address(z, tmp3, Address::times_4,  0), carry);
 8756   subl(tmp3, 1);
 8757   jccb(Assembler::negative, L_done);
 8758 
 8759   shrq(carry, 32);
 8760   movl(Address(z, tmp3, Address::times_4,  0), carry);
 8761   jmp(L_second_loop);
 8762 
 8763   // Next infrequent code is moved outside loops.
 8764   bind(L_last_x);
 8765   if (UseBMI2Instructions) {
 8766     movl(rdx, Address(x,  0));
 8767   } else {
 8768     movl(x_xstart, Address(x,  0));
 8769   }
 8770   jmp(L_third_loop_prologue);
 8771 
 8772   bind(L_done);
 8773 
 8774   pop(zlen);
 8775   pop(xlen);
 8776 
 8777   pop(tmp5);
 8778   pop(tmp4);
 8779   pop(tmp3);
 8780   pop(tmp2);
 8781   pop(tmp1);
 8782 }
 8783 
 8784 void MacroAssembler::vectorized_mismatch(Register obja, Register objb, Register length, Register log2_array_indxscale,
 8785   Register result, Register tmp1, Register tmp2, XMMRegister rymm0, XMMRegister rymm1, XMMRegister rymm2){
 8786   assert(UseSSE42Intrinsics, &quot;SSE4.2 must be enabled.&quot;);
 8787   Label VECTOR16_LOOP, VECTOR8_LOOP, VECTOR4_LOOP;
 8788   Label VECTOR8_TAIL, VECTOR4_TAIL;
 8789   Label VECTOR32_NOT_EQUAL, VECTOR16_NOT_EQUAL, VECTOR8_NOT_EQUAL, VECTOR4_NOT_EQUAL;
 8790   Label SAME_TILL_END, DONE;
 8791   Label BYTES_LOOP, BYTES_TAIL, BYTES_NOT_EQUAL;
 8792 
 8793   //scale is in rcx in both Win64 and Unix
 8794   ShortBranchVerifier sbv(this);
 8795 
 8796   shlq(length);
 8797   xorq(result, result);
 8798 
 8799   if ((AVX3Threshold == 0) &amp;&amp; (UseAVX &gt; 2) &amp;&amp;
 8800       VM_Version::supports_avx512vlbw()) {
 8801     Label VECTOR64_LOOP, VECTOR64_NOT_EQUAL, VECTOR32_TAIL;
 8802 
 8803     cmpq(length, 64);
 8804     jcc(Assembler::less, VECTOR32_TAIL);
 8805 
 8806     movq(tmp1, length);
 8807     andq(tmp1, 0x3F);      // tail count
 8808     andq(length, ~(0x3F)); //vector count
 8809 
 8810     bind(VECTOR64_LOOP);
 8811     // AVX512 code to compare 64 byte vectors.
 8812     evmovdqub(rymm0, Address(obja, result), Assembler::AVX_512bit);
 8813     evpcmpeqb(k7, rymm0, Address(objb, result), Assembler::AVX_512bit);
 8814     kortestql(k7, k7);
 8815     jcc(Assembler::aboveEqual, VECTOR64_NOT_EQUAL);     // mismatch
 8816     addq(result, 64);
 8817     subq(length, 64);
 8818     jccb(Assembler::notZero, VECTOR64_LOOP);
 8819 
 8820     //bind(VECTOR64_TAIL);
 8821     testq(tmp1, tmp1);
 8822     jcc(Assembler::zero, SAME_TILL_END);
 8823 
 8824     //bind(VECTOR64_TAIL);
 8825     // AVX512 code to compare upto 63 byte vectors.
 8826     mov64(tmp2, 0xFFFFFFFFFFFFFFFF);
 8827     shlxq(tmp2, tmp2, tmp1);
 8828     notq(tmp2);
 8829     kmovql(k3, tmp2);
 8830 
 8831     evmovdqub(rymm0, k3, Address(obja, result), Assembler::AVX_512bit);
 8832     evpcmpeqb(k7, k3, rymm0, Address(objb, result), Assembler::AVX_512bit);
 8833 
 8834     ktestql(k7, k3);
 8835     jcc(Assembler::below, SAME_TILL_END);     // not mismatch
 8836 
 8837     bind(VECTOR64_NOT_EQUAL);
 8838     kmovql(tmp1, k7);
 8839     notq(tmp1);
 8840     tzcntq(tmp1, tmp1);
 8841     addq(result, tmp1);
 8842     shrq(result);
 8843     jmp(DONE);
 8844     bind(VECTOR32_TAIL);
 8845   }
 8846 
 8847   cmpq(length, 8);
 8848   jcc(Assembler::equal, VECTOR8_LOOP);
 8849   jcc(Assembler::less, VECTOR4_TAIL);
 8850 
 8851   if (UseAVX &gt;= 2) {
 8852     Label VECTOR16_TAIL, VECTOR32_LOOP;
 8853 
 8854     cmpq(length, 16);
 8855     jcc(Assembler::equal, VECTOR16_LOOP);
 8856     jcc(Assembler::less, VECTOR8_LOOP);
 8857 
 8858     cmpq(length, 32);
 8859     jccb(Assembler::less, VECTOR16_TAIL);
 8860 
 8861     subq(length, 32);
 8862     bind(VECTOR32_LOOP);
 8863     vmovdqu(rymm0, Address(obja, result));
 8864     vmovdqu(rymm1, Address(objb, result));
 8865     vpxor(rymm2, rymm0, rymm1, Assembler::AVX_256bit);
 8866     vptest(rymm2, rymm2);
 8867     jcc(Assembler::notZero, VECTOR32_NOT_EQUAL);//mismatch found
 8868     addq(result, 32);
 8869     subq(length, 32);
 8870     jcc(Assembler::greaterEqual, VECTOR32_LOOP);
 8871     addq(length, 32);
 8872     jcc(Assembler::equal, SAME_TILL_END);
 8873     //falling through if less than 32 bytes left //close the branch here.
 8874 
 8875     bind(VECTOR16_TAIL);
 8876     cmpq(length, 16);
 8877     jccb(Assembler::less, VECTOR8_TAIL);
 8878     bind(VECTOR16_LOOP);
 8879     movdqu(rymm0, Address(obja, result));
 8880     movdqu(rymm1, Address(objb, result));
 8881     vpxor(rymm2, rymm0, rymm1, Assembler::AVX_128bit);
 8882     ptest(rymm2, rymm2);
 8883     jcc(Assembler::notZero, VECTOR16_NOT_EQUAL);//mismatch found
 8884     addq(result, 16);
 8885     subq(length, 16);
 8886     jcc(Assembler::equal, SAME_TILL_END);
 8887     //falling through if less than 16 bytes left
 8888   } else {//regular intrinsics
 8889 
 8890     cmpq(length, 16);
 8891     jccb(Assembler::less, VECTOR8_TAIL);
 8892 
 8893     subq(length, 16);
 8894     bind(VECTOR16_LOOP);
 8895     movdqu(rymm0, Address(obja, result));
 8896     movdqu(rymm1, Address(objb, result));
 8897     pxor(rymm0, rymm1);
 8898     ptest(rymm0, rymm0);
 8899     jcc(Assembler::notZero, VECTOR16_NOT_EQUAL);//mismatch found
 8900     addq(result, 16);
 8901     subq(length, 16);
 8902     jccb(Assembler::greaterEqual, VECTOR16_LOOP);
 8903     addq(length, 16);
 8904     jcc(Assembler::equal, SAME_TILL_END);
 8905     //falling through if less than 16 bytes left
 8906   }
 8907 
 8908   bind(VECTOR8_TAIL);
 8909   cmpq(length, 8);
 8910   jccb(Assembler::less, VECTOR4_TAIL);
 8911   bind(VECTOR8_LOOP);
 8912   movq(tmp1, Address(obja, result));
 8913   movq(tmp2, Address(objb, result));
 8914   xorq(tmp1, tmp2);
 8915   testq(tmp1, tmp1);
 8916   jcc(Assembler::notZero, VECTOR8_NOT_EQUAL);//mismatch found
 8917   addq(result, 8);
 8918   subq(length, 8);
 8919   jcc(Assembler::equal, SAME_TILL_END);
 8920   //falling through if less than 8 bytes left
 8921 
 8922   bind(VECTOR4_TAIL);
 8923   cmpq(length, 4);
 8924   jccb(Assembler::less, BYTES_TAIL);
 8925   bind(VECTOR4_LOOP);
 8926   movl(tmp1, Address(obja, result));
 8927   xorl(tmp1, Address(objb, result));
 8928   testl(tmp1, tmp1);
 8929   jcc(Assembler::notZero, VECTOR4_NOT_EQUAL);//mismatch found
 8930   addq(result, 4);
 8931   subq(length, 4);
 8932   jcc(Assembler::equal, SAME_TILL_END);
 8933   //falling through if less than 4 bytes left
 8934 
 8935   bind(BYTES_TAIL);
 8936   bind(BYTES_LOOP);
 8937   load_unsigned_byte(tmp1, Address(obja, result));
 8938   load_unsigned_byte(tmp2, Address(objb, result));
 8939   xorl(tmp1, tmp2);
 8940   testl(tmp1, tmp1);
 8941   jcc(Assembler::notZero, BYTES_NOT_EQUAL);//mismatch found
 8942   decq(length);
 8943   jcc(Assembler::zero, SAME_TILL_END);
 8944   incq(result);
 8945   load_unsigned_byte(tmp1, Address(obja, result));
 8946   load_unsigned_byte(tmp2, Address(objb, result));
 8947   xorl(tmp1, tmp2);
 8948   testl(tmp1, tmp1);
 8949   jcc(Assembler::notZero, BYTES_NOT_EQUAL);//mismatch found
 8950   decq(length);
 8951   jcc(Assembler::zero, SAME_TILL_END);
 8952   incq(result);
 8953   load_unsigned_byte(tmp1, Address(obja, result));
 8954   load_unsigned_byte(tmp2, Address(objb, result));
 8955   xorl(tmp1, tmp2);
 8956   testl(tmp1, tmp1);
 8957   jcc(Assembler::notZero, BYTES_NOT_EQUAL);//mismatch found
 8958   jmp(SAME_TILL_END);
 8959 
 8960   if (UseAVX &gt;= 2) {
 8961     bind(VECTOR32_NOT_EQUAL);
 8962     vpcmpeqb(rymm2, rymm2, rymm2, Assembler::AVX_256bit);
 8963     vpcmpeqb(rymm0, rymm0, rymm1, Assembler::AVX_256bit);
 8964     vpxor(rymm0, rymm0, rymm2, Assembler::AVX_256bit);
 8965     vpmovmskb(tmp1, rymm0);
 8966     bsfq(tmp1, tmp1);
 8967     addq(result, tmp1);
 8968     shrq(result);
 8969     jmp(DONE);
 8970   }
 8971 
 8972   bind(VECTOR16_NOT_EQUAL);
 8973   if (UseAVX &gt;= 2) {
 8974     vpcmpeqb(rymm2, rymm2, rymm2, Assembler::AVX_128bit);
 8975     vpcmpeqb(rymm0, rymm0, rymm1, Assembler::AVX_128bit);
 8976     pxor(rymm0, rymm2);
 8977   } else {
 8978     pcmpeqb(rymm2, rymm2);
 8979     pxor(rymm0, rymm1);
 8980     pcmpeqb(rymm0, rymm1);
 8981     pxor(rymm0, rymm2);
 8982   }
 8983   pmovmskb(tmp1, rymm0);
 8984   bsfq(tmp1, tmp1);
 8985   addq(result, tmp1);
 8986   shrq(result);
 8987   jmpb(DONE);
 8988 
 8989   bind(VECTOR8_NOT_EQUAL);
 8990   bind(VECTOR4_NOT_EQUAL);
 8991   bsfq(tmp1, tmp1);
 8992   shrq(tmp1, 3);
 8993   addq(result, tmp1);
 8994   bind(BYTES_NOT_EQUAL);
 8995   shrq(result);
 8996   jmpb(DONE);
 8997 
 8998   bind(SAME_TILL_END);
 8999   mov64(result, -1);
 9000 
 9001   bind(DONE);
 9002 }
 9003 
 9004 //Helper functions for square_to_len()
 9005 
 9006 /**
 9007  * Store the squares of x[], right shifted one bit (divided by 2) into z[]
 9008  * Preserves x and z and modifies rest of the registers.
 9009  */
 9010 void MacroAssembler::square_rshift(Register x, Register xlen, Register z, Register tmp1, Register tmp3, Register tmp4, Register tmp5, Register rdxReg, Register raxReg) {
 9011   // Perform square and right shift by 1
 9012   // Handle odd xlen case first, then for even xlen do the following
 9013   // jlong carry = 0;
 9014   // for (int j=0, i=0; j &lt; xlen; j+=2, i+=4) {
 9015   //     huge_128 product = x[j:j+1] * x[j:j+1];
 9016   //     z[i:i+1] = (carry &lt;&lt; 63) | (jlong)(product &gt;&gt;&gt; 65);
 9017   //     z[i+2:i+3] = (jlong)(product &gt;&gt;&gt; 1);
 9018   //     carry = (jlong)product;
 9019   // }
 9020 
 9021   xorq(tmp5, tmp5);     // carry
 9022   xorq(rdxReg, rdxReg);
 9023   xorl(tmp1, tmp1);     // index for x
 9024   xorl(tmp4, tmp4);     // index for z
 9025 
 9026   Label L_first_loop, L_first_loop_exit;
 9027 
 9028   testl(xlen, 1);
 9029   jccb(Assembler::zero, L_first_loop); //jump if xlen is even
 9030 
 9031   // Square and right shift by 1 the odd element using 32 bit multiply
 9032   movl(raxReg, Address(x, tmp1, Address::times_4, 0));
 9033   imulq(raxReg, raxReg);
 9034   shrq(raxReg, 1);
 9035   adcq(tmp5, 0);
 9036   movq(Address(z, tmp4, Address::times_4, 0), raxReg);
 9037   incrementl(tmp1);
 9038   addl(tmp4, 2);
 9039 
 9040   // Square and  right shift by 1 the rest using 64 bit multiply
 9041   bind(L_first_loop);
 9042   cmpptr(tmp1, xlen);
 9043   jccb(Assembler::equal, L_first_loop_exit);
 9044 
 9045   // Square
 9046   movq(raxReg, Address(x, tmp1, Address::times_4,  0));
 9047   rorq(raxReg, 32);    // convert big-endian to little-endian
 9048   mulq(raxReg);        // 64-bit multiply rax * rax -&gt; rdx:rax
 9049 
 9050   // Right shift by 1 and save carry
 9051   shrq(tmp5, 1);       // rdx:rax:tmp5 = (tmp5:rdx:rax) &gt;&gt;&gt; 1
 9052   rcrq(rdxReg, 1);
 9053   rcrq(raxReg, 1);
 9054   adcq(tmp5, 0);
 9055 
 9056   // Store result in z
 9057   movq(Address(z, tmp4, Address::times_4, 0), rdxReg);
 9058   movq(Address(z, tmp4, Address::times_4, 8), raxReg);
 9059 
 9060   // Update indices for x and z
 9061   addl(tmp1, 2);
 9062   addl(tmp4, 4);
 9063   jmp(L_first_loop);
 9064 
 9065   bind(L_first_loop_exit);
 9066 }
 9067 
 9068 
 9069 /**
 9070  * Perform the following multiply add operation using BMI2 instructions
 9071  * carry:sum = sum + op1*op2 + carry
 9072  * op2 should be in rdx
 9073  * op2 is preserved, all other registers are modified
 9074  */
 9075 void MacroAssembler::multiply_add_64_bmi2(Register sum, Register op1, Register op2, Register carry, Register tmp2) {
 9076   // assert op2 is rdx
 9077   mulxq(tmp2, op1, op1);  //  op1 * op2 -&gt; tmp2:op1
 9078   addq(sum, carry);
 9079   adcq(tmp2, 0);
 9080   addq(sum, op1);
 9081   adcq(tmp2, 0);
 9082   movq(carry, tmp2);
 9083 }
 9084 
 9085 /**
 9086  * Perform the following multiply add operation:
 9087  * carry:sum = sum + op1*op2 + carry
 9088  * Preserves op1, op2 and modifies rest of registers
 9089  */
 9090 void MacroAssembler::multiply_add_64(Register sum, Register op1, Register op2, Register carry, Register rdxReg, Register raxReg) {
 9091   // rdx:rax = op1 * op2
 9092   movq(raxReg, op2);
 9093   mulq(op1);
 9094 
 9095   //  rdx:rax = sum + carry + rdx:rax
 9096   addq(sum, carry);
 9097   adcq(rdxReg, 0);
 9098   addq(sum, raxReg);
 9099   adcq(rdxReg, 0);
 9100 
 9101   // carry:sum = rdx:sum
 9102   movq(carry, rdxReg);
 9103 }
 9104 
 9105 /**
 9106  * Add 64 bit long carry into z[] with carry propogation.
 9107  * Preserves z and carry register values and modifies rest of registers.
 9108  *
 9109  */
 9110 void MacroAssembler::add_one_64(Register z, Register zlen, Register carry, Register tmp1) {
 9111   Label L_fourth_loop, L_fourth_loop_exit;
 9112 
 9113   movl(tmp1, 1);
 9114   subl(zlen, 2);
 9115   addq(Address(z, zlen, Address::times_4, 0), carry);
 9116 
 9117   bind(L_fourth_loop);
 9118   jccb(Assembler::carryClear, L_fourth_loop_exit);
 9119   subl(zlen, 2);
 9120   jccb(Assembler::negative, L_fourth_loop_exit);
 9121   addq(Address(z, zlen, Address::times_4, 0), tmp1);
 9122   jmp(L_fourth_loop);
 9123   bind(L_fourth_loop_exit);
 9124 }
 9125 
 9126 /**
 9127  * Shift z[] left by 1 bit.
 9128  * Preserves x, len, z and zlen registers and modifies rest of the registers.
 9129  *
 9130  */
 9131 void MacroAssembler::lshift_by_1(Register x, Register len, Register z, Register zlen, Register tmp1, Register tmp2, Register tmp3, Register tmp4) {
 9132 
 9133   Label L_fifth_loop, L_fifth_loop_exit;
 9134 
 9135   // Fifth loop
 9136   // Perform primitiveLeftShift(z, zlen, 1)
 9137 
 9138   const Register prev_carry = tmp1;
 9139   const Register new_carry = tmp4;
 9140   const Register value = tmp2;
 9141   const Register zidx = tmp3;
 9142 
 9143   // int zidx, carry;
 9144   // long value;
 9145   // carry = 0;
 9146   // for (zidx = zlen-2; zidx &gt;=0; zidx -= 2) {
 9147   //    (carry:value)  = (z[i] &lt;&lt; 1) | carry ;
 9148   //    z[i] = value;
 9149   // }
 9150 
 9151   movl(zidx, zlen);
 9152   xorl(prev_carry, prev_carry); // clear carry flag and prev_carry register
 9153 
 9154   bind(L_fifth_loop);
 9155   decl(zidx);  // Use decl to preserve carry flag
 9156   decl(zidx);
 9157   jccb(Assembler::negative, L_fifth_loop_exit);
 9158 
 9159   if (UseBMI2Instructions) {
 9160      movq(value, Address(z, zidx, Address::times_4, 0));
 9161      rclq(value, 1);
 9162      rorxq(value, value, 32);
 9163      movq(Address(z, zidx, Address::times_4,  0), value);  // Store back in big endian form
 9164   }
 9165   else {
 9166     // clear new_carry
 9167     xorl(new_carry, new_carry);
 9168 
 9169     // Shift z[i] by 1, or in previous carry and save new carry
 9170     movq(value, Address(z, zidx, Address::times_4, 0));
 9171     shlq(value, 1);
 9172     adcl(new_carry, 0);
 9173 
 9174     orq(value, prev_carry);
 9175     rorq(value, 0x20);
 9176     movq(Address(z, zidx, Address::times_4,  0), value);  // Store back in big endian form
 9177 
 9178     // Set previous carry = new carry
 9179     movl(prev_carry, new_carry);
 9180   }
 9181   jmp(L_fifth_loop);
 9182 
 9183   bind(L_fifth_loop_exit);
 9184 }
 9185 
 9186 
 9187 /**
 9188  * Code for BigInteger::squareToLen() intrinsic
 9189  *
 9190  * rdi: x
 9191  * rsi: len
 9192  * r8:  z
 9193  * rcx: zlen
 9194  * r12: tmp1
 9195  * r13: tmp2
 9196  * r14: tmp3
 9197  * r15: tmp4
 9198  * rbx: tmp5
 9199  *
 9200  */
 9201 void MacroAssembler::square_to_len(Register x, Register len, Register z, Register zlen, Register tmp1, Register tmp2, Register tmp3, Register tmp4, Register tmp5, Register rdxReg, Register raxReg) {
 9202 
 9203   Label L_second_loop, L_second_loop_exit, L_third_loop, L_third_loop_exit, L_last_x, L_multiply;
 9204   push(tmp1);
 9205   push(tmp2);
 9206   push(tmp3);
 9207   push(tmp4);
 9208   push(tmp5);
 9209 
 9210   // First loop
 9211   // Store the squares, right shifted one bit (i.e., divided by 2).
 9212   square_rshift(x, len, z, tmp1, tmp3, tmp4, tmp5, rdxReg, raxReg);
 9213 
 9214   // Add in off-diagonal sums.
 9215   //
 9216   // Second, third (nested) and fourth loops.
 9217   // zlen +=2;
 9218   // for (int xidx=len-2,zidx=zlen-4; xidx &gt; 0; xidx-=2,zidx-=4) {
 9219   //    carry = 0;
 9220   //    long op2 = x[xidx:xidx+1];
 9221   //    for (int j=xidx-2,k=zidx; j &gt;= 0; j-=2) {
 9222   //       k -= 2;
 9223   //       long op1 = x[j:j+1];
 9224   //       long sum = z[k:k+1];
 9225   //       carry:sum = multiply_add_64(sum, op1, op2, carry, tmp_regs);
 9226   //       z[k:k+1] = sum;
 9227   //    }
 9228   //    add_one_64(z, k, carry, tmp_regs);
 9229   // }
 9230 
 9231   const Register carry = tmp5;
 9232   const Register sum = tmp3;
 9233   const Register op1 = tmp4;
 9234   Register op2 = tmp2;
 9235 
 9236   push(zlen);
 9237   push(len);
 9238   addl(zlen,2);
 9239   bind(L_second_loop);
 9240   xorq(carry, carry);
 9241   subl(zlen, 4);
 9242   subl(len, 2);
 9243   push(zlen);
 9244   push(len);
 9245   cmpl(len, 0);
 9246   jccb(Assembler::lessEqual, L_second_loop_exit);
 9247 
 9248   // Multiply an array by one 64 bit long.
 9249   if (UseBMI2Instructions) {
 9250     op2 = rdxReg;
 9251     movq(op2, Address(x, len, Address::times_4,  0));
 9252     rorxq(op2, op2, 32);
 9253   }
 9254   else {
 9255     movq(op2, Address(x, len, Address::times_4,  0));
 9256     rorq(op2, 32);
 9257   }
 9258 
 9259   bind(L_third_loop);
 9260   decrementl(len);
 9261   jccb(Assembler::negative, L_third_loop_exit);
 9262   decrementl(len);
 9263   jccb(Assembler::negative, L_last_x);
 9264 
 9265   movq(op1, Address(x, len, Address::times_4,  0));
 9266   rorq(op1, 32);
 9267 
 9268   bind(L_multiply);
 9269   subl(zlen, 2);
 9270   movq(sum, Address(z, zlen, Address::times_4,  0));
 9271 
 9272   // Multiply 64 bit by 64 bit and add 64 bits lower half and upper 64 bits as carry.
 9273   if (UseBMI2Instructions) {
 9274     multiply_add_64_bmi2(sum, op1, op2, carry, tmp2);
 9275   }
 9276   else {
 9277     multiply_add_64(sum, op1, op2, carry, rdxReg, raxReg);
 9278   }
 9279 
 9280   movq(Address(z, zlen, Address::times_4, 0), sum);
 9281 
 9282   jmp(L_third_loop);
 9283   bind(L_third_loop_exit);
 9284 
 9285   // Fourth loop
 9286   // Add 64 bit long carry into z with carry propogation.
 9287   // Uses offsetted zlen.
 9288   add_one_64(z, zlen, carry, tmp1);
 9289 
 9290   pop(len);
 9291   pop(zlen);
 9292   jmp(L_second_loop);
 9293 
 9294   // Next infrequent code is moved outside loops.
 9295   bind(L_last_x);
 9296   movl(op1, Address(x, 0));
 9297   jmp(L_multiply);
 9298 
 9299   bind(L_second_loop_exit);
 9300   pop(len);
 9301   pop(zlen);
 9302   pop(len);
 9303   pop(zlen);
 9304 
 9305   // Fifth loop
 9306   // Shift z left 1 bit.
 9307   lshift_by_1(x, len, z, zlen, tmp1, tmp2, tmp3, tmp4);
 9308 
 9309   // z[zlen-1] |= x[len-1] &amp; 1;
 9310   movl(tmp3, Address(x, len, Address::times_4, -4));
 9311   andl(tmp3, 1);
 9312   orl(Address(z, zlen, Address::times_4,  -4), tmp3);
 9313 
 9314   pop(tmp5);
 9315   pop(tmp4);
 9316   pop(tmp3);
 9317   pop(tmp2);
 9318   pop(tmp1);
 9319 }
 9320 
 9321 /**
 9322  * Helper function for mul_add()
 9323  * Multiply the in[] by int k and add to out[] starting at offset offs using
 9324  * 128 bit by 32 bit multiply and return the carry in tmp5.
 9325  * Only quad int aligned length of in[] is operated on in this function.
 9326  * k is in rdxReg for BMI2Instructions, for others it is in tmp2.
 9327  * This function preserves out, in and k registers.
 9328  * len and offset point to the appropriate index in &quot;in&quot; &amp; &quot;out&quot; correspondingly
 9329  * tmp5 has the carry.
 9330  * other registers are temporary and are modified.
 9331  *
 9332  */
 9333 void MacroAssembler::mul_add_128_x_32_loop(Register out, Register in,
 9334   Register offset, Register len, Register tmp1, Register tmp2, Register tmp3,
 9335   Register tmp4, Register tmp5, Register rdxReg, Register raxReg) {
 9336 
 9337   Label L_first_loop, L_first_loop_exit;
 9338 
 9339   movl(tmp1, len);
 9340   shrl(tmp1, 2);
 9341 
 9342   bind(L_first_loop);
 9343   subl(tmp1, 1);
 9344   jccb(Assembler::negative, L_first_loop_exit);
 9345 
 9346   subl(len, 4);
 9347   subl(offset, 4);
 9348 
 9349   Register op2 = tmp2;
 9350   const Register sum = tmp3;
 9351   const Register op1 = tmp4;
 9352   const Register carry = tmp5;
 9353 
 9354   if (UseBMI2Instructions) {
 9355     op2 = rdxReg;
 9356   }
 9357 
 9358   movq(op1, Address(in, len, Address::times_4,  8));
 9359   rorq(op1, 32);
 9360   movq(sum, Address(out, offset, Address::times_4,  8));
 9361   rorq(sum, 32);
 9362   if (UseBMI2Instructions) {
 9363     multiply_add_64_bmi2(sum, op1, op2, carry, raxReg);
 9364   }
 9365   else {
 9366     multiply_add_64(sum, op1, op2, carry, rdxReg, raxReg);
 9367   }
 9368   // Store back in big endian from little endian
 9369   rorq(sum, 0x20);
 9370   movq(Address(out, offset, Address::times_4,  8), sum);
 9371 
 9372   movq(op1, Address(in, len, Address::times_4,  0));
 9373   rorq(op1, 32);
 9374   movq(sum, Address(out, offset, Address::times_4,  0));
 9375   rorq(sum, 32);
 9376   if (UseBMI2Instructions) {
 9377     multiply_add_64_bmi2(sum, op1, op2, carry, raxReg);
 9378   }
 9379   else {
 9380     multiply_add_64(sum, op1, op2, carry, rdxReg, raxReg);
 9381   }
 9382   // Store back in big endian from little endian
 9383   rorq(sum, 0x20);
 9384   movq(Address(out, offset, Address::times_4,  0), sum);
 9385 
 9386   jmp(L_first_loop);
 9387   bind(L_first_loop_exit);
 9388 }
 9389 
 9390 /**
 9391  * Code for BigInteger::mulAdd() intrinsic
 9392  *
 9393  * rdi: out
 9394  * rsi: in
 9395  * r11: offs (out.length - offset)
 9396  * rcx: len
 9397  * r8:  k
 9398  * r12: tmp1
 9399  * r13: tmp2
 9400  * r14: tmp3
 9401  * r15: tmp4
 9402  * rbx: tmp5
 9403  * Multiply the in[] by word k and add to out[], return the carry in rax
 9404  */
 9405 void MacroAssembler::mul_add(Register out, Register in, Register offs,
 9406    Register len, Register k, Register tmp1, Register tmp2, Register tmp3,
 9407    Register tmp4, Register tmp5, Register rdxReg, Register raxReg) {
 9408 
 9409   Label L_carry, L_last_in, L_done;
 9410 
 9411 // carry = 0;
 9412 // for (int j=len-1; j &gt;= 0; j--) {
 9413 //    long product = (in[j] &amp; LONG_MASK) * kLong +
 9414 //                   (out[offs] &amp; LONG_MASK) + carry;
 9415 //    out[offs--] = (int)product;
 9416 //    carry = product &gt;&gt;&gt; 32;
 9417 // }
 9418 //
 9419   push(tmp1);
 9420   push(tmp2);
 9421   push(tmp3);
 9422   push(tmp4);
 9423   push(tmp5);
 9424 
 9425   Register op2 = tmp2;
 9426   const Register sum = tmp3;
 9427   const Register op1 = tmp4;
 9428   const Register carry =  tmp5;
 9429 
 9430   if (UseBMI2Instructions) {
 9431     op2 = rdxReg;
 9432     movl(op2, k);
 9433   }
 9434   else {
 9435     movl(op2, k);
 9436   }
 9437 
 9438   xorq(carry, carry);
 9439 
 9440   //First loop
 9441 
 9442   //Multiply in[] by k in a 4 way unrolled loop using 128 bit by 32 bit multiply
 9443   //The carry is in tmp5
 9444   mul_add_128_x_32_loop(out, in, offs, len, tmp1, tmp2, tmp3, tmp4, tmp5, rdxReg, raxReg);
 9445 
 9446   //Multiply the trailing in[] entry using 64 bit by 32 bit, if any
 9447   decrementl(len);
 9448   jccb(Assembler::negative, L_carry);
 9449   decrementl(len);
 9450   jccb(Assembler::negative, L_last_in);
 9451 
 9452   movq(op1, Address(in, len, Address::times_4,  0));
 9453   rorq(op1, 32);
 9454 
 9455   subl(offs, 2);
 9456   movq(sum, Address(out, offs, Address::times_4,  0));
 9457   rorq(sum, 32);
 9458 
 9459   if (UseBMI2Instructions) {
 9460     multiply_add_64_bmi2(sum, op1, op2, carry, raxReg);
 9461   }
 9462   else {
 9463     multiply_add_64(sum, op1, op2, carry, rdxReg, raxReg);
 9464   }
 9465 
 9466   // Store back in big endian from little endian
 9467   rorq(sum, 0x20);
 9468   movq(Address(out, offs, Address::times_4,  0), sum);
 9469 
 9470   testl(len, len);
 9471   jccb(Assembler::zero, L_carry);
 9472 
 9473   //Multiply the last in[] entry, if any
 9474   bind(L_last_in);
 9475   movl(op1, Address(in, 0));
 9476   movl(sum, Address(out, offs, Address::times_4,  -4));
 9477 
 9478   movl(raxReg, k);
 9479   mull(op1); //tmp4 * eax -&gt; edx:eax
 9480   addl(sum, carry);
 9481   adcl(rdxReg, 0);
 9482   addl(sum, raxReg);
 9483   adcl(rdxReg, 0);
 9484   movl(carry, rdxReg);
 9485 
 9486   movl(Address(out, offs, Address::times_4,  -4), sum);
 9487 
 9488   bind(L_carry);
 9489   //return tmp5/carry as carry in rax
 9490   movl(rax, carry);
 9491 
 9492   bind(L_done);
 9493   pop(tmp5);
 9494   pop(tmp4);
 9495   pop(tmp3);
 9496   pop(tmp2);
 9497   pop(tmp1);
 9498 }
 9499 #endif
 9500 
 9501 /**
 9502  * Emits code to update CRC-32 with a byte value according to constants in table
 9503  *
 9504  * @param [in,out]crc   Register containing the crc.
 9505  * @param [in]val       Register containing the byte to fold into the CRC.
 9506  * @param [in]table     Register containing the table of crc constants.
 9507  *
 9508  * uint32_t crc;
 9509  * val = crc_table[(val ^ crc) &amp; 0xFF];
 9510  * crc = val ^ (crc &gt;&gt; 8);
 9511  *
 9512  */
 9513 void MacroAssembler::update_byte_crc32(Register crc, Register val, Register table) {
 9514   xorl(val, crc);
 9515   andl(val, 0xFF);
 9516   shrl(crc, 8); // unsigned shift
 9517   xorl(crc, Address(table, val, Address::times_4, 0));
 9518 }
 9519 
 9520 /**
 9521 * Fold four 128-bit data chunks
 9522 */
 9523 void MacroAssembler::fold_128bit_crc32_avx512(XMMRegister xcrc, XMMRegister xK, XMMRegister xtmp, Register buf, int offset) {
 9524   evpclmulhdq(xtmp, xK, xcrc, Assembler::AVX_512bit); // [123:64]
 9525   evpclmulldq(xcrc, xK, xcrc, Assembler::AVX_512bit); // [63:0]
 9526   evpxorq(xcrc, xcrc, Address(buf, offset), Assembler::AVX_512bit /* vector_len */);
 9527   evpxorq(xcrc, xcrc, xtmp, Assembler::AVX_512bit /* vector_len */);
 9528 }
 9529 
 9530 /**
 9531  * Fold 128-bit data chunk
 9532  */
 9533 void MacroAssembler::fold_128bit_crc32(XMMRegister xcrc, XMMRegister xK, XMMRegister xtmp, Register buf, int offset) {
 9534   if (UseAVX &gt; 0) {
 9535     vpclmulhdq(xtmp, xK, xcrc); // [123:64]
 9536     vpclmulldq(xcrc, xK, xcrc); // [63:0]
 9537     vpxor(xcrc, xcrc, Address(buf, offset), 0 /* vector_len */);
 9538     pxor(xcrc, xtmp);
 9539   } else {
 9540     movdqa(xtmp, xcrc);
 9541     pclmulhdq(xtmp, xK);   // [123:64]
 9542     pclmulldq(xcrc, xK);   // [63:0]
 9543     pxor(xcrc, xtmp);
 9544     movdqu(xtmp, Address(buf, offset));
 9545     pxor(xcrc, xtmp);
 9546   }
 9547 }
 9548 
 9549 void MacroAssembler::fold_128bit_crc32(XMMRegister xcrc, XMMRegister xK, XMMRegister xtmp, XMMRegister xbuf) {
 9550   if (UseAVX &gt; 0) {
 9551     vpclmulhdq(xtmp, xK, xcrc);
 9552     vpclmulldq(xcrc, xK, xcrc);
 9553     pxor(xcrc, xbuf);
 9554     pxor(xcrc, xtmp);
 9555   } else {
 9556     movdqa(xtmp, xcrc);
 9557     pclmulhdq(xtmp, xK);
 9558     pclmulldq(xcrc, xK);
 9559     pxor(xcrc, xbuf);
 9560     pxor(xcrc, xtmp);
 9561   }
 9562 }
 9563 
 9564 /**
 9565  * 8-bit folds to compute 32-bit CRC
 9566  *
 9567  * uint64_t xcrc;
 9568  * timesXtoThe32[xcrc &amp; 0xFF] ^ (xcrc &gt;&gt; 8);
 9569  */
 9570 void MacroAssembler::fold_8bit_crc32(XMMRegister xcrc, Register table, XMMRegister xtmp, Register tmp) {
 9571   movdl(tmp, xcrc);
 9572   andl(tmp, 0xFF);
 9573   movdl(xtmp, Address(table, tmp, Address::times_4, 0));
 9574   psrldq(xcrc, 1); // unsigned shift one byte
 9575   pxor(xcrc, xtmp);
 9576 }
 9577 
 9578 /**
 9579  * uint32_t crc;
 9580  * timesXtoThe32[crc &amp; 0xFF] ^ (crc &gt;&gt; 8);
 9581  */
 9582 void MacroAssembler::fold_8bit_crc32(Register crc, Register table, Register tmp) {
 9583   movl(tmp, crc);
 9584   andl(tmp, 0xFF);
 9585   shrl(crc, 8);
 9586   xorl(crc, Address(table, tmp, Address::times_4, 0));
 9587 }
 9588 
 9589 /**
 9590  * @param crc   register containing existing CRC (32-bit)
 9591  * @param buf   register pointing to input byte buffer (byte*)
 9592  * @param len   register containing number of bytes
 9593  * @param table register that will contain address of CRC table
 9594  * @param tmp   scratch register
 9595  */
 9596 void MacroAssembler::kernel_crc32(Register crc, Register buf, Register len, Register table, Register tmp) {
 9597   assert_different_registers(crc, buf, len, table, tmp, rax);
 9598 
 9599   Label L_tail, L_tail_restore, L_tail_loop, L_exit, L_align_loop, L_aligned;
 9600   Label L_fold_tail, L_fold_128b, L_fold_512b, L_fold_512b_loop, L_fold_tail_loop;
 9601 
 9602   // For EVEX with VL and BW, provide a standard mask, VL = 128 will guide the merge
 9603   // context for the registers used, where all instructions below are using 128-bit mode
 9604   // On EVEX without VL and BW, these instructions will all be AVX.
 9605   lea(table, ExternalAddress(StubRoutines::crc_table_addr()));
 9606   notl(crc); // ~crc
 9607   cmpl(len, 16);
 9608   jcc(Assembler::less, L_tail);
 9609 
 9610   // Align buffer to 16 bytes
 9611   movl(tmp, buf);
 9612   andl(tmp, 0xF);
 9613   jccb(Assembler::zero, L_aligned);
 9614   subl(tmp,  16);
 9615   addl(len, tmp);
 9616 
 9617   align(4);
 9618   BIND(L_align_loop);
 9619   movsbl(rax, Address(buf, 0)); // load byte with sign extension
 9620   update_byte_crc32(crc, rax, table);
 9621   increment(buf);
 9622   incrementl(tmp);
 9623   jccb(Assembler::less, L_align_loop);
 9624 
 9625   BIND(L_aligned);
 9626   movl(tmp, len); // save
 9627   shrl(len, 4);
 9628   jcc(Assembler::zero, L_tail_restore);
 9629 
 9630   // Fold crc into first bytes of vector
 9631   movdqa(xmm1, Address(buf, 0));
 9632   movdl(rax, xmm1);
 9633   xorl(crc, rax);
 9634   if (VM_Version::supports_sse4_1()) {
 9635     pinsrd(xmm1, crc, 0);
 9636   } else {
 9637     pinsrw(xmm1, crc, 0);
 9638     shrl(crc, 16);
 9639     pinsrw(xmm1, crc, 1);
 9640   }
 9641   addptr(buf, 16);
 9642   subl(len, 4); // len &gt; 0
 9643   jcc(Assembler::less, L_fold_tail);
 9644 
 9645   movdqa(xmm2, Address(buf,  0));
 9646   movdqa(xmm3, Address(buf, 16));
 9647   movdqa(xmm4, Address(buf, 32));
 9648   addptr(buf, 48);
 9649   subl(len, 3);
 9650   jcc(Assembler::lessEqual, L_fold_512b);
 9651 
 9652   // Fold total 512 bits of polynomial on each iteration,
 9653   // 128 bits per each of 4 parallel streams.
 9654   movdqu(xmm0, ExternalAddress(StubRoutines::x86::crc_by128_masks_addr() + 32));
 9655 
 9656   align(32);
 9657   BIND(L_fold_512b_loop);
 9658   fold_128bit_crc32(xmm1, xmm0, xmm5, buf,  0);
 9659   fold_128bit_crc32(xmm2, xmm0, xmm5, buf, 16);
 9660   fold_128bit_crc32(xmm3, xmm0, xmm5, buf, 32);
 9661   fold_128bit_crc32(xmm4, xmm0, xmm5, buf, 48);
 9662   addptr(buf, 64);
 9663   subl(len, 4);
 9664   jcc(Assembler::greater, L_fold_512b_loop);
 9665 
 9666   // Fold 512 bits to 128 bits.
 9667   BIND(L_fold_512b);
 9668   movdqu(xmm0, ExternalAddress(StubRoutines::x86::crc_by128_masks_addr() + 16));
 9669   fold_128bit_crc32(xmm1, xmm0, xmm5, xmm2);
 9670   fold_128bit_crc32(xmm1, xmm0, xmm5, xmm3);
 9671   fold_128bit_crc32(xmm1, xmm0, xmm5, xmm4);
 9672 
 9673   // Fold the rest of 128 bits data chunks
 9674   BIND(L_fold_tail);
 9675   addl(len, 3);
 9676   jccb(Assembler::lessEqual, L_fold_128b);
 9677   movdqu(xmm0, ExternalAddress(StubRoutines::x86::crc_by128_masks_addr() + 16));
 9678 
 9679   BIND(L_fold_tail_loop);
 9680   fold_128bit_crc32(xmm1, xmm0, xmm5, buf,  0);
 9681   addptr(buf, 16);
 9682   decrementl(len);
 9683   jccb(Assembler::greater, L_fold_tail_loop);
 9684 
 9685   // Fold 128 bits in xmm1 down into 32 bits in crc register.
 9686   BIND(L_fold_128b);
 9687   movdqu(xmm0, ExternalAddress(StubRoutines::x86::crc_by128_masks_addr()));
 9688   if (UseAVX &gt; 0) {
 9689     vpclmulqdq(xmm2, xmm0, xmm1, 0x1);
 9690     vpand(xmm3, xmm0, xmm2, 0 /* vector_len */);
 9691     vpclmulqdq(xmm0, xmm0, xmm3, 0x1);
 9692   } else {
 9693     movdqa(xmm2, xmm0);
 9694     pclmulqdq(xmm2, xmm1, 0x1);
 9695     movdqa(xmm3, xmm0);
 9696     pand(xmm3, xmm2);
 9697     pclmulqdq(xmm0, xmm3, 0x1);
 9698   }
 9699   psrldq(xmm1, 8);
 9700   psrldq(xmm2, 4);
 9701   pxor(xmm0, xmm1);
 9702   pxor(xmm0, xmm2);
 9703 
 9704   // 8 8-bit folds to compute 32-bit CRC.
 9705   for (int j = 0; j &lt; 4; j++) {
 9706     fold_8bit_crc32(xmm0, table, xmm1, rax);
 9707   }
 9708   movdl(crc, xmm0); // mov 32 bits to general register
 9709   for (int j = 0; j &lt; 4; j++) {
 9710     fold_8bit_crc32(crc, table, rax);
 9711   }
 9712 
 9713   BIND(L_tail_restore);
 9714   movl(len, tmp); // restore
 9715   BIND(L_tail);
 9716   andl(len, 0xf);
 9717   jccb(Assembler::zero, L_exit);
 9718 
 9719   // Fold the rest of bytes
 9720   align(4);
 9721   BIND(L_tail_loop);
 9722   movsbl(rax, Address(buf, 0)); // load byte with sign extension
 9723   update_byte_crc32(crc, rax, table);
 9724   increment(buf);
 9725   decrementl(len);
 9726   jccb(Assembler::greater, L_tail_loop);
 9727 
 9728   BIND(L_exit);
 9729   notl(crc); // ~c
 9730 }
 9731 
 9732 #ifdef _LP64
 9733 // S. Gueron / Information Processing Letters 112 (2012) 184
 9734 // Algorithm 4: Computing carry-less multiplication using a precomputed lookup table.
 9735 // Input: A 32 bit value B = [byte3, byte2, byte1, byte0].
 9736 // Output: the 64-bit carry-less product of B * CONST
 9737 void MacroAssembler::crc32c_ipl_alg4(Register in, uint32_t n,
 9738                                      Register tmp1, Register tmp2, Register tmp3) {
 9739   lea(tmp3, ExternalAddress(StubRoutines::crc32c_table_addr()));
 9740   if (n &gt; 0) {
 9741     addq(tmp3, n * 256 * 8);
 9742   }
 9743   //    Q1 = TABLEExt[n][B &amp; 0xFF];
 9744   movl(tmp1, in);
 9745   andl(tmp1, 0x000000FF);
 9746   shll(tmp1, 3);
 9747   addq(tmp1, tmp3);
 9748   movq(tmp1, Address(tmp1, 0));
 9749 
 9750   //    Q2 = TABLEExt[n][B &gt;&gt; 8 &amp; 0xFF];
 9751   movl(tmp2, in);
 9752   shrl(tmp2, 8);
 9753   andl(tmp2, 0x000000FF);
 9754   shll(tmp2, 3);
 9755   addq(tmp2, tmp3);
 9756   movq(tmp2, Address(tmp2, 0));
 9757 
 9758   shlq(tmp2, 8);
 9759   xorq(tmp1, tmp2);
 9760 
 9761   //    Q3 = TABLEExt[n][B &gt;&gt; 16 &amp; 0xFF];
 9762   movl(tmp2, in);
 9763   shrl(tmp2, 16);
 9764   andl(tmp2, 0x000000FF);
 9765   shll(tmp2, 3);
 9766   addq(tmp2, tmp3);
 9767   movq(tmp2, Address(tmp2, 0));
 9768 
 9769   shlq(tmp2, 16);
 9770   xorq(tmp1, tmp2);
 9771 
 9772   //    Q4 = TABLEExt[n][B &gt;&gt; 24 &amp; 0xFF];
 9773   shrl(in, 24);
 9774   andl(in, 0x000000FF);
 9775   shll(in, 3);
 9776   addq(in, tmp3);
 9777   movq(in, Address(in, 0));
 9778 
 9779   shlq(in, 24);
 9780   xorq(in, tmp1);
 9781   //    return Q1 ^ Q2 &lt;&lt; 8 ^ Q3 &lt;&lt; 16 ^ Q4 &lt;&lt; 24;
 9782 }
 9783 
 9784 void MacroAssembler::crc32c_pclmulqdq(XMMRegister w_xtmp1,
 9785                                       Register in_out,
 9786                                       uint32_t const_or_pre_comp_const_index, bool is_pclmulqdq_supported,
 9787                                       XMMRegister w_xtmp2,
 9788                                       Register tmp1,
 9789                                       Register n_tmp2, Register n_tmp3) {
 9790   if (is_pclmulqdq_supported) {
 9791     movdl(w_xtmp1, in_out); // modified blindly
 9792 
 9793     movl(tmp1, const_or_pre_comp_const_index);
 9794     movdl(w_xtmp2, tmp1);
 9795     pclmulqdq(w_xtmp1, w_xtmp2, 0);
 9796 
 9797     movdq(in_out, w_xtmp1);
 9798   } else {
 9799     crc32c_ipl_alg4(in_out, const_or_pre_comp_const_index, tmp1, n_tmp2, n_tmp3);
 9800   }
 9801 }
 9802 
 9803 // Recombination Alternative 2: No bit-reflections
 9804 // T1 = (CRC_A * U1) &lt;&lt; 1
 9805 // T2 = (CRC_B * U2) &lt;&lt; 1
 9806 // C1 = T1 &gt;&gt; 32
 9807 // C2 = T2 &gt;&gt; 32
 9808 // T1 = T1 &amp; 0xFFFFFFFF
 9809 // T2 = T2 &amp; 0xFFFFFFFF
 9810 // T1 = CRC32(0, T1)
 9811 // T2 = CRC32(0, T2)
 9812 // C1 = C1 ^ T1
 9813 // C2 = C2 ^ T2
 9814 // CRC = C1 ^ C2 ^ CRC_C
 9815 void MacroAssembler::crc32c_rec_alt2(uint32_t const_or_pre_comp_const_index_u1, uint32_t const_or_pre_comp_const_index_u2, bool is_pclmulqdq_supported, Register in_out, Register in1, Register in2,
 9816                                      XMMRegister w_xtmp1, XMMRegister w_xtmp2, XMMRegister w_xtmp3,
 9817                                      Register tmp1, Register tmp2,
 9818                                      Register n_tmp3) {
 9819   crc32c_pclmulqdq(w_xtmp1, in_out, const_or_pre_comp_const_index_u1, is_pclmulqdq_supported, w_xtmp3, tmp1, tmp2, n_tmp3);
 9820   crc32c_pclmulqdq(w_xtmp2, in1, const_or_pre_comp_const_index_u2, is_pclmulqdq_supported, w_xtmp3, tmp1, tmp2, n_tmp3);
 9821   shlq(in_out, 1);
 9822   movl(tmp1, in_out);
 9823   shrq(in_out, 32);
 9824   xorl(tmp2, tmp2);
 9825   crc32(tmp2, tmp1, 4);
 9826   xorl(in_out, tmp2); // we don&#39;t care about upper 32 bit contents here
 9827   shlq(in1, 1);
 9828   movl(tmp1, in1);
 9829   shrq(in1, 32);
 9830   xorl(tmp2, tmp2);
 9831   crc32(tmp2, tmp1, 4);
 9832   xorl(in1, tmp2);
 9833   xorl(in_out, in1);
 9834   xorl(in_out, in2);
 9835 }
 9836 
 9837 // Set N to predefined value
 9838 // Subtract from a lenght of a buffer
 9839 // execute in a loop:
 9840 // CRC_A = 0xFFFFFFFF, CRC_B = 0, CRC_C = 0
 9841 // for i = 1 to N do
 9842 //  CRC_A = CRC32(CRC_A, A[i])
 9843 //  CRC_B = CRC32(CRC_B, B[i])
 9844 //  CRC_C = CRC32(CRC_C, C[i])
 9845 // end for
 9846 // Recombine
 9847 void MacroAssembler::crc32c_proc_chunk(uint32_t size, uint32_t const_or_pre_comp_const_index_u1, uint32_t const_or_pre_comp_const_index_u2, bool is_pclmulqdq_supported,
 9848                                        Register in_out1, Register in_out2, Register in_out3,
 9849                                        Register tmp1, Register tmp2, Register tmp3,
 9850                                        XMMRegister w_xtmp1, XMMRegister w_xtmp2, XMMRegister w_xtmp3,
 9851                                        Register tmp4, Register tmp5,
 9852                                        Register n_tmp6) {
 9853   Label L_processPartitions;
 9854   Label L_processPartition;
 9855   Label L_exit;
 9856 
 9857   bind(L_processPartitions);
 9858   cmpl(in_out1, 3 * size);
 9859   jcc(Assembler::less, L_exit);
 9860     xorl(tmp1, tmp1);
 9861     xorl(tmp2, tmp2);
 9862     movq(tmp3, in_out2);
 9863     addq(tmp3, size);
 9864 
 9865     bind(L_processPartition);
 9866       crc32(in_out3, Address(in_out2, 0), 8);
 9867       crc32(tmp1, Address(in_out2, size), 8);
 9868       crc32(tmp2, Address(in_out2, size * 2), 8);
 9869       addq(in_out2, 8);
 9870       cmpq(in_out2, tmp3);
 9871       jcc(Assembler::less, L_processPartition);
 9872     crc32c_rec_alt2(const_or_pre_comp_const_index_u1, const_or_pre_comp_const_index_u2, is_pclmulqdq_supported, in_out3, tmp1, tmp2,
 9873             w_xtmp1, w_xtmp2, w_xtmp3,
 9874             tmp4, tmp5,
 9875             n_tmp6);
 9876     addq(in_out2, 2 * size);
 9877     subl(in_out1, 3 * size);
 9878     jmp(L_processPartitions);
 9879 
 9880   bind(L_exit);
 9881 }
 9882 #else
 9883 void MacroAssembler::crc32c_ipl_alg4(Register in_out, uint32_t n,
 9884                                      Register tmp1, Register tmp2, Register tmp3,
 9885                                      XMMRegister xtmp1, XMMRegister xtmp2) {
 9886   lea(tmp3, ExternalAddress(StubRoutines::crc32c_table_addr()));
 9887   if (n &gt; 0) {
 9888     addl(tmp3, n * 256 * 8);
 9889   }
 9890   //    Q1 = TABLEExt[n][B &amp; 0xFF];
 9891   movl(tmp1, in_out);
 9892   andl(tmp1, 0x000000FF);
 9893   shll(tmp1, 3);
 9894   addl(tmp1, tmp3);
 9895   movq(xtmp1, Address(tmp1, 0));
 9896 
 9897   //    Q2 = TABLEExt[n][B &gt;&gt; 8 &amp; 0xFF];
 9898   movl(tmp2, in_out);
 9899   shrl(tmp2, 8);
 9900   andl(tmp2, 0x000000FF);
 9901   shll(tmp2, 3);
 9902   addl(tmp2, tmp3);
 9903   movq(xtmp2, Address(tmp2, 0));
 9904 
 9905   psllq(xtmp2, 8);
 9906   pxor(xtmp1, xtmp2);
 9907 
 9908   //    Q3 = TABLEExt[n][B &gt;&gt; 16 &amp; 0xFF];
 9909   movl(tmp2, in_out);
 9910   shrl(tmp2, 16);
 9911   andl(tmp2, 0x000000FF);
 9912   shll(tmp2, 3);
 9913   addl(tmp2, tmp3);
 9914   movq(xtmp2, Address(tmp2, 0));
 9915 
 9916   psllq(xtmp2, 16);
 9917   pxor(xtmp1, xtmp2);
 9918 
 9919   //    Q4 = TABLEExt[n][B &gt;&gt; 24 &amp; 0xFF];
 9920   shrl(in_out, 24);
 9921   andl(in_out, 0x000000FF);
 9922   shll(in_out, 3);
 9923   addl(in_out, tmp3);
 9924   movq(xtmp2, Address(in_out, 0));
 9925 
 9926   psllq(xtmp2, 24);
 9927   pxor(xtmp1, xtmp2); // Result in CXMM
 9928   //    return Q1 ^ Q2 &lt;&lt; 8 ^ Q3 &lt;&lt; 16 ^ Q4 &lt;&lt; 24;
 9929 }
 9930 
 9931 void MacroAssembler::crc32c_pclmulqdq(XMMRegister w_xtmp1,
 9932                                       Register in_out,
 9933                                       uint32_t const_or_pre_comp_const_index, bool is_pclmulqdq_supported,
 9934                                       XMMRegister w_xtmp2,
 9935                                       Register tmp1,
 9936                                       Register n_tmp2, Register n_tmp3) {
 9937   if (is_pclmulqdq_supported) {
 9938     movdl(w_xtmp1, in_out);
 9939 
 9940     movl(tmp1, const_or_pre_comp_const_index);
 9941     movdl(w_xtmp2, tmp1);
 9942     pclmulqdq(w_xtmp1, w_xtmp2, 0);
 9943     // Keep result in XMM since GPR is 32 bit in length
 9944   } else {
 9945     crc32c_ipl_alg4(in_out, const_or_pre_comp_const_index, tmp1, n_tmp2, n_tmp3, w_xtmp1, w_xtmp2);
 9946   }
 9947 }
 9948 
 9949 void MacroAssembler::crc32c_rec_alt2(uint32_t const_or_pre_comp_const_index_u1, uint32_t const_or_pre_comp_const_index_u2, bool is_pclmulqdq_supported, Register in_out, Register in1, Register in2,
 9950                                      XMMRegister w_xtmp1, XMMRegister w_xtmp2, XMMRegister w_xtmp3,
 9951                                      Register tmp1, Register tmp2,
 9952                                      Register n_tmp3) {
 9953   crc32c_pclmulqdq(w_xtmp1, in_out, const_or_pre_comp_const_index_u1, is_pclmulqdq_supported, w_xtmp3, tmp1, tmp2, n_tmp3);
 9954   crc32c_pclmulqdq(w_xtmp2, in1, const_or_pre_comp_const_index_u2, is_pclmulqdq_supported, w_xtmp3, tmp1, tmp2, n_tmp3);
 9955 
 9956   psllq(w_xtmp1, 1);
 9957   movdl(tmp1, w_xtmp1);
 9958   psrlq(w_xtmp1, 32);
 9959   movdl(in_out, w_xtmp1);
 9960 
 9961   xorl(tmp2, tmp2);
 9962   crc32(tmp2, tmp1, 4);
 9963   xorl(in_out, tmp2);
 9964 
 9965   psllq(w_xtmp2, 1);
 9966   movdl(tmp1, w_xtmp2);
 9967   psrlq(w_xtmp2, 32);
 9968   movdl(in1, w_xtmp2);
 9969 
 9970   xorl(tmp2, tmp2);
 9971   crc32(tmp2, tmp1, 4);
 9972   xorl(in1, tmp2);
 9973   xorl(in_out, in1);
 9974   xorl(in_out, in2);
 9975 }
 9976 
 9977 void MacroAssembler::crc32c_proc_chunk(uint32_t size, uint32_t const_or_pre_comp_const_index_u1, uint32_t const_or_pre_comp_const_index_u2, bool is_pclmulqdq_supported,
 9978                                        Register in_out1, Register in_out2, Register in_out3,
 9979                                        Register tmp1, Register tmp2, Register tmp3,
 9980                                        XMMRegister w_xtmp1, XMMRegister w_xtmp2, XMMRegister w_xtmp3,
 9981                                        Register tmp4, Register tmp5,
 9982                                        Register n_tmp6) {
 9983   Label L_processPartitions;
 9984   Label L_processPartition;
 9985   Label L_exit;
 9986 
 9987   bind(L_processPartitions);
 9988   cmpl(in_out1, 3 * size);
 9989   jcc(Assembler::less, L_exit);
 9990     xorl(tmp1, tmp1);
 9991     xorl(tmp2, tmp2);
 9992     movl(tmp3, in_out2);
 9993     addl(tmp3, size);
 9994 
 9995     bind(L_processPartition);
 9996       crc32(in_out3, Address(in_out2, 0), 4);
 9997       crc32(tmp1, Address(in_out2, size), 4);
 9998       crc32(tmp2, Address(in_out2, size*2), 4);
 9999       crc32(in_out3, Address(in_out2, 0+4), 4);
10000       crc32(tmp1, Address(in_out2, size+4), 4);
10001       crc32(tmp2, Address(in_out2, size*2+4), 4);
10002       addl(in_out2, 8);
10003       cmpl(in_out2, tmp3);
10004       jcc(Assembler::less, L_processPartition);
10005 
10006         push(tmp3);
10007         push(in_out1);
10008         push(in_out2);
10009         tmp4 = tmp3;
10010         tmp5 = in_out1;
10011         n_tmp6 = in_out2;
10012 
10013       crc32c_rec_alt2(const_or_pre_comp_const_index_u1, const_or_pre_comp_const_index_u2, is_pclmulqdq_supported, in_out3, tmp1, tmp2,
10014             w_xtmp1, w_xtmp2, w_xtmp3,
10015             tmp4, tmp5,
10016             n_tmp6);
10017 
10018         pop(in_out2);
10019         pop(in_out1);
10020         pop(tmp3);
10021 
10022     addl(in_out2, 2 * size);
10023     subl(in_out1, 3 * size);
10024     jmp(L_processPartitions);
10025 
10026   bind(L_exit);
10027 }
10028 #endif //LP64
10029 
10030 #ifdef _LP64
10031 // Algorithm 2: Pipelined usage of the CRC32 instruction.
10032 // Input: A buffer I of L bytes.
10033 // Output: the CRC32C value of the buffer.
10034 // Notations:
10035 // Write L = 24N + r, with N = floor (L/24).
10036 // r = L mod 24 (0 &lt;= r &lt; 24).
10037 // Consider I as the concatenation of A|B|C|R, where A, B, C, each,
10038 // N quadwords, and R consists of r bytes.
10039 // A[j] = I [8j+7:8j], j= 0, 1, ..., N-1
10040 // B[j] = I [N + 8j+7:N + 8j], j= 0, 1, ..., N-1
10041 // C[j] = I [2N + 8j+7:2N + 8j], j= 0, 1, ..., N-1
10042 // if r &gt; 0 R[j] = I [3N +j], j= 0, 1, ...,r-1
10043 void MacroAssembler::crc32c_ipl_alg2_alt2(Register in_out, Register in1, Register in2,
10044                                           Register tmp1, Register tmp2, Register tmp3,
10045                                           Register tmp4, Register tmp5, Register tmp6,
10046                                           XMMRegister w_xtmp1, XMMRegister w_xtmp2, XMMRegister w_xtmp3,
10047                                           bool is_pclmulqdq_supported) {
10048   uint32_t const_or_pre_comp_const_index[CRC32C_NUM_PRECOMPUTED_CONSTANTS];
10049   Label L_wordByWord;
10050   Label L_byteByByteProlog;
10051   Label L_byteByByte;
10052   Label L_exit;
10053 
10054   if (is_pclmulqdq_supported ) {
10055     const_or_pre_comp_const_index[1] = *(uint32_t *)StubRoutines::_crc32c_table_addr;
10056     const_or_pre_comp_const_index[0] = *((uint32_t *)StubRoutines::_crc32c_table_addr+1);
10057 
10058     const_or_pre_comp_const_index[3] = *((uint32_t *)StubRoutines::_crc32c_table_addr + 2);
10059     const_or_pre_comp_const_index[2] = *((uint32_t *)StubRoutines::_crc32c_table_addr + 3);
10060 
10061     const_or_pre_comp_const_index[5] = *((uint32_t *)StubRoutines::_crc32c_table_addr + 4);
10062     const_or_pre_comp_const_index[4] = *((uint32_t *)StubRoutines::_crc32c_table_addr + 5);
10063     assert((CRC32C_NUM_PRECOMPUTED_CONSTANTS - 1 ) == 5, &quot;Checking whether you declared all of the constants based on the number of \&quot;chunks\&quot;&quot;);
10064   } else {
10065     const_or_pre_comp_const_index[0] = 1;
10066     const_or_pre_comp_const_index[1] = 0;
10067 
10068     const_or_pre_comp_const_index[2] = 3;
10069     const_or_pre_comp_const_index[3] = 2;
10070 
10071     const_or_pre_comp_const_index[4] = 5;
10072     const_or_pre_comp_const_index[5] = 4;
10073    }
10074   crc32c_proc_chunk(CRC32C_HIGH, const_or_pre_comp_const_index[0], const_or_pre_comp_const_index[1], is_pclmulqdq_supported,
10075                     in2, in1, in_out,
10076                     tmp1, tmp2, tmp3,
10077                     w_xtmp1, w_xtmp2, w_xtmp3,
10078                     tmp4, tmp5,
10079                     tmp6);
10080   crc32c_proc_chunk(CRC32C_MIDDLE, const_or_pre_comp_const_index[2], const_or_pre_comp_const_index[3], is_pclmulqdq_supported,
10081                     in2, in1, in_out,
10082                     tmp1, tmp2, tmp3,
10083                     w_xtmp1, w_xtmp2, w_xtmp3,
10084                     tmp4, tmp5,
10085                     tmp6);
10086   crc32c_proc_chunk(CRC32C_LOW, const_or_pre_comp_const_index[4], const_or_pre_comp_const_index[5], is_pclmulqdq_supported,
10087                     in2, in1, in_out,
10088                     tmp1, tmp2, tmp3,
10089                     w_xtmp1, w_xtmp2, w_xtmp3,
10090                     tmp4, tmp5,
10091                     tmp6);
10092   movl(tmp1, in2);
10093   andl(tmp1, 0x00000007);
10094   negl(tmp1);
10095   addl(tmp1, in2);
10096   addq(tmp1, in1);
10097 
10098   BIND(L_wordByWord);
10099   cmpq(in1, tmp1);
10100   jcc(Assembler::greaterEqual, L_byteByByteProlog);
10101     crc32(in_out, Address(in1, 0), 4);
10102     addq(in1, 4);
10103     jmp(L_wordByWord);
10104 
10105   BIND(L_byteByByteProlog);
10106   andl(in2, 0x00000007);
10107   movl(tmp2, 1);
10108 
10109   BIND(L_byteByByte);
10110   cmpl(tmp2, in2);
10111   jccb(Assembler::greater, L_exit);
10112     crc32(in_out, Address(in1, 0), 1);
10113     incq(in1);
10114     incl(tmp2);
10115     jmp(L_byteByByte);
10116 
10117   BIND(L_exit);
10118 }
10119 #else
10120 void MacroAssembler::crc32c_ipl_alg2_alt2(Register in_out, Register in1, Register in2,
10121                                           Register tmp1, Register  tmp2, Register tmp3,
10122                                           Register tmp4, Register  tmp5, Register tmp6,
10123                                           XMMRegister w_xtmp1, XMMRegister w_xtmp2, XMMRegister w_xtmp3,
10124                                           bool is_pclmulqdq_supported) {
10125   uint32_t const_or_pre_comp_const_index[CRC32C_NUM_PRECOMPUTED_CONSTANTS];
10126   Label L_wordByWord;
10127   Label L_byteByByteProlog;
10128   Label L_byteByByte;
10129   Label L_exit;
10130 
10131   if (is_pclmulqdq_supported) {
10132     const_or_pre_comp_const_index[1] = *(uint32_t *)StubRoutines::_crc32c_table_addr;
10133     const_or_pre_comp_const_index[0] = *((uint32_t *)StubRoutines::_crc32c_table_addr + 1);
10134 
10135     const_or_pre_comp_const_index[3] = *((uint32_t *)StubRoutines::_crc32c_table_addr + 2);
10136     const_or_pre_comp_const_index[2] = *((uint32_t *)StubRoutines::_crc32c_table_addr + 3);
10137 
10138     const_or_pre_comp_const_index[5] = *((uint32_t *)StubRoutines::_crc32c_table_addr + 4);
10139     const_or_pre_comp_const_index[4] = *((uint32_t *)StubRoutines::_crc32c_table_addr + 5);
10140   } else {
10141     const_or_pre_comp_const_index[0] = 1;
10142     const_or_pre_comp_const_index[1] = 0;
10143 
10144     const_or_pre_comp_const_index[2] = 3;
10145     const_or_pre_comp_const_index[3] = 2;
10146 
10147     const_or_pre_comp_const_index[4] = 5;
10148     const_or_pre_comp_const_index[5] = 4;
10149   }
10150   crc32c_proc_chunk(CRC32C_HIGH, const_or_pre_comp_const_index[0], const_or_pre_comp_const_index[1], is_pclmulqdq_supported,
10151                     in2, in1, in_out,
10152                     tmp1, tmp2, tmp3,
10153                     w_xtmp1, w_xtmp2, w_xtmp3,
10154                     tmp4, tmp5,
10155                     tmp6);
10156   crc32c_proc_chunk(CRC32C_MIDDLE, const_or_pre_comp_const_index[2], const_or_pre_comp_const_index[3], is_pclmulqdq_supported,
10157                     in2, in1, in_out,
10158                     tmp1, tmp2, tmp3,
10159                     w_xtmp1, w_xtmp2, w_xtmp3,
10160                     tmp4, tmp5,
10161                     tmp6);
10162   crc32c_proc_chunk(CRC32C_LOW, const_or_pre_comp_const_index[4], const_or_pre_comp_const_index[5], is_pclmulqdq_supported,
10163                     in2, in1, in_out,
10164                     tmp1, tmp2, tmp3,
10165                     w_xtmp1, w_xtmp2, w_xtmp3,
10166                     tmp4, tmp5,
10167                     tmp6);
10168   movl(tmp1, in2);
10169   andl(tmp1, 0x00000007);
10170   negl(tmp1);
10171   addl(tmp1, in2);
10172   addl(tmp1, in1);
10173 
10174   BIND(L_wordByWord);
10175   cmpl(in1, tmp1);
10176   jcc(Assembler::greaterEqual, L_byteByByteProlog);
10177     crc32(in_out, Address(in1,0), 4);
10178     addl(in1, 4);
10179     jmp(L_wordByWord);
10180 
10181   BIND(L_byteByByteProlog);
10182   andl(in2, 0x00000007);
10183   movl(tmp2, 1);
10184 
10185   BIND(L_byteByByte);
10186   cmpl(tmp2, in2);
10187   jccb(Assembler::greater, L_exit);
10188     movb(tmp1, Address(in1, 0));
10189     crc32(in_out, tmp1, 1);
10190     incl(in1);
10191     incl(tmp2);
10192     jmp(L_byteByByte);
10193 
10194   BIND(L_exit);
10195 }
10196 #endif // LP64
10197 #undef BIND
10198 #undef BLOCK_COMMENT
10199 
10200 // Compress char[] array to byte[].
10201 //   ..\jdk\src\java.base\share\classes\java\lang\StringUTF16.java
10202 //   @HotSpotIntrinsicCandidate
10203 //   private static int compress(char[] src, int srcOff, byte[] dst, int dstOff, int len) {
10204 //     for (int i = 0; i &lt; len; i++) {
10205 //       int c = src[srcOff++];
10206 //       if (c &gt;&gt;&gt; 8 != 0) {
10207 //         return 0;
10208 //       }
10209 //       dst[dstOff++] = (byte)c;
10210 //     }
10211 //     return len;
10212 //   }
10213 void MacroAssembler::char_array_compress(Register src, Register dst, Register len,
10214   XMMRegister tmp1Reg, XMMRegister tmp2Reg,
10215   XMMRegister tmp3Reg, XMMRegister tmp4Reg,
10216   Register tmp5, Register result) {
10217   Label copy_chars_loop, return_length, return_zero, done;
10218 
10219   // rsi: src
10220   // rdi: dst
10221   // rdx: len
10222   // rcx: tmp5
10223   // rax: result
10224 
10225   // rsi holds start addr of source char[] to be compressed
10226   // rdi holds start addr of destination byte[]
10227   // rdx holds length
10228 
10229   assert(len != result, &quot;&quot;);
10230 
10231   // save length for return
10232   push(len);
10233 
10234   if ((AVX3Threshold == 0) &amp;&amp; (UseAVX &gt; 2) &amp;&amp; // AVX512
10235     VM_Version::supports_avx512vlbw() &amp;&amp;
10236     VM_Version::supports_bmi2()) {
10237 
10238     Label copy_32_loop, copy_loop_tail, below_threshold;
10239 
10240     // alignment
10241     Label post_alignment;
10242 
10243     // if length of the string is less than 16, handle it in an old fashioned way
10244     testl(len, -32);
10245     jcc(Assembler::zero, below_threshold);
10246 
10247     // First check whether a character is compressable ( &lt;= 0xFF).
10248     // Create mask to test for Unicode chars inside zmm vector
10249     movl(result, 0x00FF);
10250     evpbroadcastw(tmp2Reg, result, Assembler::AVX_512bit);
10251 
10252     testl(len, -64);
10253     jcc(Assembler::zero, post_alignment);
10254 
10255     movl(tmp5, dst);
10256     andl(tmp5, (32 - 1));
10257     negl(tmp5);
10258     andl(tmp5, (32 - 1));
10259 
10260     // bail out when there is nothing to be done
10261     testl(tmp5, 0xFFFFFFFF);
10262     jcc(Assembler::zero, post_alignment);
10263 
10264     // ~(~0 &lt;&lt; len), where len is the # of remaining elements to process
10265     movl(result, 0xFFFFFFFF);
10266     shlxl(result, result, tmp5);
10267     notl(result);
10268     kmovdl(k3, result);
10269 
10270     evmovdquw(tmp1Reg, k3, Address(src, 0), Assembler::AVX_512bit);
10271     evpcmpuw(k2, k3, tmp1Reg, tmp2Reg, Assembler::le, Assembler::AVX_512bit);
10272     ktestd(k2, k3);
10273     jcc(Assembler::carryClear, return_zero);
10274 
10275     evpmovwb(Address(dst, 0), k3, tmp1Reg, Assembler::AVX_512bit);
10276 
10277     addptr(src, tmp5);
10278     addptr(src, tmp5);
10279     addptr(dst, tmp5);
10280     subl(len, tmp5);
10281 
10282     bind(post_alignment);
10283     // end of alignment
10284 
10285     movl(tmp5, len);
10286     andl(tmp5, (32 - 1));    // tail count (in chars)
10287     andl(len, ~(32 - 1));    // vector count (in chars)
10288     jcc(Assembler::zero, copy_loop_tail);
10289 
10290     lea(src, Address(src, len, Address::times_2));
10291     lea(dst, Address(dst, len, Address::times_1));
10292     negptr(len);
10293 
10294     bind(copy_32_loop);
10295     evmovdquw(tmp1Reg, Address(src, len, Address::times_2), Assembler::AVX_512bit);
10296     evpcmpuw(k2, tmp1Reg, tmp2Reg, Assembler::le, Assembler::AVX_512bit);
10297     kortestdl(k2, k2);
10298     jcc(Assembler::carryClear, return_zero);
10299 
10300     // All elements in current processed chunk are valid candidates for
10301     // compression. Write a truncated byte elements to the memory.
10302     evpmovwb(Address(dst, len, Address::times_1), tmp1Reg, Assembler::AVX_512bit);
10303     addptr(len, 32);
10304     jcc(Assembler::notZero, copy_32_loop);
10305 
10306     bind(copy_loop_tail);
10307     // bail out when there is nothing to be done
10308     testl(tmp5, 0xFFFFFFFF);
10309     jcc(Assembler::zero, return_length);
10310 
10311     movl(len, tmp5);
10312 
10313     // ~(~0 &lt;&lt; len), where len is the # of remaining elements to process
10314     movl(result, 0xFFFFFFFF);
10315     shlxl(result, result, len);
10316     notl(result);
10317 
10318     kmovdl(k3, result);
10319 
10320     evmovdquw(tmp1Reg, k3, Address(src, 0), Assembler::AVX_512bit);
10321     evpcmpuw(k2, k3, tmp1Reg, tmp2Reg, Assembler::le, Assembler::AVX_512bit);
10322     ktestd(k2, k3);
10323     jcc(Assembler::carryClear, return_zero);
10324 
10325     evpmovwb(Address(dst, 0), k3, tmp1Reg, Assembler::AVX_512bit);
10326     jmp(return_length);
10327 
10328     bind(below_threshold);
10329   }
10330 
10331   if (UseSSE42Intrinsics) {
10332     Label copy_32_loop, copy_16, copy_tail;
10333 
10334     movl(result, len);
10335 
10336     movl(tmp5, 0xff00ff00);   // create mask to test for Unicode chars in vectors
10337 
10338     // vectored compression
10339     andl(len, 0xfffffff0);    // vector count (in chars)
10340     andl(result, 0x0000000f);    // tail count (in chars)
10341     testl(len, len);
10342     jcc(Assembler::zero, copy_16);
10343 
10344     // compress 16 chars per iter
10345     movdl(tmp1Reg, tmp5);
10346     pshufd(tmp1Reg, tmp1Reg, 0);   // store Unicode mask in tmp1Reg
10347     pxor(tmp4Reg, tmp4Reg);
10348 
10349     lea(src, Address(src, len, Address::times_2));
10350     lea(dst, Address(dst, len, Address::times_1));
10351     negptr(len);
10352 
10353     bind(copy_32_loop);
10354     movdqu(tmp2Reg, Address(src, len, Address::times_2));     // load 1st 8 characters
10355     por(tmp4Reg, tmp2Reg);
10356     movdqu(tmp3Reg, Address(src, len, Address::times_2, 16)); // load next 8 characters
10357     por(tmp4Reg, tmp3Reg);
10358     ptest(tmp4Reg, tmp1Reg);       // check for Unicode chars in next vector
10359     jcc(Assembler::notZero, return_zero);
10360     packuswb(tmp2Reg, tmp3Reg);    // only ASCII chars; compress each to 1 byte
10361     movdqu(Address(dst, len, Address::times_1), tmp2Reg);
10362     addptr(len, 16);
10363     jcc(Assembler::notZero, copy_32_loop);
10364 
10365     // compress next vector of 8 chars (if any)
10366     bind(copy_16);
10367     movl(len, result);
10368     andl(len, 0xfffffff8);    // vector count (in chars)
10369     andl(result, 0x00000007);    // tail count (in chars)
10370     testl(len, len);
10371     jccb(Assembler::zero, copy_tail);
10372 
10373     movdl(tmp1Reg, tmp5);
10374     pshufd(tmp1Reg, tmp1Reg, 0);   // store Unicode mask in tmp1Reg
10375     pxor(tmp3Reg, tmp3Reg);
10376 
10377     movdqu(tmp2Reg, Address(src, 0));
10378     ptest(tmp2Reg, tmp1Reg);       // check for Unicode chars in vector
10379     jccb(Assembler::notZero, return_zero);
10380     packuswb(tmp2Reg, tmp3Reg);    // only LATIN1 chars; compress each to 1 byte
10381     movq(Address(dst, 0), tmp2Reg);
10382     addptr(src, 16);
10383     addptr(dst, 8);
10384 
10385     bind(copy_tail);
10386     movl(len, result);
10387   }
10388   // compress 1 char per iter
10389   testl(len, len);
10390   jccb(Assembler::zero, return_length);
10391   lea(src, Address(src, len, Address::times_2));
10392   lea(dst, Address(dst, len, Address::times_1));
10393   negptr(len);
10394 
10395   bind(copy_chars_loop);
10396   load_unsigned_short(result, Address(src, len, Address::times_2));
10397   testl(result, 0xff00);      // check if Unicode char
10398   jccb(Assembler::notZero, return_zero);
10399   movb(Address(dst, len, Address::times_1), result);  // ASCII char; compress to 1 byte
10400   increment(len);
10401   jcc(Assembler::notZero, copy_chars_loop);
10402 
10403   // if compression succeeded, return length
10404   bind(return_length);
10405   pop(result);
10406   jmpb(done);
10407 
10408   // if compression failed, return 0
10409   bind(return_zero);
10410   xorl(result, result);
10411   addptr(rsp, wordSize);
10412 
10413   bind(done);
10414 }
10415 
10416 // Inflate byte[] array to char[].
10417 //   ..\jdk\src\java.base\share\classes\java\lang\StringLatin1.java
10418 //   @HotSpotIntrinsicCandidate
10419 //   private static void inflate(byte[] src, int srcOff, char[] dst, int dstOff, int len) {
10420 //     for (int i = 0; i &lt; len; i++) {
10421 //       dst[dstOff++] = (char)(src[srcOff++] &amp; 0xff);
10422 //     }
10423 //   }
10424 void MacroAssembler::byte_array_inflate(Register src, Register dst, Register len,
10425   XMMRegister tmp1, Register tmp2) {
10426   Label copy_chars_loop, done, below_threshold, avx3_threshold;
10427   // rsi: src
10428   // rdi: dst
10429   // rdx: len
10430   // rcx: tmp2
10431 
10432   // rsi holds start addr of source byte[] to be inflated
10433   // rdi holds start addr of destination char[]
10434   // rdx holds length
10435   assert_different_registers(src, dst, len, tmp2);
10436   movl(tmp2, len);
10437   if ((UseAVX &gt; 2) &amp;&amp; // AVX512
10438     VM_Version::supports_avx512vlbw() &amp;&amp;
10439     VM_Version::supports_bmi2()) {
10440 
10441     Label copy_32_loop, copy_tail;
10442     Register tmp3_aliased = len;
10443 
10444     // if length of the string is less than 16, handle it in an old fashioned way
10445     testl(len, -16);
10446     jcc(Assembler::zero, below_threshold);
10447 
10448     testl(len, -1 * AVX3Threshold);
10449     jcc(Assembler::zero, avx3_threshold);
10450 
10451     // In order to use only one arithmetic operation for the main loop we use
10452     // this pre-calculation
10453     andl(tmp2, (32 - 1)); // tail count (in chars), 32 element wide loop
10454     andl(len, -32);     // vector count
10455     jccb(Assembler::zero, copy_tail);
10456 
10457     lea(src, Address(src, len, Address::times_1));
10458     lea(dst, Address(dst, len, Address::times_2));
10459     negptr(len);
10460 
10461 
10462     // inflate 32 chars per iter
10463     bind(copy_32_loop);
10464     vpmovzxbw(tmp1, Address(src, len, Address::times_1), Assembler::AVX_512bit);
10465     evmovdquw(Address(dst, len, Address::times_2), tmp1, Assembler::AVX_512bit);
10466     addptr(len, 32);
10467     jcc(Assembler::notZero, copy_32_loop);
10468 
10469     bind(copy_tail);
10470     // bail out when there is nothing to be done
10471     testl(tmp2, -1); // we don&#39;t destroy the contents of tmp2 here
10472     jcc(Assembler::zero, done);
10473 
10474     // ~(~0 &lt;&lt; length), where length is the # of remaining elements to process
10475     movl(tmp3_aliased, -1);
10476     shlxl(tmp3_aliased, tmp3_aliased, tmp2);
10477     notl(tmp3_aliased);
10478     kmovdl(k2, tmp3_aliased);
10479     evpmovzxbw(tmp1, k2, Address(src, 0), Assembler::AVX_512bit);
10480     evmovdquw(Address(dst, 0), k2, tmp1, Assembler::AVX_512bit);
10481 
10482     jmp(done);
10483     bind(avx3_threshold);
10484   }
10485   if (UseSSE42Intrinsics) {
10486     Label copy_16_loop, copy_8_loop, copy_bytes, copy_new_tail, copy_tail;
10487 
10488     if (UseAVX &gt; 1) {
10489       andl(tmp2, (16 - 1));
10490       andl(len, -16);
10491       jccb(Assembler::zero, copy_new_tail);
10492     } else {
10493       andl(tmp2, 0x00000007);   // tail count (in chars)
10494       andl(len, 0xfffffff8);    // vector count (in chars)
10495       jccb(Assembler::zero, copy_tail);
10496     }
10497 
10498     // vectored inflation
10499     lea(src, Address(src, len, Address::times_1));
10500     lea(dst, Address(dst, len, Address::times_2));
10501     negptr(len);
10502 
10503     if (UseAVX &gt; 1) {
10504       bind(copy_16_loop);
10505       vpmovzxbw(tmp1, Address(src, len, Address::times_1), Assembler::AVX_256bit);
10506       vmovdqu(Address(dst, len, Address::times_2), tmp1);
10507       addptr(len, 16);
10508       jcc(Assembler::notZero, copy_16_loop);
10509 
10510       bind(below_threshold);
10511       bind(copy_new_tail);
10512       movl(len, tmp2);
10513       andl(tmp2, 0x00000007);
10514       andl(len, 0xFFFFFFF8);
10515       jccb(Assembler::zero, copy_tail);
10516 
10517       pmovzxbw(tmp1, Address(src, 0));
10518       movdqu(Address(dst, 0), tmp1);
10519       addptr(src, 8);
10520       addptr(dst, 2 * 8);
10521 
10522       jmp(copy_tail, true);
10523     }
10524 
10525     // inflate 8 chars per iter
10526     bind(copy_8_loop);
10527     pmovzxbw(tmp1, Address(src, len, Address::times_1));  // unpack to 8 words
10528     movdqu(Address(dst, len, Address::times_2), tmp1);
10529     addptr(len, 8);
10530     jcc(Assembler::notZero, copy_8_loop);
10531 
10532     bind(copy_tail);
10533     movl(len, tmp2);
10534 
10535     cmpl(len, 4);
10536     jccb(Assembler::less, copy_bytes);
10537 
10538     movdl(tmp1, Address(src, 0));  // load 4 byte chars
10539     pmovzxbw(tmp1, tmp1);
10540     movq(Address(dst, 0), tmp1);
10541     subptr(len, 4);
10542     addptr(src, 4);
10543     addptr(dst, 8);
10544 
10545     bind(copy_bytes);
10546   } else {
10547     bind(below_threshold);
10548   }
10549 
10550   testl(len, len);
10551   jccb(Assembler::zero, done);
10552   lea(src, Address(src, len, Address::times_1));
10553   lea(dst, Address(dst, len, Address::times_2));
10554   negptr(len);
10555 
10556   // inflate 1 char per iter
10557   bind(copy_chars_loop);
10558   load_unsigned_byte(tmp2, Address(src, len, Address::times_1));  // load byte char
10559   movw(Address(dst, len, Address::times_2), tmp2);  // inflate byte char to word
10560   increment(len);
10561   jcc(Assembler::notZero, copy_chars_loop);
10562 
10563   bind(done);
10564 }
10565 
10566 #ifdef _LP64
10567 void MacroAssembler::convert_f2i(Register dst, XMMRegister src) {
10568   Label done;
10569   cvttss2sil(dst, src);
10570   // Conversion instructions do not match JLS for overflow, underflow and NaN -&gt; fixup in stub
10571   cmpl(dst, 0x80000000); // float_sign_flip
10572   jccb(Assembler::notEqual, done);
10573   subptr(rsp, 8);
10574   movflt(Address(rsp, 0), src);
10575   call(RuntimeAddress(CAST_FROM_FN_PTR(address, StubRoutines::x86::f2i_fixup())));
10576   pop(dst);
10577   bind(done);
10578 }
10579 
10580 void MacroAssembler::convert_d2i(Register dst, XMMRegister src) {
10581   Label done;
10582   cvttsd2sil(dst, src);
10583   // Conversion instructions do not match JLS for overflow, underflow and NaN -&gt; fixup in stub
10584   cmpl(dst, 0x80000000); // float_sign_flip
10585   jccb(Assembler::notEqual, done);
10586   subptr(rsp, 8);
10587   movdbl(Address(rsp, 0), src);
10588   call(RuntimeAddress(CAST_FROM_FN_PTR(address, StubRoutines::x86::d2i_fixup())));
10589   pop(dst);
10590   bind(done);
10591 }
10592 
10593 void MacroAssembler::convert_f2l(Register dst, XMMRegister src) {
10594   Label done;
10595   cvttss2siq(dst, src);
10596   cmp64(dst, ExternalAddress((address) StubRoutines::x86::double_sign_flip()));
10597   jccb(Assembler::notEqual, done);
10598   subptr(rsp, 8);
10599   movflt(Address(rsp, 0), src);
10600   call(RuntimeAddress(CAST_FROM_FN_PTR(address, StubRoutines::x86::f2l_fixup())));
10601   pop(dst);
10602   bind(done);
10603 }
10604 
10605 void MacroAssembler::convert_d2l(Register dst, XMMRegister src) {
10606   Label done;
10607   cvttsd2siq(dst, src);
10608   cmp64(dst, ExternalAddress((address) StubRoutines::x86::double_sign_flip()));
10609   jccb(Assembler::notEqual, done);
10610   subptr(rsp, 8);
10611   movdbl(Address(rsp, 0), src);
10612   call(RuntimeAddress(CAST_FROM_FN_PTR(address, StubRoutines::x86::d2l_fixup())));
10613   pop(dst);
10614   bind(done);
10615 }
10616 
10617 void MacroAssembler::cache_wb(Address line)
10618 {
10619   // 64 bit cpus always support clflush
10620   assert(VM_Version::supports_clflush(), &quot;clflush should be available&quot;);
10621   bool optimized = VM_Version::supports_clflushopt();
10622   bool no_evict = VM_Version::supports_clwb();
10623 
10624   // prefer clwb (writeback without evict) otherwise
10625   // prefer clflushopt (potentially parallel writeback with evict)
10626   // otherwise fallback on clflush (serial writeback with evict)
10627 
10628   if (optimized) {
10629     if (no_evict) {
10630       clwb(line);
10631     } else {
10632       clflushopt(line);
10633     }
10634   } else {
10635     // no need for fence when using CLFLUSH
10636     clflush(line);
10637   }
10638 }
10639 
10640 void MacroAssembler::cache_wbsync(bool is_pre)
10641 {
10642   assert(VM_Version::supports_clflush(), &quot;clflush should be available&quot;);
10643   bool optimized = VM_Version::supports_clflushopt();
10644   bool no_evict = VM_Version::supports_clwb();
10645 
10646   // pick the correct implementation
10647 
10648   if (!is_pre &amp;&amp; (optimized || no_evict)) {
10649     // need an sfence for post flush when using clflushopt or clwb
10650     // otherwise no no need for any synchroniaztion
10651 
10652     sfence();
10653   }
10654 }
10655 #endif // _LP64
10656 
10657 Assembler::Condition MacroAssembler::negate_condition(Assembler::Condition cond) {
10658   switch (cond) {
10659     // Note some conditions are synonyms for others
10660     case Assembler::zero:         return Assembler::notZero;
10661     case Assembler::notZero:      return Assembler::zero;
10662     case Assembler::less:         return Assembler::greaterEqual;
10663     case Assembler::lessEqual:    return Assembler::greater;
10664     case Assembler::greater:      return Assembler::lessEqual;
10665     case Assembler::greaterEqual: return Assembler::less;
10666     case Assembler::below:        return Assembler::aboveEqual;
10667     case Assembler::belowEqual:   return Assembler::above;
10668     case Assembler::above:        return Assembler::belowEqual;
10669     case Assembler::aboveEqual:   return Assembler::below;
10670     case Assembler::overflow:     return Assembler::noOverflow;
10671     case Assembler::noOverflow:   return Assembler::overflow;
10672     case Assembler::negative:     return Assembler::positive;
10673     case Assembler::positive:     return Assembler::negative;
10674     case Assembler::parity:       return Assembler::noParity;
10675     case Assembler::noParity:     return Assembler::parity;
10676   }
10677   ShouldNotReachHere(); return Assembler::overflow;
10678 }
10679 
10680 SkipIfEqual::SkipIfEqual(
10681     MacroAssembler* masm, const bool* flag_addr, bool value) {
10682   _masm = masm;
10683   _masm-&gt;cmp8(ExternalAddress((address)flag_addr), value);
10684   _masm-&gt;jcc(Assembler::equal, _label);
10685 }
10686 
10687 SkipIfEqual::~SkipIfEqual() {
10688   _masm-&gt;bind(_label);
10689 }
10690 
10691 // 32-bit Windows has its own fast-path implementation
10692 // of get_thread
10693 #if !defined(WIN32) || defined(_LP64)
10694 
10695 // This is simply a call to Thread::current()
10696 void MacroAssembler::get_thread(Register thread) {
10697   if (thread != rax) {
10698     push(rax);
10699   }
10700   LP64_ONLY(push(rdi);)
10701   LP64_ONLY(push(rsi);)
10702   push(rdx);
10703   push(rcx);
10704 #ifdef _LP64
10705   push(r8);
10706   push(r9);
10707   push(r10);
10708   push(r11);
10709 #endif
10710 
10711   MacroAssembler::call_VM_leaf_base(CAST_FROM_FN_PTR(address, Thread::current), 0);
10712 
10713 #ifdef _LP64
10714   pop(r11);
10715   pop(r10);
10716   pop(r9);
10717   pop(r8);
10718 #endif
10719   pop(rcx);
10720   pop(rdx);
10721   LP64_ONLY(pop(rsi);)
10722   LP64_ONLY(pop(rdi);)
10723   if (thread != rax) {
10724     mov(thread, rax);
10725     pop(rax);
10726   }
10727 }
10728 
10729 #endif // !WIN32 || _LP64
    </pre>
  </body>
</html>