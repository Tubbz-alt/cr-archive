diff a/src/hotspot/cpu/x86/macroAssembler_x86.cpp b/src/hotspot/cpu/x86/macroAssembler_x86.cpp
--- a/src/hotspot/cpu/x86/macroAssembler_x86.cpp
+++ b/src/hotspot/cpu/x86/macroAssembler_x86.cpp
@@ -6418,30 +6418,22 @@
   int sp_inc = unpack_value_args_common(C, receiver_only);
   // Emit code for verified entry and save increment for stack repair on return
   verified_entry(C, sp_inc);
 }
 
-int MacroAssembler::shuffle_value_args(bool is_packing, bool receiver_only, int extra_stack_offset,
-                                       BasicType* sig_bt, const GrowableArray<SigEntry>* sig_cc,
-                                       int args_passed, int args_on_stack, VMRegPair* regs,            // from
-                                       int args_passed_to, int args_on_stack_to, VMRegPair* regs_to) { // to
+void MacroAssembler::shuffle_value_args(bool is_packing, bool receiver_only, int extra_stack_offset,
+                                        BasicType* sig_bt, const GrowableArray<SigEntry>* sig_cc,
+                                        int args_passed, int args_on_stack, VMRegPair* regs,
+                                        int args_passed_to, int args_on_stack_to, VMRegPair* regs_to, int sp_inc) {
   // Check if we need to extend the stack for packing/unpacking
-  int sp_inc = (args_on_stack_to - args_on_stack) * VMRegImpl::stack_slot_size;
-  if (sp_inc > 0) {
-    sp_inc = align_up(sp_inc, StackAlignmentInBytes);
-    if (!is_packing) {
-      // Save the return address, adjust the stack (make sure it is properly
-      // 16-byte aligned) and copy the return address to the new top of the stack.
-      // (Note: C1 does this in C1_MacroAssembler::scalarized_entry).
-      pop(r13);
-      subptr(rsp, sp_inc);
-      push(r13);
-    }
-  } else {
-    // The scalarized calling convention needs less stack space than the unscalarized one.
-    // No need to extend the stack, the caller will take care of these adjustments.
-    sp_inc = 0;
+  if (sp_inc > 0 && !is_packing) {
+    // Save the return address, adjust the stack (make sure it is properly
+    // 16-byte aligned) and copy the return address to the new top of the stack.
+    // (Note: C1 does this in C1_MacroAssembler::scalarized_entry).
+    pop(r13);
+    subptr(rsp, sp_inc);
+    push(r13);
   }
 
   int ret_off; // make sure we don't overwrite the return address
   if (is_packing) {
     // For C1 code, the VVEP doesn't have reserved slots, so we store the returned address at
@@ -6450,35 +6442,30 @@
   } else {
     // C2 code ensures that sp_inc is a reserved slot.
     ret_off = sp_inc;
   }
 
-  return shuffle_value_args_common(is_packing, receiver_only, extra_stack_offset,
-                                   sig_bt, sig_cc,
-                                   args_passed, args_on_stack, regs,
-                                   args_passed_to, args_on_stack_to, regs_to,
-                                   sp_inc, ret_off);
+  shuffle_value_args_common(is_packing, receiver_only, extra_stack_offset,
+                            sig_bt, sig_cc,
+                            args_passed, args_on_stack, regs,
+                            args_passed_to, args_on_stack_to, regs_to,
+                            sp_inc, ret_off);
 }
 
 VMReg MacroAssembler::spill_reg_for(VMReg reg) {
   return reg->is_XMMRegister() ? xmm8->as_VMReg() : r14->as_VMReg();
 }
 
-// Restores the stack on return
-void MacroAssembler::restore_stack(Compile* C) {
-  int framesize = C->frame_size_in_bytes();
-  assert((framesize & (StackAlignmentInBytes-1)) == 0, "frame size not aligned");
-  // Remove word for return addr already pushed and RBP
-  framesize -= 2*wordSize;
-
-  if (C->needs_stack_repair()) {
+void MacroAssembler::remove_frame(int initial_framesize, bool needs_stack_repair, int sp_inc_offset) {
+  assert((initial_framesize & (StackAlignmentInBytes-1)) == 0, "frame size not aligned");
+  if (needs_stack_repair) {
     // Restore rbp and repair rsp by adding the stack increment
-    movq(rbp, Address(rsp, framesize));
-    addq(rsp, Address(rsp, C->sp_inc_offset()));
+    movq(rbp, Address(rsp, initial_framesize));
+    addq(rsp, Address(rsp, sp_inc_offset));
   } else {
-    if (framesize > 0) {
-      addq(rsp, framesize);
+    if (initial_framesize > 0) {
+      addq(rsp, initial_framesize);
     }
     pop(rbp);
   }
 }
 
