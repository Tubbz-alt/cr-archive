<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Cdiff src/hotspot/cpu/x86/macroAssembler_x86.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="frame_x86.cpp.cdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="macroAssembler_x86.hpp.cdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/cpu/x86/macroAssembler_x86.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<hr />
<pre>
<span class="line-old-header">*** 6418,30 ***</span>
    int sp_inc = unpack_value_args_common(C, receiver_only);
    // Emit code for verified entry and save increment for stack repair on return
    verified_entry(C, sp_inc);
  }
  
<span class="line-modified">! int MacroAssembler::shuffle_value_args(bool is_packing, bool receiver_only, int extra_stack_offset,</span>
<span class="line-modified">!                                        BasicType* sig_bt, const GrowableArray&lt;SigEntry&gt;* sig_cc,</span>
<span class="line-modified">!                                        int args_passed, int args_on_stack, VMRegPair* regs,            // from</span>
<span class="line-modified">!                                        int args_passed_to, int args_on_stack_to, VMRegPair* regs_to) { // to</span>
    // Check if we need to extend the stack for packing/unpacking
<span class="line-modified">!   int sp_inc = (args_on_stack_to - args_on_stack) * VMRegImpl::stack_slot_size;</span>
<span class="line-modified">!   if (sp_inc &gt; 0) {</span>
<span class="line-modified">!     sp_inc = align_up(sp_inc, StackAlignmentInBytes);</span>
<span class="line-modified">!     if (!is_packing) {</span>
<span class="line-modified">!       // Save the return address, adjust the stack (make sure it is properly</span>
<span class="line-modified">!       // 16-byte aligned) and copy the return address to the new top of the stack.</span>
<span class="line-modified">!       // (Note: C1 does this in C1_MacroAssembler::scalarized_entry).</span>
<span class="line-removed">-       pop(r13);</span>
<span class="line-removed">-       subptr(rsp, sp_inc);</span>
<span class="line-removed">-       push(r13);</span>
<span class="line-removed">-     }</span>
<span class="line-removed">-   } else {</span>
<span class="line-removed">-     // The scalarized calling convention needs less stack space than the unscalarized one.</span>
<span class="line-removed">-     // No need to extend the stack, the caller will take care of these adjustments.</span>
<span class="line-removed">-     sp_inc = 0;</span>
    }
  
    int ret_off; // make sure we don&#39;t overwrite the return address
    if (is_packing) {
      // For C1 code, the VVEP doesn&#39;t have reserved slots, so we store the returned address at
<span class="line-new-header">--- 6418,22 ---</span>
    int sp_inc = unpack_value_args_common(C, receiver_only);
    // Emit code for verified entry and save increment for stack repair on return
    verified_entry(C, sp_inc);
  }
  
<span class="line-modified">! void MacroAssembler::shuffle_value_args(bool is_packing, bool receiver_only, int extra_stack_offset,</span>
<span class="line-modified">!                                         BasicType* sig_bt, const GrowableArray&lt;SigEntry&gt;* sig_cc,</span>
<span class="line-modified">!                                         int args_passed, int args_on_stack, VMRegPair* regs,</span>
<span class="line-modified">!                                         int args_passed_to, int args_on_stack_to, VMRegPair* regs_to, int sp_inc) {</span>
    // Check if we need to extend the stack for packing/unpacking
<span class="line-modified">!   if (sp_inc &gt; 0 &amp;&amp; !is_packing) {</span>
<span class="line-modified">!     // Save the return address, adjust the stack (make sure it is properly</span>
<span class="line-modified">!     // 16-byte aligned) and copy the return address to the new top of the stack.</span>
<span class="line-modified">!     // (Note: C1 does this in C1_MacroAssembler::scalarized_entry).</span>
<span class="line-modified">!     pop(r13);</span>
<span class="line-modified">!     subptr(rsp, sp_inc);</span>
<span class="line-modified">!     push(r13);</span>
    }
  
    int ret_off; // make sure we don&#39;t overwrite the return address
    if (is_packing) {
      // For C1 code, the VVEP doesn&#39;t have reserved slots, so we store the returned address at
</pre>
<hr />
<pre>
<span class="line-old-header">*** 6450,35 ***</span>
    } else {
      // C2 code ensures that sp_inc is a reserved slot.
      ret_off = sp_inc;
    }
  
<span class="line-modified">!   return shuffle_value_args_common(is_packing, receiver_only, extra_stack_offset,</span>
<span class="line-modified">!                                    sig_bt, sig_cc,</span>
<span class="line-modified">!                                    args_passed, args_on_stack, regs,</span>
<span class="line-modified">!                                    args_passed_to, args_on_stack_to, regs_to,</span>
<span class="line-modified">!                                    sp_inc, ret_off);</span>
  }
  
  VMReg MacroAssembler::spill_reg_for(VMReg reg) {
    return reg-&gt;is_XMMRegister() ? xmm8-&gt;as_VMReg() : r14-&gt;as_VMReg();
  }
  
<span class="line-modified">! // Restores the stack on return</span>
<span class="line-modified">! void MacroAssembler::restore_stack(Compile* C) {</span>
<span class="line-modified">!   int framesize = C-&gt;frame_size_in_bytes();</span>
<span class="line-removed">-   assert((framesize &amp; (StackAlignmentInBytes-1)) == 0, &quot;frame size not aligned&quot;);</span>
<span class="line-removed">-   // Remove word for return addr already pushed and RBP</span>
<span class="line-removed">-   framesize -= 2*wordSize;</span>
<span class="line-removed">- </span>
<span class="line-removed">-   if (C-&gt;needs_stack_repair()) {</span>
      // Restore rbp and repair rsp by adding the stack increment
<span class="line-modified">!     movq(rbp, Address(rsp, framesize));</span>
<span class="line-modified">!     addq(rsp, Address(rsp, C-&gt;sp_inc_offset()));</span>
    } else {
<span class="line-modified">!     if (framesize &gt; 0) {</span>
<span class="line-modified">!       addq(rsp, framesize);</span>
      }
      pop(rbp);
    }
  }
  
<span class="line-new-header">--- 6442,30 ---</span>
    } else {
      // C2 code ensures that sp_inc is a reserved slot.
      ret_off = sp_inc;
    }
  
<span class="line-modified">!   shuffle_value_args_common(is_packing, receiver_only, extra_stack_offset,</span>
<span class="line-modified">!                             sig_bt, sig_cc,</span>
<span class="line-modified">!                             args_passed, args_on_stack, regs,</span>
<span class="line-modified">!                             args_passed_to, args_on_stack_to, regs_to,</span>
<span class="line-modified">!                             sp_inc, ret_off);</span>
  }
  
  VMReg MacroAssembler::spill_reg_for(VMReg reg) {
    return reg-&gt;is_XMMRegister() ? xmm8-&gt;as_VMReg() : r14-&gt;as_VMReg();
  }
  
<span class="line-modified">! void MacroAssembler::remove_frame(int initial_framesize, bool needs_stack_repair, int sp_inc_offset) {</span>
<span class="line-modified">!   assert((initial_framesize &amp; (StackAlignmentInBytes-1)) == 0, &quot;frame size not aligned&quot;);</span>
<span class="line-modified">!   if (needs_stack_repair) {</span>
      // Restore rbp and repair rsp by adding the stack increment
<span class="line-modified">!     movq(rbp, Address(rsp, initial_framesize));</span>
<span class="line-modified">!     addq(rsp, Address(rsp, sp_inc_offset));</span>
    } else {
<span class="line-modified">!     if (initial_framesize &gt; 0) {</span>
<span class="line-modified">!       addq(rsp, initial_framesize);</span>
      }
      pop(rbp);
    }
  }
  
</pre>
<center><a href="frame_x86.cpp.cdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="macroAssembler_x86.hpp.cdiff.html" target="_top">next &gt;</a></center>  </body>
</html>