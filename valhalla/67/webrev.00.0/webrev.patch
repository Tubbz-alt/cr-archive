diff a/.hgtags b/.hgtags
--- a/.hgtags
+++ b/.hgtags
@@ -635,5 +635,6 @@
 5c7ec21f5d13f6eb5cd32288c69b8be2f9cac256 jdk-15+16
 dd5198db2e5b1ebcafe065d987c03ba9fcb50fc3 jdk-15+17
 44aef192b488a48cce12422394691a6b1d16b98e jdk-15+18
 7cc27caabe6e342151e8baf549beb07a9c755ec2 jdk-15+19
 46bca5e5e6fb26efd07245d26fe96a9c3260f51e jdk-15+20
+12b55fad80f30d24b1f8fdb3b947ea6465ef9518 jdk-15+21
diff a/make/conf/jib-profiles.js b/make/conf/jib-profiles.js
--- a/make/conf/jib-profiles.js
+++ b/make/conf/jib-profiles.js
@@ -984,11 +984,11 @@
     var devkit_platform_revisions = {
         linux_x64: "gcc9.2.0-OL6.4+1.0",
         macosx_x64: "Xcode10.1-MacOSX10.14+1.0",
         solaris_x64: "SS12u4-Solaris11u1+1.0",
         solaris_sparcv9: "SS12u6-Solaris11u3+1.0",
-        windows_x64: "VS2017-15.9.16+1.0",
+        windows_x64: "VS2019-16.5.3+1.0",
         linux_aarch64: "gcc9.2.0-OL7.6+1.0",
         linux_arm: "gcc8.2.0-Fedora27+1.0",
         linux_ppc64le: "gcc8.2.0-Fedora27+1.0",
         linux_s390x: "gcc8.2.0-Fedora27+1.0"
     };
diff a/src/hotspot/cpu/aarch64/aarch64.ad b/src/hotspot/cpu/aarch64/aarch64.ad
--- a/src/hotspot/cpu/aarch64/aarch64.ad
+++ b/src/hotspot/cpu/aarch64/aarch64.ad
@@ -13312,11 +13312,11 @@
 
   ins_pipe(fp_div_s);
 %}
 
 instruct sqrtF_reg(vRegF dst, vRegF src) %{
-  match(Set dst (ConvD2F (SqrtD (ConvF2D src))));
+  match(Set dst (SqrtF src));
 
   ins_cost(INSN_COST * 50);
   format %{ "fsqrts  $dst, $src" %}
   ins_encode %{
     __ fsqrts(as_FloatRegister($dst$$reg),
@@ -17109,10 +17109,32 @@
   ins_pipe(vmuldiv_fp128);
 %}
 
 // --------------------------------- SQRT -------------------------------------
 
+instruct vsqrt2F(vecD dst, vecD src)
+%{
+  predicate(n->as_Vector()->length() == 2);
+  match(Set dst (SqrtVF src));
+  format %{ "fsqrt  $dst, $src\t# vector (2F)" %}
+  ins_encode %{
+    __ fsqrt(as_FloatRegister($dst$$reg), __ T2S, as_FloatRegister($src$$reg));
+  %}
+  ins_pipe(vunop_fp64);
+%}
+
+instruct vsqrt4F(vecX dst, vecX src)
+%{
+  predicate(n->as_Vector()->length() == 4);
+  match(Set dst (SqrtVF src));
+  format %{ "fsqrt  $dst, $src\t# vector (4F)" %}
+  ins_encode %{
+    __ fsqrt(as_FloatRegister($dst$$reg), __ T4S, as_FloatRegister($src$$reg));
+  %}
+  ins_pipe(vsqrt_fp128);
+%}
+
 instruct vsqrt2D(vecX dst, vecX src)
 %{
   predicate(n->as_Vector()->length() == 2);
   match(Set dst (SqrtVD src));
   format %{ "fsqrt  $dst, $src\t# vector (2D)" %}
diff a/src/hotspot/share/classfile/classLoader.cpp b/src/hotspot/share/classfile/classLoader.cpp
--- a/src/hotspot/share/classfile/classLoader.cpp
+++ b/src/hotspot/share/classfile/classLoader.cpp
@@ -93,10 +93,11 @@
 static ZipClose_t        ZipClose           = NULL;
 static FindEntry_t       FindEntry          = NULL;
 static ReadEntry_t       ReadEntry          = NULL;
 static GetNextEntry_t    GetNextEntry       = NULL;
 static Crc32_t           Crc32              = NULL;
+int ClassLoader::_libzip_loaded = 0;
 
 // Entry points for jimage.dll for loading jimage file entries
 
 static JImageOpen_t                    JImageOpen             = NULL;
 static JImageClose_t                   JImageClose            = NULL;
@@ -745,10 +746,11 @@
       jzfile* zip;
       {
         // enable call to C land
         ThreadToNativeFromVM ttn(thread);
         HandleMark hm(thread);
+        load_zip_library_if_needed();
         zip = (*ZipOpen)(canonical_path, &error_msg);
       }
       if (zip != NULL && error_msg == NULL) {
         new_entry = new ClassPathZipEntry(zip, path, is_boot_append, from_class_path_attr);
       } else {
@@ -794,10 +796,11 @@
         {
           // enable call to C land
           JavaThread* thread = JavaThread::current();
           ThreadToNativeFromVM ttn(thread);
           HandleMark hm(thread);
+          load_zip_library_if_needed();
           zip = (*ZipOpen)(canonical_path, &error_msg);
         }
         if (zip != NULL && error_msg == NULL) {
           // create using canonical path
           return new ClassPathZipEntry(zip, canonical_path, is_boot_append, false);
@@ -965,10 +968,18 @@
   }
 
   CanonicalizeEntry = CAST_TO_FN_PTR(canonicalize_fn_t, dll_lookup(javalib_handle, "JDK_Canonicalize", NULL));
 }
 
+void ClassLoader::release_load_zip_library() {
+  MutexLocker locker(Zip_lock, Monitor::_no_safepoint_check_flag);
+  if (_libzip_loaded == 0) {
+    load_zip_library();
+    Atomic::release_store(&_libzip_loaded, 1);
+  }
+}
+
 void ClassLoader::load_zip_library() {
   assert(ZipOpen == NULL, "should not load zip library twice");
   char path[JVM_MAXPATHLEN];
   char ebuf[1024];
   void* handle = NULL;
@@ -1006,10 +1017,11 @@
   JImageGetResource = CAST_TO_FN_PTR(JImageGetResource_t, dll_lookup(handle, "JIMAGE_GetResource", path));
   JImageResourceIterator = CAST_TO_FN_PTR(JImageResourceIterator_t, dll_lookup(handle, "JIMAGE_ResourceIterator", path));
 }
 
 int ClassLoader::crc32(int crc, const char* buf, int len) {
+  load_zip_library_if_needed();
   return (*Crc32)(crc, (const jbyte*)buf, len);
 }
 
 oop ClassLoader::get_system_package(const char* name, TRAPS) {
   // Look up the name in the boot loader's package entry table.
@@ -1464,12 +1476,10 @@
                         "unsafeDefineClassCalls");
   }
 
   // lookup java library entry points
   load_java_library();
-  // lookup zip library entry points
-  load_zip_library();
   // jimage library entry points are loaded below, in lookup_vm_options
   setup_bootstrap_search_path();
 }
 
 char* lookup_vm_resource(JImageFile *jimage, const char *jimage_version, const char *path) {
diff a/src/hotspot/share/classfile/systemDictionary.cpp b/src/hotspot/share/classfile/systemDictionary.cpp
--- a/src/hotspot/share/classfile/systemDictionary.cpp
+++ b/src/hotspot/share/classfile/systemDictionary.cpp
@@ -2342,11 +2342,11 @@
   }
 
   return klass;
 }
 
-
+bool SystemDictionary::add_loader_constraint(Symbol* class_name,
 bool SystemDictionary::add_loader_constraint(Symbol* class_name,
                                              Handle class_loader1,
                                              Handle class_loader2,
                                              Thread* THREAD) {
   ClassLoaderData* loader_data1 = class_loader_data(class_loader1);
@@ -2381,10 +2381,18 @@
     MutexLocker mu_s(THREAD, SystemDictionary_lock);
     InstanceKlass* klass1 = find_class(d_hash1, constraint_name, dictionary1);
     InstanceKlass* klass2 = find_class(d_hash2, constraint_name, dictionary2);
     bool result = constraints()->add_entry(constraint_name, klass1, class_loader1,
                                            klass2, class_loader2);
+#if INCLUDE_CDS
+    if (Arguments::is_dumping_archive() && klass_being_linked != NULL &&
+        !klass_being_linked->is_shared()) {
+         SystemDictionaryShared::record_linking_constraint(constraint_name,
+                                     InstanceKlass::cast(klass_being_linked),
+                                     class_loader1, class_loader2, THREAD);
+    }
+#endif // INCLUDE_CDS
     if (Signature::is_array(class_name)) {
       constraint_name->decrement_refcount();
     }
     return result;
   }
@@ -2502,18 +2510,19 @@
 // its supertype about the meaning of a type name, as long as that
 // class neither calls a relevant method of the supertype, nor is
 // called (perhaps via an override) from the supertype.
 //
 //
-// SystemDictionary::check_signature_loaders(sig, l1, l2)
+// SystemDictionary::check_signature_loaders(sig, klass_being_linked, l1, l2)
 //
 // Make sure all class components (including arrays) in the given
 // signature will be resolved to the same class in both loaders.
 // Returns the name of the type that failed a loader constraint check, or
 // NULL if no constraint failed.  No exception except OOME is thrown.
 // Arrays are not added to the loader constraint table, their elements are.
 Symbol* SystemDictionary::check_signature_loaders(Symbol* signature,
+                                               Klass* klass_being_linked,
                                                Handle loader1, Handle loader2,
                                                bool is_method, TRAPS)  {
   // Nothing to do if loaders are the same.
   if (loader1() == loader2()) {
     return NULL;
@@ -2523,19 +2532,18 @@
     if (ss.is_reference()) {
       Symbol* sig = ss.as_symbol();
       // Note: In the future, if template-like types can take
       // arguments, we will want to recognize them and dig out class
       // names hiding inside the argument lists.
-      if (!add_loader_constraint(sig, loader1, loader2, THREAD)) {
+      if (!add_loader_constraint(sig, klass_being_linked, loader1, loader2, THREAD)) {
         return sig;
       }
     }
   }
   return NULL;
 }
 
-
 Method* SystemDictionary::find_method_handle_intrinsic(vmIntrinsics::ID iid,
                                                        Symbol* signature,
                                                        TRAPS) {
   methodHandle empty;
   assert(MethodHandles::is_signature_polymorphic(iid) &&
diff a/src/hotspot/share/classfile/systemDictionary.hpp b/src/hotspot/share/classfile/systemDictionary.hpp
--- a/src/hotspot/share/classfile/systemDictionary.hpp
+++ b/src/hotspot/share/classfile/systemDictionary.hpp
@@ -503,14 +503,14 @@
 
 public:
   // Note:  java_lang_Class::primitive_type is the inverse of java_mirror
 
   // Check class loader constraints
-  static bool add_loader_constraint(Symbol* name, Handle loader1,
+  static bool add_loader_constraint(Symbol* name, Klass* klass_being_linked,  Handle loader1,
                                     Handle loader2, TRAPS);
-  static Symbol* check_signature_loaders(Symbol* signature, Handle loader1,
-                                         Handle loader2, bool is_method, TRAPS);
+  static Symbol* check_signature_loaders(Symbol* signature, Klass* klass_being_linked,
+                                         Handle loader1, Handle loader2, bool is_method, TRAPS);
 
   // JSR 292
   // find a java.lang.invoke.MethodHandle.invoke* method for a given signature
   // (asks Java to compute it if necessary, except in a compiler thread)
   static Method* find_method_handle_invoker(Klass* klass,
@@ -665,11 +665,16 @@
   static bool is_parallelDefine(Handle class_loader);
 
 public:
   static bool is_system_class_loader(oop class_loader);
   static bool is_platform_class_loader(oop class_loader);
-
+  static bool is_boot_class_loader(oop class_loader) { return class_loader == NULL; }
+  static bool is_builtin_class_loader(oop class_loader) {
+    return is_boot_class_loader(class_loader)      ||
+           is_platform_class_loader(class_loader)  ||
+           is_system_class_loader(class_loader);
+  }
   // Returns TRUE if the method is a non-public member of class java.lang.Object.
   static bool is_nonpublic_Object_method(Method* m) {
     assert(m != NULL, "Unexpected NULL Method*");
     return !m->is_public() && m->method_holder() == SystemDictionary::Object_klass();
   }
diff a/src/hotspot/share/interpreter/linkResolver.cpp b/src/hotspot/share/interpreter/linkResolver.cpp
--- a/src/hotspot/share/interpreter/linkResolver.cpp
+++ b/src/hotspot/share/interpreter/linkResolver.cpp
@@ -662,11 +662,13 @@
   Handle current_loader(THREAD, link_info.current_klass()->class_loader());
   Handle resolved_loader(THREAD, resolved_method->method_holder()->class_loader());
 
   ResourceMark rm(THREAD);
   Symbol* failed_type_symbol =
-    SystemDictionary::check_signature_loaders(link_info.signature(), current_loader,
+    SystemDictionary::check_signature_loaders(link_info.signature(),
+                                              /*klass_being_linked*/ NULL, // We are not linking class
+                                              current_loader,
                                               resolved_loader, true, CHECK);
   if (failed_type_symbol != NULL) {
     Klass* current_class = link_info.current_klass();
     ClassLoaderData* current_loader_data = current_class->class_loader_data();
     assert(current_loader_data != NULL, "current class has no class loader data");
@@ -698,10 +700,11 @@
   Handle sel_loader(THREAD, sel_klass->class_loader());
 
   ResourceMark rm(THREAD);  // needed for check_signature_loaders
   Symbol* failed_type_symbol =
     SystemDictionary::check_signature_loaders(sig,
+                                              /*klass_being_linked*/ NULL, // We are not linking class
                                               ref_loader, sel_loader,
                                               false,
                                               CHECK);
   if (failed_type_symbol != NULL) {
     stringStream ss;
diff a/src/hotspot/share/memory/dynamicArchive.cpp b/src/hotspot/share/memory/dynamicArchive.cpp
--- a/src/hotspot/share/memory/dynamicArchive.cpp
+++ b/src/hotspot/share/memory/dynamicArchive.cpp
@@ -515,18 +515,17 @@
   void set_symbols_permanent();
   void relocate_buffer_to_target();
   void write_archive(char* serialized_data);
 
   void init_first_dump_space(address reserved_bottom) {
-    address first_space_base = reserved_bottom;
     DumpRegion* mc_space = MetaspaceShared::misc_code_dump_space();
     DumpRegion* rw_space = MetaspaceShared::read_write_dump_space();
 
     // Use the same MC->RW->RO ordering as in the base archive.
-    MetaspaceShared::init_shared_dump_space(mc_space, first_space_base);
+    MetaspaceShared::init_shared_dump_space(mc_space);
     _current_dump_space = mc_space;
-    _last_verified_top = first_space_base;
+    _last_verified_top = reserved_bottom;
     _num_dump_regions_used = 1;
   }
 
   void reserve_buffers_for_trampolines() {
     size_t n = _estimated_trampoline_bytes;
diff a/src/hotspot/share/memory/metaspaceShared.cpp b/src/hotspot/share/memory/metaspaceShared.cpp
--- a/src/hotspot/share/memory/metaspaceShared.cpp
+++ b/src/hotspot/share/memory/metaspaceShared.cpp
@@ -77,10 +77,12 @@
 #include "gc/g1/g1CollectedHeap.hpp"
 #endif
 
 ReservedSpace MetaspaceShared::_shared_rs;
 VirtualSpace MetaspaceShared::_shared_vs;
+ReservedSpace MetaspaceShared::_symbol_rs;
+VirtualSpace MetaspaceShared::_symbol_vs;
 MetaspaceSharedStats MetaspaceShared::_stats;
 bool MetaspaceShared::_has_error_classes;
 bool MetaspaceShared::_archive_loading_failed = false;
 bool MetaspaceShared::_remapped_readwrite = false;
 address MetaspaceShared::_i2i_entry_code_buffers = NULL;
@@ -120,25 +122,28 @@
   assert(newtop >= _top, "must not grow backwards");
   if (newtop > _end) {
     MetaspaceShared::report_out_of_space(_name, newtop - _top);
     ShouldNotReachHere();
   }
-  uintx delta;
-  if (DynamicDumpSharedSpaces) {
-    delta = DynamicArchive::object_delta_uintx(newtop);
-  } else {
-    delta = MetaspaceShared::object_delta_uintx(newtop);
-  }
-  if (delta > MAX_SHARED_DELTA) {
-    // This is just a sanity check and should not appear in any real world usage. This
-    // happens only if you allocate more than 2GB of shared objects and would require
-    // millions of shared classes.
-    vm_exit_during_initialization("Out of memory in the CDS archive",
-                                  "Please reduce the number of shared classes.");
+
+  if (_rs == MetaspaceShared::shared_rs()) {
+    uintx delta;
+    if (DynamicDumpSharedSpaces) {
+      delta = DynamicArchive::object_delta_uintx(newtop);
+    } else {
+      delta = MetaspaceShared::object_delta_uintx(newtop);
+    }
+    if (delta > MAX_SHARED_DELTA) {
+      // This is just a sanity check and should not appear in any real world usage. This
+      // happens only if you allocate more than 2GB of shared objects and would require
+      // millions of shared classes.
+      vm_exit_during_initialization("Out of memory in the CDS archive",
+                                    "Please reduce the number of shared classes.");
+    }
   }
 
-  MetaspaceShared::commit_shared_space_to(newtop);
+  MetaspaceShared::commit_to(_rs, _vs, newtop);
   _top = newtop;
   return _top;
 }
 
 char* DumpRegion::allocate(size_t num_bytes, size_t alignment) {
@@ -172,30 +177,39 @@
   if (strcmp(_name, failing_region) == 0) {
     log_error(cds)(" required = %d", int(needed_bytes));
   }
 }
 
+void DumpRegion::init(ReservedSpace* rs, VirtualSpace* vs) {
+  _rs = rs;
+  _vs = vs;
+  // Start with 0 committed bytes. The memory will be committed as needed by
+  // MetaspaceShared::commit_to().
+  if (!_vs->initialize(*_rs, 0)) {
+    fatal("Unable to allocate memory for shared space");
+  }
+  _base = _top = _rs->base();
+  _end = _rs->end();
+}
+
 void DumpRegion::pack(DumpRegion* next) {
   assert(!is_packed(), "sanity");
   _end = (char*)align_up(_top, Metaspace::reserve_alignment());
   _is_packed = true;
   if (next != NULL) {
+    next->_rs = _rs;
+    next->_vs = _vs;
     next->_base = next->_top = this->_end;
-    next->_end = MetaspaceShared::shared_rs()->end();
+    next->_end = _rs->end();
   }
 }
 
-static DumpRegion _mc_region("mc"), _ro_region("ro"), _rw_region("rw");
+static DumpRegion _mc_region("mc"), _ro_region("ro"), _rw_region("rw"), _symbol_region("symbols");
 static size_t _total_closed_archive_region_size = 0, _total_open_archive_region_size = 0;
 
-void MetaspaceShared::init_shared_dump_space(DumpRegion* first_space, address first_space_bottom) {
-  // Start with 0 committed bytes. The memory will be committed as needed by
-  // MetaspaceShared::commit_shared_space_to().
-  if (!_shared_vs.initialize(_shared_rs, 0)) {
-    fatal("Unable to allocate memory for shared space");
-  }
-  first_space->init(&_shared_rs, (char*)first_space_bottom);
+void MetaspaceShared::init_shared_dump_space(DumpRegion* first_space) {
+  first_space->init(&_shared_rs, &_shared_vs);
 }
 
 DumpRegion* MetaspaceShared::misc_code_dump_space() {
   return &_mc_region;
 }
@@ -211,10 +225,14 @@
 void MetaspaceShared::pack_dump_space(DumpRegion* current, DumpRegion* next,
                                       ReservedSpace* rs) {
   current->pack(next);
 }
 
+char* MetaspaceShared::symbol_space_alloc(size_t num_bytes) {
+  return _symbol_region.allocate(num_bytes);
+}
+
 char* MetaspaceShared::misc_code_space_alloc(size_t num_bytes) {
   return _mc_region.allocate(num_bytes);
 }
 
 char* MetaspaceShared::read_only_space_alloc(size_t num_bytes) {
@@ -320,10 +338,18 @@
 
   init_shared_dump_space(&_mc_region);
   SharedBaseAddress = (size_t)_shared_rs.base();
   log_info(cds)("Allocated shared space: " SIZE_FORMAT " bytes at " PTR_FORMAT,
                 _shared_rs.size(), p2i(_shared_rs.base()));
+
+  size_t symbol_rs_size = LP64_ONLY(3 * G) NOT_LP64(128 * M);
+  _symbol_rs = ReservedSpace(symbol_rs_size);
+  if (!_symbol_rs.is_reserved()) {
+    vm_exit_during_initialization("Unable to reserve memory for symbols",
+                                  err_msg(SIZE_FORMAT " bytes.", symbol_rs_size));
+  }
+  _symbol_region.init(&_symbol_rs, &_symbol_vs);
 }
 
 // Called by universe_post_init()
 void MetaspaceShared::post_initialize(TRAPS) {
   if (UseSharedSpaces) {
@@ -397,37 +423,41 @@
       }
     }
   }
 }
 
-void MetaspaceShared::commit_shared_space_to(char* newtop) {
+void MetaspaceShared::commit_to(ReservedSpace* rs, VirtualSpace* vs, char* newtop) {
   Arguments::assert_is_dumping_archive();
-  char* base = _shared_rs.base();
+  char* base = rs->base();
   size_t need_committed_size = newtop - base;
-  size_t has_committed_size = _shared_vs.committed_size();
+  size_t has_committed_size = vs->committed_size();
   if (need_committed_size < has_committed_size) {
     return;
   }
 
   size_t min_bytes = need_committed_size - has_committed_size;
   size_t preferred_bytes = 1 * M;
-  size_t uncommitted = _shared_vs.reserved_size() - has_committed_size;
+  size_t uncommitted = vs->reserved_size() - has_committed_size;
 
   size_t commit =MAX2(min_bytes, preferred_bytes);
   commit = MIN2(commit, uncommitted);
   assert(commit <= uncommitted, "sanity");
 
-  bool result = _shared_vs.expand_by(commit, false);
-  ArchivePtrMarker::expand_ptr_end((address*)_shared_vs.high());
+  bool result = vs->expand_by(commit, false);
+  if (rs == &_shared_rs) {
+    ArchivePtrMarker::expand_ptr_end((address*)vs->high());
+  }
 
   if (!result) {
     vm_exit_during_initialization(err_msg("Failed to expand shared space to " SIZE_FORMAT " bytes",
                                           need_committed_size));
   }
 
-  log_debug(cds)("Expanding shared spaces by " SIZE_FORMAT_W(7) " bytes [total " SIZE_FORMAT_W(9)  " bytes ending at %p]",
-                 commit, _shared_vs.actual_committed_size(), _shared_vs.high());
+  assert(rs == &_shared_rs || rs == &_symbol_rs, "must be");
+  const char* which = (rs == &_shared_rs) ? "shared" : "symbol";
+  log_debug(cds)("Expanding %s spaces by " SIZE_FORMAT_W(7) " bytes [total " SIZE_FORMAT_W(9)  " bytes ending at %p]",
+                 which, commit, vs->actual_committed_size(), vs->high());
 }
 
 void MetaspaceShared::initialize_ptr_marker(CHeapBitMap* ptrmap) {
   ArchivePtrMarker::initialize(ptrmap, (address*)_shared_vs.low(), (address*)_shared_vs.high());
 }
@@ -506,10 +536,14 @@
 
 // Global object for holding classes that have been loaded.  Since this
 // is run at a safepoint just before exit, this is the entire set of classes.
 static GrowableArray<Klass*>* _global_klass_objects;
 
+static int global_klass_compare(Klass** a, Klass **b) {
+  return a[0]->name()->fast_compare(b[0]->name());
+}
+
 GrowableArray<Klass*>* MetaspaceShared::collected_klasses() {
   return _global_klass_objects;
 }
 
 static void collect_array_classes(Klass* k) {
@@ -1349,11 +1383,18 @@
       log_info(cds)("Relocating external roots ... ");
       ResourceMark rm;
       RefRelocator ext_reloc;
       iterate_roots(&ext_reloc);
     }
-
+    {
+      log_info(cds)("Fixing symbol identity hash ... ");
+      os::init_random(0x12345678);
+      GrowableArray<Symbol*>* symbols = _ssc->get_sorted_symbols();
+      for (int i=0; i<symbols->length(); i++) {
+        symbols->at(i)->update_identity_hash();
+      }
+    }
 #ifdef ASSERT
     {
       log_info(cds)("Verifying external roots ... ");
       ResourceMark rm;
       IsRefInArchiveChecker checker;
@@ -1382,10 +1423,25 @@
     // We cannot use any of the objects in the heap anymore (except for the
     // shared strings) because their headers no longer point to valid Klasses.
   }
 
   static void iterate_roots(MetaspaceClosure* it) {
+    // To ensure deterministic contents in the archive, we just need to ensure that
+    // we iterate the MetsapceObjs in a deterministic order. It doesn't matter where
+    // the MetsapceObjs are located originally, as they are copied sequentially into
+    // the archive during the iteration.
+    //
+    // The only issue here is that the symbol table and the system directories may be
+    // randomly ordered, so we copy the symbols and klasses into two arrays and sort
+    // them deterministically.
+    //
+    // During -Xshare:dump, the order of Symbol creation is strictly determined by
+    // the SharedClassListFile (class loading is done in a single thread and the JIT
+    // is disabled). Also, Symbols are allocated in monotonically increasing addresses
+    // (see Symbol::operator new(size_t, int)). So if we iterate the Symbols by
+    // ascending address order, we ensure that all Symbols are copied into deterministic
+    // locations in the archive.
     GrowableArray<Symbol*>* symbols = _ssc->get_sorted_symbols();
     for (int i=0; i<symbols->length(); i++) {
       it->push(symbols->adr_at(i));
     }
     if (_global_klass_objects != NULL) {
@@ -1543,10 +1599,11 @@
   // that so we don't have to walk the SystemDictionary again.
   SystemDictionaryShared::check_excluded_classes();
   _global_klass_objects = new GrowableArray<Klass*>(1000);
   CollectClassesClosure collect_classes;
   ClassLoaderDataGraph::loaded_classes_do(&collect_classes);
+  _global_klass_objects->sort(global_klass_compare);
 
   print_class_stats();
 
   // Ensure the ConstMethods won't be modified at run-time
   log_info(cds)("Updating ConstMethods ... ");
@@ -1576,12 +1633,14 @@
 
   char* serialized_data = dump_read_only_tables();
   _ro_region.pack();
 
   // The vtable clones contain addresses of the current process.
-  // We don't want to write these addresses into the archive.
+  // We don't want to write these addresses into the archive. Same for i2i buffer.
   MetaspaceShared::zero_cpp_vtable_clones_for_writing();
+  memset(MetaspaceShared::i2i_entry_code_buffers(), 0,
+         MetaspaceShared::i2i_entry_code_buffers_size());
 
   // relocate the data so that it can be mapped to Arguments::default_SharedBaseAddress()
   // without runtime relocation.
   relocate_to_default_base_address(&ptrmap);
 
@@ -1649,21 +1708,21 @@
   const double total_u_perc = percent_of(total_bytes, total_reserved);
 
   _mc_region.print(total_reserved);
   _rw_region.print(total_reserved);
   _ro_region.print(total_reserved);
-  print_bitmap_region_stats(bitmap_reserved, total_reserved);
+  print_bitmap_region_stats(bitmap_used, total_reserved);
   print_heap_region_stats(_closed_archive_heap_regions, "ca", total_reserved);
   print_heap_region_stats(_open_archive_heap_regions, "oa", total_reserved);
 
   log_debug(cds)("total    : " SIZE_FORMAT_W(9) " [100.0%% of total] out of " SIZE_FORMAT_W(9) " bytes [%5.1f%% used]",
                  total_bytes, total_reserved, total_u_perc);
 }
 
 void VM_PopulateDumpSharedSpace::print_bitmap_region_stats(size_t size, size_t total_size) {
-  log_debug(cds)("bm  space: " SIZE_FORMAT_W(9) " [ %4.1f%% of total] out of " SIZE_FORMAT_W(9) " bytes [100.0%% used] at " INTPTR_FORMAT,
-                 size, size/double(total_size)*100.0, size, p2i(NULL));
+  log_debug(cds)("bm  space: " SIZE_FORMAT_W(9) " [ %4.1f%% of total] out of " SIZE_FORMAT_W(9) " bytes [100.0%% used]",
+                 size, size/double(total_size)*100.0, size);
 }
 
 void VM_PopulateDumpSharedSpace::print_heap_region_stats(GrowableArray<MemRegion> *heap_mem,
                                                          const char *name, size_t total_size) {
   int arr_len = heap_mem == NULL ? 0 : heap_mem->length();
diff a/src/hotspot/share/oops/arrayKlass.cpp b/src/hotspot/share/oops/arrayKlass.cpp
--- a/src/hotspot/share/oops/arrayKlass.cpp
+++ b/src/hotspot/share/oops/arrayKlass.cpp
@@ -158,10 +158,19 @@
                                                                 /* do_zero */ true, CHECK_NULL);
   // initialization to NULL not necessary, area already cleared
   return o;
 }
 
+void ArrayKlass::array_klasses_do(void f(Klass* k, TRAPS), TRAPS) {
+  Klass* k = this;
+  // Iterate over this array klass and all higher dimensions
+  while (k != NULL) {
+    f(k, CHECK);
+    k = ArrayKlass::cast(k)->higher_dimension();
+  }
+}
+
 void ArrayKlass::array_klasses_do(void f(Klass* k)) {
   Klass* k = this;
   // Iterate over this array klass and all higher dimensions
   while (k != NULL) {
     f(k);
diff a/src/hotspot/share/oops/arrayKlass.hpp b/src/hotspot/share/oops/arrayKlass.hpp
--- a/src/hotspot/share/oops/arrayKlass.hpp
+++ b/src/hotspot/share/oops/arrayKlass.hpp
@@ -127,10 +127,11 @@
 
   virtual void metaspace_pointers_do(MetaspaceClosure* iter);
 
   // Iterators
   void array_klasses_do(void f(Klass* k));
+  void array_klasses_do(void f(Klass* k, TRAPS), TRAPS);
 
   // Return a handle.
   static void     complete_create_array_klass(ArrayKlass* k, Klass* super_klass, ModuleEntry* module, TRAPS);
 
 
diff a/src/hotspot/share/oops/instanceKlass.cpp b/src/hotspot/share/oops/instanceKlass.cpp
--- a/src/hotspot/share/oops/instanceKlass.cpp
+++ b/src/hotspot/share/oops/instanceKlass.cpp
@@ -544,14 +544,10 @@
 
     assert(NULL == _methods, "underlying memory not zeroed?");
     assert(is_instance_klass(), "is layout incorrect?");
     assert(size_helper() == parser.layout_size(), "incorrect size_helper?");
 
-  if (Arguments::is_dumping_archive()) {
-      SystemDictionaryShared::init_dumptime_info(this);
-    }
-
   // Set biased locking bit for all instances of this class; it will be
   // cleared if revocation occurs too often for this type
   if (UseBiasedLocking && BiasedLocking::enabled()) {
     set_prototype_header(markWord::biased_locking_prototype());
   }
@@ -1019,24 +1015,26 @@
       // Initialize the vtable and interface table after
       // methods have been rewritten since rewrite may
       // fabricate new Method*s.
       // also does loader constraint checking
       //
-      // initialize_vtable and initialize_itable need to be rerun for
-      // a shared class if the class is not loaded by the NULL classloader.
-      ClassLoaderData * loader_data = class_loader_data();
-      if (!(is_shared() &&
-            loader_data->is_the_null_class_loader_data())) {
+      // initialize_vtable and initialize_itable need to be rerun
+      // for a shared class if
+      // 1) the class is loaded by custom class loader or
+      // 2) the class is loaded by built-in class loader but failed to add archived loader constraints
+      bool need_init_table = true;
+      if (is_shared() && SystemDictionaryShared::check_linking_constraints(this, THREAD)) {
+        need_init_table = false;
+      }
+      if (need_init_table) {
         vtable().initialize_vtable(true, CHECK_false);
         itable().initialize_itable(true, CHECK_false);
       }
 #ifdef ASSERT
-      else {
-        vtable().verify(tty, true);
-        // In case itable verification is ever added.
-        // itable().verify(tty, true);
-      }
+      vtable().verify(tty, true);
+      // In case itable verification is ever added.
+      // itable().verify(tty, true);
 #endif
 
       set_init_state(linked);
       if (JvmtiExport::should_post_class_prepare()) {
         Thread *thread = THREAD;
@@ -1510,18 +1508,18 @@
       // Atomic creation of array_klasses
       MutexLocker ma(THREAD, MultiArray_lock);
 
       // Check if update has already taken place
       if (array_klasses() == NULL) {
-        Klass*    k = ObjArrayKlass::allocate_objArray_klass(1, this, CHECK_NULL);
+        ObjArrayKlass* k = ObjArrayKlass::allocate_objArray_klass(class_loader_data(), 1, this, CHECK_NULL);
         // use 'release' to pair with lock-free load
         release_set_array_klasses(k);
       }
     }
   }
   // _this will always be set at this point
-  ObjArrayKlass* oak = (ObjArrayKlass*)array_klasses();
+  ObjArrayKlass* oak = array_klasses();
   if (or_null) {
     return oak->array_klass_or_null(n);
   }
   return oak->array_klass(n, THREAD);
 }
@@ -1758,13 +1756,18 @@
   }
   FREE_C_HEAP_ARRAY(int, fields_sorted);
 }
 
 
+void InstanceKlass::array_klasses_do(void f(Klass* k, TRAPS), TRAPS) {
+  if (array_klasses() != NULL)
+    array_klasses()->array_klasses_do(f, THREAD);
+}
+
 void InstanceKlass::array_klasses_do(void f(Klass* k)) {
   if (array_klasses() != NULL)
-    ArrayKlass::cast(array_klasses())->array_klasses_do(f);
+    array_klasses()->array_klasses_do(f);
 }
 
 #ifdef ASSERT
 static int linear_search(const Array<Method*>* methods,
                          const Symbol* name,
@@ -2650,11 +2653,11 @@
   constants()->restore_unshareable_info(CHECK);
 
   if (array_klasses() != NULL) {
     // Array classes have null protection domain.
     // --> see ArrayKlass::complete_create_array_klass()
-    ArrayKlass::cast(array_klasses())->restore_unshareable_info(ClassLoaderData::the_null_class_loader_data(), Handle(), CHECK);
+    array_klasses()->restore_unshareable_info(ClassLoaderData::the_null_class_loader_data(), Handle(), CHECK);
   }
 
   // Initialize current biased locking state.
   if (UseBiasedLocking && BiasedLocking::enabled() && !is_value()) {
     set_prototype_header(markWord::biased_locking_prototype());
@@ -3876,13 +3879,10 @@
   if (jni_ids() != NULL) {
     jni_ids()->verify(this);
   }
 
   // Verify other fields
-  if (array_klasses() != NULL) {
-    guarantee(array_klasses()->is_klass(), "should be klass");
-  }
   if (constants() != NULL) {
     guarantee(constants()->is_constantPool(), "should be constant pool");
   }
   const Klass* anonymous_host = unsafe_anonymous_host();
   if (anonymous_host != NULL) {
diff a/src/hotspot/share/oops/instanceKlass.hpp b/src/hotspot/share/oops/instanceKlass.hpp
--- a/src/hotspot/share/oops/instanceKlass.hpp
+++ b/src/hotspot/share/oops/instanceKlass.hpp
@@ -195,11 +195,11 @@
   // Annotations for this class
   Annotations*    _annotations;
   // Package this class is defined in
   PackageEntry*   _package_entry;
   // Array classes holding elements of this class.
-  Klass* volatile _array_klasses;
+  ObjArrayKlass* volatile _array_klasses;
   // Constant pool for this class.
   ConstantPool* _constants;
   // The InnerClasses attribute and EnclosingMethod attribute. The
   // _inner_classes is an array of shorts. If the class has InnerClasses
   // attribute, then the _inner_classes array begins with 4-tuples of shorts
@@ -493,14 +493,14 @@
   // Java itable
   int  itable_length() const               { return _itable_len; }
   void set_itable_length(int len)          { _itable_len = len; }
 
   // array klasses
-  Klass* array_klasses() const             { return _array_klasses; }
-  inline Klass* array_klasses_acquire() const; // load with acquire semantics
-  void set_array_klasses(Klass* k)         { _array_klasses = k; }
-  inline void release_set_array_klasses(Klass* k); // store with release semantics
+  ObjArrayKlass* array_klasses() const     { return _array_klasses; }
+  inline ObjArrayKlass* array_klasses_acquire() const; // load with acquire semantics
+  void set_array_klasses(ObjArrayKlass* k) { _array_klasses = k; }
+  inline void release_set_array_klasses(ObjArrayKlass* k); // store with release semantics
 
   // methods
   Array<Method*>* methods() const          { return _methods; }
   void set_methods(Array<Method*>* a)      { _methods = a; }
   Method* method_with_idnum(int idnum);
@@ -1159,10 +1159,11 @@
   void do_nonstatic_fields(FieldClosure* cl); // including inherited fields
   void do_local_static_fields(void f(fieldDescriptor*, Handle, TRAPS), Handle, TRAPS);
 
   void methods_do(void f(Method* method));
   virtual void array_klasses_do(void f(Klass* k));
+  virtual void array_klasses_do(void f(Klass* k, TRAPS), TRAPS);
 
   static InstanceKlass* cast(Klass* k) {
     return const_cast<InstanceKlass*>(cast(const_cast<const Klass*>(k)));
   }
 
diff a/src/hotspot/share/oops/klass.cpp b/src/hotspot/share/oops/klass.cpp
--- a/src/hotspot/share/oops/klass.cpp
+++ b/src/hotspot/share/oops/klass.cpp
@@ -27,10 +27,11 @@
 #include "classfile/classLoaderDataGraph.inline.hpp"
 #include "classfile/dictionary.hpp"
 #include "classfile/javaClasses.hpp"
 #include "classfile/moduleEntry.hpp"
 #include "classfile/systemDictionary.hpp"
+#include "classfile/systemDictionaryShared.hpp"
 #include "classfile/vmSymbols.hpp"
 #include "gc/shared/collectedHeap.inline.hpp"
 #include "logging/log.hpp"
 #include "memory/heapShared.hpp"
 #include "memory/metadataFactory.hpp"
@@ -77,10 +78,14 @@
 }
 
 void Klass::set_name(Symbol* n) {
   _name = n;
   if (_name != NULL) _name->increment_refcount();
+
+  if (Arguments::is_dumping_archive() && is_instance_klass()) {
+    SystemDictionaryShared::init_dumptime_info(InstanceKlass::cast(this));
+  }
 }
 
 bool Klass::is_subclass_of(const Klass* k) const {
   // Run up the super chain and check
   if (this == k) return true;
diff a/src/hotspot/share/oops/klassVtable.cpp b/src/hotspot/share/oops/klassVtable.cpp
--- a/src/hotspot/share/oops/klassVtable.cpp
+++ b/src/hotspot/share/oops/klassVtable.cpp
@@ -1,7 +1,7 @@
 /*
- * Copyright (c) 1997, 2019, Oracle and/or its affiliates. All rights reserved.
+ * Copyright (c) 1997, 2020, Oracle and/or its affiliates. All rights reserved.
  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  *
  * This code is free software; you can redistribute it and/or modify it
  * under the terms of the GNU General Public License version 2 only, as
  * published by the Free Software Foundation.
@@ -489,13 +489,13 @@
           Handle super_loader(THREAD, super_klass->class_loader());
 
           if (target_loader() != super_loader()) {
             ResourceMark rm(THREAD);
             Symbol* failed_type_symbol =
-              SystemDictionary::check_signature_loaders(signature, target_loader,
-                                                        super_loader, true,
-                                                        CHECK_(false));
+              SystemDictionary::check_signature_loaders(signature, _klass,
+                                                        target_loader, super_loader,
+                                                        true, CHECK_(false));
             if (failed_type_symbol != NULL) {
               stringStream ss;
               ss.print("loader constraint violation for class %s: when selecting "
                        "overriding method '", klass->external_name());
               target_method()->print_external_name(&ss),
@@ -1229,10 +1229,11 @@
         InstanceKlass* method_holder = target->method_holder();
         if (method_holder_loader() != interface_loader()) {
           ResourceMark rm(THREAD);
           Symbol* failed_type_symbol =
             SystemDictionary::check_signature_loaders(m->signature(),
+                                                      _klass,
                                                       method_holder_loader,
                                                       interface_loader,
                                                       true, CHECK);
           if (failed_type_symbol != NULL) {
             stringStream ss;
diff a/src/hotspot/share/oops/objArrayKlass.cpp b/src/hotspot/share/oops/objArrayKlass.cpp
--- a/src/hotspot/share/oops/objArrayKlass.cpp
+++ b/src/hotspot/share/oops/objArrayKlass.cpp
@@ -53,11 +53,13 @@
   int size = ArrayKlass::static_size(ObjArrayKlass::header_size());
 
   return new (loader_data, size, THREAD) ObjArrayKlass(n, k, name);
 }
 
-Klass* ObjArrayKlass::allocate_objArray_klass(int n, Klass* element_klass, TRAPS) {
+ObjArrayKlass* ObjArrayKlass::allocate_objArray_klass(ClassLoaderData* loader_data,
+                                                      int n, Klass* element_klass, TRAPS) {
+
   // Eagerly allocate the direct array supertype.
   Klass* super_klass = NULL;
   if (!Universe::is_bootstrapping() || SystemDictionary::Object_klass_loaded()) {
     Klass* element_super = element_klass->super();
     if (element_super != NULL) {
@@ -85,11 +87,11 @@
             elem_super->array_klass(CHECK_NULL);
           }
           // Now retry from the beginning
           ek = element_klass->array_klass(n, CHECK_NULL);
         }  // re-lock
-        return ek;
+        return ObjArrayKlass::cast(ek);
       }
     } else {
       // The element type is already Object.  Object[] has direct super of Object.
       super_klass = SystemDictionary::Object_klass();
     }
@@ -97,11 +99,10 @@
 
   // Create type name for klass.
   Symbol* name = ArrayKlass::create_element_klass_array_name(element_klass, CHECK_NULL);
 
   // Initialize instance variables
-  ClassLoaderData* loader_data = element_klass->class_loader_data();
   ObjArrayKlass* oak = ObjArrayKlass::allocate(loader_data, n, element_klass, name, CHECK_NULL);
 
   ModuleEntry* module = oak->module();
   assert(module != NULL, "No module entry for array");
 
@@ -334,11 +335,11 @@
 
       // Check if another thread beat us
       if (higher_dimension() == NULL) {
 
         // Create multi-dim klass object and link them together
-        Klass* k = ObjArrayKlass::allocate_objArray_klass(dim + 1, this, CHECK_NULL);
+        Klass* k = ObjArrayKlass::allocate_objArray_klass(class_loader_data(), dim + 1, this, CHECK_NULL);
         ObjArrayKlass* ak = ObjArrayKlass::cast(k);
         ak->set_lower_dimension(this);
         // use 'release' to pair with lock-free load
         release_set_higher_dimension(ak);
         assert(ak->is_objArray_klass(), "incorrect initialization of ObjArrayKlass");
diff a/src/hotspot/share/oops/objArrayKlass.hpp b/src/hotspot/share/oops/objArrayKlass.hpp
--- a/src/hotspot/share/oops/objArrayKlass.hpp
+++ b/src/hotspot/share/oops/objArrayKlass.hpp
@@ -63,11 +63,12 @@
                                                   Array<InstanceKlass*>* transitive_interfaces);
   DEBUG_ONLY(bool is_objArray_klass_slow()  const  { return true; })
   int oop_size(oop obj) const;
 
   // Allocation
-  static Klass* allocate_objArray_klass(int n, Klass* element_klass, TRAPS);
+  static ObjArrayKlass* allocate_objArray_klass(ClassLoaderData* loader_data,
+                                                int n, Klass* element_klass, TRAPS);
 
   objArrayOop allocate(int length, TRAPS);
   oop multi_allocate(int rank, jint* sizes, TRAPS);
 
   // Copying
diff a/src/hotspot/share/oops/symbol.cpp b/src/hotspot/share/oops/symbol.cpp
--- a/src/hotspot/share/oops/symbol.cpp
+++ b/src/hotspot/share/oops/symbol.cpp
@@ -28,10 +28,11 @@
 #include "classfile/classLoaderData.hpp"
 #include "gc/shared/collectedHeap.hpp"
 #include "logging/log.hpp"
 #include "logging/logStream.hpp"
 #include "memory/allocation.inline.hpp"
+#include "memory/metaspaceShared.hpp"
 #include "memory/resourceArea.hpp"
 #include "memory/universe.hpp"
 #include "oops/symbol.hpp"
 #include "runtime/atomic.hpp"
 #include "runtime/os.hpp"
@@ -54,10 +55,26 @@
     byte_at_put(i, name[i]);
   }
 }
 
 void* Symbol::operator new(size_t sz, int len) throw() {
+#if INCLUDE_CDS
+ if (DumpSharedSpaces) {
+    // To get deterministic output from -Xshare:dump, we ensure that Symbols are allocated in
+    // increasing addresses. When the symbols are copied into the archive, we preserve their
+    // relative address order (see SortedSymbolClosure in metaspaceShared.cpp)
+    //
+    // We cannot use arena because arena chunks are allocated by the OS. As a result, for example,
+    // the archived symbol of "java/lang/Object" may sometimes be lower than "java/lang/String", and
+    // sometimes be higher. This would cause non-deterministic contents in the archive.
+   DEBUG_ONLY(static void* last = 0);
+   void* p = (void*)MetaspaceShared::symbol_space_alloc(size(len)*wordSize);
+   assert(p > last, "must increase monotonically");
+   DEBUG_ONLY(last = p);
+   return p;
+ }
+#endif
   int alloc_size = size(len)*wordSize;
   address res = (address) AllocateHeap(alloc_size, mtSymbol);
   return res;
 }
 
@@ -70,15 +87,25 @@
 void Symbol::operator delete(void *p) {
   assert(((Symbol*)p)->refcount() == 0, "should not call this");
   FreeHeap(p);
 }
 
+#if INCLUDE_CDS
+void Symbol::update_identity_hash() {
+  // This is called at a safepoint during dumping of a static CDS archive. The caller should have
+  // called os::init_random() with a deterministic seed and then iterate all archived Symbols in
+  // a deterministic order.
+  assert(SafepointSynchronize::is_at_safepoint(), "must be at a safepoint");
+  _hash_and_refcount =  pack_hash_and_refcount((short)os::random(), PERM_REFCOUNT);
+}
+
 void Symbol::set_permanent() {
   // This is called at a safepoint during dumping of a dynamic CDS archive.
   assert(SafepointSynchronize::is_at_safepoint(), "must be at a safepoint");
   _hash_and_refcount =  pack_hash_and_refcount(extract_hash(_hash_and_refcount), PERM_REFCOUNT);
 }
+#endif
 
 bool Symbol::is_Q_signature() const {
   int len = utf8_length();
   return len > 2 && char_at(0) == JVM_SIGNATURE_VALUETYPE && char_at(len - 1) == JVM_SIGNATURE_ENDCLASS;
 }
diff a/src/hotspot/share/oops/symbol.hpp b/src/hotspot/share/oops/symbol.hpp
--- a/src/hotspot/share/oops/symbol.hpp
+++ b/src/hotspot/share/oops/symbol.hpp
@@ -166,11 +166,12 @@
   void increment_refcount();
   void decrement_refcount();
   bool is_permanent() const {
     return (refcount() == PERM_REFCOUNT);
   }
-  void set_permanent();
+  void update_identity_hash() NOT_CDS_RETURN;
+  void set_permanent() NOT_CDS_RETURN;
   void make_permanent();
 
   // Function char_at() returns the Symbol's selected u1 byte as a char type.
   //
   // Note that all multi-byte chars have the sign bit set on all their bytes.
diff a/src/hotspot/share/oops/typeArrayKlass.cpp b/src/hotspot/share/oops/typeArrayKlass.cpp
--- a/src/hotspot/share/oops/typeArrayKlass.cpp
+++ b/src/hotspot/share/oops/typeArrayKlass.cpp
@@ -185,11 +185,12 @@
     {
       // Atomic create higher dimension and link into list
       MutexLocker mu(THREAD, MultiArray_lock);
 
       if (higher_dimension() == NULL) {
-        Klass* oak = ObjArrayKlass::allocate_objArray_klass(dim + 1, this, CHECK_NULL);
+        Klass* oak = ObjArrayKlass::allocate_objArray_klass(
+              class_loader_data(), dim + 1, this, CHECK_NULL);
         ObjArrayKlass* h_ak = ObjArrayKlass::cast(oak);
         h_ak->set_lower_dimension(this);
         // use 'release' to pair with lock-free load
         release_set_higher_dimension(h_ak);
         assert(h_ak->is_objArray_klass(), "incorrect initialization of ObjArrayKlass");
diff a/src/hotspot/share/oops/valueArrayKlass.cpp b/src/hotspot/share/oops/valueArrayKlass.cpp
--- a/src/hotspot/share/oops/valueArrayKlass.cpp
+++ b/src/hotspot/share/oops/valueArrayKlass.cpp
@@ -344,11 +344,11 @@
       // Check if another thread beat us
       if (higher_dimension() == NULL) {
 
         // Create multi-dim klass object and link them together
         Klass* k =
-          ObjArrayKlass::allocate_objArray_klass(dim + 1, this, CHECK_NULL);
+          ObjArrayKlass::allocate_objArray_klass(class_loader_data(), dim + 1, this, CHECK_NULL);
         ObjArrayKlass* ak = ObjArrayKlass::cast(k);
         ak->set_lower_dimension(this);
         OrderAccess::storestore();
         release_set_higher_dimension(ak);
         assert(ak->is_objArray_klass(), "incorrect initialization of ObjArrayKlass");
diff a/src/hotspot/share/oops/valueKlass.cpp b/src/hotspot/share/oops/valueKlass.cpp
--- a/src/hotspot/share/oops/valueKlass.cpp
+++ b/src/hotspot/share/oops/valueKlass.cpp
@@ -254,11 +254,17 @@
 
 Klass* ValueKlass::allocate_value_array_klass(TRAPS) {
   if (flatten_array()) {
     return ValueArrayKlass::allocate_klass(this, THREAD);
   }
-  return ObjArrayKlass::allocate_objArray_klass(1, this, THREAD);
+  return ObjArrayKlass::allocate_objArray_klass(class_loader_data(), 1, this, THREAD);
+}
+
+void ValueKlass::array_klasses_do(void f(Klass* k, TRAPS), TRAPS) {
+  InstanceKlass::array_klasses_do(f, THREAD);
+  if (get_value_array_klass() != NULL)
+    ArrayKlass::cast(get_value_array_klass())->array_klasses_do(f, THREAD);
 }
 
 void ValueKlass::array_klasses_do(void f(Klass* k)) {
   InstanceKlass::array_klasses_do(f);
   if (get_value_array_klass() != NULL)
diff a/src/hotspot/share/oops/valueKlass.hpp b/src/hotspot/share/oops/valueKlass.hpp
--- a/src/hotspot/share/oops/valueKlass.hpp
+++ b/src/hotspot/share/oops/valueKlass.hpp
@@ -193,10 +193,11 @@
     return layout_helper_to_size_helper(layout_helper());
   }
 
   // Metadata iterators
   void array_klasses_do(void f(Klass* k));
+  void array_klasses_do(void f(Klass* k, TRAPS), TRAPS);
 
   // allocate_instance() allocates a stand alone value in the Java heap
   // initialized to default value (cleared memory)
   instanceOop allocate_instance(TRAPS);
   // allocates a stand alone value buffer in the Java heap
diff a/src/hotspot/share/opto/compile.cpp b/src/hotspot/share/opto/compile.cpp
--- a/src/hotspot/share/opto/compile.cpp
+++ b/src/hotspot/share/opto/compile.cpp
@@ -2880,11 +2880,13 @@
 }
 
 bool Compile::compute_logic_cone(Node* n, Unique_Node_List& partition, Unique_Node_List& inputs) {
   assert(partition.size() == 0, "not empty");
   assert(inputs.size() == 0, "not empty");
-  assert(!is_vector_ternary_bitwise_op(n), "not supported");
+  if (is_vector_ternary_bitwise_op(n)) {
+    return false;
+  }
 
   bool is_unary_op = is_vector_unary_bitwise_op(n);
   if (is_unary_op) {
     assert(collect_unique_inputs(n, partition, inputs) == 1, "not unary");
     return false; // too few inputs
@@ -2922,10 +2924,11 @@
 
   return (partition.size() == 2 || partition.size() == 3) &&
          (inputs.size()    == 2 || inputs.size()    == 3);
 }
 
+
 void Compile::process_logic_cone_root(PhaseIterGVN &igvn, Node *n, VectorSet &visited) {
   assert(is_vector_bitwise_op(n), "not a root");
 
   visited.set(n->_idx);
 
diff a/src/hotspot/share/opto/node.cpp b/src/hotspot/share/opto/node.cpp
--- a/src/hotspot/share/opto/node.cpp
+++ b/src/hotspot/share/opto/node.cpp
@@ -1601,10 +1601,15 @@
 // call this from debugger:
 Node* find_node(Node* n, int idx) {
   return n->find(idx);
 }
 
+// call this from debugger with root node as default:
+Node* find_node(int idx) {
+  return Compile::current()->root()->find(idx);
+}
+
 //------------------------------find-------------------------------------------
 Node* Node::find(int idx) const {
   ResourceArea *area = Thread::current()->resource_area();
   VectorSet old_space(area), new_space(area);
   Node* result = NULL;
diff a/src/hotspot/share/opto/subtypenode.cpp b/src/hotspot/share/opto/subtypenode.cpp
--- a/src/hotspot/share/opto/subtypenode.cpp
+++ b/src/hotspot/share/opto/subtypenode.cpp
@@ -138,20 +138,34 @@
 
   if (addr != NULL) {
     intptr_t con = 0;
     Node* obj = AddPNode::Ideal_base_and_offset(addr, phase, con);
     if (con == oopDesc::klass_offset_in_bytes() && obj != NULL) {
-      assert(phase->type(obj)->isa_oopptr(), "only for oop input");
+#ifdef ASSERT
+      const Type* obj_t = phase->type(obj);
+      if (!obj_t->isa_oopptr() && obj_t != Type::TOP) {
+        obj->dump();
+        obj_t->dump(); tty->cr();
+        fatal("only for oop input");
+      }
+#endif
       set_req(ObjOrSubKlass, obj);
       return this;
     }
   }
 
   // AllocateNode might have more accurate klass input
   Node* allocated_klass = AllocateNode::Ideal_klass(obj_or_subklass, phase);
   if (allocated_klass != NULL) {
-    assert(phase->type(obj_or_subklass)->isa_oopptr(), "only for oop input");
+#ifdef ASSERT
+      const Type* obj_or_subklass_t = phase->type(obj_or_subklass);
+      if (!obj_or_subklass_t->isa_oopptr() && obj_or_subklass_t != Type::TOP) {
+        obj_or_subklass->dump();
+        obj_or_subklass_t->dump(); tty->cr();
+        fatal("only for oop input");
+      }
+#endif
     set_req(ObjOrSubKlass, allocated_klass);
     return this;
   }
 
   // Verify that optimizing the subtype check to a simple code pattern
diff a/src/hotspot/share/prims/jvm.cpp b/src/hotspot/share/prims/jvm.cpp
--- a/src/hotspot/share/prims/jvm.cpp
+++ b/src/hotspot/share/prims/jvm.cpp
@@ -3787,10 +3787,33 @@
   Klass* k = java_lang_Class::as_Klass(JNIHandles::resolve(cls));
   assert(k->is_klass(), "just checking");
   HeapShared::initialize_from_archived_subgraph(k);
 JVM_END
 
+JVM_ENTRY_NO_ENV(jlong, JVM_GetRandomSeedForCDSDump())
+  JVMWrapper("JVM_GetRandomSeedForCDSDump");
+  if (DumpSharedSpaces) {
+    const char* release = Abstract_VM_Version::vm_release();
+    const char* dbg_level = Abstract_VM_Version::jdk_debug_level();
+    const char* version = VM_Version::internal_vm_info_string();
+    jlong seed = (jlong)(java_lang_String::hash_code((const jbyte*)release, (int)strlen(release)) ^
+                         java_lang_String::hash_code((const jbyte*)dbg_level, (int)strlen(dbg_level)) ^
+                         java_lang_String::hash_code((const jbyte*)version, (int)strlen(version)));
+    seed += (jlong)Abstract_VM_Version::vm_major_version();
+    seed += (jlong)Abstract_VM_Version::vm_minor_version();
+    seed += (jlong)Abstract_VM_Version::vm_security_version();
+    seed += (jlong)Abstract_VM_Version::vm_patch_version();
+    if (seed == 0) { // don't let this ever be zero.
+      seed = 0x87654321;
+    }
+    log_debug(cds)("JVM_GetRandomSeedForCDSDump() = " JLONG_FORMAT, seed);
+    return seed;
+  } else {
+    return 0;
+  }
+JVM_END
+
 // Returns an array of all live Thread objects (VM internal JavaThreads,
 // jvmti agent threads, and JNI attaching threads  are skipped)
 // See CR 6404306 regarding JNI attaching threads
 JVM_ENTRY(jobjectArray, JVM_GetAllThreads(JNIEnv *env, jclass dummy))
   ResourceMark rm(THREAD);
diff a/src/hotspot/share/runtime/arguments.cpp b/src/hotspot/share/runtime/arguments.cpp
--- a/src/hotspot/share/runtime/arguments.cpp
+++ b/src/hotspot/share/runtime/arguments.cpp
@@ -520,13 +520,13 @@
   { "MinRAMFraction",               JDK_Version::jdk(10),  JDK_Version::undefined(), JDK_Version::undefined() },
   { "InitialRAMFraction",           JDK_Version::jdk(10),  JDK_Version::undefined(), JDK_Version::undefined() },
   { "UseMembar",                    JDK_Version::jdk(10), JDK_Version::jdk(12), JDK_Version::undefined() },
   { "AllowRedefinitionToAddDeleteMethods", JDK_Version::jdk(13), JDK_Version::undefined(), JDK_Version::undefined() },
   { "FlightRecorder",               JDK_Version::jdk(13), JDK_Version::undefined(), JDK_Version::undefined() },
-  { "MonitorBound",                 JDK_Version::jdk(14), JDK_Version::jdk(15), JDK_Version::jdk(16) },
   { "PrintVMQWaitTime",             JDK_Version::jdk(15), JDK_Version::jdk(16), JDK_Version::jdk(17) },
   { "UseNewFieldLayout",            JDK_Version::jdk(15), JDK_Version::jdk(16), JDK_Version::jdk(17) },
+  { "ForceNUMA",                    JDK_Version::jdk(15), JDK_Version::jdk(16), JDK_Version::jdk(17) },
 
   // --- Deprecated alias flags (see also aliased_jvm_flags) - sorted by obsolete_in then expired_in:
   { "DefaultMaxRAMFraction",        JDK_Version::jdk(8),  JDK_Version::undefined(), JDK_Version::undefined() },
   { "CreateMinidumpOnCrash",        JDK_Version::jdk(9),  JDK_Version::undefined(), JDK_Version::undefined() },
   { "TLABStats",                    JDK_Version::jdk(12), JDK_Version::undefined(), JDK_Version::undefined() },
@@ -547,10 +547,11 @@
   { "FieldsAllocationStyle",         JDK_Version::jdk(14),     JDK_Version::jdk(15), JDK_Version::jdk(16) },
 #ifndef X86
   { "UseSSE",                        JDK_Version::undefined(), JDK_Version::jdk(15), JDK_Version::jdk(16) },
 #endif // !X86
   { "UseAdaptiveGCBoundary",         JDK_Version::undefined(), JDK_Version::jdk(15), JDK_Version::jdk(16) },
+  { "MonitorBound",                  JDK_Version::jdk(14),     JDK_Version::jdk(15), JDK_Version::jdk(16) },
 
 #ifdef TEST_VERIFY_SPECIAL_JVM_FLAGS
   // These entries will generate build errors.  Their purpose is to test the macros.
   { "dep > obs",                    JDK_Version::jdk(9), JDK_Version::jdk(8), JDK_Version::undefined() },
   { "dep > exp ",                   JDK_Version::jdk(9), JDK_Version::undefined(), JDK_Version::jdk(8) },
@@ -4176,18 +4177,10 @@
     if (UseParallelGC) {
       if (FLAG_IS_DEFAULT(MinHeapDeltaBytes)) {
          FLAG_SET_DEFAULT(MinHeapDeltaBytes, 64*M);
       }
     }
-    // UseNUMAInterleaving is set to ON for all collectors and platforms when
-    // UseNUMA is set to ON. NUMA-aware collectors will interleave old gen and
-    // survivor spaces on top of NUMA allocation policy for the eden space.
-    // Non NUMA-aware collectors will interleave all of the heap spaces across
-    // NUMA nodes.
-    if (FLAG_IS_DEFAULT(UseNUMAInterleaving)) {
-      FLAG_SET_ERGO(UseNUMAInterleaving, true);
-    }
   }
   return JNI_OK;
 }
 
 int Arguments::PropertyList_count(SystemProperty* pl) {
diff a/src/hotspot/share/runtime/globals.hpp b/src/hotspot/share/runtime/globals.hpp
--- a/src/hotspot/share/runtime/globals.hpp
+++ b/src/hotspot/share/runtime/globals.hpp
@@ -190,11 +190,11 @@
   product(size_t, NUMAInterleaveGranularity, 2*M,                           \
           "Granularity to use for NUMA interleaving on Windows OS")         \
           range(os::vm_allocation_granularity(), NOT_LP64(2*G) LP64_ONLY(8192*G)) \
                                                                             \
   product(bool, ForceNUMA, false,                                           \
-          "Force NUMA optimizations on single-node/UMA systems")            \
+          "(Deprecated) Force NUMA optimizations on single-node/UMA systems") \
                                                                             \
   product(uintx, NUMAChunkResizeWeight, 20,                                 \
           "Percentage (0-100) used to weight the current sample when "      \
           "computing exponentially decaying average for "                   \
           "AdaptiveNUMAChunkSizing")                                        \
@@ -689,13 +689,10 @@
                                                                             \
   product(bool, UseLWPSynchronization, true,                                \
           "Use LWP-based instead of libthread-based synchronization "       \
           "(SPARC only)")                                                   \
                                                                             \
-  product(intx, MonitorBound, 0, "(Deprecated) Bound Monitor population")   \
-          range(0, max_jint)                                                \
-                                                                            \
   experimental(intx, MonitorUsedDeflationThreshold, 90,                     \
                 "Percentage of used monitors before triggering cleanup "    \
                 "safepoint which deflates monitors (0 is off). "            \
                 "The check is performed on GuaranteedSafepointInterval.")   \
                 range(0, 100)                                               \
diff a/src/hotspot/share/runtime/handles.hpp b/src/hotspot/share/runtime/handles.hpp
--- a/src/hotspot/share/runtime/handles.hpp
+++ b/src/hotspot/share/runtime/handles.hpp
@@ -1,7 +1,7 @@
 /*
- * Copyright (c) 1997, 2019, Oracle and/or its affiliates. All rights reserved.
+ * Copyright (c) 1997, 2020, Oracle and/or its affiliates. All rights reserved.
  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  *
  * This code is free software; you can redistribute it and/or modify it
  * under the terms of the GNU General Public License version 2 only, as
  * published by the Free Software Foundation.
@@ -79,11 +79,13 @@
   // General access
   oop     operator () () const                   { return obj(); }
   oop     operator -> () const                   { return non_null_obj(); }
 
   bool operator == (oop o) const                 { return obj() == o; }
+  bool operator != (oop o) const                 { return obj() != o; }
   bool operator == (const Handle& h) const       { return obj() == h.obj(); }
+  bool operator != (const Handle& h) const       { return obj() != h.obj(); }
 
   // Null checks
   bool    is_null() const                        { return _handle == NULL; }
   bool    not_null() const                       { return _handle != NULL; }
 
diff a/src/hotspot/share/runtime/safepoint.cpp b/src/hotspot/share/runtime/safepoint.cpp
--- a/src/hotspot/share/runtime/safepoint.cpp
+++ b/src/hotspot/share/runtime/safepoint.cpp
@@ -498,14 +498,10 @@
   if (StringTable::needs_rehashing()) return true;
   if (SymbolTable::needs_rehashing()) return true;
   return false;
 }
 
-bool SafepointSynchronize::is_forced_cleanup_needed() {
-  return ObjectSynchronizer::needs_monitor_scavenge();
-}
-
 class ParallelSPCleanupThreadClosure : public ThreadClosure {
 private:
   CodeBlobClosure* _nmethod_cl;
   DeflateMonitorCounters* _counters;
 
diff a/src/hotspot/share/runtime/synchronizer.cpp b/src/hotspot/share/runtime/synchronizer.cpp
--- a/src/hotspot/share/runtime/synchronizer.cpp
+++ b/src/hotspot/share/runtime/synchronizer.cpp
@@ -807,11 +807,10 @@
   volatile int hc_sequence;
   DEFINE_PAD_MINUS_SIZE(2, OM_CACHE_LINE_SIZE, sizeof(volatile int));
 };
 
 static SharedGlobals GVars;
-static int _forceMonitorScavenge = 0; // Scavenge required and pending
 
 static markWord read_stable_mark(oop obj) {
   markWord mark = obj->mark();
   if (!mark.is_being_inflated()) {
     return mark;       // normal fast-path return
@@ -1201,30 +1200,11 @@
     return monitor_usage > MonitorUsedDeflationThreshold;
   }
   return false;
 }
 
-// Returns true if MonitorBound is set (> 0) and if the specified
-// cnt is > MonitorBound. Otherwise returns false.
-static bool is_MonitorBound_exceeded(const int cnt) {
-  const int mx = MonitorBound;
-  return mx > 0 && cnt > mx;
-}
-
-bool ObjectSynchronizer::is_cleanup_needed() {
-  if (monitors_used_above_threshold()) {
-    // Too many monitors in use.
-    return true;
-  }
-  return needs_monitor_scavenge();
-}
-
-bool ObjectSynchronizer::needs_monitor_scavenge() {
-  if (Atomic::load(&_forceMonitorScavenge) == 1) {
-    log_info(monitorinflation)("Monitor scavenge needed, triggering safepoint cleanup.");
-    return true;
-  }
+bool ObjectSynchronizer::is_cleanup_needed() {
   return false;
 }
 
 void ObjectSynchronizer::oops_do(OopClosure* f) {
   // We only scan the global used list here (for moribund threads), and
@@ -1268,45 +1248,10 @@
 // --   unassigned and on the om_list_globals._free_list
 // --   unassigned and on a per-thread free list
 // --   assigned to an object.  The object is inflated and the mark refers
 //      to the ObjectMonitor.
 
-
-// Constraining monitor pool growth via MonitorBound ...
-//
-// If MonitorBound is not set (<= 0), MonitorBound checks are disabled.
-//
-// The monitor pool is grow-only.  We scavenge at STW safepoint-time, but the
-// the rate of scavenging is driven primarily by GC.  As such,  we can find
-// an inordinate number of monitors in circulation.
-// To avoid that scenario we can artificially induce a STW safepoint
-// if the pool appears to be growing past some reasonable bound.
-// Generally we favor time in space-time tradeoffs, but as there's no
-// natural back-pressure on the # of extant monitors we need to impose some
-// type of limit.  Beware that if MonitorBound is set to too low a value
-// we could just loop. In addition, if MonitorBound is set to a low value
-// we'll incur more safepoints, which are harmful to performance.
-// See also: GuaranteedSafepointInterval
-//
-// If MonitorBound is set, the boundry applies to
-//     (om_list_globals._population - om_list_globals._free_count)
-// i.e., if there are not enough ObjectMonitors on the global free list,
-// then a safepoint deflation is induced. Picking a good MonitorBound value
-// is non-trivial.
-
-static void InduceScavenge(Thread* self, const char * Whence) {
-  // Induce STW safepoint to trim monitors
-  // Ultimately, this results in a call to deflate_idle_monitors() in the near future.
-  // More precisely, trigger a cleanup safepoint as the number
-  // of active monitors passes the specified threshold.
-  // TODO: assert thread state is reasonable
-
-  if (Atomic::xchg(&_forceMonitorScavenge, 1) == 0) {
-    VMThread::check_for_forced_cleanup();
-  }
-}
-
 ObjectMonitor* ObjectSynchronizer::om_alloc(Thread* self) {
   // A large MAXPRIVATE value reduces both list lock contention
   // and list coherency traffic, but also tends to increase the
   // number of ObjectMonitors in circulation as well as the STW
   // scavenge costs.  As usual, we lean toward time in space-time
@@ -1346,19 +1291,10 @@
         take->Recycle();
         om_release(self, take, false);
       }
       self->om_free_provision += 1 + (self->om_free_provision / 2);
       if (self->om_free_provision > MAXPRIVATE) self->om_free_provision = MAXPRIVATE;
-
-      if (is_MonitorBound_exceeded(Atomic::load(&om_list_globals._population) -
-                                   Atomic::load(&om_list_globals._free_count))) {
-        // Not enough ObjectMonitors on the global free list.
-        // We can't safely induce a STW safepoint from om_alloc() as our thread
-        // state may not be appropriate for such activities and callers may hold
-        // naked oops, so instead we defer the action.
-        InduceScavenge(self, "om_alloc");
-      }
       continue;
     }
 
     // 3: allocate a block of new ObjectMonitors
     // Both the local and global free lists are empty -- resort to malloc().
@@ -2060,12 +1996,10 @@
                                Atomic::load(&om_list_globals._population),
                                Atomic::load(&om_list_globals._in_use_count),
                                Atomic::load(&om_list_globals._free_count));
   }
 
-  Atomic::store(&_forceMonitorScavenge, 0);    // Reset
-
   OM_PERFDATA_OP(Deflations, inc(counters->n_scavenged));
   OM_PERFDATA_OP(MonExtant, set_value(counters->n_in_circulation));
 
   GVars.stw_random = os::random();
   GVars.stw_cycle++;
diff a/src/hotspot/share/runtime/synchronizer.hpp b/src/hotspot/share/runtime/synchronizer.hpp
--- a/src/hotspot/share/runtime/synchronizer.hpp
+++ b/src/hotspot/share/runtime/synchronizer.hpp
@@ -143,11 +143,10 @@
                                   ObjectMonitor** free_tail_p);
   static bool deflate_monitor(ObjectMonitor* mid, oop obj,
                               ObjectMonitor** free_head_p,
                               ObjectMonitor** free_tail_p);
   static bool is_cleanup_needed();
-  static bool needs_monitor_scavenge();
   static void oops_do(OopClosure* f);
   // Process oops in thread local used monitors
   static void thread_local_used_oops_do(Thread* thread, OopClosure* f);
 
   // debugging
diff a/src/hotspot/share/runtime/vmStructs.cpp b/src/hotspot/share/runtime/vmStructs.cpp
--- a/src/hotspot/share/runtime/vmStructs.cpp
+++ b/src/hotspot/share/runtime/vmStructs.cpp
@@ -216,11 +216,11 @@
   nonstatic_field(ConstantPool,                _source_file_name_index,                       u2)                                    \
   nonstatic_field(ConstantPoolCache,           _resolved_references,                          OopHandle)                             \
   nonstatic_field(ConstantPoolCache,           _reference_map,                                Array<u2>*)                            \
   nonstatic_field(ConstantPoolCache,           _length,                                       int)                                   \
   nonstatic_field(ConstantPoolCache,           _constant_pool,                                ConstantPool*)                         \
-  volatile_nonstatic_field(InstanceKlass,      _array_klasses,                                Klass*)                                \
+  volatile_nonstatic_field(InstanceKlass,      _array_klasses,                                ObjArrayKlass*)                        \
   nonstatic_field(InstanceKlass,               _methods,                                      Array<Method*>*)                       \
   nonstatic_field(InstanceKlass,               _default_methods,                              Array<Method*>*)                       \
   nonstatic_field(InstanceKlass,               _local_interfaces,                             Array<InstanceKlass*>*)                \
   nonstatic_field(InstanceKlass,               _transitive_interfaces,                        Array<InstanceKlass*>*)                \
   nonstatic_field(InstanceKlass,               _fields,                                       Array<u2>*)                            \
diff a/src/java.base/share/classes/java/lang/invoke/DirectMethodHandle.java b/src/java.base/share/classes/java/lang/invoke/DirectMethodHandle.java
--- a/src/java.base/share/classes/java/lang/invoke/DirectMethodHandle.java
+++ b/src/java.base/share/classes/java/lang/invoke/DirectMethodHandle.java
@@ -49,13 +49,14 @@
  * to a class member.
  * @author jrose
  */
 class DirectMethodHandle extends MethodHandle {
     final MemberName member;
+    final boolean crackable;
 
     // Constructors and factory methods in this class *must* be package scoped or private.
-    private DirectMethodHandle(MethodType mtype, LambdaForm form, MemberName member) {
+    private DirectMethodHandle(MethodType mtype, LambdaForm form, MemberName member, boolean crackable) {
         super(mtype, form);
         if (!member.isResolved())  throw new InternalError();
 
         if (member.getDeclaringClass().isInterface() &&
             member.getReferenceKind() == REF_invokeInterface &&
@@ -68,10 +69,11 @@
                 member = m;
             }
         }
 
         this.member = member;
+        this.crackable = crackable;
     }
 
     // Factory methods:
     static DirectMethodHandle make(byte refKind, Class<?> refc, MemberName member, Class<?> callerClass) {
         MethodType mtype = member.getMethodOrFieldType();
@@ -90,22 +92,22 @@
                     // receiver check inserted
                     if (callerClass == null) {
                         throw new InternalError("callerClass must not be null for REF_invokeSpecial");
                     }
                     LambdaForm lform = preparedLambdaForm(member, callerClass.isInterface());
-                    return new Special(mtype, lform, member, callerClass);
+                    return new Special(mtype, lform, member, true, callerClass);
                 }
                 case REF_invokeInterface: {
                     // for interfaces we always need the receiver typecheck,
                     // so we always pass 'true' to ensure we adapt if needed
                     // to include the REF_invokeSpecial case
                     LambdaForm lform = preparedLambdaForm(member, true);
-                    return new Interface(mtype, lform, member, refc);
+                    return new Interface(mtype, lform, member, true, refc);
                 }
                 default: {
                     LambdaForm lform = preparedLambdaForm(member);
-                    return new DirectMethodHandle(mtype, lform, member);
+                    return new DirectMethodHandle(mtype, lform, member, true);
                 }
             }
         } else {
             LambdaForm lform = preparedFieldLambdaForm(member);
             if (member.isStatic()) {
@@ -140,22 +142,37 @@
         assert(ctor.getReferenceKind() == REF_newInvokeSpecial) : ctor;
         MethodType mtype = ctor.getMethodType().changeReturnType(instanceClass);
         LambdaForm lform = preparedLambdaForm(ctor);
         MemberName init = ctor.asSpecial();
         assert(init.getMethodType().returnType() == void.class);
-        return new Constructor(mtype, lform, ctor, init, instanceClass);
+        return new Constructor(mtype, lform, ctor, true, init, instanceClass);
     }
 
     @Override
     BoundMethodHandle rebind() {
         return BoundMethodHandle.makeReinvoker(this);
     }
 
     @Override
     MethodHandle copyWith(MethodType mt, LambdaForm lf) {
         assert(this.getClass() == DirectMethodHandle.class);  // must override in subclasses
-        return new DirectMethodHandle(mt, lf, member);
+        return new DirectMethodHandle(mt, lf, member, crackable);
+    }
+
+    @Override
+    MethodHandle viewAsType(MethodType newType, boolean strict) {
+        // No actual conversions, just a new view of the same method.
+        // However, we must not expose a DMH that is crackable into a
+        // MethodHandleInfo, so we return a cloned, uncrackable DMH
+        assert(viewAsTypeChecks(newType, strict));
+        assert(this.getClass() == DirectMethodHandle.class);  // must override in subclasses
+        return new DirectMethodHandle(newType, form, member, false);
+    }
+
+    @Override
+    boolean isCrackable() {
+        return crackable;
     }
 
     @Override
     String internalProperties() {
         return "\n& DMH.MN="+internalMemberName();
@@ -407,21 +424,26 @@
     }
 
     /** This subclass represents invokespecial instructions. */
     static class Special extends DirectMethodHandle {
         private final Class<?> caller;
-        private Special(MethodType mtype, LambdaForm form, MemberName member, Class<?> caller) {
-            super(mtype, form, member);
+        private Special(MethodType mtype, LambdaForm form, MemberName member, boolean crackable, Class<?> caller) {
+            super(mtype, form, member, crackable);
             this.caller = caller;
         }
         @Override
         boolean isInvokeSpecial() {
             return true;
         }
         @Override
         MethodHandle copyWith(MethodType mt, LambdaForm lf) {
-            return new Special(mt, lf, member, caller);
+            return new Special(mt, lf, member, crackable, caller);
+        }
+        @Override
+        MethodHandle viewAsType(MethodType newType, boolean strict) {
+            assert(viewAsTypeChecks(newType, strict));
+            return new Special(newType, form, member, false, caller);
         }
         Object checkReceiver(Object recv) {
             if (!caller.isInstance(recv)) {
                 String msg = String.format("Receiver class %s is not a subclass of caller class %s",
                                            recv.getClass().getName(), caller.getName());
@@ -432,18 +454,23 @@
     }
 
     /** This subclass represents invokeinterface instructions. */
     static class Interface extends DirectMethodHandle {
         private final Class<?> refc;
-        private Interface(MethodType mtype, LambdaForm form, MemberName member, Class<?> refc) {
-            super(mtype, form, member);
-            assert refc.isInterface() : refc;
+        private Interface(MethodType mtype, LambdaForm form, MemberName member, boolean crackable, Class<?> refc) {
+            super(mtype, form, member, crackable);
+            assert(refc.isInterface()) : refc;
             this.refc = refc;
         }
         @Override
         MethodHandle copyWith(MethodType mt, LambdaForm lf) {
-            return new Interface(mt, lf, member, refc);
+            return new Interface(mt, lf, member, crackable, refc);
+        }
+        @Override
+        MethodHandle viewAsType(MethodType newType, boolean strict) {
+            assert(viewAsTypeChecks(newType, strict));
+            return new Interface(newType, form, member, false, refc);
         }
         @Override
         Object checkReceiver(Object recv) {
             if (!refc.isInstance(recv)) {
                 String msg = String.format("Receiver class %s does not implement the requested interface %s",
@@ -457,26 +484,30 @@
     /** Used for interface receiver type checks, by Interface and Special modes. */
     Object checkReceiver(Object recv) {
         throw new InternalError("Should only be invoked on a subclass");
     }
 
-
     /** This subclass handles constructor references. */
     static class Constructor extends DirectMethodHandle {
         final MemberName initMethod;
         final Class<?>   instanceClass;
 
         private Constructor(MethodType mtype, LambdaForm form, MemberName constructor,
-                            MemberName initMethod, Class<?> instanceClass) {
-            super(mtype, form, constructor);
+                            boolean crackable, MemberName initMethod, Class<?> instanceClass) {
+            super(mtype, form, constructor, crackable);
             this.initMethod = initMethod;
             this.instanceClass = instanceClass;
             assert(initMethod.isResolved());
         }
         @Override
         MethodHandle copyWith(MethodType mt, LambdaForm lf) {
-            return new Constructor(mt, lf, member, initMethod, instanceClass);
+            return new Constructor(mt, lf, member, crackable, initMethod, instanceClass);
+        }
+        @Override
+        MethodHandle viewAsType(MethodType newType, boolean strict) {
+            assert(viewAsTypeChecks(newType, strict));
+            return new Constructor(newType, form, member, false, initMethod, instanceClass);
         }
     }
 
     /*non-public*/
     static Object constructorMethod(Object mh) {
@@ -491,43 +522,47 @@
     }
 
     /** This subclass handles non-static field references. */
     static abstract class Accessor extends DirectMethodHandle {
         final Class<?> fieldType;
-        final int fieldOffset;
-
-        protected Accessor(MethodType mtype, LambdaForm form, MemberName member,
-                           int fieldOffset) {
-            super(mtype, form, member);
-            this.fieldType = member.getFieldType();
+        final int      fieldOffset;
+        private Accessor(MethodType mtype, LambdaForm form, MemberName member,
+                         boolean crackable, int fieldOffset) {
+            super(mtype, form, member, crackable);
+            this.fieldType   = member.getFieldType();
             this.fieldOffset = fieldOffset;
         }
         abstract Object checkCast(Object obj);
         abstract MethodHandle copyWith(MethodType mt, LambdaForm lf);
     }
 
     /** This subclass handles non-static field references of indirect type */
     static class IndirectAccessor extends Accessor {
         private IndirectAccessor(MethodType mtype, LambdaForm form, MemberName member,
                                  int fieldOffset) {
-            super(mtype, form, member, fieldOffset);
+            super(mtype, form, member, true, fieldOffset);
         }
 
         @Override Object checkCast(Object obj) {
             return fieldType.cast(obj);
         }
         @Override
         MethodHandle copyWith(MethodType mt, LambdaForm lf) {
             return new IndirectAccessor(mt, lf, member, fieldOffset);
         }
+        @Override
+        MethodHandle viewAsType(MethodType newType, boolean strict) {
+            assert(viewAsTypeChecks(newType, strict));
+            return new IndirectAccessor(newType, form, member, fieldOffset);
+        }
     }
 
     /** This subclass handles non-static field references of inline type */
     static class InlineAccessor extends Accessor {
         private InlineAccessor(MethodType mtype, LambdaForm form, MemberName member,
                                int fieldOffset) {
-            super(mtype, form, member, fieldOffset);
+            super(mtype, form, member, true, fieldOffset);
         }
 
         @Override Object checkCast(Object obj) {
             return fieldType.cast(Objects.requireNonNull(obj));
         }
@@ -561,43 +596,48 @@
     static abstract class StaticAccessor extends DirectMethodHandle {
         final Class<?> fieldType;
         final Object staticBase;
         final long staticOffset;
 
-        protected StaticAccessor(MethodType mtype, LambdaForm form, MemberName member,
-                                 Object staticBase, long staticOffset) {
-            super(mtype, form, member);
-            this.fieldType = member.getFieldType();
-            this.staticBase = staticBase;
+        private StaticAccessor(MethodType mtype, LambdaForm form, MemberName member,
+                               boolean crackable, Object staticBase, long staticOffset) {
+            super(mtype, form, member, crackable);
+            this.fieldType    = member.getFieldType();
+            this.staticBase   = staticBase;
             this.staticOffset = staticOffset;
         }
         abstract Object checkCast(Object obj);
         abstract MethodHandle copyWith(MethodType mt, LambdaForm lf);
     }
 
     /** This subclass handles static field references of indirect type. */
     static class IndirectStaticAccessor extends StaticAccessor {
         private IndirectStaticAccessor(MethodType mtype, LambdaForm form, MemberName member,
                                      Object staticBase, long staticOffset) {
-            super(mtype, form, member, staticBase, staticOffset);
+            super(mtype, form, member, true, staticBase, staticOffset);
         }
 
         // indirect type is always nullable
         @Override Object checkCast(Object obj) {
             return fieldType.cast(obj);
         }
         @Override
         MethodHandle copyWith(MethodType mt, LambdaForm lf) {
             return new IndirectStaticAccessor(mt, lf, member, staticBase, staticOffset);
         }
+        @Override
+        MethodHandle viewAsType(MethodType newType, boolean strict) {
+            assert(viewAsTypeChecks(newType, strict));
+            return new IndirectStaticAccessor(newType, form, member, staticBase, staticOffset);
+        }
     }
 
     /** This subclass handles static field references of inline type . */
     static class InlineStaticAccessor extends StaticAccessor {
         private InlineStaticAccessor(MethodType mtype, LambdaForm form, MemberName member,
                                      Object staticBase, long staticOffset) {
-            super(mtype, form, member, staticBase, staticOffset);
+            super(mtype, form, member, true, staticBase, staticOffset);
         }
 
         // zero-default inline type is not-nullable
         @Override Object checkCast(Object obj) {
             assert fieldType.isInlineClass() : "null-default inline type not yet supported";
diff a/src/java.base/share/classes/java/lang/invoke/MethodHandles.java b/src/java.base/share/classes/java/lang/invoke/MethodHandles.java
--- a/src/java.base/share/classes/java/lang/invoke/MethodHandles.java
+++ b/src/java.base/share/classes/java/lang/invoke/MethodHandles.java
@@ -3304,15 +3304,14 @@
          * @throws    NullPointerException if the target is {@code null}
          * @see MethodHandleInfo
          * @since 1.8
          */
         public MethodHandleInfo revealDirect(MethodHandle target) {
-            MemberName member = target.internalMemberName();
-            if (member == null || (!member.isResolved() &&
-                                   !member.isMethodHandleInvoke() &&
-                                   !member.isVarHandleMethodInvoke()))
+            if (!target.isCrackable()) {
                 throw newIllegalArgumentException("not a direct method handle");
+            }
+            MemberName member = target.internalMemberName();
             Class<?> defc = member.getDeclaringClass();
             byte refKind = member.getReferenceKind();
             assert(MethodHandleNatives.refKindIsValid(refKind));
             if (refKind == REF_invokeSpecial && !target.isInvokeSpecial())
                 // Devirtualized method invocation is usually formally virtual.
diff a/src/java.base/share/classes/java/util/Objects.java b/src/java.base/share/classes/java/util/Objects.java
--- a/src/java.base/share/classes/java/util/Objects.java
+++ b/src/java.base/share/classes/java/util/Objects.java
@@ -36,24 +36,10 @@
  * on objects, or checking certain conditions before operation.  These utilities
  * include {@code null}-safe or {@code null}-tolerant methods for computing the
  * hash code of an object, returning a string for an object, comparing two
  * objects, and checking if indexes or sub-range values are out of bounds.
  *
- * @apiNote
- * Static methods such as {@link Objects#checkIndex},
- * {@link Objects#checkFromToIndex}, and {@link Objects#checkFromIndexSize} are
- * provided for the convenience of checking if values corresponding to indexes
- * and sub-ranges are out of bounds.
- * Variations of these static methods support customization of the runtime
- * exception, and corresponding exception detail message, that is thrown when
- * values are out of bounds.  Such methods accept a functional interface
- * argument, instances of {@code BiFunction}, that maps out-of-bound values to a
- * runtime exception.  Care should be taken when using such methods in
- * combination with an argument that is a lambda expression, method reference or
- * class that capture values.  In such cases the cost of capture, related to
- * functional interface allocation, may exceed the cost of checking bounds.
- *
  * @since 1.7
  */
 public final class Objects {
     private Objects() {
         throw new AssertionError("No java.util.Objects instances for you!");
diff a/src/jdk.compiler/share/classes/com/sun/tools/javac/comp/Flow.java b/src/jdk.compiler/share/classes/com/sun/tools/javac/comp/Flow.java
--- a/src/jdk.compiler/share/classes/com/sun/tools/javac/comp/Flow.java
+++ b/src/jdk.compiler/share/classes/com/sun/tools/javac/comp/Flow.java
@@ -28,11 +28,10 @@
 package com.sun.tools.javac.comp;
 
 import java.util.HashMap;
 import java.util.HashSet;
 import java.util.Set;
-import java.util.stream.Collectors;
 
 import com.sun.source.tree.LambdaExpressionTree.BodyKind;
 import com.sun.tools.javac.code.*;
 import com.sun.tools.javac.code.Scope.WriteableScope;
 import com.sun.tools.javac.code.Source.Feature;
@@ -42,10 +41,11 @@
 import com.sun.tools.javac.util.*;
 import com.sun.tools.javac.util.JCDiagnostic.DiagnosticPosition;
 import com.sun.tools.javac.util.JCDiagnostic.Error;
 import com.sun.tools.javac.util.JCDiagnostic.Warning;
 
+import com.sun.tools.javac.code.Kinds.Kind;
 import com.sun.tools.javac.code.Symbol.*;
 import com.sun.tools.javac.tree.JCTree.*;
 
 import static com.sun.tools.javac.code.Flags.*;
 import static com.sun.tools.javac.code.Flags.BLOCK;
@@ -698,13 +698,16 @@
         public void visitSwitchExpression(JCSwitchExpression tree) {
             ListBuffer<PendingExit> prevPendingExits = pendingExits;
             pendingExits = new ListBuffer<>();
             scan(tree.selector);
             Set<Object> constants = null;
-            if ((tree.selector.type.tsym.flags() & ENUM) != 0) {
+            TypeSymbol selectorSym = tree.selector.type.tsym;
+            if ((selectorSym.flags() & ENUM) != 0) {
                 constants = new HashSet<>();
-                for (Symbol s : tree.selector.type.tsym.members().getSymbols(s -> (s.flags() & ENUM) != 0)) {
+                Filter<Symbol> enumConstantFilter =
+                        s -> (s.flags() & ENUM) != 0 && s.kind == Kind.VAR;
+                for (Symbol s : selectorSym.members().getSymbols(enumConstantFilter)) {
                     constants.add(s.name);
                 }
             }
             boolean hasDefault = false;
             Liveness prevAlive = alive;
@@ -1267,11 +1270,12 @@
                     Type exc = ct.type;
                     if (exc != syms.unknownType) {
                         ctypes = ctypes.append(exc);
                         if (types.isSameType(exc, syms.objectType))
                             continue;
-                        checkCaughtType(l.head.pos(), exc, thrownInTry, caughtInTry);
+                        var pos = subClauses.size() > 1 ? ct.pos() : l.head.pos();
+                        checkCaughtType(pos, exc, thrownInTry, caughtInTry);
                         caughtInTry = chk.incl(exc, caughtInTry);
                     }
                 }
                 scan(param);
                 preciseRethrowTypes.put(param.sym, chk.intersect(ctypes, rethrownTypes));
diff a/test/hotspot/jtreg/TEST.groups b/test/hotspot/jtreg/TEST.groups
--- a/test/hotspot/jtreg/TEST.groups
+++ b/test/hotspot/jtreg/TEST.groups
@@ -1,7 +1,7 @@
 #
-# Copyright (c) 2013, 2019, Oracle and/or its affiliates. All rights reserved.
+# Copyright (c) 2013, 2020, Oracle and/or its affiliates. All rights reserved.
 # DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
 #
 # This code is free software; you can redistribute it and/or modify it
 # under the terms of the GNU General Public License version 2 only, as
 # published by the Free Software Foundation.
@@ -340,10 +340,11 @@
 hotspot_appcds_dynamic = \
   runtime/cds/appcds/ \
  -runtime/cds/appcds/cacheObject \
  -runtime/cds/appcds/customLoader \
  -runtime/cds/appcds/dynamicArchive \
+ -runtime/cds/appcds/loaderConstraints/DynamicLoaderConstraintsTest.java \
  -runtime/cds/appcds/javaldr/ArrayTest.java \
  -runtime/cds/appcds/javaldr/GCSharedStringsDuringDump.java \
  -runtime/cds/appcds/javaldr/HumongousDuringDump.java \
  -runtime/cds/appcds/sharedStrings \
  -runtime/cds/appcds/ArchiveRelocationTest.java \
@@ -364,11 +365,12 @@
   runtime/cds \
   runtime/modules/ModulesSymLink.java \
   runtime/modules/PatchModule/PatchModuleCDS.java \
   runtime/modules/PatchModule/PatchModuleClassList.java \
   runtime/NMT \
-  serviceability/sa
+  serviceability/sa \
+ -runtime/cds/DeterministicDump.java
 
 # A subset of AppCDS tests to be run in tier1
 tier1_runtime_appcds = \
   runtime/cds/appcds/HelloTest.java \
   runtime/cds/appcds/sharedStrings/SharedStringsBasic.java \
diff a/test/hotspot/jtreg/runtime/CompressedOops/CompressedClassPointers.java b/test/hotspot/jtreg/runtime/CompressedOops/CompressedClassPointers.java
--- a/test/hotspot/jtreg/runtime/CompressedOops/CompressedClassPointers.java
+++ b/test/hotspot/jtreg/runtime/CompressedOops/CompressedClassPointers.java
@@ -1,7 +1,7 @@
 /*
- * Copyright (c) 2013, 2019, Oracle and/or its affiliates. All rights reserved.
+ * Copyright (c) 2013, 2020, Oracle and/or its affiliates. All rights reserved.
  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  *
  * This code is free software; you can redistribute it and/or modify it
  * under the terms of the GNU General Public License version 2 only, as
  * published by the Free Software Foundation.
@@ -27,11 +27,11 @@
  * @summary Testing address of compressed class pointer space as best as possible.
  * @requires vm.bits == 64 & vm.opt.final.UseCompressedOops == true & os.family != "windows"
  * @library /test/lib
  * @modules java.base/jdk.internal.misc
  *          java.management
- * @run main CompressedClassPointers
+ * @run driver CompressedClassPointers
  */
 
 import jdk.test.lib.Platform;
 import jdk.test.lib.process.ProcessTools;
 import jdk.test.lib.process.OutputAnalyzer;
diff a/test/hotspot/jtreg/vmTestbase/jit/tiered/Test.java b/test/hotspot/jtreg/vmTestbase/jit/tiered/Test.java
--- a/test/hotspot/jtreg/vmTestbase/jit/tiered/Test.java
+++ b/test/hotspot/jtreg/vmTestbase/jit/tiered/Test.java
@@ -53,11 +53,11 @@
     private static String UNSUPPORTED_OPTION_MESSAGE = "-XX:+TieredCompilation not supported in this VM";
     private static String REGEXP = "^[0-9.]+: \\[compile level=\\d";
     public static void main(String[] args) throws Exception {
         {
             System.out.println("TieredCompilation is enabled");
-            var pb = ProcessTools.createJavaProcessBuilder(true,
+            var pb = ProcessTools.createTestJvm(
                     "-XX:+TieredCompilation",
                     "-XX:+PrintTieredEvents",
                     "-version");
             var output = new OutputAnalyzer(pb.start());
             if (output.getStderr().contains(UNSUPPORTED_OPTION_MESSAGE)) {
@@ -66,11 +66,11 @@
             output.shouldHaveExitValue(0)
                   .stdoutShouldMatch(REGEXP);
         }
         {
             System.out.println("TieredCompilation is disabled");
-            var pb = ProcessTools.createJavaProcessBuilder(true,
+            var pb = ProcessTools.createTestJvm(
                     "-XX:-TieredCompilation",
                     "-XX:+PrintTieredEvents",
                     "-version");
             var output = new OutputAnalyzer(pb.start())
                     .shouldHaveExitValue(0)
