<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/share/runtime/synchronizer.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="safepoint.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="synchronizer.hpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/share/runtime/synchronizer.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
 792 // platforms provide strong ST-ST order, so the issue is moot on IA32,
 793 // x64, and SPARC.
 794 //
 795 // As a general policy we use &quot;volatile&quot; to control compiler-based reordering
 796 // and explicit fences (barriers) to control for architectural reordering
 797 // performed by the CPU(s) or platform.
 798 
 799 struct SharedGlobals {
 800   char         _pad_prefix[OM_CACHE_LINE_SIZE];
 801   // These are highly shared mostly-read variables.
 802   // To avoid false-sharing they need to be the sole occupants of a cache line.
 803   volatile int stw_random;
 804   volatile int stw_cycle;
 805   DEFINE_PAD_MINUS_SIZE(1, OM_CACHE_LINE_SIZE, sizeof(volatile int) * 2);
 806   // Hot RW variable -- Sequester to avoid false-sharing
 807   volatile int hc_sequence;
 808   DEFINE_PAD_MINUS_SIZE(2, OM_CACHE_LINE_SIZE, sizeof(volatile int));
 809 };
 810 
 811 static SharedGlobals GVars;
<span class="line-removed"> 812 static int _forceMonitorScavenge = 0; // Scavenge required and pending</span>
 813 
 814 static markWord read_stable_mark(oop obj) {
 815   markWord mark = obj-&gt;mark();
 816   if (!mark.is_being_inflated()) {
 817     return mark;       // normal fast-path return
 818   }
 819 
 820   int its = 0;
 821   for (;;) {
 822     markWord mark = obj-&gt;mark();
 823     if (!mark.is_being_inflated()) {
 824       return mark;    // normal fast-path return
 825     }
 826 
 827     // The object is being inflated by some other thread.
 828     // The caller of read_stable_mark() must wait for inflation to complete.
 829     // Avoid live-lock
 830     // TODO: consider calling SafepointSynchronize::do_call_back() while
 831     // spinning to see if there&#39;s a safepoint pending.  If so, immediately
 832     // yielding or blocking would be appropriate.  Avoid spinning while
</pre>
<hr />
<pre>
1186     }
1187     // unmarked_next() is not needed with g_block_list (no locking
1188     // used with block linkage _next_om fields).
1189     block = (PaddedObjectMonitor*)block-&gt;next_om();
1190   }
1191 }
1192 
1193 static bool monitors_used_above_threshold() {
1194   int population = Atomic::load(&amp;om_list_globals._population);
1195   if (population == 0) {
1196     return false;
1197   }
1198   if (MonitorUsedDeflationThreshold &gt; 0) {
1199     int monitors_used = population - Atomic::load(&amp;om_list_globals._free_count);
1200     int monitor_usage = (monitors_used * 100LL) / population;
1201     return monitor_usage &gt; MonitorUsedDeflationThreshold;
1202   }
1203   return false;
1204 }
1205 
<span class="line-modified">1206 // Returns true if MonitorBound is set (&gt; 0) and if the specified</span>
<span class="line-removed">1207 // cnt is &gt; MonitorBound. Otherwise returns false.</span>
<span class="line-removed">1208 static bool is_MonitorBound_exceeded(const int cnt) {</span>
<span class="line-removed">1209   const int mx = MonitorBound;</span>
<span class="line-removed">1210   return mx &gt; 0 &amp;&amp; cnt &gt; mx;</span>
<span class="line-removed">1211 }</span>
<span class="line-removed">1212 </span>
<span class="line-removed">1213 bool ObjectSynchronizer::is_cleanup_needed() {</span>
<span class="line-removed">1214   if (monitors_used_above_threshold()) {</span>
<span class="line-removed">1215     // Too many monitors in use.</span>
<span class="line-removed">1216     return true;</span>
<span class="line-removed">1217   }</span>
<span class="line-removed">1218   return needs_monitor_scavenge();</span>
<span class="line-removed">1219 }</span>
<span class="line-removed">1220 </span>
<span class="line-removed">1221 bool ObjectSynchronizer::needs_monitor_scavenge() {</span>
<span class="line-removed">1222   if (Atomic::load(&amp;_forceMonitorScavenge) == 1) {</span>
<span class="line-removed">1223     log_info(monitorinflation)(&quot;Monitor scavenge needed, triggering safepoint cleanup.&quot;);</span>
<span class="line-removed">1224     return true;</span>
<span class="line-removed">1225   }</span>
1226   return false;
1227 }
1228 
1229 void ObjectSynchronizer::oops_do(OopClosure* f) {
1230   // We only scan the global used list here (for moribund threads), and
1231   // the thread-local monitors in Thread::oops_do().
1232   global_used_oops_do(f);
1233 }
1234 
1235 void ObjectSynchronizer::global_used_oops_do(OopClosure* f) {
1236   assert(SafepointSynchronize::is_at_safepoint(), &quot;must be at safepoint&quot;);
1237   list_oops_do(Atomic::load(&amp;om_list_globals._in_use_list), f);
1238 }
1239 
1240 void ObjectSynchronizer::thread_local_used_oops_do(Thread* thread, OopClosure* f) {
1241   assert(SafepointSynchronize::is_at_safepoint(), &quot;must be at safepoint&quot;);
1242   list_oops_do(thread-&gt;om_in_use_list, f);
1243 }
1244 
1245 void ObjectSynchronizer::list_oops_do(ObjectMonitor* list, OopClosure* f) {
</pre>
<hr />
<pre>
1253   }
1254 }
1255 
1256 
1257 // -----------------------------------------------------------------------------
1258 // ObjectMonitor Lifecycle
1259 // -----------------------
1260 // Inflation unlinks monitors from om_list_globals._free_list or a per-thread
1261 // free list and associates them with objects. Deflation -- which occurs at
1262 // STW-time -- disassociates idle monitors from objects.
1263 // Such scavenged monitors are returned to the om_list_globals._free_list.
1264 //
1265 // ObjectMonitors reside in type-stable memory (TSM) and are immortal.
1266 //
1267 // Lifecycle:
1268 // --   unassigned and on the om_list_globals._free_list
1269 // --   unassigned and on a per-thread free list
1270 // --   assigned to an object.  The object is inflated and the mark refers
1271 //      to the ObjectMonitor.
1272 
<span class="line-removed">1273 </span>
<span class="line-removed">1274 // Constraining monitor pool growth via MonitorBound ...</span>
<span class="line-removed">1275 //</span>
<span class="line-removed">1276 // If MonitorBound is not set (&lt;= 0), MonitorBound checks are disabled.</span>
<span class="line-removed">1277 //</span>
<span class="line-removed">1278 // The monitor pool is grow-only.  We scavenge at STW safepoint-time, but the</span>
<span class="line-removed">1279 // the rate of scavenging is driven primarily by GC.  As such,  we can find</span>
<span class="line-removed">1280 // an inordinate number of monitors in circulation.</span>
<span class="line-removed">1281 // To avoid that scenario we can artificially induce a STW safepoint</span>
<span class="line-removed">1282 // if the pool appears to be growing past some reasonable bound.</span>
<span class="line-removed">1283 // Generally we favor time in space-time tradeoffs, but as there&#39;s no</span>
<span class="line-removed">1284 // natural back-pressure on the # of extant monitors we need to impose some</span>
<span class="line-removed">1285 // type of limit.  Beware that if MonitorBound is set to too low a value</span>
<span class="line-removed">1286 // we could just loop. In addition, if MonitorBound is set to a low value</span>
<span class="line-removed">1287 // we&#39;ll incur more safepoints, which are harmful to performance.</span>
<span class="line-removed">1288 // See also: GuaranteedSafepointInterval</span>
<span class="line-removed">1289 //</span>
<span class="line-removed">1290 // If MonitorBound is set, the boundry applies to</span>
<span class="line-removed">1291 //     (om_list_globals._population - om_list_globals._free_count)</span>
<span class="line-removed">1292 // i.e., if there are not enough ObjectMonitors on the global free list,</span>
<span class="line-removed">1293 // then a safepoint deflation is induced. Picking a good MonitorBound value</span>
<span class="line-removed">1294 // is non-trivial.</span>
<span class="line-removed">1295 </span>
<span class="line-removed">1296 static void InduceScavenge(Thread* self, const char * Whence) {</span>
<span class="line-removed">1297   // Induce STW safepoint to trim monitors</span>
<span class="line-removed">1298   // Ultimately, this results in a call to deflate_idle_monitors() in the near future.</span>
<span class="line-removed">1299   // More precisely, trigger a cleanup safepoint as the number</span>
<span class="line-removed">1300   // of active monitors passes the specified threshold.</span>
<span class="line-removed">1301   // TODO: assert thread state is reasonable</span>
<span class="line-removed">1302 </span>
<span class="line-removed">1303   if (Atomic::xchg(&amp;_forceMonitorScavenge, 1) == 0) {</span>
<span class="line-removed">1304     VMThread::check_for_forced_cleanup();</span>
<span class="line-removed">1305   }</span>
<span class="line-removed">1306 }</span>
<span class="line-removed">1307 </span>
1308 ObjectMonitor* ObjectSynchronizer::om_alloc(Thread* self) {
1309   // A large MAXPRIVATE value reduces both list lock contention
1310   // and list coherency traffic, but also tends to increase the
1311   // number of ObjectMonitors in circulation as well as the STW
1312   // scavenge costs.  As usual, we lean toward time in space-time
1313   // tradeoffs.
1314   const int MAXPRIVATE = 1024;
1315   NoSafepointVerifier nsv;
1316 
1317   for (;;) {
1318     ObjectMonitor* m;
1319 
1320     // 1: try to allocate from the thread&#39;s local om_free_list.
1321     // Threads will attempt to allocate first from their local list, then
1322     // from the global list, and only after those attempts fail will the
1323     // thread attempt to instantiate new monitors. Thread-local free lists
1324     // improve allocation latency, as well as reducing coherency traffic
1325     // on the shared global list.
1326     m = take_from_start_of_om_free_list(self);
1327     if (m != NULL) {
</pre>
<hr />
<pre>
1331     }
1332 
1333     // 2: try to allocate from the global om_list_globals._free_list
1334     // If we&#39;re using thread-local free lists then try
1335     // to reprovision the caller&#39;s free list.
1336     if (Atomic::load(&amp;om_list_globals._free_list) != NULL) {
1337       // Reprovision the thread&#39;s om_free_list.
1338       // Use bulk transfers to reduce the allocation rate and heat
1339       // on various locks.
1340       for (int i = self-&gt;om_free_provision; --i &gt;= 0;) {
1341         ObjectMonitor* take = take_from_start_of_global_free_list();
1342         if (take == NULL) {
1343           break;  // No more are available.
1344         }
1345         guarantee(take-&gt;object() == NULL, &quot;invariant&quot;);
1346         take-&gt;Recycle();
1347         om_release(self, take, false);
1348       }
1349       self-&gt;om_free_provision += 1 + (self-&gt;om_free_provision / 2);
1350       if (self-&gt;om_free_provision &gt; MAXPRIVATE) self-&gt;om_free_provision = MAXPRIVATE;
<span class="line-removed">1351 </span>
<span class="line-removed">1352       if (is_MonitorBound_exceeded(Atomic::load(&amp;om_list_globals._population) -</span>
<span class="line-removed">1353                                    Atomic::load(&amp;om_list_globals._free_count))) {</span>
<span class="line-removed">1354         // Not enough ObjectMonitors on the global free list.</span>
<span class="line-removed">1355         // We can&#39;t safely induce a STW safepoint from om_alloc() as our thread</span>
<span class="line-removed">1356         // state may not be appropriate for such activities and callers may hold</span>
<span class="line-removed">1357         // naked oops, so instead we defer the action.</span>
<span class="line-removed">1358         InduceScavenge(self, &quot;om_alloc&quot;);</span>
<span class="line-removed">1359       }</span>
1360       continue;
1361     }
1362 
1363     // 3: allocate a block of new ObjectMonitors
1364     // Both the local and global free lists are empty -- resort to malloc().
1365     // In the current implementation ObjectMonitors are TSM - immortal.
1366     // Ideally, we&#39;d write &quot;new ObjectMonitor[_BLOCKSIZE], but we want
1367     // each ObjectMonitor to start at the beginning of a cache line,
1368     // so we use align_up().
1369     // A better solution would be to use C++ placement-new.
1370     // BEWARE: As it stands currently, we don&#39;t run the ctors!
1371     assert(_BLOCKSIZE &gt; 1, &quot;invariant&quot;);
1372     size_t neededsize = sizeof(PaddedObjectMonitor) * _BLOCKSIZE;
1373     PaddedObjectMonitor* temp;
1374     size_t aligned_size = neededsize + (OM_CACHE_LINE_SIZE - 1);
1375     void* real_malloc_addr = NEW_C_HEAP_ARRAY(char, aligned_size, mtInternal);
1376     temp = (PaddedObjectMonitor*)align_up(real_malloc_addr, OM_CACHE_LINE_SIZE);
1377     (void)memset((void *) temp, 0, neededsize);
1378 
1379     // Format the block.
</pre>
<hr />
<pre>
2045 
2046 void ObjectSynchronizer::finish_deflate_idle_monitors(DeflateMonitorCounters* counters) {
2047   // Report the cumulative time for deflating each thread&#39;s idle
2048   // monitors. Note: if the work is split among more than one
2049   // worker thread, then the reported time will likely be more
2050   // than a beginning to end measurement of the phase.
2051   log_info(safepoint, cleanup)(&quot;deflating per-thread idle monitors, %3.7f secs, monitors=%d&quot;, counters-&gt;per_thread_times, counters-&gt;per_thread_scavenged);
2052 
2053   if (log_is_enabled(Debug, monitorinflation)) {
2054     // exit_globals()&#39;s call to audit_and_print_stats() is done
2055     // at the Info level and not at a safepoint.
2056     ObjectSynchronizer::audit_and_print_stats(false /* on_exit */);
2057   } else if (log_is_enabled(Info, monitorinflation)) {
2058     log_info(monitorinflation)(&quot;global_population=%d, global_in_use_count=%d, &quot;
2059                                &quot;global_free_count=%d&quot;,
2060                                Atomic::load(&amp;om_list_globals._population),
2061                                Atomic::load(&amp;om_list_globals._in_use_count),
2062                                Atomic::load(&amp;om_list_globals._free_count));
2063   }
2064 
<span class="line-removed">2065   Atomic::store(&amp;_forceMonitorScavenge, 0);    // Reset</span>
<span class="line-removed">2066 </span>
2067   OM_PERFDATA_OP(Deflations, inc(counters-&gt;n_scavenged));
2068   OM_PERFDATA_OP(MonExtant, set_value(counters-&gt;n_in_circulation));
2069 
2070   GVars.stw_random = os::random();
2071   GVars.stw_cycle++;
2072 }
2073 
2074 void ObjectSynchronizer::deflate_thread_local_monitors(Thread* thread, DeflateMonitorCounters* counters) {
2075   assert(SafepointSynchronize::is_at_safepoint(), &quot;must be at safepoint&quot;);
2076 
2077   ObjectMonitor* free_head_p = NULL;  // Local SLL of scavenged monitors
2078   ObjectMonitor* free_tail_p = NULL;
2079   elapsedTimer timer;
2080 
2081   if (log_is_enabled(Info, safepoint, cleanup) ||
2082       log_is_enabled(Info, monitorinflation)) {
2083     timer.start();
2084   }
2085 
2086   // Update n_in_circulation before om_in_use_count is updated by deflation.
</pre>
</td>
<td>
<hr />
<pre>
 792 // platforms provide strong ST-ST order, so the issue is moot on IA32,
 793 // x64, and SPARC.
 794 //
 795 // As a general policy we use &quot;volatile&quot; to control compiler-based reordering
 796 // and explicit fences (barriers) to control for architectural reordering
 797 // performed by the CPU(s) or platform.
 798 
 799 struct SharedGlobals {
 800   char         _pad_prefix[OM_CACHE_LINE_SIZE];
 801   // These are highly shared mostly-read variables.
 802   // To avoid false-sharing they need to be the sole occupants of a cache line.
 803   volatile int stw_random;
 804   volatile int stw_cycle;
 805   DEFINE_PAD_MINUS_SIZE(1, OM_CACHE_LINE_SIZE, sizeof(volatile int) * 2);
 806   // Hot RW variable -- Sequester to avoid false-sharing
 807   volatile int hc_sequence;
 808   DEFINE_PAD_MINUS_SIZE(2, OM_CACHE_LINE_SIZE, sizeof(volatile int));
 809 };
 810 
 811 static SharedGlobals GVars;

 812 
 813 static markWord read_stable_mark(oop obj) {
 814   markWord mark = obj-&gt;mark();
 815   if (!mark.is_being_inflated()) {
 816     return mark;       // normal fast-path return
 817   }
 818 
 819   int its = 0;
 820   for (;;) {
 821     markWord mark = obj-&gt;mark();
 822     if (!mark.is_being_inflated()) {
 823       return mark;    // normal fast-path return
 824     }
 825 
 826     // The object is being inflated by some other thread.
 827     // The caller of read_stable_mark() must wait for inflation to complete.
 828     // Avoid live-lock
 829     // TODO: consider calling SafepointSynchronize::do_call_back() while
 830     // spinning to see if there&#39;s a safepoint pending.  If so, immediately
 831     // yielding or blocking would be appropriate.  Avoid spinning while
</pre>
<hr />
<pre>
1185     }
1186     // unmarked_next() is not needed with g_block_list (no locking
1187     // used with block linkage _next_om fields).
1188     block = (PaddedObjectMonitor*)block-&gt;next_om();
1189   }
1190 }
1191 
1192 static bool monitors_used_above_threshold() {
1193   int population = Atomic::load(&amp;om_list_globals._population);
1194   if (population == 0) {
1195     return false;
1196   }
1197   if (MonitorUsedDeflationThreshold &gt; 0) {
1198     int monitors_used = population - Atomic::load(&amp;om_list_globals._free_count);
1199     int monitor_usage = (monitors_used * 100LL) / population;
1200     return monitor_usage &gt; MonitorUsedDeflationThreshold;
1201   }
1202   return false;
1203 }
1204 
<span class="line-modified">1205 bool ObjectSynchronizer::is_cleanup_needed() {</span>



















1206   return monitors_used_above_threshold();
1207 }
1208 
1209 void ObjectSynchronizer::oops_do(OopClosure* f) {
1210   // We only scan the global used list here (for moribund threads), and
1211   // the thread-local monitors in Thread::oops_do().
1212   global_used_oops_do(f);
1213 }
1214 
1215 void ObjectSynchronizer::global_used_oops_do(OopClosure* f) {
1216   assert(SafepointSynchronize::is_at_safepoint(), &quot;must be at safepoint&quot;);
1217   list_oops_do(Atomic::load(&amp;om_list_globals._in_use_list), f);
1218 }
1219 
1220 void ObjectSynchronizer::thread_local_used_oops_do(Thread* thread, OopClosure* f) {
1221   assert(SafepointSynchronize::is_at_safepoint(), &quot;must be at safepoint&quot;);
1222   list_oops_do(thread-&gt;om_in_use_list, f);
1223 }
1224 
1225 void ObjectSynchronizer::list_oops_do(ObjectMonitor* list, OopClosure* f) {
</pre>
<hr />
<pre>
1233   }
1234 }
1235 
1236 
1237 // -----------------------------------------------------------------------------
1238 // ObjectMonitor Lifecycle
1239 // -----------------------
1240 // Inflation unlinks monitors from om_list_globals._free_list or a per-thread
1241 // free list and associates them with objects. Deflation -- which occurs at
1242 // STW-time -- disassociates idle monitors from objects.
1243 // Such scavenged monitors are returned to the om_list_globals._free_list.
1244 //
1245 // ObjectMonitors reside in type-stable memory (TSM) and are immortal.
1246 //
1247 // Lifecycle:
1248 // --   unassigned and on the om_list_globals._free_list
1249 // --   unassigned and on a per-thread free list
1250 // --   assigned to an object.  The object is inflated and the mark refers
1251 //      to the ObjectMonitor.
1252 



































1253 ObjectMonitor* ObjectSynchronizer::om_alloc(Thread* self) {
1254   // A large MAXPRIVATE value reduces both list lock contention
1255   // and list coherency traffic, but also tends to increase the
1256   // number of ObjectMonitors in circulation as well as the STW
1257   // scavenge costs.  As usual, we lean toward time in space-time
1258   // tradeoffs.
1259   const int MAXPRIVATE = 1024;
1260   NoSafepointVerifier nsv;
1261 
1262   for (;;) {
1263     ObjectMonitor* m;
1264 
1265     // 1: try to allocate from the thread&#39;s local om_free_list.
1266     // Threads will attempt to allocate first from their local list, then
1267     // from the global list, and only after those attempts fail will the
1268     // thread attempt to instantiate new monitors. Thread-local free lists
1269     // improve allocation latency, as well as reducing coherency traffic
1270     // on the shared global list.
1271     m = take_from_start_of_om_free_list(self);
1272     if (m != NULL) {
</pre>
<hr />
<pre>
1276     }
1277 
1278     // 2: try to allocate from the global om_list_globals._free_list
1279     // If we&#39;re using thread-local free lists then try
1280     // to reprovision the caller&#39;s free list.
1281     if (Atomic::load(&amp;om_list_globals._free_list) != NULL) {
1282       // Reprovision the thread&#39;s om_free_list.
1283       // Use bulk transfers to reduce the allocation rate and heat
1284       // on various locks.
1285       for (int i = self-&gt;om_free_provision; --i &gt;= 0;) {
1286         ObjectMonitor* take = take_from_start_of_global_free_list();
1287         if (take == NULL) {
1288           break;  // No more are available.
1289         }
1290         guarantee(take-&gt;object() == NULL, &quot;invariant&quot;);
1291         take-&gt;Recycle();
1292         om_release(self, take, false);
1293       }
1294       self-&gt;om_free_provision += 1 + (self-&gt;om_free_provision / 2);
1295       if (self-&gt;om_free_provision &gt; MAXPRIVATE) self-&gt;om_free_provision = MAXPRIVATE;









1296       continue;
1297     }
1298 
1299     // 3: allocate a block of new ObjectMonitors
1300     // Both the local and global free lists are empty -- resort to malloc().
1301     // In the current implementation ObjectMonitors are TSM - immortal.
1302     // Ideally, we&#39;d write &quot;new ObjectMonitor[_BLOCKSIZE], but we want
1303     // each ObjectMonitor to start at the beginning of a cache line,
1304     // so we use align_up().
1305     // A better solution would be to use C++ placement-new.
1306     // BEWARE: As it stands currently, we don&#39;t run the ctors!
1307     assert(_BLOCKSIZE &gt; 1, &quot;invariant&quot;);
1308     size_t neededsize = sizeof(PaddedObjectMonitor) * _BLOCKSIZE;
1309     PaddedObjectMonitor* temp;
1310     size_t aligned_size = neededsize + (OM_CACHE_LINE_SIZE - 1);
1311     void* real_malloc_addr = NEW_C_HEAP_ARRAY(char, aligned_size, mtInternal);
1312     temp = (PaddedObjectMonitor*)align_up(real_malloc_addr, OM_CACHE_LINE_SIZE);
1313     (void)memset((void *) temp, 0, neededsize);
1314 
1315     // Format the block.
</pre>
<hr />
<pre>
1981 
1982 void ObjectSynchronizer::finish_deflate_idle_monitors(DeflateMonitorCounters* counters) {
1983   // Report the cumulative time for deflating each thread&#39;s idle
1984   // monitors. Note: if the work is split among more than one
1985   // worker thread, then the reported time will likely be more
1986   // than a beginning to end measurement of the phase.
1987   log_info(safepoint, cleanup)(&quot;deflating per-thread idle monitors, %3.7f secs, monitors=%d&quot;, counters-&gt;per_thread_times, counters-&gt;per_thread_scavenged);
1988 
1989   if (log_is_enabled(Debug, monitorinflation)) {
1990     // exit_globals()&#39;s call to audit_and_print_stats() is done
1991     // at the Info level and not at a safepoint.
1992     ObjectSynchronizer::audit_and_print_stats(false /* on_exit */);
1993   } else if (log_is_enabled(Info, monitorinflation)) {
1994     log_info(monitorinflation)(&quot;global_population=%d, global_in_use_count=%d, &quot;
1995                                &quot;global_free_count=%d&quot;,
1996                                Atomic::load(&amp;om_list_globals._population),
1997                                Atomic::load(&amp;om_list_globals._in_use_count),
1998                                Atomic::load(&amp;om_list_globals._free_count));
1999   }
2000 


2001   OM_PERFDATA_OP(Deflations, inc(counters-&gt;n_scavenged));
2002   OM_PERFDATA_OP(MonExtant, set_value(counters-&gt;n_in_circulation));
2003 
2004   GVars.stw_random = os::random();
2005   GVars.stw_cycle++;
2006 }
2007 
2008 void ObjectSynchronizer::deflate_thread_local_monitors(Thread* thread, DeflateMonitorCounters* counters) {
2009   assert(SafepointSynchronize::is_at_safepoint(), &quot;must be at safepoint&quot;);
2010 
2011   ObjectMonitor* free_head_p = NULL;  // Local SLL of scavenged monitors
2012   ObjectMonitor* free_tail_p = NULL;
2013   elapsedTimer timer;
2014 
2015   if (log_is_enabled(Info, safepoint, cleanup) ||
2016       log_is_enabled(Info, monitorinflation)) {
2017     timer.start();
2018   }
2019 
2020   // Update n_in_circulation before om_in_use_count is updated by deflation.
</pre>
</td>
</tr>
</table>
<center><a href="safepoint.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="synchronizer.hpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>