<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/share/memory/metaspaceShared.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="dynamicArchive.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="../oops/arrayKlass.cpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/share/memory/metaspaceShared.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
  62 #include &quot;oops/valueArrayKlass.hpp&quot;
  63 #include &quot;oops/valueKlass.hpp&quot;
  64 #include &quot;prims/jvmtiRedefineClasses.hpp&quot;
  65 #include &quot;runtime/handles.inline.hpp&quot;
  66 #include &quot;runtime/os.hpp&quot;
  67 #include &quot;runtime/safepointVerifiers.hpp&quot;
  68 #include &quot;runtime/signature.hpp&quot;
  69 #include &quot;runtime/timerTrace.hpp&quot;
  70 #include &quot;runtime/vmThread.hpp&quot;
  71 #include &quot;runtime/vmOperations.hpp&quot;
  72 #include &quot;utilities/align.hpp&quot;
  73 #include &quot;utilities/bitMap.inline.hpp&quot;
  74 #include &quot;utilities/defaultStream.hpp&quot;
  75 #include &quot;utilities/hashtable.inline.hpp&quot;
  76 #if INCLUDE_G1GC
  77 #include &quot;gc/g1/g1CollectedHeap.hpp&quot;
  78 #endif
  79 
  80 ReservedSpace MetaspaceShared::_shared_rs;
  81 VirtualSpace MetaspaceShared::_shared_vs;


  82 MetaspaceSharedStats MetaspaceShared::_stats;
  83 bool MetaspaceShared::_has_error_classes;
  84 bool MetaspaceShared::_archive_loading_failed = false;
  85 bool MetaspaceShared::_remapped_readwrite = false;
  86 address MetaspaceShared::_i2i_entry_code_buffers = NULL;
  87 size_t MetaspaceShared::_i2i_entry_code_buffers_size = 0;
  88 void* MetaspaceShared::_shared_metaspace_static_top = NULL;
  89 intx MetaspaceShared::_relocation_delta;
  90 
  91 // The CDS archive is divided into the following regions:
  92 //     mc  - misc code (the method entry trampolines, c++ vtables)
  93 //     rw  - read-write metadata
  94 //     ro  - read-only metadata and read-only tables
  95 //
  96 //     ca0 - closed archive heap space #0
  97 //     ca1 - closed archive heap space #1 (may be empty)
  98 //     oa0 - open archive heap space #0
  99 //     oa1 - open archive heap space #1 (may be empty)
 100 //
 101 // The mc, rw, and ro regions are linearly allocated, starting from
</pre>
<hr />
<pre>
 105 // These 3 regions are populated in the following steps:
 106 // [1] All classes are loaded in MetaspaceShared::preload_classes(). All metadata are
 107 //     temporarily allocated outside of the shared regions. Only the method entry
 108 //     trampolines are written into the mc region.
 109 // [2] C++ vtables are copied into the mc region.
 110 // [3] ArchiveCompactor copies RW metadata into the rw region.
 111 // [4] ArchiveCompactor copies RO metadata into the ro region.
 112 // [5] SymbolTable, StringTable, SystemDictionary, and a few other read-only data
 113 //     are copied into the ro region as read-only tables.
 114 //
 115 // The s0/s1 and oa0/oa1 regions are populated inside HeapShared::archive_java_heap_objects.
 116 // Their layout is independent of the other 4 regions.
 117 
 118 char* DumpRegion::expand_top_to(char* newtop) {
 119   assert(is_allocatable(), &quot;must be initialized and not packed&quot;);
 120   assert(newtop &gt;= _top, &quot;must not grow backwards&quot;);
 121   if (newtop &gt; _end) {
 122     MetaspaceShared::report_out_of_space(_name, newtop - _top);
 123     ShouldNotReachHere();
 124   }
<span class="line-modified"> 125   uintx delta;</span>
<span class="line-modified"> 126   if (DynamicDumpSharedSpaces) {</span>
<span class="line-modified"> 127     delta = DynamicArchive::object_delta_uintx(newtop);</span>
<span class="line-modified"> 128   } else {</span>
<span class="line-modified"> 129     delta = MetaspaceShared::object_delta_uintx(newtop);</span>
<span class="line-modified"> 130   }</span>
<span class="line-modified"> 131   if (delta &gt; MAX_SHARED_DELTA) {</span>
<span class="line-modified"> 132     // This is just a sanity check and should not appear in any real world usage. This</span>
<span class="line-modified"> 133     // happens only if you allocate more than 2GB of shared objects and would require</span>
<span class="line-modified"> 134     // millions of shared classes.</span>
<span class="line-modified"> 135     vm_exit_during_initialization(&quot;Out of memory in the CDS archive&quot;,</span>
<span class="line-modified"> 136                                   &quot;Please reduce the number of shared classes.&quot;);</span>



 137   }
 138 
<span class="line-modified"> 139   MetaspaceShared::commit_shared_space_to(newtop);</span>
 140   _top = newtop;
 141   return _top;
 142 }
 143 
 144 char* DumpRegion::allocate(size_t num_bytes, size_t alignment) {
 145   char* p = (char*)align_up(_top, alignment);
 146   char* newtop = p + align_up(num_bytes, alignment);
 147   expand_top_to(newtop);
 148   memset(p, 0, newtop - p);
 149   return p;
 150 }
 151 
 152 void DumpRegion::append_intptr_t(intptr_t n, bool need_to_mark) {
 153   assert(is_aligned(_top, sizeof(intptr_t)), &quot;bad alignment&quot;);
 154   intptr_t *p = (intptr_t*)_top;
 155   char* newtop = _top + sizeof(intptr_t);
 156   expand_top_to(newtop);
 157   *p = n;
 158   if (need_to_mark) {
 159     ArchivePtrMarker::mark_pointer(p);
 160   }
 161 }
 162 
 163 void DumpRegion::print(size_t total_bytes) const {
 164   log_debug(cds)(&quot;%-3s space: &quot; SIZE_FORMAT_W(9) &quot; [ %4.1f%% of total] out of &quot; SIZE_FORMAT_W(9) &quot; bytes [%5.1f%% used] at &quot; INTPTR_FORMAT,
 165                  _name, used(), percent_of(used(), total_bytes), reserved(), percent_of(used(), reserved()),
 166                  p2i(_base + MetaspaceShared::final_delta()));
 167 }
 168 
 169 void DumpRegion::print_out_of_space_msg(const char* failing_region, size_t needed_bytes) {
 170   log_error(cds)(&quot;[%-8s] &quot; PTR_FORMAT &quot; - &quot; PTR_FORMAT &quot; capacity =%9d, allocated =%9d&quot;,
 171                  _name, p2i(_base), p2i(_top), int(_end - _base), int(_top - _base));
 172   if (strcmp(_name, failing_region) == 0) {
 173     log_error(cds)(&quot; required = %d&quot;, int(needed_bytes));
 174   }
 175 }
 176 












 177 void DumpRegion::pack(DumpRegion* next) {
 178   assert(!is_packed(), &quot;sanity&quot;);
 179   _end = (char*)align_up(_top, Metaspace::reserve_alignment());
 180   _is_packed = true;
 181   if (next != NULL) {


 182     next-&gt;_base = next-&gt;_top = this-&gt;_end;
<span class="line-modified"> 183     next-&gt;_end = MetaspaceShared::shared_rs()-&gt;end();</span>
 184   }
 185 }
 186 
<span class="line-modified"> 187 static DumpRegion _mc_region(&quot;mc&quot;), _ro_region(&quot;ro&quot;), _rw_region(&quot;rw&quot;);</span>
 188 static size_t _total_closed_archive_region_size = 0, _total_open_archive_region_size = 0;
 189 
<span class="line-modified"> 190 void MetaspaceShared::init_shared_dump_space(DumpRegion* first_space, address first_space_bottom) {</span>
<span class="line-modified"> 191   // Start with 0 committed bytes. The memory will be committed as needed by</span>
<span class="line-removed"> 192   // MetaspaceShared::commit_shared_space_to().</span>
<span class="line-removed"> 193   if (!_shared_vs.initialize(_shared_rs, 0)) {</span>
<span class="line-removed"> 194     fatal(&quot;Unable to allocate memory for shared space&quot;);</span>
<span class="line-removed"> 195   }</span>
<span class="line-removed"> 196   first_space-&gt;init(&amp;_shared_rs, (char*)first_space_bottom);</span>
 197 }
 198 
 199 DumpRegion* MetaspaceShared::misc_code_dump_space() {
 200   return &amp;_mc_region;
 201 }
 202 
 203 DumpRegion* MetaspaceShared::read_write_dump_space() {
 204   return &amp;_rw_region;
 205 }
 206 
 207 DumpRegion* MetaspaceShared::read_only_dump_space() {
 208   return &amp;_ro_region;
 209 }
 210 
 211 void MetaspaceShared::pack_dump_space(DumpRegion* current, DumpRegion* next,
 212                                       ReservedSpace* rs) {
 213   current-&gt;pack(next);
 214 }
 215 




 216 char* MetaspaceShared::misc_code_space_alloc(size_t num_bytes) {
 217   return _mc_region.allocate(num_bytes);
 218 }
 219 
 220 char* MetaspaceShared::read_only_space_alloc(size_t num_bytes) {
 221   return _ro_region.allocate(num_bytes);
 222 }
 223 
 224 // When reserving an address range using ReservedSpace, we need an alignment that satisfies both:
 225 // os::vm_allocation_granularity() -- so that we can sub-divide this range into multiple mmap regions,
 226 //                                    while keeping the first range at offset 0 of this range.
 227 // Metaspace::reserve_alignment()  -- so we can pass the region to
 228 //                                    Metaspace::allocate_metaspace_compressed_klass_ptrs.
 229 size_t MetaspaceShared::reserved_space_alignment() {
 230   size_t os_align = os::vm_allocation_granularity();
 231   size_t ms_align = Metaspace::reserve_alignment();
 232   if (os_align &gt;= ms_align) {
 233     assert(os_align % ms_align == 0, &quot;must be a multiple&quot;);
 234     return os_align;
 235   } else {
</pre>
<hr />
<pre>
 305     // Set up compress class pointers.
 306     CompressedKlassPointers::set_base((address)_shared_rs.base());
 307     // Set narrow_klass_shift to be LogKlassAlignmentInBytes. This is consistent
 308     // with AOT.
 309     CompressedKlassPointers::set_shift(LogKlassAlignmentInBytes);
 310     // Set the range of klass addresses to 4GB.
 311     CompressedKlassPointers::set_range(cds_total);
 312     Metaspace::initialize_class_space(tmp_class_space);
 313   }
 314   log_info(cds)(&quot;narrow_klass_base = &quot; PTR_FORMAT &quot;, narrow_klass_shift = %d&quot;,
 315                 p2i(CompressedKlassPointers::base()), CompressedKlassPointers::shift());
 316 
 317   log_info(cds)(&quot;Allocated temporary class space: &quot; SIZE_FORMAT &quot; bytes at &quot; PTR_FORMAT,
 318                 CompressedClassSpaceSize, p2i(tmp_class_space.base()));
 319 #endif
 320 
 321   init_shared_dump_space(&amp;_mc_region);
 322   SharedBaseAddress = (size_t)_shared_rs.base();
 323   log_info(cds)(&quot;Allocated shared space: &quot; SIZE_FORMAT &quot; bytes at &quot; PTR_FORMAT,
 324                 _shared_rs.size(), p2i(_shared_rs.base()));








 325 }
 326 
 327 // Called by universe_post_init()
 328 void MetaspaceShared::post_initialize(TRAPS) {
 329   if (UseSharedSpaces) {
 330     int size = FileMapInfo::get_number_of_shared_paths();
 331     if (size &gt; 0) {
 332       SystemDictionaryShared::allocate_shared_data_arrays(size, THREAD);
 333       if (!DynamicDumpSharedSpaces) {
 334         FileMapInfo* info;
 335         if (FileMapInfo::dynamic_info() == NULL) {
 336           info = FileMapInfo::current_info();
 337         } else {
 338           info = FileMapInfo::dynamic_info();
 339         }
 340         ClassLoaderExt::init_paths_start_index(info-&gt;app_class_paths_start_index());
 341         ClassLoaderExt::init_app_module_paths_start_index(info-&gt;app_module_paths_start_index());
 342       }
 343     }
 344   }
</pre>
<hr />
<pre>
 382           const HeapRegion* hr = G1CollectedHeap::heap()-&gt;heap_region_containing(body);
 383           if (hr-&gt;is_humongous()) {
 384             // Don&#39;t keep it alive, so it will be GC&#39;ed before we dump the strings, in order
 385             // to maximize free heap space and minimize fragmentation.
 386             log_warning(cds, heap)(&quot;[line %d] extra interned string ignored; size too large: %d&quot;,
 387                                 reader.last_line_no(), utf8_length);
 388             continue;
 389           }
 390         }
 391 #endif
 392         // Interned strings are GC&#39;ed if there are no references to it, so let&#39;s
 393         // add a reference to keep this string alive.
 394         assert(s != NULL, &quot;must succeed&quot;);
 395         Handle h(THREAD, s);
 396         _extra_interned_strings-&gt;append(h);
 397       }
 398     }
 399   }
 400 }
 401 
<span class="line-modified"> 402 void MetaspaceShared::commit_shared_space_to(char* newtop) {</span>
 403   Arguments::assert_is_dumping_archive();
<span class="line-modified"> 404   char* base = _shared_rs.base();</span>
 405   size_t need_committed_size = newtop - base;
<span class="line-modified"> 406   size_t has_committed_size = _shared_vs.committed_size();</span>
 407   if (need_committed_size &lt; has_committed_size) {
 408     return;
 409   }
 410 
 411   size_t min_bytes = need_committed_size - has_committed_size;
 412   size_t preferred_bytes = 1 * M;
<span class="line-modified"> 413   size_t uncommitted = _shared_vs.reserved_size() - has_committed_size;</span>
 414 
 415   size_t commit =MAX2(min_bytes, preferred_bytes);
 416   commit = MIN2(commit, uncommitted);
 417   assert(commit &lt;= uncommitted, &quot;sanity&quot;);
 418 
<span class="line-modified"> 419   bool result = _shared_vs.expand_by(commit, false);</span>
<span class="line-modified"> 420   ArchivePtrMarker::expand_ptr_end((address*)_shared_vs.high());</span>


 421 
 422   if (!result) {
 423     vm_exit_during_initialization(err_msg(&quot;Failed to expand shared space to &quot; SIZE_FORMAT &quot; bytes&quot;,
 424                                           need_committed_size));
 425   }
 426 
<span class="line-modified"> 427   log_debug(cds)(&quot;Expanding shared spaces by &quot; SIZE_FORMAT_W(7) &quot; bytes [total &quot; SIZE_FORMAT_W(9)  &quot; bytes ending at %p]&quot;,</span>
<span class="line-modified"> 428                  commit, _shared_vs.actual_committed_size(), _shared_vs.high());</span>


 429 }
 430 
 431 void MetaspaceShared::initialize_ptr_marker(CHeapBitMap* ptrmap) {
 432   ArchivePtrMarker::initialize(ptrmap, (address*)_shared_vs.low(), (address*)_shared_vs.high());
 433 }
 434 
 435 // Read/write a data stream for restoring/preserving metadata pointers and
 436 // miscellaneous data from/to the shared archive file.
 437 
 438 void MetaspaceShared::serialize(SerializeClosure* soc) {
 439   int tag = 0;
 440   soc-&gt;do_tag(--tag);
 441 
 442   // Verify the sizes of various metadata in the system.
 443   soc-&gt;do_tag(sizeof(Method));
 444   soc-&gt;do_tag(sizeof(ConstMethod));
 445   soc-&gt;do_tag(arrayOopDesc::base_offset_in_bytes(T_BYTE));
 446   soc-&gt;do_tag(sizeof(ConstantPool));
 447   soc-&gt;do_tag(sizeof(ConstantPoolCache));
 448   soc-&gt;do_tag(objArrayOopDesc::base_offset_in_bytes());
</pre>
<hr />
<pre>
 491   assert(_i2i_entry_code_buffers_size == total_size, &quot;must not change&quot;);
 492   return _i2i_entry_code_buffers;
 493 }
 494 
 495 uintx MetaspaceShared::object_delta_uintx(void* obj) {
 496   Arguments::assert_is_dumping_archive();
 497   if (DumpSharedSpaces) {
 498     assert(shared_rs()-&gt;contains(obj), &quot;must be&quot;);
 499   } else {
 500     assert(is_in_shared_metaspace(obj) || DynamicArchive::is_in_target_space(obj), &quot;must be&quot;);
 501   }
 502   address base_address = address(SharedBaseAddress);
 503   uintx deltax = address(obj) - base_address;
 504   return deltax;
 505 }
 506 
 507 // Global object for holding classes that have been loaded.  Since this
 508 // is run at a safepoint just before exit, this is the entire set of classes.
 509 static GrowableArray&lt;Klass*&gt;* _global_klass_objects;
 510 




 511 GrowableArray&lt;Klass*&gt;* MetaspaceShared::collected_klasses() {
 512   return _global_klass_objects;
 513 }
 514 
 515 static void collect_array_classes(Klass* k) {
 516   _global_klass_objects-&gt;append_if_missing(k);
 517   if (k-&gt;is_array_klass()) {
 518     // Add in the array classes too
 519     ArrayKlass* ak = ArrayKlass::cast(k);
 520     Klass* h = ak-&gt;higher_dimension();
 521     if (h != NULL) {
 522       h-&gt;array_klasses_do(collect_array_classes);
 523     }
 524   }
 525 }
 526 
 527 class CollectClassesClosure : public KlassClosure {
 528   void do_klass(Klass* k) {
 529     if (k-&gt;is_instance_klass() &amp;&amp;
 530         SystemDictionaryShared::is_excluded_class(InstanceKlass::cast(k))) {
</pre>
<hr />
<pre>
1334       // allocate and shallow-copy of RO object, immediately following the RW region
1335       log_info(cds)(&quot;Allocating RO objects ... &quot;);
1336       _rw_region.pack(&amp;_ro_region);
1337 
1338       ResourceMark rm;
1339       ShallowCopier ro_copier(true);
1340       iterate_roots(&amp;ro_copier);
1341     }
1342     {
1343       log_info(cds)(&quot;Relocating embedded pointers ... &quot;);
1344       ResourceMark rm;
1345       ShallowCopyEmbeddedRefRelocator emb_reloc;
1346       iterate_roots(&amp;emb_reloc);
1347     }
1348     {
1349       log_info(cds)(&quot;Relocating external roots ... &quot;);
1350       ResourceMark rm;
1351       RefRelocator ext_reloc;
1352       iterate_roots(&amp;ext_reloc);
1353     }
<span class="line-modified">1354 </span>







1355 #ifdef ASSERT
1356     {
1357       log_info(cds)(&quot;Verifying external roots ... &quot;);
1358       ResourceMark rm;
1359       IsRefInArchiveChecker checker;
1360       iterate_roots(&amp;checker);
1361     }
1362 #endif
1363 
1364 
1365     // cleanup
1366     _ssc = NULL;
1367   }
1368 
1369   // We must relocate the System::_well_known_klasses only after we have copied the
1370   // java objects in during dump_java_heap_objects(): during the object copy, we operate on
1371   // old objects which assert that their klass is the original klass.
1372   static void relocate_well_known_klasses() {
1373     {
1374       log_info(cds)(&quot;Relocating SystemDictionary::_well_known_klasses[] ... &quot;);
1375       ResourceMark rm;
1376       RefRelocator ext_reloc;
1377       SystemDictionary::well_known_klasses_do(&amp;ext_reloc);
1378     }
1379     // NOTE: after this point, we shouldn&#39;t have any globals that can reach the old
1380     // objects.
1381 
1382     // We cannot use any of the objects in the heap anymore (except for the
1383     // shared strings) because their headers no longer point to valid Klasses.
1384   }
1385 
1386   static void iterate_roots(MetaspaceClosure* it) {















1387     GrowableArray&lt;Symbol*&gt;* symbols = _ssc-&gt;get_sorted_symbols();
1388     for (int i=0; i&lt;symbols-&gt;length(); i++) {
1389       it-&gt;push(symbols-&gt;adr_at(i));
1390     }
1391     if (_global_klass_objects != NULL) {
1392       // Need to fix up the pointers
1393       for (int i = 0; i &lt; _global_klass_objects-&gt;length(); i++) {
1394         // NOTE -- this requires that the vtable is NOT yet patched, or else we are hosed.
1395         it-&gt;push(_global_klass_objects-&gt;adr_at(i));
1396       }
1397     }
1398     FileMapInfo::metaspace_pointers_do(it, false);
1399     SystemDictionaryShared::dumptime_classes_do(it);
1400     Universe::metaspace_pointers_do(it);
1401     SymbolTable::metaspace_pointers_do(it);
1402     vmSymbols::metaspace_pointers_do(it);
1403 
1404     it-&gt;finish();
1405   }
1406 
</pre>
<hr />
<pre>
1528 
1529   FileMapInfo::check_nonempty_dir_in_shared_path_table();
1530 
1531   NOT_PRODUCT(SystemDictionary::verify();)
1532   // The following guarantee is meant to ensure that no loader constraints
1533   // exist yet, since the constraints table is not shared.  This becomes
1534   // more important now that we don&#39;t re-initialize vtables/itables for
1535   // shared classes at runtime, where constraints were previously created.
1536   guarantee(SystemDictionary::constraints()-&gt;number_of_entries() == 0,
1537             &quot;loader constraints are not saved&quot;);
1538   guarantee(SystemDictionary::placeholders()-&gt;number_of_entries() == 0,
1539           &quot;placeholders are not saved&quot;);
1540 
1541   // At this point, many classes have been loaded.
1542   // Gather systemDictionary classes in a global array and do everything to
1543   // that so we don&#39;t have to walk the SystemDictionary again.
1544   SystemDictionaryShared::check_excluded_classes();
1545   _global_klass_objects = new GrowableArray&lt;Klass*&gt;(1000);
1546   CollectClassesClosure collect_classes;
1547   ClassLoaderDataGraph::loaded_classes_do(&amp;collect_classes);

1548 
1549   print_class_stats();
1550 
1551   // Ensure the ConstMethods won&#39;t be modified at run-time
1552   log_info(cds)(&quot;Updating ConstMethods ... &quot;);
1553   rewrite_nofast_bytecodes_and_calculate_fingerprints(THREAD);
1554   log_info(cds)(&quot;done. &quot;);
1555 
1556   // Remove all references outside the metadata
1557   log_info(cds)(&quot;Removing unshareable information ... &quot;);
1558   remove_unshareable_in_classes();
1559   log_info(cds)(&quot;done. &quot;);
1560 
1561   MetaspaceShared::allocate_cloned_cpp_vtptrs();
1562   char* cloned_vtables = _mc_region.top();
1563   MetaspaceShared::allocate_cpp_vtable_clones();
1564 
1565   ArchiveCompactor::initialize();
1566   ArchiveCompactor::copy_and_compact();
1567 
1568   dump_symbols();
1569 
1570   // Dump supported java heap objects
1571   _closed_archive_heap_regions = NULL;
1572   _open_archive_heap_regions = NULL;
1573   dump_java_heap_objects();
1574 
1575   ArchiveCompactor::relocate_well_known_klasses();
1576 
1577   char* serialized_data = dump_read_only_tables();
1578   _ro_region.pack();
1579 
1580   // The vtable clones contain addresses of the current process.
<span class="line-modified">1581   // We don&#39;t want to write these addresses into the archive.</span>
1582   MetaspaceShared::zero_cpp_vtable_clones_for_writing();


1583 
1584   // relocate the data so that it can be mapped to Arguments::default_SharedBaseAddress()
1585   // without runtime relocation.
1586   relocate_to_default_base_address(&amp;ptrmap);
1587 
1588   // Create and write the archive file that maps the shared spaces.
1589 
1590   FileMapInfo* mapinfo = new FileMapInfo(true);
1591   mapinfo-&gt;populate_header(os::vm_allocation_granularity());
1592   mapinfo-&gt;set_serialized_data(serialized_data);
1593   mapinfo-&gt;set_cloned_vtables(cloned_vtables);
1594   mapinfo-&gt;set_i2i_entry_code_buffers(MetaspaceShared::i2i_entry_code_buffers(),
1595                                       MetaspaceShared::i2i_entry_code_buffers_size());
1596   mapinfo-&gt;open_for_write();
1597   MetaspaceShared::write_core_archive_regions(mapinfo, _closed_archive_heap_oopmaps, _open_archive_heap_oopmaps);
1598   _total_closed_archive_region_size = mapinfo-&gt;write_archive_heap_regions(
1599                                         _closed_archive_heap_regions,
1600                                         _closed_archive_heap_oopmaps,
1601                                         MetaspaceShared::first_closed_archive_heap_region,
1602                                         MetaspaceShared::max_closed_archive_heap_region);
</pre>
<hr />
<pre>
1634 
1635 void VM_PopulateDumpSharedSpace::print_region_stats(FileMapInfo *map_info) {
1636   // Print statistics of all the regions
1637   const size_t bitmap_used = map_info-&gt;space_at(MetaspaceShared::bm)-&gt;used();
1638   const size_t bitmap_reserved = map_info-&gt;space_at(MetaspaceShared::bm)-&gt;used_aligned();
1639   const size_t total_reserved = _ro_region.reserved()  + _rw_region.reserved() +
1640                                 _mc_region.reserved()  +
1641                                 bitmap_reserved +
1642                                 _total_closed_archive_region_size +
1643                                 _total_open_archive_region_size;
1644   const size_t total_bytes = _ro_region.used()  + _rw_region.used() +
1645                              _mc_region.used()  +
1646                              bitmap_used +
1647                              _total_closed_archive_region_size +
1648                              _total_open_archive_region_size;
1649   const double total_u_perc = percent_of(total_bytes, total_reserved);
1650 
1651   _mc_region.print(total_reserved);
1652   _rw_region.print(total_reserved);
1653   _ro_region.print(total_reserved);
<span class="line-modified">1654   print_bitmap_region_stats(bitmap_reserved, total_reserved);</span>
1655   print_heap_region_stats(_closed_archive_heap_regions, &quot;ca&quot;, total_reserved);
1656   print_heap_region_stats(_open_archive_heap_regions, &quot;oa&quot;, total_reserved);
1657 
1658   log_debug(cds)(&quot;total    : &quot; SIZE_FORMAT_W(9) &quot; [100.0%% of total] out of &quot; SIZE_FORMAT_W(9) &quot; bytes [%5.1f%% used]&quot;,
1659                  total_bytes, total_reserved, total_u_perc);
1660 }
1661 
1662 void VM_PopulateDumpSharedSpace::print_bitmap_region_stats(size_t size, size_t total_size) {
<span class="line-modified">1663   log_debug(cds)(&quot;bm  space: &quot; SIZE_FORMAT_W(9) &quot; [ %4.1f%% of total] out of &quot; SIZE_FORMAT_W(9) &quot; bytes [100.0%% used] at &quot; INTPTR_FORMAT,</span>
<span class="line-modified">1664                  size, size/double(total_size)*100.0, size, p2i(NULL));</span>
1665 }
1666 
1667 void VM_PopulateDumpSharedSpace::print_heap_region_stats(GrowableArray&lt;MemRegion&gt; *heap_mem,
1668                                                          const char *name, size_t total_size) {
1669   int arr_len = heap_mem == NULL ? 0 : heap_mem-&gt;length();
1670   for (int i = 0; i &lt; arr_len; i++) {
1671       char* start = (char*)heap_mem-&gt;at(i).start();
1672       size_t size = heap_mem-&gt;at(i).byte_size();
1673       char* top = start + size;
1674       log_debug(cds)(&quot;%s%d space: &quot; SIZE_FORMAT_W(9) &quot; [ %4.1f%% of total] out of &quot; SIZE_FORMAT_W(9) &quot; bytes [100.0%% used] at &quot; INTPTR_FORMAT,
1675                      name, i, size, size/double(total_size)*100.0, size, p2i(start));
1676 
1677   }
1678 }
1679 
1680 void MetaspaceShared::write_core_archive_regions(FileMapInfo* mapinfo,
1681                                                  GrowableArray&lt;ArchiveHeapOopmapInfo&gt;* closed_oopmaps,
1682                                                  GrowableArray&lt;ArchiveHeapOopmapInfo&gt;* open_oopmaps) {
1683   // Make sure NUM_CDS_REGIONS (exported in cds.h) agrees with
1684   // MetaspaceShared::n_regions (internal to hotspot).
</pre>
</td>
<td>
<hr />
<pre>
  62 #include &quot;oops/valueArrayKlass.hpp&quot;
  63 #include &quot;oops/valueKlass.hpp&quot;
  64 #include &quot;prims/jvmtiRedefineClasses.hpp&quot;
  65 #include &quot;runtime/handles.inline.hpp&quot;
  66 #include &quot;runtime/os.hpp&quot;
  67 #include &quot;runtime/safepointVerifiers.hpp&quot;
  68 #include &quot;runtime/signature.hpp&quot;
  69 #include &quot;runtime/timerTrace.hpp&quot;
  70 #include &quot;runtime/vmThread.hpp&quot;
  71 #include &quot;runtime/vmOperations.hpp&quot;
  72 #include &quot;utilities/align.hpp&quot;
  73 #include &quot;utilities/bitMap.inline.hpp&quot;
  74 #include &quot;utilities/defaultStream.hpp&quot;
  75 #include &quot;utilities/hashtable.inline.hpp&quot;
  76 #if INCLUDE_G1GC
  77 #include &quot;gc/g1/g1CollectedHeap.hpp&quot;
  78 #endif
  79 
  80 ReservedSpace MetaspaceShared::_shared_rs;
  81 VirtualSpace MetaspaceShared::_shared_vs;
<span class="line-added">  82 ReservedSpace MetaspaceShared::_symbol_rs;</span>
<span class="line-added">  83 VirtualSpace MetaspaceShared::_symbol_vs;</span>
  84 MetaspaceSharedStats MetaspaceShared::_stats;
  85 bool MetaspaceShared::_has_error_classes;
  86 bool MetaspaceShared::_archive_loading_failed = false;
  87 bool MetaspaceShared::_remapped_readwrite = false;
  88 address MetaspaceShared::_i2i_entry_code_buffers = NULL;
  89 size_t MetaspaceShared::_i2i_entry_code_buffers_size = 0;
  90 void* MetaspaceShared::_shared_metaspace_static_top = NULL;
  91 intx MetaspaceShared::_relocation_delta;
  92 
  93 // The CDS archive is divided into the following regions:
  94 //     mc  - misc code (the method entry trampolines, c++ vtables)
  95 //     rw  - read-write metadata
  96 //     ro  - read-only metadata and read-only tables
  97 //
  98 //     ca0 - closed archive heap space #0
  99 //     ca1 - closed archive heap space #1 (may be empty)
 100 //     oa0 - open archive heap space #0
 101 //     oa1 - open archive heap space #1 (may be empty)
 102 //
 103 // The mc, rw, and ro regions are linearly allocated, starting from
</pre>
<hr />
<pre>
 107 // These 3 regions are populated in the following steps:
 108 // [1] All classes are loaded in MetaspaceShared::preload_classes(). All metadata are
 109 //     temporarily allocated outside of the shared regions. Only the method entry
 110 //     trampolines are written into the mc region.
 111 // [2] C++ vtables are copied into the mc region.
 112 // [3] ArchiveCompactor copies RW metadata into the rw region.
 113 // [4] ArchiveCompactor copies RO metadata into the ro region.
 114 // [5] SymbolTable, StringTable, SystemDictionary, and a few other read-only data
 115 //     are copied into the ro region as read-only tables.
 116 //
 117 // The s0/s1 and oa0/oa1 regions are populated inside HeapShared::archive_java_heap_objects.
 118 // Their layout is independent of the other 4 regions.
 119 
 120 char* DumpRegion::expand_top_to(char* newtop) {
 121   assert(is_allocatable(), &quot;must be initialized and not packed&quot;);
 122   assert(newtop &gt;= _top, &quot;must not grow backwards&quot;);
 123   if (newtop &gt; _end) {
 124     MetaspaceShared::report_out_of_space(_name, newtop - _top);
 125     ShouldNotReachHere();
 126   }
<span class="line-modified"> 127 </span>
<span class="line-modified"> 128   if (_rs == MetaspaceShared::shared_rs()) {</span>
<span class="line-modified"> 129     uintx delta;</span>
<span class="line-modified"> 130     if (DynamicDumpSharedSpaces) {</span>
<span class="line-modified"> 131       delta = DynamicArchive::object_delta_uintx(newtop);</span>
<span class="line-modified"> 132     } else {</span>
<span class="line-modified"> 133       delta = MetaspaceShared::object_delta_uintx(newtop);</span>
<span class="line-modified"> 134     }</span>
<span class="line-modified"> 135     if (delta &gt; MAX_SHARED_DELTA) {</span>
<span class="line-modified"> 136       // This is just a sanity check and should not appear in any real world usage. This</span>
<span class="line-modified"> 137       // happens only if you allocate more than 2GB of shared objects and would require</span>
<span class="line-modified"> 138       // millions of shared classes.</span>
<span class="line-added"> 139       vm_exit_during_initialization(&quot;Out of memory in the CDS archive&quot;,</span>
<span class="line-added"> 140                                     &quot;Please reduce the number of shared classes.&quot;);</span>
<span class="line-added"> 141     }</span>
 142   }
 143 
<span class="line-modified"> 144   MetaspaceShared::commit_to(_rs, _vs, newtop);</span>
 145   _top = newtop;
 146   return _top;
 147 }
 148 
 149 char* DumpRegion::allocate(size_t num_bytes, size_t alignment) {
 150   char* p = (char*)align_up(_top, alignment);
 151   char* newtop = p + align_up(num_bytes, alignment);
 152   expand_top_to(newtop);
 153   memset(p, 0, newtop - p);
 154   return p;
 155 }
 156 
 157 void DumpRegion::append_intptr_t(intptr_t n, bool need_to_mark) {
 158   assert(is_aligned(_top, sizeof(intptr_t)), &quot;bad alignment&quot;);
 159   intptr_t *p = (intptr_t*)_top;
 160   char* newtop = _top + sizeof(intptr_t);
 161   expand_top_to(newtop);
 162   *p = n;
 163   if (need_to_mark) {
 164     ArchivePtrMarker::mark_pointer(p);
 165   }
 166 }
 167 
 168 void DumpRegion::print(size_t total_bytes) const {
 169   log_debug(cds)(&quot;%-3s space: &quot; SIZE_FORMAT_W(9) &quot; [ %4.1f%% of total] out of &quot; SIZE_FORMAT_W(9) &quot; bytes [%5.1f%% used] at &quot; INTPTR_FORMAT,
 170                  _name, used(), percent_of(used(), total_bytes), reserved(), percent_of(used(), reserved()),
 171                  p2i(_base + MetaspaceShared::final_delta()));
 172 }
 173 
 174 void DumpRegion::print_out_of_space_msg(const char* failing_region, size_t needed_bytes) {
 175   log_error(cds)(&quot;[%-8s] &quot; PTR_FORMAT &quot; - &quot; PTR_FORMAT &quot; capacity =%9d, allocated =%9d&quot;,
 176                  _name, p2i(_base), p2i(_top), int(_end - _base), int(_top - _base));
 177   if (strcmp(_name, failing_region) == 0) {
 178     log_error(cds)(&quot; required = %d&quot;, int(needed_bytes));
 179   }
 180 }
 181 
<span class="line-added"> 182 void DumpRegion::init(ReservedSpace* rs, VirtualSpace* vs) {</span>
<span class="line-added"> 183   _rs = rs;</span>
<span class="line-added"> 184   _vs = vs;</span>
<span class="line-added"> 185   // Start with 0 committed bytes. The memory will be committed as needed by</span>
<span class="line-added"> 186   // MetaspaceShared::commit_to().</span>
<span class="line-added"> 187   if (!_vs-&gt;initialize(*_rs, 0)) {</span>
<span class="line-added"> 188     fatal(&quot;Unable to allocate memory for shared space&quot;);</span>
<span class="line-added"> 189   }</span>
<span class="line-added"> 190   _base = _top = _rs-&gt;base();</span>
<span class="line-added"> 191   _end = _rs-&gt;end();</span>
<span class="line-added"> 192 }</span>
<span class="line-added"> 193 </span>
 194 void DumpRegion::pack(DumpRegion* next) {
 195   assert(!is_packed(), &quot;sanity&quot;);
 196   _end = (char*)align_up(_top, Metaspace::reserve_alignment());
 197   _is_packed = true;
 198   if (next != NULL) {
<span class="line-added"> 199     next-&gt;_rs = _rs;</span>
<span class="line-added"> 200     next-&gt;_vs = _vs;</span>
 201     next-&gt;_base = next-&gt;_top = this-&gt;_end;
<span class="line-modified"> 202     next-&gt;_end = _rs-&gt;end();</span>
 203   }
 204 }
 205 
<span class="line-modified"> 206 static DumpRegion _mc_region(&quot;mc&quot;), _ro_region(&quot;ro&quot;), _rw_region(&quot;rw&quot;), _symbol_region(&quot;symbols&quot;);</span>
 207 static size_t _total_closed_archive_region_size = 0, _total_open_archive_region_size = 0;
 208 
<span class="line-modified"> 209 void MetaspaceShared::init_shared_dump_space(DumpRegion* first_space) {</span>
<span class="line-modified"> 210   first_space-&gt;init(&amp;_shared_rs, &amp;_shared_vs);</span>





 211 }
 212 
 213 DumpRegion* MetaspaceShared::misc_code_dump_space() {
 214   return &amp;_mc_region;
 215 }
 216 
 217 DumpRegion* MetaspaceShared::read_write_dump_space() {
 218   return &amp;_rw_region;
 219 }
 220 
 221 DumpRegion* MetaspaceShared::read_only_dump_space() {
 222   return &amp;_ro_region;
 223 }
 224 
 225 void MetaspaceShared::pack_dump_space(DumpRegion* current, DumpRegion* next,
 226                                       ReservedSpace* rs) {
 227   current-&gt;pack(next);
 228 }
 229 
<span class="line-added"> 230 char* MetaspaceShared::symbol_space_alloc(size_t num_bytes) {</span>
<span class="line-added"> 231   return _symbol_region.allocate(num_bytes);</span>
<span class="line-added"> 232 }</span>
<span class="line-added"> 233 </span>
 234 char* MetaspaceShared::misc_code_space_alloc(size_t num_bytes) {
 235   return _mc_region.allocate(num_bytes);
 236 }
 237 
 238 char* MetaspaceShared::read_only_space_alloc(size_t num_bytes) {
 239   return _ro_region.allocate(num_bytes);
 240 }
 241 
 242 // When reserving an address range using ReservedSpace, we need an alignment that satisfies both:
 243 // os::vm_allocation_granularity() -- so that we can sub-divide this range into multiple mmap regions,
 244 //                                    while keeping the first range at offset 0 of this range.
 245 // Metaspace::reserve_alignment()  -- so we can pass the region to
 246 //                                    Metaspace::allocate_metaspace_compressed_klass_ptrs.
 247 size_t MetaspaceShared::reserved_space_alignment() {
 248   size_t os_align = os::vm_allocation_granularity();
 249   size_t ms_align = Metaspace::reserve_alignment();
 250   if (os_align &gt;= ms_align) {
 251     assert(os_align % ms_align == 0, &quot;must be a multiple&quot;);
 252     return os_align;
 253   } else {
</pre>
<hr />
<pre>
 323     // Set up compress class pointers.
 324     CompressedKlassPointers::set_base((address)_shared_rs.base());
 325     // Set narrow_klass_shift to be LogKlassAlignmentInBytes. This is consistent
 326     // with AOT.
 327     CompressedKlassPointers::set_shift(LogKlassAlignmentInBytes);
 328     // Set the range of klass addresses to 4GB.
 329     CompressedKlassPointers::set_range(cds_total);
 330     Metaspace::initialize_class_space(tmp_class_space);
 331   }
 332   log_info(cds)(&quot;narrow_klass_base = &quot; PTR_FORMAT &quot;, narrow_klass_shift = %d&quot;,
 333                 p2i(CompressedKlassPointers::base()), CompressedKlassPointers::shift());
 334 
 335   log_info(cds)(&quot;Allocated temporary class space: &quot; SIZE_FORMAT &quot; bytes at &quot; PTR_FORMAT,
 336                 CompressedClassSpaceSize, p2i(tmp_class_space.base()));
 337 #endif
 338 
 339   init_shared_dump_space(&amp;_mc_region);
 340   SharedBaseAddress = (size_t)_shared_rs.base();
 341   log_info(cds)(&quot;Allocated shared space: &quot; SIZE_FORMAT &quot; bytes at &quot; PTR_FORMAT,
 342                 _shared_rs.size(), p2i(_shared_rs.base()));
<span class="line-added"> 343 </span>
<span class="line-added"> 344   size_t symbol_rs_size = LP64_ONLY(3 * G) NOT_LP64(128 * M);</span>
<span class="line-added"> 345   _symbol_rs = ReservedSpace(symbol_rs_size);</span>
<span class="line-added"> 346   if (!_symbol_rs.is_reserved()) {</span>
<span class="line-added"> 347     vm_exit_during_initialization(&quot;Unable to reserve memory for symbols&quot;,</span>
<span class="line-added"> 348                                   err_msg(SIZE_FORMAT &quot; bytes.&quot;, symbol_rs_size));</span>
<span class="line-added"> 349   }</span>
<span class="line-added"> 350   _symbol_region.init(&amp;_symbol_rs, &amp;_symbol_vs);</span>
 351 }
 352 
 353 // Called by universe_post_init()
 354 void MetaspaceShared::post_initialize(TRAPS) {
 355   if (UseSharedSpaces) {
 356     int size = FileMapInfo::get_number_of_shared_paths();
 357     if (size &gt; 0) {
 358       SystemDictionaryShared::allocate_shared_data_arrays(size, THREAD);
 359       if (!DynamicDumpSharedSpaces) {
 360         FileMapInfo* info;
 361         if (FileMapInfo::dynamic_info() == NULL) {
 362           info = FileMapInfo::current_info();
 363         } else {
 364           info = FileMapInfo::dynamic_info();
 365         }
 366         ClassLoaderExt::init_paths_start_index(info-&gt;app_class_paths_start_index());
 367         ClassLoaderExt::init_app_module_paths_start_index(info-&gt;app_module_paths_start_index());
 368       }
 369     }
 370   }
</pre>
<hr />
<pre>
 408           const HeapRegion* hr = G1CollectedHeap::heap()-&gt;heap_region_containing(body);
 409           if (hr-&gt;is_humongous()) {
 410             // Don&#39;t keep it alive, so it will be GC&#39;ed before we dump the strings, in order
 411             // to maximize free heap space and minimize fragmentation.
 412             log_warning(cds, heap)(&quot;[line %d] extra interned string ignored; size too large: %d&quot;,
 413                                 reader.last_line_no(), utf8_length);
 414             continue;
 415           }
 416         }
 417 #endif
 418         // Interned strings are GC&#39;ed if there are no references to it, so let&#39;s
 419         // add a reference to keep this string alive.
 420         assert(s != NULL, &quot;must succeed&quot;);
 421         Handle h(THREAD, s);
 422         _extra_interned_strings-&gt;append(h);
 423       }
 424     }
 425   }
 426 }
 427 
<span class="line-modified"> 428 void MetaspaceShared::commit_to(ReservedSpace* rs, VirtualSpace* vs, char* newtop) {</span>
 429   Arguments::assert_is_dumping_archive();
<span class="line-modified"> 430   char* base = rs-&gt;base();</span>
 431   size_t need_committed_size = newtop - base;
<span class="line-modified"> 432   size_t has_committed_size = vs-&gt;committed_size();</span>
 433   if (need_committed_size &lt; has_committed_size) {
 434     return;
 435   }
 436 
 437   size_t min_bytes = need_committed_size - has_committed_size;
 438   size_t preferred_bytes = 1 * M;
<span class="line-modified"> 439   size_t uncommitted = vs-&gt;reserved_size() - has_committed_size;</span>
 440 
 441   size_t commit =MAX2(min_bytes, preferred_bytes);
 442   commit = MIN2(commit, uncommitted);
 443   assert(commit &lt;= uncommitted, &quot;sanity&quot;);
 444 
<span class="line-modified"> 445   bool result = vs-&gt;expand_by(commit, false);</span>
<span class="line-modified"> 446   if (rs == &amp;_shared_rs) {</span>
<span class="line-added"> 447     ArchivePtrMarker::expand_ptr_end((address*)vs-&gt;high());</span>
<span class="line-added"> 448   }</span>
 449 
 450   if (!result) {
 451     vm_exit_during_initialization(err_msg(&quot;Failed to expand shared space to &quot; SIZE_FORMAT &quot; bytes&quot;,
 452                                           need_committed_size));
 453   }
 454 
<span class="line-modified"> 455   assert(rs == &amp;_shared_rs || rs == &amp;_symbol_rs, &quot;must be&quot;);</span>
<span class="line-modified"> 456   const char* which = (rs == &amp;_shared_rs) ? &quot;shared&quot; : &quot;symbol&quot;;</span>
<span class="line-added"> 457   log_debug(cds)(&quot;Expanding %s spaces by &quot; SIZE_FORMAT_W(7) &quot; bytes [total &quot; SIZE_FORMAT_W(9)  &quot; bytes ending at %p]&quot;,</span>
<span class="line-added"> 458                  which, commit, vs-&gt;actual_committed_size(), vs-&gt;high());</span>
 459 }
 460 
 461 void MetaspaceShared::initialize_ptr_marker(CHeapBitMap* ptrmap) {
 462   ArchivePtrMarker::initialize(ptrmap, (address*)_shared_vs.low(), (address*)_shared_vs.high());
 463 }
 464 
 465 // Read/write a data stream for restoring/preserving metadata pointers and
 466 // miscellaneous data from/to the shared archive file.
 467 
 468 void MetaspaceShared::serialize(SerializeClosure* soc) {
 469   int tag = 0;
 470   soc-&gt;do_tag(--tag);
 471 
 472   // Verify the sizes of various metadata in the system.
 473   soc-&gt;do_tag(sizeof(Method));
 474   soc-&gt;do_tag(sizeof(ConstMethod));
 475   soc-&gt;do_tag(arrayOopDesc::base_offset_in_bytes(T_BYTE));
 476   soc-&gt;do_tag(sizeof(ConstantPool));
 477   soc-&gt;do_tag(sizeof(ConstantPoolCache));
 478   soc-&gt;do_tag(objArrayOopDesc::base_offset_in_bytes());
</pre>
<hr />
<pre>
 521   assert(_i2i_entry_code_buffers_size == total_size, &quot;must not change&quot;);
 522   return _i2i_entry_code_buffers;
 523 }
 524 
 525 uintx MetaspaceShared::object_delta_uintx(void* obj) {
 526   Arguments::assert_is_dumping_archive();
 527   if (DumpSharedSpaces) {
 528     assert(shared_rs()-&gt;contains(obj), &quot;must be&quot;);
 529   } else {
 530     assert(is_in_shared_metaspace(obj) || DynamicArchive::is_in_target_space(obj), &quot;must be&quot;);
 531   }
 532   address base_address = address(SharedBaseAddress);
 533   uintx deltax = address(obj) - base_address;
 534   return deltax;
 535 }
 536 
 537 // Global object for holding classes that have been loaded.  Since this
 538 // is run at a safepoint just before exit, this is the entire set of classes.
 539 static GrowableArray&lt;Klass*&gt;* _global_klass_objects;
 540 
<span class="line-added"> 541 static int global_klass_compare(Klass** a, Klass **b) {</span>
<span class="line-added"> 542   return a[0]-&gt;name()-&gt;fast_compare(b[0]-&gt;name());</span>
<span class="line-added"> 543 }</span>
<span class="line-added"> 544 </span>
 545 GrowableArray&lt;Klass*&gt;* MetaspaceShared::collected_klasses() {
 546   return _global_klass_objects;
 547 }
 548 
 549 static void collect_array_classes(Klass* k) {
 550   _global_klass_objects-&gt;append_if_missing(k);
 551   if (k-&gt;is_array_klass()) {
 552     // Add in the array classes too
 553     ArrayKlass* ak = ArrayKlass::cast(k);
 554     Klass* h = ak-&gt;higher_dimension();
 555     if (h != NULL) {
 556       h-&gt;array_klasses_do(collect_array_classes);
 557     }
 558   }
 559 }
 560 
 561 class CollectClassesClosure : public KlassClosure {
 562   void do_klass(Klass* k) {
 563     if (k-&gt;is_instance_klass() &amp;&amp;
 564         SystemDictionaryShared::is_excluded_class(InstanceKlass::cast(k))) {
</pre>
<hr />
<pre>
1368       // allocate and shallow-copy of RO object, immediately following the RW region
1369       log_info(cds)(&quot;Allocating RO objects ... &quot;);
1370       _rw_region.pack(&amp;_ro_region);
1371 
1372       ResourceMark rm;
1373       ShallowCopier ro_copier(true);
1374       iterate_roots(&amp;ro_copier);
1375     }
1376     {
1377       log_info(cds)(&quot;Relocating embedded pointers ... &quot;);
1378       ResourceMark rm;
1379       ShallowCopyEmbeddedRefRelocator emb_reloc;
1380       iterate_roots(&amp;emb_reloc);
1381     }
1382     {
1383       log_info(cds)(&quot;Relocating external roots ... &quot;);
1384       ResourceMark rm;
1385       RefRelocator ext_reloc;
1386       iterate_roots(&amp;ext_reloc);
1387     }
<span class="line-modified">1388     {</span>
<span class="line-added">1389       log_info(cds)(&quot;Fixing symbol identity hash ... &quot;);</span>
<span class="line-added">1390       os::init_random(0x12345678);</span>
<span class="line-added">1391       GrowableArray&lt;Symbol*&gt;* symbols = _ssc-&gt;get_sorted_symbols();</span>
<span class="line-added">1392       for (int i=0; i&lt;symbols-&gt;length(); i++) {</span>
<span class="line-added">1393         symbols-&gt;at(i)-&gt;update_identity_hash();</span>
<span class="line-added">1394       }</span>
<span class="line-added">1395     }</span>
1396 #ifdef ASSERT
1397     {
1398       log_info(cds)(&quot;Verifying external roots ... &quot;);
1399       ResourceMark rm;
1400       IsRefInArchiveChecker checker;
1401       iterate_roots(&amp;checker);
1402     }
1403 #endif
1404 
1405 
1406     // cleanup
1407     _ssc = NULL;
1408   }
1409 
1410   // We must relocate the System::_well_known_klasses only after we have copied the
1411   // java objects in during dump_java_heap_objects(): during the object copy, we operate on
1412   // old objects which assert that their klass is the original klass.
1413   static void relocate_well_known_klasses() {
1414     {
1415       log_info(cds)(&quot;Relocating SystemDictionary::_well_known_klasses[] ... &quot;);
1416       ResourceMark rm;
1417       RefRelocator ext_reloc;
1418       SystemDictionary::well_known_klasses_do(&amp;ext_reloc);
1419     }
1420     // NOTE: after this point, we shouldn&#39;t have any globals that can reach the old
1421     // objects.
1422 
1423     // We cannot use any of the objects in the heap anymore (except for the
1424     // shared strings) because their headers no longer point to valid Klasses.
1425   }
1426 
1427   static void iterate_roots(MetaspaceClosure* it) {
<span class="line-added">1428     // To ensure deterministic contents in the archive, we just need to ensure that</span>
<span class="line-added">1429     // we iterate the MetsapceObjs in a deterministic order. It doesn&#39;t matter where</span>
<span class="line-added">1430     // the MetsapceObjs are located originally, as they are copied sequentially into</span>
<span class="line-added">1431     // the archive during the iteration.</span>
<span class="line-added">1432     //</span>
<span class="line-added">1433     // The only issue here is that the symbol table and the system directories may be</span>
<span class="line-added">1434     // randomly ordered, so we copy the symbols and klasses into two arrays and sort</span>
<span class="line-added">1435     // them deterministically.</span>
<span class="line-added">1436     //</span>
<span class="line-added">1437     // During -Xshare:dump, the order of Symbol creation is strictly determined by</span>
<span class="line-added">1438     // the SharedClassListFile (class loading is done in a single thread and the JIT</span>
<span class="line-added">1439     // is disabled). Also, Symbols are allocated in monotonically increasing addresses</span>
<span class="line-added">1440     // (see Symbol::operator new(size_t, int)). So if we iterate the Symbols by</span>
<span class="line-added">1441     // ascending address order, we ensure that all Symbols are copied into deterministic</span>
<span class="line-added">1442     // locations in the archive.</span>
1443     GrowableArray&lt;Symbol*&gt;* symbols = _ssc-&gt;get_sorted_symbols();
1444     for (int i=0; i&lt;symbols-&gt;length(); i++) {
1445       it-&gt;push(symbols-&gt;adr_at(i));
1446     }
1447     if (_global_klass_objects != NULL) {
1448       // Need to fix up the pointers
1449       for (int i = 0; i &lt; _global_klass_objects-&gt;length(); i++) {
1450         // NOTE -- this requires that the vtable is NOT yet patched, or else we are hosed.
1451         it-&gt;push(_global_klass_objects-&gt;adr_at(i));
1452       }
1453     }
1454     FileMapInfo::metaspace_pointers_do(it, false);
1455     SystemDictionaryShared::dumptime_classes_do(it);
1456     Universe::metaspace_pointers_do(it);
1457     SymbolTable::metaspace_pointers_do(it);
1458     vmSymbols::metaspace_pointers_do(it);
1459 
1460     it-&gt;finish();
1461   }
1462 
</pre>
<hr />
<pre>
1584 
1585   FileMapInfo::check_nonempty_dir_in_shared_path_table();
1586 
1587   NOT_PRODUCT(SystemDictionary::verify();)
1588   // The following guarantee is meant to ensure that no loader constraints
1589   // exist yet, since the constraints table is not shared.  This becomes
1590   // more important now that we don&#39;t re-initialize vtables/itables for
1591   // shared classes at runtime, where constraints were previously created.
1592   guarantee(SystemDictionary::constraints()-&gt;number_of_entries() == 0,
1593             &quot;loader constraints are not saved&quot;);
1594   guarantee(SystemDictionary::placeholders()-&gt;number_of_entries() == 0,
1595           &quot;placeholders are not saved&quot;);
1596 
1597   // At this point, many classes have been loaded.
1598   // Gather systemDictionary classes in a global array and do everything to
1599   // that so we don&#39;t have to walk the SystemDictionary again.
1600   SystemDictionaryShared::check_excluded_classes();
1601   _global_klass_objects = new GrowableArray&lt;Klass*&gt;(1000);
1602   CollectClassesClosure collect_classes;
1603   ClassLoaderDataGraph::loaded_classes_do(&amp;collect_classes);
<span class="line-added">1604   _global_klass_objects-&gt;sort(global_klass_compare);</span>
1605 
1606   print_class_stats();
1607 
1608   // Ensure the ConstMethods won&#39;t be modified at run-time
1609   log_info(cds)(&quot;Updating ConstMethods ... &quot;);
1610   rewrite_nofast_bytecodes_and_calculate_fingerprints(THREAD);
1611   log_info(cds)(&quot;done. &quot;);
1612 
1613   // Remove all references outside the metadata
1614   log_info(cds)(&quot;Removing unshareable information ... &quot;);
1615   remove_unshareable_in_classes();
1616   log_info(cds)(&quot;done. &quot;);
1617 
1618   MetaspaceShared::allocate_cloned_cpp_vtptrs();
1619   char* cloned_vtables = _mc_region.top();
1620   MetaspaceShared::allocate_cpp_vtable_clones();
1621 
1622   ArchiveCompactor::initialize();
1623   ArchiveCompactor::copy_and_compact();
1624 
1625   dump_symbols();
1626 
1627   // Dump supported java heap objects
1628   _closed_archive_heap_regions = NULL;
1629   _open_archive_heap_regions = NULL;
1630   dump_java_heap_objects();
1631 
1632   ArchiveCompactor::relocate_well_known_klasses();
1633 
1634   char* serialized_data = dump_read_only_tables();
1635   _ro_region.pack();
1636 
1637   // The vtable clones contain addresses of the current process.
<span class="line-modified">1638   // We don&#39;t want to write these addresses into the archive. Same for i2i buffer.</span>
1639   MetaspaceShared::zero_cpp_vtable_clones_for_writing();
<span class="line-added">1640   memset(MetaspaceShared::i2i_entry_code_buffers(), 0,</span>
<span class="line-added">1641          MetaspaceShared::i2i_entry_code_buffers_size());</span>
1642 
1643   // relocate the data so that it can be mapped to Arguments::default_SharedBaseAddress()
1644   // without runtime relocation.
1645   relocate_to_default_base_address(&amp;ptrmap);
1646 
1647   // Create and write the archive file that maps the shared spaces.
1648 
1649   FileMapInfo* mapinfo = new FileMapInfo(true);
1650   mapinfo-&gt;populate_header(os::vm_allocation_granularity());
1651   mapinfo-&gt;set_serialized_data(serialized_data);
1652   mapinfo-&gt;set_cloned_vtables(cloned_vtables);
1653   mapinfo-&gt;set_i2i_entry_code_buffers(MetaspaceShared::i2i_entry_code_buffers(),
1654                                       MetaspaceShared::i2i_entry_code_buffers_size());
1655   mapinfo-&gt;open_for_write();
1656   MetaspaceShared::write_core_archive_regions(mapinfo, _closed_archive_heap_oopmaps, _open_archive_heap_oopmaps);
1657   _total_closed_archive_region_size = mapinfo-&gt;write_archive_heap_regions(
1658                                         _closed_archive_heap_regions,
1659                                         _closed_archive_heap_oopmaps,
1660                                         MetaspaceShared::first_closed_archive_heap_region,
1661                                         MetaspaceShared::max_closed_archive_heap_region);
</pre>
<hr />
<pre>
1693 
1694 void VM_PopulateDumpSharedSpace::print_region_stats(FileMapInfo *map_info) {
1695   // Print statistics of all the regions
1696   const size_t bitmap_used = map_info-&gt;space_at(MetaspaceShared::bm)-&gt;used();
1697   const size_t bitmap_reserved = map_info-&gt;space_at(MetaspaceShared::bm)-&gt;used_aligned();
1698   const size_t total_reserved = _ro_region.reserved()  + _rw_region.reserved() +
1699                                 _mc_region.reserved()  +
1700                                 bitmap_reserved +
1701                                 _total_closed_archive_region_size +
1702                                 _total_open_archive_region_size;
1703   const size_t total_bytes = _ro_region.used()  + _rw_region.used() +
1704                              _mc_region.used()  +
1705                              bitmap_used +
1706                              _total_closed_archive_region_size +
1707                              _total_open_archive_region_size;
1708   const double total_u_perc = percent_of(total_bytes, total_reserved);
1709 
1710   _mc_region.print(total_reserved);
1711   _rw_region.print(total_reserved);
1712   _ro_region.print(total_reserved);
<span class="line-modified">1713   print_bitmap_region_stats(bitmap_used, total_reserved);</span>
1714   print_heap_region_stats(_closed_archive_heap_regions, &quot;ca&quot;, total_reserved);
1715   print_heap_region_stats(_open_archive_heap_regions, &quot;oa&quot;, total_reserved);
1716 
1717   log_debug(cds)(&quot;total    : &quot; SIZE_FORMAT_W(9) &quot; [100.0%% of total] out of &quot; SIZE_FORMAT_W(9) &quot; bytes [%5.1f%% used]&quot;,
1718                  total_bytes, total_reserved, total_u_perc);
1719 }
1720 
1721 void VM_PopulateDumpSharedSpace::print_bitmap_region_stats(size_t size, size_t total_size) {
<span class="line-modified">1722   log_debug(cds)(&quot;bm  space: &quot; SIZE_FORMAT_W(9) &quot; [ %4.1f%% of total] out of &quot; SIZE_FORMAT_W(9) &quot; bytes [100.0%% used]&quot;,</span>
<span class="line-modified">1723                  size, size/double(total_size)*100.0, size);</span>
1724 }
1725 
1726 void VM_PopulateDumpSharedSpace::print_heap_region_stats(GrowableArray&lt;MemRegion&gt; *heap_mem,
1727                                                          const char *name, size_t total_size) {
1728   int arr_len = heap_mem == NULL ? 0 : heap_mem-&gt;length();
1729   for (int i = 0; i &lt; arr_len; i++) {
1730       char* start = (char*)heap_mem-&gt;at(i).start();
1731       size_t size = heap_mem-&gt;at(i).byte_size();
1732       char* top = start + size;
1733       log_debug(cds)(&quot;%s%d space: &quot; SIZE_FORMAT_W(9) &quot; [ %4.1f%% of total] out of &quot; SIZE_FORMAT_W(9) &quot; bytes [100.0%% used] at &quot; INTPTR_FORMAT,
1734                      name, i, size, size/double(total_size)*100.0, size, p2i(start));
1735 
1736   }
1737 }
1738 
1739 void MetaspaceShared::write_core_archive_regions(FileMapInfo* mapinfo,
1740                                                  GrowableArray&lt;ArchiveHeapOopmapInfo&gt;* closed_oopmaps,
1741                                                  GrowableArray&lt;ArchiveHeapOopmapInfo&gt;* open_oopmaps) {
1742   // Make sure NUM_CDS_REGIONS (exported in cds.h) agrees with
1743   // MetaspaceShared::n_regions (internal to hotspot).
</pre>
</td>
</tr>
</table>
<center><a href="dynamicArchive.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="../oops/arrayKlass.cpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>