<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Old src/hotspot/share/memory/heapShared.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
  <body>
    <pre>
   1 /*
   2  * Copyright (c) 2018, 2020, Oracle and/or its affiliates. All rights reserved.
   3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   4  *
   5  * This code is free software; you can redistribute it and/or modify it
   6  * under the terms of the GNU General Public License version 2 only, as
   7  * published by the Free Software Foundation.
   8  *
   9  * This code is distributed in the hope that it will be useful, but WITHOUT
  10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #include &quot;precompiled.hpp&quot;
  26 #include &quot;classfile/javaClasses.inline.hpp&quot;
  27 #include &quot;classfile/stringTable.hpp&quot;
  28 #include &quot;classfile/symbolTable.hpp&quot;
  29 #include &quot;classfile/systemDictionaryShared.hpp&quot;
  30 #include &quot;classfile/vmSymbols.hpp&quot;
  31 #include &quot;logging/log.hpp&quot;
  32 #include &quot;logging/logMessage.hpp&quot;
  33 #include &quot;logging/logStream.hpp&quot;
  34 #include &quot;memory/archiveUtils.hpp&quot;
  35 #include &quot;memory/filemap.hpp&quot;
  36 #include &quot;memory/heapShared.inline.hpp&quot;
  37 #include &quot;memory/iterator.inline.hpp&quot;
  38 #include &quot;memory/metadataFactory.hpp&quot;
  39 #include &quot;memory/metaspaceClosure.hpp&quot;
  40 #include &quot;memory/metaspaceShared.hpp&quot;
  41 #include &quot;memory/resourceArea.hpp&quot;
  42 #include &quot;memory/universe.hpp&quot;
  43 #include &quot;oops/compressedOops.inline.hpp&quot;
  44 #include &quot;oops/fieldStreams.inline.hpp&quot;
  45 #include &quot;oops/oop.inline.hpp&quot;
  46 #include &quot;runtime/fieldDescriptor.inline.hpp&quot;
  47 #include &quot;runtime/safepointVerifiers.hpp&quot;
  48 #include &quot;utilities/bitMap.inline.hpp&quot;
  49 #if INCLUDE_G1GC
  50 #include &quot;gc/g1/g1CollectedHeap.hpp&quot;
  51 #endif
  52 
  53 #if INCLUDE_CDS_JAVA_HEAP
  54 
  55 bool HeapShared::_closed_archive_heap_region_mapped = false;
  56 bool HeapShared::_open_archive_heap_region_mapped = false;
  57 bool HeapShared::_archive_heap_region_fixed = false;
  58 
  59 address   HeapShared::_narrow_oop_base;
  60 int       HeapShared::_narrow_oop_shift;
  61 
  62 //
  63 // If you add new entries to the following tables, you should know what you&#39;re doing!
  64 //
  65 
  66 // Entry fields for shareable subgraphs archived in the closed archive heap
  67 // region. Warning: Objects in the subgraphs should not have reference fields
  68 // assigned at runtime.
  69 static ArchivableStaticFieldInfo closed_archive_subgraph_entry_fields[] = {
  70   {&quot;java/lang/Integer$IntegerCache&quot;,           &quot;archivedCache&quot;},
  71   {&quot;java/lang/Long$LongCache&quot;,                 &quot;archivedCache&quot;},
  72   {&quot;java/lang/Byte$ByteCache&quot;,                 &quot;archivedCache&quot;},
  73   {&quot;java/lang/Short$ShortCache&quot;,               &quot;archivedCache&quot;},
  74   {&quot;java/lang/Character$CharacterCache&quot;,       &quot;archivedCache&quot;},
  75   {&quot;java/util/jar/Attributes$Name&quot;,            &quot;KNOWN_NAMES&quot;},
  76   {&quot;sun/util/locale/BaseLocale&quot;,               &quot;constantBaseLocales&quot;},
  77 };
  78 // Entry fields for subgraphs archived in the open archive heap region.
  79 static ArchivableStaticFieldInfo open_archive_subgraph_entry_fields[] = {
  80   {&quot;jdk/internal/module/ArchivedModuleGraph&quot;,  &quot;archivedModuleGraph&quot;},
  81   {&quot;java/util/ImmutableCollections&quot;,           &quot;archivedObjects&quot;},
  82   {&quot;java/lang/module/Configuration&quot;,           &quot;EMPTY_CONFIGURATION&quot;},
  83   {&quot;jdk/internal/math/FDBigInteger&quot;,           &quot;archivedCaches&quot;},
  84 };
  85 
  86 const static int num_closed_archive_subgraph_entry_fields =
  87   sizeof(closed_archive_subgraph_entry_fields) / sizeof(ArchivableStaticFieldInfo);
  88 const static int num_open_archive_subgraph_entry_fields =
  89   sizeof(open_archive_subgraph_entry_fields) / sizeof(ArchivableStaticFieldInfo);
  90 
  91 ////////////////////////////////////////////////////////////////
  92 //
  93 // Java heap object archiving support
  94 //
  95 ////////////////////////////////////////////////////////////////
  96 void HeapShared::fixup_mapped_heap_regions() {
  97   FileMapInfo *mapinfo = FileMapInfo::current_info();
  98   mapinfo-&gt;fixup_mapped_heap_regions();
  99   set_archive_heap_region_fixed();
 100   SystemDictionaryShared::update_archived_mirror_native_pointers();
 101 }
 102 
 103 unsigned HeapShared::oop_hash(oop const&amp; p) {
 104   assert(!p-&gt;mark().has_bias_pattern(),
 105          &quot;this object should never have been locked&quot;);  // so identity_hash won&#39;t safepoin
 106   unsigned hash = (unsigned)p-&gt;identity_hash();
 107   return hash;
 108 }
 109 
 110 HeapShared::ArchivedObjectCache* HeapShared::_archived_object_cache = NULL;
 111 oop HeapShared::find_archived_heap_object(oop obj) {
 112   assert(DumpSharedSpaces, &quot;dump-time only&quot;);
 113   ArchivedObjectCache* cache = archived_object_cache();
 114   oop* p = cache-&gt;get(obj);
 115   if (p != NULL) {
 116     return *p;
 117   } else {
 118     return NULL;
 119   }
 120 }
 121 
 122 oop HeapShared::archive_heap_object(oop obj, Thread* THREAD) {
 123   assert(DumpSharedSpaces, &quot;dump-time only&quot;);
 124 
 125   oop ao = find_archived_heap_object(obj);
 126   if (ao != NULL) {
 127     // already archived
 128     return ao;
 129   }
 130 
 131   int len = obj-&gt;size();
 132   if (G1CollectedHeap::heap()-&gt;is_archive_alloc_too_large(len)) {
 133     log_debug(cds, heap)(&quot;Cannot archive, object (&quot; PTR_FORMAT &quot;) is too large: &quot; SIZE_FORMAT,
 134                          p2i(obj), (size_t)obj-&gt;size());
 135     return NULL;
 136   }
 137 
 138   // Pre-compute object identity hash at CDS dump time.
 139   obj-&gt;identity_hash();
 140 
 141   oop archived_oop = (oop)G1CollectedHeap::heap()-&gt;archive_mem_allocate(len);
 142   if (archived_oop != NULL) {
 143     Copy::aligned_disjoint_words(cast_from_oop&lt;HeapWord*&gt;(obj), cast_from_oop&lt;HeapWord*&gt;(archived_oop), len);
 144     MetaspaceShared::relocate_klass_ptr(archived_oop);
 145     // Clear age -- it might have been set if a GC happened during -Xshare:dump
 146     markWord mark = archived_oop-&gt;mark_raw();
 147     mark = mark.set_age(0);
 148     archived_oop-&gt;set_mark_raw(mark);
 149     ArchivedObjectCache* cache = archived_object_cache();
 150     cache-&gt;put(obj, archived_oop);
 151     log_debug(cds, heap)(&quot;Archived heap object &quot; PTR_FORMAT &quot; ==&gt; &quot; PTR_FORMAT,
 152                          p2i(obj), p2i(archived_oop));
 153   } else {
 154     log_error(cds, heap)(
 155       &quot;Cannot allocate space for object &quot; PTR_FORMAT &quot; in archived heap region&quot;,
 156       p2i(obj));
 157     vm_exit(1);
 158   }
 159   return archived_oop;
 160 }
 161 
 162 oop HeapShared::materialize_archived_object(narrowOop v) {
 163   assert(archive_heap_region_fixed(),
 164          &quot;must be called after archive heap regions are fixed&quot;);
 165   if (!CompressedOops::is_null(v)) {
 166     oop obj = HeapShared::decode_from_archive(v);
 167     return G1CollectedHeap::heap()-&gt;materialize_archived_object(obj);
 168   }
 169   return NULL;
 170 }
 171 
 172 void HeapShared::archive_klass_objects(Thread* THREAD) {
 173   GrowableArray&lt;Klass*&gt;* klasses = MetaspaceShared::collected_klasses();
 174   assert(klasses != NULL, &quot;sanity&quot;);
 175   for (int i = 0; i &lt; klasses-&gt;length(); i++) {
 176     Klass* k = klasses-&gt;at(i);
 177 
 178     // archive mirror object
 179     java_lang_Class::archive_mirror(k, CHECK);
 180 
 181     // archive the resolved_referenes array
 182     if (k-&gt;is_instance_klass()) {
 183       InstanceKlass* ik = InstanceKlass::cast(k);
 184       ik-&gt;constants()-&gt;archive_resolved_references(THREAD);
 185     }
 186   }
 187 }
 188 
 189 void HeapShared::archive_java_heap_objects(GrowableArray&lt;MemRegion&gt; *closed,
 190                                            GrowableArray&lt;MemRegion&gt; *open) {
 191   if (!is_heap_object_archiving_allowed()) {
 192     log_info(cds)(
 193       &quot;Archived java heap is not supported as UseG1GC, &quot;
 194       &quot;UseCompressedOops and UseCompressedClassPointers are required.&quot;
 195       &quot;Current settings: UseG1GC=%s, UseCompressedOops=%s, UseCompressedClassPointers=%s.&quot;,
 196       BOOL_TO_STR(UseG1GC), BOOL_TO_STR(UseCompressedOops),
 197       BOOL_TO_STR(UseCompressedClassPointers));
 198     return;
 199   }
 200 
 201   G1HeapVerifier::verify_ready_for_archiving();
 202 
 203   {
 204     NoSafepointVerifier nsv;
 205 
 206     // Cache for recording where the archived objects are copied to
 207     create_archived_object_cache();
 208 
 209     log_info(cds)(&quot;Dumping objects to closed archive heap region ...&quot;);
 210     NOT_PRODUCT(StringTable::verify());
 211     copy_closed_archive_heap_objects(closed);
 212 
 213     log_info(cds)(&quot;Dumping objects to open archive heap region ...&quot;);
 214     copy_open_archive_heap_objects(open);
 215 
 216     destroy_archived_object_cache();
 217   }
 218 
 219   G1HeapVerifier::verify_archive_regions();
 220 }
 221 
 222 void HeapShared::copy_closed_archive_heap_objects(
 223                                     GrowableArray&lt;MemRegion&gt; * closed_archive) {
 224   assert(is_heap_object_archiving_allowed(), &quot;Cannot archive java heap objects&quot;);
 225 
 226   Thread* THREAD = Thread::current();
 227   G1CollectedHeap::heap()-&gt;begin_archive_alloc_range();
 228 
 229   // Archive interned string objects
 230   StringTable::write_to_archive();
 231 
 232   archive_object_subgraphs(closed_archive_subgraph_entry_fields,
 233                            num_closed_archive_subgraph_entry_fields,
 234                            true /* is_closed_archive */, THREAD);
 235 
 236   G1CollectedHeap::heap()-&gt;end_archive_alloc_range(closed_archive,
 237                                                    os::vm_allocation_granularity());
 238 }
 239 
 240 void HeapShared::copy_open_archive_heap_objects(
 241                                     GrowableArray&lt;MemRegion&gt; * open_archive) {
 242   assert(is_heap_object_archiving_allowed(), &quot;Cannot archive java heap objects&quot;);
 243 
 244   Thread* THREAD = Thread::current();
 245   G1CollectedHeap::heap()-&gt;begin_archive_alloc_range(true /* open */);
 246 
 247   java_lang_Class::archive_basic_type_mirrors(THREAD);
 248 
 249   archive_klass_objects(THREAD);
 250 
 251   archive_object_subgraphs(open_archive_subgraph_entry_fields,
 252                            num_open_archive_subgraph_entry_fields,
 253                            false /* is_closed_archive */,
 254                            THREAD);
 255 
 256   G1CollectedHeap::heap()-&gt;end_archive_alloc_range(open_archive,
 257                                                    os::vm_allocation_granularity());
 258 }
 259 
 260 void HeapShared::init_narrow_oop_decoding(address base, int shift) {
 261   _narrow_oop_base = base;
 262   _narrow_oop_shift = shift;
 263 }
 264 
 265 //
 266 // Subgraph archiving support
 267 //
 268 HeapShared::DumpTimeKlassSubGraphInfoTable* HeapShared::_dump_time_subgraph_info_table = NULL;
 269 HeapShared::RunTimeKlassSubGraphInfoTable   HeapShared::_run_time_subgraph_info_table;
 270 
 271 // Get the subgraph_info for Klass k. A new subgraph_info is created if
 272 // there is no existing one for k. The subgraph_info records the relocated
 273 // Klass* of the original k.
 274 KlassSubGraphInfo* HeapShared::get_subgraph_info(Klass* k) {
 275   assert(DumpSharedSpaces, &quot;dump time only&quot;);
 276   Klass* relocated_k = MetaspaceShared::get_relocated_klass(k);
 277   KlassSubGraphInfo* info = _dump_time_subgraph_info_table-&gt;get(relocated_k);
 278   if (info == NULL) {
 279     _dump_time_subgraph_info_table-&gt;put(relocated_k, KlassSubGraphInfo(relocated_k));
 280     info = _dump_time_subgraph_info_table-&gt;get(relocated_k);
 281     ++ _dump_time_subgraph_info_table-&gt;_count;
 282   }
 283   return info;
 284 }
 285 
 286 // Add an entry field to the current KlassSubGraphInfo.
 287 void KlassSubGraphInfo::add_subgraph_entry_field(
 288       int static_field_offset, oop v, bool is_closed_archive) {
 289   assert(DumpSharedSpaces, &quot;dump time only&quot;);
 290   if (_subgraph_entry_fields == NULL) {
 291     _subgraph_entry_fields =
 292       new(ResourceObj::C_HEAP, mtClass) GrowableArray&lt;juint&gt;(10, true);
 293   }
 294   _subgraph_entry_fields-&gt;append((juint)static_field_offset);
 295   _subgraph_entry_fields-&gt;append(CompressedOops::encode(v));
 296   _subgraph_entry_fields-&gt;append(is_closed_archive ? 1 : 0);
 297 }
 298 
 299 // Add the Klass* for an object in the current KlassSubGraphInfo&#39;s subgraphs.
 300 // Only objects of boot classes can be included in sub-graph.
 301 void KlassSubGraphInfo::add_subgraph_object_klass(Klass* orig_k, Klass *relocated_k) {
 302   assert(DumpSharedSpaces, &quot;dump time only&quot;);
 303   assert(relocated_k == MetaspaceShared::get_relocated_klass(orig_k),
 304          &quot;must be the relocated Klass in the shared space&quot;);
 305 
 306   if (_subgraph_object_klasses == NULL) {
 307     _subgraph_object_klasses =
 308       new(ResourceObj::C_HEAP, mtClass) GrowableArray&lt;Klass*&gt;(50, true);
 309   }
 310 
 311   assert(relocated_k-&gt;is_shared(), &quot;must be a shared class&quot;);
 312 
 313   if (_k == relocated_k) {
 314     // Don&#39;t add the Klass containing the sub-graph to it&#39;s own klass
 315     // initialization list.
 316     return;
 317   }
 318 
 319   if (relocated_k-&gt;is_instance_klass()) {
 320     assert(InstanceKlass::cast(relocated_k)-&gt;is_shared_boot_class(),
 321           &quot;must be boot class&quot;);
 322     // SystemDictionary::xxx_klass() are not updated, need to check
 323     // the original Klass*
 324     if (orig_k == SystemDictionary::String_klass() ||
 325         orig_k == SystemDictionary::Object_klass()) {
 326       // Initialized early during VM initialization. No need to be added
 327       // to the sub-graph object class list.
 328       return;
 329     }
 330   } else if (relocated_k-&gt;is_objArray_klass()) {
 331     Klass* abk = ObjArrayKlass::cast(relocated_k)-&gt;bottom_klass();
 332     if (abk-&gt;is_instance_klass()) {
 333       assert(InstanceKlass::cast(abk)-&gt;is_shared_boot_class(),
 334             &quot;must be boot class&quot;);
 335     }
 336     if (relocated_k == Universe::objectArrayKlassObj()) {
 337       // Initialized early during Universe::genesis. No need to be added
 338       // to the list.
 339       return;
 340     }
 341   } else {
 342     assert(relocated_k-&gt;is_typeArray_klass(), &quot;must be&quot;);
 343     // Primitive type arrays are created early during Universe::genesis.
 344     return;
 345   }
 346 
 347   if (log_is_enabled(Debug, cds, heap)) {
 348     if (!_subgraph_object_klasses-&gt;contains(relocated_k)) {
 349       ResourceMark rm;
 350       log_debug(cds, heap)(&quot;Adding klass %s&quot;, orig_k-&gt;external_name());
 351     }
 352   }
 353 
 354   _subgraph_object_klasses-&gt;append_if_missing(relocated_k);
 355 }
 356 
 357 // Initialize an archived subgraph_info_record from the given KlassSubGraphInfo.
 358 void ArchivedKlassSubGraphInfoRecord::init(KlassSubGraphInfo* info) {
 359   _k = info-&gt;klass();
 360   _entry_field_records = NULL;
 361   _subgraph_object_klasses = NULL;
 362 
 363   // populate the entry fields
 364   GrowableArray&lt;juint&gt;* entry_fields = info-&gt;subgraph_entry_fields();
 365   if (entry_fields != NULL) {
 366     int num_entry_fields = entry_fields-&gt;length();
 367     assert(num_entry_fields % 3 == 0, &quot;sanity&quot;);
 368     _entry_field_records =
 369       MetaspaceShared::new_ro_array&lt;juint&gt;(num_entry_fields);
 370     for (int i = 0 ; i &lt; num_entry_fields; i++) {
 371       _entry_field_records-&gt;at_put(i, entry_fields-&gt;at(i));
 372     }
 373   }
 374 
 375   // the Klasses of the objects in the sub-graphs
 376   GrowableArray&lt;Klass*&gt;* subgraph_object_klasses = info-&gt;subgraph_object_klasses();
 377   if (subgraph_object_klasses != NULL) {
 378     int num_subgraphs_klasses = subgraph_object_klasses-&gt;length();
 379     _subgraph_object_klasses =
 380       MetaspaceShared::new_ro_array&lt;Klass*&gt;(num_subgraphs_klasses);
 381     for (int i = 0; i &lt; num_subgraphs_klasses; i++) {
 382       Klass* subgraph_k = subgraph_object_klasses-&gt;at(i);
 383       if (log_is_enabled(Info, cds, heap)) {
 384         ResourceMark rm;
 385         log_info(cds, heap)(
 386           &quot;Archived object klass %s (%2d) =&gt; %s&quot;,
 387           _k-&gt;external_name(), i, subgraph_k-&gt;external_name());
 388       }
 389       _subgraph_object_klasses-&gt;at_put(i, subgraph_k);
 390       ArchivePtrMarker::mark_pointer(_subgraph_object_klasses-&gt;adr_at(i));
 391     }
 392   }
 393 
 394   ArchivePtrMarker::mark_pointer(&amp;_k);
 395   ArchivePtrMarker::mark_pointer(&amp;_entry_field_records);
 396   ArchivePtrMarker::mark_pointer(&amp;_subgraph_object_klasses);
 397 }
 398 
 399 struct CopyKlassSubGraphInfoToArchive : StackObj {
 400   CompactHashtableWriter* _writer;
 401   CopyKlassSubGraphInfoToArchive(CompactHashtableWriter* writer) : _writer(writer) {}
 402 
 403   bool do_entry(Klass* klass, KlassSubGraphInfo&amp; info) {
 404     if (info.subgraph_object_klasses() != NULL || info.subgraph_entry_fields() != NULL) {
 405       ArchivedKlassSubGraphInfoRecord* record =
 406         (ArchivedKlassSubGraphInfoRecord*)MetaspaceShared::read_only_space_alloc(sizeof(ArchivedKlassSubGraphInfoRecord));
 407       record-&gt;init(&amp;info);
 408 
 409       unsigned int hash = SystemDictionaryShared::hash_for_shared_dictionary(klass);
 410       u4 delta = MetaspaceShared::object_delta_u4(record);
 411       _writer-&gt;add(hash, delta);
 412     }
 413     return true; // keep on iterating
 414   }
 415 };
 416 
 417 // Build the records of archived subgraph infos, which include:
 418 // - Entry points to all subgraphs from the containing class mirror. The entry
 419 //   points are static fields in the mirror. For each entry point, the field
 420 //   offset, value and is_closed_archive flag are recorded in the sub-graph
 421 //   info. The value is stored back to the corresponding field at runtime.
 422 // - A list of klasses that need to be loaded/initialized before archived
 423 //   java object sub-graph can be accessed at runtime.
 424 void HeapShared::write_subgraph_info_table() {
 425   // Allocate the contents of the hashtable(s) inside the RO region of the CDS archive.
 426   DumpTimeKlassSubGraphInfoTable* d_table = _dump_time_subgraph_info_table;
 427   CompactHashtableStats stats;
 428 
 429   _run_time_subgraph_info_table.reset();
 430 
 431   CompactHashtableWriter writer(d_table-&gt;_count, &amp;stats);
 432   CopyKlassSubGraphInfoToArchive copy(&amp;writer);
 433   d_table-&gt;iterate(&amp;copy);
 434 
 435   writer.dump(&amp;_run_time_subgraph_info_table, &quot;subgraphs&quot;);
 436 }
 437 
 438 void HeapShared::serialize_subgraph_info_table_header(SerializeClosure* soc) {
 439   _run_time_subgraph_info_table.serialize_header(soc);
 440 }
 441 
 442 void HeapShared::initialize_from_archived_subgraph(Klass* k) {
 443   if (!open_archive_heap_region_mapped()) {
 444     return; // nothing to do
 445   }
 446   assert(!DumpSharedSpaces, &quot;Should not be called with DumpSharedSpaces&quot;);
 447 
 448   unsigned int hash = SystemDictionaryShared::hash_for_shared_dictionary(k);
 449   const ArchivedKlassSubGraphInfoRecord* record = _run_time_subgraph_info_table.lookup(k, hash, 0);
 450 
 451   // Initialize from archived data. Currently this is done only
 452   // during VM initialization time. No lock is needed.
 453   if (record != NULL) {
 454     Thread* THREAD = Thread::current();
 455 
 456     int i;
 457     // Load/link/initialize the klasses of the objects in the subgraph.
 458     // NULL class loader is used.
 459     Array&lt;Klass*&gt;* klasses = record-&gt;subgraph_object_klasses();
 460     if (klasses != NULL) {
 461       for (i = 0; i &lt; klasses-&gt;length(); i++) {
 462         Klass* obj_k = klasses-&gt;at(i);
 463         Klass* resolved_k = SystemDictionary::resolve_or_null(
 464                                               (obj_k)-&gt;name(), THREAD);
 465         if (resolved_k != obj_k) {
 466           assert(!SystemDictionary::is_well_known_klass(resolved_k),
 467                  &quot;shared well-known classes must not be replaced by JVMTI ClassFileLoadHook&quot;);
 468           ResourceMark rm(THREAD);
 469           log_info(cds, heap)(&quot;Failed to load subgraph because %s was not loaded from archive&quot;,
 470                               resolved_k-&gt;external_name());
 471           return;
 472         }
 473         if ((obj_k)-&gt;is_instance_klass()) {
 474           InstanceKlass* ik = InstanceKlass::cast(obj_k);
 475           ik-&gt;initialize(THREAD);
 476         } else if ((obj_k)-&gt;is_objArray_klass()) {
 477           ObjArrayKlass* oak = ObjArrayKlass::cast(obj_k);
 478           oak-&gt;initialize(THREAD);
 479         }
 480       }
 481     }
 482 
 483     if (HAS_PENDING_EXCEPTION) {
 484       CLEAR_PENDING_EXCEPTION;
 485       // None of the field value will be set if there was an exception.
 486       // The java code will not see any of the archived objects in the
 487       // subgraphs referenced from k in this case.
 488       return;
 489     }
 490 
 491     // Load the subgraph entry fields from the record and store them back to
 492     // the corresponding fields within the mirror.
 493     oop m = k-&gt;java_mirror();
 494     Array&lt;juint&gt;* entry_field_records = record-&gt;entry_field_records();
 495     if (entry_field_records != NULL) {
 496       int efr_len = entry_field_records-&gt;length();
 497       assert(efr_len % 3 == 0, &quot;sanity&quot;);
 498       for (i = 0; i &lt; efr_len;) {
 499         int field_offset = entry_field_records-&gt;at(i);
 500         narrowOop nv = entry_field_records-&gt;at(i+1);
 501         int is_closed_archive = entry_field_records-&gt;at(i+2);
 502         oop v;
 503         if (is_closed_archive == 0) {
 504           // It&#39;s an archived object in the open archive heap regions, not shared.
 505           // The object refereced by the field becomes &#39;known&#39; by GC from this
 506           // point. All objects in the subgraph reachable from the object are
 507           // also &#39;known&#39; by GC.
 508           v = materialize_archived_object(nv);
 509         } else {
 510           // Shared object in the closed archive heap regions. Decode directly.
 511           assert(!CompressedOops::is_null(nv), &quot;shared object is null&quot;);
 512           v = HeapShared::decode_from_archive(nv);
 513         }
 514         m-&gt;obj_field_put(field_offset, v);
 515         i += 3;
 516 
 517         log_debug(cds, heap)(&quot;  &quot; PTR_FORMAT &quot; init field @ %2d = &quot; PTR_FORMAT, p2i(k), field_offset, p2i(v));
 518       }
 519 
 520       // Done. Java code can see the archived sub-graphs referenced from k&#39;s
 521       // mirror after this point.
 522       if (log_is_enabled(Info, cds, heap)) {
 523         ResourceMark rm;
 524         log_info(cds, heap)(&quot;initialize_from_archived_subgraph %s &quot; PTR_FORMAT,
 525                             k-&gt;external_name(), p2i(k));
 526       }
 527     }
 528   }
 529 }
 530 
 531 class WalkOopAndArchiveClosure: public BasicOopIterateClosure {
 532   int _level;
 533   bool _is_closed_archive;
 534   bool _record_klasses_only;
 535   KlassSubGraphInfo* _subgraph_info;
 536   oop _orig_referencing_obj;
 537   oop _archived_referencing_obj;
 538   Thread* _thread;
 539  public:
 540   WalkOopAndArchiveClosure(int level,
 541                            bool is_closed_archive,
 542                            bool record_klasses_only,
 543                            KlassSubGraphInfo* subgraph_info,
 544                            oop orig, oop archived, TRAPS) :
 545     _level(level), _is_closed_archive(is_closed_archive),
 546     _record_klasses_only(record_klasses_only),
 547     _subgraph_info(subgraph_info),
 548     _orig_referencing_obj(orig), _archived_referencing_obj(archived),
 549     _thread(THREAD) {}
 550   void do_oop(narrowOop *p) { WalkOopAndArchiveClosure::do_oop_work(p); }
 551   void do_oop(      oop *p) { WalkOopAndArchiveClosure::do_oop_work(p); }
 552 
 553  protected:
 554   template &lt;class T&gt; void do_oop_work(T *p) {
 555     oop obj = RawAccess&lt;&gt;::oop_load(p);
 556     if (!CompressedOops::is_null(obj)) {
 557       assert(!HeapShared::is_archived_object(obj),
 558              &quot;original objects must not point to archived objects&quot;);
 559 
 560       size_t field_delta = pointer_delta(p, _orig_referencing_obj, sizeof(char));
 561       T* new_p = (T*)(cast_from_oop&lt;address&gt;(_archived_referencing_obj) + field_delta);
 562       Thread* THREAD = _thread;
 563 
 564       if (!_record_klasses_only &amp;&amp; log_is_enabled(Debug, cds, heap)) {
 565         ResourceMark rm;
 566         log_debug(cds, heap)(&quot;(%d) %s[&quot; SIZE_FORMAT &quot;] ==&gt; &quot; PTR_FORMAT &quot; size %d %s&quot;, _level,
 567                              _orig_referencing_obj-&gt;klass()-&gt;external_name(), field_delta,
 568                              p2i(obj), obj-&gt;size() * HeapWordSize, obj-&gt;klass()-&gt;external_name());
 569         LogTarget(Trace, cds, heap) log;
 570         LogStream out(log);
 571         obj-&gt;print_on(&amp;out);
 572       }
 573 
 574       oop archived = HeapShared::archive_reachable_objects_from(
 575           _level + 1, _subgraph_info, obj, _is_closed_archive, THREAD);
 576       assert(archived != NULL, &quot;VM should have exited with unarchivable objects for _level &gt; 1&quot;);
 577       assert(HeapShared::is_archived_object(archived), &quot;must be&quot;);
 578 
 579       if (!_record_klasses_only) {
 580         // Update the reference in the archived copy of the referencing object.
 581         log_debug(cds, heap)(&quot;(%d) updating oop @[&quot; PTR_FORMAT &quot;] &quot; PTR_FORMAT &quot; ==&gt; &quot; PTR_FORMAT,
 582                              _level, p2i(new_p), p2i(obj), p2i(archived));
 583         RawAccess&lt;IS_NOT_NULL&gt;::oop_store(new_p, archived);
 584       }
 585     }
 586   }
 587 };
 588 
 589 void HeapShared::check_closed_archive_heap_region_object(InstanceKlass* k,
 590                                                          Thread* THREAD) {
 591   // Check fields in the object
 592   for (JavaFieldStream fs(k); !fs.done(); fs.next()) {
 593     if (!fs.access_flags().is_static()) {
 594       BasicType ft = fs.field_descriptor().field_type();
 595       if (!fs.access_flags().is_final() &amp;&amp; is_reference_type(ft)) {
 596         ResourceMark rm(THREAD);
 597         log_warning(cds, heap)(
 598           &quot;Please check reference field in %s instance in closed archive heap region: %s %s&quot;,
 599           k-&gt;external_name(), (fs.name())-&gt;as_C_string(),
 600           (fs.signature())-&gt;as_C_string());
 601       }
 602     }
 603   }
 604 }
 605 
 606 // (1) If orig_obj has not been archived yet, archive it.
 607 // (2) If orig_obj has not been seen yet (since start_recording_subgraph() was called),
 608 //     trace all  objects that are reachable from it, and make sure these objects are archived.
 609 // (3) Record the klasses of all orig_obj and all reachable objects.
 610 oop HeapShared::archive_reachable_objects_from(int level,
 611                                                KlassSubGraphInfo* subgraph_info,
 612                                                oop orig_obj,
 613                                                bool is_closed_archive,
 614                                                TRAPS) {
 615   assert(orig_obj != NULL, &quot;must be&quot;);
 616   assert(!is_archived_object(orig_obj), &quot;sanity&quot;);
 617 
 618   if (!JavaClasses::is_supported_for_archiving(orig_obj)) {
 619     // This object has injected fields that cannot be supported easily, so we disallow them for now.
 620     // If you get an error here, you probably made a change in the JDK library that has added
 621     // these objects that are referenced (directly or indirectly) by static fields.
 622     ResourceMark rm;
 623     log_error(cds, heap)(&quot;Cannot archive object of class %s&quot;, orig_obj-&gt;klass()-&gt;external_name());
 624     vm_exit(1);
 625   }
 626 
 627   // java.lang.Class instances cannot be included in an archived object sub-graph. We only support
 628   // them as Klass::_archived_mirror because they need to be specially restored at run time.
 629   //
 630   // If you get an error here, you probably made a change in the JDK library that has added a Class
 631   // object that is referenced (directly or indirectly) by static fields.
 632   if (java_lang_Class::is_instance(orig_obj)) {
 633     log_error(cds, heap)(&quot;(%d) Unknown java.lang.Class object is in the archived sub-graph&quot;, level);
 634     vm_exit(1);
 635   }
 636 
 637   oop archived_obj = find_archived_heap_object(orig_obj);
 638   if (java_lang_String::is_instance(orig_obj) &amp;&amp; archived_obj != NULL) {
 639     // To save time, don&#39;t walk strings that are already archived. They just contain
 640     // pointers to a type array, whose klass doesn&#39;t need to be recorded.
 641     return archived_obj;
 642   }
 643 
 644   if (has_been_seen_during_subgraph_recording(orig_obj)) {
 645     // orig_obj has already been archived and traced. Nothing more to do.
 646     return archived_obj;
 647   } else {
 648     set_has_been_seen_during_subgraph_recording(orig_obj);
 649   }
 650 
 651   bool record_klasses_only = (archived_obj != NULL);
 652   if (archived_obj == NULL) {
 653     ++_num_new_archived_objs;
 654     archived_obj = archive_heap_object(orig_obj, THREAD);
 655     if (archived_obj == NULL) {
 656       // Skip archiving the sub-graph referenced from the current entry field.
 657       ResourceMark rm;
 658       log_error(cds, heap)(
 659         &quot;Cannot archive the sub-graph referenced from %s object (&quot;
 660         PTR_FORMAT &quot;) size %d, skipped.&quot;,
 661         orig_obj-&gt;klass()-&gt;external_name(), p2i(orig_obj), orig_obj-&gt;size() * HeapWordSize);
 662       if (level == 1) {
 663         // Don&#39;t archive a subgraph root that&#39;s too big. For archives static fields, that&#39;s OK
 664         // as the Java code will take care of initializing this field dynamically.
 665         return NULL;
 666       } else {
 667         // We don&#39;t know how to handle an object that has been archived, but some of its reachable
 668         // objects cannot be archived. Bail out for now. We might need to fix this in the future if
 669         // we have a real use case.
 670         vm_exit(1);
 671       }
 672     }
 673   }
 674 
 675   assert(archived_obj != NULL, &quot;must be&quot;);
 676   Klass *orig_k = orig_obj-&gt;klass();
 677   Klass *relocated_k = archived_obj-&gt;klass();
 678   subgraph_info-&gt;add_subgraph_object_klass(orig_k, relocated_k);
 679 
 680   WalkOopAndArchiveClosure walker(level, is_closed_archive, record_klasses_only,
 681                                   subgraph_info, orig_obj, archived_obj, THREAD);
 682   orig_obj-&gt;oop_iterate(&amp;walker);
 683   if (is_closed_archive &amp;&amp; orig_k-&gt;is_instance_klass()) {
 684     check_closed_archive_heap_region_object(InstanceKlass::cast(orig_k), THREAD);
 685   }
 686   return archived_obj;
 687 }
 688 
 689 //
 690 // Start from the given static field in a java mirror and archive the
 691 // complete sub-graph of java heap objects that are reached directly
 692 // or indirectly from the starting object by following references.
 693 // Sub-graph archiving restrictions (current):
 694 //
 695 // - All classes of objects in the archived sub-graph (including the
 696 //   entry class) must be boot class only.
 697 // - No java.lang.Class instance (java mirror) can be included inside
 698 //   an archived sub-graph. Mirror can only be the sub-graph entry object.
 699 //
 700 // The Java heap object sub-graph archiving process (see
 701 // WalkOopAndArchiveClosure):
 702 //
 703 // 1) Java object sub-graph archiving starts from a given static field
 704 // within a Class instance (java mirror). If the static field is a
 705 // refererence field and points to a non-null java object, proceed to
 706 // the next step.
 707 //
 708 // 2) Archives the referenced java object. If an archived copy of the
 709 // current object already exists, updates the pointer in the archived
 710 // copy of the referencing object to point to the current archived object.
 711 // Otherwise, proceed to the next step.
 712 //
 713 // 3) Follows all references within the current java object and recursively
 714 // archive the sub-graph of objects starting from each reference.
 715 //
 716 // 4) Updates the pointer in the archived copy of referencing object to
 717 // point to the current archived object.
 718 //
 719 // 5) The Klass of the current java object is added to the list of Klasses
 720 // for loading and initialzing before any object in the archived graph can
 721 // be accessed at runtime.
 722 //
 723 void HeapShared::archive_reachable_objects_from_static_field(InstanceKlass *k,
 724                                                              const char* klass_name,
 725                                                              int field_offset,
 726                                                              const char* field_name,
 727                                                              bool is_closed_archive,
 728                                                              TRAPS) {
 729   assert(DumpSharedSpaces, &quot;dump time only&quot;);
 730   assert(k-&gt;is_shared_boot_class(), &quot;must be boot class&quot;);
 731 
 732   oop m = k-&gt;java_mirror();
 733 
 734   KlassSubGraphInfo* subgraph_info = get_subgraph_info(k);
 735   oop f = m-&gt;obj_field(field_offset);
 736 
 737   log_debug(cds, heap)(&quot;Start archiving from: %s::%s (&quot; PTR_FORMAT &quot;)&quot;, klass_name, field_name, p2i(f));
 738 
 739   if (!CompressedOops::is_null(f)) {
 740     if (log_is_enabled(Trace, cds, heap)) {
 741       LogTarget(Trace, cds, heap) log;
 742       LogStream out(log);
 743       f-&gt;print_on(&amp;out);
 744     }
 745 
 746     oop af = archive_reachable_objects_from(1, subgraph_info, f,
 747                                             is_closed_archive, CHECK);
 748 
 749     if (af == NULL) {
 750       log_error(cds, heap)(&quot;Archiving failed %s::%s (some reachable objects cannot be archived)&quot;,
 751                            klass_name, field_name);
 752     } else {
 753       // Note: the field value is not preserved in the archived mirror.
 754       // Record the field as a new subGraph entry point. The recorded
 755       // information is restored from the archive at runtime.
 756       subgraph_info-&gt;add_subgraph_entry_field(field_offset, af, is_closed_archive);
 757       log_info(cds, heap)(&quot;Archived field %s::%s =&gt; &quot; PTR_FORMAT, klass_name, field_name, p2i(af));
 758     }
 759   } else {
 760     // The field contains null, we still need to record the entry point,
 761     // so it can be restored at runtime.
 762     subgraph_info-&gt;add_subgraph_entry_field(field_offset, NULL, false);
 763   }
 764 }
 765 
 766 #ifndef PRODUCT
 767 class VerifySharedOopClosure: public BasicOopIterateClosure {
 768  private:
 769   bool _is_archived;
 770 
 771  public:
 772   VerifySharedOopClosure(bool is_archived) : _is_archived(is_archived) {}
 773 
 774   void do_oop(narrowOop *p) { VerifySharedOopClosure::do_oop_work(p); }
 775   void do_oop(      oop *p) { VerifySharedOopClosure::do_oop_work(p); }
 776 
 777  protected:
 778   template &lt;class T&gt; void do_oop_work(T *p) {
 779     oop obj = RawAccess&lt;&gt;::oop_load(p);
 780     if (!CompressedOops::is_null(obj)) {
 781       HeapShared::verify_reachable_objects_from(obj, _is_archived);
 782     }
 783   }
 784 };
 785 
 786 void HeapShared::verify_subgraph_from_static_field(InstanceKlass* k, int field_offset) {
 787   assert(DumpSharedSpaces, &quot;dump time only&quot;);
 788   assert(k-&gt;is_shared_boot_class(), &quot;must be boot class&quot;);
 789 
 790   oop m = k-&gt;java_mirror();
 791   oop f = m-&gt;obj_field(field_offset);
 792   if (!CompressedOops::is_null(f)) {
 793     verify_subgraph_from(f);
 794   }
 795 }
 796 
 797 void HeapShared::verify_subgraph_from(oop orig_obj) {
 798   oop archived_obj = find_archived_heap_object(orig_obj);
 799   if (archived_obj == NULL) {
 800     // It&#39;s OK for the root of a subgraph to be not archived. See comments in
 801     // archive_reachable_objects_from().
 802     return;
 803   }
 804 
 805   // Verify that all objects reachable from orig_obj are archived.
 806   init_seen_objects_table();
 807   verify_reachable_objects_from(orig_obj, false);
 808   delete_seen_objects_table();
 809 
 810   // Note: we could also verify that all objects reachable from the archived
 811   // copy of orig_obj can only point to archived objects, with:
 812   //      init_seen_objects_table();
 813   //      verify_reachable_objects_from(archived_obj, true);
 814   //      init_seen_objects_table();
 815   // but that&#39;s already done in G1HeapVerifier::verify_archive_regions so we
 816   // won&#39;t do it here.
 817 }
 818 
 819 void HeapShared::verify_reachable_objects_from(oop obj, bool is_archived) {
 820   _num_total_verifications ++;
 821   if (!has_been_seen_during_subgraph_recording(obj)) {
 822     set_has_been_seen_during_subgraph_recording(obj);
 823 
 824     if (is_archived) {
 825       assert(is_archived_object(obj), &quot;must be&quot;);
 826       assert(find_archived_heap_object(obj) == NULL, &quot;must be&quot;);
 827     } else {
 828       assert(!is_archived_object(obj), &quot;must be&quot;);
 829       assert(find_archived_heap_object(obj) != NULL, &quot;must be&quot;);
 830     }
 831 
 832     VerifySharedOopClosure walker(is_archived);
 833     obj-&gt;oop_iterate(&amp;walker);
 834   }
 835 }
 836 #endif
 837 
 838 HeapShared::SeenObjectsTable* HeapShared::_seen_objects_table = NULL;
 839 int HeapShared::_num_new_walked_objs;
 840 int HeapShared::_num_new_archived_objs;
 841 int HeapShared::_num_old_recorded_klasses;
 842 
 843 int HeapShared::_num_total_subgraph_recordings = 0;
 844 int HeapShared::_num_total_walked_objs = 0;
 845 int HeapShared::_num_total_archived_objs = 0;
 846 int HeapShared::_num_total_recorded_klasses = 0;
 847 int HeapShared::_num_total_verifications = 0;
 848 
 849 bool HeapShared::has_been_seen_during_subgraph_recording(oop obj) {
 850   return _seen_objects_table-&gt;get(obj) != NULL;
 851 }
 852 
 853 void HeapShared::set_has_been_seen_during_subgraph_recording(oop obj) {
 854   assert(!has_been_seen_during_subgraph_recording(obj), &quot;sanity&quot;);
 855   _seen_objects_table-&gt;put(obj, true);
 856   ++ _num_new_walked_objs;
 857 }
 858 
 859 void HeapShared::start_recording_subgraph(InstanceKlass *k, const char* class_name) {
 860   log_info(cds, heap)(&quot;Start recording subgraph(s) for archived fields in %s&quot;, class_name);
 861   init_seen_objects_table();
 862   _num_new_walked_objs = 0;
 863   _num_new_archived_objs = 0;
 864   _num_old_recorded_klasses = get_subgraph_info(k)-&gt;num_subgraph_object_klasses();
 865 }
 866 
 867 void HeapShared::done_recording_subgraph(InstanceKlass *k, const char* class_name) {
 868   int num_new_recorded_klasses = get_subgraph_info(k)-&gt;num_subgraph_object_klasses() -
 869     _num_old_recorded_klasses;
 870   log_info(cds, heap)(&quot;Done recording subgraph(s) for archived fields in %s: &quot;
 871                       &quot;walked %d objs, archived %d new objs, recorded %d classes&quot;,
 872                       class_name, _num_new_walked_objs, _num_new_archived_objs,
 873                       num_new_recorded_klasses);
 874 
 875   delete_seen_objects_table();
 876 
 877   _num_total_subgraph_recordings ++;
 878   _num_total_walked_objs      += _num_new_walked_objs;
 879   _num_total_archived_objs    += _num_new_archived_objs;
 880   _num_total_recorded_klasses +=  num_new_recorded_klasses;
 881 }
 882 
 883 class ArchivableStaticFieldFinder: public FieldClosure {
 884   InstanceKlass* _ik;
 885   Symbol* _field_name;
 886   bool _found;
 887   int _offset;
 888 public:
 889   ArchivableStaticFieldFinder(InstanceKlass* ik, Symbol* field_name) :
 890     _ik(ik), _field_name(field_name), _found(false), _offset(-1) {}
 891 
 892   virtual void do_field(fieldDescriptor* fd) {
 893     if (fd-&gt;name() == _field_name) {
 894       assert(!_found, &quot;fields cannot be overloaded&quot;);
 895       assert(is_reference_type(fd-&gt;field_type()), &quot;can archive only fields that are references&quot;);
 896       _found = true;
 897       _offset = fd-&gt;offset();
 898     }
 899   }
 900   bool found()     { return _found;  }
 901   int offset()     { return _offset; }
 902 };
 903 
 904 void HeapShared::init_subgraph_entry_fields(ArchivableStaticFieldInfo fields[],
 905                                             int num, Thread* THREAD) {
 906   for (int i = 0; i &lt; num; i++) {
 907     ArchivableStaticFieldInfo* info = &amp;fields[i];
 908     TempNewSymbol klass_name =  SymbolTable::new_symbol(info-&gt;klass_name);
 909     TempNewSymbol field_name =  SymbolTable::new_symbol(info-&gt;field_name);
 910 
 911     Klass* k = SystemDictionary::resolve_or_null(klass_name, THREAD);
 912     assert(k != NULL &amp;&amp; !HAS_PENDING_EXCEPTION, &quot;class must exist&quot;);
 913     InstanceKlass* ik = InstanceKlass::cast(k);
 914     assert(InstanceKlass::cast(ik)-&gt;is_shared_boot_class(),
 915            &quot;Only support boot classes&quot;);
 916     ik-&gt;initialize(THREAD);
 917     guarantee(!HAS_PENDING_EXCEPTION, &quot;exception in initialize&quot;);
 918 
 919     ArchivableStaticFieldFinder finder(ik, field_name);
 920     ik-&gt;do_local_static_fields(&amp;finder);
 921     assert(finder.found(), &quot;field must exist&quot;);
 922 
 923     info-&gt;klass = ik;
 924     info-&gt;offset = finder.offset();
 925   }
 926 }
 927 
 928 void HeapShared::init_subgraph_entry_fields(Thread* THREAD) {
 929   _dump_time_subgraph_info_table = new (ResourceObj::C_HEAP, mtClass)DumpTimeKlassSubGraphInfoTable();
 930 
 931   init_subgraph_entry_fields(closed_archive_subgraph_entry_fields,
 932                              num_closed_archive_subgraph_entry_fields,
 933                              THREAD);
 934   init_subgraph_entry_fields(open_archive_subgraph_entry_fields,
 935                              num_open_archive_subgraph_entry_fields,
 936                              THREAD);
 937 }
 938 
 939 void HeapShared::archive_object_subgraphs(ArchivableStaticFieldInfo fields[],
 940                                           int num, bool is_closed_archive,
 941                                           Thread* THREAD) {
 942   _num_total_subgraph_recordings = 0;
 943   _num_total_walked_objs = 0;
 944   _num_total_archived_objs = 0;
 945   _num_total_recorded_klasses = 0;
 946   _num_total_verifications = 0;
 947 
 948   // For each class X that has one or more archived fields:
 949   // [1] Dump the subgraph of each archived field
 950   // [2] Create a list of all the class of the objects that can be reached
 951   //     by any of these static fields.
 952   //     At runtime, these classes are initialized before X&#39;s archived fields
 953   //     are restored by HeapShared::initialize_from_archived_subgraph().
 954   int i;
 955   for (i = 0; i &lt; num; ) {
 956     ArchivableStaticFieldInfo* info = &amp;fields[i];
 957     const char* klass_name = info-&gt;klass_name;
 958     start_recording_subgraph(info-&gt;klass, klass_name);
 959 
 960     // If you have specified consecutive fields of the same klass in
 961     // fields[], these will be archived in the same
 962     // {start_recording_subgraph ... done_recording_subgraph} pass to
 963     // save time.
 964     for (; i &lt; num; i++) {
 965       ArchivableStaticFieldInfo* f = &amp;fields[i];
 966       if (f-&gt;klass_name != klass_name) {
 967         break;
 968       }
 969       archive_reachable_objects_from_static_field(f-&gt;klass, f-&gt;klass_name,
 970                                                   f-&gt;offset, f-&gt;field_name,
 971                                                   is_closed_archive, CHECK);
 972     }
 973     done_recording_subgraph(info-&gt;klass, klass_name);
 974   }
 975 
 976   log_info(cds, heap)(&quot;Archived subgraph records in %s archive heap region = %d&quot;,
 977                       is_closed_archive ? &quot;closed&quot; : &quot;open&quot;,
 978                       _num_total_subgraph_recordings);
 979   log_info(cds, heap)(&quot;  Walked %d objects&quot;, _num_total_walked_objs);
 980   log_info(cds, heap)(&quot;  Archived %d objects&quot;, _num_total_archived_objs);
 981   log_info(cds, heap)(&quot;  Recorded %d klasses&quot;, _num_total_recorded_klasses);
 982 
 983 #ifndef PRODUCT
 984   for (int i = 0; i &lt; num; i++) {
 985     ArchivableStaticFieldInfo* f = &amp;fields[i];
 986     verify_subgraph_from_static_field(f-&gt;klass, f-&gt;offset);
 987   }
 988   log_info(cds, heap)(&quot;  Verified %d references&quot;, _num_total_verifications);
 989 #endif
 990 }
 991 
 992 // At dump-time, find the location of all the non-null oop pointers in an archived heap
 993 // region. This way we can quickly relocate all the pointers without using
 994 // BasicOopIterateClosure at runtime.
 995 class FindEmbeddedNonNullPointers: public BasicOopIterateClosure {
 996   narrowOop* _start;
 997   BitMap *_oopmap;
 998   int _num_total_oops;
 999   int _num_null_oops;
1000  public:
1001   FindEmbeddedNonNullPointers(narrowOop* start, BitMap* oopmap)
1002     : _start(start), _oopmap(oopmap), _num_total_oops(0),  _num_null_oops(0) {}
1003 
1004   virtual bool should_verify_oops(void) {
1005     return false;
1006   }
1007   virtual void do_oop(narrowOop* p) {
1008     _num_total_oops ++;
1009     narrowOop v = *p;
1010     if (!CompressedOops::is_null(v)) {
1011       size_t idx = p - _start;
1012       _oopmap-&gt;set_bit(idx);
1013     } else {
1014       _num_null_oops ++;
1015     }
1016   }
1017   virtual void do_oop(oop *p) {
1018     ShouldNotReachHere();
1019   }
1020   int num_total_oops() const { return _num_total_oops; }
1021   int num_null_oops()  const { return _num_null_oops; }
1022 };
1023 
1024 ResourceBitMap HeapShared::calculate_oopmap(MemRegion region) {
1025   assert(UseCompressedOops, &quot;must be&quot;);
1026   size_t num_bits = region.byte_size() / sizeof(narrowOop);
1027   ResourceBitMap oopmap(num_bits);
1028 
1029   HeapWord* p   = region.start();
1030   HeapWord* end = region.end();
1031   FindEmbeddedNonNullPointers finder((narrowOop*)p, &amp;oopmap);
1032 
1033   int num_objs = 0;
1034   while (p &lt; end) {
1035     oop o = (oop)p;
1036     o-&gt;oop_iterate(&amp;finder);
1037     p += o-&gt;size();
1038     ++ num_objs;
1039   }
1040 
1041   log_info(cds, heap)(&quot;calculate_oopmap: objects = %6d, embedded oops = %7d, nulls = %7d&quot;,
1042                       num_objs, finder.num_total_oops(), finder.num_null_oops());
1043   return oopmap;
1044 }
1045 
1046 // Patch all the embedded oop pointers inside an archived heap region,
1047 // to be consistent with the runtime oop encoding.
1048 class PatchEmbeddedPointers: public BitMapClosure {
1049   narrowOop* _start;
1050 
1051  public:
1052   PatchEmbeddedPointers(narrowOop* start) : _start(start) {}
1053 
1054   bool do_bit(size_t offset) {
1055     narrowOop* p = _start + offset;
1056     narrowOop v = *p;
1057     assert(!CompressedOops::is_null(v), &quot;null oops should have been filtered out at dump time&quot;);
1058     oop o = HeapShared::decode_from_archive(v);
1059     RawAccess&lt;IS_NOT_NULL&gt;::oop_store(p, o);
1060     return true;
1061   }
1062 };
1063 
1064 void HeapShared::patch_archived_heap_embedded_pointers(MemRegion region, address oopmap,
1065                                                        size_t oopmap_size_in_bits) {
1066   BitMapView bm((BitMap::bm_word_t*)oopmap, oopmap_size_in_bits);
1067 
1068 #ifndef PRODUCT
1069   ResourceMark rm;
1070   ResourceBitMap checkBm = calculate_oopmap(region);
1071   assert(bm.is_same(checkBm), &quot;sanity&quot;);
1072 #endif
1073 
1074   PatchEmbeddedPointers patcher((narrowOop*)region.start());
1075   bm.iterate(&amp;patcher);
1076 }
1077 
1078 #endif // INCLUDE_CDS_JAVA_HEAP
    </pre>
  </body>
</html>