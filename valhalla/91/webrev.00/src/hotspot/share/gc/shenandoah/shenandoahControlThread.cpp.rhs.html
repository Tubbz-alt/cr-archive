<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Frames src/hotspot/share/gc/shenandoah/shenandoahControlThread.cpp</title>
    <link rel="stylesheet" href="../../../../../style.css" />
    <script type="text/javascript" src="../../../../../navigation.js"></script>
  </head>
<body onkeypress="keypress(event);">
<a name="0"></a>
<hr />
<pre>  1 /*
  2  * Copyright (c) 2013, 2020, Red Hat, Inc. All rights reserved.
  3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  4  *
  5  * This code is free software; you can redistribute it and/or modify it
  6  * under the terms of the GNU General Public License version 2 only, as
  7  * published by the Free Software Foundation.
  8  *
  9  * This code is distributed in the hope that it will be useful, but WITHOUT
 10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
 11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
 12  * version 2 for more details (a copy is included in the LICENSE file that
 13  * accompanied this code).
 14  *
 15  * You should have received a copy of the GNU General Public License version
 16  * 2 along with this work; if not, write to the Free Software Foundation,
 17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 18  *
 19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
 20  * or visit www.oracle.com if you need additional information or have any
 21  * questions.
 22  *
 23  */
 24 
 25 #include &quot;precompiled.hpp&quot;
 26 
 27 #include &quot;gc/shenandoah/shenandoahConcurrentMark.inline.hpp&quot;
 28 #include &quot;gc/shenandoah/shenandoahCollectorPolicy.hpp&quot;
 29 #include &quot;gc/shenandoah/shenandoahControlThread.hpp&quot;
 30 #include &quot;gc/shenandoah/shenandoahFreeSet.hpp&quot;
 31 #include &quot;gc/shenandoah/shenandoahPhaseTimings.hpp&quot;
 32 #include &quot;gc/shenandoah/shenandoahHeap.inline.hpp&quot;
 33 #include &quot;gc/shenandoah/shenandoahMonitoringSupport.hpp&quot;
 34 #include &quot;gc/shenandoah/shenandoahRootProcessor.inline.hpp&quot;
 35 #include &quot;gc/shenandoah/shenandoahUtils.hpp&quot;
 36 #include &quot;gc/shenandoah/shenandoahVMOperations.hpp&quot;
 37 #include &quot;gc/shenandoah/shenandoahWorkerPolicy.hpp&quot;
 38 #include &quot;gc/shenandoah/heuristics/shenandoahHeuristics.hpp&quot;
 39 #include &quot;memory/iterator.hpp&quot;
 40 #include &quot;memory/universe.hpp&quot;
 41 #include &quot;runtime/atomic.hpp&quot;
 42 
 43 ShenandoahControlThread::ShenandoahControlThread() :
 44   ConcurrentGCThread(),
 45   _alloc_failure_waiters_lock(Mutex::leaf, &quot;ShenandoahAllocFailureGC_lock&quot;, true, Monitor::_safepoint_check_always),
 46   _gc_waiters_lock(Mutex::leaf, &quot;ShenandoahRequestedGC_lock&quot;, true, Monitor::_safepoint_check_always),
 47   _periodic_task(this),
 48   _requested_gc_cause(GCCause::_no_cause_specified),
 49   _degen_point(ShenandoahHeap::_degenerated_outside_cycle),
 50   _allocs_seen(0) {
 51 
 52   reset_gc_id();
 53   create_and_start(ShenandoahCriticalControlThreadPriority ? CriticalPriority : NearMaxPriority);
 54   _periodic_task.enroll();
 55   _periodic_satb_flush_task.enroll();
<a name="1" id="anc1"></a><span class="line-added"> 56   if (ShenandoahPacing) {</span>
<span class="line-added"> 57     _periodic_pacer_notify_task.enroll();</span>
<span class="line-added"> 58   }</span>
 59 }
 60 
 61 ShenandoahControlThread::~ShenandoahControlThread() {
 62   // This is here so that super is called.
 63 }
 64 
 65 void ShenandoahPeriodicTask::task() {
 66   _thread-&gt;handle_force_counters_update();
 67   _thread-&gt;handle_counters_update();
 68 }
 69 
 70 void ShenandoahPeriodicSATBFlushTask::task() {
 71   ShenandoahHeap::heap()-&gt;force_satb_flush_all_threads();
 72 }
 73 
<a name="2" id="anc2"></a><span class="line-added"> 74 void ShenandoahPeriodicPacerNotify::task() {</span>
<span class="line-added"> 75   assert(ShenandoahPacing, &quot;Should not be here otherwise&quot;);</span>
<span class="line-added"> 76   ShenandoahHeap::heap()-&gt;pacer()-&gt;notify_waiters();</span>
<span class="line-added"> 77 }</span>
<span class="line-added"> 78 </span>
 79 void ShenandoahControlThread::run_service() {
 80   ShenandoahHeap* heap = ShenandoahHeap::heap();
 81 
 82   GCMode default_mode = concurrent_normal;
 83   GCCause::Cause default_cause = GCCause::_shenandoah_concurrent_gc;
 84   int sleep = ShenandoahControlIntervalMin;
 85 
 86   double last_shrink_time = os::elapsedTime();
 87   double last_sleep_adjust_time = os::elapsedTime();
 88 
 89   // Shrink period avoids constantly polling regions for shrinking.
 90   // Having a period 10x lower than the delay would mean we hit the
 91   // shrinking with lag of less than 1/10-th of true delay.
 92   // ShenandoahUncommitDelay is in msecs, but shrink_period is in seconds.
 93   double shrink_period = (double)ShenandoahUncommitDelay / 1000 / 10;
 94 
 95   ShenandoahCollectorPolicy* policy = heap-&gt;shenandoah_policy();
 96   ShenandoahHeuristics* heuristics = heap-&gt;heuristics();
 97   while (!in_graceful_shutdown() &amp;&amp; !should_terminate()) {
 98     // Figure out if we have pending requests.
 99     bool alloc_failure_pending = _alloc_failure_gc.is_set();
100     bool explicit_gc_requested = _gc_requested.is_set() &amp;&amp;  is_explicit_gc(_requested_gc_cause);
101     bool implicit_gc_requested = _gc_requested.is_set() &amp;&amp; !is_explicit_gc(_requested_gc_cause);
102 
103     // This control loop iteration have seen this much allocations.
104     size_t allocs_seen = Atomic::xchg(&amp;_allocs_seen, (size_t)0);
105 
106     // Choose which GC mode to run in. The block below should select a single mode.
107     GCMode mode = none;
108     GCCause::Cause cause = GCCause::_last_gc_cause;
109     ShenandoahHeap::ShenandoahDegenPoint degen_point = ShenandoahHeap::_degenerated_unset;
110 
111     if (alloc_failure_pending) {
112       // Allocation failure takes precedence: we have to deal with it first thing
113       log_info(gc)(&quot;Trigger: Handle Allocation Failure&quot;);
114 
115       cause = GCCause::_allocation_failure;
116 
117       // Consume the degen point, and seed it with default value
118       degen_point = _degen_point;
119       _degen_point = ShenandoahHeap::_degenerated_outside_cycle;
120 
121       if (ShenandoahDegeneratedGC &amp;&amp; heuristics-&gt;should_degenerate_cycle()) {
122         heuristics-&gt;record_allocation_failure_gc();
123         policy-&gt;record_alloc_failure_to_degenerated(degen_point);
124         mode = stw_degenerated;
125       } else {
126         heuristics-&gt;record_allocation_failure_gc();
127         policy-&gt;record_alloc_failure_to_full();
128         mode = stw_full;
129       }
130 
131     } else if (explicit_gc_requested) {
132       cause = _requested_gc_cause;
133       log_info(gc)(&quot;Trigger: Explicit GC request (%s)&quot;, GCCause::to_string(cause));
134 
135       heuristics-&gt;record_requested_gc();
136 
137       if (ExplicitGCInvokesConcurrent) {
138         policy-&gt;record_explicit_to_concurrent();
139         mode = default_mode;
140         // Unload and clean up everything
141         heap-&gt;set_process_references(heuristics-&gt;can_process_references());
142         heap-&gt;set_unload_classes(heuristics-&gt;can_unload_classes());
143       } else {
144         policy-&gt;record_explicit_to_full();
145         mode = stw_full;
146       }
147     } else if (implicit_gc_requested) {
148       cause = _requested_gc_cause;
149       log_info(gc)(&quot;Trigger: Implicit GC request (%s)&quot;, GCCause::to_string(cause));
150 
151       heuristics-&gt;record_requested_gc();
152 
153       if (ShenandoahImplicitGCInvokesConcurrent) {
154         policy-&gt;record_implicit_to_concurrent();
155         mode = default_mode;
156 
157         // Unload and clean up everything
158         heap-&gt;set_process_references(heuristics-&gt;can_process_references());
159         heap-&gt;set_unload_classes(heuristics-&gt;can_unload_classes());
160       } else {
161         policy-&gt;record_implicit_to_full();
162         mode = stw_full;
163       }
164     } else {
165       // Potential normal cycle: ask heuristics if it wants to act
166       if (heuristics-&gt;should_start_gc()) {
167         mode = default_mode;
168         cause = default_cause;
169       }
170 
171       // Ask policy if this cycle wants to process references or unload classes
172       heap-&gt;set_process_references(heuristics-&gt;should_process_references());
173       heap-&gt;set_unload_classes(heuristics-&gt;should_unload_classes());
174     }
175 
176     // Blow all soft references on this cycle, if handling allocation failure,
177     // or we are requested to do so unconditionally.
178     if (alloc_failure_pending || ShenandoahAlwaysClearSoftRefs) {
179       heap-&gt;soft_ref_policy()-&gt;set_should_clear_all_soft_refs(true);
180     }
181 
182     bool gc_requested = (mode != none);
183     assert (!gc_requested || cause != GCCause::_last_gc_cause, &quot;GC cause should be set&quot;);
184 
185     if (gc_requested) {
186       // GC is starting, bump the internal ID
187       update_gc_id();
188 
189       heap-&gt;reset_bytes_allocated_since_gc_start();
190 
191       // Use default constructor to snapshot the Metaspace state before GC.
192       metaspace::MetaspaceSizesSnapshot meta_sizes;
193 
194       // If GC was requested, we are sampling the counters even without actual triggers
195       // from allocation machinery. This captures GC phases more accurately.
196       set_forced_counters_update(true);
197 
198       // If GC was requested, we better dump freeset data for performance debugging
199       {
200         ShenandoahHeapLocker locker(heap-&gt;lock());
201         heap-&gt;free_set()-&gt;log_status();
202       }
203 
204       switch (mode) {
205         case concurrent_normal:
206           service_concurrent_normal_cycle(cause);
207           break;
208         case stw_degenerated:
209           service_stw_degenerated_cycle(cause, degen_point);
210           break;
211         case stw_full:
212           service_stw_full_cycle(cause);
213           break;
214         default:
215           ShouldNotReachHere();
216       }
217 
218       // If this was the requested GC cycle, notify waiters about it
219       if (explicit_gc_requested || implicit_gc_requested) {
220         notify_gc_waiters();
221       }
222 
223       // If this was the allocation failure GC cycle, notify waiters about it
224       if (alloc_failure_pending) {
225         notify_alloc_failure_waiters();
226       }
227 
228       // Report current free set state at the end of cycle, whether
229       // it is a normal completion, or the abort.
230       {
231         ShenandoahHeapLocker locker(heap-&gt;lock());
232         heap-&gt;free_set()-&gt;log_status();
233 
234         // Notify Universe about new heap usage. This has implications for
235         // global soft refs policy, and we better report it every time heap
236         // usage goes down.
237         Universe::update_heap_info_at_gc();
238       }
239 
240       // Disable forced counters update, and update counters one more time
241       // to capture the state at the end of GC session.
242       handle_force_counters_update();
243       set_forced_counters_update(false);
244 
245       // Retract forceful part of soft refs policy
246       heap-&gt;soft_ref_policy()-&gt;set_should_clear_all_soft_refs(false);
247 
248       // Clear metaspace oom flag, if current cycle unloaded classes
249       if (heap-&gt;unload_classes()) {
250         heuristics-&gt;clear_metaspace_oom();
251       }
252 
253       // Commit worker statistics to cycle data
254       heap-&gt;phase_timings()-&gt;flush_par_workers_to_cycle();
255 
256       // Print GC stats for current cycle
257       {
258         LogTarget(Info, gc, stats) lt;
259         if (lt.is_enabled()) {
260           ResourceMark rm;
261           LogStream ls(lt);
262           heap-&gt;phase_timings()-&gt;print_cycle_on(&amp;ls);
263         }
264       }
265 
266       // Commit statistics to globals
267       heap-&gt;phase_timings()-&gt;flush_cycle_to_global();
268 
269       // Print Metaspace change following GC (if logging is enabled).
270       MetaspaceUtils::print_metaspace_change(meta_sizes);
271 
272       // GC is over, we are at idle now
273       if (ShenandoahPacing) {
274         heap-&gt;pacer()-&gt;setup_for_idle();
275       }
276     } else {
277       // Allow allocators to know we have seen this much regions
278       if (ShenandoahPacing &amp;&amp; (allocs_seen &gt; 0)) {
279         heap-&gt;pacer()-&gt;report_alloc(allocs_seen);
280       }
281     }
282 
283     double current = os::elapsedTime();
284 
285     if (ShenandoahUncommit &amp;&amp; (explicit_gc_requested || (current - last_shrink_time &gt; shrink_period))) {
286       // Try to uncommit enough stale regions. Explicit GC tries to uncommit everything.
287       // Regular paths uncommit only occasionally.
288       double shrink_before = explicit_gc_requested ?
289                              current :
290                              current - (ShenandoahUncommitDelay / 1000.0);
291       service_uncommit(shrink_before);
292       heap-&gt;phase_timings()-&gt;flush_cycle_to_global();
293       last_shrink_time = current;
294     }
295 
296     // Wait before performing the next action. If allocation happened during this wait,
297     // we exit sooner, to let heuristics re-evaluate new conditions. If we are at idle,
298     // back off exponentially.
299     if (_heap_changed.try_unset()) {
300       sleep = ShenandoahControlIntervalMin;
301     } else if ((current - last_sleep_adjust_time) * 1000 &gt; ShenandoahControlIntervalAdjustPeriod){
302       sleep = MIN2&lt;int&gt;(ShenandoahControlIntervalMax, MAX2(1, sleep * 2));
303       last_sleep_adjust_time = current;
304     }
305     os::naked_short_sleep(sleep);
306   }
307 
308   // Wait for the actual stop(), can&#39;t leave run_service() earlier.
309   while (!should_terminate()) {
310     os::naked_short_sleep(ShenandoahControlIntervalMin);
311   }
312 }
313 
314 void ShenandoahControlThread::service_concurrent_normal_cycle(GCCause::Cause cause) {
315   // Normal cycle goes via all concurrent phases. If allocation failure (af) happens during
316   // any of the concurrent phases, it first degrades to Degenerated GC and completes GC there.
317   // If second allocation failure happens during Degenerated GC cycle (for example, when GC
318   // tries to evac something and no memory is available), cycle degrades to Full GC.
319   //
320   // There are also a shortcut through the normal cycle: immediate garbage shortcut, when
321   // heuristics says there are no regions to compact, and all the collection comes from immediately
322   // reclaimable regions.
323   //
324   // ................................................................................................
325   //
326   //                                    (immediate garbage shortcut)                Concurrent GC
327   //                             /-------------------------------------------\
328   //                             |                                           |
329   //                             |                                           |
330   //                             |                                           |
331   //                             |                                           v
332   // [START] ----&gt; Conc Mark ----o----&gt; Conc Evac --o--&gt; Conc Update-Refs ---o----&gt; [END]
333   //                   |                    |                 |              ^
334   //                   | (af)               | (af)            | (af)         |
335   // ..................|....................|.................|..............|.......................
336   //                   |                    |                 |              |
337   //                   |                    |                 |              |      Degenerated GC
338   //                   v                    v                 v              |
339   //               STW Mark ----------&gt; STW Evac ----&gt; STW Update-Refs -----&gt;o
340   //                   |                    |                 |              ^
341   //                   | (af)               | (af)            | (af)         |
342   // ..................|....................|.................|..............|.......................
343   //                   |                    |                 |              |
344   //                   |                    v                 |              |      Full GC
345   //                   \-------------------&gt;o&lt;----------------/              |
346   //                                        |                                |
347   //                                        v                                |
348   //                                      Full GC  --------------------------/
349   //
350   ShenandoahHeap* heap = ShenandoahHeap::heap();
351 
352   if (check_cancellation_or_degen(ShenandoahHeap::_degenerated_outside_cycle)) return;
353 
354   GCIdMark gc_id_mark;
355   ShenandoahGCSession session(cause);
356 
357   TraceCollectorStats tcs(heap-&gt;monitoring_support()-&gt;concurrent_collection_counters());
358 
359   // Reset for upcoming marking
360   heap-&gt;entry_reset();
361 
362   // Start initial mark under STW
363   heap-&gt;vmop_entry_init_mark();
364 
365   // Continue concurrent mark
366   heap-&gt;entry_mark();
367   if (check_cancellation_or_degen(ShenandoahHeap::_degenerated_mark)) return;
368 
369   // If not cancelled, can try to concurrently pre-clean
370   heap-&gt;entry_preclean();
371 
372   // Complete marking under STW, and start evacuation
373   heap-&gt;vmop_entry_final_mark();
374 
375   // Process weak roots that might still point to regions that would be broken by cleanup
376   if (heap-&gt;is_concurrent_weak_root_in_progress()) {
377     heap-&gt;entry_weak_roots();
378   }
379 
380   // Final mark might have reclaimed some immediate garbage, kick cleanup to reclaim
381   // the space. This would be the last action if there is nothing to evacuate.
382   heap-&gt;entry_cleanup_early();
383 
384   {
385     ShenandoahHeapLocker locker(heap-&gt;lock());
386     heap-&gt;free_set()-&gt;log_status();
387   }
388 
389   // Perform concurrent class unloading
390   if (heap-&gt;is_concurrent_weak_root_in_progress()) {
391     heap-&gt;entry_class_unloading();
392   }
393 
394   // Processing strong roots
395   // This may be skipped if there is nothing to update/evacuate.
396   // If so, strong_root_in_progress would be unset.
397   if (heap-&gt;is_concurrent_strong_root_in_progress()) {
398     heap-&gt;entry_strong_roots();
399   }
400 
401   // Continue the cycle with evacuation and optional update-refs.
402   // This may be skipped if there is nothing to evacuate.
403   // If so, evac_in_progress would be unset by collection set preparation code.
404   if (heap-&gt;is_evacuation_in_progress()) {
405     // Concurrently evacuate
406     heap-&gt;entry_evac();
407     if (check_cancellation_or_degen(ShenandoahHeap::_degenerated_evac)) return;
408 
409     // Perform update-refs phase.
410     heap-&gt;vmop_entry_init_updaterefs();
411     heap-&gt;entry_updaterefs();
412     if (check_cancellation_or_degen(ShenandoahHeap::_degenerated_updaterefs)) return;
413 
414     heap-&gt;vmop_entry_final_updaterefs();
415 
416     // Update references freed up collection set, kick the cleanup to reclaim the space.
417     heap-&gt;entry_cleanup_complete();
418   }
419 
420   // Cycle is complete
421   heap-&gt;heuristics()-&gt;record_success_concurrent();
422   heap-&gt;shenandoah_policy()-&gt;record_success_concurrent();
423 }
424 
425 bool ShenandoahControlThread::check_cancellation_or_degen(ShenandoahHeap::ShenandoahDegenPoint point) {
426   ShenandoahHeap* heap = ShenandoahHeap::heap();
427   if (heap-&gt;cancelled_gc()) {
428     assert (is_alloc_failure_gc() || in_graceful_shutdown(), &quot;Cancel GC either for alloc failure GC, or gracefully exiting&quot;);
429     if (!in_graceful_shutdown()) {
430       assert (_degen_point == ShenandoahHeap::_degenerated_outside_cycle,
431               &quot;Should not be set yet: %s&quot;, ShenandoahHeap::degen_point_to_string(_degen_point));
432       _degen_point = point;
433     }
434     return true;
435   }
436   return false;
437 }
438 
439 void ShenandoahControlThread::stop_service() {
440   // Nothing to do here.
441 }
442 
443 void ShenandoahControlThread::service_stw_full_cycle(GCCause::Cause cause) {
444   GCIdMark gc_id_mark;
445   ShenandoahGCSession session(cause);
446 
447   ShenandoahHeap* heap = ShenandoahHeap::heap();
448   heap-&gt;vmop_entry_full(cause);
449 
450   heap-&gt;heuristics()-&gt;record_success_full();
451   heap-&gt;shenandoah_policy()-&gt;record_success_full();
452 }
453 
454 void ShenandoahControlThread::service_stw_degenerated_cycle(GCCause::Cause cause, ShenandoahHeap::ShenandoahDegenPoint point) {
455   assert (point != ShenandoahHeap::_degenerated_unset, &quot;Degenerated point should be set&quot;);
456 
457   GCIdMark gc_id_mark;
458   ShenandoahGCSession session(cause);
459 
460   ShenandoahHeap* heap = ShenandoahHeap::heap();
461   heap-&gt;vmop_degenerated(point);
462 
463   heap-&gt;heuristics()-&gt;record_success_degenerated();
464   heap-&gt;shenandoah_policy()-&gt;record_success_degenerated();
465 }
466 
467 void ShenandoahControlThread::service_uncommit(double shrink_before) {
468   ShenandoahHeap* heap = ShenandoahHeap::heap();
469 
470   // Determine if there is work to do. This avoids taking heap lock if there is
471   // no work available, avoids spamming logs with superfluous logging messages,
472   // and minimises the amount of work while locks are taken.
473 
474   if (heap-&gt;committed() &lt;= heap-&gt;min_capacity()) return;
475 
476   bool has_work = false;
477   for (size_t i = 0; i &lt; heap-&gt;num_regions(); i++) {
478     ShenandoahHeapRegion *r = heap-&gt;get_region(i);
479     if (r-&gt;is_empty_committed() &amp;&amp; (r-&gt;empty_time() &lt; shrink_before)) {
480       has_work = true;
481       break;
482     }
483   }
484 
485   if (has_work) {
486     heap-&gt;entry_uncommit(shrink_before);
487   }
488 }
489 
490 bool ShenandoahControlThread::is_explicit_gc(GCCause::Cause cause) const {
491   return GCCause::is_user_requested_gc(cause) ||
492          GCCause::is_serviceability_requested_gc(cause);
493 }
494 
495 void ShenandoahControlThread::request_gc(GCCause::Cause cause) {
496   assert(GCCause::is_user_requested_gc(cause) ||
497          GCCause::is_serviceability_requested_gc(cause) ||
498          cause == GCCause::_metadata_GC_clear_soft_refs ||
499          cause == GCCause::_full_gc_alot ||
500          cause == GCCause::_wb_full_gc ||
501          cause == GCCause::_scavenge_alot,
502          &quot;only requested GCs here&quot;);
503 
504   if (is_explicit_gc(cause)) {
505     if (!DisableExplicitGC) {
506       handle_requested_gc(cause);
507     }
508   } else {
509     handle_requested_gc(cause);
510   }
511 }
512 
513 void ShenandoahControlThread::handle_requested_gc(GCCause::Cause cause) {
514   // Make sure we have at least one complete GC cycle before unblocking
515   // from the explicit GC request.
516   //
517   // This is especially important for weak references cleanup and/or native
518   // resources (e.g. DirectByteBuffers) machinery: when explicit GC request
519   // comes very late in the already running cycle, it would miss lots of new
520   // opportunities for cleanup that were made available before the caller
521   // requested the GC.
522   size_t required_gc_id = get_gc_id() + 1;
523 
524   MonitorLocker ml(&amp;_gc_waiters_lock);
525   while (get_gc_id() &lt; required_gc_id) {
526     _gc_requested.set();
527     _requested_gc_cause = cause;
528     ml.wait();
529   }
530 }
531 
532 void ShenandoahControlThread::handle_alloc_failure(ShenandoahAllocRequest&amp; req) {
533   ShenandoahHeap* heap = ShenandoahHeap::heap();
534 
535   assert(current()-&gt;is_Java_thread(), &quot;expect Java thread here&quot;);
536 
537   if (try_set_alloc_failure_gc()) {
538     // Only report the first allocation failure
539     log_info(gc)(&quot;Failed to allocate %s, &quot; SIZE_FORMAT &quot;%s&quot;,
540                  req.type_string(),
541                  byte_size_in_proper_unit(req.size() * HeapWordSize), proper_unit_for_byte_size(req.size() * HeapWordSize));
542 
543     // Now that alloc failure GC is scheduled, we can abort everything else
544     heap-&gt;cancel_gc(GCCause::_allocation_failure);
545   }
546 
547   MonitorLocker ml(&amp;_alloc_failure_waiters_lock);
548   while (is_alloc_failure_gc()) {
549     ml.wait();
550   }
551 }
552 
553 void ShenandoahControlThread::handle_alloc_failure_evac(size_t words) {
554   ShenandoahHeap* heap = ShenandoahHeap::heap();
555 
556   if (try_set_alloc_failure_gc()) {
557     // Only report the first allocation failure
558     log_info(gc)(&quot;Failed to allocate &quot; SIZE_FORMAT &quot;%s for evacuation&quot;,
559                  byte_size_in_proper_unit(words * HeapWordSize), proper_unit_for_byte_size(words * HeapWordSize));
560   }
561 
562   // Forcefully report allocation failure
563   heap-&gt;cancel_gc(GCCause::_shenandoah_allocation_failure_evac);
564 }
565 
566 void ShenandoahControlThread::notify_alloc_failure_waiters() {
567   _alloc_failure_gc.unset();
568   MonitorLocker ml(&amp;_alloc_failure_waiters_lock);
569   ml.notify_all();
570 }
571 
572 bool ShenandoahControlThread::try_set_alloc_failure_gc() {
573   return _alloc_failure_gc.try_set();
574 }
575 
576 bool ShenandoahControlThread::is_alloc_failure_gc() {
577   return _alloc_failure_gc.is_set();
578 }
579 
580 void ShenandoahControlThread::notify_gc_waiters() {
581   _gc_requested.unset();
582   MonitorLocker ml(&amp;_gc_waiters_lock);
583   ml.notify_all();
584 }
585 
586 void ShenandoahControlThread::handle_counters_update() {
587   if (_do_counters_update.is_set()) {
588     _do_counters_update.unset();
589     ShenandoahHeap::heap()-&gt;monitoring_support()-&gt;update_counters();
590   }
591 }
592 
593 void ShenandoahControlThread::handle_force_counters_update() {
594   if (_force_counters_update.is_set()) {
595     _do_counters_update.unset(); // reset these too, we do update now!
596     ShenandoahHeap::heap()-&gt;monitoring_support()-&gt;update_counters();
597   }
598 }
599 
600 void ShenandoahControlThread::notify_heap_changed() {
601   // This is called from allocation path, and thus should be fast.
602 
603   // Update monitoring counters when we took a new region. This amortizes the
604   // update costs on slow path.
605   if (_do_counters_update.is_unset()) {
606     _do_counters_update.set();
607   }
608   // Notify that something had changed.
609   if (_heap_changed.is_unset()) {
610     _heap_changed.set();
611   }
612 }
613 
614 void ShenandoahControlThread::pacing_notify_alloc(size_t words) {
615   assert(ShenandoahPacing, &quot;should only call when pacing is enabled&quot;);
616   Atomic::add(&amp;_allocs_seen, words);
617 }
618 
619 void ShenandoahControlThread::set_forced_counters_update(bool value) {
620   _force_counters_update.set_cond(value);
621 }
622 
623 void ShenandoahControlThread::reset_gc_id() {
624   Atomic::store(&amp;_gc_id, (size_t)0);
625 }
626 
627 void ShenandoahControlThread::update_gc_id() {
628   Atomic::inc(&amp;_gc_id);
629 }
630 
631 size_t ShenandoahControlThread::get_gc_id() {
632   return Atomic::load(&amp;_gc_id);
633 }
634 
635 void ShenandoahControlThread::print() const {
636   print_on(tty);
637 }
638 
639 void ShenandoahControlThread::print_on(outputStream* st) const {
640   st-&gt;print(&quot;Shenandoah Concurrent Thread&quot;);
641   Thread::print_on(st);
642   st-&gt;cr();
643 }
644 
645 void ShenandoahControlThread::start() {
646   create_and_start();
647 }
648 
649 void ShenandoahControlThread::prepare_for_graceful_shutdown() {
650   _graceful_shutdown.set();
651 }
652 
653 bool ShenandoahControlThread::in_graceful_shutdown() {
654   return _graceful_shutdown.is_set();
655 }
<a name="3" id="anc3"></a><b style="font-size: large; color: red">--- EOF ---</b>
















































































</pre>
<input id="eof" value="3" type="hidden" />
</body>
</html>