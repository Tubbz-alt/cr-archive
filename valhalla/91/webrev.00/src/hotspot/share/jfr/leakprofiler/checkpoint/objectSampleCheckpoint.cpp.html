<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>New src/hotspot/share/jfr/leakprofiler/checkpoint/objectSampleCheckpoint.cpp</title>
    <link rel="stylesheet" href="../../../../../../style.css" />
  </head>
  <body>
    <pre>
  1 /*
  2  * Copyright (c) 2014, 2020, Oracle and/or its affiliates. All rights reserved.
  3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  4  *
  5  * This code is free software; you can redistribute it and/or modify it
  6  * under the terms of the GNU General Public License version 2 only, as
  7  * published by the Free Software Foundation.
  8  *
  9  * This code is distributed in the hope that it will be useful, but WITHOUT
 10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
 11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
 12  * version 2 for more details (a copy is included in the LICENSE file that
 13  * accompanied this code).
 14  *
 15  * You should have received a copy of the GNU General Public License version
 16  * 2 along with this work; if not, write to the Free Software Foundation,
 17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 18  *
 19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
 20  * or visit www.oracle.com if you need additional information or have any
 21  * questions.
 22  *
 23  */
 24 
 25 #include &quot;precompiled.hpp&quot;
 26 #include &quot;jfr/jfrEvents.hpp&quot;
 27 #include &quot;jfr/jni/jfrJavaSupport.hpp&quot;
 28 #include &quot;jfr/leakprofiler/chains/edgeStore.hpp&quot;
 29 #include &quot;jfr/leakprofiler/chains/objectSampleMarker.hpp&quot;
 30 #include &quot;jfr/leakprofiler/checkpoint/objectSampleCheckpoint.hpp&quot;
 31 #include &quot;jfr/leakprofiler/checkpoint/objectSampleWriter.hpp&quot;
 32 #include &quot;jfr/leakprofiler/leakProfiler.hpp&quot;
 33 #include &quot;jfr/leakprofiler/sampling/objectSample.hpp&quot;
 34 #include &quot;jfr/leakprofiler/sampling/objectSampler.hpp&quot;
 35 #include &quot;jfr/recorder/checkpoint/jfrCheckpointWriter.hpp&quot;
 36 #include &quot;jfr/recorder/checkpoint/types/traceid/jfrTraceId.inline.hpp&quot;
 37 #include &quot;jfr/recorder/service/jfrOptionSet.hpp&quot;
 38 #include &quot;jfr/recorder/stacktrace/jfrStackTraceRepository.hpp&quot;
 39 #include &quot;jfr/support/jfrKlassUnloading.hpp&quot;
 40 #include &quot;jfr/support/jfrMethodLookup.hpp&quot;
 41 #include &quot;jfr/utilities/jfrHashtable.hpp&quot;
 42 #include &quot;jfr/utilities/jfrPredicate.hpp&quot;
 43 #include &quot;jfr/utilities/jfrRelation.hpp&quot;
 44 #include &quot;memory/resourceArea.inline.hpp&quot;
 45 #include &quot;oops/instanceKlass.inline.hpp&quot;
 46 #include &quot;runtime/interfaceSupport.inline.hpp&quot;
 47 #include &quot;runtime/mutexLocker.hpp&quot;
 48 #include &quot;runtime/safepoint.hpp&quot;
 49 #include &quot;runtime/thread.inline.hpp&quot;
 50 
 51 const int initial_array_size = 64;
 52 
 53 template &lt;typename T&gt;
 54 static GrowableArray&lt;T&gt;* c_heap_allocate_array(int size = initial_array_size) {
 55   return new (ResourceObj::C_HEAP, mtTracing) GrowableArray&lt;T&gt;(size, mtTracing);
 56 }
 57 
 58 static GrowableArray&lt;traceid&gt;* unloaded_thread_id_set = NULL;
 59 
 60 class ThreadIdExclusiveAccess : public StackObj {
 61  private:
 62   static Semaphore _mutex_semaphore;
 63  public:
 64   ThreadIdExclusiveAccess() { _mutex_semaphore.wait(); }
 65   ~ThreadIdExclusiveAccess() { _mutex_semaphore.signal(); }
 66 };
 67 
 68 Semaphore ThreadIdExclusiveAccess::_mutex_semaphore(1);
 69 
 70 static bool has_thread_exited(traceid tid) {
 71   assert(tid != 0, &quot;invariant&quot;);
 72   return unloaded_thread_id_set != NULL &amp;&amp; JfrPredicate&lt;traceid, compare_traceid&gt;::test(unloaded_thread_id_set, tid);
 73 }
 74 
 75 static bool add(GrowableArray&lt;traceid&gt;* set, traceid id) {
 76   assert(set != NULL, &quot;invariant&quot;);
 77   return JfrMutablePredicate&lt;traceid, compare_traceid&gt;::test(set, id);
 78 }
 79 
 80 static void add_to_unloaded_thread_set(traceid tid) {
 81   ThreadIdExclusiveAccess lock;
 82   if (unloaded_thread_id_set == NULL) {
 83     unloaded_thread_id_set = c_heap_allocate_array&lt;traceid&gt;();
 84   }
 85   add(unloaded_thread_id_set, tid);
 86 }
 87 
 88 void ObjectSampleCheckpoint::on_thread_exit(JavaThread* jt) {
 89   assert(jt != NULL, &quot;invariant&quot;);
 90   if (LeakProfiler::is_running()) {
 91     add_to_unloaded_thread_set(jt-&gt;jfr_thread_local()-&gt;thread_id());
 92   }
 93 }
 94 
 95 template &lt;typename Processor&gt;
 96 static void do_samples(ObjectSample* sample, const ObjectSample* end, Processor&amp; processor) {
 97   assert(sample != NULL, &quot;invariant&quot;);
 98   while (sample != end) {
 99     processor.sample_do(sample);
100     sample = sample-&gt;next();
101   }
102 }
103 
104 template &lt;typename Processor&gt;
105 static void iterate_samples(Processor&amp; processor, bool all = false) {
106   ObjectSampler* const sampler = ObjectSampler::sampler();
107   assert(sampler != NULL, &quot;invariant&quot;);
108   ObjectSample* const last = sampler-&gt;last();
109   assert(last != NULL, &quot;invariant&quot;);
110   do_samples(last, all ? NULL : sampler-&gt;last_resolved(), processor);
111 }
112 
113 class SampleMarker {
114  private:
115   ObjectSampleMarker&amp; _marker;
116   jlong _last_sweep;
117   int _count;
118  public:
119   SampleMarker(ObjectSampleMarker&amp; marker, jlong last_sweep) : _marker(marker), _last_sweep(last_sweep), _count(0) {}
120   void sample_do(ObjectSample* sample) {
121     if (sample-&gt;is_alive_and_older_than(_last_sweep)) {
122       _marker.mark(sample-&gt;object());
123       ++_count;
124     }
125   }
126   int count() const {
127     return _count;
128   }
129 };
130 
131 int ObjectSampleCheckpoint::save_mark_words(const ObjectSampler* sampler, ObjectSampleMarker&amp; marker, bool emit_all) {
132   assert(sampler != NULL, &quot;invariant&quot;);
133   if (sampler-&gt;last() == NULL) {
134     return 0;
135   }
136   SampleMarker sample_marker(marker, emit_all ? max_jlong : sampler-&gt;last_sweep().value());
137   iterate_samples(sample_marker, true);
138   return sample_marker.count();
139 }
140 
141 class BlobCache {
142   typedef HashTableHost&lt;JfrBlobHandle, traceid, JfrHashtableEntry, BlobCache&gt; BlobTable;
143   typedef BlobTable::HashEntry BlobEntry;
144  private:
145   BlobTable _table;
146   traceid _lookup_id;
147  public:
148   BlobCache(size_t size) : _table(this, size), _lookup_id(0) {}
149   JfrBlobHandle get(const ObjectSample* sample);
150   void put(const ObjectSample* sample, const JfrBlobHandle&amp; blob);
151   // Hash table callbacks
152   void on_link(const BlobEntry* entry) const;
153   bool on_equals(uintptr_t hash, const BlobEntry* entry) const;
154   void on_unlink(BlobEntry* entry) const;
155 };
156 
157 JfrBlobHandle BlobCache::get(const ObjectSample* sample) {
158   assert(sample != NULL, &quot;invariant&quot;);
159   _lookup_id = sample-&gt;stack_trace_id();
160   assert(_lookup_id != 0, &quot;invariant&quot;);
161   BlobEntry* const entry = _table.lookup_only(sample-&gt;stack_trace_hash());
162   return entry != NULL ? entry-&gt;literal() : JfrBlobHandle();
163 }
164 
165 void BlobCache::put(const ObjectSample* sample, const JfrBlobHandle&amp; blob) {
166   assert(sample != NULL, &quot;invariant&quot;);
167   assert(_table.lookup_only(sample-&gt;stack_trace_hash()) == NULL, &quot;invariant&quot;);
168   _lookup_id = sample-&gt;stack_trace_id();
169   assert(_lookup_id != 0, &quot;invariant&quot;);
170   _table.put(sample-&gt;stack_trace_hash(), blob);
171 }
172 
173 inline void BlobCache::on_link(const BlobEntry* entry) const {
174   assert(entry != NULL, &quot;invariant&quot;);
175   assert(entry-&gt;id() == 0, &quot;invariant&quot;);
176   entry-&gt;set_id(_lookup_id);
177 }
178 
179 inline bool BlobCache::on_equals(uintptr_t hash, const BlobEntry* entry) const {
180   assert(entry != NULL, &quot;invariant&quot;);
181   assert(entry-&gt;hash() == hash, &quot;invariant&quot;);
182   return entry-&gt;id() == _lookup_id;
183 }
184 
185 inline void BlobCache::on_unlink(BlobEntry* entry) const {
186   assert(entry != NULL, &quot;invariant&quot;);
187 }
188 
189 static GrowableArray&lt;traceid&gt;* id_set = NULL;
190 
191 static void prepare_for_resolution() {
192   id_set = new GrowableArray&lt;traceid&gt;(JfrOptionSet::old_object_queue_size());
193 }
194 
195 static bool stack_trace_precondition(const ObjectSample* sample) {
196   assert(sample != NULL, &quot;invariant&quot;);
197   return sample-&gt;has_stack_trace_id() &amp;&amp; !sample-&gt;is_dead();
198 }
199 
200 class StackTraceBlobInstaller {
201  private:
202   const JfrStackTraceRepository&amp; _stack_trace_repo;
203   BlobCache _cache;
204   const JfrStackTrace* resolve(const ObjectSample* sample);
205   void install(ObjectSample* sample);
206  public:
207   StackTraceBlobInstaller(const JfrStackTraceRepository&amp; stack_trace_repo);
208   void sample_do(ObjectSample* sample) {
209     if (stack_trace_precondition(sample)) {
210       install(sample);
211     }
212   }
213 };
214 
215 StackTraceBlobInstaller::StackTraceBlobInstaller(const JfrStackTraceRepository&amp; stack_trace_repo) :
216   _stack_trace_repo(stack_trace_repo), _cache(JfrOptionSet::old_object_queue_size()) {
217   prepare_for_resolution();
218 }
219 
220 const JfrStackTrace* StackTraceBlobInstaller::resolve(const ObjectSample* sample) {
221   return _stack_trace_repo.lookup(sample-&gt;stack_trace_hash(), sample-&gt;stack_trace_id());
222 }
223 
224 #ifdef ASSERT
225 static void validate_stack_trace(const ObjectSample* sample, const JfrStackTrace* stack_trace) {
226   assert(!sample-&gt;has_stacktrace(), &quot;invariant&quot;);
227   assert(stack_trace != NULL, &quot;invariant&quot;);
228   assert(stack_trace-&gt;hash() == sample-&gt;stack_trace_hash(), &quot;invariant&quot;);
229   assert(stack_trace-&gt;id() == sample-&gt;stack_trace_id(), &quot;invariant&quot;);
230 }
231 #endif
232 
233 void StackTraceBlobInstaller::install(ObjectSample* sample) {
234   JfrBlobHandle blob = _cache.get(sample);
235   if (blob.valid()) {
236     sample-&gt;set_stacktrace(blob);
237     return;
238   }
239   const JfrStackTrace* const stack_trace = resolve(sample);
240   DEBUG_ONLY(validate_stack_trace(sample, stack_trace));
241   JfrCheckpointWriter writer;
242   writer.write_type(TYPE_STACKTRACE);
243   writer.write_count(1);
244   ObjectSampleCheckpoint::write_stacktrace(stack_trace, writer);
245   blob = writer.move();
246   _cache.put(sample, blob);
247   sample-&gt;set_stacktrace(blob);
248 }
249 
250 static void install_stack_traces(const ObjectSampler* sampler, JfrStackTraceRepository&amp; stack_trace_repo) {
251   assert(sampler != NULL, &quot;invariant&quot;);
252   const ObjectSample* const last = sampler-&gt;last();
253   if (last != sampler-&gt;last_resolved()) {
254     ResourceMark rm;
255     JfrKlassUnloading::sort();
256     StackTraceBlobInstaller installer(stack_trace_repo);
257     iterate_samples(installer);
258   }
259 }
260 
261 void ObjectSampleCheckpoint::on_rotation(const ObjectSampler* sampler, JfrStackTraceRepository&amp; stack_trace_repo) {
262   assert(sampler != NULL, &quot;invariant&quot;);
263   assert(LeakProfiler::is_running(), &quot;invariant&quot;);
264   Thread* const thread = Thread::current();
265   DEBUG_ONLY(JfrJavaSupport::check_java_thread_in_native(thread);)
266   // can safepoint here
267   ThreadInVMfromNative transition((JavaThread*)thread);
268   MutexLocker lock(ClassLoaderDataGraph_lock);
269   // the lock is needed to ensure the unload lists do not grow in the middle of inspection.
270   install_stack_traces(sampler, stack_trace_repo);
271 }
272 
273 static bool is_klass_unloaded(traceid klass_id) {
274   assert(ClassLoaderDataGraph_lock-&gt;owned_by_self(), &quot;invariant&quot;);
275   return JfrKlassUnloading::is_unloaded(klass_id);
276 }
277 
278 static bool is_processed(traceid method_id) {
279   assert(method_id != 0, &quot;invariant&quot;);
280   assert(id_set != NULL, &quot;invariant&quot;);
281   return JfrMutablePredicate&lt;traceid, compare_traceid&gt;::test(id_set, method_id);
282 }
283 
284 void ObjectSampleCheckpoint::add_to_leakp_set(const InstanceKlass* ik, traceid method_id) {
285   assert(ik != NULL, &quot;invariant&quot;);
286   if (is_processed(method_id) || is_klass_unloaded(JfrMethodLookup::klass_id(method_id))) {
287     return;
288   }
289   const Method* const method = JfrMethodLookup::lookup(ik, method_id);
290   assert(method != NULL, &quot;invariant&quot;);
291   assert(method-&gt;method_holder() == ik, &quot;invariant&quot;);
292   JfrTraceId::load_leakp(ik, method);
293 }
294 
295 void ObjectSampleCheckpoint::write_stacktrace(const JfrStackTrace* trace, JfrCheckpointWriter&amp; writer) {
296   assert(trace != NULL, &quot;invariant&quot;);
297   // JfrStackTrace
298   writer.write(trace-&gt;id());
299   writer.write((u1)!trace-&gt;_reached_root);
300   writer.write(trace-&gt;_nr_of_frames);
301   // JfrStackFrames
302   for (u4 i = 0; i &lt; trace-&gt;_nr_of_frames; ++i) {
303     const JfrStackFrame&amp; frame = trace-&gt;_frames[i];
304     frame.write(writer);
305     add_to_leakp_set(frame._klass, frame._methodid);
306   }
307 }
308 
309 static void write_blob(const JfrBlobHandle&amp; blob, JfrCheckpointWriter&amp; writer, bool reset) {
310   if (reset) {
311     blob-&gt;reset_write_state();
312     return;
313   }
314   blob-&gt;exclusive_write(writer);
315 }
316 
317 static void write_type_set_blob(const ObjectSample* sample, JfrCheckpointWriter&amp; writer, bool reset) {
318   if (sample-&gt;has_type_set()) {
319     write_blob(sample-&gt;type_set(), writer, reset);
320   }
321 }
322 
323 static void write_thread_blob(const ObjectSample* sample, JfrCheckpointWriter&amp; writer, bool reset) {
324   assert(sample-&gt;has_thread(), &quot;invariant&quot;);
325   if (has_thread_exited(sample-&gt;thread_id())) {
326     write_blob(sample-&gt;thread(), writer, reset);
327   }
328 }
329 
330 static void write_stacktrace_blob(const ObjectSample* sample, JfrCheckpointWriter&amp; writer, bool reset) {
331   if (sample-&gt;has_stacktrace()) {
332     write_blob(sample-&gt;stacktrace(), writer, reset);
333   }
334 }
335 
336 static void write_blobs(const ObjectSample* sample, JfrCheckpointWriter&amp; writer, bool reset) {
337   assert(sample != NULL, &quot;invariant&quot;);
338   write_stacktrace_blob(sample, writer, reset);
339   write_thread_blob(sample, writer, reset);
340   write_type_set_blob(sample, writer, reset);
341 }
342 
343 class BlobWriter {
344  private:
345   const ObjectSampler* _sampler;
346   JfrCheckpointWriter&amp; _writer;
347   const jlong _last_sweep;
348   bool _reset;
349  public:
350   BlobWriter(const ObjectSampler* sampler, JfrCheckpointWriter&amp; writer, jlong last_sweep) :
351     _sampler(sampler), _writer(writer), _last_sweep(last_sweep), _reset(false)  {}
352   void sample_do(ObjectSample* sample) {
353     if (sample-&gt;is_alive_and_older_than(_last_sweep)) {
354       write_blobs(sample, _writer, _reset);
355     }
356   }
357   void set_reset() {
358     _reset = true;
359   }
360 };
361 
362 static void write_sample_blobs(const ObjectSampler* sampler, bool emit_all, Thread* thread) {
363   // sample set is predicated on time of last sweep
364   const jlong last_sweep = emit_all ? max_jlong : sampler-&gt;last_sweep().value();
365   JfrCheckpointWriter writer(thread, false);
366   BlobWriter cbw(sampler, writer, last_sweep);
367   iterate_samples(cbw, true);
368   // reset blob write states
369   cbw.set_reset();
370   iterate_samples(cbw, true);
371 }
372 
373 void ObjectSampleCheckpoint::write(const ObjectSampler* sampler, EdgeStore* edge_store, bool emit_all, Thread* thread) {
374   assert(sampler != NULL, &quot;invariant&quot;);
375   assert(edge_store != NULL, &quot;invariant&quot;);
376   assert(thread != NULL, &quot;invariant&quot;);
377   write_sample_blobs(sampler, emit_all, thread);
378   // write reference chains
379   if (!edge_store-&gt;is_empty()) {
380     JfrCheckpointWriter writer(thread);
381     ObjectSampleWriter osw(writer, edge_store);
382     edge_store-&gt;iterate(osw);
383   }
384 }
385 
386 // A linked list of saved type set blobs for the epoch.
387 // The link consist of a reference counted handle.
388 static JfrBlobHandle saved_type_set_blobs;
389 
390 static void release_state_for_previous_epoch() {
391   // decrements the reference count and the list is reinitialized
392   saved_type_set_blobs = JfrBlobHandle();
393 }
394 
395 class BlobInstaller {
396  public:
397   ~BlobInstaller() {
398     release_state_for_previous_epoch();
399   }
400   void sample_do(ObjectSample* sample) {
401     if (!sample-&gt;is_dead()) {
402       sample-&gt;set_type_set(saved_type_set_blobs);
403     }
404   }
405 };
406 
407 static void install_type_set_blobs() {
408   BlobInstaller installer;
409   iterate_samples(installer);
410 }
411 
412 static void save_type_set_blob(JfrCheckpointWriter&amp; writer, bool copy = false) {
413   assert(writer.has_data(), &quot;invariant&quot;);
414   const JfrBlobHandle blob = copy ? writer.copy() : writer.move();
415   if (saved_type_set_blobs.valid()) {
416     saved_type_set_blobs-&gt;set_next(blob);
417   } else {
418     saved_type_set_blobs = blob;
419   }
420 }
421 
422 void ObjectSampleCheckpoint::on_type_set(JfrCheckpointWriter&amp; writer) {
423   assert(LeakProfiler::is_running(), &quot;invariant&quot;);
424   DEBUG_ONLY(JfrJavaSupport::check_java_thread_in_vm(Thread::current());)
425   const ObjectSample* last = ObjectSampler::sampler()-&gt;last();
426   if (writer.has_data() &amp;&amp; last != NULL) {
427     save_type_set_blob(writer);
428     install_type_set_blobs();
429     ObjectSampler::sampler()-&gt;set_last_resolved(last);
430   }
431 }
432 
433 void ObjectSampleCheckpoint::on_type_set_unload(JfrCheckpointWriter&amp; writer) {
434   assert_locked_or_safepoint(ClassLoaderDataGraph_lock);
435   assert(LeakProfiler::is_running(), &quot;invariant&quot;);
436   if (writer.has_data() &amp;&amp; ObjectSampler::sampler()-&gt;last() != NULL) {
437     save_type_set_blob(writer, true);
438   }
439 }
    </pre>
  </body>
</html>