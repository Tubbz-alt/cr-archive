<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/cpu/aarch64/sharedRuntime_aarch64.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="macroAssembler_aarch64.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="../../os/bsd/globals_bsd.hpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/cpu/aarch64/sharedRuntime_aarch64.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
1388 
1389     void link(GrowableArray&lt;MoveOperation*&gt;&amp; killer) { Unimplemented(); }
1390   };
1391 
1392  private:
1393   GrowableArray&lt;MoveOperation*&gt; edges;
1394 
1395  public:
1396   ComputeMoveOrder(int total_in_args, VMRegPair* in_regs, int total_c_args, VMRegPair* out_regs,
1397                     BasicType* in_sig_bt, GrowableArray&lt;int&gt;&amp; arg_order, VMRegPair tmp_vmreg) { Unimplemented(); }
1398 
1399   // Collected all the move operations
1400   void add_edge(int src_index, VMRegPair src, int dst_index, VMRegPair dst) { Unimplemented(); }
1401 
1402   // Walk the edges breaking cycles between moves.  The result list
1403   // can be walked in order to produce the proper set of loads
1404   GrowableArray&lt;MoveOperation*&gt;* get_store_order(VMRegPair temp_register) { Unimplemented(); return 0; }
1405 };
1406 
1407 
<span class="line-modified">1408 static void rt_call(MacroAssembler* masm, address dest, int gpargs, int fpargs, int type) {</span>
1409   CodeBlob *cb = CodeCache::find_blob(dest);
1410   if (cb) {
1411     __ far_call(RuntimeAddress(dest));
1412   } else {
<span class="line-removed">1413     assert((unsigned)gpargs &lt; 256, &quot;eek!&quot;);</span>
<span class="line-removed">1414     assert((unsigned)fpargs &lt; 32, &quot;eek!&quot;);</span>
1415     __ lea(rscratch1, RuntimeAddress(dest));
1416     __ blr(rscratch1);
1417     __ maybe_isb();
1418   }
1419 }
1420 
1421 static void verify_oop_args(MacroAssembler* masm,
1422                             const methodHandle&amp; method,
1423                             const BasicType* sig_bt,
1424                             const VMRegPair* regs) {
1425   Register temp_reg = r19;  // not part of any compiled calling seq
1426   if (VerifyOops) {
1427     for (int i = 0; i &lt; method-&gt;size_of_parameters(); i++) {
1428       if (sig_bt[i] == T_OBJECT ||
1429           sig_bt[i] == T_ARRAY) {
1430         VMReg r = regs[i].first();
1431         assert(r-&gt;is_valid(), &quot;bad oop arg&quot;);
1432         if (r-&gt;is_stack()) {
1433           __ ldr(temp_reg, Address(sp, r-&gt;reg2stack() * VMRegImpl::stack_slot_size));
1434           __ verify_oop(temp_reg);
</pre>
<hr />
<pre>
2063     __ br(Assembler::NE, slow_path_lock);
2064 
2065     // Slow path will re-enter here
2066 
2067     __ bind(lock_done);
2068   }
2069 
2070 
2071   // Finally just about ready to make the JNI call
2072 
2073   // get JNIEnv* which is first argument to native
2074   if (!is_critical_native) {
2075     __ lea(c_rarg0, Address(rthread, in_bytes(JavaThread::jni_environment_offset())));
2076   }
2077 
2078   // Now set thread in native
2079   __ mov(rscratch1, _thread_in_native);
2080   __ lea(rscratch2, Address(rthread, JavaThread::thread_state_offset()));
2081   __ stlrw(rscratch1, rscratch2);
2082 
<span class="line-modified">2083   {</span>
<span class="line-removed">2084     int return_type = 0;</span>
<span class="line-removed">2085     switch (ret_type) {</span>
<span class="line-removed">2086     case T_VOID: break;</span>
<span class="line-removed">2087       return_type = 0; break;</span>
<span class="line-removed">2088     case T_CHAR:</span>
<span class="line-removed">2089     case T_BYTE:</span>
<span class="line-removed">2090     case T_SHORT:</span>
<span class="line-removed">2091     case T_INT:</span>
<span class="line-removed">2092     case T_BOOLEAN:</span>
<span class="line-removed">2093     case T_LONG:</span>
<span class="line-removed">2094       return_type = 1; break;</span>
<span class="line-removed">2095     case T_ARRAY:</span>
<span class="line-removed">2096     case T_VALUETYPE:</span>
<span class="line-removed">2097     case T_OBJECT:</span>
<span class="line-removed">2098       return_type = 1; break;</span>
<span class="line-removed">2099     case T_FLOAT:</span>
<span class="line-removed">2100       return_type = 2; break;</span>
<span class="line-removed">2101     case T_DOUBLE:</span>
<span class="line-removed">2102       return_type = 3; break;</span>
<span class="line-removed">2103     default:</span>
<span class="line-removed">2104       ShouldNotReachHere();</span>
<span class="line-removed">2105     }</span>
<span class="line-removed">2106     rt_call(masm, native_func,</span>
<span class="line-removed">2107             int_args + 2, // AArch64 passes up to 8 args in int registers</span>
<span class="line-removed">2108             float_args,   // and up to 8 float args</span>
<span class="line-removed">2109             return_type);</span>
<span class="line-removed">2110   }</span>
2111 
2112   __ bind(native_return);
2113 
2114   intptr_t return_pc = (intptr_t) __ pc();
2115   oop_maps-&gt;add_gc_map(return_pc - start, map);
2116 
2117   // Unpack native results.
2118   switch (ret_type) {
2119   case T_BOOLEAN: __ c2bool(r0);                     break;
2120   case T_CHAR   : __ ubfx(r0, r0, 0, 16);            break;
2121   case T_BYTE   : __ sbfx(r0, r0, 0, 8);             break;
2122   case T_SHORT  : __ sbfx(r0, r0, 0, 16);            break;
2123   case T_INT    : __ sbfx(r0, r0, 0, 32);            break;
2124   case T_DOUBLE :
2125   case T_FLOAT  :
2126     // Result is in v0 we&#39;ll save as needed
2127     break;
2128   case T_ARRAY:                 // Really a handle
2129   case T_VALUETYPE:
2130   case T_OBJECT:                // Really a handle
</pre>
<hr />
<pre>
2301 
2302     __ block_comment(&quot;Slow path unlock {&quot;);
2303     __ bind(slow_path_unlock);
2304 
2305     // If we haven&#39;t already saved the native result we must save it now as xmm registers
2306     // are still exposed.
2307 
2308     if (ret_type == T_FLOAT || ret_type == T_DOUBLE ) {
2309       save_native_result(masm, ret_type, stack_slots);
2310     }
2311 
2312     __ mov(c_rarg2, rthread);
2313     __ lea(c_rarg1, Address(sp, lock_slot_offset * VMRegImpl::stack_slot_size));
2314     __ mov(c_rarg0, obj_reg);
2315 
2316     // Save pending exception around call to VM (which contains an EXCEPTION_MARK)
2317     // NOTE that obj_reg == r19 currently
2318     __ ldr(r19, Address(rthread, in_bytes(Thread::pending_exception_offset())));
2319     __ str(zr, Address(rthread, in_bytes(Thread::pending_exception_offset())));
2320 
<span class="line-modified">2321     rt_call(masm, CAST_FROM_FN_PTR(address, SharedRuntime::complete_monitor_unlocking_C), 3, 0, 1);</span>
2322 
2323 #ifdef ASSERT
2324     {
2325       Label L;
2326       __ ldr(rscratch1, Address(rthread, in_bytes(Thread::pending_exception_offset())));
2327       __ cbz(rscratch1, L);
2328       __ stop(&quot;no pending exception allowed on exit complete_monitor_unlocking_C&quot;);
2329       __ bind(L);
2330     }
2331 #endif /* ASSERT */
2332 
2333     __ str(r19, Address(rthread, in_bytes(Thread::pending_exception_offset())));
2334 
2335     if (ret_type == T_FLOAT || ret_type == T_DOUBLE ) {
2336       restore_native_result(masm, ret_type, stack_slots);
2337     }
2338     __ b(unlock_done);
2339 
2340     __ block_comment(&quot;} Slow path unlock&quot;);
2341 
2342   } // synchronized
2343 
2344   // SLOW PATH Reguard the stack if needed
2345 
2346   __ bind(reguard);
2347   save_native_result(masm, ret_type, stack_slots);
<span class="line-modified">2348   rt_call(masm, CAST_FROM_FN_PTR(address, SharedRuntime::reguard_yellow_pages), 0, 0, 0);</span>
2349   restore_native_result(masm, ret_type, stack_slots);
2350   // and continue
2351   __ b(reguard_done);
2352 
2353   // SLOW PATH safepoint
2354   {
2355     __ block_comment(&quot;safepoint {&quot;);
2356     __ bind(safepoint_in_progress);
2357 
2358     // Don&#39;t use call_VM as it will see a possible pending exception and forward it
2359     // and never return here preventing us from clearing _last_native_pc down below.
2360     //
2361     save_native_result(masm, ret_type, stack_slots);
2362     __ mov(c_rarg0, rthread);
2363 #ifndef PRODUCT
2364   assert(frame::arg_reg_save_area_bytes == 0, &quot;not expecting frame reg save area&quot;);
2365 #endif
2366     if (!is_critical_native) {
2367       __ lea(rscratch1, RuntimeAddress(CAST_FROM_FN_PTR(address, JavaThread::check_special_condition_for_native_trans)));
2368     } else {
</pre>
</td>
<td>
<hr />
<pre>
1388 
1389     void link(GrowableArray&lt;MoveOperation*&gt;&amp; killer) { Unimplemented(); }
1390   };
1391 
1392  private:
1393   GrowableArray&lt;MoveOperation*&gt; edges;
1394 
1395  public:
1396   ComputeMoveOrder(int total_in_args, VMRegPair* in_regs, int total_c_args, VMRegPair* out_regs,
1397                     BasicType* in_sig_bt, GrowableArray&lt;int&gt;&amp; arg_order, VMRegPair tmp_vmreg) { Unimplemented(); }
1398 
1399   // Collected all the move operations
1400   void add_edge(int src_index, VMRegPair src, int dst_index, VMRegPair dst) { Unimplemented(); }
1401 
1402   // Walk the edges breaking cycles between moves.  The result list
1403   // can be walked in order to produce the proper set of loads
1404   GrowableArray&lt;MoveOperation*&gt;* get_store_order(VMRegPair temp_register) { Unimplemented(); return 0; }
1405 };
1406 
1407 
<span class="line-modified">1408 static void rt_call(MacroAssembler* masm, address dest) {</span>
1409   CodeBlob *cb = CodeCache::find_blob(dest);
1410   if (cb) {
1411     __ far_call(RuntimeAddress(dest));
1412   } else {


1413     __ lea(rscratch1, RuntimeAddress(dest));
1414     __ blr(rscratch1);
1415     __ maybe_isb();
1416   }
1417 }
1418 
1419 static void verify_oop_args(MacroAssembler* masm,
1420                             const methodHandle&amp; method,
1421                             const BasicType* sig_bt,
1422                             const VMRegPair* regs) {
1423   Register temp_reg = r19;  // not part of any compiled calling seq
1424   if (VerifyOops) {
1425     for (int i = 0; i &lt; method-&gt;size_of_parameters(); i++) {
1426       if (sig_bt[i] == T_OBJECT ||
1427           sig_bt[i] == T_ARRAY) {
1428         VMReg r = regs[i].first();
1429         assert(r-&gt;is_valid(), &quot;bad oop arg&quot;);
1430         if (r-&gt;is_stack()) {
1431           __ ldr(temp_reg, Address(sp, r-&gt;reg2stack() * VMRegImpl::stack_slot_size));
1432           __ verify_oop(temp_reg);
</pre>
<hr />
<pre>
2061     __ br(Assembler::NE, slow_path_lock);
2062 
2063     // Slow path will re-enter here
2064 
2065     __ bind(lock_done);
2066   }
2067 
2068 
2069   // Finally just about ready to make the JNI call
2070 
2071   // get JNIEnv* which is first argument to native
2072   if (!is_critical_native) {
2073     __ lea(c_rarg0, Address(rthread, in_bytes(JavaThread::jni_environment_offset())));
2074   }
2075 
2076   // Now set thread in native
2077   __ mov(rscratch1, _thread_in_native);
2078   __ lea(rscratch2, Address(rthread, JavaThread::thread_state_offset()));
2079   __ stlrw(rscratch1, rscratch2);
2080 
<span class="line-modified">2081   rt_call(masm, native_func);</span>



























2082 
2083   __ bind(native_return);
2084 
2085   intptr_t return_pc = (intptr_t) __ pc();
2086   oop_maps-&gt;add_gc_map(return_pc - start, map);
2087 
2088   // Unpack native results.
2089   switch (ret_type) {
2090   case T_BOOLEAN: __ c2bool(r0);                     break;
2091   case T_CHAR   : __ ubfx(r0, r0, 0, 16);            break;
2092   case T_BYTE   : __ sbfx(r0, r0, 0, 8);             break;
2093   case T_SHORT  : __ sbfx(r0, r0, 0, 16);            break;
2094   case T_INT    : __ sbfx(r0, r0, 0, 32);            break;
2095   case T_DOUBLE :
2096   case T_FLOAT  :
2097     // Result is in v0 we&#39;ll save as needed
2098     break;
2099   case T_ARRAY:                 // Really a handle
2100   case T_VALUETYPE:
2101   case T_OBJECT:                // Really a handle
</pre>
<hr />
<pre>
2272 
2273     __ block_comment(&quot;Slow path unlock {&quot;);
2274     __ bind(slow_path_unlock);
2275 
2276     // If we haven&#39;t already saved the native result we must save it now as xmm registers
2277     // are still exposed.
2278 
2279     if (ret_type == T_FLOAT || ret_type == T_DOUBLE ) {
2280       save_native_result(masm, ret_type, stack_slots);
2281     }
2282 
2283     __ mov(c_rarg2, rthread);
2284     __ lea(c_rarg1, Address(sp, lock_slot_offset * VMRegImpl::stack_slot_size));
2285     __ mov(c_rarg0, obj_reg);
2286 
2287     // Save pending exception around call to VM (which contains an EXCEPTION_MARK)
2288     // NOTE that obj_reg == r19 currently
2289     __ ldr(r19, Address(rthread, in_bytes(Thread::pending_exception_offset())));
2290     __ str(zr, Address(rthread, in_bytes(Thread::pending_exception_offset())));
2291 
<span class="line-modified">2292     rt_call(masm, CAST_FROM_FN_PTR(address, SharedRuntime::complete_monitor_unlocking_C));</span>
2293 
2294 #ifdef ASSERT
2295     {
2296       Label L;
2297       __ ldr(rscratch1, Address(rthread, in_bytes(Thread::pending_exception_offset())));
2298       __ cbz(rscratch1, L);
2299       __ stop(&quot;no pending exception allowed on exit complete_monitor_unlocking_C&quot;);
2300       __ bind(L);
2301     }
2302 #endif /* ASSERT */
2303 
2304     __ str(r19, Address(rthread, in_bytes(Thread::pending_exception_offset())));
2305 
2306     if (ret_type == T_FLOAT || ret_type == T_DOUBLE ) {
2307       restore_native_result(masm, ret_type, stack_slots);
2308     }
2309     __ b(unlock_done);
2310 
2311     __ block_comment(&quot;} Slow path unlock&quot;);
2312 
2313   } // synchronized
2314 
2315   // SLOW PATH Reguard the stack if needed
2316 
2317   __ bind(reguard);
2318   save_native_result(masm, ret_type, stack_slots);
<span class="line-modified">2319   rt_call(masm, CAST_FROM_FN_PTR(address, SharedRuntime::reguard_yellow_pages));</span>
2320   restore_native_result(masm, ret_type, stack_slots);
2321   // and continue
2322   __ b(reguard_done);
2323 
2324   // SLOW PATH safepoint
2325   {
2326     __ block_comment(&quot;safepoint {&quot;);
2327     __ bind(safepoint_in_progress);
2328 
2329     // Don&#39;t use call_VM as it will see a possible pending exception and forward it
2330     // and never return here preventing us from clearing _last_native_pc down below.
2331     //
2332     save_native_result(masm, ret_type, stack_slots);
2333     __ mov(c_rarg0, rthread);
2334 #ifndef PRODUCT
2335   assert(frame::arg_reg_save_area_bytes == 0, &quot;not expecting frame reg save area&quot;);
2336 #endif
2337     if (!is_critical_native) {
2338       __ lea(rscratch1, RuntimeAddress(CAST_FROM_FN_PTR(address, JavaThread::check_special_condition_for_native_trans)));
2339     } else {
</pre>
</td>
</tr>
</table>
<center><a href="macroAssembler_aarch64.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="../../os/bsd/globals_bsd.hpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>