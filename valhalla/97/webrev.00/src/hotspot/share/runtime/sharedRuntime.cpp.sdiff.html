<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/share/runtime/sharedRuntime.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="../opto/valuetypenode.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="../../../../test/hotspot/jtreg/compiler/valhalla/valuetypes/TestCallingConvention.java.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/share/runtime/sharedRuntime.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
1125   if (has_receiver) {
1126     // This register map must be update since we need to find the receiver for
1127     // compiled frames. The receiver might be in a register.
1128     RegisterMap reg_map2(thread);
1129     frame stubFrame   = thread-&gt;last_frame();
1130     // Caller-frame is a compiled frame
1131     frame callerFrame = stubFrame.sender(&amp;reg_map2);
1132     bool caller_is_c1 = false;
1133 
1134     if (callerFrame.is_compiled_frame() &amp;&amp; !callerFrame.is_deoptimized_frame()) {
1135       caller_is_c1 = callerFrame.cb()-&gt;is_compiled_by_c1();
1136     }
1137 
1138     Method* callee = attached_method();
1139     if (callee == NULL) {
1140       callee = bytecode.static_target(CHECK_NH);
1141       if (callee == NULL) {
1142         THROW_(vmSymbols::java_lang_NoSuchMethodException(), nullHandle);
1143       }
1144     }
<span class="line-modified">1145     if (!caller_is_c1 &amp;&amp; callee-&gt;has_scalarized_args() &amp;&amp; callee-&gt;method_holder()-&gt;is_inline_klass()) {</span>
<span class="line-modified">1146       // If the receiver is a value type that is passed as fields, no oop is available.</span>

1147       // Resolve the call without receiver null checking.
1148       assert(attached_method.not_null() &amp;&amp; !attached_method-&gt;is_abstract(), &quot;must have non-abstract attached method&quot;);
1149       if (bc == Bytecodes::_invokeinterface) {
1150         bc = Bytecodes::_invokevirtual; // C2 optimistically replaces interface calls by virtual calls
1151       }
1152       check_null_and_abstract = false;
1153     } else {
1154       // Retrieve from a compiled argument list
1155       receiver = Handle(THREAD, callerFrame.retrieve_receiver(&amp;reg_map2));
1156       if (receiver.is_null()) {
1157         THROW_(vmSymbols::java_lang_NullPointerException(), nullHandle);
1158       }
1159     }
1160   }
1161 
1162   // Resolve method
1163   if (attached_method.not_null()) {
1164     // Parameterized by attached method.
1165     LinkResolver::resolve_invoke(callinfo, receiver, attached_method, bc, check_null_and_abstract, CHECK_NH);
1166   } else {
</pre>
<hr />
<pre>
1268   CompiledMethod* callee = callee_method-&gt;code();
1269 
1270   if (callee != NULL) {
1271     assert(callee-&gt;is_compiled(), &quot;must be nmethod for patching&quot;);
1272   }
1273 
1274   if (callee != NULL &amp;&amp; !callee-&gt;is_in_use()) {
1275     // Patch call site to C2I adapter if callee nmethod is deoptimized or unloaded.
1276     callee = NULL;
1277   }
1278   nmethodLocker nl_callee(callee);
1279 #ifdef ASSERT
1280   address dest_entry_point = callee == NULL ? 0 : callee-&gt;entry_point(); // used below
1281 #endif
1282 
1283   bool is_nmethod = caller_nm-&gt;is_nmethod();
1284   bool caller_is_c1 = caller_nm-&gt;is_compiled_by_c1();
1285 
1286   if (is_virtual) {
1287     Klass* receiver_klass = NULL;
<span class="line-modified">1288     if (InlineTypePassFieldsAsArgs &amp;&amp; !caller_is_c1 &amp;&amp; callee_method-&gt;method_holder()-&gt;is_inline_klass()) {</span>

1289       // If the receiver is an inline type that is passed as fields, no oop is available
1290       receiver_klass = callee_method-&gt;method_holder();
1291     } else {
1292       assert(receiver.not_null() || invoke_code == Bytecodes::_invokehandle, &quot;sanity check&quot;);
1293       receiver_klass = invoke_code == Bytecodes::_invokehandle ? NULL : receiver-&gt;klass();
1294     }
1295     bool static_bound = call_info.resolved_method()-&gt;can_be_statically_bound();
1296     CompiledIC::compute_monomorphic_entry(callee_method, receiver_klass,
1297                      is_optimized, static_bound, is_nmethod, caller_is_c1, virtual_call_info,
1298                      CHECK_false);
1299   } else {
1300     // static call
1301     CompiledStaticCall::compute_entry(callee_method, caller_nm, static_call_info);
1302   }
1303 
1304   // grab lock, check for deoptimization and potentially patch caller
1305   {
1306     CompiledICLocker ml(caller_nm);
1307 
1308     // Lock blocks for safepoint during which both nmethods can change state.
</pre>
<hr />
<pre>
2729   }
2730 
2731   return entry;
2732 }
2733 
2734 
2735 CompiledEntrySignature::CompiledEntrySignature(Method* method) :
2736   _method(method), _num_value_args(0), _has_value_recv(false),
2737   _sig_cc(NULL), _sig_cc_ro(NULL), _regs(NULL), _regs_cc(NULL), _regs_cc_ro(NULL),
2738   _args_on_stack(0), _args_on_stack_cc(0), _args_on_stack_cc_ro(0),
2739   _c1_needs_stack_repair(false), _c2_needs_stack_repair(false), _has_scalarized_args(false) {
2740   _has_reserved_entries = false;
2741   _sig = new GrowableArray&lt;SigEntry&gt;(method-&gt;size_of_parameters());
2742 
2743 }
2744 
2745 int CompiledEntrySignature::compute_scalarized_cc(GrowableArray&lt;SigEntry&gt;*&amp; sig_cc, VMRegPair*&amp; regs_cc, bool scalar_receiver) {
2746   InstanceKlass* holder = _method-&gt;method_holder();
2747   sig_cc = new GrowableArray&lt;SigEntry&gt;(_method-&gt;size_of_parameters());
2748   if (!_method-&gt;is_static()) {
<span class="line-modified">2749     if (holder-&gt;is_inline_klass() &amp;&amp; scalar_receiver &amp;&amp; ValueKlass::cast(holder)-&gt;is_scalarizable()) {</span>
2750       sig_cc-&gt;appendAll(ValueKlass::cast(holder)-&gt;extended_sig());
2751     } else {
2752       SigEntry::add_entry(sig_cc, T_OBJECT);
2753     }
2754   }
2755   Thread* THREAD = Thread::current();
2756   for (SignatureStream ss(_method-&gt;signature()); !ss.at_return_type(); ss.next()) {
2757     if (ss.type() == T_VALUETYPE) {
2758       ValueKlass* vk = ss.as_value_klass(holder);
<span class="line-modified">2759       if (vk-&gt;is_scalarizable()) {</span>
2760         sig_cc-&gt;appendAll(vk-&gt;extended_sig());
2761       } else {
2762         SigEntry::add_entry(sig_cc, T_OBJECT);
2763       }
2764     } else {
2765       SigEntry::add_entry(sig_cc, ss.type());
2766     }
2767   }
2768   regs_cc = NEW_RESOURCE_ARRAY(VMRegPair, sig_cc-&gt;length() + 2);
2769   return SharedRuntime::java_calling_convention(sig_cc, regs_cc);
2770 }
2771 
2772 int CompiledEntrySignature::insert_reserved_entry(int ret_off) {
2773   // Find index in signature that belongs to return address slot
2774   BasicType bt = T_ILLEGAL;
2775   int i = 0;
2776   for (uint off = 0; i &lt; _sig_cc-&gt;length(); ++i) {
2777     if (SigEntry::skip_value_delimiters(_sig_cc, i)) {
2778       VMReg first = _regs_cc[off++].first();
2779       if (first-&gt;is_valid() &amp;&amp; first-&gt;is_stack()) {
</pre>
<hr />
<pre>
2816       return CodeOffsets::Verified_Value_Entry_RO;
2817     }
2818   }
2819 
2820   // Either a static method, or &lt;this&gt; is not a value type
2821   if (args_on_stack_cc() != args_on_stack_cc_ro() || _has_reserved_entries) {
2822     // No sharing:
2823     // Some arguments are passed on the stack, and we have inserted reserved entries
2824     // into the VEP, but we never insert reserved entries into the VVEP(RO).
2825     return CodeOffsets::Verified_Value_Entry_RO;
2826   } else {
2827     // Share same entry for VEP and VVEP(RO).
2828     return CodeOffsets::Verified_Entry;
2829   }
2830 }
2831 
2832 
2833 void CompiledEntrySignature::compute_calling_conventions() {
2834   // Get the (non-scalarized) signature and check for value type arguments
2835   if (!_method-&gt;is_static()) {
<span class="line-modified">2836     if (_method-&gt;method_holder()-&gt;is_inline_klass() &amp;&amp; ValueKlass::cast(_method-&gt;method_holder())-&gt;is_scalarizable()) {</span>
2837       _has_value_recv = true;
2838       _num_value_args++;
2839     }
2840     SigEntry::add_entry(_sig, T_OBJECT);
2841   }
2842   for (SignatureStream ss(_method-&gt;signature()); !ss.at_return_type(); ss.next()) {
2843     BasicType bt = ss.type();
2844     if (bt == T_VALUETYPE) {
<span class="line-modified">2845       if (ss.as_value_klass(_method-&gt;method_holder())-&gt;is_scalarizable()) {</span>
2846         _num_value_args++;
2847       }
2848       bt = T_OBJECT;
2849     }
2850     SigEntry::add_entry(_sig, bt);
2851   }
<span class="line-modified">2852   if (_method-&gt;is_abstract() &amp;&amp; !(InlineTypePassFieldsAsArgs &amp;&amp; has_value_arg())) {</span>
2853     return;
2854   }
2855 
2856   // Get a description of the compiled java calling convention and the largest used (VMReg) stack slot usage
2857   _regs = NEW_RESOURCE_ARRAY(VMRegPair, _sig-&gt;length());
2858   _args_on_stack = SharedRuntime::java_calling_convention(_sig, _regs);
2859 
2860   // Now compute the scalarized calling convention if there are value types in the signature
2861   _sig_cc = _sig;
2862   _sig_cc_ro = _sig;
2863   _regs_cc = _regs;
2864   _regs_cc_ro = _regs;
2865   _args_on_stack_cc = _args_on_stack;
2866   _args_on_stack_cc_ro = _args_on_stack;
2867 
<span class="line-modified">2868   if (InlineTypePassFieldsAsArgs &amp;&amp; has_value_arg() &amp;&amp; !_method-&gt;is_native()) {</span>
2869     _args_on_stack_cc = compute_scalarized_cc(_sig_cc, _regs_cc, /* scalar_receiver = */ true);
2870 
2871     _sig_cc_ro = _sig_cc;
2872     _regs_cc_ro = _regs_cc;
2873     _args_on_stack_cc_ro = _args_on_stack_cc;
2874     if (_has_value_recv || _args_on_stack_cc &gt; _args_on_stack) {
2875       // For interface calls, we need another entry point / adapter to unpack the receiver
2876       _args_on_stack_cc_ro = compute_scalarized_cc(_sig_cc_ro, _regs_cc_ro, /* scalar_receiver = */ false);
2877     }
2878 
2879     // Compute the stack extension that is required to convert between the calling conventions.
2880     // The stack slots at these offsets are occupied by the return address with the unscalarized
2881     // calling convention. Don&#39;t use them for arguments with the scalarized calling convention.
2882     int ret_off    = _args_on_stack_cc - _args_on_stack;
2883     int ret_off_ro = _args_on_stack_cc - _args_on_stack_cc_ro;
2884     assert(ret_off_ro &lt;= 0 || ret_off &gt; 0, &quot;receiver unpacking requires more stack space than expected&quot;);
2885 
2886     if (ret_off &gt; 0) {
2887       // Make sure the stack of the scalarized calling convention with the reserved
2888       // entries (2 slots each) remains 16-byte (4 slots) aligned after stack extension.
</pre>
</td>
<td>
<hr />
<pre>
1125   if (has_receiver) {
1126     // This register map must be update since we need to find the receiver for
1127     // compiled frames. The receiver might be in a register.
1128     RegisterMap reg_map2(thread);
1129     frame stubFrame   = thread-&gt;last_frame();
1130     // Caller-frame is a compiled frame
1131     frame callerFrame = stubFrame.sender(&amp;reg_map2);
1132     bool caller_is_c1 = false;
1133 
1134     if (callerFrame.is_compiled_frame() &amp;&amp; !callerFrame.is_deoptimized_frame()) {
1135       caller_is_c1 = callerFrame.cb()-&gt;is_compiled_by_c1();
1136     }
1137 
1138     Method* callee = attached_method();
1139     if (callee == NULL) {
1140       callee = bytecode.static_target(CHECK_NH);
1141       if (callee == NULL) {
1142         THROW_(vmSymbols::java_lang_NoSuchMethodException(), nullHandle);
1143       }
1144     }
<span class="line-modified">1145     if (!caller_is_c1 &amp;&amp; callee-&gt;has_scalarized_args() &amp;&amp; callee-&gt;method_holder()-&gt;is_inline_klass() &amp;&amp;</span>
<span class="line-modified">1146         ValueKlass::cast(callee-&gt;method_holder())-&gt;can_be_passed_as_fields()) {</span>
<span class="line-added">1147       // If the receiver is an inline type that is passed as fields, no oop is available</span>
1148       // Resolve the call without receiver null checking.
1149       assert(attached_method.not_null() &amp;&amp; !attached_method-&gt;is_abstract(), &quot;must have non-abstract attached method&quot;);
1150       if (bc == Bytecodes::_invokeinterface) {
1151         bc = Bytecodes::_invokevirtual; // C2 optimistically replaces interface calls by virtual calls
1152       }
1153       check_null_and_abstract = false;
1154     } else {
1155       // Retrieve from a compiled argument list
1156       receiver = Handle(THREAD, callerFrame.retrieve_receiver(&amp;reg_map2));
1157       if (receiver.is_null()) {
1158         THROW_(vmSymbols::java_lang_NullPointerException(), nullHandle);
1159       }
1160     }
1161   }
1162 
1163   // Resolve method
1164   if (attached_method.not_null()) {
1165     // Parameterized by attached method.
1166     LinkResolver::resolve_invoke(callinfo, receiver, attached_method, bc, check_null_and_abstract, CHECK_NH);
1167   } else {
</pre>
<hr />
<pre>
1269   CompiledMethod* callee = callee_method-&gt;code();
1270 
1271   if (callee != NULL) {
1272     assert(callee-&gt;is_compiled(), &quot;must be nmethod for patching&quot;);
1273   }
1274 
1275   if (callee != NULL &amp;&amp; !callee-&gt;is_in_use()) {
1276     // Patch call site to C2I adapter if callee nmethod is deoptimized or unloaded.
1277     callee = NULL;
1278   }
1279   nmethodLocker nl_callee(callee);
1280 #ifdef ASSERT
1281   address dest_entry_point = callee == NULL ? 0 : callee-&gt;entry_point(); // used below
1282 #endif
1283 
1284   bool is_nmethod = caller_nm-&gt;is_nmethod();
1285   bool caller_is_c1 = caller_nm-&gt;is_compiled_by_c1();
1286 
1287   if (is_virtual) {
1288     Klass* receiver_klass = NULL;
<span class="line-modified">1289     if (!caller_is_c1 &amp;&amp; callee_method-&gt;has_scalarized_args() &amp;&amp; callee_method-&gt;method_holder()-&gt;is_inline_klass() &amp;&amp;</span>
<span class="line-added">1290         ValueKlass::cast(callee_method-&gt;method_holder())-&gt;can_be_passed_as_fields()) {</span>
1291       // If the receiver is an inline type that is passed as fields, no oop is available
1292       receiver_klass = callee_method-&gt;method_holder();
1293     } else {
1294       assert(receiver.not_null() || invoke_code == Bytecodes::_invokehandle, &quot;sanity check&quot;);
1295       receiver_klass = invoke_code == Bytecodes::_invokehandle ? NULL : receiver-&gt;klass();
1296     }
1297     bool static_bound = call_info.resolved_method()-&gt;can_be_statically_bound();
1298     CompiledIC::compute_monomorphic_entry(callee_method, receiver_klass,
1299                      is_optimized, static_bound, is_nmethod, caller_is_c1, virtual_call_info,
1300                      CHECK_false);
1301   } else {
1302     // static call
1303     CompiledStaticCall::compute_entry(callee_method, caller_nm, static_call_info);
1304   }
1305 
1306   // grab lock, check for deoptimization and potentially patch caller
1307   {
1308     CompiledICLocker ml(caller_nm);
1309 
1310     // Lock blocks for safepoint during which both nmethods can change state.
</pre>
<hr />
<pre>
2731   }
2732 
2733   return entry;
2734 }
2735 
2736 
2737 CompiledEntrySignature::CompiledEntrySignature(Method* method) :
2738   _method(method), _num_value_args(0), _has_value_recv(false),
2739   _sig_cc(NULL), _sig_cc_ro(NULL), _regs(NULL), _regs_cc(NULL), _regs_cc_ro(NULL),
2740   _args_on_stack(0), _args_on_stack_cc(0), _args_on_stack_cc_ro(0),
2741   _c1_needs_stack_repair(false), _c2_needs_stack_repair(false), _has_scalarized_args(false) {
2742   _has_reserved_entries = false;
2743   _sig = new GrowableArray&lt;SigEntry&gt;(method-&gt;size_of_parameters());
2744 
2745 }
2746 
2747 int CompiledEntrySignature::compute_scalarized_cc(GrowableArray&lt;SigEntry&gt;*&amp; sig_cc, VMRegPair*&amp; regs_cc, bool scalar_receiver) {
2748   InstanceKlass* holder = _method-&gt;method_holder();
2749   sig_cc = new GrowableArray&lt;SigEntry&gt;(_method-&gt;size_of_parameters());
2750   if (!_method-&gt;is_static()) {
<span class="line-modified">2751     if (holder-&gt;is_inline_klass() &amp;&amp; scalar_receiver &amp;&amp; ValueKlass::cast(holder)-&gt;can_be_passed_as_fields()) {</span>
2752       sig_cc-&gt;appendAll(ValueKlass::cast(holder)-&gt;extended_sig());
2753     } else {
2754       SigEntry::add_entry(sig_cc, T_OBJECT);
2755     }
2756   }
2757   Thread* THREAD = Thread::current();
2758   for (SignatureStream ss(_method-&gt;signature()); !ss.at_return_type(); ss.next()) {
2759     if (ss.type() == T_VALUETYPE) {
2760       ValueKlass* vk = ss.as_value_klass(holder);
<span class="line-modified">2761       if (vk-&gt;can_be_passed_as_fields()) {</span>
2762         sig_cc-&gt;appendAll(vk-&gt;extended_sig());
2763       } else {
2764         SigEntry::add_entry(sig_cc, T_OBJECT);
2765       }
2766     } else {
2767       SigEntry::add_entry(sig_cc, ss.type());
2768     }
2769   }
2770   regs_cc = NEW_RESOURCE_ARRAY(VMRegPair, sig_cc-&gt;length() + 2);
2771   return SharedRuntime::java_calling_convention(sig_cc, regs_cc);
2772 }
2773 
2774 int CompiledEntrySignature::insert_reserved_entry(int ret_off) {
2775   // Find index in signature that belongs to return address slot
2776   BasicType bt = T_ILLEGAL;
2777   int i = 0;
2778   for (uint off = 0; i &lt; _sig_cc-&gt;length(); ++i) {
2779     if (SigEntry::skip_value_delimiters(_sig_cc, i)) {
2780       VMReg first = _regs_cc[off++].first();
2781       if (first-&gt;is_valid() &amp;&amp; first-&gt;is_stack()) {
</pre>
<hr />
<pre>
2818       return CodeOffsets::Verified_Value_Entry_RO;
2819     }
2820   }
2821 
2822   // Either a static method, or &lt;this&gt; is not a value type
2823   if (args_on_stack_cc() != args_on_stack_cc_ro() || _has_reserved_entries) {
2824     // No sharing:
2825     // Some arguments are passed on the stack, and we have inserted reserved entries
2826     // into the VEP, but we never insert reserved entries into the VVEP(RO).
2827     return CodeOffsets::Verified_Value_Entry_RO;
2828   } else {
2829     // Share same entry for VEP and VVEP(RO).
2830     return CodeOffsets::Verified_Entry;
2831   }
2832 }
2833 
2834 
2835 void CompiledEntrySignature::compute_calling_conventions() {
2836   // Get the (non-scalarized) signature and check for value type arguments
2837   if (!_method-&gt;is_static()) {
<span class="line-modified">2838     if (_method-&gt;method_holder()-&gt;is_inline_klass() &amp;&amp; ValueKlass::cast(_method-&gt;method_holder())-&gt;can_be_passed_as_fields()) {</span>
2839       _has_value_recv = true;
2840       _num_value_args++;
2841     }
2842     SigEntry::add_entry(_sig, T_OBJECT);
2843   }
2844   for (SignatureStream ss(_method-&gt;signature()); !ss.at_return_type(); ss.next()) {
2845     BasicType bt = ss.type();
2846     if (bt == T_VALUETYPE) {
<span class="line-modified">2847       if (ss.as_value_klass(_method-&gt;method_holder())-&gt;can_be_passed_as_fields()) {</span>
2848         _num_value_args++;
2849       }
2850       bt = T_OBJECT;
2851     }
2852     SigEntry::add_entry(_sig, bt);
2853   }
<span class="line-modified">2854   if (_method-&gt;is_abstract() &amp;&amp; !has_value_arg()) {</span>
2855     return;
2856   }
2857 
2858   // Get a description of the compiled java calling convention and the largest used (VMReg) stack slot usage
2859   _regs = NEW_RESOURCE_ARRAY(VMRegPair, _sig-&gt;length());
2860   _args_on_stack = SharedRuntime::java_calling_convention(_sig, _regs);
2861 
2862   // Now compute the scalarized calling convention if there are value types in the signature
2863   _sig_cc = _sig;
2864   _sig_cc_ro = _sig;
2865   _regs_cc = _regs;
2866   _regs_cc_ro = _regs;
2867   _args_on_stack_cc = _args_on_stack;
2868   _args_on_stack_cc_ro = _args_on_stack;
2869 
<span class="line-modified">2870   if (has_value_arg() &amp;&amp; !_method-&gt;is_native()) {</span>
2871     _args_on_stack_cc = compute_scalarized_cc(_sig_cc, _regs_cc, /* scalar_receiver = */ true);
2872 
2873     _sig_cc_ro = _sig_cc;
2874     _regs_cc_ro = _regs_cc;
2875     _args_on_stack_cc_ro = _args_on_stack_cc;
2876     if (_has_value_recv || _args_on_stack_cc &gt; _args_on_stack) {
2877       // For interface calls, we need another entry point / adapter to unpack the receiver
2878       _args_on_stack_cc_ro = compute_scalarized_cc(_sig_cc_ro, _regs_cc_ro, /* scalar_receiver = */ false);
2879     }
2880 
2881     // Compute the stack extension that is required to convert between the calling conventions.
2882     // The stack slots at these offsets are occupied by the return address with the unscalarized
2883     // calling convention. Don&#39;t use them for arguments with the scalarized calling convention.
2884     int ret_off    = _args_on_stack_cc - _args_on_stack;
2885     int ret_off_ro = _args_on_stack_cc - _args_on_stack_cc_ro;
2886     assert(ret_off_ro &lt;= 0 || ret_off &gt; 0, &quot;receiver unpacking requires more stack space than expected&quot;);
2887 
2888     if (ret_off &gt; 0) {
2889       // Make sure the stack of the scalarized calling convention with the reserved
2890       // entries (2 slots each) remains 16-byte (4 slots) aligned after stack extension.
</pre>
</td>
</tr>
</table>
<center><a href="../opto/valuetypenode.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="../../../../test/hotspot/jtreg/compiler/valhalla/valuetypes/TestCallingConvention.java.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>