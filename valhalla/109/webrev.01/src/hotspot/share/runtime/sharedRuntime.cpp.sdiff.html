<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/share/runtime/sharedRuntime.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="safepoint.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="sharedRuntime.hpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/share/runtime/sharedRuntime.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
  37 #include &quot;compiler/abstractCompiler.hpp&quot;
  38 #include &quot;compiler/compileBroker.hpp&quot;
  39 #include &quot;compiler/disassembler.hpp&quot;
  40 #include &quot;gc/shared/barrierSet.hpp&quot;
  41 #include &quot;gc/shared/gcLocker.inline.hpp&quot;
  42 #include &quot;interpreter/interpreter.hpp&quot;
  43 #include &quot;interpreter/interpreterRuntime.hpp&quot;
  44 #include &quot;jfr/jfrEvents.hpp&quot;
  45 #include &quot;logging/log.hpp&quot;
  46 #include &quot;memory/metaspaceShared.hpp&quot;
  47 #include &quot;memory/oopFactory.hpp&quot;
  48 #include &quot;memory/resourceArea.hpp&quot;
  49 #include &quot;memory/universe.hpp&quot;
  50 #include &quot;oops/access.hpp&quot;
  51 #include &quot;oops/fieldStreams.inline.hpp&quot;
  52 #include &quot;oops/klass.hpp&quot;
  53 #include &quot;oops/method.inline.hpp&quot;
  54 #include &quot;oops/objArrayKlass.hpp&quot;
  55 #include &quot;oops/objArrayOop.inline.hpp&quot;
  56 #include &quot;oops/oop.inline.hpp&quot;
<span class="line-modified">  57 #include &quot;oops/valueKlass.inline.hpp&quot;</span>
  58 #include &quot;prims/forte.hpp&quot;
  59 #include &quot;prims/jvmtiExport.hpp&quot;
  60 #include &quot;prims/methodHandles.hpp&quot;
  61 #include &quot;prims/nativeLookup.hpp&quot;
  62 #include &quot;runtime/arguments.hpp&quot;
  63 #include &quot;runtime/atomic.hpp&quot;
  64 #include &quot;runtime/biasedLocking.hpp&quot;
  65 #include &quot;runtime/frame.inline.hpp&quot;
  66 #include &quot;runtime/handles.inline.hpp&quot;
  67 #include &quot;runtime/init.hpp&quot;
  68 #include &quot;runtime/interfaceSupport.inline.hpp&quot;
  69 #include &quot;runtime/java.hpp&quot;
  70 #include &quot;runtime/javaCalls.hpp&quot;
  71 #include &quot;runtime/sharedRuntime.hpp&quot;
  72 #include &quot;runtime/stubRoutines.hpp&quot;
  73 #include &quot;runtime/synchronizer.hpp&quot;
  74 #include &quot;runtime/vframe.inline.hpp&quot;
  75 #include &quot;runtime/vframeArray.hpp&quot;
  76 #include &quot;utilities/copy.hpp&quot;
  77 #include &quot;utilities/dtrace.hpp&quot;
</pre>
<hr />
<pre>
1126     // This register map must be update since we need to find the receiver for
1127     // compiled frames. The receiver might be in a register.
1128     RegisterMap reg_map2(thread);
1129     frame stubFrame   = thread-&gt;last_frame();
1130     // Caller-frame is a compiled frame
1131     frame callerFrame = stubFrame.sender(&amp;reg_map2);
1132     bool caller_is_c1 = false;
1133 
1134     if (callerFrame.is_compiled_frame() &amp;&amp; !callerFrame.is_deoptimized_frame()) {
1135       caller_is_c1 = callerFrame.cb()-&gt;is_compiled_by_c1();
1136     }
1137 
1138     Method* callee = attached_method();
1139     if (callee == NULL) {
1140       callee = bytecode.static_target(CHECK_NH);
1141       if (callee == NULL) {
1142         THROW_(vmSymbols::java_lang_NoSuchMethodException(), nullHandle);
1143       }
1144     }
1145     if (!caller_is_c1 &amp;&amp; callee-&gt;has_scalarized_args() &amp;&amp; callee-&gt;method_holder()-&gt;is_inline_klass() &amp;&amp;
<span class="line-modified">1146         ValueKlass::cast(callee-&gt;method_holder())-&gt;can_be_passed_as_fields()) {</span>
1147       // If the receiver is an inline type that is passed as fields, no oop is available
1148       // Resolve the call without receiver null checking.
1149       assert(attached_method.not_null() &amp;&amp; !attached_method-&gt;is_abstract(), &quot;must have non-abstract attached method&quot;);
1150       if (bc == Bytecodes::_invokeinterface) {
1151         bc = Bytecodes::_invokevirtual; // C2 optimistically replaces interface calls by virtual calls
1152       }
1153       check_null_and_abstract = false;
1154     } else {
1155       // Retrieve from a compiled argument list
1156       receiver = Handle(THREAD, callerFrame.retrieve_receiver(&amp;reg_map2));
1157       if (receiver.is_null()) {
1158         THROW_(vmSymbols::java_lang_NullPointerException(), nullHandle);
1159       }
1160     }
1161   }
1162 
1163   // Resolve method
1164   if (attached_method.not_null()) {
1165     // Parameterized by attached method.
1166     LinkResolver::resolve_invoke(callinfo, receiver, attached_method, bc, check_null_and_abstract, CHECK_NH);
</pre>
<hr />
<pre>
1270 
1271   if (callee != NULL) {
1272     assert(callee-&gt;is_compiled(), &quot;must be nmethod for patching&quot;);
1273   }
1274 
1275   if (callee != NULL &amp;&amp; !callee-&gt;is_in_use()) {
1276     // Patch call site to C2I adapter if callee nmethod is deoptimized or unloaded.
1277     callee = NULL;
1278   }
1279   nmethodLocker nl_callee(callee);
1280 #ifdef ASSERT
1281   address dest_entry_point = callee == NULL ? 0 : callee-&gt;entry_point(); // used below
1282 #endif
1283 
1284   bool is_nmethod = caller_nm-&gt;is_nmethod();
1285   bool caller_is_c1 = caller_nm-&gt;is_compiled_by_c1();
1286 
1287   if (is_virtual) {
1288     Klass* receiver_klass = NULL;
1289     if (!caller_is_c1 &amp;&amp; callee_method-&gt;has_scalarized_args() &amp;&amp; callee_method-&gt;method_holder()-&gt;is_inline_klass() &amp;&amp;
<span class="line-modified">1290         ValueKlass::cast(callee_method-&gt;method_holder())-&gt;can_be_passed_as_fields()) {</span>
1291       // If the receiver is an inline type that is passed as fields, no oop is available
1292       receiver_klass = callee_method-&gt;method_holder();
1293     } else {
1294       assert(receiver.not_null() || invoke_code == Bytecodes::_invokehandle, &quot;sanity check&quot;);
1295       receiver_klass = invoke_code == Bytecodes::_invokehandle ? NULL : receiver-&gt;klass();
1296     }
1297     bool static_bound = call_info.resolved_method()-&gt;can_be_statically_bound();
1298     CompiledIC::compute_monomorphic_entry(callee_method, receiver_klass,
1299                      is_optimized, static_bound, is_nmethod, caller_is_c1, virtual_call_info,
1300                      CHECK_false);
1301   } else {
1302     // static call
1303     CompiledStaticCall::compute_entry(callee_method, caller_nm, static_call_info);
1304   }
1305 
1306   // grab lock, check for deoptimization and potentially patch caller
1307   {
1308     CompiledICLocker ml(caller_nm);
1309 
1310     // Lock blocks for safepoint during which both nmethods can change state.
</pre>
<hr />
<pre>
2731   }
2732 
2733   return entry;
2734 }
2735 
2736 
2737 CompiledEntrySignature::CompiledEntrySignature(Method* method) :
2738   _method(method), _num_value_args(0), _has_value_recv(false),
2739   _sig_cc(NULL), _sig_cc_ro(NULL), _regs(NULL), _regs_cc(NULL), _regs_cc_ro(NULL),
2740   _args_on_stack(0), _args_on_stack_cc(0), _args_on_stack_cc_ro(0),
2741   _c1_needs_stack_repair(false), _c2_needs_stack_repair(false), _has_scalarized_args(false) {
2742   _has_reserved_entries = false;
2743   _sig = new GrowableArray&lt;SigEntry&gt;(method-&gt;size_of_parameters());
2744 
2745 }
2746 
2747 int CompiledEntrySignature::compute_scalarized_cc(GrowableArray&lt;SigEntry&gt;*&amp; sig_cc, VMRegPair*&amp; regs_cc, bool scalar_receiver) {
2748   InstanceKlass* holder = _method-&gt;method_holder();
2749   sig_cc = new GrowableArray&lt;SigEntry&gt;(_method-&gt;size_of_parameters());
2750   if (!_method-&gt;is_static()) {
<span class="line-modified">2751     if (holder-&gt;is_inline_klass() &amp;&amp; scalar_receiver &amp;&amp; ValueKlass::cast(holder)-&gt;can_be_passed_as_fields()) {</span>
<span class="line-modified">2752       sig_cc-&gt;appendAll(ValueKlass::cast(holder)-&gt;extended_sig());</span>
2753     } else {
2754       SigEntry::add_entry(sig_cc, T_OBJECT);
2755     }
2756   }
2757   Thread* THREAD = Thread::current();
2758   for (SignatureStream ss(_method-&gt;signature()); !ss.at_return_type(); ss.next()) {
2759     if (ss.type() == T_INLINE_TYPE) {
<span class="line-modified">2760       ValueKlass* vk = ss.as_value_klass(holder);</span>
2761       if (vk-&gt;can_be_passed_as_fields()) {
2762         sig_cc-&gt;appendAll(vk-&gt;extended_sig());
2763       } else {
2764         SigEntry::add_entry(sig_cc, T_OBJECT);
2765       }
2766     } else {
2767       SigEntry::add_entry(sig_cc, ss.type());
2768     }
2769   }
2770   regs_cc = NEW_RESOURCE_ARRAY(VMRegPair, sig_cc-&gt;length() + 2);
2771   return SharedRuntime::java_calling_convention(sig_cc, regs_cc);
2772 }
2773 
2774 int CompiledEntrySignature::insert_reserved_entry(int ret_off) {
2775   // Find index in signature that belongs to return address slot
2776   BasicType bt = T_ILLEGAL;
2777   int i = 0;
2778   for (uint off = 0; i &lt; _sig_cc-&gt;length(); ++i) {
2779     if (SigEntry::skip_value_delimiters(_sig_cc, i)) {
2780       VMReg first = _regs_cc[off++].first();
</pre>
<hr />
<pre>
2790   // Insert reserved entry and re-compute calling convention
2791   SigEntry::insert_reserved_entry(_sig_cc, i, bt);
2792   return SharedRuntime::java_calling_convention(_sig_cc, _regs_cc);
2793 }
2794 
2795 // See if we can save space by sharing the same entry for VVEP and VVEP(RO),
2796 // or the same entry for VEP and VVEP(RO).
2797 CodeOffsets::Entries CompiledEntrySignature::c1_value_ro_entry_type() const {
2798   if (!has_scalarized_args()) {
2799     // VEP/VVEP/VVEP(RO) all share the same entry. There&#39;s no packing.
2800     return CodeOffsets::Verified_Entry;
2801   }
2802   if (_method-&gt;is_static()) {
2803     // Static methods don&#39;t need VVEP(RO)
2804     return CodeOffsets::Verified_Entry;
2805   }
2806 
2807   if (has_value_recv()) {
2808     if (num_value_args() == 1) {
2809       // Share same entry for VVEP and VVEP(RO).
<span class="line-modified">2810       // This is quite common: we have an instance method in a ValueKlass that has</span>
2811       // no value args other than &lt;this&gt;.
2812       return CodeOffsets::Verified_Value_Entry;
2813     } else {
2814       assert(num_value_args() &gt; 1, &quot;must be&quot;);
2815       // No sharing:
2816       //   VVEP(RO) -- &lt;this&gt; is passed as object
2817       //   VEP      -- &lt;this&gt; is passed as fields
2818       return CodeOffsets::Verified_Value_Entry_RO;
2819     }
2820   }
2821 
2822   // Either a static method, or &lt;this&gt; is not a value type
2823   if (args_on_stack_cc() != args_on_stack_cc_ro() || _has_reserved_entries) {
2824     // No sharing:
2825     // Some arguments are passed on the stack, and we have inserted reserved entries
2826     // into the VEP, but we never insert reserved entries into the VVEP(RO).
2827     return CodeOffsets::Verified_Value_Entry_RO;
2828   } else {
2829     // Share same entry for VEP and VVEP(RO).
2830     return CodeOffsets::Verified_Entry;
2831   }
2832 }
2833 
2834 
2835 void CompiledEntrySignature::compute_calling_conventions() {
<span class="line-modified">2836   // Get the (non-scalarized) signature and check for value type arguments</span>
2837   if (!_method-&gt;is_static()) {
<span class="line-modified">2838     if (_method-&gt;method_holder()-&gt;is_inline_klass() &amp;&amp; ValueKlass::cast(_method-&gt;method_holder())-&gt;can_be_passed_as_fields()) {</span>
2839       _has_value_recv = true;
2840       _num_value_args++;
2841     }
2842     SigEntry::add_entry(_sig, T_OBJECT);
2843   }
2844   for (SignatureStream ss(_method-&gt;signature()); !ss.at_return_type(); ss.next()) {
2845     BasicType bt = ss.type();
2846     if (bt == T_INLINE_TYPE) {
<span class="line-modified">2847       if (ss.as_value_klass(_method-&gt;method_holder())-&gt;can_be_passed_as_fields()) {</span>
2848         _num_value_args++;
2849       }
2850       bt = T_OBJECT;
2851     }
2852     SigEntry::add_entry(_sig, bt);
2853   }
2854   if (_method-&gt;is_abstract() &amp;&amp; !has_value_arg()) {
2855     return;
2856   }
2857 
2858   // Get a description of the compiled java calling convention and the largest used (VMReg) stack slot usage
2859   _regs = NEW_RESOURCE_ARRAY(VMRegPair, _sig-&gt;length());
2860   _args_on_stack = SharedRuntime::java_calling_convention(_sig, _regs);
2861 
2862   // Now compute the scalarized calling convention if there are value types in the signature
2863   _sig_cc = _sig;
2864   _sig_cc_ro = _sig;
2865   _regs_cc = _regs;
2866   _regs_cc_ro = _regs;
2867   _args_on_stack_cc = _args_on_stack;
</pre>
<hr />
<pre>
2897         // TODO can we avoid wasting a stack slot here?
2898         //assert(ret_off != ret_off_ro, &quot;fail&quot;);
2899         if (ret_off &gt; ret_off_ro) {
2900           swap(ret_off, ret_off_ro); // Sort by offset
2901         }
2902         _args_on_stack_cc = insert_reserved_entry(ret_off);
2903         _args_on_stack_cc = insert_reserved_entry(ret_off_ro);
2904       } else {
2905         ret_off += 2; // Account for one reserved entry (2 slots)
2906         ret_off = align_up(ret_off, alignment);
2907         _args_on_stack_cc = insert_reserved_entry(ret_off);
2908       }
2909 
2910       _has_reserved_entries = true;
2911     }
2912 
2913     // Upper bound on stack arguments to avoid hitting the argument limit and
2914     // bailing out of compilation (&quot;unsupported incoming calling sequence&quot;).
2915     // TODO we need a reasonable limit (flag?) here
2916     if (_args_on_stack_cc &gt; 50) {
<span class="line-modified">2917       // Don&#39;t scalarize value type arguments</span>
2918       _sig_cc = _sig;
2919       _sig_cc_ro = _sig;
2920       _regs_cc = _regs;
2921       _regs_cc_ro = _regs;
2922       _args_on_stack_cc = _args_on_stack;
2923       _args_on_stack_cc_ro = _args_on_stack;
2924     } else {
2925       _c1_needs_stack_repair = (_args_on_stack_cc &lt; _args_on_stack) || (_args_on_stack_cc_ro &lt; _args_on_stack);
2926       _c2_needs_stack_repair = (_args_on_stack_cc &gt; _args_on_stack) || (_args_on_stack_cc &gt; _args_on_stack_cc_ro);
2927       _has_scalarized_args = true;
2928     }
2929   }
2930 }
2931 
2932 AdapterHandlerEntry* AdapterHandlerLibrary::get_adapter0(const methodHandle&amp; method) {
2933   // Use customized signature handler.  Need to lock around updates to
2934   // the AdapterHandlerTable (it is not safe for concurrent readers
2935   // and a single writer: this could be fixed if it becomes a
2936   // problem).
2937 
</pre>
<hr />
<pre>
2950     CompiledEntrySignature ces(method());
2951     {
2952        MutexUnlocker mul(AdapterHandlerLibrary_lock);
2953        ces.compute_calling_conventions();
2954     }
2955     GrowableArray&lt;SigEntry&gt;&amp; sig       = ces.sig();
2956     GrowableArray&lt;SigEntry&gt;&amp; sig_cc    = ces.sig_cc();
2957     GrowableArray&lt;SigEntry&gt;&amp; sig_cc_ro = ces.sig_cc_ro();
2958     VMRegPair* regs         = ces.regs();
2959     VMRegPair* regs_cc      = ces.regs_cc();
2960     VMRegPair* regs_cc_ro   = ces.regs_cc_ro();
2961 
2962     if (ces.has_scalarized_args()) {
2963       method-&gt;set_has_scalarized_args(true);
2964       method-&gt;set_c1_needs_stack_repair(ces.c1_needs_stack_repair());
2965       method-&gt;set_c2_needs_stack_repair(ces.c2_needs_stack_repair());
2966     }
2967 
2968     if (method-&gt;is_abstract()) {
2969       if (ces.has_scalarized_args()) {
<span class="line-modified">2970         // Save a C heap allocated version of the signature for abstract methods with scalarized value type arguments</span>
2971         address wrong_method_abstract = SharedRuntime::get_handle_wrong_method_abstract_stub();
2972         entry = AdapterHandlerLibrary::new_entry(new AdapterFingerPrint(NULL),
2973                                                  StubRoutines::throw_AbstractMethodError_entry(),
2974                                                  wrong_method_abstract, wrong_method_abstract, wrong_method_abstract,
2975                                                  wrong_method_abstract, wrong_method_abstract);
2976         GrowableArray&lt;SigEntry&gt;* heap_sig = new (ResourceObj::C_HEAP, mtInternal) GrowableArray&lt;SigEntry&gt;(sig_cc_ro.length(), mtInternal);
2977         heap_sig-&gt;appendAll(&amp;sig_cc_ro);
2978         entry-&gt;set_sig_cc(heap_sig);
2979         return entry;
2980       } else {
2981         return _abstract_method_handler;
2982       }
2983     }
2984 
2985     // Lookup method signature&#39;s fingerprint
2986     entry = _adapters-&gt;lookup(&amp;sig_cc, regs_cc != regs_cc_ro);
2987 
2988 #ifdef ASSERT
2989     AdapterHandlerEntry* shared_entry = NULL;
2990     // Start adapter sharing verification only after the VM is booted.
</pre>
<hr />
<pre>
3572       fr = fr.java_sender();
3573     }
3574   }
3575   return activation;
3576 }
3577 
3578 void SharedRuntime::on_slowpath_allocation_exit(JavaThread* thread) {
3579   // After any safepoint, just before going back to compiled code,
3580   // we inform the GC that we will be doing initializing writes to
3581   // this object in the future without emitting card-marks, so
3582   // GC may take any compensating steps.
3583 
3584   oop new_obj = thread-&gt;vm_result();
3585   if (new_obj == NULL) return;
3586 
3587   BarrierSet *bs = BarrierSet::barrier_set();
3588   bs-&gt;on_slowpath_allocation_exit(thread, new_obj);
3589 }
3590 
3591 // We are at a compiled code to interpreter call. We need backing
<span class="line-modified">3592 // buffers for all value type arguments. Allocate an object array to</span>
3593 // hold them (convenient because once we&#39;re done with it we don&#39;t have
3594 // to worry about freeing it).
3595 oop SharedRuntime::allocate_value_types_impl(JavaThread* thread, methodHandle callee, bool allocate_receiver, TRAPS) {
3596   assert(InlineTypePassFieldsAsArgs, &quot;no reason to call this&quot;);
3597   ResourceMark rm;
3598 
3599   int nb_slots = 0;
3600   InstanceKlass* holder = callee-&gt;method_holder();
3601   allocate_receiver &amp;= !callee-&gt;is_static() &amp;&amp; holder-&gt;is_inline_klass();
3602   if (allocate_receiver) {
3603     nb_slots++;
3604   }
3605   for (SignatureStream ss(callee-&gt;signature()); !ss.at_return_type(); ss.next()) {
3606     if (ss.type() == T_INLINE_TYPE) {
3607       nb_slots++;
3608     }
3609   }
3610   objArrayOop array_oop = oopFactory::new_objectArray(nb_slots, CHECK_NULL);
3611   objArrayHandle array(THREAD, array_oop);
3612   int i = 0;
3613   if (allocate_receiver) {
<span class="line-modified">3614     ValueKlass* vk = ValueKlass::cast(holder);</span>
3615     oop res = vk-&gt;allocate_instance(CHECK_NULL);
3616     array-&gt;obj_at_put(i, res);
3617     i++;
3618   }
3619   for (SignatureStream ss(callee-&gt;signature()); !ss.at_return_type(); ss.next()) {
3620     if (ss.type() == T_INLINE_TYPE) {
<span class="line-modified">3621       ValueKlass* vk = ss.as_value_klass(holder);</span>
3622       oop res = vk-&gt;allocate_instance(CHECK_NULL);
3623       array-&gt;obj_at_put(i, res);
3624       i++;
3625     }
3626   }
3627   return array();
3628 }
3629 
3630 JRT_ENTRY(void, SharedRuntime::allocate_value_types(JavaThread* thread, Method* callee_method, bool allocate_receiver))
3631   methodHandle callee(thread, callee_method);
3632   oop array = SharedRuntime::allocate_value_types_impl(thread, callee, allocate_receiver, CHECK);
3633   thread-&gt;set_vm_result(array);
3634   thread-&gt;set_vm_result_2(callee()); // TODO: required to keep callee live?
3635 JRT_END
3636 
3637 // TODO remove this once the AARCH64 dependency is gone
3638 // Iterate over the array of heap allocated value types and apply the GC post barrier to all reference fields.
<span class="line-modified">3639 // This is called from the C2I adapter after value type arguments are heap allocated and initialized.</span>
3640 JRT_LEAF(void, SharedRuntime::apply_post_barriers(JavaThread* thread, objArrayOopDesc* array))
3641 {
3642   assert(InlineTypePassFieldsAsArgs, &quot;no reason to call this&quot;);
3643   assert(oopDesc::is_oop(array), &quot;should be oop&quot;);
3644   for (int i = 0; i &lt; array-&gt;length(); ++i) {
3645     instanceOop valueOop = (instanceOop)array-&gt;obj_at(i);
<span class="line-modified">3646     ValueKlass* vk = ValueKlass::cast(valueOop-&gt;klass());</span>
3647     if (vk-&gt;contains_oops()) {
3648       const address dst_oop_addr = ((address) (void*) valueOop);
3649       OopMapBlock* map = vk-&gt;start_of_nonstatic_oop_maps();
3650       OopMapBlock* const end = map + vk-&gt;nonstatic_oop_map_count();
3651       while (map != end) {
3652         address doop_address = dst_oop_addr + map-&gt;offset();
3653         barrier_set_cast&lt;ModRefBarrierSet&gt;(BarrierSet::barrier_set())-&gt;
3654           write_ref_array((HeapWord*) doop_address, map-&gt;count());
3655         map++;
3656       }
3657     }
3658   }
3659 }
3660 JRT_END
3661 
3662 // We&#39;re returning from an interpreted method: load each field into a
3663 // register following the calling convention
3664 JRT_LEAF(void, SharedRuntime::load_value_type_fields_in_regs(JavaThread* thread, oopDesc* res))
3665 {
3666   assert(res-&gt;klass()-&gt;is_inline_klass(), &quot;only inline types here&quot;);
3667   ResourceMark rm;
3668   RegisterMap reg_map(thread);
3669   frame stubFrame = thread-&gt;last_frame();
3670   frame callerFrame = stubFrame.sender(&amp;reg_map);
3671   assert(callerFrame.is_interpreted_frame(), &quot;should be coming from interpreter&quot;);
3672 
<span class="line-modified">3673   ValueKlass* vk = ValueKlass::cast(res-&gt;klass());</span>
3674 
3675   const Array&lt;SigEntry&gt;* sig_vk = vk-&gt;extended_sig();
3676   const Array&lt;VMRegPair&gt;* regs = vk-&gt;return_regs();
3677 
3678   if (regs == NULL) {
3679     // The fields of the value klass don&#39;t fit in registers, bail out
3680     return;
3681   }
3682 
3683   int j = 1;
3684   for (int i = 0; i &lt; sig_vk-&gt;length(); i++) {
3685     BasicType bt = sig_vk-&gt;at(i)._bt;
3686     if (bt == T_INLINE_TYPE) {
3687       continue;
3688     }
3689     if (bt == T_VOID) {
3690       if (sig_vk-&gt;at(i-1)._bt == T_LONG ||
3691           sig_vk-&gt;at(i-1)._bt == T_DOUBLE) {
3692         j++;
3693       }
</pre>
<hr />
<pre>
3743   VMRegPair pair = regs-&gt;at(0);
3744   address loc = reg_map.location(pair.first());
3745   assert(*(oopDesc**)loc == res, &quot;overwritten object&quot;);
3746 #endif
3747 
3748   thread-&gt;set_vm_result(res);
3749 }
3750 JRT_END
3751 
3752 // We&#39;ve returned to an interpreted method, the interpreter needs a
3753 // reference to a value type instance. Allocate it and initialize it
3754 // from field&#39;s values in registers.
3755 JRT_BLOCK_ENTRY(void, SharedRuntime::store_value_type_fields_to_buf(JavaThread* thread, intptr_t res))
3756 {
3757   ResourceMark rm;
3758   RegisterMap reg_map(thread);
3759   frame stubFrame = thread-&gt;last_frame();
3760   frame callerFrame = stubFrame.sender(&amp;reg_map);
3761 
3762 #ifdef ASSERT
<span class="line-modified">3763   ValueKlass* verif_vk = ValueKlass::returned_value_klass(reg_map);</span>
3764 #endif
3765 
3766   if (!is_set_nth_bit(res, 0)) {
3767     // We&#39;re not returning with value type fields in registers (the
3768     // calling convention didn&#39;t allow it for this value klass)
3769     assert(!Metaspace::contains((void*)res), &quot;should be oop or pointer in buffer area&quot;);
3770     thread-&gt;set_vm_result((oopDesc*)res);
3771     assert(verif_vk == NULL, &quot;broken calling convention&quot;);
3772     return;
3773   }
3774 
3775   clear_nth_bit(res, 0);
<span class="line-modified">3776   ValueKlass* vk = (ValueKlass*)res;</span>
3777   assert(verif_vk == vk, &quot;broken calling convention&quot;);
3778   assert(Metaspace::contains((void*)res), &quot;should be klass&quot;);
3779 
3780   // Allocate handles for every oop field so they are safe in case of
3781   // a safepoint when allocating
3782   GrowableArray&lt;Handle&gt; handles;
3783   vk-&gt;save_oop_fields(reg_map, handles);
3784 
3785   // It&#39;s unsafe to safepoint until we are here
3786   JRT_BLOCK;
3787   {
3788     Thread* THREAD = thread;
3789     oop vt = vk-&gt;realloc_result(reg_map, handles, CHECK);
3790     thread-&gt;set_vm_result(vt);
3791   }
3792   JRT_BLOCK_END;
3793 }
3794 JRT_END
3795 
</pre>
</td>
<td>
<hr />
<pre>
  37 #include &quot;compiler/abstractCompiler.hpp&quot;
  38 #include &quot;compiler/compileBroker.hpp&quot;
  39 #include &quot;compiler/disassembler.hpp&quot;
  40 #include &quot;gc/shared/barrierSet.hpp&quot;
  41 #include &quot;gc/shared/gcLocker.inline.hpp&quot;
  42 #include &quot;interpreter/interpreter.hpp&quot;
  43 #include &quot;interpreter/interpreterRuntime.hpp&quot;
  44 #include &quot;jfr/jfrEvents.hpp&quot;
  45 #include &quot;logging/log.hpp&quot;
  46 #include &quot;memory/metaspaceShared.hpp&quot;
  47 #include &quot;memory/oopFactory.hpp&quot;
  48 #include &quot;memory/resourceArea.hpp&quot;
  49 #include &quot;memory/universe.hpp&quot;
  50 #include &quot;oops/access.hpp&quot;
  51 #include &quot;oops/fieldStreams.inline.hpp&quot;
  52 #include &quot;oops/klass.hpp&quot;
  53 #include &quot;oops/method.inline.hpp&quot;
  54 #include &quot;oops/objArrayKlass.hpp&quot;
  55 #include &quot;oops/objArrayOop.inline.hpp&quot;
  56 #include &quot;oops/oop.inline.hpp&quot;
<span class="line-modified">  57 #include &quot;oops/inlineKlass.inline.hpp&quot;</span>
  58 #include &quot;prims/forte.hpp&quot;
  59 #include &quot;prims/jvmtiExport.hpp&quot;
  60 #include &quot;prims/methodHandles.hpp&quot;
  61 #include &quot;prims/nativeLookup.hpp&quot;
  62 #include &quot;runtime/arguments.hpp&quot;
  63 #include &quot;runtime/atomic.hpp&quot;
  64 #include &quot;runtime/biasedLocking.hpp&quot;
  65 #include &quot;runtime/frame.inline.hpp&quot;
  66 #include &quot;runtime/handles.inline.hpp&quot;
  67 #include &quot;runtime/init.hpp&quot;
  68 #include &quot;runtime/interfaceSupport.inline.hpp&quot;
  69 #include &quot;runtime/java.hpp&quot;
  70 #include &quot;runtime/javaCalls.hpp&quot;
  71 #include &quot;runtime/sharedRuntime.hpp&quot;
  72 #include &quot;runtime/stubRoutines.hpp&quot;
  73 #include &quot;runtime/synchronizer.hpp&quot;
  74 #include &quot;runtime/vframe.inline.hpp&quot;
  75 #include &quot;runtime/vframeArray.hpp&quot;
  76 #include &quot;utilities/copy.hpp&quot;
  77 #include &quot;utilities/dtrace.hpp&quot;
</pre>
<hr />
<pre>
1126     // This register map must be update since we need to find the receiver for
1127     // compiled frames. The receiver might be in a register.
1128     RegisterMap reg_map2(thread);
1129     frame stubFrame   = thread-&gt;last_frame();
1130     // Caller-frame is a compiled frame
1131     frame callerFrame = stubFrame.sender(&amp;reg_map2);
1132     bool caller_is_c1 = false;
1133 
1134     if (callerFrame.is_compiled_frame() &amp;&amp; !callerFrame.is_deoptimized_frame()) {
1135       caller_is_c1 = callerFrame.cb()-&gt;is_compiled_by_c1();
1136     }
1137 
1138     Method* callee = attached_method();
1139     if (callee == NULL) {
1140       callee = bytecode.static_target(CHECK_NH);
1141       if (callee == NULL) {
1142         THROW_(vmSymbols::java_lang_NoSuchMethodException(), nullHandle);
1143       }
1144     }
1145     if (!caller_is_c1 &amp;&amp; callee-&gt;has_scalarized_args() &amp;&amp; callee-&gt;method_holder()-&gt;is_inline_klass() &amp;&amp;
<span class="line-modified">1146         InlineKlass::cast(callee-&gt;method_holder())-&gt;can_be_passed_as_fields()) {</span>
1147       // If the receiver is an inline type that is passed as fields, no oop is available
1148       // Resolve the call without receiver null checking.
1149       assert(attached_method.not_null() &amp;&amp; !attached_method-&gt;is_abstract(), &quot;must have non-abstract attached method&quot;);
1150       if (bc == Bytecodes::_invokeinterface) {
1151         bc = Bytecodes::_invokevirtual; // C2 optimistically replaces interface calls by virtual calls
1152       }
1153       check_null_and_abstract = false;
1154     } else {
1155       // Retrieve from a compiled argument list
1156       receiver = Handle(THREAD, callerFrame.retrieve_receiver(&amp;reg_map2));
1157       if (receiver.is_null()) {
1158         THROW_(vmSymbols::java_lang_NullPointerException(), nullHandle);
1159       }
1160     }
1161   }
1162 
1163   // Resolve method
1164   if (attached_method.not_null()) {
1165     // Parameterized by attached method.
1166     LinkResolver::resolve_invoke(callinfo, receiver, attached_method, bc, check_null_and_abstract, CHECK_NH);
</pre>
<hr />
<pre>
1270 
1271   if (callee != NULL) {
1272     assert(callee-&gt;is_compiled(), &quot;must be nmethod for patching&quot;);
1273   }
1274 
1275   if (callee != NULL &amp;&amp; !callee-&gt;is_in_use()) {
1276     // Patch call site to C2I adapter if callee nmethod is deoptimized or unloaded.
1277     callee = NULL;
1278   }
1279   nmethodLocker nl_callee(callee);
1280 #ifdef ASSERT
1281   address dest_entry_point = callee == NULL ? 0 : callee-&gt;entry_point(); // used below
1282 #endif
1283 
1284   bool is_nmethod = caller_nm-&gt;is_nmethod();
1285   bool caller_is_c1 = caller_nm-&gt;is_compiled_by_c1();
1286 
1287   if (is_virtual) {
1288     Klass* receiver_klass = NULL;
1289     if (!caller_is_c1 &amp;&amp; callee_method-&gt;has_scalarized_args() &amp;&amp; callee_method-&gt;method_holder()-&gt;is_inline_klass() &amp;&amp;
<span class="line-modified">1290         InlineKlass::cast(callee_method-&gt;method_holder())-&gt;can_be_passed_as_fields()) {</span>
1291       // If the receiver is an inline type that is passed as fields, no oop is available
1292       receiver_klass = callee_method-&gt;method_holder();
1293     } else {
1294       assert(receiver.not_null() || invoke_code == Bytecodes::_invokehandle, &quot;sanity check&quot;);
1295       receiver_klass = invoke_code == Bytecodes::_invokehandle ? NULL : receiver-&gt;klass();
1296     }
1297     bool static_bound = call_info.resolved_method()-&gt;can_be_statically_bound();
1298     CompiledIC::compute_monomorphic_entry(callee_method, receiver_klass,
1299                      is_optimized, static_bound, is_nmethod, caller_is_c1, virtual_call_info,
1300                      CHECK_false);
1301   } else {
1302     // static call
1303     CompiledStaticCall::compute_entry(callee_method, caller_nm, static_call_info);
1304   }
1305 
1306   // grab lock, check for deoptimization and potentially patch caller
1307   {
1308     CompiledICLocker ml(caller_nm);
1309 
1310     // Lock blocks for safepoint during which both nmethods can change state.
</pre>
<hr />
<pre>
2731   }
2732 
2733   return entry;
2734 }
2735 
2736 
2737 CompiledEntrySignature::CompiledEntrySignature(Method* method) :
2738   _method(method), _num_value_args(0), _has_value_recv(false),
2739   _sig_cc(NULL), _sig_cc_ro(NULL), _regs(NULL), _regs_cc(NULL), _regs_cc_ro(NULL),
2740   _args_on_stack(0), _args_on_stack_cc(0), _args_on_stack_cc_ro(0),
2741   _c1_needs_stack_repair(false), _c2_needs_stack_repair(false), _has_scalarized_args(false) {
2742   _has_reserved_entries = false;
2743   _sig = new GrowableArray&lt;SigEntry&gt;(method-&gt;size_of_parameters());
2744 
2745 }
2746 
2747 int CompiledEntrySignature::compute_scalarized_cc(GrowableArray&lt;SigEntry&gt;*&amp; sig_cc, VMRegPair*&amp; regs_cc, bool scalar_receiver) {
2748   InstanceKlass* holder = _method-&gt;method_holder();
2749   sig_cc = new GrowableArray&lt;SigEntry&gt;(_method-&gt;size_of_parameters());
2750   if (!_method-&gt;is_static()) {
<span class="line-modified">2751     if (holder-&gt;is_inline_klass() &amp;&amp; scalar_receiver &amp;&amp; InlineKlass::cast(holder)-&gt;can_be_passed_as_fields()) {</span>
<span class="line-modified">2752       sig_cc-&gt;appendAll(InlineKlass::cast(holder)-&gt;extended_sig());</span>
2753     } else {
2754       SigEntry::add_entry(sig_cc, T_OBJECT);
2755     }
2756   }
2757   Thread* THREAD = Thread::current();
2758   for (SignatureStream ss(_method-&gt;signature()); !ss.at_return_type(); ss.next()) {
2759     if (ss.type() == T_INLINE_TYPE) {
<span class="line-modified">2760       InlineKlass* vk = ss.as_inline_klass(holder);</span>
2761       if (vk-&gt;can_be_passed_as_fields()) {
2762         sig_cc-&gt;appendAll(vk-&gt;extended_sig());
2763       } else {
2764         SigEntry::add_entry(sig_cc, T_OBJECT);
2765       }
2766     } else {
2767       SigEntry::add_entry(sig_cc, ss.type());
2768     }
2769   }
2770   regs_cc = NEW_RESOURCE_ARRAY(VMRegPair, sig_cc-&gt;length() + 2);
2771   return SharedRuntime::java_calling_convention(sig_cc, regs_cc);
2772 }
2773 
2774 int CompiledEntrySignature::insert_reserved_entry(int ret_off) {
2775   // Find index in signature that belongs to return address slot
2776   BasicType bt = T_ILLEGAL;
2777   int i = 0;
2778   for (uint off = 0; i &lt; _sig_cc-&gt;length(); ++i) {
2779     if (SigEntry::skip_value_delimiters(_sig_cc, i)) {
2780       VMReg first = _regs_cc[off++].first();
</pre>
<hr />
<pre>
2790   // Insert reserved entry and re-compute calling convention
2791   SigEntry::insert_reserved_entry(_sig_cc, i, bt);
2792   return SharedRuntime::java_calling_convention(_sig_cc, _regs_cc);
2793 }
2794 
2795 // See if we can save space by sharing the same entry for VVEP and VVEP(RO),
2796 // or the same entry for VEP and VVEP(RO).
2797 CodeOffsets::Entries CompiledEntrySignature::c1_value_ro_entry_type() const {
2798   if (!has_scalarized_args()) {
2799     // VEP/VVEP/VVEP(RO) all share the same entry. There&#39;s no packing.
2800     return CodeOffsets::Verified_Entry;
2801   }
2802   if (_method-&gt;is_static()) {
2803     // Static methods don&#39;t need VVEP(RO)
2804     return CodeOffsets::Verified_Entry;
2805   }
2806 
2807   if (has_value_recv()) {
2808     if (num_value_args() == 1) {
2809       // Share same entry for VVEP and VVEP(RO).
<span class="line-modified">2810       // This is quite common: we have an instance method in a InlineKlass that has</span>
2811       // no value args other than &lt;this&gt;.
2812       return CodeOffsets::Verified_Value_Entry;
2813     } else {
2814       assert(num_value_args() &gt; 1, &quot;must be&quot;);
2815       // No sharing:
2816       //   VVEP(RO) -- &lt;this&gt; is passed as object
2817       //   VEP      -- &lt;this&gt; is passed as fields
2818       return CodeOffsets::Verified_Value_Entry_RO;
2819     }
2820   }
2821 
2822   // Either a static method, or &lt;this&gt; is not a value type
2823   if (args_on_stack_cc() != args_on_stack_cc_ro() || _has_reserved_entries) {
2824     // No sharing:
2825     // Some arguments are passed on the stack, and we have inserted reserved entries
2826     // into the VEP, but we never insert reserved entries into the VVEP(RO).
2827     return CodeOffsets::Verified_Value_Entry_RO;
2828   } else {
2829     // Share same entry for VEP and VVEP(RO).
2830     return CodeOffsets::Verified_Entry;
2831   }
2832 }
2833 
2834 
2835 void CompiledEntrySignature::compute_calling_conventions() {
<span class="line-modified">2836   // Get the (non-scalarized) signature and check for inline type arguments</span>
2837   if (!_method-&gt;is_static()) {
<span class="line-modified">2838     if (_method-&gt;method_holder()-&gt;is_inline_klass() &amp;&amp; InlineKlass::cast(_method-&gt;method_holder())-&gt;can_be_passed_as_fields()) {</span>
2839       _has_value_recv = true;
2840       _num_value_args++;
2841     }
2842     SigEntry::add_entry(_sig, T_OBJECT);
2843   }
2844   for (SignatureStream ss(_method-&gt;signature()); !ss.at_return_type(); ss.next()) {
2845     BasicType bt = ss.type();
2846     if (bt == T_INLINE_TYPE) {
<span class="line-modified">2847       if (ss.as_inline_klass(_method-&gt;method_holder())-&gt;can_be_passed_as_fields()) {</span>
2848         _num_value_args++;
2849       }
2850       bt = T_OBJECT;
2851     }
2852     SigEntry::add_entry(_sig, bt);
2853   }
2854   if (_method-&gt;is_abstract() &amp;&amp; !has_value_arg()) {
2855     return;
2856   }
2857 
2858   // Get a description of the compiled java calling convention and the largest used (VMReg) stack slot usage
2859   _regs = NEW_RESOURCE_ARRAY(VMRegPair, _sig-&gt;length());
2860   _args_on_stack = SharedRuntime::java_calling_convention(_sig, _regs);
2861 
2862   // Now compute the scalarized calling convention if there are value types in the signature
2863   _sig_cc = _sig;
2864   _sig_cc_ro = _sig;
2865   _regs_cc = _regs;
2866   _regs_cc_ro = _regs;
2867   _args_on_stack_cc = _args_on_stack;
</pre>
<hr />
<pre>
2897         // TODO can we avoid wasting a stack slot here?
2898         //assert(ret_off != ret_off_ro, &quot;fail&quot;);
2899         if (ret_off &gt; ret_off_ro) {
2900           swap(ret_off, ret_off_ro); // Sort by offset
2901         }
2902         _args_on_stack_cc = insert_reserved_entry(ret_off);
2903         _args_on_stack_cc = insert_reserved_entry(ret_off_ro);
2904       } else {
2905         ret_off += 2; // Account for one reserved entry (2 slots)
2906         ret_off = align_up(ret_off, alignment);
2907         _args_on_stack_cc = insert_reserved_entry(ret_off);
2908       }
2909 
2910       _has_reserved_entries = true;
2911     }
2912 
2913     // Upper bound on stack arguments to avoid hitting the argument limit and
2914     // bailing out of compilation (&quot;unsupported incoming calling sequence&quot;).
2915     // TODO we need a reasonable limit (flag?) here
2916     if (_args_on_stack_cc &gt; 50) {
<span class="line-modified">2917       // Don&#39;t scalarize inline type arguments</span>
2918       _sig_cc = _sig;
2919       _sig_cc_ro = _sig;
2920       _regs_cc = _regs;
2921       _regs_cc_ro = _regs;
2922       _args_on_stack_cc = _args_on_stack;
2923       _args_on_stack_cc_ro = _args_on_stack;
2924     } else {
2925       _c1_needs_stack_repair = (_args_on_stack_cc &lt; _args_on_stack) || (_args_on_stack_cc_ro &lt; _args_on_stack);
2926       _c2_needs_stack_repair = (_args_on_stack_cc &gt; _args_on_stack) || (_args_on_stack_cc &gt; _args_on_stack_cc_ro);
2927       _has_scalarized_args = true;
2928     }
2929   }
2930 }
2931 
2932 AdapterHandlerEntry* AdapterHandlerLibrary::get_adapter0(const methodHandle&amp; method) {
2933   // Use customized signature handler.  Need to lock around updates to
2934   // the AdapterHandlerTable (it is not safe for concurrent readers
2935   // and a single writer: this could be fixed if it becomes a
2936   // problem).
2937 
</pre>
<hr />
<pre>
2950     CompiledEntrySignature ces(method());
2951     {
2952        MutexUnlocker mul(AdapterHandlerLibrary_lock);
2953        ces.compute_calling_conventions();
2954     }
2955     GrowableArray&lt;SigEntry&gt;&amp; sig       = ces.sig();
2956     GrowableArray&lt;SigEntry&gt;&amp; sig_cc    = ces.sig_cc();
2957     GrowableArray&lt;SigEntry&gt;&amp; sig_cc_ro = ces.sig_cc_ro();
2958     VMRegPair* regs         = ces.regs();
2959     VMRegPair* regs_cc      = ces.regs_cc();
2960     VMRegPair* regs_cc_ro   = ces.regs_cc_ro();
2961 
2962     if (ces.has_scalarized_args()) {
2963       method-&gt;set_has_scalarized_args(true);
2964       method-&gt;set_c1_needs_stack_repair(ces.c1_needs_stack_repair());
2965       method-&gt;set_c2_needs_stack_repair(ces.c2_needs_stack_repair());
2966     }
2967 
2968     if (method-&gt;is_abstract()) {
2969       if (ces.has_scalarized_args()) {
<span class="line-modified">2970         // Save a C heap allocated version of the signature for abstract methods with scalarized inline type arguments</span>
2971         address wrong_method_abstract = SharedRuntime::get_handle_wrong_method_abstract_stub();
2972         entry = AdapterHandlerLibrary::new_entry(new AdapterFingerPrint(NULL),
2973                                                  StubRoutines::throw_AbstractMethodError_entry(),
2974                                                  wrong_method_abstract, wrong_method_abstract, wrong_method_abstract,
2975                                                  wrong_method_abstract, wrong_method_abstract);
2976         GrowableArray&lt;SigEntry&gt;* heap_sig = new (ResourceObj::C_HEAP, mtInternal) GrowableArray&lt;SigEntry&gt;(sig_cc_ro.length(), mtInternal);
2977         heap_sig-&gt;appendAll(&amp;sig_cc_ro);
2978         entry-&gt;set_sig_cc(heap_sig);
2979         return entry;
2980       } else {
2981         return _abstract_method_handler;
2982       }
2983     }
2984 
2985     // Lookup method signature&#39;s fingerprint
2986     entry = _adapters-&gt;lookup(&amp;sig_cc, regs_cc != regs_cc_ro);
2987 
2988 #ifdef ASSERT
2989     AdapterHandlerEntry* shared_entry = NULL;
2990     // Start adapter sharing verification only after the VM is booted.
</pre>
<hr />
<pre>
3572       fr = fr.java_sender();
3573     }
3574   }
3575   return activation;
3576 }
3577 
3578 void SharedRuntime::on_slowpath_allocation_exit(JavaThread* thread) {
3579   // After any safepoint, just before going back to compiled code,
3580   // we inform the GC that we will be doing initializing writes to
3581   // this object in the future without emitting card-marks, so
3582   // GC may take any compensating steps.
3583 
3584   oop new_obj = thread-&gt;vm_result();
3585   if (new_obj == NULL) return;
3586 
3587   BarrierSet *bs = BarrierSet::barrier_set();
3588   bs-&gt;on_slowpath_allocation_exit(thread, new_obj);
3589 }
3590 
3591 // We are at a compiled code to interpreter call. We need backing
<span class="line-modified">3592 // buffers for all inline type arguments. Allocate an object array to</span>
3593 // hold them (convenient because once we&#39;re done with it we don&#39;t have
3594 // to worry about freeing it).
3595 oop SharedRuntime::allocate_value_types_impl(JavaThread* thread, methodHandle callee, bool allocate_receiver, TRAPS) {
3596   assert(InlineTypePassFieldsAsArgs, &quot;no reason to call this&quot;);
3597   ResourceMark rm;
3598 
3599   int nb_slots = 0;
3600   InstanceKlass* holder = callee-&gt;method_holder();
3601   allocate_receiver &amp;= !callee-&gt;is_static() &amp;&amp; holder-&gt;is_inline_klass();
3602   if (allocate_receiver) {
3603     nb_slots++;
3604   }
3605   for (SignatureStream ss(callee-&gt;signature()); !ss.at_return_type(); ss.next()) {
3606     if (ss.type() == T_INLINE_TYPE) {
3607       nb_slots++;
3608     }
3609   }
3610   objArrayOop array_oop = oopFactory::new_objectArray(nb_slots, CHECK_NULL);
3611   objArrayHandle array(THREAD, array_oop);
3612   int i = 0;
3613   if (allocate_receiver) {
<span class="line-modified">3614     InlineKlass* vk = InlineKlass::cast(holder);</span>
3615     oop res = vk-&gt;allocate_instance(CHECK_NULL);
3616     array-&gt;obj_at_put(i, res);
3617     i++;
3618   }
3619   for (SignatureStream ss(callee-&gt;signature()); !ss.at_return_type(); ss.next()) {
3620     if (ss.type() == T_INLINE_TYPE) {
<span class="line-modified">3621       InlineKlass* vk = ss.as_inline_klass(holder);</span>
3622       oop res = vk-&gt;allocate_instance(CHECK_NULL);
3623       array-&gt;obj_at_put(i, res);
3624       i++;
3625     }
3626   }
3627   return array();
3628 }
3629 
3630 JRT_ENTRY(void, SharedRuntime::allocate_value_types(JavaThread* thread, Method* callee_method, bool allocate_receiver))
3631   methodHandle callee(thread, callee_method);
3632   oop array = SharedRuntime::allocate_value_types_impl(thread, callee, allocate_receiver, CHECK);
3633   thread-&gt;set_vm_result(array);
3634   thread-&gt;set_vm_result_2(callee()); // TODO: required to keep callee live?
3635 JRT_END
3636 
3637 // TODO remove this once the AARCH64 dependency is gone
3638 // Iterate over the array of heap allocated value types and apply the GC post barrier to all reference fields.
<span class="line-modified">3639 // This is called from the C2I adapter after inline type arguments are heap allocated and initialized.</span>
3640 JRT_LEAF(void, SharedRuntime::apply_post_barriers(JavaThread* thread, objArrayOopDesc* array))
3641 {
3642   assert(InlineTypePassFieldsAsArgs, &quot;no reason to call this&quot;);
3643   assert(oopDesc::is_oop(array), &quot;should be oop&quot;);
3644   for (int i = 0; i &lt; array-&gt;length(); ++i) {
3645     instanceOop valueOop = (instanceOop)array-&gt;obj_at(i);
<span class="line-modified">3646     InlineKlass* vk = InlineKlass::cast(valueOop-&gt;klass());</span>
3647     if (vk-&gt;contains_oops()) {
3648       const address dst_oop_addr = ((address) (void*) valueOop);
3649       OopMapBlock* map = vk-&gt;start_of_nonstatic_oop_maps();
3650       OopMapBlock* const end = map + vk-&gt;nonstatic_oop_map_count();
3651       while (map != end) {
3652         address doop_address = dst_oop_addr + map-&gt;offset();
3653         barrier_set_cast&lt;ModRefBarrierSet&gt;(BarrierSet::barrier_set())-&gt;
3654           write_ref_array((HeapWord*) doop_address, map-&gt;count());
3655         map++;
3656       }
3657     }
3658   }
3659 }
3660 JRT_END
3661 
3662 // We&#39;re returning from an interpreted method: load each field into a
3663 // register following the calling convention
3664 JRT_LEAF(void, SharedRuntime::load_value_type_fields_in_regs(JavaThread* thread, oopDesc* res))
3665 {
3666   assert(res-&gt;klass()-&gt;is_inline_klass(), &quot;only inline types here&quot;);
3667   ResourceMark rm;
3668   RegisterMap reg_map(thread);
3669   frame stubFrame = thread-&gt;last_frame();
3670   frame callerFrame = stubFrame.sender(&amp;reg_map);
3671   assert(callerFrame.is_interpreted_frame(), &quot;should be coming from interpreter&quot;);
3672 
<span class="line-modified">3673   InlineKlass* vk = InlineKlass::cast(res-&gt;klass());</span>
3674 
3675   const Array&lt;SigEntry&gt;* sig_vk = vk-&gt;extended_sig();
3676   const Array&lt;VMRegPair&gt;* regs = vk-&gt;return_regs();
3677 
3678   if (regs == NULL) {
3679     // The fields of the value klass don&#39;t fit in registers, bail out
3680     return;
3681   }
3682 
3683   int j = 1;
3684   for (int i = 0; i &lt; sig_vk-&gt;length(); i++) {
3685     BasicType bt = sig_vk-&gt;at(i)._bt;
3686     if (bt == T_INLINE_TYPE) {
3687       continue;
3688     }
3689     if (bt == T_VOID) {
3690       if (sig_vk-&gt;at(i-1)._bt == T_LONG ||
3691           sig_vk-&gt;at(i-1)._bt == T_DOUBLE) {
3692         j++;
3693       }
</pre>
<hr />
<pre>
3743   VMRegPair pair = regs-&gt;at(0);
3744   address loc = reg_map.location(pair.first());
3745   assert(*(oopDesc**)loc == res, &quot;overwritten object&quot;);
3746 #endif
3747 
3748   thread-&gt;set_vm_result(res);
3749 }
3750 JRT_END
3751 
3752 // We&#39;ve returned to an interpreted method, the interpreter needs a
3753 // reference to a value type instance. Allocate it and initialize it
3754 // from field&#39;s values in registers.
3755 JRT_BLOCK_ENTRY(void, SharedRuntime::store_value_type_fields_to_buf(JavaThread* thread, intptr_t res))
3756 {
3757   ResourceMark rm;
3758   RegisterMap reg_map(thread);
3759   frame stubFrame = thread-&gt;last_frame();
3760   frame callerFrame = stubFrame.sender(&amp;reg_map);
3761 
3762 #ifdef ASSERT
<span class="line-modified">3763   InlineKlass* verif_vk = InlineKlass::returned_inline_klass(reg_map);</span>
3764 #endif
3765 
3766   if (!is_set_nth_bit(res, 0)) {
3767     // We&#39;re not returning with value type fields in registers (the
3768     // calling convention didn&#39;t allow it for this value klass)
3769     assert(!Metaspace::contains((void*)res), &quot;should be oop or pointer in buffer area&quot;);
3770     thread-&gt;set_vm_result((oopDesc*)res);
3771     assert(verif_vk == NULL, &quot;broken calling convention&quot;);
3772     return;
3773   }
3774 
3775   clear_nth_bit(res, 0);
<span class="line-modified">3776   InlineKlass* vk = (InlineKlass*)res;</span>
3777   assert(verif_vk == vk, &quot;broken calling convention&quot;);
3778   assert(Metaspace::contains((void*)res), &quot;should be klass&quot;);
3779 
3780   // Allocate handles for every oop field so they are safe in case of
3781   // a safepoint when allocating
3782   GrowableArray&lt;Handle&gt; handles;
3783   vk-&gt;save_oop_fields(reg_map, handles);
3784 
3785   // It&#39;s unsafe to safepoint until we are here
3786   JRT_BLOCK;
3787   {
3788     Thread* THREAD = thread;
3789     oop vt = vk-&gt;realloc_result(reg_map, handles, CHECK);
3790     thread-&gt;set_vm_result(vt);
3791   }
3792   JRT_BLOCK_END;
3793 }
3794 JRT_END
3795 
</pre>
</td>
</tr>
</table>
<center><a href="safepoint.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="sharedRuntime.hpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>