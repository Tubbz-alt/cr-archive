diff a/src/hotspot/cpu/aarch64/interp_masm_aarch64.cpp b/src/hotspot/cpu/aarch64/interp_masm_aarch64.cpp
--- a/src/hotspot/cpu/aarch64/interp_masm_aarch64.cpp
+++ b/src/hotspot/cpu/aarch64/interp_masm_aarch64.cpp
@@ -33,11 +33,11 @@
 #include "logging/log.hpp"
 #include "oops/arrayOop.hpp"
 #include "oops/markWord.hpp"
 #include "oops/method.hpp"
 #include "oops/methodData.hpp"
-#include "oops/valueKlass.hpp"
+#include "oops/inlineKlass.hpp"
 #include "prims/jvmtiExport.hpp"
 #include "prims/jvmtiThreadState.hpp"
 #include "runtime/basicLock.hpp"
 #include "runtime/biasedLocking.hpp"
 #include "runtime/frame.inline.hpp"
@@ -698,12 +698,12 @@
 
     // We are returning a value type, load its fields into registers
     // Load fields from a buffered value with a value class specific handler
 
     load_klass(rscratch1 /*dst*/, r0 /*src*/);
-    ldr(rscratch1, Address(rscratch1, InstanceKlass::adr_valueklass_fixed_block_offset()));
-    ldr(rscratch1, Address(rscratch1, ValueKlass::unpack_handler_offset()));
+    ldr(rscratch1, Address(rscratch1, InstanceKlass::adr_inlineklass_fixed_block_offset()));
+    ldr(rscratch1, Address(rscratch1, InlineKlass::unpack_handler_offset()));
     cbz(rscratch1, skip);
 
     blr(rscratch1);
 
     // call above kills the value in r1. Reload it.
diff a/src/hotspot/cpu/aarch64/macroAssembler_aarch64.cpp b/src/hotspot/cpu/aarch64/macroAssembler_aarch64.cpp
--- a/src/hotspot/cpu/aarch64/macroAssembler_aarch64.cpp
+++ b/src/hotspot/cpu/aarch64/macroAssembler_aarch64.cpp
@@ -5279,16 +5279,16 @@
   // Try to allocate a new buffered value (from the heap)
   if (UseTLAB) {
 
     if (vk != NULL) {
       // Called from C1, where the return type is statically known.
-      mov(r1, (intptr_t)vk->get_ValueKlass());
+      mov(r1, (intptr_t)vk->get_InlineKlass());
       jint lh = vk->layout_helper();
       assert(lh != Klass::_lh_neutral_value, "inline class in return type must have been resolved");
       mov(r14, lh);
     } else {
-       // Call from interpreter. R0 contains ((the ValueKlass* of the return type) | 0x01)
+       // Call from interpreter. R0 contains ((the InlineKlass* of the return type) | 0x01)
        andr(r1, r0, -2);
        // get obj size
        ldrw(r14, Address(rscratch1 /*klass*/, Klass::layout_helper_offset()));
     }
 
@@ -5324,12 +5324,12 @@
         far_call(RuntimeAddress(vk->pack_handler())); // no need for call info as this will not safepoint.
       } else {
 
         // We have our new buffered value, initialize its fields with a
         // value class specific handler
-        ldr(r1, Address(r0, InstanceKlass::adr_valueklass_fixed_block_offset()));
-        ldr(r1, Address(r1, ValueKlass::pack_handler_offset()));
+        ldr(r1, Address(r0, InstanceKlass::adr_inlineklass_fixed_block_offset()));
+        ldr(r1, Address(r1, InlineKlass::pack_handler_offset()));
 
         // Mov new class to r0 and call pack_handler
         mov(r0, r13);
         blr(r1);
       }
diff a/src/hotspot/cpu/aarch64/sharedRuntime_aarch64.cpp b/src/hotspot/cpu/aarch64/sharedRuntime_aarch64.cpp
--- a/src/hotspot/cpu/aarch64/sharedRuntime_aarch64.cpp
+++ b/src/hotspot/cpu/aarch64/sharedRuntime_aarch64.cpp
@@ -3322,11 +3322,11 @@
 
   // Set exception blob
   _exception_blob =  ExceptionBlob::create(&buffer, oop_maps, SimpleRuntimeFrame::framesize >> 1);
 }
 
-BufferedValueTypeBlob* SharedRuntime::generate_buffered_value_type_adapter(const ValueKlass* vk) {
+BufferedValueTypeBlob* SharedRuntime::generate_buffered_inline_type_adapter(const InlineKlass* vk) {
   BufferBlob* buf = BufferBlob::create("value types pack/unpack", 16 * K);
   CodeBuffer buffer(buf);
   short buffer_locs[20];
   buffer.insts()->initialize_shared_locs((relocInfo*)buffer_locs,
                                          sizeof(buffer_locs)/sizeof(relocInfo));
diff a/src/hotspot/cpu/aarch64/stubGenerator_aarch64.cpp b/src/hotspot/cpu/aarch64/stubGenerator_aarch64.cpp
--- a/src/hotspot/cpu/aarch64/stubGenerator_aarch64.cpp
+++ b/src/hotspot/cpu/aarch64/stubGenerator_aarch64.cpp
@@ -373,12 +373,12 @@
       // Check for flattened return value
       __ cbz(r0, is_long);
       // Initialize pre-allocated buffer
       __ mov(r1, r0);
       __ andr(r1, r1, -2);
-      __ ldr(r1, Address(r1, InstanceKlass::adr_valueklass_fixed_block_offset()));
-      __ ldr(r1, Address(r1, ValueKlass::pack_handler_offset()));
+      __ ldr(r1, Address(r1, InstanceKlass::adr_inlineklass_fixed_block_offset()));
+      __ ldr(r1, Address(r1, InlineKlass::pack_handler_offset()));
       __ ldr(r0, Address(j_rarg2, 0));
       __ blr(r1);
       __ b(exit);
     }
 
diff a/src/hotspot/cpu/aarch64/templateInterpreterGenerator_aarch64.cpp b/src/hotspot/cpu/aarch64/templateInterpreterGenerator_aarch64.cpp
--- a/src/hotspot/cpu/aarch64/templateInterpreterGenerator_aarch64.cpp
+++ b/src/hotspot/cpu/aarch64/templateInterpreterGenerator_aarch64.cpp
@@ -36,11 +36,11 @@
 #include "memory/resourceArea.hpp"
 #include "oops/arrayOop.hpp"
 #include "oops/methodData.hpp"
 #include "oops/method.hpp"
 #include "oops/oop.inline.hpp"
-#include "oops/valueKlass.hpp"
+#include "oops/inlineKlass.hpp"
 #include "prims/jvmtiExport.hpp"
 #include "prims/jvmtiThreadState.hpp"
 #include "runtime/arguments.hpp"
 #include "runtime/deoptimization.hpp"
 #include "runtime/frame.inline.hpp"
diff a/src/hotspot/cpu/x86/interp_masm_x86.cpp b/src/hotspot/cpu/x86/interp_masm_x86.cpp
--- a/src/hotspot/cpu/x86/interp_masm_x86.cpp
+++ b/src/hotspot/cpu/x86/interp_masm_x86.cpp
@@ -29,11 +29,11 @@
 #include "logging/log.hpp"
 #include "oops/arrayOop.hpp"
 #include "oops/markWord.hpp"
 #include "oops/methodData.hpp"
 #include "oops/method.hpp"
-#include "oops/valueKlass.hpp"
+#include "oops/inlineKlass.hpp"
 #include "prims/jvmtiExport.hpp"
 #include "prims/jvmtiThreadState.hpp"
 #include "runtime/basicLock.hpp"
 #include "runtime/biasedLocking.hpp"
 #include "runtime/frame.inline.hpp"
@@ -1168,12 +1168,12 @@
     super_call_VM_leaf(StubRoutines::load_value_type_fields_in_regs());
 #else
     // Load fields from a buffered value with a value class specific handler
     Register tmp_load_klass = LP64_ONLY(rscratch1) NOT_LP64(noreg);
     load_klass(rdi, rax, tmp_load_klass);
-    movptr(rdi, Address(rdi, InstanceKlass::adr_valueklass_fixed_block_offset()));
-    movptr(rdi, Address(rdi, ValueKlass::unpack_handler_offset()));
+    movptr(rdi, Address(rdi, InstanceKlass::adr_inlineklass_fixed_block_offset()));
+    movptr(rdi, Address(rdi, InlineKlass::unpack_handler_offset()));
 
     testptr(rdi, rdi);
     jcc(Assembler::equal, skip);
 
     call(rdi);
diff a/src/hotspot/cpu/x86/macroAssembler_x86.cpp b/src/hotspot/cpu/x86/macroAssembler_x86.cpp
--- a/src/hotspot/cpu/x86/macroAssembler_x86.cpp
+++ b/src/hotspot/cpu/x86/macroAssembler_x86.cpp
@@ -3638,12 +3638,12 @@
     bind(done_check);
   }
 #endif
   Register offset = temp_reg;
   // Getting the offset of the pre-allocated default value
-  movptr(offset, Address(value_klass, in_bytes(InstanceKlass::adr_valueklass_fixed_block_offset())));
-  movl(offset, Address(offset, in_bytes(ValueKlass::default_value_offset_offset())));
+  movptr(offset, Address(value_klass, in_bytes(InstanceKlass::adr_inlineklass_fixed_block_offset())));
+  movl(offset, Address(offset, in_bytes(InlineKlass::default_value_offset_offset())));
 
   // Getting the mirror
   movptr(obj, Address(value_klass, in_bytes(Klass::java_mirror_offset())));
   resolve_oop_handle(obj, value_klass);
 
@@ -4681,12 +4681,12 @@
   BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();
   bs->value_copy(this, decorators, src, dst, value_klass);
 }
 
 void MacroAssembler::first_field_offset(Register value_klass, Register offset) {
-  movptr(offset, Address(value_klass, InstanceKlass::adr_valueklass_fixed_block_offset()));
-  movl(offset, Address(offset, ValueKlass::first_field_offset_offset()));
+  movptr(offset, Address(value_klass, InstanceKlass::adr_inlineklass_fixed_block_offset()));
+  movl(offset, Address(offset, InlineKlass::first_field_offset_offset()));
 }
 
 void MacroAssembler::data_for_oop(Register oop, Register data, Register value_klass) {
   // ((address) (void*) o) + vk->first_field_offset();
   Register offset = (data == oop) ? rscratch1 : data;
@@ -5226,11 +5226,11 @@
       movptr(rbx, (intptr_t)vk->get_ValueKlass());
       jint lh = vk->layout_helper();
       assert(lh != Klass::_lh_neutral_value, "inline class in return type must have been resolved");
       movl(r14, lh);
     } else {
-      // Call from interpreter. RAX contains ((the ValueKlass* of the return type) | 0x01)
+      // Call from interpreter. RAX contains ((the InlineKlass* of the return type) | 0x01)
       mov(rbx, rax);
       andptr(rbx, -2);
       movl(r14, Address(rbx, Klass::layout_helper_offset()));
     }
 
@@ -5256,12 +5256,12 @@
     if (vk != NULL) {
       // FIXME -- do the packing in-line to avoid the runtime call
       mov(rax, r13);
       call(RuntimeAddress(vk->pack_handler())); // no need for call info as this will not safepoint.
     } else {
-      movptr(rbx, Address(rax, InstanceKlass::adr_valueklass_fixed_block_offset()));
-      movptr(rbx, Address(rbx, ValueKlass::pack_handler_offset()));
+      movptr(rbx, Address(rax, InstanceKlass::adr_inlineklass_fixed_block_offset()));
+      movptr(rbx, Address(rbx, InlineKlass::pack_handler_offset()));
       mov(rax, r13);
       call(rbx);
     }
     jmp(skip);
   }
diff a/src/hotspot/cpu/x86/macroAssembler_x86.hpp b/src/hotspot/cpu/x86/macroAssembler_x86.hpp
--- a/src/hotspot/cpu/x86/macroAssembler_x86.hpp
+++ b/src/hotspot/cpu/x86/macroAssembler_x86.hpp
@@ -103,14 +103,14 @@
 
   // valueKlass queries, kills temp_reg
   void test_klass_is_value(Register klass, Register temp_reg, Label& is_value);
   void test_klass_is_empty_value(Register klass, Register temp_reg, Label& is_empty_value);
 
-  // Get the default value oop for the given ValueKlass
+  // Get the default value oop for the given InlineKlass
   void get_default_value_oop(Register value_klass, Register temp_reg, Register obj);
-  // The empty value oop, for the given ValueKlass ("empty" as in no instance fields)
-  // get_default_value_oop with extra assertion for empty value klass
+  // The empty value oop, for the given InlineKlass ("empty" as in no instance fields)
+  // get_default_value_oop with extra assertion for empty inline klass
   void get_empty_value_oop(Register value_klass, Register temp_reg, Register obj);
 
   void test_field_is_inline_type(Register flags, Register temp_reg, Label& is_inline);
   void test_field_is_not_inline_type(Register flags, Register temp_reg, Label& not_inline);
   void test_field_is_inlined(Register flags, Register temp_reg, Label& is_inlined);
diff a/src/hotspot/cpu/x86/sharedRuntime_x86_32.cpp b/src/hotspot/cpu/x86/sharedRuntime_x86_32.cpp
--- a/src/hotspot/cpu/x86/sharedRuntime_x86_32.cpp
+++ b/src/hotspot/cpu/x86/sharedRuntime_x86_32.cpp
@@ -3323,9 +3323,9 @@
   // return the  blob
   // frame_size_words or bytes??
   return RuntimeStub::new_runtime_stub(name, &buffer, frame_complete, frame_size_words, oop_maps, true);
 }
 
-BufferedValueTypeBlob* SharedRuntime::generate_buffered_value_type_adapter(const ValueKlass* vk) {
+BufferedValueTypeBlob* SharedRuntime::generate_buffered_inline_type_adapter(const InlineKlass* vk) {
   Unimplemented();
   return NULL;
 }
diff a/src/hotspot/cpu/x86/sharedRuntime_x86_64.cpp b/src/hotspot/cpu/x86/sharedRuntime_x86_64.cpp
--- a/src/hotspot/cpu/x86/sharedRuntime_x86_64.cpp
+++ b/src/hotspot/cpu/x86/sharedRuntime_x86_64.cpp
@@ -4321,11 +4321,11 @@
   // Set exception blob
   _exception_blob =  ExceptionBlob::create(&buffer, oop_maps, SimpleRuntimeFrame::framesize >> 1);
 }
 #endif // COMPILER2
 
-BufferedValueTypeBlob* SharedRuntime::generate_buffered_value_type_adapter(const ValueKlass* vk) {
+BufferedValueTypeBlob* SharedRuntime::generate_buffered_inline_type_adapter(const InlineKlass* vk) {
   BufferBlob* buf = BufferBlob::create("value types pack/unpack", 16 * K);
   CodeBuffer buffer(buf);
   short buffer_locs[20];
   buffer.insts()->initialize_shared_locs((relocInfo*)buffer_locs,
                                          sizeof(buffer_locs)/sizeof(relocInfo));
diff a/src/hotspot/cpu/x86/stubGenerator_x86_64.cpp b/src/hotspot/cpu/x86/stubGenerator_x86_64.cpp
--- a/src/hotspot/cpu/x86/stubGenerator_x86_64.cpp
+++ b/src/hotspot/cpu/x86/stubGenerator_x86_64.cpp
@@ -419,12 +419,12 @@
       // Check for flattened return value
       __ testptr(rax, 1);
       __ jcc(Assembler::zero, is_long);
       // Load pack handler address
       __ andptr(rax, -2);
-      __ movptr(rax, Address(rax, InstanceKlass::adr_valueklass_fixed_block_offset()));
-      __ movptr(rbx, Address(rax, ValueKlass::pack_handler_jobject_offset()));
+      __ movptr(rax, Address(rax, InstanceKlass::adr_inlineklass_fixed_block_offset()));
+      __ movptr(rbx, Address(rax, InlineKlass::pack_handler_jobject_offset()));
       // Call pack handler to initialize the buffer
       __ call(rbx);
       __ jmp(exit);
     }
     __ BIND(is_long);
diff a/src/hotspot/cpu/x86/templateInterpreterGenerator_x86.cpp b/src/hotspot/cpu/x86/templateInterpreterGenerator_x86.cpp
--- a/src/hotspot/cpu/x86/templateInterpreterGenerator_x86.cpp
+++ b/src/hotspot/cpu/x86/templateInterpreterGenerator_x86.cpp
@@ -34,11 +34,11 @@
 #include "interpreter/templateTable.hpp"
 #include "oops/arrayOop.hpp"
 #include "oops/methodData.hpp"
 #include "oops/method.hpp"
 #include "oops/oop.inline.hpp"
-#include "oops/valueKlass.hpp"
+#include "oops/inlineKlass.hpp"
 #include "prims/jvmtiExport.hpp"
 #include "prims/jvmtiThreadState.hpp"
 #include "runtime/arguments.hpp"
 #include "runtime/deoptimization.hpp"
 #include "runtime/frame.inline.hpp"
diff a/src/hotspot/cpu/x86/templateTable_x86.cpp b/src/hotspot/cpu/x86/templateTable_x86.cpp
--- a/src/hotspot/cpu/x86/templateTable_x86.cpp
+++ b/src/hotspot/cpu/x86/templateTable_x86.cpp
@@ -31,11 +31,11 @@
 #include "interpreter/templateTable.hpp"
 #include "memory/universe.hpp"
 #include "oops/methodData.hpp"
 #include "oops/objArrayKlass.hpp"
 #include "oops/oop.inline.hpp"
-#include "oops/valueKlass.hpp"
+#include "oops/inlineKlass.hpp"
 #include "prims/methodHandles.hpp"
 #include "runtime/frame.inline.hpp"
 #include "runtime/safepointMechanism.hpp"
 #include "runtime/sharedRuntime.hpp"
 #include "runtime/stubRoutines.hpp"
@@ -4419,11 +4419,11 @@
 
   // make sure klass is fully initialized
   __ cmpb(Address(rcx, InstanceKlass::init_state_offset()), InstanceKlass::fully_initialized);
   __ jcc(Assembler::notEqual, slow_case);
 
-  // have a resolved ValueKlass in rcx, return the default value oop from it
+  // have a resolved InlineKlass in rcx, return the default value oop from it
   __ get_default_value_oop(rcx, rdx, rax);
   __ jmp(done);
 
   __ bind(slow_case);
 
diff a/src/hotspot/share/asm/macroAssembler_common.cpp b/src/hotspot/share/asm/macroAssembler_common.cpp
--- a/src/hotspot/share/asm/macroAssembler_common.cpp
+++ b/src/hotspot/share/asm/macroAssembler_common.cpp
@@ -25,11 +25,11 @@
 #include "precompiled.hpp"
 #include "jvm.h"
 #include "asm/assembler.hpp"
 #include "asm/assembler.inline.hpp"
 #include "asm/macroAssembler.hpp"
-#include "oops/valueKlass.inline.hpp"
+#include "oops/inlineKlass.inline.hpp"
 #include "runtime/signature_cc.hpp"
 #include "runtime/sharedRuntime.hpp"
 #ifdef COMPILER2
 #include "opto/compile.hpp"
 #include "opto/node.hpp"
@@ -146,11 +146,11 @@
       }
     }
   } else {
     // Only unpack the receiver, all other arguments are already scalarized
     InstanceKlass* holder = method->method_holder();
-    int rec_len = holder->is_inline_klass() ? ValueKlass::cast(holder)->extended_sig()->length() : 1;
+    int rec_len = holder->is_inline_klass() ? InlineKlass::cast(holder)->extended_sig()->length() : 1;
     // Copy scalarized signature but skip receiver, value type delimiters and reserved entries
     for (int i = 0; i < sig_cc->length(); i++) {
       if (!SigEntry::is_reserved_entry(sig_cc, i)) {
         if (SigEntry::skip_value_delimiters(sig_cc, i) && rec_len <= 0) {
           sig_bt[args_passed++] = sig_cc->at(i)._bt;
diff a/src/hotspot/share/ci/ciInstanceKlass.cpp b/src/hotspot/share/ci/ciInstanceKlass.cpp
--- a/src/hotspot/share/ci/ciInstanceKlass.cpp
+++ b/src/hotspot/share/ci/ciInstanceKlass.cpp
@@ -32,11 +32,11 @@
 #include "memory/allocation.hpp"
 #include "memory/allocation.inline.hpp"
 #include "memory/resourceArea.hpp"
 #include "oops/oop.inline.hpp"
 #include "oops/fieldStreams.inline.hpp"
-#include "oops/valueKlass.inline.hpp"
+#include "oops/inlineKlass.inline.hpp"
 #include "runtime/fieldDescriptor.inline.hpp"
 #include "runtime/handles.inline.hpp"
 #include "runtime/jniHandles.inline.hpp"
 
 // ciInstanceKlass
@@ -549,11 +549,11 @@
     if (fs.access_flags().is_static())  continue;
     fieldDescriptor& fd = fs.field_descriptor();
     if (fd.is_inlined() && flatten) {
       // Value type fields are embedded
       int field_offset = fd.offset();
-      // Get ValueKlass and adjust number of fields
+      // Get InlineKlass and adjust number of fields
       Klass* k = get_instanceKlass()->get_inline_type_field_klass(fd.index());
       ciValueKlass* vk = CURRENT_ENV->get_klass(k)->as_value_klass();
       flen += vk->nof_nonstatic_fields() - 1;
       // Iterate over fields of the flattened value type and copy them to 'this'
       for (int i = 0; i < vk->nof_nonstatic_fields(); ++i) {
@@ -819,11 +819,11 @@
       assert(!HAS_PENDING_EXCEPTION, "can resolve klass?");
       InstanceKlass* holder = fd->field_holder();
       Klass* k = SystemDictionary::find(name, Handle(THREAD, holder->class_loader()),
                                         Handle(THREAD, holder->protection_domain()), THREAD);
       assert(k != NULL && !HAS_PENDING_EXCEPTION, "can resolve klass?");
-      ValueKlass* vk = ValueKlass::cast(k);
+      InlineKlass* vk = InlineKlass::cast(k);
       oop obj;
       if (flattened) {
         int field_offset = fd->offset() - vk->first_field_offset();
         obj = (oop) (cast_from_oop<address>(mirror) + field_offset);
       } else {
diff a/src/hotspot/share/ci/ciReplay.cpp b/src/hotspot/share/ci/ciReplay.cpp
--- a/src/hotspot/share/ci/ciReplay.cpp
+++ b/src/hotspot/share/ci/ciReplay.cpp
@@ -35,11 +35,11 @@
 #include "memory/oopFactory.hpp"
 #include "memory/resourceArea.hpp"
 #include "oops/constantPool.hpp"
 #include "oops/method.inline.hpp"
 #include "oops/oop.inline.hpp"
-#include "oops/valueKlass.inline.hpp"
+#include "oops/inlineKlass.inline.hpp"
 #include "runtime/fieldDescriptor.inline.hpp"
 #include "runtime/handles.inline.hpp"
 #include "utilities/copy.hpp"
 #include "utilities/macros.hpp"
 #include "utilities/utf8.hpp"
@@ -850,11 +850,11 @@
         bool res = _replay->process_staticfield_reference(string_value, _vt, fd, THREAD);
         assert(res, "should succeed for arrays & objects");
         break;
       }
       case T_INLINE_TYPE: {
-        ValueKlass* vk = ValueKlass::cast(fd->field_holder()->get_inline_type_field_klass(fd->index()));
+        InlineKlass* vk = InlineKlass::cast(fd->field_holder()->get_inline_type_field_klass(fd->index()));
         if (fd->is_inlined()) {
           int field_offset = fd->offset() - vk->first_field_offset();
           oop obj = (oop)(cast_from_oop<address>(_vt) + field_offset);
           ValueTypeFieldInitializer init_fields(obj, _replay);
           vk->do_nonstatic_fields(&init_fields);
@@ -999,11 +999,11 @@
       const char* string_value = parse_escaped_string();
       double value = atof(string_value);
       java_mirror->double_field_put(fd.offset(), value);
     } else if (field_signature[0] == JVM_SIGNATURE_INLINE_TYPE) {
       Klass* kelem = resolve_klass(field_signature, CHECK);
-      ValueKlass* vk = ValueKlass::cast(kelem);
+      InlineKlass* vk = InlineKlass::cast(kelem);
       oop value = vk->allocate_instance(CHECK);
       ValueTypeFieldInitializer init_fields(value, this);
       vk->do_nonstatic_fields(&init_fields);
       java_mirror->obj_field_put(fd.offset(), value);
     } else {
diff a/src/hotspot/share/ci/ciValueArrayKlass.cpp b/src/hotspot/share/ci/ciValueArrayKlass.cpp
--- a/src/hotspot/share/ci/ciValueArrayKlass.cpp
+++ b/src/hotspot/share/ci/ciValueArrayKlass.cpp
@@ -40,11 +40,11 @@
 // ciValueArrayKlass::ciValueArrayKlass
 //
 // Constructor for loaded value array klasses.
 ciValueArrayKlass::ciValueArrayKlass(Klass* h_k) : ciArrayKlass(h_k) {
   assert(get_Klass()->is_valueArray_klass(), "wrong type");
-  ValueKlass* element_Klass = get_ValueArrayKlass()->element_klass();
+  InlineKlass* element_Klass = get_ValueArrayKlass()->element_klass();
   _base_element_klass = CURRENT_ENV->get_klass(element_Klass);
   assert(_base_element_klass->is_valuetype(), "bad base klass");
   if (dimension() == 1) {
     _element_klass = _base_element_klass;
   } else {
diff a/src/hotspot/share/ci/ciValueKlass.cpp b/src/hotspot/share/ci/ciValueKlass.cpp
--- a/src/hotspot/share/ci/ciValueKlass.cpp
+++ b/src/hotspot/share/ci/ciValueKlass.cpp
@@ -24,11 +24,11 @@
 
 #include "precompiled.hpp"
 #include "ci/ciField.hpp"
 #include "ci/ciUtilities.inline.hpp"
 #include "ci/ciValueKlass.hpp"
-#include "oops/valueKlass.inline.hpp"
+#include "oops/inlineKlass.inline.hpp"
 
 int ciValueKlass::compute_nonstatic_fields() {
   int result = ciInstanceKlass::compute_nonstatic_fields();
   assert(super() == NULL || !super()->has_nonstatic_fields(), "a value type must not inherit fields from its superclass");
 
@@ -132,8 +132,8 @@
 
 address ciValueKlass::unpack_handler() const {
   GUARDED_VM_ENTRY(return get_ValueKlass()->unpack_handler();)
 }
 
-ValueKlass* ciValueKlass::get_ValueKlass() const {
+InlineKlass* ciValueKlass::get_ValueKlass() const {
   GUARDED_VM_ENTRY(return to_ValueKlass();)
 }
diff a/src/hotspot/share/ci/ciValueKlass.hpp b/src/hotspot/share/ci/ciValueKlass.hpp
--- a/src/hotspot/share/ci/ciValueKlass.hpp
+++ b/src/hotspot/share/ci/ciValueKlass.hpp
@@ -28,11 +28,11 @@
 #include "ci/ciConstantPoolCache.hpp"
 #include "ci/ciEnv.hpp"
 #include "ci/ciFlags.hpp"
 #include "ci/ciInstanceKlass.hpp"
 #include "ci/ciSymbol.hpp"
-#include "oops/valueKlass.hpp"
+#include "oops/inlineKlass.hpp"
 
 // ciValueKlass
 //
 // Specialized ciInstanceKlass for value types.
 class ciValueKlass : public ciInstanceKlass {
@@ -40,17 +40,17 @@
 
 private:
   // Fields declared in the bytecode (without flattened value type fields)
   GrowableArray<ciField*>* _declared_nonstatic_fields;
 
-  ValueKlass* to_ValueKlass() const {
-    return ValueKlass::cast(get_Klass());
+  InlineKlass* to_ValueKlass() const {
+    return InlineKlass::cast(get_Klass());
   }
 
 protected:
   ciValueKlass(Klass* h_k) : ciInstanceKlass(h_k), _declared_nonstatic_fields(NULL) {
-    assert(is_final(), "ValueKlass must be final");
+    assert(is_final(), "InlineKlass must be final");
   };
 
   ciValueKlass(ciSymbol* name, jobject loader, jobject protection_domain) :
     ciInstanceKlass(name, loader, protection_domain, T_INLINE_TYPE) {}
 
@@ -87,9 +87,9 @@
   ciInstance* default_value_instance() const;
   bool contains_oops() const;
   Array<SigEntry>* extended_sig() const;
   address pack_handler() const;
   address unpack_handler() const;
-  ValueKlass* get_ValueKlass() const;
+  InlineKlass* get_ValueKlass() const;
 };
 
 #endif // SHARE_VM_CI_CIVALUEKLASS_HPP
diff a/src/hotspot/share/classfile/classFileParser.cpp b/src/hotspot/share/classfile/classFileParser.cpp
--- a/src/hotspot/share/classfile/classFileParser.cpp
+++ b/src/hotspot/share/classfile/classFileParser.cpp
@@ -57,11 +57,11 @@
 #include "oops/metadata.hpp"
 #include "oops/method.inline.hpp"
 #include "oops/oop.inline.hpp"
 #include "oops/recordComponent.hpp"
 #include "oops/symbol.hpp"
-#include "oops/valueKlass.inline.hpp"
+#include "oops/inlineKlass.inline.hpp"
 #include "prims/jvmtiExport.hpp"
 #include "prims/jvmtiThreadState.hpp"
 #include "runtime/arguments.hpp"
 #include "runtime/fieldDescriptor.inline.hpp"
 #include "runtime/handles.inline.hpp"
@@ -4483,11 +4483,11 @@
                                                             _protection_domain, true, CHECK);
       assert(klass != NULL, "Sanity check");
       if (!klass->access_flags().is_inline_type()) {
         THROW(vmSymbols::java_lang_IncompatibleClassChangeError());
       }
-      ValueKlass* vk = ValueKlass::cast(klass);
+      InlineKlass* vk = InlineKlass::cast(klass);
       // Conditions to apply flattening or not should be defined in a single place
       bool too_big_to_allocate_inline = (InlineFieldMaxFlatSize >= 0 &&
                                  (vk->size_helper() * HeapWordSize) > InlineFieldMaxFlatSize);
       bool too_atomic_to_allocate_inline = vk->is_declared_atomic();
       bool too_volatile_to_allocate_inline = fs.access_flags().is_volatile();
@@ -4499,11 +4499,11 @@
       if (!(too_big_to_allocate_inline | too_atomic_to_allocate_inline | too_volatile_to_allocate_inline)) {
         nonstatic_inline_type_indexes[nonstatic_inline_type_count] = fs.index();
         nonstatic_inline_type_klasses[nonstatic_inline_type_count] = klass;
         nonstatic_inline_type_count++;
 
-        ValueKlass* vklass = ValueKlass::cast(klass);
+        InlineKlass* vklass = InlineKlass::cast(klass);
         if (vklass->contains_oops()) {
           inline_type_oop_map_count += vklass->nonstatic_oop_map_count();
         }
         fs.set_inlined(true);
         if (!vk->is_atomic()) {  // flat and non-atomic: take note
@@ -4695,11 +4695,11 @@
       case NONSTATIC_INLINE:
         if (fs.is_inlined()) {
           Klass* klass = nonstatic_inline_type_klasses[next_inline_type_index];
           assert(klass != NULL, "Klass should have been loaded and resolved earlier");
           assert(klass->access_flags().is_inline_type(),"Must be an inline type");
-          ValueKlass* vklass = ValueKlass::cast(klass);
+          InlineKlass* vklass = InlineKlass::cast(klass);
           real_offset = next_nonstatic_inline_type_offset;
           next_nonstatic_inline_type_offset += (vklass->size_helper()) * wordSize - vklass->first_field_offset();
           // aligning next inline type on a 64 bits boundary
           next_nonstatic_inline_type_offset = align_up(next_nonstatic_inline_type_offset, BytesPerLong);
           next_inline_type_index += 1;
@@ -6176,11 +6176,11 @@
                                  ik->external_name(), aot_fp, _stream->compute_fingerprint());
     }
   }
 
   if (ik->is_inline_klass()) {
-    ValueKlass* vk = ValueKlass::cast(ik);
+    InlineKlass* vk = InlineKlass::cast(ik);
     oop val = ik->allocate_instance(CHECK_NULL);
     vk->set_default_value(val);
   }
 
   return ik;
@@ -6448,24 +6448,24 @@
       }
       klass_name->decrement_refcount();
     } else
       if (is_inline_type() && ((ik->field_access_flags(i) & JVM_ACC_FIELD_INTERNAL) != 0)
         && ((ik->field_access_flags(i) & JVM_ACC_STATIC) != 0)) {
-      ValueKlass::cast(ik)->set_default_value_offset(ik->field_offset(i));
+      InlineKlass::cast(ik)->set_default_value_offset(ik->field_offset(i));
     }
   }
 
   if (is_inline_type()) {
-    ValueKlass* vk = ValueKlass::cast(ik);
+    InlineKlass* vk = InlineKlass::cast(ik);
     if (UseNewFieldLayout) {
       vk->set_alignment(_alignment);
       vk->set_first_field_offset(_first_field_offset);
       vk->set_exact_size_in_bytes(_exact_size_in_bytes);
     } else {
       vk->set_first_field_offset(vk->first_field_offset_old());
     }
-    ValueKlass::cast(ik)->initialize_calling_convention(CHECK);
+    InlineKlass::cast(ik)->initialize_calling_convention(CHECK);
   }
 
   ClassLoadingService::notify_class_loaded(ik, false /* not shared class */);
 
   if (!is_internal()) {
diff a/src/hotspot/share/classfile/classLoaderData.cpp b/src/hotspot/share/classfile/classLoaderData.cpp
--- a/src/hotspot/share/classfile/classLoaderData.cpp
+++ b/src/hotspot/share/classfile/classLoaderData.cpp
@@ -61,11 +61,11 @@
 #include "memory/metadataFactory.hpp"
 #include "memory/resourceArea.hpp"
 #include "oops/access.inline.hpp"
 #include "oops/oop.inline.hpp"
 #include "oops/oopHandle.inline.hpp"
-#include "oops/valueKlass.inline.hpp"
+#include "oops/inlineKlass.inline.hpp"
 #include "oops/weakHandle.inline.hpp"
 #include "runtime/atomic.hpp"
 #include "runtime/handles.inline.hpp"
 #include "runtime/mutex.hpp"
 #include "runtime/safepoint.hpp"
@@ -372,15 +372,15 @@
     }
     assert(k != k->next_link(), "no loops!");
   }
 }
 
-void ClassLoaderData::value_classes_do(void f(ValueKlass*)) {
+void ClassLoaderData::inline_classes_do(void f(InlineKlass*)) {
   // Lock-free access requires load_acquire
   for (Klass* k = Atomic::load_acquire(&_klasses); k != NULL; k = k->next_link()) {
     if (k->is_inline_klass()) {
-      f(ValueKlass::cast(k));
+      f(InlineKlass::cast(k));
     }
     assert(k != k->next_link(), "no loops!");
   }
 }
 
@@ -548,11 +548,11 @@
 
   // Some items on the _deallocate_list need to free their C heap structures
   // if they are not already on the _klasses list.
   free_deallocate_list_C_heap_structures();
 
-  value_classes_do(ValueKlass::cleanup);
+  inline_classes_do(InlineKlass::cleanup);
 
   // Clean up class dependencies and tell serviceability tools
   // these classes are unloading.  Must be called
   // after erroneous classes are released.
   classes_do(InstanceKlass::unload_class);
@@ -847,11 +847,11 @@
         MetadataFactory::free_metadata(this, (ConstantPool*)m);
       } else if (m->is_klass()) {
         if (!((Klass*)m)->is_inline_klass()) {
           MetadataFactory::free_metadata(this, (InstanceKlass*)m);
         } else {
-          MetadataFactory::free_metadata(this, (ValueKlass*)m);
+          MetadataFactory::free_metadata(this, (InlineKlass*)m);
         }
       } else {
         ShouldNotReachHere();
       }
     } else {
diff a/src/hotspot/share/classfile/classLoaderData.hpp b/src/hotspot/share/classfile/classLoaderData.hpp
--- a/src/hotspot/share/classfile/classLoaderData.hpp
+++ b/src/hotspot/share/classfile/classLoaderData.hpp
@@ -187,11 +187,11 @@
   bool keep_alive() const       { return _keep_alive > 0; }
 
   void classes_do(void f(Klass* const));
   void loaded_classes_do(KlassClosure* klass_closure);
   void classes_do(void f(InstanceKlass*));
-  void value_classes_do(void f(ValueKlass*));
+  void inline_classes_do(void f(InlineKlass*));
   void methods_do(void f(Method*));
   void modules_do(void f(ModuleEntry*));
   void packages_do(void f(PackageEntry*));
 
   // Deallocate free list during class unloading.
diff a/src/hotspot/share/classfile/fieldLayoutBuilder.cpp b/src/hotspot/share/classfile/fieldLayoutBuilder.cpp
--- a/src/hotspot/share/classfile/fieldLayoutBuilder.cpp
+++ b/src/hotspot/share/classfile/fieldLayoutBuilder.cpp
@@ -29,11 +29,11 @@
 #include "memory/resourceArea.hpp"
 #include "oops/array.hpp"
 #include "oops/fieldStreams.inline.hpp"
 #include "oops/instanceMirrorKlass.hpp"
 #include "oops/klass.inline.hpp"
-#include "oops/valueKlass.inline.hpp"
+#include "oops/inlineKlass.inline.hpp"
 #include "runtime/fieldDescriptor.inline.hpp"
 
 LayoutRawBlock::LayoutRawBlock(Kind kind, int size) :
   _next_block(NULL),
   _prev_block(NULL),
@@ -99,11 +99,11 @@
   }
   _oop_fields->append(block);
   _oop_count++;
 }
 
-void FieldGroup::add_inlined_field(AllFieldStream fs, ValueKlass* vk) {
+void FieldGroup::add_inlined_field(AllFieldStream fs, InlineKlass* vk) {
   // _inlined_fields list might be merged with the _primitive_fields list in the future
   LayoutRawBlock* block = new LayoutRawBlock(fs.index(), LayoutRawBlock::INLINED, vk->get_exact_size_in_bytes(), vk->get_alignment(), false);
   block->set_value_klass(vk);
   if (_inlined_fields == NULL) {
     _inlined_fields = new(ResourceObj::RESOURCE_AREA, mtInternal) GrowableArray<LayoutRawBlock*>(INITIAL_LIST_SIZE);
@@ -319,11 +319,11 @@
       // distinction between static and non-static fields is missing
       if (fs.access_flags().is_static()) continue;
       has_instance_fields = true;
       LayoutRawBlock* block;
       if (type == T_INLINE_TYPE) {
-        ValueKlass* vk = ValueKlass::cast(ik->get_inline_type_field_klass(fs.index()));
+        InlineKlass* vk = InlineKlass::cast(ik->get_inline_type_field_klass(fs.index()));
         block = new LayoutRawBlock(fs.index(), LayoutRawBlock::INHERITED, vk->get_exact_size_in_bytes(),
                                    vk->get_alignment(), false);
 
       } else {
         int size = type2aelembytes(type);
@@ -629,11 +629,11 @@
         Klass* klass =
             SystemDictionary::resolve_inline_type_field_or_fail(&fs,
                                                                 Handle(THREAD, _class_loader_data->class_loader()),
                                                                 _protection_domain, true, THREAD);
         assert(klass != NULL, "Sanity check");
-        ValueKlass* vk = ValueKlass::cast(klass);
+        InlineKlass* vk = InlineKlass::cast(klass);
         bool too_big_to_flatten = (InlineFieldMaxFlatSize >= 0 &&
                                    (vk->size_helper() * HeapWordSize) > InlineFieldMaxFlatSize);
         bool too_atomic_to_flatten = vk->is_declared_atomic();
         bool too_volatile_to_flatten = fs.access_flags().is_volatile();
         if (vk->is_naturally_atomic()) {
@@ -730,11 +730,11 @@
         Klass* klass =
             SystemDictionary::resolve_inline_type_field_or_fail(&fs,
                 Handle(THREAD, _class_loader_data->class_loader()),
                 _protection_domain, true, CHECK);
         assert(klass != NULL, "Sanity check");
-        ValueKlass* vk = ValueKlass::cast(klass);
+        InlineKlass* vk = InlineKlass::cast(klass);
         bool too_big_to_flatten = (InlineFieldMaxFlatSize >= 0 &&
                                    (vk->size_helper() * HeapWordSize) > InlineFieldMaxFlatSize);
         bool too_atomic_to_flatten = vk->is_declared_atomic();
         bool too_volatile_to_flatten = fs.access_flags().is_volatile();
         if (vk->is_naturally_atomic()) {
@@ -880,11 +880,11 @@
 
   epilogue();
 }
 
 void FieldLayoutBuilder::add_inlined_field_oopmap(OopMapBlocksBuilder* nonstatic_oop_maps,
-                ValueKlass* vklass, int offset) {
+                InlineKlass* vklass, int offset) {
   int diff = offset - vklass->first_field_offset();
   const OopMapBlock* map = vklass->start_of_nonstatic_oop_maps();
   const OopMapBlock* last_map = map + vklass->nonstatic_oop_map_count();
   while (map < last_map) {
     nonstatic_oop_maps->add(map->offset() + diff, map->count());
@@ -913,11 +913,11 @@
 
   GrowableArray<LayoutRawBlock*>* ff = _root_group->inlined_fields();
   if (ff != NULL) {
     for (int i = 0; i < ff->length(); i++) {
       LayoutRawBlock* f = ff->at(i);
-      ValueKlass* vk = f->value_klass();
+      InlineKlass* vk = f->value_klass();
       assert(vk != NULL, "Should have been initialized");
       if (vk->contains_oops()) {
         add_inlined_field_oopmap(nonstatic_oop_maps, vk, f->offset());
       }
     }
diff a/src/hotspot/share/classfile/fieldLayoutBuilder.hpp b/src/hotspot/share/classfile/fieldLayoutBuilder.hpp
--- a/src/hotspot/share/classfile/fieldLayoutBuilder.hpp
+++ b/src/hotspot/share/classfile/fieldLayoutBuilder.hpp
@@ -62,11 +62,11 @@
   };
 
  private:
   LayoutRawBlock* _next_block;
   LayoutRawBlock* _prev_block;
-  ValueKlass* _value_klass;
+  InlineKlass* _value_klass;
   Kind _kind;
   int _offset;
   int _alignment;
   int _size;
   int _field_index;
@@ -91,15 +91,15 @@
   int field_index() const {
     assert(_field_index != -1, "Must be initialized");
     return _field_index;
   }
   bool is_reference() const { return _is_reference; }
-  ValueKlass* value_klass() const {
+  InlineKlass* value_klass() const {
     assert(_value_klass != NULL, "Must be initialized");
     return _value_klass;
   }
-  void set_value_klass(ValueKlass* value_klass) { _value_klass = value_klass; }
+  void set_value_klass(InlineKlass* value_klass) { _value_klass = value_klass; }
 
   bool fit(int size, int alignment);
 
   static int compare_offset(LayoutRawBlock** x, LayoutRawBlock** y)  { return (*x)->offset() - (*y)->offset(); }
   // compare_size_inverted() returns the opposite of a regular compare method in order to
@@ -148,11 +148,11 @@
   int contended_group() const { return _contended_group; }
   int oop_count() const { return _oop_count; }
 
   void add_primitive_field(AllFieldStream fs, BasicType type);
   void add_oop_field(AllFieldStream fs);
-  void add_inlined_field(AllFieldStream fs, ValueKlass* vk);
+  void add_inlined_field(AllFieldStream fs, InlineKlass* vk);
   void add_block(LayoutRawBlock** list, LayoutRawBlock* block);
   void sort_by_size();
 };
 
 // The FieldLayout class represents a set of fields organized
@@ -290,9 +290,9 @@
  protected:
   void prologue();
   void epilogue();
   void regular_field_sorting();
   void inline_class_field_sorting(TRAPS);
-  void add_inlined_field_oopmap(OopMapBlocksBuilder* nonstatic_oop_map, ValueKlass* vk, int offset);
+  void add_inlined_field_oopmap(OopMapBlocksBuilder* nonstatic_oop_map, InlineKlass* vk, int offset);
 };
 
 #endif // SHARE_CLASSFILE_FIELDLAYOUTBUILDER_HPP
diff a/src/hotspot/share/classfile/javaClasses.cpp b/src/hotspot/share/classfile/javaClasses.cpp
--- a/src/hotspot/share/classfile/javaClasses.cpp
+++ b/src/hotspot/share/classfile/javaClasses.cpp
@@ -51,11 +51,11 @@
 #include "oops/oop.inline.hpp"
 #include "oops/symbol.hpp"
 #include "oops/recordComponent.hpp"
 #include "oops/typeArrayOop.inline.hpp"
 #include "oops/valueArrayKlass.hpp"
-#include "oops/valueKlass.inline.hpp"
+#include "oops/inlineKlass.inline.hpp"
 #include "prims/jvmtiExport.hpp"
 #include "prims/resolvedMethodTable.hpp"
 #include "runtime/fieldDescriptor.inline.hpp"
 #include "runtime/frame.inline.hpp"
 #include "runtime/handles.inline.hpp"
@@ -1008,11 +1008,11 @@
     // It might also have a component mirror.  This mirror must already exist.
     if (k->is_array_klass()) {
       if (k->is_valueArray_klass()) {
         Klass* element_klass = (Klass*) ValueArrayKlass::cast(k)->element_klass();
         assert(element_klass->is_inline_klass(), "Must be inline type component");
-        ValueKlass* vk = ValueKlass::cast(InstanceKlass::cast(element_klass));
+        InlineKlass* vk = InlineKlass::cast(InstanceKlass::cast(element_klass));
         comp_mirror = Handle(THREAD, vk->java_mirror());
       } else if (k->is_typeArray_klass()) {
         BasicType type = TypeArrayKlass::cast(k)->element_type();
         comp_mirror = Handle(THREAD, Universe::java_mirror(type));
       } else {
diff a/src/hotspot/share/classfile/systemDictionary.cpp b/src/hotspot/share/classfile/systemDictionary.cpp
--- a/src/hotspot/share/classfile/systemDictionary.cpp
+++ b/src/hotspot/share/classfile/systemDictionary.cpp
@@ -68,11 +68,11 @@
 #include "oops/objArrayOop.inline.hpp"
 #include "oops/oop.inline.hpp"
 #include "oops/oopHandle.inline.hpp"
 #include "oops/symbol.hpp"
 #include "oops/typeArrayKlass.hpp"
-#include "oops/valueKlass.inline.hpp"
+#include "oops/inlineKlass.inline.hpp"
 #include "prims/jvmtiExport.hpp"
 #include "prims/methodHandles.hpp"
 #include "runtime/arguments.hpp"
 #include "runtime/biasedLocking.hpp"
 #include "runtime/handles.inline.hpp"
@@ -1538,11 +1538,11 @@
   }
 
   load_shared_class_misc(ik, loader_data, CHECK_NULL);
 
   if (ik->is_inline_klass()) {
-    ValueKlass* vk = ValueKlass::cast(ik);
+    InlineKlass* vk = InlineKlass::cast(ik);
     oop val = ik->allocate_instance(CHECK_NULL);
     vk->set_default_value(val);
   }
 
   return ik;
diff a/src/hotspot/share/compiler/oopMap.cpp b/src/hotspot/share/compiler/oopMap.cpp
--- a/src/hotspot/share/compiler/oopMap.cpp
+++ b/src/hotspot/share/compiler/oopMap.cpp
@@ -31,11 +31,11 @@
 #include "gc/shared/collectedHeap.hpp"
 #include "memory/allocation.inline.hpp"
 #include "memory/iterator.hpp"
 #include "memory/resourceArea.hpp"
 #include "memory/universe.hpp"
-#include "oops/valueKlass.hpp"
+#include "oops/inlineKlass.hpp"
 #include "oops/compressedOops.hpp"
 #include "runtime/frame.inline.hpp"
 #include "runtime/handles.inline.hpp"
 #include "runtime/signature.hpp"
 #include "utilities/align.hpp"
diff a/src/hotspot/share/gc/shared/barrierSet.hpp b/src/hotspot/share/gc/shared/barrierSet.hpp
--- a/src/hotspot/share/gc/shared/barrierSet.hpp
+++ b/src/hotspot/share/gc/shared/barrierSet.hpp
@@ -313,11 +313,11 @@
     // Clone barrier support
     static void clone_in_heap(oop src, oop dst, size_t size) {
       Raw::clone(src, dst, size);
     }
 
-    static void value_copy_in_heap(void* src, void* dst, ValueKlass* md) {
+    static void value_copy_in_heap(void* src, void* dst, InlineKlass* md) {
       Raw::value_copy(src, dst, md);
     }
 
     static oop resolve(oop obj) {
       return Raw::resolve(obj);
diff a/src/hotspot/share/gc/shared/barrierSetRuntime.cpp b/src/hotspot/share/gc/shared/barrierSetRuntime.cpp
--- a/src/hotspot/share/gc/shared/barrierSetRuntime.cpp
+++ b/src/hotspot/share/gc/shared/barrierSetRuntime.cpp
@@ -26,14 +26,14 @@
 #include "gc/shared/barrierSetRuntime.hpp"
 #include "oops/access.inline.hpp"
 #include "runtime/interfaceSupport.inline.hpp"
 #include "utilities/macros.hpp"
 
-JRT_LEAF(void, BarrierSetRuntime::value_copy(void* src, void* dst, ValueKlass* md))
+JRT_LEAF(void, BarrierSetRuntime::value_copy(void* src, void* dst, InlineKlass* md))
   assert(md->is_inline_type_klass(), "invariant");
   HeapAccess<>::value_copy(src, dst, md);
 JRT_END
 
-JRT_LEAF(void, BarrierSetRuntime::value_copy_is_dest_uninitialized(void* src, void* dst, ValueKlass* md))
+JRT_LEAF(void, BarrierSetRuntime::value_copy_is_dest_uninitialized(void* src, void* dst, InlineKlass* md))
   assert(md->is_inline_type_klass(), "invariant");
   HeapAccess<IS_DEST_UNINITIALIZED>::value_copy(src, dst, md);
 JRT_END
diff a/src/hotspot/share/gc/shared/barrierSetRuntime.hpp b/src/hotspot/share/gc/shared/barrierSetRuntime.hpp
--- a/src/hotspot/share/gc/shared/barrierSetRuntime.hpp
+++ b/src/hotspot/share/gc/shared/barrierSetRuntime.hpp
@@ -25,20 +25,20 @@
 #ifndef SHARE_GC_SHARED_BARRIERSETRUNTIME_HPP
 #define SHARE_GC_SHARED_BARRIERSETRUNTIME_HPP
 
 #include "memory/allocation.hpp"
 #include "oops/oopsHierarchy.hpp"
-#include "oops/valueKlass.hpp"
+#include "oops/inlineKlass.hpp"
 #include "utilities/globalDefinitions.hpp"
 #include "utilities/macros.hpp"
 
 class oopDesc;
 class JavaThread;
 
 class BarrierSetRuntime: public AllStatic {
 public:
   // Template interpreter...
-  static void value_copy(void* src, void* dst, ValueKlass* md);
-  static void value_copy_is_dest_uninitialized(void* src, void* dst, ValueKlass* md);
+  static void value_copy(void* src, void* dst, InlineKlass* md);
+  static void value_copy_is_dest_uninitialized(void* src, void* dst, InlineKlass* md);
 };
 
 #endif // SHARE_GC_SHARED_BARRIERSETRUNTIME_HPP
diff a/src/hotspot/share/gc/shared/modRefBarrierSet.hpp b/src/hotspot/share/gc/shared/modRefBarrierSet.hpp
--- a/src/hotspot/share/gc/shared/modRefBarrierSet.hpp
+++ b/src/hotspot/share/gc/shared/modRefBarrierSet.hpp
@@ -103,11 +103,11 @@
 
     static oop oop_atomic_cmpxchg_in_heap_at(oop base, ptrdiff_t offset, oop compare_value, oop new_value) {
       return oop_atomic_cmpxchg_in_heap(AccessInternal::oop_field_addr<decorators>(base, offset), compare_value, new_value);
     }
 
-    static void value_copy_in_heap(void* src, void* dst, ValueKlass* md);
+    static void value_copy_in_heap(void* src, void* dst, InlineKlass* md);
   };
 };
 
 template<>
 struct BarrierSet::GetName<ModRefBarrierSet> {
diff a/src/hotspot/share/gc/shared/modRefBarrierSet.inline.hpp b/src/hotspot/share/gc/shared/modRefBarrierSet.inline.hpp
--- a/src/hotspot/share/gc/shared/modRefBarrierSet.inline.hpp
+++ b/src/hotspot/share/gc/shared/modRefBarrierSet.inline.hpp
@@ -29,11 +29,11 @@
 #include "gc/shared/modRefBarrierSet.hpp"
 #include "oops/compressedOops.inline.hpp"
 #include "oops/klass.inline.hpp"
 #include "oops/objArrayOop.hpp"
 #include "oops/oop.hpp"
-#include "oops/valueKlass.hpp"
+#include "oops/inlineKlass.hpp"
 
 // count is number of array elements being written
 void ModRefBarrierSet::write_ref_array(HeapWord* start, size_t count) {
   HeapWord* end = (HeapWord*)((char*)start + (count*heapOopSize));
   // In the case of compressed oops, start and end may potentially be misaligned;
@@ -153,11 +153,11 @@
   bs->write_region(MemRegion((HeapWord*)(void*)dst, size));
 }
 
 template <DecoratorSet decorators, typename BarrierSetT>
 inline void ModRefBarrierSet::AccessBarrier<decorators, BarrierSetT>::
-value_copy_in_heap(void* src, void* dst, ValueKlass* md) {
+value_copy_in_heap(void* src, void* dst, InlineKlass* md) {
   if (HasDecorator<decorators, IS_DEST_UNINITIALIZED>::value || (!md->contains_oops())) {
     Raw::value_copy(src, dst, md);
   } else {
     BarrierSetT* bs = barrier_set_cast<BarrierSetT>(BarrierSet::barrier_set());
     // src/dst aren't oops, need offset to adjust oop map offset
diff a/src/hotspot/share/gc/z/zBarrier.hpp b/src/hotspot/share/gc/z/zBarrier.hpp
--- a/src/hotspot/share/gc/z/zBarrier.hpp
+++ b/src/hotspot/share/gc/z/zBarrier.hpp
@@ -24,11 +24,11 @@
 #ifndef SHARE_GC_Z_ZBARRIER_HPP
 #define SHARE_GC_Z_ZBARRIER_HPP
 
 #include "memory/allocation.hpp"
 #include "oops/oop.hpp"
-#include "oops/valueKlass.hpp"
+#include "oops/inlineKlass.hpp"
 
 typedef bool (*ZBarrierFastPath)(uintptr_t);
 typedef uintptr_t (*ZBarrierSlowPath)(uintptr_t);
 
 class ZBarrier : public AllStatic {
diff a/src/hotspot/share/gc/z/zBarrierSet.hpp b/src/hotspot/share/gc/z/zBarrierSet.hpp
--- a/src/hotspot/share/gc/z/zBarrierSet.hpp
+++ b/src/hotspot/share/gc/z/zBarrierSet.hpp
@@ -82,11 +82,11 @@
                                       arrayOop dst_obj, size_t dst_offset_in_bytes, T* dst_raw,
                                       size_t length);
 
     static void clone_in_heap(oop src, oop dst, size_t size);
 
-    static void value_copy_in_heap(void* src, void* dst, ValueKlass* md);
+    static void value_copy_in_heap(void* src, void* dst, InlineKlass* md);
 
     //
     // Not in heap
     //
     template <typename T>
diff a/src/hotspot/share/gc/z/zBarrierSet.inline.hpp b/src/hotspot/share/gc/z/zBarrierSet.inline.hpp
--- a/src/hotspot/share/gc/z/zBarrierSet.inline.hpp
+++ b/src/hotspot/share/gc/z/zBarrierSet.inline.hpp
@@ -213,11 +213,11 @@
   ZBarrier::load_barrier_on_oop_fields(src);
   Raw::clone_in_heap(src, dst, size);
 }
 
 template <DecoratorSet decorators, typename BarrierSetT>
-inline void ZBarrierSet::AccessBarrier<decorators, BarrierSetT>::value_copy_in_heap(void* src, void* dst, ValueKlass* md) {
+inline void ZBarrierSet::AccessBarrier<decorators, BarrierSetT>::value_copy_in_heap(void* src, void* dst, InlineKlass* md) {
   if (md->contains_oops()) {
     // src/dst aren't oops, need offset to adjust oop map offset
     const address src_oop_addr_offset = ((address) src) - md->first_field_offset();
 
     OopMapBlock* map = md->start_of_nonstatic_oop_maps();
diff a/src/hotspot/share/interpreter/interpreterRuntime.cpp b/src/hotspot/share/interpreter/interpreterRuntime.cpp
--- a/src/hotspot/share/interpreter/interpreterRuntime.cpp
+++ b/src/hotspot/share/interpreter/interpreterRuntime.cpp
@@ -49,11 +49,11 @@
 #include "oops/objArrayOop.inline.hpp"
 #include "oops/oop.inline.hpp"
 #include "oops/symbol.hpp"
 #include "oops/valueArrayKlass.hpp"
 #include "oops/valueArrayOop.inline.hpp"
-#include "oops/valueKlass.inline.hpp"
+#include "oops/inlineKlass.inline.hpp"
 #include "prims/jvmtiExport.hpp"
 #include "prims/nativeLookup.hpp"
 #include "runtime/atomic.hpp"
 #include "runtime/biasedLocking.hpp"
 #include "runtime/deoptimization.hpp"
@@ -297,34 +297,34 @@
     fatal("Unsupported BasicType");
   }
 }
 
 JRT_ENTRY(void, InterpreterRuntime::defaultvalue(JavaThread* thread, ConstantPool* pool, int index))
-  // Getting the ValueKlass
+  // Getting the InlineKlass
   Klass* k = pool->klass_at(index, CHECK);
   if (!k->is_inline_klass()) {
     // inconsistency with 'new' which throws an InstantiationError
     // in the future, defaultvalue will just return null instead of throwing an exception
     THROW(vmSymbols::java_lang_IncompatibleClassChangeError());
   }
   assert(k->is_inline_klass(), "defaultvalue argument must be the inline type class");
-  ValueKlass* vklass = ValueKlass::cast(k);
+  InlineKlass* vklass = InlineKlass::cast(k);
 
   vklass->initialize(THREAD);
   oop res = vklass->default_value();
   thread->set_vm_result(res);
 JRT_END
 
 JRT_ENTRY(int, InterpreterRuntime::withfield(JavaThread* thread, ConstantPoolCache* cp_cache))
   LastFrameAccessor last_frame(thread);
-  // Getting the ValueKlass
+  // Getting the InlineKlass
   int index = ConstantPool::decode_cpcache_index(last_frame.get_index_u2_cpcache(Bytecodes::_withfield));
   ConstantPoolCacheEntry* cp_entry = cp_cache->entry_at(index);
   assert(cp_entry->is_resolved(Bytecodes::_withfield), "Should have been resolved");
   Klass* klass = cp_entry->f1_as_klass();
   assert(klass->is_inline_klass(), "withfield only applies to inline types");
-  ValueKlass* vklass = ValueKlass::cast(klass);
+  InlineKlass* vklass = InlineKlass::cast(klass);
 
   // Getting Field information
   int offset = cp_entry->f2_as_index();
   int field_index = cp_entry->field_index();
   int field_offset = cp_entry->f2_as_offset();
@@ -342,22 +342,22 @@
 
   // Creating new value by copying the one passed in argument
   instanceOop new_value = vklass->allocate_instance(
       CHECK_((type2size[field_type]) * AbstractInterpreter::stackElementSize));
   Handle new_value_h = Handle(THREAD, new_value);
-  vklass->value_copy_oop_to_new_oop(old_value_h(), new_value_h());
+  vklass->inline_copy_oop_to_new_oop(old_value_h(), new_value_h());
 
   // Updating the field specified in arguments
   if (field_type == T_ARRAY || field_type == T_OBJECT) {
     oop aoop = *(oop*)f.interpreter_frame_expression_stack_at(tos_idx);
     assert(aoop == NULL || oopDesc::is_oop(aoop),"argument must be a reference type");
     new_value_h()->obj_field_put(field_offset, aoop);
   } else if (field_type == T_INLINE_TYPE) {
     if (cp_entry->is_inlined()) {
       oop vt_oop = *(oop*)f.interpreter_frame_expression_stack_at(tos_idx);
       assert(vt_oop != NULL && oopDesc::is_oop(vt_oop) && vt_oop->is_inline_type(),"argument must be an inline type");
-      ValueKlass* field_vk = ValueKlass::cast(vklass->get_inline_type_field_klass(field_index));
+      InlineKlass* field_vk = InlineKlass::cast(vklass->get_inline_type_field_klass(field_index));
       assert(vt_oop != NULL && field_vk == vt_oop->klass(), "Must match");
       field_vk->write_inlined_field(new_value_h(), offset, vt_oop, CHECK_(return_offset));
     } else { // not inlined
       oop voop = *(oop*)f.interpreter_frame_expression_stack_at(tos_idx);
       if (voop == NULL && cp_entry->is_inline_type()) {
@@ -400,11 +400,11 @@
           true, CHECK);
       assert(field_k != NULL, "Should have been loaded or an exception thrown above");
       klass->set_inline_type_field_klass(index, field_k);
     }
     field_k->initialize(CHECK);
-    oop defaultvalue = ValueKlass::cast(field_k)->default_value();
+    oop defaultvalue = InlineKlass::cast(field_k)->default_value();
     // It is safe to initialized the static field because 1) the current thread is the initializing thread
     // and is the only one that can access it, and 2) the field is actually not initialized (i.e. null)
     // otherwise the JVM should not be executing this code.
     mirror->obj_field_put(offset, defaultvalue);
     thread->set_vm_result(defaultvalue);
@@ -433,11 +433,11 @@
   assert(field_holder->is_instance_klass(), "Sanity check");
   InstanceKlass* klass = InstanceKlass::cast(field_holder);
 
   assert(klass->field_is_inlined(index), "Sanity check");
 
-  ValueKlass* field_vklass = ValueKlass::cast(klass->get_inline_type_field_klass(index));
+  InlineKlass* field_vklass = InlineKlass::cast(klass->get_inline_type_field_klass(index));
   assert(field_vklass->is_initialized(), "Must be initialized at this point");
 
   oop res = field_vklass->read_inlined_field(obj_h(), klass->field_offset(index), CHECK);
   thread->set_vm_result(res);
 JRT_END
diff a/src/hotspot/share/memory/heapInspection.cpp b/src/hotspot/share/memory/heapInspection.cpp
--- a/src/hotspot/share/memory/heapInspection.cpp
+++ b/src/hotspot/share/memory/heapInspection.cpp
@@ -33,11 +33,11 @@
 #include "memory/heapInspection.hpp"
 #include "memory/resourceArea.hpp"
 #include "memory/universe.hpp"
 #include "oops/oop.inline.hpp"
 #include "oops/reflectionAccessorImplKlassHelper.hpp"
-#include "oops/valueKlass.inline.hpp"
+#include "oops/inlineKlass.inline.hpp"
 #include "runtime/os.hpp"
 #include "runtime/fieldDescriptor.inline.hpp"
 #include "utilities/globalDefinitions.hpp"
 #include "utilities/macros.hpp"
 #include "utilities/stack.inline.hpp"
@@ -548,11 +548,11 @@
       inlined_msg);
 }
 
 static void print_inlined_field(outputStream* st, int level, int offset, InstanceKlass* klass) {
   assert(klass->is_inline_klass(), "Only inline types can be inlined");
-  ValueKlass* vklass = ValueKlass::cast(klass);
+  InlineKlass* vklass = InlineKlass::cast(klass);
   GrowableArray<FieldDesc>* fields = new (ResourceObj::C_HEAP, mtServiceability) GrowableArray<FieldDesc>(100, mtServiceability);
   for (FieldStream fd(klass, false, false); !fd.eos(); fd.next()) {
     if (!fd.access_flags().is_static()) {
       fields->append(FieldDesc(fd.field_descriptor()));
     }
diff a/src/hotspot/share/memory/metaspaceShared.cpp b/src/hotspot/share/memory/metaspaceShared.cpp
--- a/src/hotspot/share/memory/metaspaceShared.cpp
+++ b/src/hotspot/share/memory/metaspaceShared.cpp
@@ -58,11 +58,11 @@
 #include "oops/objArrayKlass.hpp"
 #include "oops/objArrayOop.hpp"
 #include "oops/oop.inline.hpp"
 #include "oops/typeArrayKlass.hpp"
 #include "oops/valueArrayKlass.hpp"
-#include "oops/valueKlass.hpp"
+#include "oops/inlineKlass.hpp"
 #include "prims/jvmtiRedefineClasses.hpp"
 #include "runtime/handles.inline.hpp"
 #include "runtime/os.hpp"
 #include "runtime/safepointVerifiers.hpp"
 #include "runtime/signature.hpp"
@@ -768,11 +768,11 @@
   f(InstanceRefKlass) \
   f(Method) \
   f(ObjArrayKlass) \
   f(TypeArrayKlass) \
   f(ValueArrayKlass) \
-  f(ValueKlass)
+  f(InlineKlass)
 
 class CppVtableInfo {
   intptr_t _vtable_size;
   intptr_t _cloned_vtable[1];
 public:
@@ -957,11 +957,11 @@
   case MetaspaceObj::ClassType:
     {
       Klass* k = (Klass*)obj;
       assert(k->is_klass(), "must be");
       if (k->is_inline_klass()) {
-        kind = ValueKlass_Kind;
+        kind = InlineKlass_Kind;
       } else if (k->is_instance_klass()) {
         InstanceKlass* ik = InstanceKlass::cast(k);
         if (ik->is_class_loader_instance_klass()) {
           kind = InstanceClassLoaderKlass_Kind;
         } else if (ik->is_reference_instance_klass()) {
diff a/src/hotspot/share/memory/oopFactory.cpp b/src/hotspot/share/memory/oopFactory.cpp
--- a/src/hotspot/share/memory/oopFactory.cpp
+++ b/src/hotspot/share/memory/oopFactory.cpp
@@ -37,11 +37,10 @@
 #include "oops/objArrayOop.inline.hpp"
 #include "oops/objArrayOop.hpp"
 #include "oops/oop.inline.hpp"
 #include "oops/typeArrayKlass.hpp"
 #include "oops/typeArrayOop.inline.hpp"
-#include "oops/valueKlass.hpp"
 #include "oops/valueArrayKlass.hpp"
 #include "oops/valueArrayOop.inline.hpp"
 #include "oops/valueArrayOop.hpp"
 #include "runtime/handles.inline.hpp"
 #include "utilities/utf8.hpp"
diff a/src/hotspot/share/oops/access.hpp b/src/hotspot/share/oops/access.hpp
--- a/src/hotspot/share/oops/access.hpp
+++ b/src/hotspot/share/oops/access.hpp
@@ -221,11 +221,11 @@
   }
 
   // inline type heap access (when inlined)...
 
   // Copy value type data from src to dst
-  static inline void value_copy(void* src, void* dst, ValueKlass* md) {
+  static inline void value_copy(void* src, void* dst, InlineKlass* md) {
     verify_heap_value_decorators<IN_HEAP>();
     AccessInternal::value_copy<decorators>(src, dst, md);
   }
 
   // Primitive accesses
diff a/src/hotspot/share/oops/access.inline.hpp b/src/hotspot/share/oops/access.inline.hpp
--- a/src/hotspot/share/oops/access.inline.hpp
+++ b/src/hotspot/share/oops/access.inline.hpp
@@ -198,11 +198,11 @@
     }
   };
 
   template <class GCBarrierType, DecoratorSet decorators>
   struct PostRuntimeDispatch<GCBarrierType, BARRIER_VALUE_COPY, decorators>: public AllStatic {
-    static void access_barrier(void* src, void* dst, ValueKlass* md) {
+    static void access_barrier(void* src, void* dst, InlineKlass* md) {
       GCBarrierType::value_copy_in_heap(src, dst, md);
     }
   };
 
   template <class GCBarrierType, DecoratorSet decorators>
@@ -359,11 +359,11 @@
     _clone_func = function;
     function(src, dst, size);
   }
 
   template <DecoratorSet decorators, typename T>
-  void RuntimeDispatch<decorators, T, BARRIER_VALUE_COPY>::value_copy_init(void* src, void* dst, ValueKlass* md) {
+  void RuntimeDispatch<decorators, T, BARRIER_VALUE_COPY>::value_copy_init(void* src, void* dst, InlineKlass* md) {
     func_t function = BarrierResolver<decorators, func_t, BARRIER_VALUE_COPY>::resolve_barrier();
     _value_copy_func = function;
     function(src, dst, md);
   }
 
diff a/src/hotspot/share/oops/accessBackend.cpp b/src/hotspot/share/oops/accessBackend.cpp
--- a/src/hotspot/share/oops/accessBackend.cpp
+++ b/src/hotspot/share/oops/accessBackend.cpp
@@ -24,11 +24,11 @@
 
 #include "precompiled.hpp"
 #include "accessBackend.inline.hpp"
 #include "gc/shared/collectedHeap.hpp"
 #include "oops/oop.inline.hpp"
-#include "oops/valueKlass.hpp"
+#include "oops/inlineKlass.hpp"
 #include "runtime/mutexLocker.hpp"
 #include "runtime/vm_version.hpp"
 #include "utilities/copy.hpp"
 
 namespace AccessInternal {
diff a/src/hotspot/share/oops/accessBackend.hpp b/src/hotspot/share/oops/accessBackend.hpp
--- a/src/hotspot/share/oops/accessBackend.hpp
+++ b/src/hotspot/share/oops/accessBackend.hpp
@@ -121,11 +121,11 @@
 
     typedef void (*arraycopy_func_t)(arrayOop src_obj, size_t src_offset_in_bytes, T* src_raw,
                                      arrayOop dst_obj, size_t dst_offset_in_bytes, T* dst_raw,
                                      size_t length);
     typedef void (*clone_func_t)(oop src, oop dst, size_t size);
-    typedef void (*value_copy_func_t)(void* src, void* dst, ValueKlass* md);
+    typedef void (*value_copy_func_t)(void* src, void* dst, InlineKlass* md);
     typedef oop (*resolve_func_t)(oop obj);
   };
 
   template <DecoratorSet decorators>
   struct AccessFunctionTypes<decorators, void> {
@@ -401,11 +401,11 @@
                             arrayOop dst_obj, size_t dst_offset_in_bytes, T* dst_raw,
                             size_t length);
 
   static void clone(oop src, oop dst, size_t size);
 
-  static void value_copy(void* src, void* dst, ValueKlass* md);
+  static void value_copy(void* src, void* dst, InlineKlass* md);
 
   static oop resolve(oop obj) { return obj; }
 };
 
 // Below is the implementation of the first 4 steps of the template pipeline:
@@ -590,13 +590,13 @@
   template <DecoratorSet decorators, typename T>
   struct RuntimeDispatch<decorators, T, BARRIER_VALUE_COPY>: AllStatic {
     typedef typename AccessFunction<decorators, T, BARRIER_VALUE_COPY>::type func_t;
     static func_t _value_copy_func;
 
-    static void value_copy_init(void* src, void* dst, ValueKlass* md);
+    static void value_copy_init(void* src, void* dst, InlineKlass* md);
 
-    static inline void value_copy(void* src, void* dst, ValueKlass* md) {
+    static inline void value_copy(void* src, void* dst, InlineKlass* md) {
       _value_copy_func(src, dst, md);
     }
   };
 
   template <DecoratorSet decorators, typename T>
@@ -977,19 +977,19 @@
     }
 
     template <DecoratorSet decorators>
     inline static typename EnableIf<
       HasDecorator<decorators, AS_RAW>::value>::type
-    value_copy(void* src, void* dst, ValueKlass* md) {
+    value_copy(void* src, void* dst, InlineKlass* md) {
       typedef RawAccessBarrier<decorators & RAW_DECORATOR_MASK> Raw;
       Raw::value_copy(src, dst, md);
     }
 
     template <DecoratorSet decorators>
     inline static typename EnableIf<
       !HasDecorator<decorators, AS_RAW>::value>::type
-      value_copy(void* src, void* dst, ValueKlass* md) {
+      value_copy(void* src, void* dst, InlineKlass* md) {
       const DecoratorSet expanded_decorators = decorators;
       RuntimeDispatch<expanded_decorators, void*, BARRIER_VALUE_COPY>::value_copy(src, dst, md);
     }
 
 
@@ -1298,11 +1298,11 @@
     const DecoratorSet expanded_decorators = DecoratorFixup<decorators>::value;
     PreRuntimeDispatch::clone<expanded_decorators>(src, dst, size);
   }
 
   template <DecoratorSet decorators>
-  inline void value_copy(void* src, void* dst, ValueKlass* md) {
+  inline void value_copy(void* src, void* dst, InlineKlass* md) {
     const DecoratorSet expanded_decorators = DecoratorFixup<decorators>::value;
     PreRuntimeDispatch::value_copy<expanded_decorators>(src, dst, md);
   }
 
   template <DecoratorSet decorators>
diff a/src/hotspot/share/oops/accessBackend.inline.hpp b/src/hotspot/share/oops/accessBackend.inline.hpp
--- a/src/hotspot/share/oops/accessBackend.inline.hpp
+++ b/src/hotspot/share/oops/accessBackend.inline.hpp
@@ -29,11 +29,11 @@
 #include "oops/accessBackend.hpp"
 #include "oops/compressedOops.inline.hpp"
 #include "oops/oopsHierarchy.hpp"
 #include "runtime/atomic.hpp"
 #include "runtime/orderAccess.hpp"
-#include "oops/valueKlass.hpp"
+#include "oops/inlineKlass.hpp"
 
 template <DecoratorSet decorators>
 template <DecoratorSet idecorators, typename T>
 inline typename EnableIf<
   AccessInternal::MustConvertCompressedOop<idecorators, T>::value, T>::type
@@ -364,10 +364,10 @@
   // Clear the header
   dst->init_mark_raw();
 }
 
 template <DecoratorSet decorators>
-inline void RawAccessBarrier<decorators>::value_copy(void* src, void* dst, ValueKlass* md) {
+inline void RawAccessBarrier<decorators>::value_copy(void* src, void* dst, InlineKlass* md) {
   assert(is_aligned(src, md->get_alignment()) && is_aligned(dst, md->get_alignment()), "Unalign value_copy");
   AccessInternal::arraycopy_conjoint_atomic(src, dst, static_cast<size_t>(md->get_exact_size_in_bytes()));
 }
 #endif // SHARE_OOPS_ACCESSBACKEND_INLINE_HPP
diff a/src/hotspot/share/oops/inlineKlass.cpp b/src/hotspot/share/oops/inlineKlass.cpp
--- /dev/null
+++ b/src/hotspot/share/oops/inlineKlass.cpp
@@ -0,0 +1,582 @@
+/*
+ * Copyright (c) 2017, 2020, Oracle and/or its affiliates. All rights reserved.
+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
+ *
+ * This code is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 only, as
+ * published by the Free Software Foundation.
+ *
+ * This code is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
+ * version 2 for more details (a copy is included in the LICENSE file that
+ * accompanied this code).
+ *
+ * You should have received a copy of the GNU General Public License version
+ * 2 along with this work; if not, write to the Free Software Foundation,
+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
+ *
+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
+ * or visit www.oracle.com if you need additional information or have any
+ * questions.
+ *
+ */
+
+#include "precompiled.hpp"
+#include "gc/shared/barrierSet.hpp"
+#include "gc/shared/collectedHeap.inline.hpp"
+#include "gc/shared/gcLocker.inline.hpp"
+#include "interpreter/interpreter.hpp"
+#include "logging/log.hpp"
+#include "memory/metaspaceClosure.hpp"
+#include "memory/metadataFactory.hpp"
+#include "oops/access.hpp"
+#include "oops/compressedOops.inline.hpp"
+#include "oops/fieldStreams.inline.hpp"
+#include "oops/instanceKlass.inline.hpp"
+#include "oops/method.hpp"
+#include "oops/oop.inline.hpp"
+#include "oops/objArrayKlass.hpp"
+#include "oops/inlineKlass.inline.hpp"
+#include "oops/valueArrayKlass.hpp"
+#include "runtime/fieldDescriptor.inline.hpp"
+#include "runtime/handles.inline.hpp"
+#include "runtime/safepointVerifiers.hpp"
+#include "runtime/sharedRuntime.hpp"
+#include "runtime/signature.hpp"
+#include "runtime/thread.inline.hpp"
+#include "utilities/copy.hpp"
+
+  // Constructor
+InlineKlass::InlineKlass(const ClassFileParser& parser)
+    : InstanceKlass(parser, InstanceKlass::_kind_inline_type, InstanceKlass::ID) {
+  _adr_inlineklass_fixed_block = inlineklass_static_block();
+  // Addresses used for inline type calling convention
+  *((Array<SigEntry>**)adr_extended_sig()) = NULL;
+  *((Array<VMRegPair>**)adr_return_regs()) = NULL;
+  *((address*)adr_pack_handler()) = NULL;
+  *((address*)adr_pack_handler_jobject()) = NULL;
+  *((address*)adr_unpack_handler()) = NULL;
+  assert(pack_handler() == NULL, "pack handler not null");
+  *((int*)adr_default_value_offset()) = 0;
+  *((Klass**)adr_value_array_klass()) = NULL;
+  set_prototype_header(markWord::always_locked_prototype());
+  assert(is_inline_type_klass(), "invariant");
+}
+
+oop InlineKlass::default_value() {
+  oop val = java_mirror()->obj_field_acquire(default_value_offset());
+  assert(oopDesc::is_oop(val), "Sanity check");
+  assert(val->is_inline_type(), "Sanity check");
+  assert(val->klass() == this, "sanity check");
+  return val;
+}
+
+int InlineKlass::first_field_offset_old() {
+#ifdef ASSERT
+  int first_offset = INT_MAX;
+  for (AllFieldStream fs(this); !fs.done(); fs.next()) {
+    if (fs.offset() < first_offset) first_offset= fs.offset();
+  }
+#endif
+  int base_offset = instanceOopDesc::base_offset_in_bytes();
+  // The first field of line types is aligned on a long boundary
+  base_offset = align_up(base_offset, BytesPerLong);
+  assert(base_offset == first_offset, "inconsistent offsets");
+  return base_offset;
+}
+
+int InlineKlass::raw_value_byte_size() {
+  int heapOopAlignedSize = nonstatic_field_size() << LogBytesPerHeapOop;
+  // If bigger than 64 bits or needs oop alignment, then use jlong aligned
+  // which for values should be jlong aligned, asserts in raw_field_copy otherwise
+  if (heapOopAlignedSize >= longSize || contains_oops()) {
+    return heapOopAlignedSize;
+  }
+  // Small primitives...
+  // If a few small basic type fields, return the actual size, i.e.
+  // 1 byte = 1
+  // 2 byte = 2
+  // 3 byte = 4, because pow2 needed for element stores
+  int first_offset = first_field_offset();
+  int last_offset  = 0; // find the last offset, add basic type size
+  int last_tsz     = 0;
+  for (AllFieldStream fs(this); !fs.done(); fs.next()) {
+    if (fs.access_flags().is_static()) {
+      continue;
+    } else if (fs.offset() > last_offset) {
+      BasicType type = Signature::basic_type(fs.signature());
+      if (is_java_primitive(type)) {
+        last_tsz = type2aelembytes(type);
+      } else if (type == T_INLINE_TYPE) {
+        // Not just primitives. Layout aligns embedded value, so use jlong aligned it is
+        return heapOopAlignedSize;
+      } else {
+        guarantee(0, "Unknown type %d", type);
+      }
+      assert(last_tsz != 0, "Invariant");
+      last_offset = fs.offset();
+    }
+  }
+  // Assumes VT with no fields are meaningless and illegal
+  last_offset += last_tsz;
+  assert(last_offset > first_offset && last_tsz, "Invariant");
+  return 1 << upper_log2(last_offset - first_offset);
+}
+
+instanceOop InlineKlass::allocate_instance(TRAPS) {
+  int size = size_helper();  // Query before forming handle.
+
+  instanceOop oop = (instanceOop)Universe::heap()->obj_allocate(this, size, CHECK_NULL);
+  assert(oop->mark().is_always_locked(), "Unlocked inline type");
+  return oop;
+}
+
+instanceOop InlineKlass::allocate_instance_buffer(TRAPS) {
+  int size = size_helper();  // Query before forming handle.
+
+  instanceOop oop = (instanceOop)Universe::heap()->obj_buffer_allocate(this, size, CHECK_NULL);
+  assert(oop->mark().is_always_locked(), "Unlocked inline type");
+  return oop;
+}
+
+int InlineKlass::nonstatic_oop_count() {
+  int oops = 0;
+  int map_count = nonstatic_oop_map_count();
+  OopMapBlock* block = start_of_nonstatic_oop_maps();
+  OopMapBlock* end = block + map_count;
+  while (block != end) {
+    oops += block->count();
+    block++;
+  }
+  return oops;
+}
+
+oop InlineKlass::read_inlined_field(oop obj, int offset, TRAPS) {
+  oop res = NULL;
+  this->initialize(CHECK_NULL); // will throw an exception if in error state
+  if (is_empty_inline_type()) {
+    res = (instanceOop)default_value();
+  } else {
+    Handle obj_h(THREAD, obj);
+    res = allocate_instance_buffer(CHECK_NULL);
+    inline_copy_payload_to_new_oop(((char*)(oopDesc*)obj_h()) + offset, res);
+  }
+  assert(res != NULL, "Must be set in one of two paths above");
+  return res;
+}
+
+void InlineKlass::write_inlined_field(oop obj, int offset, oop value, TRAPS) {
+  if (value == NULL) {
+    THROW(vmSymbols::java_lang_NullPointerException());
+  }
+  if (!is_empty_inline_type()) {
+    inline_copy_oop_to_payload(value, ((char*)(oopDesc*)obj) + offset);
+  }
+}
+
+// Arrays of...
+
+bool InlineKlass::flatten_array() {
+  if (!ValueArrayFlatten) {
+    return false;
+  }
+  // Too big
+  int elem_bytes = raw_value_byte_size();
+  if ((InlineArrayElemMaxFlatSize >= 0) && (elem_bytes > InlineArrayElemMaxFlatSize)) {
+    return false;
+  }
+  // Too many embedded oops
+  if ((InlineArrayElemMaxFlatOops >= 0) && (nonstatic_oop_count() > InlineArrayElemMaxFlatOops)) {
+    return false;
+  }
+  // Declared atomic but not naturally atomic.
+  if (is_declared_atomic() && !is_naturally_atomic()) {
+    return false;
+  }
+  // VM enforcing InlineArrayAtomicAccess only...
+  if (InlineArrayAtomicAccess && (!is_naturally_atomic())) {
+    return false;
+  }
+  return true;
+}
+
+void InlineKlass::remove_unshareable_info() {
+  InstanceKlass::remove_unshareable_info();
+
+  *((Array<SigEntry>**)adr_extended_sig()) = NULL;
+  *((Array<VMRegPair>**)adr_return_regs()) = NULL;
+  *((address*)adr_pack_handler()) = NULL;
+  *((address*)adr_pack_handler_jobject()) = NULL;
+  *((address*)adr_unpack_handler()) = NULL;
+  assert(pack_handler() == NULL, "pack handler not null");
+  *((Klass**)adr_value_array_klass()) = NULL;
+}
+
+void InlineKlass::restore_unshareable_info(ClassLoaderData* loader_data, Handle protection_domain, PackageEntry* pkg_entry, TRAPS) {
+  InstanceKlass::restore_unshareable_info(loader_data, protection_domain, pkg_entry, CHECK);
+}
+
+
+Klass* InlineKlass::array_klass_impl(bool or_null, int n, TRAPS) {
+  if (flatten_array()) {
+    return value_array_klass(or_null, n, THREAD);
+  } else {
+    return InstanceKlass::array_klass_impl(or_null, n, THREAD);
+  }
+}
+
+Klass* InlineKlass::array_klass_impl(bool or_null, TRAPS) {
+  return array_klass_impl(or_null, 1, THREAD);
+}
+
+Klass* InlineKlass::value_array_klass(bool or_null, int rank, TRAPS) {
+  Klass* vak = acquire_value_array_klass();
+  if (vak == NULL) {
+    if (or_null) return NULL;
+    ResourceMark rm;
+    {
+      // Atomic creation of array_klasses
+      MutexLocker ma(THREAD, MultiArray_lock);
+      if (get_value_array_klass() == NULL) {
+        vak = allocate_value_array_klass(CHECK_NULL);
+        Atomic::release_store((Klass**)adr_value_array_klass(), vak);
+      }
+    }
+  }
+  if (or_null) {
+    return vak->array_klass_or_null(rank);
+  }
+  return vak->array_klass(rank, THREAD);
+}
+
+Klass* InlineKlass::allocate_value_array_klass(TRAPS) {
+  if (flatten_array()) {
+    return ValueArrayKlass::allocate_klass(this, THREAD);
+  }
+  return ObjArrayKlass::allocate_objArray_klass(class_loader_data(), 1, this, THREAD);
+}
+
+void InlineKlass::array_klasses_do(void f(Klass* k, TRAPS), TRAPS) {
+  InstanceKlass::array_klasses_do(f, THREAD);
+  if (get_value_array_klass() != NULL)
+    ArrayKlass::cast(get_value_array_klass())->array_klasses_do(f, THREAD);
+}
+
+void InlineKlass::array_klasses_do(void f(Klass* k)) {
+  InstanceKlass::array_klasses_do(f);
+  if (get_value_array_klass() != NULL)
+    ArrayKlass::cast(get_value_array_klass())->array_klasses_do(f);
+}
+
+// Inline type arguments are not passed by reference, instead each
+// field of the inline type is passed as an argument. This helper
+// function collects the inlined field (recursively)
+// in a list. Included with the field's type is
+// the offset of each field in the inline type: i2c and c2i adapters
+// need that to load or store fields. Finally, the list of fields is
+// sorted in order of increasing offsets: the adapters and the
+// compiled code need to agree upon the order of fields.
+//
+// The list of basic types that is returned starts with a T_INLINE_TYPE
+// and ends with an extra T_VOID. T_INLINE_TYPE/T_VOID pairs are used as
+// delimiters. Every entry between the two is a field of the inline
+// type. If there's an embedded inline type in the list, it also starts
+// with a T_INLINE_TYPE and ends with a T_VOID. This is so we can
+// generate a unique fingerprint for the method's adapters and we can
+// generate the list of basic types from the interpreter point of view
+// (inline types passed as reference: iterate on the list until a
+// T_INLINE_TYPE, drop everything until and including the closing
+// T_VOID) or the compiler point of view (each field of the inline
+// types is an argument: drop all T_INLINE_TYPE/T_VOID from the list).
+int InlineKlass::collect_fields(GrowableArray<SigEntry>* sig, int base_off) {
+  int count = 0;
+  SigEntry::add_entry(sig, T_INLINE_TYPE, base_off);
+  for (AllFieldStream fs(this); !fs.done(); fs.next()) {
+    if (fs.access_flags().is_static()) continue;
+    int offset = base_off + fs.offset() - (base_off > 0 ? first_field_offset() : 0);
+    if (fs.is_inlined()) {
+      // Resolve klass of inlined field and recursively collect fields
+      Klass* vk = get_inline_type_field_klass(fs.index());
+      count += InlineKlass::cast(vk)->collect_fields(sig, offset);
+    } else {
+      BasicType bt = Signature::basic_type(fs.signature());
+      if (bt == T_INLINE_TYPE) {
+        bt = T_OBJECT;
+      }
+      SigEntry::add_entry(sig, bt, offset);
+      count += type2size[bt];
+    }
+  }
+  int offset = base_off + size_helper()*HeapWordSize - (base_off > 0 ? first_field_offset() : 0);
+  SigEntry::add_entry(sig, T_VOID, offset);
+  if (base_off == 0) {
+    sig->sort(SigEntry::compare);
+  }
+  assert(sig->at(0)._bt == T_INLINE_TYPE && sig->at(sig->length()-1)._bt == T_VOID, "broken structure");
+  return count;
+}
+
+void InlineKlass::initialize_calling_convention(TRAPS) {
+  // Because the pack and unpack handler addresses need to be loadable from generated code,
+  // they are stored at a fixed offset in the klass metadata. Since inline type klasses do
+  // not have a vtable, the vtable offset is used to store these addresses.
+  if (InlineTypeReturnedAsFields || InlineTypePassFieldsAsArgs) {
+    ResourceMark rm;
+    GrowableArray<SigEntry> sig_vk;
+    int nb_fields = collect_fields(&sig_vk);
+    Array<SigEntry>* extended_sig = MetadataFactory::new_array<SigEntry>(class_loader_data(), sig_vk.length(), CHECK);
+    *((Array<SigEntry>**)adr_extended_sig()) = extended_sig;
+    for (int i = 0; i < sig_vk.length(); i++) {
+      extended_sig->at_put(i, sig_vk.at(i));
+    }
+    if (can_be_returned_as_fields(/* init= */ true)) {
+      nb_fields++;
+      BasicType* sig_bt = NEW_RESOURCE_ARRAY(BasicType, nb_fields);
+      sig_bt[0] = T_METADATA;
+      SigEntry::fill_sig_bt(&sig_vk, sig_bt+1);
+      VMRegPair* regs = NEW_RESOURCE_ARRAY(VMRegPair, nb_fields);
+      int total = SharedRuntime::java_return_convention(sig_bt, regs, nb_fields);
+
+      if (total > 0) {
+        Array<VMRegPair>* return_regs = MetadataFactory::new_array<VMRegPair>(class_loader_data(), nb_fields, CHECK);
+        *((Array<VMRegPair>**)adr_return_regs()) = return_regs;
+        for (int i = 0; i < nb_fields; i++) {
+          return_regs->at_put(i, regs[i]);
+        }
+
+        BufferedValueTypeBlob* buffered_blob = SharedRuntime::generate_buffered_inline_type_adapter(this);
+        *((address*)adr_pack_handler()) = buffered_blob->pack_fields();
+        *((address*)adr_pack_handler_jobject()) = buffered_blob->pack_fields_jobject();
+        *((address*)adr_unpack_handler()) = buffered_blob->unpack_fields();
+        assert(CodeCache::find_blob(pack_handler()) == buffered_blob, "lost track of blob");
+        assert(can_be_returned_as_fields(), "sanity");
+      }
+    }
+    if (!can_be_returned_as_fields() && !can_be_passed_as_fields()) {
+      MetadataFactory::free_array<SigEntry>(class_loader_data(), extended_sig);
+      assert(return_regs() == NULL, "sanity");
+    }
+  }
+}
+
+void InlineKlass::deallocate_contents(ClassLoaderData* loader_data) {
+  if (extended_sig() != NULL) {
+    MetadataFactory::free_array<SigEntry>(loader_data, extended_sig());
+  }
+  if (return_regs() != NULL) {
+    MetadataFactory::free_array<VMRegPair>(loader_data, return_regs());
+  }
+  cleanup_blobs();
+  InstanceKlass::deallocate_contents(loader_data);
+}
+
+void InlineKlass::cleanup(InlineKlass* ik) {
+  ik->cleanup_blobs();
+}
+
+void InlineKlass::cleanup_blobs() {
+  if (pack_handler() != NULL) {
+    CodeBlob* buffered_blob = CodeCache::find_blob(pack_handler());
+    assert(buffered_blob->is_buffered_value_type_blob(), "bad blob type");
+    BufferBlob::free((BufferBlob*)buffered_blob);
+    *((address*)adr_pack_handler()) = NULL;
+    *((address*)adr_pack_handler_jobject()) = NULL;
+    *((address*)adr_unpack_handler()) = NULL;
+  }
+}
+
+// Can this inline type be scalarized?
+bool InlineKlass::is_scalarizable() const {
+  return ScalarizeInlineTypes;
+}
+
+// Can this inline type be passed as multiple values?
+bool InlineKlass::can_be_passed_as_fields() const {
+  return InlineTypePassFieldsAsArgs && is_scalarizable() && !is_empty_inline_type();
+}
+
+// Can this inline type be returned as multiple values?
+bool InlineKlass::can_be_returned_as_fields(bool init) const {
+  return InlineTypeReturnedAsFields && is_scalarizable() && !is_empty_inline_type() && (init || return_regs() != NULL);
+}
+
+// Create handles for all oop fields returned in registers that are going to be live across a safepoint
+void InlineKlass::save_oop_fields(const RegisterMap& reg_map, GrowableArray<Handle>& handles) const {
+  Thread* thread = Thread::current();
+  const Array<SigEntry>* sig_vk = extended_sig();
+  const Array<VMRegPair>* regs = return_regs();
+  int j = 1;
+
+  for (int i = 0; i < sig_vk->length(); i++) {
+    BasicType bt = sig_vk->at(i)._bt;
+    if (bt == T_OBJECT || bt == T_ARRAY) {
+      VMRegPair pair = regs->at(j);
+      address loc = reg_map.location(pair.first());
+      oop v = *(oop*)loc;
+      assert(v == NULL || oopDesc::is_oop(v), "not an oop?");
+      assert(Universe::heap()->is_in_or_null(v), "must be heap pointer");
+      handles.push(Handle(thread, v));
+    }
+    if (bt == T_INLINE_TYPE) {
+      continue;
+    }
+    if (bt == T_VOID &&
+        sig_vk->at(i-1)._bt != T_LONG &&
+        sig_vk->at(i-1)._bt != T_DOUBLE) {
+      continue;
+    }
+    j++;
+  }
+  assert(j == regs->length(), "missed a field?");
+}
+
+// Update oop fields in registers from handles after a safepoint
+void InlineKlass::restore_oop_results(RegisterMap& reg_map, GrowableArray<Handle>& handles) const {
+  assert(InlineTypeReturnedAsFields, "inconsistent");
+  const Array<SigEntry>* sig_vk = extended_sig();
+  const Array<VMRegPair>* regs = return_regs();
+  assert(regs != NULL, "inconsistent");
+
+  int j = 1;
+  for (int i = 0, k = 0; i < sig_vk->length(); i++) {
+    BasicType bt = sig_vk->at(i)._bt;
+    if (bt == T_OBJECT || bt == T_ARRAY) {
+      VMRegPair pair = regs->at(j);
+      address loc = reg_map.location(pair.first());
+      *(oop*)loc = handles.at(k++)();
+    }
+    if (bt == T_INLINE_TYPE) {
+      continue;
+    }
+    if (bt == T_VOID &&
+        sig_vk->at(i-1)._bt != T_LONG &&
+        sig_vk->at(i-1)._bt != T_DOUBLE) {
+      continue;
+    }
+    j++;
+  }
+  assert(j == regs->length(), "missed a field?");
+}
+
+// Fields are in registers. Create an instance of the inline type and
+// initialize it with the values of the fields.
+oop InlineKlass::realloc_result(const RegisterMap& reg_map, const GrowableArray<Handle>& handles, TRAPS) {
+  oop new_vt = allocate_instance(CHECK_NULL);
+  const Array<SigEntry>* sig_vk = extended_sig();
+  const Array<VMRegPair>* regs = return_regs();
+
+  int j = 1;
+  int k = 0;
+  for (int i = 0; i < sig_vk->length(); i++) {
+    BasicType bt = sig_vk->at(i)._bt;
+    if (bt == T_INLINE_TYPE) {
+      continue;
+    }
+    if (bt == T_VOID) {
+      if (sig_vk->at(i-1)._bt == T_LONG ||
+          sig_vk->at(i-1)._bt == T_DOUBLE) {
+        j++;
+      }
+      continue;
+    }
+    int off = sig_vk->at(i)._offset;
+    assert(off > 0, "offset in object should be positive");
+    VMRegPair pair = regs->at(j);
+    address loc = reg_map.location(pair.first());
+    switch(bt) {
+    case T_BOOLEAN: {
+      new_vt->bool_field_put(off, *(jboolean*)loc);
+      break;
+    }
+    case T_CHAR: {
+      new_vt->char_field_put(off, *(jchar*)loc);
+      break;
+    }
+    case T_BYTE: {
+      new_vt->byte_field_put(off, *(jbyte*)loc);
+      break;
+    }
+    case T_SHORT: {
+      new_vt->short_field_put(off, *(jshort*)loc);
+      break;
+    }
+    case T_INT: {
+      new_vt->int_field_put(off, *(jint*)loc);
+      break;
+    }
+    case T_LONG: {
+#ifdef _LP64
+      new_vt->double_field_put(off,  *(jdouble*)loc);
+#else
+      Unimplemented();
+#endif
+      break;
+    }
+    case T_OBJECT:
+    case T_ARRAY: {
+      Handle handle = handles.at(k++);
+      new_vt->obj_field_put(off, handle());
+      break;
+    }
+    case T_FLOAT: {
+      new_vt->float_field_put(off,  *(jfloat*)loc);
+      break;
+    }
+    case T_DOUBLE: {
+      new_vt->double_field_put(off, *(jdouble*)loc);
+      break;
+    }
+    default:
+      ShouldNotReachHere();
+    }
+    *(intptr_t*)loc = 0xDEAD;
+    j++;
+  }
+  assert(j == regs->length(), "missed a field?");
+  assert(k == handles.length(), "missed an oop?");
+  return new_vt;
+}
+
+// Check the return register for a InlineKlass oop
+InlineKlass* InlineKlass::returned_inline_klass(const RegisterMap& map) {
+  BasicType bt = T_METADATA;
+  VMRegPair pair;
+  int nb = SharedRuntime::java_return_convention(&bt, &pair, 1);
+  assert(nb == 1, "broken");
+
+  address loc = map.location(pair.first());
+  intptr_t ptr = *(intptr_t*)loc;
+  if (is_set_nth_bit(ptr, 0)) {
+    // Oop is tagged, must be a InlineKlass oop
+    clear_nth_bit(ptr, 0);
+    assert(Metaspace::contains((void*)ptr), "should be klass");
+    InlineKlass* vk = (InlineKlass*)ptr;
+    assert(vk->can_be_returned_as_fields(), "must be able to return as fields");
+    return vk;
+  }
+#ifdef ASSERT
+  // Oop is not tagged, must be a valid oop
+  if (VerifyOops) {
+    oopDesc::verify(oop((HeapWord*)ptr));
+  }
+#endif
+  return NULL;
+}
+
+void InlineKlass::verify_on(outputStream* st) {
+  InstanceKlass::verify_on(st);
+  guarantee(prototype_header().is_always_locked(), "Prototype header is not always locked");
+}
+
+void InlineKlass::oop_verify_on(oop obj, outputStream* st) {
+  InstanceKlass::oop_verify_on(obj, st);
+  guarantee(obj->mark().is_always_locked(), "Header is not always locked");
+}
+
+void InlineKlass::metaspace_pointers_do(MetaspaceClosure* it) {
+  InstanceKlass::metaspace_pointers_do(it);
+
+  InlineKlass* this_ptr = this;
+  it->push_internal_pointer(&this_ptr, (intptr_t*)&_adr_inlineklass_fixed_block);
+}
diff a/src/hotspot/share/oops/inlineKlass.hpp b/src/hotspot/share/oops/inlineKlass.hpp
--- /dev/null
+++ b/src/hotspot/share/oops/inlineKlass.hpp
@@ -0,0 +1,313 @@
+/*
+ * Copyright (c) 2017, 2020, Oracle and/or its affiliates. All rights reserved.
+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
+ *
+ * This code is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 only, as
+ * published by the Free Software Foundation.
+ *
+ * This code is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
+ * version 2 for more details (a copy is included in the LICENSE file that
+ * accompanied this code).
+ *
+ * You should have received a copy of the GNU General Public License version
+ * 2 along with this work; if not, write to the Free Software Foundation,
+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
+ *
+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
+ * or visit www.oracle.com if you need additional information or have any
+ * questions.
+ *
+ */
+
+#ifndef SHARE_VM_OOPS_INLINEKLASS_HPP
+#define SHARE_VM_OOPS_INLINEKLASS_HPP
+
+#include "classfile/javaClasses.hpp"
+#include "oops/instanceKlass.hpp"
+#include "oops/method.hpp"
+//#include "oops/oop.inline.hpp"
+
+// An InlineKlass is a specialized InstanceKlass for inline types.
+
+
+class InlineKlass: public InstanceKlass {
+  friend class VMStructs;
+  friend class InstanceKlass;
+
+ public:
+  InlineKlass() { assert(DumpSharedSpaces || UseSharedSpaces, "only for CDS"); }
+
+ private:
+
+  // Constructor
+  InlineKlass(const ClassFileParser& parser);
+
+  InlineKlassFixedBlock* inlineklass_static_block() const {
+    address adr_jf = adr_inline_type_field_klasses();
+    if (adr_jf != NULL) {
+      return (InlineKlassFixedBlock*)(adr_jf + this->java_fields_count() * sizeof(Klass*));
+    }
+
+    address adr_fing = adr_fingerprint();
+    if (adr_fing != NULL) {
+      return (InlineKlassFixedBlock*)(adr_fingerprint() + sizeof(u8));
+    }
+
+    InstanceKlass** adr_host = adr_unsafe_anonymous_host();
+    if (adr_host != NULL) {
+      return (InlineKlassFixedBlock*)(adr_host + 1);
+    }
+
+    Klass* volatile* adr_impl = adr_implementor();
+    if (adr_impl != NULL) {
+      return (InlineKlassFixedBlock*)(adr_impl + 1);
+    }
+
+    return (InlineKlassFixedBlock*)end_of_nonstatic_oop_maps();
+  }
+
+  address adr_extended_sig() const {
+    assert(_adr_inlineklass_fixed_block != NULL, "Should have been initialized");
+    return ((address)_adr_inlineklass_fixed_block) + in_bytes(byte_offset_of(InlineKlassFixedBlock, _extended_sig));
+  }
+
+  address adr_return_regs() const {
+    InlineKlassFixedBlock* vkst = inlineklass_static_block();
+    return ((address)_adr_inlineklass_fixed_block) + in_bytes(byte_offset_of(InlineKlassFixedBlock, _return_regs));
+  }
+
+  // pack and unpack handlers for inline types return
+  address adr_pack_handler() const {
+    assert(_adr_inlineklass_fixed_block != NULL, "Should have been initialized");
+    return ((address)_adr_inlineklass_fixed_block) + in_bytes(byte_offset_of(InlineKlassFixedBlock, _pack_handler));
+  }
+
+  address adr_pack_handler_jobject() const {
+    assert(_adr_inlineklass_fixed_block != NULL, "Should have been initialized");
+    return ((address)_adr_inlineklass_fixed_block) + in_bytes(byte_offset_of(InlineKlassFixedBlock, _pack_handler_jobject));
+  }
+
+  address adr_unpack_handler() const {
+    assert(_adr_inlineklass_fixed_block != NULL, "Should have been initialized");
+    return ((address)_adr_inlineklass_fixed_block) + in_bytes(byte_offset_of(InlineKlassFixedBlock, _unpack_handler));
+  }
+
+  address adr_default_value_offset() const {
+    assert(_adr_inlineklass_fixed_block != NULL, "Should have been initialized");
+    return ((address)_adr_inlineklass_fixed_block) + in_bytes(default_value_offset_offset());
+  }
+
+  address adr_value_array_klass() const {
+    assert(_adr_inlineklass_fixed_block != NULL, "Should have been initialized");
+    return ((address)_adr_inlineklass_fixed_block) + in_bytes(byte_offset_of(InlineKlassFixedBlock, _value_array_klass));
+  }
+
+  Klass* get_value_array_klass() const {
+    return *(Klass**)adr_value_array_klass();
+  }
+
+  Klass* acquire_value_array_klass() const {
+    return Atomic::load_acquire((Klass**)adr_value_array_klass());
+  }
+
+  Klass* allocate_value_array_klass(TRAPS);
+
+  address adr_alignment() const {
+    assert(_adr_inlineklass_fixed_block != NULL, "Should have been initialized");
+    return ((address)_adr_inlineklass_fixed_block) + in_bytes(byte_offset_of(InlineKlassFixedBlock, _alignment));
+  }
+
+  address adr_first_field_offset() const {
+    assert(_adr_inlineklass_fixed_block != NULL, "Should have been initialized");
+    return ((address)_adr_inlineklass_fixed_block) + in_bytes(byte_offset_of(InlineKlassFixedBlock, _first_field_offset));
+  }
+
+  address adr_exact_size_in_bytes() const {
+    assert(_adr_inlineklass_fixed_block != NULL, "Should have been initialized");
+    return ((address)_adr_inlineklass_fixed_block) + in_bytes(byte_offset_of(InlineKlassFixedBlock, _exact_size_in_bytes));
+  }
+
+ public:
+  int get_alignment() const {
+    return *(int*)adr_alignment();
+  }
+
+  void set_alignment(int alignment) {
+    *(int*)adr_alignment() = alignment;
+  }
+
+  int first_field_offset() const {
+    int offset = *(int*)adr_first_field_offset();
+    assert(offset != 0, "Must be initialized before use");
+    return *(int*)adr_first_field_offset();
+  }
+
+  void set_first_field_offset(int offset) {
+    *(int*)adr_first_field_offset() = offset;
+  }
+
+  int get_exact_size_in_bytes() const {
+    return *(int*)adr_exact_size_in_bytes();
+  }
+
+  void set_exact_size_in_bytes(int exact_size) {
+    *(int*)adr_exact_size_in_bytes() = exact_size;
+  }
+
+  int first_field_offset_old();
+
+  virtual void remove_unshareable_info();
+  virtual void restore_unshareable_info(ClassLoaderData* loader_data, Handle protection_domain, PackageEntry* pkg_entry, TRAPS);
+  virtual void metaspace_pointers_do(MetaspaceClosure* it);
+
+ private:
+  int collect_fields(GrowableArray<SigEntry>* sig, int base_off = 0);
+
+  void cleanup_blobs();
+
+
+ protected:
+  // Returns the array class for the n'th dimension
+  Klass* array_klass_impl(bool or_null, int n, TRAPS);
+
+  // Returns the array class with this class as element type
+  Klass* array_klass_impl(bool or_null, TRAPS);
+
+  // Specifically flat array klass
+  Klass* value_array_klass(bool or_null, int rank, TRAPS);
+
+ public:
+  // Type testing
+  bool is_inline_klass_slow() const        { return true; }
+
+  // Casting from Klass*
+  static InlineKlass* cast(Klass* k);
+
+  // Use this to return the size of an instance in heap words.
+  // Note that this size only applies to heap allocated stand-alone instances.
+  virtual int size_helper() const {
+    return layout_helper_to_size_helper(layout_helper());
+  }
+
+  // Metadata iterators
+  void array_klasses_do(void f(Klass* k));
+  void array_klasses_do(void f(Klass* k, TRAPS), TRAPS);
+
+  // allocate_instance() allocates a stand alone value in the Java heap
+  // initialized to default value (cleared memory)
+  instanceOop allocate_instance(TRAPS);
+  // allocates a stand alone inline buffer in the Java heap
+  // DOES NOT have memory cleared, user MUST initialize payload before
+  // returning to Java (i.e.: inline_copy)
+  instanceOop allocate_instance_buffer(TRAPS);
+
+  // minimum number of bytes occupied by nonstatic fields, HeapWord aligned or pow2
+  int raw_value_byte_size();
+
+  address data_for_oop(oop o) const;
+  oop oop_for_data(address data) const;
+
+  // Query if this class promises atomicity one way or another
+  bool is_atomic() { return is_naturally_atomic() || is_declared_atomic(); }
+
+  bool flatten_array();
+
+  bool contains_oops() const { return nonstatic_oop_map_count() > 0; }
+  int nonstatic_oop_count();
+
+  // General store methods
+  //
+  // Normally loads and store methods would be found in *Oops classes, but since values can be
+  // "in-lined" (flattened) into containing oops, these methods reside here in InlineKlass.
+  //
+  // "inline_copy_*_to_new_*" assume new memory (i.e. IS_DEST_UNINITIALIZED for write barriers)
+
+  void inline_copy_payload_to_new_oop(void* src, oop dst);
+  void inline_copy_oop_to_new_oop(oop src, oop dst);
+  void inline_copy_oop_to_new_payload(oop src, void* dst);
+  void inline_copy_oop_to_payload(oop src, void* dst);
+
+  oop read_inlined_field(oop obj, int offset, TRAPS);
+  void write_inlined_field(oop obj, int offset, oop value, TRAPS);
+
+  // oop iterate raw inline type data pointer (where oop_addr may not be an oop, but backing/array-element)
+  template <typename T, class OopClosureType>
+  inline void oop_iterate_specialized(const address oop_addr, OopClosureType* closure);
+
+  template <typename T, class OopClosureType>
+  inline void oop_iterate_specialized_bounded(const address oop_addr, OopClosureType* closure, void* lo, void* hi);
+
+  // calling convention support
+  void initialize_calling_convention(TRAPS);
+  Array<SigEntry>* extended_sig() const {
+    return *((Array<SigEntry>**)adr_extended_sig());
+  }
+  Array<VMRegPair>* return_regs() const {
+    return *((Array<VMRegPair>**)adr_return_regs());
+  }
+  bool is_scalarizable() const;
+  bool can_be_passed_as_fields() const;
+  bool can_be_returned_as_fields(bool init = false) const;
+  void save_oop_fields(const RegisterMap& map, GrowableArray<Handle>& handles) const;
+  void restore_oop_results(RegisterMap& map, GrowableArray<Handle>& handles) const;
+  oop realloc_result(const RegisterMap& reg_map, const GrowableArray<Handle>& handles, TRAPS);
+  static InlineKlass* returned_inline_klass(const RegisterMap& reg_map);
+
+  address pack_handler() const {
+    return *(address*)adr_pack_handler();
+  }
+
+  address unpack_handler() const {
+    return *(address*)adr_unpack_handler();
+  }
+
+  // pack and unpack handlers. Need to be loadable from generated code
+  // so at a fixed offset from the base of the klass pointer.
+  static ByteSize pack_handler_offset() {
+    return byte_offset_of(InlineKlassFixedBlock, _pack_handler);
+  }
+
+  static ByteSize pack_handler_jobject_offset() {
+    return byte_offset_of(InlineKlassFixedBlock, _pack_handler_jobject);
+  }
+
+  static ByteSize unpack_handler_offset() {
+    return byte_offset_of(InlineKlassFixedBlock, _unpack_handler);
+  }
+
+  static ByteSize default_value_offset_offset() {
+    return byte_offset_of(InlineKlassFixedBlock, _default_value_offset);
+  }
+
+  static ByteSize first_field_offset_offset() {
+    return byte_offset_of(InlineKlassFixedBlock, _first_field_offset);
+  }
+
+  void set_default_value_offset(int offset) {
+    *((int*)adr_default_value_offset()) = offset;
+  }
+
+  int default_value_offset() {
+    int offset = *((int*)adr_default_value_offset());
+    assert(offset != 0, "must not be called if not initialized");
+    return offset;
+  }
+
+  void set_default_value(oop val) {
+    java_mirror()->obj_field_put(default_value_offset(), val);
+  }
+
+  oop default_value();
+  void deallocate_contents(ClassLoaderData* loader_data);
+  static void cleanup(InlineKlass* ik) ;
+
+  // Verification
+  void verify_on(outputStream* st);
+  void oop_verify_on(oop obj, outputStream* st);
+
+};
+
+#endif /* SHARE_VM_OOPS_INLINEKLASS_HPP */
diff a/src/hotspot/share/oops/inlineKlass.inline.hpp b/src/hotspot/share/oops/inlineKlass.inline.hpp
--- /dev/null
+++ b/src/hotspot/share/oops/inlineKlass.inline.hpp
@@ -0,0 +1,104 @@
+/*
+ * Copyright (c) 2017, 2020, Oracle and/or its affiliates. All rights reserved.
+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
+ *
+ * This code is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 only, as
+ * published by the Free Software Foundation.
+ *
+ * This code is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
+ * version 2 for more details (a copy is included in the LICENSE file that
+ * accompanied this code).
+ *
+ * You should have received a copy of the GNU General Public License version
+ * 2 along with this work; if not, write to the Free Software Foundation,
+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
+ *
+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
+ * or visit www.oracle.com if you need additional information or have any
+ * questions.
+ *
+ */
+#ifndef SHARE_VM_OOPS_INLINEKLASS_INLINE_HPP
+#define SHARE_VM_OOPS_INLINEKLASS_INLINE_HPP
+
+#include "memory/iterator.hpp"
+#include "oops/klass.hpp"
+#include "oops/valueArrayKlass.hpp"
+#include "oops/oop.inline.hpp"
+#include "oops/inlineKlass.hpp"
+#include "utilities/macros.hpp"
+
+inline InlineKlass* InlineKlass::cast(Klass* k) {
+  assert(k->is_inline_klass(), "cast to InlineKlass");
+  return (InlineKlass*) k;
+}
+
+inline address InlineKlass::data_for_oop(oop o) const {
+  return ((address) (void*) o) + first_field_offset();
+}
+
+inline oop InlineKlass::oop_for_data(address data) const {
+  oop o = (oop) (data - first_field_offset());
+  assert(oopDesc::is_oop(o, false), "Not an oop");
+  return o;
+}
+
+inline void InlineKlass::inline_copy_payload_to_new_oop(void* src, oop dst) {
+  HeapAccess<IS_DEST_UNINITIALIZED>::value_copy(src, data_for_oop(dst), this);
+}
+
+inline void InlineKlass::inline_copy_oop_to_new_oop(oop src, oop dst) {
+  HeapAccess<IS_DEST_UNINITIALIZED>::value_copy(data_for_oop(src), data_for_oop(dst), this);
+}
+
+inline void InlineKlass::inline_copy_oop_to_new_payload(oop src, void* dst) {
+  HeapAccess<IS_DEST_UNINITIALIZED>::value_copy(data_for_oop(src), dst, this);
+}
+
+inline void InlineKlass::inline_copy_oop_to_payload(oop src, void* dst) {
+  HeapAccess<>::value_copy(data_for_oop(src), dst, this);
+}
+
+
+template <typename T, class OopClosureType>
+void InlineKlass::oop_iterate_specialized(const address oop_addr, OopClosureType* closure) {
+  OopMapBlock* map = start_of_nonstatic_oop_maps();
+  OopMapBlock* const end_map = map + nonstatic_oop_map_count();
+
+  for (; map < end_map; map++) {
+    T* p = (T*) (oop_addr + map->offset());
+    T* const end = p + map->count();
+    for (; p < end; ++p) {
+      Devirtualizer::do_oop(closure, p);
+    }
+  }
+}
+
+template <typename T, class OopClosureType>
+inline void InlineKlass::oop_iterate_specialized_bounded(const address oop_addr, OopClosureType* closure, void* lo, void* hi) {
+  OopMapBlock* map = start_of_nonstatic_oop_maps();
+  OopMapBlock* const end_map = map + nonstatic_oop_map_count();
+
+  T* const l   = (T*) lo;
+  T* const h   = (T*) hi;
+
+  for (; map < end_map; map++) {
+    T* p = (T*) (oop_addr + map->offset());
+    T* end = p + map->count();
+    if (p < l) {
+      p = l;
+    }
+    if (end > h) {
+      end = h;
+    }
+    for (; p < end; ++p) {
+      Devirtualizer::do_oop(closure, p);
+    }
+  }
+}
+
+
+#endif // SHARE_VM_OOPS_INLINEKLASS_INLINE_HPP
diff a/src/hotspot/share/oops/instanceKlass.cpp b/src/hotspot/share/oops/instanceKlass.cpp
--- a/src/hotspot/share/oops/instanceKlass.cpp
+++ b/src/hotspot/share/oops/instanceKlass.cpp
@@ -63,11 +63,11 @@
 #include "oops/klass.inline.hpp"
 #include "oops/method.hpp"
 #include "oops/oop.inline.hpp"
 #include "oops/recordComponent.hpp"
 #include "oops/symbol.hpp"
-#include "oops/valueKlass.hpp"
+#include "oops/inlineKlass.hpp"
 #include "prims/jvmtiExport.hpp"
 #include "prims/jvmtiRedefineClasses.hpp"
 #include "prims/jvmtiThreadState.hpp"
 #include "prims/methodComparator.hpp"
 #include "runtime/atomic.hpp"
@@ -495,11 +495,11 @@
     } else if (is_class_loader(class_name, parser)) {
       // class loader
       ik = new (loader_data, size, THREAD) InstanceClassLoaderKlass(parser);
     } else if (parser.is_inline_type()) {
       // inline type
-      ik = new (loader_data, size, THREAD) ValueKlass(parser);
+      ik = new (loader_data, size, THREAD) InlineKlass(parser);
     } else {
       // normal
       ik = new (loader_data, size, THREAD) InstanceKlass(parser, InstanceKlass::_kind_other);
     }
   } else {
@@ -580,11 +580,11 @@
   _nest_host_index(0),
   _init_state(allocated),
   _reference_type(parser.reference_type()),
   _init_thread(NULL),
   _inline_type_field_klasses(NULL),
-  _adr_valueklass_fixed_block(NULL)
+  _adr_inlineklass_fixed_block(NULL)
 {
   set_vtable_length(parser.vtable_size());
   set_kind(kind);
   set_access_flags(parser.access_flags());
   if (parser.is_hidden()) set_is_hidden();
@@ -1275,11 +1275,11 @@
           set_inline_type_field_klass(fs.index(), klass);
         }
         InstanceKlass::cast(klass)->initialize(CHECK);
         if (fs.access_flags().is_static()) {
           if (java_mirror()->obj_field(fs.offset()) == NULL) {
-            java_mirror()->obj_field_put(fs.offset(), ValueKlass::cast(klass)->default_value());
+            java_mirror()->obj_field_put(fs.offset(), InlineKlass::cast(klass)->default_value());
           }
         }
       }
     }
   }
@@ -1718,11 +1718,11 @@
   return NULL;
 }
 
 bool InstanceKlass::contains_field_offset(int offset) {
   if (this->is_inline_klass()) {
-    ValueKlass* vk = ValueKlass::cast(this);
+    InlineKlass* vk = InlineKlass::cast(this);
     return offset >= vk->first_field_offset() && offset < (vk->first_field_offset() + vk->get_exact_size_in_bytes());
   } else {
     fieldDescriptor fd;
     return find_field_from_offset(offset, false, &fd);
   }
@@ -2709,11 +2709,11 @@
   assert(!is_loaded(), "invalid init state");
   set_package(loader_data, pkg_entry, CHECK);
   Klass::restore_unshareable_info(loader_data, protection_domain, CHECK);
 
   if (is_inline_klass()) {
-    ValueKlass::cast(this)->initialize_calling_convention(CHECK);
+    InlineKlass::cast(this)->initialize_calling_convention(CHECK);
   }
 
   Array<Method*>* methods = this->methods();
   int num_methods = methods->length();
   for (int index = 0; index < num_methods; ++index) {
diff a/src/hotspot/share/oops/instanceKlass.hpp b/src/hotspot/share/oops/instanceKlass.hpp
--- a/src/hotspot/share/oops/instanceKlass.hpp
+++ b/src/hotspot/share/oops/instanceKlass.hpp
@@ -54,11 +54,11 @@
 //      indicating where oops are located in instances of this klass.
 //    [EMBEDDED implementor of the interface] only exist for interface
 //    [EMBEDDED unsafe_anonymous_host klass] only exist for an unsafe anonymous class (JSR 292 enabled)
 //    [EMBEDDED fingerprint       ] only if should_store_fingerprint()==true
 //    [EMBEDDED inline_type_field_klasses] only if has_inline_fields() == true
-//    [EMBEDDED ValueKlassFixedBlock] only if is a ValueKlass instance
+//    [EMBEDDED InlineKlassFixedBlock] only if is a InlineKlass instance
 
 
 // forward declaration for class -- see below for definition
 #if INCLUDE_JVMTI
 class BreakpointInfo;
@@ -136,11 +136,11 @@
 
 struct JvmtiCachedClassFileData;
 
 class SigEntry;
 
-class ValueKlassFixedBlock {
+class InlineKlassFixedBlock {
   Array<SigEntry>** _extended_sig;
   Array<VMRegPair>** _return_regs;
   address* _pack_handler;
   address* _pack_handler_jobject;
   address* _unpack_handler;
@@ -148,11 +148,11 @@
   Klass** _value_array_klass;
   int _alignment;
   int _first_field_offset;
   int _exact_size_in_bytes;
 
-  friend class ValueKlass;
+  friend class InlineKlass;
 };
 
 class InlineTypes {
 public:
   u2 _class_info_index;
@@ -357,11 +357,11 @@
   //     [generic signature index]
   //     ...
   Array<u2>*      _fields;
   const Klass**   _inline_type_field_klasses; // For "inline class" fields, NULL if none present
 
-  const ValueKlassFixedBlock* _adr_valueklass_fixed_block;
+  const InlineKlassFixedBlock* _adr_inlineklass_fixed_block;
 
   // embedded Java vtable follows here
   // embedded Java itables follows here
   // embedded static fields follows here
   // embedded nonstatic oop-map blocks follows here
@@ -1138,11 +1138,11 @@
   static ByteSize init_state_offset()  { return in_ByteSize(offset_of(InstanceKlass, _init_state)); }
   JFR_ONLY(DEFINE_KLASS_TRACE_ID_OFFSET;)
   static ByteSize init_thread_offset() { return in_ByteSize(offset_of(InstanceKlass, _init_thread)); }
 
   static ByteSize inline_type_field_klasses_offset() { return in_ByteSize(offset_of(InstanceKlass, _inline_type_field_klasses)); }
-  static ByteSize adr_valueklass_fixed_block_offset() { return in_ByteSize(offset_of(InstanceKlass, _adr_valueklass_fixed_block)); }
+  static ByteSize adr_inlineklass_fixed_block_offset() { return in_ByteSize(offset_of(InstanceKlass, _adr_inlineklass_fixed_block)); }
 
   // subclass/subinterface checks
   bool implements_interface(Klass* k) const;
   bool is_same_or_direct_interface(Klass* k) const;
 
@@ -1206,11 +1206,11 @@
            nonstatic_oop_map_size +
            (is_interface ? (int)sizeof(Klass*)/wordSize : 0) +
            (is_unsafe_anonymous ? (int)sizeof(Klass*)/wordSize : 0) +
            (has_stored_fingerprint ? (int)sizeof(uint64_t*)/wordSize : 0) +
            (java_fields * (int)sizeof(Klass*)/wordSize) +
-           (is_inline_type ? (int)sizeof(ValueKlassFixedBlock) : 0));
+           (is_inline_type ? (int)sizeof(InlineKlassFixedBlock) : 0));
   }
   int size() const                    { return size(vtable_length(),
                                                itable_length(),
                                                nonstatic_oop_map_size(),
                                                is_interface(),
diff a/src/hotspot/share/oops/method.cpp b/src/hotspot/share/oops/method.cpp
--- a/src/hotspot/share/oops/method.cpp
+++ b/src/hotspot/share/oops/method.cpp
@@ -52,11 +52,11 @@
 #include "oops/methodData.hpp"
 #include "oops/objArrayKlass.hpp"
 #include "oops/objArrayOop.inline.hpp"
 #include "oops/oop.inline.hpp"
 #include "oops/symbol.hpp"
-#include "oops/valueKlass.inline.hpp"
+#include "oops/inlineKlass.inline.hpp"
 #include "prims/jvmtiExport.hpp"
 #include "prims/methodHandles.hpp"
 #include "prims/nativeLookup.hpp"
 #include "runtime/arguments.hpp"
 #include "runtime/atomic.hpp"
@@ -600,14 +600,14 @@
   set_size_of_parameters(fp.size_of_parameters());
   constMethod()->set_result_type(fp.return_type());
   constMethod()->set_fingerprint(fp.fingerprint());
 }
 
-// ValueKlass the method is declared to return. This must not
+// InlineKlass the method is declared to return. This must not
 // safepoint as it is called with references live on the stack at
 // locations the GC is unaware of.
-ValueKlass* Method::returned_value_type(Thread* thread) const {
+InlineKlass* Method::returned_inline_type(Thread* thread) const {
   SignatureStream ss(signature());
   while (!ss.at_return_type()) {
     ss.next();
   }
   Handle class_loader(thread, method_holder()->class_loader());
@@ -616,11 +616,11 @@
   {
     NoSafepointVerifier nsv;
     k = ss.as_klass(class_loader, protection_domain, SignatureStream::ReturnNull, thread);
   }
   assert(k != NULL && !thread->has_pending_exception(), "can't resolve klass");
-  return ValueKlass::cast(k);
+  return InlineKlass::cast(k);
 }
 bool Method::is_empty_method() const {
   return  code_size() == 1
       && *code_base() == Bytecodes::_return;
 }
diff a/src/hotspot/share/oops/method.hpp b/src/hotspot/share/oops/method.hpp
--- a/src/hotspot/share/oops/method.hpp
+++ b/src/hotspot/share/oops/method.hpp
@@ -627,11 +627,11 @@
   InstanceKlass* method_holder() const         { return constants()->pool_holder(); }
 
   Symbol* klass_name() const;                    // returns the name of the method holder
   BasicType result_type() const                  { return constMethod()->result_type(); }
   bool is_returning_oop() const                  { BasicType r = result_type(); return is_reference_type(r); }
-  ValueKlass* returned_value_type(Thread* thread) const;
+  InlineKlass* returned_inline_type(Thread* thread) const;
 
   // Checked exceptions thrown by this method (resolved to mirrors)
   objArrayHandle resolved_checked_exceptions(TRAPS) { return resolved_checked_exceptions_impl(this, THREAD); }
 
   // Access flags
diff a/src/hotspot/share/oops/objArrayKlass.cpp b/src/hotspot/share/oops/objArrayKlass.cpp
--- a/src/hotspot/share/oops/objArrayKlass.cpp
+++ b/src/hotspot/share/oops/objArrayKlass.cpp
@@ -157,15 +157,15 @@
                                                        /* do_zero */ true, THREAD);
   if (populate_null_free) {
     assert(dimension() == 1, "Can only populate the final dimension");
     assert(element_klass()->is_inline_klass(), "Unexpected");
     assert(!element_klass()->is_array_klass(), "ArrayKlass unexpected here");
-    assert(!ValueKlass::cast(element_klass())->flatten_array(), "Expected valueArrayOop allocation");
+    assert(!InlineKlass::cast(element_klass())->flatten_array(), "Expected valueArrayOop allocation");
     element_klass()->initialize(CHECK_NULL);
     // Populate default values...
     objArrayHandle array_h(THREAD, array);
-    instanceOop value = (instanceOop) ValueKlass::cast(element_klass())->default_value();
+    instanceOop value = (instanceOop) InlineKlass::cast(element_klass())->default_value();
     for (int i = 0; i < length; i++) {
       array_h->obj_at_put(i, value);
     }
   }
   return array;
diff a/src/hotspot/share/oops/oopsHierarchy.hpp b/src/hotspot/share/oops/oopsHierarchy.hpp
--- a/src/hotspot/share/oops/oopsHierarchy.hpp
+++ b/src/hotspot/share/oops/oopsHierarchy.hpp
@@ -177,11 +177,11 @@
 class Klass;
 class   InstanceKlass;
 class     InstanceMirrorKlass;
 class     InstanceClassLoaderKlass;
 class     InstanceRefKlass;
-class     ValueKlass;
+class     InlineKlass;
 class   ArrayKlass;
 class     ObjArrayKlass;
 class     TypeArrayKlass;
 class     ValueArrayKlass;
 
diff a/src/hotspot/share/oops/valueArrayKlass.cpp b/src/hotspot/share/oops/valueArrayKlass.cpp
--- a/src/hotspot/share/oops/valueArrayKlass.cpp
+++ b/src/hotspot/share/oops/valueArrayKlass.cpp
@@ -40,11 +40,11 @@
 #include "oops/instanceKlass.hpp"
 #include "oops/klass.inline.hpp"
 #include "oops/objArrayKlass.hpp"
 #include "oops/objArrayOop.inline.hpp"
 #include "oops/oop.inline.hpp"
-#include "oops/valueKlass.hpp"
+#include "oops/inlineKlass.hpp"
 #include "oops/valueArrayOop.hpp"
 #include "oops/valueArrayOop.inline.hpp"
 #include "oops/verifyOopClosure.hpp"
 #include "runtime/handles.inline.hpp"
 #include "runtime/mutexLocker.hpp"
@@ -56,13 +56,13 @@
 // Allocation...
 
 ValueArrayKlass::ValueArrayKlass(Klass* element_klass, Symbol* name) : ArrayKlass(name, ID) {
   assert(element_klass->is_inline_klass(), "Expected Inline");
 
-  set_element_klass(ValueKlass::cast(element_klass));
+  set_element_klass(InlineKlass::cast(element_klass));
   set_class_loader_data(element_klass->class_loader_data());
-  set_layout_helper(array_layout_helper(ValueKlass::cast(element_klass)));
+  set_layout_helper(array_layout_helper(InlineKlass::cast(element_klass)));
 
   assert(is_array_klass(), "sanity");
   assert(is_valueArray_klass(), "sanity");
 
   CMH("tweak name symbol refcnt ?")
@@ -71,22 +71,22 @@
     print();
   }
 #endif
 }
 
-ValueKlass* ValueArrayKlass::element_klass() const {
-  return ValueKlass::cast(_element_klass);
+InlineKlass* ValueArrayKlass::element_klass() const {
+  return InlineKlass::cast(_element_klass);
 }
 
 void ValueArrayKlass::set_element_klass(Klass* k) {
   _element_klass = k;
 }
 
 ValueArrayKlass* ValueArrayKlass::allocate_klass(Klass* element_klass, TRAPS) {
   guarantee((!Universe::is_bootstrapping() || SystemDictionary::Object_klass_loaded()), "Really ?!");
   assert(ValueArrayFlatten, "Flatten array required");
-  assert(ValueKlass::cast(element_klass)->is_naturally_atomic() || (!InlineArrayAtomicAccess), "Atomic by-default");
+  assert(InlineKlass::cast(element_klass)->is_naturally_atomic() || (!InlineArrayAtomicAccess), "Atomic by-default");
 
   /*
    *  MVT->LWorld, now need to allocate secondaries array types, just like objArrayKlass...
    *  ...so now we are trying out covariant array types, just copy objArrayKlass
    *  TODO refactor any remaining commonality
@@ -157,11 +157,11 @@
   assert(rank == 1, "just checking");
   int length = *last_size;
   return allocate(length, THREAD);
 }
 
-jint ValueArrayKlass::array_layout_helper(ValueKlass* vk) {
+jint ValueArrayKlass::array_layout_helper(InlineKlass* vk) {
   BasicType etype = T_INLINE_TYPE;
   int esize = upper_log2(vk->raw_value_byte_size());
   int hsize = arrayOopDesc::base_offset_in_bytes(etype);
 
   int lh = Klass::array_layout_helper(_lh_array_tag_vt_value, true, hsize, etype, esize);
@@ -249,11 +249,11 @@
      if (!s_elem_klass->is_subtype_of(d_elem_klass)) {
        THROW(vmSymbols::java_lang_ArrayStoreException());
      }
 
      valueArrayOop sa = valueArrayOop(s);
-     ValueKlass* s_elem_vklass = element_klass();
+     InlineKlass* s_elem_vklass = element_klass();
 
      // valueArray-to-valueArray
      if (dk->is_valueArray_klass()) {
        // element types MUST be exact, subtype check would be dangerous
        if (dk != this) {
@@ -303,11 +303,11 @@
      }
    } else {
      assert(s->is_objArray(), "Expected objArray");
      objArrayOop sa = objArrayOop(s);
      assert(d->is_valueArray(), "Excepted valueArray");  // objArray-to-valueArray
-     ValueKlass* d_elem_vklass = ValueKlass::cast(d_elem_klass);
+     InlineKlass* d_elem_vklass = InlineKlass::cast(d_elem_klass);
      valueArrayOop da = valueArrayOop(d);
 
      int src_end = src_pos + length;
      int delem_incr = 1 << dk->log2_element_size();
      address dst = (address) da->value_at_addr(dst_pos, layout_helper());
@@ -318,11 +318,11 @@
        }
        // Check exact type per element
        if (se->klass() != d_elem_klass) {
          THROW(vmSymbols::java_lang_ArrayStoreException());
        }
-       d_elem_vklass->value_copy_oop_to_payload(se, dst);
+       d_elem_vklass->inline_copy_oop_to_payload(se, dst);
        dst += delem_incr;
        src_pos++;
      }
    }
 }
@@ -438,11 +438,11 @@
 
 #ifndef PRODUCT
 void ValueArrayKlass::oop_print_on(oop obj, outputStream* st) {
   ArrayKlass::oop_print_on(obj, st);
   valueArrayOop va = valueArrayOop(obj);
-  ValueKlass* vk = element_klass();
+  InlineKlass* vk = element_klass();
   int print_len = MIN2((intx) va->length(), MaxElementPrintSize);
   for(int index = 0; index < print_len; index++) {
     int off = (address) va->value_at_addr(index, layout_helper()) - cast_from_oop<address>(obj);
     st->print_cr(" - Index %3d offset %3d: ", index, off);
     oop obj = (oop) ((address)va->value_at_addr(index, layout_helper()) - vk->first_field_offset());
diff a/src/hotspot/share/oops/valueArrayKlass.hpp b/src/hotspot/share/oops/valueArrayKlass.hpp
--- a/src/hotspot/share/oops/valueArrayKlass.hpp
+++ b/src/hotspot/share/oops/valueArrayKlass.hpp
@@ -25,11 +25,11 @@
 #ifndef SHARE_VM_OOPS_VALUEARRAYKLASS_HPP
 #define SHARE_VM_OOPS_VALUEARRAYKLASS_HPP
 
 #include "classfile/classLoaderData.hpp"
 #include "oops/arrayKlass.hpp"
-#include "oops/valueKlass.hpp"
+#include "oops/inlineKlass.hpp"
 #include "utilities/macros.hpp"
 
 /**
  * Array of values, gives a layout of typeArrayOop, but needs oops iterators
  */
@@ -52,11 +52,11 @@
 
  public:
 
   ValueArrayKlass() {}
 
-  virtual ValueKlass* element_klass() const;
+  virtual InlineKlass* element_klass() const;
   virtual void set_element_klass(Klass* k);
 
   // Casting from Klass*
   static ValueArrayKlass* cast(Klass* k) {
     assert(k->is_valueArray_klass(), "cast to ValueArrayKlass");
@@ -88,11 +88,11 @@
     return element_klass()->is_atomic();
   }
 
   oop protection_domain() const;
 
-  static jint array_layout_helper(ValueKlass* vklass); // layout helper for values
+  static jint array_layout_helper(InlineKlass* vklass); // layout helper for values
 
   // sizing
   static int header_size()  { return sizeof(ValueArrayKlass)/HeapWordSize; }
   int size() const          { return ArrayKlass::static_size(header_size()); }
 
diff a/src/hotspot/share/oops/valueArrayKlass.inline.hpp b/src/hotspot/share/oops/valueArrayKlass.inline.hpp
--- a/src/hotspot/share/oops/valueArrayKlass.inline.hpp
+++ b/src/hotspot/share/oops/valueArrayKlass.inline.hpp
@@ -30,12 +30,12 @@
 #include "oops/klass.hpp"
 #include "oops/oop.inline.hpp"
 #include "oops/valueArrayKlass.hpp"
 #include "oops/valueArrayOop.hpp"
 #include "oops/valueArrayOop.inline.hpp"
-#include "oops/valueKlass.hpp"
-#include "oops/valueKlass.inline.hpp"
+#include "oops/inlineKlass.hpp"
+#include "oops/inlineKlass.inline.hpp"
 #include "utilities/macros.hpp"
 
 /*
  * Warning incomplete: requires embedded oops, not yet enabled, so consider this a "sketch-up" of oop iterators
  */
diff a/src/hotspot/share/oops/valueArrayOop.inline.hpp b/src/hotspot/share/oops/valueArrayOop.inline.hpp
--- a/src/hotspot/share/oops/valueArrayOop.inline.hpp
+++ b/src/hotspot/share/oops/valueArrayOop.inline.hpp
@@ -26,11 +26,11 @@
 #define SHARE_VM_OOPS_VALUEARRAYOOP_INLINE_HPP
 
 #include "oops/access.inline.hpp"
 #include "oops/arrayOop.inline.hpp"
 #include "oops/valueArrayOop.hpp"
-#include "oops/valueKlass.inline.hpp"
+#include "oops/inlineKlass.inline.hpp"
 #include "oops/oop.inline.hpp"
 #include "runtime/globals.hpp"
 
 inline void* valueArrayOopDesc::base() const { return arrayOopDesc::base(T_INLINE_TYPE); }
 
@@ -46,39 +46,39 @@
   return object_size(klass()->layout_helper(), length());
 }
 
 inline oop valueArrayOopDesc::value_alloc_copy_from_index(valueArrayHandle vah, int index, TRAPS) {
   ValueArrayKlass* vaklass = ValueArrayKlass::cast(vah->klass());
-  ValueKlass* vklass = vaklass->element_klass();
+  InlineKlass* vklass = vaklass->element_klass();
   if (vklass->is_empty_inline_type()) {
     return vklass->default_value();
   } else {
     oop buf = vklass->allocate_instance(CHECK_NULL);
-    vklass->value_copy_payload_to_new_oop(vah->value_at_addr(index, vaklass->layout_helper()) ,buf);
+    vklass->inline_copy_payload_to_new_oop(vah->value_at_addr(index, vaklass->layout_helper()) ,buf);
     return buf;
   }
 }
 
 inline void valueArrayOopDesc::value_copy_from_index(int index, oop dst) const {
   ValueArrayKlass* vaklass = ValueArrayKlass::cast(klass());
-  ValueKlass* vklass = vaklass->element_klass();
+  InlineKlass* vklass = vaklass->element_klass();
   if (vklass->is_empty_inline_type()) {
     return; // Assumes dst was a new and clean buffer (OptoRuntime::load_unknown_value())
   } else {
     void* src = value_at_addr(index, vaklass->layout_helper());
-    return vklass->value_copy_payload_to_new_oop(src ,dst);
+    return vklass->inline_copy_payload_to_new_oop(src ,dst);
   }
 }
 
 inline void valueArrayOopDesc::value_copy_to_index(oop src, int index) const {
   ValueArrayKlass* vaklass = ValueArrayKlass::cast(klass());
-  ValueKlass* vklass = vaklass->element_klass();
+  InlineKlass* vklass = vaklass->element_klass();
   if (vklass->is_empty_inline_type()) {
     return;
   }
   void* dst = value_at_addr(index, vaklass->layout_helper());
-  vklass->value_copy_oop_to_payload(src, dst);
+  vklass->inline_copy_oop_to_payload(src, dst);
 }
 
 
 
 #endif // SHARE_VM_OOPS_VALUEARRAYOOP_INLINE_HPP
diff a/src/hotspot/share/oops/valueKlass.cpp b/src/hotspot/share/oops/valueKlass.cpp
--- a/src/hotspot/share/oops/valueKlass.cpp
+++ /dev/null
@@ -1,582 +0,0 @@
-/*
- * Copyright (c) 2017, 2020, Oracle and/or its affiliates. All rights reserved.
- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
- *
- * This code is free software; you can redistribute it and/or modify it
- * under the terms of the GNU General Public License version 2 only, as
- * published by the Free Software Foundation.
- *
- * This code is distributed in the hope that it will be useful, but WITHOUT
- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
- * version 2 for more details (a copy is included in the LICENSE file that
- * accompanied this code).
- *
- * You should have received a copy of the GNU General Public License version
- * 2 along with this work; if not, write to the Free Software Foundation,
- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
- *
- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
- * or visit www.oracle.com if you need additional information or have any
- * questions.
- *
- */
-
-#include "precompiled.hpp"
-#include "gc/shared/barrierSet.hpp"
-#include "gc/shared/collectedHeap.inline.hpp"
-#include "gc/shared/gcLocker.inline.hpp"
-#include "interpreter/interpreter.hpp"
-#include "logging/log.hpp"
-#include "memory/metaspaceClosure.hpp"
-#include "memory/metadataFactory.hpp"
-#include "oops/access.hpp"
-#include "oops/compressedOops.inline.hpp"
-#include "oops/fieldStreams.inline.hpp"
-#include "oops/instanceKlass.inline.hpp"
-#include "oops/method.hpp"
-#include "oops/oop.inline.hpp"
-#include "oops/objArrayKlass.hpp"
-#include "oops/valueKlass.inline.hpp"
-#include "oops/valueArrayKlass.hpp"
-#include "runtime/fieldDescriptor.inline.hpp"
-#include "runtime/handles.inline.hpp"
-#include "runtime/safepointVerifiers.hpp"
-#include "runtime/sharedRuntime.hpp"
-#include "runtime/signature.hpp"
-#include "runtime/thread.inline.hpp"
-#include "utilities/copy.hpp"
-
-  // Constructor
-ValueKlass::ValueKlass(const ClassFileParser& parser)
-    : InstanceKlass(parser, InstanceKlass::_kind_inline_type, InstanceKlass::ID) {
-  _adr_valueklass_fixed_block = valueklass_static_block();
-  // Addresses used for value type calling convention
-  *((Array<SigEntry>**)adr_extended_sig()) = NULL;
-  *((Array<VMRegPair>**)adr_return_regs()) = NULL;
-  *((address*)adr_pack_handler()) = NULL;
-  *((address*)adr_pack_handler_jobject()) = NULL;
-  *((address*)adr_unpack_handler()) = NULL;
-  assert(pack_handler() == NULL, "pack handler not null");
-  *((int*)adr_default_value_offset()) = 0;
-  *((Klass**)adr_value_array_klass()) = NULL;
-  set_prototype_header(markWord::always_locked_prototype());
-  assert(is_inline_type_klass(), "invariant");
-}
-
-oop ValueKlass::default_value() {
-  oop val = java_mirror()->obj_field_acquire(default_value_offset());
-  assert(oopDesc::is_oop(val), "Sanity check");
-  assert(val->is_inline_type(), "Sanity check");
-  assert(val->klass() == this, "sanity check");
-  return val;
-}
-
-int ValueKlass::first_field_offset_old() {
-#ifdef ASSERT
-  int first_offset = INT_MAX;
-  for (AllFieldStream fs(this); !fs.done(); fs.next()) {
-    if (fs.offset() < first_offset) first_offset= fs.offset();
-  }
-#endif
-  int base_offset = instanceOopDesc::base_offset_in_bytes();
-  // The first field of value types is aligned on a long boundary
-  base_offset = align_up(base_offset, BytesPerLong);
-  assert(base_offset == first_offset, "inconsistent offsets");
-  return base_offset;
-}
-
-int ValueKlass::raw_value_byte_size() {
-  int heapOopAlignedSize = nonstatic_field_size() << LogBytesPerHeapOop;
-  // If bigger than 64 bits or needs oop alignment, then use jlong aligned
-  // which for values should be jlong aligned, asserts in raw_field_copy otherwise
-  if (heapOopAlignedSize >= longSize || contains_oops()) {
-    return heapOopAlignedSize;
-  }
-  // Small primitives...
-  // If a few small basic type fields, return the actual size, i.e.
-  // 1 byte = 1
-  // 2 byte = 2
-  // 3 byte = 4, because pow2 needed for element stores
-  int first_offset = first_field_offset();
-  int last_offset  = 0; // find the last offset, add basic type size
-  int last_tsz     = 0;
-  for (AllFieldStream fs(this); !fs.done(); fs.next()) {
-    if (fs.access_flags().is_static()) {
-      continue;
-    } else if (fs.offset() > last_offset) {
-      BasicType type = Signature::basic_type(fs.signature());
-      if (is_java_primitive(type)) {
-        last_tsz = type2aelembytes(type);
-      } else if (type == T_INLINE_TYPE) {
-        // Not just primitives. Layout aligns embedded value, so use jlong aligned it is
-        return heapOopAlignedSize;
-      } else {
-        guarantee(0, "Unknown type %d", type);
-      }
-      assert(last_tsz != 0, "Invariant");
-      last_offset = fs.offset();
-    }
-  }
-  // Assumes VT with no fields are meaningless and illegal
-  last_offset += last_tsz;
-  assert(last_offset > first_offset && last_tsz, "Invariant");
-  return 1 << upper_log2(last_offset - first_offset);
-}
-
-instanceOop ValueKlass::allocate_instance(TRAPS) {
-  int size = size_helper();  // Query before forming handle.
-
-  instanceOop oop = (instanceOop)Universe::heap()->obj_allocate(this, size, CHECK_NULL);
-  assert(oop->mark().is_always_locked(), "Unlocked value type");
-  return oop;
-}
-
-instanceOop ValueKlass::allocate_instance_buffer(TRAPS) {
-  int size = size_helper();  // Query before forming handle.
-
-  instanceOop oop = (instanceOop)Universe::heap()->obj_buffer_allocate(this, size, CHECK_NULL);
-  assert(oop->mark().is_always_locked(), "Unlocked value type");
-  return oop;
-}
-
-int ValueKlass::nonstatic_oop_count() {
-  int oops = 0;
-  int map_count = nonstatic_oop_map_count();
-  OopMapBlock* block = start_of_nonstatic_oop_maps();
-  OopMapBlock* end = block + map_count;
-  while (block != end) {
-    oops += block->count();
-    block++;
-  }
-  return oops;
-}
-
-oop ValueKlass::read_inlined_field(oop obj, int offset, TRAPS) {
-  oop res = NULL;
-  this->initialize(CHECK_NULL); // will throw an exception if in error state
-  if (is_empty_inline_type()) {
-    res = (instanceOop)default_value();
-  } else {
-    Handle obj_h(THREAD, obj);
-    res = allocate_instance_buffer(CHECK_NULL);
-    value_copy_payload_to_new_oop(((char*)(oopDesc*)obj_h()) + offset, res);
-  }
-  assert(res != NULL, "Must be set in one of two paths above");
-  return res;
-}
-
-void ValueKlass::write_inlined_field(oop obj, int offset, oop value, TRAPS) {
-  if (value == NULL) {
-    THROW(vmSymbols::java_lang_NullPointerException());
-  }
-  if (!is_empty_inline_type()) {
-    value_copy_oop_to_payload(value, ((char*)(oopDesc*)obj) + offset);
-  }
-}
-
-// Arrays of...
-
-bool ValueKlass::flatten_array() {
-  if (!ValueArrayFlatten) {
-    return false;
-  }
-  // Too big
-  int elem_bytes = raw_value_byte_size();
-  if ((InlineArrayElemMaxFlatSize >= 0) && (elem_bytes > InlineArrayElemMaxFlatSize)) {
-    return false;
-  }
-  // Too many embedded oops
-  if ((InlineArrayElemMaxFlatOops >= 0) && (nonstatic_oop_count() > InlineArrayElemMaxFlatOops)) {
-    return false;
-  }
-  // Declared atomic but not naturally atomic.
-  if (is_declared_atomic() && !is_naturally_atomic()) {
-    return false;
-  }
-  // VM enforcing InlineArrayAtomicAccess only...
-  if (InlineArrayAtomicAccess && (!is_naturally_atomic())) {
-    return false;
-  }
-  return true;
-}
-
-void ValueKlass::remove_unshareable_info() {
-  InstanceKlass::remove_unshareable_info();
-
-  *((Array<SigEntry>**)adr_extended_sig()) = NULL;
-  *((Array<VMRegPair>**)adr_return_regs()) = NULL;
-  *((address*)adr_pack_handler()) = NULL;
-  *((address*)adr_pack_handler_jobject()) = NULL;
-  *((address*)adr_unpack_handler()) = NULL;
-  assert(pack_handler() == NULL, "pack handler not null");
-  *((Klass**)adr_value_array_klass()) = NULL;
-}
-
-void ValueKlass::restore_unshareable_info(ClassLoaderData* loader_data, Handle protection_domain, PackageEntry* pkg_entry, TRAPS) {
-  InstanceKlass::restore_unshareable_info(loader_data, protection_domain, pkg_entry, CHECK);
-}
-
-
-Klass* ValueKlass::array_klass_impl(bool or_null, int n, TRAPS) {
-  if (flatten_array()) {
-    return value_array_klass(or_null, n, THREAD);
-  } else {
-    return InstanceKlass::array_klass_impl(or_null, n, THREAD);
-  }
-}
-
-Klass* ValueKlass::array_klass_impl(bool or_null, TRAPS) {
-  return array_klass_impl(or_null, 1, THREAD);
-}
-
-Klass* ValueKlass::value_array_klass(bool or_null, int rank, TRAPS) {
-  Klass* vak = acquire_value_array_klass();
-  if (vak == NULL) {
-    if (or_null) return NULL;
-    ResourceMark rm;
-    {
-      // Atomic creation of array_klasses
-      MutexLocker ma(THREAD, MultiArray_lock);
-      if (get_value_array_klass() == NULL) {
-        vak = allocate_value_array_klass(CHECK_NULL);
-        Atomic::release_store((Klass**)adr_value_array_klass(), vak);
-      }
-    }
-  }
-  if (or_null) {
-    return vak->array_klass_or_null(rank);
-  }
-  return vak->array_klass(rank, THREAD);
-}
-
-Klass* ValueKlass::allocate_value_array_klass(TRAPS) {
-  if (flatten_array()) {
-    return ValueArrayKlass::allocate_klass(this, THREAD);
-  }
-  return ObjArrayKlass::allocate_objArray_klass(class_loader_data(), 1, this, THREAD);
-}
-
-void ValueKlass::array_klasses_do(void f(Klass* k, TRAPS), TRAPS) {
-  InstanceKlass::array_klasses_do(f, THREAD);
-  if (get_value_array_klass() != NULL)
-    ArrayKlass::cast(get_value_array_klass())->array_klasses_do(f, THREAD);
-}
-
-void ValueKlass::array_klasses_do(void f(Klass* k)) {
-  InstanceKlass::array_klasses_do(f);
-  if (get_value_array_klass() != NULL)
-    ArrayKlass::cast(get_value_array_klass())->array_klasses_do(f);
-}
-
-// Value type arguments are not passed by reference, instead each
-// field of the value type is passed as an argument. This helper
-// function collects the inlined field (recursively)
-// in a list. Included with the field's type is
-// the offset of each field in the inline type: i2c and c2i adapters
-// need that to load or store fields. Finally, the list of fields is
-// sorted in order of increasing offsets: the adapters and the
-// compiled code need to agree upon the order of fields.
-//
-// The list of basic types that is returned starts with a T_INLINE_TYPE
-// and ends with an extra T_VOID. T_INLINE_TYPE/T_VOID pairs are used as
-// delimiters. Every entry between the two is a field of the value
-// type. If there's an embedded inline type in the list, it also starts
-// with a T_INLINE_TYPE and ends with a T_VOID. This is so we can
-// generate a unique fingerprint for the method's adapters and we can
-// generate the list of basic types from the interpreter point of view
-// (value types passed as reference: iterate on the list until a
-// T_INLINE_TYPE, drop everything until and including the closing
-// T_VOID) or the compiler point of view (each field of the value
-// types is an argument: drop all T_INLINE_TYPE/T_VOID from the list).
-int ValueKlass::collect_fields(GrowableArray<SigEntry>* sig, int base_off) {
-  int count = 0;
-  SigEntry::add_entry(sig, T_INLINE_TYPE, base_off);
-  for (AllFieldStream fs(this); !fs.done(); fs.next()) {
-    if (fs.access_flags().is_static()) continue;
-    int offset = base_off + fs.offset() - (base_off > 0 ? first_field_offset() : 0);
-    if (fs.is_inlined()) {
-      // Resolve klass of inlined field and recursively collect fields
-      Klass* vk = get_inline_type_field_klass(fs.index());
-      count += ValueKlass::cast(vk)->collect_fields(sig, offset);
-    } else {
-      BasicType bt = Signature::basic_type(fs.signature());
-      if (bt == T_INLINE_TYPE) {
-        bt = T_OBJECT;
-      }
-      SigEntry::add_entry(sig, bt, offset);
-      count += type2size[bt];
-    }
-  }
-  int offset = base_off + size_helper()*HeapWordSize - (base_off > 0 ? first_field_offset() : 0);
-  SigEntry::add_entry(sig, T_VOID, offset);
-  if (base_off == 0) {
-    sig->sort(SigEntry::compare);
-  }
-  assert(sig->at(0)._bt == T_INLINE_TYPE && sig->at(sig->length()-1)._bt == T_VOID, "broken structure");
-  return count;
-}
-
-void ValueKlass::initialize_calling_convention(TRAPS) {
-  // Because the pack and unpack handler addresses need to be loadable from generated code,
-  // they are stored at a fixed offset in the klass metadata. Since value type klasses do
-  // not have a vtable, the vtable offset is used to store these addresses.
-  if (InlineTypeReturnedAsFields || InlineTypePassFieldsAsArgs) {
-    ResourceMark rm;
-    GrowableArray<SigEntry> sig_vk;
-    int nb_fields = collect_fields(&sig_vk);
-    Array<SigEntry>* extended_sig = MetadataFactory::new_array<SigEntry>(class_loader_data(), sig_vk.length(), CHECK);
-    *((Array<SigEntry>**)adr_extended_sig()) = extended_sig;
-    for (int i = 0; i < sig_vk.length(); i++) {
-      extended_sig->at_put(i, sig_vk.at(i));
-    }
-    if (can_be_returned_as_fields(/* init= */ true)) {
-      nb_fields++;
-      BasicType* sig_bt = NEW_RESOURCE_ARRAY(BasicType, nb_fields);
-      sig_bt[0] = T_METADATA;
-      SigEntry::fill_sig_bt(&sig_vk, sig_bt+1);
-      VMRegPair* regs = NEW_RESOURCE_ARRAY(VMRegPair, nb_fields);
-      int total = SharedRuntime::java_return_convention(sig_bt, regs, nb_fields);
-
-      if (total > 0) {
-        Array<VMRegPair>* return_regs = MetadataFactory::new_array<VMRegPair>(class_loader_data(), nb_fields, CHECK);
-        *((Array<VMRegPair>**)adr_return_regs()) = return_regs;
-        for (int i = 0; i < nb_fields; i++) {
-          return_regs->at_put(i, regs[i]);
-        }
-
-        BufferedValueTypeBlob* buffered_blob = SharedRuntime::generate_buffered_value_type_adapter(this);
-        *((address*)adr_pack_handler()) = buffered_blob->pack_fields();
-        *((address*)adr_pack_handler_jobject()) = buffered_blob->pack_fields_jobject();
-        *((address*)adr_unpack_handler()) = buffered_blob->unpack_fields();
-        assert(CodeCache::find_blob(pack_handler()) == buffered_blob, "lost track of blob");
-        assert(can_be_returned_as_fields(), "sanity");
-      }
-    }
-    if (!can_be_returned_as_fields() && !can_be_passed_as_fields()) {
-      MetadataFactory::free_array<SigEntry>(class_loader_data(), extended_sig);
-      assert(return_regs() == NULL, "sanity");
-    }
-  }
-}
-
-void ValueKlass::deallocate_contents(ClassLoaderData* loader_data) {
-  if (extended_sig() != NULL) {
-    MetadataFactory::free_array<SigEntry>(loader_data, extended_sig());
-  }
-  if (return_regs() != NULL) {
-    MetadataFactory::free_array<VMRegPair>(loader_data, return_regs());
-  }
-  cleanup_blobs();
-  InstanceKlass::deallocate_contents(loader_data);
-}
-
-void ValueKlass::cleanup(ValueKlass* ik) {
-  ik->cleanup_blobs();
-}
-
-void ValueKlass::cleanup_blobs() {
-  if (pack_handler() != NULL) {
-    CodeBlob* buffered_blob = CodeCache::find_blob(pack_handler());
-    assert(buffered_blob->is_buffered_value_type_blob(), "bad blob type");
-    BufferBlob::free((BufferBlob*)buffered_blob);
-    *((address*)adr_pack_handler()) = NULL;
-    *((address*)adr_pack_handler_jobject()) = NULL;
-    *((address*)adr_unpack_handler()) = NULL;
-  }
-}
-
-// Can this inline type be scalarized?
-bool ValueKlass::is_scalarizable() const {
-  return ScalarizeInlineTypes;
-}
-
-// Can this value type be passed as multiple values?
-bool ValueKlass::can_be_passed_as_fields() const {
-  return InlineTypePassFieldsAsArgs && is_scalarizable() && !is_empty_inline_type();
-}
-
-// Can this value type be returned as multiple values?
-bool ValueKlass::can_be_returned_as_fields(bool init) const {
-  return InlineTypeReturnedAsFields && is_scalarizable() && !is_empty_inline_type() && (init || return_regs() != NULL);
-}
-
-// Create handles for all oop fields returned in registers that are going to be live across a safepoint
-void ValueKlass::save_oop_fields(const RegisterMap& reg_map, GrowableArray<Handle>& handles) const {
-  Thread* thread = Thread::current();
-  const Array<SigEntry>* sig_vk = extended_sig();
-  const Array<VMRegPair>* regs = return_regs();
-  int j = 1;
-
-  for (int i = 0; i < sig_vk->length(); i++) {
-    BasicType bt = sig_vk->at(i)._bt;
-    if (bt == T_OBJECT || bt == T_ARRAY) {
-      VMRegPair pair = regs->at(j);
-      address loc = reg_map.location(pair.first());
-      oop v = *(oop*)loc;
-      assert(v == NULL || oopDesc::is_oop(v), "not an oop?");
-      assert(Universe::heap()->is_in_or_null(v), "must be heap pointer");
-      handles.push(Handle(thread, v));
-    }
-    if (bt == T_INLINE_TYPE) {
-      continue;
-    }
-    if (bt == T_VOID &&
-        sig_vk->at(i-1)._bt != T_LONG &&
-        sig_vk->at(i-1)._bt != T_DOUBLE) {
-      continue;
-    }
-    j++;
-  }
-  assert(j == regs->length(), "missed a field?");
-}
-
-// Update oop fields in registers from handles after a safepoint
-void ValueKlass::restore_oop_results(RegisterMap& reg_map, GrowableArray<Handle>& handles) const {
-  assert(InlineTypeReturnedAsFields, "inconsistent");
-  const Array<SigEntry>* sig_vk = extended_sig();
-  const Array<VMRegPair>* regs = return_regs();
-  assert(regs != NULL, "inconsistent");
-
-  int j = 1;
-  for (int i = 0, k = 0; i < sig_vk->length(); i++) {
-    BasicType bt = sig_vk->at(i)._bt;
-    if (bt == T_OBJECT || bt == T_ARRAY) {
-      VMRegPair pair = regs->at(j);
-      address loc = reg_map.location(pair.first());
-      *(oop*)loc = handles.at(k++)();
-    }
-    if (bt == T_INLINE_TYPE) {
-      continue;
-    }
-    if (bt == T_VOID &&
-        sig_vk->at(i-1)._bt != T_LONG &&
-        sig_vk->at(i-1)._bt != T_DOUBLE) {
-      continue;
-    }
-    j++;
-  }
-  assert(j == regs->length(), "missed a field?");
-}
-
-// Fields are in registers. Create an instance of the value type and
-// initialize it with the values of the fields.
-oop ValueKlass::realloc_result(const RegisterMap& reg_map, const GrowableArray<Handle>& handles, TRAPS) {
-  oop new_vt = allocate_instance(CHECK_NULL);
-  const Array<SigEntry>* sig_vk = extended_sig();
-  const Array<VMRegPair>* regs = return_regs();
-
-  int j = 1;
-  int k = 0;
-  for (int i = 0; i < sig_vk->length(); i++) {
-    BasicType bt = sig_vk->at(i)._bt;
-    if (bt == T_INLINE_TYPE) {
-      continue;
-    }
-    if (bt == T_VOID) {
-      if (sig_vk->at(i-1)._bt == T_LONG ||
-          sig_vk->at(i-1)._bt == T_DOUBLE) {
-        j++;
-      }
-      continue;
-    }
-    int off = sig_vk->at(i)._offset;
-    assert(off > 0, "offset in object should be positive");
-    VMRegPair pair = regs->at(j);
-    address loc = reg_map.location(pair.first());
-    switch(bt) {
-    case T_BOOLEAN: {
-      new_vt->bool_field_put(off, *(jboolean*)loc);
-      break;
-    }
-    case T_CHAR: {
-      new_vt->char_field_put(off, *(jchar*)loc);
-      break;
-    }
-    case T_BYTE: {
-      new_vt->byte_field_put(off, *(jbyte*)loc);
-      break;
-    }
-    case T_SHORT: {
-      new_vt->short_field_put(off, *(jshort*)loc);
-      break;
-    }
-    case T_INT: {
-      new_vt->int_field_put(off, *(jint*)loc);
-      break;
-    }
-    case T_LONG: {
-#ifdef _LP64
-      new_vt->double_field_put(off,  *(jdouble*)loc);
-#else
-      Unimplemented();
-#endif
-      break;
-    }
-    case T_OBJECT:
-    case T_ARRAY: {
-      Handle handle = handles.at(k++);
-      new_vt->obj_field_put(off, handle());
-      break;
-    }
-    case T_FLOAT: {
-      new_vt->float_field_put(off,  *(jfloat*)loc);
-      break;
-    }
-    case T_DOUBLE: {
-      new_vt->double_field_put(off, *(jdouble*)loc);
-      break;
-    }
-    default:
-      ShouldNotReachHere();
-    }
-    *(intptr_t*)loc = 0xDEAD;
-    j++;
-  }
-  assert(j == regs->length(), "missed a field?");
-  assert(k == handles.length(), "missed an oop?");
-  return new_vt;
-}
-
-// Check the return register for a ValueKlass oop
-ValueKlass* ValueKlass::returned_value_klass(const RegisterMap& map) {
-  BasicType bt = T_METADATA;
-  VMRegPair pair;
-  int nb = SharedRuntime::java_return_convention(&bt, &pair, 1);
-  assert(nb == 1, "broken");
-
-  address loc = map.location(pair.first());
-  intptr_t ptr = *(intptr_t*)loc;
-  if (is_set_nth_bit(ptr, 0)) {
-    // Oop is tagged, must be a ValueKlass oop
-    clear_nth_bit(ptr, 0);
-    assert(Metaspace::contains((void*)ptr), "should be klass");
-    ValueKlass* vk = (ValueKlass*)ptr;
-    assert(vk->can_be_returned_as_fields(), "must be able to return as fields");
-    return vk;
-  }
-#ifdef ASSERT
-  // Oop is not tagged, must be a valid oop
-  if (VerifyOops) {
-    oopDesc::verify(oop((HeapWord*)ptr));
-  }
-#endif
-  return NULL;
-}
-
-void ValueKlass::verify_on(outputStream* st) {
-  InstanceKlass::verify_on(st);
-  guarantee(prototype_header().is_always_locked(), "Prototype header is not always locked");
-}
-
-void ValueKlass::oop_verify_on(oop obj, outputStream* st) {
-  InstanceKlass::oop_verify_on(obj, st);
-  guarantee(obj->mark().is_always_locked(), "Header is not always locked");
-}
-
-void ValueKlass::metaspace_pointers_do(MetaspaceClosure* it) {
-  InstanceKlass::metaspace_pointers_do(it);
-
-  ValueKlass* this_ptr = this;
-  it->push_internal_pointer(&this_ptr, (intptr_t*)&_adr_valueklass_fixed_block);
-}
diff a/src/hotspot/share/oops/valueKlass.hpp b/src/hotspot/share/oops/valueKlass.hpp
--- a/src/hotspot/share/oops/valueKlass.hpp
+++ /dev/null
@@ -1,314 +0,0 @@
-/*
- * Copyright (c) 2017, 2020, Oracle and/or its affiliates. All rights reserved.
- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
- *
- * This code is free software; you can redistribute it and/or modify it
- * under the terms of the GNU General Public License version 2 only, as
- * published by the Free Software Foundation.
- *
- * This code is distributed in the hope that it will be useful, but WITHOUT
- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
- * version 2 for more details (a copy is included in the LICENSE file that
- * accompanied this code).
- *
- * You should have received a copy of the GNU General Public License version
- * 2 along with this work; if not, write to the Free Software Foundation,
- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
- *
- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
- * or visit www.oracle.com if you need additional information or have any
- * questions.
- *
- */
-
-#ifndef SHARE_VM_OOPS_VALUEKLASS_HPP
-#define SHARE_VM_OOPS_VALUEKLASS_HPP
-
-#include "classfile/javaClasses.hpp"
-#include "oops/instanceKlass.hpp"
-#include "oops/method.hpp"
-//#include "oops/oop.inline.hpp"
-
-// A ValueKlass is a specialized InstanceKlass for value types.
-
-
-class ValueKlass: public InstanceKlass {
-  friend class VMStructs;
-  friend class InstanceKlass;
-
- public:
-  ValueKlass() { assert(DumpSharedSpaces || UseSharedSpaces, "only for CDS"); }
-
- private:
-
-  // Constructor
-  ValueKlass(const ClassFileParser& parser);
-
-  ValueKlassFixedBlock* valueklass_static_block() const {
-    address adr_jf = adr_inline_type_field_klasses();
-    if (adr_jf != NULL) {
-      return (ValueKlassFixedBlock*)(adr_jf + this->java_fields_count() * sizeof(Klass*));
-    }
-
-    address adr_fing = adr_fingerprint();
-    if (adr_fing != NULL) {
-      return (ValueKlassFixedBlock*)(adr_fingerprint() + sizeof(u8));
-    }
-
-    InstanceKlass** adr_host = adr_unsafe_anonymous_host();
-    if (adr_host != NULL) {
-      return (ValueKlassFixedBlock*)(adr_host + 1);
-    }
-
-    Klass* volatile* adr_impl = adr_implementor();
-    if (adr_impl != NULL) {
-      return (ValueKlassFixedBlock*)(adr_impl + 1);
-    }
-
-    return (ValueKlassFixedBlock*)end_of_nonstatic_oop_maps();
-  }
-
-  address adr_extended_sig() const {
-    assert(_adr_valueklass_fixed_block != NULL, "Should have been initialized");
-    return ((address)_adr_valueklass_fixed_block) + in_bytes(byte_offset_of(ValueKlassFixedBlock, _extended_sig));
-  }
-
-  address adr_return_regs() const {
-    ValueKlassFixedBlock* vkst = valueklass_static_block();
-    return ((address)_adr_valueklass_fixed_block) + in_bytes(byte_offset_of(ValueKlassFixedBlock, _return_regs));
-  }
-
-  // pack and unpack handlers for value types return
-  address adr_pack_handler() const {
-    assert(_adr_valueklass_fixed_block != NULL, "Should have been initialized");
-    return ((address)_adr_valueklass_fixed_block) + in_bytes(byte_offset_of(ValueKlassFixedBlock, _pack_handler));
-  }
-
-  address adr_pack_handler_jobject() const {
-    assert(_adr_valueklass_fixed_block != NULL, "Should have been initialized");
-    return ((address)_adr_valueklass_fixed_block) + in_bytes(byte_offset_of(ValueKlassFixedBlock, _pack_handler_jobject));
-  }
-
-  address adr_unpack_handler() const {
-    assert(_adr_valueklass_fixed_block != NULL, "Should have been initialized");
-    return ((address)_adr_valueklass_fixed_block) + in_bytes(byte_offset_of(ValueKlassFixedBlock, _unpack_handler));
-  }
-
-  address adr_default_value_offset() const {
-    assert(_adr_valueklass_fixed_block != NULL, "Should have been initialized");
-    return ((address)_adr_valueklass_fixed_block) + in_bytes(default_value_offset_offset());
-  }
-
-  address adr_value_array_klass() const {
-    assert(_adr_valueklass_fixed_block != NULL, "Should have been initialized");
-    return ((address)_adr_valueklass_fixed_block) + in_bytes(byte_offset_of(ValueKlassFixedBlock, _value_array_klass));
-  }
-
-  Klass* get_value_array_klass() const {
-    return *(Klass**)adr_value_array_klass();
-  }
-
-  Klass* acquire_value_array_klass() const {
-    return Atomic::load_acquire((Klass**)adr_value_array_klass());
-  }
-
-  Klass* allocate_value_array_klass(TRAPS);
-
-  address adr_alignment() const {
-    assert(_adr_valueklass_fixed_block != NULL, "Should have been initialized");
-    return ((address)_adr_valueklass_fixed_block) + in_bytes(byte_offset_of(ValueKlassFixedBlock, _alignment));
-  }
-
-  address adr_first_field_offset() const {
-    assert(_adr_valueklass_fixed_block != NULL, "Should have been initialized");
-    return ((address)_adr_valueklass_fixed_block) + in_bytes(byte_offset_of(ValueKlassFixedBlock, _first_field_offset));
-  }
-
-  address adr_exact_size_in_bytes() const {
-    assert(_adr_valueklass_fixed_block != NULL, "Should have been initialized");
-    return ((address)_adr_valueklass_fixed_block) + in_bytes(byte_offset_of(ValueKlassFixedBlock, _exact_size_in_bytes));
-  }
-
- public:
-  int get_alignment() const {
-    return *(int*)adr_alignment();
-  }
-
-  void set_alignment(int alignment) {
-    *(int*)adr_alignment() = alignment;
-  }
-
-  int first_field_offset() const {
-    int offset = *(int*)adr_first_field_offset();
-    assert(offset != 0, "Must be initialized before use");
-    return *(int*)adr_first_field_offset();
-  }
-
-  void set_first_field_offset(int offset) {
-    *(int*)adr_first_field_offset() = offset;
-  }
-
-  int get_exact_size_in_bytes() const {
-    return *(int*)adr_exact_size_in_bytes();
-  }
-
-  void set_exact_size_in_bytes(int exact_size) {
-    *(int*)adr_exact_size_in_bytes() = exact_size;
-  }
-
-  int first_field_offset_old();
-
-  virtual void remove_unshareable_info();
-  virtual void restore_unshareable_info(ClassLoaderData* loader_data, Handle protection_domain, PackageEntry* pkg_entry, TRAPS);
-  virtual void metaspace_pointers_do(MetaspaceClosure* it);
-
- private:
-  int collect_fields(GrowableArray<SigEntry>* sig, int base_off = 0);
-
-  void cleanup_blobs();
-
-
- protected:
-  // Returns the array class for the n'th dimension
-  Klass* array_klass_impl(bool or_null, int n, TRAPS);
-
-  // Returns the array class with this class as element type
-  Klass* array_klass_impl(bool or_null, TRAPS);
-
-  // Specifically flat array klass
-  Klass* value_array_klass(bool or_null, int rank, TRAPS);
-
- public:
-  // Type testing
-  bool is_inline_klass_slow() const        { return true; }
-
-  // Casting from Klass*
-  static ValueKlass* cast(Klass* k);
-
-  // Use this to return the size of an instance in heap words
-  // Implementation is currently simple because all value types are allocated
-  // in Java heap like Java objects.
-  virtual int size_helper() const {
-    return layout_helper_to_size_helper(layout_helper());
-  }
-
-  // Metadata iterators
-  void array_klasses_do(void f(Klass* k));
-  void array_klasses_do(void f(Klass* k, TRAPS), TRAPS);
-
-  // allocate_instance() allocates a stand alone value in the Java heap
-  // initialized to default value (cleared memory)
-  instanceOop allocate_instance(TRAPS);
-  // allocates a stand alone value buffer in the Java heap
-  // DOES NOT have memory cleared, user MUST initialize payload before
-  // returning to Java (i.e.: value_copy)
-  instanceOop allocate_instance_buffer(TRAPS);
-
-  // minimum number of bytes occupied by nonstatic fields, HeapWord aligned or pow2
-  int raw_value_byte_size();
-
-  address data_for_oop(oop o) const;
-  oop oop_for_data(address data) const;
-
-  // Query if this class promises atomicity one way or another
-  bool is_atomic() { return is_naturally_atomic() || is_declared_atomic(); }
-
-  bool flatten_array();
-
-  bool contains_oops() const { return nonstatic_oop_map_count() > 0; }
-  int nonstatic_oop_count();
-
-  // General store methods
-  //
-  // Normally loads and store methods would be found in *Oops classes, but since values can be
-  // "in-lined" (flattened) into containing oops, these methods reside here in ValueKlass.
-  //
-  // "value_copy_*_to_new_*" assume new memory (i.e. IS_DEST_UNINITIALIZED for write barriers)
-
-  void value_copy_payload_to_new_oop(void* src, oop dst);
-  void value_copy_oop_to_new_oop(oop src, oop dst);
-  void value_copy_oop_to_new_payload(oop src, void* dst);
-  void value_copy_oop_to_payload(oop src, void* dst);
-
-  oop read_inlined_field(oop obj, int offset, TRAPS);
-  void write_inlined_field(oop obj, int offset, oop value, TRAPS);
-
-  // oop iterate raw value type data pointer (where oop_addr may not be an oop, but backing/array-element)
-  template <typename T, class OopClosureType>
-  inline void oop_iterate_specialized(const address oop_addr, OopClosureType* closure);
-
-  template <typename T, class OopClosureType>
-  inline void oop_iterate_specialized_bounded(const address oop_addr, OopClosureType* closure, void* lo, void* hi);
-
-  // calling convention support
-  void initialize_calling_convention(TRAPS);
-  Array<SigEntry>* extended_sig() const {
-    return *((Array<SigEntry>**)adr_extended_sig());
-  }
-  Array<VMRegPair>* return_regs() const {
-    return *((Array<VMRegPair>**)adr_return_regs());
-  }
-  bool is_scalarizable() const;
-  bool can_be_passed_as_fields() const;
-  bool can_be_returned_as_fields(bool init = false) const;
-  void save_oop_fields(const RegisterMap& map, GrowableArray<Handle>& handles) const;
-  void restore_oop_results(RegisterMap& map, GrowableArray<Handle>& handles) const;
-  oop realloc_result(const RegisterMap& reg_map, const GrowableArray<Handle>& handles, TRAPS);
-  static ValueKlass* returned_value_klass(const RegisterMap& reg_map);
-
-  address pack_handler() const {
-    return *(address*)adr_pack_handler();
-  }
-
-  address unpack_handler() const {
-    return *(address*)adr_unpack_handler();
-  }
-
-  // pack and unpack handlers. Need to be loadable from generated code
-  // so at a fixed offset from the base of the klass pointer.
-  static ByteSize pack_handler_offset() {
-    return byte_offset_of(ValueKlassFixedBlock, _pack_handler);
-  }
-
-  static ByteSize pack_handler_jobject_offset() {
-    return byte_offset_of(ValueKlassFixedBlock, _pack_handler_jobject);
-  }
-
-  static ByteSize unpack_handler_offset() {
-    return byte_offset_of(ValueKlassFixedBlock, _unpack_handler);
-  }
-
-  static ByteSize default_value_offset_offset() {
-    return byte_offset_of(ValueKlassFixedBlock, _default_value_offset);
-  }
-
-  static ByteSize first_field_offset_offset() {
-    return byte_offset_of(ValueKlassFixedBlock, _first_field_offset);
-  }
-
-  void set_default_value_offset(int offset) {
-    *((int*)adr_default_value_offset()) = offset;
-  }
-
-  int default_value_offset() {
-    int offset = *((int*)adr_default_value_offset());
-    assert(offset != 0, "must not be called if not initialized");
-    return offset;
-  }
-
-  void set_default_value(oop val) {
-    java_mirror()->obj_field_put(default_value_offset(), val);
-  }
-
-  oop default_value();
-  void deallocate_contents(ClassLoaderData* loader_data);
-  static void cleanup(ValueKlass* ik) ;
-
-  // Verification
-  void verify_on(outputStream* st);
-  void oop_verify_on(oop obj, outputStream* st);
-
-};
-
-#endif /* SHARE_VM_OOPS_VALUEKLASS_HPP */
diff a/src/hotspot/share/oops/valueKlass.inline.hpp b/src/hotspot/share/oops/valueKlass.inline.hpp
--- a/src/hotspot/share/oops/valueKlass.inline.hpp
+++ /dev/null
@@ -1,104 +0,0 @@
-/*
- * Copyright (c) 2017, Oracle and/or its affiliates. All rights reserved.
- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
- *
- * This code is free software; you can redistribute it and/or modify it
- * under the terms of the GNU General Public License version 2 only, as
- * published by the Free Software Foundation.
- *
- * This code is distributed in the hope that it will be useful, but WITHOUT
- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
- * version 2 for more details (a copy is included in the LICENSE file that
- * accompanied this code).
- *
- * You should have received a copy of the GNU General Public License version
- * 2 along with this work; if not, write to the Free Software Foundation,
- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
- *
- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
- * or visit www.oracle.com if you need additional information or have any
- * questions.
- *
- */
-#ifndef SHARE_VM_OOPS_VALUEKLASS_INLINE_HPP
-#define SHARE_VM_OOPS_VALUEKLASS_INLINE_HPP
-
-#include "memory/iterator.hpp"
-#include "oops/klass.hpp"
-#include "oops/valueArrayKlass.hpp"
-#include "oops/oop.inline.hpp"
-#include "oops/valueKlass.hpp"
-#include "utilities/macros.hpp"
-
-inline ValueKlass* ValueKlass::cast(Klass* k) {
-  assert(k->is_inline_klass(), "cast to ValueKlass");
-  return (ValueKlass*) k;
-}
-
-inline address ValueKlass::data_for_oop(oop o) const {
-  return ((address) (void*) o) + first_field_offset();
-}
-
-inline oop ValueKlass::oop_for_data(address data) const {
-  oop o = (oop) (data - first_field_offset());
-  assert(oopDesc::is_oop(o, false), "Not an oop");
-  return o;
-}
-
-inline void ValueKlass::value_copy_payload_to_new_oop(void* src, oop dst) {
-  HeapAccess<IS_DEST_UNINITIALIZED>::value_copy(src, data_for_oop(dst), this);
-}
-
-inline void ValueKlass::value_copy_oop_to_new_oop(oop src, oop dst) {
-  HeapAccess<IS_DEST_UNINITIALIZED>::value_copy(data_for_oop(src), data_for_oop(dst), this);
-}
-
-inline void ValueKlass::value_copy_oop_to_new_payload(oop src, void* dst) {
-  HeapAccess<IS_DEST_UNINITIALIZED>::value_copy(data_for_oop(src), dst, this);
-}
-
-inline void ValueKlass::value_copy_oop_to_payload(oop src, void* dst) {
-  HeapAccess<>::value_copy(data_for_oop(src), dst, this);
-}
-
-
-template <typename T, class OopClosureType>
-void ValueKlass::oop_iterate_specialized(const address oop_addr, OopClosureType* closure) {
-  OopMapBlock* map = start_of_nonstatic_oop_maps();
-  OopMapBlock* const end_map = map + nonstatic_oop_map_count();
-
-  for (; map < end_map; map++) {
-    T* p = (T*) (oop_addr + map->offset());
-    T* const end = p + map->count();
-    for (; p < end; ++p) {
-      Devirtualizer::do_oop(closure, p);
-    }
-  }
-}
-
-template <typename T, class OopClosureType>
-inline void ValueKlass::oop_iterate_specialized_bounded(const address oop_addr, OopClosureType* closure, void* lo, void* hi) {
-  OopMapBlock* map = start_of_nonstatic_oop_maps();
-  OopMapBlock* const end_map = map + nonstatic_oop_map_count();
-
-  T* const l   = (T*) lo;
-  T* const h   = (T*) hi;
-
-  for (; map < end_map; map++) {
-    T* p = (T*) (oop_addr + map->offset());
-    T* end = p + map->count();
-    if (p < l) {
-      p = l;
-    }
-    if (end > h) {
-      end = h;
-    }
-    for (; p < end; ++p) {
-      Devirtualizer::do_oop(closure, p);
-    }
-  }
-}
-
-
-#endif // SHARE_VM_OOPS_VALUEKLASS_INLINE_HPP
diff a/src/hotspot/share/opto/escape.cpp b/src/hotspot/share/opto/escape.cpp
--- a/src/hotspot/share/opto/escape.cpp
+++ b/src/hotspot/share/opto/escape.cpp
@@ -3241,16 +3241,16 @@
           // EncodeISOArray overwrites destination array
           memnode_worklist.append_if_missing(use);
         }
       } else if (use->Opcode() == Op_Return) {
         assert(_compile->tf()->returns_value_type_as_fields(), "must return a value type");
-        // Get ValueKlass by removing the tag bit from the metadata pointer
+        // Get InlineKlass by removing the tag bit from the metadata pointer
         Node* klass = use->in(TypeFunc::Parms);
         intptr_t ptr = igvn->type(klass)->isa_rawptr()->get_con();
         clear_nth_bit(ptr, 0);
         assert(Metaspace::contains((void*)ptr), "should be klass");
-        assert(((ValueKlass*)ptr)->contains_oops(), "returned value type must contain a reference field");
+        assert(((InlineKlass*)ptr)->contains_oops(), "returned inline type must contain a reference field");
       } else {
         uint op = use->Opcode();
         if ((op == Op_StrCompressedCopy || op == Op_StrInflatedCopy) &&
             (use->in(MemNode::Memory) == n)) {
           // They overwrite memory edge corresponding to destination array,
diff a/src/hotspot/share/opto/graphKit.cpp b/src/hotspot/share/opto/graphKit.cpp
--- a/src/hotspot/share/opto/graphKit.cpp
+++ b/src/hotspot/share/opto/graphKit.cpp
@@ -4237,13 +4237,13 @@
 
     // Null-free, non-flattened value array, initialize with the default value
     set_control(_gvn.transform(new IfTrueNode(iff)));
     Node* p = basic_plus_adr(klass_node, in_bytes(ArrayKlass::element_klass_offset()));
     Node* eklass = _gvn.transform(LoadKlassNode::make(_gvn, control(), immutable_memory(), p, TypeInstPtr::KLASS));
-    Node* adr_fixed_block_addr = basic_plus_adr(eklass, in_bytes(InstanceKlass::adr_valueklass_fixed_block_offset()));
+    Node* adr_fixed_block_addr = basic_plus_adr(eklass, in_bytes(InstanceKlass::adr_inlineklass_fixed_block_offset()));
     Node* adr_fixed_block = make_load(control(), adr_fixed_block_addr, TypeRawPtr::NOTNULL, T_ADDRESS, MemNode::unordered);
-    Node* default_value_offset_addr = basic_plus_adr(adr_fixed_block, in_bytes(ValueKlass::default_value_offset_offset()));
+    Node* default_value_offset_addr = basic_plus_adr(adr_fixed_block, in_bytes(InlineKlass::default_value_offset_offset()));
     Node* default_value_offset = make_load(control(), default_value_offset_addr, TypeInt::INT, T_INT, MemNode::unordered);
     Node* elem_mirror = load_mirror_from_klass(eklass);
     Node* default_value_addr = basic_plus_adr(elem_mirror, ConvI2X(default_value_offset));
     Node* val = access_load_at(elem_mirror, default_value_addr, _gvn.type(default_value_addr)->is_ptr(), TypeInstPtr::BOTTOM, T_OBJECT, IN_HEAP);
     r->init_req(3, control());
diff a/src/hotspot/share/opto/macro.cpp b/src/hotspot/share/opto/macro.cpp
--- a/src/hotspot/share/opto/macro.cpp
+++ b/src/hotspot/share/opto/macro.cpp
@@ -2768,12 +2768,12 @@
   rawmem = make_store(slowpath_false, rawmem, old_top, oopDesc::mark_offset_in_bytes(), mark_node, T_ADDRESS);
   rawmem = make_store(slowpath_false, rawmem, old_top, oopDesc::klass_offset_in_bytes(), klass_node, T_METADATA);
   if (UseCompressedClassPointers) {
     rawmem = make_store(slowpath_false, rawmem, old_top, oopDesc::klass_gap_offset_in_bytes(), intcon(0), T_INT);
   }
-  Node* fixed_block  = make_load(slowpath_false, rawmem, klass_node, in_bytes(InstanceKlass::adr_valueklass_fixed_block_offset()), TypeRawPtr::BOTTOM, T_ADDRESS);
-  Node* pack_handler = make_load(slowpath_false, rawmem, fixed_block, in_bytes(ValueKlass::pack_handler_offset()), TypeRawPtr::BOTTOM, T_ADDRESS);
+  Node* fixed_block  = make_load(slowpath_false, rawmem, klass_node, in_bytes(InstanceKlass::adr_inlineklass_fixed_block_offset()), TypeRawPtr::BOTTOM, T_ADDRESS);
+  Node* pack_handler = make_load(slowpath_false, rawmem, fixed_block, in_bytes(InlineKlass::pack_handler_offset()), TypeRawPtr::BOTTOM, T_ADDRESS);
 
   CallLeafNoFPNode* handler_call = new CallLeafNoFPNode(OptoRuntime::pack_value_type_Type(),
                                                         NULL,
                                                         "pack handler",
                                                         TypeRawPtr::BOTTOM);
diff a/src/hotspot/share/opto/memnode.cpp b/src/hotspot/share/opto/memnode.cpp
--- a/src/hotspot/share/opto/memnode.cpp
+++ b/src/hotspot/share/opto/memnode.cpp
@@ -1930,18 +1930,18 @@
           assert(Opcode() == Op_LoadP, "must load an oop from _java_mirror");
           return TypeInstPtr::make(klass->java_mirror());
         }
       }
     } else {
-      // Check for a load of the default value offset from the ValueKlassFixedBlock:
-      // LoadI(LoadP(value_klass, adr_valueklass_fixed_block_offset), default_value_offset_offset)
+      // Check for a load of the default value offset from the InlineKlassFixedBlock:
+      // LoadI(LoadP(value_klass, adr_inlineklass_fixed_block_offset), default_value_offset_offset)
       intptr_t offset = 0;
       Node* base = AddPNode::Ideal_base_and_offset(adr, phase, offset);
-      if (base != NULL && base->is_Load() && offset == in_bytes(ValueKlass::default_value_offset_offset())) {
+      if (base != NULL && base->is_Load() && offset == in_bytes(InlineKlass::default_value_offset_offset())) {
         const TypeKlassPtr* tkls = phase->type(base->in(MemNode::Address))->isa_klassptr();
         if (tkls != NULL && tkls->is_loaded() && tkls->klass_is_exact() && tkls->isa_valuetype() &&
-            tkls->offset() == in_bytes(InstanceKlass::adr_valueklass_fixed_block_offset())) {
+            tkls->offset() == in_bytes(InstanceKlass::adr_inlineklass_fixed_block_offset())) {
           assert(base->Opcode() == Op_LoadP, "must load an oop from klass");
           assert(Opcode() == Op_LoadI, "must load an int from fixed block");
           return TypeInt::make(tkls->klass()->as_value_klass()->default_value_offset());
         }
       }
diff a/src/hotspot/share/prims/jni.cpp b/src/hotspot/share/prims/jni.cpp
--- a/src/hotspot/share/prims/jni.cpp
+++ b/src/hotspot/share/prims/jni.cpp
@@ -58,11 +58,11 @@
 #include "oops/oop.inline.hpp"
 #include "oops/symbol.hpp"
 #include "oops/typeArrayKlass.hpp"
 #include "oops/typeArrayOop.inline.hpp"
 #include "oops/valueArrayOop.inline.hpp"
-#include "oops/valueKlass.inline.hpp"
+#include "oops/inlineKlass.inline.hpp"
 #include "prims/jniCheck.hpp"
 #include "prims/jniExport.hpp"
 #include "prims/jniFastGetField.hpp"
 #include "prims/jvm_misc.hpp"
 #include "prims/jvmtiExport.hpp"
@@ -1965,11 +1965,11 @@
     assert(k->is_instance_klass(), "Only instance can have inlined fields");
     InstanceKlass* ik = InstanceKlass::cast(k);
     fieldDescriptor fd;
     ik->find_field_from_offset(offset, false, &fd);  // performance bottleneck
     InstanceKlass* holder = fd.field_holder();
-    ValueKlass* field_vklass = ValueKlass::cast(holder->get_inline_type_field_klass(fd.index()));
+    InlineKlass* field_vklass = InlineKlass::cast(holder->get_inline_type_field_klass(fd.index()));
     res = field_vklass->read_inlined_field(o, ik->field_offset(fd.index()), CHECK_NULL);
   }
   jobject ret = JNIHandles::make_local(env, res);
   HOTSPOT_JNI_GETOBJECTFIELD_RETURN(ret);
   return ret;
@@ -2074,11 +2074,11 @@
     assert(k->is_instance_klass(), "Only instances can have inlined fields");
     InstanceKlass* ik = InstanceKlass::cast(k);
     fieldDescriptor fd;
     ik->find_field_from_offset(offset, false, &fd);
     InstanceKlass* holder = fd.field_holder();
-    ValueKlass* vklass = ValueKlass::cast(holder->get_inline_type_field_klass(fd.index()));
+    InlineKlass* vklass = InlineKlass::cast(holder->get_inline_type_field_klass(fd.index()));
     oop v = JNIHandles::resolve_non_null(value);
     vklass->write_inlined_field(o, offset, v, CHECK);
   }
   HOTSPOT_JNI_SETOBJECTFIELD_RETURN();
 JNI_END
@@ -2555,11 +2555,11 @@
    if (arr->is_within_bounds(index)) {
      if (arr->is_valueArray()) {
        valueArrayOop a = valueArrayOop(JNIHandles::resolve_non_null(array));
        oop v = JNIHandles::resolve(value);
        ValueArrayKlass* vaklass = ValueArrayKlass::cast(a->klass());
-       ValueKlass* element_vklass = vaklass->element_klass();
+       InlineKlass* element_vklass = vaklass->element_klass();
        if (v != NULL && v->is_a(element_vklass)) {
          a->value_copy_to_index(v, index);
        } else {
          ResourceMark rm(THREAD);
          stringStream ss;
@@ -3435,11 +3435,11 @@
   }
   if (!a->is_valueArray()) {
     THROW_MSG_NULL(vmSymbols::java_lang_IllegalArgumentException(), "Not a flattened array");
   }
   ValueArrayKlass* vak = ValueArrayKlass::cast(a->klass());
-  ValueKlass* vk = vak->element_klass();
+  InlineKlass* vk = vak->element_klass();
   return (jclass) JNIHandles::make_local(vk->java_mirror());
 JNI_END
 
 JNI_ENTRY(jsize, jni_GetFieldOffsetInFlattenedLayout(JNIEnv* env, jclass clazz, const char *name, const char *signature, jboolean* is_inlined))
   JNIWrapper("jni_GetFieldOffsetInFlattenedLayout");
@@ -3448,11 +3448,11 @@
   Klass* k = java_lang_Class::as_Klass(mirror);
   if (!k->is_inline_klass()) {
     ResourceMark rm;
         THROW_MSG_0(vmSymbols::java_lang_IllegalArgumentException(), err_msg("%s has not flattened layout", k->external_name()));
   }
-  ValueKlass* vk = ValueKlass::cast(k);
+  InlineKlass* vk = InlineKlass::cast(k);
 
   TempNewSymbol fieldname = SymbolTable::probe(name, (int)strlen(name));
   TempNewSymbol signame = SymbolTable::probe(signature, (int)strlen(signature));
   if (fieldname == NULL || signame == NULL) {
     ResourceMark rm;
@@ -3515,11 +3515,11 @@
   Klass* k = java_lang_Class::as_Klass(semirror);
   if (!k->is_inline_klass()) {
     ResourceMark rm;
         THROW_MSG_0(vmSymbols::java_lang_IllegalArgumentException(), err_msg("%s is not an inline type", k->external_name()));
   }
-  ValueKlass* vk = ValueKlass::cast(k);
+  InlineKlass* vk = InlineKlass::cast(k);
   assert(vk->is_initialized(), "If a flattened array has been created, the element klass must have been initialized");
   int field_offset = jfieldIDWorkaround::from_instance_jfieldID(vk, fieldID);
   fieldDescriptor fd;
   if (!vk->find_field_from_offset(field_offset, false, &fd)) {
     THROW_NULL(vmSymbols::java_lang_NoSuchFieldError());
@@ -3559,17 +3559,17 @@
   if (!jdk_internal_vm_jni_SubElementSelector::getIsInlined(slct)) {
     int offset = (address)ar->base() - cast_from_oop<address>(ar) + index * vak->element_byte_size()
                       + jdk_internal_vm_jni_SubElementSelector::getOffset(slct);
     res = HeapAccess<ON_UNKNOWN_OOP_REF>::oop_load_at(ar, offset);
   } else {
-    ValueKlass* fieldKlass = ValueKlass::cast(java_lang_Class::as_Klass(jdk_internal_vm_jni_SubElementSelector::getSubElementType(slct)));
+    InlineKlass* fieldKlass = InlineKlass::cast(java_lang_Class::as_Klass(jdk_internal_vm_jni_SubElementSelector::getSubElementType(slct)));
     res = fieldKlass->allocate_instance(CHECK_NULL);
     // The array might have been moved by the GC, refreshing the arrayOop
     ar =  (valueArrayOop)JNIHandles::resolve_non_null(array);
     address addr = (address)ar->value_at_addr(index, vak->layout_helper())
               + jdk_internal_vm_jni_SubElementSelector::getOffset(slct);
-    fieldKlass->value_copy_payload_to_new_oop(addr, res);
+    fieldKlass->inline_copy_payload_to_new_oop(addr, res);
   }
   return JNIHandles::make_local(res);
 JNI_END
 
 JNI_ENTRY(void, jni_SetObjectSubElement(JNIEnv* env, jarray array, jobject selector, int index, jobject value))
@@ -3594,14 +3594,14 @@
   if (!jdk_internal_vm_jni_SubElementSelector::getIsInlined(slct)) {
     int offset = (address)ar->base() - cast_from_oop<address>(ar) + index * vak->element_byte_size()
                   + jdk_internal_vm_jni_SubElementSelector::getOffset(slct);
     HeapAccess<ON_UNKNOWN_OOP_REF>::oop_store_at(ar, offset, JNIHandles::resolve(value));
   } else {
-    ValueKlass* fieldKlass = ValueKlass::cast(java_lang_Class::as_Klass(jdk_internal_vm_jni_SubElementSelector::getSubElementType(slct)));
+    InlineKlass* fieldKlass = InlineKlass::cast(java_lang_Class::as_Klass(jdk_internal_vm_jni_SubElementSelector::getSubElementType(slct)));
     address addr = (address)ar->value_at_addr(index, vak->layout_helper())
                   + jdk_internal_vm_jni_SubElementSelector::getOffset(slct);
-    fieldKlass->value_copy_oop_to_payload(JNIHandles::resolve_non_null(value), addr);
+    fieldKlass->inline_copy_oop_to_payload(JNIHandles::resolve_non_null(value), addr);
   }
 JNI_END
 
 #define DEFINE_GETSUBELEMENT(ElementType,Result,ElementBasicType) \
 \
diff a/src/hotspot/share/prims/jvmtiGetLoadedClasses.cpp b/src/hotspot/share/prims/jvmtiGetLoadedClasses.cpp
--- a/src/hotspot/share/prims/jvmtiGetLoadedClasses.cpp
+++ b/src/hotspot/share/prims/jvmtiGetLoadedClasses.cpp
@@ -20,11 +20,11 @@
  * or visit www.oracle.com if you need additional information or have any
  * questions.
  *
  */
 
-#include <oops/valueKlass.hpp>
+#include <oops/inlineKlass.hpp>
 #include "precompiled.hpp"
 #include "classfile/classLoaderDataGraph.hpp"
 #include "classfile/dictionary.hpp"
 #include "classfile/systemDictionary.hpp"
 #include "gc/shared/collectedHeap.hpp"
@@ -76,11 +76,11 @@
       // Collect array classes this way when walking the dictionary (because array classes are
       // not in the dictionary).
       for (Klass* l = k->array_klass_or_null(); l != NULL; l = l->array_klass_or_null()) {
         _classStack.push((jclass) _env->jni_reference(Handle(_cur_thread, l->java_mirror())));
       }
-      // CMH flat arrays (ValueKlass)
+      // CMH flat arrays (InlineKlass)
     }
   }
 
   jvmtiError get_result(JvmtiEnv *env, jint* classCountPtr, jclass** classesPtr) {
     // Return results by extracting the collected contents into a list
diff a/src/hotspot/share/prims/unsafe.cpp b/src/hotspot/share/prims/unsafe.cpp
--- a/src/hotspot/share/prims/unsafe.cpp
+++ b/src/hotspot/share/prims/unsafe.cpp
@@ -40,11 +40,11 @@
 #include "oops/objArrayOop.inline.hpp"
 #include "oops/oop.inline.hpp"
 #include "oops/typeArrayOop.inline.hpp"
 #include "oops/valueArrayKlass.hpp"
 #include "oops/valueArrayOop.inline.hpp"
-#include "oops/valueKlass.inline.hpp"
+#include "oops/inlineKlass.inline.hpp"
 #include "prims/unsafe.hpp"
 #include "runtime/fieldDescriptor.inline.hpp"
 #include "runtime/globals.hpp"
 #include "runtime/handles.inline.hpp"
 #include "runtime/interfaceSupport.inline.hpp"
@@ -283,11 +283,11 @@
   }
   return found;
 }
 #endif // ASSERT
 
-static void assert_and_log_unsafe_value_access(oop p, jlong offset, ValueKlass* vk) {
+static void assert_and_log_unsafe_value_access(oop p, jlong offset, InlineKlass* vk) {
   Klass* k = p->klass();
 #ifdef ASSERT
   if (k->is_instance_klass()) {
     assert_field_offset_sane(p, offset);
     fieldDescriptor fd;
@@ -343,53 +343,53 @@
   HeapAccess<ON_UNKNOWN_OOP_REF>::oop_store_at(p, offset, x);
 } UNSAFE_END
 
 UNSAFE_ENTRY(jlong, Unsafe_ValueHeaderSize(JNIEnv *env, jobject unsafe, jclass c)) {
   Klass* k = java_lang_Class::as_Klass(JNIHandles::resolve_non_null(c));
-  ValueKlass* vk = ValueKlass::cast(k);
+  InlineKlass* vk = InlineKlass::cast(k);
   return vk->first_field_offset();
 } UNSAFE_END
 
 UNSAFE_ENTRY(jboolean, Unsafe_IsFlattenedArray(JNIEnv *env, jobject unsafe, jclass c)) {
   Klass* k = java_lang_Class::as_Klass(JNIHandles::resolve_non_null(c));
   return k->is_valueArray_klass();
 } UNSAFE_END
 
 UNSAFE_ENTRY(jobject, Unsafe_UninitializedDefaultValue(JNIEnv *env, jobject unsafe, jclass vc)) {
   Klass* k = java_lang_Class::as_Klass(JNIHandles::resolve_non_null(vc));
-  ValueKlass* vk = ValueKlass::cast(k);
+  InlineKlass* vk = InlineKlass::cast(k);
   oop v = vk->default_value();
   return JNIHandles::make_local(env, v);
 } UNSAFE_END
 
 UNSAFE_ENTRY(jobject, Unsafe_GetValue(JNIEnv *env, jobject unsafe, jobject obj, jlong offset, jclass vc)) {
   oop base = JNIHandles::resolve(obj);
   Klass* k = java_lang_Class::as_Klass(JNIHandles::resolve_non_null(vc));
-  ValueKlass* vk = ValueKlass::cast(k);
+  InlineKlass* vk = InlineKlass::cast(k);
   assert_and_log_unsafe_value_access(base, offset, vk);
   Handle base_h(THREAD, base);
   oop v = vk->read_inlined_field(base_h(), offset, CHECK_NULL);
   return JNIHandles::make_local(env, v);
 } UNSAFE_END
 
 UNSAFE_ENTRY(void, Unsafe_PutValue(JNIEnv *env, jobject unsafe, jobject obj, jlong offset, jclass vc, jobject value)) {
   oop base = JNIHandles::resolve(obj);
   Klass* k = java_lang_Class::as_Klass(JNIHandles::resolve_non_null(vc));
-  ValueKlass* vk = ValueKlass::cast(k);
+  InlineKlass* vk = InlineKlass::cast(k);
   assert(!base->is_inline_type() || base->mark().is_larval_state(), "must be an object instance or a larval inline type");
   assert_and_log_unsafe_value_access(base, offset, vk);
   oop v = JNIHandles::resolve(value);
   vk->write_inlined_field(base, offset, v, CHECK);
 } UNSAFE_END
 
 UNSAFE_ENTRY(jobject, Unsafe_MakePrivateBuffer(JNIEnv *env, jobject unsafe, jobject value)) {
   oop v = JNIHandles::resolve_non_null(value);
   assert(v->is_inline_type(), "must be an inline type instance");
   Handle vh(THREAD, v);
-  ValueKlass* vk = ValueKlass::cast(v->klass());
+  InlineKlass* vk = InlineKlass::cast(v->klass());
   instanceOop new_value = vk->allocate_instance(CHECK_NULL);
-  vk->value_copy_oop_to_new_oop(vh(),  new_value);
+  vk->inline_copy_oop_to_new_oop(vh(),  new_value);
   markWord mark = new_value->mark();
   new_value->set_mark(mark.enter_larval_state());
   return JNIHandles::make_local(env, new_value);
 } UNSAFE_END
 
@@ -737,11 +737,11 @@
     base  = tak->array_header_in_bytes();
     assert(base == arrayOopDesc::base_offset_in_bytes(tak->element_type()), "array_header_size semantics ok");
     scale = (1 << tak->log2_element_size());
   } else if (k->is_valueArray_klass()) {
     ValueArrayKlass* vak = ValueArrayKlass::cast(k);
-    ValueKlass* vklass = vak->element_klass();
+    InlineKlass* vklass = vak->element_klass();
     base = vak->array_header_in_bytes();
     scale = vak->element_byte_size();
   } else {
     ShouldNotReachHere();
   }
diff a/src/hotspot/share/runtime/deoptimization.cpp b/src/hotspot/share/runtime/deoptimization.cpp
--- a/src/hotspot/share/runtime/deoptimization.cpp
+++ b/src/hotspot/share/runtime/deoptimization.cpp
@@ -1,7 +1,4 @@
-
-
 /*
  * Copyright (c) 1997, 2020, Oracle and/or its affiliates. All rights reserved.
  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  *
  * This code is free software; you can redistribute it and/or modify it
@@ -49,11 +47,11 @@
 #include "oops/oop.inline.hpp"
 #include "oops/fieldStreams.inline.hpp"
 #include "oops/typeArrayOop.inline.hpp"
 #include "oops/valueArrayKlass.hpp"
 #include "oops/valueArrayOop.hpp"
-#include "oops/valueKlass.inline.hpp"
+#include "oops/inlineKlass.inline.hpp"
 #include "oops/verifyOopClosure.hpp"
 #include "prims/jvmtiThreadState.hpp"
 #include "runtime/atomic.hpp"
 #include "runtime/biasedLocking.hpp"
 #include "runtime/deoptimization.hpp"
@@ -188,13 +186,13 @@
   ScopeDesc* scope = chunk->at(0)->scope();
   bool save_oop_result = scope->return_oop() && !thread->popframe_forcing_deopt_reexecution() && (exec_mode == Deoptimization::Unpack_deopt);
   // In case of the return of multiple values, we must take care
   // of all oop return values.
   GrowableArray<Handle> return_oops;
-  ValueKlass* vk = NULL;
+  InlineKlass* vk = NULL;
   if (save_oop_result && scope->return_vt()) {
-    vk = ValueKlass::returned_value_klass(map);
+    vk = InlineKlass::returned_inline_klass(map);
     if (vk != NULL) {
       vk->save_oop_fields(map, return_oops);
       save_oop_result = false;
     }
   }
@@ -212,11 +210,11 @@
   }
   if (objects != NULL || vk != NULL) {
     bool skip_internal = (compiled_method != NULL) && !compiled_method->is_compiled_by_jvmci();
     JRT_BLOCK
       if (vk != NULL) {
-        realloc_failures = Deoptimization::realloc_value_type_result(vk, map, return_oops, THREAD);
+        realloc_failures = Deoptimization::realloc_inline_type_result(vk, map, return_oops, THREAD);
       }
       if (objects != NULL) {
         realloc_failures = realloc_failures || Deoptimization::realloc_objects(thread, &deoptee, &map, objects, THREAD);
         Deoptimization::reassign_fields(&deoptee, &map, objects, realloc_failures, skip_internal, THREAD);
       }
@@ -1067,15 +1065,15 @@
   }
 
   return failures;
 }
 
-// We're deoptimizing at the return of a call, value type fields are
+// We're deoptimizing at the return of a call, inline type fields are
 // in registers. When we go back to the interpreter, it will expect a
-// reference to a value type instance. Allocate and initialize it from
+// reference to an inline type instance. Allocate and initialize it from
 // the register values here.
-bool Deoptimization::realloc_value_type_result(ValueKlass* vk, const RegisterMap& map, GrowableArray<Handle>& return_oops, TRAPS) {
+bool Deoptimization::realloc_inline_type_result(InlineKlass* vk, const RegisterMap& map, GrowableArray<Handle>& return_oops, TRAPS) {
   oop new_vt = vk->realloc_result(map, return_oops, THREAD);
   if (new_vt == NULL) {
     CLEAR_PENDING_EXCEPTION;
     THROW_OOP_(Universe::out_of_memory_error_realloc_objects(), true);
   }
@@ -1285,11 +1283,11 @@
           field._type = T_OBJECT;
         }
         if (fs.is_inlined()) {
           // Resolve klass of flattened value type field
           Klass* vk = klass->get_inline_type_field_klass(fs.index());
-          field._klass = ValueKlass::cast(vk);
+          field._klass = InlineKlass::cast(vk);
           field._type = T_INLINE_TYPE;
         }
         fields->append(field);
       }
     }
@@ -1311,11 +1309,11 @@
 
       case T_INLINE_TYPE: {
         // Recursively re-assign flattened value type fields
         InstanceKlass* vk = fields->at(i)._klass;
         assert(vk != NULL, "must be resolved");
-        offset -= ValueKlass::cast(vk)->first_field_offset(); // Adjust offset to omit oop header
+        offset -= InlineKlass::cast(vk)->first_field_offset(); // Adjust offset to omit oop header
         svIndex = reassign_fields_by_klass(vk, fr, reg_map, sv, svIndex, obj, skip_internal, offset, CHECK_0);
         continue; // Continue because we don't need to increment svIndex
       }
 
       // Have to cast to INT (32 bits) pointer to avoid little/big-endian problem.
@@ -1391,17 +1389,17 @@
     svIndex++;
   }
   return svIndex;
 }
 
-// restore fields of an eliminated value type array
+// restore fields of an eliminated inline type array
 void Deoptimization::reassign_value_array_elements(frame* fr, RegisterMap* reg_map, ObjectValue* sv, valueArrayOop obj, ValueArrayKlass* vak, TRAPS) {
-  ValueKlass* vk = vak->element_klass();
-  assert(vk->flatten_array(), "should only be used for flattened value type arrays");
+  InlineKlass* vk = vak->element_klass();
+  assert(vk->flatten_array(), "should only be used for flattened inline type arrays");
   // Adjust offset to omit oop header
-  int base_offset = arrayOopDesc::base_offset_in_bytes(T_INLINE_TYPE) - ValueKlass::cast(vk)->first_field_offset();
-  // Initialize all elements of the flattened value type array
+  int base_offset = arrayOopDesc::base_offset_in_bytes(T_INLINE_TYPE) - InlineKlass::cast(vk)->first_field_offset();
+  // Initialize all elements of the flattened inline type array
   for (int i = 0; i < sv->field_size(); i++) {
     ScopeValue* val = sv->field_at(i);
     int offset = base_offset + (i << Klass::layout_helper_log2_element_size(vak->layout_helper()));
     reassign_fields_by_klass(vk, fr, reg_map, val->as_ObjectValue(), 0, (oop)obj, false /* skip_internal */, offset, CHECK);
   }
diff a/src/hotspot/share/runtime/deoptimization.hpp b/src/hotspot/share/runtime/deoptimization.hpp
--- a/src/hotspot/share/runtime/deoptimization.hpp
+++ b/src/hotspot/share/runtime/deoptimization.hpp
@@ -168,11 +168,11 @@
 #if COMPILER2_OR_JVMCI
  public:
 
   // Support for restoring non-escaping objects
   static bool realloc_objects(JavaThread* thread, frame* fr, RegisterMap* reg_map, GrowableArray<ScopeValue*>* objects, TRAPS);
-  static bool realloc_value_type_result(ValueKlass* vk, const RegisterMap& map, GrowableArray<Handle>& return_oops, TRAPS);
+  static bool realloc_inline_type_result(InlineKlass* vk, const RegisterMap& map, GrowableArray<Handle>& return_oops, TRAPS);
   static void reassign_type_array_elements(frame* fr, RegisterMap* reg_map, ObjectValue* sv, typeArrayOop obj, BasicType type);
   static void reassign_object_array_elements(frame* fr, RegisterMap* reg_map, ObjectValue* sv, objArrayOop obj);
   static void reassign_value_array_elements(frame* fr, RegisterMap* reg_map, ObjectValue* sv, valueArrayOop obj, ValueArrayKlass* vak, TRAPS);
   static void reassign_fields(frame* fr, RegisterMap* reg_map, GrowableArray<ScopeValue*>* objects, bool realloc_failures, bool skip_internal, TRAPS);
   static void relock_objects(GrowableArray<MonitorInfo*>* monitors, JavaThread* thread, bool realloc_failures);
diff a/src/hotspot/share/runtime/fieldDescriptor.cpp b/src/hotspot/share/runtime/fieldDescriptor.cpp
--- a/src/hotspot/share/runtime/fieldDescriptor.cpp
+++ b/src/hotspot/share/runtime/fieldDescriptor.cpp
@@ -29,11 +29,11 @@
 #include "oops/annotations.hpp"
 #include "oops/constantPool.hpp"
 #include "oops/instanceKlass.hpp"
 #include "oops/oop.inline.hpp"
 #include "oops/fieldStreams.inline.hpp"
-#include "oops/valueKlass.inline.hpp"
+#include "oops/inlineKlass.inline.hpp"
 #include "runtime/fieldDescriptor.inline.hpp"
 #include "runtime/handles.inline.hpp"
 #include "runtime/signature.hpp"
 
 
@@ -191,11 +191,11 @@
       st->print(" %s", obj->bool_field(offset()) ? "true" : "false");
       break;
     case T_INLINE_TYPE:
       if (is_inlined()) {
         // Print fields of inlined fields (recursively)
-        ValueKlass* vk = ValueKlass::cast(field_holder()->get_inline_type_field_klass(index()));
+        InlineKlass* vk = InlineKlass::cast(field_holder()->get_inline_type_field_klass(index()));
         int field_offset = offset() - vk->first_field_offset();
         obj = (oop)(cast_from_oop<address>(obj) + field_offset);
         st->print_cr("Inline type field inlined '%s':", vk->name()->as_C_string());
         FieldPrinter print_field(st, obj);
         vk->do_nonstatic_fields(&print_field);
diff a/src/hotspot/share/runtime/frame.cpp b/src/hotspot/share/runtime/frame.cpp
--- a/src/hotspot/share/runtime/frame.cpp
+++ b/src/hotspot/share/runtime/frame.cpp
@@ -35,11 +35,11 @@
 #include "memory/universe.hpp"
 #include "oops/markWord.hpp"
 #include "oops/method.hpp"
 #include "oops/methodData.hpp"
 #include "oops/oop.inline.hpp"
-#include "oops/valueKlass.hpp"
+#include "oops/inlineKlass.hpp"
 #include "oops/verifyOopClosure.hpp"
 #include "prims/methodHandles.hpp"
 #include "runtime/frame.inline.hpp"
 #include "runtime/handles.inline.hpp"
 #include "runtime/javaCalls.hpp"
diff a/src/hotspot/share/runtime/handles.cpp b/src/hotspot/share/runtime/handles.cpp
--- a/src/hotspot/share/runtime/handles.cpp
+++ b/src/hotspot/share/runtime/handles.cpp
@@ -24,11 +24,11 @@
 
 #include "precompiled.hpp"
 #include "memory/allocation.inline.hpp"
 #include "oops/constantPool.hpp"
 #include "oops/oop.inline.hpp"
-#include "oops/valueKlass.hpp"
+#include "oops/inlineKlass.hpp"
 #include "runtime/atomic.hpp"
 #include "runtime/handles.inline.hpp"
 #include "runtime/thread.inline.hpp"
 
 #ifdef ASSERT
diff a/src/hotspot/share/runtime/handles.hpp b/src/hotspot/share/runtime/handles.hpp
--- a/src/hotspot/share/runtime/handles.hpp
+++ b/src/hotspot/share/runtime/handles.hpp
@@ -27,11 +27,11 @@
 
 #include "memory/arena.hpp"
 #include "oops/oop.hpp"
 #include "oops/oopsHierarchy.hpp"
 
-class ValueKlass;
+class InlineKlass;
 class InstanceKlass;
 class Klass;
 class Thread;
 
 //------------------------------------------------------------------------------------------------------------------------
diff a/src/hotspot/share/runtime/javaCalls.cpp b/src/hotspot/share/runtime/javaCalls.cpp
--- a/src/hotspot/share/runtime/javaCalls.cpp
+++ b/src/hotspot/share/runtime/javaCalls.cpp
@@ -31,11 +31,11 @@
 #include "interpreter/interpreter.hpp"
 #include "interpreter/linkResolver.hpp"
 #include "memory/universe.hpp"
 #include "oops/method.inline.hpp"
 #include "oops/oop.inline.hpp"
-#include "oops/valueKlass.hpp"
+#include "oops/inlineKlass.hpp"
 #include "prims/jniCheck.hpp"
 #include "runtime/handles.inline.hpp"
 #include "runtime/interfaceSupport.inline.hpp"
 #include "runtime/javaCalls.hpp"
 #include "runtime/jniHandles.inline.hpp"
@@ -443,11 +443,11 @@
 
   jobject value_buffer = NULL;
   if (InlineTypeReturnedAsFields && result->get_type() == T_INLINE_TYPE) {
     // Pre allocate buffered value in case the result is returned
     // flattened by compiled code
-    ValueKlass* vk = method->returned_value_type(thread);
+    InlineKlass* vk = method->returned_inline_type(thread);
     if (vk->can_be_returned_as_fields()) {
       oop instance = vk->allocate_instance(CHECK);
       value_buffer = JNIHandles::make_local(thread, instance);
       result->set_jobject(value_buffer);
     }
diff a/src/hotspot/share/runtime/reflection.cpp b/src/hotspot/share/runtime/reflection.cpp
--- a/src/hotspot/share/runtime/reflection.cpp
+++ b/src/hotspot/share/runtime/reflection.cpp
@@ -38,11 +38,11 @@
 #include "memory/universe.hpp"
 #include "oops/instanceKlass.hpp"
 #include "oops/objArrayKlass.hpp"
 #include "oops/objArrayOop.inline.hpp"
 #include "oops/oop.inline.hpp"
-#include "oops/valueKlass.inline.hpp"
+#include "oops/inlineKlass.inline.hpp"
 #include "oops/typeArrayOop.inline.hpp"
 #include "prims/jvmtiExport.hpp"
 #include "runtime/arguments.hpp"
 #include "runtime/fieldDescriptor.inline.hpp"
 #include "runtime/handles.inline.hpp"
diff a/src/hotspot/share/runtime/safepoint.cpp b/src/hotspot/share/runtime/safepoint.cpp
--- a/src/hotspot/share/runtime/safepoint.cpp
+++ b/src/hotspot/share/runtime/safepoint.cpp
@@ -45,11 +45,11 @@
 #include "logging/logStream.hpp"
 #include "memory/resourceArea.hpp"
 #include "memory/universe.hpp"
 #include "oops/oop.inline.hpp"
 #include "oops/symbol.hpp"
-#include "oops/valueKlass.hpp"
+#include "oops/inlineKlass.hpp"
 #include "runtime/atomic.hpp"
 #include "runtime/deoptimization.hpp"
 #include "runtime/frame.inline.hpp"
 #include "runtime/handles.inline.hpp"
 #include "runtime/interfaceSupport.inline.hpp"
@@ -1045,25 +1045,25 @@
     // See if return type is an oop.
     Method* method = nm->method();
     bool return_oop = method->is_returning_oop();
 
     GrowableArray<Handle> return_values;
-    ValueKlass* vk = NULL;
+    InlineKlass* vk = NULL;
 
     if (return_oop && InlineTypeReturnedAsFields) {
       SignatureStream ss(method->signature());
       while (!ss.at_return_type()) {
         ss.next();
       }
       if (ss.type() == T_INLINE_TYPE) {
         // Check if value type is returned as fields
-        vk = ValueKlass::returned_value_klass(map);
+        vk = InlineKlass::returned_inline_klass(map);
         if (vk != NULL) {
           // We're at a safepoint at the return of a method that returns
           // multiple values. We must make sure we preserve the oop values
           // across the safepoint.
-          assert(vk == method->returned_value_type(thread()), "bad value klass");
+          assert(vk == method->returned_inline_type(thread()), "bad value klass");
           vk->save_oop_fields(map, return_values);
           return_oop = false;
         }
       }
     }
diff a/src/hotspot/share/runtime/sharedRuntime.cpp b/src/hotspot/share/runtime/sharedRuntime.cpp
--- a/src/hotspot/share/runtime/sharedRuntime.cpp
+++ b/src/hotspot/share/runtime/sharedRuntime.cpp
@@ -52,11 +52,11 @@
 #include "oops/klass.hpp"
 #include "oops/method.inline.hpp"
 #include "oops/objArrayKlass.hpp"
 #include "oops/objArrayOop.inline.hpp"
 #include "oops/oop.inline.hpp"
-#include "oops/valueKlass.inline.hpp"
+#include "oops/inlineKlass.inline.hpp"
 #include "prims/forte.hpp"
 #include "prims/jvmtiExport.hpp"
 #include "prims/methodHandles.hpp"
 #include "prims/nativeLookup.hpp"
 #include "runtime/arguments.hpp"
@@ -1141,11 +1141,11 @@
       if (callee == NULL) {
         THROW_(vmSymbols::java_lang_NoSuchMethodException(), nullHandle);
       }
     }
     if (!caller_is_c1 && callee->has_scalarized_args() && callee->method_holder()->is_inline_klass() &&
-        ValueKlass::cast(callee->method_holder())->can_be_passed_as_fields()) {
+        InlineKlass::cast(callee->method_holder())->can_be_passed_as_fields()) {
       // If the receiver is an inline type that is passed as fields, no oop is available
       // Resolve the call without receiver null checking.
       assert(attached_method.not_null() && !attached_method->is_abstract(), "must have non-abstract attached method");
       if (bc == Bytecodes::_invokeinterface) {
         bc = Bytecodes::_invokevirtual; // C2 optimistically replaces interface calls by virtual calls
@@ -1285,11 +1285,11 @@
   bool caller_is_c1 = caller_nm->is_compiled_by_c1();
 
   if (is_virtual) {
     Klass* receiver_klass = NULL;
     if (!caller_is_c1 && callee_method->has_scalarized_args() && callee_method->method_holder()->is_inline_klass() &&
-        ValueKlass::cast(callee_method->method_holder())->can_be_passed_as_fields()) {
+        InlineKlass::cast(callee_method->method_holder())->can_be_passed_as_fields()) {
       // If the receiver is an inline type that is passed as fields, no oop is available
       receiver_klass = callee_method->method_holder();
     } else {
       assert(receiver.not_null() || invoke_code == Bytecodes::_invokehandle, "sanity check");
       receiver_klass = invoke_code == Bytecodes::_invokehandle ? NULL : receiver->klass();
@@ -2746,20 +2746,20 @@
 
 int CompiledEntrySignature::compute_scalarized_cc(GrowableArray<SigEntry>*& sig_cc, VMRegPair*& regs_cc, bool scalar_receiver) {
   InstanceKlass* holder = _method->method_holder();
   sig_cc = new GrowableArray<SigEntry>(_method->size_of_parameters());
   if (!_method->is_static()) {
-    if (holder->is_inline_klass() && scalar_receiver && ValueKlass::cast(holder)->can_be_passed_as_fields()) {
-      sig_cc->appendAll(ValueKlass::cast(holder)->extended_sig());
+    if (holder->is_inline_klass() && scalar_receiver && InlineKlass::cast(holder)->can_be_passed_as_fields()) {
+      sig_cc->appendAll(InlineKlass::cast(holder)->extended_sig());
     } else {
       SigEntry::add_entry(sig_cc, T_OBJECT);
     }
   }
   Thread* THREAD = Thread::current();
   for (SignatureStream ss(_method->signature()); !ss.at_return_type(); ss.next()) {
     if (ss.type() == T_INLINE_TYPE) {
-      ValueKlass* vk = ss.as_value_klass(holder);
+      InlineKlass* vk = ss.as_inline_klass(holder);
       if (vk->can_be_passed_as_fields()) {
         sig_cc->appendAll(vk->extended_sig());
       } else {
         SigEntry::add_entry(sig_cc, T_OBJECT);
       }
@@ -2805,11 +2805,11 @@
   }
 
   if (has_value_recv()) {
     if (num_value_args() == 1) {
       // Share same entry for VVEP and VVEP(RO).
-      // This is quite common: we have an instance method in a ValueKlass that has
+      // This is quite common: we have an instance method in a InlineKlass that has
       // no value args other than <this>.
       return CodeOffsets::Verified_Value_Entry;
     } else {
       assert(num_value_args() > 1, "must be");
       // No sharing:
@@ -2831,22 +2831,22 @@
   }
 }
 
 
 void CompiledEntrySignature::compute_calling_conventions() {
-  // Get the (non-scalarized) signature and check for value type arguments
+  // Get the (non-scalarized) signature and check for inline type arguments
   if (!_method->is_static()) {
-    if (_method->method_holder()->is_inline_klass() && ValueKlass::cast(_method->method_holder())->can_be_passed_as_fields()) {
+    if (_method->method_holder()->is_inline_klass() && InlineKlass::cast(_method->method_holder())->can_be_passed_as_fields()) {
       _has_value_recv = true;
       _num_value_args++;
     }
     SigEntry::add_entry(_sig, T_OBJECT);
   }
   for (SignatureStream ss(_method->signature()); !ss.at_return_type(); ss.next()) {
     BasicType bt = ss.type();
     if (bt == T_INLINE_TYPE) {
-      if (ss.as_value_klass(_method->method_holder())->can_be_passed_as_fields()) {
+      if (ss.as_inline_klass(_method->method_holder())->can_be_passed_as_fields()) {
         _num_value_args++;
       }
       bt = T_OBJECT;
     }
     SigEntry::add_entry(_sig, bt);
@@ -2912,11 +2912,11 @@
 
     // Upper bound on stack arguments to avoid hitting the argument limit and
     // bailing out of compilation ("unsupported incoming calling sequence").
     // TODO we need a reasonable limit (flag?) here
     if (_args_on_stack_cc > 50) {
-      // Don't scalarize value type arguments
+      // Don't scalarize inline type arguments
       _sig_cc = _sig;
       _sig_cc_ro = _sig;
       _regs_cc = _regs;
       _regs_cc_ro = _regs;
       _args_on_stack_cc = _args_on_stack;
@@ -2965,11 +2965,11 @@
       method->set_c2_needs_stack_repair(ces.c2_needs_stack_repair());
     }
 
     if (method->is_abstract()) {
       if (ces.has_scalarized_args()) {
-        // Save a C heap allocated version of the signature for abstract methods with scalarized value type arguments
+        // Save a C heap allocated version of the signature for abstract methods with scalarized inline type arguments
         address wrong_method_abstract = SharedRuntime::get_handle_wrong_method_abstract_stub();
         entry = AdapterHandlerLibrary::new_entry(new AdapterFingerPrint(NULL),
                                                  StubRoutines::throw_AbstractMethodError_entry(),
                                                  wrong_method_abstract, wrong_method_abstract, wrong_method_abstract,
                                                  wrong_method_abstract, wrong_method_abstract);
@@ -3587,11 +3587,11 @@
   BarrierSet *bs = BarrierSet::barrier_set();
   bs->on_slowpath_allocation_exit(thread, new_obj);
 }
 
 // We are at a compiled code to interpreter call. We need backing
-// buffers for all value type arguments. Allocate an object array to
+// buffers for all inline type arguments. Allocate an object array to
 // hold them (convenient because once we're done with it we don't have
 // to worry about freeing it).
 oop SharedRuntime::allocate_value_types_impl(JavaThread* thread, methodHandle callee, bool allocate_receiver, TRAPS) {
   assert(InlineTypePassFieldsAsArgs, "no reason to call this");
   ResourceMark rm;
@@ -3609,18 +3609,18 @@
   }
   objArrayOop array_oop = oopFactory::new_objectArray(nb_slots, CHECK_NULL);
   objArrayHandle array(THREAD, array_oop);
   int i = 0;
   if (allocate_receiver) {
-    ValueKlass* vk = ValueKlass::cast(holder);
+    InlineKlass* vk = InlineKlass::cast(holder);
     oop res = vk->allocate_instance(CHECK_NULL);
     array->obj_at_put(i, res);
     i++;
   }
   for (SignatureStream ss(callee->signature()); !ss.at_return_type(); ss.next()) {
     if (ss.type() == T_INLINE_TYPE) {
-      ValueKlass* vk = ss.as_value_klass(holder);
+      InlineKlass* vk = ss.as_inline_klass(holder);
       oop res = vk->allocate_instance(CHECK_NULL);
       array->obj_at_put(i, res);
       i++;
     }
   }
@@ -3634,18 +3634,18 @@
   thread->set_vm_result_2(callee()); // TODO: required to keep callee live?
 JRT_END
 
 // TODO remove this once the AARCH64 dependency is gone
 // Iterate over the array of heap allocated value types and apply the GC post barrier to all reference fields.
-// This is called from the C2I adapter after value type arguments are heap allocated and initialized.
+// This is called from the C2I adapter after inline type arguments are heap allocated and initialized.
 JRT_LEAF(void, SharedRuntime::apply_post_barriers(JavaThread* thread, objArrayOopDesc* array))
 {
   assert(InlineTypePassFieldsAsArgs, "no reason to call this");
   assert(oopDesc::is_oop(array), "should be oop");
   for (int i = 0; i < array->length(); ++i) {
     instanceOop valueOop = (instanceOop)array->obj_at(i);
-    ValueKlass* vk = ValueKlass::cast(valueOop->klass());
+    InlineKlass* vk = InlineKlass::cast(valueOop->klass());
     if (vk->contains_oops()) {
       const address dst_oop_addr = ((address) (void*) valueOop);
       OopMapBlock* map = vk->start_of_nonstatic_oop_maps();
       OopMapBlock* const end = map + vk->nonstatic_oop_map_count();
       while (map != end) {
@@ -3668,11 +3668,11 @@
   RegisterMap reg_map(thread);
   frame stubFrame = thread->last_frame();
   frame callerFrame = stubFrame.sender(&reg_map);
   assert(callerFrame.is_interpreted_frame(), "should be coming from interpreter");
 
-  ValueKlass* vk = ValueKlass::cast(res->klass());
+  InlineKlass* vk = InlineKlass::cast(res->klass());
 
   const Array<SigEntry>* sig_vk = vk->extended_sig();
   const Array<VMRegPair>* regs = vk->return_regs();
 
   if (regs == NULL) {
@@ -3758,11 +3758,11 @@
   RegisterMap reg_map(thread);
   frame stubFrame = thread->last_frame();
   frame callerFrame = stubFrame.sender(&reg_map);
 
 #ifdef ASSERT
-  ValueKlass* verif_vk = ValueKlass::returned_value_klass(reg_map);
+  InlineKlass* verif_vk = InlineKlass::returned_inline_klass(reg_map);
 #endif
 
   if (!is_set_nth_bit(res, 0)) {
     // We're not returning with value type fields in registers (the
     // calling convention didn't allow it for this value klass)
@@ -3771,11 +3771,11 @@
     assert(verif_vk == NULL, "broken calling convention");
     return;
   }
 
   clear_nth_bit(res, 0);
-  ValueKlass* vk = (ValueKlass*)res;
+  InlineKlass* vk = (InlineKlass*)res;
   assert(verif_vk == vk, "broken calling convention");
   assert(Metaspace::contains((void*)res), "should be klass");
 
   // Allocate handles for every oop field so they are safe in case of
   // a safepoint when allocating
diff a/src/hotspot/share/runtime/sharedRuntime.hpp b/src/hotspot/share/runtime/sharedRuntime.hpp
--- a/src/hotspot/share/runtime/sharedRuntime.hpp
+++ b/src/hotspot/share/runtime/sharedRuntime.hpp
@@ -552,11 +552,11 @@
   static oop allocate_value_types_impl(JavaThread* thread, methodHandle callee, bool allocate_receiver, TRAPS);
   static void apply_post_barriers(JavaThread* thread, objArrayOopDesc* array);
 
   static address handle_unsafe_access(JavaThread* thread, address next_pc);
 
-  static BufferedValueTypeBlob* generate_buffered_value_type_adapter(const ValueKlass* vk);
+  static BufferedValueTypeBlob* generate_buffered_inline_type_adapter(const InlineKlass* vk);
 #ifndef PRODUCT
 
   // Collect and print inline cache miss statistics
  private:
   enum { maxICmiss_count = 100 };
diff a/src/hotspot/share/runtime/signature.cpp b/src/hotspot/share/runtime/signature.cpp
--- a/src/hotspot/share/runtime/signature.cpp
+++ b/src/hotspot/share/runtime/signature.cpp
@@ -30,11 +30,11 @@
 #include "memory/universe.hpp"
 #include "oops/instanceKlass.hpp"
 #include "oops/oop.inline.hpp"
 #include "oops/symbol.hpp"
 #include "oops/typeArrayKlass.hpp"
-#include "oops/valueKlass.inline.hpp"
+#include "oops/inlineKlass.inline.hpp"
 #include "runtime/fieldDescriptor.inline.hpp"
 #include "runtime/handles.inline.hpp"
 #include "runtime/safepointVerifiers.hpp"
 #include "runtime/signature.hpp"
 
@@ -372,17 +372,17 @@
   }
   _previous_name = name;
   return name;
 }
 
-ValueKlass* SignatureStream::as_value_klass(InstanceKlass* holder) {
+InlineKlass* SignatureStream::as_inline_klass(InstanceKlass* holder) {
   Thread* THREAD = Thread::current();
   Handle class_loader(THREAD, holder->class_loader());
   Handle protection_domain(THREAD, holder->protection_domain());
   Klass* k = as_klass(class_loader, protection_domain, SignatureStream::ReturnNull, THREAD);
   assert(k != NULL && !HAS_PENDING_EXCEPTION, "unresolved value klass");
-  return ValueKlass::cast(k);
+  return InlineKlass::cast(k);
 }
 
 Klass* SignatureStream::as_klass(Handle class_loader, Handle protection_domain,
                                  FailureMode failure_mode, TRAPS) {
   if (!is_reference()) {
diff a/src/hotspot/share/runtime/signature.hpp b/src/hotspot/share/runtime/signature.hpp
--- a/src/hotspot/share/runtime/signature.hpp
+++ b/src/hotspot/share/runtime/signature.hpp
@@ -562,20 +562,20 @@
 
   // free-standing lookups (bring your own CL/PD pair)
   enum FailureMode { ReturnNull, NCDFError, CachedOrNull };
 
   Klass* as_klass(Handle class_loader, Handle protection_domain, FailureMode failure_mode, TRAPS);
-  ValueKlass* as_value_klass(InstanceKlass* holder);
+  InlineKlass* as_inline_klass(InstanceKlass* holder);
   oop as_java_mirror(Handle class_loader, Handle protection_domain, FailureMode failure_mode, TRAPS);
 };
 
 class SigEntryFilter;
 typedef GrowableArrayFilterIterator<SigEntry, SigEntryFilter> ExtendedSignature;
 
 // Used for adapter generation. One SigEntry is used per element of
-// the signature of the method. Value type arguments are treated
-// specially. See comment for ValueKlass::collect_fields().
+// the signature of the method. Inline type arguments are treated
+// specially. See comment for InlineKlass::collect_fields().
 class SigEntry {
  public:
   BasicType _bt;
   int _offset;
 
diff a/src/hotspot/share/runtime/thread.cpp b/src/hotspot/share/runtime/thread.cpp
--- a/src/hotspot/share/runtime/thread.cpp
+++ b/src/hotspot/share/runtime/thread.cpp
@@ -56,11 +56,11 @@
 #include "oops/instanceKlass.hpp"
 #include "oops/objArrayOop.hpp"
 #include "oops/oop.inline.hpp"
 #include "oops/symbol.hpp"
 #include "oops/typeArrayOop.inline.hpp"
-#include "oops/valueKlass.hpp"
+#include "oops/inlineKlass.hpp"
 #include "oops/verifyOopClosure.hpp"
 #include "prims/jvm_misc.hpp"
 #include "prims/jvmtiExport.hpp"
 #include "prims/jvmtiThreadState.hpp"
 #include "runtime/arguments.hpp"
