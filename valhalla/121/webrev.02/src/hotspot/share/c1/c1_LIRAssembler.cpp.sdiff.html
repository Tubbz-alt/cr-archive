<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/share/c1/c1_LIRAssembler.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="c1_LIR.hpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="c1_LIRAssembler.hpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/share/c1/c1_LIRAssembler.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
 13  * accompanied this code).
 14  *
 15  * You should have received a copy of the GNU General Public License version
 16  * 2 along with this work; if not, write to the Free Software Foundation,
 17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 18  *
 19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
 20  * or visit www.oracle.com if you need additional information or have any
 21  * questions.
 22  *
 23  */
 24 
 25 #include &quot;precompiled.hpp&quot;
 26 #include &quot;asm/assembler.inline.hpp&quot;
 27 #include &quot;c1/c1_Compilation.hpp&quot;
 28 #include &quot;c1/c1_Instruction.hpp&quot;
 29 #include &quot;c1/c1_InstructionPrinter.hpp&quot;
 30 #include &quot;c1/c1_LIRAssembler.hpp&quot;
 31 #include &quot;c1/c1_MacroAssembler.hpp&quot;
 32 #include &quot;c1/c1_ValueStack.hpp&quot;

 33 #include &quot;ci/ciInstance.hpp&quot;
<span class="line-removed"> 34 #include &quot;ci/ciValueKlass.hpp&quot;</span>
 35 #include &quot;gc/shared/barrierSet.hpp&quot;
 36 #include &quot;runtime/os.hpp&quot;
 37 #include &quot;runtime/sharedRuntime.hpp&quot;
 38 
 39 void LIR_Assembler::patching_epilog(PatchingStub* patch, LIR_PatchCode patch_code, Register obj, CodeEmitInfo* info) {
 40   // We must have enough patching space so that call can be inserted.
 41   // We cannot use fat nops here, since the concurrent code rewrite may transiently
 42   // create the illegal instruction sequence.
 43   while ((intx) _masm-&gt;pc() - (intx) patch-&gt;pc_start() &lt; NativeGeneralJump::instruction_size) {
 44     _masm-&gt;nop();
 45   }
 46   patch-&gt;install(_masm, patch_code, obj, info);
 47   append_code_stub(patch);
 48 
 49 #ifdef ASSERT
 50   Bytecodes::Code code = info-&gt;scope()-&gt;method()-&gt;java_code_at_bci(info-&gt;stack()-&gt;bci());
 51   if (patch-&gt;id() == PatchingStub::access_field_id) {
 52     switch (code) {
 53       case Bytecodes::_putstatic:
 54       case Bytecodes::_getstatic:
</pre>
<hr />
<pre>
101 //---------------------------------------------------------------
102 
103 
104 LIR_Assembler::LIR_Assembler(Compilation* c):
105    _masm(c-&gt;masm())
106  , _bs(BarrierSet::barrier_set())
107  , _compilation(c)
108  , _frame_map(c-&gt;frame_map())
109  , _current_block(NULL)
110  , _pending_non_safepoint(NULL)
111  , _pending_non_safepoint_offset(0)
112 {
113   _slow_case_stubs = new CodeStubList();
114 }
115 
116 
117 LIR_Assembler::~LIR_Assembler() {
118   // The unwind handler label may be unnbound if this destructor is invoked because of a bail-out.
119   // Reset it here to avoid an assertion.
120   _unwind_handler_entry.reset();
<span class="line-modified">121   _verified_value_entry.reset();</span>
122 }
123 
124 
125 void LIR_Assembler::check_codespace() {
126   CodeSection* cs = _masm-&gt;code_section();
127   if (cs-&gt;remaining() &lt; (int)(NOT_LP64(1*K)LP64_ONLY(2*K))) {
128     BAILOUT(&quot;CodeBuffer overflow&quot;);
129   }
130 }
131 
132 
133 void LIR_Assembler::append_code_stub(CodeStub* stub) {
134   _slow_case_stubs-&gt;append(stub);
135 }
136 
137 void LIR_Assembler::emit_stubs(CodeStubList* stub_list) {
138   for (int m = 0; m &lt; stub_list-&gt;length(); m++) {
139     CodeStub* s = stub_list-&gt;at(m);
140 
141     check_codespace();
</pre>
<hr />
<pre>
468   case lir_optvirtual_call:
469     call(op, relocInfo::opt_virtual_call_type);
470     break;
471   case lir_icvirtual_call:
472     ic_call(op);
473     break;
474   case lir_virtual_call:
475     vtable_call(op);
476     break;
477   default:
478     fatal(&quot;unexpected op code: %s&quot;, op-&gt;name());
479     break;
480   }
481 
482   // JSR 292
483   // Record if this method has MethodHandle invokes.
484   if (op-&gt;is_method_handle_invoke()) {
485     compilation()-&gt;set_has_method_handle_invokes(true);
486   }
487 
<span class="line-modified">488   ciValueKlass* vk;</span>
489   if (op-&gt;maybe_return_as_fields(&amp;vk)) {
490     int offset = store_inline_type_fields_to_buf(vk);
491     add_call_info(offset, op-&gt;info(), true);
492   }
493 
494 #if defined(IA32) &amp;&amp; defined(TIERED)
495   // C2 leave fpu stack dirty clean it
496   if (UseSSE &lt; 2) {
497     int i;
498     for ( i = 1; i &lt;= 7 ; i++ ) {
499       ffree(i);
500     }
501     if (!op-&gt;result_opr()-&gt;is_float_kind()) {
502       ffree(0);
503     }
504   }
505 #endif // X86 &amp;&amp; TIERED
506 }
507 
508 
</pre>
<hr />
<pre>
578       break;
579     }
580 
581     case lir_monaddr:
582       monitor_address(op-&gt;in_opr()-&gt;as_constant_ptr()-&gt;as_jint(), op-&gt;result_opr());
583       break;
584 
585     case lir_unwind:
586       unwind_op(op-&gt;in_opr());
587       break;
588 
589     default:
590       Unimplemented();
591       break;
592   }
593 }
594 
595 void LIR_Assembler::add_scalarized_entry_info(int pc_offset) {
596   flush_debug_info(pc_offset);
597   DebugInformationRecorder* debug_info = compilation()-&gt;debug_info_recorder();
<span class="line-modified">598   // The VEP and VVEP(RO) of a C1-compiled method call buffer_value_args_xxx()</span>
599   // before doing any argument shuffling. This call may cause GC. When GC happens,
600   // all the parameters are still as passed by the caller, so we just use
601   // map-&gt;set_include_argument_oops() inside frame::sender_for_compiled_frame(RegisterMap* map).
602   // There&#39;s no need to build a GC map here.
603   OopMap* oop_map = new OopMap(0, 0);
604   debug_info-&gt;add_safepoint(pc_offset, oop_map);
605   DebugToken* locvals = debug_info-&gt;create_scope_values(NULL); // FIXME is this needed (for Java debugging to work properly??)
606   DebugToken* expvals = debug_info-&gt;create_scope_values(NULL); // FIXME is this needed (for Java debugging to work properly??)
607   DebugToken* monvals = debug_info-&gt;create_monitor_values(NULL); // FIXME: need testing with synchronized method
608   bool reexecute = false;
609   bool return_oop = false; // This flag will be ignored since it used only for C2 with escape analysis.
610   bool rethrow_exception = false;
611   bool is_method_handle_invoke = false;
612   debug_info-&gt;describe_scope(pc_offset, methodHandle(), method(), 0, reexecute, rethrow_exception, is_method_handle_invoke, return_oop, false, locvals, expvals, monvals);
613   debug_info-&gt;end_safepoint(pc_offset);
614 }
615 
616 // The entries points of C1-compiled methods can have the following types:
<span class="line-modified">617 // (1) Methods with no value args</span>
<span class="line-modified">618 // (2) Methods with value receiver but no value args</span>
<span class="line-modified">619 //     VVEP_RO is the same as VVEP</span>
<span class="line-modified">620 // (3) Methods with non-value receiver and some value args</span>
<span class="line-modified">621 //     VVEP_RO is the same as VEP</span>
<span class="line-modified">622 // (4) Methods with value receiver and other value args</span>
<span class="line-modified">623 //     Separate VEP, VVEP and VVEP_RO</span>
624 //
625 // (1)               (2)                 (3)                    (4)
<span class="line-modified">626 // UEP/UVEP:         VEP:                UEP:                   UEP:</span>
627 //   check_icache      pack receiver       check_icache           check_icache
<span class="line-modified">628 // VEP/VVEP/VVEP_RO    jump to VVEP      VEP/VVEP_RO:           VVEP_RO:</span>
<span class="line-modified">629 //   body            UEP/UVEP:             pack value args        pack value args (except receiver)</span>
<span class="line-modified">630 //                     check_icache        jump to VVEP           jump to VVEP</span>
<span class="line-modified">631 //                   VVEP/VVEP_RO        UVEP:                  VEP:</span>
<span class="line-modified">632 //                     body                check_icache           pack all value args</span>
<span class="line-modified">633 //                                       VVEP:                    jump to VVEP</span>
<span class="line-modified">634 //                                         body                 UVEP:</span>
635 //                                                                check_icache
<span class="line-modified">636 //                                                              VVEP:</span>
637 //                                                                body
638 void LIR_Assembler::emit_std_entries() {
639   offsets()-&gt;set_value(CodeOffsets::OSR_Entry, _masm-&gt;offset());
640 
641   _masm-&gt;align(CodeEntryAlignment);
642   const CompiledEntrySignature* ces = compilation()-&gt;compiled_entry_signature();
643   if (ces-&gt;has_scalarized_args()) {
644     assert(InlineTypePassFieldsAsArgs &amp;&amp; method()-&gt;get_Method()-&gt;has_scalarized_args(), &quot;must be&quot;);
<span class="line-modified">645     CodeOffsets::Entries ro_entry_type = ces-&gt;c1_value_ro_entry_type();</span>
646 
647     // UEP: check icache and fall-through
<span class="line-modified">648     if (ro_entry_type != CodeOffsets::Verified_Value_Entry) {</span>
649       offsets()-&gt;set_value(CodeOffsets::Entry, _masm-&gt;offset());
650       if (needs_icache(method())) {
651         check_icache();
652       }
653     }
654 
<span class="line-modified">655     // VVEP_RO: pack all value parameters, except the receiver</span>
<span class="line-modified">656     if (ro_entry_type == CodeOffsets::Verified_Value_Entry_RO) {</span>
<span class="line-modified">657       emit_std_entry(CodeOffsets::Verified_Value_Entry_RO, ces);</span>
658     }
659 
660     // VEP: pack all value parameters
661     _masm-&gt;align(CodeEntryAlignment);
662     emit_std_entry(CodeOffsets::Verified_Entry, ces);
663 
<span class="line-modified">664     // UVEP: check icache and fall-through</span>
665     _masm-&gt;align(CodeEntryAlignment);
<span class="line-modified">666     offsets()-&gt;set_value(CodeOffsets::Value_Entry, _masm-&gt;offset());</span>
<span class="line-modified">667     if (ro_entry_type == CodeOffsets::Verified_Value_Entry) {</span>
<span class="line-modified">668       // Special case if we have VVEP == VVEP(RO):</span>
<span class="line-modified">669       // this means UVEP (called by C1) == UEP (called by C2).</span>
670       offsets()-&gt;set_value(CodeOffsets::Entry, _masm-&gt;offset());
671     }
672     if (needs_icache(method())) {
673       check_icache();
674     }
675 
<span class="line-modified">676     // VVEP: all value parameters are passed as refs - no packing.</span>
<span class="line-modified">677     emit_std_entry(CodeOffsets::Verified_Value_Entry, NULL);</span>
678 
<span class="line-modified">679     if (ro_entry_type != CodeOffsets::Verified_Value_Entry_RO) {</span>
<span class="line-modified">680       // The VVEP(RO) is the same as VEP or VVEP</span>
681       assert(ro_entry_type == CodeOffsets::Verified_Entry ||
<span class="line-modified">682              ro_entry_type == CodeOffsets::Verified_Value_Entry, &quot;must be&quot;);</span>
<span class="line-modified">683       offsets()-&gt;set_value(CodeOffsets::Verified_Value_Entry_RO,</span>
684                            offsets()-&gt;value(ro_entry_type));
685     }
686   } else {
687     // All 3 entries are the same (no value-type packing)
688     offsets()-&gt;set_value(CodeOffsets::Entry, _masm-&gt;offset());
<span class="line-modified">689     offsets()-&gt;set_value(CodeOffsets::Value_Entry, _masm-&gt;offset());</span>
690     if (needs_icache(method())) {
691       check_icache();
692     }
<span class="line-modified">693     emit_std_entry(CodeOffsets::Verified_Value_Entry, NULL);</span>
<span class="line-modified">694     offsets()-&gt;set_value(CodeOffsets::Verified_Entry, offsets()-&gt;value(CodeOffsets::Verified_Value_Entry));</span>
<span class="line-modified">695     offsets()-&gt;set_value(CodeOffsets::Verified_Value_Entry_RO, offsets()-&gt;value(CodeOffsets::Verified_Value_Entry));</span>
696   }
697 }
698 
699 void LIR_Assembler::emit_std_entry(CodeOffsets::Entries entry, const CompiledEntrySignature* ces) {
700   offsets()-&gt;set_value(entry, _masm-&gt;offset());
701   _masm-&gt;verified_entry();
702   switch (entry) {
703   case CodeOffsets::Verified_Entry: {
704     if (needs_clinit_barrier_on_entry(method())) {
705       clinit_barrier(method());
706     }
<span class="line-modified">707     int rt_call_offset = _masm-&gt;verified_entry(ces, initial_frame_size_in_bytes(), bang_size_in_bytes(), in_bytes(frame_map()-&gt;sp_offset_for_orig_pc()), _verified_value_entry);</span>
708     add_scalarized_entry_info(rt_call_offset);
709     break;
710   }
<span class="line-modified">711   case CodeOffsets::Verified_Value_Entry_RO: {</span>
712     assert(!needs_clinit_barrier_on_entry(method()), &quot;can&#39;t be static&quot;);
<span class="line-modified">713     int rt_call_offset = _masm-&gt;verified_value_ro_entry(ces, initial_frame_size_in_bytes(), bang_size_in_bytes(), in_bytes(frame_map()-&gt;sp_offset_for_orig_pc()), _verified_value_entry);</span>
714     add_scalarized_entry_info(rt_call_offset);
715     break;
716   }
<span class="line-modified">717   case CodeOffsets::Verified_Value_Entry: {</span>
718     if (needs_clinit_barrier_on_entry(method())) {
719       clinit_barrier(method());
720     }
721     build_frame();
722     offsets()-&gt;set_value(CodeOffsets::Frame_Complete, _masm-&gt;offset());
723     break;
724   }
725   default:
726     ShouldNotReachHere();
727     break;
728   }
729 }
730 
731 void LIR_Assembler::emit_op0(LIR_Op0* op) {
732   switch (op-&gt;code()) {
733     case lir_nop:
734       assert(op-&gt;info() == NULL, &quot;not supported&quot;);
735       _masm-&gt;nop();
736       break;
737 
</pre>
<hr />
<pre>
875       break;
876 
877     case lir_throw:
878       throw_op(op-&gt;in_opr1(), op-&gt;in_opr2(), op-&gt;info());
879       break;
880 
881     case lir_xadd:
882     case lir_xchg:
883       atomic_op(op-&gt;code(), op-&gt;in_opr1(), op-&gt;in_opr2(), op-&gt;result_opr(), op-&gt;tmp1_opr());
884       break;
885 
886     default:
887       Unimplemented();
888       break;
889   }
890 }
891 
892 
893 void LIR_Assembler::build_frame() {
894   _masm-&gt;build_frame(initial_frame_size_in_bytes(), bang_size_in_bytes(), in_bytes(frame_map()-&gt;sp_offset_for_orig_pc()),
<span class="line-modified">895                      needs_stack_repair(), method()-&gt;has_scalarized_args(), &amp;_verified_value_entry);</span>
896 }
897 
898 
899 void LIR_Assembler::roundfp_op(LIR_Opr src, LIR_Opr tmp, LIR_Opr dest, bool pop_fpu_stack) {
900   assert(strict_fp_requires_explicit_rounding, &quot;not required&quot;);
901   assert((src-&gt;is_single_fpu() &amp;&amp; dest-&gt;is_single_stack()) ||
902          (src-&gt;is_double_fpu() &amp;&amp; dest-&gt;is_double_stack()),
903          &quot;round_fp: rounds register -&gt; stack location&quot;);
904 
905   reg2stack (src, dest, src-&gt;type(), pop_fpu_stack);
906 }
907 
908 
909 void LIR_Assembler::move_op(LIR_Opr src, LIR_Opr dest, BasicType type, LIR_PatchCode patch_code, CodeEmitInfo* info, bool pop_fpu_stack, bool unaligned, bool wide) {
910   if (src-&gt;is_register()) {
911     if (dest-&gt;is_register()) {
912       assert(patch_code == lir_patch_none &amp;&amp; info == NULL, &quot;no patching and info allowed here&quot;);
913       reg2reg(src,  dest);
914     } else if (dest-&gt;is_stack()) {
915       assert(patch_code == lir_patch_none &amp;&amp; info == NULL, &quot;no patching and info allowed here&quot;);
</pre>
</td>
<td>
<hr />
<pre>
 13  * accompanied this code).
 14  *
 15  * You should have received a copy of the GNU General Public License version
 16  * 2 along with this work; if not, write to the Free Software Foundation,
 17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 18  *
 19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
 20  * or visit www.oracle.com if you need additional information or have any
 21  * questions.
 22  *
 23  */
 24 
 25 #include &quot;precompiled.hpp&quot;
 26 #include &quot;asm/assembler.inline.hpp&quot;
 27 #include &quot;c1/c1_Compilation.hpp&quot;
 28 #include &quot;c1/c1_Instruction.hpp&quot;
 29 #include &quot;c1/c1_InstructionPrinter.hpp&quot;
 30 #include &quot;c1/c1_LIRAssembler.hpp&quot;
 31 #include &quot;c1/c1_MacroAssembler.hpp&quot;
 32 #include &quot;c1/c1_ValueStack.hpp&quot;
<span class="line-added"> 33 #include &quot;ci/ciInlineKlass.hpp&quot;</span>
 34 #include &quot;ci/ciInstance.hpp&quot;

 35 #include &quot;gc/shared/barrierSet.hpp&quot;
 36 #include &quot;runtime/os.hpp&quot;
 37 #include &quot;runtime/sharedRuntime.hpp&quot;
 38 
 39 void LIR_Assembler::patching_epilog(PatchingStub* patch, LIR_PatchCode patch_code, Register obj, CodeEmitInfo* info) {
 40   // We must have enough patching space so that call can be inserted.
 41   // We cannot use fat nops here, since the concurrent code rewrite may transiently
 42   // create the illegal instruction sequence.
 43   while ((intx) _masm-&gt;pc() - (intx) patch-&gt;pc_start() &lt; NativeGeneralJump::instruction_size) {
 44     _masm-&gt;nop();
 45   }
 46   patch-&gt;install(_masm, patch_code, obj, info);
 47   append_code_stub(patch);
 48 
 49 #ifdef ASSERT
 50   Bytecodes::Code code = info-&gt;scope()-&gt;method()-&gt;java_code_at_bci(info-&gt;stack()-&gt;bci());
 51   if (patch-&gt;id() == PatchingStub::access_field_id) {
 52     switch (code) {
 53       case Bytecodes::_putstatic:
 54       case Bytecodes::_getstatic:
</pre>
<hr />
<pre>
101 //---------------------------------------------------------------
102 
103 
104 LIR_Assembler::LIR_Assembler(Compilation* c):
105    _masm(c-&gt;masm())
106  , _bs(BarrierSet::barrier_set())
107  , _compilation(c)
108  , _frame_map(c-&gt;frame_map())
109  , _current_block(NULL)
110  , _pending_non_safepoint(NULL)
111  , _pending_non_safepoint_offset(0)
112 {
113   _slow_case_stubs = new CodeStubList();
114 }
115 
116 
117 LIR_Assembler::~LIR_Assembler() {
118   // The unwind handler label may be unnbound if this destructor is invoked because of a bail-out.
119   // Reset it here to avoid an assertion.
120   _unwind_handler_entry.reset();
<span class="line-modified">121   _verified_inline_entry.reset();</span>
122 }
123 
124 
125 void LIR_Assembler::check_codespace() {
126   CodeSection* cs = _masm-&gt;code_section();
127   if (cs-&gt;remaining() &lt; (int)(NOT_LP64(1*K)LP64_ONLY(2*K))) {
128     BAILOUT(&quot;CodeBuffer overflow&quot;);
129   }
130 }
131 
132 
133 void LIR_Assembler::append_code_stub(CodeStub* stub) {
134   _slow_case_stubs-&gt;append(stub);
135 }
136 
137 void LIR_Assembler::emit_stubs(CodeStubList* stub_list) {
138   for (int m = 0; m &lt; stub_list-&gt;length(); m++) {
139     CodeStub* s = stub_list-&gt;at(m);
140 
141     check_codespace();
</pre>
<hr />
<pre>
468   case lir_optvirtual_call:
469     call(op, relocInfo::opt_virtual_call_type);
470     break;
471   case lir_icvirtual_call:
472     ic_call(op);
473     break;
474   case lir_virtual_call:
475     vtable_call(op);
476     break;
477   default:
478     fatal(&quot;unexpected op code: %s&quot;, op-&gt;name());
479     break;
480   }
481 
482   // JSR 292
483   // Record if this method has MethodHandle invokes.
484   if (op-&gt;is_method_handle_invoke()) {
485     compilation()-&gt;set_has_method_handle_invokes(true);
486   }
487 
<span class="line-modified">488   ciInlineKlass* vk;</span>
489   if (op-&gt;maybe_return_as_fields(&amp;vk)) {
490     int offset = store_inline_type_fields_to_buf(vk);
491     add_call_info(offset, op-&gt;info(), true);
492   }
493 
494 #if defined(IA32) &amp;&amp; defined(TIERED)
495   // C2 leave fpu stack dirty clean it
496   if (UseSSE &lt; 2) {
497     int i;
498     for ( i = 1; i &lt;= 7 ; i++ ) {
499       ffree(i);
500     }
501     if (!op-&gt;result_opr()-&gt;is_float_kind()) {
502       ffree(0);
503     }
504   }
505 #endif // X86 &amp;&amp; TIERED
506 }
507 
508 
</pre>
<hr />
<pre>
578       break;
579     }
580 
581     case lir_monaddr:
582       monitor_address(op-&gt;in_opr()-&gt;as_constant_ptr()-&gt;as_jint(), op-&gt;result_opr());
583       break;
584 
585     case lir_unwind:
586       unwind_op(op-&gt;in_opr());
587       break;
588 
589     default:
590       Unimplemented();
591       break;
592   }
593 }
594 
595 void LIR_Assembler::add_scalarized_entry_info(int pc_offset) {
596   flush_debug_info(pc_offset);
597   DebugInformationRecorder* debug_info = compilation()-&gt;debug_info_recorder();
<span class="line-modified">598   // The VEP and VIEP(RO) of a C1-compiled method call buffer_inline_args_xxx()</span>
599   // before doing any argument shuffling. This call may cause GC. When GC happens,
600   // all the parameters are still as passed by the caller, so we just use
601   // map-&gt;set_include_argument_oops() inside frame::sender_for_compiled_frame(RegisterMap* map).
602   // There&#39;s no need to build a GC map here.
603   OopMap* oop_map = new OopMap(0, 0);
604   debug_info-&gt;add_safepoint(pc_offset, oop_map);
605   DebugToken* locvals = debug_info-&gt;create_scope_values(NULL); // FIXME is this needed (for Java debugging to work properly??)
606   DebugToken* expvals = debug_info-&gt;create_scope_values(NULL); // FIXME is this needed (for Java debugging to work properly??)
607   DebugToken* monvals = debug_info-&gt;create_monitor_values(NULL); // FIXME: need testing with synchronized method
608   bool reexecute = false;
609   bool return_oop = false; // This flag will be ignored since it used only for C2 with escape analysis.
610   bool rethrow_exception = false;
611   bool is_method_handle_invoke = false;
612   debug_info-&gt;describe_scope(pc_offset, methodHandle(), method(), 0, reexecute, rethrow_exception, is_method_handle_invoke, return_oop, false, locvals, expvals, monvals);
613   debug_info-&gt;end_safepoint(pc_offset);
614 }
615 
616 // The entries points of C1-compiled methods can have the following types:
<span class="line-modified">617 // (1) Methods with no inline type args</span>
<span class="line-modified">618 // (2) Methods with inline type receiver but no inline type args</span>
<span class="line-modified">619 //     VIEP_RO is the same as VIEP</span>
<span class="line-modified">620 // (3) Methods with non-inline type receiver and some inline type args</span>
<span class="line-modified">621 //     VIEP_RO is the same as VEP</span>
<span class="line-modified">622 // (4) Methods with inline type receiver and other inline type args</span>
<span class="line-modified">623 //     Separate VEP, VIEP and VIEP_RO</span>
624 //
625 // (1)               (2)                 (3)                    (4)
<span class="line-modified">626 // UEP/UIEP:         VEP:                UEP:                   UEP:</span>
627 //   check_icache      pack receiver       check_icache           check_icache
<span class="line-modified">628 // VEP/VIEP/VIEP_RO    jump to VIEP      VEP/VIEP_RO:           VIEP_RO:</span>
<span class="line-modified">629 //   body            UEP/UIEP:             pack inline args       pack inline args (except receiver)</span>
<span class="line-modified">630 //                     check_icache        jump to VIEP           jump to VIEP</span>
<span class="line-modified">631 //                   VIEP/VIEP_RO        UIEP:                  VEP:</span>
<span class="line-modified">632 //                     body                check_icache           pack all inline args</span>
<span class="line-modified">633 //                                       VIEP:                    jump to VIEP</span>
<span class="line-modified">634 //                                         body                 UIEP:</span>
635 //                                                                check_icache
<span class="line-modified">636 //                                                              VIEP:</span>
637 //                                                                body
638 void LIR_Assembler::emit_std_entries() {
639   offsets()-&gt;set_value(CodeOffsets::OSR_Entry, _masm-&gt;offset());
640 
641   _masm-&gt;align(CodeEntryAlignment);
642   const CompiledEntrySignature* ces = compilation()-&gt;compiled_entry_signature();
643   if (ces-&gt;has_scalarized_args()) {
644     assert(InlineTypePassFieldsAsArgs &amp;&amp; method()-&gt;get_Method()-&gt;has_scalarized_args(), &quot;must be&quot;);
<span class="line-modified">645     CodeOffsets::Entries ro_entry_type = ces-&gt;c1_inline_ro_entry_type();</span>
646 
647     // UEP: check icache and fall-through
<span class="line-modified">648     if (ro_entry_type != CodeOffsets::Verified_Inline_Entry) {</span>
649       offsets()-&gt;set_value(CodeOffsets::Entry, _masm-&gt;offset());
650       if (needs_icache(method())) {
651         check_icache();
652       }
653     }
654 
<span class="line-modified">655     // VIEP_RO: pack all value parameters, except the receiver</span>
<span class="line-modified">656     if (ro_entry_type == CodeOffsets::Verified_Inline_Entry_RO) {</span>
<span class="line-modified">657       emit_std_entry(CodeOffsets::Verified_Inline_Entry_RO, ces);</span>
658     }
659 
660     // VEP: pack all value parameters
661     _masm-&gt;align(CodeEntryAlignment);
662     emit_std_entry(CodeOffsets::Verified_Entry, ces);
663 
<span class="line-modified">664     // UIEP: check icache and fall-through</span>
665     _masm-&gt;align(CodeEntryAlignment);
<span class="line-modified">666     offsets()-&gt;set_value(CodeOffsets::Inline_Entry, _masm-&gt;offset());</span>
<span class="line-modified">667     if (ro_entry_type == CodeOffsets::Verified_Inline_Entry) {</span>
<span class="line-modified">668       // Special case if we have VIEP == VIEP(RO):</span>
<span class="line-modified">669       // this means UIEP (called by C1) == UEP (called by C2).</span>
670       offsets()-&gt;set_value(CodeOffsets::Entry, _masm-&gt;offset());
671     }
672     if (needs_icache(method())) {
673       check_icache();
674     }
675 
<span class="line-modified">676     // VIEP: all value parameters are passed as refs - no packing.</span>
<span class="line-modified">677     emit_std_entry(CodeOffsets::Verified_Inline_Entry, NULL);</span>
678 
<span class="line-modified">679     if (ro_entry_type != CodeOffsets::Verified_Inline_Entry_RO) {</span>
<span class="line-modified">680       // The VIEP(RO) is the same as VEP or VIEP</span>
681       assert(ro_entry_type == CodeOffsets::Verified_Entry ||
<span class="line-modified">682              ro_entry_type == CodeOffsets::Verified_Inline_Entry, &quot;must be&quot;);</span>
<span class="line-modified">683       offsets()-&gt;set_value(CodeOffsets::Verified_Inline_Entry_RO,</span>
684                            offsets()-&gt;value(ro_entry_type));
685     }
686   } else {
687     // All 3 entries are the same (no value-type packing)
688     offsets()-&gt;set_value(CodeOffsets::Entry, _masm-&gt;offset());
<span class="line-modified">689     offsets()-&gt;set_value(CodeOffsets::Inline_Entry, _masm-&gt;offset());</span>
690     if (needs_icache(method())) {
691       check_icache();
692     }
<span class="line-modified">693     emit_std_entry(CodeOffsets::Verified_Inline_Entry, NULL);</span>
<span class="line-modified">694     offsets()-&gt;set_value(CodeOffsets::Verified_Entry, offsets()-&gt;value(CodeOffsets::Verified_Inline_Entry));</span>
<span class="line-modified">695     offsets()-&gt;set_value(CodeOffsets::Verified_Inline_Entry_RO, offsets()-&gt;value(CodeOffsets::Verified_Inline_Entry));</span>
696   }
697 }
698 
699 void LIR_Assembler::emit_std_entry(CodeOffsets::Entries entry, const CompiledEntrySignature* ces) {
700   offsets()-&gt;set_value(entry, _masm-&gt;offset());
701   _masm-&gt;verified_entry();
702   switch (entry) {
703   case CodeOffsets::Verified_Entry: {
704     if (needs_clinit_barrier_on_entry(method())) {
705       clinit_barrier(method());
706     }
<span class="line-modified">707     int rt_call_offset = _masm-&gt;verified_entry(ces, initial_frame_size_in_bytes(), bang_size_in_bytes(), in_bytes(frame_map()-&gt;sp_offset_for_orig_pc()), _verified_inline_entry);</span>
708     add_scalarized_entry_info(rt_call_offset);
709     break;
710   }
<span class="line-modified">711   case CodeOffsets::Verified_Inline_Entry_RO: {</span>
712     assert(!needs_clinit_barrier_on_entry(method()), &quot;can&#39;t be static&quot;);
<span class="line-modified">713     int rt_call_offset = _masm-&gt;verified_inline_ro_entry(ces, initial_frame_size_in_bytes(), bang_size_in_bytes(), in_bytes(frame_map()-&gt;sp_offset_for_orig_pc()), _verified_inline_entry);</span>
714     add_scalarized_entry_info(rt_call_offset);
715     break;
716   }
<span class="line-modified">717   case CodeOffsets::Verified_Inline_Entry: {</span>
718     if (needs_clinit_barrier_on_entry(method())) {
719       clinit_barrier(method());
720     }
721     build_frame();
722     offsets()-&gt;set_value(CodeOffsets::Frame_Complete, _masm-&gt;offset());
723     break;
724   }
725   default:
726     ShouldNotReachHere();
727     break;
728   }
729 }
730 
731 void LIR_Assembler::emit_op0(LIR_Op0* op) {
732   switch (op-&gt;code()) {
733     case lir_nop:
734       assert(op-&gt;info() == NULL, &quot;not supported&quot;);
735       _masm-&gt;nop();
736       break;
737 
</pre>
<hr />
<pre>
875       break;
876 
877     case lir_throw:
878       throw_op(op-&gt;in_opr1(), op-&gt;in_opr2(), op-&gt;info());
879       break;
880 
881     case lir_xadd:
882     case lir_xchg:
883       atomic_op(op-&gt;code(), op-&gt;in_opr1(), op-&gt;in_opr2(), op-&gt;result_opr(), op-&gt;tmp1_opr());
884       break;
885 
886     default:
887       Unimplemented();
888       break;
889   }
890 }
891 
892 
893 void LIR_Assembler::build_frame() {
894   _masm-&gt;build_frame(initial_frame_size_in_bytes(), bang_size_in_bytes(), in_bytes(frame_map()-&gt;sp_offset_for_orig_pc()),
<span class="line-modified">895                      needs_stack_repair(), method()-&gt;has_scalarized_args(), &amp;_verified_inline_entry);</span>
896 }
897 
898 
899 void LIR_Assembler::roundfp_op(LIR_Opr src, LIR_Opr tmp, LIR_Opr dest, bool pop_fpu_stack) {
900   assert(strict_fp_requires_explicit_rounding, &quot;not required&quot;);
901   assert((src-&gt;is_single_fpu() &amp;&amp; dest-&gt;is_single_stack()) ||
902          (src-&gt;is_double_fpu() &amp;&amp; dest-&gt;is_double_stack()),
903          &quot;round_fp: rounds register -&gt; stack location&quot;);
904 
905   reg2stack (src, dest, src-&gt;type(), pop_fpu_stack);
906 }
907 
908 
909 void LIR_Assembler::move_op(LIR_Opr src, LIR_Opr dest, BasicType type, LIR_PatchCode patch_code, CodeEmitInfo* info, bool pop_fpu_stack, bool unaligned, bool wide) {
910   if (src-&gt;is_register()) {
911     if (dest-&gt;is_register()) {
912       assert(patch_code == lir_patch_none &amp;&amp; info == NULL, &quot;no patching and info allowed here&quot;);
913       reg2reg(src,  dest);
914     } else if (dest-&gt;is_stack()) {
915       assert(patch_code == lir_patch_none &amp;&amp; info == NULL, &quot;no patching and info allowed here&quot;);
</pre>
</td>
</tr>
</table>
<center><a href="c1_LIR.hpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="c1_LIRAssembler.hpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>