<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/share/runtime/sharedRuntime.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="safepoint.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="sharedRuntime.hpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/share/runtime/sharedRuntime.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
1552       oop recv = callerFrame.retrieve_receiver(&amp;reg_map);
1553       Klass *recv_klass = (recv != NULL) ? recv-&gt;klass() : NULL;
1554       LinkResolver::throw_abstract_method_error(callee, recv_klass, thread);
1555       res = StubRoutines::forward_exception_entry();
1556     }
1557   JRT_BLOCK_END
1558   return res;
1559 JRT_END
1560 
1561 
1562 // resolve a static call and patch code
1563 JRT_BLOCK_ENTRY(address, SharedRuntime::resolve_static_call_C(JavaThread *thread ))
1564   methodHandle callee_method;
1565   bool caller_is_c1;
1566   JRT_BLOCK
1567     callee_method = SharedRuntime::resolve_helper(thread, false, false, &amp;caller_is_c1, CHECK_NULL);
1568     thread-&gt;set_vm_result_2(callee_method());
1569   JRT_BLOCK_END
1570   // return compiled code entry point after potential safepoints
1571   address entry = caller_is_c1 ?
<span class="line-modified">1572     callee_method-&gt;verified_value_code_entry() : callee_method-&gt;verified_code_entry();</span>
1573   assert(entry != NULL, &quot;Jump to zero!&quot;);
1574   return entry;
1575 JRT_END
1576 
1577 
1578 // resolve virtual call and update inline cache to monomorphic
1579 JRT_BLOCK_ENTRY(address, SharedRuntime::resolve_virtual_call_C(JavaThread *thread ))
1580   methodHandle callee_method;
1581   bool caller_is_c1;
1582   JRT_BLOCK
1583     callee_method = SharedRuntime::resolve_helper(thread, true, false, &amp;caller_is_c1, CHECK_NULL);
1584     thread-&gt;set_vm_result_2(callee_method());
1585   JRT_BLOCK_END
1586   // return compiled code entry point after potential safepoints
1587   address entry = caller_is_c1 ?
<span class="line-modified">1588     callee_method-&gt;verified_value_code_entry() : callee_method-&gt;verified_value_ro_code_entry();</span>
1589   assert(entry != NULL, &quot;Jump to zero!&quot;);
1590   return entry;
1591 JRT_END
1592 
1593 
1594 // Resolve a virtual call that can be statically bound (e.g., always
1595 // monomorphic, so it has no inline cache).  Patch code to resolved target.
1596 JRT_BLOCK_ENTRY(address, SharedRuntime::resolve_opt_virtual_call_C(JavaThread *thread))
1597   methodHandle callee_method;
1598   bool caller_is_c1;
1599   JRT_BLOCK
1600     callee_method = SharedRuntime::resolve_helper(thread, true, true, &amp;caller_is_c1, CHECK_NULL);
1601     thread-&gt;set_vm_result_2(callee_method());
1602   JRT_BLOCK_END
1603   // return compiled code entry point after potential safepoints
1604   address entry = caller_is_c1 ?
<span class="line-modified">1605     callee_method-&gt;verified_value_code_entry() : callee_method-&gt;verified_code_entry();</span>
1606   assert(entry != NULL, &quot;Jump to zero!&quot;);
1607   return entry;
1608 JRT_END
1609 
1610 // The handle_ic_miss_helper_internal function returns false if it failed due
1611 // to either running out of vtable stubs or ic stubs due to IC transitions
1612 // to transitional states. The needs_ic_stub_refill value will be set if
1613 // the failure was due to running out of IC stubs, in which case handle_ic_miss_helper
1614 // refills the IC stubs and tries again.
1615 bool SharedRuntime::handle_ic_miss_helper_internal(Handle receiver, CompiledMethod* caller_nm,
1616                                                    const frame&amp; caller_frame, methodHandle callee_method,
1617                                                    Bytecodes::Code bc, CallInfo&amp; call_info,
1618                                                    bool&amp; needs_ic_stub_refill, bool&amp; is_optimized, bool caller_is_c1, TRAPS) {
1619   CompiledICLocker ml(caller_nm);
1620   CompiledIC* inline_cache = CompiledIC_before(caller_nm, caller_frame.pc());
1621   bool should_be_mono = false;
1622   if (inline_cache-&gt;is_optimized()) {
1623     if (TraceCallFixup) {
1624       ResourceMark rm(THREAD);
1625       tty-&gt;print(&quot;OPTIMIZED IC miss (%s) call to&quot;, Bytecodes::name(bc));
</pre>
<hr />
<pre>
2339  private:
2340   enum {
2341     _basic_type_bits = 4,
2342     _basic_type_mask = right_n_bits(_basic_type_bits),
2343     _basic_types_per_int = BitsPerInt / _basic_type_bits,
2344     _compact_int_count = 3
2345   };
2346   // TO DO:  Consider integrating this with a more global scheme for compressing signatures.
2347   // For now, 4 bits per components (plus T_VOID gaps after double/long) is not excessive.
2348 
2349   union {
2350     int  _compact[_compact_int_count];
2351     int* _fingerprint;
2352   } _value;
2353   int _length; // A negative length indicates the fingerprint is in the compact form,
2354                // Otherwise _value._fingerprint is the array.
2355 
2356   // Remap BasicTypes that are handled equivalently by the adapters.
2357   // These are correct for the current system but someday it might be
2358   // necessary to make this mapping platform dependent.
<span class="line-modified">2359   static int adapter_encoding(BasicType in, bool is_valuetype) {</span>
2360     switch (in) {
2361       case T_BOOLEAN:
2362       case T_BYTE:
2363       case T_SHORT:
2364       case T_CHAR: {
<span class="line-modified">2365         if (is_valuetype) {</span>
2366           // Do not widen inline type field types
2367           assert(InlineTypePassFieldsAsArgs, &quot;must be enabled&quot;);
2368           return in;
2369         } else {
2370           // They are all promoted to T_INT in the calling convention
2371           return T_INT;
2372         }
2373       }
2374 
2375       case T_INLINE_TYPE: {
2376         // If inline types are passed as fields, return &#39;in&#39; to differentiate
2377         // between a T_INLINE_TYPE and a T_OBJECT in the signature.
2378         return InlineTypePassFieldsAsArgs ? in : adapter_encoding(T_OBJECT, false);
2379       }
2380 
2381       case T_OBJECT:
2382       case T_ARRAY:
2383         // In other words, we assume that any register good enough for
2384         // an int or long is good enough for a managed pointer.
2385 #ifdef _LP64
</pre>
<hr />
<pre>
2418     } else {
2419       _length = len;
2420       _value._fingerprint = NEW_C_HEAP_ARRAY(int, _length, mtCode);
2421       ptr = _value._fingerprint;
2422     }
2423 
2424     // Now pack the BasicTypes with 8 per int
2425     int sig_index = 0;
2426     BasicType prev_sbt = T_ILLEGAL;
2427     int vt_count = 0;
2428     for (int index = 0; index &lt; len; index++) {
2429       int value = 0;
2430       for (int byte = 0; byte &lt; _basic_types_per_int; byte++) {
2431         int bt = 0;
2432         if (sig_index &lt; total_args_passed) {
2433           BasicType sbt = sig-&gt;at(sig_index++)._bt;
2434           if (InlineTypePassFieldsAsArgs &amp;&amp; sbt == T_INLINE_TYPE) {
2435             // Found start of inline type in signature
2436             vt_count++;
2437             if (sig_index == 1 &amp;&amp; has_ro_adapter) {
<span class="line-modified">2438               // With a ro_adapter, replace receiver value type delimiter by T_VOID to prevent matching</span>
<span class="line-modified">2439               // with other adapters that have the same value type as first argument and no receiver.</span>
2440               sbt = T_VOID;
2441             }
2442           } else if (InlineTypePassFieldsAsArgs &amp;&amp; sbt == T_VOID &amp;&amp;
2443                      prev_sbt != T_LONG &amp;&amp; prev_sbt != T_DOUBLE) {
2444             // Found end of inline type in signature
2445             vt_count--;
2446             assert(vt_count &gt;= 0, &quot;invalid vt_count&quot;);
2447           }
2448           bt = adapter_encoding(sbt, vt_count &gt; 0);
2449           prev_sbt = sbt;
2450         }
2451         assert((bt &amp; _basic_type_mask) == bt, &quot;must fit in 4 bits&quot;);
2452         value = (value &lt;&lt; _basic_type_bits) | bt;
2453       }
2454       ptr[index] = value;
2455     }
2456     assert(vt_count == 0, &quot;invalid vt_count&quot;);
2457   }
2458 
2459   ~AdapterFingerPrint() {
</pre>
<hr />
<pre>
2523  private:
2524 
2525 #ifndef PRODUCT
2526   static int _lookups; // number of calls to lookup
2527   static int _buckets; // number of buckets checked
2528   static int _equals;  // number of buckets checked with matching hash
2529   static int _hits;    // number of successful lookups
2530   static int _compact; // number of equals calls with compact signature
2531 #endif
2532 
2533   AdapterHandlerEntry* bucket(int i) {
2534     return (AdapterHandlerEntry*)BasicHashtable&lt;mtCode&gt;::bucket(i);
2535   }
2536 
2537  public:
2538   AdapterHandlerTable()
2539     : BasicHashtable&lt;mtCode&gt;(293, (DumpSharedSpaces ? sizeof(CDSAdapterHandlerEntry) : sizeof(AdapterHandlerEntry))) { }
2540 
2541   // Create a new entry suitable for insertion in the table
2542   AdapterHandlerEntry* new_entry(AdapterFingerPrint* fingerprint, address i2c_entry, address c2i_entry,
<span class="line-modified">2543                                  address c2i_value_entry, address c2i_value_ro_entry,</span>
<span class="line-modified">2544                                  address c2i_unverified_entry, address c2i_unverified_value_entry, address c2i_no_clinit_check_entry) {</span>
2545     AdapterHandlerEntry* entry = (AdapterHandlerEntry*)BasicHashtable&lt;mtCode&gt;::new_entry(fingerprint-&gt;compute_hash());
<span class="line-modified">2546     entry-&gt;init(fingerprint, i2c_entry, c2i_entry, c2i_value_entry, c2i_value_ro_entry,</span>
<span class="line-modified">2547                 c2i_unverified_entry, c2i_unverified_value_entry, c2i_no_clinit_check_entry);</span>
2548     if (DumpSharedSpaces) {
2549       ((CDSAdapterHandlerEntry*)entry)-&gt;init();
2550     }
2551     return entry;
2552   }
2553 
2554   // Insert an entry into the table
2555   void add(AdapterHandlerEntry* entry) {
2556     int index = hash_to_index(entry-&gt;hash());
2557     add_entry(index, entry);
2558   }
2559 
2560   void free_entry(AdapterHandlerEntry* entry) {
2561     entry-&gt;deallocate();
2562     BasicHashtable&lt;mtCode&gt;::free_entry(entry);
2563   }
2564 
2565   // Find a entry with the same fingerprint if it exists
2566   AdapterHandlerEntry* lookup(const GrowableArray&lt;SigEntry&gt;* sig, bool has_ro_adapter = false) {
2567     NOT_PRODUCT(_lookups++);
</pre>
<hr />
<pre>
2677 
2678 void AdapterHandlerLibrary::initialize() {
2679   if (_adapters != NULL) return;
2680   _adapters = new AdapterHandlerTable();
2681 
2682   // Create a special handler for abstract methods.  Abstract methods
2683   // are never compiled so an i2c entry is somewhat meaningless, but
2684   // throw AbstractMethodError just in case.
2685   // Pass wrong_method_abstract for the c2i transitions to return
2686   // AbstractMethodError for invalid invocations.
2687   address wrong_method_abstract = SharedRuntime::get_handle_wrong_method_abstract_stub();
2688   _abstract_method_handler = AdapterHandlerLibrary::new_entry(new AdapterFingerPrint(NULL),
2689                                                               StubRoutines::throw_AbstractMethodError_entry(),
2690                                                               wrong_method_abstract, wrong_method_abstract, wrong_method_abstract,
2691                                                               wrong_method_abstract, wrong_method_abstract);
2692 }
2693 
2694 AdapterHandlerEntry* AdapterHandlerLibrary::new_entry(AdapterFingerPrint* fingerprint,
2695                                                       address i2c_entry,
2696                                                       address c2i_entry,
<span class="line-modified">2697                                                       address c2i_value_entry,</span>
<span class="line-modified">2698                                                       address c2i_value_ro_entry,</span>
2699                                                       address c2i_unverified_entry,
<span class="line-modified">2700                                                       address c2i_unverified_value_entry,</span>
2701                                                       address c2i_no_clinit_check_entry) {
<span class="line-modified">2702   return _adapters-&gt;new_entry(fingerprint, i2c_entry, c2i_entry, c2i_value_entry, c2i_value_ro_entry, c2i_unverified_entry,</span>
<span class="line-modified">2703                               c2i_unverified_value_entry, c2i_no_clinit_check_entry);</span>
2704 }
2705 
2706 static void generate_trampoline(address trampoline, address destination) {
2707   if (*(int*)trampoline == 0) {
2708     CodeBuffer buffer(trampoline, (int)SharedRuntime::trampoline_size());
2709     MacroAssembler _masm(&amp;buffer);
2710     SharedRuntime::generate_trampoline(&amp;_masm, destination);
2711     assert(*(int*)trampoline != 0, &quot;Instruction(s) for trampoline must not be encoded as zeros.&quot;);
2712       _masm.flush();
2713 
2714     if (PrintInterpreter) {
2715       Disassembler::decode(buffer.insts_begin(), buffer.insts_end());
2716     }
2717   }
2718 }
2719 
2720 AdapterHandlerEntry* AdapterHandlerLibrary::get_adapter(const methodHandle&amp; method) {
2721   AdapterHandlerEntry* entry = get_adapter0(method);
2722   if (entry != NULL &amp;&amp; method-&gt;is_shared()) {
2723     // See comments around Method::link_method()
2724     MutexLocker mu(AdapterHandlerLibrary_lock);
2725     if (method-&gt;adapter() == NULL) {
2726       method-&gt;update_adapter_trampoline(entry);
2727     }
<span class="line-modified">2728     generate_trampoline(method-&gt;from_compiled_entry(),          entry-&gt;get_c2i_entry());</span>
<span class="line-modified">2729     generate_trampoline(method-&gt;from_compiled_value_ro_entry(), entry-&gt;get_c2i_value_ro_entry());</span>
<span class="line-modified">2730     generate_trampoline(method-&gt;from_compiled_value_entry(),    entry-&gt;get_c2i_value_entry());</span>
2731   }
2732 
2733   return entry;
2734 }
2735 
2736 
2737 CompiledEntrySignature::CompiledEntrySignature(Method* method) :
<span class="line-modified">2738   _method(method), _num_value_args(0), _has_value_recv(false),</span>
2739   _sig_cc(NULL), _sig_cc_ro(NULL), _regs(NULL), _regs_cc(NULL), _regs_cc_ro(NULL),
2740   _args_on_stack(0), _args_on_stack_cc(0), _args_on_stack_cc_ro(0),
2741   _c1_needs_stack_repair(false), _c2_needs_stack_repair(false), _has_scalarized_args(false) {
2742   _has_reserved_entries = false;
2743   _sig = new GrowableArray&lt;SigEntry&gt;(method-&gt;size_of_parameters());
2744 
2745 }
2746 
2747 int CompiledEntrySignature::compute_scalarized_cc(GrowableArray&lt;SigEntry&gt;*&amp; sig_cc, VMRegPair*&amp; regs_cc, bool scalar_receiver) {
2748   InstanceKlass* holder = _method-&gt;method_holder();
2749   sig_cc = new GrowableArray&lt;SigEntry&gt;(_method-&gt;size_of_parameters());
2750   if (!_method-&gt;is_static()) {
2751     if (holder-&gt;is_inline_klass() &amp;&amp; scalar_receiver &amp;&amp; InlineKlass::cast(holder)-&gt;can_be_passed_as_fields()) {
2752       sig_cc-&gt;appendAll(InlineKlass::cast(holder)-&gt;extended_sig());
2753     } else {
2754       SigEntry::add_entry(sig_cc, T_OBJECT);
2755     }
2756   }
2757   Thread* THREAD = Thread::current();
2758   for (SignatureStream ss(_method-&gt;signature()); !ss.at_return_type(); ss.next()) {
</pre>
<hr />
<pre>
2775   // Find index in signature that belongs to return address slot
2776   BasicType bt = T_ILLEGAL;
2777   int i = 0;
2778   for (uint off = 0; i &lt; _sig_cc-&gt;length(); ++i) {
2779     if (SigEntry::skip_value_delimiters(_sig_cc, i)) {
2780       VMReg first = _regs_cc[off++].first();
2781       if (first-&gt;is_valid() &amp;&amp; first-&gt;is_stack()) {
2782         // Select a type for the reserved entry that will end up on the stack
2783         bt = _sig_cc-&gt;at(i)._bt;
2784         if (((int)first-&gt;reg2stack() + VMRegImpl::slots_per_word) == ret_off) {
2785           break; // Index of the return address found
2786         }
2787       }
2788     }
2789   }
2790   // Insert reserved entry and re-compute calling convention
2791   SigEntry::insert_reserved_entry(_sig_cc, i, bt);
2792   return SharedRuntime::java_calling_convention(_sig_cc, _regs_cc);
2793 }
2794 
<span class="line-modified">2795 // See if we can save space by sharing the same entry for VVEP and VVEP(RO),</span>
<span class="line-modified">2796 // or the same entry for VEP and VVEP(RO).</span>
<span class="line-modified">2797 CodeOffsets::Entries CompiledEntrySignature::c1_value_ro_entry_type() const {</span>
2798   if (!has_scalarized_args()) {
<span class="line-modified">2799     // VEP/VVEP/VVEP(RO) all share the same entry. There&#39;s no packing.</span>
2800     return CodeOffsets::Verified_Entry;
2801   }
2802   if (_method-&gt;is_static()) {
<span class="line-modified">2803     // Static methods don&#39;t need VVEP(RO)</span>
2804     return CodeOffsets::Verified_Entry;
2805   }
2806 
<span class="line-modified">2807   if (has_value_recv()) {</span>
<span class="line-modified">2808     if (num_value_args() == 1) {</span>
<span class="line-modified">2809       // Share same entry for VVEP and VVEP(RO).</span>
<span class="line-modified">2810       // This is quite common: we have an instance method in a InlineKlass that has</span>
<span class="line-modified">2811       // no value args other than &lt;this&gt;.</span>
<span class="line-modified">2812       return CodeOffsets::Verified_Value_Entry;</span>
2813     } else {
<span class="line-modified">2814       assert(num_value_args() &gt; 1, &quot;must be&quot;);</span>
2815       // No sharing:
<span class="line-modified">2816       //   VVEP(RO) -- &lt;this&gt; is passed as object</span>
2817       //   VEP      -- &lt;this&gt; is passed as fields
<span class="line-modified">2818       return CodeOffsets::Verified_Value_Entry_RO;</span>
2819     }
2820   }
2821 
<span class="line-modified">2822   // Either a static method, or &lt;this&gt; is not a value type</span>
2823   if (args_on_stack_cc() != args_on_stack_cc_ro() || _has_reserved_entries) {
2824     // No sharing:
2825     // Some arguments are passed on the stack, and we have inserted reserved entries
<span class="line-modified">2826     // into the VEP, but we never insert reserved entries into the VVEP(RO).</span>
<span class="line-modified">2827     return CodeOffsets::Verified_Value_Entry_RO;</span>
2828   } else {
<span class="line-modified">2829     // Share same entry for VEP and VVEP(RO).</span>
2830     return CodeOffsets::Verified_Entry;
2831   }
2832 }
2833 
2834 
2835 void CompiledEntrySignature::compute_calling_conventions() {
2836   // Get the (non-scalarized) signature and check for inline type arguments
2837   if (!_method-&gt;is_static()) {
2838     if (_method-&gt;method_holder()-&gt;is_inline_klass() &amp;&amp; InlineKlass::cast(_method-&gt;method_holder())-&gt;can_be_passed_as_fields()) {
<span class="line-modified">2839       _has_value_recv = true;</span>
<span class="line-modified">2840       _num_value_args++;</span>
2841     }
2842     SigEntry::add_entry(_sig, T_OBJECT);
2843   }
2844   for (SignatureStream ss(_method-&gt;signature()); !ss.at_return_type(); ss.next()) {
2845     BasicType bt = ss.type();
2846     if (bt == T_INLINE_TYPE) {
2847       if (ss.as_inline_klass(_method-&gt;method_holder())-&gt;can_be_passed_as_fields()) {
<span class="line-modified">2848         _num_value_args++;</span>
2849       }
2850       bt = T_OBJECT;
2851     }
2852     SigEntry::add_entry(_sig, bt);
2853   }
<span class="line-modified">2854   if (_method-&gt;is_abstract() &amp;&amp; !has_value_arg()) {</span>
2855     return;
2856   }
2857 
2858   // Get a description of the compiled java calling convention and the largest used (VMReg) stack slot usage
2859   _regs = NEW_RESOURCE_ARRAY(VMRegPair, _sig-&gt;length());
2860   _args_on_stack = SharedRuntime::java_calling_convention(_sig, _regs);
2861 
<span class="line-modified">2862   // Now compute the scalarized calling convention if there are value types in the signature</span>
2863   _sig_cc = _sig;
2864   _sig_cc_ro = _sig;
2865   _regs_cc = _regs;
2866   _regs_cc_ro = _regs;
2867   _args_on_stack_cc = _args_on_stack;
2868   _args_on_stack_cc_ro = _args_on_stack;
2869 
<span class="line-modified">2870   if (has_value_arg() &amp;&amp; !_method-&gt;is_native()) {</span>
2871     _args_on_stack_cc = compute_scalarized_cc(_sig_cc, _regs_cc, /* scalar_receiver = */ true);
2872 
2873     _sig_cc_ro = _sig_cc;
2874     _regs_cc_ro = _regs_cc;
2875     _args_on_stack_cc_ro = _args_on_stack_cc;
<span class="line-modified">2876     if (_has_value_recv || _args_on_stack_cc &gt; _args_on_stack) {</span>
2877       // For interface calls, we need another entry point / adapter to unpack the receiver
2878       _args_on_stack_cc_ro = compute_scalarized_cc(_sig_cc_ro, _regs_cc_ro, /* scalar_receiver = */ false);
2879     }
2880 
2881     // Compute the stack extension that is required to convert between the calling conventions.
2882     // The stack slots at these offsets are occupied by the return address with the unscalarized
2883     // calling convention. Don&#39;t use them for arguments with the scalarized calling convention.
2884     int ret_off    = _args_on_stack_cc - _args_on_stack;
2885     int ret_off_ro = _args_on_stack_cc - _args_on_stack_cc_ro;
2886     assert(ret_off_ro &lt;= 0 || ret_off &gt; 0, &quot;receiver unpacking requires more stack space than expected&quot;);
2887 
2888     if (ret_off &gt; 0) {
2889       // Make sure the stack of the scalarized calling convention with the reserved
2890       // entries (2 slots each) remains 16-byte (4 slots) aligned after stack extension.
2891       int alignment = StackAlignmentInBytes / VMRegImpl::stack_slot_size;
2892       if (ret_off_ro != ret_off &amp;&amp; ret_off_ro &gt;= 0) {
2893         ret_off    += 4; // Account for two reserved entries (4 slots)
2894         ret_off_ro += 4;
2895         ret_off     = align_up(ret_off, alignment);
2896         ret_off_ro  = align_up(ret_off_ro, alignment);
</pre>
<hr />
<pre>
3088     char blob_id[256];
3089     jio_snprintf(blob_id,
3090                  sizeof(blob_id),
3091                  &quot;%s(%s)@&quot; PTR_FORMAT,
3092                  new_adapter-&gt;name(),
3093                  fingerprint-&gt;as_string(),
3094                  new_adapter-&gt;content_begin());
3095     Forte::register_stub(blob_id, new_adapter-&gt;content_begin(), new_adapter-&gt;content_end());
3096 
3097     if (JvmtiExport::should_post_dynamic_code_generated()) {
3098       JvmtiExport::post_dynamic_code_generated(blob_id, new_adapter-&gt;content_begin(), new_adapter-&gt;content_end());
3099     }
3100   }
3101   return entry;
3102 }
3103 
3104 address AdapterHandlerEntry::base_address() {
3105   address base = _i2c_entry;
3106   if (base == NULL)  base = _c2i_entry;
3107   assert(base &lt;= _c2i_entry || _c2i_entry == NULL, &quot;&quot;);
<span class="line-modified">3108   assert(base &lt;= _c2i_value_entry || _c2i_value_entry == NULL, &quot;&quot;);</span>
<span class="line-modified">3109   assert(base &lt;= _c2i_value_ro_entry || _c2i_value_ro_entry == NULL, &quot;&quot;);</span>
3110   assert(base &lt;= _c2i_unverified_entry || _c2i_unverified_entry == NULL, &quot;&quot;);
<span class="line-modified">3111   assert(base &lt;= _c2i_unverified_value_entry || _c2i_unverified_value_entry == NULL, &quot;&quot;);</span>
3112   assert(base &lt;= _c2i_no_clinit_check_entry || _c2i_no_clinit_check_entry == NULL, &quot;&quot;);
3113   return base;
3114 }
3115 
3116 void AdapterHandlerEntry::relocate(address new_base) {
3117   address old_base = base_address();
3118   assert(old_base != NULL, &quot;&quot;);
3119   ptrdiff_t delta = new_base - old_base;
3120   if (_i2c_entry != NULL)
3121     _i2c_entry += delta;
3122   if (_c2i_entry != NULL)
3123     _c2i_entry += delta;
<span class="line-modified">3124   if (_c2i_value_entry != NULL)</span>
<span class="line-modified">3125     _c2i_value_entry += delta;</span>
<span class="line-modified">3126   if (_c2i_value_ro_entry != NULL)</span>
<span class="line-modified">3127     _c2i_value_ro_entry += delta;</span>
3128   if (_c2i_unverified_entry != NULL)
3129     _c2i_unverified_entry += delta;
<span class="line-modified">3130   if (_c2i_unverified_value_entry != NULL)</span>
<span class="line-modified">3131     _c2i_unverified_value_entry += delta;</span>
3132   if (_c2i_no_clinit_check_entry != NULL)
3133     _c2i_no_clinit_check_entry += delta;
3134   assert(base_address() == new_base, &quot;&quot;);
3135 }
3136 
3137 
3138 void AdapterHandlerEntry::deallocate() {
3139   delete _fingerprint;
3140   if (_sig_cc != NULL) {
3141     delete _sig_cc;
3142   }
3143 #ifdef ASSERT
3144   FREE_C_HEAP_ARRAY(unsigned char, _saved_code);
3145 #endif
3146 }
3147 
3148 
3149 #ifdef ASSERT
3150 // Capture the code before relocation so that it can be compared
3151 // against other versions.  If the code is captured after relocation
</pre>
<hr />
<pre>
3461   while (iter.has_next()) {
3462     AdapterHandlerEntry* a = iter.next();
3463     if (b == CodeCache::find_blob(a-&gt;get_i2c_entry())) {
3464       st-&gt;print(&quot;Adapter for signature: &quot;);
3465       a-&gt;print_adapter_on(tty);
3466       return;
3467     }
3468   }
3469   assert(false, &quot;Should have found handler&quot;);
3470 }
3471 
3472 void AdapterHandlerEntry::print_adapter_on(outputStream* st) const {
3473   st-&gt;print(&quot;AHE@&quot; INTPTR_FORMAT &quot;: %s&quot;, p2i(this), fingerprint()-&gt;as_string());
3474   if (get_i2c_entry() != NULL) {
3475     st-&gt;print(&quot; i2c: &quot; INTPTR_FORMAT, p2i(get_i2c_entry()));
3476   }
3477   if (get_c2i_entry() != NULL) {
3478     st-&gt;print(&quot; c2i: &quot; INTPTR_FORMAT, p2i(get_c2i_entry()));
3479   }
3480   if (get_c2i_entry() != NULL) {
<span class="line-modified">3481     st-&gt;print(&quot; c2iVE: &quot; INTPTR_FORMAT, p2i(get_c2i_value_entry()));</span>
3482   }
3483   if (get_c2i_entry() != NULL) {
<span class="line-modified">3484     st-&gt;print(&quot; c2iVROE: &quot; INTPTR_FORMAT, p2i(get_c2i_value_ro_entry()));</span>
3485   }
3486   if (get_c2i_unverified_entry() != NULL) {
3487     st-&gt;print(&quot; c2iUE: &quot; INTPTR_FORMAT, p2i(get_c2i_unverified_entry()));
3488   }
3489   if (get_c2i_unverified_entry() != NULL) {
<span class="line-modified">3490     st-&gt;print(&quot; c2iUVE: &quot; INTPTR_FORMAT, p2i(get_c2i_unverified_value_entry()));</span>
3491   }
3492   if (get_c2i_no_clinit_check_entry() != NULL) {
3493     st-&gt;print(&quot; c2iNCI: &quot; INTPTR_FORMAT, p2i(get_c2i_no_clinit_check_entry()));
3494   }
3495   st-&gt;cr();
3496 }
3497 
3498 #if INCLUDE_CDS
3499 
3500 void CDSAdapterHandlerEntry::init() {
3501   assert(DumpSharedSpaces, &quot;used during dump time only&quot;);
3502   _c2i_entry_trampoline = (address)MetaspaceShared::misc_code_space_alloc(SharedRuntime::trampoline_size());
<span class="line-modified">3503   _c2i_value_ro_entry_trampoline = (address)MetaspaceShared::misc_code_space_alloc(SharedRuntime::trampoline_size());</span>
<span class="line-modified">3504   _c2i_value_entry_trampoline = (address)MetaspaceShared::misc_code_space_alloc(SharedRuntime::trampoline_size());</span>
3505   _adapter_trampoline = (AdapterHandlerEntry**)MetaspaceShared::misc_code_space_alloc(sizeof(AdapterHandlerEntry*));
3506 };
3507 
3508 #endif // INCLUDE_CDS
3509 
3510 
3511 #ifndef PRODUCT
3512 
3513 void AdapterHandlerLibrary::print_statistics() {
3514   _adapters-&gt;print_statistics();
3515 }
3516 
3517 #endif /* PRODUCT */
3518 
3519 JRT_LEAF(void, SharedRuntime::enable_stack_reserved_zone(JavaThread* thread))
3520   assert(thread-&gt;is_Java_thread(), &quot;Only Java threads have a stack reserved zone&quot;);
3521   if (thread-&gt;stack_reserved_zone_disabled()) {
3522   thread-&gt;enable_stack_reserved_zone();
3523   }
3524   thread-&gt;set_reserved_stack_activation(thread-&gt;stack_base());
</pre>
<hr />
<pre>
3575   return activation;
3576 }
3577 
3578 void SharedRuntime::on_slowpath_allocation_exit(JavaThread* thread) {
3579   // After any safepoint, just before going back to compiled code,
3580   // we inform the GC that we will be doing initializing writes to
3581   // this object in the future without emitting card-marks, so
3582   // GC may take any compensating steps.
3583 
3584   oop new_obj = thread-&gt;vm_result();
3585   if (new_obj == NULL) return;
3586 
3587   BarrierSet *bs = BarrierSet::barrier_set();
3588   bs-&gt;on_slowpath_allocation_exit(thread, new_obj);
3589 }
3590 
3591 // We are at a compiled code to interpreter call. We need backing
3592 // buffers for all inline type arguments. Allocate an object array to
3593 // hold them (convenient because once we&#39;re done with it we don&#39;t have
3594 // to worry about freeing it).
<span class="line-modified">3595 oop SharedRuntime::allocate_value_types_impl(JavaThread* thread, methodHandle callee, bool allocate_receiver, TRAPS) {</span>
3596   assert(InlineTypePassFieldsAsArgs, &quot;no reason to call this&quot;);
3597   ResourceMark rm;
3598 
3599   int nb_slots = 0;
3600   InstanceKlass* holder = callee-&gt;method_holder();
3601   allocate_receiver &amp;= !callee-&gt;is_static() &amp;&amp; holder-&gt;is_inline_klass();
3602   if (allocate_receiver) {
3603     nb_slots++;
3604   }
3605   for (SignatureStream ss(callee-&gt;signature()); !ss.at_return_type(); ss.next()) {
3606     if (ss.type() == T_INLINE_TYPE) {
3607       nb_slots++;
3608     }
3609   }
3610   objArrayOop array_oop = oopFactory::new_objectArray(nb_slots, CHECK_NULL);
3611   objArrayHandle array(THREAD, array_oop);
3612   int i = 0;
3613   if (allocate_receiver) {
3614     InlineKlass* vk = InlineKlass::cast(holder);
3615     oop res = vk-&gt;allocate_instance(CHECK_NULL);
3616     array-&gt;obj_at_put(i, res);
3617     i++;
3618   }
3619   for (SignatureStream ss(callee-&gt;signature()); !ss.at_return_type(); ss.next()) {
3620     if (ss.type() == T_INLINE_TYPE) {
3621       InlineKlass* vk = ss.as_inline_klass(holder);
3622       oop res = vk-&gt;allocate_instance(CHECK_NULL);
3623       array-&gt;obj_at_put(i, res);
3624       i++;
3625     }
3626   }
3627   return array();
3628 }
3629 
<span class="line-modified">3630 JRT_ENTRY(void, SharedRuntime::allocate_value_types(JavaThread* thread, Method* callee_method, bool allocate_receiver))</span>
3631   methodHandle callee(thread, callee_method);
<span class="line-modified">3632   oop array = SharedRuntime::allocate_value_types_impl(thread, callee, allocate_receiver, CHECK);</span>
3633   thread-&gt;set_vm_result(array);
3634   thread-&gt;set_vm_result_2(callee()); // TODO: required to keep callee live?
3635 JRT_END
3636 
3637 // TODO remove this once the AARCH64 dependency is gone
<span class="line-modified">3638 // Iterate over the array of heap allocated value types and apply the GC post barrier to all reference fields.</span>
3639 // This is called from the C2I adapter after inline type arguments are heap allocated and initialized.
3640 JRT_LEAF(void, SharedRuntime::apply_post_barriers(JavaThread* thread, objArrayOopDesc* array))
3641 {
3642   assert(InlineTypePassFieldsAsArgs, &quot;no reason to call this&quot;);
3643   assert(oopDesc::is_oop(array), &quot;should be oop&quot;);
3644   for (int i = 0; i &lt; array-&gt;length(); ++i) {
3645     instanceOop valueOop = (instanceOop)array-&gt;obj_at(i);
3646     InlineKlass* vk = InlineKlass::cast(valueOop-&gt;klass());
3647     if (vk-&gt;contains_oops()) {
3648       const address dst_oop_addr = ((address) (void*) valueOop);
3649       OopMapBlock* map = vk-&gt;start_of_nonstatic_oop_maps();
3650       OopMapBlock* const end = map + vk-&gt;nonstatic_oop_map_count();
3651       while (map != end) {
3652         address doop_address = dst_oop_addr + map-&gt;offset();
3653         barrier_set_cast&lt;ModRefBarrierSet&gt;(BarrierSet::barrier_set())-&gt;
3654           write_ref_array((HeapWord*) doop_address, map-&gt;count());
3655         map++;
3656       }
3657     }
3658   }
</pre>
</td>
<td>
<hr />
<pre>
1552       oop recv = callerFrame.retrieve_receiver(&amp;reg_map);
1553       Klass *recv_klass = (recv != NULL) ? recv-&gt;klass() : NULL;
1554       LinkResolver::throw_abstract_method_error(callee, recv_klass, thread);
1555       res = StubRoutines::forward_exception_entry();
1556     }
1557   JRT_BLOCK_END
1558   return res;
1559 JRT_END
1560 
1561 
1562 // resolve a static call and patch code
1563 JRT_BLOCK_ENTRY(address, SharedRuntime::resolve_static_call_C(JavaThread *thread ))
1564   methodHandle callee_method;
1565   bool caller_is_c1;
1566   JRT_BLOCK
1567     callee_method = SharedRuntime::resolve_helper(thread, false, false, &amp;caller_is_c1, CHECK_NULL);
1568     thread-&gt;set_vm_result_2(callee_method());
1569   JRT_BLOCK_END
1570   // return compiled code entry point after potential safepoints
1571   address entry = caller_is_c1 ?
<span class="line-modified">1572     callee_method-&gt;verified_inline_code_entry() : callee_method-&gt;verified_code_entry();</span>
1573   assert(entry != NULL, &quot;Jump to zero!&quot;);
1574   return entry;
1575 JRT_END
1576 
1577 
1578 // resolve virtual call and update inline cache to monomorphic
1579 JRT_BLOCK_ENTRY(address, SharedRuntime::resolve_virtual_call_C(JavaThread *thread ))
1580   methodHandle callee_method;
1581   bool caller_is_c1;
1582   JRT_BLOCK
1583     callee_method = SharedRuntime::resolve_helper(thread, true, false, &amp;caller_is_c1, CHECK_NULL);
1584     thread-&gt;set_vm_result_2(callee_method());
1585   JRT_BLOCK_END
1586   // return compiled code entry point after potential safepoints
1587   address entry = caller_is_c1 ?
<span class="line-modified">1588     callee_method-&gt;verified_inline_code_entry() : callee_method-&gt;verified_inline_ro_code_entry();</span>
1589   assert(entry != NULL, &quot;Jump to zero!&quot;);
1590   return entry;
1591 JRT_END
1592 
1593 
1594 // Resolve a virtual call that can be statically bound (e.g., always
1595 // monomorphic, so it has no inline cache).  Patch code to resolved target.
1596 JRT_BLOCK_ENTRY(address, SharedRuntime::resolve_opt_virtual_call_C(JavaThread *thread))
1597   methodHandle callee_method;
1598   bool caller_is_c1;
1599   JRT_BLOCK
1600     callee_method = SharedRuntime::resolve_helper(thread, true, true, &amp;caller_is_c1, CHECK_NULL);
1601     thread-&gt;set_vm_result_2(callee_method());
1602   JRT_BLOCK_END
1603   // return compiled code entry point after potential safepoints
1604   address entry = caller_is_c1 ?
<span class="line-modified">1605     callee_method-&gt;verified_inline_code_entry() : callee_method-&gt;verified_code_entry();</span>
1606   assert(entry != NULL, &quot;Jump to zero!&quot;);
1607   return entry;
1608 JRT_END
1609 
1610 // The handle_ic_miss_helper_internal function returns false if it failed due
1611 // to either running out of vtable stubs or ic stubs due to IC transitions
1612 // to transitional states. The needs_ic_stub_refill value will be set if
1613 // the failure was due to running out of IC stubs, in which case handle_ic_miss_helper
1614 // refills the IC stubs and tries again.
1615 bool SharedRuntime::handle_ic_miss_helper_internal(Handle receiver, CompiledMethod* caller_nm,
1616                                                    const frame&amp; caller_frame, methodHandle callee_method,
1617                                                    Bytecodes::Code bc, CallInfo&amp; call_info,
1618                                                    bool&amp; needs_ic_stub_refill, bool&amp; is_optimized, bool caller_is_c1, TRAPS) {
1619   CompiledICLocker ml(caller_nm);
1620   CompiledIC* inline_cache = CompiledIC_before(caller_nm, caller_frame.pc());
1621   bool should_be_mono = false;
1622   if (inline_cache-&gt;is_optimized()) {
1623     if (TraceCallFixup) {
1624       ResourceMark rm(THREAD);
1625       tty-&gt;print(&quot;OPTIMIZED IC miss (%s) call to&quot;, Bytecodes::name(bc));
</pre>
<hr />
<pre>
2339  private:
2340   enum {
2341     _basic_type_bits = 4,
2342     _basic_type_mask = right_n_bits(_basic_type_bits),
2343     _basic_types_per_int = BitsPerInt / _basic_type_bits,
2344     _compact_int_count = 3
2345   };
2346   // TO DO:  Consider integrating this with a more global scheme for compressing signatures.
2347   // For now, 4 bits per components (plus T_VOID gaps after double/long) is not excessive.
2348 
2349   union {
2350     int  _compact[_compact_int_count];
2351     int* _fingerprint;
2352   } _value;
2353   int _length; // A negative length indicates the fingerprint is in the compact form,
2354                // Otherwise _value._fingerprint is the array.
2355 
2356   // Remap BasicTypes that are handled equivalently by the adapters.
2357   // These are correct for the current system but someday it might be
2358   // necessary to make this mapping platform dependent.
<span class="line-modified">2359   static int adapter_encoding(BasicType in, bool is_inlinetype) {</span>
2360     switch (in) {
2361       case T_BOOLEAN:
2362       case T_BYTE:
2363       case T_SHORT:
2364       case T_CHAR: {
<span class="line-modified">2365         if (is_inlinetype) {</span>
2366           // Do not widen inline type field types
2367           assert(InlineTypePassFieldsAsArgs, &quot;must be enabled&quot;);
2368           return in;
2369         } else {
2370           // They are all promoted to T_INT in the calling convention
2371           return T_INT;
2372         }
2373       }
2374 
2375       case T_INLINE_TYPE: {
2376         // If inline types are passed as fields, return &#39;in&#39; to differentiate
2377         // between a T_INLINE_TYPE and a T_OBJECT in the signature.
2378         return InlineTypePassFieldsAsArgs ? in : adapter_encoding(T_OBJECT, false);
2379       }
2380 
2381       case T_OBJECT:
2382       case T_ARRAY:
2383         // In other words, we assume that any register good enough for
2384         // an int or long is good enough for a managed pointer.
2385 #ifdef _LP64
</pre>
<hr />
<pre>
2418     } else {
2419       _length = len;
2420       _value._fingerprint = NEW_C_HEAP_ARRAY(int, _length, mtCode);
2421       ptr = _value._fingerprint;
2422     }
2423 
2424     // Now pack the BasicTypes with 8 per int
2425     int sig_index = 0;
2426     BasicType prev_sbt = T_ILLEGAL;
2427     int vt_count = 0;
2428     for (int index = 0; index &lt; len; index++) {
2429       int value = 0;
2430       for (int byte = 0; byte &lt; _basic_types_per_int; byte++) {
2431         int bt = 0;
2432         if (sig_index &lt; total_args_passed) {
2433           BasicType sbt = sig-&gt;at(sig_index++)._bt;
2434           if (InlineTypePassFieldsAsArgs &amp;&amp; sbt == T_INLINE_TYPE) {
2435             // Found start of inline type in signature
2436             vt_count++;
2437             if (sig_index == 1 &amp;&amp; has_ro_adapter) {
<span class="line-modified">2438               // With a ro_adapter, replace receiver inline type delimiter by T_VOID to prevent matching</span>
<span class="line-modified">2439               // with other adapters that have the same inline type as first argument and no receiver.</span>
2440               sbt = T_VOID;
2441             }
2442           } else if (InlineTypePassFieldsAsArgs &amp;&amp; sbt == T_VOID &amp;&amp;
2443                      prev_sbt != T_LONG &amp;&amp; prev_sbt != T_DOUBLE) {
2444             // Found end of inline type in signature
2445             vt_count--;
2446             assert(vt_count &gt;= 0, &quot;invalid vt_count&quot;);
2447           }
2448           bt = adapter_encoding(sbt, vt_count &gt; 0);
2449           prev_sbt = sbt;
2450         }
2451         assert((bt &amp; _basic_type_mask) == bt, &quot;must fit in 4 bits&quot;);
2452         value = (value &lt;&lt; _basic_type_bits) | bt;
2453       }
2454       ptr[index] = value;
2455     }
2456     assert(vt_count == 0, &quot;invalid vt_count&quot;);
2457   }
2458 
2459   ~AdapterFingerPrint() {
</pre>
<hr />
<pre>
2523  private:
2524 
2525 #ifndef PRODUCT
2526   static int _lookups; // number of calls to lookup
2527   static int _buckets; // number of buckets checked
2528   static int _equals;  // number of buckets checked with matching hash
2529   static int _hits;    // number of successful lookups
2530   static int _compact; // number of equals calls with compact signature
2531 #endif
2532 
2533   AdapterHandlerEntry* bucket(int i) {
2534     return (AdapterHandlerEntry*)BasicHashtable&lt;mtCode&gt;::bucket(i);
2535   }
2536 
2537  public:
2538   AdapterHandlerTable()
2539     : BasicHashtable&lt;mtCode&gt;(293, (DumpSharedSpaces ? sizeof(CDSAdapterHandlerEntry) : sizeof(AdapterHandlerEntry))) { }
2540 
2541   // Create a new entry suitable for insertion in the table
2542   AdapterHandlerEntry* new_entry(AdapterFingerPrint* fingerprint, address i2c_entry, address c2i_entry,
<span class="line-modified">2543                                  address c2i_inline_entry, address c2i_inline_ro_entry,</span>
<span class="line-modified">2544                                  address c2i_unverified_entry, address c2i_unverified_inline_entry, address c2i_no_clinit_check_entry) {</span>
2545     AdapterHandlerEntry* entry = (AdapterHandlerEntry*)BasicHashtable&lt;mtCode&gt;::new_entry(fingerprint-&gt;compute_hash());
<span class="line-modified">2546     entry-&gt;init(fingerprint, i2c_entry, c2i_entry, c2i_inline_entry, c2i_inline_ro_entry,</span>
<span class="line-modified">2547                 c2i_unverified_entry, c2i_unverified_inline_entry, c2i_no_clinit_check_entry);</span>
2548     if (DumpSharedSpaces) {
2549       ((CDSAdapterHandlerEntry*)entry)-&gt;init();
2550     }
2551     return entry;
2552   }
2553 
2554   // Insert an entry into the table
2555   void add(AdapterHandlerEntry* entry) {
2556     int index = hash_to_index(entry-&gt;hash());
2557     add_entry(index, entry);
2558   }
2559 
2560   void free_entry(AdapterHandlerEntry* entry) {
2561     entry-&gt;deallocate();
2562     BasicHashtable&lt;mtCode&gt;::free_entry(entry);
2563   }
2564 
2565   // Find a entry with the same fingerprint if it exists
2566   AdapterHandlerEntry* lookup(const GrowableArray&lt;SigEntry&gt;* sig, bool has_ro_adapter = false) {
2567     NOT_PRODUCT(_lookups++);
</pre>
<hr />
<pre>
2677 
2678 void AdapterHandlerLibrary::initialize() {
2679   if (_adapters != NULL) return;
2680   _adapters = new AdapterHandlerTable();
2681 
2682   // Create a special handler for abstract methods.  Abstract methods
2683   // are never compiled so an i2c entry is somewhat meaningless, but
2684   // throw AbstractMethodError just in case.
2685   // Pass wrong_method_abstract for the c2i transitions to return
2686   // AbstractMethodError for invalid invocations.
2687   address wrong_method_abstract = SharedRuntime::get_handle_wrong_method_abstract_stub();
2688   _abstract_method_handler = AdapterHandlerLibrary::new_entry(new AdapterFingerPrint(NULL),
2689                                                               StubRoutines::throw_AbstractMethodError_entry(),
2690                                                               wrong_method_abstract, wrong_method_abstract, wrong_method_abstract,
2691                                                               wrong_method_abstract, wrong_method_abstract);
2692 }
2693 
2694 AdapterHandlerEntry* AdapterHandlerLibrary::new_entry(AdapterFingerPrint* fingerprint,
2695                                                       address i2c_entry,
2696                                                       address c2i_entry,
<span class="line-modified">2697                                                       address c2i_inline_entry,</span>
<span class="line-modified">2698                                                       address c2i_inline_ro_entry,</span>
2699                                                       address c2i_unverified_entry,
<span class="line-modified">2700                                                       address c2i_unverified_inline_entry,</span>
2701                                                       address c2i_no_clinit_check_entry) {
<span class="line-modified">2702   return _adapters-&gt;new_entry(fingerprint, i2c_entry, c2i_entry, c2i_inline_entry, c2i_inline_ro_entry, c2i_unverified_entry,</span>
<span class="line-modified">2703                               c2i_unverified_inline_entry, c2i_no_clinit_check_entry);</span>
2704 }
2705 
2706 static void generate_trampoline(address trampoline, address destination) {
2707   if (*(int*)trampoline == 0) {
2708     CodeBuffer buffer(trampoline, (int)SharedRuntime::trampoline_size());
2709     MacroAssembler _masm(&amp;buffer);
2710     SharedRuntime::generate_trampoline(&amp;_masm, destination);
2711     assert(*(int*)trampoline != 0, &quot;Instruction(s) for trampoline must not be encoded as zeros.&quot;);
2712       _masm.flush();
2713 
2714     if (PrintInterpreter) {
2715       Disassembler::decode(buffer.insts_begin(), buffer.insts_end());
2716     }
2717   }
2718 }
2719 
2720 AdapterHandlerEntry* AdapterHandlerLibrary::get_adapter(const methodHandle&amp; method) {
2721   AdapterHandlerEntry* entry = get_adapter0(method);
2722   if (entry != NULL &amp;&amp; method-&gt;is_shared()) {
2723     // See comments around Method::link_method()
2724     MutexLocker mu(AdapterHandlerLibrary_lock);
2725     if (method-&gt;adapter() == NULL) {
2726       method-&gt;update_adapter_trampoline(entry);
2727     }
<span class="line-modified">2728     generate_trampoline(method-&gt;from_compiled_entry(),           entry-&gt;get_c2i_entry());</span>
<span class="line-modified">2729     generate_trampoline(method-&gt;from_compiled_inline_ro_entry(), entry-&gt;get_c2i_inline_ro_entry());</span>
<span class="line-modified">2730     generate_trampoline(method-&gt;from_compiled_inline_entry(),    entry-&gt;get_c2i_inline_entry());</span>
2731   }
2732 
2733   return entry;
2734 }
2735 
2736 
2737 CompiledEntrySignature::CompiledEntrySignature(Method* method) :
<span class="line-modified">2738   _method(method), _num_inline_args(0), _has_inline_recv(false),</span>
2739   _sig_cc(NULL), _sig_cc_ro(NULL), _regs(NULL), _regs_cc(NULL), _regs_cc_ro(NULL),
2740   _args_on_stack(0), _args_on_stack_cc(0), _args_on_stack_cc_ro(0),
2741   _c1_needs_stack_repair(false), _c2_needs_stack_repair(false), _has_scalarized_args(false) {
2742   _has_reserved_entries = false;
2743   _sig = new GrowableArray&lt;SigEntry&gt;(method-&gt;size_of_parameters());
2744 
2745 }
2746 
2747 int CompiledEntrySignature::compute_scalarized_cc(GrowableArray&lt;SigEntry&gt;*&amp; sig_cc, VMRegPair*&amp; regs_cc, bool scalar_receiver) {
2748   InstanceKlass* holder = _method-&gt;method_holder();
2749   sig_cc = new GrowableArray&lt;SigEntry&gt;(_method-&gt;size_of_parameters());
2750   if (!_method-&gt;is_static()) {
2751     if (holder-&gt;is_inline_klass() &amp;&amp; scalar_receiver &amp;&amp; InlineKlass::cast(holder)-&gt;can_be_passed_as_fields()) {
2752       sig_cc-&gt;appendAll(InlineKlass::cast(holder)-&gt;extended_sig());
2753     } else {
2754       SigEntry::add_entry(sig_cc, T_OBJECT);
2755     }
2756   }
2757   Thread* THREAD = Thread::current();
2758   for (SignatureStream ss(_method-&gt;signature()); !ss.at_return_type(); ss.next()) {
</pre>
<hr />
<pre>
2775   // Find index in signature that belongs to return address slot
2776   BasicType bt = T_ILLEGAL;
2777   int i = 0;
2778   for (uint off = 0; i &lt; _sig_cc-&gt;length(); ++i) {
2779     if (SigEntry::skip_value_delimiters(_sig_cc, i)) {
2780       VMReg first = _regs_cc[off++].first();
2781       if (first-&gt;is_valid() &amp;&amp; first-&gt;is_stack()) {
2782         // Select a type for the reserved entry that will end up on the stack
2783         bt = _sig_cc-&gt;at(i)._bt;
2784         if (((int)first-&gt;reg2stack() + VMRegImpl::slots_per_word) == ret_off) {
2785           break; // Index of the return address found
2786         }
2787       }
2788     }
2789   }
2790   // Insert reserved entry and re-compute calling convention
2791   SigEntry::insert_reserved_entry(_sig_cc, i, bt);
2792   return SharedRuntime::java_calling_convention(_sig_cc, _regs_cc);
2793 }
2794 
<span class="line-modified">2795 // See if we can save space by sharing the same entry for VIEP and VIEP(RO),</span>
<span class="line-modified">2796 // or the same entry for VEP and VIEP(RO).</span>
<span class="line-modified">2797 CodeOffsets::Entries CompiledEntrySignature::c1_inline_ro_entry_type() const {</span>
2798   if (!has_scalarized_args()) {
<span class="line-modified">2799     // VEP/VIEP/VIEP(RO) all share the same entry. There&#39;s no packing.</span>
2800     return CodeOffsets::Verified_Entry;
2801   }
2802   if (_method-&gt;is_static()) {
<span class="line-modified">2803     // Static methods don&#39;t need VIEP(RO)</span>
2804     return CodeOffsets::Verified_Entry;
2805   }
2806 
<span class="line-modified">2807   if (has_inline_recv()) {</span>
<span class="line-modified">2808     if (num_inline_args() == 1) {</span>
<span class="line-modified">2809       // Share same entry for VIEP and VIEP(RO).</span>
<span class="line-modified">2810       // This is quite common: we have an instance method in an InlineKlass that has</span>
<span class="line-modified">2811       // no inline type args other than &lt;this&gt;.</span>
<span class="line-modified">2812       return CodeOffsets::Verified_Inline_Entry;</span>
2813     } else {
<span class="line-modified">2814       assert(num_inline_args() &gt; 1, &quot;must be&quot;);</span>
2815       // No sharing:
<span class="line-modified">2816       //   VIEP(RO) -- &lt;this&gt; is passed as object</span>
2817       //   VEP      -- &lt;this&gt; is passed as fields
<span class="line-modified">2818       return CodeOffsets::Verified_Inline_Entry_RO;</span>
2819     }
2820   }
2821 
<span class="line-modified">2822   // Either a static method, or &lt;this&gt; is not an inline type</span>
2823   if (args_on_stack_cc() != args_on_stack_cc_ro() || _has_reserved_entries) {
2824     // No sharing:
2825     // Some arguments are passed on the stack, and we have inserted reserved entries
<span class="line-modified">2826     // into the VEP, but we never insert reserved entries into the VIEP(RO).</span>
<span class="line-modified">2827     return CodeOffsets::Verified_Inline_Entry_RO;</span>
2828   } else {
<span class="line-modified">2829     // Share same entry for VEP and VIEP(RO).</span>
2830     return CodeOffsets::Verified_Entry;
2831   }
2832 }
2833 
2834 
2835 void CompiledEntrySignature::compute_calling_conventions() {
2836   // Get the (non-scalarized) signature and check for inline type arguments
2837   if (!_method-&gt;is_static()) {
2838     if (_method-&gt;method_holder()-&gt;is_inline_klass() &amp;&amp; InlineKlass::cast(_method-&gt;method_holder())-&gt;can_be_passed_as_fields()) {
<span class="line-modified">2839       _has_inline_recv = true;</span>
<span class="line-modified">2840       _num_inline_args++;</span>
2841     }
2842     SigEntry::add_entry(_sig, T_OBJECT);
2843   }
2844   for (SignatureStream ss(_method-&gt;signature()); !ss.at_return_type(); ss.next()) {
2845     BasicType bt = ss.type();
2846     if (bt == T_INLINE_TYPE) {
2847       if (ss.as_inline_klass(_method-&gt;method_holder())-&gt;can_be_passed_as_fields()) {
<span class="line-modified">2848         _num_inline_args++;</span>
2849       }
2850       bt = T_OBJECT;
2851     }
2852     SigEntry::add_entry(_sig, bt);
2853   }
<span class="line-modified">2854   if (_method-&gt;is_abstract() &amp;&amp; !has_inline_arg()) {</span>
2855     return;
2856   }
2857 
2858   // Get a description of the compiled java calling convention and the largest used (VMReg) stack slot usage
2859   _regs = NEW_RESOURCE_ARRAY(VMRegPair, _sig-&gt;length());
2860   _args_on_stack = SharedRuntime::java_calling_convention(_sig, _regs);
2861 
<span class="line-modified">2862   // Now compute the scalarized calling convention if there are inline types in the signature</span>
2863   _sig_cc = _sig;
2864   _sig_cc_ro = _sig;
2865   _regs_cc = _regs;
2866   _regs_cc_ro = _regs;
2867   _args_on_stack_cc = _args_on_stack;
2868   _args_on_stack_cc_ro = _args_on_stack;
2869 
<span class="line-modified">2870   if (has_inline_arg() &amp;&amp; !_method-&gt;is_native()) {</span>
2871     _args_on_stack_cc = compute_scalarized_cc(_sig_cc, _regs_cc, /* scalar_receiver = */ true);
2872 
2873     _sig_cc_ro = _sig_cc;
2874     _regs_cc_ro = _regs_cc;
2875     _args_on_stack_cc_ro = _args_on_stack_cc;
<span class="line-modified">2876     if (_has_inline_recv || _args_on_stack_cc &gt; _args_on_stack) {</span>
2877       // For interface calls, we need another entry point / adapter to unpack the receiver
2878       _args_on_stack_cc_ro = compute_scalarized_cc(_sig_cc_ro, _regs_cc_ro, /* scalar_receiver = */ false);
2879     }
2880 
2881     // Compute the stack extension that is required to convert between the calling conventions.
2882     // The stack slots at these offsets are occupied by the return address with the unscalarized
2883     // calling convention. Don&#39;t use them for arguments with the scalarized calling convention.
2884     int ret_off    = _args_on_stack_cc - _args_on_stack;
2885     int ret_off_ro = _args_on_stack_cc - _args_on_stack_cc_ro;
2886     assert(ret_off_ro &lt;= 0 || ret_off &gt; 0, &quot;receiver unpacking requires more stack space than expected&quot;);
2887 
2888     if (ret_off &gt; 0) {
2889       // Make sure the stack of the scalarized calling convention with the reserved
2890       // entries (2 slots each) remains 16-byte (4 slots) aligned after stack extension.
2891       int alignment = StackAlignmentInBytes / VMRegImpl::stack_slot_size;
2892       if (ret_off_ro != ret_off &amp;&amp; ret_off_ro &gt;= 0) {
2893         ret_off    += 4; // Account for two reserved entries (4 slots)
2894         ret_off_ro += 4;
2895         ret_off     = align_up(ret_off, alignment);
2896         ret_off_ro  = align_up(ret_off_ro, alignment);
</pre>
<hr />
<pre>
3088     char blob_id[256];
3089     jio_snprintf(blob_id,
3090                  sizeof(blob_id),
3091                  &quot;%s(%s)@&quot; PTR_FORMAT,
3092                  new_adapter-&gt;name(),
3093                  fingerprint-&gt;as_string(),
3094                  new_adapter-&gt;content_begin());
3095     Forte::register_stub(blob_id, new_adapter-&gt;content_begin(), new_adapter-&gt;content_end());
3096 
3097     if (JvmtiExport::should_post_dynamic_code_generated()) {
3098       JvmtiExport::post_dynamic_code_generated(blob_id, new_adapter-&gt;content_begin(), new_adapter-&gt;content_end());
3099     }
3100   }
3101   return entry;
3102 }
3103 
3104 address AdapterHandlerEntry::base_address() {
3105   address base = _i2c_entry;
3106   if (base == NULL)  base = _c2i_entry;
3107   assert(base &lt;= _c2i_entry || _c2i_entry == NULL, &quot;&quot;);
<span class="line-modified">3108   assert(base &lt;= _c2i_inline_entry || _c2i_inline_entry == NULL, &quot;&quot;);</span>
<span class="line-modified">3109   assert(base &lt;= _c2i_inline_ro_entry || _c2i_inline_ro_entry == NULL, &quot;&quot;);</span>
3110   assert(base &lt;= _c2i_unverified_entry || _c2i_unverified_entry == NULL, &quot;&quot;);
<span class="line-modified">3111   assert(base &lt;= _c2i_unverified_inline_entry || _c2i_unverified_inline_entry == NULL, &quot;&quot;);</span>
3112   assert(base &lt;= _c2i_no_clinit_check_entry || _c2i_no_clinit_check_entry == NULL, &quot;&quot;);
3113   return base;
3114 }
3115 
3116 void AdapterHandlerEntry::relocate(address new_base) {
3117   address old_base = base_address();
3118   assert(old_base != NULL, &quot;&quot;);
3119   ptrdiff_t delta = new_base - old_base;
3120   if (_i2c_entry != NULL)
3121     _i2c_entry += delta;
3122   if (_c2i_entry != NULL)
3123     _c2i_entry += delta;
<span class="line-modified">3124   if (_c2i_inline_entry != NULL)</span>
<span class="line-modified">3125     _c2i_inline_entry += delta;</span>
<span class="line-modified">3126   if (_c2i_inline_ro_entry != NULL)</span>
<span class="line-modified">3127     _c2i_inline_ro_entry += delta;</span>
3128   if (_c2i_unverified_entry != NULL)
3129     _c2i_unverified_entry += delta;
<span class="line-modified">3130   if (_c2i_unverified_inline_entry != NULL)</span>
<span class="line-modified">3131     _c2i_unverified_inline_entry += delta;</span>
3132   if (_c2i_no_clinit_check_entry != NULL)
3133     _c2i_no_clinit_check_entry += delta;
3134   assert(base_address() == new_base, &quot;&quot;);
3135 }
3136 
3137 
3138 void AdapterHandlerEntry::deallocate() {
3139   delete _fingerprint;
3140   if (_sig_cc != NULL) {
3141     delete _sig_cc;
3142   }
3143 #ifdef ASSERT
3144   FREE_C_HEAP_ARRAY(unsigned char, _saved_code);
3145 #endif
3146 }
3147 
3148 
3149 #ifdef ASSERT
3150 // Capture the code before relocation so that it can be compared
3151 // against other versions.  If the code is captured after relocation
</pre>
<hr />
<pre>
3461   while (iter.has_next()) {
3462     AdapterHandlerEntry* a = iter.next();
3463     if (b == CodeCache::find_blob(a-&gt;get_i2c_entry())) {
3464       st-&gt;print(&quot;Adapter for signature: &quot;);
3465       a-&gt;print_adapter_on(tty);
3466       return;
3467     }
3468   }
3469   assert(false, &quot;Should have found handler&quot;);
3470 }
3471 
3472 void AdapterHandlerEntry::print_adapter_on(outputStream* st) const {
3473   st-&gt;print(&quot;AHE@&quot; INTPTR_FORMAT &quot;: %s&quot;, p2i(this), fingerprint()-&gt;as_string());
3474   if (get_i2c_entry() != NULL) {
3475     st-&gt;print(&quot; i2c: &quot; INTPTR_FORMAT, p2i(get_i2c_entry()));
3476   }
3477   if (get_c2i_entry() != NULL) {
3478     st-&gt;print(&quot; c2i: &quot; INTPTR_FORMAT, p2i(get_c2i_entry()));
3479   }
3480   if (get_c2i_entry() != NULL) {
<span class="line-modified">3481     st-&gt;print(&quot; c2iVE: &quot; INTPTR_FORMAT, p2i(get_c2i_inline_entry()));</span>
3482   }
3483   if (get_c2i_entry() != NULL) {
<span class="line-modified">3484     st-&gt;print(&quot; c2iVROE: &quot; INTPTR_FORMAT, p2i(get_c2i_inline_ro_entry()));</span>
3485   }
3486   if (get_c2i_unverified_entry() != NULL) {
3487     st-&gt;print(&quot; c2iUE: &quot; INTPTR_FORMAT, p2i(get_c2i_unverified_entry()));
3488   }
3489   if (get_c2i_unverified_entry() != NULL) {
<span class="line-modified">3490     st-&gt;print(&quot; c2iUVE: &quot; INTPTR_FORMAT, p2i(get_c2i_unverified_inline_entry()));</span>
3491   }
3492   if (get_c2i_no_clinit_check_entry() != NULL) {
3493     st-&gt;print(&quot; c2iNCI: &quot; INTPTR_FORMAT, p2i(get_c2i_no_clinit_check_entry()));
3494   }
3495   st-&gt;cr();
3496 }
3497 
3498 #if INCLUDE_CDS
3499 
3500 void CDSAdapterHandlerEntry::init() {
3501   assert(DumpSharedSpaces, &quot;used during dump time only&quot;);
3502   _c2i_entry_trampoline = (address)MetaspaceShared::misc_code_space_alloc(SharedRuntime::trampoline_size());
<span class="line-modified">3503   _c2i_inline_ro_entry_trampoline = (address)MetaspaceShared::misc_code_space_alloc(SharedRuntime::trampoline_size());</span>
<span class="line-modified">3504   _c2i_inline_entry_trampoline = (address)MetaspaceShared::misc_code_space_alloc(SharedRuntime::trampoline_size());</span>
3505   _adapter_trampoline = (AdapterHandlerEntry**)MetaspaceShared::misc_code_space_alloc(sizeof(AdapterHandlerEntry*));
3506 };
3507 
3508 #endif // INCLUDE_CDS
3509 
3510 
3511 #ifndef PRODUCT
3512 
3513 void AdapterHandlerLibrary::print_statistics() {
3514   _adapters-&gt;print_statistics();
3515 }
3516 
3517 #endif /* PRODUCT */
3518 
3519 JRT_LEAF(void, SharedRuntime::enable_stack_reserved_zone(JavaThread* thread))
3520   assert(thread-&gt;is_Java_thread(), &quot;Only Java threads have a stack reserved zone&quot;);
3521   if (thread-&gt;stack_reserved_zone_disabled()) {
3522   thread-&gt;enable_stack_reserved_zone();
3523   }
3524   thread-&gt;set_reserved_stack_activation(thread-&gt;stack_base());
</pre>
<hr />
<pre>
3575   return activation;
3576 }
3577 
3578 void SharedRuntime::on_slowpath_allocation_exit(JavaThread* thread) {
3579   // After any safepoint, just before going back to compiled code,
3580   // we inform the GC that we will be doing initializing writes to
3581   // this object in the future without emitting card-marks, so
3582   // GC may take any compensating steps.
3583 
3584   oop new_obj = thread-&gt;vm_result();
3585   if (new_obj == NULL) return;
3586 
3587   BarrierSet *bs = BarrierSet::barrier_set();
3588   bs-&gt;on_slowpath_allocation_exit(thread, new_obj);
3589 }
3590 
3591 // We are at a compiled code to interpreter call. We need backing
3592 // buffers for all inline type arguments. Allocate an object array to
3593 // hold them (convenient because once we&#39;re done with it we don&#39;t have
3594 // to worry about freeing it).
<span class="line-modified">3595 oop SharedRuntime::allocate_inline_types_impl(JavaThread* thread, methodHandle callee, bool allocate_receiver, TRAPS) {</span>
3596   assert(InlineTypePassFieldsAsArgs, &quot;no reason to call this&quot;);
3597   ResourceMark rm;
3598 
3599   int nb_slots = 0;
3600   InstanceKlass* holder = callee-&gt;method_holder();
3601   allocate_receiver &amp;= !callee-&gt;is_static() &amp;&amp; holder-&gt;is_inline_klass();
3602   if (allocate_receiver) {
3603     nb_slots++;
3604   }
3605   for (SignatureStream ss(callee-&gt;signature()); !ss.at_return_type(); ss.next()) {
3606     if (ss.type() == T_INLINE_TYPE) {
3607       nb_slots++;
3608     }
3609   }
3610   objArrayOop array_oop = oopFactory::new_objectArray(nb_slots, CHECK_NULL);
3611   objArrayHandle array(THREAD, array_oop);
3612   int i = 0;
3613   if (allocate_receiver) {
3614     InlineKlass* vk = InlineKlass::cast(holder);
3615     oop res = vk-&gt;allocate_instance(CHECK_NULL);
3616     array-&gt;obj_at_put(i, res);
3617     i++;
3618   }
3619   for (SignatureStream ss(callee-&gt;signature()); !ss.at_return_type(); ss.next()) {
3620     if (ss.type() == T_INLINE_TYPE) {
3621       InlineKlass* vk = ss.as_inline_klass(holder);
3622       oop res = vk-&gt;allocate_instance(CHECK_NULL);
3623       array-&gt;obj_at_put(i, res);
3624       i++;
3625     }
3626   }
3627   return array();
3628 }
3629 
<span class="line-modified">3630 JRT_ENTRY(void, SharedRuntime::allocate_inline_types(JavaThread* thread, Method* callee_method, bool allocate_receiver))</span>
3631   methodHandle callee(thread, callee_method);
<span class="line-modified">3632   oop array = SharedRuntime::allocate_inline_types_impl(thread, callee, allocate_receiver, CHECK);</span>
3633   thread-&gt;set_vm_result(array);
3634   thread-&gt;set_vm_result_2(callee()); // TODO: required to keep callee live?
3635 JRT_END
3636 
3637 // TODO remove this once the AARCH64 dependency is gone
<span class="line-modified">3638 // Iterate over the array of heap allocated inline types and apply the GC post barrier to all reference fields.</span>
3639 // This is called from the C2I adapter after inline type arguments are heap allocated and initialized.
3640 JRT_LEAF(void, SharedRuntime::apply_post_barriers(JavaThread* thread, objArrayOopDesc* array))
3641 {
3642   assert(InlineTypePassFieldsAsArgs, &quot;no reason to call this&quot;);
3643   assert(oopDesc::is_oop(array), &quot;should be oop&quot;);
3644   for (int i = 0; i &lt; array-&gt;length(); ++i) {
3645     instanceOop valueOop = (instanceOop)array-&gt;obj_at(i);
3646     InlineKlass* vk = InlineKlass::cast(valueOop-&gt;klass());
3647     if (vk-&gt;contains_oops()) {
3648       const address dst_oop_addr = ((address) (void*) valueOop);
3649       OopMapBlock* map = vk-&gt;start_of_nonstatic_oop_maps();
3650       OopMapBlock* const end = map + vk-&gt;nonstatic_oop_map_count();
3651       while (map != end) {
3652         address doop_address = dst_oop_addr + map-&gt;offset();
3653         barrier_set_cast&lt;ModRefBarrierSet&gt;(BarrierSet::barrier_set())-&gt;
3654           write_ref_array((HeapWord*) doop_address, map-&gt;count());
3655         map++;
3656       }
3657     }
3658   }
</pre>
</td>
</tr>
</table>
<center><a href="safepoint.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="sharedRuntime.hpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>