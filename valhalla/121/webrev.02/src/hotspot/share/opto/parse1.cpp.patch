diff a/src/hotspot/share/opto/parse1.cpp b/src/hotspot/share/opto/parse1.cpp
--- a/src/hotspot/share/opto/parse1.cpp
+++ b/src/hotspot/share/opto/parse1.cpp
@@ -29,17 +29,17 @@
 #include "oops/method.hpp"
 #include "opto/addnode.hpp"
 #include "opto/c2compiler.hpp"
 #include "opto/castnode.hpp"
 #include "opto/idealGraphPrinter.hpp"
+#include "opto/inlinetypenode.hpp"
 #include "opto/locknode.hpp"
 #include "opto/memnode.hpp"
 #include "opto/opaquenode.hpp"
 #include "opto/parse.hpp"
 #include "opto/rootnode.hpp"
 #include "opto/runtime.hpp"
-#include "opto/valuetypenode.hpp"
 #include "runtime/arguments.hpp"
 #include "runtime/handles.inline.hpp"
 #include "runtime/safepointMechanism.hpp"
 #include "runtime/sharedRuntime.hpp"
 #include "utilities/bitMap.inline.hpp"
@@ -154,13 +154,13 @@
 // not a general type, but can only come from Type::get_typeflow_type.
 // The safepoint is a map which will feed an uncommon trap.
 Node* Parse::check_interpreter_type(Node* l, const Type* type,
                                     SafePointNode* &bad_type_exit) {
   const TypeOopPtr* tp = type->isa_oopptr();
-  if (type->isa_valuetype() != NULL) {
-    // The interpreter passes value types as oops
-    tp = TypeOopPtr::make_from_klass(type->value_klass());
+  if (type->isa_inlinetype() != NULL) {
+    // The interpreter passes inline types as oops
+    tp = TypeOopPtr::make_from_klass(type->inline_klass());
     tp = tp->join_speculative(TypePtr::NOTNULL)->is_oopptr();
   }
 
   // TypeFlow may assert null-ness if a type appears unloaded.
   if (type == TypePtr::NULL_PTR ||
@@ -181,12 +181,12 @@
   // toward more specific classes.  Make sure these specific classes
   // are still in effect.
   if (tp != NULL && tp->klass() != C->env()->Object_klass()) {
     // TypeFlow asserted a specific object type.  Value must have that type.
     Node* bad_type_ctrl = NULL;
-    if (tp->klass()->is_valuetype()) {
-      // Check value types for null here to prevent checkcast from adding an
+    if (tp->klass()->is_inlinetype()) {
+      // Check inline types for null here to prevent checkcast from adding an
       // exception state before the bytecode entry (use 'bad_type_ctrl' instead).
       l = null_check_oop(l, &bad_type_ctrl);
       bad_type_exit->control()->add_req(bad_type_ctrl);
     }
     l = gen_checkcast(l, makecon(TypeKlassPtr::make(tp->klass())), &bad_type_ctrl);
@@ -601,18 +601,18 @@
     if (log)  log->done("parse");
     C->set_default_node_notes(caller_nn);
     return;
   }
 
-  // Handle value type arguments
+  // Handle inline type arguments
   int arg_size_sig = tf()->domain_sig()->cnt();
   for (uint i = 0; i < (uint)arg_size_sig; i++) {
     Node* parm = map()->in(i);
     const Type* t = _gvn.type(parm);
-    if (t->is_valuetypeptr() && t->value_klass()->is_scalarizable() && !t->maybe_null()) {
-      // Create ValueTypeNode from the oop and replace the parameter
-      Node* vt = ValueTypeNode::make_from_oop(this, parm, t->value_klass());
+    if (t->is_inlinetypeptr() && t->inline_klass()->is_scalarizable() && !t->maybe_null()) {
+      // Create InlineTypeNode from the oop and replace the parameter
+      Node* vt = InlineTypeNode::make_from_oop(this, parm, t->inline_klass());
       map()->replace_edge(parm, vt);
     } else if (UseTypeSpeculation && (i == (uint)(arg_size_sig - 1)) && !is_osr_parse() &&
                method()->has_vararg() && t->isa_aryptr() != NULL && !t->is_aryptr()->is_not_null_free()) {
       // Speculate on varargs Object array being not null-free (and therefore also not flattened)
       const TypePtr* spec_type = t->speculative();
@@ -823,14 +823,14 @@
     // types will not join when we transform and push in do_exits().
     const TypeOopPtr* ret_oop_type = ret_type->isa_oopptr();
     if (ret_oop_type && !ret_oop_type->klass()->is_loaded()) {
       ret_type = TypeOopPtr::BOTTOM;
     }
-    if ((_caller->has_method() || tf()->returns_value_type_as_fields()) &&
-        ret_type->is_valuetypeptr() && ret_type->value_klass()->is_scalarizable() && !ret_type->maybe_null()) {
-      // Scalarize value type return when inlining or with multiple return values
-      ret_type = TypeValueType::make(ret_type->value_klass());
+    if ((_caller->has_method() || tf()->returns_inline_type_as_fields()) &&
+        ret_type->is_inlinetypeptr() && ret_type->inline_klass()->is_scalarizable() && !ret_type->maybe_null()) {
+      // Scalarize inline type return when inlining or with multiple return values
+      ret_type = TypeInlineType::make(ret_type->inline_klass());
     }
     int         ret_size = type2size[ret_type->basic_type()];
     Node*       ret_phi  = new PhiNode(region, ret_type);
     gvn().set_type_bottom(ret_phi);
     _exits.ensure_stack(ret_size);
@@ -866,20 +866,20 @@
   uint j = 0;
   ExtendedSignature sig_cc = ExtendedSignature(method()->get_sig_cc(), SigEntryFilter());
   for (uint i = 0; i < (uint)arg_size; i++) {
     const Type* t = tf->domain_sig()->field_at(i);
     Node* parm = NULL;
-    if (has_scalarized_args() && t->is_valuetypeptr() && !t->maybe_null()) {
-      // Value type arguments are not passed by reference: we get an argument per
-      // field of the value type. Build ValueTypeNodes from the value type arguments.
+    if (has_scalarized_args() && t->is_inlinetypeptr() && !t->maybe_null()) {
+      // Inline type arguments are not passed by reference: we get an argument per
+      // field of the inline type. Build InlineTypeNodes from the inline type arguments.
       GraphKit kit(jvms, &gvn);
       kit.set_control(map->control());
       Node* old_mem = map->memory();
-      // Use immutable memory for value type loads and restore it below
-      // TODO make sure value types are always loaded from immutable memory
+      // Use immutable memory for inline type loads and restore it below
+      // TODO make sure inline types are always loaded from immutable memory
       kit.set_all_memory(C->immutable_memory());
-      parm = ValueTypeNode::make_from_multi(&kit, start, sig_cc, t->value_klass(), j, true);
+      parm = InlineTypeNode::make_from_multi(&kit, start, sig_cc, t->inline_klass(), j, true);
       map->set_control(kit.control());
       map->set_memory(old_mem);
     } else {
       parm = gvn.transform(new ParmNode(start, j++));
       BasicType bt = t->basic_type();
@@ -925,22 +925,22 @@
   int ret_size = tf()->range_sig()->cnt() - TypeFunc::Parms;
   if (ret_size > 0) {
     kit.inc_sp(-ret_size);  // pop the return value(s)
     kit.sync_jvms();
     Node* res = kit.argument(0);
-    if (tf()->returns_value_type_as_fields()) {
-      // Multiple return values (value type fields): add as many edges
+    if (tf()->returns_inline_type_as_fields()) {
+      // Multiple return values (inline type fields): add as many edges
       // to the Return node as returned values.
-      assert(res->is_ValueType(), "what else supports multi value return?");
-      ValueTypeNode* vt = res->as_ValueType();
+      assert(res->is_InlineType(), "what else supports multi value return?");
+      InlineTypeNode* vt = res->as_InlineType();
       ret->add_req_batch(NULL, tf()->range_cc()->cnt() - TypeFunc::Parms);
       if (vt->is_allocated(&kit.gvn()) && !StressInlineTypeReturnedAsFields) {
         ret->init_req(TypeFunc::Parms, vt->get_oop());
       } else {
         ret->init_req(TypeFunc::Parms, vt->tagged_klass(kit.gvn()));
       }
-      const Array<SigEntry>* sig_array = vt->type()->value_klass()->extended_sig();
+      const Array<SigEntry>* sig_array = vt->type()->inline_klass()->extended_sig();
       GrowableArray<SigEntry> sig = GrowableArray<SigEntry>(sig_array->length());
       sig.appendAll(sig_array);
       ExtendedSignature sig_cc = ExtendedSignature(&sig, SigEntryFilter());
       uint idx = TypeFunc::Parms+1;
       vt->pass_fields(&kit, ret, sig_cc, idx);
@@ -1288,11 +1288,11 @@
       ciInstance* mirror = _method->holder()->java_mirror();
       const TypeInstPtr *t_lock = TypeInstPtr::make(mirror);
       lock_obj = makecon(t_lock);
     } else {                  // Else pass the "this" pointer,
       lock_obj = local(0);    // which is Parm0 from StartNode
-      assert(!_gvn.type(lock_obj)->make_oopptr()->can_be_value_type(), "can't be an inline type");
+      assert(!_gvn.type(lock_obj)->make_oopptr()->can_be_inline_type(), "can't be an inline type");
     }
     // Clear out dead values from the debug info.
     kill_dead_locals();
     // Build the FastLockNode
     _synch_lock = shared_lock(lock_obj);
@@ -1699,11 +1699,11 @@
 
   // Zap extra stack slots to top
   assert(sp() == target->start_sp(), "");
   clean_stack(sp());
 
-  // Check for merge conflicts involving value types
+  // Check for merge conflicts involving inline types
   JVMState* old_jvms = map()->jvms();
   int old_bci = bci();
   JVMState* tmp_jvms = old_jvms->clone_shallow(C);
   tmp_jvms->set_should_reexecute(true);
   map()->set_jvms(tmp_jvms);
@@ -1721,15 +1721,15 @@
         t = target->local_type_at(j - tmp_jvms->locoff());
       } else if (tmp_jvms->is_stk(j) && j < (uint)sp() + tmp_jvms->stkoff()) {
         t = target->stack_type_at(j - tmp_jvms->stkoff());
       }
       if (t != NULL && t != Type::BOTTOM) {
-        if (n->is_ValueType() && !t->isa_valuetype()) {
-          // Allocate value type in src block to be able to merge it with oop in target block
-          map()->set_req(j, n->as_ValueType()->buffer(this));
+        if (n->is_InlineType() && !t->isa_inlinetype()) {
+          // Allocate inline type in src block to be able to merge it with oop in target block
+          map()->set_req(j, n->as_InlineType()->buffer(this));
         }
-        assert(!t->isa_valuetype() || n->is_ValueType(), "inconsistent typeflow info");
+        assert(!t->isa_inlinetype() || n->is_InlineType(), "inconsistent typeflow info");
       }
     }
   }
   map()->set_jvms(old_jvms);
   set_parse_bci(old_bci);
@@ -1828,12 +1828,12 @@
       Node* m = map()->in(j);   // Current state of target.
       Node* n = newin->in(j);   // Incoming change to target state.
       PhiNode* phi;
       if (m->is_Phi() && m->as_Phi()->region() == r) {
         phi = m->as_Phi();
-      } else if (m->is_ValueType() && m->as_ValueType()->has_phi_inputs(r)){
-        phi = m->as_ValueType()->get_oop()->as_Phi();
+      } else if (m->is_InlineType() && m->as_InlineType()->has_phi_inputs(r)){
+        phi = m->as_InlineType()->get_oop()->as_Phi();
       } else {
         phi = NULL;
       }
       if (m != n) {             // Different; must merge
         switch (j) {
@@ -1866,20 +1866,20 @@
       // At this point, n might be top if:
       //  - there is no phi (because TypeFlow detected a conflict), or
       //  - the corresponding control edges is top (a dead incoming path)
       // It is a bug if we create a phi which sees a garbage value on a live path.
 
-      // Merging two value types?
-      if (phi != NULL && n->is_ValueType()) {
+      // Merging two inline types?
+      if (phi != NULL && n->is_InlineType()) {
         // Reload current state because it may have been updated by ensure_phi
         m = map()->in(j);
-        ValueTypeNode* vtm = m->as_ValueType(); // Current value type
-        ValueTypeNode* vtn = n->as_ValueType(); // Incoming value type
-        assert(vtm->get_oop() == phi, "Value type should have Phi input");
+        InlineTypeNode* vtm = m->as_InlineType(); // Current inline type
+        InlineTypeNode* vtn = n->as_InlineType(); // Incoming inline type
+        assert(vtm->get_oop() == phi, "Inline type should have Phi input");
         if (TraceOptoParse) {
 #ifdef ASSERT
-          tty->print_cr("\nMerging value types");
+          tty->print_cr("\nMerging inline types");
           tty->print_cr("Current:");
           vtm->dump(2);
           tty->print_cr("Incoming:");
           vtn->dump(2);
           tty->cr();
@@ -2061,12 +2061,12 @@
       }
     } else {
       if (n->is_Phi() && n->as_Phi()->region() == r) {
         assert(n->req() == pnum, "must be same size as region");
         n->add_req(NULL);
-      } else if (n->is_ValueType() && n->as_ValueType()->has_phi_inputs(r)) {
-        n->as_ValueType()->add_new_path(r);
+      } else if (n->is_InlineType() && n->as_InlineType()->has_phi_inputs(r)) {
+        n->as_InlineType()->add_new_path(r);
       }
     }
   }
 
   return pnum;
@@ -2085,11 +2085,11 @@
   if (o == top())  return NULL; // TOP always merges into TOP
 
   if (o->is_Phi() && o->as_Phi()->region() == region) {
     return o->as_Phi();
   }
-  ValueTypeBaseNode* vt = o->isa_ValueType();
+  InlineTypeBaseNode* vt = o->isa_InlineType();
   if (vt != NULL && vt->has_phi_inputs(region)) {
     return vt->get_oop()->as_Phi();
   }
 
   // Now use a Phi here for merging
@@ -2123,13 +2123,13 @@
     map->set_req(idx, top());
     return NULL;
   }
 
   if (vt != NULL) {
-    // Value types are merged by merging their field values.
-    // Create a cloned ValueTypeNode with phi inputs that
-    // represents the merged value type and update the map.
+    // Inline types are merged by merging their field values.
+    // Create a cloned InlineTypeNode with phi inputs that
+    // represents the merged inline type and update the map.
     vt = vt->clone_with_phis(&_gvn, region);
     map->set_req(idx, vt);
     return vt->get_oop()->as_Phi();
   } else {
     PhiNode* phi = PhiNode::make(region, o, t);
@@ -2343,33 +2343,33 @@
   // frame pointer is always same, already captured
   if (value != NULL) {
     Node* phi = _exits.argument(0);
     const Type* return_type = phi->bottom_type();
     const TypeOopPtr* tr = return_type->isa_oopptr();
-    if (return_type->isa_valuetype() && !Compile::current()->inlining_incrementally()) {
-      // Value type is returned as fields, make sure it is scalarized
-      if (!value->is_ValueType()) {
-        value = ValueTypeNode::make_from_oop(this, value, return_type->value_klass());
+    if (return_type->isa_inlinetype() && !Compile::current()->inlining_incrementally()) {
+      // Inline type is returned as fields, make sure it is scalarized
+      if (!value->is_InlineType()) {
+        value = InlineTypeNode::make_from_oop(this, value, return_type->inline_klass());
       }
       if (!_caller->has_method()) {
-        // Value type is returned as fields from root method, make sure all non-flattened
+        // Inline type is returned as fields from root method, make sure all non-flattened
         // fields are buffered and re-execute if allocation triggers deoptimization.
         PreserveReexecuteState preexecs(this);
-        assert(tf()->returns_value_type_as_fields(), "must be returned as fields");
+        assert(tf()->returns_inline_type_as_fields(), "must be returned as fields");
         jvms()->set_should_reexecute(true);
         inc_sp(1);
-        value = value->as_ValueType()->allocate_fields(this);
+        value = value->as_InlineType()->allocate_fields(this);
       }
-    } else if (value->is_ValueType()) {
-      // Value type is returned as oop, make sure it is buffered and re-execute
+    } else if (value->is_InlineType()) {
+      // Inline type is returned as oop, make sure it is buffered and re-execute
       // if allocation triggers deoptimization.
       PreserveReexecuteState preexecs(this);
       jvms()->set_should_reexecute(true);
       inc_sp(1);
-      value = value->as_ValueType()->buffer(this);
+      value = value->as_InlineType()->buffer(this);
       if (Compile::current()->inlining_incrementally()) {
-        value = value->as_ValueTypeBase()->allocate_fields(this);
+        value = value->as_InlineTypeBase()->allocate_fields(this);
       }
     } else if (tr && tr->isa_instptr() && tr->klass()->is_loaded() && tr->klass()->is_interface()) {
       // If returning oops to an interface-return, there is a silent free
       // cast from oop to interface allowed by the Verifier. Make it explicit here.
       const TypeInstPtr* tp = value->bottom_type()->isa_instptr();
