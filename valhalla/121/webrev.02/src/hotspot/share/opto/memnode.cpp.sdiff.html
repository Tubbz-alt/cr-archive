<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/share/opto/memnode.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="matcher.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="mulnode.cpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/share/opto/memnode.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
   6  * under the terms of the GNU General Public License version 2 only, as
   7  * published by the Free Software Foundation.
   8  *
   9  * This code is distributed in the hope that it will be useful, but WITHOUT
  10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #include &quot;precompiled.hpp&quot;

  26 #include &quot;classfile/systemDictionary.hpp&quot;
  27 #include &quot;compiler/compileLog.hpp&quot;
  28 #include &quot;gc/shared/barrierSet.hpp&quot;
  29 #include &quot;gc/shared/c2/barrierSetC2.hpp&quot;
  30 #include &quot;memory/allocation.inline.hpp&quot;
  31 #include &quot;memory/resourceArea.hpp&quot;
  32 #include &quot;oops/objArrayKlass.hpp&quot;
  33 #include &quot;opto/addnode.hpp&quot;
  34 #include &quot;opto/arraycopynode.hpp&quot;
  35 #include &quot;opto/cfgnode.hpp&quot;
  36 #include &quot;opto/compile.hpp&quot;
  37 #include &quot;opto/connode.hpp&quot;
  38 #include &quot;opto/convertnode.hpp&quot;

  39 #include &quot;opto/loopnode.hpp&quot;
  40 #include &quot;opto/machnode.hpp&quot;
  41 #include &quot;opto/matcher.hpp&quot;
  42 #include &quot;opto/memnode.hpp&quot;
  43 #include &quot;opto/mulnode.hpp&quot;
  44 #include &quot;opto/narrowptrnode.hpp&quot;
  45 #include &quot;opto/phaseX.hpp&quot;
  46 #include &quot;opto/regmask.hpp&quot;
  47 #include &quot;opto/rootnode.hpp&quot;
<span class="line-removed">  48 #include &quot;opto/valuetypenode.hpp&quot;</span>
  49 #include &quot;utilities/align.hpp&quot;
  50 #include &quot;utilities/copy.hpp&quot;
  51 #include &quot;utilities/macros.hpp&quot;
  52 #include &quot;utilities/powerOfTwo.hpp&quot;
  53 #include &quot;utilities/vmError.hpp&quot;
  54 
  55 // Portions of code courtesy of Clifford Click
  56 
  57 // Optimization - Graph Style
  58 
  59 static Node *step_through_mergemem(PhaseGVN *phase, MergeMemNode *mmem,  const TypePtr *tp, const TypePtr *adr_check, outputStream *st);
  60 
  61 //=============================================================================
  62 uint MemNode::size_of() const { return sizeof(*this); }
  63 
  64 const TypePtr *MemNode::adr_type() const {
  65   Node* adr = in(Address);
  66   if (adr == NULL)  return NULL; // node is dead
  67   const TypePtr* cross_check = NULL;
  68   DEBUG_ONLY(cross_check = _adr_type);
</pre>
<hr />
<pre>
 947     if (ac-&gt;as_ArrayCopy()-&gt;is_clonebasic()) {
 948       assert(ld_alloc != NULL, &quot;need an alloc&quot;);
 949       assert(addp-&gt;is_AddP(), &quot;address must be addp&quot;);
 950       BarrierSetC2* bs = BarrierSet::barrier_set()-&gt;barrier_set_c2();
 951       assert(bs-&gt;step_over_gc_barrier(addp-&gt;in(AddPNode::Base)) == bs-&gt;step_over_gc_barrier(ac-&gt;in(ArrayCopyNode::Dest)), &quot;strange pattern&quot;);
 952       assert(bs-&gt;step_over_gc_barrier(addp-&gt;in(AddPNode::Address)) == bs-&gt;step_over_gc_barrier(ac-&gt;in(ArrayCopyNode::Dest)), &quot;strange pattern&quot;);
 953       addp-&gt;set_req(AddPNode::Base, src);
 954       addp-&gt;set_req(AddPNode::Address, src);
 955     } else {
 956       assert(ac-&gt;as_ArrayCopy()-&gt;is_arraycopy_validated() ||
 957              ac-&gt;as_ArrayCopy()-&gt;is_copyof_validated() ||
 958              ac-&gt;as_ArrayCopy()-&gt;is_copyofrange_validated(), &quot;only supported cases&quot;);
 959       assert(addp-&gt;in(AddPNode::Base) == addp-&gt;in(AddPNode::Address), &quot;should be&quot;);
 960       addp-&gt;set_req(AddPNode::Base, src);
 961       addp-&gt;set_req(AddPNode::Address, src);
 962 
 963       const TypeAryPtr* ary_t = phase-&gt;type(in(MemNode::Address))-&gt;isa_aryptr();
 964       BasicType ary_elem  = ary_t-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
 965       uint header = arrayOopDesc::base_offset_in_bytes(ary_elem);
 966       uint shift  = exact_log2(type2aelembytes(ary_elem));
<span class="line-modified"> 967       if (ary_t-&gt;klass()-&gt;is_value_array_klass()) {</span>
<span class="line-modified"> 968         ciValueArrayKlass* vak = ary_t-&gt;klass()-&gt;as_value_array_klass();</span>
 969         shift = vak-&gt;log2_element_size();
 970       }
 971 
 972       Node* diff = phase-&gt;transform(new SubINode(ac-&gt;in(ArrayCopyNode::SrcPos), ac-&gt;in(ArrayCopyNode::DestPos)));
 973 #ifdef _LP64
 974       diff = phase-&gt;transform(new ConvI2LNode(diff));
 975 #endif
 976       diff = phase-&gt;transform(new LShiftXNode(diff, phase-&gt;intcon(shift)));
 977 
 978       Node* offset = phase-&gt;transform(new AddXNode(addp-&gt;in(AddPNode::Offset), diff));
 979       addp-&gt;set_req(AddPNode::Offset, offset);
 980     }
 981     addp = phase-&gt;transform(addp);
 982 #ifdef ASSERT
 983     const TypePtr* adr_type = phase-&gt;type(addp)-&gt;is_ptr();
 984     ld-&gt;_adr_type = adr_type;
 985 #endif
 986     ld-&gt;set_req(MemNode::Address, addp);
 987     ld-&gt;set_req(0, ctl);
 988     ld-&gt;set_req(MemNode::Memory, mem);
</pre>
<hr />
<pre>
1078         // the same pointer-and-offset that we stored to.
1079         // Casted version may carry a dependency and it is respected.
1080         // Thus, we are able to replace L by V.
1081       }
1082       // Now prove that we have a LoadQ matched to a StoreQ, for some Q.
1083       if (store_Opcode() != st-&gt;Opcode())
1084         return NULL;
1085       return st-&gt;in(MemNode::ValueIn);
1086     }
1087 
1088     // A load from a freshly-created object always returns zero.
1089     // (This can happen after LoadNode::Ideal resets the load&#39;s memory input
1090     // to find_captured_store, which returned InitializeNode::zero_memory.)
1091     if (st-&gt;is_Proj() &amp;&amp; st-&gt;in(0)-&gt;is_Allocate() &amp;&amp;
1092         (st-&gt;in(0) == ld_alloc) &amp;&amp;
1093         (ld_off &gt;= st-&gt;in(0)-&gt;as_Allocate()-&gt;minimum_header_size())) {
1094       // return a zero value for the load&#39;s basic type
1095       // (This is one of the few places where a generic PhaseTransform
1096       // can create new nodes.  Think of it as lazily manifesting
1097       // virtually pre-existing constants.)
<span class="line-modified">1098       assert(memory_type() != T_INLINE_TYPE, &quot;should not be used for value types&quot;);</span>
1099       Node* default_value = ld_alloc-&gt;in(AllocateNode::DefaultValue);
1100       if (default_value != NULL) {
1101         return default_value;
1102       }
1103       assert(ld_alloc-&gt;in(AllocateNode::RawDefaultValue) == NULL, &quot;default value may not be null&quot;);
1104       return phase-&gt;zerocon(memory_type());
1105     }
1106 
1107     // A load from an initialization barrier can match a captured store.
1108     if (st-&gt;is_Proj() &amp;&amp; st-&gt;in(0)-&gt;is_Initialize()) {
1109       InitializeNode* init = st-&gt;in(0)-&gt;as_Initialize();
1110       AllocateNode* alloc = init-&gt;allocation();
1111       if ((alloc != NULL) &amp;&amp; (alloc == ld_alloc)) {
1112         // examine a captured store value
1113         st = init-&gt;find_captured_store(ld_off, memory_size(), phase);
1114         if (st != NULL) {
1115           continue;             // take one more trip around
1116         }
1117       }
1118     }
</pre>
<hr />
<pre>
1141 //----------------------is_instance_field_load_with_local_phi------------------
1142 bool LoadNode::is_instance_field_load_with_local_phi(Node* ctrl) {
1143   if( in(Memory)-&gt;is_Phi() &amp;&amp; in(Memory)-&gt;in(0) == ctrl &amp;&amp;
1144       in(Address)-&gt;is_AddP() ) {
1145     const TypeOopPtr* t_oop = in(Address)-&gt;bottom_type()-&gt;isa_oopptr();
1146     // Only instances and boxed values.
1147     if( t_oop != NULL &amp;&amp;
1148         (t_oop-&gt;is_ptr_to_boxed_value() ||
1149          t_oop-&gt;is_known_instance_field()) &amp;&amp;
1150         t_oop-&gt;offset() != Type::OffsetBot &amp;&amp;
1151         t_oop-&gt;offset() != Type::OffsetTop) {
1152       return true;
1153     }
1154   }
1155   return false;
1156 }
1157 
1158 //------------------------------Identity---------------------------------------
1159 // Loads are identity if previous store is to same address
1160 Node* LoadNode::Identity(PhaseGVN* phase) {
<span class="line-modified">1161   // Loading from a ValueTypePtr? The ValueTypePtr has the values of</span>
1162   // all fields as input. Look for the field with matching offset.
1163   Node* addr = in(Address);
1164   intptr_t offset;
1165   Node* base = AddPNode::Ideal_base_and_offset(addr, phase, offset);
<span class="line-modified">1166   if (base != NULL &amp;&amp; base-&gt;is_ValueTypePtr() &amp;&amp; offset &gt; oopDesc::klass_offset_in_bytes()) {</span>
<span class="line-modified">1167     Node* value = base-&gt;as_ValueTypePtr()-&gt;field_value_by_offset((int)offset, true);</span>
<span class="line-modified">1168     if (value-&gt;is_ValueType()) {</span>
<span class="line-modified">1169       // Non-flattened value type field</span>
<span class="line-modified">1170       ValueTypeNode* vt = value-&gt;as_ValueType();</span>
1171       if (vt-&gt;is_allocated(phase)) {
1172         value = vt-&gt;get_oop();
1173       } else {
1174         // Not yet allocated, bail out
1175         value = NULL;
1176       }
1177     }
1178     if (value != NULL) {
1179       if (Opcode() == Op_LoadN) {
1180         // Encode oop value if we are loading a narrow oop
1181         assert(!phase-&gt;type(value)-&gt;isa_narrowoop(), &quot;should already be decoded&quot;);
1182         value = phase-&gt;transform(new EncodePNode(value, bottom_type()));
1183       }
1184       return value;
1185     }
1186   }
1187 
1188   // If the previous store-maker is the right kind of Store, and the store is
1189   // to the same address, then we are equal to the value stored.
1190   Node* mem = in(Memory);
</pre>
<hr />
<pre>
1825       }
1826     }
1827 
1828     // Don&#39;t do this for integer types. There is only potential profit if
1829     // the element type t is lower than _type; that is, for int types, if _type is
1830     // more restrictive than t.  This only happens here if one is short and the other
1831     // char (both 16 bits), and in those cases we&#39;ve made an intentional decision
1832     // to use one kind of load over the other. See AndINode::Ideal and 4965907.
1833     // Also, do not try to narrow the type for a LoadKlass, regardless of offset.
1834     //
1835     // Yes, it is possible to encounter an expression like (LoadKlass p1:(AddP x x 8))
1836     // where the _gvn.type of the AddP is wider than 8.  This occurs when an earlier
1837     // copy p0 of (AddP x x 8) has been proven equal to p1, and the p0 has been
1838     // subsumed by p1.  If p1 is on the worklist but has not yet been re-transformed,
1839     // it is possible that p1 will have a type like Foo*[int+]:NotNull*+any.
1840     // In fact, that could have been the original type of p1, and p1 could have
1841     // had an original form like p1:(AddP x x (LShiftL quux 3)), where the
1842     // expression (LShiftL quux 3) independently optimized to the constant 8.
1843     if ((t-&gt;isa_int() == NULL) &amp;&amp; (t-&gt;isa_long() == NULL)
1844         &amp;&amp; (_type-&gt;isa_vect() == NULL)
<span class="line-modified">1845         &amp;&amp; t-&gt;isa_valuetype() == NULL</span>
1846         &amp;&amp; Opcode() != Op_LoadKlass &amp;&amp; Opcode() != Op_LoadNKlass) {
1847       // t might actually be lower than _type, if _type is a unique
1848       // concrete subclass of abstract class t.
1849       if (off_beyond_header || off == Type::OffsetBot) {  // is the offset beyond the header?
1850         const Type* jt = t-&gt;join_speculative(_type);
1851         // In any case, do not allow the join, per se, to empty out the type.
1852         if (jt-&gt;empty() &amp;&amp; !t-&gt;empty()) {
1853           // This can happen if a interface-typed array narrows to a class type.
1854           jt = _type;
1855         }
1856 #ifdef ASSERT
1857         if (phase-&gt;C-&gt;eliminate_boxing() &amp;&amp; adr-&gt;is_AddP()) {
1858           // The pointers in the autobox arrays are always non-null
1859           Node* base = adr-&gt;in(AddPNode::Base);
1860           if ((base != NULL) &amp;&amp; base-&gt;is_DecodeN()) {
1861             // Get LoadN node which loads IntegerCache.cache field
1862             base = base-&gt;in(1);
1863           }
1864           if ((base != NULL) &amp;&amp; base-&gt;is_Con()) {
1865             const TypeAryPtr* base_type = base-&gt;bottom_type()-&gt;isa_aryptr();
</pre>
<hr />
<pre>
1873         return jt;
1874       }
1875     }
1876   } else if (tp-&gt;base() == Type::InstPtr) {
1877     assert( off != Type::OffsetBot ||
1878             // arrays can be cast to Objects
1879             tp-&gt;is_oopptr()-&gt;klass()-&gt;is_java_lang_Object() ||
1880             tp-&gt;is_oopptr()-&gt;klass() == ciEnv::current()-&gt;Class_klass() ||
1881             // unsafe field access may not have a constant offset
1882             C-&gt;has_unsafe_access(),
1883             &quot;Field accesses must be precise&quot; );
1884     // For oop loads, we expect the _type to be precise.
1885 
1886     const TypeInstPtr* tinst = tp-&gt;is_instptr();
1887     BasicType bt = memory_type();
1888 
1889     // Optimize loads from constant fields.
1890     ciObject* const_oop = tinst-&gt;const_oop();
1891     if (!is_mismatched_access() &amp;&amp; off != Type::OffsetBot &amp;&amp; const_oop != NULL &amp;&amp; const_oop-&gt;is_instance()) {
1892       ciType* mirror_type = const_oop-&gt;as_instance()-&gt;java_mirror_type();
<span class="line-modified">1893       if (mirror_type != NULL &amp;&amp; mirror_type-&gt;is_valuetype()) {</span>
<span class="line-modified">1894         ciValueKlass* vk = mirror_type-&gt;as_value_klass();</span>
1895         if (off == vk-&gt;default_value_offset()) {
<span class="line-modified">1896           // Loading a special hidden field that contains the oop of the default value type</span>
<span class="line-modified">1897           const Type* const_oop = TypeInstPtr::make(vk-&gt;default_value_instance());</span>
1898           return (bt == T_NARROWOOP) ? const_oop-&gt;make_narrowoop() : const_oop;
1899         }
1900       }
1901       const Type* con_type = Type::make_constant_from_field(const_oop-&gt;as_instance(), off, is_unsigned(), bt);
1902       if (con_type != NULL) {
1903         return con_type;
1904       }
1905     }
1906   } else if (tp-&gt;base() == Type::KlassPtr) {
1907     assert( off != Type::OffsetBot ||
1908             // arrays can be cast to Objects
1909             tp-&gt;is_klassptr()-&gt;klass() == NULL ||
1910             tp-&gt;is_klassptr()-&gt;klass()-&gt;is_java_lang_Object() ||
1911             // also allow array-loading from the primary supertype
1912             // array during subtype checks
1913             Opcode() == Op_LoadKlass,
1914             &quot;Field accesses must be precise&quot; );
1915     // For klass/static loads, we expect the _type to be precise
1916   } else if (tp-&gt;base() == Type::RawPtr &amp;&amp; !StressReflectiveCode) {
1917     if (adr-&gt;is_Load() &amp;&amp; off == 0) {
1918       /* With mirrors being an indirect in the Klass*
1919        * the VM is now using two loads. LoadKlass(LoadP(LoadP(Klass, mirror_offset), zero_offset))
1920        * The LoadP from the Klass has a RawPtr type (see LibraryCallKit::load_mirror_from_klass).
1921        *
1922        * So check the type and klass of the node before the LoadP.
1923        */
1924       Node* adr2 = adr-&gt;in(MemNode::Address);
1925       const TypeKlassPtr* tkls = phase-&gt;type(adr2)-&gt;isa_klassptr();
1926       if (tkls != NULL) {
1927         ciKlass* klass = tkls-&gt;klass();
1928         if (klass != NULL &amp;&amp; klass-&gt;is_loaded() &amp;&amp; tkls-&gt;klass_is_exact() &amp;&amp; tkls-&gt;offset() == in_bytes(Klass::java_mirror_offset())) {
1929           assert(adr-&gt;Opcode() == Op_LoadP, &quot;must load an oop from _java_mirror&quot;);
1930           assert(Opcode() == Op_LoadP, &quot;must load an oop from _java_mirror&quot;);
1931           return TypeInstPtr::make(klass-&gt;java_mirror());
1932         }
1933       }
1934     } else {
1935       // Check for a load of the default value offset from the InlineKlassFixedBlock:
<span class="line-modified">1936       // LoadI(LoadP(value_klass, adr_inlineklass_fixed_block_offset), default_value_offset_offset)</span>
1937       intptr_t offset = 0;
1938       Node* base = AddPNode::Ideal_base_and_offset(adr, phase, offset);
1939       if (base != NULL &amp;&amp; base-&gt;is_Load() &amp;&amp; offset == in_bytes(InlineKlass::default_value_offset_offset())) {
1940         const TypeKlassPtr* tkls = phase-&gt;type(base-&gt;in(MemNode::Address))-&gt;isa_klassptr();
<span class="line-modified">1941         if (tkls != NULL &amp;&amp; tkls-&gt;is_loaded() &amp;&amp; tkls-&gt;klass_is_exact() &amp;&amp; tkls-&gt;isa_valuetype() &amp;&amp;</span>
1942             tkls-&gt;offset() == in_bytes(InstanceKlass::adr_inlineklass_fixed_block_offset())) {
1943           assert(base-&gt;Opcode() == Op_LoadP, &quot;must load an oop from klass&quot;);
1944           assert(Opcode() == Op_LoadI, &quot;must load an int from fixed block&quot;);
<span class="line-modified">1945           return TypeInt::make(tkls-&gt;klass()-&gt;as_value_klass()-&gt;default_value_offset());</span>
1946         }
1947       }
1948     }
1949   }
1950 
1951   const TypeKlassPtr *tkls = tp-&gt;isa_klassptr();
1952   if (tkls != NULL &amp;&amp; !StressReflectiveCode) {
1953     ciKlass* klass = tkls-&gt;klass();
1954     if (tkls-&gt;is_loaded() &amp;&amp; tkls-&gt;klass_is_exact()) {
1955       // We are loading a field from a Klass metaobject whose identity
1956       // is known at compile time (the type is &quot;exact&quot; or &quot;precise&quot;).
1957       // Check for fields we know are maintained as constants by the VM.
1958       if (tkls-&gt;offset() == in_bytes(Klass::super_check_offset_offset())) {
1959         // The field is Klass::_super_check_offset.  Return its (constant) value.
1960         // (Folds up type checking code.)
1961         assert(Opcode() == Op_LoadI, &quot;must load an int from _super_check_offset&quot;);
1962         return TypeInt::make(klass-&gt;super_check_offset());
1963       }
1964       // Compute index into primary_supers array
1965       juint depth = (tkls-&gt;offset() - in_bytes(Klass::primary_supers_offset())) / sizeof(Klass*);
</pre>
<hr />
<pre>
2312     }
2313   }
2314 
2315   // Check for loading klass from an array klass
2316   const TypeKlassPtr *tkls = tp-&gt;isa_klassptr();
2317   if (tkls != NULL &amp;&amp; !StressReflectiveCode) {
2318     if (!tkls-&gt;is_loaded()) {
2319       return _type;             // Bail out if not loaded
2320     }
2321     ciKlass* klass = tkls-&gt;klass();
2322     if( klass-&gt;is_obj_array_klass() &amp;&amp;
2323         tkls-&gt;offset() == in_bytes(ObjArrayKlass::element_klass_offset())) {
2324       ciKlass* elem = klass-&gt;as_obj_array_klass()-&gt;element_klass();
2325       // // Always returning precise element type is incorrect,
2326       // // e.g., element type could be object and array may contain strings
2327       // return TypeKlassPtr::make(TypePtr::Constant, elem, 0);
2328 
2329       // The array&#39;s TypeKlassPtr was declared &#39;precise&#39; or &#39;not precise&#39;
2330       // according to the element type&#39;s subclassing.
2331       return TypeKlassPtr::make(tkls-&gt;ptr(), elem, Type::Offset(0), elem-&gt;flatten_array());
<span class="line-modified">2332     } else if (klass-&gt;is_value_array_klass() &amp;&amp;</span>
2333                tkls-&gt;offset() == in_bytes(ObjArrayKlass::element_klass_offset())) {
<span class="line-modified">2334       ciKlass* elem = klass-&gt;as_value_array_klass()-&gt;element_klass();</span>
2335       return TypeKlassPtr::make(tkls-&gt;ptr(), elem, Type::Offset(0), /* flat_array= */ true);
2336     }
2337     if( klass-&gt;is_instance_klass() &amp;&amp; tkls-&gt;klass_is_exact() &amp;&amp;
2338         tkls-&gt;offset() == in_bytes(Klass::super_offset())) {
2339       ciKlass* sup = klass-&gt;as_instance_klass()-&gt;super();
2340       // The field is Klass::_super.  Return its (constant) value.
2341       // (Folds up the 2nd indirection in aClassConstant.getSuperClass().)
2342       return sup ? TypeKlassPtr::make(sup) : TypePtr::NULL_PTR;
2343     }
2344   }
2345 
2346   // Bailout case
2347   return LoadNode::Value(phase);
2348 }
2349 
2350 //------------------------------Identity---------------------------------------
2351 // To clean up reflective code, simplify k.java_mirror.as_klass to plain k.
2352 // Also feed through the klass in Allocate(...klass...)._klass.
2353 Node* LoadKlassNode::Identity(PhaseGVN* phase) {
2354   return klass_identity_common(phase);
</pre>
<hr />
<pre>
2584   //return (uintptr_t)in(Control) + (uintptr_t)in(Memory) + (uintptr_t)in(Address) + (uintptr_t)in(ValueIn);
2585 
2586   // Since they are not commoned, do not hash them:
2587   return NO_HASH;
2588 }
2589 
2590 //------------------------------Ideal------------------------------------------
2591 // Change back-to-back Store(, p, x) -&gt; Store(m, p, y) to Store(m, p, x).
2592 // When a store immediately follows a relevant allocation/initialization,
2593 // try to capture it into the initialization, or hoist it above.
2594 Node *StoreNode::Ideal(PhaseGVN *phase, bool can_reshape) {
2595   Node* p = MemNode::Ideal_common(phase, can_reshape);
2596   if (p)  return (p == NodeSentinel) ? NULL : p;
2597 
2598   Node* mem     = in(MemNode::Memory);
2599   Node* address = in(MemNode::Address);
2600   // Back-to-back stores to same address?  Fold em up.  Generally
2601   // unsafe if I have intervening uses...  Also disallowed for StoreCM
2602   // since they must follow each StoreP operation.  Redundant StoreCMs
2603   // are eliminated just before matching in final_graph_reshape.
<span class="line-modified">2604   if (phase-&gt;C-&gt;get_adr_type(phase-&gt;C-&gt;get_alias_index(adr_type())) != TypeAryPtr::VALUES) {</span>
2605     Node* st = mem;
2606     // If Store &#39;st&#39; has more than one use, we cannot fold &#39;st&#39; away.
2607     // For example, &#39;st&#39; might be the final state at a conditional
2608     // return.  Or, &#39;st&#39; might be used by some node which is live at
2609     // the same time &#39;st&#39; is live, which might be unschedulable.  So,
2610     // require exactly ONE user until such time as we clone &#39;mem&#39; for
2611     // each of &#39;mem&#39;s uses (thus making the exactly-1-user-rule hold
2612     // true).
2613     while (st-&gt;is_Store() &amp;&amp; st-&gt;outcnt() == 1 &amp;&amp; st-&gt;Opcode() != Op_StoreCM) {
2614       // Looking at a dead closed cycle of memory?
2615       assert(st != st-&gt;in(MemNode::Memory), &quot;dead loop in StoreNode::Ideal&quot;);
2616       assert(Opcode() == st-&gt;Opcode() ||
2617              st-&gt;Opcode() == Op_StoreVector ||
2618              Opcode() == Op_StoreVector ||
2619              phase-&gt;C-&gt;get_alias_index(adr_type()) == Compile::AliasIdxRaw ||
2620              (Opcode() == Op_StoreL &amp;&amp; st-&gt;Opcode() == Op_StoreI) || // expanded ClearArrayNode
2621              (Opcode() == Op_StoreI &amp;&amp; st-&gt;Opcode() == Op_StoreL) || // initialization by arraycopy
2622              (Opcode() == Op_StoreL &amp;&amp; st-&gt;Opcode() == Op_StoreN) ||
2623              (is_mismatched_access() || st-&gt;as_Store()-&gt;is_mismatched_access()),
2624              &quot;no mismatched stores, except on raw memory: %s %s&quot;, NodeClassNames[Opcode()], NodeClassNames[st-&gt;Opcode()]);
</pre>
<hr />
<pre>
2692       val-&gt;as_Load()-&gt;store_Opcode() == Opcode()) {
2693     result = mem;
2694   }
2695 
2696   // Two stores in a row of the same value?
2697   if (result == this &amp;&amp;
2698       mem-&gt;is_Store() &amp;&amp;
2699       mem-&gt;in(MemNode::Address)-&gt;eqv_uncast(adr) &amp;&amp;
2700       mem-&gt;in(MemNode::ValueIn)-&gt;eqv_uncast(val) &amp;&amp;
2701       mem-&gt;Opcode() == Opcode()) {
2702     result = mem;
2703   }
2704 
2705   // Store of zero anywhere into a freshly-allocated object?
2706   // Then the store is useless.
2707   // (It must already have been captured by the InitializeNode.)
2708   if (result == this &amp;&amp; ReduceFieldZeroing) {
2709     // a newly allocated object is already all-zeroes everywhere
2710     if (mem-&gt;is_Proj() &amp;&amp; mem-&gt;in(0)-&gt;is_Allocate() &amp;&amp;
2711         (phase-&gt;type(val)-&gt;is_zero_type() || mem-&gt;in(0)-&gt;in(AllocateNode::DefaultValue) == val)) {
<span class="line-modified">2712       assert(!phase-&gt;type(val)-&gt;is_zero_type() || mem-&gt;in(0)-&gt;in(AllocateNode::DefaultValue) == NULL, &quot;storing null to value array is forbidden&quot;);</span>
2713       result = mem;
2714     }
2715 
2716     if (result == this) {
2717       // the store may also apply to zero-bits in an earlier object
2718       Node* prev_mem = find_previous_store(phase);
2719       // Steps (a), (b):  Walk past independent stores to find an exact match.
2720       if (prev_mem != NULL) {
2721         Node* prev_val = can_see_stored_value(prev_mem, phase);
2722         if (prev_val != NULL &amp;&amp; phase-&gt;eqv(prev_val, val)) {
2723           // prev_val and val might differ by a cast; it would be good
2724           // to keep the more informative of the two.
2725           if (phase-&gt;type(val)-&gt;is_zero_type()) {
2726             result = mem;
2727           } else if (prev_mem-&gt;is_Proj() &amp;&amp; prev_mem-&gt;in(0)-&gt;is_Initialize()) {
2728             InitializeNode* init = prev_mem-&gt;in(0)-&gt;as_Initialize();
2729             AllocateNode* alloc = init-&gt;allocation();
2730             if (alloc != NULL &amp;&amp; alloc-&gt;in(AllocateNode::DefaultValue) == val) {
2731               result = mem;
2732             }
</pre>
</td>
<td>
<hr />
<pre>
   6  * under the terms of the GNU General Public License version 2 only, as
   7  * published by the Free Software Foundation.
   8  *
   9  * This code is distributed in the hope that it will be useful, but WITHOUT
  10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #include &quot;precompiled.hpp&quot;
<span class="line-added">  26 #include &quot;ci/ciFlatArrayKlass.hpp&quot;</span>
  27 #include &quot;classfile/systemDictionary.hpp&quot;
  28 #include &quot;compiler/compileLog.hpp&quot;
  29 #include &quot;gc/shared/barrierSet.hpp&quot;
  30 #include &quot;gc/shared/c2/barrierSetC2.hpp&quot;
  31 #include &quot;memory/allocation.inline.hpp&quot;
  32 #include &quot;memory/resourceArea.hpp&quot;
  33 #include &quot;oops/objArrayKlass.hpp&quot;
  34 #include &quot;opto/addnode.hpp&quot;
  35 #include &quot;opto/arraycopynode.hpp&quot;
  36 #include &quot;opto/cfgnode.hpp&quot;
  37 #include &quot;opto/compile.hpp&quot;
  38 #include &quot;opto/connode.hpp&quot;
  39 #include &quot;opto/convertnode.hpp&quot;
<span class="line-added">  40 #include &quot;opto/inlinetypenode.hpp&quot;</span>
  41 #include &quot;opto/loopnode.hpp&quot;
  42 #include &quot;opto/machnode.hpp&quot;
  43 #include &quot;opto/matcher.hpp&quot;
  44 #include &quot;opto/memnode.hpp&quot;
  45 #include &quot;opto/mulnode.hpp&quot;
  46 #include &quot;opto/narrowptrnode.hpp&quot;
  47 #include &quot;opto/phaseX.hpp&quot;
  48 #include &quot;opto/regmask.hpp&quot;
  49 #include &quot;opto/rootnode.hpp&quot;

  50 #include &quot;utilities/align.hpp&quot;
  51 #include &quot;utilities/copy.hpp&quot;
  52 #include &quot;utilities/macros.hpp&quot;
  53 #include &quot;utilities/powerOfTwo.hpp&quot;
  54 #include &quot;utilities/vmError.hpp&quot;
  55 
  56 // Portions of code courtesy of Clifford Click
  57 
  58 // Optimization - Graph Style
  59 
  60 static Node *step_through_mergemem(PhaseGVN *phase, MergeMemNode *mmem,  const TypePtr *tp, const TypePtr *adr_check, outputStream *st);
  61 
  62 //=============================================================================
  63 uint MemNode::size_of() const { return sizeof(*this); }
  64 
  65 const TypePtr *MemNode::adr_type() const {
  66   Node* adr = in(Address);
  67   if (adr == NULL)  return NULL; // node is dead
  68   const TypePtr* cross_check = NULL;
  69   DEBUG_ONLY(cross_check = _adr_type);
</pre>
<hr />
<pre>
 948     if (ac-&gt;as_ArrayCopy()-&gt;is_clonebasic()) {
 949       assert(ld_alloc != NULL, &quot;need an alloc&quot;);
 950       assert(addp-&gt;is_AddP(), &quot;address must be addp&quot;);
 951       BarrierSetC2* bs = BarrierSet::barrier_set()-&gt;barrier_set_c2();
 952       assert(bs-&gt;step_over_gc_barrier(addp-&gt;in(AddPNode::Base)) == bs-&gt;step_over_gc_barrier(ac-&gt;in(ArrayCopyNode::Dest)), &quot;strange pattern&quot;);
 953       assert(bs-&gt;step_over_gc_barrier(addp-&gt;in(AddPNode::Address)) == bs-&gt;step_over_gc_barrier(ac-&gt;in(ArrayCopyNode::Dest)), &quot;strange pattern&quot;);
 954       addp-&gt;set_req(AddPNode::Base, src);
 955       addp-&gt;set_req(AddPNode::Address, src);
 956     } else {
 957       assert(ac-&gt;as_ArrayCopy()-&gt;is_arraycopy_validated() ||
 958              ac-&gt;as_ArrayCopy()-&gt;is_copyof_validated() ||
 959              ac-&gt;as_ArrayCopy()-&gt;is_copyofrange_validated(), &quot;only supported cases&quot;);
 960       assert(addp-&gt;in(AddPNode::Base) == addp-&gt;in(AddPNode::Address), &quot;should be&quot;);
 961       addp-&gt;set_req(AddPNode::Base, src);
 962       addp-&gt;set_req(AddPNode::Address, src);
 963 
 964       const TypeAryPtr* ary_t = phase-&gt;type(in(MemNode::Address))-&gt;isa_aryptr();
 965       BasicType ary_elem  = ary_t-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
 966       uint header = arrayOopDesc::base_offset_in_bytes(ary_elem);
 967       uint shift  = exact_log2(type2aelembytes(ary_elem));
<span class="line-modified"> 968       if (ary_t-&gt;klass()-&gt;is_flat_array_klass()) {</span>
<span class="line-modified"> 969         ciFlatArrayKlass* vak = ary_t-&gt;klass()-&gt;as_flat_array_klass();</span>
 970         shift = vak-&gt;log2_element_size();
 971       }
 972 
 973       Node* diff = phase-&gt;transform(new SubINode(ac-&gt;in(ArrayCopyNode::SrcPos), ac-&gt;in(ArrayCopyNode::DestPos)));
 974 #ifdef _LP64
 975       diff = phase-&gt;transform(new ConvI2LNode(diff));
 976 #endif
 977       diff = phase-&gt;transform(new LShiftXNode(diff, phase-&gt;intcon(shift)));
 978 
 979       Node* offset = phase-&gt;transform(new AddXNode(addp-&gt;in(AddPNode::Offset), diff));
 980       addp-&gt;set_req(AddPNode::Offset, offset);
 981     }
 982     addp = phase-&gt;transform(addp);
 983 #ifdef ASSERT
 984     const TypePtr* adr_type = phase-&gt;type(addp)-&gt;is_ptr();
 985     ld-&gt;_adr_type = adr_type;
 986 #endif
 987     ld-&gt;set_req(MemNode::Address, addp);
 988     ld-&gt;set_req(0, ctl);
 989     ld-&gt;set_req(MemNode::Memory, mem);
</pre>
<hr />
<pre>
1079         // the same pointer-and-offset that we stored to.
1080         // Casted version may carry a dependency and it is respected.
1081         // Thus, we are able to replace L by V.
1082       }
1083       // Now prove that we have a LoadQ matched to a StoreQ, for some Q.
1084       if (store_Opcode() != st-&gt;Opcode())
1085         return NULL;
1086       return st-&gt;in(MemNode::ValueIn);
1087     }
1088 
1089     // A load from a freshly-created object always returns zero.
1090     // (This can happen after LoadNode::Ideal resets the load&#39;s memory input
1091     // to find_captured_store, which returned InitializeNode::zero_memory.)
1092     if (st-&gt;is_Proj() &amp;&amp; st-&gt;in(0)-&gt;is_Allocate() &amp;&amp;
1093         (st-&gt;in(0) == ld_alloc) &amp;&amp;
1094         (ld_off &gt;= st-&gt;in(0)-&gt;as_Allocate()-&gt;minimum_header_size())) {
1095       // return a zero value for the load&#39;s basic type
1096       // (This is one of the few places where a generic PhaseTransform
1097       // can create new nodes.  Think of it as lazily manifesting
1098       // virtually pre-existing constants.)
<span class="line-modified">1099       assert(memory_type() != T_INLINE_TYPE, &quot;should not be used for inline types&quot;);</span>
1100       Node* default_value = ld_alloc-&gt;in(AllocateNode::DefaultValue);
1101       if (default_value != NULL) {
1102         return default_value;
1103       }
1104       assert(ld_alloc-&gt;in(AllocateNode::RawDefaultValue) == NULL, &quot;default value may not be null&quot;);
1105       return phase-&gt;zerocon(memory_type());
1106     }
1107 
1108     // A load from an initialization barrier can match a captured store.
1109     if (st-&gt;is_Proj() &amp;&amp; st-&gt;in(0)-&gt;is_Initialize()) {
1110       InitializeNode* init = st-&gt;in(0)-&gt;as_Initialize();
1111       AllocateNode* alloc = init-&gt;allocation();
1112       if ((alloc != NULL) &amp;&amp; (alloc == ld_alloc)) {
1113         // examine a captured store value
1114         st = init-&gt;find_captured_store(ld_off, memory_size(), phase);
1115         if (st != NULL) {
1116           continue;             // take one more trip around
1117         }
1118       }
1119     }
</pre>
<hr />
<pre>
1142 //----------------------is_instance_field_load_with_local_phi------------------
1143 bool LoadNode::is_instance_field_load_with_local_phi(Node* ctrl) {
1144   if( in(Memory)-&gt;is_Phi() &amp;&amp; in(Memory)-&gt;in(0) == ctrl &amp;&amp;
1145       in(Address)-&gt;is_AddP() ) {
1146     const TypeOopPtr* t_oop = in(Address)-&gt;bottom_type()-&gt;isa_oopptr();
1147     // Only instances and boxed values.
1148     if( t_oop != NULL &amp;&amp;
1149         (t_oop-&gt;is_ptr_to_boxed_value() ||
1150          t_oop-&gt;is_known_instance_field()) &amp;&amp;
1151         t_oop-&gt;offset() != Type::OffsetBot &amp;&amp;
1152         t_oop-&gt;offset() != Type::OffsetTop) {
1153       return true;
1154     }
1155   }
1156   return false;
1157 }
1158 
1159 //------------------------------Identity---------------------------------------
1160 // Loads are identity if previous store is to same address
1161 Node* LoadNode::Identity(PhaseGVN* phase) {
<span class="line-modified">1162   // Loading from an InlineTypePtr? The InlineTypePtr has the values of</span>
1163   // all fields as input. Look for the field with matching offset.
1164   Node* addr = in(Address);
1165   intptr_t offset;
1166   Node* base = AddPNode::Ideal_base_and_offset(addr, phase, offset);
<span class="line-modified">1167   if (base != NULL &amp;&amp; base-&gt;is_InlineTypePtr() &amp;&amp; offset &gt; oopDesc::klass_offset_in_bytes()) {</span>
<span class="line-modified">1168     Node* value = base-&gt;as_InlineTypePtr()-&gt;field_value_by_offset((int)offset, true);</span>
<span class="line-modified">1169     if (value-&gt;is_InlineType()) {</span>
<span class="line-modified">1170       // Non-flattened inline type field</span>
<span class="line-modified">1171       InlineTypeNode* vt = value-&gt;as_InlineType();</span>
1172       if (vt-&gt;is_allocated(phase)) {
1173         value = vt-&gt;get_oop();
1174       } else {
1175         // Not yet allocated, bail out
1176         value = NULL;
1177       }
1178     }
1179     if (value != NULL) {
1180       if (Opcode() == Op_LoadN) {
1181         // Encode oop value if we are loading a narrow oop
1182         assert(!phase-&gt;type(value)-&gt;isa_narrowoop(), &quot;should already be decoded&quot;);
1183         value = phase-&gt;transform(new EncodePNode(value, bottom_type()));
1184       }
1185       return value;
1186     }
1187   }
1188 
1189   // If the previous store-maker is the right kind of Store, and the store is
1190   // to the same address, then we are equal to the value stored.
1191   Node* mem = in(Memory);
</pre>
<hr />
<pre>
1826       }
1827     }
1828 
1829     // Don&#39;t do this for integer types. There is only potential profit if
1830     // the element type t is lower than _type; that is, for int types, if _type is
1831     // more restrictive than t.  This only happens here if one is short and the other
1832     // char (both 16 bits), and in those cases we&#39;ve made an intentional decision
1833     // to use one kind of load over the other. See AndINode::Ideal and 4965907.
1834     // Also, do not try to narrow the type for a LoadKlass, regardless of offset.
1835     //
1836     // Yes, it is possible to encounter an expression like (LoadKlass p1:(AddP x x 8))
1837     // where the _gvn.type of the AddP is wider than 8.  This occurs when an earlier
1838     // copy p0 of (AddP x x 8) has been proven equal to p1, and the p0 has been
1839     // subsumed by p1.  If p1 is on the worklist but has not yet been re-transformed,
1840     // it is possible that p1 will have a type like Foo*[int+]:NotNull*+any.
1841     // In fact, that could have been the original type of p1, and p1 could have
1842     // had an original form like p1:(AddP x x (LShiftL quux 3)), where the
1843     // expression (LShiftL quux 3) independently optimized to the constant 8.
1844     if ((t-&gt;isa_int() == NULL) &amp;&amp; (t-&gt;isa_long() == NULL)
1845         &amp;&amp; (_type-&gt;isa_vect() == NULL)
<span class="line-modified">1846         &amp;&amp; t-&gt;isa_inlinetype() == NULL</span>
1847         &amp;&amp; Opcode() != Op_LoadKlass &amp;&amp; Opcode() != Op_LoadNKlass) {
1848       // t might actually be lower than _type, if _type is a unique
1849       // concrete subclass of abstract class t.
1850       if (off_beyond_header || off == Type::OffsetBot) {  // is the offset beyond the header?
1851         const Type* jt = t-&gt;join_speculative(_type);
1852         // In any case, do not allow the join, per se, to empty out the type.
1853         if (jt-&gt;empty() &amp;&amp; !t-&gt;empty()) {
1854           // This can happen if a interface-typed array narrows to a class type.
1855           jt = _type;
1856         }
1857 #ifdef ASSERT
1858         if (phase-&gt;C-&gt;eliminate_boxing() &amp;&amp; adr-&gt;is_AddP()) {
1859           // The pointers in the autobox arrays are always non-null
1860           Node* base = adr-&gt;in(AddPNode::Base);
1861           if ((base != NULL) &amp;&amp; base-&gt;is_DecodeN()) {
1862             // Get LoadN node which loads IntegerCache.cache field
1863             base = base-&gt;in(1);
1864           }
1865           if ((base != NULL) &amp;&amp; base-&gt;is_Con()) {
1866             const TypeAryPtr* base_type = base-&gt;bottom_type()-&gt;isa_aryptr();
</pre>
<hr />
<pre>
1874         return jt;
1875       }
1876     }
1877   } else if (tp-&gt;base() == Type::InstPtr) {
1878     assert( off != Type::OffsetBot ||
1879             // arrays can be cast to Objects
1880             tp-&gt;is_oopptr()-&gt;klass()-&gt;is_java_lang_Object() ||
1881             tp-&gt;is_oopptr()-&gt;klass() == ciEnv::current()-&gt;Class_klass() ||
1882             // unsafe field access may not have a constant offset
1883             C-&gt;has_unsafe_access(),
1884             &quot;Field accesses must be precise&quot; );
1885     // For oop loads, we expect the _type to be precise.
1886 
1887     const TypeInstPtr* tinst = tp-&gt;is_instptr();
1888     BasicType bt = memory_type();
1889 
1890     // Optimize loads from constant fields.
1891     ciObject* const_oop = tinst-&gt;const_oop();
1892     if (!is_mismatched_access() &amp;&amp; off != Type::OffsetBot &amp;&amp; const_oop != NULL &amp;&amp; const_oop-&gt;is_instance()) {
1893       ciType* mirror_type = const_oop-&gt;as_instance()-&gt;java_mirror_type();
<span class="line-modified">1894       if (mirror_type != NULL &amp;&amp; mirror_type-&gt;is_inlinetype()) {</span>
<span class="line-modified">1895         ciInlineKlass* vk = mirror_type-&gt;as_inline_klass();</span>
1896         if (off == vk-&gt;default_value_offset()) {
<span class="line-modified">1897           // Loading a special hidden field that contains the oop of the default inline type</span>
<span class="line-modified">1898           const Type* const_oop = TypeInstPtr::make(vk-&gt;default_instance());</span>
1899           return (bt == T_NARROWOOP) ? const_oop-&gt;make_narrowoop() : const_oop;
1900         }
1901       }
1902       const Type* con_type = Type::make_constant_from_field(const_oop-&gt;as_instance(), off, is_unsigned(), bt);
1903       if (con_type != NULL) {
1904         return con_type;
1905       }
1906     }
1907   } else if (tp-&gt;base() == Type::KlassPtr) {
1908     assert( off != Type::OffsetBot ||
1909             // arrays can be cast to Objects
1910             tp-&gt;is_klassptr()-&gt;klass() == NULL ||
1911             tp-&gt;is_klassptr()-&gt;klass()-&gt;is_java_lang_Object() ||
1912             // also allow array-loading from the primary supertype
1913             // array during subtype checks
1914             Opcode() == Op_LoadKlass,
1915             &quot;Field accesses must be precise&quot; );
1916     // For klass/static loads, we expect the _type to be precise
1917   } else if (tp-&gt;base() == Type::RawPtr &amp;&amp; !StressReflectiveCode) {
1918     if (adr-&gt;is_Load() &amp;&amp; off == 0) {
1919       /* With mirrors being an indirect in the Klass*
1920        * the VM is now using two loads. LoadKlass(LoadP(LoadP(Klass, mirror_offset), zero_offset))
1921        * The LoadP from the Klass has a RawPtr type (see LibraryCallKit::load_mirror_from_klass).
1922        *
1923        * So check the type and klass of the node before the LoadP.
1924        */
1925       Node* adr2 = adr-&gt;in(MemNode::Address);
1926       const TypeKlassPtr* tkls = phase-&gt;type(adr2)-&gt;isa_klassptr();
1927       if (tkls != NULL) {
1928         ciKlass* klass = tkls-&gt;klass();
1929         if (klass != NULL &amp;&amp; klass-&gt;is_loaded() &amp;&amp; tkls-&gt;klass_is_exact() &amp;&amp; tkls-&gt;offset() == in_bytes(Klass::java_mirror_offset())) {
1930           assert(adr-&gt;Opcode() == Op_LoadP, &quot;must load an oop from _java_mirror&quot;);
1931           assert(Opcode() == Op_LoadP, &quot;must load an oop from _java_mirror&quot;);
1932           return TypeInstPtr::make(klass-&gt;java_mirror());
1933         }
1934       }
1935     } else {
1936       // Check for a load of the default value offset from the InlineKlassFixedBlock:
<span class="line-modified">1937       // LoadI(LoadP(inline_klass, adr_inlineklass_fixed_block_offset), default_value_offset_offset)</span>
1938       intptr_t offset = 0;
1939       Node* base = AddPNode::Ideal_base_and_offset(adr, phase, offset);
1940       if (base != NULL &amp;&amp; base-&gt;is_Load() &amp;&amp; offset == in_bytes(InlineKlass::default_value_offset_offset())) {
1941         const TypeKlassPtr* tkls = phase-&gt;type(base-&gt;in(MemNode::Address))-&gt;isa_klassptr();
<span class="line-modified">1942         if (tkls != NULL &amp;&amp; tkls-&gt;is_loaded() &amp;&amp; tkls-&gt;klass_is_exact() &amp;&amp; tkls-&gt;isa_inlinetype() &amp;&amp;</span>
1943             tkls-&gt;offset() == in_bytes(InstanceKlass::adr_inlineklass_fixed_block_offset())) {
1944           assert(base-&gt;Opcode() == Op_LoadP, &quot;must load an oop from klass&quot;);
1945           assert(Opcode() == Op_LoadI, &quot;must load an int from fixed block&quot;);
<span class="line-modified">1946           return TypeInt::make(tkls-&gt;klass()-&gt;as_inline_klass()-&gt;default_value_offset());</span>
1947         }
1948       }
1949     }
1950   }
1951 
1952   const TypeKlassPtr *tkls = tp-&gt;isa_klassptr();
1953   if (tkls != NULL &amp;&amp; !StressReflectiveCode) {
1954     ciKlass* klass = tkls-&gt;klass();
1955     if (tkls-&gt;is_loaded() &amp;&amp; tkls-&gt;klass_is_exact()) {
1956       // We are loading a field from a Klass metaobject whose identity
1957       // is known at compile time (the type is &quot;exact&quot; or &quot;precise&quot;).
1958       // Check for fields we know are maintained as constants by the VM.
1959       if (tkls-&gt;offset() == in_bytes(Klass::super_check_offset_offset())) {
1960         // The field is Klass::_super_check_offset.  Return its (constant) value.
1961         // (Folds up type checking code.)
1962         assert(Opcode() == Op_LoadI, &quot;must load an int from _super_check_offset&quot;);
1963         return TypeInt::make(klass-&gt;super_check_offset());
1964       }
1965       // Compute index into primary_supers array
1966       juint depth = (tkls-&gt;offset() - in_bytes(Klass::primary_supers_offset())) / sizeof(Klass*);
</pre>
<hr />
<pre>
2313     }
2314   }
2315 
2316   // Check for loading klass from an array klass
2317   const TypeKlassPtr *tkls = tp-&gt;isa_klassptr();
2318   if (tkls != NULL &amp;&amp; !StressReflectiveCode) {
2319     if (!tkls-&gt;is_loaded()) {
2320       return _type;             // Bail out if not loaded
2321     }
2322     ciKlass* klass = tkls-&gt;klass();
2323     if( klass-&gt;is_obj_array_klass() &amp;&amp;
2324         tkls-&gt;offset() == in_bytes(ObjArrayKlass::element_klass_offset())) {
2325       ciKlass* elem = klass-&gt;as_obj_array_klass()-&gt;element_klass();
2326       // // Always returning precise element type is incorrect,
2327       // // e.g., element type could be object and array may contain strings
2328       // return TypeKlassPtr::make(TypePtr::Constant, elem, 0);
2329 
2330       // The array&#39;s TypeKlassPtr was declared &#39;precise&#39; or &#39;not precise&#39;
2331       // according to the element type&#39;s subclassing.
2332       return TypeKlassPtr::make(tkls-&gt;ptr(), elem, Type::Offset(0), elem-&gt;flatten_array());
<span class="line-modified">2333     } else if (klass-&gt;is_flat_array_klass() &amp;&amp;</span>
2334                tkls-&gt;offset() == in_bytes(ObjArrayKlass::element_klass_offset())) {
<span class="line-modified">2335       ciKlass* elem = klass-&gt;as_flat_array_klass()-&gt;element_klass();</span>
2336       return TypeKlassPtr::make(tkls-&gt;ptr(), elem, Type::Offset(0), /* flat_array= */ true);
2337     }
2338     if( klass-&gt;is_instance_klass() &amp;&amp; tkls-&gt;klass_is_exact() &amp;&amp;
2339         tkls-&gt;offset() == in_bytes(Klass::super_offset())) {
2340       ciKlass* sup = klass-&gt;as_instance_klass()-&gt;super();
2341       // The field is Klass::_super.  Return its (constant) value.
2342       // (Folds up the 2nd indirection in aClassConstant.getSuperClass().)
2343       return sup ? TypeKlassPtr::make(sup) : TypePtr::NULL_PTR;
2344     }
2345   }
2346 
2347   // Bailout case
2348   return LoadNode::Value(phase);
2349 }
2350 
2351 //------------------------------Identity---------------------------------------
2352 // To clean up reflective code, simplify k.java_mirror.as_klass to plain k.
2353 // Also feed through the klass in Allocate(...klass...)._klass.
2354 Node* LoadKlassNode::Identity(PhaseGVN* phase) {
2355   return klass_identity_common(phase);
</pre>
<hr />
<pre>
2585   //return (uintptr_t)in(Control) + (uintptr_t)in(Memory) + (uintptr_t)in(Address) + (uintptr_t)in(ValueIn);
2586 
2587   // Since they are not commoned, do not hash them:
2588   return NO_HASH;
2589 }
2590 
2591 //------------------------------Ideal------------------------------------------
2592 // Change back-to-back Store(, p, x) -&gt; Store(m, p, y) to Store(m, p, x).
2593 // When a store immediately follows a relevant allocation/initialization,
2594 // try to capture it into the initialization, or hoist it above.
2595 Node *StoreNode::Ideal(PhaseGVN *phase, bool can_reshape) {
2596   Node* p = MemNode::Ideal_common(phase, can_reshape);
2597   if (p)  return (p == NodeSentinel) ? NULL : p;
2598 
2599   Node* mem     = in(MemNode::Memory);
2600   Node* address = in(MemNode::Address);
2601   // Back-to-back stores to same address?  Fold em up.  Generally
2602   // unsafe if I have intervening uses...  Also disallowed for StoreCM
2603   // since they must follow each StoreP operation.  Redundant StoreCMs
2604   // are eliminated just before matching in final_graph_reshape.
<span class="line-modified">2605   if (phase-&gt;C-&gt;get_adr_type(phase-&gt;C-&gt;get_alias_index(adr_type())) != TypeAryPtr::INLINES) {</span>
2606     Node* st = mem;
2607     // If Store &#39;st&#39; has more than one use, we cannot fold &#39;st&#39; away.
2608     // For example, &#39;st&#39; might be the final state at a conditional
2609     // return.  Or, &#39;st&#39; might be used by some node which is live at
2610     // the same time &#39;st&#39; is live, which might be unschedulable.  So,
2611     // require exactly ONE user until such time as we clone &#39;mem&#39; for
2612     // each of &#39;mem&#39;s uses (thus making the exactly-1-user-rule hold
2613     // true).
2614     while (st-&gt;is_Store() &amp;&amp; st-&gt;outcnt() == 1 &amp;&amp; st-&gt;Opcode() != Op_StoreCM) {
2615       // Looking at a dead closed cycle of memory?
2616       assert(st != st-&gt;in(MemNode::Memory), &quot;dead loop in StoreNode::Ideal&quot;);
2617       assert(Opcode() == st-&gt;Opcode() ||
2618              st-&gt;Opcode() == Op_StoreVector ||
2619              Opcode() == Op_StoreVector ||
2620              phase-&gt;C-&gt;get_alias_index(adr_type()) == Compile::AliasIdxRaw ||
2621              (Opcode() == Op_StoreL &amp;&amp; st-&gt;Opcode() == Op_StoreI) || // expanded ClearArrayNode
2622              (Opcode() == Op_StoreI &amp;&amp; st-&gt;Opcode() == Op_StoreL) || // initialization by arraycopy
2623              (Opcode() == Op_StoreL &amp;&amp; st-&gt;Opcode() == Op_StoreN) ||
2624              (is_mismatched_access() || st-&gt;as_Store()-&gt;is_mismatched_access()),
2625              &quot;no mismatched stores, except on raw memory: %s %s&quot;, NodeClassNames[Opcode()], NodeClassNames[st-&gt;Opcode()]);
</pre>
<hr />
<pre>
2693       val-&gt;as_Load()-&gt;store_Opcode() == Opcode()) {
2694     result = mem;
2695   }
2696 
2697   // Two stores in a row of the same value?
2698   if (result == this &amp;&amp;
2699       mem-&gt;is_Store() &amp;&amp;
2700       mem-&gt;in(MemNode::Address)-&gt;eqv_uncast(adr) &amp;&amp;
2701       mem-&gt;in(MemNode::ValueIn)-&gt;eqv_uncast(val) &amp;&amp;
2702       mem-&gt;Opcode() == Opcode()) {
2703     result = mem;
2704   }
2705 
2706   // Store of zero anywhere into a freshly-allocated object?
2707   // Then the store is useless.
2708   // (It must already have been captured by the InitializeNode.)
2709   if (result == this &amp;&amp; ReduceFieldZeroing) {
2710     // a newly allocated object is already all-zeroes everywhere
2711     if (mem-&gt;is_Proj() &amp;&amp; mem-&gt;in(0)-&gt;is_Allocate() &amp;&amp;
2712         (phase-&gt;type(val)-&gt;is_zero_type() || mem-&gt;in(0)-&gt;in(AllocateNode::DefaultValue) == val)) {
<span class="line-modified">2713       assert(!phase-&gt;type(val)-&gt;is_zero_type() || mem-&gt;in(0)-&gt;in(AllocateNode::DefaultValue) == NULL, &quot;storing null to inline type array is forbidden&quot;);</span>
2714       result = mem;
2715     }
2716 
2717     if (result == this) {
2718       // the store may also apply to zero-bits in an earlier object
2719       Node* prev_mem = find_previous_store(phase);
2720       // Steps (a), (b):  Walk past independent stores to find an exact match.
2721       if (prev_mem != NULL) {
2722         Node* prev_val = can_see_stored_value(prev_mem, phase);
2723         if (prev_val != NULL &amp;&amp; phase-&gt;eqv(prev_val, val)) {
2724           // prev_val and val might differ by a cast; it would be good
2725           // to keep the more informative of the two.
2726           if (phase-&gt;type(val)-&gt;is_zero_type()) {
2727             result = mem;
2728           } else if (prev_mem-&gt;is_Proj() &amp;&amp; prev_mem-&gt;in(0)-&gt;is_Initialize()) {
2729             InitializeNode* init = prev_mem-&gt;in(0)-&gt;as_Initialize();
2730             AllocateNode* alloc = init-&gt;allocation();
2731             if (alloc != NULL &amp;&amp; alloc-&gt;in(AllocateNode::DefaultValue) == val) {
2732               result = mem;
2733             }
</pre>
</td>
</tr>
</table>
<center><a href="matcher.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="mulnode.cpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>