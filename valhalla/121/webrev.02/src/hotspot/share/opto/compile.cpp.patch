diff a/src/hotspot/share/opto/compile.cpp b/src/hotspot/share/opto/compile.cpp
--- a/src/hotspot/share/opto/compile.cpp
+++ b/src/hotspot/share/opto/compile.cpp
@@ -49,10 +49,11 @@
 #include "opto/connode.hpp"
 #include "opto/convertnode.hpp"
 #include "opto/divnode.hpp"
 #include "opto/escape.hpp"
 #include "opto/idealGraphPrinter.hpp"
+#include "opto/inlinetypenode.hpp"
 #include "opto/loopnode.hpp"
 #include "opto/machnode.hpp"
 #include "opto/macro.hpp"
 #include "opto/matcher.hpp"
 #include "opto/mathexactnode.hpp"
@@ -66,11 +67,10 @@
 #include "opto/phaseX.hpp"
 #include "opto/rootnode.hpp"
 #include "opto/runtime.hpp"
 #include "opto/stringopts.hpp"
 #include "opto/type.hpp"
-#include "opto/valuetypenode.hpp"
 #include "opto/vectornode.hpp"
 #include "runtime/arguments.hpp"
 #include "runtime/sharedRuntime.hpp"
 #include "runtime/signature.hpp"
 #include "runtime/stubRoutines.hpp"
@@ -404,15 +404,15 @@
     Node* opaq = opaque4_node(i);
     if (!useful.member(opaq)) {
       remove_opaque4_node(opaq);
     }
   }
-  // Remove useless value type nodes
-  for (int i = _value_type_nodes->length() - 1; i >= 0; i--) {
-    Node* vt = _value_type_nodes->at(i);
+  // Remove useless inline type nodes
+  for (int i = _inline_type_nodes->length() - 1; i >= 0; i--) {
+    Node* vt = _inline_type_nodes->at(i);
     if (!useful.member(vt)) {
-      _value_type_nodes->remove(vt);
+      _inline_type_nodes->remove(vt);
     }
   }
   BarrierSetC2* bs = BarrierSet::barrier_set()->barrier_set_c2();
   bs->eliminate_useless_gc_barriers(useful, this);
   // clean up the late inline lists
@@ -1012,11 +1012,11 @@
   _macro_nodes = new(comp_arena()) GrowableArray<Node*>(comp_arena(), 8,  0, NULL);
   _predicate_opaqs = new(comp_arena()) GrowableArray<Node*>(comp_arena(), 8,  0, NULL);
   _expensive_nodes = new(comp_arena()) GrowableArray<Node*>(comp_arena(), 8,  0, NULL);
   _range_check_casts = new(comp_arena()) GrowableArray<Node*>(comp_arena(), 8,  0, NULL);
   _opaque4_nodes = new(comp_arena()) GrowableArray<Node*>(comp_arena(), 8,  0, NULL);
-  _value_type_nodes = new(comp_arena()) GrowableArray<Node*>(comp_arena(), 8,  0, NULL);
+  _inline_type_nodes = new(comp_arena()) GrowableArray<Node*>(comp_arena(), 8,  0, NULL);
   register_library_intrinsics();
 #ifdef ASSERT
   _type_verify_symmetry = true;
 #endif
 }
@@ -1273,11 +1273,11 @@
     }
   } else if( ta && _AliasLevel >= 2 ) {
     // For arrays indexed by constant indices, we flatten the alias
     // space to include all of the array body.  Only the header, klass
     // and array length can be accessed un-aliased.
-    // For flattened value type array, each field has its own slice so
+    // For flattened inline type array, each field has its own slice so
     // we must include the field offset.
     if( offset != Type::OffsetBot ) {
       if( ta->const_oop() ) { // MethodData* or Method*
         offset = Type::OffsetBot;   // Flatten constant access into array body
         tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),ta->ary(),ta->klass(),false,Type::Offset(offset), ta->field_offset());
@@ -1310,12 +1310,12 @@
     if (ta->elem()->isa_oopptr() && ta->elem() != TypeInstPtr::BOTTOM) {
       const TypeAry *tary = TypeAry::make(TypeInstPtr::BOTTOM, ta->size());
       tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,NULL,false,Type::Offset(offset), ta->field_offset());
     }
     // Initially all flattened array accesses share a single slice
-    if (ta->elem()->isa_valuetype() && ta->elem() != TypeValueType::BOTTOM && _flattened_accesses_share_alias) {
-      const TypeAry *tary = TypeAry::make(TypeValueType::BOTTOM, ta->size());
+    if (ta->elem()->isa_inlinetype() && ta->elem() != TypeInlineType::BOTTOM && _flattened_accesses_share_alias) {
+      const TypeAry *tary = TypeAry::make(TypeInlineType::BOTTOM, ta->size());
       tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,NULL,false,Type::Offset(offset), Type::Offset(Type::OffsetBot));
     }
     // Arrays of bytes and of booleans both use 'bastore' and 'baload' so
     // cannot be distinguished by bytecode alone.
     if (ta->elem() == TypeInt::BOOL) {
@@ -1625,14 +1625,14 @@
       const Type* elemtype = flat->is_aryptr()->elem();
       if (flat->offset() == TypePtr::OffsetBot) {
         alias_type(idx)->set_element(elemtype);
       }
       int field_offset = flat->is_aryptr()->field_offset().get();
-      if (elemtype->isa_valuetype() &&
-          elemtype->value_klass() != NULL &&
+      if (elemtype->isa_inlinetype() &&
+          elemtype->inline_klass() != NULL &&
           field_offset != Type::OffsetBot) {
-        ciValueKlass* vk = elemtype->value_klass();
+        ciInlineKlass* vk = elemtype->inline_klass();
         field_offset += vk->first_field_offset();
         field = vk->get_field_by_offset(field_offset, false);
       }
     }
     if (flat->isa_klassptr()) {
@@ -1660,13 +1660,13 @@
           tinst->klass() == ciEnv::current()->Class_klass() &&
           tinst->offset() >= (tinst->klass()->as_instance_klass()->size_helper() * wordSize)) {
         // static field
         ciInstanceKlass* k = tinst->const_oop()->as_instance()->java_lang_Class_klass()->as_instance_klass();
         field = k->get_field_by_offset(tinst->offset(), true);
-      } else if (tinst->klass()->is_valuetype()) {
-        // Value type field
-        ciValueKlass* vk = tinst->value_klass();
+      } else if (tinst->klass()->is_inlinetype()) {
+        // Inline type field
+        ciInlineKlass* vk = tinst->inline_klass();
         field = vk->get_field_by_offset(tinst->offset(), false);
       } else {
         ciInstanceKlass* k = tinst->klass()->as_instance_klass();
         field = k->get_field_by_offset(tinst->offset(), false);
       }
@@ -1678,12 +1678,12 @@
             field->is_static() == original_field->is_static()), "wrong field?");
     // Set field() and is_rewritable() attributes.
     if (field != NULL) {
       alias_type(idx)->set_field(field);
       if (flat->isa_aryptr()) {
-        // Fields of flattened inline type arrays are rewritable although they are declared final
-        assert(flat->is_aryptr()->elem()->isa_valuetype(), "must be a flattened value array");
+        // Fields of flat arrays are rewritable although they are declared final
+        assert(flat->is_aryptr()->elem()->isa_inlinetype(), "must be a flat array");
         alias_type(idx)->set_rewritable(true);
       }
     }
   }
 
@@ -1859,37 +1859,37 @@
     igvn.replace_node(opaq, opaq->in(2));
   }
   assert(opaque4_count() == 0, "should be empty");
 }
 
-void Compile::add_value_type(Node* n) {
-  assert(n->is_ValueTypeBase(), "unexpected node");
-  if (_value_type_nodes != NULL) {
-    _value_type_nodes->push(n);
+void Compile::add_inline_type(Node* n) {
+  assert(n->is_InlineTypeBase(), "unexpected node");
+  if (_inline_type_nodes != NULL) {
+    _inline_type_nodes->push(n);
   }
 }
 
-void Compile::remove_value_type(Node* n) {
-  assert(n->is_ValueTypeBase(), "unexpected node");
-  if (_value_type_nodes != NULL && _value_type_nodes->contains(n)) {
-    _value_type_nodes->remove(n);
+void Compile::remove_inline_type(Node* n) {
+  assert(n->is_InlineTypeBase(), "unexpected node");
+  if (_inline_type_nodes != NULL && _inline_type_nodes->contains(n)) {
+    _inline_type_nodes->remove(n);
   }
 }
 
-// Does the return value keep otherwise useless value type allocations alive?
+// Does the return value keep otherwise useless inline type allocations alive?
 static bool return_val_keeps_allocations_alive(Node* ret_val) {
   ResourceMark rm;
   Unique_Node_List wq;
   wq.push(ret_val);
   bool some_allocations = false;
   for (uint i = 0; i < wq.size(); i++) {
     Node* n = wq.at(i);
-    assert(!n->is_ValueType(), "chain of value type nodes");
+    assert(!n->is_InlineType(), "chain of inline type nodes");
     if (n->outcnt() > 1) {
       // Some other use for the allocation
       return false;
-    } else if (n->is_ValueTypePtr()) {
+    } else if (n->is_InlineTypePtr()) {
       wq.push(n->in(1));
     } else if (n->is_Phi()) {
       for (uint j = 1; j < n->req(); j++) {
         wq.push(n->in(j));
       }
@@ -1900,29 +1900,29 @@
     }
   }
   return some_allocations;
 }
 
-void Compile::process_value_types(PhaseIterGVN &igvn, bool post_ea) {
-  // Make value types scalar in safepoints
-  for (int i = _value_type_nodes->length()-1; i >= 0; i--) {
-    ValueTypeBaseNode* vt = _value_type_nodes->at(i)->as_ValueTypeBase();
+void Compile::process_inline_types(PhaseIterGVN &igvn, bool post_ea) {
+  // Make inline types scalar in safepoints
+  for (int i = _inline_type_nodes->length()-1; i >= 0; i--) {
+    InlineTypeBaseNode* vt = _inline_type_nodes->at(i)->as_InlineTypeBase();
     vt->make_scalar_in_safepoints(&igvn);
   }
-  // Remove ValueTypePtr nodes only after EA to give scalar replacement a chance
-  // to remove buffer allocations. ValueType nodes are kept until loop opts and
-  // removed via ValueTypeNode::remove_redundant_allocations.
+  // Remove InlineTypePtr nodes only after EA to give scalar replacement a chance
+  // to remove buffer allocations. InlineType nodes are kept until loop opts and
+  // removed via InlineTypeNode::remove_redundant_allocations.
   if (post_ea) {
-    while (_value_type_nodes->length() > 0) {
-      ValueTypeBaseNode* vt = _value_type_nodes->pop()->as_ValueTypeBase();
-      if (vt->is_ValueTypePtr()) {
+    while (_inline_type_nodes->length() > 0) {
+      InlineTypeBaseNode* vt = _inline_type_nodes->pop()->as_InlineTypeBase();
+      if (vt->is_InlineTypePtr()) {
         igvn.replace_node(vt, vt->get_oop());
       }
     }
   }
   // Make sure that the return value does not keep an unused allocation alive
-  if (tf()->returns_value_type_as_fields()) {
+  if (tf()->returns_inline_type_as_fields()) {
     Node* ret = NULL;
     for (uint i = 1; i < root()->req(); i++){
       Node* in = root()->in(i);
       if (in->Opcode() == Op_Return) {
         assert(ret == NULL, "only one return");
@@ -1931,11 +1931,11 @@
     }
     if (ret != NULL) {
       Node* ret_val = ret->in(TypeFunc::Parms);
       if (igvn.type(ret_val)->isa_oopptr() &&
           return_val_keeps_allocations_alive(ret_val)) {
-        igvn.replace_input_of(ret, TypeFunc::Parms, ValueTypeNode::tagged_klass(igvn.type(ret_val)->value_klass(), igvn));
+        igvn.replace_input_of(ret, TypeFunc::Parms, InlineTypeNode::tagged_klass(igvn.type(ret_val)->inline_klass(), igvn));
         assert(ret_val->outcnt() == 0, "should be dead now");
         igvn.remove_dead_node(ret_val);
       }
     }
   }
@@ -1959,11 +1959,11 @@
 
   Node_List mergememnodes;
   Node_List memnodes;
 
   // Alias index currently shared by all flattened memory accesses
-  int index = get_alias_index(TypeAryPtr::VALUES);
+  int index = get_alias_index(TypeAryPtr::INLINES);
 
   // Find MergeMem nodes and flattened array accesses
   for (uint i = 0; i < wq.size(); i++) {
     Node* n = wq.at(i);
     if (n->is_Mem()) {
@@ -1971,11 +1971,11 @@
       if (n->Opcode() == Op_StoreCM) {
         adr_type = get_adr_type(get_alias_index(n->in(MemNode::OopStore)->adr_type()));
       } else {
         adr_type = get_adr_type(get_alias_index(n->adr_type()));
       }
-      if (adr_type == TypeAryPtr::VALUES) {
+      if (adr_type == TypeAryPtr::INLINES) {
         memnodes.push(n);
       }
     } else if (n->is_MergeMem()) {
       MergeMemNode* mm = n->as_MergeMem();
       if (mm->memory_at(index) != mm->base_memory()) {
@@ -1998,11 +1998,11 @@
     // them.
     for (uint i = 0; i < AliasCacheSize; i++) {
       AliasCacheEntry* ace = &_alias_cache[i];
       if (ace->_adr_type != NULL &&
           ace->_adr_type->isa_aryptr() &&
-          ace->_adr_type->is_aryptr()->elem()->isa_valuetype()) {
+          ace->_adr_type->is_aryptr()->elem()->isa_inlinetype()) {
         ace->_adr_type = NULL;
         ace->_index = (i != 0) ? 0 : AliasIdxTop; // Make sure the NULL adr_type resolves to AliasIdxTop
       }
     }
 
@@ -2048,11 +2048,11 @@
         // Follow memory edges through memory accesses, phis and
         // narrow membars and push nodes on the stack. Once we hit
         // bottom memory, we pop element off the stack one at a
         // time, in reverse order, and move them to the right slice
         // by changing their memory edges.
-        if ((n->is_Phi() && n->adr_type() != TypePtr::BOTTOM) || n->is_Mem() || n->adr_type() == TypeAryPtr::VALUES) {
+        if ((n->is_Phi() && n->adr_type() != TypePtr::BOTTOM) || n->is_Mem() || n->adr_type() == TypeAryPtr::INLINES) {
           assert(!seen.test_set(n->_idx), "");
           // Uses (a load for instance) will need to be moved to the
           // right slice as well and will get a new memory state
           // that we don't know yet. The use could also be the
           // backedge of a loop. We put a place holder node between
@@ -2121,11 +2121,11 @@
               igvn.replace_input_of(m, idx, mm);
               if (idx == m->req()-1) {
                 Node* r = m->in(0);
                 for (uint j = (uint)start_alias; j <= (uint)stop_alias; j++) {
                   const Type* adr_type = get_adr_type(j);
-                  if (!adr_type->isa_aryptr() || !adr_type->is_aryptr()->elem()->isa_valuetype()) {
+                  if (!adr_type->isa_aryptr() || !adr_type->is_aryptr()->elem()->isa_inlinetype()) {
                     continue;
                   }
                   Node* phi = new PhiNode(r, Type::MEMORY, get_adr_type(j));
                   igvn.register_new_node_with_optimizer(phi);
                   for (uint k = 1; k < m->req(); k++) {
@@ -2151,11 +2151,11 @@
               assert(m->is_Proj(), "projection expected");
               Node* ctrl = m->in(0)->in(TypeFunc::Control);
               igvn.replace_input_of(m->in(0), TypeFunc::Control, top());
               for (uint j = (uint)start_alias; j <= (uint)stop_alias; j++) {
                 const Type* adr_type = get_adr_type(j);
-                if (!adr_type->isa_aryptr() || !adr_type->is_aryptr()->elem()->isa_valuetype()) {
+                if (!adr_type->isa_aryptr() || !adr_type->is_aryptr()->elem()->isa_inlinetype()) {
                   continue;
                 }
                 MemBarNode* mb = new MemBarCPUOrderNode(this, j, NULL);
                 igvn.register_new_node_with_optimizer(mb);
                 Node* mem = mm->memory_at(j);
@@ -2194,20 +2194,20 @@
       } while(stack.size() > 0);
       // Fix the memory state at the MergeMem we started from
       igvn.rehash_node_delayed(current);
       for (uint j = (uint)start_alias; j <= (uint)stop_alias; j++) {
         const Type* adr_type = get_adr_type(j);
-        if (!adr_type->isa_aryptr() || !adr_type->is_aryptr()->elem()->isa_valuetype()) {
+        if (!adr_type->isa_aryptr() || !adr_type->is_aryptr()->elem()->isa_inlinetype()) {
           continue;
         }
         current->set_memory_at(j, mm);
       }
       current->set_memory_at(index, current->base_memory());
     }
     igvn.optimize();
   }
-  print_method(PHASE_SPLIT_VALUES_ARRAY, 2);
+  print_method(PHASE_SPLIT_INLINES_ARRAY, 2);
 }
 
 
 // StringOpts and late inlining of string methods
 void Compile::inline_string_calls(bool parse_time) {
@@ -2486,13 +2486,13 @@
     set_for_igvn(&new_worklist);
     igvn = PhaseIterGVN(initial_gvn());
     igvn.optimize();
   }
 
-  if (_value_type_nodes->length() > 0) {
+  if (_inline_type_nodes->length() > 0) {
     // Do this once all inlining is over to avoid getting inconsistent debug info
-    process_value_types(igvn);
+    process_inline_types(igvn);
   }
 
   adjust_flattened_array_access_aliases(igvn);
 
   // Perform escape analysis
@@ -2525,13 +2525,13 @@
 
       if (failing())  return;
     }
   }
 
-  if (_value_type_nodes->length() > 0) {
-    // Process value types again now that EA might have simplified the graph
-    process_value_types(igvn, /* post_ea= */ true);
+  if (_inline_type_nodes->length() > 0) {
+    // Process inline types again now that EA might have simplified the graph
+    process_inline_types(igvn, /* post_ea= */ true);
   }
 
   // Loop transforms on the ideal graph.  Range Check Elimination,
   // peeling, unrolling, etc.
 
@@ -3912,14 +3912,14 @@
       n->subsume_by(cmp, this);
     }
     break;
   }
 #ifdef ASSERT
-  case Op_ValueTypePtr:
-  case Op_ValueType: {
+  case Op_InlineTypePtr:
+  case Op_InlineType: {
     n->dump(-1);
-    assert(false, "value type node was not removed");
+    assert(false, "inline type node was not removed");
     break;
   }
 #endif
   default:
     assert(!n->is_Call(), "");
@@ -4876,15 +4876,15 @@
 Node* Compile::optimize_acmp(PhaseGVN* phase, Node* a, Node* b) {
   const TypeInstPtr* ta = phase->type(a)->isa_instptr();
   const TypeInstPtr* tb = phase->type(b)->isa_instptr();
   if (!EnableValhalla || ta == NULL || tb == NULL ||
       ta->is_zero_type() || tb->is_zero_type() ||
-      !ta->can_be_value_type() || !tb->can_be_value_type()) {
-    // Use old acmp if one operand is null or not a value type
+      !ta->can_be_inline_type() || !tb->can_be_inline_type()) {
+    // Use old acmp if one operand is null or not an inline type
     return new CmpPNode(a, b);
-  } else if (ta->is_valuetypeptr() || tb->is_valuetypeptr()) {
-    // We know that one operand is a value type. Therefore,
+  } else if (ta->is_inlinetypeptr() || tb->is_inlinetypeptr()) {
+    // We know that one operand is an inline type. Therefore,
     // new acmp will only return true if both operands are NULL.
     // Check if both operands are null by or'ing the oops.
     a = phase->transform(new CastP2XNode(NULL, a));
     b = phase->transform(new CastP2XNode(NULL, b));
     a = phase->transform(new OrXNode(a, b));
