<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/share/opto/macro.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="machnode.hpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="macro.hpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/share/opto/macro.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
   6  * under the terms of the GNU General Public License version 2 only, as
   7  * published by the Free Software Foundation.
   8  *
   9  * This code is distributed in the hope that it will be useful, but WITHOUT
  10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #include &quot;precompiled.hpp&quot;

  26 #include &quot;compiler/compileLog.hpp&quot;
  27 #include &quot;gc/shared/collectedHeap.inline.hpp&quot;
  28 #include &quot;libadt/vectset.hpp&quot;
  29 #include &quot;memory/universe.hpp&quot;
  30 #include &quot;opto/addnode.hpp&quot;
  31 #include &quot;opto/arraycopynode.hpp&quot;
  32 #include &quot;opto/callnode.hpp&quot;
  33 #include &quot;opto/castnode.hpp&quot;
  34 #include &quot;opto/cfgnode.hpp&quot;
  35 #include &quot;opto/compile.hpp&quot;
  36 #include &quot;opto/convertnode.hpp&quot;
  37 #include &quot;opto/graphKit.hpp&quot;

  38 #include &quot;opto/intrinsicnode.hpp&quot;
  39 #include &quot;opto/locknode.hpp&quot;
  40 #include &quot;opto/loopnode.hpp&quot;
  41 #include &quot;opto/macro.hpp&quot;
  42 #include &quot;opto/memnode.hpp&quot;
  43 #include &quot;opto/narrowptrnode.hpp&quot;
  44 #include &quot;opto/node.hpp&quot;
  45 #include &quot;opto/opaquenode.hpp&quot;
  46 #include &quot;opto/phaseX.hpp&quot;
  47 #include &quot;opto/rootnode.hpp&quot;
  48 #include &quot;opto/runtime.hpp&quot;
  49 #include &quot;opto/subnode.hpp&quot;
  50 #include &quot;opto/subtypenode.hpp&quot;
  51 #include &quot;opto/type.hpp&quot;
<span class="line-removed">  52 #include &quot;opto/valuetypenode.hpp&quot;</span>
  53 #include &quot;runtime/sharedRuntime.hpp&quot;
  54 #include &quot;utilities/macros.hpp&quot;
  55 #include &quot;utilities/powerOfTwo.hpp&quot;
  56 #if INCLUDE_G1GC
  57 #include &quot;gc/g1/g1ThreadLocalData.hpp&quot;
  58 #endif // INCLUDE_G1GC
  59 #if INCLUDE_SHENANDOAHGC
  60 #include &quot;gc/shenandoah/c2/shenandoahBarrierSetC2.hpp&quot;
  61 #endif
  62 
  63 
  64 //
  65 // Replace any references to &quot;oldref&quot; in inputs to &quot;use&quot; with &quot;newref&quot;.
  66 // Returns the number of replacements made.
  67 //
  68 int PhaseMacroExpand::replace_input(Node *use, Node *oldref, Node *newref) {
  69   int nreplacements = 0;
  70   uint req = use-&gt;req();
  71   for (uint j = 0; j &lt; use-&gt;len(); j++) {
  72     Node *uin = use-&gt;in(j);
</pre>
<hr />
<pre>
 338         adr_type = _igvn.type(adr)-&gt;is_ptr();
 339         assert(adr_type == _igvn.type(base)-&gt;is_aryptr()-&gt;add_field_offset_and_offset(off), &quot;incorrect address type&quot;);
 340         if (ac-&gt;in(ArrayCopyNode::Src) == ac-&gt;in(ArrayCopyNode::Dest)) {
 341           // Don&#39;t emit a new load from src if src == dst but try to get the value from memory instead
 342           return value_from_mem(ac-&gt;in(TypeFunc::Memory), ctl, ft, ftype, adr_type-&gt;isa_oopptr(), alloc);
 343         }
 344       } else {
 345         if (ac-&gt;in(ArrayCopyNode::Src) == ac-&gt;in(ArrayCopyNode::Dest)) {
 346           // Non constant offset in the array: we can&#39;t statically
 347           // determine the value
 348           return NULL;
 349         }
 350         Node* diff = _igvn.transform(new SubINode(ac-&gt;in(ArrayCopyNode::SrcPos), ac-&gt;in(ArrayCopyNode::DestPos)));
 351 #ifdef _LP64
 352         diff = _igvn.transform(new ConvI2LNode(diff));
 353 #endif
 354         diff = _igvn.transform(new LShiftXNode(diff, intcon(shift)));
 355 
 356         Node* off = _igvn.transform(new AddXNode(MakeConX(offset), diff));
 357         adr = _igvn.transform(new AddPNode(base, base, off));
<span class="line-modified"> 358         // In the case of a flattened value type array, each field has its</span>
 359         // own slice so we need to extract the field being accessed from
 360         // the address computation
 361         adr_type = adr_type-&gt;is_aryptr()-&gt;add_field_offset_and_offset(offset)-&gt;add_offset(Type::OffsetBot);
 362         adr = _igvn.transform(new CastPPNode(adr, adr_type));
 363       }
 364       res = LoadNode::make(_igvn, ctl, mem, adr, adr_type, type, bt, MemNode::unordered, LoadNode::UnknownControl);
 365     }
 366   }
 367   if (res != NULL) {
 368     res = _igvn.transform(res);
 369     if (ftype-&gt;isa_narrowoop()) {
 370       // PhaseMacroExpand::scalar_replacement adds DecodeN nodes
 371       assert(res-&gt;isa_DecodeN(), &quot;should be narrow oop&quot;);
 372       res = _igvn.transform(new EncodePNode(res, ftype));
 373     }
 374     return res;
 375   }
 376   return NULL;
 377 }
 378 
</pre>
<hr />
<pre>
 581           Node* n = value_phis.node();
 582           _igvn.replace_node(n, C-&gt;top());
 583           value_phis.pop();
 584         }
 585       }
 586     } else if (mem-&gt;is_ArrayCopy()) {
 587       Node* ctl = mem-&gt;in(0);
 588       Node* m = mem-&gt;in(TypeFunc::Memory);
 589       if (sfpt_ctl-&gt;is_Proj() &amp;&amp; sfpt_ctl-&gt;as_Proj()-&gt;is_uncommon_trap_proj(Deoptimization::Reason_none)) {
 590         // pin the loads in the uncommon trap path
 591         ctl = sfpt_ctl;
 592         m = sfpt_mem;
 593       }
 594       return make_arraycopy_load(mem-&gt;as_ArrayCopy(), offset, ctl, m, ft, ftype, alloc);
 595     }
 596   }
 597   // Something went wrong.
 598   return NULL;
 599 }
 600 
<span class="line-modified"> 601 // Search the last value stored into the value type&#39;s fields.</span>
<span class="line-modified"> 602 Node* PhaseMacroExpand::value_type_from_mem(Node* mem, Node* ctl, ciValueKlass* vk, const TypeAryPtr* adr_type, int offset, AllocateNode* alloc) {</span>
 603   // Subtract the offset of the first field to account for the missing oop header
 604   offset -= vk-&gt;first_field_offset();
<span class="line-modified"> 605   // Create a new ValueTypeNode and retrieve the field values from memory</span>
<span class="line-modified"> 606   ValueTypeNode* vt = ValueTypeNode::make_uninitialized(_igvn, vk)-&gt;as_ValueType();</span>
 607   for (int i = 0; i &lt; vk-&gt;nof_declared_nonstatic_fields(); ++i) {
 608     ciType* field_type = vt-&gt;field_type(i);
 609     int field_offset = offset + vt-&gt;field_offset(i);
<span class="line-modified"> 610     // Each value type field has its own memory slice</span>
 611     adr_type = adr_type-&gt;with_field_offset(field_offset);
 612     Node* value = NULL;
 613     if (vt-&gt;field_is_flattened(i)) {
<span class="line-modified"> 614       value = value_type_from_mem(mem, ctl, field_type-&gt;as_value_klass(), adr_type, field_offset, alloc);</span>
 615     } else {
 616       const Type* ft = Type::get_const_type(field_type);
 617       BasicType bt = field_type-&gt;basic_type();
 618       if (UseCompressedOops &amp;&amp; !is_java_primitive(bt)) {
 619         ft = ft-&gt;make_narrowoop();
 620         bt = T_NARROWOOP;
 621       }
 622       value = value_from_mem(mem, ctl, bt, ft, adr_type, alloc);
 623       if (value != NULL &amp;&amp; ft-&gt;isa_narrowoop()) {
 624         assert(UseCompressedOops, &quot;unexpected narrow oop&quot;);
 625         value = transform_later(new DecodeNNode(value, value-&gt;get_ptr_type()));
 626       }
 627     }
 628     if (value != NULL) {
 629       vt-&gt;set_field_value(i, value);
 630     } else {
 631       // We might have reached the TrackedInitializationLimit
 632       return NULL;
 633     }
 634   }
</pre>
<hr />
<pre>
 698                   use-&gt;as_ArrayCopy()-&gt;is_copyof_validated() ||
 699                   use-&gt;as_ArrayCopy()-&gt;is_copyofrange_validated()) &amp;&amp;
 700                  use-&gt;in(ArrayCopyNode::Dest) == res) {
 701         // ok to eliminate
 702       } else if (use-&gt;is_SafePoint()) {
 703         SafePointNode* sfpt = use-&gt;as_SafePoint();
 704         if (sfpt-&gt;is_Call() &amp;&amp; sfpt-&gt;as_Call()-&gt;has_non_debug_use(res)) {
 705           // Object is passed as argument.
 706           DEBUG_ONLY(disq_node = use;)
 707           NOT_PRODUCT(fail_eliminate = &quot;Object is passed as argument&quot;;)
 708           can_eliminate = false;
 709         }
 710         Node* sfptMem = sfpt-&gt;memory();
 711         if (sfptMem == NULL || sfptMem-&gt;is_top()) {
 712           DEBUG_ONLY(disq_node = use;)
 713           NOT_PRODUCT(fail_eliminate = &quot;NULL or TOP memory&quot;;)
 714           can_eliminate = false;
 715         } else {
 716           safepoints.append_if_missing(sfpt);
 717         }
<span class="line-modified"> 718       } else if (use-&gt;is_ValueType() &amp;&amp; use-&gt;isa_ValueType()-&gt;get_oop() == res) {</span>
 719         // ok to eliminate
 720       } else if (use-&gt;Opcode() == Op_StoreX &amp;&amp; use-&gt;in(MemNode::Address) == res) {
 721         // store to mark work
 722       } else if (use-&gt;Opcode() != Op_CastP2X) { // CastP2X is used by card mark
 723         if (use-&gt;is_Phi()) {
 724           if (use-&gt;outcnt() == 1 &amp;&amp; use-&gt;unique_out()-&gt;Opcode() == Op_Return) {
 725             NOT_PRODUCT(fail_eliminate = &quot;Object is return value&quot;;)
 726           } else {
 727             NOT_PRODUCT(fail_eliminate = &quot;Object is referenced by Phi&quot;;)
 728           }
 729           DEBUG_ONLY(disq_node = use;)
 730         } else {
 731           if (use-&gt;Opcode() == Op_Return) {
 732             NOT_PRODUCT(fail_eliminate = &quot;Object is return value&quot;;)
 733           } else {
 734             NOT_PRODUCT(fail_eliminate = &quot;Object is referenced by node&quot;;)
 735           }
 736           DEBUG_ONLY(disq_node = use;)
 737         }
 738         can_eliminate = false;
</pre>
<hr />
<pre>
 784   Node* res = alloc-&gt;result_cast();
 785   assert(res == NULL || res-&gt;is_CheckCastPP(), &quot;unexpected AllocateNode result&quot;);
 786   const TypeOopPtr* res_type = NULL;
 787   if (res != NULL) { // Could be NULL when there are no users
 788     res_type = _igvn.type(res)-&gt;isa_oopptr();
 789   }
 790 
 791   if (res != NULL) {
 792     klass = res_type-&gt;klass();
 793     if (res_type-&gt;isa_instptr()) {
 794       // find the fields of the class which will be needed for safepoint debug information
 795       assert(klass-&gt;is_instance_klass(), &quot;must be an instance klass.&quot;);
 796       iklass = klass-&gt;as_instance_klass();
 797       nfields = iklass-&gt;nof_nonstatic_fields();
 798     } else {
 799       // find the array&#39;s elements which will be needed for safepoint debug information
 800       nfields = alloc-&gt;in(AllocateNode::ALength)-&gt;find_int_con(-1);
 801       assert(klass-&gt;is_array_klass() &amp;&amp; nfields &gt;= 0, &quot;must be an array klass.&quot;);
 802       elem_type = klass-&gt;as_array_klass()-&gt;element_type();
 803       basic_elem_type = elem_type-&gt;basic_type();
<span class="line-modified"> 804       if (elem_type-&gt;is_valuetype() &amp;&amp; !klass-&gt;is_value_array_klass()) {</span>
 805         assert(basic_elem_type == T_INLINE_TYPE, &quot;unexpected element basic type&quot;);
 806         basic_elem_type = T_OBJECT;
 807       }
 808       array_base = arrayOopDesc::base_offset_in_bytes(basic_elem_type);
 809       element_size = type2aelembytes(basic_elem_type);
<span class="line-modified"> 810       if (klass-&gt;is_value_array_klass()) {</span>
<span class="line-modified"> 811         // Flattened value type array</span>
<span class="line-modified"> 812         element_size = klass-&gt;as_value_array_klass()-&gt;element_byte_size();</span>
 813       }
 814     }
 815   }
 816   //
 817   // Process the safepoint uses
 818   //
 819   Unique_Node_List value_worklist;
 820   while (safepoints.length() &gt; 0) {
 821     SafePointNode* sfpt = safepoints.pop();
 822     Node* mem = sfpt-&gt;memory();
 823     Node* ctl = sfpt-&gt;control();
 824     assert(sfpt-&gt;jvms() != NULL, &quot;missed JVMS&quot;);
 825     // Fields of scalar objs are referenced only at the end
 826     // of regular debuginfo at the last (youngest) JVMS.
 827     // Record relative start index.
 828     uint first_ind = (sfpt-&gt;req() - sfpt-&gt;jvms()-&gt;scloff());
 829     SafePointScalarObjectNode* sobj = new SafePointScalarObjectNode(res_type,
 830 #ifdef ASSERT
 831                                                  alloc,
 832 #endif
 833                                                  first_ind, nfields);
 834     sobj-&gt;init_req(0, C-&gt;root());
 835     transform_later(sobj);
 836 
 837     // Scan object&#39;s fields adding an input to the safepoint for each field.
 838     for (int j = 0; j &lt; nfields; j++) {
 839       intptr_t offset;
 840       ciField* field = NULL;
 841       if (iklass != NULL) {
 842         field = iklass-&gt;nonstatic_field_at(j);
 843         offset = field-&gt;offset();
 844         elem_type = field-&gt;type();
 845         basic_elem_type = field-&gt;layout_type();
<span class="line-modified"> 846         assert(!field-&gt;is_flattened(), &quot;flattened value type fields should not have safepoint uses&quot;);</span>
 847       } else {
 848         offset = array_base + j * (intptr_t)element_size;
 849       }
 850 
 851       const Type *field_type;
 852       // The next code is taken from Parse::do_get_xxx().
 853       if (is_reference_type(basic_elem_type)) {
 854         if (!elem_type-&gt;is_loaded()) {
 855           field_type = TypeInstPtr::BOTTOM;
 856         } else if (field != NULL &amp;&amp; field-&gt;is_static_constant()) {
 857           // This can happen if the constant oop is non-perm.
 858           ciObject* con = field-&gt;constant_value().as_object();
 859           // Do not &quot;join&quot; in the previous type; it doesn&#39;t add value,
 860           // and may yield a vacuous result if the field is of interface type.
 861           field_type = TypeOopPtr::make_from_constant(con)-&gt;isa_oopptr();
 862           assert(field_type != NULL, &quot;field singleton type must be consistent&quot;);
 863         } else {
 864           field_type = TypeOopPtr::make_from_klass(elem_type-&gt;as_klass());
 865         }
 866         if (UseCompressedOops) {
 867           field_type = field_type-&gt;make_narrowoop();
 868           basic_elem_type = T_NARROWOOP;
 869         }
 870       } else {
 871         field_type = Type::get_const_basic_type(basic_elem_type);
 872       }
 873 
 874       Node* field_val = NULL;
 875       const TypeOopPtr* field_addr_type = res_type-&gt;add_offset(offset)-&gt;isa_oopptr();
<span class="line-modified"> 876       if (klass-&gt;is_value_array_klass()) {</span>
<span class="line-modified"> 877         ciValueKlass* vk = elem_type-&gt;as_value_klass();</span>
 878         assert(vk-&gt;flatten_array(), &quot;must be flattened&quot;);
<span class="line-modified"> 879         field_val = value_type_from_mem(mem, ctl, vk, field_addr_type-&gt;isa_aryptr(), 0, alloc);</span>
 880       } else {
 881         field_val = value_from_mem(mem, ctl, basic_elem_type, field_type, field_addr_type, alloc);
 882       }
 883       if (field_val == NULL) {
 884         // We weren&#39;t able to find a value for this field,
 885         // give up on eliminating this allocation.
 886 
 887         // Remove any extra entries we added to the safepoint.
 888         uint last = sfpt-&gt;req() - 1;
 889         for (int k = 0;  k &lt; j; k++) {
 890           sfpt-&gt;del_req(last--);
 891         }
 892         _igvn._worklist.push(sfpt);
 893         // rollback processed safepoints
 894         while (safepoints_done.length() &gt; 0) {
 895           SafePointNode* sfpt_done = safepoints_done.pop();
 896           // remove any extra entries we added to the safepoint
 897           last = sfpt_done-&gt;req() - 1;
 898           for (int k = 0;  k &lt; nfields; k++) {
 899             sfpt_done-&gt;del_req(last--);
</pre>
<hr />
<pre>
 920         if (PrintEliminateAllocations) {
 921           if (field != NULL) {
 922             tty-&gt;print(&quot;=== At SafePoint node %d can&#39;t find value of Field: &quot;,
 923                        sfpt-&gt;_idx);
 924             field-&gt;print();
 925             int field_idx = C-&gt;get_alias_index(field_addr_type);
 926             tty-&gt;print(&quot; (alias_idx=%d)&quot;, field_idx);
 927           } else { // Array&#39;s element
 928             tty-&gt;print(&quot;=== At SafePoint node %d can&#39;t find value of array element [%d]&quot;,
 929                        sfpt-&gt;_idx, j);
 930           }
 931           tty-&gt;print(&quot;, which prevents elimination of: &quot;);
 932           if (res == NULL)
 933             alloc-&gt;dump();
 934           else
 935             res-&gt;dump();
 936         }
 937 #endif
 938         return false;
 939       }
<span class="line-modified"> 940       if (field_val-&gt;is_ValueType()) {</span>
<span class="line-modified"> 941         // Keep track of value types to scalarize them later</span>
 942         value_worklist.push(field_val);
 943       } else if (UseCompressedOops &amp;&amp; field_type-&gt;isa_narrowoop()) {
 944         // Enable &quot;DecodeN(EncodeP(Allocate)) --&gt; Allocate&quot; transformation
 945         // to be able scalar replace the allocation.
 946         if (field_val-&gt;is_EncodeP()) {
 947           field_val = field_val-&gt;in(1);
 948         } else {
 949           field_val = transform_later(new DecodeNNode(field_val, field_val-&gt;get_ptr_type()));
 950         }
 951       }
 952       sfpt-&gt;add_req(field_val);
 953     }
 954     JVMState *jvms = sfpt-&gt;jvms();
 955     jvms-&gt;set_endoff(sfpt-&gt;req());
 956     // Now make a pass over the debug information replacing any references
 957     // to the allocated object with &quot;sobj&quot;
 958     int start = jvms-&gt;debug_start();
 959     int end   = jvms-&gt;debug_end();
 960     sfpt-&gt;replace_edges_in_range(res, sobj, start, end);
 961     _igvn._worklist.push(sfpt);
 962     safepoints_done.append_if_missing(sfpt); // keep it for rollback
 963   }
<span class="line-modified"> 964   // Scalarize value types that were added to the safepoint</span>
 965   for (uint i = 0; i &lt; value_worklist.size(); ++i) {
 966     Node* vt = value_worklist.at(i);
<span class="line-modified"> 967     vt-&gt;as_ValueType()-&gt;make_scalar_in_safepoints(&amp;_igvn);</span>
 968   }
 969   return true;
 970 }
 971 
 972 static void disconnect_projections(MultiNode* n, PhaseIterGVN&amp; igvn) {
 973   Node* ctl_proj = n-&gt;proj_out_or_null(TypeFunc::Control);
 974   Node* mem_proj = n-&gt;proj_out_or_null(TypeFunc::Memory);
 975   if (ctl_proj != NULL) {
 976     igvn.replace_node(ctl_proj, n-&gt;in(0));
 977   }
 978   if (mem_proj != NULL) {
 979     igvn.replace_node(mem_proj, n-&gt;in(TypeFunc::Memory));
 980   }
 981 }
 982 
 983 // Process users of eliminated allocation.
 984 void PhaseMacroExpand::process_users_of_allocation(CallNode *alloc, bool inline_alloc) {
 985   Node* res = alloc-&gt;result_cast();
 986   if (res != NULL) {
 987     for (DUIterator_Last jmin, j = res-&gt;last_outs(jmin); j &gt;= jmin; ) {
</pre>
<hr />
<pre>
1028 
1029           _igvn.replace_node(callprojs-&gt;fallthrough_ioproj, ac-&gt;in(TypeFunc::I_O));
1030           _igvn.replace_node(callprojs-&gt;fallthrough_memproj, ac-&gt;in(TypeFunc::Memory));
1031           _igvn.replace_node(callprojs-&gt;fallthrough_catchproj, ac-&gt;in(TypeFunc::Control));
1032 
1033           // Set control to top. IGVN will remove the remaining projections
1034           ac-&gt;set_req(0, top());
1035           ac-&gt;replace_edge(res, top());
1036 
1037           // Disconnect src right away: it can help find new
1038           // opportunities for allocation elimination
1039           Node* src = ac-&gt;in(ArrayCopyNode::Src);
1040           ac-&gt;replace_edge(src, top());
1041           // src can be top at this point if src and dest of the
1042           // arraycopy were the same
1043           if (src-&gt;outcnt() == 0 &amp;&amp; !src-&gt;is_top()) {
1044             _igvn.remove_dead_node(src);
1045           }
1046         }
1047         _igvn._worklist.push(ac);
<span class="line-modified">1048       } else if (use-&gt;is_ValueType()) {</span>
<span class="line-modified">1049         assert(use-&gt;isa_ValueType()-&gt;get_oop() == res, &quot;unexpected value type use&quot;);</span>
<span class="line-modified">1050          _igvn.rehash_node_delayed(use);</span>
<span class="line-modified">1051         use-&gt;isa_ValueType()-&gt;set_oop(_igvn.zerocon(T_INLINE_TYPE));</span>
1052       } else if (use-&gt;is_Store()) {
1053         _igvn.replace_node(use, use-&gt;in(MemNode::Memory));
1054       } else {
1055         eliminate_gc_barrier(use);
1056       }
1057       j -= (oc1 - res-&gt;outcnt());
1058     }
1059     assert(res-&gt;outcnt() == 0, &quot;all uses of allocated objects must be deleted&quot;);
1060     _igvn.remove_dead_node(res);
1061   }
1062 
1063   //
1064   // Process other users of allocation&#39;s projections
1065   //
1066   if (_resproj != NULL &amp;&amp; _resproj-&gt;outcnt() != 0) {
1067     // First disconnect stores captured by Initialize node.
1068     // If Initialize node is eliminated first in the following code,
1069     // it will kill such stores and DUIterator_Last will assert.
1070     for (DUIterator_Fast jmax, j = _resproj-&gt;fast_outs(jmax);  j &lt; jmax; j++) {
1071       Node *use = _resproj-&gt;fast_out(j);
</pre>
<hr />
<pre>
1133     _igvn.replace_node(_ioproj_catchall, C-&gt;top());
1134   }
1135   if (_catchallcatchproj != NULL) {
1136     _igvn.replace_node(_catchallcatchproj, C-&gt;top());
1137   }
1138 }
1139 
1140 bool PhaseMacroExpand::eliminate_allocate_node(AllocateNode *alloc) {
1141   // Don&#39;t do scalar replacement if the frame can be popped by JVMTI:
1142   // if reallocation fails during deoptimization we&#39;ll pop all
1143   // interpreter frames for this compiled frame and that won&#39;t play
1144   // nice with JVMTI popframe.
1145   if (!EliminateAllocations || JvmtiExport::can_pop_frame()) {
1146     return false;
1147   }
1148   Node* klass = alloc-&gt;in(AllocateNode::KlassNode);
1149   const TypeKlassPtr* tklass = _igvn.type(klass)-&gt;is_klassptr();
1150 
1151   // Attempt to eliminate inline type buffer allocations
1152   // regardless of usage and escape/replaceable status.
<span class="line-modified">1153   bool inline_alloc = tklass-&gt;klass()-&gt;is_valuetype();</span>
1154   if (!alloc-&gt;_is_non_escaping &amp;&amp; !inline_alloc) {
1155     return false;
1156   }
1157   // Eliminate boxing allocations which are not used
1158   // regardless of scalar replaceable status.
1159   Node* res = alloc-&gt;result_cast();
1160   bool boxing_alloc = (res == NULL) &amp;&amp; C-&gt;eliminate_boxing() &amp;&amp;
1161                       tklass-&gt;klass()-&gt;is_instance_klass() &amp;&amp;
1162                       tklass-&gt;klass()-&gt;as_instance_klass()-&gt;is_box_klass();
1163   if (!alloc-&gt;_is_scalar_replaceable &amp;&amp; !boxing_alloc &amp;&amp; !inline_alloc) {
1164     return false;
1165   }
1166 
1167   extract_call_projections(alloc);
1168 
1169   GrowableArray &lt;SafePointNode *&gt; safepoints;
1170   if (!can_eliminate_allocation(alloc, safepoints)) {
1171     return false;
1172   }
1173 
</pre>
<hr />
<pre>
2205         // of the same object and mark related locks as eliminated.
2206         mark_eliminated_box(box, obj);
2207       }
2208     }
2209   }
2210 }
2211 
2212 // we have determined that this lock/unlock can be eliminated, we simply
2213 // eliminate the node without expanding it.
2214 //
2215 // Note:  The membar&#39;s associated with the lock/unlock are currently not
2216 //        eliminated.  This should be investigated as a future enhancement.
2217 //
2218 bool PhaseMacroExpand::eliminate_locking_node(AbstractLockNode *alock) {
2219 
2220   if (!alock-&gt;is_eliminated()) {
2221     return false;
2222   }
2223 #ifdef ASSERT
2224   const Type* obj_type = _igvn.type(alock-&gt;obj_node());
<span class="line-modified">2225   assert(!obj_type-&gt;isa_valuetype() &amp;&amp; !obj_type-&gt;is_valuetypeptr(), &quot;Eliminating lock on value type&quot;);</span>
2226   if (!alock-&gt;is_coarsened()) {
2227     // Check that new &quot;eliminated&quot; BoxLock node is created.
2228     BoxLockNode* oldbox = alock-&gt;box_node()-&gt;as_BoxLock();
2229     assert(oldbox-&gt;is_eliminated(), &quot;should be done already&quot;);
2230   }
2231 #endif
2232 
2233   alock-&gt;log_lock_optimization(C, &quot;eliminate_lock&quot;);
2234 
2235 #ifndef PRODUCT
2236   if (PrintEliminateLocks) {
2237     if (alock-&gt;is_Lock()) {
2238       tty-&gt;print_cr(&quot;++++ Eliminated: %d Lock&quot;, alock-&gt;_idx);
2239     } else {
2240       tty-&gt;print_cr(&quot;++++ Eliminated: %d Unlock&quot;, alock-&gt;_idx);
2241     }
2242   }
2243 #endif
2244 
2245   Node* mem  = alock-&gt;in(TypeFunc::Memory);
</pre>
<hr />
<pre>
2488 
2489     slow_path-&gt;init_req(2, ctrl); // Capture slow-control
2490     slow_mem-&gt;init_req(2, fast_lock_mem_phi);
2491 
2492     transform_later(slow_path);
2493     transform_later(slow_mem);
2494     // Reset lock&#39;s memory edge.
2495     lock-&gt;set_req(TypeFunc::Memory, slow_mem);
2496 
2497   } else {
2498     region  = new RegionNode(3);
2499     // create a Phi for the memory state
2500     mem_phi = new PhiNode( region, Type::MEMORY, TypeRawPtr::BOTTOM);
2501 
2502     // Optimize test; set region slot 2
2503     slow_path = opt_bits_test(ctrl, region, 2, flock, 0, 0);
2504     mem_phi-&gt;init_req(2, mem);
2505   }
2506 
2507   const TypeOopPtr* objptr = _igvn.type(obj)-&gt;make_oopptr();
<span class="line-modified">2508   if (objptr-&gt;can_be_value_type()) {</span>
2509     // Deoptimize and re-execute if a value
<span class="line-modified">2510     assert(EnableValhalla, &quot;should only be used if value types are enabled&quot;);</span>
2511     Node* mark = make_load(slow_path, mem, obj, oopDesc::mark_offset_in_bytes(), TypeX_X, TypeX_X-&gt;basic_type());
2512     Node* value_mask = _igvn.MakeConX(markWord::always_locked_pattern);
2513     Node* is_value = _igvn.transform(new AndXNode(mark, value_mask));
2514     Node* cmp = _igvn.transform(new CmpXNode(is_value, value_mask));
2515     Node* bol = _igvn.transform(new BoolNode(cmp, BoolTest::eq));
2516     Node* unc_ctrl = generate_slow_guard(&amp;slow_path, bol, NULL);
2517 
2518     int trap_request = Deoptimization::make_trap_request(Deoptimization::Reason_class_check, Deoptimization::Action_none);
2519     address call_addr = SharedRuntime::uncommon_trap_blob()-&gt;entry_point();
2520     const TypePtr* no_memory_effects = NULL;
2521     JVMState* jvms = lock-&gt;jvms();
2522     CallNode* unc = new CallStaticJavaNode(OptoRuntime::uncommon_trap_Type(), call_addr, &quot;uncommon_trap&quot;,
2523                                            jvms-&gt;bci(), no_memory_effects);
2524 
2525     unc-&gt;init_req(TypeFunc::Control, unc_ctrl);
2526     unc-&gt;init_req(TypeFunc::I_O, lock-&gt;i_o());
2527     unc-&gt;init_req(TypeFunc::Memory, mem); // may gc ptrs
2528     unc-&gt;init_req(TypeFunc::FramePtr,  lock-&gt;in(TypeFunc::FramePtr));
2529     unc-&gt;init_req(TypeFunc::ReturnAdr, lock-&gt;in(TypeFunc::ReturnAdr));
2530     unc-&gt;init_req(TypeFunc::Parms+0, _igvn.intcon(trap_request));
</pre>
<hr />
<pre>
2630   // No exceptions for unlocking
2631   // Capture slow path
2632   // disconnect fall-through projection from call and create a new one
2633   // hook up users of fall-through projection to region
2634   Node *slow_ctrl = _fallthroughproj-&gt;clone();
2635   transform_later(slow_ctrl);
2636   _igvn.hash_delete(_fallthroughproj);
2637   _fallthroughproj-&gt;disconnect_inputs(NULL, C);
2638   region-&gt;init_req(1, slow_ctrl);
2639   // region inputs are now complete
2640   transform_later(region);
2641   _igvn.replace_node(_fallthroughproj, region);
2642 
2643   Node *memproj = transform_later(new ProjNode(call, TypeFunc::Memory) );
2644   mem_phi-&gt;init_req(1, memproj );
2645   mem_phi-&gt;init_req(2, mem);
2646   transform_later(mem_phi);
2647   _igvn.replace_node(_memproj_fallthrough, mem_phi);
2648 }
2649 
<span class="line-modified">2650 // A value type might be returned from the call but we don&#39;t know its</span>
<span class="line-modified">2651 // type. Either we get a buffered value (and nothing needs to be done)</span>
<span class="line-modified">2652 // or one of the values being returned is the klass of the value type</span>
<span class="line-modified">2653 // and we need to allocate a value type instance of that type and</span>
2654 // initialize it with other values being returned. In that case, we
2655 // first try a fast path allocation and initialize the value with the
<span class="line-modified">2656 // value klass&#39;s pack handler or we fall back to a runtime call.</span>
2657 void PhaseMacroExpand::expand_mh_intrinsic_return(CallStaticJavaNode* call) {
2658   assert(call-&gt;method()-&gt;is_method_handle_intrinsic(), &quot;must be a method handle intrinsic call&quot;);
2659   Node* ret = call-&gt;proj_out_or_null(TypeFunc::Parms);
2660   if (ret == NULL) {
2661     return;
2662   }
2663   const TypeFunc* tf = call-&gt;_tf;
<span class="line-modified">2664   const TypeTuple* domain = OptoRuntime::store_value_type_fields_Type()-&gt;domain_cc();</span>
2665   const TypeFunc* new_tf = TypeFunc::make(tf-&gt;domain_sig(), tf-&gt;domain_cc(), tf-&gt;range_sig(), domain);
2666   call-&gt;_tf = new_tf;
2667   // Make sure the change of type is applied before projections are processed by igvn
2668   _igvn.set_type(call, call-&gt;Value(&amp;_igvn));
2669   _igvn.set_type(ret, ret-&gt;Value(&amp;_igvn));
2670 
2671   // Before any new projection is added:
2672   CallProjections* projs = call-&gt;extract_projections(true, true);
2673 
2674   Node* ctl = new Node(1);
2675   Node* mem = new Node(1);
2676   Node* io = new Node(1);
2677   Node* ex_ctl = new Node(1);
2678   Node* ex_mem = new Node(1);
2679   Node* ex_io = new Node(1);
2680   Node* res = new Node(1);
2681 
2682   Node* cast = transform_later(new CastP2XNode(ctl, res));
2683   Node* mask = MakeConX(0x1);
2684   Node* masked = transform_later(new AndXNode(cast, mask));
</pre>
<hr />
<pre>
2709     Node* layout_val = make_load(NULL, mem, klass_node, in_bytes(Klass::layout_helper_offset()), TypeInt::INT, T_INT);
2710     Node* size_in_bytes = ConvI2X(layout_val);
2711     new_top = new AddPNode(top(), old_top, size_in_bytes);
2712     transform_later(new_top);
2713     Node* slowpath_cmp = new CmpPNode(new_top, end);
2714     transform_later(slowpath_cmp);
2715     slowpath_bol = new BoolNode(slowpath_cmp, BoolTest::ge);
2716     transform_later(slowpath_bol);
2717   } else {
2718     slowpath_bol = intcon(1);
2719     top_adr = top();
2720     old_top = top();
2721     new_top = top();
2722   }
2723   IfNode* slowpath_iff = new IfNode(allocation_ctl, slowpath_bol, PROB_UNLIKELY_MAG(4), COUNT_UNKNOWN);
2724   transform_later(slowpath_iff);
2725 
2726   Node* slowpath_true = new IfTrueNode(slowpath_iff);
2727   transform_later(slowpath_true);
2728 
<span class="line-modified">2729   CallStaticJavaNode* slow_call = new CallStaticJavaNode(OptoRuntime::store_value_type_fields_Type(),</span>
2730                                                          StubRoutines::store_inline_type_fields_to_buf(),
<span class="line-modified">2731                                                          &quot;store_value_type_fields&quot;,</span>
2732                                                          call-&gt;jvms()-&gt;bci(),
2733                                                          TypePtr::BOTTOM);
2734   slow_call-&gt;init_req(TypeFunc::Control, slowpath_true);
2735   slow_call-&gt;init_req(TypeFunc::Memory, mem);
2736   slow_call-&gt;init_req(TypeFunc::I_O, io);
2737   slow_call-&gt;init_req(TypeFunc::FramePtr, call-&gt;in(TypeFunc::FramePtr));
2738   slow_call-&gt;init_req(TypeFunc::ReturnAdr, call-&gt;in(TypeFunc::ReturnAdr));
2739   slow_call-&gt;init_req(TypeFunc::Parms, res);
2740 
2741   Node* slow_ctl = transform_later(new ProjNode(slow_call, TypeFunc::Control));
2742   Node* slow_mem = transform_later(new ProjNode(slow_call, TypeFunc::Memory));
2743   Node* slow_io = transform_later(new ProjNode(slow_call, TypeFunc::I_O));
2744   Node* slow_res = transform_later(new ProjNode(slow_call, TypeFunc::Parms));
2745   Node* slow_catc = transform_later(new CatchNode(slow_ctl, slow_io, 2));
2746   Node* slow_norm = transform_later(new CatchProjNode(slow_catc, CatchProjNode::fall_through_index, CatchProjNode::no_handler_bci));
2747   Node* slow_excp = transform_later(new CatchProjNode(slow_catc, CatchProjNode::catch_all_index,    CatchProjNode::no_handler_bci));
2748 
2749   Node* ex_r = new RegionNode(3);
2750   Node* ex_mem_phi = new PhiNode(ex_r, Type::MEMORY, TypePtr::BOTTOM);
2751   Node* ex_io_phi = new PhiNode(ex_r, Type::ABIO);
</pre>
<hr />
<pre>
2756   ex_mem_phi-&gt;init_req(2, ex_mem);
2757   ex_io_phi-&gt;init_req(2, ex_io);
2758 
2759   transform_later(ex_r);
2760   transform_later(ex_mem_phi);
2761   transform_later(ex_io_phi);
2762 
2763   Node* slowpath_false = new IfFalseNode(slowpath_iff);
2764   transform_later(slowpath_false);
2765   Node* rawmem = new StorePNode(slowpath_false, mem, top_adr, TypeRawPtr::BOTTOM, new_top, MemNode::unordered);
2766   transform_later(rawmem);
2767   Node* mark_node = makecon(TypeRawPtr::make((address)markWord::always_locked_prototype().value()));
2768   rawmem = make_store(slowpath_false, rawmem, old_top, oopDesc::mark_offset_in_bytes(), mark_node, T_ADDRESS);
2769   rawmem = make_store(slowpath_false, rawmem, old_top, oopDesc::klass_offset_in_bytes(), klass_node, T_METADATA);
2770   if (UseCompressedClassPointers) {
2771     rawmem = make_store(slowpath_false, rawmem, old_top, oopDesc::klass_gap_offset_in_bytes(), intcon(0), T_INT);
2772   }
2773   Node* fixed_block  = make_load(slowpath_false, rawmem, klass_node, in_bytes(InstanceKlass::adr_inlineklass_fixed_block_offset()), TypeRawPtr::BOTTOM, T_ADDRESS);
2774   Node* pack_handler = make_load(slowpath_false, rawmem, fixed_block, in_bytes(InlineKlass::pack_handler_offset()), TypeRawPtr::BOTTOM, T_ADDRESS);
2775 
<span class="line-modified">2776   CallLeafNoFPNode* handler_call = new CallLeafNoFPNode(OptoRuntime::pack_value_type_Type(),</span>
2777                                                         NULL,
2778                                                         &quot;pack handler&quot;,
2779                                                         TypeRawPtr::BOTTOM);
2780   handler_call-&gt;init_req(TypeFunc::Control, slowpath_false);
2781   handler_call-&gt;init_req(TypeFunc::Memory, rawmem);
2782   handler_call-&gt;init_req(TypeFunc::I_O, top());
2783   handler_call-&gt;init_req(TypeFunc::FramePtr, call-&gt;in(TypeFunc::FramePtr));
2784   handler_call-&gt;init_req(TypeFunc::ReturnAdr, top());
2785   handler_call-&gt;init_req(TypeFunc::Parms, pack_handler);
2786   handler_call-&gt;init_req(TypeFunc::Parms+1, old_top);
2787 
2788   // We don&#39;t know how many values are returned. This assumes the
2789   // worst case, that all available registers are used.
2790   for (uint i = TypeFunc::Parms+1; i &lt; domain-&gt;cnt(); i++) {
2791     if (domain-&gt;field_at(i) == Type::HALF) {
2792       slow_call-&gt;init_req(i, top());
2793       handler_call-&gt;init_req(i+1, top());
2794       continue;
2795     }
2796     Node* proj = transform_later(new ProjNode(call, i));
</pre>
</td>
<td>
<hr />
<pre>
   6  * under the terms of the GNU General Public License version 2 only, as
   7  * published by the Free Software Foundation.
   8  *
   9  * This code is distributed in the hope that it will be useful, but WITHOUT
  10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #include &quot;precompiled.hpp&quot;
<span class="line-added">  26 #include &quot;ci/ciFlatArrayKlass.hpp&quot;</span>
  27 #include &quot;compiler/compileLog.hpp&quot;
  28 #include &quot;gc/shared/collectedHeap.inline.hpp&quot;
  29 #include &quot;libadt/vectset.hpp&quot;
  30 #include &quot;memory/universe.hpp&quot;
  31 #include &quot;opto/addnode.hpp&quot;
  32 #include &quot;opto/arraycopynode.hpp&quot;
  33 #include &quot;opto/callnode.hpp&quot;
  34 #include &quot;opto/castnode.hpp&quot;
  35 #include &quot;opto/cfgnode.hpp&quot;
  36 #include &quot;opto/compile.hpp&quot;
  37 #include &quot;opto/convertnode.hpp&quot;
  38 #include &quot;opto/graphKit.hpp&quot;
<span class="line-added">  39 #include &quot;opto/inlinetypenode.hpp&quot;</span>
  40 #include &quot;opto/intrinsicnode.hpp&quot;
  41 #include &quot;opto/locknode.hpp&quot;
  42 #include &quot;opto/loopnode.hpp&quot;
  43 #include &quot;opto/macro.hpp&quot;
  44 #include &quot;opto/memnode.hpp&quot;
  45 #include &quot;opto/narrowptrnode.hpp&quot;
  46 #include &quot;opto/node.hpp&quot;
  47 #include &quot;opto/opaquenode.hpp&quot;
  48 #include &quot;opto/phaseX.hpp&quot;
  49 #include &quot;opto/rootnode.hpp&quot;
  50 #include &quot;opto/runtime.hpp&quot;
  51 #include &quot;opto/subnode.hpp&quot;
  52 #include &quot;opto/subtypenode.hpp&quot;
  53 #include &quot;opto/type.hpp&quot;

  54 #include &quot;runtime/sharedRuntime.hpp&quot;
  55 #include &quot;utilities/macros.hpp&quot;
  56 #include &quot;utilities/powerOfTwo.hpp&quot;
  57 #if INCLUDE_G1GC
  58 #include &quot;gc/g1/g1ThreadLocalData.hpp&quot;
  59 #endif // INCLUDE_G1GC
  60 #if INCLUDE_SHENANDOAHGC
  61 #include &quot;gc/shenandoah/c2/shenandoahBarrierSetC2.hpp&quot;
  62 #endif
  63 
  64 
  65 //
  66 // Replace any references to &quot;oldref&quot; in inputs to &quot;use&quot; with &quot;newref&quot;.
  67 // Returns the number of replacements made.
  68 //
  69 int PhaseMacroExpand::replace_input(Node *use, Node *oldref, Node *newref) {
  70   int nreplacements = 0;
  71   uint req = use-&gt;req();
  72   for (uint j = 0; j &lt; use-&gt;len(); j++) {
  73     Node *uin = use-&gt;in(j);
</pre>
<hr />
<pre>
 339         adr_type = _igvn.type(adr)-&gt;is_ptr();
 340         assert(adr_type == _igvn.type(base)-&gt;is_aryptr()-&gt;add_field_offset_and_offset(off), &quot;incorrect address type&quot;);
 341         if (ac-&gt;in(ArrayCopyNode::Src) == ac-&gt;in(ArrayCopyNode::Dest)) {
 342           // Don&#39;t emit a new load from src if src == dst but try to get the value from memory instead
 343           return value_from_mem(ac-&gt;in(TypeFunc::Memory), ctl, ft, ftype, adr_type-&gt;isa_oopptr(), alloc);
 344         }
 345       } else {
 346         if (ac-&gt;in(ArrayCopyNode::Src) == ac-&gt;in(ArrayCopyNode::Dest)) {
 347           // Non constant offset in the array: we can&#39;t statically
 348           // determine the value
 349           return NULL;
 350         }
 351         Node* diff = _igvn.transform(new SubINode(ac-&gt;in(ArrayCopyNode::SrcPos), ac-&gt;in(ArrayCopyNode::DestPos)));
 352 #ifdef _LP64
 353         diff = _igvn.transform(new ConvI2LNode(diff));
 354 #endif
 355         diff = _igvn.transform(new LShiftXNode(diff, intcon(shift)));
 356 
 357         Node* off = _igvn.transform(new AddXNode(MakeConX(offset), diff));
 358         adr = _igvn.transform(new AddPNode(base, base, off));
<span class="line-modified"> 359         // In the case of a flattened inline type array, each field has its</span>
 360         // own slice so we need to extract the field being accessed from
 361         // the address computation
 362         adr_type = adr_type-&gt;is_aryptr()-&gt;add_field_offset_and_offset(offset)-&gt;add_offset(Type::OffsetBot);
 363         adr = _igvn.transform(new CastPPNode(adr, adr_type));
 364       }
 365       res = LoadNode::make(_igvn, ctl, mem, adr, adr_type, type, bt, MemNode::unordered, LoadNode::UnknownControl);
 366     }
 367   }
 368   if (res != NULL) {
 369     res = _igvn.transform(res);
 370     if (ftype-&gt;isa_narrowoop()) {
 371       // PhaseMacroExpand::scalar_replacement adds DecodeN nodes
 372       assert(res-&gt;isa_DecodeN(), &quot;should be narrow oop&quot;);
 373       res = _igvn.transform(new EncodePNode(res, ftype));
 374     }
 375     return res;
 376   }
 377   return NULL;
 378 }
 379 
</pre>
<hr />
<pre>
 582           Node* n = value_phis.node();
 583           _igvn.replace_node(n, C-&gt;top());
 584           value_phis.pop();
 585         }
 586       }
 587     } else if (mem-&gt;is_ArrayCopy()) {
 588       Node* ctl = mem-&gt;in(0);
 589       Node* m = mem-&gt;in(TypeFunc::Memory);
 590       if (sfpt_ctl-&gt;is_Proj() &amp;&amp; sfpt_ctl-&gt;as_Proj()-&gt;is_uncommon_trap_proj(Deoptimization::Reason_none)) {
 591         // pin the loads in the uncommon trap path
 592         ctl = sfpt_ctl;
 593         m = sfpt_mem;
 594       }
 595       return make_arraycopy_load(mem-&gt;as_ArrayCopy(), offset, ctl, m, ft, ftype, alloc);
 596     }
 597   }
 598   // Something went wrong.
 599   return NULL;
 600 }
 601 
<span class="line-modified"> 602 // Search the last value stored into the inline type&#39;s fields.</span>
<span class="line-modified"> 603 Node* PhaseMacroExpand::inline_type_from_mem(Node* mem, Node* ctl, ciInlineKlass* vk, const TypeAryPtr* adr_type, int offset, AllocateNode* alloc) {</span>
 604   // Subtract the offset of the first field to account for the missing oop header
 605   offset -= vk-&gt;first_field_offset();
<span class="line-modified"> 606   // Create a new InlineTypeNode and retrieve the field values from memory</span>
<span class="line-modified"> 607   InlineTypeNode* vt = InlineTypeNode::make_uninitialized(_igvn, vk)-&gt;as_InlineType();</span>
 608   for (int i = 0; i &lt; vk-&gt;nof_declared_nonstatic_fields(); ++i) {
 609     ciType* field_type = vt-&gt;field_type(i);
 610     int field_offset = offset + vt-&gt;field_offset(i);
<span class="line-modified"> 611     // Each inline type field has its own memory slice</span>
 612     adr_type = adr_type-&gt;with_field_offset(field_offset);
 613     Node* value = NULL;
 614     if (vt-&gt;field_is_flattened(i)) {
<span class="line-modified"> 615       value = inline_type_from_mem(mem, ctl, field_type-&gt;as_inline_klass(), adr_type, field_offset, alloc);</span>
 616     } else {
 617       const Type* ft = Type::get_const_type(field_type);
 618       BasicType bt = field_type-&gt;basic_type();
 619       if (UseCompressedOops &amp;&amp; !is_java_primitive(bt)) {
 620         ft = ft-&gt;make_narrowoop();
 621         bt = T_NARROWOOP;
 622       }
 623       value = value_from_mem(mem, ctl, bt, ft, adr_type, alloc);
 624       if (value != NULL &amp;&amp; ft-&gt;isa_narrowoop()) {
 625         assert(UseCompressedOops, &quot;unexpected narrow oop&quot;);
 626         value = transform_later(new DecodeNNode(value, value-&gt;get_ptr_type()));
 627       }
 628     }
 629     if (value != NULL) {
 630       vt-&gt;set_field_value(i, value);
 631     } else {
 632       // We might have reached the TrackedInitializationLimit
 633       return NULL;
 634     }
 635   }
</pre>
<hr />
<pre>
 699                   use-&gt;as_ArrayCopy()-&gt;is_copyof_validated() ||
 700                   use-&gt;as_ArrayCopy()-&gt;is_copyofrange_validated()) &amp;&amp;
 701                  use-&gt;in(ArrayCopyNode::Dest) == res) {
 702         // ok to eliminate
 703       } else if (use-&gt;is_SafePoint()) {
 704         SafePointNode* sfpt = use-&gt;as_SafePoint();
 705         if (sfpt-&gt;is_Call() &amp;&amp; sfpt-&gt;as_Call()-&gt;has_non_debug_use(res)) {
 706           // Object is passed as argument.
 707           DEBUG_ONLY(disq_node = use;)
 708           NOT_PRODUCT(fail_eliminate = &quot;Object is passed as argument&quot;;)
 709           can_eliminate = false;
 710         }
 711         Node* sfptMem = sfpt-&gt;memory();
 712         if (sfptMem == NULL || sfptMem-&gt;is_top()) {
 713           DEBUG_ONLY(disq_node = use;)
 714           NOT_PRODUCT(fail_eliminate = &quot;NULL or TOP memory&quot;;)
 715           can_eliminate = false;
 716         } else {
 717           safepoints.append_if_missing(sfpt);
 718         }
<span class="line-modified"> 719       } else if (use-&gt;is_InlineType() &amp;&amp; use-&gt;isa_InlineType()-&gt;get_oop() == res) {</span>
 720         // ok to eliminate
 721       } else if (use-&gt;Opcode() == Op_StoreX &amp;&amp; use-&gt;in(MemNode::Address) == res) {
 722         // store to mark work
 723       } else if (use-&gt;Opcode() != Op_CastP2X) { // CastP2X is used by card mark
 724         if (use-&gt;is_Phi()) {
 725           if (use-&gt;outcnt() == 1 &amp;&amp; use-&gt;unique_out()-&gt;Opcode() == Op_Return) {
 726             NOT_PRODUCT(fail_eliminate = &quot;Object is return value&quot;;)
 727           } else {
 728             NOT_PRODUCT(fail_eliminate = &quot;Object is referenced by Phi&quot;;)
 729           }
 730           DEBUG_ONLY(disq_node = use;)
 731         } else {
 732           if (use-&gt;Opcode() == Op_Return) {
 733             NOT_PRODUCT(fail_eliminate = &quot;Object is return value&quot;;)
 734           } else {
 735             NOT_PRODUCT(fail_eliminate = &quot;Object is referenced by node&quot;;)
 736           }
 737           DEBUG_ONLY(disq_node = use;)
 738         }
 739         can_eliminate = false;
</pre>
<hr />
<pre>
 785   Node* res = alloc-&gt;result_cast();
 786   assert(res == NULL || res-&gt;is_CheckCastPP(), &quot;unexpected AllocateNode result&quot;);
 787   const TypeOopPtr* res_type = NULL;
 788   if (res != NULL) { // Could be NULL when there are no users
 789     res_type = _igvn.type(res)-&gt;isa_oopptr();
 790   }
 791 
 792   if (res != NULL) {
 793     klass = res_type-&gt;klass();
 794     if (res_type-&gt;isa_instptr()) {
 795       // find the fields of the class which will be needed for safepoint debug information
 796       assert(klass-&gt;is_instance_klass(), &quot;must be an instance klass.&quot;);
 797       iklass = klass-&gt;as_instance_klass();
 798       nfields = iklass-&gt;nof_nonstatic_fields();
 799     } else {
 800       // find the array&#39;s elements which will be needed for safepoint debug information
 801       nfields = alloc-&gt;in(AllocateNode::ALength)-&gt;find_int_con(-1);
 802       assert(klass-&gt;is_array_klass() &amp;&amp; nfields &gt;= 0, &quot;must be an array klass.&quot;);
 803       elem_type = klass-&gt;as_array_klass()-&gt;element_type();
 804       basic_elem_type = elem_type-&gt;basic_type();
<span class="line-modified"> 805       if (elem_type-&gt;is_inlinetype() &amp;&amp; !klass-&gt;is_flat_array_klass()) {</span>
 806         assert(basic_elem_type == T_INLINE_TYPE, &quot;unexpected element basic type&quot;);
 807         basic_elem_type = T_OBJECT;
 808       }
 809       array_base = arrayOopDesc::base_offset_in_bytes(basic_elem_type);
 810       element_size = type2aelembytes(basic_elem_type);
<span class="line-modified"> 811       if (klass-&gt;is_flat_array_klass()) {</span>
<span class="line-modified"> 812         // Flattened inline type array</span>
<span class="line-modified"> 813         element_size = klass-&gt;as_flat_array_klass()-&gt;element_byte_size();</span>
 814       }
 815     }
 816   }
 817   //
 818   // Process the safepoint uses
 819   //
 820   Unique_Node_List value_worklist;
 821   while (safepoints.length() &gt; 0) {
 822     SafePointNode* sfpt = safepoints.pop();
 823     Node* mem = sfpt-&gt;memory();
 824     Node* ctl = sfpt-&gt;control();
 825     assert(sfpt-&gt;jvms() != NULL, &quot;missed JVMS&quot;);
 826     // Fields of scalar objs are referenced only at the end
 827     // of regular debuginfo at the last (youngest) JVMS.
 828     // Record relative start index.
 829     uint first_ind = (sfpt-&gt;req() - sfpt-&gt;jvms()-&gt;scloff());
 830     SafePointScalarObjectNode* sobj = new SafePointScalarObjectNode(res_type,
 831 #ifdef ASSERT
 832                                                  alloc,
 833 #endif
 834                                                  first_ind, nfields);
 835     sobj-&gt;init_req(0, C-&gt;root());
 836     transform_later(sobj);
 837 
 838     // Scan object&#39;s fields adding an input to the safepoint for each field.
 839     for (int j = 0; j &lt; nfields; j++) {
 840       intptr_t offset;
 841       ciField* field = NULL;
 842       if (iklass != NULL) {
 843         field = iklass-&gt;nonstatic_field_at(j);
 844         offset = field-&gt;offset();
 845         elem_type = field-&gt;type();
 846         basic_elem_type = field-&gt;layout_type();
<span class="line-modified"> 847         assert(!field-&gt;is_flattened(), &quot;flattened inline type fields should not have safepoint uses&quot;);</span>
 848       } else {
 849         offset = array_base + j * (intptr_t)element_size;
 850       }
 851 
 852       const Type *field_type;
 853       // The next code is taken from Parse::do_get_xxx().
 854       if (is_reference_type(basic_elem_type)) {
 855         if (!elem_type-&gt;is_loaded()) {
 856           field_type = TypeInstPtr::BOTTOM;
 857         } else if (field != NULL &amp;&amp; field-&gt;is_static_constant()) {
 858           // This can happen if the constant oop is non-perm.
 859           ciObject* con = field-&gt;constant_value().as_object();
 860           // Do not &quot;join&quot; in the previous type; it doesn&#39;t add value,
 861           // and may yield a vacuous result if the field is of interface type.
 862           field_type = TypeOopPtr::make_from_constant(con)-&gt;isa_oopptr();
 863           assert(field_type != NULL, &quot;field singleton type must be consistent&quot;);
 864         } else {
 865           field_type = TypeOopPtr::make_from_klass(elem_type-&gt;as_klass());
 866         }
 867         if (UseCompressedOops) {
 868           field_type = field_type-&gt;make_narrowoop();
 869           basic_elem_type = T_NARROWOOP;
 870         }
 871       } else {
 872         field_type = Type::get_const_basic_type(basic_elem_type);
 873       }
 874 
 875       Node* field_val = NULL;
 876       const TypeOopPtr* field_addr_type = res_type-&gt;add_offset(offset)-&gt;isa_oopptr();
<span class="line-modified"> 877       if (klass-&gt;is_flat_array_klass()) {</span>
<span class="line-modified"> 878         ciInlineKlass* vk = elem_type-&gt;as_inline_klass();</span>
 879         assert(vk-&gt;flatten_array(), &quot;must be flattened&quot;);
<span class="line-modified"> 880         field_val = inline_type_from_mem(mem, ctl, vk, field_addr_type-&gt;isa_aryptr(), 0, alloc);</span>
 881       } else {
 882         field_val = value_from_mem(mem, ctl, basic_elem_type, field_type, field_addr_type, alloc);
 883       }
 884       if (field_val == NULL) {
 885         // We weren&#39;t able to find a value for this field,
 886         // give up on eliminating this allocation.
 887 
 888         // Remove any extra entries we added to the safepoint.
 889         uint last = sfpt-&gt;req() - 1;
 890         for (int k = 0;  k &lt; j; k++) {
 891           sfpt-&gt;del_req(last--);
 892         }
 893         _igvn._worklist.push(sfpt);
 894         // rollback processed safepoints
 895         while (safepoints_done.length() &gt; 0) {
 896           SafePointNode* sfpt_done = safepoints_done.pop();
 897           // remove any extra entries we added to the safepoint
 898           last = sfpt_done-&gt;req() - 1;
 899           for (int k = 0;  k &lt; nfields; k++) {
 900             sfpt_done-&gt;del_req(last--);
</pre>
<hr />
<pre>
 921         if (PrintEliminateAllocations) {
 922           if (field != NULL) {
 923             tty-&gt;print(&quot;=== At SafePoint node %d can&#39;t find value of Field: &quot;,
 924                        sfpt-&gt;_idx);
 925             field-&gt;print();
 926             int field_idx = C-&gt;get_alias_index(field_addr_type);
 927             tty-&gt;print(&quot; (alias_idx=%d)&quot;, field_idx);
 928           } else { // Array&#39;s element
 929             tty-&gt;print(&quot;=== At SafePoint node %d can&#39;t find value of array element [%d]&quot;,
 930                        sfpt-&gt;_idx, j);
 931           }
 932           tty-&gt;print(&quot;, which prevents elimination of: &quot;);
 933           if (res == NULL)
 934             alloc-&gt;dump();
 935           else
 936             res-&gt;dump();
 937         }
 938 #endif
 939         return false;
 940       }
<span class="line-modified"> 941       if (field_val-&gt;is_InlineType()) {</span>
<span class="line-modified"> 942         // Keep track of inline types to scalarize them later</span>
 943         value_worklist.push(field_val);
 944       } else if (UseCompressedOops &amp;&amp; field_type-&gt;isa_narrowoop()) {
 945         // Enable &quot;DecodeN(EncodeP(Allocate)) --&gt; Allocate&quot; transformation
 946         // to be able scalar replace the allocation.
 947         if (field_val-&gt;is_EncodeP()) {
 948           field_val = field_val-&gt;in(1);
 949         } else {
 950           field_val = transform_later(new DecodeNNode(field_val, field_val-&gt;get_ptr_type()));
 951         }
 952       }
 953       sfpt-&gt;add_req(field_val);
 954     }
 955     JVMState *jvms = sfpt-&gt;jvms();
 956     jvms-&gt;set_endoff(sfpt-&gt;req());
 957     // Now make a pass over the debug information replacing any references
 958     // to the allocated object with &quot;sobj&quot;
 959     int start = jvms-&gt;debug_start();
 960     int end   = jvms-&gt;debug_end();
 961     sfpt-&gt;replace_edges_in_range(res, sobj, start, end);
 962     _igvn._worklist.push(sfpt);
 963     safepoints_done.append_if_missing(sfpt); // keep it for rollback
 964   }
<span class="line-modified"> 965   // Scalarize inline types that were added to the safepoint</span>
 966   for (uint i = 0; i &lt; value_worklist.size(); ++i) {
 967     Node* vt = value_worklist.at(i);
<span class="line-modified"> 968     vt-&gt;as_InlineType()-&gt;make_scalar_in_safepoints(&amp;_igvn);</span>
 969   }
 970   return true;
 971 }
 972 
 973 static void disconnect_projections(MultiNode* n, PhaseIterGVN&amp; igvn) {
 974   Node* ctl_proj = n-&gt;proj_out_or_null(TypeFunc::Control);
 975   Node* mem_proj = n-&gt;proj_out_or_null(TypeFunc::Memory);
 976   if (ctl_proj != NULL) {
 977     igvn.replace_node(ctl_proj, n-&gt;in(0));
 978   }
 979   if (mem_proj != NULL) {
 980     igvn.replace_node(mem_proj, n-&gt;in(TypeFunc::Memory));
 981   }
 982 }
 983 
 984 // Process users of eliminated allocation.
 985 void PhaseMacroExpand::process_users_of_allocation(CallNode *alloc, bool inline_alloc) {
 986   Node* res = alloc-&gt;result_cast();
 987   if (res != NULL) {
 988     for (DUIterator_Last jmin, j = res-&gt;last_outs(jmin); j &gt;= jmin; ) {
</pre>
<hr />
<pre>
1029 
1030           _igvn.replace_node(callprojs-&gt;fallthrough_ioproj, ac-&gt;in(TypeFunc::I_O));
1031           _igvn.replace_node(callprojs-&gt;fallthrough_memproj, ac-&gt;in(TypeFunc::Memory));
1032           _igvn.replace_node(callprojs-&gt;fallthrough_catchproj, ac-&gt;in(TypeFunc::Control));
1033 
1034           // Set control to top. IGVN will remove the remaining projections
1035           ac-&gt;set_req(0, top());
1036           ac-&gt;replace_edge(res, top());
1037 
1038           // Disconnect src right away: it can help find new
1039           // opportunities for allocation elimination
1040           Node* src = ac-&gt;in(ArrayCopyNode::Src);
1041           ac-&gt;replace_edge(src, top());
1042           // src can be top at this point if src and dest of the
1043           // arraycopy were the same
1044           if (src-&gt;outcnt() == 0 &amp;&amp; !src-&gt;is_top()) {
1045             _igvn.remove_dead_node(src);
1046           }
1047         }
1048         _igvn._worklist.push(ac);
<span class="line-modified">1049       } else if (use-&gt;is_InlineType()) {</span>
<span class="line-modified">1050         assert(use-&gt;isa_InlineType()-&gt;get_oop() == res, &quot;unexpected inline type use&quot;);</span>
<span class="line-modified">1051         _igvn.rehash_node_delayed(use);</span>
<span class="line-modified">1052         use-&gt;isa_InlineType()-&gt;set_oop(_igvn.zerocon(T_INLINE_TYPE));</span>
1053       } else if (use-&gt;is_Store()) {
1054         _igvn.replace_node(use, use-&gt;in(MemNode::Memory));
1055       } else {
1056         eliminate_gc_barrier(use);
1057       }
1058       j -= (oc1 - res-&gt;outcnt());
1059     }
1060     assert(res-&gt;outcnt() == 0, &quot;all uses of allocated objects must be deleted&quot;);
1061     _igvn.remove_dead_node(res);
1062   }
1063 
1064   //
1065   // Process other users of allocation&#39;s projections
1066   //
1067   if (_resproj != NULL &amp;&amp; _resproj-&gt;outcnt() != 0) {
1068     // First disconnect stores captured by Initialize node.
1069     // If Initialize node is eliminated first in the following code,
1070     // it will kill such stores and DUIterator_Last will assert.
1071     for (DUIterator_Fast jmax, j = _resproj-&gt;fast_outs(jmax);  j &lt; jmax; j++) {
1072       Node *use = _resproj-&gt;fast_out(j);
</pre>
<hr />
<pre>
1134     _igvn.replace_node(_ioproj_catchall, C-&gt;top());
1135   }
1136   if (_catchallcatchproj != NULL) {
1137     _igvn.replace_node(_catchallcatchproj, C-&gt;top());
1138   }
1139 }
1140 
1141 bool PhaseMacroExpand::eliminate_allocate_node(AllocateNode *alloc) {
1142   // Don&#39;t do scalar replacement if the frame can be popped by JVMTI:
1143   // if reallocation fails during deoptimization we&#39;ll pop all
1144   // interpreter frames for this compiled frame and that won&#39;t play
1145   // nice with JVMTI popframe.
1146   if (!EliminateAllocations || JvmtiExport::can_pop_frame()) {
1147     return false;
1148   }
1149   Node* klass = alloc-&gt;in(AllocateNode::KlassNode);
1150   const TypeKlassPtr* tklass = _igvn.type(klass)-&gt;is_klassptr();
1151 
1152   // Attempt to eliminate inline type buffer allocations
1153   // regardless of usage and escape/replaceable status.
<span class="line-modified">1154   bool inline_alloc = tklass-&gt;klass()-&gt;is_inlinetype();</span>
1155   if (!alloc-&gt;_is_non_escaping &amp;&amp; !inline_alloc) {
1156     return false;
1157   }
1158   // Eliminate boxing allocations which are not used
1159   // regardless of scalar replaceable status.
1160   Node* res = alloc-&gt;result_cast();
1161   bool boxing_alloc = (res == NULL) &amp;&amp; C-&gt;eliminate_boxing() &amp;&amp;
1162                       tklass-&gt;klass()-&gt;is_instance_klass() &amp;&amp;
1163                       tklass-&gt;klass()-&gt;as_instance_klass()-&gt;is_box_klass();
1164   if (!alloc-&gt;_is_scalar_replaceable &amp;&amp; !boxing_alloc &amp;&amp; !inline_alloc) {
1165     return false;
1166   }
1167 
1168   extract_call_projections(alloc);
1169 
1170   GrowableArray &lt;SafePointNode *&gt; safepoints;
1171   if (!can_eliminate_allocation(alloc, safepoints)) {
1172     return false;
1173   }
1174 
</pre>
<hr />
<pre>
2206         // of the same object and mark related locks as eliminated.
2207         mark_eliminated_box(box, obj);
2208       }
2209     }
2210   }
2211 }
2212 
2213 // we have determined that this lock/unlock can be eliminated, we simply
2214 // eliminate the node without expanding it.
2215 //
2216 // Note:  The membar&#39;s associated with the lock/unlock are currently not
2217 //        eliminated.  This should be investigated as a future enhancement.
2218 //
2219 bool PhaseMacroExpand::eliminate_locking_node(AbstractLockNode *alock) {
2220 
2221   if (!alock-&gt;is_eliminated()) {
2222     return false;
2223   }
2224 #ifdef ASSERT
2225   const Type* obj_type = _igvn.type(alock-&gt;obj_node());
<span class="line-modified">2226   assert(!obj_type-&gt;isa_inlinetype() &amp;&amp; !obj_type-&gt;is_inlinetypeptr(), &quot;Eliminating lock on inline type&quot;);</span>
2227   if (!alock-&gt;is_coarsened()) {
2228     // Check that new &quot;eliminated&quot; BoxLock node is created.
2229     BoxLockNode* oldbox = alock-&gt;box_node()-&gt;as_BoxLock();
2230     assert(oldbox-&gt;is_eliminated(), &quot;should be done already&quot;);
2231   }
2232 #endif
2233 
2234   alock-&gt;log_lock_optimization(C, &quot;eliminate_lock&quot;);
2235 
2236 #ifndef PRODUCT
2237   if (PrintEliminateLocks) {
2238     if (alock-&gt;is_Lock()) {
2239       tty-&gt;print_cr(&quot;++++ Eliminated: %d Lock&quot;, alock-&gt;_idx);
2240     } else {
2241       tty-&gt;print_cr(&quot;++++ Eliminated: %d Unlock&quot;, alock-&gt;_idx);
2242     }
2243   }
2244 #endif
2245 
2246   Node* mem  = alock-&gt;in(TypeFunc::Memory);
</pre>
<hr />
<pre>
2489 
2490     slow_path-&gt;init_req(2, ctrl); // Capture slow-control
2491     slow_mem-&gt;init_req(2, fast_lock_mem_phi);
2492 
2493     transform_later(slow_path);
2494     transform_later(slow_mem);
2495     // Reset lock&#39;s memory edge.
2496     lock-&gt;set_req(TypeFunc::Memory, slow_mem);
2497 
2498   } else {
2499     region  = new RegionNode(3);
2500     // create a Phi for the memory state
2501     mem_phi = new PhiNode( region, Type::MEMORY, TypeRawPtr::BOTTOM);
2502 
2503     // Optimize test; set region slot 2
2504     slow_path = opt_bits_test(ctrl, region, 2, flock, 0, 0);
2505     mem_phi-&gt;init_req(2, mem);
2506   }
2507 
2508   const TypeOopPtr* objptr = _igvn.type(obj)-&gt;make_oopptr();
<span class="line-modified">2509   if (objptr-&gt;can_be_inline_type()) {</span>
2510     // Deoptimize and re-execute if a value
<span class="line-modified">2511     assert(EnableValhalla, &quot;should only be used if inline types are enabled&quot;);</span>
2512     Node* mark = make_load(slow_path, mem, obj, oopDesc::mark_offset_in_bytes(), TypeX_X, TypeX_X-&gt;basic_type());
2513     Node* value_mask = _igvn.MakeConX(markWord::always_locked_pattern);
2514     Node* is_value = _igvn.transform(new AndXNode(mark, value_mask));
2515     Node* cmp = _igvn.transform(new CmpXNode(is_value, value_mask));
2516     Node* bol = _igvn.transform(new BoolNode(cmp, BoolTest::eq));
2517     Node* unc_ctrl = generate_slow_guard(&amp;slow_path, bol, NULL);
2518 
2519     int trap_request = Deoptimization::make_trap_request(Deoptimization::Reason_class_check, Deoptimization::Action_none);
2520     address call_addr = SharedRuntime::uncommon_trap_blob()-&gt;entry_point();
2521     const TypePtr* no_memory_effects = NULL;
2522     JVMState* jvms = lock-&gt;jvms();
2523     CallNode* unc = new CallStaticJavaNode(OptoRuntime::uncommon_trap_Type(), call_addr, &quot;uncommon_trap&quot;,
2524                                            jvms-&gt;bci(), no_memory_effects);
2525 
2526     unc-&gt;init_req(TypeFunc::Control, unc_ctrl);
2527     unc-&gt;init_req(TypeFunc::I_O, lock-&gt;i_o());
2528     unc-&gt;init_req(TypeFunc::Memory, mem); // may gc ptrs
2529     unc-&gt;init_req(TypeFunc::FramePtr,  lock-&gt;in(TypeFunc::FramePtr));
2530     unc-&gt;init_req(TypeFunc::ReturnAdr, lock-&gt;in(TypeFunc::ReturnAdr));
2531     unc-&gt;init_req(TypeFunc::Parms+0, _igvn.intcon(trap_request));
</pre>
<hr />
<pre>
2631   // No exceptions for unlocking
2632   // Capture slow path
2633   // disconnect fall-through projection from call and create a new one
2634   // hook up users of fall-through projection to region
2635   Node *slow_ctrl = _fallthroughproj-&gt;clone();
2636   transform_later(slow_ctrl);
2637   _igvn.hash_delete(_fallthroughproj);
2638   _fallthroughproj-&gt;disconnect_inputs(NULL, C);
2639   region-&gt;init_req(1, slow_ctrl);
2640   // region inputs are now complete
2641   transform_later(region);
2642   _igvn.replace_node(_fallthroughproj, region);
2643 
2644   Node *memproj = transform_later(new ProjNode(call, TypeFunc::Memory) );
2645   mem_phi-&gt;init_req(1, memproj );
2646   mem_phi-&gt;init_req(2, mem);
2647   transform_later(mem_phi);
2648   _igvn.replace_node(_memproj_fallthrough, mem_phi);
2649 }
2650 
<span class="line-modified">2651 // An inline type might be returned from the call but we don&#39;t know its</span>
<span class="line-modified">2652 // type. Either we get a buffered inline type (and nothing needs to be done)</span>
<span class="line-modified">2653 // or one of the inlines being returned is the klass of the inline type</span>
<span class="line-modified">2654 // and we need to allocate an inline type instance of that type and</span>
2655 // initialize it with other values being returned. In that case, we
2656 // first try a fast path allocation and initialize the value with the
<span class="line-modified">2657 // inline klass&#39;s pack handler or we fall back to a runtime call.</span>
2658 void PhaseMacroExpand::expand_mh_intrinsic_return(CallStaticJavaNode* call) {
2659   assert(call-&gt;method()-&gt;is_method_handle_intrinsic(), &quot;must be a method handle intrinsic call&quot;);
2660   Node* ret = call-&gt;proj_out_or_null(TypeFunc::Parms);
2661   if (ret == NULL) {
2662     return;
2663   }
2664   const TypeFunc* tf = call-&gt;_tf;
<span class="line-modified">2665   const TypeTuple* domain = OptoRuntime::store_inline_type_fields_Type()-&gt;domain_cc();</span>
2666   const TypeFunc* new_tf = TypeFunc::make(tf-&gt;domain_sig(), tf-&gt;domain_cc(), tf-&gt;range_sig(), domain);
2667   call-&gt;_tf = new_tf;
2668   // Make sure the change of type is applied before projections are processed by igvn
2669   _igvn.set_type(call, call-&gt;Value(&amp;_igvn));
2670   _igvn.set_type(ret, ret-&gt;Value(&amp;_igvn));
2671 
2672   // Before any new projection is added:
2673   CallProjections* projs = call-&gt;extract_projections(true, true);
2674 
2675   Node* ctl = new Node(1);
2676   Node* mem = new Node(1);
2677   Node* io = new Node(1);
2678   Node* ex_ctl = new Node(1);
2679   Node* ex_mem = new Node(1);
2680   Node* ex_io = new Node(1);
2681   Node* res = new Node(1);
2682 
2683   Node* cast = transform_later(new CastP2XNode(ctl, res));
2684   Node* mask = MakeConX(0x1);
2685   Node* masked = transform_later(new AndXNode(cast, mask));
</pre>
<hr />
<pre>
2710     Node* layout_val = make_load(NULL, mem, klass_node, in_bytes(Klass::layout_helper_offset()), TypeInt::INT, T_INT);
2711     Node* size_in_bytes = ConvI2X(layout_val);
2712     new_top = new AddPNode(top(), old_top, size_in_bytes);
2713     transform_later(new_top);
2714     Node* slowpath_cmp = new CmpPNode(new_top, end);
2715     transform_later(slowpath_cmp);
2716     slowpath_bol = new BoolNode(slowpath_cmp, BoolTest::ge);
2717     transform_later(slowpath_bol);
2718   } else {
2719     slowpath_bol = intcon(1);
2720     top_adr = top();
2721     old_top = top();
2722     new_top = top();
2723   }
2724   IfNode* slowpath_iff = new IfNode(allocation_ctl, slowpath_bol, PROB_UNLIKELY_MAG(4), COUNT_UNKNOWN);
2725   transform_later(slowpath_iff);
2726 
2727   Node* slowpath_true = new IfTrueNode(slowpath_iff);
2728   transform_later(slowpath_true);
2729 
<span class="line-modified">2730   CallStaticJavaNode* slow_call = new CallStaticJavaNode(OptoRuntime::store_inline_type_fields_Type(),</span>
2731                                                          StubRoutines::store_inline_type_fields_to_buf(),
<span class="line-modified">2732                                                          &quot;store_inline_type_fields&quot;,</span>
2733                                                          call-&gt;jvms()-&gt;bci(),
2734                                                          TypePtr::BOTTOM);
2735   slow_call-&gt;init_req(TypeFunc::Control, slowpath_true);
2736   slow_call-&gt;init_req(TypeFunc::Memory, mem);
2737   slow_call-&gt;init_req(TypeFunc::I_O, io);
2738   slow_call-&gt;init_req(TypeFunc::FramePtr, call-&gt;in(TypeFunc::FramePtr));
2739   slow_call-&gt;init_req(TypeFunc::ReturnAdr, call-&gt;in(TypeFunc::ReturnAdr));
2740   slow_call-&gt;init_req(TypeFunc::Parms, res);
2741 
2742   Node* slow_ctl = transform_later(new ProjNode(slow_call, TypeFunc::Control));
2743   Node* slow_mem = transform_later(new ProjNode(slow_call, TypeFunc::Memory));
2744   Node* slow_io = transform_later(new ProjNode(slow_call, TypeFunc::I_O));
2745   Node* slow_res = transform_later(new ProjNode(slow_call, TypeFunc::Parms));
2746   Node* slow_catc = transform_later(new CatchNode(slow_ctl, slow_io, 2));
2747   Node* slow_norm = transform_later(new CatchProjNode(slow_catc, CatchProjNode::fall_through_index, CatchProjNode::no_handler_bci));
2748   Node* slow_excp = transform_later(new CatchProjNode(slow_catc, CatchProjNode::catch_all_index,    CatchProjNode::no_handler_bci));
2749 
2750   Node* ex_r = new RegionNode(3);
2751   Node* ex_mem_phi = new PhiNode(ex_r, Type::MEMORY, TypePtr::BOTTOM);
2752   Node* ex_io_phi = new PhiNode(ex_r, Type::ABIO);
</pre>
<hr />
<pre>
2757   ex_mem_phi-&gt;init_req(2, ex_mem);
2758   ex_io_phi-&gt;init_req(2, ex_io);
2759 
2760   transform_later(ex_r);
2761   transform_later(ex_mem_phi);
2762   transform_later(ex_io_phi);
2763 
2764   Node* slowpath_false = new IfFalseNode(slowpath_iff);
2765   transform_later(slowpath_false);
2766   Node* rawmem = new StorePNode(slowpath_false, mem, top_adr, TypeRawPtr::BOTTOM, new_top, MemNode::unordered);
2767   transform_later(rawmem);
2768   Node* mark_node = makecon(TypeRawPtr::make((address)markWord::always_locked_prototype().value()));
2769   rawmem = make_store(slowpath_false, rawmem, old_top, oopDesc::mark_offset_in_bytes(), mark_node, T_ADDRESS);
2770   rawmem = make_store(slowpath_false, rawmem, old_top, oopDesc::klass_offset_in_bytes(), klass_node, T_METADATA);
2771   if (UseCompressedClassPointers) {
2772     rawmem = make_store(slowpath_false, rawmem, old_top, oopDesc::klass_gap_offset_in_bytes(), intcon(0), T_INT);
2773   }
2774   Node* fixed_block  = make_load(slowpath_false, rawmem, klass_node, in_bytes(InstanceKlass::adr_inlineklass_fixed_block_offset()), TypeRawPtr::BOTTOM, T_ADDRESS);
2775   Node* pack_handler = make_load(slowpath_false, rawmem, fixed_block, in_bytes(InlineKlass::pack_handler_offset()), TypeRawPtr::BOTTOM, T_ADDRESS);
2776 
<span class="line-modified">2777   CallLeafNoFPNode* handler_call = new CallLeafNoFPNode(OptoRuntime::pack_inline_type_Type(),</span>
2778                                                         NULL,
2779                                                         &quot;pack handler&quot;,
2780                                                         TypeRawPtr::BOTTOM);
2781   handler_call-&gt;init_req(TypeFunc::Control, slowpath_false);
2782   handler_call-&gt;init_req(TypeFunc::Memory, rawmem);
2783   handler_call-&gt;init_req(TypeFunc::I_O, top());
2784   handler_call-&gt;init_req(TypeFunc::FramePtr, call-&gt;in(TypeFunc::FramePtr));
2785   handler_call-&gt;init_req(TypeFunc::ReturnAdr, top());
2786   handler_call-&gt;init_req(TypeFunc::Parms, pack_handler);
2787   handler_call-&gt;init_req(TypeFunc::Parms+1, old_top);
2788 
2789   // We don&#39;t know how many values are returned. This assumes the
2790   // worst case, that all available registers are used.
2791   for (uint i = TypeFunc::Parms+1; i &lt; domain-&gt;cnt(); i++) {
2792     if (domain-&gt;field_at(i) == Type::HALF) {
2793       slow_call-&gt;init_req(i, top());
2794       handler_call-&gt;init_req(i+1, top());
2795       continue;
2796     }
2797     Node* proj = transform_later(new ProjNode(call, i));
</pre>
</td>
</tr>
</table>
<center><a href="machnode.hpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="macro.hpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>