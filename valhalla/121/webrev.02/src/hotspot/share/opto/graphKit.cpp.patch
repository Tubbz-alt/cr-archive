diff a/src/hotspot/share/opto/graphKit.cpp b/src/hotspot/share/opto/graphKit.cpp
--- a/src/hotspot/share/opto/graphKit.cpp
+++ b/src/hotspot/share/opto/graphKit.cpp
@@ -21,32 +21,33 @@
  * questions.
  *
  */
 
 #include "precompiled.hpp"
+#include "ci/ciFlatArrayKlass.hpp"
+#include "ci/ciInlineKlass.hpp"
 #include "ci/ciUtilities.hpp"
 #include "compiler/compileLog.hpp"
-#include "ci/ciValueKlass.hpp"
 #include "gc/shared/barrierSet.hpp"
 #include "gc/shared/c2/barrierSetC2.hpp"
 #include "interpreter/interpreter.hpp"
 #include "memory/resourceArea.hpp"
 #include "opto/addnode.hpp"
 #include "opto/castnode.hpp"
 #include "opto/convertnode.hpp"
 #include "opto/graphKit.hpp"
 #include "opto/idealKit.hpp"
+#include "opto/inlinetypenode.hpp"
 #include "opto/intrinsicnode.hpp"
 #include "opto/locknode.hpp"
 #include "opto/machnode.hpp"
 #include "opto/narrowptrnode.hpp"
 #include "opto/opaquenode.hpp"
 #include "opto/parse.hpp"
 #include "opto/rootnode.hpp"
 #include "opto/runtime.hpp"
 #include "opto/subtypenode.hpp"
-#include "opto/valuetypenode.hpp"
 #include "runtime/deoptimization.hpp"
 #include "runtime/sharedRuntime.hpp"
 #include "utilities/bitMap.inline.hpp"
 #include "utilities/powerOfTwo.hpp"
 
@@ -1395,30 +1396,30 @@
   }
 
   return value;
 }
 
-Node* GraphKit::null2default(Node* value, ciValueKlass* vk) {
+Node* GraphKit::null2default(Node* value, ciInlineKlass* vk) {
   Node* null_ctl = top();
   value = null_check_oop(value, &null_ctl);
   if (!null_ctl->is_top()) {
     // Return default value if oop is null
     Node* region = new RegionNode(3);
     region->init_req(1, control());
     region->init_req(2, null_ctl);
     value = PhiNode::make(region, value, TypeInstPtr::make(TypePtr::BotPTR, vk));
-    value->set_req(2, ValueTypeNode::default_oop(gvn(), vk));
+    value->set_req(2, InlineTypeNode::default_oop(gvn(), vk));
     set_control(gvn().transform(region));
     value = gvn().transform(value);
   }
   return value;
 }
 
 //------------------------------cast_not_null----------------------------------
 // Cast obj to not-null on this path
 Node* GraphKit::cast_not_null(Node* obj, bool do_replace_in_map) {
-  if (obj->is_ValueType()) {
+  if (obj->is_InlineType()) {
     return obj;
   }
   const Type *t = _gvn.type(obj);
   const Type *t_not_null = t->join_speculative(TypePtr::NOTNULL);
   // Object is already not-null?
@@ -1426,13 +1427,13 @@
 
   Node *cast = new CastPPNode(obj,t_not_null);
   cast->init_req(0, control());
   cast = _gvn.transform( cast );
 
-  if (t->is_valuetypeptr() && t->value_klass()->is_scalarizable()) {
-    // Scalarize inline type know that we know it's non-null
-    cast = ValueTypeNode::make_from_oop(this, cast, t->value_klass())->buffer(this, false);
+  if (t->is_inlinetypeptr() && t->inline_klass()->is_scalarizable()) {
+    // Scalarize inline type now that we know it's non-null
+    cast = InlineTypeNode::make_from_oop(this, cast, t->inline_klass())->buffer(this, false);
   }
 
   // Scan for instances of 'obj' in the current JVM mapping.
   // These instances are known to be not-null after the test.
   if (do_replace_in_map)
@@ -1620,16 +1621,16 @@
   if (stopped()) {
     return top(); // Dead path ?
   }
 
   assert(val != NULL, "not dead path");
-  if (val->is_ValueType()) {
+  if (val->is_InlineType()) {
     // Store to non-flattened field. Buffer the inline type and make sure
     // the store is re-executed if the allocation triggers deoptimization.
     PreserveReexecuteState preexecs(this);
     jvms()->set_should_reexecute(true);
-    val = val->as_ValueType()->buffer(this, safe_for_replace);
+    val = val->as_InlineType()->buffer(this, safe_for_replace);
   }
 
   C2AccessValuePtr addr(adr, adr_type);
   C2AccessValue value(val, val_type);
   C2ParseAccess access(this, decorators | C2_WRITE_ACCESS, bt, obj, addr);
@@ -1757,12 +1758,12 @@
 //-------------------------array_element_address-------------------------
 Node* GraphKit::array_element_address(Node* ary, Node* idx, BasicType elembt,
                                       const TypeInt* sizetype, Node* ctrl) {
   uint shift  = exact_log2(type2aelembytes(elembt));
   ciKlass* arytype_klass = _gvn.type(ary)->is_aryptr()->klass();
-  if (arytype_klass != NULL && arytype_klass->is_value_array_klass()) {
-    ciValueArrayKlass* vak = arytype_klass->as_value_array_klass();
+  if (arytype_klass != NULL && arytype_klass->is_flat_array_klass()) {
+    ciFlatArrayKlass* vak = arytype_klass->as_flat_array_klass();
     shift = vak->log2_element_size();
   }
   uint header = arrayOopDesc::base_offset_in_bytes(elembt);
 
   // short-circuit a common case (saves lots of confusing waste motion)
@@ -1781,11 +1782,11 @@
 
 //-------------------------load_array_element-------------------------
 Node* GraphKit::load_array_element(Node* ctl, Node* ary, Node* idx, const TypeAryPtr* arytype) {
   const Type* elemtype = arytype->elem();
   BasicType elembt = elemtype->array_element_basic_type();
-  assert(elembt != T_INLINE_TYPE, "value types are not supported by this method");
+  assert(elembt != T_INLINE_TYPE, "inline types are not supported by this method");
   Node* adr = array_element_address(ary, idx, elembt, arytype->size());
   if (elembt == T_NARROWOOP) {
     elembt = T_OBJECT; // To satisfy switch in LoadNode::make()
   }
   Node* ld = make_load(ctl, adr, elemtype, elembt, arytype, MemNode::unordered);
@@ -1795,11 +1796,11 @@
 //-------------------------set_arguments_for_java_call-------------------------
 // Arguments (pre-popped from the stack) are taken from the JVMS.
 void GraphKit::set_arguments_for_java_call(CallJavaNode* call, bool is_late_inline) {
   PreserveReexecuteState preexecs(this);
   if (EnableValhalla) {
-    // Make sure the call is re-executed, if buffering of value type arguments triggers deoptimization
+    // Make sure the call is re-executed, if buffering of inline type arguments triggers deoptimization
     jvms()->set_should_reexecute(true);
     int arg_size = method()->get_declared_signature_at_bci(bci())->arg_size_for_bc(java_bc());
     inc_sp(arg_size);
   }
   // Add the call arguments
@@ -1807,24 +1808,24 @@
   ExtendedSignature sig_cc = ExtendedSignature(call->method()->get_sig_cc(), SigEntryFilter());
   uint nargs = domain->cnt();
   for (uint i = TypeFunc::Parms, idx = TypeFunc::Parms; i < nargs; i++) {
     Node* arg = argument(i-TypeFunc::Parms);
     const Type* t = domain->field_at(i);
-    if (call->method()->has_scalarized_args() && t->is_valuetypeptr() && !t->maybe_null()) {
-      // We don't pass value type arguments by reference but instead pass each field of the value type
-      ValueTypeNode* vt = arg->as_ValueType();
+    if (call->method()->has_scalarized_args() && t->is_inlinetypeptr() && !t->maybe_null()) {
+      // We don't pass inline type arguments by reference but instead pass each field of the inline type
+      InlineTypeNode* vt = arg->as_InlineType();
       vt->pass_fields(this, call, sig_cc, idx);
-      // If a value type argument is passed as fields, attach the Method* to the call site
+      // If an inline type argument is passed as fields, attach the Method* to the call site
       // to be able to access the extended signature later via attached_method_before_pc().
       // For example, see CompiledMethod::preserve_callee_argument_oops().
       call->set_override_symbolic_info(true);
       continue;
-    } else if (arg->is_ValueType()) {
-      // Pass value type argument via oop to callee
-      arg = arg->as_ValueType()->buffer(this);
+    } else if (arg->is_InlineType()) {
+      // Pass inline type argument via oop to callee
+      arg = arg->as_InlineType()->buffer(this);
       if (!is_late_inline) {
-        arg = arg->as_ValueTypePtr()->get_oop();
+        arg = arg->as_InlineTypePtr()->get_oop();
       }
     }
     call->init_req(idx++, arg);
     // Skip reserved arguments
     BasicType bt = t->basic_type();
@@ -1886,20 +1887,20 @@
 
   // Capture the return value, if any.
   Node* ret;
   if (call->method() == NULL || call->method()->return_type()->basic_type() == T_VOID) {
     ret = top();
-  } else if (call->tf()->returns_value_type_as_fields()) {
-    // Return of multiple values (value type fields): we create a
-    // ValueType node, each field is a projection from the call.
-    ciValueKlass* vk = call->method()->return_type()->as_value_klass();
+  } else if (call->tf()->returns_inline_type_as_fields()) {
+    // Return of multiple values (inline type fields): we create a
+    // InlineType node, each field is a projection from the call.
+    ciInlineKlass* vk = call->method()->return_type()->as_inline_klass();
     const Array<SigEntry>* sig_array = vk->extended_sig();
     GrowableArray<SigEntry> sig = GrowableArray<SigEntry>(sig_array->length());
     sig.appendAll(sig_array);
     ExtendedSignature sig_cc = ExtendedSignature(&sig, SigEntryFilter());
     uint base_input = TypeFunc::Parms + 1;
-    ret = ValueTypeNode::make_from_multi(this, call, sig_cc, vk, base_input, false);
+    ret = InlineTypeNode::make_from_multi(this, call, sig_cc, vk, base_input, false);
   } else {
     ret = _gvn.transform(new ProjNode(call, TypeFunc::Parms));
   }
 
   return ret;
@@ -2927,12 +2928,12 @@
   return gvn.transform(r_not_subtype);
 }
 
 Node* GraphKit::gen_subtype_check(Node* obj_or_subklass, Node* superklass) {
   const Type* sub_t = _gvn.type(obj_or_subklass);
-  if (sub_t->isa_valuetype()) {
-    obj_or_subklass = makecon(TypeKlassPtr::make(sub_t->value_klass()));
+  if (sub_t->isa_inlinetype()) {
+    obj_or_subklass = makecon(TypeKlassPtr::make(sub_t->inline_klass()));
   }
   if (ExpandSubTypeCheckAtParseTime) {
     MergeMemNode* mem = merged_memory();
     Node* ctrl = control();
     Node* subklass = obj_or_subklass;
@@ -2963,13 +2964,13 @@
 
   // Subsume downstream occurrences of receiver with a cast to
   // recv_xtype, since now we know what the type will be.
   Node* cast = new CheckCastPPNode(control(), receiver, recv_xtype);
   Node* res = _gvn.transform(cast);
-  if (recv_xtype->is_valuetypeptr() && recv_xtype->value_klass()->is_scalarizable()) {
+  if (recv_xtype->is_inlinetypeptr() && recv_xtype->inline_klass()->is_scalarizable()) {
     assert(!gvn().type(res)->maybe_null(), "receiver should never be null");
-    res = ValueTypeNode::make_from_oop(this, res, recv_xtype->value_klass());
+    res = InlineTypeNode::make_from_oop(this, res, recv_xtype->inline_klass());
   }
 
   (*casted_receiver) = res;
   // (User must make the replace_in_map call.)
 
@@ -3224,11 +3225,11 @@
     data = method()->method_data()->bci_to_data(bci());
   }
   bool speculative_not_null = false;
   bool never_see_null = (ProfileDynamicTypes  // aggressive use of profile
                          && seems_never_null(obj, data, speculative_not_null));
-  bool is_value = obj->is_ValueType();
+  bool is_value = obj->is_InlineType();
 
   // Null check; get casted pointer; set region slot 3
   Node* null_ctl = top();
   Node* not_null_obj = is_value ? obj : null_check_oop(obj, &null_ctl, never_see_null, safe_for_replace, speculative_not_null);
 
@@ -3270,13 +3271,13 @@
           set_control(null_ctl);    // Null is the only remaining possibility.
           return intcon(0);
         }
         if (cast_obj != NULL &&
             // A value that's sometimes null is not something we can optimize well
-            !(cast_obj->is_ValueType() && null_ctl != top())) {
+            !(cast_obj->is_InlineType() && null_ctl != top())) {
           not_null_obj = cast_obj;
-          is_value = not_null_obj->is_ValueType();
+          is_value = not_null_obj->is_InlineType();
         }
       }
     }
   }
 
@@ -3317,23 +3318,23 @@
   kill_dead_locals();           // Benefit all the uncommon traps
   const TypeKlassPtr* tk = _gvn.type(superklass)->is_klassptr();
   const TypeOopPtr* toop = TypeOopPtr::make_from_klass(tk->klass());
 
   // Check if inline types are involved
-  bool from_inline = obj->is_ValueType();
-  bool to_inline = tk->klass()->is_valuetype();
+  bool from_inline = obj->is_InlineType();
+  bool to_inline = tk->klass()->is_inlinetype();
 
   // Fast cutout:  Check the case that the cast is vacuously true.
   // This detects the common cases where the test will short-circuit
   // away completely.  We do this before we perform the null check,
   // because if the test is going to turn into zero code, we don't
   // want a residual null check left around.  (Causes a slowdown,
   // for example, in some objArray manipulations, such as a[i]=a[j].)
   if (tk->singleton()) {
     ciKlass* klass = NULL;
     if (from_inline) {
-      klass = _gvn.type(obj)->value_klass();
+      klass = _gvn.type(obj)->inline_klass();
     } else {
       const TypeOopPtr* objtp = _gvn.type(obj)->isa_oopptr();
       if (objtp != NULL) {
         klass = objtp->klass();
       }
@@ -3346,22 +3347,22 @@
         // to the type system as a speculative type.
         if (!from_inline) {
           obj = record_profiled_receiver_for_speculation(obj);
           if (to_inline) {
             obj = null_check(obj);
-            if (toop->value_klass()->is_scalarizable()) {
-              obj = ValueTypeNode::make_from_oop(this, obj, toop->value_klass());
+            if (toop->inline_klass()->is_scalarizable()) {
+              obj = InlineTypeNode::make_from_oop(this, obj, toop->inline_klass());
             }
           }
         }
         return obj;
       case Compile::SSC_always_false:
         if (from_inline || to_inline) {
           if (!from_inline) {
             null_check(obj);
           }
-          // Value type is never null. Always throw an exception.
+          // Inline type is never null. Always throw an exception.
           builtin_throw(Deoptimization::Reason_class_check, makecon(TypeKlassPtr::make(klass)));
           return top();
         } else {
           // It needs a null check because a null will *pass* the cast check.
           return null_assert(obj);
@@ -3432,11 +3433,11 @@
     // We may not have profiling here or it may not help us. If we have
     // a speculative type use it to perform an exact cast.
     ciKlass* spec_obj_type = obj_type->speculative_type();
     if (spec_obj_type != NULL || data != NULL) {
       cast_obj = maybe_cast_profiled_receiver(not_null_obj, tk->klass(), spec_obj_type, safe_for_replace);
-      if (cast_obj != NULL && cast_obj->is_ValueType()) {
+      if (cast_obj != NULL && cast_obj->is_InlineType()) {
         if (null_ctl != top()) {
           cast_obj = NULL; // A value that's sometimes null is not something we can optimize well
         } else {
           return cast_obj;
         }
@@ -3461,11 +3462,11 @@
       if (not_subtype_ctrl != top()) { // If failure is possible
         PreserveJVMState pjvms(this);
         set_control(not_subtype_ctrl);
         Node* obj_klass = NULL;
         if (from_inline) {
-          obj_klass = makecon(TypeKlassPtr::make(_gvn.type(not_null_obj)->value_klass()));
+          obj_klass = makecon(TypeKlassPtr::make(_gvn.type(not_null_obj)->inline_klass()));
         } else {
           obj_klass = load_object_klass(not_null_obj);
         }
         builtin_throw(Deoptimization::Reason_class_check, obj_klass);
       }
@@ -3492,12 +3493,12 @@
 
   // Return final merged results
   set_control( _gvn.transform(region) );
   record_for_igvn(region);
 
-  bool not_inline = !toop->can_be_value_type();
-  bool not_flattened = !UseFlatArray || not_inline || (toop->is_valuetypeptr() && !toop->value_klass()->flatten_array());
+  bool not_inline = !toop->can_be_inline_type();
+  bool not_flattened = !UseFlatArray || not_inline || (toop->is_inlinetypeptr() && !toop->inline_klass()->flatten_array());
   if (EnableValhalla && not_flattened) {
     // Check if obj has been loaded from an array
     obj = obj->isa_DecodeN() ? obj->in(1) : obj;
     Node* array = NULL;
     if (obj->isa_Load()) {
@@ -3531,20 +3532,20 @@
     }
   }
 
   if (!from_inline) {
     res = record_profiled_receiver_for_speculation(res);
-    if (to_inline && toop->value_klass()->is_scalarizable()) {
+    if (to_inline && toop->inline_klass()->is_scalarizable()) {
       assert(!gvn().type(res)->maybe_null(), "Inline types are null-free");
-      res = ValueTypeNode::make_from_oop(this, res, toop->value_klass());
+      res = InlineTypeNode::make_from_oop(this, res, toop->inline_klass());
     }
   }
   return res;
 }
 
-// Check if 'obj' is a value type by checking if it has the always_locked markWord pattern set.
-Node* GraphKit::is_value_type(Node* obj) {
+// Check if 'obj' is an inline type by checking if it has the always_locked markWord pattern set.
+Node* GraphKit::is_inline_type(Node* obj) {
   Node* mark_addr = basic_plus_adr(obj, oopDesc::mark_offset_in_bytes());
   Node* mark = make_load(NULL, mark_addr, TypeX_X, TypeX_X->basic_type(), MemNode::unordered);
   Node* mask = _gvn.MakeConX(markWord::always_locked_pattern);
   Node* andx = _gvn.transform(new AndXNode(mark, mask));
   Node* cmp = _gvn.transform(new CmpXNode(andx, mask));
@@ -3568,14 +3569,14 @@
   null_free = _gvn.transform(new AndINode(null_free, intcon(Klass::_lh_null_free_mask)));
   Node* cmp = _gvn.transform(new CmpINode(null_free, intcon(0)));
   return _gvn.transform(new BoolNode(cmp, BoolTest::eq));
 }
 
-// Deoptimize if 'ary' is a null-free value type array and 'val' is null
-Node* GraphKit::gen_value_array_null_guard(Node* ary, Node* val, int nargs, bool safe_for_replace) {
+// Deoptimize if 'ary' is a null-free inline type array and 'val' is null
+Node* GraphKit::gen_inline_array_null_guard(Node* ary, Node* val, int nargs, bool safe_for_replace) {
   const Type* val_t = _gvn.type(val);
-  if (val->is_ValueType() || !TypePtr::NULL_PTR->higher_equal(val_t)) {
+  if (val->is_InlineType() || !TypePtr::NULL_PTR->higher_equal(val_t)) {
     return ary; // Never null
   }
   RegionNode* region = new RegionNode(3);
   Node* null_ctl = top();
   null_check_oop(val, &null_ctl);
@@ -3755,11 +3756,11 @@
     return;
   if (stopped()) {               // Dead monitor?
     map()->pop_monitor();        // Kill monitor from debug info
     return;
   }
-  assert(!obj->is_ValueTypeBase(), "should not unlock on value type");
+  assert(!obj->is_InlineTypeBase(), "should not unlock on inline type");
 
   // Memory barrier to avoid floating things down past the locked region
   insert_mem_bar(Op_MemBarReleaseLock);
 
   const TypeFunc *tf = OptoRuntime::complete_monitor_exit_Type();
@@ -3801,11 +3802,11 @@
     assert(klass != NULL, "klass should not be NULL");
     bool    xklass = inst_klass->klass_is_exact();
     bool can_be_flattened = false;
     if (UseFlatArray && klass->is_obj_array_klass()) {
       ciKlass* elem = klass->as_obj_array_klass()->element_klass();
-      can_be_flattened = elem->can_be_value_klass() && (!elem->is_valuetype() || elem->as_value_klass()->flatten_array());
+      can_be_flattened = elem->can_be_inline_klass() && (!elem->is_inlinetype() || elem->as_inline_klass()->flatten_array());
     }
     if (xklass || (klass->is_array_klass() && !can_be_flattened)) {
       jint lhelper = klass->layout_helper();
       if (lhelper != Klass::_lh_neutral_value) {
         constant_value = lhelper;
@@ -3874,30 +3875,30 @@
     // to one of those has correct memory state
     set_memory(minit_out, C->get_alias_index(oop_type->add_offset(oopDesc::mark_offset_in_bytes())));
     set_memory(minit_out, C->get_alias_index(oop_type->add_offset(oopDesc::klass_offset_in_bytes())));
     if (oop_type->isa_aryptr()) {
       const TypeAryPtr* arytype = oop_type->is_aryptr();
-      if (arytype->klass()->is_value_array_klass()) {
+      if (arytype->klass()->is_flat_array_klass()) {
         // Initially all flattened array accesses share a single slice
         // but that changes after parsing. Prepare the memory graph so
         // it can optimize flattened array accesses properly once they
         // don't share a single slice.
         assert(C->flattened_accesses_share_alias(), "should be set at parse time");
         C->set_flattened_accesses_share_alias(false);
-        ciValueArrayKlass* vak = arytype->klass()->as_value_array_klass();
-        ciValueKlass* vk = vak->element_klass()->as_value_klass();
+        ciFlatArrayKlass* vak = arytype->klass()->as_flat_array_klass();
+        ciInlineKlass* vk = vak->element_klass()->as_inline_klass();
         for (int i = 0, len = vk->nof_nonstatic_fields(); i < len; i++) {
           ciField* field = vk->nonstatic_field_at(i);
           if (field->offset() >= TrackedInitializationLimit * HeapWordSize)
             continue;  // do not bother to track really large numbers of fields
           int off_in_vt = field->offset() - vk->first_field_offset();
           const TypePtr* adr_type = arytype->with_field_offset(off_in_vt)->add_offset(Type::OffsetBot);
           int fieldidx = C->get_alias_index(adr_type, true);
           hook_memory_on_init(*this, fieldidx, minit_in, minit_out);
         }
         C->set_flattened_accesses_share_alias(true);
-        hook_memory_on_init(*this, C->get_alias_index(TypeAryPtr::VALUES), minit_in, minit_out);
+        hook_memory_on_init(*this, C->get_alias_index(TypeAryPtr::INLINES), minit_in, minit_out);
       } else {
         const TypePtr* telemref = oop_type->add_offset(Type::OffsetBot);
         int            elemidx  = C->get_alias_index(telemref);
         hook_memory_on_init(*this, elemidx, minit_in, minit_out);
       }
@@ -3952,11 +3953,11 @@
 //  - deoptimize_on_exception controls how Java exceptions are handled (rethrow vs deoptimize)
 Node* GraphKit::new_instance(Node* klass_node,
                              Node* extra_slow_test,
                              Node* *return_size_val,
                              bool deoptimize_on_exception,
-                             ValueTypeBaseNode* value_node) {
+                             InlineTypeBaseNode* inline_type_node) {
   // Compute size in doublewords
   // The size is always an integral number of doublewords, represented
   // as a positive bytewise size stored in the klass's layout_helper.
   // The layout_helper also encodes (in a low bit) the need for a slow path.
   jint  layout_con = Klass::_lh_neutral_value;
@@ -4017,11 +4018,11 @@
   set_all_memory(mem); // Create new memory state
 
   AllocateNode* alloc = new AllocateNode(C, AllocateNode::alloc_type(Type::TOP),
                                          control(), mem, i_o(),
                                          size, klass_node,
-                                         initial_slow_test, value_node);
+                                         initial_slow_test, inline_type_node);
 
   return set_output_for_allocation(alloc, oop_type, deoptimize_on_exception);
 }
 
 // With compressed oops, the 64 bit init value for non flattened value
@@ -4089,14 +4090,14 @@
   int   header_size_min  = arrayOopDesc::base_offset_in_bytes(T_BYTE);
   // (T_BYTE has the weakest alignment and size restrictions...)
   if (layout_is_con) {
     int       hsize  = Klass::layout_helper_header_size(layout_con);
     int       eshift = Klass::layout_helper_log2_element_size(layout_con);
-    bool is_value_array = Klass::layout_helper_is_flatArray(layout_con);
+    bool is_flat_array = Klass::layout_helper_is_flatArray(layout_con);
     if ((round_mask & ~right_n_bits(eshift)) == 0)
       round_mask = 0;  // strength-reduce it if it goes away completely
-    assert(is_value_array || (hsize & right_n_bits(eshift)) == 0, "hsize is pre-rounded");
+    assert(is_flat_array || (hsize & right_n_bits(eshift)) == 0, "hsize is pre-rounded");
     assert(header_size_min <= hsize, "generic minimum is smallest");
     header_size_min = hsize;
     header_size = intcon(hsize + round_mask);
   } else {
     Node* hss   = intcon(Klass::_lh_header_size_shift);
@@ -4189,34 +4190,34 @@
 
   const TypeKlassPtr* ary_klass = _gvn.type(klass_node)->isa_klassptr();
   const TypeOopPtr* ary_type = ary_klass->as_instance_type();
   const TypeAryPtr* ary_ptr = ary_type->isa_aryptr();
 
-  // Value type array variants:
+  // Inline type array variants:
   // - null-ok:              MyValue.ref[] (ciObjArrayKlass "[LMyValue$ref")
   // - null-free:            MyValue.val[] (ciObjArrayKlass "[QMyValue$val")
-  // - null-free, flattened: MyValue.val[] (ciValueArrayKlass "[QMyValue$val")
-  // Check if array is a null-free, non-flattened value type array
-  // that needs to be initialized with the default value type.
+  // - null-free, flattened: MyValue.val[] (ciFlatArrayKlass "[QMyValue$val")
+  // Check if array is a null-free, non-flattened inline type array
+  // that needs to be initialized with the default inline type.
   Node* default_value = NULL;
   Node* raw_default_value = NULL;
   if (ary_ptr != NULL && ary_ptr->klass_is_exact()) {
     // Array type is known
     ciKlass* elem_klass = ary_ptr->klass()->as_array_klass()->element_klass();
-    if (elem_klass != NULL && elem_klass->is_valuetype()) {
-      ciValueKlass* vk = elem_klass->as_value_klass();
+    if (elem_klass != NULL && elem_klass->is_inlinetype()) {
+      ciInlineKlass* vk = elem_klass->as_inline_klass();
       if (!vk->flatten_array()) {
-        default_value = ValueTypeNode::default_oop(gvn(), vk);
+        default_value = InlineTypeNode::default_oop(gvn(), vk);
         if (UseCompressedOops) {
           default_value = _gvn.transform(new EncodePNode(default_value, default_value->bottom_type()->make_narrowoop()));
           raw_default_value = raw_default_for_coops(default_value, *this);
         } else {
           raw_default_value = _gvn.transform(new CastP2XNode(control(), default_value));
         }
       }
     }
-  } else if (ary_klass->klass()->can_be_value_array_klass()) {
+  } else if (ary_klass->klass()->can_be_inline_array_klass()) {
     // Array type is not known, add runtime checks
     assert(!ary_klass->klass_is_exact(), "unexpected exact type");
     Node* r = new RegionNode(4);
     default_value = new PhiNode(r, TypeInstPtr::BOTTOM);
 
@@ -4241,11 +4242,11 @@
 
     // Not null-free, initialize with all zero
     r->init_req(2, _gvn.transform(new IfFalseNode(iff)));
     default_value->init_req(2, null());
 
-    // Null-free, non-flattened value array, initialize with the default value
+    // Null-free, non-flattened inline type array, initialize with the default value
     set_control(_gvn.transform(new IfTrueNode(iff)));
     Node* p = basic_plus_adr(klass_node, in_bytes(ArrayKlass::element_klass_offset()));
     Node* eklass = _gvn.transform(LoadKlassNode::make(_gvn, control(), immutable_memory(), p, TypeInstPtr::KLASS));
     Node* adr_fixed_block_addr = basic_plus_adr(eklass, in_bytes(InstanceKlass::adr_inlineklass_fixed_block_offset()));
     Node* adr_fixed_block = make_load(control(), adr_fixed_block_addr, TypeRawPtr::NOTNULL, T_ADDRESS, MemNode::unordered);
@@ -4586,15 +4587,15 @@
   }
   const Type* con_type = Type::make_constant_from_field(field, holder, field->layout_type(),
                                                         /*is_unsigned_load=*/false);
   if (con_type != NULL) {
     Node* con = makecon(con_type);
-    assert(!field->type()->is_valuetype() || (field->is_static() && !con_type->is_zero_type()), "sanity");
+    assert(!field->type()->is_inlinetype() || (field->is_static() && !con_type->is_zero_type()), "sanity");
     // Check type of constant which might be more precise
-    if (con_type->is_valuetypeptr() && con_type->value_klass()->is_scalarizable()) {
-      // Load value type from constant oop
-      con = ValueTypeNode::make_from_oop(this, con, con_type->value_klass());
+    if (con_type->is_inlinetypeptr() && con_type->inline_klass()->is_scalarizable()) {
+      // Load inline type from constant oop
+      con = InlineTypeNode::make_from_oop(this, con, con_type->inline_klass());
     }
     return con;
   }
   return NULL;
 }
