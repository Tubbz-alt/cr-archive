diff a/src/hotspot/share/opto/parse2.cpp b/src/hotspot/share/opto/parse2.cpp
--- a/src/hotspot/share/opto/parse2.cpp
+++ b/src/hotspot/share/opto/parse2.cpp
@@ -35,17 +35,17 @@
 #include "opto/castnode.hpp"
 #include "opto/convertnode.hpp"
 #include "opto/divnode.hpp"
 #include "opto/idealGraphPrinter.hpp"
 #include "opto/idealKit.hpp"
+#include "opto/inlinetypenode.hpp"
 #include "opto/matcher.hpp"
 #include "opto/memnode.hpp"
 #include "opto/mulnode.hpp"
 #include "opto/opaquenode.hpp"
 #include "opto/parse.hpp"
 #include "opto/runtime.hpp"
-#include "opto/valuetypenode.hpp"
 #include "runtime/deoptimization.hpp"
 #include "runtime/sharedRuntime.hpp"
 
 #ifndef PRODUCT
 extern int explicit_null_checks_inserted,
@@ -76,26 +76,26 @@
   if (stopped())  return;     // guaranteed null or range check
 
   Node* idx = pop();
   Node* ary = pop();
 
-  // Handle value type arrays
+  // Handle inline type arrays
   const TypeOopPtr* elemptr = elemtype->make_oopptr();
   const TypeAryPtr* ary_t = _gvn.type(ary)->is_aryptr();
-  if (elemtype->isa_valuetype() != NULL) {
+  if (elemtype->isa_inlinetype() != NULL) {
     C->set_flattened_accesses();
-    // Load from flattened value type array
-    Node* vt = ValueTypeNode::make_from_flattened(this, elemtype->value_klass(), ary, adr);
+    // Load from flattened inline type array
+    Node* vt = InlineTypeNode::make_from_flattened(this, elemtype->inline_klass(), ary, adr);
     push(vt);
     return;
-  } else if (elemptr != NULL && elemptr->is_valuetypeptr() && !elemptr->maybe_null()) {
+  } else if (elemptr != NULL && elemptr->is_inlinetypeptr() && !elemptr->maybe_null()) {
     // Load from non-flattened inline type array (elements can never be null)
     bt = T_INLINE_TYPE;
   } else if (!ary_t->is_not_flat()) {
     // Cannot statically determine if array is flattened, emit runtime check
-    assert(UseFlatArray && is_reference_type(bt) && elemptr->can_be_value_type() && !ary_t->klass_is_exact() && !ary_t->is_not_null_free() &&
-           (!elemptr->is_valuetypeptr() || elemptr->value_klass()->flatten_array()), "array can't be flattened");
+    assert(UseFlatArray && is_reference_type(bt) && elemptr->can_be_inline_type() && !ary_t->klass_is_exact() && !ary_t->is_not_null_free() &&
+           (!elemptr->is_inlinetypeptr() || elemptr->inline_klass()->flatten_array()), "array can't be flattened");
     IdealKit ideal(this);
     IdealVariable res(ideal);
     ideal.declarations_done();
     ideal.if_then(is_non_flattened_array(ary)); {
       // non-flattened
@@ -107,23 +107,23 @@
       ideal.sync_kit(this);
       ideal.set(res, ld);
     } ideal.else_(); {
       // flattened
       sync_kit(ideal);
-      if (elemptr->is_valuetypeptr()) {
+      if (elemptr->is_inlinetypeptr()) {
         // Element type is known, cast and load from flattened representation
-        ciValueKlass* vk = elemptr->value_klass();
+        ciInlineKlass* vk = elemptr->inline_klass();
         assert(vk->flatten_array() && elemptr->maybe_null(), "never/always flat - should be optimized");
         ciArrayKlass* array_klass = ciArrayKlass::make(vk);
         const TypeAryPtr* arytype = TypeOopPtr::make_from_klass(array_klass)->isa_aryptr();
         Node* cast = _gvn.transform(new CheckCastPPNode(control(), ary, arytype));
         Node* casted_adr = array_element_address(cast, idx, T_INLINE_TYPE, ary_t->size(), control());
         // Re-execute flattened array load if buffering triggers deoptimization
         PreserveReexecuteState preexecs(this);
         jvms()->set_should_reexecute(true);
         inc_sp(2);
-        Node* vt = ValueTypeNode::make_from_flattened(this, vk, cast, casted_adr)->buffer(this, false);
+        Node* vt = InlineTypeNode::make_from_flattened(this, vk, cast, casted_adr)->buffer(this, false);
         ideal.set(res, vt);
         ideal.sync_kit(this);
       } else {
         // Element type is unknown, emit runtime call
         Node* kls = load_object_klass(ary);
@@ -143,14 +143,14 @@
         alloc->initialization()->set_complete_with_arraycopy();
 
         // This membar keeps this access to an unknown flattened array
         // correctly ordered with other unknown and known flattened
         // array accesses.
-        insert_mem_bar_volatile(Op_MemBarCPUOrder, C->get_alias_index(TypeAryPtr::VALUES));
+        insert_mem_bar_volatile(Op_MemBarCPUOrder, C->get_alias_index(TypeAryPtr::INLINES));
 
         BarrierSetC2* bs = BarrierSet::barrier_set()->barrier_set_c2();
-        // Unknown value type might contain reference fields
+        // Unknown inline type might contain reference fields
         if (false && !bs->array_copy_requires_gc_barriers(false, T_OBJECT, false, BarrierSetC2::Parsing)) {
           // FIXME 8230656 also merge changes from 8238759 in
           int base_off = sizeof(instanceOopDesc);
           Node* dst_base = basic_plus_adr(alloc_obj, base_off);
           Node* countx = obj_size;
@@ -167,27 +167,26 @@
           Node* adr = basic_plus_adr(ary, base, scale);
 
           access_clone(adr, dst_base, countx, false);
         } else {
           ideal.sync_kit(this);
-          ideal.make_leaf_call(OptoRuntime::load_unknown_value_Type(),
-                               CAST_FROM_FN_PTR(address, OptoRuntime::load_unknown_value),
-                               "load_unknown_value",
+          ideal.make_leaf_call(OptoRuntime::load_unknown_inline_type(),
+                               CAST_FROM_FN_PTR(address, OptoRuntime::load_unknown_inline),
+                               "load_unknown_inline",
                                ary, idx, alloc_obj);
           sync_kit(ideal);
         }
 
-        // This makes sure no other thread sees a partially initialized buffered value
+        // This makes sure no other thread sees a partially initialized buffered inline type
         insert_mem_bar_volatile(Op_MemBarStoreStore, Compile::AliasIdxRaw, alloc->proj_out_or_null(AllocateNode::RawAddress));
 
         // Same as MemBarCPUOrder above: keep this unknown flattened
         // array access correctly ordered with other flattened array
         // access
-        insert_mem_bar_volatile(Op_MemBarCPUOrder, C->get_alias_index(TypeAryPtr::VALUES));
+        insert_mem_bar_volatile(Op_MemBarCPUOrder, C->get_alias_index(TypeAryPtr::INLINES));
 
-        // Prevent any use of the newly allocated value before it is
-        // fully initialized
+        // Prevent any use of the newly allocated inline type before it is fully initialized
         alloc_obj = new CastPPNode(alloc_obj, _gvn.type(alloc_obj), true);
         alloc_obj->set_req(0, control());
         alloc_obj = _gvn.transform(alloc_obj);
 
         const Type* unknown_value = elemptr->is_instptr()->cast_to_flat_array();
@@ -209,17 +208,17 @@
   }
   const TypeAryPtr* adr_type = TypeAryPtr::get_array_body_type(bt);
   Node* ld = access_load_at(ary, adr, adr_type, elemtype, bt,
                             IN_HEAP | IS_ARRAY | C2_CONTROL_DEPENDENT_LOAD);
   if (bt == T_INLINE_TYPE) {
-    // Loading a non-flattened value type from an array
-    assert(!gvn().type(ld)->maybe_null(), "value type array elements should never be null");
-    if (elemptr->value_klass()->is_scalarizable()) {
-      ld = ValueTypeNode::make_from_oop(this, ld, elemptr->value_klass());
+    // Loading a non-flattened inline type from an array
+    assert(!gvn().type(ld)->maybe_null(), "inline type array elements should never be null");
+    if (elemptr->inline_klass()->is_scalarizable()) {
+      ld = InlineTypeNode::make_from_oop(this, ld, elemptr->inline_klass());
     }
   }
-  if (!ld->is_ValueType()) {
+  if (!ld->is_InlineType()) {
     ld = record_profile_for_speculation_at_array_load(ld);
   }
 
   push_node(bt, ld);
 }
@@ -249,17 +248,17 @@
     const Type* tval = _gvn.type(cast_val);
     // We may have lost type information for 'val' here due to the casts
     // emitted by the array_store_check code (see JDK-6312651)
     // TODO Remove this code once JDK-6312651 is in.
     const Type* tval_init = _gvn.type(val);
-    bool not_inline = !tval->isa_valuetype() && (tval == TypePtr::NULL_PTR || !tval_init->is_oopptr()->can_be_value_type() || !tval->is_oopptr()->can_be_value_type());
-    bool not_flattened = !UseFlatArray || not_inline || ((tval_init->is_valuetypeptr() || tval_init->isa_valuetype()) && !tval_init->value_klass()->flatten_array());
+    bool not_inline = !tval->isa_inlinetype() && (tval == TypePtr::NULL_PTR || !tval_init->is_oopptr()->can_be_inline_type() || !tval->is_oopptr()->can_be_inline_type());
+    bool not_flattened = !UseFlatArray || not_inline || ((tval_init->is_inlinetypeptr() || tval_init->isa_inlinetype()) && !tval_init->inline_klass()->flatten_array());
 
     if (!ary_t->is_not_null_free() && not_inline && (!tval->maybe_null() || !tval_init->maybe_null())) {
       // Storing a non-inline type, mark array as not null-free (-> not flat).
       // This is only legal for non-null stores because the array_store_check always passes for null.
-      // Null stores are handled in GraphKit::gen_value_array_null_guard().
+      // Null stores are handled in GraphKit::gen_inline_array_null_guard().
       ary_t = ary_t->cast_to_not_null_free();
       Node* cast = _gvn.transform(new CheckCastPPNode(control(), ary, ary_t));
       replace_in_map(ary, cast);
       ary = cast;
     } else if (!ary_t->is_not_flat() && not_flattened) {
@@ -268,52 +267,52 @@
       Node* cast = _gvn.transform(new CheckCastPPNode(control(), ary, ary_t));
       replace_in_map(ary, cast);
       ary = cast;
     }
 
-    if (ary_t->elem()->isa_valuetype() != NULL) {
-      // Store to flattened value type array
+    if (ary_t->elem()->isa_inlinetype() != NULL) {
+      // Store to flattened inline type array
       C->set_flattened_accesses();
-      if (!cast_val->is_ValueType()) {
+      if (!cast_val->is_InlineType()) {
         inc_sp(3);
         cast_val = null_check(cast_val);
         if (stopped()) return;
         dec_sp(3);
-        cast_val = ValueTypeNode::make_from_oop(this, cast_val, ary_t->elem()->value_klass());
+        cast_val = InlineTypeNode::make_from_oop(this, cast_val, ary_t->elem()->inline_klass());
       }
       // Re-execute flattened array store if buffering triggers deoptimization
       PreserveReexecuteState preexecs(this);
       inc_sp(3);
       jvms()->set_should_reexecute(true);
-      cast_val->as_ValueType()->store_flattened(this, ary, adr, NULL, 0, MO_UNORDERED | IN_HEAP | IS_ARRAY);
+      cast_val->as_InlineType()->store_flattened(this, ary, adr, NULL, 0, MO_UNORDERED | IN_HEAP | IS_ARRAY);
       return;
-    } else if (elemtype->is_valuetypeptr() && !elemtype->maybe_null()) {
+    } else if (elemtype->is_inlinetypeptr() && !elemtype->maybe_null()) {
       // Store to non-flattened inline type array (elements can never be null)
-      if (!cast_val->is_ValueType() && tval->maybe_null()) {
+      if (!cast_val->is_InlineType() && tval->maybe_null()) {
         inc_sp(3);
         cast_val = null_check(cast_val);
         if (stopped()) return;
         dec_sp(3);
       }
     } else if (!ary_t->is_not_flat()) {
       // Array might be flattened, emit runtime checks
-      assert(UseFlatArray && !not_flattened && elemtype->is_oopptr()->can_be_value_type() &&
+      assert(UseFlatArray && !not_flattened && elemtype->is_oopptr()->can_be_inline_type() &&
              !ary_t->klass_is_exact() && !ary_t->is_not_null_free(), "array can't be flattened");
       IdealKit ideal(this);
       ideal.if_then(is_non_flattened_array(ary)); {
         // non-flattened
         assert(ideal.ctrl()->in(0)->as_If()->is_non_flattened_array_check(&_gvn), "Should be found");
         sync_kit(ideal);
-        gen_value_array_null_guard(ary, cast_val, 3);
+        gen_inline_array_null_guard(ary, cast_val, 3);
         inc_sp(3);
         access_store_at(ary, adr, adr_type, cast_val, elemtype, bt, MO_UNORDERED | IN_HEAP | IS_ARRAY, false);
         dec_sp(3);
         ideal.sync_kit(this);
       } ideal.else_(); {
         Node* val = cast_val;
         // flattened
-        if (!val->is_ValueType() && tval->maybe_null()) {
+        if (!val->is_InlineType() && tval->maybe_null()) {
           // Add null check
           sync_kit(ideal);
           Node* null_ctl = top();
           val = null_check_oop(val, &null_ctl);
           if (null_ctl != top()) {
@@ -323,68 +322,68 @@
             uncommon_trap(Deoptimization::Reason_null_check, Deoptimization::Action_none);
             dec_sp(3);
           }
           ideal.sync_kit(this);
         }
-        // Try to determine the value klass
-        ciValueKlass* vk = NULL;
-        if (tval->isa_valuetype() || tval->is_valuetypeptr()) {
-          vk = tval->value_klass();
-        } else if (tval_init->isa_valuetype() || tval_init->is_valuetypeptr()) {
-          vk = tval_init->value_klass();
-        } else if (elemtype->is_valuetypeptr()) {
-          vk = elemtype->value_klass();
+        // Try to determine the inline klass
+        ciInlineKlass* vk = NULL;
+        if (tval->isa_inlinetype() || tval->is_inlinetypeptr()) {
+          vk = tval->inline_klass();
+        } else if (tval_init->isa_inlinetype() || tval_init->is_inlinetypeptr()) {
+          vk = tval_init->inline_klass();
+        } else if (elemtype->is_inlinetypeptr()) {
+          vk = elemtype->inline_klass();
         }
         Node* casted_ary = ary;
         if (vk != NULL && !stopped()) {
           // Element type is known, cast and store to flattened representation
           sync_kit(ideal);
           assert(vk->flatten_array() && elemtype->maybe_null(), "never/always flat - should be optimized");
           ciArrayKlass* array_klass = ciArrayKlass::make(vk);
           const TypeAryPtr* arytype = TypeOopPtr::make_from_klass(array_klass)->isa_aryptr();
           casted_ary = _gvn.transform(new CheckCastPPNode(control(), casted_ary, arytype));
           Node* casted_adr = array_element_address(casted_ary, idx, T_OBJECT, arytype->size(), control());
-          if (!val->is_ValueType()) {
-            assert(!gvn().type(val)->maybe_null(), "value type array elements should never be null");
-            val = ValueTypeNode::make_from_oop(this, val, vk);
+          if (!val->is_InlineType()) {
+            assert(!gvn().type(val)->maybe_null(), "inline type array elements should never be null");
+            val = InlineTypeNode::make_from_oop(this, val, vk);
           }
           // Re-execute flattened array store if buffering triggers deoptimization
           PreserveReexecuteState preexecs(this);
           inc_sp(3);
           jvms()->set_should_reexecute(true);
-          val->as_ValueType()->store_flattened(this, casted_ary, casted_adr, NULL, 0, MO_UNORDERED | IN_HEAP | IS_ARRAY);
+          val->as_InlineType()->store_flattened(this, casted_ary, casted_adr, NULL, 0, MO_UNORDERED | IN_HEAP | IS_ARRAY);
           ideal.sync_kit(this);
         } else if (!ideal.ctrl()->is_top()) {
           // Element type is unknown, emit runtime call
           sync_kit(ideal);
 
           // This membar keeps this access to an unknown flattened
           // array correctly ordered with other unknown and known
           // flattened array accesses.
-          insert_mem_bar_volatile(Op_MemBarCPUOrder, C->get_alias_index(TypeAryPtr::VALUES));
+          insert_mem_bar_volatile(Op_MemBarCPUOrder, C->get_alias_index(TypeAryPtr::INLINES));
           ideal.sync_kit(this);
 
-          ideal.make_leaf_call(OptoRuntime::store_unknown_value_Type(),
-                               CAST_FROM_FN_PTR(address, OptoRuntime::store_unknown_value),
-                               "store_unknown_value",
+          ideal.make_leaf_call(OptoRuntime::store_unknown_inline_type(),
+                               CAST_FROM_FN_PTR(address, OptoRuntime::store_unknown_inline),
+                               "store_unknown_inline",
                                val, casted_ary, idx);
 
           sync_kit(ideal);
           // Same as MemBarCPUOrder above: keep this unknown
           // flattened array access correctly ordered with other
           // flattened array accesses.
-          insert_mem_bar_volatile(Op_MemBarCPUOrder, C->get_alias_index(TypeAryPtr::VALUES));
+          insert_mem_bar_volatile(Op_MemBarCPUOrder, C->get_alias_index(TypeAryPtr::INLINES));
           ideal.sync_kit(this);
         }
       }
       ideal.end_if();
       sync_kit(ideal);
       return;
     } else if (!ary_t->is_not_null_free()) {
       // Array is not flattened but may be null free
-      assert(elemtype->is_oopptr()->can_be_value_type() && !ary_t->klass_is_exact(), "array can't be null free");
-      ary = gen_value_array_null_guard(ary, cast_val, 3, true);
+      assert(elemtype->is_oopptr()->can_be_inline_type() && !ary_t->klass_is_exact(), "array can't be null free");
+      ary = gen_inline_array_null_guard(ary, cast_val, 3, true);
     }
   }
   inc_sp(3);
   access_store_at(ary, adr, adr_type, val, elemtype, bt, MO_UNORDERED | IN_HEAP | IS_ARRAY);
   dec_sp(3);
@@ -485,19 +484,19 @@
     }
   }
   // Check for always knowing you are throwing a range-check exception
   if (stopped())  return top();
 
-  // This could be an access to a value array. We can't tell if it's
+  // This could be an access to an inline type array. We can't tell if it's
   // flat or not. Speculating it's not leads to a much simpler graph
   // shape. Check profiling.
   // For aastore, by the time we're here, the array store check should
   // have already taken advantage of profiling to cast the array to an
   // exact type reported by profiling
   const TypeOopPtr* elemptr = elemtype->make_oopptr();
-  if (elemtype->isa_valuetype() == NULL &&
-      (elemptr == NULL || !elemptr->is_valuetypeptr() || elemptr->maybe_null()) &&
+  if (elemtype->isa_inlinetype() == NULL &&
+      (elemptr == NULL || !elemptr->is_inlinetypeptr() || elemptr->maybe_null()) &&
       !arytype->is_not_flat()) {
     assert(is_reference_type(type), "Only references");
     // First check the speculative type
     Deoptimization::DeoptReason reason = Deoptimization::Reason_speculate_class_check;
     ciKlass* array_type = arytype->speculative_type();
@@ -545,12 +544,12 @@
   // flat so check this one first. Speculating on a non null free
   // array doesn't help aaload but could be profitable for a
   // subsequent aastore.
   elemptr = elemtype->make_oopptr();
   if (!arytype->is_not_null_free() &&
-      elemtype->isa_valuetype() == NULL &&
-      (elemptr == NULL || !elemptr->is_valuetypeptr()) &&
+      elemtype->isa_inlinetype() == NULL &&
+      (elemptr == NULL || !elemptr->is_inlinetypeptr()) &&
       UseArrayLoadStoreProfile) {
     assert(is_reference_type(type), "");
     bool null_free_array = true;
     Deoptimization::DeoptReason reason = Deoptimization::Reason_none;
     if (arytype->speculative() != NULL &&
@@ -576,11 +575,11 @@
       ary = better_ary;
       arytype = _gvn.type(ary)->is_aryptr();
     }
   }
 
-  if (!arytype->is_not_flat() && elemtype->isa_valuetype() == NULL) {
+  if (!arytype->is_not_flat() && elemtype->isa_inlinetype() == NULL) {
     assert(is_reference_type(type), "");
     bool flat_array = true;
     Deoptimization::DeoptReason reason = Deoptimization::Reason_none;
     if (arytype->speculative() != NULL &&
         arytype->speculative()->is_aryptr()->is_not_flat() &&
@@ -2057,32 +2056,32 @@
     cmp = optimize_cmp_with_klass(cmp);
     do_if(btest, cmp);
     return;
   }
 
-  // Allocate value type operands and re-execute on deoptimization
-  if (a->is_ValueType()) {
+  // Allocate inline type operands and re-execute on deoptimization
+  if (a->is_InlineType()) {
     PreserveReexecuteState preexecs(this);
     inc_sp(2);
     jvms()->set_should_reexecute(true);
-    a = a->as_ValueType()->buffer(this)->get_oop();
+    a = a->as_InlineType()->buffer(this)->get_oop();
   }
-  if (b->is_ValueType()) {
+  if (b->is_InlineType()) {
     PreserveReexecuteState preexecs(this);
     inc_sp(2);
     jvms()->set_should_reexecute(true);
-    b = b->as_ValueType()->buffer(this)->get_oop();
+    b = b->as_InlineType()->buffer(this)->get_oop();
   }
 
   // First, do a normal pointer comparison
   const TypeOopPtr* ta = _gvn.type(a)->isa_oopptr();
   const TypeOopPtr* tb = _gvn.type(b)->isa_oopptr();
   Node* cmp = CmpP(a, b);
   cmp = optimize_cmp_with_klass(cmp);
-  if (ta == NULL || !ta->can_be_value_type() ||
-      tb == NULL || !tb->can_be_value_type()) {
-    // This is sufficient, if one of the operands can't be a value type
+  if (ta == NULL || !ta->can_be_inline_type() ||
+      tb == NULL || !tb->can_be_inline_type()) {
+    // This is sufficient, if one of the operands can't be an inline type
     do_if(btest, cmp);
     return;
   }
   Node* eq_region = NULL;
   if (btest == BoolTest::eq) {
@@ -2129,18 +2128,18 @@
       set_control(_gvn.transform(eq_region));
     }
     return;
   }
 
-  // First operand is non-null, check if it is a value type
-  Node* is_value = is_value_type(not_null_a);
+  // First operand is non-null, check if it is an inline type
+  Node* is_value = is_inline_type(not_null_a);
   IfNode* is_value_iff = create_and_map_if(control(), is_value, PROB_FAIR, COUNT_UNKNOWN);
   Node* not_value = _gvn.transform(new IfFalseNode(is_value_iff));
   ne_region->init_req(2, not_value);
   set_control(_gvn.transform(new IfTrueNode(is_value_iff)));
 
-  // The first operand is a value type, check if the second operand is non-null
+  // The first operand is an inline type, check if the second operand is non-null
   inc_sp(2);
   null_ctl = top();
   Node* not_null_b = null_check_oop(b, &null_ctl, !too_many_traps(Deoptimization::Reason_null_check), false, false);
   dec_sp(2);
   ne_region->init_req(3, null_ctl);
@@ -2485,13 +2484,13 @@
       if (obj_type->speculative_type_not_null() != NULL) {
         ciKlass* k = obj_type->speculative_type();
         inc_sp(2);
         obj = maybe_cast_profiled_obj(obj, k);
         dec_sp(2);
-        if (obj->is_ValueType()) {
-          assert(obj->as_ValueType()->is_allocated(&_gvn), "must be allocated");
-          obj = obj->as_ValueType()->get_oop();
+        if (obj->is_InlineType()) {
+          assert(obj->as_InlineType()->is_allocated(&_gvn), "must be allocated");
+          obj = obj->as_InlineType()->get_oop();
         }
         // Make the CmpP use the casted obj
         addp = basic_plus_adr(obj, addp->in(AddPNode::Offset));
         load_klass = load_klass->clone();
         load_klass->set_req(2, addp);
@@ -3336,11 +3335,11 @@
   handle_if_null:
     // If this is a backwards branch in the bytecodes, add Safepoint
     maybe_add_safepoint(iter().get_dest());
     a = null();
     b = pop();
-    if (b->is_ValueType()) {
+    if (b->is_InlineType()) {
       // Return constant false because 'b' is always non-null
       c = _gvn.makecon(TypeInt::CC_GT);
     } else {
       if (!_gvn.type(b)->speculative_maybe_null() &&
           !too_many_traps(Deoptimization::Reason_speculate_null_check)) {
