<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/share/opto/library_call.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="graphKit.hpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="locknode.cpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/share/opto/library_call.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
  25 #include &quot;precompiled.hpp&quot;
  26 #include &quot;asm/macroAssembler.hpp&quot;
  27 #include &quot;ci/ciUtilities.inline.hpp&quot;
  28 #include &quot;classfile/systemDictionary.hpp&quot;
  29 #include &quot;classfile/vmSymbols.hpp&quot;
  30 #include &quot;compiler/compileBroker.hpp&quot;
  31 #include &quot;compiler/compileLog.hpp&quot;
  32 #include &quot;gc/shared/barrierSet.hpp&quot;
  33 #include &quot;jfr/support/jfrIntrinsics.hpp&quot;
  34 #include &quot;memory/resourceArea.hpp&quot;
  35 #include &quot;oops/klass.inline.hpp&quot;
  36 #include &quot;oops/objArrayKlass.hpp&quot;
  37 #include &quot;opto/addnode.hpp&quot;
  38 #include &quot;opto/arraycopynode.hpp&quot;
  39 #include &quot;opto/c2compiler.hpp&quot;
  40 #include &quot;opto/callGenerator.hpp&quot;
  41 #include &quot;opto/castnode.hpp&quot;
  42 #include &quot;opto/cfgnode.hpp&quot;
  43 #include &quot;opto/convertnode.hpp&quot;
  44 #include &quot;opto/countbitsnode.hpp&quot;

  45 #include &quot;opto/intrinsicnode.hpp&quot;
  46 #include &quot;opto/idealKit.hpp&quot;
  47 #include &quot;opto/mathexactnode.hpp&quot;
  48 #include &quot;opto/movenode.hpp&quot;
  49 #include &quot;opto/mulnode.hpp&quot;
  50 #include &quot;opto/narrowptrnode.hpp&quot;
  51 #include &quot;opto/opaquenode.hpp&quot;
  52 #include &quot;opto/parse.hpp&quot;
  53 #include &quot;opto/runtime.hpp&quot;
  54 #include &quot;opto/rootnode.hpp&quot;
  55 #include &quot;opto/subnode.hpp&quot;
<span class="line-removed">  56 #include &quot;opto/valuetypenode.hpp&quot;</span>
  57 #include &quot;prims/nativeLookup.hpp&quot;
  58 #include &quot;prims/unsafe.hpp&quot;
  59 #include &quot;runtime/objectMonitor.hpp&quot;
  60 #include &quot;runtime/sharedRuntime.hpp&quot;
  61 #include &quot;utilities/macros.hpp&quot;
  62 #include &quot;utilities/powerOfTwo.hpp&quot;
  63 
  64 class LibraryIntrinsic : public InlineCallGenerator {
  65   // Extend the set of intrinsics known to the runtime:
  66  public:
  67  private:
  68   bool             _is_virtual;
  69   bool             _does_virtual_dispatch;
  70   int8_t           _predicates_count;  // Intrinsic is predicated by several conditions
  71   int8_t           _last_predicate; // Last generated predicate
  72   vmIntrinsics::ID _intrinsic_id;
  73 
  74  public:
  75   LibraryIntrinsic(ciMethod* m, bool is_virtual, int predicates_count, bool does_virtual_dispatch, vmIntrinsics::ID id)
  76     : InlineCallGenerator(m),
</pre>
<hr />
<pre>
 121       _reexecute_sp = sp() + nargs;  // &quot;push&quot; arguments back on stack
 122     }
 123   }
 124 
 125   virtual LibraryCallKit* is_LibraryCallKit() const { return (LibraryCallKit*)this; }
 126 
 127   ciMethod*         caller()    const    { return jvms()-&gt;method(); }
 128   int               bci()       const    { return jvms()-&gt;bci(); }
 129   LibraryIntrinsic* intrinsic() const    { return _intrinsic; }
 130   vmIntrinsics::ID  intrinsic_id() const { return _intrinsic-&gt;intrinsic_id(); }
 131   ciMethod*         callee()    const    { return _intrinsic-&gt;method(); }
 132 
 133   bool  try_to_inline(int predicate);
 134   Node* try_to_predicate(int predicate);
 135 
 136   void push_result() {
 137     // Push the result onto the stack.
 138     Node* res = result();
 139     if (!stopped() &amp;&amp; res != NULL) {
 140       BasicType bt = res-&gt;bottom_type()-&gt;basic_type();
<span class="line-modified"> 141       if (C-&gt;inlining_incrementally() &amp;&amp; res-&gt;is_ValueType()) {</span>
 142         // The caller expects an oop when incrementally inlining an intrinsic that returns an
 143         // inline type. Make sure the call is re-executed if the allocation triggers a deoptimization.
 144         PreserveReexecuteState preexecs(this);
 145         jvms()-&gt;set_should_reexecute(true);
<span class="line-modified"> 146         res = res-&gt;as_ValueType()-&gt;buffer(this);</span>
 147       }
 148       push_node(bt, res);
 149     }
 150   }
 151 
 152  private:
 153   void fatal_unexpected_iid(vmIntrinsics::ID iid) {
 154     fatal(&quot;unexpected intrinsic %d: %s&quot;, iid, vmIntrinsics::name_at(iid));
 155   }
 156 
 157   void  set_result(Node* n) { assert(_result == NULL, &quot;only set once&quot;); _result = n; }
 158   void  set_result(RegionNode* region, PhiNode* value);
 159   Node*     result() { return _result; }
 160 
 161   virtual int reexecute_sp() { return _reexecute_sp; }
 162 
 163   // Helper functions to inline natives
 164   Node* generate_guard(Node* test, RegionNode* region, float true_prob);
 165   Node* generate_slow_guard(Node* test, RegionNode* region);
 166   Node* generate_fair_guard(Node* test, RegionNode* region);
</pre>
<hr />
<pre>
 185   }
 186   Node* load_array_klass_from_mirror(Node* mirror, bool never_see_null,
 187                                      RegionNode* region, int null_path) {
 188     int offset = java_lang_Class::array_klass_offset();
 189     return load_klass_from_mirror_common(mirror, never_see_null,
 190                                          region, null_path,
 191                                          offset);
 192   }
 193   Node* generate_access_flags_guard(Node* kls,
 194                                     int modifier_mask, int modifier_bits,
 195                                     RegionNode* region);
 196   Node* generate_interface_guard(Node* kls, RegionNode* region);
 197   Node* generate_value_guard(Node* kls, RegionNode* region);
 198 
 199   enum ArrayKind {
 200     AnyArray,
 201     NonArray,
 202     ObjectArray,
 203     NonObjectArray,
 204     TypeArray,
<span class="line-modified"> 205     ValueArray</span>
 206   };
 207 
 208   Node* generate_hidden_class_guard(Node* kls, RegionNode* region);
 209 
 210   Node* generate_array_guard(Node* kls, RegionNode* region) {
 211     return generate_array_guard_common(kls, region, AnyArray);
 212   }
 213   Node* generate_non_array_guard(Node* kls, RegionNode* region) {
 214     return generate_array_guard_common(kls, region, NonArray);
 215   }
 216   Node* generate_objArray_guard(Node* kls, RegionNode* region) {
 217     return generate_array_guard_common(kls, region, ObjectArray);
 218   }
 219   Node* generate_non_objArray_guard(Node* kls, RegionNode* region) {
 220     return generate_array_guard_common(kls, region, NonObjectArray);
 221   }
 222   Node* generate_typeArray_guard(Node* kls, RegionNode* region) {
 223     return generate_array_guard_common(kls, region, TypeArray);
 224   }
<span class="line-modified"> 225   Node* generate_valueArray_guard(Node* kls, RegionNode* region) {</span>
 226     assert(UseFlatArray, &quot;can never be flattened&quot;);
<span class="line-modified"> 227     return generate_array_guard_common(kls, region, ValueArray);</span>
 228   }
 229   Node* generate_array_guard_common(Node* kls, RegionNode* region, ArrayKind kind);
 230   Node* generate_virtual_guard(Node* obj_klass, RegionNode* slow_region);
 231   CallJavaNode* generate_method_call(vmIntrinsics::ID method_id,
 232                                      bool is_virtual = false, bool is_static = false);
 233   CallJavaNode* generate_method_call_static(vmIntrinsics::ID method_id) {
 234     return generate_method_call(method_id, false, true);
 235   }
 236   CallJavaNode* generate_method_call_virtual(vmIntrinsics::ID method_id) {
 237     return generate_method_call(method_id, true, false);
 238   }
 239   Node * load_field_from_object(Node * fromObj, const char * fieldName, const char * fieldTypeString, bool is_exact, bool is_static, ciInstanceKlass * fromKls);
 240   Node * field_address_from_object(Node * fromObj, const char * fieldName, const char * fieldTypeString, bool is_exact, bool is_static, ciInstanceKlass * fromKls);
 241 
 242   Node* make_string_method_node(int opcode, Node* str1_start, Node* cnt1, Node* str2_start, Node* cnt2, StrIntrinsicNode::ArgEnc ae);
 243   bool inline_string_compareTo(StrIntrinsicNode::ArgEnc ae);
 244   bool inline_string_indexOf(StrIntrinsicNode::ArgEnc ae);
 245   bool inline_string_indexOfI(StrIntrinsicNode::ArgEnc ae);
 246   Node* make_indexOf_node(Node* src_start, Node* src_count, Node* tgt_start, Node* tgt_count,
 247                           RegionNode* region, Node* phi, StrIntrinsicNode::ArgEnc ae);
</pre>
<hr />
<pre>
2448   C-&gt;set_has_unsafe_access(true);  // Mark eventual nmethod as &quot;unsafe&quot;.
2449 
2450   Node* receiver = argument(0);  // type: oop
2451 
2452   // Build address expression.
2453   Node* adr;
2454   Node* heap_base_oop = top();
2455   Node* offset = top();
2456   Node* val;
2457 
2458   // The base is either a Java object or a value produced by Unsafe.staticFieldBase
2459   Node* base = argument(1);  // type: oop
2460   // The offset is a value produced by Unsafe.staticFieldOffset or Unsafe.objectFieldOffset
2461   offset = argument(2);  // type: long
2462   // We currently rely on the cookies produced by Unsafe.xxxFieldOffset
2463   // to be plain byte offsets, which are also the same as those accepted
2464   // by oopDesc::field_addr.
2465   assert(Unsafe_field_offset_to_byte_offset(11) == 11,
2466          &quot;fieldOffset must be byte-scaled&quot;);
2467 
<span class="line-modified">2468   ciValueKlass* value_klass = NULL;</span>
2469   if (type == T_INLINE_TYPE) {
2470     Node* cls = null_check(argument(4));
2471     if (stopped()) {
2472       return true;
2473     }
2474     Node* kls = load_klass_from_mirror(cls, false, NULL, 0);
2475     const TypeKlassPtr* kls_t = _gvn.type(kls)-&gt;isa_klassptr();
2476     if (!kls_t-&gt;klass_is_exact()) {
2477       return false;
2478     }
2479     ciKlass* klass = kls_t-&gt;klass();
<span class="line-modified">2480     if (!klass-&gt;is_valuetype()) {</span>
2481       return false;
2482     }
<span class="line-modified">2483     value_klass = klass-&gt;as_value_klass();</span>
2484   }
2485 
2486   receiver = null_check(receiver);
2487   if (stopped()) {
2488     return true;
2489   }
2490 
<span class="line-modified">2491   if (base-&gt;is_ValueType()) {</span>
<span class="line-modified">2492     ValueTypeNode* vt = base-&gt;as_ValueType();</span>
2493 
2494     if (is_store) {
<span class="line-modified">2495       if (!vt-&gt;is_allocated(&amp;_gvn) || !_gvn.type(vt)-&gt;is_valuetype()-&gt;larval()) {</span>
2496         return false;
2497       }
2498       base = vt-&gt;get_oop();
2499     } else {
2500       if (offset-&gt;is_Con()) {
2501         long off = find_long_con(offset, 0);
<span class="line-modified">2502         ciValueKlass* vk = vt-&gt;type()-&gt;value_klass();</span>
2503         if ((long)(int)off != off || !vk-&gt;contains_field_offset(off)) {
2504           return false;
2505         }
2506 
2507         ciField* f = vk-&gt;get_non_flattened_field_by_offset((int)off);
2508 
2509         if (f != NULL) {
2510           BasicType bt = f-&gt;layout_type();
2511           if (bt == T_ARRAY || bt == T_NARROWOOP) {
2512             bt = T_OBJECT;
2513           }
2514           if (bt == type) {
<span class="line-modified">2515             if (bt != T_INLINE_TYPE || f-&gt;type() == value_klass) {</span>
2516               set_result(vt-&gt;field_value_by_offset((int)off, false));
2517               return true;
2518             }
2519           }
2520         }
2521       }
2522       // Re-execute the unsafe access if allocation triggers deoptimization.
2523       PreserveReexecuteState preexecs(this);
2524       jvms()-&gt;set_should_reexecute(true);
2525       base = vt-&gt;buffer(this)-&gt;get_oop();
2526     }
2527   }
2528 
2529   // 32-bit machines ignore the high half!
2530   offset = ConvL2X(offset);
2531   adr = make_unsafe_address(base, offset, is_store ? ACCESS_WRITE : ACCESS_READ, type, kind == Relaxed);
2532 
2533   if (_gvn.type(base)-&gt;isa_ptr() == TypePtr::NULL_PTR) {
<span class="line-modified">2534     if (type != T_OBJECT &amp;&amp; (value_klass == NULL || !value_klass-&gt;has_object_fields())) {</span>
2535       decorators |= IN_NATIVE; // off-heap primitive access
2536     } else {
2537       return false; // off-heap oop accesses are not supported
2538     }
2539   } else {
2540     heap_base_oop = base; // on-heap or mixed access
2541   }
2542 
2543   // Can base be NULL? Otherwise, always on-heap access.
2544   bool can_access_non_heap = TypePtr::NULL_PTR-&gt;higher_equal(_gvn.type(base));
2545 
2546   if (!can_access_non_heap) {
2547     decorators |= IN_HEAP;
2548   }
2549 
2550   val = is_store ? argument(4 + (type == T_INLINE_TYPE ? 1 : 0)) : NULL;
2551 
2552   const TypePtr* adr_type = _gvn.type(adr)-&gt;isa_ptr();
2553   if (adr_type == TypePtr::NULL_PTR) {
2554     return false; // off-heap access with zero address
</pre>
<hr />
<pre>
2594     if (bt == T_BYTE &amp;&amp; adr_type-&gt;isa_aryptr()) {
2595       // Alias type doesn&#39;t differentiate between byte[] and boolean[]).
2596       // Use address type to get the element type.
2597       bt = adr_type-&gt;is_aryptr()-&gt;elem()-&gt;array_element_basic_type();
2598     }
2599     if (bt == T_ARRAY || bt == T_NARROWOOP) {
2600       // accessing an array field with getReference is not a mismatch
2601       bt = T_OBJECT;
2602     }
2603     if ((bt == T_OBJECT) != (type == T_OBJECT)) {
2604       // Don&#39;t intrinsify mismatched object accesses
2605       return false;
2606     }
2607     mismatched = (bt != type);
2608   } else if (alias_type-&gt;adr_type()-&gt;isa_oopptr()) {
2609     mismatched = true; // conservatively mark all &quot;wide&quot; on-heap accesses as mismatched
2610   }
2611 
2612   if (type == T_INLINE_TYPE) {
2613     if (adr_type-&gt;isa_instptr()) {
<span class="line-modified">2614       if (field == NULL || field-&gt;type() != value_klass) {</span>
2615         mismatched = true;
2616       }
2617     } else if (adr_type-&gt;isa_aryptr()) {
2618       const Type* elem = adr_type-&gt;is_aryptr()-&gt;elem();
<span class="line-modified">2619       if (!elem-&gt;isa_valuetype()) {</span>
2620         mismatched = true;
<span class="line-modified">2621       } else if (elem-&gt;value_klass() != value_klass) {</span>
2622         mismatched = true;
2623       }
2624     }
2625     if (is_store) {
2626       const Type* val_t = _gvn.type(val);
<span class="line-modified">2627       if (!val_t-&gt;isa_valuetype() || val_t-&gt;value_klass() != value_klass) {</span>
2628         return false;
2629       }
2630     }
2631   }
2632 
2633   assert(!mismatched || alias_type-&gt;adr_type()-&gt;is_oopptr(), &quot;off-heap access can&#39;t be mismatched&quot;);
2634 
2635   if (mismatched) {
2636     decorators |= C2_MISMATCHED;
2637   }
2638 
2639   // First guess at the value type.
2640   const Type *value_type = Type::get_const_basic_type(type);
2641 
2642   // Figure out the memory ordering.
2643   decorators |= mo_decorator_for_access_kind(kind);
2644 
2645   if (!is_store) {
2646     if (type == T_OBJECT) {
2647       const TypeOopPtr* tjp = sharpen_unsafe_type(alias_type, adr_type);
</pre>
<hr />
<pre>
2655 
2656   // Heap pointers get a null-check from the interpreter,
2657   // as a courtesy.  However, this is not guaranteed by Unsafe,
2658   // and it is not possible to fully distinguish unintended nulls
2659   // from intended ones in this API.
2660 
2661   if (!is_store) {
2662     Node* p = NULL;
2663     // Try to constant fold a load from a constant field
2664 
2665     if (heap_base_oop != top() &amp;&amp; field != NULL &amp;&amp; field-&gt;is_constant() &amp;&amp; !mismatched) {
2666       // final or stable field
2667       p = make_constant_from_field(field, heap_base_oop);
2668     }
2669 
2670     if (p == NULL) { // Could not constant fold the load
2671       if (type == T_INLINE_TYPE) {
2672         if (adr_type-&gt;isa_instptr() &amp;&amp; !mismatched) {
2673           ciInstanceKlass* holder = adr_type-&gt;is_instptr()-&gt;klass()-&gt;as_instance_klass();
2674           int offset = adr_type-&gt;is_instptr()-&gt;offset();
<span class="line-modified">2675           p = ValueTypeNode::make_from_flattened(this, value_klass, base, base, holder, offset, decorators);</span>
2676         } else {
<span class="line-modified">2677           p = ValueTypeNode::make_from_flattened(this, value_klass, base, adr, NULL, 0, decorators);</span>
2678         }
2679       } else {
2680         p = access_load_at(heap_base_oop, adr, adr_type, value_type, type, decorators);
2681       }
2682       // Normalize the value returned by getBoolean in the following cases
2683       if (type == T_BOOLEAN &amp;&amp;
2684           (mismatched ||
2685            heap_base_oop == top() ||                  // - heap_base_oop is NULL or
2686            (can_access_non_heap &amp;&amp; field == NULL))    // - heap_base_oop is potentially NULL
2687                                                       //   and the unsafe access is made to large offset
2688                                                       //   (i.e., larger than the maximum offset necessary for any
2689                                                       //   field access)
2690             ) {
2691           IdealKit ideal = IdealKit(this);
2692 #define __ ideal.
2693           IdealVariable normalized_result(ideal);
2694           __ declarations_done();
2695           __ set(normalized_result, p);
2696           __ if_then(p, BoolTest::ne, ideal.ConI(0));
2697           __ set(normalized_result, ideal.ConI(1));
2698           ideal.end_if();
2699           final_sync(ideal);
2700           p = __ value(normalized_result);
2701 #undef __
2702       }
2703     }
2704     if (type == T_ADDRESS) {
2705       p = gvn().transform(new CastP2XNode(NULL, p));
2706       p = ConvX2UL(p);
2707     }
<span class="line-modified">2708     if (field != NULL &amp;&amp; field-&gt;type()-&gt;is_valuetype() &amp;&amp; !field-&gt;is_flattened()) {</span>
<span class="line-modified">2709       // Load a non-flattened value type from memory</span>
<span class="line-modified">2710       if (value_type-&gt;value_klass()-&gt;is_scalarizable()) {</span>
<span class="line-modified">2711         p = ValueTypeNode::make_from_oop(this, p, value_type-&gt;value_klass());</span>
2712       } else {
<span class="line-modified">2713         p = null2default(p, value_type-&gt;value_klass());</span>
2714       }
2715     }
2716     // The load node has the control of the preceding MemBarCPUOrder.  All
2717     // following nodes will have the control of the MemBarCPUOrder inserted at
2718     // the end of this method.  So, pushing the load onto the stack at a later
2719     // point is fine.
2720     set_result(p);
2721   } else {
2722     if (bt == T_ADDRESS) {
2723       // Repackage the long as a pointer.
2724       val = ConvL2X(val);
2725       val = gvn().transform(new CastX2PNode(val));
2726     }
2727     if (type == T_INLINE_TYPE) {
2728       if (adr_type-&gt;isa_instptr() &amp;&amp; !mismatched) {
2729         ciInstanceKlass* holder = adr_type-&gt;is_instptr()-&gt;klass()-&gt;as_instance_klass();
2730         int offset = adr_type-&gt;is_instptr()-&gt;offset();
<span class="line-modified">2731         val-&gt;as_ValueType()-&gt;store_flattened(this, base, base, holder, offset, decorators);</span>
2732       } else {
<span class="line-modified">2733         val-&gt;as_ValueType()-&gt;store_flattened(this, base, adr, NULL, 0, decorators);</span>
2734       }
2735     } else {
2736       access_store_at(heap_base_oop, adr, adr_type, val, value_type, type, decorators);
2737     }
2738   }
2739 
<span class="line-modified">2740   if (argument(1)-&gt;is_ValueType() &amp;&amp; is_store) {</span>
<span class="line-modified">2741     Node* value = ValueTypeNode::make_from_oop(this, base, _gvn.type(base)-&gt;value_klass());</span>
<span class="line-modified">2742     value = value-&gt;as_ValueType()-&gt;make_larval(this, false);</span>
2743     replace_in_map(argument(1), value);
2744   }
2745 
2746   return true;
2747 }
2748 
2749 bool LibraryCallKit::inline_unsafe_make_private_buffer() {
2750   Node* receiver = argument(0);
2751   Node* value = argument(1);
2752 
2753   receiver = null_check(receiver);
2754   if (stopped()) {
2755     return true;
2756   }
2757 
<span class="line-modified">2758   if (!value-&gt;is_ValueType()) {</span>
2759     return false;
2760   }
2761 
<span class="line-modified">2762   set_result(value-&gt;as_ValueType()-&gt;make_larval(this, true));</span>
2763 
2764   return true;
2765 }
2766 
2767 bool LibraryCallKit::inline_unsafe_finish_private_buffer() {
2768   Node* receiver = argument(0);
2769   Node* buffer = argument(1);
2770 
2771   receiver = null_check(receiver);
2772   if (stopped()) {
2773     return true;
2774   }
2775 
<span class="line-modified">2776   if (!buffer-&gt;is_ValueType()) {</span>
2777     return false;
2778   }
2779 
<span class="line-modified">2780   ValueTypeNode* vt = buffer-&gt;as_ValueType();</span>
<span class="line-modified">2781   if (!vt-&gt;is_allocated(&amp;_gvn) || !_gvn.type(vt)-&gt;is_valuetype()-&gt;larval()) {</span>
2782     return false;
2783   }
2784 
2785   set_result(vt-&gt;finish_larval(this));
2786 
2787   return true;
2788 }
2789 
2790 //----------------------------inline_unsafe_load_store----------------------------
2791 // This method serves a couple of different customers (depending on LoadStoreKind):
2792 //
2793 // LS_cmp_swap:
2794 //
2795 //   boolean compareAndSetReference(Object o, long offset, Object expected, Object x);
2796 //   boolean compareAndSetInt(   Object o, long offset, int    expected, int    x);
2797 //   boolean compareAndSetLong(  Object o, long offset, long   expected, long   x);
2798 //
2799 // LS_cmp_swap_weak:
2800 //
2801 //   boolean weakCompareAndSetReference(       Object o, long offset, Object expected, Object x);
</pre>
<hr />
<pre>
3482   region-&gt;init_req(1, control());
3483 
3484   C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
3485   set_result(region, phi);
3486   return true;
3487 }
3488 
3489 //-------------------------inline_Class_cast-------------------
3490 bool LibraryCallKit::inline_Class_cast() {
3491   Node* mirror = argument(0); // Class
3492   Node* obj    = argument(1);
3493   const TypeInstPtr* mirror_con = _gvn.type(mirror)-&gt;isa_instptr();
3494   if (mirror_con == NULL) {
3495     return false;  // dead path (mirror-&gt;is_top()).
3496   }
3497   if (obj == NULL || obj-&gt;is_top()) {
3498     return false;  // dead path
3499   }
3500   ciKlass* obj_klass = NULL;
3501   const Type* obj_t = _gvn.type(obj);
<span class="line-modified">3502   if (obj-&gt;is_ValueType()) {</span>
<span class="line-modified">3503     obj_klass = obj_t-&gt;value_klass();</span>
3504   } else if (obj_t-&gt;isa_oopptr()) {
3505     obj_klass = obj_t-&gt;is_oopptr()-&gt;klass();
3506   }
3507 
3508   // First, see if Class.cast() can be folded statically.
3509   // java_mirror_type() returns non-null for compile-time Class constants.
3510   ciType* tm = mirror_con-&gt;java_mirror_type();
3511   if (tm != NULL &amp;&amp; tm-&gt;is_klass() &amp;&amp; obj_klass != NULL) {
3512     if (!obj_klass-&gt;is_loaded()) {
3513       // Don&#39;t use intrinsic when class is not loaded.
3514       return false;
3515     } else {
<span class="line-modified">3516       if (!obj-&gt;is_ValueType() &amp;&amp; tm-&gt;as_klass()-&gt;is_valuetype()) {</span>
3517         // Casting to .val, check for null
3518         obj = null_check(obj);
3519         if (stopped()) {
3520           return true;
3521         }
3522       }
3523       int static_res = C-&gt;static_subtype_check(tm-&gt;as_klass(), obj_klass);
3524       if (static_res == Compile::SSC_always_true) {
3525         // isInstance() is true - fold the code.
3526         set_result(obj);
3527         return true;
3528       } else if (static_res == Compile::SSC_always_false) {
3529         // Don&#39;t use intrinsic, have to throw ClassCastException.
3530         // If the reference is null, the non-intrinsic bytecode will
3531         // be optimized appropriately.
3532         return false;
3533       }
3534     }
3535   }
3536 
</pre>
<hr />
<pre>
3544   // Do checkcast (Parse::do_checkcast()) optimizations here.
3545 
3546   mirror = null_check(mirror);
3547   // If mirror is dead, only null-path is taken.
3548   if (stopped()) {
3549     return true;
3550   }
3551 
3552   // Not-subtype or the mirror&#39;s klass ptr is NULL (in case it is a primitive).
3553   enum { _bad_type_path = 1, _prim_path = 2, _npe_path = 3, PATH_LIMIT };
3554   RegionNode* region = new RegionNode(PATH_LIMIT);
3555   record_for_igvn(region);
3556 
3557   // Now load the mirror&#39;s klass metaobject, and null-check it.
3558   // If kls is null, we have a primitive mirror and
3559   // nothing is an instance of a primitive type.
3560   Node* kls = load_klass_from_mirror(mirror, false, region, _prim_path);
3561 
3562   Node* res = top();
3563   if (!stopped()) {
<span class="line-modified">3564     if (EnableValhalla &amp;&amp; !obj-&gt;is_ValueType()) {</span>
3565       // Check if we are casting to .val
3566       Node* is_val_kls = generate_value_guard(kls, NULL);
3567       if (is_val_kls != NULL) {
3568         RegionNode* r = new RegionNode(3);
3569         record_for_igvn(r);
3570         r-&gt;init_req(1, control());
3571 
3572         // Casting to .val, check for null
3573         set_control(is_val_kls);
3574         Node* null_ctr = top();
3575         null_check_oop(obj, &amp;null_ctr);
3576         region-&gt;init_req(_npe_path, null_ctr);
3577         r-&gt;init_req(2, control());
3578 
3579         set_control(_gvn.transform(r));
3580       }
3581     }
3582 
3583     Node* bad_type_ctrl = top();
3584     // Do checkcast optimizations.
</pre>
<hr />
<pre>
3708   set_result(_gvn.transform(phi));
3709   return true;
3710 }
3711 
3712 //---------------------generate_array_guard_common------------------------
3713 Node* LibraryCallKit::generate_array_guard_common(Node* kls, RegionNode* region, ArrayKind kind) {
3714 
3715   if (stopped()) {
3716     return NULL;
3717   }
3718 
3719   // Like generate_guard, adds a new path onto the region.
3720   jint  layout_con = 0;
3721   Node* layout_val = get_layout_helper(kls, layout_con);
3722   if (layout_val == NULL) {
3723     bool query = 0;
3724     switch(kind) {
3725       case ObjectArray:    query = Klass::layout_helper_is_objArray(layout_con); break;
3726       case NonObjectArray: query = !Klass::layout_helper_is_objArray(layout_con); break;
3727       case TypeArray:      query = Klass::layout_helper_is_typeArray(layout_con); break;
<span class="line-modified">3728       case ValueArray:     query = Klass::layout_helper_is_flatArray(layout_con); break;</span>
3729       case AnyArray:       query = Klass::layout_helper_is_array(layout_con); break;
3730       case NonArray:       query = !Klass::layout_helper_is_array(layout_con); break;
3731       default:
3732         ShouldNotReachHere();
3733     }
3734     if (!query) {
3735       return NULL;                       // never a branch
3736     } else {                             // always a branch
3737       Node* always_branch = control();
3738       if (region != NULL)
3739         region-&gt;add_req(always_branch);
3740       set_control(top());
3741       return always_branch;
3742     }
3743   }
3744   unsigned int value = 0;
3745   BoolTest::mask btest = BoolTest::illegal;
3746   switch(kind) {
3747     case ObjectArray:
3748     case NonObjectArray: {
3749       value = Klass::_lh_array_tag_obj_value;
3750       layout_val = _gvn.transform(new RShiftINode(layout_val, intcon(Klass::_lh_array_tag_shift)));
3751       btest = kind == ObjectArray ? BoolTest::eq : BoolTest::ne;
3752       break;
3753     }
3754     case TypeArray: {
3755       value = Klass::_lh_array_tag_type_value;
3756       layout_val = _gvn.transform(new RShiftINode(layout_val, intcon(Klass::_lh_array_tag_shift)));
3757       btest = BoolTest::eq;
3758       break;
3759     }
<span class="line-modified">3760     case ValueArray: {</span>
3761       value = Klass::_lh_array_tag_vt_value;
3762       layout_val = _gvn.transform(new RShiftINode(layout_val, intcon(Klass::_lh_array_tag_shift)));
3763       btest = BoolTest::eq;
3764       break;
3765     }
3766     case AnyArray:    value = Klass::_lh_neutral_value; btest = BoolTest::lt; break;
3767     case NonArray:    value = Klass::_lh_neutral_value; btest = BoolTest::gt; break;
3768     default:
3769       ShouldNotReachHere();
3770   }
3771   // Now test the correct condition.
3772   jint nval = (jint)value;
3773   Node* cmp = _gvn.transform(new CmpINode(layout_val, intcon(nval)));
3774   Node* bol = _gvn.transform(new BoolNode(cmp, btest));
3775   return generate_fair_guard(bol, region);
3776 }
3777 
3778 
3779 //-----------------------inline_native_newArray--------------------------
3780 // private static native Object java.lang.reflect.Array.newArray(Class&lt;?&gt; componentType, int length);
</pre>
<hr />
<pre>
3881   return true;
3882 }
3883 
3884 //------------------------inline_array_copyOf----------------------------
3885 // public static &lt;T,U&gt; T[] java.util.Arrays.copyOf(     U[] original, int newLength,         Class&lt;? extends T[]&gt; newType);
3886 // public static &lt;T,U&gt; T[] java.util.Arrays.copyOfRange(U[] original, int from,      int to, Class&lt;? extends T[]&gt; newType);
3887 bool LibraryCallKit::inline_array_copyOf(bool is_copyOfRange) {
3888   if (too_many_traps(Deoptimization::Reason_intrinsic))  return false;
3889 
3890   // Get the arguments.
3891   Node* original          = argument(0);
3892   Node* start             = is_copyOfRange? argument(1): intcon(0);
3893   Node* end               = is_copyOfRange? argument(2): argument(1);
3894   Node* array_type_mirror = is_copyOfRange? argument(3): argument(2);
3895 
3896   const TypeAryPtr* original_t = _gvn.type(original)-&gt;isa_aryptr();
3897   const TypeInstPtr* mirror_t = _gvn.type(array_type_mirror)-&gt;isa_instptr();
3898   if (EnableValhalla &amp;&amp; UseFlatArray &amp;&amp;
3899       (original_t == NULL || mirror_t == NULL ||
3900        (mirror_t-&gt;java_mirror_type() == NULL &amp;&amp;
<span class="line-modified">3901         (original_t-&gt;elem()-&gt;isa_valuetype() ||</span>
3902          (original_t-&gt;elem()-&gt;make_oopptr() != NULL &amp;&amp;
<span class="line-modified">3903           original_t-&gt;elem()-&gt;make_oopptr()-&gt;can_be_value_type()))))) {</span>
3904     // We need to know statically if the copy is to a flattened array
3905     // or not but can&#39;t tell.
3906     return false;
3907   }
3908 
3909   Node* newcopy = NULL;
3910 
3911   // Set the original stack and the reexecute bit for the interpreter to reexecute
3912   // the bytecode that invokes Arrays.copyOf if deoptimization happens.
3913   { PreserveReexecuteState preexecs(this);
3914     jvms()-&gt;set_should_reexecute(true);
3915 
3916     array_type_mirror = null_check(array_type_mirror);
3917     original          = null_check(original);
3918 
3919     // Check if a null path was taken unconditionally.
3920     if (stopped())  return true;
3921 
3922     Node* orig_length = load_array_length(original);
3923 
3924     Node* klass_node = load_klass_from_mirror(array_type_mirror, false, NULL, 0);
3925     klass_node = null_check(klass_node);
3926 
3927     RegionNode* bailout = new RegionNode(1);
3928     record_for_igvn(bailout);
3929 
3930     // Despite the generic type of Arrays.copyOf, the mirror might be int, int[], etc.
3931     // Bail out if that is so.
<span class="line-modified">3932     // Value type array may have object field that would require a</span>
3933     // write barrier. Conservatively, go to slow path.
3934     BarrierSetC2* bs = BarrierSet::barrier_set()-&gt;barrier_set_c2();
3935     Node* not_objArray = !bs-&gt;array_copy_requires_gc_barriers(false, T_OBJECT, false, BarrierSetC2::Parsing) ?
3936         generate_typeArray_guard(klass_node, bailout) : generate_non_objArray_guard(klass_node, bailout);
3937     if (not_objArray != NULL) {
3938       // Improve the klass node&#39;s type from the new optimistic assumption:
3939       ciKlass* ak = ciArrayKlass::make(env()-&gt;Object_klass());
3940       const Type* akls = TypeKlassPtr::make(TypePtr::NotNull, ak, Type::Offset(0), false);
3941       Node* cast = new CastPPNode(klass_node, akls);
3942       cast-&gt;init_req(0, control());
3943       klass_node = _gvn.transform(cast);
3944     }
3945 
3946     Node* original_kls = load_object_klass(original);
3947     // ArrayCopyNode:Ideal may transform the ArrayCopyNode to
3948     // loads/stores but it is legal only if we&#39;re sure the
3949     // Arrays.copyOf would succeed. So we need all input arguments
3950     // to the copyOf to be validated, including that the copy to the
3951     // new array won&#39;t trigger an ArrayStoreException. That subtype
3952     // check can be optimized if we know something on the type of
3953     // the input array from type speculation.
3954     if (_gvn.type(klass_node)-&gt;singleton() &amp;&amp; !stopped()) {
3955       ciKlass* subk   = _gvn.type(original_kls)-&gt;is_klassptr()-&gt;klass();
3956       ciKlass* superk = _gvn.type(klass_node)-&gt;is_klassptr()-&gt;klass();
3957 
3958       int test = C-&gt;static_subtype_check(superk, subk);
3959       if (test != Compile::SSC_always_true &amp;&amp; test != Compile::SSC_always_false) {
3960         const TypeOopPtr* t_original = _gvn.type(original)-&gt;is_oopptr();
3961         if (t_original-&gt;speculative_type() != NULL) {
3962           original = maybe_cast_profiled_obj(original, t_original-&gt;speculative_type(), true);
3963           original_kls = load_object_klass(original);
3964         }
3965       }
3966     }
3967 
3968     if (UseFlatArray) {
3969       // Either both or neither new array klass and original array
3970       // klass must be flattened
<span class="line-modified">3971       Node* is_flat = generate_valueArray_guard(klass_node, NULL);</span>
3972       if (!original_t-&gt;is_not_flat()) {
<span class="line-modified">3973         generate_valueArray_guard(original_kls, bailout);</span>
3974       }
3975       if (is_flat != NULL) {
3976         RegionNode* r = new RegionNode(2);
3977         record_for_igvn(r);
3978         r-&gt;init_req(1, control());
3979         set_control(is_flat);
3980         if (!original_t-&gt;is_not_flat()) {
<span class="line-modified">3981           generate_valueArray_guard(original_kls, r);</span>
3982         }
3983         bailout-&gt;add_req(control());
3984         set_control(_gvn.transform(r));
3985       }
3986     }
3987 
3988     // Bail out if either start or end is negative.
3989     generate_negative_guard(start, bailout, &amp;start);
3990     generate_negative_guard(end,   bailout, &amp;end);
3991 
3992     Node* length = end;
3993     if (_gvn.type(start) != TypeInt::ZERO) {
3994       length = _gvn.transform(new SubINode(end, start));
3995     }
3996 
3997     // Bail out if length is negative.
3998     // Without this the new_array would throw
3999     // NegativeArraySizeException but IllegalArgumentException is what
4000     // should be thrown
4001     generate_negative_guard(length, bailout, &amp;length);
</pre>
<hr />
<pre>
4148 }
4149 
4150 
4151 /**
4152  * Build special case code for calls to hashCode on an object. This call may
4153  * be virtual (invokevirtual) or bound (invokespecial). For each case we generate
4154  * slightly different code.
4155  */
4156 bool LibraryCallKit::inline_native_hashcode(bool is_virtual, bool is_static) {
4157   assert(is_static == callee()-&gt;is_static(), &quot;correct intrinsic selection&quot;);
4158   assert(!(is_virtual &amp;&amp; is_static), &quot;either virtual, special, or static&quot;);
4159 
4160   enum { _slow_path = 1, _fast_path, _null_path, PATH_LIMIT };
4161 
4162   RegionNode* result_reg = new RegionNode(PATH_LIMIT);
4163   PhiNode*    result_val = new PhiNode(result_reg, TypeInt::INT);
4164   PhiNode*    result_io  = new PhiNode(result_reg, Type::ABIO);
4165   PhiNode*    result_mem = new PhiNode(result_reg, Type::MEMORY, TypePtr::BOTTOM);
4166   Node* obj = argument(0);
4167 
<span class="line-modified">4168   if (obj-&gt;is_ValueType() || gvn().type(obj)-&gt;is_valuetypeptr()) {</span>
4169     return false;
4170   }
4171 
4172   if (!is_static) {
4173     // Check for hashing null object
4174     obj = null_check_receiver();
4175     if (stopped())  return true;        // unconditionally null
4176     result_reg-&gt;init_req(_null_path, top());
4177     result_val-&gt;init_req(_null_path, top());
4178   } else {
4179     // Do a null check, and return zero if null.
4180     // System.identityHashCode(null) == 0
4181     Node* null_ctl = top();
4182     obj = null_check_oop(obj, &amp;null_ctl);
4183     result_reg-&gt;init_req(_null_path, null_ctl);
4184     result_val-&gt;init_req(_null_path, _gvn.intcon(0));
4185   }
4186 
4187   // Unconditionally null?  Then return right away.
4188   if (stopped()) {
</pre>
<hr />
<pre>
4200   // If this is a virtual call, we generate a funny guard.  We pull out
4201   // the vtable entry corresponding to hashCode() from the target object.
4202   // If the target method which we are calling happens to be the native
4203   // Object hashCode() method, we pass the guard.  We do not need this
4204   // guard for non-virtual calls -- the caller is known to be the native
4205   // Object hashCode().
4206   if (is_virtual) {
4207     // After null check, get the object&#39;s klass.
4208     Node* obj_klass = load_object_klass(obj);
4209     generate_virtual_guard(obj_klass, slow_region);
4210   }
4211 
4212   // Get the header out of the object, use LoadMarkNode when available
4213   Node* header_addr = basic_plus_adr(obj, oopDesc::mark_offset_in_bytes());
4214   // The control of the load must be NULL. Otherwise, the load can move before
4215   // the null check after castPP removal.
4216   Node* no_ctrl = NULL;
4217   Node* header = make_load(no_ctrl, header_addr, TypeX_X, TypeX_X-&gt;basic_type(), MemNode::unordered);
4218 
4219   // Test the header to see if it is unlocked.
<span class="line-modified">4220   // This also serves as guard against value types (they have the always_locked_pattern set).</span>
4221   Node *lock_mask      = _gvn.MakeConX(markWord::biased_lock_mask_in_place);
4222   Node *lmasked_header = _gvn.transform(new AndXNode(header, lock_mask));
4223   Node *unlocked_val   = _gvn.MakeConX(markWord::unlocked_value);
4224   Node *chk_unlocked   = _gvn.transform(new CmpXNode( lmasked_header, unlocked_val));
4225   Node *test_unlocked  = _gvn.transform(new BoolNode( chk_unlocked, BoolTest::ne));
4226 
4227   generate_slow_guard(test_unlocked, slow_region);
4228 
4229   // Get the hash value and check to see that it has been properly assigned.
4230   // We depend on hash_mask being at most 32 bits and avoid the use of
4231   // hash_mask_in_place because it could be larger than 32 bits in a 64-bit
4232   // vm: see markWord.hpp.
4233   Node *hash_mask      = _gvn.intcon(markWord::hash_mask);
4234   Node *hash_shift     = _gvn.intcon(markWord::hash_shift);
4235   Node *hshifted_header= _gvn.transform(new URShiftXNode(header, hash_shift));
4236   // This hack lets the hash bits live anywhere in the mark object now, as long
4237   // as the shift drops the relevant bits into the low 32 bits.  Note that
4238   // Java spec says that HashCode is an int so there&#39;s no point in capturing
4239   // an &#39;X&#39;-sized hashcode (32 in 32-bit build or 64 in 64-bit build).
4240   hshifted_header      = ConvX2I(hshifted_header);
</pre>
<hr />
<pre>
4268     result_reg-&gt;init_req(_slow_path, control());
4269     result_val-&gt;init_req(_slow_path, slow_result);
4270     result_io  -&gt;set_req(_slow_path, i_o());
4271     result_mem -&gt;set_req(_slow_path, reset_memory());
4272   }
4273 
4274   // Return the combined state.
4275   set_i_o(        _gvn.transform(result_io)  );
4276   set_all_memory( _gvn.transform(result_mem));
4277 
4278   set_result(result_reg, result_val);
4279   return true;
4280 }
4281 
4282 //---------------------------inline_native_getClass----------------------------
4283 // public final native Class&lt;?&gt; java.lang.Object.getClass();
4284 //
4285 // Build special case code for calls to getClass on an object.
4286 bool LibraryCallKit::inline_native_getClass() {
4287   Node* obj = argument(0);
<span class="line-modified">4288   if (obj-&gt;is_ValueType()) {</span>
<span class="line-modified">4289     ciKlass* vk = _gvn.type(obj)-&gt;value_klass();</span>
4290     set_result(makecon(TypeInstPtr::make(vk-&gt;java_mirror())));
4291     return true;
4292   }
4293   obj = null_check_receiver();
4294   if (stopped())  return true;
4295   set_result(load_mirror_from_klass(load_object_klass(obj)));
4296   return true;
4297 }
4298 
4299 //-----------------inline_native_Reflection_getCallerClass---------------------
4300 // public static native Class&lt;?&gt; sun.reflect.Reflection.getCallerClass();
4301 //
4302 // In the presence of deep enough inlining, getCallerClass() becomes a no-op.
4303 //
4304 // NOTE: This code must perform the same logic as JVM_GetCallerClass
4305 // in that it must skip particular security frames and checks for
4306 // caller sensitive methods.
4307 bool LibraryCallKit::inline_native_Reflection_getCallerClass() {
4308 #ifndef PRODUCT
4309   if ((C-&gt;print_intrinsics() || C-&gt;print_inlining()) &amp;&amp; Verbose) {
</pre>
<hr />
<pre>
4586 //
4587 // The general case has two steps, allocation and copying.
4588 // Allocation has two cases, and uses GraphKit::new_instance or new_array.
4589 //
4590 // Copying also has two cases, oop arrays and everything else.
4591 // Oop arrays use arrayof_oop_arraycopy (same as System.arraycopy).
4592 // Everything else uses the tight inline loop supplied by CopyArrayNode.
4593 //
4594 // These steps fold up nicely if and when the cloned object&#39;s klass
4595 // can be sharply typed as an object array, a type array, or an instance.
4596 //
4597 bool LibraryCallKit::inline_native_clone(bool is_virtual) {
4598   PhiNode* result_val;
4599 
4600   // Set the reexecute bit for the interpreter to reexecute
4601   // the bytecode that invokes Object.clone if deoptimization happens.
4602   { PreserveReexecuteState preexecs(this);
4603     jvms()-&gt;set_should_reexecute(true);
4604 
4605     Node* obj = argument(0);
<span class="line-modified">4606     if (obj-&gt;is_ValueType()) {</span>
4607       return false;
4608     }
4609 
4610     obj = null_check_receiver();
4611     if (stopped())  return true;
4612 
4613     const TypeOopPtr* obj_type = _gvn.type(obj)-&gt;is_oopptr();
4614 
4615     // If we are going to clone an instance, we need its exact type to
4616     // know the number and types of fields to convert the clone to
4617     // loads/stores. Maybe a speculative type can help us.
4618     if (!obj_type-&gt;klass_is_exact() &amp;&amp;
4619         obj_type-&gt;speculative_type() != NULL &amp;&amp;
4620         obj_type-&gt;speculative_type()-&gt;is_instance_klass() &amp;&amp;
<span class="line-modified">4621         !obj_type-&gt;speculative_type()-&gt;is_valuetype()) {</span>
4622       ciInstanceKlass* spec_ik = obj_type-&gt;speculative_type()-&gt;as_instance_klass();
4623       if (spec_ik-&gt;nof_nonstatic_fields() &lt;= ArrayCopyLoadStoreMaxElem &amp;&amp;
4624           !spec_ik-&gt;has_injected_fields()) {
4625         ciKlass* k = obj_type-&gt;klass();
4626         if (!k-&gt;is_instance_klass() ||
4627             k-&gt;as_instance_klass()-&gt;is_interface() ||
4628             k-&gt;as_instance_klass()-&gt;has_subklass()) {
4629           obj = maybe_cast_profiled_obj(obj, obj_type-&gt;speculative_type(), false);
4630         }
4631       }
4632     }
4633 
4634     // Conservatively insert a memory barrier on all memory slices.
4635     // Do not let writes into the original float below the clone.
4636     insert_mem_bar(Op_MemBarCPUOrder);
4637 
4638     // paths into result_reg:
4639     enum {
4640       _slow_path = 1,     // out-of-line call to clone method (virtual or not)
4641       _objArray_path,     // plain array allocation, plus arrayof_oop_arraycopy
</pre>
<hr />
<pre>
4647     result_val             = new PhiNode(result_reg, TypeInstPtr::NOTNULL);
4648     PhiNode*    result_i_o = new PhiNode(result_reg, Type::ABIO);
4649     PhiNode*    result_mem = new PhiNode(result_reg, Type::MEMORY, TypePtr::BOTTOM);
4650     record_for_igvn(result_reg);
4651 
4652     Node* obj_klass = load_object_klass(obj);
4653     // We only go to the fast case code if we pass a number of guards.
4654     // The paths which do not pass are accumulated in the slow_region.
4655     RegionNode* slow_region = new RegionNode(1);
4656     record_for_igvn(slow_region);
4657 
4658     Node* array_ctl = generate_array_guard(obj_klass, (RegionNode*)NULL);
4659     if (array_ctl != NULL) {
4660       // It&#39;s an array.
4661       PreserveJVMState pjvms(this);
4662       set_control(array_ctl);
4663 
4664       BarrierSetC2* bs = BarrierSet::barrier_set()-&gt;barrier_set_c2();
4665       if (bs-&gt;array_copy_requires_gc_barriers(true, T_OBJECT, true, BarrierSetC2::Parsing) &amp;&amp;
4666           (!obj_type-&gt;isa_aryptr() || !obj_type-&gt;is_aryptr()-&gt;is_not_flat())) {
<span class="line-modified">4667         // Flattened value type array may have object field that would require a</span>
4668         // write barrier. Conservatively, go to slow path.
<span class="line-modified">4669         generate_valueArray_guard(obj_klass, slow_region);</span>
4670       }
4671 
4672       if (!stopped()) {
4673         Node* obj_length = load_array_length(obj);
4674         Node* obj_size  = NULL;
4675         Node* alloc_obj = new_array(obj_klass, obj_length, 0, &amp;obj_size, /*deoptimize_on_exception=*/true);
4676 
4677         BarrierSetC2* bs = BarrierSet::barrier_set()-&gt;barrier_set_c2();
4678         if (bs-&gt;array_copy_requires_gc_barriers(true, T_OBJECT, true, BarrierSetC2::Parsing)) {
4679           // If it is an oop array, it requires very special treatment,
4680           // because gc barriers are required when accessing the array.
4681           Node* is_obja = generate_objArray_guard(obj_klass, (RegionNode*)NULL);
4682           if (is_obja != NULL) {
4683             PreserveJVMState pjvms2(this);
4684             set_control(is_obja);
4685             // Generate a direct call to the right arraycopy function(s).
4686             Node* alloc = tightly_coupled_allocation(alloc_obj, NULL);
4687             ArrayCopyNode* ac = ArrayCopyNode::make(this, true, obj, intcon(0), alloc_obj, intcon(0), obj_length, alloc != NULL, false);
4688             ac-&gt;set_clone_oop_array();
4689             Node* n = _gvn.transform(ac);
</pre>
<hr />
<pre>
5126     // (9) each element of an oop array must be assignable
5127     Node* dest_klass = load_object_klass(dest);
5128     if (src != dest) {
5129       Node* not_subtype_ctrl = gen_subtype_check(src, dest_klass);
5130 
5131       if (not_subtype_ctrl != top()) {
5132         PreserveJVMState pjvms(this);
5133         set_control(not_subtype_ctrl);
5134         uncommon_trap(Deoptimization::Reason_intrinsic,
5135                       Deoptimization::Action_make_not_entrant);
5136         assert(stopped(), &quot;Should be stopped&quot;);
5137       }
5138     }
5139 
5140     const TypeKlassPtr* dest_klass_t = _gvn.type(dest_klass)-&gt;is_klassptr();
5141     const Type* toop = TypeOopPtr::make_from_klass(dest_klass_t-&gt;klass());
5142     src = _gvn.transform(new CheckCastPPNode(control(), src, toop));
5143     src_type = _gvn.type(src);
5144     top_src  = src_type-&gt;isa_aryptr();
5145 
<span class="line-modified">5146     if (top_dest != NULL &amp;&amp; !top_dest-&gt;elem()-&gt;isa_valuetype() &amp;&amp; !top_dest-&gt;is_not_flat()) {</span>
<span class="line-modified">5147       generate_valueArray_guard(dest_klass, slow_region);</span>
5148     }
5149 
<span class="line-modified">5150     if (top_src != NULL &amp;&amp; !top_src-&gt;elem()-&gt;isa_valuetype() &amp;&amp; !top_src-&gt;is_not_flat()) {</span>
5151       Node* src_klass = load_object_klass(src);
<span class="line-modified">5152       generate_valueArray_guard(src_klass, slow_region);</span>
5153     }
5154 
5155     {
5156       PreserveJVMState pjvms(this);
5157       set_control(_gvn.transform(slow_region));
5158       uncommon_trap(Deoptimization::Reason_intrinsic,
5159                     Deoptimization::Action_make_not_entrant);
5160       assert(stopped(), &quot;Should be stopped&quot;);
5161     }
5162   }
5163 
5164   arraycopy_move_allocation_here(alloc, dest, saved_jvms, saved_reexecute_sp, new_idx);
5165 
5166   if (stopped()) {
5167     return true;
5168   }
5169 
5170   ArrayCopyNode* ac = ArrayCopyNode::make(this, true, src, src_offset, dest, dest_offset, length, alloc != NULL, negative_length_guard_generated,
5171                                           // Create LoadRange and LoadKlass nodes for use during macro expansion here
5172                                           // so the compiler has a chance to eliminate them: during macro expansion,
</pre>
</td>
<td>
<hr />
<pre>
  25 #include &quot;precompiled.hpp&quot;
  26 #include &quot;asm/macroAssembler.hpp&quot;
  27 #include &quot;ci/ciUtilities.inline.hpp&quot;
  28 #include &quot;classfile/systemDictionary.hpp&quot;
  29 #include &quot;classfile/vmSymbols.hpp&quot;
  30 #include &quot;compiler/compileBroker.hpp&quot;
  31 #include &quot;compiler/compileLog.hpp&quot;
  32 #include &quot;gc/shared/barrierSet.hpp&quot;
  33 #include &quot;jfr/support/jfrIntrinsics.hpp&quot;
  34 #include &quot;memory/resourceArea.hpp&quot;
  35 #include &quot;oops/klass.inline.hpp&quot;
  36 #include &quot;oops/objArrayKlass.hpp&quot;
  37 #include &quot;opto/addnode.hpp&quot;
  38 #include &quot;opto/arraycopynode.hpp&quot;
  39 #include &quot;opto/c2compiler.hpp&quot;
  40 #include &quot;opto/callGenerator.hpp&quot;
  41 #include &quot;opto/castnode.hpp&quot;
  42 #include &quot;opto/cfgnode.hpp&quot;
  43 #include &quot;opto/convertnode.hpp&quot;
  44 #include &quot;opto/countbitsnode.hpp&quot;
<span class="line-added">  45 #include &quot;opto/inlinetypenode.hpp&quot;</span>
  46 #include &quot;opto/intrinsicnode.hpp&quot;
  47 #include &quot;opto/idealKit.hpp&quot;
  48 #include &quot;opto/mathexactnode.hpp&quot;
  49 #include &quot;opto/movenode.hpp&quot;
  50 #include &quot;opto/mulnode.hpp&quot;
  51 #include &quot;opto/narrowptrnode.hpp&quot;
  52 #include &quot;opto/opaquenode.hpp&quot;
  53 #include &quot;opto/parse.hpp&quot;
  54 #include &quot;opto/runtime.hpp&quot;
  55 #include &quot;opto/rootnode.hpp&quot;
  56 #include &quot;opto/subnode.hpp&quot;

  57 #include &quot;prims/nativeLookup.hpp&quot;
  58 #include &quot;prims/unsafe.hpp&quot;
  59 #include &quot;runtime/objectMonitor.hpp&quot;
  60 #include &quot;runtime/sharedRuntime.hpp&quot;
  61 #include &quot;utilities/macros.hpp&quot;
  62 #include &quot;utilities/powerOfTwo.hpp&quot;
  63 
  64 class LibraryIntrinsic : public InlineCallGenerator {
  65   // Extend the set of intrinsics known to the runtime:
  66  public:
  67  private:
  68   bool             _is_virtual;
  69   bool             _does_virtual_dispatch;
  70   int8_t           _predicates_count;  // Intrinsic is predicated by several conditions
  71   int8_t           _last_predicate; // Last generated predicate
  72   vmIntrinsics::ID _intrinsic_id;
  73 
  74  public:
  75   LibraryIntrinsic(ciMethod* m, bool is_virtual, int predicates_count, bool does_virtual_dispatch, vmIntrinsics::ID id)
  76     : InlineCallGenerator(m),
</pre>
<hr />
<pre>
 121       _reexecute_sp = sp() + nargs;  // &quot;push&quot; arguments back on stack
 122     }
 123   }
 124 
 125   virtual LibraryCallKit* is_LibraryCallKit() const { return (LibraryCallKit*)this; }
 126 
 127   ciMethod*         caller()    const    { return jvms()-&gt;method(); }
 128   int               bci()       const    { return jvms()-&gt;bci(); }
 129   LibraryIntrinsic* intrinsic() const    { return _intrinsic; }
 130   vmIntrinsics::ID  intrinsic_id() const { return _intrinsic-&gt;intrinsic_id(); }
 131   ciMethod*         callee()    const    { return _intrinsic-&gt;method(); }
 132 
 133   bool  try_to_inline(int predicate);
 134   Node* try_to_predicate(int predicate);
 135 
 136   void push_result() {
 137     // Push the result onto the stack.
 138     Node* res = result();
 139     if (!stopped() &amp;&amp; res != NULL) {
 140       BasicType bt = res-&gt;bottom_type()-&gt;basic_type();
<span class="line-modified"> 141       if (C-&gt;inlining_incrementally() &amp;&amp; res-&gt;is_InlineType()) {</span>
 142         // The caller expects an oop when incrementally inlining an intrinsic that returns an
 143         // inline type. Make sure the call is re-executed if the allocation triggers a deoptimization.
 144         PreserveReexecuteState preexecs(this);
 145         jvms()-&gt;set_should_reexecute(true);
<span class="line-modified"> 146         res = res-&gt;as_InlineType()-&gt;buffer(this);</span>
 147       }
 148       push_node(bt, res);
 149     }
 150   }
 151 
 152  private:
 153   void fatal_unexpected_iid(vmIntrinsics::ID iid) {
 154     fatal(&quot;unexpected intrinsic %d: %s&quot;, iid, vmIntrinsics::name_at(iid));
 155   }
 156 
 157   void  set_result(Node* n) { assert(_result == NULL, &quot;only set once&quot;); _result = n; }
 158   void  set_result(RegionNode* region, PhiNode* value);
 159   Node*     result() { return _result; }
 160 
 161   virtual int reexecute_sp() { return _reexecute_sp; }
 162 
 163   // Helper functions to inline natives
 164   Node* generate_guard(Node* test, RegionNode* region, float true_prob);
 165   Node* generate_slow_guard(Node* test, RegionNode* region);
 166   Node* generate_fair_guard(Node* test, RegionNode* region);
</pre>
<hr />
<pre>
 185   }
 186   Node* load_array_klass_from_mirror(Node* mirror, bool never_see_null,
 187                                      RegionNode* region, int null_path) {
 188     int offset = java_lang_Class::array_klass_offset();
 189     return load_klass_from_mirror_common(mirror, never_see_null,
 190                                          region, null_path,
 191                                          offset);
 192   }
 193   Node* generate_access_flags_guard(Node* kls,
 194                                     int modifier_mask, int modifier_bits,
 195                                     RegionNode* region);
 196   Node* generate_interface_guard(Node* kls, RegionNode* region);
 197   Node* generate_value_guard(Node* kls, RegionNode* region);
 198 
 199   enum ArrayKind {
 200     AnyArray,
 201     NonArray,
 202     ObjectArray,
 203     NonObjectArray,
 204     TypeArray,
<span class="line-modified"> 205     FlatArray</span>
 206   };
 207 
 208   Node* generate_hidden_class_guard(Node* kls, RegionNode* region);
 209 
 210   Node* generate_array_guard(Node* kls, RegionNode* region) {
 211     return generate_array_guard_common(kls, region, AnyArray);
 212   }
 213   Node* generate_non_array_guard(Node* kls, RegionNode* region) {
 214     return generate_array_guard_common(kls, region, NonArray);
 215   }
 216   Node* generate_objArray_guard(Node* kls, RegionNode* region) {
 217     return generate_array_guard_common(kls, region, ObjectArray);
 218   }
 219   Node* generate_non_objArray_guard(Node* kls, RegionNode* region) {
 220     return generate_array_guard_common(kls, region, NonObjectArray);
 221   }
 222   Node* generate_typeArray_guard(Node* kls, RegionNode* region) {
 223     return generate_array_guard_common(kls, region, TypeArray);
 224   }
<span class="line-modified"> 225   Node* generate_flatArray_guard(Node* kls, RegionNode* region) {</span>
 226     assert(UseFlatArray, &quot;can never be flattened&quot;);
<span class="line-modified"> 227     return generate_array_guard_common(kls, region, FlatArray);</span>
 228   }
 229   Node* generate_array_guard_common(Node* kls, RegionNode* region, ArrayKind kind);
 230   Node* generate_virtual_guard(Node* obj_klass, RegionNode* slow_region);
 231   CallJavaNode* generate_method_call(vmIntrinsics::ID method_id,
 232                                      bool is_virtual = false, bool is_static = false);
 233   CallJavaNode* generate_method_call_static(vmIntrinsics::ID method_id) {
 234     return generate_method_call(method_id, false, true);
 235   }
 236   CallJavaNode* generate_method_call_virtual(vmIntrinsics::ID method_id) {
 237     return generate_method_call(method_id, true, false);
 238   }
 239   Node * load_field_from_object(Node * fromObj, const char * fieldName, const char * fieldTypeString, bool is_exact, bool is_static, ciInstanceKlass * fromKls);
 240   Node * field_address_from_object(Node * fromObj, const char * fieldName, const char * fieldTypeString, bool is_exact, bool is_static, ciInstanceKlass * fromKls);
 241 
 242   Node* make_string_method_node(int opcode, Node* str1_start, Node* cnt1, Node* str2_start, Node* cnt2, StrIntrinsicNode::ArgEnc ae);
 243   bool inline_string_compareTo(StrIntrinsicNode::ArgEnc ae);
 244   bool inline_string_indexOf(StrIntrinsicNode::ArgEnc ae);
 245   bool inline_string_indexOfI(StrIntrinsicNode::ArgEnc ae);
 246   Node* make_indexOf_node(Node* src_start, Node* src_count, Node* tgt_start, Node* tgt_count,
 247                           RegionNode* region, Node* phi, StrIntrinsicNode::ArgEnc ae);
</pre>
<hr />
<pre>
2448   C-&gt;set_has_unsafe_access(true);  // Mark eventual nmethod as &quot;unsafe&quot;.
2449 
2450   Node* receiver = argument(0);  // type: oop
2451 
2452   // Build address expression.
2453   Node* adr;
2454   Node* heap_base_oop = top();
2455   Node* offset = top();
2456   Node* val;
2457 
2458   // The base is either a Java object or a value produced by Unsafe.staticFieldBase
2459   Node* base = argument(1);  // type: oop
2460   // The offset is a value produced by Unsafe.staticFieldOffset or Unsafe.objectFieldOffset
2461   offset = argument(2);  // type: long
2462   // We currently rely on the cookies produced by Unsafe.xxxFieldOffset
2463   // to be plain byte offsets, which are also the same as those accepted
2464   // by oopDesc::field_addr.
2465   assert(Unsafe_field_offset_to_byte_offset(11) == 11,
2466          &quot;fieldOffset must be byte-scaled&quot;);
2467 
<span class="line-modified">2468   ciInlineKlass* inline_klass = NULL;</span>
2469   if (type == T_INLINE_TYPE) {
2470     Node* cls = null_check(argument(4));
2471     if (stopped()) {
2472       return true;
2473     }
2474     Node* kls = load_klass_from_mirror(cls, false, NULL, 0);
2475     const TypeKlassPtr* kls_t = _gvn.type(kls)-&gt;isa_klassptr();
2476     if (!kls_t-&gt;klass_is_exact()) {
2477       return false;
2478     }
2479     ciKlass* klass = kls_t-&gt;klass();
<span class="line-modified">2480     if (!klass-&gt;is_inlinetype()) {</span>
2481       return false;
2482     }
<span class="line-modified">2483     inline_klass = klass-&gt;as_inline_klass();</span>
2484   }
2485 
2486   receiver = null_check(receiver);
2487   if (stopped()) {
2488     return true;
2489   }
2490 
<span class="line-modified">2491   if (base-&gt;is_InlineType()) {</span>
<span class="line-modified">2492     InlineTypeNode* vt = base-&gt;as_InlineType();</span>
2493 
2494     if (is_store) {
<span class="line-modified">2495       if (!vt-&gt;is_allocated(&amp;_gvn) || !_gvn.type(vt)-&gt;is_inlinetype()-&gt;larval()) {</span>
2496         return false;
2497       }
2498       base = vt-&gt;get_oop();
2499     } else {
2500       if (offset-&gt;is_Con()) {
2501         long off = find_long_con(offset, 0);
<span class="line-modified">2502         ciInlineKlass* vk = vt-&gt;type()-&gt;inline_klass();</span>
2503         if ((long)(int)off != off || !vk-&gt;contains_field_offset(off)) {
2504           return false;
2505         }
2506 
2507         ciField* f = vk-&gt;get_non_flattened_field_by_offset((int)off);
2508 
2509         if (f != NULL) {
2510           BasicType bt = f-&gt;layout_type();
2511           if (bt == T_ARRAY || bt == T_NARROWOOP) {
2512             bt = T_OBJECT;
2513           }
2514           if (bt == type) {
<span class="line-modified">2515             if (bt != T_INLINE_TYPE || f-&gt;type() == inline_klass) {</span>
2516               set_result(vt-&gt;field_value_by_offset((int)off, false));
2517               return true;
2518             }
2519           }
2520         }
2521       }
2522       // Re-execute the unsafe access if allocation triggers deoptimization.
2523       PreserveReexecuteState preexecs(this);
2524       jvms()-&gt;set_should_reexecute(true);
2525       base = vt-&gt;buffer(this)-&gt;get_oop();
2526     }
2527   }
2528 
2529   // 32-bit machines ignore the high half!
2530   offset = ConvL2X(offset);
2531   adr = make_unsafe_address(base, offset, is_store ? ACCESS_WRITE : ACCESS_READ, type, kind == Relaxed);
2532 
2533   if (_gvn.type(base)-&gt;isa_ptr() == TypePtr::NULL_PTR) {
<span class="line-modified">2534     if (type != T_OBJECT &amp;&amp; (inline_klass == NULL || !inline_klass-&gt;has_object_fields())) {</span>
2535       decorators |= IN_NATIVE; // off-heap primitive access
2536     } else {
2537       return false; // off-heap oop accesses are not supported
2538     }
2539   } else {
2540     heap_base_oop = base; // on-heap or mixed access
2541   }
2542 
2543   // Can base be NULL? Otherwise, always on-heap access.
2544   bool can_access_non_heap = TypePtr::NULL_PTR-&gt;higher_equal(_gvn.type(base));
2545 
2546   if (!can_access_non_heap) {
2547     decorators |= IN_HEAP;
2548   }
2549 
2550   val = is_store ? argument(4 + (type == T_INLINE_TYPE ? 1 : 0)) : NULL;
2551 
2552   const TypePtr* adr_type = _gvn.type(adr)-&gt;isa_ptr();
2553   if (adr_type == TypePtr::NULL_PTR) {
2554     return false; // off-heap access with zero address
</pre>
<hr />
<pre>
2594     if (bt == T_BYTE &amp;&amp; adr_type-&gt;isa_aryptr()) {
2595       // Alias type doesn&#39;t differentiate between byte[] and boolean[]).
2596       // Use address type to get the element type.
2597       bt = adr_type-&gt;is_aryptr()-&gt;elem()-&gt;array_element_basic_type();
2598     }
2599     if (bt == T_ARRAY || bt == T_NARROWOOP) {
2600       // accessing an array field with getReference is not a mismatch
2601       bt = T_OBJECT;
2602     }
2603     if ((bt == T_OBJECT) != (type == T_OBJECT)) {
2604       // Don&#39;t intrinsify mismatched object accesses
2605       return false;
2606     }
2607     mismatched = (bt != type);
2608   } else if (alias_type-&gt;adr_type()-&gt;isa_oopptr()) {
2609     mismatched = true; // conservatively mark all &quot;wide&quot; on-heap accesses as mismatched
2610   }
2611 
2612   if (type == T_INLINE_TYPE) {
2613     if (adr_type-&gt;isa_instptr()) {
<span class="line-modified">2614       if (field == NULL || field-&gt;type() != inline_klass) {</span>
2615         mismatched = true;
2616       }
2617     } else if (adr_type-&gt;isa_aryptr()) {
2618       const Type* elem = adr_type-&gt;is_aryptr()-&gt;elem();
<span class="line-modified">2619       if (!elem-&gt;isa_inlinetype()) {</span>
2620         mismatched = true;
<span class="line-modified">2621       } else if (elem-&gt;inline_klass() != inline_klass) {</span>
2622         mismatched = true;
2623       }
2624     }
2625     if (is_store) {
2626       const Type* val_t = _gvn.type(val);
<span class="line-modified">2627       if (!val_t-&gt;isa_inlinetype() || val_t-&gt;inline_klass() != inline_klass) {</span>
2628         return false;
2629       }
2630     }
2631   }
2632 
2633   assert(!mismatched || alias_type-&gt;adr_type()-&gt;is_oopptr(), &quot;off-heap access can&#39;t be mismatched&quot;);
2634 
2635   if (mismatched) {
2636     decorators |= C2_MISMATCHED;
2637   }
2638 
2639   // First guess at the value type.
2640   const Type *value_type = Type::get_const_basic_type(type);
2641 
2642   // Figure out the memory ordering.
2643   decorators |= mo_decorator_for_access_kind(kind);
2644 
2645   if (!is_store) {
2646     if (type == T_OBJECT) {
2647       const TypeOopPtr* tjp = sharpen_unsafe_type(alias_type, adr_type);
</pre>
<hr />
<pre>
2655 
2656   // Heap pointers get a null-check from the interpreter,
2657   // as a courtesy.  However, this is not guaranteed by Unsafe,
2658   // and it is not possible to fully distinguish unintended nulls
2659   // from intended ones in this API.
2660 
2661   if (!is_store) {
2662     Node* p = NULL;
2663     // Try to constant fold a load from a constant field
2664 
2665     if (heap_base_oop != top() &amp;&amp; field != NULL &amp;&amp; field-&gt;is_constant() &amp;&amp; !mismatched) {
2666       // final or stable field
2667       p = make_constant_from_field(field, heap_base_oop);
2668     }
2669 
2670     if (p == NULL) { // Could not constant fold the load
2671       if (type == T_INLINE_TYPE) {
2672         if (adr_type-&gt;isa_instptr() &amp;&amp; !mismatched) {
2673           ciInstanceKlass* holder = adr_type-&gt;is_instptr()-&gt;klass()-&gt;as_instance_klass();
2674           int offset = adr_type-&gt;is_instptr()-&gt;offset();
<span class="line-modified">2675           p = InlineTypeNode::make_from_flattened(this, inline_klass, base, base, holder, offset, decorators);</span>
2676         } else {
<span class="line-modified">2677           p = InlineTypeNode::make_from_flattened(this, inline_klass, base, adr, NULL, 0, decorators);</span>
2678         }
2679       } else {
2680         p = access_load_at(heap_base_oop, adr, adr_type, value_type, type, decorators);
2681       }
2682       // Normalize the value returned by getBoolean in the following cases
2683       if (type == T_BOOLEAN &amp;&amp;
2684           (mismatched ||
2685            heap_base_oop == top() ||                  // - heap_base_oop is NULL or
2686            (can_access_non_heap &amp;&amp; field == NULL))    // - heap_base_oop is potentially NULL
2687                                                       //   and the unsafe access is made to large offset
2688                                                       //   (i.e., larger than the maximum offset necessary for any
2689                                                       //   field access)
2690             ) {
2691           IdealKit ideal = IdealKit(this);
2692 #define __ ideal.
2693           IdealVariable normalized_result(ideal);
2694           __ declarations_done();
2695           __ set(normalized_result, p);
2696           __ if_then(p, BoolTest::ne, ideal.ConI(0));
2697           __ set(normalized_result, ideal.ConI(1));
2698           ideal.end_if();
2699           final_sync(ideal);
2700           p = __ value(normalized_result);
2701 #undef __
2702       }
2703     }
2704     if (type == T_ADDRESS) {
2705       p = gvn().transform(new CastP2XNode(NULL, p));
2706       p = ConvX2UL(p);
2707     }
<span class="line-modified">2708     if (field != NULL &amp;&amp; field-&gt;type()-&gt;is_inlinetype() &amp;&amp; !field-&gt;is_flattened()) {</span>
<span class="line-modified">2709       // Load a non-flattened inline type from memory</span>
<span class="line-modified">2710       if (value_type-&gt;inline_klass()-&gt;is_scalarizable()) {</span>
<span class="line-modified">2711         p = InlineTypeNode::make_from_oop(this, p, value_type-&gt;inline_klass());</span>
2712       } else {
<span class="line-modified">2713         p = null2default(p, value_type-&gt;inline_klass());</span>
2714       }
2715     }
2716     // The load node has the control of the preceding MemBarCPUOrder.  All
2717     // following nodes will have the control of the MemBarCPUOrder inserted at
2718     // the end of this method.  So, pushing the load onto the stack at a later
2719     // point is fine.
2720     set_result(p);
2721   } else {
2722     if (bt == T_ADDRESS) {
2723       // Repackage the long as a pointer.
2724       val = ConvL2X(val);
2725       val = gvn().transform(new CastX2PNode(val));
2726     }
2727     if (type == T_INLINE_TYPE) {
2728       if (adr_type-&gt;isa_instptr() &amp;&amp; !mismatched) {
2729         ciInstanceKlass* holder = adr_type-&gt;is_instptr()-&gt;klass()-&gt;as_instance_klass();
2730         int offset = adr_type-&gt;is_instptr()-&gt;offset();
<span class="line-modified">2731         val-&gt;as_InlineType()-&gt;store_flattened(this, base, base, holder, offset, decorators);</span>
2732       } else {
<span class="line-modified">2733         val-&gt;as_InlineType()-&gt;store_flattened(this, base, adr, NULL, 0, decorators);</span>
2734       }
2735     } else {
2736       access_store_at(heap_base_oop, adr, adr_type, val, value_type, type, decorators);
2737     }
2738   }
2739 
<span class="line-modified">2740   if (argument(1)-&gt;is_InlineType() &amp;&amp; is_store) {</span>
<span class="line-modified">2741     Node* value = InlineTypeNode::make_from_oop(this, base, _gvn.type(base)-&gt;inline_klass());</span>
<span class="line-modified">2742     value = value-&gt;as_InlineType()-&gt;make_larval(this, false);</span>
2743     replace_in_map(argument(1), value);
2744   }
2745 
2746   return true;
2747 }
2748 
2749 bool LibraryCallKit::inline_unsafe_make_private_buffer() {
2750   Node* receiver = argument(0);
2751   Node* value = argument(1);
2752 
2753   receiver = null_check(receiver);
2754   if (stopped()) {
2755     return true;
2756   }
2757 
<span class="line-modified">2758   if (!value-&gt;is_InlineType()) {</span>
2759     return false;
2760   }
2761 
<span class="line-modified">2762   set_result(value-&gt;as_InlineType()-&gt;make_larval(this, true));</span>
2763 
2764   return true;
2765 }
2766 
2767 bool LibraryCallKit::inline_unsafe_finish_private_buffer() {
2768   Node* receiver = argument(0);
2769   Node* buffer = argument(1);
2770 
2771   receiver = null_check(receiver);
2772   if (stopped()) {
2773     return true;
2774   }
2775 
<span class="line-modified">2776   if (!buffer-&gt;is_InlineType()) {</span>
2777     return false;
2778   }
2779 
<span class="line-modified">2780   InlineTypeNode* vt = buffer-&gt;as_InlineType();</span>
<span class="line-modified">2781   if (!vt-&gt;is_allocated(&amp;_gvn) || !_gvn.type(vt)-&gt;is_inlinetype()-&gt;larval()) {</span>
2782     return false;
2783   }
2784 
2785   set_result(vt-&gt;finish_larval(this));
2786 
2787   return true;
2788 }
2789 
2790 //----------------------------inline_unsafe_load_store----------------------------
2791 // This method serves a couple of different customers (depending on LoadStoreKind):
2792 //
2793 // LS_cmp_swap:
2794 //
2795 //   boolean compareAndSetReference(Object o, long offset, Object expected, Object x);
2796 //   boolean compareAndSetInt(   Object o, long offset, int    expected, int    x);
2797 //   boolean compareAndSetLong(  Object o, long offset, long   expected, long   x);
2798 //
2799 // LS_cmp_swap_weak:
2800 //
2801 //   boolean weakCompareAndSetReference(       Object o, long offset, Object expected, Object x);
</pre>
<hr />
<pre>
3482   region-&gt;init_req(1, control());
3483 
3484   C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
3485   set_result(region, phi);
3486   return true;
3487 }
3488 
3489 //-------------------------inline_Class_cast-------------------
3490 bool LibraryCallKit::inline_Class_cast() {
3491   Node* mirror = argument(0); // Class
3492   Node* obj    = argument(1);
3493   const TypeInstPtr* mirror_con = _gvn.type(mirror)-&gt;isa_instptr();
3494   if (mirror_con == NULL) {
3495     return false;  // dead path (mirror-&gt;is_top()).
3496   }
3497   if (obj == NULL || obj-&gt;is_top()) {
3498     return false;  // dead path
3499   }
3500   ciKlass* obj_klass = NULL;
3501   const Type* obj_t = _gvn.type(obj);
<span class="line-modified">3502   if (obj-&gt;is_InlineType()) {</span>
<span class="line-modified">3503     obj_klass = obj_t-&gt;inline_klass();</span>
3504   } else if (obj_t-&gt;isa_oopptr()) {
3505     obj_klass = obj_t-&gt;is_oopptr()-&gt;klass();
3506   }
3507 
3508   // First, see if Class.cast() can be folded statically.
3509   // java_mirror_type() returns non-null for compile-time Class constants.
3510   ciType* tm = mirror_con-&gt;java_mirror_type();
3511   if (tm != NULL &amp;&amp; tm-&gt;is_klass() &amp;&amp; obj_klass != NULL) {
3512     if (!obj_klass-&gt;is_loaded()) {
3513       // Don&#39;t use intrinsic when class is not loaded.
3514       return false;
3515     } else {
<span class="line-modified">3516       if (!obj-&gt;is_InlineType() &amp;&amp; tm-&gt;as_klass()-&gt;is_inlinetype()) {</span>
3517         // Casting to .val, check for null
3518         obj = null_check(obj);
3519         if (stopped()) {
3520           return true;
3521         }
3522       }
3523       int static_res = C-&gt;static_subtype_check(tm-&gt;as_klass(), obj_klass);
3524       if (static_res == Compile::SSC_always_true) {
3525         // isInstance() is true - fold the code.
3526         set_result(obj);
3527         return true;
3528       } else if (static_res == Compile::SSC_always_false) {
3529         // Don&#39;t use intrinsic, have to throw ClassCastException.
3530         // If the reference is null, the non-intrinsic bytecode will
3531         // be optimized appropriately.
3532         return false;
3533       }
3534     }
3535   }
3536 
</pre>
<hr />
<pre>
3544   // Do checkcast (Parse::do_checkcast()) optimizations here.
3545 
3546   mirror = null_check(mirror);
3547   // If mirror is dead, only null-path is taken.
3548   if (stopped()) {
3549     return true;
3550   }
3551 
3552   // Not-subtype or the mirror&#39;s klass ptr is NULL (in case it is a primitive).
3553   enum { _bad_type_path = 1, _prim_path = 2, _npe_path = 3, PATH_LIMIT };
3554   RegionNode* region = new RegionNode(PATH_LIMIT);
3555   record_for_igvn(region);
3556 
3557   // Now load the mirror&#39;s klass metaobject, and null-check it.
3558   // If kls is null, we have a primitive mirror and
3559   // nothing is an instance of a primitive type.
3560   Node* kls = load_klass_from_mirror(mirror, false, region, _prim_path);
3561 
3562   Node* res = top();
3563   if (!stopped()) {
<span class="line-modified">3564     if (EnableValhalla &amp;&amp; !obj-&gt;is_InlineType()) {</span>
3565       // Check if we are casting to .val
3566       Node* is_val_kls = generate_value_guard(kls, NULL);
3567       if (is_val_kls != NULL) {
3568         RegionNode* r = new RegionNode(3);
3569         record_for_igvn(r);
3570         r-&gt;init_req(1, control());
3571 
3572         // Casting to .val, check for null
3573         set_control(is_val_kls);
3574         Node* null_ctr = top();
3575         null_check_oop(obj, &amp;null_ctr);
3576         region-&gt;init_req(_npe_path, null_ctr);
3577         r-&gt;init_req(2, control());
3578 
3579         set_control(_gvn.transform(r));
3580       }
3581     }
3582 
3583     Node* bad_type_ctrl = top();
3584     // Do checkcast optimizations.
</pre>
<hr />
<pre>
3708   set_result(_gvn.transform(phi));
3709   return true;
3710 }
3711 
3712 //---------------------generate_array_guard_common------------------------
3713 Node* LibraryCallKit::generate_array_guard_common(Node* kls, RegionNode* region, ArrayKind kind) {
3714 
3715   if (stopped()) {
3716     return NULL;
3717   }
3718 
3719   // Like generate_guard, adds a new path onto the region.
3720   jint  layout_con = 0;
3721   Node* layout_val = get_layout_helper(kls, layout_con);
3722   if (layout_val == NULL) {
3723     bool query = 0;
3724     switch(kind) {
3725       case ObjectArray:    query = Klass::layout_helper_is_objArray(layout_con); break;
3726       case NonObjectArray: query = !Klass::layout_helper_is_objArray(layout_con); break;
3727       case TypeArray:      query = Klass::layout_helper_is_typeArray(layout_con); break;
<span class="line-modified">3728       case FlatArray:      query = Klass::layout_helper_is_flatArray(layout_con); break;</span>
3729       case AnyArray:       query = Klass::layout_helper_is_array(layout_con); break;
3730       case NonArray:       query = !Klass::layout_helper_is_array(layout_con); break;
3731       default:
3732         ShouldNotReachHere();
3733     }
3734     if (!query) {
3735       return NULL;                       // never a branch
3736     } else {                             // always a branch
3737       Node* always_branch = control();
3738       if (region != NULL)
3739         region-&gt;add_req(always_branch);
3740       set_control(top());
3741       return always_branch;
3742     }
3743   }
3744   unsigned int value = 0;
3745   BoolTest::mask btest = BoolTest::illegal;
3746   switch(kind) {
3747     case ObjectArray:
3748     case NonObjectArray: {
3749       value = Klass::_lh_array_tag_obj_value;
3750       layout_val = _gvn.transform(new RShiftINode(layout_val, intcon(Klass::_lh_array_tag_shift)));
3751       btest = kind == ObjectArray ? BoolTest::eq : BoolTest::ne;
3752       break;
3753     }
3754     case TypeArray: {
3755       value = Klass::_lh_array_tag_type_value;
3756       layout_val = _gvn.transform(new RShiftINode(layout_val, intcon(Klass::_lh_array_tag_shift)));
3757       btest = BoolTest::eq;
3758       break;
3759     }
<span class="line-modified">3760     case FlatArray: {</span>
3761       value = Klass::_lh_array_tag_vt_value;
3762       layout_val = _gvn.transform(new RShiftINode(layout_val, intcon(Klass::_lh_array_tag_shift)));
3763       btest = BoolTest::eq;
3764       break;
3765     }
3766     case AnyArray:    value = Klass::_lh_neutral_value; btest = BoolTest::lt; break;
3767     case NonArray:    value = Klass::_lh_neutral_value; btest = BoolTest::gt; break;
3768     default:
3769       ShouldNotReachHere();
3770   }
3771   // Now test the correct condition.
3772   jint nval = (jint)value;
3773   Node* cmp = _gvn.transform(new CmpINode(layout_val, intcon(nval)));
3774   Node* bol = _gvn.transform(new BoolNode(cmp, btest));
3775   return generate_fair_guard(bol, region);
3776 }
3777 
3778 
3779 //-----------------------inline_native_newArray--------------------------
3780 // private static native Object java.lang.reflect.Array.newArray(Class&lt;?&gt; componentType, int length);
</pre>
<hr />
<pre>
3881   return true;
3882 }
3883 
3884 //------------------------inline_array_copyOf----------------------------
3885 // public static &lt;T,U&gt; T[] java.util.Arrays.copyOf(     U[] original, int newLength,         Class&lt;? extends T[]&gt; newType);
3886 // public static &lt;T,U&gt; T[] java.util.Arrays.copyOfRange(U[] original, int from,      int to, Class&lt;? extends T[]&gt; newType);
3887 bool LibraryCallKit::inline_array_copyOf(bool is_copyOfRange) {
3888   if (too_many_traps(Deoptimization::Reason_intrinsic))  return false;
3889 
3890   // Get the arguments.
3891   Node* original          = argument(0);
3892   Node* start             = is_copyOfRange? argument(1): intcon(0);
3893   Node* end               = is_copyOfRange? argument(2): argument(1);
3894   Node* array_type_mirror = is_copyOfRange? argument(3): argument(2);
3895 
3896   const TypeAryPtr* original_t = _gvn.type(original)-&gt;isa_aryptr();
3897   const TypeInstPtr* mirror_t = _gvn.type(array_type_mirror)-&gt;isa_instptr();
3898   if (EnableValhalla &amp;&amp; UseFlatArray &amp;&amp;
3899       (original_t == NULL || mirror_t == NULL ||
3900        (mirror_t-&gt;java_mirror_type() == NULL &amp;&amp;
<span class="line-modified">3901         (original_t-&gt;elem()-&gt;isa_inlinetype() ||</span>
3902          (original_t-&gt;elem()-&gt;make_oopptr() != NULL &amp;&amp;
<span class="line-modified">3903           original_t-&gt;elem()-&gt;make_oopptr()-&gt;can_be_inline_type()))))) {</span>
3904     // We need to know statically if the copy is to a flattened array
3905     // or not but can&#39;t tell.
3906     return false;
3907   }
3908 
3909   Node* newcopy = NULL;
3910 
3911   // Set the original stack and the reexecute bit for the interpreter to reexecute
3912   // the bytecode that invokes Arrays.copyOf if deoptimization happens.
3913   { PreserveReexecuteState preexecs(this);
3914     jvms()-&gt;set_should_reexecute(true);
3915 
3916     array_type_mirror = null_check(array_type_mirror);
3917     original          = null_check(original);
3918 
3919     // Check if a null path was taken unconditionally.
3920     if (stopped())  return true;
3921 
3922     Node* orig_length = load_array_length(original);
3923 
3924     Node* klass_node = load_klass_from_mirror(array_type_mirror, false, NULL, 0);
3925     klass_node = null_check(klass_node);
3926 
3927     RegionNode* bailout = new RegionNode(1);
3928     record_for_igvn(bailout);
3929 
3930     // Despite the generic type of Arrays.copyOf, the mirror might be int, int[], etc.
3931     // Bail out if that is so.
<span class="line-modified">3932     // Inline type array may have object field that would require a</span>
3933     // write barrier. Conservatively, go to slow path.
3934     BarrierSetC2* bs = BarrierSet::barrier_set()-&gt;barrier_set_c2();
3935     Node* not_objArray = !bs-&gt;array_copy_requires_gc_barriers(false, T_OBJECT, false, BarrierSetC2::Parsing) ?
3936         generate_typeArray_guard(klass_node, bailout) : generate_non_objArray_guard(klass_node, bailout);
3937     if (not_objArray != NULL) {
3938       // Improve the klass node&#39;s type from the new optimistic assumption:
3939       ciKlass* ak = ciArrayKlass::make(env()-&gt;Object_klass());
3940       const Type* akls = TypeKlassPtr::make(TypePtr::NotNull, ak, Type::Offset(0), false);
3941       Node* cast = new CastPPNode(klass_node, akls);
3942       cast-&gt;init_req(0, control());
3943       klass_node = _gvn.transform(cast);
3944     }
3945 
3946     Node* original_kls = load_object_klass(original);
3947     // ArrayCopyNode:Ideal may transform the ArrayCopyNode to
3948     // loads/stores but it is legal only if we&#39;re sure the
3949     // Arrays.copyOf would succeed. So we need all input arguments
3950     // to the copyOf to be validated, including that the copy to the
3951     // new array won&#39;t trigger an ArrayStoreException. That subtype
3952     // check can be optimized if we know something on the type of
3953     // the input array from type speculation.
3954     if (_gvn.type(klass_node)-&gt;singleton() &amp;&amp; !stopped()) {
3955       ciKlass* subk   = _gvn.type(original_kls)-&gt;is_klassptr()-&gt;klass();
3956       ciKlass* superk = _gvn.type(klass_node)-&gt;is_klassptr()-&gt;klass();
3957 
3958       int test = C-&gt;static_subtype_check(superk, subk);
3959       if (test != Compile::SSC_always_true &amp;&amp; test != Compile::SSC_always_false) {
3960         const TypeOopPtr* t_original = _gvn.type(original)-&gt;is_oopptr();
3961         if (t_original-&gt;speculative_type() != NULL) {
3962           original = maybe_cast_profiled_obj(original, t_original-&gt;speculative_type(), true);
3963           original_kls = load_object_klass(original);
3964         }
3965       }
3966     }
3967 
3968     if (UseFlatArray) {
3969       // Either both or neither new array klass and original array
3970       // klass must be flattened
<span class="line-modified">3971       Node* is_flat = generate_flatArray_guard(klass_node, NULL);</span>
3972       if (!original_t-&gt;is_not_flat()) {
<span class="line-modified">3973         generate_flatArray_guard(original_kls, bailout);</span>
3974       }
3975       if (is_flat != NULL) {
3976         RegionNode* r = new RegionNode(2);
3977         record_for_igvn(r);
3978         r-&gt;init_req(1, control());
3979         set_control(is_flat);
3980         if (!original_t-&gt;is_not_flat()) {
<span class="line-modified">3981           generate_flatArray_guard(original_kls, r);</span>
3982         }
3983         bailout-&gt;add_req(control());
3984         set_control(_gvn.transform(r));
3985       }
3986     }
3987 
3988     // Bail out if either start or end is negative.
3989     generate_negative_guard(start, bailout, &amp;start);
3990     generate_negative_guard(end,   bailout, &amp;end);
3991 
3992     Node* length = end;
3993     if (_gvn.type(start) != TypeInt::ZERO) {
3994       length = _gvn.transform(new SubINode(end, start));
3995     }
3996 
3997     // Bail out if length is negative.
3998     // Without this the new_array would throw
3999     // NegativeArraySizeException but IllegalArgumentException is what
4000     // should be thrown
4001     generate_negative_guard(length, bailout, &amp;length);
</pre>
<hr />
<pre>
4148 }
4149 
4150 
4151 /**
4152  * Build special case code for calls to hashCode on an object. This call may
4153  * be virtual (invokevirtual) or bound (invokespecial). For each case we generate
4154  * slightly different code.
4155  */
4156 bool LibraryCallKit::inline_native_hashcode(bool is_virtual, bool is_static) {
4157   assert(is_static == callee()-&gt;is_static(), &quot;correct intrinsic selection&quot;);
4158   assert(!(is_virtual &amp;&amp; is_static), &quot;either virtual, special, or static&quot;);
4159 
4160   enum { _slow_path = 1, _fast_path, _null_path, PATH_LIMIT };
4161 
4162   RegionNode* result_reg = new RegionNode(PATH_LIMIT);
4163   PhiNode*    result_val = new PhiNode(result_reg, TypeInt::INT);
4164   PhiNode*    result_io  = new PhiNode(result_reg, Type::ABIO);
4165   PhiNode*    result_mem = new PhiNode(result_reg, Type::MEMORY, TypePtr::BOTTOM);
4166   Node* obj = argument(0);
4167 
<span class="line-modified">4168   if (obj-&gt;is_InlineType() || gvn().type(obj)-&gt;is_inlinetypeptr()) {</span>
4169     return false;
4170   }
4171 
4172   if (!is_static) {
4173     // Check for hashing null object
4174     obj = null_check_receiver();
4175     if (stopped())  return true;        // unconditionally null
4176     result_reg-&gt;init_req(_null_path, top());
4177     result_val-&gt;init_req(_null_path, top());
4178   } else {
4179     // Do a null check, and return zero if null.
4180     // System.identityHashCode(null) == 0
4181     Node* null_ctl = top();
4182     obj = null_check_oop(obj, &amp;null_ctl);
4183     result_reg-&gt;init_req(_null_path, null_ctl);
4184     result_val-&gt;init_req(_null_path, _gvn.intcon(0));
4185   }
4186 
4187   // Unconditionally null?  Then return right away.
4188   if (stopped()) {
</pre>
<hr />
<pre>
4200   // If this is a virtual call, we generate a funny guard.  We pull out
4201   // the vtable entry corresponding to hashCode() from the target object.
4202   // If the target method which we are calling happens to be the native
4203   // Object hashCode() method, we pass the guard.  We do not need this
4204   // guard for non-virtual calls -- the caller is known to be the native
4205   // Object hashCode().
4206   if (is_virtual) {
4207     // After null check, get the object&#39;s klass.
4208     Node* obj_klass = load_object_klass(obj);
4209     generate_virtual_guard(obj_klass, slow_region);
4210   }
4211 
4212   // Get the header out of the object, use LoadMarkNode when available
4213   Node* header_addr = basic_plus_adr(obj, oopDesc::mark_offset_in_bytes());
4214   // The control of the load must be NULL. Otherwise, the load can move before
4215   // the null check after castPP removal.
4216   Node* no_ctrl = NULL;
4217   Node* header = make_load(no_ctrl, header_addr, TypeX_X, TypeX_X-&gt;basic_type(), MemNode::unordered);
4218 
4219   // Test the header to see if it is unlocked.
<span class="line-modified">4220   // This also serves as guard against inline types (they have the always_locked_pattern set).</span>
4221   Node *lock_mask      = _gvn.MakeConX(markWord::biased_lock_mask_in_place);
4222   Node *lmasked_header = _gvn.transform(new AndXNode(header, lock_mask));
4223   Node *unlocked_val   = _gvn.MakeConX(markWord::unlocked_value);
4224   Node *chk_unlocked   = _gvn.transform(new CmpXNode( lmasked_header, unlocked_val));
4225   Node *test_unlocked  = _gvn.transform(new BoolNode( chk_unlocked, BoolTest::ne));
4226 
4227   generate_slow_guard(test_unlocked, slow_region);
4228 
4229   // Get the hash value and check to see that it has been properly assigned.
4230   // We depend on hash_mask being at most 32 bits and avoid the use of
4231   // hash_mask_in_place because it could be larger than 32 bits in a 64-bit
4232   // vm: see markWord.hpp.
4233   Node *hash_mask      = _gvn.intcon(markWord::hash_mask);
4234   Node *hash_shift     = _gvn.intcon(markWord::hash_shift);
4235   Node *hshifted_header= _gvn.transform(new URShiftXNode(header, hash_shift));
4236   // This hack lets the hash bits live anywhere in the mark object now, as long
4237   // as the shift drops the relevant bits into the low 32 bits.  Note that
4238   // Java spec says that HashCode is an int so there&#39;s no point in capturing
4239   // an &#39;X&#39;-sized hashcode (32 in 32-bit build or 64 in 64-bit build).
4240   hshifted_header      = ConvX2I(hshifted_header);
</pre>
<hr />
<pre>
4268     result_reg-&gt;init_req(_slow_path, control());
4269     result_val-&gt;init_req(_slow_path, slow_result);
4270     result_io  -&gt;set_req(_slow_path, i_o());
4271     result_mem -&gt;set_req(_slow_path, reset_memory());
4272   }
4273 
4274   // Return the combined state.
4275   set_i_o(        _gvn.transform(result_io)  );
4276   set_all_memory( _gvn.transform(result_mem));
4277 
4278   set_result(result_reg, result_val);
4279   return true;
4280 }
4281 
4282 //---------------------------inline_native_getClass----------------------------
4283 // public final native Class&lt;?&gt; java.lang.Object.getClass();
4284 //
4285 // Build special case code for calls to getClass on an object.
4286 bool LibraryCallKit::inline_native_getClass() {
4287   Node* obj = argument(0);
<span class="line-modified">4288   if (obj-&gt;is_InlineType()) {</span>
<span class="line-modified">4289     ciKlass* vk = _gvn.type(obj)-&gt;inline_klass();</span>
4290     set_result(makecon(TypeInstPtr::make(vk-&gt;java_mirror())));
4291     return true;
4292   }
4293   obj = null_check_receiver();
4294   if (stopped())  return true;
4295   set_result(load_mirror_from_klass(load_object_klass(obj)));
4296   return true;
4297 }
4298 
4299 //-----------------inline_native_Reflection_getCallerClass---------------------
4300 // public static native Class&lt;?&gt; sun.reflect.Reflection.getCallerClass();
4301 //
4302 // In the presence of deep enough inlining, getCallerClass() becomes a no-op.
4303 //
4304 // NOTE: This code must perform the same logic as JVM_GetCallerClass
4305 // in that it must skip particular security frames and checks for
4306 // caller sensitive methods.
4307 bool LibraryCallKit::inline_native_Reflection_getCallerClass() {
4308 #ifndef PRODUCT
4309   if ((C-&gt;print_intrinsics() || C-&gt;print_inlining()) &amp;&amp; Verbose) {
</pre>
<hr />
<pre>
4586 //
4587 // The general case has two steps, allocation and copying.
4588 // Allocation has two cases, and uses GraphKit::new_instance or new_array.
4589 //
4590 // Copying also has two cases, oop arrays and everything else.
4591 // Oop arrays use arrayof_oop_arraycopy (same as System.arraycopy).
4592 // Everything else uses the tight inline loop supplied by CopyArrayNode.
4593 //
4594 // These steps fold up nicely if and when the cloned object&#39;s klass
4595 // can be sharply typed as an object array, a type array, or an instance.
4596 //
4597 bool LibraryCallKit::inline_native_clone(bool is_virtual) {
4598   PhiNode* result_val;
4599 
4600   // Set the reexecute bit for the interpreter to reexecute
4601   // the bytecode that invokes Object.clone if deoptimization happens.
4602   { PreserveReexecuteState preexecs(this);
4603     jvms()-&gt;set_should_reexecute(true);
4604 
4605     Node* obj = argument(0);
<span class="line-modified">4606     if (obj-&gt;is_InlineType()) {</span>
4607       return false;
4608     }
4609 
4610     obj = null_check_receiver();
4611     if (stopped())  return true;
4612 
4613     const TypeOopPtr* obj_type = _gvn.type(obj)-&gt;is_oopptr();
4614 
4615     // If we are going to clone an instance, we need its exact type to
4616     // know the number and types of fields to convert the clone to
4617     // loads/stores. Maybe a speculative type can help us.
4618     if (!obj_type-&gt;klass_is_exact() &amp;&amp;
4619         obj_type-&gt;speculative_type() != NULL &amp;&amp;
4620         obj_type-&gt;speculative_type()-&gt;is_instance_klass() &amp;&amp;
<span class="line-modified">4621         !obj_type-&gt;speculative_type()-&gt;is_inlinetype()) {</span>
4622       ciInstanceKlass* spec_ik = obj_type-&gt;speculative_type()-&gt;as_instance_klass();
4623       if (spec_ik-&gt;nof_nonstatic_fields() &lt;= ArrayCopyLoadStoreMaxElem &amp;&amp;
4624           !spec_ik-&gt;has_injected_fields()) {
4625         ciKlass* k = obj_type-&gt;klass();
4626         if (!k-&gt;is_instance_klass() ||
4627             k-&gt;as_instance_klass()-&gt;is_interface() ||
4628             k-&gt;as_instance_klass()-&gt;has_subklass()) {
4629           obj = maybe_cast_profiled_obj(obj, obj_type-&gt;speculative_type(), false);
4630         }
4631       }
4632     }
4633 
4634     // Conservatively insert a memory barrier on all memory slices.
4635     // Do not let writes into the original float below the clone.
4636     insert_mem_bar(Op_MemBarCPUOrder);
4637 
4638     // paths into result_reg:
4639     enum {
4640       _slow_path = 1,     // out-of-line call to clone method (virtual or not)
4641       _objArray_path,     // plain array allocation, plus arrayof_oop_arraycopy
</pre>
<hr />
<pre>
4647     result_val             = new PhiNode(result_reg, TypeInstPtr::NOTNULL);
4648     PhiNode*    result_i_o = new PhiNode(result_reg, Type::ABIO);
4649     PhiNode*    result_mem = new PhiNode(result_reg, Type::MEMORY, TypePtr::BOTTOM);
4650     record_for_igvn(result_reg);
4651 
4652     Node* obj_klass = load_object_klass(obj);
4653     // We only go to the fast case code if we pass a number of guards.
4654     // The paths which do not pass are accumulated in the slow_region.
4655     RegionNode* slow_region = new RegionNode(1);
4656     record_for_igvn(slow_region);
4657 
4658     Node* array_ctl = generate_array_guard(obj_klass, (RegionNode*)NULL);
4659     if (array_ctl != NULL) {
4660       // It&#39;s an array.
4661       PreserveJVMState pjvms(this);
4662       set_control(array_ctl);
4663 
4664       BarrierSetC2* bs = BarrierSet::barrier_set()-&gt;barrier_set_c2();
4665       if (bs-&gt;array_copy_requires_gc_barriers(true, T_OBJECT, true, BarrierSetC2::Parsing) &amp;&amp;
4666           (!obj_type-&gt;isa_aryptr() || !obj_type-&gt;is_aryptr()-&gt;is_not_flat())) {
<span class="line-modified">4667         // Flattened inline type array may have object field that would require a</span>
4668         // write barrier. Conservatively, go to slow path.
<span class="line-modified">4669         generate_flatArray_guard(obj_klass, slow_region);</span>
4670       }
4671 
4672       if (!stopped()) {
4673         Node* obj_length = load_array_length(obj);
4674         Node* obj_size  = NULL;
4675         Node* alloc_obj = new_array(obj_klass, obj_length, 0, &amp;obj_size, /*deoptimize_on_exception=*/true);
4676 
4677         BarrierSetC2* bs = BarrierSet::barrier_set()-&gt;barrier_set_c2();
4678         if (bs-&gt;array_copy_requires_gc_barriers(true, T_OBJECT, true, BarrierSetC2::Parsing)) {
4679           // If it is an oop array, it requires very special treatment,
4680           // because gc barriers are required when accessing the array.
4681           Node* is_obja = generate_objArray_guard(obj_klass, (RegionNode*)NULL);
4682           if (is_obja != NULL) {
4683             PreserveJVMState pjvms2(this);
4684             set_control(is_obja);
4685             // Generate a direct call to the right arraycopy function(s).
4686             Node* alloc = tightly_coupled_allocation(alloc_obj, NULL);
4687             ArrayCopyNode* ac = ArrayCopyNode::make(this, true, obj, intcon(0), alloc_obj, intcon(0), obj_length, alloc != NULL, false);
4688             ac-&gt;set_clone_oop_array();
4689             Node* n = _gvn.transform(ac);
</pre>
<hr />
<pre>
5126     // (9) each element of an oop array must be assignable
5127     Node* dest_klass = load_object_klass(dest);
5128     if (src != dest) {
5129       Node* not_subtype_ctrl = gen_subtype_check(src, dest_klass);
5130 
5131       if (not_subtype_ctrl != top()) {
5132         PreserveJVMState pjvms(this);
5133         set_control(not_subtype_ctrl);
5134         uncommon_trap(Deoptimization::Reason_intrinsic,
5135                       Deoptimization::Action_make_not_entrant);
5136         assert(stopped(), &quot;Should be stopped&quot;);
5137       }
5138     }
5139 
5140     const TypeKlassPtr* dest_klass_t = _gvn.type(dest_klass)-&gt;is_klassptr();
5141     const Type* toop = TypeOopPtr::make_from_klass(dest_klass_t-&gt;klass());
5142     src = _gvn.transform(new CheckCastPPNode(control(), src, toop));
5143     src_type = _gvn.type(src);
5144     top_src  = src_type-&gt;isa_aryptr();
5145 
<span class="line-modified">5146     if (top_dest != NULL &amp;&amp; !top_dest-&gt;elem()-&gt;isa_inlinetype() &amp;&amp; !top_dest-&gt;is_not_flat()) {</span>
<span class="line-modified">5147       generate_flatArray_guard(dest_klass, slow_region);</span>
5148     }
5149 
<span class="line-modified">5150     if (top_src != NULL &amp;&amp; !top_src-&gt;elem()-&gt;isa_inlinetype() &amp;&amp; !top_src-&gt;is_not_flat()) {</span>
5151       Node* src_klass = load_object_klass(src);
<span class="line-modified">5152       generate_flatArray_guard(src_klass, slow_region);</span>
5153     }
5154 
5155     {
5156       PreserveJVMState pjvms(this);
5157       set_control(_gvn.transform(slow_region));
5158       uncommon_trap(Deoptimization::Reason_intrinsic,
5159                     Deoptimization::Action_make_not_entrant);
5160       assert(stopped(), &quot;Should be stopped&quot;);
5161     }
5162   }
5163 
5164   arraycopy_move_allocation_here(alloc, dest, saved_jvms, saved_reexecute_sp, new_idx);
5165 
5166   if (stopped()) {
5167     return true;
5168   }
5169 
5170   ArrayCopyNode* ac = ArrayCopyNode::make(this, true, src, src_offset, dest, dest_offset, length, alloc != NULL, negative_length_guard_generated,
5171                                           // Create LoadRange and LoadKlass nodes for use during macro expansion here
5172                                           // so the compiler has a chance to eliminate them: during macro expansion,
</pre>
</td>
</tr>
</table>
<center><a href="graphKit.hpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="locknode.cpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>