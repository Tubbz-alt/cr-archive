<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Frames src/hotspot/cpu/aarch64/c1_Runtime1_aarch64.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
    <script type="text/javascript" src="../../../../navigation.js"></script>
  </head>
<body onkeypress="keypress(event);">
<a name="0"></a>
<hr />
<pre>   1 /*
   2  * Copyright (c) 1999, 2020, Oracle and/or its affiliates. All rights reserved.
   3  * Copyright (c) 2014, Red Hat Inc. All rights reserved.
   4  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   5  *
   6  * This code is free software; you can redistribute it and/or modify it
   7  * under the terms of the GNU General Public License version 2 only, as
   8  * published by the Free Software Foundation.
   9  *
  10  * This code is distributed in the hope that it will be useful, but WITHOUT
  11  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  12  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  13  * version 2 for more details (a copy is included in the LICENSE file that
  14  * accompanied this code).
  15  *
  16  * You should have received a copy of the GNU General Public License version
  17  * 2 along with this work; if not, write to the Free Software Foundation,
  18  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  19  *
  20  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  21  * or visit www.oracle.com if you need additional information or have any
  22  * questions.
  23  *
  24  */
  25 
  26 #include &quot;precompiled.hpp&quot;
  27 #include &quot;asm/assembler.hpp&quot;
  28 #include &quot;c1/c1_CodeStubs.hpp&quot;
  29 #include &quot;c1/c1_Defs.hpp&quot;
  30 #include &quot;c1/c1_MacroAssembler.hpp&quot;
  31 #include &quot;c1/c1_Runtime1.hpp&quot;
  32 #include &quot;compiler/disassembler.hpp&quot;
  33 #include &quot;gc/shared/cardTable.hpp&quot;
  34 #include &quot;gc/shared/cardTableBarrierSet.hpp&quot;
  35 #include &quot;interpreter/interpreter.hpp&quot;
  36 #include &quot;memory/universe.hpp&quot;
  37 #include &quot;nativeInst_aarch64.hpp&quot;
  38 #include &quot;oops/compiledICHolder.hpp&quot;
  39 #include &quot;oops/oop.inline.hpp&quot;
  40 #include &quot;prims/jvmtiExport.hpp&quot;
  41 #include &quot;register_aarch64.hpp&quot;
  42 #include &quot;runtime/sharedRuntime.hpp&quot;
  43 #include &quot;runtime/signature.hpp&quot;
  44 #include &quot;runtime/vframe.hpp&quot;
  45 #include &quot;runtime/vframeArray.hpp&quot;
  46 #include &quot;utilities/powerOfTwo.hpp&quot;
  47 #include &quot;vmreg_aarch64.inline.hpp&quot;
  48 
  49 
  50 // Implementation of StubAssembler
  51 
  52 int StubAssembler::call_RT(Register oop_result1, Register metadata_result, address entry, int args_size) {
  53   // setup registers
  54   assert(!(oop_result1-&gt;is_valid() || metadata_result-&gt;is_valid()) || oop_result1 != metadata_result, &quot;registers must be different&quot;);
  55   assert(oop_result1 != rthread &amp;&amp; metadata_result != rthread, &quot;registers must be different&quot;);
  56   assert(args_size &gt;= 0, &quot;illegal args_size&quot;);
  57   bool align_stack = false;
  58 
  59   mov(c_rarg0, rthread);
  60   set_num_rt_args(0); // Nothing on stack
  61 
  62   Label retaddr;
  63   set_last_Java_frame(sp, rfp, retaddr, rscratch1);
  64 
  65   // do the call
  66   lea(rscratch1, RuntimeAddress(entry));
  67   blr(rscratch1);
  68   bind(retaddr);
  69   int call_offset = offset();
  70   // verify callee-saved register
  71 #ifdef ASSERT
  72   push(r0, sp);
  73   { Label L;
  74     get_thread(r0);
  75     cmp(rthread, r0);
  76     br(Assembler::EQ, L);
  77     stop(&quot;StubAssembler::call_RT: rthread not callee saved?&quot;);
  78     bind(L);
  79   }
  80   pop(r0, sp);
  81 #endif
  82   reset_last_Java_frame(true);
  83   maybe_isb();
  84 
  85   // check for pending exceptions
  86   { Label L;
  87     // check for pending exceptions (java_thread is set upon return)
  88     ldr(rscratch1, Address(rthread, in_bytes(Thread::pending_exception_offset())));
  89     cbz(rscratch1, L);
  90     // exception pending =&gt; remove activation and forward to exception handler
  91     // make sure that the vm_results are cleared
  92     if (oop_result1-&gt;is_valid()) {
  93       str(zr, Address(rthread, JavaThread::vm_result_offset()));
  94     }
  95     if (metadata_result-&gt;is_valid()) {
  96       str(zr, Address(rthread, JavaThread::vm_result_2_offset()));
  97     }
  98     if (frame_size() == no_frame_size) {
  99       leave();
 100       far_jump(RuntimeAddress(StubRoutines::forward_exception_entry()));
 101     } else if (_stub_id == Runtime1::forward_exception_id) {
 102       should_not_reach_here();
 103     } else {
 104       far_jump(RuntimeAddress(Runtime1::entry_for(Runtime1::forward_exception_id)));
 105     }
 106     bind(L);
 107   }
 108   // get oop results if there are any and reset the values in the thread
 109   if (oop_result1-&gt;is_valid()) {
 110     get_vm_result(oop_result1, rthread);
 111   }
 112   if (metadata_result-&gt;is_valid()) {
 113     get_vm_result_2(metadata_result, rthread);
 114   }
 115   return call_offset;
 116 }
 117 
 118 
 119 int StubAssembler::call_RT(Register oop_result1, Register metadata_result, address entry, Register arg1) {
 120   mov(c_rarg1, arg1);
 121   return call_RT(oop_result1, metadata_result, entry, 1);
 122 }
 123 
 124 
 125 int StubAssembler::call_RT(Register oop_result1, Register metadata_result, address entry, Register arg1, Register arg2) {
 126   if (c_rarg1 == arg2) {
 127     if (c_rarg2 == arg1) {
 128       mov(rscratch1, arg1);
 129       mov(arg1, arg2);
 130       mov(arg2, rscratch1);
 131     } else {
 132       mov(c_rarg2, arg2);
 133       mov(c_rarg1, arg1);
 134     }
 135   } else {
 136     mov(c_rarg1, arg1);
 137     mov(c_rarg2, arg2);
 138   }
 139   return call_RT(oop_result1, metadata_result, entry, 2);
 140 }
 141 
 142 
 143 int StubAssembler::call_RT(Register oop_result1, Register metadata_result, address entry, Register arg1, Register arg2, Register arg3) {
 144   // if there is any conflict use the stack
 145   if (arg1 == c_rarg2 || arg1 == c_rarg3 ||
 146       arg2 == c_rarg1 || arg1 == c_rarg3 ||
 147       arg3 == c_rarg1 || arg1 == c_rarg2) {
 148     stp(arg3, arg2, Address(pre(sp, 2 * wordSize)));
 149     stp(arg1, zr, Address(pre(sp, -2 * wordSize)));
 150     ldp(c_rarg1, zr, Address(post(sp, 2 * wordSize)));
 151     ldp(c_rarg3, c_rarg2, Address(post(sp, 2 * wordSize)));
 152   } else {
 153     mov(c_rarg1, arg1);
 154     mov(c_rarg2, arg2);
 155     mov(c_rarg3, arg3);
 156   }
 157   return call_RT(oop_result1, metadata_result, entry, 3);
 158 }
 159 
 160 // Implementation of StubFrame
 161 
 162 class StubFrame: public StackObj {
 163  private:
 164   StubAssembler* _sasm;
 165 
 166  public:
 167   StubFrame(StubAssembler* sasm, const char* name, bool must_gc_arguments);
 168   void load_argument(int offset_in_words, Register reg);
 169 
 170   ~StubFrame();
 171 };;
 172 
 173 void StubAssembler::prologue(const char* name, bool must_gc_arguments) {
 174   set_info(name, must_gc_arguments);
 175   enter();
 176 }
 177 
 178 void StubAssembler::epilogue() {
 179   leave();
 180   ret(lr);
 181 }
 182 
 183 #define __ _sasm-&gt;
 184 
 185 StubFrame::StubFrame(StubAssembler* sasm, const char* name, bool must_gc_arguments) {
 186   _sasm = sasm;
 187   __ prologue(name, must_gc_arguments);
 188 }
 189 
 190 // load parameters that were stored with LIR_Assembler::store_parameter
 191 // Note: offsets for store_parameter and load_argument must match
 192 void StubFrame::load_argument(int offset_in_words, Register reg) {
 193   __ load_parameter(offset_in_words, reg);
 194 }
 195 
 196 
 197 StubFrame::~StubFrame() {
 198   __ epilogue();
 199 }
 200 
 201 #undef __
 202 
 203 
 204 // Implementation of Runtime1
 205 
 206 #define __ sasm-&gt;
 207 
 208 const int float_regs_as_doubles_size_in_slots = pd_nof_fpu_regs_frame_map * 2;
 209 
 210 // Stack layout for saving/restoring  all the registers needed during a runtime
 211 // call (this includes deoptimization)
 212 // Note: note that users of this frame may well have arguments to some runtime
 213 // while these values are on the stack. These positions neglect those arguments
 214 // but the code in save_live_registers will take the argument count into
 215 // account.
 216 //
 217 
 218 enum reg_save_layout {
 219   reg_save_frame_size = 32 /* float */ + 32 /* integer */
 220 };
 221 
 222 // Save off registers which might be killed by calls into the runtime.
 223 // Tries to smart of about FP registers.  In particular we separate
 224 // saving and describing the FPU registers for deoptimization since we
 225 // have to save the FPU registers twice if we describe them.  The
 226 // deopt blob is the only thing which needs to describe FPU registers.
 227 // In all other cases it should be sufficient to simply save their
 228 // current value.
 229 
 230 static int cpu_reg_save_offsets[FrameMap::nof_cpu_regs];
 231 static int fpu_reg_save_offsets[FrameMap::nof_fpu_regs];
 232 static int reg_save_size_in_words;
 233 static int frame_size_in_bytes = -1;
 234 
 235 static OopMap* generate_oop_map(StubAssembler* sasm, bool save_fpu_registers) {
 236   int frame_size_in_bytes = reg_save_frame_size * BytesPerWord;
 237   sasm-&gt;set_frame_size(frame_size_in_bytes / BytesPerWord);
 238   int frame_size_in_slots = frame_size_in_bytes / sizeof(jint);
 239   OopMap* oop_map = new OopMap(frame_size_in_slots, 0);
 240 
 241   for (int i = 0; i &lt; FrameMap::nof_cpu_regs; i++) {
 242     Register r = as_Register(i);
 243     if (i &lt;= 18 &amp;&amp; i != rscratch1-&gt;encoding() &amp;&amp; i != rscratch2-&gt;encoding()) {
 244       int sp_offset = cpu_reg_save_offsets[i];
 245       oop_map-&gt;set_callee_saved(VMRegImpl::stack2reg(sp_offset),
 246                                 r-&gt;as_VMReg());
 247     }
 248   }
 249 
 250   if (save_fpu_registers) {
 251     for (int i = 0; i &lt; FrameMap::nof_fpu_regs; i++) {
 252       FloatRegister r = as_FloatRegister(i);
 253       {
 254         int sp_offset = fpu_reg_save_offsets[i];
 255         oop_map-&gt;set_callee_saved(VMRegImpl::stack2reg(sp_offset),
 256                                   r-&gt;as_VMReg());
 257       }
 258     }
 259   }
 260   return oop_map;
 261 }
 262 
 263 static OopMap* save_live_registers(StubAssembler* sasm,
 264                                    bool save_fpu_registers = true) {
 265   __ block_comment(&quot;save_live_registers&quot;);
 266 
 267   __ push(RegSet::range(r0, r29), sp);         // integer registers except lr &amp; sp
 268 
 269   if (save_fpu_registers) {
 270     for (int i = 31; i&gt;= 0; i -= 4) {
 271       __ sub(sp, sp, 4 * wordSize); // no pre-increment for st1. Emulate it without modifying other registers
 272       __ st1(as_FloatRegister(i-3), as_FloatRegister(i-2), as_FloatRegister(i-1),
 273           as_FloatRegister(i), __ T1D, Address(sp));
 274     }
 275   } else {
 276     __ add(sp, sp, -32 * wordSize);
 277   }
 278 
 279   return generate_oop_map(sasm, save_fpu_registers);
 280 }
 281 
 282 static void restore_live_registers(StubAssembler* sasm, bool restore_fpu_registers = true) {
 283   if (restore_fpu_registers) {
 284     for (int i = 0; i &lt; 32; i += 4)
 285       __ ld1(as_FloatRegister(i), as_FloatRegister(i+1), as_FloatRegister(i+2),
 286           as_FloatRegister(i+3), __ T1D, Address(__ post(sp, 4 * wordSize)));
 287   } else {
 288     __ add(sp, sp, 32 * wordSize);
 289   }
 290 
 291   __ pop(RegSet::range(r0, r29), sp);
 292 }
 293 
 294 static void restore_live_registers_except_r0(StubAssembler* sasm, bool restore_fpu_registers = true)  {
 295 
 296   if (restore_fpu_registers) {
 297     for (int i = 0; i &lt; 32; i += 4)
 298       __ ld1(as_FloatRegister(i), as_FloatRegister(i+1), as_FloatRegister(i+2),
 299           as_FloatRegister(i+3), __ T1D, Address(__ post(sp, 4 * wordSize)));
 300   } else {
 301     __ add(sp, sp, 32 * wordSize);
 302   }
 303 
 304   __ ldp(zr, r1, Address(__ post(sp, 16)));
 305   __ pop(RegSet::range(r2, r29), sp);
 306 }
 307 
 308 
 309 
 310 void Runtime1::initialize_pd() {
 311   int i;
 312   int sp_offset = 0;
 313 
 314   // all float registers are saved explicitly
 315   assert(FrameMap::nof_fpu_regs == 32, &quot;double registers not handled here&quot;);
 316   for (i = 0; i &lt; FrameMap::nof_fpu_regs; i++) {
 317     fpu_reg_save_offsets[i] = sp_offset;
 318     sp_offset += 2;   // SP offsets are in halfwords
 319   }
 320 
 321   for (i = 0; i &lt; FrameMap::nof_cpu_regs; i++) {
 322     Register r = as_Register(i);
 323     cpu_reg_save_offsets[i] = sp_offset;
 324     sp_offset += 2;   // SP offsets are in halfwords
 325   }
 326 }
 327 
 328 
 329 // target: the entry point of the method that creates and posts the exception oop
 330 // has_argument: true if the exception needs arguments (passed in rscratch1 and rscratch2)
 331 
 332 OopMapSet* Runtime1::generate_exception_throw(StubAssembler* sasm, address target, bool has_argument) {
 333   // make a frame and preserve the caller&#39;s caller-save registers
 334   OopMap* oop_map = save_live_registers(sasm);
 335   int call_offset;
 336   if (!has_argument) {
 337     call_offset = __ call_RT(noreg, noreg, target);
 338   } else {
 339     __ mov(c_rarg1, rscratch1);
 340     __ mov(c_rarg2, rscratch2);
 341     call_offset = __ call_RT(noreg, noreg, target);
 342   }
 343   OopMapSet* oop_maps = new OopMapSet();
 344   oop_maps-&gt;add_gc_map(call_offset, oop_map);
 345 
 346   __ should_not_reach_here();
 347   return oop_maps;
 348 }
 349 
 350 
 351 OopMapSet* Runtime1::generate_handle_exception(StubID id, StubAssembler *sasm) {
 352   __ block_comment(&quot;generate_handle_exception&quot;);
 353 
 354   // incoming parameters
 355   const Register exception_oop = r0;
 356   const Register exception_pc  = r3;
 357   // other registers used in this stub
 358 
 359   // Save registers, if required.
 360   OopMapSet* oop_maps = new OopMapSet();
 361   OopMap* oop_map = NULL;
 362   switch (id) {
 363   case forward_exception_id:
 364     // We&#39;re handling an exception in the context of a compiled frame.
 365     // The registers have been saved in the standard places.  Perform
 366     // an exception lookup in the caller and dispatch to the handler
 367     // if found.  Otherwise unwind and dispatch to the callers
 368     // exception handler.
 369     oop_map = generate_oop_map(sasm, 1 /*thread*/);
 370 
 371     // load and clear pending exception oop into r0
 372     __ ldr(exception_oop, Address(rthread, Thread::pending_exception_offset()));
 373     __ str(zr, Address(rthread, Thread::pending_exception_offset()));
 374 
 375     // load issuing PC (the return address for this stub) into r3
 376     __ ldr(exception_pc, Address(rfp, 1*BytesPerWord));
 377 
 378     // make sure that the vm_results are cleared (may be unnecessary)
 379     __ str(zr, Address(rthread, JavaThread::vm_result_offset()));
 380     __ str(zr, Address(rthread, JavaThread::vm_result_2_offset()));
 381     break;
 382   case handle_exception_nofpu_id:
 383   case handle_exception_id:
 384     // At this point all registers MAY be live.
 385     oop_map = save_live_registers(sasm, id != handle_exception_nofpu_id);
 386     break;
 387   case handle_exception_from_callee_id: {
 388     // At this point all registers except exception oop (r0) and
 389     // exception pc (lr) are dead.
 390     const int frame_size = 2 /*fp, return address*/;
 391     oop_map = new OopMap(frame_size * VMRegImpl::slots_per_word, 0);
 392     sasm-&gt;set_frame_size(frame_size);
 393     break;
 394   }
 395   default:
 396     __ should_not_reach_here();
 397     break;
 398   }
 399 
 400   // verify that only r0 and r3 are valid at this time
 401   __ invalidate_registers(false, true, true, false, true, true);
 402   // verify that r0 contains a valid exception
 403   __ verify_not_null_oop(exception_oop);
 404 
 405 #ifdef ASSERT
 406   // check that fields in JavaThread for exception oop and issuing pc are
 407   // empty before writing to them
 408   Label oop_empty;
 409   __ ldr(rscratch1, Address(rthread, JavaThread::exception_oop_offset()));
 410   __ cbz(rscratch1, oop_empty);
 411   __ stop(&quot;exception oop already set&quot;);
 412   __ bind(oop_empty);
 413 
 414   Label pc_empty;
 415   __ ldr(rscratch1, Address(rthread, JavaThread::exception_pc_offset()));
 416   __ cbz(rscratch1, pc_empty);
 417   __ stop(&quot;exception pc already set&quot;);
 418   __ bind(pc_empty);
 419 #endif
 420 
 421   // save exception oop and issuing pc into JavaThread
 422   // (exception handler will load it from here)
 423   __ str(exception_oop, Address(rthread, JavaThread::exception_oop_offset()));
 424   __ str(exception_pc, Address(rthread, JavaThread::exception_pc_offset()));
 425 
 426   // patch throwing pc into return address (has bci &amp; oop map)
 427   __ str(exception_pc, Address(rfp, 1*BytesPerWord));
 428 
 429   // compute the exception handler.
 430   // the exception oop and the throwing pc are read from the fields in JavaThread
 431   int call_offset = __ call_RT(noreg, noreg, CAST_FROM_FN_PTR(address, exception_handler_for_pc));
 432   oop_maps-&gt;add_gc_map(call_offset, oop_map);
 433 
 434   // r0: handler address
 435   //      will be the deopt blob if nmethod was deoptimized while we looked up
 436   //      handler regardless of whether handler existed in the nmethod.
 437 
 438   // only r0 is valid at this time, all other registers have been destroyed by the runtime call
 439   __ invalidate_registers(false, true, true, true, true, true);
 440 
 441   // patch the return address, this stub will directly return to the exception handler
 442   __ str(r0, Address(rfp, 1*BytesPerWord));
 443 
 444   switch (id) {
 445   case forward_exception_id:
 446   case handle_exception_nofpu_id:
 447   case handle_exception_id:
 448     // Restore the registers that were saved at the beginning.
 449     restore_live_registers(sasm, id != handle_exception_nofpu_id);
 450     break;
 451   case handle_exception_from_callee_id:
 452     // Pop the return address.
 453     __ leave();
 454     __ ret(lr);  // jump to exception handler
 455     break;
 456   default:  ShouldNotReachHere();
 457   }
 458 
 459   return oop_maps;
 460 }
 461 
 462 
 463 void Runtime1::generate_unwind_exception(StubAssembler *sasm) {
 464   // incoming parameters
 465   const Register exception_oop = r0;
 466   // callee-saved copy of exception_oop during runtime call
 467   const Register exception_oop_callee_saved = r19;
 468   // other registers used in this stub
 469   const Register exception_pc = r3;
 470   const Register handler_addr = r1;
 471 
 472   // verify that only r0, is valid at this time
 473   __ invalidate_registers(false, true, true, true, true, true);
 474 
 475 #ifdef ASSERT
 476   // check that fields in JavaThread for exception oop and issuing pc are empty
 477   Label oop_empty;
 478   __ ldr(rscratch1, Address(rthread, JavaThread::exception_oop_offset()));
 479   __ cbz(rscratch1, oop_empty);
 480   __ stop(&quot;exception oop must be empty&quot;);
 481   __ bind(oop_empty);
 482 
 483   Label pc_empty;
 484   __ ldr(rscratch1, Address(rthread, JavaThread::exception_pc_offset()));
 485   __ cbz(rscratch1, pc_empty);
 486   __ stop(&quot;exception pc must be empty&quot;);
 487   __ bind(pc_empty);
 488 #endif
 489 
 490   // Save our return address because
 491   // exception_handler_for_return_address will destroy it.  We also
 492   // save exception_oop
 493   __ stp(lr, exception_oop, Address(__ pre(sp, -2 * wordSize)));
 494 
 495   // search the exception handler address of the caller (using the return address)
 496   __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::exception_handler_for_return_address), rthread, lr);
 497   // r0: exception handler address of the caller
 498 
 499   // Only R0 is valid at this time; all other registers have been
 500   // destroyed by the call.
 501   __ invalidate_registers(false, true, true, true, false, true);
 502 
 503   // move result of call into correct register
 504   __ mov(handler_addr, r0);
 505 
 506   // get throwing pc (= return address).
 507   // lr has been destroyed by the call
 508   __ ldp(lr, exception_oop, Address(__ post(sp, 2 * wordSize)));
 509   __ mov(r3, lr);
 510 
 511   __ verify_not_null_oop(exception_oop);
 512 
 513   // continue at exception handler (return address removed)
 514   // note: do *not* remove arguments when unwinding the
 515   //       activation since the caller assumes having
 516   //       all arguments on the stack when entering the
 517   //       runtime to determine the exception handler
 518   //       (GC happens at call site with arguments!)
 519   // r0: exception oop
 520   // r3: throwing pc
 521   // r1: exception handler
 522   __ br(handler_addr);
 523 }
 524 
 525 
 526 
 527 OopMapSet* Runtime1::generate_patching(StubAssembler* sasm, address target) {
 528   // use the maximum number of runtime-arguments here because it is difficult to
 529   // distinguish each RT-Call.
 530   // Note: This number affects also the RT-Call in generate_handle_exception because
 531   //       the oop-map is shared for all calls.
 532   DeoptimizationBlob* deopt_blob = SharedRuntime::deopt_blob();
 533   assert(deopt_blob != NULL, &quot;deoptimization blob must have been created&quot;);
 534 
 535   OopMap* oop_map = save_live_registers(sasm);
 536 
 537   __ mov(c_rarg0, rthread);
 538   Label retaddr;
 539   __ set_last_Java_frame(sp, rfp, retaddr, rscratch1);
 540   // do the call
 541   __ lea(rscratch1, RuntimeAddress(target));
 542   __ blr(rscratch1);
 543   __ bind(retaddr);
 544   OopMapSet* oop_maps = new OopMapSet();
 545   oop_maps-&gt;add_gc_map(__ offset(), oop_map);
 546   // verify callee-saved register
 547 #ifdef ASSERT
 548   { Label L;
 549     __ get_thread(rscratch1);
 550     __ cmp(rthread, rscratch1);
 551     __ br(Assembler::EQ, L);
 552     __ stop(&quot;StubAssembler::call_RT: rthread not callee saved?&quot;);
 553     __ bind(L);
 554   }
 555 #endif
 556   __ reset_last_Java_frame(true);
 557   __ maybe_isb();
 558 
 559   // check for pending exceptions
 560   { Label L;
 561     __ ldr(rscratch1, Address(rthread, Thread::pending_exception_offset()));
 562     __ cbz(rscratch1, L);
 563     // exception pending =&gt; remove activation and forward to exception handler
 564 
 565     { Label L1;
 566       __ cbnz(r0, L1);                                  // have we deoptimized?
 567       __ far_jump(RuntimeAddress(Runtime1::entry_for(Runtime1::forward_exception_id)));
 568       __ bind(L1);
 569     }
 570 
 571     // the deopt blob expects exceptions in the special fields of
 572     // JavaThread, so copy and clear pending exception.
 573 
 574     // load and clear pending exception
 575     __ ldr(r0, Address(rthread, Thread::pending_exception_offset()));
 576     __ str(zr, Address(rthread, Thread::pending_exception_offset()));
 577 
 578     // check that there is really a valid exception
 579     __ verify_not_null_oop(r0);
 580 
 581     // load throwing pc: this is the return address of the stub
 582     __ mov(r3, lr);
 583 
 584 #ifdef ASSERT
 585     // check that fields in JavaThread for exception oop and issuing pc are empty
 586     Label oop_empty;
 587     __ ldr(rscratch1, Address(rthread, Thread::pending_exception_offset()));
 588     __ cbz(rscratch1, oop_empty);
 589     __ stop(&quot;exception oop must be empty&quot;);
 590     __ bind(oop_empty);
 591 
 592     Label pc_empty;
 593     __ ldr(rscratch1, Address(rthread, JavaThread::exception_pc_offset()));
 594     __ cbz(rscratch1, pc_empty);
 595     __ stop(&quot;exception pc must be empty&quot;);
 596     __ bind(pc_empty);
 597 #endif
 598 
 599     // store exception oop and throwing pc to JavaThread
 600     __ str(r0, Address(rthread, JavaThread::exception_oop_offset()));
 601     __ str(r3, Address(rthread, JavaThread::exception_pc_offset()));
 602 
 603     restore_live_registers(sasm);
 604 
 605     __ leave();
 606 
 607     // Forward the exception directly to deopt blob. We can blow no
 608     // registers and must leave throwing pc on the stack.  A patch may
 609     // have values live in registers so the entry point with the
 610     // exception in tls.
 611     __ far_jump(RuntimeAddress(deopt_blob-&gt;unpack_with_exception_in_tls()));
 612 
 613     __ bind(L);
 614   }
 615 
 616 
 617   // Runtime will return true if the nmethod has been deoptimized during
 618   // the patching process. In that case we must do a deopt reexecute instead.
 619 
 620   Label cont;
 621 
 622   __ cbz(r0, cont);                                 // have we deoptimized?
 623 
 624   // Will reexecute. Proper return address is already on the stack we just restore
 625   // registers, pop all of our frame but the return address and jump to the deopt blob
 626   restore_live_registers(sasm);
 627   __ leave();
 628   __ far_jump(RuntimeAddress(deopt_blob-&gt;unpack_with_reexecution()));
 629 
 630   __ bind(cont);
 631   restore_live_registers(sasm);
 632   __ leave();
 633   __ ret(lr);
 634 
 635   return oop_maps;
 636 }
 637 
 638 
 639 OopMapSet* Runtime1::generate_code_for(StubID id, StubAssembler* sasm) {
 640 
 641   const Register exception_oop = r0;
 642   const Register exception_pc  = r3;
 643 
 644   // for better readability
 645   const bool must_gc_arguments = true;
 646   const bool dont_gc_arguments = false;
 647 
 648   // default value; overwritten for some optimized stubs that are called from methods that do not use the fpu
 649   bool save_fpu_registers = true;
 650 
 651   // stub code &amp; info for the different stubs
 652   OopMapSet* oop_maps = NULL;
 653   OopMap* oop_map = NULL;
 654   switch (id) {
 655     {
 656     case forward_exception_id:
 657       {
 658         oop_maps = generate_handle_exception(id, sasm);
 659         __ leave();
 660         __ ret(lr);
 661       }
 662       break;
 663 
 664     case throw_div0_exception_id:
 665       { StubFrame f(sasm, &quot;throw_div0_exception&quot;, dont_gc_arguments);
 666         oop_maps = generate_exception_throw(sasm, CAST_FROM_FN_PTR(address, throw_div0_exception), false);
 667       }
 668       break;
 669 
 670     case throw_null_pointer_exception_id:
 671       { StubFrame f(sasm, &quot;throw_null_pointer_exception&quot;, dont_gc_arguments);
 672         oop_maps = generate_exception_throw(sasm, CAST_FROM_FN_PTR(address, throw_null_pointer_exception), false);
 673       }
 674       break;
 675 
 676     case new_instance_id:
 677     case fast_new_instance_id:
 678     case fast_new_instance_init_check_id:
 679       {
 680         Register klass = r3; // Incoming
 681         Register obj   = r0; // Result
 682 
 683         if (id == new_instance_id) {
 684           __ set_info(&quot;new_instance&quot;, dont_gc_arguments);
 685         } else if (id == fast_new_instance_id) {
 686           __ set_info(&quot;fast new_instance&quot;, dont_gc_arguments);
 687         } else {
 688           assert(id == fast_new_instance_init_check_id, &quot;bad StubID&quot;);
 689           __ set_info(&quot;fast new_instance init check&quot;, dont_gc_arguments);
 690         }
 691 
 692         // If TLAB is disabled, see if there is support for inlining contiguous
 693         // allocations.
 694         // Otherwise, just go to the slow path.
 695         if ((id == fast_new_instance_id || id == fast_new_instance_init_check_id) &amp;&amp;
 696             !UseTLAB &amp;&amp; Universe::heap()-&gt;supports_inline_contig_alloc()) {
 697           Label slow_path;
 698           Register obj_size = r2;
 699           Register t1       = r19;
 700           Register t2       = r4;
 701           assert_different_registers(klass, obj, obj_size, t1, t2);
 702 
 703           __ stp(r19, zr, Address(__ pre(sp, -2 * wordSize)));
 704 
 705           if (id == fast_new_instance_init_check_id) {
 706             // make sure the klass is initialized
 707             __ ldrb(rscratch1, Address(klass, InstanceKlass::init_state_offset()));
 708             __ cmpw(rscratch1, InstanceKlass::fully_initialized);
 709             __ br(Assembler::NE, slow_path);
 710           }
 711 
 712 #ifdef ASSERT
 713           // assert object can be fast path allocated
 714           {
 715             Label ok, not_ok;
 716             __ ldrw(obj_size, Address(klass, Klass::layout_helper_offset()));
 717             __ cmp(obj_size, (u1)0);
 718             __ br(Assembler::LE, not_ok);  // make sure it&#39;s an instance (LH &gt; 0)
 719             __ tstw(obj_size, Klass::_lh_instance_slow_path_bit);
 720             __ br(Assembler::EQ, ok);
 721             __ bind(not_ok);
 722             __ stop(&quot;assert(can be fast path allocated)&quot;);
 723             __ should_not_reach_here();
 724             __ bind(ok);
 725           }
 726 #endif // ASSERT
 727 
 728           // get the instance size (size is postive so movl is fine for 64bit)
 729           __ ldrw(obj_size, Address(klass, Klass::layout_helper_offset()));
 730 
 731           __ eden_allocate(obj, obj_size, 0, t1, slow_path);
 732 
 733           __ initialize_object(obj, klass, obj_size, 0, t1, t2, /* is_tlab_allocated */ false);
 734           __ verify_oop(obj);
 735           __ ldp(r19, zr, Address(__ post(sp, 2 * wordSize)));
 736           __ ret(lr);
 737 
 738           __ bind(slow_path);
 739           __ ldp(r19, zr, Address(__ post(sp, 2 * wordSize)));
 740         }
 741 
 742         __ enter();
 743         OopMap* map = save_live_registers(sasm);
 744         int call_offset = __ call_RT(obj, noreg, CAST_FROM_FN_PTR(address, new_instance), klass);
 745         oop_maps = new OopMapSet();
 746         oop_maps-&gt;add_gc_map(call_offset, map);
 747         restore_live_registers_except_r0(sasm);
 748         __ verify_oop(obj);
 749         __ leave();
 750         __ ret(lr);
 751 
 752         // r0,: new instance
 753       }
 754 
 755       break;
 756 
 757     case counter_overflow_id:
 758       {
 759         Register bci = r0, method = r1;
 760         __ enter();
 761         OopMap* map = save_live_registers(sasm);
 762         // Retrieve bci
 763         __ ldrw(bci, Address(rfp, 2*BytesPerWord));
 764         // And a pointer to the Method*
 765         __ ldr(method, Address(rfp, 3*BytesPerWord));
 766         int call_offset = __ call_RT(noreg, noreg, CAST_FROM_FN_PTR(address, counter_overflow), bci, method);
 767         oop_maps = new OopMapSet();
 768         oop_maps-&gt;add_gc_map(call_offset, map);
 769         restore_live_registers(sasm);
 770         __ leave();
 771         __ ret(lr);
 772       }
 773       break;
 774 
 775     case new_type_array_id:
 776     case new_object_array_id:
<a name="1" id="anc1"></a><span class="line-modified"> 777     case new_flat_array_id:</span>
 778       {
 779         Register length   = r19; // Incoming
 780         Register klass    = r3; // Incoming
 781         Register obj      = r0; // Result
 782 
 783         if (id == new_type_array_id) {
 784           __ set_info(&quot;new_type_array&quot;, dont_gc_arguments);
 785         }
 786         else if (id == new_object_array_id) {
 787           __ set_info(&quot;new_object_array&quot;, dont_gc_arguments);
 788         }
 789         else {
<a name="2" id="anc2"></a><span class="line-modified"> 790           __ set_info(&quot;new_flat_array&quot;, dont_gc_arguments);</span>
 791         }
 792 
 793 #ifdef ASSERT
 794         // assert object type is really an array of the proper kind
 795         {
 796           Label ok;
 797           Register t0 = obj;
 798           __ ldrw(t0, Address(klass, Klass::layout_helper_offset()));
 799           __ asrw(t0, t0, Klass::_lh_array_tag_shift);
 800 
 801           int tag = 0;
 802           switch (id) {
 803            case new_type_array_id: tag = Klass::_lh_array_tag_type_value; break;
 804            case new_object_array_id: tag = Klass::_lh_array_tag_obj_value; break;
<a name="3" id="anc3"></a><span class="line-modified"> 805            case new_flat_array_id: tag = Klass::_lh_array_tag_vt_value; break;</span>
 806            default:  ShouldNotReachHere();
 807           }
 808           __ mov(rscratch1, tag);
 809           __ cmpw(t0, rscratch1);
 810           __ br(Assembler::EQ, ok);
 811           __ stop(&quot;assert(is an array klass)&quot;);
 812           __ should_not_reach_here();
 813           __ bind(ok);
 814         }
 815 #endif // ASSERT
 816 
 817         // If TLAB is disabled, see if there is support for inlining contiguous
 818         // allocations.
 819         // Otherwise, just go to the slow path.
 820         if (!UseTLAB &amp;&amp; Universe::heap()-&gt;supports_inline_contig_alloc()) {
 821           Register arr_size = r4;
 822           Register t1       = r2;
 823           Register t2       = r5;
 824           Label slow_path;
 825           assert_different_registers(length, klass, obj, arr_size, t1, t2);
 826 
 827           // check that array length is small enough for fast path.
 828           __ mov(rscratch1, C1_MacroAssembler::max_array_allocation_length);
 829           __ cmpw(length, rscratch1);
 830           __ br(Assembler::HI, slow_path);
 831 
 832           // get the allocation size: round_up(hdr + length &lt;&lt; (layout_helper &amp; 0x1F))
 833           // since size is positive ldrw does right thing on 64bit
 834           __ ldrw(t1, Address(klass, Klass::layout_helper_offset()));
 835           // since size is positive movw does right thing on 64bit
 836           __ movw(arr_size, length);
 837           __ lslvw(arr_size, length, t1);
 838           __ ubfx(t1, t1, Klass::_lh_header_size_shift,
 839                   exact_log2(Klass::_lh_header_size_mask + 1));
 840           __ add(arr_size, arr_size, t1);
 841           __ add(arr_size, arr_size, MinObjAlignmentInBytesMask); // align up
 842           __ andr(arr_size, arr_size, ~MinObjAlignmentInBytesMask);
 843 
 844           __ eden_allocate(obj, arr_size, 0, t1, slow_path);  // preserves arr_size
 845 
 846           __ initialize_header(obj, klass, length, t1, t2);
 847           __ ldrb(t1, Address(klass, in_bytes(Klass::layout_helper_offset()) + (Klass::_lh_header_size_shift / BitsPerByte)));
 848           assert(Klass::_lh_header_size_shift % BitsPerByte == 0, &quot;bytewise&quot;);
 849           assert(Klass::_lh_header_size_mask &lt;= 0xFF, &quot;bytewise&quot;);
 850           __ andr(t1, t1, Klass::_lh_header_size_mask);
 851           __ sub(arr_size, arr_size, t1);  // body length
 852           __ add(t1, t1, obj);       // body start
 853           __ initialize_body(t1, arr_size, 0, t2);
 854           __ membar(Assembler::StoreStore);
 855           __ verify_oop(obj);
 856 
 857           __ ret(lr);
 858 
 859           __ bind(slow_path);
 860         }
 861 
 862         __ enter();
 863         OopMap* map = save_live_registers(sasm);
 864         int call_offset;
 865         if (id == new_type_array_id) {
 866           call_offset = __ call_RT(obj, noreg, CAST_FROM_FN_PTR(address, new_type_array), klass, length);
 867         } else {
<a name="4" id="anc4"></a><span class="line-modified"> 868           // Runtime1::new_object_array handles both object and flat arrays</span>
 869           call_offset = __ call_RT(obj, noreg, CAST_FROM_FN_PTR(address, new_object_array), klass, length);
 870         }
 871 
 872         oop_maps = new OopMapSet();
 873         oop_maps-&gt;add_gc_map(call_offset, map);
 874         restore_live_registers_except_r0(sasm);
 875 
 876         __ verify_oop(obj);
 877         __ leave();
 878         __ ret(lr);
 879 
 880         // r0: new array
 881       }
 882       break;
 883 
 884     case new_multi_array_id:
 885       { StubFrame f(sasm, &quot;new_multi_array&quot;, dont_gc_arguments);
 886         // r0,: klass
 887         // r19,: rank
 888         // r2: address of 1st dimension
 889         OopMap* map = save_live_registers(sasm);
 890         __ mov(c_rarg1, r0);
 891         __ mov(c_rarg3, r2);
 892         __ mov(c_rarg2, r19);
 893         int call_offset = __ call_RT(r0, noreg, CAST_FROM_FN_PTR(address, new_multi_array), r1, r2, r3);
 894 
 895         oop_maps = new OopMapSet();
 896         oop_maps-&gt;add_gc_map(call_offset, map);
 897         restore_live_registers_except_r0(sasm);
 898 
 899         // r0,: new multi array
 900         __ verify_oop(r0);
 901       }
 902       break;
 903 
<a name="5" id="anc5"></a><span class="line-modified"> 904     case buffer_inline_args_id:</span>
<span class="line-modified"> 905     case buffer_inline_args_no_receiver_id:</span>
 906     {
<a name="6" id="anc6"></a><span class="line-modified"> 907         const char* name = (id == buffer_inline_args_id) ?</span>
<span class="line-modified"> 908           &quot;buffer_inline_args&quot; : &quot;buffer_inline_args_no_receiver&quot;;</span>
 909         StubFrame f(sasm, name, dont_gc_arguments);
 910         OopMap* map = save_live_registers(sasm, 2);
 911         Register method = r1;
<a name="7" id="anc7"></a><span class="line-modified"> 912         address entry = (id == buffer_inline_args_id) ?</span>
<span class="line-modified"> 913           CAST_FROM_FN_PTR(address, buffer_inline_args) :</span>
<span class="line-modified"> 914           CAST_FROM_FN_PTR(address, buffer_inline_args_no_receiver);</span>
 915         int call_offset = __ call_RT(r0, noreg, entry, method);
 916         oop_maps = new OopMapSet();
 917         oop_maps-&gt;add_gc_map(call_offset, map);
 918         restore_live_registers_except_r0(sasm);
 919         __ verify_oop(r0);  // r0: an array of buffered value objects
 920      }
 921      break;
 922 
 923     case load_flattened_array_id:
 924       {
 925         StubFrame f(sasm, &quot;load_flattened_array&quot;, dont_gc_arguments);
 926         OopMap* map = save_live_registers(sasm, 3);
 927 
 928         // Called with store_parameter and not C abi
 929 
 930         f.load_argument(1, r0); // r0,: array
 931         f.load_argument(0, r1); // r1,: index
 932         int call_offset = __ call_RT(r0, noreg, CAST_FROM_FN_PTR(address, load_flattened_array), r0, r1);
 933 
 934         oop_maps = new OopMapSet();
 935         oop_maps-&gt;add_gc_map(call_offset, map);
 936         restore_live_registers_except_r0(sasm);
 937 
 938         // r0: loaded element at array[index]
 939         __ verify_oop(r0);
 940       }
 941       break;
 942 
 943     case store_flattened_array_id:
 944       {
 945         StubFrame f(sasm, &quot;store_flattened_array&quot;, dont_gc_arguments);
 946         OopMap* map = save_live_registers(sasm, 4);
 947 
 948         // Called with store_parameter and not C abi
 949 
 950         f.load_argument(2, r0); // r0: array
 951         f.load_argument(1, r1); // r1: index
 952         f.load_argument(0, r2); // r2: value
 953         int call_offset = __ call_RT(noreg, noreg, CAST_FROM_FN_PTR(address, store_flattened_array), r0, r1, r2);
 954 
 955         oop_maps = new OopMapSet();
 956         oop_maps-&gt;add_gc_map(call_offset, map);
 957         restore_live_registers_except_r0(sasm);
 958       }
 959       break;
 960 
 961       case substitutability_check_id:
 962       {
 963         StubFrame f(sasm, &quot;substitutability_check&quot;, dont_gc_arguments);
 964         OopMap* map = save_live_registers(sasm, 3);
 965 
 966         // Called with store_parameter and not C abi
 967 
 968         f.load_argument(1, r0); // r0,: left
 969         f.load_argument(0, r1); // r1,: right
 970         int call_offset = __ call_RT(noreg, noreg, CAST_FROM_FN_PTR(address, substitutability_check), r0, r1);
 971 
 972         oop_maps = new OopMapSet();
 973         oop_maps-&gt;add_gc_map(call_offset, map);
 974         restore_live_registers_except_r0(sasm);
 975 
 976         // r0,: are the two operands substitutable
 977       }
 978       break;
 979 
 980 
 981 
 982     case register_finalizer_id:
 983       {
 984         __ set_info(&quot;register_finalizer&quot;, dont_gc_arguments);
 985 
 986         // This is called via call_runtime so the arguments
 987         // will be place in C abi locations
 988 
 989         __ verify_oop(c_rarg0);
 990 
 991         // load the klass and check the has finalizer flag
 992         Label register_finalizer;
 993         Register t = r5;
 994         __ load_klass(t, r0);
 995         __ ldrw(t, Address(t, Klass::access_flags_offset()));
 996         __ tbnz(t, exact_log2(JVM_ACC_HAS_FINALIZER), register_finalizer);
 997         __ ret(lr);
 998 
 999         __ bind(register_finalizer);
1000         __ enter();
1001         OopMap* oop_map = save_live_registers(sasm);
1002         int call_offset = __ call_RT(noreg, noreg, CAST_FROM_FN_PTR(address, SharedRuntime::register_finalizer), r0);
1003         oop_maps = new OopMapSet();
1004         oop_maps-&gt;add_gc_map(call_offset, oop_map);
1005 
1006         // Now restore all the live registers
1007         restore_live_registers(sasm);
1008 
1009         __ leave();
1010         __ ret(lr);
1011       }
1012       break;
1013 
1014     case throw_class_cast_exception_id:
1015       { StubFrame f(sasm, &quot;throw_class_cast_exception&quot;, dont_gc_arguments);
1016         oop_maps = generate_exception_throw(sasm, CAST_FROM_FN_PTR(address, throw_class_cast_exception), true);
1017       }
1018       break;
1019 
1020     case throw_incompatible_class_change_error_id:
1021       { StubFrame f(sasm, &quot;throw_incompatible_class_change_exception&quot;, dont_gc_arguments);
1022         oop_maps = generate_exception_throw(sasm, CAST_FROM_FN_PTR(address, throw_incompatible_class_change_error), false);
1023       }
1024       break;
1025 
1026     case throw_illegal_monitor_state_exception_id:
1027       { StubFrame f(sasm, &quot;throw_illegal_monitor_state_exception&quot;, dont_gc_arguments);
1028         oop_maps = generate_exception_throw(sasm, CAST_FROM_FN_PTR(address, throw_illegal_monitor_state_exception), false);
1029       }
1030       break;
1031 
1032     case slow_subtype_check_id:
1033       {
1034         // Typical calling sequence:
1035         // __ push(klass_RInfo);  // object klass or other subclass
1036         // __ push(sup_k_RInfo);  // array element klass or other superclass
1037         // __ bl(slow_subtype_check);
1038         // Note that the subclass is pushed first, and is therefore deepest.
1039         enum layout {
1040           r0_off, r0_off_hi,
1041           r2_off, r2_off_hi,
1042           r4_off, r4_off_hi,
1043           r5_off, r5_off_hi,
1044           sup_k_off, sup_k_off_hi,
1045           klass_off, klass_off_hi,
1046           framesize,
1047           result_off = sup_k_off
1048         };
1049 
1050         __ set_info(&quot;slow_subtype_check&quot;, dont_gc_arguments);
1051         __ push(RegSet::of(r0, r2, r4, r5), sp);
1052 
1053         // This is called by pushing args and not with C abi
1054         // __ ldr(r4, Address(sp, (klass_off) * VMRegImpl::stack_slot_size)); // subclass
1055         // __ ldr(r0, Address(sp, (sup_k_off) * VMRegImpl::stack_slot_size)); // superclass
1056 
1057         __ ldp(r4, r0, Address(sp, (sup_k_off) * VMRegImpl::stack_slot_size));
1058 
1059         Label miss;
1060         __ check_klass_subtype_slow_path(r4, r0, r2, r5, NULL, &amp;miss);
1061 
1062         // fallthrough on success:
1063         __ mov(rscratch1, 1);
1064         __ str(rscratch1, Address(sp, (result_off) * VMRegImpl::stack_slot_size)); // result
1065         __ pop(RegSet::of(r0, r2, r4, r5), sp);
1066         __ ret(lr);
1067 
1068         __ bind(miss);
1069         __ str(zr, Address(sp, (result_off) * VMRegImpl::stack_slot_size)); // result
1070         __ pop(RegSet::of(r0, r2, r4, r5), sp);
1071         __ ret(lr);
1072       }
1073       break;
1074 
1075     case monitorenter_nofpu_id:
1076       save_fpu_registers = false;
1077       // fall through
1078     case monitorenter_id:
1079       {
1080         StubFrame f(sasm, &quot;monitorenter&quot;, dont_gc_arguments);
1081         OopMap* map = save_live_registers(sasm, save_fpu_registers);
1082 
1083         // Called with store_parameter and not C abi
1084 
1085         f.load_argument(1, r0); // r0,: object
1086         f.load_argument(0, r1); // r1,: lock address
1087 
1088         int call_offset = __ call_RT(noreg, noreg, CAST_FROM_FN_PTR(address, monitorenter), r0, r1);
1089 
1090         oop_maps = new OopMapSet();
1091         oop_maps-&gt;add_gc_map(call_offset, map);
1092         restore_live_registers(sasm, save_fpu_registers);
1093       }
1094       break;
1095 
1096     case monitorexit_nofpu_id:
1097       save_fpu_registers = false;
1098       // fall through
1099     case monitorexit_id:
1100       {
1101         StubFrame f(sasm, &quot;monitorexit&quot;, dont_gc_arguments);
1102         OopMap* map = save_live_registers(sasm, save_fpu_registers);
1103 
1104         // Called with store_parameter and not C abi
1105 
1106         f.load_argument(0, r0); // r0,: lock address
1107 
1108         // note: really a leaf routine but must setup last java sp
1109         //       =&gt; use call_RT for now (speed can be improved by
1110         //       doing last java sp setup manually)
1111         int call_offset = __ call_RT(noreg, noreg, CAST_FROM_FN_PTR(address, monitorexit), r0);
1112 
1113         oop_maps = new OopMapSet();
1114         oop_maps-&gt;add_gc_map(call_offset, map);
1115         restore_live_registers(sasm, save_fpu_registers);
1116       }
1117       break;
1118 
1119     case deoptimize_id:
1120       {
1121         StubFrame f(sasm, &quot;deoptimize&quot;, dont_gc_arguments);
1122         OopMap* oop_map = save_live_registers(sasm);
1123         f.load_argument(0, c_rarg1);
1124         int call_offset = __ call_RT(noreg, noreg, CAST_FROM_FN_PTR(address, deoptimize), c_rarg1);
1125 
1126         oop_maps = new OopMapSet();
1127         oop_maps-&gt;add_gc_map(call_offset, oop_map);
1128         restore_live_registers(sasm);
1129         DeoptimizationBlob* deopt_blob = SharedRuntime::deopt_blob();
1130         assert(deopt_blob != NULL, &quot;deoptimization blob must have been created&quot;);
1131         __ leave();
1132         __ far_jump(RuntimeAddress(deopt_blob-&gt;unpack_with_reexecution()));
1133       }
1134       break;
1135 
1136     case throw_range_check_failed_id:
1137       { StubFrame f(sasm, &quot;range_check_failed&quot;, dont_gc_arguments);
1138         oop_maps = generate_exception_throw(sasm, CAST_FROM_FN_PTR(address, throw_range_check_exception), true);
1139       }
1140       break;
1141 
1142     case unwind_exception_id:
1143       { __ set_info(&quot;unwind_exception&quot;, dont_gc_arguments);
1144         // note: no stubframe since we are about to leave the current
1145         //       activation and we are calling a leaf VM function only.
1146         generate_unwind_exception(sasm);
1147       }
1148       break;
1149 
1150     case access_field_patching_id:
1151       { StubFrame f(sasm, &quot;access_field_patching&quot;, dont_gc_arguments);
1152         // we should set up register map
1153         oop_maps = generate_patching(sasm, CAST_FROM_FN_PTR(address, access_field_patching));
1154       }
1155       break;
1156 
1157     case load_klass_patching_id:
1158       { StubFrame f(sasm, &quot;load_klass_patching&quot;, dont_gc_arguments);
1159         // we should set up register map
1160         oop_maps = generate_patching(sasm, CAST_FROM_FN_PTR(address, move_klass_patching));
1161       }
1162       break;
1163 
1164     case load_mirror_patching_id:
1165       { StubFrame f(sasm, &quot;load_mirror_patching&quot;, dont_gc_arguments);
1166         // we should set up register map
1167         oop_maps = generate_patching(sasm, CAST_FROM_FN_PTR(address, move_mirror_patching));
1168       }
1169       break;
1170 
1171     case load_appendix_patching_id:
1172       { StubFrame f(sasm, &quot;load_appendix_patching&quot;, dont_gc_arguments);
1173         // we should set up register map
1174         oop_maps = generate_patching(sasm, CAST_FROM_FN_PTR(address, move_appendix_patching));
1175       }
1176       break;
1177 
1178     case handle_exception_nofpu_id:
1179     case handle_exception_id:
1180       { StubFrame f(sasm, &quot;handle_exception&quot;, dont_gc_arguments);
1181         oop_maps = generate_handle_exception(id, sasm);
1182       }
1183       break;
1184 
1185     case handle_exception_from_callee_id:
1186       { StubFrame f(sasm, &quot;handle_exception_from_callee&quot;, dont_gc_arguments);
1187         oop_maps = generate_handle_exception(id, sasm);
1188       }
1189       break;
1190 
1191     case throw_index_exception_id:
1192       { StubFrame f(sasm, &quot;index_range_check_failed&quot;, dont_gc_arguments);
1193         oop_maps = generate_exception_throw(sasm, CAST_FROM_FN_PTR(address, throw_index_exception), true);
1194       }
1195       break;
1196 
1197     case throw_array_store_exception_id:
1198       { StubFrame f(sasm, &quot;throw_array_store_exception&quot;, dont_gc_arguments);
1199         // tos + 0: link
1200         //     + 1: return address
1201         oop_maps = generate_exception_throw(sasm, CAST_FROM_FN_PTR(address, throw_array_store_exception), true);
1202       }
1203       break;
1204 
1205     case predicate_failed_trap_id:
1206       {
1207         StubFrame f(sasm, &quot;predicate_failed_trap&quot;, dont_gc_arguments);
1208 
1209         OopMap* map = save_live_registers(sasm);
1210 
1211         int call_offset = __ call_RT(noreg, noreg, CAST_FROM_FN_PTR(address, predicate_failed_trap));
1212         oop_maps = new OopMapSet();
1213         oop_maps-&gt;add_gc_map(call_offset, map);
1214         restore_live_registers(sasm);
1215         __ leave();
1216         DeoptimizationBlob* deopt_blob = SharedRuntime::deopt_blob();
1217         assert(deopt_blob != NULL, &quot;deoptimization blob must have been created&quot;);
1218 
1219         __ far_jump(RuntimeAddress(deopt_blob-&gt;unpack_with_reexecution()));
1220       }
1221       break;
1222 
1223     case dtrace_object_alloc_id:
1224       { // c_rarg0: object
1225         StubFrame f(sasm, &quot;dtrace_object_alloc&quot;, dont_gc_arguments);
1226         save_live_registers(sasm);
1227 
1228         __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::dtrace_object_alloc), c_rarg0);
1229 
1230         restore_live_registers(sasm);
1231       }
1232       break;
1233 
1234     default:
1235       // FIXME: For unhandled trap_id this code fails with assert during vm intialization
1236       // rather than insert a call to unimplemented_entry
1237       { StubFrame f(sasm, &quot;unimplemented entry&quot;, dont_gc_arguments);
1238         __ mov(r0, (int)id);
1239         __ call_RT(noreg, noreg, CAST_FROM_FN_PTR(address, unimplemented_entry), r0);
1240         __ should_not_reach_here();
1241       }
1242       break;
1243     }
1244   }
1245 
1246 
1247   return oop_maps;
1248 }
1249 
1250 #undef __
1251 
1252 const char *Runtime1::pd_name_for_address(address entry) { Unimplemented(); return 0; }
<a name="8" id="anc8"></a><b style="font-size: large; color: red">--- EOF ---</b>
















































































</pre>
<input id="eof" value="8" type="hidden" />
</body>
</html>