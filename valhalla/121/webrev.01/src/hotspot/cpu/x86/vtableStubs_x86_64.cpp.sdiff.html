<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/cpu/x86/vtableStubs_x86_64.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="stubGenerator_x86_64.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="x86_64.ad.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/cpu/x86/vtableStubs_x86_64.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
 46 #endif
 47 
 48 VtableStub* VtableStubs::create_vtable_stub(int vtable_index, bool caller_is_c1) {
 49   // Read &quot;A word on VtableStub sizing&quot; in share/code/vtableStubs.hpp for details on stub sizing.
 50   const int stub_code_length = code_size_limit(true);
 51   Register tmp_load_klass = rscratch1;
 52   VtableStub* s = new(stub_code_length) VtableStub(true, vtable_index, caller_is_c1);
 53   // Can be NULL if there is no free space in the code cache.
 54   if (s == NULL) {
 55     return NULL;
 56   }
 57 
 58   // Count unused bytes in instruction sequences of variable size.
 59   // We add them to the computed buffer size in order to avoid
 60   // overflow in subsequently generated stubs.
 61   address   start_pc;
 62   int       slop_bytes = 0;
 63   int       slop_delta = 0;
 64   // No variance was detected in vtable stub sizes. Setting index_dependent_slop == 0 will unveil any deviation from this observation.
 65   const int index_dependent_slop     = 0;
<span class="line-modified"> 66   ByteSize  entry_offset = caller_is_c1 ? Method::from_compiled_value_offset() :  Method::from_compiled_value_ro_offset();</span>
 67 
 68   ResourceMark    rm;
 69   CodeBuffer      cb(s-&gt;entry_point(), stub_code_length);
 70   MacroAssembler* masm = new MacroAssembler(&amp;cb);
 71 
 72 #if (!defined(PRODUCT) &amp;&amp; defined(COMPILER2))
 73   if (CountCompiledCalls) {
 74     __ incrementl(ExternalAddress((address) SharedRuntime::nof_megamorphic_calls_addr()));
 75   }
 76 #endif
 77 
 78   // get receiver (need to skip return address on top of stack)
 79   assert(VtableStub::receiver_location() == j_rarg0-&gt;as_VMReg(), &quot;receiver expected in j_rarg0&quot;);
 80 
 81   // Free registers (non-args) are rax, rbx
 82 
 83   // get receiver klass
 84   address npe_addr = __ pc();
 85   __ load_klass(rax, j_rarg0, tmp_load_klass);
 86 
</pre>
<hr />
<pre>
127   }
128 #endif // PRODUCT
129 
130   // rax: receiver klass
131   // method (rbx): Method*
132   // rcx: receiver
133   address ame_addr = __ pc();
134   __ jmp( Address(rbx, entry_offset));
135 
136   masm-&gt;flush();
137   slop_bytes += index_dependent_slop; // add&#39;l slop for size variance due to large itable offsets
138   bookkeeping(masm, tty, s, npe_addr, ame_addr, true, vtable_index, slop_bytes, index_dependent_slop);
139 
140   return s;
141 }
142 
143 
144 VtableStub* VtableStubs::create_itable_stub(int itable_index, bool caller_is_c1) {
145   // Read &quot;A word on VtableStub sizing&quot; in share/code/vtableStubs.hpp for details on stub sizing.
146   const int stub_code_length = code_size_limit(false);
<span class="line-modified">147   ByteSize  entry_offset = caller_is_c1 ? Method::from_compiled_value_offset() :  Method::from_compiled_value_ro_offset();</span>
148   VtableStub* s = new(stub_code_length) VtableStub(false, itable_index, caller_is_c1);
149   // Can be NULL if there is no free space in the code cache.
150   if (s == NULL) {
151     return NULL;
152   }
153   // Count unused bytes in instruction sequences of variable size.
154   // We add them to the computed buffer size in order to avoid
155   // overflow in subsequently generated stubs.
156   address   start_pc;
157   int       slop_bytes = 0;
158   int       slop_delta = 0;
159   const int index_dependent_slop = (itable_index == 0) ? 4 :     // code size change with transition from 8-bit to 32-bit constant (@index == 16).
160                                    (itable_index &lt; 16) ? 3 : 0;  // index == 0 generates even shorter code.
161 
162   ResourceMark    rm;
163   CodeBuffer      cb(s-&gt;entry_point(), stub_code_length);
164   MacroAssembler *masm = new MacroAssembler(&amp;cb);
165 
166 #if (!defined(PRODUCT) &amp;&amp; defined(COMPILER2))
167   if (CountCompiledCalls) {
</pre>
</td>
<td>
<hr />
<pre>
 46 #endif
 47 
 48 VtableStub* VtableStubs::create_vtable_stub(int vtable_index, bool caller_is_c1) {
 49   // Read &quot;A word on VtableStub sizing&quot; in share/code/vtableStubs.hpp for details on stub sizing.
 50   const int stub_code_length = code_size_limit(true);
 51   Register tmp_load_klass = rscratch1;
 52   VtableStub* s = new(stub_code_length) VtableStub(true, vtable_index, caller_is_c1);
 53   // Can be NULL if there is no free space in the code cache.
 54   if (s == NULL) {
 55     return NULL;
 56   }
 57 
 58   // Count unused bytes in instruction sequences of variable size.
 59   // We add them to the computed buffer size in order to avoid
 60   // overflow in subsequently generated stubs.
 61   address   start_pc;
 62   int       slop_bytes = 0;
 63   int       slop_delta = 0;
 64   // No variance was detected in vtable stub sizes. Setting index_dependent_slop == 0 will unveil any deviation from this observation.
 65   const int index_dependent_slop     = 0;
<span class="line-modified"> 66   ByteSize  entry_offset = caller_is_c1 ? Method::from_compiled_inline_offset() :  Method::from_compiled_inline_ro_offset();</span>
 67 
 68   ResourceMark    rm;
 69   CodeBuffer      cb(s-&gt;entry_point(), stub_code_length);
 70   MacroAssembler* masm = new MacroAssembler(&amp;cb);
 71 
 72 #if (!defined(PRODUCT) &amp;&amp; defined(COMPILER2))
 73   if (CountCompiledCalls) {
 74     __ incrementl(ExternalAddress((address) SharedRuntime::nof_megamorphic_calls_addr()));
 75   }
 76 #endif
 77 
 78   // get receiver (need to skip return address on top of stack)
 79   assert(VtableStub::receiver_location() == j_rarg0-&gt;as_VMReg(), &quot;receiver expected in j_rarg0&quot;);
 80 
 81   // Free registers (non-args) are rax, rbx
 82 
 83   // get receiver klass
 84   address npe_addr = __ pc();
 85   __ load_klass(rax, j_rarg0, tmp_load_klass);
 86 
</pre>
<hr />
<pre>
127   }
128 #endif // PRODUCT
129 
130   // rax: receiver klass
131   // method (rbx): Method*
132   // rcx: receiver
133   address ame_addr = __ pc();
134   __ jmp( Address(rbx, entry_offset));
135 
136   masm-&gt;flush();
137   slop_bytes += index_dependent_slop; // add&#39;l slop for size variance due to large itable offsets
138   bookkeeping(masm, tty, s, npe_addr, ame_addr, true, vtable_index, slop_bytes, index_dependent_slop);
139 
140   return s;
141 }
142 
143 
144 VtableStub* VtableStubs::create_itable_stub(int itable_index, bool caller_is_c1) {
145   // Read &quot;A word on VtableStub sizing&quot; in share/code/vtableStubs.hpp for details on stub sizing.
146   const int stub_code_length = code_size_limit(false);
<span class="line-modified">147   ByteSize  entry_offset = caller_is_c1 ? Method::from_compiled_inline_offset() :  Method::from_compiled_inline_ro_offset();</span>
148   VtableStub* s = new(stub_code_length) VtableStub(false, itable_index, caller_is_c1);
149   // Can be NULL if there is no free space in the code cache.
150   if (s == NULL) {
151     return NULL;
152   }
153   // Count unused bytes in instruction sequences of variable size.
154   // We add them to the computed buffer size in order to avoid
155   // overflow in subsequently generated stubs.
156   address   start_pc;
157   int       slop_bytes = 0;
158   int       slop_delta = 0;
159   const int index_dependent_slop = (itable_index == 0) ? 4 :     // code size change with transition from 8-bit to 32-bit constant (@index == 16).
160                                    (itable_index &lt; 16) ? 3 : 0;  // index == 0 generates even shorter code.
161 
162   ResourceMark    rm;
163   CodeBuffer      cb(s-&gt;entry_point(), stub_code_length);
164   MacroAssembler *masm = new MacroAssembler(&amp;cb);
165 
166 #if (!defined(PRODUCT) &amp;&amp; defined(COMPILER2))
167   if (CountCompiledCalls) {
</pre>
</td>
</tr>
</table>
<center><a href="stubGenerator_x86_64.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="x86_64.ad.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>