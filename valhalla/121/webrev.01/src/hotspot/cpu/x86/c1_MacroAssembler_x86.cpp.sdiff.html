<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/cpu/x86/c1_MacroAssembler_x86.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="c1_LIRGenerator_x86.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="c1_Runtime1_x86.cpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/cpu/x86/c1_MacroAssembler_x86.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
 48   int null_check_offset = -1;
 49 
 50   verify_oop(obj);
 51 
 52   // save object being locked into the BasicObjectLock
 53   movptr(Address(disp_hdr, BasicObjectLock::obj_offset_in_bytes()), obj);
 54 
 55   if (UseBiasedLocking) {
 56     assert(scratch != noreg, &quot;should have scratch register at this point&quot;);
 57     Register rklass_decode_tmp = LP64_ONLY(rscratch1) NOT_LP64(noreg);
 58     null_check_offset = biased_locking_enter(disp_hdr, obj, hdr, scratch, rklass_decode_tmp, false, done, &amp;slow_case);
 59   } else {
 60     null_check_offset = offset();
 61   }
 62 
 63   // Load object header
 64   movptr(hdr, Address(obj, hdr_offset));
 65   // and mark it as unlocked
 66   orptr(hdr, markWord::unlocked_value);
 67   if (EnableValhalla &amp;&amp; !UseBiasedLocking) {
<span class="line-modified"> 68     // Mask always_locked bit such that we go to the slow path if object is a value type</span>
 69     andptr(hdr, ~((int) markWord::biased_lock_bit_in_place));
 70   }
 71   // save unlocked object header into the displaced header location on the stack
 72   movptr(Address(disp_hdr, 0), hdr);
 73   // test if object header is still the same (i.e. unlocked), and if so, store the
 74   // displaced header address in the object header - if it is not the same, get the
 75   // object header instead
 76   MacroAssembler::lock(); // must be immediately before cmpxchg!
 77   cmpxchgptr(disp_hdr, Address(obj, hdr_offset));
 78   // if the object header was the same, we&#39;re done
 79   if (PrintBiasedLockingStatistics) {
 80     cond_inc32(Assembler::equal,
 81                ExternalAddress((address)BiasedLocking::fast_path_entry_count_addr()));
 82   }
 83   jcc(Assembler::equal, done);
 84   // if the object header was not the same, it is now in the hdr register
 85   // =&gt; test if it is a stack pointer into the same stack (recursive locking), i.e.:
 86   //
 87   // 1) (hdr &amp; aligned_mask) == 0
 88   // 2) rsp &lt;= hdr
</pre>
<hr />
<pre>
324   push(rbp);
325   if (PreserveFramePointer) {
326     mov(rbp, rsp);
327   }
328   #if !defined(_LP64) &amp;&amp; defined(TIERED)
329     if (UseSSE &lt; 2 ) {
330       // c2 leaves fpu stack dirty. Clean it on entry
331       empty_FPU_stack();
332     }
333   #endif // !_LP64 &amp;&amp; TIERED
334   decrement(rsp, frame_size_in_bytes);
335 
336   if (needs_stack_repair) {
337     // Save stack increment (also account for fixed framesize and rbp)
338     assert((sp_inc &amp; (StackAlignmentInBytes-1)) == 0, &quot;stack increment not aligned&quot;);
339     int real_frame_size = sp_inc + frame_size_in_bytes + wordSize;
340     movptr(Address(rsp, frame_size_in_bytes - wordSize), real_frame_size);
341   }
342 }
343 
<span class="line-modified">344 void C1_MacroAssembler::build_frame(int frame_size_in_bytes, int bang_size_in_bytes, int sp_offset_for_orig_pc, bool needs_stack_repair, bool has_scalarized_args, Label* verified_value_entry_label) {</span>
345   if (has_scalarized_args) {
346     // Initialize orig_pc to detect deoptimization during buffering in the entry points
347     movptr(Address(rsp, sp_offset_for_orig_pc - frame_size_in_bytes - wordSize), 0);
348   }
<span class="line-modified">349   if (!needs_stack_repair &amp;&amp; verified_value_entry_label != NULL) {</span>
<span class="line-modified">350     bind(*verified_value_entry_label);</span>
351   }
352   // Make sure there is enough stack space for this method&#39;s activation.
353   // Note that we do this before doing an enter(). This matches the
354   // ordering of C2&#39;s stack overflow check / rsp decrement and allows
355   // the SharedRuntime stack overflow handling to be consistent
356   // between the two compilers.
357   assert(bang_size_in_bytes &gt;= frame_size_in_bytes, &quot;stack bang size incorrect&quot;);
358   generate_stack_overflow_check(bang_size_in_bytes);
359 
360   build_frame_helper(frame_size_in_bytes, 0, needs_stack_repair);
361 
<span class="line-modified">362   if (needs_stack_repair &amp;&amp; verified_value_entry_label != NULL) {</span>
363     // Jump here from the scalarized entry points that require additional stack space
364     // for packing scalarized arguments and therefore already created the frame.
<span class="line-modified">365     bind(*verified_value_entry_label);</span>
366   }
367   BarrierSetAssembler* bs = BarrierSet::barrier_set()-&gt;barrier_set_assembler();
368   bs-&gt;nmethod_entry_barrier(this);
369 }
370 
371 void C1_MacroAssembler::verified_entry() {
372   if (C1Breakpoint || VerifyFPU || !UseStackBanging) {
373     // Verified Entry first instruction should be 5 bytes long for correct
374     // patching by patch_verified_entry().
375     //
376     // C1Breakpoint and VerifyFPU have one byte first instruction.
377     // Also first instruction will be one byte &quot;push(rbp)&quot; if stack banging
378     // code is not generated (see build_frame() above).
379     // For all these cases generate long instruction first.
380     fat_nop();
381   }
382   if (C1Breakpoint)int3();
383   // build frame
384   IA32_ONLY( verify_FPU(0, &quot;method_entry&quot;); )
385 }
386 
<span class="line-modified">387 int C1_MacroAssembler::scalarized_entry(const CompiledEntrySignature *ces, int frame_size_in_bytes, int bang_size_in_bytes, int sp_offset_for_orig_pc, Label&amp; verified_value_entry_label, bool is_value_ro_entry) {</span>
388   assert(InlineTypePassFieldsAsArgs, &quot;sanity&quot;);
389   // Make sure there is enough stack space for this method&#39;s activation.
390   assert(bang_size_in_bytes &gt;= frame_size_in_bytes, &quot;stack bang size incorrect&quot;);
391   generate_stack_overflow_check(bang_size_in_bytes);
392 
393   GrowableArray&lt;SigEntry&gt;* sig    = &amp;ces-&gt;sig();
<span class="line-modified">394   GrowableArray&lt;SigEntry&gt;* sig_cc = is_value_ro_entry ? &amp;ces-&gt;sig_cc_ro() : &amp;ces-&gt;sig_cc();</span>
395   VMRegPair* regs      = ces-&gt;regs();
<span class="line-modified">396   VMRegPair* regs_cc   = is_value_ro_entry ? ces-&gt;regs_cc_ro() : ces-&gt;regs_cc();</span>
397   int args_on_stack    = ces-&gt;args_on_stack();
<span class="line-modified">398   int args_on_stack_cc = is_value_ro_entry ? ces-&gt;args_on_stack_cc_ro() : ces-&gt;args_on_stack_cc();</span>
399 
<span class="line-modified">400   assert(sig-&gt;length() &lt;= sig_cc-&gt;length(), &quot;Zero-sized value class not allowed!&quot;);</span>
401   BasicType* sig_bt = NEW_RESOURCE_ARRAY(BasicType, sig_cc-&gt;length());
402   int args_passed = sig-&gt;length();
403   int args_passed_cc = SigEntry::fill_sig_bt(sig_cc, sig_bt);
404   int extra_stack_offset = wordSize; // tos is return address.
405 
406   // Check if we need to extend the stack for packing
407   int sp_inc = 0;
408   if (args_on_stack &gt; args_on_stack_cc) {
409     // Two additional slots to account for return address
410     sp_inc = (args_on_stack + 2) * VMRegImpl::stack_slot_size;
411     sp_inc = align_up(sp_inc, StackAlignmentInBytes);
412     pop(r13); // Copy return address
413     subptr(rsp, sp_inc);
414     push(r13);
415   }
416 
417   // Create a temp frame so we can call into the runtime. It must be properly set up to accommodate GC.
418   build_frame_helper(frame_size_in_bytes, sp_inc, ces-&gt;c1_needs_stack_repair());
419 
420   // Initialize orig_pc to detect deoptimization during buffering in below runtime call
421   movptr(Address(rsp, sp_offset_for_orig_pc), 0);
422 
<span class="line-modified">423   // FIXME -- call runtime only if we cannot in-line allocate all the incoming value args.</span>
424   movptr(rbx, (intptr_t)(ces-&gt;method()));
<span class="line-modified">425   if (is_value_ro_entry) {</span>
<span class="line-modified">426     call(RuntimeAddress(Runtime1::entry_for(Runtime1::buffer_value_args_no_receiver_id)));</span>
427   } else {
<span class="line-modified">428     call(RuntimeAddress(Runtime1::entry_for(Runtime1::buffer_value_args_id)));</span>
429   }
430   int rt_call_offset = offset();
431 
432   // Remove the temp frame
433   addptr(rsp, frame_size_in_bytes);
434   pop(rbp);
435 
<span class="line-modified">436   shuffle_value_args(true, is_value_ro_entry, extra_stack_offset, sig_bt, sig_cc,</span>
<span class="line-modified">437                      args_passed_cc, args_on_stack_cc, regs_cc, // from</span>
<span class="line-modified">438                      args_passed, args_on_stack, regs, sp_inc); // to</span>
439 
440   if (ces-&gt;c1_needs_stack_repair()) {
441     // Create the real frame. Below jump will then skip over the stack banging and frame
<span class="line-modified">442     // setup code in the verified_value_entry (which has a different real_frame_size).</span>
443     build_frame_helper(frame_size_in_bytes, sp_inc, true);
444   }
445 
<span class="line-modified">446   jmp(verified_value_entry_label);</span>
447   return rt_call_offset;
448 }
449 
450 void C1_MacroAssembler::load_parameter(int offset_in_words, Register reg) {
451   // rbp, + 0: link
452   //     + 1: return address
453   //     + 2: argument with offset 0
454   //     + 3: argument with offset 1
455   //     + 4: ...
456 
457   movptr(reg, Address(rbp, (offset_in_words + 2) * BytesPerWord));
458 }
459 
460 #ifndef PRODUCT
461 
462 void C1_MacroAssembler::verify_stack_oop(int stack_offset) {
463   if (!VerifyOops) return;
464   verify_oop_addr(Address(rsp, stack_offset));
465 }
466 
</pre>
</td>
<td>
<hr />
<pre>
 48   int null_check_offset = -1;
 49 
 50   verify_oop(obj);
 51 
 52   // save object being locked into the BasicObjectLock
 53   movptr(Address(disp_hdr, BasicObjectLock::obj_offset_in_bytes()), obj);
 54 
 55   if (UseBiasedLocking) {
 56     assert(scratch != noreg, &quot;should have scratch register at this point&quot;);
 57     Register rklass_decode_tmp = LP64_ONLY(rscratch1) NOT_LP64(noreg);
 58     null_check_offset = biased_locking_enter(disp_hdr, obj, hdr, scratch, rklass_decode_tmp, false, done, &amp;slow_case);
 59   } else {
 60     null_check_offset = offset();
 61   }
 62 
 63   // Load object header
 64   movptr(hdr, Address(obj, hdr_offset));
 65   // and mark it as unlocked
 66   orptr(hdr, markWord::unlocked_value);
 67   if (EnableValhalla &amp;&amp; !UseBiasedLocking) {
<span class="line-modified"> 68     // Mask always_locked bit such that we go to the slow path if object is an inline type</span>
 69     andptr(hdr, ~((int) markWord::biased_lock_bit_in_place));
 70   }
 71   // save unlocked object header into the displaced header location on the stack
 72   movptr(Address(disp_hdr, 0), hdr);
 73   // test if object header is still the same (i.e. unlocked), and if so, store the
 74   // displaced header address in the object header - if it is not the same, get the
 75   // object header instead
 76   MacroAssembler::lock(); // must be immediately before cmpxchg!
 77   cmpxchgptr(disp_hdr, Address(obj, hdr_offset));
 78   // if the object header was the same, we&#39;re done
 79   if (PrintBiasedLockingStatistics) {
 80     cond_inc32(Assembler::equal,
 81                ExternalAddress((address)BiasedLocking::fast_path_entry_count_addr()));
 82   }
 83   jcc(Assembler::equal, done);
 84   // if the object header was not the same, it is now in the hdr register
 85   // =&gt; test if it is a stack pointer into the same stack (recursive locking), i.e.:
 86   //
 87   // 1) (hdr &amp; aligned_mask) == 0
 88   // 2) rsp &lt;= hdr
</pre>
<hr />
<pre>
324   push(rbp);
325   if (PreserveFramePointer) {
326     mov(rbp, rsp);
327   }
328   #if !defined(_LP64) &amp;&amp; defined(TIERED)
329     if (UseSSE &lt; 2 ) {
330       // c2 leaves fpu stack dirty. Clean it on entry
331       empty_FPU_stack();
332     }
333   #endif // !_LP64 &amp;&amp; TIERED
334   decrement(rsp, frame_size_in_bytes);
335 
336   if (needs_stack_repair) {
337     // Save stack increment (also account for fixed framesize and rbp)
338     assert((sp_inc &amp; (StackAlignmentInBytes-1)) == 0, &quot;stack increment not aligned&quot;);
339     int real_frame_size = sp_inc + frame_size_in_bytes + wordSize;
340     movptr(Address(rsp, frame_size_in_bytes - wordSize), real_frame_size);
341   }
342 }
343 
<span class="line-modified">344 void C1_MacroAssembler::build_frame(int frame_size_in_bytes, int bang_size_in_bytes, int sp_offset_for_orig_pc, bool needs_stack_repair, bool has_scalarized_args, Label* verified_inline_entry_label) {</span>
345   if (has_scalarized_args) {
346     // Initialize orig_pc to detect deoptimization during buffering in the entry points
347     movptr(Address(rsp, sp_offset_for_orig_pc - frame_size_in_bytes - wordSize), 0);
348   }
<span class="line-modified">349   if (!needs_stack_repair &amp;&amp; verified_inline_entry_label != NULL) {</span>
<span class="line-modified">350     bind(*verified_inline_entry_label);</span>
351   }
352   // Make sure there is enough stack space for this method&#39;s activation.
353   // Note that we do this before doing an enter(). This matches the
354   // ordering of C2&#39;s stack overflow check / rsp decrement and allows
355   // the SharedRuntime stack overflow handling to be consistent
356   // between the two compilers.
357   assert(bang_size_in_bytes &gt;= frame_size_in_bytes, &quot;stack bang size incorrect&quot;);
358   generate_stack_overflow_check(bang_size_in_bytes);
359 
360   build_frame_helper(frame_size_in_bytes, 0, needs_stack_repair);
361 
<span class="line-modified">362   if (needs_stack_repair &amp;&amp; verified_inline_entry_label != NULL) {</span>
363     // Jump here from the scalarized entry points that require additional stack space
364     // for packing scalarized arguments and therefore already created the frame.
<span class="line-modified">365     bind(*verified_inline_entry_label);</span>
366   }
367   BarrierSetAssembler* bs = BarrierSet::barrier_set()-&gt;barrier_set_assembler();
368   bs-&gt;nmethod_entry_barrier(this);
369 }
370 
371 void C1_MacroAssembler::verified_entry() {
372   if (C1Breakpoint || VerifyFPU || !UseStackBanging) {
373     // Verified Entry first instruction should be 5 bytes long for correct
374     // patching by patch_verified_entry().
375     //
376     // C1Breakpoint and VerifyFPU have one byte first instruction.
377     // Also first instruction will be one byte &quot;push(rbp)&quot; if stack banging
378     // code is not generated (see build_frame() above).
379     // For all these cases generate long instruction first.
380     fat_nop();
381   }
382   if (C1Breakpoint)int3();
383   // build frame
384   IA32_ONLY( verify_FPU(0, &quot;method_entry&quot;); )
385 }
386 
<span class="line-modified">387 int C1_MacroAssembler::scalarized_entry(const CompiledEntrySignature* ces, int frame_size_in_bytes, int bang_size_in_bytes, int sp_offset_for_orig_pc, Label&amp; verified_inline_entry_label, bool is_inline_ro_entry) {</span>
388   assert(InlineTypePassFieldsAsArgs, &quot;sanity&quot;);
389   // Make sure there is enough stack space for this method&#39;s activation.
390   assert(bang_size_in_bytes &gt;= frame_size_in_bytes, &quot;stack bang size incorrect&quot;);
391   generate_stack_overflow_check(bang_size_in_bytes);
392 
393   GrowableArray&lt;SigEntry&gt;* sig    = &amp;ces-&gt;sig();
<span class="line-modified">394   GrowableArray&lt;SigEntry&gt;* sig_cc = is_inline_ro_entry ? &amp;ces-&gt;sig_cc_ro() : &amp;ces-&gt;sig_cc();</span>
395   VMRegPair* regs      = ces-&gt;regs();
<span class="line-modified">396   VMRegPair* regs_cc   = is_inline_ro_entry ? ces-&gt;regs_cc_ro() : ces-&gt;regs_cc();</span>
397   int args_on_stack    = ces-&gt;args_on_stack();
<span class="line-modified">398   int args_on_stack_cc = is_inline_ro_entry ? ces-&gt;args_on_stack_cc_ro() : ces-&gt;args_on_stack_cc();</span>
399 
<span class="line-modified">400   assert(sig-&gt;length() &lt;= sig_cc-&gt;length(), &quot;Zero-sized inline class not allowed!&quot;);</span>
401   BasicType* sig_bt = NEW_RESOURCE_ARRAY(BasicType, sig_cc-&gt;length());
402   int args_passed = sig-&gt;length();
403   int args_passed_cc = SigEntry::fill_sig_bt(sig_cc, sig_bt);
404   int extra_stack_offset = wordSize; // tos is return address.
405 
406   // Check if we need to extend the stack for packing
407   int sp_inc = 0;
408   if (args_on_stack &gt; args_on_stack_cc) {
409     // Two additional slots to account for return address
410     sp_inc = (args_on_stack + 2) * VMRegImpl::stack_slot_size;
411     sp_inc = align_up(sp_inc, StackAlignmentInBytes);
412     pop(r13); // Copy return address
413     subptr(rsp, sp_inc);
414     push(r13);
415   }
416 
417   // Create a temp frame so we can call into the runtime. It must be properly set up to accommodate GC.
418   build_frame_helper(frame_size_in_bytes, sp_inc, ces-&gt;c1_needs_stack_repair());
419 
420   // Initialize orig_pc to detect deoptimization during buffering in below runtime call
421   movptr(Address(rsp, sp_offset_for_orig_pc), 0);
422 
<span class="line-modified">423   // FIXME -- call runtime only if we cannot in-line allocate all the incoming inline type args.</span>
424   movptr(rbx, (intptr_t)(ces-&gt;method()));
<span class="line-modified">425   if (is_inline_ro_entry) {</span>
<span class="line-modified">426     call(RuntimeAddress(Runtime1::entry_for(Runtime1::buffer_inline_args_no_receiver_id)));</span>
427   } else {
<span class="line-modified">428     call(RuntimeAddress(Runtime1::entry_for(Runtime1::buffer_inline_args_id)));</span>
429   }
430   int rt_call_offset = offset();
431 
432   // Remove the temp frame
433   addptr(rsp, frame_size_in_bytes);
434   pop(rbp);
435 
<span class="line-modified">436   shuffle_inline_args(true, is_inline_ro_entry, extra_stack_offset, sig_bt, sig_cc,</span>
<span class="line-modified">437                       args_passed_cc, args_on_stack_cc, regs_cc, // from</span>
<span class="line-modified">438                       args_passed, args_on_stack, regs, sp_inc); // to</span>
439 
440   if (ces-&gt;c1_needs_stack_repair()) {
441     // Create the real frame. Below jump will then skip over the stack banging and frame
<span class="line-modified">442     // setup code in the verified_inline_entry (which has a different real_frame_size).</span>
443     build_frame_helper(frame_size_in_bytes, sp_inc, true);
444   }
445 
<span class="line-modified">446   jmp(verified_inline_entry_label);</span>
447   return rt_call_offset;
448 }
449 
450 void C1_MacroAssembler::load_parameter(int offset_in_words, Register reg) {
451   // rbp, + 0: link
452   //     + 1: return address
453   //     + 2: argument with offset 0
454   //     + 3: argument with offset 1
455   //     + 4: ...
456 
457   movptr(reg, Address(rbp, (offset_in_words + 2) * BytesPerWord));
458 }
459 
460 #ifndef PRODUCT
461 
462 void C1_MacroAssembler::verify_stack_oop(int stack_offset) {
463   if (!VerifyOops) return;
464   verify_oop_addr(Address(rsp, stack_offset));
465 }
466 
</pre>
</td>
</tr>
</table>
<center><a href="c1_LIRGenerator_x86.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="c1_Runtime1_x86.cpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>