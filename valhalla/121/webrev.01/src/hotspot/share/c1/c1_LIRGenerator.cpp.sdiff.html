<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/share/c1/c1_LIRGenerator.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="c1_LIRAssembler.hpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="c1_LIRGenerator.hpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/share/c1/c1_LIRGenerator.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #include &quot;precompiled.hpp&quot;
  26 #include &quot;c1/c1_Compilation.hpp&quot;
  27 #include &quot;c1/c1_Defs.hpp&quot;
  28 #include &quot;c1/c1_FrameMap.hpp&quot;
  29 #include &quot;c1/c1_Instruction.hpp&quot;
  30 #include &quot;c1/c1_LIRAssembler.hpp&quot;
  31 #include &quot;c1/c1_LIRGenerator.hpp&quot;
  32 #include &quot;c1/c1_ValueStack.hpp&quot;
  33 #include &quot;ci/ciArrayKlass.hpp&quot;


  34 #include &quot;ci/ciInstance.hpp&quot;
  35 #include &quot;ci/ciObjArray.hpp&quot;
  36 #include &quot;ci/ciUtilities.hpp&quot;
<span class="line-removed">  37 #include &quot;ci/ciValueArrayKlass.hpp&quot;</span>
<span class="line-removed">  38 #include &quot;ci/ciValueKlass.hpp&quot;</span>
  39 #include &quot;gc/shared/barrierSet.hpp&quot;
  40 #include &quot;gc/shared/c1/barrierSetC1.hpp&quot;
  41 #include &quot;oops/klass.inline.hpp&quot;
  42 #include &quot;runtime/arguments.hpp&quot;
  43 #include &quot;runtime/sharedRuntime.hpp&quot;
  44 #include &quot;runtime/stubRoutines.hpp&quot;
  45 #include &quot;runtime/vm_version.hpp&quot;
  46 #include &quot;utilities/bitMap.inline.hpp&quot;
  47 #include &quot;utilities/macros.hpp&quot;
  48 #include &quot;utilities/powerOfTwo.hpp&quot;
  49 
  50 #ifdef ASSERT
  51 #define __ gen()-&gt;lir(__FILE__, __LINE__)-&gt;
  52 #else
  53 #define __ gen()-&gt;lir()-&gt;
  54 #endif
  55 
  56 #ifndef PATCHED_ADDR
  57 #define PATCHED_ADDR  (max_jint)
  58 #endif
</pre>
<hr />
<pre>
 778         }
 779       }
 780     }
 781     // at least pass along a good guess
 782     if (expected_type == NULL) expected_type = dst_exact_type;
 783     if (expected_type == NULL) expected_type = src_declared_type;
 784     if (expected_type == NULL) expected_type = dst_declared_type;
 785 
 786     src_objarray = (src_exact_type &amp;&amp; src_exact_type-&gt;is_obj_array_klass()) || (src_declared_type &amp;&amp; src_declared_type-&gt;is_obj_array_klass());
 787     dst_objarray = (dst_exact_type &amp;&amp; dst_exact_type-&gt;is_obj_array_klass()) || (dst_declared_type &amp;&amp; dst_declared_type-&gt;is_obj_array_klass());
 788   }
 789 
 790   // if a probable array type has been identified, figure out if any
 791   // of the required checks for a fast case can be elided.
 792   int flags = LIR_OpArrayCopy::all_flags;
 793 
 794   if (!src-&gt;is_loaded_flattened_array() &amp;&amp; !dst-&gt;is_loaded_flattened_array()) {
 795     flags &amp;= ~LIR_OpArrayCopy::always_slow_path;
 796   }
 797   if (!src-&gt;maybe_flattened_array()) {
<span class="line-modified"> 798     flags &amp;= ~LIR_OpArrayCopy::src_valuetype_check;</span>
 799   }
 800   if (!dst-&gt;maybe_flattened_array() &amp;&amp; !dst-&gt;maybe_null_free_array()) {
<span class="line-modified"> 801     flags &amp;= ~LIR_OpArrayCopy::dst_valuetype_check;</span>
 802   }
 803 
 804   if (!src_objarray)
 805     flags &amp;= ~LIR_OpArrayCopy::src_objarray;
 806   if (!dst_objarray)
 807     flags &amp;= ~LIR_OpArrayCopy::dst_objarray;
 808 
 809   if (!x-&gt;arg_needs_null_check(0))
 810     flags &amp;= ~LIR_OpArrayCopy::src_null_check;
 811   if (!x-&gt;arg_needs_null_check(2))
 812     flags &amp;= ~LIR_OpArrayCopy::dst_null_check;
 813 
 814 
 815   if (expected_type != NULL) {
 816     Value length_limit = NULL;
 817 
 818     IfOp* ifop = length-&gt;as_IfOp();
 819     if (ifop != NULL) {
 820       // look for expressions like min(v, a.length) which ends up as
 821       //   x &gt; y ? y : x  or  x &gt;= y ? y : x
</pre>
<hr />
<pre>
1581   }
1582 
1583   access_store_at(decorators, field_type, object, LIR_OprFact::intConst(x-&gt;offset()),
1584                   value.result(), info != NULL ? new CodeEmitInfo(info) : NULL, info);
1585 }
1586 
1587 // FIXME -- I can&#39;t find any other way to pass an address to access_load_at().
1588 class TempResolvedAddress: public Instruction {
1589  public:
1590   TempResolvedAddress(ValueType* type, LIR_Opr addr) : Instruction(type) {
1591     set_operand(addr);
1592   }
1593   virtual void input_values_do(ValueVisitor*) {}
1594   virtual void visit(InstructionVisitor* v)   {}
1595   virtual const char* name() const  { return &quot;TempResolvedAddress&quot;; }
1596 };
1597 
1598 void LIRGenerator::access_flattened_array(bool is_load, LIRItem&amp; array, LIRItem&amp; index, LIRItem&amp; obj_item) {
1599   // Find the starting address of the source (inside the array)
1600   ciType* array_type = array.value()-&gt;declared_type();
<span class="line-modified">1601   ciValueArrayKlass* value_array_klass = array_type-&gt;as_value_array_klass();</span>
<span class="line-modified">1602   assert(value_array_klass-&gt;is_loaded(), &quot;must be&quot;);</span>
1603 
<span class="line-modified">1604   ciValueKlass* elem_klass = value_array_klass-&gt;element_klass()-&gt;as_value_klass();</span>
<span class="line-modified">1605   int array_header_size = value_array_klass-&gt;array_header_in_bytes();</span>
<span class="line-modified">1606   int shift = value_array_klass-&gt;log2_element_size();</span>
1607 
1608 #ifndef _LP64
1609   LIR_Opr index_op = new_register(T_INT);
1610   // FIXME -- on 32-bit, the shift below can overflow, so we need to check that
1611   // the top (shift+1) bits of index_op must be zero, or
1612   // else throw ArrayIndexOutOfBoundsException
1613   if (index.result()-&gt;is_constant()) {
1614     jint const_index = index.result()-&gt;as_jint();
1615     __ move(LIR_OprFact::intConst(const_index &lt;&lt; shift), index_op);
1616   } else {
1617     __ shift_left(index_op, shift, index.result());
1618   }
1619 #else
1620   LIR_Opr index_op = new_register(T_LONG);
1621   if (index.result()-&gt;is_constant()) {
1622     jint const_index = index.result()-&gt;as_jint();
1623     __ move(LIR_OprFact::longConst(const_index &lt;&lt; shift), index_op);
1624   } else {
1625     __ convert(Bytecodes::_i2l, index.result(), index_op);
1626     // Need to shift manually, as LIR_Address can scale only up to 3.
</pre>
<hr />
<pre>
1675 
1676 void LIRGenerator::check_flattened_array(LIR_Opr array, LIR_Opr value, CodeStub* slow_path) {
1677   LIR_Opr tmp = new_register(T_METADATA);
1678   __ check_flattened_array(array, value, tmp, slow_path);
1679 }
1680 
1681 void LIRGenerator::check_null_free_array(LIRItem&amp; array, LIRItem&amp; value, CodeEmitInfo* info) {
1682   LabelObj* L_end = new LabelObj();
1683   LIR_Opr tmp = new_register(T_METADATA);
1684   __ check_null_free_array(array.result(), tmp);
1685   __ branch(lir_cond_equal, L_end-&gt;label());
1686   __ null_check(value.result(), info);
1687   __ branch_destination(L_end-&gt;label());
1688 }
1689 
1690 bool LIRGenerator::needs_flattened_array_store_check(StoreIndexed* x) {
1691   if (x-&gt;elt_type() == T_OBJECT &amp;&amp; x-&gt;array()-&gt;maybe_flattened_array()) {
1692     ciType* type = x-&gt;value()-&gt;declared_type();
1693     if (type != NULL &amp;&amp; type-&gt;is_klass()) {
1694       ciKlass* klass = type-&gt;as_klass();
<span class="line-modified">1695       if (!klass-&gt;can_be_value_klass() || (klass-&gt;is_valuetype() &amp;&amp; !klass-&gt;as_value_klass()-&gt;flatten_array())) {</span>
1696         // This is known to be a non-flattened object. If the array is flattened,
1697         // it will be caught by the code generated by array_store_check().
1698         return false;
1699       }
1700     }
1701     // We&#39;re not 100% sure, so let&#39;s do the flattened_array_store_check.
1702     return true;
1703   }
1704   return false;
1705 }
1706 
1707 bool LIRGenerator::needs_null_free_array_store_check(StoreIndexed* x) {
1708   return x-&gt;elt_type() == T_OBJECT &amp;&amp; x-&gt;array()-&gt;maybe_null_free_array();
1709 }
1710 
1711 void LIRGenerator::do_StoreIndexed(StoreIndexed* x) {
1712   assert(x-&gt;is_pinned(),&quot;&quot;);
1713   assert(x-&gt;elt_type() != T_ARRAY, &quot;never used&quot;);
1714   bool is_loaded_flattened_array = x-&gt;array()-&gt;is_loaded_flattened_array();
1715   bool needs_range_check = x-&gt;compute_needs_range_check();
</pre>
<hr />
<pre>
1933       deopt = true;
1934     } else if (field_type_unloaded) {
1935       // (2) field type is unloaded -- problem: we don&#39;t know whether it&#39;s flattened or not. Let&#39;s deopt
1936       deopt = true;
1937     } else if (!field-&gt;is_flattened()) {
1938       // (3) field is not flattened -- need default value in cases of uninitialized field
1939       need_default = true;
1940     }
1941   }
1942 
1943   if (deopt) {
1944     assert(!need_default, &quot;deopt and need_default cannot both be true&quot;);
1945     assert(x-&gt;needs_patching(), &quot;must be&quot;);
1946     assert(info != NULL, &quot;must be&quot;);
1947     CodeStub* stub = new DeoptimizeStub(new CodeEmitInfo(info),
1948                                         Deoptimization::Reason_unloaded,
1949                                         Deoptimization::Action_make_not_entrant);
1950     __ branch(lir_cond_always, stub);
1951   } else if (need_default) {
1952     assert(!field_type_unloaded, &quot;must be&quot;);
<span class="line-modified">1953     assert(field-&gt;type()-&gt;is_valuetype(), &quot;must be&quot;);</span>
<span class="line-modified">1954     ciValueKlass* value_klass = field-&gt;type()-&gt;as_value_klass();</span>
<span class="line-modified">1955     assert(value_klass-&gt;is_loaded(), &quot;must be&quot;);</span>
1956 
1957     if (field-&gt;is_static() &amp;&amp; holder-&gt;is_loaded()) {
1958       ciInstance* mirror = field-&gt;holder()-&gt;java_mirror();
1959       ciObject* val = mirror-&gt;field_value(field).as_object();
1960       if (val-&gt;is_null_object()) {
1961         // This is a non-nullable static field, but it&#39;s not initialized.
1962         // We need to do a null check, and replace it with the default value.
1963       } else {
1964         // No need to perform null check on this static field
1965         need_default = false;
1966       }
1967     }
1968 
1969     if (need_default) {
<span class="line-modified">1970       default_value = new Constant(new InstanceConstant(value_klass-&gt;default_value_instance()));</span>
1971     }
1972   }
1973 
1974   return default_value;
1975 }
1976 
1977 void LIRGenerator::do_LoadField(LoadField* x) {
1978   bool needs_patching = x-&gt;needs_patching();
1979   bool is_volatile = x-&gt;field()-&gt;is_volatile();
1980   BasicType field_type = x-&gt;field_type();
1981 
1982   CodeEmitInfo* info = NULL;
1983   if (needs_patching) {
1984     assert(x-&gt;explicit_null_check() == NULL, &quot;can&#39;t fold null check into patching field access&quot;);
1985     info = state_for(x, x-&gt;state_before());
1986   } else if (x-&gt;needs_null_check()) {
1987     NullCheck* nc = x-&gt;explicit_null_check();
1988     if (nc == NULL) {
1989       info = state_for(x);
1990     } else {
</pre>
<hr />
<pre>
2152       // TODO: use a (modified) version of array_range_check that does not require a
2153       //       constant length to be loaded to a register
2154       __ cmp(lir_cond_belowEqual, length.result(), index.result());
2155       __ branch(lir_cond_belowEqual, new RangeCheckStub(range_check_info, index.result(), array.result()));
2156     } else {
2157       array_range_check(array.result(), index.result(), null_check_info, range_check_info);
2158       // The range check performs the null check, so clear it out for the load
2159       null_check_info = NULL;
2160     }
2161   }
2162 
2163   ciMethodData* md = NULL;
2164   ciArrayLoadStoreData* load_store = NULL;
2165   if (x-&gt;should_profile()) {
2166     profile_array_type(x, md, load_store);
2167   }
2168 
2169   Value element;
2170   if (x-&gt;vt() != NULL) {
2171     assert(x-&gt;array()-&gt;is_loaded_flattened_array(), &quot;must be&quot;);
<span class="line-modified">2172     // Find the destination address (of the NewValueTypeInstance).</span>
2173     LIR_Opr obj = x-&gt;vt()-&gt;operand();
2174     LIRItem obj_item(x-&gt;vt(), this);
2175 
2176     access_flattened_array(true, array, index, obj_item);
2177     set_no_result(x);
2178     element = x-&gt;vt();
2179     if (x-&gt;should_profile()) {
2180       int flag = ArrayLoadStoreData::flat_array_byte_constant() | ArrayLoadStoreData::null_free_array_byte_constant();
2181       profile_array_load_store_flags(md, load_store, flag);
2182     }
2183   } else {
2184     LIR_Opr result = rlock_result(x, x-&gt;elt_type());
2185     LoadFlattenedArrayStub* slow_path = NULL;
2186 
2187     if (x-&gt;should_profile() &amp;&amp; x-&gt;array()-&gt;maybe_null_free_array()) {
2188       profile_null_free_array(array, md, load_store);
2189     }
2190 
2191     if (x-&gt;elt_type() == T_OBJECT &amp;&amp; x-&gt;array()-&gt;maybe_flattened_array()) {
2192       index.load_item();
</pre>
<hr />
<pre>
3100       __ load_stack_address_monitor(0, lock);
3101 
3102       CodeEmitInfo* info = new CodeEmitInfo(scope()-&gt;start()-&gt;state()-&gt;copy(ValueStack::StateBefore, SynchronizationEntryBCI), NULL, x-&gt;check_flag(Instruction::DeoptimizeOnException));
3103       CodeStub* slow_path = new MonitorEnterStub(obj, lock, info);
3104 
3105       // receiver is guaranteed non-NULL so don&#39;t need CodeEmitInfo
3106       __ lock_object(syncTempOpr(), obj, lock, new_register(T_OBJECT), slow_path, NULL);
3107     }
3108   }
3109   if (compilation()-&gt;age_code()) {
3110     CodeEmitInfo* info = new CodeEmitInfo(scope()-&gt;start()-&gt;state()-&gt;copy(ValueStack::StateBefore, 0), NULL, false);
3111     decrement_age(info);
3112   }
3113   // increment invocation counters if needed
3114   if (!method()-&gt;is_accessor()) { // Accessors do not have MDOs, so no counting.
3115     profile_parameters(x);
3116     CodeEmitInfo* info = new CodeEmitInfo(scope()-&gt;start()-&gt;state()-&gt;copy(ValueStack::StateBefore, SynchronizationEntryBCI), NULL, false);
3117     increment_invocation_counter(info);
3118   }
3119   if (method()-&gt;has_scalarized_args()) {
<span class="line-modified">3120     // Check if deoptimization was triggered (i.e. orig_pc was set) while buffering scalarized value type arguments</span>
3121     // in the entry point (see comments in frame::deoptimize). If so, deoptimize only now that we have the right state.
3122     CodeEmitInfo* info = new CodeEmitInfo(scope()-&gt;start()-&gt;state()-&gt;copy(ValueStack::StateBefore, 0), NULL, false);
3123     CodeStub* deopt_stub = new DeoptimizeStub(info, Deoptimization::Reason_none, Deoptimization::Action_none);
3124     __ append(new LIR_Op0(lir_check_orig_pc));
3125     __ branch(lir_cond_notEqual, deopt_stub);
3126   }
3127 
3128   // all blocks with a successor must end with an unconditional jump
3129   // to the successor even if they are consecutive
3130   __ jump(x-&gt;default_sux());
3131 }
3132 
3133 
3134 void LIRGenerator::do_OsrEntry(OsrEntry* x) {
3135   // construct our frame and model the production of incoming pointer
3136   // to the OSR buffer.
3137   __ osr_entry(LIR_Assembler::osrBufferPointer());
3138   LIR_Opr result = rlock_result(x);
3139   __ move(LIR_Assembler::osrBufferPointer(), result);
3140 }
</pre>
<hr />
<pre>
3384   CodeEmitInfo* info = state_for(x, x-&gt;state_before());
3385 
3386   substitutability_check_common(x-&gt;x(), x-&gt;y(), left, right, equal_result, not_equal_result, result, info);
3387 
3388   assert(x-&gt;cond() == If::eql || x-&gt;cond() == If::neq, &quot;must be&quot;);
3389   __ cmp(lir_cond(x-&gt;cond()), result, equal_result);
3390 }
3391 
3392 void LIRGenerator::substitutability_check_common(Value left_val, Value right_val, LIRItem&amp; left, LIRItem&amp; right,
3393                                                  LIR_Opr equal_result, LIR_Opr not_equal_result, LIR_Opr result,
3394                                                  CodeEmitInfo* info) {
3395   LIR_Opr tmp1 = LIR_OprFact::illegalOpr;
3396   LIR_Opr tmp2 = LIR_OprFact::illegalOpr;
3397   LIR_Opr left_klass_op = LIR_OprFact::illegalOpr;
3398   LIR_Opr right_klass_op = LIR_OprFact::illegalOpr;
3399 
3400   ciKlass* left_klass  = left_val -&gt;as_loaded_klass_or_null();
3401   ciKlass* right_klass = right_val-&gt;as_loaded_klass_or_null();
3402 
3403   if ((left_klass == NULL || right_klass == NULL) ||// The klass is still unloaded, or came from a Phi node.
<span class="line-modified">3404       !left_klass-&gt;is_valuetype() || !right_klass-&gt;is_valuetype()) {</span>
3405     init_temps_for_substitutability_check(tmp1, tmp2);
3406   }
3407 
<span class="line-modified">3408   if (left_klass != NULL &amp;&amp; left_klass-&gt;is_valuetype() &amp;&amp; left_klass == right_klass) {</span>
<span class="line-modified">3409     // No need to load klass -- the operands are statically known to be the same value klass.</span>
3410   } else {
3411     BasicType t_klass = UseCompressedOops ? T_INT : T_METADATA;
3412     left_klass_op = new_register(t_klass);
3413     right_klass_op = new_register(t_klass);
3414   }
3415 
3416   CodeStub* slow_path = new SubstitutabilityCheckStub(left.result(), right.result(), info);
3417   __ substitutability_check(result, left.result(), right.result(), equal_result, not_equal_result,
3418                             tmp1, tmp2,
3419                             left_klass, right_klass, left_klass_op, right_klass_op, info, slow_path);
3420 }
3421 
3422 #ifdef JFR_HAVE_INTRINSICS
3423 void LIRGenerator::do_ClassIDIntrinsic(Intrinsic* x) {
3424   CodeEmitInfo* info = state_for(x);
3425   CodeEmitInfo* info2 = new CodeEmitInfo(info); // Clone for the second null check
3426 
3427   assert(info != NULL, &quot;must have info&quot;);
3428   LIRItem arg(x-&gt;argument_at(0), this);
3429 
</pre>
</td>
<td>
<hr />
<pre>
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #include &quot;precompiled.hpp&quot;
  26 #include &quot;c1/c1_Compilation.hpp&quot;
  27 #include &quot;c1/c1_Defs.hpp&quot;
  28 #include &quot;c1/c1_FrameMap.hpp&quot;
  29 #include &quot;c1/c1_Instruction.hpp&quot;
  30 #include &quot;c1/c1_LIRAssembler.hpp&quot;
  31 #include &quot;c1/c1_LIRGenerator.hpp&quot;
  32 #include &quot;c1/c1_ValueStack.hpp&quot;
  33 #include &quot;ci/ciArrayKlass.hpp&quot;
<span class="line-added">  34 #include &quot;ci/ciFlatArrayKlass.hpp&quot;</span>
<span class="line-added">  35 #include &quot;ci/ciInlineKlass.hpp&quot;</span>
  36 #include &quot;ci/ciInstance.hpp&quot;
  37 #include &quot;ci/ciObjArray.hpp&quot;
  38 #include &quot;ci/ciUtilities.hpp&quot;


  39 #include &quot;gc/shared/barrierSet.hpp&quot;
  40 #include &quot;gc/shared/c1/barrierSetC1.hpp&quot;
  41 #include &quot;oops/klass.inline.hpp&quot;
  42 #include &quot;runtime/arguments.hpp&quot;
  43 #include &quot;runtime/sharedRuntime.hpp&quot;
  44 #include &quot;runtime/stubRoutines.hpp&quot;
  45 #include &quot;runtime/vm_version.hpp&quot;
  46 #include &quot;utilities/bitMap.inline.hpp&quot;
  47 #include &quot;utilities/macros.hpp&quot;
  48 #include &quot;utilities/powerOfTwo.hpp&quot;
  49 
  50 #ifdef ASSERT
  51 #define __ gen()-&gt;lir(__FILE__, __LINE__)-&gt;
  52 #else
  53 #define __ gen()-&gt;lir()-&gt;
  54 #endif
  55 
  56 #ifndef PATCHED_ADDR
  57 #define PATCHED_ADDR  (max_jint)
  58 #endif
</pre>
<hr />
<pre>
 778         }
 779       }
 780     }
 781     // at least pass along a good guess
 782     if (expected_type == NULL) expected_type = dst_exact_type;
 783     if (expected_type == NULL) expected_type = src_declared_type;
 784     if (expected_type == NULL) expected_type = dst_declared_type;
 785 
 786     src_objarray = (src_exact_type &amp;&amp; src_exact_type-&gt;is_obj_array_klass()) || (src_declared_type &amp;&amp; src_declared_type-&gt;is_obj_array_klass());
 787     dst_objarray = (dst_exact_type &amp;&amp; dst_exact_type-&gt;is_obj_array_klass()) || (dst_declared_type &amp;&amp; dst_declared_type-&gt;is_obj_array_klass());
 788   }
 789 
 790   // if a probable array type has been identified, figure out if any
 791   // of the required checks for a fast case can be elided.
 792   int flags = LIR_OpArrayCopy::all_flags;
 793 
 794   if (!src-&gt;is_loaded_flattened_array() &amp;&amp; !dst-&gt;is_loaded_flattened_array()) {
 795     flags &amp;= ~LIR_OpArrayCopy::always_slow_path;
 796   }
 797   if (!src-&gt;maybe_flattened_array()) {
<span class="line-modified"> 798     flags &amp;= ~LIR_OpArrayCopy::src_inlinetype_check;</span>
 799   }
 800   if (!dst-&gt;maybe_flattened_array() &amp;&amp; !dst-&gt;maybe_null_free_array()) {
<span class="line-modified"> 801     flags &amp;= ~LIR_OpArrayCopy::dst_inlinetype_check;</span>
 802   }
 803 
 804   if (!src_objarray)
 805     flags &amp;= ~LIR_OpArrayCopy::src_objarray;
 806   if (!dst_objarray)
 807     flags &amp;= ~LIR_OpArrayCopy::dst_objarray;
 808 
 809   if (!x-&gt;arg_needs_null_check(0))
 810     flags &amp;= ~LIR_OpArrayCopy::src_null_check;
 811   if (!x-&gt;arg_needs_null_check(2))
 812     flags &amp;= ~LIR_OpArrayCopy::dst_null_check;
 813 
 814 
 815   if (expected_type != NULL) {
 816     Value length_limit = NULL;
 817 
 818     IfOp* ifop = length-&gt;as_IfOp();
 819     if (ifop != NULL) {
 820       // look for expressions like min(v, a.length) which ends up as
 821       //   x &gt; y ? y : x  or  x &gt;= y ? y : x
</pre>
<hr />
<pre>
1581   }
1582 
1583   access_store_at(decorators, field_type, object, LIR_OprFact::intConst(x-&gt;offset()),
1584                   value.result(), info != NULL ? new CodeEmitInfo(info) : NULL, info);
1585 }
1586 
1587 // FIXME -- I can&#39;t find any other way to pass an address to access_load_at().
1588 class TempResolvedAddress: public Instruction {
1589  public:
1590   TempResolvedAddress(ValueType* type, LIR_Opr addr) : Instruction(type) {
1591     set_operand(addr);
1592   }
1593   virtual void input_values_do(ValueVisitor*) {}
1594   virtual void visit(InstructionVisitor* v)   {}
1595   virtual const char* name() const  { return &quot;TempResolvedAddress&quot;; }
1596 };
1597 
1598 void LIRGenerator::access_flattened_array(bool is_load, LIRItem&amp; array, LIRItem&amp; index, LIRItem&amp; obj_item) {
1599   // Find the starting address of the source (inside the array)
1600   ciType* array_type = array.value()-&gt;declared_type();
<span class="line-modified">1601   ciFlatArrayKlass* flat_array_klass = array_type-&gt;as_flat_array_klass();</span>
<span class="line-modified">1602   assert(flat_array_klass-&gt;is_loaded(), &quot;must be&quot;);</span>
1603 
<span class="line-modified">1604   ciInlineKlass* elem_klass = flat_array_klass-&gt;element_klass()-&gt;as_inline_klass();</span>
<span class="line-modified">1605   int array_header_size = flat_array_klass-&gt;array_header_in_bytes();</span>
<span class="line-modified">1606   int shift = flat_array_klass-&gt;log2_element_size();</span>
1607 
1608 #ifndef _LP64
1609   LIR_Opr index_op = new_register(T_INT);
1610   // FIXME -- on 32-bit, the shift below can overflow, so we need to check that
1611   // the top (shift+1) bits of index_op must be zero, or
1612   // else throw ArrayIndexOutOfBoundsException
1613   if (index.result()-&gt;is_constant()) {
1614     jint const_index = index.result()-&gt;as_jint();
1615     __ move(LIR_OprFact::intConst(const_index &lt;&lt; shift), index_op);
1616   } else {
1617     __ shift_left(index_op, shift, index.result());
1618   }
1619 #else
1620   LIR_Opr index_op = new_register(T_LONG);
1621   if (index.result()-&gt;is_constant()) {
1622     jint const_index = index.result()-&gt;as_jint();
1623     __ move(LIR_OprFact::longConst(const_index &lt;&lt; shift), index_op);
1624   } else {
1625     __ convert(Bytecodes::_i2l, index.result(), index_op);
1626     // Need to shift manually, as LIR_Address can scale only up to 3.
</pre>
<hr />
<pre>
1675 
1676 void LIRGenerator::check_flattened_array(LIR_Opr array, LIR_Opr value, CodeStub* slow_path) {
1677   LIR_Opr tmp = new_register(T_METADATA);
1678   __ check_flattened_array(array, value, tmp, slow_path);
1679 }
1680 
1681 void LIRGenerator::check_null_free_array(LIRItem&amp; array, LIRItem&amp; value, CodeEmitInfo* info) {
1682   LabelObj* L_end = new LabelObj();
1683   LIR_Opr tmp = new_register(T_METADATA);
1684   __ check_null_free_array(array.result(), tmp);
1685   __ branch(lir_cond_equal, L_end-&gt;label());
1686   __ null_check(value.result(), info);
1687   __ branch_destination(L_end-&gt;label());
1688 }
1689 
1690 bool LIRGenerator::needs_flattened_array_store_check(StoreIndexed* x) {
1691   if (x-&gt;elt_type() == T_OBJECT &amp;&amp; x-&gt;array()-&gt;maybe_flattened_array()) {
1692     ciType* type = x-&gt;value()-&gt;declared_type();
1693     if (type != NULL &amp;&amp; type-&gt;is_klass()) {
1694       ciKlass* klass = type-&gt;as_klass();
<span class="line-modified">1695       if (!klass-&gt;can_be_inline_klass() || (klass-&gt;is_inlinetype() &amp;&amp; !klass-&gt;as_inline_klass()-&gt;flatten_array())) {</span>
1696         // This is known to be a non-flattened object. If the array is flattened,
1697         // it will be caught by the code generated by array_store_check().
1698         return false;
1699       }
1700     }
1701     // We&#39;re not 100% sure, so let&#39;s do the flattened_array_store_check.
1702     return true;
1703   }
1704   return false;
1705 }
1706 
1707 bool LIRGenerator::needs_null_free_array_store_check(StoreIndexed* x) {
1708   return x-&gt;elt_type() == T_OBJECT &amp;&amp; x-&gt;array()-&gt;maybe_null_free_array();
1709 }
1710 
1711 void LIRGenerator::do_StoreIndexed(StoreIndexed* x) {
1712   assert(x-&gt;is_pinned(),&quot;&quot;);
1713   assert(x-&gt;elt_type() != T_ARRAY, &quot;never used&quot;);
1714   bool is_loaded_flattened_array = x-&gt;array()-&gt;is_loaded_flattened_array();
1715   bool needs_range_check = x-&gt;compute_needs_range_check();
</pre>
<hr />
<pre>
1933       deopt = true;
1934     } else if (field_type_unloaded) {
1935       // (2) field type is unloaded -- problem: we don&#39;t know whether it&#39;s flattened or not. Let&#39;s deopt
1936       deopt = true;
1937     } else if (!field-&gt;is_flattened()) {
1938       // (3) field is not flattened -- need default value in cases of uninitialized field
1939       need_default = true;
1940     }
1941   }
1942 
1943   if (deopt) {
1944     assert(!need_default, &quot;deopt and need_default cannot both be true&quot;);
1945     assert(x-&gt;needs_patching(), &quot;must be&quot;);
1946     assert(info != NULL, &quot;must be&quot;);
1947     CodeStub* stub = new DeoptimizeStub(new CodeEmitInfo(info),
1948                                         Deoptimization::Reason_unloaded,
1949                                         Deoptimization::Action_make_not_entrant);
1950     __ branch(lir_cond_always, stub);
1951   } else if (need_default) {
1952     assert(!field_type_unloaded, &quot;must be&quot;);
<span class="line-modified">1953     assert(field-&gt;type()-&gt;is_inlinetype(), &quot;must be&quot;);</span>
<span class="line-modified">1954     ciInlineKlass* inline_klass = field-&gt;type()-&gt;as_inline_klass();</span>
<span class="line-modified">1955     assert(inline_klass-&gt;is_loaded(), &quot;must be&quot;);</span>
1956 
1957     if (field-&gt;is_static() &amp;&amp; holder-&gt;is_loaded()) {
1958       ciInstance* mirror = field-&gt;holder()-&gt;java_mirror();
1959       ciObject* val = mirror-&gt;field_value(field).as_object();
1960       if (val-&gt;is_null_object()) {
1961         // This is a non-nullable static field, but it&#39;s not initialized.
1962         // We need to do a null check, and replace it with the default value.
1963       } else {
1964         // No need to perform null check on this static field
1965         need_default = false;
1966       }
1967     }
1968 
1969     if (need_default) {
<span class="line-modified">1970       default_value = new Constant(new InstanceConstant(inline_klass-&gt;default_instance()));</span>
1971     }
1972   }
1973 
1974   return default_value;
1975 }
1976 
1977 void LIRGenerator::do_LoadField(LoadField* x) {
1978   bool needs_patching = x-&gt;needs_patching();
1979   bool is_volatile = x-&gt;field()-&gt;is_volatile();
1980   BasicType field_type = x-&gt;field_type();
1981 
1982   CodeEmitInfo* info = NULL;
1983   if (needs_patching) {
1984     assert(x-&gt;explicit_null_check() == NULL, &quot;can&#39;t fold null check into patching field access&quot;);
1985     info = state_for(x, x-&gt;state_before());
1986   } else if (x-&gt;needs_null_check()) {
1987     NullCheck* nc = x-&gt;explicit_null_check();
1988     if (nc == NULL) {
1989       info = state_for(x);
1990     } else {
</pre>
<hr />
<pre>
2152       // TODO: use a (modified) version of array_range_check that does not require a
2153       //       constant length to be loaded to a register
2154       __ cmp(lir_cond_belowEqual, length.result(), index.result());
2155       __ branch(lir_cond_belowEqual, new RangeCheckStub(range_check_info, index.result(), array.result()));
2156     } else {
2157       array_range_check(array.result(), index.result(), null_check_info, range_check_info);
2158       // The range check performs the null check, so clear it out for the load
2159       null_check_info = NULL;
2160     }
2161   }
2162 
2163   ciMethodData* md = NULL;
2164   ciArrayLoadStoreData* load_store = NULL;
2165   if (x-&gt;should_profile()) {
2166     profile_array_type(x, md, load_store);
2167   }
2168 
2169   Value element;
2170   if (x-&gt;vt() != NULL) {
2171     assert(x-&gt;array()-&gt;is_loaded_flattened_array(), &quot;must be&quot;);
<span class="line-modified">2172     // Find the destination address (of the NewInlineTypeInstance).</span>
2173     LIR_Opr obj = x-&gt;vt()-&gt;operand();
2174     LIRItem obj_item(x-&gt;vt(), this);
2175 
2176     access_flattened_array(true, array, index, obj_item);
2177     set_no_result(x);
2178     element = x-&gt;vt();
2179     if (x-&gt;should_profile()) {
2180       int flag = ArrayLoadStoreData::flat_array_byte_constant() | ArrayLoadStoreData::null_free_array_byte_constant();
2181       profile_array_load_store_flags(md, load_store, flag);
2182     }
2183   } else {
2184     LIR_Opr result = rlock_result(x, x-&gt;elt_type());
2185     LoadFlattenedArrayStub* slow_path = NULL;
2186 
2187     if (x-&gt;should_profile() &amp;&amp; x-&gt;array()-&gt;maybe_null_free_array()) {
2188       profile_null_free_array(array, md, load_store);
2189     }
2190 
2191     if (x-&gt;elt_type() == T_OBJECT &amp;&amp; x-&gt;array()-&gt;maybe_flattened_array()) {
2192       index.load_item();
</pre>
<hr />
<pre>
3100       __ load_stack_address_monitor(0, lock);
3101 
3102       CodeEmitInfo* info = new CodeEmitInfo(scope()-&gt;start()-&gt;state()-&gt;copy(ValueStack::StateBefore, SynchronizationEntryBCI), NULL, x-&gt;check_flag(Instruction::DeoptimizeOnException));
3103       CodeStub* slow_path = new MonitorEnterStub(obj, lock, info);
3104 
3105       // receiver is guaranteed non-NULL so don&#39;t need CodeEmitInfo
3106       __ lock_object(syncTempOpr(), obj, lock, new_register(T_OBJECT), slow_path, NULL);
3107     }
3108   }
3109   if (compilation()-&gt;age_code()) {
3110     CodeEmitInfo* info = new CodeEmitInfo(scope()-&gt;start()-&gt;state()-&gt;copy(ValueStack::StateBefore, 0), NULL, false);
3111     decrement_age(info);
3112   }
3113   // increment invocation counters if needed
3114   if (!method()-&gt;is_accessor()) { // Accessors do not have MDOs, so no counting.
3115     profile_parameters(x);
3116     CodeEmitInfo* info = new CodeEmitInfo(scope()-&gt;start()-&gt;state()-&gt;copy(ValueStack::StateBefore, SynchronizationEntryBCI), NULL, false);
3117     increment_invocation_counter(info);
3118   }
3119   if (method()-&gt;has_scalarized_args()) {
<span class="line-modified">3120     // Check if deoptimization was triggered (i.e. orig_pc was set) while buffering scalarized inline type arguments</span>
3121     // in the entry point (see comments in frame::deoptimize). If so, deoptimize only now that we have the right state.
3122     CodeEmitInfo* info = new CodeEmitInfo(scope()-&gt;start()-&gt;state()-&gt;copy(ValueStack::StateBefore, 0), NULL, false);
3123     CodeStub* deopt_stub = new DeoptimizeStub(info, Deoptimization::Reason_none, Deoptimization::Action_none);
3124     __ append(new LIR_Op0(lir_check_orig_pc));
3125     __ branch(lir_cond_notEqual, deopt_stub);
3126   }
3127 
3128   // all blocks with a successor must end with an unconditional jump
3129   // to the successor even if they are consecutive
3130   __ jump(x-&gt;default_sux());
3131 }
3132 
3133 
3134 void LIRGenerator::do_OsrEntry(OsrEntry* x) {
3135   // construct our frame and model the production of incoming pointer
3136   // to the OSR buffer.
3137   __ osr_entry(LIR_Assembler::osrBufferPointer());
3138   LIR_Opr result = rlock_result(x);
3139   __ move(LIR_Assembler::osrBufferPointer(), result);
3140 }
</pre>
<hr />
<pre>
3384   CodeEmitInfo* info = state_for(x, x-&gt;state_before());
3385 
3386   substitutability_check_common(x-&gt;x(), x-&gt;y(), left, right, equal_result, not_equal_result, result, info);
3387 
3388   assert(x-&gt;cond() == If::eql || x-&gt;cond() == If::neq, &quot;must be&quot;);
3389   __ cmp(lir_cond(x-&gt;cond()), result, equal_result);
3390 }
3391 
3392 void LIRGenerator::substitutability_check_common(Value left_val, Value right_val, LIRItem&amp; left, LIRItem&amp; right,
3393                                                  LIR_Opr equal_result, LIR_Opr not_equal_result, LIR_Opr result,
3394                                                  CodeEmitInfo* info) {
3395   LIR_Opr tmp1 = LIR_OprFact::illegalOpr;
3396   LIR_Opr tmp2 = LIR_OprFact::illegalOpr;
3397   LIR_Opr left_klass_op = LIR_OprFact::illegalOpr;
3398   LIR_Opr right_klass_op = LIR_OprFact::illegalOpr;
3399 
3400   ciKlass* left_klass  = left_val -&gt;as_loaded_klass_or_null();
3401   ciKlass* right_klass = right_val-&gt;as_loaded_klass_or_null();
3402 
3403   if ((left_klass == NULL || right_klass == NULL) ||// The klass is still unloaded, or came from a Phi node.
<span class="line-modified">3404       !left_klass-&gt;is_inlinetype() || !right_klass-&gt;is_inlinetype()) {</span>
3405     init_temps_for_substitutability_check(tmp1, tmp2);
3406   }
3407 
<span class="line-modified">3408   if (left_klass != NULL &amp;&amp; left_klass-&gt;is_inlinetype() &amp;&amp; left_klass == right_klass) {</span>
<span class="line-modified">3409     // No need to load klass -- the operands are statically known to be the same inline klass.</span>
3410   } else {
3411     BasicType t_klass = UseCompressedOops ? T_INT : T_METADATA;
3412     left_klass_op = new_register(t_klass);
3413     right_klass_op = new_register(t_klass);
3414   }
3415 
3416   CodeStub* slow_path = new SubstitutabilityCheckStub(left.result(), right.result(), info);
3417   __ substitutability_check(result, left.result(), right.result(), equal_result, not_equal_result,
3418                             tmp1, tmp2,
3419                             left_klass, right_klass, left_klass_op, right_klass_op, info, slow_path);
3420 }
3421 
3422 #ifdef JFR_HAVE_INTRINSICS
3423 void LIRGenerator::do_ClassIDIntrinsic(Intrinsic* x) {
3424   CodeEmitInfo* info = state_for(x);
3425   CodeEmitInfo* info2 = new CodeEmitInfo(info); // Clone for the second null check
3426 
3427   assert(info != NULL, &quot;must have info&quot;);
3428   LIRItem arg(x-&gt;argument_at(0), this);
3429 
</pre>
</td>
</tr>
</table>
<center><a href="c1_LIRAssembler.hpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="c1_LIRGenerator.hpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>