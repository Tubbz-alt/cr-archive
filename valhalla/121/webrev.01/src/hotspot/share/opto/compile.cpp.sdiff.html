<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/share/opto/compile.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="classes.hpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="compile.hpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/share/opto/compile.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
  34 #include &quot;compiler/disassembler.hpp&quot;
  35 #include &quot;compiler/oopMap.hpp&quot;
  36 #include &quot;gc/shared/barrierSet.hpp&quot;
  37 #include &quot;gc/shared/c2/barrierSetC2.hpp&quot;
  38 #include &quot;jfr/jfrEvents.hpp&quot;
  39 #include &quot;memory/resourceArea.hpp&quot;
  40 #include &quot;opto/addnode.hpp&quot;
  41 #include &quot;opto/block.hpp&quot;
  42 #include &quot;opto/c2compiler.hpp&quot;
  43 #include &quot;opto/callGenerator.hpp&quot;
  44 #include &quot;opto/callnode.hpp&quot;
  45 #include &quot;opto/castnode.hpp&quot;
  46 #include &quot;opto/cfgnode.hpp&quot;
  47 #include &quot;opto/chaitin.hpp&quot;
  48 #include &quot;opto/compile.hpp&quot;
  49 #include &quot;opto/connode.hpp&quot;
  50 #include &quot;opto/convertnode.hpp&quot;
  51 #include &quot;opto/divnode.hpp&quot;
  52 #include &quot;opto/escape.hpp&quot;
  53 #include &quot;opto/idealGraphPrinter.hpp&quot;

  54 #include &quot;opto/loopnode.hpp&quot;
  55 #include &quot;opto/machnode.hpp&quot;
  56 #include &quot;opto/macro.hpp&quot;
  57 #include &quot;opto/matcher.hpp&quot;
  58 #include &quot;opto/mathexactnode.hpp&quot;
  59 #include &quot;opto/memnode.hpp&quot;
  60 #include &quot;opto/mulnode.hpp&quot;
  61 #include &quot;opto/narrowptrnode.hpp&quot;
  62 #include &quot;opto/node.hpp&quot;
  63 #include &quot;opto/opcodes.hpp&quot;
  64 #include &quot;opto/output.hpp&quot;
  65 #include &quot;opto/parse.hpp&quot;
  66 #include &quot;opto/phaseX.hpp&quot;
  67 #include &quot;opto/rootnode.hpp&quot;
  68 #include &quot;opto/runtime.hpp&quot;
  69 #include &quot;opto/stringopts.hpp&quot;
  70 #include &quot;opto/type.hpp&quot;
<span class="line-removed">  71 #include &quot;opto/valuetypenode.hpp&quot;</span>
  72 #include &quot;opto/vectornode.hpp&quot;
  73 #include &quot;runtime/arguments.hpp&quot;
  74 #include &quot;runtime/sharedRuntime.hpp&quot;
  75 #include &quot;runtime/signature.hpp&quot;
  76 #include &quot;runtime/stubRoutines.hpp&quot;
  77 #include &quot;runtime/timer.hpp&quot;
  78 #include &quot;utilities/align.hpp&quot;
  79 #include &quot;utilities/copy.hpp&quot;
  80 #include &quot;utilities/macros.hpp&quot;
  81 #include &quot;utilities/resourceHash.hpp&quot;
  82 
  83 
  84 // -------------------- Compile::mach_constant_base_node -----------------------
  85 // Constant table base node singleton.
  86 MachConstantBaseNode* Compile::mach_constant_base_node() {
  87   if (_mach_constant_base_node == NULL) {
  88     _mach_constant_base_node = new MachConstantBaseNode();
  89     _mach_constant_base_node-&gt;add_req(C-&gt;root());
  90   }
  91   return _mach_constant_base_node;
</pre>
<hr />
<pre>
 389   for (int i = range_check_cast_count() - 1; i &gt;= 0; i--) {
 390     Node* cast = range_check_cast_node(i);
 391     if (!useful.member(cast)) {
 392       remove_range_check_cast(cast);
 393     }
 394   }
 395   // Remove useless expensive nodes
 396   for (int i = C-&gt;expensive_count()-1; i &gt;= 0; i--) {
 397     Node* n = C-&gt;expensive_node(i);
 398     if (!useful.member(n)) {
 399       remove_expensive_node(n);
 400     }
 401   }
 402   // Remove useless Opaque4 nodes
 403   for (int i = opaque4_count() - 1; i &gt;= 0; i--) {
 404     Node* opaq = opaque4_node(i);
 405     if (!useful.member(opaq)) {
 406       remove_opaque4_node(opaq);
 407     }
 408   }
<span class="line-modified"> 409   // Remove useless value type nodes</span>
<span class="line-modified"> 410   for (int i = _value_type_nodes-&gt;length() - 1; i &gt;= 0; i--) {</span>
<span class="line-modified"> 411     Node* vt = _value_type_nodes-&gt;at(i);</span>
 412     if (!useful.member(vt)) {
<span class="line-modified"> 413       _value_type_nodes-&gt;remove(vt);</span>
 414     }
 415   }
 416   BarrierSetC2* bs = BarrierSet::barrier_set()-&gt;barrier_set_c2();
 417   bs-&gt;eliminate_useless_gc_barriers(useful, this);
 418   // clean up the late inline lists
 419   remove_useless_late_inlines(&amp;_string_late_inlines, useful);
 420   remove_useless_late_inlines(&amp;_boxing_late_inlines, useful);
 421   remove_useless_late_inlines(&amp;_late_inlines, useful);
 422   debug_only(verify_graph_edges(true/*check for no_dead_code*/);)
 423 }
 424 
 425 // ============================================================================
 426 //------------------------------CompileWrapper---------------------------------
 427 class CompileWrapper : public StackObj {
 428   Compile *const _compile;
 429  public:
 430   CompileWrapper(Compile* compile);
 431 
 432   ~CompileWrapper();
 433 };
</pre>
<hr />
<pre>
 997   Copy::zero_to_bytes(ats, sizeof(AliasType)*grow_ats);
 998   {
 999     for (int i = 0; i &lt; grow_ats; i++)  _alias_types[i] = &amp;ats[i];
1000   }
1001   // Initialize the first few types.
1002   _alias_types[AliasIdxTop]-&gt;Init(AliasIdxTop, NULL);
1003   _alias_types[AliasIdxBot]-&gt;Init(AliasIdxBot, TypePtr::BOTTOM);
1004   _alias_types[AliasIdxRaw]-&gt;Init(AliasIdxRaw, TypeRawPtr::BOTTOM);
1005   _num_alias_types = AliasIdxRaw+1;
1006   // Zero out the alias type cache.
1007   Copy::zero_to_bytes(_alias_cache, sizeof(_alias_cache));
1008   // A NULL adr_type hits in the cache right away.  Preload the right answer.
1009   probe_alias_cache(NULL)-&gt;_index = AliasIdxTop;
1010 
1011   _intrinsics = NULL;
1012   _macro_nodes = new(comp_arena()) GrowableArray&lt;Node*&gt;(comp_arena(), 8,  0, NULL);
1013   _predicate_opaqs = new(comp_arena()) GrowableArray&lt;Node*&gt;(comp_arena(), 8,  0, NULL);
1014   _expensive_nodes = new(comp_arena()) GrowableArray&lt;Node*&gt;(comp_arena(), 8,  0, NULL);
1015   _range_check_casts = new(comp_arena()) GrowableArray&lt;Node*&gt;(comp_arena(), 8,  0, NULL);
1016   _opaque4_nodes = new(comp_arena()) GrowableArray&lt;Node*&gt;(comp_arena(), 8,  0, NULL);
<span class="line-modified">1017   _value_type_nodes = new(comp_arena()) GrowableArray&lt;Node*&gt;(comp_arena(), 8,  0, NULL);</span>
1018   register_library_intrinsics();
1019 #ifdef ASSERT
1020   _type_verify_symmetry = true;
1021 #endif
1022 }
1023 
1024 //---------------------------init_start----------------------------------------
1025 // Install the StartNode on this compile object.
1026 void Compile::init_start(StartNode* s) {
1027   if (failing())
1028     return; // already failing
1029   assert(s == start(), &quot;&quot;);
1030 }
1031 
1032 /**
1033  * Return the &#39;StartNode&#39;. We must not have a pending failure, since the ideal graph
1034  * can be in an inconsistent state, i.e., we can get segmentation faults when traversing
1035  * the ideal graph.
1036  */
1037 StartNode* Compile::start() const {
</pre>
<hr />
<pre>
1258   }
1259   if (ta &amp;&amp; ta-&gt;is_not_flat()) {
1260     // Erase not flat property for alias analysis.
1261     tj = ta = ta-&gt;cast_to_not_flat(false);
1262   }
1263   if (ta &amp;&amp; ta-&gt;is_not_null_free()) {
1264     // Erase not null free property for alias analysis.
1265     tj = ta = ta-&gt;cast_to_not_null_free(false);
1266   }
1267 
1268   if( ta &amp;&amp; is_known_inst ) {
1269     if ( offset != Type::OffsetBot &amp;&amp;
1270          offset &gt; arrayOopDesc::length_offset_in_bytes() ) {
1271       offset = Type::OffsetBot; // Flatten constant access into array body only
1272       tj = ta = TypeAryPtr::make(ptr, ta-&gt;ary(), ta-&gt;klass(), true, Type::Offset(offset), ta-&gt;field_offset(), ta-&gt;instance_id());
1273     }
1274   } else if( ta &amp;&amp; _AliasLevel &gt;= 2 ) {
1275     // For arrays indexed by constant indices, we flatten the alias
1276     // space to include all of the array body.  Only the header, klass
1277     // and array length can be accessed un-aliased.
<span class="line-modified">1278     // For flattened value type array, each field has its own slice so</span>
1279     // we must include the field offset.
1280     if( offset != Type::OffsetBot ) {
1281       if( ta-&gt;const_oop() ) { // MethodData* or Method*
1282         offset = Type::OffsetBot;   // Flatten constant access into array body
1283         tj = ta = TypeAryPtr::make(ptr,ta-&gt;const_oop(),ta-&gt;ary(),ta-&gt;klass(),false,Type::Offset(offset), ta-&gt;field_offset());
1284       } else if( offset == arrayOopDesc::length_offset_in_bytes() ) {
1285         // range is OK as-is.
1286         tj = ta = TypeAryPtr::RANGE;
1287       } else if( offset == oopDesc::klass_offset_in_bytes() ) {
1288         tj = TypeInstPtr::KLASS; // all klass loads look alike
1289         ta = TypeAryPtr::RANGE; // generic ignored junk
1290         ptr = TypePtr::BotPTR;
1291       } else if( offset == oopDesc::mark_offset_in_bytes() ) {
1292         tj = TypeInstPtr::MARK;
1293         ta = TypeAryPtr::RANGE; // generic ignored junk
1294         ptr = TypePtr::BotPTR;
1295       } else {                  // Random constant offset into array body
1296         offset = Type::OffsetBot;   // Flatten constant access into array body
1297         tj = ta = TypeAryPtr::make(ptr,ta-&gt;ary(),ta-&gt;klass(),false,Type::Offset(offset), ta-&gt;field_offset());
1298       }
1299     }
1300     // Arrays of fixed size alias with arrays of unknown size.
1301     if (ta-&gt;size() != TypeInt::POS) {
1302       const TypeAry *tary = TypeAry::make(ta-&gt;elem(), TypeInt::POS);
1303       tj = ta = TypeAryPtr::make(ptr,ta-&gt;const_oop(),tary,ta-&gt;klass(),false,Type::Offset(offset), ta-&gt;field_offset());
1304     }
1305     // Arrays of known objects become arrays of unknown objects.
1306     if (ta-&gt;elem()-&gt;isa_narrowoop() &amp;&amp; ta-&gt;elem() != TypeNarrowOop::BOTTOM) {
1307       const TypeAry *tary = TypeAry::make(TypeNarrowOop::BOTTOM, ta-&gt;size());
1308       tj = ta = TypeAryPtr::make(ptr,ta-&gt;const_oop(),tary,NULL,false,Type::Offset(offset), ta-&gt;field_offset());
1309     }
1310     if (ta-&gt;elem()-&gt;isa_oopptr() &amp;&amp; ta-&gt;elem() != TypeInstPtr::BOTTOM) {
1311       const TypeAry *tary = TypeAry::make(TypeInstPtr::BOTTOM, ta-&gt;size());
1312       tj = ta = TypeAryPtr::make(ptr,ta-&gt;const_oop(),tary,NULL,false,Type::Offset(offset), ta-&gt;field_offset());
1313     }
1314     // Initially all flattened array accesses share a single slice
<span class="line-modified">1315     if (ta-&gt;elem()-&gt;isa_valuetype() &amp;&amp; ta-&gt;elem() != TypeValueType::BOTTOM &amp;&amp; _flattened_accesses_share_alias) {</span>
<span class="line-modified">1316       const TypeAry *tary = TypeAry::make(TypeValueType::BOTTOM, ta-&gt;size());</span>
1317       tj = ta = TypeAryPtr::make(ptr,ta-&gt;const_oop(),tary,NULL,false,Type::Offset(offset), Type::Offset(Type::OffsetBot));
1318     }
1319     // Arrays of bytes and of booleans both use &#39;bastore&#39; and &#39;baload&#39; so
1320     // cannot be distinguished by bytecode alone.
1321     if (ta-&gt;elem() == TypeInt::BOOL) {
1322       const TypeAry *tary = TypeAry::make(TypeInt::BYTE, ta-&gt;size());
1323       ciKlass* aklass = ciTypeArrayKlass::make(T_BYTE);
1324       tj = ta = TypeAryPtr::make(ptr,ta-&gt;const_oop(),tary,aklass,false,Type::Offset(offset), ta-&gt;field_offset());
1325     }
1326     // During the 2nd round of IterGVN, NotNull castings are removed.
1327     // Make sure the Bottom and NotNull variants alias the same.
1328     // Also, make sure exact and non-exact variants alias the same.
1329     if (ptr == TypePtr::NotNull || ta-&gt;klass_is_exact() || ta-&gt;speculative() != NULL) {
1330       tj = ta = TypeAryPtr::make(TypePtr::BotPTR,ta-&gt;ary(),ta-&gt;klass(),false,Type::Offset(offset), ta-&gt;field_offset());
1331     }
1332   }
1333 
1334   // Oop pointers need some flattening
1335   const TypeInstPtr *to = tj-&gt;isa_instptr();
1336   if( to &amp;&amp; _AliasLevel &gt;= 2 &amp;&amp; to != TypeOopPtr::BOTTOM ) {
</pre>
<hr />
<pre>
1610     _alias_types[idx]-&gt;Init(idx, flat);
1611     if (flat == TypeInstPtr::KLASS)  alias_type(idx)-&gt;set_rewritable(false);
1612     if (flat == TypeAryPtr::RANGE)   alias_type(idx)-&gt;set_rewritable(false);
1613     if (flat-&gt;isa_instptr()) {
1614       if (flat-&gt;offset() == java_lang_Class::klass_offset()
1615           &amp;&amp; flat-&gt;is_instptr()-&gt;klass() == env()-&gt;Class_klass())
1616         alias_type(idx)-&gt;set_rewritable(false);
1617     }
1618     ciField* field = NULL;
1619     if (flat-&gt;isa_aryptr()) {
1620 #ifdef ASSERT
1621       const int header_size_min  = arrayOopDesc::base_offset_in_bytes(T_BYTE);
1622       // (T_BYTE has the weakest alignment and size restrictions...)
1623       assert(flat-&gt;offset() &lt; header_size_min, &quot;array body reference must be OffsetBot&quot;);
1624 #endif
1625       const Type* elemtype = flat-&gt;is_aryptr()-&gt;elem();
1626       if (flat-&gt;offset() == TypePtr::OffsetBot) {
1627         alias_type(idx)-&gt;set_element(elemtype);
1628       }
1629       int field_offset = flat-&gt;is_aryptr()-&gt;field_offset().get();
<span class="line-modified">1630       if (elemtype-&gt;isa_valuetype() &amp;&amp;</span>
<span class="line-modified">1631           elemtype-&gt;value_klass() != NULL &amp;&amp;</span>
1632           field_offset != Type::OffsetBot) {
<span class="line-modified">1633         ciValueKlass* vk = elemtype-&gt;value_klass();</span>
1634         field_offset += vk-&gt;first_field_offset();
1635         field = vk-&gt;get_field_by_offset(field_offset, false);
1636       }
1637     }
1638     if (flat-&gt;isa_klassptr()) {
1639       if (flat-&gt;offset() == in_bytes(Klass::super_check_offset_offset()))
1640         alias_type(idx)-&gt;set_rewritable(false);
1641       if (flat-&gt;offset() == in_bytes(Klass::modifier_flags_offset()))
1642         alias_type(idx)-&gt;set_rewritable(false);
1643       if (flat-&gt;offset() == in_bytes(Klass::access_flags_offset()))
1644         alias_type(idx)-&gt;set_rewritable(false);
1645       if (flat-&gt;offset() == in_bytes(Klass::java_mirror_offset()))
1646         alias_type(idx)-&gt;set_rewritable(false);
1647       if (flat-&gt;offset() == in_bytes(Klass::layout_helper_offset()))
1648         alias_type(idx)-&gt;set_rewritable(false);
1649       if (flat-&gt;offset() == in_bytes(Klass::secondary_super_cache_offset()))
1650         alias_type(idx)-&gt;set_rewritable(false);
1651     }
1652     // %%% (We would like to finalize JavaThread::threadObj_offset(),
1653     // but the base pointer type is not distinctive enough to identify
1654     // references into JavaThread.)
1655 
1656     // Check for final fields.
1657     const TypeInstPtr* tinst = flat-&gt;isa_instptr();
1658     if (tinst &amp;&amp; tinst-&gt;offset() &gt;= instanceOopDesc::base_offset_in_bytes()) {
1659       if (tinst-&gt;const_oop() != NULL &amp;&amp;
1660           tinst-&gt;klass() == ciEnv::current()-&gt;Class_klass() &amp;&amp;
1661           tinst-&gt;offset() &gt;= (tinst-&gt;klass()-&gt;as_instance_klass()-&gt;size_helper() * wordSize)) {
1662         // static field
1663         ciInstanceKlass* k = tinst-&gt;const_oop()-&gt;as_instance()-&gt;java_lang_Class_klass()-&gt;as_instance_klass();
1664         field = k-&gt;get_field_by_offset(tinst-&gt;offset(), true);
<span class="line-modified">1665       } else if (tinst-&gt;klass()-&gt;is_valuetype()) {</span>
<span class="line-modified">1666         // Value type field</span>
<span class="line-modified">1667         ciValueKlass* vk = tinst-&gt;value_klass();</span>
1668         field = vk-&gt;get_field_by_offset(tinst-&gt;offset(), false);
1669       } else {
1670         ciInstanceKlass* k = tinst-&gt;klass()-&gt;as_instance_klass();
1671         field = k-&gt;get_field_by_offset(tinst-&gt;offset(), false);
1672       }
1673     }
1674     assert(field == NULL ||
1675            original_field == NULL ||
1676            (field-&gt;holder() == original_field-&gt;holder() &amp;&amp;
1677             field-&gt;offset() == original_field-&gt;offset() &amp;&amp;
1678             field-&gt;is_static() == original_field-&gt;is_static()), &quot;wrong field?&quot;);
1679     // Set field() and is_rewritable() attributes.
1680     if (field != NULL) {
1681       alias_type(idx)-&gt;set_field(field);
1682       if (flat-&gt;isa_aryptr()) {
<span class="line-modified">1683         // Fields of flattened inline type arrays are rewritable although they are declared final</span>
<span class="line-modified">1684         assert(flat-&gt;is_aryptr()-&gt;elem()-&gt;isa_valuetype(), &quot;must be a flattened value array&quot;);</span>
1685         alias_type(idx)-&gt;set_rewritable(true);
1686       }
1687     }
1688   }
1689 
1690   // Fill the cache for next time.
1691   if (!uncached) {
1692     ace-&gt;_adr_type = adr_type;
1693     ace-&gt;_index    = idx;
1694     assert(alias_type(adr_type) == alias_type(idx),  &quot;type must be installed&quot;);
1695 
1696     // Might as well try to fill the cache for the flattened version, too.
1697     AliasCacheEntry* face = probe_alias_cache(flat);
1698     if (face-&gt;_adr_type == NULL) {
1699       face-&gt;_adr_type = flat;
1700       face-&gt;_index    = idx;
1701       assert(alias_type(flat) == alias_type(idx), &quot;flat type must work too&quot;);
1702     }
1703   }
1704 
</pre>
<hr />
<pre>
1844   }
1845   assert(range_check_cast_count() == 0, &quot;should be empty&quot;);
1846 }
1847 
1848 void Compile::add_opaque4_node(Node* n) {
1849   assert(n-&gt;Opcode() == Op_Opaque4, &quot;Opaque4 only&quot;);
1850   assert(!_opaque4_nodes-&gt;contains(n), &quot;duplicate entry in Opaque4 list&quot;);
1851   _opaque4_nodes-&gt;append(n);
1852 }
1853 
1854 // Remove all Opaque4 nodes.
1855 void Compile::remove_opaque4_nodes(PhaseIterGVN &amp;igvn) {
1856   for (int i = opaque4_count(); i &gt; 0; i--) {
1857     Node* opaq = opaque4_node(i-1);
1858     assert(opaq-&gt;Opcode() == Op_Opaque4, &quot;Opaque4 only&quot;);
1859     igvn.replace_node(opaq, opaq-&gt;in(2));
1860   }
1861   assert(opaque4_count() == 0, &quot;should be empty&quot;);
1862 }
1863 
<span class="line-modified">1864 void Compile::add_value_type(Node* n) {</span>
<span class="line-modified">1865   assert(n-&gt;is_ValueTypeBase(), &quot;unexpected node&quot;);</span>
<span class="line-modified">1866   if (_value_type_nodes != NULL) {</span>
<span class="line-modified">1867     _value_type_nodes-&gt;push(n);</span>
1868   }
1869 }
1870 
<span class="line-modified">1871 void Compile::remove_value_type(Node* n) {</span>
<span class="line-modified">1872   assert(n-&gt;is_ValueTypeBase(), &quot;unexpected node&quot;);</span>
<span class="line-modified">1873   if (_value_type_nodes != NULL &amp;&amp; _value_type_nodes-&gt;contains(n)) {</span>
<span class="line-modified">1874     _value_type_nodes-&gt;remove(n);</span>
1875   }
1876 }
1877 
<span class="line-modified">1878 // Does the return value keep otherwise useless value type allocations alive?</span>
1879 static bool return_val_keeps_allocations_alive(Node* ret_val) {
1880   ResourceMark rm;
1881   Unique_Node_List wq;
1882   wq.push(ret_val);
1883   bool some_allocations = false;
1884   for (uint i = 0; i &lt; wq.size(); i++) {
1885     Node* n = wq.at(i);
<span class="line-modified">1886     assert(!n-&gt;is_ValueType(), &quot;chain of value type nodes&quot;);</span>
1887     if (n-&gt;outcnt() &gt; 1) {
1888       // Some other use for the allocation
1889       return false;
<span class="line-modified">1890     } else if (n-&gt;is_ValueTypePtr()) {</span>
1891       wq.push(n-&gt;in(1));
1892     } else if (n-&gt;is_Phi()) {
1893       for (uint j = 1; j &lt; n-&gt;req(); j++) {
1894         wq.push(n-&gt;in(j));
1895       }
1896     } else if (n-&gt;is_CheckCastPP() &amp;&amp;
1897                n-&gt;in(1)-&gt;is_Proj() &amp;&amp;
1898                n-&gt;in(1)-&gt;in(0)-&gt;is_Allocate()) {
1899       some_allocations = true;
1900     }
1901   }
1902   return some_allocations;
1903 }
1904 
<span class="line-modified">1905 void Compile::process_value_types(PhaseIterGVN &amp;igvn, bool post_ea) {</span>
<span class="line-modified">1906   // Make value types scalar in safepoints</span>
<span class="line-modified">1907   for (int i = _value_type_nodes-&gt;length()-1; i &gt;= 0; i--) {</span>
<span class="line-modified">1908     ValueTypeBaseNode* vt = _value_type_nodes-&gt;at(i)-&gt;as_ValueTypeBase();</span>
1909     vt-&gt;make_scalar_in_safepoints(&amp;igvn);
1910   }
<span class="line-modified">1911   // Remove ValueTypePtr nodes only after EA to give scalar replacement a chance</span>
<span class="line-modified">1912   // to remove buffer allocations. ValueType nodes are kept until loop opts and</span>
<span class="line-modified">1913   // removed via ValueTypeNode::remove_redundant_allocations.</span>
1914   if (post_ea) {
<span class="line-modified">1915     while (_value_type_nodes-&gt;length() &gt; 0) {</span>
<span class="line-modified">1916       ValueTypeBaseNode* vt = _value_type_nodes-&gt;pop()-&gt;as_ValueTypeBase();</span>
<span class="line-modified">1917       if (vt-&gt;is_ValueTypePtr()) {</span>
1918         igvn.replace_node(vt, vt-&gt;get_oop());
1919       }
1920     }
1921   }
1922   // Make sure that the return value does not keep an unused allocation alive
<span class="line-modified">1923   if (tf()-&gt;returns_value_type_as_fields()) {</span>
1924     Node* ret = NULL;
1925     for (uint i = 1; i &lt; root()-&gt;req(); i++){
1926       Node* in = root()-&gt;in(i);
1927       if (in-&gt;Opcode() == Op_Return) {
1928         assert(ret == NULL, &quot;only one return&quot;);
1929         ret = in;
1930       }
1931     }
1932     if (ret != NULL) {
1933       Node* ret_val = ret-&gt;in(TypeFunc::Parms);
1934       if (igvn.type(ret_val)-&gt;isa_oopptr() &amp;&amp;
1935           return_val_keeps_allocations_alive(ret_val)) {
<span class="line-modified">1936         igvn.replace_input_of(ret, TypeFunc::Parms, ValueTypeNode::tagged_klass(igvn.type(ret_val)-&gt;value_klass(), igvn));</span>
1937         assert(ret_val-&gt;outcnt() == 0, &quot;should be dead now&quot;);
1938         igvn.remove_dead_node(ret_val);
1939       }
1940     }
1941   }
1942   igvn.optimize();
1943 }
1944 
1945 void Compile::adjust_flattened_array_access_aliases(PhaseIterGVN&amp; igvn) {
1946   if (!_has_flattened_accesses) {
1947     return;
1948   }
1949   // Initially, all flattened array accesses share the same slice to
1950   // keep dependencies with Object[] array accesses (that could be
1951   // to a flattened array) correct. We&#39;re done with parsing so we
1952   // now know all flattened array accesses in this compile
1953   // unit. Let&#39;s move flattened array accesses to their own slice,
1954   // one per element field. This should help memory access
1955   // optimizations.
1956   ResourceMark rm;
1957   Unique_Node_List wq;
1958   wq.push(root());
1959 
1960   Node_List mergememnodes;
1961   Node_List memnodes;
1962 
1963   // Alias index currently shared by all flattened memory accesses
<span class="line-modified">1964   int index = get_alias_index(TypeAryPtr::VALUES);</span>
1965 
1966   // Find MergeMem nodes and flattened array accesses
1967   for (uint i = 0; i &lt; wq.size(); i++) {
1968     Node* n = wq.at(i);
1969     if (n-&gt;is_Mem()) {
1970       const TypePtr* adr_type = NULL;
1971       if (n-&gt;Opcode() == Op_StoreCM) {
1972         adr_type = get_adr_type(get_alias_index(n-&gt;in(MemNode::OopStore)-&gt;adr_type()));
1973       } else {
1974         adr_type = get_adr_type(get_alias_index(n-&gt;adr_type()));
1975       }
<span class="line-modified">1976       if (adr_type == TypeAryPtr::VALUES) {</span>
1977         memnodes.push(n);
1978       }
1979     } else if (n-&gt;is_MergeMem()) {
1980       MergeMemNode* mm = n-&gt;as_MergeMem();
1981       if (mm-&gt;memory_at(index) != mm-&gt;base_memory()) {
1982         mergememnodes.push(n);
1983       }
1984     }
1985     for (uint j = 0; j &lt; n-&gt;req(); j++) {
1986       Node* m = n-&gt;in(j);
1987       if (m != NULL) {
1988         wq.push(m);
1989       }
1990     }
1991   }
1992 
1993   if (memnodes.size() &gt; 0) {
1994     _flattened_accesses_share_alias = false;
1995 
1996     // We are going to change the slice for the flattened array
1997     // accesses so we need to clear the cache entries that refer to
1998     // them.
1999     for (uint i = 0; i &lt; AliasCacheSize; i++) {
2000       AliasCacheEntry* ace = &amp;_alias_cache[i];
2001       if (ace-&gt;_adr_type != NULL &amp;&amp;
2002           ace-&gt;_adr_type-&gt;isa_aryptr() &amp;&amp;
<span class="line-modified">2003           ace-&gt;_adr_type-&gt;is_aryptr()-&gt;elem()-&gt;isa_valuetype()) {</span>
2004         ace-&gt;_adr_type = NULL;
2005         ace-&gt;_index = (i != 0) ? 0 : AliasIdxTop; // Make sure the NULL adr_type resolves to AliasIdxTop
2006       }
2007     }
2008 
2009     // Find what aliases we are going to add
2010     int start_alias = num_alias_types()-1;
2011     int stop_alias = 0;
2012 
2013     for (uint i = 0; i &lt; memnodes.size(); i++) {
2014       Node* m = memnodes.at(i);
2015       const TypePtr* adr_type = NULL;
2016       if (m-&gt;Opcode() == Op_StoreCM) {
2017         adr_type = m-&gt;in(MemNode::OopStore)-&gt;adr_type();
2018         Node* clone = new StoreCMNode(m-&gt;in(MemNode::Control), m-&gt;in(MemNode::Memory), m-&gt;in(MemNode::Address),
2019                                       m-&gt;adr_type(), m-&gt;in(MemNode::ValueIn), m-&gt;in(MemNode::OopStore),
2020                                       get_alias_index(adr_type));
2021         igvn.register_new_node_with_optimizer(clone);
2022         igvn.replace_node(m, clone);
2023       } else {
</pre>
<hr />
<pre>
2033 
2034     assert(stop_alias &gt;= start_alias, &quot;should have expanded aliases&quot;);
2035 
2036     Node_Stack stack(0);
2037 #ifdef ASSERT
2038     VectorSet seen(Thread::current()-&gt;resource_area());
2039 #endif
2040     // Now let&#39;s fix the memory graph so each flattened array access
2041     // is moved to the right slice. Start from the MergeMem nodes.
2042     uint last = unique();
2043     for (uint i = 0; i &lt; mergememnodes.size(); i++) {
2044       MergeMemNode* current = mergememnodes.at(i)-&gt;as_MergeMem();
2045       Node* n = current-&gt;memory_at(index);
2046       MergeMemNode* mm = NULL;
2047       do {
2048         // Follow memory edges through memory accesses, phis and
2049         // narrow membars and push nodes on the stack. Once we hit
2050         // bottom memory, we pop element off the stack one at a
2051         // time, in reverse order, and move them to the right slice
2052         // by changing their memory edges.
<span class="line-modified">2053         if ((n-&gt;is_Phi() &amp;&amp; n-&gt;adr_type() != TypePtr::BOTTOM) || n-&gt;is_Mem() || n-&gt;adr_type() == TypeAryPtr::VALUES) {</span>
2054           assert(!seen.test_set(n-&gt;_idx), &quot;&quot;);
2055           // Uses (a load for instance) will need to be moved to the
2056           // right slice as well and will get a new memory state
2057           // that we don&#39;t know yet. The use could also be the
2058           // backedge of a loop. We put a place holder node between
2059           // the memory node and its uses. We replace that place
2060           // holder with the correct memory state once we know it,
2061           // i.e. when nodes are popped off the stack. Using the
2062           // place holder make the logic work in the presence of
2063           // loops.
2064           if (n-&gt;outcnt() &gt; 1) {
2065             Node* place_holder = NULL;
2066             assert(!n-&gt;has_out_with(Op_Node), &quot;&quot;);
2067             for (DUIterator k = n-&gt;outs(); n-&gt;has_out(k); k++) {
2068               Node* u = n-&gt;out(k);
2069               if (u != current &amp;&amp; u-&gt;_idx &lt; last) {
2070                 bool success = false;
2071                 for (uint l = 0; l &lt; u-&gt;req(); l++) {
2072                   if (!stack.is_empty() &amp;&amp; u == stack.node() &amp;&amp; l == stack.index()) {
2073                     continue;
</pre>
<hr />
<pre>
2106           // nodes.
2107           mm = MergeMemNode::make(n);
2108           igvn.register_new_node_with_optimizer(mm);
2109           while (stack.size() &gt; 0) {
2110             Node* m = stack.node();
2111             uint idx = stack.index();
2112             if (m-&gt;is_Mem()) {
2113               // Move memory node to its new slice
2114               const TypePtr* adr_type = m-&gt;adr_type();
2115               int alias = get_alias_index(adr_type);
2116               Node* prev = mm-&gt;memory_at(alias);
2117               igvn.replace_input_of(m, MemNode::Memory, prev);
2118               mm-&gt;set_memory_at(alias, m);
2119             } else if (m-&gt;is_Phi()) {
2120               // We need as many new phis as there are new aliases
2121               igvn.replace_input_of(m, idx, mm);
2122               if (idx == m-&gt;req()-1) {
2123                 Node* r = m-&gt;in(0);
2124                 for (uint j = (uint)start_alias; j &lt;= (uint)stop_alias; j++) {
2125                   const Type* adr_type = get_adr_type(j);
<span class="line-modified">2126                   if (!adr_type-&gt;isa_aryptr() || !adr_type-&gt;is_aryptr()-&gt;elem()-&gt;isa_valuetype()) {</span>
2127                     continue;
2128                   }
2129                   Node* phi = new PhiNode(r, Type::MEMORY, get_adr_type(j));
2130                   igvn.register_new_node_with_optimizer(phi);
2131                   for (uint k = 1; k &lt; m-&gt;req(); k++) {
2132                     phi-&gt;init_req(k, m-&gt;in(k)-&gt;as_MergeMem()-&gt;memory_at(j));
2133                   }
2134                   mm-&gt;set_memory_at(j, phi);
2135                 }
2136                 Node* base_phi = new PhiNode(r, Type::MEMORY, TypePtr::BOTTOM);
2137                 igvn.register_new_node_with_optimizer(base_phi);
2138                 for (uint k = 1; k &lt; m-&gt;req(); k++) {
2139                   base_phi-&gt;init_req(k, m-&gt;in(k)-&gt;as_MergeMem()-&gt;base_memory());
2140                 }
2141                 mm-&gt;set_base_memory(base_phi);
2142               }
2143             } else {
2144               // This is a MemBarCPUOrder node from
2145               // Parse::array_load()/Parse::array_store(), in the
2146               // branch that handles flattened arrays hidden under
2147               // an Object[] array. We also need one new membar per
2148               // new alias to keep the unknown access that the
2149               // membars protect properly ordered with accesses to
2150               // known flattened array.
2151               assert(m-&gt;is_Proj(), &quot;projection expected&quot;);
2152               Node* ctrl = m-&gt;in(0)-&gt;in(TypeFunc::Control);
2153               igvn.replace_input_of(m-&gt;in(0), TypeFunc::Control, top());
2154               for (uint j = (uint)start_alias; j &lt;= (uint)stop_alias; j++) {
2155                 const Type* adr_type = get_adr_type(j);
<span class="line-modified">2156                 if (!adr_type-&gt;isa_aryptr() || !adr_type-&gt;is_aryptr()-&gt;elem()-&gt;isa_valuetype()) {</span>
2157                   continue;
2158                 }
2159                 MemBarNode* mb = new MemBarCPUOrderNode(this, j, NULL);
2160                 igvn.register_new_node_with_optimizer(mb);
2161                 Node* mem = mm-&gt;memory_at(j);
2162                 mb-&gt;init_req(TypeFunc::Control, ctrl);
2163                 mb-&gt;init_req(TypeFunc::Memory, mem);
2164                 ctrl = new ProjNode(mb, TypeFunc::Control);
2165                 igvn.register_new_node_with_optimizer(ctrl);
2166                 mem = new ProjNode(mb, TypeFunc::Memory);
2167                 igvn.register_new_node_with_optimizer(mem);
2168                 mm-&gt;set_memory_at(j, mem);
2169               }
2170               igvn.replace_node(m-&gt;in(0)-&gt;as_Multi()-&gt;proj_out(TypeFunc::Control), ctrl);
2171             }
2172             if (idx &lt; m-&gt;req()-1) {
2173               idx += 1;
2174               stack.set_index(idx);
2175               n = m-&gt;in(idx);
2176               break;
</pre>
<hr />
<pre>
2179             if (m-&gt;has_out_with(Op_Node)) {
2180               Node* place_holder = m-&gt;find_out_with(Op_Node);
2181               if (place_holder != NULL) {
2182                 Node* mm_clone = mm-&gt;clone();
2183                 igvn.register_new_node_with_optimizer(mm_clone);
2184                 Node* hook = new Node(1);
2185                 hook-&gt;init_req(0, mm);
2186                 igvn.replace_node(place_holder, mm_clone);
2187                 hook-&gt;destruct();
2188               }
2189               assert(!m-&gt;has_out_with(Op_Node), &quot;place holder should be gone now&quot;);
2190             }
2191             stack.pop();
2192           }
2193         }
2194       } while(stack.size() &gt; 0);
2195       // Fix the memory state at the MergeMem we started from
2196       igvn.rehash_node_delayed(current);
2197       for (uint j = (uint)start_alias; j &lt;= (uint)stop_alias; j++) {
2198         const Type* adr_type = get_adr_type(j);
<span class="line-modified">2199         if (!adr_type-&gt;isa_aryptr() || !adr_type-&gt;is_aryptr()-&gt;elem()-&gt;isa_valuetype()) {</span>
2200           continue;
2201         }
2202         current-&gt;set_memory_at(j, mm);
2203       }
2204       current-&gt;set_memory_at(index, current-&gt;base_memory());
2205     }
2206     igvn.optimize();
2207   }
<span class="line-modified">2208   print_method(PHASE_SPLIT_VALUES_ARRAY, 2);</span>
2209 }
2210 
2211 
2212 // StringOpts and late inlining of string methods
2213 void Compile::inline_string_calls(bool parse_time) {
2214   {
2215     // remove useless nodes to make the usage analysis simpler
2216     ResourceMark rm;
2217     PhaseRemoveUseless pru(initial_gvn(), for_igvn());
2218   }
2219 
2220   {
2221     ResourceMark rm;
2222     print_method(PHASE_BEFORE_STRINGOPTS, 3);
2223     PhaseStringOpts pso(initial_gvn(), for_igvn());
2224     print_method(PHASE_AFTER_STRINGOPTS, 3);
2225   }
2226 
2227   // now inline anything that we skipped the first time around
2228   if (!parse_time) {
</pre>
<hr />
<pre>
2471   remove_speculative_types(igvn);
2472 
2473   // No more new expensive nodes will be added to the list from here
2474   // so keep only the actual candidates for optimizations.
2475   cleanup_expensive_nodes(igvn);
2476 
2477   if (!failing() &amp;&amp; RenumberLiveNodes &amp;&amp; live_nodes() + NodeLimitFudgeFactor &lt; unique()) {
2478     Compile::TracePhase tp(&quot;&quot;, &amp;timers[_t_renumberLive]);
2479     initial_gvn()-&gt;replace_with(&amp;igvn);
2480     for_igvn()-&gt;clear();
2481     Unique_Node_List new_worklist(C-&gt;comp_arena());
2482     {
2483       ResourceMark rm;
2484       PhaseRenumberLive prl = PhaseRenumberLive(initial_gvn(), for_igvn(), &amp;new_worklist);
2485     }
2486     set_for_igvn(&amp;new_worklist);
2487     igvn = PhaseIterGVN(initial_gvn());
2488     igvn.optimize();
2489   }
2490 
<span class="line-modified">2491   if (_value_type_nodes-&gt;length() &gt; 0) {</span>
2492     // Do this once all inlining is over to avoid getting inconsistent debug info
<span class="line-modified">2493     process_value_types(igvn);</span>
2494   }
2495 
2496   adjust_flattened_array_access_aliases(igvn);
2497 
2498   // Perform escape analysis
2499   if (_do_escape_analysis &amp;&amp; ConnectionGraph::has_candidates(this)) {
2500     if (has_loops()) {
2501       // Cleanup graph (remove dead nodes).
2502       TracePhase tp(&quot;idealLoop&quot;, &amp;timers[_t_idealLoop]);
2503       PhaseIdealLoop::optimize(igvn, LoopOptsMaxUnroll);
2504       if (major_progress()) print_method(PHASE_PHASEIDEAL_BEFORE_EA, 2);
2505       if (failing())  return;
2506     }
2507     ConnectionGraph::do_analysis(this, &amp;igvn);
2508 
2509     if (failing())  return;
2510 
2511     // Optimize out fields loads from scalar replaceable allocations.
2512     igvn.optimize();
2513     print_method(PHASE_ITER_GVN_AFTER_EA, 2);
2514 
2515     if (failing())  return;
2516 
2517     if (congraph() != NULL &amp;&amp; macro_count() &gt; 0) {
2518       TracePhase tp(&quot;macroEliminate&quot;, &amp;timers[_t_macroEliminate]);
2519       PhaseMacroExpand mexp(igvn);
2520       mexp.eliminate_macro_nodes();
2521       igvn.set_delay_transform(false);
2522 
2523       igvn.optimize();
2524       print_method(PHASE_ITER_GVN_AFTER_ELIMINATION, 2);
2525 
2526       if (failing())  return;
2527     }
2528   }
2529 
<span class="line-modified">2530   if (_value_type_nodes-&gt;length() &gt; 0) {</span>
<span class="line-modified">2531     // Process value types again now that EA might have simplified the graph</span>
<span class="line-modified">2532     process_value_types(igvn, /* post_ea= */ true);</span>
2533   }
2534 
2535   // Loop transforms on the ideal graph.  Range Check Elimination,
2536   // peeling, unrolling, etc.
2537 
2538   // Set loop opts counter
2539   if((_loop_opts_cnt &gt; 0) &amp;&amp; (has_loops() || has_split_ifs())) {
2540     {
2541       TracePhase tp(&quot;idealLoop&quot;, &amp;timers[_t_idealLoop]);
2542       PhaseIdealLoop::optimize(igvn, LoopOptsDefault);
2543       _loop_opts_cnt--;
2544       if (major_progress()) print_method(PHASE_PHASEIDEALLOOP1, 2);
2545       if (failing())  return;
2546     }
2547     // Loop opts pass if partial peeling occurred in previous pass
2548     if(PartialPeelLoop &amp;&amp; major_progress() &amp;&amp; (_loop_opts_cnt &gt; 0)) {
2549       TracePhase tp(&quot;idealLoop&quot;, &amp;timers[_t_idealLoop]);
2550       PhaseIdealLoop::optimize(igvn, LoopOptsSkipSplitIf);
2551       _loop_opts_cnt--;
2552       if (major_progress()) print_method(PHASE_PHASEIDEALLOOP2, 2);
</pre>
<hr />
<pre>
3897           k-&gt;subsume_by(m, this);
3898         }
3899       }
3900     }
3901     break;
3902   }
3903   case Op_CmpUL: {
3904     if (!Matcher::has_match_rule(Op_CmpUL)) {
3905       // No support for unsigned long comparisons
3906       ConINode* sign_pos = new ConINode(TypeInt::make(BitsPerLong - 1));
3907       Node* sign_bit_mask = new RShiftLNode(n-&gt;in(1), sign_pos);
3908       Node* orl = new OrLNode(n-&gt;in(1), sign_bit_mask);
3909       ConLNode* remove_sign_mask = new ConLNode(TypeLong::make(max_jlong));
3910       Node* andl = new AndLNode(orl, remove_sign_mask);
3911       Node* cmp = new CmpLNode(andl, n-&gt;in(2));
3912       n-&gt;subsume_by(cmp, this);
3913     }
3914     break;
3915   }
3916 #ifdef ASSERT
<span class="line-modified">3917   case Op_ValueTypePtr:</span>
<span class="line-modified">3918   case Op_ValueType: {</span>
3919     n-&gt;dump(-1);
<span class="line-modified">3920     assert(false, &quot;value type node was not removed&quot;);</span>
3921     break;
3922   }
3923 #endif
3924   default:
3925     assert(!n-&gt;is_Call(), &quot;&quot;);
3926     assert(!n-&gt;is_Mem(), &quot;&quot;);
3927     assert(nop != Op_ProfileBoolean, &quot;should be eliminated during IGVN&quot;);
3928     break;
3929   }
3930 }
3931 
3932 //------------------------------final_graph_reshaping_walk---------------------
3933 // Replacing Opaque nodes with their input in final_graph_reshaping_impl(),
3934 // requires that the walk visits a node&#39;s inputs before visiting the node.
3935 void Compile::final_graph_reshaping_walk( Node_Stack &amp;nstack, Node *root, Final_Reshape_Counts &amp;frc ) {
3936   ResourceArea *area = Thread::current()-&gt;resource_area();
3937   Unique_Node_List sfpt(area);
3938 
3939   frc._visited.set(root-&gt;_idx); // first, mark node as visited
3940   uint cnt = root-&gt;req();
</pre>
<hr />
<pre>
4861         t = n-&gt;as_Type()-&gt;type();
4862         assert(t == t-&gt;remove_speculative(), &quot;no more speculative types&quot;);
4863       }
4864       uint max = n-&gt;len();
4865       for( uint i = 0; i &lt; max; ++i ) {
4866         Node *m = n-&gt;in(i);
4867         if (not_a_node(m))  continue;
4868         worklist.push(m);
4869       }
4870     }
4871     igvn.check_no_speculative_types();
4872 #endif
4873   }
4874 }
4875 
4876 Node* Compile::optimize_acmp(PhaseGVN* phase, Node* a, Node* b) {
4877   const TypeInstPtr* ta = phase-&gt;type(a)-&gt;isa_instptr();
4878   const TypeInstPtr* tb = phase-&gt;type(b)-&gt;isa_instptr();
4879   if (!EnableValhalla || ta == NULL || tb == NULL ||
4880       ta-&gt;is_zero_type() || tb-&gt;is_zero_type() ||
<span class="line-modified">4881       !ta-&gt;can_be_value_type() || !tb-&gt;can_be_value_type()) {</span>
<span class="line-modified">4882     // Use old acmp if one operand is null or not a value type</span>
4883     return new CmpPNode(a, b);
<span class="line-modified">4884   } else if (ta-&gt;is_valuetypeptr() || tb-&gt;is_valuetypeptr()) {</span>
<span class="line-modified">4885     // We know that one operand is a value type. Therefore,</span>
4886     // new acmp will only return true if both operands are NULL.
4887     // Check if both operands are null by or&#39;ing the oops.
4888     a = phase-&gt;transform(new CastP2XNode(NULL, a));
4889     b = phase-&gt;transform(new CastP2XNode(NULL, b));
4890     a = phase-&gt;transform(new OrXNode(a, b));
4891     return new CmpXNode(a, phase-&gt;MakeConX(0));
4892   }
4893   // Use new acmp
4894   return NULL;
4895 }
4896 
4897 // Auxiliary method to support randomized stressing/fuzzing.
4898 //
4899 // This method can be called the arbitrary number of times, with current count
4900 // as the argument. The logic allows selecting a single candidate from the
4901 // running list of candidates as follows:
4902 //    int count = 0;
4903 //    Cand* selected = null;
4904 //    while(cand = cand-&gt;next()) {
4905 //      if (randomized_select(++count)) {
</pre>
</td>
<td>
<hr />
<pre>
  34 #include &quot;compiler/disassembler.hpp&quot;
  35 #include &quot;compiler/oopMap.hpp&quot;
  36 #include &quot;gc/shared/barrierSet.hpp&quot;
  37 #include &quot;gc/shared/c2/barrierSetC2.hpp&quot;
  38 #include &quot;jfr/jfrEvents.hpp&quot;
  39 #include &quot;memory/resourceArea.hpp&quot;
  40 #include &quot;opto/addnode.hpp&quot;
  41 #include &quot;opto/block.hpp&quot;
  42 #include &quot;opto/c2compiler.hpp&quot;
  43 #include &quot;opto/callGenerator.hpp&quot;
  44 #include &quot;opto/callnode.hpp&quot;
  45 #include &quot;opto/castnode.hpp&quot;
  46 #include &quot;opto/cfgnode.hpp&quot;
  47 #include &quot;opto/chaitin.hpp&quot;
  48 #include &quot;opto/compile.hpp&quot;
  49 #include &quot;opto/connode.hpp&quot;
  50 #include &quot;opto/convertnode.hpp&quot;
  51 #include &quot;opto/divnode.hpp&quot;
  52 #include &quot;opto/escape.hpp&quot;
  53 #include &quot;opto/idealGraphPrinter.hpp&quot;
<span class="line-added">  54 #include &quot;opto/inlinetypenode.hpp&quot;</span>
  55 #include &quot;opto/loopnode.hpp&quot;
  56 #include &quot;opto/machnode.hpp&quot;
  57 #include &quot;opto/macro.hpp&quot;
  58 #include &quot;opto/matcher.hpp&quot;
  59 #include &quot;opto/mathexactnode.hpp&quot;
  60 #include &quot;opto/memnode.hpp&quot;
  61 #include &quot;opto/mulnode.hpp&quot;
  62 #include &quot;opto/narrowptrnode.hpp&quot;
  63 #include &quot;opto/node.hpp&quot;
  64 #include &quot;opto/opcodes.hpp&quot;
  65 #include &quot;opto/output.hpp&quot;
  66 #include &quot;opto/parse.hpp&quot;
  67 #include &quot;opto/phaseX.hpp&quot;
  68 #include &quot;opto/rootnode.hpp&quot;
  69 #include &quot;opto/runtime.hpp&quot;
  70 #include &quot;opto/stringopts.hpp&quot;
  71 #include &quot;opto/type.hpp&quot;

  72 #include &quot;opto/vectornode.hpp&quot;
  73 #include &quot;runtime/arguments.hpp&quot;
  74 #include &quot;runtime/sharedRuntime.hpp&quot;
  75 #include &quot;runtime/signature.hpp&quot;
  76 #include &quot;runtime/stubRoutines.hpp&quot;
  77 #include &quot;runtime/timer.hpp&quot;
  78 #include &quot;utilities/align.hpp&quot;
  79 #include &quot;utilities/copy.hpp&quot;
  80 #include &quot;utilities/macros.hpp&quot;
  81 #include &quot;utilities/resourceHash.hpp&quot;
  82 
  83 
  84 // -------------------- Compile::mach_constant_base_node -----------------------
  85 // Constant table base node singleton.
  86 MachConstantBaseNode* Compile::mach_constant_base_node() {
  87   if (_mach_constant_base_node == NULL) {
  88     _mach_constant_base_node = new MachConstantBaseNode();
  89     _mach_constant_base_node-&gt;add_req(C-&gt;root());
  90   }
  91   return _mach_constant_base_node;
</pre>
<hr />
<pre>
 389   for (int i = range_check_cast_count() - 1; i &gt;= 0; i--) {
 390     Node* cast = range_check_cast_node(i);
 391     if (!useful.member(cast)) {
 392       remove_range_check_cast(cast);
 393     }
 394   }
 395   // Remove useless expensive nodes
 396   for (int i = C-&gt;expensive_count()-1; i &gt;= 0; i--) {
 397     Node* n = C-&gt;expensive_node(i);
 398     if (!useful.member(n)) {
 399       remove_expensive_node(n);
 400     }
 401   }
 402   // Remove useless Opaque4 nodes
 403   for (int i = opaque4_count() - 1; i &gt;= 0; i--) {
 404     Node* opaq = opaque4_node(i);
 405     if (!useful.member(opaq)) {
 406       remove_opaque4_node(opaq);
 407     }
 408   }
<span class="line-modified"> 409   // Remove useless inline type nodes</span>
<span class="line-modified"> 410   for (int i = _inline_type_nodes-&gt;length() - 1; i &gt;= 0; i--) {</span>
<span class="line-modified"> 411     Node* vt = _inline_type_nodes-&gt;at(i);</span>
 412     if (!useful.member(vt)) {
<span class="line-modified"> 413       _inline_type_nodes-&gt;remove(vt);</span>
 414     }
 415   }
 416   BarrierSetC2* bs = BarrierSet::barrier_set()-&gt;barrier_set_c2();
 417   bs-&gt;eliminate_useless_gc_barriers(useful, this);
 418   // clean up the late inline lists
 419   remove_useless_late_inlines(&amp;_string_late_inlines, useful);
 420   remove_useless_late_inlines(&amp;_boxing_late_inlines, useful);
 421   remove_useless_late_inlines(&amp;_late_inlines, useful);
 422   debug_only(verify_graph_edges(true/*check for no_dead_code*/);)
 423 }
 424 
 425 // ============================================================================
 426 //------------------------------CompileWrapper---------------------------------
 427 class CompileWrapper : public StackObj {
 428   Compile *const _compile;
 429  public:
 430   CompileWrapper(Compile* compile);
 431 
 432   ~CompileWrapper();
 433 };
</pre>
<hr />
<pre>
 997   Copy::zero_to_bytes(ats, sizeof(AliasType)*grow_ats);
 998   {
 999     for (int i = 0; i &lt; grow_ats; i++)  _alias_types[i] = &amp;ats[i];
1000   }
1001   // Initialize the first few types.
1002   _alias_types[AliasIdxTop]-&gt;Init(AliasIdxTop, NULL);
1003   _alias_types[AliasIdxBot]-&gt;Init(AliasIdxBot, TypePtr::BOTTOM);
1004   _alias_types[AliasIdxRaw]-&gt;Init(AliasIdxRaw, TypeRawPtr::BOTTOM);
1005   _num_alias_types = AliasIdxRaw+1;
1006   // Zero out the alias type cache.
1007   Copy::zero_to_bytes(_alias_cache, sizeof(_alias_cache));
1008   // A NULL adr_type hits in the cache right away.  Preload the right answer.
1009   probe_alias_cache(NULL)-&gt;_index = AliasIdxTop;
1010 
1011   _intrinsics = NULL;
1012   _macro_nodes = new(comp_arena()) GrowableArray&lt;Node*&gt;(comp_arena(), 8,  0, NULL);
1013   _predicate_opaqs = new(comp_arena()) GrowableArray&lt;Node*&gt;(comp_arena(), 8,  0, NULL);
1014   _expensive_nodes = new(comp_arena()) GrowableArray&lt;Node*&gt;(comp_arena(), 8,  0, NULL);
1015   _range_check_casts = new(comp_arena()) GrowableArray&lt;Node*&gt;(comp_arena(), 8,  0, NULL);
1016   _opaque4_nodes = new(comp_arena()) GrowableArray&lt;Node*&gt;(comp_arena(), 8,  0, NULL);
<span class="line-modified">1017   _inline_type_nodes = new(comp_arena()) GrowableArray&lt;Node*&gt;(comp_arena(), 8,  0, NULL);</span>
1018   register_library_intrinsics();
1019 #ifdef ASSERT
1020   _type_verify_symmetry = true;
1021 #endif
1022 }
1023 
1024 //---------------------------init_start----------------------------------------
1025 // Install the StartNode on this compile object.
1026 void Compile::init_start(StartNode* s) {
1027   if (failing())
1028     return; // already failing
1029   assert(s == start(), &quot;&quot;);
1030 }
1031 
1032 /**
1033  * Return the &#39;StartNode&#39;. We must not have a pending failure, since the ideal graph
1034  * can be in an inconsistent state, i.e., we can get segmentation faults when traversing
1035  * the ideal graph.
1036  */
1037 StartNode* Compile::start() const {
</pre>
<hr />
<pre>
1258   }
1259   if (ta &amp;&amp; ta-&gt;is_not_flat()) {
1260     // Erase not flat property for alias analysis.
1261     tj = ta = ta-&gt;cast_to_not_flat(false);
1262   }
1263   if (ta &amp;&amp; ta-&gt;is_not_null_free()) {
1264     // Erase not null free property for alias analysis.
1265     tj = ta = ta-&gt;cast_to_not_null_free(false);
1266   }
1267 
1268   if( ta &amp;&amp; is_known_inst ) {
1269     if ( offset != Type::OffsetBot &amp;&amp;
1270          offset &gt; arrayOopDesc::length_offset_in_bytes() ) {
1271       offset = Type::OffsetBot; // Flatten constant access into array body only
1272       tj = ta = TypeAryPtr::make(ptr, ta-&gt;ary(), ta-&gt;klass(), true, Type::Offset(offset), ta-&gt;field_offset(), ta-&gt;instance_id());
1273     }
1274   } else if( ta &amp;&amp; _AliasLevel &gt;= 2 ) {
1275     // For arrays indexed by constant indices, we flatten the alias
1276     // space to include all of the array body.  Only the header, klass
1277     // and array length can be accessed un-aliased.
<span class="line-modified">1278     // For flattened inline type array, each field has its own slice so</span>
1279     // we must include the field offset.
1280     if( offset != Type::OffsetBot ) {
1281       if( ta-&gt;const_oop() ) { // MethodData* or Method*
1282         offset = Type::OffsetBot;   // Flatten constant access into array body
1283         tj = ta = TypeAryPtr::make(ptr,ta-&gt;const_oop(),ta-&gt;ary(),ta-&gt;klass(),false,Type::Offset(offset), ta-&gt;field_offset());
1284       } else if( offset == arrayOopDesc::length_offset_in_bytes() ) {
1285         // range is OK as-is.
1286         tj = ta = TypeAryPtr::RANGE;
1287       } else if( offset == oopDesc::klass_offset_in_bytes() ) {
1288         tj = TypeInstPtr::KLASS; // all klass loads look alike
1289         ta = TypeAryPtr::RANGE; // generic ignored junk
1290         ptr = TypePtr::BotPTR;
1291       } else if( offset == oopDesc::mark_offset_in_bytes() ) {
1292         tj = TypeInstPtr::MARK;
1293         ta = TypeAryPtr::RANGE; // generic ignored junk
1294         ptr = TypePtr::BotPTR;
1295       } else {                  // Random constant offset into array body
1296         offset = Type::OffsetBot;   // Flatten constant access into array body
1297         tj = ta = TypeAryPtr::make(ptr,ta-&gt;ary(),ta-&gt;klass(),false,Type::Offset(offset), ta-&gt;field_offset());
1298       }
1299     }
1300     // Arrays of fixed size alias with arrays of unknown size.
1301     if (ta-&gt;size() != TypeInt::POS) {
1302       const TypeAry *tary = TypeAry::make(ta-&gt;elem(), TypeInt::POS);
1303       tj = ta = TypeAryPtr::make(ptr,ta-&gt;const_oop(),tary,ta-&gt;klass(),false,Type::Offset(offset), ta-&gt;field_offset());
1304     }
1305     // Arrays of known objects become arrays of unknown objects.
1306     if (ta-&gt;elem()-&gt;isa_narrowoop() &amp;&amp; ta-&gt;elem() != TypeNarrowOop::BOTTOM) {
1307       const TypeAry *tary = TypeAry::make(TypeNarrowOop::BOTTOM, ta-&gt;size());
1308       tj = ta = TypeAryPtr::make(ptr,ta-&gt;const_oop(),tary,NULL,false,Type::Offset(offset), ta-&gt;field_offset());
1309     }
1310     if (ta-&gt;elem()-&gt;isa_oopptr() &amp;&amp; ta-&gt;elem() != TypeInstPtr::BOTTOM) {
1311       const TypeAry *tary = TypeAry::make(TypeInstPtr::BOTTOM, ta-&gt;size());
1312       tj = ta = TypeAryPtr::make(ptr,ta-&gt;const_oop(),tary,NULL,false,Type::Offset(offset), ta-&gt;field_offset());
1313     }
1314     // Initially all flattened array accesses share a single slice
<span class="line-modified">1315     if (ta-&gt;elem()-&gt;isa_inlinetype() &amp;&amp; ta-&gt;elem() != TypeInlineType::BOTTOM &amp;&amp; _flattened_accesses_share_alias) {</span>
<span class="line-modified">1316       const TypeAry *tary = TypeAry::make(TypeInlineType::BOTTOM, ta-&gt;size());</span>
1317       tj = ta = TypeAryPtr::make(ptr,ta-&gt;const_oop(),tary,NULL,false,Type::Offset(offset), Type::Offset(Type::OffsetBot));
1318     }
1319     // Arrays of bytes and of booleans both use &#39;bastore&#39; and &#39;baload&#39; so
1320     // cannot be distinguished by bytecode alone.
1321     if (ta-&gt;elem() == TypeInt::BOOL) {
1322       const TypeAry *tary = TypeAry::make(TypeInt::BYTE, ta-&gt;size());
1323       ciKlass* aklass = ciTypeArrayKlass::make(T_BYTE);
1324       tj = ta = TypeAryPtr::make(ptr,ta-&gt;const_oop(),tary,aklass,false,Type::Offset(offset), ta-&gt;field_offset());
1325     }
1326     // During the 2nd round of IterGVN, NotNull castings are removed.
1327     // Make sure the Bottom and NotNull variants alias the same.
1328     // Also, make sure exact and non-exact variants alias the same.
1329     if (ptr == TypePtr::NotNull || ta-&gt;klass_is_exact() || ta-&gt;speculative() != NULL) {
1330       tj = ta = TypeAryPtr::make(TypePtr::BotPTR,ta-&gt;ary(),ta-&gt;klass(),false,Type::Offset(offset), ta-&gt;field_offset());
1331     }
1332   }
1333 
1334   // Oop pointers need some flattening
1335   const TypeInstPtr *to = tj-&gt;isa_instptr();
1336   if( to &amp;&amp; _AliasLevel &gt;= 2 &amp;&amp; to != TypeOopPtr::BOTTOM ) {
</pre>
<hr />
<pre>
1610     _alias_types[idx]-&gt;Init(idx, flat);
1611     if (flat == TypeInstPtr::KLASS)  alias_type(idx)-&gt;set_rewritable(false);
1612     if (flat == TypeAryPtr::RANGE)   alias_type(idx)-&gt;set_rewritable(false);
1613     if (flat-&gt;isa_instptr()) {
1614       if (flat-&gt;offset() == java_lang_Class::klass_offset()
1615           &amp;&amp; flat-&gt;is_instptr()-&gt;klass() == env()-&gt;Class_klass())
1616         alias_type(idx)-&gt;set_rewritable(false);
1617     }
1618     ciField* field = NULL;
1619     if (flat-&gt;isa_aryptr()) {
1620 #ifdef ASSERT
1621       const int header_size_min  = arrayOopDesc::base_offset_in_bytes(T_BYTE);
1622       // (T_BYTE has the weakest alignment and size restrictions...)
1623       assert(flat-&gt;offset() &lt; header_size_min, &quot;array body reference must be OffsetBot&quot;);
1624 #endif
1625       const Type* elemtype = flat-&gt;is_aryptr()-&gt;elem();
1626       if (flat-&gt;offset() == TypePtr::OffsetBot) {
1627         alias_type(idx)-&gt;set_element(elemtype);
1628       }
1629       int field_offset = flat-&gt;is_aryptr()-&gt;field_offset().get();
<span class="line-modified">1630       if (elemtype-&gt;isa_inlinetype() &amp;&amp;</span>
<span class="line-modified">1631           elemtype-&gt;inline_klass() != NULL &amp;&amp;</span>
1632           field_offset != Type::OffsetBot) {
<span class="line-modified">1633         ciInlineKlass* vk = elemtype-&gt;inline_klass();</span>
1634         field_offset += vk-&gt;first_field_offset();
1635         field = vk-&gt;get_field_by_offset(field_offset, false);
1636       }
1637     }
1638     if (flat-&gt;isa_klassptr()) {
1639       if (flat-&gt;offset() == in_bytes(Klass::super_check_offset_offset()))
1640         alias_type(idx)-&gt;set_rewritable(false);
1641       if (flat-&gt;offset() == in_bytes(Klass::modifier_flags_offset()))
1642         alias_type(idx)-&gt;set_rewritable(false);
1643       if (flat-&gt;offset() == in_bytes(Klass::access_flags_offset()))
1644         alias_type(idx)-&gt;set_rewritable(false);
1645       if (flat-&gt;offset() == in_bytes(Klass::java_mirror_offset()))
1646         alias_type(idx)-&gt;set_rewritable(false);
1647       if (flat-&gt;offset() == in_bytes(Klass::layout_helper_offset()))
1648         alias_type(idx)-&gt;set_rewritable(false);
1649       if (flat-&gt;offset() == in_bytes(Klass::secondary_super_cache_offset()))
1650         alias_type(idx)-&gt;set_rewritable(false);
1651     }
1652     // %%% (We would like to finalize JavaThread::threadObj_offset(),
1653     // but the base pointer type is not distinctive enough to identify
1654     // references into JavaThread.)
1655 
1656     // Check for final fields.
1657     const TypeInstPtr* tinst = flat-&gt;isa_instptr();
1658     if (tinst &amp;&amp; tinst-&gt;offset() &gt;= instanceOopDesc::base_offset_in_bytes()) {
1659       if (tinst-&gt;const_oop() != NULL &amp;&amp;
1660           tinst-&gt;klass() == ciEnv::current()-&gt;Class_klass() &amp;&amp;
1661           tinst-&gt;offset() &gt;= (tinst-&gt;klass()-&gt;as_instance_klass()-&gt;size_helper() * wordSize)) {
1662         // static field
1663         ciInstanceKlass* k = tinst-&gt;const_oop()-&gt;as_instance()-&gt;java_lang_Class_klass()-&gt;as_instance_klass();
1664         field = k-&gt;get_field_by_offset(tinst-&gt;offset(), true);
<span class="line-modified">1665       } else if (tinst-&gt;klass()-&gt;is_inlinetype()) {</span>
<span class="line-modified">1666         // Inline type field</span>
<span class="line-modified">1667         ciInlineKlass* vk = tinst-&gt;inline_klass();</span>
1668         field = vk-&gt;get_field_by_offset(tinst-&gt;offset(), false);
1669       } else {
1670         ciInstanceKlass* k = tinst-&gt;klass()-&gt;as_instance_klass();
1671         field = k-&gt;get_field_by_offset(tinst-&gt;offset(), false);
1672       }
1673     }
1674     assert(field == NULL ||
1675            original_field == NULL ||
1676            (field-&gt;holder() == original_field-&gt;holder() &amp;&amp;
1677             field-&gt;offset() == original_field-&gt;offset() &amp;&amp;
1678             field-&gt;is_static() == original_field-&gt;is_static()), &quot;wrong field?&quot;);
1679     // Set field() and is_rewritable() attributes.
1680     if (field != NULL) {
1681       alias_type(idx)-&gt;set_field(field);
1682       if (flat-&gt;isa_aryptr()) {
<span class="line-modified">1683         // Fields of flat arrays are rewritable although they are declared final</span>
<span class="line-modified">1684         assert(flat-&gt;is_aryptr()-&gt;elem()-&gt;isa_inlinetype(), &quot;must be a flat array&quot;);</span>
1685         alias_type(idx)-&gt;set_rewritable(true);
1686       }
1687     }
1688   }
1689 
1690   // Fill the cache for next time.
1691   if (!uncached) {
1692     ace-&gt;_adr_type = adr_type;
1693     ace-&gt;_index    = idx;
1694     assert(alias_type(adr_type) == alias_type(idx),  &quot;type must be installed&quot;);
1695 
1696     // Might as well try to fill the cache for the flattened version, too.
1697     AliasCacheEntry* face = probe_alias_cache(flat);
1698     if (face-&gt;_adr_type == NULL) {
1699       face-&gt;_adr_type = flat;
1700       face-&gt;_index    = idx;
1701       assert(alias_type(flat) == alias_type(idx), &quot;flat type must work too&quot;);
1702     }
1703   }
1704 
</pre>
<hr />
<pre>
1844   }
1845   assert(range_check_cast_count() == 0, &quot;should be empty&quot;);
1846 }
1847 
1848 void Compile::add_opaque4_node(Node* n) {
1849   assert(n-&gt;Opcode() == Op_Opaque4, &quot;Opaque4 only&quot;);
1850   assert(!_opaque4_nodes-&gt;contains(n), &quot;duplicate entry in Opaque4 list&quot;);
1851   _opaque4_nodes-&gt;append(n);
1852 }
1853 
1854 // Remove all Opaque4 nodes.
1855 void Compile::remove_opaque4_nodes(PhaseIterGVN &amp;igvn) {
1856   for (int i = opaque4_count(); i &gt; 0; i--) {
1857     Node* opaq = opaque4_node(i-1);
1858     assert(opaq-&gt;Opcode() == Op_Opaque4, &quot;Opaque4 only&quot;);
1859     igvn.replace_node(opaq, opaq-&gt;in(2));
1860   }
1861   assert(opaque4_count() == 0, &quot;should be empty&quot;);
1862 }
1863 
<span class="line-modified">1864 void Compile::add_inline_type(Node* n) {</span>
<span class="line-modified">1865   assert(n-&gt;is_InlineTypeBase(), &quot;unexpected node&quot;);</span>
<span class="line-modified">1866   if (_inline_type_nodes != NULL) {</span>
<span class="line-modified">1867     _inline_type_nodes-&gt;push(n);</span>
1868   }
1869 }
1870 
<span class="line-modified">1871 void Compile::remove_inline_type(Node* n) {</span>
<span class="line-modified">1872   assert(n-&gt;is_InlineTypeBase(), &quot;unexpected node&quot;);</span>
<span class="line-modified">1873   if (_inline_type_nodes != NULL &amp;&amp; _inline_type_nodes-&gt;contains(n)) {</span>
<span class="line-modified">1874     _inline_type_nodes-&gt;remove(n);</span>
1875   }
1876 }
1877 
<span class="line-modified">1878 // Does the return value keep otherwise useless inline type allocations alive?</span>
1879 static bool return_val_keeps_allocations_alive(Node* ret_val) {
1880   ResourceMark rm;
1881   Unique_Node_List wq;
1882   wq.push(ret_val);
1883   bool some_allocations = false;
1884   for (uint i = 0; i &lt; wq.size(); i++) {
1885     Node* n = wq.at(i);
<span class="line-modified">1886     assert(!n-&gt;is_InlineType(), &quot;chain of inline type nodes&quot;);</span>
1887     if (n-&gt;outcnt() &gt; 1) {
1888       // Some other use for the allocation
1889       return false;
<span class="line-modified">1890     } else if (n-&gt;is_InlineTypePtr()) {</span>
1891       wq.push(n-&gt;in(1));
1892     } else if (n-&gt;is_Phi()) {
1893       for (uint j = 1; j &lt; n-&gt;req(); j++) {
1894         wq.push(n-&gt;in(j));
1895       }
1896     } else if (n-&gt;is_CheckCastPP() &amp;&amp;
1897                n-&gt;in(1)-&gt;is_Proj() &amp;&amp;
1898                n-&gt;in(1)-&gt;in(0)-&gt;is_Allocate()) {
1899       some_allocations = true;
1900     }
1901   }
1902   return some_allocations;
1903 }
1904 
<span class="line-modified">1905 void Compile::process_inline_types(PhaseIterGVN &amp;igvn, bool post_ea) {</span>
<span class="line-modified">1906   // Make inline types scalar in safepoints</span>
<span class="line-modified">1907   for (int i = _inline_type_nodes-&gt;length()-1; i &gt;= 0; i--) {</span>
<span class="line-modified">1908     InlineTypeBaseNode* vt = _inline_type_nodes-&gt;at(i)-&gt;as_InlineTypeBase();</span>
1909     vt-&gt;make_scalar_in_safepoints(&amp;igvn);
1910   }
<span class="line-modified">1911   // Remove InlineTypePtr nodes only after EA to give scalar replacement a chance</span>
<span class="line-modified">1912   // to remove buffer allocations. InlineType nodes are kept until loop opts and</span>
<span class="line-modified">1913   // removed via InlineTypeNode::remove_redundant_allocations.</span>
1914   if (post_ea) {
<span class="line-modified">1915     while (_inline_type_nodes-&gt;length() &gt; 0) {</span>
<span class="line-modified">1916       InlineTypeBaseNode* vt = _inline_type_nodes-&gt;pop()-&gt;as_InlineTypeBase();</span>
<span class="line-modified">1917       if (vt-&gt;is_InlineTypePtr()) {</span>
1918         igvn.replace_node(vt, vt-&gt;get_oop());
1919       }
1920     }
1921   }
1922   // Make sure that the return value does not keep an unused allocation alive
<span class="line-modified">1923   if (tf()-&gt;returns_inline_type_as_fields()) {</span>
1924     Node* ret = NULL;
1925     for (uint i = 1; i &lt; root()-&gt;req(); i++){
1926       Node* in = root()-&gt;in(i);
1927       if (in-&gt;Opcode() == Op_Return) {
1928         assert(ret == NULL, &quot;only one return&quot;);
1929         ret = in;
1930       }
1931     }
1932     if (ret != NULL) {
1933       Node* ret_val = ret-&gt;in(TypeFunc::Parms);
1934       if (igvn.type(ret_val)-&gt;isa_oopptr() &amp;&amp;
1935           return_val_keeps_allocations_alive(ret_val)) {
<span class="line-modified">1936         igvn.replace_input_of(ret, TypeFunc::Parms, InlineTypeNode::tagged_klass(igvn.type(ret_val)-&gt;inline_klass(), igvn));</span>
1937         assert(ret_val-&gt;outcnt() == 0, &quot;should be dead now&quot;);
1938         igvn.remove_dead_node(ret_val);
1939       }
1940     }
1941   }
1942   igvn.optimize();
1943 }
1944 
1945 void Compile::adjust_flattened_array_access_aliases(PhaseIterGVN&amp; igvn) {
1946   if (!_has_flattened_accesses) {
1947     return;
1948   }
1949   // Initially, all flattened array accesses share the same slice to
1950   // keep dependencies with Object[] array accesses (that could be
1951   // to a flattened array) correct. We&#39;re done with parsing so we
1952   // now know all flattened array accesses in this compile
1953   // unit. Let&#39;s move flattened array accesses to their own slice,
1954   // one per element field. This should help memory access
1955   // optimizations.
1956   ResourceMark rm;
1957   Unique_Node_List wq;
1958   wq.push(root());
1959 
1960   Node_List mergememnodes;
1961   Node_List memnodes;
1962 
1963   // Alias index currently shared by all flattened memory accesses
<span class="line-modified">1964   int index = get_alias_index(TypeAryPtr::INLINES);</span>
1965 
1966   // Find MergeMem nodes and flattened array accesses
1967   for (uint i = 0; i &lt; wq.size(); i++) {
1968     Node* n = wq.at(i);
1969     if (n-&gt;is_Mem()) {
1970       const TypePtr* adr_type = NULL;
1971       if (n-&gt;Opcode() == Op_StoreCM) {
1972         adr_type = get_adr_type(get_alias_index(n-&gt;in(MemNode::OopStore)-&gt;adr_type()));
1973       } else {
1974         adr_type = get_adr_type(get_alias_index(n-&gt;adr_type()));
1975       }
<span class="line-modified">1976       if (adr_type == TypeAryPtr::INLINES) {</span>
1977         memnodes.push(n);
1978       }
1979     } else if (n-&gt;is_MergeMem()) {
1980       MergeMemNode* mm = n-&gt;as_MergeMem();
1981       if (mm-&gt;memory_at(index) != mm-&gt;base_memory()) {
1982         mergememnodes.push(n);
1983       }
1984     }
1985     for (uint j = 0; j &lt; n-&gt;req(); j++) {
1986       Node* m = n-&gt;in(j);
1987       if (m != NULL) {
1988         wq.push(m);
1989       }
1990     }
1991   }
1992 
1993   if (memnodes.size() &gt; 0) {
1994     _flattened_accesses_share_alias = false;
1995 
1996     // We are going to change the slice for the flattened array
1997     // accesses so we need to clear the cache entries that refer to
1998     // them.
1999     for (uint i = 0; i &lt; AliasCacheSize; i++) {
2000       AliasCacheEntry* ace = &amp;_alias_cache[i];
2001       if (ace-&gt;_adr_type != NULL &amp;&amp;
2002           ace-&gt;_adr_type-&gt;isa_aryptr() &amp;&amp;
<span class="line-modified">2003           ace-&gt;_adr_type-&gt;is_aryptr()-&gt;elem()-&gt;isa_inlinetype()) {</span>
2004         ace-&gt;_adr_type = NULL;
2005         ace-&gt;_index = (i != 0) ? 0 : AliasIdxTop; // Make sure the NULL adr_type resolves to AliasIdxTop
2006       }
2007     }
2008 
2009     // Find what aliases we are going to add
2010     int start_alias = num_alias_types()-1;
2011     int stop_alias = 0;
2012 
2013     for (uint i = 0; i &lt; memnodes.size(); i++) {
2014       Node* m = memnodes.at(i);
2015       const TypePtr* adr_type = NULL;
2016       if (m-&gt;Opcode() == Op_StoreCM) {
2017         adr_type = m-&gt;in(MemNode::OopStore)-&gt;adr_type();
2018         Node* clone = new StoreCMNode(m-&gt;in(MemNode::Control), m-&gt;in(MemNode::Memory), m-&gt;in(MemNode::Address),
2019                                       m-&gt;adr_type(), m-&gt;in(MemNode::ValueIn), m-&gt;in(MemNode::OopStore),
2020                                       get_alias_index(adr_type));
2021         igvn.register_new_node_with_optimizer(clone);
2022         igvn.replace_node(m, clone);
2023       } else {
</pre>
<hr />
<pre>
2033 
2034     assert(stop_alias &gt;= start_alias, &quot;should have expanded aliases&quot;);
2035 
2036     Node_Stack stack(0);
2037 #ifdef ASSERT
2038     VectorSet seen(Thread::current()-&gt;resource_area());
2039 #endif
2040     // Now let&#39;s fix the memory graph so each flattened array access
2041     // is moved to the right slice. Start from the MergeMem nodes.
2042     uint last = unique();
2043     for (uint i = 0; i &lt; mergememnodes.size(); i++) {
2044       MergeMemNode* current = mergememnodes.at(i)-&gt;as_MergeMem();
2045       Node* n = current-&gt;memory_at(index);
2046       MergeMemNode* mm = NULL;
2047       do {
2048         // Follow memory edges through memory accesses, phis and
2049         // narrow membars and push nodes on the stack. Once we hit
2050         // bottom memory, we pop element off the stack one at a
2051         // time, in reverse order, and move them to the right slice
2052         // by changing their memory edges.
<span class="line-modified">2053         if ((n-&gt;is_Phi() &amp;&amp; n-&gt;adr_type() != TypePtr::BOTTOM) || n-&gt;is_Mem() || n-&gt;adr_type() == TypeAryPtr::INLINES) {</span>
2054           assert(!seen.test_set(n-&gt;_idx), &quot;&quot;);
2055           // Uses (a load for instance) will need to be moved to the
2056           // right slice as well and will get a new memory state
2057           // that we don&#39;t know yet. The use could also be the
2058           // backedge of a loop. We put a place holder node between
2059           // the memory node and its uses. We replace that place
2060           // holder with the correct memory state once we know it,
2061           // i.e. when nodes are popped off the stack. Using the
2062           // place holder make the logic work in the presence of
2063           // loops.
2064           if (n-&gt;outcnt() &gt; 1) {
2065             Node* place_holder = NULL;
2066             assert(!n-&gt;has_out_with(Op_Node), &quot;&quot;);
2067             for (DUIterator k = n-&gt;outs(); n-&gt;has_out(k); k++) {
2068               Node* u = n-&gt;out(k);
2069               if (u != current &amp;&amp; u-&gt;_idx &lt; last) {
2070                 bool success = false;
2071                 for (uint l = 0; l &lt; u-&gt;req(); l++) {
2072                   if (!stack.is_empty() &amp;&amp; u == stack.node() &amp;&amp; l == stack.index()) {
2073                     continue;
</pre>
<hr />
<pre>
2106           // nodes.
2107           mm = MergeMemNode::make(n);
2108           igvn.register_new_node_with_optimizer(mm);
2109           while (stack.size() &gt; 0) {
2110             Node* m = stack.node();
2111             uint idx = stack.index();
2112             if (m-&gt;is_Mem()) {
2113               // Move memory node to its new slice
2114               const TypePtr* adr_type = m-&gt;adr_type();
2115               int alias = get_alias_index(adr_type);
2116               Node* prev = mm-&gt;memory_at(alias);
2117               igvn.replace_input_of(m, MemNode::Memory, prev);
2118               mm-&gt;set_memory_at(alias, m);
2119             } else if (m-&gt;is_Phi()) {
2120               // We need as many new phis as there are new aliases
2121               igvn.replace_input_of(m, idx, mm);
2122               if (idx == m-&gt;req()-1) {
2123                 Node* r = m-&gt;in(0);
2124                 for (uint j = (uint)start_alias; j &lt;= (uint)stop_alias; j++) {
2125                   const Type* adr_type = get_adr_type(j);
<span class="line-modified">2126                   if (!adr_type-&gt;isa_aryptr() || !adr_type-&gt;is_aryptr()-&gt;elem()-&gt;isa_inlinetype()) {</span>
2127                     continue;
2128                   }
2129                   Node* phi = new PhiNode(r, Type::MEMORY, get_adr_type(j));
2130                   igvn.register_new_node_with_optimizer(phi);
2131                   for (uint k = 1; k &lt; m-&gt;req(); k++) {
2132                     phi-&gt;init_req(k, m-&gt;in(k)-&gt;as_MergeMem()-&gt;memory_at(j));
2133                   }
2134                   mm-&gt;set_memory_at(j, phi);
2135                 }
2136                 Node* base_phi = new PhiNode(r, Type::MEMORY, TypePtr::BOTTOM);
2137                 igvn.register_new_node_with_optimizer(base_phi);
2138                 for (uint k = 1; k &lt; m-&gt;req(); k++) {
2139                   base_phi-&gt;init_req(k, m-&gt;in(k)-&gt;as_MergeMem()-&gt;base_memory());
2140                 }
2141                 mm-&gt;set_base_memory(base_phi);
2142               }
2143             } else {
2144               // This is a MemBarCPUOrder node from
2145               // Parse::array_load()/Parse::array_store(), in the
2146               // branch that handles flattened arrays hidden under
2147               // an Object[] array. We also need one new membar per
2148               // new alias to keep the unknown access that the
2149               // membars protect properly ordered with accesses to
2150               // known flattened array.
2151               assert(m-&gt;is_Proj(), &quot;projection expected&quot;);
2152               Node* ctrl = m-&gt;in(0)-&gt;in(TypeFunc::Control);
2153               igvn.replace_input_of(m-&gt;in(0), TypeFunc::Control, top());
2154               for (uint j = (uint)start_alias; j &lt;= (uint)stop_alias; j++) {
2155                 const Type* adr_type = get_adr_type(j);
<span class="line-modified">2156                 if (!adr_type-&gt;isa_aryptr() || !adr_type-&gt;is_aryptr()-&gt;elem()-&gt;isa_inlinetype()) {</span>
2157                   continue;
2158                 }
2159                 MemBarNode* mb = new MemBarCPUOrderNode(this, j, NULL);
2160                 igvn.register_new_node_with_optimizer(mb);
2161                 Node* mem = mm-&gt;memory_at(j);
2162                 mb-&gt;init_req(TypeFunc::Control, ctrl);
2163                 mb-&gt;init_req(TypeFunc::Memory, mem);
2164                 ctrl = new ProjNode(mb, TypeFunc::Control);
2165                 igvn.register_new_node_with_optimizer(ctrl);
2166                 mem = new ProjNode(mb, TypeFunc::Memory);
2167                 igvn.register_new_node_with_optimizer(mem);
2168                 mm-&gt;set_memory_at(j, mem);
2169               }
2170               igvn.replace_node(m-&gt;in(0)-&gt;as_Multi()-&gt;proj_out(TypeFunc::Control), ctrl);
2171             }
2172             if (idx &lt; m-&gt;req()-1) {
2173               idx += 1;
2174               stack.set_index(idx);
2175               n = m-&gt;in(idx);
2176               break;
</pre>
<hr />
<pre>
2179             if (m-&gt;has_out_with(Op_Node)) {
2180               Node* place_holder = m-&gt;find_out_with(Op_Node);
2181               if (place_holder != NULL) {
2182                 Node* mm_clone = mm-&gt;clone();
2183                 igvn.register_new_node_with_optimizer(mm_clone);
2184                 Node* hook = new Node(1);
2185                 hook-&gt;init_req(0, mm);
2186                 igvn.replace_node(place_holder, mm_clone);
2187                 hook-&gt;destruct();
2188               }
2189               assert(!m-&gt;has_out_with(Op_Node), &quot;place holder should be gone now&quot;);
2190             }
2191             stack.pop();
2192           }
2193         }
2194       } while(stack.size() &gt; 0);
2195       // Fix the memory state at the MergeMem we started from
2196       igvn.rehash_node_delayed(current);
2197       for (uint j = (uint)start_alias; j &lt;= (uint)stop_alias; j++) {
2198         const Type* adr_type = get_adr_type(j);
<span class="line-modified">2199         if (!adr_type-&gt;isa_aryptr() || !adr_type-&gt;is_aryptr()-&gt;elem()-&gt;isa_inlinetype()) {</span>
2200           continue;
2201         }
2202         current-&gt;set_memory_at(j, mm);
2203       }
2204       current-&gt;set_memory_at(index, current-&gt;base_memory());
2205     }
2206     igvn.optimize();
2207   }
<span class="line-modified">2208   print_method(PHASE_SPLIT_INLINES_ARRAY, 2);</span>
2209 }
2210 
2211 
2212 // StringOpts and late inlining of string methods
2213 void Compile::inline_string_calls(bool parse_time) {
2214   {
2215     // remove useless nodes to make the usage analysis simpler
2216     ResourceMark rm;
2217     PhaseRemoveUseless pru(initial_gvn(), for_igvn());
2218   }
2219 
2220   {
2221     ResourceMark rm;
2222     print_method(PHASE_BEFORE_STRINGOPTS, 3);
2223     PhaseStringOpts pso(initial_gvn(), for_igvn());
2224     print_method(PHASE_AFTER_STRINGOPTS, 3);
2225   }
2226 
2227   // now inline anything that we skipped the first time around
2228   if (!parse_time) {
</pre>
<hr />
<pre>
2471   remove_speculative_types(igvn);
2472 
2473   // No more new expensive nodes will be added to the list from here
2474   // so keep only the actual candidates for optimizations.
2475   cleanup_expensive_nodes(igvn);
2476 
2477   if (!failing() &amp;&amp; RenumberLiveNodes &amp;&amp; live_nodes() + NodeLimitFudgeFactor &lt; unique()) {
2478     Compile::TracePhase tp(&quot;&quot;, &amp;timers[_t_renumberLive]);
2479     initial_gvn()-&gt;replace_with(&amp;igvn);
2480     for_igvn()-&gt;clear();
2481     Unique_Node_List new_worklist(C-&gt;comp_arena());
2482     {
2483       ResourceMark rm;
2484       PhaseRenumberLive prl = PhaseRenumberLive(initial_gvn(), for_igvn(), &amp;new_worklist);
2485     }
2486     set_for_igvn(&amp;new_worklist);
2487     igvn = PhaseIterGVN(initial_gvn());
2488     igvn.optimize();
2489   }
2490 
<span class="line-modified">2491   if (_inline_type_nodes-&gt;length() &gt; 0) {</span>
2492     // Do this once all inlining is over to avoid getting inconsistent debug info
<span class="line-modified">2493     process_inline_types(igvn);</span>
2494   }
2495 
2496   adjust_flattened_array_access_aliases(igvn);
2497 
2498   // Perform escape analysis
2499   if (_do_escape_analysis &amp;&amp; ConnectionGraph::has_candidates(this)) {
2500     if (has_loops()) {
2501       // Cleanup graph (remove dead nodes).
2502       TracePhase tp(&quot;idealLoop&quot;, &amp;timers[_t_idealLoop]);
2503       PhaseIdealLoop::optimize(igvn, LoopOptsMaxUnroll);
2504       if (major_progress()) print_method(PHASE_PHASEIDEAL_BEFORE_EA, 2);
2505       if (failing())  return;
2506     }
2507     ConnectionGraph::do_analysis(this, &amp;igvn);
2508 
2509     if (failing())  return;
2510 
2511     // Optimize out fields loads from scalar replaceable allocations.
2512     igvn.optimize();
2513     print_method(PHASE_ITER_GVN_AFTER_EA, 2);
2514 
2515     if (failing())  return;
2516 
2517     if (congraph() != NULL &amp;&amp; macro_count() &gt; 0) {
2518       TracePhase tp(&quot;macroEliminate&quot;, &amp;timers[_t_macroEliminate]);
2519       PhaseMacroExpand mexp(igvn);
2520       mexp.eliminate_macro_nodes();
2521       igvn.set_delay_transform(false);
2522 
2523       igvn.optimize();
2524       print_method(PHASE_ITER_GVN_AFTER_ELIMINATION, 2);
2525 
2526       if (failing())  return;
2527     }
2528   }
2529 
<span class="line-modified">2530   if (_inline_type_nodes-&gt;length() &gt; 0) {</span>
<span class="line-modified">2531     // Process inline types again now that EA might have simplified the graph</span>
<span class="line-modified">2532     process_inline_types(igvn, /* post_ea= */ true);</span>
2533   }
2534 
2535   // Loop transforms on the ideal graph.  Range Check Elimination,
2536   // peeling, unrolling, etc.
2537 
2538   // Set loop opts counter
2539   if((_loop_opts_cnt &gt; 0) &amp;&amp; (has_loops() || has_split_ifs())) {
2540     {
2541       TracePhase tp(&quot;idealLoop&quot;, &amp;timers[_t_idealLoop]);
2542       PhaseIdealLoop::optimize(igvn, LoopOptsDefault);
2543       _loop_opts_cnt--;
2544       if (major_progress()) print_method(PHASE_PHASEIDEALLOOP1, 2);
2545       if (failing())  return;
2546     }
2547     // Loop opts pass if partial peeling occurred in previous pass
2548     if(PartialPeelLoop &amp;&amp; major_progress() &amp;&amp; (_loop_opts_cnt &gt; 0)) {
2549       TracePhase tp(&quot;idealLoop&quot;, &amp;timers[_t_idealLoop]);
2550       PhaseIdealLoop::optimize(igvn, LoopOptsSkipSplitIf);
2551       _loop_opts_cnt--;
2552       if (major_progress()) print_method(PHASE_PHASEIDEALLOOP2, 2);
</pre>
<hr />
<pre>
3897           k-&gt;subsume_by(m, this);
3898         }
3899       }
3900     }
3901     break;
3902   }
3903   case Op_CmpUL: {
3904     if (!Matcher::has_match_rule(Op_CmpUL)) {
3905       // No support for unsigned long comparisons
3906       ConINode* sign_pos = new ConINode(TypeInt::make(BitsPerLong - 1));
3907       Node* sign_bit_mask = new RShiftLNode(n-&gt;in(1), sign_pos);
3908       Node* orl = new OrLNode(n-&gt;in(1), sign_bit_mask);
3909       ConLNode* remove_sign_mask = new ConLNode(TypeLong::make(max_jlong));
3910       Node* andl = new AndLNode(orl, remove_sign_mask);
3911       Node* cmp = new CmpLNode(andl, n-&gt;in(2));
3912       n-&gt;subsume_by(cmp, this);
3913     }
3914     break;
3915   }
3916 #ifdef ASSERT
<span class="line-modified">3917   case Op_InlineTypePtr:</span>
<span class="line-modified">3918   case Op_InlineType: {</span>
3919     n-&gt;dump(-1);
<span class="line-modified">3920     assert(false, &quot;inline type node was not removed&quot;);</span>
3921     break;
3922   }
3923 #endif
3924   default:
3925     assert(!n-&gt;is_Call(), &quot;&quot;);
3926     assert(!n-&gt;is_Mem(), &quot;&quot;);
3927     assert(nop != Op_ProfileBoolean, &quot;should be eliminated during IGVN&quot;);
3928     break;
3929   }
3930 }
3931 
3932 //------------------------------final_graph_reshaping_walk---------------------
3933 // Replacing Opaque nodes with their input in final_graph_reshaping_impl(),
3934 // requires that the walk visits a node&#39;s inputs before visiting the node.
3935 void Compile::final_graph_reshaping_walk( Node_Stack &amp;nstack, Node *root, Final_Reshape_Counts &amp;frc ) {
3936   ResourceArea *area = Thread::current()-&gt;resource_area();
3937   Unique_Node_List sfpt(area);
3938 
3939   frc._visited.set(root-&gt;_idx); // first, mark node as visited
3940   uint cnt = root-&gt;req();
</pre>
<hr />
<pre>
4861         t = n-&gt;as_Type()-&gt;type();
4862         assert(t == t-&gt;remove_speculative(), &quot;no more speculative types&quot;);
4863       }
4864       uint max = n-&gt;len();
4865       for( uint i = 0; i &lt; max; ++i ) {
4866         Node *m = n-&gt;in(i);
4867         if (not_a_node(m))  continue;
4868         worklist.push(m);
4869       }
4870     }
4871     igvn.check_no_speculative_types();
4872 #endif
4873   }
4874 }
4875 
4876 Node* Compile::optimize_acmp(PhaseGVN* phase, Node* a, Node* b) {
4877   const TypeInstPtr* ta = phase-&gt;type(a)-&gt;isa_instptr();
4878   const TypeInstPtr* tb = phase-&gt;type(b)-&gt;isa_instptr();
4879   if (!EnableValhalla || ta == NULL || tb == NULL ||
4880       ta-&gt;is_zero_type() || tb-&gt;is_zero_type() ||
<span class="line-modified">4881       !ta-&gt;can_be_inline_type() || !tb-&gt;can_be_inline_type()) {</span>
<span class="line-modified">4882     // Use old acmp if one operand is null or not an inline type</span>
4883     return new CmpPNode(a, b);
<span class="line-modified">4884   } else if (ta-&gt;is_inlinetypeptr() || tb-&gt;is_inlinetypeptr()) {</span>
<span class="line-modified">4885     // We know that one operand is an inline type. Therefore,</span>
4886     // new acmp will only return true if both operands are NULL.
4887     // Check if both operands are null by or&#39;ing the oops.
4888     a = phase-&gt;transform(new CastP2XNode(NULL, a));
4889     b = phase-&gt;transform(new CastP2XNode(NULL, b));
4890     a = phase-&gt;transform(new OrXNode(a, b));
4891     return new CmpXNode(a, phase-&gt;MakeConX(0));
4892   }
4893   // Use new acmp
4894   return NULL;
4895 }
4896 
4897 // Auxiliary method to support randomized stressing/fuzzing.
4898 //
4899 // This method can be called the arbitrary number of times, with current count
4900 // as the argument. The logic allows selecting a single candidate from the
4901 // running list of candidates as follows:
4902 //    int count = 0;
4903 //    Cand* selected = null;
4904 //    while(cand = cand-&gt;next()) {
4905 //      if (randomized_select(++count)) {
</pre>
</td>
</tr>
</table>
<center><a href="classes.hpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="compile.hpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>