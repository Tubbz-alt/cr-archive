<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/share/opto/parse2.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="parse1.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="parse3.cpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/share/opto/parse2.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #include &quot;precompiled.hpp&quot;
  26 #include &quot;ci/ciMethodData.hpp&quot;
  27 #include &quot;classfile/systemDictionary.hpp&quot;
  28 #include &quot;classfile/vmSymbols.hpp&quot;
  29 #include &quot;compiler/compileLog.hpp&quot;
  30 #include &quot;interpreter/linkResolver.hpp&quot;
  31 #include &quot;memory/resourceArea.hpp&quot;
  32 #include &quot;memory/universe.hpp&quot;
  33 #include &quot;oops/oop.inline.hpp&quot;
  34 #include &quot;opto/addnode.hpp&quot;
  35 #include &quot;opto/castnode.hpp&quot;
  36 #include &quot;opto/convertnode.hpp&quot;
  37 #include &quot;opto/divnode.hpp&quot;
  38 #include &quot;opto/idealGraphPrinter.hpp&quot;
  39 #include &quot;opto/idealKit.hpp&quot;

  40 #include &quot;opto/matcher.hpp&quot;
  41 #include &quot;opto/memnode.hpp&quot;
  42 #include &quot;opto/mulnode.hpp&quot;
  43 #include &quot;opto/opaquenode.hpp&quot;
  44 #include &quot;opto/parse.hpp&quot;
  45 #include &quot;opto/runtime.hpp&quot;
<span class="line-removed">  46 #include &quot;opto/valuetypenode.hpp&quot;</span>
  47 #include &quot;runtime/deoptimization.hpp&quot;
  48 #include &quot;runtime/sharedRuntime.hpp&quot;
  49 
  50 #ifndef PRODUCT
  51 extern int explicit_null_checks_inserted,
  52            explicit_null_checks_elided;
  53 #endif
  54 
  55 Node* Parse::record_profile_for_speculation_at_array_load(Node* ld) {
  56   // Feed unused profile data to type speculation
  57   if (UseTypeSpeculation &amp;&amp; UseArrayLoadStoreProfile) {
  58     ciKlass* array_type = NULL;
  59     ciKlass* element_type = NULL;
  60     ProfilePtrKind element_ptr = ProfileMaybeNull;
  61     bool flat_array = true;
  62     bool null_free_array = true;
  63     method()-&gt;array_access_profiled_type(bci(), array_type, element_type, element_ptr, flat_array, null_free_array);
  64     if (element_type != NULL || element_ptr != ProfileMaybeNull) {
  65       ld = record_profile_for_speculation(ld, element_type, element_ptr);
  66     }
  67   }
  68   return ld;
  69 }
  70 
  71 
  72 //---------------------------------array_load----------------------------------
  73 void Parse::array_load(BasicType bt) {
  74   const Type* elemtype = Type::TOP;
  75   Node* adr = array_addressing(bt, 0, elemtype);
  76   if (stopped())  return;     // guaranteed null or range check
  77 
  78   Node* idx = pop();
  79   Node* ary = pop();
  80 
<span class="line-modified">  81   // Handle value type arrays</span>
  82   const TypeOopPtr* elemptr = elemtype-&gt;make_oopptr();
  83   const TypeAryPtr* ary_t = _gvn.type(ary)-&gt;is_aryptr();
<span class="line-modified">  84   if (elemtype-&gt;isa_valuetype() != NULL) {</span>
  85     C-&gt;set_flattened_accesses();
<span class="line-modified">  86     // Load from flattened value type array</span>
<span class="line-modified">  87     Node* vt = ValueTypeNode::make_from_flattened(this, elemtype-&gt;value_klass(), ary, adr);</span>
  88     push(vt);
  89     return;
<span class="line-modified">  90   } else if (elemptr != NULL &amp;&amp; elemptr-&gt;is_valuetypeptr() &amp;&amp; !elemptr-&gt;maybe_null()) {</span>
  91     // Load from non-flattened inline type array (elements can never be null)
  92     bt = T_INLINE_TYPE;
  93   } else if (!ary_t-&gt;is_not_flat()) {
  94     // Cannot statically determine if array is flattened, emit runtime check
<span class="line-modified">  95     assert(UseFlatArray &amp;&amp; is_reference_type(bt) &amp;&amp; elemptr-&gt;can_be_value_type() &amp;&amp; !ary_t-&gt;klass_is_exact() &amp;&amp; !ary_t-&gt;is_not_null_free() &amp;&amp;</span>
<span class="line-modified">  96            (!elemptr-&gt;is_valuetypeptr() || elemptr-&gt;value_klass()-&gt;flatten_array()), &quot;array can&#39;t be flattened&quot;);</span>
  97     IdealKit ideal(this);
  98     IdealVariable res(ideal);
  99     ideal.declarations_done();
 100     ideal.if_then(is_non_flattened_array(ary)); {
 101       // non-flattened
 102       assert(ideal.ctrl()-&gt;in(0)-&gt;as_If()-&gt;is_non_flattened_array_check(&amp;_gvn), &quot;Should be found&quot;);
 103       sync_kit(ideal);
 104       const TypeAryPtr* adr_type = TypeAryPtr::get_array_body_type(bt);
 105       Node* ld = access_load_at(ary, adr, adr_type, elemptr, bt,
 106                                 IN_HEAP | IS_ARRAY | C2_CONTROL_DEPENDENT_LOAD);
 107       ideal.sync_kit(this);
 108       ideal.set(res, ld);
 109     } ideal.else_(); {
 110       // flattened
 111       sync_kit(ideal);
<span class="line-modified"> 112       if (elemptr-&gt;is_valuetypeptr()) {</span>
 113         // Element type is known, cast and load from flattened representation
<span class="line-modified"> 114         ciValueKlass* vk = elemptr-&gt;value_klass();</span>
 115         assert(vk-&gt;flatten_array() &amp;&amp; elemptr-&gt;maybe_null(), &quot;never/always flat - should be optimized&quot;);
 116         ciArrayKlass* array_klass = ciArrayKlass::make(vk);
 117         const TypeAryPtr* arytype = TypeOopPtr::make_from_klass(array_klass)-&gt;isa_aryptr();
 118         Node* cast = _gvn.transform(new CheckCastPPNode(control(), ary, arytype));
 119         Node* casted_adr = array_element_address(cast, idx, T_INLINE_TYPE, ary_t-&gt;size(), control());
 120         // Re-execute flattened array load if buffering triggers deoptimization
 121         PreserveReexecuteState preexecs(this);
 122         jvms()-&gt;set_should_reexecute(true);
 123         inc_sp(2);
<span class="line-modified"> 124         Node* vt = ValueTypeNode::make_from_flattened(this, vk, cast, casted_adr)-&gt;buffer(this, false);</span>
 125         ideal.set(res, vt);
 126         ideal.sync_kit(this);
 127       } else {
 128         // Element type is unknown, emit runtime call
 129         Node* kls = load_object_klass(ary);
 130         Node* k_adr = basic_plus_adr(kls, in_bytes(ArrayKlass::element_klass_offset()));
 131         Node* elem_klass = _gvn.transform(LoadKlassNode::make(_gvn, NULL, immutable_memory(), k_adr, TypeInstPtr::KLASS));
 132         Node* obj_size  = NULL;
 133         kill_dead_locals();
 134         // Re-execute flattened array load if buffering triggers deoptimization
 135         PreserveReexecuteState preexecs(this);
 136         jvms()-&gt;set_bci(_bci);
 137         jvms()-&gt;set_should_reexecute(true);
 138         inc_sp(2);
 139         Node* alloc_obj = new_instance(elem_klass, NULL, &amp;obj_size, /*deoptimize_on_exception=*/true);
 140 
 141         AllocateNode* alloc = AllocateNode::Ideal_allocation(alloc_obj, &amp;_gvn);
 142         assert(alloc-&gt;maybe_set_complete(&amp;_gvn), &quot;&quot;);
 143         alloc-&gt;initialization()-&gt;set_complete_with_arraycopy();
 144 
 145         // This membar keeps this access to an unknown flattened array
 146         // correctly ordered with other unknown and known flattened
 147         // array accesses.
<span class="line-modified"> 148         insert_mem_bar_volatile(Op_MemBarCPUOrder, C-&gt;get_alias_index(TypeAryPtr::VALUES));</span>
 149 
 150         BarrierSetC2* bs = BarrierSet::barrier_set()-&gt;barrier_set_c2();
<span class="line-modified"> 151         // Unknown value type might contain reference fields</span>
 152         if (false &amp;&amp; !bs-&gt;array_copy_requires_gc_barriers(false, T_OBJECT, false, BarrierSetC2::Parsing)) {
 153           // FIXME 8230656 also merge changes from 8238759 in
 154           int base_off = sizeof(instanceOopDesc);
 155           Node* dst_base = basic_plus_adr(alloc_obj, base_off);
 156           Node* countx = obj_size;
 157           countx = _gvn.transform(new SubXNode(countx, MakeConX(base_off)));
 158           countx = _gvn.transform(new URShiftXNode(countx, intcon(LogBytesPerLong)));
 159 
 160           assert(Klass::_lh_log2_element_size_shift == 0, &quot;use shift in place&quot;);
 161           Node* lhp = basic_plus_adr(kls, in_bytes(Klass::layout_helper_offset()));
 162           Node* elem_shift = make_load(NULL, lhp, TypeInt::INT, T_INT, MemNode::unordered);
 163           uint header = arrayOopDesc::base_offset_in_bytes(T_INLINE_TYPE);
 164           Node* base  = basic_plus_adr(ary, header);
 165           idx = Compile::conv_I2X_index(&amp;_gvn, idx, TypeInt::POS, control());
 166           Node* scale = _gvn.transform(new LShiftXNode(idx, elem_shift));
 167           Node* adr = basic_plus_adr(ary, base, scale);
 168 
 169           access_clone(adr, dst_base, countx, false);
 170         } else {
 171           ideal.sync_kit(this);
<span class="line-modified"> 172           ideal.make_leaf_call(OptoRuntime::load_unknown_value_Type(),</span>
<span class="line-modified"> 173                                CAST_FROM_FN_PTR(address, OptoRuntime::load_unknown_value),</span>
<span class="line-modified"> 174                                &quot;load_unknown_value&quot;,</span>
 175                                ary, idx, alloc_obj);
 176           sync_kit(ideal);
 177         }
 178 
<span class="line-modified"> 179         // This makes sure no other thread sees a partially initialized buffered value</span>
 180         insert_mem_bar_volatile(Op_MemBarStoreStore, Compile::AliasIdxRaw, alloc-&gt;proj_out_or_null(AllocateNode::RawAddress));
 181 
 182         // Same as MemBarCPUOrder above: keep this unknown flattened
 183         // array access correctly ordered with other flattened array
 184         // access
<span class="line-modified"> 185         insert_mem_bar_volatile(Op_MemBarCPUOrder, C-&gt;get_alias_index(TypeAryPtr::VALUES));</span>
 186 
<span class="line-modified"> 187         // Prevent any use of the newly allocated value before it is</span>
<span class="line-removed"> 188         // fully initialized</span>
 189         alloc_obj = new CastPPNode(alloc_obj, _gvn.type(alloc_obj), true);
 190         alloc_obj-&gt;set_req(0, control());
 191         alloc_obj = _gvn.transform(alloc_obj);
 192 
 193         const Type* unknown_value = elemptr-&gt;is_instptr()-&gt;cast_to_flat_array();
 194         alloc_obj = _gvn.transform(new CheckCastPPNode(control(), alloc_obj, unknown_value));
 195 
 196         ideal.sync_kit(this);
 197         ideal.set(res, alloc_obj);
 198       }
 199     } ideal.end_if();
 200     sync_kit(ideal);
 201     Node* ld = _gvn.transform(ideal.value(res));
 202     ld = record_profile_for_speculation_at_array_load(ld);
 203     push_node(bt, ld);
 204     return;
 205   }
 206 
 207   if (elemtype == TypeInt::BOOL) {
 208     bt = T_BOOLEAN;
 209   }
 210   const TypeAryPtr* adr_type = TypeAryPtr::get_array_body_type(bt);
 211   Node* ld = access_load_at(ary, adr, adr_type, elemtype, bt,
 212                             IN_HEAP | IS_ARRAY | C2_CONTROL_DEPENDENT_LOAD);
 213   if (bt == T_INLINE_TYPE) {
<span class="line-modified"> 214     // Loading a non-flattened value type from an array</span>
<span class="line-modified"> 215     assert(!gvn().type(ld)-&gt;maybe_null(), &quot;value type array elements should never be null&quot;);</span>
<span class="line-modified"> 216     if (elemptr-&gt;value_klass()-&gt;is_scalarizable()) {</span>
<span class="line-modified"> 217       ld = ValueTypeNode::make_from_oop(this, ld, elemptr-&gt;value_klass());</span>
 218     }
 219   }
<span class="line-modified"> 220   if (!ld-&gt;is_ValueType()) {</span>
 221     ld = record_profile_for_speculation_at_array_load(ld);
 222   }
 223 
 224   push_node(bt, ld);
 225 }
 226 
 227 
 228 //--------------------------------array_store----------------------------------
 229 void Parse::array_store(BasicType bt) {
 230   const Type* elemtype = Type::TOP;
 231   Node* adr = array_addressing(bt, type2size[bt], elemtype);
 232   if (stopped())  return;     // guaranteed null or range check
 233   Node* cast_val = NULL;
 234   if (bt == T_OBJECT) {
 235     cast_val = array_store_check();
 236     if (stopped()) return;
 237   }
 238   Node* val = pop_node(bt); // Value to store
 239   Node* idx = pop();        // Index in the array
 240   Node* ary = pop();        // The array itself
 241 
 242   const TypeAryPtr* ary_t = _gvn.type(ary)-&gt;is_aryptr();
 243   const TypeAryPtr* adr_type = TypeAryPtr::get_array_body_type(bt);
 244 
 245   if (elemtype == TypeInt::BOOL) {
 246     bt = T_BOOLEAN;
 247   } else if (bt == T_OBJECT) {
 248     elemtype = elemtype-&gt;make_oopptr();
 249     const Type* tval = _gvn.type(cast_val);
 250     // We may have lost type information for &#39;val&#39; here due to the casts
 251     // emitted by the array_store_check code (see JDK-6312651)
 252     // TODO Remove this code once JDK-6312651 is in.
 253     const Type* tval_init = _gvn.type(val);
<span class="line-modified"> 254     bool not_inline = !tval-&gt;isa_valuetype() &amp;&amp; (tval == TypePtr::NULL_PTR || !tval_init-&gt;is_oopptr()-&gt;can_be_value_type() || !tval-&gt;is_oopptr()-&gt;can_be_value_type());</span>
<span class="line-modified"> 255     bool not_flattened = !UseFlatArray || not_inline || ((tval_init-&gt;is_valuetypeptr() || tval_init-&gt;isa_valuetype()) &amp;&amp; !tval_init-&gt;value_klass()-&gt;flatten_array());</span>
 256 
 257     if (!ary_t-&gt;is_not_null_free() &amp;&amp; not_inline &amp;&amp; (!tval-&gt;maybe_null() || !tval_init-&gt;maybe_null())) {
 258       // Storing a non-inline type, mark array as not null-free (-&gt; not flat).
 259       // This is only legal for non-null stores because the array_store_check always passes for null.
<span class="line-modified"> 260       // Null stores are handled in GraphKit::gen_value_array_null_guard().</span>
 261       ary_t = ary_t-&gt;cast_to_not_null_free();
 262       Node* cast = _gvn.transform(new CheckCastPPNode(control(), ary, ary_t));
 263       replace_in_map(ary, cast);
 264       ary = cast;
 265     } else if (!ary_t-&gt;is_not_flat() &amp;&amp; not_flattened) {
 266       // Storing a non-flattened value, mark array as not flat.
 267       ary_t = ary_t-&gt;cast_to_not_flat();
 268       Node* cast = _gvn.transform(new CheckCastPPNode(control(), ary, ary_t));
 269       replace_in_map(ary, cast);
 270       ary = cast;
 271     }
 272 
<span class="line-modified"> 273     if (ary_t-&gt;elem()-&gt;isa_valuetype() != NULL) {</span>
<span class="line-modified"> 274       // Store to flattened value type array</span>
 275       C-&gt;set_flattened_accesses();
<span class="line-modified"> 276       if (!cast_val-&gt;is_ValueType()) {</span>
 277         inc_sp(3);
 278         cast_val = null_check(cast_val);
 279         if (stopped()) return;
 280         dec_sp(3);
<span class="line-modified"> 281         cast_val = ValueTypeNode::make_from_oop(this, cast_val, ary_t-&gt;elem()-&gt;value_klass());</span>
 282       }
 283       // Re-execute flattened array store if buffering triggers deoptimization
 284       PreserveReexecuteState preexecs(this);
 285       inc_sp(3);
 286       jvms()-&gt;set_should_reexecute(true);
<span class="line-modified"> 287       cast_val-&gt;as_ValueType()-&gt;store_flattened(this, ary, adr, NULL, 0, MO_UNORDERED | IN_HEAP | IS_ARRAY);</span>
 288       return;
<span class="line-modified"> 289     } else if (elemtype-&gt;is_valuetypeptr() &amp;&amp; !elemtype-&gt;maybe_null()) {</span>
 290       // Store to non-flattened inline type array (elements can never be null)
<span class="line-modified"> 291       if (!cast_val-&gt;is_ValueType() &amp;&amp; tval-&gt;maybe_null()) {</span>
 292         inc_sp(3);
 293         cast_val = null_check(cast_val);
 294         if (stopped()) return;
 295         dec_sp(3);
 296       }
 297     } else if (!ary_t-&gt;is_not_flat()) {
 298       // Array might be flattened, emit runtime checks
<span class="line-modified"> 299       assert(UseFlatArray &amp;&amp; !not_flattened &amp;&amp; elemtype-&gt;is_oopptr()-&gt;can_be_value_type() &amp;&amp;</span>
 300              !ary_t-&gt;klass_is_exact() &amp;&amp; !ary_t-&gt;is_not_null_free(), &quot;array can&#39;t be flattened&quot;);
 301       IdealKit ideal(this);
 302       ideal.if_then(is_non_flattened_array(ary)); {
 303         // non-flattened
 304         assert(ideal.ctrl()-&gt;in(0)-&gt;as_If()-&gt;is_non_flattened_array_check(&amp;_gvn), &quot;Should be found&quot;);
 305         sync_kit(ideal);
<span class="line-modified"> 306         gen_value_array_null_guard(ary, cast_val, 3);</span>
 307         inc_sp(3);
 308         access_store_at(ary, adr, adr_type, cast_val, elemtype, bt, MO_UNORDERED | IN_HEAP | IS_ARRAY, false);
 309         dec_sp(3);
 310         ideal.sync_kit(this);
 311       } ideal.else_(); {
 312         Node* val = cast_val;
 313         // flattened
<span class="line-modified"> 314         if (!val-&gt;is_ValueType() &amp;&amp; tval-&gt;maybe_null()) {</span>
 315           // Add null check
 316           sync_kit(ideal);
 317           Node* null_ctl = top();
 318           val = null_check_oop(val, &amp;null_ctl);
 319           if (null_ctl != top()) {
 320             PreserveJVMState pjvms(this);
 321             inc_sp(3);
 322             set_control(null_ctl);
 323             uncommon_trap(Deoptimization::Reason_null_check, Deoptimization::Action_none);
 324             dec_sp(3);
 325           }
 326           ideal.sync_kit(this);
 327         }
<span class="line-modified"> 328         // Try to determine the value klass</span>
<span class="line-modified"> 329         ciValueKlass* vk = NULL;</span>
<span class="line-modified"> 330         if (tval-&gt;isa_valuetype() || tval-&gt;is_valuetypeptr()) {</span>
<span class="line-modified"> 331           vk = tval-&gt;value_klass();</span>
<span class="line-modified"> 332         } else if (tval_init-&gt;isa_valuetype() || tval_init-&gt;is_valuetypeptr()) {</span>
<span class="line-modified"> 333           vk = tval_init-&gt;value_klass();</span>
<span class="line-modified"> 334         } else if (elemtype-&gt;is_valuetypeptr()) {</span>
<span class="line-modified"> 335           vk = elemtype-&gt;value_klass();</span>
 336         }
 337         Node* casted_ary = ary;
 338         if (vk != NULL &amp;&amp; !stopped()) {
 339           // Element type is known, cast and store to flattened representation
 340           sync_kit(ideal);
 341           assert(vk-&gt;flatten_array() &amp;&amp; elemtype-&gt;maybe_null(), &quot;never/always flat - should be optimized&quot;);
 342           ciArrayKlass* array_klass = ciArrayKlass::make(vk);
 343           const TypeAryPtr* arytype = TypeOopPtr::make_from_klass(array_klass)-&gt;isa_aryptr();
 344           casted_ary = _gvn.transform(new CheckCastPPNode(control(), casted_ary, arytype));
 345           Node* casted_adr = array_element_address(casted_ary, idx, T_OBJECT, arytype-&gt;size(), control());
<span class="line-modified"> 346           if (!val-&gt;is_ValueType()) {</span>
<span class="line-modified"> 347             assert(!gvn().type(val)-&gt;maybe_null(), &quot;value type array elements should never be null&quot;);</span>
<span class="line-modified"> 348             val = ValueTypeNode::make_from_oop(this, val, vk);</span>
 349           }
 350           // Re-execute flattened array store if buffering triggers deoptimization
 351           PreserveReexecuteState preexecs(this);
 352           inc_sp(3);
 353           jvms()-&gt;set_should_reexecute(true);
<span class="line-modified"> 354           val-&gt;as_ValueType()-&gt;store_flattened(this, casted_ary, casted_adr, NULL, 0, MO_UNORDERED | IN_HEAP | IS_ARRAY);</span>
 355           ideal.sync_kit(this);
 356         } else if (!ideal.ctrl()-&gt;is_top()) {
 357           // Element type is unknown, emit runtime call
 358           sync_kit(ideal);
 359 
 360           // This membar keeps this access to an unknown flattened
 361           // array correctly ordered with other unknown and known
 362           // flattened array accesses.
<span class="line-modified"> 363           insert_mem_bar_volatile(Op_MemBarCPUOrder, C-&gt;get_alias_index(TypeAryPtr::VALUES));</span>
 364           ideal.sync_kit(this);
 365 
<span class="line-modified"> 366           ideal.make_leaf_call(OptoRuntime::store_unknown_value_Type(),</span>
<span class="line-modified"> 367                                CAST_FROM_FN_PTR(address, OptoRuntime::store_unknown_value),</span>
<span class="line-modified"> 368                                &quot;store_unknown_value&quot;,</span>
 369                                val, casted_ary, idx);
 370 
 371           sync_kit(ideal);
 372           // Same as MemBarCPUOrder above: keep this unknown
 373           // flattened array access correctly ordered with other
 374           // flattened array accesses.
<span class="line-modified"> 375           insert_mem_bar_volatile(Op_MemBarCPUOrder, C-&gt;get_alias_index(TypeAryPtr::VALUES));</span>
 376           ideal.sync_kit(this);
 377         }
 378       }
 379       ideal.end_if();
 380       sync_kit(ideal);
 381       return;
 382     } else if (!ary_t-&gt;is_not_null_free()) {
 383       // Array is not flattened but may be null free
<span class="line-modified"> 384       assert(elemtype-&gt;is_oopptr()-&gt;can_be_value_type() &amp;&amp; !ary_t-&gt;klass_is_exact(), &quot;array can&#39;t be null free&quot;);</span>
<span class="line-modified"> 385       ary = gen_value_array_null_guard(ary, cast_val, 3, true);</span>
 386     }
 387   }
 388   inc_sp(3);
 389   access_store_at(ary, adr, adr_type, val, elemtype, bt, MO_UNORDERED | IN_HEAP | IS_ARRAY);
 390   dec_sp(3);
 391 }
 392 
 393 
 394 //------------------------------array_addressing-------------------------------
 395 // Pull array and index from the stack.  Compute pointer-to-element.
 396 Node* Parse::array_addressing(BasicType type, int vals, const Type*&amp; elemtype) {
 397   Node *idx   = peek(0+vals);   // Get from stack without popping
 398   Node *ary   = peek(1+vals);   // in case of exception
 399 
 400   // Null check the array base, with correct stack contents
 401   ary = null_check(ary, T_ARRAY);
 402   // Compile-time detect of null-exception?
 403   if (stopped())  return top();
 404 
 405   const TypeAryPtr* arytype  = _gvn.type(ary)-&gt;is_aryptr();
</pre>
<hr />
<pre>
 470       if (C-&gt;allow_range_check_smearing()) {
 471         // Do not use builtin_throw, since range checks are sometimes
 472         // made more stringent by an optimistic transformation.
 473         // This creates &quot;tentative&quot; range checks at this point,
 474         // which are not guaranteed to throw exceptions.
 475         // See IfNode::Ideal, is_range_check, adjust_check.
 476         uncommon_trap(Deoptimization::Reason_range_check,
 477                       Deoptimization::Action_make_not_entrant,
 478                       NULL, &quot;range_check&quot;);
 479       } else {
 480         // If we have already recompiled with the range-check-widening
 481         // heroic optimization turned off, then we must really be throwing
 482         // range check exceptions.
 483         builtin_throw(Deoptimization::Reason_range_check, idx);
 484       }
 485     }
 486   }
 487   // Check for always knowing you are throwing a range-check exception
 488   if (stopped())  return top();
 489 
<span class="line-modified"> 490   // This could be an access to a value array. We can&#39;t tell if it&#39;s</span>
 491   // flat or not. Speculating it&#39;s not leads to a much simpler graph
 492   // shape. Check profiling.
 493   // For aastore, by the time we&#39;re here, the array store check should
 494   // have already taken advantage of profiling to cast the array to an
 495   // exact type reported by profiling
 496   const TypeOopPtr* elemptr = elemtype-&gt;make_oopptr();
<span class="line-modified"> 497   if (elemtype-&gt;isa_valuetype() == NULL &amp;&amp;</span>
<span class="line-modified"> 498       (elemptr == NULL || !elemptr-&gt;is_valuetypeptr() || elemptr-&gt;maybe_null()) &amp;&amp;</span>
 499       !arytype-&gt;is_not_flat()) {
 500     assert(is_reference_type(type), &quot;Only references&quot;);
 501     // First check the speculative type
 502     Deoptimization::DeoptReason reason = Deoptimization::Reason_speculate_class_check;
 503     ciKlass* array_type = arytype-&gt;speculative_type();
 504     if (too_many_traps_or_recompiles(reason) || array_type == NULL) {
 505       // No speculative type, check profile data at this bci
 506       array_type = NULL;
 507       reason = Deoptimization::Reason_class_check;
 508       if (UseArrayLoadStoreProfile &amp;&amp; !too_many_traps_or_recompiles(reason)) {
 509         ciKlass* element_type = NULL;
 510         ProfilePtrKind element_ptr = ProfileMaybeNull;
 511         bool flat_array = true;
 512         bool null_free_array = true;
 513         method()-&gt;array_access_profiled_type(bci(), array_type, element_type, element_ptr, flat_array, null_free_array);
 514       }
 515     }
 516     if (array_type != NULL) {
 517       // Speculate that this array has the exact type reported by profile data
 518       Node* better_ary = NULL;
</pre>
<hr />
<pre>
 530     // No need to speculate: feed profile data at this bci for the
 531     // array to type speculation
 532     ciKlass* array_type = NULL;
 533     ciKlass* element_type = NULL;
 534     ProfilePtrKind element_ptr = ProfileMaybeNull;
 535     bool flat_array = true;
 536     bool null_free_array = true;
 537     method()-&gt;array_access_profiled_type(bci(), array_type, element_type, element_ptr, flat_array, null_free_array);
 538     if (array_type != NULL) {
 539       record_profile_for_speculation(ary, array_type, ProfileMaybeNull);
 540     }
 541   }
 542 
 543   // We have no exact array type from profile data. Check profile data
 544   // for a non null free or non flat array. Non null free implies non
 545   // flat so check this one first. Speculating on a non null free
 546   // array doesn&#39;t help aaload but could be profitable for a
 547   // subsequent aastore.
 548   elemptr = elemtype-&gt;make_oopptr();
 549   if (!arytype-&gt;is_not_null_free() &amp;&amp;
<span class="line-modified"> 550       elemtype-&gt;isa_valuetype() == NULL &amp;&amp;</span>
<span class="line-modified"> 551       (elemptr == NULL || !elemptr-&gt;is_valuetypeptr()) &amp;&amp;</span>
 552       UseArrayLoadStoreProfile) {
 553     assert(is_reference_type(type), &quot;&quot;);
 554     bool null_free_array = true;
 555     Deoptimization::DeoptReason reason = Deoptimization::Reason_none;
 556     if (arytype-&gt;speculative() != NULL &amp;&amp;
 557         arytype-&gt;speculative()-&gt;is_aryptr()-&gt;is_not_null_free() &amp;&amp;
 558         !too_many_traps_or_recompiles(Deoptimization::Reason_speculate_class_check)) {
 559       null_free_array = false;
 560       reason = Deoptimization::Reason_speculate_class_check;
 561     } else if (!too_many_traps_or_recompiles(Deoptimization::Reason_class_check)) {
 562       ciKlass* array_type = NULL;
 563       ciKlass* element_type = NULL;
 564       ProfilePtrKind element_ptr = ProfileMaybeNull;
 565       bool flat_array = true;
 566       method()-&gt;array_access_profiled_type(bci(), array_type, element_type, element_ptr, flat_array, null_free_array);
 567       reason = Deoptimization::Reason_class_check;
 568     }
 569     if (!null_free_array) {
 570       { // Deoptimize if null-free array
 571         BuildCutout unless(this, is_nullable_array(ary), PROB_MAX);
 572         uncommon_trap_exact(reason, Deoptimization::Action_maybe_recompile);
 573       }
 574       Node* better_ary = _gvn.transform(new CheckCastPPNode(control(), ary, arytype-&gt;cast_to_not_null_free()));
 575       replace_in_map(ary, better_ary);
 576       ary = better_ary;
 577       arytype = _gvn.type(ary)-&gt;is_aryptr();
 578     }
 579   }
 580 
<span class="line-modified"> 581   if (!arytype-&gt;is_not_flat() &amp;&amp; elemtype-&gt;isa_valuetype() == NULL) {</span>
 582     assert(is_reference_type(type), &quot;&quot;);
 583     bool flat_array = true;
 584     Deoptimization::DeoptReason reason = Deoptimization::Reason_none;
 585     if (arytype-&gt;speculative() != NULL &amp;&amp;
 586         arytype-&gt;speculative()-&gt;is_aryptr()-&gt;is_not_flat() &amp;&amp;
 587         !too_many_traps_or_recompiles(Deoptimization::Reason_speculate_class_check)) {
 588       flat_array = false;
 589       reason = Deoptimization::Reason_speculate_class_check;
 590     } else if (UseArrayLoadStoreProfile &amp;&amp; !too_many_traps_or_recompiles(reason)) {
 591       ciKlass* array_type = NULL;
 592       ciKlass* element_type = NULL;
 593       ProfilePtrKind element_ptr = ProfileMaybeNull;
 594       bool null_free_array = true;
 595       method()-&gt;array_access_profiled_type(bci(), array_type, element_type, element_ptr, flat_array, null_free_array);
 596       reason = Deoptimization::Reason_class_check;
 597     }
 598     if (!flat_array) {
 599       { // Deoptimize if flat array
 600         BuildCutout unless(this, is_non_flattened_array(ary), PROB_MAX);
 601         uncommon_trap_exact(reason, Deoptimization::Action_maybe_recompile);
</pre>
<hr />
<pre>
2042     }
2043   } else {
2044     // Update method data
2045     profile_not_taken_branch();
2046     adjust_map_after_if(untaken_btest, c, untaken_prob, next_block);
2047   }
2048 }
2049 
2050 void Parse::do_acmp(BoolTest::mask btest, Node* a, Node* b) {
2051   ciMethod* subst_method = ciEnv::current()-&gt;ValueBootstrapMethods_klass()-&gt;find_method(ciSymbol::isSubstitutable_name(), ciSymbol::object_object_boolean_signature());
2052   // If current method is ValueBootstrapMethods::isSubstitutable(),
2053   // compile the acmp as a regular pointer comparison otherwise we
2054   // could call ValueBootstrapMethods::isSubstitutable() back
2055   if (!EnableValhalla || (method() == subst_method)) {
2056     Node* cmp = CmpP(a, b);
2057     cmp = optimize_cmp_with_klass(cmp);
2058     do_if(btest, cmp);
2059     return;
2060   }
2061 
<span class="line-modified">2062   // Allocate value type operands and re-execute on deoptimization</span>
<span class="line-modified">2063   if (a-&gt;is_ValueType()) {</span>
2064     PreserveReexecuteState preexecs(this);
2065     inc_sp(2);
2066     jvms()-&gt;set_should_reexecute(true);
<span class="line-modified">2067     a = a-&gt;as_ValueType()-&gt;buffer(this)-&gt;get_oop();</span>
2068   }
<span class="line-modified">2069   if (b-&gt;is_ValueType()) {</span>
2070     PreserveReexecuteState preexecs(this);
2071     inc_sp(2);
2072     jvms()-&gt;set_should_reexecute(true);
<span class="line-modified">2073     b = b-&gt;as_ValueType()-&gt;buffer(this)-&gt;get_oop();</span>
2074   }
2075 
2076   // First, do a normal pointer comparison
2077   const TypeOopPtr* ta = _gvn.type(a)-&gt;isa_oopptr();
2078   const TypeOopPtr* tb = _gvn.type(b)-&gt;isa_oopptr();
2079   Node* cmp = CmpP(a, b);
2080   cmp = optimize_cmp_with_klass(cmp);
<span class="line-modified">2081   if (ta == NULL || !ta-&gt;can_be_value_type() ||</span>
<span class="line-modified">2082       tb == NULL || !tb-&gt;can_be_value_type()) {</span>
<span class="line-modified">2083     // This is sufficient, if one of the operands can&#39;t be a value type</span>
2084     do_if(btest, cmp);
2085     return;
2086   }
2087   Node* eq_region = NULL;
2088   if (btest == BoolTest::eq) {
2089     do_if(btest, cmp, true);
2090     if (stopped()) {
2091       return;
2092     }
2093   } else {
2094     assert(btest == BoolTest::ne, &quot;only eq or ne&quot;);
2095     Node* is_not_equal = NULL;
2096     eq_region = new RegionNode(3);
2097     {
2098       PreserveJVMState pjvms(this);
2099       do_if(btest, cmp, false, &amp;is_not_equal);
2100       if (!stopped()) {
2101         eq_region-&gt;init_req(1, control());
2102       }
2103     }
</pre>
<hr />
<pre>
2114   inc_sp(2);
2115   Node* null_ctl = top();
2116   Node* not_null_a = null_check_oop(a, &amp;null_ctl, !too_many_traps(Deoptimization::Reason_null_check), false, false);
2117   dec_sp(2);
2118   ne_region-&gt;init_req(1, null_ctl);
2119   if (stopped()) {
2120     record_for_igvn(ne_region);
2121     set_control(_gvn.transform(ne_region));
2122     if (btest == BoolTest::ne) {
2123       {
2124         PreserveJVMState pjvms(this);
2125         int target_bci = iter().get_dest();
2126         merge(target_bci);
2127       }
2128       record_for_igvn(eq_region);
2129       set_control(_gvn.transform(eq_region));
2130     }
2131     return;
2132   }
2133 
<span class="line-modified">2134   // First operand is non-null, check if it is a value type</span>
<span class="line-modified">2135   Node* is_value = is_value_type(not_null_a);</span>
2136   IfNode* is_value_iff = create_and_map_if(control(), is_value, PROB_FAIR, COUNT_UNKNOWN);
2137   Node* not_value = _gvn.transform(new IfFalseNode(is_value_iff));
2138   ne_region-&gt;init_req(2, not_value);
2139   set_control(_gvn.transform(new IfTrueNode(is_value_iff)));
2140 
<span class="line-modified">2141   // The first operand is a value type, check if the second operand is non-null</span>
2142   inc_sp(2);
2143   null_ctl = top();
2144   Node* not_null_b = null_check_oop(b, &amp;null_ctl, !too_many_traps(Deoptimization::Reason_null_check), false, false);
2145   dec_sp(2);
2146   ne_region-&gt;init_req(3, null_ctl);
2147   if (stopped()) {
2148     record_for_igvn(ne_region);
2149     set_control(_gvn.transform(ne_region));
2150     if (btest == BoolTest::ne) {
2151       {
2152         PreserveJVMState pjvms(this);
2153         int target_bci = iter().get_dest();
2154         merge(target_bci);
2155       }
2156       record_for_igvn(eq_region);
2157       set_control(_gvn.transform(eq_region));
2158     }
2159     return;
2160   }
2161 
</pre>
<hr />
<pre>
2470   if (c-&gt;Opcode() == Op_CmpP &amp;&amp;
2471       (c-&gt;in(1)-&gt;Opcode() == Op_LoadKlass || c-&gt;in(1)-&gt;Opcode() == Op_DecodeNKlass) &amp;&amp;
2472       c-&gt;in(2)-&gt;is_Con()) {
2473     Node* load_klass = NULL;
2474     Node* decode = NULL;
2475     if (c-&gt;in(1)-&gt;Opcode() == Op_DecodeNKlass) {
2476       decode = c-&gt;in(1);
2477       load_klass = c-&gt;in(1)-&gt;in(1);
2478     } else {
2479       load_klass = c-&gt;in(1);
2480     }
2481     if (load_klass-&gt;in(2)-&gt;is_AddP()) {
2482       Node* addp = load_klass-&gt;in(2);
2483       Node* obj = addp-&gt;in(AddPNode::Address);
2484       const TypeOopPtr* obj_type = _gvn.type(obj)-&gt;is_oopptr();
2485       if (obj_type-&gt;speculative_type_not_null() != NULL) {
2486         ciKlass* k = obj_type-&gt;speculative_type();
2487         inc_sp(2);
2488         obj = maybe_cast_profiled_obj(obj, k);
2489         dec_sp(2);
<span class="line-modified">2490         if (obj-&gt;is_ValueType()) {</span>
<span class="line-modified">2491           assert(obj-&gt;as_ValueType()-&gt;is_allocated(&amp;_gvn), &quot;must be allocated&quot;);</span>
<span class="line-modified">2492           obj = obj-&gt;as_ValueType()-&gt;get_oop();</span>
2493         }
2494         // Make the CmpP use the casted obj
2495         addp = basic_plus_adr(obj, addp-&gt;in(AddPNode::Offset));
2496         load_klass = load_klass-&gt;clone();
2497         load_klass-&gt;set_req(2, addp);
2498         load_klass = _gvn.transform(load_klass);
2499         if (decode != NULL) {
2500           decode = decode-&gt;clone();
2501           decode-&gt;set_req(1, load_klass);
2502           load_klass = _gvn.transform(decode);
2503         }
2504         c = c-&gt;clone();
2505         c-&gt;set_req(1, load_klass);
2506         c = _gvn.transform(c);
2507       }
2508     }
2509   }
2510   return c;
2511 }
2512 
</pre>
<hr />
<pre>
3321     // See if we can get some profile data and hand it off to the next block
3322     Block *target_block = block()-&gt;successor_for_bci(target_bci);
3323     if (target_block-&gt;pred_count() != 1)  break;
3324     ciMethodData* methodData = method()-&gt;method_data();
3325     if (!methodData-&gt;is_mature())  break;
3326     ciProfileData* data = methodData-&gt;bci_to_data(bci());
3327     assert(data != NULL &amp;&amp; data-&gt;is_JumpData(), &quot;need JumpData for taken branch&quot;);
3328     int taken = ((ciJumpData*)data)-&gt;taken();
3329     taken = method()-&gt;scale_count(taken);
3330     target_block-&gt;set_count(taken);
3331     break;
3332   }
3333 
3334   case Bytecodes::_ifnull:    btest = BoolTest::eq; goto handle_if_null;
3335   case Bytecodes::_ifnonnull: btest = BoolTest::ne; goto handle_if_null;
3336   handle_if_null:
3337     // If this is a backwards branch in the bytecodes, add Safepoint
3338     maybe_add_safepoint(iter().get_dest());
3339     a = null();
3340     b = pop();
<span class="line-modified">3341     if (b-&gt;is_ValueType()) {</span>
3342       // Return constant false because &#39;b&#39; is always non-null
3343       c = _gvn.makecon(TypeInt::CC_GT);
3344     } else {
3345       if (!_gvn.type(b)-&gt;speculative_maybe_null() &amp;&amp;
3346           !too_many_traps(Deoptimization::Reason_speculate_null_check)) {
3347         inc_sp(1);
3348         Node* null_ctl = top();
3349         b = null_check_oop(b, &amp;null_ctl, true, true, true);
3350         assert(null_ctl-&gt;is_top(), &quot;no null control here&quot;);
3351         dec_sp(1);
3352       } else if (_gvn.type(b)-&gt;speculative_always_null() &amp;&amp;
3353                  !too_many_traps(Deoptimization::Reason_speculate_null_assert)) {
3354         inc_sp(1);
3355         b = null_assert(b);
3356         dec_sp(1);
3357       }
3358       c = _gvn.transform( new CmpPNode(b, a) );
3359     }
3360     do_ifnull(btest, c);
3361     break;
</pre>
</td>
<td>
<hr />
<pre>
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #include &quot;precompiled.hpp&quot;
  26 #include &quot;ci/ciMethodData.hpp&quot;
  27 #include &quot;classfile/systemDictionary.hpp&quot;
  28 #include &quot;classfile/vmSymbols.hpp&quot;
  29 #include &quot;compiler/compileLog.hpp&quot;
  30 #include &quot;interpreter/linkResolver.hpp&quot;
  31 #include &quot;memory/resourceArea.hpp&quot;
  32 #include &quot;memory/universe.hpp&quot;
  33 #include &quot;oops/oop.inline.hpp&quot;
  34 #include &quot;opto/addnode.hpp&quot;
  35 #include &quot;opto/castnode.hpp&quot;
  36 #include &quot;opto/convertnode.hpp&quot;
  37 #include &quot;opto/divnode.hpp&quot;
  38 #include &quot;opto/idealGraphPrinter.hpp&quot;
  39 #include &quot;opto/idealKit.hpp&quot;
<span class="line-added">  40 #include &quot;opto/inlinetypenode.hpp&quot;</span>
  41 #include &quot;opto/matcher.hpp&quot;
  42 #include &quot;opto/memnode.hpp&quot;
  43 #include &quot;opto/mulnode.hpp&quot;
  44 #include &quot;opto/opaquenode.hpp&quot;
  45 #include &quot;opto/parse.hpp&quot;
  46 #include &quot;opto/runtime.hpp&quot;

  47 #include &quot;runtime/deoptimization.hpp&quot;
  48 #include &quot;runtime/sharedRuntime.hpp&quot;
  49 
  50 #ifndef PRODUCT
  51 extern int explicit_null_checks_inserted,
  52            explicit_null_checks_elided;
  53 #endif
  54 
  55 Node* Parse::record_profile_for_speculation_at_array_load(Node* ld) {
  56   // Feed unused profile data to type speculation
  57   if (UseTypeSpeculation &amp;&amp; UseArrayLoadStoreProfile) {
  58     ciKlass* array_type = NULL;
  59     ciKlass* element_type = NULL;
  60     ProfilePtrKind element_ptr = ProfileMaybeNull;
  61     bool flat_array = true;
  62     bool null_free_array = true;
  63     method()-&gt;array_access_profiled_type(bci(), array_type, element_type, element_ptr, flat_array, null_free_array);
  64     if (element_type != NULL || element_ptr != ProfileMaybeNull) {
  65       ld = record_profile_for_speculation(ld, element_type, element_ptr);
  66     }
  67   }
  68   return ld;
  69 }
  70 
  71 
  72 //---------------------------------array_load----------------------------------
  73 void Parse::array_load(BasicType bt) {
  74   const Type* elemtype = Type::TOP;
  75   Node* adr = array_addressing(bt, 0, elemtype);
  76   if (stopped())  return;     // guaranteed null or range check
  77 
  78   Node* idx = pop();
  79   Node* ary = pop();
  80 
<span class="line-modified">  81   // Handle inline type arrays</span>
  82   const TypeOopPtr* elemptr = elemtype-&gt;make_oopptr();
  83   const TypeAryPtr* ary_t = _gvn.type(ary)-&gt;is_aryptr();
<span class="line-modified">  84   if (elemtype-&gt;isa_inlinetype() != NULL) {</span>
  85     C-&gt;set_flattened_accesses();
<span class="line-modified">  86     // Load from flattened inline type array</span>
<span class="line-modified">  87     Node* vt = InlineTypeNode::make_from_flattened(this, elemtype-&gt;inline_klass(), ary, adr);</span>
  88     push(vt);
  89     return;
<span class="line-modified">  90   } else if (elemptr != NULL &amp;&amp; elemptr-&gt;is_inlinetypeptr() &amp;&amp; !elemptr-&gt;maybe_null()) {</span>
  91     // Load from non-flattened inline type array (elements can never be null)
  92     bt = T_INLINE_TYPE;
  93   } else if (!ary_t-&gt;is_not_flat()) {
  94     // Cannot statically determine if array is flattened, emit runtime check
<span class="line-modified">  95     assert(UseFlatArray &amp;&amp; is_reference_type(bt) &amp;&amp; elemptr-&gt;can_be_inline_type() &amp;&amp; !ary_t-&gt;klass_is_exact() &amp;&amp; !ary_t-&gt;is_not_null_free() &amp;&amp;</span>
<span class="line-modified">  96            (!elemptr-&gt;is_inlinetypeptr() || elemptr-&gt;inline_klass()-&gt;flatten_array()), &quot;array can&#39;t be flattened&quot;);</span>
  97     IdealKit ideal(this);
  98     IdealVariable res(ideal);
  99     ideal.declarations_done();
 100     ideal.if_then(is_non_flattened_array(ary)); {
 101       // non-flattened
 102       assert(ideal.ctrl()-&gt;in(0)-&gt;as_If()-&gt;is_non_flattened_array_check(&amp;_gvn), &quot;Should be found&quot;);
 103       sync_kit(ideal);
 104       const TypeAryPtr* adr_type = TypeAryPtr::get_array_body_type(bt);
 105       Node* ld = access_load_at(ary, adr, adr_type, elemptr, bt,
 106                                 IN_HEAP | IS_ARRAY | C2_CONTROL_DEPENDENT_LOAD);
 107       ideal.sync_kit(this);
 108       ideal.set(res, ld);
 109     } ideal.else_(); {
 110       // flattened
 111       sync_kit(ideal);
<span class="line-modified"> 112       if (elemptr-&gt;is_inlinetypeptr()) {</span>
 113         // Element type is known, cast and load from flattened representation
<span class="line-modified"> 114         ciInlineKlass* vk = elemptr-&gt;inline_klass();</span>
 115         assert(vk-&gt;flatten_array() &amp;&amp; elemptr-&gt;maybe_null(), &quot;never/always flat - should be optimized&quot;);
 116         ciArrayKlass* array_klass = ciArrayKlass::make(vk);
 117         const TypeAryPtr* arytype = TypeOopPtr::make_from_klass(array_klass)-&gt;isa_aryptr();
 118         Node* cast = _gvn.transform(new CheckCastPPNode(control(), ary, arytype));
 119         Node* casted_adr = array_element_address(cast, idx, T_INLINE_TYPE, ary_t-&gt;size(), control());
 120         // Re-execute flattened array load if buffering triggers deoptimization
 121         PreserveReexecuteState preexecs(this);
 122         jvms()-&gt;set_should_reexecute(true);
 123         inc_sp(2);
<span class="line-modified"> 124         Node* vt = InlineTypeNode::make_from_flattened(this, vk, cast, casted_adr)-&gt;buffer(this, false);</span>
 125         ideal.set(res, vt);
 126         ideal.sync_kit(this);
 127       } else {
 128         // Element type is unknown, emit runtime call
 129         Node* kls = load_object_klass(ary);
 130         Node* k_adr = basic_plus_adr(kls, in_bytes(ArrayKlass::element_klass_offset()));
 131         Node* elem_klass = _gvn.transform(LoadKlassNode::make(_gvn, NULL, immutable_memory(), k_adr, TypeInstPtr::KLASS));
 132         Node* obj_size  = NULL;
 133         kill_dead_locals();
 134         // Re-execute flattened array load if buffering triggers deoptimization
 135         PreserveReexecuteState preexecs(this);
 136         jvms()-&gt;set_bci(_bci);
 137         jvms()-&gt;set_should_reexecute(true);
 138         inc_sp(2);
 139         Node* alloc_obj = new_instance(elem_klass, NULL, &amp;obj_size, /*deoptimize_on_exception=*/true);
 140 
 141         AllocateNode* alloc = AllocateNode::Ideal_allocation(alloc_obj, &amp;_gvn);
 142         assert(alloc-&gt;maybe_set_complete(&amp;_gvn), &quot;&quot;);
 143         alloc-&gt;initialization()-&gt;set_complete_with_arraycopy();
 144 
 145         // This membar keeps this access to an unknown flattened array
 146         // correctly ordered with other unknown and known flattened
 147         // array accesses.
<span class="line-modified"> 148         insert_mem_bar_volatile(Op_MemBarCPUOrder, C-&gt;get_alias_index(TypeAryPtr::INLINES));</span>
 149 
 150         BarrierSetC2* bs = BarrierSet::barrier_set()-&gt;barrier_set_c2();
<span class="line-modified"> 151         // Unknown inline type might contain reference fields</span>
 152         if (false &amp;&amp; !bs-&gt;array_copy_requires_gc_barriers(false, T_OBJECT, false, BarrierSetC2::Parsing)) {
 153           // FIXME 8230656 also merge changes from 8238759 in
 154           int base_off = sizeof(instanceOopDesc);
 155           Node* dst_base = basic_plus_adr(alloc_obj, base_off);
 156           Node* countx = obj_size;
 157           countx = _gvn.transform(new SubXNode(countx, MakeConX(base_off)));
 158           countx = _gvn.transform(new URShiftXNode(countx, intcon(LogBytesPerLong)));
 159 
 160           assert(Klass::_lh_log2_element_size_shift == 0, &quot;use shift in place&quot;);
 161           Node* lhp = basic_plus_adr(kls, in_bytes(Klass::layout_helper_offset()));
 162           Node* elem_shift = make_load(NULL, lhp, TypeInt::INT, T_INT, MemNode::unordered);
 163           uint header = arrayOopDesc::base_offset_in_bytes(T_INLINE_TYPE);
 164           Node* base  = basic_plus_adr(ary, header);
 165           idx = Compile::conv_I2X_index(&amp;_gvn, idx, TypeInt::POS, control());
 166           Node* scale = _gvn.transform(new LShiftXNode(idx, elem_shift));
 167           Node* adr = basic_plus_adr(ary, base, scale);
 168 
 169           access_clone(adr, dst_base, countx, false);
 170         } else {
 171           ideal.sync_kit(this);
<span class="line-modified"> 172           ideal.make_leaf_call(OptoRuntime::load_unknown_inline_type(),</span>
<span class="line-modified"> 173                                CAST_FROM_FN_PTR(address, OptoRuntime::load_unknown_inline),</span>
<span class="line-modified"> 174                                &quot;load_unknown_inline&quot;,</span>
 175                                ary, idx, alloc_obj);
 176           sync_kit(ideal);
 177         }
 178 
<span class="line-modified"> 179         // This makes sure no other thread sees a partially initialized buffered inline type</span>
 180         insert_mem_bar_volatile(Op_MemBarStoreStore, Compile::AliasIdxRaw, alloc-&gt;proj_out_or_null(AllocateNode::RawAddress));
 181 
 182         // Same as MemBarCPUOrder above: keep this unknown flattened
 183         // array access correctly ordered with other flattened array
 184         // access
<span class="line-modified"> 185         insert_mem_bar_volatile(Op_MemBarCPUOrder, C-&gt;get_alias_index(TypeAryPtr::INLINES));</span>
 186 
<span class="line-modified"> 187         // Prevent any use of the newly allocated inline type before it is fully initialized</span>

 188         alloc_obj = new CastPPNode(alloc_obj, _gvn.type(alloc_obj), true);
 189         alloc_obj-&gt;set_req(0, control());
 190         alloc_obj = _gvn.transform(alloc_obj);
 191 
 192         const Type* unknown_value = elemptr-&gt;is_instptr()-&gt;cast_to_flat_array();
 193         alloc_obj = _gvn.transform(new CheckCastPPNode(control(), alloc_obj, unknown_value));
 194 
 195         ideal.sync_kit(this);
 196         ideal.set(res, alloc_obj);
 197       }
 198     } ideal.end_if();
 199     sync_kit(ideal);
 200     Node* ld = _gvn.transform(ideal.value(res));
 201     ld = record_profile_for_speculation_at_array_load(ld);
 202     push_node(bt, ld);
 203     return;
 204   }
 205 
 206   if (elemtype == TypeInt::BOOL) {
 207     bt = T_BOOLEAN;
 208   }
 209   const TypeAryPtr* adr_type = TypeAryPtr::get_array_body_type(bt);
 210   Node* ld = access_load_at(ary, adr, adr_type, elemtype, bt,
 211                             IN_HEAP | IS_ARRAY | C2_CONTROL_DEPENDENT_LOAD);
 212   if (bt == T_INLINE_TYPE) {
<span class="line-modified"> 213     // Loading a non-flattened inline type from an array</span>
<span class="line-modified"> 214     assert(!gvn().type(ld)-&gt;maybe_null(), &quot;inline type array elements should never be null&quot;);</span>
<span class="line-modified"> 215     if (elemptr-&gt;inline_klass()-&gt;is_scalarizable()) {</span>
<span class="line-modified"> 216       ld = InlineTypeNode::make_from_oop(this, ld, elemptr-&gt;inline_klass());</span>
 217     }
 218   }
<span class="line-modified"> 219   if (!ld-&gt;is_InlineType()) {</span>
 220     ld = record_profile_for_speculation_at_array_load(ld);
 221   }
 222 
 223   push_node(bt, ld);
 224 }
 225 
 226 
 227 //--------------------------------array_store----------------------------------
 228 void Parse::array_store(BasicType bt) {
 229   const Type* elemtype = Type::TOP;
 230   Node* adr = array_addressing(bt, type2size[bt], elemtype);
 231   if (stopped())  return;     // guaranteed null or range check
 232   Node* cast_val = NULL;
 233   if (bt == T_OBJECT) {
 234     cast_val = array_store_check();
 235     if (stopped()) return;
 236   }
 237   Node* val = pop_node(bt); // Value to store
 238   Node* idx = pop();        // Index in the array
 239   Node* ary = pop();        // The array itself
 240 
 241   const TypeAryPtr* ary_t = _gvn.type(ary)-&gt;is_aryptr();
 242   const TypeAryPtr* adr_type = TypeAryPtr::get_array_body_type(bt);
 243 
 244   if (elemtype == TypeInt::BOOL) {
 245     bt = T_BOOLEAN;
 246   } else if (bt == T_OBJECT) {
 247     elemtype = elemtype-&gt;make_oopptr();
 248     const Type* tval = _gvn.type(cast_val);
 249     // We may have lost type information for &#39;val&#39; here due to the casts
 250     // emitted by the array_store_check code (see JDK-6312651)
 251     // TODO Remove this code once JDK-6312651 is in.
 252     const Type* tval_init = _gvn.type(val);
<span class="line-modified"> 253     bool not_inline = !tval-&gt;isa_inlinetype() &amp;&amp; (tval == TypePtr::NULL_PTR || !tval_init-&gt;is_oopptr()-&gt;can_be_inline_type() || !tval-&gt;is_oopptr()-&gt;can_be_inline_type());</span>
<span class="line-modified"> 254     bool not_flattened = !UseFlatArray || not_inline || ((tval_init-&gt;is_inlinetypeptr() || tval_init-&gt;isa_inlinetype()) &amp;&amp; !tval_init-&gt;inline_klass()-&gt;flatten_array());</span>
 255 
 256     if (!ary_t-&gt;is_not_null_free() &amp;&amp; not_inline &amp;&amp; (!tval-&gt;maybe_null() || !tval_init-&gt;maybe_null())) {
 257       // Storing a non-inline type, mark array as not null-free (-&gt; not flat).
 258       // This is only legal for non-null stores because the array_store_check always passes for null.
<span class="line-modified"> 259       // Null stores are handled in GraphKit::gen_inline_array_null_guard().</span>
 260       ary_t = ary_t-&gt;cast_to_not_null_free();
 261       Node* cast = _gvn.transform(new CheckCastPPNode(control(), ary, ary_t));
 262       replace_in_map(ary, cast);
 263       ary = cast;
 264     } else if (!ary_t-&gt;is_not_flat() &amp;&amp; not_flattened) {
 265       // Storing a non-flattened value, mark array as not flat.
 266       ary_t = ary_t-&gt;cast_to_not_flat();
 267       Node* cast = _gvn.transform(new CheckCastPPNode(control(), ary, ary_t));
 268       replace_in_map(ary, cast);
 269       ary = cast;
 270     }
 271 
<span class="line-modified"> 272     if (ary_t-&gt;elem()-&gt;isa_inlinetype() != NULL) {</span>
<span class="line-modified"> 273       // Store to flattened inline type array</span>
 274       C-&gt;set_flattened_accesses();
<span class="line-modified"> 275       if (!cast_val-&gt;is_InlineType()) {</span>
 276         inc_sp(3);
 277         cast_val = null_check(cast_val);
 278         if (stopped()) return;
 279         dec_sp(3);
<span class="line-modified"> 280         cast_val = InlineTypeNode::make_from_oop(this, cast_val, ary_t-&gt;elem()-&gt;inline_klass());</span>
 281       }
 282       // Re-execute flattened array store if buffering triggers deoptimization
 283       PreserveReexecuteState preexecs(this);
 284       inc_sp(3);
 285       jvms()-&gt;set_should_reexecute(true);
<span class="line-modified"> 286       cast_val-&gt;as_InlineType()-&gt;store_flattened(this, ary, adr, NULL, 0, MO_UNORDERED | IN_HEAP | IS_ARRAY);</span>
 287       return;
<span class="line-modified"> 288     } else if (elemtype-&gt;is_inlinetypeptr() &amp;&amp; !elemtype-&gt;maybe_null()) {</span>
 289       // Store to non-flattened inline type array (elements can never be null)
<span class="line-modified"> 290       if (!cast_val-&gt;is_InlineType() &amp;&amp; tval-&gt;maybe_null()) {</span>
 291         inc_sp(3);
 292         cast_val = null_check(cast_val);
 293         if (stopped()) return;
 294         dec_sp(3);
 295       }
 296     } else if (!ary_t-&gt;is_not_flat()) {
 297       // Array might be flattened, emit runtime checks
<span class="line-modified"> 298       assert(UseFlatArray &amp;&amp; !not_flattened &amp;&amp; elemtype-&gt;is_oopptr()-&gt;can_be_inline_type() &amp;&amp;</span>
 299              !ary_t-&gt;klass_is_exact() &amp;&amp; !ary_t-&gt;is_not_null_free(), &quot;array can&#39;t be flattened&quot;);
 300       IdealKit ideal(this);
 301       ideal.if_then(is_non_flattened_array(ary)); {
 302         // non-flattened
 303         assert(ideal.ctrl()-&gt;in(0)-&gt;as_If()-&gt;is_non_flattened_array_check(&amp;_gvn), &quot;Should be found&quot;);
 304         sync_kit(ideal);
<span class="line-modified"> 305         gen_inline_array_null_guard(ary, cast_val, 3);</span>
 306         inc_sp(3);
 307         access_store_at(ary, adr, adr_type, cast_val, elemtype, bt, MO_UNORDERED | IN_HEAP | IS_ARRAY, false);
 308         dec_sp(3);
 309         ideal.sync_kit(this);
 310       } ideal.else_(); {
 311         Node* val = cast_val;
 312         // flattened
<span class="line-modified"> 313         if (!val-&gt;is_InlineType() &amp;&amp; tval-&gt;maybe_null()) {</span>
 314           // Add null check
 315           sync_kit(ideal);
 316           Node* null_ctl = top();
 317           val = null_check_oop(val, &amp;null_ctl);
 318           if (null_ctl != top()) {
 319             PreserveJVMState pjvms(this);
 320             inc_sp(3);
 321             set_control(null_ctl);
 322             uncommon_trap(Deoptimization::Reason_null_check, Deoptimization::Action_none);
 323             dec_sp(3);
 324           }
 325           ideal.sync_kit(this);
 326         }
<span class="line-modified"> 327         // Try to determine the inline klass</span>
<span class="line-modified"> 328         ciInlineKlass* vk = NULL;</span>
<span class="line-modified"> 329         if (tval-&gt;isa_inlinetype() || tval-&gt;is_inlinetypeptr()) {</span>
<span class="line-modified"> 330           vk = tval-&gt;inline_klass();</span>
<span class="line-modified"> 331         } else if (tval_init-&gt;isa_inlinetype() || tval_init-&gt;is_inlinetypeptr()) {</span>
<span class="line-modified"> 332           vk = tval_init-&gt;inline_klass();</span>
<span class="line-modified"> 333         } else if (elemtype-&gt;is_inlinetypeptr()) {</span>
<span class="line-modified"> 334           vk = elemtype-&gt;inline_klass();</span>
 335         }
 336         Node* casted_ary = ary;
 337         if (vk != NULL &amp;&amp; !stopped()) {
 338           // Element type is known, cast and store to flattened representation
 339           sync_kit(ideal);
 340           assert(vk-&gt;flatten_array() &amp;&amp; elemtype-&gt;maybe_null(), &quot;never/always flat - should be optimized&quot;);
 341           ciArrayKlass* array_klass = ciArrayKlass::make(vk);
 342           const TypeAryPtr* arytype = TypeOopPtr::make_from_klass(array_klass)-&gt;isa_aryptr();
 343           casted_ary = _gvn.transform(new CheckCastPPNode(control(), casted_ary, arytype));
 344           Node* casted_adr = array_element_address(casted_ary, idx, T_OBJECT, arytype-&gt;size(), control());
<span class="line-modified"> 345           if (!val-&gt;is_InlineType()) {</span>
<span class="line-modified"> 346             assert(!gvn().type(val)-&gt;maybe_null(), &quot;inline type array elements should never be null&quot;);</span>
<span class="line-modified"> 347             val = InlineTypeNode::make_from_oop(this, val, vk);</span>
 348           }
 349           // Re-execute flattened array store if buffering triggers deoptimization
 350           PreserveReexecuteState preexecs(this);
 351           inc_sp(3);
 352           jvms()-&gt;set_should_reexecute(true);
<span class="line-modified"> 353           val-&gt;as_InlineType()-&gt;store_flattened(this, casted_ary, casted_adr, NULL, 0, MO_UNORDERED | IN_HEAP | IS_ARRAY);</span>
 354           ideal.sync_kit(this);
 355         } else if (!ideal.ctrl()-&gt;is_top()) {
 356           // Element type is unknown, emit runtime call
 357           sync_kit(ideal);
 358 
 359           // This membar keeps this access to an unknown flattened
 360           // array correctly ordered with other unknown and known
 361           // flattened array accesses.
<span class="line-modified"> 362           insert_mem_bar_volatile(Op_MemBarCPUOrder, C-&gt;get_alias_index(TypeAryPtr::INLINES));</span>
 363           ideal.sync_kit(this);
 364 
<span class="line-modified"> 365           ideal.make_leaf_call(OptoRuntime::store_unknown_inline_type(),</span>
<span class="line-modified"> 366                                CAST_FROM_FN_PTR(address, OptoRuntime::store_unknown_inline),</span>
<span class="line-modified"> 367                                &quot;store_unknown_inline&quot;,</span>
 368                                val, casted_ary, idx);
 369 
 370           sync_kit(ideal);
 371           // Same as MemBarCPUOrder above: keep this unknown
 372           // flattened array access correctly ordered with other
 373           // flattened array accesses.
<span class="line-modified"> 374           insert_mem_bar_volatile(Op_MemBarCPUOrder, C-&gt;get_alias_index(TypeAryPtr::INLINES));</span>
 375           ideal.sync_kit(this);
 376         }
 377       }
 378       ideal.end_if();
 379       sync_kit(ideal);
 380       return;
 381     } else if (!ary_t-&gt;is_not_null_free()) {
 382       // Array is not flattened but may be null free
<span class="line-modified"> 383       assert(elemtype-&gt;is_oopptr()-&gt;can_be_inline_type() &amp;&amp; !ary_t-&gt;klass_is_exact(), &quot;array can&#39;t be null free&quot;);</span>
<span class="line-modified"> 384       ary = gen_inline_array_null_guard(ary, cast_val, 3, true);</span>
 385     }
 386   }
 387   inc_sp(3);
 388   access_store_at(ary, adr, adr_type, val, elemtype, bt, MO_UNORDERED | IN_HEAP | IS_ARRAY);
 389   dec_sp(3);
 390 }
 391 
 392 
 393 //------------------------------array_addressing-------------------------------
 394 // Pull array and index from the stack.  Compute pointer-to-element.
 395 Node* Parse::array_addressing(BasicType type, int vals, const Type*&amp; elemtype) {
 396   Node *idx   = peek(0+vals);   // Get from stack without popping
 397   Node *ary   = peek(1+vals);   // in case of exception
 398 
 399   // Null check the array base, with correct stack contents
 400   ary = null_check(ary, T_ARRAY);
 401   // Compile-time detect of null-exception?
 402   if (stopped())  return top();
 403 
 404   const TypeAryPtr* arytype  = _gvn.type(ary)-&gt;is_aryptr();
</pre>
<hr />
<pre>
 469       if (C-&gt;allow_range_check_smearing()) {
 470         // Do not use builtin_throw, since range checks are sometimes
 471         // made more stringent by an optimistic transformation.
 472         // This creates &quot;tentative&quot; range checks at this point,
 473         // which are not guaranteed to throw exceptions.
 474         // See IfNode::Ideal, is_range_check, adjust_check.
 475         uncommon_trap(Deoptimization::Reason_range_check,
 476                       Deoptimization::Action_make_not_entrant,
 477                       NULL, &quot;range_check&quot;);
 478       } else {
 479         // If we have already recompiled with the range-check-widening
 480         // heroic optimization turned off, then we must really be throwing
 481         // range check exceptions.
 482         builtin_throw(Deoptimization::Reason_range_check, idx);
 483       }
 484     }
 485   }
 486   // Check for always knowing you are throwing a range-check exception
 487   if (stopped())  return top();
 488 
<span class="line-modified"> 489   // This could be an access to an inline type array. We can&#39;t tell if it&#39;s</span>
 490   // flat or not. Speculating it&#39;s not leads to a much simpler graph
 491   // shape. Check profiling.
 492   // For aastore, by the time we&#39;re here, the array store check should
 493   // have already taken advantage of profiling to cast the array to an
 494   // exact type reported by profiling
 495   const TypeOopPtr* elemptr = elemtype-&gt;make_oopptr();
<span class="line-modified"> 496   if (elemtype-&gt;isa_inlinetype() == NULL &amp;&amp;</span>
<span class="line-modified"> 497       (elemptr == NULL || !elemptr-&gt;is_inlinetypeptr() || elemptr-&gt;maybe_null()) &amp;&amp;</span>
 498       !arytype-&gt;is_not_flat()) {
 499     assert(is_reference_type(type), &quot;Only references&quot;);
 500     // First check the speculative type
 501     Deoptimization::DeoptReason reason = Deoptimization::Reason_speculate_class_check;
 502     ciKlass* array_type = arytype-&gt;speculative_type();
 503     if (too_many_traps_or_recompiles(reason) || array_type == NULL) {
 504       // No speculative type, check profile data at this bci
 505       array_type = NULL;
 506       reason = Deoptimization::Reason_class_check;
 507       if (UseArrayLoadStoreProfile &amp;&amp; !too_many_traps_or_recompiles(reason)) {
 508         ciKlass* element_type = NULL;
 509         ProfilePtrKind element_ptr = ProfileMaybeNull;
 510         bool flat_array = true;
 511         bool null_free_array = true;
 512         method()-&gt;array_access_profiled_type(bci(), array_type, element_type, element_ptr, flat_array, null_free_array);
 513       }
 514     }
 515     if (array_type != NULL) {
 516       // Speculate that this array has the exact type reported by profile data
 517       Node* better_ary = NULL;
</pre>
<hr />
<pre>
 529     // No need to speculate: feed profile data at this bci for the
 530     // array to type speculation
 531     ciKlass* array_type = NULL;
 532     ciKlass* element_type = NULL;
 533     ProfilePtrKind element_ptr = ProfileMaybeNull;
 534     bool flat_array = true;
 535     bool null_free_array = true;
 536     method()-&gt;array_access_profiled_type(bci(), array_type, element_type, element_ptr, flat_array, null_free_array);
 537     if (array_type != NULL) {
 538       record_profile_for_speculation(ary, array_type, ProfileMaybeNull);
 539     }
 540   }
 541 
 542   // We have no exact array type from profile data. Check profile data
 543   // for a non null free or non flat array. Non null free implies non
 544   // flat so check this one first. Speculating on a non null free
 545   // array doesn&#39;t help aaload but could be profitable for a
 546   // subsequent aastore.
 547   elemptr = elemtype-&gt;make_oopptr();
 548   if (!arytype-&gt;is_not_null_free() &amp;&amp;
<span class="line-modified"> 549       elemtype-&gt;isa_inlinetype() == NULL &amp;&amp;</span>
<span class="line-modified"> 550       (elemptr == NULL || !elemptr-&gt;is_inlinetypeptr()) &amp;&amp;</span>
 551       UseArrayLoadStoreProfile) {
 552     assert(is_reference_type(type), &quot;&quot;);
 553     bool null_free_array = true;
 554     Deoptimization::DeoptReason reason = Deoptimization::Reason_none;
 555     if (arytype-&gt;speculative() != NULL &amp;&amp;
 556         arytype-&gt;speculative()-&gt;is_aryptr()-&gt;is_not_null_free() &amp;&amp;
 557         !too_many_traps_or_recompiles(Deoptimization::Reason_speculate_class_check)) {
 558       null_free_array = false;
 559       reason = Deoptimization::Reason_speculate_class_check;
 560     } else if (!too_many_traps_or_recompiles(Deoptimization::Reason_class_check)) {
 561       ciKlass* array_type = NULL;
 562       ciKlass* element_type = NULL;
 563       ProfilePtrKind element_ptr = ProfileMaybeNull;
 564       bool flat_array = true;
 565       method()-&gt;array_access_profiled_type(bci(), array_type, element_type, element_ptr, flat_array, null_free_array);
 566       reason = Deoptimization::Reason_class_check;
 567     }
 568     if (!null_free_array) {
 569       { // Deoptimize if null-free array
 570         BuildCutout unless(this, is_nullable_array(ary), PROB_MAX);
 571         uncommon_trap_exact(reason, Deoptimization::Action_maybe_recompile);
 572       }
 573       Node* better_ary = _gvn.transform(new CheckCastPPNode(control(), ary, arytype-&gt;cast_to_not_null_free()));
 574       replace_in_map(ary, better_ary);
 575       ary = better_ary;
 576       arytype = _gvn.type(ary)-&gt;is_aryptr();
 577     }
 578   }
 579 
<span class="line-modified"> 580   if (!arytype-&gt;is_not_flat() &amp;&amp; elemtype-&gt;isa_inlinetype() == NULL) {</span>
 581     assert(is_reference_type(type), &quot;&quot;);
 582     bool flat_array = true;
 583     Deoptimization::DeoptReason reason = Deoptimization::Reason_none;
 584     if (arytype-&gt;speculative() != NULL &amp;&amp;
 585         arytype-&gt;speculative()-&gt;is_aryptr()-&gt;is_not_flat() &amp;&amp;
 586         !too_many_traps_or_recompiles(Deoptimization::Reason_speculate_class_check)) {
 587       flat_array = false;
 588       reason = Deoptimization::Reason_speculate_class_check;
 589     } else if (UseArrayLoadStoreProfile &amp;&amp; !too_many_traps_or_recompiles(reason)) {
 590       ciKlass* array_type = NULL;
 591       ciKlass* element_type = NULL;
 592       ProfilePtrKind element_ptr = ProfileMaybeNull;
 593       bool null_free_array = true;
 594       method()-&gt;array_access_profiled_type(bci(), array_type, element_type, element_ptr, flat_array, null_free_array);
 595       reason = Deoptimization::Reason_class_check;
 596     }
 597     if (!flat_array) {
 598       { // Deoptimize if flat array
 599         BuildCutout unless(this, is_non_flattened_array(ary), PROB_MAX);
 600         uncommon_trap_exact(reason, Deoptimization::Action_maybe_recompile);
</pre>
<hr />
<pre>
2041     }
2042   } else {
2043     // Update method data
2044     profile_not_taken_branch();
2045     adjust_map_after_if(untaken_btest, c, untaken_prob, next_block);
2046   }
2047 }
2048 
2049 void Parse::do_acmp(BoolTest::mask btest, Node* a, Node* b) {
2050   ciMethod* subst_method = ciEnv::current()-&gt;ValueBootstrapMethods_klass()-&gt;find_method(ciSymbol::isSubstitutable_name(), ciSymbol::object_object_boolean_signature());
2051   // If current method is ValueBootstrapMethods::isSubstitutable(),
2052   // compile the acmp as a regular pointer comparison otherwise we
2053   // could call ValueBootstrapMethods::isSubstitutable() back
2054   if (!EnableValhalla || (method() == subst_method)) {
2055     Node* cmp = CmpP(a, b);
2056     cmp = optimize_cmp_with_klass(cmp);
2057     do_if(btest, cmp);
2058     return;
2059   }
2060 
<span class="line-modified">2061   // Allocate inline type operands and re-execute on deoptimization</span>
<span class="line-modified">2062   if (a-&gt;is_InlineType()) {</span>
2063     PreserveReexecuteState preexecs(this);
2064     inc_sp(2);
2065     jvms()-&gt;set_should_reexecute(true);
<span class="line-modified">2066     a = a-&gt;as_InlineType()-&gt;buffer(this)-&gt;get_oop();</span>
2067   }
<span class="line-modified">2068   if (b-&gt;is_InlineType()) {</span>
2069     PreserveReexecuteState preexecs(this);
2070     inc_sp(2);
2071     jvms()-&gt;set_should_reexecute(true);
<span class="line-modified">2072     b = b-&gt;as_InlineType()-&gt;buffer(this)-&gt;get_oop();</span>
2073   }
2074 
2075   // First, do a normal pointer comparison
2076   const TypeOopPtr* ta = _gvn.type(a)-&gt;isa_oopptr();
2077   const TypeOopPtr* tb = _gvn.type(b)-&gt;isa_oopptr();
2078   Node* cmp = CmpP(a, b);
2079   cmp = optimize_cmp_with_klass(cmp);
<span class="line-modified">2080   if (ta == NULL || !ta-&gt;can_be_inline_type() ||</span>
<span class="line-modified">2081       tb == NULL || !tb-&gt;can_be_inline_type()) {</span>
<span class="line-modified">2082     // This is sufficient, if one of the operands can&#39;t be an inline type</span>
2083     do_if(btest, cmp);
2084     return;
2085   }
2086   Node* eq_region = NULL;
2087   if (btest == BoolTest::eq) {
2088     do_if(btest, cmp, true);
2089     if (stopped()) {
2090       return;
2091     }
2092   } else {
2093     assert(btest == BoolTest::ne, &quot;only eq or ne&quot;);
2094     Node* is_not_equal = NULL;
2095     eq_region = new RegionNode(3);
2096     {
2097       PreserveJVMState pjvms(this);
2098       do_if(btest, cmp, false, &amp;is_not_equal);
2099       if (!stopped()) {
2100         eq_region-&gt;init_req(1, control());
2101       }
2102     }
</pre>
<hr />
<pre>
2113   inc_sp(2);
2114   Node* null_ctl = top();
2115   Node* not_null_a = null_check_oop(a, &amp;null_ctl, !too_many_traps(Deoptimization::Reason_null_check), false, false);
2116   dec_sp(2);
2117   ne_region-&gt;init_req(1, null_ctl);
2118   if (stopped()) {
2119     record_for_igvn(ne_region);
2120     set_control(_gvn.transform(ne_region));
2121     if (btest == BoolTest::ne) {
2122       {
2123         PreserveJVMState pjvms(this);
2124         int target_bci = iter().get_dest();
2125         merge(target_bci);
2126       }
2127       record_for_igvn(eq_region);
2128       set_control(_gvn.transform(eq_region));
2129     }
2130     return;
2131   }
2132 
<span class="line-modified">2133   // First operand is non-null, check if it is an inline type</span>
<span class="line-modified">2134   Node* is_value = is_inline_type(not_null_a);</span>
2135   IfNode* is_value_iff = create_and_map_if(control(), is_value, PROB_FAIR, COUNT_UNKNOWN);
2136   Node* not_value = _gvn.transform(new IfFalseNode(is_value_iff));
2137   ne_region-&gt;init_req(2, not_value);
2138   set_control(_gvn.transform(new IfTrueNode(is_value_iff)));
2139 
<span class="line-modified">2140   // The first operand is an inline type, check if the second operand is non-null</span>
2141   inc_sp(2);
2142   null_ctl = top();
2143   Node* not_null_b = null_check_oop(b, &amp;null_ctl, !too_many_traps(Deoptimization::Reason_null_check), false, false);
2144   dec_sp(2);
2145   ne_region-&gt;init_req(3, null_ctl);
2146   if (stopped()) {
2147     record_for_igvn(ne_region);
2148     set_control(_gvn.transform(ne_region));
2149     if (btest == BoolTest::ne) {
2150       {
2151         PreserveJVMState pjvms(this);
2152         int target_bci = iter().get_dest();
2153         merge(target_bci);
2154       }
2155       record_for_igvn(eq_region);
2156       set_control(_gvn.transform(eq_region));
2157     }
2158     return;
2159   }
2160 
</pre>
<hr />
<pre>
2469   if (c-&gt;Opcode() == Op_CmpP &amp;&amp;
2470       (c-&gt;in(1)-&gt;Opcode() == Op_LoadKlass || c-&gt;in(1)-&gt;Opcode() == Op_DecodeNKlass) &amp;&amp;
2471       c-&gt;in(2)-&gt;is_Con()) {
2472     Node* load_klass = NULL;
2473     Node* decode = NULL;
2474     if (c-&gt;in(1)-&gt;Opcode() == Op_DecodeNKlass) {
2475       decode = c-&gt;in(1);
2476       load_klass = c-&gt;in(1)-&gt;in(1);
2477     } else {
2478       load_klass = c-&gt;in(1);
2479     }
2480     if (load_klass-&gt;in(2)-&gt;is_AddP()) {
2481       Node* addp = load_klass-&gt;in(2);
2482       Node* obj = addp-&gt;in(AddPNode::Address);
2483       const TypeOopPtr* obj_type = _gvn.type(obj)-&gt;is_oopptr();
2484       if (obj_type-&gt;speculative_type_not_null() != NULL) {
2485         ciKlass* k = obj_type-&gt;speculative_type();
2486         inc_sp(2);
2487         obj = maybe_cast_profiled_obj(obj, k);
2488         dec_sp(2);
<span class="line-modified">2489         if (obj-&gt;is_InlineType()) {</span>
<span class="line-modified">2490           assert(obj-&gt;as_InlineType()-&gt;is_allocated(&amp;_gvn), &quot;must be allocated&quot;);</span>
<span class="line-modified">2491           obj = obj-&gt;as_InlineType()-&gt;get_oop();</span>
2492         }
2493         // Make the CmpP use the casted obj
2494         addp = basic_plus_adr(obj, addp-&gt;in(AddPNode::Offset));
2495         load_klass = load_klass-&gt;clone();
2496         load_klass-&gt;set_req(2, addp);
2497         load_klass = _gvn.transform(load_klass);
2498         if (decode != NULL) {
2499           decode = decode-&gt;clone();
2500           decode-&gt;set_req(1, load_klass);
2501           load_klass = _gvn.transform(decode);
2502         }
2503         c = c-&gt;clone();
2504         c-&gt;set_req(1, load_klass);
2505         c = _gvn.transform(c);
2506       }
2507     }
2508   }
2509   return c;
2510 }
2511 
</pre>
<hr />
<pre>
3320     // See if we can get some profile data and hand it off to the next block
3321     Block *target_block = block()-&gt;successor_for_bci(target_bci);
3322     if (target_block-&gt;pred_count() != 1)  break;
3323     ciMethodData* methodData = method()-&gt;method_data();
3324     if (!methodData-&gt;is_mature())  break;
3325     ciProfileData* data = methodData-&gt;bci_to_data(bci());
3326     assert(data != NULL &amp;&amp; data-&gt;is_JumpData(), &quot;need JumpData for taken branch&quot;);
3327     int taken = ((ciJumpData*)data)-&gt;taken();
3328     taken = method()-&gt;scale_count(taken);
3329     target_block-&gt;set_count(taken);
3330     break;
3331   }
3332 
3333   case Bytecodes::_ifnull:    btest = BoolTest::eq; goto handle_if_null;
3334   case Bytecodes::_ifnonnull: btest = BoolTest::ne; goto handle_if_null;
3335   handle_if_null:
3336     // If this is a backwards branch in the bytecodes, add Safepoint
3337     maybe_add_safepoint(iter().get_dest());
3338     a = null();
3339     b = pop();
<span class="line-modified">3340     if (b-&gt;is_InlineType()) {</span>
3341       // Return constant false because &#39;b&#39; is always non-null
3342       c = _gvn.makecon(TypeInt::CC_GT);
3343     } else {
3344       if (!_gvn.type(b)-&gt;speculative_maybe_null() &amp;&amp;
3345           !too_many_traps(Deoptimization::Reason_speculate_null_check)) {
3346         inc_sp(1);
3347         Node* null_ctl = top();
3348         b = null_check_oop(b, &amp;null_ctl, true, true, true);
3349         assert(null_ctl-&gt;is_top(), &quot;no null control here&quot;);
3350         dec_sp(1);
3351       } else if (_gvn.type(b)-&gt;speculative_always_null() &amp;&amp;
3352                  !too_many_traps(Deoptimization::Reason_speculate_null_assert)) {
3353         inc_sp(1);
3354         b = null_assert(b);
3355         dec_sp(1);
3356       }
3357       c = _gvn.transform( new CmpPNode(b, a) );
3358     }
3359     do_ifnull(btest, c);
3360     break;
</pre>
</td>
</tr>
</table>
<center><a href="parse1.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="parse3.cpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>