<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/share/opto/escape.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="doCall.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="graphKit.cpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/share/opto/escape.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
 372       // Put Lock and Unlock nodes on IGVN worklist to process them during
 373       // first IGVN optimization when escape information is still available.
 374       record_for_optimizer(n);
 375     } else if (n-&gt;is_Allocate()) {
 376       add_call_node(n-&gt;as_Call());
 377       record_for_optimizer(n);
 378     } else {
 379       if (n-&gt;is_CallStaticJava()) {
 380         const char* name = n-&gt;as_CallStaticJava()-&gt;_name;
 381         if (name != NULL &amp;&amp; strcmp(name, &quot;uncommon_trap&quot;) == 0)
 382           return; // Skip uncommon traps
 383       }
 384       // Don&#39;t mark as processed since call&#39;s arguments have to be processed.
 385       delayed_worklist-&gt;push(n);
 386       // Check if a call returns an object.
 387       if ((n-&gt;as_Call()-&gt;returns_pointer() &amp;&amp;
 388            n-&gt;as_Call()-&gt;proj_out_or_null(TypeFunc::Parms) != NULL) ||
 389           (n-&gt;is_CallStaticJava() &amp;&amp;
 390            n-&gt;as_CallStaticJava()-&gt;is_boxing_method())) {
 391         add_call_node(n-&gt;as_Call());
<span class="line-modified"> 392       } else if (n-&gt;as_Call()-&gt;tf()-&gt;returns_value_type_as_fields()) {</span>
 393         bool returns_oop = false;
 394         for (DUIterator_Fast imax, i = n-&gt;fast_outs(imax); i &lt; imax &amp;&amp; !returns_oop; i++) {
 395           ProjNode* pn = n-&gt;fast_out(i)-&gt;as_Proj();
 396           if (pn-&gt;_con &gt;= TypeFunc::Parms &amp;&amp; pn-&gt;bottom_type()-&gt;isa_ptr()) {
 397             returns_oop = true;
 398           }
 399         }
 400         if (returns_oop) {
 401           add_call_node(n-&gt;as_Call());
 402         }
 403       }
 404     }
 405     return;
 406   }
 407   // Put this check here to process call arguments since some call nodes
 408   // point to phantom_obj.
 409   if (n_ptn == phantom_obj || n_ptn == null_obj)
 410     return; // Skip predefined nodes.
 411 
 412   switch (opcode) {
</pre>
<hr />
<pre>
 414       Node* base = get_addp_base(n);
 415       PointsToNode* ptn_base = ptnode_adr(base-&gt;_idx);
 416       // Field nodes are created for all field types. They are used in
 417       // adjust_scalar_replaceable_state() and split_unique_types().
 418       // Note, non-oop fields will have only base edges in Connection
 419       // Graph because such fields are not used for oop loads and stores.
 420       int offset = address_offset(n, igvn);
 421       add_field(n, PointsToNode::NoEscape, offset);
 422       if (ptn_base == NULL) {
 423         delayed_worklist-&gt;push(n); // Process it later.
 424       } else {
 425         n_ptn = ptnode_adr(n_idx);
 426         add_base(n_ptn-&gt;as_Field(), ptn_base);
 427       }
 428       break;
 429     }
 430     case Op_CastX2P: {
 431       map_ideal_node(n, phantom_obj);
 432       break;
 433     }
<span class="line-modified"> 434     case Op_ValueTypePtr:</span>
 435     case Op_CastPP:
 436     case Op_CheckCastPP:
 437     case Op_EncodeP:
 438     case Op_DecodeN:
 439     case Op_EncodePKlass:
 440     case Op_DecodeNKlass: {
 441       add_local_var_and_edge(n, PointsToNode::NoEscape,
 442                              n-&gt;in(1), delayed_worklist);
 443       break;
 444     }
 445     case Op_CMoveP: {
 446       add_local_var(n, PointsToNode::NoEscape);
 447       // Do not add edges during first iteration because some could be
 448       // not defined yet.
 449       delayed_worklist-&gt;push(n);
 450       break;
 451     }
 452     case Op_ConP:
 453     case Op_ConN:
 454     case Op_ConNKlass: {
</pre>
<hr />
<pre>
 490       map_ideal_node(n, phantom_obj); // Result is unknown
 491       break;
 492     }
 493     case Op_Phi: {
 494       // Using isa_ptr() instead of isa_oopptr() for LoadP and Phi because
 495       // ThreadLocal has RawPtr type.
 496       const Type* t = n-&gt;as_Phi()-&gt;type();
 497       if (t-&gt;make_ptr() != NULL) {
 498         add_local_var(n, PointsToNode::NoEscape);
 499         // Do not add edges during first iteration because some could be
 500         // not defined yet.
 501         delayed_worklist-&gt;push(n);
 502       }
 503       break;
 504     }
 505     case Op_Proj: {
 506       // we are only interested in the oop result projection from a call
 507       if (n-&gt;as_Proj()-&gt;_con &gt;= TypeFunc::Parms &amp;&amp; n-&gt;in(0)-&gt;is_Call() &amp;&amp;
 508           (n-&gt;in(0)-&gt;as_Call()-&gt;returns_pointer() || n-&gt;bottom_type()-&gt;isa_ptr())) {
 509         assert((n-&gt;as_Proj()-&gt;_con == TypeFunc::Parms &amp;&amp; n-&gt;in(0)-&gt;as_Call()-&gt;returns_pointer()) ||
<span class="line-modified"> 510                n-&gt;in(0)-&gt;as_Call()-&gt;tf()-&gt;returns_value_type_as_fields(), &quot;what kind of oop return is it?&quot;);</span>
 511         add_local_var_and_edge(n, PointsToNode::NoEscape,
 512                                n-&gt;in(0), delayed_worklist);
 513       }
 514       break;
 515     }
 516     case Op_Rethrow: // Exception object escapes
 517     case Op_Return: {
 518       if (n-&gt;req() &gt; TypeFunc::Parms &amp;&amp;
 519           igvn-&gt;type(n-&gt;in(TypeFunc::Parms))-&gt;isa_oopptr()) {
 520         // Treat Return value as LocalVar with GlobalEscape escape state.
 521         add_local_var_and_edge(n, PointsToNode::GlobalEscape,
 522                                n-&gt;in(TypeFunc::Parms), delayed_worklist);
 523       }
 524       break;
 525     }
 526     case Op_CompareAndExchangeP:
 527     case Op_CompareAndExchangeN:
 528     case Op_GetAndSetP:
 529     case Op_GetAndSetN: {
 530       add_objload_to_connection_graph(n, delayed_worklist);
</pre>
<hr />
<pre>
 586   if (n-&gt;is_Call()) {
 587     process_call_arguments(n-&gt;as_Call());
 588     return;
 589   }
 590   assert(n-&gt;is_Store() || n-&gt;is_LoadStore() ||
 591          (n_ptn != NULL) &amp;&amp; (n_ptn-&gt;ideal_node() != NULL),
 592          &quot;node should be registered already&quot;);
 593   int opcode = n-&gt;Opcode();
 594   bool gc_handled = BarrierSet::barrier_set()-&gt;barrier_set_c2()-&gt;escape_add_final_edges(this, _igvn, n, opcode);
 595   if (gc_handled) {
 596     return; // Ignore node if already handled by GC.
 597   }
 598   switch (opcode) {
 599     case Op_AddP: {
 600       Node* base = get_addp_base(n);
 601       PointsToNode* ptn_base = ptnode_adr(base-&gt;_idx);
 602       assert(ptn_base != NULL, &quot;field&#39;s base should be registered&quot;);
 603       add_base(n_ptn-&gt;as_Field(), ptn_base);
 604       break;
 605     }
<span class="line-modified"> 606     case Op_ValueTypePtr:</span>
 607     case Op_CastPP:
 608     case Op_CheckCastPP:
 609     case Op_EncodeP:
 610     case Op_DecodeN:
 611     case Op_EncodePKlass:
 612     case Op_DecodeNKlass: {
 613       add_local_var_and_edge(n, PointsToNode::NoEscape,
 614                              n-&gt;in(1), NULL);
 615       break;
 616     }
 617     case Op_CMoveP: {
 618       for (uint i = CMoveNode::IfFalse; i &lt; n-&gt;req(); i++) {
 619         Node* in = n-&gt;in(i);
 620         if (in == NULL)
 621           continue;  // ignore NULL
 622         Node* uncast_in = in-&gt;uncast();
 623         if (uncast_in-&gt;is_top() || uncast_in == n)
 624           continue;  // ignore top or inputs which go back this node
 625         PointsToNode* ptn = ptnode_adr(in-&gt;_idx);
 626         assert(ptn != NULL, &quot;node should be registered&quot;);
</pre>
<hr />
<pre>
 649         for (uint i = 1; i &lt; n-&gt;req(); i++) {
 650           Node* in = n-&gt;in(i);
 651           if (in == NULL)
 652             continue;  // ignore NULL
 653           Node* uncast_in = in-&gt;uncast();
 654           if (uncast_in-&gt;is_top() || uncast_in == n)
 655             continue;  // ignore top or inputs which go back this node
 656           PointsToNode* ptn = ptnode_adr(in-&gt;_idx);
 657           assert(ptn != NULL, &quot;node should be registered&quot;);
 658           add_edge(n_ptn, ptn);
 659         }
 660         break;
 661       }
 662       ELSE_FAIL(&quot;Op_Phi&quot;);
 663     }
 664     case Op_Proj: {
 665       // we are only interested in the oop result projection from a call
 666       if (n-&gt;as_Proj()-&gt;_con &gt;= TypeFunc::Parms &amp;&amp; n-&gt;in(0)-&gt;is_Call() &amp;&amp;
 667           (n-&gt;in(0)-&gt;as_Call()-&gt;returns_pointer()|| n-&gt;bottom_type()-&gt;isa_ptr())) {
 668         assert((n-&gt;as_Proj()-&gt;_con == TypeFunc::Parms &amp;&amp; n-&gt;in(0)-&gt;as_Call()-&gt;returns_pointer()) ||
<span class="line-modified"> 669                n-&gt;in(0)-&gt;as_Call()-&gt;tf()-&gt;returns_value_type_as_fields(), &quot;what kind of oop return is it?&quot;);</span>
 670         add_local_var_and_edge(n, PointsToNode::NoEscape, n-&gt;in(0), NULL);
 671         break;
 672       }
 673       ELSE_FAIL(&quot;Op_Proj&quot;);
 674     }
 675     case Op_Rethrow: // Exception object escapes
 676     case Op_Return: {
 677       if (n-&gt;req() &gt; TypeFunc::Parms &amp;&amp;
 678           _igvn-&gt;type(n-&gt;in(TypeFunc::Parms))-&gt;isa_oopptr()) {
 679         // Treat Return value as LocalVar with GlobalEscape escape state.
 680         add_local_var_and_edge(n, PointsToNode::GlobalEscape,
 681                                n-&gt;in(TypeFunc::Parms), NULL);
 682         break;
 683       }
 684       ELSE_FAIL(&quot;Op_Return&quot;);
 685     }
 686     case Op_StoreP:
 687     case Op_StoreN:
 688     case Op_StoreNKlass:
 689     case Op_StorePConditional:
</pre>
<hr />
<pre>
 811     return true;
 812   } else if ((opcode == Op_StoreP) &amp;&amp; adr_type-&gt;isa_rawptr()) {
 813     // Stored value escapes in unsafe access.
 814     Node* val = n-&gt;in(MemNode::ValueIn);
 815     PointsToNode* ptn = ptnode_adr(val-&gt;_idx);
 816     assert(ptn != NULL, &quot;node should be registered&quot;);
 817     set_escape_state(ptn, PointsToNode::GlobalEscape);
 818     // Add edge to object for unsafe access with offset.
 819     PointsToNode* adr_ptn = ptnode_adr(adr-&gt;_idx);
 820     assert(adr_ptn != NULL, &quot;node should be registered&quot;);
 821     if (adr_ptn-&gt;is_Field()) {
 822       assert(adr_ptn-&gt;as_Field()-&gt;is_oop(), &quot;should be oop field&quot;);
 823       add_edge(adr_ptn, ptn);
 824     }
 825     return true;
 826   }
 827   return false;
 828 }
 829 
 830 void ConnectionGraph::add_call_node(CallNode* call) {
<span class="line-modified"> 831   assert(call-&gt;returns_pointer() || call-&gt;tf()-&gt;returns_value_type_as_fields(), &quot;only for call which returns pointer&quot;);</span>
 832   uint call_idx = call-&gt;_idx;
 833   if (call-&gt;is_Allocate()) {
 834     Node* k = call-&gt;in(AllocateNode::KlassNode);
 835     const TypeKlassPtr* kt = k-&gt;bottom_type()-&gt;isa_klassptr();
 836     assert(kt != NULL, &quot;TypeKlassPtr  required.&quot;);
 837     ciKlass* cik = kt-&gt;klass();
 838     PointsToNode::EscapeState es = PointsToNode::NoEscape;
 839     bool scalar_replaceable = true;
 840     if (call-&gt;is_AllocateArray()) {
 841       if (!cik-&gt;is_array_klass()) { // StressReflectiveCode
 842         es = PointsToNode::GlobalEscape;
 843       } else {
 844         int length = call-&gt;in(AllocateNode::ALength)-&gt;find_int_con(-1);
 845         if (length &lt; 0 || length &gt; EliminateAllocationArraySizeLimit) {
 846           // Not scalar replaceable if the length is not constant or too big.
 847           scalar_replaceable = false;
 848         }
 849       }
 850     } else {  // Allocate instance
 851       if (cik-&gt;is_subclass_of(_compile-&gt;env()-&gt;Thread_klass()) ||
</pre>
<hr />
<pre>
 977           //
 978           // The inline_native_clone() case when the arraycopy stub is called
 979           // after the allocation before Initialize and CheckCastPP nodes.
 980           // Or normal arraycopy for object arrays case.
 981           //
 982           // Set AddP&#39;s base (Allocate) as not scalar replaceable since
 983           // pointer to the base (with offset) is passed as argument.
 984           //
 985           arg = get_addp_base(arg);
 986         }
 987         PointsToNode* arg_ptn = ptnode_adr(arg-&gt;_idx);
 988         assert(arg_ptn != NULL, &quot;should be registered&quot;);
 989         PointsToNode::EscapeState arg_esc = arg_ptn-&gt;escape_state();
 990         if (is_arraycopy || arg_esc &lt; PointsToNode::ArgEscape) {
 991           assert(aat == Type::TOP || aat == TypePtr::NULL_PTR ||
 992                  aat-&gt;isa_ptr() != NULL, &quot;expecting an Ptr&quot;);
 993           bool arg_has_oops = aat-&gt;isa_oopptr() &amp;&amp;
 994                               (aat-&gt;isa_oopptr()-&gt;klass() == NULL || aat-&gt;isa_instptr() ||
 995                                (aat-&gt;isa_aryptr() &amp;&amp; aat-&gt;isa_aryptr()-&gt;klass()-&gt;is_obj_array_klass()) ||
 996                                (aat-&gt;isa_aryptr() &amp;&amp; aat-&gt;isa_aryptr()-&gt;elem() != NULL &amp;&amp;
<span class="line-modified"> 997                                 aat-&gt;isa_aryptr()-&gt;elem()-&gt;isa_valuetype() &amp;&amp;</span>
<span class="line-modified"> 998                                 aat-&gt;isa_aryptr()-&gt;elem()-&gt;value_klass()-&gt;contains_oops()));</span>
 999           if (i == TypeFunc::Parms) {
1000             src_has_oops = arg_has_oops;
1001           }
1002           //
1003           // src or dst could be j.l.Object when other is basic type array:
1004           //
1005           //   arraycopy(char[],0,Object*,0,size);
1006           //   arraycopy(Object*,0,char[],0,size);
1007           //
1008           // Don&#39;t add edges in such cases.
1009           //
1010           bool arg_is_arraycopy_dest = src_has_oops &amp;&amp; is_arraycopy &amp;&amp;
1011                                        arg_has_oops &amp;&amp; (i &gt; TypeFunc::Parms);
1012 #ifdef ASSERT
1013           if (!(is_arraycopy ||
1014                 BarrierSet::barrier_set()-&gt;barrier_set_c2()-&gt;is_gc_barrier_node(call) ||
1015                 (call-&gt;as_CallLeaf()-&gt;_name != NULL &amp;&amp;
1016                  (strcmp(call-&gt;as_CallLeaf()-&gt;_name, &quot;updateBytesCRC32&quot;) == 0 ||
1017                   strcmp(call-&gt;as_CallLeaf()-&gt;_name, &quot;updateBytesCRC32C&quot;) == 0 ||
1018                   strcmp(call-&gt;as_CallLeaf()-&gt;_name, &quot;updateBytesAdler32&quot;) == 0 ||
</pre>
<hr />
<pre>
1020                   strcmp(call-&gt;as_CallLeaf()-&gt;_name, &quot;aescrypt_decryptBlock&quot;) == 0 ||
1021                   strcmp(call-&gt;as_CallLeaf()-&gt;_name, &quot;cipherBlockChaining_encryptAESCrypt&quot;) == 0 ||
1022                   strcmp(call-&gt;as_CallLeaf()-&gt;_name, &quot;cipherBlockChaining_decryptAESCrypt&quot;) == 0 ||
1023                   strcmp(call-&gt;as_CallLeaf()-&gt;_name, &quot;electronicCodeBook_encryptAESCrypt&quot;) == 0 ||
1024                   strcmp(call-&gt;as_CallLeaf()-&gt;_name, &quot;electronicCodeBook_decryptAESCrypt&quot;) == 0 ||
1025                   strcmp(call-&gt;as_CallLeaf()-&gt;_name, &quot;counterMode_AESCrypt&quot;) == 0 ||
1026                   strcmp(call-&gt;as_CallLeaf()-&gt;_name, &quot;ghash_processBlocks&quot;) == 0 ||
1027                   strcmp(call-&gt;as_CallLeaf()-&gt;_name, &quot;encodeBlock&quot;) == 0 ||
1028                   strcmp(call-&gt;as_CallLeaf()-&gt;_name, &quot;sha1_implCompress&quot;) == 0 ||
1029                   strcmp(call-&gt;as_CallLeaf()-&gt;_name, &quot;sha1_implCompressMB&quot;) == 0 ||
1030                   strcmp(call-&gt;as_CallLeaf()-&gt;_name, &quot;sha256_implCompress&quot;) == 0 ||
1031                   strcmp(call-&gt;as_CallLeaf()-&gt;_name, &quot;sha256_implCompressMB&quot;) == 0 ||
1032                   strcmp(call-&gt;as_CallLeaf()-&gt;_name, &quot;sha512_implCompress&quot;) == 0 ||
1033                   strcmp(call-&gt;as_CallLeaf()-&gt;_name, &quot;sha512_implCompressMB&quot;) == 0 ||
1034                   strcmp(call-&gt;as_CallLeaf()-&gt;_name, &quot;multiplyToLen&quot;) == 0 ||
1035                   strcmp(call-&gt;as_CallLeaf()-&gt;_name, &quot;squareToLen&quot;) == 0 ||
1036                   strcmp(call-&gt;as_CallLeaf()-&gt;_name, &quot;mulAdd&quot;) == 0 ||
1037                   strcmp(call-&gt;as_CallLeaf()-&gt;_name, &quot;montgomery_multiply&quot;) == 0 ||
1038                   strcmp(call-&gt;as_CallLeaf()-&gt;_name, &quot;montgomery_square&quot;) == 0 ||
1039                   strcmp(call-&gt;as_CallLeaf()-&gt;_name, &quot;vectorizedMismatch&quot;) == 0 ||
<span class="line-modified">1040                   strcmp(call-&gt;as_CallLeaf()-&gt;_name, &quot;load_unknown_value&quot;) == 0 ||</span>
<span class="line-modified">1041                   strcmp(call-&gt;as_CallLeaf()-&gt;_name, &quot;store_unknown_value&quot;) == 0 ||</span>
1042                   strcmp(call-&gt;as_CallLeaf()-&gt;_name, &quot;bigIntegerRightShiftWorker&quot;) == 0 ||
1043                   strcmp(call-&gt;as_CallLeaf()-&gt;_name, &quot;bigIntegerLeftShiftWorker&quot;) == 0 ||
1044                   strcmp(call-&gt;as_CallLeaf()-&gt;_name, &quot;vectorizedMismatch&quot;) == 0)
1045                  ))) {
1046             call-&gt;dump();
1047             fatal(&quot;EA unexpected CallLeaf %s&quot;, call-&gt;as_CallLeaf()-&gt;_name);
1048           }
1049 #endif
1050           // Always process arraycopy&#39;s destination object since
1051           // we need to add all possible edges to references in
1052           // source object.
1053           if (arg_esc &gt;= PointsToNode::ArgEscape &amp;&amp;
1054               !arg_is_arraycopy_dest) {
1055             continue;
1056           }
1057           PointsToNode::EscapeState es = PointsToNode::ArgEscape;
1058           if (call-&gt;is_ArrayCopy()) {
1059             ArrayCopyNode* ac = call-&gt;as_ArrayCopy();
1060             if (ac-&gt;is_clonebasic() ||
1061                 ac-&gt;is_arraycopy_validated() ||
</pre>
<hr />
<pre>
1885     }
1886   }
1887 }
1888 #endif
1889 
1890 // Optimize ideal graph.
1891 void ConnectionGraph::optimize_ideal_graph(GrowableArray&lt;Node*&gt;&amp; ptr_cmp_worklist,
1892                                            GrowableArray&lt;Node*&gt;&amp; storestore_worklist) {
1893   Compile* C = _compile;
1894   PhaseIterGVN* igvn = _igvn;
1895   if (EliminateLocks) {
1896     // Mark locks before changing ideal graph.
1897     int cnt = C-&gt;macro_count();
1898     for( int i=0; i &lt; cnt; i++ ) {
1899       Node *n = C-&gt;macro_node(i);
1900       if (n-&gt;is_AbstractLock()) { // Lock and Unlock nodes
1901         AbstractLockNode* alock = n-&gt;as_AbstractLock();
1902         if (!alock-&gt;is_non_esc_obj()) {
1903           const Type* obj_type = igvn-&gt;type(alock-&gt;obj_node());
1904           if (not_global_escape(alock-&gt;obj_node()) &amp;&amp;
<span class="line-modified">1905               !obj_type-&gt;isa_valuetype() &amp;&amp; !obj_type-&gt;is_valuetypeptr()) {</span>
1906             assert(!alock-&gt;is_eliminated() || alock-&gt;is_coarsened(), &quot;sanity&quot;);
1907             // The lock could be marked eliminated by lock coarsening
1908             // code during first IGVN before EA. Replace coarsened flag
1909             // to eliminate all associated locks/unlocks.
1910 #ifdef ASSERT
1911             alock-&gt;log_lock_optimization(C, &quot;eliminate_lock_set_non_esc3&quot;);
1912 #endif
1913             alock-&gt;set_non_esc_obj();
1914           }
1915         }
1916       }
1917     }
1918   }
1919 
1920   if (OptimizePtrCompare) {
1921     // Add ConI(#CC_GT) and ConI(#CC_EQ).
1922     _pcmp_neq = igvn-&gt;makecon(TypeInt::CC_GT);
1923     _pcmp_eq = igvn-&gt;makecon(TypeInt::CC_EQ);
1924     // Optimize objects compare.
1925     while (ptr_cmp_worklist.length() != 0) {
</pre>
<hr />
<pre>
2122       ciField* field = _compile-&gt;alias_type(adr_type-&gt;is_ptr())-&gt;field();
2123       if (field != NULL) {
2124         bt = field-&gt;layout_type();
2125       } else {
2126         // Check for unsafe oop field access
2127         if (n-&gt;has_out_with(Op_StoreP, Op_LoadP, Op_StoreN, Op_LoadN) ||
2128             n-&gt;has_out_with(Op_GetAndSetP, Op_GetAndSetN, Op_CompareAndExchangeP, Op_CompareAndExchangeN) ||
2129             n-&gt;has_out_with(Op_CompareAndSwapP, Op_CompareAndSwapN, Op_WeakCompareAndSwapP, Op_WeakCompareAndSwapN) ||
2130             BarrierSet::barrier_set()-&gt;barrier_set_c2()-&gt;escape_has_out_with_unsafe_object(n)) {
2131           bt = T_OBJECT;
2132           (*unsafe) = true;
2133         }
2134       }
2135     } else if (adr_type-&gt;isa_aryptr()) {
2136       if (offset == arrayOopDesc::length_offset_in_bytes()) {
2137         // Ignore array length load.
2138       } else if (find_second_addp(n, n-&gt;in(AddPNode::Base)) != NULL) {
2139         // Ignore first AddP.
2140       } else {
2141         const Type* elemtype = adr_type-&gt;isa_aryptr()-&gt;elem();
<span class="line-modified">2142         if (elemtype-&gt;isa_valuetype() &amp;&amp; field_offset != Type::OffsetBot) {</span>
<span class="line-modified">2143           ciValueKlass* vk = elemtype-&gt;value_klass();</span>
2144           field_offset += vk-&gt;first_field_offset();
2145           bt = vk-&gt;get_field_by_offset(field_offset, false)-&gt;layout_type();
2146         } else {
2147           bt = elemtype-&gt;array_element_basic_type();
2148         }
2149       }
2150     } else if (adr_type-&gt;isa_rawptr() || adr_type-&gt;isa_klassptr()) {
2151       // Allocation initialization, ThreadLocal field access, unsafe access
2152       if (n-&gt;has_out_with(Op_StoreP, Op_LoadP, Op_StoreN, Op_LoadN) ||
2153           n-&gt;has_out_with(Op_GetAndSetP, Op_GetAndSetN, Op_CompareAndExchangeP, Op_CompareAndExchangeN) ||
2154           n-&gt;has_out_with(Op_CompareAndSwapP, Op_CompareAndSwapN, Op_WeakCompareAndSwapP, Op_WeakCompareAndSwapN) ||
2155           BarrierSet::barrier_set()-&gt;barrier_set_c2()-&gt;escape_has_out_with_unsafe_object(n)) {
2156         bt = T_OBJECT;
2157       }
2158     }
2159   }
2160   // Note: T_NARROWOOP is not classed as a real reference type
2161   return (is_reference_type(bt) || bt == T_NARROWOOP);
2162 }
2163 
</pre>
<hr />
<pre>
2444   return NULL;
2445 }
2446 
2447 //
2448 // Adjust the type and inputs of an AddP which computes the
2449 // address of a field of an instance
2450 //
2451 bool ConnectionGraph::split_AddP(Node *addp, Node *base) {
2452   PhaseGVN* igvn = _igvn;
2453   const TypeOopPtr *base_t = igvn-&gt;type(base)-&gt;isa_oopptr();
2454   assert(base_t != NULL &amp;&amp; base_t-&gt;is_known_instance(), &quot;expecting instance oopptr&quot;);
2455   const TypeOopPtr *t = igvn-&gt;type(addp)-&gt;isa_oopptr();
2456   if (t == NULL) {
2457     // We are computing a raw address for a store captured by an Initialize
2458     // compute an appropriate address type (cases #3 and #5).
2459     assert(igvn-&gt;type(addp) == TypeRawPtr::NOTNULL, &quot;must be raw pointer&quot;);
2460     assert(addp-&gt;in(AddPNode::Address)-&gt;is_Proj(), &quot;base of raw address must be result projection from allocation&quot;);
2461     intptr_t offs = (int)igvn-&gt;find_intptr_t_con(addp-&gt;in(AddPNode::Offset), Type::OffsetBot);
2462     assert(offs != Type::OffsetBot, &quot;offset must be a constant&quot;);
2463     if (base_t-&gt;isa_aryptr() != NULL) {
<span class="line-modified">2464       // In the case of a flattened value type array, each field has its</span>
2465       // own slice so we need to extract the field being accessed from
2466       // the address computation
2467       t = base_t-&gt;isa_aryptr()-&gt;add_field_offset_and_offset(offs)-&gt;is_oopptr();
2468     } else {
2469       t = base_t-&gt;add_offset(offs)-&gt;is_oopptr();
2470     }
2471   }
2472   int inst_id = base_t-&gt;instance_id();
2473   assert(!t-&gt;is_known_instance() || t-&gt;instance_id() == inst_id,
2474                              &quot;old type must be non-instance or match new type&quot;);
2475 
2476   // The type &#39;t&#39; could be subclass of &#39;base_t&#39;.
2477   // As result t-&gt;offset() could be large then base_t&#39;s size and it will
2478   // cause the failure in add_offset() with narrow oops since TypeOopPtr()
2479   // constructor verifies correctness of the offset.
2480   //
2481   // It could happened on subclass&#39;s branch (from the type profiling
2482   // inlining) which was not eliminated during parsing since the exactness
2483   // of the allocation type was not propagated to the subclass type check.
2484   //
2485   // Or the type &#39;t&#39; could be not related to &#39;base_t&#39; at all.
2486   // It could happen when CHA type is different from MDO type on a dead path
2487   // (for example, from instanceof check) which is not collapsed during parsing.
2488   //
2489   // Do nothing for such AddP node and don&#39;t process its users since
2490   // this code branch will go away.
2491   //
2492   if (!t-&gt;is_known_instance() &amp;&amp;
2493       !base_t-&gt;klass()-&gt;is_subtype_of(t-&gt;klass())) {
2494      return false; // bail out
2495   }
2496   const TypePtr* tinst = base_t-&gt;add_offset(t-&gt;offset());
2497   if (tinst-&gt;isa_aryptr() &amp;&amp; t-&gt;isa_aryptr()) {
<span class="line-modified">2498     // In the case of a flattened value type array, each field has its</span>
2499     // own slice so we need to keep track of the field being accessed.
2500     tinst = tinst-&gt;is_aryptr()-&gt;with_field_offset(t-&gt;is_aryptr()-&gt;field_offset().get());
2501   }
2502 
2503   // Do NOT remove the next line: ensure a new alias index is allocated
2504   // for the instance type. Note: C++ will not remove it since the call
2505   // has side effect.
2506   int alias_idx = _compile-&gt;get_alias_index(tinst);
2507   igvn-&gt;set_type(addp, tinst);
2508   // record the allocation in the node map
2509   set_map(addp, get_map(base-&gt;_idx));
2510   // Set addp&#39;s Base and Address to &#39;base&#39;.
2511   Node *abase = addp-&gt;in(AddPNode::Base);
2512   Node *adr   = addp-&gt;in(AddPNode::Address);
2513   if (adr-&gt;is_Proj() &amp;&amp; adr-&gt;in(0)-&gt;is_Allocate() &amp;&amp;
2514       adr-&gt;in(0)-&gt;_idx == (uint)inst_id) {
2515     // Skip AddP cases #3 and #5.
2516   } else {
2517     assert(!abase-&gt;is_top(), &quot;sanity&quot;); // AddP case #3
2518     if (abase != base) {
</pre>
<hr />
<pre>
3225                  (use-&gt;is_ConstraintCast() &amp;&amp; use-&gt;Opcode() == Op_CastPP)) {
3226         alloc_worklist.append_if_missing(use);
3227 #ifdef ASSERT
3228       } else if (use-&gt;is_Mem()) {
3229         assert(use-&gt;in(MemNode::Address) != n, &quot;EA: missing allocation reference path&quot;);
3230       } else if (use-&gt;is_MergeMem()) {
3231         assert(_mergemem_worklist.contains(use-&gt;as_MergeMem()), &quot;EA: missing MergeMem node in the worklist&quot;);
3232       } else if (use-&gt;is_SafePoint()) {
3233         // Look for MergeMem nodes for calls which reference unique allocation
3234         // (through CheckCastPP nodes) even for debug info.
3235         Node* m = use-&gt;in(TypeFunc::Memory);
3236         if (m-&gt;is_MergeMem()) {
3237           assert(_mergemem_worklist.contains(m-&gt;as_MergeMem()), &quot;EA: missing MergeMem node in the worklist&quot;);
3238         }
3239       } else if (use-&gt;Opcode() == Op_EncodeISOArray) {
3240         if (use-&gt;in(MemNode::Memory) == n || use-&gt;in(3) == n) {
3241           // EncodeISOArray overwrites destination array
3242           memnode_worklist.append_if_missing(use);
3243         }
3244       } else if (use-&gt;Opcode() == Op_Return) {
<span class="line-modified">3245         assert(_compile-&gt;tf()-&gt;returns_value_type_as_fields(), &quot;must return a value type&quot;);</span>
3246         // Get InlineKlass by removing the tag bit from the metadata pointer
3247         Node* klass = use-&gt;in(TypeFunc::Parms);
3248         intptr_t ptr = igvn-&gt;type(klass)-&gt;isa_rawptr()-&gt;get_con();
3249         clear_nth_bit(ptr, 0);
3250         assert(Metaspace::contains((void*)ptr), &quot;should be klass&quot;);
3251         assert(((InlineKlass*)ptr)-&gt;contains_oops(), &quot;returned inline type must contain a reference field&quot;);
3252       } else {
3253         uint op = use-&gt;Opcode();
3254         if ((op == Op_StrCompressedCopy || op == Op_StrInflatedCopy) &amp;&amp;
3255             (use-&gt;in(MemNode::Memory) == n)) {
3256           // They overwrite memory edge corresponding to destination array,
3257           memnode_worklist.append_if_missing(use);
3258         } else if (!(op == Op_CmpP || op == Op_Conv2B ||
3259               op == Op_CastP2X || op == Op_StoreCM ||
3260               op == Op_FastLock || op == Op_AryEq || op == Op_StrComp || op == Op_HasNegatives ||
3261               op == Op_StrCompressedCopy || op == Op_StrInflatedCopy ||
3262               op == Op_StrEquals || op == Op_StrIndexOf || op == Op_StrIndexOfChar ||
<span class="line-modified">3263               op == Op_SubTypeCheck || op == Op_ValueType || op == Op_ValueTypePtr ||</span>
3264               BarrierSet::barrier_set()-&gt;barrier_set_c2()-&gt;is_gc_barrier_node(use))) {
3265           n-&gt;dump();
3266           use-&gt;dump();
3267           assert(false, &quot;EA: missing allocation reference path&quot;);
3268         }
3269 #endif
3270       }
3271     }
3272 
3273   }
3274 
3275   // Go over all ArrayCopy nodes and if one of the inputs has a unique
3276   // type, record it in the ArrayCopy node so we know what memory this
3277   // node uses/modified.
3278   for (int next = 0; next &lt; arraycopy_worklist.length(); next++) {
3279     ArrayCopyNode* ac = arraycopy_worklist.at(next);
3280     Node* dest = ac-&gt;in(ArrayCopyNode::Dest);
3281     if (dest-&gt;is_AddP()) {
3282       dest = get_addp_base(dest);
3283     }
</pre>
<hr />
<pre>
3312   //            actually updated until phase 4.)
3313   if (memnode_worklist.length() == 0)
3314     return;  // nothing to do
3315   while (memnode_worklist.length() != 0) {
3316     Node *n = memnode_worklist.pop();
3317     if (visited.test_set(n-&gt;_idx))
3318       continue;
3319     if (n-&gt;is_Phi() || n-&gt;is_ClearArray()) {
3320       // we don&#39;t need to do anything, but the users must be pushed
3321     } else if (n-&gt;is_MemBar()) { // Initialize, MemBar nodes
3322       // we don&#39;t need to do anything, but the users must be pushed
3323       n = n-&gt;as_MemBar()-&gt;proj_out_or_null(TypeFunc::Memory);
3324       if (n == NULL)
3325         continue;
3326     } else if (n-&gt;Opcode() == Op_StrCompressedCopy ||
3327                n-&gt;Opcode() == Op_EncodeISOArray) {
3328       // get the memory projection
3329       n = n-&gt;find_out_with(Op_SCMemProj);
3330       assert(n != NULL &amp;&amp; n-&gt;Opcode() == Op_SCMemProj, &quot;memory projection required&quot;);
3331     } else if (n-&gt;is_CallLeaf() &amp;&amp; n-&gt;as_CallLeaf()-&gt;_name != NULL &amp;&amp;
<span class="line-modified">3332                strcmp(n-&gt;as_CallLeaf()-&gt;_name, &quot;store_unknown_value&quot;) == 0) {</span>
3333       n = n-&gt;as_CallLeaf()-&gt;proj_out(TypeFunc::Memory);
3334     } else {
3335       assert(n-&gt;is_Mem(), &quot;memory node required.&quot;);
3336       Node *addr = n-&gt;in(MemNode::Address);
3337       const Type *addr_t = igvn-&gt;type(addr);
3338       if (addr_t == Type::TOP)
3339         continue;
3340       assert (addr_t-&gt;isa_ptr() != NULL, &quot;pointer type required.&quot;);
3341       int alias_idx = _compile-&gt;get_alias_index(addr_t-&gt;is_ptr());
3342       assert ((uint)alias_idx &lt; new_index_end, &quot;wrong alias index&quot;);
3343       Node *mem = find_inst_mem(n-&gt;in(MemNode::Memory), alias_idx, orig_phis);
3344       if (_compile-&gt;failing()) {
3345         return;
3346       }
3347       if (mem != n-&gt;in(MemNode::Memory)) {
3348         // We delay the memory edge update since we need old one in
3349         // MergeMem code below when instances memory slices are separated.
3350         set_map(n, mem);
3351       }
3352       if (n-&gt;is_Load()) {
</pre>
<hr />
<pre>
3364         memnode_worklist.append_if_missing(use);
3365       } else if (use-&gt;is_Mem() &amp;&amp; use-&gt;in(MemNode::Memory) == n) {
3366         if (use-&gt;Opcode() == Op_StoreCM) // Ignore cardmark stores
3367           continue;
3368         memnode_worklist.append_if_missing(use);
3369       } else if (use-&gt;is_MemBar()) {
3370         if (use-&gt;in(TypeFunc::Memory) == n) { // Ignore precedent edge
3371           memnode_worklist.append_if_missing(use);
3372         }
3373 #ifdef ASSERT
3374       } else if (use-&gt;is_Mem()) {
3375         assert(use-&gt;in(MemNode::Memory) != n, &quot;EA: missing memory path&quot;);
3376       } else if (use-&gt;is_MergeMem()) {
3377         assert(_mergemem_worklist.contains(use-&gt;as_MergeMem()), &quot;EA: missing MergeMem node in the worklist&quot;);
3378       } else if (use-&gt;Opcode() == Op_EncodeISOArray) {
3379         if (use-&gt;in(MemNode::Memory) == n || use-&gt;in(3) == n) {
3380           // EncodeISOArray overwrites destination array
3381           memnode_worklist.append_if_missing(use);
3382         }
3383       } else if (use-&gt;is_CallLeaf() &amp;&amp; use-&gt;as_CallLeaf()-&gt;_name != NULL &amp;&amp;
<span class="line-modified">3384                  strcmp(use-&gt;as_CallLeaf()-&gt;_name, &quot;store_unknown_value&quot;) == 0) {</span>
<span class="line-modified">3385         // store_unknown_value overwrites destination array</span>
3386         memnode_worklist.append_if_missing(use);
3387       } else {
3388         uint op = use-&gt;Opcode();
3389         if ((use-&gt;in(MemNode::Memory) == n) &amp;&amp;
3390             (op == Op_StrCompressedCopy || op == Op_StrInflatedCopy)) {
3391           // They overwrite memory edge corresponding to destination array,
3392           memnode_worklist.append_if_missing(use);
3393         } else if (!(BarrierSet::barrier_set()-&gt;barrier_set_c2()-&gt;is_gc_barrier_node(use) ||
3394               op == Op_AryEq || op == Op_StrComp || op == Op_HasNegatives ||
3395               op == Op_StrCompressedCopy || op == Op_StrInflatedCopy ||
3396               op == Op_StrEquals || op == Op_StrIndexOf || op == Op_StrIndexOfChar)) {
3397           n-&gt;dump();
3398           use-&gt;dump();
3399           assert(false, &quot;EA: missing memory path&quot;);
3400         }
3401 #endif
3402       }
3403     }
3404   }
3405 
</pre>
</td>
<td>
<hr />
<pre>
 372       // Put Lock and Unlock nodes on IGVN worklist to process them during
 373       // first IGVN optimization when escape information is still available.
 374       record_for_optimizer(n);
 375     } else if (n-&gt;is_Allocate()) {
 376       add_call_node(n-&gt;as_Call());
 377       record_for_optimizer(n);
 378     } else {
 379       if (n-&gt;is_CallStaticJava()) {
 380         const char* name = n-&gt;as_CallStaticJava()-&gt;_name;
 381         if (name != NULL &amp;&amp; strcmp(name, &quot;uncommon_trap&quot;) == 0)
 382           return; // Skip uncommon traps
 383       }
 384       // Don&#39;t mark as processed since call&#39;s arguments have to be processed.
 385       delayed_worklist-&gt;push(n);
 386       // Check if a call returns an object.
 387       if ((n-&gt;as_Call()-&gt;returns_pointer() &amp;&amp;
 388            n-&gt;as_Call()-&gt;proj_out_or_null(TypeFunc::Parms) != NULL) ||
 389           (n-&gt;is_CallStaticJava() &amp;&amp;
 390            n-&gt;as_CallStaticJava()-&gt;is_boxing_method())) {
 391         add_call_node(n-&gt;as_Call());
<span class="line-modified"> 392       } else if (n-&gt;as_Call()-&gt;tf()-&gt;returns_inline_type_as_fields()) {</span>
 393         bool returns_oop = false;
 394         for (DUIterator_Fast imax, i = n-&gt;fast_outs(imax); i &lt; imax &amp;&amp; !returns_oop; i++) {
 395           ProjNode* pn = n-&gt;fast_out(i)-&gt;as_Proj();
 396           if (pn-&gt;_con &gt;= TypeFunc::Parms &amp;&amp; pn-&gt;bottom_type()-&gt;isa_ptr()) {
 397             returns_oop = true;
 398           }
 399         }
 400         if (returns_oop) {
 401           add_call_node(n-&gt;as_Call());
 402         }
 403       }
 404     }
 405     return;
 406   }
 407   // Put this check here to process call arguments since some call nodes
 408   // point to phantom_obj.
 409   if (n_ptn == phantom_obj || n_ptn == null_obj)
 410     return; // Skip predefined nodes.
 411 
 412   switch (opcode) {
</pre>
<hr />
<pre>
 414       Node* base = get_addp_base(n);
 415       PointsToNode* ptn_base = ptnode_adr(base-&gt;_idx);
 416       // Field nodes are created for all field types. They are used in
 417       // adjust_scalar_replaceable_state() and split_unique_types().
 418       // Note, non-oop fields will have only base edges in Connection
 419       // Graph because such fields are not used for oop loads and stores.
 420       int offset = address_offset(n, igvn);
 421       add_field(n, PointsToNode::NoEscape, offset);
 422       if (ptn_base == NULL) {
 423         delayed_worklist-&gt;push(n); // Process it later.
 424       } else {
 425         n_ptn = ptnode_adr(n_idx);
 426         add_base(n_ptn-&gt;as_Field(), ptn_base);
 427       }
 428       break;
 429     }
 430     case Op_CastX2P: {
 431       map_ideal_node(n, phantom_obj);
 432       break;
 433     }
<span class="line-modified"> 434     case Op_InlineTypePtr:</span>
 435     case Op_CastPP:
 436     case Op_CheckCastPP:
 437     case Op_EncodeP:
 438     case Op_DecodeN:
 439     case Op_EncodePKlass:
 440     case Op_DecodeNKlass: {
 441       add_local_var_and_edge(n, PointsToNode::NoEscape,
 442                              n-&gt;in(1), delayed_worklist);
 443       break;
 444     }
 445     case Op_CMoveP: {
 446       add_local_var(n, PointsToNode::NoEscape);
 447       // Do not add edges during first iteration because some could be
 448       // not defined yet.
 449       delayed_worklist-&gt;push(n);
 450       break;
 451     }
 452     case Op_ConP:
 453     case Op_ConN:
 454     case Op_ConNKlass: {
</pre>
<hr />
<pre>
 490       map_ideal_node(n, phantom_obj); // Result is unknown
 491       break;
 492     }
 493     case Op_Phi: {
 494       // Using isa_ptr() instead of isa_oopptr() for LoadP and Phi because
 495       // ThreadLocal has RawPtr type.
 496       const Type* t = n-&gt;as_Phi()-&gt;type();
 497       if (t-&gt;make_ptr() != NULL) {
 498         add_local_var(n, PointsToNode::NoEscape);
 499         // Do not add edges during first iteration because some could be
 500         // not defined yet.
 501         delayed_worklist-&gt;push(n);
 502       }
 503       break;
 504     }
 505     case Op_Proj: {
 506       // we are only interested in the oop result projection from a call
 507       if (n-&gt;as_Proj()-&gt;_con &gt;= TypeFunc::Parms &amp;&amp; n-&gt;in(0)-&gt;is_Call() &amp;&amp;
 508           (n-&gt;in(0)-&gt;as_Call()-&gt;returns_pointer() || n-&gt;bottom_type()-&gt;isa_ptr())) {
 509         assert((n-&gt;as_Proj()-&gt;_con == TypeFunc::Parms &amp;&amp; n-&gt;in(0)-&gt;as_Call()-&gt;returns_pointer()) ||
<span class="line-modified"> 510                n-&gt;in(0)-&gt;as_Call()-&gt;tf()-&gt;returns_inline_type_as_fields(), &quot;what kind of oop return is it?&quot;);</span>
 511         add_local_var_and_edge(n, PointsToNode::NoEscape,
 512                                n-&gt;in(0), delayed_worklist);
 513       }
 514       break;
 515     }
 516     case Op_Rethrow: // Exception object escapes
 517     case Op_Return: {
 518       if (n-&gt;req() &gt; TypeFunc::Parms &amp;&amp;
 519           igvn-&gt;type(n-&gt;in(TypeFunc::Parms))-&gt;isa_oopptr()) {
 520         // Treat Return value as LocalVar with GlobalEscape escape state.
 521         add_local_var_and_edge(n, PointsToNode::GlobalEscape,
 522                                n-&gt;in(TypeFunc::Parms), delayed_worklist);
 523       }
 524       break;
 525     }
 526     case Op_CompareAndExchangeP:
 527     case Op_CompareAndExchangeN:
 528     case Op_GetAndSetP:
 529     case Op_GetAndSetN: {
 530       add_objload_to_connection_graph(n, delayed_worklist);
</pre>
<hr />
<pre>
 586   if (n-&gt;is_Call()) {
 587     process_call_arguments(n-&gt;as_Call());
 588     return;
 589   }
 590   assert(n-&gt;is_Store() || n-&gt;is_LoadStore() ||
 591          (n_ptn != NULL) &amp;&amp; (n_ptn-&gt;ideal_node() != NULL),
 592          &quot;node should be registered already&quot;);
 593   int opcode = n-&gt;Opcode();
 594   bool gc_handled = BarrierSet::barrier_set()-&gt;barrier_set_c2()-&gt;escape_add_final_edges(this, _igvn, n, opcode);
 595   if (gc_handled) {
 596     return; // Ignore node if already handled by GC.
 597   }
 598   switch (opcode) {
 599     case Op_AddP: {
 600       Node* base = get_addp_base(n);
 601       PointsToNode* ptn_base = ptnode_adr(base-&gt;_idx);
 602       assert(ptn_base != NULL, &quot;field&#39;s base should be registered&quot;);
 603       add_base(n_ptn-&gt;as_Field(), ptn_base);
 604       break;
 605     }
<span class="line-modified"> 606     case Op_InlineTypePtr:</span>
 607     case Op_CastPP:
 608     case Op_CheckCastPP:
 609     case Op_EncodeP:
 610     case Op_DecodeN:
 611     case Op_EncodePKlass:
 612     case Op_DecodeNKlass: {
 613       add_local_var_and_edge(n, PointsToNode::NoEscape,
 614                              n-&gt;in(1), NULL);
 615       break;
 616     }
 617     case Op_CMoveP: {
 618       for (uint i = CMoveNode::IfFalse; i &lt; n-&gt;req(); i++) {
 619         Node* in = n-&gt;in(i);
 620         if (in == NULL)
 621           continue;  // ignore NULL
 622         Node* uncast_in = in-&gt;uncast();
 623         if (uncast_in-&gt;is_top() || uncast_in == n)
 624           continue;  // ignore top or inputs which go back this node
 625         PointsToNode* ptn = ptnode_adr(in-&gt;_idx);
 626         assert(ptn != NULL, &quot;node should be registered&quot;);
</pre>
<hr />
<pre>
 649         for (uint i = 1; i &lt; n-&gt;req(); i++) {
 650           Node* in = n-&gt;in(i);
 651           if (in == NULL)
 652             continue;  // ignore NULL
 653           Node* uncast_in = in-&gt;uncast();
 654           if (uncast_in-&gt;is_top() || uncast_in == n)
 655             continue;  // ignore top or inputs which go back this node
 656           PointsToNode* ptn = ptnode_adr(in-&gt;_idx);
 657           assert(ptn != NULL, &quot;node should be registered&quot;);
 658           add_edge(n_ptn, ptn);
 659         }
 660         break;
 661       }
 662       ELSE_FAIL(&quot;Op_Phi&quot;);
 663     }
 664     case Op_Proj: {
 665       // we are only interested in the oop result projection from a call
 666       if (n-&gt;as_Proj()-&gt;_con &gt;= TypeFunc::Parms &amp;&amp; n-&gt;in(0)-&gt;is_Call() &amp;&amp;
 667           (n-&gt;in(0)-&gt;as_Call()-&gt;returns_pointer()|| n-&gt;bottom_type()-&gt;isa_ptr())) {
 668         assert((n-&gt;as_Proj()-&gt;_con == TypeFunc::Parms &amp;&amp; n-&gt;in(0)-&gt;as_Call()-&gt;returns_pointer()) ||
<span class="line-modified"> 669                n-&gt;in(0)-&gt;as_Call()-&gt;tf()-&gt;returns_inline_type_as_fields(), &quot;what kind of oop return is it?&quot;);</span>
 670         add_local_var_and_edge(n, PointsToNode::NoEscape, n-&gt;in(0), NULL);
 671         break;
 672       }
 673       ELSE_FAIL(&quot;Op_Proj&quot;);
 674     }
 675     case Op_Rethrow: // Exception object escapes
 676     case Op_Return: {
 677       if (n-&gt;req() &gt; TypeFunc::Parms &amp;&amp;
 678           _igvn-&gt;type(n-&gt;in(TypeFunc::Parms))-&gt;isa_oopptr()) {
 679         // Treat Return value as LocalVar with GlobalEscape escape state.
 680         add_local_var_and_edge(n, PointsToNode::GlobalEscape,
 681                                n-&gt;in(TypeFunc::Parms), NULL);
 682         break;
 683       }
 684       ELSE_FAIL(&quot;Op_Return&quot;);
 685     }
 686     case Op_StoreP:
 687     case Op_StoreN:
 688     case Op_StoreNKlass:
 689     case Op_StorePConditional:
</pre>
<hr />
<pre>
 811     return true;
 812   } else if ((opcode == Op_StoreP) &amp;&amp; adr_type-&gt;isa_rawptr()) {
 813     // Stored value escapes in unsafe access.
 814     Node* val = n-&gt;in(MemNode::ValueIn);
 815     PointsToNode* ptn = ptnode_adr(val-&gt;_idx);
 816     assert(ptn != NULL, &quot;node should be registered&quot;);
 817     set_escape_state(ptn, PointsToNode::GlobalEscape);
 818     // Add edge to object for unsafe access with offset.
 819     PointsToNode* adr_ptn = ptnode_adr(adr-&gt;_idx);
 820     assert(adr_ptn != NULL, &quot;node should be registered&quot;);
 821     if (adr_ptn-&gt;is_Field()) {
 822       assert(adr_ptn-&gt;as_Field()-&gt;is_oop(), &quot;should be oop field&quot;);
 823       add_edge(adr_ptn, ptn);
 824     }
 825     return true;
 826   }
 827   return false;
 828 }
 829 
 830 void ConnectionGraph::add_call_node(CallNode* call) {
<span class="line-modified"> 831   assert(call-&gt;returns_pointer() || call-&gt;tf()-&gt;returns_inline_type_as_fields(), &quot;only for call which returns pointer&quot;);</span>
 832   uint call_idx = call-&gt;_idx;
 833   if (call-&gt;is_Allocate()) {
 834     Node* k = call-&gt;in(AllocateNode::KlassNode);
 835     const TypeKlassPtr* kt = k-&gt;bottom_type()-&gt;isa_klassptr();
 836     assert(kt != NULL, &quot;TypeKlassPtr  required.&quot;);
 837     ciKlass* cik = kt-&gt;klass();
 838     PointsToNode::EscapeState es = PointsToNode::NoEscape;
 839     bool scalar_replaceable = true;
 840     if (call-&gt;is_AllocateArray()) {
 841       if (!cik-&gt;is_array_klass()) { // StressReflectiveCode
 842         es = PointsToNode::GlobalEscape;
 843       } else {
 844         int length = call-&gt;in(AllocateNode::ALength)-&gt;find_int_con(-1);
 845         if (length &lt; 0 || length &gt; EliminateAllocationArraySizeLimit) {
 846           // Not scalar replaceable if the length is not constant or too big.
 847           scalar_replaceable = false;
 848         }
 849       }
 850     } else {  // Allocate instance
 851       if (cik-&gt;is_subclass_of(_compile-&gt;env()-&gt;Thread_klass()) ||
</pre>
<hr />
<pre>
 977           //
 978           // The inline_native_clone() case when the arraycopy stub is called
 979           // after the allocation before Initialize and CheckCastPP nodes.
 980           // Or normal arraycopy for object arrays case.
 981           //
 982           // Set AddP&#39;s base (Allocate) as not scalar replaceable since
 983           // pointer to the base (with offset) is passed as argument.
 984           //
 985           arg = get_addp_base(arg);
 986         }
 987         PointsToNode* arg_ptn = ptnode_adr(arg-&gt;_idx);
 988         assert(arg_ptn != NULL, &quot;should be registered&quot;);
 989         PointsToNode::EscapeState arg_esc = arg_ptn-&gt;escape_state();
 990         if (is_arraycopy || arg_esc &lt; PointsToNode::ArgEscape) {
 991           assert(aat == Type::TOP || aat == TypePtr::NULL_PTR ||
 992                  aat-&gt;isa_ptr() != NULL, &quot;expecting an Ptr&quot;);
 993           bool arg_has_oops = aat-&gt;isa_oopptr() &amp;&amp;
 994                               (aat-&gt;isa_oopptr()-&gt;klass() == NULL || aat-&gt;isa_instptr() ||
 995                                (aat-&gt;isa_aryptr() &amp;&amp; aat-&gt;isa_aryptr()-&gt;klass()-&gt;is_obj_array_klass()) ||
 996                                (aat-&gt;isa_aryptr() &amp;&amp; aat-&gt;isa_aryptr()-&gt;elem() != NULL &amp;&amp;
<span class="line-modified"> 997                                 aat-&gt;isa_aryptr()-&gt;elem()-&gt;isa_inlinetype() &amp;&amp;</span>
<span class="line-modified"> 998                                 aat-&gt;isa_aryptr()-&gt;elem()-&gt;inline_klass()-&gt;contains_oops()));</span>
 999           if (i == TypeFunc::Parms) {
1000             src_has_oops = arg_has_oops;
1001           }
1002           //
1003           // src or dst could be j.l.Object when other is basic type array:
1004           //
1005           //   arraycopy(char[],0,Object*,0,size);
1006           //   arraycopy(Object*,0,char[],0,size);
1007           //
1008           // Don&#39;t add edges in such cases.
1009           //
1010           bool arg_is_arraycopy_dest = src_has_oops &amp;&amp; is_arraycopy &amp;&amp;
1011                                        arg_has_oops &amp;&amp; (i &gt; TypeFunc::Parms);
1012 #ifdef ASSERT
1013           if (!(is_arraycopy ||
1014                 BarrierSet::barrier_set()-&gt;barrier_set_c2()-&gt;is_gc_barrier_node(call) ||
1015                 (call-&gt;as_CallLeaf()-&gt;_name != NULL &amp;&amp;
1016                  (strcmp(call-&gt;as_CallLeaf()-&gt;_name, &quot;updateBytesCRC32&quot;) == 0 ||
1017                   strcmp(call-&gt;as_CallLeaf()-&gt;_name, &quot;updateBytesCRC32C&quot;) == 0 ||
1018                   strcmp(call-&gt;as_CallLeaf()-&gt;_name, &quot;updateBytesAdler32&quot;) == 0 ||
</pre>
<hr />
<pre>
1020                   strcmp(call-&gt;as_CallLeaf()-&gt;_name, &quot;aescrypt_decryptBlock&quot;) == 0 ||
1021                   strcmp(call-&gt;as_CallLeaf()-&gt;_name, &quot;cipherBlockChaining_encryptAESCrypt&quot;) == 0 ||
1022                   strcmp(call-&gt;as_CallLeaf()-&gt;_name, &quot;cipherBlockChaining_decryptAESCrypt&quot;) == 0 ||
1023                   strcmp(call-&gt;as_CallLeaf()-&gt;_name, &quot;electronicCodeBook_encryptAESCrypt&quot;) == 0 ||
1024                   strcmp(call-&gt;as_CallLeaf()-&gt;_name, &quot;electronicCodeBook_decryptAESCrypt&quot;) == 0 ||
1025                   strcmp(call-&gt;as_CallLeaf()-&gt;_name, &quot;counterMode_AESCrypt&quot;) == 0 ||
1026                   strcmp(call-&gt;as_CallLeaf()-&gt;_name, &quot;ghash_processBlocks&quot;) == 0 ||
1027                   strcmp(call-&gt;as_CallLeaf()-&gt;_name, &quot;encodeBlock&quot;) == 0 ||
1028                   strcmp(call-&gt;as_CallLeaf()-&gt;_name, &quot;sha1_implCompress&quot;) == 0 ||
1029                   strcmp(call-&gt;as_CallLeaf()-&gt;_name, &quot;sha1_implCompressMB&quot;) == 0 ||
1030                   strcmp(call-&gt;as_CallLeaf()-&gt;_name, &quot;sha256_implCompress&quot;) == 0 ||
1031                   strcmp(call-&gt;as_CallLeaf()-&gt;_name, &quot;sha256_implCompressMB&quot;) == 0 ||
1032                   strcmp(call-&gt;as_CallLeaf()-&gt;_name, &quot;sha512_implCompress&quot;) == 0 ||
1033                   strcmp(call-&gt;as_CallLeaf()-&gt;_name, &quot;sha512_implCompressMB&quot;) == 0 ||
1034                   strcmp(call-&gt;as_CallLeaf()-&gt;_name, &quot;multiplyToLen&quot;) == 0 ||
1035                   strcmp(call-&gt;as_CallLeaf()-&gt;_name, &quot;squareToLen&quot;) == 0 ||
1036                   strcmp(call-&gt;as_CallLeaf()-&gt;_name, &quot;mulAdd&quot;) == 0 ||
1037                   strcmp(call-&gt;as_CallLeaf()-&gt;_name, &quot;montgomery_multiply&quot;) == 0 ||
1038                   strcmp(call-&gt;as_CallLeaf()-&gt;_name, &quot;montgomery_square&quot;) == 0 ||
1039                   strcmp(call-&gt;as_CallLeaf()-&gt;_name, &quot;vectorizedMismatch&quot;) == 0 ||
<span class="line-modified">1040                   strcmp(call-&gt;as_CallLeaf()-&gt;_name, &quot;load_unknown_inline&quot;) == 0 ||</span>
<span class="line-modified">1041                   strcmp(call-&gt;as_CallLeaf()-&gt;_name, &quot;store_unknown_inline&quot;) == 0 ||</span>
1042                   strcmp(call-&gt;as_CallLeaf()-&gt;_name, &quot;bigIntegerRightShiftWorker&quot;) == 0 ||
1043                   strcmp(call-&gt;as_CallLeaf()-&gt;_name, &quot;bigIntegerLeftShiftWorker&quot;) == 0 ||
1044                   strcmp(call-&gt;as_CallLeaf()-&gt;_name, &quot;vectorizedMismatch&quot;) == 0)
1045                  ))) {
1046             call-&gt;dump();
1047             fatal(&quot;EA unexpected CallLeaf %s&quot;, call-&gt;as_CallLeaf()-&gt;_name);
1048           }
1049 #endif
1050           // Always process arraycopy&#39;s destination object since
1051           // we need to add all possible edges to references in
1052           // source object.
1053           if (arg_esc &gt;= PointsToNode::ArgEscape &amp;&amp;
1054               !arg_is_arraycopy_dest) {
1055             continue;
1056           }
1057           PointsToNode::EscapeState es = PointsToNode::ArgEscape;
1058           if (call-&gt;is_ArrayCopy()) {
1059             ArrayCopyNode* ac = call-&gt;as_ArrayCopy();
1060             if (ac-&gt;is_clonebasic() ||
1061                 ac-&gt;is_arraycopy_validated() ||
</pre>
<hr />
<pre>
1885     }
1886   }
1887 }
1888 #endif
1889 
1890 // Optimize ideal graph.
1891 void ConnectionGraph::optimize_ideal_graph(GrowableArray&lt;Node*&gt;&amp; ptr_cmp_worklist,
1892                                            GrowableArray&lt;Node*&gt;&amp; storestore_worklist) {
1893   Compile* C = _compile;
1894   PhaseIterGVN* igvn = _igvn;
1895   if (EliminateLocks) {
1896     // Mark locks before changing ideal graph.
1897     int cnt = C-&gt;macro_count();
1898     for( int i=0; i &lt; cnt; i++ ) {
1899       Node *n = C-&gt;macro_node(i);
1900       if (n-&gt;is_AbstractLock()) { // Lock and Unlock nodes
1901         AbstractLockNode* alock = n-&gt;as_AbstractLock();
1902         if (!alock-&gt;is_non_esc_obj()) {
1903           const Type* obj_type = igvn-&gt;type(alock-&gt;obj_node());
1904           if (not_global_escape(alock-&gt;obj_node()) &amp;&amp;
<span class="line-modified">1905               !obj_type-&gt;isa_inlinetype() &amp;&amp; !obj_type-&gt;is_inlinetypeptr()) {</span>
1906             assert(!alock-&gt;is_eliminated() || alock-&gt;is_coarsened(), &quot;sanity&quot;);
1907             // The lock could be marked eliminated by lock coarsening
1908             // code during first IGVN before EA. Replace coarsened flag
1909             // to eliminate all associated locks/unlocks.
1910 #ifdef ASSERT
1911             alock-&gt;log_lock_optimization(C, &quot;eliminate_lock_set_non_esc3&quot;);
1912 #endif
1913             alock-&gt;set_non_esc_obj();
1914           }
1915         }
1916       }
1917     }
1918   }
1919 
1920   if (OptimizePtrCompare) {
1921     // Add ConI(#CC_GT) and ConI(#CC_EQ).
1922     _pcmp_neq = igvn-&gt;makecon(TypeInt::CC_GT);
1923     _pcmp_eq = igvn-&gt;makecon(TypeInt::CC_EQ);
1924     // Optimize objects compare.
1925     while (ptr_cmp_worklist.length() != 0) {
</pre>
<hr />
<pre>
2122       ciField* field = _compile-&gt;alias_type(adr_type-&gt;is_ptr())-&gt;field();
2123       if (field != NULL) {
2124         bt = field-&gt;layout_type();
2125       } else {
2126         // Check for unsafe oop field access
2127         if (n-&gt;has_out_with(Op_StoreP, Op_LoadP, Op_StoreN, Op_LoadN) ||
2128             n-&gt;has_out_with(Op_GetAndSetP, Op_GetAndSetN, Op_CompareAndExchangeP, Op_CompareAndExchangeN) ||
2129             n-&gt;has_out_with(Op_CompareAndSwapP, Op_CompareAndSwapN, Op_WeakCompareAndSwapP, Op_WeakCompareAndSwapN) ||
2130             BarrierSet::barrier_set()-&gt;barrier_set_c2()-&gt;escape_has_out_with_unsafe_object(n)) {
2131           bt = T_OBJECT;
2132           (*unsafe) = true;
2133         }
2134       }
2135     } else if (adr_type-&gt;isa_aryptr()) {
2136       if (offset == arrayOopDesc::length_offset_in_bytes()) {
2137         // Ignore array length load.
2138       } else if (find_second_addp(n, n-&gt;in(AddPNode::Base)) != NULL) {
2139         // Ignore first AddP.
2140       } else {
2141         const Type* elemtype = adr_type-&gt;isa_aryptr()-&gt;elem();
<span class="line-modified">2142         if (elemtype-&gt;isa_inlinetype() &amp;&amp; field_offset != Type::OffsetBot) {</span>
<span class="line-modified">2143           ciInlineKlass* vk = elemtype-&gt;inline_klass();</span>
2144           field_offset += vk-&gt;first_field_offset();
2145           bt = vk-&gt;get_field_by_offset(field_offset, false)-&gt;layout_type();
2146         } else {
2147           bt = elemtype-&gt;array_element_basic_type();
2148         }
2149       }
2150     } else if (adr_type-&gt;isa_rawptr() || adr_type-&gt;isa_klassptr()) {
2151       // Allocation initialization, ThreadLocal field access, unsafe access
2152       if (n-&gt;has_out_with(Op_StoreP, Op_LoadP, Op_StoreN, Op_LoadN) ||
2153           n-&gt;has_out_with(Op_GetAndSetP, Op_GetAndSetN, Op_CompareAndExchangeP, Op_CompareAndExchangeN) ||
2154           n-&gt;has_out_with(Op_CompareAndSwapP, Op_CompareAndSwapN, Op_WeakCompareAndSwapP, Op_WeakCompareAndSwapN) ||
2155           BarrierSet::barrier_set()-&gt;barrier_set_c2()-&gt;escape_has_out_with_unsafe_object(n)) {
2156         bt = T_OBJECT;
2157       }
2158     }
2159   }
2160   // Note: T_NARROWOOP is not classed as a real reference type
2161   return (is_reference_type(bt) || bt == T_NARROWOOP);
2162 }
2163 
</pre>
<hr />
<pre>
2444   return NULL;
2445 }
2446 
2447 //
2448 // Adjust the type and inputs of an AddP which computes the
2449 // address of a field of an instance
2450 //
2451 bool ConnectionGraph::split_AddP(Node *addp, Node *base) {
2452   PhaseGVN* igvn = _igvn;
2453   const TypeOopPtr *base_t = igvn-&gt;type(base)-&gt;isa_oopptr();
2454   assert(base_t != NULL &amp;&amp; base_t-&gt;is_known_instance(), &quot;expecting instance oopptr&quot;);
2455   const TypeOopPtr *t = igvn-&gt;type(addp)-&gt;isa_oopptr();
2456   if (t == NULL) {
2457     // We are computing a raw address for a store captured by an Initialize
2458     // compute an appropriate address type (cases #3 and #5).
2459     assert(igvn-&gt;type(addp) == TypeRawPtr::NOTNULL, &quot;must be raw pointer&quot;);
2460     assert(addp-&gt;in(AddPNode::Address)-&gt;is_Proj(), &quot;base of raw address must be result projection from allocation&quot;);
2461     intptr_t offs = (int)igvn-&gt;find_intptr_t_con(addp-&gt;in(AddPNode::Offset), Type::OffsetBot);
2462     assert(offs != Type::OffsetBot, &quot;offset must be a constant&quot;);
2463     if (base_t-&gt;isa_aryptr() != NULL) {
<span class="line-modified">2464       // In the case of a flattened inline type array, each field has its</span>
2465       // own slice so we need to extract the field being accessed from
2466       // the address computation
2467       t = base_t-&gt;isa_aryptr()-&gt;add_field_offset_and_offset(offs)-&gt;is_oopptr();
2468     } else {
2469       t = base_t-&gt;add_offset(offs)-&gt;is_oopptr();
2470     }
2471   }
2472   int inst_id = base_t-&gt;instance_id();
2473   assert(!t-&gt;is_known_instance() || t-&gt;instance_id() == inst_id,
2474                              &quot;old type must be non-instance or match new type&quot;);
2475 
2476   // The type &#39;t&#39; could be subclass of &#39;base_t&#39;.
2477   // As result t-&gt;offset() could be large then base_t&#39;s size and it will
2478   // cause the failure in add_offset() with narrow oops since TypeOopPtr()
2479   // constructor verifies correctness of the offset.
2480   //
2481   // It could happened on subclass&#39;s branch (from the type profiling
2482   // inlining) which was not eliminated during parsing since the exactness
2483   // of the allocation type was not propagated to the subclass type check.
2484   //
2485   // Or the type &#39;t&#39; could be not related to &#39;base_t&#39; at all.
2486   // It could happen when CHA type is different from MDO type on a dead path
2487   // (for example, from instanceof check) which is not collapsed during parsing.
2488   //
2489   // Do nothing for such AddP node and don&#39;t process its users since
2490   // this code branch will go away.
2491   //
2492   if (!t-&gt;is_known_instance() &amp;&amp;
2493       !base_t-&gt;klass()-&gt;is_subtype_of(t-&gt;klass())) {
2494      return false; // bail out
2495   }
2496   const TypePtr* tinst = base_t-&gt;add_offset(t-&gt;offset());
2497   if (tinst-&gt;isa_aryptr() &amp;&amp; t-&gt;isa_aryptr()) {
<span class="line-modified">2498     // In the case of a flattened inline type array, each field has its</span>
2499     // own slice so we need to keep track of the field being accessed.
2500     tinst = tinst-&gt;is_aryptr()-&gt;with_field_offset(t-&gt;is_aryptr()-&gt;field_offset().get());
2501   }
2502 
2503   // Do NOT remove the next line: ensure a new alias index is allocated
2504   // for the instance type. Note: C++ will not remove it since the call
2505   // has side effect.
2506   int alias_idx = _compile-&gt;get_alias_index(tinst);
2507   igvn-&gt;set_type(addp, tinst);
2508   // record the allocation in the node map
2509   set_map(addp, get_map(base-&gt;_idx));
2510   // Set addp&#39;s Base and Address to &#39;base&#39;.
2511   Node *abase = addp-&gt;in(AddPNode::Base);
2512   Node *adr   = addp-&gt;in(AddPNode::Address);
2513   if (adr-&gt;is_Proj() &amp;&amp; adr-&gt;in(0)-&gt;is_Allocate() &amp;&amp;
2514       adr-&gt;in(0)-&gt;_idx == (uint)inst_id) {
2515     // Skip AddP cases #3 and #5.
2516   } else {
2517     assert(!abase-&gt;is_top(), &quot;sanity&quot;); // AddP case #3
2518     if (abase != base) {
</pre>
<hr />
<pre>
3225                  (use-&gt;is_ConstraintCast() &amp;&amp; use-&gt;Opcode() == Op_CastPP)) {
3226         alloc_worklist.append_if_missing(use);
3227 #ifdef ASSERT
3228       } else if (use-&gt;is_Mem()) {
3229         assert(use-&gt;in(MemNode::Address) != n, &quot;EA: missing allocation reference path&quot;);
3230       } else if (use-&gt;is_MergeMem()) {
3231         assert(_mergemem_worklist.contains(use-&gt;as_MergeMem()), &quot;EA: missing MergeMem node in the worklist&quot;);
3232       } else if (use-&gt;is_SafePoint()) {
3233         // Look for MergeMem nodes for calls which reference unique allocation
3234         // (through CheckCastPP nodes) even for debug info.
3235         Node* m = use-&gt;in(TypeFunc::Memory);
3236         if (m-&gt;is_MergeMem()) {
3237           assert(_mergemem_worklist.contains(m-&gt;as_MergeMem()), &quot;EA: missing MergeMem node in the worklist&quot;);
3238         }
3239       } else if (use-&gt;Opcode() == Op_EncodeISOArray) {
3240         if (use-&gt;in(MemNode::Memory) == n || use-&gt;in(3) == n) {
3241           // EncodeISOArray overwrites destination array
3242           memnode_worklist.append_if_missing(use);
3243         }
3244       } else if (use-&gt;Opcode() == Op_Return) {
<span class="line-modified">3245         assert(_compile-&gt;tf()-&gt;returns_inline_type_as_fields(), &quot;must return an inline type&quot;);</span>
3246         // Get InlineKlass by removing the tag bit from the metadata pointer
3247         Node* klass = use-&gt;in(TypeFunc::Parms);
3248         intptr_t ptr = igvn-&gt;type(klass)-&gt;isa_rawptr()-&gt;get_con();
3249         clear_nth_bit(ptr, 0);
3250         assert(Metaspace::contains((void*)ptr), &quot;should be klass&quot;);
3251         assert(((InlineKlass*)ptr)-&gt;contains_oops(), &quot;returned inline type must contain a reference field&quot;);
3252       } else {
3253         uint op = use-&gt;Opcode();
3254         if ((op == Op_StrCompressedCopy || op == Op_StrInflatedCopy) &amp;&amp;
3255             (use-&gt;in(MemNode::Memory) == n)) {
3256           // They overwrite memory edge corresponding to destination array,
3257           memnode_worklist.append_if_missing(use);
3258         } else if (!(op == Op_CmpP || op == Op_Conv2B ||
3259               op == Op_CastP2X || op == Op_StoreCM ||
3260               op == Op_FastLock || op == Op_AryEq || op == Op_StrComp || op == Op_HasNegatives ||
3261               op == Op_StrCompressedCopy || op == Op_StrInflatedCopy ||
3262               op == Op_StrEquals || op == Op_StrIndexOf || op == Op_StrIndexOfChar ||
<span class="line-modified">3263               op == Op_SubTypeCheck || op == Op_InlineType || op == Op_InlineTypePtr ||</span>
3264               BarrierSet::barrier_set()-&gt;barrier_set_c2()-&gt;is_gc_barrier_node(use))) {
3265           n-&gt;dump();
3266           use-&gt;dump();
3267           assert(false, &quot;EA: missing allocation reference path&quot;);
3268         }
3269 #endif
3270       }
3271     }
3272 
3273   }
3274 
3275   // Go over all ArrayCopy nodes and if one of the inputs has a unique
3276   // type, record it in the ArrayCopy node so we know what memory this
3277   // node uses/modified.
3278   for (int next = 0; next &lt; arraycopy_worklist.length(); next++) {
3279     ArrayCopyNode* ac = arraycopy_worklist.at(next);
3280     Node* dest = ac-&gt;in(ArrayCopyNode::Dest);
3281     if (dest-&gt;is_AddP()) {
3282       dest = get_addp_base(dest);
3283     }
</pre>
<hr />
<pre>
3312   //            actually updated until phase 4.)
3313   if (memnode_worklist.length() == 0)
3314     return;  // nothing to do
3315   while (memnode_worklist.length() != 0) {
3316     Node *n = memnode_worklist.pop();
3317     if (visited.test_set(n-&gt;_idx))
3318       continue;
3319     if (n-&gt;is_Phi() || n-&gt;is_ClearArray()) {
3320       // we don&#39;t need to do anything, but the users must be pushed
3321     } else if (n-&gt;is_MemBar()) { // Initialize, MemBar nodes
3322       // we don&#39;t need to do anything, but the users must be pushed
3323       n = n-&gt;as_MemBar()-&gt;proj_out_or_null(TypeFunc::Memory);
3324       if (n == NULL)
3325         continue;
3326     } else if (n-&gt;Opcode() == Op_StrCompressedCopy ||
3327                n-&gt;Opcode() == Op_EncodeISOArray) {
3328       // get the memory projection
3329       n = n-&gt;find_out_with(Op_SCMemProj);
3330       assert(n != NULL &amp;&amp; n-&gt;Opcode() == Op_SCMemProj, &quot;memory projection required&quot;);
3331     } else if (n-&gt;is_CallLeaf() &amp;&amp; n-&gt;as_CallLeaf()-&gt;_name != NULL &amp;&amp;
<span class="line-modified">3332                strcmp(n-&gt;as_CallLeaf()-&gt;_name, &quot;store_unknown_inline&quot;) == 0) {</span>
3333       n = n-&gt;as_CallLeaf()-&gt;proj_out(TypeFunc::Memory);
3334     } else {
3335       assert(n-&gt;is_Mem(), &quot;memory node required.&quot;);
3336       Node *addr = n-&gt;in(MemNode::Address);
3337       const Type *addr_t = igvn-&gt;type(addr);
3338       if (addr_t == Type::TOP)
3339         continue;
3340       assert (addr_t-&gt;isa_ptr() != NULL, &quot;pointer type required.&quot;);
3341       int alias_idx = _compile-&gt;get_alias_index(addr_t-&gt;is_ptr());
3342       assert ((uint)alias_idx &lt; new_index_end, &quot;wrong alias index&quot;);
3343       Node *mem = find_inst_mem(n-&gt;in(MemNode::Memory), alias_idx, orig_phis);
3344       if (_compile-&gt;failing()) {
3345         return;
3346       }
3347       if (mem != n-&gt;in(MemNode::Memory)) {
3348         // We delay the memory edge update since we need old one in
3349         // MergeMem code below when instances memory slices are separated.
3350         set_map(n, mem);
3351       }
3352       if (n-&gt;is_Load()) {
</pre>
<hr />
<pre>
3364         memnode_worklist.append_if_missing(use);
3365       } else if (use-&gt;is_Mem() &amp;&amp; use-&gt;in(MemNode::Memory) == n) {
3366         if (use-&gt;Opcode() == Op_StoreCM) // Ignore cardmark stores
3367           continue;
3368         memnode_worklist.append_if_missing(use);
3369       } else if (use-&gt;is_MemBar()) {
3370         if (use-&gt;in(TypeFunc::Memory) == n) { // Ignore precedent edge
3371           memnode_worklist.append_if_missing(use);
3372         }
3373 #ifdef ASSERT
3374       } else if (use-&gt;is_Mem()) {
3375         assert(use-&gt;in(MemNode::Memory) != n, &quot;EA: missing memory path&quot;);
3376       } else if (use-&gt;is_MergeMem()) {
3377         assert(_mergemem_worklist.contains(use-&gt;as_MergeMem()), &quot;EA: missing MergeMem node in the worklist&quot;);
3378       } else if (use-&gt;Opcode() == Op_EncodeISOArray) {
3379         if (use-&gt;in(MemNode::Memory) == n || use-&gt;in(3) == n) {
3380           // EncodeISOArray overwrites destination array
3381           memnode_worklist.append_if_missing(use);
3382         }
3383       } else if (use-&gt;is_CallLeaf() &amp;&amp; use-&gt;as_CallLeaf()-&gt;_name != NULL &amp;&amp;
<span class="line-modified">3384                  strcmp(use-&gt;as_CallLeaf()-&gt;_name, &quot;store_unknown_inline&quot;) == 0) {</span>
<span class="line-modified">3385         // store_unknown_inline overwrites destination array</span>
3386         memnode_worklist.append_if_missing(use);
3387       } else {
3388         uint op = use-&gt;Opcode();
3389         if ((use-&gt;in(MemNode::Memory) == n) &amp;&amp;
3390             (op == Op_StrCompressedCopy || op == Op_StrInflatedCopy)) {
3391           // They overwrite memory edge corresponding to destination array,
3392           memnode_worklist.append_if_missing(use);
3393         } else if (!(BarrierSet::barrier_set()-&gt;barrier_set_c2()-&gt;is_gc_barrier_node(use) ||
3394               op == Op_AryEq || op == Op_StrComp || op == Op_HasNegatives ||
3395               op == Op_StrCompressedCopy || op == Op_StrInflatedCopy ||
3396               op == Op_StrEquals || op == Op_StrIndexOf || op == Op_StrIndexOfChar)) {
3397           n-&gt;dump();
3398           use-&gt;dump();
3399           assert(false, &quot;EA: missing memory path&quot;);
3400         }
3401 #endif
3402       }
3403     }
3404   }
3405 
</pre>
</td>
</tr>
</table>
<center><a href="doCall.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="graphKit.cpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>