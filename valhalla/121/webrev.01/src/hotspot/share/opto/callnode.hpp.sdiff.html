<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/share/opto/callnode.hpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="callnode.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="castnode.cpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/share/opto/callnode.hpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
 628   // the node the JVMState must be cloned. Default is not to clone.
 629   virtual void clone_jvms(Compile* C) {
 630     if (C-&gt;needs_clone_jvms() &amp;&amp; jvms() != NULL) {
 631       set_jvms(jvms()-&gt;clone_deep(C));
 632       jvms()-&gt;set_map_deep(this);
 633     }
 634   }
 635 
 636   // Returns true if the call may modify n
 637   virtual bool        may_modify(const TypeOopPtr *t_oop, PhaseTransform *phase);
 638   // Does this node have a use of n other than in debug information?
 639   bool                has_non_debug_use(Node *n);
 640   bool                has_debug_use(Node *n);
 641   // Returns the unique CheckCastPP of a call
 642   // or result projection is there are several CheckCastPP
 643   // or returns NULL if there is no one.
 644   Node *result_cast();
 645   // Does this node returns pointer?
 646   bool returns_pointer() const {
 647     const TypeTuple *r = tf()-&gt;range_sig();
<span class="line-modified"> 648     return (!tf()-&gt;returns_value_type_as_fields() &amp;&amp;</span>
 649             r-&gt;cnt() &gt; TypeFunc::Parms &amp;&amp;
 650             r-&gt;field_at(TypeFunc::Parms)-&gt;isa_ptr());
 651   }
 652 
 653   // Collect all the interesting edges from a call for use in
 654   // replacing the call by something else.  Used by macro expansion
 655   // and the late inlining support.
 656   CallProjections* extract_projections(bool separate_io_proj, bool do_asserts = true);
 657 
 658   virtual uint match_edge(uint idx) const;
 659 
 660   bool is_call_to_arraycopystub() const;
 661 
 662   virtual void copy_call_debug_info(PhaseIterGVN* phase, CallNode *oldcall) {}
 663 
 664 #ifndef PRODUCT
 665   virtual void        dump_req(outputStream *st = tty) const;
 666   virtual void        dump_spec(outputStream *st) const;
 667 #endif
 668 };
</pre>
<hr />
<pre>
 721 class CallStaticJavaNode : public CallJavaNode {
 722   virtual bool cmp( const Node &amp;n ) const;
 723   virtual uint size_of() const; // Size is bigger
 724 
 725   bool remove_useless_allocation(PhaseGVN *phase, Node* ctl, Node* mem, Node* unc_arg);
 726 
 727 public:
 728   CallStaticJavaNode(Compile* C, const TypeFunc* tf, address addr, ciMethod* method, int bci)
 729     : CallJavaNode(tf, addr, method, bci) {
 730     init_class_id(Class_CallStaticJava);
 731     if (C-&gt;eliminate_boxing() &amp;&amp; (method != NULL) &amp;&amp; method-&gt;is_boxing_method()) {
 732       init_flags(Flag_is_macro);
 733       C-&gt;add_macro_node(this);
 734     }
 735     const TypeTuple *r = tf-&gt;range_sig();
 736     if (InlineTypeReturnedAsFields &amp;&amp;
 737         method != NULL &amp;&amp;
 738         method-&gt;is_method_handle_intrinsic() &amp;&amp;
 739         r-&gt;cnt() &gt; TypeFunc::Parms &amp;&amp;
 740         r-&gt;field_at(TypeFunc::Parms)-&gt;isa_oopptr() &amp;&amp;
<span class="line-modified"> 741         r-&gt;field_at(TypeFunc::Parms)-&gt;is_oopptr()-&gt;can_be_value_type()) {</span>
 742       // Make sure this call is processed by PhaseMacroExpand::expand_mh_intrinsic_return
 743       init_flags(Flag_is_macro);
 744       C-&gt;add_macro_node(this);
 745     }
 746 
 747     _is_scalar_replaceable = false;
 748     _is_non_escaping = false;
 749   }
 750   CallStaticJavaNode(const TypeFunc* tf, address addr, const char* name, int bci,
 751                      const TypePtr* adr_type)
 752     : CallJavaNode(tf, addr, NULL, bci) {
 753     init_class_id(Class_CallStaticJava);
 754     // This node calls a runtime stub, which often has narrow memory effects.
 755     _adr_type = adr_type;
 756     _is_scalar_replaceable = false;
 757     _is_non_escaping = false;
 758     _name = name;
 759   }
 760 
 761   // Result of Escape Analysis
</pre>
<hr />
<pre>
 862 //------------------------------Allocate---------------------------------------
 863 // High-level memory allocation
 864 //
 865 //  AllocateNode and AllocateArrayNode are subclasses of CallNode because they will
 866 //  get expanded into a code sequence containing a call.  Unlike other CallNodes,
 867 //  they have 2 memory projections and 2 i_o projections (which are distinguished by
 868 //  the _is_io_use flag in the projection.)  This is needed when expanding the node in
 869 //  order to differentiate the uses of the projection on the normal control path from
 870 //  those on the exception return path.
 871 //
 872 class AllocateNode : public CallNode {
 873 public:
 874   enum {
 875     // Output:
 876     RawAddress  = TypeFunc::Parms,    // the newly-allocated raw address
 877     // Inputs:
 878     AllocSize   = TypeFunc::Parms,    // size (in bytes) of the new object
 879     KlassNode,                        // type (maybe dynamic) of the obj.
 880     InitialTest,                      // slow-path test (may be constant)
 881     ALength,                          // array length (or TOP if none)
<span class="line-modified"> 882     ValueNode,</span>
<span class="line-modified"> 883     DefaultValue,                     // default value in case of non flattened value array</span>
 884     RawDefaultValue,                  // same as above but as raw machine word
 885     ParmLimit
 886   };
 887 
 888   static const TypeFunc* alloc_type(const Type* t) {
 889     const Type** fields = TypeTuple::fields(ParmLimit - TypeFunc::Parms);
 890     fields[AllocSize]   = TypeInt::POS;
 891     fields[KlassNode]   = TypeInstPtr::NOTNULL;
 892     fields[InitialTest] = TypeInt::BOOL;
 893     fields[ALength]     = t;  // length (can be a bad length)
<span class="line-modified"> 894     fields[ValueNode]   = Type::BOTTOM;</span>
 895     fields[DefaultValue] = TypeInstPtr::NOTNULL;
 896     fields[RawDefaultValue] = TypeX_X;
 897 
 898     const TypeTuple *domain = TypeTuple::make(ParmLimit, fields);
 899 
 900     // create result type (range)
 901     fields = TypeTuple::fields(1);
 902     fields[TypeFunc::Parms+0] = TypeRawPtr::NOTNULL; // Returned oop
 903 
 904     const TypeTuple *range = TypeTuple::make(TypeFunc::Parms+1, fields);
 905 
 906     return TypeFunc::make(domain, range);
 907   }
 908 
 909   // Result of Escape Analysis
 910   bool _is_scalar_replaceable;
 911   bool _is_non_escaping;
 912   // True when MemBar for new is redundant with MemBar at initialzer exit
 913   bool _is_allocation_MemBar_redundant;
 914   bool _larval;
 915 
 916   virtual uint size_of() const; // Size is bigger
 917   AllocateNode(Compile* C, const TypeFunc *atype, Node *ctrl, Node *mem, Node *abio,
 918                Node *size, Node *klass_node, Node *initial_test,
<span class="line-modified"> 919                ValueTypeBaseNode* value_node = NULL);</span>
 920   // Expansion modifies the JVMState, so we need to clone it
 921   virtual void  clone_jvms(Compile* C) {
 922     if (jvms() != NULL) {
 923       set_jvms(jvms()-&gt;clone_deep(C));
 924       jvms()-&gt;set_map_deep(this);
 925     }
 926   }
 927   virtual int Opcode() const;
 928   virtual uint ideal_reg() const { return Op_RegP; }
 929   virtual bool        guaranteed_safepoint()  { return false; }
 930 
 931   // allocations do not modify their arguments
 932   virtual bool        may_modify(const TypeOopPtr *t_oop, PhaseTransform *phase) { return false;}
 933 
 934   // Pattern-match a possible usage of AllocateNode.
 935   // Return null if no allocation is recognized.
 936   // The operand is the pointer produced by the (possible) allocation.
 937   // It must be a projection of the Allocate or its subsequent CastPP.
 938   // (Note:  This function is defined in file graphKit.cpp, near
 939   // GraphKit::new_instance/new_array, whose output it recognizes.)
</pre>
</td>
<td>
<hr />
<pre>
 628   // the node the JVMState must be cloned. Default is not to clone.
 629   virtual void clone_jvms(Compile* C) {
 630     if (C-&gt;needs_clone_jvms() &amp;&amp; jvms() != NULL) {
 631       set_jvms(jvms()-&gt;clone_deep(C));
 632       jvms()-&gt;set_map_deep(this);
 633     }
 634   }
 635 
 636   // Returns true if the call may modify n
 637   virtual bool        may_modify(const TypeOopPtr *t_oop, PhaseTransform *phase);
 638   // Does this node have a use of n other than in debug information?
 639   bool                has_non_debug_use(Node *n);
 640   bool                has_debug_use(Node *n);
 641   // Returns the unique CheckCastPP of a call
 642   // or result projection is there are several CheckCastPP
 643   // or returns NULL if there is no one.
 644   Node *result_cast();
 645   // Does this node returns pointer?
 646   bool returns_pointer() const {
 647     const TypeTuple *r = tf()-&gt;range_sig();
<span class="line-modified"> 648     return (!tf()-&gt;returns_inline_type_as_fields() &amp;&amp;</span>
 649             r-&gt;cnt() &gt; TypeFunc::Parms &amp;&amp;
 650             r-&gt;field_at(TypeFunc::Parms)-&gt;isa_ptr());
 651   }
 652 
 653   // Collect all the interesting edges from a call for use in
 654   // replacing the call by something else.  Used by macro expansion
 655   // and the late inlining support.
 656   CallProjections* extract_projections(bool separate_io_proj, bool do_asserts = true);
 657 
 658   virtual uint match_edge(uint idx) const;
 659 
 660   bool is_call_to_arraycopystub() const;
 661 
 662   virtual void copy_call_debug_info(PhaseIterGVN* phase, CallNode *oldcall) {}
 663 
 664 #ifndef PRODUCT
 665   virtual void        dump_req(outputStream *st = tty) const;
 666   virtual void        dump_spec(outputStream *st) const;
 667 #endif
 668 };
</pre>
<hr />
<pre>
 721 class CallStaticJavaNode : public CallJavaNode {
 722   virtual bool cmp( const Node &amp;n ) const;
 723   virtual uint size_of() const; // Size is bigger
 724 
 725   bool remove_useless_allocation(PhaseGVN *phase, Node* ctl, Node* mem, Node* unc_arg);
 726 
 727 public:
 728   CallStaticJavaNode(Compile* C, const TypeFunc* tf, address addr, ciMethod* method, int bci)
 729     : CallJavaNode(tf, addr, method, bci) {
 730     init_class_id(Class_CallStaticJava);
 731     if (C-&gt;eliminate_boxing() &amp;&amp; (method != NULL) &amp;&amp; method-&gt;is_boxing_method()) {
 732       init_flags(Flag_is_macro);
 733       C-&gt;add_macro_node(this);
 734     }
 735     const TypeTuple *r = tf-&gt;range_sig();
 736     if (InlineTypeReturnedAsFields &amp;&amp;
 737         method != NULL &amp;&amp;
 738         method-&gt;is_method_handle_intrinsic() &amp;&amp;
 739         r-&gt;cnt() &gt; TypeFunc::Parms &amp;&amp;
 740         r-&gt;field_at(TypeFunc::Parms)-&gt;isa_oopptr() &amp;&amp;
<span class="line-modified"> 741         r-&gt;field_at(TypeFunc::Parms)-&gt;is_oopptr()-&gt;can_be_inline_type()) {</span>
 742       // Make sure this call is processed by PhaseMacroExpand::expand_mh_intrinsic_return
 743       init_flags(Flag_is_macro);
 744       C-&gt;add_macro_node(this);
 745     }
 746 
 747     _is_scalar_replaceable = false;
 748     _is_non_escaping = false;
 749   }
 750   CallStaticJavaNode(const TypeFunc* tf, address addr, const char* name, int bci,
 751                      const TypePtr* adr_type)
 752     : CallJavaNode(tf, addr, NULL, bci) {
 753     init_class_id(Class_CallStaticJava);
 754     // This node calls a runtime stub, which often has narrow memory effects.
 755     _adr_type = adr_type;
 756     _is_scalar_replaceable = false;
 757     _is_non_escaping = false;
 758     _name = name;
 759   }
 760 
 761   // Result of Escape Analysis
</pre>
<hr />
<pre>
 862 //------------------------------Allocate---------------------------------------
 863 // High-level memory allocation
 864 //
 865 //  AllocateNode and AllocateArrayNode are subclasses of CallNode because they will
 866 //  get expanded into a code sequence containing a call.  Unlike other CallNodes,
 867 //  they have 2 memory projections and 2 i_o projections (which are distinguished by
 868 //  the _is_io_use flag in the projection.)  This is needed when expanding the node in
 869 //  order to differentiate the uses of the projection on the normal control path from
 870 //  those on the exception return path.
 871 //
 872 class AllocateNode : public CallNode {
 873 public:
 874   enum {
 875     // Output:
 876     RawAddress  = TypeFunc::Parms,    // the newly-allocated raw address
 877     // Inputs:
 878     AllocSize   = TypeFunc::Parms,    // size (in bytes) of the new object
 879     KlassNode,                        // type (maybe dynamic) of the obj.
 880     InitialTest,                      // slow-path test (may be constant)
 881     ALength,                          // array length (or TOP if none)
<span class="line-modified"> 882     InlineTypeNode,                   // InlineTypeNode if this is an inline type allocation</span>
<span class="line-modified"> 883     DefaultValue,                     // default value in case of non-flattened inline array</span>
 884     RawDefaultValue,                  // same as above but as raw machine word
 885     ParmLimit
 886   };
 887 
 888   static const TypeFunc* alloc_type(const Type* t) {
 889     const Type** fields = TypeTuple::fields(ParmLimit - TypeFunc::Parms);
 890     fields[AllocSize]   = TypeInt::POS;
 891     fields[KlassNode]   = TypeInstPtr::NOTNULL;
 892     fields[InitialTest] = TypeInt::BOOL;
 893     fields[ALength]     = t;  // length (can be a bad length)
<span class="line-modified"> 894     fields[InlineTypeNode] = Type::BOTTOM;</span>
 895     fields[DefaultValue] = TypeInstPtr::NOTNULL;
 896     fields[RawDefaultValue] = TypeX_X;
 897 
 898     const TypeTuple *domain = TypeTuple::make(ParmLimit, fields);
 899 
 900     // create result type (range)
 901     fields = TypeTuple::fields(1);
 902     fields[TypeFunc::Parms+0] = TypeRawPtr::NOTNULL; // Returned oop
 903 
 904     const TypeTuple *range = TypeTuple::make(TypeFunc::Parms+1, fields);
 905 
 906     return TypeFunc::make(domain, range);
 907   }
 908 
 909   // Result of Escape Analysis
 910   bool _is_scalar_replaceable;
 911   bool _is_non_escaping;
 912   // True when MemBar for new is redundant with MemBar at initialzer exit
 913   bool _is_allocation_MemBar_redundant;
 914   bool _larval;
 915 
 916   virtual uint size_of() const; // Size is bigger
 917   AllocateNode(Compile* C, const TypeFunc *atype, Node *ctrl, Node *mem, Node *abio,
 918                Node *size, Node *klass_node, Node *initial_test,
<span class="line-modified"> 919                InlineTypeBaseNode* inline_type_node = NULL);</span>
 920   // Expansion modifies the JVMState, so we need to clone it
 921   virtual void  clone_jvms(Compile* C) {
 922     if (jvms() != NULL) {
 923       set_jvms(jvms()-&gt;clone_deep(C));
 924       jvms()-&gt;set_map_deep(this);
 925     }
 926   }
 927   virtual int Opcode() const;
 928   virtual uint ideal_reg() const { return Op_RegP; }
 929   virtual bool        guaranteed_safepoint()  { return false; }
 930 
 931   // allocations do not modify their arguments
 932   virtual bool        may_modify(const TypeOopPtr *t_oop, PhaseTransform *phase) { return false;}
 933 
 934   // Pattern-match a possible usage of AllocateNode.
 935   // Return null if no allocation is recognized.
 936   // The operand is the pointer produced by the (possible) allocation.
 937   // It must be a projection of the Allocate or its subsequent CastPP.
 938   // (Note:  This function is defined in file graphKit.cpp, near
 939   // GraphKit::new_instance/new_array, whose output it recognizes.)
</pre>
</td>
</tr>
</table>
<center><a href="callnode.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="castnode.cpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>