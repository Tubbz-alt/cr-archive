diff a/src/hotspot/cpu/aarch64/c1_MacroAssembler_aarch64.cpp b/src/hotspot/cpu/aarch64/c1_MacroAssembler_aarch64.cpp
--- a/src/hotspot/cpu/aarch64/c1_MacroAssembler_aarch64.cpp
+++ b/src/hotspot/cpu/aarch64/c1_MacroAssembler_aarch64.cpp
@@ -379,11 +379,11 @@
   nop();
   // build frame
   // verify_FPU(0, "method_entry");
 }
 
-int C1_MacroAssembler::scalarized_entry(const CompiledEntrySignature *ces, int frame_size_in_bytes, int bang_size_in_bytes, Label& verified_inline_entry_label, bool is_inline_ro_entry) {
+int C1_MacroAssembler::scalarized_entry(const CompiledEntrySignature* ces, int frame_size_in_bytes, int bang_size_in_bytes, Label& verified_inline_entry_label, bool is_inline_ro_entry) {
   // This function required to support for InlineTypePassFieldsAsArgs
   if (C1Breakpoint || VerifyFPU || !UseStackBanging) {
     // Verified Entry first instruction should be 5 bytes long for correct
     // patching by patch_verified_entry().
     //
@@ -443,12 +443,12 @@
 
   // Remove the temp frame
   add(sp, sp, frame_size_in_bytes);
 
   int n = shuffle_inline_args(true, is_inline_ro_entry, extra_stack_offset, sig_bt, sig_cc,
-                             args_passed_cc, args_on_stack_cc, regs_cc, // from
-                             args_passed, args_on_stack, regs);         // to
+                              args_passed_cc, args_on_stack_cc, regs_cc, // from
+                              args_passed, args_on_stack, regs);         // to
   assert(sp_inc == n, "must be");
 
   if (sp_inc != 0) {
     // Do the stack banging here, and skip over the stack repair code in the
     // verified_inline_entry (which has a different real_frame_size).
diff a/src/hotspot/cpu/aarch64/macroAssembler_aarch64.cpp b/src/hotspot/cpu/aarch64/macroAssembler_aarch64.cpp
--- a/src/hotspot/cpu/aarch64/macroAssembler_aarch64.cpp
+++ b/src/hotspot/cpu/aarch64/macroAssembler_aarch64.cpp
@@ -5408,11 +5408,11 @@
   return true;
 }
 
 // Read all fields from an inline type oop and store the values in registers/stack slots
 bool MacroAssembler::unpack_inline_helper(const GrowableArray<SigEntry>* sig, int& sig_index, VMReg from, VMRegPair* regs_to,
-                                         int& to_index, RegState reg_state[], int ret_off, int extra_stack_offset) {
+                                          int& to_index, RegState reg_state[], int ret_off, int extra_stack_offset) {
   Register fromReg = from->is_reg() ? from->as_Register() : noreg;
   assert(sig->at(sig_index)._bt == T_VOID, "should be at end delimiter");
 
 
   int vt = 1;
@@ -5496,12 +5496,12 @@
   return done;
 }
 
 // Pack fields back into an inline type oop
 bool MacroAssembler::pack_inline_helper(const GrowableArray<SigEntry>* sig, int& sig_index, int vtarg_index,
-                                       VMReg to, VMRegPair* regs_from, int regs_from_count, int& from_index, RegState reg_state[],
-                                       int ret_off, int extra_stack_offset) {
+                                        VMReg to, VMRegPair* regs_from, int regs_from_count, int& from_index, RegState reg_state[],
+                                        int ret_off, int extra_stack_offset) {
   assert(sig->at(sig_index)._bt == T_INLINE_TYPE, "should be at end delimiter");
   assert(to->is_valid(), "must be");
 
   if (reg_state[to->value()] == reg_written) {
     skip_unpacked_fields(sig, sig_index, regs_from, regs_from_count, from_index);
@@ -5585,13 +5585,13 @@
   // Emit code for verified entry and save increment for stack repair on return
   verified_entry(C, sp_inc);
 }
 
 int MacroAssembler::shuffle_inline_args(bool is_packing, bool receiver_only, int extra_stack_offset,
-                                       BasicType* sig_bt, const GrowableArray<SigEntry>* sig_cc,
-                                       int args_passed, int args_on_stack, VMRegPair* regs,            // from
-                                       int args_passed_to, int args_on_stack_to, VMRegPair* regs_to) { // to
+                                        BasicType* sig_bt, const GrowableArray<SigEntry>* sig_cc,
+                                        int args_passed, int args_on_stack, VMRegPair* regs,            // from
+                                        int args_passed_to, int args_on_stack_to, VMRegPair* regs_to) { // to
   // Check if we need to extend the stack for packing/unpacking
   int sp_inc = (args_on_stack_to - args_on_stack) * VMRegImpl::stack_slot_size;
   if (sp_inc > 0) {
     sp_inc = align_up(sp_inc, StackAlignmentInBytes);
     if (!is_packing) {
@@ -5618,14 +5618,14 @@
     // C2 code ensures that sp_inc is a reserved slot.
     ret_off = sp_inc;
   }
 
   return shuffle_inline_args_common(is_packing, receiver_only, extra_stack_offset,
-                                   sig_bt, sig_cc,
-                                   args_passed, args_on_stack, regs,
-                                   args_passed_to, args_on_stack_to, regs_to,
-                                   sp_inc, ret_off);
+                                    sig_bt, sig_cc,
+                                    args_passed, args_on_stack, regs,
+                                    args_passed_to, args_on_stack_to, regs_to,
+                                    sp_inc, ret_off);
 }
 
 VMReg MacroAssembler::spill_reg_for(VMReg reg) {
   return (reg->is_FloatRegister()) ? v0->as_VMReg() : r14->as_VMReg();
 }
diff a/src/hotspot/cpu/aarch64/macroAssembler_aarch64.hpp b/src/hotspot/cpu/aarch64/macroAssembler_aarch64.hpp
--- a/src/hotspot/cpu/aarch64/macroAssembler_aarch64.hpp
+++ b/src/hotspot/cpu/aarch64/macroAssembler_aarch64.hpp
@@ -1203,23 +1203,23 @@
 
 // Unpack all inline type arguments passed as oops
   void unpack_inline_args(Compile* C, bool receiver_only);
   bool move_helper(VMReg from, VMReg to, BasicType bt, RegState reg_state[], int ret_off, int extra_stack_offset);
   bool unpack_inline_helper(const GrowableArray<SigEntry>* sig, int& sig_index, VMReg from, VMRegPair* regs_to, int& to_index,
-                           RegState reg_state[], int ret_off, int extra_stack_offset);
+                            RegState reg_state[], int ret_off, int extra_stack_offset);
   bool pack_inline_helper(const GrowableArray<SigEntry>* sig, int& sig_index, int vtarg_index,
-                         VMReg to, VMRegPair* regs_from, int regs_from_count, int& from_index, RegState reg_state[],
-                         int ret_off, int extra_stack_offset);
+                          VMReg to, VMRegPair* regs_from, int regs_from_count, int& from_index, RegState reg_state[],
+                          int ret_off, int extra_stack_offset);
   void restore_stack(Compile* C);
 
   int shuffle_inline_args(bool is_packing, bool receiver_only, int extra_stack_offset,
-                         BasicType* sig_bt, const GrowableArray<SigEntry>* sig_cc,
-                         int args_passed, int args_on_stack, VMRegPair* regs,
-                         int args_passed_to, int args_on_stack_to, VMRegPair* regs_to);
+                          BasicType* sig_bt, const GrowableArray<SigEntry>* sig_cc,
+                          int args_passed, int args_on_stack, VMRegPair* regs,
+                          int args_passed_to, int args_on_stack_to, VMRegPair* regs_to);
   bool shuffle_inline_args_spill(bool is_packing,  const GrowableArray<SigEntry>* sig_cc, int sig_cc_index,
-                                VMRegPair* regs_from, int from_index, int regs_from_count,
-                                RegState* reg_state, int sp_inc, int extra_stack_offset);
+                                 VMRegPair* regs_from, int from_index, int regs_from_count,
+                                 RegState* reg_state, int sp_inc, int extra_stack_offset);
   VMReg spill_reg_for(VMReg reg);
 
 
   void tableswitch(Register index, jint lowbound, jint highbound,
                    Label &jumptable, Label &jumptable_end, int stride = 1) {
diff a/src/hotspot/cpu/x86/c1_MacroAssembler_x86.cpp b/src/hotspot/cpu/x86/c1_MacroAssembler_x86.cpp
--- a/src/hotspot/cpu/x86/c1_MacroAssembler_x86.cpp
+++ b/src/hotspot/cpu/x86/c1_MacroAssembler_x86.cpp
@@ -382,11 +382,11 @@
   if (C1Breakpoint)int3();
   // build frame
   IA32_ONLY( verify_FPU(0, "method_entry"); )
 }
 
-int C1_MacroAssembler::scalarized_entry(const CompiledEntrySignature *ces, int frame_size_in_bytes, int bang_size_in_bytes, int sp_offset_for_orig_pc, Label& verified_inline_entry_label, bool is_inline_ro_entry) {
+int C1_MacroAssembler::scalarized_entry(const CompiledEntrySignature* ces, int frame_size_in_bytes, int bang_size_in_bytes, int sp_offset_for_orig_pc, Label& verified_inline_entry_label, bool is_inline_ro_entry) {
   assert(InlineTypePassFieldsAsArgs, "sanity");
   // Make sure there is enough stack space for this method's activation.
   assert(bang_size_in_bytes >= frame_size_in_bytes, "stack bang size incorrect");
   generate_stack_overflow_check(bang_size_in_bytes);
 
@@ -432,12 +432,12 @@
   // Remove the temp frame
   addptr(rsp, frame_size_in_bytes);
   pop(rbp);
 
   shuffle_inline_args(true, is_inline_ro_entry, extra_stack_offset, sig_bt, sig_cc,
-                     args_passed_cc, args_on_stack_cc, regs_cc, // from
-                     args_passed, args_on_stack, regs, sp_inc); // to
+                      args_passed_cc, args_on_stack_cc, regs_cc, // from
+                      args_passed, args_on_stack, regs, sp_inc); // to
 
   if (ces->c1_needs_stack_repair()) {
     // Create the real frame. Below jump will then skip over the stack banging and frame
     // setup code in the verified_inline_entry (which has a different real_frame_size).
     build_frame_helper(frame_size_in_bytes, sp_inc, true);
diff a/src/hotspot/cpu/x86/macroAssembler_x86.cpp b/src/hotspot/cpu/x86/macroAssembler_x86.cpp
--- a/src/hotspot/cpu/x86/macroAssembler_x86.cpp
+++ b/src/hotspot/cpu/x86/macroAssembler_x86.cpp
@@ -5347,11 +5347,11 @@
   return true;
 }
 
 // Read all fields from an inline type oop and store the values in registers/stack slots
 bool MacroAssembler::unpack_inline_helper(const GrowableArray<SigEntry>* sig, int& sig_index, VMReg from, VMRegPair* regs_to,
-                                         int& to_index, RegState reg_state[], int ret_off, int extra_stack_offset) {
+                                          int& to_index, RegState reg_state[], int ret_off, int extra_stack_offset) {
   Register fromReg = from->is_reg() ? from->as_Register() : noreg;
   assert(sig->at(sig_index)._bt == T_VOID, "should be at end delimiter");
 
   int vt = 1;
   bool done = true;
@@ -5429,12 +5429,12 @@
   return done;
 }
 
 // Pack fields back into an inline type oop
 bool MacroAssembler::pack_inline_helper(const GrowableArray<SigEntry>* sig, int& sig_index, int vtarg_index,
-                                       VMReg to, VMRegPair* regs_from, int regs_from_count, int& from_index, RegState reg_state[],
-                                       int ret_off, int extra_stack_offset) {
+                                        VMReg to, VMRegPair* regs_from, int regs_from_count, int& from_index, RegState reg_state[],
+                                        int ret_off, int extra_stack_offset) {
   assert(sig->at(sig_index)._bt == T_INLINE_TYPE, "should be at end delimiter");
   assert(to->is_valid(), "must be");
 
   if (reg_state[to->value()] == reg_written) {
     skip_unpacked_fields(sig, sig_index, regs_from, regs_from_count, from_index);
@@ -5514,13 +5514,13 @@
   // Emit code for verified entry and save increment for stack repair on return
   verified_entry(C, sp_inc);
 }
 
 void MacroAssembler::shuffle_inline_args(bool is_packing, bool receiver_only, int extra_stack_offset,
-                                        BasicType* sig_bt, const GrowableArray<SigEntry>* sig_cc,
-                                        int args_passed, int args_on_stack, VMRegPair* regs,
-                                        int args_passed_to, int args_on_stack_to, VMRegPair* regs_to, int sp_inc) {
+                                         BasicType* sig_bt, const GrowableArray<SigEntry>* sig_cc,
+                                         int args_passed, int args_on_stack, VMRegPair* regs,
+                                         int args_passed_to, int args_on_stack_to, VMRegPair* regs_to, int sp_inc) {
   // Check if we need to extend the stack for packing/unpacking
   if (sp_inc > 0 && !is_packing) {
     // Save the return address, adjust the stack (make sure it is properly
     // 16-byte aligned) and copy the return address to the new top of the stack.
     // (Note: C1 does this in C1_MacroAssembler::scalarized_entry).
@@ -5538,14 +5538,14 @@
     // C2 code ensures that sp_inc is a reserved slot.
     ret_off = sp_inc;
   }
 
   shuffle_inline_args_common(is_packing, receiver_only, extra_stack_offset,
-                            sig_bt, sig_cc,
-                            args_passed, args_on_stack, regs,
-                            args_passed_to, args_on_stack_to, regs_to,
-                            sp_inc, ret_off);
+                             sig_bt, sig_cc,
+                             args_passed, args_on_stack, regs,
+                             args_passed_to, args_on_stack_to, regs_to,
+                             sp_inc, ret_off);
 }
 
 VMReg MacroAssembler::spill_reg_for(VMReg reg) {
   return reg->is_XMMRegister() ? xmm8->as_VMReg() : r14->as_VMReg();
 }
diff a/src/hotspot/cpu/x86/macroAssembler_x86.hpp b/src/hotspot/cpu/x86/macroAssembler_x86.hpp
--- a/src/hotspot/cpu/x86/macroAssembler_x86.hpp
+++ b/src/hotspot/cpu/x86/macroAssembler_x86.hpp
@@ -1658,23 +1658,23 @@
 
   // Unpack all inline type arguments passed as oops
   void unpack_inline_args(Compile* C, bool receiver_only);
   bool move_helper(VMReg from, VMReg to, BasicType bt, RegState reg_state[], int ret_off, int extra_stack_offset);
   bool unpack_inline_helper(const GrowableArray<SigEntry>* sig, int& sig_index, VMReg from, VMRegPair* regs_to, int& to_index,
-                           RegState reg_state[], int ret_off, int extra_stack_offset);
+                            RegState reg_state[], int ret_off, int extra_stack_offset);
   bool pack_inline_helper(const GrowableArray<SigEntry>* sig, int& sig_index, int vtarg_index,
-                         VMReg to, VMRegPair* regs_from, int regs_from_count, int& from_index, RegState reg_state[],
-                         int ret_off, int extra_stack_offset);
+                          VMReg to, VMRegPair* regs_from, int regs_from_count, int& from_index, RegState reg_state[],
+                          int ret_off, int extra_stack_offset);
   void remove_frame(int initial_framesize, bool needs_stack_repair, int sp_inc_offset);
 
   void shuffle_inline_args(bool is_packing, bool receiver_only, int extra_stack_offset,
-                          BasicType* sig_bt, const GrowableArray<SigEntry>* sig_cc,
-                          int args_passed, int args_on_stack, VMRegPair* regs,
-                          int args_passed_to, int args_on_stack_to, VMRegPair* regs_to, int sp_inc);
+                           BasicType* sig_bt, const GrowableArray<SigEntry>* sig_cc,
+                           int args_passed, int args_on_stack, VMRegPair* regs,
+                           int args_passed_to, int args_on_stack_to, VMRegPair* regs_to, int sp_inc);
   bool shuffle_inline_args_spill(bool is_packing,  const GrowableArray<SigEntry>* sig_cc, int sig_cc_index,
-                                VMRegPair* regs_from, int from_index, int regs_from_count,
-                                RegState* reg_state, int sp_inc, int extra_stack_offset);
+                                 VMRegPair* regs_from, int from_index, int regs_from_count,
+                                 RegState* reg_state, int sp_inc, int extra_stack_offset);
   VMReg spill_reg_for(VMReg reg);
 
   // clear memory of size 'cnt' qwords, starting at 'base';
   // if 'is_large' is set, do not try to produce short loop
   void clear_mem(Register base, Register cnt, Register val, XMMRegister xtmp, bool is_large, bool word_copy_only);
diff a/src/hotspot/share/asm/macroAssembler_common.cpp b/src/hotspot/share/asm/macroAssembler_common.cpp
--- a/src/hotspot/share/asm/macroAssembler_common.cpp
+++ b/src/hotspot/share/asm/macroAssembler_common.cpp
@@ -172,20 +172,20 @@
   if (args_on_stack_cc > args_on_stack) {
     sp_inc = (args_on_stack_cc - args_on_stack) * VMRegImpl::stack_slot_size;
     sp_inc = align_up(sp_inc, StackAlignmentInBytes);
   }
   shuffle_inline_args(false, receiver_only, extra_stack_offset, sig_bt, sig_cc,
-                     args_passed, args_on_stack, regs,
-                     args_passed_cc, args_on_stack_cc, regs_cc, sp_inc);
+                      args_passed, args_on_stack, regs,
+                      args_passed_cc, args_on_stack_cc, regs_cc, sp_inc);
   return sp_inc;
 }
 
 void MacroAssembler::shuffle_inline_args_common(bool is_packing, bool receiver_only, int extra_stack_offset,
-                                               BasicType* sig_bt, const GrowableArray<SigEntry>* sig_cc,
-                                               int args_passed, int args_on_stack, VMRegPair* regs,
-                                               int args_passed_to, int args_on_stack_to, VMRegPair* regs_to,
-                                               int sp_inc, int ret_off) {
+                                                BasicType* sig_bt, const GrowableArray<SigEntry>* sig_cc,
+                                                int args_passed, int args_on_stack, VMRegPair* regs,
+                                                int args_passed_to, int args_on_stack_to, VMRegPair* regs_to,
+                                                int sp_inc, int ret_off) {
   int max_stack = MAX2(args_on_stack + sp_inc/VMRegImpl::stack_slot_size, args_on_stack_to);
   RegState* reg_state = init_reg_state(is_packing, sig_cc, regs, args_passed, sp_inc, max_stack);
 
   // Emit code for packing/unpacking inline type arguments
   // We try multiple times and eventually start spilling to resolve (circular) dependencies
@@ -212,22 +212,22 @@
         assert(0 <= from_index && from_index < args_passed, "index out of bounds");
         assert(0 <= to_index && to_index < args_passed_to, "index out of bounds");
         if (spill) {
           // This call returns true IFF we should keep trying to spill in this round.
           spill = shuffle_inline_args_spill(is_packing, sig_cc, sig_index, regs, from_index, args_passed,
-                                           reg_state, ret_off, extra_stack_offset);
+                                            reg_state, ret_off, extra_stack_offset);
         }
         BasicType bt = sig_cc->at(sig_index)._bt;
         if (SigEntry::skip_value_delimiters(sig_cc, sig_index)) {
           VMReg from_reg = regs[from_index].first();
           done &= move_helper(from_reg, regs_to[to_index].first(), bt, reg_state, ret_off, extra_stack_offset);
           to_index += step;
         } else if (is_packing || !receiver_only || (from_index == 0 && bt == T_VOID)) {
           if (is_packing) {
             VMReg reg_to = regs_to[to_index].first();
             done &= pack_inline_helper(sig_cc, sig_index, vtarg_index, reg_to, regs, args_passed, from_index,
-                                      reg_state, ret_off, extra_stack_offset);
+                                       reg_state, ret_off, extra_stack_offset);
             vtarg_index ++;
             to_index ++;
             continue; // from_index already adjusted
           } else {
             VMReg from_reg = regs[from_index].first();
@@ -242,12 +242,12 @@
   }
   guarantee(done, "Could not resolve circular dependency when shuffling inline type arguments");
 }
 
 bool MacroAssembler::shuffle_inline_args_spill(bool is_packing, const GrowableArray<SigEntry>* sig_cc, int sig_cc_index,
-                                              VMRegPair* regs_from, int from_index, int regs_from_count,
-                                              RegState* reg_state, int ret_off, int extra_stack_offset) {
+                                               VMRegPair* regs_from, int from_index, int regs_from_count,
+                                               RegState* reg_state, int ret_off, int extra_stack_offset) {
   VMReg reg;
 
   if (!is_packing || SigEntry::skip_value_delimiters(sig_cc, sig_cc_index)) {
     reg = regs_from[from_index].first();
     if (!reg->is_valid() || reg_state[reg->value()] != reg_readonly) {
diff a/src/hotspot/share/asm/macroAssembler_common.hpp b/src/hotspot/share/asm/macroAssembler_common.hpp
--- a/src/hotspot/share/asm/macroAssembler_common.hpp
+++ b/src/hotspot/share/asm/macroAssembler_common.hpp
@@ -39,13 +39,13 @@
   RegState* init_reg_state(bool is_packing, const GrowableArray<SigEntry>* sig_cc,
                            VMRegPair* regs, int num_regs, int sp_inc, int max_stack);
 
   int unpack_inline_args_common(Compile* C, bool receiver_only);
   void shuffle_inline_args_common(bool is_packing, bool receiver_only, int extra_stack_offset,
-                                 BasicType* sig_bt, const GrowableArray<SigEntry>* sig_cc,
-                                 int args_passed, int args_on_stack, VMRegPair* regs,
-                                 int args_passed_to, int args_on_stack_to, VMRegPair* regs_to,
-                                 int sp_inc, int ret_off);
+                                  BasicType* sig_bt, const GrowableArray<SigEntry>* sig_cc,
+                                  int args_passed, int args_on_stack, VMRegPair* regs,
+                                  int args_passed_to, int args_on_stack_to, VMRegPair* regs_to,
+                                  int sp_inc, int ret_off);
 
 // };
 
 #endif // SHARE_ASM_MACROASSEMBLER_COMMON_HPP
diff a/src/hotspot/share/c1/c1_GraphBuilder.cpp b/src/hotspot/share/c1/c1_GraphBuilder.cpp
--- a/src/hotspot/share/c1/c1_GraphBuilder.cpp
+++ b/src/hotspot/share/c1/c1_GraphBuilder.cpp
@@ -25,11 +25,10 @@
 #include "precompiled.hpp"
 #include "c1/c1_CFGPrinter.hpp"
 #include "c1/c1_Canonicalizer.hpp"
 #include "c1/c1_Compilation.hpp"
 #include "c1/c1_GraphBuilder.hpp"
-
 #include "c1/c1_InstructionPrinter.hpp"
 #include "ci/ciCallSite.hpp"
 #include "ci/ciField.hpp"
 #include "ci/ciFlatArrayKlass.hpp"
 #include "ci/ciInlineKlass.hpp"
diff a/src/hotspot/share/c1/c1_Instruction.cpp b/src/hotspot/share/c1/c1_Instruction.cpp
--- a/src/hotspot/share/c1/c1_Instruction.cpp
+++ b/src/hotspot/share/c1/c1_Instruction.cpp
@@ -23,11 +23,10 @@
  */
 
 #include "precompiled.hpp"
 #include "c1/c1_IR.hpp"
 #include "c1/c1_Instruction.hpp"
-
 #include "c1/c1_InstructionPrinter.hpp"
 #include "c1/c1_ValueStack.hpp"
 #include "ci/ciFlatArrayKlass.hpp"
 #include "ci/ciInlineKlass.hpp"
 #include "ci/ciObjArrayKlass.hpp"
diff a/src/hotspot/share/c1/c1_InstructionPrinter.cpp b/src/hotspot/share/c1/c1_InstructionPrinter.cpp
--- a/src/hotspot/share/c1/c1_InstructionPrinter.cpp
+++ b/src/hotspot/share/c1/c1_InstructionPrinter.cpp
@@ -22,11 +22,10 @@
  *
  */
 
 #include "precompiled.hpp"
 #include "c1/c1_InstructionPrinter.hpp"
-
 #include "c1/c1_ValueStack.hpp"
 #include "ci/ciArray.hpp"
 #include "ci/ciInlineKlass.hpp"
 #include "ci/ciInstance.hpp"
 #include "ci/ciObject.hpp"
diff a/src/hotspot/share/c1/c1_LIR.cpp b/src/hotspot/share/c1/c1_LIR.cpp
--- a/src/hotspot/share/c1/c1_LIR.cpp
+++ b/src/hotspot/share/c1/c1_LIR.cpp
@@ -23,11 +23,10 @@
  */
 
 #include "precompiled.hpp"
 #include "c1/c1_InstructionPrinter.hpp"
 #include "c1/c1_LIR.hpp"
-
 #include "c1/c1_LIRAssembler.hpp"
 #include "c1/c1_ValueStack.hpp"
 #include "ci/ciInlineKlass.hpp"
 #include "ci/ciInstance.hpp"
 #include "runtime/sharedRuntime.hpp"
diff a/src/hotspot/share/c1/c1_LIRAssembler.cpp b/src/hotspot/share/c1/c1_LIRAssembler.cpp
--- a/src/hotspot/share/c1/c1_LIRAssembler.cpp
+++ b/src/hotspot/share/c1/c1_LIRAssembler.cpp
@@ -26,11 +26,10 @@
 #include "asm/assembler.inline.hpp"
 #include "c1/c1_Compilation.hpp"
 #include "c1/c1_Instruction.hpp"
 #include "c1/c1_InstructionPrinter.hpp"
 #include "c1/c1_LIRAssembler.hpp"
-
 #include "c1/c1_MacroAssembler.hpp"
 #include "c1/c1_ValueStack.hpp"
 #include "ci/ciInlineKlass.hpp"
 #include "ci/ciInstance.hpp"
 #include "gc/shared/barrierSet.hpp"
@@ -622,19 +621,19 @@
 //     VIEP_RO is the same as VEP
 // (4) Methods with inline type receiver and other inline type args
 //     Separate VEP, VIEP and VIEP_RO
 //
 // (1)               (2)                 (3)                    (4)
-// UEP/UVEP:         VEP:                UEP:                   UEP:
+// UEP/UIEP:         VEP:                UEP:                   UEP:
 //   check_icache      pack receiver       check_icache           check_icache
 // VEP/VIEP/VIEP_RO    jump to VIEP      VEP/VIEP_RO:           VIEP_RO:
-//   body            UEP/UVEP:             pack inline args       pack inline args (except receiver)
+//   body            UEP/UIEP:             pack inline args       pack inline args (except receiver)
 //                     check_icache        jump to VIEP           jump to VIEP
-//                   VIEP/VIEP_RO        UVEP:                  VEP:
+//                   VIEP/VIEP_RO        UIEP:                  VEP:
 //                     body                check_icache           pack all inline args
 //                                       VIEP:                    jump to VIEP
-//                                         body                 UVEP:
+//                                         body                 UIEP:
 //                                                                check_icache
 //                                                              VIEP:
 //                                                                body
 void LIR_Assembler::emit_std_entries() {
   offsets()->set_value(CodeOffsets::OSR_Entry, _masm->offset());
@@ -660,16 +659,16 @@
 
     // VEP: pack all value parameters
     _masm->align(CodeEntryAlignment);
     emit_std_entry(CodeOffsets::Verified_Entry, ces);
 
-    // UVEP: check icache and fall-through
+    // UIEP: check icache and fall-through
     _masm->align(CodeEntryAlignment);
     offsets()->set_value(CodeOffsets::Inline_Entry, _masm->offset());
     if (ro_entry_type == CodeOffsets::Verified_Inline_Entry) {
       // Special case if we have VIEP == VIEP(RO):
-      // this means UVEP (called by C1) == UEP (called by C2).
+      // this means UIEP (called by C1) == UEP (called by C2).
       offsets()->set_value(CodeOffsets::Entry, _masm->offset());
     }
     if (needs_icache(method())) {
       check_icache();
     }
diff a/src/hotspot/share/c1/c1_LIRGenerator.cpp b/src/hotspot/share/c1/c1_LIRGenerator.cpp
--- a/src/hotspot/share/c1/c1_LIRGenerator.cpp
+++ b/src/hotspot/share/c1/c1_LIRGenerator.cpp
@@ -27,11 +27,10 @@
 #include "c1/c1_Defs.hpp"
 #include "c1/c1_FrameMap.hpp"
 #include "c1/c1_Instruction.hpp"
 #include "c1/c1_LIRAssembler.hpp"
 #include "c1/c1_LIRGenerator.hpp"
-
 #include "c1/c1_ValueStack.hpp"
 #include "ci/ciArrayKlass.hpp"
 #include "ci/ciFlatArrayKlass.hpp"
 #include "ci/ciInlineKlass.hpp"
 #include "ci/ciInstance.hpp"
diff a/src/hotspot/share/c1/c1_MacroAssembler.hpp b/src/hotspot/share/c1/c1_MacroAssembler.hpp
--- a/src/hotspot/share/c1/c1_MacroAssembler.hpp
+++ b/src/hotspot/share/c1/c1_MacroAssembler.hpp
@@ -30,11 +30,11 @@
 
 class CodeEmitInfo;
 class CompiledEntrySignature;
 class C1_MacroAssembler: public MacroAssembler {
  private:
-  int scalarized_entry(const CompiledEntrySignature *ces, int frame_size_in_bytes, int bang_size_in_bytes, int sp_offset_for_orig_pc, Label& verified_inline_entry_label, bool is_inline_ro_entry);
+  int scalarized_entry(const CompiledEntrySignature* ces, int frame_size_in_bytes, int bang_size_in_bytes, int sp_offset_for_orig_pc, Label& verified_inline_entry_label, bool is_inline_ro_entry);
   void build_frame_helper(int frame_size_in_bytes, int sp_inc, bool needs_stack_repair);
  public:
   // creation
   C1_MacroAssembler(CodeBuffer* code) : MacroAssembler(code) { pd_init(); }
 
@@ -42,14 +42,14 @@
   void explicit_null_check(Register base);
 
   void inline_cache_check(Register receiver, Register iCache);
   void build_frame(int frame_size_in_bytes, int bang_size_in_bytes, int sp_offset_for_orig_pc = 0, bool needs_stack_repair = false, bool has_scalarized_args = false, Label* verified_inline_entry_label = NULL);
 
-  int verified_entry(const CompiledEntrySignature *ces, int frame_size_in_bytes, int bang_size_in_bytes, int sp_offset_for_orig_pc, Label& verified_inline_entry_label) {
+  int verified_entry(const CompiledEntrySignature* ces, int frame_size_in_bytes, int bang_size_in_bytes, int sp_offset_for_orig_pc, Label& verified_inline_entry_label) {
     return scalarized_entry(ces, frame_size_in_bytes, bang_size_in_bytes, sp_offset_for_orig_pc, verified_inline_entry_label, false);
   }
-  int verified_inline_ro_entry(const CompiledEntrySignature *ces, int frame_size_in_bytes, int bang_size_in_bytes, int sp_offset_for_orig_pc, Label& verified_inline_entry_label) {
+  int verified_inline_ro_entry(const CompiledEntrySignature* ces, int frame_size_in_bytes, int bang_size_in_bytes, int sp_offset_for_orig_pc, Label& verified_inline_entry_label) {
     return scalarized_entry(ces, frame_size_in_bytes, bang_size_in_bytes, sp_offset_for_orig_pc, verified_inline_entry_label, true);
   }
   void verified_entry();
   void verify_stack_oop(int offset) PRODUCT_RETURN;
   void verify_not_null_oop(Register r)  PRODUCT_RETURN;
diff a/src/hotspot/share/c1/c1_Runtime1.hpp b/src/hotspot/share/c1/c1_Runtime1.hpp
--- a/src/hotspot/share/c1/c1_Runtime1.hpp
+++ b/src/hotspot/share/c1/c1_Runtime1.hpp
@@ -49,16 +49,16 @@
   stub(new_instance)                 \
   stub(fast_new_instance)            \
   stub(fast_new_instance_init_check) \
   stub(new_type_array)               \
   stub(new_object_array)             \
-  stub(new_flat_array)              \
+  stub(new_flat_array)               \
   stub(new_multi_array)              \
   stub(load_flattened_array)         \
   stub(store_flattened_array)        \
   stub(substitutability_check)       \
-  stub(buffer_inline_args)            \
+  stub(buffer_inline_args)           \
   stub(buffer_inline_args_no_receiver)\
   stub(handle_exception_nofpu)         /* optimized version that does not preserve fpu registers */ \
   stub(handle_exception)             \
   stub(handle_exception_from_callee) \
   stub(throw_array_store_exception)  \
diff a/src/hotspot/share/ci/ciFlatArrayKlass.cpp b/src/hotspot/share/ci/ciFlatArrayKlass.cpp
--- a/src/hotspot/share/ci/ciFlatArrayKlass.cpp
+++ b/src/hotspot/share/ci/ciFlatArrayKlass.cpp
@@ -20,11 +20,10 @@
  * or visit www.oracle.com if you need additional information or have any
  * questions.
  *
  */
 
-
 #include "precompiled.hpp"
 #include "ci/ciFlatArrayKlass.hpp"
 #include "ci/ciInlineKlass.hpp"
 #include "ci/ciInstanceKlass.hpp"
 #include "ci/ciSymbol.hpp"
diff a/src/hotspot/share/code/nmethod.cpp b/src/hotspot/share/code/nmethod.cpp
--- a/src/hotspot/share/code/nmethod.cpp
+++ b/src/hotspot/share/code/nmethod.cpp
@@ -808,13 +808,13 @@
 #else
     _nmethod_end_offset      = _nul_chk_table_offset + align_up(nul_chk_table->size_in_bytes(), oopSize);
 #endif
     _entry_point             = code_begin()          + offsets->value(CodeOffsets::Entry);
     _verified_entry_point    = code_begin()          + offsets->value(CodeOffsets::Verified_Entry);
-    _inline_entry_point       = code_begin()          + offsets->value(CodeOffsets::Inline_Entry);
-    _verified_inline_entry_point = code_begin()       + offsets->value(CodeOffsets::Verified_Inline_Entry);
-    _verified_inline_ro_entry_point = code_begin()    + offsets->value(CodeOffsets::Verified_Inline_Entry_RO);
+    _inline_entry_point       = code_begin()         + offsets->value(CodeOffsets::Inline_Entry);
+    _verified_inline_entry_point = code_begin()      + offsets->value(CodeOffsets::Verified_Inline_Entry);
+    _verified_inline_ro_entry_point = code_begin()   + offsets->value(CodeOffsets::Verified_Inline_Entry_RO);
     _osr_entry_point         = code_begin()          + offsets->value(CodeOffsets::OSR_Entry);
     _exception_cache         = NULL;
     _scopes_data_begin       = (address) this + scopes_data_offset;
 
     _pc_desc_container.reset_to(scopes_pcs_begin());
@@ -3090,14 +3090,14 @@
 
 const char* nmethod::nmethod_section_label(address pos) const {
   const char* label = NULL;
   if (pos == code_begin())                                              label = "[Instructions begin]";
   if (pos == entry_point())                                             label = "[Entry Point]";
-  if (pos == inline_entry_point())                                       label = "[Value Entry Point]";
+  if (pos == inline_entry_point())                                      label = "[Value Entry Point]";
   if (pos == verified_entry_point())                                    label = "[Verified Entry Point]";
-  if (pos == verified_inline_entry_point())                              label = "[Verified Value Entry Point]";
-  if (pos == verified_inline_ro_entry_point())                           label = "[Verified Value Entry Point (RO)]";
+  if (pos == verified_inline_entry_point())                             label = "[Verified Value Entry Point]";
+  if (pos == verified_inline_ro_entry_point())                          label = "[Verified Value Entry Point (RO)]";
   if (has_method_handle_invokes() && (pos == deopt_mh_handler_begin())) label = "[Deopt MH Handler Code]";
   if (pos == consts_begin() && pos != insts_begin())                    label = "[Constants]";
   // Check stub_code before checking exception_handler or deopt_handler.
   if (pos == this->stub_begin())                                        label = "[Stub Code]";
   if (JVMCI_ONLY(_exception_offset >= 0 &&) pos == exception_begin())           label = "[Exception Handler]";
@@ -3117,13 +3117,13 @@
 
 void nmethod::print_nmethod_labels(outputStream* stream, address block_begin, bool print_section_labels) const {
   if (print_section_labels) {
     int n = 0;
     // Multiple entry points may be at the same position. Print them all.
-    n += maybe_print_entry_label(stream, block_begin, entry_point(),                   "[Entry Point]");
+    n += maybe_print_entry_label(stream, block_begin, entry_point(),                    "[Entry Point]");
     n += maybe_print_entry_label(stream, block_begin, inline_entry_point(),             "[Value Entry Point]");
-    n += maybe_print_entry_label(stream, block_begin, verified_entry_point(),          "[Verified Entry Point]");
+    n += maybe_print_entry_label(stream, block_begin, verified_entry_point(),           "[Verified Entry Point]");
     n += maybe_print_entry_label(stream, block_begin, verified_inline_entry_point(),    "[Verified Value Entry Point]");
     n += maybe_print_entry_label(stream, block_begin, verified_inline_ro_entry_point(), "[Verified Value Entry Point (RO)]");
     if (n == 0) {
       const char* label = nmethod_section_label(block_begin);
       if (label != NULL) {
diff a/src/hotspot/share/code/nmethod.hpp b/src/hotspot/share/code/nmethod.hpp
--- a/src/hotspot/share/code/nmethod.hpp
+++ b/src/hotspot/share/code/nmethod.hpp
@@ -450,15 +450,15 @@
   bool metadata_contains     (Metadata** addr) const   { return metadata_begin     () <= addr && addr < metadata_end     (); }
   bool scopes_data_contains  (address addr) const { return scopes_data_begin  () <= addr && addr < scopes_data_end  (); }
   bool scopes_pcs_contains   (PcDesc* addr) const { return scopes_pcs_begin   () <= addr && addr < scopes_pcs_end   (); }
 
   // entry points
-  address entry_point() const                     { return _entry_point;             }       // normal entry point
-  address verified_entry_point() const            { return _verified_entry_point;    }       // normal entry point without class check
-  address inline_entry_point() const               { return _inline_entry_point; }             // inline type entry point (unpack all inline type args)
-  address verified_inline_entry_point() const      { return _verified_inline_entry_point; }    // inline type entry point (unpack all inline type args) without class check
-  address verified_inline_ro_entry_point() const   { return _verified_inline_ro_entry_point; } // inline type entry point (only unpack receiver) without class check
+  address entry_point() const                     { return _entry_point;             }        // normal entry point
+  address verified_entry_point() const            { return _verified_entry_point;    }        // normal entry point without class check
+  address inline_entry_point() const              { return _inline_entry_point; }             // inline type entry point (unpack all inline type args)
+  address verified_inline_entry_point() const     { return _verified_inline_entry_point; }    // inline type entry point (unpack all inline type args) without class check
+  address verified_inline_ro_entry_point() const  { return _verified_inline_ro_entry_point; } // inline type entry point (only unpack receiver) without class check
 
   // flag accessing and manipulation
   bool  is_not_installed() const                  { return _state == not_installed; }
   bool  is_in_use() const                         { return _state <= in_use; }
   bool  is_alive() const                          { return _state < unloaded; }
diff a/src/hotspot/share/oops/method.hpp b/src/hotspot/share/oops/method.hpp
--- a/src/hotspot/share/oops/method.hpp
+++ b/src/hotspot/share/oops/method.hpp
@@ -104,11 +104,11 @@
 #endif
   // Entry point for calling both from and to the interpreter.
   address _i2i_entry;           // All-args-on-stack calling convention
   // Entry point for calling from compiled code, to compiled code if it exists
   // or else the interpreter.
-  volatile address _from_compiled_entry;          // Cache of: _code ? _code->verified_entry_point()          : _adapter->c2i_entry()
+  volatile address _from_compiled_entry;           // Cache of: _code ? _code->verified_entry_point()           : _adapter->c2i_entry()
   volatile address _from_compiled_inline_ro_entry; // Cache of: _code ? _code->verified_inline_ro_entry_point() : _adapter->c2i_inline_ro_entry()
   volatile address _from_compiled_inline_entry;    // Cache of: _code ? _code->verified_inline_entry_point()    : _adapter->c2i_inline_entry()
   // The entry point for calling both from and to compiled code is
   // "_code->entry_point()".  Because of tiered compilation and de-opt, this
   // field can come and go.  It can transition from NULL to not-null at any
diff a/src/hotspot/share/opto/compile.hpp b/src/hotspot/share/opto/compile.hpp
--- a/src/hotspot/share/opto/compile.hpp
+++ b/src/hotspot/share/opto/compile.hpp
@@ -316,11 +316,11 @@
   GrowableArray<Node*>* _macro_nodes;           // List of nodes which need to be expanded before matching.
   GrowableArray<Node*>* _predicate_opaqs;       // List of Opaque1 nodes for the loop predicates.
   GrowableArray<Node*>* _expensive_nodes;       // List of nodes that are expensive to compute and that we'd better not let the GVN freely common
   GrowableArray<Node*>* _range_check_casts;     // List of CastII nodes with a range check dependency
   GrowableArray<Node*>* _opaque4_nodes;         // List of Opaque4 nodes that have a default value
-  GrowableArray<Node*>* _inline_type_nodes;      // List of InlineType nodes
+  GrowableArray<Node*>* _inline_type_nodes;     // List of InlineType nodes
   ConnectionGraph*      _congraph;
 #ifndef PRODUCT
   IdealGraphPrinter*    _printer;
   static IdealGraphPrinter* _debug_file_printer;
   static IdealGraphPrinter* _debug_network_printer;
diff a/src/hotspot/share/opto/library_call.cpp b/src/hotspot/share/opto/library_call.cpp
--- a/src/hotspot/share/opto/library_call.cpp
+++ b/src/hotspot/share/opto/library_call.cpp
@@ -3723,11 +3723,11 @@
     bool query = 0;
     switch(kind) {
       case ObjectArray:    query = Klass::layout_helper_is_objArray(layout_con); break;
       case NonObjectArray: query = !Klass::layout_helper_is_objArray(layout_con); break;
       case TypeArray:      query = Klass::layout_helper_is_typeArray(layout_con); break;
-      case FlatArray:     query = Klass::layout_helper_is_flatArray(layout_con); break;
+      case FlatArray:      query = Klass::layout_helper_is_flatArray(layout_con); break;
       case AnyArray:       query = Klass::layout_helper_is_array(layout_con); break;
       case NonArray:       query = !Klass::layout_helper_is_array(layout_con); break;
       default:
         ShouldNotReachHere();
     }
diff a/src/hotspot/share/opto/macro.cpp b/src/hotspot/share/opto/macro.cpp
--- a/src/hotspot/share/opto/macro.cpp
+++ b/src/hotspot/share/opto/macro.cpp
@@ -1046,11 +1046,11 @@
           }
         }
         _igvn._worklist.push(ac);
       } else if (use->is_InlineType()) {
         assert(use->isa_InlineType()->get_oop() == res, "unexpected inline type use");
-         _igvn.rehash_node_delayed(use);
+        _igvn.rehash_node_delayed(use);
         use->isa_InlineType()->set_oop(_igvn.zerocon(T_INLINE_TYPE));
       } else if (use->is_Store()) {
         _igvn.replace_node(use, use->in(MemNode::Memory));
       } else {
         eliminate_gc_barrier(use);
diff a/src/hotspot/share/opto/output.cpp b/src/hotspot/share/opto/output.cpp
--- a/src/hotspot/share/opto/output.cpp
+++ b/src/hotspot/share/opto/output.cpp
@@ -364,11 +364,11 @@
     // Compute the offsets of the entry points required by the inline type calling convention
     if (!C->method()->is_static()) {
       // We have entries at the beginning of the method, implemented by the first 4 nodes.
       // Entry                     (unverified) @ offset 0
       // Verified_Inline_Entry_RO
-      // Inline_Entry               (unverified)
+      // Inline_Entry              (unverified)
       // Verified_Inline_Entry
       uint offset = 0;
       _code_offsets.set_value(CodeOffsets::Entry, offset);
 
       offset += ((MachVEPNode*)broot->get_node(0))->size(C->regalloc());
diff a/src/hotspot/share/opto/type.cpp b/src/hotspot/share/opto/type.cpp
--- a/src/hotspot/share/opto/type.cpp
+++ b/src/hotspot/share/opto/type.cpp
@@ -3840,11 +3840,11 @@
     case T_FLOAT:    return TypeF::make(constant.as_float());
     case T_DOUBLE:   return TypeD::make(constant.as_double());
     case T_LONG:     return TypeLong::make(constant.as_long());
     default:         break;
   }
-  fatal("Invalid boxed inline type '%s'", type2name(bt));
+  fatal("Invalid boxed value type '%s'", type2name(bt));
   return NULL;
 }
 
 //------------------------------cast_to_ptr_type-------------------------------
 const Type *TypeInstPtr::cast_to_ptr_type(PTR ptr) const {
diff a/src/hotspot/share/opto/type.hpp b/src/hotspot/share/opto/type.hpp
--- a/src/hotspot/share/opto/type.hpp
+++ b/src/hotspot/share/opto/type.hpp
@@ -93,11 +93,11 @@
     VectorS,                    //  32bit Vector types
     VectorD,                    //  64bit Vector types
     VectorX,                    // 128bit Vector types
     VectorY,                    // 256bit Vector types
     VectorZ,                    // 512bit Vector types
-    InlineType,                  // Inline type
+    InlineType,                 // Inline type
 
     AnyPtr,                     // Any old raw, klass, inst, or array pointer
     RawPtr,                     // Raw (non-oop) pointers
     OopPtr,                     // Any and all Java heap entities
     InstPtr,                    // Instance pointers (non-array objects)
@@ -331,12 +331,12 @@
   const TypeOopPtr   *is_oopptr() const;         // Java-style GC'd pointer
   const TypeInstPtr  *isa_instptr() const;       // Returns NULL if not InstPtr
   const TypeInstPtr  *is_instptr() const;        // Instance
   const TypeAryPtr   *isa_aryptr() const;        // Returns NULL if not AryPtr
   const TypeAryPtr   *is_aryptr() const;         // Array oop
-  const TypeInlineType* isa_inlinetype() const;    // Returns NULL if not Inline Type
-  const TypeInlineType* is_inlinetype() const;     // Inline Type
+  const TypeInlineType* isa_inlinetype() const;  // Returns NULL if not Inline Type
+  const TypeInlineType* is_inlinetype() const;   // Inline Type
 
   const TypeMetadataPtr   *isa_metadataptr() const;   // Returns NULL if not oop ptr type
   const TypeMetadataPtr   *is_metadataptr() const;    // Java-style GC'd pointer
   const TypeKlassPtr      *isa_klassptr() const;      // Returns NULL if not KlassPtr
   const TypeKlassPtr      *is_klassptr() const;       // assert if not KlassPtr
diff a/src/hotspot/share/runtime/sharedRuntime.cpp b/src/hotspot/share/runtime/sharedRuntime.cpp
--- a/src/hotspot/share/runtime/sharedRuntime.cpp
+++ b/src/hotspot/share/runtime/sharedRuntime.cpp
@@ -2723,11 +2723,11 @@
     // See comments around Method::link_method()
     MutexLocker mu(AdapterHandlerLibrary_lock);
     if (method->adapter() == NULL) {
       method->update_adapter_trampoline(entry);
     }
-    generate_trampoline(method->from_compiled_entry(),          entry->get_c2i_entry());
+    generate_trampoline(method->from_compiled_entry(),           entry->get_c2i_entry());
     generate_trampoline(method->from_compiled_inline_ro_entry(), entry->get_c2i_inline_ro_entry());
     generate_trampoline(method->from_compiled_inline_entry(),    entry->get_c2i_inline_entry());
   }
 
   return entry;
diff a/src/hotspot/share/runtime/sharedRuntime.hpp b/src/hotspot/share/runtime/sharedRuntime.hpp
--- a/src/hotspot/share/runtime/sharedRuntime.hpp
+++ b/src/hotspot/share/runtime/sharedRuntime.hpp
@@ -742,18 +742,18 @@
 // This class is used only with DumpSharedSpaces==true. It holds extra information
 // that's used only during CDS dump time.
 // For details, see comments around Method::link_method()
 class CDSAdapterHandlerEntry: public AdapterHandlerEntry {
   address               _c2i_entry_trampoline;           // allocated from shared spaces "MC" region
-  address               _c2i_inline_ro_entry_trampoline;  // allocated from shared spaces "MC" region
-  address               _c2i_inline_entry_trampoline;     // allocated from shared spaces "MC" region
+  address               _c2i_inline_ro_entry_trampoline; // allocated from shared spaces "MC" region
+  address               _c2i_inline_entry_trampoline;    // allocated from shared spaces "MC" region
   AdapterHandlerEntry** _adapter_trampoline;             // allocated from shared spaces "MD" region
 
 public:
   address get_c2i_entry_trampoline()             const { return _c2i_entry_trampoline; }
-  address get_c2i_inline_ro_entry_trampoline()    const { return _c2i_inline_ro_entry_trampoline; }
-  address get_c2i_inline_entry_trampoline()       const { return _c2i_inline_entry_trampoline; }
+  address get_c2i_inline_ro_entry_trampoline()   const { return _c2i_inline_ro_entry_trampoline; }
+  address get_c2i_inline_entry_trampoline()      const { return _c2i_inline_entry_trampoline; }
   AdapterHandlerEntry** get_adapter_trampoline() const { return _adapter_trampoline; }
   void init() NOT_CDS_RETURN;
 };
 
 
@@ -784,12 +784,12 @@
 };
 
 // Utility class for computing the calling convention of the 3 types
 // of compiled method entries:
 //     Method::_from_compiled_entry               - sig_cc
-//     Method::_from_compiled_inline_ro_entry      - sig_cc_ro
-//     Method::_from_compiled_inline_entry         - sig
+//     Method::_from_compiled_inline_ro_entry     - sig_cc_ro
+//     Method::_from_compiled_inline_entry        - sig
 class CompiledEntrySignature : public StackObj {
   Method* _method;
   int  _num_inline_args;
   bool _has_inline_recv;
   GrowableArray<SigEntry> *_sig;
@@ -827,11 +827,11 @@
   int args_on_stack()                  const { return _args_on_stack; }
   int args_on_stack_cc()               const { return _args_on_stack_cc; }
   int args_on_stack_cc_ro()            const { return _args_on_stack_cc_ro; }
 
   int  num_inline_args()               const { return _num_inline_args; }
-  bool has_inline_arg()                const { return _num_inline_args > 0;  }
+  bool has_inline_arg()                const { return _num_inline_args > 0; }
   bool has_inline_recv()               const { return _has_inline_recv; }
 
   bool has_scalarized_args()           const { return _has_scalarized_args; }
   bool c1_needs_stack_repair()         const { return _c1_needs_stack_repair; }
   bool c2_needs_stack_repair()         const { return _c2_needs_stack_repair; }
