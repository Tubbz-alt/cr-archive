<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/share/asm/macroAssembler_common.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="codeBuffer.hpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="macroAssembler_common.hpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/share/asm/macroAssembler_common.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
108   // Set all source registers/stack slots to readonly to prevent accidental overwriting
109   for (int i = 0; i &lt; num_regs; ++i) {
110     VMReg reg = regs[i].first();
111     if (!reg-&gt;is_valid()) continue;
112     if (reg-&gt;is_stack()) {
113       // Update source stack location by adding stack increment
114       reg = VMRegImpl::stack2reg(reg-&gt;reg2stack() + sp_inc/VMRegImpl::stack_slot_size);
115       regs[i] = reg;
116     }
117     assert(reg-&gt;value() &gt;= 0 &amp;&amp; reg-&gt;value() &lt; max_reg, &quot;reg value out of bounds&quot;);
118     reg_state[reg-&gt;value()] = MacroAssembler::reg_readonly;
119   }
120   if (is_packing) {
121     // The reserved entries are not used by the packed args, so make them writable
122     mark_reserved_entries_writable(sig_cc, regs, num_regs, reg_state);
123   }
124 
125   return reg_state;
126 }
127 
<span class="line-modified">128 int MacroAssembler::unpack_value_args_common(Compile* C, bool receiver_only) {</span>
<span class="line-modified">129   assert(C-&gt;has_scalarized_args(), &quot;value type argument scalarization is disabled&quot;);</span>
130   Method* method = C-&gt;method()-&gt;get_Method();
131   const GrowableArray&lt;SigEntry&gt;* sig_cc = method-&gt;adapter()-&gt;get_sig_cc();
132   assert(sig_cc != NULL, &quot;must have scalarized signature&quot;);
133 
134   // Get unscalarized calling convention
135   BasicType* sig_bt = NEW_RESOURCE_ARRAY(BasicType, sig_cc-&gt;length()); // FIXME - may underflow if we support values with no fields!
136   int args_passed = 0;
137   if (!method-&gt;is_static()) {
138     sig_bt[args_passed++] = T_OBJECT;
139   }
140   if (!receiver_only) {
141     for (SignatureStream ss(method-&gt;signature()); !ss.at_return_type(); ss.next()) {
142       BasicType bt = ss.type();
143       sig_bt[args_passed++] = bt;
144       if (type2size[bt] == 2) {
145         sig_bt[args_passed++] = T_VOID;
146       }
147     }
148   } else {
149     // Only unpack the receiver, all other arguments are already scalarized
150     InstanceKlass* holder = method-&gt;method_holder();
151     int rec_len = holder-&gt;is_inline_klass() ? InlineKlass::cast(holder)-&gt;extended_sig()-&gt;length() : 1;
<span class="line-modified">152     // Copy scalarized signature but skip receiver, value type delimiters and reserved entries</span>
153     for (int i = 0; i &lt; sig_cc-&gt;length(); i++) {
154       if (!SigEntry::is_reserved_entry(sig_cc, i)) {
155         if (SigEntry::skip_value_delimiters(sig_cc, i) &amp;&amp; rec_len &lt;= 0) {
156           sig_bt[args_passed++] = sig_cc-&gt;at(i)._bt;
157         }
158         rec_len--;
159       }
160     }
161   }
162   VMRegPair* regs = NEW_RESOURCE_ARRAY(VMRegPair, args_passed);
163   int args_on_stack = SharedRuntime::java_calling_convention(sig_bt, regs, args_passed, false);
164 
165   // Get scalarized calling convention
166   int args_passed_cc = SigEntry::fill_sig_bt(sig_cc, sig_bt);
167   VMRegPair* regs_cc = NEW_RESOURCE_ARRAY(VMRegPair, sig_cc-&gt;length());
168   int args_on_stack_cc = SharedRuntime::java_calling_convention(sig_bt, regs_cc, args_passed_cc, false);
169   int extra_stack_offset = wordSize; // stack has the returned address
170   // Compute stack increment
171   int sp_inc = 0;
172   if (args_on_stack_cc &gt; args_on_stack) {
173     sp_inc = (args_on_stack_cc - args_on_stack) * VMRegImpl::stack_slot_size;
174     sp_inc = align_up(sp_inc, StackAlignmentInBytes);
175   }
<span class="line-modified">176   shuffle_value_args(false, receiver_only, extra_stack_offset, sig_bt, sig_cc,</span>
177                      args_passed, args_on_stack, regs,
178                      args_passed_cc, args_on_stack_cc, regs_cc, sp_inc);
179   return sp_inc;
180 }
181 
<span class="line-modified">182 void MacroAssembler::shuffle_value_args_common(bool is_packing, bool receiver_only, int extra_stack_offset,</span>
183                                                BasicType* sig_bt, const GrowableArray&lt;SigEntry&gt;* sig_cc,
184                                                int args_passed, int args_on_stack, VMRegPair* regs,
185                                                int args_passed_to, int args_on_stack_to, VMRegPair* regs_to,
186                                                int sp_inc, int ret_off) {
187   int max_stack = MAX2(args_on_stack + sp_inc/VMRegImpl::stack_slot_size, args_on_stack_to);
188   RegState* reg_state = init_reg_state(is_packing, sig_cc, regs, args_passed, sp_inc, max_stack);
189 
<span class="line-modified">190   // Emit code for packing/unpacking value type arguments</span>
191   // We try multiple times and eventually start spilling to resolve (circular) dependencies
192   bool done = false;
193   for (int i = 0; i &lt; 2*args_passed_to &amp;&amp; !done; ++i) {
194     done = true;
195     bool spill = (i &gt; args_passed_to); // Start spilling?
196     // Iterate over all arguments (when unpacking, do in reverse)
197     int step = is_packing ? 1 : -1;
198     int from_index    = is_packing ? 0 : args_passed      - 1;
199     int to_index      = is_packing ? 0 : args_passed_to   - 1;
200     int sig_index     = is_packing ? 0 : sig_cc-&gt;length() - 1;
201     int sig_index_end = is_packing ? sig_cc-&gt;length() : -1;
202     int vtarg_index = 0;
203     for (; sig_index != sig_index_end; sig_index += step) {
204       assert(0 &lt;= sig_index &amp;&amp; sig_index &lt; sig_cc-&gt;length(), &quot;index out of bounds&quot;);
205       if (SigEntry::is_reserved_entry(sig_cc, sig_index)) {
206         if (is_packing) {
207           from_index += step;
208         } else {
209           to_index += step;
210         }
211       } else {
212         assert(0 &lt;= from_index &amp;&amp; from_index &lt; args_passed, &quot;index out of bounds&quot;);
213         assert(0 &lt;= to_index &amp;&amp; to_index &lt; args_passed_to, &quot;index out of bounds&quot;);
214         if (spill) {
215           // This call returns true IFF we should keep trying to spill in this round.
<span class="line-modified">216           spill = shuffle_value_args_spill(is_packing, sig_cc, sig_index, regs, from_index, args_passed,</span>
217                                            reg_state, ret_off, extra_stack_offset);
218         }
219         BasicType bt = sig_cc-&gt;at(sig_index)._bt;
220         if (SigEntry::skip_value_delimiters(sig_cc, sig_index)) {
221           VMReg from_reg = regs[from_index].first();
222           done &amp;= move_helper(from_reg, regs_to[to_index].first(), bt, reg_state, ret_off, extra_stack_offset);
223           to_index += step;
224         } else if (is_packing || !receiver_only || (from_index == 0 &amp;&amp; bt == T_VOID)) {
225           if (is_packing) {
226             VMReg reg_to = regs_to[to_index].first();
<span class="line-modified">227             done &amp;= pack_value_helper(sig_cc, sig_index, vtarg_index, reg_to, regs, args_passed, from_index,</span>
228                                       reg_state, ret_off, extra_stack_offset);
229             vtarg_index ++;
230             to_index ++;
231             continue; // from_index already adjusted
232           } else {
233             VMReg from_reg = regs[from_index].first();
<span class="line-modified">234             done &amp;= unpack_value_helper(sig_cc, sig_index, from_reg, regs_to, to_index, reg_state, ret_off, extra_stack_offset);</span>
235           }
236         } else {
237           continue;
238         }
239         from_index += step;
240       }
241     }
242   }
<span class="line-modified">243   guarantee(done, &quot;Could not resolve circular dependency when shuffling value type arguments&quot;);</span>
244 }
245 
<span class="line-modified">246 bool MacroAssembler::shuffle_value_args_spill(bool is_packing, const GrowableArray&lt;SigEntry&gt;* sig_cc, int sig_cc_index,</span>
247                                               VMRegPair* regs_from, int from_index, int regs_from_count,
248                                               RegState* reg_state, int ret_off, int extra_stack_offset) {
249   VMReg reg;
250 
251   if (!is_packing || SigEntry::skip_value_delimiters(sig_cc, sig_cc_index)) {
252     reg = regs_from[from_index].first();
253     if (!reg-&gt;is_valid() || reg_state[reg-&gt;value()] != reg_readonly) {
254       // Spilling this won&#39;t break circles
255       return true;
256     }
257   } else {
258     ScalarizedValueArgsStream stream(sig_cc, sig_cc_index, regs_from, regs_from_count, from_index);
259     VMRegPair from_pair;
260     BasicType bt;
261     bool found = false;
262     while (stream.next(from_pair, bt)) {
263       reg = from_pair.first();
264       assert(reg-&gt;is_valid(), &quot;must be&quot;);
265       if (reg_state[reg-&gt;value()] == reg_readonly) {
266         found = true;
267         break;
268       }
269     }
270     if (!found) {
<span class="line-modified">271       // Spilling fields in this value arg won&#39;t break circles</span>
272       return true;
273     }
274   }
275 
276   // Spill argument to be able to write the source and resolve circular dependencies
277   VMReg spill_reg = spill_reg_for(reg);
278   if (reg_state[spill_reg-&gt;value()] == reg_readonly) {
279     // We have already spilled (in previous round). The spilled register should be consumed by this round.
280   } else {
281     bool res = move_helper(reg, spill_reg, T_DOUBLE, reg_state, ret_off, extra_stack_offset);
282     assert(res, &quot;Spilling should not fail&quot;);
283     // Set spill_reg as new source and update state
284     reg = spill_reg;
285     regs_from[from_index].set1(reg);
286     reg_state[reg-&gt;value()] = reg_readonly;
287   }
288 
289   return false; // Do not spill again in this round
290 }
</pre>
</td>
<td>
<hr />
<pre>
108   // Set all source registers/stack slots to readonly to prevent accidental overwriting
109   for (int i = 0; i &lt; num_regs; ++i) {
110     VMReg reg = regs[i].first();
111     if (!reg-&gt;is_valid()) continue;
112     if (reg-&gt;is_stack()) {
113       // Update source stack location by adding stack increment
114       reg = VMRegImpl::stack2reg(reg-&gt;reg2stack() + sp_inc/VMRegImpl::stack_slot_size);
115       regs[i] = reg;
116     }
117     assert(reg-&gt;value() &gt;= 0 &amp;&amp; reg-&gt;value() &lt; max_reg, &quot;reg value out of bounds&quot;);
118     reg_state[reg-&gt;value()] = MacroAssembler::reg_readonly;
119   }
120   if (is_packing) {
121     // The reserved entries are not used by the packed args, so make them writable
122     mark_reserved_entries_writable(sig_cc, regs, num_regs, reg_state);
123   }
124 
125   return reg_state;
126 }
127 
<span class="line-modified">128 int MacroAssembler::unpack_inline_args_common(Compile* C, bool receiver_only) {</span>
<span class="line-modified">129   assert(C-&gt;has_scalarized_args(), &quot;inline type argument scalarization is disabled&quot;);</span>
130   Method* method = C-&gt;method()-&gt;get_Method();
131   const GrowableArray&lt;SigEntry&gt;* sig_cc = method-&gt;adapter()-&gt;get_sig_cc();
132   assert(sig_cc != NULL, &quot;must have scalarized signature&quot;);
133 
134   // Get unscalarized calling convention
135   BasicType* sig_bt = NEW_RESOURCE_ARRAY(BasicType, sig_cc-&gt;length()); // FIXME - may underflow if we support values with no fields!
136   int args_passed = 0;
137   if (!method-&gt;is_static()) {
138     sig_bt[args_passed++] = T_OBJECT;
139   }
140   if (!receiver_only) {
141     for (SignatureStream ss(method-&gt;signature()); !ss.at_return_type(); ss.next()) {
142       BasicType bt = ss.type();
143       sig_bt[args_passed++] = bt;
144       if (type2size[bt] == 2) {
145         sig_bt[args_passed++] = T_VOID;
146       }
147     }
148   } else {
149     // Only unpack the receiver, all other arguments are already scalarized
150     InstanceKlass* holder = method-&gt;method_holder();
151     int rec_len = holder-&gt;is_inline_klass() ? InlineKlass::cast(holder)-&gt;extended_sig()-&gt;length() : 1;
<span class="line-modified">152     // Copy scalarized signature but skip receiver, inline type delimiters and reserved entries</span>
153     for (int i = 0; i &lt; sig_cc-&gt;length(); i++) {
154       if (!SigEntry::is_reserved_entry(sig_cc, i)) {
155         if (SigEntry::skip_value_delimiters(sig_cc, i) &amp;&amp; rec_len &lt;= 0) {
156           sig_bt[args_passed++] = sig_cc-&gt;at(i)._bt;
157         }
158         rec_len--;
159       }
160     }
161   }
162   VMRegPair* regs = NEW_RESOURCE_ARRAY(VMRegPair, args_passed);
163   int args_on_stack = SharedRuntime::java_calling_convention(sig_bt, regs, args_passed, false);
164 
165   // Get scalarized calling convention
166   int args_passed_cc = SigEntry::fill_sig_bt(sig_cc, sig_bt);
167   VMRegPair* regs_cc = NEW_RESOURCE_ARRAY(VMRegPair, sig_cc-&gt;length());
168   int args_on_stack_cc = SharedRuntime::java_calling_convention(sig_bt, regs_cc, args_passed_cc, false);
169   int extra_stack_offset = wordSize; // stack has the returned address
170   // Compute stack increment
171   int sp_inc = 0;
172   if (args_on_stack_cc &gt; args_on_stack) {
173     sp_inc = (args_on_stack_cc - args_on_stack) * VMRegImpl::stack_slot_size;
174     sp_inc = align_up(sp_inc, StackAlignmentInBytes);
175   }
<span class="line-modified">176   shuffle_inline_args(false, receiver_only, extra_stack_offset, sig_bt, sig_cc,</span>
177                      args_passed, args_on_stack, regs,
178                      args_passed_cc, args_on_stack_cc, regs_cc, sp_inc);
179   return sp_inc;
180 }
181 
<span class="line-modified">182 void MacroAssembler::shuffle_inline_args_common(bool is_packing, bool receiver_only, int extra_stack_offset,</span>
183                                                BasicType* sig_bt, const GrowableArray&lt;SigEntry&gt;* sig_cc,
184                                                int args_passed, int args_on_stack, VMRegPair* regs,
185                                                int args_passed_to, int args_on_stack_to, VMRegPair* regs_to,
186                                                int sp_inc, int ret_off) {
187   int max_stack = MAX2(args_on_stack + sp_inc/VMRegImpl::stack_slot_size, args_on_stack_to);
188   RegState* reg_state = init_reg_state(is_packing, sig_cc, regs, args_passed, sp_inc, max_stack);
189 
<span class="line-modified">190   // Emit code for packing/unpacking inline type arguments</span>
191   // We try multiple times and eventually start spilling to resolve (circular) dependencies
192   bool done = false;
193   for (int i = 0; i &lt; 2*args_passed_to &amp;&amp; !done; ++i) {
194     done = true;
195     bool spill = (i &gt; args_passed_to); // Start spilling?
196     // Iterate over all arguments (when unpacking, do in reverse)
197     int step = is_packing ? 1 : -1;
198     int from_index    = is_packing ? 0 : args_passed      - 1;
199     int to_index      = is_packing ? 0 : args_passed_to   - 1;
200     int sig_index     = is_packing ? 0 : sig_cc-&gt;length() - 1;
201     int sig_index_end = is_packing ? sig_cc-&gt;length() : -1;
202     int vtarg_index = 0;
203     for (; sig_index != sig_index_end; sig_index += step) {
204       assert(0 &lt;= sig_index &amp;&amp; sig_index &lt; sig_cc-&gt;length(), &quot;index out of bounds&quot;);
205       if (SigEntry::is_reserved_entry(sig_cc, sig_index)) {
206         if (is_packing) {
207           from_index += step;
208         } else {
209           to_index += step;
210         }
211       } else {
212         assert(0 &lt;= from_index &amp;&amp; from_index &lt; args_passed, &quot;index out of bounds&quot;);
213         assert(0 &lt;= to_index &amp;&amp; to_index &lt; args_passed_to, &quot;index out of bounds&quot;);
214         if (spill) {
215           // This call returns true IFF we should keep trying to spill in this round.
<span class="line-modified">216           spill = shuffle_inline_args_spill(is_packing, sig_cc, sig_index, regs, from_index, args_passed,</span>
217                                            reg_state, ret_off, extra_stack_offset);
218         }
219         BasicType bt = sig_cc-&gt;at(sig_index)._bt;
220         if (SigEntry::skip_value_delimiters(sig_cc, sig_index)) {
221           VMReg from_reg = regs[from_index].first();
222           done &amp;= move_helper(from_reg, regs_to[to_index].first(), bt, reg_state, ret_off, extra_stack_offset);
223           to_index += step;
224         } else if (is_packing || !receiver_only || (from_index == 0 &amp;&amp; bt == T_VOID)) {
225           if (is_packing) {
226             VMReg reg_to = regs_to[to_index].first();
<span class="line-modified">227             done &amp;= pack_inline_helper(sig_cc, sig_index, vtarg_index, reg_to, regs, args_passed, from_index,</span>
228                                       reg_state, ret_off, extra_stack_offset);
229             vtarg_index ++;
230             to_index ++;
231             continue; // from_index already adjusted
232           } else {
233             VMReg from_reg = regs[from_index].first();
<span class="line-modified">234             done &amp;= unpack_inline_helper(sig_cc, sig_index, from_reg, regs_to, to_index, reg_state, ret_off, extra_stack_offset);</span>
235           }
236         } else {
237           continue;
238         }
239         from_index += step;
240       }
241     }
242   }
<span class="line-modified">243   guarantee(done, &quot;Could not resolve circular dependency when shuffling inline type arguments&quot;);</span>
244 }
245 
<span class="line-modified">246 bool MacroAssembler::shuffle_inline_args_spill(bool is_packing, const GrowableArray&lt;SigEntry&gt;* sig_cc, int sig_cc_index,</span>
247                                               VMRegPair* regs_from, int from_index, int regs_from_count,
248                                               RegState* reg_state, int ret_off, int extra_stack_offset) {
249   VMReg reg;
250 
251   if (!is_packing || SigEntry::skip_value_delimiters(sig_cc, sig_cc_index)) {
252     reg = regs_from[from_index].first();
253     if (!reg-&gt;is_valid() || reg_state[reg-&gt;value()] != reg_readonly) {
254       // Spilling this won&#39;t break circles
255       return true;
256     }
257   } else {
258     ScalarizedValueArgsStream stream(sig_cc, sig_cc_index, regs_from, regs_from_count, from_index);
259     VMRegPair from_pair;
260     BasicType bt;
261     bool found = false;
262     while (stream.next(from_pair, bt)) {
263       reg = from_pair.first();
264       assert(reg-&gt;is_valid(), &quot;must be&quot;);
265       if (reg_state[reg-&gt;value()] == reg_readonly) {
266         found = true;
267         break;
268       }
269     }
270     if (!found) {
<span class="line-modified">271       // Spilling fields in this inline type arg won&#39;t break circles</span>
272       return true;
273     }
274   }
275 
276   // Spill argument to be able to write the source and resolve circular dependencies
277   VMReg spill_reg = spill_reg_for(reg);
278   if (reg_state[spill_reg-&gt;value()] == reg_readonly) {
279     // We have already spilled (in previous round). The spilled register should be consumed by this round.
280   } else {
281     bool res = move_helper(reg, spill_reg, T_DOUBLE, reg_state, ret_off, extra_stack_offset);
282     assert(res, &quot;Spilling should not fail&quot;);
283     // Set spill_reg as new source and update state
284     reg = spill_reg;
285     regs_from[from_index].set1(reg);
286     reg_state[reg-&gt;value()] = reg_readonly;
287   }
288 
289   return false; // Do not spill again in this round
290 }
</pre>
</td>
</tr>
</table>
<center><a href="codeBuffer.hpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="macroAssembler_common.hpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>