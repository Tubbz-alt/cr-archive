<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/share/opto/graphKit.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="escape.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="graphKit.hpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/share/opto/graphKit.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
   6  * under the terms of the GNU General Public License version 2 only, as
   7  * published by the Free Software Foundation.
   8  *
   9  * This code is distributed in the hope that it will be useful, but WITHOUT
  10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #include &quot;precompiled.hpp&quot;


  26 #include &quot;ci/ciUtilities.hpp&quot;
  27 #include &quot;compiler/compileLog.hpp&quot;
<span class="line-removed">  28 #include &quot;ci/ciValueKlass.hpp&quot;</span>
  29 #include &quot;gc/shared/barrierSet.hpp&quot;
  30 #include &quot;gc/shared/c2/barrierSetC2.hpp&quot;
  31 #include &quot;interpreter/interpreter.hpp&quot;
  32 #include &quot;memory/resourceArea.hpp&quot;
  33 #include &quot;opto/addnode.hpp&quot;
  34 #include &quot;opto/castnode.hpp&quot;
  35 #include &quot;opto/convertnode.hpp&quot;
  36 #include &quot;opto/graphKit.hpp&quot;
  37 #include &quot;opto/idealKit.hpp&quot;

  38 #include &quot;opto/intrinsicnode.hpp&quot;
  39 #include &quot;opto/locknode.hpp&quot;
  40 #include &quot;opto/machnode.hpp&quot;
  41 #include &quot;opto/narrowptrnode.hpp&quot;
  42 #include &quot;opto/opaquenode.hpp&quot;
  43 #include &quot;opto/parse.hpp&quot;
  44 #include &quot;opto/rootnode.hpp&quot;
  45 #include &quot;opto/runtime.hpp&quot;
  46 #include &quot;opto/subtypenode.hpp&quot;
<span class="line-removed">  47 #include &quot;opto/valuetypenode.hpp&quot;</span>
  48 #include &quot;runtime/deoptimization.hpp&quot;
  49 #include &quot;runtime/sharedRuntime.hpp&quot;
  50 #include &quot;utilities/bitMap.inline.hpp&quot;
  51 #include &quot;utilities/powerOfTwo.hpp&quot;
  52 
  53 //----------------------------GraphKit-----------------------------------------
  54 // Main utility constructor.
  55 GraphKit::GraphKit(JVMState* jvms, PhaseGVN* gvn)
  56   : Phase(Phase::Parser),
  57     _env(C-&gt;env()),
  58     _gvn((gvn != NULL) ? *gvn : *C-&gt;initial_gvn()),
  59     _barrier_set(BarrierSet::barrier_set()-&gt;barrier_set_c2())
  60 {
  61   assert(gvn == NULL || !gvn-&gt;is_IterGVN() || gvn-&gt;is_IterGVN()-&gt;delay_transform(), &quot;delay transform should be enabled&quot;);
  62   _exceptions = jvms-&gt;map()-&gt;next_exception();
  63   if (_exceptions != NULL)  jvms-&gt;map()-&gt;set_next_exception(NULL);
  64   set_jvms(jvms);
  65 #ifdef ASSERT
  66   if (_gvn.is_IterGVN() != NULL) {
  67     assert(_gvn.is_IterGVN()-&gt;delay_transform(), &quot;Transformation must be delayed if IterGVN is used&quot;);
</pre>
<hr />
<pre>
1380   }
1381 
1382   if (assert_null) {
1383     // Cast obj to null on this path.
1384     replace_in_map(value, zerocon(type));
1385     return zerocon(type);
1386   }
1387 
1388   // Cast obj to not-null on this path, if there is no null_control.
1389   // (If there is a null_control, a non-null value may come back to haunt us.)
1390   if (type == T_OBJECT) {
1391     Node* cast = cast_not_null(value, false);
1392     if (null_control == NULL || (*null_control) == top())
1393       replace_in_map(value, cast);
1394     value = cast;
1395   }
1396 
1397   return value;
1398 }
1399 
<span class="line-modified">1400 Node* GraphKit::null2default(Node* value, ciValueKlass* vk) {</span>
1401   Node* null_ctl = top();
1402   value = null_check_oop(value, &amp;null_ctl);
1403   if (!null_ctl-&gt;is_top()) {
1404     // Return default value if oop is null
1405     Node* region = new RegionNode(3);
1406     region-&gt;init_req(1, control());
1407     region-&gt;init_req(2, null_ctl);
1408     value = PhiNode::make(region, value, TypeInstPtr::make(TypePtr::BotPTR, vk));
<span class="line-modified">1409     value-&gt;set_req(2, ValueTypeNode::default_oop(gvn(), vk));</span>
1410     set_control(gvn().transform(region));
1411     value = gvn().transform(value);
1412   }
1413   return value;
1414 }
1415 
1416 //------------------------------cast_not_null----------------------------------
1417 // Cast obj to not-null on this path
1418 Node* GraphKit::cast_not_null(Node* obj, bool do_replace_in_map) {
<span class="line-modified">1419   if (obj-&gt;is_ValueType()) {</span>
1420     return obj;
1421   }
1422   const Type *t = _gvn.type(obj);
1423   const Type *t_not_null = t-&gt;join_speculative(TypePtr::NOTNULL);
1424   // Object is already not-null?
1425   if( t == t_not_null ) return obj;
1426 
1427   Node *cast = new CastPPNode(obj,t_not_null);
1428   cast-&gt;init_req(0, control());
1429   cast = _gvn.transform( cast );
1430 
<span class="line-modified">1431   if (t-&gt;is_valuetypeptr() &amp;&amp; t-&gt;value_klass()-&gt;is_scalarizable()) {</span>
<span class="line-modified">1432     // Scalarize inline type know that we know it&#39;s non-null</span>
<span class="line-modified">1433     cast = ValueTypeNode::make_from_oop(this, cast, t-&gt;value_klass())-&gt;buffer(this, false);</span>
1434   }
1435 
1436   // Scan for instances of &#39;obj&#39; in the current JVM mapping.
1437   // These instances are known to be not-null after the test.
1438   if (do_replace_in_map)
1439     replace_in_map(obj, cast);
1440 
1441   return cast;                  // Return casted value
1442 }
1443 
1444 // Sometimes in intrinsics, we implicitly know an object is not null
1445 // (there&#39;s no actual null check) so we can cast it to not null. In
1446 // the course of optimizations, the input to the cast can become null.
1447 // In that case that data path will die and we need the control path
1448 // to become dead as well to keep the graph consistent. So we have to
1449 // add a check for null for which one branch can&#39;t be taken. It uses
1450 // an Opaque4 node that will cause the check to be removed after loop
1451 // opts so the test goes away and the compiled code doesn&#39;t execute a
1452 // useless check.
1453 Node* GraphKit::must_be_not_null(Node* value, bool do_replace_in_map) {
</pre>
<hr />
<pre>
1605 Node* GraphKit::access_store_at(Node* obj,
1606                                 Node* adr,
1607                                 const TypePtr* adr_type,
1608                                 Node* val,
1609                                 const Type* val_type,
1610                                 BasicType bt,
1611                                 DecoratorSet decorators,
1612                                 bool safe_for_replace) {
1613   // Transformation of a value which could be NULL pointer (CastPP #NULL)
1614   // could be delayed during Parse (for example, in adjust_map_after_if()).
1615   // Execute transformation here to avoid barrier generation in such case.
1616   if (_gvn.type(val) == TypePtr::NULL_PTR) {
1617     val = _gvn.makecon(TypePtr::NULL_PTR);
1618   }
1619 
1620   if (stopped()) {
1621     return top(); // Dead path ?
1622   }
1623 
1624   assert(val != NULL, &quot;not dead path&quot;);
<span class="line-modified">1625   if (val-&gt;is_ValueType()) {</span>
1626     // Store to non-flattened field. Buffer the inline type and make sure
1627     // the store is re-executed if the allocation triggers deoptimization.
1628     PreserveReexecuteState preexecs(this);
1629     jvms()-&gt;set_should_reexecute(true);
<span class="line-modified">1630     val = val-&gt;as_ValueType()-&gt;buffer(this, safe_for_replace);</span>
1631   }
1632 
1633   C2AccessValuePtr addr(adr, adr_type);
1634   C2AccessValue value(val, val_type);
1635   C2ParseAccess access(this, decorators | C2_WRITE_ACCESS, bt, obj, addr);
1636   if (access.is_raw()) {
1637     return _barrier_set-&gt;BarrierSetC2::store_at(access, value);
1638   } else {
1639     return _barrier_set-&gt;store_at(access, value);
1640   }
1641 }
1642 
1643 Node* GraphKit::access_load_at(Node* obj,   // containing obj
1644                                Node* adr,   // actual adress to store val at
1645                                const TypePtr* adr_type,
1646                                const Type* val_type,
1647                                BasicType bt,
1648                                DecoratorSet decorators,
1649                                Node* ctl) {
1650   if (stopped()) {
</pre>
<hr />
<pre>
1742                                      BasicType bt,
1743                                      DecoratorSet decorators) {
1744   C2AccessValuePtr addr(adr, adr_type);
1745   C2AtomicParseAccess access(this, decorators | C2_READ_ACCESS | C2_WRITE_ACCESS, bt, obj, addr, alias_idx);
1746   if (access.is_raw()) {
1747     return _barrier_set-&gt;BarrierSetC2::atomic_add_at(access, new_val, value_type);
1748   } else {
1749     return _barrier_set-&gt;atomic_add_at(access, new_val, value_type);
1750   }
1751 }
1752 
1753 void GraphKit::access_clone(Node* src_base, Node* dst_base, Node* countx, bool is_array) {
1754   return _barrier_set-&gt;clone(this, src_base, dst_base, countx, is_array);
1755 }
1756 
1757 //-------------------------array_element_address-------------------------
1758 Node* GraphKit::array_element_address(Node* ary, Node* idx, BasicType elembt,
1759                                       const TypeInt* sizetype, Node* ctrl) {
1760   uint shift  = exact_log2(type2aelembytes(elembt));
1761   ciKlass* arytype_klass = _gvn.type(ary)-&gt;is_aryptr()-&gt;klass();
<span class="line-modified">1762   if (arytype_klass != NULL &amp;&amp; arytype_klass-&gt;is_value_array_klass()) {</span>
<span class="line-modified">1763     ciValueArrayKlass* vak = arytype_klass-&gt;as_value_array_klass();</span>
1764     shift = vak-&gt;log2_element_size();
1765   }
1766   uint header = arrayOopDesc::base_offset_in_bytes(elembt);
1767 
1768   // short-circuit a common case (saves lots of confusing waste motion)
1769   jint idx_con = find_int_con(idx, -1);
1770   if (idx_con &gt;= 0) {
1771     intptr_t offset = header + ((intptr_t)idx_con &lt;&lt; shift);
1772     return basic_plus_adr(ary, offset);
1773   }
1774 
1775   // must be correct type for alignment purposes
1776   Node* base  = basic_plus_adr(ary, header);
1777   idx = Compile::conv_I2X_index(&amp;_gvn, idx, sizetype, ctrl);
1778   Node* scale = _gvn.transform( new LShiftXNode(idx, intcon(shift)) );
1779   return basic_plus_adr(ary, base, scale);
1780 }
1781 
1782 //-------------------------load_array_element-------------------------
1783 Node* GraphKit::load_array_element(Node* ctl, Node* ary, Node* idx, const TypeAryPtr* arytype) {
1784   const Type* elemtype = arytype-&gt;elem();
1785   BasicType elembt = elemtype-&gt;array_element_basic_type();
<span class="line-modified">1786   assert(elembt != T_INLINE_TYPE, &quot;value types are not supported by this method&quot;);</span>
1787   Node* adr = array_element_address(ary, idx, elembt, arytype-&gt;size());
1788   if (elembt == T_NARROWOOP) {
1789     elembt = T_OBJECT; // To satisfy switch in LoadNode::make()
1790   }
1791   Node* ld = make_load(ctl, adr, elemtype, elembt, arytype, MemNode::unordered);
1792   return ld;
1793 }
1794 
1795 //-------------------------set_arguments_for_java_call-------------------------
1796 // Arguments (pre-popped from the stack) are taken from the JVMS.
1797 void GraphKit::set_arguments_for_java_call(CallJavaNode* call, bool is_late_inline) {
1798   PreserveReexecuteState preexecs(this);
1799   if (EnableValhalla) {
<span class="line-modified">1800     // Make sure the call is re-executed, if buffering of value type arguments triggers deoptimization</span>
1801     jvms()-&gt;set_should_reexecute(true);
1802     int arg_size = method()-&gt;get_declared_signature_at_bci(bci())-&gt;arg_size_for_bc(java_bc());
1803     inc_sp(arg_size);
1804   }
1805   // Add the call arguments
1806   const TypeTuple* domain = call-&gt;tf()-&gt;domain_sig();
1807   ExtendedSignature sig_cc = ExtendedSignature(call-&gt;method()-&gt;get_sig_cc(), SigEntryFilter());
1808   uint nargs = domain-&gt;cnt();
1809   for (uint i = TypeFunc::Parms, idx = TypeFunc::Parms; i &lt; nargs; i++) {
1810     Node* arg = argument(i-TypeFunc::Parms);
1811     const Type* t = domain-&gt;field_at(i);
<span class="line-modified">1812     if (call-&gt;method()-&gt;has_scalarized_args() &amp;&amp; t-&gt;is_valuetypeptr() &amp;&amp; !t-&gt;maybe_null()) {</span>
<span class="line-modified">1813       // We don&#39;t pass value type arguments by reference but instead pass each field of the value type</span>
<span class="line-modified">1814       ValueTypeNode* vt = arg-&gt;as_ValueType();</span>
1815       vt-&gt;pass_fields(this, call, sig_cc, idx);
<span class="line-modified">1816       // If a value type argument is passed as fields, attach the Method* to the call site</span>
1817       // to be able to access the extended signature later via attached_method_before_pc().
1818       // For example, see CompiledMethod::preserve_callee_argument_oops().
1819       call-&gt;set_override_symbolic_info(true);
1820       continue;
<span class="line-modified">1821     } else if (arg-&gt;is_ValueType()) {</span>
<span class="line-modified">1822       // Pass value type argument via oop to callee</span>
<span class="line-modified">1823       arg = arg-&gt;as_ValueType()-&gt;buffer(this);</span>
1824       if (!is_late_inline) {
<span class="line-modified">1825         arg = arg-&gt;as_ValueTypePtr()-&gt;get_oop();</span>
1826       }
1827     }
1828     call-&gt;init_req(idx++, arg);
1829     // Skip reserved arguments
1830     BasicType bt = t-&gt;basic_type();
1831     while (SigEntry::next_is_reserved(sig_cc, bt, true)) {
1832       call-&gt;init_req(idx++, top());
1833       if (type2size[bt] == 2) {
1834         call-&gt;init_req(idx++, top());
1835       }
1836     }
1837   }
1838 }
1839 
1840 //---------------------------set_edges_for_java_call---------------------------
1841 // Connect a newly created call into the current JVMS.
1842 // A return value node (if any) is returned from set_edges_for_java_call.
1843 void GraphKit::set_edges_for_java_call(CallJavaNode* call, bool must_throw, bool separate_io_proj) {
1844 
1845   // Add the predefined inputs:
</pre>
<hr />
<pre>
1871 Node* GraphKit::set_results_for_java_call(CallJavaNode* call, bool separate_io_proj, bool deoptimize) {
1872   if (stopped())  return top();  // maybe the call folded up?
1873 
1874   // Note:  Since any out-of-line call can produce an exception,
1875   // we always insert an I_O projection from the call into the result.
1876 
1877   make_slow_call_ex(call, env()-&gt;Throwable_klass(), separate_io_proj, deoptimize);
1878 
1879   if (separate_io_proj) {
1880     // The caller requested separate projections be used by the fall
1881     // through and exceptional paths, so replace the projections for
1882     // the fall through path.
1883     set_i_o(_gvn.transform( new ProjNode(call, TypeFunc::I_O) ));
1884     set_all_memory(_gvn.transform( new ProjNode(call, TypeFunc::Memory) ));
1885   }
1886 
1887   // Capture the return value, if any.
1888   Node* ret;
1889   if (call-&gt;method() == NULL || call-&gt;method()-&gt;return_type()-&gt;basic_type() == T_VOID) {
1890     ret = top();
<span class="line-modified">1891   } else if (call-&gt;tf()-&gt;returns_value_type_as_fields()) {</span>
<span class="line-modified">1892     // Return of multiple values (value type fields): we create a</span>
<span class="line-modified">1893     // ValueType node, each field is a projection from the call.</span>
<span class="line-modified">1894     ciValueKlass* vk = call-&gt;method()-&gt;return_type()-&gt;as_value_klass();</span>
1895     const Array&lt;SigEntry&gt;* sig_array = vk-&gt;extended_sig();
1896     GrowableArray&lt;SigEntry&gt; sig = GrowableArray&lt;SigEntry&gt;(sig_array-&gt;length());
1897     sig.appendAll(sig_array);
1898     ExtendedSignature sig_cc = ExtendedSignature(&amp;sig, SigEntryFilter());
1899     uint base_input = TypeFunc::Parms + 1;
<span class="line-modified">1900     ret = ValueTypeNode::make_from_multi(this, call, sig_cc, vk, base_input, false);</span>
1901   } else {
1902     ret = _gvn.transform(new ProjNode(call, TypeFunc::Parms));
1903   }
1904 
1905   return ret;
1906 }
1907 
1908 //--------------------set_predefined_input_for_runtime_call--------------------
1909 // Reading and setting the memory state is way conservative here.
1910 // The real problem is that I am not doing real Type analysis on memory,
1911 // so I cannot distinguish card mark stores from other stores.  Across a GC
1912 // point the Store Barrier and the card mark memory has to agree.  I cannot
1913 // have a card mark store and its barrier split across the GC point from
1914 // either above or below.  Here I get that to happen by reading ALL of memory.
1915 // A better answer would be to separate out card marks from other memory.
1916 // For now, return the input memory state, so that it can be reused
1917 // after the call, if this call has restricted memory effects.
1918 Node* GraphKit::set_predefined_input_for_runtime_call(SafePointNode* call, Node* narrow_mem) {
1919   // Set fixed predefined input arguments
1920   Node* memory = reset_memory();
</pre>
<hr />
<pre>
2912   // Now do a linear scan of the secondary super-klass array.  Again, no real
2913   // performance impact (too rare) but it&#39;s gotta be done.
2914   // Since the code is rarely used, there is no penalty for moving it
2915   // out of line, and it can only improve I-cache density.
2916   // The decision to inline or out-of-line this final check is platform
2917   // dependent, and is found in the AD file definition of PartialSubtypeCheck.
2918   Node* psc = gvn.transform(
2919     new PartialSubtypeCheckNode(*ctrl, subklass, superklass));
2920 
2921   IfNode *iff4 = gen_subtype_check_compare(*ctrl, psc, gvn.zerocon(T_OBJECT), BoolTest::ne, PROB_FAIR, gvn, T_ADDRESS);
2922   r_not_subtype-&gt;init_req(2, gvn.transform(new IfTrueNode (iff4)));
2923   r_ok_subtype -&gt;init_req(3, gvn.transform(new IfFalseNode(iff4)));
2924 
2925   // Return false path; set default control to true path.
2926   *ctrl = gvn.transform(r_ok_subtype);
2927   return gvn.transform(r_not_subtype);
2928 }
2929 
2930 Node* GraphKit::gen_subtype_check(Node* obj_or_subklass, Node* superklass) {
2931   const Type* sub_t = _gvn.type(obj_or_subklass);
<span class="line-modified">2932   if (sub_t-&gt;isa_valuetype()) {</span>
<span class="line-modified">2933     obj_or_subklass = makecon(TypeKlassPtr::make(sub_t-&gt;value_klass()));</span>
2934   }
2935   if (ExpandSubTypeCheckAtParseTime) {
2936     MergeMemNode* mem = merged_memory();
2937     Node* ctrl = control();
2938     Node* subklass = obj_or_subklass;
2939     if (!sub_t-&gt;isa_klassptr()) {
2940       subklass = load_object_klass(obj_or_subklass);
2941     }
2942     Node* n = Phase::gen_subtype_check(subklass, superklass, &amp;ctrl, mem, _gvn);
2943     set_control(ctrl);
2944     return n;
2945   }
2946 
2947   Node* check = _gvn.transform(new SubTypeCheckNode(C, obj_or_subklass, superklass));
2948   Node* bol = _gvn.transform(new BoolNode(check, BoolTest::eq));
2949   IfNode* iff = create_and_xform_if(control(), bol, PROB_STATIC_FREQUENT, COUNT_UNKNOWN);
2950   set_control(_gvn.transform(new IfTrueNode(iff)));
2951   return _gvn.transform(new IfFalseNode(iff));
2952 }
2953 
2954 // Profile-driven exact type check:
2955 Node* GraphKit::type_check_receiver(Node* receiver, ciKlass* klass,
2956                                     float prob,
2957                                     Node* *casted_receiver) {
2958   const TypeKlassPtr* tklass = TypeKlassPtr::make(klass);
2959   Node* recv_klass = load_object_klass(receiver);
2960   Node* fail = type_check(recv_klass, tklass, prob);
2961   const TypeOopPtr* recv_xtype = tklass-&gt;as_instance_type();
2962   assert(recv_xtype-&gt;klass_is_exact(), &quot;&quot;);
2963 
2964   // Subsume downstream occurrences of receiver with a cast to
2965   // recv_xtype, since now we know what the type will be.
2966   Node* cast = new CheckCastPPNode(control(), receiver, recv_xtype);
2967   Node* res = _gvn.transform(cast);
<span class="line-modified">2968   if (recv_xtype-&gt;is_valuetypeptr() &amp;&amp; recv_xtype-&gt;value_klass()-&gt;is_scalarizable()) {</span>
2969     assert(!gvn().type(res)-&gt;maybe_null(), &quot;receiver should never be null&quot;);
<span class="line-modified">2970     res = ValueTypeNode::make_from_oop(this, res, recv_xtype-&gt;value_klass());</span>
2971   }
2972 
2973   (*casted_receiver) = res;
2974   // (User must make the replace_in_map call.)
2975 
2976   return fail;
2977 }
2978 
2979 Node* GraphKit::type_check(Node* recv_klass, const TypeKlassPtr* tklass,
2980                            float prob) {
2981   Node* want_klass = makecon(tklass);
2982   Node* cmp = _gvn.transform( new CmpPNode(recv_klass, want_klass));
2983   Node* bol = _gvn.transform( new BoolNode(cmp, BoolTest::eq) );
2984   IfNode* iff = create_and_xform_if(control(), bol, prob, COUNT_UNKNOWN);
2985   set_control(  _gvn.transform( new IfTrueNode (iff)));
2986   Node* fail = _gvn.transform( new IfFalseNode(iff));
2987   return fail;
2988 }
2989 
2990 //------------------------------subtype_check_receiver-------------------------
</pre>
<hr />
<pre>
3209 // and the reflective instance-of call.
3210 Node* GraphKit::gen_instanceof(Node* obj, Node* superklass, bool safe_for_replace) {
3211   kill_dead_locals();           // Benefit all the uncommon traps
3212   assert( !stopped(), &quot;dead parse path should be checked in callers&quot; );
3213   assert(!TypePtr::NULL_PTR-&gt;higher_equal(_gvn.type(superklass)-&gt;is_klassptr()),
3214          &quot;must check for not-null not-dead klass in callers&quot;);
3215 
3216   // Make the merge point
3217   enum { _obj_path = 1, _fail_path, _null_path, PATH_LIMIT };
3218   RegionNode* region = new RegionNode(PATH_LIMIT);
3219   Node*       phi    = new PhiNode(region, TypeInt::BOOL);
3220   C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
3221 
3222   ciProfileData* data = NULL;
3223   if (java_bc() == Bytecodes::_instanceof) {  // Only for the bytecode
3224     data = method()-&gt;method_data()-&gt;bci_to_data(bci());
3225   }
3226   bool speculative_not_null = false;
3227   bool never_see_null = (ProfileDynamicTypes  // aggressive use of profile
3228                          &amp;&amp; seems_never_null(obj, data, speculative_not_null));
<span class="line-modified">3229   bool is_value = obj-&gt;is_ValueType();</span>
3230 
3231   // Null check; get casted pointer; set region slot 3
3232   Node* null_ctl = top();
3233   Node* not_null_obj = is_value ? obj : null_check_oop(obj, &amp;null_ctl, never_see_null, safe_for_replace, speculative_not_null);
3234 
3235   // If not_null_obj is dead, only null-path is taken
3236   if (stopped()) {              // Doing instance-of on a NULL?
3237     set_control(null_ctl);
3238     return intcon(0);
3239   }
3240   region-&gt;init_req(_null_path, null_ctl);
3241   phi   -&gt;init_req(_null_path, intcon(0)); // Set null path value
3242   if (null_ctl == top()) {
3243     // Do this eagerly, so that pattern matches like is_diamond_phi
3244     // will work even during parsing.
3245     assert(_null_path == PATH_LIMIT-1, &quot;delete last&quot;);
3246     region-&gt;del_req(_null_path);
3247     phi   -&gt;del_req(_null_path);
3248   }
3249 
</pre>
<hr />
<pre>
3255       ciKlass* subk = _gvn.type(obj)-&gt;is_oopptr()-&gt;klass();
3256       if (subk != NULL &amp;&amp; subk-&gt;is_loaded()) {
3257         int static_res = C-&gt;static_subtype_check(superk, subk);
3258         known_statically = (static_res == Compile::SSC_always_true || static_res == Compile::SSC_always_false);
3259       }
3260     }
3261 
3262     if (!known_statically) {
3263       const TypeOopPtr* obj_type = _gvn.type(obj)-&gt;is_oopptr();
3264       // We may not have profiling here or it may not help us. If we
3265       // have a speculative type use it to perform an exact cast.
3266       ciKlass* spec_obj_type = obj_type-&gt;speculative_type();
3267       if (spec_obj_type != NULL || (ProfileDynamicTypes &amp;&amp; data != NULL)) {
3268         Node* cast_obj = maybe_cast_profiled_receiver(not_null_obj, NULL, spec_obj_type, safe_for_replace);
3269         if (stopped()) {            // Profile disagrees with this path.
3270           set_control(null_ctl);    // Null is the only remaining possibility.
3271           return intcon(0);
3272         }
3273         if (cast_obj != NULL &amp;&amp;
3274             // A value that&#39;s sometimes null is not something we can optimize well
<span class="line-modified">3275             !(cast_obj-&gt;is_ValueType() &amp;&amp; null_ctl != top())) {</span>
3276           not_null_obj = cast_obj;
<span class="line-modified">3277           is_value = not_null_obj-&gt;is_ValueType();</span>
3278         }
3279       }
3280     }
3281   }
3282 
3283   // Generate the subtype check
3284   Node* not_subtype_ctrl = gen_subtype_check(not_null_obj, superklass);
3285 
3286   // Plug in the success path to the general merge in slot 1.
3287   region-&gt;init_req(_obj_path, control());
3288   phi   -&gt;init_req(_obj_path, intcon(1));
3289 
3290   // Plug in the failing path to the general merge in slot 2.
3291   region-&gt;init_req(_fail_path, not_subtype_ctrl);
3292   phi   -&gt;init_req(_fail_path, intcon(0));
3293 
3294   // Return final merged results
3295   set_control( _gvn.transform(region) );
3296   record_for_igvn(region);
3297 
</pre>
<hr />
<pre>
3302     Node* casted_obj = record_profiled_receiver_for_speculation(obj);
3303     replace_in_map(obj, casted_obj);
3304   }
3305 
3306   return _gvn.transform(phi);
3307 }
3308 
3309 //-------------------------------gen_checkcast---------------------------------
3310 // Generate a checkcast idiom.  Used by both the checkcast bytecode and the
3311 // array store bytecode.  Stack must be as-if BEFORE doing the bytecode so the
3312 // uncommon-trap paths work.  Adjust stack after this call.
3313 // If failure_control is supplied and not null, it is filled in with
3314 // the control edge for the cast failure.  Otherwise, an appropriate
3315 // uncommon trap or exception is thrown.
3316 Node* GraphKit::gen_checkcast(Node *obj, Node* superklass, Node* *failure_control) {
3317   kill_dead_locals();           // Benefit all the uncommon traps
3318   const TypeKlassPtr* tk = _gvn.type(superklass)-&gt;is_klassptr();
3319   const TypeOopPtr* toop = TypeOopPtr::make_from_klass(tk-&gt;klass());
3320 
3321   // Check if inline types are involved
<span class="line-modified">3322   bool from_inline = obj-&gt;is_ValueType();</span>
<span class="line-modified">3323   bool to_inline = tk-&gt;klass()-&gt;is_valuetype();</span>
3324 
3325   // Fast cutout:  Check the case that the cast is vacuously true.
3326   // This detects the common cases where the test will short-circuit
3327   // away completely.  We do this before we perform the null check,
3328   // because if the test is going to turn into zero code, we don&#39;t
3329   // want a residual null check left around.  (Causes a slowdown,
3330   // for example, in some objArray manipulations, such as a[i]=a[j].)
3331   if (tk-&gt;singleton()) {
3332     ciKlass* klass = NULL;
3333     if (from_inline) {
<span class="line-modified">3334       klass = _gvn.type(obj)-&gt;value_klass();</span>
3335     } else {
3336       const TypeOopPtr* objtp = _gvn.type(obj)-&gt;isa_oopptr();
3337       if (objtp != NULL) {
3338         klass = objtp-&gt;klass();
3339       }
3340     }
3341     if (klass != NULL) {
3342       switch (C-&gt;static_subtype_check(tk-&gt;klass(), klass)) {
3343       case Compile::SSC_always_true:
3344         // If we know the type check always succeed then we don&#39;t use
3345         // the profiling data at this bytecode. Don&#39;t lose it, feed it
3346         // to the type system as a speculative type.
3347         if (!from_inline) {
3348           obj = record_profiled_receiver_for_speculation(obj);
3349           if (to_inline) {
3350             obj = null_check(obj);
<span class="line-modified">3351             if (toop-&gt;value_klass()-&gt;is_scalarizable()) {</span>
<span class="line-modified">3352               obj = ValueTypeNode::make_from_oop(this, obj, toop-&gt;value_klass());</span>
3353             }
3354           }
3355         }
3356         return obj;
3357       case Compile::SSC_always_false:
3358         if (from_inline || to_inline) {
3359           if (!from_inline) {
3360             null_check(obj);
3361           }
<span class="line-modified">3362           // Value type is never null. Always throw an exception.</span>
3363           builtin_throw(Deoptimization::Reason_class_check, makecon(TypeKlassPtr::make(klass)));
3364           return top();
3365         } else {
3366           // It needs a null check because a null will *pass* the cast check.
3367           return null_assert(obj);
3368         }
3369       }
3370     }
3371   }
3372 
3373   ciProfileData* data = NULL;
3374   bool safe_for_replace = false;
3375   if (failure_control == NULL) {        // use MDO in regular case only
3376     assert(java_bc() == Bytecodes::_aastore ||
3377            java_bc() == Bytecodes::_checkcast,
3378            &quot;interpreter profiles type checks only for these BCs&quot;);
3379     if (method()-&gt;method_data()-&gt;is_mature()) {
3380       data = method()-&gt;method_data()-&gt;bci_to_data(bci());
3381     }
3382     safe_for_replace = true;
</pre>
<hr />
<pre>
3417   if (null_ctl == top()) {
3418     // Do this eagerly, so that pattern matches like is_diamond_phi
3419     // will work even during parsing.
3420     assert(_null_path == PATH_LIMIT-1, &quot;delete last&quot;);
3421     region-&gt;del_req(_null_path);
3422     phi   -&gt;del_req(_null_path);
3423   }
3424 
3425   Node* cast_obj = NULL;
3426   if (!from_inline &amp;&amp; tk-&gt;klass_is_exact()) {
3427     // The following optimization tries to statically cast the speculative type of the object
3428     // (for example obtained during profiling) to the type of the superklass and then do a
3429     // dynamic check that the type of the object is what we expect. To work correctly
3430     // for checkcast and aastore the type of superklass should be exact.
3431     const TypeOopPtr* obj_type = _gvn.type(obj)-&gt;is_oopptr();
3432     // We may not have profiling here or it may not help us. If we have
3433     // a speculative type use it to perform an exact cast.
3434     ciKlass* spec_obj_type = obj_type-&gt;speculative_type();
3435     if (spec_obj_type != NULL || data != NULL) {
3436       cast_obj = maybe_cast_profiled_receiver(not_null_obj, tk-&gt;klass(), spec_obj_type, safe_for_replace);
<span class="line-modified">3437       if (cast_obj != NULL &amp;&amp; cast_obj-&gt;is_ValueType()) {</span>
3438         if (null_ctl != top()) {
3439           cast_obj = NULL; // A value that&#39;s sometimes null is not something we can optimize well
3440         } else {
3441           return cast_obj;
3442         }
3443       }
3444       if (cast_obj != NULL) {
3445         if (failure_control != NULL) // failure is now impossible
3446           (*failure_control) = top();
3447         // adjust the type of the phi to the exact klass:
3448         phi-&gt;raise_bottom_type(_gvn.type(cast_obj)-&gt;meet_speculative(TypePtr::NULL_PTR));
3449       }
3450     }
3451   }
3452 
3453   if (cast_obj == NULL) {
3454     // Generate the subtype check
3455     Node* not_subtype_ctrl = gen_subtype_check(not_null_obj, superklass);
3456 
3457     // Plug in success path into the merge
3458     cast_obj = from_inline ? not_null_obj : _gvn.transform(new CheckCastPPNode(control(), not_null_obj, toop));
3459     // Failure path ends in uncommon trap (or may be dead - failure impossible)
3460     if (failure_control == NULL) {
3461       if (not_subtype_ctrl != top()) { // If failure is possible
3462         PreserveJVMState pjvms(this);
3463         set_control(not_subtype_ctrl);
3464         Node* obj_klass = NULL;
3465         if (from_inline) {
<span class="line-modified">3466           obj_klass = makecon(TypeKlassPtr::make(_gvn.type(not_null_obj)-&gt;value_klass()));</span>
3467         } else {
3468           obj_klass = load_object_klass(not_null_obj);
3469         }
3470         builtin_throw(Deoptimization::Reason_class_check, obj_klass);
3471       }
3472     } else {
3473       (*failure_control) = not_subtype_ctrl;
3474     }
3475   }
3476 
3477   region-&gt;init_req(_obj_path, control());
3478   phi   -&gt;init_req(_obj_path, cast_obj);
3479 
3480   // A merge of NULL or Casted-NotNull obj
3481   Node* res = _gvn.transform(phi);
3482 
3483   // Note I do NOT always &#39;replace_in_map(obj,result)&#39; here.
3484   //  if( tk-&gt;klass()-&gt;can_be_primary_super()  )
3485     // This means that if I successfully store an Object into an array-of-String
3486     // I &#39;forget&#39; that the Object is really now known to be a String.  I have to
3487     // do this because we don&#39;t have true union types for interfaces - if I store
3488     // a Baz into an array-of-Interface and then tell the optimizer it&#39;s an
3489     // Interface, I forget that it&#39;s also a Baz and cannot do Baz-like field
3490     // references to it.  FIX THIS WHEN UNION TYPES APPEAR!
3491   //  replace_in_map( obj, res );
3492 
3493   // Return final merged results
3494   set_control( _gvn.transform(region) );
3495   record_for_igvn(region);
3496 
<span class="line-modified">3497   bool not_inline = !toop-&gt;can_be_value_type();</span>
<span class="line-modified">3498   bool not_flattened = !UseFlatArray || not_inline || (toop-&gt;is_valuetypeptr() &amp;&amp; !toop-&gt;value_klass()-&gt;flatten_array());</span>
3499   if (EnableValhalla &amp;&amp; not_flattened) {
3500     // Check if obj has been loaded from an array
3501     obj = obj-&gt;isa_DecodeN() ? obj-&gt;in(1) : obj;
3502     Node* array = NULL;
3503     if (obj-&gt;isa_Load()) {
3504       Node* address = obj-&gt;in(MemNode::Address);
3505       if (address-&gt;isa_AddP()) {
3506         array = address-&gt;as_AddP()-&gt;in(AddPNode::Base);
3507       }
3508     } else if (obj-&gt;is_Phi()) {
3509       Node* region = obj-&gt;in(0);
3510       // TODO make this more robust (see JDK-8231346)
3511       if (region-&gt;req() == 3 &amp;&amp; region-&gt;in(2) != NULL &amp;&amp; region-&gt;in(2)-&gt;in(0) != NULL) {
3512         IfNode* iff = region-&gt;in(2)-&gt;in(0)-&gt;isa_If();
3513         if (iff != NULL) {
3514           iff-&gt;is_non_flattened_array_check(&amp;_gvn, &amp;array);
3515         }
3516       }
3517     }
3518     if (array != NULL) {
3519       const TypeAryPtr* ary_t = _gvn.type(array)-&gt;isa_aryptr();
3520       if (ary_t != NULL) {
3521         if (!ary_t-&gt;is_not_null_free() &amp;&amp; not_inline) {
3522           // Casting array element to a non-inline-type, mark array as not null-free.
3523           Node* cast = _gvn.transform(new CheckCastPPNode(control(), array, ary_t-&gt;cast_to_not_null_free()));
3524           replace_in_map(array, cast);
3525         } else if (!ary_t-&gt;is_not_flat()) {
3526           // Casting array element to a non-flattened type, mark array as not flat.
3527           Node* cast = _gvn.transform(new CheckCastPPNode(control(), array, ary_t-&gt;cast_to_not_flat()));
3528           replace_in_map(array, cast);
3529         }
3530       }
3531     }
3532   }
3533 
3534   if (!from_inline) {
3535     res = record_profiled_receiver_for_speculation(res);
<span class="line-modified">3536     if (to_inline &amp;&amp; toop-&gt;value_klass()-&gt;is_scalarizable()) {</span>
3537       assert(!gvn().type(res)-&gt;maybe_null(), &quot;Inline types are null-free&quot;);
<span class="line-modified">3538       res = ValueTypeNode::make_from_oop(this, res, toop-&gt;value_klass());</span>
3539     }
3540   }
3541   return res;
3542 }
3543 
<span class="line-modified">3544 // Check if &#39;obj&#39; is a value type by checking if it has the always_locked markWord pattern set.</span>
<span class="line-modified">3545 Node* GraphKit::is_value_type(Node* obj) {</span>
3546   Node* mark_addr = basic_plus_adr(obj, oopDesc::mark_offset_in_bytes());
3547   Node* mark = make_load(NULL, mark_addr, TypeX_X, TypeX_X-&gt;basic_type(), MemNode::unordered);
3548   Node* mask = _gvn.MakeConX(markWord::always_locked_pattern);
3549   Node* andx = _gvn.transform(new AndXNode(mark, mask));
3550   Node* cmp = _gvn.transform(new CmpXNode(andx, mask));
3551   return _gvn.transform(new BoolNode(cmp, BoolTest::eq));
3552 }
3553 
3554 // Check if &#39;ary&#39; is a non-flattened array
3555 Node* GraphKit::is_non_flattened_array(Node* ary) {
3556   Node* kls = load_object_klass(ary);
3557   Node* tag = load_lh_array_tag(kls);
3558   Node* cmp = gen_lh_array_test(kls, Klass::_lh_array_tag_vt_value);
3559   return _gvn.transform(new BoolNode(cmp, BoolTest::ne));
3560 }
3561 
3562 // Check if &#39;ary&#39; is a nullable array
3563 Node* GraphKit::is_nullable_array(Node* ary) {
3564   Node* kls = load_object_klass(ary);
3565   Node* lhp = basic_plus_adr(kls, in_bytes(Klass::layout_helper_offset()));
3566   Node* layout_val = _gvn.transform(LoadNode::make(_gvn, NULL, immutable_memory(), lhp, lhp-&gt;bottom_type()-&gt;is_ptr(), TypeInt::INT, T_INT, MemNode::unordered));
3567   Node* null_free = _gvn.transform(new RShiftINode(layout_val, intcon(Klass::_lh_null_free_shift)));
3568   null_free = _gvn.transform(new AndINode(null_free, intcon(Klass::_lh_null_free_mask)));
3569   Node* cmp = _gvn.transform(new CmpINode(null_free, intcon(0)));
3570   return _gvn.transform(new BoolNode(cmp, BoolTest::eq));
3571 }
3572 
<span class="line-modified">3573 // Deoptimize if &#39;ary&#39; is a null-free value type array and &#39;val&#39; is null</span>
<span class="line-modified">3574 Node* GraphKit::gen_value_array_null_guard(Node* ary, Node* val, int nargs, bool safe_for_replace) {</span>
3575   const Type* val_t = _gvn.type(val);
<span class="line-modified">3576   if (val-&gt;is_ValueType() || !TypePtr::NULL_PTR-&gt;higher_equal(val_t)) {</span>
3577     return ary; // Never null
3578   }
3579   RegionNode* region = new RegionNode(3);
3580   Node* null_ctl = top();
3581   null_check_oop(val, &amp;null_ctl);
3582   if (null_ctl != top()) {
3583     PreserveJVMState pjvms(this);
3584     set_control(null_ctl);
3585     {
3586       // Deoptimize if null-free array
3587       BuildCutout unless(this, is_nullable_array(ary), PROB_MAX);
3588       inc_sp(nargs);
3589       uncommon_trap(Deoptimization::Reason_null_check,
3590                     Deoptimization::Action_none);
3591     }
3592     region-&gt;init_req(1, control());
3593   }
3594   region-&gt;init_req(2, control());
3595   set_control(_gvn.transform(region));
3596   record_for_igvn(region);
</pre>
<hr />
<pre>
3740   }
3741 #endif
3742 
3743   return flock;
3744 }
3745 
3746 
3747 //------------------------------shared_unlock----------------------------------
3748 // Emit unlocking code.
3749 void GraphKit::shared_unlock(Node* box, Node* obj) {
3750   // bci is either a monitorenter bc or InvocationEntryBci
3751   // %%% SynchronizationEntryBCI is redundant; use InvocationEntryBci in interfaces
3752   assert(SynchronizationEntryBCI == InvocationEntryBci, &quot;&quot;);
3753 
3754   if( !GenerateSynchronizationCode )
3755     return;
3756   if (stopped()) {               // Dead monitor?
3757     map()-&gt;pop_monitor();        // Kill monitor from debug info
3758     return;
3759   }
<span class="line-modified">3760   assert(!obj-&gt;is_ValueTypeBase(), &quot;should not unlock on value type&quot;);</span>
3761 
3762   // Memory barrier to avoid floating things down past the locked region
3763   insert_mem_bar(Op_MemBarReleaseLock);
3764 
3765   const TypeFunc *tf = OptoRuntime::complete_monitor_exit_Type();
3766   UnlockNode *unlock = new UnlockNode(C, tf);
3767 #ifdef ASSERT
3768   unlock-&gt;set_dbg_jvms(sync_jvms());
3769 #endif
3770   uint raw_idx = Compile::AliasIdxRaw;
3771   unlock-&gt;init_req( TypeFunc::Control, control() );
3772   unlock-&gt;init_req( TypeFunc::Memory , memory(raw_idx) );
3773   unlock-&gt;init_req( TypeFunc::I_O    , top() )     ;   // does no i/o
3774   unlock-&gt;init_req( TypeFunc::FramePtr, frameptr() );
3775   unlock-&gt;init_req( TypeFunc::ReturnAdr, top() );
3776 
3777   unlock-&gt;init_req(TypeFunc::Parms + 0, obj);
3778   unlock-&gt;init_req(TypeFunc::Parms + 1, box);
3779   unlock = _gvn.transform(unlock)-&gt;as_Unlock();
3780 
</pre>
<hr />
<pre>
3786   // Kill monitor from debug info
3787   map()-&gt;pop_monitor( );
3788 }
3789 
3790 //-------------------------------get_layout_helper-----------------------------
3791 // If the given klass is a constant or known to be an array,
3792 // fetch the constant layout helper value into constant_value
3793 // and return (Node*)NULL.  Otherwise, load the non-constant
3794 // layout helper value, and return the node which represents it.
3795 // This two-faced routine is useful because allocation sites
3796 // almost always feature constant types.
3797 Node* GraphKit::get_layout_helper(Node* klass_node, jint&amp; constant_value) {
3798   const TypeKlassPtr* inst_klass = _gvn.type(klass_node)-&gt;isa_klassptr();
3799   if (!StressReflectiveCode &amp;&amp; inst_klass != NULL) {
3800     ciKlass* klass = inst_klass-&gt;klass();
3801     assert(klass != NULL, &quot;klass should not be NULL&quot;);
3802     bool    xklass = inst_klass-&gt;klass_is_exact();
3803     bool can_be_flattened = false;
3804     if (UseFlatArray &amp;&amp; klass-&gt;is_obj_array_klass()) {
3805       ciKlass* elem = klass-&gt;as_obj_array_klass()-&gt;element_klass();
<span class="line-modified">3806       can_be_flattened = elem-&gt;can_be_value_klass() &amp;&amp; (!elem-&gt;is_valuetype() || elem-&gt;as_value_klass()-&gt;flatten_array());</span>
3807     }
3808     if (xklass || (klass-&gt;is_array_klass() &amp;&amp; !can_be_flattened)) {
3809       jint lhelper = klass-&gt;layout_helper();
3810       if (lhelper != Klass::_lh_neutral_value) {
3811         constant_value = lhelper;
3812         return (Node*) NULL;
3813       }
3814     }
3815   }
3816   constant_value = Klass::_lh_neutral_value;  // put in a known value
3817   Node* lhp = basic_plus_adr(klass_node, klass_node, in_bytes(Klass::layout_helper_offset()));
3818   return make_load(NULL, lhp, TypeInt::INT, T_INT, MemNode::unordered);
3819 }
3820 
3821 // We just put in an allocate/initialize with a big raw-memory effect.
3822 // Hook selected additional alias categories on the initialization.
3823 static void hook_memory_on_init(GraphKit&amp; kit, int alias_idx,
3824                                 MergeMemNode* init_in_merge,
3825                                 Node* init_out_raw) {
3826   DEBUG_ONLY(Node* init_in_raw = init_in_merge-&gt;base_memory());
</pre>
<hr />
<pre>
3859   assert(alloc-&gt;initialization() == init,  &quot;2-way macro link must work&quot;);
3860   assert(init -&gt;allocation()     == alloc, &quot;2-way macro link must work&quot;);
3861   {
3862     // Extract memory strands which may participate in the new object&#39;s
3863     // initialization, and source them from the new InitializeNode.
3864     // This will allow us to observe initializations when they occur,
3865     // and link them properly (as a group) to the InitializeNode.
3866     assert(init-&gt;in(InitializeNode::Memory) == malloc, &quot;&quot;);
3867     MergeMemNode* minit_in = MergeMemNode::make(malloc);
3868     init-&gt;set_req(InitializeNode::Memory, minit_in);
3869     record_for_igvn(minit_in); // fold it up later, if possible
3870     _gvn.set_type(minit_in, Type::MEMORY);
3871     Node* minit_out = memory(rawidx);
3872     assert(minit_out-&gt;is_Proj() &amp;&amp; minit_out-&gt;in(0) == init, &quot;&quot;);
3873     // Add an edge in the MergeMem for the header fields so an access
3874     // to one of those has correct memory state
3875     set_memory(minit_out, C-&gt;get_alias_index(oop_type-&gt;add_offset(oopDesc::mark_offset_in_bytes())));
3876     set_memory(minit_out, C-&gt;get_alias_index(oop_type-&gt;add_offset(oopDesc::klass_offset_in_bytes())));
3877     if (oop_type-&gt;isa_aryptr()) {
3878       const TypeAryPtr* arytype = oop_type-&gt;is_aryptr();
<span class="line-modified">3879       if (arytype-&gt;klass()-&gt;is_value_array_klass()) {</span>
3880         // Initially all flattened array accesses share a single slice
3881         // but that changes after parsing. Prepare the memory graph so
3882         // it can optimize flattened array accesses properly once they
3883         // don&#39;t share a single slice.
3884         assert(C-&gt;flattened_accesses_share_alias(), &quot;should be set at parse time&quot;);
3885         C-&gt;set_flattened_accesses_share_alias(false);
<span class="line-modified">3886         ciValueArrayKlass* vak = arytype-&gt;klass()-&gt;as_value_array_klass();</span>
<span class="line-modified">3887         ciValueKlass* vk = vak-&gt;element_klass()-&gt;as_value_klass();</span>
3888         for (int i = 0, len = vk-&gt;nof_nonstatic_fields(); i &lt; len; i++) {
3889           ciField* field = vk-&gt;nonstatic_field_at(i);
3890           if (field-&gt;offset() &gt;= TrackedInitializationLimit * HeapWordSize)
3891             continue;  // do not bother to track really large numbers of fields
3892           int off_in_vt = field-&gt;offset() - vk-&gt;first_field_offset();
3893           const TypePtr* adr_type = arytype-&gt;with_field_offset(off_in_vt)-&gt;add_offset(Type::OffsetBot);
3894           int fieldidx = C-&gt;get_alias_index(adr_type, true);
3895           hook_memory_on_init(*this, fieldidx, minit_in, minit_out);
3896         }
3897         C-&gt;set_flattened_accesses_share_alias(true);
<span class="line-modified">3898         hook_memory_on_init(*this, C-&gt;get_alias_index(TypeAryPtr::VALUES), minit_in, minit_out);</span>
3899       } else {
3900         const TypePtr* telemref = oop_type-&gt;add_offset(Type::OffsetBot);
3901         int            elemidx  = C-&gt;get_alias_index(telemref);
3902         hook_memory_on_init(*this, elemidx, minit_in, minit_out);
3903       }
3904     } else if (oop_type-&gt;isa_instptr()) {
3905       set_memory(minit_out, C-&gt;get_alias_index(oop_type)); // mark word
3906       ciInstanceKlass* ik = oop_type-&gt;klass()-&gt;as_instance_klass();
3907       for (int i = 0, len = ik-&gt;nof_nonstatic_fields(); i &lt; len; i++) {
3908         ciField* field = ik-&gt;nonstatic_field_at(i);
3909         if (field-&gt;offset() &gt;= TrackedInitializationLimit * HeapWordSize)
3910           continue;  // do not bother to track really large numbers of fields
3911         // Find (or create) the alias category for this field:
3912         int fieldidx = C-&gt;alias_type(field)-&gt;index();
3913         hook_memory_on_init(*this, fieldidx, minit_in, minit_out);
3914       }
3915     }
3916   }
3917 
3918   // Cast raw oop to the real thing...
</pre>
<hr />
<pre>
3937     }
3938   }
3939 #endif //ASSERT
3940 
3941   return javaoop;
3942 }
3943 
3944 //---------------------------new_instance--------------------------------------
3945 // This routine takes a klass_node which may be constant (for a static type)
3946 // or may be non-constant (for reflective code).  It will work equally well
3947 // for either, and the graph will fold nicely if the optimizer later reduces
3948 // the type to a constant.
3949 // The optional arguments are for specialized use by intrinsics:
3950 //  - If &#39;extra_slow_test&#39; if not null is an extra condition for the slow-path.
3951 //  - If &#39;return_size_val&#39;, report the the total object size to the caller.
3952 //  - deoptimize_on_exception controls how Java exceptions are handled (rethrow vs deoptimize)
3953 Node* GraphKit::new_instance(Node* klass_node,
3954                              Node* extra_slow_test,
3955                              Node* *return_size_val,
3956                              bool deoptimize_on_exception,
<span class="line-modified">3957                              ValueTypeBaseNode* value_node) {</span>
3958   // Compute size in doublewords
3959   // The size is always an integral number of doublewords, represented
3960   // as a positive bytewise size stored in the klass&#39;s layout_helper.
3961   // The layout_helper also encodes (in a low bit) the need for a slow path.
3962   jint  layout_con = Klass::_lh_neutral_value;
3963   Node* layout_val = get_layout_helper(klass_node, layout_con);
3964   bool  layout_is_con = (layout_val == NULL);
3965 
3966   if (extra_slow_test == NULL)  extra_slow_test = intcon(0);
3967   // Generate the initial go-slow test.  It&#39;s either ALWAYS (return a
3968   // Node for 1) or NEVER (return a NULL) or perhaps (in the reflective
3969   // case) a computed value derived from the layout_helper.
3970   Node* initial_slow_test = NULL;
3971   if (layout_is_con) {
3972     assert(!StressReflectiveCode, &quot;stress mode does not use these paths&quot;);
3973     bool must_go_slow = Klass::layout_helper_needs_slow_path(layout_con);
3974     initial_slow_test = must_go_slow ? intcon(1) : extra_slow_test;
3975   } else {   // reflective case
3976     // This reflective path is used by Unsafe.allocateInstance.
3977     // (It may be stress-tested by specifying StressReflectiveCode.)
</pre>
<hr />
<pre>
4002     (*return_size_val) = size;
4003   }
4004 
4005   // This is a precise notnull oop of the klass.
4006   // (Actually, it need not be precise if this is a reflective allocation.)
4007   // It&#39;s what we cast the result to.
4008   const TypeKlassPtr* tklass = _gvn.type(klass_node)-&gt;isa_klassptr();
4009   if (!tklass)  tklass = TypeKlassPtr::OBJECT;
4010   const TypeOopPtr* oop_type = tklass-&gt;as_instance_type();
4011 
4012   // Now generate allocation code
4013 
4014   // The entire memory state is needed for slow path of the allocation
4015   // since GC and deoptimization can happen.
4016   Node *mem = reset_memory();
4017   set_all_memory(mem); // Create new memory state
4018 
4019   AllocateNode* alloc = new AllocateNode(C, AllocateNode::alloc_type(Type::TOP),
4020                                          control(), mem, i_o(),
4021                                          size, klass_node,
<span class="line-modified">4022                                          initial_slow_test, value_node);</span>
4023 
4024   return set_output_for_allocation(alloc, oop_type, deoptimize_on_exception);
4025 }
4026 
4027 // With compressed oops, the 64 bit init value for non flattened value
4028 // arrays is built from 2 32 bit compressed oops
4029 static Node* raw_default_for_coops(Node* default_value, GraphKit&amp; kit) {
4030   Node* lower = kit.gvn().transform(new CastP2XNode(kit.control(), default_value));
4031   Node* upper = kit.gvn().transform(new LShiftLNode(lower, kit.intcon(32)));
4032   return kit.gvn().transform(new OrLNode(lower, upper));
4033 }
4034 
4035 //-------------------------------new_array-------------------------------------
4036 // helper for newarray and anewarray
4037 // The &#39;length&#39; parameter is (obviously) the length of the array.
4038 // See comments on new_instance for the meaning of the other arguments.
4039 Node* GraphKit::new_array(Node* klass_node,     // array klass (maybe variable)
4040                           Node* length,         // number of array elements
4041                           int   nargs,          // number of arguments to push back for uncommon trap
4042                           Node* *return_size_val,
</pre>
<hr />
<pre>
4074     // Increase the size limit if we have exact knowledge of array type.
4075     int log2_esize = Klass::layout_helper_log2_element_size(layout_con);
4076     fast_size_limit &lt;&lt;= MAX2(LogBytesPerLong - log2_esize, 0);
4077   }
4078 
4079   Node* initial_slow_cmp  = _gvn.transform( new CmpUNode( length, intcon( fast_size_limit ) ) );
4080   Node* initial_slow_test = _gvn.transform( new BoolNode( initial_slow_cmp, BoolTest::gt ) );
4081 
4082   // --- Size Computation ---
4083   // array_size = round_to_heap(array_header + (length &lt;&lt; elem_shift));
4084   // where round_to_heap(x) == align_to(x, MinObjAlignmentInBytes)
4085   // and align_to(x, y) == ((x + y-1) &amp; ~(y-1))
4086   // The rounding mask is strength-reduced, if possible.
4087   int round_mask = MinObjAlignmentInBytes - 1;
4088   Node* header_size = NULL;
4089   int   header_size_min  = arrayOopDesc::base_offset_in_bytes(T_BYTE);
4090   // (T_BYTE has the weakest alignment and size restrictions...)
4091   if (layout_is_con) {
4092     int       hsize  = Klass::layout_helper_header_size(layout_con);
4093     int       eshift = Klass::layout_helper_log2_element_size(layout_con);
<span class="line-modified">4094     bool is_value_array = Klass::layout_helper_is_flatArray(layout_con);</span>
4095     if ((round_mask &amp; ~right_n_bits(eshift)) == 0)
4096       round_mask = 0;  // strength-reduce it if it goes away completely
<span class="line-modified">4097     assert(is_value_array || (hsize &amp; right_n_bits(eshift)) == 0, &quot;hsize is pre-rounded&quot;);</span>
4098     assert(header_size_min &lt;= hsize, &quot;generic minimum is smallest&quot;);
4099     header_size_min = hsize;
4100     header_size = intcon(hsize + round_mask);
4101   } else {
4102     Node* hss   = intcon(Klass::_lh_header_size_shift);
4103     Node* hsm   = intcon(Klass::_lh_header_size_mask);
4104     Node* hsize = _gvn.transform( new URShiftINode(layout_val, hss) );
4105     hsize       = _gvn.transform( new AndINode(hsize, hsm) );
4106     Node* mask  = intcon(round_mask);
4107     header_size = _gvn.transform( new AddINode(hsize, mask) );
4108   }
4109 
4110   Node* elem_shift = NULL;
4111   if (layout_is_con) {
4112     int eshift = Klass::layout_helper_log2_element_size(layout_con);
4113     if (eshift != 0)
4114       elem_shift = intcon(eshift);
4115   } else {
4116     // There is no need to mask or shift this value.
4117     // The semantics of LShiftINode include an implicit mask to 0x1F.
</pre>
<hr />
<pre>
4174     // This is the size
4175     (*return_size_val) = size;
4176   }
4177 
4178   // Now generate allocation code
4179 
4180   // The entire memory state is needed for slow path of the allocation
4181   // since GC and deoptimization can happen.
4182   Node *mem = reset_memory();
4183   set_all_memory(mem); // Create new memory state
4184 
4185   if (initial_slow_test-&gt;is_Bool()) {
4186     // Hide it behind a CMoveI, or else PhaseIdealLoop::split_up will get sick.
4187     initial_slow_test = initial_slow_test-&gt;as_Bool()-&gt;as_int_value(&amp;_gvn);
4188   }
4189 
4190   const TypeKlassPtr* ary_klass = _gvn.type(klass_node)-&gt;isa_klassptr();
4191   const TypeOopPtr* ary_type = ary_klass-&gt;as_instance_type();
4192   const TypeAryPtr* ary_ptr = ary_type-&gt;isa_aryptr();
4193 
<span class="line-modified">4194   // Value type array variants:</span>
4195   // - null-ok:              MyValue.ref[] (ciObjArrayKlass &quot;[LMyValue$ref&quot;)
4196   // - null-free:            MyValue.val[] (ciObjArrayKlass &quot;[QMyValue$val&quot;)
<span class="line-modified">4197   // - null-free, flattened: MyValue.val[] (ciValueArrayKlass &quot;[QMyValue$val&quot;)</span>
<span class="line-modified">4198   // Check if array is a null-free, non-flattened value type array</span>
<span class="line-modified">4199   // that needs to be initialized with the default value type.</span>
4200   Node* default_value = NULL;
4201   Node* raw_default_value = NULL;
4202   if (ary_ptr != NULL &amp;&amp; ary_ptr-&gt;klass_is_exact()) {
4203     // Array type is known
4204     ciKlass* elem_klass = ary_ptr-&gt;klass()-&gt;as_array_klass()-&gt;element_klass();
<span class="line-modified">4205     if (elem_klass != NULL &amp;&amp; elem_klass-&gt;is_valuetype()) {</span>
<span class="line-modified">4206       ciValueKlass* vk = elem_klass-&gt;as_value_klass();</span>
4207       if (!vk-&gt;flatten_array()) {
<span class="line-modified">4208         default_value = ValueTypeNode::default_oop(gvn(), vk);</span>
4209         if (UseCompressedOops) {
4210           default_value = _gvn.transform(new EncodePNode(default_value, default_value-&gt;bottom_type()-&gt;make_narrowoop()));
4211           raw_default_value = raw_default_for_coops(default_value, *this);
4212         } else {
4213           raw_default_value = _gvn.transform(new CastP2XNode(control(), default_value));
4214         }
4215       }
4216     }
<span class="line-modified">4217   } else if (ary_klass-&gt;klass()-&gt;can_be_value_array_klass()) {</span>
4218     // Array type is not known, add runtime checks
4219     assert(!ary_klass-&gt;klass_is_exact(), &quot;unexpected exact type&quot;);
4220     Node* r = new RegionNode(4);
4221     default_value = new PhiNode(r, TypeInstPtr::BOTTOM);
4222 
4223     // Check if array is an object array
4224     Node* cmp = gen_lh_array_test(klass_node, Klass::_lh_array_tag_obj_value);
4225     Node* bol = _gvn.transform(new BoolNode(cmp, BoolTest::eq));
4226     IfNode* iff = create_and_map_if(control(), bol, PROB_FAIR, COUNT_UNKNOWN);
4227 
4228     // Not an object array, initialize with all zero
4229     r-&gt;init_req(1, _gvn.transform(new IfFalseNode(iff)));
4230     default_value-&gt;init_req(1, null());
4231 
4232     // Object array, check if null-free
4233     set_control(_gvn.transform(new IfTrueNode(iff)));
4234     Node* lhp = basic_plus_adr(klass_node, in_bytes(Klass::layout_helper_offset()));
4235     Node* layout_val = _gvn.transform(LoadNode::make(_gvn, NULL, immutable_memory(), lhp, lhp-&gt;bottom_type()-&gt;is_ptr(), TypeInt::INT, T_INT, MemNode::unordered));
4236     Node* null_free = _gvn.transform(new RShiftINode(layout_val, intcon(Klass::_lh_null_free_shift)));
4237     null_free = _gvn.transform(new AndINode(null_free, intcon(Klass::_lh_null_free_mask)));
4238     cmp = _gvn.transform(new CmpINode(null_free, intcon(0)));
4239     bol = _gvn.transform(new BoolNode(cmp, BoolTest::ne));
4240     iff = create_and_map_if(control(), bol, PROB_FAIR, COUNT_UNKNOWN);
4241 
4242     // Not null-free, initialize with all zero
4243     r-&gt;init_req(2, _gvn.transform(new IfFalseNode(iff)));
4244     default_value-&gt;init_req(2, null());
4245 
<span class="line-modified">4246     // Null-free, non-flattened value array, initialize with the default value</span>
4247     set_control(_gvn.transform(new IfTrueNode(iff)));
4248     Node* p = basic_plus_adr(klass_node, in_bytes(ArrayKlass::element_klass_offset()));
4249     Node* eklass = _gvn.transform(LoadKlassNode::make(_gvn, control(), immutable_memory(), p, TypeInstPtr::KLASS));
4250     Node* adr_fixed_block_addr = basic_plus_adr(eklass, in_bytes(InstanceKlass::adr_inlineklass_fixed_block_offset()));
4251     Node* adr_fixed_block = make_load(control(), adr_fixed_block_addr, TypeRawPtr::NOTNULL, T_ADDRESS, MemNode::unordered);
4252     Node* default_value_offset_addr = basic_plus_adr(adr_fixed_block, in_bytes(InlineKlass::default_value_offset_offset()));
4253     Node* default_value_offset = make_load(control(), default_value_offset_addr, TypeInt::INT, T_INT, MemNode::unordered);
4254     Node* elem_mirror = load_mirror_from_klass(eklass);
4255     Node* default_value_addr = basic_plus_adr(elem_mirror, ConvI2X(default_value_offset));
4256     Node* val = access_load_at(elem_mirror, default_value_addr, _gvn.type(default_value_addr)-&gt;is_ptr(), TypeInstPtr::BOTTOM, T_OBJECT, IN_HEAP);
4257     r-&gt;init_req(3, control());
4258     default_value-&gt;init_req(3, val);
4259 
4260     set_control(_gvn.transform(r));
4261     default_value = _gvn.transform(default_value);
4262     if (UseCompressedOops) {
4263       default_value = _gvn.transform(new EncodePNode(default_value, default_value-&gt;bottom_type()-&gt;make_narrowoop()));
4264       raw_default_value = raw_default_for_coops(default_value, *this);
4265     } else {
4266       raw_default_value = _gvn.transform(new CastP2XNode(control(), default_value));
</pre>
<hr />
<pre>
4571 
4572   set_control(IfFalse(iff));
4573   set_memory(st, TypeAryPtr::BYTES);
4574 }
4575 
4576 Node* GraphKit::make_constant_from_field(ciField* field, Node* obj) {
4577   if (!field-&gt;is_constant()) {
4578     return NULL; // Field not marked as constant.
4579   }
4580   ciInstance* holder = NULL;
4581   if (!field-&gt;is_static()) {
4582     ciObject* const_oop = obj-&gt;bottom_type()-&gt;is_oopptr()-&gt;const_oop();
4583     if (const_oop != NULL &amp;&amp; const_oop-&gt;is_instance()) {
4584       holder = const_oop-&gt;as_instance();
4585     }
4586   }
4587   const Type* con_type = Type::make_constant_from_field(field, holder, field-&gt;layout_type(),
4588                                                         /*is_unsigned_load=*/false);
4589   if (con_type != NULL) {
4590     Node* con = makecon(con_type);
<span class="line-modified">4591     assert(!field-&gt;type()-&gt;is_valuetype() || (field-&gt;is_static() &amp;&amp; !con_type-&gt;is_zero_type()), &quot;sanity&quot;);</span>
4592     // Check type of constant which might be more precise
<span class="line-modified">4593     if (con_type-&gt;is_valuetypeptr() &amp;&amp; con_type-&gt;value_klass()-&gt;is_scalarizable()) {</span>
<span class="line-modified">4594       // Load value type from constant oop</span>
<span class="line-modified">4595       con = ValueTypeNode::make_from_oop(this, con, con_type-&gt;value_klass());</span>
4596     }
4597     return con;
4598   }
4599   return NULL;
4600 }
4601 
4602 //---------------------------load_mirror_from_klass----------------------------
4603 // Given a klass oop, load its java mirror (a java.lang.Class oop).
4604 Node* GraphKit::load_mirror_from_klass(Node* klass) {
4605   Node* p = basic_plus_adr(klass, in_bytes(Klass::java_mirror_offset()));
4606   Node* load = make_load(NULL, p, TypeRawPtr::NOTNULL, T_ADDRESS, MemNode::unordered);
4607   // mirror = ((OopHandle)mirror)-&gt;resolve();
4608   return access_load(load, TypeInstPtr::MIRROR, T_OBJECT, IN_NATIVE);
4609 }
</pre>
</td>
<td>
<hr />
<pre>
   6  * under the terms of the GNU General Public License version 2 only, as
   7  * published by the Free Software Foundation.
   8  *
   9  * This code is distributed in the hope that it will be useful, but WITHOUT
  10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #include &quot;precompiled.hpp&quot;
<span class="line-added">  26 #include &quot;ci/ciFlatArrayKlass.hpp&quot;</span>
<span class="line-added">  27 #include &quot;ci/ciInlineKlass.hpp&quot;</span>
  28 #include &quot;ci/ciUtilities.hpp&quot;
  29 #include &quot;compiler/compileLog.hpp&quot;

  30 #include &quot;gc/shared/barrierSet.hpp&quot;
  31 #include &quot;gc/shared/c2/barrierSetC2.hpp&quot;
  32 #include &quot;interpreter/interpreter.hpp&quot;
  33 #include &quot;memory/resourceArea.hpp&quot;
  34 #include &quot;opto/addnode.hpp&quot;
  35 #include &quot;opto/castnode.hpp&quot;
  36 #include &quot;opto/convertnode.hpp&quot;
  37 #include &quot;opto/graphKit.hpp&quot;
  38 #include &quot;opto/idealKit.hpp&quot;
<span class="line-added">  39 #include &quot;opto/inlinetypenode.hpp&quot;</span>
  40 #include &quot;opto/intrinsicnode.hpp&quot;
  41 #include &quot;opto/locknode.hpp&quot;
  42 #include &quot;opto/machnode.hpp&quot;
  43 #include &quot;opto/narrowptrnode.hpp&quot;
  44 #include &quot;opto/opaquenode.hpp&quot;
  45 #include &quot;opto/parse.hpp&quot;
  46 #include &quot;opto/rootnode.hpp&quot;
  47 #include &quot;opto/runtime.hpp&quot;
  48 #include &quot;opto/subtypenode.hpp&quot;

  49 #include &quot;runtime/deoptimization.hpp&quot;
  50 #include &quot;runtime/sharedRuntime.hpp&quot;
  51 #include &quot;utilities/bitMap.inline.hpp&quot;
  52 #include &quot;utilities/powerOfTwo.hpp&quot;
  53 
  54 //----------------------------GraphKit-----------------------------------------
  55 // Main utility constructor.
  56 GraphKit::GraphKit(JVMState* jvms, PhaseGVN* gvn)
  57   : Phase(Phase::Parser),
  58     _env(C-&gt;env()),
  59     _gvn((gvn != NULL) ? *gvn : *C-&gt;initial_gvn()),
  60     _barrier_set(BarrierSet::barrier_set()-&gt;barrier_set_c2())
  61 {
  62   assert(gvn == NULL || !gvn-&gt;is_IterGVN() || gvn-&gt;is_IterGVN()-&gt;delay_transform(), &quot;delay transform should be enabled&quot;);
  63   _exceptions = jvms-&gt;map()-&gt;next_exception();
  64   if (_exceptions != NULL)  jvms-&gt;map()-&gt;set_next_exception(NULL);
  65   set_jvms(jvms);
  66 #ifdef ASSERT
  67   if (_gvn.is_IterGVN() != NULL) {
  68     assert(_gvn.is_IterGVN()-&gt;delay_transform(), &quot;Transformation must be delayed if IterGVN is used&quot;);
</pre>
<hr />
<pre>
1381   }
1382 
1383   if (assert_null) {
1384     // Cast obj to null on this path.
1385     replace_in_map(value, zerocon(type));
1386     return zerocon(type);
1387   }
1388 
1389   // Cast obj to not-null on this path, if there is no null_control.
1390   // (If there is a null_control, a non-null value may come back to haunt us.)
1391   if (type == T_OBJECT) {
1392     Node* cast = cast_not_null(value, false);
1393     if (null_control == NULL || (*null_control) == top())
1394       replace_in_map(value, cast);
1395     value = cast;
1396   }
1397 
1398   return value;
1399 }
1400 
<span class="line-modified">1401 Node* GraphKit::null2default(Node* value, ciInlineKlass* vk) {</span>
1402   Node* null_ctl = top();
1403   value = null_check_oop(value, &amp;null_ctl);
1404   if (!null_ctl-&gt;is_top()) {
1405     // Return default value if oop is null
1406     Node* region = new RegionNode(3);
1407     region-&gt;init_req(1, control());
1408     region-&gt;init_req(2, null_ctl);
1409     value = PhiNode::make(region, value, TypeInstPtr::make(TypePtr::BotPTR, vk));
<span class="line-modified">1410     value-&gt;set_req(2, InlineTypeNode::default_oop(gvn(), vk));</span>
1411     set_control(gvn().transform(region));
1412     value = gvn().transform(value);
1413   }
1414   return value;
1415 }
1416 
1417 //------------------------------cast_not_null----------------------------------
1418 // Cast obj to not-null on this path
1419 Node* GraphKit::cast_not_null(Node* obj, bool do_replace_in_map) {
<span class="line-modified">1420   if (obj-&gt;is_InlineType()) {</span>
1421     return obj;
1422   }
1423   const Type *t = _gvn.type(obj);
1424   const Type *t_not_null = t-&gt;join_speculative(TypePtr::NOTNULL);
1425   // Object is already not-null?
1426   if( t == t_not_null ) return obj;
1427 
1428   Node *cast = new CastPPNode(obj,t_not_null);
1429   cast-&gt;init_req(0, control());
1430   cast = _gvn.transform( cast );
1431 
<span class="line-modified">1432   if (t-&gt;is_inlinetypeptr() &amp;&amp; t-&gt;inline_klass()-&gt;is_scalarizable()) {</span>
<span class="line-modified">1433     // Scalarize inline type now that we know it&#39;s non-null</span>
<span class="line-modified">1434     cast = InlineTypeNode::make_from_oop(this, cast, t-&gt;inline_klass())-&gt;buffer(this, false);</span>
1435   }
1436 
1437   // Scan for instances of &#39;obj&#39; in the current JVM mapping.
1438   // These instances are known to be not-null after the test.
1439   if (do_replace_in_map)
1440     replace_in_map(obj, cast);
1441 
1442   return cast;                  // Return casted value
1443 }
1444 
1445 // Sometimes in intrinsics, we implicitly know an object is not null
1446 // (there&#39;s no actual null check) so we can cast it to not null. In
1447 // the course of optimizations, the input to the cast can become null.
1448 // In that case that data path will die and we need the control path
1449 // to become dead as well to keep the graph consistent. So we have to
1450 // add a check for null for which one branch can&#39;t be taken. It uses
1451 // an Opaque4 node that will cause the check to be removed after loop
1452 // opts so the test goes away and the compiled code doesn&#39;t execute a
1453 // useless check.
1454 Node* GraphKit::must_be_not_null(Node* value, bool do_replace_in_map) {
</pre>
<hr />
<pre>
1606 Node* GraphKit::access_store_at(Node* obj,
1607                                 Node* adr,
1608                                 const TypePtr* adr_type,
1609                                 Node* val,
1610                                 const Type* val_type,
1611                                 BasicType bt,
1612                                 DecoratorSet decorators,
1613                                 bool safe_for_replace) {
1614   // Transformation of a value which could be NULL pointer (CastPP #NULL)
1615   // could be delayed during Parse (for example, in adjust_map_after_if()).
1616   // Execute transformation here to avoid barrier generation in such case.
1617   if (_gvn.type(val) == TypePtr::NULL_PTR) {
1618     val = _gvn.makecon(TypePtr::NULL_PTR);
1619   }
1620 
1621   if (stopped()) {
1622     return top(); // Dead path ?
1623   }
1624 
1625   assert(val != NULL, &quot;not dead path&quot;);
<span class="line-modified">1626   if (val-&gt;is_InlineType()) {</span>
1627     // Store to non-flattened field. Buffer the inline type and make sure
1628     // the store is re-executed if the allocation triggers deoptimization.
1629     PreserveReexecuteState preexecs(this);
1630     jvms()-&gt;set_should_reexecute(true);
<span class="line-modified">1631     val = val-&gt;as_InlineType()-&gt;buffer(this, safe_for_replace);</span>
1632   }
1633 
1634   C2AccessValuePtr addr(adr, adr_type);
1635   C2AccessValue value(val, val_type);
1636   C2ParseAccess access(this, decorators | C2_WRITE_ACCESS, bt, obj, addr);
1637   if (access.is_raw()) {
1638     return _barrier_set-&gt;BarrierSetC2::store_at(access, value);
1639   } else {
1640     return _barrier_set-&gt;store_at(access, value);
1641   }
1642 }
1643 
1644 Node* GraphKit::access_load_at(Node* obj,   // containing obj
1645                                Node* adr,   // actual adress to store val at
1646                                const TypePtr* adr_type,
1647                                const Type* val_type,
1648                                BasicType bt,
1649                                DecoratorSet decorators,
1650                                Node* ctl) {
1651   if (stopped()) {
</pre>
<hr />
<pre>
1743                                      BasicType bt,
1744                                      DecoratorSet decorators) {
1745   C2AccessValuePtr addr(adr, adr_type);
1746   C2AtomicParseAccess access(this, decorators | C2_READ_ACCESS | C2_WRITE_ACCESS, bt, obj, addr, alias_idx);
1747   if (access.is_raw()) {
1748     return _barrier_set-&gt;BarrierSetC2::atomic_add_at(access, new_val, value_type);
1749   } else {
1750     return _barrier_set-&gt;atomic_add_at(access, new_val, value_type);
1751   }
1752 }
1753 
1754 void GraphKit::access_clone(Node* src_base, Node* dst_base, Node* countx, bool is_array) {
1755   return _barrier_set-&gt;clone(this, src_base, dst_base, countx, is_array);
1756 }
1757 
1758 //-------------------------array_element_address-------------------------
1759 Node* GraphKit::array_element_address(Node* ary, Node* idx, BasicType elembt,
1760                                       const TypeInt* sizetype, Node* ctrl) {
1761   uint shift  = exact_log2(type2aelembytes(elembt));
1762   ciKlass* arytype_klass = _gvn.type(ary)-&gt;is_aryptr()-&gt;klass();
<span class="line-modified">1763   if (arytype_klass != NULL &amp;&amp; arytype_klass-&gt;is_flat_array_klass()) {</span>
<span class="line-modified">1764     ciFlatArrayKlass* vak = arytype_klass-&gt;as_flat_array_klass();</span>
1765     shift = vak-&gt;log2_element_size();
1766   }
1767   uint header = arrayOopDesc::base_offset_in_bytes(elembt);
1768 
1769   // short-circuit a common case (saves lots of confusing waste motion)
1770   jint idx_con = find_int_con(idx, -1);
1771   if (idx_con &gt;= 0) {
1772     intptr_t offset = header + ((intptr_t)idx_con &lt;&lt; shift);
1773     return basic_plus_adr(ary, offset);
1774   }
1775 
1776   // must be correct type for alignment purposes
1777   Node* base  = basic_plus_adr(ary, header);
1778   idx = Compile::conv_I2X_index(&amp;_gvn, idx, sizetype, ctrl);
1779   Node* scale = _gvn.transform( new LShiftXNode(idx, intcon(shift)) );
1780   return basic_plus_adr(ary, base, scale);
1781 }
1782 
1783 //-------------------------load_array_element-------------------------
1784 Node* GraphKit::load_array_element(Node* ctl, Node* ary, Node* idx, const TypeAryPtr* arytype) {
1785   const Type* elemtype = arytype-&gt;elem();
1786   BasicType elembt = elemtype-&gt;array_element_basic_type();
<span class="line-modified">1787   assert(elembt != T_INLINE_TYPE, &quot;inline types are not supported by this method&quot;);</span>
1788   Node* adr = array_element_address(ary, idx, elembt, arytype-&gt;size());
1789   if (elembt == T_NARROWOOP) {
1790     elembt = T_OBJECT; // To satisfy switch in LoadNode::make()
1791   }
1792   Node* ld = make_load(ctl, adr, elemtype, elembt, arytype, MemNode::unordered);
1793   return ld;
1794 }
1795 
1796 //-------------------------set_arguments_for_java_call-------------------------
1797 // Arguments (pre-popped from the stack) are taken from the JVMS.
1798 void GraphKit::set_arguments_for_java_call(CallJavaNode* call, bool is_late_inline) {
1799   PreserveReexecuteState preexecs(this);
1800   if (EnableValhalla) {
<span class="line-modified">1801     // Make sure the call is re-executed, if buffering of inline type arguments triggers deoptimization</span>
1802     jvms()-&gt;set_should_reexecute(true);
1803     int arg_size = method()-&gt;get_declared_signature_at_bci(bci())-&gt;arg_size_for_bc(java_bc());
1804     inc_sp(arg_size);
1805   }
1806   // Add the call arguments
1807   const TypeTuple* domain = call-&gt;tf()-&gt;domain_sig();
1808   ExtendedSignature sig_cc = ExtendedSignature(call-&gt;method()-&gt;get_sig_cc(), SigEntryFilter());
1809   uint nargs = domain-&gt;cnt();
1810   for (uint i = TypeFunc::Parms, idx = TypeFunc::Parms; i &lt; nargs; i++) {
1811     Node* arg = argument(i-TypeFunc::Parms);
1812     const Type* t = domain-&gt;field_at(i);
<span class="line-modified">1813     if (call-&gt;method()-&gt;has_scalarized_args() &amp;&amp; t-&gt;is_inlinetypeptr() &amp;&amp; !t-&gt;maybe_null()) {</span>
<span class="line-modified">1814       // We don&#39;t pass inline type arguments by reference but instead pass each field of the inline type</span>
<span class="line-modified">1815       InlineTypeNode* vt = arg-&gt;as_InlineType();</span>
1816       vt-&gt;pass_fields(this, call, sig_cc, idx);
<span class="line-modified">1817       // If an inline type argument is passed as fields, attach the Method* to the call site</span>
1818       // to be able to access the extended signature later via attached_method_before_pc().
1819       // For example, see CompiledMethod::preserve_callee_argument_oops().
1820       call-&gt;set_override_symbolic_info(true);
1821       continue;
<span class="line-modified">1822     } else if (arg-&gt;is_InlineType()) {</span>
<span class="line-modified">1823       // Pass inline type argument via oop to callee</span>
<span class="line-modified">1824       arg = arg-&gt;as_InlineType()-&gt;buffer(this);</span>
1825       if (!is_late_inline) {
<span class="line-modified">1826         arg = arg-&gt;as_InlineTypePtr()-&gt;get_oop();</span>
1827       }
1828     }
1829     call-&gt;init_req(idx++, arg);
1830     // Skip reserved arguments
1831     BasicType bt = t-&gt;basic_type();
1832     while (SigEntry::next_is_reserved(sig_cc, bt, true)) {
1833       call-&gt;init_req(idx++, top());
1834       if (type2size[bt] == 2) {
1835         call-&gt;init_req(idx++, top());
1836       }
1837     }
1838   }
1839 }
1840 
1841 //---------------------------set_edges_for_java_call---------------------------
1842 // Connect a newly created call into the current JVMS.
1843 // A return value node (if any) is returned from set_edges_for_java_call.
1844 void GraphKit::set_edges_for_java_call(CallJavaNode* call, bool must_throw, bool separate_io_proj) {
1845 
1846   // Add the predefined inputs:
</pre>
<hr />
<pre>
1872 Node* GraphKit::set_results_for_java_call(CallJavaNode* call, bool separate_io_proj, bool deoptimize) {
1873   if (stopped())  return top();  // maybe the call folded up?
1874 
1875   // Note:  Since any out-of-line call can produce an exception,
1876   // we always insert an I_O projection from the call into the result.
1877 
1878   make_slow_call_ex(call, env()-&gt;Throwable_klass(), separate_io_proj, deoptimize);
1879 
1880   if (separate_io_proj) {
1881     // The caller requested separate projections be used by the fall
1882     // through and exceptional paths, so replace the projections for
1883     // the fall through path.
1884     set_i_o(_gvn.transform( new ProjNode(call, TypeFunc::I_O) ));
1885     set_all_memory(_gvn.transform( new ProjNode(call, TypeFunc::Memory) ));
1886   }
1887 
1888   // Capture the return value, if any.
1889   Node* ret;
1890   if (call-&gt;method() == NULL || call-&gt;method()-&gt;return_type()-&gt;basic_type() == T_VOID) {
1891     ret = top();
<span class="line-modified">1892   } else if (call-&gt;tf()-&gt;returns_inline_type_as_fields()) {</span>
<span class="line-modified">1893     // Return of multiple values (inline type fields): we create a</span>
<span class="line-modified">1894     // InlineType node, each field is a projection from the call.</span>
<span class="line-modified">1895     ciInlineKlass* vk = call-&gt;method()-&gt;return_type()-&gt;as_inline_klass();</span>
1896     const Array&lt;SigEntry&gt;* sig_array = vk-&gt;extended_sig();
1897     GrowableArray&lt;SigEntry&gt; sig = GrowableArray&lt;SigEntry&gt;(sig_array-&gt;length());
1898     sig.appendAll(sig_array);
1899     ExtendedSignature sig_cc = ExtendedSignature(&amp;sig, SigEntryFilter());
1900     uint base_input = TypeFunc::Parms + 1;
<span class="line-modified">1901     ret = InlineTypeNode::make_from_multi(this, call, sig_cc, vk, base_input, false);</span>
1902   } else {
1903     ret = _gvn.transform(new ProjNode(call, TypeFunc::Parms));
1904   }
1905 
1906   return ret;
1907 }
1908 
1909 //--------------------set_predefined_input_for_runtime_call--------------------
1910 // Reading and setting the memory state is way conservative here.
1911 // The real problem is that I am not doing real Type analysis on memory,
1912 // so I cannot distinguish card mark stores from other stores.  Across a GC
1913 // point the Store Barrier and the card mark memory has to agree.  I cannot
1914 // have a card mark store and its barrier split across the GC point from
1915 // either above or below.  Here I get that to happen by reading ALL of memory.
1916 // A better answer would be to separate out card marks from other memory.
1917 // For now, return the input memory state, so that it can be reused
1918 // after the call, if this call has restricted memory effects.
1919 Node* GraphKit::set_predefined_input_for_runtime_call(SafePointNode* call, Node* narrow_mem) {
1920   // Set fixed predefined input arguments
1921   Node* memory = reset_memory();
</pre>
<hr />
<pre>
2913   // Now do a linear scan of the secondary super-klass array.  Again, no real
2914   // performance impact (too rare) but it&#39;s gotta be done.
2915   // Since the code is rarely used, there is no penalty for moving it
2916   // out of line, and it can only improve I-cache density.
2917   // The decision to inline or out-of-line this final check is platform
2918   // dependent, and is found in the AD file definition of PartialSubtypeCheck.
2919   Node* psc = gvn.transform(
2920     new PartialSubtypeCheckNode(*ctrl, subklass, superklass));
2921 
2922   IfNode *iff4 = gen_subtype_check_compare(*ctrl, psc, gvn.zerocon(T_OBJECT), BoolTest::ne, PROB_FAIR, gvn, T_ADDRESS);
2923   r_not_subtype-&gt;init_req(2, gvn.transform(new IfTrueNode (iff4)));
2924   r_ok_subtype -&gt;init_req(3, gvn.transform(new IfFalseNode(iff4)));
2925 
2926   // Return false path; set default control to true path.
2927   *ctrl = gvn.transform(r_ok_subtype);
2928   return gvn.transform(r_not_subtype);
2929 }
2930 
2931 Node* GraphKit::gen_subtype_check(Node* obj_or_subklass, Node* superklass) {
2932   const Type* sub_t = _gvn.type(obj_or_subklass);
<span class="line-modified">2933   if (sub_t-&gt;isa_inlinetype()) {</span>
<span class="line-modified">2934     obj_or_subklass = makecon(TypeKlassPtr::make(sub_t-&gt;inline_klass()));</span>
2935   }
2936   if (ExpandSubTypeCheckAtParseTime) {
2937     MergeMemNode* mem = merged_memory();
2938     Node* ctrl = control();
2939     Node* subklass = obj_or_subklass;
2940     if (!sub_t-&gt;isa_klassptr()) {
2941       subklass = load_object_klass(obj_or_subklass);
2942     }
2943     Node* n = Phase::gen_subtype_check(subklass, superklass, &amp;ctrl, mem, _gvn);
2944     set_control(ctrl);
2945     return n;
2946   }
2947 
2948   Node* check = _gvn.transform(new SubTypeCheckNode(C, obj_or_subklass, superklass));
2949   Node* bol = _gvn.transform(new BoolNode(check, BoolTest::eq));
2950   IfNode* iff = create_and_xform_if(control(), bol, PROB_STATIC_FREQUENT, COUNT_UNKNOWN);
2951   set_control(_gvn.transform(new IfTrueNode(iff)));
2952   return _gvn.transform(new IfFalseNode(iff));
2953 }
2954 
2955 // Profile-driven exact type check:
2956 Node* GraphKit::type_check_receiver(Node* receiver, ciKlass* klass,
2957                                     float prob,
2958                                     Node* *casted_receiver) {
2959   const TypeKlassPtr* tklass = TypeKlassPtr::make(klass);
2960   Node* recv_klass = load_object_klass(receiver);
2961   Node* fail = type_check(recv_klass, tklass, prob);
2962   const TypeOopPtr* recv_xtype = tklass-&gt;as_instance_type();
2963   assert(recv_xtype-&gt;klass_is_exact(), &quot;&quot;);
2964 
2965   // Subsume downstream occurrences of receiver with a cast to
2966   // recv_xtype, since now we know what the type will be.
2967   Node* cast = new CheckCastPPNode(control(), receiver, recv_xtype);
2968   Node* res = _gvn.transform(cast);
<span class="line-modified">2969   if (recv_xtype-&gt;is_inlinetypeptr() &amp;&amp; recv_xtype-&gt;inline_klass()-&gt;is_scalarizable()) {</span>
2970     assert(!gvn().type(res)-&gt;maybe_null(), &quot;receiver should never be null&quot;);
<span class="line-modified">2971     res = InlineTypeNode::make_from_oop(this, res, recv_xtype-&gt;inline_klass());</span>
2972   }
2973 
2974   (*casted_receiver) = res;
2975   // (User must make the replace_in_map call.)
2976 
2977   return fail;
2978 }
2979 
2980 Node* GraphKit::type_check(Node* recv_klass, const TypeKlassPtr* tklass,
2981                            float prob) {
2982   Node* want_klass = makecon(tklass);
2983   Node* cmp = _gvn.transform( new CmpPNode(recv_klass, want_klass));
2984   Node* bol = _gvn.transform( new BoolNode(cmp, BoolTest::eq) );
2985   IfNode* iff = create_and_xform_if(control(), bol, prob, COUNT_UNKNOWN);
2986   set_control(  _gvn.transform( new IfTrueNode (iff)));
2987   Node* fail = _gvn.transform( new IfFalseNode(iff));
2988   return fail;
2989 }
2990 
2991 //------------------------------subtype_check_receiver-------------------------
</pre>
<hr />
<pre>
3210 // and the reflective instance-of call.
3211 Node* GraphKit::gen_instanceof(Node* obj, Node* superklass, bool safe_for_replace) {
3212   kill_dead_locals();           // Benefit all the uncommon traps
3213   assert( !stopped(), &quot;dead parse path should be checked in callers&quot; );
3214   assert(!TypePtr::NULL_PTR-&gt;higher_equal(_gvn.type(superklass)-&gt;is_klassptr()),
3215          &quot;must check for not-null not-dead klass in callers&quot;);
3216 
3217   // Make the merge point
3218   enum { _obj_path = 1, _fail_path, _null_path, PATH_LIMIT };
3219   RegionNode* region = new RegionNode(PATH_LIMIT);
3220   Node*       phi    = new PhiNode(region, TypeInt::BOOL);
3221   C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
3222 
3223   ciProfileData* data = NULL;
3224   if (java_bc() == Bytecodes::_instanceof) {  // Only for the bytecode
3225     data = method()-&gt;method_data()-&gt;bci_to_data(bci());
3226   }
3227   bool speculative_not_null = false;
3228   bool never_see_null = (ProfileDynamicTypes  // aggressive use of profile
3229                          &amp;&amp; seems_never_null(obj, data, speculative_not_null));
<span class="line-modified">3230   bool is_value = obj-&gt;is_InlineType();</span>
3231 
3232   // Null check; get casted pointer; set region slot 3
3233   Node* null_ctl = top();
3234   Node* not_null_obj = is_value ? obj : null_check_oop(obj, &amp;null_ctl, never_see_null, safe_for_replace, speculative_not_null);
3235 
3236   // If not_null_obj is dead, only null-path is taken
3237   if (stopped()) {              // Doing instance-of on a NULL?
3238     set_control(null_ctl);
3239     return intcon(0);
3240   }
3241   region-&gt;init_req(_null_path, null_ctl);
3242   phi   -&gt;init_req(_null_path, intcon(0)); // Set null path value
3243   if (null_ctl == top()) {
3244     // Do this eagerly, so that pattern matches like is_diamond_phi
3245     // will work even during parsing.
3246     assert(_null_path == PATH_LIMIT-1, &quot;delete last&quot;);
3247     region-&gt;del_req(_null_path);
3248     phi   -&gt;del_req(_null_path);
3249   }
3250 
</pre>
<hr />
<pre>
3256       ciKlass* subk = _gvn.type(obj)-&gt;is_oopptr()-&gt;klass();
3257       if (subk != NULL &amp;&amp; subk-&gt;is_loaded()) {
3258         int static_res = C-&gt;static_subtype_check(superk, subk);
3259         known_statically = (static_res == Compile::SSC_always_true || static_res == Compile::SSC_always_false);
3260       }
3261     }
3262 
3263     if (!known_statically) {
3264       const TypeOopPtr* obj_type = _gvn.type(obj)-&gt;is_oopptr();
3265       // We may not have profiling here or it may not help us. If we
3266       // have a speculative type use it to perform an exact cast.
3267       ciKlass* spec_obj_type = obj_type-&gt;speculative_type();
3268       if (spec_obj_type != NULL || (ProfileDynamicTypes &amp;&amp; data != NULL)) {
3269         Node* cast_obj = maybe_cast_profiled_receiver(not_null_obj, NULL, spec_obj_type, safe_for_replace);
3270         if (stopped()) {            // Profile disagrees with this path.
3271           set_control(null_ctl);    // Null is the only remaining possibility.
3272           return intcon(0);
3273         }
3274         if (cast_obj != NULL &amp;&amp;
3275             // A value that&#39;s sometimes null is not something we can optimize well
<span class="line-modified">3276             !(cast_obj-&gt;is_InlineType() &amp;&amp; null_ctl != top())) {</span>
3277           not_null_obj = cast_obj;
<span class="line-modified">3278           is_value = not_null_obj-&gt;is_InlineType();</span>
3279         }
3280       }
3281     }
3282   }
3283 
3284   // Generate the subtype check
3285   Node* not_subtype_ctrl = gen_subtype_check(not_null_obj, superklass);
3286 
3287   // Plug in the success path to the general merge in slot 1.
3288   region-&gt;init_req(_obj_path, control());
3289   phi   -&gt;init_req(_obj_path, intcon(1));
3290 
3291   // Plug in the failing path to the general merge in slot 2.
3292   region-&gt;init_req(_fail_path, not_subtype_ctrl);
3293   phi   -&gt;init_req(_fail_path, intcon(0));
3294 
3295   // Return final merged results
3296   set_control( _gvn.transform(region) );
3297   record_for_igvn(region);
3298 
</pre>
<hr />
<pre>
3303     Node* casted_obj = record_profiled_receiver_for_speculation(obj);
3304     replace_in_map(obj, casted_obj);
3305   }
3306 
3307   return _gvn.transform(phi);
3308 }
3309 
3310 //-------------------------------gen_checkcast---------------------------------
3311 // Generate a checkcast idiom.  Used by both the checkcast bytecode and the
3312 // array store bytecode.  Stack must be as-if BEFORE doing the bytecode so the
3313 // uncommon-trap paths work.  Adjust stack after this call.
3314 // If failure_control is supplied and not null, it is filled in with
3315 // the control edge for the cast failure.  Otherwise, an appropriate
3316 // uncommon trap or exception is thrown.
3317 Node* GraphKit::gen_checkcast(Node *obj, Node* superklass, Node* *failure_control) {
3318   kill_dead_locals();           // Benefit all the uncommon traps
3319   const TypeKlassPtr* tk = _gvn.type(superklass)-&gt;is_klassptr();
3320   const TypeOopPtr* toop = TypeOopPtr::make_from_klass(tk-&gt;klass());
3321 
3322   // Check if inline types are involved
<span class="line-modified">3323   bool from_inline = obj-&gt;is_InlineType();</span>
<span class="line-modified">3324   bool to_inline = tk-&gt;klass()-&gt;is_inlinetype();</span>
3325 
3326   // Fast cutout:  Check the case that the cast is vacuously true.
3327   // This detects the common cases where the test will short-circuit
3328   // away completely.  We do this before we perform the null check,
3329   // because if the test is going to turn into zero code, we don&#39;t
3330   // want a residual null check left around.  (Causes a slowdown,
3331   // for example, in some objArray manipulations, such as a[i]=a[j].)
3332   if (tk-&gt;singleton()) {
3333     ciKlass* klass = NULL;
3334     if (from_inline) {
<span class="line-modified">3335       klass = _gvn.type(obj)-&gt;inline_klass();</span>
3336     } else {
3337       const TypeOopPtr* objtp = _gvn.type(obj)-&gt;isa_oopptr();
3338       if (objtp != NULL) {
3339         klass = objtp-&gt;klass();
3340       }
3341     }
3342     if (klass != NULL) {
3343       switch (C-&gt;static_subtype_check(tk-&gt;klass(), klass)) {
3344       case Compile::SSC_always_true:
3345         // If we know the type check always succeed then we don&#39;t use
3346         // the profiling data at this bytecode. Don&#39;t lose it, feed it
3347         // to the type system as a speculative type.
3348         if (!from_inline) {
3349           obj = record_profiled_receiver_for_speculation(obj);
3350           if (to_inline) {
3351             obj = null_check(obj);
<span class="line-modified">3352             if (toop-&gt;inline_klass()-&gt;is_scalarizable()) {</span>
<span class="line-modified">3353               obj = InlineTypeNode::make_from_oop(this, obj, toop-&gt;inline_klass());</span>
3354             }
3355           }
3356         }
3357         return obj;
3358       case Compile::SSC_always_false:
3359         if (from_inline || to_inline) {
3360           if (!from_inline) {
3361             null_check(obj);
3362           }
<span class="line-modified">3363           // Inline type is never null. Always throw an exception.</span>
3364           builtin_throw(Deoptimization::Reason_class_check, makecon(TypeKlassPtr::make(klass)));
3365           return top();
3366         } else {
3367           // It needs a null check because a null will *pass* the cast check.
3368           return null_assert(obj);
3369         }
3370       }
3371     }
3372   }
3373 
3374   ciProfileData* data = NULL;
3375   bool safe_for_replace = false;
3376   if (failure_control == NULL) {        // use MDO in regular case only
3377     assert(java_bc() == Bytecodes::_aastore ||
3378            java_bc() == Bytecodes::_checkcast,
3379            &quot;interpreter profiles type checks only for these BCs&quot;);
3380     if (method()-&gt;method_data()-&gt;is_mature()) {
3381       data = method()-&gt;method_data()-&gt;bci_to_data(bci());
3382     }
3383     safe_for_replace = true;
</pre>
<hr />
<pre>
3418   if (null_ctl == top()) {
3419     // Do this eagerly, so that pattern matches like is_diamond_phi
3420     // will work even during parsing.
3421     assert(_null_path == PATH_LIMIT-1, &quot;delete last&quot;);
3422     region-&gt;del_req(_null_path);
3423     phi   -&gt;del_req(_null_path);
3424   }
3425 
3426   Node* cast_obj = NULL;
3427   if (!from_inline &amp;&amp; tk-&gt;klass_is_exact()) {
3428     // The following optimization tries to statically cast the speculative type of the object
3429     // (for example obtained during profiling) to the type of the superklass and then do a
3430     // dynamic check that the type of the object is what we expect. To work correctly
3431     // for checkcast and aastore the type of superklass should be exact.
3432     const TypeOopPtr* obj_type = _gvn.type(obj)-&gt;is_oopptr();
3433     // We may not have profiling here or it may not help us. If we have
3434     // a speculative type use it to perform an exact cast.
3435     ciKlass* spec_obj_type = obj_type-&gt;speculative_type();
3436     if (spec_obj_type != NULL || data != NULL) {
3437       cast_obj = maybe_cast_profiled_receiver(not_null_obj, tk-&gt;klass(), spec_obj_type, safe_for_replace);
<span class="line-modified">3438       if (cast_obj != NULL &amp;&amp; cast_obj-&gt;is_InlineType()) {</span>
3439         if (null_ctl != top()) {
3440           cast_obj = NULL; // A value that&#39;s sometimes null is not something we can optimize well
3441         } else {
3442           return cast_obj;
3443         }
3444       }
3445       if (cast_obj != NULL) {
3446         if (failure_control != NULL) // failure is now impossible
3447           (*failure_control) = top();
3448         // adjust the type of the phi to the exact klass:
3449         phi-&gt;raise_bottom_type(_gvn.type(cast_obj)-&gt;meet_speculative(TypePtr::NULL_PTR));
3450       }
3451     }
3452   }
3453 
3454   if (cast_obj == NULL) {
3455     // Generate the subtype check
3456     Node* not_subtype_ctrl = gen_subtype_check(not_null_obj, superklass);
3457 
3458     // Plug in success path into the merge
3459     cast_obj = from_inline ? not_null_obj : _gvn.transform(new CheckCastPPNode(control(), not_null_obj, toop));
3460     // Failure path ends in uncommon trap (or may be dead - failure impossible)
3461     if (failure_control == NULL) {
3462       if (not_subtype_ctrl != top()) { // If failure is possible
3463         PreserveJVMState pjvms(this);
3464         set_control(not_subtype_ctrl);
3465         Node* obj_klass = NULL;
3466         if (from_inline) {
<span class="line-modified">3467           obj_klass = makecon(TypeKlassPtr::make(_gvn.type(not_null_obj)-&gt;inline_klass()));</span>
3468         } else {
3469           obj_klass = load_object_klass(not_null_obj);
3470         }
3471         builtin_throw(Deoptimization::Reason_class_check, obj_klass);
3472       }
3473     } else {
3474       (*failure_control) = not_subtype_ctrl;
3475     }
3476   }
3477 
3478   region-&gt;init_req(_obj_path, control());
3479   phi   -&gt;init_req(_obj_path, cast_obj);
3480 
3481   // A merge of NULL or Casted-NotNull obj
3482   Node* res = _gvn.transform(phi);
3483 
3484   // Note I do NOT always &#39;replace_in_map(obj,result)&#39; here.
3485   //  if( tk-&gt;klass()-&gt;can_be_primary_super()  )
3486     // This means that if I successfully store an Object into an array-of-String
3487     // I &#39;forget&#39; that the Object is really now known to be a String.  I have to
3488     // do this because we don&#39;t have true union types for interfaces - if I store
3489     // a Baz into an array-of-Interface and then tell the optimizer it&#39;s an
3490     // Interface, I forget that it&#39;s also a Baz and cannot do Baz-like field
3491     // references to it.  FIX THIS WHEN UNION TYPES APPEAR!
3492   //  replace_in_map( obj, res );
3493 
3494   // Return final merged results
3495   set_control( _gvn.transform(region) );
3496   record_for_igvn(region);
3497 
<span class="line-modified">3498   bool not_inline = !toop-&gt;can_be_inline_type();</span>
<span class="line-modified">3499   bool not_flattened = !UseFlatArray || not_inline || (toop-&gt;is_inlinetypeptr() &amp;&amp; !toop-&gt;inline_klass()-&gt;flatten_array());</span>
3500   if (EnableValhalla &amp;&amp; not_flattened) {
3501     // Check if obj has been loaded from an array
3502     obj = obj-&gt;isa_DecodeN() ? obj-&gt;in(1) : obj;
3503     Node* array = NULL;
3504     if (obj-&gt;isa_Load()) {
3505       Node* address = obj-&gt;in(MemNode::Address);
3506       if (address-&gt;isa_AddP()) {
3507         array = address-&gt;as_AddP()-&gt;in(AddPNode::Base);
3508       }
3509     } else if (obj-&gt;is_Phi()) {
3510       Node* region = obj-&gt;in(0);
3511       // TODO make this more robust (see JDK-8231346)
3512       if (region-&gt;req() == 3 &amp;&amp; region-&gt;in(2) != NULL &amp;&amp; region-&gt;in(2)-&gt;in(0) != NULL) {
3513         IfNode* iff = region-&gt;in(2)-&gt;in(0)-&gt;isa_If();
3514         if (iff != NULL) {
3515           iff-&gt;is_non_flattened_array_check(&amp;_gvn, &amp;array);
3516         }
3517       }
3518     }
3519     if (array != NULL) {
3520       const TypeAryPtr* ary_t = _gvn.type(array)-&gt;isa_aryptr();
3521       if (ary_t != NULL) {
3522         if (!ary_t-&gt;is_not_null_free() &amp;&amp; not_inline) {
3523           // Casting array element to a non-inline-type, mark array as not null-free.
3524           Node* cast = _gvn.transform(new CheckCastPPNode(control(), array, ary_t-&gt;cast_to_not_null_free()));
3525           replace_in_map(array, cast);
3526         } else if (!ary_t-&gt;is_not_flat()) {
3527           // Casting array element to a non-flattened type, mark array as not flat.
3528           Node* cast = _gvn.transform(new CheckCastPPNode(control(), array, ary_t-&gt;cast_to_not_flat()));
3529           replace_in_map(array, cast);
3530         }
3531       }
3532     }
3533   }
3534 
3535   if (!from_inline) {
3536     res = record_profiled_receiver_for_speculation(res);
<span class="line-modified">3537     if (to_inline &amp;&amp; toop-&gt;inline_klass()-&gt;is_scalarizable()) {</span>
3538       assert(!gvn().type(res)-&gt;maybe_null(), &quot;Inline types are null-free&quot;);
<span class="line-modified">3539       res = InlineTypeNode::make_from_oop(this, res, toop-&gt;inline_klass());</span>
3540     }
3541   }
3542   return res;
3543 }
3544 
<span class="line-modified">3545 // Check if &#39;obj&#39; is an inline type by checking if it has the always_locked markWord pattern set.</span>
<span class="line-modified">3546 Node* GraphKit::is_inline_type(Node* obj) {</span>
3547   Node* mark_addr = basic_plus_adr(obj, oopDesc::mark_offset_in_bytes());
3548   Node* mark = make_load(NULL, mark_addr, TypeX_X, TypeX_X-&gt;basic_type(), MemNode::unordered);
3549   Node* mask = _gvn.MakeConX(markWord::always_locked_pattern);
3550   Node* andx = _gvn.transform(new AndXNode(mark, mask));
3551   Node* cmp = _gvn.transform(new CmpXNode(andx, mask));
3552   return _gvn.transform(new BoolNode(cmp, BoolTest::eq));
3553 }
3554 
3555 // Check if &#39;ary&#39; is a non-flattened array
3556 Node* GraphKit::is_non_flattened_array(Node* ary) {
3557   Node* kls = load_object_klass(ary);
3558   Node* tag = load_lh_array_tag(kls);
3559   Node* cmp = gen_lh_array_test(kls, Klass::_lh_array_tag_vt_value);
3560   return _gvn.transform(new BoolNode(cmp, BoolTest::ne));
3561 }
3562 
3563 // Check if &#39;ary&#39; is a nullable array
3564 Node* GraphKit::is_nullable_array(Node* ary) {
3565   Node* kls = load_object_klass(ary);
3566   Node* lhp = basic_plus_adr(kls, in_bytes(Klass::layout_helper_offset()));
3567   Node* layout_val = _gvn.transform(LoadNode::make(_gvn, NULL, immutable_memory(), lhp, lhp-&gt;bottom_type()-&gt;is_ptr(), TypeInt::INT, T_INT, MemNode::unordered));
3568   Node* null_free = _gvn.transform(new RShiftINode(layout_val, intcon(Klass::_lh_null_free_shift)));
3569   null_free = _gvn.transform(new AndINode(null_free, intcon(Klass::_lh_null_free_mask)));
3570   Node* cmp = _gvn.transform(new CmpINode(null_free, intcon(0)));
3571   return _gvn.transform(new BoolNode(cmp, BoolTest::eq));
3572 }
3573 
<span class="line-modified">3574 // Deoptimize if &#39;ary&#39; is a null-free inline type array and &#39;val&#39; is null</span>
<span class="line-modified">3575 Node* GraphKit::gen_inline_array_null_guard(Node* ary, Node* val, int nargs, bool safe_for_replace) {</span>
3576   const Type* val_t = _gvn.type(val);
<span class="line-modified">3577   if (val-&gt;is_InlineType() || !TypePtr::NULL_PTR-&gt;higher_equal(val_t)) {</span>
3578     return ary; // Never null
3579   }
3580   RegionNode* region = new RegionNode(3);
3581   Node* null_ctl = top();
3582   null_check_oop(val, &amp;null_ctl);
3583   if (null_ctl != top()) {
3584     PreserveJVMState pjvms(this);
3585     set_control(null_ctl);
3586     {
3587       // Deoptimize if null-free array
3588       BuildCutout unless(this, is_nullable_array(ary), PROB_MAX);
3589       inc_sp(nargs);
3590       uncommon_trap(Deoptimization::Reason_null_check,
3591                     Deoptimization::Action_none);
3592     }
3593     region-&gt;init_req(1, control());
3594   }
3595   region-&gt;init_req(2, control());
3596   set_control(_gvn.transform(region));
3597   record_for_igvn(region);
</pre>
<hr />
<pre>
3741   }
3742 #endif
3743 
3744   return flock;
3745 }
3746 
3747 
3748 //------------------------------shared_unlock----------------------------------
3749 // Emit unlocking code.
3750 void GraphKit::shared_unlock(Node* box, Node* obj) {
3751   // bci is either a monitorenter bc or InvocationEntryBci
3752   // %%% SynchronizationEntryBCI is redundant; use InvocationEntryBci in interfaces
3753   assert(SynchronizationEntryBCI == InvocationEntryBci, &quot;&quot;);
3754 
3755   if( !GenerateSynchronizationCode )
3756     return;
3757   if (stopped()) {               // Dead monitor?
3758     map()-&gt;pop_monitor();        // Kill monitor from debug info
3759     return;
3760   }
<span class="line-modified">3761   assert(!obj-&gt;is_InlineTypeBase(), &quot;should not unlock on inline type&quot;);</span>
3762 
3763   // Memory barrier to avoid floating things down past the locked region
3764   insert_mem_bar(Op_MemBarReleaseLock);
3765 
3766   const TypeFunc *tf = OptoRuntime::complete_monitor_exit_Type();
3767   UnlockNode *unlock = new UnlockNode(C, tf);
3768 #ifdef ASSERT
3769   unlock-&gt;set_dbg_jvms(sync_jvms());
3770 #endif
3771   uint raw_idx = Compile::AliasIdxRaw;
3772   unlock-&gt;init_req( TypeFunc::Control, control() );
3773   unlock-&gt;init_req( TypeFunc::Memory , memory(raw_idx) );
3774   unlock-&gt;init_req( TypeFunc::I_O    , top() )     ;   // does no i/o
3775   unlock-&gt;init_req( TypeFunc::FramePtr, frameptr() );
3776   unlock-&gt;init_req( TypeFunc::ReturnAdr, top() );
3777 
3778   unlock-&gt;init_req(TypeFunc::Parms + 0, obj);
3779   unlock-&gt;init_req(TypeFunc::Parms + 1, box);
3780   unlock = _gvn.transform(unlock)-&gt;as_Unlock();
3781 
</pre>
<hr />
<pre>
3787   // Kill monitor from debug info
3788   map()-&gt;pop_monitor( );
3789 }
3790 
3791 //-------------------------------get_layout_helper-----------------------------
3792 // If the given klass is a constant or known to be an array,
3793 // fetch the constant layout helper value into constant_value
3794 // and return (Node*)NULL.  Otherwise, load the non-constant
3795 // layout helper value, and return the node which represents it.
3796 // This two-faced routine is useful because allocation sites
3797 // almost always feature constant types.
3798 Node* GraphKit::get_layout_helper(Node* klass_node, jint&amp; constant_value) {
3799   const TypeKlassPtr* inst_klass = _gvn.type(klass_node)-&gt;isa_klassptr();
3800   if (!StressReflectiveCode &amp;&amp; inst_klass != NULL) {
3801     ciKlass* klass = inst_klass-&gt;klass();
3802     assert(klass != NULL, &quot;klass should not be NULL&quot;);
3803     bool    xklass = inst_klass-&gt;klass_is_exact();
3804     bool can_be_flattened = false;
3805     if (UseFlatArray &amp;&amp; klass-&gt;is_obj_array_klass()) {
3806       ciKlass* elem = klass-&gt;as_obj_array_klass()-&gt;element_klass();
<span class="line-modified">3807       can_be_flattened = elem-&gt;can_be_inline_klass() &amp;&amp; (!elem-&gt;is_inlinetype() || elem-&gt;as_inline_klass()-&gt;flatten_array());</span>
3808     }
3809     if (xklass || (klass-&gt;is_array_klass() &amp;&amp; !can_be_flattened)) {
3810       jint lhelper = klass-&gt;layout_helper();
3811       if (lhelper != Klass::_lh_neutral_value) {
3812         constant_value = lhelper;
3813         return (Node*) NULL;
3814       }
3815     }
3816   }
3817   constant_value = Klass::_lh_neutral_value;  // put in a known value
3818   Node* lhp = basic_plus_adr(klass_node, klass_node, in_bytes(Klass::layout_helper_offset()));
3819   return make_load(NULL, lhp, TypeInt::INT, T_INT, MemNode::unordered);
3820 }
3821 
3822 // We just put in an allocate/initialize with a big raw-memory effect.
3823 // Hook selected additional alias categories on the initialization.
3824 static void hook_memory_on_init(GraphKit&amp; kit, int alias_idx,
3825                                 MergeMemNode* init_in_merge,
3826                                 Node* init_out_raw) {
3827   DEBUG_ONLY(Node* init_in_raw = init_in_merge-&gt;base_memory());
</pre>
<hr />
<pre>
3860   assert(alloc-&gt;initialization() == init,  &quot;2-way macro link must work&quot;);
3861   assert(init -&gt;allocation()     == alloc, &quot;2-way macro link must work&quot;);
3862   {
3863     // Extract memory strands which may participate in the new object&#39;s
3864     // initialization, and source them from the new InitializeNode.
3865     // This will allow us to observe initializations when they occur,
3866     // and link them properly (as a group) to the InitializeNode.
3867     assert(init-&gt;in(InitializeNode::Memory) == malloc, &quot;&quot;);
3868     MergeMemNode* minit_in = MergeMemNode::make(malloc);
3869     init-&gt;set_req(InitializeNode::Memory, minit_in);
3870     record_for_igvn(minit_in); // fold it up later, if possible
3871     _gvn.set_type(minit_in, Type::MEMORY);
3872     Node* minit_out = memory(rawidx);
3873     assert(minit_out-&gt;is_Proj() &amp;&amp; minit_out-&gt;in(0) == init, &quot;&quot;);
3874     // Add an edge in the MergeMem for the header fields so an access
3875     // to one of those has correct memory state
3876     set_memory(minit_out, C-&gt;get_alias_index(oop_type-&gt;add_offset(oopDesc::mark_offset_in_bytes())));
3877     set_memory(minit_out, C-&gt;get_alias_index(oop_type-&gt;add_offset(oopDesc::klass_offset_in_bytes())));
3878     if (oop_type-&gt;isa_aryptr()) {
3879       const TypeAryPtr* arytype = oop_type-&gt;is_aryptr();
<span class="line-modified">3880       if (arytype-&gt;klass()-&gt;is_flat_array_klass()) {</span>
3881         // Initially all flattened array accesses share a single slice
3882         // but that changes after parsing. Prepare the memory graph so
3883         // it can optimize flattened array accesses properly once they
3884         // don&#39;t share a single slice.
3885         assert(C-&gt;flattened_accesses_share_alias(), &quot;should be set at parse time&quot;);
3886         C-&gt;set_flattened_accesses_share_alias(false);
<span class="line-modified">3887         ciFlatArrayKlass* vak = arytype-&gt;klass()-&gt;as_flat_array_klass();</span>
<span class="line-modified">3888         ciInlineKlass* vk = vak-&gt;element_klass()-&gt;as_inline_klass();</span>
3889         for (int i = 0, len = vk-&gt;nof_nonstatic_fields(); i &lt; len; i++) {
3890           ciField* field = vk-&gt;nonstatic_field_at(i);
3891           if (field-&gt;offset() &gt;= TrackedInitializationLimit * HeapWordSize)
3892             continue;  // do not bother to track really large numbers of fields
3893           int off_in_vt = field-&gt;offset() - vk-&gt;first_field_offset();
3894           const TypePtr* adr_type = arytype-&gt;with_field_offset(off_in_vt)-&gt;add_offset(Type::OffsetBot);
3895           int fieldidx = C-&gt;get_alias_index(adr_type, true);
3896           hook_memory_on_init(*this, fieldidx, minit_in, minit_out);
3897         }
3898         C-&gt;set_flattened_accesses_share_alias(true);
<span class="line-modified">3899         hook_memory_on_init(*this, C-&gt;get_alias_index(TypeAryPtr::INLINES), minit_in, minit_out);</span>
3900       } else {
3901         const TypePtr* telemref = oop_type-&gt;add_offset(Type::OffsetBot);
3902         int            elemidx  = C-&gt;get_alias_index(telemref);
3903         hook_memory_on_init(*this, elemidx, minit_in, minit_out);
3904       }
3905     } else if (oop_type-&gt;isa_instptr()) {
3906       set_memory(minit_out, C-&gt;get_alias_index(oop_type)); // mark word
3907       ciInstanceKlass* ik = oop_type-&gt;klass()-&gt;as_instance_klass();
3908       for (int i = 0, len = ik-&gt;nof_nonstatic_fields(); i &lt; len; i++) {
3909         ciField* field = ik-&gt;nonstatic_field_at(i);
3910         if (field-&gt;offset() &gt;= TrackedInitializationLimit * HeapWordSize)
3911           continue;  // do not bother to track really large numbers of fields
3912         // Find (or create) the alias category for this field:
3913         int fieldidx = C-&gt;alias_type(field)-&gt;index();
3914         hook_memory_on_init(*this, fieldidx, minit_in, minit_out);
3915       }
3916     }
3917   }
3918 
3919   // Cast raw oop to the real thing...
</pre>
<hr />
<pre>
3938     }
3939   }
3940 #endif //ASSERT
3941 
3942   return javaoop;
3943 }
3944 
3945 //---------------------------new_instance--------------------------------------
3946 // This routine takes a klass_node which may be constant (for a static type)
3947 // or may be non-constant (for reflective code).  It will work equally well
3948 // for either, and the graph will fold nicely if the optimizer later reduces
3949 // the type to a constant.
3950 // The optional arguments are for specialized use by intrinsics:
3951 //  - If &#39;extra_slow_test&#39; if not null is an extra condition for the slow-path.
3952 //  - If &#39;return_size_val&#39;, report the the total object size to the caller.
3953 //  - deoptimize_on_exception controls how Java exceptions are handled (rethrow vs deoptimize)
3954 Node* GraphKit::new_instance(Node* klass_node,
3955                              Node* extra_slow_test,
3956                              Node* *return_size_val,
3957                              bool deoptimize_on_exception,
<span class="line-modified">3958                              InlineTypeBaseNode* inline_type_node) {</span>
3959   // Compute size in doublewords
3960   // The size is always an integral number of doublewords, represented
3961   // as a positive bytewise size stored in the klass&#39;s layout_helper.
3962   // The layout_helper also encodes (in a low bit) the need for a slow path.
3963   jint  layout_con = Klass::_lh_neutral_value;
3964   Node* layout_val = get_layout_helper(klass_node, layout_con);
3965   bool  layout_is_con = (layout_val == NULL);
3966 
3967   if (extra_slow_test == NULL)  extra_slow_test = intcon(0);
3968   // Generate the initial go-slow test.  It&#39;s either ALWAYS (return a
3969   // Node for 1) or NEVER (return a NULL) or perhaps (in the reflective
3970   // case) a computed value derived from the layout_helper.
3971   Node* initial_slow_test = NULL;
3972   if (layout_is_con) {
3973     assert(!StressReflectiveCode, &quot;stress mode does not use these paths&quot;);
3974     bool must_go_slow = Klass::layout_helper_needs_slow_path(layout_con);
3975     initial_slow_test = must_go_slow ? intcon(1) : extra_slow_test;
3976   } else {   // reflective case
3977     // This reflective path is used by Unsafe.allocateInstance.
3978     // (It may be stress-tested by specifying StressReflectiveCode.)
</pre>
<hr />
<pre>
4003     (*return_size_val) = size;
4004   }
4005 
4006   // This is a precise notnull oop of the klass.
4007   // (Actually, it need not be precise if this is a reflective allocation.)
4008   // It&#39;s what we cast the result to.
4009   const TypeKlassPtr* tklass = _gvn.type(klass_node)-&gt;isa_klassptr();
4010   if (!tklass)  tklass = TypeKlassPtr::OBJECT;
4011   const TypeOopPtr* oop_type = tklass-&gt;as_instance_type();
4012 
4013   // Now generate allocation code
4014 
4015   // The entire memory state is needed for slow path of the allocation
4016   // since GC and deoptimization can happen.
4017   Node *mem = reset_memory();
4018   set_all_memory(mem); // Create new memory state
4019 
4020   AllocateNode* alloc = new AllocateNode(C, AllocateNode::alloc_type(Type::TOP),
4021                                          control(), mem, i_o(),
4022                                          size, klass_node,
<span class="line-modified">4023                                          initial_slow_test, inline_type_node);</span>
4024 
4025   return set_output_for_allocation(alloc, oop_type, deoptimize_on_exception);
4026 }
4027 
4028 // With compressed oops, the 64 bit init value for non flattened value
4029 // arrays is built from 2 32 bit compressed oops
4030 static Node* raw_default_for_coops(Node* default_value, GraphKit&amp; kit) {
4031   Node* lower = kit.gvn().transform(new CastP2XNode(kit.control(), default_value));
4032   Node* upper = kit.gvn().transform(new LShiftLNode(lower, kit.intcon(32)));
4033   return kit.gvn().transform(new OrLNode(lower, upper));
4034 }
4035 
4036 //-------------------------------new_array-------------------------------------
4037 // helper for newarray and anewarray
4038 // The &#39;length&#39; parameter is (obviously) the length of the array.
4039 // See comments on new_instance for the meaning of the other arguments.
4040 Node* GraphKit::new_array(Node* klass_node,     // array klass (maybe variable)
4041                           Node* length,         // number of array elements
4042                           int   nargs,          // number of arguments to push back for uncommon trap
4043                           Node* *return_size_val,
</pre>
<hr />
<pre>
4075     // Increase the size limit if we have exact knowledge of array type.
4076     int log2_esize = Klass::layout_helper_log2_element_size(layout_con);
4077     fast_size_limit &lt;&lt;= MAX2(LogBytesPerLong - log2_esize, 0);
4078   }
4079 
4080   Node* initial_slow_cmp  = _gvn.transform( new CmpUNode( length, intcon( fast_size_limit ) ) );
4081   Node* initial_slow_test = _gvn.transform( new BoolNode( initial_slow_cmp, BoolTest::gt ) );
4082 
4083   // --- Size Computation ---
4084   // array_size = round_to_heap(array_header + (length &lt;&lt; elem_shift));
4085   // where round_to_heap(x) == align_to(x, MinObjAlignmentInBytes)
4086   // and align_to(x, y) == ((x + y-1) &amp; ~(y-1))
4087   // The rounding mask is strength-reduced, if possible.
4088   int round_mask = MinObjAlignmentInBytes - 1;
4089   Node* header_size = NULL;
4090   int   header_size_min  = arrayOopDesc::base_offset_in_bytes(T_BYTE);
4091   // (T_BYTE has the weakest alignment and size restrictions...)
4092   if (layout_is_con) {
4093     int       hsize  = Klass::layout_helper_header_size(layout_con);
4094     int       eshift = Klass::layout_helper_log2_element_size(layout_con);
<span class="line-modified">4095     bool is_flat_array = Klass::layout_helper_is_flatArray(layout_con);</span>
4096     if ((round_mask &amp; ~right_n_bits(eshift)) == 0)
4097       round_mask = 0;  // strength-reduce it if it goes away completely
<span class="line-modified">4098     assert(is_flat_array || (hsize &amp; right_n_bits(eshift)) == 0, &quot;hsize is pre-rounded&quot;);</span>
4099     assert(header_size_min &lt;= hsize, &quot;generic minimum is smallest&quot;);
4100     header_size_min = hsize;
4101     header_size = intcon(hsize + round_mask);
4102   } else {
4103     Node* hss   = intcon(Klass::_lh_header_size_shift);
4104     Node* hsm   = intcon(Klass::_lh_header_size_mask);
4105     Node* hsize = _gvn.transform( new URShiftINode(layout_val, hss) );
4106     hsize       = _gvn.transform( new AndINode(hsize, hsm) );
4107     Node* mask  = intcon(round_mask);
4108     header_size = _gvn.transform( new AddINode(hsize, mask) );
4109   }
4110 
4111   Node* elem_shift = NULL;
4112   if (layout_is_con) {
4113     int eshift = Klass::layout_helper_log2_element_size(layout_con);
4114     if (eshift != 0)
4115       elem_shift = intcon(eshift);
4116   } else {
4117     // There is no need to mask or shift this value.
4118     // The semantics of LShiftINode include an implicit mask to 0x1F.
</pre>
<hr />
<pre>
4175     // This is the size
4176     (*return_size_val) = size;
4177   }
4178 
4179   // Now generate allocation code
4180 
4181   // The entire memory state is needed for slow path of the allocation
4182   // since GC and deoptimization can happen.
4183   Node *mem = reset_memory();
4184   set_all_memory(mem); // Create new memory state
4185 
4186   if (initial_slow_test-&gt;is_Bool()) {
4187     // Hide it behind a CMoveI, or else PhaseIdealLoop::split_up will get sick.
4188     initial_slow_test = initial_slow_test-&gt;as_Bool()-&gt;as_int_value(&amp;_gvn);
4189   }
4190 
4191   const TypeKlassPtr* ary_klass = _gvn.type(klass_node)-&gt;isa_klassptr();
4192   const TypeOopPtr* ary_type = ary_klass-&gt;as_instance_type();
4193   const TypeAryPtr* ary_ptr = ary_type-&gt;isa_aryptr();
4194 
<span class="line-modified">4195   // Inline type array variants:</span>
4196   // - null-ok:              MyValue.ref[] (ciObjArrayKlass &quot;[LMyValue$ref&quot;)
4197   // - null-free:            MyValue.val[] (ciObjArrayKlass &quot;[QMyValue$val&quot;)
<span class="line-modified">4198   // - null-free, flattened: MyValue.val[] (ciFlatArrayKlass &quot;[QMyValue$val&quot;)</span>
<span class="line-modified">4199   // Check if array is a null-free, non-flattened inline type array</span>
<span class="line-modified">4200   // that needs to be initialized with the default inline type.</span>
4201   Node* default_value = NULL;
4202   Node* raw_default_value = NULL;
4203   if (ary_ptr != NULL &amp;&amp; ary_ptr-&gt;klass_is_exact()) {
4204     // Array type is known
4205     ciKlass* elem_klass = ary_ptr-&gt;klass()-&gt;as_array_klass()-&gt;element_klass();
<span class="line-modified">4206     if (elem_klass != NULL &amp;&amp; elem_klass-&gt;is_inlinetype()) {</span>
<span class="line-modified">4207       ciInlineKlass* vk = elem_klass-&gt;as_inline_klass();</span>
4208       if (!vk-&gt;flatten_array()) {
<span class="line-modified">4209         default_value = InlineTypeNode::default_oop(gvn(), vk);</span>
4210         if (UseCompressedOops) {
4211           default_value = _gvn.transform(new EncodePNode(default_value, default_value-&gt;bottom_type()-&gt;make_narrowoop()));
4212           raw_default_value = raw_default_for_coops(default_value, *this);
4213         } else {
4214           raw_default_value = _gvn.transform(new CastP2XNode(control(), default_value));
4215         }
4216       }
4217     }
<span class="line-modified">4218   } else if (ary_klass-&gt;klass()-&gt;can_be_inline_array_klass()) {</span>
4219     // Array type is not known, add runtime checks
4220     assert(!ary_klass-&gt;klass_is_exact(), &quot;unexpected exact type&quot;);
4221     Node* r = new RegionNode(4);
4222     default_value = new PhiNode(r, TypeInstPtr::BOTTOM);
4223 
4224     // Check if array is an object array
4225     Node* cmp = gen_lh_array_test(klass_node, Klass::_lh_array_tag_obj_value);
4226     Node* bol = _gvn.transform(new BoolNode(cmp, BoolTest::eq));
4227     IfNode* iff = create_and_map_if(control(), bol, PROB_FAIR, COUNT_UNKNOWN);
4228 
4229     // Not an object array, initialize with all zero
4230     r-&gt;init_req(1, _gvn.transform(new IfFalseNode(iff)));
4231     default_value-&gt;init_req(1, null());
4232 
4233     // Object array, check if null-free
4234     set_control(_gvn.transform(new IfTrueNode(iff)));
4235     Node* lhp = basic_plus_adr(klass_node, in_bytes(Klass::layout_helper_offset()));
4236     Node* layout_val = _gvn.transform(LoadNode::make(_gvn, NULL, immutable_memory(), lhp, lhp-&gt;bottom_type()-&gt;is_ptr(), TypeInt::INT, T_INT, MemNode::unordered));
4237     Node* null_free = _gvn.transform(new RShiftINode(layout_val, intcon(Klass::_lh_null_free_shift)));
4238     null_free = _gvn.transform(new AndINode(null_free, intcon(Klass::_lh_null_free_mask)));
4239     cmp = _gvn.transform(new CmpINode(null_free, intcon(0)));
4240     bol = _gvn.transform(new BoolNode(cmp, BoolTest::ne));
4241     iff = create_and_map_if(control(), bol, PROB_FAIR, COUNT_UNKNOWN);
4242 
4243     // Not null-free, initialize with all zero
4244     r-&gt;init_req(2, _gvn.transform(new IfFalseNode(iff)));
4245     default_value-&gt;init_req(2, null());
4246 
<span class="line-modified">4247     // Null-free, non-flattened inline type array, initialize with the default value</span>
4248     set_control(_gvn.transform(new IfTrueNode(iff)));
4249     Node* p = basic_plus_adr(klass_node, in_bytes(ArrayKlass::element_klass_offset()));
4250     Node* eklass = _gvn.transform(LoadKlassNode::make(_gvn, control(), immutable_memory(), p, TypeInstPtr::KLASS));
4251     Node* adr_fixed_block_addr = basic_plus_adr(eklass, in_bytes(InstanceKlass::adr_inlineklass_fixed_block_offset()));
4252     Node* adr_fixed_block = make_load(control(), adr_fixed_block_addr, TypeRawPtr::NOTNULL, T_ADDRESS, MemNode::unordered);
4253     Node* default_value_offset_addr = basic_plus_adr(adr_fixed_block, in_bytes(InlineKlass::default_value_offset_offset()));
4254     Node* default_value_offset = make_load(control(), default_value_offset_addr, TypeInt::INT, T_INT, MemNode::unordered);
4255     Node* elem_mirror = load_mirror_from_klass(eklass);
4256     Node* default_value_addr = basic_plus_adr(elem_mirror, ConvI2X(default_value_offset));
4257     Node* val = access_load_at(elem_mirror, default_value_addr, _gvn.type(default_value_addr)-&gt;is_ptr(), TypeInstPtr::BOTTOM, T_OBJECT, IN_HEAP);
4258     r-&gt;init_req(3, control());
4259     default_value-&gt;init_req(3, val);
4260 
4261     set_control(_gvn.transform(r));
4262     default_value = _gvn.transform(default_value);
4263     if (UseCompressedOops) {
4264       default_value = _gvn.transform(new EncodePNode(default_value, default_value-&gt;bottom_type()-&gt;make_narrowoop()));
4265       raw_default_value = raw_default_for_coops(default_value, *this);
4266     } else {
4267       raw_default_value = _gvn.transform(new CastP2XNode(control(), default_value));
</pre>
<hr />
<pre>
4572 
4573   set_control(IfFalse(iff));
4574   set_memory(st, TypeAryPtr::BYTES);
4575 }
4576 
4577 Node* GraphKit::make_constant_from_field(ciField* field, Node* obj) {
4578   if (!field-&gt;is_constant()) {
4579     return NULL; // Field not marked as constant.
4580   }
4581   ciInstance* holder = NULL;
4582   if (!field-&gt;is_static()) {
4583     ciObject* const_oop = obj-&gt;bottom_type()-&gt;is_oopptr()-&gt;const_oop();
4584     if (const_oop != NULL &amp;&amp; const_oop-&gt;is_instance()) {
4585       holder = const_oop-&gt;as_instance();
4586     }
4587   }
4588   const Type* con_type = Type::make_constant_from_field(field, holder, field-&gt;layout_type(),
4589                                                         /*is_unsigned_load=*/false);
4590   if (con_type != NULL) {
4591     Node* con = makecon(con_type);
<span class="line-modified">4592     assert(!field-&gt;type()-&gt;is_inlinetype() || (field-&gt;is_static() &amp;&amp; !con_type-&gt;is_zero_type()), &quot;sanity&quot;);</span>
4593     // Check type of constant which might be more precise
<span class="line-modified">4594     if (con_type-&gt;is_inlinetypeptr() &amp;&amp; con_type-&gt;inline_klass()-&gt;is_scalarizable()) {</span>
<span class="line-modified">4595       // Load inline type from constant oop</span>
<span class="line-modified">4596       con = InlineTypeNode::make_from_oop(this, con, con_type-&gt;inline_klass());</span>
4597     }
4598     return con;
4599   }
4600   return NULL;
4601 }
4602 
4603 //---------------------------load_mirror_from_klass----------------------------
4604 // Given a klass oop, load its java mirror (a java.lang.Class oop).
4605 Node* GraphKit::load_mirror_from_klass(Node* klass) {
4606   Node* p = basic_plus_adr(klass, in_bytes(Klass::java_mirror_offset()));
4607   Node* load = make_load(NULL, p, TypeRawPtr::NOTNULL, T_ADDRESS, MemNode::unordered);
4608   // mirror = ((OopHandle)mirror)-&gt;resolve();
4609   return access_load(load, TypeInstPtr::MIRROR, T_OBJECT, IN_NATIVE);
4610 }
</pre>
</td>
</tr>
</table>
<center><a href="escape.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="graphKit.hpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>