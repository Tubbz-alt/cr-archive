diff a/src/hotspot/share/opto/library_call.cpp b/src/hotspot/share/opto/library_call.cpp
--- a/src/hotspot/share/opto/library_call.cpp
+++ b/src/hotspot/share/opto/library_call.cpp
@@ -40,10 +40,11 @@
 #include "opto/callGenerator.hpp"
 #include "opto/castnode.hpp"
 #include "opto/cfgnode.hpp"
 #include "opto/convertnode.hpp"
 #include "opto/countbitsnode.hpp"
+#include "opto/inlinetypenode.hpp"
 #include "opto/intrinsicnode.hpp"
 #include "opto/idealKit.hpp"
 #include "opto/mathexactnode.hpp"
 #include "opto/movenode.hpp"
 #include "opto/mulnode.hpp"
@@ -51,11 +52,10 @@
 #include "opto/opaquenode.hpp"
 #include "opto/parse.hpp"
 #include "opto/runtime.hpp"
 #include "opto/rootnode.hpp"
 #include "opto/subnode.hpp"
-#include "opto/valuetypenode.hpp"
 #include "prims/nativeLookup.hpp"
 #include "prims/unsafe.hpp"
 #include "runtime/objectMonitor.hpp"
 #include "runtime/sharedRuntime.hpp"
 #include "utilities/macros.hpp"
@@ -136,16 +136,16 @@
   void push_result() {
     // Push the result onto the stack.
     Node* res = result();
     if (!stopped() && res != NULL) {
       BasicType bt = res->bottom_type()->basic_type();
-      if (C->inlining_incrementally() && res->is_ValueType()) {
+      if (C->inlining_incrementally() && res->is_InlineType()) {
         // The caller expects an oop when incrementally inlining an intrinsic that returns an
         // inline type. Make sure the call is re-executed if the allocation triggers a deoptimization.
         PreserveReexecuteState preexecs(this);
         jvms()->set_should_reexecute(true);
-        res = res->as_ValueType()->buffer(this);
+        res = res->as_InlineType()->buffer(this);
       }
       push_node(bt, res);
     }
   }
 
@@ -200,11 +200,11 @@
     AnyArray,
     NonArray,
     ObjectArray,
     NonObjectArray,
     TypeArray,
-    ValueArray
+    FlatArray
   };
 
   Node* generate_hidden_class_guard(Node* kls, RegionNode* region);
 
   Node* generate_array_guard(Node* kls, RegionNode* region) {
@@ -220,13 +220,13 @@
     return generate_array_guard_common(kls, region, NonObjectArray);
   }
   Node* generate_typeArray_guard(Node* kls, RegionNode* region) {
     return generate_array_guard_common(kls, region, TypeArray);
   }
-  Node* generate_valueArray_guard(Node* kls, RegionNode* region) {
+  Node* generate_flatArray_guard(Node* kls, RegionNode* region) {
     assert(UseFlatArray, "can never be flattened");
-    return generate_array_guard_common(kls, region, ValueArray);
+    return generate_array_guard_common(kls, region, FlatArray);
   }
   Node* generate_array_guard_common(Node* kls, RegionNode* region, ArrayKind kind);
   Node* generate_virtual_guard(Node* obj_klass, RegionNode* slow_region);
   CallJavaNode* generate_method_call(vmIntrinsics::ID method_id,
                                      bool is_virtual = false, bool is_static = false);
@@ -2463,11 +2463,11 @@
   // to be plain byte offsets, which are also the same as those accepted
   // by oopDesc::field_addr.
   assert(Unsafe_field_offset_to_byte_offset(11) == 11,
          "fieldOffset must be byte-scaled");
 
-  ciValueKlass* value_klass = NULL;
+  ciInlineKlass* inline_klass = NULL;
   if (type == T_INLINE_TYPE) {
     Node* cls = null_check(argument(4));
     if (stopped()) {
       return true;
     }
@@ -2475,33 +2475,33 @@
     const TypeKlassPtr* kls_t = _gvn.type(kls)->isa_klassptr();
     if (!kls_t->klass_is_exact()) {
       return false;
     }
     ciKlass* klass = kls_t->klass();
-    if (!klass->is_valuetype()) {
+    if (!klass->is_inlinetype()) {
       return false;
     }
-    value_klass = klass->as_value_klass();
+    inline_klass = klass->as_inline_klass();
   }
 
   receiver = null_check(receiver);
   if (stopped()) {
     return true;
   }
 
-  if (base->is_ValueType()) {
-    ValueTypeNode* vt = base->as_ValueType();
+  if (base->is_InlineType()) {
+    InlineTypeNode* vt = base->as_InlineType();
 
     if (is_store) {
-      if (!vt->is_allocated(&_gvn) || !_gvn.type(vt)->is_valuetype()->larval()) {
+      if (!vt->is_allocated(&_gvn) || !_gvn.type(vt)->is_inlinetype()->larval()) {
         return false;
       }
       base = vt->get_oop();
     } else {
       if (offset->is_Con()) {
         long off = find_long_con(offset, 0);
-        ciValueKlass* vk = vt->type()->value_klass();
+        ciInlineKlass* vk = vt->type()->inline_klass();
         if ((long)(int)off != off || !vk->contains_field_offset(off)) {
           return false;
         }
 
         ciField* f = vk->get_non_flattened_field_by_offset((int)off);
@@ -2510,11 +2510,11 @@
           BasicType bt = f->layout_type();
           if (bt == T_ARRAY || bt == T_NARROWOOP) {
             bt = T_OBJECT;
           }
           if (bt == type) {
-            if (bt != T_INLINE_TYPE || f->type() == value_klass) {
+            if (bt != T_INLINE_TYPE || f->type() == inline_klass) {
               set_result(vt->field_value_by_offset((int)off, false));
               return true;
             }
           }
         }
@@ -2529,11 +2529,11 @@
   // 32-bit machines ignore the high half!
   offset = ConvL2X(offset);
   adr = make_unsafe_address(base, offset, is_store ? ACCESS_WRITE : ACCESS_READ, type, kind == Relaxed);
 
   if (_gvn.type(base)->isa_ptr() == TypePtr::NULL_PTR) {
-    if (type != T_OBJECT && (value_klass == NULL || !value_klass->has_object_fields())) {
+    if (type != T_OBJECT && (inline_klass == NULL || !inline_klass->has_object_fields())) {
       decorators |= IN_NATIVE; // off-heap primitive access
     } else {
       return false; // off-heap oop accesses are not supported
     }
   } else {
@@ -2609,24 +2609,24 @@
     mismatched = true; // conservatively mark all "wide" on-heap accesses as mismatched
   }
 
   if (type == T_INLINE_TYPE) {
     if (adr_type->isa_instptr()) {
-      if (field == NULL || field->type() != value_klass) {
+      if (field == NULL || field->type() != inline_klass) {
         mismatched = true;
       }
     } else if (adr_type->isa_aryptr()) {
       const Type* elem = adr_type->is_aryptr()->elem();
-      if (!elem->isa_valuetype()) {
+      if (!elem->isa_inlinetype()) {
         mismatched = true;
-      } else if (elem->value_klass() != value_klass) {
+      } else if (elem->inline_klass() != inline_klass) {
         mismatched = true;
       }
     }
     if (is_store) {
       const Type* val_t = _gvn.type(val);
-      if (!val_t->isa_valuetype() || val_t->value_klass() != value_klass) {
+      if (!val_t->isa_inlinetype() || val_t->inline_klass() != inline_klass) {
         return false;
       }
     }
   }
 
@@ -2670,13 +2670,13 @@
     if (p == NULL) { // Could not constant fold the load
       if (type == T_INLINE_TYPE) {
         if (adr_type->isa_instptr() && !mismatched) {
           ciInstanceKlass* holder = adr_type->is_instptr()->klass()->as_instance_klass();
           int offset = adr_type->is_instptr()->offset();
-          p = ValueTypeNode::make_from_flattened(this, value_klass, base, base, holder, offset, decorators);
+          p = InlineTypeNode::make_from_flattened(this, inline_klass, base, base, holder, offset, decorators);
         } else {
-          p = ValueTypeNode::make_from_flattened(this, value_klass, base, adr, NULL, 0, decorators);
+          p = InlineTypeNode::make_from_flattened(this, inline_klass, base, adr, NULL, 0, decorators);
         }
       } else {
         p = access_load_at(heap_base_oop, adr, adr_type, value_type, type, decorators);
       }
       // Normalize the value returned by getBoolean in the following cases
@@ -2703,16 +2703,16 @@
     }
     if (type == T_ADDRESS) {
       p = gvn().transform(new CastP2XNode(NULL, p));
       p = ConvX2UL(p);
     }
-    if (field != NULL && field->type()->is_valuetype() && !field->is_flattened()) {
-      // Load a non-flattened value type from memory
-      if (value_type->value_klass()->is_scalarizable()) {
-        p = ValueTypeNode::make_from_oop(this, p, value_type->value_klass());
+    if (field != NULL && field->type()->is_inlinetype() && !field->is_flattened()) {
+      // Load a non-flattened inline type from memory
+      if (value_type->inline_klass()->is_scalarizable()) {
+        p = InlineTypeNode::make_from_oop(this, p, value_type->inline_klass());
       } else {
-        p = null2default(p, value_type->value_klass());
+        p = null2default(p, value_type->inline_klass());
       }
     }
     // The load node has the control of the preceding MemBarCPUOrder.  All
     // following nodes will have the control of the MemBarCPUOrder inserted at
     // the end of this method.  So, pushing the load onto the stack at a later
@@ -2726,22 +2726,22 @@
     }
     if (type == T_INLINE_TYPE) {
       if (adr_type->isa_instptr() && !mismatched) {
         ciInstanceKlass* holder = adr_type->is_instptr()->klass()->as_instance_klass();
         int offset = adr_type->is_instptr()->offset();
-        val->as_ValueType()->store_flattened(this, base, base, holder, offset, decorators);
+        val->as_InlineType()->store_flattened(this, base, base, holder, offset, decorators);
       } else {
-        val->as_ValueType()->store_flattened(this, base, adr, NULL, 0, decorators);
+        val->as_InlineType()->store_flattened(this, base, adr, NULL, 0, decorators);
       }
     } else {
       access_store_at(heap_base_oop, adr, adr_type, val, value_type, type, decorators);
     }
   }
 
-  if (argument(1)->is_ValueType() && is_store) {
-    Node* value = ValueTypeNode::make_from_oop(this, base, _gvn.type(base)->value_klass());
-    value = value->as_ValueType()->make_larval(this, false);
+  if (argument(1)->is_InlineType() && is_store) {
+    Node* value = InlineTypeNode::make_from_oop(this, base, _gvn.type(base)->inline_klass());
+    value = value->as_InlineType()->make_larval(this, false);
     replace_in_map(argument(1), value);
   }
 
   return true;
 }
@@ -2753,15 +2753,15 @@
   receiver = null_check(receiver);
   if (stopped()) {
     return true;
   }
 
-  if (!value->is_ValueType()) {
+  if (!value->is_InlineType()) {
     return false;
   }
 
-  set_result(value->as_ValueType()->make_larval(this, true));
+  set_result(value->as_InlineType()->make_larval(this, true));
 
   return true;
 }
 
 bool LibraryCallKit::inline_unsafe_finish_private_buffer() {
@@ -2771,16 +2771,16 @@
   receiver = null_check(receiver);
   if (stopped()) {
     return true;
   }
 
-  if (!buffer->is_ValueType()) {
+  if (!buffer->is_InlineType()) {
     return false;
   }
 
-  ValueTypeNode* vt = buffer->as_ValueType();
-  if (!vt->is_allocated(&_gvn) || !_gvn.type(vt)->is_valuetype()->larval()) {
+  InlineTypeNode* vt = buffer->as_InlineType();
+  if (!vt->is_allocated(&_gvn) || !_gvn.type(vt)->is_inlinetype()->larval()) {
     return false;
   }
 
   set_result(vt->finish_larval(this));
 
@@ -3497,12 +3497,12 @@
   if (obj == NULL || obj->is_top()) {
     return false;  // dead path
   }
   ciKlass* obj_klass = NULL;
   const Type* obj_t = _gvn.type(obj);
-  if (obj->is_ValueType()) {
-    obj_klass = obj_t->value_klass();
+  if (obj->is_InlineType()) {
+    obj_klass = obj_t->inline_klass();
   } else if (obj_t->isa_oopptr()) {
     obj_klass = obj_t->is_oopptr()->klass();
   }
 
   // First, see if Class.cast() can be folded statically.
@@ -3511,11 +3511,11 @@
   if (tm != NULL && tm->is_klass() && obj_klass != NULL) {
     if (!obj_klass->is_loaded()) {
       // Don't use intrinsic when class is not loaded.
       return false;
     } else {
-      if (!obj->is_ValueType() && tm->as_klass()->is_valuetype()) {
+      if (!obj->is_InlineType() && tm->as_klass()->is_inlinetype()) {
         // Casting to .val, check for null
         obj = null_check(obj);
         if (stopped()) {
           return true;
         }
@@ -3559,11 +3559,11 @@
   // nothing is an instance of a primitive type.
   Node* kls = load_klass_from_mirror(mirror, false, region, _prim_path);
 
   Node* res = top();
   if (!stopped()) {
-    if (EnableValhalla && !obj->is_ValueType()) {
+    if (EnableValhalla && !obj->is_InlineType()) {
       // Check if we are casting to .val
       Node* is_val_kls = generate_value_guard(kls, NULL);
       if (is_val_kls != NULL) {
         RegionNode* r = new RegionNode(3);
         record_for_igvn(r);
@@ -3723,11 +3723,11 @@
     bool query = 0;
     switch(kind) {
       case ObjectArray:    query = Klass::layout_helper_is_objArray(layout_con); break;
       case NonObjectArray: query = !Klass::layout_helper_is_objArray(layout_con); break;
       case TypeArray:      query = Klass::layout_helper_is_typeArray(layout_con); break;
-      case ValueArray:     query = Klass::layout_helper_is_flatArray(layout_con); break;
+      case FlatArray:     query = Klass::layout_helper_is_flatArray(layout_con); break;
       case AnyArray:       query = Klass::layout_helper_is_array(layout_con); break;
       case NonArray:       query = !Klass::layout_helper_is_array(layout_con); break;
       default:
         ShouldNotReachHere();
     }
@@ -3755,11 +3755,11 @@
       value = Klass::_lh_array_tag_type_value;
       layout_val = _gvn.transform(new RShiftINode(layout_val, intcon(Klass::_lh_array_tag_shift)));
       btest = BoolTest::eq;
       break;
     }
-    case ValueArray: {
+    case FlatArray: {
       value = Klass::_lh_array_tag_vt_value;
       layout_val = _gvn.transform(new RShiftINode(layout_val, intcon(Klass::_lh_array_tag_shift)));
       btest = BoolTest::eq;
       break;
     }
@@ -3896,13 +3896,13 @@
   const TypeAryPtr* original_t = _gvn.type(original)->isa_aryptr();
   const TypeInstPtr* mirror_t = _gvn.type(array_type_mirror)->isa_instptr();
   if (EnableValhalla && UseFlatArray &&
       (original_t == NULL || mirror_t == NULL ||
        (mirror_t->java_mirror_type() == NULL &&
-        (original_t->elem()->isa_valuetype() ||
+        (original_t->elem()->isa_inlinetype() ||
          (original_t->elem()->make_oopptr() != NULL &&
-          original_t->elem()->make_oopptr()->can_be_value_type()))))) {
+          original_t->elem()->make_oopptr()->can_be_inline_type()))))) {
     // We need to know statically if the copy is to a flattened array
     // or not but can't tell.
     return false;
   }
 
@@ -3927,11 +3927,11 @@
     RegionNode* bailout = new RegionNode(1);
     record_for_igvn(bailout);
 
     // Despite the generic type of Arrays.copyOf, the mirror might be int, int[], etc.
     // Bail out if that is so.
-    // Value type array may have object field that would require a
+    // Inline type array may have object field that would require a
     // write barrier. Conservatively, go to slow path.
     BarrierSetC2* bs = BarrierSet::barrier_set()->barrier_set_c2();
     Node* not_objArray = !bs->array_copy_requires_gc_barriers(false, T_OBJECT, false, BarrierSetC2::Parsing) ?
         generate_typeArray_guard(klass_node, bailout) : generate_non_objArray_guard(klass_node, bailout);
     if (not_objArray != NULL) {
@@ -3966,21 +3966,21 @@
     }
 
     if (UseFlatArray) {
       // Either both or neither new array klass and original array
       // klass must be flattened
-      Node* is_flat = generate_valueArray_guard(klass_node, NULL);
+      Node* is_flat = generate_flatArray_guard(klass_node, NULL);
       if (!original_t->is_not_flat()) {
-        generate_valueArray_guard(original_kls, bailout);
+        generate_flatArray_guard(original_kls, bailout);
       }
       if (is_flat != NULL) {
         RegionNode* r = new RegionNode(2);
         record_for_igvn(r);
         r->init_req(1, control());
         set_control(is_flat);
         if (!original_t->is_not_flat()) {
-          generate_valueArray_guard(original_kls, r);
+          generate_flatArray_guard(original_kls, r);
         }
         bailout->add_req(control());
         set_control(_gvn.transform(r));
       }
     }
@@ -4163,11 +4163,11 @@
   PhiNode*    result_val = new PhiNode(result_reg, TypeInt::INT);
   PhiNode*    result_io  = new PhiNode(result_reg, Type::ABIO);
   PhiNode*    result_mem = new PhiNode(result_reg, Type::MEMORY, TypePtr::BOTTOM);
   Node* obj = argument(0);
 
-  if (obj->is_ValueType() || gvn().type(obj)->is_valuetypeptr()) {
+  if (obj->is_InlineType() || gvn().type(obj)->is_inlinetypeptr()) {
     return false;
   }
 
   if (!is_static) {
     // Check for hashing null object
@@ -4215,11 +4215,11 @@
   // the null check after castPP removal.
   Node* no_ctrl = NULL;
   Node* header = make_load(no_ctrl, header_addr, TypeX_X, TypeX_X->basic_type(), MemNode::unordered);
 
   // Test the header to see if it is unlocked.
-  // This also serves as guard against value types (they have the always_locked_pattern set).
+  // This also serves as guard against inline types (they have the always_locked_pattern set).
   Node *lock_mask      = _gvn.MakeConX(markWord::biased_lock_mask_in_place);
   Node *lmasked_header = _gvn.transform(new AndXNode(header, lock_mask));
   Node *unlocked_val   = _gvn.MakeConX(markWord::unlocked_value);
   Node *chk_unlocked   = _gvn.transform(new CmpXNode( lmasked_header, unlocked_val));
   Node *test_unlocked  = _gvn.transform(new BoolNode( chk_unlocked, BoolTest::ne));
@@ -4283,12 +4283,12 @@
 // public final native Class<?> java.lang.Object.getClass();
 //
 // Build special case code for calls to getClass on an object.
 bool LibraryCallKit::inline_native_getClass() {
   Node* obj = argument(0);
-  if (obj->is_ValueType()) {
-    ciKlass* vk = _gvn.type(obj)->value_klass();
+  if (obj->is_InlineType()) {
+    ciKlass* vk = _gvn.type(obj)->inline_klass();
     set_result(makecon(TypeInstPtr::make(vk->java_mirror())));
     return true;
   }
   obj = null_check_receiver();
   if (stopped())  return true;
@@ -4601,11 +4601,11 @@
   // the bytecode that invokes Object.clone if deoptimization happens.
   { PreserveReexecuteState preexecs(this);
     jvms()->set_should_reexecute(true);
 
     Node* obj = argument(0);
-    if (obj->is_ValueType()) {
+    if (obj->is_InlineType()) {
       return false;
     }
 
     obj = null_check_receiver();
     if (stopped())  return true;
@@ -4616,11 +4616,11 @@
     // know the number and types of fields to convert the clone to
     // loads/stores. Maybe a speculative type can help us.
     if (!obj_type->klass_is_exact() &&
         obj_type->speculative_type() != NULL &&
         obj_type->speculative_type()->is_instance_klass() &&
-        !obj_type->speculative_type()->is_valuetype()) {
+        !obj_type->speculative_type()->is_inlinetype()) {
       ciInstanceKlass* spec_ik = obj_type->speculative_type()->as_instance_klass();
       if (spec_ik->nof_nonstatic_fields() <= ArrayCopyLoadStoreMaxElem &&
           !spec_ik->has_injected_fields()) {
         ciKlass* k = obj_type->klass();
         if (!k->is_instance_klass() ||
@@ -4662,13 +4662,13 @@
       set_control(array_ctl);
 
       BarrierSetC2* bs = BarrierSet::barrier_set()->barrier_set_c2();
       if (bs->array_copy_requires_gc_barriers(true, T_OBJECT, true, BarrierSetC2::Parsing) &&
           (!obj_type->isa_aryptr() || !obj_type->is_aryptr()->is_not_flat())) {
-        // Flattened value type array may have object field that would require a
+        // Flattened inline type array may have object field that would require a
         // write barrier. Conservatively, go to slow path.
-        generate_valueArray_guard(obj_klass, slow_region);
+        generate_flatArray_guard(obj_klass, slow_region);
       }
 
       if (!stopped()) {
         Node* obj_length = load_array_length(obj);
         Node* obj_size  = NULL;
@@ -5141,17 +5141,17 @@
     const Type* toop = TypeOopPtr::make_from_klass(dest_klass_t->klass());
     src = _gvn.transform(new CheckCastPPNode(control(), src, toop));
     src_type = _gvn.type(src);
     top_src  = src_type->isa_aryptr();
 
-    if (top_dest != NULL && !top_dest->elem()->isa_valuetype() && !top_dest->is_not_flat()) {
-      generate_valueArray_guard(dest_klass, slow_region);
+    if (top_dest != NULL && !top_dest->elem()->isa_inlinetype() && !top_dest->is_not_flat()) {
+      generate_flatArray_guard(dest_klass, slow_region);
     }
 
-    if (top_src != NULL && !top_src->elem()->isa_valuetype() && !top_src->is_not_flat()) {
+    if (top_src != NULL && !top_src->elem()->isa_inlinetype() && !top_src->is_not_flat()) {
       Node* src_klass = load_object_klass(src);
-      generate_valueArray_guard(src_klass, slow_region);
+      generate_flatArray_guard(src_klass, slow_region);
     }
 
     {
       PreserveJVMState pjvms(this);
       set_control(_gvn.transform(slow_region));
