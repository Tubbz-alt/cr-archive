<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/share/opto/callGenerator.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="arraycopynode.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="callnode.cpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/share/opto/callGenerator.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #include &quot;precompiled.hpp&quot;
  26 #include &quot;ci/bcEscapeAnalyzer.hpp&quot;
  27 #include &quot;ci/ciCallSite.hpp&quot;
  28 #include &quot;ci/ciObjArray.hpp&quot;
  29 #include &quot;ci/ciMemberName.hpp&quot;
  30 #include &quot;ci/ciMethodHandle.hpp&quot;
  31 #include &quot;classfile/javaClasses.hpp&quot;
  32 #include &quot;compiler/compileLog.hpp&quot;
  33 #include &quot;opto/addnode.hpp&quot;
  34 #include &quot;opto/callGenerator.hpp&quot;
  35 #include &quot;opto/callnode.hpp&quot;
  36 #include &quot;opto/castnode.hpp&quot;
  37 #include &quot;opto/cfgnode.hpp&quot;

  38 #include &quot;opto/parse.hpp&quot;
  39 #include &quot;opto/rootnode.hpp&quot;
  40 #include &quot;opto/runtime.hpp&quot;
  41 #include &quot;opto/subnode.hpp&quot;
<span class="line-removed">  42 #include &quot;opto/valuetypenode.hpp&quot;</span>
  43 #include &quot;runtime/sharedRuntime.hpp&quot;
  44 
  45 // Utility function.
  46 const TypeFunc* CallGenerator::tf() const {
  47   return TypeFunc::make(method());
  48 }
  49 
  50 bool CallGenerator::is_inlined_method_handle_intrinsic(JVMState* jvms, ciMethod* m) {
  51   return is_inlined_method_handle_intrinsic(jvms-&gt;method(), jvms-&gt;bci(), m);
  52 }
  53 
  54 bool CallGenerator::is_inlined_method_handle_intrinsic(ciMethod* caller, int bci, ciMethod* m) {
  55   ciMethod* symbolic_info = caller-&gt;get_method_at_bci(bci);
  56   return is_inlined_method_handle_intrinsic(symbolic_info, m);
  57 }
  58 
  59 bool CallGenerator::is_inlined_method_handle_intrinsic(ciMethod* symbolic_info, ciMethod* m) {
  60   return symbolic_info-&gt;is_method_handle_intrinsic() &amp;&amp; !m-&gt;is_method_handle_intrinsic();
  61 }
  62 
</pre>
<hr />
<pre>
 111   return exits.transfer_exceptions_into_jvms();
 112 }
 113 
 114 //---------------------------DirectCallGenerator------------------------------
 115 // Internal class which handles all out-of-line calls w/o receiver type checks.
 116 class DirectCallGenerator : public CallGenerator {
 117  private:
 118   CallStaticJavaNode* _call_node;
 119   // Force separate memory and I/O projections for the exceptional
 120   // paths to facilitate late inlining.
 121   bool                _separate_io_proj;
 122 
 123  public:
 124   DirectCallGenerator(ciMethod* method, bool separate_io_proj)
 125     : CallGenerator(method),
 126       _call_node(NULL),
 127       _separate_io_proj(separate_io_proj)
 128   {
 129     if (InlineTypeReturnedAsFields &amp;&amp; method-&gt;is_method_handle_intrinsic()) {
 130       // If that call has not been optimized by the time optimizations are over,
<span class="line-modified"> 131       // we&#39;ll need to add a call to create a value type instance from the klass</span>
 132       // returned by the call (see PhaseMacroExpand::expand_mh_intrinsic_return).
 133       // Separating memory and I/O projections for exceptions is required to
 134       // perform that graph transformation.
 135       _separate_io_proj = true;
 136     }
 137   }
 138   virtual JVMState* generate(JVMState* jvms);
 139 
 140   CallStaticJavaNode* call_node() const { return _call_node; }
 141 };
 142 
 143 JVMState* DirectCallGenerator::generate(JVMState* jvms) {
 144   GraphKit kit(jvms);
 145   kit.C-&gt;print_inlining_update(this);
 146   PhaseGVN&amp; gvn = kit.gvn();
 147   bool is_static = method()-&gt;is_static();
 148   address target = is_static ? SharedRuntime::get_resolve_static_call_stub()
 149                              : SharedRuntime::get_resolve_opt_virtual_call_stub();
 150 
 151   if (kit.C-&gt;log() != NULL) {
</pre>
<hr />
<pre>
 199            vtable_index &gt;= 0, &quot;either invalid or usable&quot;);
 200   }
 201   virtual bool      is_virtual() const          { return true; }
 202   virtual JVMState* generate(JVMState* jvms);
 203 };
 204 
 205 JVMState* VirtualCallGenerator::generate(JVMState* jvms) {
 206   GraphKit kit(jvms);
 207   Node* receiver = kit.argument(0);
 208   kit.C-&gt;print_inlining_update(this);
 209 
 210   if (kit.C-&gt;log() != NULL) {
 211     kit.C-&gt;log()-&gt;elem(&quot;virtual_call bci=&#39;%d&#39;&quot;, jvms-&gt;bci());
 212   }
 213 
 214   // If the receiver is a constant null, do not torture the system
 215   // by attempting to call through it.  The compile will proceed
 216   // correctly, but may bail out in final_graph_reshaping, because
 217   // the call instruction will have a seemingly deficient out-count.
 218   // (The bailout says something misleading about an &quot;infinite loop&quot;.)
<span class="line-modified"> 219   if (!receiver-&gt;is_ValueType() &amp;&amp; kit.gvn().type(receiver)-&gt;higher_equal(TypePtr::NULL_PTR)) {</span>
 220     assert(Bytecodes::is_invoke(kit.java_bc()), &quot;%d: %s&quot;, kit.java_bc(), Bytecodes::name(kit.java_bc()));
 221     ciMethod* declared_method = kit.method()-&gt;get_method_at_bci(kit.bci());
 222     int arg_size = declared_method-&gt;signature()-&gt;arg_size_for_bc(kit.java_bc());
 223     kit.inc_sp(arg_size);  // restore arguments
 224     kit.uncommon_trap(Deoptimization::Reason_null_check,
 225                       Deoptimization::Action_none,
 226                       NULL, &quot;null receiver&quot;);
 227     return kit.transfer_exceptions_into_jvms();
 228   }
 229 
 230   // Ideally we would unconditionally do a null check here and let it
 231   // be converted to an implicit check based on profile information.
 232   // However currently the conversion to implicit null checks in
 233   // Block::implicit_null_check() only looks for loads and stores, not calls.
 234   ciMethod *caller = kit.method();
 235   ciMethodData *caller_md = (caller == NULL) ? NULL : caller-&gt;method_data();
 236   if (!UseInlineCaches || !ImplicitNullChecks || !os::zero_page_read_protected() ||
 237        ((ImplicitNullCheckThreshold &gt; 0) &amp;&amp; caller_md &amp;&amp;
 238        (caller_md-&gt;trap_count(Deoptimization::Reason_null_check)
 239        &gt;= (uint)ImplicitNullCheckThreshold))) {
</pre>
<hr />
<pre>
 441       map-&gt;set_req(TypeFunc::Memory, mem);
 442     }
 443 
 444     // blow away old call arguments
 445     for (uint i1 = TypeFunc::Parms; i1 &lt; r-&gt;cnt(); i1++) {
 446       map-&gt;set_req(i1, C-&gt;top());
 447     }
 448     jvms-&gt;set_map(map);
 449 
 450     // Make enough space in the expression stack to transfer
 451     // the incoming arguments and return value.
 452     map-&gt;ensure_stack(jvms, jvms-&gt;method()-&gt;max_stack());
 453     const TypeTuple *domain_sig = call-&gt;_tf-&gt;domain_sig();
 454     ExtendedSignature sig_cc = ExtendedSignature(method()-&gt;get_sig_cc(), SigEntryFilter());
 455     uint nargs = method()-&gt;arg_size();
 456     assert(domain_sig-&gt;cnt() - TypeFunc::Parms == nargs, &quot;inconsistent signature&quot;);
 457 
 458     uint j = TypeFunc::Parms;
 459     for (uint i1 = 0; i1 &lt; nargs; i1++) {
 460       const Type* t = domain_sig-&gt;field_at(TypeFunc::Parms + i1);
<span class="line-modified"> 461       if (method()-&gt;has_scalarized_args() &amp;&amp; t-&gt;is_valuetypeptr() &amp;&amp; !t-&gt;maybe_null()) {</span>
<span class="line-modified"> 462         // Value type arguments are not passed by reference: we get an argument per</span>
<span class="line-modified"> 463         // field of the value type. Build ValueTypeNodes from the value type arguments.</span>
 464         GraphKit arg_kit(jvms, &amp;gvn);
<span class="line-modified"> 465         ValueTypeNode* vt = ValueTypeNode::make_from_multi(&amp;arg_kit, call, sig_cc, t-&gt;value_klass(), j, true);</span>
 466         map-&gt;set_control(arg_kit.control());
 467         map-&gt;set_argument(jvms, i1, vt);
 468       } else {
 469         map-&gt;set_argument(jvms, i1, call-&gt;in(j++));
 470         BasicType bt = t-&gt;basic_type();
 471         while (SigEntry::next_is_reserved(sig_cc, bt, true)) {
 472           j += type2size[bt]; // Skip reserved arguments
 473         }
 474       }
 475     }
 476 
 477     C-&gt;print_inlining_assert_ready();
 478 
 479     C-&gt;print_inlining_move_to(this);
 480 
 481     C-&gt;log_late_inline(this);
 482 
 483     // This check is done here because for_method_handle_inline() method
 484     // needs jvms for inlined state.
 485     if (!do_late_inline_check(jvms)) {
 486       map-&gt;disconnect_inputs(NULL, C);
 487       return;
 488     }
 489 
<span class="line-modified"> 490     // Allocate a buffer for the returned ValueTypeNode because the caller expects an oop return.</span>
 491     // Do this before the method handle call in case the buffer allocation triggers deoptimization.
 492     Node* buffer_oop = NULL;
<span class="line-modified"> 493     if (is_mh_late_inline() &amp;&amp; _inline_cg-&gt;method()-&gt;return_type()-&gt;is_valuetype()) {</span>
 494       GraphKit arg_kit(jvms, &amp;gvn);
 495       {
 496         PreserveReexecuteState preexecs(&amp;arg_kit);
 497         arg_kit.jvms()-&gt;set_should_reexecute(true);
 498         arg_kit.inc_sp(nargs);
<span class="line-modified"> 499         Node* klass_node = arg_kit.makecon(TypeKlassPtr::make(_inline_cg-&gt;method()-&gt;return_type()-&gt;as_value_klass()));</span>
 500         buffer_oop = arg_kit.new_instance(klass_node, NULL, NULL, /* deoptimize_on_exception */ true);
 501       }
 502       jvms = arg_kit.transfer_exceptions_into_jvms();
 503     }
 504 
 505     // Setup default node notes to be picked up by the inlining
 506     Node_Notes* old_nn = C-&gt;node_notes_at(call-&gt;_idx);
 507     if (old_nn != NULL) {
 508       Node_Notes* entry_nn = old_nn-&gt;clone(C);
 509       entry_nn-&gt;set_jvms(jvms);
 510       C-&gt;set_default_node_notes(entry_nn);
 511     }
 512 
 513     // Now perform the inlining using the synthesized JVMState
 514     JVMState* new_jvms = _inline_cg-&gt;generate(jvms);
 515     if (new_jvms == NULL)  return;  // no change
 516     if (C-&gt;failing())      return;
 517 
 518     // Capture any exceptional control flow
 519     GraphKit kit(new_jvms);
 520 
 521     // Find the result object
 522     Node* result = C-&gt;top();
 523     int   result_size = method()-&gt;return_type()-&gt;size();
 524     if (result_size != 0 &amp;&amp; !kit.stopped()) {
 525       result = (result_size == 1) ? kit.pop() : kit.pop_pair();
 526     }
 527 
 528     C-&gt;set_has_loops(C-&gt;has_loops() || _inline_cg-&gt;method()-&gt;has_loops());
 529     C-&gt;env()-&gt;notice_inlined_method(_inline_cg-&gt;method());
 530     C-&gt;set_inlining_progress(true);
 531     C-&gt;set_do_cleanup(kit.stopped()); // path is dead; needs cleanup
 532 
<span class="line-modified"> 533     // Handle value type returns</span>
<span class="line-modified"> 534     bool returned_as_fields = call-&gt;tf()-&gt;returns_value_type_as_fields();</span>
<span class="line-modified"> 535     if (result-&gt;is_ValueType()) {</span>
 536       // Only possible if is_mh_late_inline() when the callee does not &quot;know&quot; that the caller expects an oop
 537       assert(is_mh_late_inline() &amp;&amp; !returned_as_fields, &quot;sanity&quot;);
 538       assert(buffer_oop != NULL, &quot;should have allocated a buffer&quot;);
<span class="line-modified"> 539       ValueTypeNode* vt = result-&gt;as_ValueType();</span>
<span class="line-modified"> 540       vt-&gt;store(&amp;kit, buffer_oop, buffer_oop, vt-&gt;type()-&gt;value_klass(), 0);</span>
 541       // Do not let stores that initialize this buffer be reordered with a subsequent
 542       // store that would make this buffer accessible by other threads.
 543       AllocateNode* alloc = AllocateNode::Ideal_allocation(buffer_oop, &amp;kit.gvn());
 544       assert(alloc != NULL, &quot;must have an allocation node&quot;);
 545       kit.insert_mem_bar(Op_MemBarStoreStore, alloc-&gt;proj_out_or_null(AllocateNode::RawAddress));
 546       result = buffer_oop;
<span class="line-modified"> 547     } else if (result-&gt;is_ValueTypePtr() &amp;&amp; returned_as_fields) {</span>
<span class="line-modified"> 548       result-&gt;as_ValueTypePtr()-&gt;replace_call_results(&amp;kit, call, C);</span>
 549     }
 550 
 551     kit.replace_call(call, result, true);
 552   }
 553 }
 554 
 555 
 556 CallGenerator* CallGenerator::for_late_inline(ciMethod* method, CallGenerator* inline_cg) {
 557   return new LateInlineCallGenerator(method, inline_cg);
 558 }
 559 
 560 class LateInlineMHCallGenerator : public LateInlineCallGenerator {
 561   ciMethod* _caller;
 562   int _attempt;
 563   bool _input_not_const;
 564 
 565   virtual bool do_late_inline_check(JVMState* jvms);
 566   virtual bool already_attempted() const { return _attempt &gt; 0; }
 567 
 568  public:
</pre>
<hr />
<pre>
 848     // Inline failed, so make a direct call.
 849     assert(_if_hit-&gt;is_inline(), &quot;must have been a failed inline&quot;);
 850     CallGenerator* cg = CallGenerator::for_direct_call(_if_hit-&gt;method());
 851     new_jvms = cg-&gt;generate(kit.sync_jvms());
 852   }
 853   kit.add_exception_states_from(new_jvms);
 854   kit.set_jvms(new_jvms);
 855 
 856   // Need to merge slow and fast?
 857   if (slow_map == NULL) {
 858     // The fast path is the only path remaining.
 859     return kit.transfer_exceptions_into_jvms();
 860   }
 861 
 862   if (kit.stopped()) {
 863     // Inlined method threw an exception, so it&#39;s just the slow path after all.
 864     kit.set_jvms(slow_jvms);
 865     return kit.transfer_exceptions_into_jvms();
 866   }
 867 
<span class="line-modified"> 868   // Allocate value types if they are merged with objects (similar to Parse::merge_common())</span>
 869   uint tos = kit.jvms()-&gt;stkoff() + kit.sp();
 870   uint limit = slow_map-&gt;req();
 871   for (uint i = TypeFunc::Parms; i &lt; limit; i++) {
 872     Node* m = kit.map()-&gt;in(i);
 873     Node* n = slow_map-&gt;in(i);
 874     const Type* t = gvn.type(m)-&gt;meet_speculative(gvn.type(n));
<span class="line-modified"> 875     if (m-&gt;is_ValueType() &amp;&amp; !t-&gt;isa_valuetype()) {</span>
<span class="line-modified"> 876       // Allocate value type in fast path</span>
<span class="line-modified"> 877       m = m-&gt;as_ValueType()-&gt;buffer(&amp;kit);</span>
 878       kit.map()-&gt;set_req(i, m);
 879     }
<span class="line-modified"> 880     if (n-&gt;is_ValueType() &amp;&amp; !t-&gt;isa_valuetype()) {</span>
<span class="line-modified"> 881       // Allocate value type in slow path</span>
 882       PreserveJVMState pjvms(&amp;kit);
 883       kit.set_map(slow_map);
<span class="line-modified"> 884       n = n-&gt;as_ValueType()-&gt;buffer(&amp;kit);</span>
 885       kit.map()-&gt;set_req(i, n);
 886       slow_map = kit.stop();
 887     }
 888   }
 889 
 890   // There are 2 branches and the replaced nodes are only valid on
 891   // one: restore the replaced nodes to what they were before the
 892   // branch.
 893   kit.map()-&gt;set_replaced_nodes(replaced_nodes);
 894 
 895   // Finish the diamond.
 896   kit.C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
 897   RegionNode* region = new RegionNode(3);
 898   region-&gt;init_req(1, kit.control());
 899   region-&gt;init_req(2, slow_map-&gt;control());
 900   kit.set_control(gvn.transform(region));
 901   Node* iophi = PhiNode::make(region, kit.i_o(), Type::ABIO);
 902   iophi-&gt;set_req(2, slow_map-&gt;i_o());
 903   kit.set_i_o(gvn.transform(iophi));
 904   // Merge memory
</pre>
<hr />
<pre>
 947 
 948   if (IncrementalInline &amp;&amp; (AlwaysIncrementalInline ||
 949                             (call_site_count &gt; 0 &amp;&amp; (input_not_const || !C-&gt;inlining_incrementally() || C-&gt;over_inlining_cutoff())))) {
 950     return CallGenerator::for_mh_late_inline(caller, callee, input_not_const);
 951   } else {
 952     // Out-of-line call.
 953     return CallGenerator::for_direct_call(callee);
 954   }
 955 }
 956 
 957 static void cast_argument(int nargs, int arg_nb, ciType* t, GraphKit&amp; kit) {
 958   PhaseGVN&amp; gvn = kit.gvn();
 959   Node* arg = kit.argument(arg_nb);
 960   const Type* arg_type = arg-&gt;bottom_type();
 961   const Type* sig_type = TypeOopPtr::make_from_klass(t-&gt;as_klass());
 962   if (arg_type-&gt;isa_oopptr() &amp;&amp; !arg_type-&gt;higher_equal(sig_type)) {
 963     const Type* narrowed_arg_type = arg_type-&gt;join_speculative(sig_type); // keep speculative part
 964     arg = gvn.transform(new CheckCastPPNode(kit.control(), arg, narrowed_arg_type));
 965     kit.set_argument(arg_nb, arg);
 966   }
<span class="line-modified"> 967   if (sig_type-&gt;is_valuetypeptr() &amp;&amp; !arg-&gt;is_ValueType() &amp;&amp;</span>
<span class="line-modified"> 968       !kit.gvn().type(arg)-&gt;maybe_null() &amp;&amp; t-&gt;as_value_klass()-&gt;is_scalarizable()) {</span>
<span class="line-modified"> 969     arg = ValueTypeNode::make_from_oop(&amp;kit, arg, t-&gt;as_value_klass());</span>
 970     kit.set_argument(arg_nb, arg);
 971   }
 972 }
 973 
 974 CallGenerator* CallGenerator::for_method_handle_inline(JVMState* jvms, ciMethod* caller, ciMethod* callee, bool&amp; input_not_const, bool delayed_forbidden) {
 975   GraphKit kit(jvms);
 976   PhaseGVN&amp; gvn = kit.gvn();
 977   Compile* C = kit.C;
 978   vmIntrinsics::ID iid = callee-&gt;intrinsic_id();
 979   input_not_const = true;
 980   switch (iid) {
 981   case vmIntrinsics::_invokeBasic:
 982     {
 983       // Get MethodHandle receiver:
 984       Node* receiver = kit.argument(0);
 985       if (receiver-&gt;Opcode() == Op_ConP) {
 986         input_not_const = false;
 987         const TypeOopPtr* oop_ptr = receiver-&gt;bottom_type()-&gt;is_oopptr();
 988         ciMethod* target = oop_ptr-&gt;const_oop()-&gt;as_method_handle()-&gt;get_vmtarget();
 989         const int vtable_index = Method::invalid_vtable_index;
</pre>
</td>
<td>
<hr />
<pre>
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #include &quot;precompiled.hpp&quot;
  26 #include &quot;ci/bcEscapeAnalyzer.hpp&quot;
  27 #include &quot;ci/ciCallSite.hpp&quot;
  28 #include &quot;ci/ciObjArray.hpp&quot;
  29 #include &quot;ci/ciMemberName.hpp&quot;
  30 #include &quot;ci/ciMethodHandle.hpp&quot;
  31 #include &quot;classfile/javaClasses.hpp&quot;
  32 #include &quot;compiler/compileLog.hpp&quot;
  33 #include &quot;opto/addnode.hpp&quot;
  34 #include &quot;opto/callGenerator.hpp&quot;
  35 #include &quot;opto/callnode.hpp&quot;
  36 #include &quot;opto/castnode.hpp&quot;
  37 #include &quot;opto/cfgnode.hpp&quot;
<span class="line-added">  38 #include &quot;opto/inlinetypenode.hpp&quot;</span>
  39 #include &quot;opto/parse.hpp&quot;
  40 #include &quot;opto/rootnode.hpp&quot;
  41 #include &quot;opto/runtime.hpp&quot;
  42 #include &quot;opto/subnode.hpp&quot;

  43 #include &quot;runtime/sharedRuntime.hpp&quot;
  44 
  45 // Utility function.
  46 const TypeFunc* CallGenerator::tf() const {
  47   return TypeFunc::make(method());
  48 }
  49 
  50 bool CallGenerator::is_inlined_method_handle_intrinsic(JVMState* jvms, ciMethod* m) {
  51   return is_inlined_method_handle_intrinsic(jvms-&gt;method(), jvms-&gt;bci(), m);
  52 }
  53 
  54 bool CallGenerator::is_inlined_method_handle_intrinsic(ciMethod* caller, int bci, ciMethod* m) {
  55   ciMethod* symbolic_info = caller-&gt;get_method_at_bci(bci);
  56   return is_inlined_method_handle_intrinsic(symbolic_info, m);
  57 }
  58 
  59 bool CallGenerator::is_inlined_method_handle_intrinsic(ciMethod* symbolic_info, ciMethod* m) {
  60   return symbolic_info-&gt;is_method_handle_intrinsic() &amp;&amp; !m-&gt;is_method_handle_intrinsic();
  61 }
  62 
</pre>
<hr />
<pre>
 111   return exits.transfer_exceptions_into_jvms();
 112 }
 113 
 114 //---------------------------DirectCallGenerator------------------------------
 115 // Internal class which handles all out-of-line calls w/o receiver type checks.
 116 class DirectCallGenerator : public CallGenerator {
 117  private:
 118   CallStaticJavaNode* _call_node;
 119   // Force separate memory and I/O projections for the exceptional
 120   // paths to facilitate late inlining.
 121   bool                _separate_io_proj;
 122 
 123  public:
 124   DirectCallGenerator(ciMethod* method, bool separate_io_proj)
 125     : CallGenerator(method),
 126       _call_node(NULL),
 127       _separate_io_proj(separate_io_proj)
 128   {
 129     if (InlineTypeReturnedAsFields &amp;&amp; method-&gt;is_method_handle_intrinsic()) {
 130       // If that call has not been optimized by the time optimizations are over,
<span class="line-modified"> 131       // we&#39;ll need to add a call to create an inline type instance from the klass</span>
 132       // returned by the call (see PhaseMacroExpand::expand_mh_intrinsic_return).
 133       // Separating memory and I/O projections for exceptions is required to
 134       // perform that graph transformation.
 135       _separate_io_proj = true;
 136     }
 137   }
 138   virtual JVMState* generate(JVMState* jvms);
 139 
 140   CallStaticJavaNode* call_node() const { return _call_node; }
 141 };
 142 
 143 JVMState* DirectCallGenerator::generate(JVMState* jvms) {
 144   GraphKit kit(jvms);
 145   kit.C-&gt;print_inlining_update(this);
 146   PhaseGVN&amp; gvn = kit.gvn();
 147   bool is_static = method()-&gt;is_static();
 148   address target = is_static ? SharedRuntime::get_resolve_static_call_stub()
 149                              : SharedRuntime::get_resolve_opt_virtual_call_stub();
 150 
 151   if (kit.C-&gt;log() != NULL) {
</pre>
<hr />
<pre>
 199            vtable_index &gt;= 0, &quot;either invalid or usable&quot;);
 200   }
 201   virtual bool      is_virtual() const          { return true; }
 202   virtual JVMState* generate(JVMState* jvms);
 203 };
 204 
 205 JVMState* VirtualCallGenerator::generate(JVMState* jvms) {
 206   GraphKit kit(jvms);
 207   Node* receiver = kit.argument(0);
 208   kit.C-&gt;print_inlining_update(this);
 209 
 210   if (kit.C-&gt;log() != NULL) {
 211     kit.C-&gt;log()-&gt;elem(&quot;virtual_call bci=&#39;%d&#39;&quot;, jvms-&gt;bci());
 212   }
 213 
 214   // If the receiver is a constant null, do not torture the system
 215   // by attempting to call through it.  The compile will proceed
 216   // correctly, but may bail out in final_graph_reshaping, because
 217   // the call instruction will have a seemingly deficient out-count.
 218   // (The bailout says something misleading about an &quot;infinite loop&quot;.)
<span class="line-modified"> 219   if (!receiver-&gt;is_InlineType() &amp;&amp; kit.gvn().type(receiver)-&gt;higher_equal(TypePtr::NULL_PTR)) {</span>
 220     assert(Bytecodes::is_invoke(kit.java_bc()), &quot;%d: %s&quot;, kit.java_bc(), Bytecodes::name(kit.java_bc()));
 221     ciMethod* declared_method = kit.method()-&gt;get_method_at_bci(kit.bci());
 222     int arg_size = declared_method-&gt;signature()-&gt;arg_size_for_bc(kit.java_bc());
 223     kit.inc_sp(arg_size);  // restore arguments
 224     kit.uncommon_trap(Deoptimization::Reason_null_check,
 225                       Deoptimization::Action_none,
 226                       NULL, &quot;null receiver&quot;);
 227     return kit.transfer_exceptions_into_jvms();
 228   }
 229 
 230   // Ideally we would unconditionally do a null check here and let it
 231   // be converted to an implicit check based on profile information.
 232   // However currently the conversion to implicit null checks in
 233   // Block::implicit_null_check() only looks for loads and stores, not calls.
 234   ciMethod *caller = kit.method();
 235   ciMethodData *caller_md = (caller == NULL) ? NULL : caller-&gt;method_data();
 236   if (!UseInlineCaches || !ImplicitNullChecks || !os::zero_page_read_protected() ||
 237        ((ImplicitNullCheckThreshold &gt; 0) &amp;&amp; caller_md &amp;&amp;
 238        (caller_md-&gt;trap_count(Deoptimization::Reason_null_check)
 239        &gt;= (uint)ImplicitNullCheckThreshold))) {
</pre>
<hr />
<pre>
 441       map-&gt;set_req(TypeFunc::Memory, mem);
 442     }
 443 
 444     // blow away old call arguments
 445     for (uint i1 = TypeFunc::Parms; i1 &lt; r-&gt;cnt(); i1++) {
 446       map-&gt;set_req(i1, C-&gt;top());
 447     }
 448     jvms-&gt;set_map(map);
 449 
 450     // Make enough space in the expression stack to transfer
 451     // the incoming arguments and return value.
 452     map-&gt;ensure_stack(jvms, jvms-&gt;method()-&gt;max_stack());
 453     const TypeTuple *domain_sig = call-&gt;_tf-&gt;domain_sig();
 454     ExtendedSignature sig_cc = ExtendedSignature(method()-&gt;get_sig_cc(), SigEntryFilter());
 455     uint nargs = method()-&gt;arg_size();
 456     assert(domain_sig-&gt;cnt() - TypeFunc::Parms == nargs, &quot;inconsistent signature&quot;);
 457 
 458     uint j = TypeFunc::Parms;
 459     for (uint i1 = 0; i1 &lt; nargs; i1++) {
 460       const Type* t = domain_sig-&gt;field_at(TypeFunc::Parms + i1);
<span class="line-modified"> 461       if (method()-&gt;has_scalarized_args() &amp;&amp; t-&gt;is_inlinetypeptr() &amp;&amp; !t-&gt;maybe_null()) {</span>
<span class="line-modified"> 462         // Inline type arguments are not passed by reference: we get an argument per</span>
<span class="line-modified"> 463         // field of the inline type. Build InlineTypeNodes from the inline type arguments.</span>
 464         GraphKit arg_kit(jvms, &amp;gvn);
<span class="line-modified"> 465         InlineTypeNode* vt = InlineTypeNode::make_from_multi(&amp;arg_kit, call, sig_cc, t-&gt;inline_klass(), j, true);</span>
 466         map-&gt;set_control(arg_kit.control());
 467         map-&gt;set_argument(jvms, i1, vt);
 468       } else {
 469         map-&gt;set_argument(jvms, i1, call-&gt;in(j++));
 470         BasicType bt = t-&gt;basic_type();
 471         while (SigEntry::next_is_reserved(sig_cc, bt, true)) {
 472           j += type2size[bt]; // Skip reserved arguments
 473         }
 474       }
 475     }
 476 
 477     C-&gt;print_inlining_assert_ready();
 478 
 479     C-&gt;print_inlining_move_to(this);
 480 
 481     C-&gt;log_late_inline(this);
 482 
 483     // This check is done here because for_method_handle_inline() method
 484     // needs jvms for inlined state.
 485     if (!do_late_inline_check(jvms)) {
 486       map-&gt;disconnect_inputs(NULL, C);
 487       return;
 488     }
 489 
<span class="line-modified"> 490     // Allocate a buffer for the returned InlineTypeNode because the caller expects an oop return.</span>
 491     // Do this before the method handle call in case the buffer allocation triggers deoptimization.
 492     Node* buffer_oop = NULL;
<span class="line-modified"> 493     if (is_mh_late_inline() &amp;&amp; _inline_cg-&gt;method()-&gt;return_type()-&gt;is_inlinetype()) {</span>
 494       GraphKit arg_kit(jvms, &amp;gvn);
 495       {
 496         PreserveReexecuteState preexecs(&amp;arg_kit);
 497         arg_kit.jvms()-&gt;set_should_reexecute(true);
 498         arg_kit.inc_sp(nargs);
<span class="line-modified"> 499         Node* klass_node = arg_kit.makecon(TypeKlassPtr::make(_inline_cg-&gt;method()-&gt;return_type()-&gt;as_inline_klass()));</span>
 500         buffer_oop = arg_kit.new_instance(klass_node, NULL, NULL, /* deoptimize_on_exception */ true);
 501       }
 502       jvms = arg_kit.transfer_exceptions_into_jvms();
 503     }
 504 
 505     // Setup default node notes to be picked up by the inlining
 506     Node_Notes* old_nn = C-&gt;node_notes_at(call-&gt;_idx);
 507     if (old_nn != NULL) {
 508       Node_Notes* entry_nn = old_nn-&gt;clone(C);
 509       entry_nn-&gt;set_jvms(jvms);
 510       C-&gt;set_default_node_notes(entry_nn);
 511     }
 512 
 513     // Now perform the inlining using the synthesized JVMState
 514     JVMState* new_jvms = _inline_cg-&gt;generate(jvms);
 515     if (new_jvms == NULL)  return;  // no change
 516     if (C-&gt;failing())      return;
 517 
 518     // Capture any exceptional control flow
 519     GraphKit kit(new_jvms);
 520 
 521     // Find the result object
 522     Node* result = C-&gt;top();
 523     int   result_size = method()-&gt;return_type()-&gt;size();
 524     if (result_size != 0 &amp;&amp; !kit.stopped()) {
 525       result = (result_size == 1) ? kit.pop() : kit.pop_pair();
 526     }
 527 
 528     C-&gt;set_has_loops(C-&gt;has_loops() || _inline_cg-&gt;method()-&gt;has_loops());
 529     C-&gt;env()-&gt;notice_inlined_method(_inline_cg-&gt;method());
 530     C-&gt;set_inlining_progress(true);
 531     C-&gt;set_do_cleanup(kit.stopped()); // path is dead; needs cleanup
 532 
<span class="line-modified"> 533     // Handle inline type returns</span>
<span class="line-modified"> 534     bool returned_as_fields = call-&gt;tf()-&gt;returns_inline_type_as_fields();</span>
<span class="line-modified"> 535     if (result-&gt;is_InlineType()) {</span>
 536       // Only possible if is_mh_late_inline() when the callee does not &quot;know&quot; that the caller expects an oop
 537       assert(is_mh_late_inline() &amp;&amp; !returned_as_fields, &quot;sanity&quot;);
 538       assert(buffer_oop != NULL, &quot;should have allocated a buffer&quot;);
<span class="line-modified"> 539       InlineTypeNode* vt = result-&gt;as_InlineType();</span>
<span class="line-modified"> 540       vt-&gt;store(&amp;kit, buffer_oop, buffer_oop, vt-&gt;type()-&gt;inline_klass(), 0);</span>
 541       // Do not let stores that initialize this buffer be reordered with a subsequent
 542       // store that would make this buffer accessible by other threads.
 543       AllocateNode* alloc = AllocateNode::Ideal_allocation(buffer_oop, &amp;kit.gvn());
 544       assert(alloc != NULL, &quot;must have an allocation node&quot;);
 545       kit.insert_mem_bar(Op_MemBarStoreStore, alloc-&gt;proj_out_or_null(AllocateNode::RawAddress));
 546       result = buffer_oop;
<span class="line-modified"> 547     } else if (result-&gt;is_InlineTypePtr() &amp;&amp; returned_as_fields) {</span>
<span class="line-modified"> 548       result-&gt;as_InlineTypePtr()-&gt;replace_call_results(&amp;kit, call, C);</span>
 549     }
 550 
 551     kit.replace_call(call, result, true);
 552   }
 553 }
 554 
 555 
 556 CallGenerator* CallGenerator::for_late_inline(ciMethod* method, CallGenerator* inline_cg) {
 557   return new LateInlineCallGenerator(method, inline_cg);
 558 }
 559 
 560 class LateInlineMHCallGenerator : public LateInlineCallGenerator {
 561   ciMethod* _caller;
 562   int _attempt;
 563   bool _input_not_const;
 564 
 565   virtual bool do_late_inline_check(JVMState* jvms);
 566   virtual bool already_attempted() const { return _attempt &gt; 0; }
 567 
 568  public:
</pre>
<hr />
<pre>
 848     // Inline failed, so make a direct call.
 849     assert(_if_hit-&gt;is_inline(), &quot;must have been a failed inline&quot;);
 850     CallGenerator* cg = CallGenerator::for_direct_call(_if_hit-&gt;method());
 851     new_jvms = cg-&gt;generate(kit.sync_jvms());
 852   }
 853   kit.add_exception_states_from(new_jvms);
 854   kit.set_jvms(new_jvms);
 855 
 856   // Need to merge slow and fast?
 857   if (slow_map == NULL) {
 858     // The fast path is the only path remaining.
 859     return kit.transfer_exceptions_into_jvms();
 860   }
 861 
 862   if (kit.stopped()) {
 863     // Inlined method threw an exception, so it&#39;s just the slow path after all.
 864     kit.set_jvms(slow_jvms);
 865     return kit.transfer_exceptions_into_jvms();
 866   }
 867 
<span class="line-modified"> 868   // Allocate inline types if they are merged with objects (similar to Parse::merge_common())</span>
 869   uint tos = kit.jvms()-&gt;stkoff() + kit.sp();
 870   uint limit = slow_map-&gt;req();
 871   for (uint i = TypeFunc::Parms; i &lt; limit; i++) {
 872     Node* m = kit.map()-&gt;in(i);
 873     Node* n = slow_map-&gt;in(i);
 874     const Type* t = gvn.type(m)-&gt;meet_speculative(gvn.type(n));
<span class="line-modified"> 875     if (m-&gt;is_InlineType() &amp;&amp; !t-&gt;isa_inlinetype()) {</span>
<span class="line-modified"> 876       // Allocate inline type in fast path</span>
<span class="line-modified"> 877       m = m-&gt;as_InlineType()-&gt;buffer(&amp;kit);</span>
 878       kit.map()-&gt;set_req(i, m);
 879     }
<span class="line-modified"> 880     if (n-&gt;is_InlineType() &amp;&amp; !t-&gt;isa_inlinetype()) {</span>
<span class="line-modified"> 881       // Allocate inline type in slow path</span>
 882       PreserveJVMState pjvms(&amp;kit);
 883       kit.set_map(slow_map);
<span class="line-modified"> 884       n = n-&gt;as_InlineType()-&gt;buffer(&amp;kit);</span>
 885       kit.map()-&gt;set_req(i, n);
 886       slow_map = kit.stop();
 887     }
 888   }
 889 
 890   // There are 2 branches and the replaced nodes are only valid on
 891   // one: restore the replaced nodes to what they were before the
 892   // branch.
 893   kit.map()-&gt;set_replaced_nodes(replaced_nodes);
 894 
 895   // Finish the diamond.
 896   kit.C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
 897   RegionNode* region = new RegionNode(3);
 898   region-&gt;init_req(1, kit.control());
 899   region-&gt;init_req(2, slow_map-&gt;control());
 900   kit.set_control(gvn.transform(region));
 901   Node* iophi = PhiNode::make(region, kit.i_o(), Type::ABIO);
 902   iophi-&gt;set_req(2, slow_map-&gt;i_o());
 903   kit.set_i_o(gvn.transform(iophi));
 904   // Merge memory
</pre>
<hr />
<pre>
 947 
 948   if (IncrementalInline &amp;&amp; (AlwaysIncrementalInline ||
 949                             (call_site_count &gt; 0 &amp;&amp; (input_not_const || !C-&gt;inlining_incrementally() || C-&gt;over_inlining_cutoff())))) {
 950     return CallGenerator::for_mh_late_inline(caller, callee, input_not_const);
 951   } else {
 952     // Out-of-line call.
 953     return CallGenerator::for_direct_call(callee);
 954   }
 955 }
 956 
 957 static void cast_argument(int nargs, int arg_nb, ciType* t, GraphKit&amp; kit) {
 958   PhaseGVN&amp; gvn = kit.gvn();
 959   Node* arg = kit.argument(arg_nb);
 960   const Type* arg_type = arg-&gt;bottom_type();
 961   const Type* sig_type = TypeOopPtr::make_from_klass(t-&gt;as_klass());
 962   if (arg_type-&gt;isa_oopptr() &amp;&amp; !arg_type-&gt;higher_equal(sig_type)) {
 963     const Type* narrowed_arg_type = arg_type-&gt;join_speculative(sig_type); // keep speculative part
 964     arg = gvn.transform(new CheckCastPPNode(kit.control(), arg, narrowed_arg_type));
 965     kit.set_argument(arg_nb, arg);
 966   }
<span class="line-modified"> 967   if (sig_type-&gt;is_inlinetypeptr() &amp;&amp; !arg-&gt;is_InlineType() &amp;&amp;</span>
<span class="line-modified"> 968       !kit.gvn().type(arg)-&gt;maybe_null() &amp;&amp; t-&gt;as_inline_klass()-&gt;is_scalarizable()) {</span>
<span class="line-modified"> 969     arg = InlineTypeNode::make_from_oop(&amp;kit, arg, t-&gt;as_inline_klass());</span>
 970     kit.set_argument(arg_nb, arg);
 971   }
 972 }
 973 
 974 CallGenerator* CallGenerator::for_method_handle_inline(JVMState* jvms, ciMethod* caller, ciMethod* callee, bool&amp; input_not_const, bool delayed_forbidden) {
 975   GraphKit kit(jvms);
 976   PhaseGVN&amp; gvn = kit.gvn();
 977   Compile* C = kit.C;
 978   vmIntrinsics::ID iid = callee-&gt;intrinsic_id();
 979   input_not_const = true;
 980   switch (iid) {
 981   case vmIntrinsics::_invokeBasic:
 982     {
 983       // Get MethodHandle receiver:
 984       Node* receiver = kit.argument(0);
 985       if (receiver-&gt;Opcode() == Op_ConP) {
 986         input_not_const = false;
 987         const TypeOopPtr* oop_ptr = receiver-&gt;bottom_type()-&gt;is_oopptr();
 988         ciMethod* target = oop_ptr-&gt;const_oop()-&gt;as_method_handle()-&gt;get_vmtarget();
 989         const int vtable_index = Method::invalid_vtable_index;
</pre>
</td>
</tr>
</table>
<center><a href="arraycopynode.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="callnode.cpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>