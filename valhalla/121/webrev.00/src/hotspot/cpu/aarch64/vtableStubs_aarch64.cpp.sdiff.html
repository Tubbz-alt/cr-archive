<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/cpu/aarch64/vtableStubs_aarch64.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="stubGenerator_aarch64.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="../x86/c1_CodeStubs_x86.cpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/cpu/aarch64/vtableStubs_aarch64.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
 48 #endif
 49 
 50 VtableStub* VtableStubs::create_vtable_stub(int vtable_index, bool caller_is_c1) {
 51   // Read &quot;A word on VtableStub sizing&quot; in share/code/vtableStubs.hpp for details on stub sizing.
 52   const int stub_code_length = code_size_limit(true);
 53   VtableStub* s = new(stub_code_length) VtableStub(true, vtable_index, caller_is_c1);
 54   // Can be NULL if there is no free space in the code cache.
 55   if (s == NULL) {
 56     return NULL;
 57   }
 58 
 59   // Count unused bytes in instruction sequences of variable size.
 60   // We add them to the computed buffer size in order to avoid
 61   // overflow in subsequently generated stubs.
 62   address   start_pc;
 63   int       slop_bytes = 0;
 64   int       slop_delta = 0;
 65 
 66 // No variance was detected in vtable stub sizes. Setting index_dependent_slop == 0 will unveil any deviation from this observation.
 67   const int index_dependent_slop     = 0;
<span class="line-modified"> 68   ByteSize  entry_offset = caller_is_c1 ? Method::from_compiled_value_offset() :  Method::from_compiled_value_ro_offset();</span>
 69 
 70   ResourceMark    rm;
 71   CodeBuffer      cb(s-&gt;entry_point(), stub_code_length);
 72   MacroAssembler* masm = new MacroAssembler(&amp;cb);
 73 
 74 #if (!defined(PRODUCT) &amp;&amp; defined(COMPILER2))
 75   if (CountCompiledCalls) {
 76     __ lea(r16, ExternalAddress((address) SharedRuntime::nof_megamorphic_calls_addr()));
 77     __ incrementw(Address(r16));
 78   }
 79 #endif
 80 
 81   // get receiver (need to skip return address on top of stack)
 82   assert(VtableStub::receiver_location() == j_rarg0-&gt;as_VMReg(), &quot;receiver expected in j_rarg0&quot;);
 83 
 84   // get receiver klass
 85   address npe_addr = __ pc();
 86   __ load_klass(r16, j_rarg0);
 87 
 88 #ifndef PRODUCT
</pre>
<hr />
<pre>
142 }
143 
144 
145 VtableStub* VtableStubs::create_itable_stub(int itable_index, bool caller_is_c1) { 
146   // Read &quot;A word on VtableStub sizing&quot; in share/code/vtableStubs.hpp for details on stub sizing.
147   const int stub_code_length = code_size_limit(false);
148   VtableStub* s = new(stub_code_length) VtableStub(false, itable_index, caller_is_c1);
149   // Can be NULL if there is no free space in the code cache.
150   if (s == NULL) {
151     return NULL;
152   }
153   // Count unused bytes in instruction sequences of variable size.
154   // We add them to the computed buffer size in order to avoid
155   // overflow in subsequently generated stubs.
156   address   start_pc;
157   int       slop_bytes = 0;
158   int       slop_delta = 0;
159 
160   const int index_dependent_slop = (itable_index == 0) ? 4 :     // code size change with transition from 8-bit to 32-bit constant (@index == 16).
161                                    (itable_index &lt; 16) ? 3 : 0;  // index == 0 generates even shorter code.
<span class="line-modified">162   ByteSize  entry_offset = caller_is_c1 ? Method::from_compiled_value_offset() :  Method::from_compiled_value_ro_offset();</span>
163 
164   ResourceMark    rm;
165   CodeBuffer      cb(s-&gt;entry_point(), stub_code_length);
166   MacroAssembler* masm = new MacroAssembler(&amp;cb);
167 
168 #if (!defined(PRODUCT) &amp;&amp; defined(COMPILER2))
169   if (CountCompiledCalls) {
170     __ lea(r10, ExternalAddress((address) SharedRuntime::nof_megamorphic_calls_addr()));
171     __ incrementw(Address(r10));
172   }
173 #endif
174 
175   // get receiver (need to skip return address on top of stack)
176   assert(VtableStub::receiver_location() == j_rarg0-&gt;as_VMReg(), &quot;receiver expected in j_rarg0&quot;);
177 
178   // Entry arguments:
179   //  rscratch2: CompiledICHolder
180   //  j_rarg0: Receiver
181 
182   // This stub is called from compiled code which has no callee-saved registers,
</pre>
</td>
<td>
<hr />
<pre>
 48 #endif
 49 
 50 VtableStub* VtableStubs::create_vtable_stub(int vtable_index, bool caller_is_c1) {
 51   // Read &quot;A word on VtableStub sizing&quot; in share/code/vtableStubs.hpp for details on stub sizing.
 52   const int stub_code_length = code_size_limit(true);
 53   VtableStub* s = new(stub_code_length) VtableStub(true, vtable_index, caller_is_c1);
 54   // Can be NULL if there is no free space in the code cache.
 55   if (s == NULL) {
 56     return NULL;
 57   }
 58 
 59   // Count unused bytes in instruction sequences of variable size.
 60   // We add them to the computed buffer size in order to avoid
 61   // overflow in subsequently generated stubs.
 62   address   start_pc;
 63   int       slop_bytes = 0;
 64   int       slop_delta = 0;
 65 
 66 // No variance was detected in vtable stub sizes. Setting index_dependent_slop == 0 will unveil any deviation from this observation.
 67   const int index_dependent_slop     = 0;
<span class="line-modified"> 68   ByteSize  entry_offset = caller_is_c1 ? Method::from_compiled_inline_offset() :  Method::from_compiled_inline_ro_offset();</span>
 69 
 70   ResourceMark    rm;
 71   CodeBuffer      cb(s-&gt;entry_point(), stub_code_length);
 72   MacroAssembler* masm = new MacroAssembler(&amp;cb);
 73 
 74 #if (!defined(PRODUCT) &amp;&amp; defined(COMPILER2))
 75   if (CountCompiledCalls) {
 76     __ lea(r16, ExternalAddress((address) SharedRuntime::nof_megamorphic_calls_addr()));
 77     __ incrementw(Address(r16));
 78   }
 79 #endif
 80 
 81   // get receiver (need to skip return address on top of stack)
 82   assert(VtableStub::receiver_location() == j_rarg0-&gt;as_VMReg(), &quot;receiver expected in j_rarg0&quot;);
 83 
 84   // get receiver klass
 85   address npe_addr = __ pc();
 86   __ load_klass(r16, j_rarg0);
 87 
 88 #ifndef PRODUCT
</pre>
<hr />
<pre>
142 }
143 
144 
145 VtableStub* VtableStubs::create_itable_stub(int itable_index, bool caller_is_c1) { 
146   // Read &quot;A word on VtableStub sizing&quot; in share/code/vtableStubs.hpp for details on stub sizing.
147   const int stub_code_length = code_size_limit(false);
148   VtableStub* s = new(stub_code_length) VtableStub(false, itable_index, caller_is_c1);
149   // Can be NULL if there is no free space in the code cache.
150   if (s == NULL) {
151     return NULL;
152   }
153   // Count unused bytes in instruction sequences of variable size.
154   // We add them to the computed buffer size in order to avoid
155   // overflow in subsequently generated stubs.
156   address   start_pc;
157   int       slop_bytes = 0;
158   int       slop_delta = 0;
159 
160   const int index_dependent_slop = (itable_index == 0) ? 4 :     // code size change with transition from 8-bit to 32-bit constant (@index == 16).
161                                    (itable_index &lt; 16) ? 3 : 0;  // index == 0 generates even shorter code.
<span class="line-modified">162   ByteSize  entry_offset = caller_is_c1 ? Method::from_compiled_inline_offset() :  Method::from_compiled_inline_ro_offset();</span>
163 
164   ResourceMark    rm;
165   CodeBuffer      cb(s-&gt;entry_point(), stub_code_length);
166   MacroAssembler* masm = new MacroAssembler(&amp;cb);
167 
168 #if (!defined(PRODUCT) &amp;&amp; defined(COMPILER2))
169   if (CountCompiledCalls) {
170     __ lea(r10, ExternalAddress((address) SharedRuntime::nof_megamorphic_calls_addr()));
171     __ incrementw(Address(r10));
172   }
173 #endif
174 
175   // get receiver (need to skip return address on top of stack)
176   assert(VtableStub::receiver_location() == j_rarg0-&gt;as_VMReg(), &quot;receiver expected in j_rarg0&quot;);
177 
178   // Entry arguments:
179   //  rscratch2: CompiledICHolder
180   //  j_rarg0: Receiver
181 
182   // This stub is called from compiled code which has no callee-saved registers,
</pre>
</td>
</tr>
</table>
<center><a href="stubGenerator_aarch64.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="../x86/c1_CodeStubs_x86.cpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>