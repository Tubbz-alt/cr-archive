<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/share/opto/library_call.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="graphKit.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="matcher.cpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/share/opto/library_call.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
  36 #include &quot;oops/objArrayKlass.hpp&quot;
  37 #include &quot;opto/addnode.hpp&quot;
  38 #include &quot;opto/arraycopynode.hpp&quot;
  39 #include &quot;opto/c2compiler.hpp&quot;
  40 #include &quot;opto/callGenerator.hpp&quot;
  41 #include &quot;opto/castnode.hpp&quot;
  42 #include &quot;opto/cfgnode.hpp&quot;
  43 #include &quot;opto/convertnode.hpp&quot;
  44 #include &quot;opto/countbitsnode.hpp&quot;
  45 #include &quot;opto/intrinsicnode.hpp&quot;
  46 #include &quot;opto/idealKit.hpp&quot;
  47 #include &quot;opto/mathexactnode.hpp&quot;
  48 #include &quot;opto/movenode.hpp&quot;
  49 #include &quot;opto/mulnode.hpp&quot;
  50 #include &quot;opto/narrowptrnode.hpp&quot;
  51 #include &quot;opto/opaquenode.hpp&quot;
  52 #include &quot;opto/parse.hpp&quot;
  53 #include &quot;opto/runtime.hpp&quot;
  54 #include &quot;opto/rootnode.hpp&quot;
  55 #include &quot;opto/subnode.hpp&quot;

  56 #include &quot;prims/nativeLookup.hpp&quot;
  57 #include &quot;prims/unsafe.hpp&quot;
  58 #include &quot;runtime/objectMonitor.hpp&quot;
  59 #include &quot;runtime/sharedRuntime.hpp&quot;
  60 #include &quot;utilities/macros.hpp&quot;
  61 #include &quot;utilities/powerOfTwo.hpp&quot;
  62 
  63 class LibraryIntrinsic : public InlineCallGenerator {
  64   // Extend the set of intrinsics known to the runtime:
  65  public:
  66  private:
  67   bool             _is_virtual;
  68   bool             _does_virtual_dispatch;
  69   int8_t           _predicates_count;  // Intrinsic is predicated by several conditions
  70   int8_t           _last_predicate; // Last generated predicate
  71   vmIntrinsics::ID _intrinsic_id;
  72 
  73  public:
  74   LibraryIntrinsic(ciMethod* m, bool is_virtual, int predicates_count, bool does_virtual_dispatch, vmIntrinsics::ID id)
  75     : InlineCallGenerator(m),
</pre>
<hr />
<pre>
 117       ciSignature* declared_signature = NULL;
 118       ciMethod* ignored_callee = caller()-&gt;get_method_at_bci(bci(), ignored_will_link, &amp;declared_signature);
 119       const int nargs = declared_signature-&gt;arg_size_for_bc(caller()-&gt;java_code_at_bci(bci()));
 120       _reexecute_sp = sp() + nargs;  // &quot;push&quot; arguments back on stack
 121     }
 122   }
 123 
 124   virtual LibraryCallKit* is_LibraryCallKit() const { return (LibraryCallKit*)this; }
 125 
 126   ciMethod*         caller()    const    { return jvms()-&gt;method(); }
 127   int               bci()       const    { return jvms()-&gt;bci(); }
 128   LibraryIntrinsic* intrinsic() const    { return _intrinsic; }
 129   vmIntrinsics::ID  intrinsic_id() const { return _intrinsic-&gt;intrinsic_id(); }
 130   ciMethod*         callee()    const    { return _intrinsic-&gt;method(); }
 131 
 132   bool  try_to_inline(int predicate);
 133   Node* try_to_predicate(int predicate);
 134 
 135   void push_result() {
 136     // Push the result onto the stack.
<span class="line-modified"> 137     if (!stopped() &amp;&amp; result() != NULL) {</span>
<span class="line-modified"> 138       BasicType bt = result()-&gt;bottom_type()-&gt;basic_type();</span>
<span class="line-modified"> 139       push_node(bt, result());</span>








 140     }
 141   }
 142 
 143  private:
 144   void fatal_unexpected_iid(vmIntrinsics::ID iid) {
 145     fatal(&quot;unexpected intrinsic %d: %s&quot;, iid, vmIntrinsics::name_at(iid));
 146   }
 147 
 148   void  set_result(Node* n) { assert(_result == NULL, &quot;only set once&quot;); _result = n; }
 149   void  set_result(RegionNode* region, PhiNode* value);
 150   Node*     result() { return _result; }
 151 
 152   virtual int reexecute_sp() { return _reexecute_sp; }
 153 
 154   // Helper functions to inline natives
 155   Node* generate_guard(Node* test, RegionNode* region, float true_prob);
 156   Node* generate_slow_guard(Node* test, RegionNode* region);
 157   Node* generate_fair_guard(Node* test, RegionNode* region);
 158   Node* generate_negative_guard(Node* index, RegionNode* region,
 159                                 // resulting CastII of index:
 160                                 Node* *pos_index = NULL);
 161   Node* generate_limit_guard(Node* offset, Node* subseq_length,
 162                              Node* array_length,
 163                              RegionNode* region);
 164   void  generate_string_range_check(Node* array, Node* offset,
 165                                     Node* length, bool char_count);
 166   Node* generate_current_thread(Node* &amp;tls_output);
<span class="line-removed"> 167   Node* load_mirror_from_klass(Node* klass);</span>
 168   Node* load_klass_from_mirror_common(Node* mirror, bool never_see_null,
 169                                       RegionNode* region, int null_path,
 170                                       int offset);
 171   Node* load_klass_from_mirror(Node* mirror, bool never_see_null,
 172                                RegionNode* region, int null_path) {
 173     int offset = java_lang_Class::klass_offset_in_bytes();
 174     return load_klass_from_mirror_common(mirror, never_see_null,
 175                                          region, null_path,
 176                                          offset);
 177   }
 178   Node* load_array_klass_from_mirror(Node* mirror, bool never_see_null,
 179                                      RegionNode* region, int null_path) {
 180     int offset = java_lang_Class::array_klass_offset_in_bytes();
 181     return load_klass_from_mirror_common(mirror, never_see_null,
 182                                          region, null_path,
 183                                          offset);
 184   }
 185   Node* generate_access_flags_guard(Node* kls,
 186                                     int modifier_mask, int modifier_bits,
 187                                     RegionNode* region);
 188   Node* generate_interface_guard(Node* kls, RegionNode* region);











 189   Node* generate_hidden_class_guard(Node* kls, RegionNode* region);

 190   Node* generate_array_guard(Node* kls, RegionNode* region) {
<span class="line-modified"> 191     return generate_array_guard_common(kls, region, false, false);</span>
 192   }
 193   Node* generate_non_array_guard(Node* kls, RegionNode* region) {
<span class="line-modified"> 194     return generate_array_guard_common(kls, region, false, true);</span>
 195   }
 196   Node* generate_objArray_guard(Node* kls, RegionNode* region) {
<span class="line-modified"> 197     return generate_array_guard_common(kls, region, true, false);</span>
 198   }
 199   Node* generate_non_objArray_guard(Node* kls, RegionNode* region) {
<span class="line-modified"> 200     return generate_array_guard_common(kls, region, true, true);</span>







 201   }
<span class="line-modified"> 202   Node* generate_array_guard_common(Node* kls, RegionNode* region,</span>
<span class="line-removed"> 203                                     bool obj_array, bool not_array);</span>
 204   Node* generate_virtual_guard(Node* obj_klass, RegionNode* slow_region);
 205   CallJavaNode* generate_method_call(vmIntrinsics::ID method_id,
 206                                      bool is_virtual = false, bool is_static = false);
 207   CallJavaNode* generate_method_call_static(vmIntrinsics::ID method_id) {
 208     return generate_method_call(method_id, false, true);
 209   }
 210   CallJavaNode* generate_method_call_virtual(vmIntrinsics::ID method_id) {
 211     return generate_method_call(method_id, true, false);
 212   }
 213   Node * load_field_from_object(Node * fromObj, const char * fieldName, const char * fieldTypeString, bool is_exact, bool is_static, ciInstanceKlass * fromKls);
 214   Node * field_address_from_object(Node * fromObj, const char * fieldName, const char * fieldTypeString, bool is_exact, bool is_static, ciInstanceKlass * fromKls);
 215 
 216   Node* make_string_method_node(int opcode, Node* str1_start, Node* cnt1, Node* str2_start, Node* cnt2, StrIntrinsicNode::ArgEnc ae);
 217   bool inline_string_compareTo(StrIntrinsicNode::ArgEnc ae);
 218   bool inline_string_indexOf(StrIntrinsicNode::ArgEnc ae);
 219   bool inline_string_indexOfI(StrIntrinsicNode::ArgEnc ae);
 220   Node* make_indexOf_node(Node* src_start, Node* src_count, Node* tgt_start, Node* tgt_count,
 221                           RegionNode* region, Node* phi, StrIntrinsicNode::ArgEnc ae);
 222   bool inline_string_indexOfChar();
 223   bool inline_string_equals(StrIntrinsicNode::ArgEnc ae);
</pre>
<hr />
<pre>
 241   bool inline_math_negateExactI();
 242   bool inline_math_negateExactL();
 243   bool inline_math_subtractExactI(bool is_decrement);
 244   bool inline_math_subtractExactL(bool is_decrement);
 245   bool inline_min_max(vmIntrinsics::ID id);
 246   bool inline_notify(vmIntrinsics::ID id);
 247   Node* generate_min_max(vmIntrinsics::ID id, Node* x, Node* y);
 248   // This returns Type::AnyPtr, RawPtr, or OopPtr.
 249   int classify_unsafe_addr(Node* &amp;base, Node* &amp;offset, BasicType type);
 250   Node* make_unsafe_address(Node*&amp; base, Node* offset, DecoratorSet decorators, BasicType type = T_ILLEGAL, bool can_cast = false);
 251 
 252   typedef enum { Relaxed, Opaque, Volatile, Acquire, Release } AccessKind;
 253   DecoratorSet mo_decorator_for_access_kind(AccessKind kind);
 254   bool inline_unsafe_access(bool is_store, BasicType type, AccessKind kind, bool is_unaligned);
 255   static bool klass_needs_init_guard(Node* kls);
 256   bool inline_unsafe_allocate();
 257   bool inline_unsafe_newArray(bool uninitialized);
 258   bool inline_unsafe_writeback0();
 259   bool inline_unsafe_writebackSync0(bool is_pre);
 260   bool inline_unsafe_copyMemory();


 261   bool inline_native_currentThread();
 262 
 263   bool inline_native_time_funcs(address method, const char* funcName);
 264 #ifdef JFR_HAVE_INTRINSICS
 265   bool inline_native_classID();
 266   bool inline_native_getEventWriter();
 267 #endif
 268   bool inline_native_Class_query(vmIntrinsics::ID id);
 269   bool inline_native_subtype_check();
 270   bool inline_native_getLength();
 271   bool inline_array_copyOf(bool is_copyOfRange);
 272   bool inline_array_equals(StrIntrinsicNode::ArgEnc ae);
 273   bool inline_preconditions_checkIndex();
 274   void copy_to_clone(Node* obj, Node* alloc_obj, Node* obj_size, bool is_array);
 275   bool inline_native_clone(bool is_virtual);
 276   bool inline_native_Reflection_getCallerClass();
 277   // Helper function for inlining native object hash method
 278   bool inline_native_hashcode(bool is_virtual, bool is_static);
 279   bool inline_native_getClass();
 280 
</pre>
<hr />
<pre>
 585   case vmIntrinsics::_indexOfU:                 return inline_string_indexOf(StrIntrinsicNode::UU);
 586   case vmIntrinsics::_indexOfUL:                return inline_string_indexOf(StrIntrinsicNode::UL);
 587   case vmIntrinsics::_indexOfIL:                return inline_string_indexOfI(StrIntrinsicNode::LL);
 588   case vmIntrinsics::_indexOfIU:                return inline_string_indexOfI(StrIntrinsicNode::UU);
 589   case vmIntrinsics::_indexOfIUL:               return inline_string_indexOfI(StrIntrinsicNode::UL);
 590   case vmIntrinsics::_indexOfU_char:            return inline_string_indexOfChar();
 591 
 592   case vmIntrinsics::_equalsL:                  return inline_string_equals(StrIntrinsicNode::LL);
 593   case vmIntrinsics::_equalsU:                  return inline_string_equals(StrIntrinsicNode::UU);
 594 
 595   case vmIntrinsics::_toBytesStringU:           return inline_string_toBytesU();
 596   case vmIntrinsics::_getCharsStringU:          return inline_string_getCharsU();
 597   case vmIntrinsics::_getCharStringU:           return inline_string_char_access(!is_store);
 598   case vmIntrinsics::_putCharStringU:           return inline_string_char_access( is_store);
 599 
 600   case vmIntrinsics::_compressStringC:
 601   case vmIntrinsics::_compressStringB:          return inline_string_copy( is_compress);
 602   case vmIntrinsics::_inflateStringC:
 603   case vmIntrinsics::_inflateStringB:           return inline_string_copy(!is_compress);
 604 


 605   case vmIntrinsics::_getReference:             return inline_unsafe_access(!is_store, T_OBJECT,   Relaxed, false);
 606   case vmIntrinsics::_getBoolean:               return inline_unsafe_access(!is_store, T_BOOLEAN,  Relaxed, false);
 607   case vmIntrinsics::_getByte:                  return inline_unsafe_access(!is_store, T_BYTE,     Relaxed, false);
 608   case vmIntrinsics::_getShort:                 return inline_unsafe_access(!is_store, T_SHORT,    Relaxed, false);
 609   case vmIntrinsics::_getChar:                  return inline_unsafe_access(!is_store, T_CHAR,     Relaxed, false);
 610   case vmIntrinsics::_getInt:                   return inline_unsafe_access(!is_store, T_INT,      Relaxed, false);
 611   case vmIntrinsics::_getLong:                  return inline_unsafe_access(!is_store, T_LONG,     Relaxed, false);
 612   case vmIntrinsics::_getFloat:                 return inline_unsafe_access(!is_store, T_FLOAT,    Relaxed, false);
 613   case vmIntrinsics::_getDouble:                return inline_unsafe_access(!is_store, T_DOUBLE,   Relaxed, false);

 614 
 615   case vmIntrinsics::_putReference:             return inline_unsafe_access( is_store, T_OBJECT,   Relaxed, false);
 616   case vmIntrinsics::_putBoolean:               return inline_unsafe_access( is_store, T_BOOLEAN,  Relaxed, false);
 617   case vmIntrinsics::_putByte:                  return inline_unsafe_access( is_store, T_BYTE,     Relaxed, false);
 618   case vmIntrinsics::_putShort:                 return inline_unsafe_access( is_store, T_SHORT,    Relaxed, false);
 619   case vmIntrinsics::_putChar:                  return inline_unsafe_access( is_store, T_CHAR,     Relaxed, false);
 620   case vmIntrinsics::_putInt:                   return inline_unsafe_access( is_store, T_INT,      Relaxed, false);
 621   case vmIntrinsics::_putLong:                  return inline_unsafe_access( is_store, T_LONG,     Relaxed, false);
 622   case vmIntrinsics::_putFloat:                 return inline_unsafe_access( is_store, T_FLOAT,    Relaxed, false);
 623   case vmIntrinsics::_putDouble:                return inline_unsafe_access( is_store, T_DOUBLE,   Relaxed, false);

 624 
 625   case vmIntrinsics::_getReferenceVolatile:     return inline_unsafe_access(!is_store, T_OBJECT,   Volatile, false);
 626   case vmIntrinsics::_getBooleanVolatile:       return inline_unsafe_access(!is_store, T_BOOLEAN,  Volatile, false);
 627   case vmIntrinsics::_getByteVolatile:          return inline_unsafe_access(!is_store, T_BYTE,     Volatile, false);
 628   case vmIntrinsics::_getShortVolatile:         return inline_unsafe_access(!is_store, T_SHORT,    Volatile, false);
 629   case vmIntrinsics::_getCharVolatile:          return inline_unsafe_access(!is_store, T_CHAR,     Volatile, false);
 630   case vmIntrinsics::_getIntVolatile:           return inline_unsafe_access(!is_store, T_INT,      Volatile, false);
 631   case vmIntrinsics::_getLongVolatile:          return inline_unsafe_access(!is_store, T_LONG,     Volatile, false);
 632   case vmIntrinsics::_getFloatVolatile:         return inline_unsafe_access(!is_store, T_FLOAT,    Volatile, false);
 633   case vmIntrinsics::_getDoubleVolatile:        return inline_unsafe_access(!is_store, T_DOUBLE,   Volatile, false);
 634 
 635   case vmIntrinsics::_putReferenceVolatile:     return inline_unsafe_access( is_store, T_OBJECT,   Volatile, false);
 636   case vmIntrinsics::_putBooleanVolatile:       return inline_unsafe_access( is_store, T_BOOLEAN,  Volatile, false);
 637   case vmIntrinsics::_putByteVolatile:          return inline_unsafe_access( is_store, T_BYTE,     Volatile, false);
 638   case vmIntrinsics::_putShortVolatile:         return inline_unsafe_access( is_store, T_SHORT,    Volatile, false);
 639   case vmIntrinsics::_putCharVolatile:          return inline_unsafe_access( is_store, T_CHAR,     Volatile, false);
 640   case vmIntrinsics::_putIntVolatile:           return inline_unsafe_access( is_store, T_INT,      Volatile, false);
 641   case vmIntrinsics::_putLongVolatile:          return inline_unsafe_access( is_store, T_LONG,     Volatile, false);
 642   case vmIntrinsics::_putFloatVolatile:         return inline_unsafe_access( is_store, T_FLOAT,    Volatile, false);
 643   case vmIntrinsics::_putDoubleVolatile:        return inline_unsafe_access( is_store, T_DOUBLE,   Volatile, false);
</pre>
<hr />
<pre>
2379   guarantee( is_store || kind != Release, &quot;Release accesses can be produced only for stores&quot;);
2380   assert(type != T_OBJECT || !unaligned, &quot;unaligned access not supported with object type&quot;);
2381 
2382   if (is_reference_type(type)) {
2383     decorators |= ON_UNKNOWN_OOP_REF;
2384   }
2385 
2386   if (unaligned) {
2387     decorators |= C2_UNALIGNED;
2388   }
2389 
2390 #ifndef PRODUCT
2391   {
2392     ResourceMark rm;
2393     // Check the signatures.
2394     ciSignature* sig = callee()-&gt;signature();
2395 #ifdef ASSERT
2396     if (!is_store) {
2397       // Object getReference(Object base, int/long offset), etc.
2398       BasicType rtype = sig-&gt;return_type()-&gt;basic_type();
<span class="line-modified">2399       assert(rtype == type, &quot;getter must return the expected value&quot;);</span>
<span class="line-modified">2400       assert(sig-&gt;count() == 2, &quot;oop getter has 2 arguments&quot;);</span>
2401       assert(sig-&gt;type_at(0)-&gt;basic_type() == T_OBJECT, &quot;getter base is object&quot;);
2402       assert(sig-&gt;type_at(1)-&gt;basic_type() == T_LONG, &quot;getter offset is correct&quot;);
2403     } else {
2404       // void putReference(Object base, int/long offset, Object x), etc.
2405       assert(sig-&gt;return_type()-&gt;basic_type() == T_VOID, &quot;putter must not return a value&quot;);
<span class="line-modified">2406       assert(sig-&gt;count() == 3, &quot;oop putter has 3 arguments&quot;);</span>
2407       assert(sig-&gt;type_at(0)-&gt;basic_type() == T_OBJECT, &quot;putter base is object&quot;);
2408       assert(sig-&gt;type_at(1)-&gt;basic_type() == T_LONG, &quot;putter offset is correct&quot;);
2409       BasicType vtype = sig-&gt;type_at(sig-&gt;count()-1)-&gt;basic_type();
<span class="line-modified">2410       assert(vtype == type, &quot;putter must accept the expected value&quot;);</span>
2411     }
2412 #endif // ASSERT
2413  }
2414 #endif //PRODUCT
2415 
2416   C-&gt;set_has_unsafe_access(true);  // Mark eventual nmethod as &quot;unsafe&quot;.
2417 
2418   Node* receiver = argument(0);  // type: oop
2419 
2420   // Build address expression.
2421   Node* adr;
2422   Node* heap_base_oop = top();
2423   Node* offset = top();
2424   Node* val;
2425 
2426   // The base is either a Java object or a value produced by Unsafe.staticFieldBase
2427   Node* base = argument(1);  // type: oop
2428   // The offset is a value produced by Unsafe.staticFieldOffset or Unsafe.objectFieldOffset
2429   offset = argument(2);  // type: long
2430   // We currently rely on the cookies produced by Unsafe.xxxFieldOffset
2431   // to be plain byte offsets, which are also the same as those accepted
2432   // by oopDesc::field_addr.
2433   assert(Unsafe_field_offset_to_byte_offset(11) == 11,
2434          &quot;fieldOffset must be byte-scaled&quot;);































































2435   // 32-bit machines ignore the high half!
2436   offset = ConvL2X(offset);
2437   adr = make_unsafe_address(base, offset, is_store ? ACCESS_WRITE : ACCESS_READ, type, kind == Relaxed);
2438 
2439   if (_gvn.type(base)-&gt;isa_ptr() == TypePtr::NULL_PTR) {
<span class="line-modified">2440     if (type != T_OBJECT) {</span>
2441       decorators |= IN_NATIVE; // off-heap primitive access
2442     } else {
2443       return false; // off-heap oop accesses are not supported
2444     }
2445   } else {
2446     heap_base_oop = base; // on-heap or mixed access
2447   }
2448 
2449   // Can base be NULL? Otherwise, always on-heap access.
2450   bool can_access_non_heap = TypePtr::NULL_PTR-&gt;higher_equal(_gvn.type(base));
2451 
2452   if (!can_access_non_heap) {
2453     decorators |= IN_HEAP;
2454   }
2455 
<span class="line-modified">2456   val = is_store ? argument(4) : NULL;</span>
2457 
2458   const TypePtr* adr_type = _gvn.type(adr)-&gt;isa_ptr();
2459   if (adr_type == TypePtr::NULL_PTR) {
2460     return false; // off-heap access with zero address
2461   }
2462 
2463   // Try to categorize the address.
2464   Compile::AliasType* alias_type = C-&gt;alias_type(adr_type);
2465   assert(alias_type-&gt;index() != Compile::AliasIdxBot, &quot;no bare pointers here&quot;);
2466 
2467   if (alias_type-&gt;adr_type() == TypeInstPtr::KLASS ||
2468       alias_type-&gt;adr_type() == TypeAryPtr::RANGE) {
2469     return false; // not supported
2470   }
2471 
2472   bool mismatched = false;
<span class="line-modified">2473   BasicType bt = alias_type-&gt;basic_type();</span>
























2474   if (bt != T_ILLEGAL) {
2475     assert(alias_type-&gt;adr_type()-&gt;is_oopptr(), &quot;should be on-heap access&quot;);
2476     if (bt == T_BYTE &amp;&amp; adr_type-&gt;isa_aryptr()) {
2477       // Alias type doesn&#39;t differentiate between byte[] and boolean[]).
2478       // Use address type to get the element type.
2479       bt = adr_type-&gt;is_aryptr()-&gt;elem()-&gt;array_element_basic_type();
2480     }
2481     if (bt == T_ARRAY || bt == T_NARROWOOP) {
2482       // accessing an array field with getReference is not a mismatch
2483       bt = T_OBJECT;
2484     }
2485     if ((bt == T_OBJECT) != (type == T_OBJECT)) {
2486       // Don&#39;t intrinsify mismatched object accesses
2487       return false;
2488     }
2489     mismatched = (bt != type);
2490   } else if (alias_type-&gt;adr_type()-&gt;isa_oopptr()) {
2491     mismatched = true; // conservatively mark all &quot;wide&quot; on-heap accesses as mismatched
2492   }
2493 





















2494   assert(!mismatched || alias_type-&gt;adr_type()-&gt;is_oopptr(), &quot;off-heap access can&#39;t be mismatched&quot;);
2495 
2496   if (mismatched) {
2497     decorators |= C2_MISMATCHED;
2498   }
2499 
2500   // First guess at the value type.
2501   const Type *value_type = Type::get_const_basic_type(type);
2502 
2503   // Figure out the memory ordering.
2504   decorators |= mo_decorator_for_access_kind(kind);
2505 
<span class="line-modified">2506   if (!is_store &amp;&amp; type == T_OBJECT) {</span>
<span class="line-modified">2507     const TypeOopPtr* tjp = sharpen_unsafe_type(alias_type, adr_type);</span>
<span class="line-modified">2508     if (tjp != NULL) {</span>
<span class="line-modified">2509       value_type = tjp;</span>




2510     }
2511   }
2512 
<span class="line-removed">2513   receiver = null_check(receiver);</span>
<span class="line-removed">2514   if (stopped()) {</span>
<span class="line-removed">2515     return true;</span>
<span class="line-removed">2516   }</span>
2517   // Heap pointers get a null-check from the interpreter,
2518   // as a courtesy.  However, this is not guaranteed by Unsafe,
2519   // and it is not possible to fully distinguish unintended nulls
2520   // from intended ones in this API.
2521 
2522   if (!is_store) {
2523     Node* p = NULL;
2524     // Try to constant fold a load from a constant field
<span class="line-modified">2525     ciField* field = alias_type-&gt;field();</span>
2526     if (heap_base_oop != top() &amp;&amp; field != NULL &amp;&amp; field-&gt;is_constant() &amp;&amp; !mismatched) {
2527       // final or stable field
2528       p = make_constant_from_field(field, heap_base_oop);
2529     }
2530 
2531     if (p == NULL) { // Could not constant fold the load
<span class="line-modified">2532       p = access_load_at(heap_base_oop, adr, adr_type, value_type, type, decorators);</span>










2533       // Normalize the value returned by getBoolean in the following cases
2534       if (type == T_BOOLEAN &amp;&amp;
2535           (mismatched ||
2536            heap_base_oop == top() ||                  // - heap_base_oop is NULL or
2537            (can_access_non_heap &amp;&amp; field == NULL))    // - heap_base_oop is potentially NULL
2538                                                       //   and the unsafe access is made to large offset
2539                                                       //   (i.e., larger than the maximum offset necessary for any
2540                                                       //   field access)
2541             ) {
2542           IdealKit ideal = IdealKit(this);
2543 #define __ ideal.
2544           IdealVariable normalized_result(ideal);
2545           __ declarations_done();
2546           __ set(normalized_result, p);
2547           __ if_then(p, BoolTest::ne, ideal.ConI(0));
2548           __ set(normalized_result, ideal.ConI(1));
2549           ideal.end_if();
2550           final_sync(ideal);
2551           p = __ value(normalized_result);
2552 #undef __
2553       }
2554     }
2555     if (type == T_ADDRESS) {
2556       p = gvn().transform(new CastP2XNode(NULL, p));
2557       p = ConvX2UL(p);
2558     }








2559     // The load node has the control of the preceding MemBarCPUOrder.  All
2560     // following nodes will have the control of the MemBarCPUOrder inserted at
2561     // the end of this method.  So, pushing the load onto the stack at a later
2562     // point is fine.
2563     set_result(p);
2564   } else {
2565     if (bt == T_ADDRESS) {
2566       // Repackage the long as a pointer.
2567       val = ConvL2X(val);
2568       val = gvn().transform(new CastX2PNode(val));
2569     }
<span class="line-modified">2570     access_store_at(heap_base_oop, adr, adr_type, val, value_type, type, decorators);</span>
































2571   }
2572 

























2573   return true;
2574 }
2575 
2576 //----------------------------inline_unsafe_load_store----------------------------
2577 // This method serves a couple of different customers (depending on LoadStoreKind):
2578 //
2579 // LS_cmp_swap:
2580 //
2581 //   boolean compareAndSetReference(Object o, long offset, Object expected, Object x);
2582 //   boolean compareAndSetInt(   Object o, long offset, int    expected, int    x);
2583 //   boolean compareAndSetLong(  Object o, long offset, long   expected, long   x);
2584 //
2585 // LS_cmp_swap_weak:
2586 //
2587 //   boolean weakCompareAndSetReference(       Object o, long offset, Object expected, Object x);
2588 //   boolean weakCompareAndSetReferencePlain(  Object o, long offset, Object expected, Object x);
2589 //   boolean weakCompareAndSetReferenceAcquire(Object o, long offset, Object expected, Object x);
2590 //   boolean weakCompareAndSetReferenceRelease(Object o, long offset, Object expected, Object x);
2591 //
2592 //   boolean weakCompareAndSetInt(          Object o, long offset, int    expected, int    x);
</pre>
<hr />
<pre>
3015   set_control(jobj_is_not_null);
3016   Node* res = access_load(jobj, TypeInstPtr::NOTNULL, T_OBJECT,
3017                           IN_NATIVE | C2_CONTROL_DEPENDENT_LOAD);
3018   result_rgn-&gt;init_req(_normal_path, control());
3019   result_val-&gt;init_req(_normal_path, res);
3020 
3021   set_result(result_rgn, result_val);
3022 
3023   return true;
3024 }
3025 
3026 #endif // JFR_HAVE_INTRINSICS
3027 
3028 //------------------------inline_native_currentThread------------------
3029 bool LibraryCallKit::inline_native_currentThread() {
3030   Node* junk = NULL;
3031   set_result(generate_current_thread(junk));
3032   return true;
3033 }
3034 
<span class="line-removed">3035 //---------------------------load_mirror_from_klass----------------------------</span>
<span class="line-removed">3036 // Given a klass oop, load its java mirror (a java.lang.Class oop).</span>
<span class="line-removed">3037 Node* LibraryCallKit::load_mirror_from_klass(Node* klass) {</span>
<span class="line-removed">3038   Node* p = basic_plus_adr(klass, in_bytes(Klass::java_mirror_offset()));</span>
<span class="line-removed">3039   Node* load = make_load(NULL, p, TypeRawPtr::NOTNULL, T_ADDRESS, MemNode::unordered);</span>
<span class="line-removed">3040   // mirror = ((OopHandle)mirror)-&gt;resolve();</span>
<span class="line-removed">3041   return access_load(load, TypeInstPtr::MIRROR, T_OBJECT, IN_NATIVE);</span>
<span class="line-removed">3042 }</span>
<span class="line-removed">3043 </span>
3044 //-----------------------load_klass_from_mirror_common-------------------------
3045 // Given a java mirror (a java.lang.Class oop), load its corresponding klass oop.
3046 // Test the klass oop for null (signifying a primitive Class like Integer.TYPE),
3047 // and branch to the given path on the region.
3048 // If never_see_null, take an uncommon trap on null, so we can optimistically
3049 // compile for the non-null case.
3050 // If the region is NULL, force never_see_null = true.
3051 Node* LibraryCallKit::load_klass_from_mirror_common(Node* mirror,
3052                                                     bool never_see_null,
3053                                                     RegionNode* region,
3054                                                     int null_path,
3055                                                     int offset) {
3056   if (region == NULL)  never_see_null = true;
3057   Node* p = basic_plus_adr(mirror, offset);
3058   const TypeKlassPtr*  kls_type = TypeKlassPtr::OBJECT_OR_NULL;
3059   Node* kls = _gvn.transform(LoadKlassNode::make(_gvn, NULL, immutable_memory(), p, TypeRawPtr::BOTTOM, kls_type));
3060   Node* null_ctl = top();
3061   kls = null_check_oop(kls, &amp;null_ctl, never_see_null);
3062   if (region != NULL) {
3063     // Set region-&gt;in(null_path) if the mirror is a primitive (e.g, int.class).
</pre>
<hr />
<pre>
3066     assert(null_ctl == top(), &quot;no loose ends&quot;);
3067   }
3068   return kls;
3069 }
3070 
3071 //--------------------(inline_native_Class_query helpers)---------------------
3072 // Use this for JVM_ACC_INTERFACE, JVM_ACC_IS_CLONEABLE_FAST, JVM_ACC_HAS_FINALIZER.
3073 // Fall through if (mods &amp; mask) == bits, take the guard otherwise.
3074 Node* LibraryCallKit::generate_access_flags_guard(Node* kls, int modifier_mask, int modifier_bits, RegionNode* region) {
3075   // Branch around if the given klass has the given modifier bit set.
3076   // Like generate_guard, adds a new path onto the region.
3077   Node* modp = basic_plus_adr(kls, in_bytes(Klass::access_flags_offset()));
3078   Node* mods = make_load(NULL, modp, TypeInt::INT, T_INT, MemNode::unordered);
3079   Node* mask = intcon(modifier_mask);
3080   Node* bits = intcon(modifier_bits);
3081   Node* mbit = _gvn.transform(new AndINode(mods, mask));
3082   Node* cmp  = _gvn.transform(new CmpINode(mbit, bits));
3083   Node* bol  = _gvn.transform(new BoolNode(cmp, BoolTest::ne));
3084   return generate_fair_guard(bol, region);
3085 }

3086 Node* LibraryCallKit::generate_interface_guard(Node* kls, RegionNode* region) {
3087   return generate_access_flags_guard(kls, JVM_ACC_INTERFACE, 0, region);
3088 }
3089 Node* LibraryCallKit::generate_hidden_class_guard(Node* kls, RegionNode* region) {
3090   return generate_access_flags_guard(kls, JVM_ACC_IS_HIDDEN_CLASS, 0, region);
3091 }
3092 




3093 //-------------------------inline_native_Class_query-------------------
3094 bool LibraryCallKit::inline_native_Class_query(vmIntrinsics::ID id) {
3095   const Type* return_type = TypeInt::BOOL;
3096   Node* prim_return_value = top();  // what happens if it&#39;s a primitive class?
3097   bool never_see_null = !too_many_traps(Deoptimization::Reason_null_check);
3098   bool expect_prim = false;     // most of these guys expect to work on refs
3099 
3100   enum { _normal_path = 1, _prim_path = 2, PATH_LIMIT };
3101 
3102   Node* mirror = argument(0);
3103   Node* obj    = top();
3104 
3105   switch (id) {
3106   case vmIntrinsics::_isInstance:
3107     // nothing is an instance of a primitive type
3108     prim_return_value = intcon(0);
3109     obj = argument(1);
3110     break;
3111   case vmIntrinsics::_getModifiers:
3112     prim_return_value = intcon(JVM_ACC_ABSTRACT | JVM_ACC_FINAL | JVM_ACC_PUBLIC);
</pre>
<hr />
<pre>
3270   // Fall-through is the normal case of a query to a real class.
3271   phi-&gt;init_req(1, query_value);
3272   region-&gt;init_req(1, control());
3273 
3274   C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
3275   set_result(region, phi);
3276   return true;
3277 }
3278 
3279 //-------------------------inline_Class_cast-------------------
3280 bool LibraryCallKit::inline_Class_cast() {
3281   Node* mirror = argument(0); // Class
3282   Node* obj    = argument(1);
3283   const TypeInstPtr* mirror_con = _gvn.type(mirror)-&gt;isa_instptr();
3284   if (mirror_con == NULL) {
3285     return false;  // dead path (mirror-&gt;is_top()).
3286   }
3287   if (obj == NULL || obj-&gt;is_top()) {
3288     return false;  // dead path
3289   }
<span class="line-modified">3290   const TypeOopPtr* tp = _gvn.type(obj)-&gt;isa_oopptr();</span>






3291 
3292   // First, see if Class.cast() can be folded statically.
3293   // java_mirror_type() returns non-null for compile-time Class constants.
3294   ciType* tm = mirror_con-&gt;java_mirror_type();
<span class="line-modified">3295   if (tm != NULL &amp;&amp; tm-&gt;is_klass() &amp;&amp;</span>
<span class="line-modified">3296       tp != NULL &amp;&amp; tp-&gt;klass() != NULL) {</span>
<span class="line-removed">3297     if (!tp-&gt;klass()-&gt;is_loaded()) {</span>
3298       // Don&#39;t use intrinsic when class is not loaded.
3299       return false;
3300     } else {
<span class="line-modified">3301       int static_res = C-&gt;static_subtype_check(tm-&gt;as_klass(), tp-&gt;klass());</span>







3302       if (static_res == Compile::SSC_always_true) {
3303         // isInstance() is true - fold the code.
3304         set_result(obj);
3305         return true;
3306       } else if (static_res == Compile::SSC_always_false) {
3307         // Don&#39;t use intrinsic, have to throw ClassCastException.
3308         // If the reference is null, the non-intrinsic bytecode will
3309         // be optimized appropriately.
3310         return false;
3311       }
3312     }
3313   }
3314 
3315   // Bailout intrinsic and do normal inlining if exception path is frequent.
3316   if (too_many_traps(Deoptimization::Reason_intrinsic)) {
3317     return false;
3318   }
3319 
3320   // Generate dynamic checks.
3321   // Class.cast() is java implementation of _checkcast bytecode.
3322   // Do checkcast (Parse::do_checkcast()) optimizations here.
3323 
3324   mirror = null_check(mirror);
3325   // If mirror is dead, only null-path is taken.
3326   if (stopped()) {
3327     return true;
3328   }
3329 
3330   // Not-subtype or the mirror&#39;s klass ptr is NULL (in case it is a primitive).
<span class="line-modified">3331   enum { _bad_type_path = 1, _prim_path = 2, PATH_LIMIT };</span>
3332   RegionNode* region = new RegionNode(PATH_LIMIT);
3333   record_for_igvn(region);
3334 
3335   // Now load the mirror&#39;s klass metaobject, and null-check it.
3336   // If kls is null, we have a primitive mirror and
3337   // nothing is an instance of a primitive type.
3338   Node* kls = load_klass_from_mirror(mirror, false, region, _prim_path);
3339 
3340   Node* res = top();
3341   if (!stopped()) {



















3342     Node* bad_type_ctrl = top();
3343     // Do checkcast optimizations.
3344     res = gen_checkcast(obj, kls, &amp;bad_type_ctrl);
3345     region-&gt;init_req(_bad_type_path, bad_type_ctrl);
3346   }
3347   if (region-&gt;in(_prim_path) != top() ||
<span class="line-modified">3348       region-&gt;in(_bad_type_path) != top()) {</span>

3349     // Let Interpreter throw ClassCastException.
3350     PreserveJVMState pjvms(this);
3351     set_control(_gvn.transform(region));
3352     uncommon_trap(Deoptimization::Reason_intrinsic,
3353                   Deoptimization::Action_maybe_recompile);
3354   }
3355   if (!stopped()) {
3356     set_result(res);
3357   }
3358   return true;
3359 }
3360 
3361 
3362 //--------------------------inline_native_subtype_check------------------------
3363 // This intrinsic takes the JNI calls out of the heart of
3364 // UnsafeFieldAccessorImpl.set, which improves Field.set, readObject, etc.
3365 bool LibraryCallKit::inline_native_subtype_check() {
3366   // Pull both arguments off the stack.
3367   Node* args[2];                // two java.lang.Class mirrors: superc, subc
3368   args[0] = argument(0);
3369   args[1] = argument(1);
3370   Node* klasses[2];             // corresponding Klasses: superk, subk
3371   klasses[0] = klasses[1] = top();
3372 
3373   enum {
3374     // A full decision tree on {superc is prim, subc is prim}:
3375     _prim_0_path = 1,           // {P,N} =&gt; false
3376                                 // {P,P} &amp; superc!=subc =&gt; false
3377     _prim_same_path,            // {P,P} &amp; superc==subc =&gt; true
3378     _prim_1_path,               // {N,P} =&gt; false
3379     _ref_subtype_path,          // {N,N} &amp; subtype check wins =&gt; true
3380     _both_ref_path,             // {N,N} &amp; subtype check loses =&gt; false
3381     PATH_LIMIT
3382   };
3383 
3384   RegionNode* region = new RegionNode(PATH_LIMIT);

3385   Node*       phi    = new PhiNode(region, TypeInt::BOOL);
3386   record_for_igvn(region);

3387 
3388   const TypePtr* adr_type = TypeRawPtr::BOTTOM;   // memory type of loads
3389   const TypeKlassPtr* kls_type = TypeKlassPtr::OBJECT_OR_NULL;
3390   int class_klass_offset = java_lang_Class::klass_offset_in_bytes();
3391 
3392   // First null-check both mirrors and load each mirror&#39;s klass metaobject.
3393   int which_arg;
3394   for (which_arg = 0; which_arg &lt;= 1; which_arg++) {
3395     Node* arg = args[which_arg];
3396     arg = null_check(arg);
3397     if (stopped())  break;
3398     args[which_arg] = arg;
3399 
3400     Node* p = basic_plus_adr(arg, class_klass_offset);
3401     Node* kls = LoadKlassNode::make(_gvn, NULL, immutable_memory(), p, adr_type, kls_type);
3402     klasses[which_arg] = _gvn.transform(kls);
3403   }
3404 
3405   // Having loaded both klasses, test each for null.
3406   bool never_see_null = !too_many_traps(Deoptimization::Reason_null_check);
3407   for (which_arg = 0; which_arg &lt;= 1; which_arg++) {
3408     Node* kls = klasses[which_arg];
3409     Node* null_ctl = top();
3410     kls = null_check_oop(kls, &amp;null_ctl, never_see_null);
<span class="line-modified">3411     int prim_path = (which_arg == 0 ? _prim_0_path : _prim_1_path);</span>
<span class="line-modified">3412     region-&gt;init_req(prim_path, null_ctl);</span>



3413     if (stopped())  break;
3414     klasses[which_arg] = kls;
3415   }
3416 
3417   if (!stopped()) {
3418     // now we have two reference types, in klasses[0..1]
3419     Node* subk   = klasses[1];  // the argument to isAssignableFrom
3420     Node* superk = klasses[0];  // the receiver
3421     region-&gt;set_req(_both_ref_path, gen_subtype_check(subk, superk));
3422     // now we have a successful reference subtype check
3423     region-&gt;set_req(_ref_subtype_path, control());
3424   }
3425 
3426   // If both operands are primitive (both klasses null), then
3427   // we must return true when they are identical primitives.
3428   // It is convenient to test this after the first null klass check.
<span class="line-modified">3429   set_control(region-&gt;in(_prim_0_path)); // go back to first null check</span>

3430   if (!stopped()) {
3431     // Since superc is primitive, make a guard for the superc==subc case.
3432     Node* cmp_eq = _gvn.transform(new CmpPNode(args[0], args[1]));
3433     Node* bol_eq = _gvn.transform(new BoolNode(cmp_eq, BoolTest::eq));
<span class="line-modified">3434     generate_guard(bol_eq, region, PROB_FAIR);</span>
3435     if (region-&gt;req() == PATH_LIMIT+1) {
3436       // A guard was added.  If the added guard is taken, superc==subc.
3437       region-&gt;swap_edges(PATH_LIMIT, _prim_same_path);
3438       region-&gt;del_req(PATH_LIMIT);
3439     }
3440     region-&gt;set_req(_prim_0_path, control()); // Not equal after all.
3441   }
3442 
3443   // these are the only paths that produce &#39;true&#39;:
3444   phi-&gt;set_req(_prim_same_path,   intcon(1));
3445   phi-&gt;set_req(_ref_subtype_path, intcon(1));
3446 
3447   // pull together the cases:
3448   assert(region-&gt;req() == PATH_LIMIT, &quot;sane region&quot;);
3449   for (uint i = 1; i &lt; region-&gt;req(); i++) {
3450     Node* ctl = region-&gt;in(i);
3451     if (ctl == NULL || ctl == top()) {
3452       region-&gt;set_req(i, top());
3453       phi   -&gt;set_req(i, top());
3454     } else if (phi-&gt;in(i) == NULL) {
3455       phi-&gt;set_req(i, intcon(0)); // all other paths produce &#39;false&#39;
3456     }
3457   }
3458 
3459   set_control(_gvn.transform(region));
3460   set_result(_gvn.transform(phi));
3461   return true;
3462 }
3463 
3464 //---------------------generate_array_guard_common------------------------
<span class="line-modified">3465 Node* LibraryCallKit::generate_array_guard_common(Node* kls, RegionNode* region,</span>
<span class="line-removed">3466                                                   bool obj_array, bool not_array) {</span>
3467 
3468   if (stopped()) {
3469     return NULL;
3470   }
3471 
<span class="line-removed">3472   // If obj_array/non_array==false/false:</span>
<span class="line-removed">3473   // Branch around if the given klass is in fact an array (either obj or prim).</span>
<span class="line-removed">3474   // If obj_array/non_array==false/true:</span>
<span class="line-removed">3475   // Branch around if the given klass is not an array klass of any kind.</span>
<span class="line-removed">3476   // If obj_array/non_array==true/true:</span>
<span class="line-removed">3477   // Branch around if the kls is not an oop array (kls is int[], String, etc.)</span>
<span class="line-removed">3478   // If obj_array/non_array==true/false:</span>
<span class="line-removed">3479   // Branch around if the kls is an oop array (Object[] or subtype)</span>
<span class="line-removed">3480   //</span>
3481   // Like generate_guard, adds a new path onto the region.
3482   jint  layout_con = 0;
3483   Node* layout_val = get_layout_helper(kls, layout_con);
3484   if (layout_val == NULL) {
<span class="line-modified">3485     bool query = (obj_array</span>
<span class="line-modified">3486                   ? Klass::layout_helper_is_objArray(layout_con)</span>
<span class="line-modified">3487                   : Klass::layout_helper_is_array(layout_con));</span>
<span class="line-modified">3488     if (query == not_array) {</span>








3489       return NULL;                       // never a branch
3490     } else {                             // always a branch
3491       Node* always_branch = control();
3492       if (region != NULL)
3493         region-&gt;add_req(always_branch);
3494       set_control(top());
3495       return always_branch;
3496     }
3497   }



























3498   // Now test the correct condition.
<span class="line-modified">3499   jint  nval = (obj_array</span>
<span class="line-removed">3500                 ? (jint)(Klass::_lh_array_tag_type_value</span>
<span class="line-removed">3501                    &lt;&lt;    Klass::_lh_array_tag_shift)</span>
<span class="line-removed">3502                 : Klass::_lh_neutral_value);</span>
3503   Node* cmp = _gvn.transform(new CmpINode(layout_val, intcon(nval)));
<span class="line-removed">3504   BoolTest::mask btest = BoolTest::lt;  // correct for testing is_[obj]array</span>
<span class="line-removed">3505   // invert the test if we are looking for a non-array</span>
<span class="line-removed">3506   if (not_array)  btest = BoolTest(btest).negate();</span>
3507   Node* bol = _gvn.transform(new BoolNode(cmp, btest));
3508   return generate_fair_guard(bol, region);
3509 }
3510 
3511 
3512 //-----------------------inline_native_newArray--------------------------
<span class="line-modified">3513 // private static native Object java.lang.reflect.newArray(Class&lt;?&gt; componentType, int length);</span>
3514 // private        native Object Unsafe.allocateUninitializedArray0(Class&lt;?&gt; cls, int size);
3515 bool LibraryCallKit::inline_unsafe_newArray(bool uninitialized) {
3516   Node* mirror;
3517   Node* count_val;
3518   if (uninitialized) {
3519     mirror    = argument(1);
3520     count_val = argument(2);
3521   } else {
3522     mirror    = argument(0);
3523     count_val = argument(1);
3524   }
3525 
3526   mirror = null_check(mirror);
3527   // If mirror or obj is dead, only null-path is taken.
3528   if (stopped())  return true;
3529 
3530   enum { _normal_path = 1, _slow_path = 2, PATH_LIMIT };
3531   RegionNode* result_reg = new RegionNode(PATH_LIMIT);
3532   PhiNode*    result_val = new PhiNode(result_reg, TypeInstPtr::NOTNULL);
3533   PhiNode*    result_io  = new PhiNode(result_reg, Type::ABIO);
</pre>
<hr />
<pre>
3609   // It could be a dynamic mix of int[], boolean[], Object[], etc.
3610   Node* result = load_array_length(array);
3611 
3612   C-&gt;set_has_split_ifs(true);  // Has chance for split-if optimization
3613   set_result(result);
3614   return true;
3615 }
3616 
3617 //------------------------inline_array_copyOf----------------------------
3618 // public static &lt;T,U&gt; T[] java.util.Arrays.copyOf(     U[] original, int newLength,         Class&lt;? extends T[]&gt; newType);
3619 // public static &lt;T,U&gt; T[] java.util.Arrays.copyOfRange(U[] original, int from,      int to, Class&lt;? extends T[]&gt; newType);
3620 bool LibraryCallKit::inline_array_copyOf(bool is_copyOfRange) {
3621   if (too_many_traps(Deoptimization::Reason_intrinsic))  return false;
3622 
3623   // Get the arguments.
3624   Node* original          = argument(0);
3625   Node* start             = is_copyOfRange? argument(1): intcon(0);
3626   Node* end               = is_copyOfRange? argument(2): argument(1);
3627   Node* array_type_mirror = is_copyOfRange? argument(3): argument(2);
3628 













3629   Node* newcopy = NULL;
3630 
3631   // Set the original stack and the reexecute bit for the interpreter to reexecute
3632   // the bytecode that invokes Arrays.copyOf if deoptimization happens.
3633   { PreserveReexecuteState preexecs(this);
3634     jvms()-&gt;set_should_reexecute(true);
3635 
3636     array_type_mirror = null_check(array_type_mirror);
3637     original          = null_check(original);
3638 
3639     // Check if a null path was taken unconditionally.
3640     if (stopped())  return true;
3641 
3642     Node* orig_length = load_array_length(original);
3643 
3644     Node* klass_node = load_klass_from_mirror(array_type_mirror, false, NULL, 0);
3645     klass_node = null_check(klass_node);
3646 
3647     RegionNode* bailout = new RegionNode(1);
3648     record_for_igvn(bailout);
3649 
3650     // Despite the generic type of Arrays.copyOf, the mirror might be int, int[], etc.
3651     // Bail out if that is so.
<span class="line-modified">3652     Node* not_objArray = generate_non_objArray_guard(klass_node, bailout);</span>




3653     if (not_objArray != NULL) {
3654       // Improve the klass node&#39;s type from the new optimistic assumption:
3655       ciKlass* ak = ciArrayKlass::make(env()-&gt;Object_klass());
<span class="line-modified">3656       const Type* akls = TypeKlassPtr::make(TypePtr::NotNull, ak, 0/*offset*/);</span>
3657       Node* cast = new CastPPNode(klass_node, akls);
3658       cast-&gt;init_req(0, control());
3659       klass_node = _gvn.transform(cast);
3660     }
3661 










































3662     // Bail out if either start or end is negative.
3663     generate_negative_guard(start, bailout, &amp;start);
3664     generate_negative_guard(end,   bailout, &amp;end);
3665 
3666     Node* length = end;
3667     if (_gvn.type(start) != TypeInt::ZERO) {
3668       length = _gvn.transform(new SubINode(end, start));
3669     }
3670 
3671     // Bail out if length is negative.
3672     // Without this the new_array would throw
3673     // NegativeArraySizeException but IllegalArgumentException is what
3674     // should be thrown
3675     generate_negative_guard(length, bailout, &amp;length);
3676 
3677     if (bailout-&gt;req() &gt; 1) {
3678       PreserveJVMState pjvms(this);
3679       set_control(_gvn.transform(bailout));
3680       uncommon_trap(Deoptimization::Reason_intrinsic,
3681                     Deoptimization::Action_maybe_recompile);
3682     }
3683 
3684     if (!stopped()) {
3685       // How many elements will we copy from the original?
3686       // The answer is MinI(orig_length - start, length).
3687       Node* orig_tail = _gvn.transform(new SubINode(orig_length, start));
3688       Node* moved = generate_min_max(vmIntrinsics::_min, orig_tail, length);
3689 
3690       // Generate a direct call to the right arraycopy function(s).
3691       // We know the copy is disjoint but we might not know if the
3692       // oop stores need checking.
3693       // Extreme case:  Arrays.copyOf((Integer[])x, 10, String[].class).
3694       // This will fail a store-check if x contains any non-nulls.
3695 
<span class="line-removed">3696       // ArrayCopyNode:Ideal may transform the ArrayCopyNode to</span>
<span class="line-removed">3697       // loads/stores but it is legal only if we&#39;re sure the</span>
<span class="line-removed">3698       // Arrays.copyOf would succeed. So we need all input arguments</span>
<span class="line-removed">3699       // to the copyOf to be validated, including that the copy to the</span>
<span class="line-removed">3700       // new array won&#39;t trigger an ArrayStoreException. That subtype</span>
<span class="line-removed">3701       // check can be optimized if we know something on the type of</span>
<span class="line-removed">3702       // the input array from type speculation.</span>
<span class="line-removed">3703       if (_gvn.type(klass_node)-&gt;singleton()) {</span>
<span class="line-removed">3704         ciKlass* subk   = _gvn.type(load_object_klass(original))-&gt;is_klassptr()-&gt;klass();</span>
<span class="line-removed">3705         ciKlass* superk = _gvn.type(klass_node)-&gt;is_klassptr()-&gt;klass();</span>
<span class="line-removed">3706 </span>
<span class="line-removed">3707         int test = C-&gt;static_subtype_check(superk, subk);</span>
<span class="line-removed">3708         if (test != Compile::SSC_always_true &amp;&amp; test != Compile::SSC_always_false) {</span>
<span class="line-removed">3709           const TypeOopPtr* t_original = _gvn.type(original)-&gt;is_oopptr();</span>
<span class="line-removed">3710           if (t_original-&gt;speculative_type() != NULL) {</span>
<span class="line-removed">3711             original = maybe_cast_profiled_obj(original, t_original-&gt;speculative_type(), true);</span>
<span class="line-removed">3712           }</span>
<span class="line-removed">3713         }</span>
<span class="line-removed">3714       }</span>
<span class="line-removed">3715 </span>
3716       bool validated = false;
3717       // Reason_class_check rather than Reason_intrinsic because we
3718       // want to intrinsify even if this traps.
3719       if (!too_many_traps(Deoptimization::Reason_class_check)) {
3720         Node* not_subtype_ctrl = gen_subtype_check(original, klass_node);
3721 
3722         if (not_subtype_ctrl != top()) {
3723           PreserveJVMState pjvms(this);
3724           set_control(not_subtype_ctrl);
3725           uncommon_trap(Deoptimization::Reason_class_check,
3726                         Deoptimization::Action_make_not_entrant);
3727           assert(stopped(), &quot;Should be stopped&quot;);
3728         }
3729         validated = true;
3730       }
3731 
3732       if (!stopped()) {
3733         newcopy = new_array(klass_node, length, 0);  // no arguments to push
3734 
3735         ArrayCopyNode* ac = ArrayCopyNode::make(this, true, original, start, newcopy, intcon(0), moved, true, false,
<span class="line-modified">3736                                                 load_object_klass(original), klass_node);</span>
3737         if (!is_copyOfRange) {
3738           ac-&gt;set_copyof(validated);
3739         } else {
3740           ac-&gt;set_copyofrange(validated);
3741         }
3742         Node* n = _gvn.transform(ac);
3743         if (n == ac) {
3744           ac-&gt;connect_outputs(this);
3745         } else {
3746           assert(validated, &quot;shouldn&#39;t transform if all arguments not validated&quot;);
3747           set_all_memory(n);
3748         }
3749       }
3750     }
3751   } // original reexecute is set back here
3752 
3753   C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
3754   if (!stopped()) {
3755     set_result(newcopy);
3756   }
</pre>
<hr />
<pre>
3840   set_edges_for_java_call(slow_call);
3841   return slow_call;
3842 }
3843 
3844 
3845 /**
3846  * Build special case code for calls to hashCode on an object. This call may
3847  * be virtual (invokevirtual) or bound (invokespecial). For each case we generate
3848  * slightly different code.
3849  */
3850 bool LibraryCallKit::inline_native_hashcode(bool is_virtual, bool is_static) {
3851   assert(is_static == callee()-&gt;is_static(), &quot;correct intrinsic selection&quot;);
3852   assert(!(is_virtual &amp;&amp; is_static), &quot;either virtual, special, or static&quot;);
3853 
3854   enum { _slow_path = 1, _fast_path, _null_path, PATH_LIMIT };
3855 
3856   RegionNode* result_reg = new RegionNode(PATH_LIMIT);
3857   PhiNode*    result_val = new PhiNode(result_reg, TypeInt::INT);
3858   PhiNode*    result_io  = new PhiNode(result_reg, Type::ABIO);
3859   PhiNode*    result_mem = new PhiNode(result_reg, Type::MEMORY, TypePtr::BOTTOM);
<span class="line-modified">3860   Node* obj = NULL;</span>





3861   if (!is_static) {
3862     // Check for hashing null object
3863     obj = null_check_receiver();
3864     if (stopped())  return true;        // unconditionally null
3865     result_reg-&gt;init_req(_null_path, top());
3866     result_val-&gt;init_req(_null_path, top());
3867   } else {
3868     // Do a null check, and return zero if null.
3869     // System.identityHashCode(null) == 0
<span class="line-removed">3870     obj = argument(0);</span>
3871     Node* null_ctl = top();
3872     obj = null_check_oop(obj, &amp;null_ctl);
3873     result_reg-&gt;init_req(_null_path, null_ctl);
3874     result_val-&gt;init_req(_null_path, _gvn.intcon(0));
3875   }
3876 
3877   // Unconditionally null?  Then return right away.
3878   if (stopped()) {
3879     set_control( result_reg-&gt;in(_null_path));
3880     if (!stopped())
3881       set_result(result_val-&gt;in(_null_path));
3882     return true;
3883   }
3884 
3885   // We only go to the fast case code if we pass a number of guards.  The
3886   // paths which do not pass are accumulated in the slow_region.
3887   RegionNode* slow_region = new RegionNode(1);
3888   record_for_igvn(slow_region);
3889 
3890   // If this is a virtual call, we generate a funny guard.  We pull out
3891   // the vtable entry corresponding to hashCode() from the target object.
3892   // If the target method which we are calling happens to be the native
3893   // Object hashCode() method, we pass the guard.  We do not need this
3894   // guard for non-virtual calls -- the caller is known to be the native
3895   // Object hashCode().
3896   if (is_virtual) {
3897     // After null check, get the object&#39;s klass.
3898     Node* obj_klass = load_object_klass(obj);
3899     generate_virtual_guard(obj_klass, slow_region);
3900   }
3901 
3902   // Get the header out of the object, use LoadMarkNode when available
3903   Node* header_addr = basic_plus_adr(obj, oopDesc::mark_offset_in_bytes());
3904   // The control of the load must be NULL. Otherwise, the load can move before
3905   // the null check after castPP removal.
3906   Node* no_ctrl = NULL;
3907   Node* header = make_load(no_ctrl, header_addr, TypeX_X, TypeX_X-&gt;basic_type(), MemNode::unordered);
3908 
3909   // Test the header to see if it is unlocked.

3910   Node *lock_mask      = _gvn.MakeConX(markWord::biased_lock_mask_in_place);
3911   Node *lmasked_header = _gvn.transform(new AndXNode(header, lock_mask));
3912   Node *unlocked_val   = _gvn.MakeConX(markWord::unlocked_value);
3913   Node *chk_unlocked   = _gvn.transform(new CmpXNode( lmasked_header, unlocked_val));
3914   Node *test_unlocked  = _gvn.transform(new BoolNode( chk_unlocked, BoolTest::ne));
3915 
3916   generate_slow_guard(test_unlocked, slow_region);
3917 
3918   // Get the hash value and check to see that it has been properly assigned.
3919   // We depend on hash_mask being at most 32 bits and avoid the use of
3920   // hash_mask_in_place because it could be larger than 32 bits in a 64-bit
3921   // vm: see markWord.hpp.
3922   Node *hash_mask      = _gvn.intcon(markWord::hash_mask);
3923   Node *hash_shift     = _gvn.intcon(markWord::hash_shift);
3924   Node *hshifted_header= _gvn.transform(new URShiftXNode(header, hash_shift));
3925   // This hack lets the hash bits live anywhere in the mark object now, as long
3926   // as the shift drops the relevant bits into the low 32 bits.  Note that
3927   // Java spec says that HashCode is an int so there&#39;s no point in capturing
3928   // an &#39;X&#39;-sized hashcode (32 in 32-bit build or 64 in 64-bit build).
3929   hshifted_header      = ConvX2I(hshifted_header);
</pre>
<hr />
<pre>
3956     // this-&gt;control() comes from set_results_for_java_call
3957     result_reg-&gt;init_req(_slow_path, control());
3958     result_val-&gt;init_req(_slow_path, slow_result);
3959     result_io  -&gt;set_req(_slow_path, i_o());
3960     result_mem -&gt;set_req(_slow_path, reset_memory());
3961   }
3962 
3963   // Return the combined state.
3964   set_i_o(        _gvn.transform(result_io)  );
3965   set_all_memory( _gvn.transform(result_mem));
3966 
3967   set_result(result_reg, result_val);
3968   return true;
3969 }
3970 
3971 //---------------------------inline_native_getClass----------------------------
3972 // public final native Class&lt;?&gt; java.lang.Object.getClass();
3973 //
3974 // Build special case code for calls to getClass on an object.
3975 bool LibraryCallKit::inline_native_getClass() {
<span class="line-modified">3976   Node* obj = null_check_receiver();</span>






3977   if (stopped())  return true;
3978   set_result(load_mirror_from_klass(load_object_klass(obj)));
3979   return true;
3980 }
3981 
3982 //-----------------inline_native_Reflection_getCallerClass---------------------
3983 // public static native Class&lt;?&gt; sun.reflect.Reflection.getCallerClass();
3984 //
3985 // In the presence of deep enough inlining, getCallerClass() becomes a no-op.
3986 //
3987 // NOTE: This code must perform the same logic as JVM_GetCallerClass
3988 // in that it must skip particular security frames and checks for
3989 // caller sensitive methods.
3990 bool LibraryCallKit::inline_native_Reflection_getCallerClass() {
3991 #ifndef PRODUCT
3992   if ((C-&gt;print_intrinsics() || C-&gt;print_inlining()) &amp;&amp; Verbose) {
3993     tty-&gt;print_cr(&quot;Attempting to inline sun.reflect.Reflection.getCallerClass&quot;);
3994   }
3995 #endif
3996 
</pre>
<hr />
<pre>
4218 // Helper function for inline_native_clone.
4219 void LibraryCallKit::copy_to_clone(Node* obj, Node* alloc_obj, Node* obj_size, bool is_array) {
4220   assert(obj_size != NULL, &quot;&quot;);
4221   Node* raw_obj = alloc_obj-&gt;in(1);
4222   assert(alloc_obj-&gt;is_CheckCastPP() &amp;&amp; raw_obj-&gt;is_Proj() &amp;&amp; raw_obj-&gt;in(0)-&gt;is_Allocate(), &quot;&quot;);
4223 
4224   AllocateNode* alloc = NULL;
4225   if (ReduceBulkZeroing) {
4226     // We will be completely responsible for initializing this object -
4227     // mark Initialize node as complete.
4228     alloc = AllocateNode::Ideal_allocation(alloc_obj, &amp;_gvn);
4229     // The object was just allocated - there should be no any stores!
4230     guarantee(alloc != NULL &amp;&amp; alloc-&gt;maybe_set_complete(&amp;_gvn), &quot;&quot;);
4231     // Mark as complete_with_arraycopy so that on AllocateNode
4232     // expansion, we know this AllocateNode is initialized by an array
4233     // copy and a StoreStore barrier exists after the array copy.
4234     alloc-&gt;initialization()-&gt;set_complete_with_arraycopy();
4235   }
4236 
4237   Node* size = _gvn.transform(obj_size);
<span class="line-modified">4238   access_clone(obj, alloc_obj, size, is_array);</span>







4239 
4240   // Do not let reads from the cloned object float above the arraycopy.
4241   if (alloc != NULL) {
4242     // Do not let stores that initialize this object be reordered with
4243     // a subsequent store that would make this object accessible by
4244     // other threads.
4245     // Record what AllocateNode this StoreStore protects so that
4246     // escape analysis can go from the MemBarStoreStoreNode to the
4247     // AllocateNode and eliminate the MemBarStoreStoreNode if possible
4248     // based on the escape status of the AllocateNode.
4249     insert_mem_bar(Op_MemBarStoreStore, alloc-&gt;proj_out_or_null(AllocateNode::RawAddress));
4250   } else {
4251     insert_mem_bar(Op_MemBarCPUOrder);
4252   }
4253 }
4254 
4255 //------------------------inline_native_clone----------------------------
4256 // protected native Object java.lang.Object.clone();
4257 //
4258 // Here are the simple edge cases:
</pre>
<hr />
<pre>
4261 //  not cloneable or finalizer =&gt; slow path to out-of-line Object.clone
4262 //
4263 // The general case has two steps, allocation and copying.
4264 // Allocation has two cases, and uses GraphKit::new_instance or new_array.
4265 //
4266 // Copying also has two cases, oop arrays and everything else.
4267 // Oop arrays use arrayof_oop_arraycopy (same as System.arraycopy).
4268 // Everything else uses the tight inline loop supplied by CopyArrayNode.
4269 //
4270 // These steps fold up nicely if and when the cloned object&#39;s klass
4271 // can be sharply typed as an object array, a type array, or an instance.
4272 //
4273 bool LibraryCallKit::inline_native_clone(bool is_virtual) {
4274   PhiNode* result_val;
4275 
4276   // Set the reexecute bit for the interpreter to reexecute
4277   // the bytecode that invokes Object.clone if deoptimization happens.
4278   { PreserveReexecuteState preexecs(this);
4279     jvms()-&gt;set_should_reexecute(true);
4280 
<span class="line-modified">4281     Node* obj = null_check_receiver();</span>





4282     if (stopped())  return true;
4283 
4284     const TypeOopPtr* obj_type = _gvn.type(obj)-&gt;is_oopptr();
4285 
4286     // If we are going to clone an instance, we need its exact type to
4287     // know the number and types of fields to convert the clone to
4288     // loads/stores. Maybe a speculative type can help us.
4289     if (!obj_type-&gt;klass_is_exact() &amp;&amp;
4290         obj_type-&gt;speculative_type() != NULL &amp;&amp;
<span class="line-modified">4291         obj_type-&gt;speculative_type()-&gt;is_instance_klass()) {</span>

4292       ciInstanceKlass* spec_ik = obj_type-&gt;speculative_type()-&gt;as_instance_klass();
4293       if (spec_ik-&gt;nof_nonstatic_fields() &lt;= ArrayCopyLoadStoreMaxElem &amp;&amp;
4294           !spec_ik-&gt;has_injected_fields()) {
4295         ciKlass* k = obj_type-&gt;klass();
4296         if (!k-&gt;is_instance_klass() ||
4297             k-&gt;as_instance_klass()-&gt;is_interface() ||
4298             k-&gt;as_instance_klass()-&gt;has_subklass()) {
4299           obj = maybe_cast_profiled_obj(obj, obj_type-&gt;speculative_type(), false);
4300         }
4301       }
4302     }
4303 
4304     // Conservatively insert a memory barrier on all memory slices.
4305     // Do not let writes into the original float below the clone.
4306     insert_mem_bar(Op_MemBarCPUOrder);
4307 
4308     // paths into result_reg:
4309     enum {
4310       _slow_path = 1,     // out-of-line call to clone method (virtual or not)
4311       _objArray_path,     // plain array allocation, plus arrayof_oop_arraycopy
4312       _array_path,        // plain array allocation, plus arrayof_long_arraycopy
4313       _instance_path,     // plain instance allocation, plus arrayof_long_arraycopy
4314       PATH_LIMIT
4315     };
4316     RegionNode* result_reg = new RegionNode(PATH_LIMIT);
4317     result_val             = new PhiNode(result_reg, TypeInstPtr::NOTNULL);
4318     PhiNode*    result_i_o = new PhiNode(result_reg, Type::ABIO);
4319     PhiNode*    result_mem = new PhiNode(result_reg, Type::MEMORY, TypePtr::BOTTOM);
4320     record_for_igvn(result_reg);
4321 
4322     Node* obj_klass = load_object_klass(obj);





4323     Node* array_ctl = generate_array_guard(obj_klass, (RegionNode*)NULL);
4324     if (array_ctl != NULL) {
4325       // It&#39;s an array.
4326       PreserveJVMState pjvms(this);
4327       set_control(array_ctl);
<span class="line-removed">4328       Node* obj_length = load_array_length(obj);</span>
<span class="line-removed">4329       Node* obj_size  = NULL;</span>
<span class="line-removed">4330       Node* alloc_obj = new_array(obj_klass, obj_length, 0, &amp;obj_size);  // no arguments to push</span>
4331 
4332       BarrierSetC2* bs = BarrierSet::barrier_set()-&gt;barrier_set_c2();
<span class="line-modified">4333       if (bs-&gt;array_copy_requires_gc_barriers(true, T_OBJECT, true, BarrierSetC2::Parsing)) {</span>
<span class="line-modified">4334         // If it is an oop array, it requires very special treatment,</span>
<span class="line-modified">4335         // because gc barriers are required when accessing the array.</span>
<span class="line-modified">4336         Node* is_obja = generate_objArray_guard(obj_klass, (RegionNode*)NULL);</span>
<span class="line-modified">4337         if (is_obja != NULL) {</span>
<span class="line-removed">4338           PreserveJVMState pjvms2(this);</span>
<span class="line-removed">4339           set_control(is_obja);</span>
<span class="line-removed">4340           // Generate a direct call to the right arraycopy function(s).</span>
<span class="line-removed">4341           Node* alloc = tightly_coupled_allocation(alloc_obj, NULL);</span>
<span class="line-removed">4342           ArrayCopyNode* ac = ArrayCopyNode::make(this, true, obj, intcon(0), alloc_obj, intcon(0), obj_length, alloc != NULL, false);</span>
<span class="line-removed">4343           ac-&gt;set_clone_oop_array();</span>
<span class="line-removed">4344           Node* n = _gvn.transform(ac);</span>
<span class="line-removed">4345           assert(n == ac, &quot;cannot disappear&quot;);</span>
<span class="line-removed">4346           ac-&gt;connect_outputs(this);</span>
<span class="line-removed">4347 </span>
<span class="line-removed">4348           result_reg-&gt;init_req(_objArray_path, control());</span>
<span class="line-removed">4349           result_val-&gt;init_req(_objArray_path, alloc_obj);</span>
<span class="line-removed">4350           result_i_o -&gt;set_req(_objArray_path, i_o());</span>
<span class="line-removed">4351           result_mem -&gt;set_req(_objArray_path, reset_memory());</span>
<span class="line-removed">4352         }</span>
4353       }
<span class="line-removed">4354       // Otherwise, there are no barriers to worry about.</span>
<span class="line-removed">4355       // (We can dispense with card marks if we know the allocation</span>
<span class="line-removed">4356       //  comes out of eden (TLAB)...  In fact, ReduceInitialCardMarks</span>
<span class="line-removed">4357       //  causes the non-eden paths to take compensating steps to</span>
<span class="line-removed">4358       //  simulate a fresh allocation, so that no further</span>
<span class="line-removed">4359       //  card marks are required in compiled code to initialize</span>
<span class="line-removed">4360       //  the object.)</span>
4361 
4362       if (!stopped()) {
<span class="line-modified">4363         copy_to_clone(obj, alloc_obj, obj_size, true);</span>
<span class="line-modified">4364 </span>
<span class="line-modified">4365         // Present the results of the copy.</span>
<span class="line-modified">4366         result_reg-&gt;init_req(_array_path, control());</span>
<span class="line-modified">4367         result_val-&gt;init_req(_array_path, alloc_obj);</span>
<span class="line-modified">4368         result_i_o -&gt;set_req(_array_path, i_o());</span>
<span class="line-modified">4369         result_mem -&gt;set_req(_array_path, reset_memory());</span>




































4370       }
4371     }
4372 
<span class="line-removed">4373     // We only go to the instance fast case code if we pass a number of guards.</span>
<span class="line-removed">4374     // The paths which do not pass are accumulated in the slow_region.</span>
<span class="line-removed">4375     RegionNode* slow_region = new RegionNode(1);</span>
<span class="line-removed">4376     record_for_igvn(slow_region);</span>
4377     if (!stopped()) {
4378       // It&#39;s an instance (we did array above).  Make the slow-path tests.
4379       // If this is a virtual call, we generate a funny guard.  We grab
4380       // the vtable entry corresponding to clone() from the target object.
4381       // If the target method which we are calling happens to be the
4382       // Object clone() method, we pass the guard.  We do not need this
4383       // guard for non-virtual calls; the caller is known to be the native
4384       // Object clone().
4385       if (is_virtual) {
4386         generate_virtual_guard(obj_klass, slow_region);
4387       }
4388 
4389       // The object must be easily cloneable and must not have a finalizer.
4390       // Both of these conditions may be checked in a single test.
4391       // We could optimize the test further, but we don&#39;t care.
4392       generate_access_flags_guard(obj_klass,
4393                                   // Test both conditions:
4394                                   JVM_ACC_IS_CLONEABLE_FAST | JVM_ACC_HAS_FINALIZER,
4395                                   // Must be cloneable but not finalizer:
4396                                   JVM_ACC_IS_CLONEABLE_FAST,
</pre>
<hr />
<pre>
4517 // array in the heap that GCs wouldn&#39;t expect. Move the allocation
4518 // after the traps so we don&#39;t allocate the array if we
4519 // deoptimize. This is possible because tightly_coupled_allocation()
4520 // guarantees there&#39;s no observer of the allocated array at this point
4521 // and the control flow is simple enough.
4522 void LibraryCallKit::arraycopy_move_allocation_here(AllocateArrayNode* alloc, Node* dest, JVMState* saved_jvms,
4523                                                     int saved_reexecute_sp, uint new_idx) {
4524   if (saved_jvms != NULL &amp;&amp; !stopped()) {
4525     assert(alloc != NULL, &quot;only with a tightly coupled allocation&quot;);
4526     // restore JVM state to the state at the arraycopy
4527     saved_jvms-&gt;map()-&gt;set_control(map()-&gt;control());
4528     assert(saved_jvms-&gt;map()-&gt;memory() == map()-&gt;memory(), &quot;memory state changed?&quot;);
4529     assert(saved_jvms-&gt;map()-&gt;i_o() == map()-&gt;i_o(), &quot;IO state changed?&quot;);
4530     // If we&#39;ve improved the types of some nodes (null check) while
4531     // emitting the guards, propagate them to the current state
4532     map()-&gt;replaced_nodes().apply(saved_jvms-&gt;map(), new_idx);
4533     set_jvms(saved_jvms);
4534     _reexecute_sp = saved_reexecute_sp;
4535 
4536     // Remove the allocation from above the guards
<span class="line-modified">4537     CallProjections callprojs;</span>
<span class="line-removed">4538     alloc-&gt;extract_projections(&amp;callprojs, true);</span>
4539     InitializeNode* init = alloc-&gt;initialization();
4540     Node* alloc_mem = alloc-&gt;in(TypeFunc::Memory);
<span class="line-modified">4541     C-&gt;gvn_replace_by(callprojs.fallthrough_ioproj, alloc-&gt;in(TypeFunc::I_O));</span>
4542     C-&gt;gvn_replace_by(init-&gt;proj_out(TypeFunc::Memory), alloc_mem);
4543     C-&gt;gvn_replace_by(init-&gt;proj_out(TypeFunc::Control), alloc-&gt;in(0));
4544 
4545     // move the allocation here (after the guards)
4546     _gvn.hash_delete(alloc);
4547     alloc-&gt;set_req(TypeFunc::Control, control());
4548     alloc-&gt;set_req(TypeFunc::I_O, i_o());
4549     Node *mem = reset_memory();
4550     set_all_memory(mem);
4551     alloc-&gt;set_req(TypeFunc::Memory, mem);
4552     set_control(init-&gt;proj_out_or_null(TypeFunc::Control));
<span class="line-modified">4553     set_i_o(callprojs.fallthrough_ioproj);</span>
4554 
4555     // Update memory as done in GraphKit::set_output_for_allocation()
4556     const TypeInt* length_type = _gvn.find_int_type(alloc-&gt;in(AllocateNode::ALength));
4557     const TypeOopPtr* ary_type = _gvn.type(alloc-&gt;in(AllocateNode::KlassNode))-&gt;is_klassptr()-&gt;as_instance_type();
4558     if (ary_type-&gt;isa_aryptr() &amp;&amp; length_type != NULL) {
4559       ary_type = ary_type-&gt;is_aryptr()-&gt;cast_to_size(length_type);
4560     }
4561     const TypePtr* telemref = ary_type-&gt;add_offset(Type::OffsetBot);
4562     int            elemidx  = C-&gt;get_alias_index(telemref);
4563     set_memory(init-&gt;proj_out_or_null(TypeFunc::Memory), Compile::AliasIdxRaw);
4564     set_memory(init-&gt;proj_out_or_null(TypeFunc::Memory), elemidx);
4565 
4566     Node* allocx = _gvn.transform(alloc);
4567     assert(allocx == alloc, &quot;where has the allocation gone?&quot;);
4568     assert(dest-&gt;is_CheckCastPP(), &quot;not an allocation result?&quot;);
4569 
4570     _gvn.hash_delete(dest);
4571     dest-&gt;set_req(0, control());
4572     Node* destx = _gvn.transform(dest);
4573     assert(destx == dest, &quot;where has the allocation result gone?&quot;);
</pre>
<hr />
<pre>
4677       if (!has_src) {
4678         src = maybe_cast_profiled_obj(src, src_k, true);
4679         src_type  = _gvn.type(src);
4680         top_src  = src_type-&gt;isa_aryptr();
4681         has_src = (top_src != NULL &amp;&amp; top_src-&gt;klass() != NULL);
4682         src_spec = true;
4683       }
4684       if (!has_dest) {
4685         dest = maybe_cast_profiled_obj(dest, dest_k, true);
4686         dest_type  = _gvn.type(dest);
4687         top_dest  = dest_type-&gt;isa_aryptr();
4688         has_dest = (top_dest != NULL &amp;&amp; top_dest-&gt;klass() != NULL);
4689         dest_spec = true;
4690       }
4691     }
4692   }
4693 
4694   if (has_src &amp;&amp; has_dest &amp;&amp; can_emit_guards) {
4695     BasicType src_elem  = top_src-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
4696     BasicType dest_elem = top_dest-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
<span class="line-modified">4697     if (is_reference_type(src_elem))   src_elem  = T_OBJECT;</span>
<span class="line-modified">4698     if (is_reference_type(dest_elem))  dest_elem = T_OBJECT;</span>
4699 
4700     if (src_elem == dest_elem &amp;&amp; src_elem == T_OBJECT) {
4701       // If both arrays are object arrays then having the exact types
4702       // for both will remove the need for a subtype check at runtime
4703       // before the call and may make it possible to pick a faster copy
4704       // routine (without a subtype check on every element)
4705       // Do we have the exact type of src?
4706       bool could_have_src = src_spec;
4707       // Do we have the exact type of dest?
4708       bool could_have_dest = dest_spec;
4709       ciKlass* src_k = top_src-&gt;klass();
4710       ciKlass* dest_k = top_dest-&gt;klass();
4711       if (!src_spec) {
4712         src_k = src_type-&gt;speculative_type_not_null();
4713         if (src_k != NULL &amp;&amp; src_k-&gt;is_array_klass()) {
4714           could_have_src = true;
4715         }
4716       }
4717       if (!dest_spec) {
4718         dest_k = dest_type-&gt;speculative_type_not_null();
</pre>
<hr />
<pre>
4778     // we also have to check it here for the case where the ArrayCopyNode will
4779     // be eliminated by Escape Analysis.
4780     if (EliminateAllocations) {
4781       generate_negative_guard(length, slow_region);
4782       negative_length_guard_generated = true;
4783     }
4784 
4785     // (9) each element of an oop array must be assignable
4786     Node* dest_klass = load_object_klass(dest);
4787     if (src != dest) {
4788       Node* not_subtype_ctrl = gen_subtype_check(src, dest_klass);
4789 
4790       if (not_subtype_ctrl != top()) {
4791         PreserveJVMState pjvms(this);
4792         set_control(not_subtype_ctrl);
4793         uncommon_trap(Deoptimization::Reason_intrinsic,
4794                       Deoptimization::Action_make_not_entrant);
4795         assert(stopped(), &quot;Should be stopped&quot;);
4796       }
4797     }
















4798     {
4799       PreserveJVMState pjvms(this);
4800       set_control(_gvn.transform(slow_region));
4801       uncommon_trap(Deoptimization::Reason_intrinsic,
4802                     Deoptimization::Action_make_not_entrant);
4803       assert(stopped(), &quot;Should be stopped&quot;);
4804     }
<span class="line-removed">4805 </span>
<span class="line-removed">4806     const TypeKlassPtr* dest_klass_t = _gvn.type(dest_klass)-&gt;is_klassptr();</span>
<span class="line-removed">4807     const Type *toop = TypeOopPtr::make_from_klass(dest_klass_t-&gt;klass());</span>
<span class="line-removed">4808     src = _gvn.transform(new CheckCastPPNode(control(), src, toop));</span>
4809   }
4810 
4811   arraycopy_move_allocation_here(alloc, dest, saved_jvms, saved_reexecute_sp, new_idx);
4812 
4813   if (stopped()) {
4814     return true;
4815   }
4816 
4817   ArrayCopyNode* ac = ArrayCopyNode::make(this, true, src, src_offset, dest, dest_offset, length, alloc != NULL, negative_length_guard_generated,
4818                                           // Create LoadRange and LoadKlass nodes for use during macro expansion here
4819                                           // so the compiler has a chance to eliminate them: during macro expansion,
4820                                           // we have to set their control (CastPP nodes are eliminated).
4821                                           load_object_klass(src), load_object_klass(dest),
4822                                           load_array_length(src), load_array_length(dest));
4823 
4824   ac-&gt;set_arraycopy(validated);
4825 
4826   Node* n = _gvn.transform(ac);
4827   if (n == ac) {
4828     ac-&gt;connect_outputs(this);
</pre>
</td>
<td>
<hr />
<pre>
  36 #include &quot;oops/objArrayKlass.hpp&quot;
  37 #include &quot;opto/addnode.hpp&quot;
  38 #include &quot;opto/arraycopynode.hpp&quot;
  39 #include &quot;opto/c2compiler.hpp&quot;
  40 #include &quot;opto/callGenerator.hpp&quot;
  41 #include &quot;opto/castnode.hpp&quot;
  42 #include &quot;opto/cfgnode.hpp&quot;
  43 #include &quot;opto/convertnode.hpp&quot;
  44 #include &quot;opto/countbitsnode.hpp&quot;
  45 #include &quot;opto/intrinsicnode.hpp&quot;
  46 #include &quot;opto/idealKit.hpp&quot;
  47 #include &quot;opto/mathexactnode.hpp&quot;
  48 #include &quot;opto/movenode.hpp&quot;
  49 #include &quot;opto/mulnode.hpp&quot;
  50 #include &quot;opto/narrowptrnode.hpp&quot;
  51 #include &quot;opto/opaquenode.hpp&quot;
  52 #include &quot;opto/parse.hpp&quot;
  53 #include &quot;opto/runtime.hpp&quot;
  54 #include &quot;opto/rootnode.hpp&quot;
  55 #include &quot;opto/subnode.hpp&quot;
<span class="line-added">  56 #include &quot;opto/valuetypenode.hpp&quot;</span>
  57 #include &quot;prims/nativeLookup.hpp&quot;
  58 #include &quot;prims/unsafe.hpp&quot;
  59 #include &quot;runtime/objectMonitor.hpp&quot;
  60 #include &quot;runtime/sharedRuntime.hpp&quot;
  61 #include &quot;utilities/macros.hpp&quot;
  62 #include &quot;utilities/powerOfTwo.hpp&quot;
  63 
  64 class LibraryIntrinsic : public InlineCallGenerator {
  65   // Extend the set of intrinsics known to the runtime:
  66  public:
  67  private:
  68   bool             _is_virtual;
  69   bool             _does_virtual_dispatch;
  70   int8_t           _predicates_count;  // Intrinsic is predicated by several conditions
  71   int8_t           _last_predicate; // Last generated predicate
  72   vmIntrinsics::ID _intrinsic_id;
  73 
  74  public:
  75   LibraryIntrinsic(ciMethod* m, bool is_virtual, int predicates_count, bool does_virtual_dispatch, vmIntrinsics::ID id)
  76     : InlineCallGenerator(m),
</pre>
<hr />
<pre>
 118       ciSignature* declared_signature = NULL;
 119       ciMethod* ignored_callee = caller()-&gt;get_method_at_bci(bci(), ignored_will_link, &amp;declared_signature);
 120       const int nargs = declared_signature-&gt;arg_size_for_bc(caller()-&gt;java_code_at_bci(bci()));
 121       _reexecute_sp = sp() + nargs;  // &quot;push&quot; arguments back on stack
 122     }
 123   }
 124 
 125   virtual LibraryCallKit* is_LibraryCallKit() const { return (LibraryCallKit*)this; }
 126 
 127   ciMethod*         caller()    const    { return jvms()-&gt;method(); }
 128   int               bci()       const    { return jvms()-&gt;bci(); }
 129   LibraryIntrinsic* intrinsic() const    { return _intrinsic; }
 130   vmIntrinsics::ID  intrinsic_id() const { return _intrinsic-&gt;intrinsic_id(); }
 131   ciMethod*         callee()    const    { return _intrinsic-&gt;method(); }
 132 
 133   bool  try_to_inline(int predicate);
 134   Node* try_to_predicate(int predicate);
 135 
 136   void push_result() {
 137     // Push the result onto the stack.
<span class="line-modified"> 138     Node* res = result();</span>
<span class="line-modified"> 139     if (!stopped() &amp;&amp; res != NULL) {</span>
<span class="line-modified"> 140       BasicType bt = res-&gt;bottom_type()-&gt;basic_type();</span>
<span class="line-added"> 141       if (C-&gt;inlining_incrementally() &amp;&amp; res-&gt;is_ValueType()) {</span>
<span class="line-added"> 142         // The caller expects an oop when incrementally inlining an intrinsic that returns an</span>
<span class="line-added"> 143         // inline type. Make sure the call is re-executed if the allocation triggers a deoptimization.</span>
<span class="line-added"> 144         PreserveReexecuteState preexecs(this);</span>
<span class="line-added"> 145         jvms()-&gt;set_should_reexecute(true);</span>
<span class="line-added"> 146         res = ValueTypePtrNode::make_from_value_type(this, res-&gt;as_ValueType());</span>
<span class="line-added"> 147       }</span>
<span class="line-added"> 148       push_node(bt, res);</span>
 149     }
 150   }
 151 
 152  private:
 153   void fatal_unexpected_iid(vmIntrinsics::ID iid) {
 154     fatal(&quot;unexpected intrinsic %d: %s&quot;, iid, vmIntrinsics::name_at(iid));
 155   }
 156 
 157   void  set_result(Node* n) { assert(_result == NULL, &quot;only set once&quot;); _result = n; }
 158   void  set_result(RegionNode* region, PhiNode* value);
 159   Node*     result() { return _result; }
 160 
 161   virtual int reexecute_sp() { return _reexecute_sp; }
 162 
 163   // Helper functions to inline natives
 164   Node* generate_guard(Node* test, RegionNode* region, float true_prob);
 165   Node* generate_slow_guard(Node* test, RegionNode* region);
 166   Node* generate_fair_guard(Node* test, RegionNode* region);
 167   Node* generate_negative_guard(Node* index, RegionNode* region,
 168                                 // resulting CastII of index:
 169                                 Node* *pos_index = NULL);
 170   Node* generate_limit_guard(Node* offset, Node* subseq_length,
 171                              Node* array_length,
 172                              RegionNode* region);
 173   void  generate_string_range_check(Node* array, Node* offset,
 174                                     Node* length, bool char_count);
 175   Node* generate_current_thread(Node* &amp;tls_output);

 176   Node* load_klass_from_mirror_common(Node* mirror, bool never_see_null,
 177                                       RegionNode* region, int null_path,
 178                                       int offset);
 179   Node* load_klass_from_mirror(Node* mirror, bool never_see_null,
 180                                RegionNode* region, int null_path) {
 181     int offset = java_lang_Class::klass_offset_in_bytes();
 182     return load_klass_from_mirror_common(mirror, never_see_null,
 183                                          region, null_path,
 184                                          offset);
 185   }
 186   Node* load_array_klass_from_mirror(Node* mirror, bool never_see_null,
 187                                      RegionNode* region, int null_path) {
 188     int offset = java_lang_Class::array_klass_offset_in_bytes();
 189     return load_klass_from_mirror_common(mirror, never_see_null,
 190                                          region, null_path,
 191                                          offset);
 192   }
 193   Node* generate_access_flags_guard(Node* kls,
 194                                     int modifier_mask, int modifier_bits,
 195                                     RegionNode* region);
 196   Node* generate_interface_guard(Node* kls, RegionNode* region);
<span class="line-added"> 197   Node* generate_value_guard(Node* kls, RegionNode* region);</span>
<span class="line-added"> 198 </span>
<span class="line-added"> 199   enum ArrayKind {</span>
<span class="line-added"> 200     AnyArray,</span>
<span class="line-added"> 201     NonArray,</span>
<span class="line-added"> 202     ObjectArray,</span>
<span class="line-added"> 203     NonObjectArray,</span>
<span class="line-added"> 204     TypeArray,</span>
<span class="line-added"> 205     ValueArray</span>
<span class="line-added"> 206   };</span>
<span class="line-added"> 207 </span>
 208   Node* generate_hidden_class_guard(Node* kls, RegionNode* region);
<span class="line-added"> 209 </span>
 210   Node* generate_array_guard(Node* kls, RegionNode* region) {
<span class="line-modified"> 211     return generate_array_guard_common(kls, region, AnyArray);</span>
 212   }
 213   Node* generate_non_array_guard(Node* kls, RegionNode* region) {
<span class="line-modified"> 214     return generate_array_guard_common(kls, region, NonArray);</span>
 215   }
 216   Node* generate_objArray_guard(Node* kls, RegionNode* region) {
<span class="line-modified"> 217     return generate_array_guard_common(kls, region, ObjectArray);</span>
 218   }
 219   Node* generate_non_objArray_guard(Node* kls, RegionNode* region) {
<span class="line-modified"> 220     return generate_array_guard_common(kls, region, NonObjectArray);</span>
<span class="line-added"> 221   }</span>
<span class="line-added"> 222   Node* generate_typeArray_guard(Node* kls, RegionNode* region) {</span>
<span class="line-added"> 223     return generate_array_guard_common(kls, region, TypeArray);</span>
<span class="line-added"> 224   }</span>
<span class="line-added"> 225   Node* generate_valueArray_guard(Node* kls, RegionNode* region) {</span>
<span class="line-added"> 226     assert(ValueArrayFlatten, &quot;can never be flattened&quot;);</span>
<span class="line-added"> 227     return generate_array_guard_common(kls, region, ValueArray);</span>
 228   }
<span class="line-modified"> 229   Node* generate_array_guard_common(Node* kls, RegionNode* region, ArrayKind kind);</span>

 230   Node* generate_virtual_guard(Node* obj_klass, RegionNode* slow_region);
 231   CallJavaNode* generate_method_call(vmIntrinsics::ID method_id,
 232                                      bool is_virtual = false, bool is_static = false);
 233   CallJavaNode* generate_method_call_static(vmIntrinsics::ID method_id) {
 234     return generate_method_call(method_id, false, true);
 235   }
 236   CallJavaNode* generate_method_call_virtual(vmIntrinsics::ID method_id) {
 237     return generate_method_call(method_id, true, false);
 238   }
 239   Node * load_field_from_object(Node * fromObj, const char * fieldName, const char * fieldTypeString, bool is_exact, bool is_static, ciInstanceKlass * fromKls);
 240   Node * field_address_from_object(Node * fromObj, const char * fieldName, const char * fieldTypeString, bool is_exact, bool is_static, ciInstanceKlass * fromKls);
 241 
 242   Node* make_string_method_node(int opcode, Node* str1_start, Node* cnt1, Node* str2_start, Node* cnt2, StrIntrinsicNode::ArgEnc ae);
 243   bool inline_string_compareTo(StrIntrinsicNode::ArgEnc ae);
 244   bool inline_string_indexOf(StrIntrinsicNode::ArgEnc ae);
 245   bool inline_string_indexOfI(StrIntrinsicNode::ArgEnc ae);
 246   Node* make_indexOf_node(Node* src_start, Node* src_count, Node* tgt_start, Node* tgt_count,
 247                           RegionNode* region, Node* phi, StrIntrinsicNode::ArgEnc ae);
 248   bool inline_string_indexOfChar();
 249   bool inline_string_equals(StrIntrinsicNode::ArgEnc ae);
</pre>
<hr />
<pre>
 267   bool inline_math_negateExactI();
 268   bool inline_math_negateExactL();
 269   bool inline_math_subtractExactI(bool is_decrement);
 270   bool inline_math_subtractExactL(bool is_decrement);
 271   bool inline_min_max(vmIntrinsics::ID id);
 272   bool inline_notify(vmIntrinsics::ID id);
 273   Node* generate_min_max(vmIntrinsics::ID id, Node* x, Node* y);
 274   // This returns Type::AnyPtr, RawPtr, or OopPtr.
 275   int classify_unsafe_addr(Node* &amp;base, Node* &amp;offset, BasicType type);
 276   Node* make_unsafe_address(Node*&amp; base, Node* offset, DecoratorSet decorators, BasicType type = T_ILLEGAL, bool can_cast = false);
 277 
 278   typedef enum { Relaxed, Opaque, Volatile, Acquire, Release } AccessKind;
 279   DecoratorSet mo_decorator_for_access_kind(AccessKind kind);
 280   bool inline_unsafe_access(bool is_store, BasicType type, AccessKind kind, bool is_unaligned);
 281   static bool klass_needs_init_guard(Node* kls);
 282   bool inline_unsafe_allocate();
 283   bool inline_unsafe_newArray(bool uninitialized);
 284   bool inline_unsafe_writeback0();
 285   bool inline_unsafe_writebackSync0(bool is_pre);
 286   bool inline_unsafe_copyMemory();
<span class="line-added"> 287   bool inline_unsafe_make_private_buffer();</span>
<span class="line-added"> 288   bool inline_unsafe_finish_private_buffer();</span>
 289   bool inline_native_currentThread();
 290 
 291   bool inline_native_time_funcs(address method, const char* funcName);
 292 #ifdef JFR_HAVE_INTRINSICS
 293   bool inline_native_classID();
 294   bool inline_native_getEventWriter();
 295 #endif
 296   bool inline_native_Class_query(vmIntrinsics::ID id);
 297   bool inline_native_subtype_check();
 298   bool inline_native_getLength();
 299   bool inline_array_copyOf(bool is_copyOfRange);
 300   bool inline_array_equals(StrIntrinsicNode::ArgEnc ae);
 301   bool inline_preconditions_checkIndex();
 302   void copy_to_clone(Node* obj, Node* alloc_obj, Node* obj_size, bool is_array);
 303   bool inline_native_clone(bool is_virtual);
 304   bool inline_native_Reflection_getCallerClass();
 305   // Helper function for inlining native object hash method
 306   bool inline_native_hashcode(bool is_virtual, bool is_static);
 307   bool inline_native_getClass();
 308 
</pre>
<hr />
<pre>
 613   case vmIntrinsics::_indexOfU:                 return inline_string_indexOf(StrIntrinsicNode::UU);
 614   case vmIntrinsics::_indexOfUL:                return inline_string_indexOf(StrIntrinsicNode::UL);
 615   case vmIntrinsics::_indexOfIL:                return inline_string_indexOfI(StrIntrinsicNode::LL);
 616   case vmIntrinsics::_indexOfIU:                return inline_string_indexOfI(StrIntrinsicNode::UU);
 617   case vmIntrinsics::_indexOfIUL:               return inline_string_indexOfI(StrIntrinsicNode::UL);
 618   case vmIntrinsics::_indexOfU_char:            return inline_string_indexOfChar();
 619 
 620   case vmIntrinsics::_equalsL:                  return inline_string_equals(StrIntrinsicNode::LL);
 621   case vmIntrinsics::_equalsU:                  return inline_string_equals(StrIntrinsicNode::UU);
 622 
 623   case vmIntrinsics::_toBytesStringU:           return inline_string_toBytesU();
 624   case vmIntrinsics::_getCharsStringU:          return inline_string_getCharsU();
 625   case vmIntrinsics::_getCharStringU:           return inline_string_char_access(!is_store);
 626   case vmIntrinsics::_putCharStringU:           return inline_string_char_access( is_store);
 627 
 628   case vmIntrinsics::_compressStringC:
 629   case vmIntrinsics::_compressStringB:          return inline_string_copy( is_compress);
 630   case vmIntrinsics::_inflateStringC:
 631   case vmIntrinsics::_inflateStringB:           return inline_string_copy(!is_compress);
 632 
<span class="line-added"> 633   case vmIntrinsics::_makePrivateBuffer:        return inline_unsafe_make_private_buffer();</span>
<span class="line-added"> 634   case vmIntrinsics::_finishPrivateBuffer:      return inline_unsafe_finish_private_buffer();</span>
 635   case vmIntrinsics::_getReference:             return inline_unsafe_access(!is_store, T_OBJECT,   Relaxed, false);
 636   case vmIntrinsics::_getBoolean:               return inline_unsafe_access(!is_store, T_BOOLEAN,  Relaxed, false);
 637   case vmIntrinsics::_getByte:                  return inline_unsafe_access(!is_store, T_BYTE,     Relaxed, false);
 638   case vmIntrinsics::_getShort:                 return inline_unsafe_access(!is_store, T_SHORT,    Relaxed, false);
 639   case vmIntrinsics::_getChar:                  return inline_unsafe_access(!is_store, T_CHAR,     Relaxed, false);
 640   case vmIntrinsics::_getInt:                   return inline_unsafe_access(!is_store, T_INT,      Relaxed, false);
 641   case vmIntrinsics::_getLong:                  return inline_unsafe_access(!is_store, T_LONG,     Relaxed, false);
 642   case vmIntrinsics::_getFloat:                 return inline_unsafe_access(!is_store, T_FLOAT,    Relaxed, false);
 643   case vmIntrinsics::_getDouble:                return inline_unsafe_access(!is_store, T_DOUBLE,   Relaxed, false);
<span class="line-added"> 644   case vmIntrinsics::_getValue:                 return inline_unsafe_access(!is_store, T_VALUETYPE,Relaxed, false);</span>
 645 
 646   case vmIntrinsics::_putReference:             return inline_unsafe_access( is_store, T_OBJECT,   Relaxed, false);
 647   case vmIntrinsics::_putBoolean:               return inline_unsafe_access( is_store, T_BOOLEAN,  Relaxed, false);
 648   case vmIntrinsics::_putByte:                  return inline_unsafe_access( is_store, T_BYTE,     Relaxed, false);
 649   case vmIntrinsics::_putShort:                 return inline_unsafe_access( is_store, T_SHORT,    Relaxed, false);
 650   case vmIntrinsics::_putChar:                  return inline_unsafe_access( is_store, T_CHAR,     Relaxed, false);
 651   case vmIntrinsics::_putInt:                   return inline_unsafe_access( is_store, T_INT,      Relaxed, false);
 652   case vmIntrinsics::_putLong:                  return inline_unsafe_access( is_store, T_LONG,     Relaxed, false);
 653   case vmIntrinsics::_putFloat:                 return inline_unsafe_access( is_store, T_FLOAT,    Relaxed, false);
 654   case vmIntrinsics::_putDouble:                return inline_unsafe_access( is_store, T_DOUBLE,   Relaxed, false);
<span class="line-added"> 655   case vmIntrinsics::_putValue:                 return inline_unsafe_access( is_store, T_VALUETYPE,Relaxed, false);</span>
 656 
 657   case vmIntrinsics::_getReferenceVolatile:     return inline_unsafe_access(!is_store, T_OBJECT,   Volatile, false);
 658   case vmIntrinsics::_getBooleanVolatile:       return inline_unsafe_access(!is_store, T_BOOLEAN,  Volatile, false);
 659   case vmIntrinsics::_getByteVolatile:          return inline_unsafe_access(!is_store, T_BYTE,     Volatile, false);
 660   case vmIntrinsics::_getShortVolatile:         return inline_unsafe_access(!is_store, T_SHORT,    Volatile, false);
 661   case vmIntrinsics::_getCharVolatile:          return inline_unsafe_access(!is_store, T_CHAR,     Volatile, false);
 662   case vmIntrinsics::_getIntVolatile:           return inline_unsafe_access(!is_store, T_INT,      Volatile, false);
 663   case vmIntrinsics::_getLongVolatile:          return inline_unsafe_access(!is_store, T_LONG,     Volatile, false);
 664   case vmIntrinsics::_getFloatVolatile:         return inline_unsafe_access(!is_store, T_FLOAT,    Volatile, false);
 665   case vmIntrinsics::_getDoubleVolatile:        return inline_unsafe_access(!is_store, T_DOUBLE,   Volatile, false);
 666 
 667   case vmIntrinsics::_putReferenceVolatile:     return inline_unsafe_access( is_store, T_OBJECT,   Volatile, false);
 668   case vmIntrinsics::_putBooleanVolatile:       return inline_unsafe_access( is_store, T_BOOLEAN,  Volatile, false);
 669   case vmIntrinsics::_putByteVolatile:          return inline_unsafe_access( is_store, T_BYTE,     Volatile, false);
 670   case vmIntrinsics::_putShortVolatile:         return inline_unsafe_access( is_store, T_SHORT,    Volatile, false);
 671   case vmIntrinsics::_putCharVolatile:          return inline_unsafe_access( is_store, T_CHAR,     Volatile, false);
 672   case vmIntrinsics::_putIntVolatile:           return inline_unsafe_access( is_store, T_INT,      Volatile, false);
 673   case vmIntrinsics::_putLongVolatile:          return inline_unsafe_access( is_store, T_LONG,     Volatile, false);
 674   case vmIntrinsics::_putFloatVolatile:         return inline_unsafe_access( is_store, T_FLOAT,    Volatile, false);
 675   case vmIntrinsics::_putDoubleVolatile:        return inline_unsafe_access( is_store, T_DOUBLE,   Volatile, false);
</pre>
<hr />
<pre>
2411   guarantee( is_store || kind != Release, &quot;Release accesses can be produced only for stores&quot;);
2412   assert(type != T_OBJECT || !unaligned, &quot;unaligned access not supported with object type&quot;);
2413 
2414   if (is_reference_type(type)) {
2415     decorators |= ON_UNKNOWN_OOP_REF;
2416   }
2417 
2418   if (unaligned) {
2419     decorators |= C2_UNALIGNED;
2420   }
2421 
2422 #ifndef PRODUCT
2423   {
2424     ResourceMark rm;
2425     // Check the signatures.
2426     ciSignature* sig = callee()-&gt;signature();
2427 #ifdef ASSERT
2428     if (!is_store) {
2429       // Object getReference(Object base, int/long offset), etc.
2430       BasicType rtype = sig-&gt;return_type()-&gt;basic_type();
<span class="line-modified">2431       assert(rtype == type || (rtype == T_OBJECT &amp;&amp; type == T_VALUETYPE), &quot;getter must return the expected value&quot;);</span>
<span class="line-modified">2432       assert(sig-&gt;count() == 2 || (type == T_VALUETYPE &amp;&amp; sig-&gt;count() == 3), &quot;oop getter has 2 or 3 arguments&quot;);</span>
2433       assert(sig-&gt;type_at(0)-&gt;basic_type() == T_OBJECT, &quot;getter base is object&quot;);
2434       assert(sig-&gt;type_at(1)-&gt;basic_type() == T_LONG, &quot;getter offset is correct&quot;);
2435     } else {
2436       // void putReference(Object base, int/long offset, Object x), etc.
2437       assert(sig-&gt;return_type()-&gt;basic_type() == T_VOID, &quot;putter must not return a value&quot;);
<span class="line-modified">2438       assert(sig-&gt;count() == 3 || (type == T_VALUETYPE &amp;&amp; sig-&gt;count() == 4), &quot;oop putter has 3 arguments&quot;);</span>
2439       assert(sig-&gt;type_at(0)-&gt;basic_type() == T_OBJECT, &quot;putter base is object&quot;);
2440       assert(sig-&gt;type_at(1)-&gt;basic_type() == T_LONG, &quot;putter offset is correct&quot;);
2441       BasicType vtype = sig-&gt;type_at(sig-&gt;count()-1)-&gt;basic_type();
<span class="line-modified">2442       assert(vtype == type || (type == T_VALUETYPE &amp;&amp; vtype == T_OBJECT), &quot;putter must accept the expected value&quot;);</span>
2443     }
2444 #endif // ASSERT
2445  }
2446 #endif //PRODUCT
2447 
2448   C-&gt;set_has_unsafe_access(true);  // Mark eventual nmethod as &quot;unsafe&quot;.
2449 
2450   Node* receiver = argument(0);  // type: oop
2451 
2452   // Build address expression.
2453   Node* adr;
2454   Node* heap_base_oop = top();
2455   Node* offset = top();
2456   Node* val;
2457 
2458   // The base is either a Java object or a value produced by Unsafe.staticFieldBase
2459   Node* base = argument(1);  // type: oop
2460   // The offset is a value produced by Unsafe.staticFieldOffset or Unsafe.objectFieldOffset
2461   offset = argument(2);  // type: long
2462   // We currently rely on the cookies produced by Unsafe.xxxFieldOffset
2463   // to be plain byte offsets, which are also the same as those accepted
2464   // by oopDesc::field_addr.
2465   assert(Unsafe_field_offset_to_byte_offset(11) == 11,
2466          &quot;fieldOffset must be byte-scaled&quot;);
<span class="line-added">2467 </span>
<span class="line-added">2468   ciValueKlass* value_klass = NULL;</span>
<span class="line-added">2469   if (type == T_VALUETYPE) {</span>
<span class="line-added">2470     Node* cls = null_check(argument(4));</span>
<span class="line-added">2471     if (stopped()) {</span>
<span class="line-added">2472       return true;</span>
<span class="line-added">2473     }</span>
<span class="line-added">2474     Node* kls = load_klass_from_mirror(cls, false, NULL, 0);</span>
<span class="line-added">2475     const TypeKlassPtr* kls_t = _gvn.type(kls)-&gt;isa_klassptr();</span>
<span class="line-added">2476     if (!kls_t-&gt;klass_is_exact()) {</span>
<span class="line-added">2477       return false;</span>
<span class="line-added">2478     }</span>
<span class="line-added">2479     ciKlass* klass = kls_t-&gt;klass();</span>
<span class="line-added">2480     if (!klass-&gt;is_valuetype()) {</span>
<span class="line-added">2481       return false;</span>
<span class="line-added">2482     }</span>
<span class="line-added">2483     value_klass = klass-&gt;as_value_klass();</span>
<span class="line-added">2484   }</span>
<span class="line-added">2485 </span>
<span class="line-added">2486   receiver = null_check(receiver);</span>
<span class="line-added">2487   if (stopped()) {</span>
<span class="line-added">2488     return true;</span>
<span class="line-added">2489   }</span>
<span class="line-added">2490 </span>
<span class="line-added">2491   if (base-&gt;is_ValueType()) {</span>
<span class="line-added">2492     ValueTypeNode* vt = base-&gt;as_ValueType();</span>
<span class="line-added">2493 </span>
<span class="line-added">2494     if (is_store) {</span>
<span class="line-added">2495       if (!vt-&gt;is_allocated(&amp;_gvn) || !_gvn.type(vt)-&gt;is_valuetype()-&gt;larval()) {</span>
<span class="line-added">2496         return false;</span>
<span class="line-added">2497       }</span>
<span class="line-added">2498       base = vt-&gt;get_oop();</span>
<span class="line-added">2499     } else {</span>
<span class="line-added">2500       if (offset-&gt;is_Con()) {</span>
<span class="line-added">2501         long off = find_long_con(offset, 0);</span>
<span class="line-added">2502         ciValueKlass* vk = vt-&gt;type()-&gt;value_klass();</span>
<span class="line-added">2503         if ((long)(int)off != off || !vk-&gt;contains_field_offset(off)) {</span>
<span class="line-added">2504           return false;</span>
<span class="line-added">2505         }</span>
<span class="line-added">2506 </span>
<span class="line-added">2507         ciField* f = vk-&gt;get_non_flattened_field_by_offset((int)off);</span>
<span class="line-added">2508 </span>
<span class="line-added">2509         if (f != NULL) {</span>
<span class="line-added">2510           BasicType bt = f-&gt;layout_type();</span>
<span class="line-added">2511           if (bt == T_ARRAY || bt == T_NARROWOOP) {</span>
<span class="line-added">2512             bt = T_OBJECT;</span>
<span class="line-added">2513           }</span>
<span class="line-added">2514           if (bt == type) {</span>
<span class="line-added">2515             if (bt != T_VALUETYPE || f-&gt;type() == value_klass) {</span>
<span class="line-added">2516               set_result(vt-&gt;field_value_by_offset((int)off, false));</span>
<span class="line-added">2517               return true;</span>
<span class="line-added">2518             }</span>
<span class="line-added">2519           }</span>
<span class="line-added">2520         }</span>
<span class="line-added">2521       }</span>
<span class="line-added">2522       // Re-execute the unsafe access if allocation triggers deoptimization.</span>
<span class="line-added">2523       PreserveReexecuteState preexecs(this);</span>
<span class="line-added">2524       jvms()-&gt;set_should_reexecute(true);</span>
<span class="line-added">2525       vt = vt-&gt;allocate(this)-&gt;as_ValueType();</span>
<span class="line-added">2526       base = vt-&gt;get_oop();</span>
<span class="line-added">2527     }</span>
<span class="line-added">2528   }</span>
<span class="line-added">2529 </span>
2530   // 32-bit machines ignore the high half!
2531   offset = ConvL2X(offset);
2532   adr = make_unsafe_address(base, offset, is_store ? ACCESS_WRITE : ACCESS_READ, type, kind == Relaxed);
2533 
2534   if (_gvn.type(base)-&gt;isa_ptr() == TypePtr::NULL_PTR) {
<span class="line-modified">2535     if (type != T_OBJECT &amp;&amp; (value_klass == NULL || !value_klass-&gt;has_object_fields())) {</span>
2536       decorators |= IN_NATIVE; // off-heap primitive access
2537     } else {
2538       return false; // off-heap oop accesses are not supported
2539     }
2540   } else {
2541     heap_base_oop = base; // on-heap or mixed access
2542   }
2543 
2544   // Can base be NULL? Otherwise, always on-heap access.
2545   bool can_access_non_heap = TypePtr::NULL_PTR-&gt;higher_equal(_gvn.type(base));
2546 
2547   if (!can_access_non_heap) {
2548     decorators |= IN_HEAP;
2549   }
2550 
<span class="line-modified">2551   val = is_store ? argument(4 + (type == T_VALUETYPE ? 1 : 0)) : NULL;</span>
2552 
2553   const TypePtr* adr_type = _gvn.type(adr)-&gt;isa_ptr();
2554   if (adr_type == TypePtr::NULL_PTR) {
2555     return false; // off-heap access with zero address
2556   }
2557 
2558   // Try to categorize the address.
2559   Compile::AliasType* alias_type = C-&gt;alias_type(adr_type);
2560   assert(alias_type-&gt;index() != Compile::AliasIdxBot, &quot;no bare pointers here&quot;);
2561 
2562   if (alias_type-&gt;adr_type() == TypeInstPtr::KLASS ||
2563       alias_type-&gt;adr_type() == TypeAryPtr::RANGE) {
2564     return false; // not supported
2565   }
2566 
2567   bool mismatched = false;
<span class="line-modified">2568   BasicType bt = T_ILLEGAL;</span>
<span class="line-added">2569   ciField* field = NULL;</span>
<span class="line-added">2570   if (adr_type-&gt;isa_instptr()) {</span>
<span class="line-added">2571     const TypeInstPtr* instptr = adr_type-&gt;is_instptr();</span>
<span class="line-added">2572     ciInstanceKlass* k = instptr-&gt;klass()-&gt;as_instance_klass();</span>
<span class="line-added">2573     int off = instptr-&gt;offset();</span>
<span class="line-added">2574     if (instptr-&gt;const_oop() != NULL &amp;&amp;</span>
<span class="line-added">2575         instptr-&gt;klass() == ciEnv::current()-&gt;Class_klass() &amp;&amp;</span>
<span class="line-added">2576         instptr-&gt;offset() &gt;= (instptr-&gt;klass()-&gt;as_instance_klass()-&gt;size_helper() * wordSize)) {</span>
<span class="line-added">2577       k = instptr-&gt;const_oop()-&gt;as_instance()-&gt;java_lang_Class_klass()-&gt;as_instance_klass();</span>
<span class="line-added">2578       field = k-&gt;get_field_by_offset(off, true);</span>
<span class="line-added">2579     } else {</span>
<span class="line-added">2580       field = k-&gt;get_non_flattened_field_by_offset(off);</span>
<span class="line-added">2581     }</span>
<span class="line-added">2582     if (field != NULL) {</span>
<span class="line-added">2583       bt = field-&gt;layout_type();</span>
<span class="line-added">2584     }</span>
<span class="line-added">2585     assert(bt == alias_type-&gt;basic_type() || bt == T_VALUETYPE, &quot;should match&quot;);</span>
<span class="line-added">2586     if (field != NULL &amp;&amp; bt == T_VALUETYPE &amp;&amp; !field-&gt;is_flattened()) {</span>
<span class="line-added">2587       bt = T_OBJECT;</span>
<span class="line-added">2588     }</span>
<span class="line-added">2589   } else {</span>
<span class="line-added">2590     bt = alias_type-&gt;basic_type();</span>
<span class="line-added">2591   }</span>
<span class="line-added">2592 </span>
2593   if (bt != T_ILLEGAL) {
2594     assert(alias_type-&gt;adr_type()-&gt;is_oopptr(), &quot;should be on-heap access&quot;);
2595     if (bt == T_BYTE &amp;&amp; adr_type-&gt;isa_aryptr()) {
2596       // Alias type doesn&#39;t differentiate between byte[] and boolean[]).
2597       // Use address type to get the element type.
2598       bt = adr_type-&gt;is_aryptr()-&gt;elem()-&gt;array_element_basic_type();
2599     }
2600     if (bt == T_ARRAY || bt == T_NARROWOOP) {
2601       // accessing an array field with getReference is not a mismatch
2602       bt = T_OBJECT;
2603     }
2604     if ((bt == T_OBJECT) != (type == T_OBJECT)) {
2605       // Don&#39;t intrinsify mismatched object accesses
2606       return false;
2607     }
2608     mismatched = (bt != type);
2609   } else if (alias_type-&gt;adr_type()-&gt;isa_oopptr()) {
2610     mismatched = true; // conservatively mark all &quot;wide&quot; on-heap accesses as mismatched
2611   }
2612 
<span class="line-added">2613   if (type == T_VALUETYPE) {</span>
<span class="line-added">2614     if (adr_type-&gt;isa_instptr()) {</span>
<span class="line-added">2615       if (field == NULL || field-&gt;type() != value_klass) {</span>
<span class="line-added">2616         mismatched = true;</span>
<span class="line-added">2617       }</span>
<span class="line-added">2618     } else if (adr_type-&gt;isa_aryptr()) {</span>
<span class="line-added">2619       const Type* elem = adr_type-&gt;is_aryptr()-&gt;elem();</span>
<span class="line-added">2620       if (!elem-&gt;isa_valuetype()) {</span>
<span class="line-added">2621         mismatched = true;</span>
<span class="line-added">2622       } else if (elem-&gt;value_klass() != value_klass) {</span>
<span class="line-added">2623         mismatched = true;</span>
<span class="line-added">2624       }</span>
<span class="line-added">2625     }</span>
<span class="line-added">2626     if (is_store) {</span>
<span class="line-added">2627       const Type* val_t = _gvn.type(val);</span>
<span class="line-added">2628       if (!val_t-&gt;isa_valuetype() || val_t-&gt;value_klass() != value_klass) {</span>
<span class="line-added">2629         return false;</span>
<span class="line-added">2630       }</span>
<span class="line-added">2631     }</span>
<span class="line-added">2632   }</span>
<span class="line-added">2633 </span>
2634   assert(!mismatched || alias_type-&gt;adr_type()-&gt;is_oopptr(), &quot;off-heap access can&#39;t be mismatched&quot;);
2635 
2636   if (mismatched) {
2637     decorators |= C2_MISMATCHED;
2638   }
2639 
2640   // First guess at the value type.
2641   const Type *value_type = Type::get_const_basic_type(type);
2642 
2643   // Figure out the memory ordering.
2644   decorators |= mo_decorator_for_access_kind(kind);
2645 
<span class="line-modified">2646   if (!is_store) {</span>
<span class="line-modified">2647     if (type == T_OBJECT) {</span>
<span class="line-modified">2648       const TypeOopPtr* tjp = sharpen_unsafe_type(alias_type, adr_type);</span>
<span class="line-modified">2649       if (tjp != NULL) {</span>
<span class="line-added">2650         value_type = tjp;</span>
<span class="line-added">2651       }</span>
<span class="line-added">2652     } else if (type == T_VALUETYPE) {</span>
<span class="line-added">2653       value_type = NULL;</span>
2654     }
2655   }
2656 




2657   // Heap pointers get a null-check from the interpreter,
2658   // as a courtesy.  However, this is not guaranteed by Unsafe,
2659   // and it is not possible to fully distinguish unintended nulls
2660   // from intended ones in this API.
2661 
2662   if (!is_store) {
2663     Node* p = NULL;
2664     // Try to constant fold a load from a constant field
<span class="line-modified">2665 </span>
2666     if (heap_base_oop != top() &amp;&amp; field != NULL &amp;&amp; field-&gt;is_constant() &amp;&amp; !mismatched) {
2667       // final or stable field
2668       p = make_constant_from_field(field, heap_base_oop);
2669     }
2670 
2671     if (p == NULL) { // Could not constant fold the load
<span class="line-modified">2672       if (type == T_VALUETYPE) {</span>
<span class="line-added">2673         if (adr_type-&gt;isa_instptr() &amp;&amp; !mismatched) {</span>
<span class="line-added">2674           ciInstanceKlass* holder = adr_type-&gt;is_instptr()-&gt;klass()-&gt;as_instance_klass();</span>
<span class="line-added">2675           int offset = adr_type-&gt;is_instptr()-&gt;offset();</span>
<span class="line-added">2676           p = ValueTypeNode::make_from_flattened(this, value_klass, base, base, holder, offset, decorators);</span>
<span class="line-added">2677         } else {</span>
<span class="line-added">2678           p = ValueTypeNode::make_from_flattened(this, value_klass, base, adr, NULL, 0, decorators);</span>
<span class="line-added">2679         }</span>
<span class="line-added">2680       } else {</span>
<span class="line-added">2681         p = access_load_at(heap_base_oop, adr, adr_type, value_type, type, decorators);</span>
<span class="line-added">2682       }</span>
2683       // Normalize the value returned by getBoolean in the following cases
2684       if (type == T_BOOLEAN &amp;&amp;
2685           (mismatched ||
2686            heap_base_oop == top() ||                  // - heap_base_oop is NULL or
2687            (can_access_non_heap &amp;&amp; field == NULL))    // - heap_base_oop is potentially NULL
2688                                                       //   and the unsafe access is made to large offset
2689                                                       //   (i.e., larger than the maximum offset necessary for any
2690                                                       //   field access)
2691             ) {
2692           IdealKit ideal = IdealKit(this);
2693 #define __ ideal.
2694           IdealVariable normalized_result(ideal);
2695           __ declarations_done();
2696           __ set(normalized_result, p);
2697           __ if_then(p, BoolTest::ne, ideal.ConI(0));
2698           __ set(normalized_result, ideal.ConI(1));
2699           ideal.end_if();
2700           final_sync(ideal);
2701           p = __ value(normalized_result);
2702 #undef __
2703       }
2704     }
2705     if (type == T_ADDRESS) {
2706       p = gvn().transform(new CastP2XNode(NULL, p));
2707       p = ConvX2UL(p);
2708     }
<span class="line-added">2709     if (field != NULL &amp;&amp; field-&gt;is_flattenable() &amp;&amp; !field-&gt;is_flattened()) {</span>
<span class="line-added">2710       // Load a non-flattened but flattenable value type from memory</span>
<span class="line-added">2711       if (value_type-&gt;value_klass()-&gt;is_scalarizable()) {</span>
<span class="line-added">2712         p = ValueTypeNode::make_from_oop(this, p, value_type-&gt;value_klass());</span>
<span class="line-added">2713       } else {</span>
<span class="line-added">2714         p = null2default(p, value_type-&gt;value_klass());</span>
<span class="line-added">2715       }</span>
<span class="line-added">2716     }</span>
2717     // The load node has the control of the preceding MemBarCPUOrder.  All
2718     // following nodes will have the control of the MemBarCPUOrder inserted at
2719     // the end of this method.  So, pushing the load onto the stack at a later
2720     // point is fine.
2721     set_result(p);
2722   } else {
2723     if (bt == T_ADDRESS) {
2724       // Repackage the long as a pointer.
2725       val = ConvL2X(val);
2726       val = gvn().transform(new CastX2PNode(val));
2727     }
<span class="line-modified">2728     if (type == T_VALUETYPE) {</span>
<span class="line-added">2729       if (adr_type-&gt;isa_instptr() &amp;&amp; !mismatched) {</span>
<span class="line-added">2730         ciInstanceKlass* holder = adr_type-&gt;is_instptr()-&gt;klass()-&gt;as_instance_klass();</span>
<span class="line-added">2731         int offset = adr_type-&gt;is_instptr()-&gt;offset();</span>
<span class="line-added">2732         val-&gt;as_ValueType()-&gt;store_flattened(this, base, base, holder, offset, decorators);</span>
<span class="line-added">2733       } else {</span>
<span class="line-added">2734         val-&gt;as_ValueType()-&gt;store_flattened(this, base, adr, NULL, 0, decorators);</span>
<span class="line-added">2735       }</span>
<span class="line-added">2736     } else {</span>
<span class="line-added">2737       access_store_at(heap_base_oop, adr, adr_type, val, value_type, type, decorators);</span>
<span class="line-added">2738     }</span>
<span class="line-added">2739   }</span>
<span class="line-added">2740 </span>
<span class="line-added">2741   if (argument(1)-&gt;is_ValueType() &amp;&amp; is_store) {</span>
<span class="line-added">2742     Node* value = ValueTypeNode::make_from_oop(this, base, _gvn.type(base)-&gt;value_klass());</span>
<span class="line-added">2743     value = value-&gt;as_ValueType()-&gt;make_larval(this, false);</span>
<span class="line-added">2744     replace_in_map(argument(1), value);</span>
<span class="line-added">2745   }</span>
<span class="line-added">2746 </span>
<span class="line-added">2747   return true;</span>
<span class="line-added">2748 }</span>
<span class="line-added">2749 </span>
<span class="line-added">2750 bool LibraryCallKit::inline_unsafe_make_private_buffer() {</span>
<span class="line-added">2751   Node* receiver = argument(0);</span>
<span class="line-added">2752   Node* value = argument(1);</span>
<span class="line-added">2753 </span>
<span class="line-added">2754   receiver = null_check(receiver);</span>
<span class="line-added">2755   if (stopped()) {</span>
<span class="line-added">2756     return true;</span>
<span class="line-added">2757   }</span>
<span class="line-added">2758 </span>
<span class="line-added">2759   if (!value-&gt;is_ValueType()) {</span>
<span class="line-added">2760     return false;</span>
2761   }
2762 
<span class="line-added">2763   set_result(value-&gt;as_ValueType()-&gt;make_larval(this, true));</span>
<span class="line-added">2764 </span>
<span class="line-added">2765   return true;</span>
<span class="line-added">2766 }</span>
<span class="line-added">2767 </span>
<span class="line-added">2768 bool LibraryCallKit::inline_unsafe_finish_private_buffer() {</span>
<span class="line-added">2769   Node* receiver = argument(0);</span>
<span class="line-added">2770   Node* buffer = argument(1);</span>
<span class="line-added">2771 </span>
<span class="line-added">2772   receiver = null_check(receiver);</span>
<span class="line-added">2773   if (stopped()) {</span>
<span class="line-added">2774     return true;</span>
<span class="line-added">2775   }</span>
<span class="line-added">2776 </span>
<span class="line-added">2777   if (!buffer-&gt;is_ValueType()) {</span>
<span class="line-added">2778     return false;</span>
<span class="line-added">2779   }</span>
<span class="line-added">2780 </span>
<span class="line-added">2781   ValueTypeNode* vt = buffer-&gt;as_ValueType();</span>
<span class="line-added">2782   if (!vt-&gt;is_allocated(&amp;_gvn) || !_gvn.type(vt)-&gt;is_valuetype()-&gt;larval()) {</span>
<span class="line-added">2783     return false;</span>
<span class="line-added">2784   }</span>
<span class="line-added">2785 </span>
<span class="line-added">2786   set_result(vt-&gt;finish_larval(this));</span>
<span class="line-added">2787 </span>
2788   return true;
2789 }
2790 
2791 //----------------------------inline_unsafe_load_store----------------------------
2792 // This method serves a couple of different customers (depending on LoadStoreKind):
2793 //
2794 // LS_cmp_swap:
2795 //
2796 //   boolean compareAndSetReference(Object o, long offset, Object expected, Object x);
2797 //   boolean compareAndSetInt(   Object o, long offset, int    expected, int    x);
2798 //   boolean compareAndSetLong(  Object o, long offset, long   expected, long   x);
2799 //
2800 // LS_cmp_swap_weak:
2801 //
2802 //   boolean weakCompareAndSetReference(       Object o, long offset, Object expected, Object x);
2803 //   boolean weakCompareAndSetReferencePlain(  Object o, long offset, Object expected, Object x);
2804 //   boolean weakCompareAndSetReferenceAcquire(Object o, long offset, Object expected, Object x);
2805 //   boolean weakCompareAndSetReferenceRelease(Object o, long offset, Object expected, Object x);
2806 //
2807 //   boolean weakCompareAndSetInt(          Object o, long offset, int    expected, int    x);
</pre>
<hr />
<pre>
3230   set_control(jobj_is_not_null);
3231   Node* res = access_load(jobj, TypeInstPtr::NOTNULL, T_OBJECT,
3232                           IN_NATIVE | C2_CONTROL_DEPENDENT_LOAD);
3233   result_rgn-&gt;init_req(_normal_path, control());
3234   result_val-&gt;init_req(_normal_path, res);
3235 
3236   set_result(result_rgn, result_val);
3237 
3238   return true;
3239 }
3240 
3241 #endif // JFR_HAVE_INTRINSICS
3242 
3243 //------------------------inline_native_currentThread------------------
3244 bool LibraryCallKit::inline_native_currentThread() {
3245   Node* junk = NULL;
3246   set_result(generate_current_thread(junk));
3247   return true;
3248 }
3249 









3250 //-----------------------load_klass_from_mirror_common-------------------------
3251 // Given a java mirror (a java.lang.Class oop), load its corresponding klass oop.
3252 // Test the klass oop for null (signifying a primitive Class like Integer.TYPE),
3253 // and branch to the given path on the region.
3254 // If never_see_null, take an uncommon trap on null, so we can optimistically
3255 // compile for the non-null case.
3256 // If the region is NULL, force never_see_null = true.
3257 Node* LibraryCallKit::load_klass_from_mirror_common(Node* mirror,
3258                                                     bool never_see_null,
3259                                                     RegionNode* region,
3260                                                     int null_path,
3261                                                     int offset) {
3262   if (region == NULL)  never_see_null = true;
3263   Node* p = basic_plus_adr(mirror, offset);
3264   const TypeKlassPtr*  kls_type = TypeKlassPtr::OBJECT_OR_NULL;
3265   Node* kls = _gvn.transform(LoadKlassNode::make(_gvn, NULL, immutable_memory(), p, TypeRawPtr::BOTTOM, kls_type));
3266   Node* null_ctl = top();
3267   kls = null_check_oop(kls, &amp;null_ctl, never_see_null);
3268   if (region != NULL) {
3269     // Set region-&gt;in(null_path) if the mirror is a primitive (e.g, int.class).
</pre>
<hr />
<pre>
3272     assert(null_ctl == top(), &quot;no loose ends&quot;);
3273   }
3274   return kls;
3275 }
3276 
3277 //--------------------(inline_native_Class_query helpers)---------------------
3278 // Use this for JVM_ACC_INTERFACE, JVM_ACC_IS_CLONEABLE_FAST, JVM_ACC_HAS_FINALIZER.
3279 // Fall through if (mods &amp; mask) == bits, take the guard otherwise.
3280 Node* LibraryCallKit::generate_access_flags_guard(Node* kls, int modifier_mask, int modifier_bits, RegionNode* region) {
3281   // Branch around if the given klass has the given modifier bit set.
3282   // Like generate_guard, adds a new path onto the region.
3283   Node* modp = basic_plus_adr(kls, in_bytes(Klass::access_flags_offset()));
3284   Node* mods = make_load(NULL, modp, TypeInt::INT, T_INT, MemNode::unordered);
3285   Node* mask = intcon(modifier_mask);
3286   Node* bits = intcon(modifier_bits);
3287   Node* mbit = _gvn.transform(new AndINode(mods, mask));
3288   Node* cmp  = _gvn.transform(new CmpINode(mbit, bits));
3289   Node* bol  = _gvn.transform(new BoolNode(cmp, BoolTest::ne));
3290   return generate_fair_guard(bol, region);
3291 }
<span class="line-added">3292 </span>
3293 Node* LibraryCallKit::generate_interface_guard(Node* kls, RegionNode* region) {
3294   return generate_access_flags_guard(kls, JVM_ACC_INTERFACE, 0, region);
3295 }
3296 Node* LibraryCallKit::generate_hidden_class_guard(Node* kls, RegionNode* region) {
3297   return generate_access_flags_guard(kls, JVM_ACC_IS_HIDDEN_CLASS, 0, region);
3298 }
3299 
<span class="line-added">3300 Node* LibraryCallKit::generate_value_guard(Node* kls, RegionNode* region) {</span>
<span class="line-added">3301   return generate_access_flags_guard(kls, JVM_ACC_VALUE, 0, region);</span>
<span class="line-added">3302 }</span>
<span class="line-added">3303 </span>
3304 //-------------------------inline_native_Class_query-------------------
3305 bool LibraryCallKit::inline_native_Class_query(vmIntrinsics::ID id) {
3306   const Type* return_type = TypeInt::BOOL;
3307   Node* prim_return_value = top();  // what happens if it&#39;s a primitive class?
3308   bool never_see_null = !too_many_traps(Deoptimization::Reason_null_check);
3309   bool expect_prim = false;     // most of these guys expect to work on refs
3310 
3311   enum { _normal_path = 1, _prim_path = 2, PATH_LIMIT };
3312 
3313   Node* mirror = argument(0);
3314   Node* obj    = top();
3315 
3316   switch (id) {
3317   case vmIntrinsics::_isInstance:
3318     // nothing is an instance of a primitive type
3319     prim_return_value = intcon(0);
3320     obj = argument(1);
3321     break;
3322   case vmIntrinsics::_getModifiers:
3323     prim_return_value = intcon(JVM_ACC_ABSTRACT | JVM_ACC_FINAL | JVM_ACC_PUBLIC);
</pre>
<hr />
<pre>
3481   // Fall-through is the normal case of a query to a real class.
3482   phi-&gt;init_req(1, query_value);
3483   region-&gt;init_req(1, control());
3484 
3485   C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
3486   set_result(region, phi);
3487   return true;
3488 }
3489 
3490 //-------------------------inline_Class_cast-------------------
3491 bool LibraryCallKit::inline_Class_cast() {
3492   Node* mirror = argument(0); // Class
3493   Node* obj    = argument(1);
3494   const TypeInstPtr* mirror_con = _gvn.type(mirror)-&gt;isa_instptr();
3495   if (mirror_con == NULL) {
3496     return false;  // dead path (mirror-&gt;is_top()).
3497   }
3498   if (obj == NULL || obj-&gt;is_top()) {
3499     return false;  // dead path
3500   }
<span class="line-modified">3501   ciKlass* obj_klass = NULL;</span>
<span class="line-added">3502   const Type* obj_t = _gvn.type(obj);</span>
<span class="line-added">3503   if (obj-&gt;is_ValueType()) {</span>
<span class="line-added">3504     obj_klass = obj_t-&gt;value_klass();</span>
<span class="line-added">3505   } else if (obj_t-&gt;isa_oopptr()) {</span>
<span class="line-added">3506     obj_klass = obj_t-&gt;is_oopptr()-&gt;klass();</span>
<span class="line-added">3507   }</span>
3508 
3509   // First, see if Class.cast() can be folded statically.
3510   // java_mirror_type() returns non-null for compile-time Class constants.
3511   ciType* tm = mirror_con-&gt;java_mirror_type();
<span class="line-modified">3512   if (tm != NULL &amp;&amp; tm-&gt;is_klass() &amp;&amp; obj_klass != NULL) {</span>
<span class="line-modified">3513     if (!obj_klass-&gt;is_loaded()) {</span>

3514       // Don&#39;t use intrinsic when class is not loaded.
3515       return false;
3516     } else {
<span class="line-modified">3517       if (!obj-&gt;is_ValueType() &amp;&amp; tm-&gt;as_klass()-&gt;is_valuetype()) {</span>
<span class="line-added">3518         // Casting to .val, check for null</span>
<span class="line-added">3519         obj = null_check(obj);</span>
<span class="line-added">3520         if (stopped()) {</span>
<span class="line-added">3521           return true;</span>
<span class="line-added">3522         }</span>
<span class="line-added">3523       }</span>
<span class="line-added">3524       int static_res = C-&gt;static_subtype_check(tm-&gt;as_klass(), obj_klass);</span>
3525       if (static_res == Compile::SSC_always_true) {
3526         // isInstance() is true - fold the code.
3527         set_result(obj);
3528         return true;
3529       } else if (static_res == Compile::SSC_always_false) {
3530         // Don&#39;t use intrinsic, have to throw ClassCastException.
3531         // If the reference is null, the non-intrinsic bytecode will
3532         // be optimized appropriately.
3533         return false;
3534       }
3535     }
3536   }
3537 
3538   // Bailout intrinsic and do normal inlining if exception path is frequent.
3539   if (too_many_traps(Deoptimization::Reason_intrinsic)) {
3540     return false;
3541   }
3542 
3543   // Generate dynamic checks.
3544   // Class.cast() is java implementation of _checkcast bytecode.
3545   // Do checkcast (Parse::do_checkcast()) optimizations here.
3546 
3547   mirror = null_check(mirror);
3548   // If mirror is dead, only null-path is taken.
3549   if (stopped()) {
3550     return true;
3551   }
3552 
3553   // Not-subtype or the mirror&#39;s klass ptr is NULL (in case it is a primitive).
<span class="line-modified">3554   enum { _bad_type_path = 1, _prim_path = 2, _npe_path = 3, PATH_LIMIT };</span>
3555   RegionNode* region = new RegionNode(PATH_LIMIT);
3556   record_for_igvn(region);
3557 
3558   // Now load the mirror&#39;s klass metaobject, and null-check it.
3559   // If kls is null, we have a primitive mirror and
3560   // nothing is an instance of a primitive type.
3561   Node* kls = load_klass_from_mirror(mirror, false, region, _prim_path);
3562 
3563   Node* res = top();
3564   if (!stopped()) {
<span class="line-added">3565     if (EnableValhalla &amp;&amp; !obj-&gt;is_ValueType()) {</span>
<span class="line-added">3566       // Check if we are casting to .val</span>
<span class="line-added">3567       Node* is_val_kls = generate_value_guard(kls, NULL);</span>
<span class="line-added">3568       if (is_val_kls != NULL) {</span>
<span class="line-added">3569         RegionNode* r = new RegionNode(3);</span>
<span class="line-added">3570         record_for_igvn(r);</span>
<span class="line-added">3571         r-&gt;init_req(1, control());</span>
<span class="line-added">3572 </span>
<span class="line-added">3573         // Casting to .val, check for null</span>
<span class="line-added">3574         set_control(is_val_kls);</span>
<span class="line-added">3575         Node* null_ctr = top();</span>
<span class="line-added">3576         null_check_oop(obj, &amp;null_ctr);</span>
<span class="line-added">3577         region-&gt;init_req(_npe_path, null_ctr);</span>
<span class="line-added">3578         r-&gt;init_req(2, control());</span>
<span class="line-added">3579 </span>
<span class="line-added">3580         set_control(_gvn.transform(r));</span>
<span class="line-added">3581       }</span>
<span class="line-added">3582     }</span>
<span class="line-added">3583 </span>
3584     Node* bad_type_ctrl = top();
3585     // Do checkcast optimizations.
3586     res = gen_checkcast(obj, kls, &amp;bad_type_ctrl);
3587     region-&gt;init_req(_bad_type_path, bad_type_ctrl);
3588   }
3589   if (region-&gt;in(_prim_path) != top() ||
<span class="line-modified">3590       region-&gt;in(_bad_type_path) != top() ||</span>
<span class="line-added">3591       region-&gt;in(_npe_path) != top()) {</span>
3592     // Let Interpreter throw ClassCastException.
3593     PreserveJVMState pjvms(this);
3594     set_control(_gvn.transform(region));
3595     uncommon_trap(Deoptimization::Reason_intrinsic,
3596                   Deoptimization::Action_maybe_recompile);
3597   }
3598   if (!stopped()) {
3599     set_result(res);
3600   }
3601   return true;
3602 }
3603 
3604 
3605 //--------------------------inline_native_subtype_check------------------------
3606 // This intrinsic takes the JNI calls out of the heart of
3607 // UnsafeFieldAccessorImpl.set, which improves Field.set, readObject, etc.
3608 bool LibraryCallKit::inline_native_subtype_check() {
3609   // Pull both arguments off the stack.
3610   Node* args[2];                // two java.lang.Class mirrors: superc, subc
3611   args[0] = argument(0);
3612   args[1] = argument(1);
3613   Node* klasses[2];             // corresponding Klasses: superk, subk
3614   klasses[0] = klasses[1] = top();
3615 
3616   enum {
3617     // A full decision tree on {superc is prim, subc is prim}:
3618     _prim_0_path = 1,           // {P,N} =&gt; false
3619                                 // {P,P} &amp; superc!=subc =&gt; false
3620     _prim_same_path,            // {P,P} &amp; superc==subc =&gt; true
3621     _prim_1_path,               // {N,P} =&gt; false
3622     _ref_subtype_path,          // {N,N} &amp; subtype check wins =&gt; true
3623     _both_ref_path,             // {N,N} &amp; subtype check loses =&gt; false
3624     PATH_LIMIT
3625   };
3626 
3627   RegionNode* region = new RegionNode(PATH_LIMIT);
<span class="line-added">3628   RegionNode* prim_region = new RegionNode(2);</span>
3629   Node*       phi    = new PhiNode(region, TypeInt::BOOL);
3630   record_for_igvn(region);
<span class="line-added">3631   record_for_igvn(prim_region);</span>
3632 
3633   const TypePtr* adr_type = TypeRawPtr::BOTTOM;   // memory type of loads
3634   const TypeKlassPtr* kls_type = TypeKlassPtr::OBJECT_OR_NULL;
3635   int class_klass_offset = java_lang_Class::klass_offset_in_bytes();
3636 
3637   // First null-check both mirrors and load each mirror&#39;s klass metaobject.
3638   int which_arg;
3639   for (which_arg = 0; which_arg &lt;= 1; which_arg++) {
3640     Node* arg = args[which_arg];
3641     arg = null_check(arg);
3642     if (stopped())  break;
3643     args[which_arg] = arg;
3644 
3645     Node* p = basic_plus_adr(arg, class_klass_offset);
3646     Node* kls = LoadKlassNode::make(_gvn, NULL, immutable_memory(), p, adr_type, kls_type);
3647     klasses[which_arg] = _gvn.transform(kls);
3648   }
3649 
3650   // Having loaded both klasses, test each for null.
3651   bool never_see_null = !too_many_traps(Deoptimization::Reason_null_check);
3652   for (which_arg = 0; which_arg &lt;= 1; which_arg++) {
3653     Node* kls = klasses[which_arg];
3654     Node* null_ctl = top();
3655     kls = null_check_oop(kls, &amp;null_ctl, never_see_null);
<span class="line-modified">3656     if (which_arg == 0) {</span>
<span class="line-modified">3657       prim_region-&gt;init_req(1, null_ctl);</span>
<span class="line-added">3658     } else {</span>
<span class="line-added">3659       region-&gt;init_req(_prim_1_path, null_ctl);</span>
<span class="line-added">3660     }</span>
3661     if (stopped())  break;
3662     klasses[which_arg] = kls;
3663   }
3664 
3665   if (!stopped()) {
3666     // now we have two reference types, in klasses[0..1]
3667     Node* subk   = klasses[1];  // the argument to isAssignableFrom
3668     Node* superk = klasses[0];  // the receiver
3669     region-&gt;set_req(_both_ref_path, gen_subtype_check(subk, superk));
3670     // now we have a successful reference subtype check
3671     region-&gt;set_req(_ref_subtype_path, control());
3672   }
3673 
3674   // If both operands are primitive (both klasses null), then
3675   // we must return true when they are identical primitives.
3676   // It is convenient to test this after the first null klass check.
<span class="line-modified">3677   // This path is also used if superc is a value mirror.</span>
<span class="line-added">3678   set_control(_gvn.transform(prim_region));</span>
3679   if (!stopped()) {
3680     // Since superc is primitive, make a guard for the superc==subc case.
3681     Node* cmp_eq = _gvn.transform(new CmpPNode(args[0], args[1]));
3682     Node* bol_eq = _gvn.transform(new BoolNode(cmp_eq, BoolTest::eq));
<span class="line-modified">3683     generate_fair_guard(bol_eq, region);</span>
3684     if (region-&gt;req() == PATH_LIMIT+1) {
3685       // A guard was added.  If the added guard is taken, superc==subc.
3686       region-&gt;swap_edges(PATH_LIMIT, _prim_same_path);
3687       region-&gt;del_req(PATH_LIMIT);
3688     }
3689     region-&gt;set_req(_prim_0_path, control()); // Not equal after all.
3690   }
3691 
3692   // these are the only paths that produce &#39;true&#39;:
3693   phi-&gt;set_req(_prim_same_path,   intcon(1));
3694   phi-&gt;set_req(_ref_subtype_path, intcon(1));
3695 
3696   // pull together the cases:
3697   assert(region-&gt;req() == PATH_LIMIT, &quot;sane region&quot;);
3698   for (uint i = 1; i &lt; region-&gt;req(); i++) {
3699     Node* ctl = region-&gt;in(i);
3700     if (ctl == NULL || ctl == top()) {
3701       region-&gt;set_req(i, top());
3702       phi   -&gt;set_req(i, top());
3703     } else if (phi-&gt;in(i) == NULL) {
3704       phi-&gt;set_req(i, intcon(0)); // all other paths produce &#39;false&#39;
3705     }
3706   }
3707 
3708   set_control(_gvn.transform(region));
3709   set_result(_gvn.transform(phi));
3710   return true;
3711 }
3712 
3713 //---------------------generate_array_guard_common------------------------
<span class="line-modified">3714 Node* LibraryCallKit::generate_array_guard_common(Node* kls, RegionNode* region, ArrayKind kind) {</span>

3715 
3716   if (stopped()) {
3717     return NULL;
3718   }
3719 









3720   // Like generate_guard, adds a new path onto the region.
3721   jint  layout_con = 0;
3722   Node* layout_val = get_layout_helper(kls, layout_con);
3723   if (layout_val == NULL) {
<span class="line-modified">3724     bool query = 0;</span>
<span class="line-modified">3725     switch(kind) {</span>
<span class="line-modified">3726       case ObjectArray:    query = Klass::layout_helper_is_objArray(layout_con); break;</span>
<span class="line-modified">3727       case NonObjectArray: query = !Klass::layout_helper_is_objArray(layout_con); break;</span>
<span class="line-added">3728       case TypeArray:      query = Klass::layout_helper_is_typeArray(layout_con); break;</span>
<span class="line-added">3729       case ValueArray:     query = Klass::layout_helper_is_valueArray(layout_con); break;</span>
<span class="line-added">3730       case AnyArray:       query = Klass::layout_helper_is_array(layout_con); break;</span>
<span class="line-added">3731       case NonArray:       query = !Klass::layout_helper_is_array(layout_con); break;</span>
<span class="line-added">3732       default:</span>
<span class="line-added">3733         ShouldNotReachHere();</span>
<span class="line-added">3734     }</span>
<span class="line-added">3735     if (!query) {</span>
3736       return NULL;                       // never a branch
3737     } else {                             // always a branch
3738       Node* always_branch = control();
3739       if (region != NULL)
3740         region-&gt;add_req(always_branch);
3741       set_control(top());
3742       return always_branch;
3743     }
3744   }
<span class="line-added">3745   unsigned int value = 0;</span>
<span class="line-added">3746   BoolTest::mask btest = BoolTest::illegal;</span>
<span class="line-added">3747   switch(kind) {</span>
<span class="line-added">3748     case ObjectArray:</span>
<span class="line-added">3749     case NonObjectArray: {</span>
<span class="line-added">3750       value = Klass::_lh_array_tag_obj_value;</span>
<span class="line-added">3751       layout_val = _gvn.transform(new RShiftINode(layout_val, intcon(Klass::_lh_array_tag_shift)));</span>
<span class="line-added">3752       btest = kind == ObjectArray ? BoolTest::eq : BoolTest::ne;</span>
<span class="line-added">3753       break;</span>
<span class="line-added">3754     }</span>
<span class="line-added">3755     case TypeArray: {</span>
<span class="line-added">3756       value = Klass::_lh_array_tag_type_value;</span>
<span class="line-added">3757       layout_val = _gvn.transform(new RShiftINode(layout_val, intcon(Klass::_lh_array_tag_shift)));</span>
<span class="line-added">3758       btest = BoolTest::eq;</span>
<span class="line-added">3759       break;</span>
<span class="line-added">3760     }</span>
<span class="line-added">3761     case ValueArray: {</span>
<span class="line-added">3762       value = Klass::_lh_array_tag_vt_value;</span>
<span class="line-added">3763       layout_val = _gvn.transform(new RShiftINode(layout_val, intcon(Klass::_lh_array_tag_shift)));</span>
<span class="line-added">3764       btest = BoolTest::eq;</span>
<span class="line-added">3765       break;</span>
<span class="line-added">3766     }</span>
<span class="line-added">3767     case AnyArray:    value = Klass::_lh_neutral_value; btest = BoolTest::lt; break;</span>
<span class="line-added">3768     case NonArray:    value = Klass::_lh_neutral_value; btest = BoolTest::gt; break;</span>
<span class="line-added">3769     default:</span>
<span class="line-added">3770       ShouldNotReachHere();</span>
<span class="line-added">3771   }</span>
3772   // Now test the correct condition.
<span class="line-modified">3773   jint nval = (jint)value;</span>



3774   Node* cmp = _gvn.transform(new CmpINode(layout_val, intcon(nval)));



3775   Node* bol = _gvn.transform(new BoolNode(cmp, btest));
3776   return generate_fair_guard(bol, region);
3777 }
3778 
3779 
3780 //-----------------------inline_native_newArray--------------------------
<span class="line-modified">3781 // private static native Object java.lang.reflect.Array.newArray(Class&lt;?&gt; componentType, int length);</span>
3782 // private        native Object Unsafe.allocateUninitializedArray0(Class&lt;?&gt; cls, int size);
3783 bool LibraryCallKit::inline_unsafe_newArray(bool uninitialized) {
3784   Node* mirror;
3785   Node* count_val;
3786   if (uninitialized) {
3787     mirror    = argument(1);
3788     count_val = argument(2);
3789   } else {
3790     mirror    = argument(0);
3791     count_val = argument(1);
3792   }
3793 
3794   mirror = null_check(mirror);
3795   // If mirror or obj is dead, only null-path is taken.
3796   if (stopped())  return true;
3797 
3798   enum { _normal_path = 1, _slow_path = 2, PATH_LIMIT };
3799   RegionNode* result_reg = new RegionNode(PATH_LIMIT);
3800   PhiNode*    result_val = new PhiNode(result_reg, TypeInstPtr::NOTNULL);
3801   PhiNode*    result_io  = new PhiNode(result_reg, Type::ABIO);
</pre>
<hr />
<pre>
3877   // It could be a dynamic mix of int[], boolean[], Object[], etc.
3878   Node* result = load_array_length(array);
3879 
3880   C-&gt;set_has_split_ifs(true);  // Has chance for split-if optimization
3881   set_result(result);
3882   return true;
3883 }
3884 
3885 //------------------------inline_array_copyOf----------------------------
3886 // public static &lt;T,U&gt; T[] java.util.Arrays.copyOf(     U[] original, int newLength,         Class&lt;? extends T[]&gt; newType);
3887 // public static &lt;T,U&gt; T[] java.util.Arrays.copyOfRange(U[] original, int from,      int to, Class&lt;? extends T[]&gt; newType);
3888 bool LibraryCallKit::inline_array_copyOf(bool is_copyOfRange) {
3889   if (too_many_traps(Deoptimization::Reason_intrinsic))  return false;
3890 
3891   // Get the arguments.
3892   Node* original          = argument(0);
3893   Node* start             = is_copyOfRange? argument(1): intcon(0);
3894   Node* end               = is_copyOfRange? argument(2): argument(1);
3895   Node* array_type_mirror = is_copyOfRange? argument(3): argument(2);
3896 
<span class="line-added">3897   const TypeAryPtr* original_t = _gvn.type(original)-&gt;isa_aryptr();</span>
<span class="line-added">3898   const TypeInstPtr* mirror_t = _gvn.type(array_type_mirror)-&gt;isa_instptr();</span>
<span class="line-added">3899   if (EnableValhalla &amp;&amp; ValueArrayFlatten &amp;&amp;</span>
<span class="line-added">3900       (original_t == NULL || mirror_t == NULL ||</span>
<span class="line-added">3901        (mirror_t-&gt;java_mirror_type() == NULL &amp;&amp;</span>
<span class="line-added">3902         (original_t-&gt;elem()-&gt;isa_valuetype() ||</span>
<span class="line-added">3903          (original_t-&gt;elem()-&gt;make_oopptr() != NULL &amp;&amp;</span>
<span class="line-added">3904           original_t-&gt;elem()-&gt;make_oopptr()-&gt;can_be_value_type()))))) {</span>
<span class="line-added">3905     // We need to know statically if the copy is to a flattened array</span>
<span class="line-added">3906     // or not but can&#39;t tell.</span>
<span class="line-added">3907     return false;</span>
<span class="line-added">3908   }</span>
<span class="line-added">3909 </span>
3910   Node* newcopy = NULL;
3911 
3912   // Set the original stack and the reexecute bit for the interpreter to reexecute
3913   // the bytecode that invokes Arrays.copyOf if deoptimization happens.
3914   { PreserveReexecuteState preexecs(this);
3915     jvms()-&gt;set_should_reexecute(true);
3916 
3917     array_type_mirror = null_check(array_type_mirror);
3918     original          = null_check(original);
3919 
3920     // Check if a null path was taken unconditionally.
3921     if (stopped())  return true;
3922 
3923     Node* orig_length = load_array_length(original);
3924 
3925     Node* klass_node = load_klass_from_mirror(array_type_mirror, false, NULL, 0);
3926     klass_node = null_check(klass_node);
3927 
3928     RegionNode* bailout = new RegionNode(1);
3929     record_for_igvn(bailout);
3930 
3931     // Despite the generic type of Arrays.copyOf, the mirror might be int, int[], etc.
3932     // Bail out if that is so.
<span class="line-modified">3933     // Value type array may have object field that would require a</span>
<span class="line-added">3934     // write barrier. Conservatively, go to slow path.</span>
<span class="line-added">3935     BarrierSetC2* bs = BarrierSet::barrier_set()-&gt;barrier_set_c2();</span>
<span class="line-added">3936     Node* not_objArray = !bs-&gt;array_copy_requires_gc_barriers(false, T_OBJECT, false, BarrierSetC2::Parsing) ?</span>
<span class="line-added">3937         generate_typeArray_guard(klass_node, bailout) : generate_non_objArray_guard(klass_node, bailout);</span>
3938     if (not_objArray != NULL) {
3939       // Improve the klass node&#39;s type from the new optimistic assumption:
3940       ciKlass* ak = ciArrayKlass::make(env()-&gt;Object_klass());
<span class="line-modified">3941       const Type* akls = TypeKlassPtr::make(TypePtr::NotNull, ak, Type::Offset(0), false);</span>
3942       Node* cast = new CastPPNode(klass_node, akls);
3943       cast-&gt;init_req(0, control());
3944       klass_node = _gvn.transform(cast);
3945     }
3946 
<span class="line-added">3947     Node* original_kls = load_object_klass(original);</span>
<span class="line-added">3948     // ArrayCopyNode:Ideal may transform the ArrayCopyNode to</span>
<span class="line-added">3949     // loads/stores but it is legal only if we&#39;re sure the</span>
<span class="line-added">3950     // Arrays.copyOf would succeed. So we need all input arguments</span>
<span class="line-added">3951     // to the copyOf to be validated, including that the copy to the</span>
<span class="line-added">3952     // new array won&#39;t trigger an ArrayStoreException. That subtype</span>
<span class="line-added">3953     // check can be optimized if we know something on the type of</span>
<span class="line-added">3954     // the input array from type speculation.</span>
<span class="line-added">3955     if (_gvn.type(klass_node)-&gt;singleton() &amp;&amp; !stopped()) {</span>
<span class="line-added">3956       ciKlass* subk   = _gvn.type(original_kls)-&gt;is_klassptr()-&gt;klass();</span>
<span class="line-added">3957       ciKlass* superk = _gvn.type(klass_node)-&gt;is_klassptr()-&gt;klass();</span>
<span class="line-added">3958 </span>
<span class="line-added">3959       int test = C-&gt;static_subtype_check(superk, subk);</span>
<span class="line-added">3960       if (test != Compile::SSC_always_true &amp;&amp; test != Compile::SSC_always_false) {</span>
<span class="line-added">3961         const TypeOopPtr* t_original = _gvn.type(original)-&gt;is_oopptr();</span>
<span class="line-added">3962         if (t_original-&gt;speculative_type() != NULL) {</span>
<span class="line-added">3963           original = maybe_cast_profiled_obj(original, t_original-&gt;speculative_type(), true);</span>
<span class="line-added">3964           original_kls = load_object_klass(original);</span>
<span class="line-added">3965         }</span>
<span class="line-added">3966       }</span>
<span class="line-added">3967     }</span>
<span class="line-added">3968 </span>
<span class="line-added">3969     if (ValueArrayFlatten) {</span>
<span class="line-added">3970       // Either both or neither new array klass and original array</span>
<span class="line-added">3971       // klass must be flattened</span>
<span class="line-added">3972       Node* is_flat = generate_valueArray_guard(klass_node, NULL);</span>
<span class="line-added">3973       if (!original_t-&gt;is_not_flat()) {</span>
<span class="line-added">3974         generate_valueArray_guard(original_kls, bailout);</span>
<span class="line-added">3975       }</span>
<span class="line-added">3976       if (is_flat != NULL) {</span>
<span class="line-added">3977         RegionNode* r = new RegionNode(2);</span>
<span class="line-added">3978         record_for_igvn(r);</span>
<span class="line-added">3979         r-&gt;init_req(1, control());</span>
<span class="line-added">3980         set_control(is_flat);</span>
<span class="line-added">3981         if (!original_t-&gt;is_not_flat()) {</span>
<span class="line-added">3982           generate_valueArray_guard(original_kls, r);</span>
<span class="line-added">3983         }</span>
<span class="line-added">3984         bailout-&gt;add_req(control());</span>
<span class="line-added">3985         set_control(_gvn.transform(r));</span>
<span class="line-added">3986       }</span>
<span class="line-added">3987     }</span>
<span class="line-added">3988 </span>
3989     // Bail out if either start or end is negative.
3990     generate_negative_guard(start, bailout, &amp;start);
3991     generate_negative_guard(end,   bailout, &amp;end);
3992 
3993     Node* length = end;
3994     if (_gvn.type(start) != TypeInt::ZERO) {
3995       length = _gvn.transform(new SubINode(end, start));
3996     }
3997 
3998     // Bail out if length is negative.
3999     // Without this the new_array would throw
4000     // NegativeArraySizeException but IllegalArgumentException is what
4001     // should be thrown
4002     generate_negative_guard(length, bailout, &amp;length);
4003 
4004     if (bailout-&gt;req() &gt; 1) {
4005       PreserveJVMState pjvms(this);
4006       set_control(_gvn.transform(bailout));
4007       uncommon_trap(Deoptimization::Reason_intrinsic,
4008                     Deoptimization::Action_maybe_recompile);
4009     }
4010 
4011     if (!stopped()) {
4012       // How many elements will we copy from the original?
4013       // The answer is MinI(orig_length - start, length).
4014       Node* orig_tail = _gvn.transform(new SubINode(orig_length, start));
4015       Node* moved = generate_min_max(vmIntrinsics::_min, orig_tail, length);
4016 
4017       // Generate a direct call to the right arraycopy function(s).
4018       // We know the copy is disjoint but we might not know if the
4019       // oop stores need checking.
4020       // Extreme case:  Arrays.copyOf((Integer[])x, 10, String[].class).
4021       // This will fail a store-check if x contains any non-nulls.
4022 




















4023       bool validated = false;
4024       // Reason_class_check rather than Reason_intrinsic because we
4025       // want to intrinsify even if this traps.
4026       if (!too_many_traps(Deoptimization::Reason_class_check)) {
4027         Node* not_subtype_ctrl = gen_subtype_check(original, klass_node);
4028 
4029         if (not_subtype_ctrl != top()) {
4030           PreserveJVMState pjvms(this);
4031           set_control(not_subtype_ctrl);
4032           uncommon_trap(Deoptimization::Reason_class_check,
4033                         Deoptimization::Action_make_not_entrant);
4034           assert(stopped(), &quot;Should be stopped&quot;);
4035         }
4036         validated = true;
4037       }
4038 
4039       if (!stopped()) {
4040         newcopy = new_array(klass_node, length, 0);  // no arguments to push
4041 
4042         ArrayCopyNode* ac = ArrayCopyNode::make(this, true, original, start, newcopy, intcon(0), moved, true, false,
<span class="line-modified">4043                                                 original_kls, klass_node);</span>
4044         if (!is_copyOfRange) {
4045           ac-&gt;set_copyof(validated);
4046         } else {
4047           ac-&gt;set_copyofrange(validated);
4048         }
4049         Node* n = _gvn.transform(ac);
4050         if (n == ac) {
4051           ac-&gt;connect_outputs(this);
4052         } else {
4053           assert(validated, &quot;shouldn&#39;t transform if all arguments not validated&quot;);
4054           set_all_memory(n);
4055         }
4056       }
4057     }
4058   } // original reexecute is set back here
4059 
4060   C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
4061   if (!stopped()) {
4062     set_result(newcopy);
4063   }
</pre>
<hr />
<pre>
4147   set_edges_for_java_call(slow_call);
4148   return slow_call;
4149 }
4150 
4151 
4152 /**
4153  * Build special case code for calls to hashCode on an object. This call may
4154  * be virtual (invokevirtual) or bound (invokespecial). For each case we generate
4155  * slightly different code.
4156  */
4157 bool LibraryCallKit::inline_native_hashcode(bool is_virtual, bool is_static) {
4158   assert(is_static == callee()-&gt;is_static(), &quot;correct intrinsic selection&quot;);
4159   assert(!(is_virtual &amp;&amp; is_static), &quot;either virtual, special, or static&quot;);
4160 
4161   enum { _slow_path = 1, _fast_path, _null_path, PATH_LIMIT };
4162 
4163   RegionNode* result_reg = new RegionNode(PATH_LIMIT);
4164   PhiNode*    result_val = new PhiNode(result_reg, TypeInt::INT);
4165   PhiNode*    result_io  = new PhiNode(result_reg, Type::ABIO);
4166   PhiNode*    result_mem = new PhiNode(result_reg, Type::MEMORY, TypePtr::BOTTOM);
<span class="line-modified">4167   Node* obj = argument(0);</span>
<span class="line-added">4168 </span>
<span class="line-added">4169   if (obj-&gt;is_ValueType() || gvn().type(obj)-&gt;is_valuetypeptr()) {</span>
<span class="line-added">4170     return false;</span>
<span class="line-added">4171   }</span>
<span class="line-added">4172 </span>
4173   if (!is_static) {
4174     // Check for hashing null object
4175     obj = null_check_receiver();
4176     if (stopped())  return true;        // unconditionally null
4177     result_reg-&gt;init_req(_null_path, top());
4178     result_val-&gt;init_req(_null_path, top());
4179   } else {
4180     // Do a null check, and return zero if null.
4181     // System.identityHashCode(null) == 0

4182     Node* null_ctl = top();
4183     obj = null_check_oop(obj, &amp;null_ctl);
4184     result_reg-&gt;init_req(_null_path, null_ctl);
4185     result_val-&gt;init_req(_null_path, _gvn.intcon(0));
4186   }
4187 
4188   // Unconditionally null?  Then return right away.
4189   if (stopped()) {
4190     set_control( result_reg-&gt;in(_null_path));
4191     if (!stopped())
4192       set_result(result_val-&gt;in(_null_path));
4193     return true;
4194   }
4195 
4196   // We only go to the fast case code if we pass a number of guards.  The
4197   // paths which do not pass are accumulated in the slow_region.
4198   RegionNode* slow_region = new RegionNode(1);
4199   record_for_igvn(slow_region);
4200 
4201   // If this is a virtual call, we generate a funny guard.  We pull out
4202   // the vtable entry corresponding to hashCode() from the target object.
4203   // If the target method which we are calling happens to be the native
4204   // Object hashCode() method, we pass the guard.  We do not need this
4205   // guard for non-virtual calls -- the caller is known to be the native
4206   // Object hashCode().
4207   if (is_virtual) {
4208     // After null check, get the object&#39;s klass.
4209     Node* obj_klass = load_object_klass(obj);
4210     generate_virtual_guard(obj_klass, slow_region);
4211   }
4212 
4213   // Get the header out of the object, use LoadMarkNode when available
4214   Node* header_addr = basic_plus_adr(obj, oopDesc::mark_offset_in_bytes());
4215   // The control of the load must be NULL. Otherwise, the load can move before
4216   // the null check after castPP removal.
4217   Node* no_ctrl = NULL;
4218   Node* header = make_load(no_ctrl, header_addr, TypeX_X, TypeX_X-&gt;basic_type(), MemNode::unordered);
4219 
4220   // Test the header to see if it is unlocked.
<span class="line-added">4221   // This also serves as guard against value types (they have the always_locked_pattern set).</span>
4222   Node *lock_mask      = _gvn.MakeConX(markWord::biased_lock_mask_in_place);
4223   Node *lmasked_header = _gvn.transform(new AndXNode(header, lock_mask));
4224   Node *unlocked_val   = _gvn.MakeConX(markWord::unlocked_value);
4225   Node *chk_unlocked   = _gvn.transform(new CmpXNode( lmasked_header, unlocked_val));
4226   Node *test_unlocked  = _gvn.transform(new BoolNode( chk_unlocked, BoolTest::ne));
4227 
4228   generate_slow_guard(test_unlocked, slow_region);
4229 
4230   // Get the hash value and check to see that it has been properly assigned.
4231   // We depend on hash_mask being at most 32 bits and avoid the use of
4232   // hash_mask_in_place because it could be larger than 32 bits in a 64-bit
4233   // vm: see markWord.hpp.
4234   Node *hash_mask      = _gvn.intcon(markWord::hash_mask);
4235   Node *hash_shift     = _gvn.intcon(markWord::hash_shift);
4236   Node *hshifted_header= _gvn.transform(new URShiftXNode(header, hash_shift));
4237   // This hack lets the hash bits live anywhere in the mark object now, as long
4238   // as the shift drops the relevant bits into the low 32 bits.  Note that
4239   // Java spec says that HashCode is an int so there&#39;s no point in capturing
4240   // an &#39;X&#39;-sized hashcode (32 in 32-bit build or 64 in 64-bit build).
4241   hshifted_header      = ConvX2I(hshifted_header);
</pre>
<hr />
<pre>
4268     // this-&gt;control() comes from set_results_for_java_call
4269     result_reg-&gt;init_req(_slow_path, control());
4270     result_val-&gt;init_req(_slow_path, slow_result);
4271     result_io  -&gt;set_req(_slow_path, i_o());
4272     result_mem -&gt;set_req(_slow_path, reset_memory());
4273   }
4274 
4275   // Return the combined state.
4276   set_i_o(        _gvn.transform(result_io)  );
4277   set_all_memory( _gvn.transform(result_mem));
4278 
4279   set_result(result_reg, result_val);
4280   return true;
4281 }
4282 
4283 //---------------------------inline_native_getClass----------------------------
4284 // public final native Class&lt;?&gt; java.lang.Object.getClass();
4285 //
4286 // Build special case code for calls to getClass on an object.
4287 bool LibraryCallKit::inline_native_getClass() {
<span class="line-modified">4288   Node* obj = argument(0);</span>
<span class="line-added">4289   if (obj-&gt;is_ValueType()) {</span>
<span class="line-added">4290     ciKlass* vk = _gvn.type(obj)-&gt;value_klass();</span>
<span class="line-added">4291     set_result(makecon(TypeInstPtr::make(vk-&gt;java_mirror())));</span>
<span class="line-added">4292     return true;</span>
<span class="line-added">4293   }</span>
<span class="line-added">4294   obj = null_check_receiver();</span>
4295   if (stopped())  return true;
4296   set_result(load_mirror_from_klass(load_object_klass(obj)));
4297   return true;
4298 }
4299 
4300 //-----------------inline_native_Reflection_getCallerClass---------------------
4301 // public static native Class&lt;?&gt; sun.reflect.Reflection.getCallerClass();
4302 //
4303 // In the presence of deep enough inlining, getCallerClass() becomes a no-op.
4304 //
4305 // NOTE: This code must perform the same logic as JVM_GetCallerClass
4306 // in that it must skip particular security frames and checks for
4307 // caller sensitive methods.
4308 bool LibraryCallKit::inline_native_Reflection_getCallerClass() {
4309 #ifndef PRODUCT
4310   if ((C-&gt;print_intrinsics() || C-&gt;print_inlining()) &amp;&amp; Verbose) {
4311     tty-&gt;print_cr(&quot;Attempting to inline sun.reflect.Reflection.getCallerClass&quot;);
4312   }
4313 #endif
4314 
</pre>
<hr />
<pre>
4536 // Helper function for inline_native_clone.
4537 void LibraryCallKit::copy_to_clone(Node* obj, Node* alloc_obj, Node* obj_size, bool is_array) {
4538   assert(obj_size != NULL, &quot;&quot;);
4539   Node* raw_obj = alloc_obj-&gt;in(1);
4540   assert(alloc_obj-&gt;is_CheckCastPP() &amp;&amp; raw_obj-&gt;is_Proj() &amp;&amp; raw_obj-&gt;in(0)-&gt;is_Allocate(), &quot;&quot;);
4541 
4542   AllocateNode* alloc = NULL;
4543   if (ReduceBulkZeroing) {
4544     // We will be completely responsible for initializing this object -
4545     // mark Initialize node as complete.
4546     alloc = AllocateNode::Ideal_allocation(alloc_obj, &amp;_gvn);
4547     // The object was just allocated - there should be no any stores!
4548     guarantee(alloc != NULL &amp;&amp; alloc-&gt;maybe_set_complete(&amp;_gvn), &quot;&quot;);
4549     // Mark as complete_with_arraycopy so that on AllocateNode
4550     // expansion, we know this AllocateNode is initialized by an array
4551     // copy and a StoreStore barrier exists after the array copy.
4552     alloc-&gt;initialization()-&gt;set_complete_with_arraycopy();
4553   }
4554 
4555   Node* size = _gvn.transform(obj_size);
<span class="line-modified">4556   // Exclude the header but include array length to copy by 8 bytes words.</span>
<span class="line-added">4557   // Can&#39;t use base_offset_in_bytes(bt) since basic type is unknown.</span>
<span class="line-added">4558   int base_off = BarrierSetC2::arraycopy_payload_base_offset(is_array);</span>
<span class="line-added">4559   Node* countx = size;</span>
<span class="line-added">4560   countx = _gvn.transform(new SubXNode(countx, MakeConX(base_off)));</span>
<span class="line-added">4561   countx = _gvn.transform(new URShiftXNode(countx, intcon(LogBytesPerLong)));</span>
<span class="line-added">4562 </span>
<span class="line-added">4563   access_clone(obj, alloc_obj, countx, is_array);</span>
4564 
4565   // Do not let reads from the cloned object float above the arraycopy.
4566   if (alloc != NULL) {
4567     // Do not let stores that initialize this object be reordered with
4568     // a subsequent store that would make this object accessible by
4569     // other threads.
4570     // Record what AllocateNode this StoreStore protects so that
4571     // escape analysis can go from the MemBarStoreStoreNode to the
4572     // AllocateNode and eliminate the MemBarStoreStoreNode if possible
4573     // based on the escape status of the AllocateNode.
4574     insert_mem_bar(Op_MemBarStoreStore, alloc-&gt;proj_out_or_null(AllocateNode::RawAddress));
4575   } else {
4576     insert_mem_bar(Op_MemBarCPUOrder);
4577   }
4578 }
4579 
4580 //------------------------inline_native_clone----------------------------
4581 // protected native Object java.lang.Object.clone();
4582 //
4583 // Here are the simple edge cases:
</pre>
<hr />
<pre>
4586 //  not cloneable or finalizer =&gt; slow path to out-of-line Object.clone
4587 //
4588 // The general case has two steps, allocation and copying.
4589 // Allocation has two cases, and uses GraphKit::new_instance or new_array.
4590 //
4591 // Copying also has two cases, oop arrays and everything else.
4592 // Oop arrays use arrayof_oop_arraycopy (same as System.arraycopy).
4593 // Everything else uses the tight inline loop supplied by CopyArrayNode.
4594 //
4595 // These steps fold up nicely if and when the cloned object&#39;s klass
4596 // can be sharply typed as an object array, a type array, or an instance.
4597 //
4598 bool LibraryCallKit::inline_native_clone(bool is_virtual) {
4599   PhiNode* result_val;
4600 
4601   // Set the reexecute bit for the interpreter to reexecute
4602   // the bytecode that invokes Object.clone if deoptimization happens.
4603   { PreserveReexecuteState preexecs(this);
4604     jvms()-&gt;set_should_reexecute(true);
4605 
<span class="line-modified">4606     Node* obj = argument(0);</span>
<span class="line-added">4607     if (obj-&gt;is_ValueType()) {</span>
<span class="line-added">4608       return false;</span>
<span class="line-added">4609     }</span>
<span class="line-added">4610 </span>
<span class="line-added">4611     obj = null_check_receiver();</span>
4612     if (stopped())  return true;
4613 
4614     const TypeOopPtr* obj_type = _gvn.type(obj)-&gt;is_oopptr();
4615 
4616     // If we are going to clone an instance, we need its exact type to
4617     // know the number and types of fields to convert the clone to
4618     // loads/stores. Maybe a speculative type can help us.
4619     if (!obj_type-&gt;klass_is_exact() &amp;&amp;
4620         obj_type-&gt;speculative_type() != NULL &amp;&amp;
<span class="line-modified">4621         obj_type-&gt;speculative_type()-&gt;is_instance_klass() &amp;&amp;</span>
<span class="line-added">4622         !obj_type-&gt;speculative_type()-&gt;is_valuetype()) {</span>
4623       ciInstanceKlass* spec_ik = obj_type-&gt;speculative_type()-&gt;as_instance_klass();
4624       if (spec_ik-&gt;nof_nonstatic_fields() &lt;= ArrayCopyLoadStoreMaxElem &amp;&amp;
4625           !spec_ik-&gt;has_injected_fields()) {
4626         ciKlass* k = obj_type-&gt;klass();
4627         if (!k-&gt;is_instance_klass() ||
4628             k-&gt;as_instance_klass()-&gt;is_interface() ||
4629             k-&gt;as_instance_klass()-&gt;has_subklass()) {
4630           obj = maybe_cast_profiled_obj(obj, obj_type-&gt;speculative_type(), false);
4631         }
4632       }
4633     }
4634 
4635     // Conservatively insert a memory barrier on all memory slices.
4636     // Do not let writes into the original float below the clone.
4637     insert_mem_bar(Op_MemBarCPUOrder);
4638 
4639     // paths into result_reg:
4640     enum {
4641       _slow_path = 1,     // out-of-line call to clone method (virtual or not)
4642       _objArray_path,     // plain array allocation, plus arrayof_oop_arraycopy
4643       _array_path,        // plain array allocation, plus arrayof_long_arraycopy
4644       _instance_path,     // plain instance allocation, plus arrayof_long_arraycopy
4645       PATH_LIMIT
4646     };
4647     RegionNode* result_reg = new RegionNode(PATH_LIMIT);
4648     result_val             = new PhiNode(result_reg, TypeInstPtr::NOTNULL);
4649     PhiNode*    result_i_o = new PhiNode(result_reg, Type::ABIO);
4650     PhiNode*    result_mem = new PhiNode(result_reg, Type::MEMORY, TypePtr::BOTTOM);
4651     record_for_igvn(result_reg);
4652 
4653     Node* obj_klass = load_object_klass(obj);
<span class="line-added">4654     // We only go to the fast case code if we pass a number of guards.</span>
<span class="line-added">4655     // The paths which do not pass are accumulated in the slow_region.</span>
<span class="line-added">4656     RegionNode* slow_region = new RegionNode(1);</span>
<span class="line-added">4657     record_for_igvn(slow_region);</span>
<span class="line-added">4658 </span>
4659     Node* array_ctl = generate_array_guard(obj_klass, (RegionNode*)NULL);
4660     if (array_ctl != NULL) {
4661       // It&#39;s an array.
4662       PreserveJVMState pjvms(this);
4663       set_control(array_ctl);



4664 
4665       BarrierSetC2* bs = BarrierSet::barrier_set()-&gt;barrier_set_c2();
<span class="line-modified">4666       if (bs-&gt;array_copy_requires_gc_barriers(true, T_OBJECT, true, BarrierSetC2::Parsing) &amp;&amp;</span>
<span class="line-modified">4667           (!obj_type-&gt;isa_aryptr() || !obj_type-&gt;is_aryptr()-&gt;is_not_flat())) {</span>
<span class="line-modified">4668         // Flattened value type array may have object field that would require a</span>
<span class="line-modified">4669         // write barrier. Conservatively, go to slow path.</span>
<span class="line-modified">4670         generate_valueArray_guard(obj_klass, slow_region);</span>















4671       }







4672 
4673       if (!stopped()) {
<span class="line-modified">4674         Node* obj_length = load_array_length(obj);</span>
<span class="line-modified">4675         Node* obj_size  = NULL;</span>
<span class="line-modified">4676         Node* alloc_obj = new_array(obj_klass, obj_length, 0, &amp;obj_size);  // no arguments to push</span>
<span class="line-modified">4677 </span>
<span class="line-modified">4678         BarrierSetC2* bs = BarrierSet::barrier_set()-&gt;barrier_set_c2();</span>
<span class="line-modified">4679         if (bs-&gt;array_copy_requires_gc_barriers(true, T_OBJECT, true, BarrierSetC2::Parsing)) {</span>
<span class="line-modified">4680           // If it is an oop array, it requires very special treatment,</span>
<span class="line-added">4681           // because gc barriers are required when accessing the array.</span>
<span class="line-added">4682           Node* is_obja = generate_objArray_guard(obj_klass, (RegionNode*)NULL);</span>
<span class="line-added">4683           if (is_obja != NULL) {</span>
<span class="line-added">4684             PreserveJVMState pjvms2(this);</span>
<span class="line-added">4685             set_control(is_obja);</span>
<span class="line-added">4686             // Generate a direct call to the right arraycopy function(s).</span>
<span class="line-added">4687             Node* alloc = tightly_coupled_allocation(alloc_obj, NULL);</span>
<span class="line-added">4688             ArrayCopyNode* ac = ArrayCopyNode::make(this, true, obj, intcon(0), alloc_obj, intcon(0), obj_length, alloc != NULL, false);</span>
<span class="line-added">4689             ac-&gt;set_clone_oop_array();</span>
<span class="line-added">4690             Node* n = _gvn.transform(ac);</span>
<span class="line-added">4691             assert(n == ac, &quot;cannot disappear&quot;);</span>
<span class="line-added">4692             ac-&gt;connect_outputs(this);</span>
<span class="line-added">4693 </span>
<span class="line-added">4694             result_reg-&gt;init_req(_objArray_path, control());</span>
<span class="line-added">4695             result_val-&gt;init_req(_objArray_path, alloc_obj);</span>
<span class="line-added">4696             result_i_o -&gt;set_req(_objArray_path, i_o());</span>
<span class="line-added">4697             result_mem -&gt;set_req(_objArray_path, reset_memory());</span>
<span class="line-added">4698           }</span>
<span class="line-added">4699         }</span>
<span class="line-added">4700         // Otherwise, there are no barriers to worry about.</span>
<span class="line-added">4701         // (We can dispense with card marks if we know the allocation</span>
<span class="line-added">4702         //  comes out of eden (TLAB)...  In fact, ReduceInitialCardMarks</span>
<span class="line-added">4703         //  causes the non-eden paths to take compensating steps to</span>
<span class="line-added">4704         //  simulate a fresh allocation, so that no further</span>
<span class="line-added">4705         //  card marks are required in compiled code to initialize</span>
<span class="line-added">4706         //  the object.)</span>
<span class="line-added">4707 </span>
<span class="line-added">4708         if (!stopped()) {</span>
<span class="line-added">4709           copy_to_clone(obj, alloc_obj, obj_size, true);</span>
<span class="line-added">4710 </span>
<span class="line-added">4711           // Present the results of the copy.</span>
<span class="line-added">4712           result_reg-&gt;init_req(_array_path, control());</span>
<span class="line-added">4713           result_val-&gt;init_req(_array_path, alloc_obj);</span>
<span class="line-added">4714           result_i_o -&gt;set_req(_array_path, i_o());</span>
<span class="line-added">4715           result_mem -&gt;set_req(_array_path, reset_memory());</span>
<span class="line-added">4716         }</span>
4717       }
4718     }
4719 




4720     if (!stopped()) {
4721       // It&#39;s an instance (we did array above).  Make the slow-path tests.
4722       // If this is a virtual call, we generate a funny guard.  We grab
4723       // the vtable entry corresponding to clone() from the target object.
4724       // If the target method which we are calling happens to be the
4725       // Object clone() method, we pass the guard.  We do not need this
4726       // guard for non-virtual calls; the caller is known to be the native
4727       // Object clone().
4728       if (is_virtual) {
4729         generate_virtual_guard(obj_klass, slow_region);
4730       }
4731 
4732       // The object must be easily cloneable and must not have a finalizer.
4733       // Both of these conditions may be checked in a single test.
4734       // We could optimize the test further, but we don&#39;t care.
4735       generate_access_flags_guard(obj_klass,
4736                                   // Test both conditions:
4737                                   JVM_ACC_IS_CLONEABLE_FAST | JVM_ACC_HAS_FINALIZER,
4738                                   // Must be cloneable but not finalizer:
4739                                   JVM_ACC_IS_CLONEABLE_FAST,
</pre>
<hr />
<pre>
4860 // array in the heap that GCs wouldn&#39;t expect. Move the allocation
4861 // after the traps so we don&#39;t allocate the array if we
4862 // deoptimize. This is possible because tightly_coupled_allocation()
4863 // guarantees there&#39;s no observer of the allocated array at this point
4864 // and the control flow is simple enough.
4865 void LibraryCallKit::arraycopy_move_allocation_here(AllocateArrayNode* alloc, Node* dest, JVMState* saved_jvms,
4866                                                     int saved_reexecute_sp, uint new_idx) {
4867   if (saved_jvms != NULL &amp;&amp; !stopped()) {
4868     assert(alloc != NULL, &quot;only with a tightly coupled allocation&quot;);
4869     // restore JVM state to the state at the arraycopy
4870     saved_jvms-&gt;map()-&gt;set_control(map()-&gt;control());
4871     assert(saved_jvms-&gt;map()-&gt;memory() == map()-&gt;memory(), &quot;memory state changed?&quot;);
4872     assert(saved_jvms-&gt;map()-&gt;i_o() == map()-&gt;i_o(), &quot;IO state changed?&quot;);
4873     // If we&#39;ve improved the types of some nodes (null check) while
4874     // emitting the guards, propagate them to the current state
4875     map()-&gt;replaced_nodes().apply(saved_jvms-&gt;map(), new_idx);
4876     set_jvms(saved_jvms);
4877     _reexecute_sp = saved_reexecute_sp;
4878 
4879     // Remove the allocation from above the guards
<span class="line-modified">4880     CallProjections* callprojs = alloc-&gt;extract_projections(true);</span>

4881     InitializeNode* init = alloc-&gt;initialization();
4882     Node* alloc_mem = alloc-&gt;in(TypeFunc::Memory);
<span class="line-modified">4883     C-&gt;gvn_replace_by(callprojs-&gt;fallthrough_ioproj, alloc-&gt;in(TypeFunc::I_O));</span>
4884     C-&gt;gvn_replace_by(init-&gt;proj_out(TypeFunc::Memory), alloc_mem);
4885     C-&gt;gvn_replace_by(init-&gt;proj_out(TypeFunc::Control), alloc-&gt;in(0));
4886 
4887     // move the allocation here (after the guards)
4888     _gvn.hash_delete(alloc);
4889     alloc-&gt;set_req(TypeFunc::Control, control());
4890     alloc-&gt;set_req(TypeFunc::I_O, i_o());
4891     Node *mem = reset_memory();
4892     set_all_memory(mem);
4893     alloc-&gt;set_req(TypeFunc::Memory, mem);
4894     set_control(init-&gt;proj_out_or_null(TypeFunc::Control));
<span class="line-modified">4895     set_i_o(callprojs-&gt;fallthrough_ioproj);</span>
4896 
4897     // Update memory as done in GraphKit::set_output_for_allocation()
4898     const TypeInt* length_type = _gvn.find_int_type(alloc-&gt;in(AllocateNode::ALength));
4899     const TypeOopPtr* ary_type = _gvn.type(alloc-&gt;in(AllocateNode::KlassNode))-&gt;is_klassptr()-&gt;as_instance_type();
4900     if (ary_type-&gt;isa_aryptr() &amp;&amp; length_type != NULL) {
4901       ary_type = ary_type-&gt;is_aryptr()-&gt;cast_to_size(length_type);
4902     }
4903     const TypePtr* telemref = ary_type-&gt;add_offset(Type::OffsetBot);
4904     int            elemidx  = C-&gt;get_alias_index(telemref);
4905     set_memory(init-&gt;proj_out_or_null(TypeFunc::Memory), Compile::AliasIdxRaw);
4906     set_memory(init-&gt;proj_out_or_null(TypeFunc::Memory), elemidx);
4907 
4908     Node* allocx = _gvn.transform(alloc);
4909     assert(allocx == alloc, &quot;where has the allocation gone?&quot;);
4910     assert(dest-&gt;is_CheckCastPP(), &quot;not an allocation result?&quot;);
4911 
4912     _gvn.hash_delete(dest);
4913     dest-&gt;set_req(0, control());
4914     Node* destx = _gvn.transform(dest);
4915     assert(destx == dest, &quot;where has the allocation result gone?&quot;);
</pre>
<hr />
<pre>
5019       if (!has_src) {
5020         src = maybe_cast_profiled_obj(src, src_k, true);
5021         src_type  = _gvn.type(src);
5022         top_src  = src_type-&gt;isa_aryptr();
5023         has_src = (top_src != NULL &amp;&amp; top_src-&gt;klass() != NULL);
5024         src_spec = true;
5025       }
5026       if (!has_dest) {
5027         dest = maybe_cast_profiled_obj(dest, dest_k, true);
5028         dest_type  = _gvn.type(dest);
5029         top_dest  = dest_type-&gt;isa_aryptr();
5030         has_dest = (top_dest != NULL &amp;&amp; top_dest-&gt;klass() != NULL);
5031         dest_spec = true;
5032       }
5033     }
5034   }
5035 
5036   if (has_src &amp;&amp; has_dest &amp;&amp; can_emit_guards) {
5037     BasicType src_elem  = top_src-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
5038     BasicType dest_elem = top_dest-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
<span class="line-modified">5039     if (src_elem  == T_ARRAY)  src_elem  = T_OBJECT;</span>
<span class="line-modified">5040     if (dest_elem == T_ARRAY)  dest_elem = T_OBJECT;</span>
5041 
5042     if (src_elem == dest_elem &amp;&amp; src_elem == T_OBJECT) {
5043       // If both arrays are object arrays then having the exact types
5044       // for both will remove the need for a subtype check at runtime
5045       // before the call and may make it possible to pick a faster copy
5046       // routine (without a subtype check on every element)
5047       // Do we have the exact type of src?
5048       bool could_have_src = src_spec;
5049       // Do we have the exact type of dest?
5050       bool could_have_dest = dest_spec;
5051       ciKlass* src_k = top_src-&gt;klass();
5052       ciKlass* dest_k = top_dest-&gt;klass();
5053       if (!src_spec) {
5054         src_k = src_type-&gt;speculative_type_not_null();
5055         if (src_k != NULL &amp;&amp; src_k-&gt;is_array_klass()) {
5056           could_have_src = true;
5057         }
5058       }
5059       if (!dest_spec) {
5060         dest_k = dest_type-&gt;speculative_type_not_null();
</pre>
<hr />
<pre>
5120     // we also have to check it here for the case where the ArrayCopyNode will
5121     // be eliminated by Escape Analysis.
5122     if (EliminateAllocations) {
5123       generate_negative_guard(length, slow_region);
5124       negative_length_guard_generated = true;
5125     }
5126 
5127     // (9) each element of an oop array must be assignable
5128     Node* dest_klass = load_object_klass(dest);
5129     if (src != dest) {
5130       Node* not_subtype_ctrl = gen_subtype_check(src, dest_klass);
5131 
5132       if (not_subtype_ctrl != top()) {
5133         PreserveJVMState pjvms(this);
5134         set_control(not_subtype_ctrl);
5135         uncommon_trap(Deoptimization::Reason_intrinsic,
5136                       Deoptimization::Action_make_not_entrant);
5137         assert(stopped(), &quot;Should be stopped&quot;);
5138       }
5139     }
<span class="line-added">5140 </span>
<span class="line-added">5141     const TypeKlassPtr* dest_klass_t = _gvn.type(dest_klass)-&gt;is_klassptr();</span>
<span class="line-added">5142     const Type* toop = TypeOopPtr::make_from_klass(dest_klass_t-&gt;klass());</span>
<span class="line-added">5143     src = _gvn.transform(new CheckCastPPNode(control(), src, toop));</span>
<span class="line-added">5144     src_type = _gvn.type(src);</span>
<span class="line-added">5145     top_src  = src_type-&gt;isa_aryptr();</span>
<span class="line-added">5146 </span>
<span class="line-added">5147     if (top_dest != NULL &amp;&amp; !top_dest-&gt;elem()-&gt;isa_valuetype() &amp;&amp; !top_dest-&gt;is_not_flat()) {</span>
<span class="line-added">5148       generate_valueArray_guard(dest_klass, slow_region);</span>
<span class="line-added">5149     }</span>
<span class="line-added">5150 </span>
<span class="line-added">5151     if (top_src != NULL &amp;&amp; !top_src-&gt;elem()-&gt;isa_valuetype() &amp;&amp; !top_src-&gt;is_not_flat()) {</span>
<span class="line-added">5152       Node* src_klass = load_object_klass(src);</span>
<span class="line-added">5153       generate_valueArray_guard(src_klass, slow_region);</span>
<span class="line-added">5154     }</span>
<span class="line-added">5155 </span>
5156     {
5157       PreserveJVMState pjvms(this);
5158       set_control(_gvn.transform(slow_region));
5159       uncommon_trap(Deoptimization::Reason_intrinsic,
5160                     Deoptimization::Action_make_not_entrant);
5161       assert(stopped(), &quot;Should be stopped&quot;);
5162     }




5163   }
5164 
5165   arraycopy_move_allocation_here(alloc, dest, saved_jvms, saved_reexecute_sp, new_idx);
5166 
5167   if (stopped()) {
5168     return true;
5169   }
5170 
5171   ArrayCopyNode* ac = ArrayCopyNode::make(this, true, src, src_offset, dest, dest_offset, length, alloc != NULL, negative_length_guard_generated,
5172                                           // Create LoadRange and LoadKlass nodes for use during macro expansion here
5173                                           // so the compiler has a chance to eliminate them: during macro expansion,
5174                                           // we have to set their control (CastPP nodes are eliminated).
5175                                           load_object_klass(src), load_object_klass(dest),
5176                                           load_array_length(src), load_array_length(dest));
5177 
5178   ac-&gt;set_arraycopy(validated);
5179 
5180   Node* n = _gvn.transform(ac);
5181   if (n == ac) {
5182     ac-&gt;connect_outputs(this);
</pre>
</td>
</tr>
</table>
<center><a href="graphKit.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="matcher.cpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>