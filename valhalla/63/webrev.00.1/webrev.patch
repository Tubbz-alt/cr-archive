diff a/.hgtags b/.hgtags
--- a/.hgtags
+++ b/.hgtags
@@ -487,10 +487,11 @@
 758deedaae8406ae60147486107a54e9864aa7b0 jdk-11+13
 3595bd343b65f8c37818ebe6a4c343ddeb1a5f88 jdk-11+14
 a11c1cb542bbd1671d25b85efe7d09b983c48525 jdk-11+15
 02934b0d661b82b7fe1052a04998d2091352e08d jdk-11+16
 64e4b1686141e57a681936a8283983341484676e jdk-11+17
+d2aa5d494481a1039a092d70efa1f5c9826c5b77 lw1_0
 e1b3def126240d5433902f3cb0e91a4c27f6db50 jdk-11+18
 36ca515343e00b021dcfc902e986d26ec994a2e5 jdk-11+19
 95aad0c785e497f1bade3955c4e4a677b629fa9d jdk-12+0
 9816d7cc655e53ba081f938b656e31971b8f097a jdk-11+20
 14708e1acdc3974f4539027cbbcfa6d69f83cf51 jdk-11+21
@@ -509,10 +510,11 @@
 ef57958c7c511162da8d9a75f0b977f0f7ac464e jdk-12+7
 76072a077ee1d815152d45d1692c4b36c53c5c49 jdk-11+28
 492b366f8e5784cc4927c2c98f9b8a3f16c067eb jdk-12+8
 31b159f30fb281016c5f0c103552809aeda84063 jdk-12+9
 8f594f75e0547d4ca16649cb3501659e3155e81b jdk-12+10
+6132641c6ff61b7b8f3f10b9cd385aafadbd72ef lworld_stable
 f0f5d23449d31f1b3580c8a73313918cafeaefd7 jdk-12+11
 15094d12a632f452a2064318a4e416d0c7a9ce0c jdk-12+12
 511a9946f83e3e3c7b9dbe1840367063fb39b4e1 jdk-12+13
 8897e41b327c0a5601c6ba2bba5d07f15a3ffc91 jdk-12+14
 8897e41b327c0a5601c6ba2bba5d07f15a3ffc91 jdk-12+14
@@ -565,10 +567,14 @@
 22b3b7983adab54e318f75aeb94471f7a4429c1e jdk-14+0
 22b3b7983adab54e318f75aeb94471f7a4429c1e jdk-13+25
 2f4e214781a1d597ed36bf5a36f20928c6c82996 jdk-14+1
 0692b67f54621991ba7afbf23e55b788f3555e69 jdk-13+26
 43627549a488b7d0b4df8fad436e36233df89877 jdk-14+2
+6132641c6ff61b7b8f3f10b9cd385aafadbd72ef lworld_stable
+2b098533f1e52d7d541121409b745d9420886945 lworld_stable
+2b098533f1e52d7d541121409b745d9420886945 lworld_stable
+7c637fd25e7d6fccdab1098bedd48ed195a86cc7 lworld_stable
 b7f68ddec66f996ae3aad03291d129ca9f02482d jdk-13+27
 e64383344f144217c36196c3c8a2df8f588a2af3 jdk-14+3
 1e95931e7d8fa7e3899340a9c7cb28dbea50c10c jdk-13+28
 19d0b382f0869f72d4381b54fa129f1c74b6e766 jdk-14+4
 3081f39a3d30d63b112098386ac2bb027c2b7223 jdk-13+29
diff a/make/conf/jib-profiles.js b/make/conf/jib-profiles.js
--- a/make/conf/jib-profiles.js
+++ b/make/conf/jib-profiles.js
@@ -1364,10 +1364,11 @@
             preString = version_numbers.get("DEFAULT_PROMOTED_VERSION_PRE");
         }
         args = concat(args, "--with-version-pre=" + preString,
                      "--with-version-opt=" + optString);
     } else {
+        args = concat(args, "--with-version-pre=lworld2ea");
         args = concat(args, "--with-version-opt=" + common.build_id);
     }
     return args;
 }
 
diff a/make/data/jdwp/jdwp.spec b/make/data/jdwp/jdwp.spec
--- a/make/data/jdwp/jdwp.spec
+++ b/make/data/jdwp/jdwp.spec
@@ -3249,10 +3249,11 @@
 (ConstantSet Tag
     (Constant ARRAY = '[' "'[' - an array object (objectID size). ")
     (Constant BYTE = 'B' "'B' - a byte value (1 byte).")
     (Constant CHAR = 'C' "'C' - a character value (2 bytes).")
     (Constant OBJECT = 'L' "'L' - an object (objectID size).")
+    (Constant INLINE_OBJECT = 'Q' "'Q' - an inline object (objectID size).")
     (Constant FLOAT = 'F' "'F' - a float value (4 bytes).")
     (Constant DOUBLE = 'D' "'D' - a double value (8 bytes).")
     (Constant INT = 'I' "'I' - an int value (4 bytes).")
     (Constant LONG = 'J' "'J' - a long value (8 bytes).")
     (Constant SHORT = 'S' "'S' - a short value (2 bytes).")
diff a/make/test/BuildMicrobenchmark.gmk b/make/test/BuildMicrobenchmark.gmk
--- a/make/test/BuildMicrobenchmark.gmk
+++ b/make/test/BuildMicrobenchmark.gmk
@@ -96,12 +96,12 @@
 ))
 
 # Build microbenchmark suite for the current JDK
 $(eval $(call SetupJavaCompilation, BUILD_JDK_MICROBENCHMARK, \
     SETUP := MICROBENCHMARK_JAVA_COMPILER, \
-    ADD_JAVAC_FLAGS := -cp $(MICROBENCHMARK_CLASSPATH), \
-    DISABLED_WARNINGS := processing rawtypes cast serial, \
+    ADD_JAVAC_FLAGS := -cp $(MICROBENCHMARK_CLASSPATH) -XDallowWithFieldOperator, \
+    DISABLED_WARNINGS := processing rawtypes unchecked cast serial deprecation, \
     SRC := $(MICROBENCHMARK_SRC), \
     BIN := $(MICROBENCHMARK_CLASSES), \
 ))
 
 $(BUILD_JDK_MICROBENCHMARK): $(JMH_COMPILE_JARS)
diff a/src/hotspot/cpu/aarch64/aarch64.ad b/src/hotspot/cpu/aarch64/aarch64.ad
--- a/src/hotspot/cpu/aarch64/aarch64.ad
+++ b/src/hotspot/cpu/aarch64/aarch64.ad
@@ -1640,38 +1640,12 @@
 
 void MachPrologNode::emit(CodeBuffer &cbuf, PhaseRegAlloc *ra_) const {
   Compile* C = ra_->C;
   C2_MacroAssembler _masm(&cbuf);
 
-  // n.b. frame size includes space for return pc and rfp
-  const long framesize = C->output()->frame_size_in_bytes();
-  assert(framesize%(2*wordSize) == 0, "must preserve 2*wordSize alignment");
-
-  // insert a nop at the start of the prolog so we can patch in a
-  // branch if we need to invalidate the method later
-  __ nop();
-
-  if (C->clinit_barrier_on_entry()) {
-    assert(!C->method()->holder()->is_not_initialized(), "initialization should have been started");
-
-    Label L_skip_barrier;
-
-    __ mov_metadata(rscratch2, C->method()->holder()->constant_encoding());
-    __ clinit_barrier(rscratch2, rscratch1, &L_skip_barrier);
-    __ far_jump(RuntimeAddress(SharedRuntime::get_handle_wrong_method_stub()));
-    __ bind(L_skip_barrier);
-  }
-
-  int bangsize = C->output()->bang_size_in_bytes();
-  if (C->output()->need_stack_bang(bangsize) && UseStackBanging)
-    __ generate_stack_overflow_check(bangsize);
-
-  __ build_frame(framesize);
-
-  if (VerifyStackAtCalls) {
-    Unimplemented();
-  }
+  __ verified_entry(C, 0);
+  __ bind(*_verified_entry);
 
   C->output()->set_frame_complete(cbuf.insts_size());
 
   if (C->has_mach_constant_base_node()) {
     // NOTE: We set the table base offset here because users might be
@@ -1977,12 +1951,50 @@
 uint BoxLockNode::size(PhaseRegAlloc *ra_) const {
   // BoxLockNode is not a MachNode, so we can't just call MachNode::size(ra_).
   return 4;
 }
 
-//=============================================================================
+///=============================================================================
+#ifndef PRODUCT
+void MachVEPNode::format(PhaseRegAlloc* ra_, outputStream* st) const
+{
+  st->print_cr("# MachVEPNode");
+  if (!_verified) {
+    st->print_cr("\t load_class");
+  } else {
+    st->print_cr("\t unpack_value_arg");
+  }
+}
+#endif
+
+void MachVEPNode::emit(CodeBuffer& cbuf, PhaseRegAlloc* ra_) const
+{
+  MacroAssembler _masm(&cbuf);
+
+  if (!_verified) {
+    Label skip;
+    __ cmp_klass(j_rarg0, rscratch2, rscratch1);
+    __ br(Assembler::EQ, skip);
+      __ far_jump(RuntimeAddress(SharedRuntime::get_ic_miss_stub()));
+    __ bind(skip);
+
+  } else {
+    // Unpack value type args passed as oop and then jump to
+    // the verified entry point (skipping the unverified entry).
+    __ unpack_value_args(ra_->C, _receiver_only);
+    __ b(*_verified_entry);
+  }
+}
+
+
+uint MachVEPNode::size(PhaseRegAlloc* ra_) const
+{
+  return MachNode::size(ra_); // too many variables; just compute it the hard way
+}
+
 
+//=============================================================================
 #ifndef PRODUCT
 void MachUEPNode::format(PhaseRegAlloc* ra_, outputStream* st) const
 {
   st->print_cr("# MachUEPNode");
   if (UseCompressedClassPointers) {
@@ -2000,13 +2012,15 @@
 
 void MachUEPNode::emit(CodeBuffer& cbuf, PhaseRegAlloc* ra_) const
 {
   // This is the unverified entry point.
   C2_MacroAssembler _masm(&cbuf);
+  Label skip;
 
+  // UseCompressedClassPointers logic are inside cmp_klass
   __ cmp_klass(j_rarg0, rscratch2, rscratch1);
-  Label skip;
+
   // TODO
   // can we avoid this skip and still use a reloc?
   __ br(Assembler::EQ, skip);
   __ far_jump(RuntimeAddress(SharedRuntime::get_ic_miss_stub()));
   __ bind(skip);
@@ -2409,11 +2423,10 @@
 }
 
 void Compile::reshape_address(AddPNode* addp) {
 }
 
-
 #define MOV_VOLATILE(REG, BASE, INDEX, SCALE, DISP, SCRATCH, INSN)      \
   C2_MacroAssembler _masm(&cbuf);                                       \
   {                                                                     \
     guarantee(INDEX == -1, "mode not permitted for volatile");          \
     guarantee(DISP == 0, "mode not permitted for volatile");            \
@@ -8269,10 +8282,25 @@
   %}
 
   ins_pipe(ialu_reg);
 %}
 
+instruct castN2X(iRegLNoSp dst, iRegN src) %{
+  match(Set dst (CastP2X src));
+
+  ins_cost(INSN_COST);
+  format %{ "mov $dst, $src\t# ptr -> long" %}
+
+  ins_encode %{
+    if ($dst$$reg != $src$$reg) {
+      __ mov(as_Register($dst$$reg), as_Register($src$$reg));
+    }
+  %}
+
+  ins_pipe(ialu_reg);
+%}
+
 instruct castP2X(iRegLNoSp dst, iRegP src) %{
   match(Set dst (CastP2X src));
 
   ins_cost(INSN_COST);
   format %{ "mov $dst, $src\t# ptr -> long" %}
@@ -8284,10 +8312,41 @@
   %}
 
   ins_pipe(ialu_reg);
 %}
 
+instruct castN2I(iRegINoSp dst, iRegN src) %{
+  match(Set dst (CastN2I src));
+
+  ins_cost(INSN_COST);
+  format %{ "movw $dst, $src\t# compressed ptr -> int" %}
+
+  ins_encode %{
+    if ($dst$$reg != $src$$reg) {
+      __ movw(as_Register($dst$$reg), as_Register($src$$reg));
+    }
+  %}
+
+  ins_pipe(ialu_reg);
+%}
+
+instruct castI2N(iRegNNoSp dst, iRegI src) %{
+  match(Set dst (CastI2N src));
+
+  ins_cost(INSN_COST);
+  format %{ "movw $dst, $src\t# int -> compressed ptr" %}
+
+  ins_encode %{
+    if ($dst$$reg != $src$$reg) {
+      __ movw(as_Register($dst$$reg), as_Register($src$$reg));
+    }
+  %}
+
+  ins_pipe(ialu_reg);
+%}
+
+
 // Convert oop into int for vectors alignment masking
 instruct convP2I(iRegINoSp dst, iRegP src) %{
   match(Set dst (ConvL2I (CastP2X src)));
 
   ins_cost(INSN_COST);
@@ -13866,37 +13925,20 @@
 %}
 
 // ============================================================================
 // clearing of an array
 
-instruct clearArray_reg_reg(iRegL_R11 cnt, iRegP_R10 base, Universe dummy, rFlagsReg cr)
+instruct clearArray_reg_reg(iRegL_R11 cnt, iRegP_R10 base, iRegL val, Universe dummy, rFlagsReg cr)
 %{
-  match(Set dummy (ClearArray cnt base));
+  match(Set dummy (ClearArray (Binary cnt base) val));
   effect(USE_KILL cnt, USE_KILL base);
 
   ins_cost(4 * INSN_COST);
-  format %{ "ClearArray $cnt, $base" %}
-
-  ins_encode %{
-    __ zero_words($base$$Register, $cnt$$Register);
-  %}
-
-  ins_pipe(pipe_class_memory);
-%}
-
-instruct clearArray_imm_reg(immL cnt, iRegP_R10 base, Universe dummy, rFlagsReg cr)
-%{
-  predicate((u_int64_t)n->in(2)->get_long()
-            < (u_int64_t)(BlockZeroingLowLimit >> LogBytesPerWord));
-  match(Set dummy (ClearArray cnt base));
-  effect(USE_KILL base);
-
-  ins_cost(4 * INSN_COST);
-  format %{ "ClearArray $cnt, $base" %}
+  format %{ "ClearArray $cnt, $base, $val" %}
 
   ins_encode %{
-    __ zero_words($base$$Register, (u_int64_t)$cnt$$constant);
+    __ fill_words($base$$Register, $cnt$$Register, $val$$Register);
   %}
 
   ins_pipe(pipe_class_memory);
 %}
 
diff a/src/hotspot/cpu/aarch64/macroAssembler_aarch64.cpp b/src/hotspot/cpu/aarch64/macroAssembler_aarch64.cpp
--- a/src/hotspot/cpu/aarch64/macroAssembler_aarch64.cpp
+++ b/src/hotspot/cpu/aarch64/macroAssembler_aarch64.cpp
@@ -44,10 +44,11 @@
 #include "runtime/biasedLocking.hpp"
 #include "runtime/icache.hpp"
 #include "runtime/interfaceSupport.inline.hpp"
 #include "runtime/jniHandles.inline.hpp"
 #include "runtime/sharedRuntime.hpp"
+#include "runtime/signature_cc.hpp"
 #include "runtime/thread.hpp"
 #include "utilities/powerOfTwo.hpp"
 #ifdef COMPILER1
 #include "c1/c1_LIRAssembler.hpp"
 #endif
@@ -1314,11 +1315,15 @@
     Unimplemented();
   }
 }
 
 void MacroAssembler::verify_oop(Register reg, const char* s) {
-  if (!VerifyOops) return;
+  if (!VerifyOops || VerifyAdapterSharing) {
+    // Below address of the code string confuses VerifyAdapterSharing
+    // because it may differ between otherwise equivalent adapters.
+    return;
+  }
 
   // Pass register number to verify_oop_subroutine
   const char* b = NULL;
   {
     ResourceMark rm;
@@ -1344,11 +1349,15 @@
 
   BLOCK_COMMENT("} verify_oop");
 }
 
 void MacroAssembler::verify_oop_addr(Address addr, const char* s) {
-  if (!VerifyOops) return;
+  if (!VerifyOops || VerifyAdapterSharing) {
+    // Below address of the code string confuses VerifyAdapterSharing
+    // because it may differ between otherwise equivalent adapters.
+    return;
+  }
 
   const char* b = NULL;
   {
     ResourceMark rm;
     stringStream ss;
@@ -1437,10 +1446,14 @@
   pass_arg1(this, arg_1);
   pass_arg2(this, arg_2);
   call_VM_leaf_base(entry_point, 3);
 }
 
+void MacroAssembler::super_call_VM_leaf(address entry_point) {
+  MacroAssembler::call_VM_leaf_base(entry_point, 1);
+}
+
 void MacroAssembler::super_call_VM_leaf(address entry_point, Register arg_0) {
   pass_arg0(this, arg_0);
   MacroAssembler::call_VM_leaf_base(entry_point, 1);
 }
 
@@ -1486,10 +1499,43 @@
     // nothing to do, (later) access of M[reg + offset]
     // will provoke OS NULL exception if reg = NULL
   }
 }
 
+void MacroAssembler::test_klass_is_value(Register klass, Register temp_reg, Label& is_value) {
+  ldrw(temp_reg, Address(klass, Klass::access_flags_offset()));
+  andr(temp_reg, temp_reg, JVM_ACC_VALUE);
+  cbnz(temp_reg, is_value);
+}
+
+void MacroAssembler::test_field_is_flattenable(Register flags, Register temp_reg, Label& is_flattenable) {
+  (void) temp_reg; // keep signature uniform with x86
+  tbnz(flags, ConstantPoolCacheEntry::is_flattenable_field_shift, is_flattenable);
+}
+
+void MacroAssembler::test_field_is_not_flattenable(Register flags, Register temp_reg, Label& not_flattenable) {
+  (void) temp_reg; // keep signature uniform with x86
+  tbz(flags, ConstantPoolCacheEntry::is_flattenable_field_shift, not_flattenable);
+}
+
+void MacroAssembler::test_field_is_flattened(Register flags, Register temp_reg, Label& is_flattened) {
+  (void) temp_reg; // keep signature uniform with x86
+  tbnz(flags, ConstantPoolCacheEntry::is_flattened_field_shift, is_flattened);
+}
+
+void MacroAssembler::test_flattened_array_oop(Register oop, Register temp_reg, Label& is_flattened_array) {
+  load_storage_props(temp_reg, oop);
+  andr(temp_reg, temp_reg, ArrayStorageProperties::flattened_value);
+  cbnz(temp_reg, is_flattened_array);
+}
+
+void MacroAssembler::test_null_free_array_oop(Register oop, Register temp_reg, Label& is_null_free_array) {
+  load_storage_props(temp_reg, oop);
+  andr(temp_reg, temp_reg, ArrayStorageProperties::null_free_value);
+  cbnz(temp_reg, is_null_free_array);
+}
+
 // MacroAssembler protected routines needed to implement
 // public methods
 
 void MacroAssembler::mov(Register r, Address dest) {
   code_section()->relocate(pc(), dest.rspec());
@@ -3697,19 +3743,28 @@
   ldr(holder, Address(method, Method::const_offset()));                      // ConstMethod*
   ldr(holder, Address(holder, ConstMethod::constants_offset()));             // ConstantPool*
   ldr(holder, Address(holder, ConstantPool::pool_holder_offset_in_bytes())); // InstanceKlass*
 }
 
-void MacroAssembler::load_klass(Register dst, Register src) {
+void MacroAssembler::load_metadata(Register dst, Register src) {
   if (UseCompressedClassPointers) {
     ldrw(dst, Address(src, oopDesc::klass_offset_in_bytes()));
-    decode_klass_not_null(dst);
   } else {
     ldr(dst, Address(src, oopDesc::klass_offset_in_bytes()));
   }
 }
 
+void MacroAssembler::load_klass(Register dst, Register src) {
+  load_metadata(dst, src);
+  if (UseCompressedClassPointers) {
+    andr(dst, dst, oopDesc::compressed_klass_mask());
+    decode_klass_not_null(dst);
+  } else {
+    ubfm(dst, dst, 0, 63 - oopDesc::storage_props_nof_bits);
+  }
+}
+
 // ((OopHandle)result).resolve();
 void MacroAssembler::resolve_oop_handle(Register result, Register tmp) {
   // OopHandle::resolve is an indirection.
   access_load_at(T_OBJECT, IN_NATIVE, result, Address(result, 0), tmp, noreg);
 }
@@ -3721,10 +3776,19 @@
   ldr(dst, Address(dst, ConstantPool::pool_holder_offset_in_bytes()));
   ldr(dst, Address(dst, mirror_offset));
   resolve_oop_handle(dst, tmp);
 }
 
+void MacroAssembler::load_storage_props(Register dst, Register src) {
+  load_metadata(dst, src);
+  if (UseCompressedClassPointers) {
+    asrw(dst, dst, oopDesc::narrow_storage_props_shift);
+  } else {
+    asr(dst, dst, oopDesc::wide_storage_props_shift);
+  }
+}
+
 void MacroAssembler::cmp_klass(Register oop, Register trial_klass, Register tmp) {
   if (UseCompressedClassPointers) {
     ldrw(tmp, Address(oop, oopDesc::klass_offset_in_bytes()));
     if (CompressedKlassPointers::base() == NULL) {
       cmp(trial_klass, tmp, LSL, CompressedKlassPointers::shift());
@@ -4058,18 +4122,19 @@
   }
 }
 
 void MacroAssembler::access_store_at(BasicType type, DecoratorSet decorators,
                                      Address dst, Register src,
-                                     Register tmp1, Register thread_tmp) {
+                                     Register tmp1, Register thread_tmp, Register tmp3) {
+
   BarrierSetAssembler *bs = BarrierSet::barrier_set()->barrier_set_assembler();
   decorators = AccessInternal::decorator_fixup(decorators);
   bool as_raw = (decorators & AS_RAW) != 0;
   if (as_raw) {
-    bs->BarrierSetAssembler::store_at(this, decorators, type, dst, src, tmp1, thread_tmp);
+    bs->BarrierSetAssembler::store_at(this, decorators, type, dst, src, tmp1, thread_tmp, tmp3);
   } else {
-    bs->store_at(this, decorators, type, dst, src, tmp1, thread_tmp);
+    bs->store_at(this, decorators, type, dst, src, tmp1, thread_tmp, tmp3);
   }
 }
 
 void MacroAssembler::resolve(DecoratorSet decorators, Register obj) {
   // Use stronger ACCESS_WRITE|ACCESS_READ by default.
@@ -4089,17 +4154,17 @@
                                             Register thread_tmp, DecoratorSet decorators) {
   access_load_at(T_OBJECT, IN_HEAP | IS_NOT_NULL | decorators, dst, src, tmp1, thread_tmp);
 }
 
 void MacroAssembler::store_heap_oop(Address dst, Register src, Register tmp1,
-                                    Register thread_tmp, DecoratorSet decorators) {
-  access_store_at(T_OBJECT, IN_HEAP | decorators, dst, src, tmp1, thread_tmp);
+                                    Register thread_tmp, Register tmp3, DecoratorSet decorators) {
+  access_store_at(T_OBJECT, IN_HEAP | decorators, dst, src, tmp1, thread_tmp, tmp3);
 }
 
 // Used for storing NULLs.
 void MacroAssembler::store_heap_oop_null(Address dst) {
-  access_store_at(T_OBJECT, IN_HEAP, dst, noreg, noreg, noreg);
+  access_store_at(T_OBJECT, IN_HEAP, dst, noreg, noreg, noreg, noreg);
 }
 
 Address MacroAssembler::allocate_metadata_address(Metadata* obj) {
   assert(oop_recorder() != NULL, "this assembler needs a Recorder");
   int index = oop_recorder()->allocate_metadata_index(obj);
@@ -5163,10 +5228,400 @@
   }
 
   pop(saved_regs, sp);
 }
 
+// C2 compiled method's prolog code
+// Moved here from aarch64.ad to support Valhalla code belows
+void MacroAssembler::verified_entry(Compile* C, int sp_inc) {
+
+// n.b. frame size includes space for return pc and rfp
+  const long framesize = C->frame_size_in_bytes();
+  assert(framesize % (2 * wordSize) == 0, "must preserve 2 * wordSize alignment");
+
+  // insert a nop at the start of the prolog so we can patch in a
+  // branch if we need to invalidate the method later
+  nop();
+
+  int bangsize = C->bang_size_in_bytes();
+  if (C->need_stack_bang(bangsize) && UseStackBanging)
+     generate_stack_overflow_check(bangsize);
+
+  build_frame(framesize);
+
+  if (VerifyStackAtCalls) {
+    Unimplemented();
+  }
+}
+
+int MacroAssembler::store_value_type_fields_to_buf(ciValueKlass* vk, bool from_interpreter) {
+  // A value type might be returned. If fields are in registers we
+  // need to allocate a value type instance and initialize it with
+  // the value of the fields.
+  Label skip;
+  // We only need a new buffered value if a new one is not returned
+  cmp(r0, (u1) 1);
+  br(Assembler::EQ, skip);
+  int call_offset = -1;
+
+  Label slow_case;
+
+  // Try to allocate a new buffered value (from the heap)
+  if (UseTLAB) {
+
+    if (vk != NULL) {
+      // Called from C1, where the return type is statically known.
+      mov(r1, (intptr_t)vk->get_ValueKlass());
+      jint lh = vk->layout_helper();
+      assert(lh != Klass::_lh_neutral_value, "inline class in return type must have been resolved");
+      mov(r14, lh);
+    } else {
+       // Call from interpreter. R0 contains ((the ValueKlass* of the return type) | 0x01)
+       andr(r1, r0, -2);
+       // get obj size
+       ldrw(r14, Address(rscratch1 /*klass*/, Klass::layout_helper_offset()));
+    }
+
+     ldr(r13, Address(rthread, in_bytes(JavaThread::tlab_top_offset())));
+
+     // check whether we have space in TLAB,
+     // rscratch1 contains pointer to just allocated obj
+      lea(r14, Address(r13, r14));
+      ldr(rscratch1, Address(rthread, in_bytes(JavaThread::tlab_end_offset())));
+
+      cmp(r14, rscratch1);
+      br(Assembler::GT, slow_case);
+
+      // OK we have room in TLAB,
+      // Set new TLAB top
+      str(r14, Address(rthread, in_bytes(JavaThread::tlab_top_offset())));
+
+      // Set new class always locked
+      mov(rscratch1, (uint64_t) markWord::always_locked_prototype().value());
+      str(rscratch1, Address(r13, oopDesc::mark_offset_in_bytes()));
+
+      store_klass_gap(r13, zr);  // zero klass gap for compressed oops
+      if (vk == NULL) {
+        // store_klass corrupts rbx, so save it in rax for later use (interpreter case only).
+         mov(r0, r1);
+      }
+
+      store_klass(r13, r1);  // klass
+
+      if (vk != NULL) {
+        // FIXME -- do the packing in-line to avoid the runtime call
+        mov(r0, r13);
+        far_call(RuntimeAddress(vk->pack_handler())); // no need for call info as this will not safepoint.
+      } else {
+
+        // We have our new buffered value, initialize its fields with a
+        // value class specific handler
+        ldr(r1, Address(r0, InstanceKlass::adr_valueklass_fixed_block_offset()));
+        ldr(r1, Address(r1, ValueKlass::pack_handler_offset()));
+
+        // Mov new class to r0 and call pack_handler
+        mov(r0, r13);
+        blr(r1);
+      }
+      b(skip);
+  }
+
+  bind(slow_case);
+  // We failed to allocate a new value, fall back to a runtime
+  // call. Some oop field may be live in some registers but we can't
+  // tell. That runtime call will take care of preserving them
+  // across a GC if there's one.
+
+
+  if (from_interpreter) {
+    super_call_VM_leaf(StubRoutines::store_value_type_fields_to_buf());
+  } else {
+    ldr(rscratch1, RuntimeAddress(StubRoutines::store_value_type_fields_to_buf()));
+    blr(rscratch1);
+    call_offset = offset();
+  }
+
+  bind(skip);
+  return call_offset;
+}
+
+// Move a value between registers/stack slots and update the reg_state
+bool MacroAssembler::move_helper(VMReg from, VMReg to, BasicType bt, RegState reg_state[], int ret_off, int extra_stack_offset) {
+  if (reg_state[to->value()] == reg_written) {
+    return true; // Already written
+  }
+
+  if (from != to && bt != T_VOID) {
+    if (reg_state[to->value()] == reg_readonly) {
+      return false; // Not yet writable
+    }
+    if (from->is_reg()) {
+      if (to->is_reg()) {
+        mov(to->as_Register(), from->as_Register());
+      } else {
+        int st_off = to->reg2stack() * VMRegImpl::stack_slot_size + extra_stack_offset;
+        Address to_addr = Address(sp, st_off);
+        if (from->is_FloatRegister()) {
+          if (bt == T_DOUBLE) {
+             strd(from->as_FloatRegister(), to_addr);
+          } else {
+             assert(bt == T_FLOAT, "must be float");
+             strs(from->as_FloatRegister(), to_addr);
+          }
+        } else {
+          str(from->as_Register(), to_addr);
+        }
+      }
+    } else {
+      Address from_addr = Address(sp, from->reg2stack() * VMRegImpl::stack_slot_size + extra_stack_offset);
+      if (to->is_reg()) {
+        if (to->is_FloatRegister()) {
+          if (bt == T_DOUBLE) {
+             ldrd(to->as_FloatRegister(), from_addr);
+          } else {
+            assert(bt == T_FLOAT, "must be float");
+            ldrs(to->as_FloatRegister(), from_addr);
+          }
+        } else {
+          ldr(to->as_Register(), from_addr);
+        }
+      } else {
+        int st_off = to->reg2stack() * VMRegImpl::stack_slot_size + extra_stack_offset;
+        ldr(rscratch1, from_addr);
+        str(rscratch1, Address(sp, st_off));
+      }
+    }
+  }
+
+  // Update register states
+  reg_state[from->value()] = reg_writable;
+  reg_state[to->value()] = reg_written;
+  return true;
+}
+
+// Read all fields from a value type oop and store the values in registers/stack slots
+bool MacroAssembler::unpack_value_helper(const GrowableArray<SigEntry>* sig, int& sig_index, VMReg from, VMRegPair* regs_to,
+                                         int& to_index, RegState reg_state[], int ret_off, int extra_stack_offset) {
+  Register fromReg = from->is_reg() ? from->as_Register() : noreg;
+  assert(sig->at(sig_index)._bt == T_VOID, "should be at end delimiter");
+
+
+  int vt = 1;
+  bool done = true;
+  bool mark_done = true;
+  do {
+    sig_index--;
+    BasicType bt = sig->at(sig_index)._bt;
+    if (bt == T_VALUETYPE) {
+      vt--;
+    } else if (bt == T_VOID &&
+               sig->at(sig_index-1)._bt != T_LONG &&
+               sig->at(sig_index-1)._bt != T_DOUBLE) {
+      vt++;
+    } else if (SigEntry::is_reserved_entry(sig, sig_index)) {
+      to_index--; // Ignore this
+    } else {
+      assert(to_index >= 0, "invalid to_index");
+      VMRegPair pair_to = regs_to[to_index--];
+      VMReg to = pair_to.first();
+
+      if (bt == T_VOID) continue;
+
+      int idx = (int) to->value();
+      if (reg_state[idx] == reg_readonly) {
+         if (idx != from->value()) {
+           mark_done = false;
+         }
+         done = false;
+         continue;
+      } else if (reg_state[idx] == reg_written) {
+        continue;
+      } else {
+        assert(reg_state[idx] == reg_writable, "must be writable");
+        reg_state[idx] = reg_written;
+      }
+
+      if (fromReg == noreg) {
+        int st_off = from->reg2stack() * VMRegImpl::stack_slot_size + extra_stack_offset;
+        ldr(rscratch2, Address(sp, st_off));
+        fromReg = rscratch2;
+      }
+
+      int off = sig->at(sig_index)._offset;
+      assert(off > 0, "offset in object should be positive");
+      bool is_oop = (bt == T_OBJECT || bt == T_ARRAY);
+
+      Address fromAddr = Address(fromReg, off);
+      bool is_signed = (bt != T_CHAR) && (bt != T_BOOLEAN);
+
+      if (!to->is_FloatRegister()) {
+
+        Register dst = to->is_stack() ? rscratch1 : to->as_Register();
+
+        if (is_oop) {
+          load_heap_oop(dst, fromAddr);
+        } else {
+          load_sized_value(dst, fromAddr, type2aelembytes(bt), is_signed);
+        }
+        if (to->is_stack()) {
+          int st_off = to->reg2stack() * VMRegImpl::stack_slot_size + extra_stack_offset;
+          str(dst, Address(sp, st_off));
+        }
+      } else {
+        if (bt == T_DOUBLE) {
+          ldrd(to->as_FloatRegister(), fromAddr);
+        } else {
+          assert(bt == T_FLOAT, "must be float");
+          ldrs(to->as_FloatRegister(), fromAddr);
+        }
+     }
+
+    }
+
+  } while (vt != 0);
+
+  if (mark_done && reg_state[from->value()] != reg_written) {
+    // This is okay because no one else will write to that slot
+    reg_state[from->value()] = reg_writable;
+  }
+  return done;
+}
+
+// Pack fields back into a value type oop
+bool MacroAssembler::pack_value_helper(const GrowableArray<SigEntry>* sig, int& sig_index, int vtarg_index,
+                                       VMReg to, VMRegPair* regs_from, int regs_from_count, int& from_index, RegState reg_state[],
+                                       int ret_off, int extra_stack_offset) {
+  assert(sig->at(sig_index)._bt == T_VALUETYPE, "should be at end delimiter");
+  assert(to->is_valid(), "must be");
+
+  if (reg_state[to->value()] == reg_written) {
+    skip_unpacked_fields(sig, sig_index, regs_from, regs_from_count, from_index);
+    return true; // Already written
+  }
+
+  Register val_array = r0;
+  Register val_obj_tmp = r11;
+  Register from_reg_tmp = r10;
+  Register tmp1 = r14;
+  Register tmp2 = r13;
+  Register tmp3 = r1;
+  Register val_obj = to->is_stack() ? val_obj_tmp : to->as_Register();
+
+  if (reg_state[to->value()] == reg_readonly) {
+    if (!is_reg_in_unpacked_fields(sig, sig_index, to, regs_from, regs_from_count, from_index)) {
+      skip_unpacked_fields(sig, sig_index, regs_from, regs_from_count, from_index);
+      return false; // Not yet writable
+    }
+    val_obj = val_obj_tmp;
+  }
+
+  int index = arrayOopDesc::base_offset_in_bytes(T_OBJECT) + vtarg_index * type2aelembytes(T_VALUETYPE);
+  load_heap_oop(val_obj, Address(val_array, index));
+
+  ScalarizedValueArgsStream stream(sig, sig_index, regs_from, regs_from_count, from_index);
+  VMRegPair from_pair;
+  BasicType bt;
+
+  while (stream.next(from_pair, bt)) {
+    int off = sig->at(stream.sig_cc_index())._offset;
+    assert(off > 0, "offset in object should be positive");
+    bool is_oop = (bt == T_OBJECT || bt == T_ARRAY);
+    size_t size_in_bytes = is_java_primitive(bt) ? type2aelembytes(bt) : wordSize;
+
+    VMReg from_r1 = from_pair.first();
+    VMReg from_r2 = from_pair.second();
+
+    // Pack the scalarized field into the value object.
+    Address dst(val_obj, off);
+
+    if (!from_r1->is_FloatRegister()) {
+      Register from_reg;
+      if (from_r1->is_stack()) {
+        from_reg = from_reg_tmp;
+        int ld_off = from_r1->reg2stack() * VMRegImpl::stack_slot_size + extra_stack_offset;
+        load_sized_value(from_reg, Address(sp, ld_off), size_in_bytes, /* is_signed */ false);
+      } else {
+        from_reg = from_r1->as_Register();
+      }
+
+      if (is_oop) {
+        DecoratorSet decorators = IN_HEAP | ACCESS_WRITE;
+        store_heap_oop(dst, from_reg, tmp1, tmp2, tmp3, decorators);
+      } else {
+        store_sized_value(dst, from_reg, size_in_bytes);
+      }
+    } else {
+      if (from_r2->is_valid()) {
+        strd(from_r1->as_FloatRegister(), dst);
+      } else {
+        strs(from_r1->as_FloatRegister(), dst);
+      }
+    }
+
+    reg_state[from_r1->value()] = reg_writable;
+  }
+  sig_index = stream.sig_cc_index();
+  from_index = stream.regs_cc_index();
+
+  assert(reg_state[to->value()] == reg_writable, "must have already been read");
+  bool success = move_helper(val_obj->as_VMReg(), to, T_OBJECT, reg_state, ret_off, extra_stack_offset);
+  assert(success, "to register must be writeable");
+
+  return true;
+}
+
+// Unpack all value type arguments passed as oops
+void MacroAssembler::unpack_value_args(Compile* C, bool receiver_only) {
+  int sp_inc = unpack_value_args_common(C, receiver_only);
+  // Emit code for verified entry and save increment for stack repair on return
+  verified_entry(C, sp_inc);
+}
+
+int MacroAssembler::shuffle_value_args(bool is_packing, bool receiver_only, int extra_stack_offset,
+                                       BasicType* sig_bt, const GrowableArray<SigEntry>* sig_cc,
+                                       int args_passed, int args_on_stack, VMRegPair* regs,            // from
+                                       int args_passed_to, int args_on_stack_to, VMRegPair* regs_to) { // to
+  // Check if we need to extend the stack for packing/unpacking
+  int sp_inc = (args_on_stack_to - args_on_stack) * VMRegImpl::stack_slot_size;
+  if (sp_inc > 0) {
+    sp_inc = align_up(sp_inc, StackAlignmentInBytes);
+    if (!is_packing) {
+      // Save the return address, adjust the stack (make sure it is properly
+      // 16-byte aligned) and copy the return address to the new top of the stack.
+      // (Note: C1 does this in C1_MacroAssembler::scalarized_entry).
+      // FIXME: We need not to preserve return address on aarch64
+      pop(rscratch1);
+      sub(sp, sp, sp_inc);
+      push(rscratch1);
+    }
+  } else {
+    // The scalarized calling convention needs less stack space than the unscalarized one.
+    // No need to extend the stack, the caller will take care of these adjustments.
+    sp_inc = 0;
+  }
+
+  int ret_off; // make sure we don't overwrite the return address
+  if (is_packing) {
+    // For C1 code, the VVEP doesn't have reserved slots, so we store the returned address at
+    // rsp[0] during shuffling.
+    ret_off = 0;
+  } else {
+    // C2 code ensures that sp_inc is a reserved slot.
+    ret_off = sp_inc;
+  }
+
+  return shuffle_value_args_common(is_packing, receiver_only, extra_stack_offset,
+                                   sig_bt, sig_cc,
+                                   args_passed, args_on_stack, regs,
+                                   args_passed_to, args_on_stack_to, regs_to,
+                                   sp_inc, ret_off);
+}
+
+VMReg MacroAssembler::spill_reg_for(VMReg reg) {
+  return (reg->is_FloatRegister()) ? v0->as_VMReg() : r14->as_VMReg();
+}
+
 void MacroAssembler::cache_wb(Address line) {
   assert(line.getMode() == Address::base_plus_offset, "mode should be base_plus_offset");
   assert(line.index() == noreg, "index should be noreg");
   assert(line.offset() == 0, "offset should be 0");
   // would like to assert this
diff a/src/hotspot/cpu/ppc/globals_ppc.hpp b/src/hotspot/cpu/ppc/globals_ppc.hpp
--- a/src/hotspot/cpu/ppc/globals_ppc.hpp
+++ b/src/hotspot/cpu/ppc/globals_ppc.hpp
@@ -65,10 +65,13 @@
 define_pd_global(bool, RewriteBytecodes,      true);
 define_pd_global(bool, RewriteFrequentPairs,  true);
 
 define_pd_global(bool, PreserveFramePointer,  false);
 
+define_pd_global(bool, ValueTypePassFieldsAsArgs, false);
+define_pd_global(bool, ValueTypeReturnedAsFields, false);
+
 define_pd_global(uintx, TypeProfileLevel, 111);
 
 define_pd_global(bool, CompactStrings, true);
 
 // 2x unrolled loop is shorter with more than 9 HeapWords.
diff a/src/hotspot/cpu/x86/methodHandles_x86.cpp b/src/hotspot/cpu/x86/methodHandles_x86.cpp
--- a/src/hotspot/cpu/x86/methodHandles_x86.cpp
+++ b/src/hotspot/cpu/x86/methodHandles_x86.cpp
@@ -146,11 +146,15 @@
     __ jccb(Assembler::zero, run_compiled_code);
     __ jmp(Address(method, Method::interpreter_entry_offset()));
     __ BIND(run_compiled_code);
   }
 
-  const ByteSize entry_offset = for_compiler_entry ? Method::from_compiled_offset() :
+  // The following jump might pass a value type argument that was erased to Object as oop to a
+  // callee that expects value type arguments to be passed as fields. We need to call the compiled
+  // value entry (_code->value_entry_point() or _adapter->c2i_value_entry()) which will take care
+  // of translating between the calling conventions.
+  const ByteSize entry_offset = for_compiler_entry ? Method::from_compiled_value_offset() :
                                                      Method::from_interpreted_offset();
   __ jmp(Address(method, entry_offset));
 
   __ bind(L_no_such_method);
   __ jump(RuntimeAddress(StubRoutines::throw_AbstractMethodError_entry()));
diff a/src/hotspot/cpu/x86/x86_32.ad b/src/hotspot/cpu/x86/x86_32.ad
--- a/src/hotspot/cpu/x86/x86_32.ad
+++ b/src/hotspot/cpu/x86/x86_32.ad
@@ -608,14 +608,11 @@
 
 void MachPrologNode::emit(CodeBuffer &cbuf, PhaseRegAlloc *ra_) const {
   Compile* C = ra_->C;
   MacroAssembler _masm(&cbuf);
 
-  int framesize = C->output()->frame_size_in_bytes();
-  int bangsize = C->output()->bang_size_in_bytes();
-
-  __ verified_entry(framesize, C->output()->need_stack_bang(bangsize)?bangsize:0, C->in_24_bit_fp_mode(), C->stub_function() != NULL);
+  __ verified_entry(C);
 
   C->output()->set_frame_complete(cbuf.insts_size());
 
   if (C->has_mach_constant_base_node()) {
     // NOTE: We set the table base offset here because users might be
diff a/src/hotspot/cpu/x86/x86_64.ad b/src/hotspot/cpu/x86/x86_64.ad
--- a/src/hotspot/cpu/x86/x86_64.ad
+++ b/src/hotspot/cpu/x86/x86_64.ad
@@ -865,13 +865,10 @@
 
 void MachPrologNode::emit(CodeBuffer &cbuf, PhaseRegAlloc *ra_) const {
   Compile* C = ra_->C;
   MacroAssembler _masm(&cbuf);
 
-  int framesize = C->output()->frame_size_in_bytes();
-  int bangsize = C->output()->bang_size_in_bytes();
-
   if (C->clinit_barrier_on_entry()) {
     assert(VM_Version::supports_fast_class_init_checks(), "sanity");
     assert(!C->method()->holder()->is_not_initialized(), "initialization should have been started");
 
     Label L_skip_barrier;
@@ -883,11 +880,17 @@
     __ jump(RuntimeAddress(SharedRuntime::get_handle_wrong_method_stub())); // slow path
 
     __ bind(L_skip_barrier);
   }
 
-  __ verified_entry(framesize, C->output()->need_stack_bang(bangsize)?bangsize:0, false, C->stub_function() != NULL);
+  __ verified_entry(C);
+  __ bind(*_verified_entry);
+
+  if (C->stub_function() == NULL) {
+    BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();
+    bs->nmethod_entry_barrier(&_masm);
+  }
 
   C->output()->set_frame_complete(cbuf.insts_size());
 
   if (C->has_mach_constant_base_node()) {
     // NOTE: We set the table base offset here because users might be
@@ -895,16 +898,10 @@
     ConstantTable& constant_table = C->output()->constant_table();
     constant_table.set_table_base_offset(constant_table.calculate_table_base_offset());
   }
 }
 
-uint MachPrologNode::size(PhaseRegAlloc* ra_) const
-{
-  return MachNode::size(ra_); // too many variables; just compute it
-                              // the hard way
-}
-
 int MachPrologNode::reloc() const
 {
   return 0; // a large enough number
 }
 
@@ -948,33 +945,13 @@
     // Clear upper bits of YMM registers when current compiled code uses
     // wide vectors to avoid AVX <-> SSE transition penalty during call.
     __ vzeroupper();
   }
 
-  int framesize = C->output()->frame_size_in_bytes();
-  assert((framesize & (StackAlignmentInBytes-1)) == 0, "frame size not aligned");
-  // Remove word for return adr already pushed
-  // and RBP
-  framesize -= 2*wordSize;
-
-  // Note that VerifyStackAtCalls' Majik cookie does not change the frame size popped here
-
-  if (framesize) {
-    emit_opcode(cbuf, Assembler::REX_W);
-    if (framesize < 0x80) {
-      emit_opcode(cbuf, 0x83); // addq rsp, #framesize
-      emit_rm(cbuf, 0x3, 0x00, RSP_enc);
-      emit_d8(cbuf, framesize);
-    } else {
-      emit_opcode(cbuf, 0x81); // addq rsp, #framesize
-      emit_rm(cbuf, 0x3, 0x00, RSP_enc);
-      emit_d32(cbuf, framesize);
-    }
-  }
-
-  // popq rbp
-  emit_opcode(cbuf, 0x58 | RBP_enc);
+  // Subtract two words to account for return address and rbp
+  int initial_framesize = C->output()->frame_size_in_bytes() - 2*wordSize;
+  __ remove_frame(initial_framesize, C->needs_stack_repair(), C->output()->sp_inc_offset());
 
   if (StackReservedPages > 0 && C->has_reserved_stack_access()) {
     __ reserved_stack_check();
   }
 
@@ -984,16 +961,10 @@
     __ relocate(relocInfo::poll_return_type);
     __ testl(rax, Address(rscratch1, 0));
   }
 }
 
-uint MachEpilogNode::size(PhaseRegAlloc* ra_) const
-{
-  return MachNode::size(ra_); // too many variables; just compute it
-                              // the hard way
-}
-
 int MachEpilogNode::reloc() const
 {
   return 2; // a large enough number
 }
 
@@ -1525,10 +1496,38 @@
 {
   int offset = ra_->reg2offset(in_RegMask(0).find_first_elem());
   return (offset < 0x80) ? 5 : 8; // REX
 }
 
+//=============================================================================
+#ifndef PRODUCT
+void MachVEPNode::format(PhaseRegAlloc* ra_, outputStream* st) const
+{
+  st->print_cr("MachVEPNode");
+}
+#endif
+
+void MachVEPNode::emit(CodeBuffer& cbuf, PhaseRegAlloc* ra_) const
+{
+  MacroAssembler masm(&cbuf);
+  if (!_verified) {  
+    uint insts_size = cbuf.insts_size();
+    if (UseCompressedClassPointers) {
+      masm.load_klass(rscratch1, j_rarg0);
+      masm.cmpptr(rax, rscratch1);
+    } else {
+      masm.cmpptr(rax, Address(j_rarg0, oopDesc::klass_offset_in_bytes()));
+    }
+    masm.jump_cc(Assembler::notEqual, RuntimeAddress(SharedRuntime::get_ic_miss_stub()));
+  } else {
+    // Unpack value type args passed as oop and then jump to
+    // the verified entry point (skipping the unverified entry).
+    masm.unpack_value_args(ra_->C, _receiver_only);
+    masm.jmp(*_verified_entry);
+  }
+}
+
 //=============================================================================
 #ifndef PRODUCT
 void MachUEPNode::format(PhaseRegAlloc* ra_, outputStream* st) const
 {
   if (UseCompressedClassPointers) {
@@ -1567,17 +1566,10 @@
   nops_cnt &= 0x3; // Do not add nops if code is aligned.
   if (nops_cnt > 0)
     masm.nop(nops_cnt);
 }
 
-uint MachUEPNode::size(PhaseRegAlloc* ra_) const
-{
-  return MachNode::size(ra_); // too many variables; just compute it
-                              // the hard way
-}
-
-
 //=============================================================================
 
 int Matcher::regnum_to_fpu_offset(int regnum)
 {
   return regnum - 32; // The FP registers are in the second chunk
@@ -3859,10 +3851,26 @@
     scale($scale);
     disp($off);
   %}
 %}
 
+// Indirect Narrow Oop Operand
+operand indCompressedOop(rRegN reg) %{
+  predicate(UseCompressedOops && (CompressedOops::shift() == Address::times_8));
+  constraint(ALLOC_IN_RC(ptr_reg));
+  match(DecodeN reg);
+
+  op_cost(10);
+  format %{"[R12 + $reg << 3] (compressed oop addressing)" %}
+  interface(MEMORY_INTER) %{
+    base(0xc); // R12
+    index($reg);
+    scale(0x3);
+    disp(0x0);
+  %}
+%}
+
 // Indirect Narrow Oop Plus Offset Operand
 // Note: x86 architecture doesn't support "scale * index + offset" without a base
 // we can't free r12 even with CompressedOops::base() == NULL.
 operand indCompressedOopOffset(rRegN reg, immL32 off) %{
   predicate(UseCompressedOops && (CompressedOops::shift() == Address::times_8));
@@ -4201,11 +4209,11 @@
 // multiple operand types with the same basic encoding and format.  The classic
 // case of this is memory operands.
 
 opclass memory(indirect, indOffset8, indOffset32, indIndexOffset, indIndex,
                indIndexScale, indPosIndexScale, indIndexScaleOffset, indPosIndexOffset, indPosIndexScaleOffset,
-               indCompressedOopOffset,
+               indCompressedOop, indCompressedOopOffset,
                indirectNarrow, indOffset8Narrow, indOffset32Narrow,
                indIndexOffsetNarrow, indIndexNarrow, indIndexScaleNarrow,
                indIndexScaleOffsetNarrow, indPosIndexOffsetNarrow, indPosIndexScaleOffsetNarrow);
 
 //----------PIPELINE-----------------------------------------------------------
@@ -6685,10 +6693,23 @@
     }
   %}
   ins_pipe(ialu_reg_reg); // XXX
 %}
 
+instruct castN2X(rRegL dst, rRegN src)
+%{
+  match(Set dst (CastP2X src));
+
+  format %{ "movq    $dst, $src\t# ptr -> long" %}
+  ins_encode %{
+    if ($dst$$reg != $src$$reg) {
+      __ movptr($dst$$Register, $src$$Register);
+    }
+  %}
+  ins_pipe(ialu_reg_reg); // XXX
+%}
+
 instruct castP2X(rRegL dst, rRegP src)
 %{
   match(Set dst (CastP2X src));
 
   format %{ "movq    $dst, $src\t# ptr -> long" %}
@@ -6698,10 +6719,37 @@
     }
   %}
   ins_pipe(ialu_reg_reg); // XXX
 %}
 
+instruct castN2I(rRegI dst, rRegN src)
+%{
+  match(Set dst (CastN2I src));
+
+  format %{ "movl    $dst, $src\t# compressed ptr -> int" %}
+  ins_encode %{
+    if ($dst$$reg != $src$$reg) {
+      __ movl($dst$$Register, $src$$Register);
+    }
+  %}
+  ins_pipe(ialu_reg_reg); // XXX
+%}
+
+instruct castI2N(rRegN dst, rRegI src)
+%{
+  match(Set dst (CastI2N src));
+
+  format %{ "movl    $dst, $src\t# int -> compressed ptr" %}
+  ins_encode %{
+    if ($dst$$reg != $src$$reg) {
+      __ movl($dst$$Register, $src$$Register);
+    }
+  %}
+  ins_pipe(ialu_reg_reg); // XXX
+%}
+
+
 // Convert oop into int for vectors alignment masking
 instruct convP2I(rRegI dst, rRegP src)
 %{
   match(Set dst (ConvL2I (CastP2X src)));
 
@@ -10918,19 +10966,18 @@
 %}
 
 
 // =======================================================================
 // fast clearing of an array
-instruct rep_stos(rcx_RegL cnt, rdi_RegP base, regD tmp, rax_RegI zero,
+instruct rep_stos(rcx_RegL cnt, rdi_RegP base, regD tmp, rax_RegL val,
                   Universe dummy, rFlagsReg cr)
 %{
-  predicate(!((ClearArrayNode*)n)->is_large());
-  match(Set dummy (ClearArray cnt base));
-  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, KILL zero, KILL cr);
+  predicate(!((ClearArrayNode*)n)->is_large() && !((ClearArrayNode*)n)->word_copy_only());
+  match(Set dummy (ClearArray (Binary cnt base) val));
+  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, KILL cr);
 
   format %{ $$template
-    $$emit$$"xorq    rax, rax\t# ClearArray:\n\t"
     $$emit$$"cmp     InitArrayShortSize,rcx\n\t"
     $$emit$$"jg      LARGE\n\t"
     $$emit$$"dec     rcx\n\t"
     $$emit$$"js      DONE\t# Zero length\n\t"
     $$emit$$"mov     rax,(rdi,rcx,8)\t# LOOP\n\t"
@@ -10940,23 +10987,24 @@
     $$emit$$"# LARGE:\n\t"
     if (UseFastStosb) {
        $$emit$$"shlq    rcx,3\t# Convert doublewords to bytes\n\t"
        $$emit$$"rep     stosb\t# Store rax to *rdi++ while rcx--\n\t"
     } else if (UseXMMForObjInit) {
-       $$emit$$"mov     rdi,rax\n\t"
-       $$emit$$"vpxor   ymm0,ymm0,ymm0\n\t"
+       $$emit$$"movdq   $tmp, $val\n\t"
+       $$emit$$"punpcklqdq $tmp, $tmp\n\t"
+       $$emit$$"vinserti128_high $tmp, $tmp\n\t"
        $$emit$$"jmpq    L_zero_64_bytes\n\t"
        $$emit$$"# L_loop:\t# 64-byte LOOP\n\t"
-       $$emit$$"vmovdqu ymm0,(rax)\n\t"
-       $$emit$$"vmovdqu ymm0,0x20(rax)\n\t"
+       $$emit$$"vmovdqu $tmp,(rax)\n\t"
+       $$emit$$"vmovdqu $tmp,0x20(rax)\n\t"
        $$emit$$"add     0x40,rax\n\t"
        $$emit$$"# L_zero_64_bytes:\n\t"
        $$emit$$"sub     0x8,rcx\n\t"
        $$emit$$"jge     L_loop\n\t"
        $$emit$$"add     0x4,rcx\n\t"
        $$emit$$"jl      L_tail\n\t"
-       $$emit$$"vmovdqu ymm0,(rax)\n\t"
+       $$emit$$"vmovdqu $tmp,(rax)\n\t"
        $$emit$$"add     0x20,rax\n\t"
        $$emit$$"sub     0x4,rcx\n\t"
        $$emit$$"# L_tail:\t# Clearing tail bytes\n\t"
        $$emit$$"add     0x4,rcx\n\t"
        $$emit$$"jle     L_end\n\t"
@@ -10971,42 +11019,98 @@
        $$emit$$"rep     stosq\t# Store rax to *rdi++ while rcx--\n\t"
     }
     $$emit$$"# DONE"
   %}
   ins_encode %{
-    __ clear_mem($base$$Register, $cnt$$Register, $zero$$Register,
-                 $tmp$$XMMRegister, false);
+    __ clear_mem($base$$Register, $cnt$$Register, $val$$Register,
+                 $tmp$$XMMRegister, false, false);
   %}
   ins_pipe(pipe_slow);
 %}
 
-instruct rep_stos_large(rcx_RegL cnt, rdi_RegP base, regD tmp, rax_RegI zero,
+instruct rep_stos_word_copy(rcx_RegL cnt, rdi_RegP base, regD tmp, rax_RegL val,
+                  Universe dummy, rFlagsReg cr)
+%{
+  predicate(!((ClearArrayNode*)n)->is_large() && ((ClearArrayNode*)n)->word_copy_only());
+  match(Set dummy (ClearArray (Binary cnt base) val));
+  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, KILL cr);
+
+  format %{ $$template
+    $$emit$$"cmp     InitArrayShortSize,rcx\n\t"
+    $$emit$$"jg      LARGE\n\t"
+    $$emit$$"dec     rcx\n\t"
+    $$emit$$"js      DONE\t# Zero length\n\t"
+    $$emit$$"mov     rax,(rdi,rcx,8)\t# LOOP\n\t"
+    $$emit$$"dec     rcx\n\t"
+    $$emit$$"jge     LOOP\n\t"
+    $$emit$$"jmp     DONE\n\t"
+    $$emit$$"# LARGE:\n\t"
+    if (UseXMMForObjInit) {
+       $$emit$$"movdq   $tmp, $val\n\t"
+       $$emit$$"punpcklqdq $tmp, $tmp\n\t"
+       $$emit$$"vinserti128_high $tmp, $tmp\n\t"
+       $$emit$$"jmpq    L_zero_64_bytes\n\t"
+       $$emit$$"# L_loop:\t# 64-byte LOOP\n\t"
+       $$emit$$"vmovdqu $tmp,(rax)\n\t"
+       $$emit$$"vmovdqu $tmp,0x20(rax)\n\t"
+       $$emit$$"add     0x40,rax\n\t"
+       $$emit$$"# L_zero_64_bytes:\n\t"
+       $$emit$$"sub     0x8,rcx\n\t"
+       $$emit$$"jge     L_loop\n\t"
+       $$emit$$"add     0x4,rcx\n\t"
+       $$emit$$"jl      L_tail\n\t"
+       $$emit$$"vmovdqu $tmp,(rax)\n\t"
+       $$emit$$"add     0x20,rax\n\t"
+       $$emit$$"sub     0x4,rcx\n\t"
+       $$emit$$"# L_tail:\t# Clearing tail bytes\n\t"
+       $$emit$$"add     0x4,rcx\n\t"
+       $$emit$$"jle     L_end\n\t"
+       $$emit$$"dec     rcx\n\t"
+       $$emit$$"# L_sloop:\t# 8-byte short loop\n\t"
+       $$emit$$"vmovq   xmm0,(rax)\n\t"
+       $$emit$$"add     0x8,rax\n\t"
+       $$emit$$"dec     rcx\n\t"
+       $$emit$$"jge     L_sloop\n\t"
+       $$emit$$"# L_end:\n\t"
+    } else {
+       $$emit$$"rep     stosq\t# Store rax to *rdi++ while rcx--\n\t"
+    }
+    $$emit$$"# DONE"
+  %}
+  ins_encode %{
+    __ clear_mem($base$$Register, $cnt$$Register, $val$$Register,
+                 $tmp$$XMMRegister, false, true);
+  %}
+  ins_pipe(pipe_slow);
+%}
+
+instruct rep_stos_large(rcx_RegL cnt, rdi_RegP base, regD tmp, rax_RegL val,
                         Universe dummy, rFlagsReg cr)
 %{
-  predicate(((ClearArrayNode*)n)->is_large());
-  match(Set dummy (ClearArray cnt base));
-  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, KILL zero, KILL cr);
+  predicate(((ClearArrayNode*)n)->is_large() && !((ClearArrayNode*)n)->word_copy_only());
+  match(Set dummy (ClearArray (Binary cnt base) val));
+  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, KILL cr);
 
   format %{ $$template
     if (UseFastStosb) {
-       $$emit$$"xorq    rax, rax\t# ClearArray:\n\t"
        $$emit$$"shlq    rcx,3\t# Convert doublewords to bytes\n\t"
        $$emit$$"rep     stosb\t# Store rax to *rdi++ while rcx--"
     } else if (UseXMMForObjInit) {
-       $$emit$$"mov     rdi,rax\t# ClearArray:\n\t"
-       $$emit$$"vpxor   ymm0,ymm0,ymm0\n\t"
+       $$emit$$"movdq   $tmp, $val\n\t"
+       $$emit$$"punpcklqdq $tmp, $tmp\n\t"
+       $$emit$$"vinserti128_high $tmp, $tmp\n\t"
        $$emit$$"jmpq    L_zero_64_bytes\n\t"
        $$emit$$"# L_loop:\t# 64-byte LOOP\n\t"
-       $$emit$$"vmovdqu ymm0,(rax)\n\t"
-       $$emit$$"vmovdqu ymm0,0x20(rax)\n\t"
+       $$emit$$"vmovdqu $tmp,(rax)\n\t"
+       $$emit$$"vmovdqu $tmp,0x20(rax)\n\t"
        $$emit$$"add     0x40,rax\n\t"
        $$emit$$"# L_zero_64_bytes:\n\t"
        $$emit$$"sub     0x8,rcx\n\t"
        $$emit$$"jge     L_loop\n\t"
        $$emit$$"add     0x4,rcx\n\t"
        $$emit$$"jl      L_tail\n\t"
-       $$emit$$"vmovdqu ymm0,(rax)\n\t"
+       $$emit$$"vmovdqu $tmp,(rax)\n\t"
        $$emit$$"add     0x20,rax\n\t"
        $$emit$$"sub     0x4,rcx\n\t"
        $$emit$$"# L_tail:\t# Clearing tail bytes\n\t"
        $$emit$$"add     0x4,rcx\n\t"
        $$emit$$"jle     L_end\n\t"
@@ -11016,17 +11120,62 @@
        $$emit$$"add     0x8,rax\n\t"
        $$emit$$"dec     rcx\n\t"
        $$emit$$"jge     L_sloop\n\t"
        $$emit$$"# L_end:\n\t"
     } else {
-       $$emit$$"xorq    rax, rax\t# ClearArray:\n\t"
        $$emit$$"rep     stosq\t# Store rax to *rdi++ while rcx--"
     }
   %}
   ins_encode %{
-    __ clear_mem($base$$Register, $cnt$$Register, $zero$$Register,
-                 $tmp$$XMMRegister, true);
+    __ clear_mem($base$$Register, $cnt$$Register, $val$$Register,
+                 $tmp$$XMMRegister, true, false);
+  %}
+  ins_pipe(pipe_slow);
+%}
+
+instruct rep_stos_large_word_copy(rcx_RegL cnt, rdi_RegP base, regD tmp, rax_RegL val, 
+                        Universe dummy, rFlagsReg cr)
+%{
+  predicate(((ClearArrayNode*)n)->is_large() && ((ClearArrayNode*)n)->word_copy_only());
+  match(Set dummy (ClearArray (Binary cnt base) val));
+  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, KILL cr);
+
+  format %{ $$template
+    if (UseXMMForObjInit) {
+       $$emit$$"movdq   $tmp, $val\n\t"
+       $$emit$$"punpcklqdq $tmp, $tmp\n\t"
+       $$emit$$"vinserti128_high $tmp, $tmp\n\t"
+       $$emit$$"jmpq    L_zero_64_bytes\n\t"
+       $$emit$$"# L_loop:\t# 64-byte LOOP\n\t"
+       $$emit$$"vmovdqu $tmp,(rax)\n\t"
+       $$emit$$"vmovdqu $tmp,0x20(rax)\n\t"
+       $$emit$$"add     0x40,rax\n\t"
+       $$emit$$"# L_zero_64_bytes:\n\t"
+       $$emit$$"sub     0x8,rcx\n\t"
+       $$emit$$"jge     L_loop\n\t"
+       $$emit$$"add     0x4,rcx\n\t"
+       $$emit$$"jl      L_tail\n\t"
+       $$emit$$"vmovdqu $tmp,(rax)\n\t"
+       $$emit$$"add     0x20,rax\n\t"
+       $$emit$$"sub     0x4,rcx\n\t"
+       $$emit$$"# L_tail:\t# Clearing tail bytes\n\t"
+       $$emit$$"add     0x4,rcx\n\t"
+       $$emit$$"jle     L_end\n\t"
+       $$emit$$"dec     rcx\n\t"
+       $$emit$$"# L_sloop:\t# 8-byte short loop\n\t"
+       $$emit$$"vmovq   xmm0,(rax)\n\t"
+       $$emit$$"add     0x8,rax\n\t"
+       $$emit$$"dec     rcx\n\t"
+       $$emit$$"jge     L_sloop\n\t"
+       $$emit$$"# L_end:\n\t"
+    } else {
+       $$emit$$"rep     stosq\t# Store rax to *rdi++ while rcx--"
+    }
+  %}
+  ins_encode %{
+    __ clear_mem($base$$Register, $cnt$$Register, $val$$Register, 
+                 $tmp$$XMMRegister, true, true);
   %}
   ins_pipe(pipe_slow);
 %}
 
 instruct string_compareL(rdi_RegP str1, rcx_RegI cnt1, rsi_RegP str2, rdx_RegI cnt2,
@@ -11587,10 +11736,21 @@
   opcode(0x85);
   ins_encode(REX_reg_mem(src, mem), OpcP, reg_mem(src, mem));
   ins_pipe(ialu_cr_reg_mem);
 %}
 
+// Fold array properties check
+instruct testI_mem_imm(rFlagsReg cr, memory mem, immI con, immI0 zero)
+%{
+  match(Set cr (CmpI (AndI (CastN2I (LoadNKlass mem)) con) zero));
+
+  format %{ "testl   $mem, $con" %}
+  opcode(0xF7, 0x00);
+  ins_encode(REX_mem(mem), OpcP, RM_opc_mem(0x00, mem), Con32(con));
+  ins_pipe(ialu_mem_imm);
+%}
+
 // Unsigned compare Instructions; really, same as signed except they
 // produce an rFlagsRegU instead of rFlagsReg.
 instruct compU_rReg(rFlagsRegU cr, rRegI op1, rRegI op2)
 %{
   match(Set cr (CmpU op1 op2));
@@ -11900,10 +12060,21 @@
   opcode(0x85);
   ins_encode(REX_reg_mem_wide(src, mem), OpcP, reg_mem(src, mem));
   ins_pipe(ialu_cr_reg_mem);
 %}
 
+// Fold array properties check
+instruct testL_reg_mem3(rFlagsReg cr, memory mem, rRegL src, immL0 zero)
+%{
+  match(Set cr (CmpL (AndL (CastP2X (LoadKlass mem)) src) zero));
+
+  format %{ "testq   $src, $mem\t# test array properties" %}
+  opcode(0x85);
+  ins_encode(REX_reg_mem_wide(src, mem), OpcP, reg_mem(src, mem));
+  ins_pipe(ialu_cr_reg_mem);
+%}
+
 // Manifest a CmpL result in an integer register.  Very painful.
 // This is the test to avoid.
 instruct cmpL3_reg_reg(rRegI dst, rRegL src1, rRegL src2, rFlagsReg flags)
 %{
   match(Set dst (CmpL3 src1 src2));
@@ -12567,12 +12738,28 @@
   ins_encode(clear_avx, Java_To_Runtime(meth));
   ins_pipe(pipe_slow);
 %}
 
 // Call runtime without safepoint
+// entry point is null, target holds the address to call
+instruct CallLeafNoFPInDirect(rRegP target)
+%{
+  predicate(n->as_Call()->entry_point() == NULL);
+  match(CallLeafNoFP target);
+
+  ins_cost(300);
+  format %{ "call_leaf_nofp,runtime indirect " %}
+  ins_encode %{
+     __ call($target$$Register);
+  %}
+
+  ins_pipe(pipe_slow);
+%}
+
 instruct CallLeafNoFPDirect(method meth)
 %{
+  predicate(n->as_Call()->entry_point() != NULL);
   match(CallLeafNoFP);
   effect(USE meth);
 
   ins_cost(300);
   format %{ "call_leaf_nofp,runtime " %}
diff a/src/hotspot/share/c1/c1_LinearScan.cpp b/src/hotspot/share/c1/c1_LinearScan.cpp
--- a/src/hotspot/share/c1/c1_LinearScan.cpp
+++ b/src/hotspot/share/c1/c1_LinearScan.cpp
@@ -59,13 +59,13 @@
 
 #endif
 
 // Map BasicType to spill size in 32-bit words, matching VMReg's notion of words
 #ifdef _LP64
-static int type2spill_size[T_CONFLICT+1]={ -1, 0, 0, 0, 1, 1, 1, 2, 1, 1, 1, 2, 2, 2, 0, 2,  1, 2, 1, -1};
+static int type2spill_size[T_CONFLICT+1]={ -1, 0, 0, 0, 1, 1, 1, 2, 1, 1, 1, 2, 2, 2, 0, 2,  1, 2, 1, 2, -1};
 #else
-static int type2spill_size[T_CONFLICT+1]={ -1, 0, 0, 0, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 0, 1, -1, 1, 1, -1};
+static int type2spill_size[T_CONFLICT+1]={ -1, 0, 0, 0, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 0, 1, -1, 1, 1, 1, -1};
 #endif
 
 
 // Implementation of LinearScan
 
@@ -256,11 +256,11 @@
     it->assign_reg(spill);
   }
 }
 
 void LinearScan::propagate_spill_slots() {
-  if (!frame_map()->finalize_frame(max_spills())) {
+  if (!frame_map()->finalize_frame(max_spills(), compilation()->needs_stack_repair())) {
     bailout("frame too large");
   }
 }
 
 // create a new interval with a predefined reg_num
@@ -2947,11 +2947,11 @@
     for (int i = 0; i < nof_locks; i++) {
       monitors->append(location_for_monitor_index(lock_offset + i));
     }
   }
 
-  return new IRScopeDebugInfo(cur_scope, cur_state->bci(), locals, expressions, monitors, caller_debug_info);
+  return new IRScopeDebugInfo(cur_scope, cur_state->bci(), locals, expressions, monitors, caller_debug_info, cur_state->should_reexecute());
 }
 
 
 void LinearScan::compute_debug_info(CodeEmitInfo* info, int op_id) {
   TRACE_LINEAR_SCAN(3, tty->print_cr("creating debug information at op_id %d", op_id));
diff a/src/hotspot/share/c1/c1_Runtime1.cpp b/src/hotspot/share/c1/c1_Runtime1.cpp
--- a/src/hotspot/share/c1/c1_Runtime1.cpp
+++ b/src/hotspot/share/c1/c1_Runtime1.cpp
@@ -52,10 +52,12 @@
 #include "memory/universe.hpp"
 #include "oops/access.inline.hpp"
 #include "oops/objArrayOop.inline.hpp"
 #include "oops/objArrayKlass.hpp"
 #include "oops/oop.inline.hpp"
+#include "oops/valueArrayKlass.hpp"
+#include "oops/valueArrayOop.inline.hpp"
 #include "runtime/atomic.hpp"
 #include "runtime/biasedLocking.hpp"
 #include "runtime/fieldDescriptor.inline.hpp"
 #include "runtime/frame.inline.hpp"
 #include "runtime/handles.inline.hpp"
@@ -116,21 +118,28 @@
 int Runtime1::_arraycopy_slowcase_cnt = 0;
 int Runtime1::_arraycopy_checkcast_cnt = 0;
 int Runtime1::_arraycopy_checkcast_attempt_cnt = 0;
 int Runtime1::_new_type_array_slowcase_cnt = 0;
 int Runtime1::_new_object_array_slowcase_cnt = 0;
+int Runtime1::_new_value_array_slowcase_cnt = 0;
 int Runtime1::_new_instance_slowcase_cnt = 0;
 int Runtime1::_new_multi_array_slowcase_cnt = 0;
+int Runtime1::_load_flattened_array_slowcase_cnt = 0;
+int Runtime1::_store_flattened_array_slowcase_cnt = 0;
+int Runtime1::_substitutability_check_slowcase_cnt = 0;
+int Runtime1::_buffer_value_args_slowcase_cnt = 0;
+int Runtime1::_buffer_value_args_no_receiver_slowcase_cnt = 0;
 int Runtime1::_monitorenter_slowcase_cnt = 0;
 int Runtime1::_monitorexit_slowcase_cnt = 0;
 int Runtime1::_patch_code_slowcase_cnt = 0;
 int Runtime1::_throw_range_check_exception_count = 0;
 int Runtime1::_throw_index_exception_count = 0;
 int Runtime1::_throw_div0_exception_count = 0;
 int Runtime1::_throw_null_pointer_exception_count = 0;
 int Runtime1::_throw_class_cast_exception_count = 0;
 int Runtime1::_throw_incompatible_class_change_error_count = 0;
+int Runtime1::_throw_illegal_monitor_state_exception_count = 0;
 int Runtime1::_throw_array_store_exception_count = 0;
 int Runtime1::_throw_count = 0;
 
 static int _byte_arraycopy_stub_cnt = 0;
 static int _short_arraycopy_stub_cnt = 0;
@@ -390,21 +399,43 @@
   // Note: no handle for klass needed since they are not used
   //       anymore after new_objArray() and no GC can happen before.
   //       (This may have to change if this code changes!)
   assert(array_klass->is_klass(), "not a class");
   Handle holder(THREAD, array_klass->klass_holder()); // keep the klass alive
-  Klass* elem_klass = ObjArrayKlass::cast(array_klass)->element_klass();
+  Klass* elem_klass = ArrayKlass::cast(array_klass)->element_klass();
   objArrayOop obj = oopFactory::new_objArray(elem_klass, length, CHECK);
   thread->set_vm_result(obj);
   // This is pretty rare but this runtime patch is stressful to deoptimization
   // if we deoptimize here so force a deopt to stress the path.
   if (DeoptimizeALot) {
     deopt_caller();
   }
 JRT_END
 
 
+JRT_ENTRY(void, Runtime1::new_value_array(JavaThread* thread, Klass* array_klass, jint length))
+  NOT_PRODUCT(_new_value_array_slowcase_cnt++;)
+
+  // Note: no handle for klass needed since they are not used
+  //       anymore after new_objArray() and no GC can happen before.
+  //       (This may have to change if this code changes!)
+  assert(array_klass->is_klass(), "not a class");
+  Handle holder(THREAD, array_klass->klass_holder()); // keep the klass alive
+  Klass* elem_klass = ArrayKlass::cast(array_klass)->element_klass();
+  assert(elem_klass->is_value(), "must be");
+  // Logically creates elements, ensure klass init
+  elem_klass->initialize(CHECK);
+  arrayOop obj = oopFactory::new_valueArray(elem_klass, length, CHECK);
+  thread->set_vm_result(obj);
+  // This is pretty rare but this runtime patch is stressful to deoptimization
+  // if we deoptimize here so force a deopt to stress the path.
+  if (DeoptimizeALot) {
+    deopt_caller();
+  }
+JRT_END
+
+
 JRT_ENTRY(void, Runtime1::new_multi_array(JavaThread* thread, Klass* klass, int rank, jint* dims))
   NOT_PRODUCT(_new_multi_array_slowcase_cnt++;)
 
   assert(klass->is_klass(), "not a class");
   assert(rank >= 1, "rank must be nonzero");
@@ -412,10 +443,87 @@
   oop obj = ArrayKlass::cast(klass)->multi_allocate(rank, dims, CHECK);
   thread->set_vm_result(obj);
 JRT_END
 
 
+static void profile_flat_array(JavaThread* thread) {
+  ResourceMark rm(thread);
+  vframeStream vfst(thread, true);
+  assert(!vfst.at_end(), "Java frame must exist");
+  int bci = vfst.bci();
+  Method* method = vfst.method();
+  MethodData* md = method->method_data();
+  if (md != NULL) {
+    ProfileData* data = md->bci_to_data(bci);
+    assert(data != NULL && data->is_ArrayLoadStoreData(), "incorrect profiling entry");
+    ArrayLoadStoreData* load_store = (ArrayLoadStoreData*)data;
+    load_store->set_flat_array();
+  }
+}
+
+JRT_ENTRY(void, Runtime1::load_flattened_array(JavaThread* thread, valueArrayOopDesc* array, int index))
+  assert(array->klass()->is_valueArray_klass(), "should not be called");
+  profile_flat_array(thread);
+
+  NOT_PRODUCT(_load_flattened_array_slowcase_cnt++;)
+  assert(array->length() > 0 && index < array->length(), "already checked");
+  valueArrayHandle vah(thread, array);
+  oop obj = valueArrayOopDesc::value_alloc_copy_from_index(vah, index, CHECK);
+  thread->set_vm_result(obj);
+JRT_END
+
+
+JRT_ENTRY(void, Runtime1::store_flattened_array(JavaThread* thread, valueArrayOopDesc* array, int index, oopDesc* value))
+  if (array->klass()->is_valueArray_klass()) {
+    profile_flat_array(thread);
+  }
+
+  NOT_PRODUCT(_store_flattened_array_slowcase_cnt++;)
+  if (value == NULL) {
+    assert(array->klass()->is_valueArray_klass() || array->klass()->is_null_free_array_klass(), "should not be called");
+    SharedRuntime::throw_and_post_jvmti_exception(thread, vmSymbols::java_lang_NullPointerException());
+  } else {
+    assert(array->klass()->is_valueArray_klass(), "should not be called");
+    array->value_copy_to_index(value, index);
+  }
+JRT_END
+
+
+JRT_ENTRY(int, Runtime1::substitutability_check(JavaThread* thread, oopDesc* left, oopDesc* right))
+  NOT_PRODUCT(_substitutability_check_slowcase_cnt++;)
+  JavaCallArguments args;
+  args.push_oop(Handle(THREAD, left));
+  args.push_oop(Handle(THREAD, right));
+  JavaValue result(T_BOOLEAN);
+  JavaCalls::call_static(&result,
+                         SystemDictionary::ValueBootstrapMethods_klass(),
+                         vmSymbols::isSubstitutable_name(),
+                         vmSymbols::object_object_boolean_signature(),
+                         &args, CHECK_0);
+  return result.get_jboolean() ? 1 : 0;
+JRT_END
+
+
+extern "C" void ps();
+
+void Runtime1::buffer_value_args_impl(JavaThread* thread, Method* m, bool allocate_receiver) {
+  Thread* THREAD = thread;
+  methodHandle method(thread, m); // We are inside the verified_entry or verified_value_ro_entry of this method.
+  oop obj = SharedRuntime::allocate_value_types_impl(thread, method, allocate_receiver, CHECK);
+  thread->set_vm_result(obj);
+}
+
+JRT_ENTRY(void, Runtime1::buffer_value_args(JavaThread* thread, Method* method))
+  NOT_PRODUCT(_buffer_value_args_slowcase_cnt++;)
+  buffer_value_args_impl(thread, method, true);
+JRT_END
+
+JRT_ENTRY(void, Runtime1::buffer_value_args_no_receiver(JavaThread* thread, Method* method))
+  NOT_PRODUCT(_buffer_value_args_no_receiver_slowcase_cnt++;)
+  buffer_value_args_impl(thread, method, false);
+JRT_END
+
 JRT_ENTRY(void, Runtime1::unimplemented_entry(JavaThread* thread, StubID id))
   tty->print_cr("Runtime1::entry_for(%d) returned unimplemented entry point", id);
 JRT_END
 
 
@@ -697,10 +805,17 @@
   ResourceMark rm(thread);
   SharedRuntime::throw_and_post_jvmti_exception(thread, vmSymbols::java_lang_IncompatibleClassChangeError());
 JRT_END
 
 
+JRT_ENTRY(void, Runtime1::throw_illegal_monitor_state_exception(JavaThread* thread))
+  NOT_PRODUCT(_throw_illegal_monitor_state_exception_count++;)
+  ResourceMark rm(thread);
+  SharedRuntime::throw_and_post_jvmti_exception(thread, vmSymbols::java_lang_IllegalMonitorStateException());
+JRT_END
+
+
 JRT_BLOCK_ENTRY(void, Runtime1::monitorenter(JavaThread* thread, oopDesc* obj, BasicObjectLock* lock))
   NOT_PRODUCT(_monitorenter_slowcase_cnt++;)
   if (!UseFastLocking) {
     lock->set_obj(obj);
   }
@@ -941,13 +1056,22 @@
       case Bytecodes::_new:
         { Bytecode_new bnew(caller_method(), caller_method->bcp_from(bci));
           k = caller_method->constants()->klass_at(bnew.index(), CHECK);
         }
         break;
+      case Bytecodes::_defaultvalue:
+        { Bytecode_defaultvalue bdefaultvalue(caller_method(), caller_method->bcp_from(bci));
+          k = caller_method->constants()->klass_at(bdefaultvalue.index(), CHECK);
+        }
+        break;
       case Bytecodes::_multianewarray:
         { Bytecode_multianewarray mna(caller_method(), caller_method->bcp_from(bci));
           k = caller_method->constants()->klass_at(mna.index(), CHECK);
+          if (k->name()->is_Q_array_signature()) {
+            // Logically creates elements, ensure klass init
+            k->initialize(CHECK);
+          }
         }
         break;
       case Bytecodes::_instanceof:
         { Bytecode_instanceof io(caller_method(), caller_method->bcp_from(bci));
           k = caller_method->constants()->klass_at(io.index(), CHECK);
@@ -1465,22 +1589,30 @@
   tty->print_cr(" _arraycopy_checkcast_cnt:        %d", _arraycopy_checkcast_cnt);
   tty->print_cr(" _arraycopy_checkcast_attempt_cnt:%d", _arraycopy_checkcast_attempt_cnt);
 
   tty->print_cr(" _new_type_array_slowcase_cnt:    %d", _new_type_array_slowcase_cnt);
   tty->print_cr(" _new_object_array_slowcase_cnt:  %d", _new_object_array_slowcase_cnt);
+  tty->print_cr(" _new_value_array_slowcase_cnt:   %d", _new_value_array_slowcase_cnt);
   tty->print_cr(" _new_instance_slowcase_cnt:      %d", _new_instance_slowcase_cnt);
   tty->print_cr(" _new_multi_array_slowcase_cnt:   %d", _new_multi_array_slowcase_cnt);
+  tty->print_cr(" _load_flattened_array_slowcase_cnt:   %d", _load_flattened_array_slowcase_cnt);
+  tty->print_cr(" _store_flattened_array_slowcase_cnt:  %d", _store_flattened_array_slowcase_cnt);
+  tty->print_cr(" _substitutability_check_slowcase_cnt: %d", _substitutability_check_slowcase_cnt);
+  tty->print_cr(" _buffer_value_args_slowcase_cnt:%d", _buffer_value_args_slowcase_cnt);
+  tty->print_cr(" _buffer_value_args_no_receiver_slowcase_cnt:%d", _buffer_value_args_no_receiver_slowcase_cnt);
+
   tty->print_cr(" _monitorenter_slowcase_cnt:      %d", _monitorenter_slowcase_cnt);
   tty->print_cr(" _monitorexit_slowcase_cnt:       %d", _monitorexit_slowcase_cnt);
   tty->print_cr(" _patch_code_slowcase_cnt:        %d", _patch_code_slowcase_cnt);
 
   tty->print_cr(" _throw_range_check_exception_count:            %d:", _throw_range_check_exception_count);
   tty->print_cr(" _throw_index_exception_count:                  %d:", _throw_index_exception_count);
   tty->print_cr(" _throw_div0_exception_count:                   %d:", _throw_div0_exception_count);
   tty->print_cr(" _throw_null_pointer_exception_count:           %d:", _throw_null_pointer_exception_count);
   tty->print_cr(" _throw_class_cast_exception_count:             %d:", _throw_class_cast_exception_count);
   tty->print_cr(" _throw_incompatible_class_change_error_count:  %d:", _throw_incompatible_class_change_error_count);
+  tty->print_cr(" _throw_illegal_monitor_state_exception_count:  %d:", _throw_illegal_monitor_state_exception_count);
   tty->print_cr(" _throw_array_store_exception_count:            %d:", _throw_array_store_exception_count);
   tty->print_cr(" _throw_count:                                  %d:", _throw_count);
 
   SharedRuntime::print_ic_miss_histogram();
   tty->cr();
diff a/src/hotspot/share/ci/bcEscapeAnalyzer.cpp b/src/hotspot/share/ci/bcEscapeAnalyzer.cpp
--- a/src/hotspot/share/ci/bcEscapeAnalyzer.cpp
+++ b/src/hotspot/share/ci/bcEscapeAnalyzer.cpp
@@ -554,10 +554,13 @@
       case Bytecodes::_aastore:
       {
         set_global_escape(state.apop());
         state.spop();
         ArgumentMap arr = state.apop();
+        // If the array is flattened, a larger part of it is modified than
+        // the size of a reference. However, if OFFSET_ANY is given as
+        // parameter to set_modified(), size is not taken into account.
         set_modified(arr, OFFSET_ANY, type2size[T_OBJECT]*HeapWordSize);
         break;
       }
       case Bytecodes::_pop:
         state.raw_pop();
@@ -938,12 +941,28 @@
             state.lpush();
           }
         }
         break;
       case Bytecodes::_new:
+      case Bytecodes::_defaultvalue:
         state.apush(allocated_obj);
         break;
+      case Bytecodes::_withfield: {
+        bool will_link;
+        ciField* field = s.get_field(will_link);
+        BasicType field_type = field->type()->basic_type();
+        if (field_type == T_OBJECT || field_type == T_ARRAY) {
+          set_global_escape(state.apop());
+        } else if (type2size[field_type] == 1) {
+          state.spop();
+        } else {
+          state.lpop();
+        }
+        set_method_escape(state.apop());
+        state.apush(allocated_obj);
+        break;
+      }
       case Bytecodes::_newarray:
       case Bytecodes::_anewarray:
         state.spop();
         state.apush(allocated_obj);
         break;
diff a/src/hotspot/share/ci/ciField.cpp b/src/hotspot/share/ci/ciField.cpp
--- a/src/hotspot/share/ci/ciField.cpp
+++ b/src/hotspot/share/ci/ciField.cpp
@@ -65,11 +65,11 @@
 // decreases for complex compilation tasks.
 
 // ------------------------------------------------------------------
 // ciField::ciField
 ciField::ciField(ciInstanceKlass* klass, int index) :
-    _known_to_link_with_put(NULL), _known_to_link_with_get(NULL) {
+  _is_flattened(false), _known_to_link_with_put(NULL), _known_to_link_with_get(NULL) {
   ASSERT_IN_VM;
   CompilerThread *THREAD = CompilerThread::current();
 
   assert(ciObjectFactory::is_initialized(), "not a shared field");
 
@@ -99,10 +99,13 @@
     _type = ciType::make(field_type);
   }
 
   _name = (ciSymbol*)ciEnv::current(THREAD)->get_symbol(name);
 
+  // this is needed if the field class is not yet loaded.
+  _is_flattenable = _signature->is_Q_signature();
+
   // Get the field's declared holder.
   //
   // Note: we actually create a ciInstanceKlass for this klass,
   // even though we may not need to.
   int holder_index = cpool->klass_ref_index_at(index);
@@ -210,10 +213,33 @@
   // Either (a) it is marked shared, or else (b) we are done bootstrapping.
   assert(is_shared() || ciObjectFactory::is_initialized(),
          "bootstrap classes must not create & cache unshared fields");
 }
 
+// Special copy constructor used to flatten value type fields by
+// copying the fields of the value type to a new holder klass.
+ciField::ciField(ciField* field, ciInstanceKlass* holder, int offset, bool is_final) {
+  assert(field->holder()->is_valuetype(), "should only be used for value type field flattening");
+  // Set the is_final flag
+  jint final = is_final ? JVM_ACC_FINAL : ~JVM_ACC_FINAL;
+  AccessFlags flags(field->flags().as_int() & final);
+  _flags = ciFlags(flags);
+  _holder = holder;
+  _offset = offset;
+  // Copy remaining fields
+  _name = field->_name;
+  _signature = field->_signature;
+  _type = field->_type;
+  _is_constant = field->_is_constant;
+  _known_to_link_with_put = field->_known_to_link_with_put;
+  _known_to_link_with_get = field->_known_to_link_with_get;
+  _constant_value = field->_constant_value;
+  assert(!field->is_flattened(), "field must not be flattened");
+  _is_flattened = false;
+  _is_flattenable = field->is_flattenable();
+}
+
 static bool trust_final_non_static_fields(ciInstanceKlass* holder) {
   if (holder == NULL)
     return false;
   if (holder->name() == ciSymbol::java_lang_System())
     // Never trust strangely unstable finals:  System.out, etc.
@@ -250,10 +276,12 @@
   _flags = ciFlags(fd->access_flags());
   _offset = fd->offset();
   Klass* field_holder = fd->field_holder();
   assert(field_holder != NULL, "null field_holder");
   _holder = CURRENT_ENV->get_instance_klass(field_holder);
+  _is_flattened = fd->is_flattened();
+  _is_flattenable = fd->is_flattenable();
 
   // Check to see if the field is constant.
   Klass* k = _holder->get_Klass();
   bool is_stable_field = FoldStableValues && is_stable();
   if ((is_final() && !has_initialized_final_update()) || is_stable_field) {
@@ -362,12 +390,12 @@
 // link errors?
 bool ciField::will_link(ciMethod* accessing_method,
                         Bytecodes::Code bc) {
   VM_ENTRY_MARK;
   assert(bc == Bytecodes::_getstatic || bc == Bytecodes::_putstatic ||
-         bc == Bytecodes::_getfield  || bc == Bytecodes::_putfield,
-         "unexpected bytecode");
+         bc == Bytecodes::_getfield  || bc == Bytecodes::_putfield  ||
+         bc == Bytecodes::_withfield, "unexpected bytecode");
 
   if (_offset == -1) {
     // at creation we couldn't link to our holder so we need to
     // maintain that stance, otherwise there's no safe way to use this
     // ciField.
@@ -428,10 +456,12 @@
   tty->print(" is_constant=%s", bool_to_str(_is_constant));
   if (_is_constant && is_static()) {
     tty->print(" constant_value=");
     _constant_value.print();
   }
+  tty->print(" is_flattenable=%s", bool_to_str(_is_flattenable));
+  tty->print(" is_flattened=%s", bool_to_str(_is_flattened));
   tty->print(">");
 }
 
 // ------------------------------------------------------------------
 // ciField::print_name_on
diff a/src/hotspot/share/ci/ciInstanceKlass.cpp b/src/hotspot/share/ci/ciInstanceKlass.cpp
--- a/src/hotspot/share/ci/ciInstanceKlass.cpp
+++ b/src/hotspot/share/ci/ciInstanceKlass.cpp
@@ -25,16 +25,18 @@
 #include "precompiled.hpp"
 #include "ci/ciField.hpp"
 #include "ci/ciInstance.hpp"
 #include "ci/ciInstanceKlass.hpp"
 #include "ci/ciUtilities.inline.hpp"
+#include "ci/ciValueKlass.hpp"
 #include "classfile/systemDictionary.hpp"
 #include "memory/allocation.hpp"
 #include "memory/allocation.inline.hpp"
 #include "memory/resourceArea.hpp"
 #include "oops/oop.inline.hpp"
 #include "oops/fieldStreams.inline.hpp"
+#include "oops/valueKlass.inline.hpp"
 #include "runtime/fieldDescriptor.inline.hpp"
 #include "runtime/handles.inline.hpp"
 #include "runtime/jniHandles.inline.hpp"
 
 // ciInstanceKlass
@@ -112,18 +114,19 @@
   _field_cache = NULL;
 }
 
 // Version for unloaded classes:
 ciInstanceKlass::ciInstanceKlass(ciSymbol* name,
-                                 jobject loader, jobject protection_domain)
-  : ciKlass(name, T_OBJECT)
+                                 jobject loader, jobject protection_domain,
+                                 BasicType bt)
+  : ciKlass(name, bt)
 {
   assert(name->char_at(0) != JVM_SIGNATURE_ARRAY, "not an instance klass");
   _init_state = (InstanceKlass::ClassState)0;
   _nonstatic_field_size = -1;
   _has_nonstatic_fields = false;
-  _nonstatic_fields = NULL;
+  _nonstatic_fields = NULL;            // initialized lazily by compute_nonstatic_fields
   _has_injected_fields = -1;
   _is_unsafe_anonymous = false;
   _is_hidden = false;
   _loader = loader;
   _protection_domain = protection_domain;
@@ -428,10 +431,33 @@
   }
   ciField* field = new (CURRENT_THREAD_ENV->arena()) ciField(&fd);
   return field;
 }
 
+ciField* ciInstanceKlass::get_non_flattened_field_by_offset(int field_offset) {
+  if (super() != NULL && super()->has_nonstatic_fields()) {
+    ciField* f = super()->get_non_flattened_field_by_offset(field_offset);
+    if (f != NULL) {
+      return f;
+    }
+  }
+
+  VM_ENTRY_MARK;
+  InstanceKlass* k = get_instanceKlass();
+  Arena* arena = CURRENT_ENV->arena();
+  for (JavaFieldStream fs(k); !fs.done(); fs.next()) {
+    if (fs.access_flags().is_static())  continue;
+    fieldDescriptor& fd = fs.field_descriptor();
+    if (fd.offset() == field_offset) {
+      ciField* f = new (arena) ciField(&fd);
+      return f;
+    }
+  }
+
+  return NULL;
+}
+
 // ------------------------------------------------------------------
 // ciInstanceKlass::get_field_by_name
 ciField* ciInstanceKlass::get_field_by_name(ciSymbol* name, ciSymbol* signature, bool is_static) {
   VM_ENTRY_MARK;
   InstanceKlass* k = get_instanceKlass();
@@ -489,22 +515,15 @@
     } else {
       return 0;
     }
   }
 
-  int flen = fields->length();
-
-  // Now sort them by offset, ascending.
-  // (In principle, they could mix with superclass fields.)
-  fields->sort(sort_field_by_offset);
-  _nonstatic_fields = fields;
+  _nonstatic_fields = fields;
   return flen;
 }
 
-GrowableArray<ciField*>*
-ciInstanceKlass::compute_nonstatic_fields_impl(GrowableArray<ciField*>*
-                                               super_fields) {
+GrowableArray<ciField*>* ciInstanceKlass::compute_nonstatic_fields_impl(GrowableArray<ciField*>* super_fields, bool flatten) {
   ASSERT_IN_VM;
   Arena* arena = CURRENT_ENV->arena();
   int flen = 0;
   GrowableArray<ciField*>* fields = NULL;
   InstanceKlass* k = get_instanceKlass();
@@ -518,22 +537,46 @@
     return NULL;  // return nothing if none are locally declared
   }
   if (super_fields != NULL) {
     flen += super_fields->length();
   }
+
   fields = new (arena) GrowableArray<ciField*>(arena, flen, 0, NULL);
   if (super_fields != NULL) {
     fields->appendAll(super_fields);
   }
 
   for (JavaFieldStream fs(k); !fs.done(); fs.next()) {
     if (fs.access_flags().is_static())  continue;
     fieldDescriptor& fd = fs.field_descriptor();
-    ciField* field = new (arena) ciField(&fd);
-    fields->append(field);
+    if (fd.is_flattened() && flatten) {
+      // Value type fields are embedded
+      int field_offset = fd.offset();
+      // Get ValueKlass and adjust number of fields
+      Klass* k = get_instanceKlass()->get_value_field_klass(fd.index());
+      ciValueKlass* vk = CURRENT_ENV->get_klass(k)->as_value_klass();
+      flen += vk->nof_nonstatic_fields() - 1;
+      // Iterate over fields of the flattened value type and copy them to 'this'
+      for (int i = 0; i < vk->nof_nonstatic_fields(); ++i) {
+        ciField* flattened_field = vk->nonstatic_field_at(i);
+        // Adjust offset to account for missing oop header
+        int offset = field_offset + (flattened_field->offset() - vk->first_field_offset());
+        // A flattened field can be treated as final if the non-flattened
+        // field is declared final or the holder klass is a value type itself.
+        bool is_final = fd.is_final() || is_valuetype();
+        ciField* field = new (arena) ciField(flattened_field, this, offset, is_final);
+        fields->append(field);
+      }
+    } else {
+      ciField* field = new (arena) ciField(&fd);
+      fields->append(field);
+    }
   }
   assert(fields->length() == flen, "sanity");
+  // Now sort them by offset, ascending.
+  // (In principle, they could mix with superclass fields.)
+  fields->sort(sort_field_by_offset);
   return fields;
 }
 
 bool ciInstanceKlass::compute_injected_fields_helper() {
   ASSERT_IN_VM;
@@ -627,10 +670,40 @@
     }
   }
   return impl;
 }
 
+bool ciInstanceKlass::can_be_value_klass(bool is_exact) {
+  if (!EnableValhalla) {
+    return false;
+  }
+  if (!is_loaded() ||   // Not loaded, might be a value klass
+      is_valuetype() || // Known to be a value klass
+      // Non-exact j.l.Object or interface klass
+      ((is_java_lang_Object() || is_interface()) && !is_exact)) {
+    return true;
+  }
+  if (is_abstract() && !is_exact && !has_nonstatic_fields()) {
+    // TODO Factor out and re-use similar code from the ClassFileParser
+    // An abstract class can only be implemented by a value type if it has no instance
+    // fields, no synchronized instance methods and an empty, no-arg constructor.
+    VM_ENTRY_MARK;
+    Array<Method*>* methods = get_instanceKlass()->methods();
+    for (int i = 0; i < methods->length(); i++) {
+      Method* m = methods->at(i);
+      if ((m->is_synchronized() && !m->is_static()) ||
+          (m->is_object_constructor() &&
+           (m->signature() != vmSymbols::void_method_signature() ||
+            !m->is_vanilla_constructor()))) {
+        return false;
+      }
+    }
+    return true;
+  }
+  return false;
+}
+
 ciInstanceKlass* ciInstanceKlass::unsafe_anonymous_host() {
   assert(is_loaded(), "must be loaded");
   if (is_unsafe_anonymous()) {
     VM_ENTRY_MARK
     Klass* unsafe_anonymous_host = get_instanceKlass()->unsafe_anonymous_host();
@@ -645,74 +718,127 @@
 // out the actual value.  For Strings it's the actual string value.
 // For array types it it's first level array size since that's the
 // only value which statically unchangeable.  For all other reference
 // types it simply prints out the dynamic type.
 
-class StaticFinalFieldPrinter : public FieldClosure {
+class StaticFieldPrinter : public FieldClosure {
+protected:
   outputStream* _out;
+public:
+  StaticFieldPrinter(outputStream* out) :
+    _out(out) {
+  }
+  void do_field_helper(fieldDescriptor* fd, oop obj, bool flattened);
+};
+
+class StaticFinalFieldPrinter : public StaticFieldPrinter {
   const char*   _holder;
  public:
   StaticFinalFieldPrinter(outputStream* out, const char* holder) :
-    _out(out),
-    _holder(holder) {
+    StaticFieldPrinter(out), _holder(holder) {
   }
   void do_field(fieldDescriptor* fd) {
     if (fd->is_final() && !fd->has_initial_value()) {
       ResourceMark rm;
-      oop mirror = fd->field_holder()->java_mirror();
-      _out->print("staticfield %s %s %s ", _holder, fd->name()->as_quoted_ascii(), fd->signature()->as_quoted_ascii());
-      switch (fd->field_type()) {
-        case T_BYTE:    _out->print_cr("%d", mirror->byte_field(fd->offset()));   break;
-        case T_BOOLEAN: _out->print_cr("%d", mirror->bool_field(fd->offset()));   break;
-        case T_SHORT:   _out->print_cr("%d", mirror->short_field(fd->offset()));  break;
-        case T_CHAR:    _out->print_cr("%d", mirror->char_field(fd->offset()));   break;
-        case T_INT:     _out->print_cr("%d", mirror->int_field(fd->offset()));    break;
-        case T_LONG:    _out->print_cr(INT64_FORMAT, (int64_t)(mirror->long_field(fd->offset())));   break;
-        case T_FLOAT: {
-          float f = mirror->float_field(fd->offset());
-          _out->print_cr("%d", *(int*)&f);
-          break;
-        }
-        case T_DOUBLE: {
-          double d = mirror->double_field(fd->offset());
-          _out->print_cr(INT64_FORMAT, *(int64_t*)&d);
-          break;
-        }
-        case T_ARRAY:  // fall-through
-        case T_OBJECT: {
-          oop value =  mirror->obj_field_acquire(fd->offset());
-          if (value == NULL) {
-            _out->print_cr("null");
-          } else if (value->is_instance()) {
-            assert(fd->field_type() == T_OBJECT, "");
-            if (value->is_a(SystemDictionary::String_klass())) {
-              const char* ascii_value = java_lang_String::as_quoted_ascii(value);
-              _out->print("\"%s\"", (ascii_value != NULL) ? ascii_value : "");
-            } else {
-              const char* klass_name  = value->klass()->name()->as_quoted_ascii();
-              _out->print_cr("%s", klass_name);
-            }
-          } else if (value->is_array()) {
-            typeArrayOop ta = (typeArrayOop)value;
-            _out->print("%d", ta->length());
-            if (value->is_objArray()) {
-              objArrayOop oa = (objArrayOop)value;
-              const char* klass_name  = value->klass()->name()->as_quoted_ascii();
-              _out->print(" %s", klass_name);
-            }
-            _out->cr();
-          } else {
-            ShouldNotReachHere();
-          }
-          break;
+      InstanceKlass* holder = fd->field_holder();
+      oop mirror = holder->java_mirror();
+      _out->print("staticfield %s %s ", _holder, fd->name()->as_quoted_ascii());
+      BasicType bt = fd->field_type();
+      if (bt != T_OBJECT && bt != T_ARRAY) {
+        _out->print("%s ", fd->signature()->as_quoted_ascii());
+      }
+      do_field_helper(fd, mirror, false);
+      _out->cr();
+    }
+  }
+};
+
+class ValueTypeFieldPrinter : public StaticFieldPrinter {
+  oop _obj;
+public:
+  ValueTypeFieldPrinter(outputStream* out, oop obj) :
+    StaticFieldPrinter(out), _obj(obj) {
+  }
+  void do_field(fieldDescriptor* fd) {
+    do_field_helper(fd, _obj, true);
+    _out->print(" ");
+  }
+};
+
+void StaticFieldPrinter::do_field_helper(fieldDescriptor* fd, oop mirror, bool flattened) {
+  BasicType bt = fd->field_type();
+  switch (bt) {
+    case T_BYTE:    _out->print("%d", mirror->byte_field(fd->offset()));   break;
+    case T_BOOLEAN: _out->print("%d", mirror->bool_field(fd->offset()));   break;
+    case T_SHORT:   _out->print("%d", mirror->short_field(fd->offset()));  break;
+    case T_CHAR:    _out->print("%d", mirror->char_field(fd->offset()));   break;
+    case T_INT:     _out->print("%d", mirror->int_field(fd->offset()));    break;
+    case T_LONG:    _out->print(INT64_FORMAT, (int64_t)(mirror->long_field(fd->offset())));   break;
+    case T_FLOAT: {
+      float f = mirror->float_field(fd->offset());
+      _out->print("%d", *(int*)&f);
+      break;
+    }
+    case T_DOUBLE: {
+      double d = mirror->double_field(fd->offset());
+      _out->print(INT64_FORMAT, *(int64_t*)&d);
+      break;
+    }
+    case T_ARRAY:  // fall-through
+    case T_OBJECT: {
+      _out->print("%s ", fd->signature()->as_quoted_ascii());
+      oop value =  mirror->obj_field_acquire(fd->offset());
+      if (value == NULL) {
+        _out->print_cr("null");
+      } else if (value->is_instance()) {
+        assert(fd->field_type() == T_OBJECT, "");
+        if (value->is_a(SystemDictionary::String_klass())) {
+          const char* ascii_value = java_lang_String::as_quoted_ascii(value);
+          _out->print("\"%s\"", (ascii_value != NULL) ? ascii_value : "");
+         } else {
+          const char* klass_name  = value->klass()->name()->as_quoted_ascii();
+          _out->print("%s", klass_name);
         }
-        default:
-          ShouldNotReachHere();
+      } else if (value->is_array()) {
+        typeArrayOop ta = (typeArrayOop)value;
+        _out->print("%d", ta->length());
+        if (value->is_objArray() || value->is_valueArray()) {
+          objArrayOop oa = (objArrayOop)value;
+          const char* klass_name  = value->klass()->name()->as_quoted_ascii();
+          _out->print(" %s", klass_name);
         }
+      } else {
+        ShouldNotReachHere();
+      }
+      break;
     }
+    case T_VALUETYPE: {
+      ResetNoHandleMark rnhm;
+      Thread* THREAD = Thread::current();
+      SignatureStream ss(fd->signature(), false);
+      Symbol* name = ss.as_symbol();
+      assert(!HAS_PENDING_EXCEPTION, "can resolve klass?");
+      InstanceKlass* holder = fd->field_holder();
+      Klass* k = SystemDictionary::find(name, Handle(THREAD, holder->class_loader()),
+                                        Handle(THREAD, holder->protection_domain()), THREAD);
+      assert(k != NULL && !HAS_PENDING_EXCEPTION, "can resolve klass?");
+      ValueKlass* vk = ValueKlass::cast(k);
+      oop obj;
+      if (flattened) {
+        int field_offset = fd->offset() - vk->first_field_offset();
+        obj = (oop) (cast_from_oop<address>(mirror) + field_offset);
+      } else {
+        obj =  mirror->obj_field_acquire(fd->offset());
+      }
+      ValueTypeFieldPrinter print_field(_out, obj);
+      vk->do_nonstatic_fields(&print_field);
+      break;
+    }
+    default:
+      ShouldNotReachHere();
   }
-};
+}
 
 
 void ciInstanceKlass::dump_replay_data(outputStream* out) {
   ResourceMark rm;
 
diff a/src/hotspot/share/ci/ciInstanceKlass.hpp b/src/hotspot/share/ci/ciInstanceKlass.hpp
--- a/src/hotspot/share/ci/ciInstanceKlass.hpp
+++ b/src/hotspot/share/ci/ciInstanceKlass.hpp
@@ -66,10 +66,11 @@
   ciInstanceKlass*       _super;
   ciInstance*            _java_mirror;
 
   ciConstantPoolCache*   _field_cache;  // cached map index->field
   GrowableArray<ciField*>* _nonstatic_fields;
+
   int                    _has_injected_fields; // any non static injected fields? lazily initialized.
 
   // The possible values of the _implementor fall into following three cases:
   //   NULL: no implementor.
   //   A ciInstanceKlass that's not itself: one implementor.
@@ -79,11 +80,11 @@
   void compute_injected_fields();
   bool compute_injected_fields_helper();
 
 protected:
   ciInstanceKlass(Klass* k);
-  ciInstanceKlass(ciSymbol* name, jobject loader, jobject protection_domain);
+  ciInstanceKlass(ciSymbol* name, jobject loader, jobject protection_domain, BasicType bt = T_OBJECT); // for unloaded klasses
 
   InstanceKlass* get_instanceKlass() const {
     return InstanceKlass::cast(get_Klass());
   }
 
@@ -103,12 +104,12 @@
 
   bool is_shared() { return _is_shared; }
 
   void compute_shared_init_state();
   bool compute_shared_has_subklass();
-  int  compute_nonstatic_fields();
-  GrowableArray<ciField*>* compute_nonstatic_fields_impl(GrowableArray<ciField*>* super_fields);
+  virtual int compute_nonstatic_fields();
+  GrowableArray<ciField*>* compute_nonstatic_fields_impl(GrowableArray<ciField*>* super_fields, bool flatten = true);
 
   // Update the init_state for shared klasses
   void update_if_shared(InstanceKlass::ClassState expected) {
     if (_is_shared && _init_state != expected) {
       if (is_loaded()) compute_shared_init_state();
@@ -201,17 +202,20 @@
   }
 
   ciInstanceKlass* get_canonical_holder(int offset);
   ciField* get_field_by_offset(int field_offset, bool is_static);
   ciField* get_field_by_name(ciSymbol* name, ciSymbol* signature, bool is_static);
+  // get field descriptor at field_offset ignoring flattening
+  ciField* get_non_flattened_field_by_offset(int field_offset);
 
   // total number of nonstatic fields (including inherited):
   int nof_nonstatic_fields() {
-    if (_nonstatic_fields == NULL)
+    if (_nonstatic_fields == NULL) {
       return compute_nonstatic_fields();
-    else
+    } else {
       return _nonstatic_fields->length();
+    }
   }
 
   bool has_injected_fields() {
     if (_has_injected_fields == -1) {
       compute_injected_fields();
@@ -255,10 +259,12 @@
     assert(is_loaded(), "must be loaded");
     ciInstanceKlass* impl = implementor();
     return (impl != this ? impl : NULL);
   }
 
+  virtual bool can_be_value_klass(bool is_exact = false);
+
   // Is the defining class loader of this class the default loader?
   bool uses_default_loader() const;
 
   bool is_java_lang_Object() const;
 
diff a/src/hotspot/share/classfile/classFileParser.cpp b/src/hotspot/share/classfile/classFileParser.cpp
--- a/src/hotspot/share/classfile/classFileParser.cpp
+++ b/src/hotspot/share/classfile/classFileParser.cpp
@@ -19,10 +19,11 @@
  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  * or visit www.oracle.com if you need additional information or have any
  * questions.
  *
  */
+
 #include "precompiled.hpp"
 #include "jvm.h"
 #include "aot/aotLoader.hpp"
 #include "classfile/classFileParser.hpp"
 #include "classfile/classFileStream.hpp"
@@ -56,10 +57,11 @@
 #include "oops/metadata.hpp"
 #include "oops/method.inline.hpp"
 #include "oops/oop.inline.hpp"
 #include "oops/recordComponent.hpp"
 #include "oops/symbol.hpp"
+#include "oops/valueKlass.inline.hpp"
 #include "prims/jvmtiExport.hpp"
 #include "prims/jvmtiThreadState.hpp"
 #include "runtime/arguments.hpp"
 #include "runtime/fieldDescriptor.inline.hpp"
 #include "runtime/handles.inline.hpp"
@@ -79,10 +81,11 @@
 #include "utilities/globalDefinitions.hpp"
 #include "utilities/growableArray.hpp"
 #include "utilities/macros.hpp"
 #include "utilities/ostream.hpp"
 #include "utilities/resourceHash.hpp"
+#include "utilities/stringUtils.hpp"
 #include "utilities/utf8.hpp"
 
 #if INCLUDE_CDS
 #include "classfile/systemDictionaryShared.hpp"
 #endif
@@ -130,10 +133,12 @@
 
 #define JAVA_14_VERSION                   58
 
 #define JAVA_15_VERSION                   59
 
+#define CONSTANT_CLASS_DESCRIPTORS        59
+
 void ClassFileParser::set_class_bad_constant_seen(short bad_constant) {
   assert((bad_constant == JVM_CONSTANT_Module ||
           bad_constant == JVM_CONSTANT_Package) && _major_version >= JAVA_9_VERSION,
          "Unexpected bad constant pool entry");
   if (_bad_constant_seen == 0) _bad_constant_seen = bad_constant;
@@ -169,11 +174,11 @@
     // Each of the following case guarantees one more byte in the stream
     // for the following tag or the access_flags following constant pool,
     // so we don't need bounds-check for reading tag.
     const u1 tag = cfs->get_u1_fast();
     switch (tag) {
-      case JVM_CONSTANT_Class : {
+      case JVM_CONSTANT_Class: {
         cfs->guarantee_more(3, CHECK);  // name_index, tag/access_flags
         const u2 name_index = cfs->get_u2_fast();
         cp->klass_index_at_put(index, name_index);
         break;
       }
@@ -499,11 +504,18 @@
       case JVM_CONSTANT_ClassIndex: {
         const int class_index = cp->klass_index_at(index);
         check_property(valid_symbol_at(class_index),
           "Invalid constant pool index %u in class file %s",
           class_index, CHECK);
-        cp->unresolved_klass_at_put(index, class_index, num_klasses++);
+
+        Symbol* const name = cp->symbol_at(class_index);
+        const unsigned int name_len = name->utf8_length();
+        if (name->is_Q_signature()) {
+          cp->unresolved_qdescriptor_at_put(index, class_index, num_klasses++);
+        } else {
+          cp->unresolved_klass_at_put(index, class_index, num_klasses++);
+        }
         break;
       }
       case JVM_CONSTANT_StringIndex: {
         const int string_index = cp->string_index_at(index);
         check_property(valid_symbol_at(string_index),
@@ -753,18 +765,29 @@
             const int name_and_type_ref_index =
               cp->name_and_type_ref_index_at(ref_index);
             const int name_ref_index =
               cp->name_ref_index_at(name_and_type_ref_index);
             const Symbol* const name = cp->symbol_at(name_ref_index);
-            if (ref_kind == JVM_REF_newInvokeSpecial) {
-              if (name != vmSymbols::object_initializer_name()) {
+            if (name != vmSymbols::object_initializer_name()) {
+              if (ref_kind == JVM_REF_newInvokeSpecial) {
                 classfile_parse_error(
                   "Bad constructor name at constant pool index %u in class file %s",
                     name_ref_index, CHECK);
               }
             } else {
-              if (name == vmSymbols::object_initializer_name()) {
+              // The allowed invocation mode of <init> depends on its signature.
+              // This test corresponds to verify_invoke_instructions in the verifier.
+              const int signature_ref_index =
+                cp->signature_ref_index_at(name_and_type_ref_index);
+              const Symbol* const signature = cp->symbol_at(signature_ref_index);
+              if (signature->is_void_method_signature()
+                  && ref_kind == JVM_REF_newInvokeSpecial) {
+                // OK, could be a constructor call
+              } else if (!signature->is_void_method_signature()
+                         && ref_kind == JVM_REF_invokeStatic) {
+                // also OK, could be a static factory call
+              } else {
                 classfile_parse_error(
                   "Bad method name at constant pool index %u in class file %s",
                   name_ref_index, CHECK);
               }
             }
@@ -919,26 +942,33 @@
 
   return true;
 }
 
 // Side-effects: populates the _local_interfaces field
-void ClassFileParser::parse_interfaces(const ClassFileStream* const stream,
-                                       const int itfs_len,
-                                       ConstantPool* const cp,
+void ClassFileParser::parse_interfaces(const ClassFileStream* stream,
+                                       int itfs_len,
+                                       ConstantPool* cp,
+                                       bool is_inline_type,
                                        bool* const has_nonstatic_concrete_methods,
+                                       // FIXME: lots of these functions
+                                       // declare their parameters as const,
+                                       // which adds only noise to the code.
+                                       // Remove the spurious const modifiers.
+                                       // Many are of the form "const int x"
+                                       // or "T* const x".
+                                       bool* const is_declared_atomic,
                                        TRAPS) {
   assert(stream != NULL, "invariant");
   assert(cp != NULL, "invariant");
   assert(has_nonstatic_concrete_methods != NULL, "invariant");
 
   if (itfs_len == 0) {
-    _local_interfaces = Universe::the_empty_instance_klass_array();
+    _temp_local_interfaces = new GrowableArray<InstanceKlass*>(0);
   } else {
     assert(itfs_len > 0, "only called for len>0");
-    _local_interfaces = MetadataFactory::new_array<InstanceKlass*>(_loader_data, itfs_len, NULL, CHECK);
-
-    int index;
+    _temp_local_interfaces = new GrowableArray<InstanceKlass*>(itfs_len);
+    int index = 0;
     for (index = 0; index < itfs_len; index++) {
       const u2 interface_index = stream->get_u2(CHECK);
       Klass* interf;
       check_property(
         valid_klass_reference_at(interface_index),
@@ -952,11 +982,11 @@
         // Don't need to check legal name because it's checked when parsing constant pool.
         // But need to make sure it's not an array type.
         guarantee_property(unresolved_klass->char_at(0) != JVM_SIGNATURE_ARRAY,
                            "Bad interface name in class file %s", CHECK);
 
-        // Call resolve_super so classcircularity is checked
+        // Call resolve_super so class circularity is checked
         interf = SystemDictionary::resolve_super_or_fail(
                                                   _class_name,
                                                   unresolved_klass,
                                                   Handle(THREAD, _loader_data->class_loader()),
                                                   _protection_domain,
@@ -970,14 +1000,33 @@
                           _class_name->as_klass_external_name(),
                           interf->external_name(),
                           interf->class_in_module_of_loader()));
       }
 
-      if (InstanceKlass::cast(interf)->has_nonstatic_concrete_methods()) {
+      InstanceKlass* ik = InstanceKlass::cast(interf);
+      if (is_inline_type && ik->invalid_inline_super()) {
+        ResourceMark rm(THREAD);
+        Exceptions::fthrow(
+          THREAD_AND_LOCATION,
+          vmSymbols::java_lang_IncompatibleClassChangeError(),
+          "Inline type %s attempts to implement interface java.lang.IdentityObject",
+          _class_name->as_klass_external_name());
+        return;
+      }
+      if (ik->invalid_inline_super()) {
+        set_invalid_inline_super();
+      }
+      if (ik->has_nonstatic_concrete_methods()) {
         *has_nonstatic_concrete_methods = true;
       }
-      _local_interfaces->at_put(index, InstanceKlass::cast(interf));
+      if (ik->is_declared_atomic()) {
+        *is_declared_atomic = true;
+      }
+      if (ik->name() == vmSymbols::java_lang_IdentityObject()) {
+        _implements_identityObject = true;
+      }
+      _temp_local_interfaces->append(ik);
     }
 
     if (!_need_verify || itfs_len <= 1) {
       return;
     }
@@ -991,11 +1040,11 @@
     bool dup = false;
     const Symbol* name = NULL;
     {
       debug_only(NoSafepointVerifier nsv;)
       for (index = 0; index < itfs_len; index++) {
-        const InstanceKlass* const k = _local_interfaces->at(index);
+        const InstanceKlass* const k = _temp_local_interfaces->at(index);
         name = k->name();
         // If no duplicates, add (name, NULL) in hashtable interface_names.
         if (!put_after_lookup(name, NULL, interface_names)) {
           dup = true;
           break;
@@ -1467,15 +1516,17 @@
   STATIC_OOP,           // Oops
   STATIC_BYTE,          // Boolean, Byte, char
   STATIC_SHORT,         // shorts
   STATIC_WORD,          // ints
   STATIC_DOUBLE,        // aligned long or double
+  STATIC_FLATTENABLE,   // flattenable field
   NONSTATIC_OOP,
   NONSTATIC_BYTE,
   NONSTATIC_SHORT,
   NONSTATIC_WORD,
   NONSTATIC_DOUBLE,
+  NONSTATIC_FLATTENABLE,
   MAX_FIELD_ALLOCATION_TYPE,
   BAD_ALLOCATION_TYPE = -1
 };
 
 static FieldAllocationType _basic_type_to_atype[2 * (T_CONFLICT + 1)] = {
@@ -1491,16 +1542,17 @@
   NONSTATIC_SHORT,     // T_SHORT       =  9,
   NONSTATIC_WORD,      // T_INT         = 10,
   NONSTATIC_DOUBLE,    // T_LONG        = 11,
   NONSTATIC_OOP,       // T_OBJECT      = 12,
   NONSTATIC_OOP,       // T_ARRAY       = 13,
-  BAD_ALLOCATION_TYPE, // T_VOID        = 14,
-  BAD_ALLOCATION_TYPE, // T_ADDRESS     = 15,
-  BAD_ALLOCATION_TYPE, // T_NARROWOOP   = 16,
-  BAD_ALLOCATION_TYPE, // T_METADATA    = 17,
-  BAD_ALLOCATION_TYPE, // T_NARROWKLASS = 18,
-  BAD_ALLOCATION_TYPE, // T_CONFLICT    = 19,
+  NONSTATIC_OOP,       // T_VALUETYPE   = 14,
+  BAD_ALLOCATION_TYPE, // T_VOID        = 15,
+  BAD_ALLOCATION_TYPE, // T_ADDRESS     = 16,
+  BAD_ALLOCATION_TYPE, // T_NARROWOOP   = 17,
+  BAD_ALLOCATION_TYPE, // T_METADATA    = 18,
+  BAD_ALLOCATION_TYPE, // T_NARROWKLASS = 19,
+  BAD_ALLOCATION_TYPE, // T_CONFLICT    = 20,
   BAD_ALLOCATION_TYPE, // 0
   BAD_ALLOCATION_TYPE, // 1
   BAD_ALLOCATION_TYPE, // 2
   BAD_ALLOCATION_TYPE, // 3
   STATIC_BYTE ,        // T_BOOLEAN     =  4,
@@ -1511,22 +1563,26 @@
   STATIC_SHORT,        // T_SHORT       =  9,
   STATIC_WORD,         // T_INT         = 10,
   STATIC_DOUBLE,       // T_LONG        = 11,
   STATIC_OOP,          // T_OBJECT      = 12,
   STATIC_OOP,          // T_ARRAY       = 13,
-  BAD_ALLOCATION_TYPE, // T_VOID        = 14,
-  BAD_ALLOCATION_TYPE, // T_ADDRESS     = 15,
-  BAD_ALLOCATION_TYPE, // T_NARROWOOP   = 16,
-  BAD_ALLOCATION_TYPE, // T_METADATA    = 17,
-  BAD_ALLOCATION_TYPE, // T_NARROWKLASS = 18,
-  BAD_ALLOCATION_TYPE, // T_CONFLICT    = 19,
+  STATIC_OOP,          // T_VALUETYPE   = 14,
+  BAD_ALLOCATION_TYPE, // T_VOID        = 15,
+  BAD_ALLOCATION_TYPE, // T_ADDRESS     = 16,
+  BAD_ALLOCATION_TYPE, // T_NARROWOOP   = 17,
+  BAD_ALLOCATION_TYPE, // T_METADATA    = 18,
+  BAD_ALLOCATION_TYPE, // T_NARROWKLASS = 19,
+  BAD_ALLOCATION_TYPE, // T_CONFLICT    = 20
 };
 
-static FieldAllocationType basic_type_to_atype(bool is_static, BasicType type) {
+static FieldAllocationType basic_type_to_atype(bool is_static, BasicType type, bool is_flattenable) {
   assert(type >= T_BOOLEAN && type < T_VOID, "only allowable values");
   FieldAllocationType result = _basic_type_to_atype[type + (is_static ? (T_CONFLICT + 1) : 0)];
   assert(result != BAD_ALLOCATION_TYPE, "bad type");
+  if (is_flattenable) {
+    result = is_static ? STATIC_FLATTENABLE : NONSTATIC_FLATTENABLE;
+  }
   return result;
 }
 
 class ClassFileParser::FieldAllocationCount : public ResourceObj {
  public:
@@ -1536,12 +1592,12 @@
     for (int i = 0; i < MAX_FIELD_ALLOCATION_TYPE; i++) {
       count[i] = 0;
     }
   }
 
-  FieldAllocationType update(bool is_static, BasicType type) {
-    FieldAllocationType atype = basic_type_to_atype(is_static, type);
+  FieldAllocationType update(bool is_static, BasicType type, bool is_flattenable) {
+    FieldAllocationType atype = basic_type_to_atype(is_static, type, is_flattenable);
     if (atype != BAD_ALLOCATION_TYPE) {
       // Make sure there is no overflow with injected fields.
       assert(count[atype] < 0xFFFF, "More than 65535 fields");
       count[atype]++;
     }
@@ -1551,10 +1607,11 @@
 
 // Side-effects: populates the _fields, _fields_annotations,
 // _fields_type_annotations fields
 void ClassFileParser::parse_fields(const ClassFileStream* const cfs,
                                    bool is_interface,
+                                   bool is_inline_type,
                                    FieldAllocationCount* const fac,
                                    ConstantPool* cp,
                                    const int cp_size,
                                    u2* const java_fields_count_ptr,
                                    TRAPS) {
@@ -1573,11 +1630,15 @@
   *java_fields_count_ptr = length;
 
   int num_injected = 0;
   const InjectedField* const injected = JavaClasses::get_injected(_class_name,
                                                                   &num_injected);
-  const int total_fields = length + num_injected;
+
+  // two more slots are required for inline classes:
+  // one for the static field with a reference to the pre-allocated default value
+  // one for the field the JVM injects when detecting an empty inline class
+  const int total_fields = length + num_injected + (is_inline_type ? 2 : 0);
 
   // The field array starts with tuples of shorts
   // [access, name index, sig index, initial value index, byte offset].
   // A generic signature slot only exists for field with generic
   // signature attribute. And the access flag is set with
@@ -1603,17 +1664,20 @@
                                               total_fields * (FieldInfo::field_slots + 1));
 
   // The generic signature slots start after all other fields' data.
   int generic_signature_slot = total_fields * FieldInfo::field_slots;
   int num_generic_signature = 0;
+  int instance_fields_count = 0;
   for (int n = 0; n < length; n++) {
     // access_flags, name_index, descriptor_index, attributes_count
     cfs->guarantee_more(8, CHECK);
 
+    jint recognized_modifiers = JVM_RECOGNIZED_FIELD_MODIFIERS;
+
+    const jint flags = cfs->get_u2_fast() & recognized_modifiers;
+    verify_legal_field_modifiers(flags, is_interface, is_inline_type, CHECK);
     AccessFlags access_flags;
-    const jint flags = cfs->get_u2_fast() & JVM_RECOGNIZED_FIELD_MODIFIERS;
-    verify_legal_field_modifiers(flags, is_interface, CHECK);
     access_flags.set_flags(flags);
 
     const u2 name_index = cfs->get_u2_fast();
     check_property(valid_symbol_at(name_index),
       "Invalid constant pool index %u for field name in class file %s",
@@ -1625,10 +1689,27 @@
     check_property(valid_symbol_at(signature_index),
       "Invalid constant pool index %u for field signature in class file %s",
       signature_index, CHECK);
     const Symbol* const sig = cp->symbol_at(signature_index);
     verify_legal_field_signature(name, sig, CHECK);
+    assert(!access_flags.is_flattenable(), "ACC_FLATTENABLE should have been filtered out");
+    if (sig->is_Q_signature()) {
+      // assert(_major_version >= CONSTANT_CLASS_DESCRIPTORS, "Q-descriptors are only supported in recent classfiles");
+      access_flags.set_is_flattenable();
+    }
+    if (access_flags.is_flattenable()) {
+      // Array flattenability cannot be specified.  Arrays of value classes are
+      // are always flattenable.  Arrays of other classes are not flattenable.
+      if (sig->utf8_length() > 1 && sig->char_at(0) == '[') {
+        classfile_parse_error(
+            "Field \"%s\" with signature \"%s\" in class file %s is invalid."
+            " ACC_FLATTENABLE cannot be specified for an array",
+            name->as_C_string(), sig->as_klass_external_name(), CHECK);
+      }
+      _has_flattenable_fields = true;
+    }
+    if (!access_flags.is_static()) instance_fields_count++;
 
     u2 constantvalue_index = 0;
     bool is_synthetic = false;
     u2 generic_signature_index = 0;
     const bool is_static = access_flags.is_static();
@@ -1684,11 +1765,11 @@
                       signature_index,
                       constantvalue_index);
     const BasicType type = cp->basic_type_for_signature_at(signature_index);
 
     // Remember how many oops we encountered and compute allocation type
-    const FieldAllocationType atype = fac->update(is_static, type);
+    const FieldAllocationType atype = fac->update(is_static, type, access_flags.is_flattenable());
     field->set_allocation_type(atype);
 
     // After field is initialized with type, we can augment it with aux info
     if (parsed_annotations.has_any_annotations()) {
       parsed_annotations.apply_to(field);
@@ -1729,16 +1810,45 @@
                         0);
 
       const BasicType type = Signature::basic_type(injected[n].signature());
 
       // Remember how many oops we encountered and compute allocation type
-      const FieldAllocationType atype = fac->update(false, type);
+      const FieldAllocationType atype = fac->update(false, type, false);
       field->set_allocation_type(atype);
       index++;
     }
   }
 
+  if (is_inline_type) {
+    FieldInfo* const field = FieldInfo::from_field_array(fa, index);
+    field->initialize(JVM_ACC_FIELD_INTERNAL | JVM_ACC_STATIC,
+                      vmSymbols::default_value_name_enum,
+                      vmSymbols::object_signature_enum,
+                      0);
+    const BasicType type = Signature::basic_type(vmSymbols::object_signature());
+    const FieldAllocationType atype = fac->update(true, type, false);
+    field->set_allocation_type(atype);
+    index++;
+  }
+
+  if (is_inline_type && instance_fields_count == 0) {
+    _is_empty_inline_type = true;
+    FieldInfo* const field = FieldInfo::from_field_array(fa, index);
+    field->initialize(JVM_ACC_FIELD_INTERNAL,
+        vmSymbols::empty_marker_name_enum,
+        vmSymbols::byte_signature_enum,
+        0);
+    const BasicType type = Signature::basic_type(vmSymbols::byte_signature());
+    const FieldAllocationType atype = fac->update(false, type, false);
+    field->set_allocation_type(atype);
+    index++;
+  }
+
+  if (instance_fields_count > 0) {
+    _has_nonstatic_fields = true;
+  }
+
   assert(NULL == _fields, "invariant");
 
   _fields =
     MetadataFactory::new_array<u2>(_loader_data,
                                    index * FieldInfo::field_slots + num_generic_signature,
@@ -2050,15 +2160,20 @@
                                             const Symbol* sig,
                                             TRAPS) const {
   assert(name != NULL, "invariant");
   assert(sig != NULL, "invariant");
 
+  const char* class_note = "";
+  if (is_inline_type() && name == vmSymbols::object_initializer_name()) {
+    class_note = " (an inline class)";
+  }
+
   ResourceMark rm(THREAD);
   Exceptions::fthrow(THREAD_AND_LOCATION,
       vmSymbols::java_lang_ClassFormatError(),
-      "%s \"%s\" in class %s has illegal signature \"%s\"", type,
-      name->as_C_string(), _class_name->as_C_string(), sig->as_C_string());
+      "%s \"%s\" in class %s%s has illegal signature \"%s\"", type,
+      name->as_C_string(), _class_name->as_C_string(), class_note, sig->as_C_string());
 }
 
 AnnotationCollector::ID
 AnnotationCollector::annotation_index(const ClassLoaderData* loader_data,
                                       const Symbol* name,
@@ -2319,10 +2434,11 @@
 // from the method back up to the containing klass. These flag values
 // are added to klass's access_flags.
 
 Method* ClassFileParser::parse_method(const ClassFileStream* const cfs,
                                       bool is_interface,
+                                      bool is_inline_type,
                                       const ConstantPool* cp,
                                       AccessFlags* const promoted_flags,
                                       TRAPS) {
   assert(cfs != NULL, "invariant");
   assert(cp != NULL, "invariant");
@@ -2359,15 +2475,57 @@
       flags &= JVM_ACC_STATIC | JVM_ACC_STRICT;
     } else {
       classfile_parse_error("Method <clinit> is not static in class file %s", CHECK_NULL);
     }
   } else {
-    verify_legal_method_modifiers(flags, is_interface, name, CHECK_NULL);
-  }
-
-  if (name == vmSymbols::object_initializer_name() && is_interface) {
-    classfile_parse_error("Interface cannot have a method named <init>, class file %s", CHECK_NULL);
+    verify_legal_method_modifiers(flags, is_interface, is_inline_type, name, CHECK_NULL);
+  }
+
+  if (name == vmSymbols::object_initializer_name()) {
+    if (is_interface) {
+      classfile_parse_error("Interface cannot have a method named <init>, class file %s", CHECK_NULL);
+    } else if (!is_inline_type && signature->is_void_method_signature()) {
+      // OK, a constructor
+    } else if (is_inline_type && !signature->is_void_method_signature()) {
+      // also OK, a static factory, as long as the return value is good
+      bool ok = false;
+      SignatureStream ss((Symbol*) signature, true);
+      while (!ss.at_return_type())  ss.next();
+      if (ss.is_reference()) {
+        Symbol* ret = ss.as_symbol();
+        const Symbol* required = class_name();
+        if (is_unsafe_anonymous()) {
+          // The original class name in the UAC byte stream gets changed.  So
+          // using the original name in the return type is no longer valid.
+          required = vmSymbols::java_lang_Object();
+        }
+        ok = (ret == required);
+      }
+      if (!ok) {
+        throwIllegalSignature("Method", name, signature, CHECK_0);
+      }
+    } else {
+      // not OK, so throw the same error as in verify_legal_method_signature.
+      throwIllegalSignature("Method", name, signature, CHECK_0);
+    }
+    // A declared <init> method must always be either a non-static
+    // object constructor, with a void return, or else it must be a
+    // static factory method, with a non-void return.  No other
+    // definition of <init> is possible.
+    //
+    // The verifier (in verify_invoke_instructions) will inspect the
+    // signature of any attempt to invoke <init>, and ensures that it
+    // returns non-void if and only if it is being invoked by
+    // invokestatic, and void if and only if it is being invoked by
+    // invokespecial.
+    //
+    // When a symbolic reference to <init> is resolved for a
+    // particular invocation mode (special or static), the mode is
+    // matched to the JVM_ACC_STATIC modifier of the <init> method.
+    // Thus, it is impossible to statically invoke a constructor, and
+    // impossible to "new + invokespecial" a static factory, either
+    // through bytecode or through reflection.
   }
 
   int args_size = -1;  // only used when _need_verify is true
   if (_need_verify) {
     args_size = ((flags & JVM_ACC_STATIC) ? 0 : 1) +
@@ -2914,10 +3072,11 @@
 // from the methods back up to the containing klass. These flag values
 // are added to klass's access_flags.
 // Side-effects: populates the _methods field in the parser
 void ClassFileParser::parse_methods(const ClassFileStream* const cfs,
                                     bool is_interface,
+                                    bool is_inline_type,
                                     AccessFlags* promoted_flags,
                                     bool* has_final_method,
                                     bool* declares_nonstatic_concrete_methods,
                                     TRAPS) {
   assert(cfs != NULL, "invariant");
@@ -2938,10 +3097,11 @@
                                                    CHECK);
 
     for (int index = 0; index < length; index++) {
       Method* method = parse_method(cfs,
                                     is_interface,
+                                    is_inline_type,
                                     _cp,
                                     promoted_flags,
                                     CHECK);
 
       if (method->is_final()) {
@@ -3130,18 +3290,24 @@
       inner_name_index, CHECK_0);
     if (_need_verify) {
       guarantee_property(inner_class_info_index != outer_class_info_index,
                          "Class is both outer and inner class in class file %s", CHECK_0);
     }
-    // Access flags
-    jint flags;
+
+    jint recognized_modifiers = RECOGNIZED_INNER_CLASS_MODIFIERS;
     // JVM_ACC_MODULE is defined in JDK-9 and later.
     if (_major_version >= JAVA_9_VERSION) {
-      flags = cfs->get_u2_fast() & (RECOGNIZED_INNER_CLASS_MODIFIERS | JVM_ACC_MODULE);
-    } else {
-      flags = cfs->get_u2_fast() & RECOGNIZED_INNER_CLASS_MODIFIERS;
+      recognized_modifiers |= JVM_ACC_MODULE;
     }
+    // JVM_ACC_VALUE is defined for class file version 55 and later
+    if (supports_inline_types()) {
+      recognized_modifiers |= JVM_ACC_VALUE;
+    }
+
+    // Access flags
+    jint flags = cfs->get_u2_fast() & recognized_modifiers;
+
     if ((flags & JVM_ACC_INTERFACE) && _major_version < JAVA_6_VERSION) {
       // Set abstract bit for old class files for backward compatibility
       flags |= JVM_ACC_ABSTRACT;
     }
     verify_legal_class_modifiers(flags, CHECK_0);
@@ -3918,11 +4084,12 @@
                                                         TRAPS) {
   assert(cp != NULL, "invariant");
   const InstanceKlass* super_klass = NULL;
 
   if (super_class_index == 0) {
-    check_property(_class_name == vmSymbols::java_lang_Object(),
+    check_property(_class_name == vmSymbols::java_lang_Object()
+                   || (_access_flags.get_flags() & JVM_ACC_VALUE),
                    "Invalid superclass index %u in class file %s",
                    super_class_index,
                    CHECK_NULL);
   } else {
     check_property(valid_klass_reference_at(super_class_index),
@@ -4098,10 +4265,29 @@
 
 void OopMapBlocksBuilder::print_value_on(outputStream* st) const {
   print_on(st);
 }
 
+void ClassFileParser::throwInlineTypeLimitation(THREAD_AND_LOCATION_DECL,
+                                                const char* msg,
+                                                const Symbol* name,
+                                                const Symbol* sig) const {
+
+  ResourceMark rm(THREAD);
+  if (name == NULL || sig == NULL) {
+    Exceptions::fthrow(THREAD_AND_LOCATION_ARGS,
+        vmSymbols::java_lang_ClassFormatError(),
+        "class: %s - %s", _class_name->as_C_string(), msg);
+  }
+  else {
+    Exceptions::fthrow(THREAD_AND_LOCATION_ARGS,
+        vmSymbols::java_lang_ClassFormatError(),
+        "\"%s\" sig: \"%s\" class: %s - %s", name->as_C_string(), sig->as_C_string(),
+        _class_name->as_C_string(), msg);
+  }
+}
+
 // Layout fields and fill in FieldLayoutInfo.  Could use more refactoring!
 void ClassFileParser::layout_fields(ConstantPool* cp,
                                     const FieldAllocationCount* fac,
                                     const ClassAnnotationCollector* parsed_annotations,
                                     FieldLayoutInfo* info,
@@ -4110,10 +4296,16 @@
   assert(cp != NULL, "invariant");
 
   // Field size and offset computation
   int nonstatic_field_size = _super_klass == NULL ? 0 :
                                _super_klass->nonstatic_field_size();
+  int next_nonstatic_inline_type_offset = 0;
+  int first_nonstatic_inline_type_offset = 0;
+
+  // Fields that are inline types are handled differently depending if they are static or not:
+  // - static fields are oops
+  // - non-static fields are embedded
 
   // Count the contended fields by type.
   //
   // We ignore static fields, because @Contended is not supported for them.
   // The layout code below will also ignore the static fields.
@@ -4130,12 +4322,13 @@
   }
 
 
   // Calculate the starting byte offsets
   int next_static_oop_offset    = InstanceMirrorKlass::offset_of_static_fields();
+  // Inline types in static fields are not embedded, they are handled with oops
   int next_static_double_offset = next_static_oop_offset +
-                                      ((fac->count[STATIC_OOP]) * heapOopSize);
+                                  ((fac->count[STATIC_OOP] + fac->count[STATIC_FLATTENABLE]) * heapOopSize);
   if (fac->count[STATIC_DOUBLE]) {
     next_static_double_offset = align_up(next_static_double_offset, BytesPerLong);
   }
 
   int next_static_word_offset   = next_static_double_offset +
@@ -4146,50 +4339,151 @@
                                   ((fac->count[STATIC_SHORT]) * BytesPerShort);
 
   int nonstatic_fields_start  = instanceOopDesc::base_offset_in_bytes() +
                                 nonstatic_field_size * heapOopSize;
 
+  // First field of inline types is aligned on a long boundary in order to ease
+  // in-lining of inline types (with header removal) in packed arrays and
+  // flatten inline types
+  int initial_inline_type_padding = 0;
+  if (is_inline_type()) {
+    int old = nonstatic_fields_start;
+    nonstatic_fields_start = align_up(nonstatic_fields_start, BytesPerLong);
+    initial_inline_type_padding = nonstatic_fields_start - old;
+  }
+
   int next_nonstatic_field_offset = nonstatic_fields_start;
 
   const bool is_contended_class     = parsed_annotations->is_contended();
 
   // Class is contended, pad before all the fields
   if (is_contended_class) {
     next_nonstatic_field_offset += ContendedPaddingWidth;
   }
 
+  // Temporary inline types restrictions
+  if (is_inline_type()) {
+    if (is_contended_class) {
+      throwInlineTypeLimitation(THREAD_AND_LOCATION, "Inline Types do not support @Contended annotation yet");
+      return;
+    }
+  }
+
   // Compute the non-contended fields count.
   // The packing code below relies on these counts to determine if some field
   // can be squeezed into the alignment gap. Contended fields are obviously
   // exempt from that.
   unsigned int nonstatic_double_count = fac->count[NONSTATIC_DOUBLE] - fac_contended.count[NONSTATIC_DOUBLE];
   unsigned int nonstatic_word_count   = fac->count[NONSTATIC_WORD]   - fac_contended.count[NONSTATIC_WORD];
   unsigned int nonstatic_short_count  = fac->count[NONSTATIC_SHORT]  - fac_contended.count[NONSTATIC_SHORT];
   unsigned int nonstatic_byte_count   = fac->count[NONSTATIC_BYTE]   - fac_contended.count[NONSTATIC_BYTE];
   unsigned int nonstatic_oop_count    = fac->count[NONSTATIC_OOP]    - fac_contended.count[NONSTATIC_OOP];
 
+  int static_inline_type_count = 0;
+  int nonstatic_inline_type_count = 0;
+  int* nonstatic_inline_type_indexes = NULL;
+  Klass** nonstatic_inline_type_klasses = NULL;
+  unsigned int inline_type_oop_map_count = 0;
+  int not_flattened_inline_types = 0;
+  int not_atomic_inline_types = 0;
+
+  int max_nonstatic_inline_type = fac->count[NONSTATIC_FLATTENABLE] + 1;
+
+  nonstatic_inline_type_indexes = NEW_RESOURCE_ARRAY_IN_THREAD(THREAD, int,
+                                                               max_nonstatic_inline_type);
+  for (int i = 0; i < max_nonstatic_inline_type; i++) {
+    nonstatic_inline_type_indexes[i] = -1;
+  }
+  nonstatic_inline_type_klasses = NEW_RESOURCE_ARRAY_IN_THREAD(THREAD, Klass*,
+                                                               max_nonstatic_inline_type);
+
+  for (AllFieldStream fs(_fields, _cp); !fs.done(); fs.next()) {
+    if (fs.allocation_type() == STATIC_FLATTENABLE) {
+      ResourceMark rm;
+      if (!fs.signature()->is_Q_signature()) {
+        THROW(vmSymbols::java_lang_ClassFormatError());
+      }
+      static_inline_type_count++;
+    } else if (fs.allocation_type() == NONSTATIC_FLATTENABLE) {
+      // Pre-resolve the flattenable field and check for inline type circularity issues.
+      ResourceMark rm;
+      if (!fs.signature()->is_Q_signature()) {
+        THROW(vmSymbols::java_lang_ClassFormatError());
+      }
+      Klass* klass =
+        SystemDictionary::resolve_flattenable_field_or_fail(&fs,
+                                                            Handle(THREAD, _loader_data->class_loader()),
+                                                            _protection_domain, true, CHECK);
+      assert(klass != NULL, "Sanity check");
+      if (!klass->access_flags().is_inline_type()) {
+        THROW(vmSymbols::java_lang_IncompatibleClassChangeError());
+      }
+      ValueKlass* vk = ValueKlass::cast(klass);
+      // Conditions to apply flattening or not should be defined in a single place
+      bool too_big_to_flatten = (ValueFieldMaxFlatSize >= 0 &&
+                                 (vk->size_helper() * HeapWordSize) > ValueFieldMaxFlatSize);
+      bool too_atomic_to_flatten = vk->is_declared_atomic();
+      bool too_volatile_to_flatten = fs.access_flags().is_volatile();
+      if (vk->is_naturally_atomic()) {
+        too_atomic_to_flatten = false;
+        //too_volatile_to_flatten = false; //FIXME
+        // volatile fields are currently never flattened, this could change in the future
+      }
+      if (!(too_big_to_flatten | too_atomic_to_flatten | too_volatile_to_flatten)) {
+        nonstatic_inline_type_indexes[nonstatic_inline_type_count] = fs.index();
+        nonstatic_inline_type_klasses[nonstatic_inline_type_count] = klass;
+        nonstatic_inline_type_count++;
+
+        ValueKlass* vklass = ValueKlass::cast(klass);
+        if (vklass->contains_oops()) {
+          inline_type_oop_map_count += vklass->nonstatic_oop_map_count();
+        }
+        fs.set_flattened(true);
+        if (!vk->is_atomic()) {  // flat and non-atomic: take note
+          not_atomic_inline_types++;
+        }
+      } else {
+        not_flattened_inline_types++;
+        fs.set_flattened(false);
+      }
+    }
+  }
+
+  // Adjusting non_static_oop_count to take into account not flattened inline types;
+  nonstatic_oop_count += not_flattened_inline_types;
+
   // Total non-static fields count, including every contended field
   unsigned int nonstatic_fields_count = fac->count[NONSTATIC_DOUBLE] + fac->count[NONSTATIC_WORD] +
                                         fac->count[NONSTATIC_SHORT] + fac->count[NONSTATIC_BYTE] +
-                                        fac->count[NONSTATIC_OOP];
+                                        fac->count[NONSTATIC_OOP] + fac->count[NONSTATIC_FLATTENABLE];
 
   const bool super_has_nonstatic_fields =
           (_super_klass != NULL && _super_klass->has_nonstatic_fields());
   const bool has_nonstatic_fields =
     super_has_nonstatic_fields || (nonstatic_fields_count != 0);
+  const bool has_nonstatic_value_fields = nonstatic_inline_type_count > 0;
 
+  if (is_inline_type() && (!has_nonstatic_fields)) {
+    // There are a number of fixes required throughout the type system and JIT
+    throwInlineTypeLimitation(THREAD_AND_LOCATION, "Inline Types do not support zero instance size yet");
+    return;
+  }
 
   // Prepare list of oops for oop map generation.
   //
   // "offset" and "count" lists are describing the set of contiguous oop
   // regions. offset[i] is the start of the i-th region, which then has
   // count[i] oops following. Before we know how many regions are required,
   // we pessimistically allocate the maps to fit all the oops into the
   // distinct regions.
-
+  //
   int super_oop_map_count = (_super_klass == NULL) ? 0 :_super_klass->nonstatic_oop_map_count();
-  int max_oop_map_count = super_oop_map_count + fac->count[NONSTATIC_OOP];
+  int max_oop_map_count =
+      super_oop_map_count +
+      fac->count[NONSTATIC_OOP] +
+      inline_type_oop_map_count +
+      not_flattened_inline_types;
 
   OopMapBlocksBuilder* nonstatic_oop_maps = new OopMapBlocksBuilder(max_oop_map_count);
   if (super_oop_map_count > 0) {
     nonstatic_oop_maps->initialize_inherited_blocks(_super_klass->start_of_nonstatic_oop_maps(),
                                                     _super_klass->nonstatic_oop_map_count());
@@ -4297,10 +4591,20 @@
       next_nonstatic_oop_offset = align_up(next_nonstatic_oop_offset, heapOopSize);
     }
     next_nonstatic_padded_offset = next_nonstatic_oop_offset + (nonstatic_oop_count * heapOopSize);
   }
 
+  // Aligning embedded inline types
+  // bug below, the current algorithm to layout embedded inline types always put them at the
+  // end of the layout, which doesn't match the different allocation policies the VM is
+  // supposed to provide => FixMe
+  // Note also that the current alignment policy is to make each inline type starting on a
+  // 64 bits boundary. This could be optimized later. For instance, it could be nice to
+  // align inline types according to their most constrained internal type.
+  next_nonstatic_inline_type_offset = align_up(next_nonstatic_padded_offset, BytesPerLong);
+  int next_inline_type_index = 0;
+
   // Iterate over fields again and compute correct offsets.
   // The field allocation type was temporarily stored in the offset slot.
   // oop fields are located before non-oop fields (static and non-static).
   for (AllFieldStream fs(_fields, cp); !fs.done(); fs.next()) {
 
@@ -4313,10 +4617,12 @@
     int real_offset = 0;
     const FieldAllocationType atype = (const FieldAllocationType) fs.allocation_type();
 
     // pack the rest of the fields
     switch (atype) {
+      // Inline types in static fields are handled with oops
+      case STATIC_FLATTENABLE:   // Fallthrough
       case STATIC_OOP:
         real_offset = next_static_oop_offset;
         next_static_oop_offset += heapOopSize;
         break;
       case STATIC_BYTE:
@@ -4333,10 +4639,35 @@
         break;
       case STATIC_DOUBLE:
         real_offset = next_static_double_offset;
         next_static_double_offset += BytesPerLong;
         break;
+      case NONSTATIC_FLATTENABLE:
+        if (fs.is_flattened()) {
+          Klass* klass = nonstatic_inline_type_klasses[next_inline_type_index];
+          assert(klass != NULL, "Klass should have been loaded and resolved earlier");
+          assert(klass->access_flags().is_inline_type(),"Must be an inline type");
+          ValueKlass* vklass = ValueKlass::cast(klass);
+          real_offset = next_nonstatic_inline_type_offset;
+          next_nonstatic_inline_type_offset += (vklass->size_helper()) * wordSize - vklass->first_field_offset();
+          // aligning next inline type on a 64 bits boundary
+          next_nonstatic_inline_type_offset = align_up(next_nonstatic_inline_type_offset, BytesPerLong);
+          next_inline_type_index += 1;
+
+          if (vklass->contains_oops()) { // add flatten oop maps
+            int diff = real_offset - vklass->first_field_offset();
+            const OopMapBlock* map = vklass->start_of_nonstatic_oop_maps();
+            const OopMapBlock* const last_map = map + vklass->nonstatic_oop_map_count();
+            while (map < last_map) {
+              nonstatic_oop_maps->add(map->offset() + diff, map->count());
+              map++;
+            }
+          }
+          break;
+        } else {
+          // Fall through
+        }
       case NONSTATIC_OOP:
         if( nonstatic_oop_space_count > 0 ) {
           real_offset = nonstatic_oop_space_offset;
           nonstatic_oop_space_offset += heapOopSize;
           nonstatic_oop_space_count  -= 1;
@@ -4451,10 +4782,16 @@
             next_nonstatic_padded_offset = align_up(next_nonstatic_padded_offset, BytesPerLong);
             real_offset = next_nonstatic_padded_offset;
             next_nonstatic_padded_offset += BytesPerLong;
             break;
 
+            // Inline types in static fields are handled with oops
+          case NONSTATIC_FLATTENABLE:
+            throwInlineTypeLimitation(THREAD_AND_LOCATION,
+                                      "@Contended annotation not supported for inline types yet", fs.name(), fs.signature());
+            return;
+
           case NONSTATIC_OOP:
             next_nonstatic_padded_offset = align_up(next_nonstatic_padded_offset, heapOopSize);
             real_offset = next_nonstatic_padded_offset;
             next_nonstatic_padded_offset += heapOopSize;
             nonstatic_oop_maps->add(real_offset, 1);
@@ -4490,16 +4827,28 @@
 
   // Entire class is contended, pad in the back.
   // This helps to alleviate memory contention effects for subclass fields
   // and/or adjacent object.
   if (is_contended_class) {
+    assert(!is_inline_type(), "@Contended not supported for inline types yet");
     next_nonstatic_padded_offset += ContendedPaddingWidth;
   }
 
-  int notaligned_nonstatic_fields_end = next_nonstatic_padded_offset;
+  int notaligned_nonstatic_fields_end;
+  if (nonstatic_inline_type_count != 0) {
+    notaligned_nonstatic_fields_end = next_nonstatic_inline_type_offset;
+  } else {
+    notaligned_nonstatic_fields_end = next_nonstatic_padded_offset;
+  }
 
-  int nonstatic_fields_end      = align_up(notaligned_nonstatic_fields_end, heapOopSize);
+  int nonstatic_field_sz_align = heapOopSize;
+  if (is_inline_type()) {
+    if ((notaligned_nonstatic_fields_end - nonstatic_fields_start) > heapOopSize) {
+      nonstatic_field_sz_align = BytesPerLong; // value copy of fields only uses jlong copy
+    }
+  }
+  int nonstatic_fields_end      = align_up(notaligned_nonstatic_fields_end, nonstatic_field_sz_align);
   int instance_end              = align_up(notaligned_nonstatic_fields_end, wordSize);
   int static_fields_end         = align_up(next_static_byte_offset, wordSize);
 
   int static_field_size         = (static_fields_end -
                                    InstanceMirrorKlass::offset_of_static_fields()) / wordSize;
@@ -4507,12 +4856,13 @@
                                   (nonstatic_fields_end - nonstatic_fields_start) / heapOopSize;
 
   int instance_size             = align_object_size(instance_end / wordSize);
 
   assert(instance_size == align_object_size(align_up(
-         (instanceOopDesc::base_offset_in_bytes() + nonstatic_field_size*heapOopSize),
-          wordSize) / wordSize), "consistent layout helper value");
+         (instanceOopDesc::base_offset_in_bytes() + nonstatic_field_size*heapOopSize)
+         + initial_inline_type_padding, wordSize) / wordSize), "consistent layout helper value");
+
 
   // Invariant: nonstatic_field end/start should only change if there are
   // nonstatic fields in the class, or if the class is contended. We compare
   // against the non-aligned value, so that end alignment will not fail the
   // assert without actually having the fields.
@@ -4522,27 +4872,48 @@
 
   // Number of non-static oop map blocks allocated at end of klass.
   nonstatic_oop_maps->compact();
 
 #ifndef PRODUCT
-  if (PrintFieldLayout) {
+  if ((PrintFieldLayout && !is_inline_type()) ||
+      (PrintValueLayout && (is_inline_type() || has_nonstatic_value_fields))) {
     print_field_layout(_class_name,
           _fields,
           cp,
           instance_size,
           nonstatic_fields_start,
           nonstatic_fields_end,
           static_fields_end);
+    nonstatic_oop_maps->print_on(tty);
+    tty->print("\n");
+    tty->print_cr("Instance size = %d", instance_size);
+    tty->print_cr("Nonstatic_field_size = %d", nonstatic_field_size);
+    tty->print_cr("Static_field_size = %d", static_field_size);
+    tty->print_cr("Has nonstatic fields = %d", has_nonstatic_fields);
+    tty->print_cr("---");
   }
 
 #endif
   // Pass back information needed for InstanceKlass creation
   info->oop_map_blocks = nonstatic_oop_maps;
   info->_instance_size = instance_size;
   info->_static_field_size = static_field_size;
   info->_nonstatic_field_size = nonstatic_field_size;
   info->_has_nonstatic_fields = has_nonstatic_fields;
+
+  // An inline type is naturally atomic if it has just one field, and
+  // that field is simple enough.
+  info->_is_naturally_atomic = (is_inline_type() &&
+                                !super_has_nonstatic_fields &&
+                                (nonstatic_fields_count <= 1) &&
+                                (not_atomic_inline_types == 0) &&
+                                (nonstatic_contended_count == 0));
+  // This may be too restrictive, since if all the fields fit in 64
+  // bits we could make the decision to align instances of this class
+  // to 64-bit boundaries, and load and store them as single words.
+  // And on machines which supported larger atomics we could similarly
+  // allow larger values to be atomic, if properly aligned.
 }
 
 void ClassFileParser::set_precomputed_flags(InstanceKlass* ik) {
   assert(ik != NULL, "invariant");
 
@@ -4574,10 +4945,15 @@
 #endif
 
   // Check if this klass supports the java.lang.Cloneable interface
   if (SystemDictionary::Cloneable_klass_loaded()) {
     if (ik->is_subtype_of(SystemDictionary::Cloneable_klass())) {
+      if (ik->is_value()) {
+        Thread *THREAD = Thread::current();
+        throwInlineTypeLimitation(THREAD_AND_LOCATION, "Inline Types do not support Cloneable");
+        return;
+      }
       ik->set_is_cloneable();
     }
   }
 
   // Check if this klass has a vanilla default constructor
@@ -4614,10 +4990,15 @@
     const jint lh = Klass::instance_layout_helper(ik->size_helper(), true);
     ik->set_layout_helper(lh);
   }
 }
 
+bool ClassFileParser::supports_inline_types() const {
+  // Inline types are only supported by class file version 55 and later
+  return _major_version >= JAVA_11_VERSION;
+}
+
 // utility methods for appending an array with check for duplicates
 
 static void append_interfaces(GrowableArray<InstanceKlass*>* result,
                               const Array<InstanceKlass*>* const ifs) {
   // iterate over new interfaces
@@ -4657,13 +5038,14 @@
     // no interfaces, use canonicalized array
     return Universe::the_empty_instance_klass_array();
   } else if (max_transitive_size == super_size) {
     // no new local interfaces added, share superklass' transitive interface array
     return super->transitive_interfaces();
-  } else if (max_transitive_size == local_size) {
-    // only local interfaces added, share local interface array
-    return local_ifs;
+    // The three lines below are commented to work around bug JDK-8245487
+//  } else if (max_transitive_size == local_size) {
+//    // only local interfaces added, share local interface array
+//    return local_ifs;
   } else {
     ResourceMark rm;
     GrowableArray<InstanceKlass*>* const result = new GrowableArray<InstanceKlass*>(max_transitive_size);
 
     // Copy down from superclass
@@ -4680,10 +5062,15 @@
     append_interfaces(result, local_ifs);
 
     // length will be less than the max_transitive_size if duplicates were removed
     const int length = result->length();
     assert(length <= max_transitive_size, "just checking");
+
+    if (length == 1 && result->at(0) == SystemDictionary::IdentityObject_klass()) {
+      return Universe::the_single_IdentityObject_klass_array();
+    }
+
     Array<InstanceKlass*>* const new_result =
       MetadataFactory::new_array<InstanceKlass*>(loader_data, length, CHECK_NULL);
     for (int i = 0; i < length; i++) {
       InstanceKlass* const e = result->at(i);
       assert(e != NULL, "just checking");
@@ -4877,21 +5264,33 @@
 
 // utility methods for format checking
 
 void ClassFileParser::verify_legal_class_modifiers(jint flags, TRAPS) const {
   const bool is_module = (flags & JVM_ACC_MODULE) != 0;
+  const bool is_inline_type = (flags & JVM_ACC_VALUE) != 0;
   assert(_major_version >= JAVA_9_VERSION || !is_module, "JVM_ACC_MODULE should not be set");
+  assert(supports_inline_types() || !is_inline_type, "JVM_ACC_VALUE should not be set");
   if (is_module) {
     ResourceMark rm(THREAD);
     Exceptions::fthrow(
       THREAD_AND_LOCATION,
       vmSymbols::java_lang_NoClassDefFoundError(),
       "%s is not a class because access_flag ACC_MODULE is set",
       _class_name->as_C_string());
     return;
   }
 
+  if (is_inline_type && !EnableValhalla) {
+    ResourceMark rm(THREAD);
+    Exceptions::fthrow(
+      THREAD_AND_LOCATION,
+      vmSymbols::java_lang_ClassFormatError(),
+      "Class modifier ACC_VALUE in class %s requires option -XX:+EnableValhalla",
+      _class_name->as_C_string()
+    );
+  }
+
   if (!_need_verify) { return; }
 
   const bool is_interface  = (flags & JVM_ACC_INTERFACE)  != 0;
   const bool is_abstract   = (flags & JVM_ACC_ABSTRACT)   != 0;
   const bool is_final      = (flags & JVM_ACC_FINAL)      != 0;
@@ -4902,17 +5301,20 @@
   const bool major_gte_14  = _major_version >= JAVA_14_VERSION;
 
   if ((is_abstract && is_final) ||
       (is_interface && !is_abstract) ||
       (is_interface && major_gte_1_5 && (is_super || is_enum)) ||
-      (!is_interface && major_gte_1_5 && is_annotation)) {
+      (!is_interface && major_gte_1_5 && is_annotation) ||
+      (is_inline_type && (is_interface || is_abstract || is_enum || !is_final))) {
     ResourceMark rm(THREAD);
+    const char* class_note = "";
+    if (is_inline_type)  class_note = " (an inline class)";
     Exceptions::fthrow(
       THREAD_AND_LOCATION,
       vmSymbols::java_lang_ClassFormatError(),
-      "Illegal class modifiers in class %s: 0x%X",
-      _class_name->as_C_string(), flags
+      "Illegal class modifiers in class %s%s: 0x%X",
+      _class_name->as_C_string(), class_note, flags
     );
     return;
   }
 }
 
@@ -4987,10 +5389,11 @@
   }
 }
 
 void ClassFileParser::verify_legal_field_modifiers(jint flags,
                                                    bool is_interface,
+                                                   bool is_inline_type,
                                                    TRAPS) const {
   if (!_need_verify) { return; }
 
   const bool is_public    = (flags & JVM_ACC_PUBLIC)    != 0;
   const bool is_protected = (flags & JVM_ACC_PROTECTED) != 0;
@@ -5011,10 +5414,14 @@
       is_illegal = true;
     }
   } else { // not interface
     if (has_illegal_visibility(flags) || (is_final && is_volatile)) {
       is_illegal = true;
+    } else {
+      if (is_inline_type && !is_static && !is_final) {
+        is_illegal = true;
+      }
     }
   }
 
   if (is_illegal) {
     ResourceMark rm(THREAD);
@@ -5027,10 +5434,11 @@
   }
 }
 
 void ClassFileParser::verify_legal_method_modifiers(jint flags,
                                                     bool is_interface,
+                                                    bool is_inline_type,
                                                     const Symbol* name,
                                                     TRAPS) const {
   if (!_need_verify) { return; }
 
   const bool is_public       = (flags & JVM_ACC_PUBLIC)       != 0;
@@ -5047,10 +5455,12 @@
   const bool major_gte_8     = _major_version >= JAVA_8_VERSION;
   const bool is_initializer  = (name == vmSymbols::object_initializer_name());
 
   bool is_illegal = false;
 
+  const char* class_note = "";
+
   if (is_interface) {
     if (major_gte_8) {
       // Class file version is JAVA_8_VERSION or later Methods of
       // interfaces may set any of the flags except ACC_PROTECTED,
       // ACC_FINAL, ACC_NATIVE, and ACC_SYNCHRONIZED; they must
@@ -5081,19 +5491,33 @@
   } else { // not interface
     if (has_illegal_visibility(flags)) {
       is_illegal = true;
     } else {
       if (is_initializer) {
-        if (is_static || is_final || is_synchronized || is_native ||
+        if (is_final || is_synchronized || is_native ||
             is_abstract || (major_gte_1_5 && is_bridge)) {
           is_illegal = true;
         }
+        if (!is_static && !is_inline_type) {
+          // OK, an object constructor in a regular class
+        } else if (is_static && is_inline_type) {
+          // OK, a static init factory in an inline class
+        } else {
+          // but no other combinations are allowed
+          is_illegal = true;
+          class_note = (is_inline_type ? " (an inline class)" : " (not an inline class)");
+        }
       } else { // not initializer
-        if (is_abstract) {
-          if ((is_final || is_native || is_private || is_static ||
-              (major_gte_1_5 && (is_synchronized || is_strict)))) {
-            is_illegal = true;
+        if (is_inline_type && is_synchronized && !is_static) {
+          is_illegal = true;
+          class_note = " (an inline class)";
+        } else {
+          if (is_abstract) {
+            if ((is_final || is_native || is_private || is_static ||
+                (major_gte_1_5 && (is_synchronized || is_strict)))) {
+              is_illegal = true;
+            }
           }
         }
       }
     }
   }
@@ -5101,12 +5525,12 @@
   if (is_illegal) {
     ResourceMark rm(THREAD);
     Exceptions::fthrow(
       THREAD_AND_LOCATION,
       vmSymbols::java_lang_ClassFormatError(),
-      "Method %s in class %s has illegal modifiers: 0x%X",
-      name->as_C_string(), _class_name->as_C_string(), flags);
+      "Method %s in class %s%s has illegal modifiers: 0x%X",
+      name->as_C_string(), _class_name->as_C_string(), class_note, flags);
     return;
   }
 }
 
 void ClassFileParser::verify_legal_utf8(const unsigned char* buffer,
@@ -5260,22 +5684,31 @@
     case JVM_SIGNATURE_INT:
     case JVM_SIGNATURE_FLOAT:
     case JVM_SIGNATURE_LONG:
     case JVM_SIGNATURE_DOUBLE:
       return signature + 1;
-    case JVM_SIGNATURE_CLASS: {
+    case JVM_SIGNATURE_VALUETYPE:
+      // Can't enable this check until JDK upgrades the bytecode generators
+      // if (_major_version < CONSTANT_CLASS_DESCRIPTORS ) {
+      //   classfile_parse_error("Class name contains illegal Q-signature "
+      //                                    "in descriptor in class file %s",
+      //                                    CHECK_0);
+      // }
+      // fall through
+    case JVM_SIGNATURE_CLASS:
+    {
       if (_major_version < JAVA_1_5_VERSION) {
         // Skip over the class name if one is there
         const char* const p = skip_over_field_name(signature + 1, true, --length);
 
         // The next character better be a semicolon
         if (p && (p - signature) > 1 && p[0] == JVM_SIGNATURE_ENDCLASS) {
           return p + 1;
         }
       }
       else {
-        // Skip leading 'L' and ignore first appearance of ';'
+        // Skip leading 'L' or 'Q' and ignore first appearance of ';'
         signature++;
         const char* c = (const char*) memchr(signature, JVM_SIGNATURE_ENDCLASS, length - 1);
         // Format check signature
         if (c != NULL) {
           int newlen = c - (char*) signature;
@@ -5326,10 +5759,13 @@
     } else if (_major_version < JAVA_1_5_VERSION) {
       if (bytes[0] != JVM_SIGNATURE_SPECIAL) {
         p = skip_over_field_name(bytes, true, length);
         legal = (p != NULL) && ((p - bytes) == (int)length);
       }
+    } else if (_major_version >= CONSTANT_CLASS_DESCRIPTORS && bytes[length - 1] == ';' ) {
+      // Support for L...; and Q...; descriptors
+      legal = verify_unqualified_name(bytes + 1, length - 2, LegalClass);
     } else {
       // 4900761: relax the constraints based on JSR202 spec
       // Class names may be drawn from the entire Unicode character set.
       // Identifiers between '/' must be unqualified names.
       // The utf8 string has been verified when parsing cpool entries.
@@ -5475,14 +5911,30 @@
     }
     // The first non-signature thing better be a ')'
     if ((length > 0) && (*p++ == JVM_SIGNATURE_ENDFUNC)) {
       length--;
       if (name->utf8_length() > 0 && name->char_at(0) == JVM_SIGNATURE_SPECIAL) {
-        // All internal methods must return void
+        // All constructor methods must return void
         if ((length == 1) && (p[0] == JVM_SIGNATURE_VOID)) {
           return args_size;
         }
+        // All static init methods must return the current class
+        if ((length >= 3) && (p[length-1] == JVM_SIGNATURE_ENDCLASS)
+            && name == vmSymbols::object_initializer_name()) {
+          nextp = skip_over_field_signature(p, true, length, CHECK_0);
+          if (nextp && ((int)length == (nextp - p))) {
+            // The actual class will be checked against current class
+            // when the method is defined (see parse_method).
+            // A reference to a static init with a bad return type
+            // will load and verify OK, but will fail to link.
+            return args_size;
+          }
+        }
+        // The distinction between static factory methods and
+        // constructors depends on the JVM_ACC_STATIC modifier.
+        // This distinction must be reflected in a void or non-void
+        // return. For declared methods, the check is in parse_method.
       } else {
         // Now we better just have a return value
         nextp = skip_over_field_signature(p, true, length, CHECK_0);
         if (nextp && ((int)length == (nextp - p))) {
           return args_size;
@@ -5636,13 +6088,53 @@
       log_info(class, fingerprint)("%s :  expected = " PTR64_FORMAT " actual = " PTR64_FORMAT,
                                  ik->external_name(), aot_fp, _stream->compute_fingerprint());
     }
   }
 
+  if (ik->is_value()) {
+    ValueKlass* vk = ValueKlass::cast(ik);
+    oop val = ik->allocate_instance(CHECK_NULL);
+    vk->set_default_value(val);
+  }
+
   return ik;
 }
 
+// Return true if the specified class is not a valid super class for an inline type.
+// A valid super class for an inline type is abstract, has no instance fields,
+// does not implement interface java.lang.IdentityObject (checked elsewhere), has
+// an empty body-less no-arg constructor, and no synchronized instance methods.
+// This function doesn't check if the class's super types are invalid.  Those checks
+// are done elsewhere.  The final determination of whether or not a class is an
+// invalid super type for an inline class is done in fill_instance_klass().
+bool ClassFileParser::is_invalid_super_for_inline_type() {
+  if (class_name() == vmSymbols::java_lang_IdentityObject()) {
+    return true;
+  }
+  if (is_interface() || class_name() == vmSymbols::java_lang_Object()) {
+    return false;
+  }
+  if (!access_flags().is_abstract() || _has_nonstatic_fields) {
+    return true;
+  } else {
+    // Look at each method
+    for (int x = 0; x < _methods->length(); x++) {
+      const Method* const method = _methods->at(x);
+      if (method->is_synchronized() && !method->is_static()) {
+        return true;
+
+      } else if (method->name() == vmSymbols::object_initializer_name()) {
+        if (method->signature() != vmSymbols::void_method_signature() ||
+            !method->is_vanilla_constructor()) {
+          return true;
+        }
+      }
+    }
+  }
+  return false;
+}
+
 void ClassFileParser::fill_instance_klass(InstanceKlass* ik,
                                           bool changed_by_loadhook,
                                           const ClassInstanceInfo& cl_inst_info,
                                           TRAPS) {
   assert(ik != NULL, "invariant");
@@ -5671,12 +6163,27 @@
   ik->set_should_verify_class(_need_verify);
 
   // Not yet: supers are done below to support the new subtype-checking fields
   ik->set_nonstatic_field_size(_field_info->_nonstatic_field_size);
   ik->set_has_nonstatic_fields(_field_info->_has_nonstatic_fields);
+  if (_field_info->_is_naturally_atomic && ik->is_value()) {
+    ik->set_is_naturally_atomic();
+  }
+  if (_is_empty_inline_type) {
+    ik->set_is_empty_inline_type();
+  }
+
+  if (this->_invalid_inline_super) {
+    ik->set_invalid_inline_super();
+  }
+
+  if (_has_injected_identityObject) {
+    ik->set_has_injected_identityObject();
+  }
+
   assert(_fac != NULL, "invariant");
-  ik->set_static_oop_field_count(_fac->count[STATIC_OOP]);
+  ik->set_static_oop_field_count(_fac->count[STATIC_OOP] + _fac->count[STATIC_FLATTENABLE]);
 
   // this transfers ownership of a lot of arrays from
   // the parser onto the InstanceKlass*
   apply_parsed_class_metadata(ik, _java_fields_count, CHECK);
 
@@ -5722,10 +6229,13 @@
 
   ik->set_minor_version(_minor_version);
   ik->set_major_version(_major_version);
   ik->set_has_nonstatic_concrete_methods(_has_nonstatic_concrete_methods);
   ik->set_declares_nonstatic_concrete_methods(_declares_nonstatic_concrete_methods);
+  if (_is_declared_atomic) {
+    ik->set_is_declared_atomic();
+  }
 
   if (_unsafe_anonymous_host != NULL) {
     assert (ik->is_unsafe_anonymous(), "should be the same");
     ik->set_unsafe_anonymous_host(_unsafe_anonymous_host);
   }
@@ -5831,10 +6341,44 @@
       // We won a potential race
       JvmtiExport::add_default_read_edges(module_handle, THREAD);
     }
   }
 
+  int nfields = ik->java_fields_count();
+  if (ik->is_value()) nfields++;
+  for (int i = 0; i < nfields; i++) {
+    if (ik->field_is_flattenable(i)) {
+      Symbol* klass_name = ik->field_signature(i)->fundamental_name(CHECK);
+      // Inline classes for instance fields must have been pre-loaded
+      // Inline classes for static fields might not have been loaded yet
+      Klass* klass = SystemDictionary::find(klass_name,
+          Handle(THREAD, ik->class_loader()),
+          Handle(THREAD, ik->protection_domain()), CHECK);
+      if (klass != NULL) {
+        assert(klass->access_flags().is_inline_type(), "Inline type expected");
+        ik->set_value_field_klass(i, klass);
+      }
+      klass_name->decrement_refcount();
+    } else
+      if (is_inline_type() && ((ik->field_access_flags(i) & JVM_ACC_FIELD_INTERNAL) != 0)
+        && ((ik->field_access_flags(i) & JVM_ACC_STATIC) != 0)) {
+      ValueKlass::cast(ik)->set_default_value_offset(ik->field_offset(i));
+    }
+  }
+
+  if (is_inline_type()) {
+    ValueKlass* vk = ValueKlass::cast(ik);
+    if (UseNewFieldLayout) {
+      vk->set_alignment(_alignment);
+      vk->set_first_field_offset(_first_field_offset);
+      vk->set_exact_size_in_bytes(_exact_size_in_bytes);
+    } else {
+      vk->set_first_field_offset(vk->first_field_offset_old());
+    }
+    ValueKlass::cast(ik)->initialize_calling_convention(CHECK);
+  }
+
   ClassLoadingService::notify_class_loaded(ik, false /* not shared class */);
 
   if (!is_internal()) {
     if (log_is_enabled(Info, class, load)) {
       ResourceMark rm;
@@ -5880,10 +6424,14 @@
   set_klass_to_deallocate(NULL);
 
   // it's official
   set_klass(ik);
 
+  if (ik->name() == vmSymbols::java_lang_IdentityObject()) {
+    Universe::initialize_the_single_IdentityObject_klass_array(ik, CHECK);
+  }
+
   debug_only(ik->verify();)
 }
 
 void ClassFileParser::update_class_name(Symbol* new_class_name) {
   // Decrement the refcount in the old name, since we're clobbering it.
@@ -5981,10 +6529,11 @@
   _methods(NULL),
   _inner_classes(NULL),
   _nest_members(NULL),
   _nest_host(0),
   _record_components(NULL),
+  _temp_local_interfaces(NULL),
   _local_interfaces(NULL),
   _transitive_interfaces(NULL),
   _combined_annotations(NULL),
   _class_annotations(NULL),
   _class_type_annotations(NULL),
@@ -6020,10 +6569,19 @@
   _relax_verify(false),
   _has_nonstatic_concrete_methods(false),
   _declares_nonstatic_concrete_methods(false),
   _has_final_method(false),
   _has_contended_fields(false),
+  _has_flattenable_fields(false),
+  _has_nonstatic_fields(false),
+  _is_empty_inline_type(false),
+  _is_naturally_atomic(false),
+  _is_declared_atomic(false),
+  _invalid_inline_super(false),
+  _invalid_identity_super(false),
+  _implements_identityObject(false),
+  _has_injected_identityObject(false),
   _has_finalizer(false),
   _has_empty_finalizer(false),
   _has_vanilla_constructor(false),
   _max_bootstrap_specifier_index(-1) {
 
@@ -6225,19 +6783,23 @@
   assert(cp_size == (const u2)cp->length(), "invariant");
 
   // ACCESS FLAGS
   stream->guarantee_more(8, CHECK);  // flags, this_class, super_class, infs_len
 
-  // Access flags
-  jint flags;
+  jint recognized_modifiers = JVM_RECOGNIZED_CLASS_MODIFIERS;
   // JVM_ACC_MODULE is defined in JDK-9 and later.
   if (_major_version >= JAVA_9_VERSION) {
-    flags = stream->get_u2_fast() & (JVM_RECOGNIZED_CLASS_MODIFIERS | JVM_ACC_MODULE);
-  } else {
-    flags = stream->get_u2_fast() & JVM_RECOGNIZED_CLASS_MODIFIERS;
+    recognized_modifiers |= JVM_ACC_MODULE;
+  }
+  // JVM_ACC_VALUE is defined for class file version 55 and later
+  if (supports_inline_types()) {
+    recognized_modifiers |= JVM_ACC_VALUE;
   }
 
+  // Access flags
+  jint flags = stream->get_u2_fast() & recognized_modifiers;
+
   if ((flags & JVM_ACC_INTERFACE) && _major_version < JAVA_6_VERSION) {
     // Set abstract bit for old class files for backward compatibility
     flags |= JVM_ACC_ABSTRACT;
   }
 
@@ -6391,19 +6953,22 @@
   // Interfaces
   _itfs_len = stream->get_u2_fast();
   parse_interfaces(stream,
                    _itfs_len,
                    cp,
+                   is_inline_type(),
                    &_has_nonstatic_concrete_methods,
+                   &_is_declared_atomic,
                    CHECK);
 
-  assert(_local_interfaces != NULL, "invariant");
+  assert(_temp_local_interfaces != NULL, "invariant");
 
   // Fields (offsets are filled in later)
   _fac = new FieldAllocationCount();
   parse_fields(stream,
-               _access_flags.is_interface(),
+               is_interface(),
+               is_inline_type(),
                _fac,
                cp,
                cp_size,
                &_java_fields_count,
                CHECK);
@@ -6411,11 +6976,12 @@
   assert(_fields != NULL, "invariant");
 
   // Methods
   AccessFlags promoted_flags;
   parse_methods(stream,
-                _access_flags.is_interface(),
+                is_interface(),
+                is_inline_type(),
                 &promoted_flags,
                 &_has_final_method,
                 &_declares_nonstatic_concrete_methods,
                 CHECK);
 
@@ -6482,18 +7048,18 @@
   assert(stream->at_eos(), "invariant");
   assert(cp != NULL, "invariant");
   assert(_loader_data != NULL, "invariant");
 
   if (_class_name == vmSymbols::java_lang_Object()) {
-    check_property(_local_interfaces == Universe::the_empty_instance_klass_array(),
-                   "java.lang.Object cannot implement an interface in class file %s",
-                   CHECK);
+    check_property(_temp_local_interfaces->length() == 0,
+        "java.lang.Object cannot implement an interface in class file %s",
+        CHECK);
   }
   // We check super class after class file is parsed and format is checked
   if (_super_class_index > 0 && NULL ==_super_klass) {
     Symbol* const super_class_name = cp->klass_name_at(_super_class_index);
-    if (_access_flags.is_interface()) {
+    if (is_interface()) {
       // Before attempting to resolve the superclass, check for class format
       // errors not checked yet.
       guarantee_property(super_class_name == vmSymbols::java_lang_Object(),
         "Interfaces must have java.lang.Object as superclass in class file %s",
         CHECK);
@@ -6510,10 +7076,13 @@
 
   if (_super_klass != NULL) {
     if (_super_klass->has_nonstatic_concrete_methods()) {
       _has_nonstatic_concrete_methods = true;
     }
+    if (_super_klass->is_declared_atomic()) {
+      _is_declared_atomic = true;
+    }
 
     if (_super_klass->is_interface()) {
       ResourceMark rm(THREAD);
       Exceptions::fthrow(
         THREAD_AND_LOCATION,
@@ -6522,15 +7091,72 @@
         _class_name->as_klass_external_name(),
         _super_klass->external_name()
       );
       return;
     }
+
     // Make sure super class is not final
     if (_super_klass->is_final()) {
       THROW_MSG(vmSymbols::java_lang_VerifyError(), "Cannot inherit from final class");
     }
+
+    // For an inline class, only java/lang/Object or special abstract classes
+    // are acceptable super classes.
+    if (is_inline_type()) {
+      const InstanceKlass* super_ik = _super_klass;
+      if (super_ik->invalid_inline_super()) {
+        ResourceMark rm(THREAD);
+        Exceptions::fthrow(
+          THREAD_AND_LOCATION,
+          vmSymbols::java_lang_IncompatibleClassChangeError(),
+          "inline class %s has an invalid super class %s",
+          _class_name->as_klass_external_name(),
+          _super_klass->external_name());
+        return;
+      }
+    }
+  }
+
+  if (_class_name == vmSymbols::java_lang_NonTearable() && _loader_data->class_loader() == NULL) {
+    // This is the original source of this condition.
+    // It propagates by inheritance, as if testing "instanceof NonTearable".
+    _is_declared_atomic = true;
+  } else if (*ForceNonTearable != '\0') {
+    // Allow a command line switch to force the same atomicity property:
+    const char* class_name_str = _class_name->as_C_string();
+    if (StringUtils::class_list_match(ForceNonTearable, class_name_str)) {
+      _is_declared_atomic = true;
+    }
+  }
+
+  // Set ik->invalid_inline_super field to TRUE if already marked as invalid,
+  // if super is marked invalid, or if is_invalid_super_for_inline_type()
+  // returns true
+  if (invalid_inline_super() ||
+      (_super_klass != NULL && _super_klass->invalid_inline_super()) ||
+      is_invalid_super_for_inline_type()) {
+    set_invalid_inline_super();
+  }
+
+  if (!is_inline_type() && invalid_inline_super() && (_super_klass == NULL || !_super_klass->invalid_inline_super())
+      && !_implements_identityObject && class_name() != vmSymbols::java_lang_IdentityObject()) {
+    _temp_local_interfaces->append(SystemDictionary::IdentityObject_klass());
+    _has_injected_identityObject = true;
+  }
+  int itfs_len = _temp_local_interfaces->length();
+  if (itfs_len == 0) {
+    _local_interfaces = Universe::the_empty_instance_klass_array();
+  } else if (itfs_len == 1 && _temp_local_interfaces->at(0) == SystemDictionary::IdentityObject_klass()) {
+    _local_interfaces = Universe::the_single_IdentityObject_klass_array();
+  } else {
+    _local_interfaces = MetadataFactory::new_array<InstanceKlass*>(_loader_data, itfs_len, NULL, CHECK);
+    for (int i = 0; i < itfs_len; i++) {
+      _local_interfaces->at_put(i, _temp_local_interfaces->at(i));
+    }
   }
+  _temp_local_interfaces = NULL;
+  assert(_local_interfaces != NULL, "invariant");
 
   // Compute the transitive list of all unique interfaces implemented by this class
   _transitive_interfaces =
     compute_transitive_interfaces(_super_klass,
                                   _local_interfaces,
@@ -6556,28 +7182,46 @@
                                                     _class_name,
                                                     _local_interfaces,
                                                     CHECK);
 
   // Size of Java itable (in words)
-  _itable_size = _access_flags.is_interface() ? 0 :
+  _itable_size = is_interface() ? 0 :
     klassItable::compute_itable_size(_transitive_interfaces);
 
   assert(_fac != NULL, "invariant");
   assert(_parsed_annotations != NULL, "invariant");
 
+
+  for (AllFieldStream fs(_fields, cp); !fs.done(); fs.next()) {
+    if (fs.is_flattenable() && !fs.access_flags().is_static()) {
+      // Pre-load value class
+      Klass* klass = SystemDictionary::resolve_flattenable_field_or_fail(&fs,
+          Handle(THREAD, _loader_data->class_loader()),
+          _protection_domain, true, CHECK);
+      assert(klass != NULL, "Sanity check");
+      assert(klass->access_flags().is_inline_type(), "Inline type expected");
+      _has_flattenable_fields = true;
+    }
+  }
+
   _field_info = new FieldLayoutInfo();
   if (UseNewFieldLayout) {
     FieldLayoutBuilder lb(class_name(), super_klass(), _cp, _fields,
-                          _parsed_annotations->is_contended(), _field_info);
-    lb.build_layout();
+        _parsed_annotations->is_contended(), is_inline_type(),
+        loader_data(), _protection_domain, _field_info);
+    lb.build_layout(CHECK);
+    if (is_inline_type()) {
+      _alignment = lb.get_alignment();
+      _first_field_offset = lb.get_first_field_offset();
+      _exact_size_in_bytes = lb.get_exact_size_in_byte();
+    }
   } else {
     layout_fields(cp, _fac, _parsed_annotations, _field_info, CHECK);
   }
 
-  // Compute reference typ
+  // Compute reference type
   _rt = (NULL ==_super_klass) ? REF_NONE : _super_klass->reference_type();
-
 }
 
 void ClassFileParser::set_klass(InstanceKlass* klass) {
 
 #ifdef ASSERT
@@ -6605,10 +7249,11 @@
 const ClassFileStream* ClassFileParser::clone_stream() const {
   assert(_stream != NULL, "invariant");
 
   return _stream->clone();
 }
+
 // ----------------------------------------------------------------------------
 // debugging
 
 #ifdef ASSERT
 
diff a/src/hotspot/share/classfile/classFileParser.hpp b/src/hotspot/share/classfile/classFileParser.hpp
--- a/src/hotspot/share/classfile/classFileParser.hpp
+++ b/src/hotspot/share/classfile/classFileParser.hpp
@@ -73,10 +73,11 @@
   OopMapBlocksBuilder* oop_map_blocks;
   int _instance_size;
   int _nonstatic_field_size;
   int _static_field_size;
   bool  _has_nonstatic_fields;
+  bool  _is_naturally_atomic;
 };
 
 // Parser for for .class files
 //
 // The bytes describing the class file structure is read from a Stream object
@@ -131,10 +132,11 @@
   Array<Method*>* _methods;
   Array<u2>* _inner_classes;
   Array<u2>* _nest_members;
   u2 _nest_host;
   Array<RecordComponent*>* _record_components;
+  GrowableArray<InstanceKlass*>* _temp_local_interfaces;
   Array<InstanceKlass*>* _local_interfaces;
   Array<InstanceKlass*>* _transitive_interfaces;
   Annotations* _combined_annotations;
   AnnotationArray* _class_annotations;
   AnnotationArray* _class_type_annotations;
@@ -156,10 +158,14 @@
   int _vtable_size;
   int _itable_size;
 
   int _num_miranda_methods;
 
+  int _alignment;
+  int _first_field_offset;
+  int _exact_size_in_bytes;
+
   ReferenceType _rt;
   Handle _protection_domain;
   AccessFlags _access_flags;
 
   // for tracing and notifications
@@ -194,10 +200,20 @@
   bool _has_nonstatic_concrete_methods;
   bool _declares_nonstatic_concrete_methods;
   bool _has_final_method;
   bool _has_contended_fields;
 
+  bool _has_flattenable_fields;
+  bool _has_nonstatic_fields;
+  bool _is_empty_inline_type;
+  bool _is_naturally_atomic;
+  bool _is_declared_atomic;
+  bool _invalid_inline_super;   // if true, invalid super type for an inline type.
+  bool _invalid_identity_super; // if true, invalid super type for an identity type.
+  bool _implements_identityObject;
+  bool _has_injected_identityObject;
+
   // precomputed flags
   bool _has_finalizer;
   bool _has_empty_finalizer;
   bool _has_vanilla_constructor;
   int _max_bootstrap_specifier_index;  // detects BSS values
@@ -243,11 +259,13 @@
 
   // Interface parsing
   void parse_interfaces(const ClassFileStream* const stream,
                         const int itfs_len,
                         ConstantPool* const cp,
+                        bool is_inline_type,
                         bool* has_nonstatic_concrete_methods,
+                        bool* is_declared_atomic,
                         TRAPS);
 
   const InstanceKlass* parse_super_class(ConstantPool* const cp,
                                          const int super_class_index,
                                          const bool need_verify,
@@ -264,25 +282,28 @@
                               FieldAnnotationCollector* parsed_annotations,
                               TRAPS);
 
   void parse_fields(const ClassFileStream* const cfs,
                     bool is_interface,
+                    bool is_inline_type,
                     FieldAllocationCount* const fac,
                     ConstantPool* cp,
                     const int cp_size,
                     u2* const java_fields_count_ptr,
                     TRAPS);
 
   // Method parsing
   Method* parse_method(const ClassFileStream* const cfs,
                        bool is_interface,
+                       bool is_inline_type,
                        const ConstantPool* cp,
                        AccessFlags* const promoted_flags,
                        TRAPS);
 
   void parse_methods(const ClassFileStream* const cfs,
                      bool is_interface,
+                     bool is_inline_type,
                      AccessFlags* const promoted_flags,
                      bool* const has_final_method,
                      bool* const declares_nonstatic_concrete_methods,
                      TRAPS);
 
@@ -435,10 +456,15 @@
   void throwIllegalSignature(const char* type,
                              const Symbol* name,
                              const Symbol* sig,
                              TRAPS) const;
 
+  void throwInlineTypeLimitation(THREAD_AND_LOCATION_DECL,
+                                 const char* msg,
+                                 const Symbol* name = NULL,
+                                 const Symbol* sig  = NULL) const;
+
   void verify_constantvalue(const ConstantPool* const cp,
                             int constantvalue_index,
                             int signature_index,
                             TRAPS) const;
 
@@ -453,13 +479,17 @@
   int  verify_legal_method_signature(const Symbol* methodname,
                                      const Symbol* signature,
                                      TRAPS) const;
 
   void verify_legal_class_modifiers(jint flags, TRAPS) const;
-  void verify_legal_field_modifiers(jint flags, bool is_interface, TRAPS) const;
+  void verify_legal_field_modifiers(jint flags,
+                                    bool is_interface,
+                                    bool is_inline_type,
+                                    TRAPS) const;
   void verify_legal_method_modifiers(jint flags,
                                      bool is_interface,
+                                     bool is_inline_type,
                                      const Symbol* name,
                                      TRAPS) const;
 
   const char* skip_over_field_signature(const char* signature,
                                         bool void_ok,
@@ -534,10 +564,13 @@
                      FieldLayoutInfo* info,
                      TRAPS);
 
   void update_class_name(Symbol* new_name);
 
+  // Check if the class file supports inline types
+  bool supports_inline_types() const;
+
  public:
   ClassFileParser(ClassFileStream* stream,
                   Symbol* name,
                   ClassLoaderData* loader_data,
                   const ClassLoadInfo* cl_info,
@@ -562,10 +595,20 @@
   u2 this_class_index() const { return _this_class_index; }
 
   bool is_unsafe_anonymous() const { return _unsafe_anonymous_host != NULL; }
   bool is_hidden() const { return _is_hidden; }
   bool is_interface() const { return _access_flags.is_interface(); }
+  bool is_inline_type() const { return _access_flags.is_inline_type(); }
+  bool is_value_capable_class() const;
+  bool has_flattenable_fields() const { return _has_flattenable_fields; }
+  bool invalid_inline_super() const { return _invalid_inline_super; }
+  void set_invalid_inline_super() { _invalid_inline_super = true; }
+  bool invalid_identity_super() const { return _invalid_identity_super; }
+  void set_invalid_identity_super() { _invalid_identity_super = true; }
+  bool is_invalid_super_for_inline_type();
+
+  u2 java_fields_count() const { return _java_fields_count; }
 
   const InstanceKlass* unsafe_anonymous_host() const { return _unsafe_anonymous_host; }
   const GrowableArray<Handle>* cp_patches() const { return _cp_patches; }
   ClassLoaderData* loader_data() const { return _loader_data; }
   const Symbol* class_name() const { return _class_name; }
diff a/src/hotspot/share/classfile/classLoader.cpp b/src/hotspot/share/classfile/classLoader.cpp
--- a/src/hotspot/share/classfile/classLoader.cpp
+++ b/src/hotspot/share/classfile/classLoader.cpp
@@ -199,11 +199,11 @@
 
     // Fully qualified class names should not contain a 'L'.
     // Set bad_class_name to true to indicate that the package name
     // could not be obtained due to an error condition.
     // In this situation, is_same_class_package returns false.
-    if (*start == JVM_SIGNATURE_CLASS) {
+    if (*start == JVM_SIGNATURE_CLASS || *start == JVM_SIGNATURE_VALUETYPE) {
       if (bad_class_name != NULL) {
         *bad_class_name = true;
       }
       return NULL;
     }
diff a/src/hotspot/share/classfile/classLoaderData.cpp b/src/hotspot/share/classfile/classLoaderData.cpp
--- a/src/hotspot/share/classfile/classLoaderData.cpp
+++ b/src/hotspot/share/classfile/classLoaderData.cpp
@@ -61,10 +61,11 @@
 #include "memory/metadataFactory.hpp"
 #include "memory/resourceArea.hpp"
 #include "oops/access.inline.hpp"
 #include "oops/oop.inline.hpp"
 #include "oops/oopHandle.inline.hpp"
+#include "oops/valueKlass.inline.hpp"
 #include "oops/weakHandle.inline.hpp"
 #include "runtime/atomic.hpp"
 #include "runtime/handles.inline.hpp"
 #include "runtime/mutex.hpp"
 #include "runtime/safepoint.hpp"
@@ -371,10 +372,20 @@
     }
     assert(k != k->next_link(), "no loops!");
   }
 }
 
+void ClassLoaderData::value_classes_do(void f(ValueKlass*)) {
+  // Lock-free access requires load_acquire
+  for (Klass* k = Atomic::load_acquire(&_klasses); k != NULL; k = k->next_link()) {
+    if (k->is_value()) {
+      f(ValueKlass::cast(k));
+    }
+    assert(k != k->next_link(), "no loops!");
+  }
+}
+
 void ClassLoaderData::modules_do(void f(ModuleEntry*)) {
   assert_locked_or_safepoint(Module_lock);
   if (_unnamed_module != NULL) {
     f(_unnamed_module);
   }
@@ -537,10 +548,12 @@
 
   // Some items on the _deallocate_list need to free their C heap structures
   // if they are not already on the _klasses list.
   free_deallocate_list_C_heap_structures();
 
+  value_classes_do(ValueKlass::cleanup);
+
   // Clean up class dependencies and tell serviceability tools
   // these classes are unloading.  Must be called
   // after erroneous classes are released.
   classes_do(InstanceKlass::unload_class);
 
@@ -831,11 +844,15 @@
       if (m->is_method()) {
         MetadataFactory::free_metadata(this, (Method*)m);
       } else if (m->is_constantPool()) {
         MetadataFactory::free_metadata(this, (ConstantPool*)m);
       } else if (m->is_klass()) {
-        MetadataFactory::free_metadata(this, (InstanceKlass*)m);
+        if (!((Klass*)m)->is_value()) {
+          MetadataFactory::free_metadata(this, (InstanceKlass*)m);
+        } else {
+          MetadataFactory::free_metadata(this, (ValueKlass*)m);
+        }
       } else {
         ShouldNotReachHere();
       }
     } else {
       // Metadata is alive.
diff a/src/hotspot/share/classfile/classLoaderData.hpp b/src/hotspot/share/classfile/classLoaderData.hpp
--- a/src/hotspot/share/classfile/classLoaderData.hpp
+++ b/src/hotspot/share/classfile/classLoaderData.hpp
@@ -187,10 +187,11 @@
   bool keep_alive() const       { return _keep_alive > 0; }
 
   void classes_do(void f(Klass* const));
   void loaded_classes_do(KlassClosure* klass_closure);
   void classes_do(void f(InstanceKlass*));
+  void value_classes_do(void f(ValueKlass*));
   void methods_do(void f(Method*));
   void modules_do(void f(ModuleEntry*));
   void packages_do(void f(PackageEntry*));
 
   // Deallocate free list during class unloading.
diff a/src/hotspot/share/classfile/javaClasses.cpp b/src/hotspot/share/classfile/javaClasses.cpp
--- a/src/hotspot/share/classfile/javaClasses.cpp
+++ b/src/hotspot/share/classfile/javaClasses.cpp
@@ -42,18 +42,20 @@
 #include "memory/oopFactory.hpp"
 #include "memory/resourceArea.hpp"
 #include "memory/universe.hpp"
 #include "oops/fieldStreams.inline.hpp"
 #include "oops/instanceKlass.hpp"
-#include "oops/instanceMirrorKlass.hpp"
+#include "oops/instanceMirrorKlass.inline.hpp"
 #include "oops/klass.hpp"
 #include "oops/method.inline.hpp"
 #include "oops/objArrayOop.inline.hpp"
 #include "oops/oop.inline.hpp"
 #include "oops/symbol.hpp"
 #include "oops/recordComponent.hpp"
 #include "oops/typeArrayOop.inline.hpp"
+#include "oops/valueArrayKlass.hpp"
+#include "oops/valueKlass.inline.hpp"
 #include "prims/jvmtiExport.hpp"
 #include "prims/resolvedMethodTable.hpp"
 #include "runtime/fieldDescriptor.inline.hpp"
 #include "runtime/frame.inline.hpp"
 #include "runtime/handles.inline.hpp"
@@ -982,11 +984,16 @@
 
     java_lang_Class::set_static_oop_field_count(mirror(), mk->compute_static_oop_field_count(mirror()));
 
     // It might also have a component mirror.  This mirror must already exist.
     if (k->is_array_klass()) {
-      if (k->is_typeArray_klass()) {
+      if (k->is_valueArray_klass()) {
+        Klass* element_klass = (Klass*) ValueArrayKlass::cast(k)->element_klass();
+        assert(element_klass->is_value(), "Must be value type component");
+        ValueKlass* vk = ValueKlass::cast(InstanceKlass::cast(element_klass));
+        comp_mirror = Handle(THREAD, vk->java_mirror());
+      } else if (k->is_typeArray_klass()) {
         BasicType type = TypeArrayKlass::cast(k)->element_type();
         comp_mirror = Handle(THREAD, Universe::java_mirror(type));
       } else {
         assert(k->is_objArray_klass(), "Must be");
         Klass* element_klass = ObjArrayKlass::cast(k)->element_klass();
@@ -1029,10 +1036,27 @@
     if (comp_mirror() != NULL) {
       // Set after k->java_mirror() is published, because compiled code running
       // concurrently doesn't expect a k to have a null java_mirror.
       release_set_array_klass(comp_mirror(), k);
     }
+
+    if (k->is_value()) {
+      InstanceKlass* super = k->java_super();
+      set_val_type_mirror(mirror(), mirror());
+
+      // if the supertype is a restricted abstract class
+      if (super != SystemDictionary::Object_klass()) {
+        assert(super->access_flags().is_abstract(), "must be an abstract class");
+        oop ref_type_oop = super->java_mirror();
+        // set the reference projection type
+        set_ref_type_mirror(mirror(), ref_type_oop);
+
+        // set the value and reference projection types
+        set_val_type_mirror(ref_type_oop, mirror());
+        set_ref_type_mirror(ref_type_oop, ref_type_oop);
+      }
+    }
   } else {
     assert(fixup_mirror_list() != NULL, "fixup_mirror_list not initialized");
     fixup_mirror_list()->push(k);
   }
 }
@@ -1185,10 +1209,16 @@
       k->set_java_mirror_handle(NULL);
       return NULL;
     }
   }
 
+  if (k->is_value()) {
+    // Values have a val type mirror and a ref type mirror. Don't handle this for now. TODO:CDS
+    k->set_java_mirror_handle(NULL);
+    return NULL;
+  }
+
   // Now start archiving the mirror object
   oop archived_mirror = HeapShared::archive_heap_object(mirror, THREAD);
   if (archived_mirror == NULL) {
     return NULL;
   }
@@ -1476,10 +1506,30 @@
 void java_lang_Class::set_source_file(oop java_class, oop source_file) {
   assert(_source_file_offset != 0, "must be set");
   java_class->obj_field_put(_source_file_offset, source_file);
 }
 
+oop java_lang_Class::val_type_mirror(oop java_class) {
+  assert(_val_type_mirror_offset != 0, "must be set");
+  return java_class->obj_field(_val_type_mirror_offset);
+}
+
+void java_lang_Class::set_val_type_mirror(oop java_class, oop mirror) {
+  assert(_val_type_mirror_offset != 0, "must be set");
+  java_class->obj_field_put(_val_type_mirror_offset, mirror);
+}
+
+oop java_lang_Class::ref_type_mirror(oop java_class) {
+  assert(_ref_type_mirror_offset != 0, "must be set");
+  return java_class->obj_field(_ref_type_mirror_offset);
+}
+
+void java_lang_Class::set_ref_type_mirror(oop java_class, oop mirror) {
+  assert(_ref_type_mirror_offset != 0, "must be set");
+  java_class->obj_field_put(_ref_type_mirror_offset, mirror);
+}
+
 oop java_lang_Class::create_basic_type_mirror(const char* basic_type_name, BasicType type, TRAPS) {
   // This should be improved by adding a field at the Java level or by
   // introducing a new VM klass (see comment in ClassFileParser)
   oop java_class = InstanceMirrorKlass::cast(SystemDictionary::Class_klass())->allocate_instance(NULL, CHECK_NULL);
   if (type != T_VOID) {
@@ -1520,22 +1570,30 @@
 
 void java_lang_Class::print_signature(oop java_class, outputStream* st) {
   assert(java_lang_Class::is_instance(java_class), "must be a Class object");
   Symbol* name = NULL;
   bool is_instance = false;
+  bool is_value = false;
   if (is_primitive(java_class)) {
     name = vmSymbols::type_signature(primitive_type(java_class));
   } else {
     Klass* k = as_Klass(java_class);
     is_instance = k->is_instance_klass();
+    is_value = k->is_value();
     name = k->name();
   }
   if (name == NULL) {
     st->print("<null>");
     return;
   }
-  if (is_instance)  st->print("L");
+  if (is_instance)  {
+    if (is_value) {
+      st->print("Q");
+    } else {
+      st->print("L");
+    }
+  }
   st->write((char*) name->base(), (int) name->utf8_length());
   if (is_instance)  st->print(";");
 }
 
 Symbol* java_lang_Class::as_signature(oop java_class, bool intern_if_not_found) {
@@ -1553,11 +1611,11 @@
       name = k->name();
       name->increment_refcount();
     } else {
       ResourceMark rm;
       const char* sigstr = k->signature_name();
-      int         siglen = (int) strlen(sigstr);
+      int siglen = (int) strlen(sigstr);
       if (!intern_if_not_found) {
         name = SymbolTable::probe(sigstr, siglen);
       } else {
         name = SymbolTable::new_symbol(sigstr, siglen);
       }
@@ -1638,10 +1696,12 @@
   macro(classRedefinedCount_offset, k, "classRedefinedCount", int_signature,         false); \
   macro(_class_loader_offset,       k, "classLoader",         classloader_signature, false); \
   macro(_component_mirror_offset,   k, "componentType",       class_signature,       false); \
   macro(_module_offset,             k, "module",              module_signature,      false); \
   macro(_name_offset,               k, "name",                string_signature,      false); \
+  macro(_val_type_mirror_offset,    k, "valType",             class_signature,       false); \
+  macro(_ref_type_mirror_offset,    k, "refType",             class_signature,       false); \
   macro(_classData_offset,          k, "classData",           object_signature,      false);
 
 void java_lang_Class::compute_offsets() {
   if (offsets_computed) {
     return;
@@ -2513,12 +2573,12 @@
     }
     if (!skip_throwableInit_check) {
       assert(skip_fillInStackTrace_check, "logic error in backtrace filtering");
 
       // skip <init> methods of the exception class and superclasses
-      // This is simlar to classic VM.
-      if (method->name() == vmSymbols::object_initializer_name() &&
+      // This is similar to classic VM (before HotSpot).
+      if (method->is_object_constructor() &&
           throwable->is_a(method->method_holder())) {
         continue;
       } else {
         // there are none or we've seen them all - either way stop checking
         skip_throwableInit_check = true;
@@ -3797,11 +3857,11 @@
   return method == NULL ? NULL : java_lang_invoke_ResolvedMethodName::vmtarget(method);
 }
 
 bool java_lang_invoke_MemberName::is_method(oop mname) {
   assert(is_instance(mname), "must be MemberName");
-  return (flags(mname) & (MN_IS_METHOD | MN_IS_CONSTRUCTOR)) > 0;
+  return (flags(mname) & (MN_IS_METHOD | MN_IS_OBJECT_CONSTRUCTOR)) > 0;
 }
 
 void java_lang_invoke_MemberName::set_method(oop mname, oop resolved_method) {
   assert(is_instance(mname), "wrong type");
   mname->obj_field_put(_method_offset, resolved_method);
@@ -4303,10 +4363,12 @@
 int java_lang_Class::_static_oop_field_count_offset;
 int java_lang_Class::_class_loader_offset;
 int java_lang_Class::_module_offset;
 int java_lang_Class::_protection_domain_offset;
 int java_lang_Class::_component_mirror_offset;
+int java_lang_Class::_val_type_mirror_offset;
+int java_lang_Class::_ref_type_mirror_offset;
 int java_lang_Class::_init_lock_offset;
 int java_lang_Class::_signers_offset;
 int java_lang_Class::_name_offset;
 int java_lang_Class::_source_file_offset;
 int java_lang_Class::_classData_offset;
@@ -4397,10 +4459,15 @@
 int java_lang_reflect_RecordComponent::type_offset;
 int java_lang_reflect_RecordComponent::accessor_offset;
 int java_lang_reflect_RecordComponent::signature_offset;
 int java_lang_reflect_RecordComponent::annotations_offset;
 int java_lang_reflect_RecordComponent::typeAnnotations_offset;
+int jdk_internal_vm_jni_SubElementSelector::_arrayElementType_offset;
+int jdk_internal_vm_jni_SubElementSelector::_subElementType_offset;
+int jdk_internal_vm_jni_SubElementSelector::_offset_offset;
+int jdk_internal_vm_jni_SubElementSelector::_isFlattened_offset;
+int jdk_internal_vm_jni_SubElementSelector::_isFlattenable_offset;
 
 
 
 #define STACKTRACEELEMENT_FIELDS_DO(macro) \
   macro(declaringClassObject_offset,  k, "declaringClassObject", class_signature, false); \
@@ -4702,10 +4769,73 @@
   BYTE_CACHE_FIELDS_DO(FIELD_SERIALIZE_OFFSET);
 }
 #endif
 #undef BYTE_CACHE_FIELDS_DO
 
+#define SUBELEMENT_SELECTOR_FIELDS_DO(macro) \
+  macro(_arrayElementType_offset,  k, "arrayElementType", class_signature, false); \
+  macro(_subElementType_offset,    k, "subElementType",   class_signature, false); \
+  macro(_offset_offset,            k, "offset",           int_signature,   false); \
+  macro(_isFlattened_offset,       k, "isFlattened",      bool_signature,  false); \
+  macro(_isFlattenable_offset,     k, "isFlattenable",    bool_signature,  false);
+
+void jdk_internal_vm_jni_SubElementSelector::compute_offsets() {
+  InstanceKlass* k = SystemDictionary::jdk_internal_vm_jni_SubElementSelector_klass();
+  SUBELEMENT_SELECTOR_FIELDS_DO(FIELD_COMPUTE_OFFSET);
+}
+
+#if INCLUDE_CDS
+void jdk_internal_vm_jni_SubElementSelector::serialize_offsets(SerializeClosure* f) {
+  SUBELEMENT_SELECTOR_FIELDS_DO(FIELD_SERIALIZE_OFFSET);
+}
+#endif
+#undef SUBELEMENT_SELECTOR_FIELDS_DO
+
+Symbol* jdk_internal_vm_jni_SubElementSelector::symbol() {
+  return vmSymbols::jdk_internal_vm_jni_SubElementSelector();
+}
+
+oop jdk_internal_vm_jni_SubElementSelector::getArrayElementType(oop obj) {
+  return obj->obj_field(_arrayElementType_offset);
+}
+
+void jdk_internal_vm_jni_SubElementSelector::setArrayElementType(oop obj, oop type) {
+  obj->obj_field_put(_arrayElementType_offset, type);
+}
+
+oop jdk_internal_vm_jni_SubElementSelector::getSubElementType(oop obj) {
+  return obj->obj_field(_subElementType_offset);
+}
+
+void jdk_internal_vm_jni_SubElementSelector::setSubElementType(oop obj, oop type) {
+  obj->obj_field_put(_subElementType_offset, type);
+}
+
+int jdk_internal_vm_jni_SubElementSelector::getOffset(oop obj) {
+  return obj->int_field(_offset_offset);
+}
+
+void jdk_internal_vm_jni_SubElementSelector::setOffset(oop obj, int offset) {
+  obj->int_field_put(_offset_offset, offset);
+}
+
+bool jdk_internal_vm_jni_SubElementSelector::getIsFlattened(oop obj) {
+  return obj->bool_field(_isFlattened_offset);
+}
+
+void jdk_internal_vm_jni_SubElementSelector::setIsFlattened(oop obj, bool b) {
+  obj->bool_field_put(_isFlattened_offset, b);
+}
+
+bool jdk_internal_vm_jni_SubElementSelector::getIsFlattenable(oop obj) {
+  return obj->bool_field(_isFlattenable_offset);
+}
+
+void jdk_internal_vm_jni_SubElementSelector::setIsFlattenable(oop obj, bool b) {
+  obj->bool_field_put(_isFlattenable_offset, b);
+}
+
 jbyte java_lang_Byte::value(oop obj) {
    jvalue v;
    java_lang_boxing_object::get_value(obj, &v);
    return v.b;
 }
diff a/src/hotspot/share/classfile/javaClasses.hpp b/src/hotspot/share/classfile/javaClasses.hpp
--- a/src/hotspot/share/classfile/javaClasses.hpp
+++ b/src/hotspot/share/classfile/javaClasses.hpp
@@ -84,10 +84,11 @@
   f(java_lang_StackTraceElement) \
   f(java_lang_StackFrameInfo) \
   f(java_lang_LiveStackFrameInfo) \
   f(java_util_concurrent_locks_AbstractOwnableSynchronizer) \
   f(jdk_internal_misc_UnsafeConstants) \
+  f(jdk_internal_vm_jni_SubElementSelector) \
   //end
 
 #define BASIC_JAVA_CLASSES_DO(f) \
         BASIC_JAVA_CLASSES_DO_PART1(f) \
         BASIC_JAVA_CLASSES_DO_PART2(f)
@@ -263,10 +264,12 @@
   static int _class_loader_offset;
   static int _module_offset;
   static int _component_mirror_offset;
   static int _name_offset;
   static int _source_file_offset;
+  static int _val_type_mirror_offset;
+  static int _ref_type_mirror_offset;
   static int _classData_offset;
 
   static bool offsets_computed;
   static int classRedefinedCount_offset;
 
@@ -322,10 +325,11 @@
   static Klass* array_klass_acquire(oop java_class);
   static void release_set_array_klass(oop java_class, Klass* klass);
   // compiler support for class operations
   static int klass_offset_in_bytes()                { return _klass_offset; }
   static int array_klass_offset_in_bytes()          { return _array_klass_offset; }
+  static int component_mirror_offset_in_bytes()     { return _component_mirror_offset; }
   // Support for classRedefinedCount field
   static int classRedefinedCount(oop the_class_mirror);
   static void set_classRedefinedCount(oop the_class_mirror, int value);
 
   // Support for embedded per-class oops
@@ -339,10 +343,16 @@
 
   static oop class_loader(oop java_class);
   static void set_module(oop java_class, oop module);
   static oop module(oop java_class);
 
+  static void set_ref_type_mirror(oop java_class, oop mirror);
+  static oop ref_type_mirror(oop java_class);
+
+  static void set_val_type_mirror(oop java_class, oop mirror);
+  static oop val_type_mirror(oop java_class);
+
   static oop name(Handle java_class, TRAPS);
 
   static oop source_file(oop java_class);
   static void set_source_file(oop java_class, oop source_file);
 
@@ -1147,11 +1157,11 @@
   static bool is_method(oop obj);
 
   // Relevant integer codes (keep these in synch. with MethodHandleNatives.Constants):
   enum {
     MN_IS_METHOD             = 0x00010000, // method (not constructor)
-    MN_IS_CONSTRUCTOR        = 0x00020000, // constructor
+    MN_IS_OBJECT_CONSTRUCTOR = 0x00020000, // constructor
     MN_IS_FIELD              = 0x00040000, // field
     MN_IS_TYPE               = 0x00080000, // nested type
     MN_CALLER_SENSITIVE      = 0x00100000, // @CallerSensitive annotation detected
     MN_REFERENCE_KIND_SHIFT  = 24, // refKind
     MN_REFERENCE_KIND_MASK   = 0x0F000000 >> MN_REFERENCE_KIND_SHIFT,
@@ -1672,10 +1682,34 @@
   static void compute_offsets(InstanceKlass* k);
   static objArrayOop  cache(InstanceKlass *k);
   static void serialize_offsets(SerializeClosure* f) NOT_CDS_RETURN;
 };
 
+class jdk_internal_vm_jni_SubElementSelector : AllStatic {
+ private:
+  static int _arrayElementType_offset;
+  static int _subElementType_offset;
+  static int _offset_offset;
+  static int _isFlattened_offset;
+  static int _isFlattenable_offset;
+ public:
+  static Symbol* symbol();
+  static void compute_offsets();
+  static void serialize_offsets(SerializeClosure* f) NOT_CDS_RETURN;
+
+  static oop getArrayElementType(oop obj);
+  static void setArrayElementType(oop obj, oop type);
+  static oop getSubElementType(oop obj);
+  static void setSubElementType(oop obj, oop type);
+  static int getOffset(oop obj);
+  static void setOffset(oop obj, int offset);
+  static bool getIsFlattened(oop obj);
+  static void setIsFlattened(oop obj, bool b);
+  static bool getIsFlattenable(oop obj);
+  static void setIsFlattenable(oop obj, bool b);
+};
+
 // Use to declare fields that need to be injected into Java classes
 // for the JVM to use.  The name_index and signature_index are
 // declared in vmSymbols.  The may_be_java flag is used to declare
 // fields that might already exist in Java but should be injected if
 // they don't.  Otherwise the field is unconditionally injected and
diff a/src/hotspot/share/classfile/systemDictionary.cpp b/src/hotspot/share/classfile/systemDictionary.cpp
--- a/src/hotspot/share/classfile/systemDictionary.cpp
+++ b/src/hotspot/share/classfile/systemDictionary.cpp
@@ -58,28 +58,31 @@
 #include "memory/metaspaceClosure.hpp"
 #include "memory/oopFactory.hpp"
 #include "memory/resourceArea.hpp"
 #include "memory/universe.hpp"
 #include "oops/access.inline.hpp"
+#include "oops/fieldStreams.inline.hpp"
 #include "oops/instanceKlass.hpp"
 #include "oops/instanceRefKlass.hpp"
 #include "oops/klass.inline.hpp"
 #include "oops/method.inline.hpp"
 #include "oops/methodData.hpp"
 #include "oops/objArrayKlass.hpp"
 #include "oops/objArrayOop.inline.hpp"
 #include "oops/oop.inline.hpp"
 #include "oops/symbol.hpp"
 #include "oops/typeArrayKlass.hpp"
+#include "oops/valueKlass.hpp"
 #include "prims/jvmtiExport.hpp"
 #include "prims/methodHandles.hpp"
 #include "runtime/arguments.hpp"
 #include "runtime/biasedLocking.hpp"
 #include "runtime/handles.inline.hpp"
 #include "runtime/java.hpp"
 #include "runtime/javaCalls.hpp"
 #include "runtime/mutexLocker.hpp"
+#include "runtime/os.hpp"
 #include "runtime/sharedRuntime.hpp"
 #include "runtime/signature.hpp"
 #include "services/classLoadingService.hpp"
 #include "services/diagnosticCommand.hpp"
 #include "services/threadService.hpp"
@@ -292,11 +295,11 @@
                                                                        Handle protection_domain,
                                                                        TRAPS) {
   assert(class_name != NULL && !Signature::is_array(class_name), "must be");
   if (Signature::has_envelope(class_name)) {
     ResourceMark rm(THREAD);
-    // Ignore wrapping L and ;.
+    // Ignore wrapping L and ;. (and Q and ; for value types);
     TempNewSymbol name = SymbolTable::new_symbol(class_name->as_C_string() + 1,
                                                  class_name->utf8_length() - 2);
     return resolve_instance_class_or_null(name, class_loader, protection_domain, THREAD);
   } else {
     return resolve_instance_class_or_null(class_name, class_loader, protection_domain, THREAD);
@@ -333,11 +336,10 @@
     k = TypeArrayKlass::cast(k)->array_klass(ndims, CHECK_NULL);
   }
   return k;
 }
 
-
 // Must be called for any super-class or super-interface resolution
 // during class definition to allow class circularity checking
 // super-interface callers:
 //    parse_interfaces - for defineClass & jvmtiRedefineClasses
 // super-class callers:
@@ -477,10 +479,55 @@
   }
 
   return superk;
 }
 
+Klass* SystemDictionary::resolve_flattenable_field_or_fail(AllFieldStream* fs,
+                                                           Handle class_loader,
+                                                           Handle protection_domain,
+                                                           bool throw_error,
+                                                           TRAPS) {
+  Symbol* class_name = fs->signature()->fundamental_name(THREAD);
+  class_loader = Handle(THREAD, java_lang_ClassLoader::non_reflection_class_loader(class_loader()));
+  ClassLoaderData* loader_data = class_loader_data(class_loader);
+  unsigned int p_hash = placeholders()->compute_hash(class_name);
+  int p_index = placeholders()->hash_to_index(p_hash);
+  bool throw_circularity_error = false;
+  PlaceholderEntry* oldprobe;
+
+  {
+    MutexLocker mu(THREAD, SystemDictionary_lock);
+    oldprobe = placeholders()->get_entry(p_index, p_hash, class_name, loader_data);
+    if (oldprobe != NULL &&
+      oldprobe->check_seen_thread(THREAD, PlaceholderTable::FLATTENABLE_FIELD)) {
+      throw_circularity_error = true;
+
+    } else {
+      placeholders()->find_and_add(p_index, p_hash, class_name, loader_data,
+                                   PlaceholderTable::FLATTENABLE_FIELD, NULL, THREAD);
+    }
+  }
+
+  Klass* klass = NULL;
+  if (!throw_circularity_error) {
+    klass = SystemDictionary::resolve_or_fail(class_name, class_loader,
+                                               protection_domain, true, THREAD);
+  } else {
+    ResourceMark rm(THREAD);
+    THROW_MSG_NULL(vmSymbols::java_lang_ClassCircularityError(), class_name->as_C_string());
+  }
+
+  {
+    MutexLocker mu(THREAD, SystemDictionary_lock);
+    placeholders()->find_and_remove(p_index, p_hash, class_name, loader_data,
+                                    PlaceholderTable::FLATTENABLE_FIELD, THREAD);
+  }
+
+  class_name->decrement_refcount();
+  return klass;
+}
+
 void SystemDictionary::validate_protection_domain(InstanceKlass* klass,
                                                   Handle class_loader,
                                                   Handle protection_domain,
                                                   TRAPS) {
   // Now we have to call back to java to check if the initating class has access
@@ -1004,11 +1051,11 @@
     // dimension and object_key in FieldArrayInfo are assigned as a
     // side-effect of this call
     SignatureStream ss(class_name, false);
     int ndims = ss.skip_array_prefix();  // skip all '['s
     BasicType t = ss.type();
-    if (t != T_OBJECT) {
+    if (t != T_OBJECT && t != T_VALUETYPE) {
       k = Universe::typeArrayKlassObj(t);
     } else {
       k = SystemDictionary::find(ss.as_symbol(), class_loader, protection_domain, THREAD);
     }
     if (k != NULL) {
@@ -2276,11 +2323,11 @@
     // For array classes, their Klass*s are not kept in the
     // constraint table. The element Klass*s are.
     SignatureStream ss(class_name, false);
     int ndims = ss.skip_array_prefix();  // skip all '['s
     BasicType t = ss.type();
-    if (t != T_OBJECT) {
+    if (t != T_OBJECT && t != T_VALUETYPE) {
       klass = Universe::typeArrayKlassObj(t);
     } else {
       MutexLocker mu(THREAD, SystemDictionary_lock);
       klass = constraints()->find_constrained_klass(ss.as_symbol(), class_loader);
     }
diff a/src/hotspot/share/classfile/systemDictionary.hpp b/src/hotspot/share/classfile/systemDictionary.hpp
--- a/src/hotspot/share/classfile/systemDictionary.hpp
+++ b/src/hotspot/share/classfile/systemDictionary.hpp
@@ -121,10 +121,11 @@
 //
 
 class BootstrapInfo;
 class ClassFileStream;
 class Dictionary;
+class AllFieldStream;
 class PlaceholderTable;
 class LoaderConstraintTable;
 template <MEMFLAGS F> class HashtableBucket;
 class ResolutionErrorTable;
 class SymbolPropertyTable;
@@ -147,10 +148,11 @@
 // of the VM start-up sequence.
 //
 #define WK_KLASSES_DO(do_klass)                                                                                 \
   /* well-known classes */                                                                                      \
   do_klass(Object_klass,                                java_lang_Object                                      ) \
+  do_klass(IdentityObject_klass,                        java_lang_IdentityObject                              ) \
   do_klass(String_klass,                                java_lang_String                                      ) \
   do_klass(Class_klass,                                 java_lang_Class                                       ) \
   do_klass(Cloneable_klass,                             java_lang_Cloneable                                   ) \
   do_klass(ClassLoader_klass,                           java_lang_ClassLoader                                 ) \
   do_klass(Serializable_klass,                          java_io_Serializable                                  ) \
@@ -217,10 +219,11 @@
   do_klass(BootstrapMethodError_klass,                  java_lang_BootstrapMethodError                        ) \
   do_klass(CallSite_klass,                              java_lang_invoke_CallSite                             ) \
   do_klass(Context_klass,                               java_lang_invoke_MethodHandleNatives_CallSiteContext  ) \
   do_klass(ConstantCallSite_klass,                      java_lang_invoke_ConstantCallSite                     ) \
   do_klass(MutableCallSite_klass,                       java_lang_invoke_MutableCallSite                      ) \
+  do_klass(ValueBootstrapMethods_klass,                 java_lang_invoke_ValueBootstrapMethods                ) \
   do_klass(VolatileCallSite_klass,                      java_lang_invoke_VolatileCallSite                     ) \
   /* Note: MethodHandle must be first, and VolatileCallSite last in group */                                    \
                                                                                                                 \
   do_klass(AssertionStatusDirectives_klass,             java_lang_AssertionStatusDirectives                   ) \
   do_klass(StringBuffer_klass,                          java_lang_StringBuffer                                ) \
@@ -263,10 +266,11 @@
   do_klass(Long_klass,                                  java_lang_Long                                        ) \
                                                                                                                 \
   /* force inline of iterators */                                                                               \
   do_klass(Iterator_klass,                              java_util_Iterator                                    ) \
                                                                                                                 \
+  do_klass(jdk_internal_vm_jni_SubElementSelector_klass, jdk_internal_vm_jni_SubElementSelector               ) \
   /* support for records */                                                                                     \
   do_klass(RecordComponent_klass,                       java_lang_reflect_RecordComponent                     ) \
                                                                                                                 \
   /*end*/
 
@@ -316,10 +320,16 @@
                                               Handle class_loader,
                                               Handle protection_domain,
                                               bool is_superclass,
                                               TRAPS);
 
+  static Klass* resolve_flattenable_field_or_fail(AllFieldStream* fs,
+                                                  Handle class_loader,
+                                                  Handle protection_domain,
+                                                  bool throw_error,
+                                                  TRAPS);
+
   // Parse new stream. This won't update the dictionary or class
   // hierarchy, simply parse the stream. Used by JVMTI RedefineClasses
   // and by Unsafe_DefineAnonymousClass and jvm_lookup_define_class.
   static InstanceKlass* parse_stream(Symbol* class_name,
                                      Handle class_loader,
@@ -410,10 +420,11 @@
     assert(k != NULL, "klass not loaded");
     return k;
   }
 
   static bool resolve_wk_klass(WKID id, TRAPS);
+  static InstanceKlass* check_klass_ValhallaClasses(InstanceKlass* k) { return k; }
   static void resolve_wk_klasses_until(WKID limit_id, WKID &start_id, TRAPS);
   static void resolve_wk_klasses_through(WKID end_id, WKID &start_id, TRAPS) {
     int limit = (int)end_id + 1;
     resolve_wk_klasses_until((WKID) limit, start_id, THREAD);
   }
diff a/src/hotspot/share/classfile/verificationType.cpp b/src/hotspot/share/classfile/verificationType.cpp
--- a/src/hotspot/share/classfile/verificationType.cpp
+++ b/src/hotspot/share/classfile/verificationType.cpp
@@ -58,19 +58,22 @@
     if (log_is_enabled(Debug, class, resolve)) {
       Verifier::trace_class_resolution(this_class, klass);
     }
   }
 
+  if (this_class->access_flags().is_inline_type()) return false;
   if (this_class->is_interface() && (!from_field_is_protected ||
       from_name != vmSymbols::java_lang_Object())) {
     // If we are not trying to access a protected field or method in
     // java.lang.Object then, for arrays, we only allow assignability
-    // to interfaces java.lang.Cloneable and java.io.Serializable.
+    // to interfaces java.lang.Cloneable, java.io.Serializable,
+    // and java.lang.IdentityObject.
     // Otherwise, we treat interfaces as java.lang.Object.
     return !from_is_array ||
       this_class == SystemDictionary::Cloneable_klass() ||
-      this_class == SystemDictionary::Serializable_klass();
+      this_class == SystemDictionary::Serializable_klass() ||
+      this_class == SystemDictionary::IdentityObject_klass();
   } else if (from_is_object) {
     Klass* from_class;
     if (klass->is_hidden() && klass->name() == from_name) {
       from_class = klass;
     } else {
@@ -118,18 +121,86 @@
     return resolve_and_check_assignability(klass, name(), from.name(),
           from_field_is_protected, from.is_array(), from.is_object(), THREAD);
   } else if (is_array() && from.is_array()) {
     VerificationType comp_this = get_component(context, CHECK_false);
     VerificationType comp_from = from.get_component(context, CHECK_false);
+
+/*
+    // This code implements non-covariance between value type arrays and both
+    // arrays of objects and arrays of interface types.  If covariance is
+    // supported for value type arrays then this code should be removed.
+    if (comp_from.is_valuetype() && !comp_this.is_null() && comp_this.is_reference()) {
+      // An array of value types is not assignable to an array of java.lang.Objects.
+      if (comp_this.name() == vmSymbols::java_lang_Object()) {
+        return false;
+      }
+
+      // Need to load 'comp_this' to see if it is an interface.
+      InstanceKlass* klass = context->current_class();
+      {
+        HandleMark hm(THREAD);
+        Klass* comp_this_class = SystemDictionary::resolve_or_fail(
+            comp_this.name(), Handle(THREAD, klass->class_loader()),
+            Handle(THREAD, klass->protection_domain()), true, CHECK_false);
+        klass->class_loader_data()->record_dependency(comp_this_class);
+        if (log_is_enabled(Debug, class, resolve)) {
+          Verifier::trace_class_resolution(comp_this_class, klass);
+        }
+        // An array of value types is not assignable to an array of interface types.
+        if (comp_this_class->is_interface()) {
+          return false;
+        }
+      }
+    }
+*/
     if (!comp_this.is_bogus() && !comp_from.is_bogus()) {
       return comp_this.is_component_assignable_from(comp_from, context,
                                                     from_field_is_protected, THREAD);
     }
   }
   return false;
 }
 
+bool VerificationType::is_valuetype_assignable_from(const VerificationType& from) const {
+  // Check that 'from' is not null, is a value type, and is the same value type.
+  assert(is_valuetype(), "called with a non-valuetype type");
+  assert(!is_null(), "valuetype is not null");
+  return (!from.is_null() && from.is_valuetype() && name() == from.name());
+}
+
+bool VerificationType::is_ref_assignable_from_value_type(const VerificationType& from, ClassVerifier* context, TRAPS) const {
+  assert(!from.is_null(), "Value type should not be null");
+  if (!is_null() && (name()->is_same_fundamental_type(from.name()) ||
+      name() == vmSymbols::java_lang_Object())) {
+    return true;
+  }
+
+  // Need to load 'this' to see if it is an interface or supertype.
+  InstanceKlass* klass = context->current_class();
+  {
+    HandleMark hm(THREAD);
+    Klass* this_class = SystemDictionary::resolve_or_fail(
+        name(), Handle(THREAD, klass->class_loader()),
+        Handle(THREAD, klass->protection_domain()), true, CHECK_false);
+    klass->class_loader_data()->record_dependency(this_class);
+    if (log_is_enabled(Debug, class, resolve)) {
+      Verifier::trace_class_resolution(this_class, klass);
+    }
+    if (this_class->is_interface()) {
+      return true;
+    } else {
+      Klass* from_class = SystemDictionary::resolve_or_fail(
+        from.name(), Handle(THREAD, klass->class_loader()),
+        Handle(THREAD, klass->protection_domain()), true, CHECK_false);
+      if (log_is_enabled(Debug, class, resolve)) {
+        Verifier::trace_class_resolution(from_class, klass);
+      }
+      return from_class->is_subclass_of(this_class);
+    }
+  }
+}
+
 VerificationType VerificationType::get_component(ClassVerifier *context, TRAPS) const {
   assert(is_array() && name()->utf8_length() >= 2, "Must be a valid array");
   SignatureStream ss(name(), false);
   ss.skip_array_prefix(1);
   switch (ss.type()) {
@@ -140,17 +211,20 @@
     case T_INT:     return VerificationType(Integer);
     case T_LONG:    return VerificationType(Long);
     case T_FLOAT:   return VerificationType(Float);
     case T_DOUBLE:  return VerificationType(Double);
     case T_ARRAY:
-    case T_OBJECT: {
+    case T_OBJECT:
+    case T_VALUETYPE: {
       guarantee(ss.is_reference(), "unchecked verifier input?");
       Symbol* component = ss.as_symbol();
       // Create another symbol to save as signature stream unreferences this symbol.
       Symbol* component_copy = context->create_temporary_symbol(component);
       assert(component_copy == component, "symbols don't match");
-      return VerificationType::reference_type(component_copy);
+      return (ss.type() == T_VALUETYPE) ?
+        VerificationType::valuetype_type(component_copy) :
+        VerificationType::reference_type(component_copy);
    }
    default:
      // Met an invalid type signature, e.g. [X
      return VerificationType::bogus_type();
   }
@@ -172,18 +246,22 @@
     case Double:           st->print("double"); break;
     case Long_2nd:         st->print("long_2nd"); break;
     case Double_2nd:       st->print("double_2nd"); break;
     case Null:             st->print("null"); break;
     case ReferenceQuery:   st->print("reference type"); break;
+    case ValueTypeQuery:   st->print("inline type"); break;
+    case NonScalarQuery:   st->print("reference or inline type"); break;
     case Category1Query:   st->print("category1 type"); break;
     case Category2Query:   st->print("category2 type"); break;
     case Category2_2ndQuery: st->print("category2_2nd type"); break;
     default:
       if (is_uninitialized_this()) {
         st->print("uninitializedThis");
       } else if (is_uninitialized()) {
         st->print("uninitialized %d", bci());
+      } else if (is_valuetype()) {
+        name()->print_Qvalue_on(st);
       } else {
         if (name() != NULL) {
           name()->print_value_on(st);
         } else {
           st->print_cr("NULL");
diff a/src/hotspot/share/classfile/verifier.cpp b/src/hotspot/share/classfile/verifier.cpp
--- a/src/hotspot/share/classfile/verifier.cpp
+++ b/src/hotspot/share/classfile/verifier.cpp
@@ -58,10 +58,11 @@
 #include "utilities/bytes.hpp"
 
 #define NOFAILOVER_MAJOR_VERSION                       51
 #define NONZERO_PADDING_BYTES_IN_SWITCH_MAJOR_VERSION  51
 #define STATIC_METHOD_IN_INTERFACE_MAJOR_VERSION       52
+#define VALUETYPE_MAJOR_VERSION                        56
 #define MAX_ARRAY_DIMENSIONS 255
 
 // Access to external entry for VerifyClassForMajorVersion - old byte code verifier
 
 extern "C" {
@@ -262,11 +263,11 @@
   bool is_reflect = refl_magic_klass != NULL && klass->is_subtype_of(refl_magic_klass);
 
   return (should_verify_for(klass->class_loader(), should_verify_class) &&
     // return if the class is a bootstrapping class
     // or defineClass specified not to verify by default (flags override passed arg)
-    // We need to skip the following four for bootstraping
+    // We need to skip the following four for bootstrapping
     name != vmSymbols::java_lang_Object() &&
     name != vmSymbols::java_lang_Class() &&
     name != vmSymbols::java_lang_String() &&
     name != vmSymbols::java_lang_Throwable() &&
 
@@ -484,10 +485,17 @@
       ss->print("Expected stackmap frame at this location.");
       break;
     case BAD_STACKMAP:
       ss->print("Invalid stackmap specification.");
       break;
+    case WRONG_VALUE_TYPE:
+      ss->print("Type ");
+      _type.details(ss);
+      ss->print(" and type ");
+      _expected.details(ss);
+      ss->print(" must be identical inline types.");
+      break;
     case UNKNOWN:
     default:
       ShouldNotReachHere();
       ss->print_cr("Unknown");
   }
@@ -578,15 +586,23 @@
   }
 }
 
 // Methods in ClassVerifier
 
+VerificationType reference_or_valuetype(InstanceKlass* klass) {
+  if (klass->is_value()) {
+    return VerificationType::valuetype_type(klass->name());
+  } else {
+    return VerificationType::reference_type(klass->name());
+  }
+}
+
 ClassVerifier::ClassVerifier(
     InstanceKlass* klass, TRAPS)
     : _thread(THREAD), _previous_symbol(NULL), _symbols(NULL), _exception_type(NULL),
       _message(NULL), _method_signatures_table(NULL), _klass(klass) {
-  _this_type = VerificationType::reference_type(klass->name());
+  _this_type = reference_or_valuetype(klass);
 }
 
 ClassVerifier::~ClassVerifier() {
   // Decrement the reference count for any symbols created.
   if (_symbols != NULL) {
@@ -1027,11 +1043,11 @@
         case Bytecodes::_aaload : {
           type = current_frame.pop_stack(
             VerificationType::integer_type(), CHECK_VERIFY(this));
           atype = current_frame.pop_stack(
             VerificationType::reference_check(), CHECK_VERIFY(this));
-          if (!atype.is_reference_array()) {
+          if (!atype.is_nonscalar_array()) {
             verify_error(ErrorContext::bad_type(bci,
                 current_frame.stack_top_ctx(),
                 TypeOrigin::implicit(VerificationType::reference_check())),
                 bad_type_msg, "aaload");
             return;
@@ -1201,11 +1217,11 @@
           type2 = current_frame.pop_stack(
             VerificationType::integer_type(), CHECK_VERIFY(this));
           atype = current_frame.pop_stack(
             VerificationType::reference_check(), CHECK_VERIFY(this));
           // more type-checking is done at runtime
-          if (!atype.is_reference_array()) {
+          if (!atype.is_nonscalar_array()) {
             verify_error(ErrorContext::bad_type(bci,
                 current_frame.stack_top_ctx(),
                 TypeOrigin::implicit(VerificationType::reference_check())),
                 bad_type_msg, "aastore");
             return;
@@ -1601,16 +1617,16 @@
             &current_frame, target, CHECK_VERIFY(this));
           no_control_flow = false; break;
         case Bytecodes::_if_acmpeq :
         case Bytecodes::_if_acmpne :
           current_frame.pop_stack(
-            VerificationType::reference_check(), CHECK_VERIFY(this));
+            VerificationType::nonscalar_check(), CHECK_VERIFY(this));
           // fall through
         case Bytecodes::_ifnull :
         case Bytecodes::_ifnonnull :
           current_frame.pop_stack(
-            VerificationType::reference_check(), CHECK_VERIFY(this));
+            VerificationType::nonscalar_check(), CHECK_VERIFY(this));
           target = bcs.dest();
           stackmap_table.check_jump_target
             (&current_frame, target, CHECK_VERIFY(this));
           no_control_flow = false; break;
         case Bytecodes::_goto :
@@ -1657,11 +1673,11 @@
           verify_return_value(return_type, type, bci,
                               &current_frame, CHECK_VERIFY(this));
           no_control_flow = true; break;
         case Bytecodes::_areturn :
           type = current_frame.pop_stack(
-            VerificationType::reference_check(), CHECK_VERIFY(this));
+            VerificationType::nonscalar_check(), CHECK_VERIFY(this));
           verify_return_value(return_type, type, bci,
                               &current_frame, CHECK_VERIFY(this));
           no_control_flow = true; break;
         case Bytecodes::_return :
           if (return_type != VerificationType::bogus_type()) {
@@ -1669,11 +1685,11 @@
                          "Method expects a return value");
             return;
           }
           // Make sure "this" has been initialized if current method is an
           // <init>.
-          if (_method->name() == vmSymbols::object_initializer_name() &&
+          if (_method->is_object_constructor() &&
               current_frame.flag_this_uninit()) {
             verify_error(ErrorContext::bad_code(bci),
                          "Constructor must call super() or this() "
                          "before return");
             return;
@@ -1689,22 +1705,29 @@
         case Bytecodes::_putfield :
           // pass FALSE, operand can't be an array type for getfield/putfield.
           verify_field_instructions(
             &bcs, &current_frame, cp, false, CHECK_VERIFY(this));
           no_control_flow = false; break;
+        case Bytecodes::_withfield :
+          if (_klass->major_version() < VALUETYPE_MAJOR_VERSION) {
+            class_format_error(
+              "withfield not supported by this class file version (%d.%d), class %s",
+              _klass->major_version(), _klass->minor_version(), _klass->external_name());
+            return;
+          }
+          // pass FALSE, operand can't be an array type for withfield.
+          verify_field_instructions(
+            &bcs, &current_frame, cp, false, CHECK_VERIFY(this));
+          no_control_flow = false; break;
         case Bytecodes::_invokevirtual :
         case Bytecodes::_invokespecial :
         case Bytecodes::_invokestatic :
-          verify_invoke_instructions(
-            &bcs, code_length, &current_frame, (bci >= ex_min && bci < ex_max),
-            &this_uninit, return_type, cp, &stackmap_table, CHECK_VERIFY(this));
-          no_control_flow = false; break;
         case Bytecodes::_invokeinterface :
         case Bytecodes::_invokedynamic :
           verify_invoke_instructions(
             &bcs, code_length, &current_frame, (bci >= ex_min && bci < ex_max),
-            &this_uninit, return_type, cp, &stackmap_table, CHECK_VERIFY(this));
+            &this_uninit, cp, &stackmap_table, CHECK_VERIFY(this));
           no_control_flow = false; break;
         case Bytecodes::_new :
         {
           index = bcs.get_index_u2();
           verify_cp_class_type(bci, index, cp, CHECK_VERIFY(this));
@@ -1718,10 +1741,32 @@
           }
           type = VerificationType::uninitialized_type(bci);
           current_frame.push_stack(type, CHECK_VERIFY(this));
           no_control_flow = false; break;
         }
+        case Bytecodes::_defaultvalue :
+        {
+          if (_klass->major_version() < VALUETYPE_MAJOR_VERSION) {
+            class_format_error(
+              "defaultvalue not supported by this class file version (%d.%d), class %s",
+              _klass->major_version(), _klass->minor_version(), _klass->external_name());
+            return;
+          }
+          index = bcs.get_index_u2();
+          verify_cp_class_type(bci, index, cp, CHECK_VERIFY(this));
+          VerificationType ref_type = cp_index_to_type(index, cp, CHECK_VERIFY(this));
+          if (!ref_type.is_object()) {
+            verify_error(ErrorContext::bad_type(bci,
+                TypeOrigin::cp(index, ref_type)),
+                "Illegal defaultvalue instruction");
+            return;
+          }
+          VerificationType value_type =
+            VerificationType::change_ref_to_valuetype(ref_type);
+          current_frame.push_stack(value_type, CHECK_VERIFY(this));
+          no_control_flow = false; break;
+        }
         case Bytecodes::_newarray :
           type = get_newarray_type(bcs.get_index(), bci, CHECK_VERIFY(this));
           current_frame.pop_stack(
             VerificationType::integer_type(),  CHECK_VERIFY(this));
           current_frame.push_stack(type, CHECK_VERIFY(this));
@@ -1758,14 +1803,15 @@
           current_frame.push_stack(
             VerificationType::integer_type(), CHECK_VERIFY(this));
           no_control_flow = false; break;
         }
         case Bytecodes::_monitorenter :
-        case Bytecodes::_monitorexit :
-          current_frame.pop_stack(
-            VerificationType::reference_check(), CHECK_VERIFY(this));
+        case Bytecodes::_monitorexit : {
+          VerificationType ref = current_frame.pop_stack(
+            VerificationType::nonscalar_check(), CHECK_VERIFY(this));
           no_control_flow = false; break;
+        }
         case Bytecodes::_multianewarray :
         {
           index = bcs.get_index_u2();
           u2 dim = *(bcs.bcp()+3);
           verify_cp_class_type(bci, index, cp, CHECK_VERIFY(this));
@@ -2020,10 +2066,11 @@
   // We must check was_recursively_verified() before we get here.
   guarantee(cp->cache() == NULL, "not rewritten yet");
 
   verify_cp_index(bci, cp, index, CHECK_VERIFY(this));
   unsigned int tag = cp->tag_at(index).value();
+
   if ((types & (1 << tag)) == 0) {
     verify_error(ErrorContext::bad_cp_index(bci, index),
       "Illegal type at constant pool entry %d in class %s",
       index, cp->pool_holder()->external_name());
     return;
@@ -2134,11 +2181,11 @@
   constantTag tag = cp->tag_at(index);
   unsigned int types = 0;
   if (opcode == Bytecodes::_ldc || opcode == Bytecodes::_ldc_w) {
     if (!tag.is_unresolved_klass()) {
       types = (1 << JVM_CONSTANT_Integer) | (1 << JVM_CONSTANT_Float)
-            | (1 << JVM_CONSTANT_String)  | (1 << JVM_CONSTANT_Class)
+            | (1 << JVM_CONSTANT_String) | (1 << JVM_CONSTANT_Class)
             | (1 << JVM_CONSTANT_MethodHandle) | (1 << JVM_CONSTANT_MethodType)
             | (1 << JVM_CONSTANT_Dynamic);
       // Note:  The class file parser already verified the legality of
       // MethodHandle and MethodType constants.
       verify_cp_type(bci, index, cp, types, CHECK_VERIFY(this));
@@ -2310,17 +2357,18 @@
 
   // Get referenced class type
   VerificationType ref_class_type = cp_ref_index_to_type(
     index, cp, CHECK_VERIFY(this));
   if (!ref_class_type.is_object() &&
-    (!allow_arrays || !ref_class_type.is_array())) {
+      (!allow_arrays || !ref_class_type.is_array())) {
     verify_error(ErrorContext::bad_type(bcs->bci(),
         TypeOrigin::cp(index, ref_class_type)),
         "Expecting reference to class in class %s at constant pool index %d",
         _klass->external_name(), index);
     return;
   }
+
   VerificationType target_class_type = ref_class_type;
 
   assert(sizeof(VerificationType) == sizeof(uintptr_t),
         "buffer type must match VerificationType size");
   uintptr_t field_type_buffer[2];
@@ -2346,10 +2394,29 @@
       for (int i = n - 1; i >= 0; i--) {
         current_frame->pop_stack(field_type[i], CHECK_VERIFY(this));
       }
       break;
     }
+    case Bytecodes::_withfield: {
+      for (int i = n - 1; i >= 0; i--) {
+        current_frame->pop_stack(field_type[i], CHECK_VERIFY(this));
+      }
+      // stack_object_type and target_class_type must be the same value type.
+      stack_object_type =
+        current_frame->pop_stack(VerificationType::valuetype_check(), CHECK_VERIFY(this));
+      VerificationType target_value_type =
+        VerificationType::change_ref_to_valuetype(target_class_type);
+      if (!stack_object_type.equals(target_value_type)) {
+        verify_error(ErrorContext::bad_value_type(bci,
+            current_frame->stack_top_ctx(),
+            TypeOrigin::cp(index, target_class_type)),
+            "Invalid type on operand stack in withfield instruction");
+        return;
+      }
+      current_frame->push_stack(target_value_type, CHECK_VERIFY(this));
+      break;
+    }
     case Bytecodes::_getfield: {
       stack_object_type = current_frame->pop_stack(
         target_class_type, CHECK_VERIFY(this));
       for (int i = 0; i < n; i++) {
         current_frame->push_stack(field_type[i], CHECK_VERIFY(this));
@@ -2754,11 +2821,11 @@
   return false;
 }
 
 void ClassVerifier::verify_invoke_instructions(
     RawBytecodeStream* bcs, u4 code_length, StackMapFrame* current_frame,
-    bool in_try_block, bool *this_uninit, VerificationType return_type,
+    bool in_try_block, bool *this_uninit,
     const constantPoolHandle& cp, StackMapTable* stackmap_table, TRAPS) {
   // Make sure the constant pool item is the right type
   u2 index = bcs->get_index_u2();
   Bytecodes::Code opcode = bcs->raw_code();
   unsigned int types = 0;
@@ -2786,11 +2853,11 @@
 
   // Method signature was checked in ClassFileParser.
   assert(SignatureVerifier::is_valid_method_signature(method_sig),
          "Invalid method signature");
 
-  // Get referenced class type
+  // Get referenced class
   VerificationType ref_class_type;
   if (opcode == Bytecodes::_invokedynamic) {
     if (_klass->major_version() < Verifier::INVOKEDYNAMIC_MAJOR_VERSION) {
       class_format_error(
         "invokedynamic instructions not supported by this class file version (%d), class %s",
@@ -2852,36 +2919,38 @@
       return;
     }
   }
 
   if (method_name->char_at(0) == JVM_SIGNATURE_SPECIAL) {
-    // Make sure <init> can only be invoked by invokespecial
-    if (opcode != Bytecodes::_invokespecial ||
+    // Make sure <init> can only be invoked by invokespecial or invokestatic.
+    // The allowed invocation mode of <init> depends on its signature.
+    if ((opcode != Bytecodes::_invokespecial &&
+         opcode != Bytecodes::_invokestatic) ||
         method_name != vmSymbols::object_initializer_name()) {
       verify_error(ErrorContext::bad_code(bci),
           "Illegal call to internal method");
       return;
     }
   } else if (opcode == Bytecodes::_invokespecial
              && !is_same_or_direct_interface(current_class(), current_type(), ref_class_type)
              && !ref_class_type.equals(VerificationType::reference_type(
-                  current_class()->super()->name()))) {
+                  current_class()->super()->name()))) { // super() can never be a value_type.
     bool subtype = false;
     bool have_imr_indirect = cp->tag_at(index).value() == JVM_CONSTANT_InterfaceMethodref;
     if (!current_class()->is_unsafe_anonymous()) {
       subtype = ref_class_type.is_assignable_from(
                  current_type(), this, false, CHECK_VERIFY(this));
     } else {
-      VerificationType unsafe_anonymous_host_type =
-                        VerificationType::reference_type(current_class()->unsafe_anonymous_host()->name());
+      InstanceKlass* unsafe_host = current_class()->unsafe_anonymous_host();
+      VerificationType unsafe_anonymous_host_type = reference_or_valuetype(unsafe_host);
       subtype = ref_class_type.is_assignable_from(unsafe_anonymous_host_type, this, false, CHECK_VERIFY(this));
 
       // If invokespecial of IMR, need to recheck for same or
       // direct interface relative to the host class
       have_imr_indirect = (have_imr_indirect &&
                            !is_same_or_direct_interface(
-                             current_class()->unsafe_anonymous_host(),
+                             unsafe_host,
                              unsafe_anonymous_host_type, ref_class_type));
     }
     if (!subtype) {
       verify_error(ErrorContext::bad_code(bci),
           "Bad invokespecial instruction: "
@@ -2907,10 +2976,11 @@
 
   // Check objectref on operand stack
   if (opcode != Bytecodes::_invokestatic &&
       opcode != Bytecodes::_invokedynamic) {
     if (method_name == vmSymbols::object_initializer_name()) {  // <init> method
+      // (use of <init> as a static factory is handled under invokestatic)
       verify_invoke_init(bcs, index, ref_class_type, current_frame,
         code_length, in_try_block, this_uninit, cp, stackmap_table,
         CHECK_VERIFY(this));
       if (was_recursively_verified()) return;
     } else {   // other methods
@@ -2921,13 +2991,14 @@
         } else {
           // anonymous class invokespecial calls: check if the
           // objectref is a subtype of the unsafe_anonymous_host of the current class
           // to allow an anonymous class to reference methods in the unsafe_anonymous_host
           VerificationType top = current_frame->pop_stack(CHECK_VERIFY(this));
-          VerificationType hosttype =
-            VerificationType::reference_type(current_class()->unsafe_anonymous_host()->name());
-          bool subtype = hosttype.is_assignable_from(top, this, false, CHECK_VERIFY(this));
+
+          InstanceKlass* unsafe_host = current_class()->unsafe_anonymous_host();
+          VerificationType host_type = reference_or_valuetype(unsafe_host);
+          bool subtype = host_type.is_assignable_from(top, this, false, CHECK_VERIFY(this));
           if (!subtype) {
             verify_error( ErrorContext::bad_type(current_frame->offset(),
               current_frame->stack_top_ctx(),
               TypeOrigin::implicit(top)),
               "Bad type on operand stack");
@@ -2976,14 +3047,13 @@
     }
   }
   // Push the result type.
   int sig_verif_types_len = sig_verif_types->length();
   if (sig_verif_types_len > nargs) {  // There's a return type
-    if (method_name == vmSymbols::object_initializer_name()) {
-      // <init> method must have a void return type
-      /* Unreachable?  Class file parser verifies that methods with '<' have
-       * void return */
+    if (method_name == vmSymbols::object_initializer_name() &&
+        opcode != Bytecodes::_invokestatic) {
+      // an <init> method must have a void return type, unless it's a static factory
       verify_error(ErrorContext::bad_code(bci),
           "Return type must be void in <init> method");
       return;
     }
 
@@ -2992,10 +3062,18 @@
     for (int i = nargs; i < sig_verif_types_len; i++) {
       assert(i == nargs || sig_verif_types->at(i).is_long2() ||
              sig_verif_types->at(i).is_double2(), "Unexpected return verificationType");
       current_frame->push_stack(sig_verif_types->at(i), CHECK_VERIFY(this));
     }
+  } else {
+    // an <init> method may not have a void return type, if it's a static factory
+    if (method_name == vmSymbols::object_initializer_name() &&
+        opcode != Bytecodes::_invokespecial) {
+      verify_error(ErrorContext::bad_code(bci),
+          "Return type must be non-void in <init> static factory method");
+      return;
+    }
   }
 }
 
 VerificationType ClassVerifier::get_newarray_type(
     u2 index, u2 bci, TRAPS) {
@@ -3039,15 +3117,16 @@
     int n = os::snprintf(arr_sig_str, length + 1, "%c%s",
                          JVM_SIGNATURE_ARRAY, component_name);
     assert(n == length, "Unexpected number of characters in string");
   } else {         // it's an object or interface
     const char* component_name = component_type.name()->as_utf8();
-    // add one dimension to component with 'L' prepended and ';' postpended.
+    char Q_or_L = component_type.is_valuetype() ? JVM_SIGNATURE_VALUETYPE : JVM_SIGNATURE_CLASS;
+    // add one dimension to component with 'L' or 'Q' prepended and ';' appended.
     length = (int)strlen(component_name) + 3;
     arr_sig_str = NEW_RESOURCE_ARRAY_IN_THREAD(THREAD, char, length + 1);
     int n = os::snprintf(arr_sig_str, length + 1, "%c%c%s;",
-                         JVM_SIGNATURE_ARRAY, JVM_SIGNATURE_CLASS, component_name);
+                         JVM_SIGNATURE_ARRAY, Q_or_L, component_name);
     assert(n == length, "Unexpected number of characters in string");
   }
   Symbol* arr_sig = create_temporary_symbol(arr_sig_str, length);
   VerificationType new_array_type = VerificationType::reference_type(arr_sig);
   current_frame->push_stack(new_array_type, CHECK_VERIFY(this));
@@ -3085,11 +3164,11 @@
     VerificationType::double2_type(), CHECK_VERIFY(this));
 }
 
 void ClassVerifier::verify_aload(u2 index, StackMapFrame* current_frame, TRAPS) {
   VerificationType type = current_frame->get_local(
-    index, VerificationType::reference_check(), CHECK_VERIFY(this));
+    index, VerificationType::nonscalar_check(), CHECK_VERIFY(this));
   current_frame->push_stack(type, CHECK_VERIFY(this));
 }
 
 void ClassVerifier::verify_istore(u2 index, StackMapFrame* current_frame, TRAPS) {
   current_frame->pop_stack(
@@ -3122,11 +3201,11 @@
     VerificationType::double2_type(), CHECK_VERIFY(this));
 }
 
 void ClassVerifier::verify_astore(u2 index, StackMapFrame* current_frame, TRAPS) {
   VerificationType type = current_frame->pop_stack(
-    VerificationType::reference_check(), CHECK_VERIFY(this));
+    VerificationType::nonscalar_check(), CHECK_VERIFY(this));
   current_frame->set_local(index, type, CHECK_VERIFY(this));
 }
 
 void ClassVerifier::verify_iinc(u2 index, StackMapFrame* current_frame, TRAPS) {
   VerificationType type = current_frame->get_local(
diff a/src/hotspot/share/classfile/vmSymbols.cpp b/src/hotspot/share/classfile/vmSymbols.cpp
--- a/src/hotspot/share/classfile/vmSymbols.cpp
+++ b/src/hotspot/share/classfile/vmSymbols.cpp
@@ -597,28 +597,32 @@
   case vmIntrinsics::_updateCRC32:
   case vmIntrinsics::_updateBytesCRC32:
   case vmIntrinsics::_updateByteBufferCRC32:
     if (!UseCRC32Intrinsics) return true;
     break;
+  case vmIntrinsics::_makePrivateBuffer:
+  case vmIntrinsics::_finishPrivateBuffer:
   case vmIntrinsics::_getReference:
   case vmIntrinsics::_getBoolean:
   case vmIntrinsics::_getByte:
   case vmIntrinsics::_getShort:
   case vmIntrinsics::_getChar:
   case vmIntrinsics::_getInt:
   case vmIntrinsics::_getLong:
   case vmIntrinsics::_getFloat:
   case vmIntrinsics::_getDouble:
+  case vmIntrinsics::_getValue:
   case vmIntrinsics::_putReference:
   case vmIntrinsics::_putBoolean:
   case vmIntrinsics::_putByte:
   case vmIntrinsics::_putShort:
   case vmIntrinsics::_putChar:
   case vmIntrinsics::_putInt:
   case vmIntrinsics::_putLong:
   case vmIntrinsics::_putFloat:
   case vmIntrinsics::_putDouble:
+  case vmIntrinsics::_putValue:
   case vmIntrinsics::_getReferenceVolatile:
   case vmIntrinsics::_getBooleanVolatile:
   case vmIntrinsics::_getByteVolatile:
   case vmIntrinsics::_getShortVolatile:
   case vmIntrinsics::_getCharVolatile:
diff a/src/hotspot/share/classfile/vmSymbols.hpp b/src/hotspot/share/classfile/vmSymbols.hpp
--- a/src/hotspot/share/classfile/vmSymbols.hpp
+++ b/src/hotspot/share/classfile/vmSymbols.hpp
@@ -52,19 +52,21 @@
 #define VM_SYMBOLS_DO(template, do_alias)                                                         \
   /* commonly used class, package, module names */                                                \
   template(java_base,                                 "java.base")                                \
   template(java_lang_System,                          "java/lang/System")                         \
   template(java_lang_Object,                          "java/lang/Object")                         \
+  template(java_lang_IdentityObject,                  "java/lang/IdentityObject")                 \
   template(java_lang_Class,                           "java/lang/Class")                          \
   template(java_lang_Package,                         "java/lang/Package")                        \
   template(java_lang_Module,                          "java/lang/Module")                         \
   template(java_lang_String,                          "java/lang/String")                         \
   template(java_lang_StringLatin1,                    "java/lang/StringLatin1")                   \
   template(java_lang_StringUTF16,                     "java/lang/StringUTF16")                    \
   template(java_lang_Thread,                          "java/lang/Thread")                         \
   template(java_lang_ThreadGroup,                     "java/lang/ThreadGroup")                    \
   template(java_lang_Cloneable,                       "java/lang/Cloneable")                      \
+  template(java_lang_NonTearable,                     "java/lang/NonTearable")                    \
   template(java_lang_Throwable,                       "java/lang/Throwable")                      \
   template(java_lang_ClassLoader,                     "java/lang/ClassLoader")                    \
   template(java_lang_ThreadDeath,                     "java/lang/ThreadDeath")                    \
   template(java_lang_Boolean,                         "java/lang/Boolean")                        \
   template(java_lang_Character,                       "java/lang/Character")                      \
@@ -325,12 +327,13 @@
   template(setTargetVolatile_name,                    "setTargetVolatile")                        \
   template(setTarget_signature,                       "(Ljava/lang/invoke/MethodHandle;)V")       \
   template(DEFAULT_CONTEXT_name,                      "DEFAULT_CONTEXT")                          \
   NOT_LP64(  do_alias(intptr_signature,               int_signature)  )                           \
   LP64_ONLY( do_alias(intptr_signature,               long_signature) )                           \
-                                                                                                                                      \
-  /* Support for JVMCI */                                                                                                             \
+                                                                                                  \
+                                                                                                  \
+  /* Support for JVMCI */                                                                         \
   JVMCI_VM_SYMBOLS_DO(template, do_alias)                                                         \
                                                                                                   \
   template(java_lang_StackWalker,                     "java/lang/StackWalker")                    \
   template(java_lang_StackFrameInfo,                  "java/lang/StackFrameInfo")                 \
   template(java_lang_LiveStackFrameInfo,              "java/lang/LiveStackFrameInfo")             \
@@ -452,10 +455,12 @@
   template(java_lang_Boolean_signature,               "Ljava/lang/Boolean;")                      \
   template(url_code_signer_array_void_signature,      "(Ljava/net/URL;[Ljava/security/CodeSigner;)V") \
   template(module_entry_name,                         "module_entry")                             \
   template(resolved_references_name,                  "<resolved_references>")                    \
   template(init_lock_name,                            "<init_lock>")                              \
+  template(default_value_name,                        ".default")                                 \
+  template(empty_marker_name,                         ".empty")                                   \
   template(address_size_name,                         "ADDRESS_SIZE0")                            \
   template(page_size_name,                            "PAGE_SIZE")                                \
   template(big_endian_name,                           "BIG_ENDIAN")                               \
   template(use_unaligned_access_name,                 "UNALIGNED_ACCESS")                         \
   template(data_cache_line_flush_size_name,           "DATA_CACHE_LINE_FLUSH_SIZE")               \
@@ -521,10 +526,11 @@
   template(thread_void_signature,                     "(Ljava/lang/Thread;)V")                                    \
   template(threadgroup_runnable_void_signature,       "(Ljava/lang/ThreadGroup;Ljava/lang/Runnable;)V")           \
   template(threadgroup_string_void_signature,         "(Ljava/lang/ThreadGroup;Ljava/lang/String;)V")             \
   template(string_class_signature,                    "(Ljava/lang/String;)Ljava/lang/Class;")                    \
   template(object_object_object_signature,            "(Ljava/lang/Object;Ljava/lang/Object;)Ljava/lang/Object;") \
+  template(object_object_boolean_signature,           "(Ljava/lang/Object;Ljava/lang/Object;)Z") \
   template(string_string_string_signature,            "(Ljava/lang/String;Ljava/lang/String;)Ljava/lang/String;") \
   template(string_string_signature,                   "(Ljava/lang/String;)Ljava/lang/String;")                   \
   template(classloader_string_long_signature,         "(Ljava/lang/ClassLoader;Ljava/lang/String;)J")             \
   template(byte_array_void_signature,                 "([B)V")                                                    \
   template(char_array_void_signature,                 "([C)V")                                                    \
@@ -671,10 +677,14 @@
   template(jdk_internal_loader_ClassLoaders,       "jdk/internal/loader/ClassLoaders")                            \
   template(toFileURL_name,                         "toFileURL")                                                   \
   template(toFileURL_signature,                    "(Ljava/lang/String;)Ljava/net/URL;")                          \
   template(url_void_signature,                     "(Ljava/net/URL;)V")                                           \
                                                                                                                   \
+  template(java_lang_invoke_ValueBootstrapMethods, "java/lang/invoke/ValueBootstrapMethods")                      \
+  template(isSubstitutable_name,                   "isSubstitutable0")                                            \
+                                                                                                                  \
+  template(jdk_internal_vm_jni_SubElementSelector, "jdk/internal/vm/jni/SubElementSelector")                      \
   /*end*/
 
 // Here are all the intrinsics known to the runtime and the CI.
 // Each intrinsic consists of a public enum name (like _hashCode),
 // followed by a specification of its klass, name, and signature:
@@ -1162,39 +1172,49 @@
   do_signature(putLong_signature,         "(Ljava/lang/Object;JJ)V")                                                    \
   do_signature(getFloat_signature,        "(Ljava/lang/Object;J)F")                                                     \
   do_signature(putFloat_signature,        "(Ljava/lang/Object;JF)V")                                                    \
   do_signature(getDouble_signature,       "(Ljava/lang/Object;J)D")                                                     \
   do_signature(putDouble_signature,       "(Ljava/lang/Object;JD)V")                                                    \
+  do_signature(getValue_signature,        "(Ljava/lang/Object;JLjava/lang/Class;)Ljava/lang/Object;")                   \
+  do_signature(putValue_signature,        "(Ljava/lang/Object;JLjava/lang/Class;Ljava/lang/Object;)V")                  \
                                                                                                                         \
   do_name(getReference_name,"getReference")     do_name(putReference_name,"putReference")                               \
   do_name(getBoolean_name,"getBoolean")         do_name(putBoolean_name,"putBoolean")                                   \
   do_name(getByte_name,"getByte")               do_name(putByte_name,"putByte")                                         \
   do_name(getShort_name,"getShort")             do_name(putShort_name,"putShort")                                       \
   do_name(getChar_name,"getChar")               do_name(putChar_name,"putChar")                                         \
   do_name(getInt_name,"getInt")                 do_name(putInt_name,"putInt")                                           \
   do_name(getLong_name,"getLong")               do_name(putLong_name,"putLong")                                         \
   do_name(getFloat_name,"getFloat")             do_name(putFloat_name,"putFloat")                                       \
   do_name(getDouble_name,"getDouble")           do_name(putDouble_name,"putDouble")                                     \
+  do_name(getValue_name,"getValue")             do_name(putValue_name,"putValue")                                       \
+  do_name(makePrivateBuffer_name,"makePrivateBuffer")                                                                   \
+  do_name(finishPrivateBuffer_name,"finishPrivateBuffer")                                                               \
                                                                                                                         \
   do_intrinsic(_getReference,       jdk_internal_misc_Unsafe,     getReference_name, getReference_signature,     F_RN)  \
   do_intrinsic(_getBoolean,         jdk_internal_misc_Unsafe,     getBoolean_name, getBoolean_signature,         F_RN)  \
   do_intrinsic(_getByte,            jdk_internal_misc_Unsafe,     getByte_name, getByte_signature,               F_RN)  \
   do_intrinsic(_getShort,           jdk_internal_misc_Unsafe,     getShort_name, getShort_signature,             F_RN)  \
   do_intrinsic(_getChar,            jdk_internal_misc_Unsafe,     getChar_name, getChar_signature,               F_RN)  \
   do_intrinsic(_getInt,             jdk_internal_misc_Unsafe,     getInt_name, getInt_signature,                 F_RN)  \
   do_intrinsic(_getLong,            jdk_internal_misc_Unsafe,     getLong_name, getLong_signature,               F_RN)  \
   do_intrinsic(_getFloat,           jdk_internal_misc_Unsafe,     getFloat_name, getFloat_signature,             F_RN)  \
   do_intrinsic(_getDouble,          jdk_internal_misc_Unsafe,     getDouble_name, getDouble_signature,           F_RN)  \
+  do_intrinsic(_getValue,           jdk_internal_misc_Unsafe,     getValue_name, getValue_signature,             F_RN)  \
   do_intrinsic(_putReference,       jdk_internal_misc_Unsafe,     putReference_name, putReference_signature,     F_RN)  \
   do_intrinsic(_putBoolean,         jdk_internal_misc_Unsafe,     putBoolean_name, putBoolean_signature,         F_RN)  \
   do_intrinsic(_putByte,            jdk_internal_misc_Unsafe,     putByte_name, putByte_signature,               F_RN)  \
   do_intrinsic(_putShort,           jdk_internal_misc_Unsafe,     putShort_name, putShort_signature,             F_RN)  \
   do_intrinsic(_putChar,            jdk_internal_misc_Unsafe,     putChar_name, putChar_signature,               F_RN)  \
   do_intrinsic(_putInt,             jdk_internal_misc_Unsafe,     putInt_name, putInt_signature,                 F_RN)  \
   do_intrinsic(_putLong,            jdk_internal_misc_Unsafe,     putLong_name, putLong_signature,               F_RN)  \
   do_intrinsic(_putFloat,           jdk_internal_misc_Unsafe,     putFloat_name, putFloat_signature,             F_RN)  \
   do_intrinsic(_putDouble,          jdk_internal_misc_Unsafe,     putDouble_name, putDouble_signature,           F_RN)  \
+  do_intrinsic(_putValue,           jdk_internal_misc_Unsafe,     putValue_name, putValue_signature,             F_RN)  \
+                                                                                                                        \
+  do_intrinsic(_makePrivateBuffer,  jdk_internal_misc_Unsafe,     makePrivateBuffer_name, object_object_signature, F_RN)   \
+  do_intrinsic(_finishPrivateBuffer,  jdk_internal_misc_Unsafe,   finishPrivateBuffer_name, object_object_signature, F_RN) \
                                                                                                                         \
   do_name(getReferenceVolatile_name,"getReferenceVolatile")   do_name(putReferenceVolatile_name,"putReferenceVolatile") \
   do_name(getBooleanVolatile_name,"getBooleanVolatile")       do_name(putBooleanVolatile_name,"putBooleanVolatile")     \
   do_name(getByteVolatile_name,"getByteVolatile")             do_name(putByteVolatile_name,"putByteVolatile")           \
   do_name(getShortVolatile_name,"getShortVolatile")           do_name(putShortVolatile_name,"putShortVolatile")         \
@@ -1535,11 +1555,11 @@
     #undef VM_ALIAS_ENUM
 
     FIRST_SID = NO_SID + 1
   };
   enum {
-    log2_SID_LIMIT = 10         // checked by an assert at start-up
+    log2_SID_LIMIT = 11         // checked by an assert at start-up
   };
 
  private:
   // The symbol array
   static Symbol* _symbols[];
diff a/src/hotspot/share/compiler/compileBroker.cpp b/src/hotspot/share/compiler/compileBroker.cpp
--- a/src/hotspot/share/compiler/compileBroker.cpp
+++ b/src/hotspot/share/compiler/compileBroker.cpp
@@ -1177,11 +1177,11 @@
 
       if (!UseJVMCINativeLibrary) {
         // Don't allow blocking compiles if inside a class initializer or while performing class loading
         vframeStream vfst((JavaThread*) thread);
         for (; !vfst.at_end(); vfst.next()) {
-          if (vfst.method()->is_static_initializer() ||
+        if (vfst.method()->is_class_initializer() ||
               (vfst.method()->method_holder()->is_subclass_of(SystemDictionary::ClassLoader_klass()) &&
                   vfst.method()->name() == vmSymbols::loadClass_name())) {
             blocking = false;
             break;
           }
diff a/src/hotspot/share/gc/parallel/psParallelCompact.cpp b/src/hotspot/share/gc/parallel/psParallelCompact.cpp
--- a/src/hotspot/share/gc/parallel/psParallelCompact.cpp
+++ b/src/hotspot/share/gc/parallel/psParallelCompact.cpp
@@ -66,10 +66,11 @@
 #include "oops/instanceKlass.inline.hpp"
 #include "oops/instanceMirrorKlass.inline.hpp"
 #include "oops/methodData.hpp"
 #include "oops/objArrayKlass.inline.hpp"
 #include "oops/oop.inline.hpp"
+#include "oops/valueArrayKlass.inline.hpp"
 #include "runtime/atomic.hpp"
 #include "runtime/handles.inline.hpp"
 #include "runtime/safepoint.hpp"
 #include "runtime/vmThread.hpp"
 #include "services/management.hpp"
diff a/src/hotspot/share/interpreter/linkResolver.cpp b/src/hotspot/share/interpreter/linkResolver.cpp
--- a/src/hotspot/share/interpreter/linkResolver.cpp
+++ b/src/hotspot/share/interpreter/linkResolver.cpp
@@ -962,15 +962,17 @@
                                  const LinkInfo& link_info,
                                  Bytecodes::Code byte, bool initialize_class,
                                  TRAPS) {
   assert(byte == Bytecodes::_getstatic || byte == Bytecodes::_putstatic ||
          byte == Bytecodes::_getfield  || byte == Bytecodes::_putfield  ||
+         byte == Bytecodes::_withfield ||
          byte == Bytecodes::_nofast_getfield  || byte == Bytecodes::_nofast_putfield  ||
          (byte == Bytecodes::_nop && !link_info.check_access()), "bad field access bytecode");
 
   bool is_static = (byte == Bytecodes::_getstatic || byte == Bytecodes::_putstatic);
-  bool is_put    = (byte == Bytecodes::_putfield  || byte == Bytecodes::_putstatic || byte == Bytecodes::_nofast_putfield);
+  bool is_put    = (byte == Bytecodes::_putfield  || byte == Bytecodes::_putstatic ||
+                    byte == Bytecodes::_nofast_putfield || byte == Bytecodes::_withfield);
   // Check if there's a resolved klass containing the field
   Klass* resolved_klass = link_info.resolved_klass();
   Symbol* field = link_info.name();
   Symbol* sig = link_info.signature();
 
@@ -1004,30 +1006,42 @@
 
     // A final field can be modified only
     // (1) by methods declared in the class declaring the field and
     // (2) by the <clinit> method (in case of a static field)
     //     or by the <init> method (in case of an instance field).
+    // (3) by withfield when field is in a value type and the
+    //     selected class and current class are nest mates.
     if (is_put && fd.access_flags().is_final()) {
 
       if (sel_klass != current_klass) {
+      // If byte code is a withfield check if they are nestmates.
+      bool are_nestmates = false;
+      if (sel_klass->is_instance_klass() &&
+          InstanceKlass::cast(sel_klass)->is_value() &&
+          current_klass->is_instance_klass()) {
+        are_nestmates = InstanceKlass::cast(link_info.current_klass())->has_nestmate_access_to(
+                                                        InstanceKlass::cast(sel_klass), THREAD);
+      }
+      if (!are_nestmates) {
         ResourceMark rm(THREAD);
         stringStream ss;
         ss.print("Update to %s final field %s.%s attempted from a different class (%s) than the field's declaring class",
                  is_static ? "static" : "non-static", resolved_klass->external_name(), fd.name()->as_C_string(),
-                current_klass->external_name());
+                  current_klass->external_name());
         THROW_MSG(vmSymbols::java_lang_IllegalAccessError(), ss.as_string());
       }
+      }
 
       if (fd.constants()->pool_holder()->major_version() >= 53) {
         Method* m = link_info.current_method();
         assert(m != NULL, "information about the current method must be available for 'put' bytecodes");
         bool is_initialized_static_final_update = (byte == Bytecodes::_putstatic &&
                                                    fd.is_static() &&
-                                                   !m->is_static_initializer());
+                                                   !m->is_class_initializer());
         bool is_initialized_instance_final_update = ((byte == Bytecodes::_putfield || byte == Bytecodes::_nofast_putfield) &&
                                                      !fd.is_static() &&
-                                                     !m->is_object_initializer());
+                                                     !m->is_object_constructor());
 
         if (is_initialized_static_final_update || is_initialized_instance_final_update) {
           ResourceMark rm(THREAD);
           stringStream ss;
           ss.print("Update to %s final field %s.%s attempted from a different method (%s) than the initializer method %s ",
@@ -1142,10 +1156,12 @@
   } else {
     resolved_method = resolve_interface_method(link_info, Bytecodes::_invokespecial, CHECK_NULL);
   }
 
   // check if method name is <init>, that it is found in same klass as static type
+  // Since this method is never inherited from a super, any appearance here under
+  // the wrong class would be an error.
   if (resolved_method->name() == vmSymbols::object_initializer_name() &&
       resolved_method->method_holder() != resolved_klass) {
     ResourceMark rm(THREAD);
     stringStream ss;
     ss.print("%s: method '", resolved_klass->external_name());
@@ -1215,11 +1231,11 @@
   // Invokespecial for a superinterface, resolved method is selected method,
   // no checks for shadowing
   methodHandle sel_method(THREAD, resolved_method());
 
   if (link_info.check_access() &&
-      // check if the method is not <init>
+      // check if the method is not <init>, which is never inherited
       resolved_method->name() != vmSymbols::object_initializer_name()) {
 
     Klass* current_klass = link_info.current_klass();
 
     // Check if the class of the resolved_klass is a superclass
@@ -1635,24 +1651,25 @@
   }
   return;
 }
 
 void LinkResolver::resolve_invoke(CallInfo& result, Handle& recv,
-                             const methodHandle& attached_method,
-                             Bytecodes::Code byte, TRAPS) {
+                                  const methodHandle& attached_method,
+                                  Bytecodes::Code byte, bool check_null_and_abstract, TRAPS) {
   Klass* defc = attached_method->method_holder();
   Symbol* name = attached_method->name();
   Symbol* type = attached_method->signature();
   LinkInfo link_info(defc, name, type);
+  Klass* recv_klass = recv.is_null() ? defc : recv->klass();
   switch(byte) {
     case Bytecodes::_invokevirtual:
-      resolve_virtual_call(result, recv, recv->klass(), link_info,
-                           /*check_null_and_abstract=*/true, CHECK);
+      resolve_virtual_call(result, recv, recv_klass, link_info,
+                           check_null_and_abstract, CHECK);
       break;
     case Bytecodes::_invokeinterface:
-      resolve_interface_call(result, recv, recv->klass(), link_info,
-                             /*check_null_and_abstract=*/true, CHECK);
+      resolve_interface_call(result, recv, recv_klass, link_info,
+                             check_null_and_abstract, CHECK);
       break;
     case Bytecodes::_invokestatic:
       resolve_static_call(result, link_info, /*initialize_class=*/false, CHECK);
       break;
     case Bytecodes::_invokespecial:
diff a/src/hotspot/share/jvmci/jvmciCodeInstaller.cpp b/src/hotspot/share/jvmci/jvmciCodeInstaller.cpp
--- a/src/hotspot/share/jvmci/jvmciCodeInstaller.cpp
+++ b/src/hotspot/share/jvmci/jvmciCodeInstaller.cpp
@@ -1178,11 +1178,11 @@
     monitors_token = _debug_recorder->create_monitor_values(monitors);
 
     throw_exception = jvmci_env()->get_BytecodeFrame_rethrowException(frame) == JNI_TRUE;
   }
 
-  _debug_recorder->describe_scope(pc_offset, method, NULL, bci, reexecute, throw_exception, false, return_oop,
+  _debug_recorder->describe_scope(pc_offset, method, NULL, bci, reexecute, throw_exception, false, return_oop, false,
                                   locals_token, expressions_token, monitors_token);
 }
 
 void CodeInstaller::site_Safepoint(CodeBuffer& buffer, jint pc_offset, JVMCIObject site, JVMCI_TRAPS) {
   JVMCIObject debug_info = jvmci_env()->get_site_Infopoint_debugInfo(site);
@@ -1334,10 +1334,12 @@
       case UNVERIFIED_ENTRY:
         _offsets.set_value(CodeOffsets::Entry, pc_offset);
         break;
       case VERIFIED_ENTRY:
         _offsets.set_value(CodeOffsets::Verified_Entry, pc_offset);
+        _offsets.set_value(CodeOffsets::Verified_Value_Entry, pc_offset);
+        _offsets.set_value(CodeOffsets::Verified_Value_Entry_RO, pc_offset);
         break;
       case OSR_ENTRY:
         _offsets.set_value(CodeOffsets::OSR_Entry, pc_offset);
         break;
       case EXCEPTION_HANDLER_ENTRY:
diff a/src/hotspot/share/jvmci/vmStructs_jvmci.cpp b/src/hotspot/share/jvmci/vmStructs_jvmci.cpp
--- a/src/hotspot/share/jvmci/vmStructs_jvmci.cpp
+++ b/src/hotspot/share/jvmci/vmStructs_jvmci.cpp
@@ -503,10 +503,11 @@
   declare_constant(DataLayout::arg_info_data_tag)                         \
   declare_constant(DataLayout::call_type_data_tag)                        \
   declare_constant(DataLayout::virtual_call_type_data_tag)                \
   declare_constant(DataLayout::parameters_type_data_tag)                  \
   declare_constant(DataLayout::speculative_trap_data_tag)                 \
+  declare_constant(DataLayout::array_load_store_data_tag)                 \
                                                                           \
   declare_constant(Deoptimization::Unpack_deopt)                          \
   declare_constant(Deoptimization::Unpack_exception)                      \
   declare_constant(Deoptimization::Unpack_uncommon_trap)                  \
   declare_constant(Deoptimization::Unpack_reexecute)                      \
diff a/src/hotspot/share/logging/logTag.hpp b/src/hotspot/share/logging/logTag.hpp
--- a/src/hotspot/share/logging/logTag.hpp
+++ b/src/hotspot/share/logging/logTag.hpp
@@ -174,10 +174,11 @@
   LOG_TAG(tracking) \
   LOG_TAG(update) \
   LOG_TAG(unload) /* Trace unloading of classes */ \
   LOG_TAG(unshareable) \
   LOG_TAG(mirror) \
+  LOG_TAG(valuetypes) \
   LOG_TAG(verification) \
   LOG_TAG(verify) \
   LOG_TAG(vmmutex) \
   LOG_TAG(vmoperation) \
   LOG_TAG(vmthread) \
diff a/src/hotspot/share/memory/metaspace.cpp b/src/hotspot/share/memory/metaspace.cpp
--- a/src/hotspot/share/memory/metaspace.cpp
+++ b/src/hotspot/share/memory/metaspace.cpp
@@ -986,10 +986,11 @@
   if (cds_base != NULL) {
     assert(UseSharedSpaces, "must be");
     lower_base = MIN2(lower_base, cds_base);
   } else {
     uint64_t klass_encoding_max = UnscaledClassSpaceMax << LogKlassAlignmentInBytes;
+    // Using oopDesc::_metadata high bits so LogKlassAlignmentInBytes shift is no longer possible
     // If compressed class space fits in lower 32G, we don't need a base.
     if (higher_address <= (address)klass_encoding_max) {
       lower_base = 0; // Effectively lower base is zero.
     }
   }
@@ -1226,11 +1227,10 @@
   set_compressed_class_space_size(CompressedClassSpaceSize);
 }
 
 void Metaspace::global_initialize() {
   MetaspaceGC::initialize();
-
   bool class_space_inited = false;
 #if INCLUDE_CDS
   if (DumpSharedSpaces) {
     MetaspaceShared::initialize_dumptime_shared_and_meta_spaces();
     class_space_inited = true;
diff a/src/hotspot/share/oops/arrayKlass.cpp b/src/hotspot/share/oops/arrayKlass.cpp
--- a/src/hotspot/share/oops/arrayKlass.cpp
+++ b/src/hotspot/share/oops/arrayKlass.cpp
@@ -23,18 +23,20 @@
  */
 
 #include "precompiled.hpp"
 #include "classfile/javaClasses.hpp"
 #include "classfile/moduleEntry.hpp"
+#include "classfile/symbolTable.hpp"
 #include "classfile/systemDictionary.hpp"
 #include "classfile/vmSymbols.hpp"
 #include "gc/shared/collectedHeap.inline.hpp"
 #include "jvmtifiles/jvmti.h"
 #include "memory/metaspaceClosure.hpp"
 #include "memory/resourceArea.hpp"
 #include "memory/universe.hpp"
 #include "oops/arrayKlass.hpp"
+#include "oops/objArrayKlass.hpp"
 #include "oops/arrayOop.hpp"
 #include "oops/instanceKlass.hpp"
 #include "oops/objArrayOop.hpp"
 #include "oops/oop.inline.hpp"
 #include "runtime/handles.inline.hpp"
@@ -95,10 +97,44 @@
     set_layout_helper(Klass::_lh_neutral_value);
     set_is_cloneable(); // All arrays are considered to be cloneable (See JLS 20.1.5)
     JFR_ONLY(INIT_ID(this);)
 }
 
+Symbol* ArrayKlass::create_element_klass_array_name(Klass* element_klass, TRAPS) {
+  Symbol* name = NULL;
+  bool is_qtype = element_klass->is_value();
+  if (!element_klass->is_instance_klass() || is_qtype ||
+      (name = InstanceKlass::cast(element_klass)->array_name()) == NULL) {
+
+    ResourceMark rm(THREAD);
+    char *name_str = element_klass->name()->as_C_string();
+    int len = element_klass->name()->utf8_length();
+    char *new_str = NEW_RESOURCE_ARRAY(char, len + 4);
+    int idx = 0;
+    new_str[idx++] = JVM_SIGNATURE_ARRAY;
+    if (element_klass->is_instance_klass()) { // it could be an array or simple type
+      if (is_qtype) {
+        new_str[idx++] = JVM_SIGNATURE_VALUETYPE;
+      } else {
+        new_str[idx++] = JVM_SIGNATURE_CLASS;
+      }
+    }
+    memcpy(&new_str[idx], name_str, len * sizeof(char));
+    idx += len;
+    if (element_klass->is_instance_klass()) {
+      new_str[idx++] = JVM_SIGNATURE_ENDCLASS;
+    }
+    new_str[idx++] = '\0';
+    name = SymbolTable::new_symbol(new_str);
+    if (element_klass->is_instance_klass()) {
+      InstanceKlass* ik = InstanceKlass::cast(element_klass);
+      ik->set_array_name(name);
+    }
+  }
+
+  return name;
+}
 
 // Initialization of vtables and mirror object is done separatly from base_create_array_klass,
 // since a GC can happen. At this point all instance variables of the ArrayKlass must be setup.
 void ArrayKlass::complete_create_array_klass(ArrayKlass* k, Klass* super_klass, ModuleEntry* module_entry, TRAPS) {
   k->initialize_supers(super_klass, NULL, CHECK);
@@ -132,28 +168,23 @@
                                                                 /* do_zero */ true, CHECK_NULL);
   // initialization to NULL not necessary, area already cleared
   return o;
 }
 
-void ArrayKlass::array_klasses_do(void f(Klass* k, TRAPS), TRAPS) {
-  Klass* k = this;
-  // Iterate over this array klass and all higher dimensions
-  while (k != NULL) {
-    f(k, CHECK);
-    k = ArrayKlass::cast(k)->higher_dimension();
-  }
-}
-
 void ArrayKlass::array_klasses_do(void f(Klass* k)) {
   Klass* k = this;
   // Iterate over this array klass and all higher dimensions
   while (k != NULL) {
     f(k);
     k = ArrayKlass::cast(k)->higher_dimension();
   }
 }
 
+oop ArrayKlass::component_mirror() const {
+  return java_lang_Class::component_mirror(java_mirror());
+}
+
 // JVM support
 
 jint ArrayKlass::compute_modifier_flags(TRAPS) const {
   return JVM_ACC_ABSTRACT | JVM_ACC_FINAL | JVM_ACC_PUBLIC;
 }
diff a/src/hotspot/share/oops/constantPool.cpp b/src/hotspot/share/oops/constantPool.cpp
--- a/src/hotspot/share/oops/constantPool.cpp
+++ b/src/hotspot/share/oops/constantPool.cpp
@@ -48,10 +48,11 @@
 #include "oops/instanceKlass.hpp"
 #include "oops/objArrayKlass.hpp"
 #include "oops/objArrayOop.inline.hpp"
 #include "oops/oop.inline.hpp"
 #include "oops/typeArrayOop.inline.hpp"
+#include "oops/valueArrayKlass.hpp"
 #include "runtime/atomic.hpp"
 #include "runtime/handles.inline.hpp"
 #include "runtime/init.hpp"
 #include "runtime/javaCalls.hpp"
 #include "runtime/signature.hpp"
@@ -211,11 +212,11 @@
       break;
 #ifndef PRODUCT
     case JVM_CONSTANT_Class:
     case JVM_CONSTANT_UnresolvedClass:
     case JVM_CONSTANT_UnresolvedClassInError:
-      // All of these should have been reverted back to ClassIndex before calling
+      // All of these should have been reverted back to Unresolved before calling
       // this function.
       ShouldNotReachHere();
 #endif
     }
   }
@@ -235,14 +236,15 @@
   Klass** adr = resolved_klasses()->adr_at(resolved_klass_index);
   Atomic::release_store(adr, k);
 
   // The interpreter assumes when the tag is stored, the klass is resolved
   // and the Klass* non-NULL, so we need hardware store ordering here.
+  jbyte qdesc_bit = (name->is_Q_signature()) ? (jbyte) JVM_CONSTANT_QDescBit : 0;
   if (k != NULL) {
-    release_tag_at_put(class_index, JVM_CONSTANT_Class);
+    release_tag_at_put(class_index, JVM_CONSTANT_Class | qdesc_bit);
   } else {
-    release_tag_at_put(class_index, JVM_CONSTANT_UnresolvedClass);
+    release_tag_at_put(class_index, JVM_CONSTANT_UnresolvedClass | qdesc_bit);
   }
 }
 
 // Unsafe anonymous class support:
 void ConstantPool::klass_at_put(int class_index, Klass* k) {
@@ -252,10 +254,11 @@
   Klass** adr = resolved_klasses()->adr_at(resolved_klass_index);
   Atomic::release_store(adr, k);
 
   // The interpreter assumes when the tag is stored, the klass is resolved
   // and the Klass* non-NULL, so we need hardware store ordering here.
+  assert(!k->name()->is_Q_signature(), "Q-type without JVM_CONSTANT_QDescBit");
   release_tag_at_put(class_index, JVM_CONSTANT_Class);
 }
 
 #if INCLUDE_CDS_JAVA_HEAP
 // Archive the resolved references
@@ -452,10 +455,16 @@
                  k->external_name());
     }
   }
 }
 
+void check_is_inline_type(Klass* k, TRAPS) {
+  if (!k->is_value()) {
+    THROW(vmSymbols::java_lang_IncompatibleClassChangeError());
+  }
+}
+
 Klass* ConstantPool::klass_at_impl(const constantPoolHandle& this_cp, int which,
                                    bool save_resolution_error, TRAPS) {
   assert(THREAD->is_Java_thread(), "must be a Java thread");
   JavaThread* javaThread = (JavaThread*)THREAD;
 
@@ -486,27 +495,51 @@
     ShouldNotReachHere();
   }
 
   Handle mirror_handle;
   Symbol* name = this_cp->symbol_at(name_index);
+  bool inline_type_signature = false;
+  if (name->is_Q_signature()) {
+    name = name->fundamental_name(THREAD);
+    inline_type_signature = true;
+  }
   Handle loader (THREAD, this_cp->pool_holder()->class_loader());
   Handle protection_domain (THREAD, this_cp->pool_holder()->protection_domain());
 
   Klass* k;
   {
     // Turn off the single stepping while doing class resolution
     JvmtiHideSingleStepping jhss(javaThread);
     k = SystemDictionary::resolve_or_fail(name, loader, protection_domain, true, THREAD);
   } //  JvmtiHideSingleStepping jhss(javaThread);
+  if (inline_type_signature) {
+    name->decrement_refcount();
+  }
 
   if (!HAS_PENDING_EXCEPTION) {
     // preserve the resolved klass from unloading
     mirror_handle = Handle(THREAD, k->java_mirror());
     // Do access check for klasses
     verify_constant_pool_resolve(this_cp, k, THREAD);
   }
 
+  if (!HAS_PENDING_EXCEPTION && inline_type_signature) {
+    check_is_inline_type(k, THREAD);
+  }
+
+  if (!HAS_PENDING_EXCEPTION) {
+    Klass* bottom_klass = NULL;
+    if (k->is_objArray_klass()) {
+      bottom_klass = ObjArrayKlass::cast(k)->bottom_klass();
+      assert(bottom_klass != NULL, "Should be set");
+      assert(bottom_klass->is_instance_klass() || bottom_klass->is_typeArray_klass(), "Sanity check");
+    } else if (k->is_valueArray_klass()) {
+      bottom_klass = ValueArrayKlass::cast(k)->element_klass();
+      assert(bottom_klass != NULL, "Should be set");
+    }
+  }
+
   // Failed to resolve class. We must record the errors so that subsequent attempts
   // to resolve this constant pool entry fail with the same error (JVMS 5.4.3).
   if (HAS_PENDING_EXCEPTION) {
     if (save_resolution_error) {
       save_and_throw_exception(this_cp, which, constantTag(JVM_CONSTANT_UnresolvedClass), CHECK_NULL);
@@ -528,11 +561,15 @@
   Klass** adr = this_cp->resolved_klasses()->adr_at(resolved_klass_index);
   Atomic::release_store(adr, k);
   // The interpreter assumes when the tag is stored, the klass is resolved
   // and the Klass* stored in _resolved_klasses is non-NULL, so we need
   // hardware store ordering here.
-  this_cp->release_tag_at_put(which, JVM_CONSTANT_Class);
+  jbyte tag = JVM_CONSTANT_Class;
+  if (this_cp->tag_at(which).is_Qdescriptor_klass()) {
+    tag |= JVM_CONSTANT_QDescBit;
+  }
+  this_cp->release_tag_at_put(which, tag);
   return k;
 }
 
 
 // Does not update ConstantPool* - to avoid any exception throwing. Used
@@ -1852,10 +1889,16 @@
         idx1 = Bytes::get_Java_u2(bytes);
         printf("class        #%03d", idx1);
         ent_size = 2;
         break;
       }
+      case (JVM_CONSTANT_Class | JVM_CONSTANT_QDescBit): {
+        idx1 = Bytes::get_Java_u2(bytes);
+        printf("qclass        #%03d", idx1);
+        ent_size = 2;
+        break;
+      }
       case JVM_CONSTANT_String: {
         idx1 = Bytes::get_Java_u2(bytes);
         printf("String       #%03d", idx1);
         ent_size = 2;
         break;
@@ -1894,10 +1937,14 @@
       }
       case JVM_CONSTANT_UnresolvedClass: {
         printf("UnresolvedClass: %s", WARN_MSG);
         break;
       }
+      case (JVM_CONSTANT_UnresolvedClass | JVM_CONSTANT_QDescBit): {
+        printf("UnresolvedQClass: %s", WARN_MSG);
+        break;
+      }
       case JVM_CONSTANT_UnresolvedClassInError: {
         printf("UnresolvedClassInErr: %s", WARN_MSG);
         break;
       }
       case JVM_CONSTANT_StringIndex: {
@@ -2065,10 +2112,11 @@
         break;
       }
       case JVM_CONSTANT_Class:
       case JVM_CONSTANT_UnresolvedClass:
       case JVM_CONSTANT_UnresolvedClassInError: {
+        assert(!tag_at(idx).is_Qdescriptor_klass(), "Failed to encode QDesc");
         *bytes = JVM_CONSTANT_Class;
         Symbol* sym = klass_name_at(idx);
         idx1 = tbl->symbol_to_value(sym);
         assert(idx1 != 0, "Have not found a hashtable entry");
         Bytes::put_Java_u2((address) (bytes+1), idx1);
diff a/src/hotspot/share/oops/cpCache.cpp b/src/hotspot/share/oops/cpCache.cpp
--- a/src/hotspot/share/oops/cpCache.cpp
+++ b/src/hotspot/share/oops/cpCache.cpp
@@ -131,18 +131,23 @@
                                        int field_index,
                                        int field_offset,
                                        TosState field_type,
                                        bool is_final,
                                        bool is_volatile,
+                                       bool is_flattened,
+                                       bool is_flattenable,
                                        Klass* root_klass) {
   set_f1(field_holder);
   set_f2(field_offset);
   assert((field_index & field_index_mask) == field_index,
          "field index does not fit in low flag bits");
+  assert(!is_flattened || is_flattenable, "Sanity check");
   set_field_flags(field_type,
                   ((is_volatile ? 1 : 0) << is_volatile_shift) |
-                  ((is_final    ? 1 : 0) << is_final_shift),
+                  ((is_final    ? 1 : 0) << is_final_shift) |
+                  ((is_flattened  ? 1 : 0) << is_flattened_field_shift) |
+                  ((is_flattenable ? 1 : 0) << is_flattenable_field_shift),
                   field_index);
   set_bytecode_1(get_code);
   set_bytecode_2(put_code);
   NOT_PRODUCT(verify(tty));
 }
@@ -296,10 +301,11 @@
       // case, the method gets reresolved with caller for each interface call
       // because the actual selected method may not be public.
       //
       // We set bytecode_2() to _invokevirtual.
       // See also interpreterRuntime.cpp. (8/25/2000)
+      invoke_code = Bytecodes::_invokevirtual;
     } else {
       assert(invoke_code == Bytecodes::_invokevirtual ||
              (invoke_code == Bytecodes::_invokeinterface &&
               ((method->is_private() ||
                 (method->is_final() && method->method_holder() == SystemDictionary::Object_klass())))),
@@ -311,11 +317,11 @@
         // We set bytecode_2() to _invokevirtual.
         set_bytecode_1(invoke_code);
       }
     }
     // set up for invokevirtual, even if linking for invokeinterface also:
-    set_bytecode_2(Bytecodes::_invokevirtual);
+    set_bytecode_2(invoke_code);
   } else {
     ShouldNotReachHere();
   }
   NOT_PRODUCT(verify(tty));
 }
diff a/src/hotspot/share/oops/instanceKlass.cpp b/src/hotspot/share/oops/instanceKlass.cpp
--- a/src/hotspot/share/oops/instanceKlass.cpp
+++ b/src/hotspot/share/oops/instanceKlass.cpp
@@ -63,10 +63,11 @@
 #include "oops/klass.inline.hpp"
 #include "oops/method.hpp"
 #include "oops/oop.inline.hpp"
 #include "oops/recordComponent.hpp"
 #include "oops/symbol.hpp"
+#include "oops/valueKlass.hpp"
 #include "prims/jvmtiExport.hpp"
 #include "prims/jvmtiRedefineClasses.hpp"
 #include "prims/jvmtiThreadState.hpp"
 #include "prims/methodComparator.hpp"
 #include "runtime/atomic.hpp"
@@ -420,11 +421,13 @@
   const int size = InstanceKlass::size(parser.vtable_size(),
                                        parser.itable_size(),
                                        nonstatic_oop_map_size(parser.total_oop_map_count()),
                                        parser.is_interface(),
                                        parser.is_unsafe_anonymous(),
-                                       should_store_fingerprint(is_hidden_or_anonymous));
+                                       should_store_fingerprint(is_hidden_or_anonymous),
+                                       parser.has_flattenable_fields() ? parser.java_fields_count() : 0,
+                                       parser.is_inline_type());
 
   const Symbol* const class_name = parser.class_name();
   assert(class_name != NULL, "invariant");
   ClassLoaderData* loader_data = parser.loader_data();
   assert(loader_data != NULL, "invariant");
@@ -434,14 +437,16 @@
   // Allocation
   if (REF_NONE == parser.reference_type()) {
     if (class_name == vmSymbols::java_lang_Class()) {
       // mirror
       ik = new (loader_data, size, THREAD) InstanceMirrorKlass(parser);
-    }
-    else if (is_class_loader(class_name, parser)) {
+    } else if (is_class_loader(class_name, parser)) {
       // class loader
       ik = new (loader_data, size, THREAD) InstanceClassLoaderKlass(parser);
+    } else if (parser.is_inline_type()) {
+      // inline type
+      ik = new (loader_data, size, THREAD) ValueKlass(parser);
     } else {
       // normal
       ik = new (loader_data, size, THREAD) InstanceKlass(parser, InstanceKlass::_misc_kind_other);
     }
   } else {
@@ -453,13 +458,43 @@
   // class count.  Can get OOM here.
   if (HAS_PENDING_EXCEPTION) {
     return NULL;
   }
 
+#ifdef ASSERT
+  assert(ik->size() == size, "");
+  ik->bounds_check((address) ik->start_of_vtable(), false, size);
+  ik->bounds_check((address) ik->start_of_itable(), false, size);
+  ik->bounds_check((address) ik->end_of_itable(), true, size);
+  ik->bounds_check((address) ik->end_of_nonstatic_oop_maps(), true, size);
+#endif //ASSERT
   return ik;
 }
 
+#ifndef PRODUCT
+bool InstanceKlass::bounds_check(address addr, bool edge_ok, intptr_t size_in_bytes) const {
+  const char* bad = NULL;
+  address end = NULL;
+  if (addr < (address)this) {
+    bad = "before";
+  } else if (addr == (address)this) {
+    if (edge_ok)  return true;
+    bad = "just before";
+  } else if (addr == (end = (address)this + sizeof(intptr_t) * (size_in_bytes < 0 ? size() : size_in_bytes))) {
+    if (edge_ok)  return true;
+    bad = "just after";
+  } else if (addr > end) {
+    bad = "after";
+  } else {
+    return true;
+  }
+  tty->print_cr("%s object bounds: " INTPTR_FORMAT " [" INTPTR_FORMAT ".." INTPTR_FORMAT "]",
+      bad, (intptr_t)addr, (intptr_t)this, (intptr_t)end);
+  Verbose = WizardMode = true; this->print(); //@@
+  return false;
+}
+#endif //PRODUCT
 
 // copy method ordering from resource area to Metaspace
 void InstanceKlass::copy_method_ordering(const intArray* m, TRAPS) {
   if (m != NULL) {
     // allocate a new array and copy contents (memcpy?)
@@ -489,33 +524,42 @@
   _static_field_size(parser.static_field_size()),
   _nonstatic_oop_map_size(nonstatic_oop_map_size(parser.total_oop_map_count())),
   _itable_len(parser.itable_size()),
   _init_thread(NULL),
   _init_state(allocated),
-  _reference_type(parser.reference_type())
+  _reference_type(parser.reference_type()),
+  _value_field_klasses(NULL),
+  _adr_valueklass_fixed_block(NULL)
 {
   set_vtable_length(parser.vtable_size());
   set_kind(kind);
   set_access_flags(parser.access_flags());
   if (parser.is_hidden()) set_is_hidden();
   set_is_unsafe_anonymous(parser.is_unsafe_anonymous());
   set_layout_helper(Klass::instance_layout_helper(parser.layout_size(),
                                                     false));
+    if (parser.has_flattenable_fields()) {
+      set_has_inline_fields();
+    }
+    _java_fields_count = parser.java_fields_count();
 
-  assert(NULL == _methods, "underlying memory not zeroed?");
-  assert(is_instance_klass(), "is layout incorrect?");
-  assert(size_helper() == parser.layout_size(), "incorrect size_helper?");
+    assert(NULL == _methods, "underlying memory not zeroed?");
+    assert(is_instance_klass(), "is layout incorrect?");
+    assert(size_helper() == parser.layout_size(), "incorrect size_helper?");
 
   if (Arguments::is_dumping_archive()) {
-    SystemDictionaryShared::init_dumptime_info(this);
-  }
+      SystemDictionaryShared::init_dumptime_info(this);
+    }
 
   // Set biased locking bit for all instances of this class; it will be
   // cleared if revocation occurs too often for this type
   if (UseBiasedLocking && BiasedLocking::enabled()) {
     set_prototype_header(markWord::biased_locking_prototype());
   }
+  if (has_inline_fields()) {
+    _value_field_klasses = (const Klass**) adr_value_fields_klasses();
+  }
 }
 
 void InstanceKlass::deallocate_methods(ClassLoaderData* loader_data,
                                        Array<Method*>* methods) {
   if (methods != NULL && methods != Universe::the_empty_method_array() &&
@@ -541,18 +585,20 @@
   Array<InstanceKlass*>* ti = transitive_interfaces;
   if (ti != Universe::the_empty_instance_klass_array() && ti != local_interfaces) {
     // check that the interfaces don't come from super class
     Array<InstanceKlass*>* sti = (super_klass == NULL) ? NULL :
                     InstanceKlass::cast(super_klass)->transitive_interfaces();
-    if (ti != sti && ti != NULL && !ti->is_shared()) {
+    if (ti != sti && ti != NULL && !ti->is_shared() &&
+        ti != Universe::the_single_IdentityObject_klass_array()) {
       MetadataFactory::free_array<InstanceKlass*>(loader_data, ti);
     }
   }
 
   // local interfaces can be empty
   if (local_interfaces != Universe::the_empty_instance_klass_array() &&
-      local_interfaces != NULL && !local_interfaces->is_shared()) {
+      local_interfaces != NULL && !local_interfaces->is_shared() &&
+      local_interfaces != Universe::the_single_IdentityObject_klass_array()) {
     MetadataFactory::free_array<InstanceKlass*>(loader_data, local_interfaces);
   }
 }
 
 void InstanceKlass::deallocate_record_components(ClassLoaderData* loader_data,
@@ -863,10 +909,66 @@
   for (int index = 0; index < num_interfaces; index++) {
     InstanceKlass* interk = interfaces->at(index);
     interk->link_class_impl(CHECK_false);
   }
 
+
+  // If a class declares a method that uses an inline class as an argument
+  // type or return inline type, this inline class must be loaded during the
+  // linking of this class because size and properties of the inline class
+  // must be known in order to be able to perform inline type optimizations.
+  // The implementation below is an approximation of this rule, the code
+  // iterates over all methods of the current class (including overridden
+  // methods), not only the methods declared by this class. This
+  // approximation makes the code simpler, and doesn't change the semantic
+  // because classes declaring methods overridden by the current class are
+  // linked (and have performed their own pre-loading) before the linking
+  // of the current class.
+
+
+  // Note:
+  // Inline class types used for flattenable fields are loaded during
+  // the loading phase (see ClassFileParser::post_process_parsed_stream()).
+  // Inline class types used as element types for array creation
+  // are not pre-loaded. Their loading is triggered by either anewarray
+  // or multianewarray bytecodes.
+
+  // Could it be possible to do the following processing only if the
+  // class uses inline types?
+  {
+    ResourceMark rm(THREAD);
+    for (int i = 0; i < methods()->length(); i++) {
+      Method* m = methods()->at(i);
+      for (SignatureStream ss(m->signature()); !ss.is_done(); ss.next()) {
+        if (ss.is_reference()) {
+          if (ss.is_array()) {
+            ss.skip_array_prefix();
+          }
+          if (ss.type() == T_VALUETYPE) {
+            Symbol* symb = ss.as_symbol();
+
+            oop loader = class_loader();
+            oop protection_domain = this->protection_domain();
+            Klass* klass = SystemDictionary::resolve_or_fail(symb,
+                                                             Handle(THREAD, loader), Handle(THREAD, protection_domain), true,
+                                                             CHECK_false);
+            if (klass == NULL) {
+              THROW_(vmSymbols::java_lang_LinkageError(), false);
+            }
+            if (!klass->is_value()) {
+              Exceptions::fthrow(
+                THREAD_AND_LOCATION,
+                vmSymbols::java_lang_IncompatibleClassChangeError(),
+                "class %s is not an inline type",
+                klass->external_name());
+            }
+          }
+        }
+      }
+    }
+  }
+
   // in case the class is linked in the process of linking its superclasses
   if (is_linked()) {
     return true;
   }
 
@@ -932,10 +1034,11 @@
         vtable().verify(tty, true);
         // In case itable verification is ever added.
         // itable().verify(tty, true);
       }
 #endif
+
       set_init_state(linked);
       if (JvmtiExport::should_post_class_prepare()) {
         Thread *thread = THREAD;
         assert(thread->is_Java_thread(), "thread->is_Java_thread()");
         JvmtiExport::post_class_prepare((JavaThread *) thread, this);
@@ -1085,15 +1188,46 @@
       DTRACE_CLASSINIT_PROBE_WAIT(super__failed, -1, wait);
       THROW_OOP(e());
     }
   }
 
+  // Step 8
+  // Initialize classes of flattenable fields
+  {
+    for (AllFieldStream fs(this); !fs.done(); fs.next()) {
+      if (fs.is_flattenable()) {
+        Klass* klass = this->get_value_field_klass_or_null(fs.index());
+        if (klass == NULL) {
+          assert(fs.access_flags().is_static() && fs.access_flags().is_flattenable(),
+              "Otherwise should have been pre-loaded");
+          klass = SystemDictionary::resolve_or_fail(field_signature(fs.index())->fundamental_name(THREAD),
+              Handle(THREAD, class_loader()),
+              Handle(THREAD, protection_domain()),
+              true, CHECK);
+          if (klass == NULL) {
+            THROW(vmSymbols::java_lang_NoClassDefFoundError());
+          }
+          if (!klass->is_value()) {
+            THROW(vmSymbols::java_lang_IncompatibleClassChangeError());
+          }
+          this->set_value_field_klass(fs.index(), klass);
+        }
+        InstanceKlass::cast(klass)->initialize(CHECK);
+        if (fs.access_flags().is_static()) {
+          if (java_mirror()->obj_field(fs.offset()) == NULL) {
+            java_mirror()->obj_field_put(fs.offset(), ValueKlass::cast(klass)->default_value());
+          }
+        }
+      }
+    }
+  }
+
 
   // Look for aot compiled methods for this klass, including class initializer.
   AOTLoader::load_for_klass(this, THREAD);
 
-  // Step 8
+  // Step 9
   {
     DTRACE_CLASSINIT_PROBE_WAIT(clinit, -1, wait);
     // Timer includes any side effects of class initialization (resolution,
     // etc), but not recursive entry into call_class_initializer().
     PerfClassTraceTime timer(ClassLoader::perf_class_init_time(),
@@ -1103,19 +1237,19 @@
                              jt->get_thread_stat()->perf_timers_addr(),
                              PerfClassTraceTime::CLASS_CLINIT);
     call_class_initializer(THREAD);
   }
 
-  // Step 9
+  // Step 10
   if (!HAS_PENDING_EXCEPTION) {
     set_initialization_state_and_notify(fully_initialized, CHECK);
     {
       debug_only(vtable().verify(tty, true);)
     }
   }
   else {
-    // Step 10 and 11
+    // Step 11 and 12
     Handle e(THREAD, PENDING_EXCEPTION);
     CLEAR_PENDING_EXCEPTION;
     // JVMTI has already reported the pending exception
     // JVMTI internal flag reset is needed in order to report ExceptionInInitializerError
     JvmtiExport::clear_detected_exception(jt);
@@ -1376,11 +1510,11 @@
       // Atomic creation of array_klasses
       MutexLocker ma(THREAD, MultiArray_lock);
 
       // Check if update has already taken place
       if (array_klasses() == NULL) {
-        Klass*    k = ObjArrayKlass::allocate_objArray_klass(class_loader_data(), 1, this, CHECK_NULL);
+        Klass*    k = ObjArrayKlass::allocate_objArray_klass(1, this, CHECK_NULL);
         // use 'release' to pair with lock-free load
         release_set_array_klasses(k);
       }
     }
   }
@@ -1399,11 +1533,11 @@
 static int call_class_initializer_counter = 0;   // for debugging
 
 Method* InstanceKlass::class_initializer() const {
   Method* clinit = find_method(
       vmSymbols::class_initializer_name(), vmSymbols::void_method_signature());
-  if (clinit != NULL && clinit->has_valid_initializer_flags()) {
+  if (clinit != NULL && clinit->is_class_initializer()) {
     return clinit;
   }
   return NULL;
 }
 
@@ -1437,11 +1571,11 @@
   InterpreterOopMap* entry_for) {
   // Lazily create the _oop_map_cache at first request
   // Lock-free access requires load_acquire.
   OopMapCache* oop_map_cache = Atomic::load_acquire(&_oop_map_cache);
   if (oop_map_cache == NULL) {
-    MutexLocker x(OopMapCacheAlloc_lock);
+    MutexLocker x(OopMapCacheAlloc_lock,  Mutex::_no_safepoint_check_flag);
     // Check if _oop_map_cache was allocated while we were waiting for this lock
     if ((oop_map_cache = _oop_map_cache) == NULL) {
       oop_map_cache = new OopMapCache();
       // Ensure _oop_map_cache is stable, since it is examined without a lock
       Atomic::release_store(&_oop_map_cache, oop_map_cache);
@@ -1449,15 +1583,10 @@
   }
   // _oop_map_cache is constant after init; lookup below does its own locking.
   oop_map_cache->lookup(method, bci, entry_for);
 }
 
-bool InstanceKlass::contains_field_offset(int offset) {
-  fieldDescriptor fd;
-  return find_field_from_offset(offset, false, &fd);
-}
-
 bool InstanceKlass::find_local_field(Symbol* name, Symbol* sig, fieldDescriptor* fd) const {
   for (JavaFieldStream fs(this); !fs.done(); fs.next()) {
     Symbol* f_name = fs.name();
     Symbol* f_sig  = fs.signature();
     if (f_name == name && f_sig == sig) {
@@ -1524,10 +1653,19 @@
   }
   // 4) otherwise field lookup fails
   return NULL;
 }
 
+bool InstanceKlass::contains_field_offset(int offset) {
+  if (this->is_value()) {
+    ValueKlass* vk = ValueKlass::cast(this);
+    return offset >= vk->first_field_offset() && offset < (vk->first_field_offset() + vk->get_exact_size_in_bytes());
+  } else {
+    fieldDescriptor fd;
+    return find_field_from_offset(offset, false, &fd);
+  }
+}
 
 bool InstanceKlass::find_local_field_from_offset(int offset, bool is_static, fieldDescriptor* fd) const {
   for (JavaFieldStream fs(this); !fs.done(); fs.next()) {
     if (fs.offset() == offset) {
       fd->reinitialize(const_cast<InstanceKlass*>(this), fs.index());
@@ -1620,15 +1758,10 @@
   }
   FREE_C_HEAP_ARRAY(int, fields_sorted);
 }
 
 
-void InstanceKlass::array_klasses_do(void f(Klass* k, TRAPS), TRAPS) {
-  if (array_klasses() != NULL)
-    ArrayKlass::cast(array_klasses())->array_klasses_do(f, THREAD);
-}
-
 void InstanceKlass::array_klasses_do(void f(Klass* k)) {
   if (array_klasses() != NULL)
     ArrayKlass::cast(array_klasses())->array_klasses_do(f);
 }
 
@@ -1908,10 +2041,13 @@
                                                                         find_static,
                                                                         private_mode);
     if (method != NULL) {
       return method;
     }
+    if (name == vmSymbols::object_initializer_name()) {
+      break;  // <init> is never inherited, not even as a static factory
+    }
     klass = klass->super();
     overpass_local_mode = skip_overpass;   // Always ignore overpass methods in superclasses
   }
   return NULL;
 }
@@ -2490,10 +2626,14 @@
   // sure the current state is <loaded.
   assert(!is_loaded(), "invalid init state");
   set_package(loader_data, pkg_entry, CHECK);
   Klass::restore_unshareable_info(loader_data, protection_domain, CHECK);
 
+  if (is_value()) {
+    ValueKlass::cast(this)->initialize_calling_convention(CHECK);
+  }
+
   Array<Method*>* methods = this->methods();
   int num_methods = methods->length();
   for (int index = 0; index < num_methods; ++index) {
     methods->at(index)->restore_unshareable_info(CHECK);
   }
@@ -2515,11 +2655,11 @@
     // --> see ArrayKlass::complete_create_array_klass()
     ArrayKlass::cast(array_klasses())->restore_unshareable_info(ClassLoaderData::the_null_class_loader_data(), Handle(), CHECK);
   }
 
   // Initialize current biased locking state.
-  if (UseBiasedLocking && BiasedLocking::enabled()) {
+  if (UseBiasedLocking && BiasedLocking::enabled() && !is_value()) {
     set_prototype_header(markWord::biased_locking_prototype());
   }
 }
 
 void InstanceKlass::set_shared_class_loader_type(s2 loader_type) {
@@ -2633,10 +2773,18 @@
   if (_name != NULL) _name->decrement_refcount();
 
   // unreference array name derived from this class name (arrays of an unloaded
   // class can't be referenced anymore).
   if (_array_name != NULL)  _array_name->decrement_refcount();
+  if (_inline_types != NULL) {
+    for (int i = 0; i < _inline_types->length(); i++) {
+      Symbol* s = _inline_types->at(i)._class_name;
+      if (s != NULL) {
+        s->decrement_refcount();
+      }
+    }
+  }
   FREE_C_HEAP_ARRAY(char, _source_debug_extension);
 }
 
 void InstanceKlass::set_source_debug_extension(const char* array, int length) {
   if (array == NULL) {
@@ -2671,13 +2819,13 @@
   const char* src = (const char*) (name()->as_C_string());
   const int src_length = (int)strlen(src);
 
   char* dest = NEW_RESOURCE_ARRAY(char, src_length + hash_len + 3);
 
-  // Add L as type indicator
+  // Add L or Q as type indicator
   int dest_index = 0;
-  dest[dest_index++] = JVM_SIGNATURE_CLASS;
+  dest[dest_index++] = is_value() ? JVM_SIGNATURE_VALUETYPE : JVM_SIGNATURE_CLASS;
 
   // Add the actual class name
   for (int src_index = 0; src_index < src_length; ) {
     dest[dest_index++] = src[src_index++];
   }
@@ -3233,33 +3381,69 @@
 
 static const char* state_names[] = {
   "allocated", "loaded", "linked", "being_initialized", "fully_initialized", "initialization_error"
 };
 
-static void print_vtable(intptr_t* start, int len, outputStream* st) {
+static void print_vtable(address self, intptr_t* start, int len, outputStream* st) {
+  ResourceMark rm;
+  int* forward_refs = NEW_RESOURCE_ARRAY(int, len);
+  for (int i = 0; i < len; i++)  forward_refs[i] = 0;
   for (int i = 0; i < len; i++) {
     intptr_t e = start[i];
     st->print("%d : " INTPTR_FORMAT, i, e);
+    if (forward_refs[i] != 0) {
+      int from = forward_refs[i];
+      int off = (int) start[from];
+      st->print(" (offset %d <= [%d])", off, from);
+    }
     if (MetaspaceObj::is_valid((Metadata*)e)) {
       st->print(" ");
       ((Metadata*)e)->print_value_on(st);
+    } else if (self != NULL && e > 0 && e < 0x10000) {
+      address location = self + e;
+      int index = (int)((intptr_t*)location - start);
+      st->print(" (offset %d => [%d])", (int)e, index);
+      if (index >= 0 && index < len)
+        forward_refs[index] = i;
     }
     st->cr();
   }
 }
 
 static void print_vtable(vtableEntry* start, int len, outputStream* st) {
-  return print_vtable(reinterpret_cast<intptr_t*>(start), len, st);
+  return print_vtable(NULL, reinterpret_cast<intptr_t*>(start), len, st);
+}
+
+template<typename T>
+ static void print_array_on(outputStream* st, Array<T>* array) {
+   if (array == NULL) { st->print_cr("NULL"); return; }
+   array->print_value_on(st); st->cr();
+   if (Verbose || WizardMode) {
+     for (int i = 0; i < array->length(); i++) {
+       st->print("%d : ", i); array->at(i)->print_value_on(st); st->cr();
+     }
+   }
+ }
+
+static void print_array_on(outputStream* st, Array<int>* array) {
+  if (array == NULL) { st->print_cr("NULL"); return; }
+  array->print_value_on(st); st->cr();
+  if (Verbose || WizardMode) {
+    for (int i = 0; i < array->length(); i++) {
+      st->print("%d : %d", i, array->at(i)); st->cr();
+    }
+  }
 }
 
 void InstanceKlass::print_on(outputStream* st) const {
   assert(is_klass(), "must be klass");
   Klass::print_on(st);
 
   st->print(BULLET"instance size:     %d", size_helper());                        st->cr();
   st->print(BULLET"klass size:        %d", size());                               st->cr();
   st->print(BULLET"access:            "); access_flags().print_on(st);            st->cr();
+  st->print(BULLET"misc flags:        0x%x", _misc_flags);                        st->cr();
   st->print(BULLET"state:             "); st->print_cr("%s", state_names[_init_state]);
   st->print(BULLET"name:              "); name()->print_value_on(st);             st->cr();
   st->print(BULLET"super:             "); Metadata::print_value_on_maybe_null(st, super()); st->cr();
   st->print(BULLET"sub:               ");
   Klass* sub = subklass();
@@ -3282,30 +3466,18 @@
       st->cr();
     }
   }
 
   st->print(BULLET"arrays:            "); Metadata::print_value_on_maybe_null(st, array_klasses()); st->cr();
-  st->print(BULLET"methods:           "); methods()->print_value_on(st);                  st->cr();
-  if (Verbose || WizardMode) {
-    Array<Method*>* method_array = methods();
-    for (int i = 0; i < method_array->length(); i++) {
-      st->print("%d : ", i); method_array->at(i)->print_value(); st->cr();
-    }
-  }
-  st->print(BULLET"method ordering:   "); method_ordering()->print_value_on(st);      st->cr();
-  st->print(BULLET"default_methods:   "); default_methods()->print_value_on(st);      st->cr();
-  if (Verbose && default_methods() != NULL) {
-    Array<Method*>* method_array = default_methods();
-    for (int i = 0; i < method_array->length(); i++) {
-      st->print("%d : ", i); method_array->at(i)->print_value(); st->cr();
-    }
-  }
+  st->print(BULLET"methods:           "); print_array_on(st, methods());
+  st->print(BULLET"method ordering:   "); print_array_on(st, method_ordering());
+  st->print(BULLET"default_methods:   "); print_array_on(st, default_methods());
   if (default_vtable_indices() != NULL) {
-    st->print(BULLET"default vtable indices:   "); default_vtable_indices()->print_value_on(st);       st->cr();
+    st->print(BULLET"default vtable indices:   "); print_array_on(st, default_vtable_indices());
   }
-  st->print(BULLET"local interfaces:  "); local_interfaces()->print_value_on(st);      st->cr();
-  st->print(BULLET"trans. interfaces: "); transitive_interfaces()->print_value_on(st); st->cr();
+  st->print(BULLET"local interfaces:  "); print_array_on(st, local_interfaces());
+  st->print(BULLET"trans. interfaces: "); print_array_on(st, transitive_interfaces());
   st->print(BULLET"constants:         "); constants()->print_value_on(st);         st->cr();
   if (class_loader_data() != NULL) {
     st->print(BULLET"class loader data:  ");
     class_loader_data()->print_value_on(st);
     st->cr();
@@ -3357,11 +3529,11 @@
     st->print_cr(BULLET"java mirror:       NULL");
   }
   st->print(BULLET"vtable length      %d  (start addr: " INTPTR_FORMAT ")", vtable_length(), p2i(start_of_vtable())); st->cr();
   if (vtable_length() > 0 && (Verbose || WizardMode))  print_vtable(start_of_vtable(), vtable_length(), st);
   st->print(BULLET"itable length      %d (start addr: " INTPTR_FORMAT ")", itable_length(), p2i(start_of_itable())); st->cr();
-  if (itable_length() > 0 && (Verbose || WizardMode))  print_vtable(start_of_itable(), itable_length(), st);
+  if (itable_length() > 0 && (Verbose || WizardMode))  print_vtable(NULL, start_of_itable(), itable_length(), st);
   st->print_cr(BULLET"---- static fields (%d words):", static_field_size());
   FieldPrinter print_static_field(st);
   ((InstanceKlass*)this)->do_local_static_fields(&print_static_field);
   st->print_cr(BULLET"---- non-static fields (%d words):", nonstatic_field_size());
   FieldPrinter print_nonstatic_field(st);
@@ -4096,5 +4268,10 @@
 
 unsigned char * InstanceKlass::get_cached_class_file_bytes() {
   return VM_RedefineClasses::get_cached_class_file_bytes(_cached_class_file);
 }
 #endif
+
+#define THROW_DVT_ERROR(s) \
+  Exceptions::fthrow(THREAD_AND_LOCATION, vmSymbols::java_lang_IncompatibleClassChangeError(), \
+      "ValueCapableClass class '%s' %s", external_name(),(s)); \
+      return
diff a/src/hotspot/share/oops/instanceKlass.hpp b/src/hotspot/share/oops/instanceKlass.hpp
--- a/src/hotspot/share/oops/instanceKlass.hpp
+++ b/src/hotspot/share/oops/instanceKlass.hpp
@@ -24,10 +24,11 @@
 
 #ifndef SHARE_OOPS_INSTANCEKLASS_HPP
 #define SHARE_OOPS_INSTANCEKLASS_HPP
 
 #include "classfile/classLoaderData.hpp"
+#include "code/vmreg.hpp"
 #include "memory/referenceType.hpp"
 #include "oops/annotations.hpp"
 #include "oops/constMethod.hpp"
 #include "oops/fieldInfo.hpp"
 #include "oops/instanceOop.hpp"
@@ -52,10 +53,11 @@
 //      The embedded nonstatic oop-map blocks are short pairs (offset, length)
 //      indicating where oops are located in instances of this klass.
 //    [EMBEDDED implementor of the interface] only exist for interface
 //    [EMBEDDED unsafe_anonymous_host klass] only exist for an unsafe anonymous class (JSR 292 enabled)
 //    [EMBEDDED fingerprint       ] only if should_store_fingerprint()==true
+//    [EMBEDDED ValueKlassFixedBlock] only if is a ValueKlass instance
 
 
 // forward declaration for class -- see below for definition
 #if INCLUDE_JVMTI
 class BreakpointInfo;
@@ -68,10 +70,11 @@
 class jniIdMapBase;
 class JNIid;
 class JvmtiCachedClassFieldMap;
 class nmethodBucket;
 class OopMapCache;
+class BufferedValueTypeBlob;
 class InterpreterOopMap;
 class PackageEntry;
 class ModuleEntry;
 
 // This is used in iterators below.
@@ -130,15 +133,39 @@
   uint _count;
 };
 
 struct JvmtiCachedClassFileData;
 
+class SigEntry;
+
+class ValueKlassFixedBlock {
+  Array<SigEntry>** _extended_sig;
+  Array<VMRegPair>** _return_regs;
+  address* _pack_handler;
+  address* _pack_handler_jobject;
+  address* _unpack_handler;
+  int* _default_value_offset;
+  Klass** _value_array_klass;
+  int _alignment;
+  int _first_field_offset;
+  int _exact_size_in_bytes;
+
+  friend class ValueKlass;
+};
+
+class InlineTypes {
+public:
+  u2 _class_info_index;
+  Symbol* _class_name;
+};
+
 class InstanceKlass: public Klass {
   friend class VMStructs;
   friend class JVMCIVMStructs;
   friend class ClassFileParser;
   friend class CompileReplay;
+  friend class TemplateTable;
 
  public:
   static const KlassID ID = InstanceKlassID;
 
  protected:
@@ -152,11 +179,11 @@
   enum ClassState {
     allocated,                          // allocated (but not yet linked)
     loaded,                             // loaded and inserted in class hierarchy (but not linked yet)
     linked,                             // successfully linked/verified (but not initialized yet)
     being_initialized,                  // currently running class initializer
-    fully_initialized,                  // initialized (successfull final state)
+    fully_initialized,                  // initialized (successful final state)
     initialization_error                // error happened during initialization
   };
 
  private:
   static InstanceKlass* allocate_instance_klass(const ClassFileParser& parser, TRAPS);
@@ -200,10 +227,12 @@
   // nest-host. Can also be set directly by JDK API's that establish nest
   // relationships.
   // By always being set it makes nest-member access checks simpler.
   InstanceKlass* _nest_host;
 
+  Array<InlineTypes>* _inline_types;
+
   // The contents of the Record attribute.
   Array<RecordComponent*>* _record_components;
 
   // the source debug extension for this klass, NULL if not specified.
   // Specified as UTF-8 string without terminating zero byte in the classfile,
@@ -230,42 +259,50 @@
   int             _itable_len;           // length of Java itable (in words)
   // _is_marked_dependent can be set concurrently, thus cannot be part of the
   // _misc_flags.
   bool            _is_marked_dependent;  // used for marking during flushing and deoptimization
 
-  // The low two bits of _misc_flags contains the kind field.
-  // This can be used to quickly discriminate among the four kinds of
+  // The low three bits of _misc_flags contains the kind field.
+  // This can be used to quickly discriminate among the five kinds of
   // InstanceKlass.
 
-  static const unsigned _misc_kind_field_size = 2;
+  static const unsigned _misc_kind_field_size = 3;
   static const unsigned _misc_kind_field_pos  = 0;
   static const unsigned _misc_kind_field_mask = (1u << _misc_kind_field_size) - 1u;
 
   static const unsigned _misc_kind_other        = 0; // concrete InstanceKlass
   static const unsigned _misc_kind_reference    = 1; // InstanceRefKlass
   static const unsigned _misc_kind_class_loader = 2; // InstanceClassLoaderKlass
   static const unsigned _misc_kind_mirror       = 3; // InstanceMirrorKlass
+  static const unsigned _misc_kind_inline_type  = 4; // InlineKlass
 
   // Start after _misc_kind field.
   enum {
-    _misc_rewritten                           = 1 << 2,  // methods rewritten.
-    _misc_has_nonstatic_fields                = 1 << 3,  // for sizing with UseCompressedOops
-    _misc_should_verify_class                 = 1 << 4,  // allow caching of preverification
-    _misc_is_unsafe_anonymous                 = 1 << 5,  // has embedded _unsafe_anonymous_host field
-    _misc_is_contended                        = 1 << 6,  // marked with contended annotation
-    _misc_has_nonstatic_concrete_methods      = 1 << 7,  // class/superclass/implemented interfaces has non-static, concrete methods
-    _misc_declares_nonstatic_concrete_methods = 1 << 8,  // directly declares non-static, concrete methods
-    _misc_has_been_redefined                  = 1 << 9,  // class has been redefined
-    _misc_has_passed_fingerprint_check        = 1 << 10, // when this class was loaded, the fingerprint computed from its
+    _misc_rewritten                           = 1 << 3,  // methods rewritten.
+    _misc_has_nonstatic_fields                = 1 << 4,  // for sizing with UseCompressedOops
+    _misc_should_verify_class                 = 1 << 5,  // allow caching of preverification
+    _misc_is_unsafe_anonymous                 = 1 << 6,  // has embedded _unsafe_anonymous_host field
+    _misc_is_contended                        = 1 << 7,  // marked with contended annotation
+    _misc_has_nonstatic_concrete_methods      = 1 << 8,  // class/superclass/implemented interfaces has non-static, concrete methods
+    _misc_declares_nonstatic_concrete_methods = 1 << 9,  // directly declares non-static, concrete methods
+    _misc_has_been_redefined                  = 1 << 10,  // class has been redefined
+    _misc_has_passed_fingerprint_check        = 1 << 11, // when this class was loaded, the fingerprint computed from its
                                                          // code source was found to be matching the value recorded by AOT.
-    _misc_is_scratch_class                    = 1 << 11, // class is the redefined scratch class
-    _misc_is_shared_boot_class                = 1 << 12, // defining class loader is boot class loader
-    _misc_is_shared_platform_class            = 1 << 13, // defining class loader is platform class loader
-    _misc_is_shared_app_class                 = 1 << 14, // defining class loader is app class loader
-    _misc_has_resolved_methods                = 1 << 15, // resolved methods table entries added for this class
-    _misc_is_being_redefined                  = 1 << 16, // used for locking redefinition
-    _misc_has_contended_annotations           = 1 << 17  // has @Contended annotation
+    _misc_is_scratch_class                    = 1 << 12, // class is the redefined scratch class
+    _misc_is_shared_boot_class                = 1 << 13, // defining class loader is boot class loader
+    _misc_is_shared_platform_class            = 1 << 14, // defining class loader is platform class loader
+    _misc_is_shared_app_class                 = 1 << 15, // defining class loader is app class loader
+    _misc_has_resolved_methods                = 1 << 16, // resolved methods table entries added for this class
+    _misc_is_being_redefined                  = 1 << 17, // used for locking redefinition
+    _misc_has_contended_annotations           = 1 << 18, // has @Contended annotation
+    _misc_has_inline_fields                   = 1 << 19, // has inline fields and related embedded section is not empty
+    _misc_is_empty_inline_type                = 1 << 20, // empty inline type
+    _misc_is_naturally_atomic                 = 1 << 21, // loaded/stored in one instruction
+    _misc_is_declared_atomic                  = 1 << 22, // implements jl.NonTearable
+    _misc_invalid_inline_super                = 1 << 23, // invalid super type for an inline type
+    _misc_invalid_identity_super              = 1 << 24, // invalid super type for an identity type
+    _misc_has_injected_identityObject         = 1 << 25  // IdentityObject has been injected by the JVM
   };
   u2 shared_loader_type_bits() const {
     return _misc_is_shared_boot_class|_misc_is_shared_platform_class|_misc_is_shared_app_class;
   }
   u4              _misc_flags;
@@ -328,10 +365,13 @@
   // fn: [access, name index, sig index, initial value index, low_offset, high_offset]
   //     [generic signature index]
   //     [generic signature index]
   //     ...
   Array<u2>*      _fields;
+  const Klass**   _value_field_klasses; // For "inline class" fields, NULL if none present
+
+  const ValueKlassFixedBlock* _adr_valueklass_fixed_block;
 
   // embedded Java vtable follows here
   // embedded Java itables follows here
   // embedded static fields follows here
   // embedded nonstatic oop-map blocks follows here
@@ -386,10 +426,75 @@
     } else {
       _misc_flags &= ~_misc_has_nonstatic_fields;
     }
   }
 
+  bool has_inline_fields() const          {
+    return (_misc_flags & _misc_has_inline_fields) != 0;
+  }
+  void set_has_inline_fields()  {
+    _misc_flags |= _misc_has_inline_fields;
+  }
+
+  bool is_empty_inline_type() const {
+    return (_misc_flags & _misc_is_empty_inline_type) != 0;
+  }
+  void set_is_empty_inline_type() {
+    _misc_flags |= _misc_is_empty_inline_type;
+  }
+
+  // Note:  The naturally_atomic property only applies to
+  // inline classes; it is never true on identity classes.
+  // The bit is placed on instanceKlass for convenience.
+
+  // Query if h/w provides atomic load/store for instances.
+  bool is_naturally_atomic() const {
+    return (_misc_flags & _misc_is_naturally_atomic) != 0;
+  }
+  // Initialized in the class file parser, not changed later.
+  void set_is_naturally_atomic() {
+    _misc_flags |= _misc_is_naturally_atomic;
+  }
+
+  // Query if this class implements jl.NonTearable or was
+  // mentioned in the JVM option AlwaysAtomicValueTypes.
+  // This bit can occur anywhere, but is only significant
+  // for inline classes *and* their super types.
+  // It inherits from supers along with NonTearable.
+  bool is_declared_atomic() const {
+    return (_misc_flags & _misc_is_declared_atomic) != 0;
+  }
+  // Initialized in the class file parser, not changed later.
+  void set_is_declared_atomic() {
+    _misc_flags |= _misc_is_declared_atomic;
+  }
+
+  // Query if class is an invalid super class for an inline type.
+  bool invalid_inline_super() const {
+    return (_misc_flags & _misc_invalid_inline_super) != 0;
+  }
+  // Initialized in the class file parser, not changed later.
+  void set_invalid_inline_super() {
+    _misc_flags |= _misc_invalid_inline_super;
+  }
+  // Query if class is an invalid super class for an identity type.
+  bool invalid_identity_super() const {
+    return (_misc_flags & _misc_invalid_identity_super) != 0;
+  }
+  // Initialized in the class file parser, not changed later.
+  void set_invalid_identity_super() {
+    _misc_flags |= _misc_invalid_identity_super;
+  }
+
+  bool has_injected_identityObject() const {
+    return (_misc_flags & _misc_has_injected_identityObject);
+  }
+
+  void set_has_injected_identityObject() {
+    _misc_flags |= _misc_has_injected_identityObject;
+  }
+
   // field sizes
   int nonstatic_field_size() const         { return _nonstatic_field_size; }
   void set_nonstatic_field_size(int size)  { _nonstatic_field_size = size; }
 
   int static_field_size() const            { return _static_field_size; }
@@ -448,10 +553,12 @@
  public:
   int     field_offset      (int index) const { return field(index)->offset(); }
   int     field_access_flags(int index) const { return field(index)->access_flags(); }
   Symbol* field_name        (int index) const { return field(index)->name(constants()); }
   Symbol* field_signature   (int index) const { return field(index)->signature(constants()); }
+  bool    field_is_flattened(int index) const { return field(index)->is_flattened(); }
+  bool    field_is_flattenable(int index) const { return field(index)->is_flattenable(); }
 
   // Number of Java declared fields
   int java_fields_count() const           { return (int)_java_fields_count; }
 
   Array<u2>* fields() const            { return _fields; }
@@ -568,10 +675,13 @@
 
   // marking
   bool is_marked_dependent() const         { return _is_marked_dependent; }
   void set_is_marked_dependent(bool value) { _is_marked_dependent = value; }
 
+  static ByteSize misc_flags_offset() { return in_ByteSize(offset_of(InstanceKlass, _misc_flags)); }
+  static u4 misc_flags_is_empty_inline_type() { return _misc_is_empty_inline_type; }
+
   // initialization (virtuals from Klass)
   bool should_be_initialized() const;  // means that initialize should be called
   void initialize(TRAPS);
   void link_class(TRAPS);
   bool link_class_or_fail(TRAPS); // returns false on failure
@@ -778,12 +888,13 @@
     }
   }
 
 #if INCLUDE_JVMTI
   // Redefinition locking.  Class can only be redefined by one thread at a time.
+
   bool is_being_redefined() const          {
-    return ((_misc_flags & _misc_is_being_redefined) != 0);
+    return (_misc_flags & _misc_is_being_redefined);
   }
   void set_is_being_redefined(bool value)  {
     if (value) {
       _misc_flags |= _misc_is_being_redefined;
     } else {
@@ -868,10 +979,11 @@
   // Other is anything that is not one of the more specialized kinds of InstanceKlass.
   bool is_other_instance_klass() const        { return is_kind(_misc_kind_other); }
   bool is_reference_instance_klass() const    { return is_kind(_misc_kind_reference); }
   bool is_mirror_instance_klass() const       { return is_kind(_misc_kind_mirror); }
   bool is_class_loader_instance_klass() const { return is_kind(_misc_kind_class_loader); }
+  bool is_inline_type_klass()           const { return is_kind(_misc_kind_inline_type); }
 
 #if INCLUDE_JVMTI
 
   void init_previous_versions() {
     _previous_versions = NULL;
@@ -1043,10 +1155,13 @@
   // support for stub routines
   static ByteSize init_state_offset()  { return in_ByteSize(offset_of(InstanceKlass, _init_state)); }
   JFR_ONLY(DEFINE_KLASS_TRACE_ID_OFFSET;)
   static ByteSize init_thread_offset() { return in_ByteSize(offset_of(InstanceKlass, _init_thread)); }
 
+  static ByteSize value_field_klasses_offset() { return in_ByteSize(offset_of(InstanceKlass, _value_field_klasses)); }
+  static ByteSize adr_valueklass_fixed_block_offset() { return in_ByteSize(offset_of(InstanceKlass, _adr_valueklass_fixed_block)); }
+
   // subclass/subinterface checks
   bool implements_interface(Klass* k) const;
   bool is_same_or_direct_interface(Klass* k) const;
 
 #ifdef ASSERT
@@ -1077,12 +1192,11 @@
   void do_local_static_fields(FieldClosure* cl);
   void do_nonstatic_fields(FieldClosure* cl); // including inherited fields
   void do_local_static_fields(void f(fieldDescriptor*, Handle, TRAPS), Handle, TRAPS);
 
   void methods_do(void f(Method* method));
-  void array_klasses_do(void f(Klass* k));
-  void array_klasses_do(void f(Klass* k, TRAPS), TRAPS);
+  virtual void array_klasses_do(void f(Klass* k));
 
   static InstanceKlass* cast(Klass* k) {
     return const_cast<InstanceKlass*>(cast(const_cast<const Klass*>(k)));
   }
 
@@ -1099,34 +1213,41 @@
   // Sizing (in words)
   static int header_size()            { return sizeof(InstanceKlass)/wordSize; }
 
   static int size(int vtable_length, int itable_length,
                   int nonstatic_oop_map_size,
-                  bool is_interface, bool is_unsafe_anonymous, bool has_stored_fingerprint) {
+                  bool is_interface, bool is_unsafe_anonymous, bool has_stored_fingerprint,
+                  int java_fields, bool is_inline_type) {
     return align_metadata_size(header_size() +
            vtable_length +
            itable_length +
            nonstatic_oop_map_size +
            (is_interface ? (int)sizeof(Klass*)/wordSize : 0) +
            (is_unsafe_anonymous ? (int)sizeof(Klass*)/wordSize : 0) +
-           (has_stored_fingerprint ? (int)sizeof(uint64_t*)/wordSize : 0));
+           (has_stored_fingerprint ? (int)sizeof(uint64_t*)/wordSize : 0) +
+           (java_fields * (int)sizeof(Klass*)/wordSize) +
+           (is_inline_type ? (int)sizeof(ValueKlassFixedBlock) : 0));
   }
   int size() const                    { return size(vtable_length(),
                                                itable_length(),
                                                nonstatic_oop_map_size(),
                                                is_interface(),
                                                is_unsafe_anonymous(),
-                                               has_stored_fingerprint());
+                                               has_stored_fingerprint(),
+                                               has_inline_fields() ? java_fields_count() : 0,
+                                               is_value());
   }
 
   intptr_t* start_of_itable()   const { return (intptr_t*)start_of_vtable() + vtable_length(); }
   intptr_t* end_of_itable()     const { return start_of_itable() + itable_length(); }
 
   int  itable_offset_in_words() const { return start_of_itable() - (intptr_t*)this; }
 
   oop static_field_base_raw() { return java_mirror(); }
 
+  bool bounds_check(address addr, bool edge_ok = false, intptr_t size_in_bytes = -1) const PRODUCT_RETURN0;
+
   OopMapBlock* start_of_nonstatic_oop_maps() const {
     return (OopMapBlock*)(start_of_itable() + itable_length());
   }
 
   Klass** end_of_nonstatic_oop_maps() const {
@@ -1171,12 +1292,57 @@
     } else {
       return NULL;
     }
   }
 
+  address adr_value_fields_klasses() const {
+    if (has_inline_fields()) {
+      address adr_fing = adr_fingerprint();
+      if (adr_fing != NULL) {
+        return adr_fingerprint() + sizeof(u8);
+      }
+
+      InstanceKlass** adr_host = adr_unsafe_anonymous_host();
+      if (adr_host != NULL) {
+        return (address)(adr_host + 1);
+      }
+
+      Klass* volatile* adr_impl = adr_implementor();
+      if (adr_impl != NULL) {
+        return (address)(adr_impl + 1);
+      }
+
+      return (address)end_of_nonstatic_oop_maps();
+    } else {
+      return NULL;
+    }
+  }
+
+  Klass* get_value_field_klass(int idx) const {
+    assert(has_inline_fields(), "Sanity checking");
+    Klass* k = ((Klass**)adr_value_fields_klasses())[idx];
+    assert(k != NULL, "Should always be set before being read");
+    assert(k->is_value(), "Must be a inline type");
+    return k;
+  }
+
+  Klass* get_value_field_klass_or_null(int idx) const {
+    assert(has_inline_fields(), "Sanity checking");
+    Klass* k = ((Klass**)adr_value_fields_klasses())[idx];
+    assert(k == NULL || k->is_value(), "Must be a inline type");
+    return k;
+  }
+
+  void set_value_field_klass(int idx, Klass* k) {
+    assert(has_inline_fields(), "Sanity checking");
+    assert(k != NULL, "Should not be set to NULL");
+    assert(((Klass**)adr_value_fields_klasses())[idx] == NULL, "Should not be set twice");
+    ((Klass**)adr_value_fields_klasses())[idx] = k;
+  }
+
   // Use this to return the size of an instance in heap words:
-  int size_helper() const {
+  virtual int size_helper() const {
     return layout_helper_to_size_helper(layout_helper());
   }
 
   // This bit is initialized in classFileParser.cpp.
   // It is false under any of the following conditions:
@@ -1308,16 +1474,18 @@
   void initialize_impl                           (TRAPS);
   void initialize_super_interfaces               (TRAPS);
   void eager_initialize_impl                     ();
   /* jni_id_for_impl for jfieldID only */
   JNIid* jni_id_for_impl                         (int offset);
-
+protected:
   // Returns the array class for the n'th dimension
-  Klass* array_klass_impl(bool or_null, int n, TRAPS);
+  virtual Klass* array_klass_impl(bool or_null, int n, TRAPS);
 
   // Returns the array class with this class as element type
-  Klass* array_klass_impl(bool or_null, TRAPS);
+  virtual Klass* array_klass_impl(bool or_null, TRAPS);
+
+private:
 
   // find a local method (returns NULL if not found)
   Method* find_method_impl(const Symbol* name,
                            const Symbol* signature,
                            OverpassLookupMode overpass_mode,
@@ -1341,11 +1509,11 @@
 #endif
 public:
   // CDS support - remove and restore oops from metadata. Oops are not shared.
   virtual void remove_unshareable_info();
   virtual void remove_java_mirror();
-  void restore_unshareable_info(ClassLoaderData* loader_data, Handle protection_domain, PackageEntry* pkg_entry, TRAPS);
+  virtual void restore_unshareable_info(ClassLoaderData* loader_data, Handle protection_domain, PackageEntry* pkg_entry, TRAPS);
 
   // jvm support
   jint compute_modifier_flags(TRAPS) const;
 
 public:
diff a/src/hotspot/share/oops/klass.cpp b/src/hotspot/share/oops/klass.cpp
--- a/src/hotspot/share/oops/klass.cpp
+++ b/src/hotspot/share/oops/klass.cpp
@@ -203,11 +203,11 @@
   // Note that T_ARRAY is not allowed here.
   int  hsize = arrayOopDesc::base_offset_in_bytes(etype);
   int  esize = type2aelembytes(etype);
   bool isobj = (etype == T_OBJECT);
   int  tag   =  isobj ? _lh_array_tag_obj_value : _lh_array_tag_type_value;
-  int lh = array_layout_helper(tag, hsize, etype, exact_log2(esize));
+  int lh = array_layout_helper(tag, false, hsize, etype, exact_log2(esize));
 
   assert(lh < (int)_lh_neutral_value, "must look like an array layout");
   assert(layout_helper_is_array(lh), "correct kind");
   assert(layout_helper_is_objArray(lh) == isobj, "correct kind");
   assert(layout_helper_is_typeArray(lh) == !isobj, "correct kind");
diff a/src/hotspot/share/oops/klass.hpp b/src/hotspot/share/oops/klass.hpp
--- a/src/hotspot/share/oops/klass.hpp
+++ b/src/hotspot/share/oops/klass.hpp
@@ -43,14 +43,15 @@
   InstanceKlassID,
   InstanceRefKlassID,
   InstanceMirrorKlassID,
   InstanceClassLoaderKlassID,
   TypeArrayKlassID,
+  ValueArrayKlassID,
   ObjArrayKlassID
 };
 
-const uint KLASS_ID_COUNT = 6;
+const uint KLASS_ID_COUNT = 7;
 
 //
 // A Klass provides:
 //  1: language level class object (method dictionary etc.)
 //  2: provide vm dispatch behavior for the object
@@ -96,11 +97,11 @@
   //
   // For arrays, layout helper is a negative number, containing four
   // distinct bytes, as follows:
   //    MSB:[tag, hsz, ebt, log2(esz)]:LSB
   // where:
-  //    tag is 0x80 if the elements are oops, 0xC0 if non-oops
+  //    tag is 0x80 if the elements are oops, 0xC0 if non-oops, 0xA0 if value types
   //    hsz is array header size in bytes (i.e., offset of first element)
   //    ebt is the BasicType of the elements
   //    esz is the element size in bytes
   // This packed word is arranged so as to be quickly unpacked by the
   // various fast paths that use the various subfields.
@@ -345,14 +346,22 @@
   static const int _lh_log2_element_size_mask  = BitsPerLong-1;
   static const int _lh_element_type_shift      = BitsPerByte*1;
   static const int _lh_element_type_mask       = right_n_bits(BitsPerByte);  // shifted mask
   static const int _lh_header_size_shift       = BitsPerByte*2;
   static const int _lh_header_size_mask        = right_n_bits(BitsPerByte);  // shifted mask
-  static const int _lh_array_tag_bits          = 2;
+  static const int _lh_array_tag_bits          = 3;
   static const int _lh_array_tag_shift         = BitsPerInt - _lh_array_tag_bits;
-  static const int _lh_array_tag_obj_value     = ~0x01;   // 0x80000000 >> 30
-
+
+  static const unsigned int _lh_array_tag_type_value = 0Xfffffffc;
+  static const unsigned int _lh_array_tag_vt_value   = 0Xfffffffd;
+  static const unsigned int _lh_array_tag_obj_value  = 0Xfffffffe;
+
+  // null-free array flag bit under the array tag bits, shift one more to get array tag value
+  static const int _lh_null_free_shift = _lh_array_tag_shift - 1;
+  static const int _lh_null_free_mask  = 1;
+
+  static const jint _lh_array_tag_vt_value_bit_inplace = (jint) (1 << _lh_array_tag_shift);
   static const unsigned int _lh_array_tag_type_value = 0Xffffffff; // ~0x00,  // 0xC0000000 >> 30
 
   static int layout_helper_size_in_bytes(jint lh) {
     assert(lh > (jint)_lh_neutral_value, "must be instance");
     return (int) lh & ~_lh_instance_slow_path_bit;
@@ -366,27 +375,37 @@
   }
   static bool layout_helper_is_array(jint lh) {
     return (jint)lh < (jint)_lh_neutral_value;
   }
   static bool layout_helper_is_typeArray(jint lh) {
-    // _lh_array_tag_type_value == (lh >> _lh_array_tag_shift);
-    return (juint)lh >= (juint)(_lh_array_tag_type_value << _lh_array_tag_shift);
+    return (juint) _lh_array_tag_type_value == (juint)(lh >> _lh_array_tag_shift);
   }
   static bool layout_helper_is_objArray(jint lh) {
-    // _lh_array_tag_obj_value == (lh >> _lh_array_tag_shift);
-    return (jint)lh < (jint)(_lh_array_tag_type_value << _lh_array_tag_shift);
+    return (juint)_lh_array_tag_obj_value == (juint)(lh >> _lh_array_tag_shift);
+  }
+  static bool layout_helper_is_valueArray(jint lh) {
+    return (juint)_lh_array_tag_vt_value == (juint)(lh >> _lh_array_tag_shift);
+  }
+  static bool layout_helper_is_null_free(jint lh) {
+    assert(layout_helper_is_valueArray(lh) || layout_helper_is_objArray(lh), "must be array of inline types");
+    return ((lh >> _lh_null_free_shift) & _lh_null_free_mask);
+  }
+  static jint layout_helper_set_null_free(jint lh) {
+    lh |= (_lh_null_free_mask << _lh_null_free_shift);
+    assert(layout_helper_is_null_free(lh), "Bad encoding");
+    return lh;
   }
   static int layout_helper_header_size(jint lh) {
     assert(lh < (jint)_lh_neutral_value, "must be array");
     int hsize = (lh >> _lh_header_size_shift) & _lh_header_size_mask;
     assert(hsize > 0 && hsize < (int)sizeof(oopDesc)*3, "sanity");
     return hsize;
   }
   static BasicType layout_helper_element_type(jint lh) {
     assert(lh < (jint)_lh_neutral_value, "must be array");
     int btvalue = (lh >> _lh_element_type_shift) & _lh_element_type_mask;
-    assert(btvalue >= T_BOOLEAN && btvalue <= T_OBJECT, "sanity");
+    assert((btvalue >= T_BOOLEAN && btvalue <= T_OBJECT) || btvalue == T_VALUETYPE, "sanity");
     return (BasicType) btvalue;
   }
 
   // Want a pattern to quickly diff against layout header in register
   // find something less clever!
@@ -403,16 +422,17 @@
   }
 
   static int layout_helper_log2_element_size(jint lh) {
     assert(lh < (jint)_lh_neutral_value, "must be array");
     int l2esz = (lh >> _lh_log2_element_size_shift) & _lh_log2_element_size_mask;
-    assert(l2esz <= LogBytesPerLong,
+    assert(layout_helper_element_type(lh) == T_VALUETYPE || l2esz <= LogBytesPerLong,
            "sanity. l2esz: 0x%x for lh: 0x%x", (uint)l2esz, (uint)lh);
     return l2esz;
   }
-  static jint array_layout_helper(jint tag, int hsize, BasicType etype, int log2_esize) {
+  static jint array_layout_helper(jint tag, bool null_free, int hsize, BasicType etype, int log2_esize) {
     return (tag        << _lh_array_tag_shift)
+      |    ((null_free ? 1 : 0) <<  _lh_null_free_shift)
       |    (hsize      << _lh_header_size_shift)
       |    ((int)etype << _lh_element_type_shift)
       |    (log2_esize << _lh_log2_element_size_shift);
   }
   static jint instance_layout_helper(jint size, bool slow_path_flag) {
@@ -549,10 +569,12 @@
   // Returns the name for a class (Resource allocated) as the class
   // would appear in a signature.
   // For arrays, this returns the name of the element with a leading '['.
   // For classes, this returns the name with a leading 'L' and a trailing ';'
   //     and the package separators as '/'.
+  // For value classes, this returns the name with a leading 'Q' and a trailing ';'
+  //     and the package separators as '/'.
   virtual const char* signature_name() const;
 
   const char* joint_in_module_of_loader(const Klass* class2, bool include_parent_loader = false) const;
   const char* class_in_module_of_loader(bool use_are = false, bool include_parent_loader = false) const;
 
@@ -564,11 +586,14 @@
  protected:
   virtual bool is_instance_klass_slow()     const { return false; }
   virtual bool is_array_klass_slow()        const { return false; }
   virtual bool is_objArray_klass_slow()     const { return false; }
   virtual bool is_typeArray_klass_slow()    const { return false; }
+  virtual bool is_valueArray_klass_slow()   const { return false; }
 #endif // ASSERT
+  // current implementation uses this method even in non debug builds
+  virtual bool is_value_slow()          const { return false; }
  public:
 
   // Fast non-virtual versions
   #ifndef ASSERT
   #define assert_same_query(xval, xcheck) xval
@@ -590,12 +615,19 @@
                                                     layout_helper_is_objArray(layout_helper()),
                                                     is_objArray_klass_slow()); }
   inline  bool is_typeArray_klass()           const { return assert_same_query(
                                                     layout_helper_is_typeArray(layout_helper()),
                                                     is_typeArray_klass_slow()); }
+  inline  bool is_value()                     const { return is_value_slow(); } //temporary hack
+  inline  bool is_valueArray_klass()          const { return assert_same_query(
+                                                    layout_helper_is_valueArray(layout_helper()),
+                                                    is_valueArray_klass_slow()); }
+
   #undef assert_same_query
 
+  inline bool is_null_free_array_klass()      const { return layout_helper_is_null_free(layout_helper()); }
+
   // Access flags
   AccessFlags access_flags() const         { return _access_flags;  }
   void set_access_flags(AccessFlags flags) { _access_flags = flags; }
 
   bool is_public() const                { return _access_flags.is_public(); }
@@ -625,11 +657,15 @@
 
   // Biased locking support
   // Note: the prototype header is always set up to be at least the
   // prototype markWord. If biased locking is enabled it may further be
   // biasable and have an epoch.
-  markWord prototype_header() const      { return _prototype_header; }
+  markWord prototype_header() const     { return _prototype_header; }
+  static inline markWord default_prototype_header(Klass* k) {
+    return (k == NULL) ? markWord::prototype() : k->prototype_header();
+  }
+
   // NOTE: once instances of this klass are floating around in the
   // system, this header must only be updated at a safepoint.
   // NOTE 2: currently we only ever set the prototype header to the
   // biasable prototype for instanceKlasses. There is no technical
   // reason why it could not be done for arrayKlasses aside from
diff a/src/hotspot/share/oops/method.cpp b/src/hotspot/share/oops/method.cpp
--- a/src/hotspot/share/oops/method.cpp
+++ b/src/hotspot/share/oops/method.cpp
@@ -52,10 +52,11 @@
 #include "oops/methodData.hpp"
 #include "oops/objArrayKlass.hpp"
 #include "oops/objArrayOop.inline.hpp"
 #include "oops/oop.inline.hpp"
 #include "oops/symbol.hpp"
+#include "oops/valueKlass.inline.hpp"
 #include "prims/jvmtiExport.hpp"
 #include "prims/methodHandles.hpp"
 #include "prims/nativeLookup.hpp"
 #include "runtime/arguments.hpp"
 #include "runtime/atomic.hpp"
@@ -111,11 +112,10 @@
 
   if (access_flags.is_native()) {
     clear_native_function();
     set_signature_handler(NULL);
   }
-
   NOT_PRODUCT(set_compiled_invocation_count(0);)
 }
 
 // Release Method*.  The nmethod will be gone when we get here because
 // we've walked the code cache.
@@ -148,15 +148,25 @@
 address Method::get_c2i_entry() {
   assert(adapter() != NULL, "must have");
   return adapter()->get_c2i_entry();
 }
 
+address Method::get_c2i_value_entry() {
+  assert(adapter() != NULL, "must have");
+  return adapter()->get_c2i_value_entry();
+}
+
 address Method::get_c2i_unverified_entry() {
   assert(adapter() != NULL, "must have");
   return adapter()->get_c2i_unverified_entry();
 }
 
+address Method::get_c2i_unverified_value_entry() {
+  assert(adapter() != NULL, "must have");
+  return adapter()->get_c2i_unverified_value_entry();
+}
+
 address Method::get_c2i_no_clinit_check_entry() {
   assert(VM_Version::supports_fast_class_init_checks(), "");
   assert(adapter() != NULL, "must have");
   return adapter()->get_c2i_no_clinit_check_entry();
 }
@@ -345,10 +355,12 @@
   it->push(&_method_counters);
 
   Method* this_ptr = this;
   it->push_method_entry(&this_ptr, (intptr_t*)&_i2i_entry);
   it->push_method_entry(&this_ptr, (intptr_t*)&_from_compiled_entry);
+  it->push_method_entry(&this_ptr, (intptr_t*)&_from_compiled_value_ro_entry);
+  it->push_method_entry(&this_ptr, (intptr_t*)&_from_compiled_value_entry);
   it->push_method_entry(&this_ptr, (intptr_t*)&_from_interpreted_entry);
 }
 
 // Attempt to return method oop to original state.  Clear any pointers
 // (to objects outside the shared spaces).  We won't be able to predict
@@ -588,21 +600,39 @@
   set_size_of_parameters(fp.size_of_parameters());
   constMethod()->set_result_type(fp.return_type());
   constMethod()->set_fingerprint(fp.fingerprint());
 }
 
+// ValueKlass the method is declared to return. This must not
+// safepoint as it is called with references live on the stack at
+// locations the GC is unaware of.
+ValueKlass* Method::returned_value_type(Thread* thread) const {
+  SignatureStream ss(signature());
+  while (!ss.at_return_type()) {
+    ss.next();
+  }
+  Handle class_loader(thread, method_holder()->class_loader());
+  Handle protection_domain(thread, method_holder()->protection_domain());
+  Klass* k = NULL;
+  {
+    NoSafepointVerifier nsv;
+    k = ss.as_klass(class_loader, protection_domain, SignatureStream::ReturnNull, thread);
+  }
+  assert(k != NULL && !thread->has_pending_exception(), "can't resolve klass");
+  return ValueKlass::cast(k);
+}
 bool Method::is_empty_method() const {
   return  code_size() == 1
       && *code_base() == Bytecodes::_return;
 }
 
 bool Method::is_vanilla_constructor() const {
   // Returns true if this method is a vanilla constructor, i.e. an "<init>" "()V" method
   // which only calls the superclass vanilla constructor and possibly does stores of
   // zero constants to local fields:
   //
-  //   aload_0
+  //   aload_0, _fast_aload_0, or _nofast_aload_0
   //   invokespecial
   //   indexbyte1
   //   indexbyte2
   //
   // followed by an (optional) sequence of:
@@ -622,11 +652,12 @@
   int size = code_size();
   // Check if size match
   if (size == 0 || size % 5 != 0) return false;
   address cb = code_base();
   int last = size - 1;
-  if (cb[0] != Bytecodes::_aload_0 || cb[1] != Bytecodes::_invokespecial || cb[last] != Bytecodes::_return) {
+  if ((cb[0] != Bytecodes::_aload_0 && cb[0] != Bytecodes::_fast_aload_0 && cb[0] != Bytecodes::_nofast_aload_0) ||
+       cb[1] != Bytecodes::_invokespecial || cb[last] != Bytecodes::_return) {
     // Does not call superclass default constructor
     return false;
   }
   // Check optional sequence
   for (int i = 4; i < last; i += 5) {
@@ -774,29 +805,31 @@
           Bytecodes::is_const(java_code_at(0)) &&
           Bytecodes::length_for(java_code_at(0)) == last_index &&
           Bytecodes::is_return(java_code_at(last_index)));
 }
 
-bool Method::is_initializer() const {
-  return is_object_initializer() || is_static_initializer();
-}
-
-bool Method::has_valid_initializer_flags() const {
-  return (is_static() ||
-          method_holder()->major_version() < 51);
+bool Method::is_object_constructor_or_class_initializer() const {
+  return (is_object_constructor() || is_class_initializer());
 }
 
-bool Method::is_static_initializer() const {
+bool Method::is_class_initializer() const {
   // For classfiles version 51 or greater, ensure that the clinit method is
   // static.  Non-static methods with the name "<clinit>" are not static
   // initializers. (older classfiles exempted for backward compatibility)
-  return name() == vmSymbols::class_initializer_name() &&
-         has_valid_initializer_flags();
+  return (name() == vmSymbols::class_initializer_name() &&
+          (is_static() ||
+           method_holder()->major_version() < 51));
+}
+
+// A method named <init>, if non-static, is a classic object constructor.
+bool Method::is_object_constructor() const {
+   return name() == vmSymbols::object_initializer_name() && !is_static();
 }
 
-bool Method::is_object_initializer() const {
-   return name() == vmSymbols::object_initializer_name();
+// A static method named <init> is a factory for an inline class.
+bool Method::is_static_init_factory() const {
+   return name() == vmSymbols::object_initializer_name() && is_static();
 }
 
 bool Method::needs_clinit_barrier() const {
   return is_static() && !method_holder()->is_initialized();
 }
@@ -850,11 +883,11 @@
   return best_line;
 }
 
 
 bool Method::is_klass_loaded_by_klass_index(int klass_index) const {
-  if( constants()->tag_at(klass_index).is_unresolved_klass() ) {
+  if( constants()->tag_at(klass_index).is_unresolved_klass()) {
     Thread *thread = Thread::current();
     Symbol* klass_name = constants()->klass_name_at(klass_index);
     Handle loader(thread, method_holder()->class_loader());
     Handle prot  (thread, method_holder()->protection_domain());
     return SystemDictionary::find(klass_name, loader, prot, thread) != NULL;
@@ -866,11 +899,13 @@
 
 bool Method::is_klass_loaded(int refinfo_index, bool must_be_resolved) const {
   int klass_index = constants()->klass_ref_index_at(refinfo_index);
   if (must_be_resolved) {
     // Make sure klass is resolved in constantpool.
-    if (constants()->tag_at(klass_index).is_unresolved_klass()) return false;
+    if (constants()->tag_at(klass_index).is_unresolved_klass()) {
+      return false;
+    }
   }
   return is_klass_loaded_by_klass_index(klass_index);
 }
 
 
@@ -1035,12 +1070,16 @@
 void Method::clear_code() {
   // this may be NULL if c2i adapters have not been made yet
   // Only should happen at allocate time.
   if (adapter() == NULL) {
     _from_compiled_entry    = NULL;
+    _from_compiled_value_entry = NULL;
+    _from_compiled_value_ro_entry = NULL;
   } else {
     _from_compiled_entry    = adapter()->get_c2i_entry();
+    _from_compiled_value_entry = adapter()->get_c2i_value_entry();
+    _from_compiled_value_ro_entry = adapter()->get_c2i_value_ro_entry();
   }
   OrderAccess::storestore();
   _from_interpreted_entry = _i2i_entry;
   OrderAccess::storestore();
   _code = NULL;
@@ -1081,13 +1120,22 @@
   } else {
     // TODO: Simplify the adapter trampoline allocation for static archiving.
     //       Remove the use of CDSAdapterHandlerEntry.
     CDSAdapterHandlerEntry* cds_adapter = (CDSAdapterHandlerEntry*)adapter();
     constMethod()->set_adapter_trampoline(cds_adapter->get_adapter_trampoline());
+
     _from_compiled_entry = cds_adapter->get_c2i_entry_trampoline();
     assert(*((int*)_from_compiled_entry) == 0,
-           "must be NULL during dump time, to be initialized at run time");
+           "instructions must be zeros during dump time, to be initialized at run time");
+
+    _from_compiled_value_ro_entry = cds_adapter->get_c2i_value_ro_entry_trampoline();
+    assert(*((int*)_from_compiled_value_ro_entry) == 0,
+           "instructions must be zeros during dump time, to be initialized at run time");
+
+    _from_compiled_value_entry = cds_adapter->get_c2i_value_entry_trampoline();
+    assert(*((int*)_from_compiled_value_entry) == 0,
+           "instructions must be zeros during dump time, to be initialized at run time");
   }
 
   if (is_native()) {
     *native_function_addr() = NULL;
     set_signature_handler(NULL);
@@ -1240,34 +1288,61 @@
   }
 
   if (mh->is_shared()) {
     assert(mh->adapter() == adapter, "must be");
     assert(mh->_from_compiled_entry != NULL, "must be");
+    assert(mh->_from_compiled_value_entry != NULL, "must be");
+    assert(mh->_from_compiled_value_ro_entry != NULL, "must be");
   } else {
     mh->set_adapter_entry(adapter);
     mh->_from_compiled_entry = adapter->get_c2i_entry();
+    mh->_from_compiled_value_entry = adapter->get_c2i_value_entry();
+    mh->_from_compiled_value_ro_entry = adapter->get_c2i_value_ro_entry();
   }
   return adapter->get_c2i_entry();
 }
 
 void Method::restore_unshareable_info(TRAPS) {
   assert(is_method() && is_valid_method(this), "ensure C++ vtable is restored");
 
+#if 0
+  /*
+   * CDS:TODO --
+   * "Q" classes in the method signature must be resolved during link_method.
+   * However, at this point we are still inside method_holder()->restore_unshareable_info.
+   * If we try to resolve method_holder(), or multually dependent classes, it will
+   * cause deadlock and other ill effects.
+   *
+   * For now, lets do method linking inside InstanceKlass::link_class(). Optimization
+   * may be possible if we know that resolution will never happen.
+   */
+
   // Since restore_unshareable_info can be called more than once for a method, don't
   // redo any work.
   if (adapter() == NULL) {
     methodHandle mh(THREAD, this);
     link_method(mh, CHECK);
   }
+#endif
 }
 
-address Method::from_compiled_entry_no_trampoline() const {
+address Method::from_compiled_entry_no_trampoline(bool caller_is_c1) const {
   CompiledMethod *code = Atomic::load_acquire(&_code);
-  if (code) {
-    return code->verified_entry_point();
+  if (caller_is_c1) {
+    // C1 - value arguments are passed as objects
+    if (code) {
+      return code->verified_value_entry_point();
+    } else {
+      return adapter()->get_c2i_value_entry();
+    }
   } else {
-    return adapter()->get_c2i_entry();
+    // C2 - value arguments may be passed as fields
+    if (code) {
+      return code->verified_entry_point();
+    } else {
+      return adapter()->get_c2i_entry();
+    }
   }
 }
 
 // The verified_code_entry() must be called when a invoke is resolved
 // on this method.
@@ -1280,10 +1355,22 @@
   debug_only(NoSafepointVerifier nsv;)
   assert(_from_compiled_entry != NULL, "must be set");
   return _from_compiled_entry;
 }
 
+address Method::verified_value_code_entry() {
+  debug_only(NoSafepointVerifier nsv;)
+  assert(_from_compiled_value_entry != NULL, "must be set");
+  return _from_compiled_value_entry;
+}
+
+address Method::verified_value_ro_code_entry() {
+  debug_only(NoSafepointVerifier nsv;)
+  assert(_from_compiled_value_ro_entry != NULL, "must be set");
+  return _from_compiled_value_ro_entry;
+}
+
 // Check that if an nmethod ref exists, it has a backlink to this or no backlink at all
 // (could be racing a deopt).
 // Not inline to avoid circular ref.
 bool Method::check_code() const {
   // cached in a register or local.  There's a race on the value of the field.
@@ -1311,10 +1398,12 @@
     mh->set_highest_comp_level(comp_level);
   }
 
   OrderAccess::storestore();
   mh->_from_compiled_entry = code->verified_entry_point();
+  mh->_from_compiled_value_entry = code->verified_value_entry_point();
+  mh->_from_compiled_value_ro_entry = code->verified_value_ro_entry_point();
   OrderAccess::storestore();
   // Instantly compiled code can execute.
   if (!mh->is_method_handle_intrinsic())
     mh->_from_interpreted_entry = mh->get_i2c_entry();
 }
@@ -2341,10 +2430,12 @@
   if (intrinsic_id() != vmIntrinsics::_none)
     st->print_cr(" - intrinsic id:      %d %s", intrinsic_id(), vmIntrinsics::name_at(intrinsic_id()));
   if (highest_comp_level() != CompLevel_none)
     st->print_cr(" - highest level:     %d", highest_comp_level());
   st->print_cr(" - vtable index:      %d",   _vtable_index);
+  if (valid_itable_index())
+    st->print_cr(" - itable index:      %d",   itable_index());
   st->print_cr(" - i2i entry:         " INTPTR_FORMAT, p2i(interpreter_entry()));
   st->print(   " - adapters:          ");
   AdapterHandlerEntry* a = ((Method*)this)->adapter();
   if (a == NULL)
     st->print_cr(INTPTR_FORMAT, p2i(a));
@@ -2418,10 +2509,11 @@
 void Method::print_value_on(outputStream* st) const {
   assert(is_method(), "must be method");
   st->print("%s", internal_name());
   print_address_on(st);
   st->print(" ");
+  if (WizardMode) access_flags().print_on(st);
   name()->print_value_on(st);
   st->print(" ");
   signature()->print_value_on(st);
   st->print(" in ");
   method_holder()->print_value_on(st);
diff a/src/hotspot/share/oops/method.hpp b/src/hotspot/share/oops/method.hpp
--- a/src/hotspot/share/oops/method.hpp
+++ b/src/hotspot/share/oops/method.hpp
@@ -88,11 +88,14 @@
     _dont_inline           = 1 << 2,
     _hidden                = 1 << 3,
     _has_injected_profile  = 1 << 4,
     _running_emcp          = 1 << 5,
     _intrinsic_candidate   = 1 << 6,
-    _reserved_stack_access = 1 << 7
+    _reserved_stack_access = 1 << 7,
+    _scalarized_args       = 1 << 8,
+    _c1_needs_stack_repair = 1 << 9,
+    _c2_needs_stack_repair = 1 << 10
   };
   mutable u2 _flags;
 
   JFR_ONLY(DEFINE_TRACE_FLAG;)
 
@@ -101,18 +104,21 @@
 #endif
   // Entry point for calling both from and to the interpreter.
   address _i2i_entry;           // All-args-on-stack calling convention
   // Entry point for calling from compiled code, to compiled code if it exists
   // or else the interpreter.
-  volatile address _from_compiled_entry;        // Cache of: _code ? _code->entry_point() : _adapter->c2i_entry()
+  volatile address _from_compiled_entry;          // Cache of: _code ? _code->verified_entry_point()          : _adapter->c2i_entry()
+  volatile address _from_compiled_value_ro_entry; // Cache of: _code ? _code->verified_value_ro_entry_point() : _adapter->c2i_value_ro_entry()
+  volatile address _from_compiled_value_entry;    // Cache of: _code ? _code->verified_value_entry_point()    : _adapter->c2i_value_entry()
   // The entry point for calling both from and to compiled code is
   // "_code->entry_point()".  Because of tiered compilation and de-opt, this
   // field can come and go.  It can transition from NULL to not-null at any
   // time (whenever a compile completes).  It can transition from not-null to
   // NULL only at safepoints (because of a de-opt).
   CompiledMethod* volatile _code;                       // Points to the corresponding piece of native code
   volatile address           _from_interpreted_entry; // Cache of _code ? _adapter->i2c_entry() : _i2i_entry
+  int _max_vt_buffer; // max number of VT buffer chunk to use before recycling
 
 #if INCLUDE_AOT && defined(TIERED)
   CompiledMethod* _aot_code;
 #endif
 
@@ -140,11 +146,13 @@
   void set_constMethod(ConstMethod* xconst)    { _constMethod = xconst; }
 
 
   static address make_adapters(const methodHandle& mh, TRAPS);
   address from_compiled_entry() const;
-  address from_compiled_entry_no_trampoline() const;
+  address from_compiled_value_ro_entry() const;
+  address from_compiled_value_entry() const;
+  address from_compiled_entry_no_trampoline(bool caller_is_c1) const;
   address from_interpreted_entry() const;
 
   // access flag
   AccessFlags access_flags() const               { return _access_flags;  }
   void set_access_flags(AccessFlags flags)       { _access_flags = flags; }
@@ -467,10 +475,12 @@
   // if this (shared) method were mapped into another JVM.
   void remove_unshareable_info();
 
   // nmethod/verified compiler entry
   address verified_code_entry();
+  address verified_value_code_entry();
+  address verified_value_ro_code_entry();
   bool check_code() const;      // Not inline to avoid circular ref
   CompiledMethod* volatile code() const;
 
   // Locks CompiledMethod_lock if not held.
   void unlink_code(CompiledMethod *compare);
@@ -491,16 +501,24 @@
   }
   void update_adapter_trampoline(AdapterHandlerEntry* adapter) {
     constMethod()->update_adapter_trampoline(adapter);
   }
   void set_from_compiled_entry(address entry) {
-    _from_compiled_entry =  entry;
+    _from_compiled_entry = entry;
+  }
+  void set_from_compiled_value_ro_entry(address entry) {
+    _from_compiled_value_ro_entry = entry;
+  }
+  void set_from_compiled_value_entry(address entry) {
+    _from_compiled_value_entry = entry;
   }
 
   address get_i2c_entry();
   address get_c2i_entry();
+  address get_c2i_value_entry();
   address get_c2i_unverified_entry();
+  address get_c2i_unverified_value_entry();
   address get_c2i_no_clinit_check_entry();
   AdapterHandlerEntry* adapter() const {
     return constMethod()->adapter();
   }
   // setup entry points
@@ -609,11 +627,11 @@
   InstanceKlass* method_holder() const         { return constants()->pool_holder(); }
 
   Symbol* klass_name() const;                    // returns the name of the method holder
   BasicType result_type() const                  { return constMethod()->result_type(); }
   bool is_returning_oop() const                  { BasicType r = result_type(); return is_reference_type(r); }
-  bool is_returning_fp() const                   { BasicType r = result_type(); return (r == T_FLOAT || r == T_DOUBLE); }
+  ValueKlass* returned_value_type(Thread* thread) const;
 
   // Checked exceptions thrown by this method (resolved to mirrors)
   objArrayHandle resolved_checked_exceptions(TRAPS) { return resolved_checked_exceptions_impl(this, THREAD); }
 
   // Access flags
@@ -682,22 +700,23 @@
   bool is_setter() const;
 
   // returns true if the method does nothing but return a constant of primitive type
   bool is_constant_getter() const;
 
-  // returns true if the method is an initializer (<init> or <clinit>).
-  bool is_initializer() const;
-
-  // returns true if the method is static OR if the classfile version < 51
-  bool has_valid_initializer_flags() const;
-
   // returns true if the method name is <clinit> and the method has
   // valid static initializer flags.
-  bool is_static_initializer() const;
+  bool is_class_initializer() const;
+
+  // returns true if the method name is <init> and the method is not a static factory
+  bool is_object_constructor() const;
 
-  // returns true if the method name is <init>
-  bool is_object_initializer() const;
+  // returns true if the method is an object constructor or class initializer
+  // (non-static <init> or <clinit>), but false for factories (static <init>).
+  bool is_object_constructor_or_class_initializer() const;
+
+  // returns true if the method name is <init> and the method is static
+  bool is_static_init_factory() const;
 
   // compiled code support
   // NOTE: code() is inherently racy as deopt can be clearing code
   // simultaneously. Use with caution.
   bool has_compiled_code() const;
@@ -719,11 +738,14 @@
 
   // interpreter support
   static ByteSize const_offset()                 { return byte_offset_of(Method, _constMethod       ); }
   static ByteSize access_flags_offset()          { return byte_offset_of(Method, _access_flags      ); }
   static ByteSize from_compiled_offset()         { return byte_offset_of(Method, _from_compiled_entry); }
+  static ByteSize from_compiled_value_offset()   { return byte_offset_of(Method, _from_compiled_value_entry); }
+  static ByteSize from_compiled_value_ro_offset(){ return byte_offset_of(Method, _from_compiled_value_ro_entry); }
   static ByteSize code_offset()                  { return byte_offset_of(Method, _code); }
+  static ByteSize flags_offset()                 { return byte_offset_of(Method, _flags); }
   static ByteSize method_data_offset()           {
     return byte_offset_of(Method, _method_data);
   }
   static ByteSize method_counters_offset()       {
     return byte_offset_of(Method, _method_counters);
@@ -740,10 +762,12 @@
   // for code generation
   static int method_data_offset_in_bytes()       { return offset_of(Method, _method_data); }
   static int intrinsic_id_offset_in_bytes()      { return offset_of(Method, _intrinsic_id); }
   static int intrinsic_id_size_in_bytes()        { return sizeof(u2); }
 
+  static ByteSize max_vt_buffer_offset()         { return byte_offset_of(Method, _max_vt_buffer); }
+
   // Static methods that are used to implement member methods where an exposed this pointer
   // is needed due to possible GCs
   static objArrayHandle resolved_checked_exceptions_impl(Method* method, TRAPS);
 
   // Returns the byte code index from the byte code pointer
@@ -920,10 +944,34 @@
 
   void set_has_reserved_stack_access(bool x) {
     _flags = x ? (_flags | _reserved_stack_access) : (_flags & ~_reserved_stack_access);
   }
 
+  bool has_scalarized_args() {
+    return (_flags & _scalarized_args) != 0;
+  }
+
+  void set_has_scalarized_args(bool x) {
+    _flags = x ? (_flags | _scalarized_args) : (_flags & ~_scalarized_args);
+  }
+
+  bool c1_needs_stack_repair() {
+    return (_flags & _c1_needs_stack_repair) != 0;
+  }
+
+  bool c2_needs_stack_repair() {
+    return (_flags & _c2_needs_stack_repair) != 0;
+  }
+
+  void set_c1_needs_stack_repair(bool x) {
+    _flags = x ? (_flags | _c1_needs_stack_repair) : (_flags & ~_c1_needs_stack_repair);
+  }
+
+  void set_c2_needs_stack_repair(bool x) {
+    _flags = x ? (_flags | _c2_needs_stack_repair) : (_flags & ~_c2_needs_stack_repair);
+  }
+
   JFR_ONLY(DEFINE_TRACE_FLAG_ACCESSOR;)
 
   ConstMethod::MethodType method_type() const {
       return _constMethod->method_type();
   }
diff a/src/hotspot/share/oops/oopsHierarchy.hpp b/src/hotspot/share/oops/oopsHierarchy.hpp
--- a/src/hotspot/share/oops/oopsHierarchy.hpp
+++ b/src/hotspot/share/oops/oopsHierarchy.hpp
@@ -45,10 +45,11 @@
 typedef class oopDesc*                    oop;
 typedef class   instanceOopDesc*            instanceOop;
 typedef class   arrayOopDesc*               arrayOop;
 typedef class     objArrayOopDesc*            objArrayOop;
 typedef class     typeArrayOopDesc*           typeArrayOop;
+typedef class     valueArrayOopDesc*          valueArrayOop;
 
 #else
 
 // When CHECK_UNHANDLED_OOPS is defined, an "oop" is a class with a
 // carefully chosen set of constructors and conversion operators to go
@@ -159,10 +160,11 @@
 
 DEF_OOP(instance);
 DEF_OOP(array);
 DEF_OOP(objArray);
 DEF_OOP(typeArray);
+DEF_OOP(valueArray);
 
 #endif // CHECK_UNHANDLED_OOPS
 
 // For CHECK_UNHANDLED_OOPS, it is ambiguous C++ behavior to have the oop
 // structure contain explicit user defined conversions of both numerical
@@ -192,10 +194,12 @@
 class Klass;
 class   InstanceKlass;
 class     InstanceMirrorKlass;
 class     InstanceClassLoaderKlass;
 class     InstanceRefKlass;
+class     ValueKlass;
 class   ArrayKlass;
 class     ObjArrayKlass;
 class     TypeArrayKlass;
+class     ValueArrayKlass;
 
 #endif // SHARE_OOPS_OOPSHIERARCHY_HPP
diff a/src/hotspot/share/opto/c2compiler.cpp b/src/hotspot/share/opto/c2compiler.cpp
--- a/src/hotspot/share/opto/c2compiler.cpp
+++ b/src/hotspot/share/opto/c2compiler.cpp
@@ -496,28 +496,32 @@
   case vmIntrinsics::_indexOfU_char:
   case vmIntrinsics::_toBytesStringU:
   case vmIntrinsics::_getCharsStringU:
   case vmIntrinsics::_getCharStringU:
   case vmIntrinsics::_putCharStringU:
+  case vmIntrinsics::_makePrivateBuffer:
+  case vmIntrinsics::_finishPrivateBuffer:
   case vmIntrinsics::_getReference:
   case vmIntrinsics::_getBoolean:
   case vmIntrinsics::_getByte:
   case vmIntrinsics::_getShort:
   case vmIntrinsics::_getChar:
   case vmIntrinsics::_getInt:
   case vmIntrinsics::_getLong:
   case vmIntrinsics::_getFloat:
   case vmIntrinsics::_getDouble:
+  case vmIntrinsics::_getValue:
   case vmIntrinsics::_putReference:
   case vmIntrinsics::_putBoolean:
   case vmIntrinsics::_putByte:
   case vmIntrinsics::_putShort:
   case vmIntrinsics::_putChar:
   case vmIntrinsics::_putInt:
   case vmIntrinsics::_putLong:
   case vmIntrinsics::_putFloat:
   case vmIntrinsics::_putDouble:
+  case vmIntrinsics::_putValue:
   case vmIntrinsics::_getReferenceVolatile:
   case vmIntrinsics::_getBooleanVolatile:
   case vmIntrinsics::_getByteVolatile:
   case vmIntrinsics::_getShortVolatile:
   case vmIntrinsics::_getCharVolatile:
diff a/src/hotspot/share/opto/castnode.cpp b/src/hotspot/share/opto/castnode.cpp
--- a/src/hotspot/share/opto/castnode.cpp
+++ b/src/hotspot/share/opto/castnode.cpp
@@ -25,14 +25,17 @@
 #include "precompiled.hpp"
 #include "opto/addnode.hpp"
 #include "opto/callnode.hpp"
 #include "opto/castnode.hpp"
 #include "opto/connode.hpp"
+#include "opto/graphKit.hpp"
 #include "opto/matcher.hpp"
 #include "opto/phaseX.hpp"
+#include "opto/rootnode.hpp"
 #include "opto/subnode.hpp"
 #include "opto/type.hpp"
+#include "opto/valuetypenode.hpp"
 
 //=============================================================================
 // If input is already higher or equal to cast type, then this is an identity.
 Node* ConstraintCastNode::Identity(PhaseGVN* phase) {
   Node* dom = dominating_cast(phase, phase);
@@ -281,10 +284,13 @@
 
 //=============================================================================
 //------------------------------Identity---------------------------------------
 // If input is already higher or equal to cast type, then this is an identity.
 Node* CheckCastPPNode::Identity(PhaseGVN* phase) {
+  if (in(1)->is_ValueTypeBase() && _type->isa_oopptr() && phase->type(in(1))->value_klass()->is_subtype_of(_type->is_oopptr()->klass())) {
+    return in(1);
+  }
   Node* dom = dominating_cast(phase, phase);
   if (dom != NULL) {
     return dom;
   }
   if (_carry_dependency) {
@@ -460,10 +466,26 @@
   if (t == Type::TOP) return Type::TOP;
   if (t->base() == Type::RawPtr && t->singleton()) {
     uintptr_t bits = (uintptr_t) t->is_rawptr()->get_con();
     return TypeX::make(bits);
   }
+
+  if (t->is_zero_type() || !t->maybe_null()) {
+    for (DUIterator_Fast imax, i = fast_outs(imax); i < imax; i++) {
+      Node* u = fast_out(i);
+      if (u->Opcode() == Op_OrL) {
+        for (DUIterator_Fast jmax, j = u->fast_outs(jmax); j < jmax; j++) {
+          Node* cmp = u->fast_out(j);
+          if (cmp->Opcode() == Op_CmpL) {
+            // Give CmpL a chance to get optimized
+            phase->record_for_igvn(cmp);
+          }
+        }
+      }
+    }
+  }
+
   return CastP2XNode::bottom_type();
 }
 
 Node *CastP2XNode::Ideal(PhaseGVN *phase, bool can_reshape) {
   return (in(0) && remove_dead_region(phase, can_reshape)) ? this : NULL;
diff a/src/hotspot/share/opto/castnode.hpp b/src/hotspot/share/opto/castnode.hpp
--- a/src/hotspot/share/opto/castnode.hpp
+++ b/src/hotspot/share/opto/castnode.hpp
@@ -147,8 +147,25 @@
   virtual const Type *bottom_type() const { return TypeX_X; }
   // Return false to keep node from moving away from an associated card mark.
   virtual bool depends_only_on_test() const { return false; }
 };
 
+// convert between a 32 bit integer and a compressed pointer
+class CastI2NNode : public TypeNode {
+  public:
+  CastI2NNode(Node *n, const Type *t) : TypeNode(t, 2) {
+    init_req(1, n);
+  }
+  virtual int Opcode() const;
+  virtual uint ideal_reg() const { return Op_RegN; }
+};
+
+class CastN2INode : public Node {
+  public:
+  CastN2INode(Node *n) : Node(NULL, n) {}
+  virtual int Opcode() const;
+  virtual uint ideal_reg() const { return Op_RegI; }
+  virtual const Type *bottom_type() const { return TypeInt::INT; }
+};
 
 
 #endif // SHARE_OPTO_CASTNODE_HPP
diff a/src/hotspot/share/opto/cfgnode.cpp b/src/hotspot/share/opto/cfgnode.cpp
--- a/src/hotspot/share/opto/cfgnode.cpp
+++ b/src/hotspot/share/opto/cfgnode.cpp
@@ -41,10 +41,11 @@
 #include "opto/mulnode.hpp"
 #include "opto/phaseX.hpp"
 #include "opto/regmask.hpp"
 #include "opto/runtime.hpp"
 #include "opto/subnode.hpp"
+#include "opto/valuetypenode.hpp"
 #include "utilities/vmError.hpp"
 
 // Portions of code courtesy of Clifford Click
 
 // Optimization - Graph Style
@@ -370,11 +371,11 @@
   }
 
   return true; // The Region node is unreachable - it is dead.
 }
 
-bool RegionNode::try_clean_mem_phi(PhaseGVN *phase) {
+Node* PhiNode::try_clean_mem_phi(PhaseGVN *phase) {
   // Incremental inlining + PhaseStringOpts sometimes produce:
   //
   // cmpP with 1 top input
   //           |
   //          If
@@ -390,31 +391,30 @@
   // the Region stays in the graph. The top input from the cmpP is
   // propagated forward and a subgraph that is useful goes away. The
   // code below replaces the Phi with the MergeMem so that the Region
   // is simplified.
 
-  PhiNode* phi = has_unique_phi();
-  if (phi && phi->type() == Type::MEMORY && req() == 3 && phi->is_diamond_phi(true)) {
+  if (type() == Type::MEMORY && is_diamond_phi(true)) {
     MergeMemNode* m = NULL;
-    assert(phi->req() == 3, "same as region");
+    assert(req() == 3, "same as region");
+    Node* r = in(0);
     for (uint i = 1; i < 3; ++i) {
-      Node *mem = phi->in(i);
-      if (mem && mem->is_MergeMem() && in(i)->outcnt() == 1) {
+      Node *mem = in(i);
+      if (mem && mem->is_MergeMem() && r->in(i)->outcnt() == 1) {
         // Nothing is control-dependent on path #i except the region itself.
         m = mem->as_MergeMem();
         uint j = 3 - i;
-        Node* other = phi->in(j);
+        Node* other = in(j);
         if (other && other == m->base_memory()) {
           // m is a successor memory to other, and is not pinned inside the diamond, so push it out.
           // This will allow the diamond to collapse completely.
-          phase->is_IterGVN()->replace_node(phi, m);
-          return true;
+          return m;
         }
       }
     }
   }
-  return false;
+  return NULL;
 }
 
 //------------------------------Ideal------------------------------------------
 // Return a node which is more "ideal" than the current node.  Must preserve
 // the CFG, but we can still strip out dead paths.
@@ -425,12 +425,19 @@
   // Check for RegionNode with no Phi users and both inputs come from either
   // arm of the same IF.  If found, then the control-flow split is useless.
   bool has_phis = false;
   if (can_reshape) {            // Need DU info to check for Phi users
     has_phis = (has_phi() != NULL);       // Cache result
-    if (has_phis && try_clean_mem_phi(phase)) {
-      has_phis = false;
+    if (has_phis) {
+      PhiNode* phi = has_unique_phi();
+      if (phi != NULL) {
+        Node* m = phi->try_clean_mem_phi(phase);
+        if (m != NULL) {
+          phase->is_IterGVN()->replace_node(phi, m);
+          has_phis = false;
+        }
+      }
     }
 
     if (!has_phis) {            // No Phi users?  Nothing merging?
       for (uint i = 1; i < req()-1; i++) {
         Node *if1 = in(i);
@@ -894,11 +901,11 @@
 
 //----------------------------make---------------------------------------------
 // create a new phi with edges matching r and set (initially) to x
 PhiNode* PhiNode::make(Node* r, Node* x, const Type *t, const TypePtr* at) {
   uint preds = r->req();   // Number of predecessor paths
-  assert(t != Type::MEMORY || at == flatten_phi_adr_type(at), "flatten at");
+  assert(t != Type::MEMORY || at == flatten_phi_adr_type(at) || (flatten_phi_adr_type(at) == TypeAryPtr::VALUES && Compile::current()->flattened_accesses_share_alias()), "flatten at");
   PhiNode* p = new PhiNode(r, t, at);
   for (uint j = 1; j < preds; j++) {
     // Fill in all inputs, except those which the region does not yet have
     if (r->in(j) != NULL)
       p->init_req(j, x);
@@ -1104,19 +1111,14 @@
   // convert the one to the other.
   const TypePtr* ttp = _type->make_ptr();
   const TypeInstPtr* ttip = (ttp != NULL) ? ttp->isa_instptr() : NULL;
   const TypeKlassPtr* ttkp = (ttp != NULL) ? ttp->isa_klassptr() : NULL;
   bool is_intf = false;
-  if (ttip != NULL) {
-    ciKlass* k = ttip->klass();
-    if (k->is_loaded() && k->is_interface())
-      is_intf = true;
-  }
-  if (ttkp != NULL) {
-    ciKlass* k = ttkp->klass();
-    if (k->is_loaded() && k->is_interface())
-      is_intf = true;
+  if (ttip != NULL && ttip->is_loaded() && ttip->klass()->is_interface()) {
+    is_intf = true;
+  } else if (ttkp != NULL && ttkp->is_loaded() && ttkp->klass()->is_interface()) {
+    is_intf = true;
   }
 
   // Default case: merge all inputs
   const Type *t = Type::TOP;        // Merged type starting value
   for (uint i = 1; i < req(); ++i) {// For all paths in
@@ -1169,13 +1171,13 @@
     // both implement interface I, but their meet is at 'j/l/O' which
     // doesn't implement I, we have no way to tell if the result should
     // be 'I' or 'j/l/O'.  Thus we'll pick 'j/l/O'.  If this then flows
     // into a Phi which "knows" it's an Interface type we'll have to
     // uplift the type.
-    if (!t->empty() && ttip && ttip->is_loaded() && ttip->klass()->is_interface()) {
+    if (!t->empty() && ttip != NULL && ttip->is_loaded() && ttip->klass()->is_interface()) {
       assert(ft == _type, ""); // Uplift to interface
-    } else if (!t->empty() && ttkp && ttkp->is_loaded() && ttkp->klass()->is_interface()) {
+    } else if (!t->empty() && ttkp != NULL && ttkp->is_loaded() && ttkp->klass()->is_interface()) {
       assert(ft == _type, ""); // Uplift to interface
     } else {
       // We also have to handle 'evil cases' of interface- vs. class-arrays
       Type::get_arrays_base_elements(jt, _type, NULL, &ttip);
       if (!t->empty() && ttip != NULL && ttip->is_loaded() && ttip->klass()->is_interface()) {
@@ -1333,10 +1335,18 @@
   if (true_path != 0) {
     Node* id = is_cmove_id(phase, true_path);
     if (id != NULL)  return id;
   }
 
+  if (phase->is_IterGVN()) {
+    Node* m = try_clean_mem_phi(phase);
+    if (m != NULL) {
+      return m;
+    }
+  }
+
+
   return this;                     // No identity
 }
 
 //-----------------------------unique_input------------------------------------
 // Find the unique value, discounting top, self-loops, and casts.
@@ -1828,10 +1838,28 @@
   // Note: During parsing, phis are often transformed before their regions.
   // This means we have to use type_or_null to defend against untyped regions.
   if( phase->type_or_null(r) == Type::TOP ) // Dead code?
     return NULL;                // No change
 
+  // If all inputs are value types of the same type, push the value type node down
+  // through the phi because value type nodes should be merged through their input values.
+  if (req() > 2 && in(1) != NULL && in(1)->is_ValueTypeBase() && (can_reshape || in(1)->is_ValueType())) {
+    int opcode = in(1)->Opcode();
+    uint i = 2;
+    // Check if inputs are values of the same type
+    for (; i < req() && in(i) && in(i)->is_ValueTypeBase() && in(i)->cmp(*in(1)); i++) {
+      assert(in(i)->Opcode() == opcode, "mixing pointers and values?");
+    }
+    if (i == req()) {
+      ValueTypeBaseNode* vt = in(1)->as_ValueTypeBase()->clone_with_phis(phase, in(0));
+      for (uint i = 2; i < req(); ++i) {
+        vt->merge_with(phase, in(i)->as_ValueTypeBase(), i, i == (req()-1));
+      }
+      return vt;
+    }
+  }
+
   Node *top = phase->C->top();
   bool new_phi = (outcnt() == 0); // transforming new Phi
   // No change for igvn if new phi is not hooked
   if (new_phi && can_reshape)
     return NULL;
@@ -2571,10 +2599,16 @@
   if( phase->type(in(1)) == Type::TOP ) return in(1);
   if( phase->type(in(0)) == Type::TOP ) return in(0);
   // We only come from CatchProj, unless the CatchProj goes away.
   // If the CatchProj is optimized away, then we just carry the
   // exception oop through.
+
+  // CheckCastPPNode::Ideal() for value types reuses the exception
+  // paths of a call to perform an allocation: we can see a Phi here.
+  if (in(1)->is_Phi()) {
+    return this;
+  }
   CallNode *call = in(1)->in(0)->as_Call();
 
   return ( in(0)->is_CatchProj() && in(0)->in(0)->in(1) == in(1) )
     ? this
     : call->in(TypeFunc::Parms);
diff a/src/hotspot/share/opto/classes.hpp b/src/hotspot/share/opto/classes.hpp
--- a/src/hotspot/share/opto/classes.hpp
+++ b/src/hotspot/share/opto/classes.hpp
@@ -61,10 +61,12 @@
 macro(CallRuntime)
 macro(CallStaticJava)
 macro(CastII)
 macro(CastX2P)
 macro(CastP2X)
+macro(CastI2N)
+macro(CastN2I)
 macro(CastPP)
 macro(Catch)
 macro(CatchProj)
 macro(CheckCastPP)
 macro(ClearArray)
@@ -316,10 +318,12 @@
 macro(Unlock)
 macro(URShiftI)
 macro(URShiftL)
 macro(XorI)
 macro(XorL)
+macro(ValueType)
+macro(ValueTypePtr)
 macro(Vector)
 macro(AddVB)
 macro(AddVS)
 macro(AddVI)
 macro(AddReductionVI)
diff a/src/hotspot/share/opto/graphKit.cpp b/src/hotspot/share/opto/graphKit.cpp
--- a/src/hotspot/share/opto/graphKit.cpp
+++ b/src/hotspot/share/opto/graphKit.cpp
@@ -23,10 +23,11 @@
  */
 
 #include "precompiled.hpp"
 #include "ci/ciUtilities.hpp"
 #include "compiler/compileLog.hpp"
+#include "ci/ciValueKlass.hpp"
 #include "gc/shared/barrierSet.hpp"
 #include "gc/shared/c2/barrierSetC2.hpp"
 #include "interpreter/interpreter.hpp"
 #include "memory/resourceArea.hpp"
 #include "opto/addnode.hpp"
@@ -35,31 +36,41 @@
 #include "opto/graphKit.hpp"
 #include "opto/idealKit.hpp"
 #include "opto/intrinsicnode.hpp"
 #include "opto/locknode.hpp"
 #include "opto/machnode.hpp"
+#include "opto/narrowptrnode.hpp"
 #include "opto/opaquenode.hpp"
 #include "opto/parse.hpp"
 #include "opto/rootnode.hpp"
 #include "opto/runtime.hpp"
 #include "opto/subtypenode.hpp"
+#include "opto/valuetypenode.hpp"
 #include "runtime/deoptimization.hpp"
 #include "runtime/sharedRuntime.hpp"
 #include "utilities/bitMap.inline.hpp"
 #include "utilities/powerOfTwo.hpp"
 
 //----------------------------GraphKit-----------------------------------------
 // Main utility constructor.
-GraphKit::GraphKit(JVMState* jvms)
+GraphKit::GraphKit(JVMState* jvms, PhaseGVN* gvn)
   : Phase(Phase::Parser),
     _env(C->env()),
-    _gvn(*C->initial_gvn()),
+    _gvn((gvn != NULL) ? *gvn : *C->initial_gvn()),
     _barrier_set(BarrierSet::barrier_set()->barrier_set_c2())
 {
+  assert(gvn == NULL || !gvn->is_IterGVN() || gvn->is_IterGVN()->delay_transform(), "delay transform should be enabled");
   _exceptions = jvms->map()->next_exception();
   if (_exceptions != NULL)  jvms->map()->set_next_exception(NULL);
   set_jvms(jvms);
+#ifdef ASSERT
+  if (_gvn.is_IterGVN() != NULL) {
+    assert(_gvn.is_IterGVN()->delay_transform(), "Transformation must be delayed if IterGVN is used");
+    // Save the initial size of _for_igvn worklist for verification (see ~GraphKit)
+    _worklist_size = _gvn.C->for_igvn()->size();
+  }
+#endif
 }
 
 // Private constructor for parser.
 GraphKit::GraphKit()
   : Phase(Phase::Parser),
@@ -824,20 +835,21 @@
   ciMethod* cur_method = jvms->method();
   int       cur_bci   = jvms->bci();
   if (cur_method != NULL && cur_bci != InvocationEntryBci) {
     Bytecodes::Code code = cur_method->java_code_at_bci(cur_bci);
     return Interpreter::bytecode_should_reexecute(code) ||
-           (is_anewarray && code == Bytecodes::_multianewarray);
+           (is_anewarray && (code == Bytecodes::_multianewarray));
     // Reexecute _multianewarray bytecode which was replaced with
     // sequence of [a]newarray. See Parse::do_multianewarray().
     //
     // Note: interpreter should not have it set since this optimization
     // is limited by dimensions and guarded by flag so in some cases
     // multianewarray() runtime calls will be generated and
     // the bytecode should not be reexecutes (stack will not be reset).
-  } else
+  } else {
     return false;
+  }
 }
 
 // Helper function for adding JVMState and debug information to node
 void GraphKit::add_safepoint_edges(SafePointNode* call, bool must_throw) {
   // Add the safepoint edges to the call (or other safepoint).
@@ -1077,10 +1089,19 @@
       assert(rsize == 1, "");
       depth = rsize - inputs;
     }
     break;
 
+  case Bytecodes::_withfield: {
+    bool ignored_will_link;
+    ciField* field = method()->get_field_at_bci(bci(), ignored_will_link);
+    int      size  = field->type()->size();
+    inputs = size+1;
+    depth = rsize - inputs;
+    break;
+  }
+
   case Bytecodes::_ireturn:
   case Bytecodes::_lreturn:
   case Bytecodes::_freturn:
   case Bytecodes::_dreturn:
   case Bytecodes::_areturn:
@@ -1159,11 +1180,11 @@
 Node* GraphKit::load_object_klass(Node* obj) {
   // Special-case a fresh allocation to avoid building nodes:
   Node* akls = AllocateNode::Ideal_klass(obj, &_gvn);
   if (akls != NULL)  return akls;
   Node* k_adr = basic_plus_adr(obj, oopDesc::klass_offset_in_bytes());
-  return _gvn.transform(LoadKlassNode::make(_gvn, NULL, immutable_memory(), k_adr, TypeInstPtr::KLASS));
+  return _gvn.transform(LoadKlassNode::make(_gvn, NULL, immutable_memory(), k_adr, TypeInstPtr::KLASS, TypeKlassPtr::OBJECT));
 }
 
 //-------------------------load_array_length-----------------------------------
 Node* GraphKit::load_array_length(Node* array) {
   // Special-case a fresh allocation to avoid building nodes:
@@ -1202,10 +1223,11 @@
   // Construct NULL check
   Node *chk = NULL;
   switch(type) {
     case T_LONG   : chk = new CmpLNode(value, _gvn.zerocon(T_LONG)); break;
     case T_INT    : chk = new CmpINode(value, _gvn.intcon(0)); break;
+    case T_VALUETYPE : // fall through
     case T_ARRAY  : // fall through
       type = T_OBJECT;  // simplify further tests
     case T_OBJECT : {
       const Type *t = _gvn.type( value );
 
@@ -1373,14 +1395,32 @@
   }
 
   return value;
 }
 
+Node* GraphKit::null2default(Node* value, ciValueKlass* vk) {
+  Node* null_ctl = top();
+  value = null_check_oop(value, &null_ctl);
+  if (!null_ctl->is_top()) {
+    // Return default value if oop is null
+    Node* region = new RegionNode(3);
+    region->init_req(1, control());
+    region->init_req(2, null_ctl);
+    value = PhiNode::make(region, value, TypeInstPtr::make(TypePtr::BotPTR, vk));
+    value->set_req(2, ValueTypeNode::default_oop(gvn(), vk));
+    set_control(gvn().transform(region));
+    value = gvn().transform(value);
+  }
+  return value;
+}
 
 //------------------------------cast_not_null----------------------------------
 // Cast obj to not-null on this path
 Node* GraphKit::cast_not_null(Node* obj, bool do_replace_in_map) {
+  if (obj->is_ValueType()) {
+    return obj;
+  }
   const Type *t = _gvn.type(obj);
   const Type *t_not_null = t->join_speculative(TypePtr::NOTNULL);
   // Object is already not-null?
   if( t == t_not_null ) return obj;
 
@@ -1509,11 +1549,12 @@
     ld = LoadDNode::make_atomic(ctl, mem, adr, adr_type, t, mo, control_dependency, unaligned, mismatched, unsafe, barrier_data);
   } else {
     ld = LoadNode::make(_gvn, ctl, mem, adr, adr_type, t, bt, mo, control_dependency, unaligned, mismatched, unsafe, barrier_data);
   }
   ld = _gvn.transform(ld);
-  if (((bt == T_OBJECT) && C->do_escape_analysis()) || C->eliminate_boxing()) {
+
+  if (((bt == T_OBJECT || bt == T_VALUETYPE) && C->do_escape_analysis()) || C->eliminate_boxing()) {
     // Improve graph before escape analysis and boxing elimination.
     record_for_igvn(ld);
   }
   return ld;
 }
@@ -1560,11 +1601,12 @@
                                 Node* adr,
                                 const TypePtr* adr_type,
                                 Node* val,
                                 const Type* val_type,
                                 BasicType bt,
-                                DecoratorSet decorators) {
+                                DecoratorSet decorators,
+                                bool safe_for_replace) {
   // Transformation of a value which could be NULL pointer (CastPP #NULL)
   // could be delayed during Parse (for example, in adjust_map_after_if()).
   // Execute transformation here to avoid barrier generation in such case.
   if (_gvn.type(val) == TypePtr::NULL_PTR) {
     val = _gvn.makecon(TypePtr::NULL_PTR);
@@ -1573,10 +1615,17 @@
   if (stopped()) {
     return top(); // Dead path ?
   }
 
   assert(val != NULL, "not dead path");
+  if (val->is_ValueType()) {
+    // Store to non-flattened field. Buffer the inline type and make sure
+    // the store is re-executed if the allocation triggers deoptimization.
+    PreserveReexecuteState preexecs(this);
+    jvms()->set_should_reexecute(true);
+    val = val->as_ValueType()->allocate(this, safe_for_replace)->get_oop();
+  }
 
   C2AccessValuePtr addr(adr, adr_type);
   C2AccessValue value(val, val_type);
   C2ParseAccess access(this, decorators | C2_WRITE_ACCESS, bt, obj, addr);
   if (access.is_raw()) {
@@ -1589,17 +1638,18 @@
 Node* GraphKit::access_load_at(Node* obj,   // containing obj
                                Node* adr,   // actual adress to store val at
                                const TypePtr* adr_type,
                                const Type* val_type,
                                BasicType bt,
-                               DecoratorSet decorators) {
+                               DecoratorSet decorators,
+                               Node* ctl) {
   if (stopped()) {
     return top(); // Dead path ?
   }
 
   C2AccessValuePtr addr(adr, adr_type);
-  C2ParseAccess access(this, decorators | C2_READ_ACCESS, bt, obj, addr);
+  C2ParseAccess access(this, decorators | C2_READ_ACCESS, bt, obj, addr, ctl);
   if (access.is_raw()) {
     return _barrier_set->BarrierSetC2::load_at(access, val_type);
   } else {
     return _barrier_set->load_at(access, val_type);
   }
@@ -1693,18 +1743,23 @@
   } else {
     return _barrier_set->atomic_add_at(access, new_val, value_type);
   }
 }
 
-void GraphKit::access_clone(Node* src, Node* dst, Node* size, bool is_array) {
-  return _barrier_set->clone(this, src, dst, size, is_array);
+void GraphKit::access_clone(Node* src_base, Node* dst_base, Node* countx, bool is_array) {
+  return _barrier_set->clone(this, src_base, dst_base, countx, is_array);
 }
 
 //-------------------------array_element_address-------------------------
 Node* GraphKit::array_element_address(Node* ary, Node* idx, BasicType elembt,
                                       const TypeInt* sizetype, Node* ctrl) {
   uint shift  = exact_log2(type2aelembytes(elembt));
+  ciKlass* arytype_klass = _gvn.type(ary)->is_aryptr()->klass();
+  if (arytype_klass != NULL && arytype_klass->is_value_array_klass()) {
+    ciValueArrayKlass* vak = arytype_klass->as_value_array_klass();
+    shift = vak->log2_element_size();
+  }
   uint header = arrayOopDesc::base_offset_in_bytes(elembt);
 
   // short-circuit a common case (saves lots of confusing waste motion)
   jint idx_con = find_int_con(idx, -1);
   if (idx_con >= 0) {
@@ -1721,26 +1776,62 @@
 
 //-------------------------load_array_element-------------------------
 Node* GraphKit::load_array_element(Node* ctl, Node* ary, Node* idx, const TypeAryPtr* arytype) {
   const Type* elemtype = arytype->elem();
   BasicType elembt = elemtype->array_element_basic_type();
+  assert(elembt != T_VALUETYPE, "value types are not supported by this method");
   Node* adr = array_element_address(ary, idx, elembt, arytype->size());
   if (elembt == T_NARROWOOP) {
     elembt = T_OBJECT; // To satisfy switch in LoadNode::make()
   }
   Node* ld = make_load(ctl, adr, elemtype, elembt, arytype, MemNode::unordered);
   return ld;
 }
 
 //-------------------------set_arguments_for_java_call-------------------------
 // Arguments (pre-popped from the stack) are taken from the JVMS.
-void GraphKit::set_arguments_for_java_call(CallJavaNode* call) {
-  // Add the call arguments:
-  uint nargs = call->method()->arg_size();
-  for (uint i = 0; i < nargs; i++) {
-    Node* arg = argument(i);
-    call->init_req(i + TypeFunc::Parms, arg);
+void GraphKit::set_arguments_for_java_call(CallJavaNode* call, bool is_late_inline) {
+  PreserveReexecuteState preexecs(this);
+  if (EnableValhalla) {
+    // Make sure the call is re-executed, if buffering of value type arguments triggers deoptimization
+    jvms()->set_should_reexecute(true);
+    int arg_size = method()->get_declared_signature_at_bci(bci())->arg_size_for_bc(java_bc());
+    inc_sp(arg_size);
+  }
+  // Add the call arguments
+  const TypeTuple* domain = call->tf()->domain_sig();
+  ExtendedSignature sig_cc = ExtendedSignature(call->method()->get_sig_cc(), SigEntryFilter());
+  uint nargs = domain->cnt();
+  for (uint i = TypeFunc::Parms, idx = TypeFunc::Parms; i < nargs; i++) {
+    Node* arg = argument(i-TypeFunc::Parms);
+    const Type* t = domain->field_at(i);
+    if (call->method()->has_scalarized_args() && t->is_valuetypeptr() && !t->maybe_null()) {
+      // We don't pass value type arguments by reference but instead pass each field of the value type
+      ValueTypeNode* vt = arg->as_ValueType();
+      vt->pass_fields(this, call, sig_cc, idx);
+      // If a value type argument is passed as fields, attach the Method* to the call site
+      // to be able to access the extended signature later via attached_method_before_pc().
+      // For example, see CompiledMethod::preserve_callee_argument_oops().
+      call->set_override_symbolic_info(true);
+      continue;
+    } else if (arg->is_ValueType()) {
+      // Pass value type argument via oop to callee
+      if (is_late_inline) {
+        arg = ValueTypePtrNode::make_from_value_type(this, arg->as_ValueType());
+      } else {
+        arg = arg->as_ValueType()->allocate(this)->get_oop();
+      }
+    }
+    call->init_req(idx++, arg);
+    // Skip reserved arguments
+    BasicType bt = t->basic_type();
+    while (SigEntry::next_is_reserved(sig_cc, bt, true)) {
+      call->init_req(idx++, top());
+      if (type2size[bt] == 2) {
+        call->init_req(idx++, top());
+      }
+    }
   }
 }
 
 //---------------------------set_edges_for_java_call---------------------------
 // Connect a newly created call into the current JVMS.
@@ -1774,17 +1865,10 @@
 }
 
 Node* GraphKit::set_results_for_java_call(CallJavaNode* call, bool separate_io_proj, bool deoptimize) {
   if (stopped())  return top();  // maybe the call folded up?
 
-  // Capture the return value, if any.
-  Node* ret;
-  if (call->method() == NULL ||
-      call->method()->return_type()->basic_type() == T_VOID)
-        ret = top();
-  else  ret = _gvn.transform(new ProjNode(call, TypeFunc::Parms));
-
   // Note:  Since any out-of-line call can produce an exception,
   // we always insert an I_O projection from the call into the result.
 
   make_slow_call_ex(call, env()->Throwable_klass(), separate_io_proj, deoptimize);
 
@@ -1793,10 +1877,29 @@
     // through and exceptional paths, so replace the projections for
     // the fall through path.
     set_i_o(_gvn.transform( new ProjNode(call, TypeFunc::I_O) ));
     set_all_memory(_gvn.transform( new ProjNode(call, TypeFunc::Memory) ));
   }
+
+  // Capture the return value, if any.
+  Node* ret;
+  if (call->method() == NULL || call->method()->return_type()->basic_type() == T_VOID) {
+    ret = top();
+  } else if (call->tf()->returns_value_type_as_fields()) {
+    // Return of multiple values (value type fields): we create a
+    // ValueType node, each field is a projection from the call.
+    ciValueKlass* vk = call->method()->return_type()->as_value_klass();
+    const Array<SigEntry>* sig_array = vk->extended_sig();
+    GrowableArray<SigEntry> sig = GrowableArray<SigEntry>(sig_array->length());
+    sig.appendAll(sig_array);
+    ExtendedSignature sig_cc = ExtendedSignature(&sig, SigEntryFilter());
+    uint base_input = TypeFunc::Parms + 1;
+    ret = ValueTypeNode::make_from_multi(this, call, sig_cc, vk, base_input, false);
+  } else {
+    ret = _gvn.transform(new ProjNode(call, TypeFunc::Parms));
+  }
+
   return ret;
 }
 
 //--------------------set_predefined_input_for_runtime_call--------------------
 // Reading and setting the memory state is way conservative here.
@@ -1883,80 +1986,80 @@
   Node* ex_ctl = top();
 
   SafePointNode* final_state = stop();
 
   // Find all the needed outputs of this call
-  CallProjections callprojs;
-  call->extract_projections(&callprojs, true);
+  CallProjections* callprojs = call->extract_projections(true);
 
   Unique_Node_List wl;
   Node* init_mem = call->in(TypeFunc::Memory);
   Node* final_mem = final_state->in(TypeFunc::Memory);
   Node* final_ctl = final_state->in(TypeFunc::Control);
   Node* final_io = final_state->in(TypeFunc::I_O);
 
   // Replace all the old call edges with the edges from the inlining result
-  if (callprojs.fallthrough_catchproj != NULL) {
-    C->gvn_replace_by(callprojs.fallthrough_catchproj, final_ctl);
+  if (callprojs->fallthrough_catchproj != NULL) {
+    C->gvn_replace_by(callprojs->fallthrough_catchproj, final_ctl);
   }
-  if (callprojs.fallthrough_memproj != NULL) {
+  if (callprojs->fallthrough_memproj != NULL) {
     if (final_mem->is_MergeMem()) {
       // Parser's exits MergeMem was not transformed but may be optimized
       final_mem = _gvn.transform(final_mem);
     }
-    C->gvn_replace_by(callprojs.fallthrough_memproj,   final_mem);
+    C->gvn_replace_by(callprojs->fallthrough_memproj,   final_mem);
     add_mergemem_users_to_worklist(wl, final_mem);
   }
-  if (callprojs.fallthrough_ioproj != NULL) {
-    C->gvn_replace_by(callprojs.fallthrough_ioproj,    final_io);
+  if (callprojs->fallthrough_ioproj != NULL) {
+    C->gvn_replace_by(callprojs->fallthrough_ioproj,    final_io);
   }
 
   // Replace the result with the new result if it exists and is used
-  if (callprojs.resproj != NULL && result != NULL) {
-    C->gvn_replace_by(callprojs.resproj, result);
+  if (callprojs->resproj[0] != NULL && result != NULL) {
+    assert(callprojs->nb_resproj == 1, "unexpected number of results");
+    C->gvn_replace_by(callprojs->resproj[0], result);
   }
 
   if (ejvms == NULL) {
     // No exception edges to simply kill off those paths
-    if (callprojs.catchall_catchproj != NULL) {
-      C->gvn_replace_by(callprojs.catchall_catchproj, C->top());
+    if (callprojs->catchall_catchproj != NULL) {
+      C->gvn_replace_by(callprojs->catchall_catchproj, C->top());
     }
-    if (callprojs.catchall_memproj != NULL) {
-      C->gvn_replace_by(callprojs.catchall_memproj,   C->top());
+    if (callprojs->catchall_memproj != NULL) {
+      C->gvn_replace_by(callprojs->catchall_memproj,   C->top());
     }
-    if (callprojs.catchall_ioproj != NULL) {
-      C->gvn_replace_by(callprojs.catchall_ioproj,    C->top());
+    if (callprojs->catchall_ioproj != NULL) {
+      C->gvn_replace_by(callprojs->catchall_ioproj,    C->top());
     }
     // Replace the old exception object with top
-    if (callprojs.exobj != NULL) {
-      C->gvn_replace_by(callprojs.exobj, C->top());
+    if (callprojs->exobj != NULL) {
+      C->gvn_replace_by(callprojs->exobj, C->top());
     }
   } else {
     GraphKit ekit(ejvms);
 
     // Load my combined exception state into the kit, with all phis transformed:
     SafePointNode* ex_map = ekit.combine_and_pop_all_exception_states();
     replaced_nodes_exception = ex_map->replaced_nodes();
 
     Node* ex_oop = ekit.use_exception_state(ex_map);
 
-    if (callprojs.catchall_catchproj != NULL) {
-      C->gvn_replace_by(callprojs.catchall_catchproj, ekit.control());
+    if (callprojs->catchall_catchproj != NULL) {
+      C->gvn_replace_by(callprojs->catchall_catchproj, ekit.control());
       ex_ctl = ekit.control();
     }
-    if (callprojs.catchall_memproj != NULL) {
+    if (callprojs->catchall_memproj != NULL) {
       Node* ex_mem = ekit.reset_memory();
-      C->gvn_replace_by(callprojs.catchall_memproj,   ex_mem);
+      C->gvn_replace_by(callprojs->catchall_memproj,   ex_mem);
       add_mergemem_users_to_worklist(wl, ex_mem);
     }
-    if (callprojs.catchall_ioproj != NULL) {
-      C->gvn_replace_by(callprojs.catchall_ioproj,    ekit.i_o());
+    if (callprojs->catchall_ioproj != NULL) {
+      C->gvn_replace_by(callprojs->catchall_ioproj,    ekit.i_o());
     }
 
     // Replace the old exception object with the newly created one
-    if (callprojs.exobj != NULL) {
-      C->gvn_replace_by(callprojs.exobj, ex_oop);
+    if (callprojs->exobj != NULL) {
+      C->gvn_replace_by(callprojs->exobj, ex_oop);
     }
   }
 
   // Disconnect the call from the graph
   call->disconnect_inputs(NULL, C);
@@ -1966,11 +2069,11 @@
   // optimizer doesn't like that.
   while (wl.size() > 0) {
     _gvn.transform(wl.pop());
   }
 
-  if (callprojs.fallthrough_catchproj != NULL && !final_ctl->is_top() && do_replaced_nodes) {
+  if (callprojs->fallthrough_catchproj != NULL && !final_ctl->is_top() && do_replaced_nodes) {
     replaced_nodes.apply(C, final_ctl);
   }
   if (!ex_ctl->is_top() && do_replaced_nodes) {
     replaced_nodes_exception.apply(C, ex_ctl);
   }
@@ -2186,11 +2289,11 @@
   }
 
   if (speculative != current_type->speculative()) {
     // Build a type with a speculative type (what we think we know
     // about the type but will need a guard when we use it)
-    const TypeOopPtr* spec_type = TypeOopPtr::make(TypePtr::BotPTR, Type::OffsetBot, TypeOopPtr::InstanceBot, speculative);
+    const TypeOopPtr* spec_type = TypeOopPtr::make(TypePtr::BotPTR, Type::Offset::bottom, TypeOopPtr::InstanceBot, speculative);
     // We're changing the type, we need a new CheckCast node to carry
     // the new type. The new type depends on the control: what
     // profiling tells us is only valid from here as far as we can
     // tell.
     Node* cast = new CheckCastPPNode(control(), n, current_type->remove_speculative()->join_speculative(spec_type));
@@ -2220,23 +2323,34 @@
        java_bc() == Bytecodes::_instanceof ||
        java_bc() == Bytecodes::_aastore) &&
       method()->method_data()->is_mature()) {
     ciProfileData* data = method()->method_data()->bci_to_data(bci());
     if (data != NULL) {
-      if (!data->as_BitData()->null_seen()) {
-        ptr_kind = ProfileNeverNull;
+      if (java_bc() == Bytecodes::_aastore) {
+        ciKlass* array_type = NULL;
+        ciKlass* element_type = NULL;
+        ProfilePtrKind element_ptr = ProfileMaybeNull;
+        bool flat_array = true;
+        bool null_free_array = true;
+        method()->array_access_profiled_type(bci(), array_type, element_type, element_ptr, flat_array, null_free_array);
+        exact_kls = element_type;
+        ptr_kind = element_ptr;
       } else {
-        assert(data->is_ReceiverTypeData(), "bad profile data type");
-        ciReceiverTypeData* call = (ciReceiverTypeData*)data->as_ReceiverTypeData();
-        uint i = 0;
-        for (; i < call->row_limit(); i++) {
-          ciKlass* receiver = call->receiver(i);
-          if (receiver != NULL) {
-            break;
+        if (!data->as_BitData()->null_seen()) {
+          ptr_kind = ProfileNeverNull;
+        } else {
+          assert(data->is_ReceiverTypeData(), "bad profile data type");
+          ciReceiverTypeData* call = (ciReceiverTypeData*)data->as_ReceiverTypeData();
+          uint i = 0;
+          for (; i < call->row_limit(); i++) {
+            ciKlass* receiver = call->receiver(i);
+            if (receiver != NULL) {
+              break;
+            }
           }
+          ptr_kind = (i == call->row_limit()) ? ProfileAlwaysNull : ProfileMaybeNull;
         }
-        ptr_kind = (i == call->row_limit()) ? ProfileAlwaysNull : ProfileMaybeNull;
       }
     }
   }
   return record_profile_for_speculation(n, exact_kls, ptr_kind);
 }
@@ -2251,14 +2365,14 @@
 void GraphKit::record_profiled_arguments_for_speculation(ciMethod* dest_method, Bytecodes::Code bc) {
   if (!UseTypeSpeculation) {
     return;
   }
   const TypeFunc* tf    = TypeFunc::make(dest_method);
-  int             nargs = tf->domain()->cnt() - TypeFunc::Parms;
+  int             nargs = tf->domain_sig()->cnt() - TypeFunc::Parms;
   int skip = Bytecodes::has_receiver(bc) ? 1 : 0;
   for (int j = skip, i = 0; j < nargs && i < TypeProfileArgsLimit; j++) {
-    const Type *targ = tf->domain()->field_at(j + TypeFunc::Parms);
+    const Type *targ = tf->domain_sig()->field_at(j + TypeFunc::Parms);
     if (is_reference_type(targ->basic_type())) {
       ProfilePtrKind ptr_kind = ProfileMaybeNull;
       ciKlass* better_type = NULL;
       if (method()->argument_profiled_type(bci(), i, better_type, ptr_kind)) {
         record_profile_for_speculation(argument(j), better_type, ptr_kind);
@@ -2325,13 +2439,13 @@
 
 void GraphKit::round_double_arguments(ciMethod* dest_method) {
   if (Matcher::strict_fp_requires_explicit_rounding) {
     // (Note:  TypeFunc::make has a cache that makes this fast.)
     const TypeFunc* tf    = TypeFunc::make(dest_method);
-    int             nargs = tf->domain()->cnt() - TypeFunc::Parms;
+    int             nargs = tf->domain_sig()->cnt() - TypeFunc::Parms;
     for (int j = 0; j < nargs; j++) {
-      const Type *targ = tf->domain()->field_at(j + TypeFunc::Parms);
+      const Type *targ = tf->domain_sig()->field_at(j + TypeFunc::Parms);
       if (targ->basic_type() == T_DOUBLE) {
         // If any parameters are doubles, they must be rounded before
         // the call, dstore_rounding does gvn.transform
         Node *arg = argument(j);
         arg = dstore_rounding(arg);
@@ -2807,24 +2921,26 @@
   *ctrl = gvn.transform(r_ok_subtype);
   return gvn.transform(r_not_subtype);
 }
 
 Node* GraphKit::gen_subtype_check(Node* obj_or_subklass, Node* superklass) {
+  const Type* sub_t = _gvn.type(obj_or_subklass);
+  if (sub_t->isa_valuetype()) {
+    obj_or_subklass = makecon(TypeKlassPtr::make(sub_t->value_klass()));
+  }
   if (ExpandSubTypeCheckAtParseTime) {
     MergeMemNode* mem = merged_memory();
     Node* ctrl = control();
     Node* subklass = obj_or_subklass;
-    if (!_gvn.type(obj_or_subklass)->isa_klassptr()) {
+    if (!sub_t->isa_klassptr()) {
       subklass = load_object_klass(obj_or_subklass);
     }
-
     Node* n = Phase::gen_subtype_check(subklass, superklass, &ctrl, mem, _gvn);
     set_control(ctrl);
     return n;
   }
 
-  const TypePtr* adr_type = TypeKlassPtr::make(TypePtr::NotNull, C->env()->Object_klass(), Type::OffsetBot);
   Node* check = _gvn.transform(new SubTypeCheckNode(C, obj_or_subklass, superklass));
   Node* bol = _gvn.transform(new BoolNode(check, BoolTest::eq));
   IfNode* iff = create_and_xform_if(control(), bol, PROB_STATIC_FREQUENT, COUNT_UNKNOWN);
   set_control(_gvn.transform(new IfTrueNode(iff)));
   return _gvn.transform(new IfFalseNode(iff));
@@ -2834,29 +2950,40 @@
 Node* GraphKit::type_check_receiver(Node* receiver, ciKlass* klass,
                                     float prob,
                                     Node* *casted_receiver) {
   const TypeKlassPtr* tklass = TypeKlassPtr::make(klass);
   Node* recv_klass = load_object_klass(receiver);
-  Node* want_klass = makecon(tklass);
-  Node* cmp = _gvn.transform( new CmpPNode(recv_klass, want_klass) );
-  Node* bol = _gvn.transform( new BoolNode(cmp, BoolTest::eq) );
-  IfNode* iff = create_and_xform_if(control(), bol, prob, COUNT_UNKNOWN);
-  set_control( _gvn.transform( new IfTrueNode (iff) ));
-  Node* fail = _gvn.transform( new IfFalseNode(iff) );
-
+  Node* fail = type_check(recv_klass, tklass, prob);
   const TypeOopPtr* recv_xtype = tklass->as_instance_type();
   assert(recv_xtype->klass_is_exact(), "");
 
   // Subsume downstream occurrences of receiver with a cast to
   // recv_xtype, since now we know what the type will be.
   Node* cast = new CheckCastPPNode(control(), receiver, recv_xtype);
-  (*casted_receiver) = _gvn.transform(cast);
+  Node* res = _gvn.transform(cast);
+  if (recv_xtype->is_valuetypeptr() && recv_xtype->value_klass()->is_scalarizable()) {
+    assert(!gvn().type(res)->maybe_null(), "receiver should never be null");
+    res = ValueTypeNode::make_from_oop(this, res, recv_xtype->value_klass());
+  }
+
+  (*casted_receiver) = res;
   // (User must make the replace_in_map call.)
 
   return fail;
 }
 
+Node* GraphKit::type_check(Node* recv_klass, const TypeKlassPtr* tklass,
+                           float prob) {
+  Node* want_klass = makecon(tklass);
+  Node* cmp = _gvn.transform( new CmpPNode(recv_klass, want_klass));
+  Node* bol = _gvn.transform( new BoolNode(cmp, BoolTest::eq) );
+  IfNode* iff = create_and_xform_if(control(), bol, prob, COUNT_UNKNOWN);
+  set_control(  _gvn.transform( new IfTrueNode (iff)));
+  Node* fail = _gvn.transform( new IfFalseNode(iff));
+  return fail;
+}
+
 //------------------------------subtype_check_receiver-------------------------
 Node* GraphKit::subtype_check_receiver(Node* receiver, ciKlass* klass,
                                        Node** casted_receiver) {
   const TypeKlassPtr* tklass = TypeKlassPtr::make(klass);
   Node* want_klass = makecon(tklass);
@@ -2892,10 +3019,13 @@
       return true;
     // If the profile has not seen a null, assume it won't happen.
     assert(java_bc() == Bytecodes::_checkcast ||
            java_bc() == Bytecodes::_instanceof ||
            java_bc() == Bytecodes::_aastore, "MDO must collect null_seen bit here");
+    if (java_bc() == Bytecodes::_aastore) {
+      return ((ciArrayLoadStoreData*)data->as_ArrayLoadStoreData())->element()->ptr_kind() == ProfileNeverNull;
+    }
     return !data->as_BitData()->null_seen();
   }
   speculating = false;
   return false;
 }
@@ -2971,11 +3101,24 @@
 
   // (No, this isn't a call, but it's enough like a virtual call
   // to use the same ciMethod accessor to get the profile info...)
   // If we have a speculative type use it instead of profiling (which
   // may not help us)
-  ciKlass* exact_kls = spec_klass == NULL ? profile_has_unique_klass() : spec_klass;
+  ciKlass* exact_kls = spec_klass;
+  if (exact_kls == NULL) {
+    if (java_bc() == Bytecodes::_aastore) {
+      ciKlass* array_type = NULL;
+      ciKlass* element_type = NULL;
+      ProfilePtrKind element_ptr = ProfileMaybeNull;
+      bool flat_array = true;
+      bool null_free_array = true;
+      method()->array_access_profiled_type(bci(), array_type, element_type, element_ptr, flat_array, null_free_array);
+      exact_kls = element_type;
+    } else {
+      exact_kls = profile_has_unique_klass();
+    }
+  }
   if (exact_kls != NULL) {// no cast failures here
     if (require_klass == NULL ||
         C->static_subtype_check(require_klass, exact_kls) == Compile::SSC_always_true) {
       // If we narrow the type to match what the type profile sees or
       // the speculative type, we can then remove the rest of the
@@ -3076,14 +3219,15 @@
     data = method()->method_data()->bci_to_data(bci());
   }
   bool speculative_not_null = false;
   bool never_see_null = (ProfileDynamicTypes  // aggressive use of profile
                          && seems_never_null(obj, data, speculative_not_null));
+  bool is_value = obj->is_ValueType();
 
   // Null check; get casted pointer; set region slot 3
   Node* null_ctl = top();
-  Node* not_null_obj = null_check_oop(obj, &null_ctl, never_see_null, safe_for_replace, speculative_not_null);
+  Node* not_null_obj = is_value ? obj : null_check_oop(obj, &null_ctl, never_see_null, safe_for_replace, speculative_not_null);
 
   // If not_null_obj is dead, only null-path is taken
   if (stopped()) {              // Doing instance-of on a NULL?
     set_control(null_ctl);
     return intcon(0);
@@ -3097,32 +3241,37 @@
     region->del_req(_null_path);
     phi   ->del_req(_null_path);
   }
 
   // Do we know the type check always succeed?
-  bool known_statically = false;
-  if (_gvn.type(superklass)->singleton()) {
-    ciKlass* superk = _gvn.type(superklass)->is_klassptr()->klass();
-    ciKlass* subk = _gvn.type(obj)->is_oopptr()->klass();
-    if (subk != NULL && subk->is_loaded()) {
-      int static_res = C->static_subtype_check(superk, subk);
-      known_statically = (static_res == Compile::SSC_always_true || static_res == Compile::SSC_always_false);
+  if (!is_value) {
+    bool known_statically = false;
+    if (_gvn.type(superklass)->singleton()) {
+      ciKlass* superk = _gvn.type(superklass)->is_klassptr()->klass();
+      ciKlass* subk = _gvn.type(obj)->is_oopptr()->klass();
+      if (subk != NULL && subk->is_loaded()) {
+        int static_res = C->static_subtype_check(superk, subk);
+        known_statically = (static_res == Compile::SSC_always_true || static_res == Compile::SSC_always_false);
+      }
     }
-  }
-
-  if (!known_statically) {
-    const TypeOopPtr* obj_type = _gvn.type(obj)->is_oopptr();
-    // We may not have profiling here or it may not help us. If we
-    // have a speculative type use it to perform an exact cast.
-    ciKlass* spec_obj_type = obj_type->speculative_type();
-    if (spec_obj_type != NULL || (ProfileDynamicTypes && data != NULL)) {
-      Node* cast_obj = maybe_cast_profiled_receiver(not_null_obj, NULL, spec_obj_type, safe_for_replace);
-      if (stopped()) {            // Profile disagrees with this path.
-        set_control(null_ctl);    // Null is the only remaining possibility.
-        return intcon(0);
-      }
-      if (cast_obj != NULL) {
+
+    if (!known_statically) {
+      const TypeOopPtr* obj_type = _gvn.type(obj)->is_oopptr();
+      // We may not have profiling here or it may not help us. If we
+      // have a speculative type use it to perform an exact cast.
+      ciKlass* spec_obj_type = obj_type->speculative_type();
+      if (spec_obj_type != NULL || (ProfileDynamicTypes && data != NULL)) {
+        Node* cast_obj = maybe_cast_profiled_receiver(not_null_obj, NULL, spec_obj_type, safe_for_replace);
+        if (stopped()) {            // Profile disagrees with this path.
+          set_control(null_ctl);    // Null is the only remaining possibility.
+          return intcon(0);
+        }
+        if (cast_obj != NULL &&
+            // A value that's sometimes null is not something we can optimize well
+            !(cast_obj->is_ValueType() && null_ctl != top())) {
+          not_null_obj = cast_obj;
+          is_value = not_null_obj->is_ValueType();
         not_null_obj = cast_obj;
       }
     }
   }
 
@@ -3142,11 +3291,11 @@
   record_for_igvn(region);
 
   // If we know the type check always succeeds then we don't use the
   // profiling data at this bytecode. Don't lose it, feed it to the
   // type system as a speculative type.
-  if (safe_for_replace) {
+  if (safe_for_replace && !is_value) {
     Node* casted_obj = record_profiled_receiver_for_speculation(obj);
     replace_in_map(obj, casted_obj);
   }
 
   return _gvn.transform(phi);
@@ -3157,63 +3306,101 @@
 // array store bytecode.  Stack must be as-if BEFORE doing the bytecode so the
 // uncommon-trap paths work.  Adjust stack after this call.
 // If failure_control is supplied and not null, it is filled in with
 // the control edge for the cast failure.  Otherwise, an appropriate
 // uncommon trap or exception is thrown.
-Node* GraphKit::gen_checkcast(Node *obj, Node* superklass,
-                              Node* *failure_control) {
+Node* GraphKit::gen_checkcast(Node *obj, Node* superklass, Node* *failure_control, bool never_null) {
   kill_dead_locals();           // Benefit all the uncommon traps
-  const TypeKlassPtr *tk = _gvn.type(superklass)->is_klassptr();
-  const Type *toop = TypeOopPtr::make_from_klass(tk->klass());
+  const TypeKlassPtr* tk = _gvn.type(superklass)->is_klassptr();
+  const TypeOopPtr* toop = TypeOopPtr::make_from_klass(tk->klass());
+  assert(!never_null || toop->is_valuetypeptr(), "must be a value type pointer");
+  bool is_value = obj->is_ValueType();
 
   // Fast cutout:  Check the case that the cast is vacuously true.
   // This detects the common cases where the test will short-circuit
   // away completely.  We do this before we perform the null check,
   // because if the test is going to turn into zero code, we don't
   // want a residual null check left around.  (Causes a slowdown,
   // for example, in some objArray manipulations, such as a[i]=a[j].)
   if (tk->singleton()) {
-    const TypeOopPtr* objtp = _gvn.type(obj)->isa_oopptr();
-    if (objtp != NULL && objtp->klass() != NULL) {
-      switch (C->static_subtype_check(tk->klass(), objtp->klass())) {
+    ciKlass* klass = NULL;
+    if (is_value) {
+      klass = _gvn.type(obj)->value_klass();
+    } else {
+      const TypeOopPtr* objtp = _gvn.type(obj)->isa_oopptr();
+      if (objtp != NULL) {
+        klass = objtp->klass();
+      }
+    }
+    if (klass != NULL) {
+      switch (C->static_subtype_check(tk->klass(), klass)) {
       case Compile::SSC_always_true:
         // If we know the type check always succeed then we don't use
         // the profiling data at this bytecode. Don't lose it, feed it
         // to the type system as a speculative type.
-        return record_profiled_receiver_for_speculation(obj);
+        if (!is_value) {
+          obj = record_profiled_receiver_for_speculation(obj);
+          if (never_null) {
+            obj = null_check(obj);
+          }
+          if (toop->is_valuetypeptr() && toop->value_klass()->is_scalarizable() && !gvn().type(obj)->maybe_null()) {
+            obj = ValueTypeNode::make_from_oop(this, obj, toop->value_klass());
+          }
+        }
+        return obj;
       case Compile::SSC_always_false:
-        // It needs a null check because a null will *pass* the cast check.
-        // A non-null value will always produce an exception.
-        return null_assert(obj);
+        if (is_value || never_null) {
+          if (!is_value) {
+            null_check(obj);
+          }
+          // Value type is never null. Always throw an exception.
+          builtin_throw(Deoptimization::Reason_class_check, makecon(TypeKlassPtr::make(klass)));
+          return top();
+        } else {
+          // It needs a null check because a null will *pass* the cast check.
+          return null_assert(obj);
+        }
       }
     }
   }
 
   ciProfileData* data = NULL;
   bool safe_for_replace = false;
   if (failure_control == NULL) {        // use MDO in regular case only
     assert(java_bc() == Bytecodes::_aastore ||
            java_bc() == Bytecodes::_checkcast,
            "interpreter profiles type checks only for these BCs");
-    data = method()->method_data()->bci_to_data(bci());
+    if (method()->method_data()->is_mature()) {
+      data = method()->method_data()->bci_to_data(bci());
+    }
     safe_for_replace = true;
   }
 
   // Make the merge point
   enum { _obj_path = 1, _null_path, PATH_LIMIT };
   RegionNode* region = new RegionNode(PATH_LIMIT);
   Node*       phi    = new PhiNode(region, toop);
+  _gvn.set_type(region, Type::CONTROL);
+  _gvn.set_type(phi, toop);
+
   C->set_has_split_ifs(true); // Has chance for split-if optimization
 
   // Use null-cast information if it is available
   bool speculative_not_null = false;
   bool never_see_null = ((failure_control == NULL)  // regular case only
                          && seems_never_null(obj, data, speculative_not_null));
 
   // Null check; get casted pointer; set region slot 3
   Node* null_ctl = top();
-  Node* not_null_obj = null_check_oop(obj, &null_ctl, never_see_null, safe_for_replace, speculative_not_null);
+  Node* not_null_obj = NULL;
+  if (is_value) {
+    not_null_obj = obj;
+  } else if (never_null) {
+    not_null_obj = null_check(obj);
+  } else {
+    not_null_obj = null_check_oop(obj, &null_ctl, never_see_null, safe_for_replace, speculative_not_null);
+  }
 
   // If not_null_obj is dead, only null-path is taken
   if (stopped()) {              // Doing instance-of on a NULL?
     set_control(null_ctl);
     return null();
@@ -3227,21 +3414,28 @@
     region->del_req(_null_path);
     phi   ->del_req(_null_path);
   }
 
   Node* cast_obj = NULL;
-  if (tk->klass_is_exact()) {
+  if (!is_value && tk->klass_is_exact()) {
     // The following optimization tries to statically cast the speculative type of the object
     // (for example obtained during profiling) to the type of the superklass and then do a
     // dynamic check that the type of the object is what we expect. To work correctly
     // for checkcast and aastore the type of superklass should be exact.
     const TypeOopPtr* obj_type = _gvn.type(obj)->is_oopptr();
     // We may not have profiling here or it may not help us. If we have
     // a speculative type use it to perform an exact cast.
     ciKlass* spec_obj_type = obj_type->speculative_type();
     if (spec_obj_type != NULL || data != NULL) {
       cast_obj = maybe_cast_profiled_receiver(not_null_obj, tk->klass(), spec_obj_type, safe_for_replace);
+      if (cast_obj != NULL && cast_obj->is_ValueType()) {
+        if (null_ctl != top()) {
+          cast_obj = NULL; // A value that's sometimes null is not something we can optimize well
+        } else {
+          return cast_obj;
+        }
+      }
       if (cast_obj != NULL) {
         if (failure_control != NULL) // failure is now impossible
           (*failure_control) = top();
         // adjust the type of the phi to the exact klass:
         phi->raise_bottom_type(_gvn.type(cast_obj)->meet_speculative(TypePtr::NULL_PTR));
@@ -3249,20 +3443,26 @@
     }
   }
 
   if (cast_obj == NULL) {
     // Generate the subtype check
-    Node* not_subtype_ctrl = gen_subtype_check(not_null_obj, superklass );
+    Node* not_subtype_ctrl = gen_subtype_check(not_null_obj, superklass);
 
     // Plug in success path into the merge
-    cast_obj = _gvn.transform(new CheckCastPPNode(control(), not_null_obj, toop));
+    cast_obj = is_value ? not_null_obj : _gvn.transform(new CheckCastPPNode(control(), not_null_obj, toop));
     // Failure path ends in uncommon trap (or may be dead - failure impossible)
     if (failure_control == NULL) {
       if (not_subtype_ctrl != top()) { // If failure is possible
         PreserveJVMState pjvms(this);
         set_control(not_subtype_ctrl);
-        builtin_throw(Deoptimization::Reason_class_check, load_object_klass(not_null_obj));
+        Node* obj_klass = NULL;
+        if (is_value) {
+          obj_klass = makecon(TypeKlassPtr::make(_gvn.type(not_null_obj)->value_klass()));
+        } else {
+          obj_klass = load_object_klass(not_null_obj);
+        }
+        builtin_throw(Deoptimization::Reason_class_check, obj_klass);
       }
     } else {
       (*failure_control) = not_subtype_ctrl;
     }
   }
@@ -3285,11 +3485,132 @@
 
   // Return final merged results
   set_control( _gvn.transform(region) );
   record_for_igvn(region);
 
-  return record_profiled_receiver_for_speculation(res);
+  bool not_null_free = !toop->can_be_value_type();
+  bool not_flattenable = !ValueArrayFlatten || not_null_free || (toop->is_valuetypeptr() && !toop->value_klass()->flatten_array());
+  if (EnableValhalla && not_flattenable) {
+    // Check if obj has been loaded from an array
+    obj = obj->isa_DecodeN() ? obj->in(1) : obj;
+    Node* array = NULL;
+    if (obj->isa_Load()) {
+      Node* address = obj->in(MemNode::Address);
+      if (address->isa_AddP()) {
+        array = address->as_AddP()->in(AddPNode::Base);
+      }
+    } else if (obj->is_Phi()) {
+      Node* region = obj->in(0);
+      // TODO make this more robust (see JDK-8231346)
+      if (region->req() == 3 && region->in(2) != NULL && region->in(2)->in(0) != NULL) {
+        IfNode* iff = region->in(2)->in(0)->isa_If();
+        if (iff != NULL) {
+          iff->is_non_flattened_array_check(&_gvn, &array);
+        }
+      }
+    }
+    if (array != NULL) {
+      const TypeAryPtr* ary_t = _gvn.type(array)->isa_aryptr();
+      if (ary_t != NULL) {
+        if (!ary_t->is_not_null_free() && not_null_free) {
+          // Casting array element to a non-inline-type, mark array as not null-free.
+          Node* cast = _gvn.transform(new CheckCastPPNode(control(), array, ary_t->cast_to_not_null_free()));
+          replace_in_map(array, cast);
+        } else if (!ary_t->is_not_flat()) {
+          // Casting array element to a non-flattenable type, mark array as not flat.
+          Node* cast = _gvn.transform(new CheckCastPPNode(control(), array, ary_t->cast_to_not_flat()));
+          replace_in_map(array, cast);
+        }
+      }
+    }
+  }
+
+  if (!is_value) {
+    res = record_profiled_receiver_for_speculation(res);
+    if (toop->is_valuetypeptr() && toop->value_klass()->is_scalarizable() && !gvn().type(res)->maybe_null()) {
+      res = ValueTypeNode::make_from_oop(this, res, toop->value_klass());
+    }
+  }
+  return res;
+}
+
+// Check if 'obj' is a value type by checking if it has the always_locked markWord pattern set.
+Node* GraphKit::is_value_type(Node* obj) {
+  Node* mark_addr = basic_plus_adr(obj, oopDesc::mark_offset_in_bytes());
+  Node* mark = make_load(NULL, mark_addr, TypeX_X, TypeX_X->basic_type(), MemNode::unordered);
+  Node* mask = _gvn.MakeConX(markWord::always_locked_pattern);
+  Node* andx = _gvn.transform(new AndXNode(mark, mask));
+  Node* cmp = _gvn.transform(new CmpXNode(andx, mask));
+  return _gvn.transform(new BoolNode(cmp, BoolTest::eq));
+}
+
+// Check if 'ary' is a non-flattened array
+Node* GraphKit::is_non_flattened_array(Node* ary) {
+  Node* kls = load_object_klass(ary);
+  Node* tag = load_lh_array_tag(kls);
+  Node* cmp = gen_lh_array_test(kls, Klass::_lh_array_tag_vt_value);
+  return _gvn.transform(new BoolNode(cmp, BoolTest::ne));
+}
+
+// Check if 'ary' is a nullable array
+Node* GraphKit::is_nullable_array(Node* ary) {
+  Node* kls = load_object_klass(ary);
+  Node* lhp = basic_plus_adr(kls, in_bytes(Klass::layout_helper_offset()));
+  Node* layout_val = _gvn.transform(LoadNode::make(_gvn, NULL, immutable_memory(), lhp, lhp->bottom_type()->is_ptr(), TypeInt::INT, T_INT, MemNode::unordered));
+  Node* null_free = _gvn.transform(new RShiftINode(layout_val, intcon(Klass::_lh_null_free_shift)));
+  null_free = _gvn.transform(new AndINode(null_free, intcon(Klass::_lh_null_free_mask)));
+  Node* cmp = _gvn.transform(new CmpINode(null_free, intcon(0)));
+  return _gvn.transform(new BoolNode(cmp, BoolTest::eq));
+}
+
+// Deoptimize if 'ary' is a null-free value type array and 'val' is null
+Node* GraphKit::gen_value_array_null_guard(Node* ary, Node* val, int nargs, bool safe_for_replace) {
+  const Type* val_t = _gvn.type(val);
+  if (val->is_ValueType() || !TypePtr::NULL_PTR->higher_equal(val_t)) {
+    return ary; // Never null
+  }
+  RegionNode* region = new RegionNode(3);
+  Node* null_ctl = top();
+  null_check_oop(val, &null_ctl);
+  if (null_ctl != top()) {
+    PreserveJVMState pjvms(this);
+    set_control(null_ctl);
+    {
+      // Deoptimize if null-free array
+      BuildCutout unless(this, is_nullable_array(ary), PROB_MAX);
+      inc_sp(nargs);
+      uncommon_trap(Deoptimization::Reason_null_check,
+                    Deoptimization::Action_none);
+    }
+    region->init_req(1, control());
+  }
+  region->init_req(2, control());
+  set_control(_gvn.transform(region));
+  record_for_igvn(region);
+  const TypeAryPtr* ary_t = _gvn.type(ary)->is_aryptr();
+  if (val_t == TypePtr::NULL_PTR && !ary_t->is_not_null_free()) {
+    // Since we were just successfully storing null, the array can't be null free.
+    ary_t = ary_t->cast_to_not_null_free();
+    Node* cast = _gvn.transform(new CheckCastPPNode(control(), ary, ary_t));
+    if (safe_for_replace) {
+      replace_in_map(ary, cast);
+    }
+    ary = cast;
+  }
+  return ary;
+}
+
+Node* GraphKit::load_lh_array_tag(Node* kls) {
+  Node* lhp = basic_plus_adr(kls, in_bytes(Klass::layout_helper_offset()));
+  Node* layout_val = _gvn.transform(LoadNode::make(_gvn, NULL, immutable_memory(), lhp, lhp->bottom_type()->is_ptr(), TypeInt::INT, T_INT, MemNode::unordered));
+  return _gvn.transform(new RShiftINode(layout_val, intcon(Klass::_lh_array_tag_shift)));
+}
+
+Node* GraphKit::gen_lh_array_test(Node* kls, unsigned int lh_value) {
+  Node* layout_val = load_lh_array_tag(kls);
+  Node* cmp = _gvn.transform(new CmpINode(layout_val, intcon(lh_value)));
+  return cmp;
 }
 
 //------------------------------next_monitor-----------------------------------
 // What number should be given to the next monitor?
 int GraphKit::next_monitor() {
@@ -3353,10 +3674,11 @@
   // %%% SynchronizationEntryBCI is redundant; use InvocationEntryBci in interfaces
   assert(SynchronizationEntryBCI == InvocationEntryBci, "");
 
   if( !GenerateSynchronizationCode )
     return NULL;                // Not locking things?
+
   if (stopped())                // Dead monitor?
     return NULL;
 
   assert(dead_locals_are_killed(), "should kill locals before sync. point");
 
@@ -3425,10 +3747,11 @@
     return;
   if (stopped()) {               // Dead monitor?
     map()->pop_monitor();        // Kill monitor from debug info
     return;
   }
+  assert(!obj->is_ValueTypeBase(), "should not unlock on value type");
 
   // Memory barrier to avoid floating things down past the locked region
   insert_mem_bar(Op_MemBarReleaseLock);
 
   const TypeFunc *tf = OptoRuntime::complete_monitor_exit_Type();
@@ -3465,12 +3788,18 @@
 // almost always feature constant types.
 Node* GraphKit::get_layout_helper(Node* klass_node, jint& constant_value) {
   const TypeKlassPtr* inst_klass = _gvn.type(klass_node)->isa_klassptr();
   if (!StressReflectiveCode && inst_klass != NULL) {
     ciKlass* klass = inst_klass->klass();
+    assert(klass != NULL, "klass should not be NULL");
     bool    xklass = inst_klass->klass_is_exact();
-    if (xklass || klass->is_array_klass()) {
+    bool can_be_flattened = false;
+    if (ValueArrayFlatten && klass->is_obj_array_klass()) {
+      ciKlass* elem = klass->as_obj_array_klass()->element_klass();
+      can_be_flattened = elem->can_be_value_klass() && (!elem->is_valuetype() || elem->as_value_klass()->flatten_array());
+    }
+    if (xklass || (klass->is_array_klass() && !can_be_flattened)) {
       jint lhelper = klass->layout_helper();
       if (lhelper != Klass::_lh_neutral_value) {
         constant_value = lhelper;
         return (Node*) NULL;
       }
@@ -3528,21 +3857,46 @@
     // and link them properly (as a group) to the InitializeNode.
     assert(init->in(InitializeNode::Memory) == malloc, "");
     MergeMemNode* minit_in = MergeMemNode::make(malloc);
     init->set_req(InitializeNode::Memory, minit_in);
     record_for_igvn(minit_in); // fold it up later, if possible
+    _gvn.set_type(minit_in, Type::MEMORY);
     Node* minit_out = memory(rawidx);
     assert(minit_out->is_Proj() && minit_out->in(0) == init, "");
     // Add an edge in the MergeMem for the header fields so an access
     // to one of those has correct memory state
     set_memory(minit_out, C->get_alias_index(oop_type->add_offset(oopDesc::mark_offset_in_bytes())));
     set_memory(minit_out, C->get_alias_index(oop_type->add_offset(oopDesc::klass_offset_in_bytes())));
     if (oop_type->isa_aryptr()) {
-      const TypePtr* telemref = oop_type->add_offset(Type::OffsetBot);
-      int            elemidx  = C->get_alias_index(telemref);
-      hook_memory_on_init(*this, elemidx, minit_in, minit_out);
+      const TypeAryPtr* arytype = oop_type->is_aryptr();
+      if (arytype->klass()->is_value_array_klass()) {
+        // Initially all flattened array accesses share a single slice
+        // but that changes after parsing. Prepare the memory graph so
+        // it can optimize flattened array accesses properly once they
+        // don't share a single slice.
+        assert(C->flattened_accesses_share_alias(), "should be set at parse time");
+        C->set_flattened_accesses_share_alias(false);
+        ciValueArrayKlass* vak = arytype->klass()->as_value_array_klass();
+        ciValueKlass* vk = vak->element_klass()->as_value_klass();
+        for (int i = 0, len = vk->nof_nonstatic_fields(); i < len; i++) {
+          ciField* field = vk->nonstatic_field_at(i);
+          if (field->offset() >= TrackedInitializationLimit * HeapWordSize)
+            continue;  // do not bother to track really large numbers of fields
+          int off_in_vt = field->offset() - vk->first_field_offset();
+          const TypePtr* adr_type = arytype->with_field_offset(off_in_vt)->add_offset(Type::OffsetBot);
+          int fieldidx = C->get_alias_index(adr_type, true);
+          hook_memory_on_init(*this, fieldidx, minit_in, minit_out);
+        }
+        C->set_flattened_accesses_share_alias(true);
+        hook_memory_on_init(*this, C->get_alias_index(TypeAryPtr::VALUES), minit_in, minit_out);
+      } else {
+        const TypePtr* telemref = oop_type->add_offset(Type::OffsetBot);
+        int            elemidx  = C->get_alias_index(telemref);
+        hook_memory_on_init(*this, elemidx, minit_in, minit_out);
+      }
     } else if (oop_type->isa_instptr()) {
+      set_memory(minit_out, C->get_alias_index(oop_type)); // mark word
       ciInstanceKlass* ik = oop_type->klass()->as_instance_klass();
       for (int i = 0, len = ik->nof_nonstatic_fields(); i < len; i++) {
         ciField* field = ik->nonstatic_field_at(i);
         if (field->offset() >= TrackedInitializationLimit * HeapWordSize)
           continue;  // do not bother to track really large numbers of fields
@@ -3589,18 +3943,19 @@
 //  - If 'return_size_val', report the the total object size to the caller.
 //  - deoptimize_on_exception controls how Java exceptions are handled (rethrow vs deoptimize)
 Node* GraphKit::new_instance(Node* klass_node,
                              Node* extra_slow_test,
                              Node* *return_size_val,
-                             bool deoptimize_on_exception) {
+                             bool deoptimize_on_exception,
+                             ValueTypeBaseNode* value_node) {
   // Compute size in doublewords
   // The size is always an integral number of doublewords, represented
   // as a positive bytewise size stored in the klass's layout_helper.
   // The layout_helper also encodes (in a low bit) the need for a slow path.
   jint  layout_con = Klass::_lh_neutral_value;
   Node* layout_val = get_layout_helper(klass_node, layout_con);
-  int   layout_is_con = (layout_val == NULL);
+  bool  layout_is_con = (layout_val == NULL);
 
   if (extra_slow_test == NULL)  extra_slow_test = intcon(0);
   // Generate the initial go-slow test.  It's either ALWAYS (return a
   // Node for 1) or NEVER (return a NULL) or perhaps (in the reflective
   // case) a computed value derived from the layout_helper.
@@ -3647,34 +4002,42 @@
   const TypeOopPtr* oop_type = tklass->as_instance_type();
 
   // Now generate allocation code
 
   // The entire memory state is needed for slow path of the allocation
-  // since GC and deoptimization can happened.
+  // since GC and deoptimization can happen.
   Node *mem = reset_memory();
   set_all_memory(mem); // Create new memory state
 
   AllocateNode* alloc = new AllocateNode(C, AllocateNode::alloc_type(Type::TOP),
                                          control(), mem, i_o(),
                                          size, klass_node,
-                                         initial_slow_test);
+                                         initial_slow_test, value_node);
 
   return set_output_for_allocation(alloc, oop_type, deoptimize_on_exception);
 }
 
+// With compressed oops, the 64 bit init value for non flattened value
+// arrays is built from 2 32 bit compressed oops
+static Node* raw_default_for_coops(Node* default_value, GraphKit& kit) {
+  Node* lower = kit.gvn().transform(new CastP2XNode(kit.control(), default_value));
+  Node* upper = kit.gvn().transform(new LShiftLNode(lower, kit.intcon(32)));
+  return kit.gvn().transform(new OrLNode(lower, upper));
+}
+
 //-------------------------------new_array-------------------------------------
-// helper for both newarray and anewarray
+// helper for newarray and anewarray
 // The 'length' parameter is (obviously) the length of the array.
 // See comments on new_instance for the meaning of the other arguments.
 Node* GraphKit::new_array(Node* klass_node,     // array klass (maybe variable)
                           Node* length,         // number of array elements
                           int   nargs,          // number of arguments to push back for uncommon trap
                           Node* *return_size_val,
                           bool deoptimize_on_exception) {
   jint  layout_con = Klass::_lh_neutral_value;
   Node* layout_val = get_layout_helper(klass_node, layout_con);
-  int   layout_is_con = (layout_val == NULL);
+  bool  layout_is_con = (layout_val == NULL);
 
   if (!layout_is_con && !StressReflectiveCode &&
       !too_many_traps(Deoptimization::Reason_class_check)) {
     // This is a reflective array creation site.
     // Optimistically assume that it is a subtype of Object[],
@@ -3700,11 +4063,11 @@
   int fast_size_limit = FastAllocateSizeLimit;
   if (layout_is_con) {
     assert(!StressReflectiveCode, "stress mode does not use these paths");
     // Increase the size limit if we have exact knowledge of array type.
     int log2_esize = Klass::layout_helper_log2_element_size(layout_con);
-    fast_size_limit <<= (LogBytesPerLong - log2_esize);
+    fast_size_limit <<= MAX2(LogBytesPerLong - log2_esize, 0);
   }
 
   Node* initial_slow_cmp  = _gvn.transform( new CmpUNode( length, intcon( fast_size_limit ) ) );
   Node* initial_slow_test = _gvn.transform( new BoolNode( initial_slow_cmp, BoolTest::gt ) );
 
@@ -3718,14 +4081,14 @@
   int   header_size_min  = arrayOopDesc::base_offset_in_bytes(T_BYTE);
   // (T_BYTE has the weakest alignment and size restrictions...)
   if (layout_is_con) {
     int       hsize  = Klass::layout_helper_header_size(layout_con);
     int       eshift = Klass::layout_helper_log2_element_size(layout_con);
-    BasicType etype  = Klass::layout_helper_element_type(layout_con);
+    bool is_value_array = Klass::layout_helper_is_valueArray(layout_con);
     if ((round_mask & ~right_n_bits(eshift)) == 0)
       round_mask = 0;  // strength-reduce it if it goes away completely
-    assert((hsize & right_n_bits(eshift)) == 0, "hsize is pre-rounded");
+    assert(is_value_array || (hsize & right_n_bits(eshift)) == 0, "hsize is pre-rounded");
     assert(header_size_min <= hsize, "generic minimum is smallest");
     header_size_min = hsize;
     header_size = intcon(hsize + round_mask);
   } else {
     Node* hss   = intcon(Klass::_lh_header_size_shift);
@@ -3805,33 +4168,112 @@
   }
 
   // Now generate allocation code
 
   // The entire memory state is needed for slow path of the allocation
-  // since GC and deoptimization can happened.
+  // since GC and deoptimization can happen.
   Node *mem = reset_memory();
   set_all_memory(mem); // Create new memory state
 
   if (initial_slow_test->is_Bool()) {
     // Hide it behind a CMoveI, or else PhaseIdealLoop::split_up will get sick.
     initial_slow_test = initial_slow_test->as_Bool()->as_int_value(&_gvn);
   }
 
+  const TypeKlassPtr* ary_klass = _gvn.type(klass_node)->isa_klassptr();
+  const TypeOopPtr* ary_type = ary_klass->as_instance_type();
+  const TypeAryPtr* ary_ptr = ary_type->isa_aryptr();
+
+  // Value type array variants:
+  // - null-ok:              MyValue.ref[] (ciObjArrayKlass "[LMyValue$ref")
+  // - null-free:            MyValue.val[] (ciObjArrayKlass "[QMyValue$val")
+  // - null-free, flattened: MyValue.val[] (ciValueArrayKlass "[QMyValue$val")
+  // Check if array is a null-free, non-flattened value type array
+  // that needs to be initialized with the default value type.
+  Node* default_value = NULL;
+  Node* raw_default_value = NULL;
+  if (ary_ptr != NULL && ary_ptr->klass_is_exact()) {
+    // Array type is known
+    ciKlass* elem_klass = ary_ptr->klass()->as_array_klass()->element_klass();
+    if (elem_klass != NULL && elem_klass->is_valuetype()) {
+      ciValueKlass* vk = elem_klass->as_value_klass();
+      if (!vk->flatten_array()) {
+        default_value = ValueTypeNode::default_oop(gvn(), vk);
+        if (UseCompressedOops) {
+          default_value = _gvn.transform(new EncodePNode(default_value, default_value->bottom_type()->make_narrowoop()));
+          raw_default_value = raw_default_for_coops(default_value, *this);
+        } else {
+          raw_default_value = _gvn.transform(new CastP2XNode(control(), default_value));
+        }
+      }
+    }
+  } else if (ary_klass->klass()->can_be_value_array_klass()) {
+    // Array type is not known, add runtime checks
+    assert(!ary_klass->klass_is_exact(), "unexpected exact type");
+    Node* r = new RegionNode(4);
+    default_value = new PhiNode(r, TypeInstPtr::BOTTOM);
+
+    // Check if array is an object array
+    Node* cmp = gen_lh_array_test(klass_node, Klass::_lh_array_tag_obj_value);
+    Node* bol = _gvn.transform(new BoolNode(cmp, BoolTest::eq));
+    IfNode* iff = create_and_map_if(control(), bol, PROB_FAIR, COUNT_UNKNOWN);
+
+    // Not an object array, initialize with all zero
+    r->init_req(1, _gvn.transform(new IfFalseNode(iff)));
+    default_value->init_req(1, null());
+
+    // Object array, check if null-free
+    set_control(_gvn.transform(new IfTrueNode(iff)));
+    Node* lhp = basic_plus_adr(klass_node, in_bytes(Klass::layout_helper_offset()));
+    Node* layout_val = _gvn.transform(LoadNode::make(_gvn, NULL, immutable_memory(), lhp, lhp->bottom_type()->is_ptr(), TypeInt::INT, T_INT, MemNode::unordered));
+    Node* null_free = _gvn.transform(new RShiftINode(layout_val, intcon(Klass::_lh_null_free_shift)));
+    null_free = _gvn.transform(new AndINode(null_free, intcon(Klass::_lh_null_free_mask)));
+    cmp = _gvn.transform(new CmpINode(null_free, intcon(0)));
+    bol = _gvn.transform(new BoolNode(cmp, BoolTest::ne));
+    iff = create_and_map_if(control(), bol, PROB_FAIR, COUNT_UNKNOWN);
+
+    // Not null-free, initialize with all zero
+    r->init_req(2, _gvn.transform(new IfFalseNode(iff)));
+    default_value->init_req(2, null());
+
+    // Null-free, non-flattened value array, initialize with the default value
+    set_control(_gvn.transform(new IfTrueNode(iff)));
+    Node* p = basic_plus_adr(klass_node, in_bytes(ArrayKlass::element_klass_offset()));
+    Node* eklass = _gvn.transform(LoadKlassNode::make(_gvn, control(), immutable_memory(), p, TypeInstPtr::KLASS));
+    Node* adr_fixed_block_addr = basic_plus_adr(eklass, in_bytes(InstanceKlass::adr_valueklass_fixed_block_offset()));
+    Node* adr_fixed_block = make_load(control(), adr_fixed_block_addr, TypeRawPtr::NOTNULL, T_ADDRESS, MemNode::unordered);
+    Node* default_value_offset_addr = basic_plus_adr(adr_fixed_block, in_bytes(ValueKlass::default_value_offset_offset()));
+    Node* default_value_offset = make_load(control(), default_value_offset_addr, TypeInt::INT, T_INT, MemNode::unordered);
+    Node* elem_mirror = load_mirror_from_klass(eklass);
+    Node* default_value_addr = basic_plus_adr(elem_mirror, ConvI2X(default_value_offset));
+    Node* val = access_load_at(elem_mirror, default_value_addr, _gvn.type(default_value_addr)->is_ptr(), TypeInstPtr::BOTTOM, T_OBJECT, IN_HEAP);
+    r->init_req(3, control());
+    default_value->init_req(3, val);
+
+    set_control(_gvn.transform(r));
+    default_value = _gvn.transform(default_value);
+    if (UseCompressedOops) {
+      default_value = _gvn.transform(new EncodePNode(default_value, default_value->bottom_type()->make_narrowoop()));
+      raw_default_value = raw_default_for_coops(default_value, *this);
+    } else {
+      raw_default_value = _gvn.transform(new CastP2XNode(control(), default_value));
+    }
+  }
+
   // Create the AllocateArrayNode and its result projections
-  AllocateArrayNode* alloc
-    = new AllocateArrayNode(C, AllocateArrayNode::alloc_type(TypeInt::INT),
-                            control(), mem, i_o(),
-                            size, klass_node,
-                            initial_slow_test,
-                            length);
+  AllocateArrayNode* alloc = new AllocateArrayNode(C, AllocateArrayNode::alloc_type(TypeInt::INT),
+                                                   control(), mem, i_o(),
+                                                   size, klass_node,
+                                                   initial_slow_test,
+                                                   length, default_value,
+                                                   raw_default_value);
 
   // Cast to correct type.  Note that the klass_node may be constant or not,
   // and in the latter case the actual array type will be inexact also.
   // (This happens via a non-constant argument to inline_native_newArray.)
   // In any case, the value of klass_node provides the desired array type.
   const TypeInt* length_type = _gvn.find_int_type(length);
-  const TypeOopPtr* ary_type = _gvn.type(klass_node)->is_klassptr()->as_instance_type();
   if (ary_type->isa_aryptr() && length_type != NULL) {
     // Try to get a better type than POS for the size
     ary_type = ary_type->is_aryptr()->cast_to_size(length_type);
   }
 
@@ -3985,15 +4427,15 @@
 }
 
 Node* GraphKit::load_String_value(Node* str, bool set_ctrl) {
   int value_offset = java_lang_String::value_offset_in_bytes();
   const TypeInstPtr* string_type = TypeInstPtr::make(TypePtr::NotNull, C->env()->String_klass(),
-                                                     false, NULL, 0);
+                                                     false, NULL, Type::Offset(0), false);
   const TypePtr* value_field_type = string_type->add_offset(value_offset);
   const TypeAryPtr* value_type = TypeAryPtr::make(TypePtr::NotNull,
-                                                  TypeAry::make(TypeInt::BYTE, TypeInt::POS),
-                                                  ciTypeArrayKlass::make(T_BYTE), true, 0);
+                                                  TypeAry::make(TypeInt::BYTE, TypeInt::POS, false, true, true),
+                                                  ciTypeArrayKlass::make(T_BYTE), true, Type::Offset(0));
   Node* p = basic_plus_adr(str, str, value_offset);
   Node* load = access_load_at(str, p, value_field_type, value_type, T_OBJECT,
                               IN_HEAP | (set_ctrl ? C2_CONTROL_DEPENDENT_LOAD : 0) | MO_UNORDERED);
   return load;
 }
@@ -4002,11 +4444,11 @@
   if (!CompactStrings) {
     return intcon(java_lang_String::CODER_UTF16);
   }
   int coder_offset = java_lang_String::coder_offset_in_bytes();
   const TypeInstPtr* string_type = TypeInstPtr::make(TypePtr::NotNull, C->env()->String_klass(),
-                                                     false, NULL, 0);
+                                                     false, NULL, Type::Offset(0), false);
   const TypePtr* coder_field_type = string_type->add_offset(coder_offset);
 
   Node* p = basic_plus_adr(str, str, coder_offset);
   Node* load = access_load_at(str, p, coder_field_type, TypeInt::BYTE, T_BYTE,
                               IN_HEAP | (set_ctrl ? C2_CONTROL_DEPENDENT_LOAD : 0) | MO_UNORDERED);
@@ -4014,21 +4456,21 @@
 }
 
 void GraphKit::store_String_value(Node* str, Node* value) {
   int value_offset = java_lang_String::value_offset_in_bytes();
   const TypeInstPtr* string_type = TypeInstPtr::make(TypePtr::NotNull, C->env()->String_klass(),
-                                                     false, NULL, 0);
+                                                     false, NULL, Type::Offset(0), false);
   const TypePtr* value_field_type = string_type->add_offset(value_offset);
 
   access_store_at(str,  basic_plus_adr(str, value_offset), value_field_type,
                   value, TypeAryPtr::BYTES, T_OBJECT, IN_HEAP | MO_UNORDERED);
 }
 
 void GraphKit::store_String_coder(Node* str, Node* value) {
   int coder_offset = java_lang_String::coder_offset_in_bytes();
   const TypeInstPtr* string_type = TypeInstPtr::make(TypePtr::NotNull, C->env()->String_klass(),
-                                                     false, NULL, 0);
+                                                     false, NULL, Type::Offset(0), false);
   const TypePtr* coder_field_type = string_type->add_offset(coder_offset);
 
   access_store_at(str, basic_plus_adr(str, coder_offset), coder_field_type,
                   value, TypeInt::BYTE, T_BYTE, IN_HEAP | MO_UNORDERED);
 }
@@ -4135,9 +4577,23 @@
     }
   }
   const Type* con_type = Type::make_constant_from_field(field, holder, field->layout_type(),
                                                         /*is_unsigned_load=*/false);
   if (con_type != NULL) {
-    return makecon(con_type);
+    Node* con = makecon(con_type);
+    if (field->layout_type() == T_VALUETYPE && field->type()->as_value_klass()->is_scalarizable() && !con_type->maybe_null()) {
+      // Load value type from constant oop
+      con = ValueTypeNode::make_from_oop(this, con, field->type()->as_value_klass());
+    }
+    return con;
   }
   return NULL;
 }
+
+//---------------------------load_mirror_from_klass----------------------------
+// Given a klass oop, load its java mirror (a java.lang.Class oop).
+Node* GraphKit::load_mirror_from_klass(Node* klass) {
+  Node* p = basic_plus_adr(klass, in_bytes(Klass::java_mirror_offset()));
+  Node* load = make_load(NULL, p, TypeRawPtr::NOTNULL, T_ADDRESS, MemNode::unordered);
+  // mirror = ((OopHandle)mirror)->resolve();
+  return access_load(load, TypeInstPtr::MIRROR, T_OBJECT, IN_NATIVE);
+}
diff a/src/hotspot/share/opto/library_call.cpp b/src/hotspot/share/opto/library_call.cpp
--- a/src/hotspot/share/opto/library_call.cpp
+++ b/src/hotspot/share/opto/library_call.cpp
@@ -51,10 +51,11 @@
 #include "opto/opaquenode.hpp"
 #include "opto/parse.hpp"
 #include "opto/runtime.hpp"
 #include "opto/rootnode.hpp"
 #include "opto/subnode.hpp"
+#include "opto/valuetypenode.hpp"
 #include "prims/nativeLookup.hpp"
 #include "prims/unsafe.hpp"
 #include "runtime/objectMonitor.hpp"
 #include "runtime/sharedRuntime.hpp"
 #include "utilities/macros.hpp"
@@ -132,13 +133,21 @@
   bool  try_to_inline(int predicate);
   Node* try_to_predicate(int predicate);
 
   void push_result() {
     // Push the result onto the stack.
-    if (!stopped() && result() != NULL) {
-      BasicType bt = result()->bottom_type()->basic_type();
-      push_node(bt, result());
+    Node* res = result();
+    if (!stopped() && res != NULL) {
+      BasicType bt = res->bottom_type()->basic_type();
+      if (C->inlining_incrementally() && res->is_ValueType()) {
+        // The caller expects an oop when incrementally inlining an intrinsic that returns an
+        // inline type. Make sure the call is re-executed if the allocation triggers a deoptimization.
+        PreserveReexecuteState preexecs(this);
+        jvms()->set_should_reexecute(true);
+        res = ValueTypePtrNode::make_from_value_type(this, res->as_ValueType());
+      }
+      push_node(bt, res);
     }
   }
 
  private:
   void fatal_unexpected_iid(vmIntrinsics::ID iid) {
@@ -162,11 +171,10 @@
                              Node* array_length,
                              RegionNode* region);
   void  generate_string_range_check(Node* array, Node* offset,
                                     Node* length, bool char_count);
   Node* generate_current_thread(Node* &tls_output);
-  Node* load_mirror_from_klass(Node* klass);
   Node* load_klass_from_mirror_common(Node* mirror, bool never_see_null,
                                       RegionNode* region, int null_path,
                                       int offset);
   Node* load_klass_from_mirror(Node* mirror, bool never_see_null,
                                RegionNode* region, int null_path) {
@@ -184,25 +192,43 @@
   }
   Node* generate_access_flags_guard(Node* kls,
                                     int modifier_mask, int modifier_bits,
                                     RegionNode* region);
   Node* generate_interface_guard(Node* kls, RegionNode* region);
+  Node* generate_value_guard(Node* kls, RegionNode* region);
+
+  enum ArrayKind {
+    AnyArray,
+    NonArray,
+    ObjectArray,
+    NonObjectArray,
+    TypeArray,
+    ValueArray
+  };
+
   Node* generate_hidden_class_guard(Node* kls, RegionNode* region);
+
   Node* generate_array_guard(Node* kls, RegionNode* region) {
-    return generate_array_guard_common(kls, region, false, false);
+    return generate_array_guard_common(kls, region, AnyArray);
   }
   Node* generate_non_array_guard(Node* kls, RegionNode* region) {
-    return generate_array_guard_common(kls, region, false, true);
+    return generate_array_guard_common(kls, region, NonArray);
   }
   Node* generate_objArray_guard(Node* kls, RegionNode* region) {
-    return generate_array_guard_common(kls, region, true, false);
+    return generate_array_guard_common(kls, region, ObjectArray);
   }
   Node* generate_non_objArray_guard(Node* kls, RegionNode* region) {
-    return generate_array_guard_common(kls, region, true, true);
+    return generate_array_guard_common(kls, region, NonObjectArray);
+  }
+  Node* generate_typeArray_guard(Node* kls, RegionNode* region) {
+    return generate_array_guard_common(kls, region, TypeArray);
+  }
+  Node* generate_valueArray_guard(Node* kls, RegionNode* region) {
+    assert(ValueArrayFlatten, "can never be flattened");
+    return generate_array_guard_common(kls, region, ValueArray);
   }
-  Node* generate_array_guard_common(Node* kls, RegionNode* region,
-                                    bool obj_array, bool not_array);
+  Node* generate_array_guard_common(Node* kls, RegionNode* region, ArrayKind kind);
   Node* generate_virtual_guard(Node* obj_klass, RegionNode* slow_region);
   CallJavaNode* generate_method_call(vmIntrinsics::ID method_id,
                                      bool is_virtual = false, bool is_static = false);
   CallJavaNode* generate_method_call_static(vmIntrinsics::ID method_id) {
     return generate_method_call(method_id, false, true);
@@ -256,10 +282,12 @@
   bool inline_unsafe_allocate();
   bool inline_unsafe_newArray(bool uninitialized);
   bool inline_unsafe_writeback0();
   bool inline_unsafe_writebackSync0(bool is_pre);
   bool inline_unsafe_copyMemory();
+  bool inline_unsafe_make_private_buffer();
+  bool inline_unsafe_finish_private_buffer();
   bool inline_native_currentThread();
 
   bool inline_native_time_funcs(address method, const char* funcName);
 #ifdef JFR_HAVE_INTRINSICS
   bool inline_native_classID();
@@ -600,29 +628,33 @@
   case vmIntrinsics::_compressStringC:
   case vmIntrinsics::_compressStringB:          return inline_string_copy( is_compress);
   case vmIntrinsics::_inflateStringC:
   case vmIntrinsics::_inflateStringB:           return inline_string_copy(!is_compress);
 
+  case vmIntrinsics::_makePrivateBuffer:        return inline_unsafe_make_private_buffer();
+  case vmIntrinsics::_finishPrivateBuffer:      return inline_unsafe_finish_private_buffer();
   case vmIntrinsics::_getReference:             return inline_unsafe_access(!is_store, T_OBJECT,   Relaxed, false);
   case vmIntrinsics::_getBoolean:               return inline_unsafe_access(!is_store, T_BOOLEAN,  Relaxed, false);
   case vmIntrinsics::_getByte:                  return inline_unsafe_access(!is_store, T_BYTE,     Relaxed, false);
   case vmIntrinsics::_getShort:                 return inline_unsafe_access(!is_store, T_SHORT,    Relaxed, false);
   case vmIntrinsics::_getChar:                  return inline_unsafe_access(!is_store, T_CHAR,     Relaxed, false);
   case vmIntrinsics::_getInt:                   return inline_unsafe_access(!is_store, T_INT,      Relaxed, false);
   case vmIntrinsics::_getLong:                  return inline_unsafe_access(!is_store, T_LONG,     Relaxed, false);
   case vmIntrinsics::_getFloat:                 return inline_unsafe_access(!is_store, T_FLOAT,    Relaxed, false);
   case vmIntrinsics::_getDouble:                return inline_unsafe_access(!is_store, T_DOUBLE,   Relaxed, false);
+  case vmIntrinsics::_getValue:                 return inline_unsafe_access(!is_store, T_VALUETYPE,Relaxed, false);
 
   case vmIntrinsics::_putReference:             return inline_unsafe_access( is_store, T_OBJECT,   Relaxed, false);
   case vmIntrinsics::_putBoolean:               return inline_unsafe_access( is_store, T_BOOLEAN,  Relaxed, false);
   case vmIntrinsics::_putByte:                  return inline_unsafe_access( is_store, T_BYTE,     Relaxed, false);
   case vmIntrinsics::_putShort:                 return inline_unsafe_access( is_store, T_SHORT,    Relaxed, false);
   case vmIntrinsics::_putChar:                  return inline_unsafe_access( is_store, T_CHAR,     Relaxed, false);
   case vmIntrinsics::_putInt:                   return inline_unsafe_access( is_store, T_INT,      Relaxed, false);
   case vmIntrinsics::_putLong:                  return inline_unsafe_access( is_store, T_LONG,     Relaxed, false);
   case vmIntrinsics::_putFloat:                 return inline_unsafe_access( is_store, T_FLOAT,    Relaxed, false);
   case vmIntrinsics::_putDouble:                return inline_unsafe_access( is_store, T_DOUBLE,   Relaxed, false);
+  case vmIntrinsics::_putValue:                 return inline_unsafe_access( is_store, T_VALUETYPE,Relaxed, false);
 
   case vmIntrinsics::_getReferenceVolatile:     return inline_unsafe_access(!is_store, T_OBJECT,   Volatile, false);
   case vmIntrinsics::_getBooleanVolatile:       return inline_unsafe_access(!is_store, T_BOOLEAN,  Volatile, false);
   case vmIntrinsics::_getByteVolatile:          return inline_unsafe_access(!is_store, T_BYTE,     Volatile, false);
   case vmIntrinsics::_getShortVolatile:         return inline_unsafe_access(!is_store, T_SHORT,    Volatile, false);
@@ -2394,22 +2426,22 @@
     ciSignature* sig = callee()->signature();
 #ifdef ASSERT
     if (!is_store) {
       // Object getReference(Object base, int/long offset), etc.
       BasicType rtype = sig->return_type()->basic_type();
-      assert(rtype == type, "getter must return the expected value");
-      assert(sig->count() == 2, "oop getter has 2 arguments");
+      assert(rtype == type || (rtype == T_OBJECT && type == T_VALUETYPE), "getter must return the expected value");
+      assert(sig->count() == 2 || (type == T_VALUETYPE && sig->count() == 3), "oop getter has 2 or 3 arguments");
       assert(sig->type_at(0)->basic_type() == T_OBJECT, "getter base is object");
       assert(sig->type_at(1)->basic_type() == T_LONG, "getter offset is correct");
     } else {
       // void putReference(Object base, int/long offset, Object x), etc.
       assert(sig->return_type()->basic_type() == T_VOID, "putter must not return a value");
-      assert(sig->count() == 3, "oop putter has 3 arguments");
+      assert(sig->count() == 3 || (type == T_VALUETYPE && sig->count() == 4), "oop putter has 3 arguments");
       assert(sig->type_at(0)->basic_type() == T_OBJECT, "putter base is object");
       assert(sig->type_at(1)->basic_type() == T_LONG, "putter offset is correct");
       BasicType vtype = sig->type_at(sig->count()-1)->basic_type();
-      assert(vtype == type, "putter must accept the expected value");
+      assert(vtype == type || (type == T_VALUETYPE && vtype == T_OBJECT), "putter must accept the expected value");
     }
 #endif // ASSERT
  }
 #endif //PRODUCT
 
@@ -2430,16 +2462,79 @@
   // We currently rely on the cookies produced by Unsafe.xxxFieldOffset
   // to be plain byte offsets, which are also the same as those accepted
   // by oopDesc::field_addr.
   assert(Unsafe_field_offset_to_byte_offset(11) == 11,
          "fieldOffset must be byte-scaled");
+
+  ciValueKlass* value_klass = NULL;
+  if (type == T_VALUETYPE) {
+    Node* cls = null_check(argument(4));
+    if (stopped()) {
+      return true;
+    }
+    Node* kls = load_klass_from_mirror(cls, false, NULL, 0);
+    const TypeKlassPtr* kls_t = _gvn.type(kls)->isa_klassptr();
+    if (!kls_t->klass_is_exact()) {
+      return false;
+    }
+    ciKlass* klass = kls_t->klass();
+    if (!klass->is_valuetype()) {
+      return false;
+    }
+    value_klass = klass->as_value_klass();
+  }
+
+  receiver = null_check(receiver);
+  if (stopped()) {
+    return true;
+  }
+
+  if (base->is_ValueType()) {
+    ValueTypeNode* vt = base->as_ValueType();
+
+    if (is_store) {
+      if (!vt->is_allocated(&_gvn) || !_gvn.type(vt)->is_valuetype()->larval()) {
+        return false;
+      }
+      base = vt->get_oop();
+    } else {
+      if (offset->is_Con()) {
+        long off = find_long_con(offset, 0);
+        ciValueKlass* vk = vt->type()->value_klass();
+        if ((long)(int)off != off || !vk->contains_field_offset(off)) {
+          return false;
+        }
+
+        ciField* f = vk->get_non_flattened_field_by_offset((int)off);
+
+        if (f != NULL) {
+          BasicType bt = f->layout_type();
+          if (bt == T_ARRAY || bt == T_NARROWOOP) {
+            bt = T_OBJECT;
+          }
+          if (bt == type) {
+            if (bt != T_VALUETYPE || f->type() == value_klass) {
+              set_result(vt->field_value_by_offset((int)off, false));
+              return true;
+            }
+          }
+        }
+      }
+      // Re-execute the unsafe access if allocation triggers deoptimization.
+      PreserveReexecuteState preexecs(this);
+      jvms()->set_should_reexecute(true);
+      vt = vt->allocate(this)->as_ValueType();
+      base = vt->get_oop();
+    }
+  }
+
   // 32-bit machines ignore the high half!
   offset = ConvL2X(offset);
   adr = make_unsafe_address(base, offset, is_store ? ACCESS_WRITE : ACCESS_READ, type, kind == Relaxed);
 
   if (_gvn.type(base)->isa_ptr() == TypePtr::NULL_PTR) {
-    if (type != T_OBJECT) {
+    if (type != T_OBJECT && (value_klass == NULL || !value_klass->has_object_fields())) {
       decorators |= IN_NATIVE; // off-heap primitive access
     } else {
       return false; // off-heap oop accesses are not supported
     }
   } else {
@@ -2451,11 +2546,11 @@
 
   if (!can_access_non_heap) {
     decorators |= IN_HEAP;
   }
 
-  val = is_store ? argument(4) : NULL;
+  val = is_store ? argument(4 + (type == T_VALUETYPE ? 1 : 0)) : NULL;
 
   const TypePtr* adr_type = _gvn.type(adr)->isa_ptr();
   if (adr_type == TypePtr::NULL_PTR) {
     return false; // off-heap access with zero address
   }
@@ -2468,11 +2563,35 @@
       alias_type->adr_type() == TypeAryPtr::RANGE) {
     return false; // not supported
   }
 
   bool mismatched = false;
-  BasicType bt = alias_type->basic_type();
+  BasicType bt = T_ILLEGAL;
+  ciField* field = NULL;
+  if (adr_type->isa_instptr()) {
+    const TypeInstPtr* instptr = adr_type->is_instptr();
+    ciInstanceKlass* k = instptr->klass()->as_instance_klass();
+    int off = instptr->offset();
+    if (instptr->const_oop() != NULL &&
+        instptr->klass() == ciEnv::current()->Class_klass() &&
+        instptr->offset() >= (instptr->klass()->as_instance_klass()->size_helper() * wordSize)) {
+      k = instptr->const_oop()->as_instance()->java_lang_Class_klass()->as_instance_klass();
+      field = k->get_field_by_offset(off, true);
+    } else {
+      field = k->get_non_flattened_field_by_offset(off);
+    }
+    if (field != NULL) {
+      bt = field->layout_type();
+    }
+    assert(bt == alias_type->basic_type() || bt == T_VALUETYPE, "should match");
+    if (field != NULL && bt == T_VALUETYPE && !field->is_flattened()) {
+      bt = T_OBJECT;
+    }
+  } else {
+    bt = alias_type->basic_type();
+  }
+
   if (bt != T_ILLEGAL) {
     assert(alias_type->adr_type()->is_oopptr(), "should be on-heap access");
     if (bt == T_BYTE && adr_type->isa_aryptr()) {
       // Alias type doesn't differentiate between byte[] and boolean[]).
       // Use address type to get the element type.
@@ -2489,10 +2608,31 @@
     mismatched = (bt != type);
   } else if (alias_type->adr_type()->isa_oopptr()) {
     mismatched = true; // conservatively mark all "wide" on-heap accesses as mismatched
   }
 
+  if (type == T_VALUETYPE) {
+    if (adr_type->isa_instptr()) {
+      if (field == NULL || field->type() != value_klass) {
+        mismatched = true;
+      }
+    } else if (adr_type->isa_aryptr()) {
+      const Type* elem = adr_type->is_aryptr()->elem();
+      if (!elem->isa_valuetype()) {
+        mismatched = true;
+      } else if (elem->value_klass() != value_klass) {
+        mismatched = true;
+      }
+    }
+    if (is_store) {
+      const Type* val_t = _gvn.type(val);
+      if (!val_t->isa_valuetype() || val_t->value_klass() != value_klass) {
+        return false;
+      }
+    }
+  }
+
   assert(!mismatched || alias_type->adr_type()->is_oopptr(), "off-heap access can't be mismatched");
 
   if (mismatched) {
     decorators |= C2_MISMATCHED;
   }
@@ -2501,37 +2641,47 @@
   const Type *value_type = Type::get_const_basic_type(type);
 
   // Figure out the memory ordering.
   decorators |= mo_decorator_for_access_kind(kind);
 
-  if (!is_store && type == T_OBJECT) {
-    const TypeOopPtr* tjp = sharpen_unsafe_type(alias_type, adr_type);
-    if (tjp != NULL) {
-      value_type = tjp;
+  if (!is_store) {
+    if (type == T_OBJECT) {
+      const TypeOopPtr* tjp = sharpen_unsafe_type(alias_type, adr_type);
+      if (tjp != NULL) {
+        value_type = tjp;
+      }
+    } else if (type == T_VALUETYPE) {
+      value_type = NULL;
     }
   }
 
-  receiver = null_check(receiver);
-  if (stopped()) {
-    return true;
-  }
   // Heap pointers get a null-check from the interpreter,
   // as a courtesy.  However, this is not guaranteed by Unsafe,
   // and it is not possible to fully distinguish unintended nulls
   // from intended ones in this API.
 
   if (!is_store) {
     Node* p = NULL;
     // Try to constant fold a load from a constant field
-    ciField* field = alias_type->field();
+
     if (heap_base_oop != top() && field != NULL && field->is_constant() && !mismatched) {
       // final or stable field
       p = make_constant_from_field(field, heap_base_oop);
     }
 
     if (p == NULL) { // Could not constant fold the load
-      p = access_load_at(heap_base_oop, adr, adr_type, value_type, type, decorators);
+      if (type == T_VALUETYPE) {
+        if (adr_type->isa_instptr() && !mismatched) {
+          ciInstanceKlass* holder = adr_type->is_instptr()->klass()->as_instance_klass();
+          int offset = adr_type->is_instptr()->offset();
+          p = ValueTypeNode::make_from_flattened(this, value_klass, base, base, holder, offset, decorators);
+        } else {
+          p = ValueTypeNode::make_from_flattened(this, value_klass, base, adr, NULL, 0, decorators);
+        }
+      } else {
+        p = access_load_at(heap_base_oop, adr, adr_type, value_type, type, decorators);
+      }
       // Normalize the value returned by getBoolean in the following cases
       if (type == T_BOOLEAN &&
           (mismatched ||
            heap_base_oop == top() ||                  // - heap_base_oop is NULL or
            (can_access_non_heap && field == NULL))    // - heap_base_oop is potentially NULL
@@ -2554,10 +2704,18 @@
     }
     if (type == T_ADDRESS) {
       p = gvn().transform(new CastP2XNode(NULL, p));
       p = ConvX2UL(p);
     }
+    if (field != NULL && field->is_flattenable() && !field->is_flattened()) {
+      // Load a non-flattened but flattenable value type from memory
+      if (value_type->value_klass()->is_scalarizable()) {
+        p = ValueTypeNode::make_from_oop(this, p, value_type->value_klass());
+      } else {
+        p = null2default(p, value_type->value_klass());
+      }
+    }
     // The load node has the control of the preceding MemBarCPUOrder.  All
     // following nodes will have the control of the MemBarCPUOrder inserted at
     // the end of this method.  So, pushing the load onto the stack at a later
     // point is fine.
     set_result(p);
@@ -2565,13 +2723,70 @@
     if (bt == T_ADDRESS) {
       // Repackage the long as a pointer.
       val = ConvL2X(val);
       val = gvn().transform(new CastX2PNode(val));
     }
-    access_store_at(heap_base_oop, adr, adr_type, val, value_type, type, decorators);
+    if (type == T_VALUETYPE) {
+      if (adr_type->isa_instptr() && !mismatched) {
+        ciInstanceKlass* holder = adr_type->is_instptr()->klass()->as_instance_klass();
+        int offset = adr_type->is_instptr()->offset();
+        val->as_ValueType()->store_flattened(this, base, base, holder, offset, decorators);
+      } else {
+        val->as_ValueType()->store_flattened(this, base, adr, NULL, 0, decorators);
+      }
+    } else {
+      access_store_at(heap_base_oop, adr, adr_type, val, value_type, type, decorators);
+    }
+  }
+
+  if (argument(1)->is_ValueType() && is_store) {
+    Node* value = ValueTypeNode::make_from_oop(this, base, _gvn.type(base)->value_klass());
+    value = value->as_ValueType()->make_larval(this, false);
+    replace_in_map(argument(1), value);
+  }
+
+  return true;
+}
+
+bool LibraryCallKit::inline_unsafe_make_private_buffer() {
+  Node* receiver = argument(0);
+  Node* value = argument(1);
+
+  receiver = null_check(receiver);
+  if (stopped()) {
+    return true;
+  }
+
+  if (!value->is_ValueType()) {
+    return false;
   }
 
+  set_result(value->as_ValueType()->make_larval(this, true));
+
+  return true;
+}
+
+bool LibraryCallKit::inline_unsafe_finish_private_buffer() {
+  Node* receiver = argument(0);
+  Node* buffer = argument(1);
+
+  receiver = null_check(receiver);
+  if (stopped()) {
+    return true;
+  }
+
+  if (!buffer->is_ValueType()) {
+    return false;
+  }
+
+  ValueTypeNode* vt = buffer->as_ValueType();
+  if (!vt->is_allocated(&_gvn) || !_gvn.type(vt)->is_valuetype()->larval()) {
+    return false;
+  }
+
+  set_result(vt->finish_larval(this));
+
   return true;
 }
 
 //----------------------------inline_unsafe_load_store----------------------------
 // This method serves a couple of different customers (depending on LoadStoreKind):
@@ -3030,19 +3245,10 @@
   Node* junk = NULL;
   set_result(generate_current_thread(junk));
   return true;
 }
 
-//---------------------------load_mirror_from_klass----------------------------
-// Given a klass oop, load its java mirror (a java.lang.Class oop).
-Node* LibraryCallKit::load_mirror_from_klass(Node* klass) {
-  Node* p = basic_plus_adr(klass, in_bytes(Klass::java_mirror_offset()));
-  Node* load = make_load(NULL, p, TypeRawPtr::NOTNULL, T_ADDRESS, MemNode::unordered);
-  // mirror = ((OopHandle)mirror)->resolve();
-  return access_load(load, TypeInstPtr::MIRROR, T_OBJECT, IN_NATIVE);
-}
-
 //-----------------------load_klass_from_mirror_common-------------------------
 // Given a java mirror (a java.lang.Class oop), load its corresponding klass oop.
 // Test the klass oop for null (signifying a primitive Class like Integer.TYPE),
 // and branch to the given path on the region.
 // If never_see_null, take an uncommon trap on null, so we can optimistically
@@ -3081,17 +3287,22 @@
   Node* mbit = _gvn.transform(new AndINode(mods, mask));
   Node* cmp  = _gvn.transform(new CmpINode(mbit, bits));
   Node* bol  = _gvn.transform(new BoolNode(cmp, BoolTest::ne));
   return generate_fair_guard(bol, region);
 }
+
 Node* LibraryCallKit::generate_interface_guard(Node* kls, RegionNode* region) {
   return generate_access_flags_guard(kls, JVM_ACC_INTERFACE, 0, region);
 }
 Node* LibraryCallKit::generate_hidden_class_guard(Node* kls, RegionNode* region) {
   return generate_access_flags_guard(kls, JVM_ACC_IS_HIDDEN_CLASS, 0, region);
 }
 
+Node* LibraryCallKit::generate_value_guard(Node* kls, RegionNode* region) {
+  return generate_access_flags_guard(kls, JVM_ACC_VALUE, 0, region);
+}
+
 //-------------------------inline_native_Class_query-------------------
 bool LibraryCallKit::inline_native_Class_query(vmIntrinsics::ID id) {
   const Type* return_type = TypeInt::BOOL;
   Node* prim_return_value = top();  // what happens if it's a primitive class?
   bool never_see_null = !too_many_traps(Deoptimization::Reason_null_check);
@@ -3285,22 +3496,34 @@
     return false;  // dead path (mirror->is_top()).
   }
   if (obj == NULL || obj->is_top()) {
     return false;  // dead path
   }
-  const TypeOopPtr* tp = _gvn.type(obj)->isa_oopptr();
+  ciKlass* obj_klass = NULL;
+  const Type* obj_t = _gvn.type(obj);
+  if (obj->is_ValueType()) {
+    obj_klass = obj_t->value_klass();
+  } else if (obj_t->isa_oopptr()) {
+    obj_klass = obj_t->is_oopptr()->klass();
+  }
 
   // First, see if Class.cast() can be folded statically.
   // java_mirror_type() returns non-null for compile-time Class constants.
   ciType* tm = mirror_con->java_mirror_type();
-  if (tm != NULL && tm->is_klass() &&
-      tp != NULL && tp->klass() != NULL) {
-    if (!tp->klass()->is_loaded()) {
+  if (tm != NULL && tm->is_klass() && obj_klass != NULL) {
+    if (!obj_klass->is_loaded()) {
       // Don't use intrinsic when class is not loaded.
       return false;
     } else {
-      int static_res = C->static_subtype_check(tm->as_klass(), tp->klass());
+      if (!obj->is_ValueType() && tm->as_klass()->is_valuetype()) {
+        // Casting to .val, check for null
+        obj = null_check(obj);
+        if (stopped()) {
+          return true;
+        }
+      }
+      int static_res = C->static_subtype_check(tm->as_klass(), obj_klass);
       if (static_res == Compile::SSC_always_true) {
         // isInstance() is true - fold the code.
         set_result(obj);
         return true;
       } else if (static_res == Compile::SSC_always_false) {
@@ -3326,28 +3549,48 @@
   if (stopped()) {
     return true;
   }
 
   // Not-subtype or the mirror's klass ptr is NULL (in case it is a primitive).
-  enum { _bad_type_path = 1, _prim_path = 2, PATH_LIMIT };
+  enum { _bad_type_path = 1, _prim_path = 2, _npe_path = 3, PATH_LIMIT };
   RegionNode* region = new RegionNode(PATH_LIMIT);
   record_for_igvn(region);
 
   // Now load the mirror's klass metaobject, and null-check it.
   // If kls is null, we have a primitive mirror and
   // nothing is an instance of a primitive type.
   Node* kls = load_klass_from_mirror(mirror, false, region, _prim_path);
 
   Node* res = top();
   if (!stopped()) {
+    if (EnableValhalla && !obj->is_ValueType()) {
+      // Check if we are casting to .val
+      Node* is_val_kls = generate_value_guard(kls, NULL);
+      if (is_val_kls != NULL) {
+        RegionNode* r = new RegionNode(3);
+        record_for_igvn(r);
+        r->init_req(1, control());
+
+        // Casting to .val, check for null
+        set_control(is_val_kls);
+        Node* null_ctr = top();
+        null_check_oop(obj, &null_ctr);
+        region->init_req(_npe_path, null_ctr);
+        r->init_req(2, control());
+
+        set_control(_gvn.transform(r));
+      }
+    }
+
     Node* bad_type_ctrl = top();
     // Do checkcast optimizations.
     res = gen_checkcast(obj, kls, &bad_type_ctrl);
     region->init_req(_bad_type_path, bad_type_ctrl);
   }
   if (region->in(_prim_path) != top() ||
-      region->in(_bad_type_path) != top()) {
+      region->in(_bad_type_path) != top() ||
+      region->in(_npe_path) != top()) {
     // Let Interpreter throw ClassCastException.
     PreserveJVMState pjvms(this);
     set_control(_gvn.transform(region));
     uncommon_trap(Deoptimization::Reason_intrinsic,
                   Deoptimization::Action_maybe_recompile);
@@ -3380,12 +3623,14 @@
     _both_ref_path,             // {N,N} & subtype check loses => false
     PATH_LIMIT
   };
 
   RegionNode* region = new RegionNode(PATH_LIMIT);
+  RegionNode* prim_region = new RegionNode(2);
   Node*       phi    = new PhiNode(region, TypeInt::BOOL);
   record_for_igvn(region);
+  record_for_igvn(prim_region);
 
   const TypePtr* adr_type = TypeRawPtr::BOTTOM;   // memory type of loads
   const TypeKlassPtr* kls_type = TypeKlassPtr::OBJECT_OR_NULL;
   int class_klass_offset = java_lang_Class::klass_offset_in_bytes();
 
@@ -3406,12 +3651,15 @@
   bool never_see_null = !too_many_traps(Deoptimization::Reason_null_check);
   for (which_arg = 0; which_arg <= 1; which_arg++) {
     Node* kls = klasses[which_arg];
     Node* null_ctl = top();
     kls = null_check_oop(kls, &null_ctl, never_see_null);
-    int prim_path = (which_arg == 0 ? _prim_0_path : _prim_1_path);
-    region->init_req(prim_path, null_ctl);
+    if (which_arg == 0) {
+      prim_region->init_req(1, null_ctl);
+    } else {
+      region->init_req(_prim_1_path, null_ctl);
+    }
     if (stopped())  break;
     klasses[which_arg] = kls;
   }
 
   if (!stopped()) {
@@ -3424,16 +3672,17 @@
   }
 
   // If both operands are primitive (both klasses null), then
   // we must return true when they are identical primitives.
   // It is convenient to test this after the first null klass check.
-  set_control(region->in(_prim_0_path)); // go back to first null check
+  // This path is also used if superc is a value mirror.
+  set_control(_gvn.transform(prim_region));
   if (!stopped()) {
     // Since superc is primitive, make a guard for the superc==subc case.
     Node* cmp_eq = _gvn.transform(new CmpPNode(args[0], args[1]));
     Node* bol_eq = _gvn.transform(new BoolNode(cmp_eq, BoolTest::eq));
-    generate_guard(bol_eq, region, PROB_FAIR);
+    generate_fair_guard(bol_eq, region);
     if (region->req() == PATH_LIMIT+1) {
       // A guard was added.  If the added guard is taken, superc==subc.
       region->swap_edges(PATH_LIMIT, _prim_same_path);
       region->del_req(PATH_LIMIT);
     }
@@ -3460,59 +3709,78 @@
   set_result(_gvn.transform(phi));
   return true;
 }
 
 //---------------------generate_array_guard_common------------------------
-Node* LibraryCallKit::generate_array_guard_common(Node* kls, RegionNode* region,
-                                                  bool obj_array, bool not_array) {
+Node* LibraryCallKit::generate_array_guard_common(Node* kls, RegionNode* region, ArrayKind kind) {
 
   if (stopped()) {
     return NULL;
   }
 
-  // If obj_array/non_array==false/false:
-  // Branch around if the given klass is in fact an array (either obj or prim).
-  // If obj_array/non_array==false/true:
-  // Branch around if the given klass is not an array klass of any kind.
-  // If obj_array/non_array==true/true:
-  // Branch around if the kls is not an oop array (kls is int[], String, etc.)
-  // If obj_array/non_array==true/false:
-  // Branch around if the kls is an oop array (Object[] or subtype)
-  //
   // Like generate_guard, adds a new path onto the region.
   jint  layout_con = 0;
   Node* layout_val = get_layout_helper(kls, layout_con);
   if (layout_val == NULL) {
-    bool query = (obj_array
-                  ? Klass::layout_helper_is_objArray(layout_con)
-                  : Klass::layout_helper_is_array(layout_con));
-    if (query == not_array) {
+    bool query = 0;
+    switch(kind) {
+      case ObjectArray:    query = Klass::layout_helper_is_objArray(layout_con); break;
+      case NonObjectArray: query = !Klass::layout_helper_is_objArray(layout_con); break;
+      case TypeArray:      query = Klass::layout_helper_is_typeArray(layout_con); break;
+      case ValueArray:     query = Klass::layout_helper_is_valueArray(layout_con); break;
+      case AnyArray:       query = Klass::layout_helper_is_array(layout_con); break;
+      case NonArray:       query = !Klass::layout_helper_is_array(layout_con); break;
+      default:
+        ShouldNotReachHere();
+    }
+    if (!query) {
       return NULL;                       // never a branch
     } else {                             // always a branch
       Node* always_branch = control();
       if (region != NULL)
         region->add_req(always_branch);
       set_control(top());
       return always_branch;
     }
   }
+  unsigned int value = 0;
+  BoolTest::mask btest = BoolTest::illegal;
+  switch(kind) {
+    case ObjectArray:
+    case NonObjectArray: {
+      value = Klass::_lh_array_tag_obj_value;
+      layout_val = _gvn.transform(new RShiftINode(layout_val, intcon(Klass::_lh_array_tag_shift)));
+      btest = kind == ObjectArray ? BoolTest::eq : BoolTest::ne;
+      break;
+    }
+    case TypeArray: {
+      value = Klass::_lh_array_tag_type_value;
+      layout_val = _gvn.transform(new RShiftINode(layout_val, intcon(Klass::_lh_array_tag_shift)));
+      btest = BoolTest::eq;
+      break;
+    }
+    case ValueArray: {
+      value = Klass::_lh_array_tag_vt_value;
+      layout_val = _gvn.transform(new RShiftINode(layout_val, intcon(Klass::_lh_array_tag_shift)));
+      btest = BoolTest::eq;
+      break;
+    }
+    case AnyArray:    value = Klass::_lh_neutral_value; btest = BoolTest::lt; break;
+    case NonArray:    value = Klass::_lh_neutral_value; btest = BoolTest::gt; break;
+    default:
+      ShouldNotReachHere();
+  }
   // Now test the correct condition.
-  jint  nval = (obj_array
-                ? (jint)(Klass::_lh_array_tag_type_value
-                   <<    Klass::_lh_array_tag_shift)
-                : Klass::_lh_neutral_value);
+  jint nval = (jint)value;
   Node* cmp = _gvn.transform(new CmpINode(layout_val, intcon(nval)));
-  BoolTest::mask btest = BoolTest::lt;  // correct for testing is_[obj]array
-  // invert the test if we are looking for a non-array
-  if (not_array)  btest = BoolTest(btest).negate();
   Node* bol = _gvn.transform(new BoolNode(cmp, btest));
   return generate_fair_guard(bol, region);
 }
 
 
 //-----------------------inline_native_newArray--------------------------
-// private static native Object java.lang.reflect.newArray(Class<?> componentType, int length);
+// private static native Object java.lang.reflect.Array.newArray(Class<?> componentType, int length);
 // private        native Object Unsafe.allocateUninitializedArray0(Class<?> cls, int size);
 bool LibraryCallKit::inline_unsafe_newArray(bool uninitialized) {
   Node* mirror;
   Node* count_val;
   if (uninitialized) {
@@ -3624,10 +3892,23 @@
   Node* original          = argument(0);
   Node* start             = is_copyOfRange? argument(1): intcon(0);
   Node* end               = is_copyOfRange? argument(2): argument(1);
   Node* array_type_mirror = is_copyOfRange? argument(3): argument(2);
 
+  const TypeAryPtr* original_t = _gvn.type(original)->isa_aryptr();
+  const TypeInstPtr* mirror_t = _gvn.type(array_type_mirror)->isa_instptr();
+  if (EnableValhalla && ValueArrayFlatten &&
+      (original_t == NULL || mirror_t == NULL ||
+       (mirror_t->java_mirror_type() == NULL &&
+        (original_t->elem()->isa_valuetype() ||
+         (original_t->elem()->make_oopptr() != NULL &&
+          original_t->elem()->make_oopptr()->can_be_value_type()))))) {
+    // We need to know statically if the copy is to a flattened array
+    // or not but can't tell.
+    return false;
+  }
+
   Node* newcopy = NULL;
 
   // Set the original stack and the reexecute bit for the interpreter to reexecute
   // the bytecode that invokes Arrays.copyOf if deoptimization happens.
   { PreserveReexecuteState preexecs(this);
@@ -3647,20 +3928,66 @@
     RegionNode* bailout = new RegionNode(1);
     record_for_igvn(bailout);
 
     // Despite the generic type of Arrays.copyOf, the mirror might be int, int[], etc.
     // Bail out if that is so.
-    Node* not_objArray = generate_non_objArray_guard(klass_node, bailout);
+    // Value type array may have object field that would require a
+    // write barrier. Conservatively, go to slow path.
+    BarrierSetC2* bs = BarrierSet::barrier_set()->barrier_set_c2();
+    Node* not_objArray = !bs->array_copy_requires_gc_barriers(false, T_OBJECT, false, BarrierSetC2::Parsing) ?
+        generate_typeArray_guard(klass_node, bailout) : generate_non_objArray_guard(klass_node, bailout);
     if (not_objArray != NULL) {
       // Improve the klass node's type from the new optimistic assumption:
       ciKlass* ak = ciArrayKlass::make(env()->Object_klass());
-      const Type* akls = TypeKlassPtr::make(TypePtr::NotNull, ak, 0/*offset*/);
+      const Type* akls = TypeKlassPtr::make(TypePtr::NotNull, ak, Type::Offset(0), false);
       Node* cast = new CastPPNode(klass_node, akls);
       cast->init_req(0, control());
       klass_node = _gvn.transform(cast);
     }
 
+    Node* original_kls = load_object_klass(original);
+    // ArrayCopyNode:Ideal may transform the ArrayCopyNode to
+    // loads/stores but it is legal only if we're sure the
+    // Arrays.copyOf would succeed. So we need all input arguments
+    // to the copyOf to be validated, including that the copy to the
+    // new array won't trigger an ArrayStoreException. That subtype
+    // check can be optimized if we know something on the type of
+    // the input array from type speculation.
+    if (_gvn.type(klass_node)->singleton() && !stopped()) {
+      ciKlass* subk   = _gvn.type(original_kls)->is_klassptr()->klass();
+      ciKlass* superk = _gvn.type(klass_node)->is_klassptr()->klass();
+
+      int test = C->static_subtype_check(superk, subk);
+      if (test != Compile::SSC_always_true && test != Compile::SSC_always_false) {
+        const TypeOopPtr* t_original = _gvn.type(original)->is_oopptr();
+        if (t_original->speculative_type() != NULL) {
+          original = maybe_cast_profiled_obj(original, t_original->speculative_type(), true);
+          original_kls = load_object_klass(original);
+        }
+      }
+    }
+
+    if (ValueArrayFlatten) {
+      // Either both or neither new array klass and original array
+      // klass must be flattened
+      Node* is_flat = generate_valueArray_guard(klass_node, NULL);
+      if (!original_t->is_not_flat()) {
+        generate_valueArray_guard(original_kls, bailout);
+      }
+      if (is_flat != NULL) {
+        RegionNode* r = new RegionNode(2);
+        record_for_igvn(r);
+        r->init_req(1, control());
+        set_control(is_flat);
+        if (!original_t->is_not_flat()) {
+          generate_valueArray_guard(original_kls, r);
+        }
+        bailout->add_req(control());
+        set_control(_gvn.transform(r));
+      }
+    }
+
     // Bail out if either start or end is negative.
     generate_negative_guard(start, bailout, &start);
     generate_negative_guard(end,   bailout, &end);
 
     Node* length = end;
@@ -3691,30 +4018,10 @@
       // We know the copy is disjoint but we might not know if the
       // oop stores need checking.
       // Extreme case:  Arrays.copyOf((Integer[])x, 10, String[].class).
       // This will fail a store-check if x contains any non-nulls.
 
-      // ArrayCopyNode:Ideal may transform the ArrayCopyNode to
-      // loads/stores but it is legal only if we're sure the
-      // Arrays.copyOf would succeed. So we need all input arguments
-      // to the copyOf to be validated, including that the copy to the
-      // new array won't trigger an ArrayStoreException. That subtype
-      // check can be optimized if we know something on the type of
-      // the input array from type speculation.
-      if (_gvn.type(klass_node)->singleton()) {
-        ciKlass* subk   = _gvn.type(load_object_klass(original))->is_klassptr()->klass();
-        ciKlass* superk = _gvn.type(klass_node)->is_klassptr()->klass();
-
-        int test = C->static_subtype_check(superk, subk);
-        if (test != Compile::SSC_always_true && test != Compile::SSC_always_false) {
-          const TypeOopPtr* t_original = _gvn.type(original)->is_oopptr();
-          if (t_original->speculative_type() != NULL) {
-            original = maybe_cast_profiled_obj(original, t_original->speculative_type(), true);
-          }
-        }
-      }
-
       bool validated = false;
       // Reason_class_check rather than Reason_intrinsic because we
       // want to intrinsify even if this traps.
       if (!too_many_traps(Deoptimization::Reason_class_check)) {
         Node* not_subtype_ctrl = gen_subtype_check(original, klass_node);
@@ -3731,11 +4038,11 @@
 
       if (!stopped()) {
         newcopy = new_array(klass_node, length, 0);  // no arguments to push
 
         ArrayCopyNode* ac = ArrayCopyNode::make(this, true, original, start, newcopy, intcon(0), moved, true, false,
-                                                load_object_klass(original), klass_node);
+                                                original_kls, klass_node);
         if (!is_copyOfRange) {
           ac->set_copyof(validated);
         } else {
           ac->set_copyofrange(validated);
         }
@@ -3855,21 +4162,25 @@
 
   RegionNode* result_reg = new RegionNode(PATH_LIMIT);
   PhiNode*    result_val = new PhiNode(result_reg, TypeInt::INT);
   PhiNode*    result_io  = new PhiNode(result_reg, Type::ABIO);
   PhiNode*    result_mem = new PhiNode(result_reg, Type::MEMORY, TypePtr::BOTTOM);
-  Node* obj = NULL;
+  Node* obj = argument(0);
+
+  if (obj->is_ValueType() || gvn().type(obj)->is_valuetypeptr()) {
+    return false;
+  }
+
   if (!is_static) {
     // Check for hashing null object
     obj = null_check_receiver();
     if (stopped())  return true;        // unconditionally null
     result_reg->init_req(_null_path, top());
     result_val->init_req(_null_path, top());
   } else {
     // Do a null check, and return zero if null.
     // System.identityHashCode(null) == 0
-    obj = argument(0);
     Node* null_ctl = top();
     obj = null_check_oop(obj, &null_ctl);
     result_reg->init_req(_null_path, null_ctl);
     result_val->init_req(_null_path, _gvn.intcon(0));
   }
@@ -3905,10 +4216,11 @@
   // the null check after castPP removal.
   Node* no_ctrl = NULL;
   Node* header = make_load(no_ctrl, header_addr, TypeX_X, TypeX_X->basic_type(), MemNode::unordered);
 
   // Test the header to see if it is unlocked.
+  // This also serves as guard against value types (they have the always_locked_pattern set).
   Node *lock_mask      = _gvn.MakeConX(markWord::biased_lock_mask_in_place);
   Node *lmasked_header = _gvn.transform(new AndXNode(header, lock_mask));
   Node *unlocked_val   = _gvn.MakeConX(markWord::unlocked_value);
   Node *chk_unlocked   = _gvn.transform(new CmpXNode( lmasked_header, unlocked_val));
   Node *test_unlocked  = _gvn.transform(new BoolNode( chk_unlocked, BoolTest::ne));
@@ -3971,11 +4283,17 @@
 //---------------------------inline_native_getClass----------------------------
 // public final native Class<?> java.lang.Object.getClass();
 //
 // Build special case code for calls to getClass on an object.
 bool LibraryCallKit::inline_native_getClass() {
-  Node* obj = null_check_receiver();
+  Node* obj = argument(0);
+  if (obj->is_ValueType()) {
+    ciKlass* vk = _gvn.type(obj)->value_klass();
+    set_result(makecon(TypeInstPtr::make(vk->java_mirror())));
+    return true;
+  }
+  obj = null_check_receiver();
   if (stopped())  return true;
   set_result(load_mirror_from_klass(load_object_klass(obj)));
   return true;
 }
 
@@ -4233,11 +4551,18 @@
     // copy and a StoreStore barrier exists after the array copy.
     alloc->initialization()->set_complete_with_arraycopy();
   }
 
   Node* size = _gvn.transform(obj_size);
-  access_clone(obj, alloc_obj, size, is_array);
+  // Exclude the header but include array length to copy by 8 bytes words.
+  // Can't use base_offset_in_bytes(bt) since basic type is unknown.
+  int base_off = BarrierSetC2::arraycopy_payload_base_offset(is_array);
+  Node* countx = size;
+  countx = _gvn.transform(new SubXNode(countx, MakeConX(base_off)));
+  countx = _gvn.transform(new URShiftXNode(countx, intcon(LogBytesPerLong)));
+
+  access_clone(obj, alloc_obj, countx, is_array);
 
   // Do not let reads from the cloned object float above the arraycopy.
   if (alloc != NULL) {
     // Do not let stores that initialize this object be reordered with
     // a subsequent store that would make this object accessible by
@@ -4276,21 +4601,27 @@
   // Set the reexecute bit for the interpreter to reexecute
   // the bytecode that invokes Object.clone if deoptimization happens.
   { PreserveReexecuteState preexecs(this);
     jvms()->set_should_reexecute(true);
 
-    Node* obj = null_check_receiver();
+    Node* obj = argument(0);
+    if (obj->is_ValueType()) {
+      return false;
+    }
+
+    obj = null_check_receiver();
     if (stopped())  return true;
 
     const TypeOopPtr* obj_type = _gvn.type(obj)->is_oopptr();
 
     // If we are going to clone an instance, we need its exact type to
     // know the number and types of fields to convert the clone to
     // loads/stores. Maybe a speculative type can help us.
     if (!obj_type->klass_is_exact() &&
         obj_type->speculative_type() != NULL &&
-        obj_type->speculative_type()->is_instance_klass()) {
+        obj_type->speculative_type()->is_instance_klass() &&
+        !obj_type->speculative_type()->is_valuetype()) {
       ciInstanceKlass* spec_ik = obj_type->speculative_type()->as_instance_klass();
       if (spec_ik->nof_nonstatic_fields() <= ArrayCopyLoadStoreMaxElem &&
           !spec_ik->has_injected_fields()) {
         ciKlass* k = obj_type->klass();
         if (!k->is_instance_klass() ||
@@ -4318,64 +4649,76 @@
     PhiNode*    result_i_o = new PhiNode(result_reg, Type::ABIO);
     PhiNode*    result_mem = new PhiNode(result_reg, Type::MEMORY, TypePtr::BOTTOM);
     record_for_igvn(result_reg);
 
     Node* obj_klass = load_object_klass(obj);
+    // We only go to the fast case code if we pass a number of guards.
+    // The paths which do not pass are accumulated in the slow_region.
+    RegionNode* slow_region = new RegionNode(1);
+    record_for_igvn(slow_region);
+
     Node* array_ctl = generate_array_guard(obj_klass, (RegionNode*)NULL);
     if (array_ctl != NULL) {
       // It's an array.
       PreserveJVMState pjvms(this);
       set_control(array_ctl);
-      Node* obj_length = load_array_length(obj);
-      Node* obj_size  = NULL;
-      Node* alloc_obj = new_array(obj_klass, obj_length, 0, &obj_size);  // no arguments to push
 
       BarrierSetC2* bs = BarrierSet::barrier_set()->barrier_set_c2();
-      if (bs->array_copy_requires_gc_barriers(true, T_OBJECT, true, BarrierSetC2::Parsing)) {
-        // If it is an oop array, it requires very special treatment,
-        // because gc barriers are required when accessing the array.
-        Node* is_obja = generate_objArray_guard(obj_klass, (RegionNode*)NULL);
-        if (is_obja != NULL) {
-          PreserveJVMState pjvms2(this);
-          set_control(is_obja);
-          // Generate a direct call to the right arraycopy function(s).
-          Node* alloc = tightly_coupled_allocation(alloc_obj, NULL);
-          ArrayCopyNode* ac = ArrayCopyNode::make(this, true, obj, intcon(0), alloc_obj, intcon(0), obj_length, alloc != NULL, false);
-          ac->set_clone_oop_array();
-          Node* n = _gvn.transform(ac);
-          assert(n == ac, "cannot disappear");
-          ac->connect_outputs(this);
-
-          result_reg->init_req(_objArray_path, control());
-          result_val->init_req(_objArray_path, alloc_obj);
-          result_i_o ->set_req(_objArray_path, i_o());
-          result_mem ->set_req(_objArray_path, reset_memory());
-        }
+      if (bs->array_copy_requires_gc_barriers(true, T_OBJECT, true, BarrierSetC2::Parsing) &&
+          (!obj_type->isa_aryptr() || !obj_type->is_aryptr()->is_not_flat())) {
+        // Flattened value type array may have object field that would require a
+        // write barrier. Conservatively, go to slow path.
+        generate_valueArray_guard(obj_klass, slow_region);
       }
-      // Otherwise, there are no barriers to worry about.
-      // (We can dispense with card marks if we know the allocation
-      //  comes out of eden (TLAB)...  In fact, ReduceInitialCardMarks
-      //  causes the non-eden paths to take compensating steps to
-      //  simulate a fresh allocation, so that no further
-      //  card marks are required in compiled code to initialize
-      //  the object.)
 
       if (!stopped()) {
-        copy_to_clone(obj, alloc_obj, obj_size, true);
-
-        // Present the results of the copy.
-        result_reg->init_req(_array_path, control());
-        result_val->init_req(_array_path, alloc_obj);
-        result_i_o ->set_req(_array_path, i_o());
-        result_mem ->set_req(_array_path, reset_memory());
+        Node* obj_length = load_array_length(obj);
+        Node* obj_size  = NULL;
+        Node* alloc_obj = new_array(obj_klass, obj_length, 0, &obj_size);  // no arguments to push
+
+        BarrierSetC2* bs = BarrierSet::barrier_set()->barrier_set_c2();
+        if (bs->array_copy_requires_gc_barriers(true, T_OBJECT, true, BarrierSetC2::Parsing)) {
+          // If it is an oop array, it requires very special treatment,
+          // because gc barriers are required when accessing the array.
+          Node* is_obja = generate_objArray_guard(obj_klass, (RegionNode*)NULL);
+          if (is_obja != NULL) {
+            PreserveJVMState pjvms2(this);
+            set_control(is_obja);
+            // Generate a direct call to the right arraycopy function(s).
+            Node* alloc = tightly_coupled_allocation(alloc_obj, NULL);
+            ArrayCopyNode* ac = ArrayCopyNode::make(this, true, obj, intcon(0), alloc_obj, intcon(0), obj_length, alloc != NULL, false);
+            ac->set_clone_oop_array();
+            Node* n = _gvn.transform(ac);
+            assert(n == ac, "cannot disappear");
+            ac->connect_outputs(this);
+
+            result_reg->init_req(_objArray_path, control());
+            result_val->init_req(_objArray_path, alloc_obj);
+            result_i_o ->set_req(_objArray_path, i_o());
+            result_mem ->set_req(_objArray_path, reset_memory());
+          }
+        }
+        // Otherwise, there are no barriers to worry about.
+        // (We can dispense with card marks if we know the allocation
+        //  comes out of eden (TLAB)...  In fact, ReduceInitialCardMarks
+        //  causes the non-eden paths to take compensating steps to
+        //  simulate a fresh allocation, so that no further
+        //  card marks are required in compiled code to initialize
+        //  the object.)
+
+        if (!stopped()) {
+          copy_to_clone(obj, alloc_obj, obj_size, true);
+
+          // Present the results of the copy.
+          result_reg->init_req(_array_path, control());
+          result_val->init_req(_array_path, alloc_obj);
+          result_i_o ->set_req(_array_path, i_o());
+          result_mem ->set_req(_array_path, reset_memory());
+        }
       }
     }
 
-    // We only go to the instance fast case code if we pass a number of guards.
-    // The paths which do not pass are accumulated in the slow_region.
-    RegionNode* slow_region = new RegionNode(1);
-    record_for_igvn(slow_region);
     if (!stopped()) {
       // It's an instance (we did array above).  Make the slow-path tests.
       // If this is a virtual call, we generate a funny guard.  We grab
       // the vtable entry corresponding to clone() from the target object.
       // If the target method which we are calling happens to be the
@@ -4532,15 +4875,14 @@
     map()->replaced_nodes().apply(saved_jvms->map(), new_idx);
     set_jvms(saved_jvms);
     _reexecute_sp = saved_reexecute_sp;
 
     // Remove the allocation from above the guards
-    CallProjections callprojs;
-    alloc->extract_projections(&callprojs, true);
+    CallProjections* callprojs = alloc->extract_projections(true);
     InitializeNode* init = alloc->initialization();
     Node* alloc_mem = alloc->in(TypeFunc::Memory);
-    C->gvn_replace_by(callprojs.fallthrough_ioproj, alloc->in(TypeFunc::I_O));
+    C->gvn_replace_by(callprojs->fallthrough_ioproj, alloc->in(TypeFunc::I_O));
     C->gvn_replace_by(init->proj_out(TypeFunc::Memory), alloc_mem);
     C->gvn_replace_by(init->proj_out(TypeFunc::Control), alloc->in(0));
 
     // move the allocation here (after the guards)
     _gvn.hash_delete(alloc);
@@ -4548,11 +4890,11 @@
     alloc->set_req(TypeFunc::I_O, i_o());
     Node *mem = reset_memory();
     set_all_memory(mem);
     alloc->set_req(TypeFunc::Memory, mem);
     set_control(init->proj_out_or_null(TypeFunc::Control));
-    set_i_o(callprojs.fallthrough_ioproj);
+    set_i_o(callprojs->fallthrough_ioproj);
 
     // Update memory as done in GraphKit::set_output_for_allocation()
     const TypeInt* length_type = _gvn.find_int_type(alloc->in(AllocateNode::ALength));
     const TypeOopPtr* ary_type = _gvn.type(alloc->in(AllocateNode::KlassNode))->is_klassptr()->as_instance_type();
     if (ary_type->isa_aryptr() && length_type != NULL) {
@@ -4692,12 +5034,12 @@
   }
 
   if (has_src && has_dest && can_emit_guards) {
     BasicType src_elem  = top_src->klass()->as_array_klass()->element_type()->basic_type();
     BasicType dest_elem = top_dest->klass()->as_array_klass()->element_type()->basic_type();
-    if (is_reference_type(src_elem))   src_elem  = T_OBJECT;
-    if (is_reference_type(dest_elem))  dest_elem = T_OBJECT;
+    if (src_elem  == T_ARRAY)  src_elem  = T_OBJECT;
+    if (dest_elem == T_ARRAY)  dest_elem = T_OBJECT;
 
     if (src_elem == dest_elem && src_elem == T_OBJECT) {
       // If both arrays are object arrays then having the exact types
       // for both will remove the need for a subtype check at runtime
       // before the call and may make it possible to pick a faster copy
@@ -4793,21 +5135,33 @@
         uncommon_trap(Deoptimization::Reason_intrinsic,
                       Deoptimization::Action_make_not_entrant);
         assert(stopped(), "Should be stopped");
       }
     }
+
+    const TypeKlassPtr* dest_klass_t = _gvn.type(dest_klass)->is_klassptr();
+    const Type* toop = TypeOopPtr::make_from_klass(dest_klass_t->klass());
+    src = _gvn.transform(new CheckCastPPNode(control(), src, toop));
+    src_type = _gvn.type(src);
+    top_src  = src_type->isa_aryptr();
+
+    if (top_dest != NULL && !top_dest->elem()->isa_valuetype() && !top_dest->is_not_flat()) {
+      generate_valueArray_guard(dest_klass, slow_region);
+    }
+
+    if (top_src != NULL && !top_src->elem()->isa_valuetype() && !top_src->is_not_flat()) {
+      Node* src_klass = load_object_klass(src);
+      generate_valueArray_guard(src_klass, slow_region);
+    }
+
     {
       PreserveJVMState pjvms(this);
       set_control(_gvn.transform(slow_region));
       uncommon_trap(Deoptimization::Reason_intrinsic,
                     Deoptimization::Action_make_not_entrant);
       assert(stopped(), "Should be stopped");
     }
-
-    const TypeKlassPtr* dest_klass_t = _gvn.type(dest_klass)->is_klassptr();
-    const Type *toop = TypeOopPtr::make_from_klass(dest_klass_t->klass());
-    src = _gvn.transform(new CheckCastPPNode(control(), src, toop));
   }
 
   arraycopy_move_allocation_here(alloc, dest, saved_jvms, saved_reexecute_sp, new_idx);
 
   if (stopped()) {
diff a/src/hotspot/share/opto/matcher.cpp b/src/hotspot/share/opto/matcher.cpp
--- a/src/hotspot/share/opto/matcher.cpp
+++ b/src/hotspot/share/opto/matcher.cpp
@@ -170,10 +170,56 @@
     }
   }
 }
 #endif
 
+// Array of RegMask, one per returned values (value type instances can
+// be returned as multiple return values, one per field)
+RegMask* Matcher::return_values_mask(const TypeTuple *range) {
+  uint cnt = range->cnt() - TypeFunc::Parms;
+  if (cnt == 0) {
+    return NULL;
+  }
+  RegMask* mask = NEW_RESOURCE_ARRAY(RegMask, cnt);
+
+  if (!ValueTypeReturnedAsFields) {
+    // Get ideal-register return type
+    uint ireg = range->field_at(TypeFunc::Parms)->ideal_reg();
+    // Get machine return register
+    OptoRegPair regs = return_value(ireg, false);
+
+    // And mask for same
+    mask[0].Clear();
+    mask[0].Insert(regs.first());
+    if (OptoReg::is_valid(regs.second())) {
+      mask[0].Insert(regs.second());
+    }
+  } else {
+    BasicType* sig_bt = NEW_RESOURCE_ARRAY(BasicType, cnt);
+    VMRegPair* vm_parm_regs = NEW_RESOURCE_ARRAY(VMRegPair, cnt);
+
+    for (uint i = 0; i < cnt; i++) {
+      sig_bt[i] = range->field_at(i+TypeFunc::Parms)->basic_type();
+    }
+
+    int regs = SharedRuntime::java_return_convention(sig_bt, vm_parm_regs, cnt);
+    assert(regs > 0, "should have been tested during graph construction");
+    for (uint i = 0; i < cnt; i++) {
+      mask[i].Clear();
+
+      OptoReg::Name reg1 = OptoReg::as_OptoReg(vm_parm_regs[i].first());
+      if (OptoReg::is_valid(reg1)) {
+        mask[i].Insert(reg1);
+      }
+      OptoReg::Name reg2 = OptoReg::as_OptoReg(vm_parm_regs[i].second());
+      if (OptoReg::is_valid(reg2)) {
+        mask[i].Insert(reg2);
+      }
+    }
+  }
+  return mask;
+}
 
 //---------------------------match---------------------------------------------
 void Matcher::match( ) {
   if( MaxLabelRootDepth < 100 ) { // Too small?
     assert(false, "invalid MaxLabelRootDepth, increase it to 100 minimum");
@@ -185,33 +231,22 @@
 #ifdef _LP64
   // Pointers take 2 slots in 64-bit land
   _return_addr_mask.Insert(OptoReg::add(return_addr(),1));
 #endif
 
-  // Map a Java-signature return type into return register-value
-  // machine registers for 0, 1 and 2 returned values.
-  const TypeTuple *range = C->tf()->range();
-  if( range->cnt() > TypeFunc::Parms ) { // If not a void function
-    // Get ideal-register return type
-    uint ireg = range->field_at(TypeFunc::Parms)->ideal_reg();
-    // Get machine return register
-    uint sop = C->start()->Opcode();
-    OptoRegPair regs = return_value(ireg, false);
-
-    // And mask for same
-    _return_value_mask = RegMask(regs.first());
-    if( OptoReg::is_valid(regs.second()) )
-      _return_value_mask.Insert(regs.second());
-  }
+  // Map Java-signature return types into return register-value
+  // machine registers.
+  const TypeTuple *range = C->tf()->range_cc();
+  _return_values_mask = return_values_mask(range);
 
   // ---------------
   // Frame Layout
 
   // Need the method signature to determine the incoming argument types,
   // because the types determine which registers the incoming arguments are
   // in, and this affects the matched code.
-  const TypeTuple *domain = C->tf()->domain();
+  const TypeTuple *domain = C->tf()->domain_cc();
   uint             argcnt = domain->cnt() - TypeFunc::Parms;
   BasicType *sig_bt        = NEW_RESOURCE_ARRAY( BasicType, argcnt );
   VMRegPair *vm_parm_regs  = NEW_RESOURCE_ARRAY( VMRegPair, argcnt );
   _parm_regs               = NEW_RESOURCE_ARRAY( OptoRegPair, argcnt );
   _calling_convention_mask = NEW_RESOURCE_ARRAY( RegMask, argcnt );
@@ -473,10 +508,29 @@
   // Add in the incoming argument area
   OptoReg::Name init_in = OptoReg::add(_old_SP, C->out_preserve_stack_slots());
   for (i = init_in; i < _in_arg_limit; i = OptoReg::add(i,1)) {
     C->FIRST_STACK_mask().Insert(i);
   }
+
+  // Check if the method has a reserved entry in the argument stack area that
+  // should not be used for spilling because it may hold the return address.
+  if (!C->is_osr_compilation() && C->method() != NULL && C->method()->has_scalarized_args()) {
+    ExtendedSignature sig_cc = ExtendedSignature(C->method()->get_sig_cc(), SigEntryFilter());
+    for (int off = 0; !sig_cc.at_end(); ) {
+      BasicType bt = (*sig_cc)._bt;
+      off += type2size[bt];
+      while (SigEntry::next_is_reserved(sig_cc, bt)) {
+        // Remove reserved stack slot from mask to avoid spilling
+        OptoRegPair reg = _parm_regs[off];
+        assert(OptoReg::is_valid(reg.first()), "invalid reserved register");
+        C->FIRST_STACK_mask().Remove(reg.first());
+        C->FIRST_STACK_mask().Remove(reg.first()+1); // Always occupies two stack slots
+        off += type2size[bt];
+      }
+    }
+  }
+
   // Add in all bits past the outgoing argument area
   guarantee(RegMask::can_represent_arg(OptoReg::add(_out_arg_limit,-1)),
             "must be able to represent all call arguments in reg mask");
   OptoReg::Name init = _out_arg_limit;
   for (i = init; RegMask::can_represent(i); i = OptoReg::add(i,1)) {
@@ -665,16 +719,15 @@
         soe_cnt++;
 
   // Input RegMask array shared by all Returns.
   // The type for doubles and longs has a count of 2, but
   // there is only 1 returned value
-  uint ret_edge_cnt = TypeFunc::Parms + ((C->tf()->range()->cnt() == TypeFunc::Parms) ? 0 : 1);
+  uint ret_edge_cnt = C->tf()->range_cc()->cnt();
   RegMask *ret_rms  = init_input_masks( ret_edge_cnt + soe_cnt, _return_addr_mask, c_frame_ptr_mask );
-  // Returns have 0 or 1 returned values depending on call signature.
-  // Return register is specified by return_value in the AD file.
-  if (ret_edge_cnt > TypeFunc::Parms)
-    ret_rms[TypeFunc::Parms+0] = _return_value_mask;
+  for (i = TypeFunc::Parms; i < ret_edge_cnt; i++) {
+    ret_rms[i] = _return_values_mask[i-TypeFunc::Parms];
+  }
 
   // Input RegMask array shared by all Rethrows.
   uint reth_edge_cnt = TypeFunc::Parms+1;
   RegMask *reth_rms  = init_input_masks( reth_edge_cnt + soe_cnt, _return_addr_mask, c_frame_ptr_mask );
   // Rethrow takes exception oop only, but in the argument 0 slot.
@@ -737,11 +790,11 @@
       default          : ShouldNotReachHere();
     }
   }
 
   // Next unused projection number from Start.
-  int proj_cnt = C->tf()->domain()->cnt();
+  int proj_cnt = C->tf()->domain_cc()->cnt();
 
   // Do all the save-on-entry registers.  Make projections from Start for
   // them, and give them a use at the exit points.  To the allocator, they
   // look like incoming register arguments.
   for( i = 0; i < _last_Mach_Reg; i++ ) {
@@ -987,11 +1040,15 @@
               m->as_MachMemBar()->set_adr_type(n->adr_type());
             }
           } else {                  // Nothing the matcher cares about
             if (n->is_Proj() && n->in(0) != NULL && n->in(0)->is_Multi()) {       // Projections?
               // Convert to machine-dependent projection
-              m = n->in(0)->as_Multi()->match( n->as_Proj(), this );
+              RegMask* mask = NULL;
+              if (n->in(0)->is_Call()) {
+                mask = return_values_mask(n->in(0)->as_Call()->tf()->range_cc());
+              }
+              m = n->in(0)->as_Multi()->match(n->as_Proj(), this, mask);
 #ifdef ASSERT
               _new2old_map.map(m->_idx, n);
 #endif
               if (m->in(0) != NULL) // m might be top
                 collect_null_checks(m, n);
@@ -1132,11 +1189,11 @@
   const TypeTuple *domain;
   ciMethod*        method = NULL;
   bool             is_method_handle_invoke = false;  // for special kill effects
   if( sfpt->is_Call() ) {
     call = sfpt->as_Call();
-    domain = call->tf()->domain();
+    domain = call->tf()->domain_cc();
     cnt = domain->cnt();
 
     // Match just the call, nothing else
     MachNode *m = match_tree(call);
     if (C->failing())  return NULL;
@@ -1207,17 +1264,20 @@
   if( call != NULL && call->is_CallRuntime() )
     out_arg_limit_per_call = OptoReg::add(out_arg_limit_per_call,C->varargs_C_out_slots_killed());
 
 
   // Do the normal argument list (parameters) register masks
-  int argcnt = cnt - TypeFunc::Parms;
+  // Null entry point is a special cast where the target of the call
+  // is in a register.
+  int adj = (call != NULL && call->entry_point() == NULL) ? 1 : 0;
+  int argcnt = cnt - TypeFunc::Parms - adj;
   if( argcnt > 0 ) {          // Skip it all if we have no args
     BasicType *sig_bt  = NEW_RESOURCE_ARRAY( BasicType, argcnt );
     VMRegPair *parm_regs = NEW_RESOURCE_ARRAY( VMRegPair, argcnt );
     int i;
     for( i = 0; i < argcnt; i++ ) {
-      sig_bt[i] = domain->field_at(i+TypeFunc::Parms)->basic_type();
+      sig_bt[i] = domain->field_at(i+TypeFunc::Parms+adj)->basic_type();
     }
     // V-call to pick proper calling convention
     call->calling_convention( sig_bt, parm_regs, argcnt );
 
 #ifdef ASSERT
@@ -1254,23 +1314,25 @@
     // Return results now can have 2 bits returned.
     // Compute max over all outgoing arguments both per call-site
     // and over the entire method.
     for( i = 0; i < argcnt; i++ ) {
       // Address of incoming argument mask to fill in
-      RegMask *rm = &mcall->_in_rms[i+TypeFunc::Parms];
+      RegMask *rm = &mcall->_in_rms[i+TypeFunc::Parms+adj];
       if( !parm_regs[i].first()->is_valid() &&
           !parm_regs[i].second()->is_valid() ) {
         continue;               // Avoid Halves
       }
       // Grab first register, adjust stack slots and insert in mask.
       OptoReg::Name reg1 = warp_outgoing_stk_arg(parm_regs[i].first(), begin_out_arg_area, out_arg_limit_per_call );
-      if (OptoReg::is_valid(reg1))
+      if (OptoReg::is_valid(reg1)) {
         rm->Insert( reg1 );
+      }
       // Grab second register (if any), adjust stack slots and insert in mask.
       OptoReg::Name reg2 = warp_outgoing_stk_arg(parm_regs[i].second(), begin_out_arg_area, out_arg_limit_per_call );
-      if (OptoReg::is_valid(reg2))
+      if (OptoReg::is_valid(reg2)) {
         rm->Insert( reg2 );
+      }
     } // End of for all arguments
 
     // Compute number of stack slots needed to restore stack in case of
     // Pascal-style argument popping.
     mcall->_argsize = out_arg_limit_per_call - begin_out_arg_area;
@@ -1286,11 +1348,11 @@
     // Kill the outgoing argument area, including any non-argument holes and
     // any legacy C-killed slots.  Use Fat-Projections to do the killing.
     // Since the max-per-method covers the max-per-call-site and debug info
     // is excluded on the max-per-method basis, debug info cannot land in
     // this killed area.
-    uint r_cnt = mcall->tf()->range()->cnt();
+    uint r_cnt = mcall->tf()->range_sig()->cnt();
     MachProjNode *proj = new MachProjNode( mcall, r_cnt+10000, RegMask::Empty, MachProjNode::fat_proj );
     if (!RegMask::can_represent_arg(OptoReg::Name(out_arg_limit_per_call-1))) {
       C->record_method_not_compilable("unsupported outgoing calling sequence");
     } else {
       for (int i = begin_out_arg_area; i < out_arg_limit_per_call; i++)
@@ -1307,11 +1369,11 @@
     jvms->set_map(sfpt);
   }
 
   // Debug inputs begin just after the last incoming parameter
   assert((mcall == NULL) || (mcall->jvms() == NULL) ||
-         (mcall->jvms()->debug_start() + mcall->_jvmadj == mcall->tf()->domain()->cnt()), "");
+         (mcall->jvms()->debug_start() + mcall->_jvmadj == mcall->tf()->domain_cc()->cnt()), "");
 
   // Move the OopMap
   msfpt->_oop_map = sfpt->_oop_map;
 
   // Add additional edges.
@@ -2271,10 +2333,17 @@
       n->set_req(2, pair2);
       n->del_req(4);
       n->del_req(3);
       break;
     }
+    case Op_ClearArray: {
+      Node* pair = new BinaryNode(n->in(2), n->in(3));
+      n->set_req(2, pair);
+      n->set_req(3, n->in(4));
+      n->del_req(4);
+      break;
+    }
     default:
       break;
   }
 }
 
diff a/src/hotspot/share/opto/matcher.hpp b/src/hotspot/share/opto/matcher.hpp
--- a/src/hotspot/share/opto/matcher.hpp
+++ b/src/hotspot/share/opto/matcher.hpp
@@ -253,10 +253,12 @@
   void match();
 
   // Helper for match
   OptoReg::Name warp_incoming_stk_arg( VMReg reg );
 
+  RegMask* return_values_mask(const TypeTuple *range);
+
   // Transform, then walk.  Does implicit DCE while walking.
   // Name changed from "transform" to avoid it being virtual.
   Node *xform( Node *old_space_node, int Nodes );
 
   // Match a single Ideal Node - turn it into a 1-Node tree; Label & Reduce.
@@ -377,11 +379,11 @@
   virtual OptoReg::Name return_addr() const;
   RegMask              _return_addr_mask;
   // Return value register.  On Intel it is EAX.  On Sparc i0/o0.
   static OptoRegPair   return_value(uint ideal_reg, bool is_outgoing);
   static OptoRegPair c_return_value(uint ideal_reg, bool is_outgoing);
-  RegMask                     _return_value_mask;
+  RegMask*             _return_values_mask;
   // Inline Cache Register
   static OptoReg::Name  inline_cache_reg();
   static int            inline_cache_reg_encode();
 
   // Register for DIVI projection of divmodI
diff a/src/hotspot/share/opto/node.hpp b/src/hotspot/share/opto/node.hpp
--- a/src/hotspot/share/opto/node.hpp
+++ b/src/hotspot/share/opto/node.hpp
@@ -99,16 +99,18 @@
 class MachIfNode;
 class MachJumpNode;
 class MachNode;
 class MachNullCheckNode;
 class MachProjNode;
+class MachPrologNode;
 class MachReturnNode;
 class MachSafePointNode;
 class MachSpillCopyNode;
 class MachTempNode;
 class MachMergeNode;
 class MachMemBarNode;
+class MachVEPNode;
 class Matcher;
 class MemBarNode;
 class MemBarStoreStoreNode;
 class MemNode;
 class MergeMemNode;
@@ -147,10 +149,13 @@
 class SubNode;
 class SubTypeCheckNode;
 class Type;
 class TypeNode;
 class UnlockNode;
+class ValueTypeBaseNode;
+class ValueTypeNode;
+class ValueTypePtrNode;
 class VectorNode;
 class LoadVectorNode;
 class StoreVectorNode;
 class VectorSet;
 typedef void (*NFunc)(Node&,void*);
@@ -659,10 +664,12 @@
       DEFINE_CLASS_ID(MachConstantBase, Mach, 4)
       DEFINE_CLASS_ID(MachConstant,     Mach, 5)
         DEFINE_CLASS_ID(MachJump,       MachConstant, 0)
       DEFINE_CLASS_ID(MachMerge,        Mach, 6)
       DEFINE_CLASS_ID(MachMemBar,       Mach, 7)
+      DEFINE_CLASS_ID(MachProlog,       Mach, 8)
+      DEFINE_CLASS_ID(MachVEP,          Mach, 9)
 
     DEFINE_CLASS_ID(Type,  Node, 2)
       DEFINE_CLASS_ID(Phi,   Type, 0)
       DEFINE_CLASS_ID(ConstraintCast, Type, 1)
         DEFINE_CLASS_ID(CastII, ConstraintCast, 0)
@@ -673,10 +680,13 @@
         DEFINE_CLASS_ID(DecodeN, DecodeNarrowPtr, 0)
         DEFINE_CLASS_ID(DecodeNKlass, DecodeNarrowPtr, 1)
       DEFINE_CLASS_ID(EncodeNarrowPtr, Type, 6)
         DEFINE_CLASS_ID(EncodeP, EncodeNarrowPtr, 0)
         DEFINE_CLASS_ID(EncodePKlass, EncodeNarrowPtr, 1)
+      DEFINE_CLASS_ID(ValueTypeBase, Type, 8)
+        DEFINE_CLASS_ID(ValueType, ValueTypeBase, 0)
+        DEFINE_CLASS_ID(ValueTypePtr, ValueTypeBase, 1)
 
     DEFINE_CLASS_ID(Proj,  Node, 3)
       DEFINE_CLASS_ID(CatchProj, Proj, 0)
       DEFINE_CLASS_ID(JumpProj,  Proj, 1)
       DEFINE_CLASS_ID(IfProj,    Proj, 2)
@@ -850,16 +860,18 @@
   DEFINE_CLASS_QUERY(MachGoto)
   DEFINE_CLASS_QUERY(MachIf)
   DEFINE_CLASS_QUERY(MachJump)
   DEFINE_CLASS_QUERY(MachNullCheck)
   DEFINE_CLASS_QUERY(MachProj)
+  DEFINE_CLASS_QUERY(MachProlog)
   DEFINE_CLASS_QUERY(MachReturn)
   DEFINE_CLASS_QUERY(MachSafePoint)
   DEFINE_CLASS_QUERY(MachSpillCopy)
   DEFINE_CLASS_QUERY(MachTemp)
   DEFINE_CLASS_QUERY(MachMemBar)
   DEFINE_CLASS_QUERY(MachMerge)
+  DEFINE_CLASS_QUERY(MachVEP)
   DEFINE_CLASS_QUERY(Mem)
   DEFINE_CLASS_QUERY(MemBar)
   DEFINE_CLASS_QUERY(MemBarStoreStore)
   DEFINE_CLASS_QUERY(MergeMem)
   DEFINE_CLASS_QUERY(Mul)
@@ -878,10 +890,13 @@
   DEFINE_CLASS_QUERY(Start)
   DEFINE_CLASS_QUERY(Store)
   DEFINE_CLASS_QUERY(Sub)
   DEFINE_CLASS_QUERY(SubTypeCheck)
   DEFINE_CLASS_QUERY(Type)
+  DEFINE_CLASS_QUERY(ValueType)
+  DEFINE_CLASS_QUERY(ValueTypeBase)
+  DEFINE_CLASS_QUERY(ValueTypePtr)
   DEFINE_CLASS_QUERY(Vector)
   DEFINE_CLASS_QUERY(LoadVector)
   DEFINE_CLASS_QUERY(StoreVector)
   DEFINE_CLASS_QUERY(Unlock)
 
diff a/src/hotspot/share/prims/jvm.cpp b/src/hotspot/share/prims/jvm.cpp
--- a/src/hotspot/share/prims/jvm.cpp
+++ b/src/hotspot/share/prims/jvm.cpp
@@ -53,10 +53,11 @@
 #include "oops/method.hpp"
 #include "oops/recordComponent.hpp"
 #include "oops/objArrayKlass.hpp"
 #include "oops/objArrayOop.inline.hpp"
 #include "oops/oop.inline.hpp"
+#include "oops/valueArrayKlass.hpp"
 #include "prims/jvm_misc.hpp"
 #include "prims/jvmtiExport.hpp"
 #include "prims/jvmtiThreadState.hpp"
 #include "prims/nativeLookup.hpp"
 #include "prims/stackwalk.hpp"
@@ -703,10 +704,11 @@
 
   // Check if class of obj supports the Cloneable interface.
   // All arrays are considered to be cloneable (See JLS 20.1.5).
   // All j.l.r.Reference classes are considered non-cloneable.
   if (!klass->is_cloneable() ||
+       klass->is_value() ||
       (klass->is_instance_klass() &&
        InstanceKlass::cast(klass)->reference_type() != REF_NONE)) {
     ResourceMark rm(THREAD);
     THROW_MSG_0(vmSymbols::java_lang_CloneNotSupportedException(), klass->external_name());
   }
@@ -1287,30 +1289,39 @@
 
   Klass* klass = java_lang_Class::as_Klass(mirror);
   // Figure size of result array
   int size;
   if (klass->is_instance_klass()) {
-    size = InstanceKlass::cast(klass)->local_interfaces()->length();
+    InstanceKlass* ik = InstanceKlass::cast(klass);
+    size = ik->local_interfaces()->length();
+    if (ik->has_injected_identityObject()) {
+      size--;
+    }
   } else {
     assert(klass->is_objArray_klass() || klass->is_typeArray_klass(), "Illegal mirror klass");
-    size = 2;
+    size = 3;
   }
 
   // Allocate result array
   objArrayOop r = oopFactory::new_objArray(SystemDictionary::Class_klass(), size, CHECK_NULL);
   objArrayHandle result (THREAD, r);
   // Fill in result
   if (klass->is_instance_klass()) {
     // Regular instance klass, fill in all local interfaces
+    int cursor = 0;
     for (int index = 0; index < size; index++) {
-      Klass* k = InstanceKlass::cast(klass)->local_interfaces()->at(index);
-      result->obj_at_put(index, k->java_mirror());
+      InstanceKlass* ik = InstanceKlass::cast(klass);
+      Klass* k = ik->local_interfaces()->at(index);
+      if (!ik->has_injected_identityObject() || k != SystemDictionary::IdentityObject_klass()) {
+        result->obj_at_put(cursor++, k->java_mirror());
+      }
     }
   } else {
-    // All arrays implement java.lang.Cloneable and java.io.Serializable
+    // All arrays implement java.lang.Cloneable, java.io.Serializable and java.lang.IdentityObject
     result->obj_at_put(0, SystemDictionary::Cloneable_klass()->java_mirror());
     result->obj_at_put(1, SystemDictionary::Serializable_klass()->java_mirror());
+    result->obj_at_put(2, SystemDictionary::IdentityObject_klass()->java_mirror());
   }
   return (jobjectArray) JNIHandles::make_local(env, result());
 JVM_END
 
 
@@ -1926,14 +1937,18 @@
   return (jobjectArray)JNIHandles::make_local(env, result);
 }
 JVM_END
 
 static bool select_method(const methodHandle& method, bool want_constructor) {
+  bool is_ctor = (method->is_object_constructor() ||
+                  method->is_static_init_factory());
   if (want_constructor) {
-    return (method->is_initializer() && !method->is_static());
+    return is_ctor;
   } else {
-    return  (!method->is_initializer() && !method->is_overpass());
+    return (!is_ctor &&
+            !method->is_class_initializer() &&
+            !method->is_overpass());
   }
 }
 
 static jobjectArray get_class_declared_methods_helper(
                                   JNIEnv *env,
@@ -1991,10 +2006,12 @@
       // Otherwise should probably put a method that throws NSME
       result->obj_at_put(i, NULL);
     } else {
       oop m;
       if (want_constructor) {
+        assert(method->is_object_constructor() ||
+               method->is_static_init_factory(), "must be");
         m = Reflection::new_constructor(method, CHECK_NULL);
       } else {
         m = Reflection::new_method(method, false, CHECK_NULL);
       }
       result->obj_at_put(i, m);
@@ -2221,14 +2238,14 @@
   methodHandle m (THREAD, k->find_method(name, sig));
   if (m.is_null()) {
     THROW_MSG_0(vmSymbols::java_lang_RuntimeException(), "Unable to look up method in target class");
   }
   oop method;
-  if (!m->is_initializer() || m->is_static()) {
-    method = Reflection::new_method(m, true, CHECK_NULL);
-  } else {
+  if (m->is_object_constructor()) {
     method = Reflection::new_constructor(m, CHECK_NULL);
+  } else {
+    method = Reflection::new_method(m, true, CHECK_NULL);
   }
   return JNIHandles::make_local(method);
 }
 
 JVM_ENTRY(jobject, JVM_ConstantPoolGetMethodAt(JNIEnv *env, jobject obj, jobject unused, jint index))
@@ -2512,10 +2529,49 @@
   JvmtiVMObjectAllocEventCollector oam;
   oop asd = JavaAssertions::createAssertionStatusDirectives(CHECK_NULL);
   return JNIHandles::make_local(env, asd);
 JVM_END
 
+// Arrays support /////////////////////////////////////////////////////////////
+
+JVM_ENTRY(jboolean, JVM_ArrayIsAccessAtomic(JNIEnv *env, jclass unused, jobject array))
+  JVMWrapper("JVM_ArrayIsAccessAtomic");
+  oop o = JNIHandles::resolve(array);
+  Klass* k = o->klass();
+  if ((o == NULL) || (!k->is_array_klass())) {
+    THROW_0(vmSymbols::java_lang_IllegalArgumentException());
+  }
+  return ArrayKlass::cast(k)->element_access_is_atomic();
+JVM_END
+
+JVM_ENTRY(jobject, JVM_ArrayEnsureAccessAtomic(JNIEnv *env, jclass unused, jobject array))
+  JVMWrapper("JVM_ArrayEnsureAccessAtomic");
+  oop o = JNIHandles::resolve(array);
+  Klass* k = o->klass();
+  if ((o == NULL) || (!k->is_array_klass())) {
+    THROW_0(vmSymbols::java_lang_IllegalArgumentException());
+  }
+  if (k->is_valueArray_klass()) {
+    ValueArrayKlass* vk = ValueArrayKlass::cast(k);
+    if (!vk->element_access_is_atomic()) {
+      /**
+       * Need to decide how to implement:
+       *
+       * 1) Change to objArrayOop layout, therefore oop->klass() differs so
+       * then "<atomic>[Qfoo;" klass needs to subclass "[Qfoo;" to pass through
+       * "checkcast" & "instanceof"
+       *
+       * 2) Use extra header in the valueArrayOop to flag atomicity required and
+       * possibly per instance lock structure. Said info, could be placed in
+       * "trailer" rather than disturb the current arrayOop
+       */
+      Unimplemented();
+    }
+  }
+  return array;
+JVM_END
+
 // Verification ////////////////////////////////////////////////////////////////////////////////
 
 // Reflection for the verifier /////////////////////////////////////////////////////////////////
 
 // RedefineClasses support: bug 6214132 caused verification to fail.
@@ -2691,11 +2747,11 @@
   JVMWrapper("JVM_IsConstructorIx");
   ResourceMark rm(THREAD);
   Klass* k = java_lang_Class::as_Klass(JNIHandles::resolve_non_null(cls));
   k = JvmtiThreadState::class_to_verify_considering_redefinition(k, thread);
   Method* method = InstanceKlass::cast(k)->methods()->at(method_index);
-  return method->name() == vmSymbols::object_initializer_name();
+  return method->is_object_constructor();
 JVM_END
 
 
 JVM_ENTRY(jboolean, JVM_IsVMGeneratedMethodIx(JNIEnv *env, jclass cls, int method_index))
   JVMWrapper("JVM_IsVMGeneratedMethodIx");
@@ -3686,11 +3742,11 @@
   JVMWrapper("JVM_InvokeMethod");
   Handle method_handle;
   if (thread->stack_available((address) &method_handle) >= JVMInvokeMethodSlack) {
     method_handle = Handle(THREAD, JNIHandles::resolve(method));
     Handle receiver(THREAD, JNIHandles::resolve(obj));
-    objArrayHandle args(THREAD, objArrayOop(JNIHandles::resolve(args0)));
+    objArrayHandle args = oopFactory::ensure_objArray(JNIHandles::resolve(args0), CHECK_NULL);
     oop result = Reflection::invoke_method(method_handle(), receiver, args, CHECK_NULL);
     jobject res = JNIHandles::make_local(env, result);
     if (JvmtiExport::should_post_vm_object_alloc()) {
       oop ret_type = java_lang_reflect_Method::return_type(method_handle());
       assert(ret_type != NULL, "sanity check: ret_type oop must not be NULL!");
@@ -3707,12 +3763,12 @@
 JVM_END
 
 
 JVM_ENTRY(jobject, JVM_NewInstanceFromConstructor(JNIEnv *env, jobject c, jobjectArray args0))
   JVMWrapper("JVM_NewInstanceFromConstructor");
+  objArrayHandle args = oopFactory::ensure_objArray(JNIHandles::resolve(args0), CHECK_NULL);
   oop constructor_mirror = JNIHandles::resolve(c);
-  objArrayHandle args(THREAD, objArrayOop(JNIHandles::resolve(args0)));
   oop result = Reflection::invoke_constructor(constructor_mirror, args, CHECK_NULL);
   jobject res = JNIHandles::make_local(env, result);
   if (JvmtiExport::should_post_vm_object_alloc()) {
     JvmtiExport::post_vm_object_alloc(JavaThread::current(), result);
   }
diff a/src/hotspot/share/prims/jvmtiEnv.cpp b/src/hotspot/share/prims/jvmtiEnv.cpp
--- a/src/hotspot/share/prims/jvmtiEnv.cpp
+++ b/src/hotspot/share/prims/jvmtiEnv.cpp
@@ -2589,11 +2589,12 @@
   int id_index = (result_count - 1);
 
   for (FilteredFieldStream src_st(ik, true, true); !src_st.eos(); src_st.next()) {
     result_list[id_index--] = jfieldIDWorkaround::to_jfieldID(
                                             ik, src_st.offset(),
-                                            src_st.access_flags().is_static());
+                                            src_st.access_flags().is_static(),
+                                            src_st.field_descriptor().is_flattened());
   }
   assert(id_index == -1, "just checking");
   // Fill in the results
   *field_count_ptr = result_count;
   *fields_ptr = result_list;
diff a/src/hotspot/share/prims/jvmtiRedefineClasses.cpp b/src/hotspot/share/prims/jvmtiRedefineClasses.cpp
--- a/src/hotspot/share/prims/jvmtiRedefineClasses.cpp
+++ b/src/hotspot/share/prims/jvmtiRedefineClasses.cpp
@@ -561,12 +561,11 @@
     case JVM_CONSTANT_Invalid: // fall through
 
     // At this stage, String could be here, but not StringIndex
     case JVM_CONSTANT_StringIndex: // fall through
 
-    // At this stage JVM_CONSTANT_UnresolvedClassInError should not be
-    // here
+    // At this stage JVM_CONSTANT_UnresolvedClassInError should not be here
     case JVM_CONSTANT_UnresolvedClassInError: // fall through
 
     default:
     {
       // leave a breadcrumb
diff a/src/hotspot/share/prims/methodHandles.cpp b/src/hotspot/share/prims/methodHandles.cpp
--- a/src/hotspot/share/prims/methodHandles.cpp
+++ b/src/hotspot/share/prims/methodHandles.cpp
@@ -119,31 +119,31 @@
 
 // MemberName support
 
 // import java_lang_invoke_MemberName.*
 enum {
-  IS_METHOD            = java_lang_invoke_MemberName::MN_IS_METHOD,
-  IS_CONSTRUCTOR       = java_lang_invoke_MemberName::MN_IS_CONSTRUCTOR,
-  IS_FIELD             = java_lang_invoke_MemberName::MN_IS_FIELD,
-  IS_TYPE              = java_lang_invoke_MemberName::MN_IS_TYPE,
-  CALLER_SENSITIVE     = java_lang_invoke_MemberName::MN_CALLER_SENSITIVE,
-  REFERENCE_KIND_SHIFT = java_lang_invoke_MemberName::MN_REFERENCE_KIND_SHIFT,
-  REFERENCE_KIND_MASK  = java_lang_invoke_MemberName::MN_REFERENCE_KIND_MASK,
-  SEARCH_SUPERCLASSES  = java_lang_invoke_MemberName::MN_SEARCH_SUPERCLASSES,
-  SEARCH_INTERFACES    = java_lang_invoke_MemberName::MN_SEARCH_INTERFACES,
-  ALL_KINDS      = IS_METHOD | IS_CONSTRUCTOR | IS_FIELD | IS_TYPE
+  IS_METHOD             = java_lang_invoke_MemberName::MN_IS_METHOD,
+  IS_OBJECT_CONSTRUCTOR = java_lang_invoke_MemberName::MN_IS_OBJECT_CONSTRUCTOR,
+  IS_FIELD              = java_lang_invoke_MemberName::MN_IS_FIELD,
+  IS_TYPE               = java_lang_invoke_MemberName::MN_IS_TYPE,
+  CALLER_SENSITIVE      = java_lang_invoke_MemberName::MN_CALLER_SENSITIVE,
+  REFERENCE_KIND_SHIFT  = java_lang_invoke_MemberName::MN_REFERENCE_KIND_SHIFT,
+  REFERENCE_KIND_MASK   = java_lang_invoke_MemberName::MN_REFERENCE_KIND_MASK,
+  SEARCH_SUPERCLASSES   = java_lang_invoke_MemberName::MN_SEARCH_SUPERCLASSES,
+  SEARCH_INTERFACES     = java_lang_invoke_MemberName::MN_SEARCH_INTERFACES,
+  ALL_KINDS      = IS_METHOD | IS_OBJECT_CONSTRUCTOR | IS_FIELD | IS_TYPE
 };
 
 int MethodHandles::ref_kind_to_flags(int ref_kind) {
   assert(ref_kind_is_valid(ref_kind), "%d", ref_kind);
   int flags = (ref_kind << REFERENCE_KIND_SHIFT);
   if (ref_kind_is_field(ref_kind)) {
     flags |= IS_FIELD;
   } else if (ref_kind_is_method(ref_kind)) {
     flags |= IS_METHOD;
   } else if (ref_kind == JVM_REF_newInvokeSpecial) {
-    flags |= IS_CONSTRUCTOR;
+    flags |= IS_OBJECT_CONSTRUCTOR;
   }
   return flags;
 }
 
 Handle MethodHandles::resolve_MemberName_type(Handle mname, Klass* caller, TRAPS) {
@@ -158,11 +158,11 @@
   }
   Handle resolved;
   int flags = java_lang_invoke_MemberName::flags(mname());
   switch (flags & ALL_KINDS) {
     case IS_METHOD:
-    case IS_CONSTRUCTOR:
+    case IS_OBJECT_CONSTRUCTOR:
       resolved = SystemDictionary::find_method_handle_type(signature, caller, CHECK_(empty));
       break;
     case IS_FIELD:
       resolved = SystemDictionary::find_field_handle_type(signature, caller, CHECK_(empty));
       break;
@@ -301,12 +301,12 @@
 
   case CallInfo::direct_call:
     vmindex = Method::nonvirtual_vtable_index;
     if (m->is_static()) {
       flags |= IS_METHOD      | (JVM_REF_invokeStatic  << REFERENCE_KIND_SHIFT);
-    } else if (m->is_initializer()) {
-      flags |= IS_CONSTRUCTOR | (JVM_REF_invokeSpecial << REFERENCE_KIND_SHIFT);
+    } else if (m->is_object_constructor()) {
+      flags |= IS_OBJECT_CONSTRUCTOR | (JVM_REF_invokeSpecial << REFERENCE_KIND_SHIFT);
     } else {
       // "special" reflects that this is a direct call, not that it
       // necessarily originates from an invokespecial. We can also do
       // direct calls for private and/or final non-static methods.
       flags |= IS_METHOD      | (JVM_REF_invokeSpecial << REFERENCE_KIND_SHIFT);
@@ -339,10 +339,16 @@
 }
 
 oop MethodHandles::init_field_MemberName(Handle mname, fieldDescriptor& fd, bool is_setter) {
   int flags = (jushort)( fd.access_flags().as_short() & JVM_RECOGNIZED_FIELD_MODIFIERS );
   flags |= IS_FIELD | ((fd.is_static() ? JVM_REF_getStatic : JVM_REF_getField) << REFERENCE_KIND_SHIFT);
+  if (fd.is_flattenable()) {
+    flags |= JVM_ACC_FIELD_FLATTENABLE;
+  }
+    if (fd.is_flattened()) {
+    flags |= JVM_ACC_FIELD_FLATTENED;
+  }
   if (is_setter)  flags += ((JVM_REF_putField - JVM_REF_getField) << REFERENCE_KIND_SHIFT);
   int vmindex        = fd.offset();  // determines the field uniquely when combined with static bit
 
   oop mname_oop = mname();
   java_lang_invoke_MemberName::set_flags  (mname_oop, flags);
@@ -787,20 +793,23 @@
       }
       result.set_resolved_method_name(CHECK_(empty));
       oop mname2 = init_method_MemberName(mname, result);
       return Handle(THREAD, mname2);
     }
-  case IS_CONSTRUCTOR:
+  case IS_OBJECT_CONSTRUCTOR:
     {
       CallInfo result;
       LinkInfo link_info(defc, name, type, caller, access_check);
       {
         assert(!HAS_PENDING_EXCEPTION, "");
-        if (name == vmSymbols::object_initializer_name()) {
+        if (name != vmSymbols::object_initializer_name()) {
+          break;                // will throw after end of switch
+        } else if (type->is_void_method_signature()) {
           LinkResolver::resolve_special_call(result, Handle(), link_info, THREAD);
         } else {
-          break;                // will throw after end of switch
+          // LinkageError unless it returns something reasonable
+          LinkResolver::resolve_static_call(result, link_info, false, THREAD);
         }
         if (HAS_PENDING_EXCEPTION) {
           if (speculative_resolve) {
             CLEAR_PENDING_EXCEPTION;
           }
@@ -856,11 +865,11 @@
 
   if (have_defc && have_name && have_type)  return;  // nothing needed
 
   switch (flags & ALL_KINDS) {
   case IS_METHOD:
-  case IS_CONSTRUCTOR:
+  case IS_OBJECT_CONSTRUCTOR:
     {
       Method* vmtarget = java_lang_invoke_MemberName::vmtarget(mname());
       if (vmtarget == NULL) {
         THROW_MSG(vmSymbols::java_lang_IllegalArgumentException(), "nothing to expand");
       }
@@ -937,11 +946,11 @@
   }
   if (sig != NULL) {
     if (sig->starts_with(JVM_SIGNATURE_FUNC))
       match_flags &= ~(IS_FIELD | IS_TYPE);
     else
-      match_flags &= ~(IS_CONSTRUCTOR | IS_METHOD);
+      match_flags &= ~(IS_OBJECT_CONSTRUCTOR | IS_METHOD);
   }
 
   if ((match_flags & IS_TYPE) != 0) {
     // NYI, and Core Reflection works quite well for this query
   }
@@ -967,45 +976,45 @@
         match_flags = 0; break; // got tired of looking at overflow
       }
     }
   }
 
-  if ((match_flags & (IS_METHOD | IS_CONSTRUCTOR)) != 0) {
+  if ((match_flags & (IS_METHOD | IS_OBJECT_CONSTRUCTOR)) != 0) {
     // watch out for these guys:
     Symbol* init_name   = vmSymbols::object_initializer_name();
     Symbol* clinit_name = vmSymbols::class_initializer_name();
     if (name == clinit_name)  clinit_name = NULL; // hack for exposing <clinit>
-    bool negate_name_test = false;
-    // fix name so that it captures the intention of IS_CONSTRUCTOR
+    bool ctor_ok = true, sfac_ok = true;
+    // fix name so that it captures the intention of IS_OBJECT_CONSTRUCTOR
     if (!(match_flags & IS_METHOD)) {
       // constructors only
       if (name == NULL) {
         name = init_name;
       } else if (name != init_name) {
         return 0;               // no constructors of this method name
       }
-    } else if (!(match_flags & IS_CONSTRUCTOR)) {
+      sfac_ok = false;
+    } else if (!(match_flags & IS_OBJECT_CONSTRUCTOR)) {
       // methods only
-      if (name == NULL) {
-        name = init_name;
-        negate_name_test = true; // if we see the name, we *omit* the entry
-      } else if (name == init_name) {
-        return 0;               // no methods of this constructor name
-      }
+      ctor_ok = false;  // but sfac_ok is true, so we might find <init>
     } else {
       // caller will accept either sort; no need to adjust name
     }
     InstanceKlass* ik = InstanceKlass::cast(k);
     for (MethodStream st(ik, local_only, !search_intfc); !st.eos(); st.next()) {
       Method* m = st.method();
       Symbol* m_name = m->name();
       if (m_name == clinit_name)
         continue;
-      if (name != NULL && ((m_name != name) ^ negate_name_test))
+      if (name != NULL && m_name != name)
           continue;
       if (sig != NULL && m->signature() != sig)
         continue;
+      if (m_name == init_name) {  // might be either ctor or sfac
+        if (m->is_object_constructor()  && !ctor_ok)  continue;
+        if (m->is_static_init_factory() && !sfac_ok)  continue;
+      }
       // passed the filters
       if (rskip > 0) {
         --rskip;
       } else if (rfill < rlimit) {
         Handle result(thread, results->obj_at(rfill++));
@@ -1101,11 +1110,11 @@
 //
 
 #ifndef PRODUCT
 #define EACH_NAMED_CON(template, requirement) \
     template(java_lang_invoke_MemberName,MN_IS_METHOD) \
-    template(java_lang_invoke_MemberName,MN_IS_CONSTRUCTOR) \
+    template(java_lang_invoke_MemberName,MN_IS_OBJECT_CONSTRUCTOR) \
     template(java_lang_invoke_MemberName,MN_IS_FIELD) \
     template(java_lang_invoke_MemberName,MN_IS_TYPE) \
     template(java_lang_invoke_MemberName,MN_CALLER_SENSITIVE) \
     template(java_lang_invoke_MemberName,MN_SEARCH_SUPERCLASSES) \
     template(java_lang_invoke_MemberName,MN_SEARCH_INTERFACES) \
@@ -1230,11 +1239,11 @@
       return NULL;
     }
     if ((flags & ALL_KINDS) == IS_FIELD) {
       THROW_MSG_NULL(vmSymbols::java_lang_NoSuchFieldError(), "field resolution failed");
     } else if ((flags & ALL_KINDS) == IS_METHOD ||
-               (flags & ALL_KINDS) == IS_CONSTRUCTOR) {
+               (flags & ALL_KINDS) == IS_OBJECT_CONSTRUCTOR) {
       THROW_MSG_NULL(vmSymbols::java_lang_NoSuchMethodError(), "method resolution failed");
     } else {
       THROW_MSG_NULL(vmSymbols::java_lang_LinkageError(), "resolution failed");
     }
   }
diff a/src/hotspot/share/prims/unsafe.cpp b/src/hotspot/share/prims/unsafe.cpp
--- a/src/hotspot/share/prims/unsafe.cpp
+++ b/src/hotspot/share/prims/unsafe.cpp
@@ -29,16 +29,22 @@
 #include "classfile/classLoader.hpp"
 #include "classfile/vmSymbols.hpp"
 #include "jfr/jfrEvents.hpp"
 #include "memory/allocation.inline.hpp"
 #include "memory/resourceArea.hpp"
+#include "logging/log.hpp"
+#include "logging/logStream.hpp"
 #include "oops/access.inline.hpp"
 #include "oops/fieldStreams.inline.hpp"
 #include "oops/objArrayOop.inline.hpp"
 #include "oops/oop.inline.hpp"
 #include "oops/typeArrayOop.inline.hpp"
+#include "oops/valueArrayKlass.hpp"
+#include "oops/valueArrayOop.inline.hpp"
+#include "oops/valueKlass.inline.hpp"
 #include "prims/unsafe.hpp"
+#include "runtime/fieldDescriptor.inline.hpp"
 #include "runtime/globals.hpp"
 #include "runtime/handles.inline.hpp"
 #include "runtime/interfaceSupport.inline.hpp"
 #include "runtime/jniHandles.inline.hpp"
 #include "runtime/orderAccess.hpp"
@@ -143,11 +149,10 @@
 }
 jlong Unsafe_field_offset_from_byte_offset(jlong byte_offset) {
   return byte_offset;
 }
 
-
 ///// Data read/writes on the Java heap and in native (off-heap) memory
 
 /**
  * Helper class to wrap memory accesses in JavaThread::doing_unsafe_access()
  */
@@ -228,15 +233,15 @@
   void put(T x) {
     if (_obj == NULL) {
       GuardUnsafeAccess guard(_thread);
       RawAccess<>::store(addr(), normalize_for_write(x));
     } else {
+      assert(!_obj->is_value() || _obj->mark().is_larval_state(), "must be an object instance or a larval value");
       HeapAccess<>::store_at(_obj, _offset, normalize_for_write(x));
     }
   }
 
-
   T get_volatile() {
     if (_obj == NULL) {
       GuardUnsafeAccess guard(_thread);
       volatile T ret = RawAccess<MO_SEQ_CST>::load(addr());
       return normalize_for_read(ret);
@@ -254,10 +259,72 @@
       HeapAccess<MO_SEQ_CST>::store_at(_obj, _offset, normalize_for_write(x));
     }
   }
 };
 
+#ifdef ASSERT
+/*
+ * Get the field descriptor of the field of the given object at the given offset.
+ */
+static bool get_field_descriptor(oop p, jlong offset, fieldDescriptor* fd) {
+  bool found = false;
+  Klass* k = p->klass();
+  if (k->is_instance_klass()) {
+    InstanceKlass* ik = InstanceKlass::cast(k);
+    found = ik->find_field_from_offset((int)offset, false, fd);
+    if (!found && ik->is_mirror_instance_klass()) {
+      Klass* k2 = java_lang_Class::as_Klass(p);
+      if (k2->is_instance_klass()) {
+        ik = InstanceKlass::cast(k2);
+        found = ik->find_field_from_offset((int)offset, true, fd);
+      }
+    }
+  }
+  return found;
+}
+#endif // ASSERT
+
+static void assert_and_log_unsafe_value_access(oop p, jlong offset, ValueKlass* vk) {
+  Klass* k = p->klass();
+#ifdef ASSERT
+  if (k->is_instance_klass()) {
+    assert_field_offset_sane(p, offset);
+    fieldDescriptor fd;
+    bool found = get_field_descriptor(p, offset, &fd);
+    if (found) {
+      assert(found, "value field not found");
+      assert(fd.is_flattened(), "field not flat");
+    } else {
+      if (log_is_enabled(Trace, valuetypes)) {
+        log_trace(valuetypes)("not a field in %s at offset " SIZE_FORMAT_HEX,
+                              p->klass()->external_name(), offset);
+      }
+    }
+  } else if (k->is_valueArray_klass()) {
+    ValueArrayKlass* vak = ValueArrayKlass::cast(k);
+    int index = (offset - vak->array_header_in_bytes()) / vak->element_byte_size();
+    address dest = (address)((valueArrayOop)p)->value_at_addr(index, vak->layout_helper());
+    assert(dest == (cast_from_oop<address>(p) + offset), "invalid offset");
+  } else {
+    ShouldNotReachHere();
+  }
+#endif // ASSERT
+  if (log_is_enabled(Trace, valuetypes)) {
+    if (k->is_valueArray_klass()) {
+      ValueArrayKlass* vak = ValueArrayKlass::cast(k);
+      int index = (offset - vak->array_header_in_bytes()) / vak->element_byte_size();
+      address dest = (address)((valueArrayOop)p)->value_at_addr(index, vak->layout_helper());
+      log_trace(valuetypes)("%s array type %s index %d element size %d offset " SIZE_FORMAT_HEX " at " INTPTR_FORMAT,
+                            p->klass()->external_name(), vak->external_name(),
+                            index, vak->element_byte_size(), offset, p2i(dest));
+    } else {
+      log_trace(valuetypes)("%s field type %s at offset " SIZE_FORMAT_HEX,
+                            p->klass()->external_name(), vk->external_name(), offset);
+    }
+  }
+}
+
 // These functions allow a null base pointer with an arbitrary address.
 // But if the base pointer is non-null, the offset should make some sense.
 // That is, it should be in the range [0, MAX_OBJECT_SIZE].
 UNSAFE_ENTRY(jobject, Unsafe_GetReference(JNIEnv *env, jobject unsafe, jobject obj, jlong offset)) {
   oop p = JNIHandles::resolve(obj);
@@ -268,13 +335,72 @@
 
 UNSAFE_ENTRY(void, Unsafe_PutReference(JNIEnv *env, jobject unsafe, jobject obj, jlong offset, jobject x_h)) {
   oop x = JNIHandles::resolve(x_h);
   oop p = JNIHandles::resolve(obj);
   assert_field_offset_sane(p, offset);
+  assert(!p->is_value() || p->mark().is_larval_state(), "must be an object instance or a larval value");
   HeapAccess<ON_UNKNOWN_OOP_REF>::oop_store_at(p, offset, x);
 } UNSAFE_END
 
+UNSAFE_ENTRY(jlong, Unsafe_ValueHeaderSize(JNIEnv *env, jobject unsafe, jclass c)) {
+  Klass* k = java_lang_Class::as_Klass(JNIHandles::resolve_non_null(c));
+  ValueKlass* vk = ValueKlass::cast(k);
+  return vk->first_field_offset();
+} UNSAFE_END
+
+UNSAFE_ENTRY(jboolean, Unsafe_IsFlattenedArray(JNIEnv *env, jobject unsafe, jclass c)) {
+  Klass* k = java_lang_Class::as_Klass(JNIHandles::resolve_non_null(c));
+  return k->is_valueArray_klass();
+} UNSAFE_END
+
+UNSAFE_ENTRY(jobject, Unsafe_UninitializedDefaultValue(JNIEnv *env, jobject unsafe, jclass vc)) {
+  Klass* k = java_lang_Class::as_Klass(JNIHandles::resolve_non_null(vc));
+  ValueKlass* vk = ValueKlass::cast(k);
+  oop v = vk->default_value();
+  return JNIHandles::make_local(env, v);
+} UNSAFE_END
+
+UNSAFE_ENTRY(jobject, Unsafe_GetValue(JNIEnv *env, jobject unsafe, jobject obj, jlong offset, jclass vc)) {
+  oop base = JNIHandles::resolve(obj);
+  Klass* k = java_lang_Class::as_Klass(JNIHandles::resolve_non_null(vc));
+  ValueKlass* vk = ValueKlass::cast(k);
+  assert_and_log_unsafe_value_access(base, offset, vk);
+  Handle base_h(THREAD, base);
+  oop v = vk->read_flattened_field(base_h(), offset, CHECK_NULL);
+  return JNIHandles::make_local(env, v);
+} UNSAFE_END
+
+UNSAFE_ENTRY(void, Unsafe_PutValue(JNIEnv *env, jobject unsafe, jobject obj, jlong offset, jclass vc, jobject value)) {
+  oop base = JNIHandles::resolve(obj);
+  Klass* k = java_lang_Class::as_Klass(JNIHandles::resolve_non_null(vc));
+  ValueKlass* vk = ValueKlass::cast(k);
+  assert(!base->is_value() || base->mark().is_larval_state(), "must be an object instance or a larval value");
+  assert_and_log_unsafe_value_access(base, offset, vk);
+  oop v = JNIHandles::resolve(value);
+  vk->write_flattened_field(base, offset, v, CHECK);
+} UNSAFE_END
+
+UNSAFE_ENTRY(jobject, Unsafe_MakePrivateBuffer(JNIEnv *env, jobject unsafe, jobject value)) {
+  oop v = JNIHandles::resolve_non_null(value);
+  assert(v->is_value(), "must be a value instance");
+  Handle vh(THREAD, v);
+  ValueKlass* vk = ValueKlass::cast(v->klass());
+  instanceOop new_value = vk->allocate_instance(CHECK_NULL);
+  vk->value_copy_oop_to_new_oop(vh(),  new_value);
+  markWord mark = new_value->mark();
+  new_value->set_mark(mark.enter_larval_state());
+  return JNIHandles::make_local(env, new_value);
+} UNSAFE_END
+
+UNSAFE_ENTRY(jobject, Unsafe_FinishPrivateBuffer(JNIEnv *env, jobject unsafe, jobject value)) {
+  oop v = JNIHandles::resolve(value);
+  assert(v->mark().is_larval_state(), "must be a larval value");
+  markWord mark = v->mark();
+  v->set_mark(mark.exit_larval_state());
+  return JNIHandles::make_local(env, v);
+} UNSAFE_END
+
 UNSAFE_ENTRY(jobject, Unsafe_GetReferenceVolatile(JNIEnv *env, jobject unsafe, jobject obj, jlong offset)) {
   oop p = JNIHandles::resolve(obj);
   assert_field_offset_sane(p, offset);
   oop v = HeapAccess<MO_SEQ_CST | ON_UNKNOWN_OOP_REF>::oop_load_at(p, offset);
   return JNIHandles::make_local(env, v);
@@ -607,10 +733,15 @@
   } else if (k->is_typeArray_klass()) {
     TypeArrayKlass* tak = TypeArrayKlass::cast(k);
     base  = tak->array_header_in_bytes();
     assert(base == arrayOopDesc::base_offset_in_bytes(tak->element_type()), "array_header_size semantics ok");
     scale = (1 << tak->log2_element_size());
+  } else if (k->is_valueArray_klass()) {
+    ValueArrayKlass* vak = ValueArrayKlass::cast(k);
+    ValueKlass* vklass = vak->element_klass();
+    base = vak->array_header_in_bytes();
+    scale = vak->element_byte_size();
   } else {
     ShouldNotReachHere();
   }
 }
 
@@ -642,10 +773,16 @@
   // but requires it to be linear in byte offset.
   return field_offset_from_byte_offset(scale) - field_offset_from_byte_offset(0);
 } UNSAFE_END
 
 
+UNSAFE_ENTRY(jlong, Unsafe_GetObjectSize0(JNIEnv* env, jobject o, jobject obj))
+  oop p = JNIHandles::resolve(obj);
+  return Universe::heap()->obj_size(p) * HeapWordSize;
+UNSAFE_END
+
+
 static inline void throw_new(JNIEnv *env, const char *ename) {
   jclass cls = env->FindClass(ename);
   if (env->ExceptionCheck()) {
     env->ExceptionClear();
     tty->print_cr("Unsafe: cannot throw %s because FindClass has failed", ename);
@@ -1066,22 +1203,30 @@
 
 #define CC (char*)  /*cast a literal from (const char*)*/
 #define FN_PTR(f) CAST_FROM_FN_PTR(void*, &f)
 
 #define DECLARE_GETPUTOOP(Type, Desc) \
-    {CC "get" #Type,      CC "(" OBJ "J)" #Desc,       FN_PTR(Unsafe_Get##Type)}, \
-    {CC "put" #Type,      CC "(" OBJ "J" #Desc ")V",   FN_PTR(Unsafe_Put##Type)}, \
-    {CC "get" #Type "Volatile",      CC "(" OBJ "J)" #Desc,       FN_PTR(Unsafe_Get##Type##Volatile)}, \
-    {CC "put" #Type "Volatile",      CC "(" OBJ "J" #Desc ")V",   FN_PTR(Unsafe_Put##Type##Volatile)}
+    {CC "get"  #Type,      CC "(" OBJ "J)" #Desc,                 FN_PTR(Unsafe_Get##Type)}, \
+    {CC "put"  #Type,      CC "(" OBJ "J" #Desc ")V",             FN_PTR(Unsafe_Put##Type)}, \
+    {CC "get"  #Type "Volatile",      CC "(" OBJ "J)" #Desc,      FN_PTR(Unsafe_Get##Type##Volatile)}, \
+    {CC "put"  #Type "Volatile",      CC "(" OBJ "J" #Desc ")V",  FN_PTR(Unsafe_Put##Type##Volatile)}
 
 
 static JNINativeMethod jdk_internal_misc_Unsafe_methods[] = {
     {CC "getReference",         CC "(" OBJ "J)" OBJ "",   FN_PTR(Unsafe_GetReference)},
     {CC "putReference",         CC "(" OBJ "J" OBJ ")V",  FN_PTR(Unsafe_PutReference)},
     {CC "getReferenceVolatile", CC "(" OBJ "J)" OBJ,      FN_PTR(Unsafe_GetReferenceVolatile)},
     {CC "putReferenceVolatile", CC "(" OBJ "J" OBJ ")V",  FN_PTR(Unsafe_PutReferenceVolatile)},
 
+    {CC "isFlattenedArray", CC "(" CLS ")Z",                     FN_PTR(Unsafe_IsFlattenedArray)},
+    {CC "getValue",         CC "(" OBJ "J" CLS ")" OBJ,          FN_PTR(Unsafe_GetValue)},
+    {CC "putValue",         CC "(" OBJ "J" CLS OBJ ")V",         FN_PTR(Unsafe_PutValue)},
+    {CC "uninitializedDefaultValue", CC "(" CLS ")" OBJ,         FN_PTR(Unsafe_UninitializedDefaultValue)},
+    {CC "makePrivateBuffer",     CC "(" OBJ ")" OBJ,             FN_PTR(Unsafe_MakePrivateBuffer)},
+    {CC "finishPrivateBuffer",   CC "(" OBJ ")" OBJ,             FN_PTR(Unsafe_FinishPrivateBuffer)},
+    {CC "valueHeaderSize",       CC "(" CLS ")J",                FN_PTR(Unsafe_ValueHeaderSize)},
+
     {CC "getUncompressedObject", CC "(" ADR ")" OBJ,  FN_PTR(Unsafe_GetUncompressedObject)},
 
     DECLARE_GETPUTOOP(Boolean, Z),
     DECLARE_GETPUTOOP(Byte, B),
     DECLARE_GETPUTOOP(Short, S),
@@ -1100,10 +1245,11 @@
     {CC "staticFieldOffset0", CC "(" FLD ")J",           FN_PTR(Unsafe_StaticFieldOffset0)},
     {CC "staticFieldBase0",   CC "(" FLD ")" OBJ,        FN_PTR(Unsafe_StaticFieldBase0)},
     {CC "ensureClassInitialized0", CC "(" CLS ")V",      FN_PTR(Unsafe_EnsureClassInitialized0)},
     {CC "arrayBaseOffset0",   CC "(" CLS ")I",           FN_PTR(Unsafe_ArrayBaseOffset0)},
     {CC "arrayIndexScale0",   CC "(" CLS ")I",           FN_PTR(Unsafe_ArrayIndexScale0)},
+    {CC "getObjectSize0",     CC "(Ljava/lang/Object;)J", FN_PTR(Unsafe_GetObjectSize0)},
 
     {CC "defineClass0",       CC "(" DC_Args ")" CLS,    FN_PTR(Unsafe_DefineClass0)},
     {CC "allocateInstance",   CC "(" CLS ")" OBJ,        FN_PTR(Unsafe_AllocateInstance)},
     {CC "throwException",     CC "(" THR ")V",           FN_PTR(Unsafe_ThrowException)},
     {CC "compareAndSetReference",CC "(" OBJ "J" OBJ "" OBJ ")Z", FN_PTR(Unsafe_CompareAndSetReference)},
diff a/src/hotspot/share/prims/whitebox.cpp b/src/hotspot/share/prims/whitebox.cpp
--- a/src/hotspot/share/prims/whitebox.cpp
+++ b/src/hotspot/share/prims/whitebox.cpp
@@ -42,21 +42,23 @@
 #include "jvmtifiles/jvmtiEnv.hpp"
 #include "memory/filemap.hpp"
 #include "memory/heapShared.inline.hpp"
 #include "memory/metaspaceShared.hpp"
 #include "memory/metadataFactory.hpp"
-#include "memory/iterator.hpp"
+#include "memory/iterator.inline.hpp"
 #include "memory/resourceArea.hpp"
 #include "memory/universe.hpp"
 #include "memory/oopFactory.hpp"
 #include "oops/array.hpp"
 #include "oops/compressedOops.hpp"
+#include "oops/compressedOops.inline.hpp"
 #include "oops/constantPool.inline.hpp"
 #include "oops/method.inline.hpp"
 #include "oops/objArrayKlass.hpp"
 #include "oops/objArrayOop.inline.hpp"
 #include "oops/oop.inline.hpp"
+#include "oops/objArrayOop.inline.hpp"
 #include "oops/typeArrayOop.inline.hpp"
 #include "prims/resolvedMethodTable.hpp"
 #include "prims/wbtestmethods/parserTests.hpp"
 #include "prims/whitebox.inline.hpp"
 #include "runtime/arguments.hpp"
@@ -1835,10 +1837,102 @@
 
 WB_ENTRY(jint, WB_ConstantPoolEncodeIndyIndex(JNIEnv* env, jobject wb, jint index))
   return ConstantPool::encode_invokedynamic_index(index);
 WB_END
 
+WB_ENTRY(jobjectArray, WB_getObjectsViaKlassOopMaps(JNIEnv* env, jobject wb, jobject thing))
+  oop aoop = JNIHandles::resolve(thing);
+  if (!aoop->is_instance()) {
+    return NULL;
+  }
+  instanceHandle ih(THREAD, (instanceOop) aoop);
+  InstanceKlass* klass = InstanceKlass::cast(aoop->klass());
+  if (klass->nonstatic_oop_map_count() == 0) {
+    return NULL;
+  }
+  const OopMapBlock* map = klass->start_of_nonstatic_oop_maps();
+  const OopMapBlock* const end = map + klass->nonstatic_oop_map_count();
+  int oop_count = 0;
+  while (map < end) {
+    oop_count += map->count();
+    map++;
+  }
+
+  objArrayOop result_array =
+      oopFactory::new_objArray(SystemDictionary::Object_klass(), oop_count, CHECK_NULL);
+  map = klass->start_of_nonstatic_oop_maps();
+  instanceOop ioop = ih();
+  int index = 0;
+  while (map < end) {
+    int offset = map->offset();
+    for (unsigned int j = 0; j < map->count(); j++) {
+      result_array->obj_at_put(index++, ioop->obj_field(offset));
+      offset += heapOopSize;
+    }
+    map++;
+  }
+  return (jobjectArray)JNIHandles::make_local(env, result_array);
+WB_END
+
+class CollectOops : public BasicOopIterateClosure {
+ public:
+  GrowableArray<Handle>* array;
+
+  objArrayOop create_results(TRAPS) {
+    objArrayOop result_array =
+        oopFactory::new_objArray(SystemDictionary::Object_klass(), array->length(), CHECK_NULL);
+    for (int i = 0 ; i < array->length(); i++) {
+      result_array->obj_at_put(i, array->at(i)());
+    }
+    return result_array;
+  }
+
+  jobjectArray create_jni_result(JNIEnv* env, TRAPS) {
+    return (jobjectArray)JNIHandles::make_local(env, create_results(THREAD));
+  }
+
+  void add_oop(oop o) {
+    // Value might be oop, but JLS can't see as Object, just iterate through it...
+    if (o != NULL && o->is_value()) {
+      o->oop_iterate(this);
+    } else {
+      array->append(Handle(Thread::current(), o));
+    }
+  }
+
+  void do_oop(oop* o) { add_oop(*o); }
+  void do_oop(narrowOop* v) { add_oop(CompressedOops::decode(*v)); }
+};
+
+
+WB_ENTRY(jobjectArray, WB_getObjectsViaOopIterator(JNIEnv* env, jobject wb, jobject thing))
+  ResourceMark rm(THREAD);
+  GrowableArray<Handle>* array = new GrowableArray<Handle>(128);
+  CollectOops collectOops;
+  collectOops.array = array;
+
+  JNIHandles::resolve(thing)->oop_iterate(&collectOops);
+
+  return collectOops.create_jni_result(env, THREAD);
+WB_END
+
+WB_ENTRY(jobjectArray, WB_getObjectsViaFrameOopIterator(JNIEnv* env, jobject wb, jint depth))
+  ResourceMark rm(THREAD);
+  GrowableArray<Handle>* array = new GrowableArray<Handle>(128);
+  CollectOops collectOops;
+  collectOops.array = array;
+  StackFrameStream sfs(thread);
+  while (depth > 0) { // Skip the native WB API frame
+    sfs.next();
+    frame* f = sfs.current();
+    f->oops_do(&collectOops, NULL, sfs.register_map());
+    depth--;
+  }
+  return collectOops.create_jni_result(env, THREAD);
+WB_END
+
+
 WB_ENTRY(void, WB_ClearInlineCaches(JNIEnv* env, jobject wb, jboolean preserve_static_stubs))
   VM_ClearICs clear_ics(preserve_static_stubs == JNI_TRUE);
   VMThread::execute(&clear_ics);
 WB_END
 
@@ -2412,10 +2506,16 @@
   {CC"getConstantPoolCacheLength0", CC"(Ljava/lang/Class;)I",  (void*)&WB_GetConstantPoolCacheLength},
   {CC"remapInstructionOperandFromCPCache0",
       CC"(Ljava/lang/Class;I)I",                      (void*)&WB_ConstantPoolRemapInstructionOperandFromCache},
   {CC"encodeConstantPoolIndyIndex0",
       CC"(I)I",                      (void*)&WB_ConstantPoolEncodeIndyIndex},
+  {CC"getObjectsViaKlassOopMaps0",
+      CC"(Ljava/lang/Object;)[Ljava/lang/Object;",    (void*)&WB_getObjectsViaKlassOopMaps},
+  {CC"getObjectsViaOopIterator0",
+          CC"(Ljava/lang/Object;)[Ljava/lang/Object;",(void*)&WB_getObjectsViaOopIterator},
+  {CC"getObjectsViaFrameOopIterator",
+      CC"(I)[Ljava/lang/Object;",                     (void*)&WB_getObjectsViaFrameOopIterator},
   {CC"getMethodBooleanOption",
       CC"(Ljava/lang/reflect/Executable;Ljava/lang/String;)Ljava/lang/Boolean;",
                                                       (void*)&WB_GetMethodBooleaneOption},
   {CC"getMethodIntxOption",
       CC"(Ljava/lang/reflect/Executable;Ljava/lang/String;)Ljava/lang/Long;",
diff a/src/hotspot/share/runtime/arguments.cpp b/src/hotspot/share/runtime/arguments.cpp
--- a/src/hotspot/share/runtime/arguments.cpp
+++ b/src/hotspot/share/runtime/arguments.cpp
@@ -2122,10 +2122,20 @@
   }
 #endif
 
   status = status && GCArguments::check_args_consistency();
 
+  if (AMD64_ONLY(false &&) !FLAG_IS_DEFAULT(ValueTypePassFieldsAsArgs)) {
+    FLAG_SET_CMDLINE(ValueTypePassFieldsAsArgs, false);
+    warning("ValueTypePassFieldsAsArgs is not supported on this platform");
+  }
+
+  if (AMD64_ONLY(false &&) !FLAG_IS_DEFAULT(ValueTypeReturnedAsFields)) {
+    FLAG_SET_CMDLINE(ValueTypeReturnedAsFields, false);
+    warning("ValueTypeReturnedAsFields is not supported on this platform");
+  }
+
   return status;
 }
 
 bool Arguments::is_bad_option(const JavaVMOption* option, jboolean ignore,
   const char* option_type) {
@@ -3028,10 +3038,16 @@
     } else if (is_bad_option(option, args->ignoreUnrecognized)) {
       return JNI_ERR;
     }
   }
 
+  if (EnableValhalla) {
+    if (!create_property("valhalla.enableValhalla", "true", InternalProperty)) {
+      return JNI_ENOMEM;
+    }
+  }
+
   // PrintSharedArchiveAndExit will turn on
   //   -Xshare:on
   //   -Xlog:class+path=info
   if (PrintSharedArchiveAndExit) {
     if (FLAG_SET_CMDLINE(UseSharedSpaces, true) != JVMFlag::SUCCESS) {
@@ -4119,10 +4135,15 @@
   // verification is not as if both were enabled.
   if (BytecodeVerificationLocal && !BytecodeVerificationRemote) {
     log_info(verification)("Turning on remote verification because local verification is on");
     FLAG_SET_DEFAULT(BytecodeVerificationRemote, true);
   }
+  if (!EnableValhalla || (is_interpreter_only() && !is_dumping_archive())) {
+    // Disable calling convention optimizations if value types are not supported
+    ValueTypePassFieldsAsArgs = false;
+    ValueTypeReturnedAsFields = false;
+  }
 
 #ifndef PRODUCT
   if (!LogVMOutput && FLAG_IS_DEFAULT(LogVMOutput)) {
     if (use_vm_log()) {
       LogVMOutput = true;
diff a/src/hotspot/share/runtime/deoptimization.cpp b/src/hotspot/share/runtime/deoptimization.cpp
--- a/src/hotspot/share/runtime/deoptimization.cpp
+++ b/src/hotspot/share/runtime/deoptimization.cpp
@@ -46,10 +46,13 @@
 #include "oops/objArrayKlass.hpp"
 #include "oops/objArrayOop.inline.hpp"
 #include "oops/oop.inline.hpp"
 #include "oops/fieldStreams.inline.hpp"
 #include "oops/typeArrayOop.inline.hpp"
+#include "oops/valueArrayKlass.hpp"
+#include "oops/valueArrayOop.hpp"
+#include "oops/valueKlass.inline.hpp"
 #include "oops/verifyOopClosure.hpp"
 #include "prims/jvmtiThreadState.hpp"
 #include "runtime/atomic.hpp"
 #include "runtime/biasedLocking.hpp"
 #include "runtime/deoptimization.hpp"
@@ -179,41 +182,63 @@
   // It is not guaranteed that we can get such information here only
   // by analyzing bytecode in deoptimized frames. This is why this flag
   // is set during method compilation (see Compile::Process_OopMap_Node()).
   // If the previous frame was popped or if we are dispatching an exception,
   // we don't have an oop result.
-  bool save_oop_result = chunk->at(0)->scope()->return_oop() && !thread->popframe_forcing_deopt_reexecution() && (exec_mode == Deoptimization::Unpack_deopt);
-  Handle return_value;
+  ScopeDesc* scope = chunk->at(0)->scope();
+  bool save_oop_result = scope->return_oop() && !thread->popframe_forcing_deopt_reexecution() && (exec_mode == Deoptimization::Unpack_deopt);
+  // In case of the return of multiple values, we must take care
+  // of all oop return values.
+  GrowableArray<Handle> return_oops;
+  ValueKlass* vk = NULL;
+  if (save_oop_result && scope->return_vt()) {
+    vk = ValueKlass::returned_value_klass(map);
+    if (vk != NULL) {
+      vk->save_oop_fields(map, return_oops);
+      save_oop_result = false;
+    }
+  }
   if (save_oop_result) {
     // Reallocation may trigger GC. If deoptimization happened on return from
     // call which returns oop we need to save it since it is not in oopmap.
     oop result = deoptee.saved_oop_result(&map);
     assert(oopDesc::is_oop_or_null(result), "must be oop");
-    return_value = Handle(thread, result);
+    return_oops.push(Handle(thread, result));
     assert(Universe::heap()->is_in_or_null(result), "must be heap pointer");
     if (TraceDeoptimization) {
       ttyLocker ttyl;
       tty->print_cr("SAVED OOP RESULT " INTPTR_FORMAT " in thread " INTPTR_FORMAT, p2i(result), p2i(thread));
     }
   }
-  if (objects != NULL) {
+  if (objects != NULL || vk != NULL) {
+    bool skip_internal = (compiled_method != NULL) && !compiled_method->is_compiled_by_jvmci();
     JRT_BLOCK
-      realloc_failures = Deoptimization::realloc_objects(thread, &deoptee, &map, objects, THREAD);
+      if (vk != NULL) {
+        realloc_failures = Deoptimization::realloc_value_type_result(vk, map, return_oops, THREAD);
+      }
+      if (objects != NULL) {
+        realloc_failures = realloc_failures || Deoptimization::realloc_objects(thread, &deoptee, &map, objects, THREAD);
+        Deoptimization::reassign_fields(&deoptee, &map, objects, realloc_failures, skip_internal, THREAD);
+      }
     JRT_END
-    bool skip_internal = (compiled_method != NULL) && !compiled_method->is_compiled_by_jvmci();
-    Deoptimization::reassign_fields(&deoptee, &map, objects, realloc_failures, skip_internal);
 #ifndef PRODUCT
     if (TraceDeoptimization) {
       ttyLocker ttyl;
       tty->print_cr("REALLOC OBJECTS in thread " INTPTR_FORMAT, p2i(thread));
-      Deoptimization::print_objects(objects, realloc_failures);
+      if (objects != NULL) {
+        Deoptimization::print_objects(objects, realloc_failures);
+      } else {
+        Handle obj = realloc_failures ? Handle() : return_oops.first();
+        Deoptimization::print_object(vk, obj, realloc_failures);
+      }
     }
 #endif
   }
-  if (save_oop_result) {
+  if (save_oop_result || vk != NULL) {
     // Restore result.
-    deoptee.set_saved_oop_result(&map, return_value());
+    assert(return_oops.length() == 1, "no value type");
+    deoptee.set_saved_oop_result(&map, return_oops.pop()());
   }
   return realloc_failures;
 }
 
 static void eliminate_locks(JavaThread* thread, GrowableArray<compiledVFrame*>* chunk, bool realloc_failures) {
@@ -510,11 +535,11 @@
     // non-parameter locals of the first unpacked interpreted frame.
     // Compute that adjustment.
     caller_adjustment = last_frame_adjust(callee_parameters, callee_locals);
   }
 
-  // If the sender is deoptimized the we must retrieve the address of the handler
+  // If the sender is deoptimized we must retrieve the address of the handler
   // since the frame will "magically" show the original pc before the deopt
   // and we'd undo the deopt.
 
   frame_pcs[0] = deopt_sender.raw_pc();
 
@@ -1001,10 +1026,14 @@
 #endif // INCLUDE_JVMCI || INCLUDE_AOT
       InstanceKlass* ik = InstanceKlass::cast(k);
       if (obj == NULL) {
         obj = ik->allocate_instance(THREAD);
       }
+    } else if (k->is_valueArray_klass()) {
+      ValueArrayKlass* ak = ValueArrayKlass::cast(k);
+      // Value type array must be zeroed because not all memory is reassigned
+      obj = ak->allocate(sv->field_size(), THREAD);
     } else if (k->is_typeArray_klass()) {
       TypeArrayKlass* ak = TypeArrayKlass::cast(k);
       assert(sv->field_size() % type2size[ak->element_type()] == 0, "non-integral array length");
       int len = sv->field_size() / type2size[ak->element_type()];
       obj = ak->allocate(len, THREAD);
@@ -1030,10 +1059,25 @@
   }
 
   return failures;
 }
 
+// We're deoptimizing at the return of a call, value type fields are
+// in registers. When we go back to the interpreter, it will expect a
+// reference to a value type instance. Allocate and initialize it from
+// the register values here.
+bool Deoptimization::realloc_value_type_result(ValueKlass* vk, const RegisterMap& map, GrowableArray<Handle>& return_oops, TRAPS) {
+  oop new_vt = vk->realloc_result(map, return_oops, THREAD);
+  if (new_vt == NULL) {
+    CLEAR_PENDING_EXCEPTION;
+    THROW_OOP_(Universe::out_of_memory_error_realloc_objects(), true);
+  }
+  return_oops.clear();
+  return_oops.push(Handle(THREAD, new_vt));
+  return false;
+}
+
 #if INCLUDE_JVMCI
 /**
  * For primitive types whose kind gets "erased" at runtime (shorts become stack ints),
  * we need to somehow be able to recover the actual kind to be able to write the correct
  * amount of bytes.
@@ -1226,50 +1270,72 @@
 
 class ReassignedField {
 public:
   int _offset;
   BasicType _type;
+  InstanceKlass* _klass;
 public:
   ReassignedField() {
     _offset = 0;
     _type = T_ILLEGAL;
+    _klass = NULL;
   }
 };
 
 int compare(ReassignedField* left, ReassignedField* right) {
   return left->_offset - right->_offset;
 }
 
 // Restore fields of an eliminated instance object using the same field order
 // returned by HotSpotResolvedObjectTypeImpl.getInstanceFields(true)
-static int reassign_fields_by_klass(InstanceKlass* klass, frame* fr, RegisterMap* reg_map, ObjectValue* sv, int svIndex, oop obj, bool skip_internal) {
+static int reassign_fields_by_klass(InstanceKlass* klass, frame* fr, RegisterMap* reg_map, ObjectValue* sv, int svIndex, oop obj, bool skip_internal, int base_offset, TRAPS) {
+
   GrowableArray<ReassignedField>* fields = new GrowableArray<ReassignedField>();
   InstanceKlass* ik = klass;
   while (ik != NULL) {
     for (AllFieldStream fs(ik); !fs.done(); fs.next()) {
       if (!fs.access_flags().is_static() && (!skip_internal || !fs.access_flags().is_internal())) {
         ReassignedField field;
         field._offset = fs.offset();
         field._type = Signature::basic_type(fs.signature());
+        if (field._type == T_VALUETYPE) {
+          field._type = T_OBJECT;
+        }
+        if (fs.is_flattened()) {
+          // Resolve klass of flattened value type field
+          Klass* vk = klass->get_value_field_klass(fs.index());
+          field._klass = ValueKlass::cast(vk);
+          field._type = T_VALUETYPE;
+        }
         fields->append(field);
       }
     }
     ik = ik->superklass();
   }
   fields->sort(compare);
   for (int i = 0; i < fields->length(); i++) {
     intptr_t val;
     ScopeValue* scope_field = sv->field_at(svIndex);
     StackValue* value = StackValue::create_stack_value(fr, reg_map, scope_field);
-    int offset = fields->at(i)._offset;
+    int offset = base_offset + fields->at(i)._offset;
     BasicType type = fields->at(i)._type;
     switch (type) {
-      case T_OBJECT: case T_ARRAY:
+      case T_OBJECT:
+      case T_ARRAY:
         assert(value->type() == T_OBJECT, "Agreement.");
         obj->obj_field_put(offset, value->get_obj()());
         break;
 
+      case T_VALUETYPE: {
+        // Recursively re-assign flattened value type fields
+        InstanceKlass* vk = fields->at(i)._klass;
+        assert(vk != NULL, "must be resolved");
+        offset -= ValueKlass::cast(vk)->first_field_offset(); // Adjust offset to omit oop header
+        svIndex = reassign_fields_by_klass(vk, fr, reg_map, sv, svIndex, obj, skip_internal, offset, CHECK_0);
+        continue; // Continue because we don't need to increment svIndex
+      }
+
       // Have to cast to INT (32 bits) pointer to avoid little/big-endian problem.
       case T_INT: case T_FLOAT: { // 4 bytes.
         assert(value->type() == T_INT, "Agreement.");
         bool big_value = false;
         if (i+1 < fields->length() && fields->at(i+1)._type == T_INT) {
@@ -1346,12 +1412,26 @@
     svIndex++;
   }
   return svIndex;
 }
 
+// restore fields of an eliminated value type array
+void Deoptimization::reassign_value_array_elements(frame* fr, RegisterMap* reg_map, ObjectValue* sv, valueArrayOop obj, ValueArrayKlass* vak, TRAPS) {
+  ValueKlass* vk = vak->element_klass();
+  assert(vk->flatten_array(), "should only be used for flattened value type arrays");
+  // Adjust offset to omit oop header
+  int base_offset = arrayOopDesc::base_offset_in_bytes(T_VALUETYPE) - ValueKlass::cast(vk)->first_field_offset();
+  // Initialize all elements of the flattened value type array
+  for (int i = 0; i < sv->field_size(); i++) {
+    ScopeValue* val = sv->field_at(i);
+    int offset = base_offset + (i << Klass::layout_helper_log2_element_size(vak->layout_helper()));
+    reassign_fields_by_klass(vk, fr, reg_map, val->as_ObjectValue(), 0, (oop)obj, false /* skip_internal */, offset, CHECK);
+  }
+}
+
 // restore fields of all eliminated objects and arrays
-void Deoptimization::reassign_fields(frame* fr, RegisterMap* reg_map, GrowableArray<ScopeValue*>* objects, bool realloc_failures, bool skip_internal) {
+void Deoptimization::reassign_fields(frame* fr, RegisterMap* reg_map, GrowableArray<ScopeValue*>* objects, bool realloc_failures, bool skip_internal, TRAPS) {
   for (int i = 0; i < objects->length(); i++) {
     ObjectValue* sv = (ObjectValue*) objects->at(i);
     Klass* k = java_lang_Class::as_Klass(sv->klass()->as_ConstantOopReadValue()->value()());
     Handle obj = sv->value();
     assert(obj.not_null() || realloc_failures, "reallocation was missed");
@@ -1367,11 +1447,14 @@
       continue;
     }
 #endif // INCLUDE_JVMCI || INCLUDE_AOT
     if (k->is_instance_klass()) {
       InstanceKlass* ik = InstanceKlass::cast(k);
-      reassign_fields_by_klass(ik, fr, reg_map, sv, 0, obj(), skip_internal);
+      reassign_fields_by_klass(ik, fr, reg_map, sv, 0, obj(), skip_internal, 0, CHECK);
+    } else if (k->is_valueArray_klass()) {
+      ValueArrayKlass* vak = ValueArrayKlass::cast(k);
+      reassign_value_array_elements(fr, reg_map, sv, (valueArrayOop) obj(), vak, CHECK);
     } else if (k->is_typeArray_klass()) {
       TypeArrayKlass* ak = TypeArrayKlass::cast(k);
       reassign_type_array_elements(fr, reg_map, sv, (typeArrayOop) obj(), ak->element_type());
     } else if (k->is_objArray_klass()) {
       reassign_object_array_elements(fr, reg_map, sv, (objArrayOop) obj());
@@ -1410,29 +1493,30 @@
 
 #ifndef PRODUCT
 // print information about reallocated objects
 void Deoptimization::print_objects(GrowableArray<ScopeValue*>* objects, bool realloc_failures) {
   fieldDescriptor fd;
-
   for (int i = 0; i < objects->length(); i++) {
     ObjectValue* sv = (ObjectValue*) objects->at(i);
     Klass* k = java_lang_Class::as_Klass(sv->klass()->as_ConstantOopReadValue()->value()());
-    Handle obj = sv->value();
+    print_object(k, sv->value(), realloc_failures);
+  }
+}
 
-    tty->print("     object <" INTPTR_FORMAT "> of type ", p2i(sv->value()()));
-    k->print_value();
-    assert(obj.not_null() || realloc_failures, "reallocation was missed");
-    if (obj.is_null()) {
-      tty->print(" allocation failed");
-    } else {
-      tty->print(" allocated (%d bytes)", obj->size() * HeapWordSize);
-    }
-    tty->cr();
+void Deoptimization::print_object(Klass* k, Handle obj, bool realloc_failures) {
+  tty->print("     object <" INTPTR_FORMAT "> of type ", p2i(obj()));
+  k->print_value();
+  assert(obj.not_null() || realloc_failures, "reallocation was missed");
+  if (obj.is_null()) {
+    tty->print(" allocation failed");
+  } else {
+    tty->print(" allocated (%d bytes)", obj->size() * HeapWordSize);
+  }
+  tty->cr();
 
-    if (Verbose && !obj.is_null()) {
-      k->oop_print_on(obj(), tty);
-    }
+  if (Verbose && !obj.is_null()) {
+    k->oop_print_on(obj(), tty);
   }
 }
 #endif
 #endif // COMPILER2_OR_JVMCI
 
@@ -1601,11 +1685,11 @@
   // deopt the execution state and return to the interpreter.
   fr.deoptimize(thread);
 }
 
 void Deoptimization::deoptimize(JavaThread* thread, frame fr, DeoptReason reason) {
-  // Deoptimize only if the frame comes from compile code.
+  // Deoptimize only if the frame comes from compiled code.
   // Do not deoptimize the frame which is already patched
   // during the execution of the loops below.
   if (!fr.is_compiled_frame() || fr.is_deoptimized_frame()) {
     return;
   }
diff a/src/hotspot/share/runtime/deoptimization.hpp b/src/hotspot/share/runtime/deoptimization.hpp
--- a/src/hotspot/share/runtime/deoptimization.hpp
+++ b/src/hotspot/share/runtime/deoptimization.hpp
@@ -168,16 +168,21 @@
 #if COMPILER2_OR_JVMCI
  public:
 
   // Support for restoring non-escaping objects
   static bool realloc_objects(JavaThread* thread, frame* fr, RegisterMap* reg_map, GrowableArray<ScopeValue*>* objects, TRAPS);
+  static bool realloc_value_type_result(ValueKlass* vk, const RegisterMap& map, GrowableArray<Handle>& return_oops, TRAPS);
   static void reassign_type_array_elements(frame* fr, RegisterMap* reg_map, ObjectValue* sv, typeArrayOop obj, BasicType type);
   static void reassign_object_array_elements(frame* fr, RegisterMap* reg_map, ObjectValue* sv, objArrayOop obj);
-  static void reassign_fields(frame* fr, RegisterMap* reg_map, GrowableArray<ScopeValue*>* objects, bool realloc_failures, bool skip_internal);
+  static void reassign_value_array_elements(frame* fr, RegisterMap* reg_map, ObjectValue* sv, valueArrayOop obj, ValueArrayKlass* vak, TRAPS);
+  static void reassign_fields(frame* fr, RegisterMap* reg_map, GrowableArray<ScopeValue*>* objects, bool realloc_failures, bool skip_internal, TRAPS);
   static void relock_objects(GrowableArray<MonitorInfo*>* monitors, JavaThread* thread, bool realloc_failures);
   static void pop_frames_failed_reallocs(JavaThread* thread, vframeArray* array);
-  NOT_PRODUCT(static void print_objects(GrowableArray<ScopeValue*>* objects, bool realloc_failures);)
+#ifndef PRODUCT
+  static void print_objects(GrowableArray<ScopeValue*>* objects, bool realloc_failures);
+  static void print_object(Klass* k, Handle obj, bool realloc_failures);
+#endif
 #endif // COMPILER2_OR_JVMCI
 
   public:
   static vframeArray* create_vframeArray(JavaThread* thread, frame fr, RegisterMap *reg_map, GrowableArray<compiledVFrame*>* chunk, bool realloc_failures);
 
diff a/src/hotspot/share/runtime/globals.hpp b/src/hotspot/share/runtime/globals.hpp
--- a/src/hotspot/share/runtime/globals.hpp
+++ b/src/hotspot/share/runtime/globals.hpp
@@ -763,10 +763,28 @@
           "Use SSE2 MOVQ instruction for Arraycopy")                        \
                                                                             \
   notproduct(bool, PrintFieldLayout, false,                                 \
           "Print field layout for each class")                              \
                                                                             \
+  notproduct(bool, PrintValueLayout, false,                                 \
+          "Print field layout for each value type")                         \
+                                                                            \
+  notproduct(bool, PrintValueArrayLayout, false,                            \
+          "Print array layout for each value type array")                   \
+                                                                            \
+  product(intx, ValueArrayElemMaxFlatSize, -1,                              \
+          "Max size for flattening value array elements, <0 no limit")      \
+                                                                            \
+  product(intx, ValueFieldMaxFlatSize, 128,                                 \
+          "Max size for flattening value type fields, <0 no limit")         \
+                                                                            \
+  product(intx, ValueArrayElemMaxFlatOops, 4,                               \
+          "Max nof embedded object references in a value type to flatten, <0 no limit")  \
+                                                                            \
+  product(bool, ValueArrayAtomicAccess, false,                              \
+          "Atomic value array accesses by-default, for all value arrays")   \
+                                                                            \
   /* Need to limit the extent of the padding to reasonable size.          */\
   /* 8K is well beyond the reasonable HW cache line size, even with       */\
   /* aggressive prefetching, while still leaving the room for segregating */\
   /* among the distinct pages.                                            */\
   product(intx, ContendedPaddingWidth, 128,                                 \
@@ -2475,18 +2493,46 @@
           "Start flight recording with options"))                           \
                                                                             \
   experimental(bool, UseFastUnorderedTimeStamps, false,                     \
           "Use platform unstable time where supported for timestamps only") \
                                                                             \
+  product(bool, EnableValhalla, true,                                       \
+          "Enable experimental Valhalla features")                          \
+                                                                            \
+  product_pd(bool, ValueTypePassFieldsAsArgs,                               \
+          "Pass each value type field as an argument at calls")             \
+                                                                            \
+  product_pd(bool, ValueTypeReturnedAsFields,                               \
+          "Return fields instead of a value type reference")                \
+                                                                            \
+  develop(bool, StressValueTypeReturnedAsFields, false,                     \
+          "Stress return of fields instead of a value type reference")      \
+                                                                            \
+  develop(bool, ScalarizeValueTypes, true,                                  \
+          "Scalarize value types in compiled code")                         \
+                                                                            \
+  diagnostic(ccstrlist, ForceNonTearable, "",                               \
+          "List of inline classes which are forced to be atomic "           \
+          "(whitespace and commas separate names, "                         \
+          "and leading and trailing stars '*' are wildcards)")              \
+                                                                            \
+  product(bool, PrintNewLayout, false,                                      \
+               "Print layout compute by new algorithm")                     \
+                                                                            \
+  product(bool, PrintFlattenableLayouts, false,                             \
+                "Print layout of inline classes and classes with "          \
+                "flattenable fields")                                       \
+                                                                            \
   product(bool, UseNewFieldLayout, true,                                    \
-               "(Deprecated) Use new algorithm to compute field layouts")   \
+                "(Deprecated) Use new algorithm to compute field layouts")  \
                                                                             \
   product(bool, UseEmptySlotsInSupers, true,                                \
                 "Allow allocating fields in empty slots of super-classes")  \
                                                                             \
 
 
+
 // Interface macros
 #define DECLARE_PRODUCT_FLAG(type, name, value, doc)      extern "C" type name;
 #define DECLARE_PD_PRODUCT_FLAG(type, name, doc)          extern "C" type name;
 #define DECLARE_DIAGNOSTIC_FLAG(type, name, value, doc)   extern "C" type name;
 #define DECLARE_PD_DIAGNOSTIC_FLAG(type, name, doc)       extern "C" type name;
diff a/src/hotspot/share/runtime/reflection.cpp b/src/hotspot/share/runtime/reflection.cpp
--- a/src/hotspot/share/runtime/reflection.cpp
+++ b/src/hotspot/share/runtime/reflection.cpp
@@ -38,10 +38,11 @@
 #include "memory/universe.hpp"
 #include "oops/instanceKlass.hpp"
 #include "oops/objArrayKlass.hpp"
 #include "oops/objArrayOop.inline.hpp"
 #include "oops/oop.inline.hpp"
+#include "oops/valueKlass.inline.hpp"
 #include "oops/typeArrayOop.inline.hpp"
 #include "prims/jvmtiExport.hpp"
 #include "runtime/arguments.hpp"
 #include "runtime/fieldDescriptor.inline.hpp"
 #include "runtime/handles.inline.hpp"
@@ -49,10 +50,11 @@
 #include "runtime/reflection.hpp"
 #include "runtime/reflectionUtils.hpp"
 #include "runtime/signature.hpp"
 #include "runtime/thread.inline.hpp"
 #include "runtime/vframe.inline.hpp"
+#include "utilities/globalDefinitions.hpp"
 
 static void trace_class_resolution(oop mirror) {
   if (mirror == NULL || java_lang_Class::is_primitive(mirror)) {
     return;
   }
@@ -344,11 +346,15 @@
   } else {
     Klass* k = java_lang_Class::as_Klass(element_mirror);
     if (k->is_array_klass() && ArrayKlass::cast(k)->dimension() >= MAX_DIM) {
       THROW_0(vmSymbols::java_lang_IllegalArgumentException());
     }
-    return oopFactory::new_objArray(k, length, THREAD);
+    if (k->is_value()) {
+      return oopFactory::new_valueArray(k, length, THREAD);
+    } else {
+      return oopFactory::new_objArray(k, length, THREAD);
+    }
   }
 }
 
 
 arrayOop Reflection::reflect_new_multi_array(oop element_mirror, typeArrayOop dim_array, TRAPS) {
@@ -786,21 +792,17 @@
 }
 
 static Handle new_type(Symbol* signature, Klass* k, TRAPS) {
   ResolvingSignatureStream ss(signature, k, false);
   oop nt = ss.as_java_mirror(SignatureStream::NCDFError, CHECK_NH);
-  if (log_is_enabled(Debug, class, resolve)) {
-    trace_class_resolution(nt);
-  }
   return Handle(THREAD, nt);
 }
 
 
 oop Reflection::new_method(const methodHandle& method, bool for_constant_pool_access, TRAPS) {
   // Allow sun.reflect.ConstantPool to refer to <clinit> methods as java.lang.reflect.Methods.
-  assert(!method()->is_initializer() ||
-         (for_constant_pool_access && method()->is_static()),
+  assert(!method()->name()->starts_with('<') || for_constant_pool_access,
          "should call new_constructor instead");
   InstanceKlass* holder = method->method_holder();
   int slot = method->method_idnum();
 
   Symbol*  signature  = method->signature();
@@ -846,11 +848,13 @@
   return mh();
 }
 
 
 oop Reflection::new_constructor(const methodHandle& method, TRAPS) {
-  assert(method()->is_initializer(), "should call new_method instead");
+  assert(method()->is_object_constructor() ||
+         method()->is_static_init_factory(),
+         "should call new_method instead");
 
   InstanceKlass* holder = method->method_holder();
   int slot = method->method_idnum();
 
   Symbol*  signature  = method->signature();
@@ -896,11 +900,20 @@
   java_lang_reflect_Field::set_clazz(rh(), fd->field_holder()->java_mirror());
   java_lang_reflect_Field::set_slot(rh(), fd->index());
   java_lang_reflect_Field::set_name(rh(), name());
   java_lang_reflect_Field::set_type(rh(), type());
   // Note the ACC_ANNOTATION bit, which is a per-class access flag, is never set here.
-  java_lang_reflect_Field::set_modifiers(rh(), fd->access_flags().as_int() & JVM_RECOGNIZED_FIELD_MODIFIERS);
+  int modifiers = fd->access_flags().as_int() & JVM_RECOGNIZED_FIELD_MODIFIERS;
+  if (fd->is_flattenable()) {
+    modifiers |= JVM_ACC_FIELD_FLATTENABLE;
+    // JVM_ACC_FLATTENABLE should not be set in LWorld.  set_is_flattenable should be re-examined.
+    modifiers &= ~JVM_ACC_FLATTENABLE;
+  }
+  if (fd->is_flattened()) {
+    modifiers |= JVM_ACC_FIELD_FLATTENED;
+  }
+  java_lang_reflect_Field::set_modifiers(rh(), modifiers);
   java_lang_reflect_Field::set_override(rh(), false);
   if (fd->has_generic_signature()) {
     Symbol*  gs = fd->generic_signature();
     Handle sig = java_lang_String::create_from_symbol(gs, CHECK_NULL);
     java_lang_reflect_Field::set_signature(rh(), sig());
@@ -1171,10 +1184,12 @@
 
   oop return_type_mirror = java_lang_reflect_Method::return_type(method_mirror);
   BasicType rtype;
   if (java_lang_Class::is_primitive(return_type_mirror)) {
     rtype = basic_type_mirror_to_basic_type(return_type_mirror, CHECK_NULL);
+  } else if (java_lang_Class::as_Klass(return_type_mirror)->is_value()) {
+    rtype = T_VALUETYPE;
   } else {
     rtype = T_OBJECT;
   }
 
   InstanceKlass* klass = InstanceKlass::cast(java_lang_Class::as_Klass(mirror));
@@ -1205,10 +1220,20 @@
   // Make sure klass gets initialize
   klass->initialize(CHECK_NULL);
 
   // Create new instance (the receiver)
   klass->check_valid_for_instantiation(false, CHECK_NULL);
+
+  // Special case for factory methods
+  if (!method->signature()->is_void_method_signature()) {
+    assert(klass->is_value(), "inline classes must use factory methods");
+    Handle no_receiver; // null instead of receiver
+    return invoke(klass, method, no_receiver, override, ptypes, T_VALUETYPE, args, false, CHECK_NULL);
+  }
+
+  // main branch of code creates a non-inline object:
+  assert(!klass->is_value(), "classic constructors are only for non-inline classes");
   Handle receiver = klass->allocate_instance_handle(CHECK_NULL);
 
   // Ignore result from call and return receiver
   invoke(klass, method, receiver, override, ptypes, T_VOID, args, false, CHECK_NULL);
   return receiver();
diff a/src/hotspot/share/runtime/safepoint.cpp b/src/hotspot/share/runtime/safepoint.cpp
--- a/src/hotspot/share/runtime/safepoint.cpp
+++ b/src/hotspot/share/runtime/safepoint.cpp
@@ -45,10 +45,11 @@
 #include "logging/logStream.hpp"
 #include "memory/resourceArea.hpp"
 #include "memory/universe.hpp"
 #include "oops/oop.inline.hpp"
 #include "oops/symbol.hpp"
+#include "oops/valueKlass.hpp"
 #include "runtime/atomic.hpp"
 #include "runtime/deoptimization.hpp"
 #include "runtime/frame.inline.hpp"
 #include "runtime/handles.inline.hpp"
 #include "runtime/interfaceSupport.inline.hpp"
@@ -1033,29 +1034,56 @@
   // has already had the effect of causing the return to occur, so the execution
   // will continue immediately after the call. In addition, the oopmap at the
   // return point does not mark the return value as an oop (if it is), so
   // it needs a handle here to be updated.
   if( nm->is_at_poll_return(real_return_addr) ) {
+    ResourceMark rm;
     // See if return type is an oop.
-    bool return_oop = nm->method()->is_returning_oop();
-    Handle return_value;
+    Method* method = nm->method();
+    bool return_oop = method->is_returning_oop();
+
+    GrowableArray<Handle> return_values;
+    ValueKlass* vk = NULL;
+
+    if (return_oop && ValueTypeReturnedAsFields) {
+      SignatureStream ss(method->signature());
+      while (!ss.at_return_type()) {
+        ss.next();
+      }
+      if (ss.type() == T_VALUETYPE) {
+        // Check if value type is returned as fields
+        vk = ValueKlass::returned_value_klass(map);
+        if (vk != NULL) {
+          // We're at a safepoint at the return of a method that returns
+          // multiple values. We must make sure we preserve the oop values
+          // across the safepoint.
+          assert(vk == method->returned_value_type(thread()), "bad value klass");
+          vk->save_oop_fields(map, return_values);
+          return_oop = false;
+        }
+      }
+    }
+
     if (return_oop) {
       // The oop result has been saved on the stack together with all
       // the other registers. In order to preserve it over GCs we need
       // to keep it in a handle.
       oop result = caller_fr.saved_oop_result(&map);
       assert(oopDesc::is_oop_or_null(result), "must be oop");
-      return_value = Handle(thread(), result);
+      return_values.push(Handle(thread(), result));
       assert(Universe::heap()->is_in_or_null(result), "must be heap pointer");
     }
 
     // Block the thread
     SafepointMechanism::block_if_requested(thread());
 
     // restore oop result, if any
     if (return_oop) {
-      caller_fr.set_saved_oop_result(&map, return_value());
+      assert(return_values.length() == 1, "only one return value");
+      caller_fr.set_saved_oop_result(&map, return_values.pop()());
+    } else if (vk != NULL) {
+      vk->restore_oop_results(map, return_values);
     }
   }
 
   // This is a safepoint poll. Verify the return address and block.
   else {
diff a/src/hotspot/share/runtime/sharedRuntime.cpp b/src/hotspot/share/runtime/sharedRuntime.cpp
--- a/src/hotspot/share/runtime/sharedRuntime.cpp
+++ b/src/hotspot/share/runtime/sharedRuntime.cpp
@@ -42,16 +42,21 @@
 #include "interpreter/interpreter.hpp"
 #include "interpreter/interpreterRuntime.hpp"
 #include "jfr/jfrEvents.hpp"
 #include "logging/log.hpp"
 #include "memory/metaspaceShared.hpp"
+#include "memory/oopFactory.hpp"
 #include "memory/resourceArea.hpp"
 #include "memory/universe.hpp"
+#include "oops/access.hpp"
+#include "oops/fieldStreams.inline.hpp"
 #include "oops/klass.hpp"
 #include "oops/method.inline.hpp"
 #include "oops/objArrayKlass.hpp"
+#include "oops/objArrayOop.inline.hpp"
 #include "oops/oop.inline.hpp"
+#include "oops/valueKlass.inline.hpp"
 #include "prims/forte.hpp"
 #include "prims/jvmtiExport.hpp"
 #include "prims/methodHandles.hpp"
 #include "prims/nativeLookup.hpp"
 #include "runtime/arguments.hpp"
@@ -82,11 +87,10 @@
 RuntimeStub*        SharedRuntime::_wrong_method_abstract_blob;
 RuntimeStub*        SharedRuntime::_ic_miss_blob;
 RuntimeStub*        SharedRuntime::_resolve_opt_virtual_call_blob;
 RuntimeStub*        SharedRuntime::_resolve_virtual_call_blob;
 RuntimeStub*        SharedRuntime::_resolve_static_call_blob;
-address             SharedRuntime::_resolve_static_call_entry;
 
 DeoptimizationBlob* SharedRuntime::_deopt_blob;
 SafepointBlob*      SharedRuntime::_polling_page_vectors_safepoint_handler_blob;
 SafepointBlob*      SharedRuntime::_polling_page_safepoint_handler_blob;
 SafepointBlob*      SharedRuntime::_polling_page_return_handler_blob;
@@ -102,11 +106,10 @@
   _wrong_method_abstract_blob          = generate_resolve_blob(CAST_FROM_FN_PTR(address, SharedRuntime::handle_wrong_method_abstract), "wrong_method_abstract_stub");
   _ic_miss_blob                        = generate_resolve_blob(CAST_FROM_FN_PTR(address, SharedRuntime::handle_wrong_method_ic_miss),  "ic_miss_stub");
   _resolve_opt_virtual_call_blob       = generate_resolve_blob(CAST_FROM_FN_PTR(address, SharedRuntime::resolve_opt_virtual_call_C),   "resolve_opt_virtual_call");
   _resolve_virtual_call_blob           = generate_resolve_blob(CAST_FROM_FN_PTR(address, SharedRuntime::resolve_virtual_call_C),       "resolve_virtual_call");
   _resolve_static_call_blob            = generate_resolve_blob(CAST_FROM_FN_PTR(address, SharedRuntime::resolve_static_call_C),        "resolve_static_call");
-  _resolve_static_call_entry           = _resolve_static_call_blob->entry_point();
 
 #if COMPILER2_OR_JVMCI
   // Vectors are generated only by C2 and JVMCI.
   bool support_wide = is_wide_vector(MaxVectorSize);
   if (support_wide) {
@@ -1049,10 +1052,25 @@
 
   // Find caller and bci from vframe
   methodHandle caller(THREAD, vfst.method());
   int          bci   = vfst.bci();
 
+  // Substitutability test implementation piggy backs on static call resolution
+  Bytecodes::Code code = caller->java_code_at(bci);
+  if (code == Bytecodes::_if_acmpeq || code == Bytecodes::_if_acmpne) {
+    bc = Bytecodes::_invokestatic;
+    methodHandle attached_method(THREAD, extract_attached_method(vfst));
+    assert(attached_method.not_null(), "must have attached method");
+    SystemDictionary::ValueBootstrapMethods_klass()->initialize(CHECK_NH);
+    LinkResolver::resolve_invoke(callinfo, receiver, attached_method, bc, false, CHECK_NH);
+#ifdef ASSERT
+    Method* is_subst = SystemDictionary::ValueBootstrapMethods_klass()->find_method(vmSymbols::isSubstitutable_name(), vmSymbols::object_object_boolean_signature());
+    assert(callinfo.selected_method() == is_subst, "must be isSubstitutable method");
+#endif
+    return receiver;
+  }
+
   Bytecode_invoke bytecode(caller, bci);
   int bytecode_index = bytecode.index();
   bc = bytecode.invoke_code();
 
   methodHandle attached_method(THREAD, extract_attached_method(vfst));
@@ -1084,56 +1102,77 @@
           }
           break;
         default:
           break;
       }
+    } else {
+      assert(attached_method->has_scalarized_args(), "invalid use of attached method");
+      if (!attached_method->method_holder()->is_value()) {
+        // Ignore the attached method in this case to not confuse below code
+        attached_method = methodHandle(thread, NULL);
+      }
     }
   }
 
   assert(bc != Bytecodes::_illegal, "not initialized");
 
   bool has_receiver = bc != Bytecodes::_invokestatic &&
                       bc != Bytecodes::_invokedynamic &&
                       bc != Bytecodes::_invokehandle;
+  bool check_null_and_abstract = true;
 
   // Find receiver for non-static call
   if (has_receiver) {
     // This register map must be update since we need to find the receiver for
     // compiled frames. The receiver might be in a register.
     RegisterMap reg_map2(thread);
     frame stubFrame   = thread->last_frame();
     // Caller-frame is a compiled frame
     frame callerFrame = stubFrame.sender(&reg_map2);
+    bool caller_is_c1 = false;
 
-    if (attached_method.is_null()) {
-      Method* callee = bytecode.static_target(CHECK_NH);
+    if (callerFrame.is_compiled_frame() && !callerFrame.is_deoptimized_frame()) {
+      caller_is_c1 = callerFrame.cb()->is_compiled_by_c1();
+    }
+
+    Method* callee = attached_method();
+    if (callee == NULL) {
+      callee = bytecode.static_target(CHECK_NH);
       if (callee == NULL) {
         THROW_(vmSymbols::java_lang_NoSuchMethodException(), nullHandle);
       }
     }
-
-    // Retrieve from a compiled argument list
-    receiver = Handle(THREAD, callerFrame.retrieve_receiver(&reg_map2));
-
-    if (receiver.is_null()) {
-      THROW_(vmSymbols::java_lang_NullPointerException(), nullHandle);
+    if (!caller_is_c1 && callee->has_scalarized_args() && callee->method_holder()->is_value()) {
+      // If the receiver is a value type that is passed as fields, no oop is available.
+      // Resolve the call without receiver null checking.
+      assert(attached_method.not_null() && !attached_method->is_abstract(), "must have non-abstract attached method");
+      if (bc == Bytecodes::_invokeinterface) {
+        bc = Bytecodes::_invokevirtual; // C2 optimistically replaces interface calls by virtual calls
+      }
+      check_null_and_abstract = false;
+    } else {
+      // Retrieve from a compiled argument list
+      receiver = Handle(THREAD, callerFrame.retrieve_receiver(&reg_map2));
+      if (receiver.is_null()) {
+        THROW_(vmSymbols::java_lang_NullPointerException(), nullHandle);
+      }
     }
   }
 
   // Resolve method
   if (attached_method.not_null()) {
     // Parameterized by attached method.
-    LinkResolver::resolve_invoke(callinfo, receiver, attached_method, bc, CHECK_NH);
+    LinkResolver::resolve_invoke(callinfo, receiver, attached_method, bc, check_null_and_abstract, CHECK_NH);
   } else {
     // Parameterized by bytecode.
     constantPoolHandle constants(THREAD, caller->constants());
     LinkResolver::resolve_invoke(callinfo, receiver, constants, bytecode_index, bc, CHECK_NH);
   }
 
 #ifdef ASSERT
   // Check that the receiver klass is of the right subtype and that it is initialized for virtual calls
-  if (has_receiver) {
+  if (has_receiver && check_null_and_abstract) {
     assert(receiver.not_null(), "should have thrown exception");
     Klass* receiver_klass = receiver->klass();
     Klass* rk = NULL;
     if (attached_method.not_null()) {
       // In case there's resolved method attached, use its holder during the check.
@@ -1188,13 +1227,14 @@
 }
 
 // Resolves a call.
 methodHandle SharedRuntime::resolve_helper(JavaThread *thread,
                                            bool is_virtual,
-                                           bool is_optimized, TRAPS) {
+                                           bool is_optimized,
+                                           bool* caller_is_c1, TRAPS) {
   methodHandle callee_method;
-  callee_method = resolve_sub_helper(thread, is_virtual, is_optimized, THREAD);
+  callee_method = resolve_sub_helper(thread, is_virtual, is_optimized, caller_is_c1, THREAD);
   if (JvmtiExport::can_hotswap_or_post_breakpoint()) {
     int retry_count = 0;
     while (!HAS_PENDING_EXCEPTION && callee_method->is_old() &&
            callee_method->method_holder() != SystemDictionary::Object_klass()) {
       // If has a pending exception then there is no need to re-try to
@@ -1207,11 +1247,11 @@
       // in the middle of resolve. If it is looping here more than 100 times
       // means then there could be a bug here.
       guarantee((retry_count++ < 100),
                 "Could not resolve to latest version of redefined method");
       // method is redefined in the middle of resolve so re-try.
-      callee_method = resolve_sub_helper(thread, is_virtual, is_optimized, THREAD);
+      callee_method = resolve_sub_helper(thread, is_virtual, is_optimized, caller_is_c1, THREAD);
     }
   }
   return callee_method;
 }
 
@@ -1238,21 +1278,28 @@
 #ifdef ASSERT
   address dest_entry_point = callee == NULL ? 0 : callee->entry_point(); // used below
 #endif
 
   bool is_nmethod = caller_nm->is_nmethod();
+  bool caller_is_c1 = caller_nm->is_compiled_by_c1();
 
   if (is_virtual) {
-    assert(receiver.not_null() || invoke_code == Bytecodes::_invokehandle, "sanity check");
+    Klass* receiver_klass = NULL;
+    if (ValueTypePassFieldsAsArgs && !caller_is_c1 && callee_method->method_holder()->is_value()) {
+      // If the receiver is a value type that is passed as fields, no oop is available
+      receiver_klass = callee_method->method_holder();
+    } else {
+      assert(receiver.not_null() || invoke_code == Bytecodes::_invokehandle, "sanity check");
+      receiver_klass = invoke_code == Bytecodes::_invokehandle ? NULL : receiver->klass();
+    }
     bool static_bound = call_info.resolved_method()->can_be_statically_bound();
-    Klass* klass = invoke_code == Bytecodes::_invokehandle ? NULL : receiver->klass();
-    CompiledIC::compute_monomorphic_entry(callee_method, klass,
-                     is_optimized, static_bound, is_nmethod, virtual_call_info,
+    CompiledIC::compute_monomorphic_entry(callee_method, receiver_klass,
+                     is_optimized, static_bound, is_nmethod, caller_is_c1, virtual_call_info,
                      CHECK_false);
   } else {
     // static call
-    CompiledStaticCall::compute_entry(callee_method, is_nmethod, static_call_info);
+    CompiledStaticCall::compute_entry(callee_method, caller_nm, static_call_info);
   }
 
   // grab lock, check for deoptimization and potentially patch caller
   {
     CompiledICLocker ml(caller_nm);
@@ -1300,19 +1347,21 @@
 
 // Resolves a call.  The compilers generate code for calls that go here
 // and are patched with the real destination of the call.
 methodHandle SharedRuntime::resolve_sub_helper(JavaThread *thread,
                                                bool is_virtual,
-                                               bool is_optimized, TRAPS) {
+                                               bool is_optimized,
+                                               bool* caller_is_c1, TRAPS) {
 
   ResourceMark rm(thread);
   RegisterMap cbl_map(thread, false);
   frame caller_frame = thread->last_frame().sender(&cbl_map);
 
   CodeBlob* caller_cb = caller_frame.cb();
   guarantee(caller_cb != NULL && caller_cb->is_compiled(), "must be called from compiled method");
   CompiledMethod* caller_nm = caller_cb->as_compiled_method_or_null();
+  *caller_is_c1 = caller_nm->is_compiled_by_c1();
 
   // make sure caller is not getting deoptimized
   // and removed before we are done with it.
   // CLEANUP - with lazy deopt shouldn't need this lock
   nmethodLocker caller_lock(caller_nm);
@@ -1410,18 +1459,19 @@
   frame caller_frame = stub_frame.sender(&reg_map);
   assert(!caller_frame.is_interpreted_frame() && !caller_frame.is_entry_frame(), "unexpected frame");
 #endif /* ASSERT */
 
   methodHandle callee_method;
+  bool is_optimized = false;
+  bool caller_is_c1 = false;
   JRT_BLOCK
-    callee_method = SharedRuntime::handle_ic_miss_helper(thread, CHECK_NULL);
+    callee_method = SharedRuntime::handle_ic_miss_helper(thread, is_optimized, caller_is_c1, CHECK_NULL);
     // Return Method* through TLS
     thread->set_vm_result_2(callee_method());
   JRT_BLOCK_END
   // return compiled code entry point after potential safepoints
-  assert(callee_method->verified_code_entry() != NULL, " Jump to zero!");
-  return callee_method->verified_code_entry();
+  return entry_for_handle_wrong_method(callee_method, false, is_optimized, caller_is_c1);
 JRT_END
 
 
 // Handle call site that has been made non-entrant
 JRT_BLOCK_ENTRY(address, SharedRuntime::handle_wrong_method(JavaThread* thread))
@@ -1460,18 +1510,20 @@
     }
   }
 
   // Must be compiled to compiled path which is safe to stackwalk
   methodHandle callee_method;
+  bool is_static_call = false;
+  bool is_optimized = false;
+  bool caller_is_c1 = false;
   JRT_BLOCK
     // Force resolving of caller (if we called from compiled frame)
-    callee_method = SharedRuntime::reresolve_call_site(thread, CHECK_NULL);
+    callee_method = SharedRuntime::reresolve_call_site(thread, is_static_call, is_optimized, caller_is_c1, CHECK_NULL);
     thread->set_vm_result_2(callee_method());
   JRT_BLOCK_END
   // return compiled code entry point after potential safepoints
-  assert(callee_method->verified_code_entry() != NULL, " Jump to zero!");
-  return callee_method->verified_code_entry();
+  return entry_for_handle_wrong_method(callee_method, is_static_call, is_optimized, caller_is_c1);
 JRT_END
 
 // Handle abstract method call
 JRT_BLOCK_ENTRY(address, SharedRuntime::handle_wrong_method_abstract(JavaThread* thread))
   // Verbose error message for AbstractMethodError.
@@ -1505,65 +1557,75 @@
 
 
 // resolve a static call and patch code
 JRT_BLOCK_ENTRY(address, SharedRuntime::resolve_static_call_C(JavaThread *thread ))
   methodHandle callee_method;
+  bool caller_is_c1;
   JRT_BLOCK
-    callee_method = SharedRuntime::resolve_helper(thread, false, false, CHECK_NULL);
+    callee_method = SharedRuntime::resolve_helper(thread, false, false, &caller_is_c1, CHECK_NULL);
     thread->set_vm_result_2(callee_method());
   JRT_BLOCK_END
   // return compiled code entry point after potential safepoints
-  assert(callee_method->verified_code_entry() != NULL, " Jump to zero!");
-  return callee_method->verified_code_entry();
+  address entry = caller_is_c1 ?
+    callee_method->verified_value_code_entry() : callee_method->verified_code_entry();
+  assert(entry != NULL, "Jump to zero!");
+  return entry;
 JRT_END
 
 
 // resolve virtual call and update inline cache to monomorphic
 JRT_BLOCK_ENTRY(address, SharedRuntime::resolve_virtual_call_C(JavaThread *thread ))
   methodHandle callee_method;
+  bool caller_is_c1;
   JRT_BLOCK
-    callee_method = SharedRuntime::resolve_helper(thread, true, false, CHECK_NULL);
+    callee_method = SharedRuntime::resolve_helper(thread, true, false, &caller_is_c1, CHECK_NULL);
     thread->set_vm_result_2(callee_method());
   JRT_BLOCK_END
   // return compiled code entry point after potential safepoints
-  assert(callee_method->verified_code_entry() != NULL, " Jump to zero!");
-  return callee_method->verified_code_entry();
+  address entry = caller_is_c1 ?
+    callee_method->verified_value_code_entry() : callee_method->verified_value_ro_code_entry();
+  assert(entry != NULL, "Jump to zero!");
+  return entry;
 JRT_END
 
 
 // Resolve a virtual call that can be statically bound (e.g., always
 // monomorphic, so it has no inline cache).  Patch code to resolved target.
 JRT_BLOCK_ENTRY(address, SharedRuntime::resolve_opt_virtual_call_C(JavaThread *thread))
   methodHandle callee_method;
+  bool caller_is_c1;
   JRT_BLOCK
-    callee_method = SharedRuntime::resolve_helper(thread, true, true, CHECK_NULL);
+    callee_method = SharedRuntime::resolve_helper(thread, true, true, &caller_is_c1, CHECK_NULL);
     thread->set_vm_result_2(callee_method());
   JRT_BLOCK_END
   // return compiled code entry point after potential safepoints
-  assert(callee_method->verified_code_entry() != NULL, " Jump to zero!");
-  return callee_method->verified_code_entry();
+  address entry = caller_is_c1 ?
+    callee_method->verified_value_code_entry() : callee_method->verified_code_entry();
+  assert(entry != NULL, "Jump to zero!");
+  return entry;
 JRT_END
 
 // The handle_ic_miss_helper_internal function returns false if it failed due
 // to either running out of vtable stubs or ic stubs due to IC transitions
 // to transitional states. The needs_ic_stub_refill value will be set if
 // the failure was due to running out of IC stubs, in which case handle_ic_miss_helper
 // refills the IC stubs and tries again.
 bool SharedRuntime::handle_ic_miss_helper_internal(Handle receiver, CompiledMethod* caller_nm,
                                                    const frame& caller_frame, methodHandle callee_method,
                                                    Bytecodes::Code bc, CallInfo& call_info,
-                                                   bool& needs_ic_stub_refill, TRAPS) {
+                                                   bool& needs_ic_stub_refill, bool& is_optimized, bool caller_is_c1, TRAPS) {
   CompiledICLocker ml(caller_nm);
   CompiledIC* inline_cache = CompiledIC_before(caller_nm, caller_frame.pc());
   bool should_be_mono = false;
   if (inline_cache->is_optimized()) {
     if (TraceCallFixup) {
       ResourceMark rm(THREAD);
       tty->print("OPTIMIZED IC miss (%s) call to", Bytecodes::name(bc));
       callee_method->print_short_name(tty);
       tty->print_cr(" code: " INTPTR_FORMAT, p2i(callee_method->code()));
     }
+    is_optimized = true;
     should_be_mono = true;
   } else if (inline_cache->is_icholder_call()) {
     CompiledICHolder* ic_oop = inline_cache->cached_icholder();
     if (ic_oop != NULL) {
       if (!ic_oop->is_loader_alive()) {
@@ -1597,19 +1659,20 @@
     Klass* receiver_klass = receiver()->klass();
     inline_cache->compute_monomorphic_entry(callee_method,
                                             receiver_klass,
                                             inline_cache->is_optimized(),
                                             false, caller_nm->is_nmethod(),
+                                            caller_nm->is_compiled_by_c1(),
                                             info, CHECK_false);
     if (!inline_cache->set_to_monomorphic(info)) {
       needs_ic_stub_refill = true;
       return false;
     }
   } else if (!inline_cache->is_megamorphic() && !inline_cache->is_clean()) {
     // Potential change to megamorphic
 
-    bool successful = inline_cache->set_to_megamorphic(&call_info, bc, needs_ic_stub_refill, CHECK_false);
+    bool successful = inline_cache->set_to_megamorphic(&call_info, bc, needs_ic_stub_refill, caller_is_c1, CHECK_false);
     if (needs_ic_stub_refill) {
       return false;
     }
     if (!successful) {
       if (!inline_cache->set_to_clean()) {
@@ -1621,11 +1684,11 @@
     // Either clean or megamorphic
   }
   return true;
 }
 
-methodHandle SharedRuntime::handle_ic_miss_helper(JavaThread *thread, TRAPS) {
+methodHandle SharedRuntime::handle_ic_miss_helper(JavaThread *thread, bool& is_optimized, bool& caller_is_c1, TRAPS) {
   ResourceMark rm(thread);
   CallInfo call_info;
   Bytecodes::Code bc;
 
   // receiver is NULL for static calls. An exception is thrown for NULL
@@ -1641,11 +1704,13 @@
   // plain ic_miss) and the site will be converted to an optimized virtual call site
   // never to miss again. I don't believe C2 will produce code like this but if it
   // did this would still be the correct thing to do for it too, hence no ifdef.
   //
   if (call_info.resolved_method()->can_be_statically_bound()) {
-    methodHandle callee_method = SharedRuntime::reresolve_call_site(thread, CHECK_(methodHandle()));
+    bool is_static_call = false;
+    methodHandle callee_method = SharedRuntime::reresolve_call_site(thread, is_static_call, is_optimized, caller_is_c1, CHECK_(methodHandle()));
+    assert(!is_static_call, "IC miss at static call?");
     if (TraceCallFixup) {
       RegisterMap reg_map(thread, false);
       frame caller_frame = thread->last_frame().sender(&reg_map);
       ResourceMark rm(thread);
       tty->print("converting IC miss to reresolve (%s) call to", Bytecodes::name(bc));
@@ -1691,16 +1756,17 @@
   // that refills them.
   RegisterMap reg_map(thread, false);
   frame caller_frame = thread->last_frame().sender(&reg_map);
   CodeBlob* cb = caller_frame.cb();
   CompiledMethod* caller_nm = cb->as_compiled_method();
+  caller_is_c1 = caller_nm->is_compiled_by_c1();
 
   for (;;) {
     ICRefillVerifier ic_refill_verifier;
     bool needs_ic_stub_refill = false;
     bool successful = handle_ic_miss_helper_internal(receiver, caller_nm, caller_frame, callee_method,
-                                                     bc, call_info, needs_ic_stub_refill, CHECK_(methodHandle()));
+                                                     bc, call_info, needs_ic_stub_refill, is_optimized, caller_is_c1, CHECK_(methodHandle()));
     if (successful || !needs_ic_stub_refill) {
       return callee_method;
     } else {
       InlineCacheBuffer::refill_ic_stubs();
     }
@@ -1728,11 +1794,11 @@
 // Resets a call-site in compiled code so it will get resolved again.
 // This routines handles both virtual call sites, optimized virtual call
 // sites, and static call sites. Typically used to change a call sites
 // destination from compiled to interpreted.
 //
-methodHandle SharedRuntime::reresolve_call_site(JavaThread *thread, TRAPS) {
+methodHandle SharedRuntime::reresolve_call_site(JavaThread *thread, bool& is_static_call, bool& is_optimized, bool& caller_is_c1, TRAPS) {
   ResourceMark rm(thread);
   RegisterMap reg_map(thread, false);
   frame stub_frame = thread->last_frame();
   assert(stub_frame.is_runtime_frame(), "must be a runtimeStub");
   frame caller = stub_frame.sender(&reg_map);
@@ -1744,11 +1810,11 @@
   if (caller.is_compiled_frame() && !caller.is_deoptimized_frame()) {
 
     address pc = caller.pc();
 
     // Check for static or virtual call
-    bool is_static_call = false;
+    CompiledMethod* caller_nm = CodeCache::find_compiled(pc);
     CompiledMethod* caller_nm = CodeCache::find_compiled(pc);
 
     // Default call_addr is the location of the "basic" call.
     // Determine the address of the call we a reresolving. With
     // Inline Caches we will always find a recognizable call.
@@ -1789,10 +1855,11 @@
           is_static_call = true;
         } else {
           assert(iter.type() == relocInfo::virtual_call_type ||
                  iter.type() == relocInfo::opt_virtual_call_type
                 , "unexpected relocInfo. type");
+          is_optimized = (iter.type() == relocInfo::opt_virtual_call_type);
         }
       } else {
         assert(!UseInlineCaches, "relocation info. must exist for this address");
       }
 
@@ -1814,11 +1881,10 @@
     }
   }
 
   methodHandle callee_method = find_callee_method(thread, CHECK_(methodHandle()));
 
-
 #ifndef PRODUCT
   Atomic::inc(&_wrong_method_ctr);
 
   if (TraceCallFixup) {
     ResourceMark rm(thread);
@@ -1909,12 +1975,10 @@
 // interpreted. If the caller is compiled we attempt to patch the caller
 // so he no longer calls into the interpreter.
 JRT_LEAF(void, SharedRuntime::fixup_callers_callsite(Method* method, address caller_pc))
   Method* moop(method);
 
-  address entry_point = moop->from_compiled_entry_no_trampoline();
-
   // It's possible that deoptimization can occur at a call site which hasn't
   // been resolved yet, in which case this function will be called from
   // an nmethod that has been patched for deopt and we can ignore the
   // request for a fixup.
   // Also it is possible that we lost a race in that from_compiled_entry
@@ -1922,11 +1986,15 @@
   // we did we'd leap into space because the callsite needs to use
   // "to interpreter" stub in order to load up the Method*. Don't
   // ask me how I know this...
 
   CodeBlob* cb = CodeCache::find_blob(caller_pc);
-  if (cb == NULL || !cb->is_compiled() || entry_point == moop->get_c2i_entry()) {
+  if (cb == NULL || !cb->is_compiled()) {
+    return;
+  }
+  address entry_point = moop->from_compiled_entry_no_trampoline(cb->is_compiled_by_c1());
+  if (entry_point == moop->get_c2i_entry()) {
     return;
   }
 
   // The check above makes sure this is a nmethod.
   CompiledMethod* nm = cb->as_compiled_method_or_null();
@@ -2283,18 +2351,31 @@
                // Otherwise _value._fingerprint is the array.
 
   // Remap BasicTypes that are handled equivalently by the adapters.
   // These are correct for the current system but someday it might be
   // necessary to make this mapping platform dependent.
-  static int adapter_encoding(BasicType in) {
+  static int adapter_encoding(BasicType in, bool is_valuetype) {
     switch (in) {
       case T_BOOLEAN:
       case T_BYTE:
       case T_SHORT:
-      case T_CHAR:
-        // There are all promoted to T_INT in the calling convention
-        return T_INT;
+      case T_CHAR: {
+        if (is_valuetype) {
+          // Do not widen value type field types
+          assert(ValueTypePassFieldsAsArgs, "must be enabled");
+          return in;
+        } else {
+          // They are all promoted to T_INT in the calling convention
+          return T_INT;
+        }
+      }
+
+      case T_VALUETYPE: {
+        // If value types are passed as fields, return 'in' to differentiate
+        // between a T_VALUETYPE and a T_OBJECT in the signature.
+        return ValueTypePassFieldsAsArgs ? in : adapter_encoding(T_OBJECT, false);
+      }
 
       case T_OBJECT:
       case T_ARRAY:
         // In other words, we assume that any register good enough for
         // an int or long is good enough for a managed pointer.
@@ -2316,13 +2397,14 @@
         return T_CONFLICT;
     }
   }
 
  public:
-  AdapterFingerPrint(int total_args_passed, BasicType* sig_bt) {
+  AdapterFingerPrint(const GrowableArray<SigEntry>* sig, bool has_ro_adapter = false) {
     // The fingerprint is based on the BasicType signature encoded
     // into an array of ints with eight entries per int.
+    int total_args_passed = (sig != NULL) ? sig->length() : 0;
     int* ptr;
     int len = (total_args_passed + (_basic_types_per_int-1)) / _basic_types_per_int;
     if (len <= _compact_int_count) {
       assert(_compact_int_count == 3, "else change next line");
       _value._compact[0] = _value._compact[1] = _value._compact[2] = 0;
@@ -2336,21 +2418,41 @@
       ptr = _value._fingerprint;
     }
 
     // Now pack the BasicTypes with 8 per int
     int sig_index = 0;
+    BasicType prev_sbt = T_ILLEGAL;
+    int vt_count = 0;
     for (int index = 0; index < len; index++) {
       int value = 0;
       for (int byte = 0; byte < _basic_types_per_int; byte++) {
-        int bt = ((sig_index < total_args_passed)
-                  ? adapter_encoding(sig_bt[sig_index++])
-                  : 0);
+        int bt = 0;
+        if (sig_index < total_args_passed) {
+          BasicType sbt = sig->at(sig_index++)._bt;
+          if (ValueTypePassFieldsAsArgs && sbt == T_VALUETYPE) {
+            // Found start of value type in signature
+            vt_count++;
+            if (sig_index == 1 && has_ro_adapter) {
+              // With a ro_adapter, replace receiver value type delimiter by T_VOID to prevent matching
+              // with other adapters that have the same value type as first argument and no receiver.
+              sbt = T_VOID;
+            }
+          } else if (ValueTypePassFieldsAsArgs && sbt == T_VOID &&
+                     prev_sbt != T_LONG && prev_sbt != T_DOUBLE) {
+            // Found end of value type in signature
+            vt_count--;
+            assert(vt_count >= 0, "invalid vt_count");
+          }
+          bt = adapter_encoding(sbt, vt_count > 0);
+          prev_sbt = sbt;
+        }
         assert((bt & _basic_type_mask) == bt, "must fit in 4 bits");
         value = (value << _basic_type_bits) | bt;
       }
       ptr[index] = value;
     }
+    assert(vt_count == 0, "invalid vt_count");
   }
 
   ~AdapterFingerPrint() {
     if (_length > 0) {
       FREE_C_HEAP_ARRAY(int, _value._fingerprint);
@@ -2432,13 +2534,16 @@
  public:
   AdapterHandlerTable()
     : BasicHashtable<mtCode>(293, (DumpSharedSpaces ? sizeof(CDSAdapterHandlerEntry) : sizeof(AdapterHandlerEntry))) { }
 
   // Create a new entry suitable for insertion in the table
-  AdapterHandlerEntry* new_entry(AdapterFingerPrint* fingerprint, address i2c_entry, address c2i_entry, address c2i_unverified_entry, address c2i_no_clinit_check_entry) {
+  AdapterHandlerEntry* new_entry(AdapterFingerPrint* fingerprint, address i2c_entry, address c2i_entry,
+                                 address c2i_value_entry, address c2i_value_ro_entry,
+                                 address c2i_unverified_entry, address c2i_unverified_value_entry, address c2i_no_clinit_check_entry) {
     AdapterHandlerEntry* entry = (AdapterHandlerEntry*)BasicHashtable<mtCode>::new_entry(fingerprint->compute_hash());
-    entry->init(fingerprint, i2c_entry, c2i_entry, c2i_unverified_entry, c2i_no_clinit_check_entry);
+    entry->init(fingerprint, i2c_entry, c2i_entry, c2i_value_entry, c2i_value_ro_entry,
+                c2i_unverified_entry, c2i_unverified_value_entry, c2i_no_clinit_check_entry);
     if (DumpSharedSpaces) {
       ((CDSAdapterHandlerEntry*)entry)->init();
     }
     return entry;
   }
@@ -2453,13 +2558,13 @@
     entry->deallocate();
     BasicHashtable<mtCode>::free_entry(entry);
   }
 
   // Find a entry with the same fingerprint if it exists
-  AdapterHandlerEntry* lookup(int total_args_passed, BasicType* sig_bt) {
+  AdapterHandlerEntry* lookup(const GrowableArray<SigEntry>* sig, bool has_ro_adapter = false) {
     NOT_PRODUCT(_lookups++);
-    AdapterFingerPrint fp(total_args_passed, sig_bt);
+    AdapterFingerPrint fp(sig, has_ro_adapter);
     unsigned int hash = fp.compute_hash();
     int index = hash_to_index(hash);
     for (AdapterHandlerEntry* e = bucket(index); e != NULL; e = e->next()) {
       NOT_PRODUCT(_buckets++);
       if (e->hash() == hash) {
@@ -2551,11 +2656,11 @@
 
 // ---------------------------------------------------------------------------
 // Implementation of AdapterHandlerLibrary
 AdapterHandlerTable* AdapterHandlerLibrary::_adapters = NULL;
 AdapterHandlerEntry* AdapterHandlerLibrary::_abstract_method_handler = NULL;
-const int AdapterHandlerLibrary_size = 16*K;
+const int AdapterHandlerLibrary_size = 32*K;
 BufferBlob* AdapterHandlerLibrary::_buffer = NULL;
 
 BufferBlob* AdapterHandlerLibrary::buffer_blob() {
   // Should be called only when AdapterHandlerLibrary_lock is active.
   if (_buffer == NULL) // Initialize lazily
@@ -2575,86 +2680,309 @@
   // are never compiled so an i2c entry is somewhat meaningless, but
   // throw AbstractMethodError just in case.
   // Pass wrong_method_abstract for the c2i transitions to return
   // AbstractMethodError for invalid invocations.
   address wrong_method_abstract = SharedRuntime::get_handle_wrong_method_abstract_stub();
-  _abstract_method_handler = AdapterHandlerLibrary::new_entry(new AdapterFingerPrint(0, NULL),
+  _abstract_method_handler = AdapterHandlerLibrary::new_entry(new AdapterFingerPrint(NULL),
                                                               StubRoutines::throw_AbstractMethodError_entry(),
+                                                              wrong_method_abstract, wrong_method_abstract, wrong_method_abstract,
                                                               wrong_method_abstract, wrong_method_abstract);
 }
 
 AdapterHandlerEntry* AdapterHandlerLibrary::new_entry(AdapterFingerPrint* fingerprint,
                                                       address i2c_entry,
                                                       address c2i_entry,
+                                                      address c2i_value_entry,
+                                                      address c2i_value_ro_entry,
                                                       address c2i_unverified_entry,
+                                                      address c2i_unverified_value_entry,
                                                       address c2i_no_clinit_check_entry) {
-  return _adapters->new_entry(fingerprint, i2c_entry, c2i_entry, c2i_unverified_entry, c2i_no_clinit_check_entry);
+  return _adapters->new_entry(fingerprint, i2c_entry, c2i_entry, c2i_value_entry, c2i_value_ro_entry, c2i_unverified_entry,
+                              c2i_unverified_value_entry, c2i_no_clinit_check_entry);
+}
+
+static void generate_trampoline(address trampoline, address destination) {
+  if (*(int*)trampoline == 0) {
+    CodeBuffer buffer(trampoline, (int)SharedRuntime::trampoline_size());
+    MacroAssembler _masm(&buffer);
+    SharedRuntime::generate_trampoline(&_masm, destination);
+    assert(*(int*)trampoline != 0, "Instruction(s) for trampoline must not be encoded as zeros.");
+      _masm.flush();
+
+    if (PrintInterpreter) {
+      Disassembler::decode(buffer.insts_begin(), buffer.insts_end());
+    }
+  }
 }
 
 AdapterHandlerEntry* AdapterHandlerLibrary::get_adapter(const methodHandle& method) {
   AdapterHandlerEntry* entry = get_adapter0(method);
   if (entry != NULL && method->is_shared()) {
     // See comments around Method::link_method()
     MutexLocker mu(AdapterHandlerLibrary_lock);
     if (method->adapter() == NULL) {
       method->update_adapter_trampoline(entry);
     }
-    address trampoline = method->from_compiled_entry();
-    if (*(int*)trampoline == 0) {
-      CodeBuffer buffer(trampoline, (int)SharedRuntime::trampoline_size());
-      MacroAssembler _masm(&buffer);
-      SharedRuntime::generate_trampoline(&_masm, entry->get_c2i_entry());
-      assert(*(int*)trampoline != 0, "Instruction(s) for trampoline must not be encoded as zeros.");
-      _masm.flush();
+    generate_trampoline(method->from_compiled_entry(),          entry->get_c2i_entry());
+    generate_trampoline(method->from_compiled_value_ro_entry(), entry->get_c2i_value_ro_entry());
+    generate_trampoline(method->from_compiled_value_entry(),    entry->get_c2i_value_entry());
+  }
+
+  return entry;
+}
+
 
-      if (PrintInterpreter) {
-        Disassembler::decode(buffer.insts_begin(), buffer.insts_end());
+CompiledEntrySignature::CompiledEntrySignature(Method* method) :
+  _method(method), _num_value_args(0), _has_value_recv(false),
+  _sig_cc(NULL), _sig_cc_ro(NULL), _regs(NULL), _regs_cc(NULL), _regs_cc_ro(NULL),
+  _args_on_stack(0), _args_on_stack_cc(0), _args_on_stack_cc_ro(0),
+  _c1_needs_stack_repair(false), _c2_needs_stack_repair(false), _has_scalarized_args(false) {
+  _has_reserved_entries = false;
+  _sig = new GrowableArray<SigEntry>(method->size_of_parameters());
+
+}
+
+int CompiledEntrySignature::compute_scalarized_cc(GrowableArray<SigEntry>*& sig_cc, VMRegPair*& regs_cc, bool scalar_receiver) {
+  InstanceKlass* holder = _method->method_holder();
+  sig_cc = new GrowableArray<SigEntry>(_method->size_of_parameters());
+  if (!_method->is_static()) {
+    if (holder->is_value() && scalar_receiver && ValueKlass::cast(holder)->is_scalarizable()) {
+      sig_cc->appendAll(ValueKlass::cast(holder)->extended_sig());
+    } else {
+      SigEntry::add_entry(sig_cc, T_OBJECT);
+    }
+  }
+  Thread* THREAD = Thread::current();
+  for (SignatureStream ss(_method->signature()); !ss.at_return_type(); ss.next()) {
+    if (ss.type() == T_VALUETYPE) {
+      ValueKlass* vk = ss.as_value_klass(holder);
+      if (vk->is_scalarizable()) {
+        sig_cc->appendAll(vk->extended_sig());
+      } else {
+        SigEntry::add_entry(sig_cc, T_OBJECT);
       }
+    } else {
+      SigEntry::add_entry(sig_cc, ss.type());
     }
   }
+  regs_cc = NEW_RESOURCE_ARRAY(VMRegPair, sig_cc->length() + 2);
+  return SharedRuntime::java_calling_convention(sig_cc, regs_cc);
+}
 
-  return entry;
+int CompiledEntrySignature::insert_reserved_entry(int ret_off) {
+  // Find index in signature that belongs to return address slot
+  BasicType bt = T_ILLEGAL;
+  int i = 0;
+  for (uint off = 0; i < _sig_cc->length(); ++i) {
+    if (SigEntry::skip_value_delimiters(_sig_cc, i)) {
+      VMReg first = _regs_cc[off++].first();
+      if (first->is_valid() && first->is_stack()) {
+        // Select a type for the reserved entry that will end up on the stack
+        bt = _sig_cc->at(i)._bt;
+        if (((int)first->reg2stack() + VMRegImpl::slots_per_word) == ret_off) {
+          break; // Index of the return address found
+        }
+      }
+    }
+  }
+  // Insert reserved entry and re-compute calling convention
+  SigEntry::insert_reserved_entry(_sig_cc, i, bt);
+  return SharedRuntime::java_calling_convention(_sig_cc, _regs_cc);
+}
+
+// See if we can save space by sharing the same entry for VVEP and VVEP(RO),
+// or the same entry for VEP and VVEP(RO).
+CodeOffsets::Entries CompiledEntrySignature::c1_value_ro_entry_type() const {
+  if (!has_scalarized_args()) {
+    // VEP/VVEP/VVEP(RO) all share the same entry. There's no packing.
+    return CodeOffsets::Verified_Entry;
+  }
+  if (_method->is_static()) {
+    // Static methods don't need VVEP(RO)
+    return CodeOffsets::Verified_Entry;
+  }
+
+  if (has_value_recv()) {
+    if (num_value_args() == 1) {
+      // Share same entry for VVEP and VVEP(RO).
+      // This is quite common: we have an instance method in a ValueKlass that has
+      // no value args other than <this>.
+      return CodeOffsets::Verified_Value_Entry;
+    } else {
+      assert(num_value_args() > 1, "must be");
+      // No sharing:
+      //   VVEP(RO) -- <this> is passed as object
+      //   VEP      -- <this> is passed as fields
+      return CodeOffsets::Verified_Value_Entry_RO;
+    }
+  }
+
+  // Either a static method, or <this> is not a value type
+  if (args_on_stack_cc() != args_on_stack_cc_ro() || _has_reserved_entries) {
+    // No sharing:
+    // Some arguments are passed on the stack, and we have inserted reserved entries
+    // into the VEP, but we never insert reserved entries into the VVEP(RO).
+    return CodeOffsets::Verified_Value_Entry_RO;
+  } else {
+    // Share same entry for VEP and VVEP(RO).
+    return CodeOffsets::Verified_Entry;
+  }
+}
+
+
+void CompiledEntrySignature::compute_calling_conventions() {
+  // Get the (non-scalarized) signature and check for value type arguments
+  if (!_method->is_static()) {
+    if (_method->method_holder()->is_value() && ValueKlass::cast(_method->method_holder())->is_scalarizable()) {
+      _has_value_recv = true;
+      _num_value_args++;
+    }
+    SigEntry::add_entry(_sig, T_OBJECT);
+  }
+  for (SignatureStream ss(_method->signature()); !ss.at_return_type(); ss.next()) {
+    BasicType bt = ss.type();
+    if (bt == T_VALUETYPE) {
+      if (ss.as_value_klass(_method->method_holder())->is_scalarizable()) {
+        _num_value_args++;
+      }
+      bt = T_OBJECT;
+    }
+    SigEntry::add_entry(_sig, bt);
+  }
+  if (_method->is_abstract() && !(ValueTypePassFieldsAsArgs && has_value_arg())) {
+    return;
+  }
+
+  // Get a description of the compiled java calling convention and the largest used (VMReg) stack slot usage
+  _regs = NEW_RESOURCE_ARRAY(VMRegPair, _sig->length());
+  _args_on_stack = SharedRuntime::java_calling_convention(_sig, _regs);
+
+  // Now compute the scalarized calling convention if there are value types in the signature
+  _sig_cc = _sig;
+  _sig_cc_ro = _sig;
+  _regs_cc = _regs;
+  _regs_cc_ro = _regs;
+  _args_on_stack_cc = _args_on_stack;
+  _args_on_stack_cc_ro = _args_on_stack;
+
+  if (ValueTypePassFieldsAsArgs && has_value_arg() && !_method->is_native()) {
+    _args_on_stack_cc = compute_scalarized_cc(_sig_cc, _regs_cc, /* scalar_receiver = */ true);
+
+    _sig_cc_ro = _sig_cc;
+    _regs_cc_ro = _regs_cc;
+    _args_on_stack_cc_ro = _args_on_stack_cc;
+    if (_has_value_recv || _args_on_stack_cc > _args_on_stack) {
+      // For interface calls, we need another entry point / adapter to unpack the receiver
+      _args_on_stack_cc_ro = compute_scalarized_cc(_sig_cc_ro, _regs_cc_ro, /* scalar_receiver = */ false);
+    }
+
+    // Compute the stack extension that is required to convert between the calling conventions.
+    // The stack slots at these offsets are occupied by the return address with the unscalarized
+    // calling convention. Don't use them for arguments with the scalarized calling convention.
+    int ret_off    = _args_on_stack_cc - _args_on_stack;
+    int ret_off_ro = _args_on_stack_cc - _args_on_stack_cc_ro;
+    assert(ret_off_ro <= 0 || ret_off > 0, "receiver unpacking requires more stack space than expected");
+
+    if (ret_off > 0) {
+      // Make sure the stack of the scalarized calling convention with the reserved
+      // entries (2 slots each) remains 16-byte (4 slots) aligned after stack extension.
+      int alignment = StackAlignmentInBytes / VMRegImpl::stack_slot_size;
+      if (ret_off_ro != ret_off && ret_off_ro >= 0) {
+        ret_off    += 4; // Account for two reserved entries (4 slots)
+        ret_off_ro += 4;
+        ret_off     = align_up(ret_off, alignment);
+        ret_off_ro  = align_up(ret_off_ro, alignment);
+        // TODO can we avoid wasting a stack slot here?
+        //assert(ret_off != ret_off_ro, "fail");
+        if (ret_off > ret_off_ro) {
+          swap(ret_off, ret_off_ro); // Sort by offset
+        }
+        _args_on_stack_cc = insert_reserved_entry(ret_off);
+        _args_on_stack_cc = insert_reserved_entry(ret_off_ro);
+      } else {
+        ret_off += 2; // Account for one reserved entry (2 slots)
+        ret_off = align_up(ret_off, alignment);
+        _args_on_stack_cc = insert_reserved_entry(ret_off);
+      }
+
+      _has_reserved_entries = true;
+    }
+
+    // Upper bound on stack arguments to avoid hitting the argument limit and
+    // bailing out of compilation ("unsupported incoming calling sequence").
+    // TODO we need a reasonable limit (flag?) here
+    if (_args_on_stack_cc > 50) {
+      // Don't scalarize value type arguments
+      _sig_cc = _sig;
+      _sig_cc_ro = _sig;
+      _regs_cc = _regs;
+      _regs_cc_ro = _regs;
+      _args_on_stack_cc = _args_on_stack;
+      _args_on_stack_cc_ro = _args_on_stack;
+    } else {
+      _c1_needs_stack_repair = (_args_on_stack_cc < _args_on_stack) || (_args_on_stack_cc_ro < _args_on_stack);
+      _c2_needs_stack_repair = (_args_on_stack_cc > _args_on_stack) || (_args_on_stack_cc > _args_on_stack_cc_ro);
+      _has_scalarized_args = true;
+    }
+  }
 }
 
 AdapterHandlerEntry* AdapterHandlerLibrary::get_adapter0(const methodHandle& method) {
   // Use customized signature handler.  Need to lock around updates to
   // the AdapterHandlerTable (it is not safe for concurrent readers
   // and a single writer: this could be fixed if it becomes a
   // problem).
 
   ResourceMark rm;
 
-  NOT_PRODUCT(int insts_size);
+  NOT_PRODUCT(int insts_size = 0);
   AdapterBlob* new_adapter = NULL;
   AdapterHandlerEntry* entry = NULL;
   AdapterFingerPrint* fingerprint = NULL;
+
   {
     MutexLocker mu(AdapterHandlerLibrary_lock);
     // make sure data structure is initialized
     initialize();
 
-    if (method->is_abstract()) {
-      return _abstract_method_handler;
+    CompiledEntrySignature ces(method());
+    {
+       MutexUnlocker mul(AdapterHandlerLibrary_lock);
+       ces.compute_calling_conventions();
     }
+    GrowableArray<SigEntry>& sig       = ces.sig();
+    GrowableArray<SigEntry>& sig_cc    = ces.sig_cc();
+    GrowableArray<SigEntry>& sig_cc_ro = ces.sig_cc_ro();
+    VMRegPair* regs         = ces.regs();
+    VMRegPair* regs_cc      = ces.regs_cc();
+    VMRegPair* regs_cc_ro   = ces.regs_cc_ro();
 
-    // Fill in the signature array, for the calling-convention call.
-    int total_args_passed = method->size_of_parameters(); // All args on stack
+    if (ces.has_scalarized_args()) {
+      method->set_has_scalarized_args(true);
+      method->set_c1_needs_stack_repair(ces.c1_needs_stack_repair());
+      method->set_c2_needs_stack_repair(ces.c2_needs_stack_repair());
+    }
 
-    BasicType* sig_bt = NEW_RESOURCE_ARRAY(BasicType, total_args_passed);
-    VMRegPair* regs   = NEW_RESOURCE_ARRAY(VMRegPair, total_args_passed);
-    int i = 0;
-    if (!method->is_static())  // Pass in receiver first
-      sig_bt[i++] = T_OBJECT;
-    for (SignatureStream ss(method->signature()); !ss.at_return_type(); ss.next()) {
-      sig_bt[i++] = ss.type();  // Collect remaining bits of signature
-      if (ss.type() == T_LONG || ss.type() == T_DOUBLE)
-        sig_bt[i++] = T_VOID;   // Longs & doubles take 2 Java slots
+    if (method->is_abstract()) {
+      if (ces.has_scalarized_args()) {
+        // Save a C heap allocated version of the signature for abstract methods with scalarized value type arguments
+        address wrong_method_abstract = SharedRuntime::get_handle_wrong_method_abstract_stub();
+        entry = AdapterHandlerLibrary::new_entry(new AdapterFingerPrint(NULL),
+                                                 StubRoutines::throw_AbstractMethodError_entry(),
+                                                 wrong_method_abstract, wrong_method_abstract, wrong_method_abstract,
+                                                 wrong_method_abstract, wrong_method_abstract);
+        GrowableArray<SigEntry>* heap_sig = new (ResourceObj::C_HEAP, mtInternal) GrowableArray<SigEntry>(sig_cc_ro.length(), true);
+        heap_sig->appendAll(&sig_cc_ro);
+        entry->set_sig_cc(heap_sig);
+        return entry;
+      } else {
+        return _abstract_method_handler;
+      }
     }
-    assert(i == total_args_passed, "");
 
     // Lookup method signature's fingerprint
-    entry = _adapters->lookup(total_args_passed, sig_bt);
+    entry = _adapters->lookup(&sig_cc, regs_cc != regs_cc_ro);
 
 #ifdef ASSERT
     AdapterHandlerEntry* shared_entry = NULL;
     // Start adapter sharing verification only after the VM is booted.
     if (VerifyAdapterSharing && (entry != NULL)) {
@@ -2665,14 +2993,11 @@
 
     if (entry != NULL) {
       return entry;
     }
 
-    // Get a description of the compiled java calling convention and the largest used (VMReg) stack slot usage
-    int comp_args_on_stack = SharedRuntime::java_calling_convention(sig_bt, regs, total_args_passed, false);
-
-    // Make a C heap allocated version of the fingerprint to store in the adapter
+    // Make a C heap allocated version of the fingerprint to store in the adapter
     fingerprint = new AdapterFingerPrint(total_args_passed, sig_bt);
 
     // StubRoutines::code2() is initialized after this function can be called. As a result,
     // VerifyAdapterCalls and VerifyAdapterSharing can fail if we re-use code that generated
     // prior to StubRoutines::code2() being set. Checks refer to checks generated in an I2C
@@ -2687,29 +3012,43 @@
       buffer.insts()->initialize_shared_locs((relocInfo*)buffer_locs,
                                              sizeof(buffer_locs)/sizeof(relocInfo));
 
       MacroAssembler _masm(&buffer);
       entry = SharedRuntime::generate_i2c2i_adapters(&_masm,
-                                                     total_args_passed,
-                                                     comp_args_on_stack,
-                                                     sig_bt,
+                                                     ces.args_on_stack(),
+                                                     &sig,
                                                      regs,
-                                                     fingerprint);
+                                                     &sig_cc,
+                                                     regs_cc,
+                                                     &sig_cc_ro,
+                                                     regs_cc_ro,
+                                                     fingerprint,
+                                                     new_adapter);
+
+      if (ces.has_scalarized_args()) {
+        // Save a C heap allocated version of the scalarized signature and store it in the adapter
+        GrowableArray<SigEntry>* heap_sig = new (ResourceObj::C_HEAP, mtInternal) GrowableArray<SigEntry>(sig_cc.length(), true);
+        heap_sig->appendAll(&sig_cc);
+        entry->set_sig_cc(heap_sig);
+      }
+
 #ifdef ASSERT
       if (VerifyAdapterSharing) {
         if (shared_entry != NULL) {
+          if (!shared_entry->compare_code(buf->code_begin(), buffer.insts_size())) {
+            method->print();
+          }
           assert(shared_entry->compare_code(buf->code_begin(), buffer.insts_size()), "code must match");
           // Release the one just created and return the original
           _adapters->free_entry(entry);
           return shared_entry;
         } else  {
           entry->save_code(buf->code_begin(), buffer.insts_size());
         }
       }
 #endif
 
-      new_adapter = AdapterBlob::create(&buffer);
       NOT_PRODUCT(insts_size = buffer.insts_size());
     }
     if (new_adapter == NULL) {
       // CodeCache is full, disable compilation
       // Ought to log this but compile log is only per compile thread
@@ -2761,11 +3100,14 @@
 
 address AdapterHandlerEntry::base_address() {
   address base = _i2c_entry;
   if (base == NULL)  base = _c2i_entry;
   assert(base <= _c2i_entry || _c2i_entry == NULL, "");
+  assert(base <= _c2i_value_entry || _c2i_value_entry == NULL, "");
+  assert(base <= _c2i_value_ro_entry || _c2i_value_ro_entry == NULL, "");
   assert(base <= _c2i_unverified_entry || _c2i_unverified_entry == NULL, "");
+  assert(base <= _c2i_unverified_value_entry || _c2i_unverified_value_entry == NULL, "");
   assert(base <= _c2i_no_clinit_check_entry || _c2i_no_clinit_check_entry == NULL, "");
   return base;
 }
 
 void AdapterHandlerEntry::relocate(address new_base) {
@@ -2774,20 +3116,29 @@
   ptrdiff_t delta = new_base - old_base;
   if (_i2c_entry != NULL)
     _i2c_entry += delta;
   if (_c2i_entry != NULL)
     _c2i_entry += delta;
+  if (_c2i_value_entry != NULL)
+    _c2i_value_entry += delta;
+  if (_c2i_value_ro_entry != NULL)
+    _c2i_value_ro_entry += delta;
   if (_c2i_unverified_entry != NULL)
     _c2i_unverified_entry += delta;
+  if (_c2i_unverified_value_entry != NULL)
+    _c2i_unverified_value_entry += delta;
   if (_c2i_no_clinit_check_entry != NULL)
     _c2i_no_clinit_check_entry += delta;
   assert(base_address() == new_base, "");
 }
 
 
 void AdapterHandlerEntry::deallocate() {
   delete _fingerprint;
+  if (_sig_cc != NULL) {
+    delete _sig_cc;
+  }
 #ifdef ASSERT
   FREE_C_HEAP_ARRAY(unsigned char, _saved_code);
 #endif
 }
 
@@ -2861,11 +3212,12 @@
       int i=0;
       if (!method->is_static())  // Pass in receiver first
         sig_bt[i++] = T_OBJECT;
       SignatureStream ss(method->signature());
       for (; !ss.at_return_type(); ss.next()) {
-        sig_bt[i++] = ss.type();  // Collect remaining bits of signature
+        BasicType bt = ss.type();
+        sig_bt[i++] = bt;  // Collect remaining bits of signature
         if (ss.type() == T_LONG || ss.type() == T_DOUBLE)
           sig_bt[i++] = T_VOID;   // Longs & doubles take 2 Java slots
       }
       assert(i == total_args_passed, "");
       BasicType ret_type = ss.type();
@@ -3109,12 +3461,21 @@
     st->print(" i2c: " INTPTR_FORMAT, p2i(get_i2c_entry()));
   }
   if (get_c2i_entry() != NULL) {
     st->print(" c2i: " INTPTR_FORMAT, p2i(get_c2i_entry()));
   }
+  if (get_c2i_entry() != NULL) {
+    st->print(" c2iVE: " INTPTR_FORMAT, p2i(get_c2i_value_entry()));
+  }
+  if (get_c2i_entry() != NULL) {
+    st->print(" c2iVROE: " INTPTR_FORMAT, p2i(get_c2i_value_ro_entry()));
+  }
   if (get_c2i_unverified_entry() != NULL) {
-    st->print(" c2iUV: " INTPTR_FORMAT, p2i(get_c2i_unverified_entry()));
+    st->print(" c2iUE: " INTPTR_FORMAT, p2i(get_c2i_unverified_entry()));
+  }
+  if (get_c2i_unverified_entry() != NULL) {
+    st->print(" c2iUVE: " INTPTR_FORMAT, p2i(get_c2i_unverified_value_entry()));
   }
   if (get_c2i_no_clinit_check_entry() != NULL) {
     st->print(" c2iNCI: " INTPTR_FORMAT, p2i(get_c2i_no_clinit_check_entry()));
   }
   st->cr();
@@ -3123,10 +3484,12 @@
 #if INCLUDE_CDS
 
 void CDSAdapterHandlerEntry::init() {
   assert(DumpSharedSpaces, "used during dump time only");
   _c2i_entry_trampoline = (address)MetaspaceShared::misc_code_space_alloc(SharedRuntime::trampoline_size());
+  _c2i_value_ro_entry_trampoline = (address)MetaspaceShared::misc_code_space_alloc(SharedRuntime::trampoline_size());
+  _c2i_value_entry_trampoline = (address)MetaspaceShared::misc_code_space_alloc(SharedRuntime::trampoline_size());
   _adapter_trampoline = (AdapterHandlerEntry**)MetaspaceShared::misc_code_space_alloc(sizeof(AdapterHandlerEntry*));
 };
 
 #endif // INCLUDE_CDS
 
@@ -3208,5 +3571,211 @@
   if (new_obj == NULL) return;
 
   BarrierSet *bs = BarrierSet::barrier_set();
   bs->on_slowpath_allocation_exit(thread, new_obj);
 }
+
+// We are at a compiled code to interpreter call. We need backing
+// buffers for all value type arguments. Allocate an object array to
+// hold them (convenient because once we're done with it we don't have
+// to worry about freeing it).
+oop SharedRuntime::allocate_value_types_impl(JavaThread* thread, methodHandle callee, bool allocate_receiver, TRAPS) {
+  assert(ValueTypePassFieldsAsArgs, "no reason to call this");
+  ResourceMark rm;
+
+  int nb_slots = 0;
+  InstanceKlass* holder = callee->method_holder();
+  allocate_receiver &= !callee->is_static() && holder->is_value();
+  if (allocate_receiver) {
+    nb_slots++;
+  }
+  for (SignatureStream ss(callee->signature()); !ss.at_return_type(); ss.next()) {
+    if (ss.type() == T_VALUETYPE) {
+      nb_slots++;
+    }
+  }
+  objArrayOop array_oop = oopFactory::new_objectArray(nb_slots, CHECK_NULL);
+  objArrayHandle array(THREAD, array_oop);
+  int i = 0;
+  if (allocate_receiver) {
+    ValueKlass* vk = ValueKlass::cast(holder);
+    oop res = vk->allocate_instance(CHECK_NULL);
+    array->obj_at_put(i, res);
+    i++;
+  }
+  for (SignatureStream ss(callee->signature()); !ss.at_return_type(); ss.next()) {
+    if (ss.type() == T_VALUETYPE) {
+      ValueKlass* vk = ss.as_value_klass(holder);
+      oop res = vk->allocate_instance(CHECK_NULL);
+      array->obj_at_put(i, res);
+      i++;
+    }
+  }
+  return array();
+}
+
+JRT_ENTRY(void, SharedRuntime::allocate_value_types(JavaThread* thread, Method* callee_method, bool allocate_receiver))
+  methodHandle callee(thread, callee_method);
+  oop array = SharedRuntime::allocate_value_types_impl(thread, callee, allocate_receiver, CHECK);
+  thread->set_vm_result(array);
+  thread->set_vm_result_2(callee()); // TODO: required to keep callee live?
+JRT_END
+
+// TODO remove this once the AARCH64 dependency is gone
+// Iterate over the array of heap allocated value types and apply the GC post barrier to all reference fields.
+// This is called from the C2I adapter after value type arguments are heap allocated and initialized.
+JRT_LEAF(void, SharedRuntime::apply_post_barriers(JavaThread* thread, objArrayOopDesc* array))
+{
+  assert(ValueTypePassFieldsAsArgs, "no reason to call this");
+  assert(oopDesc::is_oop(array), "should be oop");
+  for (int i = 0; i < array->length(); ++i) {
+    instanceOop valueOop = (instanceOop)array->obj_at(i);
+    ValueKlass* vk = ValueKlass::cast(valueOop->klass());
+    if (vk->contains_oops()) {
+      const address dst_oop_addr = ((address) (void*) valueOop);
+      OopMapBlock* map = vk->start_of_nonstatic_oop_maps();
+      OopMapBlock* const end = map + vk->nonstatic_oop_map_count();
+      while (map != end) {
+        address doop_address = dst_oop_addr + map->offset();
+        barrier_set_cast<ModRefBarrierSet>(BarrierSet::barrier_set())->
+          write_ref_array((HeapWord*) doop_address, map->count());
+        map++;
+      }
+    }
+  }
+}
+JRT_END
+
+// We're returning from an interpreted method: load each field into a
+// register following the calling convention
+JRT_LEAF(void, SharedRuntime::load_value_type_fields_in_regs(JavaThread* thread, oopDesc* res))
+{
+  assert(res->klass()->is_value(), "only value types here");
+  ResourceMark rm;
+  RegisterMap reg_map(thread);
+  frame stubFrame = thread->last_frame();
+  frame callerFrame = stubFrame.sender(&reg_map);
+  assert(callerFrame.is_interpreted_frame(), "should be coming from interpreter");
+
+  ValueKlass* vk = ValueKlass::cast(res->klass());
+
+  const Array<SigEntry>* sig_vk = vk->extended_sig();
+  const Array<VMRegPair>* regs = vk->return_regs();
+
+  if (regs == NULL) {
+    // The fields of the value klass don't fit in registers, bail out
+    return;
+  }
+
+  int j = 1;
+  for (int i = 0; i < sig_vk->length(); i++) {
+    BasicType bt = sig_vk->at(i)._bt;
+    if (bt == T_VALUETYPE) {
+      continue;
+    }
+    if (bt == T_VOID) {
+      if (sig_vk->at(i-1)._bt == T_LONG ||
+          sig_vk->at(i-1)._bt == T_DOUBLE) {
+        j++;
+      }
+      continue;
+    }
+    int off = sig_vk->at(i)._offset;
+    assert(off > 0, "offset in object should be positive");
+    VMRegPair pair = regs->at(j);
+    address loc = reg_map.location(pair.first());
+    switch(bt) {
+    case T_BOOLEAN:
+      *(jboolean*)loc = res->bool_field(off);
+      break;
+    case T_CHAR:
+      *(jchar*)loc = res->char_field(off);
+      break;
+    case T_BYTE:
+      *(jbyte*)loc = res->byte_field(off);
+      break;
+    case T_SHORT:
+      *(jshort*)loc = res->short_field(off);
+      break;
+    case T_INT: {
+      *(jint*)loc = res->int_field(off);
+      break;
+    }
+    case T_LONG:
+#ifdef _LP64
+      *(intptr_t*)loc = res->long_field(off);
+#else
+      Unimplemented();
+#endif
+      break;
+    case T_OBJECT:
+    case T_ARRAY: {
+      *(oop*)loc = res->obj_field(off);
+      break;
+    }
+    case T_FLOAT:
+      *(jfloat*)loc = res->float_field(off);
+      break;
+    case T_DOUBLE:
+      *(jdouble*)loc = res->double_field(off);
+      break;
+    default:
+      ShouldNotReachHere();
+    }
+    j++;
+  }
+  assert(j == regs->length(), "missed a field?");
+
+#ifdef ASSERT
+  VMRegPair pair = regs->at(0);
+  address loc = reg_map.location(pair.first());
+  assert(*(oopDesc**)loc == res, "overwritten object");
+#endif
+
+  thread->set_vm_result(res);
+}
+JRT_END
+
+// We've returned to an interpreted method, the interpreter needs a
+// reference to a value type instance. Allocate it and initialize it
+// from field's values in registers.
+JRT_BLOCK_ENTRY(void, SharedRuntime::store_value_type_fields_to_buf(JavaThread* thread, intptr_t res))
+{
+  ResourceMark rm;
+  RegisterMap reg_map(thread);
+  frame stubFrame = thread->last_frame();
+  frame callerFrame = stubFrame.sender(&reg_map);
+
+#ifdef ASSERT
+  ValueKlass* verif_vk = ValueKlass::returned_value_klass(reg_map);
+#endif
+
+  if (!is_set_nth_bit(res, 0)) {
+    // We're not returning with value type fields in registers (the
+    // calling convention didn't allow it for this value klass)
+    assert(!Metaspace::contains((void*)res), "should be oop or pointer in buffer area");
+    thread->set_vm_result((oopDesc*)res);
+    assert(verif_vk == NULL, "broken calling convention");
+    return;
+  }
+
+  clear_nth_bit(res, 0);
+  ValueKlass* vk = (ValueKlass*)res;
+  assert(verif_vk == vk, "broken calling convention");
+  assert(Metaspace::contains((void*)res), "should be klass");
+
+  // Allocate handles for every oop field so they are safe in case of
+  // a safepoint when allocating
+  GrowableArray<Handle> handles;
+  vk->save_oop_fields(reg_map, handles);
+
+  // It's unsafe to safepoint until we are here
+  JRT_BLOCK;
+  {
+    Thread* THREAD = thread;
+    oop vt = vk->realloc_result(reg_map, handles, CHECK);
+    thread->set_vm_result(vt);
+  }
+  JRT_BLOCK_END;
+}
+JRT_END
+
diff a/src/hotspot/share/runtime/sharedRuntime.hpp b/src/hotspot/share/runtime/sharedRuntime.hpp
--- a/src/hotspot/share/runtime/sharedRuntime.hpp
+++ b/src/hotspot/share/runtime/sharedRuntime.hpp
@@ -23,22 +23,25 @@
  */
 
 #ifndef SHARE_RUNTIME_SHAREDRUNTIME_HPP
 #define SHARE_RUNTIME_SHAREDRUNTIME_HPP
 
+#include "asm/codeBuffer.hpp"
 #include "interpreter/bytecodeHistogram.hpp"
 #include "interpreter/bytecodeTracer.hpp"
 #include "interpreter/linkResolver.hpp"
 #include "memory/allocation.hpp"
 #include "memory/resourceArea.hpp"
+#include "runtime/signature.hpp"
 #include "utilities/hashtable.hpp"
 #include "utilities/macros.hpp"
 
 class AdapterHandlerEntry;
 class AdapterHandlerTable;
 class AdapterFingerPrint;
 class vframeStream;
+class SigEntry;
 
 // Runtime is the base class for various runtime interfaces
 // (InterpreterRuntime, CompilerRuntime, etc.). It provides
 // shared functionality such as exception forwarding (C++ to
 // Java exceptions), locking/unlocking mechanisms, statistical
@@ -51,21 +54,21 @@
   static bool resolve_sub_helper_internal(methodHandle callee_method, const frame& caller_frame,
                                           CompiledMethod* caller_nm, bool is_virtual, bool is_optimized,
                                           Handle receiver, CallInfo& call_info, Bytecodes::Code invoke_code, TRAPS);
   static methodHandle resolve_sub_helper(JavaThread *thread,
                                          bool is_virtual,
-                                         bool is_optimized, TRAPS);
+                                         bool is_optimized,
+                                         bool* caller_is_c1, TRAPS);
 
   // Shared stub locations
 
   static RuntimeStub*        _wrong_method_blob;
   static RuntimeStub*        _wrong_method_abstract_blob;
   static RuntimeStub*        _ic_miss_blob;
   static RuntimeStub*        _resolve_opt_virtual_call_blob;
   static RuntimeStub*        _resolve_virtual_call_blob;
   static RuntimeStub*        _resolve_static_call_blob;
-  static address             _resolve_static_call_entry;
 
   static DeoptimizationBlob* _deopt_blob;
 
   static SafepointBlob*      _polling_page_vectors_safepoint_handler_blob;
   static SafepointBlob*      _polling_page_safepoint_handler_blob;
@@ -82,11 +85,10 @@
 
  private:
   enum { POLL_AT_RETURN,  POLL_AT_LOOP, POLL_AT_VECTOR_LOOP };
   static SafepointBlob* generate_handler_blob(address call_ptr, int poll_type);
   static RuntimeStub*   generate_resolve_blob(address destination, const char* name);
-
  public:
   static void generate_stubs(void);
 
   // max bytes for each dtrace string parameter
   enum { max_dtrace_string_size = 256 };
@@ -316,37 +318,51 @@
 
   // Resolves a call site- may patch in the destination of the call into the
   // compiled code.
   static methodHandle resolve_helper(JavaThread *thread,
                                      bool is_virtual,
-                                     bool is_optimized, TRAPS);
+                                     bool is_optimized,
+                                     bool* caller_is_c1, TRAPS);
 
  private:
   // deopt blob
   static void generate_deopt_blob(void);
 
   static bool handle_ic_miss_helper_internal(Handle receiver, CompiledMethod* caller_nm, const frame& caller_frame,
                                              methodHandle callee_method, Bytecodes::Code bc, CallInfo& call_info,
-                                             bool& needs_ic_stub_refill, TRAPS);
+                                             bool& needs_ic_stub_refill, bool& is_optimized, bool caller_is_c1, TRAPS);
 
  public:
   static DeoptimizationBlob* deopt_blob(void)      { return _deopt_blob; }
 
   // Resets a call-site in compiled code so it will get resolved again.
-  static methodHandle reresolve_call_site(JavaThread *thread, TRAPS);
+  static methodHandle reresolve_call_site(JavaThread *thread, bool& is_static_call, bool& is_optimized, bool& caller_is_c1, TRAPS);
 
   // In the code prolog, if the klass comparison fails, the inline cache
   // misses and the call site is patched to megamorphic
-  static methodHandle handle_ic_miss_helper(JavaThread* thread, TRAPS);
+  static methodHandle handle_ic_miss_helper(JavaThread* thread, bool& is_optimized, bool& caller_is_c1, TRAPS);
 
   // Find the method that called us.
   static methodHandle find_callee_method(JavaThread* thread, TRAPS);
 
   static void monitor_enter_helper(oopDesc* obj, BasicLock* lock, JavaThread* thread);
 
   static void monitor_exit_helper(oopDesc* obj, BasicLock* lock, JavaThread* thread);
 
+  static address entry_for_handle_wrong_method(methodHandle callee_method, bool is_static_call, bool is_optimized, bool caller_is_c1) {
+    assert(callee_method->verified_code_entry() != NULL, "Jump to zero!");
+    assert(callee_method->verified_value_code_entry() != NULL, "Jump to zero!");
+    assert(callee_method->verified_value_ro_code_entry() != NULL, "Jump to zero!");
+    if (caller_is_c1) {
+      return callee_method->verified_value_code_entry();
+    } else if (is_static_call || is_optimized) {
+      return callee_method->verified_code_entry();
+    } else {
+      return callee_method->verified_value_ro_code_entry();
+    }
+  }
+
  private:
   static Handle find_callee_info(JavaThread* thread,
                                  Bytecodes::Code& bc,
                                  CallInfo& callinfo, TRAPS);
   static Handle find_callee_info_helper(JavaThread* thread,
@@ -354,14 +370,10 @@
                                         Bytecodes::Code& bc,
                                         CallInfo& callinfo, TRAPS);
 
   static Method* extract_attached_method(vframeStream& vfst);
 
-  static address clean_virtual_call_entry();
-  static address clean_opt_virtual_call_entry();
-  static address clean_static_call_entry();
-
 #if defined(X86) && defined(COMPILER1)
   // For Object.hashCode, System.identityHashCode try to pull hashCode from object header if available.
   static void inline_check_hashcode_from_object_header(MacroAssembler* masm, const methodHandle& method, Register obj_reg, Register result);
 #endif // X86 && COMPILER1
 
@@ -376,10 +388,18 @@
   // 4-bytes higher. So for sparc because the register window save area is at
   // the bottom of the frame the first 16 words will be skipped and SharedInfo::stack0
   // will be just above it. (
   // return value is the maximum number of VMReg stack slots the convention will use.
   static int java_calling_convention(const BasicType* sig_bt, VMRegPair* regs, int total_args_passed, int is_outgoing);
+  static int java_calling_convention(const GrowableArray<SigEntry>* sig, VMRegPair* regs) {
+    BasicType* sig_bt = NEW_RESOURCE_ARRAY(BasicType, sig->length());
+    int total_args_passed = SigEntry::fill_sig_bt(sig, sig_bt);
+    return java_calling_convention(sig_bt, regs, total_args_passed, false);
+  }
+  static int java_return_convention(const BasicType* sig_bt, VMRegPair* regs, int total_args_passed);
+  static const uint java_return_convention_max_int;
+  static const uint java_return_convention_max_float;
 
   static void check_member_name_argument_is_last_argument(const methodHandle& method,
                                                           const BasicType* sig_bt,
                                                           const VMRegPair* regs) NOT_DEBUG_RETURN;
 
@@ -423,20 +443,23 @@
   // that the interpreter before it does any call dispatch will record the current
   // stack pointer in the interpreter frame. On return it will restore the stack
   // pointer as needed. This means the i2c adapter code doesn't need any special
   // handshaking path with compiled code to keep the stack walking correct.
 
-  static AdapterHandlerEntry* generate_i2c2i_adapters(MacroAssembler *_masm,
-                                                      int total_args_passed,
-                                                      int max_arg,
-                                                      const BasicType *sig_bt,
-                                                      const VMRegPair *regs,
-                                                      AdapterFingerPrint* fingerprint);
+  static AdapterHandlerEntry* generate_i2c2i_adapters(MacroAssembler *masm,
+                                                      int comp_args_on_stack,
+                                                      const GrowableArray<SigEntry>* sig,
+                                                      const VMRegPair* regs,
+                                                      const GrowableArray<SigEntry>* sig_cc,
+                                                      const VMRegPair* regs_cc,
+                                                      const GrowableArray<SigEntry>* sig_cc_ro,
+                                                      const VMRegPair* regs_cc_ro,
+                                                      AdapterFingerPrint* fingerprint,
+                                                      AdapterBlob*& new_adapter);
 
   static void gen_i2c_adapter(MacroAssembler *_masm,
-                              int total_args_passed,
-                              int comp_args_on_stack,
+                              int comp_args_on_stack,
                               const BasicType *sig_bt,
                               const VMRegPair *regs);
 
   // OSR support
 
@@ -510,23 +533,30 @@
   // Resolving of calls
   static address resolve_static_call_C     (JavaThread *thread);
   static address resolve_virtual_call_C    (JavaThread *thread);
   static address resolve_opt_virtual_call_C(JavaThread *thread);
 
+  static void load_value_type_fields_in_regs(JavaThread *thread, oopDesc* res);
+  static void store_value_type_fields_to_buf(JavaThread *thread, intptr_t res);
+
   // arraycopy, the non-leaf version.  (See StubRoutines for all the leaf calls.)
   static void slow_arraycopy_C(oopDesc* src,  jint src_pos,
                                oopDesc* dest, jint dest_pos,
                                jint length, JavaThread* thread);
 
   // handle ic miss with caller being compiled code
   // wrong method handling (inline cache misses, zombie methods)
   static address handle_wrong_method(JavaThread* thread);
   static address handle_wrong_method_abstract(JavaThread* thread);
   static address handle_wrong_method_ic_miss(JavaThread* thread);
+  static void allocate_value_types(JavaThread* thread, Method* callee, bool allocate_receiver);
+  static oop allocate_value_types_impl(JavaThread* thread, methodHandle callee, bool allocate_receiver, TRAPS);
+  static void apply_post_barriers(JavaThread* thread, objArrayOopDesc* array);
 
   static address handle_unsafe_access(JavaThread* thread, address next_pc);
 
+  static BufferedValueTypeBlob* generate_buffered_value_type_adapter(const ValueKlass* vk);
 #ifndef PRODUCT
 
   // Collect and print inline cache miss statistics
  private:
   enum { maxICmiss_count = 100 };
@@ -637,26 +667,37 @@
 
  private:
   AdapterFingerPrint* _fingerprint;
   address _i2c_entry;
   address _c2i_entry;
+  address _c2i_value_entry;
+  address _c2i_value_ro_entry;
   address _c2i_unverified_entry;
+  address _c2i_unverified_value_entry;
   address _c2i_no_clinit_check_entry;
 
+  // Support for scalarized value type calling convention
+  const GrowableArray<SigEntry>* _sig_cc;
+
 #ifdef ASSERT
   // Captures code and signature used to generate this adapter when
   // verifying adapter equivalence.
   unsigned char* _saved_code;
   int            _saved_code_length;
 #endif
 
-  void init(AdapterFingerPrint* fingerprint, address i2c_entry, address c2i_entry, address c2i_unverified_entry, address c2i_no_clinit_check_entry) {
+  void init(AdapterFingerPrint* fingerprint, address i2c_entry, address c2i_entry, address c2i_value_entry,
+            address c2i_value_ro_entry, address c2i_unverified_entry, address c2i_unverified_value_entry, address c2i_no_clinit_check_entry) {
     _fingerprint = fingerprint;
     _i2c_entry = i2c_entry;
     _c2i_entry = c2i_entry;
+    _c2i_value_entry = c2i_value_entry;
+    _c2i_value_ro_entry = c2i_value_ro_entry;
     _c2i_unverified_entry = c2i_unverified_entry;
+    _c2i_unverified_value_entry = c2i_unverified_value_entry;
     _c2i_no_clinit_check_entry = c2i_no_clinit_check_entry;
+    _sig_cc = NULL;
 #ifdef ASSERT
     _saved_code = NULL;
     _saved_code_length = 0;
 #endif
   }
@@ -667,16 +708,23 @@
   AdapterHandlerEntry();
 
  public:
   address get_i2c_entry()                  const { return _i2c_entry; }
   address get_c2i_entry()                  const { return _c2i_entry; }
+  address get_c2i_value_entry()            const { return _c2i_value_entry; }
+  address get_c2i_value_ro_entry()         const { return _c2i_value_ro_entry; }
   address get_c2i_unverified_entry()       const { return _c2i_unverified_entry; }
+  address get_c2i_unverified_value_entry() const { return _c2i_unverified_value_entry; }
   address get_c2i_no_clinit_check_entry()  const { return _c2i_no_clinit_check_entry; }
 
   address base_address();
   void relocate(address new_base);
 
+  // Support for scalarized value type calling convention
+  void set_sig_cc(const GrowableArray<SigEntry>* sig)  { _sig_cc = sig; }
+  const GrowableArray<SigEntry>* get_sig_cc()    const { return _sig_cc; }
+
   AdapterFingerPrint* fingerprint() const { return _fingerprint; }
 
   AdapterHandlerEntry* next() {
     return (AdapterHandlerEntry*)BasicHashtableEntry<mtCode>::next();
   }
@@ -693,15 +741,19 @@
 
 // This class is used only with DumpSharedSpaces==true. It holds extra information
 // that's used only during CDS dump time.
 // For details, see comments around Method::link_method()
 class CDSAdapterHandlerEntry: public AdapterHandlerEntry {
-  address               _c2i_entry_trampoline;   // allocated from shared spaces "MC" region
-  AdapterHandlerEntry** _adapter_trampoline;     // allocated from shared spaces "MD" region
+  address               _c2i_entry_trampoline;           // allocated from shared spaces "MC" region
+  address               _c2i_value_ro_entry_trampoline;  // allocated from shared spaces "MC" region
+  address               _c2i_value_entry_trampoline;     // allocated from shared spaces "MC" region
+  AdapterHandlerEntry** _adapter_trampoline;             // allocated from shared spaces "MD" region
 
 public:
   address get_c2i_entry_trampoline()             const { return _c2i_entry_trampoline; }
+  address get_c2i_value_ro_entry_trampoline()    const { return _c2i_value_ro_entry_trampoline; }
+  address get_c2i_value_entry_trampoline()       const { return _c2i_value_entry_trampoline; }
   AdapterHandlerEntry** get_adapter_trampoline() const { return _adapter_trampoline; }
   void init() NOT_CDS_RETURN;
 };
 
 
@@ -715,14 +767,12 @@
   static AdapterHandlerEntry* get_adapter0(const methodHandle& method);
 
  public:
 
   static AdapterHandlerEntry* new_entry(AdapterFingerPrint* fingerprint,
-                                        address i2c_entry,
-                                        address c2i_entry,
-                                        address c2i_unverified_entry,
-                                        address c2i_no_clinit_check_entry = NULL);
+                                        address i2c_entry, address c2i_entry, address c2i_value_entry, address c2i_value_ro_entry,
+                                        address c2i_unverified_entry, address c2i_unverified_value_entry, address c2i_no_clinit_check_entry = NULL);
   static void create_native_wrapper(const methodHandle& method);
   static AdapterHandlerEntry* get_adapter(const methodHandle& method);
 
   static void print_handler(const CodeBlob* b) { print_handler_on(tty, b); }
   static void print_handler_on(outputStream* st, const CodeBlob* b);
@@ -731,6 +781,68 @@
   static void print_statistics();
 #endif // PRODUCT
 
 };
 
+// Utility class for computing the calling convention of the 3 types
+// of compiled method entries:
+//     Method::_from_compiled_entry               - sig_cc
+//     Method::_from_compiled_value_ro_entry      - sig_cc_ro
+//     Method::_from_compiled_value_entry         - sig
+class CompiledEntrySignature : public StackObj {
+  Method* _method;
+  int  _num_value_args;
+  bool _has_value_recv;
+  GrowableArray<SigEntry> *_sig;
+  GrowableArray<SigEntry> *_sig_cc;
+  GrowableArray<SigEntry> *_sig_cc_ro;
+  VMRegPair* _regs;
+  VMRegPair* _regs_cc;
+  VMRegPair* _regs_cc_ro;
+
+  int _args_on_stack;
+  int _args_on_stack_cc;
+  int _args_on_stack_cc_ro;
+
+  bool _c1_needs_stack_repair;
+  bool _c2_needs_stack_repair;
+  bool _has_scalarized_args;
+  bool _has_reserved_entries;
+
+public:
+  Method* method()                     const { return _method; }
+
+  // Used by Method::_from_compiled_value_entry
+  GrowableArray<SigEntry>& sig()       const { return *_sig; }
+
+  // Used by Method::_from_compiled_entry
+  GrowableArray<SigEntry>& sig_cc()    const { return *_sig_cc; }
+
+  // Used by Method::_from_compiled_value_ro_entry
+  GrowableArray<SigEntry>& sig_cc_ro() const { return *_sig_cc_ro; }
+
+  VMRegPair* regs()                    const { return _regs; }
+  VMRegPair* regs_cc()                 const { return _regs_cc; }
+  VMRegPair* regs_cc_ro()              const { return _regs_cc_ro; }
+
+  int args_on_stack()                  const { return _args_on_stack; }
+  int args_on_stack_cc()               const { return _args_on_stack_cc; }
+  int args_on_stack_cc_ro()            const { return _args_on_stack_cc_ro; }
+
+  int  num_value_args()                const { return _num_value_args; }
+  bool has_value_arg()                 const { return _num_value_args > 0;  }
+  bool has_value_recv()                const { return _has_value_recv; }
+
+  bool has_scalarized_args()           const { return _has_scalarized_args; }
+  bool c1_needs_stack_repair()         const { return _c1_needs_stack_repair; }
+  bool c2_needs_stack_repair()         const { return _c2_needs_stack_repair; }
+  CodeOffsets::Entries c1_value_ro_entry_type() const;
+
+  CompiledEntrySignature(Method* method);
+  void compute_calling_conventions();
+
+private:
+  int compute_scalarized_cc(GrowableArray<SigEntry>*& sig_cc, VMRegPair*& regs_cc, bool scalar_receiver);
+  int insert_reserved_entry(int ret_off);
+};
+
 #endif // SHARE_RUNTIME_SHAREDRUNTIME_HPP
diff a/src/hotspot/share/runtime/thread.cpp b/src/hotspot/share/runtime/thread.cpp
--- a/src/hotspot/share/runtime/thread.cpp
+++ b/src/hotspot/share/runtime/thread.cpp
@@ -56,10 +56,11 @@
 #include "oops/instanceKlass.hpp"
 #include "oops/objArrayOop.hpp"
 #include "oops/oop.inline.hpp"
 #include "oops/symbol.hpp"
 #include "oops/typeArrayOop.inline.hpp"
+#include "oops/valueKlass.hpp"
 #include "oops/verifyOopClosure.hpp"
 #include "prims/jvm_misc.hpp"
 #include "prims/jvmtiExport.hpp"
 #include "prims/jvmtiThreadState.hpp"
 #include "runtime/arguments.hpp"
@@ -1641,10 +1642,11 @@
   set_entry_point(NULL);
   set_jni_functions(jni_functions());
   set_callee_target(NULL);
   set_vm_result(NULL);
   set_vm_result_2(NULL);
+  set_return_buffered_value(NULL);
   set_vframe_array_head(NULL);
   set_vframe_array_last(NULL);
   set_deferred_locals(NULL);
   set_deopt_mark(NULL);
   set_deopt_compiled_method(NULL);
@@ -2832,10 +2834,13 @@
 }
 
 void JavaThread::frames_do(void f(frame*, const RegisterMap* map)) {
   // ignore is there is no stack
   if (!has_last_Java_frame()) return;
+  // Because this method is used to verify oops, it must support
+  // oops in buffered values
+
   // traverse the stack frames. Starts from top frame.
   for (StackFrameStream fst(this); !fst.is_done(); fst.next()) {
     frame* fr = fst.current();
     f(fr, fst.register_map());
   }
diff a/src/hotspot/share/runtime/vmOperations.hpp b/src/hotspot/share/runtime/vmOperations.hpp
--- a/src/hotspot/share/runtime/vmOperations.hpp
+++ b/src/hotspot/share/runtime/vmOperations.hpp
@@ -119,10 +119,11 @@
   template(ICBufferFull)                          \
   template(ScavengeMonitors)                      \
   template(PrintMetadata)                         \
   template(GTestExecuteAtSafepoint)               \
   template(JFROldObject)                          \
+  template(ClassPrintLayout)                      \
 
 class VM_Operation : public StackObj {
  public:
   enum VMOp_Type {
     VM_OPS_DO(VM_OP_ENUM)
@@ -431,10 +432,20 @@
   VM_PrintCompileQueue(outputStream* st) : _out(st) {}
   VMOp_Type type() const { return VMOp_PrintCompileQueue; }
   void doit();
 };
 
+class VM_PrintClassLayout: public VM_Operation {
+ private:
+  outputStream* _out;
+  char* _class_name;
+ public:
+  VM_PrintClassLayout(outputStream* st, char* class_name): _out(st), _class_name(class_name) {}
+  VMOp_Type type() const { return VMOp_PrintClassHierarchy; }
+  void doit();
+};
+
 #if INCLUDE_SERVICES
 class VM_PrintClassHierarchy: public VM_Operation {
  private:
   outputStream* _out;
   bool _print_interfaces;
diff a/src/hotspot/share/runtime/vmStructs.cpp b/src/hotspot/share/runtime/vmStructs.cpp
--- a/src/hotspot/share/runtime/vmStructs.cpp
+++ b/src/hotspot/share/runtime/vmStructs.cpp
@@ -1593,13 +1593,15 @@
   declare_c2_type(ConvF2INode, Node)                                      \
   declare_c2_type(ConvF2LNode, Node)                                      \
   declare_c2_type(ConvI2DNode, Node)                                      \
   declare_c2_type(ConvI2FNode, Node)                                      \
   declare_c2_type(ConvI2LNode, TypeNode)                                  \
+  declare_c2_type(CastI2NNode, TypeNode)                                  \
   declare_c2_type(ConvL2DNode, Node)                                      \
   declare_c2_type(ConvL2FNode, Node)                                      \
   declare_c2_type(ConvL2INode, Node)                                      \
+  declare_c2_type(CastN2INode, Node)                                      \
   declare_c2_type(CastX2PNode, Node)                                      \
   declare_c2_type(CastP2XNode, Node)                                      \
   declare_c2_type(SetVectMaskINode, Node)                                 \
   declare_c2_type(MemBarNode, MultiNode)                                  \
   declare_c2_type(MemBarAcquireNode, MemBarNode)                          \
@@ -1636,10 +1638,11 @@
   declare_c2_type(MachNode, Node)                                         \
   declare_c2_type(MachIdealNode, MachNode)                                \
   declare_c2_type(MachTypeNode, MachNode)                                 \
   declare_c2_type(MachBreakpointNode, MachIdealNode)                      \
   declare_c2_type(MachUEPNode, MachIdealNode)                             \
+  declare_c2_type(MachVEPNode, MachIdealNode)                             \
   declare_c2_type(MachPrologNode, MachIdealNode)                          \
   declare_c2_type(MachEpilogNode, MachIdealNode)                          \
   declare_c2_type(MachNopNode, MachIdealNode)                             \
   declare_c2_type(MachSpillCopyNode, MachIdealNode)                       \
   declare_c2_type(MachNullCheckNode, MachIdealNode)                       \
@@ -2295,10 +2298,12 @@
   declare_constant(InstanceKlass::_misc_has_passed_fingerprint_check)     \
   declare_constant(InstanceKlass::_misc_is_scratch_class)                 \
   declare_constant(InstanceKlass::_misc_is_shared_boot_class)             \
   declare_constant(InstanceKlass::_misc_is_shared_platform_class)         \
   declare_constant(InstanceKlass::_misc_is_shared_app_class)              \
+  declare_constant(InstanceKlass::_misc_invalid_inline_super)             \
+  declare_constant(InstanceKlass::_misc_invalid_identity_super)           \
                                                                           \
   /*********************************/                                     \
   /* Symbol* - symbol max length */                                       \
   /*********************************/                                     \
                                                                           \
diff a/src/hotspot/share/utilities/accessFlags.hpp b/src/hotspot/share/utilities/accessFlags.hpp
--- a/src/hotspot/share/utilities/accessFlags.hpp
+++ b/src/hotspot/share/utilities/accessFlags.hpp
@@ -82,18 +82,23 @@
   // Note that the class-related ACC_ANNOTATION bit conflicts with these flags.
   JVM_ACC_FIELD_ACCESS_WATCHED            = 0x00002000, // field access is watched by JVMTI
   JVM_ACC_FIELD_MODIFICATION_WATCHED      = 0x00008000, // field modification is watched by JVMTI
   JVM_ACC_FIELD_INTERNAL                  = 0x00000400, // internal field, same as JVM_ACC_ABSTRACT
   JVM_ACC_FIELD_STABLE                    = 0x00000020, // @Stable field, same as JVM_ACC_SYNCHRONIZED and JVM_ACC_SUPER
-  JVM_ACC_FIELD_INITIALIZED_FINAL_UPDATE  = 0x00000100, // (static) final field updated outside (class) initializer, same as JVM_ACC_NATIVE
+  JVM_ACC_FIELD_INITIALIZED_FINAL_UPDATE  = 0x00000200, // (static) final field updated outside (class) initializer, same as JVM_ACC_NATIVE
   JVM_ACC_FIELD_HAS_GENERIC_SIGNATURE     = 0x00000800, // field has generic signature
+  JVM_ACC_FIELD_FLATTENABLE               = 0x00004000, // flattenable field
+  JVM_ACC_FIELD_FLATTENED                 = 0x00008000, // flattened field
 
   JVM_ACC_FIELD_INTERNAL_FLAGS       = JVM_ACC_FIELD_ACCESS_WATCHED |
                                        JVM_ACC_FIELD_MODIFICATION_WATCHED |
                                        JVM_ACC_FIELD_INTERNAL |
                                        JVM_ACC_FIELD_STABLE |
-                                       JVM_ACC_FIELD_HAS_GENERIC_SIGNATURE,
+                                       JVM_ACC_FIELD_HAS_GENERIC_SIGNATURE |
+                                       JVM_ACC_FLATTENABLE |
+                                       JVM_ACC_FIELD_FLATTENABLE |
+                                       JVM_ACC_FIELD_FLATTENED,
 
                                                     // flags accepted by set_field_flags()
   JVM_ACC_FIELD_FLAGS                = JVM_RECOGNIZED_FIELD_MODIFIERS | JVM_ACC_FIELD_INTERNAL_FLAGS
 
 };
@@ -120,10 +125,12 @@
   bool is_transient   () const         { return (_flags & JVM_ACC_TRANSIENT   ) != 0; }
   bool is_native      () const         { return (_flags & JVM_ACC_NATIVE      ) != 0; }
   bool is_interface   () const         { return (_flags & JVM_ACC_INTERFACE   ) != 0; }
   bool is_abstract    () const         { return (_flags & JVM_ACC_ABSTRACT    ) != 0; }
   bool is_strict      () const         { return (_flags & JVM_ACC_STRICT      ) != 0; }
+  bool is_inline_type () const         { return (_flags & JVM_ACC_VALUE       ) != 0; }
+  bool is_flattenable () const         { return (_flags & JVM_ACC_FLATTENABLE ) != 0; }
 
   // Attribute flags
   bool is_synthetic   () const         { return (_flags & JVM_ACC_SYNTHETIC   ) != 0; }
 
   // Method* flags
@@ -210,10 +217,11 @@
   void set_has_jsrs()                  { atomic_set_bits(JVM_ACC_HAS_JSRS);                }
   void set_is_old()                    { atomic_set_bits(JVM_ACC_IS_OLD);                  }
   void set_is_obsolete()               { atomic_set_bits(JVM_ACC_IS_OBSOLETE);             }
   void set_is_deleted()                { atomic_set_bits(JVM_ACC_IS_DELETED);              }
   void set_is_prefixed_native()        { atomic_set_bits(JVM_ACC_IS_PREFIXED_NATIVE);      }
+  void set_is_flattenable()            { atomic_set_bits(JVM_ACC_FLATTENABLE);             }
 
   void clear_not_c1_compilable()       { atomic_clear_bits(JVM_ACC_NOT_C1_COMPILABLE);       }
   void clear_not_c2_compilable()       { atomic_clear_bits(JVM_ACC_NOT_C2_COMPILABLE);       }
   void clear_not_c2_osr_compilable()   { atomic_clear_bits(JVM_ACC_NOT_C2_OSR_COMPILABLE);   }
   // Klass* flags
diff a/src/hotspot/share/utilities/exceptions.hpp b/src/hotspot/share/utilities/exceptions.hpp
--- a/src/hotspot/share/utilities/exceptions.hpp
+++ b/src/hotspot/share/utilities/exceptions.hpp
@@ -236,10 +236,12 @@
 // The THROW... macros should be used to throw an exception. They require a THREAD variable to be
 // visible within the scope containing the THROW. Usually this is achieved by declaring the function
 // with a TRAPS argument.
 
 #define THREAD_AND_LOCATION                      THREAD, __FILE__, __LINE__
+#define THREAD_AND_LOCATION_DECL                 TRAPS, const char* file, int line
+#define THREAD_AND_LOCATION_ARGS                 THREAD, file, line
 
 #define THROW_OOP(e)                                \
   { Exceptions::_throw_oop(THREAD_AND_LOCATION, e);                             return;  }
 
 #define THROW_HANDLE(e)                                \
diff a/src/java.base/share/classes/java/lang/Class.java b/src/java.base/share/classes/java/lang/Class.java
--- a/src/java.base/share/classes/java/lang/Class.java
+++ b/src/java.base/share/classes/java/lang/Class.java
@@ -195,13 +195,14 @@
                               GenericDeclaration,
                               Type,
                               AnnotatedElement,
                               TypeDescriptor.OfField<Class<?>>,
                               Constable {
-    private static final int ANNOTATION= 0x00002000;
-    private static final int ENUM      = 0x00004000;
-    private static final int SYNTHETIC = 0x00001000;
+    private static final int ANNOTATION = 0x00002000;
+    private static final int ENUM       = 0x00004000;
+    private static final int SYNTHETIC  = 0x00001000;
+    private static final int INLINE     = 0x00000100;
 
     private static native void registerNatives();
     static {
         registerNatives();
     }
@@ -229,12 +230,13 @@
      * this method returns "class " followed by {@code getName}.
      *
      * @return a string representation of this {@code Class} object.
      */
     public String toString() {
-        return (isInterface() ? "interface " : (isPrimitive() ? "" : "class "))
-            + getName();
+        return (isInlineClass() ? "inline " : "")
+               + (isInterface() ? "interface " : (isPrimitive() ? "" : "class "))
+               + getName();
     }
 
     /**
      * Returns a string describing this {@code Class}, including
      * information about modifiers and type parameters.
@@ -292,10 +294,14 @@
                 }
 
                 if (isAnnotation()) {
                     sb.append('@');
                 }
+                if (isInlineClass()) {
+                    sb.append("inline");
+                    sb.append(' ');
+                }
                 if (isInterface()) { // Note: all annotation types are interfaces
                     sb.append("interface");
                 } else {
                     if (isEnum())
                         sb.append("enum");
@@ -466,12 +472,12 @@
         return forName0(name, initialize, loader, caller);
     }
 
     /** Called after security check for system loader access checks have been made. */
     private static native Class<?> forName0(String name, boolean initialize,
-                                            ClassLoader loader,
-                                            Class<?> caller)
+                                    ClassLoader loader,
+                                    Class<?> caller)
         throws ClassNotFoundException;
 
 
     /**
      * Returns the {@code Class} with the given <a href="ClassLoader.html#binary-name">
@@ -545,10 +551,57 @@
         } else {
             return BootLoader.loadClass(module, name);
         }
     }
 
+    /**
+     * Returns {@code true} if this class is an inline class.
+     *
+     * @return {@code true} if this class is an inline class
+     * @since Valhalla
+     */
+    public boolean isInlineClass() {
+        return (this.getModifiers() & INLINE) != 0;
+    }
+
+    /**
+     * Returns a {@code Class} object representing the <em>value projection</em>
+     * type of this class if this {@code Class} is the reference projection type
+     * of an {@linkplain #isInlineClass() inline class}.  Otherwise an empty
+     * {@link Optional} is returned.
+     *
+     * @return the {@code Class} object representing the value projection type of
+     *         this class if this class is the reference projection type of an
+     *         inline class; an empty {@link Optional} otherwise
+     * @since Valhalla
+     */
+    public Optional<Class<T>> valueType() {
+        return Optional.ofNullable(valType);
+    }
+
+    /**
+     * Returns a {@code Class} object representing the <em>reference projection</em>
+     * type of this class if this class is an {@linkplain #isInlineClass() inline class}
+     * with a reference projection.
+     * If this class is an {@linkplain #isInlineClass() inline class}
+     * without a reference projection or this class is not an inline class,
+     * then this method returns an empty {@link Optional}.
+     *
+     * @return the {@code Class} object representing the value projection type of
+     *         this class if this class is the reference projection type of an
+     *         inline class; an empty {@link Optional} otherwise
+     * @since Valhalla
+     */
+    public Optional<Class<T>> referenceType() {
+        return valType != null ? Optional.ofNullable(refType) : Optional.of(this);
+    }
+
+    // set by VM if this class is an inline type
+    // otherwise, these two fields are null
+    private transient Class<T> valType;
+    private transient Class<T> refType;
+
     /**
      * Creates a new instance of the class represented by this {@code Class}
      * object.  The class is instantiated as if by a {@code new}
      * expression with an empty argument list.  The class is initialized if it
      * has not already been initialized.
@@ -824,10 +877,12 @@
      * <tr><th scope="row"> {@code boolean} <td style="text-align:center"> {@code Z}
      * <tr><th scope="row"> {@code byte}    <td style="text-align:center"> {@code B}
      * <tr><th scope="row"> {@code char}    <td style="text-align:center"> {@code C}
      * <tr><th scope="row"> class or interface with <a href="ClassLoader.html#binary-name">binary name</a> <i>N</i>
      *                                      <td style="text-align:center"> {@code L}<em>N</em>{@code ;}
+     * <tr><th scope="row"> {@linkplain #isInlineClass() inline class} with <a href="ClassLoader.html#binary-name">binary name</a> <i>N</i>
+     *                                      <td style="text-align:center"> {@code Q}<em>N</em>{@code ;}
      * <tr><th scope="row"> {@code double}  <td style="text-align:center"> {@code D}
      * <tr><th scope="row"> {@code float}   <td style="text-align:center"> {@code F}
      * <tr><th scope="row"> {@code int}     <td style="text-align:center"> {@code I}
      * <tr><th scope="row"> {@code long}    <td style="text-align:center"> {@code J}
      * <tr><th scope="row"> {@code short}   <td style="text-align:center"> {@code S}
@@ -842,12 +897,18 @@
      * <blockquote><pre>
      * String.class.getName()
      *     returns "java.lang.String"
      * byte.class.getName()
      *     returns "byte"
+     * Point.class.getName()
+     *     returns "Point"
      * (new Object[3]).getClass().getName()
      *     returns "[Ljava.lang.Object;"
+     * (new Point[3]).getClass().getName()
+     *     returns "[QPoint;"
+     * (new Point.ref[3][4]).getClass().getName()
+     *     returns "[[LPoint$ref;"
      * (new int[3][4][5][6][7][8][9]).getClass().getName()
      *     returns "[[[[[[[I"
      * </pre></blockquote>
      *
      * @return  the name of the class, interface, or other entity
@@ -1274,22 +1335,20 @@
      * @since 1.1
      */
     @HotSpotIntrinsicCandidate
     public native int getModifiers();
 
-
     /**
      * Gets the signers of this class.
      *
      * @return  the signers of this class, or null if there are no signers.  In
      *          particular, this method returns null if this {@code Class} object represents
      *          a primitive type or void.
      * @since   1.1
      */
     public native Object[] getSigners();
 
-
     /**
      * Set the signers of this class.
      */
     native void setSigners(Object[] signers);
 
@@ -1666,11 +1725,11 @@
                 int dimensions = 0;
                 do {
                     dimensions++;
                     cl = cl.getComponentType();
                 } while (cl.isArray());
-                return cl.getName() + "[]".repeat(dimensions);
+                return cl.getTypeName() + "[]".repeat(dimensions);
             } catch (Throwable e) { /*FALLTHRU*/ }
         }
         return getName();
     }
 
@@ -3801,17 +3860,22 @@
      *
      * @param obj the object to be cast
      * @return the object after casting, or null if obj is null
      *
      * @throws ClassCastException if the object is not
-     * null and is not assignable to the type T.
+     * {@code null} and is not assignable to the type T.
+     * @throws NullPointerException if this is an {@linkplain #isInlineClass()
+     * inline type} and the object is {@code null}
      *
      * @since 1.5
      */
     @SuppressWarnings("unchecked")
     @HotSpotIntrinsicCandidate
     public T cast(Object obj) {
+        if (isInlineClass() && obj == null)
+            throw new NullPointerException(getName() + " is an inline class");
+
         if (obj != null && !isInstance(obj))
             throw new ClassCastException(cannotCastMsg(obj));
         return (T) obj;
     }
 
@@ -4103,11 +4167,11 @@
      *
      * @return an array representing the superinterfaces
      * @since 1.8
      */
     public AnnotatedType[] getAnnotatedInterfaces() {
-         return TypeAnnotationParser.buildAnnotatedInterfaces(getRawTypeAnnotations(), getConstantPool(), this);
+        return TypeAnnotationParser.buildAnnotatedInterfaces(getRawTypeAnnotations(), getConstantPool(), this);
     }
 
     private native Class<?> getNestHost0();
 
     /**
@@ -4316,17 +4380,19 @@
         if (isPrimitive())
             return Wrapper.forPrimitiveType(this).basicTypeString();
 
         if (isArray()) {
             return "[" + componentType.descriptorString();
-        } else if (isHidden()) {
+        }
+        String typeDesc = isInlineClass() ? "Q" : "L";
+        if (isHidden()) {
             String name = getName();
             int index = name.indexOf('/');
-            return "L" + name.substring(0, index).replace('.', '/')
+            return typeDesc + name.substring(0, index).replace('.', '/')
                        + "." + name.substring(index+1) + ";";
         } else {
-            return "L" + getName().replace('.', '/') + ";";
+            return typeDesc + getName().replace('.', '/') + ";";
         }
     }
 
     /**
      * Returns the component type of this {@code Class}, if it describes
diff a/src/java.base/share/classes/java/lang/System.java b/src/java.base/share/classes/java/lang/System.java
--- a/src/java.base/share/classes/java/lang/System.java
+++ b/src/java.base/share/classes/java/lang/System.java
@@ -2257,11 +2257,10 @@
             }
 
             public byte[] getBytesUTF8NoRepl(String s) {
                 return StringCoding.getBytesUTF8NoRepl(s);
             }
-
             public void setCause(Throwable t, Throwable cause) {
                 t.setCause(cause);
             }
 
             public ProtectionDomain protectionDomain(Class<?> c) {
diff a/src/java.base/share/classes/java/lang/invoke/GenerateJLIClassesHelper.java b/src/java.base/share/classes/java/lang/invoke/GenerateJLIClassesHelper.java
--- a/src/java.base/share/classes/java/lang/invoke/GenerateJLIClassesHelper.java
+++ b/src/java.base/share/classes/java/lang/invoke/GenerateJLIClassesHelper.java
@@ -92,20 +92,20 @@
         for (Wrapper wrapper : Wrapper.values()) {
             if (wrapper == Wrapper.VOID) {
                 continue;
             }
             for (byte b = DirectMethodHandle.AF_GETFIELD; b < DirectMethodHandle.AF_LIMIT; b++) {
-                int ftype = DirectMethodHandle.ftypeKind(wrapper.primitiveType());
+                int ftype = DirectMethodHandle.ftypeKind(wrapper.primitiveType(), true);
                 LambdaForm form = DirectMethodHandle
-                        .makePreparedFieldLambdaForm(b, /*isVolatile*/false, ftype);
+                        .makePreparedFieldLambdaForm(b, false /*isVolatile*/, ftype);
                 if (form.kind != LambdaForm.Kind.GENERIC) {
                     forms.add(form);
                     names.add(form.kind.defaultLambdaName);
                 }
                 // volatile
                 form = DirectMethodHandle
-                        .makePreparedFieldLambdaForm(b, /*isVolatile*/true, ftype);
+                        .makePreparedFieldLambdaForm(b, true /*isVolatile*/, ftype);
                 if (form.kind != LambdaForm.Kind.GENERIC) {
                     forms.add(form);
                     names.add(form.kind.defaultLambdaName);
                 }
             }
diff a/src/java.base/share/classes/java/lang/invoke/InnerClassLambdaMetafactory.java b/src/java.base/share/classes/java/lang/invoke/InnerClassLambdaMetafactory.java
--- a/src/java.base/share/classes/java/lang/invoke/InnerClassLambdaMetafactory.java
+++ b/src/java.base/share/classes/java/lang/invoke/InnerClassLambdaMetafactory.java
@@ -23,10 +23,12 @@
  * questions.
  */
 
 package java.lang.invoke;
 
+import jdk.internal.access.JavaLangAccess;
+import jdk.internal.access.SharedSecrets;
 import jdk.internal.org.objectweb.asm.*;
 import sun.invoke.util.BytecodeDescriptor;
 import sun.security.action.GetPropertyAction;
 import sun.security.action.GetBooleanAction;
 
diff a/src/java.base/share/classes/java/lang/invoke/InvokerBytecodeGenerator.java b/src/java.base/share/classes/java/lang/invoke/InvokerBytecodeGenerator.java
--- a/src/java.base/share/classes/java/lang/invoke/InvokerBytecodeGenerator.java
+++ b/src/java.base/share/classes/java/lang/invoke/InvokerBytecodeGenerator.java
@@ -1009,11 +1009,12 @@
         return isStaticallyInvocable(name.function.member());
     }
 
     static boolean isStaticallyInvocable(MemberName member) {
         if (member == null)  return false;
-        if (member.isConstructor())  return false;
+        if (member.isObjectConstructorOrStaticInitMethod())  return false;
+
         Class<?> cls = member.getDeclaringClass();
         // Fast-path non-private members declared by MethodHandles, which is a common
         // case
         if (MethodHandle.class.isAssignableFrom(cls) && !member.isPrivate()) {
             assert(isStaticallyInvocableType(member.getMethodOrFieldType()));
diff a/src/java.base/share/classes/java/lang/invoke/MethodHandleNatives.java b/src/java.base/share/classes/java/lang/invoke/MethodHandleNatives.java
--- a/src/java.base/share/classes/java/lang/invoke/MethodHandleNatives.java
+++ b/src/java.base/share/classes/java/lang/invoke/MethodHandleNatives.java
@@ -111,20 +111,20 @@
      */
     static class Constants {
         Constants() { } // static only
 
         static final int
-            MN_IS_METHOD           = 0x00010000, // method (not constructor)
-            MN_IS_CONSTRUCTOR      = 0x00020000, // constructor
-            MN_IS_FIELD            = 0x00040000, // field
-            MN_IS_TYPE             = 0x00080000, // nested type
-            MN_CALLER_SENSITIVE    = 0x00100000, // @CallerSensitive annotation detected
-            MN_REFERENCE_KIND_SHIFT = 24, // refKind
-            MN_REFERENCE_KIND_MASK = 0x0F000000 >> MN_REFERENCE_KIND_SHIFT,
+            MN_IS_METHOD             = 0x00010000, // method (not object constructor)
+            MN_IS_OBJECT_CONSTRUCTOR = 0x00020000, // object constructor
+            MN_IS_FIELD              = 0x00040000, // field
+            MN_IS_TYPE               = 0x00080000, // nested type
+            MN_CALLER_SENSITIVE      = 0x00100000, // @CallerSensitive annotation detected
+            MN_REFERENCE_KIND_SHIFT  = 24, // refKind
+            MN_REFERENCE_KIND_MASK   = 0x0F000000 >> MN_REFERENCE_KIND_SHIFT,
             // The SEARCH_* bits are not for MN.flags but for the matchFlags argument of MHN.getMembers:
-            MN_SEARCH_SUPERCLASSES = 0x00100000,
-            MN_SEARCH_INTERFACES   = 0x00200000;
+            MN_SEARCH_SUPERCLASSES   = 0x00100000,
+            MN_SEARCH_INTERFACES     = 0x00200000;
 
         /**
          * Constant pool reference-kind codes, as used by CONSTANT_MethodHandle CP entries.
          */
         static final byte
@@ -165,11 +165,11 @@
         return refKindIsField(refKind) && !refKindIsGetter(refKind);
     }
     static boolean refKindIsMethod(byte refKind) {
         return !refKindIsField(refKind) && (refKind != REF_newInvokeSpecial);
     }
-    static boolean refKindIsConstructor(byte refKind) {
+    static boolean refKindIsObjectConstructor(byte refKind) {
         return (refKind == REF_newInvokeSpecial);
     }
     static boolean refKindHasReceiver(byte refKind) {
         assert(refKindIsValid(refKind));
         return (refKind & 1) != 0;
@@ -574,16 +574,16 @@
         StringBuilder sb = new StringBuilder(prefix.length() + guardType.parameterCount());
 
         sb.append(prefix);
         for (int i = 1; i < guardType.parameterCount() - 1; i++) {
             Class<?> pt = guardType.parameterType(i);
-            sb.append(getCharType(pt));
+            sb.append(getCharErasedType(pt));
         }
-        sb.append('_').append(getCharType(guardType.returnType()));
+        sb.append('_').append(getCharErasedType(guardType.returnType()));
         return sb.toString();
     }
-    static char getCharType(Class<?> pt) {
+    static char getCharErasedType(Class<?> pt) {
         return Wrapper.forBasicType(pt).basicTypeChar();
     }
     static NoSuchMethodError newNoSuchMethodErrorOnVarHandle(String name, MethodType mtype) {
         return new NoSuchMethodError("VarHandle." + name + mtype);
     }
diff a/src/java.base/share/classes/java/lang/invoke/MethodHandles.java b/src/java.base/share/classes/java/lang/invoke/MethodHandles.java
--- a/src/java.base/share/classes/java/lang/invoke/MethodHandles.java
+++ b/src/java.base/share/classes/java/lang/invoke/MethodHandles.java
@@ -2329,10 +2329,16 @@
          *                              <a href="MethodHandles.Lookup.html#secmgr">refuses access</a>
          * @throws NullPointerException if any argument is null
          */
         public MethodHandle findStatic(Class<?> refc, String name, MethodType type) throws NoSuchMethodException, IllegalAccessException {
             MemberName method = resolveOrFail(REF_invokeStatic, refc, name, type);
+            // resolveOrFail could return a non-static <init> method if present
+            // detect and throw NSME before producing a MethodHandle
+            if (!method.isStatic() && name.equals("<init>")) {
+                throw new NoSuchMethodException("illegal method name: " + name);
+            }
+
             return getDirectMethod(REF_invokeStatic, refc, method, findBoundCallerLookup(method));
         }
 
         /**
          * Produces a method handle for a virtual method.
@@ -2474,10 +2480,17 @@
   ProcessBuilder.class, methodType(void.class, String[].class));
 ProcessBuilder pb = (ProcessBuilder)
   MH_newProcessBuilder.invoke("x", "y", "z");
 assertEquals("[x, y, z]", pb.command().toString());
          * }</pre></blockquote>
+         *
+         * @apiNote
+         * This method does not find a static {@code <init>} factory method as it is invoked
+         * via {@code invokestatic} bytecode as opposed to {@code invokespecial} for an
+         * object constructor.  To look up static {@code <init>} factory method, use
+         * the {@link #findStatic(Class, String, MethodType) findStatic} method.
+         *
          * @param refc the class or interface from which the method is accessed
          * @param type the type of the method, with the receiver argument omitted, and a void return type
          * @return the desired method handle
          * @throws NoSuchMethodException if the constructor does not exist
          * @throws IllegalAccessException if access checking fails
@@ -2489,10 +2502,13 @@
          */
         public MethodHandle findConstructor(Class<?> refc, MethodType type) throws NoSuchMethodException, IllegalAccessException {
             if (refc.isArray()) {
                 throw new NoSuchMethodException("no constructor for array class: " + refc.getName());
             }
+            if (type.returnType() != void.class) {
+                throw new NoSuchMethodException("Constructors must have void return type: " + refc.getName());
+            }
             String name = "<init>";
             MemberName ctor = resolveOrFail(REF_newInvokeSpecial, refc, name, type);
             return getDirectConstructor(refc, ctor);
         }
 
@@ -3106,14 +3122,22 @@
          *                                is set and {@code asVarargsCollector} fails
          * @throws NullPointerException if the argument is null
          */
         public MethodHandle unreflectConstructor(Constructor<?> c) throws IllegalAccessException {
             MemberName ctor = new MemberName(c);
-            assert(ctor.isConstructor());
+            assert(ctor.isObjectConstructorOrStaticInitMethod());
             @SuppressWarnings("deprecation")
             Lookup lookup = c.isAccessible() ? IMPL_LOOKUP : this;
-            return lookup.getDirectConstructorNoSecurityManager(ctor.getDeclaringClass(), ctor);
+            if (ctor.isObjectConstructor()) {
+                assert(ctor.getReturnType() == void.class);
+                return lookup.getDirectConstructorNoSecurityManager(ctor.getDeclaringClass(), ctor);
+            } else {
+                // static init factory is a static method
+                assert(ctor.isMethod() && ctor.getReturnType() == ctor.getDeclaringClass() && ctor.getReferenceKind() == REF_invokeStatic);
+                assert(!MethodHandleNatives.isCallerSensitive(ctor));  // must not be caller-sensitive
+                return lookup.getDirectMethodNoSecurityManager(ctor.getReferenceKind(), ctor.getDeclaringClass(), ctor, lookup);
+            }
         }
 
         /**
          * Produces a method handle giving read access to a reflected field.
          * The type of the method handle will have a return type of the field's
@@ -3363,15 +3387,17 @@
             return caller == null || VerifyAccess.isClassAccessible(refc, caller, prevLookupClass, allowedModes);
         }
 
         /** Check name for an illegal leading "&lt;" character. */
         void checkMethodName(byte refKind, String name) throws NoSuchMethodException {
-            if (name.startsWith("<") && refKind != REF_newInvokeSpecial)
-                throw new NoSuchMethodException("illegal method name: "+name);
+            // "<init>" can only be invoked via invokespecial or it's a static init factory
+            if (name.startsWith("<") && refKind != REF_newInvokeSpecial &&
+                    !(refKind == REF_invokeStatic && name.equals("<init>"))) {
+                    throw new NoSuchMethodException("illegal method name: " + name);
+            }
         }
 
-
         /**
          * Find my trustable caller class if m is a caller sensitive method.
          * If this lookup object has full privilege access, then the caller class is the lookupClass.
          * Otherwise, if m is caller-sensitive, throw IllegalAccessException.
          */
@@ -3452,11 +3478,11 @@
         }
 
         void checkMethod(byte refKind, Class<?> refc, MemberName m) throws IllegalAccessException {
             boolean wantStatic = (refKind == REF_invokeStatic);
             String message;
-            if (m.isConstructor())
+            if (m.isObjectConstructor())
                 message = "expected a method, not a constructor";
             else if (!m.isMethod())
                 message = "expected a method";
             else if (wantStatic != m.isStatic())
                 message = wantStatic ? "expected a static method" : "expected a non-static method";
@@ -3751,11 +3777,11 @@
             return getDirectConstructorCommon(refc, ctor, checkSecurity);
         }
         /** Common code for all constructors; do not call directly except from immediately above. */
         private MethodHandle getDirectConstructorCommon(Class<?> refc, MemberName ctor,
                                                   boolean checkSecurity) throws IllegalAccessException {
-            assert(ctor.isConstructor());
+            assert(ctor.isObjectConstructor());
             checkAccess(REF_newInvokeSpecial, refc, ctor);
             // Optionally check with the security manager; this isn't needed for unreflect* calls.
             if (checkSecurity)
                 checkSecurityManager(refc, ctor);
             assert(!MethodHandleNatives.isCallerSensitive(ctor));  // maybeBindCaller not relevant here
@@ -3941,10 +3967,13 @@
      * @throws NullPointerException if the argument is null
      * @throws IllegalArgumentException if arrayClass is not an array type
      * @jvms 6.5 {@code aastore} Instruction
      */
     public static MethodHandle arrayElementSetter(Class<?> arrayClass) throws IllegalArgumentException {
+        if (arrayClass.isInlineClass()) {
+            throw new UnsupportedOperationException();
+        }
         return MethodHandleImpl.makeArrayElementAccessor(arrayClass, MethodHandleImpl.ArrayAccess.SET);
     }
 
     /**
      * Produces a VarHandle giving access to elements of an array of type
@@ -4701,11 +4730,17 @@
      * @see MethodHandles#explicitCastArguments
      * @since 9
      */
     public static MethodHandle zero(Class<?> type) {
         Objects.requireNonNull(type);
-        return type.isPrimitive() ?  zero(Wrapper.forPrimitiveType(type), type) : zero(Wrapper.OBJECT, type);
+        if (type.isPrimitive()) {
+            return zero(Wrapper.forPrimitiveType(type), type);
+        } else if (type.isInlineClass()) {
+            throw new UnsupportedOperationException();
+        } else {
+            return zero(Wrapper.OBJECT, type);
+        }
     }
 
     private static MethodHandle identityOrVoid(Class<?> type) {
         return type == void.class ? zero(type) : identity(type);
     }
@@ -4731,11 +4766,11 @@
         return dropArguments(zero(type.returnType()), 0, type.parameterList());
     }
 
     private static final MethodHandle[] IDENTITY_MHS = new MethodHandle[Wrapper.COUNT];
     private static MethodHandle makeIdentity(Class<?> ptype) {
-        MethodType mtype = methodType(ptype, ptype);
+        MethodType mtype = MethodType.methodType(ptype, ptype);
         LambdaForm lform = LambdaForm.identityForm(BasicType.basicType(ptype));
         return MethodHandleImpl.makeIntrinsic(mtype, lform, Intrinsic.IDENTITY);
     }
 
     private static MethodHandle zero(Wrapper btw, Class<?> rtype) {
diff a/src/java.base/share/classes/java/lang/reflect/AccessibleObject.java b/src/java.base/share/classes/java/lang/reflect/AccessibleObject.java
--- a/src/java.base/share/classes/java/lang/reflect/AccessibleObject.java
+++ b/src/java.base/share/classes/java/lang/reflect/AccessibleObject.java
@@ -300,27 +300,40 @@
                                           boolean throwExceptionIfDenied) {
         if (caller == MethodHandle.class) {
             throw new IllegalCallerException();   // should not happen
         }
 
-        Module callerModule = caller.getModule();
-        Module declaringModule = declaringClass.getModule();
-
-        if (callerModule == declaringModule) return true;
-        if (callerModule == Object.class.getModule()) return true;
-        if (!declaringModule.isNamed()) return true;
-
-        String pn = declaringClass.getPackageName();
         int modifiers;
         if (this instanceof Executable) {
             modifiers = ((Executable) this).getModifiers();
         } else {
             modifiers = ((Field) this).getModifiers();
         }
 
+        // Do not allow suppression of access check for inline class's field
+        if (declaringClass.isInlineClass() &&
+                this instanceof Field
+                && Modifier.isFinal(modifiers)) {
+            if (throwExceptionIfDenied) {
+                String msg = "Unable to make field accessible of inline class "
+                                + declaringClass.getName();
+                throw new InaccessibleObjectException(msg);
+            } else {
+                return false;
+            }
+        }
+
+        Module callerModule = caller.getModule();
+        Module declaringModule = declaringClass.getModule();
+
+        if (callerModule == declaringModule) return true;
+        if (callerModule == Object.class.getModule()) return true;
+        if (!declaringModule.isNamed()) return true;
+
         // class is public and package is exported to caller
         boolean isClassPublic = Modifier.isPublic(declaringClass.getModifiers());
+        String pn = declaringClass.getPackageName();
         if (isClassPublic && declaringModule.isExported(pn, callerModule)) {
             // member is public
             if (Modifier.isPublic(modifiers)) {
                 logIfExportedForIllegalAccess(caller, declaringClass);
                 return true;
diff a/src/java.base/share/classes/java/lang/reflect/Field.java b/src/java.base/share/classes/java/lang/reflect/Field.java
--- a/src/java.base/share/classes/java/lang/reflect/Field.java
+++ b/src/java.base/share/classes/java/lang/reflect/Field.java
@@ -163,11 +163,18 @@
      */
     @Override
     @CallerSensitive
     public void setAccessible(boolean flag) {
         AccessibleObject.checkPermission();
-        if (flag) checkCanSetAccessible(Reflection.getCallerClass());
+
+        if (clazz.isInlineClass()) {
+            throw new InaccessibleObjectException("cannot make a field accessible of inline class "
+                    + clazz.getName());
+        }
+        if (flag) {
+            checkCanSetAccessible(Reflection.getCallerClass());
+        }
         setAccessible0(flag);
     }
 
     @Override
     void checkCanSetAccessible(Class<?> caller) {
diff a/src/java.base/share/classes/jdk/internal/org/objectweb/asm/ClassReader.java b/src/java.base/share/classes/jdk/internal/org/objectweb/asm/ClassReader.java
--- a/src/java.base/share/classes/jdk/internal/org/objectweb/asm/ClassReader.java
+++ b/src/java.base/share/classes/jdk/internal/org/objectweb/asm/ClassReader.java
@@ -1823,10 +1823,12 @@
                 case Constants.LDC2_W:
                 case Opcodes.GETSTATIC:
                 case Opcodes.PUTSTATIC:
                 case Opcodes.GETFIELD:
                 case Opcodes.PUTFIELD:
+                case Opcodes.DEFAULT:
+                case Opcodes.WITHFIELD:
                 case Opcodes.INVOKEVIRTUAL:
                 case Opcodes.INVOKESPECIAL:
                 case Opcodes.INVOKESTATIC:
                 case Opcodes.NEW:
                 case Opcodes.ANEWARRAY:
@@ -2435,22 +2437,23 @@
                 case Opcodes.PUTFIELD:
                 case Opcodes.INVOKEVIRTUAL:
                 case Opcodes.INVOKESPECIAL:
                 case Opcodes.INVOKESTATIC:
                 case Opcodes.INVOKEINTERFACE:
+                case Opcodes.WITHFIELD:
                     {
                         int cpInfoOffset = cpInfoOffsets[readUnsignedShort(currentOffset + 1)];
                         int nameAndTypeCpInfoOffset = cpInfoOffsets[readUnsignedShort(cpInfoOffset + 2)];
                         String owner = readClass(cpInfoOffset, charBuffer);
                         String name = readUTF8(nameAndTypeCpInfoOffset, charBuffer);
                         String descriptor = readUTF8(nameAndTypeCpInfoOffset + 2, charBuffer);
-                        if (opcode < Opcodes.INVOKEVIRTUAL) {
-                            methodVisitor.visitFieldInsn(opcode, owner, name, descriptor);
-                        } else {
+                        if (opcode >= Opcodes.INVOKEVIRTUAL && opcode <= Opcodes.INVOKEINTERFACE) {
                             boolean isInterface =
                                     classBuffer[cpInfoOffset - 1] == Symbol.CONSTANT_INTERFACE_METHODREF_TAG;
                             methodVisitor.visitMethodInsn(opcode, owner, name, descriptor, isInterface);
+                        } else {
+                            methodVisitor.visitFieldInsn(opcode, owner, name, descriptor);
                         }
                         if (opcode == Opcodes.INVOKEINTERFACE) {
                             currentOffset += 5;
                         } else {
                             currentOffset += 3;
@@ -2481,10 +2484,11 @@
                     }
                 case Opcodes.NEW:
                 case Opcodes.ANEWARRAY:
                 case Opcodes.CHECKCAST:
                 case Opcodes.INSTANCEOF:
+                case Opcodes.DEFAULT:
                     methodVisitor.visitTypeInsn(opcode, readClass(currentOffset + 1, charBuffer));
                     currentOffset += 3;
                     break;
                 case Opcodes.IINC:
                     methodVisitor.visitIincInsn(
@@ -3232,11 +3236,12 @@
                     break;
                 case '[':
                     while (methodDescriptor.charAt(currentMethodDescritorOffset) == '[') {
                         ++currentMethodDescritorOffset;
                     }
-                    if (methodDescriptor.charAt(currentMethodDescritorOffset) == 'L') {
+                    char descType = methodDescriptor.charAt(currentMethodDescritorOffset);
+                    if (descType == 'L' || descType == 'Q') {
                         ++currentMethodDescritorOffset;
                         while (methodDescriptor.charAt(currentMethodDescritorOffset) != ';') {
                             ++currentMethodDescritorOffset;
                         }
                     }
diff a/src/java.base/share/classes/jdk/internal/org/objectweb/asm/Constants.java b/src/java.base/share/classes/jdk/internal/org/objectweb/asm/Constants.java
--- a/src/java.base/share/classes/jdk/internal/org/objectweb/asm/Constants.java
+++ b/src/java.base/share/classes/jdk/internal/org/objectweb/asm/Constants.java
@@ -182,14 +182,12 @@
 
     // Constants to convert JVM opcodes to the equivalent ASM specific opcodes, and vice versa.
 
     // The delta between the ASM_IFEQ, ..., ASM_IF_ACMPNE, ASM_GOTO and ASM_JSR opcodes
     // and IFEQ, ..., IF_ACMPNE, GOTO and JSR.
-    static final int ASM_OPCODE_DELTA = 49;
-
-    // The delta between the ASM_IFNULL and ASM_IFNONNULL opcodes and IFNULL and IFNONNULL.
-    static final int ASM_IFNULL_OPCODE_DELTA = 20;
+    // Offset to next available opcode after WITHFIELD from IFEQ
+    static final int ASM_OPCODE_DELTA = (Opcodes.WITHFIELD + 1) - Opcodes.IFEQ;
 
     // ASM specific opcodes, used for long forward jump instructions.
 
     static final int ASM_IFEQ = Opcodes.IFEQ + ASM_OPCODE_DELTA;
     static final int ASM_IFNE = Opcodes.IFNE + ASM_OPCODE_DELTA;
@@ -205,13 +203,18 @@
     static final int ASM_IF_ICMPLE = Opcodes.IF_ICMPLE + ASM_OPCODE_DELTA;
     static final int ASM_IF_ACMPEQ = Opcodes.IF_ACMPEQ + ASM_OPCODE_DELTA;
     static final int ASM_IF_ACMPNE = Opcodes.IF_ACMPNE + ASM_OPCODE_DELTA;
     static final int ASM_GOTO = Opcodes.GOTO + ASM_OPCODE_DELTA;
     static final int ASM_JSR = Opcodes.JSR + ASM_OPCODE_DELTA;
+
+    // The delta between the ASM_IFNULL and ASM_IFNONNULL opcodes and IFNULL and IFNONNULL.
+    // Offset to next available opcode after ASM_JSR from IFNULL.
+    static final int ASM_IFNULL_OPCODE_DELTA = (ASM_JSR + 1) - Opcodes.IFNULL;
+
     static final int ASM_IFNULL = Opcodes.IFNULL + ASM_IFNULL_OPCODE_DELTA;
     static final int ASM_IFNONNULL = Opcodes.IFNONNULL + ASM_IFNULL_OPCODE_DELTA;
-    static final int ASM_GOTO_W = 220;
+    static final int ASM_GOTO_W = GOTO_W + ASM_IFNULL_OPCODE_DELTA;
 
     private Constants() {}
 
     static void checkAsmExperimental(final Object caller) {
         Class<?> callerClass = caller.getClass();
diff a/src/java.base/share/classes/jdk/internal/org/objectweb/asm/Frame.java b/src/java.base/share/classes/jdk/internal/org/objectweb/asm/Frame.java
--- a/src/java.base/share/classes/jdk/internal/org/objectweb/asm/Frame.java
+++ b/src/java.base/share/classes/jdk/internal/org/objectweb/asm/Frame.java
@@ -358,10 +358,11 @@
             case 'J':
                 return LONG;
             case 'D':
                 return DOUBLE;
             case 'L':
+            case 'Q':
                 internalName = buffer.substring(offset + 1, buffer.length() - 1);
                 return REFERENCE_KIND | symbolTable.addType(internalName);
             case '[':
                 int elementDescriptorOffset = offset + 1;
                 while (buffer.charAt(elementDescriptorOffset) == '[') {
@@ -392,10 +393,11 @@
                         break;
                     case 'D':
                         typeValue = DOUBLE;
                         break;
                     case 'L':
+                    case 'Q':
                         internalName = buffer.substring(elementDescriptorOffset + 1, buffer.length() - 1);
                         typeValue = REFERENCE_KIND | symbolTable.addType(internalName);
                         break;
                     default:
                         throw new IllegalArgumentException();
diff a/src/java.base/share/classes/jdk/internal/org/objectweb/asm/Opcodes.java b/src/java.base/share/classes/jdk/internal/org/objectweb/asm/Opcodes.java
--- a/src/java.base/share/classes/jdk/internal/org/objectweb/asm/Opcodes.java
+++ b/src/java.base/share/classes/jdk/internal/org/objectweb/asm/Opcodes.java
@@ -583,6 +583,8 @@
     int MONITORENTER = 194; // visitInsn
     int MONITOREXIT = 195; // -
     int MULTIANEWARRAY = 197; // visitMultiANewArrayInsn
     int IFNULL = 198; // visitJumpInsn
     int IFNONNULL = 199; // -
+    int DEFAULT = 203; // visitTypeInsn
+    int WITHFIELD = 204; // visitFieldInsn
 }
diff a/src/java.base/share/classes/jdk/internal/org/objectweb/asm/Type.java b/src/java.base/share/classes/jdk/internal/org/objectweb/asm/Type.java
--- a/src/java.base/share/classes/jdk/internal/org/objectweb/asm/Type.java
+++ b/src/java.base/share/classes/jdk/internal/org/objectweb/asm/Type.java
@@ -57,10 +57,11 @@
  * THE POSSIBILITY OF SUCH DAMAGE.
  */
 package jdk.internal.org.objectweb.asm;
 
 import java.lang.reflect.Constructor;
+import java.lang.reflect.InvocationTargetException;
 import java.lang.reflect.Method;
 
 /**
  * A Java field or method type. This class can be used to make it easier to manipulate type and
  * method descriptors.
@@ -332,11 +333,12 @@
         // Parse the argument types, one at a each loop iteration.
         while (methodDescriptor.charAt(currentOffset) != ')') {
             while (methodDescriptor.charAt(currentOffset) == '[') {
                 currentOffset++;
             }
-            if (methodDescriptor.charAt(currentOffset++) == 'L') {
+            char c = methodDescriptor.charAt(currentOffset++);
+            if (c == 'L' || c == 'Q') {
                 // Skip the argument descriptor content.
                 int semiColumnOffset = methodDescriptor.indexOf(';', currentOffset);
                 currentOffset = Math.max(currentOffset, semiColumnOffset + 1);
             }
             ++numArgumentTypes;
@@ -351,11 +353,12 @@
         while (methodDescriptor.charAt(currentOffset) != ')') {
             final int currentArgumentTypeOffset = currentOffset;
             while (methodDescriptor.charAt(currentOffset) == '[') {
                 currentOffset++;
             }
-            if (methodDescriptor.charAt(currentOffset++) == 'L') {
+            char c = methodDescriptor.charAt(currentOffset++);
+            if (c == 'L' || c == 'Q') {
                 // Skip the argument descriptor content.
                 int semiColumnOffset = methodDescriptor.indexOf(';', currentOffset);
                 currentOffset = Math.max(currentOffset, semiColumnOffset + 1);
             }
             argumentTypes[currentArgumentTypeIndex++] =
@@ -422,11 +425,12 @@
         // Skip the argument types, one at a each loop iteration.
         while (methodDescriptor.charAt(currentOffset) != ')') {
             while (methodDescriptor.charAt(currentOffset) == '[') {
                 currentOffset++;
             }
-            if (methodDescriptor.charAt(currentOffset++) == 'L') {
+            char c = methodDescriptor.charAt(currentOffset++);
+            if (c == 'L' || c == 'Q') {
                 // Skip the argument descriptor content.
                 int semiColumnOffset = methodDescriptor.indexOf(';', currentOffset);
                 currentOffset = Math.max(currentOffset, semiColumnOffset + 1);
             }
         }
@@ -465,10 +469,11 @@
             case 'D':
                 return DOUBLE_TYPE;
             case '[':
                 return new Type(ARRAY, descriptorBuffer, descriptorBegin, descriptorEnd);
             case 'L':
+            case 'Q':
                 return new Type(OBJECT, descriptorBuffer, descriptorBegin + 1, descriptorEnd - 1);
             case '(':
                 return new Type(METHOD, descriptorBuffer, descriptorBegin, descriptorEnd);
             default:
                 throw new IllegalArgumentException();
@@ -670,14 +675,19 @@
             } else {
                 throw new AssertionError();
             }
             stringBuilder.append(descriptor);
         } else {
-            stringBuilder.append('L').append(getInternalName(currentClass)).append(';');
+            stringBuilder.append(isInlineClass(currentClass) ? 'Q' : 'L').append(getInternalName(currentClass)).append(';');
         }
     }
 
+    static boolean isInlineClass(Class<?> clazz) {
+        int mods = clazz.getModifiers();
+        return (mods & 0x00000100) != 0;
+    }
+
     // -----------------------------------------------------------------------------------------------
     // Methods to get the sort, dimension, size, and opcodes corresponding to a Type or descriptor.
     // -----------------------------------------------------------------------------------------------
 
     /**
@@ -767,11 +777,12 @@
                 argumentsSize += 2;
             } else {
                 while (methodDescriptor.charAt(currentOffset) == '[') {
                     currentOffset++;
                 }
-                if (methodDescriptor.charAt(currentOffset++) == 'L') {
+                char c = methodDescriptor.charAt(currentOffset++);
+                if (c == 'L' || c == 'Q') {
                     // Skip the argument descriptor content.
                     int semiColumnOffset = methodDescriptor.indexOf(';', currentOffset);
                     currentOffset = Math.max(currentOffset, semiColumnOffset + 1);
                 }
                 argumentsSize += 1;
diff a/src/java.base/share/classes/jdk/internal/org/objectweb/asm/commons/AnalyzerAdapter.java b/src/java.base/share/classes/jdk/internal/org/objectweb/asm/commons/AnalyzerAdapter.java
--- a/src/java.base/share/classes/jdk/internal/org/objectweb/asm/commons/AnalyzerAdapter.java
+++ b/src/java.base/share/classes/jdk/internal/org/objectweb/asm/commons/AnalyzerAdapter.java
@@ -531,10 +531,11 @@
                 return;
             case '[':
                 push(descriptor);
                 break;
             case 'L':
+            case 'Q':
                 push(descriptor.substring(1, descriptor.length() - 1));
                 break;
             default:
                 throw new AssertionError();
         }
diff a/src/java.base/share/classes/jdk/internal/org/objectweb/asm/commons/Method.java b/src/java.base/share/classes/jdk/internal/org/objectweb/asm/commons/Method.java
--- a/src/java.base/share/classes/jdk/internal/org/objectweb/asm/commons/Method.java
+++ b/src/java.base/share/classes/jdk/internal/org/objectweb/asm/commons/Method.java
@@ -220,10 +220,11 @@
         String elementType = type.substring(0, type.length() - stringBuilder.length() * 2);
         String descriptor = PRIMITIVE_TYPE_DESCRIPTORS.get(elementType);
         if (descriptor != null) {
             stringBuilder.append(descriptor);
         } else {
+            // FIXME: support Q-type
             stringBuilder.append('L');
             if (elementType.indexOf('.') < 0) {
                 if (!defaultPackage) {
                     stringBuilder.append("java/lang/");
                 }
diff a/src/java.base/share/classes/jdk/internal/org/objectweb/asm/commons/Remapper.java b/src/java.base/share/classes/jdk/internal/org/objectweb/asm/commons/Remapper.java
--- a/src/java.base/share/classes/jdk/internal/org/objectweb/asm/commons/Remapper.java
+++ b/src/java.base/share/classes/jdk/internal/org/objectweb/asm/commons/Remapper.java
@@ -81,10 +81,11 @@
       *     #map(String)} (if the descriptor corresponds to an array or object type, otherwise the
       *     descriptor is returned as is).
       */
     public String mapDesc(final String descriptor) {
         return mapType(Type.getType(descriptor)).getDescriptor();
+            // FIXME: support Q-type
     }
 
     /**
       * Returns the given {@link Type}, remapped with {@link #map(String)} or {@link
       * #mapMethodDesc(String)}.
diff a/src/java.base/share/classes/jdk/internal/org/objectweb/asm/util/CheckClassAdapter.java b/src/java.base/share/classes/jdk/internal/org/objectweb/asm/util/CheckClassAdapter.java
--- a/src/java.base/share/classes/jdk/internal/org/objectweb/asm/util/CheckClassAdapter.java
+++ b/src/java.base/share/classes/jdk/internal/org/objectweb/asm/util/CheckClassAdapter.java
@@ -631,11 +631,11 @@
         int pos = 0;
         if (getChar(signature, 0) == '<') {
             pos = checkTypeParameters(signature, pos);
         }
         pos = checkClassTypeSignature(signature, pos);
-        while (getChar(signature, pos) == 'L') {
+        while (getChar(signature, pos) == 'L' || getChar(signature, pos) == 'Q') {
             pos = checkClassTypeSignature(signature, pos);
         }
         if (pos != signature.length()) {
             throw new IllegalArgumentException(signature + ERROR_AT + pos);
         }
@@ -659,22 +659,22 @@
         int pos = 0;
         if (getChar(signature, 0) == '<') {
             pos = checkTypeParameters(signature, pos);
         }
         pos = checkChar('(', signature, pos);
-        while ("ZCBSIFJDL[T".indexOf(getChar(signature, pos)) != -1) {
+        while ("ZCBSIFJDLQ[T".indexOf(getChar(signature, pos)) != -1) {
             pos = checkJavaTypeSignature(signature, pos);
         }
         pos = checkChar(')', signature, pos);
         if (getChar(signature, pos) == 'V') {
             ++pos;
         } else {
             pos = checkJavaTypeSignature(signature, pos);
         }
         while (getChar(signature, pos) == '^') {
             ++pos;
-            if (getChar(signature, pos) == 'L') {
+            if (getChar(signature, pos) == 'L' || getChar(signature, pos) == 'Q') {
                 pos = checkClassTypeSignature(signature, pos);
             } else {
                 pos = checkTypeVariableSignature(signature, pos);
             }
         }
@@ -786,11 +786,16 @@
         // SimpleClassTypeSignature:
         //   Identifier [TypeArguments]
         // ClassTypeSignatureSuffix:
         //   . SimpleClassTypeSignature
         int pos = startPos;
-        pos = checkChar('L', signature, pos);
+        if (getChar(signature, pos) == 'L' || getChar(signature, pos) == 'Q') {
+            pos = pos + 1;
+        } else {
+            throw new IllegalArgumentException(signature + ": 'L' or 'Q' expected at index " + pos);
+        }
+
         pos = checkSignatureIdentifier(signature, pos);
         while (getChar(signature, pos) == '/') {
             pos = checkSignatureIdentifier(signature, pos + 1);
         }
         if (getChar(signature, pos) == '<') {
diff a/src/java.base/share/classes/jdk/internal/org/objectweb/asm/util/CheckMethodAdapter.java b/src/java.base/share/classes/jdk/internal/org/objectweb/asm/util/CheckMethodAdapter.java
--- a/src/java.base/share/classes/jdk/internal/org/objectweb/asm/util/CheckMethodAdapter.java
+++ b/src/java.base/share/classes/jdk/internal/org/objectweb/asm/util/CheckMethodAdapter.java
@@ -1411,10 +1411,11 @@
                     return checkDescriptor(version, descriptor, pos, false);
                 } else {
                     throw new IllegalArgumentException(INVALID_DESCRIPTOR + descriptor);
                 }
             case 'L':
+            case 'Q':
                 int endPos = descriptor.indexOf(';', startPos);
                 if (startPos == -1 || endPos - startPos < 2) {
                     throw new IllegalArgumentException(INVALID_DESCRIPTOR + descriptor);
                 }
                 try {
diff a/src/java.base/share/classes/jdk/internal/reflect/UnsafeFieldAccessorFactory.java b/src/java.base/share/classes/jdk/internal/reflect/UnsafeFieldAccessorFactory.java
--- a/src/java.base/share/classes/jdk/internal/reflect/UnsafeFieldAccessorFactory.java
+++ b/src/java.base/share/classes/jdk/internal/reflect/UnsafeFieldAccessorFactory.java
@@ -33,11 +33,13 @@
         Class<?> type = field.getType();
         boolean isStatic = Modifier.isStatic(field.getModifiers());
         boolean isFinal = Modifier.isFinal(field.getModifiers());
         boolean isVolatile = Modifier.isVolatile(field.getModifiers());
         boolean isQualified = isFinal || isVolatile;
-        boolean isReadOnly = isFinal && (isStatic || !override || field.getDeclaringClass().isHidden());
+        boolean isReadOnly = isFinal && (isStatic || !override ||
+                                         field.getDeclaringClass().isHidden() ||
+                                         field.getDeclaringClass().isInlineClass());
         if (isStatic) {
             // This code path does not guarantee that the field's
             // declaring class has been initialized, but it must be
             // before performing reflective operations.
             UnsafeFieldAccessorImpl.unsafe.ensureClassInitialized(field.getDeclaringClass());
diff a/src/java.base/share/classes/module-info.java b/src/java.base/share/classes/module-info.java
--- a/src/java.base/share/classes/module-info.java
+++ b/src/java.base/share/classes/module-info.java
@@ -127,11 +127,10 @@
     exports javax.security.auth.login;
     exports javax.security.auth.spi;
     exports javax.security.auth.x500;
     exports javax.security.cert;
 
-
     // additional qualified exports may be inserted at build time
     // see make/gensrc/GenModuleInfo.gmk
 
     exports sun.invoke.util to
         jdk.compiler,
@@ -337,11 +336,10 @@
         java.logging,
         java.prefs;
     exports sun.util.resources to
         jdk.localedata;
 
-
     // the service types defined by the APIs in this module
 
     uses java.lang.System.LoggerFinder;
     uses java.net.ContentHandlerFactory;
     uses java.net.spi.URLStreamHandlerProvider;
diff a/src/java.base/share/classes/sun/invoke/util/BytecodeDescriptor.java b/src/java.base/share/classes/sun/invoke/util/BytecodeDescriptor.java
--- a/src/java.base/share/classes/sun/invoke/util/BytecodeDescriptor.java
+++ b/src/java.base/share/classes/sun/invoke/util/BytecodeDescriptor.java
@@ -1,7 +1,7 @@
 /*
- * Copyright (c) 2008, 2015, Oracle and/or its affiliates. All rights reserved.
+ * Copyright (c) 2008, 2020, Oracle and/or its affiliates. All rights reserved.
  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  *
  * This code is free software; you can redistribute it and/or modify it
  * under the terms of the GNU General Public License version 2 only, as
  * published by the Free Software Foundation.  Oracle designates this
@@ -82,11 +82,11 @@
      *               bootstrap class loader)
      */
     private static Class<?> parseSig(String str, int[] i, int end, ClassLoader loader) {
         if (i[0] == end)  return null;
         char c = str.charAt(i[0]++);
-        if (c == 'L') {
+        if (c == 'L' || c == 'Q') {
             int begc = i[0], endc = str.indexOf(';', begc);
             if (endc < 0)  return null;
             i[0] = endc+1;
             String name = str.substring(begc, endc).replace('/', '.');
             try {
diff a/src/jdk.compiler/share/classes/com/sun/tools/javac/code/Symbol.java b/src/jdk.compiler/share/classes/com/sun/tools/javac/code/Symbol.java
--- a/src/jdk.compiler/share/classes/com/sun/tools/javac/code/Symbol.java
+++ b/src/jdk.compiler/share/classes/com/sun/tools/javac/code/Symbol.java
@@ -412,10 +412,37 @@
 
     public boolean isPrivate() {
         return (flags_field & Flags.AccessFlags) == PRIVATE;
     }
 
+    public boolean isValue() {
+        return (flags() & VALUE) != 0;
+    }
+
+    /**
+     * Is this a *derived* reference projection symbol ??
+     */
+    public boolean isReferenceProjection() {
+        return false;
+    }
+
+    /**
+     * Return the value projection IFF 'this' happens to be derived reference projection, null
+     * otherwise.
+     */
+    public Symbol valueProjection() {
+        return null;
+    }
+
+    /**
+     * Return the reference projection IFF 'this' happens to be value projection, null
+     * otherwise.
+     */
+    public Symbol referenceProjection() {
+        return null;
+    }
+
     public boolean isPublic() {
         return (flags_field & Flags.AccessFlags) == PUBLIC;
     }
 
     public boolean isEnum() {
@@ -445,11 +472,17 @@
     }
 
     /** Is this symbol a constructor?
      */
     public boolean isConstructor() {
-        return name == name.table.names.init;
+        return name == name.table.names.init && (flags() & STATIC) == 0;
+    }
+
+    /** Is this symbol a value factory?
+     */
+    public boolean isValueFactory() {
+        return ((name == name.table.names.init && this.type.getReturnType().tsym == this.owner));
     }
 
     public boolean isDynamic() {
         return false;
     }
@@ -508,19 +541,21 @@
         }
         return (ClassSymbol)c;
     }
 
     /** The outermost class which indirectly owns this symbol.
+     * 'outermost' being a lexical construct, should transcend
+     *  projections
      */
     public ClassSymbol outermostClass() {
         Symbol sym = this;
         Symbol prev = null;
         while (sym.kind != PCK) {
             prev = sym;
             sym = sym.owner;
         }
-        return (ClassSymbol) prev;
+        return (ClassSymbol) (prev!= null && prev.isReferenceProjection() ? prev.valueProjection() : prev);
     }
 
     /** The package which indirectly owns this symbol.
      */
     public PackageSymbol packge() {
@@ -1283,10 +1318,16 @@
         /* the list of any of record components, only non empty if the class is a record
          * and it has at least one record component
          */
         private List<RecordComponent> recordComponents = List.nil();
 
+        /* the 'other' projection: If 'this' is an inline class then 'projection' is its reference projection
+           and vice versa.
+         */
+        public ClassSymbol projection;
+
+
         public ClassSymbol(long flags, Name name, Type type, Symbol owner) {
             super(TYP, flags, name, type, owner);
             this.members_field = null;
             this.fullname = formFullName(name, owner);
             this.flatname = formFlatName(name, owner);
@@ -1342,11 +1383,11 @@
 
         public String className() {
             if (name.isEmpty())
                 return
                     Log.getLocalizedString("anonymous.class", flatname);
-            else
+
                 return fullname.toString();
         }
 
         @DefinedBy(Api.LANGUAGE_MODEL)
         public Name getQualifiedName() {
@@ -1596,10 +1637,65 @@
         }
 
         public boolean isRecord() {
             return (flags_field & RECORD) != 0;
         }
+
+        @Override
+        public boolean isReferenceProjection() {
+            return projection != null && projection.isValue();
+        }
+
+        @Override
+        public ClassSymbol valueProjection() {
+            return isReferenceProjection() ? projection : null;
+        }
+
+        @Override
+        public ClassSymbol referenceProjection() {
+            if (!isValue() || projection != null)
+                return projection;
+
+            ClassType ct = (ClassType) this.type;
+            ClassType projectedType = new ClassType(ct.getEnclosingType(), ct.typarams_field, null);
+            projectedType.allparams_field = ct.allparams_field;
+            projectedType.supertype_field = ct.supertype_field;
+
+            projectedType.interfaces_field = ct.interfaces_field;
+            projectedType.all_interfaces_field = ct.all_interfaces_field;
+            projectedType.projection = ct;
+            ct.projection = projectedType;
+
+            Name projectionName = this.name.append('$', this.name.table.names.ref);
+            long projectionFlags = (this.flags() & ~(VALUE | UNATTRIBUTED));
+
+            projection = new ClassSymbol(projectionFlags, projectionName, projectedType, this.owner);
+            projection.members_field = WriteableScope.create(projection);
+            for (Symbol s : this.members().getSymbols(s->(s.kind == MTH || s.kind == VAR), NON_RECURSIVE)) {
+                Symbol clone = null;
+                if (s.kind == MTH) {
+                    MethodSymbol valMethod = (MethodSymbol)s;
+                    MethodSymbol refMethod = valMethod.clone(projection);
+                    valMethod.projection = refMethod;
+                    refMethod.projection = valMethod;
+                    clone = refMethod;
+                } else if (s.kind == VAR) {
+                    VarSymbol valVar = (VarSymbol)s;
+                    VarSymbol refVar = valVar.clone(projection);
+                    valVar.projection = refVar;
+                    refVar.projection = valVar;
+                    clone = refVar;
+                }
+                projection.members_field.enter(clone);
+            }
+            projection.completer = Completer.NULL_COMPLETER;
+            projection.sourcefile = this.sourcefile;
+            projection.flatname = this.flatname.append('$', this.name.table.names.ref);
+            projection.projection = this;
+            projectedType.tsym = projection;
+            return projection;
+        }
     }
 
 
     /** A class for variable symbols
      */
@@ -1618,10 +1714,15 @@
          *  Code generation:
          *    If this is a local variable, its logical slot number.
          */
         public int adr = -1;
 
+        /* The 'other' projection: If 'this' is a field of an inline class, then 'projection' is the
+           its doppleganger in its referene projection class and vice versa.
+        */
+        public VarSymbol projection;
+
         /** Construct a variable symbol, given its flags, name, type and owner.
          */
         public VarSymbol(long flags, Name name, Type type, Symbol owner) {
             super(VAR, flags, name, type, owner);
         }
@@ -1650,10 +1751,11 @@
                 }
             };
             v.pos = pos;
             v.adr = adr;
             v.data = data;
+            v.projection = projection;
 //          System.out.println("clone " + v + " in " + newOwner);//DEBUG
             return v;
         }
 
         public String toString() {
@@ -1702,10 +1804,22 @@
                                       final JCVariableDecl variable)
         {
             setData((Callable<Object>)() -> attr.attribLazyConstantValue(env, variable, type));
         }
 
+        @Override
+        public VarSymbol referenceProjection() {
+            return this.owner.isValue() ?
+                    this.owner.referenceProjection() != null ? projection : null
+                               : null;
+        }
+
+        @Override
+        public VarSymbol valueProjection() {
+            return  projection != null ? projection.owner.isValue() ? projection : null: null;
+        }
+
         /**
          * The variable's constant value, if this is a constant.
          * Before the constant value is evaluated, it points to an
          * initializer environment.  If this is not a constant, it can
          * be used for other stuff.
@@ -1852,10 +1966,15 @@
          *  The value is null if none appeared in the method
          *  declaration.
          */
         public Attribute defaultValue = null;
 
+        /* The 'other' projection: If 'this' is a method of an inline class, then 'projection' is the
+           its doppleganger in its referene projection class and vice versa.
+        */
+        public MethodSymbol projection;
+
         /** Construct a method symbol, given its flags, name, type and owner.
          */
         public MethodSymbol(long flags, Name name, Type type, Symbol owner) {
             super(MTH, flags, name, type, owner);
             if (owner.type.hasTag(TYPEVAR)) Assert.error(owner + "." + name);
@@ -1874,10 +1993,11 @@
                 public Object poolKey(Types types) {
                     return new Pair<>(newOwner, baseSymbol());
                 }
             };
             m.code = code;
+            m.projection = projection;
             return m;
         }
 
         @Override @DefinedBy(Api.LANGUAGE_MODEL)
         public Set<Modifier> getModifiers() {
@@ -2022,10 +2142,20 @@
          */
         public boolean overrides(Symbol _other, TypeSymbol origin, Types types, boolean checkResult,
                                             boolean requireConcreteIfInherited) {
             if (isConstructor() || _other.kind != MTH) return false;
 
+
+            /* If any inline types are involved, ask the same question in the reference universe,
+               where the hierarchy is navigable
+            */
+            if (origin.isValue())
+                origin = (TypeSymbol) origin.referenceProjection();
+
+            if (this.owner.isValue())
+                return this.projection.overrides(_other, origin, types, checkResult, requireConcreteIfInherited);
+
             if (this == _other) return true;
             MethodSymbol other = (MethodSymbol)_other;
 
             // check for a direct implementation
             if (other.isOverridableIn((TypeSymbol)owner) &&
@@ -2076,10 +2206,19 @@
             }
         }
 
         @Override
         public boolean isInheritedIn(Symbol clazz, Types types) {
+
+            /* If any inline types are involved, ask the same question in the reference universe,
+               where the hierarchy is navigable
+            */
+            if (clazz.isValue())
+                clazz = clazz.referenceProjection();
+            if (this.owner.isValue())
+                return this.projection.isInheritedIn(clazz, types);
+
             switch ((int)(flags_field & Flags.AccessFlags)) {
                 case PUBLIC:
                     return !this.owner.isInterface() ||
                             clazz == owner ||
                             (flags_field & STATIC) == 0;
@@ -2090,10 +2229,22 @@
 
         public boolean isLambdaMethod() {
             return (flags() & LAMBDA_METHOD) == LAMBDA_METHOD;
         }
 
+        @Override
+        public MethodSymbol referenceProjection() {
+            return this.owner.isValue() ?
+                    this.owner.referenceProjection() != null ? projection : null
+                    : null;
+        }
+
+        @Override
+        public MethodSymbol valueProjection() {
+            return  projection != null ? projection.owner.isValue() ? projection : null : null;
+        }
+
         /** override this method to point to the original enclosing method if this method symbol represents a synthetic
          *  lambda method
          */
         public MethodSymbol originalEnclosingMethod() {
             return this;
@@ -2383,11 +2534,11 @@
             }
             accessCode = AccessCode.from(tag, opcode);
             return accessCode;
         }
 
-        /** Access codes for dereferencing, assignment,
+        /** Access codes for dereferencing, assignment, withfield
          *  and pre/post increment/decrement.
 
          *  All access codes for accesses to the current class are even.
          *  If a member of the superclass should be accessed instead (because
          *  access was via a qualified super), add one to the corresponding code
@@ -2403,11 +2554,12 @@
             ASSIGN(2, Tag.ASSIGN),
             PREINC(4, Tag.PREINC),
             PREDEC(6, Tag.PREDEC),
             POSTINC(8, Tag.POSTINC),
             POSTDEC(10, Tag.POSTDEC),
-            FIRSTASGOP(12, Tag.NO_TAG);
+            WITHFIELD(12, Tag.WITHFIELD),
+            FIRSTASGOP(14, Tag.NO_TAG);
 
             public final int code;
             public final Tag tag;
             public static final int numberOfAccessCodes = (lushrl - ishll + lxor + 2 - iadd) * 2 + FIRSTASGOP.code + 2;
 
@@ -2436,10 +2588,12 @@
                         return AccessCode.PREDEC.code;
                     case POSTINC:
                         return AccessCode.POSTINC.code;
                     case POSTDEC:
                         return AccessCode.POSTDEC.code;
+                    case WITHFIELD:
+                        return AccessCode.WITHFIELD.code;
                 }
                 if (iadd <= opcode && opcode <= lxor) {
                     return (opcode - iadd) * 2 + FIRSTASGOP.code;
                 } else if (opcode == string_add) {
                     return (lxor + 1 - iadd) * 2 + FIRSTASGOP.code;
diff a/src/jdk.compiler/share/classes/com/sun/tools/javac/code/Symtab.java b/src/jdk.compiler/share/classes/com/sun/tools/javac/code/Symtab.java
--- a/src/jdk.compiler/share/classes/com/sun/tools/javac/code/Symtab.java
+++ b/src/jdk.compiler/share/classes/com/sun/tools/javac/code/Symtab.java
@@ -50,10 +50,11 @@
 import com.sun.tools.javac.code.Type.ErrorType;
 import com.sun.tools.javac.code.Type.JCPrimitiveType;
 import com.sun.tools.javac.code.Type.JCVoidType;
 import com.sun.tools.javac.code.Type.MethodType;
 import com.sun.tools.javac.code.Type.UnknownType;
+import com.sun.tools.javac.code.Type.WildcardType;
 import com.sun.tools.javac.code.Types.UniqueType;
 import com.sun.tools.javac.comp.Modules;
 import com.sun.tools.javac.jvm.Target;
 import com.sun.tools.javac.util.Assert;
 import com.sun.tools.javac.util.Context;
@@ -63,10 +64,11 @@
 import com.sun.tools.javac.util.Iterators;
 import com.sun.tools.javac.util.JavacMessages;
 import com.sun.tools.javac.util.List;
 import com.sun.tools.javac.util.Name;
 import com.sun.tools.javac.util.Names;
+import com.sun.tools.javac.util.Options;
 
 import static com.sun.tools.javac.code.Flags.*;
 import static com.sun.tools.javac.code.Kinds.Kind.*;
 import static com.sun.tools.javac.code.TypeTag.*;
 
@@ -210,17 +212,20 @@
     public final Type systemType;
     public final Type autoCloseableType;
     public final Type trustMeType;
     public final Type lambdaMetafactory;
     public final Type stringConcatFactory;
+    public final Type valueBootstrapMethods;
     public final Type repeatableType;
     public final Type documentedType;
     public final Type elementTypeType;
     public final Type functionalInterfaceType;
     public final Type previewFeatureType;
     public final Type typeDescriptorType;
     public final Type recordType;
+    public final Type valueBasedType;
+    public final Type identityObjectType;
 
     /** The symbol representing the length field of an array.
      */
     public final VarSymbol lengthVar;
 
@@ -259,12 +264,21 @@
 
     public VarSymbol getClassField(Type type, Types types) {
         return classFields.computeIfAbsent(
             new UniqueType(type, types), k -> {
                 Type arg = null;
-                if (type.getTag() == ARRAY || type.getTag() == CLASS)
-                    arg = types.erasure(type);
+                if (type.getTag() == ARRAY || type.getTag() == CLASS) {
+                    /* Temporary treatment for inline class: Given an inline class V that implements
+                       I1, I2, ... In, V.class is typed to be Class<? extends Object & I1 & I2 .. & In>
+                    */
+                    if (type.isValue()) {
+                        List<Type> bounds = List.of(objectType).appendList(((ClassSymbol) type.tsym).getInterfaces());
+                        arg = new WildcardType(types.makeIntersectionType(bounds), BoundKind.EXTENDS, boundClass);
+                    } else {
+                        arg = types.erasure(type);
+                    }
+                }
                 else if (type.isPrimitiveOrVoid())
                     arg = types.boxedClass(type).type;
                 else
                     throw new AssertionError(type);
 
@@ -576,21 +590,25 @@
                              autoCloseableType.tsym);
         trustMeType = enterClass("java.lang.SafeVarargs");
         nativeHeaderType = enterClass("java.lang.annotation.Native");
         lambdaMetafactory = enterClass("java.lang.invoke.LambdaMetafactory");
         stringConcatFactory = enterClass("java.lang.invoke.StringConcatFactory");
+        valueBootstrapMethods = enterClass("java.lang.invoke.ValueBootstrapMethods");
         functionalInterfaceType = enterClass("java.lang.FunctionalInterface");
         previewFeatureType = enterClass("jdk.internal.PreviewFeature");
         typeDescriptorType = enterClass("java.lang.invoke.TypeDescriptor");
         recordType = enterClass("java.lang.Record");
+        valueBasedType = enterClass("java.lang.ValueBased");
+        identityObjectType = enterClass("java.lang.IdentityObject");
 
         synthesizeEmptyInterfaceIfMissing(autoCloseableType);
         synthesizeEmptyInterfaceIfMissing(cloneableType);
         synthesizeEmptyInterfaceIfMissing(serializableType);
         synthesizeEmptyInterfaceIfMissing(lambdaMetafactory);
         synthesizeEmptyInterfaceIfMissing(serializedLambdaType);
         synthesizeEmptyInterfaceIfMissing(stringConcatFactory);
+        synthesizeEmptyInterfaceIfMissing(identityObjectType);
         synthesizeBoxTypeIfMissing(doubleType);
         synthesizeBoxTypeIfMissing(floatType);
         synthesizeBoxTypeIfMissing(voidType);
 
         // Enter a synthetic class that is used to mark internal
@@ -607,11 +625,13 @@
         // Enter a class for arrays.
         // The class implements java.lang.Cloneable and java.io.Serializable.
         // It has a final length field and a clone method.
         ClassType arrayClassType = (ClassType)arrayClass.type;
         arrayClassType.supertype_field = objectType;
-        arrayClassType.interfaces_field = List.of(cloneableType, serializableType);
+        arrayClassType.interfaces_field =
+                List.of(cloneableType, serializableType, identityObjectType);
+
         arrayClass.members_field = WriteableScope.create(arrayClass);
         lengthVar = new VarSymbol(
             PUBLIC | FINAL,
             names.length,
             intType,
diff a/src/jdk.compiler/share/classes/com/sun/tools/javac/comp/Attr.java b/src/jdk.compiler/share/classes/com/sun/tools/javac/comp/Attr.java
--- a/src/jdk.compiler/share/classes/com/sun/tools/javac/comp/Attr.java
+++ b/src/jdk.compiler/share/classes/com/sun/tools/javac/comp/Attr.java
@@ -25,11 +25,10 @@
 
 package com.sun.tools.javac.comp;
 
 import java.util.*;
 import java.util.function.BiConsumer;
-import java.util.stream.Collectors;
 
 import javax.lang.model.element.ElementKind;
 import javax.tools.JavaFileObject;
 
 import com.sun.source.tree.CaseTree;
@@ -164,15 +163,18 @@
         allowPoly = Feature.POLY.allowedInSource(source);
         allowTypeAnnos = Feature.TYPE_ANNOTATIONS.allowedInSource(source);
         allowLambda = Feature.LAMBDA.allowedInSource(source);
         allowDefaultMethods = Feature.DEFAULT_METHODS.allowedInSource(source);
         allowStaticInterfaceMethods = Feature.STATIC_INTERFACE_METHODS.allowedInSource(source);
+        allowInlineTypes = Feature.INLINE_TYPES.allowedInSource(source);
         allowReifiableTypesInInstanceof =
                 Feature.REIFIABLE_TYPES_INSTANCEOF.allowedInSource(source) &&
                 (!preview.isPreview(Feature.REIFIABLE_TYPES_INSTANCEOF) || preview.isEnabled());
         sourceName = source.name;
         useBeforeDeclarationWarning = options.isSet("useBeforeDeclarationWarning");
+        allowEmptyValues = options.isSet("allowEmptyValues");
+        allowValueMemberCycles = options.isSet("allowValueMemberCycles");
 
         statInfo = new ResultInfo(KindSelector.NIL, Type.noType);
         varAssignmentInfo = new ResultInfo(KindSelector.ASG, Type.noType);
         unknownExprInfo = new ResultInfo(KindSelector.VAL, Type.noType);
         methodAttrInfo = new MethodAttrInfo();
@@ -195,10 +197,14 @@
 
     /** Switch: support default methods ?
      */
     boolean allowDefaultMethods;
 
+    /** Switch: allow inline types?
+     */
+    boolean allowInlineTypes;
+
     /** Switch: static interface methods enabled?
      */
     boolean allowStaticInterfaceMethods;
 
     /** Switch: reifiable types in instanceof enabled?
@@ -209,10 +215,20 @@
      * Switch: warn about use of variable before declaration?
      * RFE: 6425594
      */
     boolean useBeforeDeclarationWarning;
 
+    /**
+     * Switch: Allow value types with no instance state?
+     */
+    boolean allowEmptyValues;
+
+    /**
+     * Switch: Allow value type member cycles?
+     */
+    boolean allowValueMemberCycles;
+
     /**
      * Switch: name of source level; used for error reporting.
      */
     String sourceName;
 
@@ -305,11 +321,21 @@
             if (v.isResourceVariable()) { //TWR resource
                 log.error(pos, Errors.TryResourceMayNotBeAssigned(v));
             } else if ((v.flags() & MATCH_BINDING) != 0) {
                 log.error(pos, Errors.PatternBindingMayNotBeAssigned(v));
             } else {
-                log.error(pos, Errors.CantAssignValToFinalVar(v));
+                boolean complain = true;
+                /* Allow updates to instance fields of value classes by any method in the same nest via the
+                   withfield operator -This does not result in mutation of final fields; the code generator
+                   would implement `copy on write' semantics via the opcode `withfield'.
+                */
+                if (env.info.inWithField && v.getKind() == ElementKind.FIELD && (v.flags() & STATIC) == 0 && types.isValue(v.owner.type)) {
+                    if (env.enclClass.sym.outermostClass() == v.owner.outermostClass())
+                        complain = false;
+                }
+                if (complain)
+                    log.error(pos, Errors.CantAssignValToFinalVar(v));
             }
         }
     }
 
     /** Does tree represent a static reference to an identifier?
@@ -797,13 +823,13 @@
         for (JCTypeParameter tvar : typarams) {
             TypeVar a = (TypeVar)tvar.type;
             a.tsym.flags_field |= UNATTRIBUTED;
             a.setUpperBound(Type.noType);
             if (!tvar.bounds.isEmpty()) {
-                List<Type> bounds = List.of(attribType(tvar.bounds.head, env));
+                List<Type> bounds = List.of(chk.checkRefType(tvar.bounds.head, attribType(tvar.bounds.head, env), false));
                 for (JCExpression bound : tvar.bounds.tail)
-                    bounds = bounds.prepend(attribType(bound, env));
+                    bounds = bounds.prepend(chk.checkRefType(bound, attribType(bound, env), false));
                 types.setBounds(a, bounds.reverse());
             } else {
                 // if no bounds are given, assume a single bound of
                 // java.lang.Object.
                 types.setBounds(a, List.of(syms.objectType));
@@ -960,10 +986,13 @@
                 // (This would be an illegal access to "this before super").
                 if (env.info.isSelfCall &&
                         env.tree.hasTag(NEWCLASS)) {
                     c.flags_field |= NOOUTERTHIS;
                 }
+                if (env.tree.hasTag(NEWCLASS) && types.isValue(c.getSuperclass())) {
+                    c.flags_field |= VALUE; // avoid further secondary errors.
+                }
                 attribClass(tree.pos(), c);
                 result = tree.type = c.type;
             }
         } finally {
             localCacheContext.ifPresent(LocalCacheContext::leave);
@@ -1157,11 +1186,11 @@
                 // super(...) or this(...) is given
                 // or we are compiling class java.lang.Object.
                 if (tree.name == names.init && owner.type != syms.objectType) {
                     JCBlock body = tree.body;
                     if (body.stats.isEmpty() ||
-                            TreeInfo.getConstructorInvocationName(body.stats, names) == names.empty) {
+                            TreeInfo.getConstructorInvocationName(body.stats, names, true) == names.empty) {
                         JCStatement supCall = make.at(body.pos).Exec(make.Apply(List.nil(),
                                 make.Ident(names._super), make.Idents(List.nil())));
                         body.stats = body.stats.prepend(supCall);
                     } else if ((env.enclClass.sym.flags() & ENUM) != 0 &&
                             (tree.mods.flags & GENERATEDCONSTR) == 0 &&
@@ -1193,10 +1222,16 @@
                                             Fragments.ThrowsClauseNotAllowedForCanonicalConstructor(
                                                     TreeInfo.isCompactConstructor(tree) ? Fragments.Compact : Fragments.Canonical)));
                         }
                     }
                 }
+                if (m.isConstructor() && m.type.getParameterTypes().size() == 0) {
+                    if ((owner.type == syms.objectType) ||
+                            (tree.body.stats.size() == 1 && TreeInfo.getConstructorInvocationName(tree.body.stats, names, false) == names._super)) {
+                        m.flags_field |= EMPTYNOARGCONSTR;
+                    }
+                }
 
                 // Attribute all type annotations in the body
                 annotate.queueScanTreeAndTypeAnnotate(tree.body, localEnv, m, null);
                 annotate.flush();
 
@@ -1262,12 +1297,15 @@
         try {
             v.getConstValue(); // ensure compile-time constant initializer is evaluated
             deferredLintHandler.flush(tree.pos());
             chk.checkDeprecatedAnnotation(tree.pos(), v);
 
+            /* Don't want constant propagation/folding for instance fields of value classes,
+               as these can undergo updates via copy on write.
+            */
             if (tree.init != null) {
-                if ((v.flags_field & FINAL) == 0 ||
+                if ((v.flags_field & FINAL) == 0 || ((v.flags_field & STATIC) == 0 && types.isValue(v.owner.type)) ||
                     !memberEnter.needsLazyConstValue(tree.init)) {
                     // Not a compile-time constant
                     // Attribute initializer in a new environment
                     // with the declared variable as owner.
                     // Check that initializer conforms to variable's declared type.
@@ -1387,11 +1425,15 @@
                     env.info.scope.owner.flags() & STRICTFP, names.empty, null,
                     env.info.scope.owner);
             final Env<AttrContext> localEnv =
                 env.dup(tree, env.info.dup(env.info.scope.dupUnshared(fakeOwner)));
 
-            if ((tree.flags & STATIC) != 0) localEnv.info.staticLevel++;
+            if ((tree.flags & STATIC) != 0)
+                localEnv.info.staticLevel++;
+            else if (tree.stats.size() > 0)
+                env.info.scope.owner.flags_field |= HASINITBLOCK;
+
             // Attribute all type annotations in the block
             annotate.queueScanTreeAndTypeAnnotate(tree, localEnv, localEnv.info.scope.owner, null);
             annotate.flush();
             attribStats(tree.stats, localEnv);
 
@@ -1452,10 +1494,43 @@
     private boolean breaksOutOf(JCTree loop, JCTree body) {
         preFlow(body);
         return flow.breaksOutOf(env, loop, body, make);
     }
 
+    public void visitWithField(JCWithField tree) {
+        boolean inWithField = env.info.inWithField;
+        try {
+            env.info.inWithField = true;
+            Type fieldtype = attribTree(tree.field, env.dup(tree), varAssignmentInfo);
+            attribExpr(tree.value, env, fieldtype);
+            Type capturedType = syms.errType;
+            if (tree.field.type != null && !tree.field.type.isErroneous()) {
+                final Symbol sym = TreeInfo.symbol(tree.field);
+                if (sym == null || sym.kind != VAR || sym.owner.kind != TYP ||
+                        (sym.flags() & STATIC) != 0 || !types.isValue(sym.owner.type)) {
+                    log.error(tree.field.pos(), Errors.ValueInstanceFieldExpectedHere);
+                } else {
+                    Type ownType = sym.owner.type;
+                    switch(tree.field.getTag()) {
+                        case IDENT:
+                            JCIdent ident = (JCIdent) tree.field;
+                            ownType = ident.sym.owner.type;
+                            break;
+                        case SELECT:
+                            JCFieldAccess fieldAccess = (JCFieldAccess) tree.field;
+                            ownType = fieldAccess.selected.type;
+                            break;
+                    }
+                    capturedType = capture(ownType);
+                }
+            }
+            result = check(tree, capturedType, KindSelector.VAL, resultInfo);
+        } finally {
+            env.info.inWithField = inWithField;
+        }
+    }
+
     public void visitForLoop(JCForLoop tree) {
         Env<AttrContext> loopEnv =
             env.dup(env.tree, env.info.dup(env.info.scope.dup()));
         MatchBindings condBindings = MatchBindingsComputer.EMPTY;
         try {
@@ -1495,11 +1570,11 @@
             Type exprType = types.cvarUpperBound(attribExpr(tree.expr, loopEnv));
             chk.checkNonVoid(tree.pos(), exprType);
             Type elemtype = types.elemtype(exprType); // perhaps expr is an array?
             if (elemtype == null) {
                 // or perhaps expr implements Iterable<T>?
-                Type base = types.asSuper(exprType, syms.iterableType.tsym);
+                Type base = types.asSuper(exprType, syms.iterableType.tsym, true);
                 if (base == null) {
                     log.error(tree.expr.pos(),
                               Errors.ForeachNotApplicableToType(exprType,
                                                                 Fragments.TypeReqArrayOrIterable));
                     elemtype = types.createErrorType(exprType);
@@ -1709,11 +1784,11 @@
         }
         return null;
     }
 
     public void visitSynchronized(JCSynchronized tree) {
-        chk.checkRefType(tree.pos(), attribExpr(tree.lock, env));
+        chk.checkRefType(tree.pos(), attribExpr(tree.lock, env), false);
         attribStat(tree.body, env);
         result = null;
     }
 
     public void visitTry(JCTry tree) {
@@ -1790,11 +1865,11 @@
         }
     }
 
     void checkAutoCloseable(DiagnosticPosition pos, Env<AttrContext> env, Type resource) {
         if (!resource.isErroneous() &&
-            types.asSuper(resource, syms.autoCloseableType.tsym) != null &&
+            types.asSuper(resource, syms.autoCloseableType.tsym, true) != null &&
             !types.isSameType(resource, syms.autoCloseableType)) { // Don't emit warning for AutoCloseable itself
             Symbol close = syms.noSymbol;
             Log.DiagnosticHandler discardHandler = new Log.DiscardDiagnosticHandler(log);
             try {
                 close = rs.resolveQualifiedMethod(pos,
@@ -2390,10 +2465,46 @@
             Symbol msym = TreeInfo.symbol(tree.meth);
             restype = adjustMethodReturnType(msym, qualifier, methName, argtypes, restype);
 
             chk.checkRefTypes(tree.typeargs, typeargtypes);
 
+            final Symbol symbol = TreeInfo.symbol(tree.meth);
+            if (symbol != null) {
+                /* Is this an ill conceived attempt to invoke jlO methods not available on value types ??
+                 */
+                boolean superCallOnValueReceiver = types.isValue(env.enclClass.sym.type)
+                        && (tree.meth.hasTag(SELECT))
+                        && ((JCFieldAccess)tree.meth).selected.hasTag(IDENT)
+                        && TreeInfo.name(((JCFieldAccess)tree.meth).selected) == names._super;
+                if (types.isValue(qualifier) || superCallOnValueReceiver) {
+                    int argSize = argtypes.size();
+                    Name name = symbol.name;
+                    switch (name.toString()) {
+                        case "wait":
+                            if (argSize == 0
+                                    || (types.isConvertible(argtypes.head, syms.longType) &&
+                                    (argSize == 1 || (argSize == 2 && types.isConvertible(argtypes.tail.head, syms.intType))))) {
+                                log.error(tree.pos(), Errors.ValueDoesNotSupport(name));
+                            }
+                            break;
+                        case "notify":
+                        case "notifyAll":
+                        case "clone":
+                        case "finalize":
+                            if (argSize == 0)
+                                log.error(tree.pos(), Errors.ValueDoesNotSupport(name));
+                            break;
+                        case "hashCode":
+                        case "equals":
+                        case "toString":
+                            if (superCallOnValueReceiver)
+                                log.error(tree.pos(), Errors.ValueDoesNotSupport(names.fromString("invocation of super." + name)));
+                            break;
+                    }
+                }
+            }
+
             // Check that value of resulting type is admissible in the
             // current context.  Also, capture the return type
             Type capturedRes = resultInfo.checkContext.inferenceContext().cachedCapture(tree, restype, true);
             result = check(tree, capturedRes, KindSelector.VAL, resultInfo);
         }
@@ -2404,12 +2515,21 @@
             if (msym != null &&
                     msym.owner == syms.objectType.tsym &&
                     methodName == names.getClass &&
                     argtypes.isEmpty()) {
                 // as a special case, x.getClass() has type Class<? extends |X|>
+                // Temporary treatment for inline class: Given an inline class V that implements
+                // I1, I2, ... In, v.getClass() is typed to be Class<? extends Object & I1 & I2 .. & In>
+                Type wcb;
+                if (qualifierType.isValue()) {
+                    List<Type> bounds = List.of(syms.objectType).appendList(((ClassSymbol) qualifierType.tsym).getInterfaces());
+                    wcb = types.makeIntersectionType(bounds);
+                } else {
+                    wcb = types.erasure(qualifierType);
+                }
                 return new ClassType(restype.getEnclosingType(),
-                        List.of(new WildcardType(types.erasure(qualifierType),
+                        List.of(new WildcardType(wcb,
                                 BoundKind.EXTENDS,
                                 syms.boundClass)),
                         restype.tsym,
                         restype.getMetadata());
             } else if (msym != null &&
@@ -2749,10 +2869,11 @@
                     }
                     // For <>(){}, inferred types must also be accessible.
                     for (Type t : clazztype.getTypeArguments()) {
                         rs.checkAccessibleType(env, t);
                     }
+                    chk.checkParameterizationWithValues(tree, clazztype);
                 }
 
                 // If we already errored, be careful to avoid a further avalanche. ErrorType answers
                 // false for isInterface call even when the original type is an interface.
                 boolean implementing = clazztype.tsym.isInterface() ||
@@ -2821,10 +2942,13 @@
      */
     public JCExpression makeNullCheck(JCExpression arg) {
         // optimization: new Outer() can never be null; skip null check
         if (arg.getTag() == NEWCLASS)
             return arg;
+        // Likewise arg can't be null if it is a value.
+        if (types.isValue(arg.type))
+            return arg;
         // optimization: X.this is never null; skip null check
         Name name = TreeInfo.name(arg);
         if (name == names._this || name == names._super) return arg;
 
         JCTree.Tag optag = NULLCHK;
@@ -3827,10 +3951,11 @@
             // comparisons will not have an acmp* opc at this point.
             if ((opc == ByteCodes.if_acmpeq || opc == ByteCodes.if_acmpne)) {
                 if (!types.isCastable(left, right, new Warner(tree.pos()))) {
                     log.error(tree.pos(), Errors.IncomparableTypes(left, right));
                 }
+                chk.checkForSuspectClassLiteralComparison(tree, left, right);
             }
 
             chk.checkDivZero(tree.rhs.pos(), operator, right);
         }
         result = check(tree, owntype, KindSelector.VAL, resultInfo);
@@ -4024,12 +4149,16 @@
 
     public void visitSelect(JCFieldAccess tree) {
         // Determine the expected kind of the qualifier expression.
         KindSelector skind = KindSelector.NIL;
         if (tree.name == names._this || tree.name == names._super ||
-                tree.name == names._class)
+                tree.name == names._class || tree.name == names._default)
         {
+            if (tree.name == names._default && !allowInlineTypes) {
+                log.error(DiagnosticFlag.SOURCE_LEVEL, tree.pos(),
+                        Feature.INLINE_TYPES.error(sourceName));
+            }
             skind = KindSelector.TYP;
         } else {
             if (pkind().contains(KindSelector.PCK))
                 skind = KindSelector.of(skind, KindSelector.PCK);
             if (pkind().contains(KindSelector.TYP))
@@ -4047,21 +4176,27 @@
         if (skind == KindSelector.TYP) {
             Type elt = site;
             while (elt.hasTag(ARRAY))
                 elt = ((ArrayType)elt).elemtype;
             if (elt.hasTag(TYPEVAR)) {
-                log.error(tree.pos(), Errors.TypeVarCantBeDeref);
-                result = tree.type = types.createErrorType(tree.name, site.tsym, site);
-                tree.sym = tree.type.tsym;
-                return ;
+                if (tree.name == names._default) {
+                    result = check(tree, litType(BOT).constType(null),
+                            KindSelector.VAL, resultInfo);
+                } else {
+                    log.error(tree.pos(), Errors.TypeVarCantBeDeref);
+                    result = tree.type = types.createErrorType(tree.name, site.tsym, site);
+                    tree.sym = tree.type.tsym;
+                    return;
+                }
             }
         }
 
         // If qualifier symbol is a type or `super', assert `selectSuper'
         // for the selection. This is relevant for determining whether
         // protected symbols are accessible.
         Symbol sitesym = TreeInfo.symbol(tree.selected);
+
         boolean selectSuperPrev = env.info.selectSuper;
         env.info.selectSuper =
             sitesym != null &&
             sitesym.name == names._super;
 
@@ -4193,19 +4328,29 @@
                     return rs.resolveSelf(pos, env, site.tsym, name);
                 } else if (name == names._class) {
                     // In this case, we have already made sure in
                     // visitSelect that qualifier expression is a type.
                     return syms.getClassField(site, types);
+                } else if (name == names._default) {
+                    return new VarSymbol(STATIC, names._default, site, site.tsym);
+                } else if (name == names.ref && site.isValue() && resultInfo.pkind.contains(KindSelector.TYP)) {
+                    return site.tsym.referenceProjection();
+                } else if (name == names.val && site.isValue() && resultInfo.pkind.contains(KindSelector.TYP)) {
+                    return site.tsym;
                 } else {
                     // We are seeing a plain identifier as selector.
                     Symbol sym = rs.findIdentInType(pos, env, site, name, resultInfo.pkind);
                         sym = rs.accessBase(sym, pos, location, site, name, true);
                     return sym;
                 }
             case WILDCARD:
                 throw new AssertionError(tree);
             case TYPEVAR:
+                if (name == names._default) {
+                    // Be sure to return the default value before examining bounds
+                    return new VarSymbol(STATIC, names._default, site, site.tsym);
+                }
                 // Normally, site.getUpperBound() shouldn't be null.
                 // It should only happen during memberEnter/attribBase
                 // when determining the super type which *must* be
                 // done before attributing the type variables.  In
                 // other words, we are seeing this illegal program:
@@ -4226,15 +4371,17 @@
             case ERROR:
                 // preserve identifier names through errors
                 return types.createErrorType(name, site.tsym, site).tsym;
             default:
                 // The qualifier expression is of a primitive type -- only
-                // .class is allowed for these.
+                // .class and .default is allowed for these.
                 if (name == names._class) {
                     // In this case, we have already made sure in Select that
                     // qualifier expression is a type.
                     return syms.getClassField(site, types);
+                } else if (name == names._default) {
+                    return new VarSymbol(STATIC, names._default, site, site.tsym);
                 } else {
                     log.error(pos, Errors.CantDeref(site));
                     return syms.errSymbol;
                 }
             }
@@ -4842,11 +4989,11 @@
             } else {
                 extending = null;
                 implementing = bounds;
             }
             JCClassDecl cd = make.at(tree).ClassDef(
-                make.Modifiers(PUBLIC | ABSTRACT),
+                make.Modifiers(PUBLIC | ABSTRACT | (extending != null && TreeInfo.symbol(extending).isValue() ? VALUE : 0)),
                 names.empty, List.nil(),
                 extending, implementing, List.nil());
 
             ClassSymbol c = (ClassSymbol)owntype.tsym;
             Assert.check((c.flags() & COMPOUND) != 0);
@@ -4865,11 +5012,11 @@
     public void visitWildcard(JCWildcard tree) {
         //- System.err.println("visitWildcard("+tree+");");//DEBUG
         Type type = (tree.kind.kind == BoundKind.UNBOUND)
             ? syms.objectType
             : attribType(tree.inner, env);
-        result = check(tree, new WildcardType(chk.checkRefType(tree.pos(), type),
+        result = check(tree, new WildcardType(chk.checkRefType(tree.pos(), type, false),
                                               tree.kind.kind,
                                               syms.boundClass),
                 KindSelector.TYP, resultInfo);
     }
 
@@ -4969,10 +5116,17 @@
      */
     public void attribClass(DiagnosticPosition pos, ClassSymbol c) {
         try {
             annotate.flush();
             attribClass(c);
+            if (types.isValue(c.type)) {
+                final Env<AttrContext> env = typeEnvs.get(c);
+                if (!allowValueMemberCycles) {
+                    if (env != null && env.tree != null && env.tree.hasTag(CLASSDEF))
+                        chk.checkNonCyclicMembership((JCClassDecl)env.tree);
+                }
+            }
         } catch (CompletionFailure ex) {
             chk.completionError(pos, ex);
         }
     }
 
@@ -5041,10 +5195,18 @@
                     env.info.isSerializable = true;
                 }
 
                 attribClassBody(env, c);
 
+                if ((c.flags() & (VALUE | ABSTRACT)) == VALUE) { // for non-intersection, concrete values.
+                    Assert.check(env.tree.hasTag(CLASSDEF));
+                    JCClassDecl classDecl = (JCClassDecl) env.tree;
+                    if (classDecl.extending != null) {
+                        chk.checkConstraintsOfInlineSuper(env.tree.pos(), c);
+                    }
+                }
+
                 chk.checkDeprecatedAnnotation(env.tree.pos(), c);
                 chk.checkClassOverrideEqualsAndHashIfNeeded(env.tree.pos(), c);
                 chk.checkFunctionalInterface((JCClassDecl) env.tree, c);
                 chk.checkLeaksNotAccessible(env, (JCClassDecl) env.tree);
             } finally {
@@ -5146,13 +5308,18 @@
         chk.checkImplementations(tree);
 
         //check that a resource implementing AutoCloseable cannot throw InterruptedException
         checkAutoCloseable(tree.pos(), env, c.type);
 
+        boolean hasInstanceFields = false;
         for (List<JCTree> l = tree.defs; l.nonEmpty(); l = l.tail) {
             // Attribute declaration
             attribStat(l.head, env);
+
+            if (l.head.hasTag(VARDEF) && (TreeInfo.flags(l.head) & STATIC) == 0)
+                hasInstanceFields = true;
+
             // Check that declarations in inner classes are not static (JLS 8.1.2)
             // Make an exception for static constants.
             if (c.owner.kind != PCK &&
                 ((c.flags() & STATIC) == 0 || c.name == names.empty) &&
                 (TreeInfo.flags(l.head) & (STATIC | INTERFACE)) != 0) {
@@ -5162,10 +5329,13 @@
                     sym.kind != VAR ||
                     ((VarSymbol) sym).getConstValue() == null)
                     log.error(l.head.pos(), Errors.IclsCantHaveStaticDecl(c));
             }
         }
+        if (!allowEmptyValues && !hasInstanceFields && (c.flags() & (VALUE | SYNTHETIC)) == VALUE) {
+            log.error(tree.pos(), Errors.EmptyValueNotYet);
+        }
 
         // Check for cycles among non-initial constructors.
         chk.checkCyclicConstructors(tree);
 
         // Check for cycles among annotation elements.
diff a/src/jdk.compiler/share/classes/com/sun/tools/javac/comp/Check.java b/src/jdk.compiler/share/classes/com/sun/tools/javac/comp/Check.java
--- a/src/jdk.compiler/share/classes/com/sun/tools/javac/comp/Check.java
+++ b/src/jdk.compiler/share/classes/com/sun/tools/javac/comp/Check.java
@@ -93,10 +93,11 @@
     private final Source source;
     private final Target target;
     private final Profile profile;
     private final Preview preview;
     private final boolean warnOnAnyAccessToMembers;
+    private final boolean allowValueBasedClasses;
 
     // The set of lint options currently in effect. It is initialized
     // from the context, and then is set/reset as needed by Attr as it
     // visits all the various parts of the trees during attribution.
     private Lint lint;
@@ -133,11 +134,11 @@
         fileManager = context.get(JavaFileManager.class);
 
         source = Source.instance(context);
         target = Target.instance(context);
         warnOnAnyAccessToMembers = options.isSet("warnOnAccessToMembers");
-
+        allowValueBasedClasses = options.isSet("allowValueBasedClasses");
         Target target = Target.instance(context);
         syntheticNameChar = target.syntheticNameChar();
 
         profile = Profile.instance(context);
         preview = Preview.instance(context);
@@ -488,11 +489,11 @@
 
     public void removeCompiled(ClassSymbol csym) {
         compiled.remove(Pair.of(csym.packge().modle, csym.flatname));
     }
 
-/* *************************************************************************
+    /* *************************************************************************
  * Type Checking
  **************************************************************************/
 
     /**
      * A check context is an object that can be used to perform compatibility
@@ -594,16 +595,24 @@
     Type checkType(final DiagnosticPosition pos, final Type found, final Type req, final CheckContext checkContext) {
         final InferenceContext inferenceContext = checkContext.inferenceContext();
         if (inferenceContext.free(req) || inferenceContext.free(found)) {
             inferenceContext.addFreeTypeListener(List.of(req, found),
                     solvedContext -> checkType(pos, solvedContext.asInstType(found), solvedContext.asInstType(req), checkContext));
+        } else {
+            if (found.hasTag(CLASS)) {
+                if (inferenceContext != infer.emptyContext)
+                    checkParameterizationWithValues(pos, found);
+            }
         }
         if (req.hasTag(ERROR))
             return req;
         if (req.hasTag(NONE))
             return found;
         if (checkContext.compatible(found, req, checkContext.checkWarner(pos, found, req))) {
+            if (found.hasTag(BOT) && types.isValueBased(req)) {
+                log.warning(pos, Warnings.SuspiciousMixOfNullWithValueBasedClass(req));
+            }
             return found;
         } else {
             if (found.isNumeric() && req.isNumeric()) {
                 checkContext.report(pos, diags.fragment(Fragments.PossibleLossOfPrecision(found, req)));
                 return types.createErrorType(found);
@@ -622,10 +631,17 @@
     Type checkCastable(DiagnosticPosition pos, Type found, Type req) {
         return checkCastable(pos, found, req, basicHandler);
     }
     Type checkCastable(DiagnosticPosition pos, Type found, Type req, CheckContext checkContext) {
         if (types.isCastable(found, req, castWarner(pos, found, req))) {
+            if (types.isValueBased(req)) {
+                if (found.hasTag(BOT)) {
+                    log.warning(pos, Warnings.SuspiciousMixOfNullWithValueBasedClass(req));
+                } else if (!types.isValueBased(found)) {
+                    log.warning(pos, Warnings.PotentialNullPollution(found));
+                }
+            }
             return req;
         } else {
             checkContext.report(pos, diags.fragment(Fragments.InconvertibleTypes(found, req)));
             return types.createErrorType(found);
         }
@@ -724,10 +740,53 @@
             return (t.hasTag(TYPEVAR))
                                     ? diags.fragment(Fragments.TypeParameter(t))
                                     : t;
         }
 
+    void checkConstraintsOfInlineSuper(DiagnosticPosition pos, ClassSymbol c) {
+        for(Type st = types.supertype(c.type); st != Type.noType; st = types.supertype(st)) {
+            if (st == null || st.tsym == null || st.tsym.kind == ERR)
+                return;
+            if  (st.tsym == syms.objectType.tsym)
+                return;
+            if (!st.tsym.isAbstract()) {
+                log.error(pos, Errors.ConcreteSupertypeForInlineClass(c, st));
+            }
+            if ((st.tsym.flags() & HASINITBLOCK) != 0) {
+                log.error(pos, Errors.SuperClassDeclaresInitBlock(c, st));
+            }
+            // No instance fields and no arged constructors both mean inner classes cannot be inline supers.
+            Type encl = st.getEnclosingType();
+            if (encl != null && encl.hasTag(CLASS)) {
+                log.error(pos, Errors.SuperClassCannotBeInner(c, st));
+            }
+            for (Symbol s : st.tsym.members().getSymbols(NON_RECURSIVE)) {
+                switch (s.kind) {
+                case VAR:
+                    if ((s.flags() & STATIC) == 0) {
+                        log.error(pos, Errors.SuperFieldNotAllowed(s, c, st));
+                    }
+                    break;
+                case MTH:
+                    if ((s.flags() & SYNCHRONIZED) != 0) {
+                        log.error(pos, Errors.SuperMethodCannotBeSynchronized(s, c, st));
+                    } else if (s.isConstructor()) {
+                        MethodSymbol m = (MethodSymbol)s;
+                        if (m.getParameters().size() > 0) {
+                            log.error(pos, Errors.SuperConstructorCannotTakeArguments(m, c, st));
+                        } else {
+                            if ((m.flags() & (GENERATEDCONSTR | EMPTYNOARGCONSTR)) == 0) {
+                                log.error(pos, Errors.SuperNoArgConstructorMustBeEmpty(m, c, st));
+                            }
+                        }
+                    }
+                    break;
+                }
+            }
+        }
+    }
+
     /** Check that type is a valid qualifier for a constructor reference expression
      */
     Type checkConstructorRefType(DiagnosticPosition pos, Type t) {
         t = checkClassOrArrayType(pos, t);
         if (t.hasTag(CLASS)) {
@@ -771,29 +830,39 @@
 
     /** Check that type is a reference type, i.e. a class, interface or array type
      *  or a type variable.
      *  @param pos           Position to be used for error reporting.
      *  @param t             The type to be checked.
+     *  @param valueOK       If false, a value class does not qualify
      */
-    Type checkRefType(DiagnosticPosition pos, Type t) {
-        if (t.isReference())
+    Type checkRefType(DiagnosticPosition pos, Type t, boolean valueOK) {
+        if (t.isReference() && (valueOK || !types.isValue(t)))
             return t;
         else
             return typeTagError(pos,
                                 diags.fragment(Fragments.TypeReqRef),
                                 t);
     }
 
+    /** Check that type is a reference type, i.e. a class, interface or array type
+     *  or a type variable.
+     *  @param pos           Position to be used for error reporting.
+     *  @param t             The type to be checked.
+     */
+    Type checkRefType(DiagnosticPosition pos, Type t) {
+        return checkRefType(pos, t, true);
+    }
+
     /** Check that each type is a reference type, i.e. a class, interface or array type
      *  or a type variable.
      *  @param trees         Original trees, used for error reporting.
      *  @param types         The types to be checked.
      */
     List<Type> checkRefTypes(List<JCExpression> trees, List<Type> types) {
         List<JCExpression> tl = trees;
         for (List<Type> l = types; l.nonEmpty(); l = l.tail) {
-            l.head = checkRefType(tl.head.pos(), l.head);
+            l.head = checkRefType(tl.head.pos(), l.head, false);
             tl = tl.tail;
         }
         return types;
     }
 
@@ -825,10 +894,58 @@
             return false;
         } else
             return true;
     }
 
+    void checkParameterizationWithValues(DiagnosticPosition pos, Type t) {
+        valueParameterizationChecker.visit(t, pos);
+    }
+
+    /** valueParameterizationChecker: A type visitor that descends down the given type looking for instances of value types
+     *  being used as type arguments and issues error against those usages.
+     */
+    private final Types.SimpleVisitor<Void, DiagnosticPosition> valueParameterizationChecker = new Types.SimpleVisitor<Void, DiagnosticPosition>() {
+
+        @Override
+        public Void visitType(Type t, DiagnosticPosition pos) {
+            return null;
+        }
+
+        @Override
+        public Void visitClassType(ClassType t, DiagnosticPosition pos) {
+            for (Type targ : t.allparams()) {
+                if (types.isValue(targ)) {
+                    log.error(pos, Errors.GenericParameterizationWithValueType(t));
+                }
+                visit(targ, pos);
+            }
+            return null;
+        }
+
+        @Override
+        public Void visitTypeVar(TypeVar t, DiagnosticPosition pos) {
+             return null;
+        }
+
+        @Override
+        public Void visitCapturedType(CapturedType t, DiagnosticPosition pos) {
+            return null;
+        }
+
+        @Override
+        public Void visitArrayType(ArrayType t, DiagnosticPosition pos) {
+            return visit(t.elemtype, pos);
+        }
+
+        @Override
+        public Void visitWildcardType(WildcardType t, DiagnosticPosition pos) {
+            return visit(t.type, pos);
+        }
+    };
+
+
+
     /** Check that usage of diamond operator is correct (i.e. diamond should not
      * be used with non-generic classes or in anonymous class creation expressions)
      */
     Type checkDiamond(JCNewClass tree, Type t) {
         if (!TreeInfo.isDiamond(tree) ||
@@ -973,11 +1090,48 @@
             log.error(pos, Errors.CantInferLocalVarType(name, Fragments.LocalCantInferVoid));
             return types.createErrorType(t);
         }
 
         //upward project the initializer type
-        return types.upward(t, types.captures(t));
+        Type varType = types.upward(t, types.captures(t));
+        if (varType.hasTag(CLASS)) {
+            checkParameterizationWithValues(pos, varType);
+        }
+        return varType;
+    }
+
+    public void checkForSuspectClassLiteralComparison(
+            final JCBinary tree,
+            final Type leftType,
+            final Type rightType) {
+
+        if (lint.isEnabled(LintCategory.MIGRATION)) {
+            if (isInvocationOfGetClass(tree.lhs) && isClassOfSomeInterface(rightType) ||
+                    isInvocationOfGetClass(tree.rhs) && isClassOfSomeInterface(leftType)) {
+                log.warning(LintCategory.MIGRATION, tree.pos(), Warnings.GetClassComparedWithInterface);
+            }
+        }
+    }
+    //where
+    private boolean isClassOfSomeInterface(Type someClass) {
+        if (someClass.tsym.flatName() == names.java_lang_Class) {
+            List<Type> arguments = someClass.getTypeArguments();
+            if (arguments.length() == 1) {
+                return arguments.head.isInterface();
+            }
+        }
+        return false;
+    }
+    //where
+    private boolean isInvocationOfGetClass(JCExpression tree) {
+        tree = TreeInfo.skipParens(tree);
+        if (tree.hasTag(APPLY)) {
+            JCMethodInvocation apply = (JCMethodInvocation)tree;
+            MethodSymbol msym = (MethodSymbol)TreeInfo.symbol(apply.meth);
+            return msym.name == names.getClass && msym.implementedIn(syms.objectType.tsym, types) != null;
+        }
+        return false;
     }
 
     Type checkMethod(final Type mtype,
             final Symbol sym,
             final Env<AttrContext> env,
@@ -1171,12 +1325,16 @@
                 mask = ReceiverParamFlags;
             else if (sym.owner.kind != TYP)
                 mask = LocalVarFlags;
             else if ((sym.owner.flags_field & INTERFACE) != 0)
                 mask = implicit = InterfaceVarFlags;
-            else
+            else {
                 mask = VarFlags;
+                if (types.isValue(sym.owner.type) && (flags & STATIC) == 0) {
+                    implicit |= FINAL;
+                }
+            }
             break;
         case MTH:
             if (sym.name == names.init) {
                 if ((sym.owner.flags_field & ENUM) != 0) {
                     // enum constructors cannot be declared public or
@@ -1200,11 +1358,13 @@
                     mask = implicit = InterfaceMethodFlags;
                 }
             } else if ((sym.owner.flags_field & RECORD) != 0) {
                 mask = RecordMethodFlags;
             } else {
-                mask = MethodFlags;
+                // instance methods of value types do not have a monitor associated with their `this'
+                mask = ((sym.owner.flags_field & VALUE) != 0 && (flags & Flags.STATIC) == 0) ?
+                        MethodFlags & ~SYNCHRONIZED : MethodFlags;
             }
             // Imply STRICTFP if owner has STRICTFP set.
             if (((flags|implicit) & Flags.ABSTRACT) == 0 ||
                 ((flags) & Flags.DEFAULT) != 0)
                 implicit |= sym.owner.flags_field & STRICTFP;
@@ -1239,12 +1399,12 @@
             }
             // Interfaces are always ABSTRACT
             if ((flags & INTERFACE) != 0) implicit |= ABSTRACT;
 
             if ((flags & ENUM) != 0) {
-                // enums can't be declared abstract or final
-                mask &= ~(ABSTRACT | FINAL);
+                // enums can't be declared abstract or final or value type
+                mask &= ~(ABSTRACT | FINAL | VALUE);
                 implicit |= implicitEnumFinalFlag(tree);
             }
             if ((flags & RECORD) != 0) {
                 // records can't be declared abstract
                 mask &= ~ABSTRACT;
@@ -1278,21 +1438,21 @@
                                 STATIC | PRIVATE,
                                 DEFAULT)
                  &&
                  checkDisjoint(pos, flags,
                                ABSTRACT | INTERFACE,
-                               FINAL | NATIVE | SYNCHRONIZED)
+                               FINAL | NATIVE | SYNCHRONIZED | VALUE)
                  &&
                  checkDisjoint(pos, flags,
                                PUBLIC,
                                PRIVATE | PROTECTED)
                  &&
                  checkDisjoint(pos, flags,
                                PRIVATE,
                                PUBLIC | PROTECTED)
                  &&
-                 checkDisjoint(pos, flags,
+                 checkDisjoint(pos, (flags | implicit), // complain against volatile & implcitly final entities too.
                                FINAL,
                                VOLATILE)
                  &&
                  (sym.kind == TYP ||
                   checkDisjoint(pos, flags,
@@ -1456,14 +1616,17 @@
             }
         }
 
         public void visitSelectInternal(JCFieldAccess tree) {
             if (tree.type.tsym.isStatic() &&
-                tree.selected.type.isParameterized()) {
+                tree.selected.type.isParameterized() &&
+                    (tree.name != names.ref || !tree.type.isReferenceProjection())) {
                 // The enclosing type is not a class, so we are
                 // looking at a static member type.  However, the
                 // qualifying expression is parameterized.
+                // Tolerate the pseudo-select V.ref: V<T>.ref will be static if V<T> is and
+                // should not be confused as selecting a static member of a parameterized type.
                 log.error(tree.pos(), Errors.CantSelectStaticClassFromParamType);
             } else {
                 // otherwise validate the rest of the expression
                 tree.selected.accept(this);
             }
@@ -1786,10 +1949,19 @@
                                                           asFlagSet(other.flags() & AccessFlags)));
             m.flags_field |= BAD_OVERRIDE;
             return;
         }
 
+        if (origin.isValue() && other.owner == syms.objectType.tsym && m.type.getParameterTypes().size() == 0) {
+            if (m.name == names.clone || m.name == names.finalize) {
+                log.error(TreeInfo.diagnosticPositionFor(m, tree),
+                        Errors.InlineClassMayNotOverride(m.name));
+                m.flags_field |= BAD_OVERRIDE;
+                return;
+            }
+        }
+
         Type mt = types.memberType(origin.type, m);
         Type ot = types.memberType(origin.type, other);
         // Error if overriding result type is different
         // (or, in the case of generics mode, not a subtype) of
         // overridden result type. We have to rename any type parameters
@@ -2088,11 +2260,12 @@
         final boolean explicitOverride = m.attribute(syms.overrideType.tsym) != null;
         // Check if this method must override a super method due to being annotated with @Override
         // or by virtue of being a member of a diamond inferred anonymous class. Latter case is to
         // be treated "as if as they were annotated" with @Override.
         boolean mustOverride = explicitOverride ||
-                (env.info.isAnonymousDiamond && !m.isConstructor() && !m.isPrivate());
+                (env.info.isAnonymousDiamond && !m.isConstructor() && !m.isPrivate() &&
+                        (!m.owner.isValue() || (tree.body.flags & SYNTHETIC) == 0));
         if (mustOverride && !isOverrider(m)) {
             DiagnosticPosition pos = tree.pos();
             for (JCAnnotation a : tree.getModifiers().annotations) {
                 if (a.annotationType.type.tsym == syms.overrideType.tsym) {
                     pos = a.pos();
@@ -2214,10 +2387,49 @@
             log.error(pos,
                       Errors.DoesNotOverrideAbstract(c, undef1, undef1.location()));
         }
     }
 
+    // A value class cannot contain a field of its own type either or indirectly.
+    void checkNonCyclicMembership(JCClassDecl tree) {
+        Assert.check((tree.sym.flags_field & LOCKED) == 0);
+        try {
+            tree.sym.flags_field |= LOCKED;
+            for (List<? extends JCTree> l = tree.defs; l.nonEmpty(); l = l.tail) {
+                if (l.head.hasTag(VARDEF)) {
+                    JCVariableDecl field = (JCVariableDecl) l.head;
+                    if (cyclePossible(field.sym)) {
+                        Type fieldType = field.sym.type;
+                        checkNonCyclicMembership((ClassSymbol) fieldType.tsym, field.pos());
+                    }
+                }
+            }
+        } finally {
+            tree.sym.flags_field &= ~LOCKED;
+        }
+
+    }
+    // where
+    private void checkNonCyclicMembership(ClassSymbol c, DiagnosticPosition pos) {
+        if ((c.flags_field & LOCKED) != 0) {
+            log.error(pos, Errors.CyclicValueTypeMembership(c));
+            return;
+        }
+        try {
+            c.flags_field |= LOCKED;
+            for (Symbol fld : c.members().getSymbols(s -> s.kind == VAR && cyclePossible((VarSymbol) s), NON_RECURSIVE)) {
+                checkNonCyclicMembership((ClassSymbol) fld.type.tsym, pos);
+            }
+        } finally {
+            c.flags_field &= ~LOCKED;
+        }
+    }
+        // where
+        private boolean cyclePossible(VarSymbol symbol) {
+            return (symbol.flags() & STATIC) == 0 && types.isValue(symbol.type);
+        }
+
     void checkNonCyclicDecl(JCClassDecl tree) {
         CycleChecker cc = new CycleChecker();
         cc.scan(tree);
         if (!cc.errorFound && !cc.partialCheck) {
             tree.sym.flags_field |= ACYCLIC;
@@ -2462,10 +2674,14 @@
             for (List<Type> m = supertypes; m != l; m = m.tail)
                 if (!checkCompatibleAbstracts(pos, l.head, m.head, c))
                     return;
         }
         checkCompatibleConcretes(pos, c);
+
+        if (c.isValue() && types.asSuper(c, syms.identityObjectType.tsym, true) != null) {
+            log.error(pos, Errors.InlineTypeMustNotImplementIdentityObject(c));
+        }
     }
 
     /** Check that all non-override equivalent methods accessible from 'site'
      *  are mutually compatible (JLS 8.4.8/9.4.1).
      *
@@ -3021,10 +3237,17 @@
                 log.error(a.pos(), Errors.BadFunctionalIntfAnno);
             } else if (!s.isInterface() || (s.flags() & ANNOTATION) != 0) {
                 log.error(a.pos(), Errors.BadFunctionalIntfAnno1(Fragments.NotAFunctionalIntf(s)));
             }
         }
+        if (a.annotationType.type.tsym == syms.valueBasedType.tsym) {
+            if (s.isInterface() || s.isEnum()) {
+                log.error(a.pos(), Errors.BadValueBasedAnno);
+            } else if (allowValueBasedClasses) {
+                s.flags_field |= VALUEBASED;
+            }
+        }
     }
 
     public void validateTypeAnnotation(JCAnnotation a, boolean isTypeParameter) {
         Assert.checkNonNull(a.type);
         validateAnnotationTree(a);
diff a/src/jdk.compiler/share/classes/com/sun/tools/javac/comp/Flow.java b/src/jdk.compiler/share/classes/com/sun/tools/javac/comp/Flow.java
--- a/src/jdk.compiler/share/classes/com/sun/tools/javac/comp/Flow.java
+++ b/src/jdk.compiler/share/classes/com/sun/tools/javac/comp/Flow.java
@@ -50,10 +50,13 @@
 import static com.sun.tools.javac.code.Flags.*;
 import static com.sun.tools.javac.code.Flags.BLOCK;
 import static com.sun.tools.javac.code.Kinds.Kind.*;
 import static com.sun.tools.javac.code.TypeTag.BOOLEAN;
 import static com.sun.tools.javac.code.TypeTag.VOID;
+import static com.sun.tools.javac.comp.Flow.ThisExposability.ALLOWED;
+import static com.sun.tools.javac.comp.Flow.ThisExposability.BANNED;
+import static com.sun.tools.javac.comp.Flow.ThisExposability.DISCOURAGED;
 import static com.sun.tools.javac.tree.JCTree.Tag.*;
 
 /** This pass implements dataflow analysis for Java programs though
  *  different AST visitor steps. Liveness analysis (see AliveAnalyzer) checks that
  *  every statement is reachable. Exception analysis (see FlowAnalyzer) ensures that
@@ -1229,11 +1232,11 @@
             for (JCTree resource : tree.resources) {
                 List<Type> closeableSupertypes = resource.type.isCompound() ?
                     types.interfaces(resource.type).prepend(types.supertype(resource.type)) :
                     List.of(resource.type);
                 for (Type sup : closeableSupertypes) {
-                    if (types.asSuper(sup, syms.autoCloseableType.tsym) != null) {
+                    if (types.asSuper(sup, syms.autoCloseableType.tsym, true) != null) {
                         Symbol closeMethod = rs.resolveQualifiedMethod(tree,
                                 attrEnv,
                                 types.skipTypeVars(sup, false),
                                 names.close,
                                 List.nil(),
@@ -1647,10 +1650,19 @@
         public void visitClassDef(JCClassDecl tree) {
             //skip
         }
     }
 
+    /** Enum to model whether constructors allowed to "leak" this reference before
+        all instance fields are DA.
+     */
+    enum ThisExposability {
+        ALLOWED,     // Normal Object classes - NOP
+        BANNED,      // Value types           - Error
+        DISCOURAGED  // Value based types     - Warning
+    }
+
     /**
      * This pass implements (i) definite assignment analysis, which ensures that
      * each variable is assigned when used and (ii) definite unassignment analysis,
      * which ensures that no final variable is assigned more than once. This visitor
      * depends on the results of the liveliness analyzer. This pass is also used to mark
@@ -1735,10 +1747,13 @@
                 inits.andSet(exit_inits);
                 uninits.andSet(exit_uninits);
             }
         }
 
+        // Are constructors allowed to leak this reference ?
+        ThisExposability thisExposability = ALLOWED;
+
         public AssignAnalyzer() {
             this.inits = new Bits();
             uninits = new Bits();
             uninitsTry = new Bits();
             initsWhenTrue = new Bits(true);
@@ -1860,10 +1875,34 @@
                     letInit(tree.pos(), (VarSymbol)sym);
                 }
             }
         }
 
+        void checkEmbryonicThisExposure(JCTree node) {
+            if (this.thisExposability == ALLOWED || classDef == null)
+                return;
+
+            // Note: for non-initial constructors, firstadr is post all instance fields.
+            for (int i = firstadr; i < nextadr; i++) {
+                VarSymbol sym = vardecls[i].sym;
+                if (sym.owner != classDef.sym)
+                    continue;
+                if ((sym.flags() & (FINAL | HASINIT | STATIC | PARAMETER)) != FINAL)
+                    continue;
+                if (sym.pos < startPos || sym.adr < firstadr)
+                    continue;
+                if (!inits.isMember(sym.adr)) {
+                    if (this.thisExposability == BANNED) {
+                        log.error(node, Errors.ThisExposedPrematurely);
+                    } else {
+                        log.warning(node, Warnings.ThisExposedPrematurely);
+                    }
+                    return; // don't flog a dead horse.
+                }
+            }
+        }
+
         /** Check that trackable variable is initialized.
          */
         void checkInit(DiagnosticPosition pos, VarSymbol sym) {
             checkInit(pos, sym, Errors.VarMightNotHaveBeenInitialized(sym));
         }
@@ -2055,10 +2094,11 @@
                 return;
             }
 
             Lint lintPrev = lint;
             lint = lint.augment(tree.sym);
+            ThisExposability priorThisExposability = this.thisExposability;
             try {
                 if (tree.body == null) {
                     return;
                 }
                 /*  Ignore synthetic methods, except for translated lambda methods.
@@ -2078,10 +2118,18 @@
                 try {
                     isInitialConstructor = TreeInfo.isInitialConstructor(tree);
 
                     if (!isInitialConstructor) {
                         firstadr = nextadr;
+                        this.thisExposability = ALLOWED;
+                    } else {
+                        if (types.isValueBased(tree.sym.owner.type))
+                            this.thisExposability = DISCOURAGED;
+                        else if (types.isValue(tree.sym.owner.type))
+                            this.thisExposability = BANNED;
+                        else
+                            this.thisExposability = ALLOWED;
                     }
                     for (List<JCVariableDecl> l = tree.params; l.nonEmpty(); l = l.tail) {
                         JCVariableDecl def = l.head;
                         scan(def);
                         Assert.check((def.sym.flags() & PARAMETER) != 0, "Method parameter without PARAMETER flag");
@@ -2140,10 +2188,11 @@
                     returnadr = returnadrPrev;
                     isInitialConstructor = lastInitialConstructor;
                 }
             } finally {
                 lint = lintPrev;
+                this.thisExposability = priorThisExposability;
             }
         }
 
         private void clearPendingExits(boolean inMethod) {
             List<PendingExit> exits = pendingExits.toList();
@@ -2615,16 +2664,27 @@
         }
 
         public void visitApply(JCMethodInvocation tree) {
             scanExpr(tree.meth);
             scanExprs(tree.args);
+            if (tree.meth.hasTag(IDENT)) {
+                JCIdent ident = (JCIdent) tree.meth;
+                if (ident.name != names._super && !ident.sym.isStatic())
+                    checkEmbryonicThisExposure(tree);
+            }
         }
 
         public void visitNewClass(JCNewClass tree) {
             scanExpr(tree.encl);
             scanExprs(tree.args);
             scan(tree.def);
+            if (classDef != null && tree.encl == null && tree.clazz.hasTag(IDENT)) {
+                JCIdent clazz = (JCIdent) tree.clazz;
+                if (!clazz.sym.isStatic() && clazz.type.getEnclosingType().tsym == classDef.sym) {
+                    checkEmbryonicThisExposure(tree);
+                }
+            }
         }
 
         @Override
         public void visitLambda(JCLambda tree) {
             final Bits prevUninits = new Bits(uninits);
@@ -2683,14 +2743,24 @@
         }
 
         // check fields accessed through this.<field> are definitely
         // assigned before reading their value
         public void visitSelect(JCFieldAccess tree) {
-            super.visitSelect(tree);
+            ThisExposability priorThisExposability = this.thisExposability;
+            try {
+                if (tree.name == names._this && classDef != null && tree.sym.owner == classDef.sym) {
+                    checkEmbryonicThisExposure(tree);
+                } else if (tree.sym.kind == VAR || tree.sym.isStatic()) {
+                    this.thisExposability = ALLOWED;
+                }
+                super.visitSelect(tree);
             if (TreeInfo.isThisQualifier(tree.selected) &&
                 tree.sym.kind == VAR) {
-                checkInit(tree.pos(), (VarSymbol)tree.sym);
+                    checkInit(tree.pos(), (VarSymbol)tree.sym);
+                }
+            } finally {
+                 this.thisExposability = priorThisExposability;
             }
         }
 
         public void visitAssignop(JCAssignOp tree) {
             scanExpr(tree.lhs);
@@ -2750,10 +2820,13 @@
         public void visitIdent(JCIdent tree) {
             if (tree.sym.kind == VAR) {
                 checkInit(tree.pos(), (VarSymbol)tree.sym);
                 referenced(tree.sym);
             }
+            if (tree.name == names._this) {
+                checkEmbryonicThisExposure(tree);
+            }
         }
 
         void referenced(Symbol sym) {
             unrefdResources.remove(sym);
         }
diff a/src/jdk.compiler/share/classes/com/sun/tools/javac/comp/LambdaToMethod.java b/src/jdk.compiler/share/classes/com/sun/tools/javac/comp/LambdaToMethod.java
--- a/src/jdk.compiler/share/classes/com/sun/tools/javac/comp/LambdaToMethod.java
+++ b/src/jdk.compiler/share/classes/com/sun/tools/javac/comp/LambdaToMethod.java
@@ -2254,10 +2254,19 @@
                 //when 292 issue is fixed we should remove this and change the backend
                 //code to always generate a method handle to an accessible method
                 return tree.ownerAccessible;
             }
 
+            /* Per our interim inline class translation scheme, the reference projection classes
+               are completely empty, so we want the methods in the value class to be invoked instead.
+               As the lambda meta factory isn't clued into this, it will try to invoke the method in
+               C$ref.class and fail with a NoSuchMethodError. we need to workaround it ourselves.
+            */
+            boolean receiverIsReferenceProjection() {
+                return tree.sym.kind == MTH && tree.sym.owner.isReferenceProjection();
+            }
+
             /**
              * This method should be called only when target release <= 14
              * where LambdaMetaFactory does not spin nestmate classes.
              *
              * This method should be removed when --release 14 is not supported.
@@ -2311,13 +2320,14 @@
                         needsVarArgsConversion() ||
                         isArrayOp() ||
                         (!nestmateLambdas && isPrivateInOtherClass()) ||
                         isProtectedInSuperClassOfEnclosingClassInOtherPackage() ||
                         !receiverAccessible() ||
+                        receiverIsReferenceProjection() ||
                         (tree.getMode() == ReferenceMode.NEW &&
                           tree.kind != ReferenceKind.ARRAY_CTOR &&
-                          (tree.sym.owner.isLocal() || tree.sym.owner.isInner()));
+                          (tree.sym.owner.isLocal() || tree.sym.owner.isInner() || tree.sym.owner.isValue()));
             }
 
             Type generatedRefSig() {
                 return types.erasure(tree.sym.type);
             }
diff a/src/jdk.compiler/share/classes/com/sun/tools/javac/jvm/Target.java b/src/jdk.compiler/share/classes/com/sun/tools/javac/jvm/Target.java
--- a/src/jdk.compiler/share/classes/com/sun/tools/javac/jvm/Target.java
+++ b/src/jdk.compiler/share/classes/com/sun/tools/javac/jvm/Target.java
@@ -180,7 +180,6 @@
     /** Does the target VM support virtual private invocations?
      */
     public boolean hasVirtualPrivateInvoke() {
         return compareTo(JDK1_11) >= 0;
     }
-
 }
diff a/src/jdk.hotspot.agent/share/classes/sun/jvm/hotspot/tools/jcore/ClassWriter.java b/src/jdk.hotspot.agent/share/classes/sun/jvm/hotspot/tools/jcore/ClassWriter.java
--- a/src/jdk.hotspot.agent/share/classes/sun/jvm/hotspot/tools/jcore/ClassWriter.java
+++ b/src/jdk.hotspot.agent/share/classes/sun/jvm/hotspot/tools/jcore/ClassWriter.java
@@ -358,12 +358,14 @@
         // write interfaces count
         dos.writeShort((short) len);
         for (int i = 0; i < len; i++) {
            Klass k = interfaces.getAt(i);
            Short index = (Short) classToIndex.get(k.getName().asString());
-           dos.writeShort(index.shortValue());
-           if (DEBUG) debugMessage("\t" + index);
+           if (index != null) { // Injected interfaces have no constant pool entry
+               dos.writeShort(index.shortValue());
+               if (DEBUG) debugMessage("\t" + index);
+           }
         }
     }
 
     protected void writeFields() throws IOException {
         final int javaFieldsCount = klass.getJavaFieldsCount();
diff a/src/jdk.jdi/share/classes/com/sun/tools/jdi/JNITypeParser.java b/src/jdk.jdi/share/classes/com/sun/tools/jdi/JNITypeParser.java
--- a/src/jdk.jdi/share/classes/com/sun/tools/jdi/JNITypeParser.java
+++ b/src/jdk.jdi/share/classes/com/sun/tools/jdi/JNITypeParser.java
@@ -161,10 +161,11 @@
         switch(key) {
             case (JDWP.Tag.ARRAY):
                 return  key + nextSignature();
 
             case (JDWP.Tag.OBJECT):
+            case (JDWP.Tag.INLINE_OBJECT):
                 int endClass = signature.indexOf(SIGNATURE_ENDCLASS,
                                                  currentIndex);
                 String retVal = signature.substring(currentIndex - 1,
                                                     endClass + 1);
                 currentIndex = endClass + 1;
@@ -204,10 +205,11 @@
 
             case (JDWP.Tag.CHAR):
                 return "char";
 
             case (JDWP.Tag.OBJECT):
+            case (JDWP.Tag.INLINE_OBJECT):
                 int endClass = signature.indexOf(SIGNATURE_ENDCLASS,
                                                  currentIndex);
                 String retVal = signature.substring(currentIndex,
                                                     endClass);
                 int index = retVal.indexOf(".");
diff a/src/jdk.jdwp.agent/share/native/libjdwp/util.c b/src/jdk.jdwp.agent/share/native/libjdwp/util.c
--- a/src/jdk.jdwp.agent/share/native/libjdwp/util.c
+++ b/src/jdk.jdwp.agent/share/native/libjdwp/util.c
@@ -290,10 +290,11 @@
 }
 
 jboolean
 isObjectTag(jbyte tag) {
     return (tag == JDWP_TAG(OBJECT)) ||
+           (tag == JDWP_TAG(INLINE_OBJECT)) ||
            (tag == JDWP_TAG(STRING)) ||
            (tag == JDWP_TAG(THREAD)) ||
            (tag == JDWP_TAG(THREAD_GROUP)) ||
            (tag == JDWP_TAG(CLASS_LOADER)) ||
            (tag == JDWP_TAG(CLASS_OBJECT)) ||
@@ -348,17 +349,18 @@
 
     /*
      * For primitive types, the type key is bounced back as is. Objects
      * are handled in the switch statement below.
      */
-    if ((typeKey != JDWP_TAG(OBJECT)) && (typeKey != JDWP_TAG(ARRAY))) {
+    if ((typeKey != JDWP_TAG(OBJECT)) && (typeKey != JDWP_TAG(ARRAY)) && (typeKey != JDWP_TAG(INLINE_OBJECT))) {
         (void)outStream_writeByte(out, typeKey);
     }
 
     switch (typeKey) {
         case JDWP_TAG(OBJECT):
-        case JDWP_TAG(ARRAY):   {
+        case JDWP_TAG(ARRAY):
+        case JDWP_TAG(INLINE_OBJECT): {
             jobject value = JNI_FUNC_PTR(env,GetObjectField)(env, object, field);
             (void)outStream_writeByte(out, specificTypeKey(env, value));
             (void)outStream_writeObjectRef(env, out, value);
             break;
         }
@@ -423,17 +425,18 @@
 
     /*
      * For primitive types, the type key is bounced back as is. Objects
      * are handled in the switch statement below.
      */
-    if ((typeKey != JDWP_TAG(OBJECT)) && (typeKey != JDWP_TAG(ARRAY))) {
+    if ((typeKey != JDWP_TAG(OBJECT)) && (typeKey != JDWP_TAG(ARRAY)) && (typeKey != JDWP_TAG(INLINE_OBJECT))) {
         (void)outStream_writeByte(out, typeKey);
     }
 
     switch (typeKey) {
         case JDWP_TAG(OBJECT):
-        case JDWP_TAG(ARRAY):   {
+        case JDWP_TAG(ARRAY):
+        case JDWP_TAG(INLINE_OBJECT): {
             jobject value = JNI_FUNC_PTR(env,GetStaticObjectField)(env, clazz, field);
             (void)outStream_writeByte(out, specificTypeKey(env, value));
             (void)outStream_writeObjectRef(env, out, value);
             break;
         }
diff a/src/jdk.unsupported/share/classes/sun/misc/Unsafe.java b/src/jdk.unsupported/share/classes/sun/misc/Unsafe.java
--- a/src/jdk.unsupported/share/classes/sun/misc/Unsafe.java
+++ b/src/jdk.unsupported/share/classes/sun/misc/Unsafe.java
@@ -641,10 +641,13 @@
             throw new NullPointerException();
         }
         if (f.getDeclaringClass().isHidden()) {
             throw new UnsupportedOperationException("can't get field offset on a hidden class: " + f);
         }
+        if (f.getDeclaringClass().isInlineClass()) {
+            throw new UnsupportedOperationException("can't get field offset on an inline class: " + f);
+        }
         return theInternalUnsafe.objectFieldOffset(f);
     }
 
     /**
      * Reports the location of a given static field, in conjunction with {@link
@@ -669,10 +672,13 @@
             throw new NullPointerException();
         }
         if (f.getDeclaringClass().isHidden()) {
             throw new UnsupportedOperationException("can't get field offset on a hidden class: " + f);
         }
+        if (f.getDeclaringClass().isInlineClass()) {
+            throw new UnsupportedOperationException("can't get static field offset on an inline class: " + f);
+        }
         return theInternalUnsafe.staticFieldOffset(f);
     }
 
     /**
      * Reports the location of a given static field, in conjunction with {@link
@@ -690,10 +696,13 @@
             throw new NullPointerException();
         }
         if (f.getDeclaringClass().isHidden()) {
             throw new UnsupportedOperationException("can't get base address on a hidden class: " + f);
         }
+        if (f.getDeclaringClass().isInlineClass()) {
+            throw new UnsupportedOperationException("can't get base address on an inline class: " + f);
+        }
         return theInternalUnsafe.staticFieldBase(f);
     }
 
     /**
      * Detects if the given class may need to be initialized. This is often
diff a/test/hotspot/jtreg/ProblemList.txt b/test/hotspot/jtreg/ProblemList.txt
--- a/test/hotspot/jtreg/ProblemList.txt
+++ b/test/hotspot/jtreg/ProblemList.txt
@@ -69,10 +69,87 @@
 compiler/rtm/locking/TestRTMSpinLoopCount.java 8183263 generic-x64
 compiler/rtm/locking/TestUseRTMDeopt.java 8183263 generic-x64
 compiler/rtm/locking/TestUseRTMXendForLockBusy.java 8183263 generic-x64
 compiler/rtm/print/TestPrintPreciseRTMLockingStatistics.java 8183263 generic-x64
 
+# Valhalla
+compiler/arguments/CheckCICompilerCount.java                        8205030 generic-all
+compiler/arguments/CheckCompileThresholdScaling.java                8205030 generic-all
+compiler/codecache/CheckSegmentedCodeCache.java                     8205030 generic-all
+compiler/codecache/cli/TestSegmentedCodeCacheOption.java            8205030 generic-all
+compiler/codecache/cli/codeheapsize/TestCodeHeapSizeOptions.java    8205030 generic-all
+compiler/codecache/cli/printcodecache/TestPrintCodeCacheOption.java 8205030 generic-all
+compiler/whitebox/OSRFailureLevel4Test.java                         8205030 generic-all
+
+compiler/aot/cli/DisabledAOTWithLibraryTest.java 8226295 generic-all
+compiler/aot/cli/SingleAOTOptionTest.java 8226295 generic-all
+compiler/aot/cli/MultipleAOTLibraryTest.java 8226295 generic-all
+compiler/aot/cli/jaotc/CompileClassWithDebugTest.java 8226295 generic-all
+compiler/aot/cli/jaotc/CompileModuleTest.java 8226295 generic-all
+compiler/aot/cli/jaotc/AtFileTest.java 8226295 generic-all
+compiler/aot/cli/jaotc/ListOptionWrongFileTest.java 8226295 generic-all
+compiler/aot/cli/jaotc/ClasspathOptionUnknownClassTest.java 8226295 generic-all
+compiler/aot/cli/jaotc/CompileDirectoryTest.java 8226295 generic-all
+compiler/aot/cli/jaotc/ListOptionTest.java 8226295 generic-all
+compiler/aot/cli/jaotc/ListOptionNotExistingTest.java 8226295 generic-all
+compiler/aot/cli/jaotc/CompileClassTest.java 8226295 generic-all
+compiler/aot/cli/jaotc/CompileJarTest.java 8226295 generic-all
+compiler/aot/cli/jaotc/IgnoreErrorsTest.java 8226295 generic-all
+compiler/aot/cli/jaotc/CompileAbsoluteDirectoryTest.java 8226295 generic-all
+compiler/aot/cli/NonExistingAOTLibraryTest.java 8226295 generic-all
+compiler/aot/cli/SingleAOTLibraryTest.java 8226295 generic-all
+compiler/aot/cli/IncorrectAOTLibraryTest.java 8226295 generic-all
+compiler/aot/RecompilationTest.java 8226295 generic-all
+compiler/aot/SharedUsageTest.java 8226295 generic-all
+compiler/aot/jdk.tools.jaotc.test/src/jdk/tools/jaotc/test/collect/ClassSearchTest.java 8226295 generic-all
+compiler/aot/jdk.tools.jaotc.test/src/jdk/tools/jaotc/test/collect/SearchPathTest.java 8226295 generic-all
+compiler/aot/jdk.tools.jaotc.test/src/jdk/tools/jaotc/test/collect/module/ModuleSourceProviderTest.java 8226295 generic-all
+compiler/aot/jdk.tools.jaotc.test/src/jdk/tools/jaotc/test/collect/ClassSourceTest.java 8226295 generic-all
+compiler/aot/jdk.tools.jaotc.test/src/jdk/tools/jaotc/test/collect/directory/DirectorySourceProviderTest.java 8226295 generic-all
+compiler/aot/jdk.tools.jaotc.test/src/jdk/tools/jaotc/test/collect/jar/JarSourceProviderTest.java 8226295 generic-all
+compiler/aot/jdk.tools.jaotc.test/src/jdk/tools/jaotc/test/NativeOrderOutputStreamTest.java 8226295 generic-all
+compiler/aot/verification/vmflags/TrackedFlagTest.java 8226295 generic-all
+compiler/aot/verification/vmflags/NotTrackedFlagTest.java 8226295 generic-all
+compiler/aot/verification/ClassAndLibraryNotMatchTest.java 8226295 generic-all
+compiler/aot/DeoptimizationTest.java 8226295 generic-all
+compiler/aot/calls/fromAot/AotInvokeVirtual2NativeTest.java 8226295 generic-all
+compiler/aot/calls/fromAot/AotInvokeStatic2CompiledTest.java 8226295 generic-all
+compiler/aot/calls/fromAot/AotInvokeSpecial2CompiledTest.java 8226295 generic-all
+compiler/aot/calls/fromAot/AotInvokeVirtual2CompiledTest.java 8226295 generic-all
+compiler/aot/calls/fromAot/AotInvokeVirtual2InterpretedTest.java 8226295 generic-all
+compiler/aot/calls/fromAot/AotInvokeDynamic2NativeTest.java 8226295 generic-all
+compiler/aot/calls/fromAot/AotInvokeDynamic2InterpretedTest.java 8226295 generic-all
+compiler/aot/calls/fromAot/AotInvokeStatic2InterpretedTest.java 8226295 generic-all
+compiler/aot/calls/fromAot/AotInvokeInterface2InterpretedTest.java 8226295 generic-all
+compiler/aot/calls/fromAot/AotInvokeVirtual2AotTest.java 8226295 generic-all
+compiler/aot/calls/fromAot/AotInvokeInterface2AotTest.java 8226295 generic-all
+compiler/aot/calls/fromAot/AotInvokeSpecial2NativeTest.java 8226295 generic-all
+compiler/aot/calls/fromAot/AotInvokeDynamic2AotTest.java 8226295 generic-all
+compiler/aot/calls/fromAot/AotInvokeInterface2NativeTest.java 8226295 generic-all
+compiler/aot/calls/fromAot/AotInvokeStatic2AotTest.java 8226295 generic-all
+compiler/aot/calls/fromAot/AotInvokeInterface2CompiledTest.java 8226295 generic-all
+compiler/aot/calls/fromAot/AotInvokeDynamic2CompiledTest.java 8226295 generic-all
+compiler/aot/calls/fromAot/AotInvokeStatic2NativeTest.java 8226295 generic-all
+compiler/aot/calls/fromAot/AotInvokeSpecial2AotTest.java 8226295 generic-all
+compiler/aot/calls/fromAot/AotInvokeSpecial2InterpretedTest.java 8226295 generic-all
+compiler/aot/calls/fromNative/NativeInvokeStatic2AotTest.java 8226295 generic-all
+compiler/aot/calls/fromNative/NativeInvokeSpecial2AotTest.java 8226295 generic-all
+compiler/aot/calls/fromNative/NativeInvokeVirtual2AotTest.java 8226295 generic-all
+compiler/aot/calls/fromCompiled/CompiledInvokeInterface2AotTest.java 8226295 generic-all
+compiler/aot/calls/fromCompiled/CompiledInvokeVirtual2AotTest.java 8226295 generic-all
+compiler/aot/calls/fromCompiled/CompiledInvokeDynamic2AotTest.java 8226295 generic-all
+compiler/aot/calls/fromCompiled/CompiledInvokeStatic2AotTest.java 8226295 generic-all
+compiler/aot/calls/fromCompiled/CompiledInvokeSpecial2AotTest.java 8226295 generic-all
+compiler/aot/calls/fromInterpreted/InterpretedInvokeDynamic2AotTest.java 8226295 generic-all
+compiler/aot/calls/fromInterpreted/InterpretedInvokeStatic2AotTest.java 8226295 generic-all
+compiler/aot/calls/fromInterpreted/InterpretedInvokeVirtual2AotTest.java 8226295 generic-all
+compiler/aot/calls/fromInterpreted/InterpretedInvokeSpecial2AotTest.java 8226295 generic-all
+compiler/aot/calls/fromInterpreted/InterpretedInvokeInterface2AotTest.java 8226295 generic-all
+compiler/aot/fingerprint/SelfChanged.java 8226295 generic-all
+compiler/aot/fingerprint/SelfChangedCDS.java 8226295 generic-all
+compiler/aot/fingerprint/SuperChanged.java 8226295 generic-all
+
 compiler/c2/Test8004741.java 8235801 generic-all
 
 compiler/jsr292/CreatesInterfaceDotEqualsCallInfo.java 8242923 generic-all
 
 #############################################################################
@@ -95,10 +172,32 @@
 
 runtime/jni/terminatedThread/TestTerminatedThread.java 8219652 aix-ppc64
 runtime/ReservedStack/ReservedStackTest.java 8231031 generic-all
 runtime/CompactStrings/TestMethodNames.java 8242921 generic-all
 
+# Valhalla TODO:
+runtime/CompressedOops/CompressedClassPointers.java 8210258 generic-all
+runtime/RedefineTests/RedefineLeak.java 8205032 generic-all
+runtime/SharedArchiveFile/BootAppendTests.java 8210258 generic-all
+runtime/SharedArchiveFile/CdsDifferentCompactStrings.java 8210258 generic-all
+runtime/SharedArchiveFile/CdsDifferentObjectAlignment.java 8210258 generic-all
+runtime/SharedArchiveFile/NonBootLoaderClasses.java 8210258 generic-all
+runtime/SharedArchiveFile/PrintSharedArchiveAndExit.java 8210258 generic-all
+runtime/SharedArchiveFile/SharedArchiveFile.java 8210258 generic-all
+runtime/SharedArchiveFile/SharedStringsDedup.java 8210258 generic-all
+runtime/SharedArchiveFile/SharedStringsRunAuto.java 8210258 generic-all
+runtime/SharedArchiveFile/SharedSymbolTableBucketSize.java 8210258 generic-all
+runtime/SharedArchiveFile/SpaceUtilizationCheck.java 8210258 generic-all
+runtime/SharedArchiveFile/TestInterpreterMethodEntries.java 8210258 generic-all
+runtime/SharedArchiveFile/serviceability/transformRelatedClasses/TransformInterfaceAndImplementor.java 8210258 generic-all
+runtime/SharedArchiveFile/serviceability/transformRelatedClasses/TransformSuperAndSubClasses.java 8210258 generic-all
+runtime/SharedArchiveFile/serviceability/transformRelatedClasses/TransformSuperSubTwoPckgs.java 8210258 generic-all
+runtime/appcds/ClassLoaderTest.java 8210258 generic-all
+runtime/appcds/HelloTest.java 8210258 generic-all
+runtime/appcds/sharedStrings/SharedStringsBasic.java 8210258 generic-all
+
+
 #############################################################################
 
 # :hotspot_serviceability
 
 serviceability/sa/sadebugd/DebugdConnectTest.java 8239062 macosx-x64
@@ -107,10 +206,36 @@
 serviceability/sa/TestRevPtrsForInvokeDynamic.java 8241235 generic-all
 
 serviceability/jvmti/HeapMonitor/MyPackage/HeapMonitorStatIntervalTest.java 8214032 generic-all
 serviceability/jvmti/HeapMonitor/MyPackage/HeapMonitorStatArrayCorrectnessTest.java 8224150 generic-all
 
+# Valhalla TODO:
+serviceability/sa/ClhsdbCDSCore.java 8190936 generic-all
+serviceability/sa/ClhsdbCDSJstackPrintAll.java 8190936 generic-all
+serviceability/sa/ClhsdbFindPC.java 8190936 generic-all
+serviceability/sa/ClhsdbInspect.java 8190936 generic-all
+serviceability/sa/ClhsdbJdis.java 8190936 generic-all
+serviceability/sa/ClhsdbJstack.java 8190936 generic-all
+serviceability/sa/ClhsdbPrintAll.java 8190936 generic-all
+serviceability/sa/ClhsdbPrintAs.java 8190936 generic-all
+serviceability/sa/ClhsdbPrintStatics.java 8190936 generic-all
+serviceability/sa/ClhsdbSource.java 8190936 generic-all
+serviceability/sa/ClhsdbSymbol.java 8190936 generic-all
+serviceability/sa/ClhsdbWhere.java 8190936 generic-all
+serviceability/sa/JhsdbThreadInfoTest.java 8190936 generic-all
+serviceability/sa/TestClassDump.java 8190936 generic-all
+serviceability/sa/TestClhsdbJstackLock.java 8190936 generic-all
+serviceability/sa/TestCpoolForInvokeDynamic.java 8190936 generic-all
+serviceability/sa/TestHeapDumpForInvokeDynamic.java 8190936 generic-all
+serviceability/sa/TestHeapDumpForLargeArray.java 8190936 generic-all
+serviceability/sa/TestIntConstant.java 8190936 generic-all
+serviceability/sa/TestJhsdbJstackLock.java 8190936 generic-all
+serviceability/sa/TestJmapCore.java 8190936 generic-all
+serviceability/sa/TestJmapCoreMetaspace.java 8190936 generic-all
+serviceability/sa/TestPrintMdo.java 8190936 generic-all
+serviceability/sa/jmap-hprof/JMapHProfLargeHeapTest.java 8190936 generic-all
+
 #############################################################################
 
 # :hotspot_misc
 
 #############################################################################
diff a/test/hotspot/jtreg/TEST.groups b/test/hotspot/jtreg/TEST.groups
--- a/test/hotspot/jtreg/TEST.groups
+++ b/test/hotspot/jtreg/TEST.groups
@@ -45,18 +45,25 @@
   gc \
   -gc/nvdimm
 
 # By design this group should include ALL tests under runtime sub-directory
 hotspot_runtime = \
-  runtime
+  runtime \
 
 hotspot_handshake = \
   runtime/handshake
 
 hotspot_serviceability = \
   serviceability
 
+hotspot_valhalla = \
+  runtime/valhalla \
+  compiler/valhalla
+
+hotspot_valhalla_runtime = \
+  runtime/valhalla
+
 hotspot_resourcehogs = \
   resourcehogs
 
 hotspot_misc = \
   / \
@@ -90,11 +97,11 @@
   -:tier1_compiler \
   -:hotspot_slow_compiler \
   -compiler/graalunit
 
 hotspot_slow_compiler = \
-  compiler/codegen/aes \
+  compiler/codecache/stress \
   compiler/codecache/stress \
   compiler/gcbarriers/PreserveFPRegistersTest.java
 
 tier1_compiler_1 = \
   compiler/arraycopy/ \
@@ -139,10 +146,11 @@
   compiler/runtime/ \
   compiler/startup/ \
   compiler/types/ \
   compiler/uncommontrap/ \
   compiler/unsafe/ \
+  compiler/valhalla/ \
   compiler/vectorization/ \
   -compiler/intrinsics/bmi \
   -compiler/intrinsics/mathexact \
   -compiler/intrinsics/sha \
   -compiler/intrinsics/bigInteger/TestMultiplyToLen.java \
@@ -156,10 +164,17 @@
 
 tier1_compiler_aot_jvmci = \
   compiler/aot \
   compiler/jvmci
 
+tier1_compiler_no_valhalla = \
+  :tier1_compiler_1 \
+  :tier1_compiler_2 \
+  :tier1_compiler_3 \
+  :tier1_compiler_not_xcomp \
+  -compiler/valhalla
+
 tier1_compiler_graal = \
   compiler/graalunit/HotspotTest.java
 
 ctw_1 = \
   applications/ctw/modules/ \
@@ -311,10 +326,14 @@
   sanity/ \
   testlibrary_tests/TestMutuallyExclusivePlatformPredicates.java \
  -:tier1_runtime_appcds_exclude \
  -runtime/signal
 
+tier1_runtime_no_valhalla = \
+  :tier1_runtime \
+  -runtime/valhalla
+
 hotspot_cds = \
   runtime/cds/ \
   runtime/CompressedOops/
 
 
diff a/test/hotspot/jtreg/compiler/jvmci/jdk.vm.ci.runtime.test/src/jdk/vm/ci/runtime/test/TestResolvedJavaType.java b/test/hotspot/jtreg/compiler/jvmci/jdk.vm.ci.runtime.test/src/jdk/vm/ci/runtime/test/TestResolvedJavaType.java
--- a/test/hotspot/jtreg/compiler/jvmci/jdk.vm.ci.runtime.test/src/jdk/vm/ci/runtime/test/TestResolvedJavaType.java
+++ b/test/hotspot/jtreg/compiler/jvmci/jdk.vm.ci.runtime.test/src/jdk/vm/ci/runtime/test/TestResolvedJavaType.java
@@ -271,11 +271,13 @@
     public void getInterfacesTest() {
         for (Class<?> c : classes) {
             ResolvedJavaType type = metaAccess.lookupJavaType(c);
             Class<?>[] expected = c.getInterfaces();
             ResolvedJavaType[] actual = type.getInterfaces();
-            assertEquals(expected.length, actual.length);
+            // With injection of the IdentityObject interface by the JVM, the number of
+            // interfaces visible through reflection and through JVMCI could differ by one
+            assertTrue(expected.length == actual.length || (actual.length - expected.length) == 1);
             for (int i = 0; i < expected.length; i++) {
                 assertTrue(actual[i].equals(metaAccess.lookupJavaType(expected[i])));
             }
         }
     }
diff a/test/jdk/TEST.groups b/test/jdk/TEST.groups
--- a/test/jdk/TEST.groups
+++ b/test/jdk/TEST.groups
@@ -101,11 +101,18 @@
     jdk/internal/loader \
     jdk/internal/misc \
     jdk/internal/ref \
     jdk/internal/jimage \
     jdk/internal/math \
-    jdk/modules
+    jdk/modules \
+    valhalla
+
+# valhalla lworld tests
+jdk_valhalla = \
+    java/lang/invoke \
+    valhalla \
+    com/sun/jdi/JdbInlineTypesTest.java
 
 # All of the java.util package
 jdk_util = \
     :jdk_util_other \
     :jdk_collections \
