<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Old src/hotspot/cpu/aarch64/stubGenerator_aarch64.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
  <body>
    <pre>
   1 /*
   2  * Copyright (c) 2003, 2020, Oracle and/or its affiliates. All rights reserved.
   3  * Copyright (c) 2014, 2020, Red Hat Inc. All rights reserved.
   4  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   5  *
   6  * This code is free software; you can redistribute it and/or modify it
   7  * under the terms of the GNU General Public License version 2 only, as
   8  * published by the Free Software Foundation.
   9  *
  10  * This code is distributed in the hope that it will be useful, but WITHOUT
  11  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  12  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  13  * version 2 for more details (a copy is included in the LICENSE file that
  14  * accompanied this code).
  15  *
  16  * You should have received a copy of the GNU General Public License version
  17  * 2 along with this work; if not, write to the Free Software Foundation,
  18  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  19  *
  20  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  21  * or visit www.oracle.com if you need additional information or have any
  22  * questions.
  23  *
  24  */
  25 
  26 #include &quot;precompiled.hpp&quot;
  27 #include &quot;asm/macroAssembler.hpp&quot;
  28 #include &quot;asm/macroAssembler.inline.hpp&quot;
  29 #include &quot;gc/shared/barrierSet.hpp&quot;
  30 #include &quot;gc/shared/barrierSetAssembler.hpp&quot;
  31 #include &quot;interpreter/interpreter.hpp&quot;
  32 #include &quot;memory/universe.hpp&quot;
  33 #include &quot;nativeInst_aarch64.hpp&quot;
  34 #include &quot;oops/instanceOop.hpp&quot;
  35 #include &quot;oops/method.hpp&quot;
  36 #include &quot;oops/objArrayKlass.hpp&quot;
  37 #include &quot;oops/oop.inline.hpp&quot;
  38 #include &quot;prims/methodHandles.hpp&quot;
  39 #include &quot;runtime/frame.inline.hpp&quot;
  40 #include &quot;runtime/handles.inline.hpp&quot;
  41 #include &quot;runtime/sharedRuntime.hpp&quot;
  42 #include &quot;runtime/stubCodeGenerator.hpp&quot;
  43 #include &quot;runtime/stubRoutines.hpp&quot;
  44 #include &quot;runtime/thread.inline.hpp&quot;
  45 #include &quot;utilities/align.hpp&quot;
  46 #include &quot;utilities/powerOfTwo.hpp&quot;
  47 #ifdef COMPILER2
  48 #include &quot;opto/runtime.hpp&quot;
  49 #endif
  50 #if INCLUDE_ZGC
  51 #include &quot;gc/z/zThreadLocalData.hpp&quot;
  52 #endif
  53 
  54 // Declaration and definition of StubGenerator (no .hpp file).
  55 // For a more detailed description of the stub routine structure
  56 // see the comment in stubRoutines.hpp
  57 
  58 #undef __
  59 #define __ _masm-&gt;
  60 #define TIMES_OOP Address::sxtw(exact_log2(UseCompressedOops ? 4 : 8))
  61 
  62 #ifdef PRODUCT
  63 #define BLOCK_COMMENT(str) /* nothing */
  64 #else
  65 #define BLOCK_COMMENT(str) __ block_comment(str)
  66 #endif
  67 
  68 #define BIND(label) bind(label); BLOCK_COMMENT(#label &quot;:&quot;)
  69 
  70 // Stub Code definitions
  71 
  72 class StubGenerator: public StubCodeGenerator {
  73  private:
  74 
  75 #ifdef PRODUCT
  76 #define inc_counter_np(counter) ((void)0)
  77 #else
  78   void inc_counter_np_(int&amp; counter) {
  79     __ lea(rscratch2, ExternalAddress((address)&amp;counter));
  80     __ ldrw(rscratch1, Address(rscratch2));
  81     __ addw(rscratch1, rscratch1, 1);
  82     __ strw(rscratch1, Address(rscratch2));
  83   }
  84 #define inc_counter_np(counter) \
  85   BLOCK_COMMENT(&quot;inc_counter &quot; #counter); \
  86   inc_counter_np_(counter);
  87 #endif
  88 
  89   // Call stubs are used to call Java from C
  90   //
  91   // Arguments:
  92   //    c_rarg0:   call wrapper address                   address
  93   //    c_rarg1:   result                                 address
  94   //    c_rarg2:   result type                            BasicType
  95   //    c_rarg3:   method                                 Method*
  96   //    c_rarg4:   (interpreter) entry point              address
  97   //    c_rarg5:   parameters                             intptr_t*
  98   //    c_rarg6:   parameter size (in words)              int
  99   //    c_rarg7:   thread                                 Thread*
 100   //
 101   // There is no return from the stub itself as any Java result
 102   // is written to result
 103   //
 104   // we save r30 (lr) as the return PC at the base of the frame and
 105   // link r29 (fp) below it as the frame pointer installing sp (r31)
 106   // into fp.
 107   //
 108   // we save r0-r7, which accounts for all the c arguments.
 109   //
 110   // TODO: strictly do we need to save them all? they are treated as
 111   // volatile by C so could we omit saving the ones we are going to
 112   // place in global registers (thread? method?) or those we only use
 113   // during setup of the Java call?
 114   //
 115   // we don&#39;t need to save r8 which C uses as an indirect result location
 116   // return register.
 117   //
 118   // we don&#39;t need to save r9-r15 which both C and Java treat as
 119   // volatile
 120   //
 121   // we don&#39;t need to save r16-18 because Java does not use them
 122   //
 123   // we save r19-r28 which Java uses as scratch registers and C
 124   // expects to be callee-save
 125   //
 126   // we save the bottom 64 bits of each value stored in v8-v15; it is
 127   // the responsibility of the caller to preserve larger values.
 128   //
 129   // so the stub frame looks like this when we enter Java code
 130   //
 131   //     [ return_from_Java     ] &lt;--- sp
 132   //     [ argument word n      ]
 133   //      ...
 134   // -27 [ argument word 1      ]
 135   // -26 [ saved v15            ] &lt;--- sp_after_call
 136   // -25 [ saved v14            ]
 137   // -24 [ saved v13            ]
 138   // -23 [ saved v12            ]
 139   // -22 [ saved v11            ]
 140   // -21 [ saved v10            ]
 141   // -20 [ saved v9             ]
 142   // -19 [ saved v8             ]
 143   // -18 [ saved r28            ]
 144   // -17 [ saved r27            ]
 145   // -16 [ saved r26            ]
 146   // -15 [ saved r25            ]
 147   // -14 [ saved r24            ]
 148   // -13 [ saved r23            ]
 149   // -12 [ saved r22            ]
 150   // -11 [ saved r21            ]
 151   // -10 [ saved r20            ]
 152   //  -9 [ saved r19            ]
 153   //  -8 [ call wrapper    (r0) ]
 154   //  -7 [ result          (r1) ]
 155   //  -6 [ result type     (r2) ]
 156   //  -5 [ method          (r3) ]
 157   //  -4 [ entry point     (r4) ]
 158   //  -3 [ parameters      (r5) ]
 159   //  -2 [ parameter size  (r6) ]
 160   //  -1 [ thread (r7)          ]
 161   //   0 [ saved fp       (r29) ] &lt;--- fp == saved sp (r31)
 162   //   1 [ saved lr       (r30) ]
 163 
 164   // Call stub stack layout word offsets from fp
 165   enum call_stub_layout {
 166     sp_after_call_off = -26,
 167 
 168     d15_off            = -26,
 169     d13_off            = -24,
 170     d11_off            = -22,
 171     d9_off             = -20,
 172 
 173     r28_off            = -18,
 174     r26_off            = -16,
 175     r24_off            = -14,
 176     r22_off            = -12,
 177     r20_off            = -10,
 178     call_wrapper_off   =  -8,
 179     result_off         =  -7,
 180     result_type_off    =  -6,
 181     method_off         =  -5,
 182     entry_point_off    =  -4,
 183     parameter_size_off =  -2,
 184     thread_off         =  -1,
 185     fp_f               =   0,
 186     retaddr_off        =   1,
 187   };
 188 
 189   address generate_call_stub(address&amp; return_address) {
 190     assert((int)frame::entry_frame_after_call_words == -(int)sp_after_call_off + 1 &amp;&amp;
 191            (int)frame::entry_frame_call_wrapper_offset == (int)call_wrapper_off,
 192            &quot;adjust this code&quot;);
 193 
 194     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;call_stub&quot;);
 195     address start = __ pc();
 196 
 197     const Address sp_after_call(rfp, sp_after_call_off * wordSize);
 198 
 199     const Address call_wrapper  (rfp, call_wrapper_off   * wordSize);
 200     const Address result        (rfp, result_off         * wordSize);
 201     const Address result_type   (rfp, result_type_off    * wordSize);
 202     const Address method        (rfp, method_off         * wordSize);
 203     const Address entry_point   (rfp, entry_point_off    * wordSize);
 204     const Address parameter_size(rfp, parameter_size_off * wordSize);
 205 
 206     const Address thread        (rfp, thread_off         * wordSize);
 207 
 208     const Address d15_save      (rfp, d15_off * wordSize);
 209     const Address d13_save      (rfp, d13_off * wordSize);
 210     const Address d11_save      (rfp, d11_off * wordSize);
 211     const Address d9_save       (rfp, d9_off * wordSize);
 212 
 213     const Address r28_save      (rfp, r28_off * wordSize);
 214     const Address r26_save      (rfp, r26_off * wordSize);
 215     const Address r24_save      (rfp, r24_off * wordSize);
 216     const Address r22_save      (rfp, r22_off * wordSize);
 217     const Address r20_save      (rfp, r20_off * wordSize);
 218 
 219     // stub code
 220 
 221     address aarch64_entry = __ pc();
 222 
 223     // set up frame and move sp to end of save area
 224     __ enter();
 225     __ sub(sp, rfp, -sp_after_call_off * wordSize);
 226 
 227     // save register parameters and Java scratch/global registers
 228     // n.b. we save thread even though it gets installed in
 229     // rthread because we want to sanity check rthread later
 230     __ str(c_rarg7,  thread);
 231     __ strw(c_rarg6, parameter_size);
 232     __ stp(c_rarg4, c_rarg5,  entry_point);
 233     __ stp(c_rarg2, c_rarg3,  result_type);
 234     __ stp(c_rarg0, c_rarg1,  call_wrapper);
 235 
 236     __ stp(r20, r19,   r20_save);
 237     __ stp(r22, r21,   r22_save);
 238     __ stp(r24, r23,   r24_save);
 239     __ stp(r26, r25,   r26_save);
 240     __ stp(r28, r27,   r28_save);
 241 
 242     __ stpd(v9,  v8,   d9_save);
 243     __ stpd(v11, v10,  d11_save);
 244     __ stpd(v13, v12,  d13_save);
 245     __ stpd(v15, v14,  d15_save);
 246 
 247     // install Java thread in global register now we have saved
 248     // whatever value it held
 249     __ mov(rthread, c_rarg7);
 250     // And method
 251     __ mov(rmethod, c_rarg3);
 252 
 253     // set up the heapbase register
 254     __ reinit_heapbase();
 255 
 256 #ifdef ASSERT
 257     // make sure we have no pending exceptions
 258     {
 259       Label L;
 260       __ ldr(rscratch1, Address(rthread, in_bytes(Thread::pending_exception_offset())));
 261       __ cmp(rscratch1, (u1)NULL_WORD);
 262       __ br(Assembler::EQ, L);
 263       __ stop(&quot;StubRoutines::call_stub: entered with pending exception&quot;);
 264       __ BIND(L);
 265     }
 266 #endif
 267     // pass parameters if any
 268     __ mov(esp, sp);
 269     __ sub(rscratch1, sp, c_rarg6, ext::uxtw, LogBytesPerWord); // Move SP out of the way
 270     __ andr(sp, rscratch1, -2 * wordSize);
 271 
 272     BLOCK_COMMENT(&quot;pass parameters if any&quot;);
 273     Label parameters_done;
 274     // parameter count is still in c_rarg6
 275     // and parameter pointer identifying param 1 is in c_rarg5
 276     __ cbzw(c_rarg6, parameters_done);
 277 
 278     address loop = __ pc();
 279     __ ldr(rscratch1, Address(__ post(c_rarg5, wordSize)));
 280     __ subsw(c_rarg6, c_rarg6, 1);
 281     __ push(rscratch1);
 282     __ br(Assembler::GT, loop);
 283 
 284     __ BIND(parameters_done);
 285 
 286     // call Java entry -- passing methdoOop, and current sp
 287     //      rmethod: Method*
 288     //      r13: sender sp
 289     BLOCK_COMMENT(&quot;call Java function&quot;);
 290     __ mov(r13, sp);
 291     __ blr(c_rarg4);
 292 
 293     // we do this here because the notify will already have been done
 294     // if we get to the next instruction via an exception
 295     //
 296     // n.b. adding this instruction here affects the calculation of
 297     // whether or not a routine returns to the call stub (used when
 298     // doing stack walks) since the normal test is to check the return
 299     // pc against the address saved below. so we may need to allow for
 300     // this extra instruction in the check.
 301 
 302     // save current address for use by exception handling code
 303 
 304     return_address = __ pc();
 305 
 306     // store result depending on type (everything that is not
 307     // T_OBJECT, T_INLINE_TYPE, T_LONG, T_FLOAT or T_DOUBLE is treated as T_INT)
 308     // n.b. this assumes Java returns an integral result in r0
 309     // and a floating result in j_farg0
 310     __ ldr(j_rarg2, result);
 311     Label is_long, is_float, is_double, is_value, exit;
 312     __ ldr(j_rarg1, result_type);
 313     __ cmp(j_rarg1, (u1)T_OBJECT);
 314     __ br(Assembler::EQ, is_long);
 315     __ cmp(j_rarg1, (u1)T_INLINE_TYPE);
 316     __ br(Assembler::EQ, is_value);
 317     __ cmp(j_rarg1, (u1)T_LONG);
 318     __ br(Assembler::EQ, is_long);
 319     __ cmp(j_rarg1, (u1)T_FLOAT);
 320     __ br(Assembler::EQ, is_float);
 321     __ cmp(j_rarg1, (u1)T_DOUBLE);
 322     __ br(Assembler::EQ, is_double);
 323 
 324     // handle T_INT case
 325     __ strw(r0, Address(j_rarg2));
 326 
 327     __ BIND(exit);
 328 
 329     // pop parameters
 330     __ sub(esp, rfp, -sp_after_call_off * wordSize);
 331 
 332 #ifdef ASSERT
 333     // verify that threads correspond
 334     {
 335       Label L, S;
 336       __ ldr(rscratch1, thread);
 337       __ cmp(rthread, rscratch1);
 338       __ br(Assembler::NE, S);
 339       __ get_thread(rscratch1);
 340       __ cmp(rthread, rscratch1);
 341       __ br(Assembler::EQ, L);
 342       __ BIND(S);
 343       __ stop(&quot;StubRoutines::call_stub: threads must correspond&quot;);
 344       __ BIND(L);
 345     }
 346 #endif
 347 
 348     // restore callee-save registers
 349     __ ldpd(v15, v14,  d15_save);
 350     __ ldpd(v13, v12,  d13_save);
 351     __ ldpd(v11, v10,  d11_save);
 352     __ ldpd(v9,  v8,   d9_save);
 353 
 354     __ ldp(r28, r27,   r28_save);
 355     __ ldp(r26, r25,   r26_save);
 356     __ ldp(r24, r23,   r24_save);
 357     __ ldp(r22, r21,   r22_save);
 358     __ ldp(r20, r19,   r20_save);
 359 
 360     __ ldp(c_rarg0, c_rarg1,  call_wrapper);
 361     __ ldrw(c_rarg2, result_type);
 362     __ ldr(c_rarg3,  method);
 363     __ ldp(c_rarg4, c_rarg5,  entry_point);
 364     __ ldp(c_rarg6, c_rarg7,  parameter_size);
 365 
 366     // leave frame and return to caller
 367     __ leave();
 368     __ ret(lr);
 369 
 370     // handle return types different from T_INT
 371     __ BIND(is_value);
 372     if (InlineTypeReturnedAsFields) {
 373       // Check for flattened return value
 374       __ cbz(r0, is_long);
 375       // Initialize pre-allocated buffer
 376       __ mov(r1, r0);
 377       __ andr(r1, r1, -2);
 378       __ ldr(r1, Address(r1, InstanceKlass::adr_inlineklass_fixed_block_offset()));
 379       __ ldr(r1, Address(r1, InlineKlass::pack_handler_offset()));
 380       __ ldr(r0, Address(j_rarg2, 0));
 381       __ blr(r1);
 382       __ b(exit);
 383     }
 384 
 385     __ BIND(is_long);
 386     __ str(r0, Address(j_rarg2, 0));
 387     __ br(Assembler::AL, exit);
 388 
 389     __ BIND(is_float);
 390     __ strs(j_farg0, Address(j_rarg2, 0));
 391     __ br(Assembler::AL, exit);
 392 
 393     __ BIND(is_double);
 394     __ strd(j_farg0, Address(j_rarg2, 0));
 395     __ br(Assembler::AL, exit);
 396 
 397     return start;
 398   }
 399 
 400   // Return point for a Java call if there&#39;s an exception thrown in
 401   // Java code.  The exception is caught and transformed into a
 402   // pending exception stored in JavaThread that can be tested from
 403   // within the VM.
 404   //
 405   // Note: Usually the parameters are removed by the callee. In case
 406   // of an exception crossing an activation frame boundary, that is
 407   // not the case if the callee is compiled code =&gt; need to setup the
 408   // rsp.
 409   //
 410   // r0: exception oop
 411 
 412   address generate_catch_exception() {
 413     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;catch_exception&quot;);
 414     address start = __ pc();
 415 
 416     // same as in generate_call_stub():
 417     const Address sp_after_call(rfp, sp_after_call_off * wordSize);
 418     const Address thread        (rfp, thread_off         * wordSize);
 419 
 420 #ifdef ASSERT
 421     // verify that threads correspond
 422     {
 423       Label L, S;
 424       __ ldr(rscratch1, thread);
 425       __ cmp(rthread, rscratch1);
 426       __ br(Assembler::NE, S);
 427       __ get_thread(rscratch1);
 428       __ cmp(rthread, rscratch1);
 429       __ br(Assembler::EQ, L);
 430       __ bind(S);
 431       __ stop(&quot;StubRoutines::catch_exception: threads must correspond&quot;);
 432       __ bind(L);
 433     }
 434 #endif
 435 
 436     // set pending exception
 437     __ verify_oop(r0);
 438 
 439     __ str(r0, Address(rthread, Thread::pending_exception_offset()));
 440     __ mov(rscratch1, (address)__FILE__);
 441     __ str(rscratch1, Address(rthread, Thread::exception_file_offset()));
 442     __ movw(rscratch1, (int)__LINE__);
 443     __ strw(rscratch1, Address(rthread, Thread::exception_line_offset()));
 444 
 445     // complete return to VM
 446     assert(StubRoutines::_call_stub_return_address != NULL,
 447            &quot;_call_stub_return_address must have been generated before&quot;);
 448     __ b(StubRoutines::_call_stub_return_address);
 449 
 450     return start;
 451   }
 452 
 453   // Continuation point for runtime calls returning with a pending
 454   // exception.  The pending exception check happened in the runtime
 455   // or native call stub.  The pending exception in Thread is
 456   // converted into a Java-level exception.
 457   //
 458   // Contract with Java-level exception handlers:
 459   // r0: exception
 460   // r3: throwing pc
 461   //
 462   // NOTE: At entry of this stub, exception-pc must be in LR !!
 463 
 464   // NOTE: this is always used as a jump target within generated code
 465   // so it just needs to be generated code wiht no x86 prolog
 466 
 467   address generate_forward_exception() {
 468     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;forward exception&quot;);
 469     address start = __ pc();
 470 
 471     // Upon entry, LR points to the return address returning into
 472     // Java (interpreted or compiled) code; i.e., the return address
 473     // becomes the throwing pc.
 474     //
 475     // Arguments pushed before the runtime call are still on the stack
 476     // but the exception handler will reset the stack pointer -&gt;
 477     // ignore them.  A potential result in registers can be ignored as
 478     // well.
 479 
 480 #ifdef ASSERT
 481     // make sure this code is only executed if there is a pending exception
 482     {
 483       Label L;
 484       __ ldr(rscratch1, Address(rthread, Thread::pending_exception_offset()));
 485       __ cbnz(rscratch1, L);
 486       __ stop(&quot;StubRoutines::forward exception: no pending exception (1)&quot;);
 487       __ bind(L);
 488     }
 489 #endif
 490 
 491     // compute exception handler into r19
 492 
 493     // call the VM to find the handler address associated with the
 494     // caller address. pass thread in r0 and caller pc (ret address)
 495     // in r1. n.b. the caller pc is in lr, unlike x86 where it is on
 496     // the stack.
 497     __ mov(c_rarg1, lr);
 498     // lr will be trashed by the VM call so we move it to R19
 499     // (callee-saved) because we also need to pass it to the handler
 500     // returned by this call.
 501     __ mov(r19, lr);
 502     BLOCK_COMMENT(&quot;call exception_handler_for_return_address&quot;);
 503     __ call_VM_leaf(CAST_FROM_FN_PTR(address,
 504                          SharedRuntime::exception_handler_for_return_address),
 505                     rthread, c_rarg1);
 506     // we should not really care that lr is no longer the callee
 507     // address. we saved the value the handler needs in r19 so we can
 508     // just copy it to r3. however, the C2 handler will push its own
 509     // frame and then calls into the VM and the VM code asserts that
 510     // the PC for the frame above the handler belongs to a compiled
 511     // Java method. So, we restore lr here to satisfy that assert.
 512     __ mov(lr, r19);
 513     // setup r0 &amp; r3 &amp; clear pending exception
 514     __ mov(r3, r19);
 515     __ mov(r19, r0);
 516     __ ldr(r0, Address(rthread, Thread::pending_exception_offset()));
 517     __ str(zr, Address(rthread, Thread::pending_exception_offset()));
 518 
 519 #ifdef ASSERT
 520     // make sure exception is set
 521     {
 522       Label L;
 523       __ cbnz(r0, L);
 524       __ stop(&quot;StubRoutines::forward exception: no pending exception (2)&quot;);
 525       __ bind(L);
 526     }
 527 #endif
 528 
 529     // continue at exception handler
 530     // r0: exception
 531     // r3: throwing pc
 532     // r19: exception handler
 533     __ verify_oop(r0);
 534     __ br(r19);
 535 
 536     return start;
 537   }
 538 
 539   // Non-destructive plausibility checks for oops
 540   //
 541   // Arguments:
 542   //    r0: oop to verify
 543   //    rscratch1: error message
 544   //
 545   // Stack after saving c_rarg3:
 546   //    [tos + 0]: saved c_rarg3
 547   //    [tos + 1]: saved c_rarg2
 548   //    [tos + 2]: saved lr
 549   //    [tos + 3]: saved rscratch2
 550   //    [tos + 4]: saved r0
 551   //    [tos + 5]: saved rscratch1
 552   address generate_verify_oop() {
 553 
 554     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;verify_oop&quot;);
 555     address start = __ pc();
 556 
 557     Label exit, error;
 558 
 559     // save c_rarg2 and c_rarg3
 560     __ stp(c_rarg3, c_rarg2, Address(__ pre(sp, -16)));
 561 
 562     // __ incrementl(ExternalAddress((address) StubRoutines::verify_oop_count_addr()));
 563     __ lea(c_rarg2, ExternalAddress((address) StubRoutines::verify_oop_count_addr()));
 564     __ ldr(c_rarg3, Address(c_rarg2));
 565     __ add(c_rarg3, c_rarg3, 1);
 566     __ str(c_rarg3, Address(c_rarg2));
 567 
 568     // object is in r0
 569     // make sure object is &#39;reasonable&#39;
 570     __ cbz(r0, exit); // if obj is NULL it is OK
 571 
 572 #if INCLUDE_ZGC
 573     if (UseZGC) {
 574       // Check if mask is good.
 575       // verifies that ZAddressBadMask &amp; r0 == 0
 576       __ ldr(c_rarg3, Address(rthread, ZThreadLocalData::address_bad_mask_offset()));
 577       __ andr(c_rarg2, r0, c_rarg3);
 578       __ cbnz(c_rarg2, error);
 579     }
 580 #endif
 581 
 582     // Check if the oop is in the right area of memory
 583     __ mov(c_rarg3, (intptr_t) Universe::verify_oop_mask());
 584     __ andr(c_rarg2, r0, c_rarg3);
 585     __ mov(c_rarg3, (intptr_t) Universe::verify_oop_bits());
 586 
 587     // Compare c_rarg2 and c_rarg3.  We don&#39;t use a compare
 588     // instruction here because the flags register is live.
 589     __ eor(c_rarg2, c_rarg2, c_rarg3);
 590     __ cbnz(c_rarg2, error);
 591 
 592     // make sure klass is &#39;reasonable&#39;, which is not zero.
 593     __ load_klass(r0, r0);  // get klass
 594     __ cbz(r0, error);      // if klass is NULL it is broken
 595 
 596     // return if everything seems ok
 597     __ bind(exit);
 598 
 599     __ ldp(c_rarg3, c_rarg2, Address(__ post(sp, 16)));
 600     __ ret(lr);
 601 
 602     // handle errors
 603     __ bind(error);
 604     __ ldp(c_rarg3, c_rarg2, Address(__ post(sp, 16)));
 605 
 606     __ push(RegSet::range(r0, r29), sp);
 607     // debug(char* msg, int64_t pc, int64_t regs[])
 608     __ mov(c_rarg0, rscratch1);      // pass address of error message
 609     __ mov(c_rarg1, lr);             // pass return address
 610     __ mov(c_rarg2, sp);             // pass address of regs on stack
 611 #ifndef PRODUCT
 612     assert(frame::arg_reg_save_area_bytes == 0, &quot;not expecting frame reg save area&quot;);
 613 #endif
 614     BLOCK_COMMENT(&quot;call MacroAssembler::debug&quot;);
 615     __ mov(rscratch1, CAST_FROM_FN_PTR(address, MacroAssembler::debug64));
 616     __ blr(rscratch1);
 617     __ hlt(0);
 618 
 619     return start;
 620   }
 621 
 622   void array_overlap_test(Label&amp; L_no_overlap, Address::sxtw sf) { __ b(L_no_overlap); }
 623 
 624   // The inner part of zero_words().  This is the bulk operation,
 625   // zeroing words in blocks, possibly using DC ZVA to do it.  The
 626   // caller is responsible for zeroing the last few words.
 627   //
 628   // Inputs:
 629   // r10: the HeapWord-aligned base address of an array to zero.
 630   // r11: the count in HeapWords, r11 &gt; 0.
 631   //
 632   // Returns r10 and r11, adjusted for the caller to clear.
 633   // r10: the base address of the tail of words left to clear.
 634   // r11: the number of words in the tail.
 635   //      r11 &lt; MacroAssembler::zero_words_block_size.
 636 
 637   address generate_zero_blocks() {
 638     Label done;
 639     Label base_aligned;
 640 
 641     Register base = r10, cnt = r11;
 642 
 643     __ align(CodeEntryAlignment);
 644     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;zero_blocks&quot;);
 645     address start = __ pc();
 646 
 647     if (UseBlockZeroing) {
 648       int zva_length = VM_Version::zva_length();
 649 
 650       // Ensure ZVA length can be divided by 16. This is required by
 651       // the subsequent operations.
 652       assert (zva_length % 16 == 0, &quot;Unexpected ZVA Length&quot;);
 653 
 654       __ tbz(base, 3, base_aligned);
 655       __ str(zr, Address(__ post(base, 8)));
 656       __ sub(cnt, cnt, 1);
 657       __ bind(base_aligned);
 658 
 659       // Ensure count &gt;= zva_length * 2 so that it still deserves a zva after
 660       // alignment.
 661       Label small;
 662       int low_limit = MAX2(zva_length * 2, (int)BlockZeroingLowLimit);
 663       __ subs(rscratch1, cnt, low_limit &gt;&gt; 3);
 664       __ br(Assembler::LT, small);
 665       __ zero_dcache_blocks(base, cnt);
 666       __ bind(small);
 667     }
 668 
 669     {
 670       // Number of stp instructions we&#39;ll unroll
 671       const int unroll =
 672         MacroAssembler::zero_words_block_size / 2;
 673       // Clear the remaining blocks.
 674       Label loop;
 675       __ subs(cnt, cnt, unroll * 2);
 676       __ br(Assembler::LT, done);
 677       __ bind(loop);
 678       for (int i = 0; i &lt; unroll; i++)
 679         __ stp(zr, zr, __ post(base, 16));
 680       __ subs(cnt, cnt, unroll * 2);
 681       __ br(Assembler::GE, loop);
 682       __ bind(done);
 683       __ add(cnt, cnt, unroll * 2);
 684     }
 685 
 686     __ ret(lr);
 687 
 688     return start;
 689   }
 690 
 691 
 692   typedef enum {
 693     copy_forwards = 1,
 694     copy_backwards = -1
 695   } copy_direction;
 696 
 697   // Bulk copy of blocks of 8 words.
 698   //
 699   // count is a count of words.
 700   //
 701   // Precondition: count &gt;= 8
 702   //
 703   // Postconditions:
 704   //
 705   // The least significant bit of count contains the remaining count
 706   // of words to copy.  The rest of count is trash.
 707   //
 708   // s and d are adjusted to point to the remaining words to copy
 709   //
 710   void generate_copy_longs(Label &amp;start, Register s, Register d, Register count,
 711                            copy_direction direction) {
 712     int unit = wordSize * direction;
 713     int bias = (UseSIMDForMemoryOps ? 4:2) * wordSize;
 714 
 715     int offset;
 716     const Register t0 = r3, t1 = r4, t2 = r5, t3 = r6,
 717       t4 = r7, t5 = r10, t6 = r11, t7 = r12;
 718     const Register stride = r13;
 719 
 720     assert_different_registers(rscratch1, t0, t1, t2, t3, t4, t5, t6, t7);
 721     assert_different_registers(s, d, count, rscratch1);
 722 
 723     Label again, drain;
 724     const char *stub_name;
 725     if (direction == copy_forwards)
 726       stub_name = &quot;forward_copy_longs&quot;;
 727     else
 728       stub_name = &quot;backward_copy_longs&quot;;
 729 
 730     __ align(CodeEntryAlignment);
 731 
 732     StubCodeMark mark(this, &quot;StubRoutines&quot;, stub_name);
 733 
 734     __ bind(start);
 735 
 736     Label unaligned_copy_long;
 737     if (AvoidUnalignedAccesses) {
 738       __ tbnz(d, 3, unaligned_copy_long);
 739     }
 740 
 741     if (direction == copy_forwards) {
 742       __ sub(s, s, bias);
 743       __ sub(d, d, bias);
 744     }
 745 
 746 #ifdef ASSERT
 747     // Make sure we are never given &lt; 8 words
 748     {
 749       Label L;
 750       __ cmp(count, (u1)8);
 751       __ br(Assembler::GE, L);
 752       __ stop(&quot;genrate_copy_longs called with &lt; 8 words&quot;);
 753       __ bind(L);
 754     }
 755 #endif
 756 
 757     // Fill 8 registers
 758     if (UseSIMDForMemoryOps) {
 759       __ ldpq(v0, v1, Address(s, 4 * unit));
 760       __ ldpq(v2, v3, Address(__ pre(s, 8 * unit)));
 761     } else {
 762       __ ldp(t0, t1, Address(s, 2 * unit));
 763       __ ldp(t2, t3, Address(s, 4 * unit));
 764       __ ldp(t4, t5, Address(s, 6 * unit));
 765       __ ldp(t6, t7, Address(__ pre(s, 8 * unit)));
 766     }
 767 
 768     __ subs(count, count, 16);
 769     __ br(Assembler::LO, drain);
 770 
 771     int prefetch = PrefetchCopyIntervalInBytes;
 772     bool use_stride = false;
 773     if (direction == copy_backwards) {
 774        use_stride = prefetch &gt; 256;
 775        prefetch = -prefetch;
 776        if (use_stride) __ mov(stride, prefetch);
 777     }
 778 
 779     __ bind(again);
 780 
 781     if (PrefetchCopyIntervalInBytes &gt; 0)
 782       __ prfm(use_stride ? Address(s, stride) : Address(s, prefetch), PLDL1KEEP);
 783 
 784     if (UseSIMDForMemoryOps) {
 785       __ stpq(v0, v1, Address(d, 4 * unit));
 786       __ ldpq(v0, v1, Address(s, 4 * unit));
 787       __ stpq(v2, v3, Address(__ pre(d, 8 * unit)));
 788       __ ldpq(v2, v3, Address(__ pre(s, 8 * unit)));
 789     } else {
 790       __ stp(t0, t1, Address(d, 2 * unit));
 791       __ ldp(t0, t1, Address(s, 2 * unit));
 792       __ stp(t2, t3, Address(d, 4 * unit));
 793       __ ldp(t2, t3, Address(s, 4 * unit));
 794       __ stp(t4, t5, Address(d, 6 * unit));
 795       __ ldp(t4, t5, Address(s, 6 * unit));
 796       __ stp(t6, t7, Address(__ pre(d, 8 * unit)));
 797       __ ldp(t6, t7, Address(__ pre(s, 8 * unit)));
 798     }
 799 
 800     __ subs(count, count, 8);
 801     __ br(Assembler::HS, again);
 802 
 803     // Drain
 804     __ bind(drain);
 805     if (UseSIMDForMemoryOps) {
 806       __ stpq(v0, v1, Address(d, 4 * unit));
 807       __ stpq(v2, v3, Address(__ pre(d, 8 * unit)));
 808     } else {
 809       __ stp(t0, t1, Address(d, 2 * unit));
 810       __ stp(t2, t3, Address(d, 4 * unit));
 811       __ stp(t4, t5, Address(d, 6 * unit));
 812       __ stp(t6, t7, Address(__ pre(d, 8 * unit)));
 813     }
 814 
 815     {
 816       Label L1, L2;
 817       __ tbz(count, exact_log2(4), L1);
 818       if (UseSIMDForMemoryOps) {
 819         __ ldpq(v0, v1, Address(__ pre(s, 4 * unit)));
 820         __ stpq(v0, v1, Address(__ pre(d, 4 * unit)));
 821       } else {
 822         __ ldp(t0, t1, Address(s, 2 * unit));
 823         __ ldp(t2, t3, Address(__ pre(s, 4 * unit)));
 824         __ stp(t0, t1, Address(d, 2 * unit));
 825         __ stp(t2, t3, Address(__ pre(d, 4 * unit)));
 826       }
 827       __ bind(L1);
 828 
 829       if (direction == copy_forwards) {
 830         __ add(s, s, bias);
 831         __ add(d, d, bias);
 832       }
 833 
 834       __ tbz(count, 1, L2);
 835       __ ldp(t0, t1, Address(__ adjust(s, 2 * unit, direction == copy_backwards)));
 836       __ stp(t0, t1, Address(__ adjust(d, 2 * unit, direction == copy_backwards)));
 837       __ bind(L2);
 838     }
 839 
 840     __ ret(lr);
 841 
 842     if (AvoidUnalignedAccesses) {
 843       Label drain, again;
 844       // Register order for storing. Order is different for backward copy.
 845 
 846       __ bind(unaligned_copy_long);
 847 
 848       // source address is even aligned, target odd aligned
 849       //
 850       // when forward copying word pairs we read long pairs at offsets
 851       // {0, 2, 4, 6} (in long words). when backwards copying we read
 852       // long pairs at offsets {-2, -4, -6, -8}. We adjust the source
 853       // address by -2 in the forwards case so we can compute the
 854       // source offsets for both as {2, 4, 6, 8} * unit where unit = 1
 855       // or -1.
 856       //
 857       // when forward copying we need to store 1 word, 3 pairs and
 858       // then 1 word at offsets {0, 1, 3, 5, 7}. Rather thna use a
 859       // zero offset We adjust the destination by -1 which means we
 860       // have to use offsets { 1, 2, 4, 6, 8} * unit for the stores.
 861       //
 862       // When backwards copyng we need to store 1 word, 3 pairs and
 863       // then 1 word at offsets {-1, -3, -5, -7, -8} i.e. we use
 864       // offsets {1, 3, 5, 7, 8} * unit.
 865 
 866       if (direction == copy_forwards) {
 867         __ sub(s, s, 16);
 868         __ sub(d, d, 8);
 869       }
 870 
 871       // Fill 8 registers
 872       //
 873       // for forwards copy s was offset by -16 from the original input
 874       // value of s so the register contents are at these offsets
 875       // relative to the 64 bit block addressed by that original input
 876       // and so on for each successive 64 byte block when s is updated
 877       //
 878       // t0 at offset 0,  t1 at offset 8
 879       // t2 at offset 16, t3 at offset 24
 880       // t4 at offset 32, t5 at offset 40
 881       // t6 at offset 48, t7 at offset 56
 882 
 883       // for backwards copy s was not offset so the register contents
 884       // are at these offsets into the preceding 64 byte block
 885       // relative to that original input and so on for each successive
 886       // preceding 64 byte block when s is updated. this explains the
 887       // slightly counter-intuitive looking pattern of register usage
 888       // in the stp instructions for backwards copy.
 889       //
 890       // t0 at offset -16, t1 at offset -8
 891       // t2 at offset -32, t3 at offset -24
 892       // t4 at offset -48, t5 at offset -40
 893       // t6 at offset -64, t7 at offset -56
 894 
 895       __ ldp(t0, t1, Address(s, 2 * unit));
 896       __ ldp(t2, t3, Address(s, 4 * unit));
 897       __ ldp(t4, t5, Address(s, 6 * unit));
 898       __ ldp(t6, t7, Address(__ pre(s, 8 * unit)));
 899 
 900       __ subs(count, count, 16);
 901       __ br(Assembler::LO, drain);
 902 
 903       int prefetch = PrefetchCopyIntervalInBytes;
 904       bool use_stride = false;
 905       if (direction == copy_backwards) {
 906          use_stride = prefetch &gt; 256;
 907          prefetch = -prefetch;
 908          if (use_stride) __ mov(stride, prefetch);
 909       }
 910 
 911       __ bind(again);
 912 
 913       if (PrefetchCopyIntervalInBytes &gt; 0)
 914         __ prfm(use_stride ? Address(s, stride) : Address(s, prefetch), PLDL1KEEP);
 915 
 916       if (direction == copy_forwards) {
 917        // allowing for the offset of -8 the store instructions place
 918        // registers into the target 64 bit block at the following
 919        // offsets
 920        //
 921        // t0 at offset 0
 922        // t1 at offset 8,  t2 at offset 16
 923        // t3 at offset 24, t4 at offset 32
 924        // t5 at offset 40, t6 at offset 48
 925        // t7 at offset 56
 926 
 927         __ str(t0, Address(d, 1 * unit));
 928         __ stp(t1, t2, Address(d, 2 * unit));
 929         __ ldp(t0, t1, Address(s, 2 * unit));
 930         __ stp(t3, t4, Address(d, 4 * unit));
 931         __ ldp(t2, t3, Address(s, 4 * unit));
 932         __ stp(t5, t6, Address(d, 6 * unit));
 933         __ ldp(t4, t5, Address(s, 6 * unit));
 934         __ str(t7, Address(__ pre(d, 8 * unit)));
 935         __ ldp(t6, t7, Address(__ pre(s, 8 * unit)));
 936       } else {
 937        // d was not offset when we started so the registers are
 938        // written into the 64 bit block preceding d with the following
 939        // offsets
 940        //
 941        // t1 at offset -8
 942        // t3 at offset -24, t0 at offset -16
 943        // t5 at offset -48, t2 at offset -32
 944        // t7 at offset -56, t4 at offset -48
 945        //                   t6 at offset -64
 946        //
 947        // note that this matches the offsets previously noted for the
 948        // loads
 949 
 950         __ str(t1, Address(d, 1 * unit));
 951         __ stp(t3, t0, Address(d, 3 * unit));
 952         __ ldp(t0, t1, Address(s, 2 * unit));
 953         __ stp(t5, t2, Address(d, 5 * unit));
 954         __ ldp(t2, t3, Address(s, 4 * unit));
 955         __ stp(t7, t4, Address(d, 7 * unit));
 956         __ ldp(t4, t5, Address(s, 6 * unit));
 957         __ str(t6, Address(__ pre(d, 8 * unit)));
 958         __ ldp(t6, t7, Address(__ pre(s, 8 * unit)));
 959       }
 960 
 961       __ subs(count, count, 8);
 962       __ br(Assembler::HS, again);
 963 
 964       // Drain
 965       //
 966       // this uses the same pattern of offsets and register arguments
 967       // as above
 968       __ bind(drain);
 969       if (direction == copy_forwards) {
 970         __ str(t0, Address(d, 1 * unit));
 971         __ stp(t1, t2, Address(d, 2 * unit));
 972         __ stp(t3, t4, Address(d, 4 * unit));
 973         __ stp(t5, t6, Address(d, 6 * unit));
 974         __ str(t7, Address(__ pre(d, 8 * unit)));
 975       } else {
 976         __ str(t1, Address(d, 1 * unit));
 977         __ stp(t3, t0, Address(d, 3 * unit));
 978         __ stp(t5, t2, Address(d, 5 * unit));
 979         __ stp(t7, t4, Address(d, 7 * unit));
 980         __ str(t6, Address(__ pre(d, 8 * unit)));
 981       }
 982       // now we need to copy any remaining part block which may
 983       // include a 4 word block subblock and/or a 2 word subblock.
 984       // bits 2 and 1 in the count are the tell-tale for whetehr we
 985       // have each such subblock
 986       {
 987         Label L1, L2;
 988         __ tbz(count, exact_log2(4), L1);
 989        // this is the same as above but copying only 4 longs hence
 990        // with ony one intervening stp between the str instructions
 991        // but note that the offsets and registers still follow the
 992        // same pattern
 993         __ ldp(t0, t1, Address(s, 2 * unit));
 994         __ ldp(t2, t3, Address(__ pre(s, 4 * unit)));
 995         if (direction == copy_forwards) {
 996           __ str(t0, Address(d, 1 * unit));
 997           __ stp(t1, t2, Address(d, 2 * unit));
 998           __ str(t3, Address(__ pre(d, 4 * unit)));
 999         } else {
1000           __ str(t1, Address(d, 1 * unit));
1001           __ stp(t3, t0, Address(d, 3 * unit));
1002           __ str(t2, Address(__ pre(d, 4 * unit)));
1003         }
1004         __ bind(L1);
1005 
1006         __ tbz(count, 1, L2);
1007        // this is the same as above but copying only 2 longs hence
1008        // there is no intervening stp between the str instructions
1009        // but note that the offset and register patterns are still
1010        // the same
1011         __ ldp(t0, t1, Address(__ pre(s, 2 * unit)));
1012         if (direction == copy_forwards) {
1013           __ str(t0, Address(d, 1 * unit));
1014           __ str(t1, Address(__ pre(d, 2 * unit)));
1015         } else {
1016           __ str(t1, Address(d, 1 * unit));
1017           __ str(t0, Address(__ pre(d, 2 * unit)));
1018         }
1019         __ bind(L2);
1020 
1021        // for forwards copy we need to re-adjust the offsets we
1022        // applied so that s and d are follow the last words written
1023 
1024        if (direction == copy_forwards) {
1025          __ add(s, s, 16);
1026          __ add(d, d, 8);
1027        }
1028 
1029       }
1030 
1031       __ ret(lr);
1032       }
1033   }
1034 
1035   // Small copy: less than 16 bytes.
1036   //
1037   // NB: Ignores all of the bits of count which represent more than 15
1038   // bytes, so a caller doesn&#39;t have to mask them.
1039 
1040   void copy_memory_small(Register s, Register d, Register count, Register tmp, int step) {
1041     bool is_backwards = step &lt; 0;
1042     size_t granularity = uabs(step);
1043     int direction = is_backwards ? -1 : 1;
1044     int unit = wordSize * direction;
1045 
1046     Label Lword, Lint, Lshort, Lbyte;
1047 
1048     assert(granularity
1049            &amp;&amp; granularity &lt;= sizeof (jlong), &quot;Impossible granularity in copy_memory_small&quot;);
1050 
1051     const Register t0 = r3, t1 = r4, t2 = r5, t3 = r6;
1052 
1053     // ??? I don&#39;t know if this bit-test-and-branch is the right thing
1054     // to do.  It does a lot of jumping, resulting in several
1055     // mispredicted branches.  It might make more sense to do this
1056     // with something like Duff&#39;s device with a single computed branch.
1057 
1058     __ tbz(count, 3 - exact_log2(granularity), Lword);
1059     __ ldr(tmp, Address(__ adjust(s, unit, is_backwards)));
1060     __ str(tmp, Address(__ adjust(d, unit, is_backwards)));
1061     __ bind(Lword);
1062 
1063     if (granularity &lt;= sizeof (jint)) {
1064       __ tbz(count, 2 - exact_log2(granularity), Lint);
1065       __ ldrw(tmp, Address(__ adjust(s, sizeof (jint) * direction, is_backwards)));
1066       __ strw(tmp, Address(__ adjust(d, sizeof (jint) * direction, is_backwards)));
1067       __ bind(Lint);
1068     }
1069 
1070     if (granularity &lt;= sizeof (jshort)) {
1071       __ tbz(count, 1 - exact_log2(granularity), Lshort);
1072       __ ldrh(tmp, Address(__ adjust(s, sizeof (jshort) * direction, is_backwards)));
1073       __ strh(tmp, Address(__ adjust(d, sizeof (jshort) * direction, is_backwards)));
1074       __ bind(Lshort);
1075     }
1076 
1077     if (granularity &lt;= sizeof (jbyte)) {
1078       __ tbz(count, 0, Lbyte);
1079       __ ldrb(tmp, Address(__ adjust(s, sizeof (jbyte) * direction, is_backwards)));
1080       __ strb(tmp, Address(__ adjust(d, sizeof (jbyte) * direction, is_backwards)));
1081       __ bind(Lbyte);
1082     }
1083   }
1084 
1085   Label copy_f, copy_b;
1086 
1087   // All-singing all-dancing memory copy.
1088   //
1089   // Copy count units of memory from s to d.  The size of a unit is
1090   // step, which can be positive or negative depending on the direction
1091   // of copy.  If is_aligned is false, we align the source address.
1092   //
1093 
1094   void copy_memory(bool is_aligned, Register s, Register d,
1095                    Register count, Register tmp, int step) {
1096     copy_direction direction = step &lt; 0 ? copy_backwards : copy_forwards;
1097     bool is_backwards = step &lt; 0;
1098     int granularity = uabs(step);
1099     const Register t0 = r3, t1 = r4;
1100 
1101     // &lt;= 96 bytes do inline. Direction doesn&#39;t matter because we always
1102     // load all the data before writing anything
1103     Label copy4, copy8, copy16, copy32, copy80, copy_big, finish;
1104     const Register t2 = r5, t3 = r6, t4 = r7, t5 = r8;
1105     const Register t6 = r9, t7 = r10, t8 = r11, t9 = r12;
1106     const Register send = r17, dend = r18;
1107 
1108     if (PrefetchCopyIntervalInBytes &gt; 0)
1109       __ prfm(Address(s, 0), PLDL1KEEP);
1110     __ cmp(count, u1((UseSIMDForMemoryOps ? 96:80)/granularity));
1111     __ br(Assembler::HI, copy_big);
1112 
1113     __ lea(send, Address(s, count, Address::lsl(exact_log2(granularity))));
1114     __ lea(dend, Address(d, count, Address::lsl(exact_log2(granularity))));
1115 
1116     __ cmp(count, u1(16/granularity));
1117     __ br(Assembler::LS, copy16);
1118 
1119     __ cmp(count, u1(64/granularity));
1120     __ br(Assembler::HI, copy80);
1121 
1122     __ cmp(count, u1(32/granularity));
1123     __ br(Assembler::LS, copy32);
1124 
1125     // 33..64 bytes
1126     if (UseSIMDForMemoryOps) {
1127       __ ldpq(v0, v1, Address(s, 0));
1128       __ ldpq(v2, v3, Address(send, -32));
1129       __ stpq(v0, v1, Address(d, 0));
1130       __ stpq(v2, v3, Address(dend, -32));
1131     } else {
1132       __ ldp(t0, t1, Address(s, 0));
1133       __ ldp(t2, t3, Address(s, 16));
1134       __ ldp(t4, t5, Address(send, -32));
1135       __ ldp(t6, t7, Address(send, -16));
1136 
1137       __ stp(t0, t1, Address(d, 0));
1138       __ stp(t2, t3, Address(d, 16));
1139       __ stp(t4, t5, Address(dend, -32));
1140       __ stp(t6, t7, Address(dend, -16));
1141     }
1142     __ b(finish);
1143 
1144     // 17..32 bytes
1145     __ bind(copy32);
1146     __ ldp(t0, t1, Address(s, 0));
1147     __ ldp(t2, t3, Address(send, -16));
1148     __ stp(t0, t1, Address(d, 0));
1149     __ stp(t2, t3, Address(dend, -16));
1150     __ b(finish);
1151 
1152     // 65..80/96 bytes
1153     // (96 bytes if SIMD because we do 32 byes per instruction)
1154     __ bind(copy80);
1155     if (UseSIMDForMemoryOps) {
1156       __ ld4(v0, v1, v2, v3, __ T16B, Address(s, 0));
1157       __ ldpq(v4, v5, Address(send, -32));
1158       __ st4(v0, v1, v2, v3, __ T16B, Address(d, 0));
1159       __ stpq(v4, v5, Address(dend, -32));
1160     } else {
1161       __ ldp(t0, t1, Address(s, 0));
1162       __ ldp(t2, t3, Address(s, 16));
1163       __ ldp(t4, t5, Address(s, 32));
1164       __ ldp(t6, t7, Address(s, 48));
1165       __ ldp(t8, t9, Address(send, -16));
1166 
1167       __ stp(t0, t1, Address(d, 0));
1168       __ stp(t2, t3, Address(d, 16));
1169       __ stp(t4, t5, Address(d, 32));
1170       __ stp(t6, t7, Address(d, 48));
1171       __ stp(t8, t9, Address(dend, -16));
1172     }
1173     __ b(finish);
1174 
1175     // 0..16 bytes
1176     __ bind(copy16);
1177     __ cmp(count, u1(8/granularity));
1178     __ br(Assembler::LO, copy8);
1179 
1180     // 8..16 bytes
1181     __ ldr(t0, Address(s, 0));
1182     __ ldr(t1, Address(send, -8));
1183     __ str(t0, Address(d, 0));
1184     __ str(t1, Address(dend, -8));
1185     __ b(finish);
1186 
1187     if (granularity &lt; 8) {
1188       // 4..7 bytes
1189       __ bind(copy8);
1190       __ tbz(count, 2 - exact_log2(granularity), copy4);
1191       __ ldrw(t0, Address(s, 0));
1192       __ ldrw(t1, Address(send, -4));
1193       __ strw(t0, Address(d, 0));
1194       __ strw(t1, Address(dend, -4));
1195       __ b(finish);
1196       if (granularity &lt; 4) {
1197         // 0..3 bytes
1198         __ bind(copy4);
1199         __ cbz(count, finish); // get rid of 0 case
1200         if (granularity == 2) {
1201           __ ldrh(t0, Address(s, 0));
1202           __ strh(t0, Address(d, 0));
1203         } else { // granularity == 1
1204           // Now 1..3 bytes. Handle the 1 and 2 byte case by copying
1205           // the first and last byte.
1206           // Handle the 3 byte case by loading and storing base + count/2
1207           // (count == 1 (s+0)-&gt;(d+0), count == 2,3 (s+1) -&gt; (d+1))
1208           // This does means in the 1 byte case we load/store the same
1209           // byte 3 times.
1210           __ lsr(count, count, 1);
1211           __ ldrb(t0, Address(s, 0));
1212           __ ldrb(t1, Address(send, -1));
1213           __ ldrb(t2, Address(s, count));
1214           __ strb(t0, Address(d, 0));
1215           __ strb(t1, Address(dend, -1));
1216           __ strb(t2, Address(d, count));
1217         }
1218         __ b(finish);
1219       }
1220     }
1221 
1222     __ bind(copy_big);
1223     if (is_backwards) {
1224       __ lea(s, Address(s, count, Address::lsl(exact_log2(-step))));
1225       __ lea(d, Address(d, count, Address::lsl(exact_log2(-step))));
1226     }
1227 
1228     // Now we&#39;ve got the small case out of the way we can align the
1229     // source address on a 2-word boundary.
1230 
1231     Label aligned;
1232 
1233     if (is_aligned) {
1234       // We may have to adjust by 1 word to get s 2-word-aligned.
1235       __ tbz(s, exact_log2(wordSize), aligned);
1236       __ ldr(tmp, Address(__ adjust(s, direction * wordSize, is_backwards)));
1237       __ str(tmp, Address(__ adjust(d, direction * wordSize, is_backwards)));
1238       __ sub(count, count, wordSize/granularity);
1239     } else {
1240       if (is_backwards) {
1241         __ andr(rscratch2, s, 2 * wordSize - 1);
1242       } else {
1243         __ neg(rscratch2, s);
1244         __ andr(rscratch2, rscratch2, 2 * wordSize - 1);
1245       }
1246       // rscratch2 is the byte adjustment needed to align s.
1247       __ cbz(rscratch2, aligned);
1248       int shift = exact_log2(granularity);
1249       if (shift)  __ lsr(rscratch2, rscratch2, shift);
1250       __ sub(count, count, rscratch2);
1251 
1252 #if 0
1253       // ?? This code is only correct for a disjoint copy.  It may or
1254       // may not make sense to use it in that case.
1255 
1256       // Copy the first pair; s and d may not be aligned.
1257       __ ldp(t0, t1, Address(s, is_backwards ? -2 * wordSize : 0));
1258       __ stp(t0, t1, Address(d, is_backwards ? -2 * wordSize : 0));
1259 
1260       // Align s and d, adjust count
1261       if (is_backwards) {
1262         __ sub(s, s, rscratch2);
1263         __ sub(d, d, rscratch2);
1264       } else {
1265         __ add(s, s, rscratch2);
1266         __ add(d, d, rscratch2);
1267       }
1268 #else
1269       copy_memory_small(s, d, rscratch2, rscratch1, step);
1270 #endif
1271     }
1272 
1273     __ bind(aligned);
1274 
1275     // s is now 2-word-aligned.
1276 
1277     // We have a count of units and some trailing bytes.  Adjust the
1278     // count and do a bulk copy of words.
1279     __ lsr(rscratch2, count, exact_log2(wordSize/granularity));
1280     if (direction == copy_forwards)
1281       __ bl(copy_f);
1282     else
1283       __ bl(copy_b);
1284 
1285     // And the tail.
1286     copy_memory_small(s, d, count, tmp, step);
1287 
1288     if (granularity &gt;= 8) __ bind(copy8);
1289     if (granularity &gt;= 4) __ bind(copy4);
1290     __ bind(finish);
1291   }
1292 
1293 
1294   void clobber_registers() {
1295 #ifdef ASSERT
1296     __ mov(rscratch1, (uint64_t)0xdeadbeef);
1297     __ orr(rscratch1, rscratch1, rscratch1, Assembler::LSL, 32);
1298     for (Register r = r3; r &lt;= r18; r++)
1299       if (r != rscratch1) __ mov(r, rscratch1);
1300 #endif
1301   }
1302 
1303   // Scan over array at a for count oops, verifying each one.
1304   // Preserves a and count, clobbers rscratch1 and rscratch2.
1305   void verify_oop_array (size_t size, Register a, Register count, Register temp) {
1306     Label loop, end;
1307     __ mov(rscratch1, a);
1308     __ mov(rscratch2, zr);
1309     __ bind(loop);
1310     __ cmp(rscratch2, count);
1311     __ br(Assembler::HS, end);
1312     if (size == (size_t)wordSize) {
1313       __ ldr(temp, Address(a, rscratch2, Address::lsl(exact_log2(size))));
1314       __ verify_oop(temp);
1315     } else {
1316       __ ldrw(r16, Address(a, rscratch2, Address::lsl(exact_log2(size))));
1317       __ decode_heap_oop(temp); // calls verify_oop
1318     }
1319     __ add(rscratch2, rscratch2, size);
1320     __ b(loop);
1321     __ bind(end);
1322   }
1323 
1324   // Arguments:
1325   //   aligned - true =&gt; Input and output aligned on a HeapWord == 8-byte boundary
1326   //             ignored
1327   //   is_oop  - true =&gt; oop array, so generate store check code
1328   //   name    - stub name string
1329   //
1330   // Inputs:
1331   //   c_rarg0   - source array address
1332   //   c_rarg1   - destination array address
1333   //   c_rarg2   - element count, treated as ssize_t, can be zero
1334   //
1335   // If &#39;from&#39; and/or &#39;to&#39; are aligned on 4-byte boundaries, we let
1336   // the hardware handle it.  The two dwords within qwords that span
1337   // cache line boundaries will still be loaded and stored atomicly.
1338   //
1339   // Side Effects:
1340   //   disjoint_int_copy_entry is set to the no-overlap entry point
1341   //   used by generate_conjoint_int_oop_copy().
1342   //
1343   address generate_disjoint_copy(size_t size, bool aligned, bool is_oop, address *entry,
1344                                   const char *name, bool dest_uninitialized = false) {
1345     Register s = c_rarg0, d = c_rarg1, count = c_rarg2;
1346     RegSet saved_reg = RegSet::of(s, d, count);
1347     __ align(CodeEntryAlignment);
1348     StubCodeMark mark(this, &quot;StubRoutines&quot;, name);
1349     address start = __ pc();
1350     __ enter();
1351 
1352     if (entry != NULL) {
1353       *entry = __ pc();
1354       // caller can pass a 64-bit byte count here (from Unsafe.copyMemory)
1355       BLOCK_COMMENT(&quot;Entry:&quot;);
1356     }
1357 
1358     DecoratorSet decorators = IN_HEAP | IS_ARRAY | ARRAYCOPY_DISJOINT;
1359     if (dest_uninitialized) {
1360       decorators |= IS_DEST_UNINITIALIZED;
1361     }
1362     if (aligned) {
1363       decorators |= ARRAYCOPY_ALIGNED;
1364     }
1365 
1366     BarrierSetAssembler *bs = BarrierSet::barrier_set()-&gt;barrier_set_assembler();
1367     bs-&gt;arraycopy_prologue(_masm, decorators, is_oop, s, d, count, saved_reg);
1368 
1369     if (is_oop) {
1370       // save regs before copy_memory
1371       __ push(RegSet::of(d, count), sp);
1372     }
1373     {
1374       // UnsafeCopyMemory page error: continue after ucm
1375       bool add_entry = !is_oop &amp;&amp; (!aligned || sizeof(jlong) == size);
1376       UnsafeCopyMemoryMark ucmm(this, add_entry, true);
1377       copy_memory(aligned, s, d, count, rscratch1, size);
1378     }
1379 
1380     if (is_oop) {
1381       __ pop(RegSet::of(d, count), sp);
1382       if (VerifyOops)
1383         verify_oop_array(size, d, count, r16);
1384     }
1385 
1386     bs-&gt;arraycopy_epilogue(_masm, decorators, is_oop, d, count, rscratch1, RegSet());
1387 
1388     __ leave();
1389     __ mov(r0, zr); // return 0
1390     __ ret(lr);
1391     return start;
1392   }
1393 
1394   // Arguments:
1395   //   aligned - true =&gt; Input and output aligned on a HeapWord == 8-byte boundary
1396   //             ignored
1397   //   is_oop  - true =&gt; oop array, so generate store check code
1398   //   name    - stub name string
1399   //
1400   // Inputs:
1401   //   c_rarg0   - source array address
1402   //   c_rarg1   - destination array address
1403   //   c_rarg2   - element count, treated as ssize_t, can be zero
1404   //
1405   // If &#39;from&#39; and/or &#39;to&#39; are aligned on 4-byte boundaries, we let
1406   // the hardware handle it.  The two dwords within qwords that span
1407   // cache line boundaries will still be loaded and stored atomicly.
1408   //
1409   address generate_conjoint_copy(size_t size, bool aligned, bool is_oop, address nooverlap_target,
1410                                  address *entry, const char *name,
1411                                  bool dest_uninitialized = false) {
1412     Register s = c_rarg0, d = c_rarg1, count = c_rarg2;
1413     RegSet saved_regs = RegSet::of(s, d, count);
1414     StubCodeMark mark(this, &quot;StubRoutines&quot;, name);
1415     address start = __ pc();
1416     __ enter();
1417 
1418     if (entry != NULL) {
1419       *entry = __ pc();
1420       // caller can pass a 64-bit byte count here (from Unsafe.copyMemory)
1421       BLOCK_COMMENT(&quot;Entry:&quot;);
1422     }
1423 
1424     // use fwd copy when (d-s) above_equal (count*size)
1425     __ sub(rscratch1, d, s);
1426     __ cmp(rscratch1, count, Assembler::LSL, exact_log2(size));
1427     __ br(Assembler::HS, nooverlap_target);
1428 
1429     DecoratorSet decorators = IN_HEAP | IS_ARRAY;
1430     if (dest_uninitialized) {
1431       decorators |= IS_DEST_UNINITIALIZED;
1432     }
1433     if (aligned) {
1434       decorators |= ARRAYCOPY_ALIGNED;
1435     }
1436 
1437     BarrierSetAssembler *bs = BarrierSet::barrier_set()-&gt;barrier_set_assembler();
1438     bs-&gt;arraycopy_prologue(_masm, decorators, is_oop, s, d, count, saved_regs);
1439 
1440     if (is_oop) {
1441       // save regs before copy_memory
1442       __ push(RegSet::of(d, count), sp);
1443     }
1444     {
1445       // UnsafeCopyMemory page error: continue after ucm
1446       bool add_entry = !is_oop &amp;&amp; (!aligned || sizeof(jlong) == size);
1447       UnsafeCopyMemoryMark ucmm(this, add_entry, true);
1448       copy_memory(aligned, s, d, count, rscratch1, -size);
1449     }
1450     if (is_oop) {
1451       __ pop(RegSet::of(d, count), sp);
1452       if (VerifyOops)
1453         verify_oop_array(size, d, count, r16);
1454     }
1455     bs-&gt;arraycopy_epilogue(_masm, decorators, is_oop, d, count, rscratch1, RegSet());
1456     __ leave();
1457     __ mov(r0, zr); // return 0
1458     __ ret(lr);
1459     return start;
1460 }
1461 
1462   // Arguments:
1463   //   aligned - true =&gt; Input and output aligned on a HeapWord == 8-byte boundary
1464   //             ignored
1465   //   name    - stub name string
1466   //
1467   // Inputs:
1468   //   c_rarg0   - source array address
1469   //   c_rarg1   - destination array address
1470   //   c_rarg2   - element count, treated as ssize_t, can be zero
1471   //
1472   // If &#39;from&#39; and/or &#39;to&#39; are aligned on 4-, 2-, or 1-byte boundaries,
1473   // we let the hardware handle it.  The one to eight bytes within words,
1474   // dwords or qwords that span cache line boundaries will still be loaded
1475   // and stored atomically.
1476   //
1477   // Side Effects:
1478   //   disjoint_byte_copy_entry is set to the no-overlap entry point  //
1479   // If &#39;from&#39; and/or &#39;to&#39; are aligned on 4-, 2-, or 1-byte boundaries,
1480   // we let the hardware handle it.  The one to eight bytes within words,
1481   // dwords or qwords that span cache line boundaries will still be loaded
1482   // and stored atomically.
1483   //
1484   // Side Effects:
1485   //   disjoint_byte_copy_entry is set to the no-overlap entry point
1486   //   used by generate_conjoint_byte_copy().
1487   //
1488   address generate_disjoint_byte_copy(bool aligned, address* entry, const char *name) {
1489     const bool not_oop = false;
1490     return generate_disjoint_copy(sizeof (jbyte), aligned, not_oop, entry, name);
1491   }
1492 
1493   // Arguments:
1494   //   aligned - true =&gt; Input and output aligned on a HeapWord == 8-byte boundary
1495   //             ignored
1496   //   name    - stub name string
1497   //
1498   // Inputs:
1499   //   c_rarg0   - source array address
1500   //   c_rarg1   - destination array address
1501   //   c_rarg2   - element count, treated as ssize_t, can be zero
1502   //
1503   // If &#39;from&#39; and/or &#39;to&#39; are aligned on 4-, 2-, or 1-byte boundaries,
1504   // we let the hardware handle it.  The one to eight bytes within words,
1505   // dwords or qwords that span cache line boundaries will still be loaded
1506   // and stored atomically.
1507   //
1508   address generate_conjoint_byte_copy(bool aligned, address nooverlap_target,
1509                                       address* entry, const char *name) {
1510     const bool not_oop = false;
1511     return generate_conjoint_copy(sizeof (jbyte), aligned, not_oop, nooverlap_target, entry, name);
1512   }
1513 
1514   // Arguments:
1515   //   aligned - true =&gt; Input and output aligned on a HeapWord == 8-byte boundary
1516   //             ignored
1517   //   name    - stub name string
1518   //
1519   // Inputs:
1520   //   c_rarg0   - source array address
1521   //   c_rarg1   - destination array address
1522   //   c_rarg2   - element count, treated as ssize_t, can be zero
1523   //
1524   // If &#39;from&#39; and/or &#39;to&#39; are aligned on 4- or 2-byte boundaries, we
1525   // let the hardware handle it.  The two or four words within dwords
1526   // or qwords that span cache line boundaries will still be loaded
1527   // and stored atomically.
1528   //
1529   // Side Effects:
1530   //   disjoint_short_copy_entry is set to the no-overlap entry point
1531   //   used by generate_conjoint_short_copy().
1532   //
1533   address generate_disjoint_short_copy(bool aligned,
1534                                        address* entry, const char *name) {
1535     const bool not_oop = false;
1536     return generate_disjoint_copy(sizeof (jshort), aligned, not_oop, entry, name);
1537   }
1538 
1539   // Arguments:
1540   //   aligned - true =&gt; Input and output aligned on a HeapWord == 8-byte boundary
1541   //             ignored
1542   //   name    - stub name string
1543   //
1544   // Inputs:
1545   //   c_rarg0   - source array address
1546   //   c_rarg1   - destination array address
1547   //   c_rarg2   - element count, treated as ssize_t, can be zero
1548   //
1549   // If &#39;from&#39; and/or &#39;to&#39; are aligned on 4- or 2-byte boundaries, we
1550   // let the hardware handle it.  The two or four words within dwords
1551   // or qwords that span cache line boundaries will still be loaded
1552   // and stored atomically.
1553   //
1554   address generate_conjoint_short_copy(bool aligned, address nooverlap_target,
1555                                        address *entry, const char *name) {
1556     const bool not_oop = false;
1557     return generate_conjoint_copy(sizeof (jshort), aligned, not_oop, nooverlap_target, entry, name);
1558 
1559   }
1560   // Arguments:
1561   //   aligned - true =&gt; Input and output aligned on a HeapWord == 8-byte boundary
1562   //             ignored
1563   //   name    - stub name string
1564   //
1565   // Inputs:
1566   //   c_rarg0   - source array address
1567   //   c_rarg1   - destination array address
1568   //   c_rarg2   - element count, treated as ssize_t, can be zero
1569   //
1570   // If &#39;from&#39; and/or &#39;to&#39; are aligned on 4-byte boundaries, we let
1571   // the hardware handle it.  The two dwords within qwords that span
1572   // cache line boundaries will still be loaded and stored atomicly.
1573   //
1574   // Side Effects:
1575   //   disjoint_int_copy_entry is set to the no-overlap entry point
1576   //   used by generate_conjoint_int_oop_copy().
1577   //
1578   address generate_disjoint_int_copy(bool aligned, address *entry,
1579                                          const char *name, bool dest_uninitialized = false) {
1580     const bool not_oop = false;
1581     return generate_disjoint_copy(sizeof (jint), aligned, not_oop, entry, name);
1582   }
1583 
1584   // Arguments:
1585   //   aligned - true =&gt; Input and output aligned on a HeapWord == 8-byte boundary
1586   //             ignored
1587   //   name    - stub name string
1588   //
1589   // Inputs:
1590   //   c_rarg0   - source array address
1591   //   c_rarg1   - destination array address
1592   //   c_rarg2   - element count, treated as ssize_t, can be zero
1593   //
1594   // If &#39;from&#39; and/or &#39;to&#39; are aligned on 4-byte boundaries, we let
1595   // the hardware handle it.  The two dwords within qwords that span
1596   // cache line boundaries will still be loaded and stored atomicly.
1597   //
1598   address generate_conjoint_int_copy(bool aligned, address nooverlap_target,
1599                                      address *entry, const char *name,
1600                                      bool dest_uninitialized = false) {
1601     const bool not_oop = false;
1602     return generate_conjoint_copy(sizeof (jint), aligned, not_oop, nooverlap_target, entry, name);
1603   }
1604 
1605 
1606   // Arguments:
1607   //   aligned - true =&gt; Input and output aligned on a HeapWord boundary == 8 bytes
1608   //             ignored
1609   //   name    - stub name string
1610   //
1611   // Inputs:
1612   //   c_rarg0   - source array address
1613   //   c_rarg1   - destination array address
1614   //   c_rarg2   - element count, treated as size_t, can be zero
1615   //
1616   // Side Effects:
1617   //   disjoint_oop_copy_entry or disjoint_long_copy_entry is set to the
1618   //   no-overlap entry point used by generate_conjoint_long_oop_copy().
1619   //
1620   address generate_disjoint_long_copy(bool aligned, address *entry,
1621                                           const char *name, bool dest_uninitialized = false) {
1622     const bool not_oop = false;
1623     return generate_disjoint_copy(sizeof (jlong), aligned, not_oop, entry, name);
1624   }
1625 
1626   // Arguments:
1627   //   aligned - true =&gt; Input and output aligned on a HeapWord boundary == 8 bytes
1628   //             ignored
1629   //   name    - stub name string
1630   //
1631   // Inputs:
1632   //   c_rarg0   - source array address
1633   //   c_rarg1   - destination array address
1634   //   c_rarg2   - element count, treated as size_t, can be zero
1635   //
1636   address generate_conjoint_long_copy(bool aligned,
1637                                       address nooverlap_target, address *entry,
1638                                       const char *name, bool dest_uninitialized = false) {
1639     const bool not_oop = false;
1640     return generate_conjoint_copy(sizeof (jlong), aligned, not_oop, nooverlap_target, entry, name);
1641   }
1642 
1643   // Arguments:
1644   //   aligned - true =&gt; Input and output aligned on a HeapWord boundary == 8 bytes
1645   //             ignored
1646   //   name    - stub name string
1647   //
1648   // Inputs:
1649   //   c_rarg0   - source array address
1650   //   c_rarg1   - destination array address
1651   //   c_rarg2   - element count, treated as size_t, can be zero
1652   //
1653   // Side Effects:
1654   //   disjoint_oop_copy_entry or disjoint_long_copy_entry is set to the
1655   //   no-overlap entry point used by generate_conjoint_long_oop_copy().
1656   //
1657   address generate_disjoint_oop_copy(bool aligned, address *entry,
1658                                      const char *name, bool dest_uninitialized) {
1659     const bool is_oop = true;
1660     const size_t size = UseCompressedOops ? sizeof (jint) : sizeof (jlong);
1661     return generate_disjoint_copy(size, aligned, is_oop, entry, name, dest_uninitialized);
1662   }
1663 
1664   // Arguments:
1665   //   aligned - true =&gt; Input and output aligned on a HeapWord boundary == 8 bytes
1666   //             ignored
1667   //   name    - stub name string
1668   //
1669   // Inputs:
1670   //   c_rarg0   - source array address
1671   //   c_rarg1   - destination array address
1672   //   c_rarg2   - element count, treated as size_t, can be zero
1673   //
1674   address generate_conjoint_oop_copy(bool aligned,
1675                                      address nooverlap_target, address *entry,
1676                                      const char *name, bool dest_uninitialized) {
1677     const bool is_oop = true;
1678     const size_t size = UseCompressedOops ? sizeof (jint) : sizeof (jlong);
1679     return generate_conjoint_copy(size, aligned, is_oop, nooverlap_target, entry,
1680                                   name, dest_uninitialized);
1681   }
1682 
1683 
1684   // Helper for generating a dynamic type check.
1685   // Smashes rscratch1, rscratch2.
1686   void generate_type_check(Register sub_klass,
1687                            Register super_check_offset,
1688                            Register super_klass,
1689                            Label&amp; L_success) {
1690     assert_different_registers(sub_klass, super_check_offset, super_klass);
1691 
1692     BLOCK_COMMENT(&quot;type_check:&quot;);
1693 
1694     Label L_miss;
1695 
1696     __ check_klass_subtype_fast_path(sub_klass, super_klass, noreg,        &amp;L_success, &amp;L_miss, NULL,
1697                                      super_check_offset);
1698     __ check_klass_subtype_slow_path(sub_klass, super_klass, noreg, noreg, &amp;L_success, NULL);
1699 
1700     // Fall through on failure!
1701     __ BIND(L_miss);
1702   }
1703 
1704   //
1705   //  Generate checkcasting array copy stub
1706   //
1707   //  Input:
1708   //    c_rarg0   - source array address
1709   //    c_rarg1   - destination array address
1710   //    c_rarg2   - element count, treated as ssize_t, can be zero
1711   //    c_rarg3   - size_t ckoff (super_check_offset)
1712   //    c_rarg4   - oop ckval (super_klass)
1713   //
1714   //  Output:
1715   //    r0 ==  0  -  success
1716   //    r0 == -1^K - failure, where K is partial transfer count
1717   //
1718   address generate_checkcast_copy(const char *name, address *entry,
1719                                   bool dest_uninitialized = false) {
1720 
1721     Label L_load_element, L_store_element, L_do_card_marks, L_done, L_done_pop;
1722 
1723     // Input registers (after setup_arg_regs)
1724     const Register from        = c_rarg0;   // source array address
1725     const Register to          = c_rarg1;   // destination array address
1726     const Register count       = c_rarg2;   // elementscount
1727     const Register ckoff       = c_rarg3;   // super_check_offset
1728     const Register ckval       = c_rarg4;   // super_klass
1729 
1730     RegSet wb_pre_saved_regs = RegSet::range(c_rarg0, c_rarg4);
1731     RegSet wb_post_saved_regs = RegSet::of(count);
1732 
1733     // Registers used as temps (r18, r19, r20 are save-on-entry)
1734     const Register count_save  = r21;       // orig elementscount
1735     const Register start_to    = r20;       // destination array start address
1736     const Register copied_oop  = r18;       // actual oop copied
1737     const Register r19_klass   = r19;       // oop._klass
1738 
1739     //---------------------------------------------------------------
1740     // Assembler stub will be used for this call to arraycopy
1741     // if the two arrays are subtypes of Object[] but the
1742     // destination array type is not equal to or a supertype
1743     // of the source type.  Each element must be separately
1744     // checked.
1745 
1746     assert_different_registers(from, to, count, ckoff, ckval, start_to,
1747                                copied_oop, r19_klass, count_save);
1748 
1749     __ align(CodeEntryAlignment);
1750     StubCodeMark mark(this, &quot;StubRoutines&quot;, name);
1751     address start = __ pc();
1752 
1753     __ enter(); // required for proper stackwalking of RuntimeStub frame
1754 
1755 #ifdef ASSERT
1756     // caller guarantees that the arrays really are different
1757     // otherwise, we would have to make conjoint checks
1758     { Label L;
1759       array_overlap_test(L, TIMES_OOP);
1760       __ stop(&quot;checkcast_copy within a single array&quot;);
1761       __ bind(L);
1762     }
1763 #endif //ASSERT
1764 
1765     // Caller of this entry point must set up the argument registers.
1766     if (entry != NULL) {
1767       *entry = __ pc();
1768       BLOCK_COMMENT(&quot;Entry:&quot;);
1769     }
1770 
1771      // Empty array:  Nothing to do.
1772     __ cbz(count, L_done);
1773 
1774     __ push(RegSet::of(r18, r19, r20, r21), sp);
1775 
1776 #ifdef ASSERT
1777     BLOCK_COMMENT(&quot;assert consistent ckoff/ckval&quot;);
1778     // The ckoff and ckval must be mutually consistent,
1779     // even though caller generates both.
1780     { Label L;
1781       int sco_offset = in_bytes(Klass::super_check_offset_offset());
1782       __ ldrw(start_to, Address(ckval, sco_offset));
1783       __ cmpw(ckoff, start_to);
1784       __ br(Assembler::EQ, L);
1785       __ stop(&quot;super_check_offset inconsistent&quot;);
1786       __ bind(L);
1787     }
1788 #endif //ASSERT
1789 
1790     DecoratorSet decorators = IN_HEAP | IS_ARRAY | ARRAYCOPY_CHECKCAST | ARRAYCOPY_DISJOINT;
1791     bool is_oop = true;
1792     if (dest_uninitialized) {
1793       decorators |= IS_DEST_UNINITIALIZED;
1794     }
1795 
1796     BarrierSetAssembler *bs = BarrierSet::barrier_set()-&gt;barrier_set_assembler();
1797     bs-&gt;arraycopy_prologue(_masm, decorators, is_oop, from, to, count, wb_pre_saved_regs);
1798 
1799     // save the original count
1800     __ mov(count_save, count);
1801 
1802     // Copy from low to high addresses
1803     __ mov(start_to, to);              // Save destination array start address
1804     __ b(L_load_element);
1805 
1806     // ======== begin loop ========
1807     // (Loop is rotated; its entry is L_load_element.)
1808     // Loop control:
1809     //   for (; count != 0; count--) {
1810     //     copied_oop = load_heap_oop(from++);
1811     //     ... generate_type_check ...;
1812     //     store_heap_oop(to++, copied_oop);
1813     //   }
1814     __ align(OptoLoopAlignment);
1815 
1816     __ BIND(L_store_element);
1817     __ store_heap_oop(__ post(to, UseCompressedOops ? 4 : 8), copied_oop, noreg, noreg, noreg, AS_RAW);  // store the oop
1818     __ sub(count, count, 1);
1819     __ cbz(count, L_do_card_marks);
1820 
1821     // ======== loop entry is here ========
1822     __ BIND(L_load_element);
1823     __ load_heap_oop(copied_oop, __ post(from, UseCompressedOops ? 4 : 8), noreg, noreg, AS_RAW); // load the oop
1824     __ cbz(copied_oop, L_store_element);
1825 
1826     __ load_klass(r19_klass, copied_oop);// query the object klass
1827     generate_type_check(r19_klass, ckoff, ckval, L_store_element);
1828     // ======== end loop ========
1829 
1830     // It was a real error; we must depend on the caller to finish the job.
1831     // Register count = remaining oops, count_orig = total oops.
1832     // Emit GC store barriers for the oops we have copied and report
1833     // their number to the caller.
1834 
1835     __ subs(count, count_save, count);     // K = partially copied oop count
1836     __ eon(count, count, zr);                   // report (-1^K) to caller
1837     __ br(Assembler::EQ, L_done_pop);
1838 
1839     __ BIND(L_do_card_marks);
1840     bs-&gt;arraycopy_epilogue(_masm, decorators, is_oop, start_to, count_save, rscratch1, wb_post_saved_regs);
1841 
1842     __ bind(L_done_pop);
1843     __ pop(RegSet::of(r18, r19, r20, r21), sp);
1844     inc_counter_np(SharedRuntime::_checkcast_array_copy_ctr);
1845 
1846     __ bind(L_done);
1847     __ mov(r0, count);
1848     __ leave();
1849     __ ret(lr);
1850 
1851     return start;
1852   }
1853 
1854   // Perform range checks on the proposed arraycopy.
1855   // Kills temp, but nothing else.
1856   // Also, clean the sign bits of src_pos and dst_pos.
1857   void arraycopy_range_checks(Register src,     // source array oop (c_rarg0)
1858                               Register src_pos, // source position (c_rarg1)
1859                               Register dst,     // destination array oo (c_rarg2)
1860                               Register dst_pos, // destination position (c_rarg3)
1861                               Register length,
1862                               Register temp,
1863                               Label&amp; L_failed) {
1864     BLOCK_COMMENT(&quot;arraycopy_range_checks:&quot;);
1865 
1866     assert_different_registers(rscratch1, temp);
1867 
1868     //  if (src_pos + length &gt; arrayOop(src)-&gt;length())  FAIL;
1869     __ ldrw(rscratch1, Address(src, arrayOopDesc::length_offset_in_bytes()));
1870     __ addw(temp, length, src_pos);
1871     __ cmpw(temp, rscratch1);
1872     __ br(Assembler::HI, L_failed);
1873 
1874     //  if (dst_pos + length &gt; arrayOop(dst)-&gt;length())  FAIL;
1875     __ ldrw(rscratch1, Address(dst, arrayOopDesc::length_offset_in_bytes()));
1876     __ addw(temp, length, dst_pos);
1877     __ cmpw(temp, rscratch1);
1878     __ br(Assembler::HI, L_failed);
1879 
1880     // Have to clean up high 32 bits of &#39;src_pos&#39; and &#39;dst_pos&#39;.
1881     __ movw(src_pos, src_pos);
1882     __ movw(dst_pos, dst_pos);
1883 
1884     BLOCK_COMMENT(&quot;arraycopy_range_checks done&quot;);
1885   }
1886 
1887   // These stubs get called from some dumb test routine.
1888   // I&#39;ll write them properly when they&#39;re called from
1889   // something that&#39;s actually doing something.
1890   static void fake_arraycopy_stub(address src, address dst, int count) {
1891     assert(count == 0, &quot;huh?&quot;);
1892   }
1893 
1894 
1895   //
1896   //  Generate &#39;unsafe&#39; array copy stub
1897   //  Though just as safe as the other stubs, it takes an unscaled
1898   //  size_t argument instead of an element count.
1899   //
1900   //  Input:
1901   //    c_rarg0   - source array address
1902   //    c_rarg1   - destination array address
1903   //    c_rarg2   - byte count, treated as ssize_t, can be zero
1904   //
1905   // Examines the alignment of the operands and dispatches
1906   // to a long, int, short, or byte copy loop.
1907   //
1908   address generate_unsafe_copy(const char *name,
1909                                address byte_copy_entry,
1910                                address short_copy_entry,
1911                                address int_copy_entry,
1912                                address long_copy_entry) {
1913     Label L_long_aligned, L_int_aligned, L_short_aligned;
1914     Register s = c_rarg0, d = c_rarg1, count = c_rarg2;
1915 
1916     __ align(CodeEntryAlignment);
1917     StubCodeMark mark(this, &quot;StubRoutines&quot;, name);
1918     address start = __ pc();
1919     __ enter(); // required for proper stackwalking of RuntimeStub frame
1920 
1921     // bump this on entry, not on exit:
1922     inc_counter_np(SharedRuntime::_unsafe_array_copy_ctr);
1923 
1924     __ orr(rscratch1, s, d);
1925     __ orr(rscratch1, rscratch1, count);
1926 
1927     __ andr(rscratch1, rscratch1, BytesPerLong-1);
1928     __ cbz(rscratch1, L_long_aligned);
1929     __ andr(rscratch1, rscratch1, BytesPerInt-1);
1930     __ cbz(rscratch1, L_int_aligned);
1931     __ tbz(rscratch1, 0, L_short_aligned);
1932     __ b(RuntimeAddress(byte_copy_entry));
1933 
1934     __ BIND(L_short_aligned);
1935     __ lsr(count, count, LogBytesPerShort);  // size =&gt; short_count
1936     __ b(RuntimeAddress(short_copy_entry));
1937     __ BIND(L_int_aligned);
1938     __ lsr(count, count, LogBytesPerInt);    // size =&gt; int_count
1939     __ b(RuntimeAddress(int_copy_entry));
1940     __ BIND(L_long_aligned);
1941     __ lsr(count, count, LogBytesPerLong);   // size =&gt; long_count
1942     __ b(RuntimeAddress(long_copy_entry));
1943 
1944     return start;
1945   }
1946 
1947   //
1948   //  Generate generic array copy stubs
1949   //
1950   //  Input:
1951   //    c_rarg0    -  src oop
1952   //    c_rarg1    -  src_pos (32-bits)
1953   //    c_rarg2    -  dst oop
1954   //    c_rarg3    -  dst_pos (32-bits)
1955   //    c_rarg4    -  element count (32-bits)
1956   //
1957   //  Output:
1958   //    r0 ==  0  -  success
1959   //    r0 == -1^K - failure, where K is partial transfer count
1960   //
1961   address generate_generic_copy(const char *name,
1962                                 address byte_copy_entry, address short_copy_entry,
1963                                 address int_copy_entry, address oop_copy_entry,
1964                                 address long_copy_entry, address checkcast_copy_entry) {
1965 
1966     Label L_failed, L_objArray;
1967     Label L_copy_bytes, L_copy_shorts, L_copy_ints, L_copy_longs;
1968 
1969     // Input registers
1970     const Register src        = c_rarg0;  // source array oop
1971     const Register src_pos    = c_rarg1;  // source position
1972     const Register dst        = c_rarg2;  // destination array oop
1973     const Register dst_pos    = c_rarg3;  // destination position
1974     const Register length     = c_rarg4;
1975 
1976 
1977     // Registers used as temps
1978     const Register dst_klass  = c_rarg5;
1979 
1980     __ align(CodeEntryAlignment);
1981 
1982     StubCodeMark mark(this, &quot;StubRoutines&quot;, name);
1983 
1984     address start = __ pc();
1985 
1986     __ enter(); // required for proper stackwalking of RuntimeStub frame
1987 
1988     // bump this on entry, not on exit:
1989     inc_counter_np(SharedRuntime::_generic_array_copy_ctr);
1990 
1991     //-----------------------------------------------------------------------
1992     // Assembler stub will be used for this call to arraycopy
1993     // if the following conditions are met:
1994     //
1995     // (1) src and dst must not be null.
1996     // (2) src_pos must not be negative.
1997     // (3) dst_pos must not be negative.
1998     // (4) length  must not be negative.
1999     // (5) src klass and dst klass should be the same and not NULL.
2000     // (6) src and dst should be arrays.
2001     // (7) src_pos + length must not exceed length of src.
2002     // (8) dst_pos + length must not exceed length of dst.
2003     //
2004 
2005     //  if (src == NULL) return -1;
2006     __ cbz(src, L_failed);
2007 
2008     //  if (src_pos &lt; 0) return -1;
2009     __ tbnz(src_pos, 31, L_failed);  // i.e. sign bit set
2010 
2011     //  if (dst == NULL) return -1;
2012     __ cbz(dst, L_failed);
2013 
2014     //  if (dst_pos &lt; 0) return -1;
2015     __ tbnz(dst_pos, 31, L_failed);  // i.e. sign bit set
2016 
2017     // registers used as temp
2018     const Register scratch_length    = r16; // elements count to copy
2019     const Register scratch_src_klass = r17; // array klass
2020     const Register lh                = r18; // layout helper
2021 
2022     //  if (length &lt; 0) return -1;
2023     __ movw(scratch_length, length);        // length (elements count, 32-bits value)
2024     __ tbnz(scratch_length, 31, L_failed);  // i.e. sign bit set
2025 
2026     __ load_klass(scratch_src_klass, src);
2027 #ifdef ASSERT
2028     //  assert(src-&gt;klass() != NULL);
2029     {
2030       BLOCK_COMMENT(&quot;assert klasses not null {&quot;);
2031       Label L1, L2;
2032       __ cbnz(scratch_src_klass, L2);   // it is broken if klass is NULL
2033       __ bind(L1);
2034       __ stop(&quot;broken null klass&quot;);
2035       __ bind(L2);
2036       __ load_klass(rscratch1, dst);
2037       __ cbz(rscratch1, L1);     // this would be broken also
2038       BLOCK_COMMENT(&quot;} assert klasses not null done&quot;);
2039     }
2040 #endif
2041 
2042     // Load layout helper (32-bits)
2043     //
2044     //  |array_tag|     | header_size | element_type |     |log2_element_size|
2045     // 32        30    24            16              8     2                 0
2046     //
2047     //   array_tag: typeArray = 0x3, objArray = 0x2, non-array = 0x0
2048     //
2049 
2050     const int lh_offset = in_bytes(Klass::layout_helper_offset());
2051 
2052     // Handle objArrays completely differently...
2053     const jint objArray_lh = Klass::array_layout_helper(T_OBJECT);
2054     __ ldrw(lh, Address(scratch_src_klass, lh_offset));
2055     __ movw(rscratch1, objArray_lh);
2056     __ eorw(rscratch2, lh, rscratch1);
2057     __ cbzw(rscratch2, L_objArray);
2058 
2059     //  if (src-&gt;klass() != dst-&gt;klass()) return -1;
2060     __ load_klass(rscratch2, dst);
2061     __ eor(rscratch2, rscratch2, scratch_src_klass);
2062     __ cbnz(rscratch2, L_failed);
2063 
2064     //  if (!src-&gt;is_Array()) return -1;
2065     __ tbz(lh, 31, L_failed);  // i.e. (lh &gt;= 0)
2066 
2067     // At this point, it is known to be a typeArray (array_tag 0x3).
2068 #ifdef ASSERT
2069     {
2070       BLOCK_COMMENT(&quot;assert primitive array {&quot;);
2071       Label L;
2072       __ movw(rscratch2, Klass::_lh_array_tag_type_value &lt;&lt; Klass::_lh_array_tag_shift);
2073       __ cmpw(lh, rscratch2);
2074       __ br(Assembler::GE, L);
2075       __ stop(&quot;must be a primitive array&quot;);
2076       __ bind(L);
2077       BLOCK_COMMENT(&quot;} assert primitive array done&quot;);
2078     }
2079 #endif
2080 
2081     arraycopy_range_checks(src, src_pos, dst, dst_pos, scratch_length,
2082                            rscratch2, L_failed);
2083 
2084     // TypeArrayKlass
2085     //
2086     // src_addr = (src + array_header_in_bytes()) + (src_pos &lt;&lt; log2elemsize);
2087     // dst_addr = (dst + array_header_in_bytes()) + (dst_pos &lt;&lt; log2elemsize);
2088     //
2089 
2090     const Register rscratch1_offset = rscratch1;    // array offset
2091     const Register r18_elsize = lh; // element size
2092 
2093     __ ubfx(rscratch1_offset, lh, Klass::_lh_header_size_shift,
2094            exact_log2(Klass::_lh_header_size_mask+1));   // array_offset
2095     __ add(src, src, rscratch1_offset);           // src array offset
2096     __ add(dst, dst, rscratch1_offset);           // dst array offset
2097     BLOCK_COMMENT(&quot;choose copy loop based on element size&quot;);
2098 
2099     // next registers should be set before the jump to corresponding stub
2100     const Register from     = c_rarg0;  // source array address
2101     const Register to       = c_rarg1;  // destination array address
2102     const Register count    = c_rarg2;  // elements count
2103 
2104     // &#39;from&#39;, &#39;to&#39;, &#39;count&#39; registers should be set in such order
2105     // since they are the same as &#39;src&#39;, &#39;src_pos&#39;, &#39;dst&#39;.
2106 
2107     assert(Klass::_lh_log2_element_size_shift == 0, &quot;fix this code&quot;);
2108 
2109     // The possible values of elsize are 0-3, i.e. exact_log2(element
2110     // size in bytes).  We do a simple bitwise binary search.
2111   __ BIND(L_copy_bytes);
2112     __ tbnz(r18_elsize, 1, L_copy_ints);
2113     __ tbnz(r18_elsize, 0, L_copy_shorts);
2114     __ lea(from, Address(src, src_pos));// src_addr
2115     __ lea(to,   Address(dst, dst_pos));// dst_addr
2116     __ movw(count, scratch_length); // length
2117     __ b(RuntimeAddress(byte_copy_entry));
2118 
2119   __ BIND(L_copy_shorts);
2120     __ lea(from, Address(src, src_pos, Address::lsl(1)));// src_addr
2121     __ lea(to,   Address(dst, dst_pos, Address::lsl(1)));// dst_addr
2122     __ movw(count, scratch_length); // length
2123     __ b(RuntimeAddress(short_copy_entry));
2124 
2125   __ BIND(L_copy_ints);
2126     __ tbnz(r18_elsize, 0, L_copy_longs);
2127     __ lea(from, Address(src, src_pos, Address::lsl(2)));// src_addr
2128     __ lea(to,   Address(dst, dst_pos, Address::lsl(2)));// dst_addr
2129     __ movw(count, scratch_length); // length
2130     __ b(RuntimeAddress(int_copy_entry));
2131 
2132   __ BIND(L_copy_longs);
2133 #ifdef ASSERT
2134     {
2135       BLOCK_COMMENT(&quot;assert long copy {&quot;);
2136       Label L;
2137       __ andw(lh, lh, Klass::_lh_log2_element_size_mask); // lh -&gt; r18_elsize
2138       __ cmpw(r18_elsize, LogBytesPerLong);
2139       __ br(Assembler::EQ, L);
2140       __ stop(&quot;must be long copy, but elsize is wrong&quot;);
2141       __ bind(L);
2142       BLOCK_COMMENT(&quot;} assert long copy done&quot;);
2143     }
2144 #endif
2145     __ lea(from, Address(src, src_pos, Address::lsl(3)));// src_addr
2146     __ lea(to,   Address(dst, dst_pos, Address::lsl(3)));// dst_addr
2147     __ movw(count, scratch_length); // length
2148     __ b(RuntimeAddress(long_copy_entry));
2149 
2150     // ObjArrayKlass
2151   __ BIND(L_objArray);
2152     // live at this point:  scratch_src_klass, scratch_length, src[_pos], dst[_pos]
2153 
2154     Label L_plain_copy, L_checkcast_copy;
2155     //  test array classes for subtyping
2156     __ load_klass(r18, dst);
2157     __ cmp(scratch_src_klass, r18); // usual case is exact equality
2158     __ br(Assembler::NE, L_checkcast_copy);
2159 
2160     // Identically typed arrays can be copied without element-wise checks.
2161     arraycopy_range_checks(src, src_pos, dst, dst_pos, scratch_length,
2162                            rscratch2, L_failed);
2163 
2164     __ lea(from, Address(src, src_pos, Address::lsl(LogBytesPerHeapOop)));
2165     __ add(from, from, arrayOopDesc::base_offset_in_bytes(T_OBJECT));
2166     __ lea(to, Address(dst, dst_pos, Address::lsl(LogBytesPerHeapOop)));
2167     __ add(to, to, arrayOopDesc::base_offset_in_bytes(T_OBJECT));
2168     __ movw(count, scratch_length); // length
2169   __ BIND(L_plain_copy);
2170     __ b(RuntimeAddress(oop_copy_entry));
2171 
2172   __ BIND(L_checkcast_copy);
2173     // live at this point:  scratch_src_klass, scratch_length, r18 (dst_klass)
2174     {
2175       // Before looking at dst.length, make sure dst is also an objArray.
2176       __ ldrw(rscratch1, Address(r18, lh_offset));
2177       __ movw(rscratch2, objArray_lh);
2178       __ eorw(rscratch1, rscratch1, rscratch2);
2179       __ cbnzw(rscratch1, L_failed);
2180 
2181       // It is safe to examine both src.length and dst.length.
2182       arraycopy_range_checks(src, src_pos, dst, dst_pos, scratch_length,
2183                              r18, L_failed);
2184 
2185       __ load_klass(dst_klass, dst); // reload
2186 
2187       // Marshal the base address arguments now, freeing registers.
2188       __ lea(from, Address(src, src_pos, Address::lsl(LogBytesPerHeapOop)));
2189       __ add(from, from, arrayOopDesc::base_offset_in_bytes(T_OBJECT));
2190       __ lea(to, Address(dst, dst_pos, Address::lsl(LogBytesPerHeapOop)));
2191       __ add(to, to, arrayOopDesc::base_offset_in_bytes(T_OBJECT));
2192       __ movw(count, length);           // length (reloaded)
2193       Register sco_temp = c_rarg3;      // this register is free now
2194       assert_different_registers(from, to, count, sco_temp,
2195                                  dst_klass, scratch_src_klass);
2196       // assert_clean_int(count, sco_temp);
2197 
2198       // Generate the type check.
2199       const int sco_offset = in_bytes(Klass::super_check_offset_offset());
2200       __ ldrw(sco_temp, Address(dst_klass, sco_offset));
2201 
2202       // Smashes rscratch1, rscratch2
2203       generate_type_check(scratch_src_klass, sco_temp, dst_klass, L_plain_copy);
2204 
2205       // Fetch destination element klass from the ObjArrayKlass header.
2206       int ek_offset = in_bytes(ObjArrayKlass::element_klass_offset());
2207       __ ldr(dst_klass, Address(dst_klass, ek_offset));
2208       __ ldrw(sco_temp, Address(dst_klass, sco_offset));
2209 
2210       // the checkcast_copy loop needs two extra arguments:
2211       assert(c_rarg3 == sco_temp, &quot;#3 already in place&quot;);
2212       // Set up arguments for checkcast_copy_entry.
2213       __ mov(c_rarg4, dst_klass);  // dst.klass.element_klass
2214       __ b(RuntimeAddress(checkcast_copy_entry));
2215     }
2216 
2217   __ BIND(L_failed);
2218     __ mov(r0, -1);
2219     __ leave();   // required for proper stackwalking of RuntimeStub frame
2220     __ ret(lr);
2221 
2222     return start;
2223   }
2224 
2225   //
2226   // Generate stub for array fill. If &quot;aligned&quot; is true, the
2227   // &quot;to&quot; address is assumed to be heapword aligned.
2228   //
2229   // Arguments for generated stub:
2230   //   to:    c_rarg0
2231   //   value: c_rarg1
2232   //   count: c_rarg2 treated as signed
2233   //
2234   address generate_fill(BasicType t, bool aligned, const char *name) {
2235     __ align(CodeEntryAlignment);
2236     StubCodeMark mark(this, &quot;StubRoutines&quot;, name);
2237     address start = __ pc();
2238 
2239     BLOCK_COMMENT(&quot;Entry:&quot;);
2240 
2241     const Register to        = c_rarg0;  // source array address
2242     const Register value     = c_rarg1;  // value
2243     const Register count     = c_rarg2;  // elements count
2244 
2245     const Register bz_base = r10;        // base for block_zero routine
2246     const Register cnt_words = r11;      // temp register
2247 
2248     __ enter();
2249 
2250     Label L_fill_elements, L_exit1;
2251 
2252     int shift = -1;
2253     switch (t) {
2254       case T_BYTE:
2255         shift = 0;
2256         __ cmpw(count, 8 &gt;&gt; shift); // Short arrays (&lt; 8 bytes) fill by element
2257         __ bfi(value, value, 8, 8);   // 8 bit -&gt; 16 bit
2258         __ bfi(value, value, 16, 16); // 16 bit -&gt; 32 bit
2259         __ br(Assembler::LO, L_fill_elements);
2260         break;
2261       case T_SHORT:
2262         shift = 1;
2263         __ cmpw(count, 8 &gt;&gt; shift); // Short arrays (&lt; 8 bytes) fill by element
2264         __ bfi(value, value, 16, 16); // 16 bit -&gt; 32 bit
2265         __ br(Assembler::LO, L_fill_elements);
2266         break;
2267       case T_INT:
2268         shift = 2;
2269         __ cmpw(count, 8 &gt;&gt; shift); // Short arrays (&lt; 8 bytes) fill by element
2270         __ br(Assembler::LO, L_fill_elements);
2271         break;
2272       default: ShouldNotReachHere();
2273     }
2274 
2275     // Align source address at 8 bytes address boundary.
2276     Label L_skip_align1, L_skip_align2, L_skip_align4;
2277     if (!aligned) {
2278       switch (t) {
2279         case T_BYTE:
2280           // One byte misalignment happens only for byte arrays.
2281           __ tbz(to, 0, L_skip_align1);
2282           __ strb(value, Address(__ post(to, 1)));
2283           __ subw(count, count, 1);
2284           __ bind(L_skip_align1);
2285           // Fallthrough
2286         case T_SHORT:
2287           // Two bytes misalignment happens only for byte and short (char) arrays.
2288           __ tbz(to, 1, L_skip_align2);
2289           __ strh(value, Address(__ post(to, 2)));
2290           __ subw(count, count, 2 &gt;&gt; shift);
2291           __ bind(L_skip_align2);
2292           // Fallthrough
2293         case T_INT:
2294           // Align to 8 bytes, we know we are 4 byte aligned to start.
2295           __ tbz(to, 2, L_skip_align4);
2296           __ strw(value, Address(__ post(to, 4)));
2297           __ subw(count, count, 4 &gt;&gt; shift);
2298           __ bind(L_skip_align4);
2299           break;
2300         default: ShouldNotReachHere();
2301       }
2302     }
2303 
2304     //
2305     //  Fill large chunks
2306     //
2307     __ lsrw(cnt_words, count, 3 - shift); // number of words
2308     __ bfi(value, value, 32, 32);         // 32 bit -&gt; 64 bit
2309     __ subw(count, count, cnt_words, Assembler::LSL, 3 - shift);
2310     if (UseBlockZeroing) {
2311       Label non_block_zeroing, rest;
2312       // If the fill value is zero we can use the fast zero_words().
2313       __ cbnz(value, non_block_zeroing);
2314       __ mov(bz_base, to);
2315       __ add(to, to, cnt_words, Assembler::LSL, LogBytesPerWord);
2316       __ zero_words(bz_base, cnt_words);
2317       __ b(rest);
2318       __ bind(non_block_zeroing);
2319       __ fill_words(to, cnt_words, value);
2320       __ bind(rest);
2321     } else {
2322       __ fill_words(to, cnt_words, value);
2323     }
2324 
2325     // Remaining count is less than 8 bytes. Fill it by a single store.
2326     // Note that the total length is no less than 8 bytes.
2327     if (t == T_BYTE || t == T_SHORT) {
2328       Label L_exit1;
2329       __ cbzw(count, L_exit1);
2330       __ add(to, to, count, Assembler::LSL, shift); // points to the end
2331       __ str(value, Address(to, -8));    // overwrite some elements
2332       __ bind(L_exit1);
2333       __ leave();
2334       __ ret(lr);
2335     }
2336 
2337     // Handle copies less than 8 bytes.
2338     Label L_fill_2, L_fill_4, L_exit2;
2339     __ bind(L_fill_elements);
2340     switch (t) {
2341       case T_BYTE:
2342         __ tbz(count, 0, L_fill_2);
2343         __ strb(value, Address(__ post(to, 1)));
2344         __ bind(L_fill_2);
2345         __ tbz(count, 1, L_fill_4);
2346         __ strh(value, Address(__ post(to, 2)));
2347         __ bind(L_fill_4);
2348         __ tbz(count, 2, L_exit2);
2349         __ strw(value, Address(to));
2350         break;
2351       case T_SHORT:
2352         __ tbz(count, 0, L_fill_4);
2353         __ strh(value, Address(__ post(to, 2)));
2354         __ bind(L_fill_4);
2355         __ tbz(count, 1, L_exit2);
2356         __ strw(value, Address(to));
2357         break;
2358       case T_INT:
2359         __ cbzw(count, L_exit2);
2360         __ strw(value, Address(to));
2361         break;
2362       default: ShouldNotReachHere();
2363     }
2364     __ bind(L_exit2);
2365     __ leave();
2366     __ ret(lr);
2367     return start;
2368   }
2369 
2370   address generate_data_cache_writeback() {
2371     const Register line        = c_rarg0;  // address of line to write back
2372 
2373     __ align(CodeEntryAlignment);
2374 
2375     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;_data_cache_writeback&quot;);
2376 
2377     address start = __ pc();
2378     __ enter();
2379     __ cache_wb(Address(line, 0));
2380     __ leave();
2381     __ ret(lr);
2382 
2383     return start;
2384   }
2385 
2386   address generate_data_cache_writeback_sync() {
2387     const Register is_pre     = c_rarg0;  // pre or post sync
2388 
2389     __ align(CodeEntryAlignment);
2390 
2391     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;_data_cache_writeback_sync&quot;);
2392 
2393     // pre wbsync is a no-op
2394     // post wbsync translates to an sfence
2395 
2396     Label skip;
2397     address start = __ pc();
2398     __ enter();
2399     __ cbnz(is_pre, skip);
2400     __ cache_wbsync(false);
2401     __ bind(skip);
2402     __ leave();
2403     __ ret(lr);
2404 
2405     return start;
2406   }
2407 
2408   void generate_arraycopy_stubs() {
2409     address entry;
2410     address entry_jbyte_arraycopy;
2411     address entry_jshort_arraycopy;
2412     address entry_jint_arraycopy;
2413     address entry_oop_arraycopy;
2414     address entry_jlong_arraycopy;
2415     address entry_checkcast_arraycopy;
2416 
2417     generate_copy_longs(copy_f, r0, r1, rscratch2, copy_forwards);
2418     generate_copy_longs(copy_b, r0, r1, rscratch2, copy_backwards);
2419 
2420     StubRoutines::aarch64::_zero_blocks = generate_zero_blocks();
2421 
2422     //*** jbyte
2423     // Always need aligned and unaligned versions
2424     StubRoutines::_jbyte_disjoint_arraycopy         = generate_disjoint_byte_copy(false, &amp;entry,
2425                                                                                   &quot;jbyte_disjoint_arraycopy&quot;);
2426     StubRoutines::_jbyte_arraycopy                  = generate_conjoint_byte_copy(false, entry,
2427                                                                                   &amp;entry_jbyte_arraycopy,
2428                                                                                   &quot;jbyte_arraycopy&quot;);
2429     StubRoutines::_arrayof_jbyte_disjoint_arraycopy = generate_disjoint_byte_copy(true, &amp;entry,
2430                                                                                   &quot;arrayof_jbyte_disjoint_arraycopy&quot;);
2431     StubRoutines::_arrayof_jbyte_arraycopy          = generate_conjoint_byte_copy(true, entry, NULL,
2432                                                                                   &quot;arrayof_jbyte_arraycopy&quot;);
2433 
2434     //*** jshort
2435     // Always need aligned and unaligned versions
2436     StubRoutines::_jshort_disjoint_arraycopy         = generate_disjoint_short_copy(false, &amp;entry,
2437                                                                                     &quot;jshort_disjoint_arraycopy&quot;);
2438     StubRoutines::_jshort_arraycopy                  = generate_conjoint_short_copy(false, entry,
2439                                                                                     &amp;entry_jshort_arraycopy,
2440                                                                                     &quot;jshort_arraycopy&quot;);
2441     StubRoutines::_arrayof_jshort_disjoint_arraycopy = generate_disjoint_short_copy(true, &amp;entry,
2442                                                                                     &quot;arrayof_jshort_disjoint_arraycopy&quot;);
2443     StubRoutines::_arrayof_jshort_arraycopy          = generate_conjoint_short_copy(true, entry, NULL,
2444                                                                                     &quot;arrayof_jshort_arraycopy&quot;);
2445 
2446     //*** jint
2447     // Aligned versions
2448     StubRoutines::_arrayof_jint_disjoint_arraycopy = generate_disjoint_int_copy(true, &amp;entry,
2449                                                                                 &quot;arrayof_jint_disjoint_arraycopy&quot;);
2450     StubRoutines::_arrayof_jint_arraycopy          = generate_conjoint_int_copy(true, entry, &amp;entry_jint_arraycopy,
2451                                                                                 &quot;arrayof_jint_arraycopy&quot;);
2452     // In 64 bit we need both aligned and unaligned versions of jint arraycopy.
2453     // entry_jint_arraycopy always points to the unaligned version
2454     StubRoutines::_jint_disjoint_arraycopy         = generate_disjoint_int_copy(false, &amp;entry,
2455                                                                                 &quot;jint_disjoint_arraycopy&quot;);
2456     StubRoutines::_jint_arraycopy                  = generate_conjoint_int_copy(false, entry,
2457                                                                                 &amp;entry_jint_arraycopy,
2458                                                                                 &quot;jint_arraycopy&quot;);
2459 
2460     //*** jlong
2461     // It is always aligned
2462     StubRoutines::_arrayof_jlong_disjoint_arraycopy = generate_disjoint_long_copy(true, &amp;entry,
2463                                                                                   &quot;arrayof_jlong_disjoint_arraycopy&quot;);
2464     StubRoutines::_arrayof_jlong_arraycopy          = generate_conjoint_long_copy(true, entry, &amp;entry_jlong_arraycopy,
2465                                                                                   &quot;arrayof_jlong_arraycopy&quot;);
2466     StubRoutines::_jlong_disjoint_arraycopy         = StubRoutines::_arrayof_jlong_disjoint_arraycopy;
2467     StubRoutines::_jlong_arraycopy                  = StubRoutines::_arrayof_jlong_arraycopy;
2468 
2469     //*** oops
2470     {
2471       // With compressed oops we need unaligned versions; notice that
2472       // we overwrite entry_oop_arraycopy.
2473       bool aligned = !UseCompressedOops;
2474 
2475       StubRoutines::_arrayof_oop_disjoint_arraycopy
2476         = generate_disjoint_oop_copy(aligned, &amp;entry, &quot;arrayof_oop_disjoint_arraycopy&quot;,
2477                                      /*dest_uninitialized*/false);
2478       StubRoutines::_arrayof_oop_arraycopy
2479         = generate_conjoint_oop_copy(aligned, entry, &amp;entry_oop_arraycopy, &quot;arrayof_oop_arraycopy&quot;,
2480                                      /*dest_uninitialized*/false);
2481       // Aligned versions without pre-barriers
2482       StubRoutines::_arrayof_oop_disjoint_arraycopy_uninit
2483         = generate_disjoint_oop_copy(aligned, &amp;entry, &quot;arrayof_oop_disjoint_arraycopy_uninit&quot;,
2484                                      /*dest_uninitialized*/true);
2485       StubRoutines::_arrayof_oop_arraycopy_uninit
2486         = generate_conjoint_oop_copy(aligned, entry, NULL, &quot;arrayof_oop_arraycopy_uninit&quot;,
2487                                      /*dest_uninitialized*/true);
2488     }
2489 
2490     StubRoutines::_oop_disjoint_arraycopy            = StubRoutines::_arrayof_oop_disjoint_arraycopy;
2491     StubRoutines::_oop_arraycopy                     = StubRoutines::_arrayof_oop_arraycopy;
2492     StubRoutines::_oop_disjoint_arraycopy_uninit     = StubRoutines::_arrayof_oop_disjoint_arraycopy_uninit;
2493     StubRoutines::_oop_arraycopy_uninit              = StubRoutines::_arrayof_oop_arraycopy_uninit;
2494 
2495     StubRoutines::_checkcast_arraycopy        = generate_checkcast_copy(&quot;checkcast_arraycopy&quot;, &amp;entry_checkcast_arraycopy);
2496     StubRoutines::_checkcast_arraycopy_uninit = generate_checkcast_copy(&quot;checkcast_arraycopy_uninit&quot;, NULL,
2497                                                                         /*dest_uninitialized*/true);
2498 
2499     StubRoutines::_unsafe_arraycopy    = generate_unsafe_copy(&quot;unsafe_arraycopy&quot;,
2500                                                               entry_jbyte_arraycopy,
2501                                                               entry_jshort_arraycopy,
2502                                                               entry_jint_arraycopy,
2503                                                               entry_jlong_arraycopy);
2504 
2505     StubRoutines::_generic_arraycopy   = generate_generic_copy(&quot;generic_arraycopy&quot;,
2506                                                                entry_jbyte_arraycopy,
2507                                                                entry_jshort_arraycopy,
2508                                                                entry_jint_arraycopy,
2509                                                                entry_oop_arraycopy,
2510                                                                entry_jlong_arraycopy,
2511                                                                entry_checkcast_arraycopy);
2512 
2513     StubRoutines::_jbyte_fill = generate_fill(T_BYTE, false, &quot;jbyte_fill&quot;);
2514     StubRoutines::_jshort_fill = generate_fill(T_SHORT, false, &quot;jshort_fill&quot;);
2515     StubRoutines::_jint_fill = generate_fill(T_INT, false, &quot;jint_fill&quot;);
2516     StubRoutines::_arrayof_jbyte_fill = generate_fill(T_BYTE, true, &quot;arrayof_jbyte_fill&quot;);
2517     StubRoutines::_arrayof_jshort_fill = generate_fill(T_SHORT, true, &quot;arrayof_jshort_fill&quot;);
2518     StubRoutines::_arrayof_jint_fill = generate_fill(T_INT, true, &quot;arrayof_jint_fill&quot;);
2519   }
2520 
2521   void generate_math_stubs() { Unimplemented(); }
2522 
2523   // Arguments:
2524   //
2525   // Inputs:
2526   //   c_rarg0   - source byte array address
2527   //   c_rarg1   - destination byte array address
2528   //   c_rarg2   - K (key) in little endian int array
2529   //
2530   address generate_aescrypt_encryptBlock() {
2531     __ align(CodeEntryAlignment);
2532     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;aescrypt_encryptBlock&quot;);
2533 
2534     Label L_doLast;
2535 
2536     const Register from        = c_rarg0;  // source array address
2537     const Register to          = c_rarg1;  // destination array address
2538     const Register key         = c_rarg2;  // key array address
2539     const Register keylen      = rscratch1;
2540 
2541     address start = __ pc();
2542     __ enter();
2543 
2544     __ ldrw(keylen, Address(key, arrayOopDesc::length_offset_in_bytes() - arrayOopDesc::base_offset_in_bytes(T_INT)));
2545 
2546     __ ld1(v0, __ T16B, from); // get 16 bytes of input
2547 
2548     __ ld1(v1, v2, v3, v4, __ T16B, __ post(key, 64));
2549     __ rev32(v1, __ T16B, v1);
2550     __ rev32(v2, __ T16B, v2);
2551     __ rev32(v3, __ T16B, v3);
2552     __ rev32(v4, __ T16B, v4);
2553     __ aese(v0, v1);
2554     __ aesmc(v0, v0);
2555     __ aese(v0, v2);
2556     __ aesmc(v0, v0);
2557     __ aese(v0, v3);
2558     __ aesmc(v0, v0);
2559     __ aese(v0, v4);
2560     __ aesmc(v0, v0);
2561 
2562     __ ld1(v1, v2, v3, v4, __ T16B, __ post(key, 64));
2563     __ rev32(v1, __ T16B, v1);
2564     __ rev32(v2, __ T16B, v2);
2565     __ rev32(v3, __ T16B, v3);
2566     __ rev32(v4, __ T16B, v4);
2567     __ aese(v0, v1);
2568     __ aesmc(v0, v0);
2569     __ aese(v0, v2);
2570     __ aesmc(v0, v0);
2571     __ aese(v0, v3);
2572     __ aesmc(v0, v0);
2573     __ aese(v0, v4);
2574     __ aesmc(v0, v0);
2575 
2576     __ ld1(v1, v2, __ T16B, __ post(key, 32));
2577     __ rev32(v1, __ T16B, v1);
2578     __ rev32(v2, __ T16B, v2);
2579 
2580     __ cmpw(keylen, 44);
2581     __ br(Assembler::EQ, L_doLast);
2582 
2583     __ aese(v0, v1);
2584     __ aesmc(v0, v0);
2585     __ aese(v0, v2);
2586     __ aesmc(v0, v0);
2587 
2588     __ ld1(v1, v2, __ T16B, __ post(key, 32));
2589     __ rev32(v1, __ T16B, v1);
2590     __ rev32(v2, __ T16B, v2);
2591 
2592     __ cmpw(keylen, 52);
2593     __ br(Assembler::EQ, L_doLast);
2594 
2595     __ aese(v0, v1);
2596     __ aesmc(v0, v0);
2597     __ aese(v0, v2);
2598     __ aesmc(v0, v0);
2599 
2600     __ ld1(v1, v2, __ T16B, __ post(key, 32));
2601     __ rev32(v1, __ T16B, v1);
2602     __ rev32(v2, __ T16B, v2);
2603 
2604     __ BIND(L_doLast);
2605 
2606     __ aese(v0, v1);
2607     __ aesmc(v0, v0);
2608     __ aese(v0, v2);
2609 
2610     __ ld1(v1, __ T16B, key);
2611     __ rev32(v1, __ T16B, v1);
2612     __ eor(v0, __ T16B, v0, v1);
2613 
2614     __ st1(v0, __ T16B, to);
2615 
2616     __ mov(r0, 0);
2617 
2618     __ leave();
2619     __ ret(lr);
2620 
2621     return start;
2622   }
2623 
2624   // Arguments:
2625   //
2626   // Inputs:
2627   //   c_rarg0   - source byte array address
2628   //   c_rarg1   - destination byte array address
2629   //   c_rarg2   - K (key) in little endian int array
2630   //
2631   address generate_aescrypt_decryptBlock() {
2632     assert(UseAES, &quot;need AES instructions and misaligned SSE support&quot;);
2633     __ align(CodeEntryAlignment);
2634     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;aescrypt_decryptBlock&quot;);
2635     Label L_doLast;
2636 
2637     const Register from        = c_rarg0;  // source array address
2638     const Register to          = c_rarg1;  // destination array address
2639     const Register key         = c_rarg2;  // key array address
2640     const Register keylen      = rscratch1;
2641 
2642     address start = __ pc();
2643     __ enter(); // required for proper stackwalking of RuntimeStub frame
2644 
2645     __ ldrw(keylen, Address(key, arrayOopDesc::length_offset_in_bytes() - arrayOopDesc::base_offset_in_bytes(T_INT)));
2646 
2647     __ ld1(v0, __ T16B, from); // get 16 bytes of input
2648 
2649     __ ld1(v5, __ T16B, __ post(key, 16));
2650     __ rev32(v5, __ T16B, v5);
2651 
2652     __ ld1(v1, v2, v3, v4, __ T16B, __ post(key, 64));
2653     __ rev32(v1, __ T16B, v1);
2654     __ rev32(v2, __ T16B, v2);
2655     __ rev32(v3, __ T16B, v3);
2656     __ rev32(v4, __ T16B, v4);
2657     __ aesd(v0, v1);
2658     __ aesimc(v0, v0);
2659     __ aesd(v0, v2);
2660     __ aesimc(v0, v0);
2661     __ aesd(v0, v3);
2662     __ aesimc(v0, v0);
2663     __ aesd(v0, v4);
2664     __ aesimc(v0, v0);
2665 
2666     __ ld1(v1, v2, v3, v4, __ T16B, __ post(key, 64));
2667     __ rev32(v1, __ T16B, v1);
2668     __ rev32(v2, __ T16B, v2);
2669     __ rev32(v3, __ T16B, v3);
2670     __ rev32(v4, __ T16B, v4);
2671     __ aesd(v0, v1);
2672     __ aesimc(v0, v0);
2673     __ aesd(v0, v2);
2674     __ aesimc(v0, v0);
2675     __ aesd(v0, v3);
2676     __ aesimc(v0, v0);
2677     __ aesd(v0, v4);
2678     __ aesimc(v0, v0);
2679 
2680     __ ld1(v1, v2, __ T16B, __ post(key, 32));
2681     __ rev32(v1, __ T16B, v1);
2682     __ rev32(v2, __ T16B, v2);
2683 
2684     __ cmpw(keylen, 44);
2685     __ br(Assembler::EQ, L_doLast);
2686 
2687     __ aesd(v0, v1);
2688     __ aesimc(v0, v0);
2689     __ aesd(v0, v2);
2690     __ aesimc(v0, v0);
2691 
2692     __ ld1(v1, v2, __ T16B, __ post(key, 32));
2693     __ rev32(v1, __ T16B, v1);
2694     __ rev32(v2, __ T16B, v2);
2695 
2696     __ cmpw(keylen, 52);
2697     __ br(Assembler::EQ, L_doLast);
2698 
2699     __ aesd(v0, v1);
2700     __ aesimc(v0, v0);
2701     __ aesd(v0, v2);
2702     __ aesimc(v0, v0);
2703 
2704     __ ld1(v1, v2, __ T16B, __ post(key, 32));
2705     __ rev32(v1, __ T16B, v1);
2706     __ rev32(v2, __ T16B, v2);
2707 
2708     __ BIND(L_doLast);
2709 
2710     __ aesd(v0, v1);
2711     __ aesimc(v0, v0);
2712     __ aesd(v0, v2);
2713 
2714     __ eor(v0, __ T16B, v0, v5);
2715 
2716     __ st1(v0, __ T16B, to);
2717 
2718     __ mov(r0, 0);
2719 
2720     __ leave();
2721     __ ret(lr);
2722 
2723     return start;
2724   }
2725 
2726   // Arguments:
2727   //
2728   // Inputs:
2729   //   c_rarg0   - source byte array address
2730   //   c_rarg1   - destination byte array address
2731   //   c_rarg2   - K (key) in little endian int array
2732   //   c_rarg3   - r vector byte array address
2733   //   c_rarg4   - input length
2734   //
2735   // Output:
2736   //   x0        - input length
2737   //
2738   address generate_cipherBlockChaining_encryptAESCrypt() {
2739     assert(UseAES, &quot;need AES instructions and misaligned SSE support&quot;);
2740     __ align(CodeEntryAlignment);
2741     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;cipherBlockChaining_encryptAESCrypt&quot;);
2742 
2743     Label L_loadkeys_44, L_loadkeys_52, L_aes_loop, L_rounds_44, L_rounds_52;
2744 
2745     const Register from        = c_rarg0;  // source array address
2746     const Register to          = c_rarg1;  // destination array address
2747     const Register key         = c_rarg2;  // key array address
2748     const Register rvec        = c_rarg3;  // r byte array initialized from initvector array address
2749                                            // and left with the results of the last encryption block
2750     const Register len_reg     = c_rarg4;  // src len (must be multiple of blocksize 16)
2751     const Register keylen      = rscratch1;
2752 
2753     address start = __ pc();
2754 
2755       __ enter();
2756 
2757       __ movw(rscratch2, len_reg);
2758 
2759       __ ldrw(keylen, Address(key, arrayOopDesc::length_offset_in_bytes() - arrayOopDesc::base_offset_in_bytes(T_INT)));
2760 
2761       __ ld1(v0, __ T16B, rvec);
2762 
2763       __ cmpw(keylen, 52);
2764       __ br(Assembler::CC, L_loadkeys_44);
2765       __ br(Assembler::EQ, L_loadkeys_52);
2766 
2767       __ ld1(v17, v18, __ T16B, __ post(key, 32));
2768       __ rev32(v17, __ T16B, v17);
2769       __ rev32(v18, __ T16B, v18);
2770     __ BIND(L_loadkeys_52);
2771       __ ld1(v19, v20, __ T16B, __ post(key, 32));
2772       __ rev32(v19, __ T16B, v19);
2773       __ rev32(v20, __ T16B, v20);
2774     __ BIND(L_loadkeys_44);
2775       __ ld1(v21, v22, v23, v24, __ T16B, __ post(key, 64));
2776       __ rev32(v21, __ T16B, v21);
2777       __ rev32(v22, __ T16B, v22);
2778       __ rev32(v23, __ T16B, v23);
2779       __ rev32(v24, __ T16B, v24);
2780       __ ld1(v25, v26, v27, v28, __ T16B, __ post(key, 64));
2781       __ rev32(v25, __ T16B, v25);
2782       __ rev32(v26, __ T16B, v26);
2783       __ rev32(v27, __ T16B, v27);
2784       __ rev32(v28, __ T16B, v28);
2785       __ ld1(v29, v30, v31, __ T16B, key);
2786       __ rev32(v29, __ T16B, v29);
2787       __ rev32(v30, __ T16B, v30);
2788       __ rev32(v31, __ T16B, v31);
2789 
2790     __ BIND(L_aes_loop);
2791       __ ld1(v1, __ T16B, __ post(from, 16));
2792       __ eor(v0, __ T16B, v0, v1);
2793 
2794       __ br(Assembler::CC, L_rounds_44);
2795       __ br(Assembler::EQ, L_rounds_52);
2796 
2797       __ aese(v0, v17); __ aesmc(v0, v0);
2798       __ aese(v0, v18); __ aesmc(v0, v0);
2799     __ BIND(L_rounds_52);
2800       __ aese(v0, v19); __ aesmc(v0, v0);
2801       __ aese(v0, v20); __ aesmc(v0, v0);
2802     __ BIND(L_rounds_44);
2803       __ aese(v0, v21); __ aesmc(v0, v0);
2804       __ aese(v0, v22); __ aesmc(v0, v0);
2805       __ aese(v0, v23); __ aesmc(v0, v0);
2806       __ aese(v0, v24); __ aesmc(v0, v0);
2807       __ aese(v0, v25); __ aesmc(v0, v0);
2808       __ aese(v0, v26); __ aesmc(v0, v0);
2809       __ aese(v0, v27); __ aesmc(v0, v0);
2810       __ aese(v0, v28); __ aesmc(v0, v0);
2811       __ aese(v0, v29); __ aesmc(v0, v0);
2812       __ aese(v0, v30);
2813       __ eor(v0, __ T16B, v0, v31);
2814 
2815       __ st1(v0, __ T16B, __ post(to, 16));
2816 
2817       __ subw(len_reg, len_reg, 16);
2818       __ cbnzw(len_reg, L_aes_loop);
2819 
2820       __ st1(v0, __ T16B, rvec);
2821 
2822       __ mov(r0, rscratch2);
2823 
2824       __ leave();
2825       __ ret(lr);
2826 
2827       return start;
2828   }
2829 
2830   // Arguments:
2831   //
2832   // Inputs:
2833   //   c_rarg0   - source byte array address
2834   //   c_rarg1   - destination byte array address
2835   //   c_rarg2   - K (key) in little endian int array
2836   //   c_rarg3   - r vector byte array address
2837   //   c_rarg4   - input length
2838   //
2839   // Output:
2840   //   r0        - input length
2841   //
2842   address generate_cipherBlockChaining_decryptAESCrypt() {
2843     assert(UseAES, &quot;need AES instructions and misaligned SSE support&quot;);
2844     __ align(CodeEntryAlignment);
2845     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;cipherBlockChaining_decryptAESCrypt&quot;);
2846 
2847     Label L_loadkeys_44, L_loadkeys_52, L_aes_loop, L_rounds_44, L_rounds_52;
2848 
2849     const Register from        = c_rarg0;  // source array address
2850     const Register to          = c_rarg1;  // destination array address
2851     const Register key         = c_rarg2;  // key array address
2852     const Register rvec        = c_rarg3;  // r byte array initialized from initvector array address
2853                                            // and left with the results of the last encryption block
2854     const Register len_reg     = c_rarg4;  // src len (must be multiple of blocksize 16)
2855     const Register keylen      = rscratch1;
2856 
2857     address start = __ pc();
2858 
2859       __ enter();
2860 
2861       __ movw(rscratch2, len_reg);
2862 
2863       __ ldrw(keylen, Address(key, arrayOopDesc::length_offset_in_bytes() - arrayOopDesc::base_offset_in_bytes(T_INT)));
2864 
2865       __ ld1(v2, __ T16B, rvec);
2866 
2867       __ ld1(v31, __ T16B, __ post(key, 16));
2868       __ rev32(v31, __ T16B, v31);
2869 
2870       __ cmpw(keylen, 52);
2871       __ br(Assembler::CC, L_loadkeys_44);
2872       __ br(Assembler::EQ, L_loadkeys_52);
2873 
2874       __ ld1(v17, v18, __ T16B, __ post(key, 32));
2875       __ rev32(v17, __ T16B, v17);
2876       __ rev32(v18, __ T16B, v18);
2877     __ BIND(L_loadkeys_52);
2878       __ ld1(v19, v20, __ T16B, __ post(key, 32));
2879       __ rev32(v19, __ T16B, v19);
2880       __ rev32(v20, __ T16B, v20);
2881     __ BIND(L_loadkeys_44);
2882       __ ld1(v21, v22, v23, v24, __ T16B, __ post(key, 64));
2883       __ rev32(v21, __ T16B, v21);
2884       __ rev32(v22, __ T16B, v22);
2885       __ rev32(v23, __ T16B, v23);
2886       __ rev32(v24, __ T16B, v24);
2887       __ ld1(v25, v26, v27, v28, __ T16B, __ post(key, 64));
2888       __ rev32(v25, __ T16B, v25);
2889       __ rev32(v26, __ T16B, v26);
2890       __ rev32(v27, __ T16B, v27);
2891       __ rev32(v28, __ T16B, v28);
2892       __ ld1(v29, v30, __ T16B, key);
2893       __ rev32(v29, __ T16B, v29);
2894       __ rev32(v30, __ T16B, v30);
2895 
2896     __ BIND(L_aes_loop);
2897       __ ld1(v0, __ T16B, __ post(from, 16));
2898       __ orr(v1, __ T16B, v0, v0);
2899 
2900       __ br(Assembler::CC, L_rounds_44);
2901       __ br(Assembler::EQ, L_rounds_52);
2902 
2903       __ aesd(v0, v17); __ aesimc(v0, v0);
2904       __ aesd(v0, v18); __ aesimc(v0, v0);
2905     __ BIND(L_rounds_52);
2906       __ aesd(v0, v19); __ aesimc(v0, v0);
2907       __ aesd(v0, v20); __ aesimc(v0, v0);
2908     __ BIND(L_rounds_44);
2909       __ aesd(v0, v21); __ aesimc(v0, v0);
2910       __ aesd(v0, v22); __ aesimc(v0, v0);
2911       __ aesd(v0, v23); __ aesimc(v0, v0);
2912       __ aesd(v0, v24); __ aesimc(v0, v0);
2913       __ aesd(v0, v25); __ aesimc(v0, v0);
2914       __ aesd(v0, v26); __ aesimc(v0, v0);
2915       __ aesd(v0, v27); __ aesimc(v0, v0);
2916       __ aesd(v0, v28); __ aesimc(v0, v0);
2917       __ aesd(v0, v29); __ aesimc(v0, v0);
2918       __ aesd(v0, v30);
2919       __ eor(v0, __ T16B, v0, v31);
2920       __ eor(v0, __ T16B, v0, v2);
2921 
2922       __ st1(v0, __ T16B, __ post(to, 16));
2923       __ orr(v2, __ T16B, v1, v1);
2924 
2925       __ subw(len_reg, len_reg, 16);
2926       __ cbnzw(len_reg, L_aes_loop);
2927 
2928       __ st1(v2, __ T16B, rvec);
2929 
2930       __ mov(r0, rscratch2);
2931 
2932       __ leave();
2933       __ ret(lr);
2934 
2935     return start;
2936   }
2937 
2938   // Arguments:
2939   //
2940   // Inputs:
2941   //   c_rarg0   - byte[]  source+offset
2942   //   c_rarg1   - int[]   SHA.state
2943   //   c_rarg2   - int     offset
2944   //   c_rarg3   - int     limit
2945   //
2946   address generate_sha1_implCompress(bool multi_block, const char *name) {
2947     __ align(CodeEntryAlignment);
2948     StubCodeMark mark(this, &quot;StubRoutines&quot;, name);
2949     address start = __ pc();
2950 
2951     Register buf   = c_rarg0;
2952     Register state = c_rarg1;
2953     Register ofs   = c_rarg2;
2954     Register limit = c_rarg3;
2955 
2956     Label keys;
2957     Label sha1_loop;
2958 
2959     // load the keys into v0..v3
2960     __ adr(rscratch1, keys);
2961     __ ld4r(v0, v1, v2, v3, __ T4S, Address(rscratch1));
2962     // load 5 words state into v6, v7
2963     __ ldrq(v6, Address(state, 0));
2964     __ ldrs(v7, Address(state, 16));
2965 
2966 
2967     __ BIND(sha1_loop);
2968     // load 64 bytes of data into v16..v19
2969     __ ld1(v16, v17, v18, v19, __ T4S, multi_block ? __ post(buf, 64) : buf);
2970     __ rev32(v16, __ T16B, v16);
2971     __ rev32(v17, __ T16B, v17);
2972     __ rev32(v18, __ T16B, v18);
2973     __ rev32(v19, __ T16B, v19);
2974 
2975     // do the sha1
2976     __ addv(v4, __ T4S, v16, v0);
2977     __ orr(v20, __ T16B, v6, v6);
2978 
2979     FloatRegister d0 = v16;
2980     FloatRegister d1 = v17;
2981     FloatRegister d2 = v18;
2982     FloatRegister d3 = v19;
2983 
2984     for (int round = 0; round &lt; 20; round++) {
2985       FloatRegister tmp1 = (round &amp; 1) ? v4 : v5;
2986       FloatRegister tmp2 = (round &amp; 1) ? v21 : v22;
2987       FloatRegister tmp3 = round ? ((round &amp; 1) ? v22 : v21) : v7;
2988       FloatRegister tmp4 = (round &amp; 1) ? v5 : v4;
2989       FloatRegister key = (round &lt; 4) ? v0 : ((round &lt; 9) ? v1 : ((round &lt; 14) ? v2 : v3));
2990 
2991       if (round &lt; 16) __ sha1su0(d0, __ T4S, d1, d2);
2992       if (round &lt; 19) __ addv(tmp1, __ T4S, d1, key);
2993       __ sha1h(tmp2, __ T4S, v20);
2994       if (round &lt; 5)
2995         __ sha1c(v20, __ T4S, tmp3, tmp4);
2996       else if (round &lt; 10 || round &gt;= 15)
2997         __ sha1p(v20, __ T4S, tmp3, tmp4);
2998       else
2999         __ sha1m(v20, __ T4S, tmp3, tmp4);
3000       if (round &lt; 16) __ sha1su1(d0, __ T4S, d3);
3001 
3002       tmp1 = d0; d0 = d1; d1 = d2; d2 = d3; d3 = tmp1;
3003     }
3004 
3005     __ addv(v7, __ T2S, v7, v21);
3006     __ addv(v6, __ T4S, v6, v20);
3007 
3008     if (multi_block) {
3009       __ add(ofs, ofs, 64);
3010       __ cmp(ofs, limit);
3011       __ br(Assembler::LE, sha1_loop);
3012       __ mov(c_rarg0, ofs); // return ofs
3013     }
3014 
3015     __ strq(v6, Address(state, 0));
3016     __ strs(v7, Address(state, 16));
3017 
3018     __ ret(lr);
3019 
3020     __ bind(keys);
3021     __ emit_int32(0x5a827999);
3022     __ emit_int32(0x6ed9eba1);
3023     __ emit_int32(0x8f1bbcdc);
3024     __ emit_int32(0xca62c1d6);
3025 
3026     return start;
3027   }
3028 
3029 
3030   // Arguments:
3031   //
3032   // Inputs:
3033   //   c_rarg0   - byte[]  source+offset
3034   //   c_rarg1   - int[]   SHA.state
3035   //   c_rarg2   - int     offset
3036   //   c_rarg3   - int     limit
3037   //
3038   address generate_sha256_implCompress(bool multi_block, const char *name) {
3039     static const uint32_t round_consts[64] = {
3040       0x428a2f98, 0x71374491, 0xb5c0fbcf, 0xe9b5dba5,
3041       0x3956c25b, 0x59f111f1, 0x923f82a4, 0xab1c5ed5,
3042       0xd807aa98, 0x12835b01, 0x243185be, 0x550c7dc3,
3043       0x72be5d74, 0x80deb1fe, 0x9bdc06a7, 0xc19bf174,
3044       0xe49b69c1, 0xefbe4786, 0x0fc19dc6, 0x240ca1cc,
3045       0x2de92c6f, 0x4a7484aa, 0x5cb0a9dc, 0x76f988da,
3046       0x983e5152, 0xa831c66d, 0xb00327c8, 0xbf597fc7,
3047       0xc6e00bf3, 0xd5a79147, 0x06ca6351, 0x14292967,
3048       0x27b70a85, 0x2e1b2138, 0x4d2c6dfc, 0x53380d13,
3049       0x650a7354, 0x766a0abb, 0x81c2c92e, 0x92722c85,
3050       0xa2bfe8a1, 0xa81a664b, 0xc24b8b70, 0xc76c51a3,
3051       0xd192e819, 0xd6990624, 0xf40e3585, 0x106aa070,
3052       0x19a4c116, 0x1e376c08, 0x2748774c, 0x34b0bcb5,
3053       0x391c0cb3, 0x4ed8aa4a, 0x5b9cca4f, 0x682e6ff3,
3054       0x748f82ee, 0x78a5636f, 0x84c87814, 0x8cc70208,
3055       0x90befffa, 0xa4506ceb, 0xbef9a3f7, 0xc67178f2,
3056     };
3057     __ align(CodeEntryAlignment);
3058     StubCodeMark mark(this, &quot;StubRoutines&quot;, name);
3059     address start = __ pc();
3060 
3061     Register buf   = c_rarg0;
3062     Register state = c_rarg1;
3063     Register ofs   = c_rarg2;
3064     Register limit = c_rarg3;
3065 
3066     Label sha1_loop;
3067 
3068     __ stpd(v8, v9, __ pre(sp, -32));
3069     __ stpd(v10, v11, Address(sp, 16));
3070 
3071 // dga == v0
3072 // dgb == v1
3073 // dg0 == v2
3074 // dg1 == v3
3075 // dg2 == v4
3076 // t0 == v6
3077 // t1 == v7
3078 
3079     // load 16 keys to v16..v31
3080     __ lea(rscratch1, ExternalAddress((address)round_consts));
3081     __ ld1(v16, v17, v18, v19, __ T4S, __ post(rscratch1, 64));
3082     __ ld1(v20, v21, v22, v23, __ T4S, __ post(rscratch1, 64));
3083     __ ld1(v24, v25, v26, v27, __ T4S, __ post(rscratch1, 64));
3084     __ ld1(v28, v29, v30, v31, __ T4S, rscratch1);
3085 
3086     // load 8 words (256 bits) state
3087     __ ldpq(v0, v1, state);
3088 
3089     __ BIND(sha1_loop);
3090     // load 64 bytes of data into v8..v11
3091     __ ld1(v8, v9, v10, v11, __ T4S, multi_block ? __ post(buf, 64) : buf);
3092     __ rev32(v8, __ T16B, v8);
3093     __ rev32(v9, __ T16B, v9);
3094     __ rev32(v10, __ T16B, v10);
3095     __ rev32(v11, __ T16B, v11);
3096 
3097     __ addv(v6, __ T4S, v8, v16);
3098     __ orr(v2, __ T16B, v0, v0);
3099     __ orr(v3, __ T16B, v1, v1);
3100 
3101     FloatRegister d0 = v8;
3102     FloatRegister d1 = v9;
3103     FloatRegister d2 = v10;
3104     FloatRegister d3 = v11;
3105 
3106 
3107     for (int round = 0; round &lt; 16; round++) {
3108       FloatRegister tmp1 = (round &amp; 1) ? v6 : v7;
3109       FloatRegister tmp2 = (round &amp; 1) ? v7 : v6;
3110       FloatRegister tmp3 = (round &amp; 1) ? v2 : v4;
3111       FloatRegister tmp4 = (round &amp; 1) ? v4 : v2;
3112 
3113       if (round &lt; 12) __ sha256su0(d0, __ T4S, d1);
3114        __ orr(v4, __ T16B, v2, v2);
3115       if (round &lt; 15)
3116         __ addv(tmp1, __ T4S, d1, as_FloatRegister(round + 17));
3117       __ sha256h(v2, __ T4S, v3, tmp2);
3118       __ sha256h2(v3, __ T4S, v4, tmp2);
3119       if (round &lt; 12) __ sha256su1(d0, __ T4S, d2, d3);
3120 
3121       tmp1 = d0; d0 = d1; d1 = d2; d2 = d3; d3 = tmp1;
3122     }
3123 
3124     __ addv(v0, __ T4S, v0, v2);
3125     __ addv(v1, __ T4S, v1, v3);
3126 
3127     if (multi_block) {
3128       __ add(ofs, ofs, 64);
3129       __ cmp(ofs, limit);
3130       __ br(Assembler::LE, sha1_loop);
3131       __ mov(c_rarg0, ofs); // return ofs
3132     }
3133 
3134     __ ldpd(v10, v11, Address(sp, 16));
3135     __ ldpd(v8, v9, __ post(sp, 32));
3136 
3137     __ stpq(v0, v1, state);
3138 
3139     __ ret(lr);
3140 
3141     return start;
3142   }
3143 
3144   // Safefetch stubs.
3145   void generate_safefetch(const char* name, int size, address* entry,
3146                           address* fault_pc, address* continuation_pc) {
3147     // safefetch signatures:
3148     //   int      SafeFetch32(int*      adr, int      errValue);
3149     //   intptr_t SafeFetchN (intptr_t* adr, intptr_t errValue);
3150     //
3151     // arguments:
3152     //   c_rarg0 = adr
3153     //   c_rarg1 = errValue
3154     //
3155     // result:
3156     //   PPC_RET  = *adr or errValue
3157 
3158     StubCodeMark mark(this, &quot;StubRoutines&quot;, name);
3159 
3160     // Entry point, pc or function descriptor.
3161     *entry = __ pc();
3162 
3163     // Load *adr into c_rarg1, may fault.
3164     *fault_pc = __ pc();
3165     switch (size) {
3166       case 4:
3167         // int32_t
3168         __ ldrw(c_rarg1, Address(c_rarg0, 0));
3169         break;
3170       case 8:
3171         // int64_t
3172         __ ldr(c_rarg1, Address(c_rarg0, 0));
3173         break;
3174       default:
3175         ShouldNotReachHere();
3176     }
3177 
3178     // return errValue or *adr
3179     *continuation_pc = __ pc();
3180     __ mov(r0, c_rarg1);
3181     __ ret(lr);
3182   }
3183 
3184   /**
3185    *  Arguments:
3186    *
3187    * Inputs:
3188    *   c_rarg0   - int crc
3189    *   c_rarg1   - byte* buf
3190    *   c_rarg2   - int length
3191    *
3192    * Ouput:
3193    *       rax   - int crc result
3194    */
3195   address generate_updateBytesCRC32() {
3196     assert(UseCRC32Intrinsics, &quot;what are we doing here?&quot;);
3197 
3198     __ align(CodeEntryAlignment);
3199     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;updateBytesCRC32&quot;);
3200 
3201     address start = __ pc();
3202 
3203     const Register crc   = c_rarg0;  // crc
3204     const Register buf   = c_rarg1;  // source java byte array address
3205     const Register len   = c_rarg2;  // length
3206     const Register table0 = c_rarg3; // crc_table address
3207     const Register table1 = c_rarg4;
3208     const Register table2 = c_rarg5;
3209     const Register table3 = c_rarg6;
3210     const Register tmp3 = c_rarg7;
3211 
3212     BLOCK_COMMENT(&quot;Entry:&quot;);
3213     __ enter(); // required for proper stackwalking of RuntimeStub frame
3214 
3215     __ kernel_crc32(crc, buf, len,
3216               table0, table1, table2, table3, rscratch1, rscratch2, tmp3);
3217 
3218     __ leave(); // required for proper stackwalking of RuntimeStub frame
3219     __ ret(lr);
3220 
3221     return start;
3222   }
3223 
3224   /**
3225    *  Arguments:
3226    *
3227    * Inputs:
3228    *   c_rarg0   - int crc
3229    *   c_rarg1   - byte* buf
3230    *   c_rarg2   - int length
3231    *   c_rarg3   - int* table
3232    *
3233    * Ouput:
3234    *       r0   - int crc result
3235    */
3236   address generate_updateBytesCRC32C() {
3237     assert(UseCRC32CIntrinsics, &quot;what are we doing here?&quot;);
3238 
3239     __ align(CodeEntryAlignment);
3240     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;updateBytesCRC32C&quot;);
3241 
3242     address start = __ pc();
3243 
3244     const Register crc   = c_rarg0;  // crc
3245     const Register buf   = c_rarg1;  // source java byte array address
3246     const Register len   = c_rarg2;  // length
3247     const Register table0 = c_rarg3; // crc_table address
3248     const Register table1 = c_rarg4;
3249     const Register table2 = c_rarg5;
3250     const Register table3 = c_rarg6;
3251     const Register tmp3 = c_rarg7;
3252 
3253     BLOCK_COMMENT(&quot;Entry:&quot;);
3254     __ enter(); // required for proper stackwalking of RuntimeStub frame
3255 
3256     __ kernel_crc32c(crc, buf, len,
3257               table0, table1, table2, table3, rscratch1, rscratch2, tmp3);
3258 
3259     __ leave(); // required for proper stackwalking of RuntimeStub frame
3260     __ ret(lr);
3261 
3262     return start;
3263   }
3264 
3265   /***
3266    *  Arguments:
3267    *
3268    *  Inputs:
3269    *   c_rarg0   - int   adler
3270    *   c_rarg1   - byte* buff
3271    *   c_rarg2   - int   len
3272    *
3273    * Output:
3274    *   c_rarg0   - int adler result
3275    */
3276   address generate_updateBytesAdler32() {
3277     __ align(CodeEntryAlignment);
3278     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;updateBytesAdler32&quot;);
3279     address start = __ pc();
3280 
3281     Label L_simple_by1_loop, L_nmax, L_nmax_loop, L_by16, L_by16_loop, L_by1_loop, L_do_mod, L_combine, L_by1;
3282 
3283     // Aliases
3284     Register adler  = c_rarg0;
3285     Register s1     = c_rarg0;
3286     Register s2     = c_rarg3;
3287     Register buff   = c_rarg1;
3288     Register len    = c_rarg2;
3289     Register nmax  = r4;
3290     Register base  = r5;
3291     Register count = r6;
3292     Register temp0 = rscratch1;
3293     Register temp1 = rscratch2;
3294     FloatRegister vbytes = v0;
3295     FloatRegister vs1acc = v1;
3296     FloatRegister vs2acc = v2;
3297     FloatRegister vtable = v3;
3298 
3299     // Max number of bytes we can process before having to take the mod
3300     // 0x15B0 is 5552 in decimal, the largest n such that 255n(n+1)/2 + (n+1)(BASE-1) &lt;= 2^32-1
3301     unsigned long BASE = 0xfff1;
3302     unsigned long NMAX = 0x15B0;
3303 
3304     __ mov(base, BASE);
3305     __ mov(nmax, NMAX);
3306 
3307     // Load accumulation coefficients for the upper 16 bits
3308     __ lea(temp0, ExternalAddress((address) StubRoutines::aarch64::_adler_table));
3309     __ ld1(vtable, __ T16B, Address(temp0));
3310 
3311     // s1 is initialized to the lower 16 bits of adler
3312     // s2 is initialized to the upper 16 bits of adler
3313     __ ubfx(s2, adler, 16, 16);  // s2 = ((adler &gt;&gt; 16) &amp; 0xffff)
3314     __ uxth(s1, adler);          // s1 = (adler &amp; 0xffff)
3315 
3316     // The pipelined loop needs at least 16 elements for 1 iteration
3317     // It does check this, but it is more effective to skip to the cleanup loop
3318     __ cmp(len, (u1)16);
3319     __ br(Assembler::HS, L_nmax);
3320     __ cbz(len, L_combine);
3321 
3322     __ bind(L_simple_by1_loop);
3323     __ ldrb(temp0, Address(__ post(buff, 1)));
3324     __ add(s1, s1, temp0);
3325     __ add(s2, s2, s1);
3326     __ subs(len, len, 1);
3327     __ br(Assembler::HI, L_simple_by1_loop);
3328 
3329     // s1 = s1 % BASE
3330     __ subs(temp0, s1, base);
3331     __ csel(s1, temp0, s1, Assembler::HS);
3332 
3333     // s2 = s2 % BASE
3334     __ lsr(temp0, s2, 16);
3335     __ lsl(temp1, temp0, 4);
3336     __ sub(temp1, temp1, temp0);
3337     __ add(s2, temp1, s2, ext::uxth);
3338 
3339     __ subs(temp0, s2, base);
3340     __ csel(s2, temp0, s2, Assembler::HS);
3341 
3342     __ b(L_combine);
3343 
3344     __ bind(L_nmax);
3345     __ subs(len, len, nmax);
3346     __ sub(count, nmax, 16);
3347     __ br(Assembler::LO, L_by16);
3348 
3349     __ bind(L_nmax_loop);
3350 
3351     generate_updateBytesAdler32_accum(s1, s2, buff, temp0, temp1,
3352                                       vbytes, vs1acc, vs2acc, vtable);
3353 
3354     __ subs(count, count, 16);
3355     __ br(Assembler::HS, L_nmax_loop);
3356 
3357     // s1 = s1 % BASE
3358     __ lsr(temp0, s1, 16);
3359     __ lsl(temp1, temp0, 4);
3360     __ sub(temp1, temp1, temp0);
3361     __ add(temp1, temp1, s1, ext::uxth);
3362 
3363     __ lsr(temp0, temp1, 16);
3364     __ lsl(s1, temp0, 4);
3365     __ sub(s1, s1, temp0);
3366     __ add(s1, s1, temp1, ext:: uxth);
3367 
3368     __ subs(temp0, s1, base);
3369     __ csel(s1, temp0, s1, Assembler::HS);
3370 
3371     // s2 = s2 % BASE
3372     __ lsr(temp0, s2, 16);
3373     __ lsl(temp1, temp0, 4);
3374     __ sub(temp1, temp1, temp0);
3375     __ add(temp1, temp1, s2, ext::uxth);
3376 
3377     __ lsr(temp0, temp1, 16);
3378     __ lsl(s2, temp0, 4);
3379     __ sub(s2, s2, temp0);
3380     __ add(s2, s2, temp1, ext:: uxth);
3381 
3382     __ subs(temp0, s2, base);
3383     __ csel(s2, temp0, s2, Assembler::HS);
3384 
3385     __ subs(len, len, nmax);
3386     __ sub(count, nmax, 16);
3387     __ br(Assembler::HS, L_nmax_loop);
3388 
3389     __ bind(L_by16);
3390     __ adds(len, len, count);
3391     __ br(Assembler::LO, L_by1);
3392 
3393     __ bind(L_by16_loop);
3394 
3395     generate_updateBytesAdler32_accum(s1, s2, buff, temp0, temp1,
3396                                       vbytes, vs1acc, vs2acc, vtable);
3397 
3398     __ subs(len, len, 16);
3399     __ br(Assembler::HS, L_by16_loop);
3400 
3401     __ bind(L_by1);
3402     __ adds(len, len, 15);
3403     __ br(Assembler::LO, L_do_mod);
3404 
3405     __ bind(L_by1_loop);
3406     __ ldrb(temp0, Address(__ post(buff, 1)));
3407     __ add(s1, temp0, s1);
3408     __ add(s2, s2, s1);
3409     __ subs(len, len, 1);
3410     __ br(Assembler::HS, L_by1_loop);
3411 
3412     __ bind(L_do_mod);
3413     // s1 = s1 % BASE
3414     __ lsr(temp0, s1, 16);
3415     __ lsl(temp1, temp0, 4);
3416     __ sub(temp1, temp1, temp0);
3417     __ add(temp1, temp1, s1, ext::uxth);
3418 
3419     __ lsr(temp0, temp1, 16);
3420     __ lsl(s1, temp0, 4);
3421     __ sub(s1, s1, temp0);
3422     __ add(s1, s1, temp1, ext:: uxth);
3423 
3424     __ subs(temp0, s1, base);
3425     __ csel(s1, temp0, s1, Assembler::HS);
3426 
3427     // s2 = s2 % BASE
3428     __ lsr(temp0, s2, 16);
3429     __ lsl(temp1, temp0, 4);
3430     __ sub(temp1, temp1, temp0);
3431     __ add(temp1, temp1, s2, ext::uxth);
3432 
3433     __ lsr(temp0, temp1, 16);
3434     __ lsl(s2, temp0, 4);
3435     __ sub(s2, s2, temp0);
3436     __ add(s2, s2, temp1, ext:: uxth);
3437 
3438     __ subs(temp0, s2, base);
3439     __ csel(s2, temp0, s2, Assembler::HS);
3440 
3441     // Combine lower bits and higher bits
3442     __ bind(L_combine);
3443     __ orr(s1, s1, s2, Assembler::LSL, 16); // adler = s1 | (s2 &lt;&lt; 16)
3444 
3445     __ ret(lr);
3446 
3447     return start;
3448   }
3449 
3450   void generate_updateBytesAdler32_accum(Register s1, Register s2, Register buff,
3451           Register temp0, Register temp1, FloatRegister vbytes,
3452           FloatRegister vs1acc, FloatRegister vs2acc, FloatRegister vtable) {
3453     // Below is a vectorized implementation of updating s1 and s2 for 16 bytes.
3454     // We use b1, b2, ..., b16 to denote the 16 bytes loaded in each iteration.
3455     // In non-vectorized code, we update s1 and s2 as:
3456     //   s1 &lt;- s1 + b1
3457     //   s2 &lt;- s2 + s1
3458     //   s1 &lt;- s1 + b2
3459     //   s2 &lt;- s2 + b1
3460     //   ...
3461     //   s1 &lt;- s1 + b16
3462     //   s2 &lt;- s2 + s1
3463     // Putting above assignments together, we have:
3464     //   s1_new = s1 + b1 + b2 + ... + b16
3465     //   s2_new = s2 + (s1 + b1) + (s1 + b1 + b2) + ... + (s1 + b1 + b2 + ... + b16)
3466     //          = s2 + s1 * 16 + (b1 * 16 + b2 * 15 + ... + b16 * 1)
3467     //          = s2 + s1 * 16 + (b1, b2, ... b16) dot (16, 15, ... 1)
3468     __ ld1(vbytes, __ T16B, Address(__ post(buff, 16)));
3469 
3470     // s2 = s2 + s1 * 16
3471     __ add(s2, s2, s1, Assembler::LSL, 4);
3472 
3473     // vs1acc = b1 + b2 + b3 + ... + b16
3474     // vs2acc = (b1 * 16) + (b2 * 15) + (b3 * 14) + ... + (b16 * 1)
3475     __ umullv(vs2acc, __ T8B, vtable, vbytes);
3476     __ umlalv(vs2acc, __ T16B, vtable, vbytes);
3477     __ uaddlv(vs1acc, __ T16B, vbytes);
3478     __ uaddlv(vs2acc, __ T8H, vs2acc);
3479 
3480     // s1 = s1 + vs1acc, s2 = s2 + vs2acc
3481     __ fmovd(temp0, vs1acc);
3482     __ fmovd(temp1, vs2acc);
3483     __ add(s1, s1, temp0);
3484     __ add(s2, s2, temp1);
3485   }
3486 
3487   /**
3488    *  Arguments:
3489    *
3490    *  Input:
3491    *    c_rarg0   - x address
3492    *    c_rarg1   - x length
3493    *    c_rarg2   - y address
3494    *    c_rarg3   - y lenth
3495    *    c_rarg4   - z address
3496    *    c_rarg5   - z length
3497    */
3498   address generate_multiplyToLen() {
3499     __ align(CodeEntryAlignment);
3500     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;multiplyToLen&quot;);
3501 
3502     address start = __ pc();
3503     const Register x     = r0;
3504     const Register xlen  = r1;
3505     const Register y     = r2;
3506     const Register ylen  = r3;
3507     const Register z     = r4;
3508     const Register zlen  = r5;
3509 
3510     const Register tmp1  = r10;
3511     const Register tmp2  = r11;
3512     const Register tmp3  = r12;
3513     const Register tmp4  = r13;
3514     const Register tmp5  = r14;
3515     const Register tmp6  = r15;
3516     const Register tmp7  = r16;
3517 
3518     BLOCK_COMMENT(&quot;Entry:&quot;);
3519     __ enter(); // required for proper stackwalking of RuntimeStub frame
3520     __ multiply_to_len(x, xlen, y, ylen, z, zlen, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7);
3521     __ leave(); // required for proper stackwalking of RuntimeStub frame
3522     __ ret(lr);
3523 
3524     return start;
3525   }
3526 
3527   address generate_squareToLen() {
3528     // squareToLen algorithm for sizes 1..127 described in java code works
3529     // faster than multiply_to_len on some CPUs and slower on others, but
3530     // multiply_to_len shows a bit better overall results
3531     __ align(CodeEntryAlignment);
3532     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;squareToLen&quot;);
3533     address start = __ pc();
3534 
3535     const Register x     = r0;
3536     const Register xlen  = r1;
3537     const Register z     = r2;
3538     const Register zlen  = r3;
3539     const Register y     = r4; // == x
3540     const Register ylen  = r5; // == xlen
3541 
3542     const Register tmp1  = r10;
3543     const Register tmp2  = r11;
3544     const Register tmp3  = r12;
3545     const Register tmp4  = r13;
3546     const Register tmp5  = r14;
3547     const Register tmp6  = r15;
3548     const Register tmp7  = r16;
3549 
3550     RegSet spilled_regs = RegSet::of(y, ylen);
3551     BLOCK_COMMENT(&quot;Entry:&quot;);
3552     __ enter();
3553     __ push(spilled_regs, sp);
3554     __ mov(y, x);
3555     __ mov(ylen, xlen);
3556     __ multiply_to_len(x, xlen, y, ylen, z, zlen, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7);
3557     __ pop(spilled_regs, sp);
3558     __ leave();
3559     __ ret(lr);
3560     return start;
3561   }
3562 
3563   address generate_mulAdd() {
3564     __ align(CodeEntryAlignment);
3565     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;mulAdd&quot;);
3566 
3567     address start = __ pc();
3568 
3569     const Register out     = r0;
3570     const Register in      = r1;
3571     const Register offset  = r2;
3572     const Register len     = r3;
3573     const Register k       = r4;
3574 
3575     BLOCK_COMMENT(&quot;Entry:&quot;);
3576     __ enter();
3577     __ mul_add(out, in, offset, len, k);
3578     __ leave();
3579     __ ret(lr);
3580 
3581     return start;
3582   }
3583 
3584   void ghash_multiply(FloatRegister result_lo, FloatRegister result_hi,
3585                       FloatRegister a, FloatRegister b, FloatRegister a1_xor_a0,
3586                       FloatRegister tmp1, FloatRegister tmp2, FloatRegister tmp3, FloatRegister tmp4) {
3587     // Karatsuba multiplication performs a 128*128 -&gt; 256-bit
3588     // multiplication in three 128-bit multiplications and a few
3589     // additions.
3590     //
3591     // (C1:C0) = A1*B1, (D1:D0) = A0*B0, (E1:E0) = (A0+A1)(B0+B1)
3592     // (A1:A0)(B1:B0) = C1:(C0+C1+D1+E1):(D1+C0+D0+E0):D0
3593     //
3594     // Inputs:
3595     //
3596     // A0 in a.d[0]     (subkey)
3597     // A1 in a.d[1]
3598     // (A1+A0) in a1_xor_a0.d[0]
3599     //
3600     // B0 in b.d[0]     (state)
3601     // B1 in b.d[1]
3602 
3603     __ ext(tmp1, __ T16B, b, b, 0x08);
3604     __ pmull2(result_hi, __ T1Q, b, a, __ T2D);  // A1*B1
3605     __ eor(tmp1, __ T16B, tmp1, b);            // (B1+B0)
3606     __ pmull(result_lo,  __ T1Q, b, a, __ T1D);  // A0*B0
3607     __ pmull(tmp2, __ T1Q, tmp1, a1_xor_a0, __ T1D); // (A1+A0)(B1+B0)
3608 
3609     __ ext(tmp4, __ T16B, result_lo, result_hi, 0x08);
3610     __ eor(tmp3, __ T16B, result_hi, result_lo); // A1*B1+A0*B0
3611     __ eor(tmp2, __ T16B, tmp2, tmp4);
3612     __ eor(tmp2, __ T16B, tmp2, tmp3);
3613 
3614     // Register pair &lt;result_hi:result_lo&gt; holds the result of carry-less multiplication
3615     __ ins(result_hi, __ D, tmp2, 0, 1);
3616     __ ins(result_lo, __ D, tmp2, 1, 0);
3617   }
3618 
3619   void ghash_reduce(FloatRegister result, FloatRegister lo, FloatRegister hi,
3620                     FloatRegister p, FloatRegister z, FloatRegister t1) {
3621     const FloatRegister t0 = result;
3622 
3623     // The GCM field polynomial f is z^128 + p(z), where p =
3624     // z^7+z^2+z+1.
3625     //
3626     //    z^128 === -p(z)  (mod (z^128 + p(z)))
3627     //
3628     // so, given that the product we&#39;re reducing is
3629     //    a == lo + hi * z^128
3630     // substituting,
3631     //      === lo - hi * p(z)  (mod (z^128 + p(z)))
3632     //
3633     // we reduce by multiplying hi by p(z) and subtracting the result
3634     // from (i.e. XORing it with) lo.  Because p has no nonzero high
3635     // bits we can do this with two 64-bit multiplications, lo*p and
3636     // hi*p.
3637 
3638     __ pmull2(t0, __ T1Q, hi, p, __ T2D);
3639     __ ext(t1, __ T16B, t0, z, 8);
3640     __ eor(hi, __ T16B, hi, t1);
3641     __ ext(t1, __ T16B, z, t0, 8);
3642     __ eor(lo, __ T16B, lo, t1);
3643     __ pmull(t0, __ T1Q, hi, p, __ T1D);
3644     __ eor(result, __ T16B, lo, t0);
3645   }
3646 
3647   address generate_has_negatives(address &amp;has_negatives_long) {
3648     const u1 large_loop_size = 64;
3649     const uint64_t UPPER_BIT_MASK=0x8080808080808080;
3650     int dcache_line = VM_Version::dcache_line_size();
3651 
3652     Register ary1 = r1, len = r2, result = r0;
3653 
3654     __ align(CodeEntryAlignment);
3655 
3656     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;has_negatives&quot;);
3657 
3658     address entry = __ pc();
3659 
3660     __ enter();
3661 
3662   Label RET_TRUE, RET_TRUE_NO_POP, RET_FALSE, ALIGNED, LOOP16, CHECK_16, DONE,
3663         LARGE_LOOP, POST_LOOP16, LEN_OVER_15, LEN_OVER_8, POST_LOOP16_LOAD_TAIL;
3664 
3665   __ cmp(len, (u1)15);
3666   __ br(Assembler::GT, LEN_OVER_15);
3667   // The only case when execution falls into this code is when pointer is near
3668   // the end of memory page and we have to avoid reading next page
3669   __ add(ary1, ary1, len);
3670   __ subs(len, len, 8);
3671   __ br(Assembler::GT, LEN_OVER_8);
3672   __ ldr(rscratch2, Address(ary1, -8));
3673   __ sub(rscratch1, zr, len, __ LSL, 3);  // LSL 3 is to get bits from bytes.
3674   __ lsrv(rscratch2, rscratch2, rscratch1);
3675   __ tst(rscratch2, UPPER_BIT_MASK);
3676   __ cset(result, Assembler::NE);
3677   __ leave();
3678   __ ret(lr);
3679   __ bind(LEN_OVER_8);
3680   __ ldp(rscratch1, rscratch2, Address(ary1, -16));
3681   __ sub(len, len, 8); // no data dep., then sub can be executed while loading
3682   __ tst(rscratch2, UPPER_BIT_MASK);
3683   __ br(Assembler::NE, RET_TRUE_NO_POP);
3684   __ sub(rscratch2, zr, len, __ LSL, 3); // LSL 3 is to get bits from bytes
3685   __ lsrv(rscratch1, rscratch1, rscratch2);
3686   __ tst(rscratch1, UPPER_BIT_MASK);
3687   __ cset(result, Assembler::NE);
3688   __ leave();
3689   __ ret(lr);
3690 
3691   Register tmp1 = r3, tmp2 = r4, tmp3 = r5, tmp4 = r6, tmp5 = r7, tmp6 = r10;
3692   const RegSet spilled_regs = RegSet::range(tmp1, tmp5) + tmp6;
3693 
3694   has_negatives_long = __ pc(); // 2nd entry point
3695 
3696   __ enter();
3697 
3698   __ bind(LEN_OVER_15);
3699     __ push(spilled_regs, sp);
3700     __ andr(rscratch2, ary1, 15); // check pointer for 16-byte alignment
3701     __ cbz(rscratch2, ALIGNED);
3702     __ ldp(tmp6, tmp1, Address(ary1));
3703     __ mov(tmp5, 16);
3704     __ sub(rscratch1, tmp5, rscratch2); // amount of bytes until aligned address
3705     __ add(ary1, ary1, rscratch1);
3706     __ sub(len, len, rscratch1);
3707     __ orr(tmp6, tmp6, tmp1);
3708     __ tst(tmp6, UPPER_BIT_MASK);
3709     __ br(Assembler::NE, RET_TRUE);
3710 
3711   __ bind(ALIGNED);
3712     __ cmp(len, large_loop_size);
3713     __ br(Assembler::LT, CHECK_16);
3714     // Perform 16-byte load as early return in pre-loop to handle situation
3715     // when initially aligned large array has negative values at starting bytes,
3716     // so LARGE_LOOP would do 4 reads instead of 1 (in worst case), which is
3717     // slower. Cases with negative bytes further ahead won&#39;t be affected that
3718     // much. In fact, it&#39;ll be faster due to early loads, less instructions and
3719     // less branches in LARGE_LOOP.
3720     __ ldp(tmp6, tmp1, Address(__ post(ary1, 16)));
3721     __ sub(len, len, 16);
3722     __ orr(tmp6, tmp6, tmp1);
3723     __ tst(tmp6, UPPER_BIT_MASK);
3724     __ br(Assembler::NE, RET_TRUE);
3725     __ cmp(len, large_loop_size);
3726     __ br(Assembler::LT, CHECK_16);
3727 
3728     if (SoftwarePrefetchHintDistance &gt;= 0
3729         &amp;&amp; SoftwarePrefetchHintDistance &gt;= dcache_line) {
3730       // initial prefetch
3731       __ prfm(Address(ary1, SoftwarePrefetchHintDistance - dcache_line));
3732     }
3733   __ bind(LARGE_LOOP);
3734     if (SoftwarePrefetchHintDistance &gt;= 0) {
3735       __ prfm(Address(ary1, SoftwarePrefetchHintDistance));
3736     }
3737     // Issue load instructions first, since it can save few CPU/MEM cycles, also
3738     // instead of 4 triples of &quot;orr(...), addr(...);cbnz(...);&quot; (for each ldp)
3739     // better generate 7 * orr(...) + 1 andr(...) + 1 cbnz(...) which saves 3
3740     // instructions per cycle and have less branches, but this approach disables
3741     // early return, thus, all 64 bytes are loaded and checked every time.
3742     __ ldp(tmp2, tmp3, Address(ary1));
3743     __ ldp(tmp4, tmp5, Address(ary1, 16));
3744     __ ldp(rscratch1, rscratch2, Address(ary1, 32));
3745     __ ldp(tmp6, tmp1, Address(ary1, 48));
3746     __ add(ary1, ary1, large_loop_size);
3747     __ sub(len, len, large_loop_size);
3748     __ orr(tmp2, tmp2, tmp3);
3749     __ orr(tmp4, tmp4, tmp5);
3750     __ orr(rscratch1, rscratch1, rscratch2);
3751     __ orr(tmp6, tmp6, tmp1);
3752     __ orr(tmp2, tmp2, tmp4);
3753     __ orr(rscratch1, rscratch1, tmp6);
3754     __ orr(tmp2, tmp2, rscratch1);
3755     __ tst(tmp2, UPPER_BIT_MASK);
3756     __ br(Assembler::NE, RET_TRUE);
3757     __ cmp(len, large_loop_size);
3758     __ br(Assembler::GE, LARGE_LOOP);
3759 
3760   __ bind(CHECK_16); // small 16-byte load pre-loop
3761     __ cmp(len, (u1)16);
3762     __ br(Assembler::LT, POST_LOOP16);
3763 
3764   __ bind(LOOP16); // small 16-byte load loop
3765     __ ldp(tmp2, tmp3, Address(__ post(ary1, 16)));
3766     __ sub(len, len, 16);
3767     __ orr(tmp2, tmp2, tmp3);
3768     __ tst(tmp2, UPPER_BIT_MASK);
3769     __ br(Assembler::NE, RET_TRUE);
3770     __ cmp(len, (u1)16);
3771     __ br(Assembler::GE, LOOP16); // 16-byte load loop end
3772 
3773   __ bind(POST_LOOP16); // 16-byte aligned, so we can read unconditionally
3774     __ cmp(len, (u1)8);
3775     __ br(Assembler::LE, POST_LOOP16_LOAD_TAIL);
3776     __ ldr(tmp3, Address(__ post(ary1, 8)));
3777     __ sub(len, len, 8);
3778     __ tst(tmp3, UPPER_BIT_MASK);
3779     __ br(Assembler::NE, RET_TRUE);
3780 
3781   __ bind(POST_LOOP16_LOAD_TAIL);
3782     __ cbz(len, RET_FALSE); // Can&#39;t shift left by 64 when len==0
3783     __ ldr(tmp1, Address(ary1));
3784     __ mov(tmp2, 64);
3785     __ sub(tmp4, tmp2, len, __ LSL, 3);
3786     __ lslv(tmp1, tmp1, tmp4);
3787     __ tst(tmp1, UPPER_BIT_MASK);
3788     __ br(Assembler::NE, RET_TRUE);
3789     // Fallthrough
3790 
3791   __ bind(RET_FALSE);
3792     __ pop(spilled_regs, sp);
3793     __ leave();
3794     __ mov(result, zr);
3795     __ ret(lr);
3796 
3797   __ bind(RET_TRUE);
3798     __ pop(spilled_regs, sp);
3799   __ bind(RET_TRUE_NO_POP);
3800     __ leave();
3801     __ mov(result, 1);
3802     __ ret(lr);
3803 
3804   __ bind(DONE);
3805     __ pop(spilled_regs, sp);
3806     __ leave();
3807     __ ret(lr);
3808     return entry;
3809   }
3810 
3811   void generate_large_array_equals_loop_nonsimd(int loopThreshold,
3812         bool usePrefetch, Label &amp;NOT_EQUAL) {
3813     Register a1 = r1, a2 = r2, result = r0, cnt1 = r10, tmp1 = rscratch1,
3814         tmp2 = rscratch2, tmp3 = r3, tmp4 = r4, tmp5 = r5, tmp6 = r11,
3815         tmp7 = r12, tmp8 = r13;
3816     Label LOOP;
3817 
3818     __ ldp(tmp1, tmp3, Address(__ post(a1, 2 * wordSize)));
3819     __ ldp(tmp2, tmp4, Address(__ post(a2, 2 * wordSize)));
3820     __ bind(LOOP);
3821     if (usePrefetch) {
3822       __ prfm(Address(a1, SoftwarePrefetchHintDistance));
3823       __ prfm(Address(a2, SoftwarePrefetchHintDistance));
3824     }
3825     __ ldp(tmp5, tmp7, Address(__ post(a1, 2 * wordSize)));
3826     __ eor(tmp1, tmp1, tmp2);
3827     __ eor(tmp3, tmp3, tmp4);
3828     __ ldp(tmp6, tmp8, Address(__ post(a2, 2 * wordSize)));
3829     __ orr(tmp1, tmp1, tmp3);
3830     __ cbnz(tmp1, NOT_EQUAL);
3831     __ ldp(tmp1, tmp3, Address(__ post(a1, 2 * wordSize)));
3832     __ eor(tmp5, tmp5, tmp6);
3833     __ eor(tmp7, tmp7, tmp8);
3834     __ ldp(tmp2, tmp4, Address(__ post(a2, 2 * wordSize)));
3835     __ orr(tmp5, tmp5, tmp7);
3836     __ cbnz(tmp5, NOT_EQUAL);
3837     __ ldp(tmp5, tmp7, Address(__ post(a1, 2 * wordSize)));
3838     __ eor(tmp1, tmp1, tmp2);
3839     __ eor(tmp3, tmp3, tmp4);
3840     __ ldp(tmp6, tmp8, Address(__ post(a2, 2 * wordSize)));
3841     __ orr(tmp1, tmp1, tmp3);
3842     __ cbnz(tmp1, NOT_EQUAL);
3843     __ ldp(tmp1, tmp3, Address(__ post(a1, 2 * wordSize)));
3844     __ eor(tmp5, tmp5, tmp6);
3845     __ sub(cnt1, cnt1, 8 * wordSize);
3846     __ eor(tmp7, tmp7, tmp8);
3847     __ ldp(tmp2, tmp4, Address(__ post(a2, 2 * wordSize)));
3848     // tmp6 is not used. MacroAssembler::subs is used here (rather than
3849     // cmp) because subs allows an unlimited range of immediate operand.
3850     __ subs(tmp6, cnt1, loopThreshold);
3851     __ orr(tmp5, tmp5, tmp7);
3852     __ cbnz(tmp5, NOT_EQUAL);
3853     __ br(__ GE, LOOP);
3854     // post-loop
3855     __ eor(tmp1, tmp1, tmp2);
3856     __ eor(tmp3, tmp3, tmp4);
3857     __ orr(tmp1, tmp1, tmp3);
3858     __ sub(cnt1, cnt1, 2 * wordSize);
3859     __ cbnz(tmp1, NOT_EQUAL);
3860   }
3861 
3862   void generate_large_array_equals_loop_simd(int loopThreshold,
3863         bool usePrefetch, Label &amp;NOT_EQUAL) {
3864     Register a1 = r1, a2 = r2, result = r0, cnt1 = r10, tmp1 = rscratch1,
3865         tmp2 = rscratch2;
3866     Label LOOP;
3867 
3868     __ bind(LOOP);
3869     if (usePrefetch) {
3870       __ prfm(Address(a1, SoftwarePrefetchHintDistance));
3871       __ prfm(Address(a2, SoftwarePrefetchHintDistance));
3872     }
3873     __ ld1(v0, v1, v2, v3, __ T2D, Address(__ post(a1, 4 * 2 * wordSize)));
3874     __ sub(cnt1, cnt1, 8 * wordSize);
3875     __ ld1(v4, v5, v6, v7, __ T2D, Address(__ post(a2, 4 * 2 * wordSize)));
3876     __ subs(tmp1, cnt1, loopThreshold);
3877     __ eor(v0, __ T16B, v0, v4);
3878     __ eor(v1, __ T16B, v1, v5);
3879     __ eor(v2, __ T16B, v2, v6);
3880     __ eor(v3, __ T16B, v3, v7);
3881     __ orr(v0, __ T16B, v0, v1);
3882     __ orr(v1, __ T16B, v2, v3);
3883     __ orr(v0, __ T16B, v0, v1);
3884     __ umov(tmp1, v0, __ D, 0);
3885     __ umov(tmp2, v0, __ D, 1);
3886     __ orr(tmp1, tmp1, tmp2);
3887     __ cbnz(tmp1, NOT_EQUAL);
3888     __ br(__ GE, LOOP);
3889   }
3890 
3891   // a1 = r1 - array1 address
3892   // a2 = r2 - array2 address
3893   // result = r0 - return value. Already contains &quot;false&quot;
3894   // cnt1 = r10 - amount of elements left to check, reduced by wordSize
3895   // r3-r5 are reserved temporary registers
3896   address generate_large_array_equals() {
3897     Register a1 = r1, a2 = r2, result = r0, cnt1 = r10, tmp1 = rscratch1,
3898         tmp2 = rscratch2, tmp3 = r3, tmp4 = r4, tmp5 = r5, tmp6 = r11,
3899         tmp7 = r12, tmp8 = r13;
3900     Label TAIL, NOT_EQUAL, EQUAL, NOT_EQUAL_NO_POP, NO_PREFETCH_LARGE_LOOP,
3901         SMALL_LOOP, POST_LOOP;
3902     const int PRE_LOOP_SIZE = UseSIMDForArrayEquals ? 0 : 16;
3903     // calculate if at least 32 prefetched bytes are used
3904     int prefetchLoopThreshold = SoftwarePrefetchHintDistance + 32;
3905     int nonPrefetchLoopThreshold = (64 + PRE_LOOP_SIZE);
3906     RegSet spilled_regs = RegSet::range(tmp6, tmp8);
3907     assert_different_registers(a1, a2, result, cnt1, tmp1, tmp2, tmp3, tmp4,
3908         tmp5, tmp6, tmp7, tmp8);
3909 
3910     __ align(CodeEntryAlignment);
3911 
3912     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;large_array_equals&quot;);
3913 
3914     address entry = __ pc();
3915     __ enter();
3916     __ sub(cnt1, cnt1, wordSize);  // first 8 bytes were loaded outside of stub
3917     // also advance pointers to use post-increment instead of pre-increment
3918     __ add(a1, a1, wordSize);
3919     __ add(a2, a2, wordSize);
3920     if (AvoidUnalignedAccesses) {
3921       // both implementations (SIMD/nonSIMD) are using relatively large load
3922       // instructions (ld1/ldp), which has huge penalty (up to x2 exec time)
3923       // on some CPUs in case of address is not at least 16-byte aligned.
3924       // Arrays are 8-byte aligned currently, so, we can make additional 8-byte
3925       // load if needed at least for 1st address and make if 16-byte aligned.
3926       Label ALIGNED16;
3927       __ tbz(a1, 3, ALIGNED16);
3928       __ ldr(tmp1, Address(__ post(a1, wordSize)));
3929       __ ldr(tmp2, Address(__ post(a2, wordSize)));
3930       __ sub(cnt1, cnt1, wordSize);
3931       __ eor(tmp1, tmp1, tmp2);
3932       __ cbnz(tmp1, NOT_EQUAL_NO_POP);
3933       __ bind(ALIGNED16);
3934     }
3935     if (UseSIMDForArrayEquals) {
3936       if (SoftwarePrefetchHintDistance &gt;= 0) {
3937         __ subs(tmp1, cnt1, prefetchLoopThreshold);
3938         __ br(__ LE, NO_PREFETCH_LARGE_LOOP);
3939         generate_large_array_equals_loop_simd(prefetchLoopThreshold,
3940             /* prfm = */ true, NOT_EQUAL);
3941         __ subs(zr, cnt1, nonPrefetchLoopThreshold);
3942         __ br(__ LT, TAIL);
3943       }
3944       __ bind(NO_PREFETCH_LARGE_LOOP);
3945       generate_large_array_equals_loop_simd(nonPrefetchLoopThreshold,
3946           /* prfm = */ false, NOT_EQUAL);
3947     } else {
3948       __ push(spilled_regs, sp);
3949       if (SoftwarePrefetchHintDistance &gt;= 0) {
3950         __ subs(tmp1, cnt1, prefetchLoopThreshold);
3951         __ br(__ LE, NO_PREFETCH_LARGE_LOOP);
3952         generate_large_array_equals_loop_nonsimd(prefetchLoopThreshold,
3953             /* prfm = */ true, NOT_EQUAL);
3954         __ subs(zr, cnt1, nonPrefetchLoopThreshold);
3955         __ br(__ LT, TAIL);
3956       }
3957       __ bind(NO_PREFETCH_LARGE_LOOP);
3958       generate_large_array_equals_loop_nonsimd(nonPrefetchLoopThreshold,
3959           /* prfm = */ false, NOT_EQUAL);
3960     }
3961     __ bind(TAIL);
3962       __ cbz(cnt1, EQUAL);
3963       __ subs(cnt1, cnt1, wordSize);
3964       __ br(__ LE, POST_LOOP);
3965     __ bind(SMALL_LOOP);
3966       __ ldr(tmp1, Address(__ post(a1, wordSize)));
3967       __ ldr(tmp2, Address(__ post(a2, wordSize)));
3968       __ subs(cnt1, cnt1, wordSize);
3969       __ eor(tmp1, tmp1, tmp2);
3970       __ cbnz(tmp1, NOT_EQUAL);
3971       __ br(__ GT, SMALL_LOOP);
3972     __ bind(POST_LOOP);
3973       __ ldr(tmp1, Address(a1, cnt1));
3974       __ ldr(tmp2, Address(a2, cnt1));
3975       __ eor(tmp1, tmp1, tmp2);
3976       __ cbnz(tmp1, NOT_EQUAL);
3977     __ bind(EQUAL);
3978       __ mov(result, true);
3979     __ bind(NOT_EQUAL);
3980       if (!UseSIMDForArrayEquals) {
3981         __ pop(spilled_regs, sp);
3982       }
3983     __ bind(NOT_EQUAL_NO_POP);
3984     __ leave();
3985     __ ret(lr);
3986     return entry;
3987   }
3988 
3989   address generate_dsin_dcos(bool isCos) {
3990     __ align(CodeEntryAlignment);
3991     StubCodeMark mark(this, &quot;StubRoutines&quot;, isCos ? &quot;libmDcos&quot; : &quot;libmDsin&quot;);
3992     address start = __ pc();
3993     __ generate_dsin_dcos(isCos, (address)StubRoutines::aarch64::_npio2_hw,
3994         (address)StubRoutines::aarch64::_two_over_pi,
3995         (address)StubRoutines::aarch64::_pio2,
3996         (address)StubRoutines::aarch64::_dsin_coef,
3997         (address)StubRoutines::aarch64::_dcos_coef);
3998     return start;
3999   }
4000 
4001   address generate_dlog() {
4002     __ align(CodeEntryAlignment);
4003     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;dlog&quot;);
4004     address entry = __ pc();
4005     FloatRegister vtmp0 = v0, vtmp1 = v1, vtmp2 = v2, vtmp3 = v3, vtmp4 = v4,
4006         vtmp5 = v5, tmpC1 = v16, tmpC2 = v17, tmpC3 = v18, tmpC4 = v19;
4007     Register tmp1 = r0, tmp2 = r1, tmp3 = r2, tmp4 = r3, tmp5 = r4;
4008     __ fast_log(vtmp0, vtmp1, vtmp2, vtmp3, vtmp4, vtmp5, tmpC1, tmpC2, tmpC3,
4009         tmpC4, tmp1, tmp2, tmp3, tmp4, tmp5);
4010     return entry;
4011   }
4012 
4013   // code for comparing 16 bytes of strings with same encoding
4014   void compare_string_16_bytes_same(Label &amp;DIFF1, Label &amp;DIFF2) {
4015     Register result = r0, str1 = r1, cnt1 = r2, str2 = r3, tmp1 = r10, tmp2 = r11;
4016     __ ldr(rscratch1, Address(__ post(str1, 8)));
4017     __ eor(rscratch2, tmp1, tmp2);
4018     __ ldr(cnt1, Address(__ post(str2, 8)));
4019     __ cbnz(rscratch2, DIFF1);
4020     __ ldr(tmp1, Address(__ post(str1, 8)));
4021     __ eor(rscratch2, rscratch1, cnt1);
4022     __ ldr(tmp2, Address(__ post(str2, 8)));
4023     __ cbnz(rscratch2, DIFF2);
4024   }
4025 
4026   // code for comparing 16 characters of strings with Latin1 and Utf16 encoding
4027   void compare_string_16_x_LU(Register tmpL, Register tmpU, Label &amp;DIFF1,
4028       Label &amp;DIFF2) {
4029     Register cnt1 = r2, tmp2 = r11, tmp3 = r12;
4030     FloatRegister vtmp = v1, vtmpZ = v0, vtmp3 = v2;
4031 
4032     __ ldrq(vtmp, Address(__ post(tmp2, 16)));
4033     __ ldr(tmpU, Address(__ post(cnt1, 8)));
4034     __ zip1(vtmp3, __ T16B, vtmp, vtmpZ);
4035     // now we have 32 bytes of characters (converted to U) in vtmp:vtmp3
4036 
4037     __ fmovd(tmpL, vtmp3);
4038     __ eor(rscratch2, tmp3, tmpL);
4039     __ cbnz(rscratch2, DIFF2);
4040 
4041     __ ldr(tmp3, Address(__ post(cnt1, 8)));
4042     __ umov(tmpL, vtmp3, __ D, 1);
4043     __ eor(rscratch2, tmpU, tmpL);
4044     __ cbnz(rscratch2, DIFF1);
4045 
4046     __ zip2(vtmp, __ T16B, vtmp, vtmpZ);
4047     __ ldr(tmpU, Address(__ post(cnt1, 8)));
4048     __ fmovd(tmpL, vtmp);
4049     __ eor(rscratch2, tmp3, tmpL);
4050     __ cbnz(rscratch2, DIFF2);
4051 
4052     __ ldr(tmp3, Address(__ post(cnt1, 8)));
4053     __ umov(tmpL, vtmp, __ D, 1);
4054     __ eor(rscratch2, tmpU, tmpL);
4055     __ cbnz(rscratch2, DIFF1);
4056   }
4057 
4058   // r0  = result
4059   // r1  = str1
4060   // r2  = cnt1
4061   // r3  = str2
4062   // r4  = cnt2
4063   // r10 = tmp1
4064   // r11 = tmp2
4065   address generate_compare_long_string_different_encoding(bool isLU) {
4066     __ align(CodeEntryAlignment);
4067     StubCodeMark mark(this, &quot;StubRoutines&quot;, isLU
4068         ? &quot;compare_long_string_different_encoding LU&quot;
4069         : &quot;compare_long_string_different_encoding UL&quot;);
4070     address entry = __ pc();
4071     Label SMALL_LOOP, TAIL, TAIL_LOAD_16, LOAD_LAST, DIFF1, DIFF2,
4072         DONE, CALCULATE_DIFFERENCE, LARGE_LOOP_PREFETCH, NO_PREFETCH,
4073         LARGE_LOOP_PREFETCH_REPEAT1, LARGE_LOOP_PREFETCH_REPEAT2;
4074     Register result = r0, str1 = r1, cnt1 = r2, str2 = r3, cnt2 = r4,
4075         tmp1 = r10, tmp2 = r11, tmp3 = r12, tmp4 = r14;
4076     FloatRegister vtmpZ = v0, vtmp = v1, vtmp3 = v2;
4077     RegSet spilled_regs = RegSet::of(tmp3, tmp4);
4078 
4079     int prefetchLoopExitCondition = MAX(64, SoftwarePrefetchHintDistance/2);
4080 
4081     __ eor(vtmpZ, __ T16B, vtmpZ, vtmpZ);
4082     // cnt2 == amount of characters left to compare
4083     // Check already loaded first 4 symbols(vtmp and tmp2(LU)/tmp1(UL))
4084     __ zip1(vtmp, __ T8B, vtmp, vtmpZ);
4085     __ add(str1, str1, isLU ? wordSize/2 : wordSize);
4086     __ add(str2, str2, isLU ? wordSize : wordSize/2);
4087     __ fmovd(isLU ? tmp1 : tmp2, vtmp);
4088     __ subw(cnt2, cnt2, 8); // Already loaded 4 symbols. Last 4 is special case.
4089     __ eor(rscratch2, tmp1, tmp2);
4090     __ mov(rscratch1, tmp2);
4091     __ cbnz(rscratch2, CALCULATE_DIFFERENCE);
4092     Register tmpU = isLU ? rscratch1 : tmp1, // where to keep U for comparison
4093              tmpL = isLU ? tmp1 : rscratch1; // where to keep L for comparison
4094     __ push(spilled_regs, sp);
4095     __ mov(tmp2, isLU ? str1 : str2); // init the pointer to L next load
4096     __ mov(cnt1, isLU ? str2 : str1); // init the pointer to U next load
4097 
4098     __ ldr(tmp3, Address(__ post(cnt1, 8)));
4099 
4100     if (SoftwarePrefetchHintDistance &gt;= 0) {
4101       __ subs(rscratch2, cnt2, prefetchLoopExitCondition);
4102       __ br(__ LT, NO_PREFETCH);
4103       __ bind(LARGE_LOOP_PREFETCH);
4104         __ prfm(Address(tmp2, SoftwarePrefetchHintDistance));
4105         __ mov(tmp4, 2);
4106         __ prfm(Address(cnt1, SoftwarePrefetchHintDistance));
4107         __ bind(LARGE_LOOP_PREFETCH_REPEAT1);
4108           compare_string_16_x_LU(tmpL, tmpU, DIFF1, DIFF2);
4109           __ subs(tmp4, tmp4, 1);
4110           __ br(__ GT, LARGE_LOOP_PREFETCH_REPEAT1);
4111           __ prfm(Address(cnt1, SoftwarePrefetchHintDistance));
4112           __ mov(tmp4, 2);
4113         __ bind(LARGE_LOOP_PREFETCH_REPEAT2);
4114           compare_string_16_x_LU(tmpL, tmpU, DIFF1, DIFF2);
4115           __ subs(tmp4, tmp4, 1);
4116           __ br(__ GT, LARGE_LOOP_PREFETCH_REPEAT2);
4117           __ sub(cnt2, cnt2, 64);
4118           __ subs(rscratch2, cnt2, prefetchLoopExitCondition);
4119           __ br(__ GE, LARGE_LOOP_PREFETCH);
4120     }
4121     __ cbz(cnt2, LOAD_LAST); // no characters left except last load
4122     __ bind(NO_PREFETCH);
4123     __ subs(cnt2, cnt2, 16);
4124     __ br(__ LT, TAIL);
4125     __ align(OptoLoopAlignment);
4126     __ bind(SMALL_LOOP); // smaller loop
4127       __ subs(cnt2, cnt2, 16);
4128       compare_string_16_x_LU(tmpL, tmpU, DIFF1, DIFF2);
4129       __ br(__ GE, SMALL_LOOP);
4130       __ cmn(cnt2, (u1)16);
4131       __ br(__ EQ, LOAD_LAST);
4132     __ bind(TAIL); // 1..15 characters left until last load (last 4 characters)
4133       __ add(cnt1, cnt1, cnt2, __ LSL, 1); // Address of 32 bytes before last 4 characters in UTF-16 string
4134       __ add(tmp2, tmp2, cnt2); // Address of 16 bytes before last 4 characters in Latin1 string
4135       __ ldr(tmp3, Address(cnt1, -8));
4136       compare_string_16_x_LU(tmpL, tmpU, DIFF1, DIFF2); // last 16 characters before last load
4137       __ b(LOAD_LAST);
4138     __ bind(DIFF2);
4139       __ mov(tmpU, tmp3);
4140     __ bind(DIFF1);
4141       __ pop(spilled_regs, sp);
4142       __ b(CALCULATE_DIFFERENCE);
4143     __ bind(LOAD_LAST);
4144       // Last 4 UTF-16 characters are already pre-loaded into tmp3 by compare_string_16_x_LU.
4145       // No need to load it again
4146       __ mov(tmpU, tmp3);
4147       __ pop(spilled_regs, sp);
4148 
4149       // tmp2 points to the address of the last 4 Latin1 characters right now
4150       __ ldrs(vtmp, Address(tmp2));
4151       __ zip1(vtmp, __ T8B, vtmp, vtmpZ);
4152       __ fmovd(tmpL, vtmp);
4153 
4154       __ eor(rscratch2, tmpU, tmpL);
4155       __ cbz(rscratch2, DONE);
4156 
4157     // Find the first different characters in the longwords and
4158     // compute their difference.
4159     __ bind(CALCULATE_DIFFERENCE);
4160       __ rev(rscratch2, rscratch2);
4161       __ clz(rscratch2, rscratch2);
4162       __ andr(rscratch2, rscratch2, -16);
4163       __ lsrv(tmp1, tmp1, rscratch2);
4164       __ uxthw(tmp1, tmp1);
4165       __ lsrv(rscratch1, rscratch1, rscratch2);
4166       __ uxthw(rscratch1, rscratch1);
4167       __ subw(result, tmp1, rscratch1);
4168     __ bind(DONE);
4169       __ ret(lr);
4170     return entry;
4171   }
4172 
4173     address generate_method_entry_barrier() {
4174     __ align(CodeEntryAlignment);
4175     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;nmethod_entry_barrier&quot;);
4176 
4177     Label deoptimize_label;
4178 
4179     address start = __ pc();
4180 
4181     __ set_last_Java_frame(sp, rfp, lr, rscratch1);
4182 
4183     __ enter();
4184     __ add(rscratch2, sp, wordSize);  // rscratch2 points to the saved lr
4185 
4186     __ sub(sp, sp, 4 * wordSize);  // four words for the returned {sp, fp, lr, pc}
4187 
4188     __ push_call_clobbered_registers();
4189 
4190     __ mov(c_rarg0, rscratch2);
4191     __ call_VM_leaf
4192          (CAST_FROM_FN_PTR
4193           (address, BarrierSetNMethod::nmethod_stub_entry_barrier), 1);
4194 
4195     __ reset_last_Java_frame(true);
4196 
4197     __ mov(rscratch1, r0);
4198 
4199     __ pop_call_clobbered_registers();
4200 
4201     __ cbnz(rscratch1, deoptimize_label);
4202 
4203     __ leave();
4204     __ ret(lr);
4205 
4206     __ BIND(deoptimize_label);
4207 
4208     __ ldp(/* new sp */ rscratch1, rfp, Address(sp, 0 * wordSize));
4209     __ ldp(lr, /* new pc*/ rscratch2, Address(sp, 2 * wordSize));
4210 
4211     __ mov(sp, rscratch1);
4212     __ br(rscratch2);
4213 
4214     return start;
4215   }
4216 
4217   // r0  = result
4218   // r1  = str1
4219   // r2  = cnt1
4220   // r3  = str2
4221   // r4  = cnt2
4222   // r10 = tmp1
4223   // r11 = tmp2
4224   address generate_compare_long_string_same_encoding(bool isLL) {
4225     __ align(CodeEntryAlignment);
4226     StubCodeMark mark(this, &quot;StubRoutines&quot;, isLL
4227         ? &quot;compare_long_string_same_encoding LL&quot;
4228         : &quot;compare_long_string_same_encoding UU&quot;);
4229     address entry = __ pc();
4230     Register result = r0, str1 = r1, cnt1 = r2, str2 = r3, cnt2 = r4,
4231         tmp1 = r10, tmp2 = r11;
4232     Label SMALL_LOOP, LARGE_LOOP_PREFETCH, CHECK_LAST, DIFF2, TAIL,
4233         LENGTH_DIFF, DIFF, LAST_CHECK_AND_LENGTH_DIFF,
4234         DIFF_LAST_POSITION, DIFF_LAST_POSITION2;
4235     // exit from large loop when less than 64 bytes left to read or we&#39;re about
4236     // to prefetch memory behind array border
4237     int largeLoopExitCondition = MAX(64, SoftwarePrefetchHintDistance)/(isLL ? 1 : 2);
4238     // cnt1/cnt2 contains amount of characters to compare. cnt1 can be re-used
4239     // update cnt2 counter with already loaded 8 bytes
4240     __ sub(cnt2, cnt2, wordSize/(isLL ? 1 : 2));
4241     // update pointers, because of previous read
4242     __ add(str1, str1, wordSize);
4243     __ add(str2, str2, wordSize);
4244     if (SoftwarePrefetchHintDistance &gt;= 0) {
4245       __ bind(LARGE_LOOP_PREFETCH);
4246         __ prfm(Address(str1, SoftwarePrefetchHintDistance));
4247         __ prfm(Address(str2, SoftwarePrefetchHintDistance));
4248         compare_string_16_bytes_same(DIFF, DIFF2);
4249         compare_string_16_bytes_same(DIFF, DIFF2);
4250         __ sub(cnt2, cnt2, isLL ? 64 : 32);
4251         compare_string_16_bytes_same(DIFF, DIFF2);
4252         __ subs(rscratch2, cnt2, largeLoopExitCondition);
4253         compare_string_16_bytes_same(DIFF, DIFF2);
4254         __ br(__ GT, LARGE_LOOP_PREFETCH);
4255         __ cbz(cnt2, LAST_CHECK_AND_LENGTH_DIFF); // no more chars left?
4256     }
4257     // less than 16 bytes left?
4258     __ subs(cnt2, cnt2, isLL ? 16 : 8);
4259     __ br(__ LT, TAIL);
4260     __ align(OptoLoopAlignment);
4261     __ bind(SMALL_LOOP);
4262       compare_string_16_bytes_same(DIFF, DIFF2);
4263       __ subs(cnt2, cnt2, isLL ? 16 : 8);
4264       __ br(__ GE, SMALL_LOOP);
4265     __ bind(TAIL);
4266       __ adds(cnt2, cnt2, isLL ? 16 : 8);
4267       __ br(__ EQ, LAST_CHECK_AND_LENGTH_DIFF);
4268       __ subs(cnt2, cnt2, isLL ? 8 : 4);
4269       __ br(__ LE, CHECK_LAST);
4270       __ eor(rscratch2, tmp1, tmp2);
4271       __ cbnz(rscratch2, DIFF);
4272       __ ldr(tmp1, Address(__ post(str1, 8)));
4273       __ ldr(tmp2, Address(__ post(str2, 8)));
4274       __ sub(cnt2, cnt2, isLL ? 8 : 4);
4275     __ bind(CHECK_LAST);
4276       if (!isLL) {
4277         __ add(cnt2, cnt2, cnt2); // now in bytes
4278       }
4279       __ eor(rscratch2, tmp1, tmp2);
4280       __ cbnz(rscratch2, DIFF);
4281       __ ldr(rscratch1, Address(str1, cnt2));
4282       __ ldr(cnt1, Address(str2, cnt2));
4283       __ eor(rscratch2, rscratch1, cnt1);
4284       __ cbz(rscratch2, LENGTH_DIFF);
4285       // Find the first different characters in the longwords and
4286       // compute their difference.
4287     __ bind(DIFF2);
4288       __ rev(rscratch2, rscratch2);
4289       __ clz(rscratch2, rscratch2);
4290       __ andr(rscratch2, rscratch2, isLL ? -8 : -16);
4291       __ lsrv(rscratch1, rscratch1, rscratch2);
4292       if (isLL) {
4293         __ lsrv(cnt1, cnt1, rscratch2);
4294         __ uxtbw(rscratch1, rscratch1);
4295         __ uxtbw(cnt1, cnt1);
4296       } else {
4297         __ lsrv(cnt1, cnt1, rscratch2);
4298         __ uxthw(rscratch1, rscratch1);
4299         __ uxthw(cnt1, cnt1);
4300       }
4301       __ subw(result, rscratch1, cnt1);
4302       __ b(LENGTH_DIFF);
4303     __ bind(DIFF);
4304       __ rev(rscratch2, rscratch2);
4305       __ clz(rscratch2, rscratch2);
4306       __ andr(rscratch2, rscratch2, isLL ? -8 : -16);
4307       __ lsrv(tmp1, tmp1, rscratch2);
4308       if (isLL) {
4309         __ lsrv(tmp2, tmp2, rscratch2);
4310         __ uxtbw(tmp1, tmp1);
4311         __ uxtbw(tmp2, tmp2);
4312       } else {
4313         __ lsrv(tmp2, tmp2, rscratch2);
4314         __ uxthw(tmp1, tmp1);
4315         __ uxthw(tmp2, tmp2);
4316       }
4317       __ subw(result, tmp1, tmp2);
4318       __ b(LENGTH_DIFF);
4319     __ bind(LAST_CHECK_AND_LENGTH_DIFF);
4320       __ eor(rscratch2, tmp1, tmp2);
4321       __ cbnz(rscratch2, DIFF);
4322     __ bind(LENGTH_DIFF);
4323       __ ret(lr);
4324     return entry;
4325   }
4326 
4327   void generate_compare_long_strings() {
4328       StubRoutines::aarch64::_compare_long_string_LL
4329           = generate_compare_long_string_same_encoding(true);
4330       StubRoutines::aarch64::_compare_long_string_UU
4331           = generate_compare_long_string_same_encoding(false);
4332       StubRoutines::aarch64::_compare_long_string_LU
4333           = generate_compare_long_string_different_encoding(true);
4334       StubRoutines::aarch64::_compare_long_string_UL
4335           = generate_compare_long_string_different_encoding(false);
4336   }
4337 
4338   // R0 = result
4339   // R1 = str2
4340   // R2 = cnt1
4341   // R3 = str1
4342   // R4 = cnt2
4343   // This generic linear code use few additional ideas, which makes it faster:
4344   // 1) we can safely keep at least 1st register of pattern(since length &gt;= 8)
4345   // in order to skip initial loading(help in systems with 1 ld pipeline)
4346   // 2) we can use &quot;fast&quot; algorithm of finding single character to search for
4347   // first symbol with less branches(1 branch per each loaded register instead
4348   // of branch for each symbol), so, this is where constants like
4349   // 0x0101...01, 0x00010001...0001, 0x7f7f...7f, 0x7fff7fff...7fff comes from
4350   // 3) after loading and analyzing 1st register of source string, it can be
4351   // used to search for every 1st character entry, saving few loads in
4352   // comparison with &quot;simplier-but-slower&quot; implementation
4353   // 4) in order to avoid lots of push/pop operations, code below is heavily
4354   // re-using/re-initializing/compressing register values, which makes code
4355   // larger and a bit less readable, however, most of extra operations are
4356   // issued during loads or branches, so, penalty is minimal
4357   address generate_string_indexof_linear(bool str1_isL, bool str2_isL) {
4358     const char* stubName = str1_isL
4359         ? (str2_isL ? &quot;indexof_linear_ll&quot; : &quot;indexof_linear_ul&quot;)
4360         : &quot;indexof_linear_uu&quot;;
4361     __ align(CodeEntryAlignment);
4362     StubCodeMark mark(this, &quot;StubRoutines&quot;, stubName);
4363     address entry = __ pc();
4364 
4365     int str1_chr_size = str1_isL ? 1 : 2;
4366     int str2_chr_size = str2_isL ? 1 : 2;
4367     int str1_chr_shift = str1_isL ? 0 : 1;
4368     int str2_chr_shift = str2_isL ? 0 : 1;
4369     bool isL = str1_isL &amp;&amp; str2_isL;
4370    // parameters
4371     Register result = r0, str2 = r1, cnt1 = r2, str1 = r3, cnt2 = r4;
4372     // temporary registers
4373     Register tmp1 = r20, tmp2 = r21, tmp3 = r22, tmp4 = r23;
4374     RegSet spilled_regs = RegSet::range(tmp1, tmp4);
4375     // redefinitions
4376     Register ch1 = rscratch1, ch2 = rscratch2, first = tmp3;
4377 
4378     __ push(spilled_regs, sp);
4379     Label L_LOOP, L_LOOP_PROCEED, L_SMALL, L_HAS_ZERO,
4380         L_HAS_ZERO_LOOP, L_CMP_LOOP, L_CMP_LOOP_NOMATCH, L_SMALL_PROCEED,
4381         L_SMALL_HAS_ZERO_LOOP, L_SMALL_CMP_LOOP_NOMATCH, L_SMALL_CMP_LOOP,
4382         L_POST_LOOP, L_CMP_LOOP_LAST_CMP, L_HAS_ZERO_LOOP_NOMATCH,
4383         L_SMALL_CMP_LOOP_LAST_CMP, L_SMALL_CMP_LOOP_LAST_CMP2,
4384         L_CMP_LOOP_LAST_CMP2, DONE, NOMATCH;
4385     // Read whole register from str1. It is safe, because length &gt;=8 here
4386     __ ldr(ch1, Address(str1));
4387     // Read whole register from str2. It is safe, because length &gt;=8 here
4388     __ ldr(ch2, Address(str2));
4389     __ sub(cnt2, cnt2, cnt1);
4390     __ andr(first, ch1, str1_isL ? 0xFF : 0xFFFF);
4391     if (str1_isL != str2_isL) {
4392       __ eor(v0, __ T16B, v0, v0);
4393     }
4394     __ mov(tmp1, str2_isL ? 0x0101010101010101 : 0x0001000100010001);
4395     __ mul(first, first, tmp1);
4396     // check if we have less than 1 register to check
4397     __ subs(cnt2, cnt2, wordSize/str2_chr_size - 1);
4398     if (str1_isL != str2_isL) {
4399       __ fmovd(v1, ch1);
4400     }
4401     __ br(__ LE, L_SMALL);
4402     __ eor(ch2, first, ch2);
4403     if (str1_isL != str2_isL) {
4404       __ zip1(v1, __ T16B, v1, v0);
4405     }
4406     __ sub(tmp2, ch2, tmp1);
4407     __ orr(ch2, ch2, str2_isL ? 0x7f7f7f7f7f7f7f7f : 0x7fff7fff7fff7fff);
4408     __ bics(tmp2, tmp2, ch2);
4409     if (str1_isL != str2_isL) {
4410       __ fmovd(ch1, v1);
4411     }
4412     __ br(__ NE, L_HAS_ZERO);
4413     __ subs(cnt2, cnt2, wordSize/str2_chr_size);
4414     __ add(result, result, wordSize/str2_chr_size);
4415     __ add(str2, str2, wordSize);
4416     __ br(__ LT, L_POST_LOOP);
4417     __ BIND(L_LOOP);
4418       __ ldr(ch2, Address(str2));
4419       __ eor(ch2, first, ch2);
4420       __ sub(tmp2, ch2, tmp1);
4421       __ orr(ch2, ch2, str2_isL ? 0x7f7f7f7f7f7f7f7f : 0x7fff7fff7fff7fff);
4422       __ bics(tmp2, tmp2, ch2);
4423       __ br(__ NE, L_HAS_ZERO);
4424     __ BIND(L_LOOP_PROCEED);
4425       __ subs(cnt2, cnt2, wordSize/str2_chr_size);
4426       __ add(str2, str2, wordSize);
4427       __ add(result, result, wordSize/str2_chr_size);
4428       __ br(__ GE, L_LOOP);
4429     __ BIND(L_POST_LOOP);
4430       __ subs(zr, cnt2, -wordSize/str2_chr_size); // no extra characters to check
4431       __ br(__ LE, NOMATCH);
4432       __ ldr(ch2, Address(str2));
4433       __ sub(cnt2, zr, cnt2, __ LSL, LogBitsPerByte + str2_chr_shift);
4434       __ eor(ch2, first, ch2);
4435       __ sub(tmp2, ch2, tmp1);
4436       __ orr(ch2, ch2, str2_isL ? 0x7f7f7f7f7f7f7f7f : 0x7fff7fff7fff7fff);
4437       __ mov(tmp4, -1); // all bits set
4438       __ b(L_SMALL_PROCEED);
4439     __ align(OptoLoopAlignment);
4440     __ BIND(L_SMALL);
4441       __ sub(cnt2, zr, cnt2, __ LSL, LogBitsPerByte + str2_chr_shift);
4442       __ eor(ch2, first, ch2);
4443       if (str1_isL != str2_isL) {
4444         __ zip1(v1, __ T16B, v1, v0);
4445       }
4446       __ sub(tmp2, ch2, tmp1);
4447       __ mov(tmp4, -1); // all bits set
4448       __ orr(ch2, ch2, str2_isL ? 0x7f7f7f7f7f7f7f7f : 0x7fff7fff7fff7fff);
4449       if (str1_isL != str2_isL) {
4450         __ fmovd(ch1, v1); // move converted 4 symbols
4451       }
4452     __ BIND(L_SMALL_PROCEED);
4453       __ lsrv(tmp4, tmp4, cnt2); // mask. zeroes on useless bits.
4454       __ bic(tmp2, tmp2, ch2);
4455       __ ands(tmp2, tmp2, tmp4); // clear useless bits and check
4456       __ rbit(tmp2, tmp2);
4457       __ br(__ EQ, NOMATCH);
4458     __ BIND(L_SMALL_HAS_ZERO_LOOP);
4459       __ clz(tmp4, tmp2); // potentially long. Up to 4 cycles on some cpu&#39;s
4460       __ cmp(cnt1, u1(wordSize/str2_chr_size));
4461       __ br(__ LE, L_SMALL_CMP_LOOP_LAST_CMP2);
4462       if (str2_isL) { // LL
4463         __ add(str2, str2, tmp4, __ LSR, LogBitsPerByte); // address of &quot;index&quot;
4464         __ ldr(ch2, Address(str2)); // read whole register of str2. Safe.
4465         __ lslv(tmp2, tmp2, tmp4); // shift off leading zeroes from match info
4466         __ add(result, result, tmp4, __ LSR, LogBitsPerByte);
4467         __ lsl(tmp2, tmp2, 1); // shift off leading &quot;1&quot; from match info
4468       } else {
4469         __ mov(ch2, 0xE); // all bits in byte set except last one
4470         __ andr(ch2, ch2, tmp4, __ LSR, LogBitsPerByte); // byte shift amount
4471         __ ldr(ch2, Address(str2, ch2)); // read whole register of str2. Safe.
4472         __ lslv(tmp2, tmp2, tmp4);
4473         __ add(result, result, tmp4, __ LSR, LogBitsPerByte + str2_chr_shift);
4474         __ add(str2, str2, tmp4, __ LSR, LogBitsPerByte + str2_chr_shift);
4475         __ lsl(tmp2, tmp2, 1); // shift off leading &quot;1&quot; from match info
4476         __ add(str2, str2, tmp4, __ LSR, LogBitsPerByte + str2_chr_shift);
4477       }
4478       __ cmp(ch1, ch2);
4479       __ mov(tmp4, wordSize/str2_chr_size);
4480       __ br(__ NE, L_SMALL_CMP_LOOP_NOMATCH);
4481     __ BIND(L_SMALL_CMP_LOOP);
4482       str1_isL ? __ ldrb(first, Address(str1, tmp4, Address::lsl(str1_chr_shift)))
4483                : __ ldrh(first, Address(str1, tmp4, Address::lsl(str1_chr_shift)));
4484       str2_isL ? __ ldrb(ch2, Address(str2, tmp4, Address::lsl(str2_chr_shift)))
4485                : __ ldrh(ch2, Address(str2, tmp4, Address::lsl(str2_chr_shift)));
4486       __ add(tmp4, tmp4, 1);
4487       __ cmp(tmp4, cnt1);
4488       __ br(__ GE, L_SMALL_CMP_LOOP_LAST_CMP);
4489       __ cmp(first, ch2);
4490       __ br(__ EQ, L_SMALL_CMP_LOOP);
4491     __ BIND(L_SMALL_CMP_LOOP_NOMATCH);
4492       __ cbz(tmp2, NOMATCH); // no more matches. exit
4493       __ clz(tmp4, tmp2);
4494       __ add(result, result, 1); // advance index
4495       __ add(str2, str2, str2_chr_size); // advance pointer
4496       __ b(L_SMALL_HAS_ZERO_LOOP);
4497     __ align(OptoLoopAlignment);
4498     __ BIND(L_SMALL_CMP_LOOP_LAST_CMP);
4499       __ cmp(first, ch2);
4500       __ br(__ NE, L_SMALL_CMP_LOOP_NOMATCH);
4501       __ b(DONE);
4502     __ align(OptoLoopAlignment);
4503     __ BIND(L_SMALL_CMP_LOOP_LAST_CMP2);
4504       if (str2_isL) { // LL
4505         __ add(str2, str2, tmp4, __ LSR, LogBitsPerByte); // address of &quot;index&quot;
4506         __ ldr(ch2, Address(str2)); // read whole register of str2. Safe.
4507         __ lslv(tmp2, tmp2, tmp4); // shift off leading zeroes from match info
4508         __ add(result, result, tmp4, __ LSR, LogBitsPerByte);
4509         __ lsl(tmp2, tmp2, 1); // shift off leading &quot;1&quot; from match info
4510       } else {
4511         __ mov(ch2, 0xE); // all bits in byte set except last one
4512         __ andr(ch2, ch2, tmp4, __ LSR, LogBitsPerByte); // byte shift amount
4513         __ ldr(ch2, Address(str2, ch2)); // read whole register of str2. Safe.
4514         __ lslv(tmp2, tmp2, tmp4);
4515         __ add(result, result, tmp4, __ LSR, LogBitsPerByte + str2_chr_shift);
4516         __ add(str2, str2, tmp4, __ LSR, LogBitsPerByte + str2_chr_shift);
4517         __ lsl(tmp2, tmp2, 1); // shift off leading &quot;1&quot; from match info
4518         __ add(str2, str2, tmp4, __ LSR, LogBitsPerByte + str2_chr_shift);
4519       }
4520       __ cmp(ch1, ch2);
4521       __ br(__ NE, L_SMALL_CMP_LOOP_NOMATCH);
4522       __ b(DONE);
4523     __ align(OptoLoopAlignment);
4524     __ BIND(L_HAS_ZERO);
4525       __ rbit(tmp2, tmp2);
4526       __ clz(tmp4, tmp2); // potentially long. Up to 4 cycles on some CPU&#39;s
4527       // Now, perform compression of counters(cnt2 and cnt1) into one register.
4528       // It&#39;s fine because both counters are 32bit and are not changed in this
4529       // loop. Just restore it on exit. So, cnt1 can be re-used in this loop.
4530       __ orr(cnt2, cnt2, cnt1, __ LSL, BitsPerByte * wordSize / 2);
4531       __ sub(result, result, 1);
4532     __ BIND(L_HAS_ZERO_LOOP);
4533       __ mov(cnt1, wordSize/str2_chr_size);
4534       __ cmp(cnt1, cnt2, __ LSR, BitsPerByte * wordSize / 2);
4535       __ br(__ GE, L_CMP_LOOP_LAST_CMP2); // case of 8 bytes only to compare
4536       if (str2_isL) {
4537         __ lsr(ch2, tmp4, LogBitsPerByte + str2_chr_shift); // char index
4538         __ ldr(ch2, Address(str2, ch2)); // read whole register of str2. Safe.
4539         __ lslv(tmp2, tmp2, tmp4);
4540         __ add(str2, str2, tmp4, __ LSR, LogBitsPerByte + str2_chr_shift);
4541         __ add(tmp4, tmp4, 1);
4542         __ add(result, result, tmp4, __ LSR, LogBitsPerByte + str2_chr_shift);
4543         __ lsl(tmp2, tmp2, 1);
4544         __ mov(tmp4, wordSize/str2_chr_size);
4545       } else {
4546         __ mov(ch2, 0xE);
4547         __ andr(ch2, ch2, tmp4, __ LSR, LogBitsPerByte); // byte shift amount
4548         __ ldr(ch2, Address(str2, ch2)); // read whole register of str2. Safe.
4549         __ lslv(tmp2, tmp2, tmp4);
4550         __ add(tmp4, tmp4, 1);
4551         __ add(result, result, tmp4, __ LSR, LogBitsPerByte + str2_chr_shift);
4552         __ add(str2, str2, tmp4, __ LSR, LogBitsPerByte);
4553         __ lsl(tmp2, tmp2, 1);
4554         __ mov(tmp4, wordSize/str2_chr_size);
4555         __ sub(str2, str2, str2_chr_size);
4556       }
4557       __ cmp(ch1, ch2);
4558       __ mov(tmp4, wordSize/str2_chr_size);
4559       __ br(__ NE, L_CMP_LOOP_NOMATCH);
4560     __ BIND(L_CMP_LOOP);
4561       str1_isL ? __ ldrb(cnt1, Address(str1, tmp4, Address::lsl(str1_chr_shift)))
4562                : __ ldrh(cnt1, Address(str1, tmp4, Address::lsl(str1_chr_shift)));
4563       str2_isL ? __ ldrb(ch2, Address(str2, tmp4, Address::lsl(str2_chr_shift)))
4564                : __ ldrh(ch2, Address(str2, tmp4, Address::lsl(str2_chr_shift)));
4565       __ add(tmp4, tmp4, 1);
4566       __ cmp(tmp4, cnt2, __ LSR, BitsPerByte * wordSize / 2);
4567       __ br(__ GE, L_CMP_LOOP_LAST_CMP);
4568       __ cmp(cnt1, ch2);
4569       __ br(__ EQ, L_CMP_LOOP);
4570     __ BIND(L_CMP_LOOP_NOMATCH);
4571       // here we&#39;re not matched
4572       __ cbz(tmp2, L_HAS_ZERO_LOOP_NOMATCH); // no more matches. Proceed to main loop
4573       __ clz(tmp4, tmp2);
4574       __ add(str2, str2, str2_chr_size); // advance pointer
4575       __ b(L_HAS_ZERO_LOOP);
4576     __ align(OptoLoopAlignment);
4577     __ BIND(L_CMP_LOOP_LAST_CMP);
4578       __ cmp(cnt1, ch2);
4579       __ br(__ NE, L_CMP_LOOP_NOMATCH);
4580       __ b(DONE);
4581     __ align(OptoLoopAlignment);
4582     __ BIND(L_CMP_LOOP_LAST_CMP2);
4583       if (str2_isL) {
4584         __ lsr(ch2, tmp4, LogBitsPerByte + str2_chr_shift); // char index
4585         __ ldr(ch2, Address(str2, ch2)); // read whole register of str2. Safe.
4586         __ lslv(tmp2, tmp2, tmp4);
4587         __ add(str2, str2, tmp4, __ LSR, LogBitsPerByte + str2_chr_shift);
4588         __ add(tmp4, tmp4, 1);
4589         __ add(result, result, tmp4, __ LSR, LogBitsPerByte + str2_chr_shift);
4590         __ lsl(tmp2, tmp2, 1);
4591       } else {
4592         __ mov(ch2, 0xE);
4593         __ andr(ch2, ch2, tmp4, __ LSR, LogBitsPerByte); // byte shift amount
4594         __ ldr(ch2, Address(str2, ch2)); // read whole register of str2. Safe.
4595         __ lslv(tmp2, tmp2, tmp4);
4596         __ add(tmp4, tmp4, 1);
4597         __ add(result, result, tmp4, __ LSR, LogBitsPerByte + str2_chr_shift);
4598         __ add(str2, str2, tmp4, __ LSR, LogBitsPerByte);
4599         __ lsl(tmp2, tmp2, 1);
4600         __ sub(str2, str2, str2_chr_size);
4601       }
4602       __ cmp(ch1, ch2);
4603       __ br(__ NE, L_CMP_LOOP_NOMATCH);
4604       __ b(DONE);
4605     __ align(OptoLoopAlignment);
4606     __ BIND(L_HAS_ZERO_LOOP_NOMATCH);
4607       // 1) Restore &quot;result&quot; index. Index was wordSize/str2_chr_size * N until
4608       // L_HAS_ZERO block. Byte octet was analyzed in L_HAS_ZERO_LOOP,
4609       // so, result was increased at max by wordSize/str2_chr_size - 1, so,
4610       // respective high bit wasn&#39;t changed. L_LOOP_PROCEED will increase
4611       // result by analyzed characters value, so, we can just reset lower bits
4612       // in result here. Clear 2 lower bits for UU/UL and 3 bits for LL
4613       // 2) restore cnt1 and cnt2 values from &quot;compressed&quot; cnt2
4614       // 3) advance str2 value to represent next str2 octet. result &amp; 7/3 is
4615       // index of last analyzed substring inside current octet. So, str2 in at
4616       // respective start address. We need to advance it to next octet
4617       __ andr(tmp2, result, wordSize/str2_chr_size - 1); // symbols analyzed
4618       __ lsr(cnt1, cnt2, BitsPerByte * wordSize / 2);
4619       __ bfm(result, zr, 0, 2 - str2_chr_shift);
4620       __ sub(str2, str2, tmp2, __ LSL, str2_chr_shift); // restore str2
4621       __ movw(cnt2, cnt2);
4622       __ b(L_LOOP_PROCEED);
4623     __ align(OptoLoopAlignment);
4624     __ BIND(NOMATCH);
4625       __ mov(result, -1);
4626     __ BIND(DONE);
4627       __ pop(spilled_regs, sp);
4628       __ ret(lr);
4629     return entry;
4630   }
4631 
4632   void generate_string_indexof_stubs() {
4633     StubRoutines::aarch64::_string_indexof_linear_ll = generate_string_indexof_linear(true, true);
4634     StubRoutines::aarch64::_string_indexof_linear_uu = generate_string_indexof_linear(false, false);
4635     StubRoutines::aarch64::_string_indexof_linear_ul = generate_string_indexof_linear(true, false);
4636   }
4637 
4638   void inflate_and_store_2_fp_registers(bool generatePrfm,
4639       FloatRegister src1, FloatRegister src2) {
4640     Register dst = r1;
4641     __ zip1(v1, __ T16B, src1, v0);
4642     __ zip2(v2, __ T16B, src1, v0);
4643     if (generatePrfm) {
4644       __ prfm(Address(dst, SoftwarePrefetchHintDistance), PSTL1STRM);
4645     }
4646     __ zip1(v3, __ T16B, src2, v0);
4647     __ zip2(v4, __ T16B, src2, v0);
4648     __ st1(v1, v2, v3, v4, __ T16B, Address(__ post(dst, 64)));
4649   }
4650 
4651   // R0 = src
4652   // R1 = dst
4653   // R2 = len
4654   // R3 = len &gt;&gt; 3
4655   // V0 = 0
4656   // v1 = loaded 8 bytes
4657   address generate_large_byte_array_inflate() {
4658     __ align(CodeEntryAlignment);
4659     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;large_byte_array_inflate&quot;);
4660     address entry = __ pc();
4661     Label LOOP, LOOP_START, LOOP_PRFM, LOOP_PRFM_START, DONE;
4662     Register src = r0, dst = r1, len = r2, octetCounter = r3;
4663     const int large_loop_threshold = MAX(64, SoftwarePrefetchHintDistance)/8 + 4;
4664 
4665     // do one more 8-byte read to have address 16-byte aligned in most cases
4666     // also use single store instruction
4667     __ ldrd(v2, __ post(src, 8));
4668     __ sub(octetCounter, octetCounter, 2);
4669     __ zip1(v1, __ T16B, v1, v0);
4670     __ zip1(v2, __ T16B, v2, v0);
4671     __ st1(v1, v2, __ T16B, __ post(dst, 32));
4672     __ ld1(v3, v4, v5, v6, __ T16B, Address(__ post(src, 64)));
4673     __ subs(rscratch1, octetCounter, large_loop_threshold);
4674     __ br(__ LE, LOOP_START);
4675     __ b(LOOP_PRFM_START);
4676     __ bind(LOOP_PRFM);
4677       __ ld1(v3, v4, v5, v6, __ T16B, Address(__ post(src, 64)));
4678     __ bind(LOOP_PRFM_START);
4679       __ prfm(Address(src, SoftwarePrefetchHintDistance));
4680       __ sub(octetCounter, octetCounter, 8);
4681       __ subs(rscratch1, octetCounter, large_loop_threshold);
4682       inflate_and_store_2_fp_registers(true, v3, v4);
4683       inflate_and_store_2_fp_registers(true, v5, v6);
4684       __ br(__ GT, LOOP_PRFM);
4685       __ cmp(octetCounter, (u1)8);
4686       __ br(__ LT, DONE);
4687     __ bind(LOOP);
4688       __ ld1(v3, v4, v5, v6, __ T16B, Address(__ post(src, 64)));
4689       __ bind(LOOP_START);
4690       __ sub(octetCounter, octetCounter, 8);
4691       __ cmp(octetCounter, (u1)8);
4692       inflate_and_store_2_fp_registers(false, v3, v4);
4693       inflate_and_store_2_fp_registers(false, v5, v6);
4694       __ br(__ GE, LOOP);
4695     __ bind(DONE);
4696       __ ret(lr);
4697     return entry;
4698   }
4699 
4700   /**
4701    *  Arguments:
4702    *
4703    *  Input:
4704    *  c_rarg0   - current state address
4705    *  c_rarg1   - H key address
4706    *  c_rarg2   - data address
4707    *  c_rarg3   - number of blocks
4708    *
4709    *  Output:
4710    *  Updated state at c_rarg0
4711    */
4712   address generate_ghash_processBlocks() {
4713     // Bafflingly, GCM uses little-endian for the byte order, but
4714     // big-endian for the bit order.  For example, the polynomial 1 is
4715     // represented as the 16-byte string 80 00 00 00 | 12 bytes of 00.
4716     //
4717     // So, we must either reverse the bytes in each word and do
4718     // everything big-endian or reverse the bits in each byte and do
4719     // it little-endian.  On AArch64 it&#39;s more idiomatic to reverse
4720     // the bits in each byte (we have an instruction, RBIT, to do
4721     // that) and keep the data in little-endian bit order throught the
4722     // calculation, bit-reversing the inputs and outputs.
4723 
4724     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;ghash_processBlocks&quot;);
4725     __ align(wordSize * 2);
4726     address p = __ pc();
4727     __ emit_int64(0x87);  // The low-order bits of the field
4728                           // polynomial (i.e. p = z^7+z^2+z+1)
4729                           // repeated in the low and high parts of a
4730                           // 128-bit vector
4731     __ emit_int64(0x87);
4732 
4733     __ align(CodeEntryAlignment);
4734     address start = __ pc();
4735 
4736     Register state   = c_rarg0;
4737     Register subkeyH = c_rarg1;
4738     Register data    = c_rarg2;
4739     Register blocks  = c_rarg3;
4740 
4741     FloatRegister vzr = v30;
4742     __ eor(vzr, __ T16B, vzr, vzr); // zero register
4743 
4744     __ ldrq(v0, Address(state));
4745     __ ldrq(v1, Address(subkeyH));
4746 
4747     __ rev64(v0, __ T16B, v0);          // Bit-reverse words in state and subkeyH
4748     __ rbit(v0, __ T16B, v0);
4749     __ rev64(v1, __ T16B, v1);
4750     __ rbit(v1, __ T16B, v1);
4751 
4752     __ ldrq(v26, p);
4753 
4754     __ ext(v16, __ T16B, v1, v1, 0x08); // long-swap subkeyH into v1
4755     __ eor(v16, __ T16B, v16, v1);      // xor subkeyH into subkeyL (Karatsuba: (A1+A0))
4756 
4757     {
4758       Label L_ghash_loop;
4759       __ bind(L_ghash_loop);
4760 
4761       __ ldrq(v2, Address(__ post(data, 0x10))); // Load the data, bit
4762                                                  // reversing each byte
4763       __ rbit(v2, __ T16B, v2);
4764       __ eor(v2, __ T16B, v0, v2);   // bit-swapped data ^ bit-swapped state
4765 
4766       // Multiply state in v2 by subkey in v1
4767       ghash_multiply(/*result_lo*/v5, /*result_hi*/v7,
4768                      /*a*/v1, /*b*/v2, /*a1_xor_a0*/v16,
4769                      /*temps*/v6, v20, v18, v21);
4770       // Reduce v7:v5 by the field polynomial
4771       ghash_reduce(v0, v5, v7, v26, vzr, v20);
4772 
4773       __ sub(blocks, blocks, 1);
4774       __ cbnz(blocks, L_ghash_loop);
4775     }
4776 
4777     // The bit-reversed result is at this point in v0
4778     __ rev64(v1, __ T16B, v0);
4779     __ rbit(v1, __ T16B, v1);
4780 
4781     __ st1(v1, __ T16B, state);
4782     __ ret(lr);
4783 
4784     return start;
4785   }
4786 
4787   // Continuation point for throwing of implicit exceptions that are
4788   // not handled in the current activation. Fabricates an exception
4789   // oop and initiates normal exception dispatching in this
4790   // frame. Since we need to preserve callee-saved values (currently
4791   // only for C2, but done for C1 as well) we need a callee-saved oop
4792   // map and therefore have to make these stubs into RuntimeStubs
4793   // rather than BufferBlobs.  If the compiler needs all registers to
4794   // be preserved between the fault point and the exception handler
4795   // then it must assume responsibility for that in
4796   // AbstractCompiler::continuation_for_implicit_null_exception or
4797   // continuation_for_implicit_division_by_zero_exception. All other
4798   // implicit exceptions (e.g., NullPointerException or
4799   // AbstractMethodError on entry) are either at call sites or
4800   // otherwise assume that stack unwinding will be initiated, so
4801   // caller saved registers were assumed volatile in the compiler.
4802 
4803 #undef __
4804 #define __ masm-&gt;
4805 
4806   address generate_throw_exception(const char* name,
4807                                    address runtime_entry,
4808                                    Register arg1 = noreg,
4809                                    Register arg2 = noreg) {
4810     // Information about frame layout at time of blocking runtime call.
4811     // Note that we only have to preserve callee-saved registers since
4812     // the compilers are responsible for supplying a continuation point
4813     // if they expect all registers to be preserved.
4814     // n.b. aarch64 asserts that frame::arg_reg_save_area_bytes == 0
4815     enum layout {
4816       rfp_off = 0,
4817       rfp_off2,
4818       return_off,
4819       return_off2,
4820       framesize // inclusive of return address
4821     };
4822 
4823     int insts_size = 512;
4824     int locs_size  = 64;
4825 
4826     CodeBuffer code(name, insts_size, locs_size);
4827     OopMapSet* oop_maps  = new OopMapSet();
4828     MacroAssembler* masm = new MacroAssembler(&amp;code);
4829 
4830     address start = __ pc();
4831 
4832     // This is an inlined and slightly modified version of call_VM
4833     // which has the ability to fetch the return PC out of
4834     // thread-local storage and also sets up last_Java_sp slightly
4835     // differently than the real call_VM
4836 
4837     __ enter(); // Save FP and LR before call
4838 
4839     assert(is_even(framesize/2), &quot;sp not 16-byte aligned&quot;);
4840 
4841     // lr and fp are already in place
4842     __ sub(sp, rfp, ((unsigned)framesize-4) &lt;&lt; LogBytesPerInt); // prolog
4843 
4844     int frame_complete = __ pc() - start;
4845 
4846     // Set up last_Java_sp and last_Java_fp
4847     address the_pc = __ pc();
4848     __ set_last_Java_frame(sp, rfp, the_pc, rscratch1);
4849 
4850     // Call runtime
4851     if (arg1 != noreg) {
4852       assert(arg2 != c_rarg1, &quot;clobbered&quot;);
4853       __ mov(c_rarg1, arg1);
4854     }
4855     if (arg2 != noreg) {
4856       __ mov(c_rarg2, arg2);
4857     }
4858     __ mov(c_rarg0, rthread);
4859     BLOCK_COMMENT(&quot;call runtime_entry&quot;);
4860     __ mov(rscratch1, runtime_entry);
4861     __ blr(rscratch1);
4862 
4863     // Generate oop map
4864     OopMap* map = new OopMap(framesize, 0);
4865 
4866     oop_maps-&gt;add_gc_map(the_pc - start, map);
4867 
4868     __ reset_last_Java_frame(true);
4869     __ maybe_isb();
4870 
4871     __ leave();
4872 
4873     // check for pending exceptions
4874 #ifdef ASSERT
4875     Label L;
4876     __ ldr(rscratch1, Address(rthread, Thread::pending_exception_offset()));
4877     __ cbnz(rscratch1, L);
4878     __ should_not_reach_here();
4879     __ bind(L);
4880 #endif // ASSERT
4881     __ far_jump(RuntimeAddress(StubRoutines::forward_exception_entry()));
4882 
4883 
4884     // codeBlob framesize is in words (not VMRegImpl::slot_size)
4885     RuntimeStub* stub =
4886       RuntimeStub::new_runtime_stub(name,
4887                                     &amp;code,
4888                                     frame_complete,
4889                                     (framesize &gt;&gt; (LogBytesPerWord - LogBytesPerInt)),
4890                                     oop_maps, false);
4891     return stub-&gt;entry_point();
4892   }
4893 
4894   class MontgomeryMultiplyGenerator : public MacroAssembler {
4895 
4896     Register Pa_base, Pb_base, Pn_base, Pm_base, inv, Rlen, Ra, Rb, Rm, Rn,
4897       Pa, Pb, Pn, Pm, Rhi_ab, Rlo_ab, Rhi_mn, Rlo_mn, t0, t1, t2, Ri, Rj;
4898 
4899     RegSet _toSave;
4900     bool _squaring;
4901 
4902   public:
4903     MontgomeryMultiplyGenerator (Assembler *as, bool squaring)
4904       : MacroAssembler(as-&gt;code()), _squaring(squaring) {
4905 
4906       // Register allocation
4907 
4908       Register reg = c_rarg0;
4909       Pa_base = reg;       // Argument registers
4910       if (squaring)
4911         Pb_base = Pa_base;
4912       else
4913         Pb_base = ++reg;
4914       Pn_base = ++reg;
4915       Rlen= ++reg;
4916       inv = ++reg;
4917       Pm_base = ++reg;
4918 
4919                           // Working registers:
4920       Ra =  ++reg;        // The current digit of a, b, n, and m.
4921       Rb =  ++reg;
4922       Rm =  ++reg;
4923       Rn =  ++reg;
4924 
4925       Pa =  ++reg;        // Pointers to the current/next digit of a, b, n, and m.
4926       Pb =  ++reg;
4927       Pm =  ++reg;
4928       Pn =  ++reg;
4929 
4930       t0 =  ++reg;        // Three registers which form a
4931       t1 =  ++reg;        // triple-precision accumuator.
4932       t2 =  ++reg;
4933 
4934       Ri =  ++reg;        // Inner and outer loop indexes.
4935       Rj =  ++reg;
4936 
4937       Rhi_ab = ++reg;     // Product registers: low and high parts
4938       Rlo_ab = ++reg;     // of a*b and m*n.
4939       Rhi_mn = ++reg;
4940       Rlo_mn = ++reg;
4941 
4942       // r19 and up are callee-saved.
4943       _toSave = RegSet::range(r19, reg) + Pm_base;
4944     }
4945 
4946   private:
4947     void save_regs() {
4948       push(_toSave, sp);
4949     }
4950 
4951     void restore_regs() {
4952       pop(_toSave, sp);
4953     }
4954 
4955     template &lt;typename T&gt;
4956     void unroll_2(Register count, T block) {
4957       Label loop, end, odd;
4958       tbnz(count, 0, odd);
4959       cbz(count, end);
4960       align(16);
4961       bind(loop);
4962       (this-&gt;*block)();
4963       bind(odd);
4964       (this-&gt;*block)();
4965       subs(count, count, 2);
4966       br(Assembler::GT, loop);
4967       bind(end);
4968     }
4969 
4970     template &lt;typename T&gt;
4971     void unroll_2(Register count, T block, Register d, Register s, Register tmp) {
4972       Label loop, end, odd;
4973       tbnz(count, 0, odd);
4974       cbz(count, end);
4975       align(16);
4976       bind(loop);
4977       (this-&gt;*block)(d, s, tmp);
4978       bind(odd);
4979       (this-&gt;*block)(d, s, tmp);
4980       subs(count, count, 2);
4981       br(Assembler::GT, loop);
4982       bind(end);
4983     }
4984 
4985     void pre1(RegisterOrConstant i) {
4986       block_comment(&quot;pre1&quot;);
4987       // Pa = Pa_base;
4988       // Pb = Pb_base + i;
4989       // Pm = Pm_base;
4990       // Pn = Pn_base + i;
4991       // Ra = *Pa;
4992       // Rb = *Pb;
4993       // Rm = *Pm;
4994       // Rn = *Pn;
4995       ldr(Ra, Address(Pa_base));
4996       ldr(Rb, Address(Pb_base, i, Address::uxtw(LogBytesPerWord)));
4997       ldr(Rm, Address(Pm_base));
4998       ldr(Rn, Address(Pn_base, i, Address::uxtw(LogBytesPerWord)));
4999       lea(Pa, Address(Pa_base));
5000       lea(Pb, Address(Pb_base, i, Address::uxtw(LogBytesPerWord)));
5001       lea(Pm, Address(Pm_base));
5002       lea(Pn, Address(Pn_base, i, Address::uxtw(LogBytesPerWord)));
5003 
5004       // Zero the m*n result.
5005       mov(Rhi_mn, zr);
5006       mov(Rlo_mn, zr);
5007     }
5008 
5009     // The core multiply-accumulate step of a Montgomery
5010     // multiplication.  The idea is to schedule operations as a
5011     // pipeline so that instructions with long latencies (loads and
5012     // multiplies) have time to complete before their results are
5013     // used.  This most benefits in-order implementations of the
5014     // architecture but out-of-order ones also benefit.
5015     void step() {
5016       block_comment(&quot;step&quot;);
5017       // MACC(Ra, Rb, t0, t1, t2);
5018       // Ra = *++Pa;
5019       // Rb = *--Pb;
5020       umulh(Rhi_ab, Ra, Rb);
5021       mul(Rlo_ab, Ra, Rb);
5022       ldr(Ra, pre(Pa, wordSize));
5023       ldr(Rb, pre(Pb, -wordSize));
5024       acc(Rhi_mn, Rlo_mn, t0, t1, t2); // The pending m*n from the
5025                                        // previous iteration.
5026       // MACC(Rm, Rn, t0, t1, t2);
5027       // Rm = *++Pm;
5028       // Rn = *--Pn;
5029       umulh(Rhi_mn, Rm, Rn);
5030       mul(Rlo_mn, Rm, Rn);
5031       ldr(Rm, pre(Pm, wordSize));
5032       ldr(Rn, pre(Pn, -wordSize));
5033       acc(Rhi_ab, Rlo_ab, t0, t1, t2);
5034     }
5035 
5036     void post1() {
5037       block_comment(&quot;post1&quot;);
5038 
5039       // MACC(Ra, Rb, t0, t1, t2);
5040       // Ra = *++Pa;
5041       // Rb = *--Pb;
5042       umulh(Rhi_ab, Ra, Rb);
5043       mul(Rlo_ab, Ra, Rb);
5044       acc(Rhi_mn, Rlo_mn, t0, t1, t2);  // The pending m*n
5045       acc(Rhi_ab, Rlo_ab, t0, t1, t2);
5046 
5047       // *Pm = Rm = t0 * inv;
5048       mul(Rm, t0, inv);
5049       str(Rm, Address(Pm));
5050 
5051       // MACC(Rm, Rn, t0, t1, t2);
5052       // t0 = t1; t1 = t2; t2 = 0;
5053       umulh(Rhi_mn, Rm, Rn);
5054 
5055 #ifndef PRODUCT
5056       // assert(m[i] * n[0] + t0 == 0, &quot;broken Montgomery multiply&quot;);
5057       {
5058         mul(Rlo_mn, Rm, Rn);
5059         add(Rlo_mn, t0, Rlo_mn);
5060         Label ok;
5061         cbz(Rlo_mn, ok); {
5062           stop(&quot;broken Montgomery multiply&quot;);
5063         } bind(ok);
5064       }
5065 #endif
5066       // We have very carefully set things up so that
5067       // m[i]*n[0] + t0 == 0 (mod b), so we don&#39;t have to calculate
5068       // the lower half of Rm * Rn because we know the result already:
5069       // it must be -t0.  t0 + (-t0) must generate a carry iff
5070       // t0 != 0.  So, rather than do a mul and an adds we just set
5071       // the carry flag iff t0 is nonzero.
5072       //
5073       // mul(Rlo_mn, Rm, Rn);
5074       // adds(zr, t0, Rlo_mn);
5075       subs(zr, t0, 1); // Set carry iff t0 is nonzero
5076       adcs(t0, t1, Rhi_mn);
5077       adc(t1, t2, zr);
5078       mov(t2, zr);
5079     }
5080 
5081     void pre2(RegisterOrConstant i, RegisterOrConstant len) {
5082       block_comment(&quot;pre2&quot;);
5083       // Pa = Pa_base + i-len;
5084       // Pb = Pb_base + len;
5085       // Pm = Pm_base + i-len;
5086       // Pn = Pn_base + len;
5087 
5088       if (i.is_register()) {
5089         sub(Rj, i.as_register(), len);
5090       } else {
5091         mov(Rj, i.as_constant());
5092         sub(Rj, Rj, len);
5093       }
5094       // Rj == i-len
5095 
5096       lea(Pa, Address(Pa_base, Rj, Address::uxtw(LogBytesPerWord)));
5097       lea(Pb, Address(Pb_base, len, Address::uxtw(LogBytesPerWord)));
5098       lea(Pm, Address(Pm_base, Rj, Address::uxtw(LogBytesPerWord)));
5099       lea(Pn, Address(Pn_base, len, Address::uxtw(LogBytesPerWord)));
5100 
5101       // Ra = *++Pa;
5102       // Rb = *--Pb;
5103       // Rm = *++Pm;
5104       // Rn = *--Pn;
5105       ldr(Ra, pre(Pa, wordSize));
5106       ldr(Rb, pre(Pb, -wordSize));
5107       ldr(Rm, pre(Pm, wordSize));
5108       ldr(Rn, pre(Pn, -wordSize));
5109 
5110       mov(Rhi_mn, zr);
5111       mov(Rlo_mn, zr);
5112     }
5113 
5114     void post2(RegisterOrConstant i, RegisterOrConstant len) {
5115       block_comment(&quot;post2&quot;);
5116       if (i.is_constant()) {
5117         mov(Rj, i.as_constant()-len.as_constant());
5118       } else {
5119         sub(Rj, i.as_register(), len);
5120       }
5121 
5122       adds(t0, t0, Rlo_mn); // The pending m*n, low part
5123 
5124       // As soon as we know the least significant digit of our result,
5125       // store it.
5126       // Pm_base[i-len] = t0;
5127       str(t0, Address(Pm_base, Rj, Address::uxtw(LogBytesPerWord)));
5128 
5129       // t0 = t1; t1 = t2; t2 = 0;
5130       adcs(t0, t1, Rhi_mn); // The pending m*n, high part
5131       adc(t1, t2, zr);
5132       mov(t2, zr);
5133     }
5134 
5135     // A carry in t0 after Montgomery multiplication means that we
5136     // should subtract multiples of n from our result in m.  We&#39;ll
5137     // keep doing that until there is no carry.
5138     void normalize(RegisterOrConstant len) {
5139       block_comment(&quot;normalize&quot;);
5140       // while (t0)
5141       //   t0 = sub(Pm_base, Pn_base, t0, len);
5142       Label loop, post, again;
5143       Register cnt = t1, i = t2; // Re-use registers; we&#39;re done with them now
5144       cbz(t0, post); {
5145         bind(again); {
5146           mov(i, zr);
5147           mov(cnt, len);
5148           ldr(Rm, Address(Pm_base, i, Address::uxtw(LogBytesPerWord)));
5149           ldr(Rn, Address(Pn_base, i, Address::uxtw(LogBytesPerWord)));
5150           subs(zr, zr, zr); // set carry flag, i.e. no borrow
5151           align(16);
5152           bind(loop); {
5153             sbcs(Rm, Rm, Rn);
5154             str(Rm, Address(Pm_base, i, Address::uxtw(LogBytesPerWord)));
5155             add(i, i, 1);
5156             ldr(Rm, Address(Pm_base, i, Address::uxtw(LogBytesPerWord)));
5157             ldr(Rn, Address(Pn_base, i, Address::uxtw(LogBytesPerWord)));
5158             sub(cnt, cnt, 1);
5159           } cbnz(cnt, loop);
5160           sbc(t0, t0, zr);
5161         } cbnz(t0, again);
5162       } bind(post);
5163     }
5164 
5165     // Move memory at s to d, reversing words.
5166     //    Increments d to end of copied memory
5167     //    Destroys tmp1, tmp2
5168     //    Preserves len
5169     //    Leaves s pointing to the address which was in d at start
5170     void reverse(Register d, Register s, Register len, Register tmp1, Register tmp2) {
5171       assert(tmp1 &lt; r19 &amp;&amp; tmp2 &lt; r19, &quot;register corruption&quot;);
5172 
5173       lea(s, Address(s, len, Address::uxtw(LogBytesPerWord)));
5174       mov(tmp1, len);
5175       unroll_2(tmp1, &amp;MontgomeryMultiplyGenerator::reverse1, d, s, tmp2);
5176       sub(s, d, len, ext::uxtw, LogBytesPerWord);
5177     }
5178     // where
5179     void reverse1(Register d, Register s, Register tmp) {
5180       ldr(tmp, pre(s, -wordSize));
5181       ror(tmp, tmp, 32);
5182       str(tmp, post(d, wordSize));
5183     }
5184 
5185     void step_squaring() {
5186       // An extra ACC
5187       step();
5188       acc(Rhi_ab, Rlo_ab, t0, t1, t2);
5189     }
5190 
5191     void last_squaring(RegisterOrConstant i) {
5192       Label dont;
5193       // if ((i &amp; 1) == 0) {
5194       tbnz(i.as_register(), 0, dont); {
5195         // MACC(Ra, Rb, t0, t1, t2);
5196         // Ra = *++Pa;
5197         // Rb = *--Pb;
5198         umulh(Rhi_ab, Ra, Rb);
5199         mul(Rlo_ab, Ra, Rb);
5200         acc(Rhi_ab, Rlo_ab, t0, t1, t2);
5201       } bind(dont);
5202     }
5203 
5204     void extra_step_squaring() {
5205       acc(Rhi_mn, Rlo_mn, t0, t1, t2);  // The pending m*n
5206 
5207       // MACC(Rm, Rn, t0, t1, t2);
5208       // Rm = *++Pm;
5209       // Rn = *--Pn;
5210       umulh(Rhi_mn, Rm, Rn);
5211       mul(Rlo_mn, Rm, Rn);
5212       ldr(Rm, pre(Pm, wordSize));
5213       ldr(Rn, pre(Pn, -wordSize));
5214     }
5215 
5216     void post1_squaring() {
5217       acc(Rhi_mn, Rlo_mn, t0, t1, t2);  // The pending m*n
5218 
5219       // *Pm = Rm = t0 * inv;
5220       mul(Rm, t0, inv);
5221       str(Rm, Address(Pm));
5222 
5223       // MACC(Rm, Rn, t0, t1, t2);
5224       // t0 = t1; t1 = t2; t2 = 0;
5225       umulh(Rhi_mn, Rm, Rn);
5226 
5227 #ifndef PRODUCT
5228       // assert(m[i] * n[0] + t0 == 0, &quot;broken Montgomery multiply&quot;);
5229       {
5230         mul(Rlo_mn, Rm, Rn);
5231         add(Rlo_mn, t0, Rlo_mn);
5232         Label ok;
5233         cbz(Rlo_mn, ok); {
5234           stop(&quot;broken Montgomery multiply&quot;);
5235         } bind(ok);
5236       }
5237 #endif
5238       // We have very carefully set things up so that
5239       // m[i]*n[0] + t0 == 0 (mod b), so we don&#39;t have to calculate
5240       // the lower half of Rm * Rn because we know the result already:
5241       // it must be -t0.  t0 + (-t0) must generate a carry iff
5242       // t0 != 0.  So, rather than do a mul and an adds we just set
5243       // the carry flag iff t0 is nonzero.
5244       //
5245       // mul(Rlo_mn, Rm, Rn);
5246       // adds(zr, t0, Rlo_mn);
5247       subs(zr, t0, 1); // Set carry iff t0 is nonzero
5248       adcs(t0, t1, Rhi_mn);
5249       adc(t1, t2, zr);
5250       mov(t2, zr);
5251     }
5252 
5253     void acc(Register Rhi, Register Rlo,
5254              Register t0, Register t1, Register t2) {
5255       adds(t0, t0, Rlo);
5256       adcs(t1, t1, Rhi);
5257       adc(t2, t2, zr);
5258     }
5259 
5260   public:
5261     /**
5262      * Fast Montgomery multiplication.  The derivation of the
5263      * algorithm is in A Cryptographic Library for the Motorola
5264      * DSP56000, Dusse and Kaliski, Proc. EUROCRYPT 90, pp. 230-237.
5265      *
5266      * Arguments:
5267      *
5268      * Inputs for multiplication:
5269      *   c_rarg0   - int array elements a
5270      *   c_rarg1   - int array elements b
5271      *   c_rarg2   - int array elements n (the modulus)
5272      *   c_rarg3   - int length
5273      *   c_rarg4   - int inv
5274      *   c_rarg5   - int array elements m (the result)
5275      *
5276      * Inputs for squaring:
5277      *   c_rarg0   - int array elements a
5278      *   c_rarg1   - int array elements n (the modulus)
5279      *   c_rarg2   - int length
5280      *   c_rarg3   - int inv
5281      *   c_rarg4   - int array elements m (the result)
5282      *
5283      */
5284     address generate_multiply() {
5285       Label argh, nothing;
5286       bind(argh);
5287       stop(&quot;MontgomeryMultiply total_allocation must be &lt;= 8192&quot;);
5288 
5289       align(CodeEntryAlignment);
5290       address entry = pc();
5291 
5292       cbzw(Rlen, nothing);
5293 
5294       enter();
5295 
5296       // Make room.
5297       cmpw(Rlen, 512);
5298       br(Assembler::HI, argh);
5299       sub(Ra, sp, Rlen, ext::uxtw, exact_log2(4 * sizeof (jint)));
5300       andr(sp, Ra, -2 * wordSize);
5301 
5302       lsrw(Rlen, Rlen, 1);  // length in longwords = len/2
5303 
5304       {
5305         // Copy input args, reversing as we go.  We use Ra as a
5306         // temporary variable.
5307         reverse(Ra, Pa_base, Rlen, t0, t1);
5308         if (!_squaring)
5309           reverse(Ra, Pb_base, Rlen, t0, t1);
5310         reverse(Ra, Pn_base, Rlen, t0, t1);
5311       }
5312 
5313       // Push all call-saved registers and also Pm_base which we&#39;ll need
5314       // at the end.
5315       save_regs();
5316 
5317 #ifndef PRODUCT
5318       // assert(inv * n[0] == -1UL, &quot;broken inverse in Montgomery multiply&quot;);
5319       {
5320         ldr(Rn, Address(Pn_base, 0));
5321         mul(Rlo_mn, Rn, inv);
5322         subs(zr, Rlo_mn, -1);
5323         Label ok;
5324         br(EQ, ok); {
5325           stop(&quot;broken inverse in Montgomery multiply&quot;);
5326         } bind(ok);
5327       }
5328 #endif
5329 
5330       mov(Pm_base, Ra);
5331 
5332       mov(t0, zr);
5333       mov(t1, zr);
5334       mov(t2, zr);
5335 
5336       block_comment(&quot;for (int i = 0; i &lt; len; i++) {&quot;);
5337       mov(Ri, zr); {
5338         Label loop, end;
5339         cmpw(Ri, Rlen);
5340         br(Assembler::GE, end);
5341 
5342         bind(loop);
5343         pre1(Ri);
5344 
5345         block_comment(&quot;  for (j = i; j; j--) {&quot;); {
5346           movw(Rj, Ri);
5347           unroll_2(Rj, &amp;MontgomeryMultiplyGenerator::step);
5348         } block_comment(&quot;  } // j&quot;);
5349 
5350         post1();
5351         addw(Ri, Ri, 1);
5352         cmpw(Ri, Rlen);
5353         br(Assembler::LT, loop);
5354         bind(end);
5355         block_comment(&quot;} // i&quot;);
5356       }
5357 
5358       block_comment(&quot;for (int i = len; i &lt; 2*len; i++) {&quot;);
5359       mov(Ri, Rlen); {
5360         Label loop, end;
5361         cmpw(Ri, Rlen, Assembler::LSL, 1);
5362         br(Assembler::GE, end);
5363 
5364         bind(loop);
5365         pre2(Ri, Rlen);
5366 
5367         block_comment(&quot;  for (j = len*2-i-1; j; j--) {&quot;); {
5368           lslw(Rj, Rlen, 1);
5369           subw(Rj, Rj, Ri);
5370           subw(Rj, Rj, 1);
5371           unroll_2(Rj, &amp;MontgomeryMultiplyGenerator::step);
5372         } block_comment(&quot;  } // j&quot;);
5373 
5374         post2(Ri, Rlen);
5375         addw(Ri, Ri, 1);
5376         cmpw(Ri, Rlen, Assembler::LSL, 1);
5377         br(Assembler::LT, loop);
5378         bind(end);
5379       }
5380       block_comment(&quot;} // i&quot;);
5381 
5382       normalize(Rlen);
5383 
5384       mov(Ra, Pm_base);  // Save Pm_base in Ra
5385       restore_regs();  // Restore caller&#39;s Pm_base
5386 
5387       // Copy our result into caller&#39;s Pm_base
5388       reverse(Pm_base, Ra, Rlen, t0, t1);
5389 
5390       leave();
5391       bind(nothing);
5392       ret(lr);
5393 
5394       return entry;
5395     }
5396     // In C, approximately:
5397 
5398     // void
5399     // montgomery_multiply(unsigned long Pa_base[], unsigned long Pb_base[],
5400     //                     unsigned long Pn_base[], unsigned long Pm_base[],
5401     //                     unsigned long inv, int len) {
5402     //   unsigned long t0 = 0, t1 = 0, t2 = 0; // Triple-precision accumulator
5403     //   unsigned long *Pa, *Pb, *Pn, *Pm;
5404     //   unsigned long Ra, Rb, Rn, Rm;
5405 
5406     //   int i;
5407 
5408     //   assert(inv * Pn_base[0] == -1UL, &quot;broken inverse in Montgomery multiply&quot;);
5409 
5410     //   for (i = 0; i &lt; len; i++) {
5411     //     int j;
5412 
5413     //     Pa = Pa_base;
5414     //     Pb = Pb_base + i;
5415     //     Pm = Pm_base;
5416     //     Pn = Pn_base + i;
5417 
5418     //     Ra = *Pa;
5419     //     Rb = *Pb;
5420     //     Rm = *Pm;
5421     //     Rn = *Pn;
5422 
5423     //     int iters = i;
5424     //     for (j = 0; iters--; j++) {
5425     //       assert(Ra == Pa_base[j] &amp;&amp; Rb == Pb_base[i-j], &quot;must be&quot;);
5426     //       MACC(Ra, Rb, t0, t1, t2);
5427     //       Ra = *++Pa;
5428     //       Rb = *--Pb;
5429     //       assert(Rm == Pm_base[j] &amp;&amp; Rn == Pn_base[i-j], &quot;must be&quot;);
5430     //       MACC(Rm, Rn, t0, t1, t2);
5431     //       Rm = *++Pm;
5432     //       Rn = *--Pn;
5433     //     }
5434 
5435     //     assert(Ra == Pa_base[i] &amp;&amp; Rb == Pb_base[0], &quot;must be&quot;);
5436     //     MACC(Ra, Rb, t0, t1, t2);
5437     //     *Pm = Rm = t0 * inv;
5438     //     assert(Rm == Pm_base[i] &amp;&amp; Rn == Pn_base[0], &quot;must be&quot;);
5439     //     MACC(Rm, Rn, t0, t1, t2);
5440 
5441     //     assert(t0 == 0, &quot;broken Montgomery multiply&quot;);
5442 
5443     //     t0 = t1; t1 = t2; t2 = 0;
5444     //   }
5445 
5446     //   for (i = len; i &lt; 2*len; i++) {
5447     //     int j;
5448 
5449     //     Pa = Pa_base + i-len;
5450     //     Pb = Pb_base + len;
5451     //     Pm = Pm_base + i-len;
5452     //     Pn = Pn_base + len;
5453 
5454     //     Ra = *++Pa;
5455     //     Rb = *--Pb;
5456     //     Rm = *++Pm;
5457     //     Rn = *--Pn;
5458 
5459     //     int iters = len*2-i-1;
5460     //     for (j = i-len+1; iters--; j++) {
5461     //       assert(Ra == Pa_base[j] &amp;&amp; Rb == Pb_base[i-j], &quot;must be&quot;);
5462     //       MACC(Ra, Rb, t0, t1, t2);
5463     //       Ra = *++Pa;
5464     //       Rb = *--Pb;
5465     //       assert(Rm == Pm_base[j] &amp;&amp; Rn == Pn_base[i-j], &quot;must be&quot;);
5466     //       MACC(Rm, Rn, t0, t1, t2);
5467     //       Rm = *++Pm;
5468     //       Rn = *--Pn;
5469     //     }
5470 
5471     //     Pm_base[i-len] = t0;
5472     //     t0 = t1; t1 = t2; t2 = 0;
5473     //   }
5474 
5475     //   while (t0)
5476     //     t0 = sub(Pm_base, Pn_base, t0, len);
5477     // }
5478 
5479     /**
5480      * Fast Montgomery squaring.  This uses asymptotically 25% fewer
5481      * multiplies than Montgomery multiplication so it should be up to
5482      * 25% faster.  However, its loop control is more complex and it
5483      * may actually run slower on some machines.
5484      *
5485      * Arguments:
5486      *
5487      * Inputs:
5488      *   c_rarg0   - int array elements a
5489      *   c_rarg1   - int array elements n (the modulus)
5490      *   c_rarg2   - int length
5491      *   c_rarg3   - int inv
5492      *   c_rarg4   - int array elements m (the result)
5493      *
5494      */
5495     address generate_square() {
5496       Label argh;
5497       bind(argh);
5498       stop(&quot;MontgomeryMultiply total_allocation must be &lt;= 8192&quot;);
5499 
5500       align(CodeEntryAlignment);
5501       address entry = pc();
5502 
5503       enter();
5504 
5505       // Make room.
5506       cmpw(Rlen, 512);
5507       br(Assembler::HI, argh);
5508       sub(Ra, sp, Rlen, ext::uxtw, exact_log2(4 * sizeof (jint)));
5509       andr(sp, Ra, -2 * wordSize);
5510 
5511       lsrw(Rlen, Rlen, 1);  // length in longwords = len/2
5512 
5513       {
5514         // Copy input args, reversing as we go.  We use Ra as a
5515         // temporary variable.
5516         reverse(Ra, Pa_base, Rlen, t0, t1);
5517         reverse(Ra, Pn_base, Rlen, t0, t1);
5518       }
5519 
5520       // Push all call-saved registers and also Pm_base which we&#39;ll need
5521       // at the end.
5522       save_regs();
5523 
5524       mov(Pm_base, Ra);
5525 
5526       mov(t0, zr);
5527       mov(t1, zr);
5528       mov(t2, zr);
5529 
5530       block_comment(&quot;for (int i = 0; i &lt; len; i++) {&quot;);
5531       mov(Ri, zr); {
5532         Label loop, end;
5533         bind(loop);
5534         cmp(Ri, Rlen);
5535         br(Assembler::GE, end);
5536 
5537         pre1(Ri);
5538 
5539         block_comment(&quot;for (j = (i+1)/2; j; j--) {&quot;); {
5540           add(Rj, Ri, 1);
5541           lsr(Rj, Rj, 1);
5542           unroll_2(Rj, &amp;MontgomeryMultiplyGenerator::step_squaring);
5543         } block_comment(&quot;  } // j&quot;);
5544 
5545         last_squaring(Ri);
5546 
5547         block_comment(&quot;  for (j = i/2; j; j--) {&quot;); {
5548           lsr(Rj, Ri, 1);
5549           unroll_2(Rj, &amp;MontgomeryMultiplyGenerator::extra_step_squaring);
5550         } block_comment(&quot;  } // j&quot;);
5551 
5552         post1_squaring();
5553         add(Ri, Ri, 1);
5554         cmp(Ri, Rlen);
5555         br(Assembler::LT, loop);
5556 
5557         bind(end);
5558         block_comment(&quot;} // i&quot;);
5559       }
5560 
5561       block_comment(&quot;for (int i = len; i &lt; 2*len; i++) {&quot;);
5562       mov(Ri, Rlen); {
5563         Label loop, end;
5564         bind(loop);
5565         cmp(Ri, Rlen, Assembler::LSL, 1);
5566         br(Assembler::GE, end);
5567 
5568         pre2(Ri, Rlen);
5569 
5570         block_comment(&quot;  for (j = (2*len-i-1)/2; j; j--) {&quot;); {
5571           lsl(Rj, Rlen, 1);
5572           sub(Rj, Rj, Ri);
5573           sub(Rj, Rj, 1);
5574           lsr(Rj, Rj, 1);
5575           unroll_2(Rj, &amp;MontgomeryMultiplyGenerator::step_squaring);
5576         } block_comment(&quot;  } // j&quot;);
5577 
5578         last_squaring(Ri);
5579 
5580         block_comment(&quot;  for (j = (2*len-i)/2; j; j--) {&quot;); {
5581           lsl(Rj, Rlen, 1);
5582           sub(Rj, Rj, Ri);
5583           lsr(Rj, Rj, 1);
5584           unroll_2(Rj, &amp;MontgomeryMultiplyGenerator::extra_step_squaring);
5585         } block_comment(&quot;  } // j&quot;);
5586 
5587         post2(Ri, Rlen);
5588         add(Ri, Ri, 1);
5589         cmp(Ri, Rlen, Assembler::LSL, 1);
5590 
5591         br(Assembler::LT, loop);
5592         bind(end);
5593         block_comment(&quot;} // i&quot;);
5594       }
5595 
5596       normalize(Rlen);
5597 
5598       mov(Ra, Pm_base);  // Save Pm_base in Ra
5599       restore_regs();  // Restore caller&#39;s Pm_base
5600 
5601       // Copy our result into caller&#39;s Pm_base
5602       reverse(Pm_base, Ra, Rlen, t0, t1);
5603 
5604       leave();
5605       ret(lr);
5606 
5607       return entry;
5608     }
5609     // In C, approximately:
5610 
5611     // void
5612     // montgomery_square(unsigned long Pa_base[], unsigned long Pn_base[],
5613     //                   unsigned long Pm_base[], unsigned long inv, int len) {
5614     //   unsigned long t0 = 0, t1 = 0, t2 = 0; // Triple-precision accumulator
5615     //   unsigned long *Pa, *Pb, *Pn, *Pm;
5616     //   unsigned long Ra, Rb, Rn, Rm;
5617 
5618     //   int i;
5619 
5620     //   assert(inv * Pn_base[0] == -1UL, &quot;broken inverse in Montgomery multiply&quot;);
5621 
5622     //   for (i = 0; i &lt; len; i++) {
5623     //     int j;
5624 
5625     //     Pa = Pa_base;
5626     //     Pb = Pa_base + i;
5627     //     Pm = Pm_base;
5628     //     Pn = Pn_base + i;
5629 
5630     //     Ra = *Pa;
5631     //     Rb = *Pb;
5632     //     Rm = *Pm;
5633     //     Rn = *Pn;
5634 
5635     //     int iters = (i+1)/2;
5636     //     for (j = 0; iters--; j++) {
5637     //       assert(Ra == Pa_base[j] &amp;&amp; Rb == Pa_base[i-j], &quot;must be&quot;);
5638     //       MACC2(Ra, Rb, t0, t1, t2);
5639     //       Ra = *++Pa;
5640     //       Rb = *--Pb;
5641     //       assert(Rm == Pm_base[j] &amp;&amp; Rn == Pn_base[i-j], &quot;must be&quot;);
5642     //       MACC(Rm, Rn, t0, t1, t2);
5643     //       Rm = *++Pm;
5644     //       Rn = *--Pn;
5645     //     }
5646     //     if ((i &amp; 1) == 0) {
5647     //       assert(Ra == Pa_base[j], &quot;must be&quot;);
5648     //       MACC(Ra, Ra, t0, t1, t2);
5649     //     }
5650     //     iters = i/2;
5651     //     assert(iters == i-j, &quot;must be&quot;);
5652     //     for (; iters--; j++) {
5653     //       assert(Rm == Pm_base[j] &amp;&amp; Rn == Pn_base[i-j], &quot;must be&quot;);
5654     //       MACC(Rm, Rn, t0, t1, t2);
5655     //       Rm = *++Pm;
5656     //       Rn = *--Pn;
5657     //     }
5658 
5659     //     *Pm = Rm = t0 * inv;
5660     //     assert(Rm == Pm_base[i] &amp;&amp; Rn == Pn_base[0], &quot;must be&quot;);
5661     //     MACC(Rm, Rn, t0, t1, t2);
5662 
5663     //     assert(t0 == 0, &quot;broken Montgomery multiply&quot;);
5664 
5665     //     t0 = t1; t1 = t2; t2 = 0;
5666     //   }
5667 
5668     //   for (i = len; i &lt; 2*len; i++) {
5669     //     int start = i-len+1;
5670     //     int end = start + (len - start)/2;
5671     //     int j;
5672 
5673     //     Pa = Pa_base + i-len;
5674     //     Pb = Pa_base + len;
5675     //     Pm = Pm_base + i-len;
5676     //     Pn = Pn_base + len;
5677 
5678     //     Ra = *++Pa;
5679     //     Rb = *--Pb;
5680     //     Rm = *++Pm;
5681     //     Rn = *--Pn;
5682 
5683     //     int iters = (2*len-i-1)/2;
5684     //     assert(iters == end-start, &quot;must be&quot;);
5685     //     for (j = start; iters--; j++) {
5686     //       assert(Ra == Pa_base[j] &amp;&amp; Rb == Pa_base[i-j], &quot;must be&quot;);
5687     //       MACC2(Ra, Rb, t0, t1, t2);
5688     //       Ra = *++Pa;
5689     //       Rb = *--Pb;
5690     //       assert(Rm == Pm_base[j] &amp;&amp; Rn == Pn_base[i-j], &quot;must be&quot;);
5691     //       MACC(Rm, Rn, t0, t1, t2);
5692     //       Rm = *++Pm;
5693     //       Rn = *--Pn;
5694     //     }
5695     //     if ((i &amp; 1) == 0) {
5696     //       assert(Ra == Pa_base[j], &quot;must be&quot;);
5697     //       MACC(Ra, Ra, t0, t1, t2);
5698     //     }
5699     //     iters =  (2*len-i)/2;
5700     //     assert(iters == len-j, &quot;must be&quot;);
5701     //     for (; iters--; j++) {
5702     //       assert(Rm == Pm_base[j] &amp;&amp; Rn == Pn_base[i-j], &quot;must be&quot;);
5703     //       MACC(Rm, Rn, t0, t1, t2);
5704     //       Rm = *++Pm;
5705     //       Rn = *--Pn;
5706     //     }
5707     //     Pm_base[i-len] = t0;
5708     //     t0 = t1; t1 = t2; t2 = 0;
5709     //   }
5710 
5711     //   while (t0)
5712     //     t0 = sub(Pm_base, Pn_base, t0, len);
5713     // }
5714   };
5715 
5716 
5717   // Call here from the interpreter or compiled code to either load
5718   // multiple returned values from the value type instance being
5719   // returned to registers or to store returned values to a newly
5720   // allocated value type instance.
5721   address generate_return_value_stub(address destination, const char* name, bool has_res) {
5722 
5723     // Information about frame layout at time of blocking runtime call.
5724     // Note that we only have to preserve callee-saved registers since
5725     // the compilers are responsible for supplying a continuation point
5726     // if they expect all registers to be preserved.
5727     // n.b. aarch64 asserts that frame::arg_reg_save_area_bytes == 0
5728     enum layout {
5729       rfp_off = 0, rfp_off2,
5730 
5731       j_rarg7_off, j_rarg7_2,
5732       j_rarg6_off, j_rarg6_2,
5733       j_rarg5_off, j_rarg5_2,
5734       j_rarg4_off, j_rarg4_2,
5735       j_rarg3_off, j_rarg3_2,
5736       j_rarg2_off, j_rarg2_2,
5737       j_rarg1_off, j_rarg1_2,
5738       j_rarg0_off, j_rarg0_2,
5739 
5740       j_farg0_off, j_farg0_2,
5741       j_farg1_off, j_farg1_2,
5742       j_farg2_off, j_farg2_2,
5743       j_farg3_off, j_farg3_2,
5744       j_farg4_off, j_farg4_2,
5745       j_farg5_off, j_farg5_2,
5746       j_farg6_off, j_farg6_2,
5747       j_farg7_off, j_farg7_2,
5748 
5749       return_off, return_off2,
5750       framesize // inclusive of return address
5751     };
5752 
5753     int insts_size = 512;
5754     int locs_size  = 64;
5755 
5756     CodeBuffer code(name, insts_size, locs_size);
5757     OopMapSet* oop_maps  = new OopMapSet();
5758     MacroAssembler* masm = new MacroAssembler(&amp;code);
5759 
5760     address start = __ pc();
5761 
5762     const Address f7_save       (rfp, j_farg7_off * wordSize);
5763     const Address f6_save       (rfp, j_farg6_off * wordSize);
5764     const Address f5_save       (rfp, j_farg5_off * wordSize);
5765     const Address f4_save       (rfp, j_farg4_off * wordSize);
5766     const Address f3_save       (rfp, j_farg3_off * wordSize);
5767     const Address f2_save       (rfp, j_farg2_off * wordSize);
5768     const Address f1_save       (rfp, j_farg1_off * wordSize);
5769     const Address f0_save       (rfp, j_farg0_off * wordSize);
5770 
5771     const Address r0_save      (rfp, j_rarg0_off * wordSize);
5772     const Address r1_save      (rfp, j_rarg1_off * wordSize);
5773     const Address r2_save      (rfp, j_rarg2_off * wordSize);
5774     const Address r3_save      (rfp, j_rarg3_off * wordSize);
5775     const Address r4_save      (rfp, j_rarg4_off * wordSize);
5776     const Address r5_save      (rfp, j_rarg5_off * wordSize);
5777     const Address r6_save      (rfp, j_rarg6_off * wordSize);
5778     const Address r7_save      (rfp, j_rarg7_off * wordSize);
5779 
5780     // Generate oop map
5781     OopMap* map = new OopMap(framesize, 0);
5782 
5783     map-&gt;set_callee_saved(VMRegImpl::stack2reg(rfp_off), rfp-&gt;as_VMReg());
5784     map-&gt;set_callee_saved(VMRegImpl::stack2reg(j_rarg7_off), j_rarg7-&gt;as_VMReg());
5785     map-&gt;set_callee_saved(VMRegImpl::stack2reg(j_rarg6_off), j_rarg6-&gt;as_VMReg());
5786     map-&gt;set_callee_saved(VMRegImpl::stack2reg(j_rarg5_off), j_rarg5-&gt;as_VMReg());
5787     map-&gt;set_callee_saved(VMRegImpl::stack2reg(j_rarg4_off), j_rarg4-&gt;as_VMReg());
5788     map-&gt;set_callee_saved(VMRegImpl::stack2reg(j_rarg3_off), j_rarg3-&gt;as_VMReg());
5789     map-&gt;set_callee_saved(VMRegImpl::stack2reg(j_rarg2_off), j_rarg2-&gt;as_VMReg());
5790     map-&gt;set_callee_saved(VMRegImpl::stack2reg(j_rarg1_off), j_rarg1-&gt;as_VMReg());
5791     map-&gt;set_callee_saved(VMRegImpl::stack2reg(j_rarg0_off), j_rarg0-&gt;as_VMReg());
5792 
5793     map-&gt;set_callee_saved(VMRegImpl::stack2reg(j_farg0_off), j_farg0-&gt;as_VMReg());
5794     map-&gt;set_callee_saved(VMRegImpl::stack2reg(j_farg1_off), j_farg1-&gt;as_VMReg());
5795     map-&gt;set_callee_saved(VMRegImpl::stack2reg(j_farg2_off), j_farg2-&gt;as_VMReg());
5796     map-&gt;set_callee_saved(VMRegImpl::stack2reg(j_farg3_off), j_farg3-&gt;as_VMReg());
5797     map-&gt;set_callee_saved(VMRegImpl::stack2reg(j_farg4_off), j_farg4-&gt;as_VMReg());
5798     map-&gt;set_callee_saved(VMRegImpl::stack2reg(j_farg5_off), j_farg5-&gt;as_VMReg());
5799     map-&gt;set_callee_saved(VMRegImpl::stack2reg(j_farg6_off), j_farg6-&gt;as_VMReg());
5800     map-&gt;set_callee_saved(VMRegImpl::stack2reg(j_farg7_off), j_farg7-&gt;as_VMReg());
5801 
5802     // This is an inlined and slightly modified version of call_VM
5803     // which has the ability to fetch the return PC out of
5804     // thread-local storage and also sets up last_Java_sp slightly
5805     // differently than the real call_VM
5806 
5807     __ enter(); // Save FP and LR before call
5808 
5809     assert(is_even(framesize/2), &quot;sp not 16-byte aligned&quot;);
5810 
5811     // lr and fp are already in place
5812     __ sub(sp, rfp, ((unsigned)framesize - 4) &lt;&lt; LogBytesPerInt); // prolog
5813 
5814     __ strd(j_farg7, f7_save);
5815     __ strd(j_farg6, f6_save);
5816     __ strd(j_farg5, f5_save);
5817     __ strd(j_farg4, f4_save);
5818     __ strd(j_farg3, f3_save);
5819     __ strd(j_farg2, f2_save);
5820     __ strd(j_farg1, f1_save);
5821     __ strd(j_farg0, f0_save);
5822 
5823     __ str(j_rarg0, r0_save);
5824     __ str(j_rarg1, r1_save);
5825     __ str(j_rarg2, r2_save);
5826     __ str(j_rarg3, r3_save);
5827     __ str(j_rarg4, r4_save);
5828     __ str(j_rarg5, r5_save);
5829     __ str(j_rarg6, r6_save);
5830     __ str(j_rarg7, r7_save);
5831 
5832     int frame_complete = __ pc() - start;
5833 
5834     // Set up last_Java_sp and last_Java_fp
5835     address the_pc = __ pc();
5836     __ set_last_Java_frame(sp, rfp, the_pc, rscratch1);
5837 
5838     // Call runtime
5839     __ mov(c_rarg0, rthread);
5840     __ mov(c_rarg1, r0);
5841 
5842     BLOCK_COMMENT(&quot;call runtime_entry&quot;);
5843     __ mov(rscratch1, destination);
5844     __ blr(rscratch1);
5845 
5846     oop_maps-&gt;add_gc_map(the_pc - start, map);
5847 
5848     __ reset_last_Java_frame(false);
5849     __ maybe_isb();
5850 
5851     __ ldrd(j_farg7, f7_save);
5852     __ ldrd(j_farg6, f6_save);
5853     __ ldrd(j_farg5, f5_save);
5854     __ ldrd(j_farg4, f4_save);
5855     __ ldrd(j_farg3, f3_save);
5856     __ ldrd(j_farg3, f2_save);
5857     __ ldrd(j_farg1, f1_save);
5858     __ ldrd(j_farg0, f0_save);
5859 
5860     __ ldr(j_rarg0, r0_save);
5861     __ ldr(j_rarg1, r1_save);
5862     __ ldr(j_rarg2, r2_save);
5863     __ ldr(j_rarg3, r3_save);
5864     __ ldr(j_rarg4, r4_save);
5865     __ ldr(j_rarg5, r5_save);
5866     __ ldr(j_rarg6, r6_save);
5867     __ ldr(j_rarg7, r7_save);
5868 
5869     __ leave();
5870 
5871     // check for pending exceptions
5872     Label pending;
5873     __ ldr(rscratch1, Address(rthread, in_bytes(Thread::pending_exception_offset())));
5874     __ cmp(rscratch1, (u1)NULL_WORD);
5875     __ br(Assembler::NE, pending);
5876 
5877     if (has_res) {
5878       __ get_vm_result(r0, rthread);
5879     }
5880     __ ret(lr);
5881 
5882     __ bind(pending);
5883     __ ldr(r0, Address(rthread, in_bytes(Thread::pending_exception_offset())));
5884     __ far_jump(RuntimeAddress(StubRoutines::forward_exception_entry()));
5885 
5886 
5887     // codeBlob framesize is in words (not VMRegImpl::slot_size)
5888     int frame_size_in_words = (framesize &gt;&gt; (LogBytesPerWord - LogBytesPerInt));
5889     RuntimeStub* stub =
5890       RuntimeStub::new_runtime_stub(name, &amp;code, frame_complete, frame_size_in_words, oop_maps, false);
5891 
5892     return stub-&gt;entry_point();
5893   }
5894 
5895   // Initialization
5896   void generate_initial() {
5897     // Generate initial stubs and initializes the entry points
5898 
5899     // entry points that exist in all platforms Note: This is code
5900     // that could be shared among different platforms - however the
5901     // benefit seems to be smaller than the disadvantage of having a
5902     // much more complicated generator structure. See also comment in
5903     // stubRoutines.hpp.
5904 
5905     StubRoutines::_forward_exception_entry = generate_forward_exception();
5906 
5907     StubRoutines::_call_stub_entry =
5908       generate_call_stub(StubRoutines::_call_stub_return_address);
5909 
5910     // is referenced by megamorphic call
5911     StubRoutines::_catch_exception_entry = generate_catch_exception();
5912 
5913     // Build this early so it&#39;s available for the interpreter.
5914     StubRoutines::_throw_StackOverflowError_entry =
5915       generate_throw_exception(&quot;StackOverflowError throw_exception&quot;,
5916                                CAST_FROM_FN_PTR(address,
5917                                                 SharedRuntime::throw_StackOverflowError));
5918     StubRoutines::_throw_delayed_StackOverflowError_entry =
5919       generate_throw_exception(&quot;delayed StackOverflowError throw_exception&quot;,
5920                                CAST_FROM_FN_PTR(address,
5921                                                 SharedRuntime::throw_delayed_StackOverflowError));
5922     if (UseCRC32Intrinsics) {
5923       // set table address before stub generation which use it
5924       StubRoutines::_crc_table_adr = (address)StubRoutines::aarch64::_crc_table;
5925       StubRoutines::_updateBytesCRC32 = generate_updateBytesCRC32();
5926     }
5927 
5928     if (UseCRC32CIntrinsics) {
5929       StubRoutines::_updateBytesCRC32C = generate_updateBytesCRC32C();
5930     }
5931 
5932     // Disabled until JDK-8210858 is fixed
5933     // if (vmIntrinsics::is_intrinsic_available(vmIntrinsics::_dlog)) {
5934     //   StubRoutines::_dlog = generate_dlog();
5935     // }
5936 
5937     if (vmIntrinsics::is_intrinsic_available(vmIntrinsics::_dsin)) {
5938       StubRoutines::_dsin = generate_dsin_dcos(/* isCos = */ false);
5939     }
5940 
5941     if (vmIntrinsics::is_intrinsic_available(vmIntrinsics::_dcos)) {
5942       StubRoutines::_dcos = generate_dsin_dcos(/* isCos = */ true);
5943     }
5944 
5945     StubRoutines::_load_value_type_fields_in_regs =
5946          generate_return_value_stub(CAST_FROM_FN_PTR(address, SharedRuntime::load_value_type_fields_in_regs), &quot;load_value_type_fields_in_regs&quot;, false);
5947     StubRoutines::_store_value_type_fields_to_buf =
5948          generate_return_value_stub(CAST_FROM_FN_PTR(address, SharedRuntime::store_value_type_fields_to_buf), &quot;store_value_type_fields_to_buf&quot;, true);
5949 
5950     // Safefetch stubs.
5951     generate_safefetch(&quot;SafeFetch32&quot;, sizeof(int),     &amp;StubRoutines::_safefetch32_entry,
5952                                                        &amp;StubRoutines::_safefetch32_fault_pc,
5953                                                        &amp;StubRoutines::_safefetch32_continuation_pc);
5954     generate_safefetch(&quot;SafeFetchN&quot;, sizeof(intptr_t), &amp;StubRoutines::_safefetchN_entry,
5955                                                        &amp;StubRoutines::_safefetchN_fault_pc,
5956                                                        &amp;StubRoutines::_safefetchN_continuation_pc);
5957   }
5958 
5959   void generate_all() {
5960     // support for verify_oop (must happen after universe_init)
5961     StubRoutines::_verify_oop_subroutine_entry     = generate_verify_oop();
5962     StubRoutines::_throw_AbstractMethodError_entry =
5963       generate_throw_exception(&quot;AbstractMethodError throw_exception&quot;,
5964                                CAST_FROM_FN_PTR(address,
5965                                                 SharedRuntime::
5966                                                 throw_AbstractMethodError));
5967 
5968     StubRoutines::_throw_IncompatibleClassChangeError_entry =
5969       generate_throw_exception(&quot;IncompatibleClassChangeError throw_exception&quot;,
5970                                CAST_FROM_FN_PTR(address,
5971                                                 SharedRuntime::
5972                                                 throw_IncompatibleClassChangeError));
5973 
5974     StubRoutines::_throw_NullPointerException_at_call_entry =
5975       generate_throw_exception(&quot;NullPointerException at call throw_exception&quot;,
5976                                CAST_FROM_FN_PTR(address,
5977                                                 SharedRuntime::
5978                                                 throw_NullPointerException_at_call));
5979 
5980     // arraycopy stubs used by compilers
5981     generate_arraycopy_stubs();
5982 
5983     // has negatives stub for large arrays.
5984     StubRoutines::aarch64::_has_negatives = generate_has_negatives(StubRoutines::aarch64::_has_negatives_long);
5985 
5986     // array equals stub for large arrays.
5987     if (!UseSimpleArrayEquals) {
5988       StubRoutines::aarch64::_large_array_equals = generate_large_array_equals();
5989     }
5990 
5991     generate_compare_long_strings();
5992 
5993     generate_string_indexof_stubs();
5994 
5995     // byte_array_inflate stub for large arrays.
5996     StubRoutines::aarch64::_large_byte_array_inflate = generate_large_byte_array_inflate();
5997 
5998     BarrierSetNMethod* bs_nm = BarrierSet::barrier_set()-&gt;barrier_set_nmethod();
5999     if (bs_nm != NULL) {
6000       StubRoutines::aarch64::_method_entry_barrier = generate_method_entry_barrier();
6001     }
6002 #ifdef COMPILER2
6003     if (UseMultiplyToLenIntrinsic) {
6004       StubRoutines::_multiplyToLen = generate_multiplyToLen();
6005     }
6006 
6007     if (UseSquareToLenIntrinsic) {
6008       StubRoutines::_squareToLen = generate_squareToLen();
6009     }
6010 
6011     if (UseMulAddIntrinsic) {
6012       StubRoutines::_mulAdd = generate_mulAdd();
6013     }
6014 
6015     if (UseMontgomeryMultiplyIntrinsic) {
6016       StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;montgomeryMultiply&quot;);
6017       MontgomeryMultiplyGenerator g(_masm, /*squaring*/false);
6018       StubRoutines::_montgomeryMultiply = g.generate_multiply();
6019     }
6020 
6021     if (UseMontgomerySquareIntrinsic) {
6022       StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;montgomerySquare&quot;);
6023       MontgomeryMultiplyGenerator g(_masm, /*squaring*/true);
6024       // We use generate_multiply() rather than generate_square()
6025       // because it&#39;s faster for the sizes of modulus we care about.
6026       StubRoutines::_montgomerySquare = g.generate_multiply();
6027     }
6028 #endif // COMPILER2
6029 
6030     // generate GHASH intrinsics code
6031     if (UseGHASHIntrinsics) {
6032       StubRoutines::_ghash_processBlocks = generate_ghash_processBlocks();
6033     }
6034 
6035     // data cache line writeback
6036     StubRoutines::_data_cache_writeback = generate_data_cache_writeback();
6037     StubRoutines::_data_cache_writeback_sync = generate_data_cache_writeback_sync();
6038 
6039     if (UseAESIntrinsics) {
6040       StubRoutines::_aescrypt_encryptBlock = generate_aescrypt_encryptBlock();
6041       StubRoutines::_aescrypt_decryptBlock = generate_aescrypt_decryptBlock();
6042       StubRoutines::_cipherBlockChaining_encryptAESCrypt = generate_cipherBlockChaining_encryptAESCrypt();
6043       StubRoutines::_cipherBlockChaining_decryptAESCrypt = generate_cipherBlockChaining_decryptAESCrypt();
6044     }
6045 
6046     if (UseSHA1Intrinsics) {
6047       StubRoutines::_sha1_implCompress     = generate_sha1_implCompress(false,   &quot;sha1_implCompress&quot;);
6048       StubRoutines::_sha1_implCompressMB   = generate_sha1_implCompress(true,    &quot;sha1_implCompressMB&quot;);
6049     }
6050     if (UseSHA256Intrinsics) {
6051       StubRoutines::_sha256_implCompress   = generate_sha256_implCompress(false, &quot;sha256_implCompress&quot;);
6052       StubRoutines::_sha256_implCompressMB = generate_sha256_implCompress(true,  &quot;sha256_implCompressMB&quot;);
6053     }
6054 
6055     // generate Adler32 intrinsics code
6056     if (UseAdler32Intrinsics) {
6057       StubRoutines::_updateBytesAdler32 = generate_updateBytesAdler32();
6058     }
6059 
6060     StubRoutines::aarch64::set_completed();
6061   }
6062 
6063  public:
6064   StubGenerator(CodeBuffer* code, bool all) : StubCodeGenerator(code) {
6065     if (all) {
6066       generate_all();
6067     } else {
6068       generate_initial();
6069     }
6070   }
6071 }; // end class declaration
6072 
6073 #define UCM_TABLE_MAX_ENTRIES 8
6074 void StubGenerator_generate(CodeBuffer* code, bool all) {
6075   if (UnsafeCopyMemory::_table == NULL) {
6076     UnsafeCopyMemory::create_table(UCM_TABLE_MAX_ENTRIES);
6077   }
6078   StubGenerator g(code, all);
6079 }
    </pre>
  </body>
</html>