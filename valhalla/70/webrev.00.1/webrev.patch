diff a/.hgtags b/.hgtags
--- a/.hgtags
+++ b/.hgtags
@@ -487,10 +487,11 @@
 758deedaae8406ae60147486107a54e9864aa7b0 jdk-11+13
 3595bd343b65f8c37818ebe6a4c343ddeb1a5f88 jdk-11+14
 a11c1cb542bbd1671d25b85efe7d09b983c48525 jdk-11+15
 02934b0d661b82b7fe1052a04998d2091352e08d jdk-11+16
 64e4b1686141e57a681936a8283983341484676e jdk-11+17
+d2aa5d494481a1039a092d70efa1f5c9826c5b77 lw1_0
 e1b3def126240d5433902f3cb0e91a4c27f6db50 jdk-11+18
 36ca515343e00b021dcfc902e986d26ec994a2e5 jdk-11+19
 95aad0c785e497f1bade3955c4e4a677b629fa9d jdk-12+0
 9816d7cc655e53ba081f938b656e31971b8f097a jdk-11+20
 14708e1acdc3974f4539027cbbcfa6d69f83cf51 jdk-11+21
@@ -509,10 +510,11 @@
 ef57958c7c511162da8d9a75f0b977f0f7ac464e jdk-12+7
 76072a077ee1d815152d45d1692c4b36c53c5c49 jdk-11+28
 492b366f8e5784cc4927c2c98f9b8a3f16c067eb jdk-12+8
 31b159f30fb281016c5f0c103552809aeda84063 jdk-12+9
 8f594f75e0547d4ca16649cb3501659e3155e81b jdk-12+10
+6132641c6ff61b7b8f3f10b9cd385aafadbd72ef lworld_stable
 f0f5d23449d31f1b3580c8a73313918cafeaefd7 jdk-12+11
 15094d12a632f452a2064318a4e416d0c7a9ce0c jdk-12+12
 511a9946f83e3e3c7b9dbe1840367063fb39b4e1 jdk-12+13
 8897e41b327c0a5601c6ba2bba5d07f15a3ffc91 jdk-12+14
 8897e41b327c0a5601c6ba2bba5d07f15a3ffc91 jdk-12+14
@@ -565,10 +567,14 @@
 22b3b7983adab54e318f75aeb94471f7a4429c1e jdk-14+0
 22b3b7983adab54e318f75aeb94471f7a4429c1e jdk-13+25
 2f4e214781a1d597ed36bf5a36f20928c6c82996 jdk-14+1
 0692b67f54621991ba7afbf23e55b788f3555e69 jdk-13+26
 43627549a488b7d0b4df8fad436e36233df89877 jdk-14+2
+6132641c6ff61b7b8f3f10b9cd385aafadbd72ef lworld_stable
+2b098533f1e52d7d541121409b745d9420886945 lworld_stable
+2b098533f1e52d7d541121409b745d9420886945 lworld_stable
+7c637fd25e7d6fccdab1098bedd48ed195a86cc7 lworld_stable
 b7f68ddec66f996ae3aad03291d129ca9f02482d jdk-13+27
 e64383344f144217c36196c3c8a2df8f588a2af3 jdk-14+3
 1e95931e7d8fa7e3899340a9c7cb28dbea50c10c jdk-13+28
 19d0b382f0869f72d4381b54fa129f1c74b6e766 jdk-14+4
 3081f39a3d30d63b112098386ac2bb027c2b7223 jdk-13+29
diff a/make/Images.gmk b/make/Images.gmk
--- a/make/Images.gmk
+++ b/make/Images.gmk
@@ -52,10 +52,15 @@
 
 ################################################################################
 
 BASE_RELEASE_FILE := $(JDK_OUTPUTDIR)/release
 
+$(JDK_IMAGE_DIR)/README: $(TOPDIR)/src/README
+	$(install-file)
+
+JDK_TARGETS += $(JDK_IMAGE_DIR)/README
+
 JMODS := $(wildcard $(IMAGES_OUTPUTDIR)/jmods/*.jmod)
 
 JLINK_ORDER_RESOURCES := **module-info.class
 JLINK_JLI_CLASSES :=
 ifeq ($(ENABLE_GENERATE_CLASSLIST), true)
diff a/make/conf/jib-profiles.js b/make/conf/jib-profiles.js
--- a/make/conf/jib-profiles.js
+++ b/make/conf/jib-profiles.js
@@ -1369,10 +1369,11 @@
             preString = version_numbers.get("DEFAULT_PROMOTED_VERSION_PRE");
         }
         args = concat(args, "--with-version-pre=" + preString,
                      "--with-version-opt=" + optString);
     } else {
+        args = concat(args, "--with-version-pre=lworld2ea");
         args = concat(args, "--with-version-opt=" + common.build_id);
     }
     return args;
 }
 
diff a/make/modules/java.base/gensrc/GensrcVarHandles.gmk b/make/modules/java.base/gensrc/GensrcVarHandles.gmk
--- a/make/modules/java.base/gensrc/GensrcVarHandles.gmk
+++ b/make/modules/java.base/gensrc/GensrcVarHandles.gmk
@@ -50,13 +50,23 @@
 
   ifneq ($$(findstring $$($1_Type), Byte Short Char), )
     $1_ARGS += -KShorterThanInt
   endif
 
+  ifeq ($$($1_Type), Reference)
+    $1_ARGS += -KReference
+  endif
+
+  ifeq ($$($1_Type), Value)
+    $1_ARGS += -KValue
+  endif
+
   $$($1_FILENAME): $(VARHANDLES_SRC_DIR)/X-VarHandle.java.template $(BUILD_TOOLS_JDK)
         ifeq ($$($1_Type), Reference)
 	  $$(eval $1_type := Object)
+        else ifeq ($$($1_Type), Value)
+	  $$(eval $1_type := Object)
         else
 	  $$(eval $1_type := $$$$(shell $(TR) '[:upper:]' '[:lower:]' <<< $$$$($1_Type)))
         endif
 	$$(call MakeDir, $$(@D))
 	$(RM) $$@
@@ -260,11 +270,11 @@
 endef
 
 ################################################################################
 
 # List the types to generate source for, with capitalized first letter
-VARHANDLES_TYPES := Boolean Byte Short Char Int Long Float Double Reference
+VARHANDLES_TYPES := Boolean Byte Short Char Int Long Float Double Reference Value
 $(foreach t, $(VARHANDLES_TYPES), \
   $(eval $(call GenerateVarHandle,VAR_HANDLE_$t,$t)))
 
 # List the types to generate source for, with capitalized first letter
 VARHANDLES_BYTE_ARRAY_TYPES := Short Char Int Long Float Double
diff a/src/hotspot/cpu/aarch64/aarch64.ad b/src/hotspot/cpu/aarch64/aarch64.ad
--- a/src/hotspot/cpu/aarch64/aarch64.ad
+++ b/src/hotspot/cpu/aarch64/aarch64.ad
@@ -1654,10 +1654,12 @@
 
 void MachPrologNode::emit(CodeBuffer &cbuf, PhaseRegAlloc *ra_) const {
   Compile* C = ra_->C;
   C2_MacroAssembler _masm(&cbuf);
 
+  __ verified_entry(C, 0);
+  __ bind(*_verified_entry);
   // n.b. frame size includes space for return pc and rfp
   const long framesize = C->output()->frame_size_in_bytes();
   assert(framesize%(2*wordSize) == 0, "must preserve 2*wordSize alignment");
 
   // insert a nop at the start of the prolog so we can patch in a
@@ -1996,12 +1998,50 @@
 uint BoxLockNode::size(PhaseRegAlloc *ra_) const {
   // BoxLockNode is not a MachNode, so we can't just call MachNode::size(ra_).
   return 4;
 }
 
-//=============================================================================
+///=============================================================================
+#ifndef PRODUCT
+void MachVEPNode::format(PhaseRegAlloc* ra_, outputStream* st) const
+{
+  st->print_cr("# MachVEPNode");
+  if (!_verified) {
+    st->print_cr("\t load_class");
+  } else {
+    st->print_cr("\t unpack_value_arg");
+  }
+}
+#endif
+
+void MachVEPNode::emit(CodeBuffer& cbuf, PhaseRegAlloc* ra_) const
+{
+  MacroAssembler _masm(&cbuf);
+
+  if (!_verified) {
+    Label skip;
+    __ cmp_klass(j_rarg0, rscratch2, rscratch1);
+    __ br(Assembler::EQ, skip);
+      __ far_jump(RuntimeAddress(SharedRuntime::get_ic_miss_stub()));
+    __ bind(skip);
+
+  } else {
+    // Unpack value type args passed as oop and then jump to
+    // the verified entry point (skipping the unverified entry).
+    __ unpack_value_args(ra_->C, _receiver_only);
+    __ b(*_verified_entry);
+  }
+}
+
+
+uint MachVEPNode::size(PhaseRegAlloc* ra_) const
+{
+  return MachNode::size(ra_); // too many variables; just compute it the hard way
+}
 
+
+//=============================================================================
 #ifndef PRODUCT
 void MachUEPNode::format(PhaseRegAlloc* ra_, outputStream* st) const
 {
   st->print_cr("# MachUEPNode");
   if (UseCompressedClassPointers) {
@@ -2019,13 +2059,15 @@
 
 void MachUEPNode::emit(CodeBuffer& cbuf, PhaseRegAlloc* ra_) const
 {
   // This is the unverified entry point.
   C2_MacroAssembler _masm(&cbuf);
+  Label skip;
 
+  // UseCompressedClassPointers logic are inside cmp_klass
   __ cmp_klass(j_rarg0, rscratch2, rscratch1);
-  Label skip;
+
   // TODO
   // can we avoid this skip and still use a reloc?
   __ br(Assembler::EQ, skip);
   __ far_jump(RuntimeAddress(SharedRuntime::get_ic_miss_stub()));
   __ bind(skip);
@@ -2428,11 +2470,10 @@
 }
 
 void Compile::reshape_address(AddPNode* addp) {
 }
 
-
 #define MOV_VOLATILE(REG, BASE, INDEX, SCALE, DISP, SCRATCH, INSN)      \
   C2_MacroAssembler _masm(&cbuf);                                       \
   {                                                                     \
     guarantee(INDEX == -1, "mode not permitted for volatile");          \
     guarantee(DISP == 0, "mode not permitted for volatile");            \
@@ -8288,10 +8329,25 @@
   %}
 
   ins_pipe(ialu_reg);
 %}
 
+instruct castN2X(iRegLNoSp dst, iRegN src) %{
+  match(Set dst (CastP2X src));
+
+  ins_cost(INSN_COST);
+  format %{ "mov $dst, $src\t# ptr -> long" %}
+
+  ins_encode %{
+    if ($dst$$reg != $src$$reg) {
+      __ mov(as_Register($dst$$reg), as_Register($src$$reg));
+    }
+  %}
+
+  ins_pipe(ialu_reg);
+%}
+
 instruct castP2X(iRegLNoSp dst, iRegP src) %{
   match(Set dst (CastP2X src));
 
   ins_cost(INSN_COST);
   format %{ "mov $dst, $src\t# ptr -> long" %}
@@ -8303,10 +8359,41 @@
   %}
 
   ins_pipe(ialu_reg);
 %}
 
+instruct castN2I(iRegINoSp dst, iRegN src) %{
+  match(Set dst (CastN2I src));
+
+  ins_cost(INSN_COST);
+  format %{ "movw $dst, $src\t# compressed ptr -> int" %}
+
+  ins_encode %{
+    if ($dst$$reg != $src$$reg) {
+      __ movw(as_Register($dst$$reg), as_Register($src$$reg));
+    }
+  %}
+
+  ins_pipe(ialu_reg);
+%}
+
+instruct castI2N(iRegNNoSp dst, iRegI src) %{
+  match(Set dst (CastI2N src));
+
+  ins_cost(INSN_COST);
+  format %{ "movw $dst, $src\t# int -> compressed ptr" %}
+
+  ins_encode %{
+    if ($dst$$reg != $src$$reg) {
+      __ movw(as_Register($dst$$reg), as_Register($src$$reg));
+    }
+  %}
+
+  ins_pipe(ialu_reg);
+%}
+
+
 // Convert oop into int for vectors alignment masking
 instruct convP2I(iRegINoSp dst, iRegP src) %{
   match(Set dst (ConvL2I (CastP2X src)));
 
   ins_cost(INSN_COST);
@@ -13837,37 +13924,20 @@
 %}
 
 // ============================================================================
 // clearing of an array
 
-instruct clearArray_reg_reg(iRegL_R11 cnt, iRegP_R10 base, Universe dummy, rFlagsReg cr)
+instruct clearArray_reg_reg(iRegL_R11 cnt, iRegP_R10 base, iRegL val, Universe dummy, rFlagsReg cr)
 %{
-  match(Set dummy (ClearArray cnt base));
+  match(Set dummy (ClearArray (Binary cnt base) val));
   effect(USE_KILL cnt, USE_KILL base);
 
   ins_cost(4 * INSN_COST);
-  format %{ "ClearArray $cnt, $base" %}
-
-  ins_encode %{
-    __ zero_words($base$$Register, $cnt$$Register);
-  %}
-
-  ins_pipe(pipe_class_memory);
-%}
-
-instruct clearArray_imm_reg(immL cnt, iRegP_R10 base, Universe dummy, rFlagsReg cr)
-%{
-  predicate((u_int64_t)n->in(2)->get_long()
-            < (u_int64_t)(BlockZeroingLowLimit >> LogBytesPerWord));
-  match(Set dummy (ClearArray cnt base));
-  effect(USE_KILL base);
-
-  ins_cost(4 * INSN_COST);
-  format %{ "ClearArray $cnt, $base" %}
+  format %{ "ClearArray $cnt, $base, $val" %}
 
   ins_encode %{
-    __ zero_words($base$$Register, (u_int64_t)$cnt$$constant);
+    __ fill_words($base$$Register, $cnt$$Register, $val$$Register);
   %}
 
   ins_pipe(pipe_class_memory);
 %}
 
diff a/src/hotspot/cpu/aarch64/stubGenerator_aarch64.cpp b/src/hotspot/cpu/aarch64/stubGenerator_aarch64.cpp
--- a/src/hotspot/cpu/aarch64/stubGenerator_aarch64.cpp
+++ b/src/hotspot/cpu/aarch64/stubGenerator_aarch64.cpp
@@ -302,18 +302,20 @@
     // save current address for use by exception handling code
 
     return_address = __ pc();
 
     // store result depending on type (everything that is not
-    // T_OBJECT, T_LONG, T_FLOAT or T_DOUBLE is treated as T_INT)
+    // T_OBJECT, T_VALUETYPE, T_LONG, T_FLOAT or T_DOUBLE is treated as T_INT)
     // n.b. this assumes Java returns an integral result in r0
     // and a floating result in j_farg0
     __ ldr(j_rarg2, result);
-    Label is_long, is_float, is_double, exit;
+    Label is_long, is_float, is_double, is_value, exit;
     __ ldr(j_rarg1, result_type);
     __ cmp(j_rarg1, (u1)T_OBJECT);
     __ br(Assembler::EQ, is_long);
+    __ cmp(j_rarg1, (u1)T_VALUETYPE);
+    __ br(Assembler::EQ, is_value);
     __ cmp(j_rarg1, (u1)T_LONG);
     __ br(Assembler::EQ, is_long);
     __ cmp(j_rarg1, (u1)T_FLOAT);
     __ br(Assembler::EQ, is_float);
     __ cmp(j_rarg1, (u1)T_DOUBLE);
@@ -364,10 +366,23 @@
     // leave frame and return to caller
     __ leave();
     __ ret(lr);
 
     // handle return types different from T_INT
+    __ BIND(is_value);
+    if (InlineTypeReturnedAsFields) {
+      // Check for flattened return value
+      __ cbz(r0, is_long);
+      // Initialize pre-allocated buffer
+      __ mov(r1, r0);
+      __ andr(r1, r1, -2);
+      __ ldr(r1, Address(r1, InstanceKlass::adr_valueklass_fixed_block_offset()));
+      __ ldr(r1, Address(r1, ValueKlass::pack_handler_offset()));
+      __ ldr(r0, Address(j_rarg2, 0));
+      __ blr(r1);
+      __ b(exit);
+    }
 
     __ BIND(is_long);
     __ str(r0, Address(j_rarg2, 0));
     __ br(Assembler::AL, exit);
 
@@ -1797,11 +1812,11 @@
     //     store_heap_oop(to++, copied_oop);
     //   }
     __ align(OptoLoopAlignment);
 
     __ BIND(L_store_element);
-    __ store_heap_oop(__ post(to, UseCompressedOops ? 4 : 8), copied_oop, noreg, noreg, AS_RAW);  // store the oop
+    __ store_heap_oop(__ post(to, UseCompressedOops ? 4 : 8), copied_oop, noreg, noreg, noreg, AS_RAW);  // store the oop
     __ sub(count, count, 1);
     __ cbz(count, L_do_card_marks);
 
     // ======== loop entry is here ========
     __ BIND(L_load_element);
@@ -5697,10 +5712,188 @@
     //     t0 = sub(Pm_base, Pn_base, t0, len);
     // }
   };
 
 
+  // Call here from the interpreter or compiled code to either load
+  // multiple returned values from the value type instance being
+  // returned to registers or to store returned values to a newly
+  // allocated value type instance.
+  address generate_return_value_stub(address destination, const char* name, bool has_res) {
+
+    // Information about frame layout at time of blocking runtime call.
+    // Note that we only have to preserve callee-saved registers since
+    // the compilers are responsible for supplying a continuation point
+    // if they expect all registers to be preserved.
+    // n.b. aarch64 asserts that frame::arg_reg_save_area_bytes == 0
+    enum layout {
+      rfp_off = 0, rfp_off2,
+
+      j_rarg7_off, j_rarg7_2,
+      j_rarg6_off, j_rarg6_2,
+      j_rarg5_off, j_rarg5_2,
+      j_rarg4_off, j_rarg4_2,
+      j_rarg3_off, j_rarg3_2,
+      j_rarg2_off, j_rarg2_2,
+      j_rarg1_off, j_rarg1_2,
+      j_rarg0_off, j_rarg0_2,
+
+      j_farg0_off, j_farg0_2,
+      j_farg1_off, j_farg1_2,
+      j_farg2_off, j_farg2_2,
+      j_farg3_off, j_farg3_2,
+      j_farg4_off, j_farg4_2,
+      j_farg5_off, j_farg5_2,
+      j_farg6_off, j_farg6_2,
+      j_farg7_off, j_farg7_2,
+
+      return_off, return_off2,
+      framesize // inclusive of return address
+    };
+
+    int insts_size = 512;
+    int locs_size  = 64;
+
+    CodeBuffer code(name, insts_size, locs_size);
+    OopMapSet* oop_maps  = new OopMapSet();
+    MacroAssembler* masm = new MacroAssembler(&code);
+
+    address start = __ pc();
+
+    const Address f7_save       (rfp, j_farg7_off * wordSize);
+    const Address f6_save       (rfp, j_farg6_off * wordSize);
+    const Address f5_save       (rfp, j_farg5_off * wordSize);
+    const Address f4_save       (rfp, j_farg4_off * wordSize);
+    const Address f3_save       (rfp, j_farg3_off * wordSize);
+    const Address f2_save       (rfp, j_farg2_off * wordSize);
+    const Address f1_save       (rfp, j_farg1_off * wordSize);
+    const Address f0_save       (rfp, j_farg0_off * wordSize);
+
+    const Address r0_save      (rfp, j_rarg0_off * wordSize);
+    const Address r1_save      (rfp, j_rarg1_off * wordSize);
+    const Address r2_save      (rfp, j_rarg2_off * wordSize);
+    const Address r3_save      (rfp, j_rarg3_off * wordSize);
+    const Address r4_save      (rfp, j_rarg4_off * wordSize);
+    const Address r5_save      (rfp, j_rarg5_off * wordSize);
+    const Address r6_save      (rfp, j_rarg6_off * wordSize);
+    const Address r7_save      (rfp, j_rarg7_off * wordSize);
+
+    // Generate oop map
+    OopMap* map = new OopMap(framesize, 0);
+
+    map->set_callee_saved(VMRegImpl::stack2reg(rfp_off), rfp->as_VMReg());
+    map->set_callee_saved(VMRegImpl::stack2reg(j_rarg7_off), j_rarg7->as_VMReg());
+    map->set_callee_saved(VMRegImpl::stack2reg(j_rarg6_off), j_rarg6->as_VMReg());
+    map->set_callee_saved(VMRegImpl::stack2reg(j_rarg5_off), j_rarg5->as_VMReg());
+    map->set_callee_saved(VMRegImpl::stack2reg(j_rarg4_off), j_rarg4->as_VMReg());
+    map->set_callee_saved(VMRegImpl::stack2reg(j_rarg3_off), j_rarg3->as_VMReg());
+    map->set_callee_saved(VMRegImpl::stack2reg(j_rarg2_off), j_rarg2->as_VMReg());
+    map->set_callee_saved(VMRegImpl::stack2reg(j_rarg1_off), j_rarg1->as_VMReg());
+    map->set_callee_saved(VMRegImpl::stack2reg(j_rarg0_off), j_rarg0->as_VMReg());
+
+    map->set_callee_saved(VMRegImpl::stack2reg(j_farg0_off), j_farg0->as_VMReg());
+    map->set_callee_saved(VMRegImpl::stack2reg(j_farg1_off), j_farg1->as_VMReg());
+    map->set_callee_saved(VMRegImpl::stack2reg(j_farg2_off), j_farg2->as_VMReg());
+    map->set_callee_saved(VMRegImpl::stack2reg(j_farg3_off), j_farg3->as_VMReg());
+    map->set_callee_saved(VMRegImpl::stack2reg(j_farg4_off), j_farg4->as_VMReg());
+    map->set_callee_saved(VMRegImpl::stack2reg(j_farg5_off), j_farg5->as_VMReg());
+    map->set_callee_saved(VMRegImpl::stack2reg(j_farg6_off), j_farg6->as_VMReg());
+    map->set_callee_saved(VMRegImpl::stack2reg(j_farg7_off), j_farg7->as_VMReg());
+
+    // This is an inlined and slightly modified version of call_VM
+    // which has the ability to fetch the return PC out of
+    // thread-local storage and also sets up last_Java_sp slightly
+    // differently than the real call_VM
+
+    __ enter(); // Save FP and LR before call
+
+    assert(is_even(framesize/2), "sp not 16-byte aligned");
+
+    // lr and fp are already in place
+    __ sub(sp, rfp, ((unsigned)framesize - 4) << LogBytesPerInt); // prolog
+
+    __ strd(j_farg7, f7_save);
+    __ strd(j_farg6, f6_save);
+    __ strd(j_farg5, f5_save);
+    __ strd(j_farg4, f4_save);
+    __ strd(j_farg3, f3_save);
+    __ strd(j_farg2, f2_save);
+    __ strd(j_farg1, f1_save);
+    __ strd(j_farg0, f0_save);
+
+    __ str(j_rarg0, r0_save);
+    __ str(j_rarg1, r1_save);
+    __ str(j_rarg2, r2_save);
+    __ str(j_rarg3, r3_save);
+    __ str(j_rarg4, r4_save);
+    __ str(j_rarg5, r5_save);
+    __ str(j_rarg6, r6_save);
+    __ str(j_rarg7, r7_save);
+
+    int frame_complete = __ pc() - start;
+
+    // Set up last_Java_sp and last_Java_fp
+    address the_pc = __ pc();
+    __ set_last_Java_frame(sp, rfp, the_pc, rscratch1);
+
+    // Call runtime
+    __ mov(c_rarg0, rthread);
+    __ mov(c_rarg1, r0);
+
+    BLOCK_COMMENT("call runtime_entry");
+    __ mov(rscratch1, destination);
+    __ blr(rscratch1);
+
+    oop_maps->add_gc_map(the_pc - start, map);
+
+    __ reset_last_Java_frame(false);
+    __ maybe_isb();
+
+    __ ldrd(j_farg7, f7_save);
+    __ ldrd(j_farg6, f6_save);
+    __ ldrd(j_farg5, f5_save);
+    __ ldrd(j_farg4, f4_save);
+    __ ldrd(j_farg3, f3_save);
+    __ ldrd(j_farg3, f2_save);
+    __ ldrd(j_farg1, f1_save);
+    __ ldrd(j_farg0, f0_save);
+
+    __ ldr(j_rarg0, r0_save);
+    __ ldr(j_rarg1, r1_save);
+    __ ldr(j_rarg2, r2_save);
+    __ ldr(j_rarg3, r3_save);
+    __ ldr(j_rarg4, r4_save);
+    __ ldr(j_rarg5, r5_save);
+    __ ldr(j_rarg6, r6_save);
+    __ ldr(j_rarg7, r7_save);
+
+    __ leave();
+
+    // check for pending exceptions
+    Label pending;
+    __ ldr(rscratch1, Address(rthread, in_bytes(Thread::pending_exception_offset())));
+    __ cmp(rscratch1, (u1)NULL_WORD);
+    __ br(Assembler::NE, pending);
+
+    if (has_res) {
+      __ get_vm_result(r0, rthread);
+    }
+    __ ret(lr);
+
+    __ bind(pending);
+    __ ldr(r0, Address(rthread, in_bytes(Thread::pending_exception_offset())));
+    __ far_jump(RuntimeAddress(StubRoutines::forward_exception_entry()));
+
+
+    // codeBlob framesize is in words (not VMRegImpl::slot_size)
+    int frame_size_in_words = (framesize >> (LogBytesPerWord - LogBytesPerInt));
+    RuntimeStub* stub =
+      RuntimeStub::new_runtime_stub(name, &code, frame_complete, frame_size_in_words, oop_maps, false);
+
+    return stub->entry_point();
+  }
+
   // Initialization
   void generate_initial() {
     // Generate initial stubs and initializes the entry points
 
     // entry points that exist in all platforms Note: This is code
@@ -5747,10 +5940,15 @@
 
     if (vmIntrinsics::is_intrinsic_available(vmIntrinsics::_dcos)) {
       StubRoutines::_dcos = generate_dsin_dcos(/* isCos = */ true);
     }
 
+    StubRoutines::_load_value_type_fields_in_regs =
+         generate_return_value_stub(CAST_FROM_FN_PTR(address, SharedRuntime::load_value_type_fields_in_regs), "load_value_type_fields_in_regs", false);
+    StubRoutines::_store_value_type_fields_to_buf =
+         generate_return_value_stub(CAST_FROM_FN_PTR(address, SharedRuntime::store_value_type_fields_to_buf), "store_value_type_fields_to_buf", true);
+
     // Safefetch stubs.
     generate_safefetch("SafeFetch32", sizeof(int),     &StubRoutines::_safefetch32_entry,
                                                        &StubRoutines::_safefetch32_fault_pc,
                                                        &StubRoutines::_safefetch32_continuation_pc);
     generate_safefetch("SafeFetchN", sizeof(intptr_t), &StubRoutines::_safefetchN_entry,
diff a/src/hotspot/cpu/x86/c1_globals_x86.hpp b/src/hotspot/cpu/x86/c1_globals_x86.hpp
--- a/src/hotspot/cpu/x86/c1_globals_x86.hpp
+++ b/src/hotspot/cpu/x86/c1_globals_x86.hpp
@@ -37,11 +37,11 @@
 define_pd_global(bool, ResizeTLAB,                     true );
 define_pd_global(bool, InlineIntrinsics,               true );
 define_pd_global(bool, PreferInterpreterNativeStubs,   false);
 define_pd_global(bool, ProfileTraps,                   false);
 define_pd_global(bool, UseOnStackReplacement,          true );
-define_pd_global(bool, TieredCompilation,              false);
+define_pd_global(bool, TieredCompilation,              true);
 define_pd_global(intx, CompileThreshold,               1500 );
 
 define_pd_global(intx,   OnStackReplacePercentage,     933  );
 define_pd_global(size_t, NewSizeThreadIncrease,        4*K  );
 define_pd_global(uintx,  InitialCodeCacheSize,         160*K);
diff a/src/hotspot/cpu/x86/macroAssembler_x86.cpp b/src/hotspot/cpu/x86/macroAssembler_x86.cpp
--- a/src/hotspot/cpu/x86/macroAssembler_x86.cpp
+++ b/src/hotspot/cpu/x86/macroAssembler_x86.cpp
@@ -43,14 +43,19 @@
 #include "runtime/objectMonitor.hpp"
 #include "runtime/os.hpp"
 #include "runtime/safepoint.hpp"
 #include "runtime/safepointMechanism.hpp"
 #include "runtime/sharedRuntime.hpp"
+#include "runtime/signature_cc.hpp"
 #include "runtime/stubRoutines.hpp"
 #include "runtime/thread.hpp"
 #include "utilities/macros.hpp"
+#include "vmreg_x86.inline.hpp"
 #include "crc32c.h"
+#ifdef COMPILER2
+#include "opto/output.hpp"
+#endif
 
 #ifdef PRODUCT
 #define BLOCK_COMMENT(str) /* nothing */
 #define STOP(error) stop(error)
 #else
@@ -1638,10 +1643,14 @@
   pass_arg1(this, arg_1);
   pass_arg0(this, arg_0);
   call_VM_leaf(entry_point, 3);
 }
 
+void MacroAssembler::super_call_VM_leaf(address entry_point) {
+  MacroAssembler::call_VM_leaf_base(entry_point, 1);
+}
+
 void MacroAssembler::super_call_VM_leaf(address entry_point, Register arg_0) {
   pass_arg0(this, arg_0);
   MacroAssembler::call_VM_leaf_base(entry_point, 1);
 }
 
@@ -2606,10 +2615,104 @@
     // nothing to do, (later) access of M[reg + offset]
     // will provoke OS NULL exception if reg = NULL
   }
 }
 
+void MacroAssembler::test_klass_is_value(Register klass, Register temp_reg, Label& is_value) {
+  movl(temp_reg, Address(klass, Klass::access_flags_offset()));
+  testl(temp_reg, JVM_ACC_VALUE);
+  jcc(Assembler::notZero, is_value);
+}
+
+void MacroAssembler::test_klass_is_empty_value(Register klass, Register temp_reg, Label& is_empty_value) {
+#ifdef ASSERT
+  {
+    Label done_check;
+    test_klass_is_value(klass, temp_reg, done_check);
+    stop("test_klass_is_empty_value with non value klass");
+    bind(done_check);
+  }
+#endif
+  movl(temp_reg, Address(klass, InstanceKlass::misc_flags_offset()));
+  testl(temp_reg, InstanceKlass::misc_flags_is_empty_inline_type());
+  jcc(Assembler::notZero, is_empty_value);
+}
+
+void MacroAssembler::test_field_is_flattenable(Register flags, Register temp_reg, Label& is_flattenable) {
+  movl(temp_reg, flags);
+  shrl(temp_reg, ConstantPoolCacheEntry::is_flattenable_field_shift);
+  andl(temp_reg, 0x1);
+  testl(temp_reg, temp_reg);
+  jcc(Assembler::notZero, is_flattenable);
+}
+
+void MacroAssembler::test_field_is_not_flattenable(Register flags, Register temp_reg, Label& notFlattenable) {
+  movl(temp_reg, flags);
+  shrl(temp_reg, ConstantPoolCacheEntry::is_flattenable_field_shift);
+  andl(temp_reg, 0x1);
+  testl(temp_reg, temp_reg);
+  jcc(Assembler::zero, notFlattenable);
+}
+
+void MacroAssembler::test_field_is_flattened(Register flags, Register temp_reg, Label& is_flattened) {
+  movl(temp_reg, flags);
+  shrl(temp_reg, ConstantPoolCacheEntry::is_flattened_field_shift);
+  andl(temp_reg, 0x1);
+  testl(temp_reg, temp_reg);
+  jcc(Assembler::notZero, is_flattened);
+}
+
+void MacroAssembler::test_flattened_array_oop(Register oop, Register temp_reg,
+                                              Label&is_flattened_array) {
+  Register tmp_load_klass = LP64_ONLY(rscratch1) NOT_LP64(noreg);
+  load_klass(temp_reg, oop, tmp_load_klass);
+  movl(temp_reg, Address(temp_reg, Klass::layout_helper_offset()));
+  test_flattened_array_layout(temp_reg, is_flattened_array);
+}
+
+void MacroAssembler::test_non_flattened_array_oop(Register oop, Register temp_reg,
+                                                  Label&is_non_flattened_array) {
+  Register tmp_load_klass = LP64_ONLY(rscratch1) NOT_LP64(noreg);
+  load_klass(temp_reg, oop, tmp_load_klass);
+  movl(temp_reg, Address(temp_reg, Klass::layout_helper_offset()));
+  test_non_flattened_array_layout(temp_reg, is_non_flattened_array);
+}
+
+void MacroAssembler::test_null_free_array_oop(Register oop, Register temp_reg, Label&is_null_free_array) {
+  Register tmp_load_klass = LP64_ONLY(rscratch1) NOT_LP64(noreg);
+  load_klass(temp_reg, oop, tmp_load_klass);
+  movl(temp_reg, Address(temp_reg, Klass::layout_helper_offset()));
+  test_null_free_array_layout(temp_reg, is_null_free_array);
+}
+
+void MacroAssembler::test_non_null_free_array_oop(Register oop, Register temp_reg, Label&is_non_null_free_array) {
+  Register tmp_load_klass = LP64_ONLY(rscratch1) NOT_LP64(noreg);
+  load_klass(temp_reg, oop, tmp_load_klass);
+  movl(temp_reg, Address(temp_reg, Klass::layout_helper_offset()));
+  test_non_null_free_array_layout(temp_reg, is_non_null_free_array);
+}
+
+void MacroAssembler::test_flattened_array_layout(Register lh, Label& is_flattened_array) {
+  testl(lh, Klass::_lh_array_tag_vt_value_bit_inplace);
+  jcc(Assembler::notZero, is_flattened_array);
+}
+void MacroAssembler::test_non_flattened_array_layout(Register lh, Label& is_non_flattened_array) {
+  testl(lh, Klass::_lh_array_tag_vt_value_bit_inplace);
+  jcc(Assembler::zero, is_non_flattened_array);
+}
+
+void MacroAssembler::test_null_free_array_layout(Register lh, Label& is_null_free_array) {
+  testl(lh, Klass::_lh_null_free_bit_inplace);
+  jcc(Assembler::notZero, is_null_free_array);
+}
+
+void MacroAssembler::test_non_null_free_array_layout(Register lh, Label& is_non_null_free_array) {
+  testl(lh, Klass::_lh_null_free_bit_inplace);
+  jcc(Assembler::zero, is_non_null_free_array);
+}
+
+
 void MacroAssembler::os_breakpoint() {
   // instead of directly emitting a breakpoint, call os:breakpoint for better debugability
   // (e.g., MSVC can't call ps() otherwise)
   call(RuntimeAddress(CAST_FROM_FN_PTR(address, os::breakpoint)));
 }
@@ -3304,10 +3407,139 @@
 
 void MacroAssembler::testptr(Register dst, Register src) {
   LP64_ONLY(testq(dst, src)) NOT_LP64(testl(dst, src));
 }
 
+// Object / value buffer allocation...
+//
+// Kills klass and rsi on LP64
+void MacroAssembler::allocate_instance(Register klass, Register new_obj,
+                                       Register t1, Register t2,
+                                       bool clear_fields, Label& alloc_failed)
+{
+  Label done, initialize_header, initialize_object, slow_case, slow_case_no_pop;
+  Register layout_size = t1;
+  assert(new_obj == rax, "needs to be rax, according to barrier asm eden_allocate");
+  assert_different_registers(klass, new_obj, t1, t2);
+
+#ifdef ASSERT
+  {
+    Label L;
+    cmpb(Address(klass, InstanceKlass::init_state_offset()), InstanceKlass::fully_initialized);
+    jcc(Assembler::equal, L);
+    stop("klass not initialized");
+    bind(L);
+  }
+#endif
+
+  // get instance_size in InstanceKlass (scaled to a count of bytes)
+  movl(layout_size, Address(klass, Klass::layout_helper_offset()));
+  // test to see if it has a finalizer or is malformed in some way
+  testl(layout_size, Klass::_lh_instance_slow_path_bit);
+  jcc(Assembler::notZero, slow_case_no_pop);
+
+  // Allocate the instance:
+  //  If TLAB is enabled:
+  //    Try to allocate in the TLAB.
+  //    If fails, go to the slow path.
+  //  Else If inline contiguous allocations are enabled:
+  //    Try to allocate in eden.
+  //    If fails due to heap end, go to slow path.
+  //
+  //  If TLAB is enabled OR inline contiguous is enabled:
+  //    Initialize the allocation.
+  //    Exit.
+  //
+  //  Go to slow path.
+  const bool allow_shared_alloc =
+    Universe::heap()->supports_inline_contig_alloc();
+
+  push(klass);
+  const Register thread = LP64_ONLY(r15_thread) NOT_LP64(klass);
+#ifndef _LP64
+  if (UseTLAB || allow_shared_alloc) {
+    get_thread(thread);
+  }
+#endif // _LP64
+
+  if (UseTLAB) {
+    tlab_allocate(thread, new_obj, layout_size, 0, klass, t2, slow_case);
+    if (ZeroTLAB || (!clear_fields)) {
+      // the fields have been already cleared
+      jmp(initialize_header);
+    } else {
+      // initialize both the header and fields
+      jmp(initialize_object);
+    }
+  } else {
+    // Allocation in the shared Eden, if allowed.
+    //
+    eden_allocate(thread, new_obj, layout_size, 0, t2, slow_case);
+  }
+
+  // If UseTLAB or allow_shared_alloc are true, the object is created above and
+  // there is an initialize need. Otherwise, skip and go to the slow path.
+  if (UseTLAB || allow_shared_alloc) {
+    if (clear_fields) {
+      // The object is initialized before the header.  If the object size is
+      // zero, go directly to the header initialization.
+      bind(initialize_object);
+      decrement(layout_size, sizeof(oopDesc));
+      jcc(Assembler::zero, initialize_header);
+
+      // Initialize topmost object field, divide size by 8, check if odd and
+      // test if zero.
+      Register zero = klass;
+      xorl(zero, zero);    // use zero reg to clear memory (shorter code)
+      shrl(layout_size, LogBytesPerLong); // divide by 2*oopSize and set carry flag if odd
+
+  #ifdef ASSERT
+      // make sure instance_size was multiple of 8
+      Label L;
+      // Ignore partial flag stall after shrl() since it is debug VM
+      jcc(Assembler::carryClear, L);
+      stop("object size is not multiple of 2 - adjust this code");
+      bind(L);
+      // must be > 0, no extra check needed here
+  #endif
+
+      // initialize remaining object fields: instance_size was a multiple of 8
+      {
+        Label loop;
+        bind(loop);
+        movptr(Address(new_obj, layout_size, Address::times_8, sizeof(oopDesc) - 1*oopSize), zero);
+        NOT_LP64(movptr(Address(new_obj, layout_size, Address::times_8, sizeof(oopDesc) - 2*oopSize), zero));
+        decrement(layout_size);
+        jcc(Assembler::notZero, loop);
+      }
+    } // clear_fields
+
+    // initialize object header only.
+    bind(initialize_header);
+    pop(klass);
+    Register mark_word = t2;
+    movptr(mark_word, Address(klass, Klass::prototype_header_offset()));
+    movptr(Address(new_obj, oopDesc::mark_offset_in_bytes ()), mark_word);
+#ifdef _LP64
+    xorl(rsi, rsi);                 // use zero reg to clear memory (shorter code)
+    store_klass_gap(new_obj, rsi);  // zero klass gap for compressed oops
+#endif
+    movptr(t2, klass);         // preserve klass
+    Register tmp_store_klass = LP64_ONLY(rscratch1) NOT_LP64(noreg);
+    store_klass(new_obj, t2, tmp_store_klass);  // src klass reg is potentially compressed
+
+    jmp(done);
+  }
+
+  bind(slow_case);
+  pop(klass);
+  bind(slow_case_no_pop);
+  jmp(alloc_failed);
+
+  bind(done);
+}
+
 // Defines obj, preserves var_size_in_bytes, okay for t2 == var_size_in_bytes.
 void MacroAssembler::tlab_allocate(Register thread, Register obj,
                                    Register var_size_in_bytes,
                                    int con_size_in_bytes,
                                    Register t1,
@@ -3381,10 +3613,60 @@
   }
 
   bind(done);
 }
 
+void MacroAssembler::get_value_field_klass(Register klass, Register index, Register value_klass) {
+  movptr(value_klass, Address(klass, InstanceKlass::value_field_klasses_offset()));
+#ifdef ASSERT
+  {
+    Label done;
+    cmpptr(value_klass, 0);
+    jcc(Assembler::notEqual, done);
+    stop("get_value_field_klass contains no inline klasses");
+    bind(done);
+  }
+#endif
+  movptr(value_klass, Address(value_klass, index, Address::times_ptr));
+}
+
+void MacroAssembler::get_default_value_oop(Register value_klass, Register temp_reg, Register obj) {
+#ifdef ASSERT
+  {
+    Label done_check;
+    test_klass_is_value(value_klass, temp_reg, done_check);
+    stop("get_default_value_oop from non-value klass");
+    bind(done_check);
+  }
+#endif
+  Register offset = temp_reg;
+  // Getting the offset of the pre-allocated default value
+  movptr(offset, Address(value_klass, in_bytes(InstanceKlass::adr_valueklass_fixed_block_offset())));
+  movl(offset, Address(offset, in_bytes(ValueKlass::default_value_offset_offset())));
+
+  // Getting the mirror
+  movptr(obj, Address(value_klass, in_bytes(Klass::java_mirror_offset())));
+  resolve_oop_handle(obj, value_klass);
+
+  // Getting the pre-allocated default value from the mirror
+  Address field(obj, offset, Address::times_1);
+  load_heap_oop(obj, field);
+}
+
+void MacroAssembler::get_empty_value_oop(Register value_klass, Register temp_reg, Register obj) {
+#ifdef ASSERT
+  {
+    Label done_check;
+    test_klass_is_empty_value(value_klass, temp_reg, done_check);
+    stop("get_empty_value from non-empty value klass");
+    bind(done_check);
+  }
+#endif
+  get_default_value_oop(value_klass, temp_reg, obj);
+}
+
+
 // Look up the method for a megamorphic invokeinterface call.
 // The target method is determined by <intf_klass, itable_index>.
 // The receiver klass is in recv_klass.
 // On success, the result will be in method_result, and execution falls through.
 // On failure, execution transfers to the given label.
@@ -3729,11 +4011,15 @@
     bind(L);
   }
 }
 
 void MacroAssembler::_verify_oop(Register reg, const char* s, const char* file, int line) {
-  if (!VerifyOops) return;
+  if (!VerifyOops || VerifyAdapterSharing) {
+    // Below address of the code string confuses VerifyAdapterSharing
+    // because it may differ between otherwise equivalent adapters.
+    return;
+  }
 
   // Pass register number to verify_oop_subroutine
   const char* b = NULL;
   {
     ResourceMark rm;
@@ -3827,11 +4113,15 @@
   return Address(rsp, scale_reg, scale_factor, offset);
 }
 
 
 void MacroAssembler::_verify_oop_addr(Address addr, const char* s, const char* file, int line) {
-  if (!VerifyOops) return;
+  if (!VerifyOops || VerifyAdapterSharing) {
+    // Below address of the code string confuses VerifyAdapterSharing
+    // because it may differ between otherwise equivalent adapters.
+    return;
+  }
 
   // Address adjust(addr.base(), addr.index(), addr.scale(), addr.disp() + BytesPerWord);
   // Pass register number to verify_oop_subroutine
   const char* b = NULL;
   {
@@ -4323,20 +4613,28 @@
   movptr(holder, Address(method, Method::const_offset()));                      // ConstMethod*
   movptr(holder, Address(holder, ConstMethod::constants_offset()));             // ConstantPool*
   movptr(holder, Address(holder, ConstantPool::pool_holder_offset_in_bytes())); // InstanceKlass*
 }
 
+void MacroAssembler::load_metadata(Register dst, Register src) {
+  if (UseCompressedClassPointers) {
+    movl(dst, Address(src, oopDesc::klass_offset_in_bytes()));
+  } else {
+    movptr(dst, Address(src, oopDesc::klass_offset_in_bytes()));
+  }
+}
+
 void MacroAssembler::load_klass(Register dst, Register src, Register tmp) {
   assert_different_registers(src, tmp);
   assert_different_registers(dst, tmp);
 #ifdef _LP64
   if (UseCompressedClassPointers) {
     movl(dst, Address(src, oopDesc::klass_offset_in_bytes()));
     decode_klass_not_null(dst, tmp);
   } else
 #endif
-    movptr(dst, Address(src, oopDesc::klass_offset_in_bytes()));
+  movptr(dst, Address(src, oopDesc::klass_offset_in_bytes()));
 }
 
 void MacroAssembler::load_prototype_header(Register dst, Register src, Register tmp) {
   load_klass(dst, src, tmp);
   movptr(dst, Address(dst, Klass::prototype_header_offset()));
@@ -4365,21 +4663,61 @@
     bs->load_at(this, decorators, type, dst, src, tmp1, thread_tmp);
   }
 }
 
 void MacroAssembler::access_store_at(BasicType type, DecoratorSet decorators, Address dst, Register src,
-                                     Register tmp1, Register tmp2) {
+                                     Register tmp1, Register tmp2, Register tmp3) {
   BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();
   decorators = AccessInternal::decorator_fixup(decorators);
   bool as_raw = (decorators & AS_RAW) != 0;
   if (as_raw) {
-    bs->BarrierSetAssembler::store_at(this, decorators, type, dst, src, tmp1, tmp2);
+    bs->BarrierSetAssembler::store_at(this, decorators, type, dst, src, tmp1, tmp2, tmp3);
+  } else {
+    bs->store_at(this, decorators, type, dst, src, tmp1, tmp2, tmp3);
+  }
+}
+
+void MacroAssembler::access_value_copy(DecoratorSet decorators, Register src, Register dst,
+                                       Register value_klass) {
+  BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();
+  bs->value_copy(this, decorators, src, dst, value_klass);
+}
+
+void MacroAssembler::first_field_offset(Register value_klass, Register offset) {
+  movptr(offset, Address(value_klass, InstanceKlass::adr_valueklass_fixed_block_offset()));
+  movl(offset, Address(offset, ValueKlass::first_field_offset_offset()));
+}
+
+void MacroAssembler::data_for_oop(Register oop, Register data, Register value_klass) {
+  // ((address) (void*) o) + vk->first_field_offset();
+  Register offset = (data == oop) ? rscratch1 : data;
+  first_field_offset(value_klass, offset);
+  if (data == oop) {
+    addptr(data, offset);
   } else {
-    bs->store_at(this, decorators, type, dst, src, tmp1, tmp2);
+    lea(data, Address(oop, offset));
   }
 }
 
+void MacroAssembler::data_for_value_array_index(Register array, Register array_klass,
+                                                Register index, Register data) {
+  assert(index != rcx, "index needs to shift by rcx");
+  assert_different_registers(array, array_klass, index);
+  assert_different_registers(rcx, array, index);
+
+  // array->base() + (index << Klass::layout_helper_log2_element_size(lh));
+  movl(rcx, Address(array_klass, Klass::layout_helper_offset()));
+
+  // Klass::layout_helper_log2_element_size(lh)
+  // (lh >> _lh_log2_element_size_shift) & _lh_log2_element_size_mask;
+  shrl(rcx, Klass::_lh_log2_element_size_shift);
+  andl(rcx, Klass::_lh_log2_element_size_mask);
+  shlptr(index); // index << rcx
+
+  lea(data, Address(array, index, Address::times_1, arrayOopDesc::base_offset_in_bytes(T_VALUETYPE)));
+}
+
 void MacroAssembler::resolve(DecoratorSet decorators, Register obj) {
   // Use stronger ACCESS_WRITE|ACCESS_READ by default.
   if ((decorators & (ACCESS_READ | ACCESS_WRITE)) == 0) {
     decorators |= ACCESS_READ | ACCESS_WRITE;
   }
@@ -4397,17 +4735,17 @@
                                             Register thread_tmp, DecoratorSet decorators) {
   access_load_at(T_OBJECT, IN_HEAP | IS_NOT_NULL | decorators, dst, src, tmp1, thread_tmp);
 }
 
 void MacroAssembler::store_heap_oop(Address dst, Register src, Register tmp1,
-                                    Register tmp2, DecoratorSet decorators) {
-  access_store_at(T_OBJECT, IN_HEAP | decorators, dst, src, tmp1, tmp2);
+                                    Register tmp2, Register tmp3, DecoratorSet decorators) {
+  access_store_at(T_OBJECT, IN_HEAP | decorators, dst, src, tmp1, tmp2, tmp3);
 }
 
 // Used for storing NULLs.
 void MacroAssembler::store_heap_oop_null(Address dst) {
-  access_store_at(T_OBJECT, IN_HEAP, dst, noreg, noreg, noreg);
+  access_store_at(T_OBJECT, IN_HEAP, dst, noreg, noreg, noreg, noreg);
 }
 
 #ifdef _LP64
 void MacroAssembler::store_klass_gap(Register dst, Register src) {
   if (UseCompressedClassPointers) {
@@ -4718,11 +5056,15 @@
 }
 
 #endif // _LP64
 
 // C2 compiled method's prolog code.
-void MacroAssembler::verified_entry(int framesize, int stack_bang_size, bool fp_mode_24b, bool is_stub) {
+void MacroAssembler::verified_entry(Compile* C, int sp_inc) {
+  int framesize = C->output()->frame_size_in_bytes();
+  int bangsize = C->output()->bang_size_in_bytes();
+  bool fp_mode_24b = false;
+  int stack_bang_size = C->output()->need_stack_bang(bangsize) ? bangsize : 0;
 
   // WARNING: Initial instruction MUST be 5 bytes or longer so that
   // NativeJump::patch_verified_entry will be able to patch out the entry
   // code safely. The push to verify stack depth is ok at 5 bytes,
   // the frame allocation can be either 3 or 6 bytes. So if we don't do
@@ -4771,10 +5113,16 @@
         addptr(rbp, framesize);
       }
     }
   }
 
+  if (C->needs_stack_repair()) {
+    // Save stack increment (also account for fixed framesize and rbp)
+    assert((sp_inc & (StackAlignmentInBytes-1)) == 0, "stack increment not aligned");
+    movptr(Address(rsp, C->output()->sp_inc_offset()), sp_inc + framesize + wordSize);
+  }
+
   if (VerifyStackAtCalls) { // Majik cookie to verify stack depth
     framesize -= wordSize;
     movptr(Address(rsp, framesize), (int32_t)0xbadb100d);
   }
 
@@ -4799,26 +5147,23 @@
     jcc(Assembler::equal, L);
     STOP("Stack is not properly aligned!");
     bind(L);
   }
 #endif
-
-  if (!is_stub) {
-    BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();
-    bs->nmethod_entry_barrier(this);
-  }
 }
 
 // clear memory of size 'cnt' qwords, starting at 'base' using XMM/YMM registers
-void MacroAssembler::xmm_clear_mem(Register base, Register cnt, XMMRegister xtmp) {
+void MacroAssembler::xmm_clear_mem(Register base, Register cnt, Register val, XMMRegister xtmp) {
   // cnt - number of qwords (8-byte words).
   // base - start address, qword aligned.
   Label L_zero_64_bytes, L_loop, L_sloop, L_tail, L_end;
+  movdq(xtmp, val);
   if (UseAVX >= 2) {
-    vpxor(xtmp, xtmp, xtmp, AVX_256bit);
+    punpcklqdq(xtmp, xtmp);
+    vinserti128_high(xtmp, xtmp);
   } else {
-    pxor(xtmp, xtmp);
+    punpcklqdq(xtmp, xtmp);
   }
   jmp(L_zero_64_bytes);
 
   BIND(L_loop);
   if (UseAVX >= 2) {
@@ -4858,26 +5203,381 @@
   decrement(cnt);
   jccb(Assembler::greaterEqual, L_sloop);
   BIND(L_end);
 }
 
-void MacroAssembler::clear_mem(Register base, Register cnt, Register tmp, XMMRegister xtmp, bool is_large) {
+int MacroAssembler::store_value_type_fields_to_buf(ciValueKlass* vk, bool from_interpreter) {
+  // A value type might be returned. If fields are in registers we
+  // need to allocate a value type instance and initialize it with
+  // the value of the fields.
+  Label skip;
+  // We only need a new buffered value if a new one is not returned
+  testptr(rax, 1);
+  jcc(Assembler::zero, skip);
+  int call_offset = -1;
+
+#ifdef _LP64
+  Label slow_case;
+
+  // Try to allocate a new buffered value (from the heap)
+  if (UseTLAB) {
+    // FIXME -- for smaller code, the inline allocation (and the slow case) should be moved inside the pack handler.
+    if (vk != NULL) {
+      // Called from C1, where the return type is statically known.
+      movptr(rbx, (intptr_t)vk->get_ValueKlass());
+      jint lh = vk->layout_helper();
+      assert(lh != Klass::_lh_neutral_value, "inline class in return type must have been resolved");
+      movl(r14, lh);
+    } else {
+      // Call from interpreter. RAX contains ((the ValueKlass* of the return type) | 0x01)
+      mov(rbx, rax);
+      andptr(rbx, -2);
+      movl(r14, Address(rbx, Klass::layout_helper_offset()));
+    }
+
+    movptr(r13, Address(r15_thread, in_bytes(JavaThread::tlab_top_offset())));
+    lea(r14, Address(r13, r14, Address::times_1));
+    cmpptr(r14, Address(r15_thread, in_bytes(JavaThread::tlab_end_offset())));
+    jcc(Assembler::above, slow_case);
+    movptr(Address(r15_thread, in_bytes(JavaThread::tlab_top_offset())), r14);
+    movptr(Address(r13, oopDesc::mark_offset_in_bytes()), (intptr_t)markWord::always_locked_prototype().value());
+
+    xorl(rax, rax); // use zero reg to clear memory (shorter code)
+    store_klass_gap(r13, rax);  // zero klass gap for compressed oops
+
+    if (vk == NULL) {
+      // store_klass corrupts rbx, so save it in rax for later use (interpreter case only).
+      mov(rax, rbx);
+    }
+    Register tmp_store_klass = LP64_ONLY(rscratch1) NOT_LP64(noreg);
+    store_klass(r13, rbx, tmp_store_klass);  // klass
+
+    // We have our new buffered value, initialize its fields with a
+    // value class specific handler
+    if (vk != NULL) {
+      // FIXME -- do the packing in-line to avoid the runtime call
+      mov(rax, r13);
+      call(RuntimeAddress(vk->pack_handler())); // no need for call info as this will not safepoint.
+    } else {
+      movptr(rbx, Address(rax, InstanceKlass::adr_valueklass_fixed_block_offset()));
+      movptr(rbx, Address(rbx, ValueKlass::pack_handler_offset()));
+      mov(rax, r13);
+      call(rbx);
+    }
+    jmp(skip);
+  }
+
+  bind(slow_case);
+  // We failed to allocate a new value, fall back to a runtime
+  // call. Some oop field may be live in some registers but we can't
+  // tell. That runtime call will take care of preserving them
+  // across a GC if there's one.
+#endif
+
+  if (from_interpreter) {
+    super_call_VM_leaf(StubRoutines::store_value_type_fields_to_buf());
+  } else {
+    call(RuntimeAddress(StubRoutines::store_value_type_fields_to_buf()));
+    call_offset = offset();
+  }
+
+  bind(skip);
+  return call_offset;
+}
+
+
+// Move a value between registers/stack slots and update the reg_state
+bool MacroAssembler::move_helper(VMReg from, VMReg to, BasicType bt, RegState reg_state[], int ret_off, int extra_stack_offset) {
+  if (reg_state[to->value()] == reg_written) {
+    return true; // Already written
+  }
+  if (from != to && bt != T_VOID) {
+    if (reg_state[to->value()] == reg_readonly) {
+      return false; // Not yet writable
+    }
+    if (from->is_reg()) {
+      if (to->is_reg()) {
+        if (from->is_XMMRegister()) {
+          if (bt == T_DOUBLE) {
+            movdbl(to->as_XMMRegister(), from->as_XMMRegister());
+          } else {
+            assert(bt == T_FLOAT, "must be float");
+            movflt(to->as_XMMRegister(), from->as_XMMRegister());
+          }
+        } else {
+          movq(to->as_Register(), from->as_Register());
+        }
+      } else {
+        int st_off = to->reg2stack() * VMRegImpl::stack_slot_size + extra_stack_offset;
+        assert(st_off != ret_off, "overwriting return address at %d", st_off);
+        Address to_addr = Address(rsp, st_off);
+        if (from->is_XMMRegister()) {
+          if (bt == T_DOUBLE) {
+            movdbl(to_addr, from->as_XMMRegister());
+          } else {
+            assert(bt == T_FLOAT, "must be float");
+            movflt(to_addr, from->as_XMMRegister());
+          }
+        } else {
+          movq(to_addr, from->as_Register());
+        }
+      }
+    } else {
+      Address from_addr = Address(rsp, from->reg2stack() * VMRegImpl::stack_slot_size + extra_stack_offset);
+      if (to->is_reg()) {
+        if (to->is_XMMRegister()) {
+          if (bt == T_DOUBLE) {
+            movdbl(to->as_XMMRegister(), from_addr);
+          } else {
+            assert(bt == T_FLOAT, "must be float");
+            movflt(to->as_XMMRegister(), from_addr);
+          }
+        } else {
+          movq(to->as_Register(), from_addr);
+        }
+      } else {
+        int st_off = to->reg2stack() * VMRegImpl::stack_slot_size + extra_stack_offset;
+        assert(st_off != ret_off, "overwriting return address at %d", st_off);
+        movq(r13, from_addr);
+        movq(Address(rsp, st_off), r13);
+      }
+    }
+  }
+  // Update register states
+  reg_state[from->value()] = reg_writable;
+  reg_state[to->value()] = reg_written;
+  return true;
+}
+
+// Read all fields from a value type oop and store the values in registers/stack slots
+bool MacroAssembler::unpack_value_helper(const GrowableArray<SigEntry>* sig, int& sig_index, VMReg from, VMRegPair* regs_to,
+                                         int& to_index, RegState reg_state[], int ret_off, int extra_stack_offset) {
+  Register fromReg = from->is_reg() ? from->as_Register() : noreg;
+  assert(sig->at(sig_index)._bt == T_VOID, "should be at end delimiter");
+
+  int vt = 1;
+  bool done = true;
+  bool mark_done = true;
+  do {
+    sig_index--;
+    BasicType bt = sig->at(sig_index)._bt;
+    if (bt == T_VALUETYPE) {
+      vt--;
+    } else if (bt == T_VOID &&
+               sig->at(sig_index-1)._bt != T_LONG &&
+               sig->at(sig_index-1)._bt != T_DOUBLE) {
+      vt++;
+    } else if (SigEntry::is_reserved_entry(sig, sig_index)) {
+      to_index--; // Ignore this
+    } else {
+      assert(to_index >= 0, "invalid to_index");
+      VMRegPair pair_to = regs_to[to_index--];
+      VMReg to = pair_to.first();
+
+      if (bt == T_VOID) continue;
+
+      int idx = (int)to->value();
+      if (reg_state[idx] == reg_readonly) {
+         if (idx != from->value()) {
+           mark_done = false;
+         }
+         done = false;
+         continue;
+      } else if (reg_state[idx] == reg_written) {
+        continue;
+      } else {
+        assert(reg_state[idx] == reg_writable, "must be writable");
+        reg_state[idx] = reg_written;
+       }
+
+      if (fromReg == noreg) {
+        int st_off = from->reg2stack() * VMRegImpl::stack_slot_size + extra_stack_offset;
+        movq(r10, Address(rsp, st_off));
+        fromReg = r10;
+      }
+
+      int off = sig->at(sig_index)._offset;
+      assert(off > 0, "offset in object should be positive");
+      bool is_oop = (bt == T_OBJECT || bt == T_ARRAY);
+
+      Address fromAddr = Address(fromReg, off);
+      bool is_signed = (bt != T_CHAR) && (bt != T_BOOLEAN);
+      if (!to->is_XMMRegister()) {
+        Register dst = to->is_stack() ? r13 : to->as_Register();
+        if (is_oop) {
+          load_heap_oop(dst, fromAddr);
+        } else {
+          load_sized_value(dst, fromAddr, type2aelembytes(bt), is_signed);
+        }
+        if (to->is_stack()) {
+          int st_off = to->reg2stack() * VMRegImpl::stack_slot_size + extra_stack_offset;
+          assert(st_off != ret_off, "overwriting return address at %d", st_off);
+          movq(Address(rsp, st_off), dst);
+        }
+      } else {
+        if (bt == T_DOUBLE) {
+          movdbl(to->as_XMMRegister(), fromAddr);
+        } else {
+          assert(bt == T_FLOAT, "must be float");
+          movflt(to->as_XMMRegister(), fromAddr);
+        }
+      }
+    }
+  } while (vt != 0);
+  if (mark_done && reg_state[from->value()] != reg_written) {
+    // This is okay because no one else will write to that slot
+    reg_state[from->value()] = reg_writable;
+  }
+  return done;
+}
+
+// Pack fields back into a value type oop
+bool MacroAssembler::pack_value_helper(const GrowableArray<SigEntry>* sig, int& sig_index, int vtarg_index,
+                                       VMReg to, VMRegPair* regs_from, int regs_from_count, int& from_index, RegState reg_state[],
+                                       int ret_off, int extra_stack_offset) {
+  assert(sig->at(sig_index)._bt == T_VALUETYPE, "should be at end delimiter");
+  assert(to->is_valid(), "must be");
+
+  if (reg_state[to->value()] == reg_written) {
+    skip_unpacked_fields(sig, sig_index, regs_from, regs_from_count, from_index);
+    return true; // Already written
+  }
+
+  Register val_array = rax;
+  Register val_obj_tmp = r11;
+  Register from_reg_tmp = r14; // Be careful with r14 because it's used for spilling
+  Register tmp1 = r10;
+  Register tmp2 = r13;
+  Register tmp3 = rbx;
+  Register val_obj = to->is_stack() ? val_obj_tmp : to->as_Register();
+
+  if (reg_state[to->value()] == reg_readonly) {
+    if (!is_reg_in_unpacked_fields(sig, sig_index, to, regs_from, regs_from_count, from_index)) {
+      skip_unpacked_fields(sig, sig_index, regs_from, regs_from_count, from_index);
+      return false; // Not yet writable
+    }
+    val_obj = val_obj_tmp;
+  }
+
+  int index = arrayOopDesc::base_offset_in_bytes(T_OBJECT) + vtarg_index * type2aelembytes(T_VALUETYPE);
+  load_heap_oop(val_obj, Address(val_array, index));
+
+  ScalarizedValueArgsStream stream(sig, sig_index, regs_from, regs_from_count, from_index);
+  VMRegPair from_pair;
+  BasicType bt;
+  while (stream.next(from_pair, bt)) {
+    int off = sig->at(stream.sig_cc_index())._offset;
+    assert(off > 0, "offset in object should be positive");
+    bool is_oop = (bt == T_OBJECT || bt == T_ARRAY);
+    size_t size_in_bytes = is_java_primitive(bt) ? type2aelembytes(bt) : wordSize;
+
+    VMReg from_r1 = from_pair.first();
+    VMReg from_r2 = from_pair.second();
+
+    // Pack the scalarized field into the value object.
+    Address dst(val_obj, off);
+    if (!from_r1->is_XMMRegister()) {
+      Register from_reg;
+      if (from_r1->is_stack()) {
+        from_reg = from_reg_tmp;
+        int ld_off = from_r1->reg2stack() * VMRegImpl::stack_slot_size + extra_stack_offset;
+        load_sized_value(from_reg, Address(rsp, ld_off), size_in_bytes, /* is_signed */ false);
+      } else {
+        from_reg = from_r1->as_Register();
+      }
+      assert_different_registers(dst.base(), from_reg, tmp1, tmp2, tmp3, val_array);
+      if (is_oop) {
+        store_heap_oop(dst, from_reg, tmp1, tmp2, tmp3, IN_HEAP | ACCESS_WRITE | IS_DEST_UNINITIALIZED);
+      } else {
+        store_sized_value(dst, from_reg, size_in_bytes);
+      }
+    } else {
+      if (from_r2->is_valid()) {
+        movdbl(dst, from_r1->as_XMMRegister());
+      } else {
+        movflt(dst, from_r1->as_XMMRegister());
+      }
+    }
+    reg_state[from_r1->value()] = reg_writable;
+  }
+  sig_index = stream.sig_cc_index();
+  from_index = stream.regs_cc_index();
+
+  assert(reg_state[to->value()] == reg_writable, "must have already been read");
+  bool success = move_helper(val_obj->as_VMReg(), to, T_OBJECT, reg_state, ret_off, extra_stack_offset);
+  assert(success, "to register must be writeable");
+
+  return true;
+}
+
+// Unpack all value type arguments passed as oops
+void MacroAssembler::unpack_value_args(Compile* C, bool receiver_only) {
+  int sp_inc = unpack_value_args_common(C, receiver_only);
+  // Emit code for verified entry and save increment for stack repair on return
+  verified_entry(C, sp_inc);
+}
+
+void MacroAssembler::shuffle_value_args(bool is_packing, bool receiver_only, int extra_stack_offset,
+                                        BasicType* sig_bt, const GrowableArray<SigEntry>* sig_cc,
+                                        int args_passed, int args_on_stack, VMRegPair* regs,
+                                        int args_passed_to, int args_on_stack_to, VMRegPair* regs_to, int sp_inc) {
+  // Check if we need to extend the stack for packing/unpacking
+  if (sp_inc > 0 && !is_packing) {
+    // Save the return address, adjust the stack (make sure it is properly
+    // 16-byte aligned) and copy the return address to the new top of the stack.
+    // (Note: C1 does this in C1_MacroAssembler::scalarized_entry).
+    pop(r13);
+    subptr(rsp, sp_inc);
+    push(r13);
+  }
+
+  int ret_off; // make sure we don't overwrite the return address
+  if (is_packing) {
+    // For C1 code, the VVEP doesn't have reserved slots, so we store the returned address at
+    // rsp[0] during shuffling.
+    ret_off = 0;
+  } else {
+    // C2 code ensures that sp_inc is a reserved slot.
+    ret_off = sp_inc;
+  }
+
+  shuffle_value_args_common(is_packing, receiver_only, extra_stack_offset,
+                            sig_bt, sig_cc,
+                            args_passed, args_on_stack, regs,
+                            args_passed_to, args_on_stack_to, regs_to,
+                            sp_inc, ret_off);
+}
+
+VMReg MacroAssembler::spill_reg_for(VMReg reg) {
+  return reg->is_XMMRegister() ? xmm8->as_VMReg() : r14->as_VMReg();
+}
+
+void MacroAssembler::remove_frame(int initial_framesize, bool needs_stack_repair, int sp_inc_offset) {
+  assert((initial_framesize & (StackAlignmentInBytes-1)) == 0, "frame size not aligned");
+  if (needs_stack_repair) {
+    movq(rbp, Address(rsp, initial_framesize));
+    addq(rsp, Address(rsp, sp_inc_offset));
+  } else {
+    if (initial_framesize > 0) {
+      addq(rsp, initial_framesize);
+    }
+    pop(rbp);
+  }
+}
+
+void MacroAssembler::clear_mem(Register base, Register cnt, Register val, XMMRegister xtmp, bool is_large, bool word_copy_only) {
   // cnt - number of qwords (8-byte words).
   // base - start address, qword aligned.
   // is_large - if optimizers know cnt is larger than InitArrayShortSize
   assert(base==rdi, "base register must be edi for rep stos");
-  assert(tmp==rax,   "tmp register must be eax for rep stos");
+  assert(val==rax,   "tmp register must be eax for rep stos");
   assert(cnt==rcx,   "cnt register must be ecx for rep stos");
   assert(InitArrayShortSize % BytesPerLong == 0,
     "InitArrayShortSize should be the multiple of BytesPerLong");
 
   Label DONE;
 
-  if (!is_large || !UseXMMForObjInit) {
-    xorptr(tmp, tmp);
-  }
-
   if (!is_large) {
     Label LOOP, LONG;
     cmpptr(cnt, InitArrayShortSize/BytesPerLong);
     jccb(Assembler::greater, LONG);
 
@@ -4886,25 +5586,24 @@
     decrement(cnt);
     jccb(Assembler::negative, DONE); // Zero length
 
     // Use individual pointer-sized stores for small counts:
     BIND(LOOP);
-    movptr(Address(base, cnt, Address::times_ptr), tmp);
+    movptr(Address(base, cnt, Address::times_ptr), val);
     decrement(cnt);
     jccb(Assembler::greaterEqual, LOOP);
     jmpb(DONE);
 
     BIND(LONG);
   }
 
   // Use longer rep-prefixed ops for non-small counts:
-  if (UseFastStosb) {
+  if (UseFastStosb && !word_copy_only) {
     shlptr(cnt, 3); // convert to number of bytes
     rep_stosb();
   } else if (UseXMMForObjInit) {
-    movptr(tmp, base);
-    xmm_clear_mem(tmp, cnt, xtmp);
+    xmm_clear_mem(base, cnt, val, xtmp);
   } else {
     NOT_LP64(shlptr(cnt, 1);) // convert to number of 32-bit words for 32-bit VM
     rep_stos();
   }
 
diff a/src/hotspot/cpu/x86/methodHandles_x86.cpp b/src/hotspot/cpu/x86/methodHandles_x86.cpp
--- a/src/hotspot/cpu/x86/methodHandles_x86.cpp
+++ b/src/hotspot/cpu/x86/methodHandles_x86.cpp
@@ -146,11 +146,15 @@
     __ jccb(Assembler::zero, run_compiled_code);
     __ jmp(Address(method, Method::interpreter_entry_offset()));
     __ BIND(run_compiled_code);
   }
 
-  const ByteSize entry_offset = for_compiler_entry ? Method::from_compiled_offset() :
+  // The following jump might pass a value type argument that was erased to Object as oop to a
+  // callee that expects value type arguments to be passed as fields. We need to call the compiled
+  // value entry (_code->value_entry_point() or _adapter->c2i_value_entry()) which will take care
+  // of translating between the calling conventions.
+  const ByteSize entry_offset = for_compiler_entry ? Method::from_compiled_value_offset() :
                                                      Method::from_interpreted_offset();
   __ jmp(Address(method, entry_offset));
 
   __ bind(L_no_such_method);
   __ jump(RuntimeAddress(StubRoutines::throw_AbstractMethodError_entry()));
diff a/src/hotspot/cpu/x86/stubGenerator_x86_64.cpp b/src/hotspot/cpu/x86/stubGenerator_x86_64.cpp
--- a/src/hotspot/cpu/x86/stubGenerator_x86_64.cpp
+++ b/src/hotspot/cpu/x86/stubGenerator_x86_64.cpp
@@ -1,7 +1,7 @@
 /*
- * Copyright (c) 2003, 2019, Oracle and/or its affiliates. All rights reserved.
+ * Copyright (c) 2003, 2020, Oracle and/or its affiliates. All rights reserved.
  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  *
  * This code is free software; you can redistribute it and/or modify it
  * under the terms of the GNU General Public License version 2 only, as
  * published by the Free Software Foundation.
@@ -334,25 +334,27 @@
 
     BLOCK_COMMENT("call_stub_return_address:");
     return_address = __ pc();
 
     // store result depending on type (everything that is not
-    // T_OBJECT, T_LONG, T_FLOAT or T_DOUBLE is treated as T_INT)
-    __ movptr(c_rarg0, result);
-    Label is_long, is_float, is_double, exit;
-    __ movl(c_rarg1, result_type);
-    __ cmpl(c_rarg1, T_OBJECT);
+    // T_OBJECT, T_VALUETYPE, T_LONG, T_FLOAT or T_DOUBLE is treated as T_INT)
+    __ movptr(r13, result);
+    Label is_long, is_float, is_double, is_value, exit;
+    __ movl(rbx, result_type);
+    __ cmpl(rbx, T_OBJECT);
     __ jcc(Assembler::equal, is_long);
-    __ cmpl(c_rarg1, T_LONG);
+    __ cmpl(rbx, T_VALUETYPE);
+    __ jcc(Assembler::equal, is_value);
+    __ cmpl(rbx, T_LONG);
     __ jcc(Assembler::equal, is_long);
-    __ cmpl(c_rarg1, T_FLOAT);
+    __ cmpl(rbx, T_FLOAT);
     __ jcc(Assembler::equal, is_float);
-    __ cmpl(c_rarg1, T_DOUBLE);
+    __ cmpl(rbx, T_DOUBLE);
     __ jcc(Assembler::equal, is_double);
 
     // handle T_INT case
-    __ movl(Address(c_rarg0, 0), rax);
+    __ movl(Address(r13, 0), rax);
 
     __ BIND(exit);
 
     // pop parameters
     __ lea(rsp, rsp_after_call);
@@ -410,20 +412,33 @@
     __ vzeroupper();
     __ pop(rbp);
     __ ret(0);
 
     // handle return types different from T_INT
+    __ BIND(is_value);
+    if (InlineTypeReturnedAsFields) {
+      // Check for flattened return value
+      __ testptr(rax, 1);
+      __ jcc(Assembler::zero, is_long);
+      // Load pack handler address
+      __ andptr(rax, -2);
+      __ movptr(rax, Address(rax, InstanceKlass::adr_valueklass_fixed_block_offset()));
+      __ movptr(rbx, Address(rax, ValueKlass::pack_handler_jobject_offset()));
+      // Call pack handler to initialize the buffer
+      __ call(rbx);
+      __ jmp(exit);
+    }
     __ BIND(is_long);
-    __ movq(Address(c_rarg0, 0), rax);
+    __ movq(Address(r13, 0), rax);
     __ jmp(exit);
 
     __ BIND(is_float);
-    __ movflt(Address(c_rarg0, 0), xmm0);
+    __ movflt(Address(r13, 0), xmm0);
     __ jmp(exit);
 
     __ BIND(is_double);
-    __ movdbl(Address(c_rarg0, 0), xmm0);
+    __ movdbl(Address(r13, 0), xmm0);
     __ jmp(exit);
 
     return start;
   }
 
@@ -2510,11 +2525,11 @@
     //   for (count = -count; count != 0; count++)
     // Base pointers src, dst are biased by 8*(count-1),to last element.
     __ align(OptoLoopAlignment);
 
     __ BIND(L_store_element);
-    __ store_heap_oop(to_element_addr, rax_oop, noreg, noreg, AS_RAW);  // store the oop
+    __ store_heap_oop(to_element_addr, rax_oop, noreg, noreg, noreg, AS_RAW);  // store the oop
     __ increment(count);               // increment the count toward zero
     __ jcc(Assembler::zero, L_do_card_marks);
 
     // ======== loop entry is here ========
     __ BIND(L_load_element);
@@ -6301,10 +6316,150 @@
     StubRoutines::_fpu_subnormal_bias2[0]= 0x00000000; // 2^(+15360) == 0x7bff 8000 0000 0000 0000
     StubRoutines::_fpu_subnormal_bias2[1]= 0x80000000;
     StubRoutines::_fpu_subnormal_bias2[2]= 0x7bff;
   }
 
+  // Call here from the interpreter or compiled code to either load
+  // multiple returned values from the value type instance being
+  // returned to registers or to store returned values to a newly
+  // allocated value type instance.
+  address generate_return_value_stub(address destination, const char* name, bool has_res) {
+    // We need to save all registers the calling convention may use so
+    // the runtime calls read or update those registers. This needs to
+    // be in sync with SharedRuntime::java_return_convention().
+    enum layout {
+      pad_off = frame::arg_reg_save_area_bytes/BytesPerInt, pad_off_2,
+      rax_off, rax_off_2,
+      j_rarg5_off, j_rarg5_2,
+      j_rarg4_off, j_rarg4_2,
+      j_rarg3_off, j_rarg3_2,
+      j_rarg2_off, j_rarg2_2,
+      j_rarg1_off, j_rarg1_2,
+      j_rarg0_off, j_rarg0_2,
+      j_farg0_off, j_farg0_2,
+      j_farg1_off, j_farg1_2,
+      j_farg2_off, j_farg2_2,
+      j_farg3_off, j_farg3_2,
+      j_farg4_off, j_farg4_2,
+      j_farg5_off, j_farg5_2,
+      j_farg6_off, j_farg6_2,
+      j_farg7_off, j_farg7_2,
+      rbp_off, rbp_off_2,
+      return_off, return_off_2,
+
+      framesize
+    };
+
+    CodeBuffer buffer(name, 1000, 512);
+    MacroAssembler* masm = new MacroAssembler(&buffer);
+
+    int frame_size_in_bytes = align_up(framesize*BytesPerInt, 16);
+    assert(frame_size_in_bytes == framesize*BytesPerInt, "misaligned");
+    int frame_size_in_slots = frame_size_in_bytes / BytesPerInt;
+    int frame_size_in_words = frame_size_in_bytes / wordSize;
+
+    OopMapSet *oop_maps = new OopMapSet();
+    OopMap* map = new OopMap(frame_size_in_slots, 0);
+
+    map->set_callee_saved(VMRegImpl::stack2reg(rax_off), rax->as_VMReg());
+    map->set_callee_saved(VMRegImpl::stack2reg(j_rarg5_off), j_rarg5->as_VMReg());
+    map->set_callee_saved(VMRegImpl::stack2reg(j_rarg4_off), j_rarg4->as_VMReg());
+    map->set_callee_saved(VMRegImpl::stack2reg(j_rarg3_off), j_rarg3->as_VMReg());
+    map->set_callee_saved(VMRegImpl::stack2reg(j_rarg2_off), j_rarg2->as_VMReg());
+    map->set_callee_saved(VMRegImpl::stack2reg(j_rarg1_off), j_rarg1->as_VMReg());
+    map->set_callee_saved(VMRegImpl::stack2reg(j_rarg0_off), j_rarg0->as_VMReg());
+    map->set_callee_saved(VMRegImpl::stack2reg(j_farg0_off), j_farg0->as_VMReg());
+    map->set_callee_saved(VMRegImpl::stack2reg(j_farg1_off), j_farg1->as_VMReg());
+    map->set_callee_saved(VMRegImpl::stack2reg(j_farg2_off), j_farg2->as_VMReg());
+    map->set_callee_saved(VMRegImpl::stack2reg(j_farg3_off), j_farg3->as_VMReg());
+    map->set_callee_saved(VMRegImpl::stack2reg(j_farg4_off), j_farg4->as_VMReg());
+    map->set_callee_saved(VMRegImpl::stack2reg(j_farg5_off), j_farg5->as_VMReg());
+    map->set_callee_saved(VMRegImpl::stack2reg(j_farg6_off), j_farg6->as_VMReg());
+    map->set_callee_saved(VMRegImpl::stack2reg(j_farg7_off), j_farg7->as_VMReg());
+
+    int start = __ offset();
+
+    __ subptr(rsp, frame_size_in_bytes - 8 /* return address*/);
+
+    __ movptr(Address(rsp, rbp_off * BytesPerInt), rbp);
+    __ movdbl(Address(rsp, j_farg7_off * BytesPerInt), j_farg7);
+    __ movdbl(Address(rsp, j_farg6_off * BytesPerInt), j_farg6);
+    __ movdbl(Address(rsp, j_farg5_off * BytesPerInt), j_farg5);
+    __ movdbl(Address(rsp, j_farg4_off * BytesPerInt), j_farg4);
+    __ movdbl(Address(rsp, j_farg3_off * BytesPerInt), j_farg3);
+    __ movdbl(Address(rsp, j_farg2_off * BytesPerInt), j_farg2);
+    __ movdbl(Address(rsp, j_farg1_off * BytesPerInt), j_farg1);
+    __ movdbl(Address(rsp, j_farg0_off * BytesPerInt), j_farg0);
+
+    __ movptr(Address(rsp, j_rarg0_off * BytesPerInt), j_rarg0);
+    __ movptr(Address(rsp, j_rarg1_off * BytesPerInt), j_rarg1);
+    __ movptr(Address(rsp, j_rarg2_off * BytesPerInt), j_rarg2);
+    __ movptr(Address(rsp, j_rarg3_off * BytesPerInt), j_rarg3);
+    __ movptr(Address(rsp, j_rarg4_off * BytesPerInt), j_rarg4);
+    __ movptr(Address(rsp, j_rarg5_off * BytesPerInt), j_rarg5);
+    __ movptr(Address(rsp, rax_off * BytesPerInt), rax);
+
+    int frame_complete = __ offset();
+
+    __ set_last_Java_frame(noreg, noreg, NULL);
+
+    __ mov(c_rarg0, r15_thread);
+    __ mov(c_rarg1, rax);
+
+    __ call(RuntimeAddress(destination));
+
+    // Set an oopmap for the call site.
+
+    oop_maps->add_gc_map( __ offset() - start, map);
+
+    // clear last_Java_sp
+    __ reset_last_Java_frame(false);
+
+    __ movptr(rbp, Address(rsp, rbp_off * BytesPerInt));
+    __ movdbl(j_farg7, Address(rsp, j_farg7_off * BytesPerInt));
+    __ movdbl(j_farg6, Address(rsp, j_farg6_off * BytesPerInt));
+    __ movdbl(j_farg5, Address(rsp, j_farg5_off * BytesPerInt));
+    __ movdbl(j_farg4, Address(rsp, j_farg4_off * BytesPerInt));
+    __ movdbl(j_farg3, Address(rsp, j_farg3_off * BytesPerInt));
+    __ movdbl(j_farg2, Address(rsp, j_farg2_off * BytesPerInt));
+    __ movdbl(j_farg1, Address(rsp, j_farg1_off * BytesPerInt));
+    __ movdbl(j_farg0, Address(rsp, j_farg0_off * BytesPerInt));
+
+    __ movptr(j_rarg0, Address(rsp, j_rarg0_off * BytesPerInt));
+    __ movptr(j_rarg1, Address(rsp, j_rarg1_off * BytesPerInt));
+    __ movptr(j_rarg2, Address(rsp, j_rarg2_off * BytesPerInt));
+    __ movptr(j_rarg3, Address(rsp, j_rarg3_off * BytesPerInt));
+    __ movptr(j_rarg4, Address(rsp, j_rarg4_off * BytesPerInt));
+    __ movptr(j_rarg5, Address(rsp, j_rarg5_off * BytesPerInt));
+    __ movptr(rax, Address(rsp, rax_off * BytesPerInt));
+
+    __ addptr(rsp, frame_size_in_bytes-8);
+
+    // check for pending exceptions
+    Label pending;
+    __ cmpptr(Address(r15_thread, Thread::pending_exception_offset()), (int32_t)NULL_WORD);
+    __ jcc(Assembler::notEqual, pending);
+
+    if (has_res) {
+      __ get_vm_result(rax, r15_thread);
+    }
+
+    __ ret(0);
+
+    __ bind(pending);
+
+    __ movptr(rax, Address(r15_thread, Thread::pending_exception_offset()));
+    __ jump(RuntimeAddress(StubRoutines::forward_exception_entry()));
+
+    // -------------
+    // make sure all code is generated
+    masm->flush();
+
+    RuntimeStub* stub = RuntimeStub::new_runtime_stub(name, &buffer, frame_complete, frame_size_in_words, oop_maps, false);
+    return stub->entry_point();
+  }
+
   // Initialization
   void generate_initial() {
     // Generates all stubs and initializes the entry points
 
     // This platform-specific settings are needed by generate_call_stub()
@@ -6316,12 +6471,15 @@
     // much more complicated generator structure. See also comment in
     // stubRoutines.hpp.
 
     StubRoutines::_forward_exception_entry = generate_forward_exception();
 
-    StubRoutines::_call_stub_entry =
-      generate_call_stub(StubRoutines::_call_stub_return_address);
+    // Generate these first because they are called from other stubs
+    StubRoutines::_load_value_type_fields_in_regs = generate_return_value_stub(CAST_FROM_FN_PTR(address, SharedRuntime::load_value_type_fields_in_regs), "load_value_type_fields_in_regs", false);
+    StubRoutines::_store_value_type_fields_to_buf = generate_return_value_stub(CAST_FROM_FN_PTR(address, SharedRuntime::store_value_type_fields_to_buf), "store_value_type_fields_to_buf", true);
+
+    StubRoutines::_call_stub_entry = generate_call_stub(StubRoutines::_call_stub_return_address);
 
     // is referenced by megamorphic call
     StubRoutines::_catch_exception_entry = generate_catch_exception();
 
     // atomic calls
diff a/src/hotspot/share/c1/c1_GraphBuilder.cpp b/src/hotspot/share/c1/c1_GraphBuilder.cpp
--- a/src/hotspot/share/c1/c1_GraphBuilder.cpp
+++ b/src/hotspot/share/c1/c1_GraphBuilder.cpp
@@ -31,10 +31,11 @@
 #include "ci/ciCallSite.hpp"
 #include "ci/ciField.hpp"
 #include "ci/ciKlass.hpp"
 #include "ci/ciMemberName.hpp"
 #include "ci/ciUtilities.inline.hpp"
+#include "ci/ciValueKlass.hpp"
 #include "compiler/compilationPolicy.hpp"
 #include "compiler/compileBroker.hpp"
 #include "compiler/compilerEvent.hpp"
 #include "interpreter/bytecode.hpp"
 #include "jfr/jfrEvents.hpp"
@@ -654,10 +655,21 @@
     } else {
       _fields.at(index)->kill();
     }
   }
 
+  // Record this newly allocated object
+  void new_instance(NewValueTypeInstance* object) {
+    int index = _newobjects.length();
+    _newobjects.append(object);
+    if (_fields.at_grow(index, NULL) == NULL) {
+      _fields.at_put(index, new FieldBuffer());
+    } else {
+      _fields.at(index)->kill();
+    }
+  }
+
   void store_value(Value value) {
     int index = _newobjects.find(value);
     if (index != -1) {
       // stored a newly allocated object into another object.
       // Assume we've lost track of it as separate slice of memory.
@@ -969,38 +981,80 @@
     } else if (index == scope_data()->jsr_return_address_local()) {
       scope_data()->set_jsr_return_address_local(-1);
     }
   }
 
+  x->set_local_index(index);
   state->store_local(index, round_fp(x));
 }
 
 
 void GraphBuilder::load_indexed(BasicType type) {
   // In case of in block code motion in range check elimination
-  ValueStack* state_before = copy_state_indexed_access();
+  ValueStack* state_before = NULL;
+  int array_idx = state()->stack_size() - 2;
+  if (type == T_OBJECT && state()->stack_at(array_idx)->maybe_flattened_array()) {
+    // Save the entire state and re-execute on deopt when accessing flattened arrays
+    state_before = copy_state_before();
+    state_before->set_should_reexecute(true);
+  } else {
+    state_before = copy_state_indexed_access();
+  }
   compilation()->set_has_access_indexed(true);
   Value index = ipop();
   Value array = apop();
   Value length = NULL;
   if (CSEArrayLength ||
       (array->as_AccessField() && array->as_AccessField()->field()->is_constant()) ||
       (array->as_NewArray() && array->as_NewArray()->length() && array->as_NewArray()->length()->type()->is_constant())) {
     length = append(new ArrayLength(array, state_before));
   }
-  push(as_ValueType(type), append(new LoadIndexed(array, index, length, type, state_before)));
+
+  LoadIndexed* load_indexed = NULL;
+  Instruction* result = NULL;
+  if (array->is_loaded_flattened_array()) {
+    ciType* array_type = array->declared_type();
+    ciValueKlass* elem_klass = array_type->as_value_array_klass()->element_klass()->as_value_klass();
+    NewValueTypeInstance* new_instance = new NewValueTypeInstance(elem_klass, state_before, false);
+    _memory->new_instance(new_instance);
+    apush(append_split(new_instance));
+    load_indexed = new LoadIndexed(array, index, length, type, state_before);
+    load_indexed->set_vt(new_instance);
+  } else {
+    load_indexed = new LoadIndexed(array, index, length, type, state_before);
+  }
+  if (profile_array_accesses() && is_reference_type(type)) {
+    compilation()->set_would_profile(true);
+    load_indexed->set_should_profile(true);
+    load_indexed->set_profiled_method(method());
+    load_indexed->set_profiled_bci(bci());
+  }
+  result = append(load_indexed);
+  assert(!(profile_array_accesses() && is_reference_type(type)) || load_indexed == result, "should not be optimized out");
+  if (!array->is_loaded_flattened_array()) {
+    push(as_ValueType(type), result);
+  }
 }
 
 
 void GraphBuilder::store_indexed(BasicType type) {
   // In case of in block code motion in range check elimination
-  ValueStack* state_before = copy_state_indexed_access();
+  ValueStack* state_before = NULL;
+  int array_idx = state()->stack_size() - 3;
+  if (type == T_OBJECT && state()->stack_at(array_idx)->maybe_flattened_array()) {
+    // Save the entire state and re-execute on deopt when accessing flattened arrays
+    state_before = copy_state_before();
+    state_before->set_should_reexecute(true);
+  } else {
+    state_before = copy_state_indexed_access();
+  }
   compilation()->set_has_access_indexed(true);
   Value value = pop(as_ValueType(type));
   Value index = ipop();
   Value array = apop();
   Value length = NULL;
+  value->set_escaped();
   if (CSEArrayLength ||
       (array->as_AccessField() && array->as_AccessField()->field()->is_constant()) ||
       (array->as_NewArray() && array->as_NewArray()->length() && array->as_NewArray()->length()->type()->is_constant())) {
     length = append(new ArrayLength(array, state_before));
   }
@@ -1014,24 +1068,22 @@
       value = append(new LogicOp(Bytecodes::_iand, value, mask));
     }
   } else if (type == T_BYTE) {
     check_boolean = true;
   }
-  StoreIndexed* result = new StoreIndexed(array, index, length, type, value, state_before, check_boolean);
-  append(result);
-  _memory->store_value(value);
-
-  if (type == T_OBJECT && is_profiling()) {
+
+  StoreIndexed* store_indexed = new StoreIndexed(array, index, length, type, value, state_before, check_boolean);
     // Note that we'd collect profile data in this method if we wanted it.
     compilation()->set_would_profile(true);
-
-    if (profile_checkcasts()) {
-      result->set_profiled_method(method());
-      result->set_profiled_bci(bci());
-      result->set_should_profile(true);
-    }
+    store_indexed->set_should_profile(true);
+    store_indexed->set_profiled_method(method());
+    store_indexed->set_profiled_bci(bci());
   }
+  Instruction* result = append(store_indexed);
+  assert(!store_indexed->should_profile() || store_indexed == result, "should not be optimized out");
+  _memory->store_value(value);
+
 }
 
 
 void GraphBuilder::stack_op(Bytecodes::Code code) {
   switch (code) {
@@ -1222,13 +1274,39 @@
 
 void GraphBuilder::if_node(Value x, If::Condition cond, Value y, ValueStack* state_before) {
   BlockBegin* tsux = block_at(stream()->get_dest());
   BlockBegin* fsux = block_at(stream()->next_bci());
   bool is_bb = tsux->bci() < stream()->cur_bci() || fsux->bci() < stream()->cur_bci();
+
+  bool subst_check = false;
+  if (EnableValhalla && (stream()->cur_bc() == Bytecodes::_if_acmpeq || stream()->cur_bc() == Bytecodes::_if_acmpne) &&
+      method() != ciEnv::current()->ValueBootstrapMethods_klass()->find_method(ciSymbol::isSubstitutable_name(), ciSymbol::object_object_boolean_signature())) {
+    // If current method is ValueBootstrapMethods::isSubstitutable(),
+    // compile the acmp as a regular pointer comparison otherwise we
+    // could call ValueBootstrapMethods::isSubstitutable() back
+    ValueType* left_vt = x->type();
+    ValueType* right_vt = y->type();
+    if (left_vt->is_object()) {
+      assert(right_vt->is_object(), "must be");
+      ciKlass* left_klass = x->as_loaded_klass_or_null();
+      ciKlass* right_klass = y->as_loaded_klass_or_null();
+
+      if (left_klass == NULL || right_klass == NULL) {
+        // The klass is still unloaded, or came from a Phi node. Go slow case;
+        subst_check = true;
+      } else if (left_klass->can_be_value_klass() || right_klass->can_be_value_klass()) {
+        // Either operand may be a value object, but we're not sure. Go slow case;
+        subst_check = true;
+      } else {
+        // No need to do substitutability check
+      }
+    }
+  }
+
   // In case of loop invariant code motion or predicate insertion
   // before the body of a loop the state is needed
-  Instruction *i = append(new If(x, cond, false, y, tsux, fsux, (is_bb || compilation()->is_optimistic()) ? state_before : NULL, is_bb));
+  Instruction *i = append(new If(x, cond, false, y, tsux, fsux, (is_bb || compilation()->is_optimistic() || subst_check) ? state_before : NULL, is_bb, subst_check));
 
   assert(i->as_Goto() == NULL ||
          (i->as_Goto()->sux_at(0) == tsux  && i->as_Goto()->is_safepoint() == tsux->bci() < stream()->cur_bci()) ||
          (i->as_Goto()->sux_at(0) == fsux  && i->as_Goto()->is_safepoint() == fsux->bci() < stream()->cur_bci()),
          "safepoint state of Goto returned by canonicalizer incorrect");
@@ -1475,11 +1553,11 @@
     call_register_finalizer();
   }
 
   // The conditions for a memory barrier are described in Parse::do_exits().
   bool need_mem_bar = false;
-  if (method()->name() == ciSymbol::object_initializer_name() &&
+  if (method()->is_object_constructor() &&
        (scope()->wrote_final() ||
          (AlwaysSafeConstructors && scope()->wrote_fields()) ||
          (support_IRIW_for_not_multiple_copy_atomic_cpu && scope()->wrote_volatile()))) {
     need_mem_bar = true;
   }
@@ -1626,16 +1704,31 @@
     default:
       return new Constant(value);
   }
 }
 
+void GraphBuilder::copy_value_content(ciValueKlass* vk, Value src, int src_off, Value dest, int dest_off,
+    ValueStack* state_before, bool needs_patching) {
+  src->set_escaped();
+  for (int i = 0; i < vk->nof_nonstatic_fields(); i++) {
+    ciField* inner_field = vk->nonstatic_field_at(i);
+    assert(!inner_field->is_flattened(), "the iteration over nested fields is handled by the loop itself");
+    int off = inner_field->offset() - vk->first_field_offset();
+    LoadField* load = new LoadField(src, src_off + off, inner_field, false, state_before, needs_patching);
+    Value replacement = append(load);
+    StoreField* store = new StoreField(dest, dest_off + off, inner_field, replacement, false, state_before, needs_patching);
+    append(store);
+  }
+}
+
 void GraphBuilder::access_field(Bytecodes::Code code) {
   bool will_link;
   ciField* field = stream()->get_field(will_link);
   ciInstanceKlass* holder = field->holder();
   BasicType field_type = field->type()->basic_type();
   ValueType* type = as_ValueType(field_type);
+
   // call will_link again to determine if the field is valid.
   const bool needs_patching = !holder->is_loaded() ||
                               !field->will_link(method(), code) ||
                               PatchALot;
 
@@ -1654,15 +1747,15 @@
     } else {
       obj = new Constant(new InstanceConstant(holder->java_mirror()));
     }
   }
 
-  if (field->is_final() && (code == Bytecodes::_putfield)) {
+  if (field->is_final() && (code == Bytecodes::_putfield || code == Bytecodes::_withfield)) {
     scope()->set_wrote_final();
   }
 
-  if (code == Bytecodes::_putfield) {
+  if (code == Bytecodes::_putfield || code == Bytecodes::_withfield) {
     scope()->set_wrote_fields();
     if (field->is_volatile()) {
       scope()->set_wrote_volatile();
     }
   }
@@ -1682,17 +1775,22 @@
         push(type, append(constant));
       } else {
         if (state_before == NULL) {
           state_before = copy_state_for_exception();
         }
-        push(type, append(new LoadField(append(obj), offset, field, true,
-                                        state_before, needs_patching)));
+        LoadField* load_field = new LoadField(append(obj), offset, field, true,
+                                        state_before, needs_patching);
+        if (field->is_flattenable()) {
+          load_field->set_never_null(true);
+        }
+        push(type, append(load_field));
       }
       break;
     }
     case Bytecodes::_putstatic: {
       Value val = pop(type);
+      val->set_escaped();
       if (state_before == NULL) {
         state_before = copy_state_for_exception();
       }
       if (field->type()->basic_type() == T_BOOLEAN) {
         Value mask = append(new Constant(new IntConstant(1)));
@@ -1702,13 +1800,18 @@
       break;
     }
     case Bytecodes::_getfield: {
       // Check for compile-time constants, i.e., trusted final non-static fields.
       Value constant = NULL;
+      if (state_before == NULL && field->is_flattened()) {
+        // Save the entire state and re-execute on deopt when accessing flattened fields
+        assert(Interpreter::bytecode_should_reexecute(code), "should reexecute");
+        state_before = copy_state_before();
+      }
       obj = apop();
       ObjectType* obj_type = obj->type()->as_ObjectType();
-      if (field->is_constant() && obj_type->is_constant() && !PatchALot) {
+      if (field->is_constant() && !field->is_flattened() && obj_type->is_constant() && !PatchALot) {
         ciObject* const_oop = obj_type->constant_value();
         if (!const_oop->is_null_object() && const_oop->is_loaded()) {
           ciConstant field_value = field->constant_value_of(const_oop);
           if (field_value.is_valid()) {
             constant = make_constant(field_value, field);
@@ -1727,61 +1830,197 @@
         push(type, append(constant));
       } else {
         if (state_before == NULL) {
           state_before = copy_state_for_exception();
         }
-        LoadField* load = new LoadField(obj, offset, field, false, state_before, needs_patching);
-        Value replacement = !needs_patching ? _memory->load(load) : load;
-        if (replacement != load) {
-          assert(replacement->is_linked() || !replacement->can_be_linked(), "should already by linked");
-          // Writing an (integer) value to a boolean, byte, char or short field includes an implicit narrowing
-          // conversion. Emit an explicit conversion here to get the correct field value after the write.
-          BasicType bt = field->type()->basic_type();
-          switch (bt) {
-          case T_BOOLEAN:
-          case T_BYTE:
-            replacement = append(new Convert(Bytecodes::_i2b, replacement, as_ValueType(bt)));
-            break;
-          case T_CHAR:
-            replacement = append(new Convert(Bytecodes::_i2c, replacement, as_ValueType(bt)));
-            break;
-          case T_SHORT:
-            replacement = append(new Convert(Bytecodes::_i2s, replacement, as_ValueType(bt)));
-            break;
-          default:
-            break;
+        if (!field->is_flattened()) {
+          LoadField* load = new LoadField(obj, offset, field, false, state_before, needs_patching);
+          Value replacement = !needs_patching ? _memory->load(load) : load;
+          if (replacement != load) {
+            assert(replacement->is_linked() || !replacement->can_be_linked(), "should already by linked");
+            // Writing an (integer) value to a boolean, byte, char or short field includes an implicit narrowing
+            // conversion. Emit an explicit conversion here to get the correct field value after the write.
+            BasicType bt = field->type()->basic_type();
+            switch (bt) {
+            case T_BOOLEAN:
+            case T_BYTE:
+              replacement = append(new Convert(Bytecodes::_i2b, replacement, as_ValueType(bt)));
+              break;
+            case T_CHAR:
+              replacement = append(new Convert(Bytecodes::_i2c, replacement, as_ValueType(bt)));
+              break;
+            case T_SHORT:
+              replacement = append(new Convert(Bytecodes::_i2s, replacement, as_ValueType(bt)));
+              break;
+            default:
+              break;
+            }
+            push(type, replacement);
+          } else {
+            push(type, append(load));
           }
-          push(type, replacement);
-        } else {
-          push(type, append(load));
+        } else { // flattened field, not optimized solution: re-instantiate the flattened value
+          assert(field->type()->is_valuetype(), "Sanity check");
+          ciValueKlass* value_klass = field->type()->as_value_klass();
+          int flattening_offset = field->offset() - value_klass->first_field_offset();
+          assert(field->type()->is_valuetype(), "Sanity check");
+          scope()->set_wrote_final();
+          scope()->set_wrote_fields();
+          NewValueTypeInstance* new_instance = new NewValueTypeInstance(value_klass, state_before, false);
+          _memory->new_instance(new_instance);
+          apush(append_split(new_instance));
+          copy_value_content(value_klass, obj, field->offset(), new_instance, value_klass->first_field_offset(),
+                       state_before, needs_patching);
         }
       }
       break;
     }
+    case Bytecodes::_withfield:
     case Bytecodes::_putfield: {
       Value val = pop(type);
+      val->set_escaped();
       obj = apop();
       if (state_before == NULL) {
         state_before = copy_state_for_exception();
       }
       if (field->type()->basic_type() == T_BOOLEAN) {
         Value mask = append(new Constant(new IntConstant(1)));
         val = append(new LogicOp(Bytecodes::_iand, val, mask));
       }
-      StoreField* store = new StoreField(obj, offset, field, val, false, state_before, needs_patching);
-      if (!needs_patching) store = _memory->store(store);
-      if (store != NULL) {
-        append(store);
+
+      if (!field->is_flattened()) {
+        StoreField* store = new StoreField(obj, offset, field, val, false, state_before, needs_patching);
+        if (!needs_patching) store = _memory->store(store);
+        if (store != NULL) {
+          append(store);
+        }
+      } else {
+        assert(field->type()->is_valuetype(), "Sanity check");
+        ciValueKlass* value_klass = field->type()->as_value_klass();
+        int flattening_offset = field->offset() - value_klass->first_field_offset();
+        copy_value_content(value_klass, val, value_klass->first_field_offset(), obj, field->offset(),
+                   state_before, needs_patching);
       }
       break;
     }
     default:
       ShouldNotReachHere();
       break;
   }
 }
 
+// Baseline version of withfield, allocate every time
+void GraphBuilder::withfield(int field_index)
+{
+  bool will_link;
+  ciField* field_modify = stream()->get_field(will_link);
+  ciInstanceKlass* holder = field_modify->holder();
+  BasicType field_type = field_modify->type()->basic_type();
+  ValueType* type = as_ValueType(field_type);
+
+  // call will_link again to determine if the field is valid.
+  const bool needs_patching = !holder->is_loaded() ||
+                              !field_modify->will_link(method(), Bytecodes::_withfield) ||
+                              PatchALot;
+
+
+  scope()->set_wrote_final();
+  scope()->set_wrote_fields();
+
+  const int offset = !needs_patching ? field_modify->offset() : -1;
+
+  if (!holder->is_loaded()
+      || needs_patching /* FIXME: 8228634 - field_modify->will_link() may incorrectly return false */
+      ) {
+    ValueStack* state_before = copy_state_before();
+    Value val = pop(type);
+    Value obj = apop();
+    apush(append_split(new WithField(state_before)));
+    return;
+  }
+  ValueStack* state_before = copy_state_before();
+
+  Value val = pop(type);
+  Value obj = apop();
+
+  if (!needs_patching && obj->is_optimizable_for_withfield()) {
+    int astore_index;
+    ciBytecodeStream s(method());
+    s.force_bci(bci());
+    s.next();
+    switch (s.cur_bc()) {
+    case Bytecodes::_astore:    astore_index = s.get_index(); break;
+    case Bytecodes::_astore_0:  astore_index = 0; break;
+    case Bytecodes::_astore_1:  astore_index = 1; break;
+    case Bytecodes::_astore_2:  astore_index = 2; break;
+    case Bytecodes::_astore_3:  astore_index = 3; break;
+    default: astore_index = -1;
+    }
+
+    if (astore_index >= 0 && obj == state()->local_at(astore_index)) {
+      // We have a sequence like this, where we load a value object from a local slot,
+      // and overwrite the same local slot with a modified copy of the value object.
+      //      defaultvalue #1 // class compiler/valhalla/valuetypes/MyValue1
+      //      astore 9
+      //      ...
+      //      iload_0
+      //      aload 9
+      //      swap
+      //      withfield #7 // Field x:I
+      //      astore 9
+      // If this object was created by defaultvalue, and has not escaped, and is not stored
+      // in any other local slots, we can effectively treat the withfield/astore
+      // sequence as a single putfield bytecode.
+      push(objectType, obj);
+      push(type, val);
+      access_field(Bytecodes::_withfield);
+      stream()->next(); // skip the next astore/astore_n bytecode.
+      return;
+    }
+  }
+
+  assert(holder->is_valuetype(), "must be a value klass");
+  // Save the entire state and re-execute on deopt when executing withfield
+  state_before->set_should_reexecute(true);
+  NewValueTypeInstance* new_instance = new NewValueTypeInstance(holder->as_value_klass(), state_before, false);
+  _memory->new_instance(new_instance);
+  apush(append_split(new_instance));
+
+  for (int i = 0; i < holder->nof_nonstatic_fields(); i++) {
+    ciField* field = holder->nonstatic_field_at(i);
+    int off = field->offset();
+
+    if (field->offset() != offset) {
+      if (field->is_flattened()) {
+        assert(field->type()->is_valuetype(), "Sanity check");
+        assert(field->type()->is_valuetype(), "Only value types can be flattened");
+        ciValueKlass* vk = field->type()->as_value_klass();
+        copy_value_content(vk, obj, off, new_instance, vk->first_field_offset(), state_before, needs_patching);
+      } else {
+        // Only load those fields who are not modified
+        LoadField* load = new LoadField(obj, off, field, false, state_before, needs_patching);
+        Value replacement = append(load);
+        StoreField* store = new StoreField(new_instance, off, field, replacement, false, state_before, needs_patching);
+        append(store);
+      }
+    }
+  }
+
+  // Field to modify
+  if (field_modify->type()->basic_type() == T_BOOLEAN) {
+    Value mask = append(new Constant(new IntConstant(1)));
+    val = append(new LogicOp(Bytecodes::_iand, val, mask));
+  }
+  if (field_modify->is_flattened()) {
+    assert(field_modify->type()->is_valuetype(), "Only value types can be flattened");
+    ciValueKlass* vk = field_modify->type()->as_value_klass();
+    copy_value_content(vk, val, vk->first_field_offset(), new_instance, field_modify->offset(), state_before, needs_patching);
+  } else {
+    StoreField* store = new StoreField(new_instance, offset, field_modify, val, false, state_before, needs_patching);
+    append(store);
+  }
+}
 
 Dependencies* GraphBuilder::dependency_recorder() const {
   assert(DeoptC1, "need debug information");
   return compilation()->dependency_recorder();
 }
@@ -1863,11 +2102,11 @@
       log->elem("call method='%d' instr='%s'",
                 log->identify(target),
                 Bytecodes::name(code));
 
   // invoke-special-super
-  if (bc_raw == Bytecodes::_invokespecial && !target->is_object_initializer()) {
+  if (bc_raw == Bytecodes::_invokespecial && !target->is_object_constructor()) {
     ciInstanceKlass* sender_klass =
           calling_klass->is_unsafe_anonymous() ? calling_klass->unsafe_anonymous_host() :
                                                  calling_klass;
     if (sender_klass->is_interface()) {
       int index = state()->stack_size() - (target->arg_size_no_receiver() + 1);
@@ -2135,11 +2374,19 @@
         profile_call(target, recv, target_klass, collect_args_for_profiling(args, NULL, false), false);
       }
     }
   }
 
-  Invoke* result = new Invoke(code, result_type, recv, args, vtable_index, target, state_before);
+  if (recv != NULL) {
+    recv->set_escaped();
+  }
+  for (int i=0; i<args->length(); i++) {
+    args->at(0)->set_escaped();
+  }
+
+  Invoke* result = new Invoke(code, result_type, recv, args, vtable_index, target, state_before,
+                              declared_signature->returns_never_null());
   // push result
   append_split(result);
 
   if (result_type != voidType) {
     if (method()->is_strict()) {
@@ -2157,27 +2404,39 @@
 void GraphBuilder::new_instance(int klass_index) {
   ValueStack* state_before = copy_state_exhandling();
   bool will_link;
   ciKlass* klass = stream()->get_klass(will_link);
   assert(klass->is_instance_klass(), "must be an instance klass");
+  assert(!klass->is_valuetype(), "must not be a value klass");
   NewInstance* new_instance = new NewInstance(klass->as_instance_klass(), state_before, stream()->is_unresolved_klass());
   _memory->new_instance(new_instance);
   apush(append_split(new_instance));
 }
 
+void GraphBuilder::default_value(int klass_index) {
+  bool will_link;
+  ciValueKlass* vk = stream()->get_klass(will_link)->as_value_klass();
+  if (!stream()->is_unresolved_klass()) {
+    apush(append(new Constant(new InstanceConstant(vk->default_value_instance()))));
+  } else {
+    ValueStack* state_before = copy_state_before();
+    apush(append_split(new DefaultValue(state_before)));
+  }
+}
 
 void GraphBuilder::new_type_array() {
   ValueStack* state_before = copy_state_exhandling();
   apush(append_split(new NewTypeArray(ipop(), (BasicType)stream()->get_index(), state_before)));
 }
 
 
 void GraphBuilder::new_object_array() {
   bool will_link;
   ciKlass* klass = stream()->get_klass(will_link);
+  bool never_null = stream()->is_klass_never_null();
   ValueStack* state_before = !klass->is_loaded() || PatchALot ? copy_state_before() : copy_state_exhandling();
-  NewArray* n = new NewObjectArray(klass, ipop(), state_before);
+  NewArray* n = new NewObjectArray(klass, ipop(), state_before, never_null);
   apush(append_split(n));
 }
 
 
 bool GraphBuilder::direct_compare(ciKlass* k) {
@@ -2198,12 +2457,13 @@
 
 
 void GraphBuilder::check_cast(int klass_index) {
   bool will_link;
   ciKlass* klass = stream()->get_klass(will_link);
+  bool never_null = stream()->is_klass_never_null();
   ValueStack* state_before = !klass->is_loaded() || PatchALot ? copy_state_before() : copy_state_for_exception();
-  CheckCast* c = new CheckCast(klass, apop(), state_before);
+  CheckCast* c = new CheckCast(klass, apop(), state_before, never_null);
   apush(append_split(c));
   c->set_direct_compare(direct_compare(klass));
 
   if (is_profiling()) {
     // Note that we'd collect profile data in this method if we wanted it.
@@ -2238,13 +2498,32 @@
   }
 }
 
 
 void GraphBuilder::monitorenter(Value x, int bci) {
+  bool maybe_valuetype = false;
+  if (bci == InvocationEntryBci) {
+    // Called by GraphBuilder::inline_sync_entry.
+#ifdef ASSERT
+    ciType* obj_type = x->declared_type();
+    assert(obj_type == NULL || !obj_type->is_valuetype(), "valuetypes cannot have synchronized methods");
+#endif
+  } else {
+    // We are compiling a monitorenter bytecode
+    if (EnableValhalla) {
+      ciType* obj_type = x->declared_type();
+      if (obj_type == NULL || obj_type->as_klass()->can_be_value_klass()) {
+        // If we're (possibly) locking on a valuetype, check for markWord::always_locked_pattern
+        // and throw IMSE. (obj_type is null for Phi nodes, so let's just be conservative).
+        maybe_valuetype = true;
+      }
+    }
+  }
+
   // save state before locking in case of deoptimization after a NullPointerException
   ValueStack* state_before = copy_state_for_exception_with_bci(bci);
-  append_with_bci(new MonitorEnter(x, state()->lock(x), state_before), bci);
+  append_with_bci(new MonitorEnter(x, state()->lock(x), state_before, maybe_valuetype), bci);
   kill_all();
 }
 
 
 void GraphBuilder::monitorexit(Value x, int bci) {
@@ -2902,10 +3181,12 @@
       case Bytecodes::_multianewarray : new_multi_array(s.cur_bcp()[3]); break;
       case Bytecodes::_ifnull         : if_null(objectType, If::eql); break;
       case Bytecodes::_ifnonnull      : if_null(objectType, If::neq); break;
       case Bytecodes::_goto_w         : _goto(s.cur_bci(), s.get_far_dest()); break;
       case Bytecodes::_jsr_w          : jsr(s.get_far_dest()); break;
+      case Bytecodes::_defaultvalue   : default_value(s.get_index_u2()); break;
+      case Bytecodes::_withfield      : withfield(s.get_index_u2()); break;
       case Bytecodes::_breakpoint     : BAILOUT_("concurrent setting of breakpoint", NULL);
       default                         : ShouldNotReachHere(); break;
     }
 
     if (log != NULL)
@@ -3187,11 +3468,12 @@
 
   // Set up locals for receiver
   int idx = 0;
   if (!method()->is_static()) {
     // we should always see the receiver
-    state->store_local(idx, new Local(method()->holder(), objectType, idx, true));
+    state->store_local(idx, new Local(method()->holder(), objectType, idx,
+             /*receiver*/ true, /*never_null*/ method()->holder()->is_value_array_klass()));
     idx = 1;
   }
 
   // Set up locals for incoming arguments
   ciSignature* sig = method()->signature();
@@ -3199,11 +3481,11 @@
     ciType* type = sig->type_at(i);
     BasicType basic_type = type->basic_type();
     // don't allow T_ARRAY to propagate into locals types
     if (is_reference_type(basic_type)) basic_type = T_OBJECT;
     ValueType* vt = as_ValueType(basic_type);
-    state->store_local(idx, new Local(type, vt, idx, false));
+    state->store_local(idx, new Local(type, vt, idx, false, sig->is_never_null_at(i)));
     idx += type->size();
   }
 
   // lock synchronized method
   if (method()->is_synchronized()) {
diff a/src/hotspot/share/c1/c1_Instruction.hpp b/src/hotspot/share/c1/c1_Instruction.hpp
--- a/src/hotspot/share/c1/c1_Instruction.hpp
+++ b/src/hotspot/share/c1/c1_Instruction.hpp
@@ -70,14 +70,17 @@
 class   OsrEntry;
 class   ExceptionObject;
 class   StateSplit;
 class     Invoke;
 class     NewInstance;
+class     NewValueTypeInstance;
 class     NewArray;
 class       NewTypeArray;
 class       NewObjectArray;
 class       NewMultiArray;
+class     WithField;
+class     DefaultValue;
 class     TypeCheck;
 class       CheckCast;
 class       InstanceOf;
 class     AccessMonitor;
 class       MonitorEnter;
@@ -175,13 +178,16 @@
   virtual void do_Convert        (Convert*         x) = 0;
   virtual void do_NullCheck      (NullCheck*       x) = 0;
   virtual void do_TypeCast       (TypeCast*        x) = 0;
   virtual void do_Invoke         (Invoke*          x) = 0;
   virtual void do_NewInstance    (NewInstance*     x) = 0;
+  virtual void do_NewValueTypeInstance(NewValueTypeInstance* x) = 0;
   virtual void do_NewTypeArray   (NewTypeArray*    x) = 0;
   virtual void do_NewObjectArray (NewObjectArray*  x) = 0;
   virtual void do_NewMultiArray  (NewMultiArray*   x) = 0;
+  virtual void do_WithField      (WithField*       x) = 0;
+  virtual void do_DefaultValue   (DefaultValue*    x) = 0;
   virtual void do_CheckCast      (CheckCast*       x) = 0;
   virtual void do_InstanceOf     (InstanceOf*      x) = 0;
   virtual void do_MonitorEnter   (MonitorEnter*    x) = 0;
   virtual void do_MonitorExit    (MonitorExit*     x) = 0;
   virtual void do_Intrinsic      (Intrinsic*       x) = 0;
@@ -218,13 +224,14 @@
 //
 // Note: This hash functions affect the performance
 //       of ValueMap - make changes carefully!
 
 #define HASH1(x1            )                    ((intx)(x1))
-#define HASH2(x1, x2        )                    ((HASH1(x1        ) << 7) ^ HASH1(x2))
-#define HASH3(x1, x2, x3    )                    ((HASH2(x1, x2    ) << 7) ^ HASH1(x3))
-#define HASH4(x1, x2, x3, x4)                    ((HASH3(x1, x2, x3) << 7) ^ HASH1(x4))
+#define HASH2(x1, x2        )                    ((HASH1(x1            ) << 7) ^ HASH1(x2))
+#define HASH3(x1, x2, x3    )                    ((HASH2(x1, x2        ) << 7) ^ HASH1(x3))
+#define HASH4(x1, x2, x3, x4)                    ((HASH3(x1, x2, x3    ) << 7) ^ HASH1(x4))
+#define HASH5(x1, x2, x3, x4, x5)                ((HASH4(x1, x2, x3, x4) << 7) ^ HASH1(x5))
 
 
 // The following macros are used to implement instruction-specific hashing.
 // By default, each instruction implements hash() and is_equal(Value), used
 // for value numbering/common subexpression elimination. The default imple-
@@ -279,10 +286,25 @@
     if (f2 != _v->f2) return false;                   \
     if (f3 != _v->f3) return false;                   \
     return true;                                      \
   }                                                   \
 
+#define HASHING4(class_name, enabled, f1, f2, f3, f4) \
+  virtual intx hash() const {                         \
+    return (enabled) ? HASH5(name(), f1, f2, f3, f4) : 0; \
+  }                                                   \
+  virtual bool is_equal(Value v) const {              \
+    if (!(enabled)  ) return false;                   \
+    class_name* _v = v->as_##class_name();            \
+    if (_v == NULL  ) return false;                   \
+    if (f1 != _v->f1) return false;                   \
+    if (f2 != _v->f2) return false;                   \
+    if (f3 != _v->f3) return false;                   \
+    if (f4 != _v->f4) return false;                   \
+    return true;                                      \
+  }                                                   \
+
 
 // The mother of all instructions...
 
 class Instruction: public CompilationResourceObj {
  private:
@@ -353,10 +375,11 @@
 
   static const int no_bci = -99;
 
   enum InstructionFlag {
     NeedsNullCheckFlag = 0,
+    NeverNullFlag,          // For "Q" signatures
     CanTrapFlag,
     DirectCompareFlag,
     IsEliminatedFlag,
     IsSafepointFlag,
     IsStaticFlag,
@@ -447,20 +470,29 @@
   Instruction* subst()                           { return _subst == NULL ? this : _subst->subst(); }
   LIR_Opr operand() const                        { return _operand; }
 
   void set_needs_null_check(bool f)              { set_flag(NeedsNullCheckFlag, f); }
   bool needs_null_check() const                  { return check_flag(NeedsNullCheckFlag); }
+  void set_never_null(bool f)                    { set_flag(NeverNullFlag, f); }
+  bool is_never_null() const                     { return check_flag(NeverNullFlag); }
   bool is_linked() const                         { return check_flag(IsLinkedInBlockFlag); }
   bool can_be_linked()                           { return as_Local() == NULL && as_Phi() == NULL; }
 
   bool is_null_obj()                             { return as_Constant() != NULL && type()->as_ObjectType()->constant_value()->is_null_object(); }
 
   bool has_uses() const                          { return use_count() > 0; }
   ValueStack* state_before() const               { return _state_before; }
   ValueStack* exception_state() const            { return _exception_state; }
   virtual bool needs_exception_state() const     { return true; }
   XHandlers* exception_handlers() const          { return _exception_handlers; }
+  ciKlass* as_loaded_klass_or_null() const;
+
+  // withfield optimization
+  virtual void set_escaped()                     { }
+  virtual void set_local_index(int index)        { }
+  virtual bool is_optimizable_for_withfield() const { return false; }
+
 
   // manipulation
   void pin(PinReason reason)                     { _pin_state |= reason; }
   void pin()                                     { _pin_state |= PinUnknown; }
   // DANGEROUS: only used by EliminateStores
@@ -501,10 +533,14 @@
     set_next(i);
     i->set_next(n);
     return _next;
   }
 
+  bool is_loaded_flattened_array() const;
+  bool maybe_flattened_array();
+  bool maybe_null_free_array();
+
   Instruction *insert_after_same_bci(Instruction *i) {
 #ifndef PRODUCT
     i->set_printable_bci(printable_bci());
 #endif
     return insert_after(i);
@@ -548,14 +584,17 @@
   virtual NullCheck*        as_NullCheck()       { return NULL; }
   virtual OsrEntry*         as_OsrEntry()        { return NULL; }
   virtual StateSplit*       as_StateSplit()      { return NULL; }
   virtual Invoke*           as_Invoke()          { return NULL; }
   virtual NewInstance*      as_NewInstance()     { return NULL; }
+  virtual NewValueTypeInstance* as_NewValueTypeInstance() { return NULL; }
   virtual NewArray*         as_NewArray()        { return NULL; }
   virtual NewTypeArray*     as_NewTypeArray()    { return NULL; }
   virtual NewObjectArray*   as_NewObjectArray()  { return NULL; }
   virtual NewMultiArray*    as_NewMultiArray()   { return NULL; }
+  virtual WithField*        as_WithField()       { return NULL; }
+  virtual DefaultValue*     as_DefaultValue()    { return NULL; }
   virtual TypeCheck*        as_TypeCheck()       { return NULL; }
   virtual CheckCast*        as_CheckCast()       { return NULL; }
   virtual InstanceOf*       as_InstanceOf()      { return NULL; }
   virtual TypeCast*         as_TypeCast()        { return NULL; }
   virtual AccessMonitor*    as_AccessMonitor()   { return NULL; }
@@ -701,16 +740,17 @@
   int      _java_index;                          // the local index within the method to which the local belongs
   bool     _is_receiver;                         // if local variable holds the receiver: "this" for non-static methods
   ciType*  _declared_type;
  public:
   // creation
-  Local(ciType* declared, ValueType* type, int index, bool receiver)
+  Local(ciType* declared, ValueType* type, int index, bool receiver, bool never_null)
     : Instruction(type)
     , _java_index(index)
     , _is_receiver(receiver)
     , _declared_type(declared)
   {
+    set_never_null(never_null);
     NOT_PRODUCT(set_printable_bci(-1));
   }
 
   // accessors
   int java_index() const                         { return _java_index; }
@@ -826,11 +866,12 @@
 
 LEAF(LoadField, AccessField)
  public:
   // creation
   LoadField(Value obj, int offset, ciField* field, bool is_static,
-            ValueStack* state_before, bool needs_patching)
+            ValueStack* state_before, bool needs_patching,
+            ciValueKlass* value_klass = NULL, Value default_value = NULL )
   : AccessField(obj, offset, field, is_static, state_before, needs_patching)
   {}
 
   ciType* declared_type() const;
 
@@ -913,19 +954,22 @@
  private:
   Value     _index;
   Value     _length;
   BasicType _elt_type;
   bool      _mismatched;
+  ciMethod* _profiled_method;
+  int       _profiled_bci;
 
  public:
   // creation
   AccessIndexed(Value array, Value index, Value length, BasicType elt_type, ValueStack* state_before, bool mismatched)
   : AccessArray(as_ValueType(elt_type), array, state_before)
   , _index(index)
   , _length(length)
   , _elt_type(elt_type)
   , _mismatched(mismatched)
+  , _profiled_method(NULL), _profiled_bci(0)
   {
     set_flag(Instruction::NeedsRangeCheckFlag, true);
     ASSERT_VALUES
   }
 
@@ -937,24 +981,34 @@
 
   void clear_length()                            { _length = NULL; }
   // perform elimination of range checks involving constants
   bool compute_needs_range_check();
 
-  // generic
+  // Helpers for MethodData* profiling
+  void set_should_profile(bool value)                { set_flag(ProfileMDOFlag, value); }
+  void set_profiled_method(ciMethod* method)         { _profiled_method = method;   }
+  void set_profiled_bci(int bci)                     { _profiled_bci = bci;         }
+  bool      should_profile() const                   { return check_flag(ProfileMDOFlag); }
+  ciMethod* profiled_method() const                  { return _profiled_method;     }
+  int       profiled_bci() const                     { return _profiled_bci;        }
+
+
+// generic
   virtual void input_values_do(ValueVisitor* f)   { AccessArray::input_values_do(f); f->visit(&_index); if (_length != NULL) f->visit(&_length); }
 };
 
 
 LEAF(LoadIndexed, AccessIndexed)
  private:
   NullCheck*  _explicit_null_check;              // For explicit null check elimination
+  NewValueTypeInstance* _vt;
 
  public:
   // creation
   LoadIndexed(Value array, Value index, Value length, BasicType elt_type, ValueStack* state_before, bool mismatched = false)
   : AccessIndexed(array, index, length, elt_type, state_before, mismatched)
-  , _explicit_null_check(NULL) {}
+  , _explicit_null_check(NULL), _vt(NULL) {}
 
   // accessors
   NullCheck* explicit_null_check() const         { return _explicit_null_check; }
 
   // setters
@@ -962,29 +1016,30 @@
   void set_explicit_null_check(NullCheck* check) { _explicit_null_check = check; }
 
   ciType* exact_type() const;
   ciType* declared_type() const;
 
-  // generic;
-  HASHING3(LoadIndexed, true, type()->tag(), array()->subst(), index()->subst())
+  NewValueTypeInstance* vt() const { return _vt; }
+  void set_vt(NewValueTypeInstance* vt) { _vt = vt; }
+
+  // generic
+  HASHING4(LoadIndexed, !should_profile(), type()->tag(), array()->subst(), index()->subst(), vt())
 };
 
 
 LEAF(StoreIndexed, AccessIndexed)
  private:
   Value       _value;
 
-  ciMethod* _profiled_method;
-  int       _profiled_bci;
   bool      _check_boolean;
 
  public:
   // creation
   StoreIndexed(Value array, Value index, Value length, BasicType elt_type, Value value, ValueStack* state_before,
                bool check_boolean, bool mismatched = false)
   : AccessIndexed(array, index, length, elt_type, state_before, mismatched)
-  , _value(value), _profiled_method(NULL), _profiled_bci(0), _check_boolean(check_boolean)
+  , _value(value), _check_boolean(check_boolean)
   {
     set_flag(NeedsWriteBarrierFlag, (as_ValueType(elt_type)->is_object()));
     set_flag(NeedsStoreCheckFlag, (as_ValueType(elt_type)->is_object()));
     ASSERT_VALUES
     pin();
@@ -993,17 +1048,13 @@
   // accessors
   Value value() const                            { return _value; }
   bool needs_write_barrier() const               { return check_flag(NeedsWriteBarrierFlag); }
   bool needs_store_check() const                 { return check_flag(NeedsStoreCheckFlag); }
   bool check_boolean() const                     { return _check_boolean; }
-  // Helpers for MethodData* profiling
-  void set_should_profile(bool value)                { set_flag(ProfileMDOFlag, value); }
-  void set_profiled_method(ciMethod* method)         { _profiled_method = method;   }
-  void set_profiled_bci(int bci)                     { _profiled_bci = bci;         }
-  bool      should_profile() const                   { return check_flag(ProfileMDOFlag); }
-  ciMethod* profiled_method() const                  { return _profiled_method;     }
-  int       profiled_bci() const                     { return _profiled_bci;        }
+
+  // Flattened array support
+  bool is_exact_flattened_array_store() const;
   // generic
   virtual void input_values_do(ValueVisitor* f)   { AccessIndexed::input_values_do(f); f->visit(&_value); }
 };
 
 
@@ -1114,29 +1165,32 @@
 
 LEAF(IfOp, Op2)
  private:
   Value _tval;
   Value _fval;
+  bool _substitutability_check;
 
  public:
   // creation
-  IfOp(Value x, Condition cond, Value y, Value tval, Value fval)
+  IfOp(Value x, Condition cond, Value y, Value tval, Value fval, ValueStack* state_before, bool substitutability_check)
   : Op2(tval->type()->meet(fval->type()), (Bytecodes::Code)cond, x, y)
   , _tval(tval)
   , _fval(fval)
+  , _substitutability_check(substitutability_check)
   {
     ASSERT_VALUES
     assert(tval->type()->tag() == fval->type()->tag(), "types must match");
+    set_state_before(state_before);
   }
 
   // accessors
   virtual bool is_commutative() const;
   Bytecodes::Code op() const                     { ShouldNotCallThis(); return Bytecodes::_illegal; }
   Condition cond() const                         { return (Condition)Op2::op(); }
   Value tval() const                             { return _tval; }
   Value fval() const                             { return _fval; }
-
+  bool substitutability_check() const             { return _substitutability_check; }
   // generic
   virtual void input_values_do(ValueVisitor* f)   { Op2::input_values_do(f); f->visit(&_tval); f->visit(&_fval); }
 };
 
 
@@ -1252,11 +1306,11 @@
   ciMethod*       _target;
 
  public:
   // creation
   Invoke(Bytecodes::Code code, ValueType* result_type, Value recv, Values* args,
-         int vtable_index, ciMethod* target, ValueStack* state_before);
+         int vtable_index, ciMethod* target, ValueStack* state_before, bool never_null);
 
   // accessors
   Bytecodes::Code code() const                   { return _code; }
   Value receiver() const                         { return _recv; }
   bool has_receiver() const                      { return receiver() != NULL; }
@@ -1313,10 +1367,65 @@
   virtual bool can_trap() const                  { return true; }
   ciType* exact_type() const;
   ciType* declared_type() const;
 };
 
+LEAF(NewValueTypeInstance, StateSplit)
+  bool _is_unresolved;
+  ciValueKlass* _klass;
+  Value _depends_on;      // Link to instance on with withfield was called on
+  bool _is_optimizable_for_withfield;
+  int _first_local_index;
+public:
+
+  // Default creation, always allocated for now
+  NewValueTypeInstance(ciValueKlass* klass, ValueStack* state_before, bool is_unresolved, Value depends_on = NULL, bool from_default_value = false)
+  : StateSplit(instanceType, state_before)
+   , _is_unresolved(is_unresolved)
+   , _klass(klass)
+   , _is_optimizable_for_withfield(from_default_value)
+   , _first_local_index(-1)
+  {
+    if (depends_on == NULL) {
+      _depends_on = this;
+    } else {
+      _depends_on = depends_on;
+    }
+    set_never_null(true);
+  }
+
+  // accessors
+  bool is_unresolved() const                     { return _is_unresolved; }
+  Value depends_on();
+
+  ciValueKlass* klass() const { return _klass; }
+
+  virtual bool needs_exception_state() const     { return false; }
+
+  // generic
+  virtual bool can_trap() const                  { return true; }
+  ciType* exact_type() const;
+  ciType* declared_type() const;
+
+  // Only done in LIR Generator -> map everything to object
+  void set_to_object_type() { set_type(instanceType); }
+
+  // withfield optimization
+  virtual void set_escaped() {
+    _is_optimizable_for_withfield = false;
+  }
+  virtual void set_local_index(int index) {
+    if (_first_local_index != index) {
+      if (_first_local_index == -1) {
+        _first_local_index = index;
+      } else {
+        _is_optimizable_for_withfield = false;
+      }
+    }
+  }
+  virtual bool is_optimizable_for_withfield() const {  return _is_optimizable_for_withfield; }
+};
 
 BASE(NewArray, StateSplit)
  private:
   Value       _length;
 
@@ -1364,11 +1473,14 @@
  private:
   ciKlass* _klass;
 
  public:
   // creation
-  NewObjectArray(ciKlass* klass, Value length, ValueStack* state_before) : NewArray(length, state_before), _klass(klass) {}
+  NewObjectArray(ciKlass* klass, Value length, ValueStack* state_before, bool never_null)
+  : NewArray(length, state_before), _klass(klass) {
+    set_never_null(never_null);
+  }
 
   // accessors
   ciKlass* klass() const                         { return _klass; }
   ciType* exact_type() const;
 };
@@ -1399,12 +1511,27 @@
     // get updated, and the value must not be traversed twice. Was bug
     // - kbr 4/10/2001
     StateSplit::input_values_do(f);
     for (int i = 0; i < _dims->length(); i++) f->visit(_dims->adr_at(i));
   }
+
+  ciType* exact_type() const;
 };
 
+LEAF(WithField, StateSplit)
+ public:
+  // creation
+  WithField(ValueStack* state_before)
+  : StateSplit(objectType, state_before) {}
+};
+
+LEAF(DefaultValue, StateSplit)
+ public:
+  // creation
+  DefaultValue(ValueStack* state_before)
+  : StateSplit(objectType, state_before) {}
+};
 
 BASE(TypeCheck, StateSplit)
  private:
   ciKlass*    _klass;
   Value       _obj;
@@ -1445,12 +1572,14 @@
 
 
 LEAF(CheckCast, TypeCheck)
  public:
   // creation
-  CheckCast(ciKlass* klass, Value obj, ValueStack* state_before)
-  : TypeCheck(klass, obj, objectType, state_before) {}
+  CheckCast(ciKlass* klass, Value obj, ValueStack* state_before, bool never_null = false)
+  : TypeCheck(klass, obj, objectType, state_before) {
+    set_never_null(never_null);
+  }
 
   void set_incompatible_class_change_check() {
     set_flag(ThrowIncompatibleClassChangeErrorFlag, true);
   }
   bool is_incompatible_class_change_check() const {
@@ -1504,18 +1633,23 @@
   virtual void input_values_do(ValueVisitor* f)   { StateSplit::input_values_do(f); f->visit(&_obj); }
 };
 
 
 LEAF(MonitorEnter, AccessMonitor)
+  bool _maybe_valuetype;
  public:
   // creation
-  MonitorEnter(Value obj, int monitor_no, ValueStack* state_before)
+  MonitorEnter(Value obj, int monitor_no, ValueStack* state_before, bool maybe_valuetype)
   : AccessMonitor(obj, monitor_no, state_before)
+  , _maybe_valuetype(maybe_valuetype)
   {
     ASSERT_VALUES
   }
 
+  // accessors
+  bool maybe_valuetype() const                   { return _maybe_valuetype; }
+
   // generic
   virtual bool can_trap() const                  { return true; }
 };
 
 
@@ -1976,21 +2110,23 @@
   Value       _y;
   ciMethod*   _profiled_method;
   int         _profiled_bci; // Canonicalizer may alter bci of If node
   bool        _swapped;      // Is the order reversed with respect to the original If in the
                              // bytecode stream?
+  bool        _substitutability_check;
  public:
   // creation
   // unordered_is_true is valid for float/double compares only
-  If(Value x, Condition cond, bool unordered_is_true, Value y, BlockBegin* tsux, BlockBegin* fsux, ValueStack* state_before, bool is_safepoint)
+  If(Value x, Condition cond, bool unordered_is_true, Value y, BlockBegin* tsux, BlockBegin* fsux, ValueStack* state_before, bool is_safepoint, bool substitutability_check=false)
     : BlockEnd(illegalType, state_before, is_safepoint)
   , _x(x)
   , _cond(cond)
   , _y(y)
   , _profiled_method(NULL)
   , _profiled_bci(0)
   , _swapped(false)
+  , _substitutability_check(substitutability_check)
   {
     ASSERT_VALUES
     set_flag(UnorderedIsTrueFlag, unordered_is_true);
     assert(x->type()->tag() == y->type()->tag(), "types must match");
     BlockList* s = new BlockList(2);
@@ -2029,10 +2165,11 @@
 
   void set_should_profile(bool value)             { set_flag(ProfileMDOFlag, value); }
   void set_profiled_method(ciMethod* method)      { _profiled_method = method; }
   void set_profiled_bci(int bci)                  { _profiled_bci = bci;       }
   void set_swapped(bool value)                    { _swapped = value;         }
+  bool substitutability_check() const              { return _substitutability_check; }
   // generic
   virtual void input_values_do(ValueVisitor* f)   { BlockEnd::input_values_do(f); f->visit(&_x); f->visit(&_y); }
 };
 
 
diff a/src/hotspot/share/c1/c1_ValueMap.cpp b/src/hotspot/share/c1/c1_ValueMap.cpp
--- a/src/hotspot/share/c1/c1_ValueMap.cpp
+++ b/src/hotspot/share/c1/c1_ValueMap.cpp
@@ -581,10 +581,12 @@
 
 void GlobalValueNumbering::substitute(Instruction* instr) {
   assert(!instr->has_subst(), "substitution already set");
   Value subst = current_map()->find_insert(instr);
   if (subst != instr) {
+    assert(instr->as_LoadIndexed() == NULL || !instr->as_LoadIndexed()->should_profile(), "should not be optimized out");
+    assert(instr->as_StoreIndexed() == NULL, "should not be optimized out");
     assert(!subst->has_subst(), "can't have a substitution");
 
     TRACE_VALUE_NUMBERING(tty->print_cr("substitution for %d set to %d", instr->id(), subst->id()));
     instr->set_subst(subst);
     _has_substitutions = true;
diff a/src/hotspot/share/ci/ciEnv.cpp b/src/hotspot/share/ci/ciEnv.cpp
--- a/src/hotspot/share/ci/ciEnv.cpp
+++ b/src/hotspot/share/ci/ciEnv.cpp
@@ -31,10 +31,11 @@
 #include "ci/ciInstanceKlass.hpp"
 #include "ci/ciMethod.hpp"
 #include "ci/ciNullObject.hpp"
 #include "ci/ciReplay.hpp"
 #include "ci/ciUtilities.inline.hpp"
+#include "ci/ciValueKlass.hpp"
 #include "classfile/symbolTable.hpp"
 #include "classfile/systemDictionary.hpp"
 #include "classfile/vmSymbols.hpp"
 #include "code/codeCache.hpp"
 #include "code/scopeDesc.hpp"
@@ -464,11 +465,13 @@
   // In either case, if we can find the element type in the system dictionary,
   // we must build an array type around it.  The CI requires array klasses
   // to be loaded if their element klasses are loaded, except when memory
   // is exhausted.
   if (Signature::is_array(sym) &&
-      (sym->char_at(1) == JVM_SIGNATURE_ARRAY || sym->char_at(1) == JVM_SIGNATURE_CLASS)) {
+      (sym->char_at(1) == JVM_SIGNATURE_ARRAY ||
+       sym->char_at(1) == JVM_SIGNATURE_CLASS ||
+       sym->char_at(1) == JVM_SIGNATURE_VALUETYPE )) {
     // We have an unloaded array.
     // Build it on the fly if the element class exists.
     SignatureStream ss(sym, false);
     ss.skip_array_prefix(1);
     // Get element ciKlass recursively.
@@ -477,11 +480,11 @@
                              cpool,
                              get_symbol(ss.as_symbol()),
                              require_local);
     if (elem_klass != NULL && elem_klass->is_loaded()) {
       // Now make an array for it
-      return ciObjArrayKlass::make_impl(elem_klass);
+      return ciArrayKlass::make(elem_klass);
     }
   }
 
   if (found_klass == NULL && !cpool.is_null() && cpool->has_preresolution()) {
     // Look inside the constant pool for pre-resolved class entries.
@@ -503,10 +506,25 @@
 
   if (require_local)  return NULL;
 
   // Not yet loaded into the VM, or not governed by loader constraints.
   // Make a CI representative for it.
+  int i = 0;
+  while (sym->char_at(i) == JVM_SIGNATURE_ARRAY) {
+    i++;
+  }
+  if (i > 0 && sym->char_at(i) == JVM_SIGNATURE_VALUETYPE) {
+    // An unloaded array class of value types is an ObjArrayKlass, an
+    // unloaded value type class is an InstanceKlass. For consistency,
+    // make the signature of the unloaded array of value type use L
+    // rather than Q.
+    char *new_name = CURRENT_THREAD_ENV->name_buffer(sym->utf8_length()+1);
+    strncpy(new_name, (char*)sym->base(), sym->utf8_length());
+    new_name[i] = JVM_SIGNATURE_CLASS;
+    new_name[sym->utf8_length()] = '\0';
+    return get_unloaded_klass(accessing_klass, ciSymbol::make(new_name));
+  }
   return get_unloaded_klass(accessing_klass, name);
 }
 
 // ------------------------------------------------------------------
 // ciEnv::get_klass_by_name
@@ -533,11 +551,11 @@
 
   if (cpool->tag_at(index).is_symbol()) {
     klass_name = cpool->symbol_at(index);
   } else {
     // Check if it's resolved if it's not a symbol constant pool entry.
-    klass =  ConstantPool::klass_at_if_loaded(cpool, index);
+    klass = ConstantPool::klass_at_if_loaded(cpool, index);
     // Try to look it up by name.
     if (klass == NULL) {
       klass_name = cpool->klass_name_at(index);
     }
   }
@@ -585,10 +603,18 @@
                                    bool& is_accessible,
                                    ciInstanceKlass* accessor) {
   GUARDED_VM_ENTRY(return get_klass_by_index_impl(cpool, index, is_accessible, accessor);)
 }
 
+// ------------------------------------------------------------------
+// ciEnv::is_klass_never_null
+//
+// Get information about nullability from the constant pool.
+bool ciEnv::is_klass_never_null(const constantPoolHandle& cpool, int index) {
+  GUARDED_VM_ENTRY(return cpool->klass_name_at(index)->is_Q_signature();)
+}
+
 // ------------------------------------------------------------------
 // ciEnv::get_constant_by_index_impl
 //
 // Implementation of get_constant_by_index().
 ciConstant ciEnv::get_constant_by_index_impl(const constantPoolHandle& cpool,
diff a/src/hotspot/share/classfile/bytecodeAssembler.cpp b/src/hotspot/share/classfile/bytecodeAssembler.cpp
--- a/src/hotspot/share/classfile/bytecodeAssembler.cpp
+++ b/src/hotspot/share/classfile/bytecodeAssembler.cpp
@@ -184,10 +184,11 @@
     case T_SHORT:
     case T_INT:     iload(index); break;
     case T_FLOAT:   fload(index); break;
     case T_DOUBLE:  dload(index); break;
     case T_LONG:    lload(index); break;
+    case T_VALUETYPE:
     default:
       if (is_reference_type(bt)) {
                     aload(index);
                     break;
       }
@@ -253,10 +254,11 @@
     case T_SHORT:
     case T_INT:     ireturn(); break;
     case T_FLOAT:   freturn(); break;
     case T_DOUBLE:  dreturn(); break;
     case T_LONG:    lreturn(); break;
+    case T_VALUETYPE:
     case T_VOID:    _return(); break;
     default:
       if (is_reference_type(bt)) {
                     areturn();
                     break;
diff a/src/hotspot/share/classfile/classFileParser.cpp b/src/hotspot/share/classfile/classFileParser.cpp
--- a/src/hotspot/share/classfile/classFileParser.cpp
+++ b/src/hotspot/share/classfile/classFileParser.cpp
@@ -19,10 +19,11 @@
  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  * or visit www.oracle.com if you need additional information or have any
  * questions.
  *
  */
+
 #include "precompiled.hpp"
 #include "jvm.h"
 #include "aot/aotLoader.hpp"
 #include "classfile/classFileParser.hpp"
 #include "classfile/classFileStream.hpp"
@@ -56,10 +57,11 @@
 #include "oops/metadata.hpp"
 #include "oops/method.inline.hpp"
 #include "oops/oop.inline.hpp"
 #include "oops/recordComponent.hpp"
 #include "oops/symbol.hpp"
+#include "oops/valueKlass.inline.hpp"
 #include "prims/jvmtiExport.hpp"
 #include "prims/jvmtiThreadState.hpp"
 #include "runtime/arguments.hpp"
 #include "runtime/fieldDescriptor.inline.hpp"
 #include "runtime/handles.inline.hpp"
@@ -79,10 +81,11 @@
 #include "utilities/globalDefinitions.hpp"
 #include "utilities/growableArray.hpp"
 #include "utilities/macros.hpp"
 #include "utilities/ostream.hpp"
 #include "utilities/resourceHash.hpp"
+#include "utilities/stringUtils.hpp"
 #include "utilities/utf8.hpp"
 
 #if INCLUDE_CDS
 #include "classfile/systemDictionaryShared.hpp"
 #endif
@@ -130,10 +133,12 @@
 
 #define JAVA_14_VERSION                   58
 
 #define JAVA_15_VERSION                   59
 
+#define CONSTANT_CLASS_DESCRIPTORS        59
+
 void ClassFileParser::set_class_bad_constant_seen(short bad_constant) {
   assert((bad_constant == JVM_CONSTANT_Module ||
           bad_constant == JVM_CONSTANT_Package) && _major_version >= JAVA_9_VERSION,
          "Unexpected bad constant pool entry");
   if (_bad_constant_seen == 0) _bad_constant_seen = bad_constant;
@@ -169,11 +174,11 @@
     // Each of the following case guarantees one more byte in the stream
     // for the following tag or the access_flags following constant pool,
     // so we don't need bounds-check for reading tag.
     const u1 tag = cfs->get_u1_fast();
     switch (tag) {
-      case JVM_CONSTANT_Class : {
+      case JVM_CONSTANT_Class: {
         cfs->guarantee_more(3, CHECK);  // name_index, tag/access_flags
         const u2 name_index = cfs->get_u2_fast();
         cp->klass_index_at_put(index, name_index);
         break;
       }
@@ -499,11 +504,18 @@
       case JVM_CONSTANT_ClassIndex: {
         const int class_index = cp->klass_index_at(index);
         check_property(valid_symbol_at(class_index),
           "Invalid constant pool index %u in class file %s",
           class_index, CHECK);
-        cp->unresolved_klass_at_put(index, class_index, num_klasses++);
+
+        Symbol* const name = cp->symbol_at(class_index);
+        const unsigned int name_len = name->utf8_length();
+        if (name->is_Q_signature()) {
+          cp->unresolved_qdescriptor_at_put(index, class_index, num_klasses++);
+        } else {
+          cp->unresolved_klass_at_put(index, class_index, num_klasses++);
+        }
         break;
       }
       case JVM_CONSTANT_StringIndex: {
         const int string_index = cp->string_index_at(index);
         check_property(valid_symbol_at(string_index),
@@ -753,18 +765,29 @@
             const int name_and_type_ref_index =
               cp->name_and_type_ref_index_at(ref_index);
             const int name_ref_index =
               cp->name_ref_index_at(name_and_type_ref_index);
             const Symbol* const name = cp->symbol_at(name_ref_index);
-            if (ref_kind == JVM_REF_newInvokeSpecial) {
-              if (name != vmSymbols::object_initializer_name()) {
+            if (name != vmSymbols::object_initializer_name()) {
+              if (ref_kind == JVM_REF_newInvokeSpecial) {
                 classfile_parse_error(
                   "Bad constructor name at constant pool index %u in class file %s",
                     name_ref_index, CHECK);
               }
             } else {
-              if (name == vmSymbols::object_initializer_name()) {
+              // The allowed invocation mode of <init> depends on its signature.
+              // This test corresponds to verify_invoke_instructions in the verifier.
+              const int signature_ref_index =
+                cp->signature_ref_index_at(name_and_type_ref_index);
+              const Symbol* const signature = cp->symbol_at(signature_ref_index);
+              if (signature->is_void_method_signature()
+                  && ref_kind == JVM_REF_newInvokeSpecial) {
+                // OK, could be a constructor call
+              } else if (!signature->is_void_method_signature()
+                         && ref_kind == JVM_REF_invokeStatic) {
+                // also OK, could be a static factory call
+              } else {
                 classfile_parse_error(
                   "Bad method name at constant pool index %u in class file %s",
                   name_ref_index, CHECK);
               }
             }
@@ -919,26 +942,33 @@
 
   return true;
 }
 
 // Side-effects: populates the _local_interfaces field
-void ClassFileParser::parse_interfaces(const ClassFileStream* const stream,
-                                       const int itfs_len,
-                                       ConstantPool* const cp,
+void ClassFileParser::parse_interfaces(const ClassFileStream* stream,
+                                       int itfs_len,
+                                       ConstantPool* cp,
+                                       bool is_inline_type,
                                        bool* const has_nonstatic_concrete_methods,
+                                       // FIXME: lots of these functions
+                                       // declare their parameters as const,
+                                       // which adds only noise to the code.
+                                       // Remove the spurious const modifiers.
+                                       // Many are of the form "const int x"
+                                       // or "T* const x".
+                                       bool* const is_declared_atomic,
                                        TRAPS) {
   assert(stream != NULL, "invariant");
   assert(cp != NULL, "invariant");
   assert(has_nonstatic_concrete_methods != NULL, "invariant");
 
   if (itfs_len == 0) {
-    _local_interfaces = Universe::the_empty_instance_klass_array();
+    _temp_local_interfaces = new GrowableArray<InstanceKlass*>(0);
   } else {
     assert(itfs_len > 0, "only called for len>0");
-    _local_interfaces = MetadataFactory::new_array<InstanceKlass*>(_loader_data, itfs_len, NULL, CHECK);
-
-    int index;
+    _temp_local_interfaces = new GrowableArray<InstanceKlass*>(itfs_len);
+    int index = 0;
     for (index = 0; index < itfs_len; index++) {
       const u2 interface_index = stream->get_u2(CHECK);
       Klass* interf;
       check_property(
         valid_klass_reference_at(interface_index),
@@ -952,11 +982,11 @@
         // Don't need to check legal name because it's checked when parsing constant pool.
         // But need to make sure it's not an array type.
         guarantee_property(unresolved_klass->char_at(0) != JVM_SIGNATURE_ARRAY,
                            "Bad interface name in class file %s", CHECK);
 
-        // Call resolve_super so classcircularity is checked
+        // Call resolve_super so class circularity is checked
         interf = SystemDictionary::resolve_super_or_fail(
                                                   _class_name,
                                                   unresolved_klass,
                                                   Handle(THREAD, _loader_data->class_loader()),
                                                   _protection_domain,
@@ -970,14 +1000,33 @@
                           _class_name->as_klass_external_name(),
                           interf->external_name(),
                           interf->class_in_module_of_loader()));
       }
 
-      if (InstanceKlass::cast(interf)->has_nonstatic_concrete_methods()) {
+      InstanceKlass* ik = InstanceKlass::cast(interf);
+      if (is_inline_type && ik->invalid_inline_super()) {
+        ResourceMark rm(THREAD);
+        Exceptions::fthrow(
+          THREAD_AND_LOCATION,
+          vmSymbols::java_lang_IncompatibleClassChangeError(),
+          "Inline type %s attempts to implement interface java.lang.IdentityObject",
+          _class_name->as_klass_external_name());
+        return;
+      }
+      if (ik->invalid_inline_super()) {
+        set_invalid_inline_super();
+      }
+      if (ik->has_nonstatic_concrete_methods()) {
         *has_nonstatic_concrete_methods = true;
       }
-      _local_interfaces->at_put(index, InstanceKlass::cast(interf));
+      if (ik->is_declared_atomic()) {
+        *is_declared_atomic = true;
+      }
+      if (ik->name() == vmSymbols::java_lang_IdentityObject()) {
+        _implements_identityObject = true;
+      }
+      _temp_local_interfaces->append(ik);
     }
 
     if (!_need_verify || itfs_len <= 1) {
       return;
     }
@@ -991,11 +1040,11 @@
     bool dup = false;
     const Symbol* name = NULL;
     {
       debug_only(NoSafepointVerifier nsv;)
       for (index = 0; index < itfs_len; index++) {
-        const InstanceKlass* const k = _local_interfaces->at(index);
+        const InstanceKlass* const k = _temp_local_interfaces->at(index);
         name = k->name();
         // If no duplicates, add (name, NULL) in hashtable interface_names.
         if (!put_after_lookup(name, NULL, interface_names)) {
           dup = true;
           break;
@@ -1467,15 +1516,17 @@
   STATIC_OOP,           // Oops
   STATIC_BYTE,          // Boolean, Byte, char
   STATIC_SHORT,         // shorts
   STATIC_WORD,          // ints
   STATIC_DOUBLE,        // aligned long or double
+  STATIC_FLATTENABLE,   // flattenable field
   NONSTATIC_OOP,
   NONSTATIC_BYTE,
   NONSTATIC_SHORT,
   NONSTATIC_WORD,
   NONSTATIC_DOUBLE,
+  NONSTATIC_FLATTENABLE,
   MAX_FIELD_ALLOCATION_TYPE,
   BAD_ALLOCATION_TYPE = -1
 };
 
 static FieldAllocationType _basic_type_to_atype[2 * (T_CONFLICT + 1)] = {
@@ -1491,16 +1542,17 @@
   NONSTATIC_SHORT,     // T_SHORT       =  9,
   NONSTATIC_WORD,      // T_INT         = 10,
   NONSTATIC_DOUBLE,    // T_LONG        = 11,
   NONSTATIC_OOP,       // T_OBJECT      = 12,
   NONSTATIC_OOP,       // T_ARRAY       = 13,
-  BAD_ALLOCATION_TYPE, // T_VOID        = 14,
-  BAD_ALLOCATION_TYPE, // T_ADDRESS     = 15,
-  BAD_ALLOCATION_TYPE, // T_NARROWOOP   = 16,
-  BAD_ALLOCATION_TYPE, // T_METADATA    = 17,
-  BAD_ALLOCATION_TYPE, // T_NARROWKLASS = 18,
-  BAD_ALLOCATION_TYPE, // T_CONFLICT    = 19,
+  NONSTATIC_OOP,       // T_VALUETYPE   = 14,
+  BAD_ALLOCATION_TYPE, // T_VOID        = 15,
+  BAD_ALLOCATION_TYPE, // T_ADDRESS     = 16,
+  BAD_ALLOCATION_TYPE, // T_NARROWOOP   = 17,
+  BAD_ALLOCATION_TYPE, // T_METADATA    = 18,
+  BAD_ALLOCATION_TYPE, // T_NARROWKLASS = 19,
+  BAD_ALLOCATION_TYPE, // T_CONFLICT    = 20,
   BAD_ALLOCATION_TYPE, // 0
   BAD_ALLOCATION_TYPE, // 1
   BAD_ALLOCATION_TYPE, // 2
   BAD_ALLOCATION_TYPE, // 3
   STATIC_BYTE ,        // T_BOOLEAN     =  4,
@@ -1511,22 +1563,26 @@
   STATIC_SHORT,        // T_SHORT       =  9,
   STATIC_WORD,         // T_INT         = 10,
   STATIC_DOUBLE,       // T_LONG        = 11,
   STATIC_OOP,          // T_OBJECT      = 12,
   STATIC_OOP,          // T_ARRAY       = 13,
-  BAD_ALLOCATION_TYPE, // T_VOID        = 14,
-  BAD_ALLOCATION_TYPE, // T_ADDRESS     = 15,
-  BAD_ALLOCATION_TYPE, // T_NARROWOOP   = 16,
-  BAD_ALLOCATION_TYPE, // T_METADATA    = 17,
-  BAD_ALLOCATION_TYPE, // T_NARROWKLASS = 18,
-  BAD_ALLOCATION_TYPE, // T_CONFLICT    = 19,
+  STATIC_OOP,          // T_VALUETYPE   = 14,
+  BAD_ALLOCATION_TYPE, // T_VOID        = 15,
+  BAD_ALLOCATION_TYPE, // T_ADDRESS     = 16,
+  BAD_ALLOCATION_TYPE, // T_NARROWOOP   = 17,
+  BAD_ALLOCATION_TYPE, // T_METADATA    = 18,
+  BAD_ALLOCATION_TYPE, // T_NARROWKLASS = 19,
+  BAD_ALLOCATION_TYPE, // T_CONFLICT    = 20
 };
 
-static FieldAllocationType basic_type_to_atype(bool is_static, BasicType type) {
+static FieldAllocationType basic_type_to_atype(bool is_static, BasicType type, bool is_flattenable) {
   assert(type >= T_BOOLEAN && type < T_VOID, "only allowable values");
   FieldAllocationType result = _basic_type_to_atype[type + (is_static ? (T_CONFLICT + 1) : 0)];
   assert(result != BAD_ALLOCATION_TYPE, "bad type");
+  if (is_flattenable) {
+    result = is_static ? STATIC_FLATTENABLE : NONSTATIC_FLATTENABLE;
+  }
   return result;
 }
 
 class ClassFileParser::FieldAllocationCount : public ResourceObj {
  public:
@@ -1536,12 +1592,12 @@
     for (int i = 0; i < MAX_FIELD_ALLOCATION_TYPE; i++) {
       count[i] = 0;
     }
   }
 
-  FieldAllocationType update(bool is_static, BasicType type) {
-    FieldAllocationType atype = basic_type_to_atype(is_static, type);
+  FieldAllocationType update(bool is_static, BasicType type, bool is_flattenable) {
+    FieldAllocationType atype = basic_type_to_atype(is_static, type, is_flattenable);
     if (atype != BAD_ALLOCATION_TYPE) {
       // Make sure there is no overflow with injected fields.
       assert(count[atype] < 0xFFFF, "More than 65535 fields");
       count[atype]++;
     }
@@ -1551,10 +1607,11 @@
 
 // Side-effects: populates the _fields, _fields_annotations,
 // _fields_type_annotations fields
 void ClassFileParser::parse_fields(const ClassFileStream* const cfs,
                                    bool is_interface,
+                                   bool is_inline_type,
                                    FieldAllocationCount* const fac,
                                    ConstantPool* cp,
                                    const int cp_size,
                                    u2* const java_fields_count_ptr,
                                    TRAPS) {
@@ -1573,11 +1630,15 @@
   *java_fields_count_ptr = length;
 
   int num_injected = 0;
   const InjectedField* const injected = JavaClasses::get_injected(_class_name,
                                                                   &num_injected);
-  const int total_fields = length + num_injected;
+
+  // two more slots are required for inline classes:
+  // one for the static field with a reference to the pre-allocated default value
+  // one for the field the JVM injects when detecting an empty inline class
+  const int total_fields = length + num_injected + (is_inline_type ? 2 : 0);
 
   // The field array starts with tuples of shorts
   // [access, name index, sig index, initial value index, byte offset].
   // A generic signature slot only exists for field with generic
   // signature attribute. And the access flag is set with
@@ -1603,17 +1664,20 @@
                                               total_fields * (FieldInfo::field_slots + 1));
 
   // The generic signature slots start after all other fields' data.
   int generic_signature_slot = total_fields * FieldInfo::field_slots;
   int num_generic_signature = 0;
+  int instance_fields_count = 0;
   for (int n = 0; n < length; n++) {
     // access_flags, name_index, descriptor_index, attributes_count
     cfs->guarantee_more(8, CHECK);
 
+    jint recognized_modifiers = JVM_RECOGNIZED_FIELD_MODIFIERS;
+
+    const jint flags = cfs->get_u2_fast() & recognized_modifiers;
+    verify_legal_field_modifiers(flags, is_interface, is_inline_type, CHECK);
     AccessFlags access_flags;
-    const jint flags = cfs->get_u2_fast() & JVM_RECOGNIZED_FIELD_MODIFIERS;
-    verify_legal_field_modifiers(flags, is_interface, CHECK);
     access_flags.set_flags(flags);
 
     const u2 name_index = cfs->get_u2_fast();
     check_property(valid_symbol_at(name_index),
       "Invalid constant pool index %u for field name in class file %s",
@@ -1625,10 +1689,27 @@
     check_property(valid_symbol_at(signature_index),
       "Invalid constant pool index %u for field signature in class file %s",
       signature_index, CHECK);
     const Symbol* const sig = cp->symbol_at(signature_index);
     verify_legal_field_signature(name, sig, CHECK);
+    assert(!access_flags.is_flattenable(), "ACC_FLATTENABLE should have been filtered out");
+    if (sig->is_Q_signature()) {
+      // assert(_major_version >= CONSTANT_CLASS_DESCRIPTORS, "Q-descriptors are only supported in recent classfiles");
+      access_flags.set_is_flattenable();
+    }
+    if (access_flags.is_flattenable()) {
+      // Array flattenability cannot be specified.  Arrays of value classes are
+      // are always flattenable.  Arrays of other classes are not flattenable.
+      if (sig->utf8_length() > 1 && sig->char_at(0) == '[') {
+        classfile_parse_error(
+            "Field \"%s\" with signature \"%s\" in class file %s is invalid."
+            " ACC_FLATTENABLE cannot be specified for an array",
+            name->as_C_string(), sig->as_klass_external_name(), CHECK);
+      }
+      _has_flattenable_fields = true;
+    }
+    if (!access_flags.is_static()) instance_fields_count++;
 
     u2 constantvalue_index = 0;
     bool is_synthetic = false;
     u2 generic_signature_index = 0;
     const bool is_static = access_flags.is_static();
@@ -1684,11 +1765,11 @@
                       signature_index,
                       constantvalue_index);
     const BasicType type = cp->basic_type_for_signature_at(signature_index);
 
     // Remember how many oops we encountered and compute allocation type
-    const FieldAllocationType atype = fac->update(is_static, type);
+    const FieldAllocationType atype = fac->update(is_static, type, access_flags.is_flattenable());
     field->set_allocation_type(atype);
 
     // After field is initialized with type, we can augment it with aux info
     if (parsed_annotations.has_any_annotations()) {
       parsed_annotations.apply_to(field);
@@ -1729,16 +1810,45 @@
                         0);
 
       const BasicType type = Signature::basic_type(injected[n].signature());
 
       // Remember how many oops we encountered and compute allocation type
-      const FieldAllocationType atype = fac->update(false, type);
+      const FieldAllocationType atype = fac->update(false, type, false);
       field->set_allocation_type(atype);
       index++;
     }
   }
 
+  if (is_inline_type) {
+    FieldInfo* const field = FieldInfo::from_field_array(fa, index);
+    field->initialize(JVM_ACC_FIELD_INTERNAL | JVM_ACC_STATIC,
+                      vmSymbols::default_value_name_enum,
+                      vmSymbols::object_signature_enum,
+                      0);
+    const BasicType type = Signature::basic_type(vmSymbols::object_signature());
+    const FieldAllocationType atype = fac->update(true, type, false);
+    field->set_allocation_type(atype);
+    index++;
+  }
+
+  if (is_inline_type && instance_fields_count == 0) {
+    _is_empty_inline_type = true;
+    FieldInfo* const field = FieldInfo::from_field_array(fa, index);
+    field->initialize(JVM_ACC_FIELD_INTERNAL,
+        vmSymbols::empty_marker_name_enum,
+        vmSymbols::byte_signature_enum,
+        0);
+    const BasicType type = Signature::basic_type(vmSymbols::byte_signature());
+    const FieldAllocationType atype = fac->update(false, type, false);
+    field->set_allocation_type(atype);
+    index++;
+  }
+
+  if (instance_fields_count > 0) {
+    _has_nonstatic_fields = true;
+  }
+
   assert(NULL == _fields, "invariant");
 
   _fields =
     MetadataFactory::new_array<u2>(_loader_data,
                                    index * FieldInfo::field_slots + num_generic_signature,
@@ -2050,15 +2160,20 @@
                                             const Symbol* sig,
                                             TRAPS) const {
   assert(name != NULL, "invariant");
   assert(sig != NULL, "invariant");
 
+  const char* class_note = "";
+  if (is_inline_type() && name == vmSymbols::object_initializer_name()) {
+    class_note = " (an inline class)";
+  }
+
   ResourceMark rm(THREAD);
   Exceptions::fthrow(THREAD_AND_LOCATION,
       vmSymbols::java_lang_ClassFormatError(),
-      "%s \"%s\" in class %s has illegal signature \"%s\"", type,
-      name->as_C_string(), _class_name->as_C_string(), sig->as_C_string());
+      "%s \"%s\" in class %s%s has illegal signature \"%s\"", type,
+      name->as_C_string(), _class_name->as_C_string(), class_note, sig->as_C_string());
 }
 
 AnnotationCollector::ID
 AnnotationCollector::annotation_index(const ClassLoaderData* loader_data,
                                       const Symbol* name,
@@ -2319,10 +2434,11 @@
 // from the method back up to the containing klass. These flag values
 // are added to klass's access_flags.
 
 Method* ClassFileParser::parse_method(const ClassFileStream* const cfs,
                                       bool is_interface,
+                                      bool is_inline_type,
                                       const ConstantPool* cp,
                                       AccessFlags* const promoted_flags,
                                       TRAPS) {
   assert(cfs != NULL, "invariant");
   assert(cp != NULL, "invariant");
@@ -2359,15 +2475,57 @@
       flags &= JVM_ACC_STATIC | JVM_ACC_STRICT;
     } else {
       classfile_parse_error("Method <clinit> is not static in class file %s", CHECK_NULL);
     }
   } else {
-    verify_legal_method_modifiers(flags, is_interface, name, CHECK_NULL);
-  }
-
-  if (name == vmSymbols::object_initializer_name() && is_interface) {
-    classfile_parse_error("Interface cannot have a method named <init>, class file %s", CHECK_NULL);
+    verify_legal_method_modifiers(flags, is_interface, is_inline_type, name, CHECK_NULL);
+  }
+
+  if (name == vmSymbols::object_initializer_name()) {
+    if (is_interface) {
+      classfile_parse_error("Interface cannot have a method named <init>, class file %s", CHECK_NULL);
+    } else if (!is_inline_type && signature->is_void_method_signature()) {
+      // OK, a constructor
+    } else if (is_inline_type && !signature->is_void_method_signature()) {
+      // also OK, a static factory, as long as the return value is good
+      bool ok = false;
+      SignatureStream ss((Symbol*) signature, true);
+      while (!ss.at_return_type())  ss.next();
+      if (ss.is_reference()) {
+        Symbol* ret = ss.as_symbol();
+        const Symbol* required = class_name();
+        if (is_unsafe_anonymous()) {
+          // The original class name in the UAC byte stream gets changed.  So
+          // using the original name in the return type is no longer valid.
+          required = vmSymbols::java_lang_Object();
+        }
+        ok = (ret == required);
+      }
+      if (!ok) {
+        throwIllegalSignature("Method", name, signature, CHECK_0);
+      }
+    } else {
+      // not OK, so throw the same error as in verify_legal_method_signature.
+      throwIllegalSignature("Method", name, signature, CHECK_0);
+    }
+    // A declared <init> method must always be either a non-static
+    // object constructor, with a void return, or else it must be a
+    // static factory method, with a non-void return.  No other
+    // definition of <init> is possible.
+    //
+    // The verifier (in verify_invoke_instructions) will inspect the
+    // signature of any attempt to invoke <init>, and ensures that it
+    // returns non-void if and only if it is being invoked by
+    // invokestatic, and void if and only if it is being invoked by
+    // invokespecial.
+    //
+    // When a symbolic reference to <init> is resolved for a
+    // particular invocation mode (special or static), the mode is
+    // matched to the JVM_ACC_STATIC modifier of the <init> method.
+    // Thus, it is impossible to statically invoke a constructor, and
+    // impossible to "new + invokespecial" a static factory, either
+    // through bytecode or through reflection.
   }
 
   int args_size = -1;  // only used when _need_verify is true
   if (_need_verify) {
     args_size = ((flags & JVM_ACC_STATIC) ? 0 : 1) +
@@ -2914,10 +3072,11 @@
 // from the methods back up to the containing klass. These flag values
 // are added to klass's access_flags.
 // Side-effects: populates the _methods field in the parser
 void ClassFileParser::parse_methods(const ClassFileStream* const cfs,
                                     bool is_interface,
+                                    bool is_inline_type,
                                     AccessFlags* promoted_flags,
                                     bool* has_final_method,
                                     bool* declares_nonstatic_concrete_methods,
                                     TRAPS) {
   assert(cfs != NULL, "invariant");
@@ -2938,10 +3097,11 @@
                                                    CHECK);
 
     for (int index = 0; index < length; index++) {
       Method* method = parse_method(cfs,
                                     is_interface,
+                                    is_inline_type,
                                     _cp,
                                     promoted_flags,
                                     CHECK);
 
       if (method->is_final()) {
@@ -3130,18 +3290,24 @@
       inner_name_index, CHECK_0);
     if (_need_verify) {
       guarantee_property(inner_class_info_index != outer_class_info_index,
                          "Class is both outer and inner class in class file %s", CHECK_0);
     }
-    // Access flags
-    jint flags;
+
+    jint recognized_modifiers = RECOGNIZED_INNER_CLASS_MODIFIERS;
     // JVM_ACC_MODULE is defined in JDK-9 and later.
     if (_major_version >= JAVA_9_VERSION) {
-      flags = cfs->get_u2_fast() & (RECOGNIZED_INNER_CLASS_MODIFIERS | JVM_ACC_MODULE);
-    } else {
-      flags = cfs->get_u2_fast() & RECOGNIZED_INNER_CLASS_MODIFIERS;
+      recognized_modifiers |= JVM_ACC_MODULE;
     }
+    // JVM_ACC_VALUE is defined for class file version 55 and later
+    if (supports_inline_types()) {
+      recognized_modifiers |= JVM_ACC_VALUE;
+    }
+
+    // Access flags
+    jint flags = cfs->get_u2_fast() & recognized_modifiers;
+
     if ((flags & JVM_ACC_INTERFACE) && _major_version < JAVA_6_VERSION) {
       // Set abstract bit for old class files for backward compatibility
       flags |= JVM_ACC_ABSTRACT;
     }
     verify_legal_class_modifiers(flags, CHECK_0);
@@ -3918,11 +4084,12 @@
                                                         TRAPS) {
   assert(cp != NULL, "invariant");
   const InstanceKlass* super_klass = NULL;
 
   if (super_class_index == 0) {
-    check_property(_class_name == vmSymbols::java_lang_Object(),
+    check_property(_class_name == vmSymbols::java_lang_Object()
+                   || (_access_flags.get_flags() & JVM_ACC_VALUE),
                    "Invalid superclass index %u in class file %s",
                    super_class_index,
                    CHECK_NULL);
   } else {
     check_property(valid_klass_reference_at(super_class_index),
@@ -4098,10 +4265,29 @@
 
 void OopMapBlocksBuilder::print_value_on(outputStream* st) const {
   print_on(st);
 }
 
+void ClassFileParser::throwInlineTypeLimitation(THREAD_AND_LOCATION_DECL,
+                                                const char* msg,
+                                                const Symbol* name,
+                                                const Symbol* sig) const {
+
+  ResourceMark rm(THREAD);
+  if (name == NULL || sig == NULL) {
+    Exceptions::fthrow(THREAD_AND_LOCATION_ARGS,
+        vmSymbols::java_lang_ClassFormatError(),
+        "class: %s - %s", _class_name->as_C_string(), msg);
+  }
+  else {
+    Exceptions::fthrow(THREAD_AND_LOCATION_ARGS,
+        vmSymbols::java_lang_ClassFormatError(),
+        "\"%s\" sig: \"%s\" class: %s - %s", name->as_C_string(), sig->as_C_string(),
+        _class_name->as_C_string(), msg);
+  }
+}
+
 // Layout fields and fill in FieldLayoutInfo.  Could use more refactoring!
 void ClassFileParser::layout_fields(ConstantPool* cp,
                                     const FieldAllocationCount* fac,
                                     const ClassAnnotationCollector* parsed_annotations,
                                     FieldLayoutInfo* info,
@@ -4110,10 +4296,16 @@
   assert(cp != NULL, "invariant");
 
   // Field size and offset computation
   int nonstatic_field_size = _super_klass == NULL ? 0 :
                                _super_klass->nonstatic_field_size();
+  int next_nonstatic_inline_type_offset = 0;
+  int first_nonstatic_inline_type_offset = 0;
+
+  // Fields that are inline types are handled differently depending if they are static or not:
+  // - static fields are oops
+  // - non-static fields are embedded
 
   // Count the contended fields by type.
   //
   // We ignore static fields, because @Contended is not supported for them.
   // The layout code below will also ignore the static fields.
@@ -4130,12 +4322,13 @@
   }
 
 
   // Calculate the starting byte offsets
   int next_static_oop_offset    = InstanceMirrorKlass::offset_of_static_fields();
+  // Inline types in static fields are not embedded, they are handled with oops
   int next_static_double_offset = next_static_oop_offset +
-                                      ((fac->count[STATIC_OOP]) * heapOopSize);
+                                  ((fac->count[STATIC_OOP] + fac->count[STATIC_FLATTENABLE]) * heapOopSize);
   if (fac->count[STATIC_DOUBLE]) {
     next_static_double_offset = align_up(next_static_double_offset, BytesPerLong);
   }
 
   int next_static_word_offset   = next_static_double_offset +
@@ -4146,50 +4339,151 @@
                                   ((fac->count[STATIC_SHORT]) * BytesPerShort);
 
   int nonstatic_fields_start  = instanceOopDesc::base_offset_in_bytes() +
                                 nonstatic_field_size * heapOopSize;
 
+  // First field of inline types is aligned on a long boundary in order to ease
+  // in-lining of inline types (with header removal) in packed arrays and
+  // flatten inline types
+  int initial_inline_type_padding = 0;
+  if (is_inline_type()) {
+    int old = nonstatic_fields_start;
+    nonstatic_fields_start = align_up(nonstatic_fields_start, BytesPerLong);
+    initial_inline_type_padding = nonstatic_fields_start - old;
+  }
+
   int next_nonstatic_field_offset = nonstatic_fields_start;
 
   const bool is_contended_class     = parsed_annotations->is_contended();
 
   // Class is contended, pad before all the fields
   if (is_contended_class) {
     next_nonstatic_field_offset += ContendedPaddingWidth;
   }
 
+  // Temporary inline types restrictions
+  if (is_inline_type()) {
+    if (is_contended_class) {
+      throwInlineTypeLimitation(THREAD_AND_LOCATION, "Inline Types do not support @Contended annotation yet");
+      return;
+    }
+  }
+
   // Compute the non-contended fields count.
   // The packing code below relies on these counts to determine if some field
   // can be squeezed into the alignment gap. Contended fields are obviously
   // exempt from that.
   unsigned int nonstatic_double_count = fac->count[NONSTATIC_DOUBLE] - fac_contended.count[NONSTATIC_DOUBLE];
   unsigned int nonstatic_word_count   = fac->count[NONSTATIC_WORD]   - fac_contended.count[NONSTATIC_WORD];
   unsigned int nonstatic_short_count  = fac->count[NONSTATIC_SHORT]  - fac_contended.count[NONSTATIC_SHORT];
   unsigned int nonstatic_byte_count   = fac->count[NONSTATIC_BYTE]   - fac_contended.count[NONSTATIC_BYTE];
   unsigned int nonstatic_oop_count    = fac->count[NONSTATIC_OOP]    - fac_contended.count[NONSTATIC_OOP];
 
+  int static_inline_type_count = 0;
+  int nonstatic_inline_type_count = 0;
+  int* nonstatic_inline_type_indexes = NULL;
+  Klass** nonstatic_inline_type_klasses = NULL;
+  unsigned int inline_type_oop_map_count = 0;
+  int not_flattened_inline_types = 0;
+  int not_atomic_inline_types = 0;
+
+  int max_nonstatic_inline_type = fac->count[NONSTATIC_FLATTENABLE] + 1;
+
+  nonstatic_inline_type_indexes = NEW_RESOURCE_ARRAY_IN_THREAD(THREAD, int,
+                                                               max_nonstatic_inline_type);
+  for (int i = 0; i < max_nonstatic_inline_type; i++) {
+    nonstatic_inline_type_indexes[i] = -1;
+  }
+  nonstatic_inline_type_klasses = NEW_RESOURCE_ARRAY_IN_THREAD(THREAD, Klass*,
+                                                               max_nonstatic_inline_type);
+
+  for (AllFieldStream fs(_fields, _cp); !fs.done(); fs.next()) {
+    if (fs.allocation_type() == STATIC_FLATTENABLE) {
+      ResourceMark rm;
+      if (!fs.signature()->is_Q_signature()) {
+        THROW(vmSymbols::java_lang_ClassFormatError());
+      }
+      static_inline_type_count++;
+    } else if (fs.allocation_type() == NONSTATIC_FLATTENABLE) {
+      // Pre-resolve the flattenable field and check for inline type circularity issues.
+      ResourceMark rm;
+      if (!fs.signature()->is_Q_signature()) {
+        THROW(vmSymbols::java_lang_ClassFormatError());
+      }
+      Klass* klass =
+        SystemDictionary::resolve_flattenable_field_or_fail(&fs,
+                                                            Handle(THREAD, _loader_data->class_loader()),
+                                                            _protection_domain, true, CHECK);
+      assert(klass != NULL, "Sanity check");
+      if (!klass->access_flags().is_inline_type()) {
+        THROW(vmSymbols::java_lang_IncompatibleClassChangeError());
+      }
+      ValueKlass* vk = ValueKlass::cast(klass);
+      // Conditions to apply flattening or not should be defined in a single place
+      bool too_big_to_flatten = (InlineFieldMaxFlatSize >= 0 &&
+                                 (vk->size_helper() * HeapWordSize) > InlineFieldMaxFlatSize);
+      bool too_atomic_to_flatten = vk->is_declared_atomic();
+      bool too_volatile_to_flatten = fs.access_flags().is_volatile();
+      if (vk->is_naturally_atomic()) {
+        too_atomic_to_flatten = false;
+        //too_volatile_to_flatten = false; //FIXME
+        // volatile fields are currently never flattened, this could change in the future
+      }
+      if (!(too_big_to_flatten | too_atomic_to_flatten | too_volatile_to_flatten)) {
+        nonstatic_inline_type_indexes[nonstatic_inline_type_count] = fs.index();
+        nonstatic_inline_type_klasses[nonstatic_inline_type_count] = klass;
+        nonstatic_inline_type_count++;
+
+        ValueKlass* vklass = ValueKlass::cast(klass);
+        if (vklass->contains_oops()) {
+          inline_type_oop_map_count += vklass->nonstatic_oop_map_count();
+        }
+        fs.set_flattened(true);
+        if (!vk->is_atomic()) {  // flat and non-atomic: take note
+          not_atomic_inline_types++;
+        }
+      } else {
+        not_flattened_inline_types++;
+        fs.set_flattened(false);
+      }
+    }
+  }
+
+  // Adjusting non_static_oop_count to take into account not flattened inline types;
+  nonstatic_oop_count += not_flattened_inline_types;
+
   // Total non-static fields count, including every contended field
   unsigned int nonstatic_fields_count = fac->count[NONSTATIC_DOUBLE] + fac->count[NONSTATIC_WORD] +
                                         fac->count[NONSTATIC_SHORT] + fac->count[NONSTATIC_BYTE] +
-                                        fac->count[NONSTATIC_OOP];
+                                        fac->count[NONSTATIC_OOP] + fac->count[NONSTATIC_FLATTENABLE];
 
   const bool super_has_nonstatic_fields =
           (_super_klass != NULL && _super_klass->has_nonstatic_fields());
   const bool has_nonstatic_fields =
     super_has_nonstatic_fields || (nonstatic_fields_count != 0);
+  const bool has_nonstatic_value_fields = nonstatic_inline_type_count > 0;
 
+  if (is_inline_type() && (!has_nonstatic_fields)) {
+    // There are a number of fixes required throughout the type system and JIT
+    throwInlineTypeLimitation(THREAD_AND_LOCATION, "Inline Types do not support zero instance size yet");
+    return;
+  }
 
   // Prepare list of oops for oop map generation.
   //
   // "offset" and "count" lists are describing the set of contiguous oop
   // regions. offset[i] is the start of the i-th region, which then has
   // count[i] oops following. Before we know how many regions are required,
   // we pessimistically allocate the maps to fit all the oops into the
   // distinct regions.
-
+  //
   int super_oop_map_count = (_super_klass == NULL) ? 0 :_super_klass->nonstatic_oop_map_count();
-  int max_oop_map_count = super_oop_map_count + fac->count[NONSTATIC_OOP];
+  int max_oop_map_count =
+      super_oop_map_count +
+      fac->count[NONSTATIC_OOP] +
+      inline_type_oop_map_count +
+      not_flattened_inline_types;
 
   OopMapBlocksBuilder* nonstatic_oop_maps = new OopMapBlocksBuilder(max_oop_map_count);
   if (super_oop_map_count > 0) {
     nonstatic_oop_maps->initialize_inherited_blocks(_super_klass->start_of_nonstatic_oop_maps(),
                                                     _super_klass->nonstatic_oop_map_count());
@@ -4280,10 +4574,20 @@
       next_nonstatic_oop_offset = align_up(next_nonstatic_oop_offset, heapOopSize);
     }
     next_nonstatic_padded_offset = next_nonstatic_oop_offset + (nonstatic_oop_count * heapOopSize);
   }
 
+  // Aligning embedded inline types
+  // bug below, the current algorithm to layout embedded inline types always put them at the
+  // end of the layout, which doesn't match the different allocation policies the VM is
+  // supposed to provide => FixMe
+  // Note also that the current alignment policy is to make each inline type starting on a
+  // 64 bits boundary. This could be optimized later. For instance, it could be nice to
+  // align inline types according to their most constrained internal type.
+  next_nonstatic_inline_type_offset = align_up(next_nonstatic_padded_offset, BytesPerLong);
+  int next_inline_type_index = 0;
+
   // Iterate over fields again and compute correct offsets.
   // The field allocation type was temporarily stored in the offset slot.
   // oop fields are located before non-oop fields (static and non-static).
   for (AllFieldStream fs(_fields, cp); !fs.done(); fs.next()) {
 
@@ -4296,10 +4600,12 @@
     int real_offset = 0;
     const FieldAllocationType atype = (const FieldAllocationType) fs.allocation_type();
 
     // pack the rest of the fields
     switch (atype) {
+      // Inline types in static fields are handled with oops
+      case STATIC_FLATTENABLE:   // Fallthrough
       case STATIC_OOP:
         real_offset = next_static_oop_offset;
         next_static_oop_offset += heapOopSize;
         break;
       case STATIC_BYTE:
@@ -4316,10 +4622,35 @@
         break;
       case STATIC_DOUBLE:
         real_offset = next_static_double_offset;
         next_static_double_offset += BytesPerLong;
         break;
+      case NONSTATIC_FLATTENABLE:
+        if (fs.is_flattened()) {
+          Klass* klass = nonstatic_inline_type_klasses[next_inline_type_index];
+          assert(klass != NULL, "Klass should have been loaded and resolved earlier");
+          assert(klass->access_flags().is_inline_type(),"Must be an inline type");
+          ValueKlass* vklass = ValueKlass::cast(klass);
+          real_offset = next_nonstatic_inline_type_offset;
+          next_nonstatic_inline_type_offset += (vklass->size_helper()) * wordSize - vklass->first_field_offset();
+          // aligning next inline type on a 64 bits boundary
+          next_nonstatic_inline_type_offset = align_up(next_nonstatic_inline_type_offset, BytesPerLong);
+          next_inline_type_index += 1;
+
+          if (vklass->contains_oops()) { // add flatten oop maps
+            int diff = real_offset - vklass->first_field_offset();
+            const OopMapBlock* map = vklass->start_of_nonstatic_oop_maps();
+            const OopMapBlock* const last_map = map + vklass->nonstatic_oop_map_count();
+            while (map < last_map) {
+              nonstatic_oop_maps->add(map->offset() + diff, map->count());
+              map++;
+            }
+          }
+          break;
+        } else {
+          // Fall through
+        }
       case NONSTATIC_OOP:
         if( nonstatic_oop_space_count > 0 ) {
           real_offset = nonstatic_oop_space_offset;
           nonstatic_oop_space_offset += heapOopSize;
           nonstatic_oop_space_count  -= 1;
@@ -4434,10 +4765,16 @@
             next_nonstatic_padded_offset = align_up(next_nonstatic_padded_offset, BytesPerLong);
             real_offset = next_nonstatic_padded_offset;
             next_nonstatic_padded_offset += BytesPerLong;
             break;
 
+            // Inline types in static fields are handled with oops
+          case NONSTATIC_FLATTENABLE:
+            throwInlineTypeLimitation(THREAD_AND_LOCATION,
+                                      "@Contended annotation not supported for inline types yet", fs.name(), fs.signature());
+            return;
+
           case NONSTATIC_OOP:
             next_nonstatic_padded_offset = align_up(next_nonstatic_padded_offset, heapOopSize);
             real_offset = next_nonstatic_padded_offset;
             next_nonstatic_padded_offset += heapOopSize;
             nonstatic_oop_maps->add(real_offset, 1);
@@ -4473,16 +4810,28 @@
 
   // Entire class is contended, pad in the back.
   // This helps to alleviate memory contention effects for subclass fields
   // and/or adjacent object.
   if (is_contended_class) {
+    assert(!is_inline_type(), "@Contended not supported for inline types yet");
     next_nonstatic_padded_offset += ContendedPaddingWidth;
   }
 
-  int notaligned_nonstatic_fields_end = next_nonstatic_padded_offset;
+  int notaligned_nonstatic_fields_end;
+  if (nonstatic_inline_type_count != 0) {
+    notaligned_nonstatic_fields_end = next_nonstatic_inline_type_offset;
+  } else {
+    notaligned_nonstatic_fields_end = next_nonstatic_padded_offset;
+  }
 
-  int nonstatic_fields_end      = align_up(notaligned_nonstatic_fields_end, heapOopSize);
+  int nonstatic_field_sz_align = heapOopSize;
+  if (is_inline_type()) {
+    if ((notaligned_nonstatic_fields_end - nonstatic_fields_start) > heapOopSize) {
+      nonstatic_field_sz_align = BytesPerLong; // value copy of fields only uses jlong copy
+    }
+  }
+  int nonstatic_fields_end      = align_up(notaligned_nonstatic_fields_end, nonstatic_field_sz_align);
   int instance_end              = align_up(notaligned_nonstatic_fields_end, wordSize);
   int static_fields_end         = align_up(next_static_byte_offset, wordSize);
 
   int static_field_size         = (static_fields_end -
                                    InstanceMirrorKlass::offset_of_static_fields()) / wordSize;
@@ -4490,12 +4839,13 @@
                                   (nonstatic_fields_end - nonstatic_fields_start) / heapOopSize;
 
   int instance_size             = align_object_size(instance_end / wordSize);
 
   assert(instance_size == align_object_size(align_up(
-         (instanceOopDesc::base_offset_in_bytes() + nonstatic_field_size*heapOopSize),
-          wordSize) / wordSize), "consistent layout helper value");
+         (instanceOopDesc::base_offset_in_bytes() + nonstatic_field_size*heapOopSize)
+         + initial_inline_type_padding, wordSize) / wordSize), "consistent layout helper value");
+
 
   // Invariant: nonstatic_field end/start should only change if there are
   // nonstatic fields in the class, or if the class is contended. We compare
   // against the non-aligned value, so that end alignment will not fail the
   // assert without actually having the fields.
@@ -4505,27 +4855,48 @@
 
   // Number of non-static oop map blocks allocated at end of klass.
   nonstatic_oop_maps->compact();
 
 #ifndef PRODUCT
-  if (PrintFieldLayout) {
+  if ((PrintFieldLayout && !is_inline_type()) ||
+      (PrintInlineLayout && (is_inline_type() || has_nonstatic_value_fields))) {
     print_field_layout(_class_name,
           _fields,
           cp,
           instance_size,
           nonstatic_fields_start,
           nonstatic_fields_end,
           static_fields_end);
+    nonstatic_oop_maps->print_on(tty);
+    tty->print("\n");
+    tty->print_cr("Instance size = %d", instance_size);
+    tty->print_cr("Nonstatic_field_size = %d", nonstatic_field_size);
+    tty->print_cr("Static_field_size = %d", static_field_size);
+    tty->print_cr("Has nonstatic fields = %d", has_nonstatic_fields);
+    tty->print_cr("---");
   }
 
 #endif
   // Pass back information needed for InstanceKlass creation
   info->oop_map_blocks = nonstatic_oop_maps;
   info->_instance_size = instance_size;
   info->_static_field_size = static_field_size;
   info->_nonstatic_field_size = nonstatic_field_size;
   info->_has_nonstatic_fields = has_nonstatic_fields;
+
+  // An inline type is naturally atomic if it has just one field, and
+  // that field is simple enough.
+  info->_is_naturally_atomic = (is_inline_type() &&
+                                !super_has_nonstatic_fields &&
+                                (nonstatic_fields_count <= 1) &&
+                                (not_atomic_inline_types == 0) &&
+                                (nonstatic_contended_count == 0));
+  // This may be too restrictive, since if all the fields fit in 64
+  // bits we could make the decision to align instances of this class
+  // to 64-bit boundaries, and load and store them as single words.
+  // And on machines which supported larger atomics we could similarly
+  // allow larger values to be atomic, if properly aligned.
 }
 
 void ClassFileParser::set_precomputed_flags(InstanceKlass* ik) {
   assert(ik != NULL, "invariant");
 
@@ -4557,10 +4928,15 @@
 #endif
 
   // Check if this klass supports the java.lang.Cloneable interface
   if (SystemDictionary::Cloneable_klass_loaded()) {
     if (ik->is_subtype_of(SystemDictionary::Cloneable_klass())) {
+      if (ik->is_value()) {
+        Thread *THREAD = Thread::current();
+        throwInlineTypeLimitation(THREAD_AND_LOCATION, "Inline Types do not support Cloneable");
+        return;
+      }
       ik->set_is_cloneable();
     }
   }
 
   // Check if this klass has a vanilla default constructor
@@ -4597,10 +4973,15 @@
     const jint lh = Klass::instance_layout_helper(ik->size_helper(), true);
     ik->set_layout_helper(lh);
   }
 }
 
+bool ClassFileParser::supports_inline_types() const {
+  // Inline types are only supported by class file version 55 and later
+  return _major_version >= JAVA_11_VERSION;
+}
+
 // utility methods for appending an array with check for duplicates
 
 static void append_interfaces(GrowableArray<InstanceKlass*>* result,
                               const Array<InstanceKlass*>* const ifs) {
   // iterate over new interfaces
@@ -4640,13 +5021,14 @@
     // no interfaces, use canonicalized array
     return Universe::the_empty_instance_klass_array();
   } else if (max_transitive_size == super_size) {
     // no new local interfaces added, share superklass' transitive interface array
     return super->transitive_interfaces();
-  } else if (max_transitive_size == local_size) {
-    // only local interfaces added, share local interface array
-    return local_ifs;
+    // The three lines below are commented to work around bug JDK-8245487
+//  } else if (max_transitive_size == local_size) {
+//    // only local interfaces added, share local interface array
+//    return local_ifs;
   } else {
     ResourceMark rm;
     GrowableArray<InstanceKlass*>* const result = new GrowableArray<InstanceKlass*>(max_transitive_size);
 
     // Copy down from superclass
@@ -4663,10 +5045,15 @@
     append_interfaces(result, local_ifs);
 
     // length will be less than the max_transitive_size if duplicates were removed
     const int length = result->length();
     assert(length <= max_transitive_size, "just checking");
+
+    if (length == 1 && result->at(0) == SystemDictionary::IdentityObject_klass()) {
+      return Universe::the_single_IdentityObject_klass_array();
+    }
+
     Array<InstanceKlass*>* const new_result =
       MetadataFactory::new_array<InstanceKlass*>(loader_data, length, CHECK_NULL);
     for (int i = 0; i < length; i++) {
       InstanceKlass* const e = result->at(i);
       assert(e != NULL, "just checking");
@@ -4860,21 +5247,33 @@
 
 // utility methods for format checking
 
 void ClassFileParser::verify_legal_class_modifiers(jint flags, TRAPS) const {
   const bool is_module = (flags & JVM_ACC_MODULE) != 0;
+  const bool is_inline_type = (flags & JVM_ACC_VALUE) != 0;
   assert(_major_version >= JAVA_9_VERSION || !is_module, "JVM_ACC_MODULE should not be set");
+  assert(supports_inline_types() || !is_inline_type, "JVM_ACC_VALUE should not be set");
   if (is_module) {
     ResourceMark rm(THREAD);
     Exceptions::fthrow(
       THREAD_AND_LOCATION,
       vmSymbols::java_lang_NoClassDefFoundError(),
       "%s is not a class because access_flag ACC_MODULE is set",
       _class_name->as_C_string());
     return;
   }
 
+  if (is_inline_type && !EnableValhalla) {
+    ResourceMark rm(THREAD);
+    Exceptions::fthrow(
+      THREAD_AND_LOCATION,
+      vmSymbols::java_lang_ClassFormatError(),
+      "Class modifier ACC_VALUE in class %s requires option -XX:+EnableValhalla",
+      _class_name->as_C_string()
+    );
+  }
+
   if (!_need_verify) { return; }
 
   const bool is_interface  = (flags & JVM_ACC_INTERFACE)  != 0;
   const bool is_abstract   = (flags & JVM_ACC_ABSTRACT)   != 0;
   const bool is_final      = (flags & JVM_ACC_FINAL)      != 0;
@@ -4885,17 +5284,20 @@
   const bool major_gte_14  = _major_version >= JAVA_14_VERSION;
 
   if ((is_abstract && is_final) ||
       (is_interface && !is_abstract) ||
       (is_interface && major_gte_1_5 && (is_super || is_enum)) ||
-      (!is_interface && major_gte_1_5 && is_annotation)) {
+      (!is_interface && major_gte_1_5 && is_annotation) ||
+      (is_inline_type && (is_interface || is_abstract || is_enum || !is_final))) {
     ResourceMark rm(THREAD);
+    const char* class_note = "";
+    if (is_inline_type)  class_note = " (an inline class)";
     Exceptions::fthrow(
       THREAD_AND_LOCATION,
       vmSymbols::java_lang_ClassFormatError(),
-      "Illegal class modifiers in class %s: 0x%X",
-      _class_name->as_C_string(), flags
+      "Illegal class modifiers in class %s%s: 0x%X",
+      _class_name->as_C_string(), class_note, flags
     );
     return;
   }
 }
 
@@ -4970,10 +5372,11 @@
   }
 }
 
 void ClassFileParser::verify_legal_field_modifiers(jint flags,
                                                    bool is_interface,
+                                                   bool is_inline_type,
                                                    TRAPS) const {
   if (!_need_verify) { return; }
 
   const bool is_public    = (flags & JVM_ACC_PUBLIC)    != 0;
   const bool is_protected = (flags & JVM_ACC_PROTECTED) != 0;
@@ -4994,10 +5397,14 @@
       is_illegal = true;
     }
   } else { // not interface
     if (has_illegal_visibility(flags) || (is_final && is_volatile)) {
       is_illegal = true;
+    } else {
+      if (is_inline_type && !is_static && !is_final) {
+        is_illegal = true;
+      }
     }
   }
 
   if (is_illegal) {
     ResourceMark rm(THREAD);
@@ -5010,10 +5417,11 @@
   }
 }
 
 void ClassFileParser::verify_legal_method_modifiers(jint flags,
                                                     bool is_interface,
+                                                    bool is_inline_type,
                                                     const Symbol* name,
                                                     TRAPS) const {
   if (!_need_verify) { return; }
 
   const bool is_public       = (flags & JVM_ACC_PUBLIC)       != 0;
@@ -5030,10 +5438,12 @@
   const bool major_gte_8     = _major_version >= JAVA_8_VERSION;
   const bool is_initializer  = (name == vmSymbols::object_initializer_name());
 
   bool is_illegal = false;
 
+  const char* class_note = "";
+
   if (is_interface) {
     if (major_gte_8) {
       // Class file version is JAVA_8_VERSION or later Methods of
       // interfaces may set any of the flags except ACC_PROTECTED,
       // ACC_FINAL, ACC_NATIVE, and ACC_SYNCHRONIZED; they must
@@ -5064,19 +5474,33 @@
   } else { // not interface
     if (has_illegal_visibility(flags)) {
       is_illegal = true;
     } else {
       if (is_initializer) {
-        if (is_static || is_final || is_synchronized || is_native ||
+        if (is_final || is_synchronized || is_native ||
             is_abstract || (major_gte_1_5 && is_bridge)) {
           is_illegal = true;
         }
+        if (!is_static && !is_inline_type) {
+          // OK, an object constructor in a regular class
+        } else if (is_static && is_inline_type) {
+          // OK, a static init factory in an inline class
+        } else {
+          // but no other combinations are allowed
+          is_illegal = true;
+          class_note = (is_inline_type ? " (an inline class)" : " (not an inline class)");
+        }
       } else { // not initializer
-        if (is_abstract) {
-          if ((is_final || is_native || is_private || is_static ||
-              (major_gte_1_5 && (is_synchronized || is_strict)))) {
-            is_illegal = true;
+        if (is_inline_type && is_synchronized && !is_static) {
+          is_illegal = true;
+          class_note = " (an inline class)";
+        } else {
+          if (is_abstract) {
+            if ((is_final || is_native || is_private || is_static ||
+                (major_gte_1_5 && (is_synchronized || is_strict)))) {
+              is_illegal = true;
+            }
           }
         }
       }
     }
   }
@@ -5084,12 +5508,12 @@
   if (is_illegal) {
     ResourceMark rm(THREAD);
     Exceptions::fthrow(
       THREAD_AND_LOCATION,
       vmSymbols::java_lang_ClassFormatError(),
-      "Method %s in class %s has illegal modifiers: 0x%X",
-      name->as_C_string(), _class_name->as_C_string(), flags);
+      "Method %s in class %s%s has illegal modifiers: 0x%X",
+      name->as_C_string(), _class_name->as_C_string(), class_note, flags);
     return;
   }
 }
 
 void ClassFileParser::verify_legal_utf8(const unsigned char* buffer,
@@ -5243,22 +5667,31 @@
     case JVM_SIGNATURE_INT:
     case JVM_SIGNATURE_FLOAT:
     case JVM_SIGNATURE_LONG:
     case JVM_SIGNATURE_DOUBLE:
       return signature + 1;
-    case JVM_SIGNATURE_CLASS: {
+    case JVM_SIGNATURE_VALUETYPE:
+      // Can't enable this check until JDK upgrades the bytecode generators
+      // if (_major_version < CONSTANT_CLASS_DESCRIPTORS ) {
+      //   classfile_parse_error("Class name contains illegal Q-signature "
+      //                                    "in descriptor in class file %s",
+      //                                    CHECK_0);
+      // }
+      // fall through
+    case JVM_SIGNATURE_CLASS:
+    {
       if (_major_version < JAVA_1_5_VERSION) {
         // Skip over the class name if one is there
         const char* const p = skip_over_field_name(signature + 1, true, --length);
 
         // The next character better be a semicolon
         if (p && (p - signature) > 1 && p[0] == JVM_SIGNATURE_ENDCLASS) {
           return p + 1;
         }
       }
       else {
-        // Skip leading 'L' and ignore first appearance of ';'
+        // Skip leading 'L' or 'Q' and ignore first appearance of ';'
         signature++;
         const char* c = (const char*) memchr(signature, JVM_SIGNATURE_ENDCLASS, length - 1);
         // Format check signature
         if (c != NULL) {
           int newlen = c - (char*) signature;
@@ -5309,10 +5742,13 @@
     } else if (_major_version < JAVA_1_5_VERSION) {
       if (bytes[0] != JVM_SIGNATURE_SPECIAL) {
         p = skip_over_field_name(bytes, true, length);
         legal = (p != NULL) && ((p - bytes) == (int)length);
       }
+    } else if (_major_version >= CONSTANT_CLASS_DESCRIPTORS && bytes[length - 1] == ';' ) {
+      // Support for L...; and Q...; descriptors
+      legal = verify_unqualified_name(bytes + 1, length - 2, LegalClass);
     } else {
       // 4900761: relax the constraints based on JSR202 spec
       // Class names may be drawn from the entire Unicode character set.
       // Identifiers between '/' must be unqualified names.
       // The utf8 string has been verified when parsing cpool entries.
@@ -5458,14 +5894,30 @@
     }
     // The first non-signature thing better be a ')'
     if ((length > 0) && (*p++ == JVM_SIGNATURE_ENDFUNC)) {
       length--;
       if (name->utf8_length() > 0 && name->char_at(0) == JVM_SIGNATURE_SPECIAL) {
-        // All internal methods must return void
+        // All constructor methods must return void
         if ((length == 1) && (p[0] == JVM_SIGNATURE_VOID)) {
           return args_size;
         }
+        // All static init methods must return the current class
+        if ((length >= 3) && (p[length-1] == JVM_SIGNATURE_ENDCLASS)
+            && name == vmSymbols::object_initializer_name()) {
+          nextp = skip_over_field_signature(p, true, length, CHECK_0);
+          if (nextp && ((int)length == (nextp - p))) {
+            // The actual class will be checked against current class
+            // when the method is defined (see parse_method).
+            // A reference to a static init with a bad return type
+            // will load and verify OK, but will fail to link.
+            return args_size;
+          }
+        }
+        // The distinction between static factory methods and
+        // constructors depends on the JVM_ACC_STATIC modifier.
+        // This distinction must be reflected in a void or non-void
+        // return. For declared methods, the check is in parse_method.
       } else {
         // Now we better just have a return value
         nextp = skip_over_field_signature(p, true, length, CHECK_0);
         if (nextp && ((int)length == (nextp - p))) {
           return args_size;
@@ -5619,13 +6071,53 @@
       log_info(class, fingerprint)("%s :  expected = " PTR64_FORMAT " actual = " PTR64_FORMAT,
                                  ik->external_name(), aot_fp, _stream->compute_fingerprint());
     }
   }
 
+  if (ik->is_value()) {
+    ValueKlass* vk = ValueKlass::cast(ik);
+    oop val = ik->allocate_instance(CHECK_NULL);
+    vk->set_default_value(val);
+  }
+
   return ik;
 }
 
+// Return true if the specified class is not a valid super class for an inline type.
+// A valid super class for an inline type is abstract, has no instance fields,
+// does not implement interface java.lang.IdentityObject (checked elsewhere), has
+// an empty body-less no-arg constructor, and no synchronized instance methods.
+// This function doesn't check if the class's super types are invalid.  Those checks
+// are done elsewhere.  The final determination of whether or not a class is an
+// invalid super type for an inline class is done in fill_instance_klass().
+bool ClassFileParser::is_invalid_super_for_inline_type() {
+  if (class_name() == vmSymbols::java_lang_IdentityObject()) {
+    return true;
+  }
+  if (is_interface() || class_name() == vmSymbols::java_lang_Object()) {
+    return false;
+  }
+  if (!access_flags().is_abstract() || _has_nonstatic_fields) {
+    return true;
+  } else {
+    // Look at each method
+    for (int x = 0; x < _methods->length(); x++) {
+      const Method* const method = _methods->at(x);
+      if (method->is_synchronized() && !method->is_static()) {
+        return true;
+
+      } else if (method->name() == vmSymbols::object_initializer_name()) {
+        if (method->signature() != vmSymbols::void_method_signature() ||
+            !method->is_vanilla_constructor()) {
+          return true;
+        }
+      }
+    }
+  }
+  return false;
+}
+
 void ClassFileParser::fill_instance_klass(InstanceKlass* ik,
                                           bool changed_by_loadhook,
                                           const ClassInstanceInfo& cl_inst_info,
                                           TRAPS) {
   assert(ik != NULL, "invariant");
@@ -5654,12 +6146,27 @@
   ik->set_should_verify_class(_need_verify);
 
   // Not yet: supers are done below to support the new subtype-checking fields
   ik->set_nonstatic_field_size(_field_info->_nonstatic_field_size);
   ik->set_has_nonstatic_fields(_field_info->_has_nonstatic_fields);
+  if (_field_info->_is_naturally_atomic && ik->is_value()) {
+    ik->set_is_naturally_atomic();
+  }
+  if (_is_empty_inline_type) {
+    ik->set_is_empty_inline_type();
+  }
+
+  if (this->_invalid_inline_super) {
+    ik->set_invalid_inline_super();
+  }
+
+  if (_has_injected_identityObject) {
+    ik->set_has_injected_identityObject();
+  }
+
   assert(_fac != NULL, "invariant");
-  ik->set_static_oop_field_count(_fac->count[STATIC_OOP]);
+  ik->set_static_oop_field_count(_fac->count[STATIC_OOP] + _fac->count[STATIC_FLATTENABLE]);
 
   // this transfers ownership of a lot of arrays from
   // the parser onto the InstanceKlass*
   apply_parsed_class_metadata(ik, _java_fields_count, CHECK);
 
@@ -5705,10 +6212,13 @@
 
   ik->set_minor_version(_minor_version);
   ik->set_major_version(_major_version);
   ik->set_has_nonstatic_concrete_methods(_has_nonstatic_concrete_methods);
   ik->set_declares_nonstatic_concrete_methods(_declares_nonstatic_concrete_methods);
+  if (_is_declared_atomic) {
+    ik->set_is_declared_atomic();
+  }
 
   if (_unsafe_anonymous_host != NULL) {
     assert (ik->is_unsafe_anonymous(), "should be the same");
     ik->set_unsafe_anonymous_host(_unsafe_anonymous_host);
   }
@@ -5814,10 +6324,44 @@
       // We won a potential race
       JvmtiExport::add_default_read_edges(module_handle, THREAD);
     }
   }
 
+  int nfields = ik->java_fields_count();
+  if (ik->is_value()) nfields++;
+  for (int i = 0; i < nfields; i++) {
+    if (ik->field_is_flattenable(i)) {
+      Symbol* klass_name = ik->field_signature(i)->fundamental_name(CHECK);
+      // Inline classes for instance fields must have been pre-loaded
+      // Inline classes for static fields might not have been loaded yet
+      Klass* klass = SystemDictionary::find(klass_name,
+          Handle(THREAD, ik->class_loader()),
+          Handle(THREAD, ik->protection_domain()), CHECK);
+      if (klass != NULL) {
+        assert(klass->access_flags().is_inline_type(), "Inline type expected");
+        ik->set_value_field_klass(i, klass);
+      }
+      klass_name->decrement_refcount();
+    } else
+      if (is_inline_type() && ((ik->field_access_flags(i) & JVM_ACC_FIELD_INTERNAL) != 0)
+        && ((ik->field_access_flags(i) & JVM_ACC_STATIC) != 0)) {
+      ValueKlass::cast(ik)->set_default_value_offset(ik->field_offset(i));
+    }
+  }
+
+  if (is_inline_type()) {
+    ValueKlass* vk = ValueKlass::cast(ik);
+    if (UseNewFieldLayout) {
+      vk->set_alignment(_alignment);
+      vk->set_first_field_offset(_first_field_offset);
+      vk->set_exact_size_in_bytes(_exact_size_in_bytes);
+    } else {
+      vk->set_first_field_offset(vk->first_field_offset_old());
+    }
+    ValueKlass::cast(ik)->initialize_calling_convention(CHECK);
+  }
+
   ClassLoadingService::notify_class_loaded(ik, false /* not shared class */);
 
   if (!is_internal()) {
     if (log_is_enabled(Info, class, load)) {
       ResourceMark rm;
@@ -5863,10 +6407,14 @@
   set_klass_to_deallocate(NULL);
 
   // it's official
   set_klass(ik);
 
+  if (ik->name() == vmSymbols::java_lang_IdentityObject()) {
+    Universe::initialize_the_single_IdentityObject_klass_array(ik, CHECK);
+  }
+
   debug_only(ik->verify();)
 }
 
 void ClassFileParser::update_class_name(Symbol* new_class_name) {
   // Decrement the refcount in the old name, since we're clobbering it.
@@ -5964,10 +6512,11 @@
   _methods(NULL),
   _inner_classes(NULL),
   _nest_members(NULL),
   _nest_host(0),
   _record_components(NULL),
+  _temp_local_interfaces(NULL),
   _local_interfaces(NULL),
   _transitive_interfaces(NULL),
   _combined_annotations(NULL),
   _class_annotations(NULL),
   _class_type_annotations(NULL),
@@ -6003,10 +6552,19 @@
   _relax_verify(false),
   _has_nonstatic_concrete_methods(false),
   _declares_nonstatic_concrete_methods(false),
   _has_final_method(false),
   _has_contended_fields(false),
+  _has_flattenable_fields(false),
+  _has_nonstatic_fields(false),
+  _is_empty_inline_type(false),
+  _is_naturally_atomic(false),
+  _is_declared_atomic(false),
+  _invalid_inline_super(false),
+  _invalid_identity_super(false),
+  _implements_identityObject(false),
+  _has_injected_identityObject(false),
   _has_finalizer(false),
   _has_empty_finalizer(false),
   _has_vanilla_constructor(false),
   _max_bootstrap_specifier_index(-1) {
 
@@ -6208,19 +6766,23 @@
   assert(cp_size == (const u2)cp->length(), "invariant");
 
   // ACCESS FLAGS
   stream->guarantee_more(8, CHECK);  // flags, this_class, super_class, infs_len
 
-  // Access flags
-  jint flags;
+  jint recognized_modifiers = JVM_RECOGNIZED_CLASS_MODIFIERS;
   // JVM_ACC_MODULE is defined in JDK-9 and later.
   if (_major_version >= JAVA_9_VERSION) {
-    flags = stream->get_u2_fast() & (JVM_RECOGNIZED_CLASS_MODIFIERS | JVM_ACC_MODULE);
-  } else {
-    flags = stream->get_u2_fast() & JVM_RECOGNIZED_CLASS_MODIFIERS;
+    recognized_modifiers |= JVM_ACC_MODULE;
+  }
+  // JVM_ACC_VALUE is defined for class file version 55 and later
+  if (supports_inline_types()) {
+    recognized_modifiers |= JVM_ACC_VALUE;
   }
 
+  // Access flags
+  jint flags = stream->get_u2_fast() & recognized_modifiers;
+
   if ((flags & JVM_ACC_INTERFACE) && _major_version < JAVA_6_VERSION) {
     // Set abstract bit for old class files for backward compatibility
     flags |= JVM_ACC_ABSTRACT;
   }
 
@@ -6374,19 +6936,22 @@
   // Interfaces
   _itfs_len = stream->get_u2_fast();
   parse_interfaces(stream,
                    _itfs_len,
                    cp,
+                   is_inline_type(),
                    &_has_nonstatic_concrete_methods,
+                   &_is_declared_atomic,
                    CHECK);
 
-  assert(_local_interfaces != NULL, "invariant");
+  assert(_temp_local_interfaces != NULL, "invariant");
 
   // Fields (offsets are filled in later)
   _fac = new FieldAllocationCount();
   parse_fields(stream,
-               _access_flags.is_interface(),
+               is_interface(),
+               is_inline_type(),
                _fac,
                cp,
                cp_size,
                &_java_fields_count,
                CHECK);
@@ -6394,11 +6959,12 @@
   assert(_fields != NULL, "invariant");
 
   // Methods
   AccessFlags promoted_flags;
   parse_methods(stream,
-                _access_flags.is_interface(),
+                is_interface(),
+                is_inline_type(),
                 &promoted_flags,
                 &_has_final_method,
                 &_declares_nonstatic_concrete_methods,
                 CHECK);
 
@@ -6465,18 +7031,18 @@
   assert(stream->at_eos(), "invariant");
   assert(cp != NULL, "invariant");
   assert(_loader_data != NULL, "invariant");
 
   if (_class_name == vmSymbols::java_lang_Object()) {
-    check_property(_local_interfaces == Universe::the_empty_instance_klass_array(),
-                   "java.lang.Object cannot implement an interface in class file %s",
-                   CHECK);
+    check_property(_temp_local_interfaces->length() == 0,
+        "java.lang.Object cannot implement an interface in class file %s",
+        CHECK);
   }
   // We check super class after class file is parsed and format is checked
   if (_super_class_index > 0 && NULL ==_super_klass) {
     Symbol* const super_class_name = cp->klass_name_at(_super_class_index);
-    if (_access_flags.is_interface()) {
+    if (is_interface()) {
       // Before attempting to resolve the superclass, check for class format
       // errors not checked yet.
       guarantee_property(super_class_name == vmSymbols::java_lang_Object(),
         "Interfaces must have java.lang.Object as superclass in class file %s",
         CHECK);
@@ -6493,10 +7059,13 @@
 
   if (_super_klass != NULL) {
     if (_super_klass->has_nonstatic_concrete_methods()) {
       _has_nonstatic_concrete_methods = true;
     }
+    if (_super_klass->is_declared_atomic()) {
+      _is_declared_atomic = true;
+    }
 
     if (_super_klass->is_interface()) {
       ResourceMark rm(THREAD);
       Exceptions::fthrow(
         THREAD_AND_LOCATION,
@@ -6505,15 +7074,72 @@
         _class_name->as_klass_external_name(),
         _super_klass->external_name()
       );
       return;
     }
+
     // Make sure super class is not final
     if (_super_klass->is_final()) {
       THROW_MSG(vmSymbols::java_lang_VerifyError(), "Cannot inherit from final class");
     }
+
+    // For an inline class, only java/lang/Object or special abstract classes
+    // are acceptable super classes.
+    if (is_inline_type()) {
+      const InstanceKlass* super_ik = _super_klass;
+      if (super_ik->invalid_inline_super()) {
+        ResourceMark rm(THREAD);
+        Exceptions::fthrow(
+          THREAD_AND_LOCATION,
+          vmSymbols::java_lang_IncompatibleClassChangeError(),
+          "inline class %s has an invalid super class %s",
+          _class_name->as_klass_external_name(),
+          _super_klass->external_name());
+        return;
+      }
+    }
+  }
+
+  if (_class_name == vmSymbols::java_lang_NonTearable() && _loader_data->class_loader() == NULL) {
+    // This is the original source of this condition.
+    // It propagates by inheritance, as if testing "instanceof NonTearable".
+    _is_declared_atomic = true;
+  } else if (*ForceNonTearable != '\0') {
+    // Allow a command line switch to force the same atomicity property:
+    const char* class_name_str = _class_name->as_C_string();
+    if (StringUtils::class_list_match(ForceNonTearable, class_name_str)) {
+      _is_declared_atomic = true;
+    }
+  }
+
+  // Set ik->invalid_inline_super field to TRUE if already marked as invalid,
+  // if super is marked invalid, or if is_invalid_super_for_inline_type()
+  // returns true
+  if (invalid_inline_super() ||
+      (_super_klass != NULL && _super_klass->invalid_inline_super()) ||
+      is_invalid_super_for_inline_type()) {
+    set_invalid_inline_super();
+  }
+
+  if (!is_inline_type() && invalid_inline_super() && (_super_klass == NULL || !_super_klass->invalid_inline_super())
+      && !_implements_identityObject && class_name() != vmSymbols::java_lang_IdentityObject()) {
+    _temp_local_interfaces->append(SystemDictionary::IdentityObject_klass());
+    _has_injected_identityObject = true;
+  }
+  int itfs_len = _temp_local_interfaces->length();
+  if (itfs_len == 0) {
+    _local_interfaces = Universe::the_empty_instance_klass_array();
+  } else if (itfs_len == 1 && _temp_local_interfaces->at(0) == SystemDictionary::IdentityObject_klass()) {
+    _local_interfaces = Universe::the_single_IdentityObject_klass_array();
+  } else {
+    _local_interfaces = MetadataFactory::new_array<InstanceKlass*>(_loader_data, itfs_len, NULL, CHECK);
+    for (int i = 0; i < itfs_len; i++) {
+      _local_interfaces->at_put(i, _temp_local_interfaces->at(i));
+    }
   }
+  _temp_local_interfaces = NULL;
+  assert(_local_interfaces != NULL, "invariant");
 
   // Compute the transitive list of all unique interfaces implemented by this class
   _transitive_interfaces =
     compute_transitive_interfaces(_super_klass,
                                   _local_interfaces,
@@ -6539,28 +7165,46 @@
                                                     _class_name,
                                                     _local_interfaces,
                                                     CHECK);
 
   // Size of Java itable (in words)
-  _itable_size = _access_flags.is_interface() ? 0 :
+  _itable_size = is_interface() ? 0 :
     klassItable::compute_itable_size(_transitive_interfaces);
 
   assert(_fac != NULL, "invariant");
   assert(_parsed_annotations != NULL, "invariant");
 
+
+  for (AllFieldStream fs(_fields, cp); !fs.done(); fs.next()) {
+    if (fs.is_flattenable() && !fs.access_flags().is_static()) {
+      // Pre-load value class
+      Klass* klass = SystemDictionary::resolve_flattenable_field_or_fail(&fs,
+          Handle(THREAD, _loader_data->class_loader()),
+          _protection_domain, true, CHECK);
+      assert(klass != NULL, "Sanity check");
+      assert(klass->access_flags().is_inline_type(), "Inline type expected");
+      _has_flattenable_fields = true;
+    }
+  }
+
   _field_info = new FieldLayoutInfo();
   if (UseNewFieldLayout) {
     FieldLayoutBuilder lb(class_name(), super_klass(), _cp, _fields,
-                          _parsed_annotations->is_contended(), _field_info);
-    lb.build_layout();
+        _parsed_annotations->is_contended(), is_inline_type(),
+        loader_data(), _protection_domain, _field_info);
+    lb.build_layout(CHECK);
+    if (is_inline_type()) {
+      _alignment = lb.get_alignment();
+      _first_field_offset = lb.get_first_field_offset();
+      _exact_size_in_bytes = lb.get_exact_size_in_byte();
+    }
   } else {
     layout_fields(cp, _fac, _parsed_annotations, _field_info, CHECK);
   }
 
-  // Compute reference typ
+  // Compute reference type
   _rt = (NULL ==_super_klass) ? REF_NONE : _super_klass->reference_type();
-
 }
 
 void ClassFileParser::set_klass(InstanceKlass* klass) {
 
 #ifdef ASSERT
@@ -6588,10 +7232,11 @@
 const ClassFileStream* ClassFileParser::clone_stream() const {
   assert(_stream != NULL, "invariant");
 
   return _stream->clone();
 }
+
 // ----------------------------------------------------------------------------
 // debugging
 
 #ifdef ASSERT
 
diff a/src/hotspot/share/classfile/javaClasses.cpp b/src/hotspot/share/classfile/javaClasses.cpp
--- a/src/hotspot/share/classfile/javaClasses.cpp
+++ b/src/hotspot/share/classfile/javaClasses.cpp
@@ -42,18 +42,20 @@
 #include "memory/oopFactory.hpp"
 #include "memory/resourceArea.hpp"
 #include "memory/universe.hpp"
 #include "oops/fieldStreams.inline.hpp"
 #include "oops/instanceKlass.hpp"
-#include "oops/instanceMirrorKlass.hpp"
+#include "oops/instanceMirrorKlass.inline.hpp"
 #include "oops/klass.hpp"
 #include "oops/method.inline.hpp"
 #include "oops/objArrayOop.inline.hpp"
 #include "oops/oop.inline.hpp"
 #include "oops/symbol.hpp"
 #include "oops/recordComponent.hpp"
 #include "oops/typeArrayOop.inline.hpp"
+#include "oops/valueArrayKlass.hpp"
+#include "oops/valueKlass.inline.hpp"
 #include "prims/jvmtiExport.hpp"
 #include "prims/resolvedMethodTable.hpp"
 #include "runtime/fieldDescriptor.inline.hpp"
 #include "runtime/frame.inline.hpp"
 #include "runtime/handles.inline.hpp"
@@ -982,11 +984,16 @@
 
     java_lang_Class::set_static_oop_field_count(mirror(), mk->compute_static_oop_field_count(mirror()));
 
     // It might also have a component mirror.  This mirror must already exist.
     if (k->is_array_klass()) {
-      if (k->is_typeArray_klass()) {
+      if (k->is_valueArray_klass()) {
+        Klass* element_klass = (Klass*) ValueArrayKlass::cast(k)->element_klass();
+        assert(element_klass->is_value(), "Must be value type component");
+        ValueKlass* vk = ValueKlass::cast(InstanceKlass::cast(element_klass));
+        comp_mirror = Handle(THREAD, vk->java_mirror());
+      } else if (k->is_typeArray_klass()) {
         BasicType type = TypeArrayKlass::cast(k)->element_type();
         comp_mirror = Handle(THREAD, Universe::java_mirror(type));
       } else {
         assert(k->is_objArray_klass(), "Must be");
         Klass* element_klass = ObjArrayKlass::cast(k)->element_klass();
@@ -1029,10 +1036,27 @@
     if (comp_mirror() != NULL) {
       // Set after k->java_mirror() is published, because compiled code running
       // concurrently doesn't expect a k to have a null java_mirror.
       release_set_array_klass(comp_mirror(), k);
     }
+
+    if (k->is_value()) {
+      InstanceKlass* super = k->java_super();
+      set_val_type_mirror(mirror(), mirror());
+
+      // if the supertype is a restricted abstract class
+      if (super != SystemDictionary::Object_klass()) {
+        assert(super->access_flags().is_abstract(), "must be an abstract class");
+        oop ref_type_oop = super->java_mirror();
+        // set the reference projection type
+        set_ref_type_mirror(mirror(), ref_type_oop);
+
+        // set the value and reference projection types
+        set_val_type_mirror(ref_type_oop, mirror());
+        set_ref_type_mirror(ref_type_oop, ref_type_oop);
+      }
+    }
   } else {
     assert(fixup_mirror_list() != NULL, "fixup_mirror_list not initialized");
     fixup_mirror_list()->push(k);
   }
 }
@@ -1185,10 +1209,16 @@
       k->set_java_mirror_handle(NULL);
       return NULL;
     }
   }
 
+  if (k->is_value()) {
+    // Values have a val type mirror and a ref type mirror. Don't handle this for now. TODO:CDS
+    k->set_java_mirror_handle(NULL);
+    return NULL;
+  }
+
   // Now start archiving the mirror object
   oop archived_mirror = HeapShared::archive_heap_object(mirror, THREAD);
   if (archived_mirror == NULL) {
     return NULL;
   }
@@ -1476,10 +1506,30 @@
 void java_lang_Class::set_source_file(oop java_class, oop source_file) {
   assert(_source_file_offset != 0, "must be set");
   java_class->obj_field_put(_source_file_offset, source_file);
 }
 
+oop java_lang_Class::val_type_mirror(oop java_class) {
+  assert(_val_type_mirror_offset != 0, "must be set");
+  return java_class->obj_field(_val_type_mirror_offset);
+}
+
+void java_lang_Class::set_val_type_mirror(oop java_class, oop mirror) {
+  assert(_val_type_mirror_offset != 0, "must be set");
+  java_class->obj_field_put(_val_type_mirror_offset, mirror);
+}
+
+oop java_lang_Class::ref_type_mirror(oop java_class) {
+  assert(_ref_type_mirror_offset != 0, "must be set");
+  return java_class->obj_field(_ref_type_mirror_offset);
+}
+
+void java_lang_Class::set_ref_type_mirror(oop java_class, oop mirror) {
+  assert(_ref_type_mirror_offset != 0, "must be set");
+  java_class->obj_field_put(_ref_type_mirror_offset, mirror);
+}
+
 oop java_lang_Class::create_basic_type_mirror(const char* basic_type_name, BasicType type, TRAPS) {
   // This should be improved by adding a field at the Java level or by
   // introducing a new VM klass (see comment in ClassFileParser)
   oop java_class = InstanceMirrorKlass::cast(SystemDictionary::Class_klass())->allocate_instance(NULL, CHECK_NULL);
   if (type != T_VOID) {
@@ -1512,22 +1562,30 @@
 
 void java_lang_Class::print_signature(oop java_class, outputStream* st) {
   assert(java_lang_Class::is_instance(java_class), "must be a Class object");
   Symbol* name = NULL;
   bool is_instance = false;
+  bool is_value = false;
   if (is_primitive(java_class)) {
     name = vmSymbols::type_signature(primitive_type(java_class));
   } else {
     Klass* k = as_Klass(java_class);
     is_instance = k->is_instance_klass();
+    is_value = k->is_value();
     name = k->name();
   }
   if (name == NULL) {
     st->print("<null>");
     return;
   }
-  if (is_instance)  st->print("L");
+  if (is_instance)  {
+    if (is_value) {
+      st->print("Q");
+    } else {
+      st->print("L");
+    }
+  }
   st->write((char*) name->base(), (int) name->utf8_length());
   if (is_instance)  st->print(";");
 }
 
 Symbol* java_lang_Class::as_signature(oop java_class, bool intern_if_not_found) {
@@ -1545,11 +1603,11 @@
       name = k->name();
       name->increment_refcount();
     } else {
       ResourceMark rm;
       const char* sigstr = k->signature_name();
-      int         siglen = (int) strlen(sigstr);
+      int siglen = (int) strlen(sigstr);
       if (!intern_if_not_found) {
         name = SymbolTable::probe(sigstr, siglen);
       } else {
         name = SymbolTable::new_symbol(sigstr, siglen);
       }
@@ -1630,10 +1688,12 @@
   macro(classRedefinedCount_offset, k, "classRedefinedCount", int_signature,         false); \
   macro(_class_loader_offset,       k, "classLoader",         classloader_signature, false); \
   macro(_component_mirror_offset,   k, "componentType",       class_signature,       false); \
   macro(_module_offset,             k, "module",              module_signature,      false); \
   macro(_name_offset,               k, "name",                string_signature,      false); \
+  macro(_val_type_mirror_offset,    k, "valType",             class_signature,       false); \
+  macro(_ref_type_mirror_offset,    k, "refType",             class_signature,       false); \
   macro(_classData_offset,          k, "classData",           object_signature,      false);
 
 void java_lang_Class::compute_offsets() {
   if (offsets_computed) {
     return;
@@ -2505,12 +2565,12 @@
     }
     if (!skip_throwableInit_check) {
       assert(skip_fillInStackTrace_check, "logic error in backtrace filtering");
 
       // skip <init> methods of the exception class and superclasses
-      // This is simlar to classic VM.
-      if (method->name() == vmSymbols::object_initializer_name() &&
+      // This is similar to classic VM (before HotSpot).
+      if (method->is_object_constructor() &&
           throwable->is_a(method->method_holder())) {
         continue;
       } else {
         // there are none or we've seen them all - either way stop checking
         skip_throwableInit_check = true;
@@ -3827,11 +3887,11 @@
   return method == NULL ? NULL : java_lang_invoke_ResolvedMethodName::vmtarget(method);
 }
 
 bool java_lang_invoke_MemberName::is_method(oop mname) {
   assert(is_instance(mname), "must be MemberName");
-  return (flags(mname) & (MN_IS_METHOD | MN_IS_CONSTRUCTOR)) > 0;
+  return (flags(mname) & (MN_IS_METHOD | MN_IS_OBJECT_CONSTRUCTOR)) > 0;
 }
 
 void java_lang_invoke_MemberName::set_method(oop mname, oop resolved_method) {
   assert(is_instance(mname), "wrong type");
   mname->obj_field_put(_method_offset, resolved_method);
@@ -4333,10 +4393,12 @@
 int java_lang_Class::_static_oop_field_count_offset;
 int java_lang_Class::_class_loader_offset;
 int java_lang_Class::_module_offset;
 int java_lang_Class::_protection_domain_offset;
 int java_lang_Class::_component_mirror_offset;
+int java_lang_Class::_val_type_mirror_offset;
+int java_lang_Class::_ref_type_mirror_offset;
 int java_lang_Class::_init_lock_offset;
 int java_lang_Class::_signers_offset;
 int java_lang_Class::_name_offset;
 int java_lang_Class::_source_file_offset;
 int java_lang_Class::_classData_offset;
@@ -4428,10 +4490,15 @@
 int java_lang_reflect_RecordComponent::type_offset;
 int java_lang_reflect_RecordComponent::accessor_offset;
 int java_lang_reflect_RecordComponent::signature_offset;
 int java_lang_reflect_RecordComponent::annotations_offset;
 int java_lang_reflect_RecordComponent::typeAnnotations_offset;
+int jdk_internal_vm_jni_SubElementSelector::_arrayElementType_offset;
+int jdk_internal_vm_jni_SubElementSelector::_subElementType_offset;
+int jdk_internal_vm_jni_SubElementSelector::_offset_offset;
+int jdk_internal_vm_jni_SubElementSelector::_isFlattened_offset;
+int jdk_internal_vm_jni_SubElementSelector::_isFlattenable_offset;
 
 
 
 #define STACKTRACEELEMENT_FIELDS_DO(macro) \
   macro(declaringClassObject_offset,  k, "declaringClassObject", class_signature, false); \
@@ -4733,10 +4800,73 @@
   BYTE_CACHE_FIELDS_DO(FIELD_SERIALIZE_OFFSET);
 }
 #endif
 #undef BYTE_CACHE_FIELDS_DO
 
+#define SUBELEMENT_SELECTOR_FIELDS_DO(macro) \
+  macro(_arrayElementType_offset,  k, "arrayElementType", class_signature, false); \
+  macro(_subElementType_offset,    k, "subElementType",   class_signature, false); \
+  macro(_offset_offset,            k, "offset",           int_signature,   false); \
+  macro(_isFlattened_offset,       k, "isFlattened",      bool_signature,  false); \
+  macro(_isFlattenable_offset,     k, "isFlattenable",    bool_signature,  false);
+
+void jdk_internal_vm_jni_SubElementSelector::compute_offsets() {
+  InstanceKlass* k = SystemDictionary::jdk_internal_vm_jni_SubElementSelector_klass();
+  SUBELEMENT_SELECTOR_FIELDS_DO(FIELD_COMPUTE_OFFSET);
+}
+
+#if INCLUDE_CDS
+void jdk_internal_vm_jni_SubElementSelector::serialize_offsets(SerializeClosure* f) {
+  SUBELEMENT_SELECTOR_FIELDS_DO(FIELD_SERIALIZE_OFFSET);
+}
+#endif
+#undef SUBELEMENT_SELECTOR_FIELDS_DO
+
+Symbol* jdk_internal_vm_jni_SubElementSelector::symbol() {
+  return vmSymbols::jdk_internal_vm_jni_SubElementSelector();
+}
+
+oop jdk_internal_vm_jni_SubElementSelector::getArrayElementType(oop obj) {
+  return obj->obj_field(_arrayElementType_offset);
+}
+
+void jdk_internal_vm_jni_SubElementSelector::setArrayElementType(oop obj, oop type) {
+  obj->obj_field_put(_arrayElementType_offset, type);
+}
+
+oop jdk_internal_vm_jni_SubElementSelector::getSubElementType(oop obj) {
+  return obj->obj_field(_subElementType_offset);
+}
+
+void jdk_internal_vm_jni_SubElementSelector::setSubElementType(oop obj, oop type) {
+  obj->obj_field_put(_subElementType_offset, type);
+}
+
+int jdk_internal_vm_jni_SubElementSelector::getOffset(oop obj) {
+  return obj->int_field(_offset_offset);
+}
+
+void jdk_internal_vm_jni_SubElementSelector::setOffset(oop obj, int offset) {
+  obj->int_field_put(_offset_offset, offset);
+}
+
+bool jdk_internal_vm_jni_SubElementSelector::getIsFlattened(oop obj) {
+  return obj->bool_field(_isFlattened_offset);
+}
+
+void jdk_internal_vm_jni_SubElementSelector::setIsFlattened(oop obj, bool b) {
+  obj->bool_field_put(_isFlattened_offset, b);
+}
+
+bool jdk_internal_vm_jni_SubElementSelector::getIsFlattenable(oop obj) {
+  return obj->bool_field(_isFlattenable_offset);
+}
+
+void jdk_internal_vm_jni_SubElementSelector::setIsFlattenable(oop obj, bool b) {
+  obj->bool_field_put(_isFlattenable_offset, b);
+}
+
 jbyte java_lang_Byte::value(oop obj) {
    jvalue v;
    java_lang_boxing_object::get_value(obj, &v);
    return v.b;
 }
diff a/src/hotspot/share/classfile/javaClasses.hpp b/src/hotspot/share/classfile/javaClasses.hpp
--- a/src/hotspot/share/classfile/javaClasses.hpp
+++ b/src/hotspot/share/classfile/javaClasses.hpp
@@ -71,10 +71,11 @@
   f(java_lang_StackTraceElement) \
   f(java_lang_StackFrameInfo) \
   f(java_lang_LiveStackFrameInfo) \
   f(java_util_concurrent_locks_AbstractOwnableSynchronizer) \
   f(jdk_internal_misc_UnsafeConstants) \
+  f(jdk_internal_vm_jni_SubElementSelector) \
   f(java_lang_boxing_object) \
   //end
 
 #define BASIC_JAVA_CLASSES_DO(f) \
         BASIC_JAVA_CLASSES_DO_PART1(f) \
@@ -251,10 +252,12 @@
   static int _class_loader_offset;
   static int _module_offset;
   static int _component_mirror_offset;
   static int _name_offset;
   static int _source_file_offset;
+  static int _val_type_mirror_offset;
+  static int _ref_type_mirror_offset;
   static int _classData_offset;
 
   static bool offsets_computed;
   static int classRedefinedCount_offset;
 
@@ -310,10 +313,11 @@
   static Klass* array_klass_acquire(oop java_class);
   static void release_set_array_klass(oop java_class, Klass* klass);
   // compiler support for class operations
   static int klass_offset_in_bytes()                { return _klass_offset; }
   static int array_klass_offset_in_bytes()          { return _array_klass_offset; }
+  static int component_mirror_offset_in_bytes()     { return _component_mirror_offset; }
   // Support for classRedefinedCount field
   static int classRedefinedCount(oop the_class_mirror);
   static void set_classRedefinedCount(oop the_class_mirror, int value);
 
   // Support for embedded per-class oops
@@ -327,10 +331,16 @@
 
   static oop class_loader(oop java_class);
   static void set_module(oop java_class, oop module);
   static oop module(oop java_class);
 
+  static void set_ref_type_mirror(oop java_class, oop mirror);
+  static oop ref_type_mirror(oop java_class);
+
+  static void set_val_type_mirror(oop java_class, oop mirror);
+  static oop val_type_mirror(oop java_class);
+
   static oop name(Handle java_class, TRAPS);
 
   static oop source_file(oop java_class);
   static void set_source_file(oop java_class, oop source_file);
 
@@ -1126,11 +1136,11 @@
   static bool is_method(oop obj);
 
   // Relevant integer codes (keep these in synch. with MethodHandleNatives.Constants):
   enum {
     MN_IS_METHOD             = 0x00010000, // method (not constructor)
-    MN_IS_CONSTRUCTOR        = 0x00020000, // constructor
+    MN_IS_OBJECT_CONSTRUCTOR = 0x00020000, // constructor
     MN_IS_FIELD              = 0x00040000, // field
     MN_IS_TYPE               = 0x00080000, // nested type
     MN_CALLER_SENSITIVE      = 0x00100000, // @CallerSensitive annotation detected
     MN_REFERENCE_KIND_SHIFT  = 24, // refKind
     MN_REFERENCE_KIND_MASK   = 0x0F000000 >> MN_REFERENCE_KIND_SHIFT,
@@ -1651,10 +1661,34 @@
   static void compute_offsets(InstanceKlass* k);
   static objArrayOop  cache(InstanceKlass *k);
   static void serialize_offsets(SerializeClosure* f) NOT_CDS_RETURN;
 };
 
+class jdk_internal_vm_jni_SubElementSelector : AllStatic {
+ private:
+  static int _arrayElementType_offset;
+  static int _subElementType_offset;
+  static int _offset_offset;
+  static int _isFlattened_offset;
+  static int _isFlattenable_offset;
+ public:
+  static Symbol* symbol();
+  static void compute_offsets();
+  static void serialize_offsets(SerializeClosure* f) NOT_CDS_RETURN;
+
+  static oop getArrayElementType(oop obj);
+  static void setArrayElementType(oop obj, oop type);
+  static oop getSubElementType(oop obj);
+  static void setSubElementType(oop obj, oop type);
+  static int getOffset(oop obj);
+  static void setOffset(oop obj, int offset);
+  static bool getIsFlattened(oop obj);
+  static void setIsFlattened(oop obj, bool b);
+  static bool getIsFlattenable(oop obj);
+  static void setIsFlattenable(oop obj, bool b);
+};
+
 // Use to declare fields that need to be injected into Java classes
 // for the JVM to use.  The name_index and signature_index are
 // declared in vmSymbols.  The may_be_java flag is used to declare
 // fields that might already exist in Java but should be injected if
 // they don't.  Otherwise the field is unconditionally injected and
diff a/src/hotspot/share/classfile/systemDictionary.cpp b/src/hotspot/share/classfile/systemDictionary.cpp
--- a/src/hotspot/share/classfile/systemDictionary.cpp
+++ b/src/hotspot/share/classfile/systemDictionary.cpp
@@ -58,28 +58,31 @@
 #include "memory/metaspaceClosure.hpp"
 #include "memory/oopFactory.hpp"
 #include "memory/resourceArea.hpp"
 #include "memory/universe.hpp"
 #include "oops/access.inline.hpp"
+#include "oops/fieldStreams.inline.hpp"
 #include "oops/instanceKlass.hpp"
 #include "oops/instanceRefKlass.hpp"
 #include "oops/klass.inline.hpp"
 #include "oops/method.inline.hpp"
 #include "oops/methodData.hpp"
 #include "oops/objArrayKlass.hpp"
 #include "oops/objArrayOop.inline.hpp"
 #include "oops/oop.inline.hpp"
 #include "oops/symbol.hpp"
 #include "oops/typeArrayKlass.hpp"
+#include "oops/valueKlass.hpp"
 #include "prims/jvmtiExport.hpp"
 #include "prims/methodHandles.hpp"
 #include "runtime/arguments.hpp"
 #include "runtime/biasedLocking.hpp"
 #include "runtime/handles.inline.hpp"
 #include "runtime/java.hpp"
 #include "runtime/javaCalls.hpp"
 #include "runtime/mutexLocker.hpp"
+#include "runtime/os.hpp"
 #include "runtime/sharedRuntime.hpp"
 #include "runtime/signature.hpp"
 #include "services/classLoadingService.hpp"
 #include "services/diagnosticCommand.hpp"
 #include "services/threadService.hpp"
@@ -297,11 +300,11 @@
                                                                        Handle protection_domain,
                                                                        TRAPS) {
   assert(class_name != NULL && !Signature::is_array(class_name), "must be");
   if (Signature::has_envelope(class_name)) {
     ResourceMark rm(THREAD);
-    // Ignore wrapping L and ;.
+    // Ignore wrapping L and ;. (and Q and ; for value types);
     TempNewSymbol name = SymbolTable::new_symbol(class_name->as_C_string() + 1,
                                                  class_name->utf8_length() - 2);
     return resolve_instance_class_or_null(name, class_loader, protection_domain, THREAD);
   } else {
     return resolve_instance_class_or_null(class_name, class_loader, protection_domain, THREAD);
@@ -338,11 +341,10 @@
     k = TypeArrayKlass::cast(k)->array_klass(ndims, CHECK_NULL);
   }
   return k;
 }
 
-
 // Must be called for any super-class or super-interface resolution
 // during class definition to allow class circularity checking
 // super-interface callers:
 //    parse_interfaces - for defineClass & jvmtiRedefineClasses
 // super-class callers:
@@ -482,10 +484,55 @@
   }
 
   return superk;
 }
 
+Klass* SystemDictionary::resolve_flattenable_field_or_fail(AllFieldStream* fs,
+                                                           Handle class_loader,
+                                                           Handle protection_domain,
+                                                           bool throw_error,
+                                                           TRAPS) {
+  Symbol* class_name = fs->signature()->fundamental_name(THREAD);
+  class_loader = Handle(THREAD, java_lang_ClassLoader::non_reflection_class_loader(class_loader()));
+  ClassLoaderData* loader_data = class_loader_data(class_loader);
+  unsigned int p_hash = placeholders()->compute_hash(class_name);
+  int p_index = placeholders()->hash_to_index(p_hash);
+  bool throw_circularity_error = false;
+  PlaceholderEntry* oldprobe;
+
+  {
+    MutexLocker mu(THREAD, SystemDictionary_lock);
+    oldprobe = placeholders()->get_entry(p_index, p_hash, class_name, loader_data);
+    if (oldprobe != NULL &&
+      oldprobe->check_seen_thread(THREAD, PlaceholderTable::FLATTENABLE_FIELD)) {
+      throw_circularity_error = true;
+
+    } else {
+      placeholders()->find_and_add(p_index, p_hash, class_name, loader_data,
+                                   PlaceholderTable::FLATTENABLE_FIELD, NULL, THREAD);
+    }
+  }
+
+  Klass* klass = NULL;
+  if (!throw_circularity_error) {
+    klass = SystemDictionary::resolve_or_fail(class_name, class_loader,
+                                               protection_domain, true, THREAD);
+  } else {
+    ResourceMark rm(THREAD);
+    THROW_MSG_NULL(vmSymbols::java_lang_ClassCircularityError(), class_name->as_C_string());
+  }
+
+  {
+    MutexLocker mu(THREAD, SystemDictionary_lock);
+    placeholders()->find_and_remove(p_index, p_hash, class_name, loader_data,
+                                    PlaceholderTable::FLATTENABLE_FIELD, THREAD);
+  }
+
+  class_name->decrement_refcount();
+  return klass;
+}
+
 void SystemDictionary::validate_protection_domain(InstanceKlass* klass,
                                                   Handle class_loader,
                                                   Handle protection_domain,
                                                   TRAPS) {
   // Now we have to call back to java to check if the initating class has access
@@ -1009,11 +1056,11 @@
     // dimension and object_key in FieldArrayInfo are assigned as a
     // side-effect of this call
     SignatureStream ss(class_name, false);
     int ndims = ss.skip_array_prefix();  // skip all '['s
     BasicType t = ss.type();
-    if (t != T_OBJECT) {
+    if (t != T_OBJECT && t != T_VALUETYPE) {
       k = Universe::typeArrayKlassObj(t);
     } else {
       k = SystemDictionary::find(ss.as_symbol(), class_loader, protection_domain, THREAD);
     }
     if (k != NULL) {
@@ -2280,11 +2327,11 @@
     // For array classes, their Klass*s are not kept in the
     // constraint table. The element Klass*s are.
     SignatureStream ss(class_name, false);
     int ndims = ss.skip_array_prefix();  // skip all '['s
     BasicType t = ss.type();
-    if (t != T_OBJECT) {
+    if (t != T_OBJECT && t != T_VALUETYPE) {
       klass = Universe::typeArrayKlassObj(t);
     } else {
       MutexLocker mu(THREAD, SystemDictionary_lock);
       klass = constraints()->find_constrained_klass(ss.as_symbol(), class_loader);
     }
diff a/src/hotspot/share/classfile/vmSymbols.hpp b/src/hotspot/share/classfile/vmSymbols.hpp
--- a/src/hotspot/share/classfile/vmSymbols.hpp
+++ b/src/hotspot/share/classfile/vmSymbols.hpp
@@ -52,19 +52,21 @@
 #define VM_SYMBOLS_DO(template, do_alias)                                                         \
   /* commonly used class, package, module names */                                                \
   template(java_base,                                 "java.base")                                \
   template(java_lang_System,                          "java/lang/System")                         \
   template(java_lang_Object,                          "java/lang/Object")                         \
+  template(java_lang_IdentityObject,                  "java/lang/IdentityObject")                 \
   template(java_lang_Class,                           "java/lang/Class")                          \
   template(java_lang_Package,                         "java/lang/Package")                        \
   template(java_lang_Module,                          "java/lang/Module")                         \
   template(java_lang_String,                          "java/lang/String")                         \
   template(java_lang_StringLatin1,                    "java/lang/StringLatin1")                   \
   template(java_lang_StringUTF16,                     "java/lang/StringUTF16")                    \
   template(java_lang_Thread,                          "java/lang/Thread")                         \
   template(java_lang_ThreadGroup,                     "java/lang/ThreadGroup")                    \
   template(java_lang_Cloneable,                       "java/lang/Cloneable")                      \
+  template(java_lang_NonTearable,                     "java/lang/NonTearable")                    \
   template(java_lang_Throwable,                       "java/lang/Throwable")                      \
   template(java_lang_ClassLoader,                     "java/lang/ClassLoader")                    \
   template(java_lang_ThreadDeath,                     "java/lang/ThreadDeath")                    \
   template(java_lang_Boolean,                         "java/lang/Boolean")                        \
   template(java_lang_Character,                       "java/lang/Character")                      \
@@ -325,12 +327,13 @@
   template(setTargetVolatile_name,                    "setTargetVolatile")                        \
   template(setTarget_signature,                       "(Ljava/lang/invoke/MethodHandle;)V")       \
   template(DEFAULT_CONTEXT_name,                      "DEFAULT_CONTEXT")                          \
   NOT_LP64(  do_alias(intptr_signature,               int_signature)  )                           \
   LP64_ONLY( do_alias(intptr_signature,               long_signature) )                           \
-                                                                                                                                      \
-  /* Support for JVMCI */                                                                                                             \
+                                                                                                  \
+                                                                                                  \
+  /* Support for JVMCI */                                                                         \
   JVMCI_VM_SYMBOLS_DO(template, do_alias)                                                         \
                                                                                                   \
   template(java_lang_StackWalker,                     "java/lang/StackWalker")                    \
   template(java_lang_StackFrameInfo,                  "java/lang/StackFrameInfo")                 \
   template(java_lang_LiveStackFrameInfo,              "java/lang/LiveStackFrameInfo")             \
@@ -450,10 +453,12 @@
   template(java_lang_Boolean_signature,               "Ljava/lang/Boolean;")                      \
   template(url_code_signer_array_void_signature,      "(Ljava/net/URL;[Ljava/security/CodeSigner;)V") \
   template(module_entry_name,                         "module_entry")                             \
   template(resolved_references_name,                  "<resolved_references>")                    \
   template(init_lock_name,                            "<init_lock>")                              \
+  template(default_value_name,                        ".default")                                 \
+  template(empty_marker_name,                         ".empty")                                   \
   template(address_size_name,                         "ADDRESS_SIZE0")                            \
   template(page_size_name,                            "PAGE_SIZE")                                \
   template(big_endian_name,                           "BIG_ENDIAN")                               \
   template(use_unaligned_access_name,                 "UNALIGNED_ACCESS")                         \
   template(data_cache_line_flush_size_name,           "DATA_CACHE_LINE_FLUSH_SIZE")               \
@@ -519,10 +524,11 @@
   template(thread_void_signature,                     "(Ljava/lang/Thread;)V")                                    \
   template(threadgroup_runnable_void_signature,       "(Ljava/lang/ThreadGroup;Ljava/lang/Runnable;)V")           \
   template(threadgroup_string_void_signature,         "(Ljava/lang/ThreadGroup;Ljava/lang/String;)V")             \
   template(string_class_signature,                    "(Ljava/lang/String;)Ljava/lang/Class;")                    \
   template(object_object_object_signature,            "(Ljava/lang/Object;Ljava/lang/Object;)Ljava/lang/Object;") \
+  template(object_object_boolean_signature,           "(Ljava/lang/Object;Ljava/lang/Object;)Z") \
   template(string_string_string_signature,            "(Ljava/lang/String;Ljava/lang/String;)Ljava/lang/String;") \
   template(string_string_signature,                   "(Ljava/lang/String;)Ljava/lang/String;")                   \
   template(classloader_string_long_signature,         "(Ljava/lang/ClassLoader;Ljava/lang/String;)J")             \
   template(byte_array_void_signature,                 "([B)V")                                                    \
   template(char_array_void_signature,                 "([C)V")                                                    \
@@ -670,10 +676,14 @@
   template(jdk_internal_loader_ClassLoaders,       "jdk/internal/loader/ClassLoaders")                            \
   template(toFileURL_name,                         "toFileURL")                                                   \
   template(toFileURL_signature,                    "(Ljava/lang/String;)Ljava/net/URL;")                          \
   template(url_void_signature,                     "(Ljava/net/URL;)V")                                           \
                                                                                                                   \
+  template(java_lang_invoke_ValueBootstrapMethods, "java/lang/invoke/ValueBootstrapMethods")                      \
+  template(isSubstitutable_name,                   "isSubstitutable0")                                            \
+                                                                                                                  \
+  template(jdk_internal_vm_jni_SubElementSelector, "jdk/internal/vm/jni/SubElementSelector")                      \
   /*end*/
 
 // Here are all the intrinsics known to the runtime and the CI.
 // Each intrinsic consists of a public enum name (like _hashCode),
 // followed by a specification of its klass, name, and signature:
@@ -1161,39 +1171,49 @@
   do_signature(putLong_signature,         "(Ljava/lang/Object;JJ)V")                                                    \
   do_signature(getFloat_signature,        "(Ljava/lang/Object;J)F")                                                     \
   do_signature(putFloat_signature,        "(Ljava/lang/Object;JF)V")                                                    \
   do_signature(getDouble_signature,       "(Ljava/lang/Object;J)D")                                                     \
   do_signature(putDouble_signature,       "(Ljava/lang/Object;JD)V")                                                    \
+  do_signature(getValue_signature,        "(Ljava/lang/Object;JLjava/lang/Class;)Ljava/lang/Object;")                   \
+  do_signature(putValue_signature,        "(Ljava/lang/Object;JLjava/lang/Class;Ljava/lang/Object;)V")                  \
                                                                                                                         \
   do_name(getReference_name,"getReference")     do_name(putReference_name,"putReference")                               \
   do_name(getBoolean_name,"getBoolean")         do_name(putBoolean_name,"putBoolean")                                   \
   do_name(getByte_name,"getByte")               do_name(putByte_name,"putByte")                                         \
   do_name(getShort_name,"getShort")             do_name(putShort_name,"putShort")                                       \
   do_name(getChar_name,"getChar")               do_name(putChar_name,"putChar")                                         \
   do_name(getInt_name,"getInt")                 do_name(putInt_name,"putInt")                                           \
   do_name(getLong_name,"getLong")               do_name(putLong_name,"putLong")                                         \
   do_name(getFloat_name,"getFloat")             do_name(putFloat_name,"putFloat")                                       \
   do_name(getDouble_name,"getDouble")           do_name(putDouble_name,"putDouble")                                     \
+  do_name(getValue_name,"getValue")             do_name(putValue_name,"putValue")                                       \
+  do_name(makePrivateBuffer_name,"makePrivateBuffer")                                                                   \
+  do_name(finishPrivateBuffer_name,"finishPrivateBuffer")                                                               \
                                                                                                                         \
   do_intrinsic(_getReference,       jdk_internal_misc_Unsafe,     getReference_name, getReference_signature,     F_RN)  \
   do_intrinsic(_getBoolean,         jdk_internal_misc_Unsafe,     getBoolean_name, getBoolean_signature,         F_RN)  \
   do_intrinsic(_getByte,            jdk_internal_misc_Unsafe,     getByte_name, getByte_signature,               F_RN)  \
   do_intrinsic(_getShort,           jdk_internal_misc_Unsafe,     getShort_name, getShort_signature,             F_RN)  \
   do_intrinsic(_getChar,            jdk_internal_misc_Unsafe,     getChar_name, getChar_signature,               F_RN)  \
   do_intrinsic(_getInt,             jdk_internal_misc_Unsafe,     getInt_name, getInt_signature,                 F_RN)  \
   do_intrinsic(_getLong,            jdk_internal_misc_Unsafe,     getLong_name, getLong_signature,               F_RN)  \
   do_intrinsic(_getFloat,           jdk_internal_misc_Unsafe,     getFloat_name, getFloat_signature,             F_RN)  \
   do_intrinsic(_getDouble,          jdk_internal_misc_Unsafe,     getDouble_name, getDouble_signature,           F_RN)  \
+  do_intrinsic(_getValue,           jdk_internal_misc_Unsafe,     getValue_name, getValue_signature,             F_RN)  \
   do_intrinsic(_putReference,       jdk_internal_misc_Unsafe,     putReference_name, putReference_signature,     F_RN)  \
   do_intrinsic(_putBoolean,         jdk_internal_misc_Unsafe,     putBoolean_name, putBoolean_signature,         F_RN)  \
   do_intrinsic(_putByte,            jdk_internal_misc_Unsafe,     putByte_name, putByte_signature,               F_RN)  \
   do_intrinsic(_putShort,           jdk_internal_misc_Unsafe,     putShort_name, putShort_signature,             F_RN)  \
   do_intrinsic(_putChar,            jdk_internal_misc_Unsafe,     putChar_name, putChar_signature,               F_RN)  \
   do_intrinsic(_putInt,             jdk_internal_misc_Unsafe,     putInt_name, putInt_signature,                 F_RN)  \
   do_intrinsic(_putLong,            jdk_internal_misc_Unsafe,     putLong_name, putLong_signature,               F_RN)  \
   do_intrinsic(_putFloat,           jdk_internal_misc_Unsafe,     putFloat_name, putFloat_signature,             F_RN)  \
   do_intrinsic(_putDouble,          jdk_internal_misc_Unsafe,     putDouble_name, putDouble_signature,           F_RN)  \
+  do_intrinsic(_putValue,           jdk_internal_misc_Unsafe,     putValue_name, putValue_signature,             F_RN)  \
+                                                                                                                        \
+  do_intrinsic(_makePrivateBuffer,  jdk_internal_misc_Unsafe,     makePrivateBuffer_name, object_object_signature, F_RN)   \
+  do_intrinsic(_finishPrivateBuffer,  jdk_internal_misc_Unsafe,   finishPrivateBuffer_name, object_object_signature, F_RN) \
                                                                                                                         \
   do_name(getReferenceVolatile_name,"getReferenceVolatile")   do_name(putReferenceVolatile_name,"putReferenceVolatile") \
   do_name(getBooleanVolatile_name,"getBooleanVolatile")       do_name(putBooleanVolatile_name,"putBooleanVolatile")     \
   do_name(getByteVolatile_name,"getByteVolatile")             do_name(putByteVolatile_name,"putByteVolatile")           \
   do_name(getShortVolatile_name,"getShortVolatile")           do_name(putShortVolatile_name,"putShortVolatile")         \
diff a/src/hotspot/share/code/compiledMethod.cpp b/src/hotspot/share/code/compiledMethod.cpp
--- a/src/hotspot/share/code/compiledMethod.cpp
+++ b/src/hotspot/share/code/compiledMethod.cpp
@@ -286,19 +286,19 @@
 ScopeDesc* CompiledMethod::scope_desc_at(address pc) {
   PcDesc* pd = pc_desc_at(pc);
   guarantee(pd != NULL, "scope must be present");
   return new ScopeDesc(this, pd->scope_decode_offset(),
                        pd->obj_decode_offset(), pd->should_reexecute(), pd->rethrow_exception(),
-                       pd->return_oop());
+                       pd->return_oop(), pd->return_vt());
 }
 
 ScopeDesc* CompiledMethod::scope_desc_near(address pc) {
   PcDesc* pd = pc_desc_near(pc);
   guarantee(pd != NULL, "scope must be present");
   return new ScopeDesc(this, pd->scope_decode_offset(),
                        pd->obj_decode_offset(), pd->should_reexecute(), pd->rethrow_exception(),
-                       pd->return_oop());
+                       pd->return_oop(), pd->return_vt());
 }
 
 address CompiledMethod::oops_reloc_begin() const {
   // If the method is not entrant or zombie then a JMP is plastered over the
   // first few bytes.  If an oop in the old code was there, that oop
@@ -354,23 +354,37 @@
 // Method that knows how to preserve outgoing arguments at call. This method must be
 // called with a frame corresponding to a Java invoke
 void CompiledMethod::preserve_callee_argument_oops(frame fr, const RegisterMap *reg_map, OopClosure* f) {
   if (method() != NULL && !method()->is_native()) {
     address pc = fr.pc();
-    SimpleScopeDesc ssd(this, pc);
-    Bytecode_invoke call(methodHandle(Thread::current(), ssd.method()), ssd.bci());
-    bool has_receiver = call.has_receiver();
-    bool has_appendix = call.has_appendix();
-    Symbol* signature = call.signature();
-
     // The method attached by JIT-compilers should be used, if present.
     // Bytecode can be inaccurate in such case.
     Method* callee = attached_method_before_pc(pc);
+    bool has_receiver = false;
+    bool has_appendix = false;
+    Symbol* signature = NULL;
     if (callee != NULL) {
       has_receiver = !(callee->access_flags().is_static());
       has_appendix = false;
       signature = callee->signature();
+
+      // If value types are passed as fields, use the extended signature
+      // which contains the types of all (oop) fields of the value type.
+      if (this->is_compiled_by_c2() && callee->has_scalarized_args()) {
+        const GrowableArray<SigEntry>* sig = callee->adapter()->get_sig_cc();
+        assert(sig != NULL, "sig should never be null");
+        TempNewSymbol tmp_sig = SigEntry::create_symbol(sig);
+        has_receiver = false; // The extended signature contains the receiver type
+        fr.oops_compiled_arguments_do(tmp_sig, has_receiver, has_appendix, reg_map, f);
+        return;
+      }
+    } else {
+      SimpleScopeDesc ssd(this, pc);
+      Bytecode_invoke call(methodHandle(Thread::current(), ssd.method()), ssd.bci());
+      has_receiver = call.has_receiver();
+      has_appendix = call.has_appendix();
+      signature = call.signature();
     }
 
     fr.oops_compiled_arguments_do(signature, has_receiver, has_appendix, reg_map, f);
   }
 }
diff a/src/hotspot/share/compiler/compileBroker.cpp b/src/hotspot/share/compiler/compileBroker.cpp
--- a/src/hotspot/share/compiler/compileBroker.cpp
+++ b/src/hotspot/share/compiler/compileBroker.cpp
@@ -1178,11 +1178,11 @@
 
       if (!UseJVMCINativeLibrary) {
         // Don't allow blocking compiles if inside a class initializer or while performing class loading
         vframeStream vfst((JavaThread*) thread);
         for (; !vfst.at_end(); vfst.next()) {
-          if (vfst.method()->is_static_initializer() ||
+        if (vfst.method()->is_class_initializer() ||
               (vfst.method()->method_holder()->is_subclass_of(SystemDictionary::ClassLoader_klass()) &&
                   vfst.method()->name() == vmSymbols::loadClass_name())) {
             blocking = false;
             break;
           }
diff a/src/hotspot/share/gc/parallel/psParallelCompact.cpp b/src/hotspot/share/gc/parallel/psParallelCompact.cpp
--- a/src/hotspot/share/gc/parallel/psParallelCompact.cpp
+++ b/src/hotspot/share/gc/parallel/psParallelCompact.cpp
@@ -66,10 +66,11 @@
 #include "oops/instanceKlass.inline.hpp"
 #include "oops/instanceMirrorKlass.inline.hpp"
 #include "oops/methodData.hpp"
 #include "oops/objArrayKlass.inline.hpp"
 #include "oops/oop.inline.hpp"
+#include "oops/valueArrayKlass.inline.hpp"
 #include "runtime/atomic.hpp"
 #include "runtime/handles.inline.hpp"
 #include "runtime/safepoint.hpp"
 #include "runtime/vmThread.hpp"
 #include "services/management.hpp"
diff a/src/hotspot/share/gc/shenandoah/c2/shenandoahBarrierSetC2.cpp b/src/hotspot/share/gc/shenandoah/c2/shenandoahBarrierSetC2.cpp
--- a/src/hotspot/share/gc/shenandoah/c2/shenandoahBarrierSetC2.cpp
+++ b/src/hotspot/share/gc/shenandoah/c2/shenandoahBarrierSetC2.cpp
@@ -1044,11 +1044,11 @@
 }
 #endif
 
 Node* ShenandoahBarrierSetC2::ideal_node(PhaseGVN* phase, Node* n, bool can_reshape) const {
   if (is_shenandoah_wb_pre_call(n)) {
-    uint cnt = ShenandoahBarrierSetC2::write_ref_field_pre_entry_Type()->domain()->cnt();
+    uint cnt = ShenandoahBarrierSetC2::write_ref_field_pre_entry_Type()->domain_sig()->cnt();
     if (n->req() > cnt) {
       Node* addp = n->in(cnt);
       if (has_only_shenandoah_wb_pre_uses(addp)) {
         n->del_req(cnt);
         if (can_reshape) {
@@ -1131,11 +1131,11 @@
     case Op_CallLeaf:
     case Op_CallLeafNoFP: {
       assert (n->is_Call(), "");
       CallNode *call = n->as_Call();
       if (ShenandoahBarrierSetC2::is_shenandoah_wb_pre_call(call)) {
-        uint cnt = ShenandoahBarrierSetC2::write_ref_field_pre_entry_Type()->domain()->cnt();
+        uint cnt = ShenandoahBarrierSetC2::write_ref_field_pre_entry_Type()->domain_sig()->cnt();
         if (call->req() > cnt) {
           assert(call->req() == cnt + 1, "only one extra input");
           Node *addp = call->in(cnt);
           assert(!ShenandoahBarrierSetC2::has_only_shenandoah_wb_pre_uses(addp), "useless address computation?");
           call->del_req(cnt);
diff a/src/hotspot/share/gc/shenandoah/c2/shenandoahSupport.cpp b/src/hotspot/share/gc/shenandoah/c2/shenandoahSupport.cpp
--- a/src/hotspot/share/gc/shenandoah/c2/shenandoahSupport.cpp
+++ b/src/hotspot/share/gc/shenandoah/c2/shenandoahSupport.cpp
@@ -457,11 +457,11 @@
           { -1,  ShenandoahNone},                 { -1,  ShenandoahNone},                 { -1,  ShenandoahNone} },
       };
 
       if (call->is_call_to_arraycopystub()) {
         Node* dest = NULL;
-        const TypeTuple* args = n->as_Call()->_tf->domain();
+        const TypeTuple* args = n->as_Call()->_tf->domain_sig();
         for (uint i = TypeFunc::Parms, j = 0; i < args->cnt(); i++) {
           if (args->field_at(i)->isa_ptr()) {
             j++;
             if (j == 2) {
               dest = n->in(i);
@@ -576,11 +576,11 @@
       for (; i < others_len; i++) {
         if (others[i].opcode == n->Opcode()) {
           break;
         }
       }
-      uint stop = n->is_Call() ? n->as_Call()->tf()->domain()->cnt() : n->req();
+      uint stop = n->is_Call() ? n->as_Call()->tf()->domain_sig()->cnt() : n->req();
       if (i != others_len) {
         const uint inputs_len = sizeof(others[0].inputs) / sizeof(others[0].inputs[0]);
         for (uint j = 0; j < inputs_len; j++) {
           int pos = others[i].inputs[j].pos;
           if (pos == -1) {
@@ -796,22 +796,21 @@
           }
         }
       }
     } else {
       if (c->is_Call() && c->as_Call()->adr_type() != NULL) {
-        CallProjections projs;
-        c->as_Call()->extract_projections(&projs, true, false);
-        if (projs.fallthrough_memproj != NULL) {
-          if (projs.fallthrough_memproj->adr_type() == TypePtr::BOTTOM) {
-            if (projs.catchall_memproj == NULL) {
-              mem = projs.fallthrough_memproj;
+        CallProjections* projs = c->as_Call()->extract_projections(true, false);
+        if (projs->fallthrough_memproj != NULL) {
+          if (projs->fallthrough_memproj->adr_type() == TypePtr::BOTTOM) {
+            if (projs->catchall_memproj == NULL) {
+              mem = projs->fallthrough_memproj;
             } else {
-              if (phase->is_dominator(projs.fallthrough_catchproj, ctrl)) {
-                mem = projs.fallthrough_memproj;
+              if (phase->is_dominator(projs->fallthrough_catchproj, ctrl)) {
+                mem = projs->fallthrough_memproj;
               } else {
-                assert(phase->is_dominator(projs.catchall_catchproj, ctrl), "one proj must dominate barrier");
-                mem = projs.catchall_memproj;
+                assert(phase->is_dominator(projs->catchall_catchproj, ctrl), "one proj must dominate barrier");
+                mem = projs->catchall_memproj;
               }
             }
           }
         } else {
           Node* proj = c->as_Call()->proj_out(TypeFunc::Memory);
@@ -1040,11 +1039,11 @@
       }
     }
   }
 }
 
-static Node* create_phis_on_call_return(Node* ctrl, Node* c, Node* n, Node* n_clone, const CallProjections& projs, PhaseIdealLoop* phase) {
+static Node* create_phis_on_call_return(Node* ctrl, Node* c, Node* n, Node* n_clone, const CallProjections* projs, PhaseIdealLoop* phase) {
   Node* region = NULL;
   while (c != ctrl) {
     if (c->is_Region()) {
       region = c;
     }
@@ -1052,13 +1051,13 @@
   }
   assert(region != NULL, "");
   Node* phi = new PhiNode(region, n->bottom_type());
   for (uint j = 1; j < region->req(); j++) {
     Node* in = region->in(j);
-    if (phase->is_dominator(projs.fallthrough_catchproj, in)) {
+    if (phase->is_dominator(projs->fallthrough_catchproj, in)) {
       phi->init_req(j, n);
-    } else if (phase->is_dominator(projs.catchall_catchproj, in)) {
+    } else if (phase->is_dominator(projs->catchall_catchproj, in)) {
       phi->init_req(j, n_clone);
     } else {
       phi->init_req(j, create_phis_on_call_return(ctrl, in, n, n_clone, projs, phase));
     }
   }
@@ -1172,19 +1171,17 @@
             stack.pop();
           }
         } while(stack.size() > 0);
         continue;
       }
-      CallProjections projs;
-      call->extract_projections(&projs, false, false);
-
+      CallProjections* projs = call->extract_projections(false, false);
 #ifdef ASSERT
       VectorSet cloned(Thread::current()->resource_area());
 #endif
       Node* lrb_clone = lrb->clone();
-      phase->register_new_node(lrb_clone, projs.catchall_catchproj);
-      phase->set_ctrl(lrb, projs.fallthrough_catchproj);
+      phase->register_new_node(lrb_clone, projs->catchall_catchproj);
+      phase->set_ctrl(lrb, projs->fallthrough_catchproj);
 
       stack.push(lrb, 0);
       clones.push(lrb_clone);
 
       do {
@@ -1202,42 +1199,42 @@
         uint idx = stack.index();
         Node* n_clone = clones.at(clones.size()-1);
         if (idx < n->outcnt()) {
           Node* u = n->raw_out(idx);
           Node* c = phase->ctrl_or_self(u);
-          if (phase->is_dominator(call, c) && phase->is_dominator(c, projs.fallthrough_proj)) {
+          if (phase->is_dominator(call, c) && phase->is_dominator(c, projs->fallthrough_proj)) {
             stack.set_index(idx+1);
             assert(!u->is_CFG(), "");
             stack.push(u, 0);
             assert(!cloned.test_set(u->_idx), "only one clone");
             Node* u_clone = u->clone();
             int nb = u_clone->replace_edge(n, n_clone);
             assert(nb > 0, "should have replaced some uses");
-            phase->register_new_node(u_clone, projs.catchall_catchproj);
+            phase->register_new_node(u_clone, projs->catchall_catchproj);
             clones.push(u_clone);
-            phase->set_ctrl(u, projs.fallthrough_catchproj);
+            phase->set_ctrl(u, projs->fallthrough_catchproj);
           } else {
             bool replaced = false;
             if (u->is_Phi()) {
               for (uint k = 1; k < u->req(); k++) {
                 if (u->in(k) == n) {
-                  if (phase->is_dominator(projs.catchall_catchproj, u->in(0)->in(k))) {
+                  if (phase->is_dominator(projs->catchall_catchproj, u->in(0)->in(k))) {
                     phase->igvn().replace_input_of(u, k, n_clone);
                     replaced = true;
-                  } else if (!phase->is_dominator(projs.fallthrough_catchproj, u->in(0)->in(k))) {
+                  } else if (!phase->is_dominator(projs->fallthrough_catchproj, u->in(0)->in(k))) {
                     phase->igvn().replace_input_of(u, k, create_phis_on_call_return(ctrl, u->in(0)->in(k), n, n_clone, projs, phase));
                     replaced = true;
                   }
                 }
               }
             } else {
-              if (phase->is_dominator(projs.catchall_catchproj, c)) {
+              if (phase->is_dominator(projs->catchall_catchproj, c)) {
                 phase->igvn().rehash_node_delayed(u);
                 int nb = u->replace_edge(n, n_clone);
                 assert(nb > 0, "should have replaced some uses");
                 replaced = true;
-              } else if (!phase->is_dominator(projs.fallthrough_catchproj, c)) {
+              } else if (!phase->is_dominator(projs->fallthrough_catchproj, c)) {
                 if (u->is_If()) {
                   // Can't break If/Bool/Cmp chain
                   assert(n->is_Bool(), "unexpected If shape");
                   assert(stack.node_at(stack.size()-2)->is_Cmp(), "unexpected If shape");
                   assert(n_clone->is_Bool(), "unexpected clone");
@@ -2388,18 +2385,17 @@
 Node* MemoryGraphFixer::get_ctrl(Node* n) const {
   Node* c = _phase->get_ctrl(n);
   if (n->is_Proj() && n->in(0) != NULL && n->in(0)->is_Call()) {
     assert(c == n->in(0), "");
     CallNode* call = c->as_Call();
-    CallProjections projs;
-    call->extract_projections(&projs, true, false);
-    if (projs.catchall_memproj != NULL) {
-      if (projs.fallthrough_memproj == n) {
-        c = projs.fallthrough_catchproj;
+    CallProjections* projs = call->extract_projections(true, false);
+    if (projs->catchall_memproj != NULL) {
+      if (projs->fallthrough_memproj == n) {
+        c = projs->fallthrough_catchproj;
       } else {
-        assert(projs.catchall_memproj == n, "");
-        c = projs.catchall_catchproj;
+        assert(projs->catchall_memproj == n, "");
+        c = projs->catchall_catchproj;
       }
     }
   }
   return c;
 }
diff a/src/hotspot/share/interpreter/templateInterpreter.cpp b/src/hotspot/share/interpreter/templateInterpreter.cpp
--- a/src/hotspot/share/interpreter/templateInterpreter.cpp
+++ b/src/hotspot/share/interpreter/templateInterpreter.cpp
@@ -76,11 +76,11 @@
 
 //------------------------------------------------------------------------------------------------------------------------
 // Implementation of EntryPoint
 
 EntryPoint::EntryPoint() {
-  assert(number_of_states == 10, "check the code below");
+  assert(number_of_states == 10 , "check the code below");
   _entry[btos] = NULL;
   _entry[ztos] = NULL;
   _entry[ctos] = NULL;
   _entry[stos] = NULL;
   _entry[atos] = NULL;
diff a/src/hotspot/share/jvmci/jvmciCompilerToVM.cpp b/src/hotspot/share/jvmci/jvmciCompilerToVM.cpp
--- a/src/hotspot/share/jvmci/jvmciCompilerToVM.cpp
+++ b/src/hotspot/share/jvmci/jvmciCompilerToVM.cpp
@@ -1257,11 +1257,11 @@
                     objects->append(sv);
                   }
                 }
               }
               bool realloc_failures = Deoptimization::realloc_objects(thread, fst.current(), fst.register_map(), objects, CHECK_NULL);
-              Deoptimization::reassign_fields(fst.current(), fst.register_map(), objects, realloc_failures, false);
+              Deoptimization::reassign_fields(fst.current(), fst.register_map(), objects, realloc_failures, false, CHECK_NULL);
               realloc_called = true;
 
               GrowableArray<ScopeValue*>* local_values = scope->locals();
               assert(local_values != NULL, "NULL locals");
               typeArrayOop array_oop = oopFactory::new_boolArray(local_values->length(), CHECK_NULL);
@@ -1517,11 +1517,11 @@
     // no objects to materialize
     return;
   }
 
   bool realloc_failures = Deoptimization::realloc_objects(thread, fstAfterDeopt.current(), fstAfterDeopt.register_map(), objects, CHECK);
-  Deoptimization::reassign_fields(fstAfterDeopt.current(), fstAfterDeopt.register_map(), objects, realloc_failures, false);
+  Deoptimization::reassign_fields(fstAfterDeopt.current(), fstAfterDeopt.register_map(), objects, realloc_failures, false, THREAD);
 
   for (int frame_index = 0; frame_index < virtualFrames->length(); frame_index++) {
     compiledVFrame* cvf = virtualFrames->at(frame_index);
 
     GrowableArray<ScopeValue*>* scopeLocals = cvf->scope()->locals();
@@ -1905,11 +1905,11 @@
   iklass->link_class(CHECK_NULL);
 
   GrowableArray<Method*> constructors_array;
   for (int i = 0; i < iklass->methods()->length(); i++) {
     Method* m = iklass->methods()->at(i);
-    if (m->is_initializer() && !m->is_static()) {
+    if (m->is_object_constructor()) {
       constructors_array.append(m);
     }
   }
   JVMCIObjectArray methods = JVMCIENV->new_ResolvedJavaMethod_array(constructors_array.length(), JVMCI_CHECK_NULL);
   for (int i = 0; i < constructors_array.length(); i++) {
@@ -1935,11 +1935,11 @@
   iklass->link_class(CHECK_NULL);
 
   GrowableArray<Method*> methods_array;
   for (int i = 0; i < iklass->methods()->length(); i++) {
     Method* m = iklass->methods()->at(i);
-    if (!m->is_initializer() && !m->is_overpass()) {
+    if (!(m->is_object_constructor() || m->is_class_initializer()) && !m->is_overpass()) {
       methods_array.append(m);
     }
   }
   JVMCIObjectArray methods = JVMCIENV->new_ResolvedJavaMethod_array(methods_array.length(), JVMCI_CHECK_NULL);
   for (int i = 0; i < methods_array.length(); i++) {
@@ -2531,15 +2531,15 @@
 
 C2V_VMENTRY_NULL(jobject, asReflectionExecutable, (JNIEnv* env, jobject, jobject jvmci_method))
   requireInHotSpot("asReflectionExecutable", JVMCI_CHECK_NULL);
   methodHandle m(THREAD, JVMCIENV->asMethod(jvmci_method));
   oop executable;
-  if (m->is_initializer()) {
-    if (m->is_static_initializer()) {
+  if (m->is_class_initializer()) {
       JVMCI_THROW_MSG_NULL(IllegalArgumentException,
           "Cannot create java.lang.reflect.Method for class initializer");
-    }
+  }
+  else if (m->is_object_constructor()) {
     executable = Reflection::new_constructor(m, CHECK_NULL);
   } else {
     executable = Reflection::new_method(m, false, CHECK_NULL);
   }
   return JNIHandles::make_local(THREAD, executable);
diff a/src/hotspot/share/memory/oopFactory.cpp b/src/hotspot/share/memory/oopFactory.cpp
--- a/src/hotspot/share/memory/oopFactory.cpp
+++ b/src/hotspot/share/memory/oopFactory.cpp
@@ -32,14 +32,19 @@
 #include "memory/resourceArea.hpp"
 #include "memory/universe.hpp"
 #include "oops/instanceKlass.hpp"
 #include "oops/instanceOop.hpp"
 #include "oops/objArrayKlass.hpp"
+#include "oops/objArrayOop.inline.hpp"
 #include "oops/objArrayOop.hpp"
 #include "oops/oop.inline.hpp"
 #include "oops/typeArrayKlass.hpp"
 #include "oops/typeArrayOop.inline.hpp"
+#include "oops/valueKlass.hpp"
+#include "oops/valueArrayKlass.hpp"
+#include "oops/valueArrayOop.inline.hpp"
+#include "oops/valueArrayOop.hpp"
 #include "runtime/handles.inline.hpp"
 #include "utilities/utf8.hpp"
 
 typeArrayOop oopFactory::new_boolArray(int length, TRAPS) {
   return TypeArrayKlass::cast(Universe::boolArrayKlassObj())->allocate(length, THREAD);
@@ -122,9 +127,42 @@
   } else {
     return InstanceKlass::cast(klass)->allocate_objArray(1, length, THREAD);
   }
 }
 
+arrayOop oopFactory::new_valueArray(Klass* klass, int length, TRAPS) {
+  assert(klass->is_value(), "Klass must be value type");
+  // Request flattened, but we might not actually get it...either way "null-free" are the aaload/aastore semantics
+  Klass* array_klass = klass->array_klass(1, CHECK_NULL);
+  assert(array_klass->is_null_free_array_klass(), "Expect a null-free array class here");
+
+  arrayOop oop;
+  if (array_klass->is_valueArray_klass()) {
+    oop = (arrayOop) ValueArrayKlass::cast(array_klass)->allocate(length, THREAD);
+  } else {
+    oop = (arrayOop) ObjArrayKlass::cast(array_klass)->allocate(length, THREAD);
+  }
+  assert(oop == NULL || oop->klass()->is_null_free_array_klass(), "Bad array storage encoding");
+  return oop;
+}
+
+objArrayHandle oopFactory::copy_valueArray_to_objArray(valueArrayHandle array, TRAPS) {
+  int len = array->length();
+  ValueArrayKlass* vak = ValueArrayKlass::cast(array->klass());
+  objArrayOop oarray = new_objectArray(array->length(), CHECK_(objArrayHandle()));
+  objArrayHandle oarrayh(THREAD, oarray);
+  vak->copy_array(array(), 0, oarrayh(), 0, len, CHECK_(objArrayHandle()));
+  return oarrayh;
+}
+
+objArrayHandle  oopFactory::ensure_objArray(oop array, TRAPS) {
+  if (array != NULL && array->is_valueArray()) {
+    return copy_valueArray_to_objArray(valueArrayHandle(THREAD, valueArrayOop(array)), THREAD);
+  } else {
+    return objArrayHandle(THREAD, objArrayOop(array));
+  }
+}
+
 objArrayHandle oopFactory::new_objArray_handle(Klass* klass, int length, TRAPS) {
   objArrayOop obj = new_objArray(klass, length, CHECK_(objArrayHandle()));
   return objArrayHandle(THREAD, obj);
 }
diff a/src/hotspot/share/memory/oopFactory.hpp b/src/hotspot/share/memory/oopFactory.hpp
--- a/src/hotspot/share/memory/oopFactory.hpp
+++ b/src/hotspot/share/memory/oopFactory.hpp
@@ -54,10 +54,23 @@
   static typeArrayOop    new_symbolArray(int length, TRAPS);
 
   // Regular object arrays
   static objArrayOop     new_objArray(Klass* klass, int length, TRAPS);
 
+  // Value arrays...
+  // LWorld:
+  //    - Q-type signature allocation should use this path.
+  //    - L-type signature allocation should use new_objArray
+  //
+  // Method specifically null free and possibly flattened if possible
+  // i.e. valueArrayOop if flattening can be done, else "null free" objArrayOop
+  static arrayOop        new_valueArray(Klass* klass, int length, TRAPS);
+
+  // Helper conversions from value to obj array...
+  static objArrayHandle  copy_valueArray_to_objArray(valueArrayHandle array, TRAPS);
+  static objArrayHandle  ensure_objArray(oop array, TRAPS); // copy into new objArray if not already an objArray
+
   // Helper that returns a Handle
   static objArrayHandle  new_objArray_handle(Klass* klass, int length, TRAPS);
 };
 
 #endif // SHARE_MEMORY_OOPFACTORY_HPP
diff a/src/hotspot/share/memory/universe.cpp b/src/hotspot/share/memory/universe.cpp
--- a/src/hotspot/share/memory/universe.cpp
+++ b/src/hotspot/share/memory/universe.cpp
@@ -114,10 +114,11 @@
 LatestMethodCache* Universe::_finalizer_register_cache = NULL;
 LatestMethodCache* Universe::_loader_addClass_cache    = NULL;
 LatestMethodCache* Universe::_throw_illegal_access_error_cache = NULL;
 LatestMethodCache* Universe::_throw_no_such_method_error_cache = NULL;
 LatestMethodCache* Universe::_do_stack_walk_cache     = NULL;
+LatestMethodCache* Universe::_is_substitutable_cache  = NULL;
 oop Universe::_out_of_memory_error_java_heap          = NULL;
 oop Universe::_out_of_memory_error_metaspace          = NULL;
 oop Universe::_out_of_memory_error_class_metaspace    = NULL;
 oop Universe::_out_of_memory_error_array_size         = NULL;
 oop Universe::_out_of_memory_error_gc_overhead_limit  = NULL;
@@ -136,10 +137,11 @@
 
 Array<int>* Universe::_the_empty_int_array            = NULL;
 Array<u2>* Universe::_the_empty_short_array           = NULL;
 Array<Klass*>* Universe::_the_empty_klass_array     = NULL;
 Array<InstanceKlass*>* Universe::_the_empty_instance_klass_array  = NULL;
+Array<InstanceKlass*>* Universe::_the_single_IdentityObject_klass_array = NULL;
 Array<Method*>* Universe::_the_empty_method_array   = NULL;
 
 // These variables are guarded by FullGCALot_lock.
 debug_only(objArrayOop Universe::_fullgc_alot_dummy_array = NULL;)
 debug_only(int Universe::_fullgc_alot_dummy_next      = 0;)
@@ -222,16 +224,18 @@
   it->push(&_the_empty_short_array);
   it->push(&_the_empty_klass_array);
   it->push(&_the_empty_instance_klass_array);
   it->push(&_the_empty_method_array);
   it->push(&_the_array_interfaces_array);
+  it->push(&_the_single_IdentityObject_klass_array);
 
   _finalizer_register_cache->metaspace_pointers_do(it);
   _loader_addClass_cache->metaspace_pointers_do(it);
   _throw_illegal_access_error_cache->metaspace_pointers_do(it);
   _throw_no_such_method_error_cache->metaspace_pointers_do(it);
   _do_stack_walk_cache->metaspace_pointers_do(it);
+  _is_substitutable_cache->metaspace_pointers_do(it);
 }
 
 #define ASSERT_MIRROR_NULL(m) \
   assert(m == NULL, "archived mirrors should be NULL");
 
@@ -259,15 +263,17 @@
   f->do_ptr((void**)&_the_empty_int_array);
   f->do_ptr((void**)&_the_empty_short_array);
   f->do_ptr((void**)&_the_empty_method_array);
   f->do_ptr((void**)&_the_empty_klass_array);
   f->do_ptr((void**)&_the_empty_instance_klass_array);
+  f->do_ptr((void**)&_the_single_IdentityObject_klass_array);
   _finalizer_register_cache->serialize(f);
   _loader_addClass_cache->serialize(f);
   _throw_illegal_access_error_cache->serialize(f);
   _throw_no_such_method_error_cache->serialize(f);
   _do_stack_walk_cache->serialize(f);
+  _is_substitutable_cache->serialize(f);
 }
 
 void Universe::check_alignment(uintx size, uintx alignment, const char* name) {
   if (size < alignment || size % alignment != 0) {
     vm_exit_during_initialization(
@@ -311,11 +317,11 @@
           _typeArrayKlassObjs[i] = TypeArrayKlass::create_klass((BasicType)i, CHECK);
         }
 
         ClassLoaderData* null_cld = ClassLoaderData::the_null_class_loader_data();
 
-        _the_array_interfaces_array     = MetadataFactory::new_array<Klass*>(null_cld, 2, NULL, CHECK);
+        _the_array_interfaces_array     = MetadataFactory::new_array<Klass*>(null_cld, 3, NULL, CHECK);
         _the_empty_int_array            = MetadataFactory::new_array<int>(null_cld, 0, CHECK);
         _the_empty_short_array          = MetadataFactory::new_array<u2>(null_cld, 0, CHECK);
         _the_empty_method_array         = MetadataFactory::new_array<Method*>(null_cld, 0, CHECK);
         _the_empty_klass_array          = MetadataFactory::new_array<Klass*>(null_cld, 0, CHECK);
         _the_empty_instance_klass_array = MetadataFactory::new_array<InstanceKlass*>(null_cld, 0, CHECK);
@@ -336,16 +342,22 @@
       // Verify shared interfaces array.
       assert(_the_array_interfaces_array->at(0) ==
              SystemDictionary::Cloneable_klass(), "u3");
       assert(_the_array_interfaces_array->at(1) ==
              SystemDictionary::Serializable_klass(), "u3");
+      assert(_the_array_interfaces_array->at(2) ==
+                   SystemDictionary::IdentityObject_klass(), "u3");
+
+      assert(_the_single_IdentityObject_klass_array->at(0) ==
+          SystemDictionary::IdentityObject_klass(), "u3");
     } else
 #endif
     {
       // Set up shared interfaces array.  (Do this before supers are set up.)
       _the_array_interfaces_array->at_put(0, SystemDictionary::Cloneable_klass());
       _the_array_interfaces_array->at_put(1, SystemDictionary::Serializable_klass());
+      _the_array_interfaces_array->at_put(2, SystemDictionary::IdentityObject_klass());
     }
 
     initialize_basic_type_klass(boolArrayKlassObj(), CHECK);
     initialize_basic_type_klass(charArrayKlassObj(), CHECK);
     initialize_basic_type_klass(floatArrayKlassObj(), CHECK);
@@ -463,10 +475,18 @@
     _mirrors[T_VOID]    = _void_mirror;
   //_mirrors[T_OBJECT]  = _object_klass->java_mirror();
   //_mirrors[T_ARRAY]   = _object_klass->java_mirror();
 }
 
+void Universe::initialize_the_single_IdentityObject_klass_array(InstanceKlass* ik, TRAPS) {
+    assert(_the_single_IdentityObject_klass_array == NULL, "Must not be initialized twice");
+    assert(ik->name() == vmSymbols::java_lang_IdentityObject(), "Must be");
+    Array<InstanceKlass*>* array = MetadataFactory::new_array<InstanceKlass*>(ik->class_loader_data(), 1, NULL, CHECK);
+    array->at_put(0, ik);
+    _the_single_IdentityObject_klass_array = array;
+  }
+
 void Universe::fixup_mirrors(TRAPS) {
   // Bootstrap problem: all classes gets a mirror (java.lang.Class instance) assigned eagerly,
   // but we cannot do that for classes created before java.lang.Class is loaded. Here we simply
   // walk over permanent objects created so far (mostly classes) and fixup their mirrors. Note
   // that the number of objects allocated at this point is very small.
@@ -659,11 +679,10 @@
   }
 
   Universe::initialize_tlab();
 
   Metaspace::global_initialize();
-
   // Initialize performance counters for metaspaces
   MetaspaceCounters::initialize_performance_counters();
   CompressedClassSpaceCounters::initialize_performance_counters();
 
   AOTLoader::universe_init();
@@ -682,10 +701,11 @@
   Universe::_finalizer_register_cache = new LatestMethodCache();
   Universe::_loader_addClass_cache    = new LatestMethodCache();
   Universe::_throw_illegal_access_error_cache = new LatestMethodCache();
   Universe::_throw_no_such_method_error_cache = new LatestMethodCache();
   Universe::_do_stack_walk_cache = new LatestMethodCache();
+  Universe::_is_substitutable_cache = new LatestMethodCache();
 
 #if INCLUDE_CDS
   if (UseSharedSpaces) {
     // Read the data structures supporting the shared spaces (shared
     // system dictionary, symbol table, etc.).  After that, access to
@@ -837,10 +857,17 @@
   // Set up method for stack walking
   initialize_known_method(_do_stack_walk_cache,
                           SystemDictionary::AbstractStackWalker_klass(),
                           "doStackWalk",
                           vmSymbols::doStackWalk_signature(), false, CHECK);
+
+  // Set up substitutability testing
+  ResourceMark rm;
+  initialize_known_method(_is_substitutable_cache,
+                          SystemDictionary::ValueBootstrapMethods_klass(),
+                          vmSymbols::isSubstitutable_name()->as_C_string(),
+                          vmSymbols::object_object_boolean_signature(), true, CHECK);
 }
 
 void universe2_init() {
   EXCEPTION_MARK;
   Universe::genesis(CATCH);
diff a/src/hotspot/share/oops/access.hpp b/src/hotspot/share/oops/access.hpp
--- a/src/hotspot/share/oops/access.hpp
+++ b/src/hotspot/share/oops/access.hpp
@@ -55,10 +55,11 @@
 // * atomic_cmpxchg_at: Atomically compare-and-swap a new value at an internal pointer address if previous value matched the compared value.
 // * atomic_xchg: Atomically swap a new value at an address if previous value matched the compared value.
 // * atomic_xchg_at: Atomically swap a new value at an internal pointer address if previous value matched the compared value.
 // * arraycopy: Copy data from one heap array to another heap array. The ArrayAccess class has convenience functions for this.
 // * clone: Clone the contents of an object to a newly allocated object.
+// * value_copy: Copy the contents of a value type from one heap address to another
 // * resolve: Resolve a stable to-space invariant oop that is guaranteed not to relocate its payload until a subsequent thread transition.
 //
 // == IMPLEMENTATION ==
 // Each access goes through the following steps in a template pipeline.
 // There are essentially 5 steps for each access:
@@ -121,25 +122,31 @@
     const DecoratorSet heap_oop_decorators = AS_DECORATOR_MASK | ON_DECORATOR_MASK |
                                              IN_HEAP | IS_ARRAY | IS_NOT_NULL;
     verify_decorators<expected_mo_decorators | heap_oop_decorators>();
   }
 
+  template <DecoratorSet expected_mo_decorators>
+  static void verify_heap_value_decorators() {
+    const DecoratorSet heap_value_decorators = IN_HEAP | IS_DEST_UNINITIALIZED;
+    verify_decorators<expected_mo_decorators | heap_value_decorators>();
+  }
+
   static const DecoratorSet load_mo_decorators = MO_UNORDERED | MO_RELAXED | MO_ACQUIRE | MO_SEQ_CST;
   static const DecoratorSet store_mo_decorators = MO_UNORDERED | MO_RELAXED | MO_RELEASE | MO_SEQ_CST;
   static const DecoratorSet atomic_xchg_mo_decorators = MO_SEQ_CST;
   static const DecoratorSet atomic_cmpxchg_mo_decorators = MO_RELAXED | MO_SEQ_CST;
 
 protected:
   template <typename T>
-  static inline bool oop_arraycopy(arrayOop src_obj, size_t src_offset_in_bytes, const T* src_raw,
+  static inline void oop_arraycopy(arrayOop src_obj, size_t src_offset_in_bytes, const T* src_raw,
                                    arrayOop dst_obj, size_t dst_offset_in_bytes, T* dst_raw,
                                    size_t length) {
     verify_decorators<ARRAYCOPY_DECORATOR_MASK | IN_HEAP |
                       AS_DECORATOR_MASK | IS_ARRAY | IS_DEST_UNINITIALIZED>();
-    return AccessInternal::arraycopy<decorators | INTERNAL_VALUE_IS_OOP>(src_obj, src_offset_in_bytes, src_raw,
-                                                                         dst_obj, dst_offset_in_bytes, dst_raw,
-                                                                         length);
+    AccessInternal::arraycopy<decorators | INTERNAL_VALUE_IS_OOP>(src_obj, src_offset_in_bytes, src_raw,
+                                                                  dst_obj, dst_offset_in_bytes, dst_raw,
+                                                                  length);
   }
 
   template <typename T>
   static inline void arraycopy(arrayOop src_obj, size_t src_offset_in_bytes, const T* src_raw,
                                arrayOop dst_obj, size_t dst_offset_in_bytes, T* dst_raw,
@@ -211,10 +218,18 @@
   static inline void clone(oop src, oop dst, size_t size) {
     verify_decorators<IN_HEAP>();
     AccessInternal::clone<decorators>(src, dst, size);
   }
 
+  // Value type inline heap access (flattened)...
+
+  // Copy value type data from src to dst
+  static inline void value_copy(void* src, void* dst, ValueKlass* md) {
+    verify_heap_value_decorators<IN_HEAP>();
+    AccessInternal::value_copy<decorators>(src, dst, md);
+  }
+
   // Primitive accesses
   template <typename P>
   static inline P load(P* addr) {
     verify_primitive_decorators<load_mo_decorators>();
     return AccessInternal::load<decorators, P, P>(addr);
@@ -321,23 +336,23 @@
     AccessT::arraycopy(NULL, 0, src,
                        dst_obj, dst_offset_in_bytes, reinterpret_cast<T*>(NULL),
                        length);
   }
 
-  static inline bool oop_arraycopy(arrayOop src_obj, size_t src_offset_in_bytes,
+  static inline void oop_arraycopy(arrayOop src_obj, size_t src_offset_in_bytes,
                                    arrayOop dst_obj, size_t dst_offset_in_bytes,
                                    size_t length) {
-    return AccessT::oop_arraycopy(src_obj, src_offset_in_bytes, reinterpret_cast<const HeapWord*>(NULL),
-                                  dst_obj, dst_offset_in_bytes, reinterpret_cast<HeapWord*>(NULL),
-                                  length);
+    AccessT::oop_arraycopy(src_obj, src_offset_in_bytes, reinterpret_cast<const HeapWord*>(NULL),
+                           dst_obj, dst_offset_in_bytes, reinterpret_cast<HeapWord*>(NULL),
+                           length);
   }
 
   template <typename T>
-  static inline bool oop_arraycopy_raw(T* src, T* dst, size_t length) {
-    return AccessT::oop_arraycopy(NULL, 0, src,
-                                  NULL, 0, dst,
-                                  length);
+  static inline void oop_arraycopy_raw(T* src, T* dst, size_t length) {
+    AccessT::oop_arraycopy(NULL, 0, src,
+                           NULL, 0, dst,
+                           length);
   }
 
 };
 
 template <DecoratorSet decorators>
diff a/src/hotspot/share/oops/accessBackend.hpp b/src/hotspot/share/oops/accessBackend.hpp
--- a/src/hotspot/share/oops/accessBackend.hpp
+++ b/src/hotspot/share/oops/accessBackend.hpp
@@ -50,10 +50,18 @@
   static const bool needs_oop_compress = HasDecorator<decorators, INTERNAL_CONVERT_COMPRESSED_OOP>::value &&
                                          HasDecorator<decorators, INTERNAL_RT_USE_COMPRESSED_OOPS>::value;
   typedef typename Conditional<needs_oop_compress, narrowOop, oop>::type type;
 };
 
+// This meta-function returns either oop or narrowOop depending on whether
+// a back-end needs to consider compressed oops types or not.
+template <DecoratorSet decorators>
+struct ValueOopType: AllStatic {
+  static const bool needs_oop_compress = HasDecorator<decorators, INTERNAL_RT_USE_COMPRESSED_OOPS>::value;
+  typedef typename Conditional<needs_oop_compress, narrowOop, oop>::type type;
+};
+
 namespace AccessInternal {
   enum BarrierType {
     BARRIER_STORE,
     BARRIER_STORE_AT,
     BARRIER_LOAD,
@@ -62,10 +70,11 @@
     BARRIER_ATOMIC_CMPXCHG_AT,
     BARRIER_ATOMIC_XCHG,
     BARRIER_ATOMIC_XCHG_AT,
     BARRIER_ARRAYCOPY,
     BARRIER_CLONE,
+    BARRIER_VALUE_COPY,
     BARRIER_RESOLVE
   };
 
   template <DecoratorSet decorators, typename T>
   struct MustConvertCompressedOop: public IntegralConstant<bool,
@@ -108,20 +117,21 @@
     typedef T (*load_func_t)(void* addr);
     typedef void (*store_func_t)(void* addr, T value);
     typedef T (*atomic_cmpxchg_func_t)(void* addr, T compare_value, T new_value);
     typedef T (*atomic_xchg_func_t)(void* addr, T new_value);
 
-    typedef bool (*arraycopy_func_t)(arrayOop src_obj, size_t src_offset_in_bytes, T* src_raw,
+    typedef void (*arraycopy_func_t)(arrayOop src_obj, size_t src_offset_in_bytes, T* src_raw,
                                      arrayOop dst_obj, size_t dst_offset_in_bytes, T* dst_raw,
                                      size_t length);
     typedef void (*clone_func_t)(oop src, oop dst, size_t size);
+    typedef void (*value_copy_func_t)(void* src, void* dst, ValueKlass* md);
     typedef oop (*resolve_func_t)(oop obj);
   };
 
   template <DecoratorSet decorators>
   struct AccessFunctionTypes<decorators, void> {
-    typedef bool (*arraycopy_func_t)(arrayOop src_obj, size_t src_offset_in_bytes, void* src,
+    typedef void (*arraycopy_func_t)(arrayOop src_obj, size_t src_offset_in_bytes, void* src,
                                      arrayOop dst_obj, size_t dst_offset_in_bytes, void* dst,
                                      size_t length);
   };
 
   template <DecoratorSet decorators, typename T, BarrierType barrier> struct AccessFunction {};
@@ -139,10 +149,11 @@
   ACCESS_GENERATE_ACCESS_FUNCTION(BARRIER_ATOMIC_CMPXCHG_AT, atomic_cmpxchg_at_func_t);
   ACCESS_GENERATE_ACCESS_FUNCTION(BARRIER_ATOMIC_XCHG, atomic_xchg_func_t);
   ACCESS_GENERATE_ACCESS_FUNCTION(BARRIER_ATOMIC_XCHG_AT, atomic_xchg_at_func_t);
   ACCESS_GENERATE_ACCESS_FUNCTION(BARRIER_ARRAYCOPY, arraycopy_func_t);
   ACCESS_GENERATE_ACCESS_FUNCTION(BARRIER_CLONE, clone_func_t);
+  ACCESS_GENERATE_ACCESS_FUNCTION(BARRIER_VALUE_COPY, value_copy_func_t);
   ACCESS_GENERATE_ACCESS_FUNCTION(BARRIER_RESOLVE, resolve_func_t);
 #undef ACCESS_GENERATE_ACCESS_FUNCTION
 
   template <DecoratorSet decorators, typename T, BarrierType barrier_type>
   typename AccessFunction<decorators, T, barrier_type>::type resolve_barrier();
@@ -339,11 +350,11 @@
   static inline T atomic_xchg(void* addr, T new_value) {
     return atomic_xchg_maybe_locked<decorators>(addr, new_value);
   }
 
   template <typename T>
-  static bool arraycopy(arrayOop src_obj, size_t src_offset_in_bytes, T* src_raw,
+  static void arraycopy(arrayOop src_obj, size_t src_offset_in_bytes, T* src_raw,
                         arrayOop dst_obj, size_t dst_offset_in_bytes, T* dst_raw,
                         size_t length);
 
   template <typename T>
   static void oop_store(void* addr, T value);
@@ -384,16 +395,18 @@
   static T atomic_xchg_at(oop base, ptrdiff_t offset, T new_value) {
     return atomic_xchg(field_addr(base, offset), new_value);
   }
 
   template <typename T>
-  static bool oop_arraycopy(arrayOop src_obj, size_t src_offset_in_bytes, T* src_raw,
+  static void oop_arraycopy(arrayOop src_obj, size_t src_offset_in_bytes, T* src_raw,
                             arrayOop dst_obj, size_t dst_offset_in_bytes, T* dst_raw,
                             size_t length);
 
   static void clone(oop src, oop dst, size_t size);
 
+  static void value_copy(void* src, void* dst, ValueKlass* md);
+
   static oop resolve(oop obj) { return obj; }
 };
 
 // Below is the implementation of the first 4 steps of the template pipeline:
 // * Step 1: Set default decorators and decay types. This step gets rid of CV qualifiers
@@ -547,15 +560,15 @@
   template <DecoratorSet decorators, typename T>
   struct RuntimeDispatch<decorators, T, BARRIER_ARRAYCOPY>: AllStatic {
     typedef typename AccessFunction<decorators, T, BARRIER_ARRAYCOPY>::type func_t;
     static func_t _arraycopy_func;
 
-    static bool arraycopy_init(arrayOop src_obj, size_t src_offset_in_bytes, T* src_raw,
+    static void arraycopy_init(arrayOop src_obj, size_t src_offset_in_bytes, T* src_raw,
                                arrayOop dst_obj, size_t dst_offset_in_bytes, T* dst_raw,
                                size_t length);
 
-    static inline bool arraycopy(arrayOop src_obj, size_t src_offset_in_bytes, T* src_raw,
+    static inline void arraycopy(arrayOop src_obj, size_t src_offset_in_bytes, T* src_raw,
                                  arrayOop dst_obj, size_t dst_offset_in_bytes, T* dst_raw,
                                  size_t length) {
       return _arraycopy_func(src_obj, src_offset_in_bytes, src_raw,
                              dst_obj, dst_offset_in_bytes, dst_raw,
                              length);
@@ -572,10 +585,22 @@
     static inline void clone(oop src, oop dst, size_t size) {
       _clone_func(src, dst, size);
     }
   };
 
+  template <DecoratorSet decorators, typename T>
+  struct RuntimeDispatch<decorators, T, BARRIER_VALUE_COPY>: AllStatic {
+    typedef typename AccessFunction<decorators, T, BARRIER_VALUE_COPY>::type func_t;
+    static func_t _value_copy_func;
+
+    static void value_copy_init(void* src, void* dst, ValueKlass* md);
+
+    static inline void value_copy(void* src, void* dst, ValueKlass* md) {
+      _value_copy_func(src, dst, md);
+    }
+  };
+
   template <DecoratorSet decorators, typename T>
   struct RuntimeDispatch<decorators, T, BARRIER_RESOLVE>: AllStatic {
     typedef typename AccessFunction<decorators, T, BARRIER_RESOLVE>::type func_t;
     static func_t _resolve_func;
 
@@ -625,10 +650,14 @@
 
   template <DecoratorSet decorators, typename T>
   typename AccessFunction<decorators, T, BARRIER_CLONE>::type
   RuntimeDispatch<decorators, T, BARRIER_CLONE>::_clone_func = &clone_init;
 
+  template <DecoratorSet decorators, typename T>
+  typename AccessFunction<decorators, T, BARRIER_VALUE_COPY>::type
+  RuntimeDispatch<decorators, T, BARRIER_VALUE_COPY>::_value_copy_func = &value_copy_init;
+
   template <DecoratorSet decorators, typename T>
   typename AccessFunction<decorators, T, BARRIER_RESOLVE>::type
   RuntimeDispatch<decorators, T, BARRIER_RESOLVE>::_resolve_func = &resolve_init;
 
   // Step 3: Pre-runtime dispatching.
@@ -877,60 +906,60 @@
       }
     }
 
     template <DecoratorSet decorators, typename T>
     inline static typename EnableIf<
-      HasDecorator<decorators, AS_RAW>::value && CanHardwireRaw<decorators>::value, bool>::type
+      HasDecorator<decorators, AS_RAW>::value && CanHardwireRaw<decorators>::value, void>::type
     arraycopy(arrayOop src_obj, size_t src_offset_in_bytes, T* src_raw,
               arrayOop dst_obj, size_t dst_offset_in_bytes, T* dst_raw,
               size_t length) {
       typedef RawAccessBarrier<decorators & RAW_DECORATOR_MASK> Raw;
       if (HasDecorator<decorators, INTERNAL_VALUE_IS_OOP>::value) {
-        return Raw::oop_arraycopy(src_obj, src_offset_in_bytes, src_raw,
-                                  dst_obj, dst_offset_in_bytes, dst_raw,
-                                  length);
+        Raw::oop_arraycopy(src_obj, src_offset_in_bytes, src_raw,
+                           dst_obj, dst_offset_in_bytes, dst_raw,
+                           length);
       } else {
-        return Raw::arraycopy(src_obj, src_offset_in_bytes, src_raw,
-                              dst_obj, dst_offset_in_bytes, dst_raw,
-                              length);
+        Raw::arraycopy(src_obj, src_offset_in_bytes, src_raw,
+                       dst_obj, dst_offset_in_bytes, dst_raw,
+                       length);
       }
     }
 
     template <DecoratorSet decorators, typename T>
     inline static typename EnableIf<
-      HasDecorator<decorators, AS_RAW>::value && !CanHardwireRaw<decorators>::value, bool>::type
+      HasDecorator<decorators, AS_RAW>::value && !CanHardwireRaw<decorators>::value, void>::type
     arraycopy(arrayOop src_obj, size_t src_offset_in_bytes, T* src_raw,
               arrayOop dst_obj, size_t dst_offset_in_bytes, T* dst_raw,
               size_t length) {
       if (UseCompressedOops) {
         const DecoratorSet expanded_decorators = decorators | convert_compressed_oops;
-        return PreRuntimeDispatch::arraycopy<expanded_decorators>(src_obj, src_offset_in_bytes, src_raw,
-                                                                  dst_obj, dst_offset_in_bytes, dst_raw,
-                                                                  length);
+        PreRuntimeDispatch::arraycopy<expanded_decorators>(src_obj, src_offset_in_bytes, src_raw,
+                                                           dst_obj, dst_offset_in_bytes, dst_raw,
+                                                           length);
       } else {
         const DecoratorSet expanded_decorators = decorators & ~convert_compressed_oops;
-        return PreRuntimeDispatch::arraycopy<expanded_decorators>(src_obj, src_offset_in_bytes, src_raw,
-                                                                  dst_obj, dst_offset_in_bytes, dst_raw,
-                                                                  length);
+        PreRuntimeDispatch::arraycopy<expanded_decorators>(src_obj, src_offset_in_bytes, src_raw,
+                                                           dst_obj, dst_offset_in_bytes, dst_raw,
+                                                           length);
       }
     }
 
     template <DecoratorSet decorators, typename T>
     inline static typename EnableIf<
-      !HasDecorator<decorators, AS_RAW>::value, bool>::type
+      !HasDecorator<decorators, AS_RAW>::value, void>::type
     arraycopy(arrayOop src_obj, size_t src_offset_in_bytes, T* src_raw,
               arrayOop dst_obj, size_t dst_offset_in_bytes, T* dst_raw,
               size_t length) {
       if (is_hardwired_primitive<decorators>()) {
         const DecoratorSet expanded_decorators = decorators | AS_RAW;
-        return PreRuntimeDispatch::arraycopy<expanded_decorators>(src_obj, src_offset_in_bytes, src_raw,
-                                                                  dst_obj, dst_offset_in_bytes, dst_raw,
-                                                                  length);
+        PreRuntimeDispatch::arraycopy<expanded_decorators>(src_obj, src_offset_in_bytes, src_raw,
+                                                           dst_obj, dst_offset_in_bytes, dst_raw,
+                                                           length);
       } else {
-        return RuntimeDispatch<decorators, T, BARRIER_ARRAYCOPY>::arraycopy(src_obj, src_offset_in_bytes, src_raw,
-                                                                            dst_obj, dst_offset_in_bytes, dst_raw,
-                                                                            length);
+        RuntimeDispatch<decorators, T, BARRIER_ARRAYCOPY>::arraycopy(src_obj, src_offset_in_bytes, src_raw,
+                                                                     dst_obj, dst_offset_in_bytes, dst_raw,
+                                                                     length);
       }
     }
 
     template <DecoratorSet decorators>
     inline static typename EnableIf<
@@ -945,10 +974,27 @@
       !HasDecorator<decorators, AS_RAW>::value>::type
     clone(oop src, oop dst, size_t size) {
       RuntimeDispatch<decorators, oop, BARRIER_CLONE>::clone(src, dst, size);
     }
 
+    template <DecoratorSet decorators>
+    inline static typename EnableIf<
+      HasDecorator<decorators, AS_RAW>::value>::type
+    value_copy(void* src, void* dst, ValueKlass* md) {
+      typedef RawAccessBarrier<decorators & RAW_DECORATOR_MASK> Raw;
+      Raw::value_copy(src, dst, md);
+    }
+
+    template <DecoratorSet decorators>
+    inline static typename EnableIf<
+      !HasDecorator<decorators, AS_RAW>::value>::type
+      value_copy(void* src, void* dst, ValueKlass* md) {
+      const DecoratorSet expanded_decorators = decorators;
+      RuntimeDispatch<expanded_decorators, void*, BARRIER_VALUE_COPY>::value_copy(src, dst, md);
+    }
+
+
     template <DecoratorSet decorators>
     inline static typename EnableIf<
       HasDecorator<decorators, INTERNAL_BT_TO_SPACE_INVARIANT>::value, oop>::type
     resolve(oop obj) {
       typedef RawAccessBarrier<decorators & RAW_DECORATOR_MASK> Raw;
@@ -1073,37 +1119,37 @@
     const DecoratorSet expanded_decorators = decorators | INTERNAL_CONVERT_COMPRESSED_OOP;
     return PreRuntimeDispatch::load<expanded_decorators, oop>(addr);
   }
 
   template <DecoratorSet decorators, typename T>
-  inline bool arraycopy_reduce_types(arrayOop src_obj, size_t src_offset_in_bytes, T* src_raw,
+  inline void arraycopy_reduce_types(arrayOop src_obj, size_t src_offset_in_bytes, T* src_raw,
                                      arrayOop dst_obj, size_t dst_offset_in_bytes, T* dst_raw,
                                      size_t length) {
-    return PreRuntimeDispatch::arraycopy<decorators>(src_obj, src_offset_in_bytes, src_raw,
-                                                     dst_obj, dst_offset_in_bytes, dst_raw,
-                                                     length);
+    PreRuntimeDispatch::arraycopy<decorators>(src_obj, src_offset_in_bytes, src_raw,
+                                              dst_obj, dst_offset_in_bytes, dst_raw,
+                                              length);
   }
 
   template <DecoratorSet decorators>
-  inline bool arraycopy_reduce_types(arrayOop src_obj, size_t src_offset_in_bytes, HeapWord* src_raw,
+  inline void arraycopy_reduce_types(arrayOop src_obj, size_t src_offset_in_bytes, HeapWord* src_raw,
                                      arrayOop dst_obj, size_t dst_offset_in_bytes, HeapWord* dst_raw,
                                      size_t length) {
     const DecoratorSet expanded_decorators = decorators | INTERNAL_CONVERT_COMPRESSED_OOP;
-    return PreRuntimeDispatch::arraycopy<expanded_decorators>(src_obj, src_offset_in_bytes, src_raw,
-                                                              dst_obj, dst_offset_in_bytes, dst_raw,
-                                                              length);
+    PreRuntimeDispatch::arraycopy<expanded_decorators>(src_obj, src_offset_in_bytes, src_raw,
+                                                       dst_obj, dst_offset_in_bytes, dst_raw,
+                                                       length);
   }
 
   template <DecoratorSet decorators>
-  inline bool arraycopy_reduce_types(arrayOop src_obj, size_t src_offset_in_bytes, narrowOop* src_raw,
+  inline void arraycopy_reduce_types(arrayOop src_obj, size_t src_offset_in_bytes, narrowOop* src_raw,
                                      arrayOop dst_obj, size_t dst_offset_in_bytes, narrowOop* dst_raw,
                                      size_t length) {
     const DecoratorSet expanded_decorators = decorators | INTERNAL_CONVERT_COMPRESSED_OOP |
                                              INTERNAL_RT_USE_COMPRESSED_OOPS;
-    return PreRuntimeDispatch::arraycopy<expanded_decorators>(src_obj, src_offset_in_bytes, src_raw,
-                                                              dst_obj, dst_offset_in_bytes, dst_raw,
-                                                              length);
+    PreRuntimeDispatch::arraycopy<expanded_decorators>(src_obj, src_offset_in_bytes, src_raw,
+                                                       dst_obj, dst_offset_in_bytes, dst_raw,
+                                                       length);
   }
 
   // Step 1: Set default decorators. This step remembers if a type was volatile
   // and then sets the MO_RELAXED decorator by default. Otherwise, a default
   // memory ordering is set for the access, and the implied decorator rules
@@ -1232,29 +1278,35 @@
                                               INTERNAL_CONVERT_COMPRESSED_OOP : DECORATORS_NONE)>::value;
     return PreRuntimeDispatch::atomic_xchg_at<expanded_decorators>(base, offset, new_decayed_value);
   }
 
   template <DecoratorSet decorators, typename T>
-  inline bool arraycopy(arrayOop src_obj, size_t src_offset_in_bytes, const T* src_raw,
+  inline void arraycopy(arrayOop src_obj, size_t src_offset_in_bytes, const T* src_raw,
                         arrayOop dst_obj, size_t dst_offset_in_bytes, T* dst_raw,
                         size_t length) {
     STATIC_ASSERT((HasDecorator<decorators, INTERNAL_VALUE_IS_OOP>::value ||
                    (IsSame<T, void>::value || IsIntegral<T>::value) ||
                     IsFloatingPoint<T>::value)); // arraycopy allows type erased void elements
     typedef typename Decay<T>::type DecayedT;
     const DecoratorSet expanded_decorators = DecoratorFixup<decorators | IS_ARRAY | IN_HEAP>::value;
-    return arraycopy_reduce_types<expanded_decorators>(src_obj, src_offset_in_bytes, const_cast<DecayedT*>(src_raw),
-                                                       dst_obj, dst_offset_in_bytes, const_cast<DecayedT*>(dst_raw),
-                                                       length);
+    arraycopy_reduce_types<expanded_decorators>(src_obj, src_offset_in_bytes, const_cast<DecayedT*>(src_raw),
+                                                dst_obj, dst_offset_in_bytes, const_cast<DecayedT*>(dst_raw),
+                                                length);
   }
 
   template <DecoratorSet decorators>
   inline void clone(oop src, oop dst, size_t size) {
     const DecoratorSet expanded_decorators = DecoratorFixup<decorators>::value;
     PreRuntimeDispatch::clone<expanded_decorators>(src, dst, size);
   }
 
+  template <DecoratorSet decorators>
+  inline void value_copy(void* src, void* dst, ValueKlass* md) {
+    const DecoratorSet expanded_decorators = DecoratorFixup<decorators>::value;
+    PreRuntimeDispatch::value_copy<expanded_decorators>(src, dst, md);
+  }
+
   template <DecoratorSet decorators>
   inline oop resolve(oop obj) {
     const DecoratorSet expanded_decorators = DecoratorFixup<decorators>::value;
     return PreRuntimeDispatch::resolve<expanded_decorators>(obj);
   }
diff a/src/hotspot/share/oops/accessDecorators.hpp b/src/hotspot/share/oops/accessDecorators.hpp
--- a/src/hotspot/share/oops/accessDecorators.hpp
+++ b/src/hotspot/share/oops/accessDecorators.hpp
@@ -196,33 +196,36 @@
 // == Arraycopy Decorators ==
 // * ARRAYCOPY_CHECKCAST: This property means that the class of the objects in source
 //   are not guaranteed to be subclasses of the class of the destination array. This requires
 //   a check-cast barrier during the copying operation. If this is not set, it is assumed
 //   that the array is covariant: (the source array type is-a destination array type)
+// * ARRAYCOPY_NOTNULL: This property means that the source array may contain null elements
+//   but the destination does not allow null elements (i.e. throw NPE)
 // * ARRAYCOPY_DISJOINT: This property means that it is known that the two array ranges
 //   are disjoint.
 // * ARRAYCOPY_ARRAYOF: The copy is in the arrayof form.
 // * ARRAYCOPY_ATOMIC: The accesses have to be atomic over the size of its elements.
 // * ARRAYCOPY_ALIGNED: The accesses have to be aligned on a HeapWord.
 const DecoratorSet ARRAYCOPY_CHECKCAST            = UCONST64(1) << 23;
-const DecoratorSet ARRAYCOPY_DISJOINT             = UCONST64(1) << 24;
-const DecoratorSet ARRAYCOPY_ARRAYOF              = UCONST64(1) << 25;
-const DecoratorSet ARRAYCOPY_ATOMIC               = UCONST64(1) << 26;
-const DecoratorSet ARRAYCOPY_ALIGNED              = UCONST64(1) << 27;
-const DecoratorSet ARRAYCOPY_DECORATOR_MASK       = ARRAYCOPY_CHECKCAST | ARRAYCOPY_DISJOINT |
+const DecoratorSet ARRAYCOPY_NOTNULL              = UCONST64(1) << 24;
+const DecoratorSet ARRAYCOPY_DISJOINT             = UCONST64(1) << 25;
+const DecoratorSet ARRAYCOPY_ARRAYOF              = UCONST64(1) << 26;
+const DecoratorSet ARRAYCOPY_ATOMIC               = UCONST64(1) << 27;
+const DecoratorSet ARRAYCOPY_ALIGNED              = UCONST64(1) << 28;
+const DecoratorSet ARRAYCOPY_DECORATOR_MASK       = ARRAYCOPY_CHECKCAST | ARRAYCOPY_NOTNULL |
                                                     ARRAYCOPY_DISJOINT | ARRAYCOPY_ARRAYOF |
                                                     ARRAYCOPY_ATOMIC | ARRAYCOPY_ALIGNED;
 
 // == Resolve barrier decorators ==
 // * ACCESS_READ: Indicate that the resolved object is accessed read-only. This allows the GC
 //   backend to use weaker and more efficient barriers.
 // * ACCESS_WRITE: Indicate that the resolved object is used for write access.
-const DecoratorSet ACCESS_READ                    = UCONST64(1) << 28;
-const DecoratorSet ACCESS_WRITE                   = UCONST64(1) << 29;
+const DecoratorSet ACCESS_READ                    = UCONST64(1) << 29;
+const DecoratorSet ACCESS_WRITE                   = UCONST64(1) << 30;
 
 // Keep track of the last decorator.
-const DecoratorSet DECORATOR_LAST = UCONST64(1) << 29;
+const DecoratorSet DECORATOR_LAST = UCONST64(1) << 30;
 
 namespace AccessInternal {
   // This class adds implied decorators that follow according to decorator rules.
   // For example adding default reference strength and default memory ordering
   // semantics.
diff a/src/hotspot/share/oops/method.cpp b/src/hotspot/share/oops/method.cpp
--- a/src/hotspot/share/oops/method.cpp
+++ b/src/hotspot/share/oops/method.cpp
@@ -52,10 +52,11 @@
 #include "oops/methodData.hpp"
 #include "oops/objArrayKlass.hpp"
 #include "oops/objArrayOop.inline.hpp"
 #include "oops/oop.inline.hpp"
 #include "oops/symbol.hpp"
+#include "oops/valueKlass.inline.hpp"
 #include "prims/jvmtiExport.hpp"
 #include "prims/methodHandles.hpp"
 #include "prims/nativeLookup.hpp"
 #include "runtime/arguments.hpp"
 #include "runtime/atomic.hpp"
@@ -111,11 +112,10 @@
 
   if (access_flags.is_native()) {
     clear_native_function();
     set_signature_handler(NULL);
   }
-
   NOT_PRODUCT(set_compiled_invocation_count(0);)
 }
 
 // Release Method*.  The nmethod will be gone when we get here because
 // we've walked the code cache.
@@ -148,15 +148,25 @@
 address Method::get_c2i_entry() {
   assert(adapter() != NULL, "must have");
   return adapter()->get_c2i_entry();
 }
 
+address Method::get_c2i_value_entry() {
+  assert(adapter() != NULL, "must have");
+  return adapter()->get_c2i_value_entry();
+}
+
 address Method::get_c2i_unverified_entry() {
   assert(adapter() != NULL, "must have");
   return adapter()->get_c2i_unverified_entry();
 }
 
+address Method::get_c2i_unverified_value_entry() {
+  assert(adapter() != NULL, "must have");
+  return adapter()->get_c2i_unverified_value_entry();
+}
+
 address Method::get_c2i_no_clinit_check_entry() {
   assert(VM_Version::supports_fast_class_init_checks(), "");
   assert(adapter() != NULL, "must have");
   return adapter()->get_c2i_no_clinit_check_entry();
 }
@@ -345,10 +355,12 @@
   it->push(&_method_counters);
 
   Method* this_ptr = this;
   it->push_method_entry(&this_ptr, (intptr_t*)&_i2i_entry);
   it->push_method_entry(&this_ptr, (intptr_t*)&_from_compiled_entry);
+  it->push_method_entry(&this_ptr, (intptr_t*)&_from_compiled_value_ro_entry);
+  it->push_method_entry(&this_ptr, (intptr_t*)&_from_compiled_value_entry);
   it->push_method_entry(&this_ptr, (intptr_t*)&_from_interpreted_entry);
 }
 
 // Attempt to return method oop to original state.  Clear any pointers
 // (to objects outside the shared spaces).  We won't be able to predict
@@ -588,21 +600,39 @@
   set_size_of_parameters(fp.size_of_parameters());
   constMethod()->set_result_type(fp.return_type());
   constMethod()->set_fingerprint(fp.fingerprint());
 }
 
+// ValueKlass the method is declared to return. This must not
+// safepoint as it is called with references live on the stack at
+// locations the GC is unaware of.
+ValueKlass* Method::returned_value_type(Thread* thread) const {
+  SignatureStream ss(signature());
+  while (!ss.at_return_type()) {
+    ss.next();
+  }
+  Handle class_loader(thread, method_holder()->class_loader());
+  Handle protection_domain(thread, method_holder()->protection_domain());
+  Klass* k = NULL;
+  {
+    NoSafepointVerifier nsv;
+    k = ss.as_klass(class_loader, protection_domain, SignatureStream::ReturnNull, thread);
+  }
+  assert(k != NULL && !thread->has_pending_exception(), "can't resolve klass");
+  return ValueKlass::cast(k);
+}
 bool Method::is_empty_method() const {
   return  code_size() == 1
       && *code_base() == Bytecodes::_return;
 }
 
 bool Method::is_vanilla_constructor() const {
   // Returns true if this method is a vanilla constructor, i.e. an "<init>" "()V" method
   // which only calls the superclass vanilla constructor and possibly does stores of
   // zero constants to local fields:
   //
-  //   aload_0
+  //   aload_0, _fast_aload_0, or _nofast_aload_0
   //   invokespecial
   //   indexbyte1
   //   indexbyte2
   //
   // followed by an (optional) sequence of:
@@ -622,11 +652,12 @@
   int size = code_size();
   // Check if size match
   if (size == 0 || size % 5 != 0) return false;
   address cb = code_base();
   int last = size - 1;
-  if (cb[0] != Bytecodes::_aload_0 || cb[1] != Bytecodes::_invokespecial || cb[last] != Bytecodes::_return) {
+  if ((cb[0] != Bytecodes::_aload_0 && cb[0] != Bytecodes::_fast_aload_0 && cb[0] != Bytecodes::_nofast_aload_0) ||
+       cb[1] != Bytecodes::_invokespecial || cb[last] != Bytecodes::_return) {
     // Does not call superclass default constructor
     return false;
   }
   // Check optional sequence
   for (int i = 4; i < last; i += 5) {
@@ -774,29 +805,31 @@
           Bytecodes::is_const(java_code_at(0)) &&
           Bytecodes::length_for(java_code_at(0)) == last_index &&
           Bytecodes::is_return(java_code_at(last_index)));
 }
 
-bool Method::is_initializer() const {
-  return is_object_initializer() || is_static_initializer();
-}
-
-bool Method::has_valid_initializer_flags() const {
-  return (is_static() ||
-          method_holder()->major_version() < 51);
+bool Method::is_object_constructor_or_class_initializer() const {
+  return (is_object_constructor() || is_class_initializer());
 }
 
-bool Method::is_static_initializer() const {
+bool Method::is_class_initializer() const {
   // For classfiles version 51 or greater, ensure that the clinit method is
   // static.  Non-static methods with the name "<clinit>" are not static
   // initializers. (older classfiles exempted for backward compatibility)
-  return name() == vmSymbols::class_initializer_name() &&
-         has_valid_initializer_flags();
+  return (name() == vmSymbols::class_initializer_name() &&
+          (is_static() ||
+           method_holder()->major_version() < 51));
+}
+
+// A method named <init>, if non-static, is a classic object constructor.
+bool Method::is_object_constructor() const {
+   return name() == vmSymbols::object_initializer_name() && !is_static();
 }
 
-bool Method::is_object_initializer() const {
-   return name() == vmSymbols::object_initializer_name();
+// A static method named <init> is a factory for an inline class.
+bool Method::is_static_init_factory() const {
+   return name() == vmSymbols::object_initializer_name() && is_static();
 }
 
 bool Method::needs_clinit_barrier() const {
   return is_static() && !method_holder()->is_initialized();
 }
@@ -850,11 +883,11 @@
   return best_line;
 }
 
 
 bool Method::is_klass_loaded_by_klass_index(int klass_index) const {
-  if( constants()->tag_at(klass_index).is_unresolved_klass() ) {
+  if( constants()->tag_at(klass_index).is_unresolved_klass()) {
     Thread *thread = Thread::current();
     Symbol* klass_name = constants()->klass_name_at(klass_index);
     Handle loader(thread, method_holder()->class_loader());
     Handle prot  (thread, method_holder()->protection_domain());
     return SystemDictionary::find(klass_name, loader, prot, thread) != NULL;
@@ -866,11 +899,13 @@
 
 bool Method::is_klass_loaded(int refinfo_index, bool must_be_resolved) const {
   int klass_index = constants()->klass_ref_index_at(refinfo_index);
   if (must_be_resolved) {
     // Make sure klass is resolved in constantpool.
-    if (constants()->tag_at(klass_index).is_unresolved_klass()) return false;
+    if (constants()->tag_at(klass_index).is_unresolved_klass()) {
+      return false;
+    }
   }
   return is_klass_loaded_by_klass_index(klass_index);
 }
 
 
@@ -1035,12 +1070,16 @@
 void Method::clear_code() {
   // this may be NULL if c2i adapters have not been made yet
   // Only should happen at allocate time.
   if (adapter() == NULL) {
     _from_compiled_entry    = NULL;
+    _from_compiled_value_entry = NULL;
+    _from_compiled_value_ro_entry = NULL;
   } else {
     _from_compiled_entry    = adapter()->get_c2i_entry();
+    _from_compiled_value_entry = adapter()->get_c2i_value_entry();
+    _from_compiled_value_ro_entry = adapter()->get_c2i_value_ro_entry();
   }
   OrderAccess::storestore();
   _from_interpreted_entry = _i2i_entry;
   OrderAccess::storestore();
   _code = NULL;
@@ -1081,13 +1120,22 @@
   } else {
     // TODO: Simplify the adapter trampoline allocation for static archiving.
     //       Remove the use of CDSAdapterHandlerEntry.
     CDSAdapterHandlerEntry* cds_adapter = (CDSAdapterHandlerEntry*)adapter();
     constMethod()->set_adapter_trampoline(cds_adapter->get_adapter_trampoline());
+
     _from_compiled_entry = cds_adapter->get_c2i_entry_trampoline();
     assert(*((int*)_from_compiled_entry) == 0,
-           "must be NULL during dump time, to be initialized at run time");
+           "instructions must be zeros during dump time, to be initialized at run time");
+
+    _from_compiled_value_ro_entry = cds_adapter->get_c2i_value_ro_entry_trampoline();
+    assert(*((int*)_from_compiled_value_ro_entry) == 0,
+           "instructions must be zeros during dump time, to be initialized at run time");
+
+    _from_compiled_value_entry = cds_adapter->get_c2i_value_entry_trampoline();
+    assert(*((int*)_from_compiled_value_entry) == 0,
+           "instructions must be zeros during dump time, to be initialized at run time");
   }
 
   if (is_native()) {
     *native_function_addr() = NULL;
     set_signature_handler(NULL);
@@ -1236,34 +1284,61 @@
   }
 
   if (mh->is_shared()) {
     assert(mh->adapter() == adapter, "must be");
     assert(mh->_from_compiled_entry != NULL, "must be");
+    assert(mh->_from_compiled_value_entry != NULL, "must be");
+    assert(mh->_from_compiled_value_ro_entry != NULL, "must be");
   } else {
     mh->set_adapter_entry(adapter);
     mh->_from_compiled_entry = adapter->get_c2i_entry();
+    mh->_from_compiled_value_entry = adapter->get_c2i_value_entry();
+    mh->_from_compiled_value_ro_entry = adapter->get_c2i_value_ro_entry();
   }
   return adapter->get_c2i_entry();
 }
 
 void Method::restore_unshareable_info(TRAPS) {
   assert(is_method() && is_valid_method(this), "ensure C++ vtable is restored");
 
+#if 0
+  /*
+   * CDS:TODO --
+   * "Q" classes in the method signature must be resolved during link_method.
+   * However, at this point we are still inside method_holder()->restore_unshareable_info.
+   * If we try to resolve method_holder(), or multually dependent classes, it will
+   * cause deadlock and other ill effects.
+   *
+   * For now, lets do method linking inside InstanceKlass::link_class(). Optimization
+   * may be possible if we know that resolution will never happen.
+   */
+
   // Since restore_unshareable_info can be called more than once for a method, don't
   // redo any work.
   if (adapter() == NULL) {
     methodHandle mh(THREAD, this);
     link_method(mh, CHECK);
   }
+#endif
 }
 
-address Method::from_compiled_entry_no_trampoline() const {
+address Method::from_compiled_entry_no_trampoline(bool caller_is_c1) const {
   CompiledMethod *code = Atomic::load_acquire(&_code);
-  if (code) {
-    return code->verified_entry_point();
+  if (caller_is_c1) {
+    // C1 - value arguments are passed as objects
+    if (code) {
+      return code->verified_value_entry_point();
+    } else {
+      return adapter()->get_c2i_value_entry();
+    }
   } else {
-    return adapter()->get_c2i_entry();
+    // C2 - value arguments may be passed as fields
+    if (code) {
+      return code->verified_entry_point();
+    } else {
+      return adapter()->get_c2i_entry();
+    }
   }
 }
 
 // The verified_code_entry() must be called when a invoke is resolved
 // on this method.
@@ -1276,10 +1351,22 @@
   debug_only(NoSafepointVerifier nsv;)
   assert(_from_compiled_entry != NULL, "must be set");
   return _from_compiled_entry;
 }
 
+address Method::verified_value_code_entry() {
+  debug_only(NoSafepointVerifier nsv;)
+  assert(_from_compiled_value_entry != NULL, "must be set");
+  return _from_compiled_value_entry;
+}
+
+address Method::verified_value_ro_code_entry() {
+  debug_only(NoSafepointVerifier nsv;)
+  assert(_from_compiled_value_ro_entry != NULL, "must be set");
+  return _from_compiled_value_ro_entry;
+}
+
 // Check that if an nmethod ref exists, it has a backlink to this or no backlink at all
 // (could be racing a deopt).
 // Not inline to avoid circular ref.
 bool Method::check_code() const {
   // cached in a register or local.  There's a race on the value of the field.
@@ -1307,10 +1394,12 @@
     mh->set_highest_comp_level(comp_level);
   }
 
   OrderAccess::storestore();
   mh->_from_compiled_entry = code->verified_entry_point();
+  mh->_from_compiled_value_entry = code->verified_value_entry_point();
+  mh->_from_compiled_value_ro_entry = code->verified_value_ro_entry_point();
   OrderAccess::storestore();
   // Instantly compiled code can execute.
   if (!mh->is_method_handle_intrinsic())
     mh->_from_interpreted_entry = mh->get_i2c_entry();
 }
@@ -2338,10 +2427,12 @@
   if (intrinsic_id() != vmIntrinsics::_none)
     st->print_cr(" - intrinsic id:      %d %s", intrinsic_id(), vmIntrinsics::name_at(intrinsic_id()));
   if (highest_comp_level() != CompLevel_none)
     st->print_cr(" - highest level:     %d", highest_comp_level());
   st->print_cr(" - vtable index:      %d",   _vtable_index);
+  if (valid_itable_index())
+    st->print_cr(" - itable index:      %d",   itable_index());
   st->print_cr(" - i2i entry:         " INTPTR_FORMAT, p2i(interpreter_entry()));
   st->print(   " - adapters:          ");
   AdapterHandlerEntry* a = ((Method*)this)->adapter();
   if (a == NULL)
     st->print_cr(INTPTR_FORMAT, p2i(a));
@@ -2415,10 +2506,11 @@
 void Method::print_value_on(outputStream* st) const {
   assert(is_method(), "must be method");
   st->print("%s", internal_name());
   print_address_on(st);
   st->print(" ");
+  if (WizardMode) access_flags().print_on(st);
   name()->print_value_on(st);
   st->print(" ");
   signature()->print_value_on(st);
   st->print(" in ");
   method_holder()->print_value_on(st);
diff a/src/hotspot/share/oops/oop.hpp b/src/hotspot/share/oops/oop.hpp
--- a/src/hotspot/share/oops/oop.hpp
+++ b/src/hotspot/share/oops/oop.hpp
@@ -37,10 +37,20 @@
 // the format of Java objects so the fields can be accessed from C++.
 // oopDesc is abstract.
 // (see oopHierarchy for complete oop class hierarchy)
 //
 // no virtual functions allowed
+//
+// oopDesc::_mark - the "oop mark word" encoding to be found separately in markWord.hpp
+//
+// oopDesc::_metadata - encodes the object's klass pointer, as a raw pointer in "_klass"
+//                      or compressed pointer in "_compressed_klass"
+//
+// The overall size of the _metadata field is dependent on "UseCompressedClassPointers",
+// hence the terms "narrow" (32 bits) vs "wide" (64 bits).
+//
+
 
 // Forward declarations.
 class OopClosure;
 class ScanClosure;
 class FastScanClosure;
@@ -109,16 +119,20 @@
   // type test operations (inlined in oop.inline.hpp)
   inline bool is_instance()            const;
   inline bool is_array()               const;
   inline bool is_objArray()            const;
   inline bool is_typeArray()           const;
+  inline bool is_value()               const;
+  inline bool is_valueArray()          const;
 
   // type test operations that don't require inclusion of oop.inline.hpp.
   bool is_instance_noinline()          const;
   bool is_array_noinline()             const;
   bool is_objArray_noinline()          const;
   bool is_typeArray_noinline()         const;
+  bool is_value_noinline()             const;
+  bool is_valueArray_noinline()        const;
 
  protected:
   inline oop        as_oop() const { return const_cast<oopDesc*>(this); }
 
  public:
diff a/src/hotspot/share/oops/oop.inline.hpp b/src/hotspot/share/oops/oop.inline.hpp
--- a/src/hotspot/share/oops/oop.inline.hpp
+++ b/src/hotspot/share/oops/oop.inline.hpp
@@ -158,10 +158,11 @@
     Atomic::release_store(compressed_klass_addr(mem),
                           CompressedKlassPointers::encode_not_null(klass));
   } else {
     Atomic::release_store(klass_addr(mem), klass);
   }
+  assert(((oopDesc*)mem)->klass() == klass, "failed oopDesc::klass() encode/decode");
 }
 
 #undef CHECK_SET_KLASS
 
 int oopDesc::klass_gap() const {
@@ -244,10 +245,12 @@
 
 bool oopDesc::is_instance()  const { return klass()->is_instance_klass();  }
 bool oopDesc::is_array()     const { return klass()->is_array_klass();     }
 bool oopDesc::is_objArray()  const { return klass()->is_objArray_klass();  }
 bool oopDesc::is_typeArray() const { return klass()->is_typeArray_klass(); }
+bool oopDesc::is_value()     const { return klass()->is_value(); }
+bool oopDesc::is_valueArray()  const { return klass()->is_valueArray_klass(); }
 
 void*    oopDesc::field_addr_raw(int offset)     const { return reinterpret_cast<void*>(cast_from_oop<intptr_t>(as_oop()) + offset); }
 void*    oopDesc::field_addr(int offset)         const { return Access<>::resolve(as_oop())->field_addr_raw(offset); }
 
 template <class T>
@@ -298,10 +301,11 @@
 
 bool oopDesc::has_bias_pattern() const {
   return mark().has_bias_pattern();
 }
 
+
 bool oopDesc::has_bias_pattern_raw() const {
   return mark_raw().has_bias_pattern();
 }
 
 // Used only for markSweep, scavenging
diff a/src/hotspot/share/opto/bytecodeInfo.cpp b/src/hotspot/share/opto/bytecodeInfo.cpp
--- a/src/hotspot/share/opto/bytecodeInfo.cpp
+++ b/src/hotspot/share/opto/bytecodeInfo.cpp
@@ -84,14 +84,14 @@
 static bool is_init_with_ea(ciMethod* callee_method,
                             ciMethod* caller_method, Compile* C) {
   if (!C->do_escape_analysis() || !EliminateAllocations) {
     return false; // EA is off
   }
-  if (callee_method->is_initializer()) {
-    return true; // constuctor
+  if (callee_method->is_object_constructor()) {
+    return true; // constructor
   }
-  if (caller_method->is_initializer() &&
+  if (caller_method->is_object_constructor_or_class_initializer() &&
       caller_method != C->method() &&
       caller_method->holder()->is_subclass_of(callee_method->holder())) {
     return true; // super constructor is called from inlined constructor
   }
   if (C->eliminate_boxing() && callee_method->is_boxing_method()) {
diff a/src/hotspot/share/opto/c2_globals.hpp b/src/hotspot/share/opto/c2_globals.hpp
--- a/src/hotspot/share/opto/c2_globals.hpp
+++ b/src/hotspot/share/opto/c2_globals.hpp
@@ -779,9 +779,12 @@
           range(0, max_juint)                                               \
                                                                             \
   product(bool, UseProfiledLoopPredicate, true,                             \
           "Move predicates out of loops based on profiling data")           \
                                                                             \
+  product(bool, UseArrayLoadStoreProfile, false,                            \
+          "Take advantage of profiling at array load/store")                \
+                                                                            \
   diagnostic(bool, ExpandSubTypeCheckAtParseTime, false,                    \
           "Do not use subtype check macro node")                            \
 
 #endif // SHARE_OPTO_C2_GLOBALS_HPP
diff a/src/hotspot/share/opto/compile.cpp b/src/hotspot/share/opto/compile.cpp
--- a/src/hotspot/share/opto/compile.cpp
+++ b/src/hotspot/share/opto/compile.cpp
@@ -66,10 +66,11 @@
 #include "opto/phaseX.hpp"
 #include "opto/rootnode.hpp"
 #include "opto/runtime.hpp"
 #include "opto/stringopts.hpp"
 #include "opto/type.hpp"
+#include "opto/valuetypenode.hpp"
 #include "opto/vectornode.hpp"
 #include "runtime/arguments.hpp"
 #include "runtime/sharedRuntime.hpp"
 #include "runtime/signature.hpp"
 #include "runtime/stubRoutines.hpp"
@@ -403,10 +404,14 @@
     Node* opaq = opaque4_node(i);
     if (!useful.member(opaq)) {
       remove_opaque4_node(opaq);
     }
   }
+  // Remove useless value type nodes
+  if (_value_type_nodes != NULL) {
+    _value_type_nodes->remove_useless_nodes(useful.member_set());
+  }
   BarrierSetC2* bs = BarrierSet::barrier_set()->barrier_set_c2();
   bs->eliminate_useless_gc_barriers(useful, this);
   // clean up the late inline lists
   remove_useless_late_inlines(&_string_late_inlines, useful);
   remove_useless_late_inlines(&_boxing_late_inlines, useful);
@@ -635,21 +640,19 @@
     initial_gvn()->transform_no_reclaim(top());
 
     // Set up tf(), start(), and find a CallGenerator.
     CallGenerator* cg = NULL;
     if (is_osr_compilation()) {
-      const TypeTuple *domain = StartOSRNode::osr_domain();
-      const TypeTuple *range = TypeTuple::make_range(method()->signature());
-      init_tf(TypeFunc::make(domain, range));
-      StartNode* s = new StartOSRNode(root(), domain);
+      init_tf(TypeFunc::make(method(), /* is_osr_compilation = */ true));
+      StartNode* s = new StartOSRNode(root(), tf()->domain_sig());
       initial_gvn()->set_type_bottom(s);
       init_start(s);
       cg = CallGenerator::for_osr(method(), entry_bci());
     } else {
       // Normal case.
       init_tf(TypeFunc::make(method()));
-      StartNode* s = new StartNode(root(), tf()->domain());
+      StartNode* s = new StartNode(root(), tf()->domain_cc());
       initial_gvn()->set_type_bottom(s);
       init_start(s);
       if (method()->intrinsic_id() == vmIntrinsics::_Reference_get) {
         // With java.lang.ref.reference.get() we must go through the
         // intrinsic - even when get() is the root
@@ -770,10 +773,14 @@
   }
 
   // Now that we know the size of all the monitors we can add a fixed slot
   // for the original deopt pc.
   int next_slot = fixed_slots() + (sizeof(address) / VMRegImpl::stack_slot_size);
+  if (needs_stack_repair()) {
+    // One extra slot for the special stack increment value
+    next_slot += 2;
+  }
   set_fixed_slots(next_slot);
 
   // Compute when to use implicit null checks. Used by matching trap based
   // nodes and NullCheck optimization.
   set_allowed_deopt_reasons();
@@ -925,10 +932,13 @@
   Copy::zero_to_bytes(_trap_hist, sizeof(_trap_hist));
   set_decompile_count(0);
 
   set_do_freq_based_layout(_directive->BlockLayoutByFrequencyOption);
   _loop_opts_cnt = LoopOptsCount;
+  _has_flattened_accesses = false;
+  _flattened_accesses_share_alias = true;
+
   set_do_inlining(Inline);
   set_max_inline_size(MaxInlineSize);
   set_freq_inline_size(FreqInlineSize);
   set_do_scheduling(OptoScheduling);
   set_do_count_invocations(false);
@@ -1008,10 +1018,11 @@
   _macro_nodes = new(comp_arena()) GrowableArray<Node*>(comp_arena(), 8,  0, NULL);
   _predicate_opaqs = new(comp_arena()) GrowableArray<Node*>(comp_arena(), 8,  0, NULL);
   _expensive_nodes = new(comp_arena()) GrowableArray<Node*>(comp_arena(), 8,  0, NULL);
   _range_check_casts = new(comp_arena()) GrowableArray<Node*>(comp_arena(), 8,  0, NULL);
   _opaque4_nodes = new(comp_arena()) GrowableArray<Node*>(comp_arena(), 8,  0, NULL);
+  _value_type_nodes = new (comp_arena()) Unique_Node_List(comp_arena());
   register_library_intrinsics();
 #ifdef ASSERT
   _type_verify_symmetry = true;
 #endif
 }
@@ -1235,11 +1246,12 @@
   bool is_known_inst = tj->isa_oopptr() != NULL &&
                        tj->is_oopptr()->is_known_instance();
 
   // Process weird unsafe references.
   if (offset == Type::OffsetBot && (tj->isa_instptr() /*|| tj->isa_klassptr()*/)) {
-    assert(InlineUnsafeOps, "indeterminate pointers come only from unsafe ops");
+    bool default_value_load = EnableValhalla && tj->is_instptr()->klass() == ciEnv::current()->Class_klass();
+    assert(InlineUnsafeOps || default_value_load, "indeterminate pointers come only from unsafe ops");
     assert(!is_known_inst, "scalarizable allocation should not have unsafe references");
     tj = TypeOopPtr::BOTTOM;
     ptr = tj->ptr();
     offset = tj->offset();
   }
@@ -1248,24 +1260,35 @@
   const TypeAryPtr *ta = tj->isa_aryptr();
   if (ta && ta->is_stable()) {
     // Erase stability property for alias analysis.
     tj = ta = ta->cast_to_stable(false);
   }
+  if (ta && ta->is_not_flat()) {
+    // Erase not flat property for alias analysis.
+    tj = ta = ta->cast_to_not_flat(false);
+  }
+  if (ta && ta->is_not_null_free()) {
+    // Erase not null free property for alias analysis.
+    tj = ta = ta->cast_to_not_null_free(false);
+  }
+
   if( ta && is_known_inst ) {
     if ( offset != Type::OffsetBot &&
          offset > arrayOopDesc::length_offset_in_bytes() ) {
       offset = Type::OffsetBot; // Flatten constant access into array body only
-      tj = ta = TypeAryPtr::make(ptr, ta->ary(), ta->klass(), true, offset, ta->instance_id());
+      tj = ta = TypeAryPtr::make(ptr, ta->ary(), ta->klass(), true, Type::Offset(offset), ta->field_offset(), ta->instance_id());
     }
   } else if( ta && _AliasLevel >= 2 ) {
     // For arrays indexed by constant indices, we flatten the alias
     // space to include all of the array body.  Only the header, klass
     // and array length can be accessed un-aliased.
+    // For flattened value type array, each field has its own slice so
+    // we must include the field offset.
     if( offset != Type::OffsetBot ) {
       if( ta->const_oop() ) { // MethodData* or Method*
         offset = Type::OffsetBot;   // Flatten constant access into array body
-        tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),ta->ary(),ta->klass(),false,offset);
+        tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),ta->ary(),ta->klass(),false,Type::Offset(offset), ta->field_offset());
       } else if( offset == arrayOopDesc::length_offset_in_bytes() ) {
         // range is OK as-is.
         tj = ta = TypeAryPtr::RANGE;
       } else if( offset == oopDesc::klass_offset_in_bytes() ) {
         tj = TypeInstPtr::KLASS; // all klass loads look alike
@@ -1275,39 +1298,44 @@
         tj = TypeInstPtr::MARK;
         ta = TypeAryPtr::RANGE; // generic ignored junk
         ptr = TypePtr::BotPTR;
       } else {                  // Random constant offset into array body
         offset = Type::OffsetBot;   // Flatten constant access into array body
-        tj = ta = TypeAryPtr::make(ptr,ta->ary(),ta->klass(),false,offset);
+        tj = ta = TypeAryPtr::make(ptr,ta->ary(),ta->klass(),false,Type::Offset(offset), ta->field_offset());
       }
     }
     // Arrays of fixed size alias with arrays of unknown size.
     if (ta->size() != TypeInt::POS) {
       const TypeAry *tary = TypeAry::make(ta->elem(), TypeInt::POS);
-      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,ta->klass(),false,offset);
+      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,ta->klass(),false,Type::Offset(offset), ta->field_offset());
     }
     // Arrays of known objects become arrays of unknown objects.
     if (ta->elem()->isa_narrowoop() && ta->elem() != TypeNarrowOop::BOTTOM) {
       const TypeAry *tary = TypeAry::make(TypeNarrowOop::BOTTOM, ta->size());
-      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,NULL,false,offset);
+      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,NULL,false,Type::Offset(offset), ta->field_offset());
     }
     if (ta->elem()->isa_oopptr() && ta->elem() != TypeInstPtr::BOTTOM) {
       const TypeAry *tary = TypeAry::make(TypeInstPtr::BOTTOM, ta->size());
-      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,NULL,false,offset);
+      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,NULL,false,Type::Offset(offset), ta->field_offset());
+    }
+    // Initially all flattened array accesses share a single slice
+    if (ta->elem()->isa_valuetype() && ta->elem() != TypeValueType::BOTTOM && _flattened_accesses_share_alias) {
+      const TypeAry *tary = TypeAry::make(TypeValueType::BOTTOM, ta->size());
+      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,NULL,false,Type::Offset(offset), Type::Offset(Type::OffsetBot));
     }
     // Arrays of bytes and of booleans both use 'bastore' and 'baload' so
     // cannot be distinguished by bytecode alone.
     if (ta->elem() == TypeInt::BOOL) {
       const TypeAry *tary = TypeAry::make(TypeInt::BYTE, ta->size());
       ciKlass* aklass = ciTypeArrayKlass::make(T_BYTE);
-      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,aklass,false,offset);
+      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,aklass,false,Type::Offset(offset), ta->field_offset());
     }
     // During the 2nd round of IterGVN, NotNull castings are removed.
     // Make sure the Bottom and NotNull variants alias the same.
     // Also, make sure exact and non-exact variants alias the same.
     if (ptr == TypePtr::NotNull || ta->klass_is_exact() || ta->speculative() != NULL) {
-      tj = ta = TypeAryPtr::make(TypePtr::BotPTR,ta->ary(),ta->klass(),false,offset);
+      tj = ta = TypeAryPtr::make(TypePtr::BotPTR,ta->ary(),ta->klass(),false,Type::Offset(offset), ta->field_offset());
     }
   }
 
   // Oop pointers need some flattening
   const TypeInstPtr *to = tj->isa_instptr();
@@ -1317,29 +1345,29 @@
       if (to->klass() != ciEnv::current()->Class_klass() ||
           offset < k->size_helper() * wordSize) {
         // No constant oop pointers (such as Strings); they alias with
         // unknown strings.
         assert(!is_known_inst, "not scalarizable allocation");
-        tj = to = TypeInstPtr::make(TypePtr::BotPTR,to->klass(),false,0,offset);
+        tj = to = TypeInstPtr::make(TypePtr::BotPTR,to->klass(),false,0,Type::Offset(offset), to->klass()->flatten_array());
       }
     } else if( is_known_inst ) {
       tj = to; // Keep NotNull and klass_is_exact for instance type
     } else if( ptr == TypePtr::NotNull || to->klass_is_exact() ) {
       // During the 2nd round of IterGVN, NotNull castings are removed.
       // Make sure the Bottom and NotNull variants alias the same.
       // Also, make sure exact and non-exact variants alias the same.
-      tj = to = TypeInstPtr::make(TypePtr::BotPTR,to->klass(),false,0,offset);
+      tj = to = TypeInstPtr::make(TypePtr::BotPTR,to->klass(),false,0,Type::Offset(offset), to->klass()->flatten_array());
     }
     if (to->speculative() != NULL) {
-      tj = to = TypeInstPtr::make(to->ptr(),to->klass(),to->klass_is_exact(),to->const_oop(),to->offset(), to->instance_id());
+      tj = to = TypeInstPtr::make(to->ptr(),to->klass(),to->klass_is_exact(),to->const_oop(),Type::Offset(to->offset()), to->klass()->flatten_array(), to->instance_id());
     }
     // Canonicalize the holder of this field
     if (offset >= 0 && offset < instanceOopDesc::base_offset_in_bytes()) {
       // First handle header references such as a LoadKlassNode, even if the
       // object's klass is unloaded at compile time (4965979).
       if (!is_known_inst) { // Do it only for non-instance types
-        tj = to = TypeInstPtr::make(TypePtr::BotPTR, env()->Object_klass(), false, NULL, offset);
+        tj = to = TypeInstPtr::make(TypePtr::BotPTR, env()->Object_klass(), false, NULL, Type::Offset(offset), false);
       }
     } else if (offset < 0 || offset >= k->size_helper() * wordSize) {
       // Static fields are in the space above the normal instance
       // fields in the java.lang.Class instance.
       if (to->klass() != ciEnv::current()->Class_klass()) {
@@ -1349,13 +1377,13 @@
       }
     } else {
       ciInstanceKlass *canonical_holder = k->get_canonical_holder(offset);
       if (!k->equals(canonical_holder) || tj->offset() != offset) {
         if( is_known_inst ) {
-          tj = to = TypeInstPtr::make(to->ptr(), canonical_holder, true, NULL, offset, to->instance_id());
+          tj = to = TypeInstPtr::make(to->ptr(), canonical_holder, true, NULL, Type::Offset(offset), canonical_holder->flatten_array(), to->instance_id());
         } else {
-          tj = to = TypeInstPtr::make(to->ptr(), canonical_holder, false, NULL, offset);
+          tj = to = TypeInstPtr::make(to->ptr(), canonical_holder, false, NULL, Type::Offset(offset), canonical_holder->flatten_array());
         }
       }
     }
   }
 
@@ -1368,19 +1396,20 @@
     // use NotNull as the PTR.
     if ( offset == Type::OffsetBot || (offset >= 0 && (size_t)offset < sizeof(Klass)) ) {
 
       tj = tk = TypeKlassPtr::make(TypePtr::NotNull,
                                    TypeKlassPtr::OBJECT->klass(),
-                                   offset);
+                                   Type::Offset(offset),
+                                   false);
     }
 
     ciKlass* klass = tk->klass();
-    if( klass->is_obj_array_klass() ) {
+    if (klass != NULL && klass->is_obj_array_klass()) {
       ciKlass* k = TypeAryPtr::OOPS->klass();
       if( !k || !k->is_loaded() )                  // Only fails for some -Xcomp runs
         k = TypeInstPtr::BOTTOM->klass();
-      tj = tk = TypeKlassPtr::make( TypePtr::NotNull, k, offset );
+      tj = tk = TypeKlassPtr::make(TypePtr::NotNull, k, Type::Offset(offset), false);
     }
 
     // Check for precise loads from the primary supertype array and force them
     // to the supertype cache alias index.  Check for generic array loads from
     // the primary supertype array and also force them to the supertype cache
@@ -1392,11 +1421,11 @@
     if (offset == Type::OffsetBot ||
         (offset >= primary_supers_offset &&
          offset < (int)(primary_supers_offset + Klass::primary_super_limit() * wordSize)) ||
         offset == (int)in_bytes(Klass::secondary_super_cache_offset())) {
       offset = in_bytes(Klass::secondary_super_cache_offset());
-      tj = tk = TypeKlassPtr::make( TypePtr::NotNull, tk->klass(), offset );
+      tj = tk = TypeKlassPtr::make(TypePtr::NotNull, tk->klass(), Type::Offset(offset), tk->flat_array());
     }
   }
 
   // Flatten all Raw pointers together.
   if (tj->base() == Type::RawPtr)
@@ -1531,17 +1560,20 @@
   for (int i = 0; i < new_ats; i++)  _alias_types[old_ats+i] = &ats[i];
 }
 
 
 //--------------------------------find_alias_type------------------------------
-Compile::AliasType* Compile::find_alias_type(const TypePtr* adr_type, bool no_create, ciField* original_field) {
+Compile::AliasType* Compile::find_alias_type(const TypePtr* adr_type, bool no_create, ciField* original_field, bool uncached) {
   if (_AliasLevel == 0)
     return alias_type(AliasIdxBot);
 
-  AliasCacheEntry* ace = probe_alias_cache(adr_type);
-  if (ace->_adr_type == adr_type) {
-    return alias_type(ace->_index);
+  AliasCacheEntry* ace = NULL;
+  if (!uncached) {
+    ace = probe_alias_cache(adr_type);
+    if (ace->_adr_type == adr_type) {
+      return alias_type(ace->_index);
+    }
   }
 
   // Handle special cases.
   if (adr_type == NULL)             return alias_type(AliasIdxTop);
   if (adr_type == TypePtr::BOTTOM)  return alias_type(AliasIdxBot);
@@ -1587,18 +1619,28 @@
     if (flat->isa_instptr()) {
       if (flat->offset() == java_lang_Class::klass_offset_in_bytes()
           && flat->is_instptr()->klass() == env()->Class_klass())
         alias_type(idx)->set_rewritable(false);
     }
+    ciField* field = NULL;
     if (flat->isa_aryptr()) {
 #ifdef ASSERT
       const int header_size_min  = arrayOopDesc::base_offset_in_bytes(T_BYTE);
       // (T_BYTE has the weakest alignment and size restrictions...)
       assert(flat->offset() < header_size_min, "array body reference must be OffsetBot");
 #endif
+      const Type* elemtype = flat->is_aryptr()->elem();
       if (flat->offset() == TypePtr::OffsetBot) {
-        alias_type(idx)->set_element(flat->is_aryptr()->elem());
+        alias_type(idx)->set_element(elemtype);
+      }
+      int field_offset = flat->is_aryptr()->field_offset().get();
+      if (elemtype->isa_valuetype() &&
+          elemtype->value_klass() != NULL &&
+          field_offset != Type::OffsetBot) {
+        ciValueKlass* vk = elemtype->value_klass();
+        field_offset += vk->first_field_offset();
+        field = vk->get_field_by_offset(field_offset, false);
       }
     }
     if (flat->isa_klassptr()) {
       if (flat->offset() == in_bytes(Klass::super_check_offset_offset()))
         alias_type(idx)->set_rewritable(false);
@@ -1606,52 +1648,66 @@
         alias_type(idx)->set_rewritable(false);
       if (flat->offset() == in_bytes(Klass::access_flags_offset()))
         alias_type(idx)->set_rewritable(false);
       if (flat->offset() == in_bytes(Klass::java_mirror_offset()))
         alias_type(idx)->set_rewritable(false);
+      if (flat->offset() == in_bytes(Klass::layout_helper_offset()))
+        alias_type(idx)->set_rewritable(false);
       if (flat->offset() == in_bytes(Klass::secondary_super_cache_offset()))
         alias_type(idx)->set_rewritable(false);
     }
     // %%% (We would like to finalize JavaThread::threadObj_offset(),
     // but the base pointer type is not distinctive enough to identify
     // references into JavaThread.)
 
     // Check for final fields.
     const TypeInstPtr* tinst = flat->isa_instptr();
     if (tinst && tinst->offset() >= instanceOopDesc::base_offset_in_bytes()) {
-      ciField* field;
       if (tinst->const_oop() != NULL &&
           tinst->klass() == ciEnv::current()->Class_klass() &&
           tinst->offset() >= (tinst->klass()->as_instance_klass()->size_helper() * wordSize)) {
         // static field
         ciInstanceKlass* k = tinst->const_oop()->as_instance()->java_lang_Class_klass()->as_instance_klass();
         field = k->get_field_by_offset(tinst->offset(), true);
+      } else if (tinst->klass()->is_valuetype()) {
+        // Value type field
+        ciValueKlass* vk = tinst->value_klass();
+        field = vk->get_field_by_offset(tinst->offset(), false);
       } else {
-        ciInstanceKlass *k = tinst->klass()->as_instance_klass();
+        ciInstanceKlass* k = tinst->klass()->as_instance_klass();
         field = k->get_field_by_offset(tinst->offset(), false);
       }
-      assert(field == NULL ||
-             original_field == NULL ||
-             (field->holder() == original_field->holder() &&
-              field->offset() == original_field->offset() &&
-              field->is_static() == original_field->is_static()), "wrong field?");
-      // Set field() and is_rewritable() attributes.
-      if (field != NULL)  alias_type(idx)->set_field(field);
+    }
+    assert(field == NULL ||
+           original_field == NULL ||
+           (field->holder() == original_field->holder() &&
+            field->offset() == original_field->offset() &&
+            field->is_static() == original_field->is_static()), "wrong field?");
+    // Set field() and is_rewritable() attributes.
+    if (field != NULL) {
+      alias_type(idx)->set_field(field);
+      if (flat->isa_aryptr()) {
+        // Fields of flattened inline type arrays are rewritable although they are declared final
+        assert(flat->is_aryptr()->elem()->isa_valuetype(), "must be a flattened value array");
+        alias_type(idx)->set_rewritable(true);
+      }
     }
   }
 
   // Fill the cache for next time.
-  ace->_adr_type = adr_type;
-  ace->_index    = idx;
-  assert(alias_type(adr_type) == alias_type(idx),  "type must be installed");
+  if (!uncached) {
+    ace->_adr_type = adr_type;
+    ace->_index    = idx;
+    assert(alias_type(adr_type) == alias_type(idx),  "type must be installed");
 
-  // Might as well try to fill the cache for the flattened version, too.
-  AliasCacheEntry* face = probe_alias_cache(flat);
-  if (face->_adr_type == NULL) {
-    face->_adr_type = flat;
-    face->_index    = idx;
-    assert(alias_type(flat) == alias_type(idx), "flat type must work too");
+    // Might as well try to fill the cache for the flattened version, too.
+    AliasCacheEntry* face = probe_alias_cache(flat);
+    if (face->_adr_type == NULL) {
+      face->_adr_type = flat;
+      face->_index    = idx;
+      assert(alias_type(flat) == alias_type(idx), "flat type must work too");
+    }
   }
 
   return alias_type(idx);
 }
 
@@ -1809,10 +1865,351 @@
     igvn.replace_node(opaq, opaq->in(2));
   }
   assert(opaque4_count() == 0, "should be empty");
 }
 
+void Compile::add_value_type(Node* n) {
+  assert(n->is_ValueTypeBase(), "unexpected node");
+  if (_value_type_nodes != NULL) {
+    _value_type_nodes->push(n);
+  }
+}
+
+void Compile::remove_value_type(Node* n) {
+  assert(n->is_ValueTypeBase(), "unexpected node");
+  if (_value_type_nodes != NULL) {
+    _value_type_nodes->remove(n);
+  }
+}
+
+// Does the return value keep otherwise useless value type allocations
+// alive?
+static bool return_val_keeps_allocations_alive(Node* ret_val) {
+  ResourceMark rm;
+  Unique_Node_List wq;
+  wq.push(ret_val);
+  bool some_allocations = false;
+  for (uint i = 0; i < wq.size(); i++) {
+    Node* n = wq.at(i);
+    assert(!n->is_ValueTypeBase(), "chain of value type nodes");
+    if (n->outcnt() > 1) {
+      // Some other use for the allocation
+      return false;
+    } else if (n->is_Phi()) {
+      for (uint j = 1; j < n->req(); j++) {
+        wq.push(n->in(j));
+      }
+    } else if (n->is_CheckCastPP() &&
+               n->in(1)->is_Proj() &&
+               n->in(1)->in(0)->is_Allocate()) {
+      some_allocations = true;
+    }
+  }
+  return some_allocations;
+}
+
+void Compile::process_value_types(PhaseIterGVN &igvn) {
+  // Make value types scalar in safepoints
+  while (_value_type_nodes->size() != 0) {
+    ValueTypeBaseNode* vt = _value_type_nodes->pop()->as_ValueTypeBase();
+    vt->make_scalar_in_safepoints(&igvn);
+    if (vt->is_ValueTypePtr()) {
+      igvn.replace_node(vt, vt->get_oop());
+    } else if (vt->outcnt() == 0) {
+      igvn.remove_dead_node(vt);
+    }
+  }
+  _value_type_nodes = NULL;
+  if (tf()->returns_value_type_as_fields()) {
+    Node* ret = NULL;
+    for (uint i = 1; i < root()->req(); i++){
+      Node* in = root()->in(i);
+      if (in->Opcode() == Op_Return) {
+        assert(ret == NULL, "only one return");
+        ret = in;
+      }
+    }
+    if (ret != NULL) {
+      Node* ret_val = ret->in(TypeFunc::Parms);
+      if (igvn.type(ret_val)->isa_oopptr() &&
+          return_val_keeps_allocations_alive(ret_val)) {
+        igvn.replace_input_of(ret, TypeFunc::Parms, ValueTypeNode::tagged_klass(igvn.type(ret_val)->value_klass(), igvn));
+        assert(ret_val->outcnt() == 0, "should be dead now");
+        igvn.remove_dead_node(ret_val);
+      }
+    }
+  }
+  igvn.optimize();
+}
+
+void Compile::adjust_flattened_array_access_aliases(PhaseIterGVN& igvn) {
+  if (!_has_flattened_accesses) {
+    return;
+  }
+  // Initially, all flattened array accesses share the same slice to
+  // keep dependencies with Object[] array accesses (that could be
+  // to a flattened array) correct. We're done with parsing so we
+  // now know all flattened array accesses in this compile
+  // unit. Let's move flattened array accesses to their own slice,
+  // one per element field. This should help memory access
+  // optimizations.
+  ResourceMark rm;
+  Unique_Node_List wq;
+  wq.push(root());
+
+  Node_List mergememnodes;
+  Node_List memnodes;
+
+  // Alias index currently shared by all flattened memory accesses
+  int index = get_alias_index(TypeAryPtr::VALUES);
+
+  // Find MergeMem nodes and flattened array accesses
+  for (uint i = 0; i < wq.size(); i++) {
+    Node* n = wq.at(i);
+    if (n->is_Mem()) {
+      const TypePtr* adr_type = NULL;
+      if (n->Opcode() == Op_StoreCM) {
+        adr_type = get_adr_type(get_alias_index(n->in(MemNode::OopStore)->adr_type()));
+      } else {
+        adr_type = get_adr_type(get_alias_index(n->adr_type()));
+      }
+      if (adr_type == TypeAryPtr::VALUES) {
+        memnodes.push(n);
+      }
+    } else if (n->is_MergeMem()) {
+      MergeMemNode* mm = n->as_MergeMem();
+      if (mm->memory_at(index) != mm->base_memory()) {
+        mergememnodes.push(n);
+      }
+    }
+    for (uint j = 0; j < n->req(); j++) {
+      Node* m = n->in(j);
+      if (m != NULL) {
+        wq.push(m);
+      }
+    }
+  }
+
+  if (memnodes.size() > 0) {
+    _flattened_accesses_share_alias = false;
+
+    // We are going to change the slice for the flattened array
+    // accesses so we need to clear the cache entries that refer to
+    // them.
+    for (uint i = 0; i < AliasCacheSize; i++) {
+      AliasCacheEntry* ace = &_alias_cache[i];
+      if (ace->_adr_type != NULL &&
+          ace->_adr_type->isa_aryptr() &&
+          ace->_adr_type->is_aryptr()->elem()->isa_valuetype()) {
+        ace->_adr_type = NULL;
+        ace->_index = (i != 0) ? 0 : AliasIdxTop; // Make sure the NULL adr_type resolves to AliasIdxTop
+      }
+    }
+
+    // Find what aliases we are going to add
+    int start_alias = num_alias_types()-1;
+    int stop_alias = 0;
+
+    for (uint i = 0; i < memnodes.size(); i++) {
+      Node* m = memnodes.at(i);
+      const TypePtr* adr_type = NULL;
+      if (m->Opcode() == Op_StoreCM) {
+        adr_type = m->in(MemNode::OopStore)->adr_type();
+        Node* clone = new StoreCMNode(m->in(MemNode::Control), m->in(MemNode::Memory), m->in(MemNode::Address),
+                                      m->adr_type(), m->in(MemNode::ValueIn), m->in(MemNode::OopStore),
+                                      get_alias_index(adr_type));
+        igvn.register_new_node_with_optimizer(clone);
+        igvn.replace_node(m, clone);
+      } else {
+        adr_type = m->adr_type();
+#ifdef ASSERT
+        m->as_Mem()->set_adr_type(adr_type);
+#endif
+      }
+      int idx = get_alias_index(adr_type);
+      start_alias = MIN2(start_alias, idx);
+      stop_alias = MAX2(stop_alias, idx);
+    }
+
+    assert(stop_alias >= start_alias, "should have expanded aliases");
+
+    Node_Stack stack(0);
+#ifdef ASSERT
+    VectorSet seen(Thread::current()->resource_area());
+#endif
+    // Now let's fix the memory graph so each flattened array access
+    // is moved to the right slice. Start from the MergeMem nodes.
+    uint last = unique();
+    for (uint i = 0; i < mergememnodes.size(); i++) {
+      MergeMemNode* current = mergememnodes.at(i)->as_MergeMem();
+      Node* n = current->memory_at(index);
+      MergeMemNode* mm = NULL;
+      do {
+        // Follow memory edges through memory accesses, phis and
+        // narrow membars and push nodes on the stack. Once we hit
+        // bottom memory, we pop element off the stack one at a
+        // time, in reverse order, and move them to the right slice
+        // by changing their memory edges.
+        if ((n->is_Phi() && n->adr_type() != TypePtr::BOTTOM) || n->is_Mem() || n->adr_type() == TypeAryPtr::VALUES) {
+          assert(!seen.test_set(n->_idx), "");
+          // Uses (a load for instance) will need to be moved to the
+          // right slice as well and will get a new memory state
+          // that we don't know yet. The use could also be the
+          // backedge of a loop. We put a place holder node between
+          // the memory node and its uses. We replace that place
+          // holder with the correct memory state once we know it,
+          // i.e. when nodes are popped off the stack. Using the
+          // place holder make the logic work in the presence of
+          // loops.
+          if (n->outcnt() > 1) {
+            Node* place_holder = NULL;
+            assert(!n->has_out_with(Op_Node), "");
+            for (DUIterator k = n->outs(); n->has_out(k); k++) {
+              Node* u = n->out(k);
+              if (u != current && u->_idx < last) {
+                bool success = false;
+                for (uint l = 0; l < u->req(); l++) {
+                  if (!stack.is_empty() && u == stack.node() && l == stack.index()) {
+                    continue;
+                  }
+                  Node* in = u->in(l);
+                  if (in == n) {
+                    if (place_holder == NULL) {
+                      place_holder = new Node(1);
+                      place_holder->init_req(0, n);
+                    }
+                    igvn.replace_input_of(u, l, place_holder);
+                    success = true;
+                  }
+                }
+                if (success) {
+                  --k;
+                }
+              }
+            }
+          }
+          if (n->is_Phi()) {
+            stack.push(n, 1);
+            n = n->in(1);
+          } else if (n->is_Mem()) {
+            stack.push(n, n->req());
+            n = n->in(MemNode::Memory);
+          } else {
+            assert(n->is_Proj() && n->in(0)->Opcode() == Op_MemBarCPUOrder, "");
+            stack.push(n, n->req());
+            n = n->in(0)->in(TypeFunc::Memory);
+          }
+        } else {
+          assert(n->adr_type() == TypePtr::BOTTOM || (n->Opcode() == Op_Node && n->_idx >= last) || (n->is_Proj() && n->in(0)->is_Initialize()), "");
+          // Build a new MergeMem node to carry the new memory state
+          // as we build it. IGVN should fold extraneous MergeMem
+          // nodes.
+          mm = MergeMemNode::make(n);
+          igvn.register_new_node_with_optimizer(mm);
+          while (stack.size() > 0) {
+            Node* m = stack.node();
+            uint idx = stack.index();
+            if (m->is_Mem()) {
+              // Move memory node to its new slice
+              const TypePtr* adr_type = m->adr_type();
+              int alias = get_alias_index(adr_type);
+              Node* prev = mm->memory_at(alias);
+              igvn.replace_input_of(m, MemNode::Memory, prev);
+              mm->set_memory_at(alias, m);
+            } else if (m->is_Phi()) {
+              // We need as many new phis as there are new aliases
+              igvn.replace_input_of(m, idx, mm);
+              if (idx == m->req()-1) {
+                Node* r = m->in(0);
+                for (uint j = (uint)start_alias; j <= (uint)stop_alias; j++) {
+                  const Type* adr_type = get_adr_type(j);
+                  if (!adr_type->isa_aryptr() || !adr_type->is_aryptr()->elem()->isa_valuetype()) {
+                    continue;
+                  }
+                  Node* phi = new PhiNode(r, Type::MEMORY, get_adr_type(j));
+                  igvn.register_new_node_with_optimizer(phi);
+                  for (uint k = 1; k < m->req(); k++) {
+                    phi->init_req(k, m->in(k)->as_MergeMem()->memory_at(j));
+                  }
+                  mm->set_memory_at(j, phi);
+                }
+                Node* base_phi = new PhiNode(r, Type::MEMORY, TypePtr::BOTTOM);
+                igvn.register_new_node_with_optimizer(base_phi);
+                for (uint k = 1; k < m->req(); k++) {
+                  base_phi->init_req(k, m->in(k)->as_MergeMem()->base_memory());
+                }
+                mm->set_base_memory(base_phi);
+              }
+            } else {
+              // This is a MemBarCPUOrder node from
+              // Parse::array_load()/Parse::array_store(), in the
+              // branch that handles flattened arrays hidden under
+              // an Object[] array. We also need one new membar per
+              // new alias to keep the unknown access that the
+              // membars protect properly ordered with accesses to
+              // known flattened array.
+              assert(m->is_Proj(), "projection expected");
+              Node* ctrl = m->in(0)->in(TypeFunc::Control);
+              igvn.replace_input_of(m->in(0), TypeFunc::Control, top());
+              for (uint j = (uint)start_alias; j <= (uint)stop_alias; j++) {
+                const Type* adr_type = get_adr_type(j);
+                if (!adr_type->isa_aryptr() || !adr_type->is_aryptr()->elem()->isa_valuetype()) {
+                  continue;
+                }
+                MemBarNode* mb = new MemBarCPUOrderNode(this, j, NULL);
+                igvn.register_new_node_with_optimizer(mb);
+                Node* mem = mm->memory_at(j);
+                mb->init_req(TypeFunc::Control, ctrl);
+                mb->init_req(TypeFunc::Memory, mem);
+                ctrl = new ProjNode(mb, TypeFunc::Control);
+                igvn.register_new_node_with_optimizer(ctrl);
+                mem = new ProjNode(mb, TypeFunc::Memory);
+                igvn.register_new_node_with_optimizer(mem);
+                mm->set_memory_at(j, mem);
+              }
+              igvn.replace_node(m->in(0)->as_Multi()->proj_out(TypeFunc::Control), ctrl);
+            }
+            if (idx < m->req()-1) {
+              idx += 1;
+              stack.set_index(idx);
+              n = m->in(idx);
+              break;
+            }
+            // Take care of place holder nodes
+            if (m->has_out_with(Op_Node)) {
+              Node* place_holder = m->find_out_with(Op_Node);
+              if (place_holder != NULL) {
+                Node* mm_clone = mm->clone();
+                igvn.register_new_node_with_optimizer(mm_clone);
+                Node* hook = new Node(1);
+                hook->init_req(0, mm);
+                igvn.replace_node(place_holder, mm_clone);
+                hook->destruct();
+              }
+              assert(!m->has_out_with(Op_Node), "place holder should be gone now");
+            }
+            stack.pop();
+          }
+        }
+      } while(stack.size() > 0);
+      // Fix the memory state at the MergeMem we started from
+      igvn.rehash_node_delayed(current);
+      for (uint j = (uint)start_alias; j <= (uint)stop_alias; j++) {
+        const Type* adr_type = get_adr_type(j);
+        if (!adr_type->isa_aryptr() || !adr_type->is_aryptr()->elem()->isa_valuetype()) {
+          continue;
+        }
+        current->set_memory_at(j, mm);
+      }
+      current->set_memory_at(index, current->base_memory());
+    }
+    igvn.optimize();
+  }
+  print_method(PHASE_SPLIT_VALUES_ARRAY, 2);
+}
+
+
 // StringOpts and late inlining of string methods
 void Compile::inline_string_calls(bool parse_time) {
   {
     // remove useless nodes to make the usage analysis simpler
     ResourceMark rm;
@@ -2088,10 +2485,17 @@
     set_for_igvn(&new_worklist);
     igvn = PhaseIterGVN(initial_gvn());
     igvn.optimize();
   }
 
+  if (_value_type_nodes->size() > 0) {
+    // Do this once all inlining is over to avoid getting inconsistent debug info
+    process_value_types(igvn);
+  }
+
+  adjust_flattened_array_access_aliases(igvn);
+
   // Perform escape analysis
   if (_do_escape_analysis && ConnectionGraph::has_candidates(this)) {
     if (has_loops()) {
       // Cleanup graph (remove dead nodes).
       TracePhase tp("idealLoop", &timers[_t_idealLoop]);
@@ -2762,10 +3166,11 @@
       mem = prev->in(MemNode::Memory);
     }
   }
 }
 
+
 //------------------------------final_graph_reshaping_impl----------------------
 // Implement items 1-5 from final_graph_reshaping below.
 void Compile::final_graph_reshaping_impl( Node *n, Final_Reshape_Counts &frc) {
 
   if ( n->outcnt() == 0 ) return; // dead node
@@ -3500,10 +3905,18 @@
       Node* cmp = new CmpLNode(andl, n->in(2));
       n->subsume_by(cmp, this);
     }
     break;
   }
+#ifdef ASSERT
+  case Op_ValueTypePtr:
+  case Op_ValueType: {
+    n->dump(-1);
+    assert(false, "value type node was not removed");
+    break;
+  }
+#endif
   default:
     assert(!n->is_Call(), "");
     assert(!n->is_Mem(), "");
     assert(nop != Op_ProfileBoolean, "should be eliminated during IGVN");
     break;
@@ -3848,20 +4261,20 @@
   if (holder->is_being_initialized()) {
     if (accessing_method->holder() == holder) {
       // Access inside a class. The barrier can be elided when access happens in <clinit>,
       // <init>, or a static method. In all those cases, there was an initialization
       // barrier on the holder klass passed.
-      if (accessing_method->is_static_initializer() ||
-          accessing_method->is_object_initializer() ||
+      if (accessing_method->is_class_initializer() ||
+          accessing_method->is_object_constructor() ||
           accessing_method->is_static()) {
         return false;
       }
     } else if (accessing_method->holder()->is_subclass_of(holder)) {
       // Access from a subclass. The barrier can be elided only when access happens in <clinit>.
       // In case of <init> or a static method, the barrier is on the subclass is not enough:
       // child class can become fully initialized while its parent class is still being initialized.
-      if (accessing_method->is_static_initializer()) {
+      if (accessing_method->is_class_initializer()) {
         return false;
       }
     }
     ciMethod* root = method(); // the root method of compilation
     if (root != accessing_method) {
@@ -3979,21 +4392,23 @@
 // (0) superklass is java.lang.Object (can occur in reflective code)
 // (1) subklass is already limited to a subtype of superklass => always ok
 // (2) subklass does not overlap with superklass => always fail
 // (3) superklass has NO subtypes and we can check with a simple compare.
 int Compile::static_subtype_check(ciKlass* superk, ciKlass* subk) {
-  if (StressReflectiveCode) {
+  if (StressReflectiveCode || superk == NULL || subk == NULL) {
     return SSC_full_test;       // Let caller generate the general case.
   }
 
   if (superk == env()->Object_klass()) {
     return SSC_always_true;     // (0) this test cannot fail
   }
 
   ciType* superelem = superk;
-  if (superelem->is_array_klass())
+  if (superelem->is_array_klass()) {
+    ciArrayKlass* ak = superelem->as_array_klass();
     superelem = superelem->as_array_klass()->base_element_type();
+  }
 
   if (!subk->is_interface()) {  // cannot trust static interface types yet
     if (subk->is_subtype_of(superk)) {
       return SSC_always_true;   // (1) false path dead; no dynamic test needed
     }
@@ -4450,10 +4865,31 @@
     igvn.check_no_speculative_types();
 #endif
   }
 }
 
+Node* Compile::optimize_acmp(PhaseGVN* phase, Node* a, Node* b) {
+  const TypeInstPtr* ta = phase->type(a)->isa_instptr();
+  const TypeInstPtr* tb = phase->type(b)->isa_instptr();
+  if (!EnableValhalla || ta == NULL || tb == NULL ||
+      ta->is_zero_type() || tb->is_zero_type() ||
+      !ta->can_be_value_type() || !tb->can_be_value_type()) {
+    // Use old acmp if one operand is null or not a value type
+    return new CmpPNode(a, b);
+  } else if (ta->is_valuetypeptr() || tb->is_valuetypeptr()) {
+    // We know that one operand is a value type. Therefore,
+    // new acmp will only return true if both operands are NULL.
+    // Check if both operands are null by or'ing the oops.
+    a = phase->transform(new CastP2XNode(NULL, a));
+    b = phase->transform(new CastP2XNode(NULL, b));
+    a = phase->transform(new OrXNode(a, b));
+    return new CmpXNode(a, phase->MakeConX(0));
+  }
+  // Use new acmp
+  return NULL;
+}
+
 // Auxiliary method to support randomized stressing/fuzzing.
 //
 // This method can be called the arbitrary number of times, with current count
 // as the argument. The logic allows selecting a single candidate from the
 // running list of candidates as follows:
diff a/src/hotspot/share/opto/compile.hpp b/src/hotspot/share/opto/compile.hpp
--- a/src/hotspot/share/opto/compile.hpp
+++ b/src/hotspot/share/opto/compile.hpp
@@ -46,10 +46,11 @@
 
 class AddPNode;
 class Block;
 class Bundle;
 class CallGenerator;
+class CallNode;
 class CloneMap;
 class ConnectionGraph;
 class IdealGraphPrinter;
 class InlineTree;
 class Int_Array;
@@ -83,10 +84,11 @@
 class TypePtr;
 class TypeOopPtr;
 class TypeFunc;
 class TypeVect;
 class Unique_Node_List;
+class ValueTypeBaseNode;
 class nmethod;
 class WarmCallInfo;
 class Node_Stack;
 struct Final_Reshape_Counts;
 
@@ -298,10 +300,12 @@
   // JSR 292
   bool                  _has_method_handle_invokes; // True if this method has MethodHandle invokes.
   RTMState              _rtm_state;             // State of Restricted Transactional Memory usage
   int                   _loop_opts_cnt;         // loop opts round
   bool                  _clinit_barrier_on_entry; // True if clinit barrier is needed on nmethod entry
+  bool                  _has_flattened_accesses; // Any known flattened array accesses?
+  bool                  _flattened_accesses_share_alias; // Initially all flattened array share a single slice
 
   // Compilation environment.
   Arena                 _comp_arena;            // Arena with lifetime equivalent to Compile
   void*                 _barrier_set_state;     // Potential GC barrier state for Compile
   ciEnv*                _env;                   // CI interface
@@ -312,10 +316,11 @@
   GrowableArray<Node*>* _macro_nodes;           // List of nodes which need to be expanded before matching.
   GrowableArray<Node*>* _predicate_opaqs;       // List of Opaque1 nodes for the loop predicates.
   GrowableArray<Node*>* _expensive_nodes;       // List of nodes that are expensive to compute and that we'd better not let the GVN freely common
   GrowableArray<Node*>* _range_check_casts;     // List of CastII nodes with a range check dependency
   GrowableArray<Node*>* _opaque4_nodes;         // List of Opaque4 nodes that have a default value
+  Unique_Node_List*     _value_type_nodes;      // List of ValueType nodes
   ConnectionGraph*      _congraph;
 #ifndef PRODUCT
   IdealGraphPrinter*    _printer;
   static IdealGraphPrinter* _debug_file_printer;
   static IdealGraphPrinter* _debug_network_printer;
@@ -590,10 +595,17 @@
   bool          profile_rtm() const              { return _rtm_state == ProfileRTM; }
   uint              max_node_limit() const       { return (uint)_max_node_limit; }
   void          set_max_node_limit(uint n)       { _max_node_limit = n; }
   bool              clinit_barrier_on_entry()       { return _clinit_barrier_on_entry; }
   void          set_clinit_barrier_on_entry(bool z) { _clinit_barrier_on_entry = z; }
+  void          set_flattened_accesses()         { _has_flattened_accesses = true; }
+  bool          flattened_accesses_share_alias() const { return _flattened_accesses_share_alias; }
+  void          set_flattened_accesses_share_alias(bool z) { _flattened_accesses_share_alias = z; }
+
+  // Support for scalarized value type calling convention
+  bool              has_scalarized_args() const  { return _method != NULL && _method->has_scalarized_args(); }
+  bool              needs_stack_repair()  const  { return _method != NULL && _method->get_Method()->c2_needs_stack_repair(); }
 
   // check the CompilerOracle for special behaviours for this compile
   bool          method_has_option(const char * option) {
     return method() != NULL && method()->has_option(option);
   }
@@ -697,10 +709,18 @@
   }
   Node* opaque4_node(int idx) const { return _opaque4_nodes->at(idx);  }
   int   opaque4_count()       const { return _opaque4_nodes->length(); }
   void  remove_opaque4_nodes(PhaseIterGVN &igvn);
 
+  // Keep track of value type nodes for later processing
+  void add_value_type(Node* n);
+  void remove_value_type(Node* n);
+  void process_value_types(PhaseIterGVN &igvn);
+  bool can_add_value_type() const { return _value_type_nodes != NULL; }
+
+  void adjust_flattened_array_access_aliases(PhaseIterGVN& igvn);
+
   void sort_macro_nodes();
 
   // remove the opaque nodes that protect the predicates so that the unused checks and
   // uncommon traps will be eliminated from the graph.
   void cleanup_loop_predicates(PhaseIterGVN &igvn);
@@ -835,15 +855,15 @@
     _last_tf_m = m;
     _last_tf = tf;
   }
 
   AliasType*        alias_type(int                idx)  { assert(idx < num_alias_types(), "oob"); return _alias_types[idx]; }
-  AliasType*        alias_type(const TypePtr* adr_type, ciField* field = NULL) { return find_alias_type(adr_type, false, field); }
+  AliasType*        alias_type(const TypePtr* adr_type, ciField* field = NULL, bool uncached = false) { return find_alias_type(adr_type, false, field, uncached); }
   bool         have_alias_type(const TypePtr* adr_type);
   AliasType*        alias_type(ciField*         field);
 
-  int               get_alias_index(const TypePtr* at)  { return alias_type(at)->index(); }
+  int               get_alias_index(const TypePtr* at, bool uncached = false) { return alias_type(at, NULL, uncached)->index(); }
   const TypePtr*    get_adr_type(uint aidx)             { return alias_type(aidx)->adr_type(); }
   int               get_general_index(uint aidx)        { return alias_type(aidx)->general_index(); }
 
   // Building nodes
   void              rethrow_exceptions(JVMState* jvms);
@@ -1063,11 +1083,11 @@
 
   // Management of the AliasType table.
   void grow_alias_types();
   AliasCacheEntry* probe_alias_cache(const TypePtr* adr_type);
   const TypePtr *flatten_alias_type(const TypePtr* adr_type) const;
-  AliasType* find_alias_type(const TypePtr* adr_type, bool no_create, ciField* field);
+  AliasType* find_alias_type(const TypePtr* adr_type, bool no_create, ciField* field, bool uncached = false);
 
   void verify_top(Node*) const PRODUCT_RETURN;
 
   // Intrinsic setup.
   void           register_library_intrinsics();                            // initializer
@@ -1136,10 +1156,12 @@
                               Node* ctrl = NULL);
 
   // Convert integer value to a narrowed long type dependent on ctrl (for example, a range check)
   static Node* constrained_convI2L(PhaseGVN* phase, Node* value, const TypeInt* itype, Node* ctrl);
 
+  Node* optimize_acmp(PhaseGVN* phase, Node* a, Node* b);
+
   // Auxiliary method for randomized fuzzing/stressing
   static bool randomized_select(int count);
 
   // supporting clone_map
   CloneMap&     clone_map();
diff a/src/hotspot/share/prims/whitebox.cpp b/src/hotspot/share/prims/whitebox.cpp
--- a/src/hotspot/share/prims/whitebox.cpp
+++ b/src/hotspot/share/prims/whitebox.cpp
@@ -43,21 +43,23 @@
 #include "jvmtifiles/jvmtiEnv.hpp"
 #include "memory/filemap.hpp"
 #include "memory/heapShared.inline.hpp"
 #include "memory/metaspaceShared.hpp"
 #include "memory/metadataFactory.hpp"
-#include "memory/iterator.hpp"
+#include "memory/iterator.inline.hpp"
 #include "memory/resourceArea.hpp"
 #include "memory/universe.hpp"
 #include "memory/oopFactory.hpp"
 #include "oops/array.hpp"
 #include "oops/compressedOops.hpp"
+#include "oops/compressedOops.inline.hpp"
 #include "oops/constantPool.inline.hpp"
 #include "oops/method.inline.hpp"
 #include "oops/objArrayKlass.hpp"
 #include "oops/objArrayOop.inline.hpp"
 #include "oops/oop.inline.hpp"
+#include "oops/objArrayOop.inline.hpp"
 #include "oops/typeArrayOop.inline.hpp"
 #include "prims/resolvedMethodTable.hpp"
 #include "prims/wbtestmethods/parserTests.hpp"
 #include "prims/whitebox.inline.hpp"
 #include "runtime/arguments.hpp"
@@ -1836,10 +1838,102 @@
 
 WB_ENTRY(jint, WB_ConstantPoolEncodeIndyIndex(JNIEnv* env, jobject wb, jint index))
   return ConstantPool::encode_invokedynamic_index(index);
 WB_END
 
+WB_ENTRY(jobjectArray, WB_getObjectsViaKlassOopMaps(JNIEnv* env, jobject wb, jobject thing))
+  oop aoop = JNIHandles::resolve(thing);
+  if (!aoop->is_instance()) {
+    return NULL;
+  }
+  instanceHandle ih(THREAD, (instanceOop) aoop);
+  InstanceKlass* klass = InstanceKlass::cast(aoop->klass());
+  if (klass->nonstatic_oop_map_count() == 0) {
+    return NULL;
+  }
+  const OopMapBlock* map = klass->start_of_nonstatic_oop_maps();
+  const OopMapBlock* const end = map + klass->nonstatic_oop_map_count();
+  int oop_count = 0;
+  while (map < end) {
+    oop_count += map->count();
+    map++;
+  }
+
+  objArrayOop result_array =
+      oopFactory::new_objArray(SystemDictionary::Object_klass(), oop_count, CHECK_NULL);
+  map = klass->start_of_nonstatic_oop_maps();
+  instanceOop ioop = ih();
+  int index = 0;
+  while (map < end) {
+    int offset = map->offset();
+    for (unsigned int j = 0; j < map->count(); j++) {
+      result_array->obj_at_put(index++, ioop->obj_field(offset));
+      offset += heapOopSize;
+    }
+    map++;
+  }
+  return (jobjectArray)JNIHandles::make_local(env, result_array);
+WB_END
+
+class CollectOops : public BasicOopIterateClosure {
+ public:
+  GrowableArray<Handle>* array;
+
+  objArrayOop create_results(TRAPS) {
+    objArrayOop result_array =
+        oopFactory::new_objArray(SystemDictionary::Object_klass(), array->length(), CHECK_NULL);
+    for (int i = 0 ; i < array->length(); i++) {
+      result_array->obj_at_put(i, array->at(i)());
+    }
+    return result_array;
+  }
+
+  jobjectArray create_jni_result(JNIEnv* env, TRAPS) {
+    return (jobjectArray)JNIHandles::make_local(env, create_results(THREAD));
+  }
+
+  void add_oop(oop o) {
+    // Value might be oop, but JLS can't see as Object, just iterate through it...
+    if (o != NULL && o->is_value()) {
+      o->oop_iterate(this);
+    } else {
+      array->append(Handle(Thread::current(), o));
+    }
+  }
+
+  void do_oop(oop* o) { add_oop(*o); }
+  void do_oop(narrowOop* v) { add_oop(CompressedOops::decode(*v)); }
+};
+
+
+WB_ENTRY(jobjectArray, WB_getObjectsViaOopIterator(JNIEnv* env, jobject wb, jobject thing))
+  ResourceMark rm(THREAD);
+  GrowableArray<Handle>* array = new GrowableArray<Handle>(128);
+  CollectOops collectOops;
+  collectOops.array = array;
+
+  JNIHandles::resolve(thing)->oop_iterate(&collectOops);
+
+  return collectOops.create_jni_result(env, THREAD);
+WB_END
+
+WB_ENTRY(jobjectArray, WB_getObjectsViaFrameOopIterator(JNIEnv* env, jobject wb, jint depth))
+  ResourceMark rm(THREAD);
+  GrowableArray<Handle>* array = new GrowableArray<Handle>(128);
+  CollectOops collectOops;
+  collectOops.array = array;
+  StackFrameStream sfs(thread);
+  while (depth > 0) { // Skip the native WB API frame
+    sfs.next();
+    frame* f = sfs.current();
+    f->oops_do(&collectOops, NULL, sfs.register_map());
+    depth--;
+  }
+  return collectOops.create_jni_result(env, THREAD);
+WB_END
+
+
 WB_ENTRY(void, WB_ClearInlineCaches(JNIEnv* env, jobject wb, jboolean preserve_static_stubs))
   VM_ClearICs clear_ics(preserve_static_stubs == JNI_TRUE);
   VMThread::execute(&clear_ics);
 WB_END
 
@@ -2467,10 +2561,16 @@
   {CC"getConstantPoolCacheLength0", CC"(Ljava/lang/Class;)I",  (void*)&WB_GetConstantPoolCacheLength},
   {CC"remapInstructionOperandFromCPCache0",
       CC"(Ljava/lang/Class;I)I",                      (void*)&WB_ConstantPoolRemapInstructionOperandFromCache},
   {CC"encodeConstantPoolIndyIndex0",
       CC"(I)I",                      (void*)&WB_ConstantPoolEncodeIndyIndex},
+  {CC"getObjectsViaKlassOopMaps0",
+      CC"(Ljava/lang/Object;)[Ljava/lang/Object;",    (void*)&WB_getObjectsViaKlassOopMaps},
+  {CC"getObjectsViaOopIterator0",
+          CC"(Ljava/lang/Object;)[Ljava/lang/Object;",(void*)&WB_getObjectsViaOopIterator},
+  {CC"getObjectsViaFrameOopIterator",
+      CC"(I)[Ljava/lang/Object;",                     (void*)&WB_getObjectsViaFrameOopIterator},
   {CC"getMethodBooleanOption",
       CC"(Ljava/lang/reflect/Executable;Ljava/lang/String;)Ljava/lang/Boolean;",
                                                       (void*)&WB_GetMethodBooleaneOption},
   {CC"getMethodIntxOption",
       CC"(Ljava/lang/reflect/Executable;Ljava/lang/String;)Ljava/lang/Long;",
diff a/src/hotspot/share/runtime/arguments.cpp b/src/hotspot/share/runtime/arguments.cpp
--- a/src/hotspot/share/runtime/arguments.cpp
+++ b/src/hotspot/share/runtime/arguments.cpp
@@ -2154,10 +2154,20 @@
   }
 #endif
 
   status = status && GCArguments::check_args_consistency();
 
+  if (AMD64_ONLY(false &&) !FLAG_IS_DEFAULT(InlineTypePassFieldsAsArgs)) {
+    FLAG_SET_CMDLINE(InlineTypePassFieldsAsArgs, false);
+    warning("InlineTypePassFieldsAsArgs is not supported on this platform");
+  }
+
+  if (AMD64_ONLY(false &&) !FLAG_IS_DEFAULT(InlineTypeReturnedAsFields)) {
+    FLAG_SET_CMDLINE(InlineTypeReturnedAsFields, false);
+    warning("InlineTypeReturnedAsFields is not supported on this platform");
+  }
+
   return status;
 }
 
 bool Arguments::is_bad_option(const JavaVMOption* option, jboolean ignore,
   const char* option_type) {
@@ -3060,10 +3070,16 @@
     } else if (is_bad_option(option, args->ignoreUnrecognized)) {
       return JNI_ERR;
     }
   }
 
+  if (EnableValhalla) {
+    if (!create_property("valhalla.enableValhalla", "true", InternalProperty)) {
+      return JNI_ENOMEM;
+    }
+  }
+
   // PrintSharedArchiveAndExit will turn on
   //   -Xshare:on
   //   -Xlog:class+path=info
   if (PrintSharedArchiveAndExit) {
     if (FLAG_SET_CMDLINE(UseSharedSpaces, true) != JVMFlag::SUCCESS) {
@@ -4154,10 +4170,15 @@
   // verification is not as if both were enabled.
   if (BytecodeVerificationLocal && !BytecodeVerificationRemote) {
     log_info(verification)("Turning on remote verification because local verification is on");
     FLAG_SET_DEFAULT(BytecodeVerificationRemote, true);
   }
+  if (!EnableValhalla || (is_interpreter_only() && !is_dumping_archive())) {
+    // Disable calling convention optimizations if value types are not supported
+    InlineTypePassFieldsAsArgs = false;
+    InlineTypeReturnedAsFields = false;
+  }
 
 #ifndef PRODUCT
   if (!LogVMOutput && FLAG_IS_DEFAULT(LogVMOutput)) {
     if (use_vm_log()) {
       LogVMOutput = true;
diff a/src/hotspot/share/runtime/globals.hpp b/src/hotspot/share/runtime/globals.hpp
--- a/src/hotspot/share/runtime/globals.hpp
+++ b/src/hotspot/share/runtime/globals.hpp
@@ -760,10 +760,28 @@
           "Use SSE2 MOVQ instruction for Arraycopy")                        \
                                                                             \
   notproduct(bool, PrintFieldLayout, false,                                 \
           "Print field layout for each class")                              \
                                                                             \
+  notproduct(bool, PrintInlineLayout, false,                                \
+          "Print field layout for each inline type")                        \
+                                                                            \
+  notproduct(bool, PrintInlineArrayLayout, false,                           \
+          "Print array layout for each inline type array")                  \
+                                                                            \
+  product(intx, InlineArrayElemMaxFlatSize, -1,                             \
+          "Max size for flattening inline array elements, <0 no limit")     \
+                                                                            \
+  product(intx, InlineFieldMaxFlatSize, 128,                                \
+          "Max size for flattening inline type fields, <0 no limit")        \
+                                                                            \
+  product(intx, InlineArrayElemMaxFlatOops, 4,                              \
+          "Max nof embedded object references in an inline type to flatten, <0 no limit")  \
+                                                                            \
+  product(bool, InlineArrayAtomicAccess, false,                             \
+          "Atomic inline array accesses by-default, for all inline arrays") \
+                                                                            \
   /* Need to limit the extent of the padding to reasonable size.          */\
   /* 8K is well beyond the reasonable HW cache line size, even with       */\
   /* aggressive prefetching, while still leaving the room for segregating */\
   /* among the distinct pages.                                            */\
   product(intx, ContendedPaddingWidth, 128,                                 \
@@ -2449,19 +2467,47 @@
           "Start flight recording with options"))                           \
                                                                             \
   experimental(bool, UseFastUnorderedTimeStamps, false,                     \
           "Use platform unstable time where supported for timestamps only") \
                                                                             \
+  product(bool, EnableValhalla, true,                                       \
+          "Enable experimental Valhalla features")                          \
+                                                                            \
+  product_pd(bool, InlineTypePassFieldsAsArgs,                              \
+          "Pass each inline type field as an argument at calls")            \
+                                                                            \
+  product_pd(bool, InlineTypeReturnedAsFields,                              \
+          "Return fields instead of an inline type reference")              \
+                                                                            \
+  develop(bool, StressInlineTypeReturnedAsFields, false,                    \
+          "Stress return of fields instead of an inline type reference")    \
+                                                                            \
+  develop(bool, ScalarizeInlineTypes, true,                                 \
+          "Scalarize inline types in compiled code")                        \
+                                                                            \
+  diagnostic(ccstrlist, ForceNonTearable, "",                               \
+          "List of inline classes which are forced to be atomic "           \
+          "(whitespace and commas separate names, "                         \
+          "and leading and trailing stars '*' are wildcards)")              \
+                                                                            \
+  product(bool, PrintNewLayout, false,                                      \
+               "Print layout compute by new algorithm")                     \
+                                                                            \
+  product(bool, PrintFlattenableLayouts, false,                             \
+                "Print layout of inline classes and classes with "          \
+                "flattenable fields")                                       \
+                                                                            \
   product(bool, UseNewFieldLayout, true,                                    \
-               "(Deprecated) Use new algorithm to compute field layouts")   \
+                "(Deprecated) Use new algorithm to compute field layouts")  \
                                                                             \
   product(bool, UseEmptySlotsInSupers, true,                                \
                 "Allow allocating fields in empty slots of super-classes")  \
                                                                             \
   diagnostic(bool, DeoptimizeNMethodBarriersALot, false,                    \
                 "Make nmethod barriers deoptimise a lot.")                  \
 
+
 // Interface macros
 #define DECLARE_PRODUCT_FLAG(type, name, value, doc)      extern "C" type name;
 #define DECLARE_PD_PRODUCT_FLAG(type, name, doc)          extern "C" type name;
 #define DECLARE_DIAGNOSTIC_FLAG(type, name, value, doc)   extern "C" type name;
 #define DECLARE_PD_DIAGNOSTIC_FLAG(type, name, doc)       extern "C" type name;
diff a/src/hotspot/share/runtime/init.cpp b/src/hotspot/share/runtime/init.cpp
--- a/src/hotspot/share/runtime/init.cpp
+++ b/src/hotspot/share/runtime/init.cpp
@@ -111,21 +111,21 @@
   bytecodes_init();
   classLoader_init1();
   compilationPolicy_init();
   codeCache_init();
   VM_Version_init();
+  VMRegImpl::set_regName();  // need this before generate_stubs (for printing oop maps).
   stubRoutines_init1();
   jint status = universe_init();  // dependent on codeCache_init and
                                   // stubRoutines_init1 and metaspace_init.
   if (status != JNI_OK)
     return status;
 
   gc_barrier_stubs_init();  // depends on universe_init, must be before interpreter_init
   interpreter_init_stub();  // before methods get loaded
   accessFlags_init();
   InterfaceSupport_init();
-  VMRegImpl::set_regName(); // need this before generate_stubs (for printing oop maps).
   SharedRuntime::generate_stubs();
   universe2_init();  // dependent on codeCache_init and stubRoutines_init1
   javaClasses_init();// must happen after vtable initialization, before referenceProcessor_init
   interpreter_init_code();  // after javaClasses_init and before any method gets linked
   invocationCounter_init(); // after javaClasses_init and before any method gets linked
diff a/src/hotspot/share/runtime/thread.cpp b/src/hotspot/share/runtime/thread.cpp
--- a/src/hotspot/share/runtime/thread.cpp
+++ b/src/hotspot/share/runtime/thread.cpp
@@ -56,10 +56,11 @@
 #include "oops/instanceKlass.hpp"
 #include "oops/objArrayOop.hpp"
 #include "oops/oop.inline.hpp"
 #include "oops/symbol.hpp"
 #include "oops/typeArrayOop.inline.hpp"
+#include "oops/valueKlass.hpp"
 #include "oops/verifyOopClosure.hpp"
 #include "prims/jvm_misc.hpp"
 #include "prims/jvmtiExport.hpp"
 #include "prims/jvmtiThreadState.hpp"
 #include "runtime/arguments.hpp"
@@ -1641,10 +1642,11 @@
   set_entry_point(NULL);
   set_jni_functions(jni_functions());
   set_callee_target(NULL);
   set_vm_result(NULL);
   set_vm_result_2(NULL);
+  set_return_buffered_value(NULL);
   set_vframe_array_head(NULL);
   set_vframe_array_last(NULL);
   set_deferred_locals(NULL);
   set_deopt_mark(NULL);
   set_deopt_compiled_method(NULL);
@@ -2840,10 +2842,13 @@
 }
 
 void JavaThread::frames_do(void f(frame*, const RegisterMap* map)) {
   // ignore is there is no stack
   if (!has_last_Java_frame()) return;
+  // Because this method is used to verify oops, it must support
+  // oops in buffered values
+
   // traverse the stack frames. Starts from top frame.
   for (StackFrameStream fst(this); !fst.is_done(); fst.next()) {
     frame* fr = fst.current();
     f(fr, fst.register_map());
   }
diff a/src/hotspot/share/runtime/thread.hpp b/src/hotspot/share/runtime/thread.hpp
--- a/src/hotspot/share/runtime/thread.hpp
+++ b/src/hotspot/share/runtime/thread.hpp
@@ -443,10 +443,11 @@
  public:
   enum {
     is_definitely_current_thread = true
   };
 
+ public:
   // Constructor
   Thread();
   virtual ~Thread() = 0;        // Thread is abstract.
 
   // Manage Thread::current()
@@ -1015,10 +1016,11 @@
 
 class JavaThread: public Thread {
   friend class VMStructs;
   friend class JVMCIVMStructs;
   friend class WhiteBox;
+  friend class VTBuffer;
   friend class ThreadsSMRSupport; // to access _threadObj for exiting_threads_oops_do
  private:
   bool           _on_thread_list;                // Is set when this JavaThread is added to the Threads list
   oop            _threadObj;                     // The Java level thread object
 
@@ -1073,10 +1075,11 @@
   Method*       _callee_target;
 
   // Used to pass back results to the interpreter or generated code running Java code.
   oop           _vm_result;    // oop result is GC-preserved
   Metadata*     _vm_result_2;  // non-oop result
+  oop           _return_buffered_value; // buffered value being returned
 
   // See ReduceInitialCardMarks: this holds the precise space interval of
   // the most recent slow path allocation for which compiled code has
   // elided card-marks for performance along the fast-path.
   MemRegion     _deferred_card_mark;
@@ -1553,10 +1556,13 @@
   void set_vm_result  (oop x)                    { _vm_result   = x; }
 
   Metadata*    vm_result_2() const               { return _vm_result_2; }
   void set_vm_result_2  (Metadata* x)          { _vm_result_2   = x; }
 
+  oop return_buffered_value() const              { return _return_buffered_value; }
+  void set_return_buffered_value(oop val)        { _return_buffered_value = val; }
+
   MemRegion deferred_card_mark() const           { return _deferred_card_mark; }
   void set_deferred_card_mark(MemRegion mr)      { _deferred_card_mark = mr;   }
 
 #if INCLUDE_JVMCI
   int  pending_deoptimization() const             { return _pending_deoptimization; }
@@ -1792,10 +1798,11 @@
     return byte_offset_of(JavaThread, _anchor);
   }
   static ByteSize callee_target_offset()         { return byte_offset_of(JavaThread, _callee_target); }
   static ByteSize vm_result_offset()             { return byte_offset_of(JavaThread, _vm_result); }
   static ByteSize vm_result_2_offset()           { return byte_offset_of(JavaThread, _vm_result_2); }
+  static ByteSize return_buffered_value_offset() { return byte_offset_of(JavaThread, _return_buffered_value); }
   static ByteSize thread_state_offset()          { return byte_offset_of(JavaThread, _thread_state); }
   static ByteSize saved_exception_pc_offset()    { return byte_offset_of(JavaThread, _saved_exception_pc); }
   static ByteSize osthread_offset()              { return byte_offset_of(JavaThread, _osthread); }
 #if INCLUDE_JVMCI
   static ByteSize pending_deoptimization_offset() { return byte_offset_of(JavaThread, _pending_deoptimization); }
diff a/src/java.base/share/classes/java/io/ObjectStreamClass.java b/src/java.base/share/classes/java/io/ObjectStreamClass.java
--- a/src/java.base/share/classes/java/io/ObjectStreamClass.java
+++ b/src/java.base/share/classes/java/io/ObjectStreamClass.java
@@ -31,10 +31,11 @@
 import java.lang.ref.ReferenceQueue;
 import java.lang.ref.SoftReference;
 import java.lang.ref.WeakReference;
 import java.lang.reflect.Constructor;
 import java.lang.reflect.Field;
+import java.lang.reflect.InaccessibleObjectException;
 import java.lang.reflect.InvocationTargetException;
 import java.lang.reflect.RecordComponent;
 import java.lang.reflect.UndeclaredThrowableException;
 import java.lang.reflect.Member;
 import java.lang.reflect.Method;
@@ -490,10 +491,11 @@
     private ObjectStreamClass(final Class<?> cl) {
         this.cl = cl;
         name = cl.getName();
         isProxy = Proxy.isProxyClass(cl);
         isEnum = Enum.class.isAssignableFrom(cl);
+        boolean isInlineClass = cl.isInlineClass();
         isRecord = isRecord(cl);
         serializable = Serializable.class.isAssignableFrom(cl);
         externalizable = Externalizable.class.isAssignableFrom(cl);
 
         Class<?> superCl = cl.getSuperclass();
@@ -560,10 +562,12 @@
         }
 
         if (deserializeEx == null) {
             if (isEnum) {
                 deserializeEx = new ExceptionInfo(name, "enum type");
+            } else if (isInlineClass && writeReplaceMethod == null) {
+                deserializeEx = new ExceptionInfo(name, "inline class");
             } else if (cons == null && !isRecord) {
                 deserializeEx = new ExceptionInfo(name, "no valid constructor");
             }
         }
         if (isRecord && canonicalCtr == null) {
@@ -1556,11 +1560,11 @@
         try {
             Constructor<?> cons = cl.getDeclaredConstructor((Class<?>[]) null);
             cons.setAccessible(true);
             return ((cons.getModifiers() & Modifier.PUBLIC) != 0) ?
                 cons : null;
-        } catch (NoSuchMethodException ex) {
+        } catch (NoSuchMethodException | InaccessibleObjectException ex) {
             return null;
         }
     }
 
     /**
@@ -1887,12 +1891,14 @@
                 String[] ifaceNames = new String[interfaces.length];
                 for (int i = 0; i < interfaces.length; i++) {
                     ifaceNames[i] = interfaces[i].getName();
                 }
                 Arrays.sort(ifaceNames);
+                // Skip IdentityObject to keep the computed SVUID the same.
                 for (int i = 0; i < ifaceNames.length; i++) {
-                    dout.writeUTF(ifaceNames[i]);
+                    if (!"java.lang.IdentityObject".equals(ifaceNames[i]))
+                        dout.writeUTF(ifaceNames[i]);
                 }
             }
 
             Field[] fields = cl.getDeclaredFields();
             MemberSignature[] fieldSigs = new MemberSignature[fields.length];
diff a/src/java.base/share/classes/java/lang/System.java b/src/java.base/share/classes/java/lang/System.java
--- a/src/java.base/share/classes/java/lang/System.java
+++ b/src/java.base/share/classes/java/lang/System.java
@@ -2268,11 +2268,10 @@
             }
 
             public byte[] getBytesUTF8NoRepl(String s) {
                 return StringCoding.getBytesUTF8NoRepl(s);
             }
-
             public void setCause(Throwable t, Throwable cause) {
                 t.setCause(cause);
             }
 
             public ProtectionDomain protectionDomain(Class<?> c) {
diff a/src/java.base/share/classes/java/lang/invoke/MethodHandles.java b/src/java.base/share/classes/java/lang/invoke/MethodHandles.java
--- a/src/java.base/share/classes/java/lang/invoke/MethodHandles.java
+++ b/src/java.base/share/classes/java/lang/invoke/MethodHandles.java
@@ -2329,10 +2329,16 @@
          *                              <a href="MethodHandles.Lookup.html#secmgr">refuses access</a>
          * @throws NullPointerException if any argument is null
          */
         public MethodHandle findStatic(Class<?> refc, String name, MethodType type) throws NoSuchMethodException, IllegalAccessException {
             MemberName method = resolveOrFail(REF_invokeStatic, refc, name, type);
+            // resolveOrFail could return a non-static <init> method if present
+            // detect and throw NSME before producing a MethodHandle
+            if (!method.isStatic() && name.equals("<init>")) {
+                throw new NoSuchMethodException("illegal method name: " + name);
+            }
+
             return getDirectMethod(REF_invokeStatic, refc, method, findBoundCallerLookup(method));
         }
 
         /**
          * Produces a method handle for a virtual method.
@@ -2474,10 +2480,17 @@
   ProcessBuilder.class, methodType(void.class, String[].class));
 ProcessBuilder pb = (ProcessBuilder)
   MH_newProcessBuilder.invoke("x", "y", "z");
 assertEquals("[x, y, z]", pb.command().toString());
          * }</pre></blockquote>
+         *
+         * @apiNote
+         * This method does not find a static {@code <init>} factory method as it is invoked
+         * via {@code invokestatic} bytecode as opposed to {@code invokespecial} for an
+         * object constructor.  To look up static {@code <init>} factory method, use
+         * the {@link #findStatic(Class, String, MethodType) findStatic} method.
+         *
          * @param refc the class or interface from which the method is accessed
          * @param type the type of the method, with the receiver argument omitted, and a void return type
          * @return the desired method handle
          * @throws NoSuchMethodException if the constructor does not exist
          * @throws IllegalAccessException if access checking fails
@@ -2489,10 +2502,13 @@
          */
         public MethodHandle findConstructor(Class<?> refc, MethodType type) throws NoSuchMethodException, IllegalAccessException {
             if (refc.isArray()) {
                 throw new NoSuchMethodException("no constructor for array class: " + refc.getName());
             }
+            if (type.returnType() != void.class) {
+                throw new NoSuchMethodException("Constructors must have void return type: " + refc.getName());
+            }
             String name = "<init>";
             MemberName ctor = resolveOrFail(REF_newInvokeSpecial, refc, name, type);
             return getDirectConstructor(refc, ctor);
         }
 
@@ -3106,14 +3122,22 @@
          *                                is set and {@code asVarargsCollector} fails
          * @throws NullPointerException if the argument is null
          */
         public MethodHandle unreflectConstructor(Constructor<?> c) throws IllegalAccessException {
             MemberName ctor = new MemberName(c);
-            assert(ctor.isConstructor());
+            assert(ctor.isObjectConstructorOrStaticInitMethod());
             @SuppressWarnings("deprecation")
             Lookup lookup = c.isAccessible() ? IMPL_LOOKUP : this;
-            return lookup.getDirectConstructorNoSecurityManager(ctor.getDeclaringClass(), ctor);
+            if (ctor.isObjectConstructor()) {
+                assert(ctor.getReturnType() == void.class);
+                return lookup.getDirectConstructorNoSecurityManager(ctor.getDeclaringClass(), ctor);
+            } else {
+                // static init factory is a static method
+                assert(ctor.isMethod() && ctor.getReturnType() == ctor.getDeclaringClass() && ctor.getReferenceKind() == REF_invokeStatic);
+                assert(!MethodHandleNatives.isCallerSensitive(ctor));  // must not be caller-sensitive
+                return lookup.getDirectMethodNoSecurityManager(ctor.getReferenceKind(), ctor.getDeclaringClass(), ctor, lookup);
+            }
         }
 
         /**
          * Produces a method handle giving read access to a reflected field.
          * The type of the method handle will have a return type of the field's
@@ -3362,15 +3386,17 @@
             return caller == null || VerifyAccess.isClassAccessible(refc, caller, prevLookupClass, allowedModes);
         }
 
         /** Check name for an illegal leading "&lt;" character. */
         void checkMethodName(byte refKind, String name) throws NoSuchMethodException {
-            if (name.startsWith("<") && refKind != REF_newInvokeSpecial)
-                throw new NoSuchMethodException("illegal method name: "+name);
+            // "<init>" can only be invoked via invokespecial or it's a static init factory
+            if (name.startsWith("<") && refKind != REF_newInvokeSpecial &&
+                    !(refKind == REF_invokeStatic && name.equals("<init>"))) {
+                    throw new NoSuchMethodException("illegal method name: " + name);
+            }
         }
 
-
         /**
          * Find my trustable caller class if m is a caller sensitive method.
          * If this lookup object has full privilege access, then the caller class is the lookupClass.
          * Otherwise, if m is caller-sensitive, throw IllegalAccessException.
          */
@@ -3451,11 +3477,11 @@
         }
 
         void checkMethod(byte refKind, Class<?> refc, MemberName m) throws IllegalAccessException {
             boolean wantStatic = (refKind == REF_invokeStatic);
             String message;
-            if (m.isConstructor())
+            if (m.isObjectConstructor())
                 message = "expected a method, not a constructor";
             else if (!m.isMethod())
                 message = "expected a method";
             else if (wantStatic != m.isStatic())
                 message = wantStatic ? "expected a static method" : "expected a non-static method";
@@ -3750,11 +3776,11 @@
             return getDirectConstructorCommon(refc, ctor, checkSecurity);
         }
         /** Common code for all constructors; do not call directly except from immediately above. */
         private MethodHandle getDirectConstructorCommon(Class<?> refc, MemberName ctor,
                                                   boolean checkSecurity) throws IllegalAccessException {
-            assert(ctor.isConstructor());
+            assert(ctor.isObjectConstructor());
             checkAccess(REF_newInvokeSpecial, refc, ctor);
             // Optionally check with the security manager; this isn't needed for unreflect* calls.
             if (checkSecurity)
                 checkSecurityManager(refc, ctor);
             assert(!MethodHandleNatives.isCallerSensitive(ctor));  // maybeBindCaller not relevant here
@@ -3940,10 +3966,13 @@
      * @throws NullPointerException if the argument is null
      * @throws IllegalArgumentException if arrayClass is not an array type
      * @jvms 6.5 {@code aastore} Instruction
      */
     public static MethodHandle arrayElementSetter(Class<?> arrayClass) throws IllegalArgumentException {
+        if (arrayClass.isInlineClass()) {
+            throw new UnsupportedOperationException();
+        }
         return MethodHandleImpl.makeArrayElementAccessor(arrayClass, MethodHandleImpl.ArrayAccess.SET);
     }
 
     /**
      * Produces a VarHandle giving access to elements of an array of type
@@ -4700,11 +4729,17 @@
      * @see MethodHandles#explicitCastArguments
      * @since 9
      */
     public static MethodHandle zero(Class<?> type) {
         Objects.requireNonNull(type);
-        return type.isPrimitive() ?  zero(Wrapper.forPrimitiveType(type), type) : zero(Wrapper.OBJECT, type);
+        if (type.isPrimitive()) {
+            return zero(Wrapper.forPrimitiveType(type), type);
+        } else if (type.isInlineClass()) {
+            throw new UnsupportedOperationException();
+        } else {
+            return zero(Wrapper.OBJECT, type);
+        }
     }
 
     private static MethodHandle identityOrVoid(Class<?> type) {
         return type == void.class ? zero(type) : identity(type);
     }
@@ -4730,11 +4765,11 @@
         return dropArguments(zero(type.returnType()), 0, type.parameterList());
     }
 
     private static final MethodHandle[] IDENTITY_MHS = new MethodHandle[Wrapper.COUNT];
     private static MethodHandle makeIdentity(Class<?> ptype) {
-        MethodType mtype = methodType(ptype, ptype);
+        MethodType mtype = MethodType.methodType(ptype, ptype);
         LambdaForm lform = LambdaForm.identityForm(BasicType.basicType(ptype));
         return MethodHandleImpl.makeIntrinsic(mtype, lform, Intrinsic.IDENTITY);
     }
 
     private static MethodHandle zero(Wrapper btw, Class<?> rtype) {
diff a/src/java.base/share/classes/module-info.java b/src/java.base/share/classes/module-info.java
--- a/src/java.base/share/classes/module-info.java
+++ b/src/java.base/share/classes/module-info.java
@@ -127,11 +127,10 @@
     exports javax.security.auth.login;
     exports javax.security.auth.spi;
     exports javax.security.auth.x500;
     exports javax.security.cert;
 
-
     // additional qualified exports may be inserted at build time
     // see make/gensrc/GenModuleInfo.gmk
 
     exports sun.invoke.util to
         jdk.compiler,
@@ -338,11 +337,10 @@
         java.logging,
         java.prefs;
     exports sun.util.resources to
         jdk.localedata;
 
-
     // the service types defined by the APIs in this module
 
     uses java.lang.System.LoggerFinder;
     uses java.net.ContentHandlerFactory;
     uses java.net.spi.URLStreamHandlerProvider;
diff a/src/jdk.compiler/share/classes/com/sun/tools/javac/code/Flags.java b/src/jdk.compiler/share/classes/com/sun/tools/javac/code/Flags.java
--- a/src/jdk.compiler/share/classes/com/sun/tools/javac/code/Flags.java
+++ b/src/jdk.compiler/share/classes/com/sun/tools/javac/code/Flags.java
@@ -96,19 +96,23 @@
     public static final int ENUM         = 1<<14;
 
     /** Added in SE8, represents constructs implicitly declared in source. */
     public static final int MANDATED     = 1<<15;
 
+    /** Marks a type as a value-type */
+    public static final int VALUE        = 1<<16;
+
     public static final int StandardFlags = 0x0fff;
 
     // Because the following access flags are overloaded with other
     // bit positions, we translate them when reading and writing class
     // files into unique bits positions: ACC_SYNTHETIC <-> SYNTHETIC,
     // for example.
     public static final int ACC_SUPER    = 0x0020;
     public static final int ACC_BRIDGE   = 0x0040;
     public static final int ACC_VARARGS  = 0x0080;
+    public static final int ACC_VALUE    = 0x0100;
     public static final int ACC_MODULE   = 0x8000;
 
     /*****************************************
      * Internal compiler flags (no bits in the lower 16).
      *****************************************/
@@ -120,10 +124,27 @@
     /** Flag is set for a variable symbol if the variable's definition
      *  has an initializer part.
      */
     public static final int HASINIT          = 1<<18;
 
+    /** Flag is set for a class symbol if it defines one or more non-empty
+     *  instance initializer block(s). This is relevenat only for class symbols
+     *  that originate from source types. For binary types the instance initializer
+     *  blocks are "normalized" into the constructors.
+     */
+    public static final int HASINITBLOCK         = 1<<18;
+
+    /** Flag is set for a method symbol if it is an empty no-arg ctor.
+     *  i.e one that simply returns (jlO) or merely chains to a super's
+     *  EMPTYNOARGCONSTR
+     */
+    public static final int EMPTYNOARGCONSTR         = 1<<18;
+
+    /** Flag is set for a value based class.
+     */
+    public static final int VALUEBASED       = 1<<19;
+
     /** Flag is set for compiler-generated anonymous method symbols
      *  that `own' an initializer block.
      */
     public static final int BLOCK            = 1<<20;
 
@@ -218,12 +239,10 @@
     /**
      * Flag that marks a multi-catch parameter.
      */
     public static final long UNION = 1L<<39;
 
-    // Flag bit (1L << 40) is available.
-
     /**
      * Flag that marks an 'effectively final' local variable.
      */
     public static final long EFFECTIVELY_FINAL = 1L<<41;
 
@@ -368,30 +387,30 @@
 
     /** Modifier masks.
      */
     public static final int
         AccessFlags           = PUBLIC | PROTECTED | PRIVATE,
-        LocalClassFlags       = FINAL | ABSTRACT | STRICTFP | ENUM | SYNTHETIC,
+        LocalClassFlags       = FINAL | ABSTRACT | STRICTFP | ENUM | SYNTHETIC | VALUE,
         StaticLocalFlags      = LocalClassFlags | STATIC | INTERFACE | ANNOTATION,
         MemberClassFlags      = LocalClassFlags | INTERFACE | AccessFlags,
         MemberRecordFlags     = MemberClassFlags | STATIC,
         ClassFlags            = LocalClassFlags | INTERFACE | PUBLIC | ANNOTATION,
         InterfaceVarFlags     = FINAL | STATIC | PUBLIC,
-        VarFlags              = AccessFlags | FINAL | STATIC |
-                                VOLATILE | TRANSIENT | ENUM,
         ConstructorFlags      = AccessFlags,
         InterfaceMethodFlags  = ABSTRACT | PUBLIC,
         MethodFlags           = AccessFlags | ABSTRACT | STATIC | NATIVE |
                                 SYNCHRONIZED | FINAL | STRICTFP,
         RecordMethodFlags     = AccessFlags | ABSTRACT | STATIC |
                                 SYNCHRONIZED | FINAL | STRICTFP;
     public static final long
-        ExtendedStandardFlags       = (long)StandardFlags | DEFAULT,
+        ExtendedStandardFlags       = (long)StandardFlags | DEFAULT | VALUE,
         ModifierFlags               = ((long)StandardFlags & ~INTERFACE) | DEFAULT,
         InterfaceMethodMask         = ABSTRACT | PRIVATE | STATIC | PUBLIC | STRICTFP | DEFAULT,
         AnnotationTypeElementMask   = ABSTRACT | PUBLIC,
         LocalVarFlags               = FINAL | PARAMETER,
+        VarFlags              = AccessFlags | FINAL | STATIC |
+                                VOLATILE | TRANSIENT | ENUM,
         ReceiverParamFlags          = PARAMETER;
 
 
     public static Set<Modifier> asModifierSet(long flags) {
         Set<Modifier> modifiers = modifierSets.get(flags);
@@ -408,10 +427,11 @@
             if (0 != (flags & SYNCHRONIZED))
                                           modifiers.add(Modifier.SYNCHRONIZED);
             if (0 != (flags & NATIVE))    modifiers.add(Modifier.NATIVE);
             if (0 != (flags & STRICTFP))  modifiers.add(Modifier.STRICTFP);
             if (0 != (flags & DEFAULT))   modifiers.add(Modifier.DEFAULT);
+            if (0 != (flags & VALUE))     modifiers.add(Modifier.VALUE);
             modifiers = Collections.unmodifiableSet(modifiers);
             modifierSets.put(flags, modifiers);
         }
         return modifiers;
     }
@@ -449,13 +469,16 @@
         BRIDGE(Flags.BRIDGE),
         SYNTHETIC(Flags.SYNTHETIC),
         ANNOTATION(Flags.ANNOTATION),
         DEPRECATED(Flags.DEPRECATED),
         HASINIT(Flags.HASINIT),
+        HASINITBLOCK(Flags.HASINITBLOCK),
+        EMPTYNOARGCONSTR(Flags.EMPTYNOARGCONSTR),
         BLOCK(Flags.BLOCK),
         ENUM(Flags.ENUM),
         MANDATED(Flags.MANDATED),
+        INLINE(Flags.VALUE),
         NOOUTERTHIS(Flags.NOOUTERTHIS),
         EXISTS(Flags.EXISTS),
         COMPOUND(Flags.COMPOUND),
         CLASS_SEEN(Flags.CLASS_SEEN),
         SOURCE_SEEN(Flags.SOURCE_SEEN),
@@ -485,10 +508,11 @@
         DEPRECATED_ANNOTATION(Flags.DEPRECATED_ANNOTATION),
         DEPRECATED_REMOVAL(Flags.DEPRECATED_REMOVAL),
         HAS_RESOURCE(Flags.HAS_RESOURCE),
         POTENTIALLY_AMBIGUOUS(Flags.POTENTIALLY_AMBIGUOUS),
         ANONCONSTR_BASED(Flags.ANONCONSTR_BASED),
+        VALUEBASED(Flags.VALUEBASED),
         NAME_FILLED(Flags.NAME_FILLED),
         PREVIEW_API(Flags.PREVIEW_API),
         PREVIEW_ESSENTIAL_API(Flags.PREVIEW_ESSENTIAL_API),
         MATCH_BINDING(Flags.MATCH_BINDING),
         MATCH_BINDING_TO_OUTER(Flags.MATCH_BINDING_TO_OUTER),
diff a/src/jdk.compiler/share/classes/com/sun/tools/javac/code/Symbol.java b/src/jdk.compiler/share/classes/com/sun/tools/javac/code/Symbol.java
--- a/src/jdk.compiler/share/classes/com/sun/tools/javac/code/Symbol.java
+++ b/src/jdk.compiler/share/classes/com/sun/tools/javac/code/Symbol.java
@@ -412,10 +412,37 @@
 
     public boolean isPrivate() {
         return (flags_field & Flags.AccessFlags) == PRIVATE;
     }
 
+    public boolean isValue() {
+        return (flags() & VALUE) != 0;
+    }
+
+    /**
+     * Is this a *derived* reference projection symbol ??
+     */
+    public boolean isReferenceProjection() {
+        return false;
+    }
+
+    /**
+     * Return the value projection IFF 'this' happens to be derived reference projection, null
+     * otherwise.
+     */
+    public Symbol valueProjection() {
+        return null;
+    }
+
+    /**
+     * Return the reference projection IFF 'this' happens to be value projection, null
+     * otherwise.
+     */
+    public Symbol referenceProjection() {
+        return null;
+    }
+
     public boolean isPublic() {
         return (flags_field & Flags.AccessFlags) == PUBLIC;
     }
 
     public boolean isEnum() {
@@ -445,11 +472,17 @@
     }
 
     /** Is this symbol a constructor?
      */
     public boolean isConstructor() {
-        return name == name.table.names.init;
+        return name == name.table.names.init && (flags() & STATIC) == 0;
+    }
+
+    /** Is this symbol a value factory?
+     */
+    public boolean isValueFactory() {
+        return ((name == name.table.names.init && this.type.getReturnType().tsym == this.owner));
     }
 
     public boolean isDynamic() {
         return false;
     }
@@ -508,19 +541,21 @@
         }
         return (ClassSymbol)c;
     }
 
     /** The outermost class which indirectly owns this symbol.
+     * 'outermost' being a lexical construct, should transcend
+     *  projections
      */
     public ClassSymbol outermostClass() {
         Symbol sym = this;
         Symbol prev = null;
         while (sym.kind != PCK) {
             prev = sym;
             sym = sym.owner;
         }
-        return (ClassSymbol) prev;
+        return (ClassSymbol) (prev!= null && prev.isReferenceProjection() ? prev.valueProjection() : prev);
     }
 
     /** The package which indirectly owns this symbol.
      */
     public PackageSymbol packge() {
@@ -1283,10 +1318,16 @@
         /* the list of any of record components, only non empty if the class is a record
          * and it has at least one record component
          */
         private List<RecordComponent> recordComponents = List.nil();
 
+        /* the 'other' projection: If 'this' is an inline class then 'projection' is its reference projection
+           and vice versa.
+         */
+        public ClassSymbol projection;
+
+
         public ClassSymbol(long flags, Name name, Type type, Symbol owner) {
             super(TYP, flags, name, type, owner);
             this.members_field = null;
             this.fullname = formFullName(name, owner);
             this.flatname = formFlatName(name, owner);
@@ -1342,11 +1383,11 @@
 
         public String className() {
             if (name.isEmpty())
                 return
                     Log.getLocalizedString("anonymous.class", flatname);
-            else
+
                 return fullname.toString();
         }
 
         @DefinedBy(Api.LANGUAGE_MODEL)
         public Name getQualifiedName() {
@@ -1599,10 +1640,65 @@
         }
 
         public boolean isRecord() {
             return (flags_field & RECORD) != 0;
         }
+
+        @Override
+        public boolean isReferenceProjection() {
+            return projection != null && projection.isValue();
+        }
+
+        @Override
+        public ClassSymbol valueProjection() {
+            return isReferenceProjection() ? projection : null;
+        }
+
+        @Override
+        public ClassSymbol referenceProjection() {
+            if (!isValue() || projection != null)
+                return projection;
+
+            ClassType ct = (ClassType) this.type;
+            ClassType projectedType = new ClassType(ct.getEnclosingType(), ct.typarams_field, null);
+            projectedType.allparams_field = ct.allparams_field;
+            projectedType.supertype_field = ct.supertype_field;
+
+            projectedType.interfaces_field = ct.interfaces_field;
+            projectedType.all_interfaces_field = ct.all_interfaces_field;
+            projectedType.projection = ct;
+            ct.projection = projectedType;
+
+            Name projectionName = this.name.append('$', this.name.table.names.ref);
+            long projectionFlags = (this.flags() & ~(VALUE | UNATTRIBUTED));
+
+            projection = new ClassSymbol(projectionFlags, projectionName, projectedType, this.owner);
+            projection.members_field = WriteableScope.create(projection);
+            for (Symbol s : this.members().getSymbols(s->(s.kind == MTH || s.kind == VAR), NON_RECURSIVE)) {
+                Symbol clone = null;
+                if (s.kind == MTH) {
+                    MethodSymbol valMethod = (MethodSymbol)s;
+                    MethodSymbol refMethod = valMethod.clone(projection);
+                    valMethod.projection = refMethod;
+                    refMethod.projection = valMethod;
+                    clone = refMethod;
+                } else if (s.kind == VAR) {
+                    VarSymbol valVar = (VarSymbol)s;
+                    VarSymbol refVar = valVar.clone(projection);
+                    valVar.projection = refVar;
+                    refVar.projection = valVar;
+                    clone = refVar;
+                }
+                projection.members_field.enter(clone);
+            }
+            projection.completer = Completer.NULL_COMPLETER;
+            projection.sourcefile = this.sourcefile;
+            projection.flatname = this.flatname.append('$', this.name.table.names.ref);
+            projection.projection = this;
+            projectedType.tsym = projection;
+            return projection;
+        }
     }
 
 
     /** A class for variable symbols
      */
@@ -1621,10 +1717,15 @@
          *  Code generation:
          *    If this is a local variable, its logical slot number.
          */
         public int adr = -1;
 
+        /* The 'other' projection: If 'this' is a field of an inline class, then 'projection' is the
+           its doppleganger in its referene projection class and vice versa.
+        */
+        public VarSymbol projection;
+
         /** Construct a variable symbol, given its flags, name, type and owner.
          */
         public VarSymbol(long flags, Name name, Type type, Symbol owner) {
             super(VAR, flags, name, type, owner);
         }
@@ -1653,10 +1754,11 @@
                 }
             };
             v.pos = pos;
             v.adr = adr;
             v.data = data;
+            v.projection = projection;
 //          System.out.println("clone " + v + " in " + newOwner);//DEBUG
             return v;
         }
 
         public String toString() {
@@ -1705,10 +1807,22 @@
                                       final JCVariableDecl variable)
         {
             setData((Callable<Object>)() -> attr.attribLazyConstantValue(env, variable, type));
         }
 
+        @Override
+        public VarSymbol referenceProjection() {
+            return this.owner.isValue() ?
+                    this.owner.referenceProjection() != null ? projection : null
+                               : null;
+        }
+
+        @Override
+        public VarSymbol valueProjection() {
+            return  projection != null ? projection.owner.isValue() ? projection : null: null;
+        }
+
         /**
          * The variable's constant value, if this is a constant.
          * Before the constant value is evaluated, it points to an
          * initializer environment.  If this is not a constant, it can
          * be used for other stuff.
@@ -1866,10 +1980,15 @@
          *  The value is null if none appeared in the method
          *  declaration.
          */
         public Attribute defaultValue = null;
 
+        /* The 'other' projection: If 'this' is a method of an inline class, then 'projection' is the
+           its doppleganger in its referene projection class and vice versa.
+        */
+        public MethodSymbol projection;
+
         /** Construct a method symbol, given its flags, name, type and owner.
          */
         public MethodSymbol(long flags, Name name, Type type, Symbol owner) {
             super(MTH, flags, name, type, owner);
             if (owner.type.hasTag(TYPEVAR)) Assert.error(owner + "." + name);
@@ -1888,10 +2007,11 @@
                 public Object poolKey(Types types) {
                     return new Pair<>(newOwner, baseSymbol());
                 }
             };
             m.code = code;
+            m.projection = projection;
             return m;
         }
 
         @Override @DefinedBy(Api.LANGUAGE_MODEL)
         public Set<Modifier> getModifiers() {
@@ -2036,10 +2156,20 @@
          */
         public boolean overrides(Symbol _other, TypeSymbol origin, Types types, boolean checkResult,
                                             boolean requireConcreteIfInherited) {
             if (isConstructor() || _other.kind != MTH) return false;
 
+
+            /* If any inline types are involved, ask the same question in the reference universe,
+               where the hierarchy is navigable
+            */
+            if (origin.isValue())
+                origin = (TypeSymbol) origin.referenceProjection();
+
+            if (this.owner.isValue())
+                return this.projection.overrides(_other, origin, types, checkResult, requireConcreteIfInherited);
+
             if (this == _other) return true;
             MethodSymbol other = (MethodSymbol)_other;
 
             // check for a direct implementation
             if (other.isOverridableIn((TypeSymbol)owner) &&
@@ -2090,10 +2220,19 @@
             }
         }
 
         @Override
         public boolean isInheritedIn(Symbol clazz, Types types) {
+
+            /* If any inline types are involved, ask the same question in the reference universe,
+               where the hierarchy is navigable
+            */
+            if (clazz.isValue())
+                clazz = clazz.referenceProjection();
+            if (this.owner.isValue())
+                return this.projection.isInheritedIn(clazz, types);
+
             switch ((int)(flags_field & Flags.AccessFlags)) {
                 case PUBLIC:
                     return !this.owner.isInterface() ||
                             clazz == owner ||
                             (flags_field & STATIC) == 0;
@@ -2104,10 +2243,22 @@
 
         public boolean isLambdaMethod() {
             return (flags() & LAMBDA_METHOD) == LAMBDA_METHOD;
         }
 
+        @Override
+        public MethodSymbol referenceProjection() {
+            return this.owner.isValue() ?
+                    this.owner.referenceProjection() != null ? projection : null
+                    : null;
+        }
+
+        @Override
+        public MethodSymbol valueProjection() {
+            return  projection != null ? projection.owner.isValue() ? projection : null : null;
+        }
+
         /** override this method to point to the original enclosing method if this method symbol represents a synthetic
          *  lambda method
          */
         public MethodSymbol originalEnclosingMethod() {
             return this;
@@ -2397,11 +2548,11 @@
             }
             accessCode = AccessCode.from(tag, opcode);
             return accessCode;
         }
 
-        /** Access codes for dereferencing, assignment,
+        /** Access codes for dereferencing, assignment, withfield
          *  and pre/post increment/decrement.
 
          *  All access codes for accesses to the current class are even.
          *  If a member of the superclass should be accessed instead (because
          *  access was via a qualified super), add one to the corresponding code
@@ -2417,11 +2568,12 @@
             ASSIGN(2, Tag.ASSIGN),
             PREINC(4, Tag.PREINC),
             PREDEC(6, Tag.PREDEC),
             POSTINC(8, Tag.POSTINC),
             POSTDEC(10, Tag.POSTDEC),
-            FIRSTASGOP(12, Tag.NO_TAG);
+            WITHFIELD(12, Tag.WITHFIELD),
+            FIRSTASGOP(14, Tag.NO_TAG);
 
             public final int code;
             public final Tag tag;
             public static final int numberOfAccessCodes = (lushrl - ishll + lxor + 2 - iadd) * 2 + FIRSTASGOP.code + 2;
 
@@ -2450,10 +2602,12 @@
                         return AccessCode.PREDEC.code;
                     case POSTINC:
                         return AccessCode.POSTINC.code;
                     case POSTDEC:
                         return AccessCode.POSTDEC.code;
+                    case WITHFIELD:
+                        return AccessCode.WITHFIELD.code;
                 }
                 if (iadd <= opcode && opcode <= lxor) {
                     return (opcode - iadd) * 2 + FIRSTASGOP.code;
                 } else if (opcode == string_add) {
                     return (lxor + 1 - iadd) * 2 + FIRSTASGOP.code;
diff a/src/jdk.compiler/share/classes/com/sun/tools/javac/comp/Attr.java b/src/jdk.compiler/share/classes/com/sun/tools/javac/comp/Attr.java
--- a/src/jdk.compiler/share/classes/com/sun/tools/javac/comp/Attr.java
+++ b/src/jdk.compiler/share/classes/com/sun/tools/javac/comp/Attr.java
@@ -25,11 +25,10 @@
 
 package com.sun.tools.javac.comp;
 
 import java.util.*;
 import java.util.function.BiConsumer;
-import java.util.stream.Collectors;
 
 import javax.lang.model.element.ElementKind;
 import javax.tools.JavaFileObject;
 
 import com.sun.source.tree.CaseTree;
@@ -164,15 +163,18 @@
         allowPoly = Feature.POLY.allowedInSource(source);
         allowTypeAnnos = Feature.TYPE_ANNOTATIONS.allowedInSource(source);
         allowLambda = Feature.LAMBDA.allowedInSource(source);
         allowDefaultMethods = Feature.DEFAULT_METHODS.allowedInSource(source);
         allowStaticInterfaceMethods = Feature.STATIC_INTERFACE_METHODS.allowedInSource(source);
+        allowInlineTypes = Feature.INLINE_TYPES.allowedInSource(source);
         allowReifiableTypesInInstanceof =
                 Feature.REIFIABLE_TYPES_INSTANCEOF.allowedInSource(source) &&
                 (!preview.isPreview(Feature.REIFIABLE_TYPES_INSTANCEOF) || preview.isEnabled());
         sourceName = source.name;
         useBeforeDeclarationWarning = options.isSet("useBeforeDeclarationWarning");
+        allowEmptyValues = options.isSet("allowEmptyValues");
+        allowValueMemberCycles = options.isSet("allowValueMemberCycles");
 
         statInfo = new ResultInfo(KindSelector.NIL, Type.noType);
         varAssignmentInfo = new ResultInfo(KindSelector.ASG, Type.noType);
         unknownExprInfo = new ResultInfo(KindSelector.VAL, Type.noType);
         methodAttrInfo = new MethodAttrInfo();
@@ -195,10 +197,14 @@
 
     /** Switch: support default methods ?
      */
     boolean allowDefaultMethods;
 
+    /** Switch: allow inline types?
+     */
+    boolean allowInlineTypes;
+
     /** Switch: static interface methods enabled?
      */
     boolean allowStaticInterfaceMethods;
 
     /** Switch: reifiable types in instanceof enabled?
@@ -209,10 +215,20 @@
      * Switch: warn about use of variable before declaration?
      * RFE: 6425594
      */
     boolean useBeforeDeclarationWarning;
 
+    /**
+     * Switch: Allow value types with no instance state?
+     */
+    boolean allowEmptyValues;
+
+    /**
+     * Switch: Allow value type member cycles?
+     */
+    boolean allowValueMemberCycles;
+
     /**
      * Switch: name of source level; used for error reporting.
      */
     String sourceName;
 
@@ -307,11 +323,21 @@
             if (v.isResourceVariable()) { //TWR resource
                 log.error(pos, Errors.TryResourceMayNotBeAssigned(v));
             } else if ((v.flags() & MATCH_BINDING) != 0) {
                 log.error(pos, Errors.PatternBindingMayNotBeAssigned(v));
             } else {
-                log.error(pos, Errors.CantAssignValToFinalVar(v));
+                boolean complain = true;
+                /* Allow updates to instance fields of value classes by any method in the same nest via the
+                   withfield operator -This does not result in mutation of final fields; the code generator
+                   would implement `copy on write' semantics via the opcode `withfield'.
+                */
+                if (env.info.inWithField && v.getKind() == ElementKind.FIELD && (v.flags() & STATIC) == 0 && types.isValue(v.owner.type)) {
+                    if (env.enclClass.sym.outermostClass() == v.owner.outermostClass())
+                        complain = false;
+                }
+                if (complain)
+                    log.error(pos, Errors.CantAssignValToFinalVar(v));
             }
         }
     }
 
     /** Does tree represent a static reference to an identifier?
@@ -799,13 +825,13 @@
         for (JCTypeParameter tvar : typarams) {
             TypeVar a = (TypeVar)tvar.type;
             a.tsym.flags_field |= UNATTRIBUTED;
             a.setUpperBound(Type.noType);
             if (!tvar.bounds.isEmpty()) {
-                List<Type> bounds = List.of(attribType(tvar.bounds.head, env));
+                List<Type> bounds = List.of(chk.checkRefType(tvar.bounds.head, attribType(tvar.bounds.head, env), false));
                 for (JCExpression bound : tvar.bounds.tail)
-                    bounds = bounds.prepend(attribType(bound, env));
+                    bounds = bounds.prepend(chk.checkRefType(bound, attribType(bound, env), false));
                 types.setBounds(a, bounds.reverse());
             } else {
                 // if no bounds are given, assume a single bound of
                 // java.lang.Object.
                 types.setBounds(a, List.of(syms.objectType));
@@ -962,10 +988,13 @@
                 // (This would be an illegal access to "this before super").
                 if (env.info.isSelfCall &&
                         env.tree.hasTag(NEWCLASS)) {
                     c.flags_field |= NOOUTERTHIS;
                 }
+                if (env.tree.hasTag(NEWCLASS) && types.isValue(c.getSuperclass())) {
+                    c.flags_field |= VALUE; // avoid further secondary errors.
+                }
                 attribClass(tree.pos(), c);
                 result = tree.type = c.type;
             }
         } finally {
             localCacheContext.ifPresent(LocalCacheContext::leave);
@@ -1183,11 +1212,11 @@
                 // super(...) or this(...) is given
                 // or we are compiling class java.lang.Object.
                 if (tree.name == names.init && owner.type != syms.objectType) {
                     JCBlock body = tree.body;
                     if (body.stats.isEmpty() ||
-                            TreeInfo.getConstructorInvocationName(body.stats, names) == names.empty) {
+                            TreeInfo.getConstructorInvocationName(body.stats, names, true) == names.empty) {
                         JCStatement supCall = make.at(body.pos).Exec(make.Apply(List.nil(),
                                 make.Ident(names._super), make.Idents(List.nil())));
                         body.stats = body.stats.prepend(supCall);
                     } else if ((env.enclClass.sym.flags() & ENUM) != 0 &&
                             (tree.mods.flags & GENERATEDCONSTR) == 0 &&
@@ -1214,10 +1243,16 @@
                                             Fragments.ThrowsClauseNotAllowedForCanonicalConstructor(
                                                     TreeInfo.isCompactConstructor(tree) ? Fragments.Compact : Fragments.Canonical)));
                         }
                     }
                 }
+                if (m.isConstructor() && m.type.getParameterTypes().size() == 0) {
+                    if ((owner.type == syms.objectType) ||
+                            (tree.body.stats.size() == 1 && TreeInfo.getConstructorInvocationName(tree.body.stats, names, false) == names._super)) {
+                        m.flags_field |= EMPTYNOARGCONSTR;
+                    }
+                }
 
                 // Attribute all type annotations in the body
                 annotate.queueScanTreeAndTypeAnnotate(tree.body, localEnv, m, null);
                 annotate.flush();
 
@@ -1283,12 +1318,15 @@
         try {
             v.getConstValue(); // ensure compile-time constant initializer is evaluated
             deferredLintHandler.flush(tree.pos());
             chk.checkDeprecatedAnnotation(tree.pos(), v);
 
+            /* Don't want constant propagation/folding for instance fields of value classes,
+               as these can undergo updates via copy on write.
+            */
             if (tree.init != null) {
-                if ((v.flags_field & FINAL) == 0 ||
+                if ((v.flags_field & FINAL) == 0 || ((v.flags_field & STATIC) == 0 && types.isValue(v.owner.type)) ||
                     !memberEnter.needsLazyConstValue(tree.init)) {
                     // Not a compile-time constant
                     // Attribute initializer in a new environment
                     // with the declared variable as owner.
                     // Check that initializer conforms to variable's declared type.
@@ -1408,11 +1446,15 @@
                     env.info.scope.owner.flags() & STRICTFP, names.empty, null,
                     env.info.scope.owner);
             final Env<AttrContext> localEnv =
                 env.dup(tree, env.info.dup(env.info.scope.dupUnshared(fakeOwner)));
 
-            if ((tree.flags & STATIC) != 0) localEnv.info.staticLevel++;
+            if ((tree.flags & STATIC) != 0)
+                localEnv.info.staticLevel++;
+            else if (tree.stats.size() > 0)
+                env.info.scope.owner.flags_field |= HASINITBLOCK;
+
             // Attribute all type annotations in the block
             annotate.queueScanTreeAndTypeAnnotate(tree, localEnv, localEnv.info.scope.owner, null);
             annotate.flush();
             attribStats(tree.stats, localEnv);
 
@@ -1473,10 +1515,43 @@
     private boolean breaksOutOf(JCTree loop, JCTree body) {
         preFlow(body);
         return flow.breaksOutOf(env, loop, body, make);
     }
 
+    public void visitWithField(JCWithField tree) {
+        boolean inWithField = env.info.inWithField;
+        try {
+            env.info.inWithField = true;
+            Type fieldtype = attribTree(tree.field, env.dup(tree), varAssignmentInfo);
+            attribExpr(tree.value, env, fieldtype);
+            Type capturedType = syms.errType;
+            if (tree.field.type != null && !tree.field.type.isErroneous()) {
+                final Symbol sym = TreeInfo.symbol(tree.field);
+                if (sym == null || sym.kind != VAR || sym.owner.kind != TYP ||
+                        (sym.flags() & STATIC) != 0 || !types.isValue(sym.owner.type)) {
+                    log.error(tree.field.pos(), Errors.ValueInstanceFieldExpectedHere);
+                } else {
+                    Type ownType = sym.owner.type;
+                    switch(tree.field.getTag()) {
+                        case IDENT:
+                            JCIdent ident = (JCIdent) tree.field;
+                            ownType = ident.sym.owner.type;
+                            break;
+                        case SELECT:
+                            JCFieldAccess fieldAccess = (JCFieldAccess) tree.field;
+                            ownType = fieldAccess.selected.type;
+                            break;
+                    }
+                    capturedType = capture(ownType);
+                }
+            }
+            result = check(tree, capturedType, KindSelector.VAL, resultInfo);
+        } finally {
+            env.info.inWithField = inWithField;
+        }
+    }
+
     public void visitForLoop(JCForLoop tree) {
         Env<AttrContext> loopEnv =
             env.dup(env.tree, env.info.dup(env.info.scope.dup()));
         MatchBindings condBindings = MatchBindingsComputer.EMPTY;
         try {
@@ -1516,11 +1591,11 @@
             Type exprType = types.cvarUpperBound(attribExpr(tree.expr, loopEnv));
             chk.checkNonVoid(tree.pos(), exprType);
             Type elemtype = types.elemtype(exprType); // perhaps expr is an array?
             if (elemtype == null) {
                 // or perhaps expr implements Iterable<T>?
-                Type base = types.asSuper(exprType, syms.iterableType.tsym);
+                Type base = types.asSuper(exprType, syms.iterableType.tsym, true);
                 if (base == null) {
                     log.error(tree.expr.pos(),
                               Errors.ForeachNotApplicableToType(exprType,
                                                                 Fragments.TypeReqArrayOrIterable));
                     elemtype = types.createErrorType(exprType);
@@ -1730,11 +1805,11 @@
         }
         return null;
     }
 
     public void visitSynchronized(JCSynchronized tree) {
-        chk.checkRefType(tree.pos(), attribExpr(tree.lock, env));
+        chk.checkRefType(tree.pos(), attribExpr(tree.lock, env), false);
         attribStat(tree.body, env);
         result = null;
     }
 
     public void visitTry(JCTry tree) {
@@ -1811,11 +1886,11 @@
         }
     }
 
     void checkAutoCloseable(DiagnosticPosition pos, Env<AttrContext> env, Type resource) {
         if (!resource.isErroneous() &&
-            types.asSuper(resource, syms.autoCloseableType.tsym) != null &&
+            types.asSuper(resource, syms.autoCloseableType.tsym, true) != null &&
             !types.isSameType(resource, syms.autoCloseableType)) { // Don't emit warning for AutoCloseable itself
             Symbol close = syms.noSymbol;
             Log.DiagnosticHandler discardHandler = new Log.DiscardDiagnosticHandler(log);
             try {
                 close = rs.resolveQualifiedMethod(pos,
@@ -2411,10 +2486,46 @@
             Symbol msym = TreeInfo.symbol(tree.meth);
             restype = adjustMethodReturnType(msym, qualifier, methName, argtypes, restype);
 
             chk.checkRefTypes(tree.typeargs, typeargtypes);
 
+            final Symbol symbol = TreeInfo.symbol(tree.meth);
+            if (symbol != null) {
+                /* Is this an ill conceived attempt to invoke jlO methods not available on value types ??
+                 */
+                boolean superCallOnValueReceiver = types.isValue(env.enclClass.sym.type)
+                        && (tree.meth.hasTag(SELECT))
+                        && ((JCFieldAccess)tree.meth).selected.hasTag(IDENT)
+                        && TreeInfo.name(((JCFieldAccess)tree.meth).selected) == names._super;
+                if (types.isValue(qualifier) || superCallOnValueReceiver) {
+                    int argSize = argtypes.size();
+                    Name name = symbol.name;
+                    switch (name.toString()) {
+                        case "wait":
+                            if (argSize == 0
+                                    || (types.isConvertible(argtypes.head, syms.longType) &&
+                                    (argSize == 1 || (argSize == 2 && types.isConvertible(argtypes.tail.head, syms.intType))))) {
+                                log.error(tree.pos(), Errors.ValueDoesNotSupport(name));
+                            }
+                            break;
+                        case "notify":
+                        case "notifyAll":
+                        case "clone":
+                        case "finalize":
+                            if (argSize == 0)
+                                log.error(tree.pos(), Errors.ValueDoesNotSupport(name));
+                            break;
+                        case "hashCode":
+                        case "equals":
+                        case "toString":
+                            if (superCallOnValueReceiver)
+                                log.error(tree.pos(), Errors.ValueDoesNotSupport(names.fromString("invocation of super." + name)));
+                            break;
+                    }
+                }
+            }
+
             // Check that value of resulting type is admissible in the
             // current context.  Also, capture the return type
             Type capturedRes = resultInfo.checkContext.inferenceContext().cachedCapture(tree, restype, true);
             result = check(tree, capturedRes, KindSelector.VAL, resultInfo);
         }
@@ -2425,12 +2536,21 @@
             if (msym != null &&
                     msym.owner == syms.objectType.tsym &&
                     methodName == names.getClass &&
                     argtypes.isEmpty()) {
                 // as a special case, x.getClass() has type Class<? extends |X|>
+                // Temporary treatment for inline class: Given an inline class V that implements
+                // I1, I2, ... In, v.getClass() is typed to be Class<? extends Object & I1 & I2 .. & In>
+                Type wcb;
+                if (qualifierType.isValue()) {
+                    List<Type> bounds = List.of(syms.objectType).appendList(((ClassSymbol) qualifierType.tsym).getInterfaces());
+                    wcb = types.makeIntersectionType(bounds);
+                } else {
+                    wcb = types.erasure(qualifierType);
+                }
                 return new ClassType(restype.getEnclosingType(),
-                        List.of(new WildcardType(types.erasure(qualifierType),
+                        List.of(new WildcardType(wcb,
                                 BoundKind.EXTENDS,
                                 syms.boundClass)),
                         restype.tsym,
                         restype.getMetadata());
             } else if (msym != null &&
@@ -2770,10 +2890,11 @@
                     }
                     // For <>(){}, inferred types must also be accessible.
                     for (Type t : clazztype.getTypeArguments()) {
                         rs.checkAccessibleType(env, t);
                     }
+                    chk.checkParameterizationWithValues(tree, clazztype);
                 }
 
                 // If we already errored, be careful to avoid a further avalanche. ErrorType answers
                 // false for isInterface call even when the original type is an interface.
                 boolean implementing = clazztype.tsym.isInterface() ||
@@ -2842,10 +2963,13 @@
      */
     public JCExpression makeNullCheck(JCExpression arg) {
         // optimization: new Outer() can never be null; skip null check
         if (arg.getTag() == NEWCLASS)
             return arg;
+        // Likewise arg can't be null if it is a value.
+        if (types.isValue(arg.type))
+            return arg;
         // optimization: X.this is never null; skip null check
         Name name = TreeInfo.name(arg);
         if (name == names._this || name == names._super) return arg;
 
         JCTree.Tag optag = NULLCHK;
@@ -3848,10 +3972,11 @@
             // comparisons will not have an acmp* opc at this point.
             if ((opc == ByteCodes.if_acmpeq || opc == ByteCodes.if_acmpne)) {
                 if (!types.isCastable(left, right, new Warner(tree.pos()))) {
                     log.error(tree.pos(), Errors.IncomparableTypes(left, right));
                 }
+                chk.checkForSuspectClassLiteralComparison(tree, left, right);
             }
 
             chk.checkDivZero(tree.rhs.pos(), operator, right);
         }
         result = check(tree, owntype, KindSelector.VAL, resultInfo);
@@ -4045,12 +4170,16 @@
 
     public void visitSelect(JCFieldAccess tree) {
         // Determine the expected kind of the qualifier expression.
         KindSelector skind = KindSelector.NIL;
         if (tree.name == names._this || tree.name == names._super ||
-                tree.name == names._class)
+                tree.name == names._class || tree.name == names._default)
         {
+            if (tree.name == names._default && !allowInlineTypes) {
+                log.error(DiagnosticFlag.SOURCE_LEVEL, tree.pos(),
+                        Feature.INLINE_TYPES.error(sourceName));
+            }
             skind = KindSelector.TYP;
         } else {
             if (pkind().contains(KindSelector.PCK))
                 skind = KindSelector.of(skind, KindSelector.PCK);
             if (pkind().contains(KindSelector.TYP))
@@ -4068,21 +4197,27 @@
         if (skind == KindSelector.TYP) {
             Type elt = site;
             while (elt.hasTag(ARRAY))
                 elt = ((ArrayType)elt).elemtype;
             if (elt.hasTag(TYPEVAR)) {
-                log.error(tree.pos(), Errors.TypeVarCantBeDeref);
-                result = tree.type = types.createErrorType(tree.name, site.tsym, site);
-                tree.sym = tree.type.tsym;
-                return ;
+                if (tree.name == names._default) {
+                    result = check(tree, litType(BOT).constType(null),
+                            KindSelector.VAL, resultInfo);
+                } else {
+                    log.error(tree.pos(), Errors.TypeVarCantBeDeref);
+                    result = tree.type = types.createErrorType(tree.name, site.tsym, site);
+                    tree.sym = tree.type.tsym;
+                    return;
+                }
             }
         }
 
         // If qualifier symbol is a type or `super', assert `selectSuper'
         // for the selection. This is relevant for determining whether
         // protected symbols are accessible.
         Symbol sitesym = TreeInfo.symbol(tree.selected);
+
         boolean selectSuperPrev = env.info.selectSuper;
         env.info.selectSuper =
             sitesym != null &&
             sitesym.name == names._super;
 
@@ -4214,19 +4349,29 @@
                     return rs.resolveSelf(pos, env, site.tsym, name);
                 } else if (name == names._class) {
                     // In this case, we have already made sure in
                     // visitSelect that qualifier expression is a type.
                     return syms.getClassField(site, types);
+                } else if (name == names._default) {
+                    return new VarSymbol(STATIC, names._default, site, site.tsym);
+                } else if (name == names.ref && site.isValue() && resultInfo.pkind.contains(KindSelector.TYP)) {
+                    return site.tsym.referenceProjection();
+                } else if (name == names.val && site.isValue() && resultInfo.pkind.contains(KindSelector.TYP)) {
+                    return site.tsym;
                 } else {
                     // We are seeing a plain identifier as selector.
                     Symbol sym = rs.findIdentInType(pos, env, site, name, resultInfo.pkind);
                         sym = rs.accessBase(sym, pos, location, site, name, true);
                     return sym;
                 }
             case WILDCARD:
                 throw new AssertionError(tree);
             case TYPEVAR:
+                if (name == names._default) {
+                    // Be sure to return the default value before examining bounds
+                    return new VarSymbol(STATIC, names._default, site, site.tsym);
+                }
                 // Normally, site.getUpperBound() shouldn't be null.
                 // It should only happen during memberEnter/attribBase
                 // when determining the super type which *must* be
                 // done before attributing the type variables.  In
                 // other words, we are seeing this illegal program:
@@ -4247,15 +4392,17 @@
             case ERROR:
                 // preserve identifier names through errors
                 return types.createErrorType(name, site.tsym, site).tsym;
             default:
                 // The qualifier expression is of a primitive type -- only
-                // .class is allowed for these.
+                // .class and .default is allowed for these.
                 if (name == names._class) {
                     // In this case, we have already made sure in Select that
                     // qualifier expression is a type.
                     return syms.getClassField(site, types);
+                } else if (name == names._default) {
+                    return new VarSymbol(STATIC, names._default, site, site.tsym);
                 } else {
                     log.error(pos, Errors.CantDeref(site));
                     return syms.errSymbol;
                 }
             }
@@ -4863,11 +5010,11 @@
             } else {
                 extending = null;
                 implementing = bounds;
             }
             JCClassDecl cd = make.at(tree).ClassDef(
-                make.Modifiers(PUBLIC | ABSTRACT),
+                make.Modifiers(PUBLIC | ABSTRACT | (extending != null && TreeInfo.symbol(extending).isValue() ? VALUE : 0)),
                 names.empty, List.nil(),
                 extending, implementing, List.nil());
 
             ClassSymbol c = (ClassSymbol)owntype.tsym;
             Assert.check((c.flags() & COMPOUND) != 0);
@@ -4886,11 +5033,11 @@
     public void visitWildcard(JCWildcard tree) {
         //- System.err.println("visitWildcard("+tree+");");//DEBUG
         Type type = (tree.kind.kind == BoundKind.UNBOUND)
             ? syms.objectType
             : attribType(tree.inner, env);
-        result = check(tree, new WildcardType(chk.checkRefType(tree.pos(), type),
+        result = check(tree, new WildcardType(chk.checkRefType(tree.pos(), type, false),
                                               tree.kind.kind,
                                               syms.boundClass),
                 KindSelector.TYP, resultInfo);
     }
 
@@ -4990,10 +5137,17 @@
      */
     public void attribClass(DiagnosticPosition pos, ClassSymbol c) {
         try {
             annotate.flush();
             attribClass(c);
+            if (types.isValue(c.type)) {
+                final Env<AttrContext> env = typeEnvs.get(c);
+                if (!allowValueMemberCycles) {
+                    if (env != null && env.tree != null && env.tree.hasTag(CLASSDEF))
+                        chk.checkNonCyclicMembership((JCClassDecl)env.tree);
+                }
+            }
         } catch (CompletionFailure ex) {
             chk.completionError(pos, ex);
         }
     }
 
@@ -5062,10 +5216,18 @@
                     env.info.isSerializable = true;
                 }
 
                 attribClassBody(env, c);
 
+                if ((c.flags() & (VALUE | ABSTRACT)) == VALUE) { // for non-intersection, concrete values.
+                    Assert.check(env.tree.hasTag(CLASSDEF));
+                    JCClassDecl classDecl = (JCClassDecl) env.tree;
+                    if (classDecl.extending != null) {
+                        chk.checkConstraintsOfInlineSuper(env.tree.pos(), c);
+                    }
+                }
+
                 chk.checkDeprecatedAnnotation(env.tree.pos(), c);
                 chk.checkClassOverrideEqualsAndHashIfNeeded(env.tree.pos(), c);
                 chk.checkFunctionalInterface((JCClassDecl) env.tree, c);
                 chk.checkLeaksNotAccessible(env, (JCClassDecl) env.tree);
             } finally {
@@ -5167,13 +5329,18 @@
         chk.checkImplementations(tree);
 
         //check that a resource implementing AutoCloseable cannot throw InterruptedException
         checkAutoCloseable(tree.pos(), env, c.type);
 
+        boolean hasInstanceFields = false;
         for (List<JCTree> l = tree.defs; l.nonEmpty(); l = l.tail) {
             // Attribute declaration
             attribStat(l.head, env);
+
+            if (l.head.hasTag(VARDEF) && (TreeInfo.flags(l.head) & STATIC) == 0)
+                hasInstanceFields = true;
+
             // Check that declarations in inner classes are not static (JLS 8.1.2)
             // Make an exception for static constants.
             if (c.owner.kind != PCK &&
                 ((c.flags() & STATIC) == 0 || c.name == names.empty) &&
                 (TreeInfo.flags(l.head) & (STATIC | INTERFACE)) != 0) {
@@ -5183,10 +5350,13 @@
                     sym.kind != VAR ||
                     ((VarSymbol) sym).getConstValue() == null)
                     log.error(l.head.pos(), Errors.IclsCantHaveStaticDecl(c));
             }
         }
+        if (!allowEmptyValues && !hasInstanceFields && (c.flags() & (VALUE | SYNTHETIC)) == VALUE) {
+            log.error(tree.pos(), Errors.EmptyValueNotYet);
+        }
 
         // Check for cycles among non-initial constructors.
         chk.checkCyclicConstructors(tree);
 
         // Check for cycles among annotation elements.
diff a/src/jdk.compiler/share/classes/com/sun/tools/javac/comp/Check.java b/src/jdk.compiler/share/classes/com/sun/tools/javac/comp/Check.java
--- a/src/jdk.compiler/share/classes/com/sun/tools/javac/comp/Check.java
+++ b/src/jdk.compiler/share/classes/com/sun/tools/javac/comp/Check.java
@@ -93,10 +93,11 @@
     private final Source source;
     private final Target target;
     private final Profile profile;
     private final Preview preview;
     private final boolean warnOnAnyAccessToMembers;
+    private final boolean allowValueBasedClasses;
 
     // The set of lint options currently in effect. It is initialized
     // from the context, and then is set/reset as needed by Attr as it
     // visits all the various parts of the trees during attribution.
     private Lint lint;
@@ -133,11 +134,11 @@
         fileManager = context.get(JavaFileManager.class);
 
         source = Source.instance(context);
         target = Target.instance(context);
         warnOnAnyAccessToMembers = options.isSet("warnOnAccessToMembers");
-
+        allowValueBasedClasses = options.isSet("allowValueBasedClasses");
         Target target = Target.instance(context);
         syntheticNameChar = target.syntheticNameChar();
 
         profile = Profile.instance(context);
         preview = Preview.instance(context);
@@ -488,11 +489,11 @@
 
     public void removeCompiled(ClassSymbol csym) {
         compiled.remove(Pair.of(csym.packge().modle, csym.flatname));
     }
 
-/* *************************************************************************
+    /* *************************************************************************
  * Type Checking
  **************************************************************************/
 
     /**
      * A check context is an object that can be used to perform compatibility
@@ -594,16 +595,24 @@
     Type checkType(final DiagnosticPosition pos, final Type found, final Type req, final CheckContext checkContext) {
         final InferenceContext inferenceContext = checkContext.inferenceContext();
         if (inferenceContext.free(req) || inferenceContext.free(found)) {
             inferenceContext.addFreeTypeListener(List.of(req, found),
                     solvedContext -> checkType(pos, solvedContext.asInstType(found), solvedContext.asInstType(req), checkContext));
+        } else {
+            if (found.hasTag(CLASS)) {
+                if (inferenceContext != infer.emptyContext)
+                    checkParameterizationWithValues(pos, found);
+            }
         }
         if (req.hasTag(ERROR))
             return req;
         if (req.hasTag(NONE))
             return found;
         if (checkContext.compatible(found, req, checkContext.checkWarner(pos, found, req))) {
+            if (found.hasTag(BOT) && types.isValueBased(req)) {
+                log.warning(pos, Warnings.SuspiciousMixOfNullWithValueBasedClass(req));
+            }
             return found;
         } else {
             if (found.isNumeric() && req.isNumeric()) {
                 checkContext.report(pos, diags.fragment(Fragments.PossibleLossOfPrecision(found, req)));
                 return types.createErrorType(found);
@@ -622,10 +631,17 @@
     Type checkCastable(DiagnosticPosition pos, Type found, Type req) {
         return checkCastable(pos, found, req, basicHandler);
     }
     Type checkCastable(DiagnosticPosition pos, Type found, Type req, CheckContext checkContext) {
         if (types.isCastable(found, req, castWarner(pos, found, req))) {
+            if (types.isValueBased(req)) {
+                if (found.hasTag(BOT)) {
+                    log.warning(pos, Warnings.SuspiciousMixOfNullWithValueBasedClass(req));
+                } else if (!types.isValueBased(found)) {
+                    log.warning(pos, Warnings.PotentialNullPollution(found));
+                }
+            }
             return req;
         } else {
             checkContext.report(pos, diags.fragment(Fragments.InconvertibleTypes(found, req)));
             return types.createErrorType(found);
         }
@@ -724,10 +740,53 @@
             return (t.hasTag(TYPEVAR))
                                     ? diags.fragment(Fragments.TypeParameter(t))
                                     : t;
         }
 
+    void checkConstraintsOfInlineSuper(DiagnosticPosition pos, ClassSymbol c) {
+        for(Type st = types.supertype(c.type); st != Type.noType; st = types.supertype(st)) {
+            if (st == null || st.tsym == null || st.tsym.kind == ERR)
+                return;
+            if  (st.tsym == syms.objectType.tsym)
+                return;
+            if (!st.tsym.isAbstract()) {
+                log.error(pos, Errors.ConcreteSupertypeForInlineClass(c, st));
+            }
+            if ((st.tsym.flags() & HASINITBLOCK) != 0) {
+                log.error(pos, Errors.SuperClassDeclaresInitBlock(c, st));
+            }
+            // No instance fields and no arged constructors both mean inner classes cannot be inline supers.
+            Type encl = st.getEnclosingType();
+            if (encl != null && encl.hasTag(CLASS)) {
+                log.error(pos, Errors.SuperClassCannotBeInner(c, st));
+            }
+            for (Symbol s : st.tsym.members().getSymbols(NON_RECURSIVE)) {
+                switch (s.kind) {
+                case VAR:
+                    if ((s.flags() & STATIC) == 0) {
+                        log.error(pos, Errors.SuperFieldNotAllowed(s, c, st));
+                    }
+                    break;
+                case MTH:
+                    if ((s.flags() & SYNCHRONIZED) != 0) {
+                        log.error(pos, Errors.SuperMethodCannotBeSynchronized(s, c, st));
+                    } else if (s.isConstructor()) {
+                        MethodSymbol m = (MethodSymbol)s;
+                        if (m.getParameters().size() > 0) {
+                            log.error(pos, Errors.SuperConstructorCannotTakeArguments(m, c, st));
+                        } else {
+                            if ((m.flags() & (GENERATEDCONSTR | EMPTYNOARGCONSTR)) == 0) {
+                                log.error(pos, Errors.SuperNoArgConstructorMustBeEmpty(m, c, st));
+                            }
+                        }
+                    }
+                    break;
+                }
+            }
+        }
+    }
+
     /** Check that type is a valid qualifier for a constructor reference expression
      */
     Type checkConstructorRefType(DiagnosticPosition pos, Type t) {
         t = checkClassOrArrayType(pos, t);
         if (t.hasTag(CLASS)) {
@@ -771,29 +830,39 @@
 
     /** Check that type is a reference type, i.e. a class, interface or array type
      *  or a type variable.
      *  @param pos           Position to be used for error reporting.
      *  @param t             The type to be checked.
+     *  @param valueOK       If false, a value class does not qualify
      */
-    Type checkRefType(DiagnosticPosition pos, Type t) {
-        if (t.isReference())
+    Type checkRefType(DiagnosticPosition pos, Type t, boolean valueOK) {
+        if (t.isReference() && (valueOK || !types.isValue(t)))
             return t;
         else
             return typeTagError(pos,
                                 diags.fragment(Fragments.TypeReqRef),
                                 t);
     }
 
+    /** Check that type is a reference type, i.e. a class, interface or array type
+     *  or a type variable.
+     *  @param pos           Position to be used for error reporting.
+     *  @param t             The type to be checked.
+     */
+    Type checkRefType(DiagnosticPosition pos, Type t) {
+        return checkRefType(pos, t, true);
+    }
+
     /** Check that each type is a reference type, i.e. a class, interface or array type
      *  or a type variable.
      *  @param trees         Original trees, used for error reporting.
      *  @param types         The types to be checked.
      */
     List<Type> checkRefTypes(List<JCExpression> trees, List<Type> types) {
         List<JCExpression> tl = trees;
         for (List<Type> l = types; l.nonEmpty(); l = l.tail) {
-            l.head = checkRefType(tl.head.pos(), l.head);
+            l.head = checkRefType(tl.head.pos(), l.head, false);
             tl = tl.tail;
         }
         return types;
     }
 
@@ -825,10 +894,58 @@
             return false;
         } else
             return true;
     }
 
+    void checkParameterizationWithValues(DiagnosticPosition pos, Type t) {
+        valueParameterizationChecker.visit(t, pos);
+    }
+
+    /** valueParameterizationChecker: A type visitor that descends down the given type looking for instances of value types
+     *  being used as type arguments and issues error against those usages.
+     */
+    private final Types.SimpleVisitor<Void, DiagnosticPosition> valueParameterizationChecker = new Types.SimpleVisitor<Void, DiagnosticPosition>() {
+
+        @Override
+        public Void visitType(Type t, DiagnosticPosition pos) {
+            return null;
+        }
+
+        @Override
+        public Void visitClassType(ClassType t, DiagnosticPosition pos) {
+            for (Type targ : t.allparams()) {
+                if (types.isValue(targ)) {
+                    log.error(pos, Errors.GenericParameterizationWithValueType(t));
+                }
+                visit(targ, pos);
+            }
+            return null;
+        }
+
+        @Override
+        public Void visitTypeVar(TypeVar t, DiagnosticPosition pos) {
+             return null;
+        }
+
+        @Override
+        public Void visitCapturedType(CapturedType t, DiagnosticPosition pos) {
+            return null;
+        }
+
+        @Override
+        public Void visitArrayType(ArrayType t, DiagnosticPosition pos) {
+            return visit(t.elemtype, pos);
+        }
+
+        @Override
+        public Void visitWildcardType(WildcardType t, DiagnosticPosition pos) {
+            return visit(t.type, pos);
+        }
+    };
+
+
+
     /** Check that usage of diamond operator is correct (i.e. diamond should not
      * be used with non-generic classes or in anonymous class creation expressions)
      */
     Type checkDiamond(JCNewClass tree, Type t) {
         if (!TreeInfo.isDiamond(tree) ||
@@ -973,11 +1090,48 @@
             log.error(pos, Errors.CantInferLocalVarType(name, Fragments.LocalCantInferVoid));
             return types.createErrorType(t);
         }
 
         //upward project the initializer type
-        return types.upward(t, types.captures(t));
+        Type varType = types.upward(t, types.captures(t));
+        if (varType.hasTag(CLASS)) {
+            checkParameterizationWithValues(pos, varType);
+        }
+        return varType;
+    }
+
+    public void checkForSuspectClassLiteralComparison(
+            final JCBinary tree,
+            final Type leftType,
+            final Type rightType) {
+
+        if (lint.isEnabled(LintCategory.MIGRATION)) {
+            if (isInvocationOfGetClass(tree.lhs) && isClassOfSomeInterface(rightType) ||
+                    isInvocationOfGetClass(tree.rhs) && isClassOfSomeInterface(leftType)) {
+                log.warning(LintCategory.MIGRATION, tree.pos(), Warnings.GetClassComparedWithInterface);
+            }
+        }
+    }
+    //where
+    private boolean isClassOfSomeInterface(Type someClass) {
+        if (someClass.tsym.flatName() == names.java_lang_Class) {
+            List<Type> arguments = someClass.getTypeArguments();
+            if (arguments.length() == 1) {
+                return arguments.head.isInterface();
+            }
+        }
+        return false;
+    }
+    //where
+    private boolean isInvocationOfGetClass(JCExpression tree) {
+        tree = TreeInfo.skipParens(tree);
+        if (tree.hasTag(APPLY)) {
+            JCMethodInvocation apply = (JCMethodInvocation)tree;
+            MethodSymbol msym = (MethodSymbol)TreeInfo.symbol(apply.meth);
+            return msym.name == names.getClass && msym.implementedIn(syms.objectType.tsym, types) != null;
+        }
+        return false;
     }
 
     Type checkMethod(final Type mtype,
             final Symbol sym,
             final Env<AttrContext> env,
@@ -1171,12 +1325,16 @@
                 mask = ReceiverParamFlags;
             else if (sym.owner.kind != TYP)
                 mask = LocalVarFlags;
             else if ((sym.owner.flags_field & INTERFACE) != 0)
                 mask = implicit = InterfaceVarFlags;
-            else
+            else {
                 mask = VarFlags;
+                if (types.isValue(sym.owner.type) && (flags & STATIC) == 0) {
+                    implicit |= FINAL;
+                }
+            }
             break;
         case MTH:
             if (sym.name == names.init) {
                 if ((sym.owner.flags_field & ENUM) != 0) {
                     // enum constructors cannot be declared public or
@@ -1200,11 +1358,13 @@
                     mask = implicit = InterfaceMethodFlags;
                 }
             } else if ((sym.owner.flags_field & RECORD) != 0) {
                 mask = RecordMethodFlags;
             } else {
-                mask = MethodFlags;
+                // instance methods of value types do not have a monitor associated with their `this'
+                mask = ((sym.owner.flags_field & VALUE) != 0 && (flags & Flags.STATIC) == 0) ?
+                        MethodFlags & ~SYNCHRONIZED : MethodFlags;
             }
             // Imply STRICTFP if owner has STRICTFP set.
             if (((flags|implicit) & Flags.ABSTRACT) == 0 ||
                 ((flags) & Flags.DEFAULT) != 0)
                 implicit |= sym.owner.flags_field & STRICTFP;
@@ -1236,12 +1396,12 @@
             }
             // Interfaces are always ABSTRACT
             if ((flags & INTERFACE) != 0) implicit |= ABSTRACT;
 
             if ((flags & ENUM) != 0) {
-                // enums can't be declared abstract or final
-                mask &= ~(ABSTRACT | FINAL);
+                // enums can't be declared abstract or final or value type
+                mask &= ~(ABSTRACT | FINAL | VALUE);
                 implicit |= implicitEnumFinalFlag(tree);
             }
             if ((flags & RECORD) != 0) {
                 // records can't be declared abstract
                 mask &= ~ABSTRACT;
@@ -1275,21 +1435,21 @@
                                 STATIC | PRIVATE,
                                 DEFAULT)
                  &&
                  checkDisjoint(pos, flags,
                                ABSTRACT | INTERFACE,
-                               FINAL | NATIVE | SYNCHRONIZED)
+                               FINAL | NATIVE | SYNCHRONIZED | VALUE)
                  &&
                  checkDisjoint(pos, flags,
                                PUBLIC,
                                PRIVATE | PROTECTED)
                  &&
                  checkDisjoint(pos, flags,
                                PRIVATE,
                                PUBLIC | PROTECTED)
                  &&
-                 checkDisjoint(pos, flags,
+                 checkDisjoint(pos, (flags | implicit), // complain against volatile & implcitly final entities too.
                                FINAL,
                                VOLATILE)
                  &&
                  (sym.kind == TYP ||
                   checkDisjoint(pos, flags,
@@ -1453,14 +1613,17 @@
             }
         }
 
         public void visitSelectInternal(JCFieldAccess tree) {
             if (tree.type.tsym.isStatic() &&
-                tree.selected.type.isParameterized()) {
+                tree.selected.type.isParameterized() &&
+                    (tree.name != names.ref || !tree.type.isReferenceProjection())) {
                 // The enclosing type is not a class, so we are
                 // looking at a static member type.  However, the
                 // qualifying expression is parameterized.
+                // Tolerate the pseudo-select V.ref: V<T>.ref will be static if V<T> is and
+                // should not be confused as selecting a static member of a parameterized type.
                 log.error(tree.pos(), Errors.CantSelectStaticClassFromParamType);
             } else {
                 // otherwise validate the rest of the expression
                 tree.selected.accept(this);
             }
@@ -1783,10 +1946,19 @@
                                                           asFlagSet(other.flags() & AccessFlags)));
             m.flags_field |= BAD_OVERRIDE;
             return;
         }
 
+        if (origin.isValue() && other.owner == syms.objectType.tsym && m.type.getParameterTypes().size() == 0) {
+            if (m.name == names.clone || m.name == names.finalize) {
+                log.error(TreeInfo.diagnosticPositionFor(m, tree),
+                        Errors.InlineClassMayNotOverride(m.name));
+                m.flags_field |= BAD_OVERRIDE;
+                return;
+            }
+        }
+
         Type mt = types.memberType(origin.type, m);
         Type ot = types.memberType(origin.type, other);
         // Error if overriding result type is different
         // (or, in the case of generics mode, not a subtype) of
         // overridden result type. We have to rename any type parameters
@@ -2095,11 +2267,12 @@
         final boolean explicitOverride = m.attribute(syms.overrideType.tsym) != null;
         // Check if this method must override a super method due to being annotated with @Override
         // or by virtue of being a member of a diamond inferred anonymous class. Latter case is to
         // be treated "as if as they were annotated" with @Override.
         boolean mustOverride = explicitOverride ||
-                (env.info.isAnonymousDiamond && !m.isConstructor() && !m.isPrivate());
+                (env.info.isAnonymousDiamond && !m.isConstructor() && !m.isPrivate() &&
+                        (!m.owner.isValue() || (tree.body.flags & SYNTHETIC) == 0));
         if (mustOverride && !isOverrider(m)) {
             DiagnosticPosition pos = tree.pos();
             for (JCAnnotation a : tree.getModifiers().annotations) {
                 if (a.annotationType.type.tsym == syms.overrideType.tsym) {
                     pos = a.pos();
@@ -2221,10 +2394,49 @@
             log.error(pos,
                       Errors.DoesNotOverrideAbstract(c, undef1, undef1.location()));
         }
     }
 
+    // A value class cannot contain a field of its own type either or indirectly.
+    void checkNonCyclicMembership(JCClassDecl tree) {
+        Assert.check((tree.sym.flags_field & LOCKED) == 0);
+        try {
+            tree.sym.flags_field |= LOCKED;
+            for (List<? extends JCTree> l = tree.defs; l.nonEmpty(); l = l.tail) {
+                if (l.head.hasTag(VARDEF)) {
+                    JCVariableDecl field = (JCVariableDecl) l.head;
+                    if (cyclePossible(field.sym)) {
+                        Type fieldType = field.sym.type;
+                        checkNonCyclicMembership((ClassSymbol) fieldType.tsym, field.pos());
+                    }
+                }
+            }
+        } finally {
+            tree.sym.flags_field &= ~LOCKED;
+        }
+
+    }
+    // where
+    private void checkNonCyclicMembership(ClassSymbol c, DiagnosticPosition pos) {
+        if ((c.flags_field & LOCKED) != 0) {
+            log.error(pos, Errors.CyclicValueTypeMembership(c));
+            return;
+        }
+        try {
+            c.flags_field |= LOCKED;
+            for (Symbol fld : c.members().getSymbols(s -> s.kind == VAR && cyclePossible((VarSymbol) s), NON_RECURSIVE)) {
+                checkNonCyclicMembership((ClassSymbol) fld.type.tsym, pos);
+            }
+        } finally {
+            c.flags_field &= ~LOCKED;
+        }
+    }
+        // where
+        private boolean cyclePossible(VarSymbol symbol) {
+            return (symbol.flags() & STATIC) == 0 && types.isValue(symbol.type);
+        }
+
     void checkNonCyclicDecl(JCClassDecl tree) {
         CycleChecker cc = new CycleChecker();
         cc.scan(tree);
         if (!cc.errorFound && !cc.partialCheck) {
             tree.sym.flags_field |= ACYCLIC;
@@ -2469,10 +2681,14 @@
             for (List<Type> m = supertypes; m != l; m = m.tail)
                 if (!checkCompatibleAbstracts(pos, l.head, m.head, c))
                     return;
         }
         checkCompatibleConcretes(pos, c);
+
+        if (c.isValue() && types.asSuper(c, syms.identityObjectType.tsym, true) != null) {
+            log.error(pos, Errors.InlineTypeMustNotImplementIdentityObject(c));
+        }
     }
 
     /** Check that all non-override equivalent methods accessible from 'site'
      *  are mutually compatible (JLS 8.4.8/9.4.1).
      *
@@ -3028,10 +3244,17 @@
                 log.error(a.pos(), Errors.BadFunctionalIntfAnno);
             } else if (!s.isInterface() || (s.flags() & ANNOTATION) != 0) {
                 log.error(a.pos(), Errors.BadFunctionalIntfAnno1(Fragments.NotAFunctionalIntf(s)));
             }
         }
+        if (a.annotationType.type.tsym == syms.valueBasedType.tsym) {
+            if (s.isInterface() || s.isEnum()) {
+                log.error(a.pos(), Errors.BadValueBasedAnno);
+            } else if (allowValueBasedClasses) {
+                s.flags_field |= VALUEBASED;
+            }
+        }
     }
 
     public void validateTypeAnnotation(JCAnnotation a, boolean isTypeParameter) {
         Assert.checkNonNull(a.type);
         validateAnnotationTree(a);
diff a/src/jdk.compiler/share/classes/com/sun/tools/javac/comp/TypeEnter.java b/src/jdk.compiler/share/classes/com/sun/tools/javac/comp/TypeEnter.java
--- a/src/jdk.compiler/share/classes/com/sun/tools/javac/comp/TypeEnter.java
+++ b/src/jdk.compiler/share/classes/com/sun/tools/javac/comp/TypeEnter.java
@@ -37,10 +37,11 @@
 import com.sun.tools.javac.code.Scope.NamedImportScope;
 import com.sun.tools.javac.code.Scope.StarImportScope;
 import com.sun.tools.javac.code.Scope.WriteableScope;
 import com.sun.tools.javac.code.Source.Feature;
 import com.sun.tools.javac.comp.Annotate.AnnotationTypeMetadata;
+import com.sun.tools.javac.jvm.Target;
 import com.sun.tools.javac.tree.*;
 import com.sun.tools.javac.util.*;
 import com.sun.tools.javac.util.DefinedBy.Api;
 
 import com.sun.tools.javac.code.Symbol.*;
@@ -674,10 +675,11 @@
             ClassSymbol sym = tree.sym;
             ClassType ct = (ClassType)sym.type;
             // Determine supertype.
             Type supertype;
             JCExpression extending;
+            final boolean isValueType = (tree.mods.flags & Flags.VALUE) != 0;
 
             if (tree.extending != null) {
                 extending = clearTypeParams(tree.extending);
                 supertype = attr.attribBase(extending, baseEnv, true, false, true);
                 if (supertype == syms.recordType) {
@@ -717,10 +719,19 @@
             }  else {
                 ct.interfaces_field = interfaces.toList();
                 ct.all_interfaces_field = (all_interfaces == null)
                         ? ct.interfaces_field : all_interfaces.toList();
             }
+            if (ct.isValue()) {
+                ClassSymbol cSym = (ClassSymbol) ct.tsym;
+                if (cSym.projection != null) {
+                    ClassType projectedType = (ClassType) cSym.projection.type;
+                    projectedType.supertype_field = ct.supertype_field;
+                    projectedType.interfaces_field = ct.interfaces_field;
+                    projectedType.all_interfaces_field = ct.all_interfaces_field;
+                }
+            }
         }
             //where:
             protected JCExpression clearTypeParams(JCExpression superType) {
                 return superType;
             }
@@ -1025,10 +1036,13 @@
                     tree.defs.diff(alreadyEntered) : tree.defs;
             memberEnter.memberEnter(defsToEnter, env);
             if (isRecord) {
                 addRecordMembersIfNeeded(tree, env);
             }
+            if ((tree.mods.flags & (Flags.VALUE | Flags.INTERFACE)) == Flags.VALUE && !tree.sym.type.hasTag(ERROR)) {
+                addValueMembers(tree, env);
+            }
             if (tree.sym.isAnnotationType()) {
                 Assert.check(tree.sym.isCompleted());
                 tree.sym.setAnnotationTypeMetadata(new AnnotationTypeMetadata(tree.sym, annotate.annotationTypeSourceCompleter()));
             }
         }
@@ -1097,10 +1111,98 @@
                           null,
                           null);
             memberEnter.memberEnter(valueOf, env);
         }
 
+        /** Add the implicit members for a value type to the parse tree and the symbol table.
+         */
+        private void addValueMembers(JCClassDecl tree, Env<AttrContext> env) {
+
+            boolean requireHashCode = true, requireEquals = true, requireToString = true;
+
+            for (JCTree def : tree.defs) {
+                if (def.getTag() == METHODDEF) {
+                    JCMethodDecl methodDecl = (JCMethodDecl)def;
+                    if (methodDecl.sym != null
+                            && methodDecl.sym.type != null
+                            && !methodDecl.sym.type.isErroneous()
+                            && (methodDecl.sym.flags() & STATIC) == 0) {
+                        final List<Type> parameterTypes = methodDecl.sym.type.getParameterTypes();
+                        switch (parameterTypes.size()) {
+                            case 0:
+                                String name = methodDecl.name.toString();
+                                if (name.equals("hashCode"))
+                                    requireHashCode = false;
+                                else if (name.equals("toString"))
+                                    requireToString = false;
+                                break;
+                            case 1:
+                                name = methodDecl.name.toString();
+                                if (name.equals("equals") && parameterTypes.head.tsym == syms.objectType.tsym)
+                                    requireEquals = false;
+                                break;
+                        }
+                    }
+                }
+            }
+
+            make.at(tree.pos);
+            // Make a body comprising { throw new RuntimeException(""Internal error: This method must have been replaced by javac"); }
+            JCBlock body = make.Block(Flags.SYNTHETIC, List.of(make.Throw(
+                    make.NewClass(null,
+                            null,
+                            make.Ident(names.fromString("RuntimeException")),
+                            List.of(make.Literal(CLASS, "Internal error: This method must have been replaced by javac")),
+                            null))));
+
+            if (requireHashCode) {
+                // public int hashCode() { throw new RuntimeException(message); }
+                JCMethodDecl hashCode = make.
+                        MethodDef(make.Modifiers(Flags.PUBLIC | Flags.FINAL),
+                                names.hashCode,
+                                make.TypeIdent(TypeTag.INT),
+                                List.nil(),
+                                List.nil(),
+                                List.nil(), // thrown
+                                body,
+                                null);
+                memberEnter.memberEnter(hashCode, env);
+                tree.defs = tree.defs.append(hashCode);
+            }
+
+            if (requireEquals) {
+                // public boolean equals(Object o) { throw new RuntimeException(message); }
+                JCMethodDecl equals = make.
+                        MethodDef(make.Modifiers(Flags.PUBLIC | Flags.FINAL),
+                                names.equals,
+                                make.TypeIdent(TypeTag.BOOLEAN),
+                                List.nil(),
+                                List.of(make.VarDef(make.Modifiers(PARAMETER), names.fromString("o"), make.Ident(names.fromString("Object")), null )),
+                                List.nil(), // thrown
+                                body,
+                                null);
+                memberEnter.memberEnter(equals, env);
+                tree.defs = tree.defs.append(equals);
+            }
+
+            if (requireToString) {
+                // public String toString() { throw new RuntimeException(message); }
+                JCMethodDecl toString = make.
+                        MethodDef(make.Modifiers(Flags.PUBLIC | Flags.FINAL),
+                                names.toString,
+                                make.Ident(names.fromString("String")),
+                                List.nil(),
+                                List.nil(),
+                                List.nil(), // thrown
+                                body,
+                                null);
+                memberEnter.memberEnter(toString, env);
+                tree.defs = tree.defs.append(toString);
+            }
+
+        }
+
         JCMethodDecl getCanonicalConstructorDecl(JCClassDecl tree) {
             // let's check if there is a constructor with exactly the same arguments as the record components
             List<Type> recordComponentErasedTypes = types.erasure(TreeInfo.recordFields(tree).map(vd -> vd.sym.type));
             JCMethodDecl canonicalDecl = null;
             for (JCTree def : tree.defs) {
diff a/src/jdk.compiler/share/classes/com/sun/tools/javac/parser/JavacParser.java b/src/jdk.compiler/share/classes/com/sun/tools/javac/parser/JavacParser.java
--- a/src/jdk.compiler/share/classes/com/sun/tools/javac/parser/JavacParser.java
+++ b/src/jdk.compiler/share/classes/com/sun/tools/javac/parser/JavacParser.java
@@ -32,10 +32,11 @@
 import com.sun.source.tree.CaseTree;
 import com.sun.source.tree.MemberReferenceTree.ReferenceMode;
 import com.sun.source.tree.ModuleTree.ModuleKind;
 
 import com.sun.tools.javac.code.*;
+import com.sun.tools.javac.code.Flags.Flag;
 import com.sun.tools.javac.code.Source.Feature;
 import com.sun.tools.javac.parser.Tokens.*;
 import com.sun.tools.javac.parser.Tokens.Comment.CommentStyle;
 import com.sun.tools.javac.resources.CompilerProperties.Errors;
 import com.sun.tools.javac.resources.CompilerProperties.Fragments;
@@ -46,18 +47,20 @@
 import com.sun.tools.javac.util.JCDiagnostic.DiagnosticFlag;
 import com.sun.tools.javac.util.JCDiagnostic.Error;
 import com.sun.tools.javac.util.JCDiagnostic.Fragment;
 import com.sun.tools.javac.util.List;
 
+import static com.sun.tools.javac.code.Flags.asFlagSet;
 import static com.sun.tools.javac.parser.Tokens.TokenKind.*;
 import static com.sun.tools.javac.parser.Tokens.TokenKind.ASSERT;
 import static com.sun.tools.javac.parser.Tokens.TokenKind.CASE;
 import static com.sun.tools.javac.parser.Tokens.TokenKind.CATCH;
 import static com.sun.tools.javac.parser.Tokens.TokenKind.EQ;
 import static com.sun.tools.javac.parser.Tokens.TokenKind.GT;
 import static com.sun.tools.javac.parser.Tokens.TokenKind.IMPORT;
 import static com.sun.tools.javac.parser.Tokens.TokenKind.LT;
+import static com.sun.tools.javac.parser.Tokens.TokenKind.SYNCHRONIZED;
 import static com.sun.tools.javac.tree.JCTree.Tag.*;
 import static com.sun.tools.javac.resources.CompilerProperties.Fragments.ImplicitAndExplicitNotAllowed;
 import static com.sun.tools.javac.resources.CompilerProperties.Fragments.VarAndExplicitNotAllowed;
 import static com.sun.tools.javac.resources.CompilerProperties.Fragments.VarAndImplicitNotAllowed;
 
@@ -182,10 +185,11 @@
         this.keepLineMap = keepLineMap;
         this.errorTree = F.Erroneous();
         endPosTable = newEndPosTable(keepEndPositions);
         this.allowYieldStatement = (!preview.isPreview(Feature.SWITCH_EXPRESSION) || preview.isEnabled()) &&
                 Feature.SWITCH_EXPRESSION.allowedInSource(source);
+        this.allowWithFieldOperator = fac.options.isSet("allowWithFieldOperator");
         this.allowRecords = (!preview.isPreview(Feature.RECORDS) || preview.isEnabled()) &&
                 Feature.RECORDS.allowedInSource(source);
     }
 
     protected AbstractEndPosTable newEndPosTable(boolean keepEndPositions) {
@@ -200,10 +204,14 @@
 
     /** Switch: should we fold strings?
      */
     boolean allowStringFolding;
 
+    /** Switch: should we allow withField operator at source level ?
+    */
+    boolean allowWithFieldOperator;
+
     /** Switch: should we keep docComments?
      */
     boolean keepDocComments;
 
     /** Switch: should we keep line table?
@@ -295,10 +303,17 @@
         return tk1.accepts(S.token(lookahead + 1).kind) &&
                 tk2.accepts(S.token(lookahead + 2).kind) &&
                 tk3.accepts(S.token(lookahead + 3).kind);
     }
 
+    protected boolean peekToken(int lookahead, Filter<TokenKind> tk1, Filter<TokenKind> tk2, Filter<TokenKind> tk3, Filter<TokenKind> tk4) {
+        return tk1.accepts(S.token(lookahead + 1).kind) &&
+                tk2.accepts(S.token(lookahead + 2).kind) &&
+                tk3.accepts(S.token(lookahead + 3).kind) &&
+                tk4.accepts(S.token(lookahead + 4).kind);
+    }
+
     @SuppressWarnings("unchecked")
     protected boolean peekToken(Filter<TokenKind>... kinds) {
         return peekToken(0, kinds);
     }
 
@@ -464,10 +479,26 @@
             setErrorEndPos(token.pos);
             reportSyntaxError(S.prevToken().endPos, errorProvider.apply(tk));
         }
     }
 
+    /** If next input token matches one of the two given tokens, skip it, otherwise report
+     *  an error.
+     *
+     * @return The actual token kind.
+     */
+    public TokenKind accept2(TokenKind tk1, TokenKind tk2) {
+        TokenKind returnValue = token.kind;
+        if (token.kind == tk1 || token.kind == tk2) {
+            nextToken();
+        } else {
+            setErrorEndPos(token.pos);
+            reportSyntaxError(S.prevToken().endPos, Errors.Expected2(tk1, tk2));
+        }
+        return returnValue;
+    }
+
     /** Report an illegal start of expression/type error at given position.
      */
     JCExpression illegal(int pos) {
         setErrorEndPos(pos);
         if ((mode & EXPR) != 0)
@@ -1107,10 +1138,25 @@
                     t = term3();
                     return F.at(pos).Unary(unoptag(tk), t);
                 }
             } else return illegal();
             break;
+        case WITHFIELD:
+            if (!allowWithFieldOperator) {
+                log.error(pos, Errors.WithFieldOperatorDisallowed);
+            }
+            if (typeArgs == null && (mode & EXPR) != 0) {
+                nextToken();
+                accept(LPAREN);
+                mode = EXPR;
+                t = term();
+                accept(COMMA);
+                mode = EXPR;
+                JCExpression v = term();
+                accept(RPAREN);
+                return F.at(pos).WithField(t, v);
+            } else return illegal();
         case LPAREN:
             if (typeArgs == null && (mode & EXPR) != 0) {
                 ParensResult pres = analyzeParens();
                 switch (pres) {
                     case CAST:
@@ -1275,10 +1321,16 @@
                         mode &= ~NOPARAMS;
                         typeArgs = typeArgumentsOpt(EXPR);
                         mode = oldmode;
                         if ((mode & EXPR) != 0) {
                             switch (token.kind) {
+                            case DEFAULT:
+                                if (typeArgs != null) return illegal();
+                                selectExprMode();
+                                t = to(F.at(pos).Select(t, names._default));
+                                nextToken();
+                                break loop;
                             case CLASS:
                                 if (typeArgs != null) return illegal();
                                 selectExprMode();
                                 t = to(F.at(pos).Select(t, names._class));
                                 nextToken();
@@ -1324,13 +1376,14 @@
                             // Don't return here -- error recovery attempt
                             illegal(annos.head.pos);
                         }
                         break loop;
                     case LT:
-                        if ((mode & TYPE) == 0 && isUnboundMemberRef()) {
-                            //this is an unbound method reference whose qualifier
-                            //is a generic type i.e. A<S>::m
+                        if ((mode & TYPE) == 0 && isParameterizedTypePrefix()) {
+                            //this is either an unbound method reference whose qualifier
+                            //is a generic type i.e. A<S>::m or a default value creation of
+                            //the form ValueType<S>.default
                             int pos1 = token.pos;
                             accept(LT);
                             ListBuffer<JCExpression> args = new ListBuffer<>();
                             args.append(typeArgument());
                             while (token.kind == COMMA) {
@@ -1339,10 +1392,16 @@
                             }
                             accept(GT);
                             t = toP(F.at(pos1).TypeApply(t, args.toList()));
                             while (token.kind == DOT) {
                                 nextToken();
+                                if (token.kind == DEFAULT) {
+                                    t =  toP(F.at(token.pos).Select(t, names._default));
+                                    nextToken();
+                                    selectExprMode();
+                                    return term3Rest(t, typeArgs);
+                                }
                                 selectTypeMode();
                                 t = toP(F.at(token.pos).Select(t, ident()));
                                 t = typeArgumentsOpt(t);
                             }
                             t = bracketsOpt(t);
@@ -1503,11 +1562,11 @@
                     selectExprMode();
                     t = to(F.at(pos1).Select(t, names._super));
                     nextToken();
                     t = arguments(typeArgs, t);
                     typeArgs = null;
-                } else if (token.kind == NEW && (mode & EXPR) != 0) {
+                } else if ((token.kind == NEW) && (mode & EXPR) != 0) {
                     if (typeArgs != null) return illegal();
                     selectExprMode();
                     int pos2 = token.pos;
                     nextToken();
                     if (token.kind == LT) typeArgs = typeArguments(false);
@@ -1550,15 +1609,16 @@
         return toP(t);
     }
 
     /**
      * If we see an identifier followed by a '&lt;' it could be an unbound
-     * method reference or a binary expression. To disambiguate, look for a
+     * method reference or a default value creation that uses a parameterized type
+     * or a binary expression. To disambiguate, look for a
      * matching '&gt;' and see if the subsequent terminal is either '.' or '::'.
      */
     @SuppressWarnings("fallthrough")
-    boolean isUnboundMemberRef() {
+    boolean isParameterizedTypePrefix() {
         int pos = 0, depth = 0;
         outer: for (Token t = S.token(pos) ; ; t = S.token(++pos)) {
             switch (t.kind) {
                 case IDENTIFIER: case UNDERSCORE: case QUES: case EXTENDS: case SUPER:
                 case DOT: case RBRACKET: case LBRACKET: case COMMA:
@@ -1676,12 +1736,12 @@
                     }
                 case UNDERSCORE:
                 case ASSERT:
                 case ENUM:
                 case IDENTIFIER:
-                    if (peekToken(lookahead, LAX_IDENTIFIER)) {
-                        // Identifier, Identifier/'_'/'assert'/'enum' -> explicit lambda
+                    if (peekToken(lookahead, LAX_IDENTIFIER) || (peekToken(lookahead, QUES, LAX_IDENTIFIER) && (peekToken(lookahead + 2, RPAREN) || peekToken(lookahead + 2, COMMA)))) {
+                        // Identifier[?], Identifier/'_'/'assert'/'enum' -> explicit lambda
                         return ParensResult.EXPLICIT_LAMBDA;
                     } else if (peekToken(lookahead, RPAREN, ARROW)) {
                         // Identifier, ')' '->' -> implicit lambda
                         return (mode & NOLAMBDA) == 0 ? ParensResult.IMPLICIT_LAMBDA
                                                       : ParensResult.PARENS;
@@ -1753,10 +1813,12 @@
                             // '>', ')' -> cast
                             // '>', '&' -> cast
                             return ParensResult.CAST;
                         } else if (peekToken(lookahead, LAX_IDENTIFIER, COMMA) ||
                                 peekToken(lookahead, LAX_IDENTIFIER, RPAREN, ARROW) ||
+                                peekToken(lookahead, QUES, LAX_IDENTIFIER, COMMA) ||
+                                peekToken(lookahead, QUES, LAX_IDENTIFIER, RPAREN, ARROW) ||
                                 peekToken(lookahead, ELLIPSIS)) {
                             // '>', Identifier/'_'/'assert'/'enum', ',' -> explicit lambda
                             // '>', Identifier/'_'/'assert'/'enum', ')', '->' -> explicit lambda
                             // '>', '...' -> explicit lambda
                             return ParensResult.EXPLICIT_LAMBDA;
@@ -2149,11 +2211,11 @@
     JCExpression bracketsSuffix(JCExpression t) {
         if ((mode & EXPR) != 0 && token.kind == DOT) {
             selectExprMode();
             int pos = token.pos;
             nextToken();
-            accept(CLASS);
+            TokenKind selector = accept2(CLASS, DEFAULT);
             if (token.pos == endPosTable.errorEndPos) {
                 // error recovery
                 Name name;
                 if (LAX_IDENTIFIER.accepts(token.kind)) {
                     name = token.name();
@@ -2167,11 +2229,11 @@
                 // Type annotations are illegal on class literals. Annotated non array class literals
                 // are complained about directly in term3(), Here check for type annotations on dimensions
                 // taking care to handle some interior dimension(s) being annotated.
                 if ((tag == TYPEARRAY && TreeInfo.containsTypeAnnotation(t)) || tag == ANNOTATED_TYPE)
                     syntaxError(token.pos, Errors.NoAnnotationsOnDotClass);
-                t = toP(F.at(pos).Select(t, names._class));
+                t = toP(F.at(pos).Select(t, selector == CLASS ? names._class : names._default));
             }
         } else if ((mode & TYPE) != 0) {
             if (token.kind != COLCOL) {
                 selectTypeMode();
             }
@@ -2212,15 +2274,22 @@
     }
 
     /** Creator = [Annotations] Qualident [TypeArguments] ( ArrayCreatorRest | ClassCreatorRest )
      */
     JCExpression creator(int newpos, List<JCExpression> typeArgs) {
-        List<JCAnnotation> newAnnotations = typeAnnotationsOpt();
-
+        final JCModifiers mods = modifiersOpt();
+        List<JCAnnotation> newAnnotations = mods.annotations;
+        if (!newAnnotations.isEmpty()) {
+            checkSourceLevel(newAnnotations.head.pos, Feature.TYPE_ANNOTATIONS);
+        }
         switch (token.kind) {
         case BYTE: case SHORT: case CHAR: case INT: case LONG: case FLOAT:
         case DOUBLE: case BOOLEAN:
+            if (mods.flags != 0) {
+                long badModifiers = (mods.flags & Flags.VALUE) != 0 ? mods.flags & ~Flags.FINAL : mods.flags;
+                log.error(token.pos, Errors.ModNotAllowedHere(asFlagSet(badModifiers)));
+            }
             if (typeArgs == null) {
                 if (newAnnotations.isEmpty()) {
                     return arrayCreatorRest(newpos, basicType());
                 } else {
                     return arrayCreatorRest(newpos, toP(F.at(newAnnotations.head.pos).AnnotatedType(newAnnotations, basicType())));
@@ -2285,15 +2354,23 @@
                 reportSyntaxError(err, Errors.CannotCreateArrayWithTypeArguments);
                 return toP(err);
             }
             return e;
         } else if (token.kind == LPAREN) {
+            long badModifiers = mods.flags & ~(Flags.VALUE | Flags.FINAL);
+            if (badModifiers != 0)
+                log.error(token.pos, Errors.ModNotAllowedHere(asFlagSet(badModifiers)));
             // handle type annotations for instantiations and anonymous classes
             if (newAnnotations.nonEmpty()) {
                 t = insertAnnotationsToMostInner(t, newAnnotations, false);
             }
-            return classCreatorRest(newpos, null, typeArgs, t);
+            JCNewClass newClass = classCreatorRest(newpos, null, typeArgs, t, mods.flags);
+            if ((newClass.def == null) && (mods.flags != 0)) {
+                badModifiers = (mods.flags & Flags.VALUE) != 0 ? mods.flags & ~Flags.FINAL : mods.flags;
+                log.error(newClass.pos, Errors.ModNotAllowedHere(asFlagSet(badModifiers)));
+            }
+            return newClass;
         } else {
             setErrorEndPos(token.pos);
             reportSyntaxError(token.pos, Errors.Expected2(LPAREN, LBRACKET));
             t = toP(F.at(newpos).NewClass(null, typeArgs, t, List.nil(), null));
             return toP(F.at(newpos).Erroneous(List.<JCTree>of(t)));
@@ -2314,11 +2391,11 @@
         if (token.kind == LT) {
             int oldmode = mode;
             t = typeArguments(t, true);
             mode = oldmode;
         }
-        return classCreatorRest(newpos, encl, typeArgs, t);
+        return classCreatorRest(newpos, encl, typeArgs, t, 0);
     }
 
     /** ArrayCreatorRest = [Annotations] "[" ( "]" BracketsOpt ArrayInitializer
      *                         | Expression "]" {[Annotations]  "[" Expression "]"} BracketsOpt )
      */
@@ -2396,21 +2473,23 @@
     /** ClassCreatorRest = Arguments [ClassBody]
      */
     JCNewClass classCreatorRest(int newpos,
                                   JCExpression encl,
                                   List<JCExpression> typeArgs,
-                                  JCExpression t)
+                                  JCExpression t,
+                                  long flags)
     {
         List<JCExpression> args = arguments();
         JCClassDecl body = null;
         if (token.kind == LBRACE) {
             int pos = token.pos;
             List<JCTree> defs = classInterfaceOrRecordBody(names.empty, false, false);
-            JCModifiers mods = F.at(Position.NOPOS).Modifiers(0);
+            JCModifiers mods = F.at(Position.NOPOS).Modifiers(flags);
             body = toP(F.at(pos).AnonymousClassDef(mods, defs));
         }
-        return toP(F.at(newpos).NewClass(encl, typeArgs, t, args, body));
+        JCNewClass newClass = toP(F.at(newpos).NewClass(encl, typeArgs, t, args, body));
+        return newClass;
     }
 
     /** ArrayInitializer = "{" [VariableInitializer {"," VariableInitializer}] [","] "}"
      */
     JCExpression arrayInitializer(int newpos, JCExpression t) {
@@ -2537,20 +2616,22 @@
     /**This method parses a statement appearing inside a block.
      */
     @SuppressWarnings("fallthrough")
     List<JCStatement> blockStatement() {
         //todo: skip to anchor on error(?)
+        token = recastToken(token);
         Comment dc;
         int pos = token.pos;
         switch (token.kind) {
         case RBRACE: case CASE: case DEFAULT: case EOF:
             return List.nil();
         case LBRACE: case IF: case FOR: case WHILE: case DO: case TRY:
         case SWITCH: case SYNCHRONIZED: case RETURN: case THROW: case BREAK:
         case CONTINUE: case SEMI: case ELSE: case FINALLY: case CATCH:
         case ASSERT:
             return List.of(parseSimpleStatement());
+        case VALUE:
         case MONKEYS_AT:
         case FINAL: {
             dc = token.comment(CommentStyle.JAVADOC);
             JCModifiers mods = modifiersOpt();
             if (token.kind == INTERFACE ||
@@ -2985,11 +3066,14 @@
         if (token.kind == FINAL || token.kind == MONKEYS_AT) {
             return variableDeclarators(optFinal(0), parseType(true), stats, true).toList();
         } else {
             JCExpression t = term(EXPR | TYPE);
             if ((lastmode & TYPE) != 0 && LAX_IDENTIFIER.accepts(token.kind)) {
-                return variableDeclarators(modifiersOpt(), t, stats, true).toList();
+                pos = token.pos;
+                JCModifiers mods = F.at(Position.NOPOS).Modifiers(0);
+                F.at(pos);
+                return variableDeclarators(mods, t, stats, true).toList();
             } else if ((lastmode & TYPE) != 0 && token.kind == COLON) {
                 log.error(DiagnosticFlag.SYNTAX, pos, Errors.BadInitializer("for-loop"));
                 return List.of((JCStatement)F.at(pos).VarDef(modifiersOpt(), names.error, t, null));
             } else {
                 return moreStatementExpressions(pos, t, stats).toList();
@@ -3055,19 +3139,21 @@
         }
         int lastPos;
     loop:
         while (true) {
             long flag;
+            token = recastToken(token);
             switch (token.kind) {
             case PRIVATE     : flag = Flags.PRIVATE; break;
             case PROTECTED   : flag = Flags.PROTECTED; break;
             case PUBLIC      : flag = Flags.PUBLIC; break;
             case STATIC      : flag = Flags.STATIC; break;
             case TRANSIENT   : flag = Flags.TRANSIENT; break;
             case FINAL       : flag = Flags.FINAL; break;
             case ABSTRACT    : flag = Flags.ABSTRACT; break;
             case NATIVE      : flag = Flags.NATIVE; break;
+            case VALUE       : flag = Flags.VALUE; break;
             case VOLATILE    : flag = Flags.VOLATILE; break;
             case SYNCHRONIZED: flag = Flags.SYNCHRONIZED; break;
             case STRICTFP    : flag = Flags.STRICTFP; break;
             case MONKEYS_AT  : flag = Flags.ANNOTATION; break;
             case DEFAULT     : checkSourceLevel(Feature.DEFAULT_METHODS); flag = Flags.DEFAULT; break;
@@ -3081,12 +3167,17 @@
                 if (token.kind != INTERFACE) {
                     JCAnnotation ann = annotation(lastPos, Tag.ANNOTATION);
                     // if first modifier is an annotation, set pos to annotation's.
                     if (flags == 0 && annotations.isEmpty())
                         pos = ann.pos;
-                    annotations.append(ann);
-                    flag = 0;
+                    final Name name = TreeInfo.name(ann.annotationType);
+                    if (name == names.__inline__ || name == names.java_lang___inline__) {
+                        flag = Flags.VALUE;
+                    } else {
+                        annotations.append(ann);
+                        flag = 0;
+                    }
                 }
             }
             flags |= flag;
         }
         switch (token.kind) {
@@ -3098,10 +3189,15 @@
         /* A modifiers tree with no modifier tokens or annotations
          * has no text position. */
         if ((flags & (Flags.ModifierFlags | Flags.ANNOTATION)) == 0 && annotations.isEmpty())
             pos = Position.NOPOS;
 
+        // Force value classes to be automatically final.
+        if ((flags & (Flags.VALUE | Flags.ABSTRACT | Flags.INTERFACE | Flags.ENUM)) == Flags.VALUE) {
+            flags |= Flags.FINAL;
+        }
+
         JCModifiers mods = F.at(pos).Modifiers(flags, annotations.toList());
         if (pos != Position.NOPOS)
             storeEnd(mods, S.prevToken().endPos);
         return mods;
     }
@@ -3285,10 +3381,46 @@
         attach(result, dc);
         result.startPos = startPos;
         return result;
     }
 
+    // Does the given token signal an inline modifier ? If yes, suitably reclassify token.
+    Token recastToken(Token token) {
+        if (token.kind != IDENTIFIER || token.name() != names.inline) {
+            return token;
+        }
+        if (peekToken(t->t == PRIVATE ||
+                         t == PROTECTED ||
+                         t == PUBLIC ||
+                         t == STATIC ||
+                         t == TRANSIENT ||
+                         t == FINAL ||
+                         t == ABSTRACT ||
+                         t == NATIVE ||
+                         t == VOLATILE ||
+                         t == SYNCHRONIZED ||
+                         t == STRICTFP ||
+                         t == MONKEYS_AT ||
+                         t == DEFAULT ||
+                         t == BYTE ||
+                         t == SHORT ||
+                         t == CHAR ||
+                         t == INT ||
+                         t == LONG ||
+                         t == FLOAT ||
+                         t == DOUBLE ||
+                         t == BOOLEAN ||
+                         t == CLASS ||
+                         t == INTERFACE ||
+                         t == ENUM ||
+                         t == IDENTIFIER)) { // new value Comparable() {}
+            checkSourceLevel(Feature.INLINE_TYPES);
+            return new Token(VALUE, token.pos, token.endPos, token.comments);
+        }
+        return token;
+    }
+
     Name restrictedTypeName(JCExpression e, boolean shouldWarn) {
         switch (e.getTag()) {
             case IDENT:
                 return restrictedTypeNameStartingAtSource(((JCIdent)e).name, e.pos, shouldWarn) != null ? ((JCIdent)e).name : null;
             case TYPEARRAY:
diff a/src/jdk.compiler/share/classes/com/sun/tools/javac/resources/compiler.properties b/src/jdk.compiler/share/classes/com/sun/tools/javac/resources/compiler.properties
--- a/src/jdk.compiler/share/classes/com/sun/tools/javac/resources/compiler.properties
+++ b/src/jdk.compiler/share/classes/com/sun/tools/javac/resources/compiler.properties
@@ -394,10 +394,14 @@
 
 # 0: symbol or type
 compiler.err.cyclic.inheritance=\
     cyclic inheritance involving {0}
 
+# 0: symbol
+compiler.err.cyclic.value.type.membership=\
+    cyclic inline type membership involving {0}
+
 # 0: symbol
 compiler.err.cyclic.annotation.element=\
     type of element {0} is cyclic
 
 # 0: symbol
@@ -1769,10 +1773,13 @@
 
 # 0: string
 compiler.warn.incubating.modules=\
     using incubating module(s): {0}
 
+compiler.warn.get.class.compared.with.interface=\
+    return value of getClass() can never equal the class literal of an interface
+
 # 0: symbol, 1: symbol
 compiler.warn.has.been.deprecated=\
     {0} in {1} has been deprecated
 
 # 0: symbol, 1: symbol
@@ -2928,10 +2935,13 @@
     pattern matching in instanceof
 
 compiler.misc.feature.reifiable.types.instanceof=\
     reifiable types in instanceof
 
+compiler.misc.feature.inline.type=\
+    inline type
+
 compiler.misc.feature.records=\
     records
 
 compiler.warn.underscore.as.identifier=\
     as of release 9, ''_'' is a keyword, and may not be used as an identifier
@@ -3647,5 +3657,81 @@
     invalid source release {0} with --enable-preview\n\
     (preview language features are only supported for release {1})
 
 compiler.err.preview.without.source.or.release=\
     --enable-preview must be used with either -source or --release
+
+# 0: name (of method)
+compiler.err.inline.class.may.not.override=\
+    Inline classes may not override the method {0} from Object
+
+# 0: name (of method)
+compiler.err.value.does.not.support=\
+    Inline types do not support {0}
+
+compiler.err.value.may.not.extend=\
+    Inline type may not extend another inline type or class
+
+compiler.err.value.instance.field.expected.here=\
+    withfield operator requires an instance field of an inline class here
+
+compiler.err.bad.value.based.anno=\
+    Unexpected @ValueBased annotation
+
+# 0: type
+compiler.warn.suspicious.mix.of.null.with.value.based.class=\
+    Suspicious mix of null with value based class {0}
+
+# 0: type
+compiler.warn.potential.null.pollution=\
+    Potential null pollution from nullable type {0}
+
+compiler.err.with.field.operator.disallowed=\
+    WithField operator is allowed only with -XDallowWithFieldOperator
+
+compiler.err.empty.value.not.yet=\
+    Inline types with zero instance size are not supported yet
+
+compiler.err.this.exposed.prematurely=\
+    Inine type instance should not be passed around before being fully initialized
+
+compiler.warn.this.exposed.prematurely=\
+    value based type instance should not be passed around before being fully initialized
+
+# 0: type
+compiler.err.generic.parameterization.with.value.type=\
+    Inferred type {0} involves generic parameterization by an inline type
+
+# 0: type
+compiler.err.inline.type.must.not.implement.identity.object=\
+    The inline type {0} attempts to implement the incompatible interface IdentityObject
+
+# 0: symbol, 1: type
+compiler.err.concrete.supertype.for.inline.class=\
+    The concrete class {1} is not allowed to be a super class of the inline class {0} either directly or indirectly
+
+# 0: symbol, 1: symbol, 2: type
+compiler.err.super.method.cannot.be.synchronized=\
+    The method {0} in the super class {2} of the inline type {1} is synchronized. This is disallowed
+
+# 0: symbol, 1: symbol, 2: type
+compiler.err.super.constructor.cannot.take.arguments=\
+    The super class {2} of the inline type {1} defines a constructor {0} that takes arguments. This is disallowed
+
+# 0: symbol, 1: symbol, 2: type
+compiler.err.super.field.not.allowed=\
+    The super class {2} of the inline type {1} defines an instance field {0}. This is disallowed
+
+# 0: symbol, 1: symbol, 2: type
+compiler.err.super.no.arg.constructor.must.be.empty=\
+    The super class {2} of the inline type {1} defines a nonempty no-arg constructor {0}. This is disallowed
+
+# 0: symbol, 1: type
+compiler.err.super.class.declares.init.block=\
+    The super class {1} of the inline class {0} declares one or more non-empty instance initializer blocks. This is disallowed.
+
+# 0: symbol, 1: type
+compiler.err.super.class.cannot.be.inner=\
+    The super class {1} of the inline class {0} is an inner class. This is disallowed.
+
+
+
diff a/src/jdk.internal.vm.ci/share/classes/jdk.vm.ci.hotspot/src/jdk/vm/ci/hotspot/HotSpotResolvedObjectTypeImpl.java b/src/jdk.internal.vm.ci/share/classes/jdk.vm.ci.hotspot/src/jdk/vm/ci/hotspot/HotSpotResolvedObjectTypeImpl.java
--- a/src/jdk.internal.vm.ci/share/classes/jdk.vm.ci.hotspot/src/jdk/vm/ci/hotspot/HotSpotResolvedObjectTypeImpl.java
+++ b/src/jdk.internal.vm.ci/share/classes/jdk.vm.ci.hotspot/src/jdk/vm/ci/hotspot/HotSpotResolvedObjectTypeImpl.java
@@ -275,13 +275,14 @@
 
     @Override
     public HotSpotResolvedObjectTypeImpl[] getInterfaces() {
         if (interfaces == null) {
             if (isArray()) {
-                HotSpotResolvedObjectTypeImpl[] types = new HotSpotResolvedObjectTypeImpl[2];
+                HotSpotResolvedObjectTypeImpl[] types = new HotSpotResolvedObjectTypeImpl[3];
                 types[0] = runtime().getJavaLangCloneable();
                 types[1] = runtime().getJavaLangSerializable();
+                types[2] = runtime().getJavaLangIdentityObject();
                 this.interfaces = types;
             } else {
                 interfaces = runtime().compilerToVm.getInterfaces(this);
             }
         }
diff a/src/jdk.jdi/share/classes/com/sun/tools/jdi/ArrayTypeImpl.java b/src/jdk.jdi/share/classes/com/sun/tools/jdi/ArrayTypeImpl.java
--- a/src/jdk.jdi/share/classes/com/sun/tools/jdi/ArrayTypeImpl.java
+++ b/src/jdk.jdi/share/classes/com/sun/tools/jdi/ArrayTypeImpl.java
@@ -94,11 +94,11 @@
 
         JNITypeParser sig = new JNITypeParser(signature);
         if (sig.isReference()) {
             // It's a reference type
             JNITypeParser parser = new JNITypeParser(componentSignature());
-            List<ReferenceType> list = vm.classesByName(parser.typeName());
+            List<ReferenceType> list = vm.classesBySignature(componentSignature());
             Iterator<ReferenceType> iter = list.iterator();
             while (iter.hasNext()) {
                 ReferenceType type = iter.next();
                 ClassLoaderReference cl = type.classLoader();
                 if ((cl == null)?
diff a/src/jdk.jdi/share/classes/com/sun/tools/jdi/JNITypeParser.java b/src/jdk.jdi/share/classes/com/sun/tools/jdi/JNITypeParser.java
--- a/src/jdk.jdi/share/classes/com/sun/tools/jdi/JNITypeParser.java
+++ b/src/jdk.jdi/share/classes/com/sun/tools/jdi/JNITypeParser.java
@@ -220,10 +220,11 @@
         switch(key) {
             case (JDWP.Tag.ARRAY):
                 return  key + nextSignature();
 
             case (JDWP.Tag.OBJECT):
+            case (JDWP.Tag.INLINE_OBJECT):
                 int endClass = signature.indexOf(SIGNATURE_ENDCLASS,
                                                  currentIndex);
                 String retVal = signature.substring(currentIndex - 1,
                                                     endClass + 1);
                 currentIndex = endClass + 1;
@@ -263,10 +264,11 @@
 
             case (JDWP.Tag.CHAR):
                 return "char";
 
             case (JDWP.Tag.OBJECT):
+            case (JDWP.Tag.INLINE_OBJECT):
                 int endClass = signature.indexOf(SIGNATURE_ENDCLASS,
                                                  currentIndex);
                 String retVal = signature.substring(currentIndex,
                                                     endClass);
                 int index = retVal.indexOf(".");
diff a/src/jdk.jdi/share/classes/com/sun/tools/jdi/ObjectReferenceImpl.java b/src/jdk.jdi/share/classes/com/sun/tools/jdi/ObjectReferenceImpl.java
--- a/src/jdk.jdi/share/classes/com/sun/tools/jdi/ObjectReferenceImpl.java
+++ b/src/jdk.jdi/share/classes/com/sun/tools/jdi/ObjectReferenceImpl.java
@@ -74,10 +74,14 @@
         synchronized (vm.state()) {
             cache = markerCache;
         }
     }
 
+    private boolean isInlineType() {
+        return referenceType().signature().startsWith("Q");
+    }
+
     // Override in subclasses
     protected Cache newCache() {
         return new Cache();
     }
 
@@ -143,15 +147,38 @@
                 return true;
             }
         }
     }
 
+    // The isSubstitutable() method must perform the same comparison on
+    // remote inline objects as the one performed ValueBootstrapMethods.isSubstitutable()
+    // on local instances.
+    private boolean isSubstitutable(ObjectReferenceImpl other) {
+        if (referenceType() != other.referenceType()) return false;
+        List<Field> fields = referenceType().fields();
+        for (Field f : fields) {
+            if (f.isStatic()) {
+                fields.remove(f);
+            }
+        }
+        Map<Field,Value> thisFields = getValues(fields);
+        Map<Field,Value> otherFields = other.getValues(fields);
+        for (Field f : fields) {
+            if (!thisFields.get(f).equals(otherFields.get(f))) return false;
+        }
+        return true;
+    }
+
     public boolean equals(Object obj) {
         if ((obj != null) && (obj instanceof ObjectReferenceImpl)) {
-            ObjectReferenceImpl other = (ObjectReferenceImpl)obj;
-            return (ref() == other.ref()) &&
-                   super.equals(obj);
+            ObjectReferenceImpl other = (ObjectReferenceImpl) obj;
+            if (isInlineType()) {
+                return isSubstitutable(other);
+            } else {
+                return (ref() == other.ref()) &&
+                        super.equals(obj);
+            }
         } else {
             return false;
         }
     }
 
@@ -468,10 +495,13 @@
             throw exc.toJDIException();
         }
     }
 
     public long uniqueID() {
+        if (isInlineType()) {
+            throw new UnsupportedOperationException("Inline types cannot have unique IDs");
+        }
         return ref();
     }
 
     JDWP.ObjectReference.MonitorInfo jdwpMonitorInfo()
                              throws IncompatibleThreadStateException {
@@ -608,15 +638,23 @@
                                            " to " + destTypeName);
         }
     }
 
     public String toString() {
-        return "instance of " + referenceType().name() + "(id=" + uniqueID() + ")";
+        if (isInlineType()) {
+            return "instance of " + referenceType().name();
+        } else {
+            return "instance of " + referenceType().name() + "(id=" + uniqueID() + ")";
+        }
     }
 
     byte typeValueKey() {
-        return JDWP.Tag.OBJECT;
+        if (isInlineType()) {
+            return JDWP.Tag.INLINE_OBJECT;
+        } else {
+            return JDWP.Tag.OBJECT;
+        }
     }
 
     private static boolean isNonVirtual(int options) {
         return (options & INVOKE_NONVIRTUAL) != 0;
     }
diff a/src/jdk.jdi/share/classes/com/sun/tools/jdi/ValueImpl.java b/src/jdk.jdi/share/classes/com/sun/tools/jdi/ValueImpl.java
--- a/src/jdk.jdi/share/classes/com/sun/tools/jdi/ValueImpl.java
+++ b/src/jdk.jdi/share/classes/com/sun/tools/jdi/ValueImpl.java
@@ -42,10 +42,13 @@
         if (value == null) {
             JNITypeParser sig = new JNITypeParser(destination.signature());
             if (sig.isPrimitive()) {
                 throw new InvalidTypeException("Can't set a primitive type to null");
             }
+            if (destination.signature().charAt(0) == 'Q') {
+                throw new InvalidTypeException("Can't set an inline type to null");
+            }
             return null;    // no further checking or conversion necessary
         } else {
             return ((ValueImpl)value).prepareForAssignmentTo(destination);
         }
     }
diff a/src/jdk.jdwp.agent/share/native/libjdwp/signature.h b/src/jdk.jdwp.agent/share/native/libjdwp/signature.h
--- a/src/jdk.jdwp.agent/share/native/libjdwp/signature.h
+++ b/src/jdk.jdwp.agent/share/native/libjdwp/signature.h
@@ -27,10 +27,11 @@
 #define JDWP_SIGNATURE_H
 
 #define JVM_TYPE_ASSERT(typeKey)\
 JDI_ASSERT_MSG(JDWP_Tag_OBJECT == typeKey || \
                JDWP_Tag_ARRAY == typeKey || \
+               JDWP_Tag_INLINE_OBJECT == typeKey || \
                JDWP_Tag_BOOLEAN == typeKey || \
                JDWP_Tag_BYTE == typeKey || \
                JDWP_Tag_CHAR == typeKey || \
                JDWP_Tag_DOUBLE == typeKey || \
                JDWP_Tag_FLOAT == typeKey || \
@@ -45,11 +46,11 @@
      return signature[0];
 }
 
 static inline jboolean isReferenceTag(jbyte typeKey) {
     JVM_TYPE_ASSERT(typeKey);
-    return (typeKey == JDWP_TAG(OBJECT)) || (typeKey == JDWP_TAG(ARRAY));
+    return (typeKey == JDWP_TAG(OBJECT)) || (typeKey == JDWP_TAG(ARRAY)) || (typeKey == JDWP_TAG(INLINE_OBJECT));
 }
 
 static inline jboolean isArrayTag(jbyte typeKey) {
     JVM_TYPE_ASSERT(typeKey);
     return (typeKey == JDWP_TAG(ARRAY));
diff a/src/jdk.jdwp.agent/share/native/libjdwp/util.c b/src/jdk.jdwp.agent/share/native/libjdwp/util.c
--- a/src/jdk.jdwp.agent/share/native/libjdwp/util.c
+++ b/src/jdk.jdwp.agent/share/native/libjdwp/util.c
@@ -294,10 +294,11 @@
 }
 
 jboolean
 isObjectTag(jbyte tag) {
     return (tag == JDWP_TAG(OBJECT)) ||
+           (tag == JDWP_TAG(INLINE_OBJECT)) ||
            (tag == JDWP_TAG(STRING)) ||
            (tag == JDWP_TAG(THREAD)) ||
            (tag == JDWP_TAG(THREAD_GROUP)) ||
            (tag == JDWP_TAG(CLASS_LOADER)) ||
            (tag == JDWP_TAG(CLASS_OBJECT)) ||
@@ -360,11 +361,10 @@
     }
 
     /*
      * For primitive types, the type key is bounced back as is.
      */
-
     (void)outStream_writeByte(out, typeKey);
 
     switch (typeKey) {
         case JDWP_TAG(BYTE):
             (void)outStream_writeByte(out,
diff a/test/hotspot/jtreg/ProblemList.txt b/test/hotspot/jtreg/ProblemList.txt
--- a/test/hotspot/jtreg/ProblemList.txt
+++ b/test/hotspot/jtreg/ProblemList.txt
@@ -69,10 +69,87 @@
 compiler/rtm/locking/TestRTMSpinLoopCount.java 8183263 generic-x64
 compiler/rtm/locking/TestUseRTMDeopt.java 8183263 generic-x64
 compiler/rtm/locking/TestUseRTMXendForLockBusy.java 8183263 generic-x64
 compiler/rtm/print/TestPrintPreciseRTMLockingStatistics.java 8183263 generic-x64
 
+# Valhalla
+compiler/arguments/CheckCICompilerCount.java                        8205030 generic-all
+compiler/arguments/CheckCompileThresholdScaling.java                8205030 generic-all
+compiler/codecache/CheckSegmentedCodeCache.java                     8205030 generic-all
+compiler/codecache/cli/TestSegmentedCodeCacheOption.java            8205030 generic-all
+compiler/codecache/cli/codeheapsize/TestCodeHeapSizeOptions.java    8205030 generic-all
+compiler/codecache/cli/printcodecache/TestPrintCodeCacheOption.java 8205030 generic-all
+compiler/whitebox/OSRFailureLevel4Test.java                         8205030 generic-all
+
+compiler/aot/cli/DisabledAOTWithLibraryTest.java 8226295 generic-all
+compiler/aot/cli/SingleAOTOptionTest.java 8226295 generic-all
+compiler/aot/cli/MultipleAOTLibraryTest.java 8226295 generic-all
+compiler/aot/cli/jaotc/CompileClassWithDebugTest.java 8226295 generic-all
+compiler/aot/cli/jaotc/CompileModuleTest.java 8226295 generic-all
+compiler/aot/cli/jaotc/AtFileTest.java 8226295 generic-all
+compiler/aot/cli/jaotc/ListOptionWrongFileTest.java 8226295 generic-all
+compiler/aot/cli/jaotc/ClasspathOptionUnknownClassTest.java 8226295 generic-all
+compiler/aot/cli/jaotc/CompileDirectoryTest.java 8226295 generic-all
+compiler/aot/cli/jaotc/ListOptionTest.java 8226295 generic-all
+compiler/aot/cli/jaotc/ListOptionNotExistingTest.java 8226295 generic-all
+compiler/aot/cli/jaotc/CompileClassTest.java 8226295 generic-all
+compiler/aot/cli/jaotc/CompileJarTest.java 8226295 generic-all
+compiler/aot/cli/jaotc/IgnoreErrorsTest.java 8226295 generic-all
+compiler/aot/cli/jaotc/CompileAbsoluteDirectoryTest.java 8226295 generic-all
+compiler/aot/cli/NonExistingAOTLibraryTest.java 8226295 generic-all
+compiler/aot/cli/SingleAOTLibraryTest.java 8226295 generic-all
+compiler/aot/cli/IncorrectAOTLibraryTest.java 8226295 generic-all
+compiler/aot/RecompilationTest.java 8226295 generic-all
+compiler/aot/SharedUsageTest.java 8226295 generic-all
+compiler/aot/jdk.tools.jaotc.test/src/jdk/tools/jaotc/test/collect/ClassSearchTest.java 8226295 generic-all
+compiler/aot/jdk.tools.jaotc.test/src/jdk/tools/jaotc/test/collect/SearchPathTest.java 8226295 generic-all
+compiler/aot/jdk.tools.jaotc.test/src/jdk/tools/jaotc/test/collect/module/ModuleSourceProviderTest.java 8226295 generic-all
+compiler/aot/jdk.tools.jaotc.test/src/jdk/tools/jaotc/test/collect/ClassSourceTest.java 8226295 generic-all
+compiler/aot/jdk.tools.jaotc.test/src/jdk/tools/jaotc/test/collect/directory/DirectorySourceProviderTest.java 8226295 generic-all
+compiler/aot/jdk.tools.jaotc.test/src/jdk/tools/jaotc/test/collect/jar/JarSourceProviderTest.java 8226295 generic-all
+compiler/aot/jdk.tools.jaotc.test/src/jdk/tools/jaotc/test/NativeOrderOutputStreamTest.java 8226295 generic-all
+compiler/aot/verification/vmflags/TrackedFlagTest.java 8226295 generic-all
+compiler/aot/verification/vmflags/NotTrackedFlagTest.java 8226295 generic-all
+compiler/aot/verification/ClassAndLibraryNotMatchTest.java 8226295 generic-all
+compiler/aot/DeoptimizationTest.java 8226295 generic-all
+compiler/aot/calls/fromAot/AotInvokeVirtual2NativeTest.java 8226295 generic-all
+compiler/aot/calls/fromAot/AotInvokeStatic2CompiledTest.java 8226295 generic-all
+compiler/aot/calls/fromAot/AotInvokeSpecial2CompiledTest.java 8226295 generic-all
+compiler/aot/calls/fromAot/AotInvokeVirtual2CompiledTest.java 8226295 generic-all
+compiler/aot/calls/fromAot/AotInvokeVirtual2InterpretedTest.java 8226295 generic-all
+compiler/aot/calls/fromAot/AotInvokeDynamic2NativeTest.java 8226295 generic-all
+compiler/aot/calls/fromAot/AotInvokeDynamic2InterpretedTest.java 8226295 generic-all
+compiler/aot/calls/fromAot/AotInvokeStatic2InterpretedTest.java 8226295 generic-all
+compiler/aot/calls/fromAot/AotInvokeInterface2InterpretedTest.java 8226295 generic-all
+compiler/aot/calls/fromAot/AotInvokeVirtual2AotTest.java 8226295 generic-all
+compiler/aot/calls/fromAot/AotInvokeInterface2AotTest.java 8226295 generic-all
+compiler/aot/calls/fromAot/AotInvokeSpecial2NativeTest.java 8226295 generic-all
+compiler/aot/calls/fromAot/AotInvokeDynamic2AotTest.java 8226295 generic-all
+compiler/aot/calls/fromAot/AotInvokeInterface2NativeTest.java 8226295 generic-all
+compiler/aot/calls/fromAot/AotInvokeStatic2AotTest.java 8226295 generic-all
+compiler/aot/calls/fromAot/AotInvokeInterface2CompiledTest.java 8226295 generic-all
+compiler/aot/calls/fromAot/AotInvokeDynamic2CompiledTest.java 8226295 generic-all
+compiler/aot/calls/fromAot/AotInvokeStatic2NativeTest.java 8226295 generic-all
+compiler/aot/calls/fromAot/AotInvokeSpecial2AotTest.java 8226295 generic-all
+compiler/aot/calls/fromAot/AotInvokeSpecial2InterpretedTest.java 8226295 generic-all
+compiler/aot/calls/fromNative/NativeInvokeStatic2AotTest.java 8226295 generic-all
+compiler/aot/calls/fromNative/NativeInvokeSpecial2AotTest.java 8226295 generic-all
+compiler/aot/calls/fromNative/NativeInvokeVirtual2AotTest.java 8226295 generic-all
+compiler/aot/calls/fromCompiled/CompiledInvokeInterface2AotTest.java 8226295 generic-all
+compiler/aot/calls/fromCompiled/CompiledInvokeVirtual2AotTest.java 8226295 generic-all
+compiler/aot/calls/fromCompiled/CompiledInvokeDynamic2AotTest.java 8226295 generic-all
+compiler/aot/calls/fromCompiled/CompiledInvokeStatic2AotTest.java 8226295 generic-all
+compiler/aot/calls/fromCompiled/CompiledInvokeSpecial2AotTest.java 8226295 generic-all
+compiler/aot/calls/fromInterpreted/InterpretedInvokeDynamic2AotTest.java 8226295 generic-all
+compiler/aot/calls/fromInterpreted/InterpretedInvokeStatic2AotTest.java 8226295 generic-all
+compiler/aot/calls/fromInterpreted/InterpretedInvokeVirtual2AotTest.java 8226295 generic-all
+compiler/aot/calls/fromInterpreted/InterpretedInvokeSpecial2AotTest.java 8226295 generic-all
+compiler/aot/calls/fromInterpreted/InterpretedInvokeInterface2AotTest.java 8226295 generic-all
+compiler/aot/fingerprint/SelfChanged.java 8226295 generic-all
+compiler/aot/fingerprint/SelfChangedCDS.java 8226295 generic-all
+compiler/aot/fingerprint/SuperChanged.java 8226295 generic-all
+
 compiler/c2/Test8004741.java 8235801 generic-all
 
 compiler/jsr292/CreatesInterfaceDotEqualsCallInfo.java 8242923 generic-all
 
 #############################################################################
@@ -96,10 +173,32 @@
 runtime/jni/terminatedThread/TestTerminatedThread.java 8219652 aix-ppc64
 runtime/ReservedStack/ReservedStackTest.java 8231031 generic-all
 
 runtime/cds/appcds/SignedJar.java 8245264 generic-all
 
+# Valhalla TODO:
+runtime/CompressedOops/CompressedClassPointers.java 8210258 generic-all
+runtime/RedefineTests/RedefineLeak.java 8205032 generic-all
+runtime/SharedArchiveFile/BootAppendTests.java 8210258 generic-all
+runtime/SharedArchiveFile/CdsDifferentCompactStrings.java 8210258 generic-all
+runtime/SharedArchiveFile/CdsDifferentObjectAlignment.java 8210258 generic-all
+runtime/SharedArchiveFile/NonBootLoaderClasses.java 8210258 generic-all
+runtime/SharedArchiveFile/PrintSharedArchiveAndExit.java 8210258 generic-all
+runtime/SharedArchiveFile/SharedArchiveFile.java 8210258 generic-all
+runtime/SharedArchiveFile/SharedStringsDedup.java 8210258 generic-all
+runtime/SharedArchiveFile/SharedStringsRunAuto.java 8210258 generic-all
+runtime/SharedArchiveFile/SharedSymbolTableBucketSize.java 8210258 generic-all
+runtime/SharedArchiveFile/SpaceUtilizationCheck.java 8210258 generic-all
+runtime/SharedArchiveFile/TestInterpreterMethodEntries.java 8210258 generic-all
+runtime/SharedArchiveFile/serviceability/transformRelatedClasses/TransformInterfaceAndImplementor.java 8210258 generic-all
+runtime/SharedArchiveFile/serviceability/transformRelatedClasses/TransformSuperAndSubClasses.java 8210258 generic-all
+runtime/SharedArchiveFile/serviceability/transformRelatedClasses/TransformSuperSubTwoPckgs.java 8210258 generic-all
+runtime/appcds/ClassLoaderTest.java 8210258 generic-all
+runtime/appcds/HelloTest.java 8210258 generic-all
+runtime/appcds/sharedStrings/SharedStringsBasic.java 8210258 generic-all
+
+
 #############################################################################
 
 # :hotspot_serviceability
 
 serviceability/sa/sadebugd/DebugdConnectTest.java 8239062 macosx-x64
@@ -109,10 +208,36 @@
 
 serviceability/jvmti/HeapMonitor/MyPackage/HeapMonitorStatIntervalTest.java 8214032 generic-all
 serviceability/jvmti/HeapMonitor/MyPackage/HeapMonitorStatArrayCorrectnessTest.java 8224150 generic-all
 serviceability/jvmti/HiddenClass/P/Q/HiddenClassSigTest.java 8244571 generic-all
 
+# Valhalla TODO:
+serviceability/sa/ClhsdbCDSCore.java 8190936 generic-all
+serviceability/sa/ClhsdbCDSJstackPrintAll.java 8190936 generic-all
+serviceability/sa/ClhsdbFindPC.java 8190936 generic-all
+serviceability/sa/ClhsdbInspect.java 8190936 generic-all
+serviceability/sa/ClhsdbJdis.java 8190936 generic-all
+serviceability/sa/ClhsdbJstack.java 8190936 generic-all
+serviceability/sa/ClhsdbPrintAll.java 8190936 generic-all
+serviceability/sa/ClhsdbPrintAs.java 8190936 generic-all
+serviceability/sa/ClhsdbPrintStatics.java 8190936 generic-all
+serviceability/sa/ClhsdbSource.java 8190936 generic-all
+serviceability/sa/ClhsdbSymbol.java 8190936 generic-all
+serviceability/sa/ClhsdbWhere.java 8190936 generic-all
+serviceability/sa/JhsdbThreadInfoTest.java 8190936 generic-all
+serviceability/sa/TestClassDump.java 8190936 generic-all
+serviceability/sa/TestClhsdbJstackLock.java 8190936 generic-all
+serviceability/sa/TestCpoolForInvokeDynamic.java 8190936 generic-all
+serviceability/sa/TestHeapDumpForInvokeDynamic.java 8190936 generic-all
+serviceability/sa/TestHeapDumpForLargeArray.java 8190936 generic-all
+serviceability/sa/TestIntConstant.java 8190936 generic-all
+serviceability/sa/TestJhsdbJstackLock.java 8190936 generic-all
+serviceability/sa/TestJmapCore.java 8190936 generic-all
+serviceability/sa/TestJmapCoreMetaspace.java 8190936 generic-all
+serviceability/sa/TestPrintMdo.java 8190936 generic-all
+serviceability/sa/jmap-hprof/JMapHProfLargeHeapTest.java 8190936 generic-all
+
 #############################################################################
 
 # :hotspot_misc
 
 #############################################################################
diff a/test/hotspot/jtreg/runtime/cds/appcds/dynamicArchive/DynamicArchiveRelocationTest.java b/test/hotspot/jtreg/runtime/cds/appcds/dynamicArchive/DynamicArchiveRelocationTest.java
--- a/test/hotspot/jtreg/runtime/cds/appcds/dynamicArchive/DynamicArchiveRelocationTest.java
+++ b/test/hotspot/jtreg/runtime/cds/appcds/dynamicArchive/DynamicArchiveRelocationTest.java
@@ -28,13 +28,13 @@
  * @requires vm.cds
  * @summary Testing relocation of dynamic CDS archive (during both dump time and run time)
  * @comment JDK-8231610 Relocate the CDS archive if it cannot be mapped to the requested address
  * @bug 8231610
  * @library /test/lib /test/hotspot/jtreg/runtime/cds/appcds /test/hotspot/jtreg/runtime/cds/appcds/test-classes
- * @build Hello
+ * @build HelloRelocation
  * @build sun.hotspot.WhiteBox
- * @run driver ClassFileInstaller -jar hello.jar Hello
+ * @run driver ClassFileInstaller -jar hello.jar HelloRelocation HelloInlineClassApp HelloInlineClassApp$Point HelloInlineClassApp$Point$ref
  * @run driver ClassFileInstaller sun.hotspot.WhiteBox sun.hotspot.WhiteBox$WhiteBoxPermission
  * @run main/othervm -XX:+UnlockDiagnosticVMOptions -XX:+WhiteBoxAPI -Xbootclasspath/a:. DynamicArchiveRelocationTest
  */
 
 import jdk.test.lib.process.OutputAnalyzer;
@@ -78,11 +78,11 @@
                            + ", top = " + dump_top_reloc
                            + ", run = " + run_reloc);
         System.out.println("============================================================");
 
         String appJar = ClassFileInstaller.getJarPath("hello.jar");
-        String mainClass = "Hello";
+        String mainClass = "HelloRelocation";
         String forceRelocation = "-XX:ArchiveRelocationMode=1";
         String dumpBaseRelocArg = dump_base_reloc ? forceRelocation : "-showversion";
         String dumpTopRelocArg  = dump_top_reloc  ? forceRelocation : "-showversion";
         String runRelocArg      = run_reloc       ? forceRelocation : "-showversion";
         String logArg = "-Xlog:cds=debug,cds+reloc=debug";
diff a/test/lib/sun/hotspot/WhiteBox.java b/test/lib/sun/hotspot/WhiteBox.java
--- a/test/lib/sun/hotspot/WhiteBox.java
+++ b/test/lib/sun/hotspot/WhiteBox.java
@@ -144,10 +144,24 @@
   private native int encodeConstantPoolIndyIndex0(int index);
   public         int encodeConstantPoolIndyIndex(int index) {
     return encodeConstantPoolIndyIndex0(index);
   }
 
+  private native Object[] getObjectsViaKlassOopMaps0(Object thing);
+  public Object[] getObjectsViaKlassOopMaps(Object thing) {
+    Objects.requireNonNull(thing);
+    return getObjectsViaKlassOopMaps0(thing);
+  }
+
+  private native Object[] getObjectsViaOopIterator0(Object thing);
+  public Object[] getObjectsViaOopIterator(Object thing) {
+    Objects.requireNonNull(thing);
+    return getObjectsViaOopIterator0(thing);
+  }
+
+  public native Object[] getObjectsViaFrameOopIterator(int depth);
+
   // JVMTI
   private native void addToBootstrapClassLoaderSearch0(String segment);
   public         void addToBootstrapClassLoaderSearch(String segment){
     Objects.requireNonNull(segment);
     addToBootstrapClassLoaderSearch0(segment);
