<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Old src/hotspot/share/opto/bytecodeInfo.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
  <body>
    <pre>
  1 /*
  2  * Copyright (c) 1998, 2020, Oracle and/or its affiliates. All rights reserved.
  3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  4  *
  5  * This code is free software; you can redistribute it and/or modify it
  6  * under the terms of the GNU General Public License version 2 only, as
  7  * published by the Free Software Foundation.
  8  *
  9  * This code is distributed in the hope that it will be useful, but WITHOUT
 10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
 11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
 12  * version 2 for more details (a copy is included in the LICENSE file that
 13  * accompanied this code).
 14  *
 15  * You should have received a copy of the GNU General Public License version
 16  * 2 along with this work; if not, write to the Free Software Foundation,
 17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 18  *
 19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
 20  * or visit www.oracle.com if you need additional information or have any
 21  * questions.
 22  *
 23  */
 24 
 25 #include &quot;precompiled.hpp&quot;
 26 #include &quot;ci/ciReplay.hpp&quot;
 27 #include &quot;classfile/systemDictionary.hpp&quot;
 28 #include &quot;classfile/vmSymbols.hpp&quot;
 29 #include &quot;compiler/compileBroker.hpp&quot;
 30 #include &quot;compiler/compilerEvent.hpp&quot;
 31 #include &quot;compiler/compileLog.hpp&quot;
 32 #include &quot;interpreter/linkResolver.hpp&quot;
 33 #include &quot;jfr/jfrEvents.hpp&quot;
 34 #include &quot;oops/objArrayKlass.hpp&quot;
 35 #include &quot;opto/callGenerator.hpp&quot;
 36 #include &quot;opto/parse.hpp&quot;
 37 #include &quot;runtime/handles.inline.hpp&quot;
 38 #include &quot;utilities/events.hpp&quot;
 39 
 40 //=============================================================================
 41 //------------------------------InlineTree-------------------------------------
 42 InlineTree::InlineTree(Compile* c,
 43                        const InlineTree *caller_tree, ciMethod* callee,
 44                        JVMState* caller_jvms, int caller_bci,
 45                        float site_invoke_ratio, int max_inline_level) :
 46   C(c),
 47   _caller_jvms(caller_jvms),
 48   _method(callee),
 49   _caller_tree((InlineTree*) caller_tree),
 50   _count_inline_bcs(method()-&gt;code_size_for_inlining()),
 51   _site_invoke_ratio(site_invoke_ratio),
 52   _max_inline_level(max_inline_level),
 53   _subtrees(c-&gt;comp_arena(), 2, 0, NULL),
 54   _msg(NULL)
 55 {
 56 #ifndef PRODUCT
 57   _count_inlines = 0;
 58   _forced_inline = false;
 59 #endif
 60   if (_caller_jvms != NULL) {
 61     // Keep a private copy of the caller_jvms:
 62     _caller_jvms = new (C) JVMState(caller_jvms-&gt;method(), caller_tree-&gt;caller_jvms());
 63     _caller_jvms-&gt;set_bci(caller_jvms-&gt;bci());
 64     assert(!caller_jvms-&gt;should_reexecute(), &quot;there should be no reexecute bytecode with inlining&quot;);
 65   }
 66   assert(_caller_jvms-&gt;same_calls_as(caller_jvms), &quot;consistent JVMS&quot;);
 67   assert((caller_tree == NULL ? 0 : caller_tree-&gt;stack_depth() + 1) == stack_depth(), &quot;correct (redundant) depth parameter&quot;);
 68   assert(caller_bci == this-&gt;caller_bci(), &quot;correct (redundant) bci parameter&quot;);
 69   // Update hierarchical counts, count_inline_bcs() and count_inlines()
 70   InlineTree *caller = (InlineTree *)caller_tree;
 71   for( ; caller != NULL; caller = ((InlineTree *)(caller-&gt;caller_tree())) ) {
 72     caller-&gt;_count_inline_bcs += count_inline_bcs();
 73     NOT_PRODUCT(caller-&gt;_count_inlines++;)
 74   }
 75 }
 76 
 77 /**
 78  *  Return true when EA is ON and a java constructor is called or
 79  *  a super constructor is called from an inlined java constructor.
 80  *  Also return true for boxing methods.
 81  *  Also return true for methods returning Iterator (including Iterable::iterator())
 82  *  that is essential for forall-loops performance.
 83  */
 84 static bool is_init_with_ea(ciMethod* callee_method,
 85                             ciMethod* caller_method, Compile* C) {
 86   if (!C-&gt;do_escape_analysis() || !EliminateAllocations) {
 87     return false; // EA is off
 88   }
 89   if (callee_method-&gt;is_initializer()) {
 90     return true; // constuctor
 91   }
 92   if (caller_method-&gt;is_initializer() &amp;&amp;
 93       caller_method != C-&gt;method() &amp;&amp;
 94       caller_method-&gt;holder()-&gt;is_subclass_of(callee_method-&gt;holder())) {
 95     return true; // super constructor is called from inlined constructor
 96   }
 97   if (C-&gt;eliminate_boxing() &amp;&amp; callee_method-&gt;is_boxing_method()) {
 98     return true;
 99   }
100   ciType *retType = callee_method-&gt;signature()-&gt;return_type();
101   ciKlass *iter = C-&gt;env()-&gt;Iterator_klass();
102   if(retType-&gt;is_loaded() &amp;&amp; iter-&gt;is_loaded() &amp;&amp; retType-&gt;is_subtype_of(iter)) {
103     return true;
104   }
105   return false;
106 }
107 
108 /**
109  *  Force inlining unboxing accessor.
110  */
111 static bool is_unboxing_method(ciMethod* callee_method, Compile* C) {
112   return C-&gt;eliminate_boxing() &amp;&amp; callee_method-&gt;is_unboxing_method();
113 }
114 
115 // positive filter: should callee be inlined?
116 bool InlineTree::should_inline(ciMethod* callee_method, ciMethod* caller_method,
117                                int caller_bci, ciCallProfile&amp; profile,
118                                WarmCallInfo* wci_result) {
119   // Allows targeted inlining
120   if (C-&gt;directive()-&gt;should_inline(callee_method)) {
121     *wci_result = *(WarmCallInfo::always_hot());
122     if (C-&gt;print_inlining() &amp;&amp; Verbose) {
123       CompileTask::print_inline_indent(inline_level());
124       tty-&gt;print_cr(&quot;Inlined method is hot: &quot;);
125     }
126     set_msg(&quot;force inline by CompileCommand&quot;);
127     _forced_inline = true;
128     return true;
129   }
130 
131   if (callee_method-&gt;force_inline()) {
132       set_msg(&quot;force inline by annotation&quot;);
133       _forced_inline = true;
134       return true;
135   }
136 
137 #ifndef PRODUCT
138   int inline_depth = inline_level()+1;
139   if (ciReplay::should_inline(C-&gt;replay_inline_data(), callee_method, caller_bci, inline_depth)) {
140     set_msg(&quot;force inline by ciReplay&quot;);
141     _forced_inline = true;
142     return true;
143   }
144 #endif
145 
146   int size = callee_method-&gt;code_size_for_inlining();
147 
148   // Check for too many throws (and not too huge)
149   if(callee_method-&gt;interpreter_throwout_count() &gt; InlineThrowCount &amp;&amp;
150      size &lt; InlineThrowMaxSize ) {
151     wci_result-&gt;set_profit(wci_result-&gt;profit() * 100);
152     if (C-&gt;print_inlining() &amp;&amp; Verbose) {
153       CompileTask::print_inline_indent(inline_level());
154       tty-&gt;print_cr(&quot;Inlined method with many throws (throws=%d):&quot;, callee_method-&gt;interpreter_throwout_count());
155     }
156     set_msg(&quot;many throws&quot;);
157     return true;
158   }
159 
160   int default_max_inline_size = C-&gt;max_inline_size();
161   int inline_small_code_size  = InlineSmallCode / 4;
162   int max_inline_size         = default_max_inline_size;
163 
164   int call_site_count  = method()-&gt;scale_count(profile.count());
165   int invoke_count     = method()-&gt;interpreter_invocation_count();
166 
167   assert(invoke_count != 0, &quot;require invocation count greater than zero&quot;);
168   int freq = call_site_count / invoke_count;
169 
170   // bump the max size if the call is frequent
171   if ((freq &gt;= InlineFrequencyRatio) ||
172       (call_site_count &gt;= InlineFrequencyCount) ||
173       is_unboxing_method(callee_method, C) ||
174       is_init_with_ea(callee_method, caller_method, C)) {
175 
176     max_inline_size = C-&gt;freq_inline_size();
177     if (size &lt;= max_inline_size &amp;&amp; TraceFrequencyInlining) {
178       CompileTask::print_inline_indent(inline_level());
179       tty-&gt;print_cr(&quot;Inlined frequent method (freq=%d count=%d):&quot;, freq, call_site_count);
180       CompileTask::print_inline_indent(inline_level());
181       callee_method-&gt;print();
182       tty-&gt;cr();
183     }
184   } else {
185     // Not hot.  Check for medium-sized pre-existing nmethod at cold sites.
186     if (callee_method-&gt;has_compiled_code() &amp;&amp;
187         callee_method-&gt;instructions_size() &gt; inline_small_code_size) {
188       set_msg(&quot;already compiled into a medium method&quot;);
189       return false;
190     }
191   }
192   if (size &gt; max_inline_size) {
193     if (max_inline_size &gt; default_max_inline_size) {
194       set_msg(&quot;hot method too big&quot;);
195     } else {
196       set_msg(&quot;too big&quot;);
197     }
198     return false;
199   }
200   return true;
201 }
202 
203 
204 // negative filter: should callee NOT be inlined?
205 bool InlineTree::should_not_inline(ciMethod *callee_method,
206                                    ciMethod* caller_method,
207                                    JVMState* jvms,
208                                    WarmCallInfo* wci_result) {
209 
210   const char* fail_msg = NULL;
211 
212   // First check all inlining restrictions which are required for correctness
213   if (callee_method-&gt;is_abstract()) {
214     fail_msg = &quot;abstract method&quot;; // // note: we allow ik-&gt;is_abstract()
215   } else if (!callee_method-&gt;holder()-&gt;is_initialized() &amp;&amp;
216              // access allowed in the context of static initializer
217              C-&gt;needs_clinit_barrier(callee_method-&gt;holder(), caller_method)) {
218     fail_msg = &quot;method holder not initialized&quot;;
219   } else if (callee_method-&gt;is_native()) {
220     fail_msg = &quot;native method&quot;;
221   } else if (callee_method-&gt;dont_inline()) {
222     fail_msg = &quot;don&#39;t inline by annotation&quot;;
223   }
224 
225   // one more inlining restriction
226   if (fail_msg == NULL &amp;&amp; callee_method-&gt;has_unloaded_classes_in_signature()) {
227     fail_msg = &quot;unloaded signature classes&quot;;
228   }
229 
230   if (fail_msg != NULL) {
231     set_msg(fail_msg);
232     return true;
233   }
234 
235   // ignore heuristic controls on inlining
236   if (C-&gt;directive()-&gt;should_inline(callee_method)) {
237     set_msg(&quot;force inline by CompileCommand&quot;);
238     return false;
239   }
240 
241   if (C-&gt;directive()-&gt;should_not_inline(callee_method)) {
242     set_msg(&quot;disallowed by CompileCommand&quot;);
243     return true;
244   }
245 
246 #ifndef PRODUCT
247   int caller_bci = jvms-&gt;bci();
248   int inline_depth = inline_level()+1;
249   if (ciReplay::should_inline(C-&gt;replay_inline_data(), callee_method, caller_bci, inline_depth)) {
250     set_msg(&quot;force inline by ciReplay&quot;);
251     return false;
252   }
253 
254   if (ciReplay::should_not_inline(C-&gt;replay_inline_data(), callee_method, caller_bci, inline_depth)) {
255     set_msg(&quot;disallowed by ciReplay&quot;);
256     return true;
257   }
258 
259   if (ciReplay::should_not_inline(callee_method)) {
260     set_msg(&quot;disallowed by ciReplay&quot;);
261     return true;
262   }
263 #endif
264 
265   if (callee_method-&gt;force_inline()) {
266     set_msg(&quot;force inline by annotation&quot;);
267     return false;
268   }
269 
270   // Now perform checks which are heuristic
271 
272   if (is_unboxing_method(callee_method, C)) {
273     // Inline unboxing methods.
274     return false;
275   }
276 
277   if (callee_method-&gt;has_compiled_code() &amp;&amp;
278       callee_method-&gt;instructions_size() &gt; InlineSmallCode) {
279     set_msg(&quot;already compiled into a big method&quot;);
280     return true;
281   }
282 
283   // don&#39;t inline exception code unless the top method belongs to an
284   // exception class
285   if (caller_tree() != NULL &amp;&amp;
286       callee_method-&gt;holder()-&gt;is_subclass_of(C-&gt;env()-&gt;Throwable_klass())) {
287     const InlineTree *top = this;
288     while (top-&gt;caller_tree() != NULL) top = top-&gt;caller_tree();
289     ciInstanceKlass* k = top-&gt;method()-&gt;holder();
290     if (!k-&gt;is_subclass_of(C-&gt;env()-&gt;Throwable_klass())) {
291       set_msg(&quot;exception method&quot;);
292       return true;
293     }
294   }
295 
296   // use frequency-based objections only for non-trivial methods
297   if (callee_method-&gt;code_size() &lt;= MaxTrivialSize) {
298     return false;
299   }
300 
301   // don&#39;t use counts with -Xcomp
302   if (UseInterpreter) {
303 
304     if (!callee_method-&gt;has_compiled_code() &amp;&amp;
305         !callee_method-&gt;was_executed_more_than(0)) {
306       set_msg(&quot;never executed&quot;);
307       return true;
308     }
309 
310     if (is_init_with_ea(callee_method, caller_method, C)) {
311       // Escape Analysis: inline all executed constructors
312       return false;
313     } else {
314       intx counter_high_value;
315       // Tiered compilation uses a different &quot;high value&quot; than non-tiered compilation.
316       // Determine the right value to use.
317       if (TieredCompilation) {
318         counter_high_value = InvocationCounter::count_limit / 2;
319       } else {
320         counter_high_value = CompileThreshold / 2;
321       }
322       if (!callee_method-&gt;was_executed_more_than(MIN2(MinInliningThreshold, counter_high_value))) {
323         set_msg(&quot;executed &lt; MinInliningThreshold times&quot;);
324         return true;
325       }
326     }
327   }
328 
329   return false;
330 }
331 
332 bool InlineTree::is_not_reached(ciMethod* callee_method, ciMethod* caller_method, int caller_bci, ciCallProfile&amp; profile) {
333   if (!UseInterpreter) {
334     return false; // -Xcomp
335   }
336   if (profile.count() &gt; 0) {
337     return false; // reachable according to profile
338   }
339   if (!callee_method-&gt;was_executed_more_than(0)) {
340     return true; // callee was never executed
341   }
342   if (caller_method-&gt;is_not_reached(caller_bci)) {
343     return true; // call site not resolved
344   }
345   if (profile.count() == -1) {
346     return false; // immature profile; optimistically treat as reached
347   }
348   assert(profile.count() == 0, &quot;sanity&quot;);
349 
350   // Profile info is scarce.
351   // Try to guess: check if the call site belongs to a start block.
352   // Call sites in a start block should be reachable if no exception is thrown earlier.
353   ciMethodBlocks* caller_blocks = caller_method-&gt;get_method_blocks();
354   bool is_start_block = caller_blocks-&gt;block_containing(caller_bci)-&gt;start_bci() == 0;
355   if (is_start_block) {
356     return false; // treat the call reached as part of start block
357   }
358   return true; // give up and treat the call site as not reached
359 }
360 
361 //-----------------------------try_to_inline-----------------------------------
362 // return true if ok
363 // Relocated from &quot;InliningClosure::try_to_inline&quot;
364 bool InlineTree::try_to_inline(ciMethod* callee_method, ciMethod* caller_method,
365                                int caller_bci, JVMState* jvms, ciCallProfile&amp; profile,
366                                WarmCallInfo* wci_result, bool&amp; should_delay) {
367 
368   if (ClipInlining &amp;&amp; (int)count_inline_bcs() &gt;= DesiredMethodLimit) {
369     if (!callee_method-&gt;force_inline() || !IncrementalInline) {
370       set_msg(&quot;size &gt; DesiredMethodLimit&quot;);
371       return false;
372     } else if (!C-&gt;inlining_incrementally()) {
373       should_delay = true;
374     }
375   }
376 
377   _forced_inline = false; // Reset
378   if (!should_inline(callee_method, caller_method, caller_bci, profile,
379                      wci_result)) {
380     return false;
381   }
382   if (should_not_inline(callee_method, caller_method, jvms, wci_result)) {
383     return false;
384   }
385 
386   if (InlineAccessors &amp;&amp; callee_method-&gt;is_accessor()) {
387     // accessor methods are not subject to any of the following limits.
388     set_msg(&quot;accessor&quot;);
389     return true;
390   }
391 
392   // suppress a few checks for accessors and trivial methods
393   if (callee_method-&gt;code_size() &gt; MaxTrivialSize) {
394 
395     // don&#39;t inline into giant methods
396     if (C-&gt;over_inlining_cutoff()) {
397       if ((!callee_method-&gt;force_inline() &amp;&amp; !caller_method-&gt;is_compiled_lambda_form())
398           || !IncrementalInline) {
399         set_msg(&quot;NodeCountInliningCutoff&quot;);
400         return false;
401       } else {
402         should_delay = true;
403       }
404     }
405 
406     if (!UseInterpreter &amp;&amp;
407         is_init_with_ea(callee_method, caller_method, C)) {
408       // Escape Analysis stress testing when running Xcomp:
409       // inline constructors even if they are not reached.
410     } else if (forced_inline()) {
411       // Inlining was forced by CompilerOracle, ciReplay or annotation
412     } else if (is_not_reached(callee_method, caller_method, caller_bci, profile)) {
413       // don&#39;t inline unreached call sites
414        set_msg(&quot;call site not reached&quot;);
415        return false;
416     }
417   }
418 
419   if (!C-&gt;do_inlining() &amp;&amp; InlineAccessors) {
420     set_msg(&quot;not an accessor&quot;);
421     return false;
422   }
423 
424   // Limit inlining depth in case inlining is forced or
425   // _max_inline_level was increased to compensate for lambda forms.
426   if (inline_level() &gt; MaxForceInlineLevel) {
427     set_msg(&quot;MaxForceInlineLevel&quot;);
428     return false;
429   }
430   if (inline_level() &gt; _max_inline_level) {
431     if (!callee_method-&gt;force_inline() || !IncrementalInline) {
432       set_msg(&quot;inlining too deep&quot;);
433       return false;
434     } else if (!C-&gt;inlining_incrementally()) {
435       should_delay = true;
436     }
437   }
438 
439   // detect direct and indirect recursive inlining
440   {
441     // count the current method and the callee
442     const bool is_compiled_lambda_form = callee_method-&gt;is_compiled_lambda_form();
443     int inline_level = 0;
444     if (!is_compiled_lambda_form) {
445       if (method() == callee_method) {
446         inline_level++;
447       }
448     }
449     // count callers of current method and callee
450     Node* callee_argument0 = is_compiled_lambda_form ? jvms-&gt;map()-&gt;argument(jvms, 0)-&gt;uncast() : NULL;
451     for (JVMState* j = jvms-&gt;caller(); j != NULL &amp;&amp; j-&gt;has_method(); j = j-&gt;caller()) {
452       if (j-&gt;method() == callee_method) {
453         if (is_compiled_lambda_form) {
454           // Since compiled lambda forms are heavily reused we allow recursive inlining.  If it is truly
455           // a recursion (using the same &quot;receiver&quot;) we limit inlining otherwise we can easily blow the
456           // compiler stack.
457           Node* caller_argument0 = j-&gt;map()-&gt;argument(j, 0)-&gt;uncast();
458           if (caller_argument0 == callee_argument0) {
459             inline_level++;
460           }
461         } else {
462           inline_level++;
463         }
464       }
465     }
466     if (inline_level &gt; MaxRecursiveInlineLevel) {
467       set_msg(&quot;recursive inlining is too deep&quot;);
468       return false;
469     }
470   }
471 
472   int size = callee_method-&gt;code_size_for_inlining();
473 
474   if (ClipInlining &amp;&amp; (int)count_inline_bcs() + size &gt;= DesiredMethodLimit) {
475     if (!callee_method-&gt;force_inline() || !IncrementalInline) {
476       set_msg(&quot;size &gt; DesiredMethodLimit&quot;);
477       return false;
478     } else if (!C-&gt;inlining_incrementally()) {
479       should_delay = true;
480     }
481   }
482 
483   // ok, inline this method
484   return true;
485 }
486 
487 //------------------------------pass_initial_checks----------------------------
488 bool InlineTree::pass_initial_checks(ciMethod* caller_method, int caller_bci, ciMethod* callee_method) {
489   // Check if a callee_method was suggested
490   if (callee_method == NULL) {
491     return false;
492   }
493   ciInstanceKlass *callee_holder = callee_method-&gt;holder();
494   // Check if klass of callee_method is loaded
495   if (!callee_holder-&gt;is_loaded()) {
496     return false;
497   }
498   if (!callee_holder-&gt;is_initialized() &amp;&amp;
499       // access allowed in the context of static initializer
500       C-&gt;needs_clinit_barrier(callee_holder, caller_method)) {
501     return false;
502   }
503   if( !UseInterpreter ) /* running Xcomp */ {
504     // Checks that constant pool&#39;s call site has been visited
505     // stricter than callee_holder-&gt;is_initialized()
506     ciBytecodeStream iter(caller_method);
507     iter.force_bci(caller_bci);
508     Bytecodes::Code call_bc = iter.cur_bc();
509     // An invokedynamic instruction does not have a klass.
510     if (call_bc != Bytecodes::_invokedynamic) {
511       int index = iter.get_index_u2_cpcache();
512       if (!caller_method-&gt;is_klass_loaded(index, true)) {
513         return false;
514       }
515       // Try to do constant pool resolution if running Xcomp
516       if( !caller_method-&gt;check_call(index, call_bc == Bytecodes::_invokestatic) ) {
517         return false;
518       }
519     }
520   }
521   return true;
522 }
523 
524 //------------------------------check_can_parse--------------------------------
525 const char* InlineTree::check_can_parse(ciMethod* callee) {
526   // Certain methods cannot be parsed at all:
527   if ( callee-&gt;is_native())                     return &quot;native method&quot;;
528   if ( callee-&gt;is_abstract())                   return &quot;abstract method&quot;;
529   if (!callee-&gt;has_balanced_monitors())         return &quot;not compilable (unbalanced monitors)&quot;;
530   if ( callee-&gt;get_flow_analysis()-&gt;failing())  return &quot;not compilable (flow analysis failed)&quot;;
531   if (!callee-&gt;can_be_parsed())                 return &quot;cannot be parsed&quot;;
532   return NULL;
533 }
534 
535 //------------------------------print_inlining---------------------------------
536 void InlineTree::print_inlining(ciMethod* callee_method, int caller_bci,
537                                 ciMethod* caller_method, bool success) const {
538   const char* inline_msg = msg();
539   assert(inline_msg != NULL, &quot;just checking&quot;);
540   if (C-&gt;log() != NULL) {
541     if (success) {
542       C-&gt;log()-&gt;inline_success(inline_msg);
543     } else {
544       C-&gt;log()-&gt;inline_fail(inline_msg);
545     }
546   }
547   CompileTask::print_inlining_ul(callee_method, inline_level(),
548                                                caller_bci, inline_msg);
549   if (C-&gt;print_inlining()) {
550     C-&gt;print_inlining(callee_method, inline_level(), caller_bci, inline_msg);
551     guarantee(callee_method != NULL, &quot;would crash in CompilerEvent::InlineEvent::post&quot;);
552     if (Verbose) {
553       const InlineTree *top = this;
554       while (top-&gt;caller_tree() != NULL) { top = top-&gt;caller_tree(); }
555       //tty-&gt;print(&quot;  bcs: %d+%d  invoked: %d&quot;, top-&gt;count_inline_bcs(), callee_method-&gt;code_size(), callee_method-&gt;interpreter_invocation_count());
556     }
557   }
558   EventCompilerInlining event;
559   if (event.should_commit()) {
560     CompilerEvent::InlineEvent::post(event, C-&gt;compile_id(), caller_method-&gt;get_Method(), callee_method, success, inline_msg, caller_bci);
561   }
562 }
563 
564 //------------------------------ok_to_inline-----------------------------------
565 WarmCallInfo* InlineTree::ok_to_inline(ciMethod* callee_method, JVMState* jvms, ciCallProfile&amp; profile, WarmCallInfo* initial_wci, bool&amp; should_delay) {
566   assert(callee_method != NULL, &quot;caller checks for optimized virtual!&quot;);
567   assert(!should_delay, &quot;should be initialized to false&quot;);
568 #ifdef ASSERT
569   // Make sure the incoming jvms has the same information content as me.
570   // This means that we can eventually make this whole class AllStatic.
571   if (jvms-&gt;caller() == NULL) {
572     assert(_caller_jvms == NULL, &quot;redundant instance state&quot;);
573   } else {
574     assert(_caller_jvms-&gt;same_calls_as(jvms-&gt;caller()), &quot;redundant instance state&quot;);
575   }
576   assert(_method == jvms-&gt;method(), &quot;redundant instance state&quot;);
577 #endif
578   int         caller_bci    = jvms-&gt;bci();
579   ciMethod*   caller_method = jvms-&gt;method();
580 
581   // Do some initial checks.
582   if (!pass_initial_checks(caller_method, caller_bci, callee_method)) {
583     set_msg(&quot;failed initial checks&quot;);
584     print_inlining(callee_method, caller_bci, caller_method, false /* !success */);
585     return NULL;
586   }
587 
588   // Do some parse checks.
589   set_msg(check_can_parse(callee_method));
590   if (msg() != NULL) {
591     print_inlining(callee_method, caller_bci, caller_method, false /* !success */);
592     return NULL;
593   }
594 
595   // Check if inlining policy says no.
596   WarmCallInfo wci = *(initial_wci);
597   bool success = try_to_inline(callee_method, caller_method, caller_bci,
598                                jvms, profile, &amp;wci, should_delay);
599 
600 #ifndef PRODUCT
601   if (InlineWarmCalls &amp;&amp; (PrintOpto || C-&gt;print_inlining())) {
602     bool cold = wci.is_cold();
603     bool hot  = !cold &amp;&amp; wci.is_hot();
604     bool old_cold = !success;
605     if (old_cold != cold || (Verbose || WizardMode)) {
606       if (msg() == NULL) {
607         set_msg(&quot;OK&quot;);
608       }
609       tty-&gt;print(&quot;   OldInlining= %4s : %s\n           WCI=&quot;,
610                  old_cold ? &quot;cold&quot; : &quot;hot&quot;, msg());
611       wci.print();
612     }
613   }
614 #endif
615   if (success) {
616     wci = *(WarmCallInfo::always_hot());
617   } else {
618     wci = *(WarmCallInfo::always_cold());
619   }
620 
621   if (!InlineWarmCalls) {
622     if (!wci.is_cold() &amp;&amp; !wci.is_hot()) {
623       // Do not inline the warm calls.
624       wci = *(WarmCallInfo::always_cold());
625     }
626   }
627 
628   if (!wci.is_cold()) {
629     // Inline!
630     if (msg() == NULL) {
631       set_msg(&quot;inline (hot)&quot;);
632     }
633     print_inlining(callee_method, caller_bci, caller_method, true /* success */);
634     build_inline_tree_for_callee(callee_method, jvms, caller_bci);
635     if (InlineWarmCalls &amp;&amp; !wci.is_hot()) {
636       return new (C) WarmCallInfo(wci);  // copy to heap
637     }
638     return WarmCallInfo::always_hot();
639   }
640 
641   // Do not inline
642   if (msg() == NULL) {
643     set_msg(&quot;too cold to inline&quot;);
644   }
645   print_inlining(callee_method, caller_bci, caller_method, false /* !success */ );
646   return NULL;
647 }
648 
649 //------------------------------compute_callee_frequency-----------------------
650 float InlineTree::compute_callee_frequency( int caller_bci ) const {
651   int count  = method()-&gt;interpreter_call_site_count(caller_bci);
652   int invcnt = method()-&gt;interpreter_invocation_count();
653   float freq = (float)count/(float)invcnt;
654   // Call-site count / interpreter invocation count, scaled recursively.
655   // Always between 0.0 and 1.0.  Represents the percentage of the method&#39;s
656   // total execution time used at this call site.
657 
658   return freq;
659 }
660 
661 //------------------------------build_inline_tree_for_callee-------------------
662 InlineTree *InlineTree::build_inline_tree_for_callee( ciMethod* callee_method, JVMState* caller_jvms, int caller_bci) {
663   float recur_frequency = _site_invoke_ratio * compute_callee_frequency(caller_bci);
664   // Attempt inlining.
665   InlineTree* old_ilt = callee_at(caller_bci, callee_method);
666   if (old_ilt != NULL) {
667     return old_ilt;
668   }
669   int max_inline_level_adjust = 0;
670   if (caller_jvms-&gt;method() != NULL) {
671     if (caller_jvms-&gt;method()-&gt;is_compiled_lambda_form()) {
672       max_inline_level_adjust += 1;  // don&#39;t count actions in MH or indy adapter frames
673     } else if (callee_method-&gt;is_method_handle_intrinsic() ||
674                callee_method-&gt;is_compiled_lambda_form()) {
675       max_inline_level_adjust += 1;  // don&#39;t count method handle calls from java.lang.invoke implementation
676     }
677     if (max_inline_level_adjust != 0 &amp;&amp; C-&gt;print_inlining() &amp;&amp; (Verbose || WizardMode)) {
678       CompileTask::print_inline_indent(inline_level());
679       tty-&gt;print_cr(&quot; \\-&gt; discounting inline depth&quot;);
680     }
681     if (max_inline_level_adjust != 0 &amp;&amp; C-&gt;log()) {
682       int id1 = C-&gt;log()-&gt;identify(caller_jvms-&gt;method());
683       int id2 = C-&gt;log()-&gt;identify(callee_method);
684       C-&gt;log()-&gt;elem(&quot;inline_level_discount caller=&#39;%d&#39; callee=&#39;%d&#39;&quot;, id1, id2);
685     }
686   }
687   // Allocate in the comp_arena to make sure the InlineTree is live when dumping a replay compilation file
688   InlineTree* ilt = new (C-&gt;comp_arena()) InlineTree(C, this, callee_method, caller_jvms, caller_bci, recur_frequency, _max_inline_level + max_inline_level_adjust);
689   _subtrees.append(ilt);
690 
691   NOT_PRODUCT( _count_inlines += 1; )
692 
693   return ilt;
694 }
695 
696 
697 //---------------------------------------callee_at-----------------------------
698 InlineTree *InlineTree::callee_at(int bci, ciMethod* callee) const {
699   for (int i = 0; i &lt; _subtrees.length(); i++) {
700     InlineTree* sub = _subtrees.at(i);
701     if (sub-&gt;caller_bci() == bci &amp;&amp; callee == sub-&gt;method()) {
702       return sub;
703     }
704   }
705   return NULL;
706 }
707 
708 
709 //------------------------------build_inline_tree_root-------------------------
710 InlineTree *InlineTree::build_inline_tree_root() {
711   Compile* C = Compile::current();
712 
713   // Root of inline tree
714   InlineTree* ilt = new InlineTree(C, NULL, C-&gt;method(), NULL, -1, 1.0F, MaxInlineLevel);
715 
716   return ilt;
717 }
718 
719 
720 //-------------------------find_subtree_from_root-----------------------------
721 // Given a jvms, which determines a call chain from the root method,
722 // find the corresponding inline tree.
723 // Note: This method will be removed or replaced as InlineTree goes away.
724 InlineTree* InlineTree::find_subtree_from_root(InlineTree* root, JVMState* jvms, ciMethod* callee) {
725   InlineTree* iltp = root;
726   uint depth = jvms &amp;&amp; jvms-&gt;has_method() ? jvms-&gt;depth() : 0;
727   for (uint d = 1; d &lt;= depth; d++) {
728     JVMState* jvmsp  = jvms-&gt;of_depth(d);
729     // Select the corresponding subtree for this bci.
730     assert(jvmsp-&gt;method() == iltp-&gt;method(), &quot;tree still in sync&quot;);
731     ciMethod* d_callee = (d == depth) ? callee : jvms-&gt;of_depth(d+1)-&gt;method();
732     InlineTree* sub = iltp-&gt;callee_at(jvmsp-&gt;bci(), d_callee);
733     if (sub == NULL) {
734       if (d == depth) {
735         sub = iltp-&gt;build_inline_tree_for_callee(d_callee, jvmsp, jvmsp-&gt;bci());
736       }
737       guarantee(sub != NULL, &quot;should be a sub-ilt here&quot;);
738       return sub;
739     }
740     iltp = sub;
741   }
742   return iltp;
743 }
744 
745 // Count number of nodes in this subtree
746 int InlineTree::count() const {
747   int result = 1;
748   for (int i = 0 ; i &lt; _subtrees.length(); i++) {
749     result += _subtrees.at(i)-&gt;count();
750   }
751   return result;
752 }
753 
754 void InlineTree::dump_replay_data(outputStream* out) {
755   out-&gt;print(&quot; %d %d &quot;, inline_level(), caller_bci());
756   method()-&gt;dump_name_as_ascii(out);
757   for (int i = 0 ; i &lt; _subtrees.length(); i++) {
758     _subtrees.at(i)-&gt;dump_replay_data(out);
759   }
760 }
761 
762 
763 #ifndef PRODUCT
764 void InlineTree::print_impl(outputStream* st, int indent) const {
765   for (int i = 0; i &lt; indent; i++) st-&gt;print(&quot; &quot;);
766   st-&gt;print(&quot; @ %d&quot;, caller_bci());
767   method()-&gt;print_short_name(st);
768   st-&gt;cr();
769 
770   for (int i = 0 ; i &lt; _subtrees.length(); i++) {
771     _subtrees.at(i)-&gt;print_impl(st, indent + 2);
772   }
773 }
774 
775 void InlineTree::print_value_on(outputStream* st) const {
776   print_impl(st, 2);
777 }
778 #endif
    </pre>
  </body>
</html>