<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Cdiff src/hotspot/os/windows/os_windows.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="os_perf_windows.cpp.cdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="pdh_interface.cpp.cdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/os/windows/os_windows.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<hr />
<pre>
<span class="line-old-header">*** 2714,12 ***</span>
  
  #ifndef MEM_LARGE_PAGES
    #define MEM_LARGE_PAGES 0x20000000
  #endif
  
<span class="line-modified">! static HANDLE    _hProcess;</span>
<span class="line-modified">! static HANDLE    _hToken;</span>
  
  // Container for NUMA node list info
  class NUMANodeListHolder {
   private:
    int *_numa_used_node_list;  // allocated below
<span class="line-new-header">--- 2714,125 ---</span>
  
  #ifndef MEM_LARGE_PAGES
    #define MEM_LARGE_PAGES 0x20000000
  #endif
  
<span class="line-modified">! #define VirtualFreeChecked(mem, size, type)                       \</span>
<span class="line-modified">!   do {                                                            \</span>
<span class="line-added">+     bool ret = VirtualFree(mem, size, type);                      \</span>
<span class="line-added">+     assert(ret, &quot;Failed to free memory: &quot; PTR_FORMAT, p2i(mem));  \</span>
<span class="line-added">+   } while (false)</span>
<span class="line-added">+ </span>
<span class="line-added">+ // The number of bytes is setup to match 1 pixel and 32 bits per pixel.</span>
<span class="line-added">+ static const int gdi_tiny_bitmap_width_bytes = 4;</span>
<span class="line-added">+ </span>
<span class="line-added">+ static HBITMAP gdi_create_tiny_bitmap(void* mem) {</span>
<span class="line-added">+   // The documentation for CreateBitmap states a word-alignment requirement.</span>
<span class="line-added">+   STATIC_ASSERT(is_aligned_(gdi_tiny_bitmap_width_bytes, sizeof(WORD)));</span>
<span class="line-added">+ </span>
<span class="line-added">+   // Some callers use this function to test if memory crossing separate memory</span>
<span class="line-added">+   // reservations can be used. Create a height of 2 to make sure that one pixel</span>
<span class="line-added">+   // ends up in the first reservation and the other in the second.</span>
<span class="line-added">+   int nHeight = 2;</span>
<span class="line-added">+ </span>
<span class="line-added">+   assert(is_aligned(mem, gdi_tiny_bitmap_width_bytes), &quot;Incorrect alignment&quot;);</span>
<span class="line-added">+ </span>
<span class="line-added">+   // Width is one pixel and correlates with gdi_tiny_bitmap_width_bytes.</span>
<span class="line-added">+   int nWidth = 1;</span>
<span class="line-added">+ </span>
<span class="line-added">+   // Calculate bit count - will be 32.</span>
<span class="line-added">+   UINT nBitCount = gdi_tiny_bitmap_width_bytes / nWidth * BitsPerByte;</span>
<span class="line-added">+ </span>
<span class="line-added">+   return CreateBitmap(</span>
<span class="line-added">+       nWidth,</span>
<span class="line-added">+       nHeight,</span>
<span class="line-added">+       1,         // nPlanes</span>
<span class="line-added">+       nBitCount,</span>
<span class="line-added">+       mem);      // lpBits</span>
<span class="line-added">+ }</span>
<span class="line-added">+ </span>
<span class="line-added">+ // It has been found that some of the GDI functions fail under these two situations:</span>
<span class="line-added">+ //  1) When used with large pages</span>
<span class="line-added">+ //  2) When mem crosses the boundary between two separate memory reservations.</span>
<span class="line-added">+ //</span>
<span class="line-added">+ // This is a small test used to see if the current GDI implementation is</span>
<span class="line-added">+ // susceptible to any of these problems.</span>
<span class="line-added">+ static bool gdi_can_use_memory(void* mem) {</span>
<span class="line-added">+   HBITMAP bitmap = gdi_create_tiny_bitmap(mem);</span>
<span class="line-added">+   if (bitmap != NULL) {</span>
<span class="line-added">+     DeleteObject(bitmap);</span>
<span class="line-added">+     return true;</span>
<span class="line-added">+   }</span>
<span class="line-added">+ </span>
<span class="line-added">+   // Verify that the bitmap could be created with a normal page.</span>
<span class="line-added">+   // If this fails, the testing method above isn&#39;t reliable.</span>
<span class="line-added">+ #ifdef ASSERT</span>
<span class="line-added">+   void* verify_mem = ::malloc(4 * 1024);</span>
<span class="line-added">+   HBITMAP verify_bitmap = gdi_create_tiny_bitmap(verify_mem);</span>
<span class="line-added">+   if (verify_bitmap == NULL) {</span>
<span class="line-added">+     fatal(&quot;Couldn&#39;t create test bitmap with malloced memory&quot;);</span>
<span class="line-added">+   } else {</span>
<span class="line-added">+     DeleteObject(verify_bitmap);</span>
<span class="line-added">+   }</span>
<span class="line-added">+   ::free(verify_mem);</span>
<span class="line-added">+ #endif</span>
<span class="line-added">+ </span>
<span class="line-added">+   return false;</span>
<span class="line-added">+ }</span>
<span class="line-added">+ </span>
<span class="line-added">+ // Test if GDI functions work when memory spans</span>
<span class="line-added">+ // two adjacent memory reservations.</span>
<span class="line-added">+ static bool gdi_can_use_split_reservation_memory(bool use_large_pages, size_t granule) {</span>
<span class="line-added">+   DWORD mem_large_pages = use_large_pages ? MEM_LARGE_PAGES : 0;</span>
<span class="line-added">+ </span>
<span class="line-added">+   // Find virtual memory range. Two granules for regions and one for alignment.</span>
<span class="line-added">+   void* reserved = VirtualAlloc(NULL,</span>
<span class="line-added">+                                 granule * 3,</span>
<span class="line-added">+                                 MEM_RESERVE,</span>
<span class="line-added">+                                 PAGE_NOACCESS);</span>
<span class="line-added">+   if (reserved == NULL) {</span>
<span class="line-added">+     // Can&#39;t proceed with test - pessimistically report false</span>
<span class="line-added">+     return false;</span>
<span class="line-added">+   }</span>
<span class="line-added">+   VirtualFreeChecked(reserved, 0, MEM_RELEASE);</span>
<span class="line-added">+ </span>
<span class="line-added">+   // Ensure proper alignment</span>
<span class="line-added">+   void* res0 = align_up(reserved, granule);</span>
<span class="line-added">+   void* res1 = (char*)res0 + granule;</span>
<span class="line-added">+ </span>
<span class="line-added">+   // Reserve and commit the first part</span>
<span class="line-added">+   void* mem0 = VirtualAlloc(res0,</span>
<span class="line-added">+                             granule,</span>
<span class="line-added">+                             MEM_RESERVE|MEM_COMMIT|mem_large_pages,</span>
<span class="line-added">+                             PAGE_READWRITE);</span>
<span class="line-added">+   if (mem0 != res0) {</span>
<span class="line-added">+     // Can&#39;t proceed with test - pessimistically report false</span>
<span class="line-added">+     return false;</span>
<span class="line-added">+   }</span>
<span class="line-added">+ </span>
<span class="line-added">+   // Reserve and commit the second part</span>
<span class="line-added">+   void* mem1 = VirtualAlloc(res1,</span>
<span class="line-added">+                             granule,</span>
<span class="line-added">+                             MEM_RESERVE|MEM_COMMIT|mem_large_pages,</span>
<span class="line-added">+                             PAGE_READWRITE);</span>
<span class="line-added">+   if (mem1 != res1) {</span>
<span class="line-added">+     VirtualFreeChecked(mem0, 0, MEM_RELEASE);</span>
<span class="line-added">+     // Can&#39;t proceed with test - pessimistically report false</span>
<span class="line-added">+     return false;</span>
<span class="line-added">+   }</span>
<span class="line-added">+ </span>
<span class="line-added">+   // Set the bitmap&#39;s bits to point one &quot;width&quot; bytes before, so that</span>
<span class="line-added">+   // the bitmap extends across the reservation boundary.</span>
<span class="line-added">+   void* bitmapBits = (char*)mem1 - gdi_tiny_bitmap_width_bytes;</span>
<span class="line-added">+ </span>
<span class="line-added">+   bool success = gdi_can_use_memory(bitmapBits);</span>
<span class="line-added">+ </span>
<span class="line-added">+   VirtualFreeChecked(mem1, 0, MEM_RELEASE);</span>
<span class="line-added">+   VirtualFreeChecked(mem0, 0, MEM_RELEASE);</span>
<span class="line-added">+ </span>
<span class="line-added">+   return success;</span>
<span class="line-added">+ }</span>
  
  // Container for NUMA node list info
  class NUMANodeListHolder {
   private:
    int *_numa_used_node_list;  // allocated below
</pre>
<hr />
<pre>
<span class="line-old-header">*** 2764,75 ***</span>
      return (n &lt; _numa_used_node_count ? _numa_used_node_list[n] : -1);
    }
  
  } numa_node_list_holder;
  
<span class="line-removed">- </span>
<span class="line-removed">- </span>
  static size_t _large_page_size = 0;
  
  static bool request_lock_memory_privilege() {
<span class="line-modified">!   _hProcess = OpenProcess(PROCESS_QUERY_INFORMATION, FALSE,</span>
<span class="line-modified">!                           os::current_process_id());</span>
  
    LUID luid;
<span class="line-modified">!   if (_hProcess != NULL &amp;&amp;</span>
<span class="line-modified">!       OpenProcessToken(_hProcess, TOKEN_ADJUST_PRIVILEGES, &amp;_hToken) &amp;&amp;</span>
        LookupPrivilegeValue(NULL, &quot;SeLockMemoryPrivilege&quot;, &amp;luid)) {
  
      TOKEN_PRIVILEGES tp;
      tp.PrivilegeCount = 1;
      tp.Privileges[0].Luid = luid;
      tp.Privileges[0].Attributes = SE_PRIVILEGE_ENABLED;
  
      // AdjustTokenPrivileges() may return TRUE even when it couldn&#39;t change the
      // privilege. Check GetLastError() too. See MSDN document.
<span class="line-modified">!     if (AdjustTokenPrivileges(_hToken, false, &amp;tp, sizeof(tp), NULL, NULL) &amp;&amp;</span>
          (GetLastError() == ERROR_SUCCESS)) {
<span class="line-modified">!       return true;</span>
      }
    }
  
<span class="line-modified">!   return false;</span>
<span class="line-modified">! }</span>
  
<span class="line-modified">! static void cleanup_after_large_page_init() {</span>
<span class="line-removed">-   if (_hProcess) CloseHandle(_hProcess);</span>
<span class="line-removed">-   _hProcess = NULL;</span>
<span class="line-removed">-   if (_hToken) CloseHandle(_hToken);</span>
<span class="line-removed">-   _hToken = NULL;</span>
  }
  
  static bool numa_interleaving_init() {
    bool success = false;
<span class="line-removed">-   bool use_numa_interleaving_specified = !FLAG_IS_DEFAULT(UseNUMAInterleaving);</span>
  
    // print a warning if UseNUMAInterleaving flag is specified on command line
<span class="line-modified">!   bool warn_on_failure = use_numa_interleaving_specified;</span>
  #define WARN(msg) if (warn_on_failure) { warning(msg); }
  
    // NUMAInterleaveGranularity cannot be less than vm_allocation_granularity (or _large_page_size if using large pages)
    size_t min_interleave_granularity = UseLargePages ? _large_page_size : os::vm_allocation_granularity();
    NUMAInterleaveGranularity = align_up(NUMAInterleaveGranularity, min_interleave_granularity);
  
<span class="line-modified">!   if (numa_node_list_holder.build()) {</span>
<span class="line-removed">-     if (log_is_enabled(Debug, os, cpu)) {</span>
<span class="line-removed">-       Log(os, cpu) log;</span>
<span class="line-removed">-       log.debug(&quot;NUMA UsedNodeCount=%d, namely &quot;, numa_node_list_holder.get_count());</span>
<span class="line-removed">-       for (int i = 0; i &lt; numa_node_list_holder.get_count(); i++) {</span>
<span class="line-removed">-         log.debug(&quot;  %d &quot;, numa_node_list_holder.get_node_list_entry(i));</span>
<span class="line-removed">-       }</span>
<span class="line-removed">-     }</span>
<span class="line-removed">-     success = true;</span>
<span class="line-removed">-   } else {</span>
      WARN(&quot;Process does not cover multiple NUMA nodes.&quot;);
    }
<span class="line-modified">!   if (!success) {</span>
<span class="line-modified">!     if (use_numa_interleaving_specified) WARN(&quot;...Ignoring UseNUMAInterleaving flag.&quot;);</span>
    }
<span class="line-modified">!   return success;</span>
  #undef WARN
  }
  
  // this routine is used whenever we need to reserve a contiguous VA range
  // but we need to make separate VirtualAlloc calls for each piece of the range
  // Reasons for doing this:
<span class="line-new-header">--- 2877,82 ---</span>
      return (n &lt; _numa_used_node_count ? _numa_used_node_list[n] : -1);
    }
  
  } numa_node_list_holder;
  
  static size_t _large_page_size = 0;
  
  static bool request_lock_memory_privilege() {
<span class="line-modified">!   HANDLE hProcess = OpenProcess(PROCESS_QUERY_INFORMATION, FALSE,</span>
<span class="line-modified">!                                 os::current_process_id());</span>
  
<span class="line-added">+   bool success = false;</span>
<span class="line-added">+   HANDLE hToken = NULL;</span>
    LUID luid;
<span class="line-modified">!   if (hProcess != NULL &amp;&amp;</span>
<span class="line-modified">!       OpenProcessToken(hProcess, TOKEN_ADJUST_PRIVILEGES, &amp;hToken) &amp;&amp;</span>
        LookupPrivilegeValue(NULL, &quot;SeLockMemoryPrivilege&quot;, &amp;luid)) {
  
      TOKEN_PRIVILEGES tp;
      tp.PrivilegeCount = 1;
      tp.Privileges[0].Luid = luid;
      tp.Privileges[0].Attributes = SE_PRIVILEGE_ENABLED;
  
      // AdjustTokenPrivileges() may return TRUE even when it couldn&#39;t change the
      // privilege. Check GetLastError() too. See MSDN document.
<span class="line-modified">!     if (AdjustTokenPrivileges(hToken, false, &amp;tp, sizeof(tp), NULL, NULL) &amp;&amp;</span>
          (GetLastError() == ERROR_SUCCESS)) {
<span class="line-modified">!       success = true;</span>
      }
    }
  
<span class="line-modified">!   // Cleanup</span>
<span class="line-modified">!   if (hProcess != NULL) {</span>
<span class="line-added">+     CloseHandle(hProcess);</span>
<span class="line-added">+   }</span>
<span class="line-added">+   if (hToken != NULL) {</span>
<span class="line-added">+     CloseHandle(hToken);</span>
<span class="line-added">+   }</span>
  
<span class="line-modified">!   return success;</span>
  }
  
  static bool numa_interleaving_init() {
    bool success = false;
  
    // print a warning if UseNUMAInterleaving flag is specified on command line
<span class="line-modified">!   bool warn_on_failure = !FLAG_IS_DEFAULT(UseNUMAInterleaving);</span>
<span class="line-added">+ </span>
  #define WARN(msg) if (warn_on_failure) { warning(msg); }
  
    // NUMAInterleaveGranularity cannot be less than vm_allocation_granularity (or _large_page_size if using large pages)
    size_t min_interleave_granularity = UseLargePages ? _large_page_size : os::vm_allocation_granularity();
    NUMAInterleaveGranularity = align_up(NUMAInterleaveGranularity, min_interleave_granularity);
  
<span class="line-modified">!   if (!numa_node_list_holder.build()) {</span>
      WARN(&quot;Process does not cover multiple NUMA nodes.&quot;);
<span class="line-added">+     WARN(&quot;...Ignoring UseNUMAInterleaving flag.&quot;);</span>
<span class="line-added">+     return false;</span>
    }
<span class="line-modified">! </span>
<span class="line-modified">!   if (!gdi_can_use_split_reservation_memory(UseLargePages, min_interleave_granularity)) {</span>
<span class="line-added">+     WARN(&quot;Windows GDI cannot handle split reservations.&quot;);</span>
<span class="line-added">+     WARN(&quot;...Ignoring UseNUMAInterleaving flag.&quot;);</span>
<span class="line-added">+     return false;</span>
    }
<span class="line-modified">! </span>
<span class="line-added">+   if (log_is_enabled(Debug, os, cpu)) {</span>
<span class="line-added">+     Log(os, cpu) log;</span>
<span class="line-added">+     log.debug(&quot;NUMA UsedNodeCount=%d, namely &quot;, numa_node_list_holder.get_count());</span>
<span class="line-added">+     for (int i = 0; i &lt; numa_node_list_holder.get_count(); i++) {</span>
<span class="line-added">+       log.debug(&quot;  %d &quot;, numa_node_list_holder.get_node_list_entry(i));</span>
<span class="line-added">+     }</span>
<span class="line-added">+   }</span>
<span class="line-added">+ </span>
  #undef WARN
<span class="line-added">+ </span>
<span class="line-added">+   return true;</span>
  }
  
  // this routine is used whenever we need to reserve a contiguous VA range
  // but we need to make separate VirtualAlloc calls for each piece of the range
  // Reasons for doing this:
</pre>
<hr />
<pre>
<span class="line-old-header">*** 2949,55 ***</span>
  
    // made it this far, success
    return p_buf;
  }
  
<span class="line-modified">! </span>
<span class="line-removed">- </span>
<span class="line-removed">- void os::large_page_init() {</span>
<span class="line-removed">-   if (!UseLargePages) return;</span>
<span class="line-removed">- </span>
    // print a warning if any large page related flag is specified on command line
    bool warn_on_failure = !FLAG_IS_DEFAULT(UseLargePages) ||
                           !FLAG_IS_DEFAULT(LargePageSizeInBytes);
<span class="line-removed">-   bool success = false;</span>
  
  #define WARN(msg) if (warn_on_failure) { warning(msg); }
<span class="line-modified">!   if (request_lock_memory_privilege()) {</span>
<span class="line-modified">!     size_t s = GetLargePageMinimum();</span>
<span class="line-modified">!     if (s) {</span>
<span class="line-modified">! #if defined(IA32) || defined(AMD64)</span>
<span class="line-modified">!       if (s &gt; 4*M || LargePageSizeInBytes &gt; 4*M) {</span>
<span class="line-modified">!         WARN(&quot;JVM cannot use large pages bigger than 4mb.&quot;);</span>
<span class="line-modified">!       } else {</span>
<span class="line-modified">! #endif</span>
<span class="line-modified">!         if (LargePageSizeInBytes &amp;&amp; LargePageSizeInBytes % s == 0) {</span>
<span class="line-modified">!           _large_page_size = LargePageSizeInBytes;</span>
<span class="line-modified">!         } else {</span>
<span class="line-modified">!           _large_page_size = s;</span>
<span class="line-removed">-         }</span>
<span class="line-removed">-         success = true;</span>
  #if defined(IA32) || defined(AMD64)
<span class="line-modified">!       }</span>
  #endif
<span class="line-modified">!     } else {</span>
<span class="line-modified">!       WARN(&quot;Large page is not supported by the processor.&quot;);</span>
<span class="line-modified">!     }</span>
<span class="line-modified">!   } else {</span>
<span class="line-modified">!     WARN(&quot;JVM cannot use large page memory because it does not have enough privilege to lock pages in memory.&quot;);</span>
    }
  #undef WARN
  
    const size_t default_page_size = (size_t) vm_page_size();
<span class="line-modified">!   if (success &amp;&amp; _large_page_size &gt; default_page_size) {</span>
      _page_sizes[0] = _large_page_size;
      _page_sizes[1] = default_page_size;
      _page_sizes[2] = 0;
    }
  
<span class="line-modified">!   cleanup_after_large_page_init();</span>
<span class="line-modified">!   UseLargePages = success;</span>
  }
  
  int os::create_file_for_heap(const char* dir) {
  
    const char name_template[] = &quot;/jvmheap.XXXXXX&quot;;
<span class="line-new-header">--- 3069,88 ---</span>
  
    // made it this far, success
    return p_buf;
  }
  
<span class="line-modified">! static size_t large_page_init_decide_size() {</span>
    // print a warning if any large page related flag is specified on command line
    bool warn_on_failure = !FLAG_IS_DEFAULT(UseLargePages) ||
                           !FLAG_IS_DEFAULT(LargePageSizeInBytes);
  
  #define WARN(msg) if (warn_on_failure) { warning(msg); }
<span class="line-modified">! </span>
<span class="line-modified">!   if (!request_lock_memory_privilege()) {</span>
<span class="line-modified">!     WARN(&quot;JVM cannot use large page memory because it does not have enough privilege to lock pages in memory.&quot;);</span>
<span class="line-modified">!     return 0;</span>
<span class="line-modified">!   }</span>
<span class="line-modified">! </span>
<span class="line-modified">!   size_t size = GetLargePageMinimum();</span>
<span class="line-modified">!   if (size == 0) {</span>
<span class="line-modified">!     WARN(&quot;Large page is not supported by the processor.&quot;);</span>
<span class="line-modified">!     return 0;</span>
<span class="line-modified">!   }</span>
<span class="line-modified">! </span>
  #if defined(IA32) || defined(AMD64)
<span class="line-modified">!   if (size &gt; 4*M || LargePageSizeInBytes &gt; 4*M) {</span>
<span class="line-added">+     WARN(&quot;JVM cannot use large pages bigger than 4mb.&quot;);</span>
<span class="line-added">+     return 0;</span>
<span class="line-added">+   }</span>
  #endif
<span class="line-modified">! </span>
<span class="line-modified">!   if (LargePageSizeInBytes &gt; 0 &amp;&amp; LargePageSizeInBytes % size == 0) {</span>
<span class="line-modified">!     size = LargePageSizeInBytes;</span>
<span class="line-modified">!   }</span>
<span class="line-modified">! </span>
<span class="line-added">+   // Now test allocating a page</span>
<span class="line-added">+   void* large_page = VirtualAlloc(NULL,</span>
<span class="line-added">+                                   size,</span>
<span class="line-added">+                                   MEM_RESERVE|MEM_COMMIT|MEM_LARGE_PAGES,</span>
<span class="line-added">+                                   PAGE_READWRITE);</span>
<span class="line-added">+   if (large_page == NULL) {</span>
<span class="line-added">+     WARN(&quot;JVM cannot allocate one single large page.&quot;);</span>
<span class="line-added">+     return 0;</span>
<span class="line-added">+   }</span>
<span class="line-added">+ </span>
<span class="line-added">+   // Detect if GDI can use memory backed by large pages</span>
<span class="line-added">+   if (!gdi_can_use_memory(large_page)) {</span>
<span class="line-added">+     WARN(&quot;JVM cannot use large pages because of bug in Windows GDI.&quot;);</span>
<span class="line-added">+     return 0;</span>
    }
<span class="line-added">+ </span>
<span class="line-added">+   // Release test page</span>
<span class="line-added">+   VirtualFreeChecked(large_page, 0, MEM_RELEASE);</span>
<span class="line-added">+ </span>
  #undef WARN
  
<span class="line-added">+   return size;</span>
<span class="line-added">+ }</span>
<span class="line-added">+ </span>
<span class="line-added">+ void os::large_page_init() {</span>
<span class="line-added">+   if (!UseLargePages) {</span>
<span class="line-added">+     return;</span>
<span class="line-added">+   }</span>
<span class="line-added">+ </span>
<span class="line-added">+   _large_page_size = large_page_init_decide_size();</span>
<span class="line-added">+ </span>
    const size_t default_page_size = (size_t) vm_page_size();
<span class="line-modified">!   if (_large_page_size &gt; default_page_size) {</span>
      _page_sizes[0] = _large_page_size;
      _page_sizes[1] = default_page_size;
      _page_sizes[2] = 0;
    }
  
<span class="line-modified">!   UseLargePages = _large_page_size != 0;</span>
<span class="line-modified">! </span>
<span class="line-added">+   if (UseLargePages &amp;&amp; UseLargePagesIndividualAllocation) {</span>
<span class="line-added">+     if (!gdi_can_use_split_reservation_memory(true /* use_large_pages */, _large_page_size)) {</span>
<span class="line-added">+       if (FLAG_IS_CMDLINE(UseLargePagesIndividualAllocation)) {</span>
<span class="line-added">+         warning(&quot;Windows GDI cannot handle split reservations.&quot;);</span>
<span class="line-added">+         warning(&quot;...Ignoring UseLargePagesIndividualAllocation flag.&quot;);</span>
<span class="line-added">+       }</span>
<span class="line-added">+       UseLargePagesIndividualAllocation = false;</span>
<span class="line-added">+     }</span>
<span class="line-added">+   }</span>
  }
  
  int os::create_file_for_heap(const char* dir) {
  
    const char name_template[] = &quot;/jvmheap.XXXXXX&quot;;
</pre>
<hr />
<pre>
<span class="line-old-header">*** 3069,21 ***</span>
  }
  
  // On win32, one cannot release just a part of reserved memory, it&#39;s an
  // all or nothing deal.  When we split a reservation, we must break the
  // reservation into two reservations.
<span class="line-modified">! void os::pd_split_reserved_memory(char *base, size_t size, size_t split,</span>
<span class="line-modified">!                                   bool realloc) {</span>
<span class="line-modified">!   if (size &gt; 0) {</span>
<span class="line-modified">!     release_memory(base, size);</span>
<span class="line-modified">!     if (realloc) {</span>
<span class="line-modified">!       reserve_memory(split, base);</span>
<span class="line-modified">!     }</span>
<span class="line-modified">!     if (size != split) {</span>
<span class="line-modified">!       reserve_memory(size - split, base + split);</span>
<span class="line-modified">!     }</span>
<span class="line-modified">!   }</span>
  }
  
  // Multiple threads can race in this code but it&#39;s not possible to unmap small sections of
  // virtual space to get requested alignment, like posix-like os&#39;s.
  // Windows prevents multiple thread from remapping over each other so this loop is thread-safe.
<span class="line-new-header">--- 3222,23 ---</span>
  }
  
  // On win32, one cannot release just a part of reserved memory, it&#39;s an
  // all or nothing deal.  When we split a reservation, we must break the
  // reservation into two reservations.
<span class="line-modified">! void os::split_reserved_memory(char *base, size_t size, size_t split) {</span>
<span class="line-modified">! </span>
<span class="line-modified">!   char* const split_address = base + split;</span>
<span class="line-modified">!   assert(size &gt; 0, &quot;Sanity&quot;);</span>
<span class="line-modified">!   assert(size &gt; split, &quot;Sanity&quot;);</span>
<span class="line-modified">!   assert(split &gt; 0, &quot;Sanity&quot;);</span>
<span class="line-modified">!   assert(is_aligned(base, os::vm_allocation_granularity()), &quot;Sanity&quot;);</span>
<span class="line-modified">!   assert(is_aligned(split_address, os::vm_allocation_granularity()), &quot;Sanity&quot;);</span>
<span class="line-modified">! </span>
<span class="line-modified">!   release_memory(base, size);</span>
<span class="line-modified">!   reserve_memory(split, base);</span>
<span class="line-added">+   reserve_memory(size - split, split_address);</span>
<span class="line-added">+ </span>
  }
  
  // Multiple threads can race in this code but it&#39;s not possible to unmap small sections of
  // virtual space to get requested alignment, like posix-like os&#39;s.
  // Windows prevents multiple thread from remapping over each other so this loop is thread-safe.
</pre>
<center><a href="os_perf_windows.cpp.cdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="pdh_interface.cpp.cdiff.html" target="_top">next &gt;</a></center>  </body>
</html>