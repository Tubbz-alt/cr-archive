<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/os/windows/os_windows.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="os_perf_windows.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="pdh_interface.cpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/os/windows/os_windows.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
2699 // to the user:
2700 //   + select Control Panel -&gt; Administrative Tools -&gt; Local Security Policy
2701 //   + select Local Policies -&gt; User Rights Assignment
2702 //   + double click &quot;Lock pages in memory&quot;, add users and/or groups
2703 //   + reboot
2704 // Note the above steps are needed for administrator as well, as administrators
2705 // by default do not have the privilege to lock pages in memory.
2706 //
2707 // Note about Windows 2003: although the API supports committing large page
2708 // memory on a page-by-page basis and VirtualAlloc() returns success under this
2709 // scenario, I found through experiment it only uses large page if the entire
2710 // memory region is reserved and committed in a single VirtualAlloc() call.
2711 // This makes Windows large page support more or less like Solaris ISM, in
2712 // that the entire heap must be committed upfront. This probably will change
2713 // in the future, if so the code below needs to be revisited.
2714 
2715 #ifndef MEM_LARGE_PAGES
2716   #define MEM_LARGE_PAGES 0x20000000
2717 #endif
2718 
<span class="line-modified">2719 static HANDLE    _hProcess;</span>
<span class="line-modified">2720 static HANDLE    _hToken;</span>

















































































































2721 
2722 // Container for NUMA node list info
2723 class NUMANodeListHolder {
2724  private:
2725   int *_numa_used_node_list;  // allocated below
2726   int _numa_used_node_count;
2727 
2728   void free_node_list() {
2729     FREE_C_HEAP_ARRAY(int, _numa_used_node_list);
2730   }
2731 
2732  public:
2733   NUMANodeListHolder() {
2734     _numa_used_node_count = 0;
2735     _numa_used_node_list = NULL;
2736     // do rest of initialization in build routine (after function pointers are set up)
2737   }
2738 
2739   ~NUMANodeListHolder() {
2740     free_node_list();
</pre>
<hr />
<pre>
2749     free_node_list();
2750     _numa_used_node_list = NEW_C_HEAP_ARRAY(int, highest_node_number + 1, mtInternal);
2751     for (unsigned int i = 0; i &lt;= highest_node_number; i++) {
2752       ULONGLONG proc_mask_numa_node;
2753       if (!GetNumaNodeProcessorMask(i, &amp;proc_mask_numa_node)) return false;
2754       if ((proc_aff_mask &amp; proc_mask_numa_node)!=0) {
2755         _numa_used_node_list[_numa_used_node_count++] = i;
2756       }
2757     }
2758     return (_numa_used_node_count &gt; 1);
2759   }
2760 
2761   int get_count() { return _numa_used_node_count; }
2762   int get_node_list_entry(int n) {
2763     // for indexes out of range, returns -1
2764     return (n &lt; _numa_used_node_count ? _numa_used_node_list[n] : -1);
2765   }
2766 
2767 } numa_node_list_holder;
2768 
<span class="line-removed">2769 </span>
<span class="line-removed">2770 </span>
2771 static size_t _large_page_size = 0;
2772 
2773 static bool request_lock_memory_privilege() {
<span class="line-modified">2774   _hProcess = OpenProcess(PROCESS_QUERY_INFORMATION, FALSE,</span>
<span class="line-modified">2775                           os::current_process_id());</span>
2776 


2777   LUID luid;
<span class="line-modified">2778   if (_hProcess != NULL &amp;&amp;</span>
<span class="line-modified">2779       OpenProcessToken(_hProcess, TOKEN_ADJUST_PRIVILEGES, &amp;_hToken) &amp;&amp;</span>
2780       LookupPrivilegeValue(NULL, &quot;SeLockMemoryPrivilege&quot;, &amp;luid)) {
2781 
2782     TOKEN_PRIVILEGES tp;
2783     tp.PrivilegeCount = 1;
2784     tp.Privileges[0].Luid = luid;
2785     tp.Privileges[0].Attributes = SE_PRIVILEGE_ENABLED;
2786 
2787     // AdjustTokenPrivileges() may return TRUE even when it couldn&#39;t change the
2788     // privilege. Check GetLastError() too. See MSDN document.
<span class="line-modified">2789     if (AdjustTokenPrivileges(_hToken, false, &amp;tp, sizeof(tp), NULL, NULL) &amp;&amp;</span>
2790         (GetLastError() == ERROR_SUCCESS)) {
<span class="line-modified">2791       return true;</span>
2792     }
2793   }
2794 
<span class="line-modified">2795   return false;</span>
<span class="line-modified">2796 }</span>





2797 
<span class="line-modified">2798 static void cleanup_after_large_page_init() {</span>
<span class="line-removed">2799   if (_hProcess) CloseHandle(_hProcess);</span>
<span class="line-removed">2800   _hProcess = NULL;</span>
<span class="line-removed">2801   if (_hToken) CloseHandle(_hToken);</span>
<span class="line-removed">2802   _hToken = NULL;</span>
2803 }
2804 
2805 static bool numa_interleaving_init() {
2806   bool success = false;
<span class="line-removed">2807   bool use_numa_interleaving_specified = !FLAG_IS_DEFAULT(UseNUMAInterleaving);</span>
2808 
2809   // print a warning if UseNUMAInterleaving flag is specified on command line
<span class="line-modified">2810   bool warn_on_failure = use_numa_interleaving_specified;</span>

2811 #define WARN(msg) if (warn_on_failure) { warning(msg); }
2812 
2813   // NUMAInterleaveGranularity cannot be less than vm_allocation_granularity (or _large_page_size if using large pages)
2814   size_t min_interleave_granularity = UseLargePages ? _large_page_size : os::vm_allocation_granularity();
2815   NUMAInterleaveGranularity = align_up(NUMAInterleaveGranularity, min_interleave_granularity);
2816 
<span class="line-modified">2817   if (numa_node_list_holder.build()) {</span>
<span class="line-removed">2818     if (log_is_enabled(Debug, os, cpu)) {</span>
<span class="line-removed">2819       Log(os, cpu) log;</span>
<span class="line-removed">2820       log.debug(&quot;NUMA UsedNodeCount=%d, namely &quot;, numa_node_list_holder.get_count());</span>
<span class="line-removed">2821       for (int i = 0; i &lt; numa_node_list_holder.get_count(); i++) {</span>
<span class="line-removed">2822         log.debug(&quot;  %d &quot;, numa_node_list_holder.get_node_list_entry(i));</span>
<span class="line-removed">2823       }</span>
<span class="line-removed">2824     }</span>
<span class="line-removed">2825     success = true;</span>
<span class="line-removed">2826   } else {</span>
2827     WARN(&quot;Process does not cover multiple NUMA nodes.&quot;);


2828   }
<span class="line-modified">2829   if (!success) {</span>
<span class="line-modified">2830     if (use_numa_interleaving_specified) WARN(&quot;...Ignoring UseNUMAInterleaving flag.&quot;);</span>



2831   }
<span class="line-modified">2832   return success;</span>








2833 #undef WARN


2834 }
2835 
2836 // this routine is used whenever we need to reserve a contiguous VA range
2837 // but we need to make separate VirtualAlloc calls for each piece of the range
2838 // Reasons for doing this:
2839 //  * UseLargePagesIndividualAllocation was set (normally only needed on WS2003 but possible to be set otherwise)
2840 //  * UseNUMAInterleaving requires a separate node for each piece
2841 static char* allocate_pages_individually(size_t bytes, char* addr, DWORD flags,
2842                                          DWORD prot,
2843                                          bool should_inject_error = false) {
2844   char * p_buf;
2845   // note: at setup time we guaranteed that NUMAInterleaveGranularity was aligned up to a page size
2846   size_t page_size = UseLargePages ? _large_page_size : os::vm_allocation_granularity();
2847   size_t chunk_size = UseNUMAInterleaving ? NUMAInterleaveGranularity : page_size;
2848 
2849   // first reserve enough address space in advance since we want to be
2850   // able to break a single contiguous virtual address range into multiple
2851   // large page commits but WS2003 does not allow reserving large page space
2852   // so we just use 4K pages for reserve, this gives us a legal contiguous
2853   // address space. then we will deallocate that reservation, and re alloc
</pre>
<hr />
<pre>
2934 #endif
2935       return NULL;
2936     }
2937 
2938     bytes_remaining -= bytes_to_rq;
2939     next_alloc_addr += bytes_to_rq;
2940     count++;
2941   }
2942   // Although the memory is allocated individually, it is returned as one.
2943   // NMT records it as one block.
2944   if ((flags &amp; MEM_COMMIT) != 0) {
2945     MemTracker::record_virtual_memory_reserve_and_commit((address)p_buf, bytes, CALLER_PC);
2946   } else {
2947     MemTracker::record_virtual_memory_reserve((address)p_buf, bytes, CALLER_PC);
2948   }
2949 
2950   // made it this far, success
2951   return p_buf;
2952 }
2953 
<span class="line-modified">2954 </span>
<span class="line-removed">2955 </span>
<span class="line-removed">2956 void os::large_page_init() {</span>
<span class="line-removed">2957   if (!UseLargePages) return;</span>
<span class="line-removed">2958 </span>
2959   // print a warning if any large page related flag is specified on command line
2960   bool warn_on_failure = !FLAG_IS_DEFAULT(UseLargePages) ||
2961                          !FLAG_IS_DEFAULT(LargePageSizeInBytes);
<span class="line-removed">2962   bool success = false;</span>
2963 
2964 #define WARN(msg) if (warn_on_failure) { warning(msg); }
<span class="line-modified">2965   if (request_lock_memory_privilege()) {</span>
<span class="line-modified">2966     size_t s = GetLargePageMinimum();</span>
<span class="line-modified">2967     if (s) {</span>
<span class="line-modified">2968 #if defined(IA32) || defined(AMD64)</span>
<span class="line-modified">2969       if (s &gt; 4*M || LargePageSizeInBytes &gt; 4*M) {</span>
<span class="line-modified">2970         WARN(&quot;JVM cannot use large pages bigger than 4mb.&quot;);</span>
<span class="line-modified">2971       } else {</span>
<span class="line-modified">2972 #endif</span>
<span class="line-modified">2973         if (LargePageSizeInBytes &amp;&amp; LargePageSizeInBytes % s == 0) {</span>
<span class="line-modified">2974           _large_page_size = LargePageSizeInBytes;</span>
<span class="line-modified">2975         } else {</span>
<span class="line-modified">2976           _large_page_size = s;</span>
<span class="line-removed">2977         }</span>
<span class="line-removed">2978         success = true;</span>
2979 #if defined(IA32) || defined(AMD64)
<span class="line-modified">2980       }</span>



2981 #endif
<span class="line-modified">2982     } else {</span>
<span class="line-modified">2983       WARN(&quot;Large page is not supported by the processor.&quot;);</span>
<span class="line-modified">2984     }</span>
<span class="line-modified">2985   } else {</span>
<span class="line-modified">2986     WARN(&quot;JVM cannot use large page memory because it does not have enough privilege to lock pages in memory.&quot;);</span>














2987   }




2988 #undef WARN
2989 










2990   const size_t default_page_size = (size_t) vm_page_size();
<span class="line-modified">2991   if (success &amp;&amp; _large_page_size &gt; default_page_size) {</span>
2992     _page_sizes[0] = _large_page_size;
2993     _page_sizes[1] = default_page_size;
2994     _page_sizes[2] = 0;
2995   }
2996 
<span class="line-modified">2997   cleanup_after_large_page_init();</span>
<span class="line-modified">2998   UseLargePages = success;</span>









2999 }
3000 
3001 int os::create_file_for_heap(const char* dir) {
3002 
3003   const char name_template[] = &quot;/jvmheap.XXXXXX&quot;;
3004 
3005   size_t fullname_len = strlen(dir) + strlen(name_template);
3006   char *fullname = (char*)os::malloc(fullname_len + 1, mtInternal);
3007   if (fullname == NULL) {
3008     vm_exit_during_initialization(err_msg(&quot;Malloc failed during creation of backing file for heap (%s)&quot;, os::strerror(errno)));
3009     return -1;
3010   }
3011   int n = snprintf(fullname, fullname_len + 1, &quot;%s%s&quot;, dir, name_template);
3012   assert((size_t)n == fullname_len, &quot;Unexpected number of characters in string&quot;);
3013 
3014   os::native_path(fullname);
3015 
3016   char *path = _mktemp(fullname);
3017   if (path == NULL) {
3018     warning(&quot;_mktemp could not create file name from template %s (%s)&quot;, fullname, os::strerror(errno));
</pre>
<hr />
<pre>
3054   }
3055 
3056   LPVOID addr = MapViewOfFileEx(fileMapping, FILE_MAP_WRITE, 0, 0, size, base);
3057 
3058   CloseHandle(fileMapping);
3059 
3060   return (char*)addr;
3061 }
3062 
3063 char* os::replace_existing_mapping_with_file_mapping(char* base, size_t size, int fd) {
3064   assert(fd != -1, &quot;File descriptor is not valid&quot;);
3065   assert(base != NULL, &quot;Base address cannot be NULL&quot;);
3066 
3067   release_memory(base, size);
3068   return map_memory_to_file(base, size, fd);
3069 }
3070 
3071 // On win32, one cannot release just a part of reserved memory, it&#39;s an
3072 // all or nothing deal.  When we split a reservation, we must break the
3073 // reservation into two reservations.
<span class="line-modified">3074 void os::pd_split_reserved_memory(char *base, size_t size, size_t split,</span>
<span class="line-modified">3075                                   bool realloc) {</span>
<span class="line-modified">3076   if (size &gt; 0) {</span>
<span class="line-modified">3077     release_memory(base, size);</span>
<span class="line-modified">3078     if (realloc) {</span>
<span class="line-modified">3079       reserve_memory(split, base);</span>
<span class="line-modified">3080     }</span>
<span class="line-modified">3081     if (size != split) {</span>
<span class="line-modified">3082       reserve_memory(size - split, base + split);</span>
<span class="line-modified">3083     }</span>
<span class="line-modified">3084   }</span>


3085 }
3086 
3087 // Multiple threads can race in this code but it&#39;s not possible to unmap small sections of
3088 // virtual space to get requested alignment, like posix-like os&#39;s.
3089 // Windows prevents multiple thread from remapping over each other so this loop is thread-safe.
3090 char* os::reserve_memory_aligned(size_t size, size_t alignment, int file_desc) {
3091   assert((alignment &amp; (os::vm_allocation_granularity() - 1)) == 0,
3092          &quot;Alignment must be a multiple of allocation granularity (page size)&quot;);
3093   assert((size &amp; (alignment -1)) == 0, &quot;size must be &#39;alignment&#39; aligned&quot;);
3094 
3095   size_t extra_size = size + alignment;
3096   assert(extra_size &gt;= size, &quot;overflow, size is too large to allow alignment&quot;);
3097 
3098   char* aligned_base = NULL;
3099 
3100   do {
3101     char* extra_base = os::reserve_memory(extra_size, NULL, alignment, file_desc);
3102     if (extra_base == NULL) {
3103       return NULL;
3104     }
</pre>
</td>
<td>
<hr />
<pre>
2699 // to the user:
2700 //   + select Control Panel -&gt; Administrative Tools -&gt; Local Security Policy
2701 //   + select Local Policies -&gt; User Rights Assignment
2702 //   + double click &quot;Lock pages in memory&quot;, add users and/or groups
2703 //   + reboot
2704 // Note the above steps are needed for administrator as well, as administrators
2705 // by default do not have the privilege to lock pages in memory.
2706 //
2707 // Note about Windows 2003: although the API supports committing large page
2708 // memory on a page-by-page basis and VirtualAlloc() returns success under this
2709 // scenario, I found through experiment it only uses large page if the entire
2710 // memory region is reserved and committed in a single VirtualAlloc() call.
2711 // This makes Windows large page support more or less like Solaris ISM, in
2712 // that the entire heap must be committed upfront. This probably will change
2713 // in the future, if so the code below needs to be revisited.
2714 
2715 #ifndef MEM_LARGE_PAGES
2716   #define MEM_LARGE_PAGES 0x20000000
2717 #endif
2718 
<span class="line-modified">2719 #define VirtualFreeChecked(mem, size, type)                       \</span>
<span class="line-modified">2720   do {                                                            \</span>
<span class="line-added">2721     bool ret = VirtualFree(mem, size, type);                      \</span>
<span class="line-added">2722     assert(ret, &quot;Failed to free memory: &quot; PTR_FORMAT, p2i(mem));  \</span>
<span class="line-added">2723   } while (false)</span>
<span class="line-added">2724 </span>
<span class="line-added">2725 // The number of bytes is setup to match 1 pixel and 32 bits per pixel.</span>
<span class="line-added">2726 static const int gdi_tiny_bitmap_width_bytes = 4;</span>
<span class="line-added">2727 </span>
<span class="line-added">2728 static HBITMAP gdi_create_tiny_bitmap(void* mem) {</span>
<span class="line-added">2729   // The documentation for CreateBitmap states a word-alignment requirement.</span>
<span class="line-added">2730   STATIC_ASSERT(is_aligned_(gdi_tiny_bitmap_width_bytes, sizeof(WORD)));</span>
<span class="line-added">2731 </span>
<span class="line-added">2732   // Some callers use this function to test if memory crossing separate memory</span>
<span class="line-added">2733   // reservations can be used. Create a height of 2 to make sure that one pixel</span>
<span class="line-added">2734   // ends up in the first reservation and the other in the second.</span>
<span class="line-added">2735   int nHeight = 2;</span>
<span class="line-added">2736 </span>
<span class="line-added">2737   assert(is_aligned(mem, gdi_tiny_bitmap_width_bytes), &quot;Incorrect alignment&quot;);</span>
<span class="line-added">2738 </span>
<span class="line-added">2739   // Width is one pixel and correlates with gdi_tiny_bitmap_width_bytes.</span>
<span class="line-added">2740   int nWidth = 1;</span>
<span class="line-added">2741 </span>
<span class="line-added">2742   // Calculate bit count - will be 32.</span>
<span class="line-added">2743   UINT nBitCount = gdi_tiny_bitmap_width_bytes / nWidth * BitsPerByte;</span>
<span class="line-added">2744 </span>
<span class="line-added">2745   return CreateBitmap(</span>
<span class="line-added">2746       nWidth,</span>
<span class="line-added">2747       nHeight,</span>
<span class="line-added">2748       1,         // nPlanes</span>
<span class="line-added">2749       nBitCount,</span>
<span class="line-added">2750       mem);      // lpBits</span>
<span class="line-added">2751 }</span>
<span class="line-added">2752 </span>
<span class="line-added">2753 // It has been found that some of the GDI functions fail under these two situations:</span>
<span class="line-added">2754 //  1) When used with large pages</span>
<span class="line-added">2755 //  2) When mem crosses the boundary between two separate memory reservations.</span>
<span class="line-added">2756 //</span>
<span class="line-added">2757 // This is a small test used to see if the current GDI implementation is</span>
<span class="line-added">2758 // susceptible to any of these problems.</span>
<span class="line-added">2759 static bool gdi_can_use_memory(void* mem) {</span>
<span class="line-added">2760   HBITMAP bitmap = gdi_create_tiny_bitmap(mem);</span>
<span class="line-added">2761   if (bitmap != NULL) {</span>
<span class="line-added">2762     DeleteObject(bitmap);</span>
<span class="line-added">2763     return true;</span>
<span class="line-added">2764   }</span>
<span class="line-added">2765 </span>
<span class="line-added">2766   // Verify that the bitmap could be created with a normal page.</span>
<span class="line-added">2767   // If this fails, the testing method above isn&#39;t reliable.</span>
<span class="line-added">2768 #ifdef ASSERT</span>
<span class="line-added">2769   void* verify_mem = ::malloc(4 * 1024);</span>
<span class="line-added">2770   HBITMAP verify_bitmap = gdi_create_tiny_bitmap(verify_mem);</span>
<span class="line-added">2771   if (verify_bitmap == NULL) {</span>
<span class="line-added">2772     fatal(&quot;Couldn&#39;t create test bitmap with malloced memory&quot;);</span>
<span class="line-added">2773   } else {</span>
<span class="line-added">2774     DeleteObject(verify_bitmap);</span>
<span class="line-added">2775   }</span>
<span class="line-added">2776   ::free(verify_mem);</span>
<span class="line-added">2777 #endif</span>
<span class="line-added">2778 </span>
<span class="line-added">2779   return false;</span>
<span class="line-added">2780 }</span>
<span class="line-added">2781 </span>
<span class="line-added">2782 // Test if GDI functions work when memory spans</span>
<span class="line-added">2783 // two adjacent memory reservations.</span>
<span class="line-added">2784 static bool gdi_can_use_split_reservation_memory(bool use_large_pages, size_t granule) {</span>
<span class="line-added">2785   DWORD mem_large_pages = use_large_pages ? MEM_LARGE_PAGES : 0;</span>
<span class="line-added">2786 </span>
<span class="line-added">2787   // Find virtual memory range. Two granules for regions and one for alignment.</span>
<span class="line-added">2788   void* reserved = VirtualAlloc(NULL,</span>
<span class="line-added">2789                                 granule * 3,</span>
<span class="line-added">2790                                 MEM_RESERVE,</span>
<span class="line-added">2791                                 PAGE_NOACCESS);</span>
<span class="line-added">2792   if (reserved == NULL) {</span>
<span class="line-added">2793     // Can&#39;t proceed with test - pessimistically report false</span>
<span class="line-added">2794     return false;</span>
<span class="line-added">2795   }</span>
<span class="line-added">2796   VirtualFreeChecked(reserved, 0, MEM_RELEASE);</span>
<span class="line-added">2797 </span>
<span class="line-added">2798   // Ensure proper alignment</span>
<span class="line-added">2799   void* res0 = align_up(reserved, granule);</span>
<span class="line-added">2800   void* res1 = (char*)res0 + granule;</span>
<span class="line-added">2801 </span>
<span class="line-added">2802   // Reserve and commit the first part</span>
<span class="line-added">2803   void* mem0 = VirtualAlloc(res0,</span>
<span class="line-added">2804                             granule,</span>
<span class="line-added">2805                             MEM_RESERVE|MEM_COMMIT|mem_large_pages,</span>
<span class="line-added">2806                             PAGE_READWRITE);</span>
<span class="line-added">2807   if (mem0 != res0) {</span>
<span class="line-added">2808     // Can&#39;t proceed with test - pessimistically report false</span>
<span class="line-added">2809     return false;</span>
<span class="line-added">2810   }</span>
<span class="line-added">2811 </span>
<span class="line-added">2812   // Reserve and commit the second part</span>
<span class="line-added">2813   void* mem1 = VirtualAlloc(res1,</span>
<span class="line-added">2814                             granule,</span>
<span class="line-added">2815                             MEM_RESERVE|MEM_COMMIT|mem_large_pages,</span>
<span class="line-added">2816                             PAGE_READWRITE);</span>
<span class="line-added">2817   if (mem1 != res1) {</span>
<span class="line-added">2818     VirtualFreeChecked(mem0, 0, MEM_RELEASE);</span>
<span class="line-added">2819     // Can&#39;t proceed with test - pessimistically report false</span>
<span class="line-added">2820     return false;</span>
<span class="line-added">2821   }</span>
<span class="line-added">2822 </span>
<span class="line-added">2823   // Set the bitmap&#39;s bits to point one &quot;width&quot; bytes before, so that</span>
<span class="line-added">2824   // the bitmap extends across the reservation boundary.</span>
<span class="line-added">2825   void* bitmapBits = (char*)mem1 - gdi_tiny_bitmap_width_bytes;</span>
<span class="line-added">2826 </span>
<span class="line-added">2827   bool success = gdi_can_use_memory(bitmapBits);</span>
<span class="line-added">2828 </span>
<span class="line-added">2829   VirtualFreeChecked(mem1, 0, MEM_RELEASE);</span>
<span class="line-added">2830   VirtualFreeChecked(mem0, 0, MEM_RELEASE);</span>
<span class="line-added">2831 </span>
<span class="line-added">2832   return success;</span>
<span class="line-added">2833 }</span>
2834 
2835 // Container for NUMA node list info
2836 class NUMANodeListHolder {
2837  private:
2838   int *_numa_used_node_list;  // allocated below
2839   int _numa_used_node_count;
2840 
2841   void free_node_list() {
2842     FREE_C_HEAP_ARRAY(int, _numa_used_node_list);
2843   }
2844 
2845  public:
2846   NUMANodeListHolder() {
2847     _numa_used_node_count = 0;
2848     _numa_used_node_list = NULL;
2849     // do rest of initialization in build routine (after function pointers are set up)
2850   }
2851 
2852   ~NUMANodeListHolder() {
2853     free_node_list();
</pre>
<hr />
<pre>
2862     free_node_list();
2863     _numa_used_node_list = NEW_C_HEAP_ARRAY(int, highest_node_number + 1, mtInternal);
2864     for (unsigned int i = 0; i &lt;= highest_node_number; i++) {
2865       ULONGLONG proc_mask_numa_node;
2866       if (!GetNumaNodeProcessorMask(i, &amp;proc_mask_numa_node)) return false;
2867       if ((proc_aff_mask &amp; proc_mask_numa_node)!=0) {
2868         _numa_used_node_list[_numa_used_node_count++] = i;
2869       }
2870     }
2871     return (_numa_used_node_count &gt; 1);
2872   }
2873 
2874   int get_count() { return _numa_used_node_count; }
2875   int get_node_list_entry(int n) {
2876     // for indexes out of range, returns -1
2877     return (n &lt; _numa_used_node_count ? _numa_used_node_list[n] : -1);
2878   }
2879 
2880 } numa_node_list_holder;
2881 


2882 static size_t _large_page_size = 0;
2883 
2884 static bool request_lock_memory_privilege() {
<span class="line-modified">2885   HANDLE hProcess = OpenProcess(PROCESS_QUERY_INFORMATION, FALSE,</span>
<span class="line-modified">2886                                 os::current_process_id());</span>
2887 
<span class="line-added">2888   bool success = false;</span>
<span class="line-added">2889   HANDLE hToken = NULL;</span>
2890   LUID luid;
<span class="line-modified">2891   if (hProcess != NULL &amp;&amp;</span>
<span class="line-modified">2892       OpenProcessToken(hProcess, TOKEN_ADJUST_PRIVILEGES, &amp;hToken) &amp;&amp;</span>
2893       LookupPrivilegeValue(NULL, &quot;SeLockMemoryPrivilege&quot;, &amp;luid)) {
2894 
2895     TOKEN_PRIVILEGES tp;
2896     tp.PrivilegeCount = 1;
2897     tp.Privileges[0].Luid = luid;
2898     tp.Privileges[0].Attributes = SE_PRIVILEGE_ENABLED;
2899 
2900     // AdjustTokenPrivileges() may return TRUE even when it couldn&#39;t change the
2901     // privilege. Check GetLastError() too. See MSDN document.
<span class="line-modified">2902     if (AdjustTokenPrivileges(hToken, false, &amp;tp, sizeof(tp), NULL, NULL) &amp;&amp;</span>
2903         (GetLastError() == ERROR_SUCCESS)) {
<span class="line-modified">2904       success = true;</span>
2905     }
2906   }
2907 
<span class="line-modified">2908   // Cleanup</span>
<span class="line-modified">2909   if (hProcess != NULL) {</span>
<span class="line-added">2910     CloseHandle(hProcess);</span>
<span class="line-added">2911   }</span>
<span class="line-added">2912   if (hToken != NULL) {</span>
<span class="line-added">2913     CloseHandle(hToken);</span>
<span class="line-added">2914   }</span>
2915 
<span class="line-modified">2916   return success;</span>




2917 }
2918 
2919 static bool numa_interleaving_init() {
2920   bool success = false;

2921 
2922   // print a warning if UseNUMAInterleaving flag is specified on command line
<span class="line-modified">2923   bool warn_on_failure = !FLAG_IS_DEFAULT(UseNUMAInterleaving);</span>
<span class="line-added">2924 </span>
2925 #define WARN(msg) if (warn_on_failure) { warning(msg); }
2926 
2927   // NUMAInterleaveGranularity cannot be less than vm_allocation_granularity (or _large_page_size if using large pages)
2928   size_t min_interleave_granularity = UseLargePages ? _large_page_size : os::vm_allocation_granularity();
2929   NUMAInterleaveGranularity = align_up(NUMAInterleaveGranularity, min_interleave_granularity);
2930 
<span class="line-modified">2931   if (!numa_node_list_holder.build()) {</span>









2932     WARN(&quot;Process does not cover multiple NUMA nodes.&quot;);
<span class="line-added">2933     WARN(&quot;...Ignoring UseNUMAInterleaving flag.&quot;);</span>
<span class="line-added">2934     return false;</span>
2935   }
<span class="line-modified">2936 </span>
<span class="line-modified">2937   if (!gdi_can_use_split_reservation_memory(UseLargePages, min_interleave_granularity)) {</span>
<span class="line-added">2938     WARN(&quot;Windows GDI cannot handle split reservations.&quot;);</span>
<span class="line-added">2939     WARN(&quot;...Ignoring UseNUMAInterleaving flag.&quot;);</span>
<span class="line-added">2940     return false;</span>
2941   }
<span class="line-modified">2942 </span>
<span class="line-added">2943   if (log_is_enabled(Debug, os, cpu)) {</span>
<span class="line-added">2944     Log(os, cpu) log;</span>
<span class="line-added">2945     log.debug(&quot;NUMA UsedNodeCount=%d, namely &quot;, numa_node_list_holder.get_count());</span>
<span class="line-added">2946     for (int i = 0; i &lt; numa_node_list_holder.get_count(); i++) {</span>
<span class="line-added">2947       log.debug(&quot;  %d &quot;, numa_node_list_holder.get_node_list_entry(i));</span>
<span class="line-added">2948     }</span>
<span class="line-added">2949   }</span>
<span class="line-added">2950 </span>
2951 #undef WARN
<span class="line-added">2952 </span>
<span class="line-added">2953   return true;</span>
2954 }
2955 
2956 // this routine is used whenever we need to reserve a contiguous VA range
2957 // but we need to make separate VirtualAlloc calls for each piece of the range
2958 // Reasons for doing this:
2959 //  * UseLargePagesIndividualAllocation was set (normally only needed on WS2003 but possible to be set otherwise)
2960 //  * UseNUMAInterleaving requires a separate node for each piece
2961 static char* allocate_pages_individually(size_t bytes, char* addr, DWORD flags,
2962                                          DWORD prot,
2963                                          bool should_inject_error = false) {
2964   char * p_buf;
2965   // note: at setup time we guaranteed that NUMAInterleaveGranularity was aligned up to a page size
2966   size_t page_size = UseLargePages ? _large_page_size : os::vm_allocation_granularity();
2967   size_t chunk_size = UseNUMAInterleaving ? NUMAInterleaveGranularity : page_size;
2968 
2969   // first reserve enough address space in advance since we want to be
2970   // able to break a single contiguous virtual address range into multiple
2971   // large page commits but WS2003 does not allow reserving large page space
2972   // so we just use 4K pages for reserve, this gives us a legal contiguous
2973   // address space. then we will deallocate that reservation, and re alloc
</pre>
<hr />
<pre>
3054 #endif
3055       return NULL;
3056     }
3057 
3058     bytes_remaining -= bytes_to_rq;
3059     next_alloc_addr += bytes_to_rq;
3060     count++;
3061   }
3062   // Although the memory is allocated individually, it is returned as one.
3063   // NMT records it as one block.
3064   if ((flags &amp; MEM_COMMIT) != 0) {
3065     MemTracker::record_virtual_memory_reserve_and_commit((address)p_buf, bytes, CALLER_PC);
3066   } else {
3067     MemTracker::record_virtual_memory_reserve((address)p_buf, bytes, CALLER_PC);
3068   }
3069 
3070   // made it this far, success
3071   return p_buf;
3072 }
3073 
<span class="line-modified">3074 static size_t large_page_init_decide_size() {</span>




3075   // print a warning if any large page related flag is specified on command line
3076   bool warn_on_failure = !FLAG_IS_DEFAULT(UseLargePages) ||
3077                          !FLAG_IS_DEFAULT(LargePageSizeInBytes);

3078 
3079 #define WARN(msg) if (warn_on_failure) { warning(msg); }
<span class="line-modified">3080 </span>
<span class="line-modified">3081   if (!request_lock_memory_privilege()) {</span>
<span class="line-modified">3082     WARN(&quot;JVM cannot use large page memory because it does not have enough privilege to lock pages in memory.&quot;);</span>
<span class="line-modified">3083     return 0;</span>
<span class="line-modified">3084   }</span>
<span class="line-modified">3085 </span>
<span class="line-modified">3086   size_t size = GetLargePageMinimum();</span>
<span class="line-modified">3087   if (size == 0) {</span>
<span class="line-modified">3088     WARN(&quot;Large page is not supported by the processor.&quot;);</span>
<span class="line-modified">3089     return 0;</span>
<span class="line-modified">3090   }</span>
<span class="line-modified">3091 </span>


3092 #if defined(IA32) || defined(AMD64)
<span class="line-modified">3093   if (size &gt; 4*M || LargePageSizeInBytes &gt; 4*M) {</span>
<span class="line-added">3094     WARN(&quot;JVM cannot use large pages bigger than 4mb.&quot;);</span>
<span class="line-added">3095     return 0;</span>
<span class="line-added">3096   }</span>
3097 #endif
<span class="line-modified">3098 </span>
<span class="line-modified">3099   if (LargePageSizeInBytes &gt; 0 &amp;&amp; LargePageSizeInBytes % size == 0) {</span>
<span class="line-modified">3100     size = LargePageSizeInBytes;</span>
<span class="line-modified">3101   }</span>
<span class="line-modified">3102 </span>
<span class="line-added">3103   // Now test allocating a page</span>
<span class="line-added">3104   void* large_page = VirtualAlloc(NULL,</span>
<span class="line-added">3105                                   size,</span>
<span class="line-added">3106                                   MEM_RESERVE|MEM_COMMIT|MEM_LARGE_PAGES,</span>
<span class="line-added">3107                                   PAGE_READWRITE);</span>
<span class="line-added">3108   if (large_page == NULL) {</span>
<span class="line-added">3109     WARN(&quot;JVM cannot allocate one single large page.&quot;);</span>
<span class="line-added">3110     return 0;</span>
<span class="line-added">3111   }</span>
<span class="line-added">3112 </span>
<span class="line-added">3113   // Detect if GDI can use memory backed by large pages</span>
<span class="line-added">3114   if (!gdi_can_use_memory(large_page)) {</span>
<span class="line-added">3115     WARN(&quot;JVM cannot use large pages because of bug in Windows GDI.&quot;);</span>
<span class="line-added">3116     return 0;</span>
3117   }
<span class="line-added">3118 </span>
<span class="line-added">3119   // Release test page</span>
<span class="line-added">3120   VirtualFreeChecked(large_page, 0, MEM_RELEASE);</span>
<span class="line-added">3121 </span>
3122 #undef WARN
3123 
<span class="line-added">3124   return size;</span>
<span class="line-added">3125 }</span>
<span class="line-added">3126 </span>
<span class="line-added">3127 void os::large_page_init() {</span>
<span class="line-added">3128   if (!UseLargePages) {</span>
<span class="line-added">3129     return;</span>
<span class="line-added">3130   }</span>
<span class="line-added">3131 </span>
<span class="line-added">3132   _large_page_size = large_page_init_decide_size();</span>
<span class="line-added">3133 </span>
3134   const size_t default_page_size = (size_t) vm_page_size();
<span class="line-modified">3135   if (_large_page_size &gt; default_page_size) {</span>
3136     _page_sizes[0] = _large_page_size;
3137     _page_sizes[1] = default_page_size;
3138     _page_sizes[2] = 0;
3139   }
3140 
<span class="line-modified">3141   UseLargePages = _large_page_size != 0;</span>
<span class="line-modified">3142 </span>
<span class="line-added">3143   if (UseLargePages &amp;&amp; UseLargePagesIndividualAllocation) {</span>
<span class="line-added">3144     if (!gdi_can_use_split_reservation_memory(true /* use_large_pages */, _large_page_size)) {</span>
<span class="line-added">3145       if (FLAG_IS_CMDLINE(UseLargePagesIndividualAllocation)) {</span>
<span class="line-added">3146         warning(&quot;Windows GDI cannot handle split reservations.&quot;);</span>
<span class="line-added">3147         warning(&quot;...Ignoring UseLargePagesIndividualAllocation flag.&quot;);</span>
<span class="line-added">3148       }</span>
<span class="line-added">3149       UseLargePagesIndividualAllocation = false;</span>
<span class="line-added">3150     }</span>
<span class="line-added">3151   }</span>
3152 }
3153 
3154 int os::create_file_for_heap(const char* dir) {
3155 
3156   const char name_template[] = &quot;/jvmheap.XXXXXX&quot;;
3157 
3158   size_t fullname_len = strlen(dir) + strlen(name_template);
3159   char *fullname = (char*)os::malloc(fullname_len + 1, mtInternal);
3160   if (fullname == NULL) {
3161     vm_exit_during_initialization(err_msg(&quot;Malloc failed during creation of backing file for heap (%s)&quot;, os::strerror(errno)));
3162     return -1;
3163   }
3164   int n = snprintf(fullname, fullname_len + 1, &quot;%s%s&quot;, dir, name_template);
3165   assert((size_t)n == fullname_len, &quot;Unexpected number of characters in string&quot;);
3166 
3167   os::native_path(fullname);
3168 
3169   char *path = _mktemp(fullname);
3170   if (path == NULL) {
3171     warning(&quot;_mktemp could not create file name from template %s (%s)&quot;, fullname, os::strerror(errno));
</pre>
<hr />
<pre>
3207   }
3208 
3209   LPVOID addr = MapViewOfFileEx(fileMapping, FILE_MAP_WRITE, 0, 0, size, base);
3210 
3211   CloseHandle(fileMapping);
3212 
3213   return (char*)addr;
3214 }
3215 
3216 char* os::replace_existing_mapping_with_file_mapping(char* base, size_t size, int fd) {
3217   assert(fd != -1, &quot;File descriptor is not valid&quot;);
3218   assert(base != NULL, &quot;Base address cannot be NULL&quot;);
3219 
3220   release_memory(base, size);
3221   return map_memory_to_file(base, size, fd);
3222 }
3223 
3224 // On win32, one cannot release just a part of reserved memory, it&#39;s an
3225 // all or nothing deal.  When we split a reservation, we must break the
3226 // reservation into two reservations.
<span class="line-modified">3227 void os::split_reserved_memory(char *base, size_t size, size_t split) {</span>
<span class="line-modified">3228 </span>
<span class="line-modified">3229   char* const split_address = base + split;</span>
<span class="line-modified">3230   assert(size &gt; 0, &quot;Sanity&quot;);</span>
<span class="line-modified">3231   assert(size &gt; split, &quot;Sanity&quot;);</span>
<span class="line-modified">3232   assert(split &gt; 0, &quot;Sanity&quot;);</span>
<span class="line-modified">3233   assert(is_aligned(base, os::vm_allocation_granularity()), &quot;Sanity&quot;);</span>
<span class="line-modified">3234   assert(is_aligned(split_address, os::vm_allocation_granularity()), &quot;Sanity&quot;);</span>
<span class="line-modified">3235 </span>
<span class="line-modified">3236   release_memory(base, size);</span>
<span class="line-modified">3237   reserve_memory(split, base);</span>
<span class="line-added">3238   reserve_memory(size - split, split_address);</span>
<span class="line-added">3239 </span>
3240 }
3241 
3242 // Multiple threads can race in this code but it&#39;s not possible to unmap small sections of
3243 // virtual space to get requested alignment, like posix-like os&#39;s.
3244 // Windows prevents multiple thread from remapping over each other so this loop is thread-safe.
3245 char* os::reserve_memory_aligned(size_t size, size_t alignment, int file_desc) {
3246   assert((alignment &amp; (os::vm_allocation_granularity() - 1)) == 0,
3247          &quot;Alignment must be a multiple of allocation granularity (page size)&quot;);
3248   assert((size &amp; (alignment -1)) == 0, &quot;size must be &#39;alignment&#39; aligned&quot;);
3249 
3250   size_t extra_size = size + alignment;
3251   assert(extra_size &gt;= size, &quot;overflow, size is too large to allow alignment&quot;);
3252 
3253   char* aligned_base = NULL;
3254 
3255   do {
3256     char* extra_base = os::reserve_memory(extra_size, NULL, alignment, file_desc);
3257     if (extra_base == NULL) {
3258       return NULL;
3259     }
</pre>
</td>
</tr>
</table>
<center><a href="os_perf_windows.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="pdh_interface.cpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>