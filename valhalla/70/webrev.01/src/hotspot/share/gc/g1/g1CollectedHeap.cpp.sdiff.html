<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/share/gc/g1/g1CollectedHeap.cpp</title>
    <link rel="stylesheet" href="../../../../../style.css" />
  </head>
<body>
<center><a href="g1BarrierSet.inline.hpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../index.html" target="_top">index</a> <a href="g1CollectedHeap.hpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/share/gc/g1/g1CollectedHeap.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
  24 
  25 #include &quot;precompiled.hpp&quot;
  26 #include &quot;classfile/classLoaderDataGraph.hpp&quot;
  27 #include &quot;classfile/metadataOnStackMark.hpp&quot;
  28 #include &quot;classfile/stringTable.hpp&quot;
  29 #include &quot;code/codeCache.hpp&quot;
  30 #include &quot;code/icBuffer.hpp&quot;
  31 #include &quot;gc/g1/g1Allocator.inline.hpp&quot;
  32 #include &quot;gc/g1/g1Arguments.hpp&quot;
  33 #include &quot;gc/g1/g1BarrierSet.hpp&quot;
  34 #include &quot;gc/g1/g1CardTableEntryClosure.hpp&quot;
  35 #include &quot;gc/g1/g1CollectedHeap.inline.hpp&quot;
  36 #include &quot;gc/g1/g1CollectionSet.hpp&quot;
  37 #include &quot;gc/g1/g1CollectorState.hpp&quot;
  38 #include &quot;gc/g1/g1ConcurrentRefine.hpp&quot;
  39 #include &quot;gc/g1/g1ConcurrentRefineThread.hpp&quot;
  40 #include &quot;gc/g1/g1ConcurrentMarkThread.inline.hpp&quot;
  41 #include &quot;gc/g1/g1DirtyCardQueue.hpp&quot;
  42 #include &quot;gc/g1/g1EvacStats.inline.hpp&quot;
  43 #include &quot;gc/g1/g1FullCollector.hpp&quot;

  44 #include &quot;gc/g1/g1GCPhaseTimes.hpp&quot;
  45 #include &quot;gc/g1/g1HeapSizingPolicy.hpp&quot;
  46 #include &quot;gc/g1/g1HeapTransition.hpp&quot;
  47 #include &quot;gc/g1/g1HeapVerifier.hpp&quot;
  48 #include &quot;gc/g1/g1HotCardCache.hpp&quot;
  49 #include &quot;gc/g1/g1MemoryPool.hpp&quot;
  50 #include &quot;gc/g1/g1OopClosures.inline.hpp&quot;
  51 #include &quot;gc/g1/g1ParallelCleaning.hpp&quot;
  52 #include &quot;gc/g1/g1ParScanThreadState.inline.hpp&quot;
  53 #include &quot;gc/g1/g1Policy.hpp&quot;
  54 #include &quot;gc/g1/g1RedirtyCardsQueue.hpp&quot;
  55 #include &quot;gc/g1/g1RegionToSpaceMapper.hpp&quot;
  56 #include &quot;gc/g1/g1RemSet.hpp&quot;
  57 #include &quot;gc/g1/g1RootClosures.hpp&quot;
  58 #include &quot;gc/g1/g1RootProcessor.hpp&quot;
  59 #include &quot;gc/g1/g1SATBMarkQueueSet.hpp&quot;
  60 #include &quot;gc/g1/g1StringDedup.hpp&quot;
  61 #include &quot;gc/g1/g1ThreadLocalData.hpp&quot;
  62 #include &quot;gc/g1/g1Trace.hpp&quot;
  63 #include &quot;gc/g1/g1YCTypes.hpp&quot;
</pre>
<hr />
<pre>
1524   _is_alive_closure_stw(this),
1525   _is_subject_to_discovery_stw(this),
1526   _ref_processor_cm(NULL),
1527   _is_alive_closure_cm(this),
1528   _is_subject_to_discovery_cm(this),
1529   _region_attr() {
1530 
1531   _verifier = new G1HeapVerifier(this);
1532 
1533   _allocator = new G1Allocator(this);
1534 
1535   _heap_sizing_policy = G1HeapSizingPolicy::create(this, _policy-&gt;analytics());
1536 
1537   _humongous_object_threshold_in_words = humongous_threshold_for(HeapRegion::GrainWords);
1538 
1539   // Override the default _filler_array_max_size so that no humongous filler
1540   // objects are created.
1541   _filler_array_max_size = _humongous_object_threshold_in_words;
1542 
1543   uint n_queues = ParallelGCThreads;
<span class="line-modified">1544   _task_queues = new RefToScanQueueSet(n_queues);</span>
1545 
1546   _evacuation_failed_info_array = NEW_C_HEAP_ARRAY(EvacuationFailedInfo, n_queues, mtGC);
1547 
1548   for (uint i = 0; i &lt; n_queues; i++) {
<span class="line-modified">1549     RefToScanQueue* q = new RefToScanQueue();</span>
1550     q-&gt;initialize();
1551     _task_queues-&gt;register_queue(i, q);
1552     ::new (&amp;_evacuation_failed_info_array[i]) EvacuationFailedInfo();
1553   }
1554 
1555   // Initialize the G1EvacuationFailureALot counters and flags.
1556   NOT_PRODUCT(reset_evacuation_should_fail();)
1557   _gc_tracer_stw-&gt;initialize();
1558 
1559   guarantee(_task_queues != NULL, &quot;task_queues allocation failure.&quot;);
1560 }
1561 
1562 static size_t actual_reserved_page_size(ReservedSpace rs) {
1563   size_t page_size = os::vm_page_size();
1564   if (UseLargePages) {
1565     // There are two ways to manage large page memory.
1566     // 1. OS supports committing large page memory.
1567     // 2. OS doesn&#39;t support committing large page memory so ReservedSpace manages it.
1568     //    And ReservedSpace calls it &#39;special&#39;. If we failed to set &#39;special&#39;,
1569     //    we reserved memory without large page.
</pre>
<hr />
<pre>
3381 
3382   virtual void do_oop(narrowOop* p) { do_oop_work(p); }
3383   virtual void do_oop(      oop* p) { do_oop_work(p); }
3384 
3385   template &lt;class T&gt; void do_oop_work(T* p) {
3386     oop obj = RawAccess&lt;&gt;::oop_load(p);
3387 
3388     if (_g1h-&gt;is_in_cset_or_humongous(obj)) {
3389       // If the referent object has been forwarded (either copied
3390       // to a new location or to itself in the event of an
3391       // evacuation failure) then we need to update the reference
3392       // field and, if both reference and referent are in the G1
3393       // heap, update the RSet for the referent.
3394       //
3395       // If the referent has not been forwarded then we have to keep
3396       // it alive by policy. Therefore we have copy the referent.
3397       //
3398       // When the queue is drained (after each phase of reference processing)
3399       // the object and it&#39;s followers will be copied, the reference field set
3400       // to point to the new location, and the RSet updated.
<span class="line-modified">3401       _par_scan_state-&gt;push_on_queue(p);</span>
3402     }
3403   }
3404 };
3405 
3406 // Serial drain queue closure. Called as the &#39;complete_gc&#39;
3407 // closure for each discovered list in some of the
3408 // reference processing phases.
3409 
3410 class G1STWDrainQueueClosure: public VoidClosure {
3411 protected:
3412   G1CollectedHeap* _g1h;
3413   G1ParScanThreadState* _par_scan_state;
3414 
3415   G1ParScanThreadState*   par_scan_state() { return _par_scan_state; }
3416 
3417 public:
3418   G1STWDrainQueueClosure(G1CollectedHeap* g1h, G1ParScanThreadState* pss) :
3419     _g1h(g1h),
3420     _par_scan_state(pss)
3421   { }
3422 
3423   void do_void() {
3424     G1ParScanThreadState* const pss = par_scan_state();
3425     pss-&gt;trim_queue();
3426   }
3427 };
3428 
3429 // Parallel Reference Processing closures
3430 
3431 // Implementation of AbstractRefProcTaskExecutor for parallel reference
3432 // processing during G1 evacuation pauses.
3433 
3434 class G1STWRefProcTaskExecutor: public AbstractRefProcTaskExecutor {
3435 private:
3436   G1CollectedHeap*          _g1h;
3437   G1ParScanThreadStateSet*  _pss;
<span class="line-modified">3438   RefToScanQueueSet*        _queues;</span>
3439   WorkGang*                 _workers;
3440 
3441 public:
3442   G1STWRefProcTaskExecutor(G1CollectedHeap* g1h,
3443                            G1ParScanThreadStateSet* per_thread_states,
3444                            WorkGang* workers,
<span class="line-modified">3445                            RefToScanQueueSet *task_queues) :</span>
3446     _g1h(g1h),
3447     _pss(per_thread_states),
3448     _queues(task_queues),
3449     _workers(workers)
3450   {
3451     g1h-&gt;ref_processor_stw()-&gt;set_active_mt_degree(workers-&gt;active_workers());
3452   }
3453 
3454   // Executes the given task using concurrent marking worker threads.
3455   virtual void execute(ProcessTask&amp; task, uint ergo_workers);
3456 };
3457 
3458 // Gang task for possibly parallel reference processing
3459 
3460 class G1STWRefProcTaskProxy: public AbstractGangTask {
3461   typedef AbstractRefProcTaskExecutor::ProcessTask ProcessTask;
3462   ProcessTask&amp;     _proc_task;
3463   G1CollectedHeap* _g1h;
3464   G1ParScanThreadStateSet* _pss;
<span class="line-modified">3465   RefToScanQueueSet* _task_queues;</span>
3466   TaskTerminator* _terminator;
3467 
3468 public:
3469   G1STWRefProcTaskProxy(ProcessTask&amp; proc_task,
3470                         G1CollectedHeap* g1h,
3471                         G1ParScanThreadStateSet* per_thread_states,
<span class="line-modified">3472                         RefToScanQueueSet *task_queues,</span>
3473                         TaskTerminator* terminator) :
3474     AbstractGangTask(&quot;Process reference objects in parallel&quot;),
3475     _proc_task(proc_task),
3476     _g1h(g1h),
3477     _pss(per_thread_states),
3478     _task_queues(task_queues),
3479     _terminator(terminator)
3480   {}
3481 
3482   virtual void work(uint worker_id) {
3483     // The reference processing task executed by a single worker.
3484     ResourceMark rm;
3485     HandleMark   hm;
3486 
3487     G1STWIsAliveClosure is_alive(_g1h);
3488 
3489     G1ParScanThreadState* pss = _pss-&gt;state_for_worker(worker_id);
3490     pss-&gt;set_ref_discoverer(NULL);
3491 
3492     // Keep alive closure.
</pre>
<hr />
<pre>
3783   // InitialMark needs claim bits to keep track of the marked-through CLDs.
3784   if (collector_state()-&gt;in_initial_mark_gc()) {
3785     concurrent_mark()-&gt;pre_initial_mark();
3786 
3787     double start_clear_claimed_marks = os::elapsedTime();
3788 
3789     ClassLoaderDataGraph::clear_claimed_marks();
3790 
3791     double recorded_clear_claimed_marks_time_ms = (os::elapsedTime() - start_clear_claimed_marks) * 1000.0;
3792     phase_times()-&gt;record_clear_claimed_marks_time_ms(recorded_clear_claimed_marks_time_ms);
3793   }
3794 
3795   // Should G1EvacuationFailureALot be in effect for this GC?
3796   NOT_PRODUCT(set_evacuation_failure_alot_for_current_gc();)
3797 }
3798 
3799 class G1EvacuateRegionsBaseTask : public AbstractGangTask {
3800 protected:
3801   G1CollectedHeap* _g1h;
3802   G1ParScanThreadStateSet* _per_thread_states;
<span class="line-modified">3803   RefToScanQueueSet* _task_queues;</span>
3804   TaskTerminator _terminator;
3805   uint _num_workers;
3806 
3807   void evacuate_live_objects(G1ParScanThreadState* pss,
3808                              uint worker_id,
3809                              G1GCPhaseTimes::GCParPhases objcopy_phase,
3810                              G1GCPhaseTimes::GCParPhases termination_phase) {
3811     G1GCPhaseTimes* p = _g1h-&gt;phase_times();
3812 
3813     Ticks start = Ticks::now();
3814     G1ParEvacuateFollowersClosure cl(_g1h, pss, _task_queues, &amp;_terminator, objcopy_phase);
3815     cl.do_void();
3816 
3817     assert(pss-&gt;queue_is_empty(), &quot;should be empty&quot;);
3818 
3819     Tickspan evac_time = (Ticks::now() - start);
3820     p-&gt;record_or_add_time_secs(objcopy_phase, worker_id, evac_time.seconds() - cl.term_time());
3821 
3822     if (termination_phase == G1GCPhaseTimes::Termination) {
3823       p-&gt;record_time_secs(termination_phase, worker_id, cl.term_time());
3824       p-&gt;record_thread_work_item(termination_phase, worker_id, cl.term_attempts());
3825     } else {
3826       p-&gt;record_or_add_time_secs(termination_phase, worker_id, cl.term_time());
3827       p-&gt;record_or_add_thread_work_item(termination_phase, worker_id, cl.term_attempts());
3828     }
3829     assert(pss-&gt;trim_ticks().seconds() == 0.0, &quot;Unexpected partial trimming during evacuation&quot;);
3830   }
3831 
3832   virtual void start_work(uint worker_id) { }
3833 
3834   virtual void end_work(uint worker_id) { }
3835 
3836   virtual void scan_roots(G1ParScanThreadState* pss, uint worker_id) = 0;
3837 
3838   virtual void evacuate_live_objects(G1ParScanThreadState* pss, uint worker_id) = 0;
3839 
3840 public:
<span class="line-modified">3841   G1EvacuateRegionsBaseTask(const char* name, G1ParScanThreadStateSet* per_thread_states, RefToScanQueueSet* task_queues, uint num_workers) :</span>



3842     AbstractGangTask(name),
3843     _g1h(G1CollectedHeap::heap()),
3844     _per_thread_states(per_thread_states),
3845     _task_queues(task_queues),
3846     _terminator(num_workers, _task_queues),
3847     _num_workers(num_workers)
3848   { }
3849 
3850   void work(uint worker_id) {
3851     start_work(worker_id);
3852 
3853     {
3854       ResourceMark rm;
3855       HandleMark   hm;
3856 
3857       G1ParScanThreadState* pss = _per_thread_states-&gt;state_for_worker(worker_id);
3858       pss-&gt;set_ref_discoverer(_g1h-&gt;ref_processor_stw());
3859 
3860       scan_roots(pss, worker_id);
3861       evacuate_live_objects(pss, worker_id);
</pre>
<hr />
<pre>
3872     _root_processor-&gt;evacuate_roots(pss, worker_id);
3873     _g1h-&gt;rem_set()-&gt;scan_heap_roots(pss, worker_id, G1GCPhaseTimes::ScanHR, G1GCPhaseTimes::ObjCopy);
3874     _g1h-&gt;rem_set()-&gt;scan_collection_set_regions(pss, worker_id, G1GCPhaseTimes::ScanHR, G1GCPhaseTimes::CodeRoots, G1GCPhaseTimes::ObjCopy);
3875   }
3876 
3877   void evacuate_live_objects(G1ParScanThreadState* pss, uint worker_id) {
3878     G1EvacuateRegionsBaseTask::evacuate_live_objects(pss, worker_id, G1GCPhaseTimes::ObjCopy, G1GCPhaseTimes::Termination);
3879   }
3880 
3881   void start_work(uint worker_id) {
3882     _g1h-&gt;phase_times()-&gt;record_time_secs(G1GCPhaseTimes::GCWorkerStart, worker_id, Ticks::now().seconds());
3883   }
3884 
3885   void end_work(uint worker_id) {
3886     _g1h-&gt;phase_times()-&gt;record_time_secs(G1GCPhaseTimes::GCWorkerEnd, worker_id, Ticks::now().seconds());
3887   }
3888 
3889 public:
3890   G1EvacuateRegionsTask(G1CollectedHeap* g1h,
3891                         G1ParScanThreadStateSet* per_thread_states,
<span class="line-modified">3892                         RefToScanQueueSet* task_queues,</span>
3893                         G1RootProcessor* root_processor,
3894                         uint num_workers) :
3895     G1EvacuateRegionsBaseTask(&quot;G1 Evacuate Regions&quot;, per_thread_states, task_queues, num_workers),
3896     _root_processor(root_processor)
3897   { }
3898 };
3899 
3900 void G1CollectedHeap::evacuate_initial_collection_set(G1ParScanThreadStateSet* per_thread_states) {
3901   G1GCPhaseTimes* p = phase_times();
3902 
3903   {
3904     Ticks start = Ticks::now();
3905     rem_set()-&gt;merge_heap_roots(true /* initial_evacuation */);
3906     p-&gt;record_merge_heap_roots_time((Ticks::now() - start).seconds() * 1000.0);
3907   }
3908 
3909   Tickspan task_time;
3910   const uint num_workers = workers()-&gt;active_workers();
3911 
3912   Ticks start_processing = Ticks::now();
</pre>
<hr />
<pre>
3920   }
3921   Tickspan total_processing = Ticks::now() - start_processing;
3922 
3923   p-&gt;record_initial_evac_time(task_time.seconds() * 1000.0);
3924   p-&gt;record_or_add_code_root_fixup_time((total_processing - task_time).seconds() * 1000.0);
3925 }
3926 
3927 class G1EvacuateOptionalRegionsTask : public G1EvacuateRegionsBaseTask {
3928 
3929   void scan_roots(G1ParScanThreadState* pss, uint worker_id) {
3930     _g1h-&gt;rem_set()-&gt;scan_heap_roots(pss, worker_id, G1GCPhaseTimes::OptScanHR, G1GCPhaseTimes::OptObjCopy);
3931     _g1h-&gt;rem_set()-&gt;scan_collection_set_regions(pss, worker_id, G1GCPhaseTimes::OptScanHR, G1GCPhaseTimes::OptCodeRoots, G1GCPhaseTimes::OptObjCopy);
3932   }
3933 
3934   void evacuate_live_objects(G1ParScanThreadState* pss, uint worker_id) {
3935     G1EvacuateRegionsBaseTask::evacuate_live_objects(pss, worker_id, G1GCPhaseTimes::OptObjCopy, G1GCPhaseTimes::OptTermination);
3936   }
3937 
3938 public:
3939   G1EvacuateOptionalRegionsTask(G1ParScanThreadStateSet* per_thread_states,
<span class="line-modified">3940                                 RefToScanQueueSet* queues,</span>
3941                                 uint num_workers) :
3942     G1EvacuateRegionsBaseTask(&quot;G1 Evacuate Optional Regions&quot;, per_thread_states, queues, num_workers) {
3943   }
3944 };
3945 
3946 void G1CollectedHeap::evacuate_next_optional_regions(G1ParScanThreadStateSet* per_thread_states) {
3947   class G1MarkScope : public MarkScope { };
3948 
3949   Tickspan task_time;
3950 
3951   Ticks start_processing = Ticks::now();
3952   {
3953     G1MarkScope code_mark_scope;
3954     G1EvacuateOptionalRegionsTask task(per_thread_states, _task_queues, workers()-&gt;active_workers());
3955     task_time = run_task(&amp;task);
3956     // See comment in evacuate_collection_set() for the reason of the scope.
3957   }
3958   Tickspan total_processing = Ticks::now() - start_processing;
3959 
3960   G1GCPhaseTimes* p = phase_times();
</pre>
</td>
<td>
<hr />
<pre>
  24 
  25 #include &quot;precompiled.hpp&quot;
  26 #include &quot;classfile/classLoaderDataGraph.hpp&quot;
  27 #include &quot;classfile/metadataOnStackMark.hpp&quot;
  28 #include &quot;classfile/stringTable.hpp&quot;
  29 #include &quot;code/codeCache.hpp&quot;
  30 #include &quot;code/icBuffer.hpp&quot;
  31 #include &quot;gc/g1/g1Allocator.inline.hpp&quot;
  32 #include &quot;gc/g1/g1Arguments.hpp&quot;
  33 #include &quot;gc/g1/g1BarrierSet.hpp&quot;
  34 #include &quot;gc/g1/g1CardTableEntryClosure.hpp&quot;
  35 #include &quot;gc/g1/g1CollectedHeap.inline.hpp&quot;
  36 #include &quot;gc/g1/g1CollectionSet.hpp&quot;
  37 #include &quot;gc/g1/g1CollectorState.hpp&quot;
  38 #include &quot;gc/g1/g1ConcurrentRefine.hpp&quot;
  39 #include &quot;gc/g1/g1ConcurrentRefineThread.hpp&quot;
  40 #include &quot;gc/g1/g1ConcurrentMarkThread.inline.hpp&quot;
  41 #include &quot;gc/g1/g1DirtyCardQueue.hpp&quot;
  42 #include &quot;gc/g1/g1EvacStats.inline.hpp&quot;
  43 #include &quot;gc/g1/g1FullCollector.hpp&quot;
<span class="line-added">  44 #include &quot;gc/g1/g1GCParPhaseTimesTracker.hpp&quot;</span>
  45 #include &quot;gc/g1/g1GCPhaseTimes.hpp&quot;
  46 #include &quot;gc/g1/g1HeapSizingPolicy.hpp&quot;
  47 #include &quot;gc/g1/g1HeapTransition.hpp&quot;
  48 #include &quot;gc/g1/g1HeapVerifier.hpp&quot;
  49 #include &quot;gc/g1/g1HotCardCache.hpp&quot;
  50 #include &quot;gc/g1/g1MemoryPool.hpp&quot;
  51 #include &quot;gc/g1/g1OopClosures.inline.hpp&quot;
  52 #include &quot;gc/g1/g1ParallelCleaning.hpp&quot;
  53 #include &quot;gc/g1/g1ParScanThreadState.inline.hpp&quot;
  54 #include &quot;gc/g1/g1Policy.hpp&quot;
  55 #include &quot;gc/g1/g1RedirtyCardsQueue.hpp&quot;
  56 #include &quot;gc/g1/g1RegionToSpaceMapper.hpp&quot;
  57 #include &quot;gc/g1/g1RemSet.hpp&quot;
  58 #include &quot;gc/g1/g1RootClosures.hpp&quot;
  59 #include &quot;gc/g1/g1RootProcessor.hpp&quot;
  60 #include &quot;gc/g1/g1SATBMarkQueueSet.hpp&quot;
  61 #include &quot;gc/g1/g1StringDedup.hpp&quot;
  62 #include &quot;gc/g1/g1ThreadLocalData.hpp&quot;
  63 #include &quot;gc/g1/g1Trace.hpp&quot;
  64 #include &quot;gc/g1/g1YCTypes.hpp&quot;
</pre>
<hr />
<pre>
1525   _is_alive_closure_stw(this),
1526   _is_subject_to_discovery_stw(this),
1527   _ref_processor_cm(NULL),
1528   _is_alive_closure_cm(this),
1529   _is_subject_to_discovery_cm(this),
1530   _region_attr() {
1531 
1532   _verifier = new G1HeapVerifier(this);
1533 
1534   _allocator = new G1Allocator(this);
1535 
1536   _heap_sizing_policy = G1HeapSizingPolicy::create(this, _policy-&gt;analytics());
1537 
1538   _humongous_object_threshold_in_words = humongous_threshold_for(HeapRegion::GrainWords);
1539 
1540   // Override the default _filler_array_max_size so that no humongous filler
1541   // objects are created.
1542   _filler_array_max_size = _humongous_object_threshold_in_words;
1543 
1544   uint n_queues = ParallelGCThreads;
<span class="line-modified">1545   _task_queues = new ScannerTasksQueueSet(n_queues);</span>
1546 
1547   _evacuation_failed_info_array = NEW_C_HEAP_ARRAY(EvacuationFailedInfo, n_queues, mtGC);
1548 
1549   for (uint i = 0; i &lt; n_queues; i++) {
<span class="line-modified">1550     ScannerTasksQueue* q = new ScannerTasksQueue();</span>
1551     q-&gt;initialize();
1552     _task_queues-&gt;register_queue(i, q);
1553     ::new (&amp;_evacuation_failed_info_array[i]) EvacuationFailedInfo();
1554   }
1555 
1556   // Initialize the G1EvacuationFailureALot counters and flags.
1557   NOT_PRODUCT(reset_evacuation_should_fail();)
1558   _gc_tracer_stw-&gt;initialize();
1559 
1560   guarantee(_task_queues != NULL, &quot;task_queues allocation failure.&quot;);
1561 }
1562 
1563 static size_t actual_reserved_page_size(ReservedSpace rs) {
1564   size_t page_size = os::vm_page_size();
1565   if (UseLargePages) {
1566     // There are two ways to manage large page memory.
1567     // 1. OS supports committing large page memory.
1568     // 2. OS doesn&#39;t support committing large page memory so ReservedSpace manages it.
1569     //    And ReservedSpace calls it &#39;special&#39;. If we failed to set &#39;special&#39;,
1570     //    we reserved memory without large page.
</pre>
<hr />
<pre>
3382 
3383   virtual void do_oop(narrowOop* p) { do_oop_work(p); }
3384   virtual void do_oop(      oop* p) { do_oop_work(p); }
3385 
3386   template &lt;class T&gt; void do_oop_work(T* p) {
3387     oop obj = RawAccess&lt;&gt;::oop_load(p);
3388 
3389     if (_g1h-&gt;is_in_cset_or_humongous(obj)) {
3390       // If the referent object has been forwarded (either copied
3391       // to a new location or to itself in the event of an
3392       // evacuation failure) then we need to update the reference
3393       // field and, if both reference and referent are in the G1
3394       // heap, update the RSet for the referent.
3395       //
3396       // If the referent has not been forwarded then we have to keep
3397       // it alive by policy. Therefore we have copy the referent.
3398       //
3399       // When the queue is drained (after each phase of reference processing)
3400       // the object and it&#39;s followers will be copied, the reference field set
3401       // to point to the new location, and the RSet updated.
<span class="line-modified">3402       _par_scan_state-&gt;push_on_queue(ScannerTask(p));</span>
3403     }
3404   }
3405 };
3406 
3407 // Serial drain queue closure. Called as the &#39;complete_gc&#39;
3408 // closure for each discovered list in some of the
3409 // reference processing phases.
3410 
3411 class G1STWDrainQueueClosure: public VoidClosure {
3412 protected:
3413   G1CollectedHeap* _g1h;
3414   G1ParScanThreadState* _par_scan_state;
3415 
3416   G1ParScanThreadState*   par_scan_state() { return _par_scan_state; }
3417 
3418 public:
3419   G1STWDrainQueueClosure(G1CollectedHeap* g1h, G1ParScanThreadState* pss) :
3420     _g1h(g1h),
3421     _par_scan_state(pss)
3422   { }
3423 
3424   void do_void() {
3425     G1ParScanThreadState* const pss = par_scan_state();
3426     pss-&gt;trim_queue();
3427   }
3428 };
3429 
3430 // Parallel Reference Processing closures
3431 
3432 // Implementation of AbstractRefProcTaskExecutor for parallel reference
3433 // processing during G1 evacuation pauses.
3434 
3435 class G1STWRefProcTaskExecutor: public AbstractRefProcTaskExecutor {
3436 private:
3437   G1CollectedHeap*          _g1h;
3438   G1ParScanThreadStateSet*  _pss;
<span class="line-modified">3439   ScannerTasksQueueSet*     _queues;</span>
3440   WorkGang*                 _workers;
3441 
3442 public:
3443   G1STWRefProcTaskExecutor(G1CollectedHeap* g1h,
3444                            G1ParScanThreadStateSet* per_thread_states,
3445                            WorkGang* workers,
<span class="line-modified">3446                            ScannerTasksQueueSet *task_queues) :</span>
3447     _g1h(g1h),
3448     _pss(per_thread_states),
3449     _queues(task_queues),
3450     _workers(workers)
3451   {
3452     g1h-&gt;ref_processor_stw()-&gt;set_active_mt_degree(workers-&gt;active_workers());
3453   }
3454 
3455   // Executes the given task using concurrent marking worker threads.
3456   virtual void execute(ProcessTask&amp; task, uint ergo_workers);
3457 };
3458 
3459 // Gang task for possibly parallel reference processing
3460 
3461 class G1STWRefProcTaskProxy: public AbstractGangTask {
3462   typedef AbstractRefProcTaskExecutor::ProcessTask ProcessTask;
3463   ProcessTask&amp;     _proc_task;
3464   G1CollectedHeap* _g1h;
3465   G1ParScanThreadStateSet* _pss;
<span class="line-modified">3466   ScannerTasksQueueSet* _task_queues;</span>
3467   TaskTerminator* _terminator;
3468 
3469 public:
3470   G1STWRefProcTaskProxy(ProcessTask&amp; proc_task,
3471                         G1CollectedHeap* g1h,
3472                         G1ParScanThreadStateSet* per_thread_states,
<span class="line-modified">3473                         ScannerTasksQueueSet *task_queues,</span>
3474                         TaskTerminator* terminator) :
3475     AbstractGangTask(&quot;Process reference objects in parallel&quot;),
3476     _proc_task(proc_task),
3477     _g1h(g1h),
3478     _pss(per_thread_states),
3479     _task_queues(task_queues),
3480     _terminator(terminator)
3481   {}
3482 
3483   virtual void work(uint worker_id) {
3484     // The reference processing task executed by a single worker.
3485     ResourceMark rm;
3486     HandleMark   hm;
3487 
3488     G1STWIsAliveClosure is_alive(_g1h);
3489 
3490     G1ParScanThreadState* pss = _pss-&gt;state_for_worker(worker_id);
3491     pss-&gt;set_ref_discoverer(NULL);
3492 
3493     // Keep alive closure.
</pre>
<hr />
<pre>
3784   // InitialMark needs claim bits to keep track of the marked-through CLDs.
3785   if (collector_state()-&gt;in_initial_mark_gc()) {
3786     concurrent_mark()-&gt;pre_initial_mark();
3787 
3788     double start_clear_claimed_marks = os::elapsedTime();
3789 
3790     ClassLoaderDataGraph::clear_claimed_marks();
3791 
3792     double recorded_clear_claimed_marks_time_ms = (os::elapsedTime() - start_clear_claimed_marks) * 1000.0;
3793     phase_times()-&gt;record_clear_claimed_marks_time_ms(recorded_clear_claimed_marks_time_ms);
3794   }
3795 
3796   // Should G1EvacuationFailureALot be in effect for this GC?
3797   NOT_PRODUCT(set_evacuation_failure_alot_for_current_gc();)
3798 }
3799 
3800 class G1EvacuateRegionsBaseTask : public AbstractGangTask {
3801 protected:
3802   G1CollectedHeap* _g1h;
3803   G1ParScanThreadStateSet* _per_thread_states;
<span class="line-modified">3804   ScannerTasksQueueSet* _task_queues;</span>
3805   TaskTerminator _terminator;
3806   uint _num_workers;
3807 
3808   void evacuate_live_objects(G1ParScanThreadState* pss,
3809                              uint worker_id,
3810                              G1GCPhaseTimes::GCParPhases objcopy_phase,
3811                              G1GCPhaseTimes::GCParPhases termination_phase) {
3812     G1GCPhaseTimes* p = _g1h-&gt;phase_times();
3813 
3814     Ticks start = Ticks::now();
3815     G1ParEvacuateFollowersClosure cl(_g1h, pss, _task_queues, &amp;_terminator, objcopy_phase);
3816     cl.do_void();
3817 
3818     assert(pss-&gt;queue_is_empty(), &quot;should be empty&quot;);
3819 
3820     Tickspan evac_time = (Ticks::now() - start);
3821     p-&gt;record_or_add_time_secs(objcopy_phase, worker_id, evac_time.seconds() - cl.term_time());
3822 
3823     if (termination_phase == G1GCPhaseTimes::Termination) {
3824       p-&gt;record_time_secs(termination_phase, worker_id, cl.term_time());
3825       p-&gt;record_thread_work_item(termination_phase, worker_id, cl.term_attempts());
3826     } else {
3827       p-&gt;record_or_add_time_secs(termination_phase, worker_id, cl.term_time());
3828       p-&gt;record_or_add_thread_work_item(termination_phase, worker_id, cl.term_attempts());
3829     }
3830     assert(pss-&gt;trim_ticks().seconds() == 0.0, &quot;Unexpected partial trimming during evacuation&quot;);
3831   }
3832 
3833   virtual void start_work(uint worker_id) { }
3834 
3835   virtual void end_work(uint worker_id) { }
3836 
3837   virtual void scan_roots(G1ParScanThreadState* pss, uint worker_id) = 0;
3838 
3839   virtual void evacuate_live_objects(G1ParScanThreadState* pss, uint worker_id) = 0;
3840 
3841 public:
<span class="line-modified">3842   G1EvacuateRegionsBaseTask(const char* name,</span>
<span class="line-added">3843                             G1ParScanThreadStateSet* per_thread_states,</span>
<span class="line-added">3844                             ScannerTasksQueueSet* task_queues,</span>
<span class="line-added">3845                             uint num_workers) :</span>
3846     AbstractGangTask(name),
3847     _g1h(G1CollectedHeap::heap()),
3848     _per_thread_states(per_thread_states),
3849     _task_queues(task_queues),
3850     _terminator(num_workers, _task_queues),
3851     _num_workers(num_workers)
3852   { }
3853 
3854   void work(uint worker_id) {
3855     start_work(worker_id);
3856 
3857     {
3858       ResourceMark rm;
3859       HandleMark   hm;
3860 
3861       G1ParScanThreadState* pss = _per_thread_states-&gt;state_for_worker(worker_id);
3862       pss-&gt;set_ref_discoverer(_g1h-&gt;ref_processor_stw());
3863 
3864       scan_roots(pss, worker_id);
3865       evacuate_live_objects(pss, worker_id);
</pre>
<hr />
<pre>
3876     _root_processor-&gt;evacuate_roots(pss, worker_id);
3877     _g1h-&gt;rem_set()-&gt;scan_heap_roots(pss, worker_id, G1GCPhaseTimes::ScanHR, G1GCPhaseTimes::ObjCopy);
3878     _g1h-&gt;rem_set()-&gt;scan_collection_set_regions(pss, worker_id, G1GCPhaseTimes::ScanHR, G1GCPhaseTimes::CodeRoots, G1GCPhaseTimes::ObjCopy);
3879   }
3880 
3881   void evacuate_live_objects(G1ParScanThreadState* pss, uint worker_id) {
3882     G1EvacuateRegionsBaseTask::evacuate_live_objects(pss, worker_id, G1GCPhaseTimes::ObjCopy, G1GCPhaseTimes::Termination);
3883   }
3884 
3885   void start_work(uint worker_id) {
3886     _g1h-&gt;phase_times()-&gt;record_time_secs(G1GCPhaseTimes::GCWorkerStart, worker_id, Ticks::now().seconds());
3887   }
3888 
3889   void end_work(uint worker_id) {
3890     _g1h-&gt;phase_times()-&gt;record_time_secs(G1GCPhaseTimes::GCWorkerEnd, worker_id, Ticks::now().seconds());
3891   }
3892 
3893 public:
3894   G1EvacuateRegionsTask(G1CollectedHeap* g1h,
3895                         G1ParScanThreadStateSet* per_thread_states,
<span class="line-modified">3896                         ScannerTasksQueueSet* task_queues,</span>
3897                         G1RootProcessor* root_processor,
3898                         uint num_workers) :
3899     G1EvacuateRegionsBaseTask(&quot;G1 Evacuate Regions&quot;, per_thread_states, task_queues, num_workers),
3900     _root_processor(root_processor)
3901   { }
3902 };
3903 
3904 void G1CollectedHeap::evacuate_initial_collection_set(G1ParScanThreadStateSet* per_thread_states) {
3905   G1GCPhaseTimes* p = phase_times();
3906 
3907   {
3908     Ticks start = Ticks::now();
3909     rem_set()-&gt;merge_heap_roots(true /* initial_evacuation */);
3910     p-&gt;record_merge_heap_roots_time((Ticks::now() - start).seconds() * 1000.0);
3911   }
3912 
3913   Tickspan task_time;
3914   const uint num_workers = workers()-&gt;active_workers();
3915 
3916   Ticks start_processing = Ticks::now();
</pre>
<hr />
<pre>
3924   }
3925   Tickspan total_processing = Ticks::now() - start_processing;
3926 
3927   p-&gt;record_initial_evac_time(task_time.seconds() * 1000.0);
3928   p-&gt;record_or_add_code_root_fixup_time((total_processing - task_time).seconds() * 1000.0);
3929 }
3930 
3931 class G1EvacuateOptionalRegionsTask : public G1EvacuateRegionsBaseTask {
3932 
3933   void scan_roots(G1ParScanThreadState* pss, uint worker_id) {
3934     _g1h-&gt;rem_set()-&gt;scan_heap_roots(pss, worker_id, G1GCPhaseTimes::OptScanHR, G1GCPhaseTimes::OptObjCopy);
3935     _g1h-&gt;rem_set()-&gt;scan_collection_set_regions(pss, worker_id, G1GCPhaseTimes::OptScanHR, G1GCPhaseTimes::OptCodeRoots, G1GCPhaseTimes::OptObjCopy);
3936   }
3937 
3938   void evacuate_live_objects(G1ParScanThreadState* pss, uint worker_id) {
3939     G1EvacuateRegionsBaseTask::evacuate_live_objects(pss, worker_id, G1GCPhaseTimes::OptObjCopy, G1GCPhaseTimes::OptTermination);
3940   }
3941 
3942 public:
3943   G1EvacuateOptionalRegionsTask(G1ParScanThreadStateSet* per_thread_states,
<span class="line-modified">3944                                 ScannerTasksQueueSet* queues,</span>
3945                                 uint num_workers) :
3946     G1EvacuateRegionsBaseTask(&quot;G1 Evacuate Optional Regions&quot;, per_thread_states, queues, num_workers) {
3947   }
3948 };
3949 
3950 void G1CollectedHeap::evacuate_next_optional_regions(G1ParScanThreadStateSet* per_thread_states) {
3951   class G1MarkScope : public MarkScope { };
3952 
3953   Tickspan task_time;
3954 
3955   Ticks start_processing = Ticks::now();
3956   {
3957     G1MarkScope code_mark_scope;
3958     G1EvacuateOptionalRegionsTask task(per_thread_states, _task_queues, workers()-&gt;active_workers());
3959     task_time = run_task(&amp;task);
3960     // See comment in evacuate_collection_set() for the reason of the scope.
3961   }
3962   Tickspan total_processing = Ticks::now() - start_processing;
3963 
3964   G1GCPhaseTimes* p = phase_times();
</pre>
</td>
</tr>
</table>
<center><a href="g1BarrierSet.inline.hpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../index.html" target="_top">index</a> <a href="g1CollectedHeap.hpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>