<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Frames src/hotspot/share/c1/c1_Runtime1.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
    <script type="text/javascript" src="../../../../navigation.js"></script>
  </head>
<body onkeypress="keypress(event);">
<a name="0"></a>
<hr />
<pre>   1 /*
   2  * Copyright (c) 1999, 2020, Oracle and/or its affiliates. All rights reserved.
   3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   4  *
   5  * This code is free software; you can redistribute it and/or modify it
   6  * under the terms of the GNU General Public License version 2 only, as
   7  * published by the Free Software Foundation.
   8  *
   9  * This code is distributed in the hope that it will be useful, but WITHOUT
  10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #include &quot;precompiled.hpp&quot;
  26 #include &quot;asm/codeBuffer.hpp&quot;
  27 #include &quot;c1/c1_CodeStubs.hpp&quot;
  28 #include &quot;c1/c1_Defs.hpp&quot;
  29 #include &quot;c1/c1_FrameMap.hpp&quot;
  30 #include &quot;c1/c1_LIRAssembler.hpp&quot;
  31 #include &quot;c1/c1_MacroAssembler.hpp&quot;
  32 #include &quot;c1/c1_Runtime1.hpp&quot;
  33 #include &quot;classfile/javaClasses.inline.hpp&quot;
  34 #include &quot;classfile/systemDictionary.hpp&quot;
  35 #include &quot;classfile/vmSymbols.hpp&quot;
  36 #include &quot;code/codeBlob.hpp&quot;
  37 #include &quot;code/compiledIC.hpp&quot;
  38 #include &quot;code/pcDesc.hpp&quot;
  39 #include &quot;code/scopeDesc.hpp&quot;
  40 #include &quot;code/vtableStubs.hpp&quot;
  41 #include &quot;compiler/compilationPolicy.hpp&quot;
  42 #include &quot;compiler/disassembler.hpp&quot;
  43 #include &quot;gc/shared/barrierSet.hpp&quot;
  44 #include &quot;gc/shared/c1/barrierSetC1.hpp&quot;
  45 #include &quot;gc/shared/collectedHeap.hpp&quot;
  46 #include &quot;interpreter/bytecode.hpp&quot;
  47 #include &quot;interpreter/interpreter.hpp&quot;
  48 #include &quot;jfr/support/jfrIntrinsics.hpp&quot;
  49 #include &quot;logging/log.hpp&quot;
  50 #include &quot;memory/allocation.inline.hpp&quot;
  51 #include &quot;memory/oopFactory.hpp&quot;
  52 #include &quot;memory/resourceArea.hpp&quot;
  53 #include &quot;memory/universe.hpp&quot;
  54 #include &quot;oops/access.inline.hpp&quot;
  55 #include &quot;oops/objArrayOop.inline.hpp&quot;
  56 #include &quot;oops/objArrayKlass.hpp&quot;
  57 #include &quot;oops/oop.inline.hpp&quot;
  58 #include &quot;oops/valueArrayKlass.hpp&quot;
  59 #include &quot;oops/valueArrayOop.inline.hpp&quot;
  60 #include &quot;runtime/atomic.hpp&quot;
  61 #include &quot;runtime/biasedLocking.hpp&quot;
  62 #include &quot;runtime/fieldDescriptor.inline.hpp&quot;
  63 #include &quot;runtime/frame.inline.hpp&quot;
  64 #include &quot;runtime/handles.inline.hpp&quot;
  65 #include &quot;runtime/interfaceSupport.inline.hpp&quot;
  66 #include &quot;runtime/javaCalls.hpp&quot;
  67 #include &quot;runtime/sharedRuntime.hpp&quot;
  68 #include &quot;runtime/threadCritical.hpp&quot;
  69 #include &quot;runtime/vframe.inline.hpp&quot;
  70 #include &quot;runtime/vframeArray.hpp&quot;
  71 #include &quot;runtime/vm_version.hpp&quot;
  72 #include &quot;utilities/copy.hpp&quot;
  73 #include &quot;utilities/events.hpp&quot;
  74 
  75 
  76 // Implementation of StubAssembler
  77 
  78 StubAssembler::StubAssembler(CodeBuffer* code, const char * name, int stub_id) : C1_MacroAssembler(code) {
  79   _name = name;
  80   _must_gc_arguments = false;
  81   _frame_size = no_frame_size;
  82   _num_rt_args = 0;
  83   _stub_id = stub_id;
  84 }
  85 
  86 
  87 void StubAssembler::set_info(const char* name, bool must_gc_arguments) {
  88   _name = name;
  89   _must_gc_arguments = must_gc_arguments;
  90 }
  91 
  92 
  93 void StubAssembler::set_frame_size(int size) {
  94   if (_frame_size == no_frame_size) {
  95     _frame_size = size;
  96   }
  97   assert(_frame_size == size, &quot;can&#39;t change the frame size&quot;);
  98 }
  99 
 100 
 101 void StubAssembler::set_num_rt_args(int args) {
 102   if (_num_rt_args == 0) {
 103     _num_rt_args = args;
 104   }
 105   assert(_num_rt_args == args, &quot;can&#39;t change the number of args&quot;);
 106 }
 107 
 108 // Implementation of Runtime1
 109 
 110 CodeBlob* Runtime1::_blobs[Runtime1::number_of_ids];
 111 const char *Runtime1::_blob_names[] = {
 112   RUNTIME1_STUBS(STUB_NAME, LAST_STUB_NAME)
 113 };
 114 
 115 #ifndef PRODUCT
 116 // statistics
 117 int Runtime1::_generic_arraycopy_cnt = 0;
 118 int Runtime1::_generic_arraycopystub_cnt = 0;
 119 int Runtime1::_arraycopy_slowcase_cnt = 0;
 120 int Runtime1::_arraycopy_checkcast_cnt = 0;
 121 int Runtime1::_arraycopy_checkcast_attempt_cnt = 0;
 122 int Runtime1::_new_type_array_slowcase_cnt = 0;
 123 int Runtime1::_new_object_array_slowcase_cnt = 0;
 124 int Runtime1::_new_value_array_slowcase_cnt = 0;
 125 int Runtime1::_new_instance_slowcase_cnt = 0;
 126 int Runtime1::_new_multi_array_slowcase_cnt = 0;
 127 int Runtime1::_load_flattened_array_slowcase_cnt = 0;
 128 int Runtime1::_store_flattened_array_slowcase_cnt = 0;
 129 int Runtime1::_substitutability_check_slowcase_cnt = 0;
 130 int Runtime1::_buffer_value_args_slowcase_cnt = 0;
 131 int Runtime1::_buffer_value_args_no_receiver_slowcase_cnt = 0;
 132 int Runtime1::_monitorenter_slowcase_cnt = 0;
 133 int Runtime1::_monitorexit_slowcase_cnt = 0;
 134 int Runtime1::_patch_code_slowcase_cnt = 0;
 135 int Runtime1::_throw_range_check_exception_count = 0;
 136 int Runtime1::_throw_index_exception_count = 0;
 137 int Runtime1::_throw_div0_exception_count = 0;
 138 int Runtime1::_throw_null_pointer_exception_count = 0;
 139 int Runtime1::_throw_class_cast_exception_count = 0;
 140 int Runtime1::_throw_incompatible_class_change_error_count = 0;
 141 int Runtime1::_throw_illegal_monitor_state_exception_count = 0;
 142 int Runtime1::_throw_array_store_exception_count = 0;
 143 int Runtime1::_throw_count = 0;
 144 
 145 static int _byte_arraycopy_stub_cnt = 0;
 146 static int _short_arraycopy_stub_cnt = 0;
 147 static int _int_arraycopy_stub_cnt = 0;
 148 static int _long_arraycopy_stub_cnt = 0;
 149 static int _oop_arraycopy_stub_cnt = 0;
 150 
 151 address Runtime1::arraycopy_count_address(BasicType type) {
 152   switch (type) {
 153   case T_BOOLEAN:
 154   case T_BYTE:   return (address)&amp;_byte_arraycopy_stub_cnt;
 155   case T_CHAR:
 156   case T_SHORT:  return (address)&amp;_short_arraycopy_stub_cnt;
 157   case T_FLOAT:
 158   case T_INT:    return (address)&amp;_int_arraycopy_stub_cnt;
 159   case T_DOUBLE:
 160   case T_LONG:   return (address)&amp;_long_arraycopy_stub_cnt;
 161   case T_ARRAY:
 162   case T_OBJECT: return (address)&amp;_oop_arraycopy_stub_cnt;
 163   default:
 164     ShouldNotReachHere();
 165     return NULL;
 166   }
 167 }
 168 
 169 
 170 #endif
 171 
 172 // Simple helper to see if the caller of a runtime stub which
 173 // entered the VM has been deoptimized
 174 
 175 static bool caller_is_deopted() {
 176   JavaThread* thread = JavaThread::current();
 177   RegisterMap reg_map(thread, false);
 178   frame runtime_frame = thread-&gt;last_frame();
 179   frame caller_frame = runtime_frame.sender(&amp;reg_map);
 180   assert(caller_frame.is_compiled_frame(), &quot;must be compiled&quot;);
 181   return caller_frame.is_deoptimized_frame();
 182 }
 183 
 184 // Stress deoptimization
 185 static void deopt_caller() {
 186   if ( !caller_is_deopted()) {
 187     JavaThread* thread = JavaThread::current();
 188     RegisterMap reg_map(thread, false);
 189     frame runtime_frame = thread-&gt;last_frame();
 190     frame caller_frame = runtime_frame.sender(&amp;reg_map);
 191     Deoptimization::deoptimize_frame(thread, caller_frame.id());
 192     assert(caller_is_deopted(), &quot;Must be deoptimized&quot;);
 193   }
 194 }
 195 
 196 class StubIDStubAssemblerCodeGenClosure: public StubAssemblerCodeGenClosure {
 197  private:
 198   Runtime1::StubID _id;
 199  public:
 200   StubIDStubAssemblerCodeGenClosure(Runtime1::StubID id) : _id(id) {}
 201   virtual OopMapSet* generate_code(StubAssembler* sasm) {
 202     return Runtime1::generate_code_for(_id, sasm);
 203   }
 204 };
 205 
 206 CodeBlob* Runtime1::generate_blob(BufferBlob* buffer_blob, int stub_id, const char* name, bool expect_oop_map, StubAssemblerCodeGenClosure* cl) {
 207   ResourceMark rm;
 208   // create code buffer for code storage
 209   CodeBuffer code(buffer_blob);
 210 
 211   OopMapSet* oop_maps;
 212   int frame_size;
 213   bool must_gc_arguments;
 214 
 215   Compilation::setup_code_buffer(&amp;code, 0);
 216 
 217   // create assembler for code generation
 218   StubAssembler* sasm = new StubAssembler(&amp;code, name, stub_id);
 219   // generate code for runtime stub
 220   oop_maps = cl-&gt;generate_code(sasm);
 221   assert(oop_maps == NULL || sasm-&gt;frame_size() != no_frame_size,
 222          &quot;if stub has an oop map it must have a valid frame size&quot;);
 223   assert(!expect_oop_map || oop_maps != NULL, &quot;must have an oopmap&quot;);
 224 
 225   // align so printing shows nop&#39;s instead of random code at the end (SimpleStubs are aligned)
 226   sasm-&gt;align(BytesPerWord);
 227   // make sure all code is in code buffer
 228   sasm-&gt;flush();
 229 
 230   frame_size = sasm-&gt;frame_size();
 231   must_gc_arguments = sasm-&gt;must_gc_arguments();
 232   // create blob - distinguish a few special cases
 233   CodeBlob* blob = RuntimeStub::new_runtime_stub(name,
 234                                                  &amp;code,
 235                                                  CodeOffsets::frame_never_safe,
 236                                                  frame_size,
 237                                                  oop_maps,
 238                                                  must_gc_arguments);
 239   assert(blob != NULL, &quot;blob must exist&quot;);
 240   return blob;
 241 }
 242 
 243 void Runtime1::generate_blob_for(BufferBlob* buffer_blob, StubID id) {
 244   assert(0 &lt;= id &amp;&amp; id &lt; number_of_ids, &quot;illegal stub id&quot;);
 245   bool expect_oop_map = true;
 246 #ifdef ASSERT
 247   // Make sure that stubs that need oopmaps have them
 248   switch (id) {
 249     // These stubs don&#39;t need to have an oopmap
 250   case dtrace_object_alloc_id:
 251   case slow_subtype_check_id:
 252   case fpu2long_stub_id:
 253   case unwind_exception_id:
 254   case counter_overflow_id:
 255 #if defined(PPC32)
 256   case handle_exception_nofpu_id:  // Unused on sparc
 257 #endif
 258     expect_oop_map = false;
 259     break;
 260   default:
 261     break;
 262   }
 263 #endif
 264   StubIDStubAssemblerCodeGenClosure cl(id);
 265   CodeBlob* blob = generate_blob(buffer_blob, id, name_for(id), expect_oop_map, &amp;cl);
 266   // install blob
 267   _blobs[id] = blob;
 268 }
 269 
 270 void Runtime1::initialize(BufferBlob* blob) {
 271   // platform-dependent initialization
 272   initialize_pd();
 273   // generate stubs
 274   for (int id = 0; id &lt; number_of_ids; id++) generate_blob_for(blob, (StubID)id);
 275   // printing
 276 #ifndef PRODUCT
 277   if (PrintSimpleStubs) {
 278     ResourceMark rm;
 279     for (int id = 0; id &lt; number_of_ids; id++) {
 280       _blobs[id]-&gt;print();
 281       if (_blobs[id]-&gt;oop_maps() != NULL) {
 282         _blobs[id]-&gt;oop_maps()-&gt;print();
 283       }
 284     }
 285   }
 286 #endif
 287   BarrierSetC1* bs = BarrierSet::barrier_set()-&gt;barrier_set_c1();
 288   bs-&gt;generate_c1_runtime_stubs(blob);
 289 }
 290 
 291 CodeBlob* Runtime1::blob_for(StubID id) {
 292   assert(0 &lt;= id &amp;&amp; id &lt; number_of_ids, &quot;illegal stub id&quot;);
 293   return _blobs[id];
 294 }
 295 
 296 
 297 const char* Runtime1::name_for(StubID id) {
 298   assert(0 &lt;= id &amp;&amp; id &lt; number_of_ids, &quot;illegal stub id&quot;);
 299   return _blob_names[id];
 300 }
 301 
 302 const char* Runtime1::name_for_address(address entry) {
 303   for (int id = 0; id &lt; number_of_ids; id++) {
 304     if (entry == entry_for((StubID)id)) return name_for((StubID)id);
 305   }
 306 
 307   BarrierSetC1* bsc1 = BarrierSet::barrier_set()-&gt;barrier_set_c1();
 308   const char* name = bsc1-&gt;rtcall_name_for_address(entry);
 309   if (name != NULL) {
 310     return name;
 311   }
 312 
 313 #define FUNCTION_CASE(a, f) \
 314   if ((intptr_t)a == CAST_FROM_FN_PTR(intptr_t, f))  return #f
 315 
 316   FUNCTION_CASE(entry, os::javaTimeMillis);
 317   FUNCTION_CASE(entry, os::javaTimeNanos);
 318   FUNCTION_CASE(entry, SharedRuntime::OSR_migration_end);
 319   FUNCTION_CASE(entry, SharedRuntime::d2f);
 320   FUNCTION_CASE(entry, SharedRuntime::d2i);
 321   FUNCTION_CASE(entry, SharedRuntime::d2l);
 322   FUNCTION_CASE(entry, SharedRuntime::dcos);
 323   FUNCTION_CASE(entry, SharedRuntime::dexp);
 324   FUNCTION_CASE(entry, SharedRuntime::dlog);
 325   FUNCTION_CASE(entry, SharedRuntime::dlog10);
 326   FUNCTION_CASE(entry, SharedRuntime::dpow);
 327   FUNCTION_CASE(entry, SharedRuntime::drem);
 328   FUNCTION_CASE(entry, SharedRuntime::dsin);
 329   FUNCTION_CASE(entry, SharedRuntime::dtan);
 330   FUNCTION_CASE(entry, SharedRuntime::f2i);
 331   FUNCTION_CASE(entry, SharedRuntime::f2l);
 332   FUNCTION_CASE(entry, SharedRuntime::frem);
 333   FUNCTION_CASE(entry, SharedRuntime::l2d);
 334   FUNCTION_CASE(entry, SharedRuntime::l2f);
 335   FUNCTION_CASE(entry, SharedRuntime::ldiv);
 336   FUNCTION_CASE(entry, SharedRuntime::lmul);
 337   FUNCTION_CASE(entry, SharedRuntime::lrem);
 338   FUNCTION_CASE(entry, SharedRuntime::lrem);
 339   FUNCTION_CASE(entry, SharedRuntime::dtrace_method_entry);
 340   FUNCTION_CASE(entry, SharedRuntime::dtrace_method_exit);
 341   FUNCTION_CASE(entry, is_instance_of);
 342   FUNCTION_CASE(entry, trace_block_entry);
 343 #ifdef JFR_HAVE_INTRINSICS
 344   FUNCTION_CASE(entry, JFR_TIME_FUNCTION);
 345 #endif
 346   FUNCTION_CASE(entry, StubRoutines::updateBytesCRC32());
 347   FUNCTION_CASE(entry, StubRoutines::updateBytesCRC32C());
 348   FUNCTION_CASE(entry, StubRoutines::vectorizedMismatch());
 349   FUNCTION_CASE(entry, StubRoutines::dexp());
 350   FUNCTION_CASE(entry, StubRoutines::dlog());
 351   FUNCTION_CASE(entry, StubRoutines::dlog10());
 352   FUNCTION_CASE(entry, StubRoutines::dpow());
 353   FUNCTION_CASE(entry, StubRoutines::dsin());
 354   FUNCTION_CASE(entry, StubRoutines::dcos());
 355   FUNCTION_CASE(entry, StubRoutines::dtan());
 356 
 357 #undef FUNCTION_CASE
 358 
 359   // Soft float adds more runtime names.
 360   return pd_name_for_address(entry);
 361 }
 362 
 363 
 364 JRT_ENTRY(void, Runtime1::new_instance(JavaThread* thread, Klass* klass))
 365   NOT_PRODUCT(_new_instance_slowcase_cnt++;)
 366 
 367   assert(klass-&gt;is_klass(), &quot;not a class&quot;);
 368   Handle holder(THREAD, klass-&gt;klass_holder()); // keep the klass alive
 369   InstanceKlass* h = InstanceKlass::cast(klass);
 370   h-&gt;check_valid_for_instantiation(true, CHECK);
 371   // make sure klass is initialized
 372   h-&gt;initialize(CHECK);
 373   // allocate instance and return via TLS
 374   oop obj = h-&gt;allocate_instance(CHECK);
 375   thread-&gt;set_vm_result(obj);
 376 JRT_END
 377 
 378 
 379 JRT_ENTRY(void, Runtime1::new_type_array(JavaThread* thread, Klass* klass, jint length))
 380   NOT_PRODUCT(_new_type_array_slowcase_cnt++;)
 381   // Note: no handle for klass needed since they are not used
 382   //       anymore after new_typeArray() and no GC can happen before.
 383   //       (This may have to change if this code changes!)
 384   assert(klass-&gt;is_klass(), &quot;not a class&quot;);
 385   BasicType elt_type = TypeArrayKlass::cast(klass)-&gt;element_type();
 386   oop obj = oopFactory::new_typeArray(elt_type, length, CHECK);
 387   thread-&gt;set_vm_result(obj);
 388   // This is pretty rare but this runtime patch is stressful to deoptimization
 389   // if we deoptimize here so force a deopt to stress the path.
 390   if (DeoptimizeALot) {
 391     deopt_caller();
 392   }
 393 
 394 JRT_END
 395 
 396 
 397 JRT_ENTRY(void, Runtime1::new_object_array(JavaThread* thread, Klass* array_klass, jint length))
 398   NOT_PRODUCT(_new_object_array_slowcase_cnt++;)
 399 
 400   // Note: no handle for klass needed since they are not used
 401   //       anymore after new_objArray() and no GC can happen before.
 402   //       (This may have to change if this code changes!)
 403   assert(array_klass-&gt;is_klass(), &quot;not a class&quot;);
 404   Handle holder(THREAD, array_klass-&gt;klass_holder()); // keep the klass alive
 405   Klass* elem_klass = ArrayKlass::cast(array_klass)-&gt;element_klass();
 406   objArrayOop obj = oopFactory::new_objArray(elem_klass, length, CHECK);
 407   thread-&gt;set_vm_result(obj);
 408   // This is pretty rare but this runtime patch is stressful to deoptimization
 409   // if we deoptimize here so force a deopt to stress the path.
 410   if (DeoptimizeALot) {
 411     deopt_caller();
 412   }
 413 JRT_END
 414 
 415 
 416 JRT_ENTRY(void, Runtime1::new_value_array(JavaThread* thread, Klass* array_klass, jint length))
 417   NOT_PRODUCT(_new_value_array_slowcase_cnt++;)
 418 
 419   // Note: no handle for klass needed since they are not used
 420   //       anymore after new_objArray() and no GC can happen before.
 421   //       (This may have to change if this code changes!)
 422   assert(array_klass-&gt;is_klass(), &quot;not a class&quot;);
 423   Handle holder(THREAD, array_klass-&gt;klass_holder()); // keep the klass alive
 424   Klass* elem_klass = ArrayKlass::cast(array_klass)-&gt;element_klass();
<a name="1" id="anc1"></a><span class="line-modified"> 425   assert(elem_klass-&gt;is_value(), &quot;must be&quot;);</span>
 426   // Logically creates elements, ensure klass init
 427   elem_klass-&gt;initialize(CHECK);
 428   arrayOop obj = oopFactory::new_valueArray(elem_klass, length, CHECK);
 429   thread-&gt;set_vm_result(obj);
 430   // This is pretty rare but this runtime patch is stressful to deoptimization
 431   // if we deoptimize here so force a deopt to stress the path.
 432   if (DeoptimizeALot) {
 433     deopt_caller();
 434   }
 435 JRT_END
 436 
 437 
 438 JRT_ENTRY(void, Runtime1::new_multi_array(JavaThread* thread, Klass* klass, int rank, jint* dims))
 439   NOT_PRODUCT(_new_multi_array_slowcase_cnt++;)
 440 
 441   assert(klass-&gt;is_klass(), &quot;not a class&quot;);
 442   assert(rank &gt;= 1, &quot;rank must be nonzero&quot;);
 443   Handle holder(THREAD, klass-&gt;klass_holder()); // keep the klass alive
 444   oop obj = ArrayKlass::cast(klass)-&gt;multi_allocate(rank, dims, CHECK);
 445   thread-&gt;set_vm_result(obj);
 446 JRT_END
 447 
 448 
 449 static void profile_flat_array(JavaThread* thread) {
 450   ResourceMark rm(thread);
 451   vframeStream vfst(thread, true);
 452   assert(!vfst.at_end(), &quot;Java frame must exist&quot;);
 453   int bci = vfst.bci();
 454   Method* method = vfst.method();
 455   MethodData* md = method-&gt;method_data();
 456   if (md != NULL) {
 457     ProfileData* data = md-&gt;bci_to_data(bci);
 458     assert(data != NULL &amp;&amp; data-&gt;is_ArrayLoadStoreData(), &quot;incorrect profiling entry&quot;);
 459     ArrayLoadStoreData* load_store = (ArrayLoadStoreData*)data;
 460     load_store-&gt;set_flat_array();
 461   }
 462 }
 463 
 464 JRT_ENTRY(void, Runtime1::load_flattened_array(JavaThread* thread, valueArrayOopDesc* array, int index))
 465   assert(array-&gt;klass()-&gt;is_valueArray_klass(), &quot;should not be called&quot;);
 466   profile_flat_array(thread);
 467 
 468   NOT_PRODUCT(_load_flattened_array_slowcase_cnt++;)
 469   assert(array-&gt;length() &gt; 0 &amp;&amp; index &lt; array-&gt;length(), &quot;already checked&quot;);
 470   valueArrayHandle vah(thread, array);
 471   oop obj = valueArrayOopDesc::value_alloc_copy_from_index(vah, index, CHECK);
 472   thread-&gt;set_vm_result(obj);
 473 JRT_END
 474 
 475 
 476 JRT_ENTRY(void, Runtime1::store_flattened_array(JavaThread* thread, valueArrayOopDesc* array, int index, oopDesc* value))
 477   if (array-&gt;klass()-&gt;is_valueArray_klass()) {
 478     profile_flat_array(thread);
 479   }
 480 
 481   NOT_PRODUCT(_store_flattened_array_slowcase_cnt++;)
 482   if (value == NULL) {
 483     assert(array-&gt;klass()-&gt;is_valueArray_klass() || array-&gt;klass()-&gt;is_null_free_array_klass(), &quot;should not be called&quot;);
 484     SharedRuntime::throw_and_post_jvmti_exception(thread, vmSymbols::java_lang_NullPointerException());
 485   } else {
 486     assert(array-&gt;klass()-&gt;is_valueArray_klass(), &quot;should not be called&quot;);
 487     array-&gt;value_copy_to_index(value, index);
 488   }
 489 JRT_END
 490 
 491 
 492 JRT_ENTRY(int, Runtime1::substitutability_check(JavaThread* thread, oopDesc* left, oopDesc* right))
 493   NOT_PRODUCT(_substitutability_check_slowcase_cnt++;)
 494   JavaCallArguments args;
 495   args.push_oop(Handle(THREAD, left));
 496   args.push_oop(Handle(THREAD, right));
 497   JavaValue result(T_BOOLEAN);
 498   JavaCalls::call_static(&amp;result,
 499                          SystemDictionary::ValueBootstrapMethods_klass(),
 500                          vmSymbols::isSubstitutable_name(),
 501                          vmSymbols::object_object_boolean_signature(),
 502                          &amp;args, CHECK_0);
 503   return result.get_jboolean() ? 1 : 0;
 504 JRT_END
 505 
 506 
 507 extern &quot;C&quot; void ps();
 508 
 509 void Runtime1::buffer_value_args_impl(JavaThread* thread, Method* m, bool allocate_receiver) {
 510   Thread* THREAD = thread;
 511   methodHandle method(thread, m); // We are inside the verified_entry or verified_value_ro_entry of this method.
 512   oop obj = SharedRuntime::allocate_value_types_impl(thread, method, allocate_receiver, CHECK);
 513   thread-&gt;set_vm_result(obj);
 514 }
 515 
 516 JRT_ENTRY(void, Runtime1::buffer_value_args(JavaThread* thread, Method* method))
 517   NOT_PRODUCT(_buffer_value_args_slowcase_cnt++;)
 518   buffer_value_args_impl(thread, method, true);
 519 JRT_END
 520 
 521 JRT_ENTRY(void, Runtime1::buffer_value_args_no_receiver(JavaThread* thread, Method* method))
 522   NOT_PRODUCT(_buffer_value_args_no_receiver_slowcase_cnt++;)
 523   buffer_value_args_impl(thread, method, false);
 524 JRT_END
 525 
 526 JRT_ENTRY(void, Runtime1::unimplemented_entry(JavaThread* thread, StubID id))
 527   tty-&gt;print_cr(&quot;Runtime1::entry_for(%d) returned unimplemented entry point&quot;, id);
 528 JRT_END
 529 
 530 
 531 JRT_ENTRY(void, Runtime1::throw_array_store_exception(JavaThread* thread, oopDesc* obj))
 532   ResourceMark rm(thread);
 533   const char* klass_name = obj-&gt;klass()-&gt;external_name();
 534   SharedRuntime::throw_and_post_jvmti_exception(thread, vmSymbols::java_lang_ArrayStoreException(), klass_name);
 535 JRT_END
 536 
 537 
 538 // counter_overflow() is called from within C1-compiled methods. The enclosing method is the method
 539 // associated with the top activation record. The inlinee (that is possibly included in the enclosing
 540 // method) method oop is passed as an argument. In order to do that it is embedded in the code as
 541 // a constant.
 542 static nmethod* counter_overflow_helper(JavaThread* THREAD, int branch_bci, Method* m) {
 543   nmethod* osr_nm = NULL;
 544   methodHandle method(THREAD, m);
 545 
 546   RegisterMap map(THREAD, false);
 547   frame fr =  THREAD-&gt;last_frame().sender(&amp;map);
 548   nmethod* nm = (nmethod*) fr.cb();
 549   assert(nm!= NULL &amp;&amp; nm-&gt;is_nmethod(), &quot;Sanity check&quot;);
 550   methodHandle enclosing_method(THREAD, nm-&gt;method());
 551 
 552   CompLevel level = (CompLevel)nm-&gt;comp_level();
 553   int bci = InvocationEntryBci;
 554   if (branch_bci != InvocationEntryBci) {
 555     // Compute destination bci
 556     address pc = method()-&gt;code_base() + branch_bci;
 557     Bytecodes::Code branch = Bytecodes::code_at(method(), pc);
 558     int offset = 0;
 559     switch (branch) {
 560       case Bytecodes::_if_icmplt: case Bytecodes::_iflt:
 561       case Bytecodes::_if_icmpgt: case Bytecodes::_ifgt:
 562       case Bytecodes::_if_icmple: case Bytecodes::_ifle:
 563       case Bytecodes::_if_icmpge: case Bytecodes::_ifge:
 564       case Bytecodes::_if_icmpeq: case Bytecodes::_if_acmpeq: case Bytecodes::_ifeq:
 565       case Bytecodes::_if_icmpne: case Bytecodes::_if_acmpne: case Bytecodes::_ifne:
 566       case Bytecodes::_ifnull: case Bytecodes::_ifnonnull: case Bytecodes::_goto:
 567         offset = (int16_t)Bytes::get_Java_u2(pc + 1);
 568         break;
 569       case Bytecodes::_goto_w:
 570         offset = Bytes::get_Java_u4(pc + 1);
 571         break;
 572       default: ;
 573     }
 574     bci = branch_bci + offset;
 575   }
 576   assert(!HAS_PENDING_EXCEPTION, &quot;Should not have any exceptions pending&quot;);
 577   osr_nm = CompilationPolicy::policy()-&gt;event(enclosing_method, method, branch_bci, bci, level, nm, THREAD);
 578   assert(!HAS_PENDING_EXCEPTION, &quot;Event handler should not throw any exceptions&quot;);
 579   return osr_nm;
 580 }
 581 
 582 JRT_BLOCK_ENTRY(address, Runtime1::counter_overflow(JavaThread* thread, int bci, Method* method))
 583   nmethod* osr_nm;
 584   JRT_BLOCK
 585     osr_nm = counter_overflow_helper(thread, bci, method);
 586     if (osr_nm != NULL) {
 587       RegisterMap map(thread, false);
 588       frame fr =  thread-&gt;last_frame().sender(&amp;map);
 589       Deoptimization::deoptimize_frame(thread, fr.id());
 590     }
 591   JRT_BLOCK_END
 592   return NULL;
 593 JRT_END
 594 
 595 extern void vm_exit(int code);
 596 
 597 // Enter this method from compiled code handler below. This is where we transition
 598 // to VM mode. This is done as a helper routine so that the method called directly
 599 // from compiled code does not have to transition to VM. This allows the entry
 600 // method to see if the nmethod that we have just looked up a handler for has
 601 // been deoptimized while we were in the vm. This simplifies the assembly code
 602 // cpu directories.
 603 //
 604 // We are entering here from exception stub (via the entry method below)
 605 // If there is a compiled exception handler in this method, we will continue there;
 606 // otherwise we will unwind the stack and continue at the caller of top frame method
 607 // Note: we enter in Java using a special JRT wrapper. This wrapper allows us to
 608 // control the area where we can allow a safepoint. After we exit the safepoint area we can
 609 // check to see if the handler we are going to return is now in a nmethod that has
 610 // been deoptimized. If that is the case we return the deopt blob
 611 // unpack_with_exception entry instead. This makes life for the exception blob easier
 612 // because making that same check and diverting is painful from assembly language.
 613 JRT_ENTRY_NO_ASYNC(static address, exception_handler_for_pc_helper(JavaThread* thread, oopDesc* ex, address pc, nmethod*&amp; nm))
 614   // Reset method handle flag.
 615   thread-&gt;set_is_method_handle_return(false);
 616 
 617   Handle exception(thread, ex);
 618   nm = CodeCache::find_nmethod(pc);
 619   assert(nm != NULL, &quot;this is not an nmethod&quot;);
 620   // Adjust the pc as needed/
 621   if (nm-&gt;is_deopt_pc(pc)) {
 622     RegisterMap map(thread, false);
 623     frame exception_frame = thread-&gt;last_frame().sender(&amp;map);
 624     // if the frame isn&#39;t deopted then pc must not correspond to the caller of last_frame
 625     assert(exception_frame.is_deoptimized_frame(), &quot;must be deopted&quot;);
 626     pc = exception_frame.pc();
 627   }
 628 #ifdef ASSERT
 629   assert(exception.not_null(), &quot;NULL exceptions should be handled by throw_exception&quot;);
 630   // Check that exception is a subclass of Throwable, otherwise we have a VerifyError
 631   if (!(exception-&gt;is_a(SystemDictionary::Throwable_klass()))) {
 632     if (ExitVMOnVerifyError) vm_exit(-1);
 633     ShouldNotReachHere();
 634   }
 635 #endif
 636 
 637   // Check the stack guard pages and reenable them if necessary and there is
 638   // enough space on the stack to do so.  Use fast exceptions only if the guard
 639   // pages are enabled.
 640   bool guard_pages_enabled = thread-&gt;stack_guards_enabled();
 641   if (!guard_pages_enabled) guard_pages_enabled = thread-&gt;reguard_stack();
 642 
 643   if (JvmtiExport::can_post_on_exceptions()) {
 644     // To ensure correct notification of exception catches and throws
 645     // we have to deoptimize here.  If we attempted to notify the
 646     // catches and throws during this exception lookup it&#39;s possible
 647     // we could deoptimize on the way out of the VM and end back in
 648     // the interpreter at the throw site.  This would result in double
 649     // notifications since the interpreter would also notify about
 650     // these same catches and throws as it unwound the frame.
 651 
 652     RegisterMap reg_map(thread);
 653     frame stub_frame = thread-&gt;last_frame();
 654     frame caller_frame = stub_frame.sender(&amp;reg_map);
 655 
 656     // We don&#39;t really want to deoptimize the nmethod itself since we
 657     // can actually continue in the exception handler ourselves but I
 658     // don&#39;t see an easy way to have the desired effect.
 659     Deoptimization::deoptimize_frame(thread, caller_frame.id());
 660     assert(caller_is_deopted(), &quot;Must be deoptimized&quot;);
 661 
 662     return SharedRuntime::deopt_blob()-&gt;unpack_with_exception_in_tls();
 663   }
 664 
 665   // ExceptionCache is used only for exceptions at call sites and not for implicit exceptions
 666   if (guard_pages_enabled) {
 667     address fast_continuation = nm-&gt;handler_for_exception_and_pc(exception, pc);
 668     if (fast_continuation != NULL) {
 669       // Set flag if return address is a method handle call site.
 670       thread-&gt;set_is_method_handle_return(nm-&gt;is_method_handle_return(pc));
 671       return fast_continuation;
 672     }
 673   }
 674 
 675   // If the stack guard pages are enabled, check whether there is a handler in
 676   // the current method.  Otherwise (guard pages disabled), force an unwind and
 677   // skip the exception cache update (i.e., just leave continuation==NULL).
 678   address continuation = NULL;
 679   if (guard_pages_enabled) {
 680 
 681     // New exception handling mechanism can support inlined methods
 682     // with exception handlers since the mappings are from PC to PC
 683 
 684     // debugging support
 685     // tracing
 686     if (log_is_enabled(Info, exceptions)) {
 687       ResourceMark rm;
 688       stringStream tempst;
 689       assert(nm-&gt;method() != NULL, &quot;Unexpected NULL method()&quot;);
 690       tempst.print(&quot;compiled method &lt;%s&gt;\n&quot;
 691                    &quot; at PC&quot; INTPTR_FORMAT &quot; for thread &quot; INTPTR_FORMAT,
 692                    nm-&gt;method()-&gt;print_value_string(), p2i(pc), p2i(thread));
 693       Exceptions::log_exception(exception, tempst.as_string());
 694     }
 695     // for AbortVMOnException flag
 696     Exceptions::debug_check_abort(exception);
 697 
 698     // Clear out the exception oop and pc since looking up an
 699     // exception handler can cause class loading, which might throw an
 700     // exception and those fields are expected to be clear during
 701     // normal bytecode execution.
 702     thread-&gt;clear_exception_oop_and_pc();
 703 
 704     bool recursive_exception = false;
 705     continuation = SharedRuntime::compute_compiled_exc_handler(nm, pc, exception, false, false, recursive_exception);
 706     // If an exception was thrown during exception dispatch, the exception oop may have changed
 707     thread-&gt;set_exception_oop(exception());
 708     thread-&gt;set_exception_pc(pc);
 709 
 710     // the exception cache is used only by non-implicit exceptions
 711     // Update the exception cache only when there didn&#39;t happen
 712     // another exception during the computation of the compiled
 713     // exception handler. Checking for exception oop equality is not
 714     // sufficient because some exceptions are pre-allocated and reused.
 715     if (continuation != NULL &amp;&amp; !recursive_exception) {
 716       nm-&gt;add_handler_for_exception_and_pc(exception, pc, continuation);
 717     }
 718   }
 719 
 720   thread-&gt;set_vm_result(exception());
 721   // Set flag if return address is a method handle call site.
 722   thread-&gt;set_is_method_handle_return(nm-&gt;is_method_handle_return(pc));
 723 
 724   if (log_is_enabled(Info, exceptions)) {
 725     ResourceMark rm;
 726     log_info(exceptions)(&quot;Thread &quot; PTR_FORMAT &quot; continuing at PC &quot; PTR_FORMAT
 727                          &quot; for exception thrown at PC &quot; PTR_FORMAT,
 728                          p2i(thread), p2i(continuation), p2i(pc));
 729   }
 730 
 731   return continuation;
 732 JRT_END
 733 
 734 // Enter this method from compiled code only if there is a Java exception handler
 735 // in the method handling the exception.
 736 // We are entering here from exception stub. We don&#39;t do a normal VM transition here.
 737 // We do it in a helper. This is so we can check to see if the nmethod we have just
 738 // searched for an exception handler has been deoptimized in the meantime.
 739 address Runtime1::exception_handler_for_pc(JavaThread* thread) {
 740   oop exception = thread-&gt;exception_oop();
 741   address pc = thread-&gt;exception_pc();
 742   // Still in Java mode
 743   DEBUG_ONLY(ResetNoHandleMark rnhm);
 744   nmethod* nm = NULL;
 745   address continuation = NULL;
 746   {
 747     // Enter VM mode by calling the helper
 748     ResetNoHandleMark rnhm;
 749     continuation = exception_handler_for_pc_helper(thread, exception, pc, nm);
 750   }
 751   // Back in JAVA, use no oops DON&#39;T safepoint
 752 
 753   // Now check to see if the nmethod we were called from is now deoptimized.
 754   // If so we must return to the deopt blob and deoptimize the nmethod
 755   if (nm != NULL &amp;&amp; caller_is_deopted()) {
 756     continuation = SharedRuntime::deopt_blob()-&gt;unpack_with_exception_in_tls();
 757   }
 758 
 759   assert(continuation != NULL, &quot;no handler found&quot;);
 760   return continuation;
 761 }
 762 
 763 
 764 JRT_ENTRY(void, Runtime1::throw_range_check_exception(JavaThread* thread, int index, arrayOopDesc* a))
 765   NOT_PRODUCT(_throw_range_check_exception_count++;)
 766   const int len = 35;
 767   assert(len &lt; strlen(&quot;Index %d out of bounds for length %d&quot;), &quot;Must allocate more space for message.&quot;);
 768   char message[2 * jintAsStringSize + len];
 769   sprintf(message, &quot;Index %d out of bounds for length %d&quot;, index, a-&gt;length());
 770   SharedRuntime::throw_and_post_jvmti_exception(thread, vmSymbols::java_lang_ArrayIndexOutOfBoundsException(), message);
 771 JRT_END
 772 
 773 
 774 JRT_ENTRY(void, Runtime1::throw_index_exception(JavaThread* thread, int index))
 775   NOT_PRODUCT(_throw_index_exception_count++;)
 776   char message[16];
 777   sprintf(message, &quot;%d&quot;, index);
 778   SharedRuntime::throw_and_post_jvmti_exception(thread, vmSymbols::java_lang_IndexOutOfBoundsException(), message);
 779 JRT_END
 780 
 781 
 782 JRT_ENTRY(void, Runtime1::throw_div0_exception(JavaThread* thread))
 783   NOT_PRODUCT(_throw_div0_exception_count++;)
 784   SharedRuntime::throw_and_post_jvmti_exception(thread, vmSymbols::java_lang_ArithmeticException(), &quot;/ by zero&quot;);
 785 JRT_END
 786 
 787 
 788 JRT_ENTRY(void, Runtime1::throw_null_pointer_exception(JavaThread* thread))
 789   NOT_PRODUCT(_throw_null_pointer_exception_count++;)
 790   SharedRuntime::throw_and_post_jvmti_exception(thread, vmSymbols::java_lang_NullPointerException());
 791 JRT_END
 792 
 793 
 794 JRT_ENTRY(void, Runtime1::throw_class_cast_exception(JavaThread* thread, oopDesc* object))
 795   NOT_PRODUCT(_throw_class_cast_exception_count++;)
 796   ResourceMark rm(thread);
 797   char* message = SharedRuntime::generate_class_cast_message(
 798     thread, object-&gt;klass());
 799   SharedRuntime::throw_and_post_jvmti_exception(
 800     thread, vmSymbols::java_lang_ClassCastException(), message);
 801 JRT_END
 802 
 803 
 804 JRT_ENTRY(void, Runtime1::throw_incompatible_class_change_error(JavaThread* thread))
 805   NOT_PRODUCT(_throw_incompatible_class_change_error_count++;)
 806   ResourceMark rm(thread);
 807   SharedRuntime::throw_and_post_jvmti_exception(thread, vmSymbols::java_lang_IncompatibleClassChangeError());
 808 JRT_END
 809 
 810 
 811 JRT_ENTRY(void, Runtime1::throw_illegal_monitor_state_exception(JavaThread* thread))
 812   NOT_PRODUCT(_throw_illegal_monitor_state_exception_count++;)
 813   ResourceMark rm(thread);
 814   SharedRuntime::throw_and_post_jvmti_exception(thread, vmSymbols::java_lang_IllegalMonitorStateException());
 815 JRT_END
 816 
 817 
 818 JRT_BLOCK_ENTRY(void, Runtime1::monitorenter(JavaThread* thread, oopDesc* obj, BasicObjectLock* lock))
 819   NOT_PRODUCT(_monitorenter_slowcase_cnt++;)
 820   if (!UseFastLocking) {
 821     lock-&gt;set_obj(obj);
 822   }
 823   assert(obj == lock-&gt;obj(), &quot;must match&quot;);
 824   SharedRuntime::monitor_enter_helper(obj, lock-&gt;lock(), thread);
 825 JRT_END
 826 
 827 
 828 JRT_LEAF(void, Runtime1::monitorexit(JavaThread* thread, BasicObjectLock* lock))
 829   NOT_PRODUCT(_monitorexit_slowcase_cnt++;)
 830   assert(thread-&gt;last_Java_sp(), &quot;last_Java_sp must be set&quot;);
 831   oop obj = lock-&gt;obj();
 832   assert(oopDesc::is_oop(obj), &quot;must be NULL or an object&quot;);
 833   SharedRuntime::monitor_exit_helper(obj, lock-&gt;lock(), thread);
 834 JRT_END
 835 
 836 // Cf. OptoRuntime::deoptimize_caller_frame
 837 JRT_ENTRY(void, Runtime1::deoptimize(JavaThread* thread, jint trap_request))
 838   // Called from within the owner thread, so no need for safepoint
 839   RegisterMap reg_map(thread, false);
 840   frame stub_frame = thread-&gt;last_frame();
 841   assert(stub_frame.is_runtime_frame(), &quot;Sanity check&quot;);
 842   frame caller_frame = stub_frame.sender(&amp;reg_map);
 843   nmethod* nm = caller_frame.cb()-&gt;as_nmethod_or_null();
 844   assert(nm != NULL, &quot;Sanity check&quot;);
 845   methodHandle method(thread, nm-&gt;method());
 846   assert(nm == CodeCache::find_nmethod(caller_frame.pc()), &quot;Should be the same&quot;);
 847   Deoptimization::DeoptAction action = Deoptimization::trap_request_action(trap_request);
 848   Deoptimization::DeoptReason reason = Deoptimization::trap_request_reason(trap_request);
 849 
 850   if (action == Deoptimization::Action_make_not_entrant) {
 851     if (nm-&gt;make_not_entrant()) {
 852       if (reason == Deoptimization::Reason_tenured) {
 853         MethodData* trap_mdo = Deoptimization::get_method_data(thread, method, true /*create_if_missing*/);
 854         if (trap_mdo != NULL) {
 855           trap_mdo-&gt;inc_tenure_traps();
 856         }
 857       }
 858     }
 859   }
 860 
 861   // Deoptimize the caller frame.
 862   Deoptimization::deoptimize_frame(thread, caller_frame.id());
 863   // Return to the now deoptimized frame.
 864 JRT_END
 865 
 866 
 867 #ifndef DEOPTIMIZE_WHEN_PATCHING
 868 
 869 static Klass* resolve_field_return_klass(const methodHandle&amp; caller, int bci, TRAPS) {
 870   Bytecode_field field_access(caller, bci);
 871   // This can be static or non-static field access
 872   Bytecodes::Code code       = field_access.code();
 873 
 874   // We must load class, initialize class and resolve the field
 875   fieldDescriptor result; // initialize class if needed
 876   constantPoolHandle constants(THREAD, caller-&gt;constants());
 877   LinkResolver::resolve_field_access(result, constants, field_access.index(), caller, Bytecodes::java_code(code), CHECK_NULL);
 878   return result.field_holder();
 879 }
 880 
 881 
 882 //
 883 // This routine patches sites where a class wasn&#39;t loaded or
 884 // initialized at the time the code was generated.  It handles
 885 // references to classes, fields and forcing of initialization.  Most
 886 // of the cases are straightforward and involving simply forcing
 887 // resolution of a class, rewriting the instruction stream with the
 888 // needed constant and replacing the call in this function with the
 889 // patched code.  The case for static field is more complicated since
 890 // the thread which is in the process of initializing a class can
 891 // access it&#39;s static fields but other threads can&#39;t so the code
 892 // either has to deoptimize when this case is detected or execute a
 893 // check that the current thread is the initializing thread.  The
 894 // current
 895 //
 896 // Patches basically look like this:
 897 //
 898 //
 899 // patch_site: jmp patch stub     ;; will be patched
 900 // continue:   ...
 901 //             ...
 902 //             ...
 903 //             ...
 904 //
 905 // They have a stub which looks like this:
 906 //
 907 //             ;; patch body
 908 //             movl &lt;const&gt;, reg           (for class constants)
 909 //        &lt;or&gt; movl [reg1 + &lt;const&gt;], reg  (for field offsets)
 910 //        &lt;or&gt; movl reg, [reg1 + &lt;const&gt;]  (for field offsets)
 911 //             &lt;being_init offset&gt; &lt;bytes to copy&gt; &lt;bytes to skip&gt;
 912 // patch_stub: call Runtime1::patch_code (through a runtime stub)
 913 //             jmp patch_site
 914 //
 915 //
 916 // A normal patch is done by rewriting the patch body, usually a move,
 917 // and then copying it into place over top of the jmp instruction
 918 // being careful to flush caches and doing it in an MP-safe way.  The
 919 // constants following the patch body are used to find various pieces
 920 // of the patch relative to the call site for Runtime1::patch_code.
 921 // The case for getstatic and putstatic is more complicated because
 922 // getstatic and putstatic have special semantics when executing while
 923 // the class is being initialized.  getstatic/putstatic on a class
 924 // which is being_initialized may be executed by the initializing
 925 // thread but other threads have to block when they execute it.  This
 926 // is accomplished in compiled code by executing a test of the current
 927 // thread against the initializing thread of the class.  It&#39;s emitted
 928 // as boilerplate in their stub which allows the patched code to be
 929 // executed before it&#39;s copied back into the main body of the nmethod.
 930 //
 931 // being_init: get_thread(&lt;tmp reg&gt;
 932 //             cmpl [reg1 + &lt;init_thread_offset&gt;], &lt;tmp reg&gt;
 933 //             jne patch_stub
 934 //             movl [reg1 + &lt;const&gt;], reg  (for field offsets)  &lt;or&gt;
 935 //             movl reg, [reg1 + &lt;const&gt;]  (for field offsets)
 936 //             jmp continue
 937 //             &lt;being_init offset&gt; &lt;bytes to copy&gt; &lt;bytes to skip&gt;
 938 // patch_stub: jmp Runtim1::patch_code (through a runtime stub)
 939 //             jmp patch_site
 940 //
 941 // If the class is being initialized the patch body is rewritten and
 942 // the patch site is rewritten to jump to being_init, instead of
 943 // patch_stub.  Whenever this code is executed it checks the current
 944 // thread against the intializing thread so other threads will enter
 945 // the runtime and end up blocked waiting the class to finish
 946 // initializing inside the calls to resolve_field below.  The
 947 // initializing class will continue on it&#39;s way.  Once the class is
 948 // fully_initialized, the intializing_thread of the class becomes
 949 // NULL, so the next thread to execute this code will fail the test,
 950 // call into patch_code and complete the patching process by copying
 951 // the patch body back into the main part of the nmethod and resume
 952 // executing.
 953 
 954 // NB:
 955 //
 956 // Patchable instruction sequences inherently exhibit race conditions,
 957 // where thread A is patching an instruction at the same time thread B
 958 // is executing it.  The algorithms we use ensure that any observation
 959 // that B can make on any intermediate states during A&#39;s patching will
 960 // always end up with a correct outcome.  This is easiest if there are
 961 // few or no intermediate states.  (Some inline caches have two
 962 // related instructions that must be patched in tandem.  For those,
 963 // intermediate states seem to be unavoidable, but we will get the
 964 // right answer from all possible observation orders.)
 965 //
 966 // When patching the entry instruction at the head of a method, or a
 967 // linkable call instruction inside of a method, we try very hard to
 968 // use a patch sequence which executes as a single memory transaction.
 969 // This means, in practice, that when thread A patches an instruction,
 970 // it should patch a 32-bit or 64-bit word that somehow overlaps the
 971 // instruction or is contained in it.  We believe that memory hardware
 972 // will never break up such a word write, if it is naturally aligned
 973 // for the word being written.  We also know that some CPUs work very
 974 // hard to create atomic updates even of naturally unaligned words,
 975 // but we don&#39;t want to bet the farm on this always working.
 976 //
 977 // Therefore, if there is any chance of a race condition, we try to
 978 // patch only naturally aligned words, as single, full-word writes.
 979 
 980 JRT_ENTRY(void, Runtime1::patch_code(JavaThread* thread, Runtime1::StubID stub_id ))
 981   NOT_PRODUCT(_patch_code_slowcase_cnt++;)
 982 
 983   ResourceMark rm(thread);
 984   RegisterMap reg_map(thread, false);
 985   frame runtime_frame = thread-&gt;last_frame();
 986   frame caller_frame = runtime_frame.sender(&amp;reg_map);
 987 
 988   // last java frame on stack
 989   vframeStream vfst(thread, true);
 990   assert(!vfst.at_end(), &quot;Java frame must exist&quot;);
 991 
 992   methodHandle caller_method(THREAD, vfst.method());
 993   // Note that caller_method-&gt;code() may not be same as caller_code because of OSR&#39;s
 994   // Note also that in the presence of inlining it is not guaranteed
 995   // that caller_method() == caller_code-&gt;method()
 996 
 997   int bci = vfst.bci();
 998   Bytecodes::Code code = caller_method()-&gt;java_code_at(bci);
 999 
1000   // this is used by assertions in the access_field_patching_id
1001   BasicType patch_field_type = T_ILLEGAL;
1002   bool deoptimize_for_volatile = false;
1003   bool deoptimize_for_atomic = false;
1004   int patch_field_offset = -1;
1005   Klass* init_klass = NULL; // klass needed by load_klass_patching code
1006   Klass* load_klass = NULL; // klass needed by load_klass_patching code
1007   Handle mirror(THREAD, NULL);                    // oop needed by load_mirror_patching code
1008   Handle appendix(THREAD, NULL);                  // oop needed by appendix_patching code
1009   bool load_klass_or_mirror_patch_id =
1010     (stub_id == Runtime1::load_klass_patching_id || stub_id == Runtime1::load_mirror_patching_id);
1011 
1012   if (stub_id == Runtime1::access_field_patching_id) {
1013 
1014     Bytecode_field field_access(caller_method, bci);
1015     fieldDescriptor result; // initialize class if needed
1016     Bytecodes::Code code = field_access.code();
1017     constantPoolHandle constants(THREAD, caller_method-&gt;constants());
1018     LinkResolver::resolve_field_access(result, constants, field_access.index(), caller_method, Bytecodes::java_code(code), CHECK);
1019     patch_field_offset = result.offset();
1020 
1021     // If we&#39;re patching a field which is volatile then at compile it
1022     // must not have been know to be volatile, so the generated code
1023     // isn&#39;t correct for a volatile reference.  The nmethod has to be
1024     // deoptimized so that the code can be regenerated correctly.
1025     // This check is only needed for access_field_patching since this
1026     // is the path for patching field offsets.  load_klass is only
1027     // used for patching references to oops which don&#39;t need special
1028     // handling in the volatile case.
1029 
1030     deoptimize_for_volatile = result.access_flags().is_volatile();
1031 
1032     // If we are patching a field which should be atomic, then
1033     // the generated code is not correct either, force deoptimizing.
1034     // We need to only cover T_LONG and T_DOUBLE fields, as we can
1035     // break access atomicity only for them.
1036 
1037     // Strictly speaking, the deoptimization on 64-bit platforms
1038     // is unnecessary, and T_LONG stores on 32-bit platforms need
1039     // to be handled by special patching code when AlwaysAtomicAccesses
1040     // becomes product feature. At this point, we are still going
1041     // for the deoptimization for consistency against volatile
1042     // accesses.
1043 
1044     patch_field_type = result.field_type();
1045     deoptimize_for_atomic = (AlwaysAtomicAccesses &amp;&amp; (patch_field_type == T_DOUBLE || patch_field_type == T_LONG));
1046 
1047   } else if (load_klass_or_mirror_patch_id) {
1048     Klass* k = NULL;
1049     switch (code) {
1050       case Bytecodes::_putstatic:
1051       case Bytecodes::_getstatic:
1052         { Klass* klass = resolve_field_return_klass(caller_method, bci, CHECK);
1053           init_klass = klass;
1054           mirror = Handle(THREAD, klass-&gt;java_mirror());
1055         }
1056         break;
1057       case Bytecodes::_new:
1058         { Bytecode_new bnew(caller_method(), caller_method-&gt;bcp_from(bci));
1059           k = caller_method-&gt;constants()-&gt;klass_at(bnew.index(), CHECK);
1060         }
1061         break;
1062       case Bytecodes::_defaultvalue:
1063         { Bytecode_defaultvalue bdefaultvalue(caller_method(), caller_method-&gt;bcp_from(bci));
1064           k = caller_method-&gt;constants()-&gt;klass_at(bdefaultvalue.index(), CHECK);
1065         }
1066         break;
1067       case Bytecodes::_multianewarray:
1068         { Bytecode_multianewarray mna(caller_method(), caller_method-&gt;bcp_from(bci));
1069           k = caller_method-&gt;constants()-&gt;klass_at(mna.index(), CHECK);
1070           if (k-&gt;name()-&gt;is_Q_array_signature()) {
1071             // Logically creates elements, ensure klass init
1072             k-&gt;initialize(CHECK);
1073           }
1074         }
1075         break;
1076       case Bytecodes::_instanceof:
1077         { Bytecode_instanceof io(caller_method(), caller_method-&gt;bcp_from(bci));
1078           k = caller_method-&gt;constants()-&gt;klass_at(io.index(), CHECK);
1079         }
1080         break;
1081       case Bytecodes::_checkcast:
1082         { Bytecode_checkcast cc(caller_method(), caller_method-&gt;bcp_from(bci));
1083           k = caller_method-&gt;constants()-&gt;klass_at(cc.index(), CHECK);
1084         }
1085         break;
1086       case Bytecodes::_anewarray:
1087         { Bytecode_anewarray anew(caller_method(), caller_method-&gt;bcp_from(bci));
1088           Klass* ek = caller_method-&gt;constants()-&gt;klass_at(anew.index(), CHECK);
1089           k = ek-&gt;array_klass(CHECK);
1090         }
1091         break;
1092       case Bytecodes::_ldc:
1093       case Bytecodes::_ldc_w:
1094         {
1095           Bytecode_loadconstant cc(caller_method, bci);
1096           oop m = cc.resolve_constant(CHECK);
1097           mirror = Handle(THREAD, m);
1098         }
1099         break;
1100       default: fatal(&quot;unexpected bytecode for load_klass_or_mirror_patch_id&quot;);
1101     }
1102     load_klass = k;
1103   } else if (stub_id == load_appendix_patching_id) {
1104     Bytecode_invoke bytecode(caller_method, bci);
1105     Bytecodes::Code bc = bytecode.invoke_code();
1106 
1107     CallInfo info;
1108     constantPoolHandle pool(thread, caller_method-&gt;constants());
1109     int index = bytecode.index();
1110     LinkResolver::resolve_invoke(info, Handle(), pool, index, bc, CHECK);
1111     switch (bc) {
1112       case Bytecodes::_invokehandle: {
1113         int cache_index = ConstantPool::decode_cpcache_index(index, true);
1114         assert(cache_index &gt;= 0 &amp;&amp; cache_index &lt; pool-&gt;cache()-&gt;length(), &quot;unexpected cache index&quot;);
1115         ConstantPoolCacheEntry* cpce = pool-&gt;cache()-&gt;entry_at(cache_index);
1116         cpce-&gt;set_method_handle(pool, info);
1117         appendix = Handle(THREAD, cpce-&gt;appendix_if_resolved(pool)); // just in case somebody already resolved the entry
1118         break;
1119       }
1120       case Bytecodes::_invokedynamic: {
1121         ConstantPoolCacheEntry* cpce = pool-&gt;invokedynamic_cp_cache_entry_at(index);
1122         cpce-&gt;set_dynamic_call(pool, info);
1123         appendix = Handle(THREAD, cpce-&gt;appendix_if_resolved(pool)); // just in case somebody already resolved the entry
1124         break;
1125       }
1126       default: fatal(&quot;unexpected bytecode for load_appendix_patching_id&quot;);
1127     }
1128   } else {
1129     ShouldNotReachHere();
1130   }
1131 
1132   if (deoptimize_for_volatile || deoptimize_for_atomic) {
1133     // At compile time we assumed the field wasn&#39;t volatile/atomic but after
1134     // loading it turns out it was volatile/atomic so we have to throw the
1135     // compiled code out and let it be regenerated.
1136     if (TracePatching) {
1137       if (deoptimize_for_volatile) {
1138         tty-&gt;print_cr(&quot;Deoptimizing for patching volatile field reference&quot;);
1139       }
1140       if (deoptimize_for_atomic) {
1141         tty-&gt;print_cr(&quot;Deoptimizing for patching atomic field reference&quot;);
1142       }
1143     }
1144 
1145     // It&#39;s possible the nmethod was invalidated in the last
1146     // safepoint, but if it&#39;s still alive then make it not_entrant.
1147     nmethod* nm = CodeCache::find_nmethod(caller_frame.pc());
1148     if (nm != NULL) {
1149       nm-&gt;make_not_entrant();
1150     }
1151 
1152     Deoptimization::deoptimize_frame(thread, caller_frame.id());
1153 
1154     // Return to the now deoptimized frame.
1155   }
1156 
1157   // Now copy code back
1158 
1159   {
1160     MutexLocker ml_patch (THREAD, Patching_lock, Mutex::_no_safepoint_check_flag);
1161     //
1162     // Deoptimization may have happened while we waited for the lock.
1163     // In that case we don&#39;t bother to do any patching we just return
1164     // and let the deopt happen
1165     if (!caller_is_deopted()) {
1166       NativeGeneralJump* jump = nativeGeneralJump_at(caller_frame.pc());
1167       address instr_pc = jump-&gt;jump_destination();
1168       NativeInstruction* ni = nativeInstruction_at(instr_pc);
1169       if (ni-&gt;is_jump() ) {
1170         // the jump has not been patched yet
1171         // The jump destination is slow case and therefore not part of the stubs
1172         // (stubs are only for StaticCalls)
1173 
1174         // format of buffer
1175         //    ....
1176         //    instr byte 0     &lt;-- copy_buff
1177         //    instr byte 1
1178         //    ..
1179         //    instr byte n-1
1180         //      n
1181         //    ....             &lt;-- call destination
1182 
1183         address stub_location = caller_frame.pc() + PatchingStub::patch_info_offset();
1184         unsigned char* byte_count = (unsigned char*) (stub_location - 1);
1185         unsigned char* byte_skip = (unsigned char*) (stub_location - 2);
1186         unsigned char* being_initialized_entry_offset = (unsigned char*) (stub_location - 3);
1187         address copy_buff = stub_location - *byte_skip - *byte_count;
1188         address being_initialized_entry = stub_location - *being_initialized_entry_offset;
1189         if (TracePatching) {
1190           ttyLocker ttyl;
1191           tty-&gt;print_cr(&quot; Patching %s at bci %d at address &quot; INTPTR_FORMAT &quot;  (%s)&quot;, Bytecodes::name(code), bci,
1192                         p2i(instr_pc), (stub_id == Runtime1::access_field_patching_id) ? &quot;field&quot; : &quot;klass&quot;);
1193           nmethod* caller_code = CodeCache::find_nmethod(caller_frame.pc());
1194           assert(caller_code != NULL, &quot;nmethod not found&quot;);
1195 
1196           // NOTE we use pc() not original_pc() because we already know they are
1197           // identical otherwise we&#39;d have never entered this block of code
1198 
1199           const ImmutableOopMap* map = caller_code-&gt;oop_map_for_return_address(caller_frame.pc());
1200           assert(map != NULL, &quot;null check&quot;);
1201           map-&gt;print();
1202           tty-&gt;cr();
1203 
1204           Disassembler::decode(copy_buff, copy_buff + *byte_count, tty);
1205         }
1206         // depending on the code below, do_patch says whether to copy the patch body back into the nmethod
1207         bool do_patch = true;
1208         if (stub_id == Runtime1::access_field_patching_id) {
1209           // The offset may not be correct if the class was not loaded at code generation time.
1210           // Set it now.
1211           NativeMovRegMem* n_move = nativeMovRegMem_at(copy_buff);
1212           assert(n_move-&gt;offset() == 0 || (n_move-&gt;offset() == 4 &amp;&amp; (patch_field_type == T_DOUBLE || patch_field_type == T_LONG)), &quot;illegal offset for type&quot;);
1213           assert(patch_field_offset &gt;= 0, &quot;illegal offset&quot;);
1214           n_move-&gt;add_offset_in_bytes(patch_field_offset);
1215         } else if (load_klass_or_mirror_patch_id) {
1216           // If a getstatic or putstatic is referencing a klass which
1217           // isn&#39;t fully initialized, the patch body isn&#39;t copied into
1218           // place until initialization is complete.  In this case the
1219           // patch site is setup so that any threads besides the
1220           // initializing thread are forced to come into the VM and
1221           // block.
1222           do_patch = (code != Bytecodes::_getstatic &amp;&amp; code != Bytecodes::_putstatic) ||
1223                      InstanceKlass::cast(init_klass)-&gt;is_initialized();
1224           NativeGeneralJump* jump = nativeGeneralJump_at(instr_pc);
1225           if (jump-&gt;jump_destination() == being_initialized_entry) {
1226             assert(do_patch == true, &quot;initialization must be complete at this point&quot;);
1227           } else {
1228             // patch the instruction &lt;move reg, klass&gt;
1229             NativeMovConstReg* n_copy = nativeMovConstReg_at(copy_buff);
1230 
1231             assert(n_copy-&gt;data() == 0 ||
1232                    n_copy-&gt;data() == (intptr_t)Universe::non_oop_word(),
1233                    &quot;illegal init value&quot;);
1234             if (stub_id == Runtime1::load_klass_patching_id) {
1235               assert(load_klass != NULL, &quot;klass not set&quot;);
1236               n_copy-&gt;set_data((intx) (load_klass));
1237             } else {
1238               assert(mirror() != NULL, &quot;klass not set&quot;);
1239               // Don&#39;t need a G1 pre-barrier here since we assert above that data isn&#39;t an oop.
1240               n_copy-&gt;set_data(cast_from_oop&lt;intx&gt;(mirror()));
1241             }
1242 
1243             if (TracePatching) {
1244               Disassembler::decode(copy_buff, copy_buff + *byte_count, tty);
1245             }
1246           }
1247         } else if (stub_id == Runtime1::load_appendix_patching_id) {
1248           NativeMovConstReg* n_copy = nativeMovConstReg_at(copy_buff);
1249           assert(n_copy-&gt;data() == 0 ||
1250                  n_copy-&gt;data() == (intptr_t)Universe::non_oop_word(),
1251                  &quot;illegal init value&quot;);
1252           n_copy-&gt;set_data(cast_from_oop&lt;intx&gt;(appendix()));
1253 
1254           if (TracePatching) {
1255             Disassembler::decode(copy_buff, copy_buff + *byte_count, tty);
1256           }
1257         } else {
1258           ShouldNotReachHere();
1259         }
1260 
1261 #if defined(PPC32)
1262         if (load_klass_or_mirror_patch_id ||
1263             stub_id == Runtime1::load_appendix_patching_id) {
1264           // Update the location in the nmethod with the proper
1265           // metadata.  When the code was generated, a NULL was stuffed
1266           // in the metadata table and that table needs to be update to
1267           // have the right value.  On intel the value is kept
1268           // directly in the instruction instead of in the metadata
1269           // table, so set_data above effectively updated the value.
1270           nmethod* nm = CodeCache::find_nmethod(instr_pc);
1271           assert(nm != NULL, &quot;invalid nmethod_pc&quot;);
1272           RelocIterator mds(nm, copy_buff, copy_buff + 1);
1273           bool found = false;
1274           while (mds.next() &amp;&amp; !found) {
1275             if (mds.type() == relocInfo::oop_type) {
1276               assert(stub_id == Runtime1::load_mirror_patching_id ||
1277                      stub_id == Runtime1::load_appendix_patching_id, &quot;wrong stub id&quot;);
1278               oop_Relocation* r = mds.oop_reloc();
1279               oop* oop_adr = r-&gt;oop_addr();
1280               *oop_adr = stub_id == Runtime1::load_mirror_patching_id ? mirror() : appendix();
1281               r-&gt;fix_oop_relocation();
1282               found = true;
1283             } else if (mds.type() == relocInfo::metadata_type) {
1284               assert(stub_id == Runtime1::load_klass_patching_id, &quot;wrong stub id&quot;);
1285               metadata_Relocation* r = mds.metadata_reloc();
1286               Metadata** metadata_adr = r-&gt;metadata_addr();
1287               *metadata_adr = load_klass;
1288               r-&gt;fix_metadata_relocation();
1289               found = true;
1290             }
1291           }
1292           assert(found, &quot;the metadata must exist!&quot;);
1293         }
1294 #endif
1295         if (do_patch) {
1296           // replace instructions
1297           // first replace the tail, then the call
1298 #ifdef ARM
1299           if((load_klass_or_mirror_patch_id ||
1300               stub_id == Runtime1::load_appendix_patching_id) &amp;&amp;
1301               nativeMovConstReg_at(copy_buff)-&gt;is_pc_relative()) {
1302             nmethod* nm = CodeCache::find_nmethod(instr_pc);
1303             address addr = NULL;
1304             assert(nm != NULL, &quot;invalid nmethod_pc&quot;);
1305             RelocIterator mds(nm, copy_buff, copy_buff + 1);
1306             while (mds.next()) {
1307               if (mds.type() == relocInfo::oop_type) {
1308                 assert(stub_id == Runtime1::load_mirror_patching_id ||
1309                        stub_id == Runtime1::load_appendix_patching_id, &quot;wrong stub id&quot;);
1310                 oop_Relocation* r = mds.oop_reloc();
1311                 addr = (address)r-&gt;oop_addr();
1312                 break;
1313               } else if (mds.type() == relocInfo::metadata_type) {
1314                 assert(stub_id == Runtime1::load_klass_patching_id, &quot;wrong stub id&quot;);
1315                 metadata_Relocation* r = mds.metadata_reloc();
1316                 addr = (address)r-&gt;metadata_addr();
1317                 break;
1318               }
1319             }
1320             assert(addr != NULL, &quot;metadata relocation must exist&quot;);
1321             copy_buff -= *byte_count;
1322             NativeMovConstReg* n_copy2 = nativeMovConstReg_at(copy_buff);
1323             n_copy2-&gt;set_pc_relative_offset(addr, instr_pc);
1324           }
1325 #endif
1326 
1327           for (int i = NativeGeneralJump::instruction_size; i &lt; *byte_count; i++) {
1328             address ptr = copy_buff + i;
1329             int a_byte = (*ptr) &amp; 0xFF;
1330             address dst = instr_pc + i;
1331             *(unsigned char*)dst = (unsigned char) a_byte;
1332           }
1333           ICache::invalidate_range(instr_pc, *byte_count);
1334           NativeGeneralJump::replace_mt_safe(instr_pc, copy_buff);
1335 
1336           if (load_klass_or_mirror_patch_id ||
1337               stub_id == Runtime1::load_appendix_patching_id) {
1338             relocInfo::relocType rtype =
1339               (stub_id == Runtime1::load_klass_patching_id) ?
1340                                    relocInfo::metadata_type :
1341                                    relocInfo::oop_type;
1342             // update relocInfo to metadata
1343             nmethod* nm = CodeCache::find_nmethod(instr_pc);
1344             assert(nm != NULL, &quot;invalid nmethod_pc&quot;);
1345 
1346             // The old patch site is now a move instruction so update
1347             // the reloc info so that it will get updated during
1348             // future GCs.
1349             RelocIterator iter(nm, (address)instr_pc, (address)(instr_pc + 1));
1350             relocInfo::change_reloc_info_for_address(&amp;iter, (address) instr_pc,
1351                                                      relocInfo::none, rtype);
1352 #ifdef PPC32
1353           { address instr_pc2 = instr_pc + NativeMovConstReg::lo_offset;
1354             RelocIterator iter2(nm, instr_pc2, instr_pc2 + 1);
1355             relocInfo::change_reloc_info_for_address(&amp;iter2, (address) instr_pc2,
1356                                                      relocInfo::none, rtype);
1357           }
1358 #endif
1359           }
1360 
1361         } else {
1362           ICache::invalidate_range(copy_buff, *byte_count);
1363           NativeGeneralJump::insert_unconditional(instr_pc, being_initialized_entry);
1364         }
1365       }
1366     }
1367   }
1368 
1369   // If we are patching in a non-perm oop, make sure the nmethod
1370   // is on the right list.
1371   {
1372     MutexLocker ml_code (THREAD, CodeCache_lock, Mutex::_no_safepoint_check_flag);
1373     nmethod* nm = CodeCache::find_nmethod(caller_frame.pc());
1374     guarantee(nm != NULL, &quot;only nmethods can contain non-perm oops&quot;);
1375 
1376     // Since we&#39;ve patched some oops in the nmethod,
1377     // (re)register it with the heap.
1378     Universe::heap()-&gt;register_nmethod(nm);
1379   }
1380 JRT_END
1381 
1382 #else // DEOPTIMIZE_WHEN_PATCHING
1383 
1384 JRT_ENTRY(void, Runtime1::patch_code(JavaThread* thread, Runtime1::StubID stub_id ))
1385   RegisterMap reg_map(thread, false);
1386 
1387   NOT_PRODUCT(_patch_code_slowcase_cnt++;)
1388   if (TracePatching) {
1389     tty-&gt;print_cr(&quot;Deoptimizing because patch is needed&quot;);
1390   }
1391 
1392   frame runtime_frame = thread-&gt;last_frame();
1393   frame caller_frame = runtime_frame.sender(&amp;reg_map);
1394 
1395   // It&#39;s possible the nmethod was invalidated in the last
1396   // safepoint, but if it&#39;s still alive then make it not_entrant.
1397   nmethod* nm = CodeCache::find_nmethod(caller_frame.pc());
1398   if (nm != NULL) {
1399     nm-&gt;make_not_entrant();
1400   }
1401 
1402   Deoptimization::deoptimize_frame(thread, caller_frame.id());
1403 
1404   // Return to the now deoptimized frame.
1405 JRT_END
1406 
1407 #endif // DEOPTIMIZE_WHEN_PATCHING
1408 
1409 //
1410 // Entry point for compiled code. We want to patch a nmethod.
1411 // We don&#39;t do a normal VM transition here because we want to
1412 // know after the patching is complete and any safepoint(s) are taken
1413 // if the calling nmethod was deoptimized. We do this by calling a
1414 // helper method which does the normal VM transition and when it
1415 // completes we can check for deoptimization. This simplifies the
1416 // assembly code in the cpu directories.
1417 //
1418 int Runtime1::move_klass_patching(JavaThread* thread) {
1419 //
1420 // NOTE: we are still in Java
1421 //
1422   Thread* THREAD = thread;
1423   debug_only(NoHandleMark nhm;)
1424   {
1425     // Enter VM mode
1426 
1427     ResetNoHandleMark rnhm;
1428     patch_code(thread, load_klass_patching_id);
1429   }
1430   // Back in JAVA, use no oops DON&#39;T safepoint
1431 
1432   // Return true if calling code is deoptimized
1433 
1434   return caller_is_deopted();
1435 }
1436 
1437 int Runtime1::move_mirror_patching(JavaThread* thread) {
1438 //
1439 // NOTE: we are still in Java
1440 //
1441   Thread* THREAD = thread;
1442   debug_only(NoHandleMark nhm;)
1443   {
1444     // Enter VM mode
1445 
1446     ResetNoHandleMark rnhm;
1447     patch_code(thread, load_mirror_patching_id);
1448   }
1449   // Back in JAVA, use no oops DON&#39;T safepoint
1450 
1451   // Return true if calling code is deoptimized
1452 
1453   return caller_is_deopted();
1454 }
1455 
1456 int Runtime1::move_appendix_patching(JavaThread* thread) {
1457 //
1458 // NOTE: we are still in Java
1459 //
1460   Thread* THREAD = thread;
1461   debug_only(NoHandleMark nhm;)
1462   {
1463     // Enter VM mode
1464 
1465     ResetNoHandleMark rnhm;
1466     patch_code(thread, load_appendix_patching_id);
1467   }
1468   // Back in JAVA, use no oops DON&#39;T safepoint
1469 
1470   // Return true if calling code is deoptimized
1471 
1472   return caller_is_deopted();
1473 }
1474 //
1475 // Entry point for compiled code. We want to patch a nmethod.
1476 // We don&#39;t do a normal VM transition here because we want to
1477 // know after the patching is complete and any safepoint(s) are taken
1478 // if the calling nmethod was deoptimized. We do this by calling a
1479 // helper method which does the normal VM transition and when it
1480 // completes we can check for deoptimization. This simplifies the
1481 // assembly code in the cpu directories.
1482 //
1483 
1484 int Runtime1::access_field_patching(JavaThread* thread) {
1485 //
1486 // NOTE: we are still in Java
1487 //
1488   Thread* THREAD = thread;
1489   debug_only(NoHandleMark nhm;)
1490   {
1491     // Enter VM mode
1492 
1493     ResetNoHandleMark rnhm;
1494     patch_code(thread, access_field_patching_id);
1495   }
1496   // Back in JAVA, use no oops DON&#39;T safepoint
1497 
1498   // Return true if calling code is deoptimized
1499 
1500   return caller_is_deopted();
1501 JRT_END
1502 
1503 
1504 JRT_LEAF(void, Runtime1::trace_block_entry(jint block_id))
1505   // for now we just print out the block id
1506   tty-&gt;print(&quot;%d &quot;, block_id);
1507 JRT_END
1508 
1509 
1510 JRT_LEAF(int, Runtime1::is_instance_of(oopDesc* mirror, oopDesc* obj))
1511   // had to return int instead of bool, otherwise there may be a mismatch
1512   // between the C calling convention and the Java one.
1513   // e.g., on x86, GCC may clear only %al when returning a bool false, but
1514   // JVM takes the whole %eax as the return value, which may misinterpret
1515   // the return value as a boolean true.
1516 
1517   assert(mirror != NULL, &quot;should null-check on mirror before calling&quot;);
1518   Klass* k = java_lang_Class::as_Klass(mirror);
1519   return (k != NULL &amp;&amp; obj != NULL &amp;&amp; obj-&gt;is_a(k)) ? 1 : 0;
1520 JRT_END
1521 
1522 JRT_ENTRY(void, Runtime1::predicate_failed_trap(JavaThread* thread))
1523   ResourceMark rm;
1524 
1525   assert(!TieredCompilation, &quot;incompatible with tiered compilation&quot;);
1526 
1527   RegisterMap reg_map(thread, false);
1528   frame runtime_frame = thread-&gt;last_frame();
1529   frame caller_frame = runtime_frame.sender(&amp;reg_map);
1530 
1531   nmethod* nm = CodeCache::find_nmethod(caller_frame.pc());
1532   assert (nm != NULL, &quot;no more nmethod?&quot;);
1533   nm-&gt;make_not_entrant();
1534 
1535   methodHandle m(thread, nm-&gt;method());
1536   MethodData* mdo = m-&gt;method_data();
1537 
1538   if (mdo == NULL &amp;&amp; !HAS_PENDING_EXCEPTION) {
1539     // Build an MDO.  Ignore errors like OutOfMemory;
1540     // that simply means we won&#39;t have an MDO to update.
1541     Method::build_interpreter_method_data(m, THREAD);
1542     if (HAS_PENDING_EXCEPTION) {
1543       assert((PENDING_EXCEPTION-&gt;is_a(SystemDictionary::OutOfMemoryError_klass())), &quot;we expect only an OOM error here&quot;);
1544       CLEAR_PENDING_EXCEPTION;
1545     }
1546     mdo = m-&gt;method_data();
1547   }
1548 
1549   if (mdo != NULL) {
1550     mdo-&gt;inc_trap_count(Deoptimization::Reason_none);
1551   }
1552 
1553   if (TracePredicateFailedTraps) {
1554     stringStream ss1, ss2;
1555     vframeStream vfst(thread);
1556     Method* inlinee = vfst.method();
1557     inlinee-&gt;print_short_name(&amp;ss1);
1558     m-&gt;print_short_name(&amp;ss2);
1559     tty-&gt;print_cr(&quot;Predicate failed trap in method %s at bci %d inlined in %s at pc &quot; INTPTR_FORMAT, ss1.as_string(), vfst.bci(), ss2.as_string(), p2i(caller_frame.pc()));
1560   }
1561 
1562 
1563   Deoptimization::deoptimize_frame(thread, caller_frame.id());
1564 
1565 JRT_END
1566 
1567 #ifndef PRODUCT
1568 void Runtime1::print_statistics() {
1569   tty-&gt;print_cr(&quot;C1 Runtime statistics:&quot;);
1570   tty-&gt;print_cr(&quot; _resolve_invoke_virtual_cnt:     %d&quot;, SharedRuntime::_resolve_virtual_ctr);
1571   tty-&gt;print_cr(&quot; _resolve_invoke_opt_virtual_cnt: %d&quot;, SharedRuntime::_resolve_opt_virtual_ctr);
1572   tty-&gt;print_cr(&quot; _resolve_invoke_static_cnt:      %d&quot;, SharedRuntime::_resolve_static_ctr);
1573   tty-&gt;print_cr(&quot; _handle_wrong_method_cnt:        %d&quot;, SharedRuntime::_wrong_method_ctr);
1574   tty-&gt;print_cr(&quot; _ic_miss_cnt:                    %d&quot;, SharedRuntime::_ic_miss_ctr);
1575   tty-&gt;print_cr(&quot; _generic_arraycopy_cnt:          %d&quot;, _generic_arraycopy_cnt);
1576   tty-&gt;print_cr(&quot; _generic_arraycopystub_cnt:      %d&quot;, _generic_arraycopystub_cnt);
1577   tty-&gt;print_cr(&quot; _byte_arraycopy_cnt:             %d&quot;, _byte_arraycopy_stub_cnt);
1578   tty-&gt;print_cr(&quot; _short_arraycopy_cnt:            %d&quot;, _short_arraycopy_stub_cnt);
1579   tty-&gt;print_cr(&quot; _int_arraycopy_cnt:              %d&quot;, _int_arraycopy_stub_cnt);
1580   tty-&gt;print_cr(&quot; _long_arraycopy_cnt:             %d&quot;, _long_arraycopy_stub_cnt);
1581   tty-&gt;print_cr(&quot; _oop_arraycopy_cnt:              %d&quot;, _oop_arraycopy_stub_cnt);
1582   tty-&gt;print_cr(&quot; _arraycopy_slowcase_cnt:         %d&quot;, _arraycopy_slowcase_cnt);
1583   tty-&gt;print_cr(&quot; _arraycopy_checkcast_cnt:        %d&quot;, _arraycopy_checkcast_cnt);
1584   tty-&gt;print_cr(&quot; _arraycopy_checkcast_attempt_cnt:%d&quot;, _arraycopy_checkcast_attempt_cnt);
1585 
1586   tty-&gt;print_cr(&quot; _new_type_array_slowcase_cnt:    %d&quot;, _new_type_array_slowcase_cnt);
1587   tty-&gt;print_cr(&quot; _new_object_array_slowcase_cnt:  %d&quot;, _new_object_array_slowcase_cnt);
1588   tty-&gt;print_cr(&quot; _new_value_array_slowcase_cnt:   %d&quot;, _new_value_array_slowcase_cnt);
1589   tty-&gt;print_cr(&quot; _new_instance_slowcase_cnt:      %d&quot;, _new_instance_slowcase_cnt);
1590   tty-&gt;print_cr(&quot; _new_multi_array_slowcase_cnt:   %d&quot;, _new_multi_array_slowcase_cnt);
1591   tty-&gt;print_cr(&quot; _load_flattened_array_slowcase_cnt:   %d&quot;, _load_flattened_array_slowcase_cnt);
1592   tty-&gt;print_cr(&quot; _store_flattened_array_slowcase_cnt:  %d&quot;, _store_flattened_array_slowcase_cnt);
1593   tty-&gt;print_cr(&quot; _substitutability_check_slowcase_cnt: %d&quot;, _substitutability_check_slowcase_cnt);
1594   tty-&gt;print_cr(&quot; _buffer_value_args_slowcase_cnt:%d&quot;, _buffer_value_args_slowcase_cnt);
1595   tty-&gt;print_cr(&quot; _buffer_value_args_no_receiver_slowcase_cnt:%d&quot;, _buffer_value_args_no_receiver_slowcase_cnt);
1596 
1597   tty-&gt;print_cr(&quot; _monitorenter_slowcase_cnt:      %d&quot;, _monitorenter_slowcase_cnt);
1598   tty-&gt;print_cr(&quot; _monitorexit_slowcase_cnt:       %d&quot;, _monitorexit_slowcase_cnt);
1599   tty-&gt;print_cr(&quot; _patch_code_slowcase_cnt:        %d&quot;, _patch_code_slowcase_cnt);
1600 
1601   tty-&gt;print_cr(&quot; _throw_range_check_exception_count:            %d:&quot;, _throw_range_check_exception_count);
1602   tty-&gt;print_cr(&quot; _throw_index_exception_count:                  %d:&quot;, _throw_index_exception_count);
1603   tty-&gt;print_cr(&quot; _throw_div0_exception_count:                   %d:&quot;, _throw_div0_exception_count);
1604   tty-&gt;print_cr(&quot; _throw_null_pointer_exception_count:           %d:&quot;, _throw_null_pointer_exception_count);
1605   tty-&gt;print_cr(&quot; _throw_class_cast_exception_count:             %d:&quot;, _throw_class_cast_exception_count);
1606   tty-&gt;print_cr(&quot; _throw_incompatible_class_change_error_count:  %d:&quot;, _throw_incompatible_class_change_error_count);
1607   tty-&gt;print_cr(&quot; _throw_illegal_monitor_state_exception_count:  %d:&quot;, _throw_illegal_monitor_state_exception_count);
1608   tty-&gt;print_cr(&quot; _throw_array_store_exception_count:            %d:&quot;, _throw_array_store_exception_count);
1609   tty-&gt;print_cr(&quot; _throw_count:                                  %d:&quot;, _throw_count);
1610 
1611   SharedRuntime::print_ic_miss_histogram();
1612   tty-&gt;cr();
1613 }
1614 #endif // PRODUCT
<a name="2" id="anc2"></a><b style="font-size: large; color: red">--- EOF ---</b>
















































































</pre>
<input id="eof" value="2" type="hidden" />
</body>
</html>