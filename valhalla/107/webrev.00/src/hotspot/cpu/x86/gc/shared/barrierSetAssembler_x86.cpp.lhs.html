<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Frames src/hotspot/cpu/x86/gc/shared/barrierSetAssembler_x86.cpp</title>
    <link rel="stylesheet" href="../../../../../../style.css" />
    <script type="text/javascript" src="../../../../../../navigation.js"></script>
  </head>
<body onkeypress="keypress(event);">
<a name="0"></a>
<hr />
<pre>  1 /*
  2  * Copyright (c) 2018, 2019, Oracle and/or its affiliates. All rights reserved.
  3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  4  *
  5  * This code is free software; you can redistribute it and/or modify it
  6  * under the terms of the GNU General Public License version 2 only, as
  7  * published by the Free Software Foundation.
  8  *
  9  * This code is distributed in the hope that it will be useful, but WITHOUT
 10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
 11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
 12  * version 2 for more details (a copy is included in the LICENSE file that
 13  * accompanied this code).
 14  *
 15  * You should have received a copy of the GNU General Public License version
 16  * 2 along with this work; if not, write to the Free Software Foundation,
 17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 18  *
 19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
 20  * or visit www.oracle.com if you need additional information or have any
 21  * questions.
 22  *
 23  */
 24 
 25 #include &quot;precompiled.hpp&quot;
 26 #include &quot;asm/macroAssembler.inline.hpp&quot;
 27 #include &quot;gc/shared/barrierSet.hpp&quot;
 28 #include &quot;gc/shared/barrierSetAssembler.hpp&quot;
 29 #include &quot;gc/shared/barrierSetNMethod.hpp&quot;
 30 #include &quot;gc/shared/barrierSetRuntime.hpp&quot;
 31 #include &quot;gc/shared/collectedHeap.hpp&quot;
 32 #include &quot;interpreter/interp_masm.hpp&quot;
 33 #include &quot;memory/universe.hpp&quot;
 34 #include &quot;runtime/jniHandles.hpp&quot;
 35 #include &quot;runtime/sharedRuntime.hpp&quot;
 36 #include &quot;runtime/thread.hpp&quot;
 37 
 38 #define __ masm-&gt;
 39 
 40 void BarrierSetAssembler::load_at(MacroAssembler* masm, DecoratorSet decorators, BasicType type,
 41                                   Register dst, Address src, Register tmp1, Register tmp_thread) {
 42   bool in_heap = (decorators &amp; IN_HEAP) != 0;
 43   bool in_native = (decorators &amp; IN_NATIVE) != 0;
 44   bool is_not_null = (decorators &amp; IS_NOT_NULL) != 0;
 45   bool atomic = (decorators &amp; MO_RELAXED) != 0;
 46 
<a name="1" id="anc1"></a><span class="line-modified"> 47   assert(type != T_VALUETYPE, &quot;Not supported yet&quot;);</span>
 48   switch (type) {
 49   case T_OBJECT:
 50   case T_ARRAY: {
 51     if (in_heap) {
 52 #ifdef _LP64
 53       if (UseCompressedOops) {
 54         __ movl(dst, src);
 55         if (is_not_null) {
 56           __ decode_heap_oop_not_null(dst);
 57         } else {
 58           __ decode_heap_oop(dst);
 59         }
 60       } else
 61 #endif
 62       {
 63         __ movptr(dst, src);
 64       }
 65     } else {
 66       assert(in_native, &quot;why else?&quot;);
 67       __ movptr(dst, src);
 68     }
 69     break;
 70   }
 71   case T_BOOLEAN: __ load_unsigned_byte(dst, src);  break;
 72   case T_BYTE:    __ load_signed_byte(dst, src);    break;
 73   case T_CHAR:    __ load_unsigned_short(dst, src); break;
 74   case T_SHORT:   __ load_signed_short(dst, src);   break;
 75   case T_INT:     __ movl  (dst, src);              break;
 76   case T_ADDRESS: __ movptr(dst, src);              break;
 77   case T_FLOAT:
 78     assert(dst == noreg, &quot;only to ftos&quot;);
 79     __ load_float(src);
 80     break;
 81   case T_DOUBLE:
 82     assert(dst == noreg, &quot;only to dtos&quot;);
 83     __ load_double(src);
 84     break;
 85   case T_LONG:
 86     assert(dst == noreg, &quot;only to ltos&quot;);
 87 #ifdef _LP64
 88     __ movq(rax, src);
 89 #else
 90     if (atomic) {
 91       __ fild_d(src);               // Must load atomically
 92       __ subptr(rsp,2*wordSize);    // Make space for store
 93       __ fistp_d(Address(rsp,0));
 94       __ pop(rax);
 95       __ pop(rdx);
 96     } else {
 97       __ movl(rax, src);
 98       __ movl(rdx, src.plus_disp(wordSize));
 99     }
100 #endif
101     break;
102   default: Unimplemented();
103   }
104 }
105 
106 void BarrierSetAssembler::store_at(MacroAssembler* masm, DecoratorSet decorators, BasicType type,
107                                    Address dst, Register val, Register tmp1, Register tmp2, Register tmp3) {
108   bool in_heap = (decorators &amp; IN_HEAP) != 0;
109   bool in_native = (decorators &amp; IN_NATIVE) != 0;
110   bool is_not_null = (decorators &amp; IS_NOT_NULL) != 0;
111   bool atomic = (decorators &amp; MO_RELAXED) != 0;
112 
<a name="2" id="anc2"></a><span class="line-modified">113   assert(type != T_VALUETYPE, &quot;Not supported yet&quot;);</span>
114   switch (type) {
115   case T_OBJECT:
116   case T_ARRAY: {
117     if (in_heap) {
118       if (val == noreg) {
119         assert(!is_not_null, &quot;inconsistent access&quot;);
120 #ifdef _LP64
121         if (UseCompressedOops) {
122           __ movl(dst, (int32_t)NULL_WORD);
123         } else {
124           __ movslq(dst, (int32_t)NULL_WORD);
125         }
126 #else
127         __ movl(dst, (int32_t)NULL_WORD);
128 #endif
129       } else {
130 #ifdef _LP64
131         if (UseCompressedOops) {
132           assert(!dst.uses(val), &quot;not enough registers&quot;);
133           if (is_not_null) {
134             __ encode_heap_oop_not_null(val);
135           } else {
136             __ encode_heap_oop(val);
137           }
138           __ movl(dst, val);
139         } else
140 #endif
141         {
142           __ movptr(dst, val);
143         }
144       }
145     } else {
146       assert(in_native, &quot;why else?&quot;);
147       assert(val != noreg, &quot;not supported&quot;);
148       __ movptr(dst, val);
149     }
150     break;
151   }
152   case T_BOOLEAN:
153     __ andl(val, 0x1);  // boolean is true if LSB is 1
154     __ movb(dst, val);
155     break;
156   case T_BYTE:
157     __ movb(dst, val);
158     break;
159   case T_SHORT:
160     __ movw(dst, val);
161     break;
162   case T_CHAR:
163     __ movw(dst, val);
164     break;
165   case T_INT:
166     __ movl(dst, val);
167     break;
168   case T_LONG:
169     assert(val == noreg, &quot;only tos&quot;);
170 #ifdef _LP64
171     __ movq(dst, rax);
172 #else
173     if (atomic) {
174       __ push(rdx);
175       __ push(rax);                 // Must update atomically with FIST
176       __ fild_d(Address(rsp,0));    // So load into FPU register
177       __ fistp_d(dst);              // and put into memory atomically
178       __ addptr(rsp, 2*wordSize);
179     } else {
180       __ movptr(dst, rax);
181       __ movptr(dst.plus_disp(wordSize), rdx);
182     }
183 #endif
184     break;
185   case T_FLOAT:
186     assert(val == noreg, &quot;only tos&quot;);
187     __ store_float(dst);
188     break;
189   case T_DOUBLE:
190     assert(val == noreg, &quot;only tos&quot;);
191     __ store_double(dst);
192     break;
193   case T_ADDRESS:
194     __ movptr(dst, val);
195     break;
196   default: Unimplemented();
197   }
198 }
199 
200 void BarrierSetAssembler::value_copy(MacroAssembler* masm, DecoratorSet decorators,
201                                      Register src, Register dst, Register value_klass) {
202   // value_copy implementation is fairly complex, and there are not any
203   // &quot;short-cuts&quot; to be made from asm. What there is, appears to have the same
204   // cost in C++, so just &quot;call_VM_leaf&quot; for now rather than maintain hundreds
205   // of hand-rolled instructions...
206   if (decorators &amp; IS_DEST_UNINITIALIZED) {
207     __ call_VM_leaf(CAST_FROM_FN_PTR(address, BarrierSetRuntime::value_copy_is_dest_uninitialized), src, dst, value_klass);
208   } else {
209     __ call_VM_leaf(CAST_FROM_FN_PTR(address, BarrierSetRuntime::value_copy), src, dst, value_klass);
210   }
211 }
212 
213 
214 #ifndef _LP64
215 void BarrierSetAssembler::obj_equals(MacroAssembler* masm,
216                                      Address obj1, jobject obj2) {
217   __ cmpoop_raw(obj1, obj2);
218 }
219 
220 void BarrierSetAssembler::obj_equals(MacroAssembler* masm,
221                                      Register obj1, jobject obj2) {
222   __ cmpoop_raw(obj1, obj2);
223 }
224 #endif
225 void BarrierSetAssembler::obj_equals(MacroAssembler* masm,
226                                      Register obj1, Address obj2) {
227   __ cmpptr(obj1, obj2);
228 }
229 
230 void BarrierSetAssembler::obj_equals(MacroAssembler* masm,
231                                      Register obj1, Register obj2) {
232   __ cmpptr(obj1, obj2);
233 }
234 
235 void BarrierSetAssembler::try_resolve_jobject_in_native(MacroAssembler* masm, Register jni_env,
236                                                         Register obj, Register tmp, Label&amp; slowpath) {
237   __ clear_jweak_tag(obj);
238   __ movptr(obj, Address(obj, 0));
239 }
240 
241 void BarrierSetAssembler::tlab_allocate(MacroAssembler* masm,
242                                         Register thread, Register obj,
243                                         Register var_size_in_bytes,
244                                         int con_size_in_bytes,
245                                         Register t1,
246                                         Register t2,
247                                         Label&amp; slow_case) {
248   assert_different_registers(obj, t1, t2);
249   assert_different_registers(obj, var_size_in_bytes, t1);
250   Register end = t2;
251   if (!thread-&gt;is_valid()) {
252 #ifdef _LP64
253     thread = r15_thread;
254 #else
255     assert(t1-&gt;is_valid(), &quot;need temp reg&quot;);
256     thread = t1;
257     __ get_thread(thread);
258 #endif
259   }
260 
261   __ verify_tlab();
262 
263   __ movptr(obj, Address(thread, JavaThread::tlab_top_offset()));
264   if (var_size_in_bytes == noreg) {
265     __ lea(end, Address(obj, con_size_in_bytes));
266   } else {
267     __ lea(end, Address(obj, var_size_in_bytes, Address::times_1));
268   }
269   __ cmpptr(end, Address(thread, JavaThread::tlab_end_offset()));
270   __ jcc(Assembler::above, slow_case);
271 
272   // update the tlab top pointer
273   __ movptr(Address(thread, JavaThread::tlab_top_offset()), end);
274 
275   // recover var_size_in_bytes if necessary
276   if (var_size_in_bytes == end) {
277     __ subptr(var_size_in_bytes, obj);
278   }
279   __ verify_tlab();
280 }
281 
282 // Defines obj, preserves var_size_in_bytes
283 void BarrierSetAssembler::eden_allocate(MacroAssembler* masm,
284                                         Register thread, Register obj,
285                                         Register var_size_in_bytes,
286                                         int con_size_in_bytes,
287                                         Register t1,
288                                         Label&amp; slow_case) {
289   assert(obj == rax, &quot;obj must be in rax, for cmpxchg&quot;);
290   assert_different_registers(obj, var_size_in_bytes, t1);
291   if (!Universe::heap()-&gt;supports_inline_contig_alloc()) {
292     __ jmp(slow_case);
293   } else {
294     Register end = t1;
295     Label retry;
296     __ bind(retry);
297     ExternalAddress heap_top((address) Universe::heap()-&gt;top_addr());
298     __ movptr(obj, heap_top);
299     if (var_size_in_bytes == noreg) {
300       __ lea(end, Address(obj, con_size_in_bytes));
301     } else {
302       __ lea(end, Address(obj, var_size_in_bytes, Address::times_1));
303     }
304     // if end &lt; obj then we wrapped around =&gt; object too long =&gt; slow case
305     __ cmpptr(end, obj);
306     __ jcc(Assembler::below, slow_case);
307     __ cmpptr(end, ExternalAddress((address) Universe::heap()-&gt;end_addr()));
308     __ jcc(Assembler::above, slow_case);
309     // Compare obj with the top addr, and if still equal, store the new top addr in
310     // end at the address of the top addr pointer. Sets ZF if was equal, and clears
311     // it otherwise. Use lock prefix for atomicity on MPs.
312     __ locked_cmpxchgptr(end, heap_top);
313     __ jcc(Assembler::notEqual, retry);
314     incr_allocated_bytes(masm, thread, var_size_in_bytes, con_size_in_bytes, thread-&gt;is_valid() ? noreg : t1);
315   }
316 }
317 
318 void BarrierSetAssembler::incr_allocated_bytes(MacroAssembler* masm, Register thread,
319                                                Register var_size_in_bytes,
320                                                int con_size_in_bytes,
321                                                Register t1) {
322   if (!thread-&gt;is_valid()) {
323 #ifdef _LP64
324     thread = r15_thread;
325 #else
326     assert(t1-&gt;is_valid(), &quot;need temp reg&quot;);
327     thread = t1;
328     __ get_thread(thread);
329 #endif
330   }
331 
332 #ifdef _LP64
333   if (var_size_in_bytes-&gt;is_valid()) {
334     __ addq(Address(thread, in_bytes(JavaThread::allocated_bytes_offset())), var_size_in_bytes);
335   } else {
336     __ addq(Address(thread, in_bytes(JavaThread::allocated_bytes_offset())), con_size_in_bytes);
337   }
338 #else
339   if (var_size_in_bytes-&gt;is_valid()) {
340     __ addl(Address(thread, in_bytes(JavaThread::allocated_bytes_offset())), var_size_in_bytes);
341   } else {
342     __ addl(Address(thread, in_bytes(JavaThread::allocated_bytes_offset())), con_size_in_bytes);
343   }
344   __ adcl(Address(thread, in_bytes(JavaThread::allocated_bytes_offset())+4), 0);
345 #endif
346 }
347 
348 #ifdef _LP64
349 void BarrierSetAssembler::nmethod_entry_barrier(MacroAssembler* masm) {
350   BarrierSetNMethod* bs_nm = BarrierSet::barrier_set()-&gt;barrier_set_nmethod();
351   if (bs_nm == NULL) {
352     return;
353   }
354   Label continuation;
355   Register thread = r15_thread;
356   Address disarmed_addr(thread, in_bytes(bs_nm-&gt;thread_disarmed_offset()));
357   __ align(8);
358   __ cmpl(disarmed_addr, 0);
359   __ jcc(Assembler::equal, continuation);
360   __ call(RuntimeAddress(StubRoutines::x86::method_entry_barrier()));
361   __ bind(continuation);
362 }
363 #else
364 void BarrierSetAssembler::nmethod_entry_barrier(MacroAssembler* masm) {
365   BarrierSetNMethod* bs_nm = BarrierSet::barrier_set()-&gt;barrier_set_nmethod();
366   if (bs_nm == NULL) {
367     return;
368   }
369 
370   Label continuation;
371 
372   Register tmp = rdi;
373   __ push(tmp);
374   __ movptr(tmp, (intptr_t)bs_nm-&gt;disarmed_value_address());
375   Address disarmed_addr(tmp, 0);
376   __ align(4);
377   __ cmpl(disarmed_addr, 0);
378   __ pop(tmp);
379   __ jcc(Assembler::equal, continuation);
380   __ call(RuntimeAddress(StubRoutines::x86::method_entry_barrier()));
381   __ bind(continuation);
382 }
383 #endif
384 
385 void BarrierSetAssembler::c2i_entry_barrier(MacroAssembler* masm) {
386   BarrierSetNMethod* bs = BarrierSet::barrier_set()-&gt;barrier_set_nmethod();
387   if (bs == NULL) {
388     return;
389   }
390 
391   Label bad_call;
392   __ cmpptr(rbx, 0); // rbx contains the incoming method for c2i adapters.
393   __ jcc(Assembler::equal, bad_call);
394 
395 #ifdef _LP64
396   Register tmp1 = rscratch1;
397   Register tmp2 = rscratch2;
398 #else
399   Register tmp1 = rax;
400   Register tmp2 = rcx;
401   __ push(tmp1);
402   __ push(tmp2);
403 #endif // _LP64
404 
405   // Pointer chase to the method holder to find out if the method is concurrently unloading.
406   Label method_live;
407   __ load_method_holder_cld(tmp1, rbx);
408 
409    // Is it a strong CLD?
410   __ cmpl(Address(tmp1, ClassLoaderData::keep_alive_offset()), 0);
411   __ jcc(Assembler::greater, method_live);
412 
413    // Is it a weak but alive CLD?
414   __ movptr(tmp1, Address(tmp1, ClassLoaderData::holder_offset()));
415   __ resolve_weak_handle(tmp1, tmp2);
416   __ cmpptr(tmp1, 0);
417   __ jcc(Assembler::notEqual, method_live);
418 
419 #ifndef _LP64
420   __ pop(tmp2);
421   __ pop(tmp1);
422 #endif
423 
424   __ bind(bad_call);
425   __ jump(RuntimeAddress(SharedRuntime::get_handle_wrong_method_stub()));
426   __ bind(method_live);
427 
428 #ifndef _LP64
429   __ pop(tmp2);
430   __ pop(tmp1);
431 #endif
432 }
<a name="3" id="anc3"></a><b style="font-size: large; color: red">--- EOF ---</b>
















































































</pre>
<input id="eof" value="3" type="hidden" />
</body>
</html>