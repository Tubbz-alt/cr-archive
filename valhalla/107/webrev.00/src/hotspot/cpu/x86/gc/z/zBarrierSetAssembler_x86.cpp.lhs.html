<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Frames src/hotspot/cpu/x86/gc/z/zBarrierSetAssembler_x86.cpp</title>
    <link rel="stylesheet" href="../../../../../../style.css" />
    <script type="text/javascript" src="../../../../../../navigation.js"></script>
  </head>
<body onkeypress="keypress(event);">
<a name="0"></a>
<hr />
<pre>  1 /*
  2  * Copyright (c) 2018, 2020, Oracle and/or its affiliates. All rights reserved.
  3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  4  *
  5  * This code is free software; you can redistribute it and/or modify it
  6  * under the terms of the GNU General Public License version 2 only, as
  7  * published by the Free Software Foundation.
  8  *
  9  * This code is distributed in the hope that it will be useful, but WITHOUT
 10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
 11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
 12  * version 2 for more details (a copy is included in the LICENSE file that
 13  * accompanied this code).
 14  *
 15  * You should have received a copy of the GNU General Public License version
 16  * 2 along with this work; if not, write to the Free Software Foundation,
 17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 18  *
 19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
 20  * or visit www.oracle.com if you need additional information or have any
 21  * questions.
 22  */
 23 
 24 #include &quot;precompiled.hpp&quot;
 25 #include &quot;asm/macroAssembler.inline.hpp&quot;
 26 #include &quot;code/codeBlob.hpp&quot;
 27 #include &quot;code/vmreg.inline.hpp&quot;
 28 #include &quot;gc/z/zBarrier.inline.hpp&quot;
 29 #include &quot;gc/z/zBarrierSet.hpp&quot;
 30 #include &quot;gc/z/zBarrierSetAssembler.hpp&quot;
 31 #include &quot;gc/z/zBarrierSetRuntime.hpp&quot;
 32 #include &quot;memory/resourceArea.hpp&quot;
 33 #include &quot;runtime/sharedRuntime.hpp&quot;
 34 #include &quot;utilities/macros.hpp&quot;
 35 #ifdef COMPILER1
 36 #include &quot;c1/c1_LIRAssembler.hpp&quot;
 37 #include &quot;c1/c1_MacroAssembler.hpp&quot;
 38 #include &quot;gc/z/c1/zBarrierSetC1.hpp&quot;
 39 #endif // COMPILER1
 40 #ifdef COMPILER2
 41 #include &quot;gc/z/c2/zBarrierSetC2.hpp&quot;
 42 #endif // COMPILER2
 43 
 44 #ifdef PRODUCT
 45 #define BLOCK_COMMENT(str) /* nothing */
 46 #else
 47 #define BLOCK_COMMENT(str) __ block_comment(str)
 48 #endif
 49 
 50 #undef __
 51 #define __ masm-&gt;
 52 
 53 static void call_vm(MacroAssembler* masm,
 54                     address entry_point,
 55                     Register arg0,
 56                     Register arg1) {
 57   // Setup arguments
 58   if (arg1 == c_rarg0) {
 59     if (arg0 == c_rarg1) {
 60       __ xchgptr(c_rarg1, c_rarg0);
 61     } else {
 62       __ movptr(c_rarg1, arg1);
 63       __ movptr(c_rarg0, arg0);
 64     }
 65   } else {
 66     if (arg0 != c_rarg0) {
 67       __ movptr(c_rarg0, arg0);
 68     }
 69     if (arg1 != c_rarg1) {
 70       __ movptr(c_rarg1, arg1);
 71     }
 72   }
 73 
 74   // Call VM
 75   __ MacroAssembler::call_VM_leaf_base(entry_point, 2);
 76 }
 77 
 78 void ZBarrierSetAssembler::load_at(MacroAssembler* masm,
 79                                    DecoratorSet decorators,
 80                                    BasicType type,
 81                                    Register dst,
 82                                    Address src,
 83                                    Register tmp1,
 84                                    Register tmp_thread) {
 85   if (!ZBarrierSet::barrier_needed(decorators, type)) {
 86     // Barrier not needed
 87     BarrierSetAssembler::load_at(masm, decorators, type, dst, src, tmp1, tmp_thread);
 88     return;
 89   }
 90 
 91   BLOCK_COMMENT(&quot;ZBarrierSetAssembler::load_at {&quot;);
 92 
 93   // Allocate scratch register
 94   Register scratch = tmp1;
 95   if (tmp1 == noreg) {
 96     scratch = r12;
 97     __ push(scratch);
 98   }
 99 
100   assert_different_registers(dst, scratch);
101 
102   Label done;
103 
104   //
105   // Fast Path
106   //
107 
108   // Load address
109   __ lea(scratch, src);
110 
111   // Load oop at address
112   __ movptr(dst, Address(scratch, 0));
113 
114   // Test address bad mask
115   __ testptr(dst, address_bad_mask_from_thread(r15_thread));
116   __ jcc(Assembler::zero, done);
117 
118   //
119   // Slow path
120   //
121 
122   // Save registers
123   __ push(rax);
124   __ push(rcx);
125   __ push(rdx);
126   __ push(rdi);
127   __ push(rsi);
128   __ push(r8);
129   __ push(r9);
130   __ push(r10);
131   __ push(r11);
132 
133   // We may end up here from generate_native_wrapper, then the method may have
134   // floats as arguments, and we must spill them before calling the VM runtime
135   // leaf. From the interpreter all floats are passed on the stack.
136   assert(Argument::n_float_register_parameters_j == 8, &quot;Assumption&quot;);
137   const int xmm_size = wordSize * 2;
138   const int xmm_spill_size = xmm_size * Argument::n_float_register_parameters_j;
139   __ subptr(rsp, xmm_spill_size);
140   __ movdqu(Address(rsp, xmm_size * 7), xmm7);
141   __ movdqu(Address(rsp, xmm_size * 6), xmm6);
142   __ movdqu(Address(rsp, xmm_size * 5), xmm5);
143   __ movdqu(Address(rsp, xmm_size * 4), xmm4);
144   __ movdqu(Address(rsp, xmm_size * 3), xmm3);
145   __ movdqu(Address(rsp, xmm_size * 2), xmm2);
146   __ movdqu(Address(rsp, xmm_size * 1), xmm1);
147   __ movdqu(Address(rsp, xmm_size * 0), xmm0);
148 
149   // Call VM
150   call_vm(masm, ZBarrierSetRuntime::load_barrier_on_oop_field_preloaded_addr(decorators), dst, scratch);
151 
152   // Restore registers
153   __ movdqu(xmm0, Address(rsp, xmm_size * 0));
154   __ movdqu(xmm1, Address(rsp, xmm_size * 1));
155   __ movdqu(xmm2, Address(rsp, xmm_size * 2));
156   __ movdqu(xmm3, Address(rsp, xmm_size * 3));
157   __ movdqu(xmm4, Address(rsp, xmm_size * 4));
158   __ movdqu(xmm5, Address(rsp, xmm_size * 5));
159   __ movdqu(xmm6, Address(rsp, xmm_size * 6));
160   __ movdqu(xmm7, Address(rsp, xmm_size * 7));
161   __ addptr(rsp, xmm_spill_size);
162 
163   __ pop(r11);
164   __ pop(r10);
165   __ pop(r9);
166   __ pop(r8);
167   __ pop(rsi);
168   __ pop(rdi);
169   __ pop(rdx);
170   __ pop(rcx);
171 
172   if (dst == rax) {
173     __ addptr(rsp, wordSize);
174   } else {
175     __ movptr(dst, rax);
176     __ pop(rax);
177   }
178 
179   __ bind(done);
180 
181   // Restore scratch register
182   if (tmp1 == noreg) {
183     __ pop(scratch);
184   }
185 
186   BLOCK_COMMENT(&quot;} ZBarrierSetAssembler::load_at&quot;);
187 }
188 
189 #ifdef ASSERT
190 
191 void ZBarrierSetAssembler::store_at(MacroAssembler* masm,
192                                     DecoratorSet decorators,
193                                     BasicType type,
194                                     Address dst,
195                                     Register src,
196                                     Register tmp1,
197                                     Register tmp2,
198                                     Register tmp3) {
199   BLOCK_COMMENT(&quot;ZBarrierSetAssembler::store_at {&quot;);
200 
<a name="1" id="anc1"></a><span class="line-modified">201   assert(type != T_VALUETYPE, &quot;Not supported yet&quot;);</span>
202   // Verify oop store
203   if (is_reference_type(type)) {
204     // Note that src could be noreg, which means we
205     // are storing null and can skip verification.
206     if (src != noreg) {
207       Label done;
208       __ testptr(src, address_bad_mask_from_thread(r15_thread));
209       __ jcc(Assembler::zero, done);
210       __ stop(&quot;Verify oop store failed&quot;);
211       __ should_not_reach_here();
212       __ bind(done);
213     }
214   }
215 
216   // Store value
217   BarrierSetAssembler::store_at(masm, decorators, type, dst, src, tmp1, tmp2, tmp3);
218 
219   BLOCK_COMMENT(&quot;} ZBarrierSetAssembler::store_at&quot;);
220 }
221 
222 #endif // ASSERT
223 
224 void ZBarrierSetAssembler::arraycopy_prologue(MacroAssembler* masm,
225                                               DecoratorSet decorators,
226                                               BasicType type,
227                                               Register src,
228                                               Register dst,
229                                               Register count) {
230   if (!ZBarrierSet::barrier_needed(decorators, type)) {
231     // Barrier not needed
232     return;
233   }
234 
235   BLOCK_COMMENT(&quot;ZBarrierSetAssembler::arraycopy_prologue {&quot;);
236 
237   // Save registers
238   __ pusha();
239 
240   // Call VM
241   call_vm(masm, ZBarrierSetRuntime::load_barrier_on_oop_array_addr(), src, count);
242 
243   // Restore registers
244   __ popa();
245 
246   BLOCK_COMMENT(&quot;} ZBarrierSetAssembler::arraycopy_prologue&quot;);
247 }
248 
249 void ZBarrierSetAssembler::try_resolve_jobject_in_native(MacroAssembler* masm,
250                                                          Register jni_env,
251                                                          Register obj,
252                                                          Register tmp,
253                                                          Label&amp; slowpath) {
254   BLOCK_COMMENT(&quot;ZBarrierSetAssembler::try_resolve_jobject_in_native {&quot;);
255 
256   // Resolve jobject
257   BarrierSetAssembler::try_resolve_jobject_in_native(masm, jni_env, obj, tmp, slowpath);
258 
259   // Test address bad mask
260   __ testptr(obj, address_bad_mask_from_jni_env(jni_env));
261   __ jcc(Assembler::notZero, slowpath);
262 
263   BLOCK_COMMENT(&quot;} ZBarrierSetAssembler::try_resolve_jobject_in_native&quot;);
264 }
265 
266 #ifdef COMPILER1
267 
268 #undef __
269 #define __ ce-&gt;masm()-&gt;
270 
271 void ZBarrierSetAssembler::generate_c1_load_barrier_test(LIR_Assembler* ce,
272                                                          LIR_Opr ref) const {
273   __ testptr(ref-&gt;as_register(), address_bad_mask_from_thread(r15_thread));
274 }
275 
276 void ZBarrierSetAssembler::generate_c1_load_barrier_stub(LIR_Assembler* ce,
277                                                          ZLoadBarrierStubC1* stub) const {
278   // Stub entry
279   __ bind(*stub-&gt;entry());
280 
281   Register ref = stub-&gt;ref()-&gt;as_register();
282   Register ref_addr = noreg;
283   Register tmp = noreg;
284 
285   if (stub-&gt;tmp()-&gt;is_valid()) {
286     // Load address into tmp register
287     ce-&gt;leal(stub-&gt;ref_addr(), stub-&gt;tmp());
288     ref_addr = tmp = stub-&gt;tmp()-&gt;as_pointer_register();
289   } else {
290     // Address already in register
291     ref_addr = stub-&gt;ref_addr()-&gt;as_address_ptr()-&gt;base()-&gt;as_pointer_register();
292   }
293 
294   assert_different_registers(ref, ref_addr, noreg);
295 
296   // Save rax unless it is the result or tmp register
297   if (ref != rax &amp;&amp; tmp != rax) {
298     __ push(rax);
299   }
300 
301   // Setup arguments and call runtime stub
302   __ subptr(rsp, 2 * BytesPerWord);
303   ce-&gt;store_parameter(ref_addr, 1);
304   ce-&gt;store_parameter(ref, 0);
305   __ call(RuntimeAddress(stub-&gt;runtime_stub()));
306   __ addptr(rsp, 2 * BytesPerWord);
307 
308   // Verify result
309   __ verify_oop(rax);
310 
311   // Move result into place
312   if (ref != rax) {
313     __ movptr(ref, rax);
314   }
315 
316   // Restore rax unless it is the result or tmp register
317   if (ref != rax &amp;&amp; tmp != rax) {
318     __ pop(rax);
319   }
320 
321   // Stub exit
322   __ jmp(*stub-&gt;continuation());
323 }
324 
325 #undef __
326 #define __ sasm-&gt;
327 
328 void ZBarrierSetAssembler::generate_c1_load_barrier_runtime_stub(StubAssembler* sasm,
329                                                                  DecoratorSet decorators) const {
330   // Enter and save registers
331   __ enter();
332   __ save_live_registers_no_oop_map(true /* save_fpu_registers */);
333 
334   // Setup arguments
335   __ load_parameter(1, c_rarg1);
336   __ load_parameter(0, c_rarg0);
337 
338   // Call VM
339   __ call_VM_leaf(ZBarrierSetRuntime::load_barrier_on_oop_field_preloaded_addr(decorators), c_rarg0, c_rarg1);
340 
341   // Restore registers and return
342   __ restore_live_registers_except_rax(true /* restore_fpu_registers */);
343   __ leave();
344   __ ret(0);
345 }
346 
347 #endif // COMPILER1
348 
349 #ifdef COMPILER2
350 
351 OptoReg::Name ZBarrierSetAssembler::refine_register(const Node* node, OptoReg::Name opto_reg) {
352   if (!OptoReg::is_reg(opto_reg)) {
353     return OptoReg::Bad;
354   }
355 
356   const VMReg vm_reg = OptoReg::as_VMReg(opto_reg);
357   if (vm_reg-&gt;is_XMMRegister()) {
358     opto_reg &amp;= ~15;
359     switch (node-&gt;ideal_reg()) {
360       case Op_VecX:
361         opto_reg |= 2;
362         break;
363       case Op_VecY:
364         opto_reg |= 4;
365         break;
366       case Op_VecZ:
367         opto_reg |= 8;
368         break;
369       default:
370         opto_reg |= 1;
371         break;
372     }
373   }
374 
375   return opto_reg;
376 }
377 
378 // We use the vec_spill_helper from the x86.ad file to avoid reinventing this wheel
379 extern int vec_spill_helper(CodeBuffer *cbuf, bool do_size, bool is_load,
380                             int stack_offset, int reg, uint ireg, outputStream* st);
381 
382 #undef __
383 #define __ _masm-&gt;
384 
385 class ZSaveLiveRegisters {
386 private:
387   struct XMMRegisterData {
388     XMMRegister _reg;
389     int         _size;
390 
391     // Used by GrowableArray::find()
392     bool operator == (const XMMRegisterData&amp; other) {
393       return _reg == other._reg;
394     }
395   };
396 
397   MacroAssembler* const          _masm;
398   GrowableArray&lt;Register&gt;        _gp_registers;
399   GrowableArray&lt;XMMRegisterData&gt; _xmm_registers;
400   int                            _spill_size;
401   int                            _spill_offset;
402 
403   static int xmm_compare_register_size(XMMRegisterData* left, XMMRegisterData* right) {
404     if (left-&gt;_size == right-&gt;_size) {
405       return 0;
406     }
407 
408     return (left-&gt;_size &lt; right-&gt;_size) ? -1 : 1;
409   }
410 
411   static int xmm_slot_size(OptoReg::Name opto_reg) {
412     // The low order 4 bytes denote what size of the XMM register is live
413     return (opto_reg &amp; 15) &lt;&lt; 3;
414   }
415 
416   static uint xmm_ideal_reg_for_size(int reg_size) {
417     switch (reg_size) {
418     case 8:
419       return Op_VecD;
420     case 16:
421       return Op_VecX;
422     case 32:
423       return Op_VecY;
424     case 64:
425       return Op_VecZ;
426     default:
427       fatal(&quot;Invalid register size %d&quot;, reg_size);
428       return 0;
429     }
430   }
431 
432   bool xmm_needs_vzeroupper() const {
433     return _xmm_registers.is_nonempty() &amp;&amp; _xmm_registers.at(0)._size &gt; 16;
434   }
435 
436   void xmm_register_save(const XMMRegisterData&amp; reg_data) {
437     const OptoReg::Name opto_reg = OptoReg::as_OptoReg(reg_data._reg-&gt;as_VMReg());
438     const uint ideal_reg = xmm_ideal_reg_for_size(reg_data._size);
439     _spill_offset -= reg_data._size;
440     vec_spill_helper(__ code(), false /* do_size */, false /* is_load */, _spill_offset, opto_reg, ideal_reg, tty);
441   }
442 
443   void xmm_register_restore(const XMMRegisterData&amp; reg_data) {
444     const OptoReg::Name opto_reg = OptoReg::as_OptoReg(reg_data._reg-&gt;as_VMReg());
445     const uint ideal_reg = xmm_ideal_reg_for_size(reg_data._size);
446     vec_spill_helper(__ code(), false /* do_size */, true /* is_load */, _spill_offset, opto_reg, ideal_reg, tty);
447     _spill_offset += reg_data._size;
448   }
449 
450   void gp_register_save(Register reg) {
451     _spill_offset -= 8;
452     __ movq(Address(rsp, _spill_offset), reg);
453   }
454 
455   void gp_register_restore(Register reg) {
456     __ movq(reg, Address(rsp, _spill_offset));
457     _spill_offset += 8;
458   }
459 
460   void initialize(ZLoadBarrierStubC2* stub) {
461     // Create mask of caller saved registers that need to
462     // be saved/restored if live
463     RegMask caller_saved;
464     caller_saved.Insert(OptoReg::as_OptoReg(rax-&gt;as_VMReg()));
465     caller_saved.Insert(OptoReg::as_OptoReg(rcx-&gt;as_VMReg()));
466     caller_saved.Insert(OptoReg::as_OptoReg(rdx-&gt;as_VMReg()));
467     caller_saved.Insert(OptoReg::as_OptoReg(rsi-&gt;as_VMReg()));
468     caller_saved.Insert(OptoReg::as_OptoReg(rdi-&gt;as_VMReg()));
469     caller_saved.Insert(OptoReg::as_OptoReg(r8-&gt;as_VMReg()));
470     caller_saved.Insert(OptoReg::as_OptoReg(r9-&gt;as_VMReg()));
471     caller_saved.Insert(OptoReg::as_OptoReg(r10-&gt;as_VMReg()));
472     caller_saved.Insert(OptoReg::as_OptoReg(r11-&gt;as_VMReg()));
473     caller_saved.Remove(OptoReg::as_OptoReg(stub-&gt;ref()-&gt;as_VMReg()));
474 
475     // Create mask of live registers
476     RegMask live = stub-&gt;live();
477     if (stub-&gt;tmp() != noreg) {
478       live.Insert(OptoReg::as_OptoReg(stub-&gt;tmp()-&gt;as_VMReg()));
479     }
480 
481     int gp_spill_size = 0;
482     int xmm_spill_size = 0;
483 
484     // Record registers that needs to be saved/restored
485     while (live.is_NotEmpty()) {
486       const OptoReg::Name opto_reg = live.find_first_elem();
487       const VMReg vm_reg = OptoReg::as_VMReg(opto_reg);
488 
489       live.Remove(opto_reg);
490 
491       if (vm_reg-&gt;is_Register()) {
492         if (caller_saved.Member(opto_reg)) {
493           _gp_registers.append(vm_reg-&gt;as_Register());
494           gp_spill_size += 8;
495         }
496       } else if (vm_reg-&gt;is_XMMRegister()) {
497         // We encode in the low order 4 bits of the opto_reg, how large part of the register is live
498         const VMReg vm_reg_base = OptoReg::as_VMReg(opto_reg &amp; ~15);
499         const int reg_size = xmm_slot_size(opto_reg);
500         const XMMRegisterData reg_data = { vm_reg_base-&gt;as_XMMRegister(), reg_size };
501         const int reg_index = _xmm_registers.find(reg_data);
502         if (reg_index == -1) {
503           // Not previously appended
504           _xmm_registers.append(reg_data);
505           xmm_spill_size += reg_size;
506         } else {
507           // Previously appended, update size
508           const int reg_size_prev = _xmm_registers.at(reg_index)._size;
509           if (reg_size &gt; reg_size_prev) {
510             _xmm_registers.at_put(reg_index, reg_data);
511             xmm_spill_size += reg_size - reg_size_prev;
512           }
513         }
514       } else {
515         fatal(&quot;Unexpected register type&quot;);
516       }
517     }
518 
519     // Sort by size, largest first
520     _xmm_registers.sort(xmm_compare_register_size);
521 
522     // On Windows, the caller reserves stack space for spilling register arguments
523     const int arg_spill_size = frame::arg_reg_save_area_bytes;
524 
525     // Stack pointer must be 16 bytes aligned for the call
526     _spill_offset = _spill_size = align_up(xmm_spill_size + gp_spill_size + arg_spill_size, 16);
527   }
528 
529 public:
530   ZSaveLiveRegisters(MacroAssembler* masm, ZLoadBarrierStubC2* stub) :
531       _masm(masm),
532       _gp_registers(),
533       _xmm_registers(),
534       _spill_size(0),
535       _spill_offset(0) {
536 
537     //
538     // Stack layout after registers have been spilled:
539     //
540     // | ...            | original rsp, 16 bytes aligned
541     // ------------------
542     // | zmm0 high      |
543     // | ...            |
544     // | zmm0 low       | 16 bytes aligned
545     // | ...            |
546     // | ymm1 high      |
547     // | ...            |
548     // | ymm1 low       | 16 bytes aligned
549     // | ...            |
550     // | xmmN high      |
551     // | ...            |
552     // | xmmN low       | 8 bytes aligned
553     // | reg0           | 8 bytes aligned
554     // | reg1           |
555     // | ...            |
556     // | regN           | new rsp, if 16 bytes aligned
557     // | &lt;padding&gt;      | else new rsp, 16 bytes aligned
558     // ------------------
559     //
560 
561     // Figure out what registers to save/restore
562     initialize(stub);
563 
564     // Allocate stack space
565     if (_spill_size &gt; 0) {
566       __ subptr(rsp, _spill_size);
567     }
568 
569     // Save XMM/YMM/ZMM registers
570     for (int i = 0; i &lt; _xmm_registers.length(); i++) {
571       xmm_register_save(_xmm_registers.at(i));
572     }
573 
574     if (xmm_needs_vzeroupper()) {
575       __ vzeroupper();
576     }
577 
578     // Save general purpose registers
579     for (int i = 0; i &lt; _gp_registers.length(); i++) {
580       gp_register_save(_gp_registers.at(i));
581     }
582   }
583 
584   ~ZSaveLiveRegisters() {
585     // Restore general purpose registers
586     for (int i = _gp_registers.length() - 1; i &gt;= 0; i--) {
587       gp_register_restore(_gp_registers.at(i));
588     }
589 
590     __ vzeroupper();
591 
592     // Restore XMM/YMM/ZMM registers
593     for (int i = _xmm_registers.length() - 1; i &gt;= 0; i--) {
594       xmm_register_restore(_xmm_registers.at(i));
595     }
596 
597     // Free stack space
598     if (_spill_size &gt; 0) {
599       __ addptr(rsp, _spill_size);
600     }
601   }
602 };
603 
604 class ZSetupArguments {
605 private:
606   MacroAssembler* const _masm;
607   const Register        _ref;
608   const Address         _ref_addr;
609 
610 public:
611   ZSetupArguments(MacroAssembler* masm, ZLoadBarrierStubC2* stub) :
612       _masm(masm),
613       _ref(stub-&gt;ref()),
614       _ref_addr(stub-&gt;ref_addr()) {
615 
616     // Setup arguments
617     if (_ref_addr.base() == noreg) {
618       // No self healing
619       if (_ref != c_rarg0) {
620         __ movq(c_rarg0, _ref);
621       }
622       __ xorq(c_rarg1, c_rarg1);
623     } else {
624       // Self healing
625       if (_ref == c_rarg0) {
626         __ lea(c_rarg1, _ref_addr);
627       } else if (_ref != c_rarg1) {
628         __ lea(c_rarg1, _ref_addr);
629         __ movq(c_rarg0, _ref);
630       } else if (_ref_addr.base() != c_rarg0 &amp;&amp; _ref_addr.index() != c_rarg0) {
631         __ movq(c_rarg0, _ref);
632         __ lea(c_rarg1, _ref_addr);
633       } else {
634         __ xchgq(c_rarg0, c_rarg1);
635         if (_ref_addr.base() == c_rarg0) {
636           __ lea(c_rarg1, Address(c_rarg1, _ref_addr.index(), _ref_addr.scale(), _ref_addr.disp()));
637         } else if (_ref_addr.index() == c_rarg0) {
638           __ lea(c_rarg1, Address(_ref_addr.base(), c_rarg1, _ref_addr.scale(), _ref_addr.disp()));
639         } else {
640           ShouldNotReachHere();
641         }
642       }
643     }
644   }
645 
646   ~ZSetupArguments() {
647     // Transfer result
648     if (_ref != rax) {
649       __ movq(_ref, rax);
650     }
651   }
652 };
653 
654 #undef __
655 #define __ masm-&gt;
656 
657 void ZBarrierSetAssembler::generate_c2_load_barrier_stub(MacroAssembler* masm, ZLoadBarrierStubC2* stub) const {
658   BLOCK_COMMENT(&quot;ZLoadBarrierStubC2&quot;);
659 
660   // Stub entry
661   __ bind(*stub-&gt;entry());
662 
663   {
664     ZSaveLiveRegisters save_live_registers(masm, stub);
665     ZSetupArguments setup_arguments(masm, stub);
666     __ call(RuntimeAddress(stub-&gt;slow_path()));
667   }
668 
669   // Stub exit
670   __ jmp(*stub-&gt;continuation());
671 }
672 
673 #undef __
674 
675 #endif // COMPILER2
<a name="2" id="anc2"></a><b style="font-size: large; color: red">--- EOF ---</b>
















































































</pre>
<input id="eof" value="2" type="hidden" />
</body>
</html>