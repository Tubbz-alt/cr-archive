<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>New src/hotspot/share/prims/jvmtiRedefineClasses.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
  <body>
    <pre>
   1 /*
   2  * Copyright (c) 2003, 2020, Oracle and/or its affiliates. All rights reserved.
   3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   4  *
   5  * This code is free software; you can redistribute it and/or modify it
   6  * under the terms of the GNU General Public License version 2 only, as
   7  * published by the Free Software Foundation.
   8  *
   9  * This code is distributed in the hope that it will be useful, but WITHOUT
  10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #include &quot;precompiled.hpp&quot;
  26 #include &quot;aot/aotLoader.hpp&quot;
  27 #include &quot;classfile/classLoaderDataGraph.hpp&quot;
  28 #include &quot;classfile/classFileStream.hpp&quot;
  29 #include &quot;classfile/javaClasses.inline.hpp&quot;
  30 #include &quot;classfile/metadataOnStackMark.hpp&quot;
  31 #include &quot;classfile/symbolTable.hpp&quot;
  32 #include &quot;classfile/systemDictionary.hpp&quot;
  33 #include &quot;classfile/verifier.hpp&quot;
  34 #include &quot;code/codeCache.hpp&quot;
  35 #include &quot;compiler/compileBroker.hpp&quot;
  36 #include &quot;interpreter/oopMapCache.hpp&quot;
  37 #include &quot;interpreter/rewriter.hpp&quot;
  38 #include &quot;jfr/jfrEvents.hpp&quot;
  39 #include &quot;logging/logStream.hpp&quot;
  40 #include &quot;memory/metadataFactory.hpp&quot;
  41 #include &quot;memory/metaspaceShared.hpp&quot;
  42 #include &quot;memory/resourceArea.hpp&quot;
  43 #include &quot;memory/universe.hpp&quot;
  44 #include &quot;oops/annotations.hpp&quot;
  45 #include &quot;oops/constantPool.hpp&quot;
  46 #include &quot;oops/fieldStreams.inline.hpp&quot;
  47 #include &quot;oops/klassVtable.hpp&quot;
  48 #include &quot;oops/oop.inline.hpp&quot;
  49 #include &quot;oops/recordComponent.hpp&quot;
  50 #include &quot;prims/jvmtiImpl.hpp&quot;
  51 #include &quot;prims/jvmtiRedefineClasses.hpp&quot;
  52 #include &quot;prims/jvmtiThreadState.inline.hpp&quot;
  53 #include &quot;prims/resolvedMethodTable.hpp&quot;
  54 #include &quot;prims/methodComparator.hpp&quot;
  55 #include &quot;runtime/atomic.hpp&quot;
  56 #include &quot;runtime/deoptimization.hpp&quot;
  57 #include &quot;runtime/handles.inline.hpp&quot;
  58 #include &quot;runtime/jniHandles.inline.hpp&quot;
  59 #include &quot;runtime/relocator.hpp&quot;
  60 #include &quot;runtime/safepointVerifiers.hpp&quot;
  61 #include &quot;utilities/bitMap.inline.hpp&quot;
  62 #include &quot;utilities/events.hpp&quot;
  63 
  64 Array&lt;Method*&gt;* VM_RedefineClasses::_old_methods = NULL;
  65 Array&lt;Method*&gt;* VM_RedefineClasses::_new_methods = NULL;
  66 Method**  VM_RedefineClasses::_matching_old_methods = NULL;
  67 Method**  VM_RedefineClasses::_matching_new_methods = NULL;
  68 Method**  VM_RedefineClasses::_deleted_methods      = NULL;
  69 Method**  VM_RedefineClasses::_added_methods        = NULL;
  70 int       VM_RedefineClasses::_matching_methods_length = 0;
  71 int       VM_RedefineClasses::_deleted_methods_length  = 0;
  72 int       VM_RedefineClasses::_added_methods_length    = 0;
  73 
  74 // This flag is global as the constructor does not reset it:
  75 bool      VM_RedefineClasses::_has_redefined_Object = false;
  76 u8        VM_RedefineClasses::_id_counter = 0;
  77 
  78 VM_RedefineClasses::VM_RedefineClasses(jint class_count,
  79                                        const jvmtiClassDefinition *class_defs,
  80                                        JvmtiClassLoadKind class_load_kind) {
  81   _class_count = class_count;
  82   _class_defs = class_defs;
  83   _class_load_kind = class_load_kind;
  84   _any_class_has_resolved_methods = false;
  85   _res = JVMTI_ERROR_NONE;
  86   _the_class = NULL;
  87   _id = next_id();
  88 }
  89 
  90 static inline InstanceKlass* get_ik(jclass def) {
  91   oop mirror = JNIHandles::resolve_non_null(def);
  92   return InstanceKlass::cast(java_lang_Class::as_Klass(mirror));
  93 }
  94 
  95 // If any of the classes are being redefined, wait
  96 // Parallel constant pool merging leads to indeterminate constant pools.
  97 void VM_RedefineClasses::lock_classes() {
  98   MonitorLocker ml(RedefineClasses_lock);
  99   bool has_redefined;
 100   do {
 101     has_redefined = false;
 102     // Go through classes each time until none are being redefined.
 103     for (int i = 0; i &lt; _class_count; i++) {
 104       if (get_ik(_class_defs[i].klass)-&gt;is_being_redefined()) {
 105         ml.wait();
 106         has_redefined = true;
 107         break;  // for loop
 108       }
 109     }
 110   } while (has_redefined);
 111   for (int i = 0; i &lt; _class_count; i++) {
 112     get_ik(_class_defs[i].klass)-&gt;set_is_being_redefined(true);
 113   }
 114   ml.notify_all();
 115 }
 116 
 117 void VM_RedefineClasses::unlock_classes() {
 118   MonitorLocker ml(RedefineClasses_lock);
 119   for (int i = 0; i &lt; _class_count; i++) {
 120     assert(get_ik(_class_defs[i].klass)-&gt;is_being_redefined(),
 121            &quot;should be being redefined to get here&quot;);
 122     get_ik(_class_defs[i].klass)-&gt;set_is_being_redefined(false);
 123   }
 124   ml.notify_all();
 125 }
 126 
 127 bool VM_RedefineClasses::doit_prologue() {
 128   if (_class_count == 0) {
 129     _res = JVMTI_ERROR_NONE;
 130     return false;
 131   }
 132   if (_class_defs == NULL) {
 133     _res = JVMTI_ERROR_NULL_POINTER;
 134     return false;
 135   }
 136 
 137   for (int i = 0; i &lt; _class_count; i++) {
 138     if (_class_defs[i].klass == NULL) {
 139       _res = JVMTI_ERROR_INVALID_CLASS;
 140       return false;
 141     }
 142     if (_class_defs[i].class_byte_count == 0) {
 143       _res = JVMTI_ERROR_INVALID_CLASS_FORMAT;
 144       return false;
 145     }
 146     if (_class_defs[i].class_bytes == NULL) {
 147       _res = JVMTI_ERROR_NULL_POINTER;
 148       return false;
 149     }
 150 
 151     oop mirror = JNIHandles::resolve_non_null(_class_defs[i].klass);
 152     // classes for primitives, arrays, hidden and vm unsafe anonymous classes
 153     // cannot be redefined.
 154     if (!is_modifiable_class(mirror)) {
 155       _res = JVMTI_ERROR_UNMODIFIABLE_CLASS;
 156       return false;
 157     }
 158   }
 159 
 160   // Start timer after all the sanity checks; not quite accurate, but
 161   // better than adding a bunch of stop() calls.
 162   if (log_is_enabled(Info, redefine, class, timer)) {
 163     _timer_vm_op_prologue.start();
 164   }
 165 
 166   lock_classes();
 167   // We first load new class versions in the prologue, because somewhere down the
 168   // call chain it is required that the current thread is a Java thread.
 169   _res = load_new_class_versions(Thread::current());
 170   if (_res != JVMTI_ERROR_NONE) {
 171     // free any successfully created classes, since none are redefined
 172     for (int i = 0; i &lt; _class_count; i++) {
 173       if (_scratch_classes[i] != NULL) {
 174         ClassLoaderData* cld = _scratch_classes[i]-&gt;class_loader_data();
 175         // Free the memory for this class at class unloading time.  Not before
 176         // because CMS might think this is still live.
 177         InstanceKlass* ik = get_ik(_class_defs[i].klass);
 178         if (ik-&gt;get_cached_class_file() == _scratch_classes[i]-&gt;get_cached_class_file()) {
 179           // Don&#39;t double-free cached_class_file copied from the original class if error.
 180           _scratch_classes[i]-&gt;set_cached_class_file(NULL);
 181         }
 182         cld-&gt;add_to_deallocate_list(InstanceKlass::cast(_scratch_classes[i]));
 183       }
 184     }
 185     // Free os::malloc allocated memory in load_new_class_version.
 186     os::free(_scratch_classes);
 187     _timer_vm_op_prologue.stop();
 188     unlock_classes();
 189     return false;
 190   }
 191 
 192   _timer_vm_op_prologue.stop();
 193   return true;
 194 }
 195 
 196 void VM_RedefineClasses::doit() {
 197   Thread *thread = Thread::current();
 198 
 199 #if INCLUDE_CDS
 200   if (UseSharedSpaces) {
 201     // Sharing is enabled so we remap the shared readonly space to
 202     // shared readwrite, private just in case we need to redefine
 203     // a shared class. We do the remap during the doit() phase of
 204     // the safepoint to be safer.
 205     if (!MetaspaceShared::remap_shared_readonly_as_readwrite()) {
 206       log_info(redefine, class, load)(&quot;failed to remap shared readonly space to readwrite, private&quot;);
 207       _res = JVMTI_ERROR_INTERNAL;
 208       return;
 209     }
 210   }
 211 #endif
 212 
 213   // Mark methods seen on stack and everywhere else so old methods are not
 214   // cleaned up if they&#39;re on the stack.
 215   MetadataOnStackMark md_on_stack(/*walk_all_metadata*/true, /*redefinition_walk*/true);
 216   HandleMark hm(thread);   // make sure any handles created are deleted
 217                            // before the stack walk again.
 218 
 219   for (int i = 0; i &lt; _class_count; i++) {
 220     redefine_single_class(_class_defs[i].klass, _scratch_classes[i], thread);
 221   }
 222 
 223   // Flush all compiled code that depends on the classes redefined.
 224   flush_dependent_code();
 225 
 226   // Adjust constantpool caches and vtables for all classes
 227   // that reference methods of the evolved classes.
 228   // Have to do this after all classes are redefined and all methods that
 229   // are redefined are marked as old.
 230   AdjustAndCleanMetadata adjust_and_clean_metadata(thread);
 231   ClassLoaderDataGraph::classes_do(&amp;adjust_and_clean_metadata);
 232 
 233   // JSR-292 support
 234   if (_any_class_has_resolved_methods) {
 235     bool trace_name_printed = false;
 236     ResolvedMethodTable::adjust_method_entries(&amp;trace_name_printed);
 237   }
 238 
 239   // Increment flag indicating that some invariants are no longer true.
 240   // See jvmtiExport.hpp for detailed explanation.
 241   JvmtiExport::increment_redefinition_count();
 242 
 243   // check_class() is optionally called for product bits, but is
 244   // always called for non-product bits.
 245 #ifdef PRODUCT
 246   if (log_is_enabled(Trace, redefine, class, obsolete, metadata)) {
 247 #endif
 248     log_trace(redefine, class, obsolete, metadata)(&quot;calling check_class&quot;);
 249     CheckClass check_class(thread);
 250     ClassLoaderDataGraph::classes_do(&amp;check_class);
 251 #ifdef PRODUCT
 252   }
 253 #endif
 254 
 255   // Clean up any metadata now unreferenced while MetadataOnStackMark is set.
 256   ClassLoaderDataGraph::clean_deallocate_lists(false);
 257 }
 258 
 259 void VM_RedefineClasses::doit_epilogue() {
 260   unlock_classes();
 261 
 262   // Free os::malloc allocated memory.
 263   os::free(_scratch_classes);
 264 
 265   // Reset the_class to null for error printing.
 266   _the_class = NULL;
 267 
 268   if (log_is_enabled(Info, redefine, class, timer)) {
 269     // Used to have separate timers for &quot;doit&quot; and &quot;all&quot;, but the timer
 270     // overhead skewed the measurements.
 271     julong doit_time = _timer_rsc_phase1.milliseconds() +
 272                        _timer_rsc_phase2.milliseconds();
 273     julong all_time = _timer_vm_op_prologue.milliseconds() + doit_time;
 274 
 275     log_info(redefine, class, timer)
 276       (&quot;vm_op: all=&quot; JULONG_FORMAT &quot;  prologue=&quot; JULONG_FORMAT &quot;  doit=&quot; JULONG_FORMAT,
 277        all_time, (julong)_timer_vm_op_prologue.milliseconds(), doit_time);
 278     log_info(redefine, class, timer)
 279       (&quot;redefine_single_class: phase1=&quot; JULONG_FORMAT &quot;  phase2=&quot; JULONG_FORMAT,
 280        (julong)_timer_rsc_phase1.milliseconds(), (julong)_timer_rsc_phase2.milliseconds());
 281   }
 282 }
 283 
 284 bool VM_RedefineClasses::is_modifiable_class(oop klass_mirror) {
 285   // classes for primitives cannot be redefined
 286   if (java_lang_Class::is_primitive(klass_mirror)) {
 287     return false;
 288   }
 289   Klass* k = java_lang_Class::as_Klass(klass_mirror);
 290   // classes for arrays cannot be redefined
 291   if (k == NULL || !k-&gt;is_instance_klass()) {
 292     return false;
 293   }
 294 
 295   // Cannot redefine or retransform interface java.lang.IdentityObject.
 296   if (k-&gt;name() == vmSymbols::java_lang_IdentityObject()) {
 297     return false;
 298   }
 299 
 300   // Cannot redefine or retransform a hidden or an unsafe anonymous class.
 301   if (InstanceKlass::cast(k)-&gt;is_hidden() ||
 302       InstanceKlass::cast(k)-&gt;is_unsafe_anonymous()) {
 303     return false;
 304   }
 305   return true;
 306 }
 307 
 308 // Append the current entry at scratch_i in scratch_cp to *merge_cp_p
 309 // where the end of *merge_cp_p is specified by *merge_cp_length_p. For
 310 // direct CP entries, there is just the current entry to append. For
 311 // indirect and double-indirect CP entries, there are zero or more
 312 // referenced CP entries along with the current entry to append.
 313 // Indirect and double-indirect CP entries are handled by recursive
 314 // calls to append_entry() as needed. The referenced CP entries are
 315 // always appended to *merge_cp_p before the referee CP entry. These
 316 // referenced CP entries may already exist in *merge_cp_p in which case
 317 // there is nothing extra to append and only the current entry is
 318 // appended.
 319 void VM_RedefineClasses::append_entry(const constantPoolHandle&amp; scratch_cp,
 320        int scratch_i, constantPoolHandle *merge_cp_p, int *merge_cp_length_p,
 321        TRAPS) {
 322 
 323   // append is different depending on entry tag type
 324   switch (scratch_cp-&gt;tag_at(scratch_i).value()) {
 325 
 326     // The old verifier is implemented outside the VM. It loads classes,
 327     // but does not resolve constant pool entries directly so we never
 328     // see Class entries here with the old verifier. Similarly the old
 329     // verifier does not like Class entries in the input constant pool.
 330     // The split-verifier is implemented in the VM so it can optionally
 331     // and directly resolve constant pool entries to load classes. The
 332     // split-verifier can accept either Class entries or UnresolvedClass
 333     // entries in the input constant pool. We revert the appended copy
 334     // back to UnresolvedClass so that either verifier will be happy
 335     // with the constant pool entry.
 336     //
 337     // this is an indirect CP entry so it needs special handling
 338     case JVM_CONSTANT_Class:
 339     case JVM_CONSTANT_UnresolvedClass:
 340     {
 341       int name_i = scratch_cp-&gt;klass_name_index_at(scratch_i);
 342       int new_name_i = find_or_append_indirect_entry(scratch_cp, name_i, merge_cp_p,
 343                                                      merge_cp_length_p, THREAD);
 344 
 345       if (new_name_i != name_i) {
 346         log_trace(redefine, class, constantpool)
 347           (&quot;Class entry@%d name_index change: %d to %d&quot;,
 348            *merge_cp_length_p, name_i, new_name_i);
 349       }
 350 
 351       (*merge_cp_p)-&gt;temp_unresolved_klass_at_put(*merge_cp_length_p, new_name_i);
 352       if (scratch_i != *merge_cp_length_p) {
 353         // The new entry in *merge_cp_p is at a different index than
 354         // the new entry in scratch_cp so we need to map the index values.
 355         map_index(scratch_cp, scratch_i, *merge_cp_length_p);
 356       }
 357       (*merge_cp_length_p)++;
 358     } break;
 359 
 360     // these are direct CP entries so they can be directly appended,
 361     // but double and long take two constant pool entries
 362     case JVM_CONSTANT_Double:  // fall through
 363     case JVM_CONSTANT_Long:
 364     {
 365       ConstantPool::copy_entry_to(scratch_cp, scratch_i, *merge_cp_p, *merge_cp_length_p,
 366         THREAD);
 367 
 368       if (scratch_i != *merge_cp_length_p) {
 369         // The new entry in *merge_cp_p is at a different index than
 370         // the new entry in scratch_cp so we need to map the index values.
 371         map_index(scratch_cp, scratch_i, *merge_cp_length_p);
 372       }
 373       (*merge_cp_length_p) += 2;
 374     } break;
 375 
 376     // these are direct CP entries so they can be directly appended
 377     case JVM_CONSTANT_Float:   // fall through
 378     case JVM_CONSTANT_Integer: // fall through
 379     case JVM_CONSTANT_Utf8:    // fall through
 380 
 381     // This was an indirect CP entry, but it has been changed into
 382     // Symbol*s so this entry can be directly appended.
 383     case JVM_CONSTANT_String:      // fall through
 384     {
 385       ConstantPool::copy_entry_to(scratch_cp, scratch_i, *merge_cp_p, *merge_cp_length_p,
 386         THREAD);
 387 
 388       if (scratch_i != *merge_cp_length_p) {
 389         // The new entry in *merge_cp_p is at a different index than
 390         // the new entry in scratch_cp so we need to map the index values.
 391         map_index(scratch_cp, scratch_i, *merge_cp_length_p);
 392       }
 393       (*merge_cp_length_p)++;
 394     } break;
 395 
 396     // this is an indirect CP entry so it needs special handling
 397     case JVM_CONSTANT_NameAndType:
 398     {
 399       int name_ref_i = scratch_cp-&gt;name_ref_index_at(scratch_i);
 400       int new_name_ref_i = find_or_append_indirect_entry(scratch_cp, name_ref_i, merge_cp_p,
 401                                                          merge_cp_length_p, THREAD);
 402 
 403       int signature_ref_i = scratch_cp-&gt;signature_ref_index_at(scratch_i);
 404       int new_signature_ref_i = find_or_append_indirect_entry(scratch_cp, signature_ref_i,
 405                                                               merge_cp_p, merge_cp_length_p,
 406                                                               THREAD);
 407 
 408       // If the referenced entries already exist in *merge_cp_p, then
 409       // both new_name_ref_i and new_signature_ref_i will both be 0.
 410       // In that case, all we are appending is the current entry.
 411       if (new_name_ref_i != name_ref_i) {
 412         log_trace(redefine, class, constantpool)
 413           (&quot;NameAndType entry@%d name_ref_index change: %d to %d&quot;,
 414            *merge_cp_length_p, name_ref_i, new_name_ref_i);
 415       }
 416       if (new_signature_ref_i != signature_ref_i) {
 417         log_trace(redefine, class, constantpool)
 418           (&quot;NameAndType entry@%d signature_ref_index change: %d to %d&quot;,
 419            *merge_cp_length_p, signature_ref_i, new_signature_ref_i);
 420       }
 421 
 422       (*merge_cp_p)-&gt;name_and_type_at_put(*merge_cp_length_p,
 423         new_name_ref_i, new_signature_ref_i);
 424       if (scratch_i != *merge_cp_length_p) {
 425         // The new entry in *merge_cp_p is at a different index than
 426         // the new entry in scratch_cp so we need to map the index values.
 427         map_index(scratch_cp, scratch_i, *merge_cp_length_p);
 428       }
 429       (*merge_cp_length_p)++;
 430     } break;
 431 
 432     // this is a double-indirect CP entry so it needs special handling
 433     case JVM_CONSTANT_Fieldref:           // fall through
 434     case JVM_CONSTANT_InterfaceMethodref: // fall through
 435     case JVM_CONSTANT_Methodref:
 436     {
 437       int klass_ref_i = scratch_cp-&gt;uncached_klass_ref_index_at(scratch_i);
 438       int new_klass_ref_i = find_or_append_indirect_entry(scratch_cp, klass_ref_i,
 439                                                           merge_cp_p, merge_cp_length_p, THREAD);
 440 
 441       int name_and_type_ref_i = scratch_cp-&gt;uncached_name_and_type_ref_index_at(scratch_i);
 442       int new_name_and_type_ref_i = find_or_append_indirect_entry(scratch_cp, name_and_type_ref_i,
 443                                                           merge_cp_p, merge_cp_length_p, THREAD);
 444 
 445       const char *entry_name = NULL;
 446       switch (scratch_cp-&gt;tag_at(scratch_i).value()) {
 447       case JVM_CONSTANT_Fieldref:
 448         entry_name = &quot;Fieldref&quot;;
 449         (*merge_cp_p)-&gt;field_at_put(*merge_cp_length_p, new_klass_ref_i,
 450           new_name_and_type_ref_i);
 451         break;
 452       case JVM_CONSTANT_InterfaceMethodref:
 453         entry_name = &quot;IFMethodref&quot;;
 454         (*merge_cp_p)-&gt;interface_method_at_put(*merge_cp_length_p,
 455           new_klass_ref_i, new_name_and_type_ref_i);
 456         break;
 457       case JVM_CONSTANT_Methodref:
 458         entry_name = &quot;Methodref&quot;;
 459         (*merge_cp_p)-&gt;method_at_put(*merge_cp_length_p, new_klass_ref_i,
 460           new_name_and_type_ref_i);
 461         break;
 462       default:
 463         guarantee(false, &quot;bad switch&quot;);
 464         break;
 465       }
 466 
 467       if (klass_ref_i != new_klass_ref_i) {
 468         log_trace(redefine, class, constantpool)
 469           (&quot;%s entry@%d class_index changed: %d to %d&quot;, entry_name, *merge_cp_length_p, klass_ref_i, new_klass_ref_i);
 470       }
 471       if (name_and_type_ref_i != new_name_and_type_ref_i) {
 472         log_trace(redefine, class, constantpool)
 473           (&quot;%s entry@%d name_and_type_index changed: %d to %d&quot;,
 474            entry_name, *merge_cp_length_p, name_and_type_ref_i, new_name_and_type_ref_i);
 475       }
 476 
 477       if (scratch_i != *merge_cp_length_p) {
 478         // The new entry in *merge_cp_p is at a different index than
 479         // the new entry in scratch_cp so we need to map the index values.
 480         map_index(scratch_cp, scratch_i, *merge_cp_length_p);
 481       }
 482       (*merge_cp_length_p)++;
 483     } break;
 484 
 485     // this is an indirect CP entry so it needs special handling
 486     case JVM_CONSTANT_MethodType:
 487     {
 488       int ref_i = scratch_cp-&gt;method_type_index_at(scratch_i);
 489       int new_ref_i = find_or_append_indirect_entry(scratch_cp, ref_i, merge_cp_p,
 490                                                     merge_cp_length_p, THREAD);
 491       if (new_ref_i != ref_i) {
 492         log_trace(redefine, class, constantpool)
 493           (&quot;MethodType entry@%d ref_index change: %d to %d&quot;, *merge_cp_length_p, ref_i, new_ref_i);
 494       }
 495       (*merge_cp_p)-&gt;method_type_index_at_put(*merge_cp_length_p, new_ref_i);
 496       if (scratch_i != *merge_cp_length_p) {
 497         // The new entry in *merge_cp_p is at a different index than
 498         // the new entry in scratch_cp so we need to map the index values.
 499         map_index(scratch_cp, scratch_i, *merge_cp_length_p);
 500       }
 501       (*merge_cp_length_p)++;
 502     } break;
 503 
 504     // this is an indirect CP entry so it needs special handling
 505     case JVM_CONSTANT_MethodHandle:
 506     {
 507       int ref_kind = scratch_cp-&gt;method_handle_ref_kind_at(scratch_i);
 508       int ref_i = scratch_cp-&gt;method_handle_index_at(scratch_i);
 509       int new_ref_i = find_or_append_indirect_entry(scratch_cp, ref_i, merge_cp_p,
 510                                                     merge_cp_length_p, THREAD);
 511       if (new_ref_i != ref_i) {
 512         log_trace(redefine, class, constantpool)
 513           (&quot;MethodHandle entry@%d ref_index change: %d to %d&quot;, *merge_cp_length_p, ref_i, new_ref_i);
 514       }
 515       (*merge_cp_p)-&gt;method_handle_index_at_put(*merge_cp_length_p, ref_kind, new_ref_i);
 516       if (scratch_i != *merge_cp_length_p) {
 517         // The new entry in *merge_cp_p is at a different index than
 518         // the new entry in scratch_cp so we need to map the index values.
 519         map_index(scratch_cp, scratch_i, *merge_cp_length_p);
 520       }
 521       (*merge_cp_length_p)++;
 522     } break;
 523 
 524     // this is an indirect CP entry so it needs special handling
 525     case JVM_CONSTANT_Dynamic:  // fall through
 526     case JVM_CONSTANT_InvokeDynamic:
 527     {
 528       // Index of the bootstrap specifier in the operands array
 529       int old_bs_i = scratch_cp-&gt;bootstrap_methods_attribute_index(scratch_i);
 530       int new_bs_i = find_or_append_operand(scratch_cp, old_bs_i, merge_cp_p,
 531                                             merge_cp_length_p, THREAD);
 532       // The bootstrap method NameAndType_info index
 533       int old_ref_i = scratch_cp-&gt;bootstrap_name_and_type_ref_index_at(scratch_i);
 534       int new_ref_i = find_or_append_indirect_entry(scratch_cp, old_ref_i, merge_cp_p,
 535                                                     merge_cp_length_p, THREAD);
 536       if (new_bs_i != old_bs_i) {
 537         log_trace(redefine, class, constantpool)
 538           (&quot;Dynamic entry@%d bootstrap_method_attr_index change: %d to %d&quot;,
 539            *merge_cp_length_p, old_bs_i, new_bs_i);
 540       }
 541       if (new_ref_i != old_ref_i) {
 542         log_trace(redefine, class, constantpool)
 543           (&quot;Dynamic entry@%d name_and_type_index change: %d to %d&quot;, *merge_cp_length_p, old_ref_i, new_ref_i);
 544       }
 545 
 546       if (scratch_cp-&gt;tag_at(scratch_i).is_dynamic_constant())
 547         (*merge_cp_p)-&gt;dynamic_constant_at_put(*merge_cp_length_p, new_bs_i, new_ref_i);
 548       else
 549         (*merge_cp_p)-&gt;invoke_dynamic_at_put(*merge_cp_length_p, new_bs_i, new_ref_i);
 550       if (scratch_i != *merge_cp_length_p) {
 551         // The new entry in *merge_cp_p is at a different index than
 552         // the new entry in scratch_cp so we need to map the index values.
 553         map_index(scratch_cp, scratch_i, *merge_cp_length_p);
 554       }
 555       (*merge_cp_length_p)++;
 556     } break;
 557 
 558     // At this stage, Class or UnresolvedClass could be in scratch_cp, but not
 559     // ClassIndex
 560     case JVM_CONSTANT_ClassIndex: // fall through
 561 
 562     // Invalid is used as the tag for the second constant pool entry
 563     // occupied by JVM_CONSTANT_Double or JVM_CONSTANT_Long. It should
 564     // not be seen by itself.
 565     case JVM_CONSTANT_Invalid: // fall through
 566 
 567     // At this stage, String could be here, but not StringIndex
 568     case JVM_CONSTANT_StringIndex: // fall through
 569 
 570     // At this stage JVM_CONSTANT_UnresolvedClassInError should not be here
 571     case JVM_CONSTANT_UnresolvedClassInError: // fall through
 572 
 573     default:
 574     {
 575       // leave a breadcrumb
 576       jbyte bad_value = scratch_cp-&gt;tag_at(scratch_i).value();
 577       ShouldNotReachHere();
 578     } break;
 579   } // end switch tag value
 580 } // end append_entry()
 581 
 582 
 583 int VM_RedefineClasses::find_or_append_indirect_entry(const constantPoolHandle&amp; scratch_cp,
 584       int ref_i, constantPoolHandle *merge_cp_p, int *merge_cp_length_p, TRAPS) {
 585 
 586   int new_ref_i = ref_i;
 587   bool match = (ref_i &lt; *merge_cp_length_p) &amp;&amp;
 588                scratch_cp-&gt;compare_entry_to(ref_i, *merge_cp_p, ref_i, THREAD);
 589 
 590   if (!match) {
 591     // forward reference in *merge_cp_p or not a direct match
 592     int found_i = scratch_cp-&gt;find_matching_entry(ref_i, *merge_cp_p, THREAD);
 593     if (found_i != 0) {
 594       guarantee(found_i != ref_i, &quot;compare_entry_to() and find_matching_entry() do not agree&quot;);
 595       // Found a matching entry somewhere else in *merge_cp_p so just need a mapping entry.
 596       new_ref_i = found_i;
 597       map_index(scratch_cp, ref_i, found_i);
 598     } else {
 599       // no match found so we have to append this entry to *merge_cp_p
 600       append_entry(scratch_cp, ref_i, merge_cp_p, merge_cp_length_p, THREAD);
 601       // The above call to append_entry() can only append one entry
 602       // so the post call query of *merge_cp_length_p is only for
 603       // the sake of consistency.
 604       new_ref_i = *merge_cp_length_p - 1;
 605     }
 606   }
 607 
 608   return new_ref_i;
 609 } // end find_or_append_indirect_entry()
 610 
 611 
 612 // Append a bootstrap specifier into the merge_cp operands that is semantically equal
 613 // to the scratch_cp operands bootstrap specifier passed by the old_bs_i index.
 614 // Recursively append new merge_cp entries referenced by the new bootstrap specifier.
 615 void VM_RedefineClasses::append_operand(const constantPoolHandle&amp; scratch_cp, int old_bs_i,
 616        constantPoolHandle *merge_cp_p, int *merge_cp_length_p, TRAPS) {
 617 
 618   int old_ref_i = scratch_cp-&gt;operand_bootstrap_method_ref_index_at(old_bs_i);
 619   int new_ref_i = find_or_append_indirect_entry(scratch_cp, old_ref_i, merge_cp_p,
 620                                                 merge_cp_length_p, THREAD);
 621   if (new_ref_i != old_ref_i) {
 622     log_trace(redefine, class, constantpool)
 623       (&quot;operands entry@%d bootstrap method ref_index change: %d to %d&quot;, _operands_cur_length, old_ref_i, new_ref_i);
 624   }
 625 
 626   Array&lt;u2&gt;* merge_ops = (*merge_cp_p)-&gt;operands();
 627   int new_bs_i = _operands_cur_length;
 628   // We have _operands_cur_length == 0 when the merge_cp operands is empty yet.
 629   // However, the operand_offset_at(0) was set in the extend_operands() call.
 630   int new_base = (new_bs_i == 0) ? (*merge_cp_p)-&gt;operand_offset_at(0)
 631                                  : (*merge_cp_p)-&gt;operand_next_offset_at(new_bs_i - 1);
 632   int argc     = scratch_cp-&gt;operand_argument_count_at(old_bs_i);
 633 
 634   ConstantPool::operand_offset_at_put(merge_ops, _operands_cur_length, new_base);
 635   merge_ops-&gt;at_put(new_base++, new_ref_i);
 636   merge_ops-&gt;at_put(new_base++, argc);
 637 
 638   for (int i = 0; i &lt; argc; i++) {
 639     int old_arg_ref_i = scratch_cp-&gt;operand_argument_index_at(old_bs_i, i);
 640     int new_arg_ref_i = find_or_append_indirect_entry(scratch_cp, old_arg_ref_i, merge_cp_p,
 641                                                       merge_cp_length_p, THREAD);
 642     merge_ops-&gt;at_put(new_base++, new_arg_ref_i);
 643     if (new_arg_ref_i != old_arg_ref_i) {
 644       log_trace(redefine, class, constantpool)
 645         (&quot;operands entry@%d bootstrap method argument ref_index change: %d to %d&quot;,
 646          _operands_cur_length, old_arg_ref_i, new_arg_ref_i);
 647     }
 648   }
 649   if (old_bs_i != _operands_cur_length) {
 650     // The bootstrap specifier in *merge_cp_p is at a different index than
 651     // that in scratch_cp so we need to map the index values.
 652     map_operand_index(old_bs_i, new_bs_i);
 653   }
 654   _operands_cur_length++;
 655 } // end append_operand()
 656 
 657 
 658 int VM_RedefineClasses::find_or_append_operand(const constantPoolHandle&amp; scratch_cp,
 659       int old_bs_i, constantPoolHandle *merge_cp_p, int *merge_cp_length_p, TRAPS) {
 660 
 661   int new_bs_i = old_bs_i; // bootstrap specifier index
 662   bool match = (old_bs_i &lt; _operands_cur_length) &amp;&amp;
 663                scratch_cp-&gt;compare_operand_to(old_bs_i, *merge_cp_p, old_bs_i, THREAD);
 664 
 665   if (!match) {
 666     // forward reference in *merge_cp_p or not a direct match
 667     int found_i = scratch_cp-&gt;find_matching_operand(old_bs_i, *merge_cp_p,
 668                                                     _operands_cur_length, THREAD);
 669     if (found_i != -1) {
 670       guarantee(found_i != old_bs_i, &quot;compare_operand_to() and find_matching_operand() disagree&quot;);
 671       // found a matching operand somewhere else in *merge_cp_p so just need a mapping
 672       new_bs_i = found_i;
 673       map_operand_index(old_bs_i, found_i);
 674     } else {
 675       // no match found so we have to append this bootstrap specifier to *merge_cp_p
 676       append_operand(scratch_cp, old_bs_i, merge_cp_p, merge_cp_length_p, THREAD);
 677       new_bs_i = _operands_cur_length - 1;
 678     }
 679   }
 680   return new_bs_i;
 681 } // end find_or_append_operand()
 682 
 683 
 684 void VM_RedefineClasses::finalize_operands_merge(const constantPoolHandle&amp; merge_cp, TRAPS) {
 685   if (merge_cp-&gt;operands() == NULL) {
 686     return;
 687   }
 688   // Shrink the merge_cp operands
 689   merge_cp-&gt;shrink_operands(_operands_cur_length, CHECK);
 690 
 691   if (log_is_enabled(Trace, redefine, class, constantpool)) {
 692     // don&#39;t want to loop unless we are tracing
 693     int count = 0;
 694     for (int i = 1; i &lt; _operands_index_map_p-&gt;length(); i++) {
 695       int value = _operands_index_map_p-&gt;at(i);
 696       if (value != -1) {
 697         log_trace(redefine, class, constantpool)(&quot;operands_index_map[%d]: old=%d new=%d&quot;, count, i, value);
 698         count++;
 699       }
 700     }
 701   }
 702   // Clean-up
 703   _operands_index_map_p = NULL;
 704   _operands_cur_length = 0;
 705   _operands_index_map_count = 0;
 706 } // end finalize_operands_merge()
 707 
 708 // Symbol* comparator for qsort
 709 // The caller must have an active ResourceMark.
 710 static int symcmp(const void* a, const void* b) {
 711   char* astr = (*(Symbol**)a)-&gt;as_C_string();
 712   char* bstr = (*(Symbol**)b)-&gt;as_C_string();
 713   return strcmp(astr, bstr);
 714 }
 715 
 716 // The caller must have an active ResourceMark.
 717 static jvmtiError check_attribute_arrays(const char* attr_name,
 718            InstanceKlass* the_class, InstanceKlass* scratch_class,
 719            Array&lt;u2&gt;* the_array, Array&lt;u2&gt;* scr_array) {
 720   bool the_array_exists = the_array != Universe::the_empty_short_array();
 721   bool scr_array_exists = scr_array != Universe::the_empty_short_array();
 722 
 723   int array_len = the_array-&gt;length();
 724   if (the_array_exists &amp;&amp; scr_array_exists) {
 725     if (array_len != scr_array-&gt;length()) {
 726       log_trace(redefine, class)
 727         (&quot;redefined class %s attribute change error: %s len=%d changed to len=%d&quot;,
 728          the_class-&gt;external_name(), attr_name, array_len, scr_array-&gt;length());
 729       return JVMTI_ERROR_UNSUPPORTED_REDEFINITION_CLASS_ATTRIBUTE_CHANGED;
 730     }
 731 
 732     // The order of entries in the attribute array is not specified so we
 733     // have to explicitly check for the same contents. We do this by copying
 734     // the referenced symbols into their own arrays, sorting them and then
 735     // comparing each element pair.
 736 
 737     Symbol** the_syms = NEW_RESOURCE_ARRAY_RETURN_NULL(Symbol*, array_len);
 738     Symbol** scr_syms = NEW_RESOURCE_ARRAY_RETURN_NULL(Symbol*, array_len);
 739 
 740     if (the_syms == NULL || scr_syms == NULL) {
 741       return JVMTI_ERROR_OUT_OF_MEMORY;
 742     }
 743 
 744     for (int i = 0; i &lt; array_len; i++) {
 745       int the_cp_index = the_array-&gt;at(i);
 746       int scr_cp_index = scr_array-&gt;at(i);
 747       the_syms[i] = the_class-&gt;constants()-&gt;klass_name_at(the_cp_index);
 748       scr_syms[i] = scratch_class-&gt;constants()-&gt;klass_name_at(scr_cp_index);
 749     }
 750 
 751     qsort(the_syms, array_len, sizeof(Symbol*), symcmp);
 752     qsort(scr_syms, array_len, sizeof(Symbol*), symcmp);
 753 
 754     for (int i = 0; i &lt; array_len; i++) {
 755       if (the_syms[i] != scr_syms[i]) {
 756         log_trace(redefine, class)
 757           (&quot;redefined class %s attribute change error: %s[%d]: %s changed to %s&quot;,
 758            the_class-&gt;external_name(), attr_name, i,
 759            the_syms[i]-&gt;as_C_string(), scr_syms[i]-&gt;as_C_string());
 760         return JVMTI_ERROR_UNSUPPORTED_REDEFINITION_CLASS_ATTRIBUTE_CHANGED;
 761       }
 762     }
 763   } else if (the_array_exists ^ scr_array_exists) {
 764     const char* action_str = (the_array_exists) ? &quot;removed&quot; : &quot;added&quot;;
 765     log_trace(redefine, class)
 766       (&quot;redefined class %s attribute change error: %s attribute %s&quot;,
 767        the_class-&gt;external_name(), attr_name, action_str);
 768     return JVMTI_ERROR_UNSUPPORTED_REDEFINITION_CLASS_ATTRIBUTE_CHANGED;
 769   }
 770   return JVMTI_ERROR_NONE;
 771 }
 772 
 773 static jvmtiError check_nest_attributes(InstanceKlass* the_class,
 774                                         InstanceKlass* scratch_class) {
 775   // Check whether the class NestHost attribute has been changed.
 776   Thread* thread = Thread::current();
 777   ResourceMark rm(thread);
 778   u2 the_nest_host_idx = the_class-&gt;nest_host_index();
 779   u2 scr_nest_host_idx = scratch_class-&gt;nest_host_index();
 780 
 781   if (the_nest_host_idx != 0 &amp;&amp; scr_nest_host_idx != 0) {
 782     Symbol* the_sym = the_class-&gt;constants()-&gt;klass_name_at(the_nest_host_idx);
 783     Symbol* scr_sym = scratch_class-&gt;constants()-&gt;klass_name_at(scr_nest_host_idx);
 784     if (the_sym != scr_sym) {
 785       log_trace(redefine, class, nestmates)
 786         (&quot;redefined class %s attribute change error: NestHost class: %s replaced with: %s&quot;,
 787          the_class-&gt;external_name(), the_sym-&gt;as_C_string(), scr_sym-&gt;as_C_string());
 788       return JVMTI_ERROR_UNSUPPORTED_REDEFINITION_CLASS_ATTRIBUTE_CHANGED;
 789     }
 790   } else if ((the_nest_host_idx == 0) ^ (scr_nest_host_idx == 0)) {
 791     const char* action_str = (the_nest_host_idx != 0) ? &quot;removed&quot; : &quot;added&quot;;
 792     log_trace(redefine, class, nestmates)
 793       (&quot;redefined class %s attribute change error: NestHost attribute %s&quot;,
 794        the_class-&gt;external_name(), action_str);
 795     return JVMTI_ERROR_UNSUPPORTED_REDEFINITION_CLASS_ATTRIBUTE_CHANGED;
 796   }
 797 
 798   // Check whether the class NestMembers attribute has been changed.
 799   return check_attribute_arrays(&quot;NestMembers&quot;,
 800                                 the_class, scratch_class,
 801                                 the_class-&gt;nest_members(),
 802                                 scratch_class-&gt;nest_members());
 803 }
 804 
 805 // Return an error status if the class Record attribute was changed.
 806 static jvmtiError check_record_attribute(InstanceKlass* the_class, InstanceKlass* scratch_class) {
 807   // Get lists of record components.
 808   Array&lt;RecordComponent*&gt;* the_record = the_class-&gt;record_components();
 809   Array&lt;RecordComponent*&gt;* scr_record = scratch_class-&gt;record_components();
 810   bool the_record_exists = the_record != NULL;
 811   bool scr_record_exists = scr_record != NULL;
 812 
 813   if (the_record_exists &amp;&amp; scr_record_exists) {
 814     int the_num_components = the_record-&gt;length();
 815     int scr_num_components = scr_record-&gt;length();
 816     if (the_num_components != scr_num_components) {
 817       log_trace(redefine, class, record)
 818         (&quot;redefined class %s attribute change error: Record num_components=%d changed to num_components=%d&quot;,
 819          the_class-&gt;external_name(), the_num_components, scr_num_components);
 820       return JVMTI_ERROR_UNSUPPORTED_REDEFINITION_CLASS_ATTRIBUTE_CHANGED;
 821     }
 822 
 823     // Compare each field in each record component.
 824     ConstantPool* the_cp =  the_class-&gt;constants();
 825     ConstantPool* scr_cp =  scratch_class-&gt;constants();
 826     for (int x = 0; x &lt; the_num_components; x++) {
 827       RecordComponent* the_component = the_record-&gt;at(x);
 828       RecordComponent* scr_component = scr_record-&gt;at(x);
 829       const Symbol* const the_name = the_cp-&gt;symbol_at(the_component-&gt;name_index());
 830       const Symbol* const scr_name = scr_cp-&gt;symbol_at(scr_component-&gt;name_index());
 831       const Symbol* const the_descr = the_cp-&gt;symbol_at(the_component-&gt;descriptor_index());
 832       const Symbol* const scr_descr = scr_cp-&gt;symbol_at(scr_component-&gt;descriptor_index());
 833       if (the_name != scr_name || the_descr != scr_descr) {
 834         log_trace(redefine, class, record)
 835           (&quot;redefined class %s attribute change error: Record name_index, descriptor_index, and/or attributes_count changed&quot;,
 836            the_class-&gt;external_name());
 837         return JVMTI_ERROR_UNSUPPORTED_REDEFINITION_CLASS_ATTRIBUTE_CHANGED;
 838       }
 839 
 840       int the_gen_sig = the_component-&gt;generic_signature_index();
 841       int scr_gen_sig = scr_component-&gt;generic_signature_index();
 842       const Symbol* const the_gen_sig_sym = (the_gen_sig == 0 ? NULL :
 843         the_cp-&gt;symbol_at(the_component-&gt;generic_signature_index()));
 844       const Symbol* const scr_gen_sig_sym = (scr_gen_sig == 0 ? NULL :
 845         scr_cp-&gt;symbol_at(scr_component-&gt;generic_signature_index()));
 846       if (the_gen_sig_sym != scr_gen_sig_sym) {
 847         log_trace(redefine, class, record)
 848           (&quot;redefined class %s attribute change error: Record generic_signature attribute changed&quot;,
 849            the_class-&gt;external_name());
 850         return JVMTI_ERROR_UNSUPPORTED_REDEFINITION_CLASS_ATTRIBUTE_CHANGED;
 851       }
 852 
 853       // It&#39;s okay if a record component&#39;s annotations were changed.
 854     }
 855 
 856   } else if (the_record_exists ^ scr_record_exists) {
 857     const char* action_str = (the_record_exists) ? &quot;removed&quot; : &quot;added&quot;;
 858     log_trace(redefine, class, record)
 859       (&quot;redefined class %s attribute change error: Record attribute %s&quot;,
 860        the_class-&gt;external_name(), action_str);
 861     return JVMTI_ERROR_UNSUPPORTED_REDEFINITION_CLASS_ATTRIBUTE_CHANGED;
 862   }
 863 
 864   return JVMTI_ERROR_NONE;
 865 }
 866 
 867 
 868 static jvmtiError check_permitted_subclasses_attribute(InstanceKlass* the_class,
 869                                                        InstanceKlass* scratch_class) {
 870   Thread* thread = Thread::current();
 871   ResourceMark rm(thread);
 872 
 873   // Check whether the class PermittedSubclasses attribute has been changed.
 874   return check_attribute_arrays(&quot;PermittedSubclasses&quot;,
 875                                 the_class, scratch_class,
 876                                 the_class-&gt;permitted_subclasses(),
 877                                 scratch_class-&gt;permitted_subclasses());
 878 }
 879 
 880 static bool can_add_or_delete(Method* m) {
 881       // Compatibility mode
 882   return (AllowRedefinitionToAddDeleteMethods &amp;&amp;
 883           (m-&gt;is_private() &amp;&amp; (m-&gt;is_static() || m-&gt;is_final())));
 884 }
 885 
 886 jvmtiError VM_RedefineClasses::compare_and_normalize_class_versions(
 887              InstanceKlass* the_class,
 888              InstanceKlass* scratch_class) {
 889   int i;
 890 
 891   // Check superclasses, or rather their names, since superclasses themselves can be
 892   // requested to replace.
 893   // Check for NULL superclass first since this might be java.lang.Object
 894   if (the_class-&gt;super() != scratch_class-&gt;super() &amp;&amp;
 895       (the_class-&gt;super() == NULL || scratch_class-&gt;super() == NULL ||
 896        the_class-&gt;super()-&gt;name() !=
 897        scratch_class-&gt;super()-&gt;name())) {
 898     return JVMTI_ERROR_UNSUPPORTED_REDEFINITION_HIERARCHY_CHANGED;
 899   }
 900 
 901   // Check if the number, names and order of directly implemented interfaces are the same.
 902   // I think in principle we should just check if the sets of names of directly implemented
 903   // interfaces are the same, i.e. the order of declaration (which, however, if changed in the
 904   // .java file, also changes in .class file) should not matter. However, comparing sets is
 905   // technically a bit more difficult, and, more importantly, I am not sure at present that the
 906   // order of interfaces does not matter on the implementation level, i.e. that the VM does not
 907   // rely on it somewhere.
 908   Array&lt;InstanceKlass*&gt;* k_interfaces = the_class-&gt;local_interfaces();
 909   Array&lt;InstanceKlass*&gt;* k_new_interfaces = scratch_class-&gt;local_interfaces();
 910   int n_intfs = k_interfaces-&gt;length();
 911   if (n_intfs != k_new_interfaces-&gt;length()) {
 912     return JVMTI_ERROR_UNSUPPORTED_REDEFINITION_HIERARCHY_CHANGED;
 913   }
 914   for (i = 0; i &lt; n_intfs; i++) {
 915     if (k_interfaces-&gt;at(i)-&gt;name() !=
 916         k_new_interfaces-&gt;at(i)-&gt;name()) {
 917       return JVMTI_ERROR_UNSUPPORTED_REDEFINITION_HIERARCHY_CHANGED;
 918     }
 919   }
 920 
 921   // Check whether class is in the error init state.
 922   if (the_class-&gt;is_in_error_state()) {
 923     // TBD #5057930: special error code is needed in 1.6
 924     return JVMTI_ERROR_INVALID_CLASS;
 925   }
 926 
 927   // Check whether the nest-related attributes have been changed.
 928   jvmtiError err = check_nest_attributes(the_class, scratch_class);
 929   if (err != JVMTI_ERROR_NONE) {
 930     return err;
 931   }
 932 
 933   // Check whether the Record attribute has been changed.
 934   err = check_record_attribute(the_class, scratch_class);
 935   if (err != JVMTI_ERROR_NONE) {
 936     return err;
 937   }
 938 
 939   // Check whether the PermittedSubclasses attribute has been changed.
 940   err = check_permitted_subclasses_attribute(the_class, scratch_class);
 941   if (err != JVMTI_ERROR_NONE) {
 942     return err;
 943   }
 944 
 945   // Check whether class modifiers are the same.
 946   jushort old_flags = (jushort) the_class-&gt;access_flags().get_flags();
 947   jushort new_flags = (jushort) scratch_class-&gt;access_flags().get_flags();
 948   if (old_flags != new_flags) {
 949     return JVMTI_ERROR_UNSUPPORTED_REDEFINITION_CLASS_MODIFIERS_CHANGED;
 950   }
 951 
 952   // Check if the number, names, types and order of fields declared in these classes
 953   // are the same.
 954   JavaFieldStream old_fs(the_class);
 955   JavaFieldStream new_fs(scratch_class);
 956   for (; !old_fs.done() &amp;&amp; !new_fs.done(); old_fs.next(), new_fs.next()) {
 957     // access
 958     old_flags = old_fs.access_flags().as_short();
 959     new_flags = new_fs.access_flags().as_short();
 960     if ((old_flags ^ new_flags) &amp; JVM_RECOGNIZED_FIELD_MODIFIERS) {
 961       return JVMTI_ERROR_UNSUPPORTED_REDEFINITION_SCHEMA_CHANGED;
 962     }
 963     // offset
 964     if (old_fs.offset() != new_fs.offset()) {
 965       return JVMTI_ERROR_UNSUPPORTED_REDEFINITION_SCHEMA_CHANGED;
 966     }
 967     // name and signature
 968     Symbol* name_sym1 = the_class-&gt;constants()-&gt;symbol_at(old_fs.name_index());
 969     Symbol* sig_sym1 = the_class-&gt;constants()-&gt;symbol_at(old_fs.signature_index());
 970     Symbol* name_sym2 = scratch_class-&gt;constants()-&gt;symbol_at(new_fs.name_index());
 971     Symbol* sig_sym2 = scratch_class-&gt;constants()-&gt;symbol_at(new_fs.signature_index());
 972     if (name_sym1 != name_sym2 || sig_sym1 != sig_sym2) {
 973       return JVMTI_ERROR_UNSUPPORTED_REDEFINITION_SCHEMA_CHANGED;
 974     }
 975   }
 976 
 977   // If both streams aren&#39;t done then we have a differing number of
 978   // fields.
 979   if (!old_fs.done() || !new_fs.done()) {
 980     return JVMTI_ERROR_UNSUPPORTED_REDEFINITION_SCHEMA_CHANGED;
 981   }
 982 
 983   // Do a parallel walk through the old and new methods. Detect
 984   // cases where they match (exist in both), have been added in
 985   // the new methods, or have been deleted (exist only in the
 986   // old methods).  The class file parser places methods in order
 987   // by method name, but does not order overloaded methods by
 988   // signature.  In order to determine what fate befell the methods,
 989   // this code places the overloaded new methods that have matching
 990   // old methods in the same order as the old methods and places
 991   // new overloaded methods at the end of overloaded methods of
 992   // that name. The code for this order normalization is adapted
 993   // from the algorithm used in InstanceKlass::find_method().
 994   // Since we are swapping out of order entries as we find them,
 995   // we only have to search forward through the overloaded methods.
 996   // Methods which are added and have the same name as an existing
 997   // method (but different signature) will be put at the end of
 998   // the methods with that name, and the name mismatch code will
 999   // handle them.
1000   Array&lt;Method*&gt;* k_old_methods(the_class-&gt;methods());
1001   Array&lt;Method*&gt;* k_new_methods(scratch_class-&gt;methods());
1002   int n_old_methods = k_old_methods-&gt;length();
1003   int n_new_methods = k_new_methods-&gt;length();
1004   Thread* thread = Thread::current();
1005 
1006   int ni = 0;
1007   int oi = 0;
1008   while (true) {
1009     Method* k_old_method;
1010     Method* k_new_method;
1011     enum { matched, added, deleted, undetermined } method_was = undetermined;
1012 
1013     if (oi &gt;= n_old_methods) {
1014       if (ni &gt;= n_new_methods) {
1015         break; // we&#39;ve looked at everything, done
1016       }
1017       // New method at the end
1018       k_new_method = k_new_methods-&gt;at(ni);
1019       method_was = added;
1020     } else if (ni &gt;= n_new_methods) {
1021       // Old method, at the end, is deleted
1022       k_old_method = k_old_methods-&gt;at(oi);
1023       method_was = deleted;
1024     } else {
1025       // There are more methods in both the old and new lists
1026       k_old_method = k_old_methods-&gt;at(oi);
1027       k_new_method = k_new_methods-&gt;at(ni);
1028       if (k_old_method-&gt;name() != k_new_method-&gt;name()) {
1029         // Methods are sorted by method name, so a mismatch means added
1030         // or deleted
1031         if (k_old_method-&gt;name()-&gt;fast_compare(k_new_method-&gt;name()) &gt; 0) {
1032           method_was = added;
1033         } else {
1034           method_was = deleted;
1035         }
1036       } else if (k_old_method-&gt;signature() == k_new_method-&gt;signature()) {
1037         // Both the name and signature match
1038         method_was = matched;
1039       } else {
1040         // The name matches, but the signature doesn&#39;t, which means we have to
1041         // search forward through the new overloaded methods.
1042         int nj;  // outside the loop for post-loop check
1043         for (nj = ni + 1; nj &lt; n_new_methods; nj++) {
1044           Method* m = k_new_methods-&gt;at(nj);
1045           if (k_old_method-&gt;name() != m-&gt;name()) {
1046             // reached another method name so no more overloaded methods
1047             method_was = deleted;
1048             break;
1049           }
1050           if (k_old_method-&gt;signature() == m-&gt;signature()) {
1051             // found a match so swap the methods
1052             k_new_methods-&gt;at_put(ni, m);
1053             k_new_methods-&gt;at_put(nj, k_new_method);
1054             k_new_method = m;
1055             method_was = matched;
1056             break;
1057           }
1058         }
1059 
1060         if (nj &gt;= n_new_methods) {
1061           // reached the end without a match; so method was deleted
1062           method_was = deleted;
1063         }
1064       }
1065     }
1066 
1067     switch (method_was) {
1068     case matched:
1069       // methods match, be sure modifiers do too
1070       old_flags = (jushort) k_old_method-&gt;access_flags().get_flags();
1071       new_flags = (jushort) k_new_method-&gt;access_flags().get_flags();
1072       if ((old_flags ^ new_flags) &amp; ~(JVM_ACC_NATIVE)) {
1073         return JVMTI_ERROR_UNSUPPORTED_REDEFINITION_METHOD_MODIFIERS_CHANGED;
1074       }
1075       {
1076         u2 new_num = k_new_method-&gt;method_idnum();
1077         u2 old_num = k_old_method-&gt;method_idnum();
1078         if (new_num != old_num) {
1079           Method* idnum_owner = scratch_class-&gt;method_with_idnum(old_num);
1080           if (idnum_owner != NULL) {
1081             // There is already a method assigned this idnum -- switch them
1082             // Take current and original idnum from the new_method
1083             idnum_owner-&gt;set_method_idnum(new_num);
1084             idnum_owner-&gt;set_orig_method_idnum(k_new_method-&gt;orig_method_idnum());
1085           }
1086           // Take current and original idnum from the old_method
1087           k_new_method-&gt;set_method_idnum(old_num);
1088           k_new_method-&gt;set_orig_method_idnum(k_old_method-&gt;orig_method_idnum());
1089           if (thread-&gt;has_pending_exception()) {
1090             return JVMTI_ERROR_OUT_OF_MEMORY;
1091           }
1092         }
1093       }
1094       log_trace(redefine, class, normalize)
1095         (&quot;Method matched: new: %s [%d] == old: %s [%d]&quot;,
1096          k_new_method-&gt;name_and_sig_as_C_string(), ni, k_old_method-&gt;name_and_sig_as_C_string(), oi);
1097       // advance to next pair of methods
1098       ++oi;
1099       ++ni;
1100       break;
1101     case added:
1102       // method added, see if it is OK
1103       if (!can_add_or_delete(k_new_method)) {
1104         return JVMTI_ERROR_UNSUPPORTED_REDEFINITION_METHOD_ADDED;
1105       }
1106       {
1107         u2 num = the_class-&gt;next_method_idnum();
1108         if (num == ConstMethod::UNSET_IDNUM) {
1109           // cannot add any more methods
1110           return JVMTI_ERROR_UNSUPPORTED_REDEFINITION_METHOD_ADDED;
1111         }
1112         u2 new_num = k_new_method-&gt;method_idnum();
1113         Method* idnum_owner = scratch_class-&gt;method_with_idnum(num);
1114         if (idnum_owner != NULL) {
1115           // There is already a method assigned this idnum -- switch them
1116           // Take current and original idnum from the new_method
1117           idnum_owner-&gt;set_method_idnum(new_num);
1118           idnum_owner-&gt;set_orig_method_idnum(k_new_method-&gt;orig_method_idnum());
1119         }
1120         k_new_method-&gt;set_method_idnum(num);
1121         k_new_method-&gt;set_orig_method_idnum(num);
1122         if (thread-&gt;has_pending_exception()) {
1123           return JVMTI_ERROR_OUT_OF_MEMORY;
1124         }
1125       }
1126       log_trace(redefine, class, normalize)
1127         (&quot;Method added: new: %s [%d]&quot;, k_new_method-&gt;name_and_sig_as_C_string(), ni);
1128       ++ni; // advance to next new method
1129       break;
1130     case deleted:
1131       // method deleted, see if it is OK
1132       if (!can_add_or_delete(k_old_method)) {
1133         return JVMTI_ERROR_UNSUPPORTED_REDEFINITION_METHOD_DELETED;
1134       }
1135       log_trace(redefine, class, normalize)
1136         (&quot;Method deleted: old: %s [%d]&quot;, k_old_method-&gt;name_and_sig_as_C_string(), oi);
1137       ++oi; // advance to next old method
1138       break;
1139     default:
1140       ShouldNotReachHere();
1141     }
1142   }
1143 
1144   return JVMTI_ERROR_NONE;
1145 }
1146 
1147 
1148 // Find new constant pool index value for old constant pool index value
1149 // by seaching the index map. Returns zero (0) if there is no mapped
1150 // value for the old constant pool index.
1151 int VM_RedefineClasses::find_new_index(int old_index) {
1152   if (_index_map_count == 0) {
1153     // map is empty so nothing can be found
1154     return 0;
1155   }
1156 
1157   if (old_index &lt; 1 || old_index &gt;= _index_map_p-&gt;length()) {
1158     // The old_index is out of range so it is not mapped. This should
1159     // not happen in regular constant pool merging use, but it can
1160     // happen if a corrupt annotation is processed.
1161     return 0;
1162   }
1163 
1164   int value = _index_map_p-&gt;at(old_index);
1165   if (value == -1) {
1166     // the old_index is not mapped
1167     return 0;
1168   }
1169 
1170   return value;
1171 } // end find_new_index()
1172 
1173 
1174 // Find new bootstrap specifier index value for old bootstrap specifier index
1175 // value by seaching the index map. Returns unused index (-1) if there is
1176 // no mapped value for the old bootstrap specifier index.
1177 int VM_RedefineClasses::find_new_operand_index(int old_index) {
1178   if (_operands_index_map_count == 0) {
1179     // map is empty so nothing can be found
1180     return -1;
1181   }
1182 
1183   if (old_index == -1 || old_index &gt;= _operands_index_map_p-&gt;length()) {
1184     // The old_index is out of range so it is not mapped.
1185     // This should not happen in regular constant pool merging use.
1186     return -1;
1187   }
1188 
1189   int value = _operands_index_map_p-&gt;at(old_index);
1190   if (value == -1) {
1191     // the old_index is not mapped
1192     return -1;
1193   }
1194 
1195   return value;
1196 } // end find_new_operand_index()
1197 
1198 
1199 // Returns true if the current mismatch is due to a resolved/unresolved
1200 // class pair. Otherwise, returns false.
1201 bool VM_RedefineClasses::is_unresolved_class_mismatch(const constantPoolHandle&amp; cp1,
1202        int index1, const constantPoolHandle&amp; cp2, int index2) {
1203 
1204   jbyte t1 = cp1-&gt;tag_at(index1).value();
1205   if (t1 != JVM_CONSTANT_Class &amp;&amp; t1 != JVM_CONSTANT_UnresolvedClass) {
1206     return false;  // wrong entry type; not our special case
1207   }
1208 
1209   jbyte t2 = cp2-&gt;tag_at(index2).value();
1210   if (t2 != JVM_CONSTANT_Class &amp;&amp; t2 != JVM_CONSTANT_UnresolvedClass) {
1211     return false;  // wrong entry type; not our special case
1212   }
1213 
1214   if (t1 == t2) {
1215     return false;  // not a mismatch; not our special case
1216   }
1217 
1218   char *s1 = cp1-&gt;klass_name_at(index1)-&gt;as_C_string();
1219   char *s2 = cp2-&gt;klass_name_at(index2)-&gt;as_C_string();
1220   if (strcmp(s1, s2) != 0) {
1221     return false;  // strings don&#39;t match; not our special case
1222   }
1223 
1224   return true;  // made it through the gauntlet; this is our special case
1225 } // end is_unresolved_class_mismatch()
1226 
1227 
1228 jvmtiError VM_RedefineClasses::load_new_class_versions(TRAPS) {
1229 
1230   // For consistency allocate memory using os::malloc wrapper.
1231   _scratch_classes = (InstanceKlass**)
1232     os::malloc(sizeof(InstanceKlass*) * _class_count, mtClass);
1233   if (_scratch_classes == NULL) {
1234     return JVMTI_ERROR_OUT_OF_MEMORY;
1235   }
1236   // Zero initialize the _scratch_classes array.
1237   for (int i = 0; i &lt; _class_count; i++) {
1238     _scratch_classes[i] = NULL;
1239   }
1240 
1241   ResourceMark rm(THREAD);
1242 
1243   JvmtiThreadState *state = JvmtiThreadState::state_for(JavaThread::current());
1244   // state can only be NULL if the current thread is exiting which
1245   // should not happen since we&#39;re trying to do a RedefineClasses
1246   guarantee(state != NULL, &quot;exiting thread calling load_new_class_versions&quot;);
1247   for (int i = 0; i &lt; _class_count; i++) {
1248     // Create HandleMark so that any handles created while loading new class
1249     // versions are deleted. Constant pools are deallocated while merging
1250     // constant pools
1251     HandleMark hm(THREAD);
1252     InstanceKlass* the_class = get_ik(_class_defs[i].klass);
1253     Symbol*  the_class_sym = the_class-&gt;name();
1254 
1255     log_debug(redefine, class, load)
1256       (&quot;loading name=%s kind=%d (avail_mem=&quot; UINT64_FORMAT &quot;K)&quot;,
1257        the_class-&gt;external_name(), _class_load_kind, os::available_memory() &gt;&gt; 10);
1258 
1259     ClassFileStream st((u1*)_class_defs[i].class_bytes,
1260                        _class_defs[i].class_byte_count,
1261                        &quot;__VM_RedefineClasses__&quot;,
1262                        ClassFileStream::verify);
1263 
1264     // Parse the stream.
1265     Handle the_class_loader(THREAD, the_class-&gt;class_loader());
1266     Handle protection_domain(THREAD, the_class-&gt;protection_domain());
1267     // Set redefined class handle in JvmtiThreadState class.
1268     // This redefined class is sent to agent event handler for class file
1269     // load hook event.
1270     state-&gt;set_class_being_redefined(the_class, _class_load_kind);
1271 
1272     ClassLoadInfo cl_info(protection_domain);
1273     InstanceKlass* scratch_class = SystemDictionary::parse_stream(
1274                                                       the_class_sym,
1275                                                       the_class_loader,
1276                                                       &amp;st,
1277                                                       cl_info,
1278                                                       THREAD);
1279     // Clear class_being_redefined just to be sure.
1280     state-&gt;clear_class_being_redefined();
1281 
1282     // TODO: if this is retransform, and nothing changed we can skip it
1283 
1284     // Need to clean up allocated InstanceKlass if there&#39;s an error so assign
1285     // the result here. Caller deallocates all the scratch classes in case of
1286     // an error.
1287     _scratch_classes[i] = scratch_class;
1288 
1289     if (HAS_PENDING_EXCEPTION) {
1290       Symbol* ex_name = PENDING_EXCEPTION-&gt;klass()-&gt;name();
1291       log_info(redefine, class, load, exceptions)(&quot;parse_stream exception: &#39;%s&#39;&quot;, ex_name-&gt;as_C_string());
1292       CLEAR_PENDING_EXCEPTION;
1293 
1294       if (ex_name == vmSymbols::java_lang_UnsupportedClassVersionError()) {
1295         return JVMTI_ERROR_UNSUPPORTED_VERSION;
1296       } else if (ex_name == vmSymbols::java_lang_ClassFormatError()) {
1297         return JVMTI_ERROR_INVALID_CLASS_FORMAT;
1298       } else if (ex_name == vmSymbols::java_lang_ClassCircularityError()) {
1299         return JVMTI_ERROR_CIRCULAR_CLASS_DEFINITION;
1300       } else if (ex_name == vmSymbols::java_lang_NoClassDefFoundError()) {
1301         // The message will be &quot;XXX (wrong name: YYY)&quot;
1302         return JVMTI_ERROR_NAMES_DONT_MATCH;
1303       } else if (ex_name == vmSymbols::java_lang_OutOfMemoryError()) {
1304         return JVMTI_ERROR_OUT_OF_MEMORY;
1305       } else {  // Just in case more exceptions can be thrown..
1306         return JVMTI_ERROR_FAILS_VERIFICATION;
1307       }
1308     }
1309 
1310     // Ensure class is linked before redefine
1311     if (!the_class-&gt;is_linked()) {
1312       the_class-&gt;link_class(THREAD);
1313       if (HAS_PENDING_EXCEPTION) {
1314         Symbol* ex_name = PENDING_EXCEPTION-&gt;klass()-&gt;name();
1315         oop message = java_lang_Throwable::message(PENDING_EXCEPTION);
1316         if (message != NULL) {
1317           char* ex_msg = java_lang_String::as_utf8_string(message);
1318           log_info(redefine, class, load, exceptions)(&quot;link_class exception: &#39;%s %s&#39;&quot;,
1319                    ex_name-&gt;as_C_string(), ex_msg);
1320         } else {
1321           log_info(redefine, class, load, exceptions)(&quot;link_class exception: &#39;%s&#39;&quot;,
1322                    ex_name-&gt;as_C_string());
1323         }
1324         CLEAR_PENDING_EXCEPTION;
1325         if (ex_name == vmSymbols::java_lang_OutOfMemoryError()) {
1326           return JVMTI_ERROR_OUT_OF_MEMORY;
1327         } else if (ex_name == vmSymbols::java_lang_NoClassDefFoundError()) {
1328           return JVMTI_ERROR_INVALID_CLASS;
1329         } else {
1330           return JVMTI_ERROR_INTERNAL;
1331         }
1332       }
1333     }
1334 
1335     // Do the validity checks in compare_and_normalize_class_versions()
1336     // before verifying the byte codes. By doing these checks first, we
1337     // limit the number of functions that require redirection from
1338     // the_class to scratch_class. In particular, we don&#39;t have to
1339     // modify JNI GetSuperclass() and thus won&#39;t change its performance.
1340     jvmtiError res = compare_and_normalize_class_versions(the_class,
1341                        scratch_class);
1342     if (res != JVMTI_ERROR_NONE) {
1343       return res;
1344     }
1345 
1346     // verify what the caller passed us
1347     {
1348       // The bug 6214132 caused the verification to fail.
1349       // Information about the_class and scratch_class is temporarily
1350       // recorded into jvmtiThreadState. This data is used to redirect
1351       // the_class to scratch_class in the JVM_* functions called by the
1352       // verifier. Please, refer to jvmtiThreadState.hpp for the detailed
1353       // description.
1354       RedefineVerifyMark rvm(the_class, scratch_class, state);
1355       Verifier::verify(scratch_class, true, THREAD);
1356     }
1357 
1358     if (HAS_PENDING_EXCEPTION) {
1359       Symbol* ex_name = PENDING_EXCEPTION-&gt;klass()-&gt;name();
1360       log_info(redefine, class, load, exceptions)(&quot;verify_byte_codes exception: &#39;%s&#39;&quot;, ex_name-&gt;as_C_string());
1361       CLEAR_PENDING_EXCEPTION;
1362       if (ex_name == vmSymbols::java_lang_OutOfMemoryError()) {
1363         return JVMTI_ERROR_OUT_OF_MEMORY;
1364       } else {
1365         // tell the caller the bytecodes are bad
1366         return JVMTI_ERROR_FAILS_VERIFICATION;
1367       }
1368     }
1369 
1370     res = merge_cp_and_rewrite(the_class, scratch_class, THREAD);
1371     if (HAS_PENDING_EXCEPTION) {
1372       Symbol* ex_name = PENDING_EXCEPTION-&gt;klass()-&gt;name();
1373       log_info(redefine, class, load, exceptions)(&quot;merge_cp_and_rewrite exception: &#39;%s&#39;&quot;, ex_name-&gt;as_C_string());
1374       CLEAR_PENDING_EXCEPTION;
1375       if (ex_name == vmSymbols::java_lang_OutOfMemoryError()) {
1376         return JVMTI_ERROR_OUT_OF_MEMORY;
1377       } else {
1378         return JVMTI_ERROR_INTERNAL;
1379       }
1380     }
1381 
1382     if (VerifyMergedCPBytecodes) {
1383       // verify what we have done during constant pool merging
1384       {
1385         RedefineVerifyMark rvm(the_class, scratch_class, state);
1386         Verifier::verify(scratch_class, true, THREAD);
1387       }
1388 
1389       if (HAS_PENDING_EXCEPTION) {
1390         Symbol* ex_name = PENDING_EXCEPTION-&gt;klass()-&gt;name();
1391         log_info(redefine, class, load, exceptions)
1392           (&quot;verify_byte_codes post merge-CP exception: &#39;%s&#39;&quot;, ex_name-&gt;as_C_string());
1393         CLEAR_PENDING_EXCEPTION;
1394         if (ex_name == vmSymbols::java_lang_OutOfMemoryError()) {
1395           return JVMTI_ERROR_OUT_OF_MEMORY;
1396         } else {
1397           // tell the caller that constant pool merging screwed up
1398           return JVMTI_ERROR_INTERNAL;
1399         }
1400       }
1401     }
1402 
1403     Rewriter::rewrite(scratch_class, THREAD);
1404     if (!HAS_PENDING_EXCEPTION) {
1405       scratch_class-&gt;link_methods(THREAD);
1406     }
1407     if (HAS_PENDING_EXCEPTION) {
1408       Symbol* ex_name = PENDING_EXCEPTION-&gt;klass()-&gt;name();
1409       log_info(redefine, class, load, exceptions)
1410         (&quot;Rewriter::rewrite or link_methods exception: &#39;%s&#39;&quot;, ex_name-&gt;as_C_string());
1411       CLEAR_PENDING_EXCEPTION;
1412       if (ex_name == vmSymbols::java_lang_OutOfMemoryError()) {
1413         return JVMTI_ERROR_OUT_OF_MEMORY;
1414       } else {
1415         return JVMTI_ERROR_INTERNAL;
1416       }
1417     }
1418 
1419     log_debug(redefine, class, load)
1420       (&quot;loaded name=%s (avail_mem=&quot; UINT64_FORMAT &quot;K)&quot;, the_class-&gt;external_name(), os::available_memory() &gt;&gt; 10);
1421   }
1422 
1423   return JVMTI_ERROR_NONE;
1424 }
1425 
1426 
1427 // Map old_index to new_index as needed. scratch_cp is only needed
1428 // for log calls.
1429 void VM_RedefineClasses::map_index(const constantPoolHandle&amp; scratch_cp,
1430        int old_index, int new_index) {
1431   if (find_new_index(old_index) != 0) {
1432     // old_index is already mapped
1433     return;
1434   }
1435 
1436   if (old_index == new_index) {
1437     // no mapping is needed
1438     return;
1439   }
1440 
1441   _index_map_p-&gt;at_put(old_index, new_index);
1442   _index_map_count++;
1443 
1444   log_trace(redefine, class, constantpool)
1445     (&quot;mapped tag %d at index %d to %d&quot;, scratch_cp-&gt;tag_at(old_index).value(), old_index, new_index);
1446 } // end map_index()
1447 
1448 
1449 // Map old_index to new_index as needed.
1450 void VM_RedefineClasses::map_operand_index(int old_index, int new_index) {
1451   if (find_new_operand_index(old_index) != -1) {
1452     // old_index is already mapped
1453     return;
1454   }
1455 
1456   if (old_index == new_index) {
1457     // no mapping is needed
1458     return;
1459   }
1460 
1461   _operands_index_map_p-&gt;at_put(old_index, new_index);
1462   _operands_index_map_count++;
1463 
1464   log_trace(redefine, class, constantpool)(&quot;mapped bootstrap specifier at index %d to %d&quot;, old_index, new_index);
1465 } // end map_index()
1466 
1467 
1468 // Merge old_cp and scratch_cp and return the results of the merge via
1469 // merge_cp_p. The number of entries in *merge_cp_p is returned via
1470 // merge_cp_length_p. The entries in old_cp occupy the same locations
1471 // in *merge_cp_p. Also creates a map of indices from entries in
1472 // scratch_cp to the corresponding entry in *merge_cp_p. Index map
1473 // entries are only created for entries in scratch_cp that occupy a
1474 // different location in *merged_cp_p.
1475 bool VM_RedefineClasses::merge_constant_pools(const constantPoolHandle&amp; old_cp,
1476        const constantPoolHandle&amp; scratch_cp, constantPoolHandle *merge_cp_p,
1477        int *merge_cp_length_p, TRAPS) {
1478 
1479   if (merge_cp_p == NULL) {
1480     assert(false, &quot;caller must provide scratch constantPool&quot;);
1481     return false; // robustness
1482   }
1483   if (merge_cp_length_p == NULL) {
1484     assert(false, &quot;caller must provide scratch CP length&quot;);
1485     return false; // robustness
1486   }
1487   // Worst case we need old_cp-&gt;length() + scratch_cp()-&gt;length(),
1488   // but the caller might be smart so make sure we have at least
1489   // the minimum.
1490   if ((*merge_cp_p)-&gt;length() &lt; old_cp-&gt;length()) {
1491     assert(false, &quot;merge area too small&quot;);
1492     return false; // robustness
1493   }
1494 
1495   log_info(redefine, class, constantpool)(&quot;old_cp_len=%d, scratch_cp_len=%d&quot;, old_cp-&gt;length(), scratch_cp-&gt;length());
1496 
1497   {
1498     // Pass 0:
1499     // The old_cp is copied to *merge_cp_p; this means that any code
1500     // using old_cp does not have to change. This work looks like a
1501     // perfect fit for ConstantPool*::copy_cp_to(), but we need to
1502     // handle one special case:
1503     // - revert JVM_CONSTANT_Class to JVM_CONSTANT_UnresolvedClass
1504     // This will make verification happy.
1505 
1506     int old_i;  // index into old_cp
1507 
1508     // index zero (0) is not used in constantPools
1509     for (old_i = 1; old_i &lt; old_cp-&gt;length(); old_i++) {
1510       // leave debugging crumb
1511       jbyte old_tag = old_cp-&gt;tag_at(old_i).value();
1512       switch (old_tag) {
1513       case JVM_CONSTANT_Class:
1514       case JVM_CONSTANT_UnresolvedClass:
1515         // revert the copy to JVM_CONSTANT_UnresolvedClass
1516         // May be resolving while calling this so do the same for
1517         // JVM_CONSTANT_UnresolvedClass (klass_name_at() deals with transition)
1518         (*merge_cp_p)-&gt;temp_unresolved_klass_at_put(old_i,
1519           old_cp-&gt;klass_name_index_at(old_i));
1520         break;
1521 
1522       case JVM_CONSTANT_Double:
1523       case JVM_CONSTANT_Long:
1524         // just copy the entry to *merge_cp_p, but double and long take
1525         // two constant pool entries
1526         ConstantPool::copy_entry_to(old_cp, old_i, *merge_cp_p, old_i, CHECK_false);
1527         old_i++;
1528         break;
1529 
1530       default:
1531         // just copy the entry to *merge_cp_p
1532         ConstantPool::copy_entry_to(old_cp, old_i, *merge_cp_p, old_i, CHECK_false);
1533         break;
1534       }
1535     } // end for each old_cp entry
1536 
1537     ConstantPool::copy_operands(old_cp, *merge_cp_p, CHECK_false);
1538     (*merge_cp_p)-&gt;extend_operands(scratch_cp, CHECK_false);
1539 
1540     // We don&#39;t need to sanity check that *merge_cp_length_p is within
1541     // *merge_cp_p bounds since we have the minimum on-entry check above.
1542     (*merge_cp_length_p) = old_i;
1543   }
1544 
1545   // merge_cp_len should be the same as old_cp-&gt;length() at this point
1546   // so this trace message is really a &quot;warm-and-breathing&quot; message.
1547   log_debug(redefine, class, constantpool)(&quot;after pass 0: merge_cp_len=%d&quot;, *merge_cp_length_p);
1548 
1549   int scratch_i;  // index into scratch_cp
1550   {
1551     // Pass 1a:
1552     // Compare scratch_cp entries to the old_cp entries that we have
1553     // already copied to *merge_cp_p. In this pass, we are eliminating
1554     // exact duplicates (matching entry at same index) so we only
1555     // compare entries in the common indice range.
1556     int increment = 1;
1557     int pass1a_length = MIN2(old_cp-&gt;length(), scratch_cp-&gt;length());
1558     for (scratch_i = 1; scratch_i &lt; pass1a_length; scratch_i += increment) {
1559       switch (scratch_cp-&gt;tag_at(scratch_i).value()) {
1560       case JVM_CONSTANT_Double:
1561       case JVM_CONSTANT_Long:
1562         // double and long take two constant pool entries
1563         increment = 2;
1564         break;
1565 
1566       default:
1567         increment = 1;
1568         break;
1569       }
1570 
1571       bool match = scratch_cp-&gt;compare_entry_to(scratch_i, *merge_cp_p,
1572         scratch_i, CHECK_false);
1573       if (match) {
1574         // found a match at the same index so nothing more to do
1575         continue;
1576       } else if (is_unresolved_class_mismatch(scratch_cp, scratch_i,
1577                                               *merge_cp_p, scratch_i)) {
1578         // The mismatch in compare_entry_to() above is because of a
1579         // resolved versus unresolved class entry at the same index
1580         // with the same string value. Since Pass 0 reverted any
1581         // class entries to unresolved class entries in *merge_cp_p,
1582         // we go with the unresolved class entry.
1583         continue;
1584       }
1585 
1586       int found_i = scratch_cp-&gt;find_matching_entry(scratch_i, *merge_cp_p,
1587         CHECK_false);
1588       if (found_i != 0) {
1589         guarantee(found_i != scratch_i,
1590           &quot;compare_entry_to() and find_matching_entry() do not agree&quot;);
1591 
1592         // Found a matching entry somewhere else in *merge_cp_p so
1593         // just need a mapping entry.
1594         map_index(scratch_cp, scratch_i, found_i);
1595         continue;
1596       }
1597 
1598       // The find_matching_entry() call above could fail to find a match
1599       // due to a resolved versus unresolved class or string entry situation
1600       // like we solved above with the is_unresolved_*_mismatch() calls.
1601       // However, we would have to call is_unresolved_*_mismatch() over
1602       // all of *merge_cp_p (potentially) and that doesn&#39;t seem to be
1603       // worth the time.
1604 
1605       // No match found so we have to append this entry and any unique
1606       // referenced entries to *merge_cp_p.
1607       append_entry(scratch_cp, scratch_i, merge_cp_p, merge_cp_length_p,
1608         CHECK_false);
1609     }
1610   }
1611 
1612   log_debug(redefine, class, constantpool)
1613     (&quot;after pass 1a: merge_cp_len=%d, scratch_i=%d, index_map_len=%d&quot;,
1614      *merge_cp_length_p, scratch_i, _index_map_count);
1615 
1616   if (scratch_i &lt; scratch_cp-&gt;length()) {
1617     // Pass 1b:
1618     // old_cp is smaller than scratch_cp so there are entries in
1619     // scratch_cp that we have not yet processed. We take care of
1620     // those now.
1621     int increment = 1;
1622     for (; scratch_i &lt; scratch_cp-&gt;length(); scratch_i += increment) {
1623       switch (scratch_cp-&gt;tag_at(scratch_i).value()) {
1624       case JVM_CONSTANT_Double:
1625       case JVM_CONSTANT_Long:
1626         // double and long take two constant pool entries
1627         increment = 2;
1628         break;
1629 
1630       default:
1631         increment = 1;
1632         break;
1633       }
1634 
1635       int found_i =
1636         scratch_cp-&gt;find_matching_entry(scratch_i, *merge_cp_p, CHECK_false);
1637       if (found_i != 0) {
1638         // Found a matching entry somewhere else in *merge_cp_p so
1639         // just need a mapping entry.
1640         map_index(scratch_cp, scratch_i, found_i);
1641         continue;
1642       }
1643 
1644       // No match found so we have to append this entry and any unique
1645       // referenced entries to *merge_cp_p.
1646       append_entry(scratch_cp, scratch_i, merge_cp_p, merge_cp_length_p,
1647         CHECK_false);
1648     }
1649 
1650     log_debug(redefine, class, constantpool)
1651       (&quot;after pass 1b: merge_cp_len=%d, scratch_i=%d, index_map_len=%d&quot;,
1652        *merge_cp_length_p, scratch_i, _index_map_count);
1653   }
1654   finalize_operands_merge(*merge_cp_p, THREAD);
1655 
1656   return true;
1657 } // end merge_constant_pools()
1658 
1659 
1660 // Scoped object to clean up the constant pool(s) created for merging
1661 class MergeCPCleaner {
1662   ClassLoaderData*   _loader_data;
1663   ConstantPool*      _cp;
1664   ConstantPool*      _scratch_cp;
1665  public:
1666   MergeCPCleaner(ClassLoaderData* loader_data, ConstantPool* merge_cp) :
1667                  _loader_data(loader_data), _cp(merge_cp), _scratch_cp(NULL) {}
1668   ~MergeCPCleaner() {
1669     _loader_data-&gt;add_to_deallocate_list(_cp);
1670     if (_scratch_cp != NULL) {
1671       _loader_data-&gt;add_to_deallocate_list(_scratch_cp);
1672     }
1673   }
1674   void add_scratch_cp(ConstantPool* scratch_cp) { _scratch_cp = scratch_cp; }
1675 };
1676 
1677 // Merge constant pools between the_class and scratch_class and
1678 // potentially rewrite bytecodes in scratch_class to use the merged
1679 // constant pool.
1680 jvmtiError VM_RedefineClasses::merge_cp_and_rewrite(
1681              InstanceKlass* the_class, InstanceKlass* scratch_class,
1682              TRAPS) {
1683   // worst case merged constant pool length is old and new combined
1684   int merge_cp_length = the_class-&gt;constants()-&gt;length()
1685         + scratch_class-&gt;constants()-&gt;length();
1686 
1687   // Constant pools are not easily reused so we allocate a new one
1688   // each time.
1689   // merge_cp is created unsafe for concurrent GC processing.  It
1690   // should be marked safe before discarding it. Even though
1691   // garbage,  if it crosses a card boundary, it may be scanned
1692   // in order to find the start of the first complete object on the card.
1693   ClassLoaderData* loader_data = the_class-&gt;class_loader_data();
1694   ConstantPool* merge_cp_oop =
1695     ConstantPool::allocate(loader_data,
1696                            merge_cp_length,
1697                            CHECK_(JVMTI_ERROR_OUT_OF_MEMORY));
1698   MergeCPCleaner cp_cleaner(loader_data, merge_cp_oop);
1699 
1700   HandleMark hm(THREAD);  // make sure handles are cleared before
1701                           // MergeCPCleaner clears out merge_cp_oop
1702   constantPoolHandle merge_cp(THREAD, merge_cp_oop);
1703 
1704   // Get constants() from the old class because it could have been rewritten
1705   // while we were at a safepoint allocating a new constant pool.
1706   constantPoolHandle old_cp(THREAD, the_class-&gt;constants());
1707   constantPoolHandle scratch_cp(THREAD, scratch_class-&gt;constants());
1708 
1709   // If the length changed, the class was redefined out from under us. Return
1710   // an error.
1711   if (merge_cp_length != the_class-&gt;constants()-&gt;length()
1712          + scratch_class-&gt;constants()-&gt;length()) {
1713     return JVMTI_ERROR_INTERNAL;
1714   }
1715 
1716   // Update the version number of the constant pools (may keep scratch_cp)
1717   merge_cp-&gt;increment_and_save_version(old_cp-&gt;version());
1718   scratch_cp-&gt;increment_and_save_version(old_cp-&gt;version());
1719 
1720   ResourceMark rm(THREAD);
1721   _index_map_count = 0;
1722   _index_map_p = new intArray(scratch_cp-&gt;length(), scratch_cp-&gt;length(), -1);
1723 
1724   _operands_cur_length = ConstantPool::operand_array_length(old_cp-&gt;operands());
1725   _operands_index_map_count = 0;
1726   int operands_index_map_len = ConstantPool::operand_array_length(scratch_cp-&gt;operands());
1727   _operands_index_map_p = new intArray(operands_index_map_len, operands_index_map_len, -1);
1728 
1729   // reference to the cp holder is needed for copy_operands()
1730   merge_cp-&gt;set_pool_holder(scratch_class);
1731   bool result = merge_constant_pools(old_cp, scratch_cp, &amp;merge_cp,
1732                   &amp;merge_cp_length, THREAD);
1733   merge_cp-&gt;set_pool_holder(NULL);
1734 
1735   if (!result) {
1736     // The merge can fail due to memory allocation failure or due
1737     // to robustness checks.
1738     return JVMTI_ERROR_INTERNAL;
1739   }
1740 
1741   // Save fields from the old_cp.
1742   merge_cp-&gt;copy_fields(old_cp());
1743   scratch_cp-&gt;copy_fields(old_cp());
1744 
1745   log_info(redefine, class, constantpool)(&quot;merge_cp_len=%d, index_map_len=%d&quot;, merge_cp_length, _index_map_count);
1746 
1747   if (_index_map_count == 0) {
1748     // there is nothing to map between the new and merged constant pools
1749 
1750     if (old_cp-&gt;length() == scratch_cp-&gt;length()) {
1751       // The old and new constant pools are the same length and the
1752       // index map is empty. This means that the three constant pools
1753       // are equivalent (but not the same). Unfortunately, the new
1754       // constant pool has not gone through link resolution nor have
1755       // the new class bytecodes gone through constant pool cache
1756       // rewriting so we can&#39;t use the old constant pool with the new
1757       // class.
1758 
1759       // toss the merged constant pool at return
1760     } else if (old_cp-&gt;length() &lt; scratch_cp-&gt;length()) {
1761       // The old constant pool has fewer entries than the new constant
1762       // pool and the index map is empty. This means the new constant
1763       // pool is a superset of the old constant pool. However, the old
1764       // class bytecodes have already gone through constant pool cache
1765       // rewriting so we can&#39;t use the new constant pool with the old
1766       // class.
1767 
1768       // toss the merged constant pool at return
1769     } else {
1770       // The old constant pool has more entries than the new constant
1771       // pool and the index map is empty. This means that both the old
1772       // and merged constant pools are supersets of the new constant
1773       // pool.
1774 
1775       // Replace the new constant pool with a shrunken copy of the
1776       // merged constant pool
1777       set_new_constant_pool(loader_data, scratch_class, merge_cp, merge_cp_length,
1778                             CHECK_(JVMTI_ERROR_OUT_OF_MEMORY));
1779       // The new constant pool replaces scratch_cp so have cleaner clean it up.
1780       // It can&#39;t be cleaned up while there are handles to it.
1781       cp_cleaner.add_scratch_cp(scratch_cp());
1782     }
1783   } else {
1784     if (log_is_enabled(Trace, redefine, class, constantpool)) {
1785       // don&#39;t want to loop unless we are tracing
1786       int count = 0;
1787       for (int i = 1; i &lt; _index_map_p-&gt;length(); i++) {
1788         int value = _index_map_p-&gt;at(i);
1789 
1790         if (value != -1) {
1791           log_trace(redefine, class, constantpool)(&quot;index_map[%d]: old=%d new=%d&quot;, count, i, value);
1792           count++;
1793         }
1794       }
1795     }
1796 
1797     // We have entries mapped between the new and merged constant pools
1798     // so we have to rewrite some constant pool references.
1799     if (!rewrite_cp_refs(scratch_class, THREAD)) {
1800       return JVMTI_ERROR_INTERNAL;
1801     }
1802 
1803     // Replace the new constant pool with a shrunken copy of the
1804     // merged constant pool so now the rewritten bytecodes have
1805     // valid references; the previous new constant pool will get
1806     // GCed.
1807     set_new_constant_pool(loader_data, scratch_class, merge_cp, merge_cp_length,
1808                           CHECK_(JVMTI_ERROR_OUT_OF_MEMORY));
1809     // The new constant pool replaces scratch_cp so have cleaner clean it up.
1810     // It can&#39;t be cleaned up while there are handles to it.
1811     cp_cleaner.add_scratch_cp(scratch_cp());
1812   }
1813 
1814   return JVMTI_ERROR_NONE;
1815 } // end merge_cp_and_rewrite()
1816 
1817 
1818 // Rewrite constant pool references in klass scratch_class.
1819 bool VM_RedefineClasses::rewrite_cp_refs(InstanceKlass* scratch_class,
1820        TRAPS) {
1821 
1822   // rewrite constant pool references in the nest attributes:
1823   if (!rewrite_cp_refs_in_nest_attributes(scratch_class)) {
1824     // propagate failure back to caller
1825     return false;
1826   }
1827 
1828   // rewrite constant pool references in the Record attribute:
1829   if (!rewrite_cp_refs_in_record_attribute(scratch_class, THREAD)) {
1830     // propagate failure back to caller
1831     return false;
1832   }
1833 
1834   // rewrite constant pool references in the PermittedSubclasses attribute:
1835   if (!rewrite_cp_refs_in_permitted_subclasses_attribute(scratch_class)) {
1836     // propagate failure back to caller
1837     return false;
1838   }
1839 
1840   // rewrite constant pool references in the methods:
1841   if (!rewrite_cp_refs_in_methods(scratch_class, THREAD)) {
1842     // propagate failure back to caller
1843     return false;
1844   }
1845 
1846   // rewrite constant pool references in the class_annotations:
1847   if (!rewrite_cp_refs_in_class_annotations(scratch_class, THREAD)) {
1848     // propagate failure back to caller
1849     return false;
1850   }
1851 
1852   // rewrite constant pool references in the fields_annotations:
1853   if (!rewrite_cp_refs_in_fields_annotations(scratch_class, THREAD)) {
1854     // propagate failure back to caller
1855     return false;
1856   }
1857 
1858   // rewrite constant pool references in the methods_annotations:
1859   if (!rewrite_cp_refs_in_methods_annotations(scratch_class, THREAD)) {
1860     // propagate failure back to caller
1861     return false;
1862   }
1863 
1864   // rewrite constant pool references in the methods_parameter_annotations:
1865   if (!rewrite_cp_refs_in_methods_parameter_annotations(scratch_class,
1866          THREAD)) {
1867     // propagate failure back to caller
1868     return false;
1869   }
1870 
1871   // rewrite constant pool references in the methods_default_annotations:
1872   if (!rewrite_cp_refs_in_methods_default_annotations(scratch_class,
1873          THREAD)) {
1874     // propagate failure back to caller
1875     return false;
1876   }
1877 
1878   // rewrite constant pool references in the class_type_annotations:
1879   if (!rewrite_cp_refs_in_class_type_annotations(scratch_class, THREAD)) {
1880     // propagate failure back to caller
1881     return false;
1882   }
1883 
1884   // rewrite constant pool references in the fields_type_annotations:
1885   if (!rewrite_cp_refs_in_fields_type_annotations(scratch_class, THREAD)) {
1886     // propagate failure back to caller
1887     return false;
1888   }
1889 
1890   // rewrite constant pool references in the methods_type_annotations:
1891   if (!rewrite_cp_refs_in_methods_type_annotations(scratch_class, THREAD)) {
1892     // propagate failure back to caller
1893     return false;
1894   }
1895 
1896   // There can be type annotations in the Code part of a method_info attribute.
1897   // These annotations are not accessible, even by reflection.
1898   // Currently they are not even parsed by the ClassFileParser.
1899   // If runtime access is added they will also need to be rewritten.
1900 
1901   // rewrite source file name index:
1902   u2 source_file_name_idx = scratch_class-&gt;source_file_name_index();
1903   if (source_file_name_idx != 0) {
1904     u2 new_source_file_name_idx = find_new_index(source_file_name_idx);
1905     if (new_source_file_name_idx != 0) {
1906       scratch_class-&gt;set_source_file_name_index(new_source_file_name_idx);
1907     }
1908   }
1909 
1910   // rewrite class generic signature index:
1911   u2 generic_signature_index = scratch_class-&gt;generic_signature_index();
1912   if (generic_signature_index != 0) {
1913     u2 new_generic_signature_index = find_new_index(generic_signature_index);
1914     if (new_generic_signature_index != 0) {
1915       scratch_class-&gt;set_generic_signature_index(new_generic_signature_index);
1916     }
1917   }
1918 
1919   return true;
1920 } // end rewrite_cp_refs()
1921 
1922 // Rewrite constant pool references in the NestHost and NestMembers attributes.
1923 bool VM_RedefineClasses::rewrite_cp_refs_in_nest_attributes(
1924        InstanceKlass* scratch_class) {
1925 
1926   u2 cp_index = scratch_class-&gt;nest_host_index();
1927   if (cp_index != 0) {
1928     scratch_class-&gt;set_nest_host_index(find_new_index(cp_index));
1929   }
1930   Array&lt;u2&gt;* nest_members = scratch_class-&gt;nest_members();
1931   for (int i = 0; i &lt; nest_members-&gt;length(); i++) {
1932     u2 cp_index = nest_members-&gt;at(i);
1933     nest_members-&gt;at_put(i, find_new_index(cp_index));
1934   }
1935   return true;
1936 }
1937 
1938 // Rewrite constant pool references in the Record attribute.
1939 bool VM_RedefineClasses::rewrite_cp_refs_in_record_attribute(
1940        InstanceKlass* scratch_class, TRAPS) {
1941   Array&lt;RecordComponent*&gt;* components = scratch_class-&gt;record_components();
1942   if (components != NULL) {
1943     for (int i = 0; i &lt; components-&gt;length(); i++) {
1944       RecordComponent* component = components-&gt;at(i);
1945       u2 cp_index = component-&gt;name_index();
1946       component-&gt;set_name_index(find_new_index(cp_index));
1947       cp_index = component-&gt;descriptor_index();
1948       component-&gt;set_descriptor_index(find_new_index(cp_index));
1949       cp_index = component-&gt;generic_signature_index();
1950       if (cp_index != 0) {
1951         component-&gt;set_generic_signature_index(find_new_index(cp_index));
1952       }
1953 
1954       AnnotationArray* annotations = component-&gt;annotations();
1955       if (annotations != NULL &amp;&amp; annotations-&gt;length() != 0) {
1956         int byte_i = 0;  // byte index into annotations
1957         if (!rewrite_cp_refs_in_annotations_typeArray(annotations, byte_i, THREAD)) {
1958           log_debug(redefine, class, annotation)(&quot;bad record_component_annotations at %d&quot;, i);
1959           // propagate failure back to caller
1960           return false;
1961         }
1962       }
1963 
1964       AnnotationArray* type_annotations = component-&gt;type_annotations();
1965       if (type_annotations != NULL &amp;&amp; type_annotations-&gt;length() != 0) {
1966         int byte_i = 0;  // byte index into annotations
1967         if (!rewrite_cp_refs_in_annotations_typeArray(type_annotations, byte_i, THREAD)) {
1968           log_debug(redefine, class, annotation)(&quot;bad record_component_type_annotations at %d&quot;, i);
1969           // propagate failure back to caller
1970           return false;
1971         }
1972       }
1973     }
1974   }
1975   return true;
1976 }
1977 
1978 // Rewrite constant pool references in the PermittedSubclasses attribute.
1979 bool VM_RedefineClasses::rewrite_cp_refs_in_permitted_subclasses_attribute(
1980        InstanceKlass* scratch_class) {
1981 
1982   Array&lt;u2&gt;* permitted_subclasses = scratch_class-&gt;permitted_subclasses();
1983   assert(permitted_subclasses != NULL, &quot;unexpected null permitted_subclasses&quot;);
1984   for (int i = 0; i &lt; permitted_subclasses-&gt;length(); i++) {
1985     u2 cp_index = permitted_subclasses-&gt;at(i);
1986     permitted_subclasses-&gt;at_put(i, find_new_index(cp_index));
1987   }
1988   return true;
1989 }
1990 
1991 // Rewrite constant pool references in the methods.
1992 bool VM_RedefineClasses::rewrite_cp_refs_in_methods(
1993        InstanceKlass* scratch_class, TRAPS) {
1994 
1995   Array&lt;Method*&gt;* methods = scratch_class-&gt;methods();
1996 
1997   if (methods == NULL || methods-&gt;length() == 0) {
1998     // no methods so nothing to do
1999     return true;
2000   }
2001 
2002   // rewrite constant pool references in the methods:
2003   for (int i = methods-&gt;length() - 1; i &gt;= 0; i--) {
2004     methodHandle method(THREAD, methods-&gt;at(i));
2005     methodHandle new_method;
2006     rewrite_cp_refs_in_method(method, &amp;new_method, THREAD);
2007     if (!new_method.is_null()) {
2008       // the method has been replaced so save the new method version
2009       // even in the case of an exception.  original method is on the
2010       // deallocation list.
2011       methods-&gt;at_put(i, new_method());
2012     }
2013     if (HAS_PENDING_EXCEPTION) {
2014       Symbol* ex_name = PENDING_EXCEPTION-&gt;klass()-&gt;name();
2015       log_info(redefine, class, load, exceptions)(&quot;rewrite_cp_refs_in_method exception: &#39;%s&#39;&quot;, ex_name-&gt;as_C_string());
2016       // Need to clear pending exception here as the super caller sets
2017       // the JVMTI_ERROR_INTERNAL if the returned value is false.
2018       CLEAR_PENDING_EXCEPTION;
2019       return false;
2020     }
2021   }
2022 
2023   return true;
2024 }
2025 
2026 
2027 // Rewrite constant pool references in the specific method. This code
2028 // was adapted from Rewriter::rewrite_method().
2029 void VM_RedefineClasses::rewrite_cp_refs_in_method(methodHandle method,
2030        methodHandle *new_method_p, TRAPS) {
2031 
2032   *new_method_p = methodHandle();  // default is no new method
2033 
2034   // We cache a pointer to the bytecodes here in code_base. If GC
2035   // moves the Method*, then the bytecodes will also move which
2036   // will likely cause a crash. We create a NoSafepointVerifier
2037   // object to detect whether we pass a possible safepoint in this
2038   // code block.
2039   NoSafepointVerifier nsv;
2040 
2041   // Bytecodes and their length
2042   address code_base = method-&gt;code_base();
2043   int code_length = method-&gt;code_size();
2044 
2045   int bc_length;
2046   for (int bci = 0; bci &lt; code_length; bci += bc_length) {
2047     address bcp = code_base + bci;
2048     Bytecodes::Code c = (Bytecodes::Code)(*bcp);
2049 
2050     bc_length = Bytecodes::length_for(c);
2051     if (bc_length == 0) {
2052       // More complicated bytecodes report a length of zero so
2053       // we have to try again a slightly different way.
2054       bc_length = Bytecodes::length_at(method(), bcp);
2055     }
2056 
2057     assert(bc_length != 0, &quot;impossible bytecode length&quot;);
2058 
2059     switch (c) {
2060       case Bytecodes::_ldc:
2061       {
2062         int cp_index = *(bcp + 1);
2063         int new_index = find_new_index(cp_index);
2064 
2065         if (StressLdcRewrite &amp;&amp; new_index == 0) {
2066           // If we are stressing ldc -&gt; ldc_w rewriting, then we
2067           // always need a new_index value.
2068           new_index = cp_index;
2069         }
2070         if (new_index != 0) {
2071           // the original index is mapped so we have more work to do
2072           if (!StressLdcRewrite &amp;&amp; new_index &lt;= max_jubyte) {
2073             // The new value can still use ldc instead of ldc_w
2074             // unless we are trying to stress ldc -&gt; ldc_w rewriting
2075             log_trace(redefine, class, constantpool)
2076               (&quot;%s@&quot; INTPTR_FORMAT &quot; old=%d, new=%d&quot;, Bytecodes::name(c), p2i(bcp), cp_index, new_index);
2077             *(bcp + 1) = new_index;
2078           } else {
2079             log_trace(redefine, class, constantpool)
2080               (&quot;%s-&gt;ldc_w@&quot; INTPTR_FORMAT &quot; old=%d, new=%d&quot;, Bytecodes::name(c), p2i(bcp), cp_index, new_index);
2081             // the new value needs ldc_w instead of ldc
2082             u_char inst_buffer[4]; // max instruction size is 4 bytes
2083             bcp = (address)inst_buffer;
2084             // construct new instruction sequence
2085             *bcp = Bytecodes::_ldc_w;
2086             bcp++;
2087             // Rewriter::rewrite_method() does not rewrite ldc -&gt; ldc_w.
2088             // See comment below for difference between put_Java_u2()
2089             // and put_native_u2().
2090             Bytes::put_Java_u2(bcp, new_index);
2091 
2092             Relocator rc(method, NULL /* no RelocatorListener needed */);
2093             methodHandle m;
2094             {
2095               PauseNoSafepointVerifier pnsv(&amp;nsv);
2096 
2097               // ldc is 2 bytes and ldc_w is 3 bytes
2098               m = rc.insert_space_at(bci, 3, inst_buffer, CHECK);
2099             }
2100 
2101             // return the new method so that the caller can update
2102             // the containing class
2103             *new_method_p = method = m;
2104             // switch our bytecode processing loop from the old method
2105             // to the new method
2106             code_base = method-&gt;code_base();
2107             code_length = method-&gt;code_size();
2108             bcp = code_base + bci;
2109             c = (Bytecodes::Code)(*bcp);
2110             bc_length = Bytecodes::length_for(c);
2111             assert(bc_length != 0, &quot;sanity check&quot;);
2112           } // end we need ldc_w instead of ldc
2113         } // end if there is a mapped index
2114       } break;
2115 
2116       // these bytecodes have a two-byte constant pool index
2117       case Bytecodes::_anewarray      : // fall through
2118       case Bytecodes::_checkcast      : // fall through
2119       case Bytecodes::_getfield       : // fall through
2120       case Bytecodes::_getstatic      : // fall through
2121       case Bytecodes::_instanceof     : // fall through
2122       case Bytecodes::_invokedynamic  : // fall through
2123       case Bytecodes::_invokeinterface: // fall through
2124       case Bytecodes::_invokespecial  : // fall through
2125       case Bytecodes::_invokestatic   : // fall through
2126       case Bytecodes::_invokevirtual  : // fall through
2127       case Bytecodes::_ldc_w          : // fall through
2128       case Bytecodes::_ldc2_w         : // fall through
2129       case Bytecodes::_multianewarray : // fall through
2130       case Bytecodes::_new            : // fall through
2131       case Bytecodes::_putfield       : // fall through
2132       case Bytecodes::_putstatic      :
2133       {
2134         address p = bcp + 1;
2135         int cp_index = Bytes::get_Java_u2(p);
2136         int new_index = find_new_index(cp_index);
2137         if (new_index != 0) {
2138           // the original index is mapped so update w/ new value
2139           log_trace(redefine, class, constantpool)
2140             (&quot;%s@&quot; INTPTR_FORMAT &quot; old=%d, new=%d&quot;, Bytecodes::name(c),p2i(bcp), cp_index, new_index);
2141           // Rewriter::rewrite_method() uses put_native_u2() in this
2142           // situation because it is reusing the constant pool index
2143           // location for a native index into the ConstantPoolCache.
2144           // Since we are updating the constant pool index prior to
2145           // verification and ConstantPoolCache initialization, we
2146           // need to keep the new index in Java byte order.
2147           Bytes::put_Java_u2(p, new_index);
2148         }
2149       } break;
2150       default:
2151         break;
2152     }
2153   } // end for each bytecode
2154 
2155   // We also need to rewrite the parameter name indexes, if there is
2156   // method parameter data present
2157   if(method-&gt;has_method_parameters()) {
2158     const int len = method-&gt;method_parameters_length();
2159     MethodParametersElement* elem = method-&gt;method_parameters_start();
2160 
2161     for (int i = 0; i &lt; len; i++) {
2162       const u2 cp_index = elem[i].name_cp_index;
2163       const u2 new_cp_index = find_new_index(cp_index);
2164       if (new_cp_index != 0) {
2165         elem[i].name_cp_index = new_cp_index;
2166       }
2167     }
2168   }
2169 } // end rewrite_cp_refs_in_method()
2170 
2171 
2172 // Rewrite constant pool references in the class_annotations field.
2173 bool VM_RedefineClasses::rewrite_cp_refs_in_class_annotations(
2174        InstanceKlass* scratch_class, TRAPS) {
2175 
2176   AnnotationArray* class_annotations = scratch_class-&gt;class_annotations();
2177   if (class_annotations == NULL || class_annotations-&gt;length() == 0) {
2178     // no class_annotations so nothing to do
2179     return true;
2180   }
2181 
2182   log_debug(redefine, class, annotation)(&quot;class_annotations length=%d&quot;, class_annotations-&gt;length());
2183 
2184   int byte_i = 0;  // byte index into class_annotations
2185   return rewrite_cp_refs_in_annotations_typeArray(class_annotations, byte_i,
2186            THREAD);
2187 }
2188 
2189 
2190 // Rewrite constant pool references in an annotations typeArray. This
2191 // &quot;structure&quot; is adapted from the RuntimeVisibleAnnotations_attribute
2192 // that is described in section 4.8.15 of the 2nd-edition of the VM spec:
2193 //
2194 // annotations_typeArray {
2195 //   u2 num_annotations;
2196 //   annotation annotations[num_annotations];
2197 // }
2198 //
2199 bool VM_RedefineClasses::rewrite_cp_refs_in_annotations_typeArray(
2200        AnnotationArray* annotations_typeArray, int &amp;byte_i_ref, TRAPS) {
2201 
2202   if ((byte_i_ref + 2) &gt; annotations_typeArray-&gt;length()) {
2203     // not enough room for num_annotations field
2204     log_debug(redefine, class, annotation)(&quot;length() is too small for num_annotations field&quot;);
2205     return false;
2206   }
2207 
2208   u2 num_annotations = Bytes::get_Java_u2((address)
2209                          annotations_typeArray-&gt;adr_at(byte_i_ref));
2210   byte_i_ref += 2;
2211 
2212   log_debug(redefine, class, annotation)(&quot;num_annotations=%d&quot;, num_annotations);
2213 
2214   int calc_num_annotations = 0;
2215   for (; calc_num_annotations &lt; num_annotations; calc_num_annotations++) {
2216     if (!rewrite_cp_refs_in_annotation_struct(annotations_typeArray,
2217            byte_i_ref, THREAD)) {
2218       log_debug(redefine, class, annotation)(&quot;bad annotation_struct at %d&quot;, calc_num_annotations);
2219       // propagate failure back to caller
2220       return false;
2221     }
2222   }
2223   assert(num_annotations == calc_num_annotations, &quot;sanity check&quot;);
2224 
2225   return true;
2226 } // end rewrite_cp_refs_in_annotations_typeArray()
2227 
2228 
2229 // Rewrite constant pool references in the annotation struct portion of
2230 // an annotations_typeArray. This &quot;structure&quot; is from section 4.8.15 of
2231 // the 2nd-edition of the VM spec:
2232 //
2233 // struct annotation {
2234 //   u2 type_index;
2235 //   u2 num_element_value_pairs;
2236 //   {
2237 //     u2 element_name_index;
2238 //     element_value value;
2239 //   } element_value_pairs[num_element_value_pairs];
2240 // }
2241 //
2242 bool VM_RedefineClasses::rewrite_cp_refs_in_annotation_struct(
2243        AnnotationArray* annotations_typeArray, int &amp;byte_i_ref, TRAPS) {
2244   if ((byte_i_ref + 2 + 2) &gt; annotations_typeArray-&gt;length()) {
2245     // not enough room for smallest annotation_struct
2246     log_debug(redefine, class, annotation)(&quot;length() is too small for annotation_struct&quot;);
2247     return false;
2248   }
2249 
2250   u2 type_index = rewrite_cp_ref_in_annotation_data(annotations_typeArray,
2251                     byte_i_ref, &quot;type_index&quot;, THREAD);
2252 
2253   u2 num_element_value_pairs = Bytes::get_Java_u2((address)
2254                                  annotations_typeArray-&gt;adr_at(byte_i_ref));
2255   byte_i_ref += 2;
2256 
2257   log_debug(redefine, class, annotation)
2258     (&quot;type_index=%d  num_element_value_pairs=%d&quot;, type_index, num_element_value_pairs);
2259 
2260   int calc_num_element_value_pairs = 0;
2261   for (; calc_num_element_value_pairs &lt; num_element_value_pairs;
2262        calc_num_element_value_pairs++) {
2263     if ((byte_i_ref + 2) &gt; annotations_typeArray-&gt;length()) {
2264       // not enough room for another element_name_index, let alone
2265       // the rest of another component
2266       log_debug(redefine, class, annotation)(&quot;length() is too small for element_name_index&quot;);
2267       return false;
2268     }
2269 
2270     u2 element_name_index = rewrite_cp_ref_in_annotation_data(
2271                               annotations_typeArray, byte_i_ref,
2272                               &quot;element_name_index&quot;, THREAD);
2273 
2274     log_debug(redefine, class, annotation)(&quot;element_name_index=%d&quot;, element_name_index);
2275 
2276     if (!rewrite_cp_refs_in_element_value(annotations_typeArray,
2277            byte_i_ref, THREAD)) {
2278       log_debug(redefine, class, annotation)(&quot;bad element_value at %d&quot;, calc_num_element_value_pairs);
2279       // propagate failure back to caller
2280       return false;
2281     }
2282   } // end for each component
2283   assert(num_element_value_pairs == calc_num_element_value_pairs,
2284     &quot;sanity check&quot;);
2285 
2286   return true;
2287 } // end rewrite_cp_refs_in_annotation_struct()
2288 
2289 
2290 // Rewrite a constant pool reference at the current position in
2291 // annotations_typeArray if needed. Returns the original constant
2292 // pool reference if a rewrite was not needed or the new constant
2293 // pool reference if a rewrite was needed.
2294 u2 VM_RedefineClasses::rewrite_cp_ref_in_annotation_data(
2295      AnnotationArray* annotations_typeArray, int &amp;byte_i_ref,
2296      const char * trace_mesg, TRAPS) {
2297 
2298   address cp_index_addr = (address)
2299     annotations_typeArray-&gt;adr_at(byte_i_ref);
2300   u2 old_cp_index = Bytes::get_Java_u2(cp_index_addr);
2301   u2 new_cp_index = find_new_index(old_cp_index);
2302   if (new_cp_index != 0) {
2303     log_debug(redefine, class, annotation)(&quot;mapped old %s=%d&quot;, trace_mesg, old_cp_index);
2304     Bytes::put_Java_u2(cp_index_addr, new_cp_index);
2305     old_cp_index = new_cp_index;
2306   }
2307   byte_i_ref += 2;
2308   return old_cp_index;
2309 }
2310 
2311 
2312 // Rewrite constant pool references in the element_value portion of an
2313 // annotations_typeArray. This &quot;structure&quot; is from section 4.8.15.1 of
2314 // the 2nd-edition of the VM spec:
2315 //
2316 // struct element_value {
2317 //   u1 tag;
2318 //   union {
2319 //     u2 const_value_index;
2320 //     {
2321 //       u2 type_name_index;
2322 //       u2 const_name_index;
2323 //     } enum_const_value;
2324 //     u2 class_info_index;
2325 //     annotation annotation_value;
2326 //     struct {
2327 //       u2 num_values;
2328 //       element_value values[num_values];
2329 //     } array_value;
2330 //   } value;
2331 // }
2332 //
2333 bool VM_RedefineClasses::rewrite_cp_refs_in_element_value(
2334        AnnotationArray* annotations_typeArray, int &amp;byte_i_ref, TRAPS) {
2335 
2336   if ((byte_i_ref + 1) &gt; annotations_typeArray-&gt;length()) {
2337     // not enough room for a tag let alone the rest of an element_value
2338     log_debug(redefine, class, annotation)(&quot;length() is too small for a tag&quot;);
2339     return false;
2340   }
2341 
2342   u1 tag = annotations_typeArray-&gt;at(byte_i_ref);
2343   byte_i_ref++;
2344   log_debug(redefine, class, annotation)(&quot;tag=&#39;%c&#39;&quot;, tag);
2345 
2346   switch (tag) {
2347     // These BaseType tag values are from Table 4.2 in VM spec:
2348     case JVM_SIGNATURE_BYTE:
2349     case JVM_SIGNATURE_CHAR:
2350     case JVM_SIGNATURE_DOUBLE:
2351     case JVM_SIGNATURE_FLOAT:
2352     case JVM_SIGNATURE_INT:
2353     case JVM_SIGNATURE_LONG:
2354     case JVM_SIGNATURE_SHORT:
2355     case JVM_SIGNATURE_BOOLEAN:
2356 
2357     // The remaining tag values are from Table 4.8 in the 2nd-edition of
2358     // the VM spec:
2359     case &#39;s&#39;:
2360     {
2361       // For the above tag values (including the BaseType values),
2362       // value.const_value_index is right union field.
2363 
2364       if ((byte_i_ref + 2) &gt; annotations_typeArray-&gt;length()) {
2365         // not enough room for a const_value_index
2366         log_debug(redefine, class, annotation)(&quot;length() is too small for a const_value_index&quot;);
2367         return false;
2368       }
2369 
2370       u2 const_value_index = rewrite_cp_ref_in_annotation_data(
2371                                annotations_typeArray, byte_i_ref,
2372                                &quot;const_value_index&quot;, THREAD);
2373 
2374       log_debug(redefine, class, annotation)(&quot;const_value_index=%d&quot;, const_value_index);
2375     } break;
2376 
2377     case &#39;e&#39;:
2378     {
2379       // for the above tag value, value.enum_const_value is right union field
2380 
2381       if ((byte_i_ref + 4) &gt; annotations_typeArray-&gt;length()) {
2382         // not enough room for a enum_const_value
2383         log_debug(redefine, class, annotation)(&quot;length() is too small for a enum_const_value&quot;);
2384         return false;
2385       }
2386 
2387       u2 type_name_index = rewrite_cp_ref_in_annotation_data(
2388                              annotations_typeArray, byte_i_ref,
2389                              &quot;type_name_index&quot;, THREAD);
2390 
2391       u2 const_name_index = rewrite_cp_ref_in_annotation_data(
2392                               annotations_typeArray, byte_i_ref,
2393                               &quot;const_name_index&quot;, THREAD);
2394 
2395       log_debug(redefine, class, annotation)
2396         (&quot;type_name_index=%d  const_name_index=%d&quot;, type_name_index, const_name_index);
2397     } break;
2398 
2399     case &#39;c&#39;:
2400     {
2401       // for the above tag value, value.class_info_index is right union field
2402 
2403       if ((byte_i_ref + 2) &gt; annotations_typeArray-&gt;length()) {
2404         // not enough room for a class_info_index
2405         log_debug(redefine, class, annotation)(&quot;length() is too small for a class_info_index&quot;);
2406         return false;
2407       }
2408 
2409       u2 class_info_index = rewrite_cp_ref_in_annotation_data(
2410                               annotations_typeArray, byte_i_ref,
2411                               &quot;class_info_index&quot;, THREAD);
2412 
2413       log_debug(redefine, class, annotation)(&quot;class_info_index=%d&quot;, class_info_index);
2414     } break;
2415 
2416     case &#39;@&#39;:
2417       // For the above tag value, value.attr_value is the right union
2418       // field. This is a nested annotation.
2419       if (!rewrite_cp_refs_in_annotation_struct(annotations_typeArray,
2420              byte_i_ref, THREAD)) {
2421         // propagate failure back to caller
2422         return false;
2423       }
2424       break;
2425 
2426     case JVM_SIGNATURE_ARRAY:
2427     {
2428       if ((byte_i_ref + 2) &gt; annotations_typeArray-&gt;length()) {
2429         // not enough room for a num_values field
2430         log_debug(redefine, class, annotation)(&quot;length() is too small for a num_values field&quot;);
2431         return false;
2432       }
2433 
2434       // For the above tag value, value.array_value is the right union
2435       // field. This is an array of nested element_value.
2436       u2 num_values = Bytes::get_Java_u2((address)
2437                         annotations_typeArray-&gt;adr_at(byte_i_ref));
2438       byte_i_ref += 2;
2439       log_debug(redefine, class, annotation)(&quot;num_values=%d&quot;, num_values);
2440 
2441       int calc_num_values = 0;
2442       for (; calc_num_values &lt; num_values; calc_num_values++) {
2443         if (!rewrite_cp_refs_in_element_value(
2444                annotations_typeArray, byte_i_ref, THREAD)) {
2445           log_debug(redefine, class, annotation)(&quot;bad nested element_value at %d&quot;, calc_num_values);
2446           // propagate failure back to caller
2447           return false;
2448         }
2449       }
2450       assert(num_values == calc_num_values, &quot;sanity check&quot;);
2451     } break;
2452 
2453     default:
2454       log_debug(redefine, class, annotation)(&quot;bad tag=0x%x&quot;, tag);
2455       return false;
2456   } // end decode tag field
2457 
2458   return true;
2459 } // end rewrite_cp_refs_in_element_value()
2460 
2461 
2462 // Rewrite constant pool references in a fields_annotations field.
2463 bool VM_RedefineClasses::rewrite_cp_refs_in_fields_annotations(
2464        InstanceKlass* scratch_class, TRAPS) {
2465 
2466   Array&lt;AnnotationArray*&gt;* fields_annotations = scratch_class-&gt;fields_annotations();
2467 
2468   if (fields_annotations == NULL || fields_annotations-&gt;length() == 0) {
2469     // no fields_annotations so nothing to do
2470     return true;
2471   }
2472 
2473   log_debug(redefine, class, annotation)(&quot;fields_annotations length=%d&quot;, fields_annotations-&gt;length());
2474 
2475   for (int i = 0; i &lt; fields_annotations-&gt;length(); i++) {
2476     AnnotationArray* field_annotations = fields_annotations-&gt;at(i);
2477     if (field_annotations == NULL || field_annotations-&gt;length() == 0) {
2478       // this field does not have any annotations so skip it
2479       continue;
2480     }
2481 
2482     int byte_i = 0;  // byte index into field_annotations
2483     if (!rewrite_cp_refs_in_annotations_typeArray(field_annotations, byte_i,
2484            THREAD)) {
2485       log_debug(redefine, class, annotation)(&quot;bad field_annotations at %d&quot;, i);
2486       // propagate failure back to caller
2487       return false;
2488     }
2489   }
2490 
2491   return true;
2492 } // end rewrite_cp_refs_in_fields_annotations()
2493 
2494 
2495 // Rewrite constant pool references in a methods_annotations field.
2496 bool VM_RedefineClasses::rewrite_cp_refs_in_methods_annotations(
2497        InstanceKlass* scratch_class, TRAPS) {
2498 
2499   for (int i = 0; i &lt; scratch_class-&gt;methods()-&gt;length(); i++) {
2500     Method* m = scratch_class-&gt;methods()-&gt;at(i);
2501     AnnotationArray* method_annotations = m-&gt;constMethod()-&gt;method_annotations();
2502 
2503     if (method_annotations == NULL || method_annotations-&gt;length() == 0) {
2504       // this method does not have any annotations so skip it
2505       continue;
2506     }
2507 
2508     int byte_i = 0;  // byte index into method_annotations
2509     if (!rewrite_cp_refs_in_annotations_typeArray(method_annotations, byte_i,
2510            THREAD)) {
2511       log_debug(redefine, class, annotation)(&quot;bad method_annotations at %d&quot;, i);
2512       // propagate failure back to caller
2513       return false;
2514     }
2515   }
2516 
2517   return true;
2518 } // end rewrite_cp_refs_in_methods_annotations()
2519 
2520 
2521 // Rewrite constant pool references in a methods_parameter_annotations
2522 // field. This &quot;structure&quot; is adapted from the
2523 // RuntimeVisibleParameterAnnotations_attribute described in section
2524 // 4.8.17 of the 2nd-edition of the VM spec:
2525 //
2526 // methods_parameter_annotations_typeArray {
2527 //   u1 num_parameters;
2528 //   {
2529 //     u2 num_annotations;
2530 //     annotation annotations[num_annotations];
2531 //   } parameter_annotations[num_parameters];
2532 // }
2533 //
2534 bool VM_RedefineClasses::rewrite_cp_refs_in_methods_parameter_annotations(
2535        InstanceKlass* scratch_class, TRAPS) {
2536 
2537   for (int i = 0; i &lt; scratch_class-&gt;methods()-&gt;length(); i++) {
2538     Method* m = scratch_class-&gt;methods()-&gt;at(i);
2539     AnnotationArray* method_parameter_annotations = m-&gt;constMethod()-&gt;parameter_annotations();
2540     if (method_parameter_annotations == NULL
2541         || method_parameter_annotations-&gt;length() == 0) {
2542       // this method does not have any parameter annotations so skip it
2543       continue;
2544     }
2545 
2546     if (method_parameter_annotations-&gt;length() &lt; 1) {
2547       // not enough room for a num_parameters field
2548       log_debug(redefine, class, annotation)(&quot;length() is too small for a num_parameters field at %d&quot;, i);
2549       return false;
2550     }
2551 
2552     int byte_i = 0;  // byte index into method_parameter_annotations
2553 
2554     u1 num_parameters = method_parameter_annotations-&gt;at(byte_i);
2555     byte_i++;
2556 
2557     log_debug(redefine, class, annotation)(&quot;num_parameters=%d&quot;, num_parameters);
2558 
2559     int calc_num_parameters = 0;
2560     for (; calc_num_parameters &lt; num_parameters; calc_num_parameters++) {
2561       if (!rewrite_cp_refs_in_annotations_typeArray(
2562              method_parameter_annotations, byte_i, THREAD)) {
2563         log_debug(redefine, class, annotation)(&quot;bad method_parameter_annotations at %d&quot;, calc_num_parameters);
2564         // propagate failure back to caller
2565         return false;
2566       }
2567     }
2568     assert(num_parameters == calc_num_parameters, &quot;sanity check&quot;);
2569   }
2570 
2571   return true;
2572 } // end rewrite_cp_refs_in_methods_parameter_annotations()
2573 
2574 
2575 // Rewrite constant pool references in a methods_default_annotations
2576 // field. This &quot;structure&quot; is adapted from the AnnotationDefault_attribute
2577 // that is described in section 4.8.19 of the 2nd-edition of the VM spec:
2578 //
2579 // methods_default_annotations_typeArray {
2580 //   element_value default_value;
2581 // }
2582 //
2583 bool VM_RedefineClasses::rewrite_cp_refs_in_methods_default_annotations(
2584        InstanceKlass* scratch_class, TRAPS) {
2585 
2586   for (int i = 0; i &lt; scratch_class-&gt;methods()-&gt;length(); i++) {
2587     Method* m = scratch_class-&gt;methods()-&gt;at(i);
2588     AnnotationArray* method_default_annotations = m-&gt;constMethod()-&gt;default_annotations();
2589     if (method_default_annotations == NULL
2590         || method_default_annotations-&gt;length() == 0) {
2591       // this method does not have any default annotations so skip it
2592       continue;
2593     }
2594 
2595     int byte_i = 0;  // byte index into method_default_annotations
2596 
2597     if (!rewrite_cp_refs_in_element_value(
2598            method_default_annotations, byte_i, THREAD)) {
2599       log_debug(redefine, class, annotation)(&quot;bad default element_value at %d&quot;, i);
2600       // propagate failure back to caller
2601       return false;
2602     }
2603   }
2604 
2605   return true;
2606 } // end rewrite_cp_refs_in_methods_default_annotations()
2607 
2608 
2609 // Rewrite constant pool references in a class_type_annotations field.
2610 bool VM_RedefineClasses::rewrite_cp_refs_in_class_type_annotations(
2611        InstanceKlass* scratch_class, TRAPS) {
2612 
2613   AnnotationArray* class_type_annotations = scratch_class-&gt;class_type_annotations();
2614   if (class_type_annotations == NULL || class_type_annotations-&gt;length() == 0) {
2615     // no class_type_annotations so nothing to do
2616     return true;
2617   }
2618 
2619   log_debug(redefine, class, annotation)(&quot;class_type_annotations length=%d&quot;, class_type_annotations-&gt;length());
2620 
2621   int byte_i = 0;  // byte index into class_type_annotations
2622   return rewrite_cp_refs_in_type_annotations_typeArray(class_type_annotations,
2623       byte_i, &quot;ClassFile&quot;, THREAD);
2624 } // end rewrite_cp_refs_in_class_type_annotations()
2625 
2626 
2627 // Rewrite constant pool references in a fields_type_annotations field.
2628 bool VM_RedefineClasses::rewrite_cp_refs_in_fields_type_annotations(
2629        InstanceKlass* scratch_class, TRAPS) {
2630 
2631   Array&lt;AnnotationArray*&gt;* fields_type_annotations = scratch_class-&gt;fields_type_annotations();
2632   if (fields_type_annotations == NULL || fields_type_annotations-&gt;length() == 0) {
2633     // no fields_type_annotations so nothing to do
2634     return true;
2635   }
2636 
2637   log_debug(redefine, class, annotation)(&quot;fields_type_annotations length=%d&quot;, fields_type_annotations-&gt;length());
2638 
2639   for (int i = 0; i &lt; fields_type_annotations-&gt;length(); i++) {
2640     AnnotationArray* field_type_annotations = fields_type_annotations-&gt;at(i);
2641     if (field_type_annotations == NULL || field_type_annotations-&gt;length() == 0) {
2642       // this field does not have any annotations so skip it
2643       continue;
2644     }
2645 
2646     int byte_i = 0;  // byte index into field_type_annotations
2647     if (!rewrite_cp_refs_in_type_annotations_typeArray(field_type_annotations,
2648            byte_i, &quot;field_info&quot;, THREAD)) {
2649       log_debug(redefine, class, annotation)(&quot;bad field_type_annotations at %d&quot;, i);
2650       // propagate failure back to caller
2651       return false;
2652     }
2653   }
2654 
2655   return true;
2656 } // end rewrite_cp_refs_in_fields_type_annotations()
2657 
2658 
2659 // Rewrite constant pool references in a methods_type_annotations field.
2660 bool VM_RedefineClasses::rewrite_cp_refs_in_methods_type_annotations(
2661        InstanceKlass* scratch_class, TRAPS) {
2662 
2663   for (int i = 0; i &lt; scratch_class-&gt;methods()-&gt;length(); i++) {
2664     Method* m = scratch_class-&gt;methods()-&gt;at(i);
2665     AnnotationArray* method_type_annotations = m-&gt;constMethod()-&gt;type_annotations();
2666 
2667     if (method_type_annotations == NULL || method_type_annotations-&gt;length() == 0) {
2668       // this method does not have any annotations so skip it
2669       continue;
2670     }
2671 
2672     log_debug(redefine, class, annotation)(&quot;methods type_annotations length=%d&quot;, method_type_annotations-&gt;length());
2673 
2674     int byte_i = 0;  // byte index into method_type_annotations
2675     if (!rewrite_cp_refs_in_type_annotations_typeArray(method_type_annotations,
2676            byte_i, &quot;method_info&quot;, THREAD)) {
2677       log_debug(redefine, class, annotation)(&quot;bad method_type_annotations at %d&quot;, i);
2678       // propagate failure back to caller
2679       return false;
2680     }
2681   }
2682 
2683   return true;
2684 } // end rewrite_cp_refs_in_methods_type_annotations()
2685 
2686 
2687 // Rewrite constant pool references in a type_annotations
2688 // field. This &quot;structure&quot; is adapted from the
2689 // RuntimeVisibleTypeAnnotations_attribute described in
2690 // section 4.7.20 of the Java SE 8 Edition of the VM spec:
2691 //
2692 // type_annotations_typeArray {
2693 //   u2              num_annotations;
2694 //   type_annotation annotations[num_annotations];
2695 // }
2696 //
2697 bool VM_RedefineClasses::rewrite_cp_refs_in_type_annotations_typeArray(
2698        AnnotationArray* type_annotations_typeArray, int &amp;byte_i_ref,
2699        const char * location_mesg, TRAPS) {
2700 
2701   if ((byte_i_ref + 2) &gt; type_annotations_typeArray-&gt;length()) {
2702     // not enough room for num_annotations field
2703     log_debug(redefine, class, annotation)(&quot;length() is too small for num_annotations field&quot;);
2704     return false;
2705   }
2706 
2707   u2 num_annotations = Bytes::get_Java_u2((address)
2708                          type_annotations_typeArray-&gt;adr_at(byte_i_ref));
2709   byte_i_ref += 2;
2710 
2711   log_debug(redefine, class, annotation)(&quot;num_type_annotations=%d&quot;, num_annotations);
2712 
2713   int calc_num_annotations = 0;
2714   for (; calc_num_annotations &lt; num_annotations; calc_num_annotations++) {
2715     if (!rewrite_cp_refs_in_type_annotation_struct(type_annotations_typeArray,
2716            byte_i_ref, location_mesg, THREAD)) {
2717       log_debug(redefine, class, annotation)(&quot;bad type_annotation_struct at %d&quot;, calc_num_annotations);
2718       // propagate failure back to caller
2719       return false;
2720     }
2721   }
2722   assert(num_annotations == calc_num_annotations, &quot;sanity check&quot;);
2723 
2724   if (byte_i_ref != type_annotations_typeArray-&gt;length()) {
2725     log_debug(redefine, class, annotation)
2726       (&quot;read wrong amount of bytes at end of processing type_annotations_typeArray (%d of %d bytes were read)&quot;,
2727        byte_i_ref, type_annotations_typeArray-&gt;length());
2728     return false;
2729   }
2730 
2731   return true;
2732 } // end rewrite_cp_refs_in_type_annotations_typeArray()
2733 
2734 
2735 // Rewrite constant pool references in a type_annotation
2736 // field. This &quot;structure&quot; is adapted from the
2737 // RuntimeVisibleTypeAnnotations_attribute described in
2738 // section 4.7.20 of the Java SE 8 Edition of the VM spec:
2739 //
2740 // type_annotation {
2741 //   u1 target_type;
2742 //   union {
2743 //     type_parameter_target;
2744 //     supertype_target;
2745 //     type_parameter_bound_target;
2746 //     empty_target;
2747 //     method_formal_parameter_target;
2748 //     throws_target;
2749 //     localvar_target;
2750 //     catch_target;
2751 //     offset_target;
2752 //     type_argument_target;
2753 //   } target_info;
2754 //   type_path target_path;
2755 //   annotation anno;
2756 // }
2757 //
2758 bool VM_RedefineClasses::rewrite_cp_refs_in_type_annotation_struct(
2759        AnnotationArray* type_annotations_typeArray, int &amp;byte_i_ref,
2760        const char * location_mesg, TRAPS) {
2761 
2762   if (!skip_type_annotation_target(type_annotations_typeArray,
2763          byte_i_ref, location_mesg, THREAD)) {
2764     return false;
2765   }
2766 
2767   if (!skip_type_annotation_type_path(type_annotations_typeArray,
2768          byte_i_ref, THREAD)) {
2769     return false;
2770   }
2771 
2772   if (!rewrite_cp_refs_in_annotation_struct(type_annotations_typeArray,
2773          byte_i_ref, THREAD)) {
2774     return false;
2775   }
2776 
2777   return true;
2778 } // end rewrite_cp_refs_in_type_annotation_struct()
2779 
2780 
2781 // Read, verify and skip over the target_type and target_info part
2782 // so that rewriting can continue in the later parts of the struct.
2783 //
2784 // u1 target_type;
2785 // union {
2786 //   type_parameter_target;
2787 //   supertype_target;
2788 //   type_parameter_bound_target;
2789 //   empty_target;
2790 //   method_formal_parameter_target;
2791 //   throws_target;
2792 //   localvar_target;
2793 //   catch_target;
2794 //   offset_target;
2795 //   type_argument_target;
2796 // } target_info;
2797 //
2798 bool VM_RedefineClasses::skip_type_annotation_target(
2799        AnnotationArray* type_annotations_typeArray, int &amp;byte_i_ref,
2800        const char * location_mesg, TRAPS) {
2801 
2802   if ((byte_i_ref + 1) &gt; type_annotations_typeArray-&gt;length()) {
2803     // not enough room for a target_type let alone the rest of a type_annotation
2804     log_debug(redefine, class, annotation)(&quot;length() is too small for a target_type&quot;);
2805     return false;
2806   }
2807 
2808   u1 target_type = type_annotations_typeArray-&gt;at(byte_i_ref);
2809   byte_i_ref += 1;
2810   log_debug(redefine, class, annotation)(&quot;target_type=0x%.2x&quot;, target_type);
2811   log_debug(redefine, class, annotation)(&quot;location=%s&quot;, location_mesg);
2812 
2813   // Skip over target_info
2814   switch (target_type) {
2815     case 0x00:
2816     // kind: type parameter declaration of generic class or interface
2817     // location: ClassFile
2818     case 0x01:
2819     // kind: type parameter declaration of generic method or constructor
2820     // location: method_info
2821 
2822     {
2823       // struct:
2824       // type_parameter_target {
2825       //   u1 type_parameter_index;
2826       // }
2827       //
2828       if ((byte_i_ref + 1) &gt; type_annotations_typeArray-&gt;length()) {
2829         log_debug(redefine, class, annotation)(&quot;length() is too small for a type_parameter_target&quot;);
2830         return false;
2831       }
2832 
2833       u1 type_parameter_index = type_annotations_typeArray-&gt;at(byte_i_ref);
2834       byte_i_ref += 1;
2835 
2836       log_debug(redefine, class, annotation)(&quot;type_parameter_target: type_parameter_index=%d&quot;, type_parameter_index);
2837     } break;
2838 
2839     case 0x10:
2840     // kind: type in extends clause of class or interface declaration
2841     //       (including the direct superclass of an unsafe anonymous class declaration),
2842     //       or in implements clause of interface declaration
2843     // location: ClassFile
2844 
2845     {
2846       // struct:
2847       // supertype_target {
2848       //   u2 supertype_index;
2849       // }
2850       //
2851       if ((byte_i_ref + 2) &gt; type_annotations_typeArray-&gt;length()) {
2852         log_debug(redefine, class, annotation)(&quot;length() is too small for a supertype_target&quot;);
2853         return false;
2854       }
2855 
2856       u2 supertype_index = Bytes::get_Java_u2((address)
2857                              type_annotations_typeArray-&gt;adr_at(byte_i_ref));
2858       byte_i_ref += 2;
2859 
2860       log_debug(redefine, class, annotation)(&quot;supertype_target: supertype_index=%d&quot;, supertype_index);
2861     } break;
2862 
2863     case 0x11:
2864     // kind: type in bound of type parameter declaration of generic class or interface
2865     // location: ClassFile
2866     case 0x12:
2867     // kind: type in bound of type parameter declaration of generic method or constructor
2868     // location: method_info
2869 
2870     {
2871       // struct:
2872       // type_parameter_bound_target {
2873       //   u1 type_parameter_index;
2874       //   u1 bound_index;
2875       // }
2876       //
2877       if ((byte_i_ref + 2) &gt; type_annotations_typeArray-&gt;length()) {
2878         log_debug(redefine, class, annotation)(&quot;length() is too small for a type_parameter_bound_target&quot;);
2879         return false;
2880       }
2881 
2882       u1 type_parameter_index = type_annotations_typeArray-&gt;at(byte_i_ref);
2883       byte_i_ref += 1;
2884       u1 bound_index = type_annotations_typeArray-&gt;at(byte_i_ref);
2885       byte_i_ref += 1;
2886 
2887       log_debug(redefine, class, annotation)
2888         (&quot;type_parameter_bound_target: type_parameter_index=%d, bound_index=%d&quot;, type_parameter_index, bound_index);
2889     } break;
2890 
2891     case 0x13:
2892     // kind: type in field declaration
2893     // location: field_info
2894     case 0x14:
2895     // kind: return type of method, or type of newly constructed object
2896     // location: method_info
2897     case 0x15:
2898     // kind: receiver type of method or constructor
2899     // location: method_info
2900 
2901     {
2902       // struct:
2903       // empty_target {
2904       // }
2905       //
2906       log_debug(redefine, class, annotation)(&quot;empty_target&quot;);
2907     } break;
2908 
2909     case 0x16:
2910     // kind: type in formal parameter declaration of method, constructor, or lambda expression
2911     // location: method_info
2912 
2913     {
2914       // struct:
2915       // formal_parameter_target {
2916       //   u1 formal_parameter_index;
2917       // }
2918       //
2919       if ((byte_i_ref + 1) &gt; type_annotations_typeArray-&gt;length()) {
2920         log_debug(redefine, class, annotation)(&quot;length() is too small for a formal_parameter_target&quot;);
2921         return false;
2922       }
2923 
2924       u1 formal_parameter_index = type_annotations_typeArray-&gt;at(byte_i_ref);
2925       byte_i_ref += 1;
2926 
2927       log_debug(redefine, class, annotation)
2928         (&quot;formal_parameter_target: formal_parameter_index=%d&quot;, formal_parameter_index);
2929     } break;
2930 
2931     case 0x17:
2932     // kind: type in throws clause of method or constructor
2933     // location: method_info
2934 
2935     {
2936       // struct:
2937       // throws_target {
2938       //   u2 throws_type_index
2939       // }
2940       //
2941       if ((byte_i_ref + 2) &gt; type_annotations_typeArray-&gt;length()) {
2942         log_debug(redefine, class, annotation)(&quot;length() is too small for a throws_target&quot;);
2943         return false;
2944       }
2945 
2946       u2 throws_type_index = Bytes::get_Java_u2((address)
2947                                type_annotations_typeArray-&gt;adr_at(byte_i_ref));
2948       byte_i_ref += 2;
2949 
2950       log_debug(redefine, class, annotation)(&quot;throws_target: throws_type_index=%d&quot;, throws_type_index);
2951     } break;
2952 
2953     case 0x40:
2954     // kind: type in local variable declaration
2955     // location: Code
2956     case 0x41:
2957     // kind: type in resource variable declaration
2958     // location: Code
2959 
2960     {
2961       // struct:
2962       // localvar_target {
2963       //   u2 table_length;
2964       //   struct {
2965       //     u2 start_pc;
2966       //     u2 length;
2967       //     u2 index;
2968       //   } table[table_length];
2969       // }
2970       //
2971       if ((byte_i_ref + 2) &gt; type_annotations_typeArray-&gt;length()) {
2972         // not enough room for a table_length let alone the rest of a localvar_target
2973         log_debug(redefine, class, annotation)(&quot;length() is too small for a localvar_target table_length&quot;);
2974         return false;
2975       }
2976 
2977       u2 table_length = Bytes::get_Java_u2((address)
2978                           type_annotations_typeArray-&gt;adr_at(byte_i_ref));
2979       byte_i_ref += 2;
2980 
2981       log_debug(redefine, class, annotation)(&quot;localvar_target: table_length=%d&quot;, table_length);
2982 
2983       int table_struct_size = 2 + 2 + 2; // 3 u2 variables per table entry
2984       int table_size = table_length * table_struct_size;
2985 
2986       if ((byte_i_ref + table_size) &gt; type_annotations_typeArray-&gt;length()) {
2987         // not enough room for a table
2988         log_debug(redefine, class, annotation)(&quot;length() is too small for a table array of length %d&quot;, table_length);
2989         return false;
2990       }
2991 
2992       // Skip over table
2993       byte_i_ref += table_size;
2994     } break;
2995 
2996     case 0x42:
2997     // kind: type in exception parameter declaration
2998     // location: Code
2999 
3000     {
3001       // struct:
3002       // catch_target {
3003       //   u2 exception_table_index;
3004       // }
3005       //
3006       if ((byte_i_ref + 2) &gt; type_annotations_typeArray-&gt;length()) {
3007         log_debug(redefine, class, annotation)(&quot;length() is too small for a catch_target&quot;);
3008         return false;
3009       }
3010 
3011       u2 exception_table_index = Bytes::get_Java_u2((address)
3012                                    type_annotations_typeArray-&gt;adr_at(byte_i_ref));
3013       byte_i_ref += 2;
3014 
3015       log_debug(redefine, class, annotation)(&quot;catch_target: exception_table_index=%d&quot;, exception_table_index);
3016     } break;
3017 
3018     case 0x43:
3019     // kind: type in instanceof expression
3020     // location: Code
3021     case 0x44:
3022     // kind: type in new expression
3023     // location: Code
3024     case 0x45:
3025     // kind: type in method reference expression using ::new
3026     // location: Code
3027     case 0x46:
3028     // kind: type in method reference expression using ::Identifier
3029     // location: Code
3030 
3031     {
3032       // struct:
3033       // offset_target {
3034       //   u2 offset;
3035       // }
3036       //
3037       if ((byte_i_ref + 2) &gt; type_annotations_typeArray-&gt;length()) {
3038         log_debug(redefine, class, annotation)(&quot;length() is too small for a offset_target&quot;);
3039         return false;
3040       }
3041 
3042       u2 offset = Bytes::get_Java_u2((address)
3043                     type_annotations_typeArray-&gt;adr_at(byte_i_ref));
3044       byte_i_ref += 2;
3045 
3046       log_debug(redefine, class, annotation)(&quot;offset_target: offset=%d&quot;, offset);
3047     } break;
3048 
3049     case 0x47:
3050     // kind: type in cast expression
3051     // location: Code
3052     case 0x48:
3053     // kind: type argument for generic constructor in new expression or
3054     //       explicit constructor invocation statement
3055     // location: Code
3056     case 0x49:
3057     // kind: type argument for generic method in method invocation expression
3058     // location: Code
3059     case 0x4A:
3060     // kind: type argument for generic constructor in method reference expression using ::new
3061     // location: Code
3062     case 0x4B:
3063     // kind: type argument for generic method in method reference expression using ::Identifier
3064     // location: Code
3065 
3066     {
3067       // struct:
3068       // type_argument_target {
3069       //   u2 offset;
3070       //   u1 type_argument_index;
3071       // }
3072       //
3073       if ((byte_i_ref + 3) &gt; type_annotations_typeArray-&gt;length()) {
3074         log_debug(redefine, class, annotation)(&quot;length() is too small for a type_argument_target&quot;);
3075         return false;
3076       }
3077 
3078       u2 offset = Bytes::get_Java_u2((address)
3079                     type_annotations_typeArray-&gt;adr_at(byte_i_ref));
3080       byte_i_ref += 2;
3081       u1 type_argument_index = type_annotations_typeArray-&gt;at(byte_i_ref);
3082       byte_i_ref += 1;
3083 
3084       log_debug(redefine, class, annotation)
3085         (&quot;type_argument_target: offset=%d, type_argument_index=%d&quot;, offset, type_argument_index);
3086     } break;
3087 
3088     default:
3089       log_debug(redefine, class, annotation)(&quot;unknown target_type&quot;);
3090 #ifdef ASSERT
3091       ShouldNotReachHere();
3092 #endif
3093       return false;
3094   }
3095 
3096   return true;
3097 } // end skip_type_annotation_target()
3098 
3099 
3100 // Read, verify and skip over the type_path part so that rewriting
3101 // can continue in the later parts of the struct.
3102 //
3103 // type_path {
3104 //   u1 path_length;
3105 //   {
3106 //     u1 type_path_kind;
3107 //     u1 type_argument_index;
3108 //   } path[path_length];
3109 // }
3110 //
3111 bool VM_RedefineClasses::skip_type_annotation_type_path(
3112        AnnotationArray* type_annotations_typeArray, int &amp;byte_i_ref, TRAPS) {
3113 
3114   if ((byte_i_ref + 1) &gt; type_annotations_typeArray-&gt;length()) {
3115     // not enough room for a path_length let alone the rest of the type_path
3116     log_debug(redefine, class, annotation)(&quot;length() is too small for a type_path&quot;);
3117     return false;
3118   }
3119 
3120   u1 path_length = type_annotations_typeArray-&gt;at(byte_i_ref);
3121   byte_i_ref += 1;
3122 
3123   log_debug(redefine, class, annotation)(&quot;type_path: path_length=%d&quot;, path_length);
3124 
3125   int calc_path_length = 0;
3126   for (; calc_path_length &lt; path_length; calc_path_length++) {
3127     if ((byte_i_ref + 1 + 1) &gt; type_annotations_typeArray-&gt;length()) {
3128       // not enough room for a path
3129       log_debug(redefine, class, annotation)
3130         (&quot;length() is too small for path entry %d of %d&quot;, calc_path_length, path_length);
3131       return false;
3132     }
3133 
3134     u1 type_path_kind = type_annotations_typeArray-&gt;at(byte_i_ref);
3135     byte_i_ref += 1;
3136     u1 type_argument_index = type_annotations_typeArray-&gt;at(byte_i_ref);
3137     byte_i_ref += 1;
3138 
3139     log_debug(redefine, class, annotation)
3140       (&quot;type_path: path[%d]: type_path_kind=%d, type_argument_index=%d&quot;,
3141        calc_path_length, type_path_kind, type_argument_index);
3142 
3143     if (type_path_kind &gt; 3 || (type_path_kind != 3 &amp;&amp; type_argument_index != 0)) {
3144       // not enough room for a path
3145       log_debug(redefine, class, annotation)(&quot;inconsistent type_path values&quot;);
3146       return false;
3147     }
3148   }
3149   assert(path_length == calc_path_length, &quot;sanity check&quot;);
3150 
3151   return true;
3152 } // end skip_type_annotation_type_path()
3153 
3154 
3155 // Rewrite constant pool references in the method&#39;s stackmap table.
3156 // These &quot;structures&quot; are adapted from the StackMapTable_attribute that
3157 // is described in section 4.8.4 of the 6.0 version of the VM spec
3158 // (dated 2005.10.26):
3159 // file:///net/quincunx.sfbay/export/gbracha/ClassFile-Java6.pdf
3160 //
3161 // stack_map {
3162 //   u2 number_of_entries;
3163 //   stack_map_frame entries[number_of_entries];
3164 // }
3165 //
3166 void VM_RedefineClasses::rewrite_cp_refs_in_stack_map_table(
3167        const methodHandle&amp; method, TRAPS) {
3168 
3169   if (!method-&gt;has_stackmap_table()) {
3170     return;
3171   }
3172 
3173   AnnotationArray* stackmap_data = method-&gt;stackmap_data();
3174   address stackmap_p = (address)stackmap_data-&gt;adr_at(0);
3175   address stackmap_end = stackmap_p + stackmap_data-&gt;length();
3176 
3177   assert(stackmap_p + 2 &lt;= stackmap_end, &quot;no room for number_of_entries&quot;);
3178   u2 number_of_entries = Bytes::get_Java_u2(stackmap_p);
3179   stackmap_p += 2;
3180 
3181   log_debug(redefine, class, stackmap)(&quot;number_of_entries=%u&quot;, number_of_entries);
3182 
3183   // walk through each stack_map_frame
3184   u2 calc_number_of_entries = 0;
3185   for (; calc_number_of_entries &lt; number_of_entries; calc_number_of_entries++) {
3186     // The stack_map_frame structure is a u1 frame_type followed by
3187     // 0 or more bytes of data:
3188     //
3189     // union stack_map_frame {
3190     //   same_frame;
3191     //   same_locals_1_stack_item_frame;
3192     //   same_locals_1_stack_item_frame_extended;
3193     //   chop_frame;
3194     //   same_frame_extended;
3195     //   append_frame;
3196     //   full_frame;
3197     // }
3198 
3199     assert(stackmap_p + 1 &lt;= stackmap_end, &quot;no room for frame_type&quot;);
3200     u1 frame_type = *stackmap_p;
3201     stackmap_p++;
3202 
3203     // same_frame {
3204     //   u1 frame_type = SAME; /* 0-63 */
3205     // }
3206     if (frame_type &lt;= 63) {
3207       // nothing more to do for same_frame
3208     }
3209 
3210     // same_locals_1_stack_item_frame {
3211     //   u1 frame_type = SAME_LOCALS_1_STACK_ITEM; /* 64-127 */
3212     //   verification_type_info stack[1];
3213     // }
3214     else if (frame_type &gt;= 64 &amp;&amp; frame_type &lt;= 127) {
3215       rewrite_cp_refs_in_verification_type_info(stackmap_p, stackmap_end,
3216         calc_number_of_entries, frame_type, THREAD);
3217     }
3218 
3219     // reserved for future use
3220     else if (frame_type &gt;= 128 &amp;&amp; frame_type &lt;= 246) {
3221       // nothing more to do for reserved frame_types
3222     }
3223 
3224     // same_locals_1_stack_item_frame_extended {
3225     //   u1 frame_type = SAME_LOCALS_1_STACK_ITEM_EXTENDED; /* 247 */
3226     //   u2 offset_delta;
3227     //   verification_type_info stack[1];
3228     // }
3229     else if (frame_type == 247) {
3230       stackmap_p += 2;
3231       rewrite_cp_refs_in_verification_type_info(stackmap_p, stackmap_end,
3232         calc_number_of_entries, frame_type, THREAD);
3233     }
3234 
3235     // chop_frame {
3236     //   u1 frame_type = CHOP; /* 248-250 */
3237     //   u2 offset_delta;
3238     // }
3239     else if (frame_type &gt;= 248 &amp;&amp; frame_type &lt;= 250) {
3240       stackmap_p += 2;
3241     }
3242 
3243     // same_frame_extended {
3244     //   u1 frame_type = SAME_FRAME_EXTENDED; /* 251*/
3245     //   u2 offset_delta;
3246     // }
3247     else if (frame_type == 251) {
3248       stackmap_p += 2;
3249     }
3250 
3251     // append_frame {
3252     //   u1 frame_type = APPEND; /* 252-254 */
3253     //   u2 offset_delta;
3254     //   verification_type_info locals[frame_type - 251];
3255     // }
3256     else if (frame_type &gt;= 252 &amp;&amp; frame_type &lt;= 254) {
3257       assert(stackmap_p + 2 &lt;= stackmap_end,
3258         &quot;no room for offset_delta&quot;);
3259       stackmap_p += 2;
3260       u1 len = frame_type - 251;
3261       for (u1 i = 0; i &lt; len; i++) {
3262         rewrite_cp_refs_in_verification_type_info(stackmap_p, stackmap_end,
3263           calc_number_of_entries, frame_type, THREAD);
3264       }
3265     }
3266 
3267     // full_frame {
3268     //   u1 frame_type = FULL_FRAME; /* 255 */
3269     //   u2 offset_delta;
3270     //   u2 number_of_locals;
3271     //   verification_type_info locals[number_of_locals];
3272     //   u2 number_of_stack_items;
3273     //   verification_type_info stack[number_of_stack_items];
3274     // }
3275     else if (frame_type == 255) {
3276       assert(stackmap_p + 2 + 2 &lt;= stackmap_end,
3277         &quot;no room for smallest full_frame&quot;);
3278       stackmap_p += 2;
3279 
3280       u2 number_of_locals = Bytes::get_Java_u2(stackmap_p);
3281       stackmap_p += 2;
3282 
3283       for (u2 locals_i = 0; locals_i &lt; number_of_locals; locals_i++) {
3284         rewrite_cp_refs_in_verification_type_info(stackmap_p, stackmap_end,
3285           calc_number_of_entries, frame_type, THREAD);
3286       }
3287 
3288       // Use the largest size for the number_of_stack_items, but only get
3289       // the right number of bytes.
3290       u2 number_of_stack_items = Bytes::get_Java_u2(stackmap_p);
3291       stackmap_p += 2;
3292 
3293       for (u2 stack_i = 0; stack_i &lt; number_of_stack_items; stack_i++) {
3294         rewrite_cp_refs_in_verification_type_info(stackmap_p, stackmap_end,
3295           calc_number_of_entries, frame_type, THREAD);
3296       }
3297     }
3298   } // end while there is a stack_map_frame
3299   assert(number_of_entries == calc_number_of_entries, &quot;sanity check&quot;);
3300 } // end rewrite_cp_refs_in_stack_map_table()
3301 
3302 
3303 // Rewrite constant pool references in the verification type info
3304 // portion of the method&#39;s stackmap table. These &quot;structures&quot; are
3305 // adapted from the StackMapTable_attribute that is described in
3306 // section 4.8.4 of the 6.0 version of the VM spec (dated 2005.10.26):
3307 // file:///net/quincunx.sfbay/export/gbracha/ClassFile-Java6.pdf
3308 //
3309 // The verification_type_info structure is a u1 tag followed by 0 or
3310 // more bytes of data:
3311 //
3312 // union verification_type_info {
3313 //   Top_variable_info;
3314 //   Integer_variable_info;
3315 //   Float_variable_info;
3316 //   Long_variable_info;
3317 //   Double_variable_info;
3318 //   Null_variable_info;
3319 //   UninitializedThis_variable_info;
3320 //   Object_variable_info;
3321 //   Uninitialized_variable_info;
3322 // }
3323 //
3324 void VM_RedefineClasses::rewrite_cp_refs_in_verification_type_info(
3325        address&amp; stackmap_p_ref, address stackmap_end, u2 frame_i,
3326        u1 frame_type, TRAPS) {
3327 
3328   assert(stackmap_p_ref + 1 &lt;= stackmap_end, &quot;no room for tag&quot;);
3329   u1 tag = *stackmap_p_ref;
3330   stackmap_p_ref++;
3331 
3332   switch (tag) {
3333   // Top_variable_info {
3334   //   u1 tag = ITEM_Top; /* 0 */
3335   // }
3336   // verificationType.hpp has zero as ITEM_Bogus instead of ITEM_Top
3337   case 0:  // fall through
3338 
3339   // Integer_variable_info {
3340   //   u1 tag = ITEM_Integer; /* 1 */
3341   // }
3342   case ITEM_Integer:  // fall through
3343 
3344   // Float_variable_info {
3345   //   u1 tag = ITEM_Float; /* 2 */
3346   // }
3347   case ITEM_Float:  // fall through
3348 
3349   // Double_variable_info {
3350   //   u1 tag = ITEM_Double; /* 3 */
3351   // }
3352   case ITEM_Double:  // fall through
3353 
3354   // Long_variable_info {
3355   //   u1 tag = ITEM_Long; /* 4 */
3356   // }
3357   case ITEM_Long:  // fall through
3358 
3359   // Null_variable_info {
3360   //   u1 tag = ITEM_Null; /* 5 */
3361   // }
3362   case ITEM_Null:  // fall through
3363 
3364   // UninitializedThis_variable_info {
3365   //   u1 tag = ITEM_UninitializedThis; /* 6 */
3366   // }
3367   case ITEM_UninitializedThis:
3368     // nothing more to do for the above tag types
3369     break;
3370 
3371   // Object_variable_info {
3372   //   u1 tag = ITEM_Object; /* 7 */
3373   //   u2 cpool_index;
3374   // }
3375   case ITEM_Object:
3376   {
3377     assert(stackmap_p_ref + 2 &lt;= stackmap_end, &quot;no room for cpool_index&quot;);
3378     u2 cpool_index = Bytes::get_Java_u2(stackmap_p_ref);
3379     u2 new_cp_index = find_new_index(cpool_index);
3380     if (new_cp_index != 0) {
3381       log_debug(redefine, class, stackmap)(&quot;mapped old cpool_index=%d&quot;, cpool_index);
3382       Bytes::put_Java_u2(stackmap_p_ref, new_cp_index);
3383       cpool_index = new_cp_index;
3384     }
3385     stackmap_p_ref += 2;
3386 
3387     log_debug(redefine, class, stackmap)
3388       (&quot;frame_i=%u, frame_type=%u, cpool_index=%d&quot;, frame_i, frame_type, cpool_index);
3389   } break;
3390 
3391   // Uninitialized_variable_info {
3392   //   u1 tag = ITEM_Uninitialized; /* 8 */
3393   //   u2 offset;
3394   // }
3395   case ITEM_Uninitialized:
3396     assert(stackmap_p_ref + 2 &lt;= stackmap_end, &quot;no room for offset&quot;);
3397     stackmap_p_ref += 2;
3398     break;
3399 
3400   default:
3401     log_debug(redefine, class, stackmap)(&quot;frame_i=%u, frame_type=%u, bad tag=0x%x&quot;, frame_i, frame_type, tag);
3402     ShouldNotReachHere();
3403     break;
3404   } // end switch (tag)
3405 } // end rewrite_cp_refs_in_verification_type_info()
3406 
3407 
3408 // Change the constant pool associated with klass scratch_class to
3409 // scratch_cp. If shrink is true, then scratch_cp_length elements
3410 // are copied from scratch_cp to a smaller constant pool and the
3411 // smaller constant pool is associated with scratch_class.
3412 void VM_RedefineClasses::set_new_constant_pool(
3413        ClassLoaderData* loader_data,
3414        InstanceKlass* scratch_class, constantPoolHandle scratch_cp,
3415        int scratch_cp_length, TRAPS) {
3416   assert(scratch_cp-&gt;length() &gt;= scratch_cp_length, &quot;sanity check&quot;);
3417 
3418   // scratch_cp is a merged constant pool and has enough space for a
3419   // worst case merge situation. We want to associate the minimum
3420   // sized constant pool with the klass to save space.
3421   ConstantPool* cp = ConstantPool::allocate(loader_data, scratch_cp_length, CHECK);
3422   constantPoolHandle smaller_cp(THREAD, cp);
3423 
3424   // preserve version() value in the smaller copy
3425   int version = scratch_cp-&gt;version();
3426   assert(version != 0, &quot;sanity check&quot;);
3427   smaller_cp-&gt;set_version(version);
3428 
3429   // attach klass to new constant pool
3430   // reference to the cp holder is needed for copy_operands()
3431   smaller_cp-&gt;set_pool_holder(scratch_class);
3432 
3433   smaller_cp-&gt;copy_fields(scratch_cp());
3434 
3435   scratch_cp-&gt;copy_cp_to(1, scratch_cp_length - 1, smaller_cp, 1, THREAD);
3436   if (HAS_PENDING_EXCEPTION) {
3437     // Exception is handled in the caller
3438     loader_data-&gt;add_to_deallocate_list(smaller_cp());
3439     return;
3440   }
3441   scratch_cp = smaller_cp;
3442 
3443   // attach new constant pool to klass
3444   scratch_class-&gt;set_constants(scratch_cp());
3445   scratch_cp-&gt;initialize_unresolved_klasses(loader_data, CHECK);
3446 
3447   int i;  // for portability
3448 
3449   // update each field in klass to use new constant pool indices as needed
3450   for (JavaFieldStream fs(scratch_class); !fs.done(); fs.next()) {
3451     jshort cur_index = fs.name_index();
3452     jshort new_index = find_new_index(cur_index);
3453     if (new_index != 0) {
3454       log_trace(redefine, class, constantpool)(&quot;field-name_index change: %d to %d&quot;, cur_index, new_index);
3455       fs.set_name_index(new_index);
3456     }
3457     cur_index = fs.signature_index();
3458     new_index = find_new_index(cur_index);
3459     if (new_index != 0) {
3460       log_trace(redefine, class, constantpool)(&quot;field-signature_index change: %d to %d&quot;, cur_index, new_index);
3461       fs.set_signature_index(new_index);
3462     }
3463     cur_index = fs.initval_index();
3464     new_index = find_new_index(cur_index);
3465     if (new_index != 0) {
3466       log_trace(redefine, class, constantpool)(&quot;field-initval_index change: %d to %d&quot;, cur_index, new_index);
3467       fs.set_initval_index(new_index);
3468     }
3469     cur_index = fs.generic_signature_index();
3470     new_index = find_new_index(cur_index);
3471     if (new_index != 0) {
3472       log_trace(redefine, class, constantpool)(&quot;field-generic_signature change: %d to %d&quot;, cur_index, new_index);
3473       fs.set_generic_signature_index(new_index);
3474     }
3475   } // end for each field
3476 
3477   // Update constant pool indices in the inner classes info to use
3478   // new constant indices as needed. The inner classes info is a
3479   // quadruple:
3480   // (inner_class_info, outer_class_info, inner_name, inner_access_flags)
3481   InnerClassesIterator iter(scratch_class);
3482   for (; !iter.done(); iter.next()) {
3483     int cur_index = iter.inner_class_info_index();
3484     if (cur_index == 0) {
3485       continue;  // JVM spec. allows null inner class refs so skip it
3486     }
3487     int new_index = find_new_index(cur_index);
3488     if (new_index != 0) {
3489       log_trace(redefine, class, constantpool)(&quot;inner_class_info change: %d to %d&quot;, cur_index, new_index);
3490       iter.set_inner_class_info_index(new_index);
3491     }
3492     cur_index = iter.outer_class_info_index();
3493     new_index = find_new_index(cur_index);
3494     if (new_index != 0) {
3495       log_trace(redefine, class, constantpool)(&quot;outer_class_info change: %d to %d&quot;, cur_index, new_index);
3496       iter.set_outer_class_info_index(new_index);
3497     }
3498     cur_index = iter.inner_name_index();
3499     new_index = find_new_index(cur_index);
3500     if (new_index != 0) {
3501       log_trace(redefine, class, constantpool)(&quot;inner_name change: %d to %d&quot;, cur_index, new_index);
3502       iter.set_inner_name_index(new_index);
3503     }
3504   } // end for each inner class
3505 
3506   // Attach each method in klass to the new constant pool and update
3507   // to use new constant pool indices as needed:
3508   Array&lt;Method*&gt;* methods = scratch_class-&gt;methods();
3509   for (i = methods-&gt;length() - 1; i &gt;= 0; i--) {
3510     methodHandle method(THREAD, methods-&gt;at(i));
3511     method-&gt;set_constants(scratch_cp());
3512 
3513     int new_index = find_new_index(method-&gt;name_index());
3514     if (new_index != 0) {
3515       log_trace(redefine, class, constantpool)
3516         (&quot;method-name_index change: %d to %d&quot;, method-&gt;name_index(), new_index);
3517       method-&gt;set_name_index(new_index);
3518     }
3519     new_index = find_new_index(method-&gt;signature_index());
3520     if (new_index != 0) {
3521       log_trace(redefine, class, constantpool)
3522         (&quot;method-signature_index change: %d to %d&quot;, method-&gt;signature_index(), new_index);
3523       method-&gt;set_signature_index(new_index);
3524     }
3525     new_index = find_new_index(method-&gt;generic_signature_index());
3526     if (new_index != 0) {
3527       log_trace(redefine, class, constantpool)
3528         (&quot;method-generic_signature_index change: %d to %d&quot;, method-&gt;generic_signature_index(), new_index);
3529       method-&gt;set_generic_signature_index(new_index);
3530     }
3531 
3532     // Update constant pool indices in the method&#39;s checked exception
3533     // table to use new constant indices as needed.
3534     int cext_length = method-&gt;checked_exceptions_length();
3535     if (cext_length &gt; 0) {
3536       CheckedExceptionElement * cext_table =
3537         method-&gt;checked_exceptions_start();
3538       for (int j = 0; j &lt; cext_length; j++) {
3539         int cur_index = cext_table[j].class_cp_index;
3540         int new_index = find_new_index(cur_index);
3541         if (new_index != 0) {
3542           log_trace(redefine, class, constantpool)(&quot;cext-class_cp_index change: %d to %d&quot;, cur_index, new_index);
3543           cext_table[j].class_cp_index = (u2)new_index;
3544         }
3545       } // end for each checked exception table entry
3546     } // end if there are checked exception table entries
3547 
3548     // Update each catch type index in the method&#39;s exception table
3549     // to use new constant pool indices as needed. The exception table
3550     // holds quadruple entries of the form:
3551     //   (beg_bci, end_bci, handler_bci, klass_index)
3552 
3553     ExceptionTable ex_table(method());
3554     int ext_length = ex_table.length();
3555 
3556     for (int j = 0; j &lt; ext_length; j ++) {
3557       int cur_index = ex_table.catch_type_index(j);
3558       int new_index = find_new_index(cur_index);
3559       if (new_index != 0) {
3560         log_trace(redefine, class, constantpool)(&quot;ext-klass_index change: %d to %d&quot;, cur_index, new_index);
3561         ex_table.set_catch_type_index(j, new_index);
3562       }
3563     } // end for each exception table entry
3564 
3565     // Update constant pool indices in the method&#39;s local variable
3566     // table to use new constant indices as needed. The local variable
3567     // table hold sextuple entries of the form:
3568     // (start_pc, length, name_index, descriptor_index, signature_index, slot)
3569     int lvt_length = method-&gt;localvariable_table_length();
3570     if (lvt_length &gt; 0) {
3571       LocalVariableTableElement * lv_table =
3572         method-&gt;localvariable_table_start();
3573       for (int j = 0; j &lt; lvt_length; j++) {
3574         int cur_index = lv_table[j].name_cp_index;
3575         int new_index = find_new_index(cur_index);
3576         if (new_index != 0) {
3577           log_trace(redefine, class, constantpool)(&quot;lvt-name_cp_index change: %d to %d&quot;, cur_index, new_index);
3578           lv_table[j].name_cp_index = (u2)new_index;
3579         }
3580         cur_index = lv_table[j].descriptor_cp_index;
3581         new_index = find_new_index(cur_index);
3582         if (new_index != 0) {
3583           log_trace(redefine, class, constantpool)(&quot;lvt-descriptor_cp_index change: %d to %d&quot;, cur_index, new_index);
3584           lv_table[j].descriptor_cp_index = (u2)new_index;
3585         }
3586         cur_index = lv_table[j].signature_cp_index;
3587         new_index = find_new_index(cur_index);
3588         if (new_index != 0) {
3589           log_trace(redefine, class, constantpool)(&quot;lvt-signature_cp_index change: %d to %d&quot;, cur_index, new_index);
3590           lv_table[j].signature_cp_index = (u2)new_index;
3591         }
3592       } // end for each local variable table entry
3593     } // end if there are local variable table entries
3594 
3595     rewrite_cp_refs_in_stack_map_table(method, THREAD);
3596   } // end for each method
3597 } // end set_new_constant_pool()
3598 
3599 
3600 // Unevolving classes may point to methods of the_class directly
3601 // from their constant pool caches, itables, and/or vtables. We
3602 // use the ClassLoaderDataGraph::classes_do() facility and this helper
3603 // to fix up these pointers.  MethodData also points to old methods and
3604 // must be cleaned.
3605 
3606 // Adjust cpools and vtables closure
3607 void VM_RedefineClasses::AdjustAndCleanMetadata::do_klass(Klass* k) {
3608 
3609   // This is a very busy routine. We don&#39;t want too much tracing
3610   // printed out.
3611   bool trace_name_printed = false;
3612 
3613   // If the class being redefined is java.lang.Object, we need to fix all
3614   // array class vtables also. The _has_redefined_Object flag is global.
3615   // Once the java.lang.Object has been redefined (by the current or one
3616   // of the previous VM_RedefineClasses operations) we have to always
3617   // adjust method entries for array classes.
3618   if (k-&gt;is_array_klass() &amp;&amp; _has_redefined_Object) {
3619     k-&gt;vtable().adjust_method_entries(&amp;trace_name_printed);
3620 
3621   } else if (k-&gt;is_instance_klass()) {
3622     HandleMark hm(_thread);
3623     InstanceKlass *ik = InstanceKlass::cast(k);
3624 
3625     // Clean MethodData of this class&#39;s methods so they don&#39;t refer to
3626     // old methods that are no longer running.
3627     Array&lt;Method*&gt;* methods = ik-&gt;methods();
3628     int num_methods = methods-&gt;length();
3629     for (int index = 0; index &lt; num_methods; ++index) {
3630       if (methods-&gt;at(index)-&gt;method_data() != NULL) {
3631         methods-&gt;at(index)-&gt;method_data()-&gt;clean_weak_method_links();
3632       }
3633     }
3634 
3635     // Adjust all vtables, default methods and itables, to clean out old methods.
3636     ResourceMark rm(_thread);
3637     if (ik-&gt;vtable_length() &gt; 0) {
3638       ik-&gt;vtable().adjust_method_entries(&amp;trace_name_printed);
3639       ik-&gt;adjust_default_methods(&amp;trace_name_printed);
3640     }
3641 
3642     if (ik-&gt;itable_length() &gt; 0) {
3643       ik-&gt;itable().adjust_method_entries(&amp;trace_name_printed);
3644     }
3645 
3646     // The constant pools in other classes (other_cp) can refer to
3647     // old methods.  We have to update method information in
3648     // other_cp&#39;s cache. If other_cp has a previous version, then we
3649     // have to repeat the process for each previous version. The
3650     // constant pool cache holds the Method*s for non-virtual
3651     // methods and for virtual, final methods.
3652     //
3653     // Special case: if the current class is being redefined by the current
3654     // VM_RedefineClasses operation, then new_cp has already been attached
3655     // to the_class and old_cp has already been added as a previous version.
3656     // The new_cp doesn&#39;t have any cached references to old methods so it
3657     // doesn&#39;t need to be updated and we could optimize by skipping it.
3658     // However, the current class can be marked as being redefined by another
3659     // VM_RedefineClasses operation which has already executed its doit_prologue
3660     // and needs cpcache method entries adjusted. For simplicity, the cpcache
3661     // update is done unconditionally. It should result in doing nothing for
3662     // classes being redefined by the current VM_RedefineClasses operation.
3663     // Method entries in the previous version(s) are adjusted as well.
3664     ConstantPoolCache* cp_cache;
3665 
3666     // this klass&#39; constant pool cache may need adjustment
3667     ConstantPool* other_cp = ik-&gt;constants();
3668     cp_cache = other_cp-&gt;cache();
3669     if (cp_cache != NULL) {
3670       cp_cache-&gt;adjust_method_entries(&amp;trace_name_printed);
3671     }
3672 
3673     // the previous versions&#39; constant pool caches may need adjustment
3674     for (InstanceKlass* pv_node = ik-&gt;previous_versions();
3675          pv_node != NULL;
3676          pv_node = pv_node-&gt;previous_versions()) {
3677       cp_cache = pv_node-&gt;constants()-&gt;cache();
3678       if (cp_cache != NULL) {
3679         cp_cache-&gt;adjust_method_entries(&amp;trace_name_printed);
3680       }
3681     }
3682   }
3683 }
3684 
3685 void VM_RedefineClasses::update_jmethod_ids(Thread* thread) {
3686   for (int j = 0; j &lt; _matching_methods_length; ++j) {
3687     Method* old_method = _matching_old_methods[j];
3688     jmethodID jmid = old_method-&gt;find_jmethod_id_or_null();
3689     if (jmid != NULL) {
3690       // There is a jmethodID, change it to point to the new method
3691       methodHandle new_method_h(thread, _matching_new_methods[j]);
3692       Method::change_method_associated_with_jmethod_id(jmid, new_method_h());
3693       assert(Method::resolve_jmethod_id(jmid) == _matching_new_methods[j],
3694              &quot;should be replaced&quot;);
3695     }
3696   }
3697 }
3698 
3699 int VM_RedefineClasses::check_methods_and_mark_as_obsolete() {
3700   int emcp_method_count = 0;
3701   int obsolete_count = 0;
3702   int old_index = 0;
3703   for (int j = 0; j &lt; _matching_methods_length; ++j, ++old_index) {
3704     Method* old_method = _matching_old_methods[j];
3705     Method* new_method = _matching_new_methods[j];
3706     Method* old_array_method;
3707 
3708     // Maintain an old_index into the _old_methods array by skipping
3709     // deleted methods
3710     while ((old_array_method = _old_methods-&gt;at(old_index)) != old_method) {
3711       ++old_index;
3712     }
3713 
3714     if (MethodComparator::methods_EMCP(old_method, new_method)) {
3715       // The EMCP definition from JSR-163 requires the bytecodes to be
3716       // the same with the exception of constant pool indices which may
3717       // differ. However, the constants referred to by those indices
3718       // must be the same.
3719       //
3720       // We use methods_EMCP() for comparison since constant pool
3721       // merging can remove duplicate constant pool entries that were
3722       // present in the old method and removed from the rewritten new
3723       // method. A faster binary comparison function would consider the
3724       // old and new methods to be different when they are actually
3725       // EMCP.
3726       //
3727       // The old and new methods are EMCP and you would think that we
3728       // could get rid of one of them here and now and save some space.
3729       // However, the concept of EMCP only considers the bytecodes and
3730       // the constant pool entries in the comparison. Other things,
3731       // e.g., the line number table (LNT) or the local variable table
3732       // (LVT) don&#39;t count in the comparison. So the new (and EMCP)
3733       // method can have a new LNT that we need so we can&#39;t just
3734       // overwrite the new method with the old method.
3735       //
3736       // When this routine is called, we have already attached the new
3737       // methods to the_class so the old methods are effectively
3738       // overwritten. However, if an old method is still executing,
3739       // then the old method cannot be collected until sometime after
3740       // the old method call has returned. So the overwriting of old
3741       // methods by new methods will save us space except for those
3742       // (hopefully few) old methods that are still executing.
3743       //
3744       // A method refers to a ConstMethod* and this presents another
3745       // possible avenue to space savings. The ConstMethod* in the
3746       // new method contains possibly new attributes (LNT, LVT, etc).
3747       // At first glance, it seems possible to save space by replacing
3748       // the ConstMethod* in the old method with the ConstMethod*
3749       // from the new method. The old and new methods would share the
3750       // same ConstMethod* and we would save the space occupied by
3751       // the old ConstMethod*. However, the ConstMethod* contains
3752       // a back reference to the containing method. Sharing the
3753       // ConstMethod* between two methods could lead to confusion in
3754       // the code that uses the back reference. This would lead to
3755       // brittle code that could be broken in non-obvious ways now or
3756       // in the future.
3757       //
3758       // Another possibility is to copy the ConstMethod* from the new
3759       // method to the old method and then overwrite the new method with
3760       // the old method. Since the ConstMethod* contains the bytecodes
3761       // for the method embedded in the oop, this option would change
3762       // the bytecodes out from under any threads executing the old
3763       // method and make the thread&#39;s bcp invalid. Since EMCP requires
3764       // that the bytecodes be the same modulo constant pool indices, it
3765       // is straight forward to compute the correct new bcp in the new
3766       // ConstMethod* from the old bcp in the old ConstMethod*. The
3767       // time consuming part would be searching all the frames in all
3768       // of the threads to find all of the calls to the old method.
3769       //
3770       // It looks like we will have to live with the limited savings
3771       // that we get from effectively overwriting the old methods
3772       // when the new methods are attached to the_class.
3773 
3774       // Count number of methods that are EMCP.  The method will be marked
3775       // old but not obsolete if it is EMCP.
3776       emcp_method_count++;
3777 
3778       // An EMCP method is _not_ obsolete. An obsolete method has a
3779       // different jmethodID than the current method. An EMCP method
3780       // has the same jmethodID as the current method. Having the
3781       // same jmethodID for all EMCP versions of a method allows for
3782       // a consistent view of the EMCP methods regardless of which
3783       // EMCP method you happen to have in hand. For example, a
3784       // breakpoint set in one EMCP method will work for all EMCP
3785       // versions of the method including the current one.
3786     } else {
3787       // mark obsolete methods as such
3788       old_method-&gt;set_is_obsolete();
3789       obsolete_count++;
3790 
3791       // obsolete methods need a unique idnum so they become new entries in
3792       // the jmethodID cache in InstanceKlass
3793       assert(old_method-&gt;method_idnum() == new_method-&gt;method_idnum(), &quot;must match&quot;);
3794       u2 num = InstanceKlass::cast(_the_class)-&gt;next_method_idnum();
3795       if (num != ConstMethod::UNSET_IDNUM) {
3796         old_method-&gt;set_method_idnum(num);
3797       }
3798 
3799       // With tracing we try not to &quot;yack&quot; too much. The position of
3800       // this trace assumes there are fewer obsolete methods than
3801       // EMCP methods.
3802       if (log_is_enabled(Trace, redefine, class, obsolete, mark)) {
3803         ResourceMark rm;
3804         log_trace(redefine, class, obsolete, mark)
3805           (&quot;mark %s(%s) as obsolete&quot;, old_method-&gt;name()-&gt;as_C_string(), old_method-&gt;signature()-&gt;as_C_string());
3806       }
3807     }
3808     old_method-&gt;set_is_old();
3809   }
3810   for (int i = 0; i &lt; _deleted_methods_length; ++i) {
3811     Method* old_method = _deleted_methods[i];
3812 
3813     assert(!old_method-&gt;has_vtable_index(),
3814            &quot;cannot delete methods with vtable entries&quot;);;
3815 
3816     // Mark all deleted methods as old, obsolete and deleted
3817     old_method-&gt;set_is_deleted();
3818     old_method-&gt;set_is_old();
3819     old_method-&gt;set_is_obsolete();
3820     ++obsolete_count;
3821     // With tracing we try not to &quot;yack&quot; too much. The position of
3822     // this trace assumes there are fewer obsolete methods than
3823     // EMCP methods.
3824     if (log_is_enabled(Trace, redefine, class, obsolete, mark)) {
3825       ResourceMark rm;
3826       log_trace(redefine, class, obsolete, mark)
3827         (&quot;mark deleted %s(%s) as obsolete&quot;, old_method-&gt;name()-&gt;as_C_string(), old_method-&gt;signature()-&gt;as_C_string());
3828     }
3829   }
3830   assert((emcp_method_count + obsolete_count) == _old_methods-&gt;length(),
3831     &quot;sanity check&quot;);
3832   log_trace(redefine, class, obsolete, mark)(&quot;EMCP_cnt=%d, obsolete_cnt=%d&quot;, emcp_method_count, obsolete_count);
3833   return emcp_method_count;
3834 }
3835 
3836 // This internal class transfers the native function registration from old methods
3837 // to new methods.  It is designed to handle both the simple case of unchanged
3838 // native methods and the complex cases of native method prefixes being added and/or
3839 // removed.
3840 // It expects only to be used during the VM_RedefineClasses op (a safepoint).
3841 //
3842 // This class is used after the new methods have been installed in &quot;the_class&quot;.
3843 //
3844 // So, for example, the following must be handled.  Where &#39;m&#39; is a method and
3845 // a number followed by an underscore is a prefix.
3846 //
3847 //                                      Old Name    New Name
3848 // Simple transfer to new method        m       -&gt;  m
3849 // Add prefix                           m       -&gt;  1_m
3850 // Remove prefix                        1_m     -&gt;  m
3851 // Simultaneous add of prefixes         m       -&gt;  3_2_1_m
3852 // Simultaneous removal of prefixes     3_2_1_m -&gt;  m
3853 // Simultaneous add and remove          1_m     -&gt;  2_m
3854 // Same, caused by prefix removal only  3_2_1_m -&gt;  3_2_m
3855 //
3856 class TransferNativeFunctionRegistration {
3857  private:
3858   InstanceKlass* the_class;
3859   int prefix_count;
3860   char** prefixes;
3861 
3862   // Recursively search the binary tree of possibly prefixed method names.
3863   // Iteration could be used if all agents were well behaved. Full tree walk is
3864   // more resilent to agents not cleaning up intermediate methods.
3865   // Branch at each depth in the binary tree is:
3866   //    (1) without the prefix.
3867   //    (2) with the prefix.
3868   // where &#39;prefix&#39; is the prefix at that &#39;depth&#39; (first prefix, second prefix,...)
3869   Method* search_prefix_name_space(int depth, char* name_str, size_t name_len,
3870                                      Symbol* signature) {
3871     TempNewSymbol name_symbol = SymbolTable::probe(name_str, (int)name_len);
3872     if (name_symbol != NULL) {
3873       Method* method = the_class-&gt;lookup_method(name_symbol, signature);
3874       if (method != NULL) {
3875         // Even if prefixed, intermediate methods must exist.
3876         if (method-&gt;is_native()) {
3877           // Wahoo, we found a (possibly prefixed) version of the method, return it.
3878           return method;
3879         }
3880         if (depth &lt; prefix_count) {
3881           // Try applying further prefixes (other than this one).
3882           method = search_prefix_name_space(depth+1, name_str, name_len, signature);
3883           if (method != NULL) {
3884             return method; // found
3885           }
3886 
3887           // Try adding this prefix to the method name and see if it matches
3888           // another method name.
3889           char* prefix = prefixes[depth];
3890           size_t prefix_len = strlen(prefix);
3891           size_t trial_len = name_len + prefix_len;
3892           char* trial_name_str = NEW_RESOURCE_ARRAY(char, trial_len + 1);
3893           strcpy(trial_name_str, prefix);
3894           strcat(trial_name_str, name_str);
3895           method = search_prefix_name_space(depth+1, trial_name_str, trial_len,
3896                                             signature);
3897           if (method != NULL) {
3898             // If found along this branch, it was prefixed, mark as such
3899             method-&gt;set_is_prefixed_native();
3900             return method; // found
3901           }
3902         }
3903       }
3904     }
3905     return NULL;  // This whole branch bore nothing
3906   }
3907 
3908   // Return the method name with old prefixes stripped away.
3909   char* method_name_without_prefixes(Method* method) {
3910     Symbol* name = method-&gt;name();
3911     char* name_str = name-&gt;as_utf8();
3912 
3913     // Old prefixing may be defunct, strip prefixes, if any.
3914     for (int i = prefix_count-1; i &gt;= 0; i--) {
3915       char* prefix = prefixes[i];
3916       size_t prefix_len = strlen(prefix);
3917       if (strncmp(prefix, name_str, prefix_len) == 0) {
3918         name_str += prefix_len;
3919       }
3920     }
3921     return name_str;
3922   }
3923 
3924   // Strip any prefixes off the old native method, then try to find a
3925   // (possibly prefixed) new native that matches it.
3926   Method* strip_and_search_for_new_native(Method* method) {
3927     ResourceMark rm;
3928     char* name_str = method_name_without_prefixes(method);
3929     return search_prefix_name_space(0, name_str, strlen(name_str),
3930                                     method-&gt;signature());
3931   }
3932 
3933  public:
3934 
3935   // Construct a native method transfer processor for this class.
3936   TransferNativeFunctionRegistration(InstanceKlass* _the_class) {
3937     assert(SafepointSynchronize::is_at_safepoint(), &quot;sanity check&quot;);
3938 
3939     the_class = _the_class;
3940     prefixes = JvmtiExport::get_all_native_method_prefixes(&amp;prefix_count);
3941   }
3942 
3943   // Attempt to transfer any of the old or deleted methods that are native
3944   void transfer_registrations(Method** old_methods, int methods_length) {
3945     for (int j = 0; j &lt; methods_length; j++) {
3946       Method* old_method = old_methods[j];
3947 
3948       if (old_method-&gt;is_native() &amp;&amp; old_method-&gt;has_native_function()) {
3949         Method* new_method = strip_and_search_for_new_native(old_method);
3950         if (new_method != NULL) {
3951           // Actually set the native function in the new method.
3952           // Redefine does not send events (except CFLH), certainly not this
3953           // behind the scenes re-registration.
3954           new_method-&gt;set_native_function(old_method-&gt;native_function(),
3955                               !Method::native_bind_event_is_interesting);
3956         }
3957       }
3958     }
3959   }
3960 };
3961 
3962 // Don&#39;t lose the association between a native method and its JNI function.
3963 void VM_RedefineClasses::transfer_old_native_function_registrations(InstanceKlass* the_class) {
3964   TransferNativeFunctionRegistration transfer(the_class);
3965   transfer.transfer_registrations(_deleted_methods, _deleted_methods_length);
3966   transfer.transfer_registrations(_matching_old_methods, _matching_methods_length);
3967 }
3968 
3969 // Deoptimize all compiled code that depends on this class.
3970 //
3971 // If the can_redefine_classes capability is obtained in the onload
3972 // phase then the compiler has recorded all dependencies from startup.
3973 // In that case we need only deoptimize and throw away all compiled code
3974 // that depends on the class.
3975 //
3976 // If can_redefine_classes is obtained sometime after the onload
3977 // phase then the dependency information may be incomplete. In that case
3978 // the first call to RedefineClasses causes all compiled code to be
3979 // thrown away. As can_redefine_classes has been obtained then
3980 // all future compilations will record dependencies so second and
3981 // subsequent calls to RedefineClasses need only throw away code
3982 // that depends on the class.
3983 //
3984 
3985 // First step is to walk the code cache for each class redefined and mark
3986 // dependent methods.  Wait until all classes are processed to deoptimize everything.
3987 void VM_RedefineClasses::mark_dependent_code(InstanceKlass* ik) {
3988   assert_locked_or_safepoint(Compile_lock);
3989 
3990   // All dependencies have been recorded from startup or this is a second or
3991   // subsequent use of RedefineClasses
3992   if (JvmtiExport::all_dependencies_are_recorded()) {
3993     CodeCache::mark_for_evol_deoptimization(ik);
3994   }
3995 }
3996 
3997 void VM_RedefineClasses::flush_dependent_code() {
3998   assert(SafepointSynchronize::is_at_safepoint(), &quot;sanity check&quot;);
3999 
4000   bool deopt_needed;
4001 
4002   // This is the first redefinition, mark all the nmethods for deoptimization
4003   if (!JvmtiExport::all_dependencies_are_recorded()) {
4004     log_debug(redefine, class, nmethod)(&quot;Marked all nmethods for deopt&quot;);
4005     CodeCache::mark_all_nmethods_for_evol_deoptimization();
4006     deopt_needed = true;
4007   } else {
4008     int deopt = CodeCache::mark_dependents_for_evol_deoptimization();
4009     log_debug(redefine, class, nmethod)(&quot;Marked %d dependent nmethods for deopt&quot;, deopt);
4010     deopt_needed = (deopt != 0);
4011   }
4012 
4013   if (deopt_needed) {
4014     CodeCache::flush_evol_dependents();
4015   }
4016 
4017   // From now on we know that the dependency information is complete
4018   JvmtiExport::set_all_dependencies_are_recorded(true);
4019 }
4020 
4021 void VM_RedefineClasses::compute_added_deleted_matching_methods() {
4022   Method* old_method;
4023   Method* new_method;
4024 
4025   _matching_old_methods = NEW_RESOURCE_ARRAY(Method*, _old_methods-&gt;length());
4026   _matching_new_methods = NEW_RESOURCE_ARRAY(Method*, _old_methods-&gt;length());
4027   _added_methods        = NEW_RESOURCE_ARRAY(Method*, _new_methods-&gt;length());
4028   _deleted_methods      = NEW_RESOURCE_ARRAY(Method*, _old_methods-&gt;length());
4029 
4030   _matching_methods_length = 0;
4031   _deleted_methods_length  = 0;
4032   _added_methods_length    = 0;
4033 
4034   int nj = 0;
4035   int oj = 0;
4036   while (true) {
4037     if (oj &gt;= _old_methods-&gt;length()) {
4038       if (nj &gt;= _new_methods-&gt;length()) {
4039         break; // we&#39;ve looked at everything, done
4040       }
4041       // New method at the end
4042       new_method = _new_methods-&gt;at(nj);
4043       _added_methods[_added_methods_length++] = new_method;
4044       ++nj;
4045     } else if (nj &gt;= _new_methods-&gt;length()) {
4046       // Old method, at the end, is deleted
4047       old_method = _old_methods-&gt;at(oj);
4048       _deleted_methods[_deleted_methods_length++] = old_method;
4049       ++oj;
4050     } else {
4051       old_method = _old_methods-&gt;at(oj);
4052       new_method = _new_methods-&gt;at(nj);
4053       if (old_method-&gt;name() == new_method-&gt;name()) {
4054         if (old_method-&gt;signature() == new_method-&gt;signature()) {
4055           _matching_old_methods[_matching_methods_length  ] = old_method;
4056           _matching_new_methods[_matching_methods_length++] = new_method;
4057           ++nj;
4058           ++oj;
4059         } else {
4060           // added overloaded have already been moved to the end,
4061           // so this is a deleted overloaded method
4062           _deleted_methods[_deleted_methods_length++] = old_method;
4063           ++oj;
4064         }
4065       } else { // names don&#39;t match
4066         if (old_method-&gt;name()-&gt;fast_compare(new_method-&gt;name()) &gt; 0) {
4067           // new method
4068           _added_methods[_added_methods_length++] = new_method;
4069           ++nj;
4070         } else {
4071           // deleted method
4072           _deleted_methods[_deleted_methods_length++] = old_method;
4073           ++oj;
4074         }
4075       }
4076     }
4077   }
4078   assert(_matching_methods_length + _deleted_methods_length == _old_methods-&gt;length(), &quot;sanity&quot;);
4079   assert(_matching_methods_length + _added_methods_length == _new_methods-&gt;length(), &quot;sanity&quot;);
4080 }
4081 
4082 
4083 void VM_RedefineClasses::swap_annotations(InstanceKlass* the_class,
4084                                           InstanceKlass* scratch_class) {
4085   // Swap annotation fields values
4086   Annotations* old_annotations = the_class-&gt;annotations();
4087   the_class-&gt;set_annotations(scratch_class-&gt;annotations());
4088   scratch_class-&gt;set_annotations(old_annotations);
4089 }
4090 
4091 
4092 // Install the redefinition of a class:
4093 //    - house keeping (flushing breakpoints and caches, deoptimizing
4094 //      dependent compiled code)
4095 //    - replacing parts in the_class with parts from scratch_class
4096 //    - adding a weak reference to track the obsolete but interesting
4097 //      parts of the_class
4098 //    - adjusting constant pool caches and vtables in other classes
4099 //      that refer to methods in the_class. These adjustments use the
4100 //      ClassLoaderDataGraph::classes_do() facility which only allows
4101 //      a helper method to be specified. The interesting parameters
4102 //      that we would like to pass to the helper method are saved in
4103 //      static global fields in the VM operation.
4104 void VM_RedefineClasses::redefine_single_class(jclass the_jclass,
4105        InstanceKlass* scratch_class, TRAPS) {
4106 
4107   HandleMark hm(THREAD);   // make sure handles from this call are freed
4108 
4109   if (log_is_enabled(Info, redefine, class, timer)) {
4110     _timer_rsc_phase1.start();
4111   }
4112 
4113   InstanceKlass* the_class = get_ik(the_jclass);
4114 
4115   // Set a flag to control and optimize adjusting method entries
4116   _has_redefined_Object |= the_class == SystemDictionary::Object_klass();
4117 
4118   // Remove all breakpoints in methods of this class
4119   JvmtiBreakpoints&amp; jvmti_breakpoints = JvmtiCurrentBreakpoints::get_jvmti_breakpoints();
4120   jvmti_breakpoints.clearall_in_class_at_safepoint(the_class);
4121 
4122   // Mark all compiled code that depends on this class
4123   mark_dependent_code(the_class);
4124 
4125   _old_methods = the_class-&gt;methods();
4126   _new_methods = scratch_class-&gt;methods();
4127   _the_class = the_class;
4128   compute_added_deleted_matching_methods();
4129   update_jmethod_ids(THREAD);
4130 
4131   _any_class_has_resolved_methods = the_class-&gt;has_resolved_methods() || _any_class_has_resolved_methods;
4132 
4133   // Attach new constant pool to the original klass. The original
4134   // klass still refers to the old constant pool (for now).
4135   scratch_class-&gt;constants()-&gt;set_pool_holder(the_class);
4136 
4137 #if 0
4138   // In theory, with constant pool merging in place we should be able
4139   // to save space by using the new, merged constant pool in place of
4140   // the old constant pool(s). By &quot;pool(s)&quot; I mean the constant pool in
4141   // the klass version we are replacing now and any constant pool(s) in
4142   // previous versions of klass. Nice theory, doesn&#39;t work in practice.
4143   // When this code is enabled, even simple programs throw NullPointer
4144   // exceptions. I&#39;m guessing that this is caused by some constant pool
4145   // cache difference between the new, merged constant pool and the
4146   // constant pool that was just being used by the klass. I&#39;m keeping
4147   // this code around to archive the idea, but the code has to remain
4148   // disabled for now.
4149 
4150   // Attach each old method to the new constant pool. This can be
4151   // done here since we are past the bytecode verification and
4152   // constant pool optimization phases.
4153   for (int i = _old_methods-&gt;length() - 1; i &gt;= 0; i--) {
4154     Method* method = _old_methods-&gt;at(i);
4155     method-&gt;set_constants(scratch_class-&gt;constants());
4156   }
4157 
4158   // NOTE: this doesn&#39;t work because you can redefine the same class in two
4159   // threads, each getting their own constant pool data appended to the
4160   // original constant pool.  In order for the new methods to work when they
4161   // become old methods, they need to keep their updated copy of the constant pool.
4162 
4163   {
4164     // walk all previous versions of the klass
4165     InstanceKlass *ik = the_class;
4166     PreviousVersionWalker pvw(ik);
4167     do {
4168       ik = pvw.next_previous_version();
4169       if (ik != NULL) {
4170 
4171         // attach previous version of klass to the new constant pool
4172         ik-&gt;set_constants(scratch_class-&gt;constants());
4173 
4174         // Attach each method in the previous version of klass to the
4175         // new constant pool
4176         Array&lt;Method*&gt;* prev_methods = ik-&gt;methods();
4177         for (int i = prev_methods-&gt;length() - 1; i &gt;= 0; i--) {
4178           Method* method = prev_methods-&gt;at(i);
4179           method-&gt;set_constants(scratch_class-&gt;constants());
4180         }
4181       }
4182     } while (ik != NULL);
4183   }
4184 #endif
4185 
4186   // Replace methods and constantpool
4187   the_class-&gt;set_methods(_new_methods);
4188   scratch_class-&gt;set_methods(_old_methods);     // To prevent potential GCing of the old methods,
4189                                           // and to be able to undo operation easily.
4190 
4191   Array&lt;int&gt;* old_ordering = the_class-&gt;method_ordering();
4192   the_class-&gt;set_method_ordering(scratch_class-&gt;method_ordering());
4193   scratch_class-&gt;set_method_ordering(old_ordering);
4194 
4195   ConstantPool* old_constants = the_class-&gt;constants();
4196   the_class-&gt;set_constants(scratch_class-&gt;constants());
4197   scratch_class-&gt;set_constants(old_constants);  // See the previous comment.
4198 #if 0
4199   // We are swapping the guts of &quot;the new class&quot; with the guts of &quot;the
4200   // class&quot;. Since the old constant pool has just been attached to &quot;the
4201   // new class&quot;, it seems logical to set the pool holder in the old
4202   // constant pool also. However, doing this will change the observable
4203   // class hierarchy for any old methods that are still executing. A
4204   // method can query the identity of its &quot;holder&quot; and this query uses
4205   // the method&#39;s constant pool link to find the holder. The change in
4206   // holding class from &quot;the class&quot; to &quot;the new class&quot; can confuse
4207   // things.
4208   //
4209   // Setting the old constant pool&#39;s holder will also cause
4210   // verification done during vtable initialization below to fail.
4211   // During vtable initialization, the vtable&#39;s class is verified to be
4212   // a subtype of the method&#39;s holder. The vtable&#39;s class is &quot;the
4213   // class&quot; and the method&#39;s holder is gotten from the constant pool
4214   // link in the method itself. For &quot;the class&quot;&#39;s directly implemented
4215   // methods, the method holder is &quot;the class&quot; itself (as gotten from
4216   // the new constant pool). The check works fine in this case. The
4217   // check also works fine for methods inherited from super classes.
4218   //
4219   // Miranda methods are a little more complicated. A miranda method is
4220   // provided by an interface when the class implementing the interface
4221   // does not provide its own method.  These interfaces are implemented
4222   // internally as an InstanceKlass. These special instanceKlasses
4223   // share the constant pool of the class that &quot;implements&quot; the
4224   // interface. By sharing the constant pool, the method holder of a
4225   // miranda method is the class that &quot;implements&quot; the interface. In a
4226   // non-redefine situation, the subtype check works fine. However, if
4227   // the old constant pool&#39;s pool holder is modified, then the check
4228   // fails because there is no class hierarchy relationship between the
4229   // vtable&#39;s class and &quot;the new class&quot;.
4230 
4231   old_constants-&gt;set_pool_holder(scratch_class());
4232 #endif
4233 
4234   // track number of methods that are EMCP for add_previous_version() call below
4235   int emcp_method_count = check_methods_and_mark_as_obsolete();
4236   transfer_old_native_function_registrations(the_class);
4237 
4238   // The class file bytes from before any retransformable agents mucked
4239   // with them was cached on the scratch class, move to the_class.
4240   // Note: we still want to do this if nothing needed caching since it
4241   // should get cleared in the_class too.
4242   if (the_class-&gt;get_cached_class_file() == 0) {
4243     // the_class doesn&#39;t have a cache yet so copy it
4244     the_class-&gt;set_cached_class_file(scratch_class-&gt;get_cached_class_file());
4245   }
4246   else if (scratch_class-&gt;get_cached_class_file() !=
4247            the_class-&gt;get_cached_class_file()) {
4248     // The same class can be present twice in the scratch classes list or there
4249     // are multiple concurrent RetransformClasses calls on different threads.
4250     // In such cases we have to deallocate scratch_class cached_class_file.
4251     os::free(scratch_class-&gt;get_cached_class_file());
4252   }
4253 
4254   // NULL out in scratch class to not delete twice.  The class to be redefined
4255   // always owns these bytes.
4256   scratch_class-&gt;set_cached_class_file(NULL);
4257 
4258   // Replace inner_classes
4259   Array&lt;u2&gt;* old_inner_classes = the_class-&gt;inner_classes();
4260   the_class-&gt;set_inner_classes(scratch_class-&gt;inner_classes());
4261   scratch_class-&gt;set_inner_classes(old_inner_classes);
4262 
4263   // Initialize the vtable and interface table after
4264   // methods have been rewritten
4265   // no exception should happen here since we explicitly
4266   // do not check loader constraints.
4267   // compare_and_normalize_class_versions has already checked:
4268   //  - classloaders unchanged, signatures unchanged
4269   //  - all instanceKlasses for redefined classes reused &amp; contents updated
4270   the_class-&gt;vtable().initialize_vtable(false, THREAD);
4271   the_class-&gt;itable().initialize_itable(false, THREAD);
4272   assert(!HAS_PENDING_EXCEPTION || (THREAD-&gt;pending_exception()-&gt;is_a(SystemDictionary::ThreadDeath_klass())), &quot;redefine exception&quot;);
4273 
4274   // Leave arrays of jmethodIDs and itable index cache unchanged
4275 
4276   // Copy the &quot;source file name&quot; attribute from new class version
4277   the_class-&gt;set_source_file_name_index(
4278     scratch_class-&gt;source_file_name_index());
4279 
4280   // Copy the &quot;source debug extension&quot; attribute from new class version
4281   the_class-&gt;set_source_debug_extension(
4282     scratch_class-&gt;source_debug_extension(),
4283     scratch_class-&gt;source_debug_extension() == NULL ? 0 :
4284     (int)strlen(scratch_class-&gt;source_debug_extension()));
4285 
4286   // Use of javac -g could be different in the old and the new
4287   if (scratch_class-&gt;access_flags().has_localvariable_table() !=
4288       the_class-&gt;access_flags().has_localvariable_table()) {
4289 
4290     AccessFlags flags = the_class-&gt;access_flags();
4291     if (scratch_class-&gt;access_flags().has_localvariable_table()) {
4292       flags.set_has_localvariable_table();
4293     } else {
4294       flags.clear_has_localvariable_table();
4295     }
4296     the_class-&gt;set_access_flags(flags);
4297   }
4298 
4299   swap_annotations(the_class, scratch_class);
4300 
4301   // Replace CP indexes for class and name+type of enclosing method
4302   u2 old_class_idx  = the_class-&gt;enclosing_method_class_index();
4303   u2 old_method_idx = the_class-&gt;enclosing_method_method_index();
4304   the_class-&gt;set_enclosing_method_indices(
4305     scratch_class-&gt;enclosing_method_class_index(),
4306     scratch_class-&gt;enclosing_method_method_index());
4307   scratch_class-&gt;set_enclosing_method_indices(old_class_idx, old_method_idx);
4308 
4309   // Replace fingerprint data
4310   the_class-&gt;set_has_passed_fingerprint_check(scratch_class-&gt;has_passed_fingerprint_check());
4311   the_class-&gt;store_fingerprint(scratch_class-&gt;get_stored_fingerprint());
4312 
4313   the_class-&gt;set_has_been_redefined();
4314 
4315   if (!the_class-&gt;should_be_initialized()) {
4316     // Class was already initialized, so AOT has only seen the original version.
4317     // We need to let AOT look at it again.
4318     AOTLoader::load_for_klass(the_class, THREAD);
4319   }
4320 
4321   // keep track of previous versions of this class
4322   the_class-&gt;add_previous_version(scratch_class, emcp_method_count);
4323 
4324   _timer_rsc_phase1.stop();
4325   if (log_is_enabled(Info, redefine, class, timer)) {
4326     _timer_rsc_phase2.start();
4327   }
4328 
4329   if (the_class-&gt;oop_map_cache() != NULL) {
4330     // Flush references to any obsolete methods from the oop map cache
4331     // so that obsolete methods are not pinned.
4332     the_class-&gt;oop_map_cache()-&gt;flush_obsolete_entries();
4333   }
4334 
4335   increment_class_counter((InstanceKlass *)the_class, THREAD);
4336 
4337   if (EventClassRedefinition::is_enabled()) {
4338     EventClassRedefinition event;
4339     event.set_classModificationCount(java_lang_Class::classRedefinedCount(the_class-&gt;java_mirror()));
4340     event.set_redefinedClass(the_class);
4341     event.set_redefinitionId(_id);
4342     event.commit();
4343   }
4344 
4345   {
4346     ResourceMark rm(THREAD);
4347     // increment the classRedefinedCount field in the_class and in any
4348     // direct and indirect subclasses of the_class
4349     log_info(redefine, class, load)
4350       (&quot;redefined name=%s, count=%d (avail_mem=&quot; UINT64_FORMAT &quot;K)&quot;,
4351        the_class-&gt;external_name(), java_lang_Class::classRedefinedCount(the_class-&gt;java_mirror()), os::available_memory() &gt;&gt; 10);
4352     Events::log_redefinition(THREAD, &quot;redefined class name=%s, count=%d&quot;,
4353                              the_class-&gt;external_name(),
4354                              java_lang_Class::classRedefinedCount(the_class-&gt;java_mirror()));
4355 
4356   }
4357   _timer_rsc_phase2.stop();
4358 
4359 } // end redefine_single_class()
4360 
4361 
4362 // Increment the classRedefinedCount field in the specific InstanceKlass
4363 // and in all direct and indirect subclasses.
4364 void VM_RedefineClasses::increment_class_counter(InstanceKlass *ik, TRAPS) {
4365   oop class_mirror = ik-&gt;java_mirror();
4366   Klass* class_oop = java_lang_Class::as_Klass(class_mirror);
4367   int new_count = java_lang_Class::classRedefinedCount(class_mirror) + 1;
4368   java_lang_Class::set_classRedefinedCount(class_mirror, new_count);
4369 
4370   if (class_oop != _the_class) {
4371     // _the_class count is printed at end of redefine_single_class()
4372     log_debug(redefine, class, subclass)(&quot;updated count in subclass=%s to %d&quot;, ik-&gt;external_name(), new_count);
4373   }
4374 
4375   for (Klass *subk = ik-&gt;subklass(); subk != NULL;
4376        subk = subk-&gt;next_sibling()) {
4377     if (subk-&gt;is_instance_klass()) {
4378       // Only update instanceKlasses
4379       InstanceKlass *subik = InstanceKlass::cast(subk);
4380       // recursively do subclasses of the current subclass
4381       increment_class_counter(subik, THREAD);
4382     }
4383   }
4384 }
4385 
4386 void VM_RedefineClasses::CheckClass::do_klass(Klass* k) {
4387   bool no_old_methods = true;  // be optimistic
4388 
4389   // Both array and instance classes have vtables.
4390   // a vtable should never contain old or obsolete methods
4391   ResourceMark rm(_thread);
4392   if (k-&gt;vtable_length() &gt; 0 &amp;&amp;
4393       !k-&gt;vtable().check_no_old_or_obsolete_entries()) {
4394     if (log_is_enabled(Trace, redefine, class, obsolete, metadata)) {
4395       log_trace(redefine, class, obsolete, metadata)
4396         (&quot;klassVtable::check_no_old_or_obsolete_entries failure -- OLD or OBSOLETE method found -- class: %s&quot;,
4397          k-&gt;signature_name());
4398       k-&gt;vtable().dump_vtable();
4399     }
4400     no_old_methods = false;
4401   }
4402 
4403   if (k-&gt;is_instance_klass()) {
4404     HandleMark hm(_thread);
4405     InstanceKlass *ik = InstanceKlass::cast(k);
4406 
4407     // an itable should never contain old or obsolete methods
4408     if (ik-&gt;itable_length() &gt; 0 &amp;&amp;
4409         !ik-&gt;itable().check_no_old_or_obsolete_entries()) {
4410       if (log_is_enabled(Trace, redefine, class, obsolete, metadata)) {
4411         log_trace(redefine, class, obsolete, metadata)
4412           (&quot;klassItable::check_no_old_or_obsolete_entries failure -- OLD or OBSOLETE method found -- class: %s&quot;,
4413            ik-&gt;signature_name());
4414         ik-&gt;itable().dump_itable();
4415       }
4416       no_old_methods = false;
4417     }
4418 
4419     // the constant pool cache should never contain non-deleted old or obsolete methods
4420     if (ik-&gt;constants() != NULL &amp;&amp;
4421         ik-&gt;constants()-&gt;cache() != NULL &amp;&amp;
4422         !ik-&gt;constants()-&gt;cache()-&gt;check_no_old_or_obsolete_entries()) {
4423       if (log_is_enabled(Trace, redefine, class, obsolete, metadata)) {
4424         log_trace(redefine, class, obsolete, metadata)
4425           (&quot;cp-cache::check_no_old_or_obsolete_entries failure -- OLD or OBSOLETE method found -- class: %s&quot;,
4426            ik-&gt;signature_name());
4427         ik-&gt;constants()-&gt;cache()-&gt;dump_cache();
4428       }
4429       no_old_methods = false;
4430     }
4431   }
4432 
4433   // print and fail guarantee if old methods are found.
4434   if (!no_old_methods) {
4435     if (log_is_enabled(Trace, redefine, class, obsolete, metadata)) {
4436       dump_methods();
4437     } else {
4438       log_trace(redefine, class)(&quot;Use the &#39;-Xlog:redefine+class*:&#39; option &quot;
4439         &quot;to see more info about the following guarantee() failure.&quot;);
4440     }
4441     guarantee(false, &quot;OLD and/or OBSOLETE method(s) found&quot;);
4442   }
4443 }
4444 
4445 u8 VM_RedefineClasses::next_id() {
4446   while (true) {
4447     u8 id = _id_counter;
4448     u8 next_id = id + 1;
4449     u8 result = Atomic::cmpxchg(&amp;_id_counter, id, next_id);
4450     if (result == id) {
4451       return next_id;
4452     }
4453   }
4454 }
4455 
4456 void VM_RedefineClasses::dump_methods() {
4457   int j;
4458   log_trace(redefine, class, dump)(&quot;_old_methods --&quot;);
4459   for (j = 0; j &lt; _old_methods-&gt;length(); ++j) {
4460     LogStreamHandle(Trace, redefine, class, dump) log_stream;
4461     Method* m = _old_methods-&gt;at(j);
4462     log_stream.print(&quot;%4d  (%5d)  &quot;, j, m-&gt;vtable_index());
4463     m-&gt;access_flags().print_on(&amp;log_stream);
4464     log_stream.print(&quot; --  &quot;);
4465     m-&gt;print_name(&amp;log_stream);
4466     log_stream.cr();
4467   }
4468   log_trace(redefine, class, dump)(&quot;_new_methods --&quot;);
4469   for (j = 0; j &lt; _new_methods-&gt;length(); ++j) {
4470     LogStreamHandle(Trace, redefine, class, dump) log_stream;
4471     Method* m = _new_methods-&gt;at(j);
4472     log_stream.print(&quot;%4d  (%5d)  &quot;, j, m-&gt;vtable_index());
4473     m-&gt;access_flags().print_on(&amp;log_stream);
4474     log_stream.print(&quot; --  &quot;);
4475     m-&gt;print_name(&amp;log_stream);
4476     log_stream.cr();
4477   }
4478   log_trace(redefine, class, dump)(&quot;_matching_methods --&quot;);
4479   for (j = 0; j &lt; _matching_methods_length; ++j) {
4480     LogStreamHandle(Trace, redefine, class, dump) log_stream;
4481     Method* m = _matching_old_methods[j];
4482     log_stream.print(&quot;%4d  (%5d)  &quot;, j, m-&gt;vtable_index());
4483     m-&gt;access_flags().print_on(&amp;log_stream);
4484     log_stream.print(&quot; --  &quot;);
4485     m-&gt;print_name();
4486     log_stream.cr();
4487 
4488     m = _matching_new_methods[j];
4489     log_stream.print(&quot;      (%5d)  &quot;, m-&gt;vtable_index());
4490     m-&gt;access_flags().print_on(&amp;log_stream);
4491     log_stream.cr();
4492   }
4493   log_trace(redefine, class, dump)(&quot;_deleted_methods --&quot;);
4494   for (j = 0; j &lt; _deleted_methods_length; ++j) {
4495     LogStreamHandle(Trace, redefine, class, dump) log_stream;
4496     Method* m = _deleted_methods[j];
4497     log_stream.print(&quot;%4d  (%5d)  &quot;, j, m-&gt;vtable_index());
4498     m-&gt;access_flags().print_on(&amp;log_stream);
4499     log_stream.print(&quot; --  &quot;);
4500     m-&gt;print_name(&amp;log_stream);
4501     log_stream.cr();
4502   }
4503   log_trace(redefine, class, dump)(&quot;_added_methods --&quot;);
4504   for (j = 0; j &lt; _added_methods_length; ++j) {
4505     LogStreamHandle(Trace, redefine, class, dump) log_stream;
4506     Method* m = _added_methods[j];
4507     log_stream.print(&quot;%4d  (%5d)  &quot;, j, m-&gt;vtable_index());
4508     m-&gt;access_flags().print_on(&amp;log_stream);
4509     log_stream.print(&quot; --  &quot;);
4510     m-&gt;print_name(&amp;log_stream);
4511     log_stream.cr();
4512   }
4513 }
4514 
4515 void VM_RedefineClasses::print_on_error(outputStream* st) const {
4516   VM_Operation::print_on_error(st);
4517   if (_the_class != NULL) {
4518     ResourceMark rm;
4519     st-&gt;print_cr(&quot;, redefining class %s&quot;, _the_class-&gt;external_name());
4520   }
4521 }
    </pre>
  </body>
</html>