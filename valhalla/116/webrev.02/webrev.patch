diff a/src/hotspot/cpu/aarch64/c1_LIRAssembler_aarch64.cpp b/src/hotspot/cpu/aarch64/c1_LIRAssembler_aarch64.cpp
--- a/src/hotspot/cpu/aarch64/c1_LIRAssembler_aarch64.cpp
+++ b/src/hotspot/cpu/aarch64/c1_LIRAssembler_aarch64.cpp
@@ -496,24 +496,23 @@
 void LIR_Assembler::return_op(LIR_Opr result) {
   assert(result->is_illegal() || !result->is_single_cpu() || result->as_register() == r0, "word returns are in r0,");
 
   ciMethod* method = compilation()->method();
 
-  if (InlineTypeReturnedAsFields && method->signature()->returns_never_null()) {
-    ciType* return_type = method->return_type();
-    if (return_type->is_valuetype()) {
-      ciValueKlass* vk = return_type->as_value_klass();
-      if (vk->can_be_returned_as_fields()) {
-        address unpack_handler = vk->unpack_handler();
-        assert(unpack_handler != NULL, "must be");
-        __ far_call(RuntimeAddress(unpack_handler));
-        // At this point, rax points to the value object (for interpreter or C1 caller).
-        // The fields of the object are copied into registers (for C2 caller).
-      }
+  ciType* return_type = method->return_type();
+  if (InlineTypeReturnedAsFields && return_type->is_valuetype()) {
+    ciValueKlass* vk = return_type->as_value_klass();
+    if (vk->can_be_returned_as_fields()) {
+      address unpack_handler = vk->unpack_handler();
+      assert(unpack_handler != NULL, "must be");
+      __ far_call(RuntimeAddress(unpack_handler));
+      // At this point, rax points to the value object (for interpreter or C1 caller).
+      // The fields of the object are copied into registers (for C2 caller).
     }
   }
 
+
   // Pop the stack before the safepoint code
   __ remove_frame(initial_frame_size_in_bytes(), needs_stack_repair());
 
   if (StackReservedPages > 0 && compilation()->has_reserved_stack_access()) {
     __ reserved_stack_check();
@@ -1584,11 +1583,11 @@
 
 }
 
 void LIR_Assembler::emit_opNullFreeArrayCheck(LIR_OpNullFreeArrayCheck* op) {
   // This is called when we use aastore into a an array declared as "[LVT;",
-  // where we know VT is not flattenable (due to InlineArrayElemMaxFlatSize, etc).
+  // where we know VT is not flattened (due to InlineArrayElemMaxFlatSize, etc).
   // However, we need to do a NULL check if the actual array is a "[QVT;".
 
   __ load_storage_props(op->tmp()->as_register(), op->array()->as_register());
   __ mov(rscratch1, (uint64_t) ArrayStorageProperties::null_free_value);
   __ cmp(op->tmp()->as_register(), rscratch1);
diff a/src/hotspot/cpu/x86/c1_LIRAssembler_x86.cpp b/src/hotspot/cpu/x86/c1_LIRAssembler_x86.cpp
--- a/src/hotspot/cpu/x86/c1_LIRAssembler_x86.cpp
+++ b/src/hotspot/cpu/x86/c1_LIRAssembler_x86.cpp
@@ -526,25 +526,23 @@
   if (!result->is_illegal() && result->is_float_kind() && !result->is_xmm_register()) {
     assert(result->fpu() == 0, "result must already be on TOS");
   }
 
   ciMethod* method = compilation()->method();
-  if (InlineTypeReturnedAsFields && method->signature()->returns_never_null()) {
-    ciType* return_type = method->return_type();
-    if (return_type->is_valuetype()) {
-      ciValueKlass* vk = return_type->as_value_klass();
-      if (vk->can_be_returned_as_fields()) {
+  ciType* return_type = method->return_type();
+  if (InlineTypeReturnedAsFields && return_type->is_valuetype()) {
+    ciValueKlass* vk = return_type->as_value_klass();
+    if (vk->can_be_returned_as_fields()) {
 #ifndef _LP64
-        Unimplemented();
+      Unimplemented();
 #else
-        address unpack_handler = vk->unpack_handler();
-        assert(unpack_handler != NULL, "must be");
-        __ call(RuntimeAddress(unpack_handler));
-        // At this point, rax points to the value object (for interpreter or C1 caller).
-        // The fields of the object are copied into registers (for C2 caller).
+      address unpack_handler = vk->unpack_handler();
+      assert(unpack_handler != NULL, "must be");
+      __ call(RuntimeAddress(unpack_handler));
+      // At this point, rax points to the value object (for interpreter or C1 caller).
+      // The fields of the object are copied into registers (for C2 caller).
 #endif
-      }
     }
   }
 
   // Pop the stack before the safepoint code
   int initial_framesize = initial_frame_size_in_bytes();
diff a/src/hotspot/share/c1/c1_Canonicalizer.cpp b/src/hotspot/share/c1/c1_Canonicalizer.cpp
--- a/src/hotspot/share/c1/c1_Canonicalizer.cpp
+++ b/src/hotspot/share/c1/c1_Canonicalizer.cpp
@@ -645,11 +645,11 @@
 void Canonicalizer::do_NewObjectArray (NewObjectArray*  x) {}
 void Canonicalizer::do_NewMultiArray  (NewMultiArray*   x) {}
 void Canonicalizer::do_WithField      (WithField*       x) {}
 void Canonicalizer::do_DefaultValue   (DefaultValue*    x) {}
 void Canonicalizer::do_CheckCast      (CheckCast*       x) {
-  if (x->klass()->is_loaded() && !x->is_never_null()) {
+  if (x->klass()->is_loaded() && !x->klass()->is_valuetype()) {
     // Don't canonicalize for non-nullable types -- we need to throw NPE.
     Value obj = x->obj();
     ciType* klass = obj->exact_type();
     if (klass == NULL) {
       klass = obj->declared_type();
diff a/src/hotspot/share/c1/c1_GraphBuilder.cpp b/src/hotspot/share/c1/c1_GraphBuilder.cpp
--- a/src/hotspot/share/c1/c1_GraphBuilder.cpp
+++ b/src/hotspot/share/c1/c1_GraphBuilder.cpp
@@ -1777,24 +1777,21 @@
         if (state_before == NULL) {
           state_before = copy_state_for_exception();
         }
         LoadField* load_field = new LoadField(append(obj), offset, field, true,
                                         state_before, needs_patching);
-        if (field->is_flattenable()) {
-          load_field->set_never_null(true);
-        }
         push(type, append(load_field));
       }
       break;
     }
     case Bytecodes::_putstatic: {
       Value val = pop(type);
       val->set_escaped();
       if (state_before == NULL) {
         state_before = copy_state_for_exception();
       }
-      if (field->type()->basic_type() == T_BOOLEAN) {
+      if (field_type == T_BOOLEAN) {
         Value mask = append(new Constant(new IntConstant(1)));
         val = append(new LogicOp(Bytecodes::_iand, val, mask));
       }
       append(new StoreField(append(obj), offset, field, val, true, state_before, needs_patching));
       break;
@@ -1812,12 +1809,12 @@
       if (field->is_constant() && !field->is_flattened() && obj_type->is_constant() && !PatchALot) {
         ciObject* const_oop = obj_type->constant_value();
         if (!const_oop->is_null_object() && const_oop->is_loaded()) {
           ciConstant field_value = field->constant_value_of(const_oop);
           if (field_value.is_valid()) {
-            if (field->is_flattenable() && field_value.is_null_or_zero()) {
-              // Non-flattened but flattenable inline type field. Replace null by the default value.
+            if (field->signature()->is_Q_signature() && field_value.is_null_or_zero()) {
+              // Non-flattened inline type field. Replace null by the default value.
               constant = new Constant(new InstanceConstant(field->type()->as_value_klass()->default_value_instance()));
             } else {
               constant = make_constant(field_value, field);
             }
             // For CallSite objects add a dependency for invalidation of the optimization.
@@ -2370,11 +2367,11 @@
   for (int i=0; i<args->length(); i++) {
     args->at(0)->set_escaped();
   }
 
   Invoke* result = new Invoke(code, result_type, recv, args, vtable_index, target, state_before,
-                              declared_signature->returns_never_null());
+                              declared_signature->return_type()->is_valuetype());
   // push result
   append_split(result);
 
   if (result_type != voidType) {
     if (method()->is_strict()) {
@@ -2418,11 +2415,11 @@
 
 
 void GraphBuilder::new_object_array() {
   bool will_link;
   ciKlass* klass = stream()->get_klass(will_link);
-  bool never_null = stream()->is_klass_never_null();
+  bool never_null = stream()->is_inline_klass();
   ValueStack* state_before = !klass->is_loaded() || PatchALot ? copy_state_before() : copy_state_exhandling();
   NewArray* n = new NewObjectArray(klass, ipop(), state_before, never_null);
   apush(append_split(n));
 }
 
@@ -2445,11 +2442,11 @@
 
 
 void GraphBuilder::check_cast(int klass_index) {
   bool will_link;
   ciKlass* klass = stream()->get_klass(will_link);
-  bool never_null = stream()->is_klass_never_null();
+  bool never_null = stream()->is_inline_klass();
   ValueStack* state_before = !klass->is_loaded() || PatchALot ? copy_state_before() : copy_state_for_exception();
   CheckCast* c = new CheckCast(klass, apop(), state_before, never_null);
   apush(append_split(c));
   c->set_direct_compare(direct_compare(klass));
 
@@ -3469,11 +3466,11 @@
     ciType* type = sig->type_at(i);
     BasicType basic_type = type->basic_type();
     // don't allow T_ARRAY to propagate into locals types
     if (is_reference_type(basic_type)) basic_type = T_OBJECT;
     ValueType* vt = as_ValueType(basic_type);
-    state->store_local(idx, new Local(type, vt, idx, false, sig->is_never_null_at(i)));
+    state->store_local(idx, new Local(type, vt, idx, false, type->is_valuetype()));
     idx += type->size();
   }
 
   // lock synchronized method
   if (method()->is_synchronized()) {
diff a/src/hotspot/share/c1/c1_Instruction.hpp b/src/hotspot/share/c1/c1_Instruction.hpp
--- a/src/hotspot/share/c1/c1_Instruction.hpp
+++ b/src/hotspot/share/c1/c1_Instruction.hpp
@@ -869,11 +869,13 @@
   // creation
   LoadField(Value obj, int offset, ciField* field, bool is_static,
             ValueStack* state_before, bool needs_patching,
             ciValueKlass* value_klass = NULL, Value default_value = NULL )
   : AccessField(obj, offset, field, is_static, state_before, needs_patching)
-  {}
+  {
+    set_never_null(field->signature()->is_Q_signature());
+  }
 
   ciType* declared_type() const;
 
   // generic; cannot be eliminated if needs patching or if volatile.
   HASHING3(LoadField, !needs_patching() && !field()->is_volatile(), obj()->subst(), offset(), declared_type())
diff a/src/hotspot/share/c1/c1_LIR.cpp b/src/hotspot/share/c1/c1_LIR.cpp
--- a/src/hotspot/share/c1/c1_LIR.cpp
+++ b/src/hotspot/share/c1/c1_LIR.cpp
@@ -1038,11 +1038,11 @@
   masm->emit_call(this);
 }
 
 bool LIR_OpJavaCall::maybe_return_as_fields(ciValueKlass** vk_ret) const {
   if (InlineTypeReturnedAsFields) {
-    if (method()->signature()->maybe_returns_never_null()) {
+    if (method()->signature()->maybe_returns_value_type()) {
       ciType* return_type = method()->return_type();
       if (return_type->is_valuetype()) {
         ciValueKlass* vk = return_type->as_value_klass();
         if (vk->can_be_returned_as_fields()) {
           if (vk_ret != NULL) {
diff a/src/hotspot/share/c1/c1_LIRGenerator.cpp b/src/hotspot/share/c1/c1_LIRGenerator.cpp
--- a/src/hotspot/share/c1/c1_LIRGenerator.cpp
+++ b/src/hotspot/share/c1/c1_LIRGenerator.cpp
@@ -1552,11 +1552,11 @@
 #endif
 
   if (x->needs_null_check() &&
       (needs_patching ||
        MacroAssembler::needs_explicit_null_check(x->offset()))) {
-    if (needs_patching && x->field()->is_flattenable()) {
+    if (needs_patching && x->field()->signature()->is_Q_signature()) {
       // We are storing a field of type "QT;" into holder class H, but H is not yet
       // loaded. (If H had been loaded, then T must also have already been loaded
       // due to the "Q" signature, and needs_patching would be false).
       assert(!x->field()->holder()->is_loaded(), "must be");
       // We don't know the offset of this field. Let's deopt and recompile.
@@ -1691,11 +1691,11 @@
   if (x->elt_type() == T_OBJECT && x->array()->maybe_flattened_array()) {
     ciType* type = x->value()->declared_type();
     if (type != NULL && type->is_klass()) {
       ciKlass* klass = type->as_klass();
       if (!klass->can_be_value_klass() || (klass->is_valuetype() && !klass->as_value_klass()->flatten_array())) {
-        // This is known to be a non-flattenable object. If the array is flattened,
+        // This is known to be a non-flattened object. If the array is flattened,
         // it will be caught by the code generated by array_store_check().
         return false;
       }
     }
     // We're not 100% sure, so let's do the flattened_array_store_check.
@@ -1898,11 +1898,11 @@
   }
 
   return _barrier_set->resolve(this, decorators, obj);
 }
 
-Constant* LIRGenerator::flattenable_load_field_prolog(LoadField* x, CodeEmitInfo* info) {
+Constant* LIRGenerator::flattened_field_load_prolog(LoadField* x, CodeEmitInfo* info) {
   ciField* field = x->field();
   ciInstanceKlass* holder = field->holder();
   Constant* default_value = NULL;
 
   // Unloaded "QV;" klasses are represented by a ciInstanceKlass
@@ -2002,12 +2002,12 @@
                   x->is_static() ?  "static" : "field", x->printable_bci());
   }
 #endif
 
   Constant* default_value = NULL;
-  if (x->field()->is_flattenable()) {
-    default_value = flattenable_load_field_prolog(x, info);
+  if (x->field()->signature()->is_Q_signature()) {
+    default_value = flattened_field_load_prolog(x, info);
   }
 
   bool stress_deopt = StressLoopInvariantCodeMotion && info && info->deoptimize_on_exception();
   if (x->needs_null_check() &&
       (needs_patching ||
diff a/src/hotspot/share/c1/c1_LIRGenerator.hpp b/src/hotspot/share/c1/c1_LIRGenerator.hpp
--- a/src/hotspot/share/c1/c1_LIRGenerator.hpp
+++ b/src/hotspot/share/c1/c1_LIRGenerator.hpp
@@ -266,11 +266,11 @@
   void do_Reference_get(Intrinsic* x);
   void do_update_CRC32(Intrinsic* x);
   void do_update_CRC32C(Intrinsic* x);
   void do_vectorizedMismatch(Intrinsic* x);
 
-  Constant* flattenable_load_field_prolog(LoadField* x, CodeEmitInfo* info);
+  Constant* flattened_field_load_prolog(LoadField* x, CodeEmitInfo* info);
   void access_flattened_array(bool is_load, LIRItem& array, LIRItem& index, LIRItem& obj_item);
   bool needs_flattened_array_store_check(StoreIndexed* x);
   void check_flattened_array(LIR_Opr array, LIR_Opr value, CodeStub* slow_path);
   bool needs_null_free_array_store_check(StoreIndexed* x);
   void check_null_free_array(LIRItem& array, LIRItem& value,  CodeEmitInfo* info);
diff a/src/hotspot/share/ci/ciClassList.hpp b/src/hotspot/share/ci/ciClassList.hpp
--- a/src/hotspot/share/ci/ciClassList.hpp
+++ b/src/hotspot/share/ci/ciClassList.hpp
@@ -65,11 +65,10 @@
 class       ciValueKlass;
 class     ciArrayKlass;
 class       ciValueArrayKlass;
 class       ciObjArrayKlass;
 class       ciTypeArrayKlass;
-class    ciWrapper;
 
 // Simulate Java Language style package-private access with
 // friend declarations.
 // This is a great idea but gcc and other C++ compilers give an
 // error for being friends with yourself, so this macro does not
@@ -113,11 +112,10 @@
 friend class ciMetadata;               \
 friend class ciReplay;                 \
 friend class ciTypeArray;              \
 friend class ciType;                   \
 friend class ciReturnAddress;          \
-friend class ciWrapper;                \
 friend class ciKlass;                  \
 friend class ciInstanceKlass;          \
 friend class ciValueKlass;             \
 friend class ciArrayKlass;             \
 friend class ciValueArrayKlass;        \
diff a/src/hotspot/share/ci/ciEnv.cpp b/src/hotspot/share/ci/ciEnv.cpp
--- a/src/hotspot/share/ci/ciEnv.cpp
+++ b/src/hotspot/share/ci/ciEnv.cpp
@@ -606,14 +606,14 @@
                                    ciInstanceKlass* accessor) {
   GUARDED_VM_ENTRY(return get_klass_by_index_impl(cpool, index, is_accessible, accessor);)
 }
 
 // ------------------------------------------------------------------
-// ciEnv::is_klass_never_null
+// ciEnv::is_inline_klass
 //
-// Get information about nullability from the constant pool.
-bool ciEnv::is_klass_never_null(const constantPoolHandle& cpool, int index) {
+// Check if the klass is an inline klass.
+bool ciEnv::is_inline_klass(const constantPoolHandle& cpool, int index) {
   GUARDED_VM_ENTRY(return cpool->klass_name_at(index)->is_Q_signature();)
 }
 
 // ------------------------------------------------------------------
 // ciEnv::get_constant_by_index_impl
diff a/src/hotspot/share/ci/ciEnv.hpp b/src/hotspot/share/ci/ciEnv.hpp
--- a/src/hotspot/share/ci/ciEnv.hpp
+++ b/src/hotspot/share/ci/ciEnv.hpp
@@ -132,12 +132,12 @@
   ciField*   get_field_by_index(ciInstanceKlass* loading_klass,
                                 int field_index);
   ciMethod*  get_method_by_index(const constantPoolHandle& cpool,
                                  int method_index, Bytecodes::Code bc,
                                  ciInstanceKlass* loading_klass);
-  bool       is_klass_never_null(const constantPoolHandle& cpool,
-                                 int klass_index);
+  bool       is_inline_klass(const constantPoolHandle& cpool,
+                             int klass_index);
 
   // Implementation methods for loading and constant pool access.
   ciKlass* get_klass_by_name_impl(ciKlass* accessing_klass,
                                   const constantPoolHandle& cpool,
                                   ciSymbol* klass_name,
@@ -473,12 +473,8 @@
   void dump_replay_data(int compile_id);
   void dump_inline_data(int compile_id);
   void dump_replay_data(outputStream* out);
   void dump_replay_data_unsafe(outputStream* out);
   void dump_compile_data(outputStream* out);
-
-  ciWrapper* make_never_null_wrapper(ciType* type) {
-    return _factory->make_never_null_wrapper(type);
-  }
 };
 
 #endif // SHARE_CI_CIENV_HPP
diff a/src/hotspot/share/ci/ciField.cpp b/src/hotspot/share/ci/ciField.cpp
--- a/src/hotspot/share/ci/ciField.cpp
+++ b/src/hotspot/share/ci/ciField.cpp
@@ -99,13 +99,10 @@
     _type = ciType::make(field_type);
   }
 
   _name = (ciSymbol*)ciEnv::current(THREAD)->get_symbol(name);
 
-  // this is needed if the field class is not yet loaded.
-  _is_flattenable = _signature->is_Q_signature();
-
   // Get the field's declared holder.
   //
   // Note: we actually create a ciInstanceKlass for this klass,
   // even though we may not need to.
   int holder_index = cpool->klass_ref_index_at(index);
@@ -234,11 +231,10 @@
   _known_to_link_with_put = field->_known_to_link_with_put;
   _known_to_link_with_get = field->_known_to_link_with_get;
   _constant_value = field->_constant_value;
   assert(!field->is_flattened(), "field must not be flattened");
   _is_flattened = false;
-  _is_flattenable = field->is_flattenable();
 }
 
 static bool trust_final_non_static_fields(ciInstanceKlass* holder) {
   if (holder == NULL)
     return false;
@@ -281,11 +277,10 @@
   _offset = fd->offset();
   Klass* field_holder = fd->field_holder();
   assert(field_holder != NULL, "null field_holder");
   _holder = CURRENT_ENV->get_instance_klass(field_holder);
   _is_flattened = fd->is_inlined();
-  _is_flattenable = fd->is_inline_type();
 
   // Check to see if the field is constant.
   Klass* k = _holder->get_Klass();
   bool is_stable_field = FoldStableValues && is_stable();
   if ((is_final() && !has_initialized_final_update()) || is_stable_field) {
@@ -460,11 +455,10 @@
   tty->print(" is_constant=%s", bool_to_str(_is_constant));
   if (_is_constant && is_static()) {
     tty->print(" constant_value=");
     _constant_value.print();
   }
-  tty->print(" is_flattenable=%s", bool_to_str(_is_flattenable));
   tty->print(" is_flattened=%s", bool_to_str(_is_flattened));
   tty->print(">");
 }
 
 // ------------------------------------------------------------------
diff a/src/hotspot/share/ci/ciField.hpp b/src/hotspot/share/ci/ciField.hpp
--- a/src/hotspot/share/ci/ciField.hpp
+++ b/src/hotspot/share/ci/ciField.hpp
@@ -48,11 +48,10 @@
   ciSymbol*        _signature;
   ciType*          _type;
   int              _offset;
   bool             _is_constant;
   bool             _is_flattened;
-  bool             _is_flattenable;
   ciMethod*        _known_to_link_with_put;
   ciInstanceKlass* _known_to_link_with_get;
   ciConstant       _constant_value;
 
   ciType* compute_type();
@@ -176,11 +175,10 @@
   bool is_final                () const { return flags().is_final(); }
   bool is_stable               () const { return flags().is_stable(); }
   bool is_volatile             () const { return flags().is_volatile(); }
   bool is_transient            () const { return flags().is_transient(); }
   bool is_flattened            () const { return _is_flattened; }
-  bool is_flattenable          () const { return _is_flattenable; }
 
   // The field is modified outside of instance initializer methods
   // (or class/initializer methods if the field is static).
   bool has_initialized_final_update() const { return flags().has_initialized_final_update(); }
 
diff a/src/hotspot/share/ci/ciMetadata.hpp b/src/hotspot/share/ci/ciMetadata.hpp
--- a/src/hotspot/share/ci/ciMetadata.hpp
+++ b/src/hotspot/share/ci/ciMetadata.hpp
@@ -59,11 +59,10 @@
   virtual bool is_valuetype() const         { return false; }
   virtual bool is_array_klass() const       { return false; }
   virtual bool is_value_array_klass() const { return false; }
   virtual bool is_obj_array_klass() const   { return false; }
   virtual bool is_type_array_klass() const  { return false; }
-  virtual bool is_wrapper() const           { return false; }
   virtual bool flatten_array() const        { return false; }
   virtual void dump_replay_data(outputStream* st) { /* do nothing */ }
 
   ciMethod*                as_method() {
     assert(is_method(), "bad cast");
@@ -111,14 +110,10 @@
   }
   ciValueKlass*            as_value_klass() {
     assert(is_valuetype(), "bad cast");
     return (ciValueKlass*)this;
   }
-  ciWrapper*               as_wrapper() {
-    assert(is_wrapper(), "bad cast");
-    return (ciWrapper*)this;
-  }
 
   Metadata* constant_encoding() { return _metadata; }
 
   bool equals(ciMetadata* obj) const { return (this == obj); }
 
diff a/src/hotspot/share/ci/ciMethodType.cpp b/src/hotspot/share/ci/ciMethodType.cpp
--- a/src/hotspot/share/ci/ciMethodType.cpp
+++ b/src/hotspot/share/ci/ciMethodType.cpp
@@ -36,14 +36,13 @@
     Klass* k = java_lang_Class::as_Klass(klass_oop);
     return CURRENT_ENV->get_klass(k);
   }
 }
 
-ciType* ciMethodType::rtype(bool& never_null) const {
+ciType* ciMethodType::rtype() const {
   GUARDED_VM_ENTRY(
     oop rtype = java_lang_invoke_MethodType::rtype(get_oop());
-    never_null = (java_lang_Class::as_Klass(rtype)->is_inline_klass());
     return class_to_citype(rtype);
   )
 }
 
 int ciMethodType::ptype_count() const {
@@ -52,12 +51,11 @@
 
 int ciMethodType::ptype_slot_count() const {
   GUARDED_VM_ENTRY(return java_lang_invoke_MethodType::ptype_slot_count(get_oop());)
 }
 
-ciType* ciMethodType::ptype_at(int index, bool& never_null) const {
+ciType* ciMethodType::ptype_at(int index) const {
   GUARDED_VM_ENTRY(
     oop ptype = java_lang_invoke_MethodType::ptype(get_oop(), index);
-    never_null = (java_lang_Class::as_Klass(ptype)->is_inline_klass());
     return class_to_citype(ptype);
   )
 }
diff a/src/hotspot/share/ci/ciMethodType.hpp b/src/hotspot/share/ci/ciMethodType.hpp
--- a/src/hotspot/share/ci/ciMethodType.hpp
+++ b/src/hotspot/share/ci/ciMethodType.hpp
@@ -38,14 +38,14 @@
   ciMethodType(instanceHandle h_i) : ciInstance(h_i) {}
 
   // What kind of ciObject is this?
   bool is_method_type() const { return true; }
 
-  ciType* rtype(bool& never_null) const;
+  ciType* rtype() const;
 
   int ptype_count() const;
   int ptype_slot_count() const ;
 
-  ciType* ptype_at(int index, bool& never_null) const;
+  ciType* ptype_at(int index) const;
 };
 
 #endif // SHARE_CI_CIMETHODTYPE_HPP
diff a/src/hotspot/share/ci/ciObjectFactory.cpp b/src/hotspot/share/ci/ciObjectFactory.cpp
--- a/src/hotspot/share/ci/ciObjectFactory.cpp
+++ b/src/hotspot/share/ci/ciObjectFactory.cpp
@@ -632,16 +632,10 @@
   init_ident_of(new_ret_addr);
   _return_addresses->append(new_ret_addr);
   return new_ret_addr;
 }
 
-ciWrapper* ciObjectFactory::make_never_null_wrapper(ciType* type) {
-  ciWrapper* wrapper = new (arena()) ciWrapper(type, /* never_null */ true);
-  init_ident_of(wrapper);
-  return wrapper;
-}
-
 // ------------------------------------------------------------------
 // ciObjectFactory::init_ident_of
 void ciObjectFactory::init_ident_of(ciBaseObject* obj) {
   obj->set_ident(_next_ident++);
 }
diff a/src/hotspot/share/ci/ciObjectFactory.hpp b/src/hotspot/share/ci/ciObjectFactory.hpp
--- a/src/hotspot/share/ci/ciObjectFactory.hpp
+++ b/src/hotspot/share/ci/ciObjectFactory.hpp
@@ -142,10 +142,8 @@
   // RedefineClasses support
   void metadata_do(MetadataClosure* f);
 
   void print_contents();
   void print();
-
-  ciWrapper* make_never_null_wrapper(ciType* type);
 };
 
 #endif // SHARE_CI_CIOBJECTFACTORY_HPP
diff a/src/hotspot/share/ci/ciSignature.cpp b/src/hotspot/share/ci/ciSignature.cpp
--- a/src/hotspot/share/ci/ciSignature.cpp
+++ b/src/hotspot/share/ci/ciSignature.cpp
@@ -60,13 +60,10 @@
       type = ciType::make(ss.type());
     } else {
       ciSymbol* klass_name = env->get_symbol(ss.as_symbol());
       type = env->get_klass_by_name_impl(_accessing_klass, cpool, klass_name, false);
     }
-    if (type->is_valuetype() && ss.type() == T_INLINE_TYPE) {
-      type = env->make_never_null_wrapper(type);
-    }
     _types->append(type);
     if (ss.at_return_type()) {
       // Done processing the return type; do not add it into the count.
       break;
     }
@@ -88,78 +85,51 @@
   ASSERT_IN_VM;
   EXCEPTION_CONTEXT;
   ciEnv* env =  CURRENT_ENV;
   Arena* arena = env->arena();
   _types = new (arena) GrowableArray<ciType*>(arena, _count + 1, 0, NULL);
-  ciType* type = NULL;
-  bool never_null = false;
   for (int i = 0; i < _count; i++) {
-    type = method_type->ptype_at(i, never_null);
-    if (type->is_valuetype() && never_null) {
-      type = env->make_never_null_wrapper(type);
-    }
-    _types->append(type);
-  }
-  type = method_type->rtype(never_null);
-  if (type->is_valuetype() && never_null) {
-    type = env->make_never_null_wrapper(type);
+    _types->append(method_type->ptype_at(i));
   }
-  _types->append(type);
+  _types->append(method_type->rtype());
 }
 
 // ------------------------------------------------------------------
 // ciSignature::return_type
 //
 // What is the return type of this signature?
 ciType* ciSignature::return_type() const {
-  return _types->at(_count)->unwrap();
+  return _types->at(_count);
 }
 
 // ------------------------------------------------------------------
 // ciSignature::type_at
 //
 // What is the type of the index'th element of this
 // signature?
 ciType* ciSignature::type_at(int index) const {
   assert(index < _count, "out of bounds");
   // The first _klasses element holds the return klass.
-  return _types->at(index)->unwrap();
+  return _types->at(index);
 }
 
 // ------------------------------------------------------------------
-// ciSignature::return_never_null
-//
-// True if we statically know that the return value is never null.
-bool ciSignature::returns_never_null() const {
-  return _types->at(_count)->is_never_null();
-}
-
-// ------------------------------------------------------------------
-// ciSignature::maybe_return_never_null
+// ciSignature::maybe_returns_value_type
 //
 // True if we statically know that the return value is never null, or
 // if the return type has a Q signature but is not yet loaded, in which case
 // it could be a never-null type.
-bool ciSignature::maybe_returns_never_null() const {
+bool ciSignature::maybe_returns_value_type() const {
   ciType* ret_type = _types->at(_count);
-  if (ret_type->is_never_null()) {
+  if (ret_type->is_valuetype()) {
     return true;
   } else if (ret_type->is_instance_klass() && !ret_type->as_instance_klass()->is_loaded()) {
     GUARDED_VM_ENTRY(if (get_symbol()->is_Q_method_signature()) { return true; })
   }
   return false;
 }
 
-// ------------------------------------------------------------------
-// ciSignature::never_null_at
-//
-// True if we statically know that the argument at 'index' is never null.
-bool ciSignature::is_never_null_at(int index) const {
-  assert(index < _count, "out of bounds");
-  return _types->at(index)->is_never_null();
-}
-
 // ------------------------------------------------------------------
 // ciSignature::equals
 //
 // Compare this signature to another one.  Signatures with different
 // accessing classes but with signature-types resolved to the same
diff a/src/hotspot/share/ci/ciSignature.hpp b/src/hotspot/share/ci/ciSignature.hpp
--- a/src/hotspot/share/ci/ciSignature.hpp
+++ b/src/hotspot/share/ci/ciSignature.hpp
@@ -58,13 +58,11 @@
   ciSymbol* as_symbol() const                    { return _symbol; }
   ciKlass*  accessing_klass() const              { return _accessing_klass; }
 
   ciType*   return_type() const;
   ciType*   type_at(int index) const;
-  bool      returns_never_null() const;
-  bool      maybe_returns_never_null() const;
-  bool      is_never_null_at(int index) const;
+  bool      maybe_returns_value_type() const;
 
   int       size() const                         { return _size; }
   int       count() const                        { return _count; }
 
   int       arg_size_for_bc(Bytecodes::Code bc)  { return size() + (Bytecodes::has_receiver(bc) ? 1 : 0); }
diff a/src/hotspot/share/ci/ciStreams.cpp b/src/hotspot/share/ci/ciStreams.cpp
--- a/src/hotspot/share/ci/ciStreams.cpp
+++ b/src/hotspot/share/ci/ciStreams.cpp
@@ -190,17 +190,17 @@
   constantPoolHandle cpool(THREAD, _method->get_Method()->constants());
   return CURRENT_ENV->get_klass_by_index(cpool, get_klass_index(), will_link, _holder);
 }
 
 // ------------------------------------------------------------------
-// ciBytecodeStream::is_klass_never_null
+// ciBytecodeStream::is_inline_klass
 //
-// Get information about nullability from the constant pool.
-bool ciBytecodeStream::is_klass_never_null() const {
+// Check if the klass is an inline klass.
+bool ciBytecodeStream::is_inline_klass() const {
   VM_ENTRY_MARK;
   constantPoolHandle cpool(THREAD, _method->get_Method()->constants());
-  return CURRENT_ENV->is_klass_never_null(cpool, get_klass_index());
+  return CURRENT_ENV->is_inline_klass(cpool, get_klass_index());
 }
 
 // ------------------------------------------------------------------
 // ciBytecodeStream::get_constant_raw_index
 //
diff a/src/hotspot/share/ci/ciStreams.hpp b/src/hotspot/share/ci/ciStreams.hpp
--- a/src/hotspot/share/ci/ciStreams.hpp
+++ b/src/hotspot/share/ci/ciStreams.hpp
@@ -219,11 +219,11 @@
 
   // If this bytecode is a new, newarray, multianewarray, instanceof,
   // or checkcast, get the referenced klass.
   ciKlass* get_klass(bool& will_link);
   int get_klass_index() const;
-  bool is_klass_never_null() const;
+  bool is_inline_klass() const;
 
   // If this bytecode is one of the ldc variants, get the referenced
   // constant.  Do not attempt to resolve it, since that would require
   // execution of Java code.  If it is not resolved, return an unloaded
   // object (ciConstant.as_object()->is_loaded() == false).
@@ -286,18 +286,10 @@
     } else {
       return _sig->type_at(_pos);
     }
   }
 
-  bool is_never_null() {
-    if (at_return_type()) {
-      return _sig->returns_never_null();
-    } else {
-      return _sig->is_never_null_at(_pos);
-    }
-  }
-
   // next klass in the signature
   ciKlass* next_klass() {
     ciKlass* sig_k;
     if (_holder != NULL) {
       sig_k = _holder;
diff a/src/hotspot/share/ci/ciType.hpp b/src/hotspot/share/ci/ciType.hpp
--- a/src/hotspot/share/ci/ciType.hpp
+++ b/src/hotspot/share/ci/ciType.hpp
@@ -33,11 +33,10 @@
 
 class ciType : public ciMetadata {
   CI_PACKAGE_ACCESS
   friend class ciKlass;
   friend class ciReturnAddress;
-  friend class ciWrapper;
 
 private:
   BasicType _basic_type;
 
   ciType(BasicType t);     // for primitive and unloaded types
@@ -76,13 +75,10 @@
 
   // What kind of ciObject is this?
   bool is_type() const                      { return true; }
   bool is_classless() const                 { return is_primitive_type(); }
 
-  virtual ciType*     unwrap()              { return this; }
-  virtual bool is_never_null() const        { return false; }
-
   const char* name();
   virtual void print_name_on(outputStream* st);
   void print_name() {
     print_name_on(tty);
   }
@@ -114,34 +110,6 @@
   int  bci() { return _bci; }
 
   static ciReturnAddress* make(int bci);
 };
 
-// ciWrapper
-//
-// This class wraps another type to carry additional information like nullability.
-// Should only be instantiated and used by ciTypeFlow and ciSignature.
-class ciWrapper : public ciType {
-  CI_PACKAGE_ACCESS
-
-private:
-  ciType* _type;
-  bool _never_null;
-
-  ciWrapper(ciType* type, bool never_null) : ciType(type->basic_type()) {
-    assert(type->is_valuetype(), "should only be used for value types");
-    _type = type;
-    _never_null = never_null;
-  }
-
-  const char* type_string() { return "ciWrapper"; }
-
-  void print_impl(outputStream* st) { _type->print_impl(st); }
-
-public:
-  bool    is_wrapper() const { return true; }
-
-  ciType*     unwrap()       { return _type; }
-  bool is_never_null() const { return _never_null; }
-};
-
 #endif // SHARE_CI_CITYPE_HPP
diff a/src/hotspot/share/ci/ciTypeFlow.cpp b/src/hotspot/share/ci/ciTypeFlow.cpp
--- a/src/hotspot/share/ci/ciTypeFlow.cpp
+++ b/src/hotspot/share/ci/ciTypeFlow.cpp
@@ -274,30 +274,25 @@
   assert(t1 != t2, "checked in caller");
   if (t1->equals(top_type())) {
     return t2;
   } else if (t2->equals(top_type())) {
     return t1;
-  }
-
-  // Unwrap after saving nullness information and handling top meets
-  bool never_null1 = t1->is_never_null();
-  bool never_null2 = t2->is_never_null();
-  if (t1->unwrap() == t2->unwrap() && never_null1 == never_null2) {
-    return t1;
-  }
-  t1 = t1->unwrap();
-  t2 = t2->unwrap();
-
-  if (t1->is_primitive_type() || t2->is_primitive_type()) {
+  } else if (t1->is_primitive_type() || t2->is_primitive_type()) {
     // Special case null_type.  null_type meet any reference type T
-    // is T.  null_type meet null_type is null_type.
+    // is T (except for inline types).  null_type meet null_type is null_type.
     if (t1->equals(null_type())) {
-      if (!t2->is_primitive_type() || t2->equals(null_type())) {
+      if (t2->is_valuetype()) {
+        // Inline types are null-free, return the super type
+        return t2->as_value_klass()->super();
+      } else if (!t2->is_primitive_type() || t2->equals(null_type())) {
         return t2;
       }
     } else if (t2->equals(null_type())) {
-      if (!t1->is_primitive_type()) {
+      if (t1->is_valuetype()) {
+        // Inline types are null-free, return the super type
+        return t1->as_value_klass()->super();
+      } else if (!t1->is_primitive_type()) {
         return t1;
       }
     }
 
     // At least one of the two types is a non-top primitive type.
@@ -347,16 +342,11 @@
     }
   } else {
     // Must be two plain old instance klasses.
     assert(k1->is_instance_klass(), "previous cases handle non-instances");
     assert(k2->is_instance_klass(), "previous cases handle non-instances");
-    ciType* result = k1->least_common_ancestor(k2);
-    if (never_null1 && never_null2 && result->is_valuetype()) {
-      // Both value types are never null, mark the result as never null
-      result = analyzer->mark_as_never_null(result);
-    }
-    return result;
+    return k1->least_common_ancestor(k2);
   }
 }
 
 
 // ------------------------------------------------------------------
@@ -414,26 +404,17 @@
     // even if it were possible for an OSR entry point to be at bci zero.
   }
   // "Push" the method signature into the first few locals.
   state->set_stack_size(-max_locals());
   if (!method()->is_static()) {
-    ciType* holder = method()->holder();
-    if (holder->is_valuetype()) {
-      // The receiver is never null
-      holder = mark_as_never_null(holder);
-    }
-    state->push(holder);
+    state->push(method()->holder());
     assert(state->tos() == state->local(0), "");
   }
   for (ciSignatureStream str(method()->signature());
        !str.at_return_type();
        str.next()) {
-    ciType* arg = str.type();
-    if (str.is_never_null()) {
-      arg = mark_as_never_null(arg);
-    }
-    state->push_translate(arg);
+    state->push_translate(str.type());
   }
   // Set the rest of the locals to bottom.
   Cell cell = state->next_cell(state->tos());
   state->set_stack_size(0);
   int limit = state->limit_cell();
@@ -605,28 +586,22 @@
     trap(str, element_klass,
          Deoptimization::make_trap_request
          (Deoptimization::Reason_unloaded,
           Deoptimization::Action_reinterpret));
   } else {
-    if (element_klass->is_valuetype()) {
-      // Value type array elements are never null
-      push(outer()->mark_as_never_null(element_klass));
-    } else {
-      push_object(element_klass);
-    }
+    push_object(element_klass);
   }
 }
 
 
 // ------------------------------------------------------------------
 // ciTypeFlow::StateVector::do_checkcast
 void ciTypeFlow::StateVector::do_checkcast(ciBytecodeStream* str) {
   bool will_link;
   ciKlass* klass = str->get_klass(will_link);
-  bool never_null = str->is_klass_never_null();
   if (!will_link) {
-    if (never_null) {
+    if (str->is_inline_klass()) {
       trap(str, klass,
            Deoptimization::make_trap_request
            (Deoptimization::Reason_unloaded,
             Deoptimization::Action_reinterpret));
     } else {
@@ -636,17 +611,12 @@
       // 2) C2 does an OSR compile in a later block (see bug 4778368).
       pop_object();
       do_null_assert(klass);
     }
   } else {
-    ciType* type = pop_value();
-    if (klass->is_valuetype() && (never_null || type->is_never_null())) {
-      // Casting to a Q-Type contains a NULL check
-      push(outer()->mark_as_never_null(klass));
-    } else {
-      push_object(klass);
-    }
+    pop_object();
+    push_object(klass);
   }
 }
 
 // ------------------------------------------------------------------
 // ciTypeFlow::StateVector::do_getfield
@@ -684,14 +654,10 @@
       // here can make an OSR entry point unreachable, triggering the
       // assert on non_osr_block in ciTypeFlow::get_start_state.
       // (See bug 4379915.)
       do_null_assert(field_type->as_klass());
     } else {
-      if (field->is_flattenable()) {
-        // A flattenable field is never null
-        field_type = outer()->mark_as_never_null(field_type);
-      }
       push_translate(field_type);
     }
   }
 }
 
@@ -755,13 +721,10 @@
         // ever sees a non-null value, loading has occurred.
         //
         // See do_getstatic() for similar explanation, as well as bug 4684993.
         do_null_assert(return_type->as_klass());
       } else {
-        if (sigstr.is_never_null()) {
-          return_type = outer()->mark_as_never_null(return_type);
-        }
         push_translate(return_type);
       }
     }
   }
 }
@@ -787,15 +750,11 @@
     ciObject* obj = con.as_object();
     if (obj->is_null_object()) {
       push_null();
     } else {
       assert(obj->is_instance() || obj->is_array(), "must be java_mirror of klass");
-      ciType* type = obj->klass();
-      if (type->is_valuetype()) {
-        type = outer()->mark_as_never_null(type);
-      }
-      push(type);
+      push_object(obj->klass());
     }
   } else {
     push_translate(ciType::make(basic_type));
   }
 }
@@ -834,12 +793,12 @@
   bool will_link;
   ciKlass* klass = str->get_klass(will_link);
   if (!will_link) {
     trap(str, klass, str->get_klass_index());
   } else {
-    // The default value type is never null
-    push(outer()->mark_as_never_null(klass));
+    assert(klass->is_valuetype(), "should be value type");
+    push_object(klass);
   }
 }
 
 // ------------------------------------------------------------------
 // ciTypeFlow::StateVector::do_withfield
@@ -857,12 +816,12 @@
       ciType* type2 = pop_value();
       assert(type2->is_two_word(), "must be 2nd half");
       assert(type == half_type(type2), "must be 2nd half");
     }
     pop_object();
-    // The newly created value type can never be null
-    push(outer()->mark_as_never_null(klass));
+    assert(klass->is_valuetype(), "should be value type");
+    push_object(klass);
   }
 }
 
 // ------------------------------------------------------------------
 // ciTypeFlow::StateVector::do_newarray
@@ -3077,15 +3036,10 @@
     // Record the first failure reason.
     _failure_reason = reason;
   }
 }
 
-ciType* ciTypeFlow::mark_as_never_null(ciType* type) {
-  // Wrap the type to carry the information that it is never null
-  return env()->make_never_null_wrapper(type);
-}
-
 #ifndef PRODUCT
 // ------------------------------------------------------------------
 // ciTypeFlow::print_on
 void ciTypeFlow::print_on(outputStream* st) const {
   // Walk through CI blocks
diff a/src/hotspot/share/ci/ciTypeFlow.hpp b/src/hotspot/share/ci/ciTypeFlow.hpp
--- a/src/hotspot/share/ci/ciTypeFlow.hpp
+++ b/src/hotspot/share/ci/ciTypeFlow.hpp
@@ -845,12 +845,10 @@
   int start_block_num() const       { return 0; }
   Block* rpo_at(int rpo) const      { assert(0 <= rpo && rpo < block_count(), "out of bounds");
                                       return _block_map[rpo]; }
   int inc_next_pre_order()          { return _next_pre_order++; }
 
-  ciType* mark_as_never_null(ciType* type);
-
 private:
   // A work list used during flow analysis.
   Block* _work_list;
 
   // List of blocks in reverse post order
diff a/src/hotspot/share/oops/symbol.cpp b/src/hotspot/share/oops/symbol.cpp
--- a/src/hotspot/share/oops/symbol.cpp
+++ b/src/hotspot/share/oops/symbol.cpp
@@ -138,16 +138,10 @@
     }
   }
   return false;
 }
 
-bool Symbol::is_Q_singledim_array_signature() const {
-  int len = utf8_length();
-  return len > 3 && char_at(0) == JVM_SIGNATURE_ARRAY && char_at(1) == JVM_SIGNATURE_INLINE_TYPE &&
-                    char_at(len - 1) == JVM_SIGNATURE_ENDCLASS;
-}
-
 Symbol* Symbol::fundamental_name(TRAPS) {
   if ((char_at(0) == JVM_SIGNATURE_INLINE_TYPE || char_at(0) == JVM_SIGNATURE_CLASS) && ends_with(JVM_SIGNATURE_ENDCLASS)) {
     return SymbolTable::new_symbol(this, 1, utf8_length() - 1);
   } else {
     // reference count is incremented to be consistent with the behavior with
diff a/src/hotspot/share/opto/doCall.cpp b/src/hotspot/share/opto/doCall.cpp
--- a/src/hotspot/share/opto/doCall.cpp
+++ b/src/hotspot/share/opto/doCall.cpp
@@ -712,12 +712,11 @@
         } else if (is_reference_type(rt)) {
           assert(is_reference_type(ct), "rt=%s, ct=%s", type2name(rt), type2name(ct));
           if (ctype->is_loaded()) {
             const TypeOopPtr* arg_type = TypeOopPtr::make_from_klass(rtype->as_klass());
             const Type*       sig_type = TypeOopPtr::make_from_klass(ctype->as_klass());
-            if (declared_signature->returns_never_null()) {
-              assert(ct == T_INLINE_TYPE, "should be a value type");
+            if (ct == T_INLINE_TYPE) {
               sig_type = sig_type->join_speculative(TypePtr::NOTNULL);
             }
             if (arg_type != NULL && !arg_type->higher_equal(sig_type) && !peek()->is_ValueType()) {
               Node* retnode = pop();
               Node* cast_obj = _gvn.transform(new CheckCastPPNode(control(), retnode, sig_type));
@@ -744,11 +743,12 @@
              "mismatched return types: rtype=%s, ctype=%s", rtype->name(), ctype->name());
     }
 
     if (rtype->basic_type() == T_INLINE_TYPE && !peek()->is_ValueType()) {
       Node* retnode = pop();
-      if (!gvn().type(retnode)->maybe_null() && rtype->as_value_klass()->is_scalarizable()) {
+      assert(!gvn().type(retnode)->maybe_null(), "should never be null");
+      if (rtype->as_value_klass()->is_scalarizable()) {
         retnode = ValueTypeNode::make_from_oop(this, retnode, rtype->as_value_klass());
       }
       push_node(T_INLINE_TYPE, retnode);
     }
 
diff a/src/hotspot/share/opto/graphKit.cpp b/src/hotspot/share/opto/graphKit.cpp
--- a/src/hotspot/share/opto/graphKit.cpp
+++ b/src/hotspot/share/opto/graphKit.cpp
@@ -3306,26 +3306,28 @@
 // array store bytecode.  Stack must be as-if BEFORE doing the bytecode so the
 // uncommon-trap paths work.  Adjust stack after this call.
 // If failure_control is supplied and not null, it is filled in with
 // the control edge for the cast failure.  Otherwise, an appropriate
 // uncommon trap or exception is thrown.
-Node* GraphKit::gen_checkcast(Node *obj, Node* superklass, Node* *failure_control, bool never_null) {
+Node* GraphKit::gen_checkcast(Node *obj, Node* superklass, Node* *failure_control) {
   kill_dead_locals();           // Benefit all the uncommon traps
   const TypeKlassPtr* tk = _gvn.type(superklass)->is_klassptr();
   const TypeOopPtr* toop = TypeOopPtr::make_from_klass(tk->klass());
-  assert(!never_null || toop->is_valuetypeptr(), "must be a value type pointer");
-  bool is_value = obj->is_ValueType();
+
+  // Check if inline types are involved
+  bool from_inline = obj->is_ValueType();
+  bool to_inline = tk->klass()->is_valuetype();
 
   // Fast cutout:  Check the case that the cast is vacuously true.
   // This detects the common cases where the test will short-circuit
   // away completely.  We do this before we perform the null check,
   // because if the test is going to turn into zero code, we don't
   // want a residual null check left around.  (Causes a slowdown,
   // for example, in some objArray manipulations, such as a[i]=a[j].)
   if (tk->singleton()) {
     ciKlass* klass = NULL;
-    if (is_value) {
+    if (from_inline) {
       klass = _gvn.type(obj)->value_klass();
     } else {
       const TypeOopPtr* objtp = _gvn.type(obj)->isa_oopptr();
       if (objtp != NULL) {
         klass = objtp->klass();
@@ -3335,23 +3337,23 @@
       switch (C->static_subtype_check(tk->klass(), klass)) {
       case Compile::SSC_always_true:
         // If we know the type check always succeed then we don't use
         // the profiling data at this bytecode. Don't lose it, feed it
         // to the type system as a speculative type.
-        if (!is_value) {
+        if (!from_inline) {
           obj = record_profiled_receiver_for_speculation(obj);
-          if (never_null) {
+          if (to_inline) {
             obj = null_check(obj);
-          }
-          if (toop->is_valuetypeptr() && toop->value_klass()->is_scalarizable() && !gvn().type(obj)->maybe_null()) {
-            obj = ValueTypeNode::make_from_oop(this, obj, toop->value_klass());
+            if (toop->value_klass()->is_scalarizable()) {
+              obj = ValueTypeNode::make_from_oop(this, obj, toop->value_klass());
+            }
           }
         }
         return obj;
       case Compile::SSC_always_false:
-        if (is_value || never_null) {
-          if (!is_value) {
+        if (from_inline || to_inline) {
+          if (!from_inline) {
             null_check(obj);
           }
           // Value type is never null. Always throw an exception.
           builtin_throw(Deoptimization::Reason_class_check, makecon(TypeKlassPtr::make(klass)));
           return top();
@@ -3390,13 +3392,13 @@
                          && seems_never_null(obj, data, speculative_not_null));
 
   // Null check; get casted pointer; set region slot 3
   Node* null_ctl = top();
   Node* not_null_obj = NULL;
-  if (is_value) {
+  if (from_inline) {
     not_null_obj = obj;
-  } else if (never_null) {
+  } else if (to_inline) {
     not_null_obj = null_check(obj);
   } else {
     not_null_obj = null_check_oop(obj, &null_ctl, never_see_null, safe_for_replace, speculative_not_null);
   }
 
@@ -3414,11 +3416,11 @@
     region->del_req(_null_path);
     phi   ->del_req(_null_path);
   }
 
   Node* cast_obj = NULL;
-  if (!is_value && tk->klass_is_exact()) {
+  if (!from_inline && tk->klass_is_exact()) {
     // The following optimization tries to statically cast the speculative type of the object
     // (for example obtained during profiling) to the type of the superklass and then do a
     // dynamic check that the type of the object is what we expect. To work correctly
     // for checkcast and aastore the type of superklass should be exact.
     const TypeOopPtr* obj_type = _gvn.type(obj)->is_oopptr();
@@ -3446,18 +3448,18 @@
   if (cast_obj == NULL) {
     // Generate the subtype check
     Node* not_subtype_ctrl = gen_subtype_check(not_null_obj, superklass);
 
     // Plug in success path into the merge
-    cast_obj = is_value ? not_null_obj : _gvn.transform(new CheckCastPPNode(control(), not_null_obj, toop));
+    cast_obj = from_inline ? not_null_obj : _gvn.transform(new CheckCastPPNode(control(), not_null_obj, toop));
     // Failure path ends in uncommon trap (or may be dead - failure impossible)
     if (failure_control == NULL) {
       if (not_subtype_ctrl != top()) { // If failure is possible
         PreserveJVMState pjvms(this);
         set_control(not_subtype_ctrl);
         Node* obj_klass = NULL;
-        if (is_value) {
+        if (from_inline) {
           obj_klass = makecon(TypeKlassPtr::make(_gvn.type(not_null_obj)->value_klass()));
         } else {
           obj_klass = load_object_klass(not_null_obj);
         }
         builtin_throw(Deoptimization::Reason_class_check, obj_klass);
@@ -3485,13 +3487,13 @@
 
   // Return final merged results
   set_control( _gvn.transform(region) );
   record_for_igvn(region);
 
-  bool not_null_free = !toop->can_be_value_type();
-  bool not_flattenable = !UseFlatArray || not_null_free || (toop->is_valuetypeptr() && !toop->value_klass()->flatten_array());
-  if (EnableValhalla && not_flattenable) {
+  bool not_inline = !toop->can_be_value_type();
+  bool not_flattened = !UseFlatArray || not_inline || (toop->is_valuetypeptr() && !toop->value_klass()->flatten_array());
+  if (EnableValhalla && not_flattened) {
     // Check if obj has been loaded from an array
     obj = obj->isa_DecodeN() ? obj->in(1) : obj;
     Node* array = NULL;
     if (obj->isa_Load()) {
       Node* address = obj->in(MemNode::Address);
@@ -3509,26 +3511,27 @@
       }
     }
     if (array != NULL) {
       const TypeAryPtr* ary_t = _gvn.type(array)->isa_aryptr();
       if (ary_t != NULL) {
-        if (!ary_t->is_not_null_free() && not_null_free) {
+        if (!ary_t->is_not_null_free() && not_inline) {
           // Casting array element to a non-inline-type, mark array as not null-free.
           Node* cast = _gvn.transform(new CheckCastPPNode(control(), array, ary_t->cast_to_not_null_free()));
           replace_in_map(array, cast);
         } else if (!ary_t->is_not_flat()) {
-          // Casting array element to a non-flattenable type, mark array as not flat.
+          // Casting array element to a non-flattened type, mark array as not flat.
           Node* cast = _gvn.transform(new CheckCastPPNode(control(), array, ary_t->cast_to_not_flat()));
           replace_in_map(array, cast);
         }
       }
     }
   }
 
-  if (!is_value) {
+  if (!from_inline) {
     res = record_profiled_receiver_for_speculation(res);
-    if (toop->is_valuetypeptr() && toop->value_klass()->is_scalarizable() && !gvn().type(res)->maybe_null()) {
+    if (to_inline && toop->value_klass()->is_scalarizable()) {
+      assert(!gvn().type(res)->maybe_null(), "Inline types are null-free");
       res = ValueTypeNode::make_from_oop(this, res, toop->value_klass());
     }
   }
   return res;
 }
@@ -4578,11 +4581,11 @@
   }
   const Type* con_type = Type::make_constant_from_field(field, holder, field->layout_type(),
                                                         /*is_unsigned_load=*/false);
   if (con_type != NULL) {
     Node* con = makecon(con_type);
-    assert(!field->is_flattenable() || (field->is_static() && !con_type->is_zero_type()), "sanity");
+    assert(!field->type()->is_valuetype() || (field->is_static() && !con_type->is_zero_type()), "sanity");
     // Check type of constant which might be more precise
     if (con_type->is_valuetypeptr() && con_type->value_klass()->is_scalarizable()) {
       // Load value type from constant oop
       con = ValueTypeNode::make_from_oop(this, con, con_type->value_klass());
     }
diff a/src/hotspot/share/opto/graphKit.hpp b/src/hotspot/share/opto/graphKit.hpp
--- a/src/hotspot/share/opto/graphKit.hpp
+++ b/src/hotspot/share/opto/graphKit.hpp
@@ -849,11 +849,11 @@
   // and the reflective instance-of call.
   Node* gen_instanceof(Node *subobj, Node* superkls, bool safe_for_replace = false);
 
   // Generate a check-cast idiom.  Used by both the check-cast bytecode
   // and the array-store bytecode
-  Node* gen_checkcast(Node *subobj, Node* superkls, Node* *failure_control = NULL, bool never_null = false);
+  Node* gen_checkcast(Node *subobj, Node* superkls, Node* *failure_control = NULL);
 
   Node* is_value_type(Node* obj);
   Node* is_non_flattened_array(Node* ary);
   Node* is_nullable_array(Node* ary);
   Node* gen_value_array_null_guard(Node* ary, Node* val, int nargs, bool safe_for_replace = false);
diff a/src/hotspot/share/opto/library_call.cpp b/src/hotspot/share/opto/library_call.cpp
--- a/src/hotspot/share/opto/library_call.cpp
+++ b/src/hotspot/share/opto/library_call.cpp
@@ -2703,12 +2703,12 @@
     }
     if (type == T_ADDRESS) {
       p = gvn().transform(new CastP2XNode(NULL, p));
       p = ConvX2UL(p);
     }
-    if (field != NULL && field->is_flattenable() && !field->is_flattened()) {
-      // Load a non-flattened but flattenable value type from memory
+    if (field != NULL && field->type()->is_valuetype() && !field->is_flattened()) {
+      // Load a non-flattened value type from memory
       if (value_type->value_klass()->is_scalarizable()) {
         p = ValueTypeNode::make_from_oop(this, p, value_type->value_klass());
       } else {
         p = null2default(p, value_type->value_klass());
       }
diff a/src/hotspot/share/opto/parse1.cpp b/src/hotspot/share/opto/parse1.cpp
--- a/src/hotspot/share/opto/parse1.cpp
+++ b/src/hotspot/share/opto/parse1.cpp
@@ -181,11 +181,11 @@
   // toward more specific classes.  Make sure these specific classes
   // are still in effect.
   if (tp != NULL && tp->klass() != C->env()->Object_klass()) {
     // TypeFlow asserted a specific object type.  Value must have that type.
     Node* bad_type_ctrl = NULL;
-    if (tp->is_valuetypeptr() && !tp->maybe_null()) {
+    if (tp->klass()->is_valuetype()) {
       // Check value types for null here to prevent checkcast from adding an
       // exception state before the bytecode entry (use 'bad_type_ctrl' instead).
       l = null_check_oop(l, &bad_type_ctrl);
       bad_type_exit->control()->add_req(bad_type_ctrl);
     }
diff a/src/hotspot/share/opto/parse2.cpp b/src/hotspot/share/opto/parse2.cpp
--- a/src/hotspot/share/opto/parse2.cpp
+++ b/src/hotspot/share/opto/parse2.cpp
@@ -86,11 +86,11 @@
     // Load from flattened value type array
     Node* vt = ValueTypeNode::make_from_flattened(this, elemtype->value_klass(), ary, adr);
     push(vt);
     return;
   } else if (elemptr != NULL && elemptr->is_valuetypeptr() && !elemptr->maybe_null()) {
-    // Load from non-flattened but flattenable value type array (elements can never be null)
+    // Load from non-flattened inline type array (elements can never be null)
     bt = T_INLINE_TYPE;
   } else if (!ary_t->is_not_flat()) {
     // Cannot statically determine if array is flattened, emit runtime check
     assert(UseFlatArray && is_reference_type(bt) && elemptr->can_be_value_type() && !ary_t->klass_is_exact() && !ary_t->is_not_null_free() &&
            (!elemptr->is_valuetypeptr() || elemptr->value_klass()->flatten_array()), "array can't be flattened");
@@ -110,11 +110,11 @@
       // flattened
       sync_kit(ideal);
       if (elemptr->is_valuetypeptr()) {
         // Element type is known, cast and load from flattened representation
         ciValueKlass* vk = elemptr->value_klass();
-        assert(vk->flatten_array() && elemptr->maybe_null(), "must be a flattenable and nullable array");
+        assert(vk->flatten_array() && elemptr->maybe_null(), "never/always flat - should be optimized");
         ciArrayKlass* array_klass = ciArrayKlass::make(vk);
         const TypeAryPtr* arytype = TypeOopPtr::make_from_klass(array_klass)->isa_aryptr();
         Node* cast = _gvn.transform(new CheckCastPPNode(control(), ary, arytype));
         Node* casted_adr = array_element_address(cast, idx, T_INLINE_TYPE, ary_t->size(), control());
         // Re-execute flattened array load if buffering triggers deoptimization
@@ -209,11 +209,11 @@
   }
   const TypeAryPtr* adr_type = TypeAryPtr::get_array_body_type(bt);
   Node* ld = access_load_at(ary, adr, adr_type, elemtype, bt,
                             IN_HEAP | IS_ARRAY | C2_CONTROL_DEPENDENT_LOAD);
   if (bt == T_INLINE_TYPE) {
-    // Loading a non-flattened (but flattenable) value type from an array
+    // Loading a non-flattened value type from an array
     assert(!gvn().type(ld)->maybe_null(), "value type array elements should never be null");
     if (elemptr->value_klass()->is_scalarizable()) {
       ld = ValueTypeNode::make_from_oop(this, ld, elemptr->value_klass());
     }
   }
@@ -249,29 +249,27 @@
     const Type* tval = _gvn.type(cast_val);
     // We may have lost type information for 'val' here due to the casts
     // emitted by the array_store_check code (see JDK-6312651)
     // TODO Remove this code once JDK-6312651 is in.
     const Type* tval_init = _gvn.type(val);
-    bool can_be_value_type = tval->isa_valuetype() || (tval != TypePtr::NULL_PTR && tval_init->is_oopptr()->can_be_value_type() && tval->is_oopptr()->can_be_value_type());
-    bool not_flattenable = !can_be_value_type || ((tval_init->is_valuetypeptr() || tval_init->isa_valuetype()) && !tval_init->value_klass()->flatten_array());
+    bool not_inline = !tval->isa_valuetype() && (tval == TypePtr::NULL_PTR || !tval_init->is_oopptr()->can_be_value_type() || !tval->is_oopptr()->can_be_value_type());
+    bool not_flattened = !ValueArrayFlatten || not_inline || ((tval_init->is_valuetypeptr() || tval_init->isa_valuetype()) && !tval_init->value_klass()->flatten_array());
 
-    if (!ary_t->is_not_null_free() && !can_be_value_type && (!tval->maybe_null() || !tval_init->maybe_null())) {
-      // Storing a non-inline-type, mark array as not null-free.
-      // This is only legal for non-null stores because the array_store_check passes for null.
+    if (!ary_t->is_not_null_free() && not_inline && (!tval->maybe_null() || !tval_init->maybe_null())) {
+      // Storing a non-inline type, mark array as not null-free (-> not flat).
+      // This is only legal for non-null stores because the array_store_check always passes for null.
+      // Null stores are handled in GraphKit::gen_value_array_null_guard().
       ary_t = ary_t->cast_to_not_null_free();
       Node* cast = _gvn.transform(new CheckCastPPNode(control(), ary, ary_t));
       replace_in_map(ary, cast);
       ary = cast;
-    } else if (!ary_t->is_not_flat() && not_flattenable) {
-      // Storing a non-flattenable value, mark array as not flat.
+    } else if (!ary_t->is_not_flat() && not_flattened) {
+      // Storing a non-flattened value, mark array as not flat.
       ary_t = ary_t->cast_to_not_flat();
-      if (tval != TypePtr::NULL_PTR) {
-        // For NULL, this transformation is only valid after the null guard below
-        Node* cast = _gvn.transform(new CheckCastPPNode(control(), ary, ary_t));
-        replace_in_map(ary, cast);
-        ary = cast;
-      }
+      Node* cast = _gvn.transform(new CheckCastPPNode(control(), ary, ary_t));
+      replace_in_map(ary, cast);
+      ary = cast;
     }
 
     if (ary_t->elem()->isa_valuetype() != NULL) {
       // Store to flattened value type array
       C->set_flattened_accesses();
@@ -287,20 +285,20 @@
       inc_sp(3);
       jvms()->set_should_reexecute(true);
       cast_val->as_ValueType()->store_flattened(this, ary, adr, NULL, 0, MO_UNORDERED | IN_HEAP | IS_ARRAY);
       return;
     } else if (elemtype->is_valuetypeptr() && !elemtype->maybe_null()) {
-      // Store to non-flattened but flattenable value type array (elements can never be null)
+      // Store to non-flattened inline type array (elements can never be null)
       if (!cast_val->is_ValueType() && tval->maybe_null()) {
         inc_sp(3);
         cast_val = null_check(cast_val);
         if (stopped()) return;
         dec_sp(3);
       }
     } else if (!ary_t->is_not_flat()) {
       // Array might be flattened, emit runtime checks
-      assert(UseFlatArray && !not_flattenable && elemtype->is_oopptr()->can_be_value_type() &&
+      assert(UseFlatArray && !not_flattened && elemtype->is_oopptr()->can_be_value_type() &&
              !ary_t->klass_is_exact() && !ary_t->is_not_null_free(), "array can't be flattened");
       IdealKit ideal(this);
       ideal.if_then(is_non_flattened_array(ary)); {
         // non-flattened
         assert(ideal.ctrl()->in(0)->as_If()->is_non_flattened_array_check(&_gvn), "Should be found");
@@ -338,11 +336,11 @@
         }
         Node* casted_ary = ary;
         if (vk != NULL && !stopped()) {
           // Element type is known, cast and store to flattened representation
           sync_kit(ideal);
-          assert(vk->flatten_array() && elemtype->maybe_null(), "must be a flattenable and nullable array");
+          assert(vk->flatten_array() && elemtype->maybe_null(), "never/always flat - should be optimized");
           ciArrayKlass* array_klass = ciArrayKlass::make(vk);
           const TypeAryPtr* arytype = TypeOopPtr::make_from_klass(array_klass)->isa_aryptr();
           casted_ary = _gvn.transform(new CheckCastPPNode(control(), casted_ary, arytype));
           Node* casted_adr = array_element_address(casted_ary, idx, T_OBJECT, arytype->size(), control());
           if (!val->is_ValueType()) {
diff a/src/hotspot/share/opto/parse3.cpp b/src/hotspot/share/opto/parse3.cpp
--- a/src/hotspot/share/opto/parse3.cpp
+++ b/src/hotspot/share/opto/parse3.cpp
@@ -138,11 +138,10 @@
   }
 
   ciType* field_klass = field->type();
   bool is_vol = field->is_volatile();
   bool flattened = field->is_flattened();
-  bool flattenable = field->is_flattenable();
 
   // Compute address and memory type.
   int offset = field->offset_in_bytes();
   const TypePtr* adr_type = C->alias_type(field)->adr_type();
   Node *adr = basic_plus_adr(obj, obj, offset);
@@ -172,11 +171,11 @@
         type = TypeOopPtr::make_from_constant(con)->isa_oopptr();
       }
       assert(type != NULL, "field singleton type must be consistent");
     } else {
       type = TypeOopPtr::make_from_klass(field_klass->as_klass());
-      if (bt == T_INLINE_TYPE && field->is_static() && flattenable) {
+      if (bt == T_INLINE_TYPE && field->is_static()) {
         // Check if static value type field is already initialized
         assert(!flattened, "static fields should not be flattened");
         ciInstance* mirror = field->holder()->java_mirror();
         ciObject* val = mirror->field_value(field).as_object();
         if (!val->is_null_object()) {
@@ -194,12 +193,12 @@
     ld = ValueTypeNode::make_from_flattened(this, field_klass->as_value_klass(), obj, obj, field->holder(), offset);
   } else {
     DecoratorSet decorators = IN_HEAP;
     decorators |= is_vol ? MO_SEQ_CST : MO_UNORDERED;
     ld = access_load_at(obj, adr, adr_type, type, bt, decorators);
-    if (flattenable) {
-      // Load a non-flattened but flattenable value type from memory
+    if (bt == T_INLINE_TYPE) {
+      // Load a non-flattened value type from memory
       if (field_klass->as_value_klass()->is_scalarizable()) {
         ld = ValueTypeNode::make_from_oop(this, ld, field_klass->as_value_klass());
       } else {
         ld = null2default(ld, field_klass->as_value_klass());
       }
@@ -262,15 +261,17 @@
     } else {
       field_type = Type::BOTTOM;
     }
   }
 
-  if (field->is_flattenable() && !val->is_ValueType()) {
-    inc_sp(1);
-    val = null_check(val);
-    dec_sp(1);
-    if (stopped()) return;
+  if (bt == T_INLINE_TYPE && !val->is_ValueType()) {
+    // We can see a null constant here
+    assert(val->bottom_type()->remove_speculative() == TypePtr::NULL_PTR, "Anything other than null?");
+    push(null());
+    uncommon_trap(Deoptimization::Reason_null_check, Deoptimization::Action_none);
+    assert(stopped(), "dead path");
+    return;
   }
 
   if (field->is_flattened()) {
     // Store flattened value type to a non-static field
     if (!val->is_ValueType()) {
diff a/src/hotspot/share/opto/parseHelper.cpp b/src/hotspot/share/opto/parseHelper.cpp
--- a/src/hotspot/share/opto/parseHelper.cpp
+++ b/src/hotspot/share/opto/parseHelper.cpp
@@ -67,20 +67,18 @@
 //=============================================================================
 //------------------------------do_checkcast-----------------------------------
 void Parse::do_checkcast() {
   bool will_link;
   ciKlass* klass = iter().get_klass(will_link);
-  bool never_null = iter().is_klass_never_null();
-
   Node *obj = peek();
 
   // Throw uncommon trap if class is not loaded or the value we are casting
   // _from_ is not loaded, and value is not null.  If the value _is_ NULL,
   // then the checkcast does nothing.
   const TypeOopPtr *tp = _gvn.type(obj)->isa_oopptr();
   if (!will_link || (tp && tp->klass() && !tp->klass()->is_loaded())) {
-    assert(!never_null, "Null-free value type should be loaded");
+    assert(!iter().is_inline_klass(), "Inline type should be loaded");
     if (C->log() != NULL) {
       if (!will_link) {
         C->log()->elem("assert_null reason='checkcast' klass='%d'",
                        C->log()->identify(klass));
       }
@@ -96,11 +94,11 @@
       profile_null_checkcast();
     }
     return;
   }
 
-  Node* res = gen_checkcast(obj, makecon(TypeKlassPtr::make(klass)), NULL, never_null);
+  Node* res = gen_checkcast(obj, makecon(TypeKlassPtr::make(klass)));
   if (stopped()) {
     return;
   }
 
   // Pop from stack AFTER gen_checkcast because it can uncommon trap and
@@ -355,27 +353,21 @@
   ciValueKlass* holder_klass = field->holder()->as_value_klass();
   Node* holder = pop();
   int nargs = 1 + field->type()->size();
 
   if (!holder->is_ValueType()) {
-    // Null check and scalarize value type holder
-    inc_sp(nargs);
-    holder = null_check(holder);
-    dec_sp(nargs);
-    if (stopped()) return;
+    // Scalarize value type holder
+    assert(!gvn().type(holder)->maybe_null(), "Inline types are null-free");
     holder = ValueTypeNode::make_from_oop(this, holder, holder_klass);
   }
-  if (!val->is_ValueType() && field->is_flattenable()) {
-    // Null check and scalarize value type field value
-    inc_sp(nargs);
-    val = null_check(val);
-    dec_sp(nargs);
-    if (stopped()) return;
+  if (!val->is_ValueType() && field->type()->is_valuetype()) {
+    // Scalarize value type field value
+    assert(!gvn().type(holder)->maybe_null(), "Inline types are null-free");
     val = ValueTypeNode::make_from_oop(this, val, gvn().type(val)->value_klass());
-  } else if (val->is_ValueType() && !field->is_flattenable()) {
-    // Non-flattenable field value needs to be allocated because it can be merged
-    // with an oop. Re-execute withfield if buffering triggers deoptimization.
+  } else if (val->is_ValueType() && !field->type()->is_valuetype()) {
+    // Field value needs to be allocated because it can be merged with an oop.
+    // Re-execute withfield if buffering triggers deoptimization.
     PreserveReexecuteState preexecs(this);
     jvms()->set_should_reexecute(true);
     inc_sp(nargs);
     val = val->as_ValueType()->buffer(this);
   }
diff a/src/hotspot/share/opto/type.cpp b/src/hotspot/share/opto/type.cpp
--- a/src/hotspot/share/opto/type.cpp
+++ b/src/hotspot/share/opto/type.cpp
@@ -258,16 +258,15 @@
   case T_ADDRESS:
     assert(type->is_return_address(), "");
     return TypeRawPtr::make((address)(intptr_t)type->as_return_address()->bci());
 
   case T_INLINE_TYPE: {
-    bool is_never_null = type->is_never_null();
-    ciValueKlass* vk = type->unwrap()->as_value_klass();
-    if (vk->is_scalarizable() && is_never_null) {
+    ciValueKlass* vk = type->as_value_klass();
+    if (vk->is_scalarizable()) {
       return TypeValueType::make(vk);
     } else {
-      return TypeOopPtr::make_from_klass(vk)->join_speculative(is_never_null ? TypePtr::NOTNULL : TypePtr::BOTTOM);
+      return TypeOopPtr::make_from_klass(vk)->join_speculative(TypePtr::NOTNULL);
     }
   }
 
   default:
     // make sure we did not mix up the cases:
@@ -2007,11 +2006,11 @@
       field_array[pos] = TypePtr::BOTTOM;
       pos++;
       ExtendedSignature sig = ExtendedSignature(NULL, SigEntryFilter());
       collect_value_fields(return_type->as_value_klass(), field_array, pos, sig);
     } else {
-      field_array[TypeFunc::Parms] = get_const_type(return_type)->join_speculative(sig->returns_never_null() ? TypePtr::NOTNULL : TypePtr::BOTTOM);
+      field_array[TypeFunc::Parms] = get_const_type(return_type)->join_speculative(TypePtr::NOTNULL);
     }
     break;
   case T_VOID:
     break;
   default:
@@ -2073,16 +2072,15 @@
     case T_BYTE:
     case T_SHORT:
       field_array[pos++] = TypeInt::INT;
       break;
     case T_INLINE_TYPE: {
-      bool never_null = sig->is_never_null_at(i);
-      if (vt_fields_as_args && type->as_value_klass()->can_be_passed_as_fields() && never_null) {
+      if (vt_fields_as_args && type->as_value_klass()->can_be_passed_as_fields()) {
         is_flattened = true;
         collect_value_fields(type->as_value_klass(), field_array, pos, sig_cc);
       } else {
-        field_array[pos++] = get_const_type(type)->join_speculative(never_null ? TypePtr::NOTNULL : TypePtr::BOTTOM);
+        field_array[pos++] = get_const_type(type)->join_speculative(TypePtr::NOTNULL);
       }
       break;
     }
     default:
       ShouldNotReachHere();
@@ -5776,11 +5774,11 @@
   // convention (with a value type argument/return as a list of its fields).
   bool has_scalar_args = method->has_scalarized_args() && !is_osr_compilation;
   const TypeTuple* domain_sig = is_osr_compilation ? osr_domain() : TypeTuple::make_domain(method, false);
   const TypeTuple* domain_cc = has_scalar_args ? TypeTuple::make_domain(method, true) : domain_sig;
   ciSignature* sig = method->signature();
-  bool has_scalar_ret = sig->returns_never_null() && sig->return_type()->as_value_klass()->can_be_returned_as_fields();
+  bool has_scalar_ret = sig->return_type()->is_valuetype() && sig->return_type()->as_value_klass()->can_be_returned_as_fields();
   const TypeTuple* range_sig = TypeTuple::make_range(sig, false);
   const TypeTuple* range_cc = has_scalar_ret ? TypeTuple::make_range(sig, true) : range_sig;
   tf = TypeFunc::make(domain_sig, domain_cc, range_sig, range_cc);
   if (!is_osr_compilation) {
     C->set_last_tf(method, tf);  // fill cache
diff a/src/hotspot/share/opto/valuetypenode.cpp b/src/hotspot/share/opto/valuetypenode.cpp
--- a/src/hotspot/share/opto/valuetypenode.cpp
+++ b/src/hotspot/share/opto/valuetypenode.cpp
@@ -190,17 +190,10 @@
   ciField* field = value_klass()->declared_nonstatic_field_at(index);
   assert(!field->is_flattened() || field->type()->is_valuetype(), "must be a value type");
   return field->is_flattened();
 }
 
-bool ValueTypeBaseNode::field_is_flattenable(uint index) const {
-  assert(index < field_count(), "index out of bounds");
-  ciField* field = value_klass()->declared_nonstatic_field_at(index);
-  assert(!field->is_flattenable() || field->type()->is_valuetype(), "must be a value type");
-  return field->is_flattenable();
-}
-
 int ValueTypeBaseNode::make_scalar_in_safepoint(PhaseIterGVN* igvn, Unique_Node_List& worklist, SafePointNode* sfpt) {
   ciValueKlass* vk = value_klass();
   uint nfields = vk->nof_nonstatic_fields();
   JVMState* jvms = sfpt->jvms();
   int start = jvms->debug_start();
@@ -278,11 +271,10 @@
   // memory and adding the values as input edges to the node.
   for (uint i = 0; i < field_count(); ++i) {
     int offset = holder_offset + field_offset(i);
     Node* value = NULL;
     ciType* ft = field_type(i);
-    bool is_flattenable = field_is_flattenable(i);
     if (field_is_flattened(i)) {
       // Recursively load the flattened value type field
       value = ValueTypeNode::make_from_flattened(kit, ft->as_value_klass(), base, ptr, holder, offset, decorators);
     } else {
       const TypeOopPtr* oop_ptr = kit->gvn().type(base)->isa_oopptr();
@@ -295,15 +287,14 @@
         assert(field != NULL, "field not found");
         ciConstant constant = constant_oop->as_instance()->field_value(field);
         const Type* con_type = Type::make_from_constant(constant, /*require_const=*/ true);
         assert(con_type != NULL, "type not found");
         value = kit->gvn().transform(kit->makecon(con_type));
-        // Check type of constant which might be more precise
-        if (con_type->is_valuetypeptr() && !con_type->is_zero_type()) {
-          // Null-free, treat as flattenable
+        // Check type of constant which might be more precise than the static field type
+        if (con_type->is_valuetypeptr()) {
+          assert(!con_type->is_zero_type(), "Value types are null-free");
           ft = con_type->value_klass();
-          is_flattenable = true;
         }
       } else {
         // Load field value from memory
         const TypePtr* adr_type = field_adr_type(base, offset, holder, decorators, kit->gvn());
         Node* adr = kit->basic_plus_adr(base, ptr, offset);
@@ -313,12 +304,12 @@
         if (is_array) {
           decorators |= IS_ARRAY;
         }
         value = kit->access_load_at(base, adr, adr_type, val_type, bt, decorators);
       }
-      if (is_flattenable) {
-        // Loading a non-flattened but flattenable value type from memory
+      if (ft->is_valuetype()) {
+        // Loading a non-flattened value type from memory
         if (ft->as_value_klass()->is_scalarizable()) {
           value = ValueTypeNode::make_from_oop(kit, value, ft->as_value_klass());
         } else {
           value = kit->null2default(value, ft->as_value_klass());
         }
@@ -345,11 +336,11 @@
     Node* value = field_value(i);
     ciType* ft = field_type(i);
     if (field_is_flattened(i)) {
       // Recursively store the flattened value type field
       if (!value->is_ValueType()) {
-        assert(!kit->gvn().type(value)->maybe_null(), "should never be null");
+        assert(!kit->gvn().type(value)->maybe_null(), "Inline types are null-free");
         value = ValueTypeNode::make_from_oop(kit, value, ft->as_value_klass());
       }
       value->as_ValueType()->store_flattened(kit, base, ptr, holder, offset, decorators);
     } else {
       // Store field value to memory
@@ -508,11 +499,11 @@
   // Create a new ValueTypeNode with default values
   ValueTypeNode* vt = new ValueTypeNode(vk, default_oop(gvn, vk));
   for (uint i = 0; i < vt->field_count(); ++i) {
     ciType* field_type = vt->field_type(i);
     Node* value = NULL;
-    if (field_type->is_valuetype() && vt->field_is_flattenable(i)) {
+    if (field_type->is_valuetype()) {
       ciValueKlass* field_klass = field_type->as_value_klass();
       if (field_klass->is_scalarizable() || vt->field_is_flattened(i)) {
         value = ValueTypeNode::make_default(gvn, field_klass);
       } else {
         value = default_oop(gvn, field_klass);
@@ -762,12 +753,12 @@
       } else if (in) {
         parm = multi->as_Call()->in(base_input);
       } else {
         parm = gvn.transform(new ProjNode(multi->as_Call(), base_input));
       }
-      if (field_is_flattenable(idx)) {
-        // Non-flattened but flattenable value type
+      if (type->is_valuetype()) {
+        // Non-flattened value type field
         if (type->as_value_klass()->is_scalarizable()) {
           parm = ValueTypeNode::make_from_oop(kit, parm, type->as_value_klass());
         } else {
           parm = kit->null2default(parm, type->as_value_klass());
         }
diff a/src/hotspot/share/opto/valuetypenode.hpp b/src/hotspot/share/opto/valuetypenode.hpp
--- a/src/hotspot/share/opto/valuetypenode.hpp
+++ b/src/hotspot/share/opto/valuetypenode.hpp
@@ -71,11 +71,10 @@
   void      set_field_value_by_offset(int offset, Node* value);
   int           field_offset(uint index) const;
   uint          field_index(int offset) const;
   ciType*       field_type(uint index) const;
   bool          field_is_flattened(uint index) const;
-  bool          field_is_flattenable(uint index) const;
 
   // Replace ValueTypeNodes in debug info at safepoints with SafePointScalarObjectNodes
   void make_scalar_in_safepoints(PhaseIterGVN* igvn);
 
   // Store the value type as a flattened (headerless) representation
