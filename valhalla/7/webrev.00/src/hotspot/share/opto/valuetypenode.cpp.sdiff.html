<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/share/opto/valuetypenode.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="../oops/valueKlass.hpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="valuetypenode.hpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/share/opto/valuetypenode.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
382   PhiNode* mem = PhiNode::make(region, kit-&gt;merged_memory(), Type::MEMORY, TypePtr::BOTTOM);
383 
384   int bci = kit-&gt;bci();
385   bool reexecute = kit-&gt;jvms()-&gt;should_reexecute();
386   {
387     // Oop is NULL, allocate and initialize buffer
388     PreserveJVMState pjvms(kit);
389     // Propagate re-execution state and bci
390     kit-&gt;set_bci(bci);
391     kit-&gt;jvms()-&gt;set_bci(bci);
392     kit-&gt;jvms()-&gt;set_should_reexecute(reexecute);
393     kit-&gt;set_control(null_ctl);
394     kit-&gt;kill_dead_locals();
395     ciValueKlass* vk = value_klass();
396     Node* klass_node = kit-&gt;makecon(TypeKlassPtr::make(vk));
397     Node* alloc_oop  = kit-&gt;new_instance(klass_node, NULL, NULL, /* deoptimize_on_exception */ true, this);
398     store(kit, alloc_oop, alloc_oop, vk, 0);
399 
400     // Do not let stores that initialize this buffer be reordered with a subsequent
401     // store that would make this buffer accessible by other threads.

402     AllocateNode* alloc = AllocateNode::Ideal_allocation(alloc_oop, &amp;kit-&gt;gvn());
403     assert(alloc != NULL, &quot;must have an allocation node&quot;);
404     kit-&gt;insert_mem_bar(Op_MemBarStoreStore, alloc-&gt;proj_out_or_null(AllocateNode::RawAddress));
405 
406     region-&gt;init_req(2, kit-&gt;control());
407     oop   -&gt;init_req(2, alloc_oop);
408     io    -&gt;init_req(2, kit-&gt;i_o());
409     mem   -&gt;init_req(2, kit-&gt;merged_memory());
410   }
411 
412   // Update GraphKit
413   kit-&gt;set_control(kit-&gt;gvn().transform(region));
414   kit-&gt;set_i_o(kit-&gt;gvn().transform(io));
415   kit-&gt;set_all_memory(kit-&gt;gvn().transform(mem));
416   kit-&gt;record_for_igvn(region);
417   kit-&gt;record_for_igvn(oop);
418   kit-&gt;record_for_igvn(io);
419   kit-&gt;record_for_igvn(mem);
420 
421   // Use cloned ValueTypeNode to propagate oop from now on
</pre>
<hr />
<pre>
612     Node* klass_node = kit-&gt;makecon(TypeKlassPtr::make(vk));
613     Node* alloc_oop  = kit-&gt;new_instance(klass_node, NULL, NULL, true);
614     AllocateNode* alloc = AllocateNode::Ideal_allocation(alloc_oop, &amp;kit-&gt;gvn());
615     alloc-&gt;_larval = true;
616 
617     store(kit, alloc_oop, alloc_oop, vk, 0);
618     res-&gt;set_oop(alloc_oop);
619   }
620   res-&gt;set_type(TypeValueType::make(vk, true));
621   res = kit-&gt;gvn().transform(res)-&gt;as_ValueType();
622   assert(!allocate || res-&gt;is_allocated(&amp;kit-&gt;gvn()), &quot;must be allocated&quot;);
623   return res;
624 }
625 
626 ValueTypeNode* ValueTypeNode::finish_larval(GraphKit* kit) const {
627   Node* obj = get_oop();
628   Node* mark_addr = kit-&gt;basic_plus_adr(obj, oopDesc::mark_offset_in_bytes());
629   Node* mark = kit-&gt;make_load(NULL, mark_addr, TypeX_X, TypeX_X-&gt;basic_type(), MemNode::unordered);
630   mark = kit-&gt;gvn().transform(new AndXNode(mark, kit-&gt;MakeConX(~markWord::larval_mask_in_place)));
631   kit-&gt;store_to_memory(kit-&gt;control(), mark_addr, mark, TypeX_X-&gt;basic_type(), kit-&gt;gvn().type(mark_addr)-&gt;is_ptr(), MemNode::unordered);

632 
633   // Do not let stores that initialize this buffer be reordered with a subsequent
634   // store that would make this buffer accessible by other threads.
635   AllocateNode* alloc = AllocateNode::Ideal_allocation(obj, &amp;kit-&gt;gvn());
636   assert(alloc != NULL, &quot;must have an allocation node&quot;);
637   kit-&gt;insert_mem_bar(Op_MemBarStoreStore, alloc-&gt;proj_out_or_null(AllocateNode::RawAddress));
638 
639   ciValueKlass* vk = value_klass();
640   ValueTypeNode* res = clone()-&gt;as_ValueType();
641   res-&gt;set_type(TypeValueType::make(vk, false));
642   res = kit-&gt;gvn().transform(res)-&gt;as_ValueType();
643   return res;
644 }
645 











646 Node* ValueTypeNode::is_loaded(PhaseGVN* phase, ciValueKlass* vk, Node* base, int holder_offset) {
647   if (vk == NULL) {
648     vk = value_klass();
649   }
650   if (field_count() == 0) {
651     assert(is_allocated(phase), &quot;must be allocated&quot;);
652     return get_oop();
653   }
654   for (uint i = 0; i &lt; field_count(); ++i) {
655     int offset = holder_offset + field_offset(i);
656     Node* value = field_value(i);
657     if (value-&gt;is_ValueType()) {
658       ValueTypeNode* vt = value-&gt;as_ValueType();
659       if (field_is_flattened(i)) {
660         // Check value type field load recursively
661         base = vt-&gt;is_loaded(phase, vk, base, offset - vt-&gt;value_klass()-&gt;first_field_offset());
662         if (base == NULL) {
663           return NULL;
664         }
665         continue;
</pre>
</td>
<td>
<hr />
<pre>
382   PhiNode* mem = PhiNode::make(region, kit-&gt;merged_memory(), Type::MEMORY, TypePtr::BOTTOM);
383 
384   int bci = kit-&gt;bci();
385   bool reexecute = kit-&gt;jvms()-&gt;should_reexecute();
386   {
387     // Oop is NULL, allocate and initialize buffer
388     PreserveJVMState pjvms(kit);
389     // Propagate re-execution state and bci
390     kit-&gt;set_bci(bci);
391     kit-&gt;jvms()-&gt;set_bci(bci);
392     kit-&gt;jvms()-&gt;set_should_reexecute(reexecute);
393     kit-&gt;set_control(null_ctl);
394     kit-&gt;kill_dead_locals();
395     ciValueKlass* vk = value_klass();
396     Node* klass_node = kit-&gt;makecon(TypeKlassPtr::make(vk));
397     Node* alloc_oop  = kit-&gt;new_instance(klass_node, NULL, NULL, /* deoptimize_on_exception */ true, this);
398     store(kit, alloc_oop, alloc_oop, vk, 0);
399 
400     // Do not let stores that initialize this buffer be reordered with a subsequent
401     // store that would make this buffer accessible by other threads.
<span class="line-added">402     // FIXME: coordinate with ready_to_publish(kit, alloc_oop)</span>
403     AllocateNode* alloc = AllocateNode::Ideal_allocation(alloc_oop, &amp;kit-&gt;gvn());
404     assert(alloc != NULL, &quot;must have an allocation node&quot;);
405     kit-&gt;insert_mem_bar(Op_MemBarStoreStore, alloc-&gt;proj_out_or_null(AllocateNode::RawAddress));
406 
407     region-&gt;init_req(2, kit-&gt;control());
408     oop   -&gt;init_req(2, alloc_oop);
409     io    -&gt;init_req(2, kit-&gt;i_o());
410     mem   -&gt;init_req(2, kit-&gt;merged_memory());
411   }
412 
413   // Update GraphKit
414   kit-&gt;set_control(kit-&gt;gvn().transform(region));
415   kit-&gt;set_i_o(kit-&gt;gvn().transform(io));
416   kit-&gt;set_all_memory(kit-&gt;gvn().transform(mem));
417   kit-&gt;record_for_igvn(region);
418   kit-&gt;record_for_igvn(oop);
419   kit-&gt;record_for_igvn(io);
420   kit-&gt;record_for_igvn(mem);
421 
422   // Use cloned ValueTypeNode to propagate oop from now on
</pre>
<hr />
<pre>
613     Node* klass_node = kit-&gt;makecon(TypeKlassPtr::make(vk));
614     Node* alloc_oop  = kit-&gt;new_instance(klass_node, NULL, NULL, true);
615     AllocateNode* alloc = AllocateNode::Ideal_allocation(alloc_oop, &amp;kit-&gt;gvn());
616     alloc-&gt;_larval = true;
617 
618     store(kit, alloc_oop, alloc_oop, vk, 0);
619     res-&gt;set_oop(alloc_oop);
620   }
621   res-&gt;set_type(TypeValueType::make(vk, true));
622   res = kit-&gt;gvn().transform(res)-&gt;as_ValueType();
623   assert(!allocate || res-&gt;is_allocated(&amp;kit-&gt;gvn()), &quot;must be allocated&quot;);
624   return res;
625 }
626 
627 ValueTypeNode* ValueTypeNode::finish_larval(GraphKit* kit) const {
628   Node* obj = get_oop();
629   Node* mark_addr = kit-&gt;basic_plus_adr(obj, oopDesc::mark_offset_in_bytes());
630   Node* mark = kit-&gt;make_load(NULL, mark_addr, TypeX_X, TypeX_X-&gt;basic_type(), MemNode::unordered);
631   mark = kit-&gt;gvn().transform(new AndXNode(mark, kit-&gt;MakeConX(~markWord::larval_mask_in_place)));
632   kit-&gt;store_to_memory(kit-&gt;control(), mark_addr, mark, TypeX_X-&gt;basic_type(), kit-&gt;gvn().type(mark_addr)-&gt;is_ptr(), MemNode::unordered);
<span class="line-added">633   ready_to_publish(kit, obj);</span>
634 
635   // Do not let stores that initialize this buffer be reordered with a subsequent
636   // store that would make this buffer accessible by other threads.
637   AllocateNode* alloc = AllocateNode::Ideal_allocation(obj, &amp;kit-&gt;gvn());
638   assert(alloc != NULL, &quot;must have an allocation node&quot;);
639   kit-&gt;insert_mem_bar(Op_MemBarStoreStore, alloc-&gt;proj_out_or_null(AllocateNode::RawAddress));
640 
641   ciValueKlass* vk = value_klass();
642   ValueTypeNode* res = clone()-&gt;as_ValueType();
643   res-&gt;set_type(TypeValueType::make(vk, false));
644   res = kit-&gt;gvn().transform(res)-&gt;as_ValueType();
645   return res;
646 }
647 
<span class="line-added">648 void ValueTypeBaseNode::ready_to_publish(GraphKit* kit, Node* base) const {</span>
<span class="line-added">649   // Do not let stores that initialize this buffer be reordered with</span>
<span class="line-added">650   // a subsequent store that would make it accessible by other threads.</span>
<span class="line-added">651   // Required for correct non-flat array element publication.</span>
<span class="line-added">652   // (See jtreg test ValueTearing.java.)</span>
<span class="line-added">653   Node* raw_address_proj = NULL;  //FIXME</span>
<span class="line-added">654   kit-&gt;insert_mem_bar(Op_MemBarStoreStore, raw_address_proj);</span>
<span class="line-added">655   // Fails to prevent array element tearing:</span>
<span class="line-added">656   //kit-&gt;insert_mem_bar_volatile(Op_MemBarStoreStore, Compile::AliasIdxRaw, raw_address_proj);</span>
<span class="line-added">657 }</span>
<span class="line-added">658 </span>
659 Node* ValueTypeNode::is_loaded(PhaseGVN* phase, ciValueKlass* vk, Node* base, int holder_offset) {
660   if (vk == NULL) {
661     vk = value_klass();
662   }
663   if (field_count() == 0) {
664     assert(is_allocated(phase), &quot;must be allocated&quot;);
665     return get_oop();
666   }
667   for (uint i = 0; i &lt; field_count(); ++i) {
668     int offset = holder_offset + field_offset(i);
669     Node* value = field_value(i);
670     if (value-&gt;is_ValueType()) {
671       ValueTypeNode* vt = value-&gt;as_ValueType();
672       if (field_is_flattened(i)) {
673         // Check value type field load recursively
674         base = vt-&gt;is_loaded(phase, vk, base, offset - vt-&gt;value_klass()-&gt;first_field_offset());
675         if (base == NULL) {
676           return NULL;
677         }
678         continue;
</pre>
</td>
</tr>
</table>
<center><a href="../oops/valueKlass.hpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="valuetypenode.hpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>